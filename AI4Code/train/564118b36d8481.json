{"cell_type":{"adcd5d47":"code","db5986ed":"code","48270294":"code","93b53454":"code","5c047515":"code","7895edfc":"code","fdd2c232":"code","e12bb5ce":"code","8396e407":"code","5c90eabc":"code","6c07b71b":"code","21b1bb54":"code","1b0174a9":"code","03beacc3":"code","5aece4aa":"code","ca16b241":"code","4777c0a0":"code","19d51198":"code","b96166e0":"code","5900779d":"code","f3138710":"code","7180b35c":"code","0a2fdcbe":"code","351c87eb":"code","13d7ba91":"code","f675da69":"code","317933ee":"code","c571a77c":"code","78fa2ce9":"code","3a782146":"code","5962f4a4":"code","26d6727c":"code","64332891":"code","65598848":"code","23786dd8":"code","d18b5e1d":"code","ca825f9c":"code","79764114":"code","a0bed680":"code","44edde95":"code","83573ac3":"code","e3af7d1b":"code","9092f055":"code","36d7b5a6":"code","09743878":"code","a3f16ed8":"code","770211a0":"code","50c20829":"code","902cd669":"code","b6fbea6f":"code","3c2ea615":"code","0d3c9475":"code","2925d828":"code","d4345978":"code","b1f1db9e":"code","18c4f3f6":"code","dc9542a4":"code","230a82af":"code","c78aef35":"code","e35f4d7d":"code","5fbea398":"code","23cfaaf0":"code","668c9918":"code","dfb197e7":"code","48353d23":"code","bdc69d08":"code","c7909310":"code","74fd1f3c":"code","343f61d5":"code","ecce890c":"code","9be88627":"code","637780b9":"code","b5920528":"code","da054a80":"code","bf15b4e7":"code","21d337f7":"code","114506ef":"code","d4ace302":"code","70e367c7":"code","8817c261":"code","3486bb80":"code","7b68bc29":"code","27c3e354":"code","1c0fb5ff":"code","ac6bab6b":"code","4d2884d3":"code","7a8d167e":"code","e2411bcb":"code","bc59838f":"code","785339e2":"code","49e2f1c1":"code","0a809523":"code","f0cd5c87":"code","9992855b":"code","33a113ea":"code","45ae3b67":"code","1cc355b5":"code","3c168a85":"code","ac7c7fbd":"code","c7c53ce3":"code","cd725981":"code","4a7b7c13":"code","f4475038":"code","874319f3":"code","d6280f17":"code","5d794623":"code","ce2338de":"code","a899e9c2":"code","febd7646":"code","dce832b1":"code","a7d509bf":"code","a13605ef":"code","fc7280c1":"code","f8ec638d":"code","74023d33":"code","9821da54":"code","99bc194b":"code","d042575f":"code","0bfcebd0":"code","50919735":"code","8a8144af":"code","649a4238":"code","d21051f9":"code","7bc45aa1":"code","6b67d5a4":"code","ef28472d":"code","3a60f2cc":"code","3e5841b6":"code","fa13cbb2":"code","2f6107f1":"code","a8308d91":"code","1953ab20":"code","8633f274":"code","2f90143c":"code","096e44c7":"code","a6460962":"code","e7d55f2e":"code","7da365ee":"code","07294b82":"code","bfddc29e":"code","065e495c":"code","c7c21115":"code","6e1dee2f":"code","c3905665":"code","32f7a014":"code","a8d1b946":"code","affd063a":"code","e647cc29":"markdown","faf0db43":"markdown","60a5ae53":"markdown","d8451260":"markdown","6349f8e5":"markdown","2948333f":"markdown","95018208":"markdown","86633b8a":"markdown","a386c8d6":"markdown","1d5169ea":"markdown","6b162b88":"markdown","55722a02":"markdown","21482657":"markdown","738bf470":"markdown","fc1ea060":"markdown","83572fef":"markdown","ddb8dfae":"markdown","92765550":"markdown","b25b12f9":"markdown","9e387186":"markdown","f2d7e323":"markdown","991087b7":"markdown","7b9f7d81":"markdown","1891302b":"markdown","785a9ea4":"markdown","67d11292":"markdown","42b01703":"markdown","684c4c16":"markdown","d7f26a60":"markdown","8fc717b7":"markdown","7881f2c6":"markdown","28db25c9":"markdown","ee76f9ad":"markdown","65bdad95":"markdown","f0b8657f":"markdown","7d9d3816":"markdown","b0fab84c":"markdown","e6c270a8":"markdown","50a876b6":"markdown","daed543f":"markdown","c996be1b":"markdown","42c4831b":"markdown","deb6f7cd":"markdown","75d9e06e":"markdown","737e80f5":"markdown","dfdbd0ad":"markdown","ab493eb0":"markdown","6a1ce4de":"markdown","c759b55f":"markdown","e61a1304":"markdown","eee6ebdf":"markdown","7dcd32fb":"markdown","8c786323":"markdown","c9f76fac":"markdown","997e3c67":"markdown","b0ed56e3":"markdown","d0b68e04":"markdown","c00bc945":"markdown","65817547":"markdown","b7b57b37":"markdown","0c4e178d":"markdown","b724d10c":"markdown","d84f64b8":"markdown","18f56f43":"markdown","8203e5e3":"markdown","ad794565":"markdown","61c99c93":"markdown","93b6f19e":"markdown","2586bb24":"markdown","63e609d7":"markdown"},"source":{"adcd5d47":"pip install sweetviz","db5986ed":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import colors as mcolors\nimport sweetviz\n#plt.style.available\nplt.style.use('Solarize_Light2')\nplt.style.use('bmh')\n\nfrom scipy import stats \nfrom scipy.special import boxcox1p\nimport statsmodels.api as sm\n\ncolors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,MinMaxScaler,StandardScaler\n\nfrom sklearn.metrics import r2_score,mean_absolute_error,median_absolute_error,mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,ExtraTreesRegressor,AdaBoostRegressor,StackingRegressor\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.impute import KNNImputer\n\n# for feature slection\nfrom sklearn.feature_selection import SelectFromModel\n\nimport matplotlib\nall_colors = matplotlib.colors.cnames.keys()\n\nimport random\nkeys = random.sample(list(all_colors), 20)\n\nimport plotly_express as px\n\nimport warnings\nwarnings.filterwarnings('ignore',category=DeprecationWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)","48270294":"house_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhouse_test =  pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","93b53454":"my_report1 = sweetviz.compare([house_train, \"Train\"], [house_test, \"Test\"], \"SalePrice\")\nmy_report1.show_html('Compare_train_test.html')","5c047515":"compare_datatypes = pd.DataFrame({'TrainDataType':house_train.dtypes,'TestDataType':house_test.dtypes}).reset_index()\n\ncompare_datatypes['TrainDataType'] = compare_datatypes['TrainDataType'].astype('str')\ncompare_datatypes['TestDataType'] = compare_datatypes['TestDataType'].astype('str')","7895edfc":"compare_datatypes.head()","fdd2c232":"datatype_cnvt_col =[]\nfor i,j,k in zip(compare_datatypes['TrainDataType'],compare_datatypes['TestDataType'],compare_datatypes['index']):\n    if i!=j and k not in ['SalePrice']:\n        datatype_cnvt_col.append(k)\n        print(i,j,k)","e12bb5ce":"def float_to_int(dataset,columns):\n    dataset[columns] = dataset[columns].astype('Int64')\n    return dataset","8396e407":"house_test = float_to_int(house_test,datatype_cnvt_col)","5c90eabc":"house_train.head()","6c07b71b":"house_train.shape","21b1bb54":"def print_missing_values(dataset):\n    \n    '''Here we will check the percentage of nan values present in each feature'''\n    ## 1 -step make the list of features which has missing values\n    global feature_with_na\n    feature_with_na = [features for features in dataset.columns if dataset[features].isnull().sum()>1]\n\n    ## 2- step print the feature name and the percentage of missing values\n    for feature in feature_with_na:\n        print(feature,np.round(dataset[feature].isnull().mean(),4),' % missing values')","1b0174a9":"print('Missing Values for Training Data \\n')\nprint_missing_values(house_train)","03beacc3":"print('Missing Values for Testing Data \\n')\nprint_missing_values(house_test)","5aece4aa":"plt.style.use('Solarize_Light2')\nplt.style.use('bmh')\nfor feature in feature_with_na:\n    data = house_train.copy()\n    plt.figure(figsize=(10,5))\n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n    \n    # let's calculate the mean SalePrice where the information is missing or present    \n    sns.barplot(data=data.groupby(feature)['SalePrice'].median().reset_index(),x=feature,y='SalePrice')\n    #data.groupby(feature)['SalePrice'].median().plot.bar(color= keys)                                                  \n    plt.title(feature)\n    plt.show()","ca16b241":"# list of numerical variables\nnumeric_features_train = [features for features in house_train.columns if house_train[features].dtype !='O']\nprint('Number of Train numerical variables: ', len(numeric_features_train))\n\nnumeric_features_test = [features for features in house_test.columns if house_test[features].dtype !='O']\nprint('Number of Test numerical variables: ', len(numeric_features_test))\n\n# visualise the numerical variables\nhouse_train[numeric_features_train].head()","4777c0a0":"# list of variables that contain year information\nyear_feature_train = [feature for feature in numeric_features_train if 'Yr' in feature or 'Year' in feature]\nyear_feature_test = [feature for feature in numeric_features_test if 'Yr' in feature or 'Year' in feature]\nyear_feature_train","19d51198":"## Lets analyze the Temporal Datetime Variables\n## We will check whether there is a relation between year the house is sold and the sales price\n\nhouse_train.groupby('YrSold')['SalePrice'].median().plot(figsize=(10,5))\nplt.xlabel('Year Sold')\nplt.ylabel('Median House Price')\nplt.title(\"House Price vs YearSold\")","b96166e0":"## Here we will compare the difference between All years feature with SalePrice\n\nfor feature in year_feature_train:\n    if feature!='YrSold':\n        data=house_train.copy()\n        ## We will capture the difference between year variable and year the house was sold for\n        plt.figure(figsize=(10,5))\n        data[feature]=data['YrSold']-data[feature]\n        sns.lineplot(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.show()","5900779d":"discrete_var_train = [features for features in numeric_features_train if len(house_train[features].unique())<25 \n                and features not in year_feature_train+['Id']]\n\ndiscrete_var_test = [features for features in numeric_features_test if len(house_test[features].unique())<25 \n                and features not in year_feature_test+['Id']]","f3138710":"print('Total Train discrete variables are',len(discrete_var_train))\nprint(house_train[discrete_var_train].head())\n\nprint('Total Test discrete variables are',len(discrete_var_train))\nprint(house_test[discrete_var_test].head())","7180b35c":"## Lets Find the realtionship between them and Sale PRice\nfor feature in discrete_var_train:\n    data=house_train.copy()\n    plt.figure(figsize=(12,5))\n    sns.barplot(data=data.groupby(feature)['SalePrice'].median().reset_index(),x=feature,y='SalePrice')\n    plt.title(feature)\n    plt.show()","0a2fdcbe":"continuous_feature_train=[feature for feature in numeric_features_train if feature not in discrete_var_train+ year_feature_train+['Id']]\ncontinuous_feature_test=[feature for feature in numeric_features_test if feature not in discrete_var_test + year_feature_test+['Id']]\n\nprint(\"Continuous feature Train dataset Count {}\".format(len(continuous_feature_train)))\nprint(house_train[continuous_feature_train].head())\n\nprint(\"\\nContinuous feature Test dataset Count {}\".format(len(continuous_feature_test)))\nprint(house_test[continuous_feature_test].head())","351c87eb":"## Lets analyse the continuous values by creating histograms to understand the distribution\n\nfor feature in continuous_feature_train:\n    data=house_train.copy()\n    plt.figure(figsize=(15,5))\n    plt.subplot(1,2,1)\n    sns.distplot(data[feature],bins=25,hist_kws=dict(edgecolor=\"green\", linewidth=1))\n    plt.subplot(1,2,2)\n    stats.probplot(data[feature], dist=\"norm\", plot=plt)\n    #data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","13d7ba91":"## We will be using logarithmic transformation\nfor feature in continuous_feature_train:\n    data=house_train.copy()\n    plt.figure(figsize=(10,5))\n    data[feature]=np.log1p(data[feature])\n    sns.regplot(data[feature],data['SalePrice'],x_jitter=True,y_jitter=True)\n    #plt.scatter(data[feature],data['SalePrice'])\n    plt.xlabel(feature)\n    plt.ylabel('SalesPrice')\n    plt.title(feature)\n    plt.show()","f675da69":"for feature in continuous_feature_train:\n    data=house_train.copy()\n    plt.figure(figsize=(10,5))\n    data[feature]=np.log1p(data[feature])\n    sns.boxplot(data = data[feature])\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()","317933ee":"categorical_features_train = [feature for feature in house_train.columns if house_train[feature].dtypes=='O']\ncategorical_features_test = [feature for feature in house_test.columns if house_test[feature].dtypes=='O']\n\nprint(\"Total categorical_features for Train Data are: \",np.count_nonzero(categorical_features_train))\nprint(\"Total categorical_features Test Data are: \",np.count_nonzero(categorical_features_test))\nhouse_train[categorical_features_train].head()","c571a77c":"for feature in categorical_features_train:\n    data=house_train.copy()\n    plt.figure(figsize=(10,5))\n    sns.barplot(data=house_train.groupby(feature)['SalePrice'].median().reset_index(),x=feature,y='SalePrice')\n    #data.groupby(feature)['SalePrice'].median().plot.bar(color=keys,width=0.7)\n    data.groupby(feature)['SalePrice'].median().plot.line('o-')\n    plt.title(feature)\n    plt.show()","78fa2ce9":"## Let us capture all the nan values for Train Data and Test Data\n## First lets handle Categorical features which are missing\ncateg_features_nan_train=[feature for feature in house_train.columns if house_train[feature].isnull().sum()>=1 and \n              house_train[feature].dtypes=='O']\n\ncateg_features_nan_test=[feature for feature in house_test.columns if house_test[feature].isnull().sum()>=1 and \n              house_test[feature].dtypes=='O']\n\nfor feature in categ_features_nan_train:\n    print(\"{}: {}% missing values\".format(feature,np.round(house_train[feature].isnull().mean(),4)))","3a782146":"## Replace missing value with a new label\ndef replace_categorical_missing_values(dataset,features_nan):\n    data=dataset.copy()\n    #data[features_nan]=data[features_nan].fillna('Missing')\n    for col in features_nan:\n        data[col]=data[col].fillna(data[col].mode()[0])\n    return data\n\nhouse_train_final=replace_categorical_missing_values(house_train,categ_features_nan_train)\n\nhouse_test_final=replace_categorical_missing_values(house_test,categ_features_nan_test)\n\nprint('Train Data categorical values')\nprint(house_train_final[categ_features_nan_train].isnull().sum())\n\nprint('\\nTest Data categorical values')\nprint(house_test_final[categ_features_nan_test].isnull().sum())","5962f4a4":"house_train.shape","26d6727c":"house_train_final.head()","64332891":"house_test_final.head()","65598848":"## Temporal Variables (Date Time Variables)\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n    house_train_final[feature]=house_train_final['YrSold']-house_train_final[feature]\n    \nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n    house_test_final[feature]=house_test_final['YrSold']-house_test_final[feature]","23786dd8":"print(house_train_final[['YearBuilt','YearRemodAdd','GarageYrBlt']].head())\nprint(house_test_final[['YearBuilt','YearRemodAdd','GarageYrBlt']].head())","d18b5e1d":"## Now lets check for numerical variables the contains missing values for train data and test data\nnumerical_with_nan_train=[feature for feature in house_train_final.columns if house_train_final[feature].isnull()\n                    .sum()>=1 and house_train_final[feature].dtypes!='O']\n\nnumerical_with_nan_test=[feature for feature in house_test_final.columns if house_test_final[feature].isnull()\n                    .sum()>=1 and house_test_final[feature].dtypes!='O']\n\n## We will print the numerical nan variables and percentage of missing values\nprint('Numerical Missing Values in Train Data')\nfor feature in numerical_with_nan_train:\n    print(\"{}: {}% missing value\".format(feature,np.around(house_train_final[feature].isnull().mean(),4)))\n\nprint('\\n')\nprint('Numerical Missing Values in Test Data')\nfor feature in numerical_with_nan_test:\n    print(\"{}: {}% missing value\".format(feature,np.around(house_test_final[feature].isnull().mean(),4)))","ca825f9c":"house_test_final = float_to_int(house_test_final,numerical_with_nan_test)","79764114":"def relace_numeric_missing_knnImputer(dataset,features_nan):\n    imputer = KNNImputer()\n    \n    df_filled = imputer.fit_transform(dataset[features_nan])\n    new_df_imputer = pd.DataFrame(df_filled,columns=features_nan)\n    \n    data1 = pd.concat([new_df_imputer,dataset.drop(features_nan,axis=1)],axis=1)\n    return data1","a0bed680":"house_train_final = relace_numeric_missing_knnImputer(house_train_final,numerical_with_nan_train)\nhouse_test_final = relace_numeric_missing_knnImputer(house_test_final,numerical_with_nan_test)\n\n#house_test_final.loc[:,house_test_final.isnull().sum()>1].head()","44edde95":"## Replacing the numerical Missing Values\ndef replace_numeric_missing_values(dataset,features_nan):\n    for feature in features_nan:\n        \n        ## We will replace by using median since there are outliers\n        dataset[feature].fillna(round(dataset[feature].median()),inplace=True)\n    return dataset","83573ac3":"#house_train_final = replace_numeric_missing_values(house_train_final,numerical_with_nan_train)\n#house_test_final = replace_numeric_missing_values(house_test_final,numerical_with_nan_test)\n\nprint('Train Data Numeric values')\nprint(house_train_final[numerical_with_nan_train].isnull().sum())\n\nprint('\\nTest Data Numeric values')\nprint(house_test_final[numerical_with_nan_test].isnull().sum())","e3af7d1b":"house_train_final['Electrical'].replace(np.nan,'SBrkr',inplace=True)\nhouse_test_final['Electrical'].replace(np.nan,'SBrkr',inplace=True)","9092f055":"house_train_final.head()","36d7b5a6":"house_test_final.head()","09743878":"house_train_final.shape","a3f16ed8":"# For Train Dataset\nhouse_train_final['TotalSF']=house_train_final['TotalBsmtSF'] + house_train_final['1stFlrSF'] + house_train_final['2ndFlrSF']\n\nhouse_train_final['Total_Bathrooms'] = (house_train_final['FullBath'] + (0.5 * house_train_final['HalfBath']) +\n                               house_train_final['BsmtFullBath'] + (0.5 * house_train_final['BsmtHalfBath']))\n\nhouse_train_final['Total_porch_sf'] = (house_train_final['OpenPorchSF'] + house_train_final['3SsnPorch'] +\n                              house_train_final['EnclosedPorch'] + house_train_final['ScreenPorch'] +\n                              house_train_final['WoodDeckSF'])\n\nhouse_train_final[\"OverallQual_Garage_GrLivArea\"] = (house_train_final[\"OverallQual\"] * house_train_final[\"GarageArea\"] * \n                                                    house_train_final[\"GrLivArea\"])\n\nhouse_train_final['haspool'] = house_train_final['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nhouse_train_final['has2ndfloor'] = house_train_final['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nhouse_train_final['hasgarage'] = house_train_final['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nhouse_train_final['hasbsmt'] = house_train_final['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nhouse_train_final['hasfireplace'] = house_train_final['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n# For Test Dataset\nhouse_test_final['TotalSF']=house_test_final['TotalBsmtSF'] + house_test_final['1stFlrSF'] + house_test_final['2ndFlrSF']\n\nhouse_test_final['Total_Bathrooms'] = (house_test_final['FullBath'] + (0.5 * house_test_final['HalfBath']) +\n                               house_test_final['BsmtFullBath'] + (0.5 * house_test_final['BsmtHalfBath']))\n\nhouse_test_final['Total_porch_sf'] = (house_test_final['OpenPorchSF'] + house_test_final['3SsnPorch'] +\n                              house_test_final['EnclosedPorch'] + house_test_final['ScreenPorch'] +\n                              house_test_final['WoodDeckSF'])\n\nhouse_test_final[\"OverallQual_Garage_GrLivArea\"] = (house_test_final[\"OverallQual\"] * house_test_final[\"GarageArea\"] * \n                                                    house_test_final[\"GrLivArea\"])\n\nhouse_test_final['haspool'] = house_test_final['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nhouse_test_final['has2ndfloor'] = house_test_final['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nhouse_test_final['hasgarage'] = house_test_final['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nhouse_test_final['hasbsmt'] = house_test_final['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nhouse_test_final['hasfireplace'] = house_test_final['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","770211a0":"continuous_feature_train.extend(['TotalSF','Total_Bathrooms','Total_porch_sf','OverallQual_Garage_GrLivArea'])\ncontinuous_feature_test.extend(['TotalSF','Total_Bathrooms','Total_porch_sf','OverallQual_Garage_GrLivArea'])\ndiscrete_var_train.extend(['haspool','has2ndfloor','hasgarage','hasbsmt','hasfireplace'])\ndiscrete_var_test.extend(['haspool','has2ndfloor','hasgarage','hasbsmt','hasfireplace'])","50c20829":"def replace_outliers(dataset,columns):\n    \n    for feature in columns:\n        # calculate interquartile range\n        q25, q75 = np.percentile(dataset[feature],25) , np.percentile(dataset[feature],75)\n        iqr = q75 - q25\n        # calculate the outlier cutoff\n        cut_off = iqr * 1.5\n        lower, upper = q25 - cut_off, q75 + cut_off\n        median = dataset[feature].median()\n        # replace outlier with upper limit value\n        dataset[feature] = dataset[feature].apply(lambda x: upper if x > upper else(lower if x<lower else x))","902cd669":"replace_outliers(house_train_final,continuous_feature_train)\nreplace_outliers(house_test_final,continuous_feature_test)","b6fbea6f":"for feature in continuous_feature_train:\n    data=house_train_final.copy()\n    plt.figure(figsize=(10,5))\n    #data[feature]=np.log1p(data[feature])\n    sns.boxplot(data = data[feature])\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()","3c2ea615":"for feature in continuous_feature_test:\n    data=house_test_final.copy()\n    plt.figure(figsize=(10,5))\n    #data[feature]=np.log1p(data[feature])\n    sns.boxplot(data = data[feature])\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()","0d3c9475":"#Numerical Variables\n#Since the numerical variables are skewed we are converting skewed distribution to log normal distribution\ndef convert_to_lognormal_distribution(dataset,cont_features):\n    for feature in cont_features:\n        dataset[feature]=np.log1p(dataset[feature])\n    return dataset","2925d828":"def convert_to_boxcox_distribution(dataset,cont_features):\n    lam = 0.15\n    for feature in cont_features:\n        dataset = dataset.apply(lambda x: boxcox1p(x,lam))\n    return dataset\n\n#transformed_data, best_lambda = stats.boxcox(house_train_final['TotalSF']) \n#plot the distribution of the transformed data values\n#sns.distplot(transformed_data, hist=False, kde=True) ","d4345978":"house_train_final2 = house_train_final.copy()\nhouse_test_final2 = house_test_final.copy()\n\nhouse_train_final1 = convert_to_lognormal_distribution(house_train_final,continuous_feature_train)\nhouse_test_final1 = convert_to_lognormal_distribution(house_test_final,continuous_feature_test)","b1f1db9e":"## Lets analyse the continuous values by creating histograms to understand the distribution\n\nfor feature in continuous_feature_train:\n    data=house_train_final1.copy()\n    plt.figure(figsize=(15,5))\n    plt.subplot(1,2,1)\n    sns.distplot(data[feature],bins=25,hist_kws=dict(edgecolor=\"green\", linewidth=1))\n    plt.subplot(1,2,2)\n    stats.probplot(data[feature], dist=\"norm\", plot=plt)\n    #data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","18c4f3f6":"house_train_final1.head()","dc9542a4":"house_test_final1.head()","230a82af":"house_train_final1.drop('Id',axis=1,inplace=True)\nhouse_test_final1.drop('Id',axis=1,inplace=True)\n\nhouse_train_final2.drop('Id',axis=1,inplace=True)\nhouse_test_final2.drop('Id',axis=1,inplace=True)","c78aef35":"combine_data1 = pd.concat([house_train_final1,house_test_final1],axis=0)\ncombine_data2 = pd.concat([house_train_final2,house_test_final2],axis=0)","e35f4d7d":"combine_data2.shape","5fbea398":"# Find the total number of unique features in categorical variables\nprint('Train Dataset total number of unique features in categorical variables')\nfor features in categorical_features_train:\n    print(features,len(combine_data1[features].unique()))\n\nprint('\\n Test Dataset total number of unique features in categorical variables')\nfor features in categorical_features_test:\n    print(features,len(combine_data2[features].unique()))","23cfaaf0":"#Label Encoding will be applied to these columns because they contain ordinal data \n'''Alley,LotShape,LandContour,Street,Utilities,ExterQual,ExterCond,BsmtQual,BsmtCond,BsmtExposure,\nBsmtFinType1,BsmtFinType2,HeatingQC,CentralAir,KitchenQual,FireplaceQu,GarageFinish,GarageQual,GarageCond,\nPavedDrive,PoolQC,Fence '''\n\n#Other than this all other variable will be done onehot encoding.","668c9918":"#Applying label encoding\ndef label_encoding(dataset,columns):\n    label_encoder = LabelEncoder()\n    for i in columns:\n        dataset[i]=label_encoder.fit_transform(dataset[i])\n    return dataset","dfb197e7":"column_to_label_encode = ['Alley','LotShape','LandContour','Street','Utilities','ExterQual','ExterCond','BsmtQual',\n           'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC','CentralAir','KitchenQual',\n           'FireplaceQu','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence']\n\ncombine_data1 = label_encoding(combine_data1,column_to_label_encode)\ncombine_data2 = label_encoding(combine_data2,column_to_label_encode)","48353d23":"def one_hot_encoding(origin_dataset,label_enc_columns,numeric_columns):\n    #Getting all columns except columns used for Label Encoding\n    column_for_oneHot_encoding = list(set(origin_dataset.columns).difference(set(label_enc_columns)).\n                                      difference(set(numeric_columns)))\n    origin_dataset2 = pd.get_dummies(origin_dataset,columns=column_for_oneHot_encoding,drop_first=True)\n    return origin_dataset2","bdc69d08":"# list of numerical variables\nnumeric_features_train = [features for features in combine_data1.columns if combine_data1[features].dtype !='O']\nprint('Number of Train numerical variables: ', len(numeric_features_train))\n\ncombine_data1 = one_hot_encoding(combine_data1,column_to_label_encode,numeric_features_train)\ncombine_data2 = one_hot_encoding(combine_data2,column_to_label_encode,numeric_features_train)","c7909310":"combine_data1.shape","74fd1f3c":"combine_data2.shape","343f61d5":"def feature_scaling(dataset):\n    \n    scaler=MinMaxScaler()\n    data = dataset.copy()\n    \n    if 'SalePrice' in dataset.columns:\n           \n        feature_scale=[feature for feature in data.columns if feature not in ['SalePrice']]\n        \n        # transform the train and test set, and add on the Id and SalePrice variables\n        data1 = pd.concat([data[['SalePrice']].reset_index(drop=True),\n                            pd.DataFrame(scaler.fit_transform(data[feature_scale]),columns=feature_scale)],\n                            axis=1)        \n    return data1","ecce890c":"combine_data1 = feature_scaling(combine_data1)","9be88627":"house_train_final1 = combine_data1.iloc[0:1460,:]\nhouse_train_final2 = combine_data2.iloc[0:1460,:]\n\nhouse_test_final1 = combine_data1.iloc[1460:,1:]\nhouse_test_final2 = combine_data2.iloc[1460:,1:]","637780b9":"house_train_final1.head()","b5920528":"house_test_final1.head()","da054a80":"## Capture the dependent feature\ny_train=house_train_final1[['SalePrice']]\ny_train2=house_train_final2[['SalePrice']]\n\n## drop dependent feature from dataset\nX_train=house_train_final1.drop(['SalePrice'],axis=1)\nX_train2=house_train_final2.drop(['SalePrice'],axis=1)\n\nX_test = house_test_final1.copy()\nX_test2 = house_test_final2.copy()","bf15b4e7":"house_test_final2.shape","21d337f7":"# remember to set the seed, the random state in this function\n#0.0007 (70) with Lasso Model alone give 13.5 acc\nfeature_sel_model = SelectFromModel(Lasso(alpha=0.0007))\nfeature_sel_model.fit(X_train, y_train)\n\nfeature_sel_model.get_support()","114506ef":"l1 = 0\nlen([l1+1 for i in feature_sel_model.get_support() if i])","d4ace302":"# this is how we can make a list of the selected features\nselected_feat = X_train.columns[(feature_sel_model.get_support())]\n\n# let's print some stats\nprint('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(np.sum(feature_sel_model.estimator_.coef_ == 0)))","70e367c7":"X_train = X_train[selected_feat]\nX_train2 = X_train2[selected_feat]\n\nX_test = X_test[selected_feat]\nX_test2 = X_test2[selected_feat]","8817c261":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns   df.corr()>0.7 OR df.corr()<-0.7\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (i!=j) and corr_matrix.iloc[i, j] >= threshold and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","3486bb80":"corr_features = correlation(X_train, 0.90) #setting threshold to 90%\nprint(set(corr_features))\n\n#dropping highly correlated features\nX_train.drop(corr_features,axis=1,inplace=True)\nX_train2.drop(corr_features,axis=1,inplace=True)\nX_test.drop(corr_features,axis=1,inplace=True)\nX_test2.drop(corr_features,axis=1,inplace=True)","7b68bc29":"## Lets analyse the continuous values by creating histograms to understand the distribution\nselected_feat = list(set(selected_feat).difference(set(corr_features)))\nfor feature in selected_feat[0:10]:\n    plt.figure(figsize=(12,5))\n    sns.histplot(X_train[feature],bins=25,kde=True)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","27c3e354":"## Lets analyse the continuous values by creating histograms to understand the distribution\nfor feature in selected_feat[0:10]:\n    plt.figure(figsize=(12,5))\n    sns.boxplot(data=X_train[feature])\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","1c0fb5ff":"sm.OLS(y_train,X_train).fit().summary()","ac6bab6b":"#feature_with_high_t_value = [feature for feature, value in dict(round(sm.OLS(y_train,X_train).fit().pvalues,4)).items() if value>0.05]\n#feature_with_high_t_value\n\n#We can notice there are some features where P>|t| condition fails i.e for some features have |t| value higher than P value which is 0.05. And features should be removed to optimize the model and keep Null Hypothesis condition valid i.e no relationship between two measured phenomena.\n\n#feature_with_high_t_value = ['HeatingQC','WoodDeckSF','GarageCars','GarageArea','OpenPorchSF','TotalSF','Total_Bathrooms']\n\n# for l1 in feature_with_high_t_value:\n#     selected_feat.remove(l1)\n    \n# X_train = X_train[selected_feat]\n# X_test = X_test[selected_feat]","4d2884d3":"model_result = pd.DataFrame()\n\n# define the base models for implementing stacking\nlevel0 = list()","7a8d167e":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.3, random_state = 0)\n\nX_train2, X_val2, y_train2, y_val2 = train_test_split(X_train2, y_train2, test_size = 0.3, random_state = 0)","e2411bcb":"# Fitting Simple Linear Regression to the Training set\nlin_regressor = LinearRegression()\nlin_regressor.fit(X_train, y_train)\n\n# Predicting the Validation set results\ny_pred = lin_regressor.predict(X_val)\n\nprint('R2 score of training data for Linear regression is {0} %'.format(\n    round(r2_score(y_train, lin_regressor.predict(X_train)),2)*100))\nprint('R2 score of CV data for Linear regression is {0} %'.format(round(r2_score(y_val, y_pred),2)*100))\nprint('RMSE of of CV data for Linear regression is {0}'.format(round(mean_squared_error(y_val, y_pred,squared=False),3)))\nprint('MAE of CV data for Linear regression is {0}'.format(round(mean_absolute_error(y_val, y_pred),3)))\nprint('Median Absolute Error of CV data for Linear regression is {0}'.format(round(median_absolute_error(y_val, y_pred),3)))","bc59838f":"plt.figure(figsize=(15,5))\nsns.regplot(x=y_val,y=y_pred)\nplt.show()","785339e2":"# Predicting the Test set results\ny_pred_linear_reg = lin_regressor.predict(X_test)\n\nlevel0.append(('Linar-Reg',lin_regressor))","49e2f1c1":"model_result = model_result.append({\n                'Algorithm' : str(lin_regressor).split('(')[0],\n                'Train_R2_Score' : round(r2_score(y_train, lin_regressor.predict(X_train)),2)*100,\n                'Test_R2_Score' : round(r2_score(y_val, y_pred),2)*100,\n                'RMSE' : round(mean_squared_error(np.exp(y_val), np.exp(y_pred),squared=False),2),\n                'MAE' : round(mean_absolute_error(np.exp(y_val), np.exp(y_pred)),2),\n                'Median AE' : round(median_absolute_error(np.exp(y_val), np.exp(y_pred)),2)\n               },ignore_index=True)","0a809523":"#Creating Lasso Regressor\nlasso_regressor=Lasso()\nlasso_regressor.fit(X_train2, y_train2)\n\n# Predicting the Validation set results\ny_pred = lasso_regressor.predict(X_val2)\n\nprint('R2 score of training data for Lasso regression is {0} %'.format(\n    round(r2_score(y_train2, lasso_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for Lasso regression is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for Lasso regression is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for Lasso regression is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for Lasso regression is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","f0cd5c87":"plt.figure(figsize=(15,5))\nsns.regplot(x=y_val2,y=y_pred)\nplt.show()","9992855b":"# Predicting the Test set results\ny_pred_lasso_reg = lasso_regressor.predict(X_test2)\n\nlevel0.append(('Lasso-Reg',lasso_regressor))","33a113ea":"model_result = model_result.append({\n                'Algorithm' : str(lasso_regressor).split('(')[0],\n                'Train_R2_Score' : round(r2_score(y_train2, lasso_regressor.predict(X_train2)),2)*100,\n                'Test_R2_Score' : round(r2_score(y_val2, y_pred),2)*100,\n                'RMSE' : round(mean_squared_error(y_val2,y_pred,squared=False),2),\n                'MAE' : round(mean_absolute_error(y_val2,y_pred),2),\n                'Median AE' : round(median_absolute_error(y_val2,y_pred),2)\n               },ignore_index=True)","45ae3b67":"#Creating Ridge Regressor\nridge_regressor=Ridge(alpha=1)\nridge_regressor.fit(X_train2, y_train2)\n\n# Predicting the Validation set results\ny_pred = ridge_regressor.predict(X_val2)\n\nprint('R2 score of training data for Ridge regression is {0} %'.format(\n    round(r2_score(y_train2, ridge_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for Ridge regression is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for Ridge regression is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for Ridge regression is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for Ridge regression is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","1cc355b5":"plt.figure(figsize=(15,5))\nsns.regplot(x=y_val,y=y_pred)\nplt.show()","3c168a85":"# Predicting the Test set results\ny_pred_ridge_reg = ridge_regressor.predict(X_test2)\n\nlevel0.append(('Ridge-Reg',ridge_regressor))","ac7c7fbd":"model_result = model_result.append({\n                'Algorithm' : str(ridge_regressor).split('(')[0],\n                'Train_R2_Score' : round(r2_score(y_train2, ridge_regressor.predict(X_train2)),2)*100,\n                'Test_R2_Score' : round(r2_score(y_val2, y_pred),2)*100,\n                'RMSE' : round(mean_squared_error(y_val2,y_pred,squared=False),2),\n                'MAE' : round(mean_absolute_error(y_val2,y_pred),2),\n                'Median AE' : round(median_absolute_error(y_val2,y_pred),2)\n               },ignore_index=True)","c7c53ce3":"#Creating Ridge Regressor\nknn_regressor= KNeighborsRegressor()\nknn_regressor.fit(X_train, y_train)\n\n# Predicting the Validation set results\ny_pred = knn_regressor.predict(X_val)\n\nprint('R2 score of training data for KNN regression is {0} %'.format(\n    round(r2_score(y_train, knn_regressor.predict(X_train)),2)*100))\nprint('R2 score of CV data for KNN regression is {0} %'.format(round(r2_score(y_val, y_pred),2)*100))\nprint('RMSE of of CV data for KNN regression is {0}'.format(round(mean_squared_error(y_val, y_pred,squared=False),3)))\nprint('MAE of CV data for KNN regression is {0}'.format(round(mean_absolute_error(y_val, y_pred),3)))\nprint('Median Absolute Error of CV data for KNN regression is {0}'.format(round(median_absolute_error(y_val, y_pred),3)))","cd725981":"def print_result(results):\n    print('Best Parameter {0} \\n'.format(results.best_params_))\n    print('Best Score {0} \\n'.format(results.best_score_))\n    \nparameters = {\n    'n_neighbors' : [5,7,9,11,15,17],\n    'metric':['minkowski'],\n    'leaf_size' :[30,35,40,45,50],\n    'p':[1,2],\n    'weights':['uniform', 'distance'],\n    'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute']\n}\ncv = RandomizedSearchCV(estimator=knn_regressor,param_distributions=parameters,n_iter=35,cv=10,n_jobs=-1)\ncv.fit(X_train,y_train)\nprint_result(cv)","4a7b7c13":"#Creating Decision Tree Regressor\nknn_regressor=cv.best_estimator_\n\nknn_regressor.fit(X_train, y_train)\n\n# Predicting the Validation set results\ny_pred = knn_regressor.predict(X_val)\n\nprint('R2 score of training data for KNN regression is {0} %'.format(\n    round(r2_score(y_train, knn_regressor.predict(X_train)),2)*100))\nprint('R2 score of CV data for KNN regression is {0} %'.format(round(r2_score(y_val, y_pred),2)*100))\nprint('RMSE of of CV data for KNN regression is {0}'.format(round(mean_squared_error(y_val, y_pred,squared=False),3)))\nprint('MAE of CV data for KNN regression is {0}'.format(round(mean_absolute_error(y_val, y_pred),3)))\nprint('Median Absolute Error of CV data for KNN regression is {0}'.format(round(median_absolute_error(y_val, y_pred),3)))","f4475038":"#Creating Decision Tree Regressor\ndt_regressor=DecisionTreeRegressor(random_state=69)\ndt_regressor.fit(X_train2, y_train2)\n\n# Predicting the Validation set results\ny_pred = dt_regressor.predict(X_val2)\n\nprint('R2 score of training data for Decison Tree regression is {0} %'.format(\n    round(r2_score(y_train2, dt_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for Decison Tree regression is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for Decison Tree regression is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for Decison Tree regression is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for Decison Tree regression is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","874319f3":"path = dt_regressor.cost_complexity_pruning_path(X_train2, y_train2)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\nccp_alphas[0:10]","d6280f17":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeRegressor(random_state=67, ccp_alpha=ccp_alpha)\n    clf.fit(X_train2, y_train2)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(clfs[-1].tree_.node_count, ccp_alphas[-1]))","5d794623":"train_scores = [clf.score(X_train2, y_train2) for clf in clfs]\ntest_scores = [clf.score(X_val2, y_val2) for clf in clfs]\n\nfig, ax = plt.subplots(figsize=(15,5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","ce2338de":"def print_result(results):\n    print('Best Parameter {0} \\n'.format(results.best_params_))\n    print('Best Score {0} \\n'.format(results.best_score_))","a899e9c2":"parameters = {\n    'criterion' : [\"mse\", \"friedman_mse\", \"mae\"],\n    'max_depth' : [3,5,7,9,11],\n    'min_samples_split' : [5,10,20,30,40],\n    'min_samples_leaf':[5,10,20,30,40],\n    'max_features':[\"auto\",\"sqrt\",\"log2\"]\n}\ncv = RandomizedSearchCV(estimator=dt_regressor,param_distributions=parameters,n_iter=35,cv=10,n_jobs=-1)\ncv.fit(X_train2,y_train2)\nprint_result(cv)","febd7646":"plt.figure(figsize=(15,20))\nfeat_imp = cv.best_estimator_.feature_importances_\nindices = np.argsort(feat_imp)\nplt.yticks(range(len(indices)),[X_train2.columns[i] for i in indices])\nplt.barh(range(len(indices)),feat_imp[indices],color='r',align='center')\nplt.show()","dce832b1":"feature_imp = pd.Series(dt_regressor.feature_importances_,index=X_train.columns).sort_values(ascending=False)\nfeature_imp.head()","a7d509bf":"#Creating Decision Tree Regressor\ndt_regressor=cv.best_estimator_\ndt_regressor.fit(X_train2, y_train2)\n\n# Predicting the Validation set results\ny_pred = dt_regressor.predict(X_val2)\n\nprint('R2 score of training data for Decison Tree regression is {0} %'.format(\n    round(r2_score(y_train2, dt_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for Decison Tree regression is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for Decison Tree regression is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for Decison Tree regression is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for Decison Tree regression is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","a13605ef":"# Predicting the Test set results\ny_pred_dec_tree_reg = dt_regressor.predict(X_test2)\n\nlevel0.append(('DT-Reg',dt_regressor))","fc7280c1":"model_result = model_result.append({\n                'Algorithm' : str(cv.best_estimator_).split('(')[0],\n                'Train_R2_Score' : round(r2_score(y_train2, dt_regressor.predict(X_train2)),2)*100,\n                'Test_R2_Score' : round(r2_score(y_val2, y_pred),2)*100,\n                'RMSE' : round(mean_squared_error(y_val2,y_pred,squared=False),2),\n                'MAE' : round(mean_absolute_error(y_val2,y_pred),2),\n                'Median AE' : round(median_absolute_error(y_val2,y_pred),2)\n               },ignore_index=True)","f8ec638d":"#Creating Random Forest Regressor\nrf_regressor=RandomForestRegressor(random_state=6)\nrf_regressor.fit(X_train2, y_train2.values.reshape(1022,))\n\n# Predicting the Validation set results\ny_pred = rf_regressor.predict(X_val2)\n\nprint('R2 score of training data for Random Forest regression is {0} %'.format(\n    round(r2_score(y_train2, rf_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for Random Forest regression is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for Random Forest regression is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for Random Forest regression is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for Random Forest regression is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","74023d33":"parameters = {\n    'n_estimators' :[50,100,200,300,400],\n    'criterion' : [\"mse\", \"friedman_mse\", \"mae\"],\n    'max_features' : ['auto','sqrt','log2'],\n    'max_depth' : [3,5,7,9,11],\n    'min_samples_split' : [5,10,20,30,40],\n    'bootstrap':[True],\n    'min_samples_leaf':[5,10,20,30],\n}\ncv = RandomizedSearchCV(estimator=rf_regressor,param_distributions=parameters,n_iter=30,cv=10,n_jobs=-1)\ncv.fit(X_train2,y_train2.values.ravel())\nprint_result(cv)","9821da54":"#Creating Random Forest Regressor\nrf_regressor=cv.best_estimator_\nrf_regressor.fit(X_train2, y_train2.values.reshape(1022,))\n\n# Predicting the Validation set results\ny_pred = rf_regressor.predict(X_val2)\n\nprint('R2 score of training data for Random Forest regression is {0} %'.format(\n    round(r2_score(y_train2, rf_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for Random Forest regression is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for Random Forest regression is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for Random Forest regression is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for Random Forest regression is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","99bc194b":"# Predicting the Test set results\ny_pred_rf_reg = rf_regressor.predict(X_test2)\n\nlevel0.append(('RF-Reg',rf_regressor))","d042575f":"model_result = model_result.append({\n                'Algorithm' : str(cv.best_estimator_).split('(')[0],\n                'Train_R2_Score' : round(r2_score(y_train2, rf_regressor.predict(X_train2)),2)*100,\n                'Test_R2_Score' : round(r2_score(y_val2, y_pred),2)*100,\n                'RMSE' : round(mean_squared_error(y_val2,y_pred,squared=False),2),\n                'MAE' : round(mean_absolute_error(y_val2,y_pred),2),\n                'Median AE' : round(median_absolute_error(y_val2,y_pred),2)\n               },ignore_index=True)","0bfcebd0":"#Creating Extra Tree Regressor\next_regressor=ExtraTreesRegressor(random_state=20)\next_regressor.fit(X_train2, y_train2.values.ravel())\n\n# Predicting the Validation set results\ny_pred = ext_regressor.predict(X_val2)\n\nprint('R2 score of training data for Extra Tree regression is {0} %'.format(\n    round(r2_score(y_train2, ext_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for Extra Tree regression is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for Extra Tree regression is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for Extra Tree regression is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for Extra Tree regression is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","50919735":"parameters = {\n    'n_estimators' :[50,100,200,300,400],\n    'criterion' : [\"mse\", \"friedman_mse\", \"mae\"],\n    'max_features' : ['auto','sqrt','log2'],\n    'max_depth' : [3,5,7,9,11],\n    'min_samples_split' : [5,10,20,30,40],\n    'bootstrap':[True,False],\n    'min_samples_leaf':[5,10,20,30]\n}\ncv = RandomizedSearchCV(estimator=ext_regressor,param_distributions=parameters,n_iter=25,cv=10,n_jobs=-1)\ncv.fit(X_train2,y_train2.values.ravel())\nprint_result(cv)","8a8144af":"#Creating Extra Tree Regressor\next_regressor=cv.best_estimator_\next_regressor.fit(X_train2, y_train2.values.ravel())\n\n# Predicting the Validation set results\ny_pred = ext_regressor.predict(X_val2)\n\nprint('R2 score of training data for Extra Tree regression is {0} %'.format(\n    round(r2_score(y_train2, ext_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for Extra Tree regression is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for Extra Tree regression is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for Extra Tree regression is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for Extra Tree regression is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","649a4238":"# Predicting the Test set results\ny_pred_ex_tree_reg = ext_regressor.predict(X_test2)\n\nlevel0.append(('ExT-Reg',ext_regressor))","d21051f9":"model_result = model_result.append({\n                'Algorithm' : str(cv.best_estimator_).split('(')[0],\n                'Train_R2_Score' : round(r2_score(y_train2, ext_regressor.predict(X_train2)),2)*100,\n                'Test_R2_Score' : round(r2_score(y_val2, y_pred),2)*100,\n                'RMSE' : round(mean_squared_error(y_val2,y_pred,squared=False),2),\n                'MAE' : round(mean_absolute_error(y_val2,y_pred),2),\n                'Median AE' : round(median_absolute_error(y_val2,y_pred),2)\n               },ignore_index=True)","7bc45aa1":"#Creating AdaBoost Regressor\nada_boost_regressor=AdaBoostRegressor(random_state=0)\nada_boost_regressor.fit(X_train2, y_train2.values.ravel())\n\n# Predicting the Validation set results\ny_pred = ada_boost_regressor.predict(X_val2)\n\nprint('R2 score of training data for AdaBoost Regressor is {0} %'.format(\n    round(r2_score(y_train2, ada_boost_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for AdaBoost Regressor is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for AdaBoost Regressor is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for AdaBoost Regressor is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for AdaBoost Regressor is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","6b67d5a4":"parameters = {\n    'base_estimator':[None,5,7,9],\n    'n_estimators' :[50,100,200,300,400],\n    'learning_rate':[0.01,0.1,1,5],\n    'loss': ['linear', 'square', 'exponential'],\n}\ncv = RandomizedSearchCV(estimator=ada_boost_regressor,param_distributions=parameters,n_iter=25,cv=10,n_jobs=-1)\ncv.fit(X_train2,y_train2.values.ravel())\nprint_result(cv)","ef28472d":"#Creating AdaBoost Regressor\nada_boost_regressor=cv.best_estimator_\nada_boost_regressor.fit(X_train2, y_train2.values.ravel())\n\n# Predicting the Validation set results\ny_pred = ada_boost_regressor.predict(X_val2)\n\nprint('R2 score of training data for AdaBoost Regressor is {0} %'.format(\n    round(r2_score(y_train2, ada_boost_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for AdaBoost Regressor is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for AdaBoost Regressor is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for AdaBoost Regressor is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for AdaBoost Regressor is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","3a60f2cc":"# Predicting the Test set results\ny_pred_ada_boost_reg = ada_boost_regressor.predict(X_test2)\n\nlevel0.append(('AdaBoost-Reg',ada_boost_regressor))","3e5841b6":"model_result = model_result.append({\n                'Algorithm' : str(cv.best_estimator_).split('(')[0],\n                'Train_R2_Score' : round(r2_score(y_train2, ada_boost_regressor.predict(X_train2)),2)*100,\n                'Test_R2_Score' : round(r2_score(y_val2, y_pred),2)*100,\n                'RMSE' : round(mean_squared_error(y_val2,y_pred,squared=False),2),\n                'MAE' : round(mean_absolute_error(y_val2,y_pred),2),\n                'Median AE' : round(median_absolute_error(y_val2,y_pred),2)\n               },ignore_index=True)","fa13cbb2":"#Creating Gradient Boost Regressor\ngb_regressor=GradientBoostingRegressor(random_state=0)\ngb_regressor.fit(X_train2, y_train2.values.reshape(1022,))\n\n# Predicting the Validation set results\ny_pred = gb_regressor.predict(X_val2)\n\nprint('R2 score of training data for Gradient Boost regression is {0} %'.format(\n    round(r2_score(y_train2, gb_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for Gradient Boost regression is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for Gradient Boost regression is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for Gradient Boost regression is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for Gradient Boost regression is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","2f6107f1":"parameters = {\n    'n_estimators' :[100,200,300,400],\n    'learning_rate':[0.01,0.1,0.5,1],\n    'criterion' : [\"mse\", \"friedman_mse\", \"mae\"],\n    'loss': ['ls', 'lad', 'huber', 'quantile'],\n    'max_depth' : [3,5,7,9],\n    'max_features' : [None,'auto', 'sqrt', 'log2'],\n    'min_samples_split' : [2,5,10,20,30,40],\n    'min_samples_leaf':[1,5,10,20]\n}\ncv = RandomizedSearchCV(estimator=gb_regressor,param_distributions=parameters,n_iter=30,cv=10,n_jobs=-1)\ncv.fit(X_train2,y_train2.values.ravel())\nprint_result(cv)","a8308d91":"#Creating Gradient Boost Regressor\ngb_regressor=cv.best_estimator_\ngb_regressor.fit(X_train2, y_train2.values.reshape(1022,))\n\n# Predicting the Validation set results\ny_pred = gb_regressor.predict(X_val2)\n\nprint('R2 score of training data for Gradient Boost regression is {0} %'.format(\n    round(r2_score(y_train2, gb_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for Gradient Boost regression is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for Gradient Boost regression is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for Gradient Boost regression is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for Gradient Boost regression is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","1953ab20":"# Predicting the Test set results\ny_pred_gboost_reg = gb_regressor.predict(X_test2)\n\nlevel0.append(('GradBoost-Reg',gb_regressor))","8633f274":"model_result = model_result.append({\n                'Algorithm' : str(cv.best_estimator_).split('(')[0],\n                'Train_R2_Score' : round(r2_score(y_train2, gb_regressor.predict(X_train2)),2)*100,\n                'Test_R2_Score' : round(r2_score(y_val2, y_pred),2)*100,\n                'RMSE' : round(mean_squared_error(y_val2,y_pred,squared=False),2),\n                'MAE' : round(mean_absolute_error(y_val2,y_pred),2),\n                'Median AE' : round(median_absolute_error(y_val2,y_pred),2)\n               },ignore_index=True)","2f90143c":"#Creating Gradient Boost Regressor\nxgboost = XGBRegressor()\nxgboost.fit(X_train2, y_train2.values.reshape(1022,))\n\n# Predicting the Validation set results\ny_pred = xgboost.predict(X_val2)\n\nprint('R2 score of training data for XG Boost regression is {0} %'.format(\n    round(r2_score(y_train2, xgboost.predict(X_train2)),2)*100))\nprint('R2 score of CV data for XG Boost regression is {0} %'.format(round(r2_score(y_val2, y_pred),2)*100))\nprint('RMSE of of CV data for XG Boost regression is {0}'.format(round(mean_squared_error(y_val2, y_pred,squared=False),3)))\nprint('MAE of CV data for XG Boost regression is {0}'.format(round(mean_absolute_error(y_val2, y_pred),3)))\nprint('Median Absolute Error of CV data for XG Boost regression is {0}'.format(round(median_absolute_error(y_val2, y_pred),3)))","096e44c7":"# parameters = {\n#     'n_estimators' :[1000,2000,3000,4000,5000],\n#     'learning_rate':[0.1,0.5],\n#     'booster' :['gbtree','dart'],\n#     'min_child_weight ' : [0,1,3,5,7,9],\n#     'max_depth' : [3,5,7,9,11],\n#     'subsample' : [0.4,0.6,0.8,1],\n#     'lambda' : [0,1,5],\n#     'gamma' : [0,1,5], \n#     'colsample_bytree' : [0.4,0.6,0.8,1],\n#     'objective' :['reg:squarederror'],\n#     'importance_type' :[\"gain\", \"weight\", \"cover\", \"total_gain\" ,\"total_cover\"],\n#     'scale_pos_weight': [1],\n#     'max_features' : [None,'auto', 'sqrt', 'log2'],\n#     'min_samples_split' : [2,5,10,20,30,40],\n#     'min_samples_leaf':[1,5,10,20]\n# }\n# cv = RandomizedSearchCV(estimator=xgboost,param_distributions=parameters,n_iter=35,cv=10,n_jobs=-1,verbose=1)\n# cv.fit(X_train2,y_train2.values.ravel())\n# print_result(cv)","a6460962":"#from tqdm import tqdm \nxgb_regressor = XGBRegressor(learning_rate=0.01,n_estimators=4500,\n                                     max_depth=3, min_child_weight=1,\n                                     gamma=1, subsample=.5,reg_alpha=0, reg_lambda=1,\n                                     colsample_bytree=.5,booster='gbtree',\n                                     objective='reg:squarederror', nthread=-1,\n                                     scale_pos_weight=1)\n#xgb_regressor = cv.best_estimator_\nxgb_regressor.fit(X_train2, y_train2.values)\n\n# Predicting the Test set results\ny_pred2 = xgb_regressor.predict(X_val2)\n\n\nprint('R2 score of training data for XG Boost regression is {0} %'.format(\n    round(r2_score(y_train2, xgb_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for XG Boost regression is {0} %'.format(round(r2_score(y_val2, y_pred2),2)*100))\nprint('RMSE of of CV data for XG Boost regression is {0}'.format(round(mean_squared_error(y_val2, y_pred2,squared=False),3)))\nprint('MAE of CV data for XG Boost regression is {0}'.format(round(mean_absolute_error(y_val2, y_pred2),3)))\nprint('Median Absolute Error of CV data for XG Boost regression is {0}'.format(round(median_absolute_error(y_val2, y_pred2),3)))","e7d55f2e":"# Predicting the Test set results\ny_pred_xgboost_reg = xgb_regressor.predict(X_test2)\n\nlevel0.append(('XGBoost-Reg',xgb_regressor))","7da365ee":"y_pred_xgboost_reg[0:15]","07294b82":"model_result = model_result.append({\n                'Algorithm' : str(xgb_regressor).split('(')[0],\n                'Train_R2_Score' : round(r2_score(y_train2, xgb_regressor.predict(X_train2)),2)*100,\n                'Test_R2_Score' : round(r2_score(y_val2, y_pred2),2)*100,\n                'RMSE' : round(mean_squared_error(y_val2,y_pred2,squared=False),2),\n                'MAE' : round(mean_absolute_error(y_val2,y_pred2),2),\n                'Median AE' : round(median_absolute_error(y_val2,y_pred2),2)\n               },ignore_index=True)","bfddc29e":"level0","065e495c":"# define meta learner model, taking XG boost\nlevel1 = xgb_regressor\n\n# define the stacking ensemble\nstak_regressor = StackingRegressor(estimators=level0, final_estimator=level1, cv=10)\n# fit the model on all available data\n#xgb_regressor = cv.best_estimator_\nstak_regressor.fit(X_train2, y_train2.values.reshape(1022,))\n\n# Predicting the Test set results\ny_pred2 = stak_regressor.predict(X_val2)\n\nprint('R2 score of training data for Stacking regressor is {0} %'.format(\n    round(r2_score(y_train2, stak_regressor.predict(X_train2)),2)*100))\nprint('R2 score of CV data for Stacking regressor is {0} %'.format(round(r2_score(y_val2, y_pred2),2)*100))\nprint('RMSE of of CV data for Stacking regressor is {0}'.format(round(mean_squared_error(y_val2, y_pred2,squared=False),3)))\nprint('MAE of CV data for XG Boost regressor is {0}'.format(round(mean_absolute_error(y_val2, y_pred2),3)))\nprint('Median Absolute Error of CV data for XG Boost regressor is {0}'.format(round(median_absolute_error(y_val2, y_pred2),3)))\n\n# Predicting the Test set results\ny_pred_stack_reg = stak_regressor.predict(X_test2)","c7c21115":"model_result = model_result.append({\n                'Algorithm' : str(stak_regressor).split('(')[0],\n                'Train_R2_Score' : round(r2_score(y_train2, stak_regressor.predict(X_train2)),2)*100,\n                'Test_R2_Score' : round(r2_score(y_val2, y_pred2),2)*100,\n                'RMSE' : round(mean_squared_error(y_val2,y_pred2,squared=False),2),\n                'MAE' : round(mean_absolute_error(y_val2,y_pred2),2),\n                'Median AE' : round(median_absolute_error(y_val2,y_pred2),2)\n               },ignore_index=True)","6e1dee2f":"model_result","c3905665":"plt.figure(figsize=(19,5))\nplt.subplot(121)\nsns.lineplot(data=model_result,x='Algorithm',y='Train_R2_Score',linestyle='dashed', marker='o',color='red',markersize=7)\nsns.lineplot(data=model_result,x='Algorithm',y='Test_R2_Score',linestyle='dashed', marker='o',color='green',markersize=7)\nplt.xticks(rotation=55)\nplt.yticks(np.arange(0,110,10))\nplt.text(5, 70, 'Train_R2_Score', style='italic',bbox={'facecolor': 'Red'},fontsize=10)\nplt.text(5, 60, 'Test_R2_Score', style='italic',bbox={'facecolor': 'Green'},fontsize=10)\nplt.title('R2 Score Comparison')\n\nplt.subplot(122)\nsns.lineplot(data=model_result,x='Algorithm',y='RMSE',linestyle='dashed', marker='o',color='blue',markersize=7)\nplt.xticks(rotation=55)\nplt.text(5,29000, 'RMSE Value', style='italic',bbox={'facecolor': 'skyblue'},fontsize=15)\nplt.title('RMSE Comparison')\nplt.show()","32f7a014":"predictions = pd.DataFrame({'Linear_reg_predValues':list(np.round(np.exp(y_pred_linear_reg.ravel()),2)),\n                           'Lasso_reg_predValues':list(np.round(y_pred_lasso_reg.ravel(),2)),\n                           'Ridge_reg_predValues':list(np.round(y_pred_ridge_reg.ravel(),2)),\n                           'DeciTree_reg_predValues':list(np.round(y_pred_dec_tree_reg.ravel(),2)),\n                           'ExtraTree_reg_predValues':list(np.round(y_pred_ex_tree_reg.ravel(),2)),\n                           'RandForest_reg_predValues':list(np.round(y_pred_rf_reg.ravel(),2)),\n                           'AdaBoost_reg_predValues':list(np.round(y_pred_ada_boost_reg.ravel(),2)),\n                           'Gradboost_reg_predValues':list(np.round(y_pred_gboost_reg.ravel(),2)),\n                           'XGboost_reg_predValues':list(np.round(y_pred_xgboost_reg.ravel(),2)),\n                           'Stacking_reg_predValues':list(np.round(y_pred_stack_reg.ravel(),2))\n                          })\npredictions.head()","a8d1b946":"submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","affd063a":"submission.iloc[:,1] = list(np.round(np.exp(y_pred_linear_reg).ravel(),2))\nsubmission.to_csv('house_prediction_linear.csv',index=False)\n\nsubmission.iloc[:,1] = list(np.round(y_pred_lasso_reg.ravel(),2))\nsubmission.to_csv('house_prediction_lasso.csv',index=False)\n\nsubmission.iloc[:,1] = list(np.round(y_pred_ridge_reg.ravel(),2))\nsubmission.to_csv('house_prediction_ridge.csv',index=False)\n\nsubmission.iloc[:,1] = list(np.round(y_pred_rf_reg.ravel(),2))\nsubmission.to_csv('house_prediction_rf.csv',index=False)\n\nsubmission.iloc[:,1] = list(np.round(y_pred_gboost_reg.ravel(),2))\nsubmission.to_csv('house_prediction_gb.csv',index=False)\n\nsubmission.iloc[:,1] = list(np.round(y_pred_xgboost_reg.ravel(),2))\nsubmission.to_csv('house_prediction_xg.csv',index=False)\n\nsubmission.iloc[:,1] = list(np.round(y_pred_stack_reg.ravel(),2))\nsubmission.to_csv('house_prediction_stack.csv',index=False)","e647cc29":"#### <u>Sweetviz library to visually everything in one go for EDA<\/u>","faf0db43":"<ul>\n    <li><h4><b>We can see there are quite a lot of outliers and we need to remove them.<\/b><\/h4><\/li>\n<\/ul>\n","60a5ae53":"#### Using Randomized Searched CV for Extra Tree Regressor","d8451260":"<ol>\n    <li><h3><b><u>Analysing Numerical variables<\/u><\/b><\/h3><\/li>\n<\/ol>\n\n<ul>\n    <li><h4><b>Numerical variables are usually of 2 type i.e<\/b><\/h4><\/li>\n    <ol>\n        <li><h4><b>Continous variable<\/b><\/h4><\/li>\n        <li><h4><b>Discrete variable<\/b><\/h4><\/li>\n    <\/ol>\n<\/ul>\n\n### a).Discrete Variables","6349f8e5":"<ul>\n    <li><h4><b>Checking all missing values in train and test dataset <\/b><\/h4><\/li>\n<\/ul>","2948333f":"<h3><u><b> 7. Feature Selection <\/b><\/u><\/h3>","95018208":"* ##### We can see the R2 square difference b\/w training and test is 10 , there could be overfitting again , we will be doing Hyperparameter Tuning to find out this.","86633b8a":"### 1. Applying Lasso Regression model to Find Best Features\n\n* First, I specify the Lasso Regression model, and I select a suitable alpha (equivalent of penalty).\n* The bigger the alpha the less features that will be selected.\n\n*  Then I use the selectFromModel object from sklearn, which will select the features which coefficients are non-zero","a386c8d6":"### 3. Random Forest (Base Model)","1d5169ea":"### 2. Dropping Highly Correlated independent features","6b162b88":"### 6. XG Boost Regressor (Base Model)","55722a02":"### 1) Linear Regression","21482657":"\n* Finding the Best Features","738bf470":"#### <u><b>Define Meta Learner Model<\/b><\/u>","fc1ea060":"<ul>\n    <li><h4><b>Checking and Fixing the Datatypes Mismatch if any <\/b><\/h4><\/li>\n<\/ul>","83572fef":"<i>We can see the R2 square difference b\/w training and test between 10-12 , there could be overfitting again , we will be doing Hyperparameter Tuning to find out this.<\/i>","ddb8dfae":"### <u><i>Decision Tree with Hyperpameter Tuning<\/i><\/u>","92765550":"# 4. Decision Tree Regression (base Model)","b25b12f9":"### 3. KNN Regressor","9e387186":"* <h4> We can see many of the features are getting classified into 2 parts which tells these features may have many outliers <\/h4>","f2d7e323":"### 4. Extra Tree Regressor (Base Model)","991087b7":"### <u><i>Gradient Boost after HyperParameter Tuning<\/i><\/u>","7b9f7d81":"<ul>\n    <li><h3><b>Analysing Outliers<\/b><\/h3><\/li>\n<\/ul>","1891302b":"<ul>\n    <li><h4><b>Analysing Year Variables<\/b><\/h4><\/li>\n<\/ul>","785a9ea4":"### <u><i>Random Forest After Hyperparameter Tuning<\/i><\/u>","67d11292":"#### <u> Spliting data back to train-test as encoding is done <\/u>","42b01703":"1. We can observe Skewness and Kutosis values are pretty good.\n2. Durbin-Watson values is also close to 2 which should be as close to 2 for no autocorrelation\n3. Std Error is also close to 0 for most of them.","684c4c16":"<h4><b>4. Fixing Outliers Issues <\/b><\/h4>","d7f26a60":"#### As tree based algorithm don't require normalization so creating 2 set for , 1st for tree algo and 2nd for rest algo","8fc717b7":"<h3><u> Import Required Packages <\/u><\/h3>","7881f2c6":"### 6. AdaBoost Regressor (Base Model)","28db25c9":"##### We can see that we have reduced the R2 score difference b\/w training and test error to high extend and there is no overfitting now.","ee76f9ad":"<h4><b> 9. Applying One Hot Encoding to required columns<\/b><\/h4>","65bdad95":"#### Using Randomized Searched CV for GB","f0b8657f":"#### Using Randomized Searched CV for XG Boost","7d9d3816":"1. **We can consider from the model_select values that *Gradient Boost and XG Boost are giving the best* have performed best as it has the best *R2 score* and lowest *RMSE* value among all algorithms**\n\n2. **Using KNN Imputer over median or mean improved my Model Score**\n\n3. **But *XG Boost* Results gave me the best RMSE of 17828.33 and R2 Score of 93% on testset in Kaggle Competition**","b0fab84c":"### 3. Ridge Regression","e6c270a8":"### b)Continuous Variable","50a876b6":"<h3><u> Reading Train and Test Data <\/u><\/h3>","daed543f":"<h3><b><u> 4. Feature Addition <\/u><\/b><\/h3>","c996be1b":"* Creating a dataframe to save model result to compare at end.","42c4831b":"#### <u>As there is probleam of different count of features in Train and Test set it we will get different columns count after Encoding so combining the train test data to encode and then again split into train - test data.<\/u>","deb6f7cd":"<h4><b>6. Dropping the non required features<\/b><\/h4>","75d9e06e":"#### Using Randomized Searched CV for RF","737e80f5":"* #### <i> We can notice there is high difference in R2 Score for training and testing for DT , that means the base model in overfitting and we need to do hyperparameter tuning.<\/i>","dfdbd0ad":"<h3><u><b>5. Converting Data to Numeric For ML Algorithm<\/b><\/u><\/h3>","ab493eb0":"### Converting the House Price values back to original form using exponential","6a1ce4de":"* <h4><b> After analysing all columns types we can understand that<\/b><\/h4>","c759b55f":"<h4><b>5. Log Normal Distribution for Continuous Features to remove skewness<\/b><\/h4>","e61a1304":"### <i><u>Extra Tree Regressor After Hyperparameter Tuning<\/u><\/i>","eee6ebdf":"#### The distribution is skewed and we can normalize it using log normal distribution","7dcd32fb":"2. <h4><b>Handling Missing Values for Numeric Features<\/b><\/h4>","8c786323":"<u><h3><b>2. Analysing Categorical Variables<\/b><\/h3><\/u>","c9f76fac":"### 5. Gradient Boosting Regressor (Base Model)","997e3c67":"* Creating some extra features that seems can be extracted from data","b0ed56e3":"<h4><b> 8. Applying Label Encoding to required columns<\/b><\/h4>","d0b68e04":"<u><h3><b>3. Feature engineering<\/b><\/h3><\/u>","c00bc945":"1. <h4><b>Handling Missing Values for Categorical Features<\/b><\/h4>","65817547":"#### Analyzing Train and Test dataframe in one go(and its optional target feature)","b7b57b37":"### Accuracy vs alpha for training and testing sets\u00b6\n<h4>When ccp_alpha is set to zero and keeping the other default parameters of :class:DecisionTreeClassifier, the tree overfits, leading to a 100% training accuracy and 76% testing accuracy. As alpha increases, more of the tree is pruned, thus creating a decision tree that generalizes better. In this example, setting ccp_alpha=0.015 maximizes the testing accuracy.<\/h4>","0c4e178d":"<h3><b><u>6. Feature Scaling <\/u><\/b><\/h3>","b724d10c":"3. <h4><b>Replace the missing numeric values with KNN Imputer<\/b><\/h4>","d84f64b8":"* #### <i> We can see that we have reduced the R2 score difference b\/w training and test error to high extend and there is no overfitting now<\/i>","18f56f43":"#### Using Randomized Searched CV for DT","8203e5e3":"Cost complexity pruning provides another option to control the size of a tree. In :class:DecisionTreeClassifier, this pruning technique is parameterized by the cost complexity parameter, ccp_alpha. Greater values of ccp_alpha increase the number of nodes pruned","ad794565":"### 2. Lasso Regression","61c99c93":"<ul>\n    <li><h3><b>Analysing Numerical Variables<\/b><\/h3><\/li>\n<\/ul>","93b6f19e":"### <u>Performing Stacking to improve performance<\/u>\n\n#### <u><b>Define Base Models<\/b><\/u>","2586bb24":"<h3><b><u> 9.Building ML Algo on our final dataset <\/u><\/b><\/h3>","63e609d7":"### <u>Visualize the Results to getter understanding<\/u>"}}