{"cell_type":{"f2a46630":"code","91b97871":"code","c4174913":"code","25673cec":"code","03fe4e52":"code","d050ba8e":"code","51856740":"code","a099950d":"code","7f717771":"code","02665992":"code","658d58a7":"code","003bfef4":"code","3ef16938":"code","de51cd29":"code","04a2476a":"code","9bd69496":"code","da726fab":"code","6cdebbb6":"code","d0a1ca0e":"code","e589cd80":"code","3547b3e8":"code","e08fce71":"code","72a28c27":"code","1b4c4568":"code","0c6ef612":"code","b59424fa":"code","77c0c169":"code","965e9a41":"code","7dfcf9e4":"code","c2c65e8d":"code","35c84fbf":"code","dd915187":"code","6ce593f4":"code","3c5996e1":"markdown","4bbe4ed7":"markdown","806e4b93":"markdown","24d4641f":"markdown","dc9bbaf3":"markdown","95b03a5a":"markdown","2d3f0330":"markdown","6f25a0cb":"markdown","e2d11ab7":"markdown","744f5715":"markdown","12e6a3d4":"markdown","40ea4d6e":"markdown","0c6c0273":"markdown","1c840d7b":"markdown","7ff7f06c":"markdown","df62a6e9":"markdown","bb0bc5ec":"markdown","0634a835":"markdown","a52de186":"markdown","7885e037":"markdown","87668b96":"markdown","485a0ce1":"markdown","a208856b":"markdown","a44bd9d5":"markdown","3b00c40e":"markdown","a83c2b08":"markdown","cf0e2823":"markdown","c0bc4b5d":"markdown","8e833e7f":"markdown","ca801e4b":"markdown","243aa7d9":"markdown","c40370d3":"markdown","f2f2a8b9":"markdown","c53665bf":"markdown","98e263c3":"markdown","2d1f3be8":"markdown","08a6ae85":"markdown","6e7d4a7e":"markdown","93dc6567":"markdown","571fc07b":"markdown","e07a4db5":"markdown","1987703d":"markdown","71b19d5f":"markdown","f6121aa1":"markdown","67aa252f":"markdown"},"source":{"f2a46630":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","91b97871":"import matplotlib.pyplot as plt     ##For DataVisualization \nimport seaborn as sns               ##For DataVisualization \nfrom sklearn.model_selection import train_test_split                \nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,Flatten,Dense,Dropout,MaxPooling2D,BatchNormalization,Activation\nfrom keras.layers import MaxPool2D\nfrom keras.layers import AveragePooling2D\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator                 #Data Augmentation\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ReduceLROnPlateau,EarlyStopping              ##For model Optimization\nfrom sklearn.metrics import multilabel_confusion_matrix,f1_score","c4174913":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')    ##Loading The Train Data\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')      ##Loading the Test Data","25673cec":"train.head()                          ##Top 5 rows of Train Dataset","03fe4e52":"train.shape                           ## Shape oF Train Dataset","d050ba8e":"train.info()                         ","51856740":"train.isnull().sum()[train.isnull().sum()>0]","a099950d":"test.isnull().sum()[test.isnull().sum()>0]","7f717771":"Y = train.label\ntrain.drop('label',inplace = True,axis =1)","02665992":"train.head()","658d58a7":"sns.countplot(Y)","003bfef4":"## Normalizing the pixels between 0 -1\ntrain = train\/255.0                   \ntest = test\/255.0","3ef16938":"Y = pd.get_dummies(Y).values.astype('float')","de51cd29":"Y","04a2476a":"train = train.values.reshape(-1,28,28,1)","9bd69496":"test = test.values.reshape(-1,28,28,1)","da726fab":"\nxtrain,xval,ytrain,yval = train_test_split(train,Y,test_size= 0.10,random_state = 234)","6cdebbb6":"plt.imshow(xtrain[34])","d0a1ca0e":"model = Sequential()\n\nmodel.add(Conv2D(filters = 6 ,kernel_size = (5,5),activation = 'relu',input_shape = (28,28,1)))  ## 5*5 Convolution\nmodel.add(AveragePooling2D(pool_size = (2,2)))\n\nmodel.add(Conv2D(filters = 16 ,kernel_size = (5,5),activation = 'relu'))              ## 5*5 Convolution\nmodel.add(AveragePooling2D(pool_size = (2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(4096,activation ='relu'))\nmodel.add(Dense(4096,activation ='relu'))\nmodel.add(Dense(10,activation ='softmax'))\n\n\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","e589cd80":"model.summary()","3547b3e8":"learning_rate_decay = ReduceLROnPlateau(min_lr = 0.0001,verbose =1,patience  =2,factor = 0.5,monitor ='val_accuracy')\nstopping = EarlyStopping(monitor ='val_accuracy',patience =2,verbose =2)\n","e08fce71":"callbacks = [learning_rate_decay,stopping]","72a28c27":"datagen = ImageDataGenerator(rotation_range = 0.2,zoom_range =0.2,shear_range =0.3)","1b4c4568":"datagen.fit(xtrain)","0c6ef612":"history = model.fit_generator(datagen.flow(xtrain,ytrain,batch_size=86),validation_data = (xval,yval),verbose  =2,callbacks=[learning_rate_decay],steps_per_epoch = xtrain.shape[0]\/\/86,epochs = 30)","b59424fa":"fig,ax = plt.subplots(ncols =2,nrows =1,figsize = (15,6))\nplt.figure()\nax[0].plot(history.history['loss'],color = 'r',label = 'Training Loss')\nax[0].plot(history.history['val_loss'],'b',label = 'Validation Loss')\nlegend = ax[0].legend(loc= 'best',shadow =True)\n\nax[1].plot(history.history['accuracy'],color = 'b',label = 'Training Accuracy')\nax[1].plot(history.history['val_accuracy'],'r',label = 'Validation Accuracy')\nlegend = ax[1].legend(loc= 'best',shadow =True)\n","77c0c169":"z = model.predict(xval)","965e9a41":"z = z.round()\nz","7dfcf9e4":"np.where(z!=yval)","c2c65e8d":"fig,ax = plt.subplots(2,3,figsize= (10,8))\nax = np.array(ax).reshape(-1,1)\nn=0\nfor i in np.where(z!=yval)[0][0:12:2]:\n    ax[n][0].imshow(xval[i])\n    ax[n][0].set_title('predicted_label = {} \\n True label = {}'.format(np.where(z==1)[1][i],np.where(yval==1)[1][i]))\n    n+=1","35c84fbf":"pred_test = model.predict(test)\npred_test","dd915187":"label = np.argmax(pred_test,axis=1)","6ce593f4":"submission = pd.DataFrame({'ImageId':range(1,28001),'Label':label})\n\nsubmission.to_csv('mnist submission.csv',index = False)","3c5996e1":"## Model Evaluation","4bbe4ed7":"## Checking For Null Values","806e4b93":"The performance of deep learning neural networks often improves with the amount of data available. Data augmentation is a technique to artificially create new training data from existing training data. ... This means, variations of the training set images that are likely to be seen by the model.\n\nSo here to improve our network performance,  we are applying Data Augmentation where are playing with rotation,shear and zoom range of the image to create more such data which has same output but have different features so that the model learns more quickly and do not overfits.\n\n## Note:\n         We are not doing changes in horizontal flip or vertical Flips of the images because it can cause error in the data as 6 can be labelled as 9 or vice versa.","24d4641f":"* Saving our Predictions into a CSV file after converting them into a DataFrame.  ","dc9bbaf3":"## Making Predictions ","95b03a5a":" We can Clearly Observe that the Data is not imbalance enough that we need to balance it. All the Class Counts are approximately equal to each other.","2d3f0330":"Plotting Those digits which gives error and also showing what is their predicted and true Label","6f25a0cb":"Here FOR Making our network more Robust and efficient we are setting some callbacks which will help in faster convergence of loss function and thus we will need less time for training our network.\nHere we are setting two Callbacks :\n* Early Stopping:\n                 Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset. In this tutorial, you will discover the Keras API for adding early stopping to overfit deep learning neural network models.\n                 \n* ReduceLROnPlateau:\n                    Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.","e2d11ab7":"## Setting Callbacks","744f5715":"If you like My Work please hit the Upvote button. This will motivate me to do more such work .","12e6a3d4":"Reshaping the Dataset into arrays of (28,28,1) dimensions where (28,28) are image dimensions while the 1 denotes the number of channels which is 1 in this case as the image is grayscale so we are using 1 other for colourful images we will use 3 i.e (28,28,3)","40ea4d6e":"## Checking For Errors in Predictions","0c6c0273":"## Plotting an Image From Train Set","1c840d7b":"## Splitting into Label and Features","7ff7f06c":"Here in our network we are setting callbacks such that if there is not changes in the validation accuracy for two epochs in a row then the network will reduce the learning rate and its minimum value =0.0001 and if it reaches to the minimum thenn it will stop if accuracy doesn't improve for two consecutive epochs.","df62a6e9":"### For Train","bb0bc5ec":"ALSO IF YOU FIND ANY FLAW THROUGHOUT THE NOTEBOOK , PLEASE COMMENT SO THAT I CAN RECTIFY IT AND GET BETTER.","0634a835":"                --> We are given two files (Train.csv and Test.csv) a Train.csv contains the data which we will use to train our model with while test.csv will be used to submit the predictions as submission.csv file . There are 42000 digit records in train dataset while test dataset contains 28000 records.The dataset is in the form of features denoting pixel values for all digits.\n                --> There are a total of 784 pixel values for each record .","a52de186":"## Rescale the Pixels","7885e037":"Now its time to make predictions using our model on the given test set ......\n","87668b96":"## Loading The Dataset ","485a0ce1":"Our Target Column is Label column while our olumns are features","a208856b":"### For Train Data","a44bd9d5":"# MNIST Digit Recognizer\n## Notebook By Priyank Dubey\n\n![Mnist Dataset](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRQA61llWYFBErMv6NuFvLC1MYHNX_KC7Dxcw&usqp=CAU)\n\n* Introduction:\n                In this Notebook, I have Performed Digit Recognition on Mnist Dataset using Convolutional Neural Network . Here i have fitted the CNN Network Using The Most Basic CNN archieectecture i.e LENET. \n                \n                \n* Data Loading:\n               \n* Data Preprocessing: \n    1. Splitting Into Features And Labels\n    2. Checking if Dataset is Imbalanced\n    3. Rescale the pixels\n    4. Converting the Labels into Dummy Variables\n    5. Converting The dataset into arrays for Processing \n    6. Splitting Dataset Into Validation and Training Set\n                \n* Building ConvNet:\n    1. LENET for CNN \n    2. Setting Callbacks \n* Data Augmentation \n* Training CNN Model\n* Model Evaluation\n* Submitting Predictions\n                \n                \n      \n","3b00c40e":"We will Convert the Labels into Dummy Variable where for each label it shows 0 if the label is not true for that record (0 for all while 1 for the True Label)","a83c2b08":"Here after Plotting the Training accuracy and Validation Accuracy we can see that after 1 epoch the accuracy has a sudden spike which is due to random initialization of weights . \n\n\nIn Contrary , we can also see that both Training Loss and Validation Loss decreased as number of epochs r","cf0e2823":"* np.argmax(array)-----------------> This method return the index of the maximum element in the array either row wise or column wise ,axis =1 means row-wise ","c0bc4b5d":":) Thanks For Coming!! ","8e833e7f":"### For Test","ca801e4b":"* The network has got trained now after running 6 epochs , i have set max epochs -30 while it run only 6 epochs because there was not much change in the accuracy for two consecutive epochs and thus model stopped training. \n\n* This has made the model computationally cheaper and also saves time","243aa7d9":"We are rescaling the pixel values into a range between 0 to 1 :\n For the two Following Reasons:\n*             * CNN performs betters for greyscale Image i.e Image which have pixels between 0--1\n*             * Converting into Greyscale image so that the intensity of Illumination does not affect the model learning (because 0 depicts black while 255 depicts White).","c40370d3":"## Reshaping the Dataset into 3D matrices","f2f2a8b9":"Splitting the dataset into Validation and Training so that we can train our model with train and can validate our data with Validation Dataset. Validation set is used at each epoch to calculate the loss and train the parameters. So having a good size of Validation set is too necessary.\n\nHere we are dividing the dataset into 90:10 Ratio as we have 42000 records in original dataset  so we will use 37800 for training and 4200 for validation purpose so that our validation also comes true enough","c53665bf":"Total Number of Parameters in Our Network are 17Million .","98e263c3":"## Building CovnNet","2d1f3be8":"In the above plots we can clearly see that the model is making error in those digits only which are contradictory like 1 and 7 ,9 and 4. ","08a6ae85":"## Encoding the Labels into Dummy Values","6e7d4a7e":"## Importing Necessary Libraries","93dc6567":"## Splitting The Dataset into Training and Validation","571fc07b":"## Checking IF Dataset is Imbalance.","e07a4db5":"## LENET","1987703d":"LeNet is a convolutional neural network structure proposed by Yann LeCun et al. in 1989. ... Convolutional neural networks are a kind of feed-forward neural network whose artificial neurons can respond to a part of the surrounding cells in the coverage range and perform well in large-scale image processing.\n## LeNet Architecture\n![LeNet Architecture](https:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2016\/06\/lenet_architecture-768x226.png)","71b19d5f":"## Data Augmentation","f6121aa1":"### For Test Data","67aa252f":"## Saving Predictions"}}