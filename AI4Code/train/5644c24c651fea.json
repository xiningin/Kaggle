{"cell_type":{"7cdabfa9":"code","0fcc638e":"code","1b268e64":"code","5ff207df":"code","f1dc03c6":"code","7a302fed":"code","3b51e07e":"code","cc2e11f5":"code","ca1b20ab":"code","a3efd5eb":"code","f5dba9cb":"code","9a7f2ed1":"code","d01434c6":"code","181a253f":"code","5686da99":"code","08151b70":"markdown","0682b13b":"markdown","5dd84a6f":"markdown","29de2a18":"markdown"},"source":{"7cdabfa9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0fcc638e":"df = pd.read_json('\/kaggle\/input\/whats-cooking-kernels-only\/train.json')\ndf.head()","1b268e64":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 6))\ndf.cuisine.value_counts().plot.bar(title='Classes Counts')\nplt.show()","5ff207df":"from sklearn.preprocessing import LabelEncoder\n\nn_classes = len(df['cuisine'].unique())\nprint(\"Number of classes\", n_classes)\n\n# get the length of the tokens\ndf['length'] = df.ingredients.map(lambda x: len(x))\n\n# get the number of classes\nle = LabelEncoder()\ndf['categorical_label'] = le.fit_transform(df.cuisine)\ndf.head()","f1dc03c6":"from sklearn.model_selection import train_test_split\n\n# split dataset\ntrain_set, valid_set = train_test_split(df, test_size=0.15, stratify=df.cuisine, random_state=42)\n\nprint(train_set.shape)\nprint(valid_set.shape)\n\ntrain_sentences = [','.join(sentence) for sentence in train_set.ingredients.values.tolist()]\nvalid_sentences = [','.join(sentence) for sentence in valid_set.ingredients.values.tolist()]\n\n# get the labels\ny_train = train_set.categorical_label\ny_valid = valid_set.categorical_label\n\ntrain_sentences[:3]","7a302fed":"import tensorflow as tf\n\n# get sequence max length\nsequence_length = int(df['length'].max())\n\n# create vectorization layer\nvectorization_layer = tf.keras.layers.TextVectorization(max_tokens=None, output_mode='int', output_sequence_length=sequence_length, \n                                                        split=lambda x: tf.strings.split(x, ','), standardize=lambda x: tf.strings.lower(x))\nvectorization_layer.adapt(train_sentences)\n\n# create vectorization layer\nvectorizer = tf.keras.models.Sequential()\nvectorizer.add(tf.keras.Input(shape=(1,), dtype=tf.string))\nvectorizer.add(vectorization_layer)\n\n# get sequences\ntrain_sequences = vectorizer.predict(train_sentences)\nvalid_sequences = vectorizer.predict(valid_sentences)\n\nprint(train_sentences[:3])\nprint(train_sequences[:3])","3b51e07e":"print(len(vectorization_layer.get_vocabulary()))\nprint(vectorization_layer.get_vocabulary()[:10])","cc2e11f5":"embedding_dim = 50\nvocab_size = vectorization_layer.vocabulary_size()\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=sequence_length, mask_zero=True),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalMaxPool1D(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(n_classes, activation='softmax')\n])\n    \nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","ca1b20ab":"early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5)\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"cooking_deep.h5\", save_best_only=True)\n\nhistory = model.fit(train_sequences, y_train, epochs=30, validation_data=(valid_sequences, y_valid),\n                    callbacks=[early_stopping_cb, checkpoint_cb])","a3efd5eb":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(loss))  # Get number of epochs\n\n# Plot training and validation loss per epoch\nplt.figure(figsize=(8, 6))\nplt.plot(epochs, loss, 'r', label=\"Training Loss\")\nplt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\nplt.legend()\nplt.show()","f5dba9cb":"model = tf.keras.models.load_model(\"cooking_deep.h5\")\nprint(model.evaluate(valid_sequences, y_valid))","9a7f2ed1":"import zipfile\nzip_ref = zipfile.ZipFile(\"\/kaggle\/input\/whats-cooking-kernels-only\/sample_submission.csv.zip\", 'r')\nzip_ref.extractall('\/kaggle\/temp')\nzip_ref.close()\n\npd.read_csv('\/kaggle\/temp\/sample_submission.csv').head()","d01434c6":"test_set = pd.read_json('\/kaggle\/input\/whats-cooking-kernels-only\/test.json')\ntest_sentences = [','.join(sentence) for sentence in test_set.ingredients.values.tolist()]\ntest_sequences = vectorizer.predict(test_sentences)\npredictions = model.predict(test_sequences)\npredictions","181a253f":"test_set[\"cuisine\"] = le.inverse_transform(np.argmax(predictions, axis=1))\ntest_set[['id', 'cuisine']].to_csv('submission.csv', index=False)","5686da99":"pd.read_csv('submission.csv').head()","08151b70":"# Make Predictions","0682b13b":"# Create Classification Model","5dd84a6f":"# Text Vectorization","29de2a18":"# Explore Dataset"}}