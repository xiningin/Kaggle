{"cell_type":{"fdf9a2f6":"code","e88e21b5":"code","913b88b5":"code","6f408696":"code","3f665187":"code","222c4ca8":"code","d9298d6a":"code","9b91dd59":"code","89c83e51":"code","a069d45b":"code","801468bd":"code","13d636a5":"code","99356395":"code","b15e40e4":"code","840b8ab7":"markdown","07305b68":"markdown","fcc4f779":"markdown","c6c7ff7d":"markdown","3a4caf61":"markdown","6993e12e":"markdown","878d6ed1":"markdown","605a4b16":"markdown"},"source":{"fdf9a2f6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.options.mode.chained_assignment = None  # default='warn'\n\nimport warnings\nwarnings.simplefilter(\"ignore\", UserWarning)\n\nimport os","e88e21b5":"X_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\nX_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')","913b88b5":"y = X_train['Survived']\nX_train.drop(axis=1, columns=['Survived'], inplace=True)","6f408696":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nX_train.hist(bins=30, figsize=(25,25))\nplt.show()","3f665187":"cat_cols = [cname for cname in X_train.columns if X_train[cname].dtype == 'object']\n\nf = plt.figure(figsize=(25, 25))\n\nfor i in range(len(cat_cols)):\n    f.add_subplot(10, 5, i+1)\n    sns.countplot(x=X_train[cat_cols].iloc[:,i], data=X_train)\n\nplt.tight_layout()\nplt.show()","222c4ca8":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\n\nclass Imputer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        return\n    \n    def fit(self, X, y=None):\n \n        features = {'freq_cols': ['Pclass','Sex','Embarked'],\n                    'none_cols': ['Name','Ticket','Cabin'],\n                    'mean_cols': ['Age','SibSp','Parch','Fare']}\n        \n        self.freq_imputer = SimpleImputer(strategy=\"most_frequent\")\n        self.none_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"none\")\n        self.mean_imputer = SimpleImputer(strategy=\"mean\")\n        \n        for cname in features['freq_cols']:\n            self.freq_imputer.fit(X[[cname]])\n            \n        for cname in features['none_cols']:\n            self.none_imputer.fit(X[[cname]])\n            \n        for cname in features['mean_cols']:\n            self.mean_imputer.fit(X[[cname]])\n            \n        self.features = features\n        return self\n    \n    def transform(self, X):\n        \n        for cname in self.features['freq_cols']:\n            X[cname] = self.freq_imputer.transform(X[[cname]])\n            \n        for cname in self.features['none_cols']:\n            X[cname] = self.none_imputer.transform(X[[cname]])\n            \n        for cname in self.features['mean_cols']:\n            X[cname] = self.mean_imputer.transform(X[[cname]])\n        return X","d9298d6a":"from sklearn.preprocessing import KBinsDiscretizer\n\nclass Features(BaseEstimator, TransformerMixin):\n    def __init__(self, age_bins=6, age_strategy='uniform', fare_bins=4, fare_strategy='quantile', use_shared_names=True, use_shared_tickets=True):\n        self.age_bins = age_bins\n        self.age_strategy = age_strategy\n        self.fare_bins = fare_bins\n        self.fare_strategy = fare_strategy\n        self.use_shared_names = use_shared_names\n        self.use_shared_tickets = use_shared_tickets\n    \n    def fit(self, X, y=None):\n        \n        # Find the list of individual tickets\n        self.tickets = X['Ticket']\n\n        # Find the list of individual names\n        X['Last_Name'] = X['Name'].apply(lambda x: str.split(x, \",\")[0])\n        self.names = X['Last_Name']\n        \n        # Split age and fare into bins\n        self.age_splitter = KBinsDiscretizer(n_bins=self.age_bins, encode='ordinal', strategy=self.age_strategy)\n        self.fare_splitter = KBinsDiscretizer(n_bins=self.fare_bins, encode='ordinal', strategy=self.fare_strategy)\n        self.age_splitter.fit(X[['Age']])\n        self.fare_splitter.fit(X[['Fare']])\n        return self\n    \n    def transform(self, X):\n        \n        X['Last_Name'] = X['Name'].apply(lambda x: str.split(x, \",\")[0])\n        \n        # If the data frame is not the same as the fit data (X_test and X_train), then add X_train to the list of ticket numbers and last names\n        if X['Ticket'].shape[0] != len(self.tickets):\n            self.tickets = self.tickets.append(X['Ticket'])\n            self.names = self.names.append(X['Last_Name'])\n        \n        # Count how many times a ticket or last name appears\n        self.unique_tickets = self.tickets.value_counts()\n        self.unique_names = self.names.value_counts()\n        \n        # Find the number of shared tickets and second names and assign to new features\n        for index, row in X.iterrows():\n            if self.use_shared_tickets:\n                X.loc[index, 'Shared_Tickets'] = self.unique_tickets.get(row['Ticket'])\n            if self.use_shared_names:\n                X.loc[index, 'Shared_Names'] = self.unique_names.get(row['Last_Name'])\n        \n        # Split age and fare into bins\n        X['Age'] = self.age_splitter.transform(X[['Age']])\n        X['Fare'] = self.fare_splitter.transform(X[['Fare']])\n        \n        # Replace uncommon titles with 'uncommon_title'\n        X['Title'] = X['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n        X['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'}, inplace = True)\n\n        all_titals = X['Title'].unique()\n        common_titles = []\n        \n        for title in all_titals:\n            if X['Title'].value_counts()[title] > 30:\n                common_titles.append(title)\n            \n        uncommon_titles = list(set(all_titals) - set(common_titles))\n        X['Title'].replace(uncommon_titles, 'uncommon_title', inplace=True)\n        \n        # Family count\n        X['Family_Count'] = X['SibSp'] + X['Parch']\n        return X","9b91dd59":"from sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\n\nclass Encoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        return\n    \n    def fit(self, X, y=None):\n        \n        self.cat_cols = ['Pclass', 'Sex', 'Age', 'Fare', 'Cabin', 'Embarked', 'Shared_Tickets','Shared_Names','Title','Family_Count']\n        \n        self.OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        self.count_encoder = ce.CountEncoder()\n        \n        self.OH_encoder.fit(X[['Sex', 'Embarked', 'Title']])\n        self.count_encoder.fit(X[self.cat_cols])\n        return self\n    \n    def transform(self, X):\n        \n        OH_X = pd.DataFrame(self.OH_encoder.transform(X[['Sex', 'Embarked', 'Title']]))                \n        OH_X.index = X.index\n        X = pd.concat([X, OH_X], axis=1)\n        \n#         count_encoded = self.count_encoder.transform(X[self.cat_cols])\n#         X = X.join(count_encoded.add_suffix(\"_count\"))\n        return X","89c83e51":"class Log_Skewed(BaseEstimator, TransformerMixin):\n    def __init__(self, skew_threshold=0.9):\n        self.skew_threshold = skew_threshold\n        \n    def fit(self, X, y=None):\n        skewed_cols = X.select_dtypes(include=[np.number]).skew()\n        self.top_skewed_cols = skewed_cols[abs(skewed_cols > self.skew_threshold)].index\n        return self\n    \n    def transform(self, X):\n        for cname in list(self.top_skewed_cols):\n            X[cname] = np.log1p(X[cname])\n            # try box-cox\n        return X","a069d45b":"class Scaler(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        return\n    \n    def fit(self, X, y=None):\n        self.scaler = StandardScaler()\n        self.scaler.fit(X)\n        return self\n    \n    def transform(self, X):\n        X = self.scaler.transform(X)\n        return X","801468bd":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler \n\ndef drop_columns(X):\n    X.drop(axis=1, columns=['SibSp', 'Parch', 'Name', 'Ticket', 'Cabin', 'Last_Name', 'Sex', 'Embarked', 'Title'], inplace=True)\n    return X\n\npreprocessing_pipe = Pipeline([\n    ('impute', Imputer()),\n    ('feature', Features()),\n    ('encode', Encoder()),\n    ('drop', FunctionTransformer(drop_columns)),\n    ('log_skewed', Log_Skewed()),\n    ('scale', Scaler())\n])","13d636a5":"# Find best baseline models\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import cross_val_score\n\nrandom_state=8\n\n# Baseline cross validation\nmodels = [\n    XGBClassifier(random_state=random_state),\n    SVC(random_state=random_state),\n    DecisionTreeClassifier(random_state=random_state),\n    AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state)),\n    RandomForestClassifier(random_state=random_state),\n    ExtraTreesClassifier(random_state=random_state),\n    GradientBoostingClassifier(random_state=random_state),\n    MLPClassifier(random_state=random_state),\n    KNeighborsClassifier(),\n    LogisticRegression(random_state=random_state),\n    LinearDiscriminantAnalysis()\n]\n\ndef evaluate_model(X, y, models, preprocessing_pipe):\n    performance = {}\n    for model in models:\n        X_transformed  = preprocessing_pipe.fit_transform(X, y)\n        print(\"Model: {}, mean cross validated score: {}\".format(type(model).__name__, cross_val_score(model, X_transformed, y, cv=5).mean()))\n        performance[type(model).__name__] = cross_val_score(model, X_transformed, y, cv=10).mean()\n    return performance\n        \nmodel_scores = evaluate_model(X_train, y, models, preprocessing_pipe)\ntop_models = sorted(model_scores, key=model_scores.get, reverse=True)[:5]\n\nprint(\"The best models are: {}\".format(top_models))","99356395":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# ['LinearDiscriminantAnalysis', 'SVC', 'LogisticRegression', 'MLPClassifier', 'GradientBoostingClassifier']\n\nLDA_params = {'model__n_components': [1]}\n\nSVC_params = {'model__kernel': ['rbf'], \n              'model__gamma': np.arange(0.001, 1, 0.01),\n              'model__C': np.arange(5, 1100, 10)}\n\nLR_params = {'model__solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n             'model__C' : [100, 10, 1.0, 0.1, 0.01]}\n\nMLP_params = {'model__hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n              'model__activation': ['tanh', 'relu'],\n              'model__solver': ['sgd', 'adam'],\n              'model__alpha': [0.0001, 0.05],\n              'model__learning_rate': ['constant','adaptive']}\n\nGBC_params = {'model__loss' : [\"deviance\"],\n              'model__n_estimators' : np.arange(100,500,50),\n              'model__learning_rate': np.arange(0.01, 0.1, 0.01),\n              'model__max_depth': np.arange(2,10,1),\n              'model__min_samples_leaf': np.arange(100,200,10),\n              'model__max_features': np.arange(0.1, 0.4, 0.1)}\n\n\nmodel_list = [LinearDiscriminantAnalysis(), SVC(probability=True), LogisticRegression(), MLPClassifier(), GradientBoostingClassifier()]\nparam_list = [LDA_params, SVC_params, LR_params, MLP_params, GBC_params]\n\niterations = 500\nbest_estimators = {}\n\nfor i in range(len(model_list)):\n    \n    model_name = type(model_list[i]).__name__\n    \n    full_pipeline = Pipeline([\n        ('preprocessing', preprocessing_pipe),\n        ('model', model_list[i])\n    ])\n    \n    fitted_model = RandomizedSearchCV(full_pipeline, param_distributions = param_list[i], n_iter=iterations, cv=5, scoring=\"accuracy\", verbose=0)\n    fitted_model.fit(X_train, y)\n    print(\"Model: {}, best cross validated score: {}\".format(model_name, fitted_model.best_score_))\n\n    best_estimators[model_name] = fitted_model.best_estimator_\n    \n# Best voting between estimators\nvoting = VotingClassifier(\n    voting='hard',\n    estimators=[\n    ('LinearDiscriminantAnalysis', best_estimators['LinearDiscriminantAnalysis']),\n    ('SVC', best_estimators['SVC']),\n    ('LogisticRegression',best_estimators['LogisticRegression']),\n    ('MLPClassifier', best_estimators['MLPClassifier']),\n    ('GradientBoostingClassifier',best_estimators['GradientBoostingClassifier']),\n])\n\nvoting = voting.fit(X_train, y)\npreds = voting.predict(X_test)\n\n# Try removing LDA and XGB as they give the exact same score (maybe biasing the voting result)\n\n# Model: SVC, best cross validated score: 0.8069549934090766\n# Model: LinearDiscriminantAnalysis, best cross validated score: 0.8069487163392128\n# Model: XGBClassifier, best cross validated score: 0.819295712761283\n# Model: GradientBoostingClassifier, best cross validated score: 0.819295712761283\n# Model: LogisticRegression, best cross validated score: 0.8125415855878476","b15e40e4":"# Save test predictions to file\noutput = pd.DataFrame({'PassengerId': X_test.index,\n                       'Survived': preds})\noutput.to_csv('submission.csv', index=False)","840b8ab7":"# Imputation strategy\nUsing the visualisations above I've decided on the following imputation strategy. The reason i've done this for every column is that in reality we would not know what data could be missing in the test set. We should assume anything could be missing, and therefore define an imputation method for each column. You'll notice that some numeric data is categorical like Pclass.\n\nI've used 'most frequent' for features that are categorical and have an order to it (ordinal features).\n'none' for features that are categorical  but are purely catogorical and 'mean' for any features that are numeric.\n\nSee the feature name,number of missing values in training data and the imputation strategy below.\n*         Pclass        0 - most freq\n*         Name          0 - 'none'\n*         Sex           0 - most freq\n*         Age         177 - mean\n*         SibSp         0 - mean\n*         Parch         0 - mean\n*         Ticket        0 - 'none'\n*         Fare          0 - mean\n*         Cabin       687 - 'none'\n*         Embarked      2 - most freq","07305b68":"# Creating the imputer\nDon't get scared! I promise it's much simpler than it looks. This is a custom transformer used in a pipeline. Here's how it works.\n\n    class Imputer(BaseEstimator, TransformerMixin):\n\nThis defines the name of the transformer, and tells it to inherit from the 'BaseEstimator' and 'TransformerMixin' SKLearn classes. This gives you some useful properties of SKLearn transformers such as .fit_transform(X).\n\n**Initialisation method**\n\n    def __init__(self):\n        return\n        \nThis is the initialization function for the transformer. If you want to try different parameters in the transformer you can define them here. You'll see examples of this in other transformers i've made below. In this case we're not going to experiment with any parameters so this just returns when initialised.\n\n**Fit method**\n\n    def fit(self, X, y=None):\n    \n        # fit some stuff\n    \n    return self\n\nThe fit function is where you would fit all of your parameters. For example if you were going to impute a numeric columns with the mean of it's values, then you would call:\n    \n    def fit(self, X, y=None):\n    \n        self.mean_imputer = SimpleImputer(strategy=\"mean\").fit(X['numeric column'])\n    \n    return self\n    \nThis means that when you are only using the mean of the training data. This becomes more important when using cross validation, because every fold is going to re-calculate the mean, and using this method with a pipeline it means you are reducing data leakage as you're only taking the mean of the train portion since the test data isn't included.\n\nEverything you fit you should add as a parameter on 'self'. You can then access 'self' in the transform method.\n\n**Transformer method**\n\n    def transform(self, X):\n        \n        X['numeric_column] = self.mean_imputer.transform(X[['numeric_column]])\n\n    return X\n\nThe transformer function is where you actually transform the data. It takes X as an input and returns X after you've modified it. In this case we are applying the mean that we calculated in the .fit function to the numeric column. \n\nYou would do .fit_transform(X_train) on your training data, and then .transform(X_test) on your test data. This means that the mean calculated from your training data is applied to the test data.","fcc4f779":"# Feature Creation\nSimilarly to the imputer transformer here we have the same __init__, fit and transform methods. In this case I wanted to experiment with the following parameters to see how they changed the performance of the model:\n\n        def __init__(self, age_bins=6, age_strategy='uniform', fare_bins=4, fare_strategy='quantile', use_shared_names=True, use_shared_tickets=True):\n            self.age_bins = age_bins\n            self.age_strategy = age_strategy\n            self.fare_bins = fare_bins\n            self.fare_strategy = fare_strategy\n            self.use_shared_names = use_shared_names\n            self.use_shared_tickets = use_shared_tickets\n        \nAs i don't know if it's better to split the age feature into 3, 4, 5 or 6 groups, i'm letting the computer do the hard work, and using RandomSearchCV with a parameter called 'preprocessing__Features__age_bins = [3, 4, 5, 6]. This will randomly try different age bins and then select the best scoring value. This makes it really easy to optimise models when you aren't sure what's going to work best.\n\nI want to make sure I use the same number of age groups for the test data. To do this I can use the .fit method:\n                \n    self.age_splitter = KBinsDiscretizer(n_bins=self.age_bins, encode='ordinal', strategy=self.age_strategy)\n    \nHere i'm defining the splitter, so that it can be used to transform X_train and X_test. The fit method is only applied to the training data since .fit(X_train) is only called on X_train. \n\nUsing the bins defined above X_test can then be split into the same bins without any data leakage in the .transform() method as shown below:\n\n        # Split age and fare into bins\n        X['Age'] = self.age_splitter.transform(X[['Age']])\n        \nPlease let me know if you found this useful and would like me to continue writing it up.","c6c7ff7d":"Visualise the cateogorical data","3a4caf61":"# Easy optimisation with pipelines\nWhilst pipelines can make the code look complicated at first glance, it actually makes everything a lot simpler. \nThese are the benefits of pipelines:\n1. It's really easy to iterate over parameters to find the optimium combinations\n2. By fitting the pipeline on train data and transforming the test you reduce data leakage as test and train is segregated\n3. The code is really modular and easy to structure\n\nIn this notebook I also tried ensembling for the first time. I'm going to cover:\n* Simple exploratory data analysis\n* Building a pipeline estimator to impute values\n* Building a pipeline estimator to create new features\n* Building a pipeline estimator to encode, log and scale\n* Cross validating various models to find the best for the data set\n* Combining the predictions from the top 5 models based on a voting system\n* Submitting the final prediction","6993e12e":"# Data Analysis\nVisualise the numeric data. It's clear that there's some categorical data in here. We'll deal with that in a second.","878d6ed1":"# Fit the top models","605a4b16":"# Cross validate baseline models to find the best for ensembling"}}