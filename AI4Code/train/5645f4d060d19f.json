{"cell_type":{"61549f33":"code","caf76fea":"code","897a70cf":"code","5e23c1fb":"code","4270ee1f":"code","020b1ceb":"code","aae56be2":"code","b850a326":"code","c0eb0196":"code","05e26f77":"code","c889c912":"code","3ede2958":"code","e849f899":"code","41f2870f":"code","4d9ebd23":"code","3e469268":"code","b58bc088":"code","43b9001d":"markdown","81b1692f":"markdown","bcf1750a":"markdown","43e998a8":"markdown","97cfae7f":"markdown","9d0f3577":"markdown","d156a354":"markdown","1811229f":"markdown","95330f96":"markdown","69097d41":"markdown","eb584b3e":"markdown"},"source":{"61549f33":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","caf76fea":"!pip install transformers","897a70cf":"import numpy as np\nimport pandas as pd\nimport torch\nfrom tqdm import tqdm\n\nfrom transformers import BertTokenizer, BertModel\n\n\n","5e23c1fb":"MODEL_TYPE = 'bert-base-uncased'\nMAX_SIZE = 150\nBATCH_SIZE = 200","4270ee1f":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","020b1ceb":"tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)\nmodel = BertModel.from_pretrained(MODEL_TYPE)","aae56be2":"tokenized_input = train_df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))","b850a326":"print(tokenized_input[1])\nprint(\"Here 101 -> [CLS] and 102 -> [SEP]\")","c0eb0196":"padded_tokenized_input = np.array([i + [0]*(MAX_SIZE-len(i)) for i in tokenized_input.values])","05e26f77":"print(padded_tokenized_input[0])","c889c912":"attention_masks  = np.where(padded_tokenized_input != 0, 1, 0)","3ede2958":"print(attention_masks[0])","e849f899":"input_ids = torch.tensor(padded_tokenized_input)  \nattention_masks = torch.tensor(attention_masks)","41f2870f":"all_train_embedding = []\n\nwith torch.no_grad():\n  for i in tqdm(range(0,len(input_ids),200)):    \n    last_hidden_states = model(input_ids[i:min(i+200,len(train_df))], attention_mask = attention_masks[i:min(i+200,len(train_df))])[0][:,0,:].numpy()\n    all_train_embedding.append(last_hidden_states)\n","4d9ebd23":"unbatched_train = []\nfor batch in all_train_embedding:\n    for seq in batch:\n        unbatched_train.append(seq)\n\ntrain_labels = train_df['target']","3e469268":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test =  train_test_split(unbatched_train, train_labels, test_size=0.33, random_state=42, stratify=train_labels)\n","b58bc088":"len(X_train)","43b9001d":"## **Using BERT Transformer**","81b1692f":"##### Now we have the train embeddings.This can be used as an input to other machine learning models","bcf1750a":"Following are the basic steps involved in using any transformer,\n\n### **For preprocessing** \n1. Tokenize the input data and other input details such as Attention Mask for BERT to not ignore the attention on padded sequences.\n2. Convert tokens to input ID sequences.\n3. Pad the IDs to a fixed length.\n\n### **For modelling**\n1. Load the model and feed in the input ID sequence (Do it batch wise suitably based on the memory available).\n2. Get the output of the last hidden layer\n    * Last hidden layer has the sequence representation embedding at 0th index, hence we address the output as last_hidden_layer[0].\n3. These embeddings can be used as the inputs for different machine learning or deep learning models.","43e998a8":"## **Convert Text to Tokens**","97cfae7f":"Now let's pad the sequence to fixed length","9d0f3577":"Let's tell BERT to ignore attention on padded inputs.","d156a354":"**References:  **\n\nhttp:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n\nhttps:\/\/huggingface.co\/transformers\/","1811229f":"**Here 101 -> [CLS] and 102 -> [SEP]**\n\n[CLS] token refers to the classification token. We need to take the embedding of the token from the output layer. It represents entire sequence embedding.\n\n[SEP] refers to end of the sequence.","95330f96":"## **Load the required tokenizer and model**","69097d41":"# **Hello world!**\n\nIn this tutorial, I will got through usage of SOTA transformers opensourced by HuggingFace team.\nWe will be using BERT transformer model for this tutorial.\nYou can check this link to understand more about HuggingFace transformers https:\/\/huggingface.co\/transformers\/pretrained_models.html","eb584b3e":"## Get the sequence embedding"}}