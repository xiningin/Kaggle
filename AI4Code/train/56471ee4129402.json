{"cell_type":{"ead99e30":"code","1fae23dc":"code","b6cb862f":"code","bb1ec039":"code","9ec8811e":"code","eefe3974":"code","d74e3125":"code","97fd5664":"code","71d071b7":"code","930d3681":"code","3848eac9":"code","eba161b6":"code","d48aba10":"code","61f76e5e":"code","cb88007a":"code","46968659":"code","523dc2e6":"code","a9d0d1af":"code","88d38f4d":"code","2a1a1592":"code","d02abf54":"code","688bc875":"code","9289e7e8":"code","81c5fefd":"code","61f5c09b":"code","584d1c6e":"code","f5b0c5a1":"code","ac0b4bcf":"markdown","257c4555":"markdown","2679965e":"markdown","1958d8bd":"markdown","8e1d0a2b":"markdown","f2823ab2":"markdown","8fa1812d":"markdown","fe651ea7":"markdown","4565410f":"markdown","7e94fc97":"markdown","b8fe79d3":"markdown","760761e0":"markdown","ca1bde0c":"markdown","e14810d5":"markdown","f45f3a5f":"markdown","91002bac":"markdown","39c2281e":"markdown","9ff24b3f":"markdown","f66c6aff":"markdown","b196a9ed":"markdown","9b10c78f":"markdown","567591a9":"markdown","b69b78f5":"markdown"},"source":{"ead99e30":"#Import libraries\n# Select TensorFlow 2.X, numpy, pandas, and the dataset\n#%tensorflow_version 2.x\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import backend, models, layers, optimizers, regularizers\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\nfrom numpy import expand_dims \n\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom IPython.display import display \nfrom PIL import Image \nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array \nfrom keras.preprocessing.image import array_to_img\nfrom keras.applications.vgg19 import preprocess_input\nfrom keras.applications.inception_v3 import InceptionV3 \n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator \nimport os, shutil, datetime\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input \nfrom keras.layers import Conv2D \nfrom keras.layers import MaxPooling2D \nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import Dense \nfrom keras.layers import Flatten \nfrom keras.layers import Dropout \nfrom keras.optimizers import SGD, Adam\nimport pandas as pd\nfrom IPython.core.display import display, HTML\n\nimport os\nfrom statistics import mean, median\nfrom numpy.random import seed\n# Select TensorFlow 2.X, numpy, pandas, and the dataset\n#%tensorflow_version 2.x\n\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))","1fae23dc":"# Helper Function(s)\ndef plot_eval(Model, hist):\n    import matplotlib.pyplot as plt\n    \n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    epochs = range(1, len(loss) + 1)\n    \n    f = plt.figure(figsize=(12,6))\n    ax1 = f.add_subplot(121)\n    ax2 = f.add_subplot(122)\n    \n    plt.clf()   # clear figure\n    plt.subplot(121)\n    plt.plot(epochs, loss, color = 'blue', label='Training Loss')\n    plt.plot(epochs, val_loss, color = 'orange', label='Validation Loss')\n    plt.title(Model + ' Cross Entropy Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    acc = hist['accuracy']\n    val_acc = hist['val_accuracy']\n    \n    plt.subplot(122)\n    plt.plot(epochs, acc, color = 'blue', label='Training Accuracy')\n    plt.plot(epochs, val_acc, color = 'orange', label='Validation Accuracy')\n    plt.title(Model + ' Classification Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.show()","b6cb862f":"# from google.colab import drive\n# drive.mount('\/k')\n# import os\n# os.listdir('\/k\/My Drive\/data\/train\/ALBATROSS')","bb1ec039":"seed(1234)\nbase       = '\/kaggle\/input\/100-bird-species\/'\ntrain_dir  = 'train'\ntest_dir   = 'test'\nvalid_dir  = 'valid'\n\n# Model Constants\nepochs = 50\nb_size = 32  # 19\nimg_width =  224\nimg_height = 224\n\n# Define callback functions to monitor early stopping and LR\ncallbacks_list = [\n  EarlyStopping(\n        monitor = 'accuracy', \n        patience = 10, \n        mode = 'auto',\n        restore_best_weights=True),\n  ReduceLROnPlateau( \n      monitor='accuracy',\n      factor=0.1, \n      patience=1)\n]\n\n\n# Count images for each species\ndef cntSamples(directory):\n    specs = []\n    for root, dirs, files in os.walk(directory, topdown=True):\n        dirs.sort()\n        for name in dirs:\n            if name not in specs:\n                specs.append(name)\n\n    # file counts for each species \n    nums = []\n    for b in specs:\n        path = os.path.join(directory,b)\n        num_files = len(os.listdir(path))\n        nums.append(num_files)\n \n    # Create Dictionary\n    adict = {specs[i]:nums[i] for i in range(len(specs))}\n    return adict\n\n# Count labels in train, valid and test\nDIR_TEST =  base + 'test'\nDIR_TRAIN = base + 'train'\nDIR_VALID = base + 'valid'\n\n\ntestDict =  cntSamples(DIR_TEST)\ntrainDict = cntSamples(DIR_TRAIN)\nvalidDict = cntSamples(DIR_VALID)","9ec8811e":"train_tbl = pd.DataFrame.from_dict(trainDict, \n                orient='index', dtype=None, columns=['Images'])\n\nnum_classes = len(trainDict)\nlabel_index = list(range(num_classes))\n#print(num_classes)\ntrain_tbl.insert(0,'Label Index',label_index, True)\n\ndisplay(HTML(train_tbl.to_html()))","eefe3974":"from statistics import mean, median\nfrom IPython.display import display, HTML\n\n# Create dictionary of dataset partitions statistics. \ndata = {'Images': [sum(trainDict.values()), \n                   sum(testDict.values()), \n                   sum(validDict.values())],\n        \n        'Species': [len(trainDict.values()), \n                    len(testDict.values()), \n                    len(validDict.values())],\n        \n        'Minimum': [min(trainDict.values()), \n                    min(testDict.values()), \n                    min(validDict.values())],\n        \n        'Maximum': [max(trainDict.values()), \n                    max(testDict.values()), \n                    max(validDict.values())],\n        \n        'Median': [int(median(trainDict.values())), \n                   int(median(testDict.values())), \n                   int(median(validDict.values()))]}","d74e3125":"image_cnt  = train_tbl['Images']\nimage_indx = train_tbl['Label Index']\ny_pos = range(len(image_indx))\nplt.bar(y_pos, image_cnt, align='center', alpha=0.5)\nplt.xlabel(\"Class Number (0-189)\")\nplt.ylabel('Number of Images')\nplt.title('Train Data: Number of Images in each Class (Species)')\n\nplt.show()","97fd5664":"# Creates pandas DataFrame. \nlblSummary = pd.DataFrame(data, index =['Train', 'Test', 'Valid']) \n  \n# print the data \n#print(lblSummary)\ndisplay(HTML(lblSummary.to_html()))","71d071b7":"# Example images\nfrom numpy import expand_dims, reshape\nfrom keras.preprocessing.image import load_img \nfrom keras.preprocessing.image import img_to_array \nfrom keras.preprocessing.image import array_to_img\n\nfrom keras.preprocessing.image import ImageDataGenerator \nimport keras\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef getKeybyValue(LabelDict, value):\n    listItems = LabelDict.items()\n    for item in listItems:\n        if item[1] == value:\n            return item[0]\n    \n    return None\n\nseed(1234)\n\ndef pltFourImages(dir):\n    datagen2 = ImageDataGenerator()\n    it2 = datagen2.flow_from_directory(\n            dir,\n            target_size=(224, 224),\n            batch_size=4,\n            class_mode='binary')\n\n    labDict = it2.class_indices\n    batchX, batchy = it2.next() \n    num_img = batchX.shape[0]\n    imgs = [array_to_img(batchX[i]) for i in range(num_img)]\n    indx = [int(batchy[i]) for i in range(len(batchy))]\n    labs = [getKeybyValue(labDict, i) for i in indx]\n   \n    # settings\n    h, w = 10, 10        \n    nrows, ncols = 2, 2  \n    figsize = [18,12]     \n\n    # create figure (fig), and array of axes (ax)\n    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, dpi = 80)\n\n    # plot image on each sub-plot\n    for i, axi in enumerate(ax.flat):\n        # i runs from 0 to (nrows*ncols-1)\n        # axi is equivalent with ax[rowid][colid]\n        axi.imshow(imgs[i], aspect = 'auto')\n\n        # write Label as title\n        axi.set_title(labs[i])\n\n    plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.2, wspace=0.2)\n    plt.show()\n    return","930d3681":"# Print four images from each of train, test and valid\nfor d in ['train', 'test', 'valid']:\n    print('\\n\\nImages from ', d)\n    pltFourImages(base + d)","3848eac9":"if K.image_data_format() == 'channels_first':\n    input_shape = (3,img_width, img_height)\nelse:\n    input_shape = (img_width, img_height,3)\n    \n# create a data generator \ndatagen = ImageDataGenerator(rescale = 1.0 \/ 255)\n\n# Train Generator for InceptionV3\ntrain_generator = datagen.flow_from_directory(\n    base + train_dir , \n    target_size = (img_width, img_height),\n    class_mode='categorical', \n    batch_size=b_size) \n\n# Validation Generator \nvalidation_generator = datagen.flow_from_directory(\n    base + valid_dir, \n    target_size = (img_width, img_height),\n    class_mode='categorical', \n    batch_size=b_size) \n\n# Test Generator \ntest_generator = datagen.flow_from_directory(\n    base + test_dir, \n    target_size = (img_width, img_height),\n    class_mode='categorical',\n    shuffle = False,\n    batch_size=b_size) ","eba161b6":"# Build the model.\n\nbackend.clear_session()\ninput_shape = (img_width, img_height, 3)\n\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.layers import Input\n\n# Input Tensor\ninput_tensor = Input(shape=input_shape)  \n\nbase_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n\nflat_1 = Flatten()(base_model.output)\n\n# Add a fully-connected layer consisting of 2 x Dense(512)\nx = Dense(512, activation='relu')(flat_1)\nx = Dense(512, activation='relu')(x)\n\n# Logistic layer -- for ever-changing number of classes\npredictions = Dense(num_classes, activation='softmax')(x)\n\n# Model\nmodel_nda = Model(inputs=base_model.input, outputs=predictions)\nprint(\"InceptionV3 layers in model: \", len(model_nda.layers))\n#print(model_nda.summary())\n","d48aba10":"# Compile\nfrom keras.optimizers import SGD\nmodel_nda.compile(optimizer=SGD(lr=0.0015, momentum=0.8), \n              loss='categorical_crossentropy', metrics = ['accuracy'])","61f76e5e":"start = datetime.datetime.now()\n# Train\nepochs = 100 # Should yield ~98 percent accuracy\nhistory_nda = model_nda.fit_generator(train_generator, \n                            steps_per_epoch=len(train_generator)\/\/b_size, \n                            validation_data=validation_generator, \n                            validation_steps=len(validation_generator)\/\/b_size, \n                            epochs=epochs, \n                            #callbacks = callbacks_list,\n                            verbose=2)\n\nend = datetime.datetime.now()\nelapsed = end - start\nprint ('InceptionV3 Model Training complete. Elapsed: ', elapsed)","cb88007a":"# Evaluate\nloss, accuracy = model_nda.evaluate_generator(test_generator) \n \n# Report\nhist_nda = pd.DataFrame(history_nda.history)\ne_exe = hist_nda.shape[0]\n\nprint(\"\\n\\n        InceptionV3\\n\\n\")\nprint(\"  Epochs completed: \", e_exe)\nprint(\"              Loss: {0:.4f}\".format(loss))\nprint(\"          Accuracy: {0:.4f} % \".format(accuracy * 100.0))\nprint(\"           Elapsed:\",elapsed)","46968659":"plot_eval(\"InceptionV3 Model\", hist_nda)","523dc2e6":"import math\n# Create a special test generator\n# Prediction Generator \n\npred_generator = datagen.flow_from_directory(\n    base + test_dir, \n    target_size = (img_width, img_height),\n    class_mode='categorical',\n    shuffle = False,\n    batch_size=b_size) \n\npred = model_nda.predict_generator(pred_generator, steps= math.ceil(950\/b_size))","a9d0d1af":"from sklearn.metrics import classification_report\npred_generator.reset\nspecies = list(train_tbl.index)\n\npred_labs = np.argmax(pred,axis=1)\n\nprint(pred_labs)\n#test_generator.reset()\npred_generator.reset\nrepDict = classification_report(pred_generator.labels, pred_labs, target_names=species, output_dict = True)\nreport = classification_report(pred_generator.labels, pred_labs, target_names=species)\nprint(report)","88d38f4d":"pred_generator.reset\nerrors = np.where(pred_labs != pred_generator.classes)[0]\ndata = []\nfor i in errors:\n   j = pred_labs[i]\n   data.append([pred_generator.filenames[i], train_tbl.index.values[j]])    \n  \n# Create the pandas DataFrame \nmisl_tbl = pd.DataFrame(data, columns = ['File Name', 'Mis-Labeled AS'])\n  \ndisplay(HTML(misl_tbl.to_html()))","2a1a1592":"if K.image_data_format() == 'channels_first':\n    input_shape = (3,img_width, img_height)\nelse:\n    input_shape = (img_width, img_height,3)\n    \n# create a data generator \ndatagen = ImageDataGenerator(rescale = 1.0 \/ 255)\n\n# Train Generator\ntrain_generator = datagen.flow_from_directory(\n    base + train_dir , \n    target_size = (img_width, img_height),\n    class_mode='categorical', \n    batch_size=b_size) \n\n# Validation Generator \nvalidation_generator = datagen.flow_from_directory(\n    base + valid_dir, \n    target_size = (img_width, img_height),\n    class_mode='categorical', \n    batch_size=b_size) \n\n# Test Generator\ntest_generator = datagen.flow_from_directory(\n    base + test_dir, \n    target_size = (img_width, img_height),\n    class_mode='categorical', \n    batch_size=b_size) ","d02abf54":"# InceptionResNetV2\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.preprocessing import image\nfrom keras.utils.vis_utils import plot_model\nimport numpy as np\n\nbackend.clear_session()\ninput_shape = (img_width, img_height, 3)\ninput_tensor = Input(shape = input_shape)\n\nmodel = InceptionResNetV2(include_top=False, weights='imagenet', \n            input_tensor=input_tensor, input_shape=input_shape)\n\n# Freeze the pre-trained layers\ncnt_trainable = 0\ncnt_untrainable = 0\nset_trainable = False\nfor layer in model.layers:\n    if  layer.name == 'block8_1_mixed':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n        cnt_trainable += 1\n    else:\n        layer.trainable = False\n        cnt_untrainable += 1\n        \nprint(\"  Trainable layers: \", cnt_trainable)\nprint(\"Untrainable layers: \", cnt_untrainable)\n        \n\n#print(model.summary())\n\nx = Flatten()(model.output)\nx = Dense(512, activation = 'relu') (x)\nx = Dense(512, activation = 'relu') (x)\noutput = Dense(num_classes, activation = 'softmax') (x)\n\n\n# Model\nmodel_irn = Model(inputs=model.inputs, outputs=output)\nprint(\"IRNV2 layers in model: \", len(model_irn.layers))\n\n\n#print(model_irn.summary())","688bc875":"start = datetime.datetime.now()\nepochs = 150 # should yield ~ 93 percent accuracy\nb_size = 25  # 32\n\n# Compile\nmodel_irn.compile(optimizer=SGD(lr=0.001, momentum=0.9), \n              loss='categorical_crossentropy', \n              metrics = ['accuracy'])\n\n# Train\nhistory_irn = model_irn.fit_generator(train_generator, \n                            steps_per_epoch=len(train_generator)\/\/b_size, \n                            validation_data=validation_generator, \n                            validation_steps=len(validation_generator)\/\/b_size, \n                            epochs=epochs, \n                            #callbacks = callbacks_list, \n                            verbose=2)\n\nend = datetime.datetime.now()\nelapsed = end - start\nprint ('InceptionResNetV2 training complete. Elapsed: ', elapsed)","9289e7e8":"# Evaluate\nloss, accuracy = model_irn.evaluate_generator(test_generator, \n                    steps=len(test_generator), verbose=2) \n \n# Report\nhist_irn = pd.DataFrame(history_irn.history)\ne_exe = hist_irn.shape[0]\n\nprint(\"\\n\\n         InceptionResNetV2 Model \\n\\n\")\nprint(\"  Epochs completed: \", e_exe)\nprint(\"              Loss: {0:.4f}\".format(loss))\nprint(\"          Accuracy: {0:.4f} %\".format(accuracy * 100.0))\nprint(\"           Elapsed:\", elapsed,\"\\n\\n\")\n","81c5fefd":"plot_eval(\"InceptionResNetV2 Model\", hist_irn)","61f5c09b":"import math\n# Create a special test generator\n# Prediction Generator \n\npred_generator = datagen.flow_from_directory(\n    base + test_dir, \n    target_size = (img_width, img_height),\n    class_mode='categorical',\n    shuffle = False,\n    batch_size=b_size) \n\n\n\npred = model_irn.predict_generator(pred_generator, steps= math.ceil(950\/b_size))\n","584d1c6e":"from sklearn.metrics import classification_report\npred_generator.reset\nspecies = list(train_tbl.index)\n\npred_labs = np.argmax(pred,axis=1)\n\ntest_generator.reset()\n\nreport = classification_report(pred_generator.labels, pred_labs, target_names=species)\nprint(report)","f5b0c5a1":"pred_generator.reset\nerrors = np.where(pred_labs != pred_generator.classes)[0]\ndata = []\nfor i in errors:\n   j = pred_labs[i]\n   data.append([pred_generator.filenames[i], train_tbl.index.values[j]])    \n  \n# Create the pandas DataFrame \nmisl_tbl = pd.DataFrame(data, columns = ['File Name', 'Mis-Labeled AS'])\n  \ndisplay(HTML(misl_tbl.to_html()))","ac0b4bcf":"# Classification Models for Bird Species","257c4555":"### Create InceptionV3 Model with ImageNet Weights\u00b6","2679965e":" ### Validate the Distribution of Images in Each of the Data Partitions\n\nThe distribution of samples are validated by the table following:","1958d8bd":"#### Calculate Basic Statistics for Images by Partition","8e1d0a2b":"### Compile and Train the InceptionResNetV2  Model","f2823ab2":"## Classification Report for InceptionV3 Model","8fa1812d":"### Graph Diagnostic Data for InceptionV3 Model","fe651ea7":"## List Mis-Labeled Samples for InceptionResNetV2","4565410f":"### Compile and Train InceptionV3 Model","7e94fc97":"Notebook end.  JD Reed May 3, 2020","b8fe79d3":"### Create InceptionResNetV2 Model with ImageNet Weights\u00b6\nUse New Data Generators.","760761e0":"## InceptionV3 Model\n\n### Data Generators","ca1bde0c":"> # Analysis of Bird Species Dataset\n#### James D Reed (jreed011@regis.edu)\n---\n\n ## Overview of Assignment\n\nThe assignment is to apply skills acquired at Regis University, MSDS686 Deep Learning, to a Kaggle dataset.  The object is to build a convolutional neural network model to classify images.  The choices of datasets are:\n\n * [Classify 200 Bird Species](https:\/\/www.kaggle.com\/gpiosenka\/100-bird-species)\n * [Classify 120 Fruit Varieties](https:\/\/www.kaggle.com\/moltean\/fruits)\n\nThe subject of this analysis is the **Bird Species** data, which at the time of this report, consists of **200** species.  I have produces two models for comparison using the ImageNet pretrained models for **InceptionV3** and **InceptionResNetV2.**\n \n---\n\n## Analysis Objectives\n\n * Describe Dataset \n   + Data layout and size\n   + Data partitioning (train, test and validate)\n   + Map of labels to species.\n   + Validate the number of species images in each of the data partitions.\n   \n * Build Classification Model\n   + Convolutional Neural Network\n   + Pretrained InceptionV3 and InceptionResNetV2 models from Keras.\n   + Data Augementation  - [yielded no improvement of model]\n   + Enable early-stopping and learning-rate optimization callbacks\n   + Compare results of model with and with-out Data Augmentation\n   + Use validation data during model training\n   + Evaluate the Accuracy of the model using the test data\n   + Illustrate Training and Validation Loss and Accuracy\n   + Produce a classification report to show accuracy on a by-class basis.\n\n## Description of Data\n \n The data is a Kaggle dataset titled [_190 Bird Species_](https:\/\/www.kaggle.com\/gpiosenka\/100-bird-species).  It was curated earlier this year on February 29, 2020 by Kaggle user **Gerry Piosenka.**   The data consists of 25,812 _train images_, 950 _validation images_ and 950 _test images_.  Each image is 224 by 224 pixels by three colors (RGB). All of the images have been cropped so that the bird takes up approximately fifty percent of the image.  After cropping each image is resized to 224 X 224 X 3.  The three sets of data are named per their intended use: **train**, **valid** and **test**.  The **valid** and **test** partitions are balanced having exactly **five** images per species.  The **train** set is not balanced having between 87 and 300 images per species.  Each image in all three sets is labled by an integer denoting the species of the bird depicted in the image.  The integers correspond to the position of the species name in a list: the number \"0\" for ALBATROSS and \"189\" for YELLOW HEADED BLACKBIRD.\n \n One interesting aspect of these data mentioned by **Gerry Piosenka** is that male and female of the same species can be very different in appearance.  Almost all of the images in the **valid** and **test** sets are male.  Only twenty percent of the **train** set are female.  This could result in poor classification performance for female images.\n\n## Summary of Methods\n\nFollowing is a summary of the main methods used in this analysis:\n\n * **InceptionV3:**  Both the InceptionV3 and VGG19 models are recognized as providing innovations to the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).  Both models are well adapted to handliing image recognition models.\n \n * **VGG19**  The VGG19 model built (not included in this report) required many epochs to reach high accuracy prediction.  _EarlyStopping_ callback closed the training session too early.  Perhaps there is a hyper parameter I did not adjust that could fix this issue?  In any case, this model was abandoned and, instead, I instaniated a pretrained InceptioResNetV2 model.\n \n * **Data Augmentation:** Although I did try this on the Inception model, the results were not better than the model without DA.  The Bird Species dataset has more than enough images to provide a robust training, validation and testing environment without data augmentation.\n \n  * **InceptionResNetV2** As mentioned above, this model was created after the VGG19 model performance was unsatisfactory.  This model took some major tweaking to get to a level of moderate accuracy.  The original model had over 70 million trainable parameters.  Experimintation with freezing some layers resulted in getting the trainable parameters down to ~ 40 million.  EarlyStopping callback did NOT help reduce the number of epoch.  Trial and error led me to fix the number of epochs to 150.  This yielded an accuracy value of 93 percent.\n \n * **Knowledge Transfer:** Knowledge transfer is used in both the InceptionV3 and InceptionResNetV2 models.  Because these models both had appropriate weights (ImageNet) to leverage, it made sense to have this as a starting point for the analysis.  In the caces of the InceptionV3 model, the initial weights were applied, but all layers were _trainable_.  On the otherhand, the InceptionResNetV2 model used ImageNet weights to start but many blocks were frozen ()\n \n * **Data Generators:**  These data required a method to access the samples **without** reading them all in at one time.  Instead, I used the  **flow_from_directory** method to provide a \"just-in-time\" semantic for all three sample partitions.\n\n * **Keras API:**  As mentioned above, knowledge transfer allowed the use of pre-computed weights in a proven architecture.  The Keras API, as applied to building models allowed customizations for the \"Input\" layer and the classifier layers.\n \n * **Classification Report** A classification report (from _sklearn.metrics_) is produced for both models showing the accuracy of each class and the overall accuracy.\n\n\n## Summary of Models\n\nHere are the pertinent facts about each model:\n\n1. **InceptionV3:**\n   + Convolutional Neural Network (CNN) - considered standard architecture for image recognition\/classification.\n + Trainable parameters: 48.3 million. \n  + input tensor: Input(224,224,3)\n  + output classifier:\n    - Flatten()\n    - Dense(512, activation = 'relu')\n    - Dense(512, activation = 'relu')\n    - Dense(190, activation = 'softmax')\n  + number of layers:   **315**\n\n\n2. **InceptionResNetV2:**\n  + Convolutional Neural Network (CNN) - considered one of the standard architecture for image recognition\/classification.\n  + Trainable parameters: xx.x million. \n  + input tensor: Input(224,224,3)\n  + output classifier:\n    - Flatten()\n    - Dense(512, activation = 'relu')\n    - Dense(512, activation = 'relu')\n    - Dense(190, activation = 'softmax')\n  + number of layers:   **784**\n  + frozen (untrainable) layers:  **630**\n  + trainable layers:  **150**\n\n\n## Analysis of Results\n\nNote, specific run times and accuracy observed are based on the models and hyperparameters documented here.  \n\n1. **InceptionV3**\nThis model was more robust than the **InceptionResNetV2**.  Initially, I used an EarlyStopping callback but found that, given more epochs, accuracy continued to improve long after EarlyStopping would have halted training.\n\n   + Epochs: **100**\n   + Optimizer: Stochastic Gradient Descent **(SGD)**\n   + Learning Rate: **0.0015**\n   + Momentum: **0.80**\n   + Elapsed Time (hh:mm:ss): **00:20:21**\n   + Accuracy (as measured with test data): **98 %**\n\n2. **InceptionResNetV2**\nThe InceptionResNetV2 model is very large and very deep.  It was more wieldly to manage. \nAgain, EarlyStopping callback was not effective.\n\n   * Epochs: **150**\n   * Optimizer: Stochastic Gradient Descent **(SGD)**\n   * Learning Rate: **0.0001**\n   * Momentum: **0.90**\n   * Elapsed Time (hh:mm:ss): **00:24:39**\n   * Accuracy (as measured with test data): **93.26 %**\n\n\n\n","e14810d5":"### Graph Diagnostic Data for InceptionResNetV2 Model","f45f3a5f":"### Sample Images from Train, Test and Valid Partitions","91002bac":"### Evaluate  InceptionResNetV2 Model Using Test Data","39c2281e":"---\n## InceptionResNetV2","9ff24b3f":"### Evaluate InceptionV3 Model Using Test Data","f66c6aff":"## Image Count per Species in Train Dataset Partition","b196a9ed":"## Classification Report for InceptionResNetV3 Model","9b10c78f":"### Bar Graph of Train Image Species count","567591a9":"## List Mis-Labeled Samples","b69b78f5":"### Verify Number of Images in Each Partition"}}