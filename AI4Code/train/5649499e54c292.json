{"cell_type":{"76d9a1bc":"code","1c17f511":"code","8fb170d3":"code","65de5d3d":"code","bd0fc286":"code","f0dc2841":"code","0f2bbcda":"code","20823ce3":"code","255d7748":"code","b73a5560":"code","66d350e3":"code","e1ce70b5":"code","62557b0a":"code","5ebf4802":"code","4294f6e2":"code","1b812775":"code","21e361b1":"markdown","bad64fd7":"markdown","984bc40e":"markdown","855a1b73":"markdown","f9cf1e09":"markdown","0b62e2c7":"markdown","b9cb1fae":"markdown","f435a25c":"markdown","09a143e4":"markdown","547e601c":"markdown"},"source":{"76d9a1bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c17f511":"### Importing necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12, 8\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nfrom sklearn.impute import SimpleImputer\n\n# For better viewing of datasets\npd.set_option('display.max_columns', None)","8fb170d3":"!pip install pyod -qq","65de5d3d":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain.head()","bd0fc286":"print(train.shape)\n\nprint(test.shape)\n\nprint(train.columns)\n\ntrain.info()","f0dc2841":"plt.scatter(range(train.shape[0]),np.sort(train['SalePrice'].values))\nplt.xlabel('index')\nplt.ylabel('SalePrice')\nplt.title(\"SalePrice distribution\")\nsns.despine()","0f2bbcda":"sns.distplot(train['SalePrice'], fit=norm)\nplt.title(\"Distribution of SalePrice\")\nsns.despine()\n\nprint(\"Skewness is %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis is %f\" % train['SalePrice'].kurt())","20823ce3":"import pyod\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom pyod.models.iforest import IForest\n\nclass compiler:\n    \n    def __init__(self, \n                 drop_col_share=0.5,\n                 use_impute=False,\n                 encode_categorical=True,\n                 ordinal_to_skip=None,\n                 use_scaler=False,\n                 random_state=42,\n                 outliers_fraction=0):\n        \"\"\"\n        Create a class object to pre-process data (train\/test) before model training or predictions\n        Parameters\n        ----------\n        drop_col_share: float, default 0.5\n        Minimum share of filled rows required for a column to be kept, i.e. any column having less than these % of rows will be dropped\n        use_impute: bool, default False\n        If you want to impute missing values using mean & mode\n        encode_categorical: bool, default True\n        If you want to create dummy variables for the categorical columns using drop-first\n        ordinal_to_skip: list(str), default None\n        If there are any ordinal variables that you would want not to be encoded\n        use_scaler: bool, default False\n        If you want to use Standard Scaler for standardizing numeric columns\n        random_state: int, default 42\n        To make sure repeated runs do no result in different outcomes for outlier detection\n        outliers_fraction: float, default 0\n        Value between 0-1 to drop x% of data points from low density regions using pyod package\n        \"\"\"\n        # Add assert statements to validate inputs        \n        self.drop_col_share = drop_col_share\n        self.use_impute = use_impute\n        self.encode_categorical = encode_categorical\n        self.ordinal_to_skip = ordinal_to_skip\n        self.use_scaler = use_scaler\n        self.imputer_num = None\n        self.imputer_cat = None\n        self.scaler = None\n        self.random_state = np.random.RandomState(random_state)\n        self.outliers_fraction = outliers_fraction\n        self.drop_cols = None\n        self.numeric_cols = None\n        self.cat_cols = None\n        self.ordinal_cols = None\n        self.missing_val_num = None\n        self.missing_val_cat = None\n        self.clf = None\n        \n        \n    def fit_transform(self, data=None):\n        # Creating a copy of supplied dataframe\n        df = data.copy()\n        \n        # Computing the number of NA values for each column\n        NA_counts = df.isna().sum(axis=0).values\n\n        # Creating a list of columns to drop as they have more than specified values missing\n        self.drop_cols = df.columns[NA_counts > self.drop_col_share * df.shape[0]]\n\n        # Let's drop these columns from both train & test datasets\n        df.drop(columns=self.drop_cols.values,inplace=True)\n        \n        # If imputing is required\n        if(self.use_impute):\n            \n            # Creating an array with number of missing values per column\n            NA_count_num = df.select_dtypes(exclude=['object']).isna().sum().values\n            NA_count_cat = df.select_dtypes(include=['object']).isna().sum().values\n\n            # Creating a list of column names with missing values\n            self.missing_val_num = df.select_dtypes(exclude=['object']).columns[NA_count_num > 0]\n            self.missing_val_cat = df.select_dtypes(include=['object']).columns[NA_count_cat > 0]\n\n            # Initialize simple imputers for both column types\n            # There are further complicated ways to impute like regression, KNN, etc. but we will start simple here\n            self.imputer_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n            self.imputer_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\n            # Doing the imputation in training data\n            df[self.missing_val_num] = pd.DataFrame(self.imputer_num.fit_transform(df[self.missing_val_num]), columns=self.missing_val_num)\n            df[self.missing_val_cat] = pd.DataFrame(self.imputer_cat.fit_transform(df[self.missing_val_cat]), columns=self.missing_val_cat)\n        \n        \n        # Let's break down the columns with missing values into numeric, categorical, and ordinal\n        self.numeric_cols = df.select_dtypes(exclude=['object']).columns\n        if(self.ordinal_to_skip):\n            self.cat_cols = df.select_dtypes(include=['object']).columns.difference(self.ordinal_to_skip)\n            self.ordinal_cols = self.ordinal_to_skip\n        else:\n            self.cat_cols = df.select_dtypes(include=['object']).columns        \n        \n        # If scaling is required\n        if(self.use_scaler):\n            \n            # Scaling the numeric columns\n            self.scaler = StandardScaler()\n            df[self.numeric_cols] = pd.DataFrame(self.scaler.fit_transform(df[self.numeric_cols]), columns=self.numeric_cols)\n        \n        # If encoding is required\n        if(self.encode_categorical):\n            \n            # Encoding categorical data\n            encoded_cols = pd.get_dummies(df[self.cat_cols], prefix=self.cat_cols, drop_first=True)\n            \n            # Changing df as column structure has changed\n            if(self.ordinal_to_skip):\n                df = pd.concat([df[self.numeric_cols], encoded_cols, df[self.ordinal_cols]], axis=1)\n            else:\n                df = pd.concat([df[self.numeric_cols], encoded_cols], axis=1)\n                \n        \n        # If outlier removal is required\n        if(self.outliers_fraction > 0):\n            \n            # Initializing an Isolation forest object\n            self.clf = IForest(contamination=self.outliers_fraction,random_state=self.random_state)\n            self.clf.fit(df)\n\n            # predict raw anomaly score\n            scores_pred = self.clf.decision_function(df) * -1\n\n            # prediction of a datapoint category outlier or inlier\n            y_pred = self.clf.predict(df)\n\n            # Adding outlier label as a column\n            df['outlier'] = y_pred.tolist()\n        \n            df = df[df['outlier'] == 0]\n        \n        df = df.drop(columns = ['outlier'], errors='ignore')\n        \n        # Save the list of final columns to class object\n        self.final_cols = df.columns\n        return df\n    \n    def transform(self, data=None):\n        # Creating a copy of supplied dataframe\n        df = data.copy()\n        \n        # Let's bring column names created from training data as local variables here\n        numeric_cols = self.numeric_cols\n        ordinal_cols = self.ordinal_cols\n        cat_cols = self.cat_cols\n        drop_cols = self.drop_cols\n        missing_val_num = self.missing_val_num\n        missing_val_cat = self.missing_val_cat\n\n        # Let's drop these columns from both train & test datasets\n        df.drop(columns=drop_cols.values,inplace=True)\n        \n        # If imputing is required\n        if(self.use_impute):\n            \n            # Doing the imputation in test data\n            df[missing_val_num] = pd.DataFrame(self.imputer_num.transform(df[missing_val_num]), columns=missing_val_num)\n            df[missing_val_cat] = pd.DataFrame(self.imputer_cat.transform(df[missing_val_cat]), columns=missing_val_cat)\n        \n        \n\n        # If scaling is required\n        if(self.use_scaler):\n            \n            df[numeric_cols] = pd.DataFrame(self.scaler.transform(df[numeric_cols]), columns=numeric_cols)\n        \n        # If encoding is required\n        if(self.encode_categorical):\n            \n            # Encoding categorical data\n            encoded_cols = pd.get_dummies(df[cat_cols], prefix=cat_cols, drop_first=True)\n            \n            # Changing df as column structure has changed\n            if(self.ordinal_to_skip):\n                df = pd.concat([df[numeric_cols], encoded_cols, df[ordinal_cols]], axis=1)\n            else:\n                df = pd.concat([df[numeric_cols], encoded_cols], axis=1)\n        \n        # Make sure the columns are same as training (Because of missing nominal variable values)\n        # Get missing columns in the training test\n        missing_cols = set( self.final_cols ) - set( df.columns )\n        # Add a missing column in test set with default value equal to 0\n        for c in missing_cols:\n            df[c] = 0\n        # Ensure the order of column in the test set is in the same order than in train set\n        df = df[self.final_cols]\n        \n        return df","255d7748":"# Initializing a processor object\nprocessor = compiler(drop_col_share=0.5,\n                     use_impute=True,\n                     encode_categorical=True,\n                     ordinal_to_skip=None,\n                     use_scaler=True,\n                     outliers_fraction=0.05)\n\n# Creating X & y datasets from train\nX = train.iloc[:,:-1]\ny = train.iloc[:,-1]\n# As test data is similar to X here, we don't need to subset that\n\ntrain_X = processor.fit_transform(X)\ntest_X = processor.transform(test)\ntrain_y = y.loc[train_X.index]\n\nprint(train_X.shape)\nprint(test_X.shape)\n\n# Reset index as some observations were dropped from training data\ntrain_X.reset_index(inplace=True, drop=True)\ntrain_y.reset_index(inplace=True, drop=True)","b73a5560":"from sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import f_regression, SelectKBest, RFE\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# Initialize a linear regression object\nlinreg = LinearRegression()\n\n# define feature selection\nkey_fs = SelectKBest(score_func=f_regression, k=30)\n\n# apply feature selection\nX_selected = key_fs.fit_transform(train_X, train_y)\n\n# Capturing the column names for selected features\nkbest_features = train_X.columns[key_fs.get_support()]\n\n# Taking out 20% of training data for testing\nX_train, X_test, y_train, y_test = train_test_split(X_selected, train_y, test_size = 0.2, random_state = 42)\n\n# Training the linear regression model\ncross_val_score(linreg, X_train, y_train, cv=5, scoring = 'neg_root_mean_squared_error').mean()\n\nfrom sklearn.model_selection import GridSearchCV\n\n# creating a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n\n# specify range of hyperparameters\nhyper_params = [{'n_features_to_select': list(range(5, 30))}]\n\n# specify model\nlinreg.fit(X_train, y_train)\nrfe = RFE(linreg)             \n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = rfe, \n                        param_grid = hyper_params, \n                        scoring= 'r2', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train, y_train)\n\n# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","66d350e3":"# plotting cv results\nplt.figure(figsize=(16,6))\n\nplt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\nplt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\nplt.xlabel('number of features')\nplt.ylabel('r-squared')\nplt.title(\"Optimal Number of Features\")\nplt.legend(['test score', 'train score'], loc='upper left')","e1ce70b5":"from sklearn.metrics import r2_score, mean_squared_error\n\n# Final Linear Regression Model\nn_features_optimal = 25\n\n# Fitting the model\nlinreg.fit(X_train, y_train)\n\nrfe = RFE(linreg, n_features_to_select=n_features_optimal)             \nrfe = rfe.fit(X_train, y_train)\n\n# predict prices of X_test\ny_pred = rfe.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nprint(r2)\n\n# Let's compute RMSE as well\nprint(mean_squared_error(y_test, y_pred, squared=False))\n\n# Capturing the column names for final features selected\nrfe_features = kbest_features[rfe.get_support()]","62557b0a":"from scipy.stats import shapiro,kstest\nfrom statsmodels.stats.stattools import durbin_watson\nfrom statsmodels.stats.diagnostic import het_breuschpagan\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n\ndef check_linreg_assumptions(X_train = None,\n                             y_train = None,\n                             feature_names = None,\n                            linmodel = None,\n                            significance_level = 0.05):\n    \n    # Get the predicted values for training set from model\n    y_pred = linmodel.predict(X_train)\n    \n    # Compute residuals\n    resid = y_train - y_pred\n    \n    # Plotting the residuals\n    print(\"Here is a distribution of residuals:\\n\")\n    sns.distplot(resid, fit=norm)\n    plt.title(\"Distribution of Residuals\")\n    sns.despine()\n    plt.show()\n    \n    # Printing the skewness and kurtosis\n    print(\"Skewness is {:.2f}\\tKurtosis is {:.2f}\\n\\n\".format(resid.skew(),resid.kurt()))\n    \n    # Testing for normality of residuals\n    print(\"Test 1\/4: Testing for normality of residuals:\\n\")\n    print(\"Using Shapiro Wilik test for checking normality of residuals:\\n\")\n    shapiro_result = shapiro(resid)\n    if(shapiro_result.pvalue > significance_level):\n        print(\"p-value is {:e}. Residuals are normally distributed.\\n\".format(shapiro_result.pvalue))\n    else:\n        print(\"p-value is {:e}. Residuals are not normally distributed.\\n\".format(shapiro_result.pvalue))\n        \n    print(\"Using Kosmogorov-Smirnov test for checking normality of residuals:\\n\")\n    ks_result = kstest(resid, 'norm')\n    if(ks_result.pvalue > significance_level):\n        print(\"p-value is {:e}. Residuals are normally distributed.\\n\".format(ks_result.pvalue))\n    else:\n        print(\"p-value is {:e}. Residuals are not normally distributed.\\n\".format(ks_result.pvalue))\n        \n    # Testing for auto-correlation of reiduals\n    print(\"Test 2\/4: Testing for auto-collinearity in residuals:\\n\")\n    print(\"Using Durbin-Watson test for auto-collinearity:\\n\")\n    \n    dw_result =  durbin_watson(resid)\n    if(dw_result >= 1.5 and dw_result <= 2.5):\n        print(\"Test-statistic is {:.2f}. There is no significant auto-correlation.\\n\".format(dw_result))\n    elif(dw_result < 1.5):\n        print(\"Test-statistic is {:.2f}. There is significant positive auto-correlation.\\n\".format(dw_result))\n    else:\n        print(\"Test-statistic is {:.2f}. There is significant negative auto-correlation.\\n\".format(dw_result))\n        \n    \n    # Testing for Homoskedasticity of residuals\n    print(\"Test 3\/4: Testing for homoskedasticity of residuals:\\n\")\n    print(\"Using Breush-Pagan test for this:\\n\")\n    \n    bp_result = het_breuschpagan(resid, X_train) # See if you need to supply just values\n    if(bp_result[3] > significance_level):\n        print(\"p-value is {:e}. Residuals are homoskedastic relative to given X values.\\n\".format(bp_result[3]))\n    else:\n        print(\"p-value is {:e}. Residuals are not homoskedastic relative to given X values.\\n\".format(bp_result[3]))\n    \n    \n    # Testing for multicollinearity within X-variables\n    print(\"Test 4\/4: Testing for multicollinearity among X variables:\\n\")\n    print(\"Using Variance Inflation Factor (VIF) to test for this:\\n\")\n    # VIF dataframe \n    vif_data = pd.DataFrame() \n    vif_data[\"feature\"] = feature_names\n\n    # calculating VIF for each feature \n    vif_data[\"VIF\"] = [VIF(X_train, i) \n                              for i in range(len(feature_names))]\n    \n    print(vif_data)\n    \n    if(max(vif_data['VIF']) > 5):\n        print(\"There is significant multicollinearity amongst the features provided. Look at the table above.\")\n    else:\n        print(\"There is no significant multicollinearity in the features provided. All good!\")","5ebf4802":"check_linreg_assumptions(X_train, y_train, feature_names=rfe_features, linmodel=rfe)","4294f6e2":"from sklearn.linear_model import ElasticNetCV\n\n# Initializing the Elastic Net model\nelnetcv = ElasticNetCV(l1_ratio=[0.01, .1, .5, .7, .9, .95, .99, 1],cv=5)\n\n# Creating train & test datasets from original wide datasets\nX_train_new, X_test_new, y_train, y_test = train_test_split(train_X, train_y, test_size = 0.2, random_state = 42)\n\n# Training the model\nelnetcv.fit(X_train_new, y_train)\n\n# Print the cv score on training set\nprint(elnetcv.score(X_train_new, y_train))\n\n# Evaluating metrics on test set\ny_pred = elnetcv.predict(X_test_new)\nprint(r2_score(y_test, y_pred))\n\n# Let's compute RMSE as well\nprint(mean_squared_error(y_test, y_pred, squared=False))","1b812775":"top_coef = pd.Series(elnetcv.coef_, index=list(train_X.columns)).sort_values()\n\ntop_coef.tail(25).plot(kind = 'barh')","21e361b1":"## Conclusion\nWhile the residual distribution appears normal to the eye, it is not. Moreover, it fails the other assumptions outlined for a linear regression model. Therefore, despite explaining 87% of the variance Linear Regression Model is not a good choice in this case. We will still give a try at using lasso regularization to see if that provides any better metric results.","bad64fd7":"So, on the training set our variance went up to 93%, which is great. However, on the test set we are up by 2% only. But the RMSE has dropped significantly","984bc40e":"Let's start by exploring the basic linear models. We will be trying out the following:\n* Linear Regression\n* Lasso Regression (L1 Regularization)\n* Ridge Regression (L2 Regularization)\n* ElasticNet Regression (Combination of L1 & L2)\n* Polynomial Regression\n\n#### Linear Regression\n\nStarting with the linear regression model, we will first need to perform some data wrangling steps to make the data suitable for Linear Regression:\n* Feature selection as Linear Regression will not be able to work with 237 features. We will use \"SelectBestK\" from scikit-learn and pearson correlation to get the best 10-15 variables\n* Validating the assumptions of linear regression. Once we have the residuals & predictions, we can check for the 5 assumptions of linear regressions -\n  - **Linearity**: This one is fine as we assuming that there is a linear relationship between the 'SalePrice' and features of the house\n  - **Normality**: We will conduct Shapiro-Wilk & Kolmogorov-Smirnov test for normality of the residuals\n  - **Auto-collinearity**: Durbin-Watson test for checking auto-correlation in residuals\n  - **Homoskedasticity**: We will use the Breush-Pagan test from statsmodel to test for this\n  - **Multicollinearity**: We will use VIF to check if there is significant multicollinearity in the data\n\nFirst we will reduce the feature set to a lower number (30) using \"Filter\" category of feature selection, based on correlation using SelectKBest & f_regression. After that, we will use RFE to test what number of features works best from 5 to 30 and accordingly use that many features","855a1b73":"### Exploratory Data Analysis\n\nLet's start by looking at the distribution of sales price. It would be helpfult to know whether the distribution is normal, if not what is the skewness and if we need to remove any outliers","f9cf1e09":"### Handling missing values & Categorical data","0b62e2c7":"## Conclusion\nInstead of spending herculean efforts on making the linear regression model work, we can get equal or better results by directly using ElasticNet model with cross-validation to get the best features. Also, while RFE does not tell which feature is best or better than others, ElasticNet gives us exact coefficient values to understand relative importance. For e.g. *GrLivArea has the largest impact on saleprice here*\n\n# Next up: Other regression techniques\nLet's try now to assess if we can get better performance R2-score and RMSE values using more complicated regression techniques. The ones I am planning to cover are:\n* Support Vector\n* Decision Tree \n* Random Forest\n* XGBoost\n* LightGBM\n* Multilayer Perceptron","b9cb1fae":"#### From this, it appears **25** can be the right number of params to select","f435a25c":"Now that we have the results from Linear Regression Model. Let's test the assumptions for linear regression as well.","09a143e4":"### Reading in the data","547e601c":"So now we have removed outliers from train data, and have transformed both train & test data\n\n### Modelling Stage\n\n### Linear Models"}}