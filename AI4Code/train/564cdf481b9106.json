{"cell_type":{"cde3eb73":"code","7769318c":"code","6f0273a2":"code","04e962d4":"code","e6fce275":"code","cc02d3ee":"code","24ef3a57":"code","db86c31c":"code","400c42fc":"markdown"},"source":{"cde3eb73":"START_TEST = 1942\nMODEL_NAME = f'LGBM_002_{START_TEST}'\n\nfrom m5_helpers import *\nimport lightgbm as lgb\nimport pickle\nfrom sklearn.model_selection import train_test_split\n\n########################### Global variables ###############################\nTARGET = 'sales'\n\nSTART_TRAIN = 140\nP_HORIZON = 28\nSEED      = 42                     # Seed for deterministic processes\nNUM_ITERATIONS = 6000              # Rounds for lgbm\nITEM_ID_SAMPLE_SIZE = 1            # In case we want to do fast training or fit more features for testing \n\nDATA_RAW_PATH = '..\/input\/m5-forecasting-accuracy\/'","7769318c":"############################# Original features ###############################\n###############################################################################\n######################### DO NOT ALTER THIS LIST ##############################\noriginal_features = ['id',\n 'd',\n 'sales',\n                     ###### BASIC FEATURES ######\n 'item_id',\n 'dept_id',\n 'cat_id',\n 'store_id',\n 'state_id',\n 'sell_price',\n 'price_min',\n 'price_max',\n 'price_median',\n 'price_mode',\n 'price_mean',\n 'price_std',\n 'price_norm_max',\n 'price_norm_mode',\n 'price_norm_mean',\n 'price_momentum',\n 'price_roll_momentum_4',\n 'price_roll_momentum_24',\n 'price_end_digits',\n 'event_name_1',\n 'event_type_1',\n 'event_name_2',\n 'event_type_2',\n 'snap_CA',\n 'snap_TX',\n 'snap_WI',\n 'tm_d',\n 'tm_w',\n 'tm_m',\n 'tm_y',\n 'tm_wm',\n 'tm_dw',\n 'tm_w_end',\n 'snap_transform_1',\n 'snap_transform_2',\n 'next_event_type_1',\n 'last_event_type_1',\n 'days_since_event',\n 'days_until_event',\n                          ###### ENCODING FEATURES ######\n 'enc_state_id_mean',\n 'enc_state_id_std',\n 'enc_store_id_mean',\n 'enc_store_id_std',\n 'enc_cat_id_mean',\n 'enc_cat_id_std',\n 'enc_dept_id_mean',\n 'enc_dept_id_std',\n 'enc_state_id_cat_id_mean',\n 'enc_state_id_cat_id_std',\n 'enc_state_id_dept_id_mean',\n 'enc_state_id_dept_id_std',\n 'enc_store_id_cat_id_mean',\n 'enc_store_id_cat_id_std',\n 'enc_store_id_dept_id_mean',\n 'enc_store_id_dept_id_std',\n 'enc_item_id_mean',\n 'enc_item_id_std',\n 'enc_item_id_state_id_mean',\n 'enc_item_id_state_id_std',\n 'enc_item_id_store_id_mean',\n 'enc_item_id_store_id_std',\n 'enc_store_id_dept_id_snap_transform_1_mean',\n 'enc_store_id_dept_id_snap_transform_1_std',\n 'enc_item_id_store_id_snap_transform_1_mean',\n 'enc_item_id_store_id_snap_transform_1_std',\n 'enc_store_id_dept_id_tm_m_mean',\n 'enc_store_id_dept_id_tm_m_std',\n 'enc_item_id_store_id_tm_m_mean',\n 'enc_item_id_store_id_tm_m_std',\n 'enc_store_id_dept_id_snap_transform_1_tm_m_mean',\n 'enc_store_id_dept_id_snap_transform_1_tm_m_std',\n 'enc_item_id_store_id_snap_transform_1_tm_m_mean',\n 'enc_item_id_store_id_snap_transform_1_tm_m_std',\n '0.5%',\n '2.5%',\n '16.5%',\n '25%',\n '50%',\n '75%',\n '83.5%',\n '97.5%',\n '99.5%',\n 'max',\n 'fraction_0',\n 'max_streak_allowed',\n 'probability_0',\n                     ####### LAG FEATURES #######\n 'mean_4_dow_0',\n 'mean_4_dow_1',\n 'mean_4_dow_2',\n 'mean_4_dow_3',\n 'mean_4_dow_4',\n 'mean_4_dow_5',\n 'mean_4_dow_6',\n 'mean_20_dow_0',\n 'mean_20_dow_1',\n 'mean_20_dow_2',\n 'mean_20_dow_3',\n 'mean_20_dow_4',\n 'mean_20_dow_5',\n 'mean_20_dow_6',\n 'last_sale_day',\n 'lag_1',\n 'lag_2',\n 'lag_3',\n 'lag_4',\n 'lag_5',\n 'lag_6',\n 'lag_7',\n 'lag_8',\n 'lag_9',\n 'lag_10',\n 'lag_11',\n 'lag_12',\n 'lag_13',\n 'lag_14',\n 'lag_15',\n 'lag_16',\n 'lag_17',\n 'lag_18',\n 'lag_19',\n 'lag_20',\n 'lag_21',\n 'lag_22',\n 'lag_23',\n 'lag_24',\n 'lag_25',\n 'lag_26',\n 'lag_27',\n 'lag_28',\n 'lag_29',\n 'lag_30',\n 'lag_31',\n 'lag_32',\n 'lag_33',\n 'lag_34',\n 'lag_35',\n 'lag_36',\n 'lag_37',\n 'lag_38',\n 'lag_39',\n 'lag_40',\n 'lag_41',\n 'lag_42',\n 'lag_43',\n 'lag_44',\n 'lag_45',\n 'lag_46',\n 'lag_47',\n 'lag_48',\n 'lag_49',\n 'lag_50',\n 'lag_51',\n 'lag_52',\n 'lag_53',\n 'lag_54',\n 'lag_55',\n 'lag_56',\n 'lag_57',\n 'lag_58',\n 'lag_59',\n 'lag_60',\n 'lag_61',\n 'lag_62',\n 'lag_63',\n 'lag_64',\n 'lag_65',\n 'lag_66',\n 'lag_67',\n 'lag_68',\n 'lag_69',\n 'lag_70',\n 'lag_71',\n 'lag_72',\n 'lag_73',\n 'lag_74',\n 'lag_75',\n 'lag_76',\n 'lag_77',\n 'lag_78',\n 'lag_79',\n 'lag_80',\n 'lag_81',\n 'lag_82',\n 'lag_83',\n 'lag_84',\n 'shift_1_rolling_nan_mean_3',\n 'shift_1_rolling_nan_median_3',\n 'shift_1_rolling_mean_decay_3',\n 'shift_1_rolling_diff_nan_mean_3',\n 'shift_1_rolling_nan_min_3',\n 'shift_1_rolling_nan_max_3',\n 'shift_1_rolling_nan_std_3',\n 'shift_1_rolling_nan_mean_7',\n 'shift_1_rolling_nan_median_7',\n 'shift_1_rolling_mean_decay_7',\n 'shift_1_rolling_diff_nan_mean_7',\n 'shift_1_rolling_nan_min_7',\n 'shift_1_rolling_nan_max_7',\n 'shift_1_rolling_nan_std_7',\n 'shift_1_rolling_nan_mean_14',\n 'shift_1_rolling_nan_median_14',\n 'shift_1_rolling_mean_decay_14',\n 'shift_1_rolling_diff_nan_mean_14',\n 'shift_1_rolling_nan_min_14',\n 'shift_1_rolling_nan_max_14',\n 'shift_1_rolling_nan_std_14',\n 'shift_1_rolling_nan_mean_30',\n 'shift_1_rolling_nan_median_30',\n 'shift_1_rolling_mean_decay_30',\n 'shift_1_rolling_diff_nan_mean_30',\n 'shift_1_rolling_nan_min_30',\n 'shift_1_rolling_nan_max_30',\n 'shift_1_rolling_nan_std_30',\n 'shift_1_rolling_nan_mean_60',\n 'shift_1_rolling_nan_median_60',\n 'shift_1_rolling_mean_decay_60',\n 'shift_1_rolling_diff_nan_mean_60',\n 'shift_1_rolling_nan_min_60',\n 'shift_1_rolling_nan_max_60',\n 'shift_1_rolling_nan_std_60',\n 'shift_1_rolling_nan_mean_140',\n 'shift_1_rolling_nan_median_140',\n 'shift_1_rolling_mean_decay_140',\n 'shift_1_rolling_diff_nan_mean_140',\n 'shift_1_rolling_nan_min_140',\n 'shift_1_rolling_nan_max_140',\n 'shift_1_rolling_nan_std_140',\n 'shift_8_rolling_nan_mean_7',\n 'shift_8_rolling_mean_decay_7',\n 'shift_8_rolling_diff_nan_mean_7',\n 'shift_29_rolling_nan_mean_7',\n 'shift_29_rolling_mean_decay_7',\n 'shift_29_rolling_diff_nan_mean_7',\n 'momentum_7_rolling_nan_mean_7',\n 'momentum_28_rolling_nan_mean_7',\n 'momentum_7_rolling_mean_decay_7',\n 'momentum_28_rolling_mean_decay_7',\n 'momentum_7_rolling_diff_nan_mean_7',\n 'momentum_28_rolling_diff_nan_mean_7',\n 'shift_8_rolling_nan_mean_30',\n 'shift_8_rolling_mean_decay_30',\n 'shift_8_rolling_diff_nan_mean_30',\n 'shift_29_rolling_nan_mean_30',\n 'shift_29_rolling_mean_decay_30',\n 'shift_29_rolling_diff_nan_mean_30',\n 'momentum_7_rolling_nan_mean_30',\n 'momentum_28_rolling_nan_mean_30',\n 'momentum_7_rolling_mean_decay_30',\n 'momentum_28_rolling_mean_decay_30',\n 'momentum_7_rolling_diff_nan_mean_30',\n 'momentum_28_rolling_diff_nan_mean_30',\n 'shift_29_rolling_nan_mean_60',\n 'shift_29_rolling_mean_decay_60',\n 'shift_29_rolling_diff_nan_mean_60',\n 'shift_91_rolling_nan_mean_60',\n 'shift_91_rolling_mean_decay_60',\n 'shift_91_rolling_diff_nan_mean_60',\n 'momentum_28_rolling_nan_mean_60',\n 'momentum_90_rolling_nan_mean_60',\n 'momentum_28_rolling_mean_decay_60',\n 'momentum_90_rolling_mean_decay_60',\n 'momentum_28_rolling_diff_nan_mean_60',\n 'momentum_90_rolling_diff_nan_mean_60',\n                    \n                    ############# PCA ########################### \n '29_84_lags_ipca_comp_1',\n '29_84_lags_ipca_comp_2',\n '29_84_lags_ipca_comp_3',\n '29_84_lags_ipca_comp_4',\n '29_84_lags_ipca_comp_5',\n '29_84_lags_ipca_comp_6',\n '29_84_lags_ipca_comp_7',\n '29_84_lags_ipca_comp_8',\n '29_84_lags_ipca_comp_9',\n '29_84_lags_ipca_comp_10',\n '29_84_lags_ipca_comp_11',\n '29_84_lags_ipca_comp_12',\n '29_84_lags_ipca_comp_13',\n '29_84_lags_ipca_comp_14', \n                     \n '15_84_lags_ipca_comp_1',\n '15_84_lags_ipca_comp_2',\n '15_84_lags_ipca_comp_3',\n '15_84_lags_ipca_comp_4',\n '15_84_lags_ipca_comp_5',\n '15_84_lags_ipca_comp_6',\n '15_84_lags_ipca_comp_7',\n '15_84_lags_ipca_comp_8',\n '15_84_lags_ipca_comp_9',\n '15_84_lags_ipca_comp_10',\n '15_84_lags_ipca_comp_11',\n '15_84_lags_ipca_comp_12',\n '15_84_lags_ipca_comp_13',\n '15_84_lags_ipca_comp_14']","6f0273a2":"keep_features = [\n    \n 'id', \n 'd', \n 'sales',\n#                      ###### BASIC FEATURES ######\n#  'item_id', #####################\n 'dept_id',\n#  'cat_id',#####################################################################\n 'store_id',\n#  'state_id',#####################################################################\n 'sell_price',\n 'price_min',\n 'price_max',\n 'price_median',\n 'price_mode',\n 'price_mean',\n 'price_std',\n 'price_norm_max',\n 'price_norm_mode',\n 'price_norm_mean',\n 'price_momentum',\n 'price_roll_momentum_4',\n 'price_roll_momentum_24',\n 'price_end_digits',\n#  'event_name_1',#######################################################################################\n#  'event_type_1',#######################################################################################\n#  'event_name_2',#######################################################################################\n#  'event_type_2',#######################################################################################\n#  'snap_CA',#######################################################################################\n#  'snap_TX',#######################################################################################\n#  'snap_WI',#######################################################################################\n 'tm_d',\n 'tm_w',\n 'tm_m',\n#  'tm_y',\n#  'tm_wm',#########################################################\n 'tm_dw',\n 'tm_w_end',\n 'snap_transform_1',\n 'snap_transform_2',\n 'next_event_type_1',\n 'last_event_type_1',\n 'days_since_event',\n 'days_until_event',\n#                           ###### ENCODING FEATURES ######\n#  'enc_state_id_mean', ####################################################################\n#  'enc_state_id_std',####################################################################\n#  'enc_store_id_mean',####################################################################\n#  'enc_store_id_std',####################################################################\n#  'enc_cat_id_mean',####################################################################\n#  'enc_cat_id_std',####################################################################\n#  'enc_dept_id_mean',####################################################################\n#  'enc_dept_id_std',####################################################################\n#  'enc_state_id_cat_id_mean',####################################################################\n#  'enc_state_id_cat_id_std',####################################################################\n#  'enc_state_id_dept_id_mean',####################################################################\n#  'enc_state_id_dept_id_std',####################################################################\n#  'enc_store_id_cat_id_mean',####################################################################\n#  'enc_store_id_cat_id_std',####################################################################\n#  'enc_store_id_dept_id_mean',####################################################################\n#  'enc_store_id_dept_id_std',####################################################################\n#  'enc_item_id_mean',####################################################################\n#  'enc_item_id_std',####################################################################\n#  'enc_item_id_state_id_mean',####################################################################\n#  'enc_item_id_state_id_std',####################################################################\n#  'enc_item_id_store_id_mean',####################################################################\n#  'enc_item_id_store_id_std',####################################################################\n#  'enc_store_id_dept_id_snap_transform_1_mean',####################################################################\n#  'enc_store_id_dept_id_snap_transform_1_std',####################################################################\n#  'enc_item_id_store_id_snap_transform_1_mean',####################################################################\n#  'enc_item_id_store_id_snap_transform_1_std',####################################################################\n#  'enc_store_id_dept_id_tm_m_mean',####################################################################\n#  'enc_store_id_dept_id_tm_m_std',####################################################################\n#  'enc_item_id_store_id_tm_m_mean',####################################################################\n#  'enc_item_id_store_id_tm_m_std',####################################################################\n#  'enc_store_id_dept_id_snap_transform_1_tm_m_mean',####################################################################\n#  'enc_store_id_dept_id_snap_transform_1_tm_m_std',####################################################################\n#  'enc_item_id_store_id_snap_transform_1_tm_m_mean',####################################################################\n#  'enc_item_id_store_id_snap_transform_1_tm_m_std',####################################################################\n#  '0.5%',#########################################################################################\n#  '2.5%',#########################################################################################\n#  '16.5%',#########################################################################################\n#  '25%',#########################################################################################\n#  '50%',#########################################################################################\n#  '75%',#########################################################################################\n#  '83.5%',#########################################################################################\n#  '97.5%',#########################################################################################\n#  '99.5%',#########################################################################################\n#  'max',\n 'fraction_0',\n#  'max_streak_allowed', \n#  'probability_0',\n#                      ####### LAG FEATURES #######\n 'mean_4_dow_0',\n 'mean_4_dow_1',\n 'mean_4_dow_2',\n 'mean_4_dow_3',\n 'mean_4_dow_4',\n 'mean_4_dow_5',\n 'mean_4_dow_6',\n 'mean_20_dow_0',\n 'mean_20_dow_1',\n 'mean_20_dow_2',\n 'mean_20_dow_3',\n 'mean_20_dow_4',\n 'mean_20_dow_5',\n 'mean_20_dow_6',\n 'last_sale_day',\n 'lag_1',\n 'lag_2',\n 'lag_3',\n 'lag_4',\n 'lag_5',\n 'lag_6',\n 'lag_7',\n 'lag_8',\n 'lag_9',\n 'lag_10',\n 'lag_11',\n 'lag_12',\n 'lag_13',\n 'lag_14',\n#  'lag_15',\n#  'lag_16',\n#  'lag_17',\n#  'lag_18',\n#  'lag_19',\n#  'lag_20',\n#  'lag_21',\n#  'lag_22',\n#  'lag_23',\n#  'lag_24',\n#  'lag_25',\n#  'lag_26',\n#  'lag_27',\n#  'lag_28',\n#  'lag_29',\n#  'lag_30',\n#  'lag_31',\n#  'lag_32',\n#  'lag_33',\n#  'lag_34',\n#  'lag_35',\n#  'lag_36',\n#  'lag_37',\n#  'lag_38',\n#  'lag_39',\n#  'lag_40',\n#  'lag_41',\n#  'lag_42',\n#  'lag_43',\n#  'lag_44',\n#  'lag_45',\n#  'lag_46',\n#  'lag_47',\n#  'lag_48',\n#  'lag_49',\n#  'lag_50',\n#  'lag_51',\n#  'lag_52',\n#  'lag_53',\n#  'lag_54',\n#  'lag_55',\n#  'lag_56',\n#  'lag_57',\n#  'lag_58',\n#  'lag_59',\n#  'lag_60',\n#  'lag_61',\n#  'lag_62',\n#  'lag_63',\n#  'lag_64',\n#  'lag_65',\n#  'lag_66',\n#  'lag_67',\n#  'lag_68',\n#  'lag_69',\n#  'lag_70',\n#  'lag_71',\n#  'lag_72',\n#  'lag_73',\n#  'lag_74',\n#  'lag_75',\n#  'lag_76',\n#  'lag_77',\n#  'lag_78',\n#  'lag_79',\n#  'lag_80',\n#  'lag_81',\n#  'lag_82',\n#  'lag_83',\n#  'lag_84',\n 'shift_1_rolling_nan_mean_3',\n#  'shift_1_rolling_nan_median_3',\n 'shift_1_rolling_mean_decay_3',\n#  'shift_1_rolling_diff_nan_mean_3',\n# #  'shift_1_rolling_nan_min_3',\n# #  'shift_1_rolling_nan_max_3',\n#  'shift_1_rolling_nan_std_3',\n 'shift_1_rolling_nan_mean_7',\n#  'shift_1_rolling_nan_median_7',\n 'shift_1_rolling_mean_decay_7',\n#  'shift_1_rolling_diff_nan_mean_7',\n#  'shift_1_rolling_nan_min_7',\n#  'shift_1_rolling_nan_max_7',\n 'shift_1_rolling_nan_std_7',\n 'shift_1_rolling_nan_mean_14',\n#  'shift_1_rolling_nan_median_14',\n 'shift_1_rolling_mean_decay_14',\n 'shift_1_rolling_diff_nan_mean_14',\n#  'shift_1_rolling_nan_min_14',\n#  'shift_1_rolling_nan_max_14',\n 'shift_1_rolling_nan_std_14',\n 'shift_1_rolling_nan_mean_30',\n#  'shift_1_rolling_nan_median_30',\n 'shift_1_rolling_mean_decay_30',\n#  'shift_1_rolling_diff_nan_mean_30',\n#  'shift_1_rolling_nan_min_30',\n#  'shift_1_rolling_nan_max_30',\n#  'shift_1_rolling_nan_std_30',\n 'shift_1_rolling_nan_mean_60',\n 'shift_1_rolling_nan_median_60',\n 'shift_1_rolling_mean_decay_60',\n#  'shift_1_rolling_diff_nan_mean_60',\n#  'shift_1_rolling_nan_min_60',\n#  'shift_1_rolling_nan_max_60',\n 'shift_1_rolling_nan_std_60',\n 'shift_1_rolling_nan_mean_140',\n#  'shift_1_rolling_nan_median_140',\n 'shift_1_rolling_mean_decay_140',\n#  'shift_1_rolling_diff_nan_mean_140',\n#  'shift_1_rolling_nan_min_140',\n#  'shift_1_rolling_nan_max_140',\n 'shift_1_rolling_nan_std_140',\n 'shift_8_rolling_nan_mean_7',\n 'shift_8_rolling_mean_decay_7',\n#  'shift_8_rolling_diff_nan_mean_7',\n 'shift_29_rolling_nan_mean_7',\n#  'shift_29_rolling_mean_decay_7',\n#  'shift_29_rolling_diff_nan_mean_7',\n 'momentum_7_rolling_nan_mean_7',\n 'momentum_28_rolling_nan_mean_7',\n 'momentum_7_rolling_mean_decay_7',\n#  'momentum_28_rolling_mean_decay_7',\n 'momentum_7_rolling_diff_nan_mean_7',\n 'momentum_28_rolling_diff_nan_mean_7',\n 'shift_8_rolling_nan_mean_30',\n 'shift_8_rolling_mean_decay_30',\n#  'shift_8_rolling_diff_nan_mean_30',\n 'shift_29_rolling_nan_mean_30',\n 'shift_29_rolling_mean_decay_30',\n#  'shift_29_rolling_diff_nan_mean_30',\n 'momentum_7_rolling_nan_mean_30',\n 'momentum_28_rolling_nan_mean_30',\n#  'momentum_7_rolling_mean_decay_30',\n#  'momentum_28_rolling_mean_decay_30',\n#  'momentum_7_rolling_diff_nan_mean_30',\n#  'momentum_28_rolling_diff_nan_mean_30',\n 'shift_29_rolling_nan_mean_60',\n#  'shift_29_rolling_mean_decay_60',\n#  'shift_29_rolling_diff_nan_mean_60',\n 'shift_91_rolling_nan_mean_60',\n 'shift_91_rolling_mean_decay_60',\n#  'shift_91_rolling_diff_nan_mean_60',\n#  'momentum_28_rolling_nan_mean_60',\n#  'momentum_90_rolling_nan_mean_60',\n#  'momentum_28_rolling_mean_decay_60',\n#  'momentum_90_rolling_mean_decay_60',\n#  'momentum_28_rolling_diff_nan_mean_60',\n#  'momentum_90_rolling_diff_nan_mean_60',\n    \n#  '1_84_lags_ipca_comp_1',\n#  '1_84_lags_ipca_comp_2',\n#  '1_84_lags_ipca_comp_3',\n#  '1_84_lags_ipca_comp_4',\n#  '1_84_lags_ipca_comp_5',\n#  '1_84_lags_ipca_comp_6',\n#  '1_84_lags_ipca_comp_7',\n#  '1_84_lags_ipca_comp_8',\n#  '1_84_lags_ipca_comp_9',\n#  '1_84_lags_ipca_comp_10',\n#  '1_84_lags_ipca_comp_11',\n#  '1_84_lags_ipca_comp_12',\n#  '1_84_lags_ipca_comp_13',\n#  '1_84_lags_ipca_comp_14',\n '15_84_lags_ipca_comp_1',\n '15_84_lags_ipca_comp_2',\n '15_84_lags_ipca_comp_3',\n '15_84_lags_ipca_comp_4',\n '15_84_lags_ipca_comp_5',\n '15_84_lags_ipca_comp_6',\n '15_84_lags_ipca_comp_7',\n#  '15_84_lags_ipca_comp_8',\n#  '15_84_lags_ipca_comp_9',\n#  '15_84_lags_ipca_comp_10',\n#  '15_84_lags_ipca_comp_11',\n#  '15_84_lags_ipca_comp_12',\n#  '15_84_lags_ipca_comp_13',\n#  '15_84_lags_ipca_comp_14'\n]","04e962d4":"drop_features_from_training = [fe for fe in original_features if fe not in keep_features]\nprint(len(drop_features_from_training))\nprint(f'we have {len(keep_features)} features')","e6fce275":"########################## w_12 ################################\nDATA_INTERIM_PATH = '..\/input\/m5-w-df-with-all-scaled-weights\/'\nw_12 = pd.read_pickle(F'{DATA_INTERIM_PATH}w_12_{START_TEST}.pkl')\nif START_TEST < 1942: \n    df = pd.read_pickle(F'{DATA_INTERIM_PATH}w_12_1942.pkl')\n    w_12 = w_12.join(df[['oos_level_12_scale']])\n\n####################### For validation #########################\ndef get_actuals(train_df, days):\n    return train_df[[f'd_{d}' for d in days]].values\n\n################################################################\n###################### Evaluators  #############################\nclass WRMSSELGBM(WRMSSE): \n    def feval(self, preds, train_data):\n        preds = preds.reshape(self.actuals.shape[1], -1).T\n        score = self.score(preds)\n        return 'WRMSSE', score, False\n    \n    def full_score(self, preds, train_data):\n        preds = preds.reshape(self.actuals.shape[1], -1).T\n        score = self.score_all(preds)\n        return 'WRMSSE', score, False\n    \ndef get_evaluators(keep_id, start_test_days=[1914], keep_all=False): \n    train_df, cal_df, prices_df, _ = read_data(F'{DATA_RAW_PATH}')\n    if not keep_all:\n        train_df = train_df[train_df.item_id.isin(keep_id)]\n\n    evaluators = []\n    for start_test in start_test_days: \n        e = WRMSSELGBM(train_df, cal_df, prices_df, start_test)\n        evaluators.append(e)\n    return evaluators\n\n################################################################\n###################### Objective functions #####################\n\ndef get_wrmsse(w_12_train):\n    \"\"\"w_12_train must be aligned with grid_df like\n    w_12_train = w_12.reindex(grid_df[train_mask].id)\n    \"\"\"\n    weight = w_12_train['total_sw'] \/ w_12_train['total_sw'].mean()\n    \n    def wrmsse(preds, train_data): \n        actuals = train_data.get_label()\n        diff = actuals - preds\n        grad = -diff * weight\n        hess = np.ones_like(diff)\n        return grad, hess\n    return wrmsse\n\ndef get_oos_rmsse(w_12_train): \n    \n    oos_scale = 1\/w_12_train.oos_level_12_scale\n    \n    def oos_rmsse(preds, train_data): \n        actuals = train_data.get_label()\n        diff = actuals - preds\n        grad = -diff * oos_scale\n        hess = np.ones_like(diff)\n        return grad, hess\n    return oos_rmsse\n\n\n################################################################\n######################## Custom metrics ########################\n\ndef get_wrmsse_metric(w_12_valid):\n    weight = w_12_valid['total_sw'] \/ w_12_valid['total_sw'].mean()\n\n    def wrmsse_metric(preds, train_data): \n        actuals = train_data.get_label()\n        diff = actuals - preds\n        res = np.sum(diff**2 * weight)\n        return 'custom_wrmsse_metric', res, False\n    return wrmsse_metric\n\ndef get_oos_rmsse_metric(w_12_valid): \n    oos_scale = 1\/w_12_valid.oos_level_12_scale\n    \n    def oos_rmsse_metric(preds, train_data): \n        actuals = train_data.get_label()\n        diff = actuals - preds\n        res = np.sum(diff**2 * oos_scale**2)\n        return 'oos_rmsse', res, False\n    return oos_rmsse_metric\n\ndef get_rmsse_metric(w_12_valid):\n    scale = 1\/np.sqrt(w_12_valid.scale)\n    \n    def rmsse_metric(preds, train_data): \n        actuals = train_data.get_label()\n        diff = actuals - preds\n        res = np.sum(diff**2 * scale**2)\n        return 'rmsse', res, False\n    return rmsse_metric\n\ndef get_l12_wrmsse_metric(w_12_valid):\n    scale = w_12_valid.scaled_weight\n    \n    def l12_wrmsse_metric(preds, train_data): \n        actuals = train_data.get_label()\n        diff = actuals - preds\n        res = np.sum(np.abs(diff) * scale)\n        return 'rmsse', res, False\n    return l12_wrmsse_metric\n\n################################################################\n################# Simple model training function ###############\ndef train_lgbm(grid_df,             # All data, train and valid\n               lgb_params,     \n               remove_features,\n               start_train=140, \n               test_day=1914,\n               TARGET='sales',\n               objective=None, \n               metric=None,\n               fobj=None,           # Custom objective function\n               feval=None,          # Custom metric function\n               verbose_eval=25, \n               categories=None,     # Must use if categories are int type!!!!!\n               w_12=None, \n               estimator=None,\n               ):    \n    \"\"\"returns an lgbm estimator: this version is made for day\n    to day models. \"\"\"\n    \n    feature_cols = [col for col in list(grid_df) if col not in remove_features]\n    lgb_params = lgb_params.copy()\n    \n    ####################### test set #########################\n    ##########################################################\n    test_mask = (grid_df.d == test_day) \n    test_x = grid_df[test_mask][feature_cols]\n    \n    ############ train, valid and test masks #################\n    start_valid = test_day - 28\n    valid_mask = (grid_df.d == start_valid) \n    train_mask = (grid_df.d >= start_train) & (grid_df.d < start_valid) & (grid_df[TARGET].notnull())\n    \n    ########################### train, valid and test #########################\n    train_x, train_y = grid_df[train_mask][feature_cols], grid_df[train_mask][TARGET]\n    valid_x, valid_y = grid_df[valid_mask][feature_cols], grid_df[valid_mask][TARGET]\n    \n    ######## Switching to numpy array to avoid RAM usage spike ################\n#     features = list(train_x)\n#     train_x = train_x.values.astype(np.float32)\n#     valid_x = valid_x.values.astype(np.float32)\n\n#     train_data = lgb.Dataset(train_x, feature_name=features, categorical_feature=categories, label=train_y)\n#     valid_data = lgb.Dataset(valid_x, feature_name=features, categorical_feature=categories, label=valid_y)\n    train_data = lgb.Dataset(train_x, label=train_y)\n    valid_data = lgb.Dataset(valid_x, label=valid_y)\n\n    print('\\n', '#' * 72,'\\n', f'Training set: {start_train} to {start_valid - 1}\\n', \n         f'Valid set: {test_day - 28}')\n    \n    ###################### Update objective ########################\n    if objective == 'tweedie': \n        lgb_params['objective'] = 'tweedie'\n        lgb_params['tweedie_variance_power'] = 1.1\n        \n    \n        \n\n    \n    estimator = lgb.train(\n                                lgb_params,\n                                train_set=train_data,\n                                valid_sets=[valid_data],\n                                fobj = fobj,\n                                feval = feval,\n                                verbose_eval=verbose_eval, \n                                \n    )\n    gc.collect()    \n    \n    estimator.save_model(f'{MODEL_NAME}_{test_day - START_TEST}.bin', -1)\n    preds = estimator.predict(test_x)\n    \n    ax = lgb.plot_importance(estimator, max_num_features=266, figsize=(15,40), title=f'{MODEL_NAME}_{test_day}')\n    plt.show()\n    \n    return estimator, preds, test_x, test_mask, feature_cols","cc02d3ee":"###########################################################################\n######################## Process before training ##########################\n\n########################### Load competition data #########################\ncal_df = pd.read_csv(F'{DATA_RAW_PATH}calendar.csv')\nprices_df = pd.read_csv(F'{DATA_RAW_PATH}sell_prices.csv')\ntrain_df = pd.read_csv(F'{DATA_RAW_PATH}sales_train_evaluation.csv')\n\n######################### Find items to drop ##############################\n# We don't want items that haven't had sales at all stores for at least \n# 72 days, because this is the situation we had for the public validation\n# set, and it could make our evaluator not behave correclty\n\n###########################################################################\nDATA_FEATURES_PATH = '\/kaggle\/input\/m5-fe-basic-features\/'\ngrid_df = pd.read_pickle(F'{DATA_FEATURES_PATH}grid_df_base_fe.pkl')\nfirst_sale = grid_df[grid_df.sales.notnull()].drop_duplicates('id')\nfirst_sale = first_sale.groupby('item_id')['d'].max()\nkeep_id = first_sale[(START_TEST - first_sale) >= 68].index\n\n###########################################################################\n##################### START_TEST specific processing ######################\n\n######################## Truncate train_df ################################\n# Create a truncated train_df so we can easily get accurate data\ntrain_df_truncated = train_df[train_df.item_id.isin(keep_id)]\n\n####################### Raw data for validation ###########################\nvalid_days = [d for d in range(START_TEST - 28, START_TEST)]\nvalid_actuals = get_actuals(train_df_truncated, valid_days)","24ef3a57":"############################## Evaluator ##################################\ne = WRMSSELGBM(train_df_truncated, cal_df, prices_df, START_TEST)\nif START_TEST!=1942:\n    e_actuals = e.actuals.copy()\n\n############################## prediction df ##############################\nprediction_df = train_df_truncated[['id']]","db86c31c":"for i in range(14):\n    ###########################################################################\n    ##################### test_day specific processing ########################\n    # (START_TEST + test_day) is the day we are building a model to predict. \n    # for i in range(28):\n    #     print(f'day {i + 1}')\n    test_day = i\n\n    # Validation set only a single day!... a month before predicting...weird. \n    # The thought is that this the best single day I could pick. This is also\n    # what the grocery favorita winners did... I think. \n    e.actuals = valid_actuals[:, test_day].reshape((-1,1))\n\n\n\n    ######################### Features #############################\n    ################################################################\n\n    ##################### Feature selection ########################\n\n    #################### Start with the basics #####################\n    DATA_FEATURES_PATH = '\/kaggle\/input\/m5-fe-basic-features\/'\n    grid_df = pd.read_pickle(F'{DATA_FEATURES_PATH}grid_df_base_fe.pkl')\n    grid_df['sales'] = grid_df['sales'].astype(np.float16)\n\n    ####################### Truncate data  #########################\n    # Item_id\n    grid_df = grid_df[grid_df.item_id.isin(keep_id)]\n\n    # Days\n    days = [d for d in range(START_TRAIN, START_TEST + test_day + 1) if d%7 == (START_TEST + test_day)%7]\n    grid_df = grid_df[grid_df.d.isin(days)]\n\n    # Features\n    keep_cols = [col for col in list(grid_df) if col not in drop_features_from_training]\n    grid_df = grid_df[keep_cols]\n\n\n\n    ############ Features that don't need shifting #################\n    # These will be features that \"stay\" with the id and sale date, \n    # while other features such as lags and rolling windows will \n    # need to be shifted so that we prevent leakage. All lags will \n    # look the same \n\n    ###### add price features ######\n    df = pd.read_pickle(F'{DATA_FEATURES_PATH}grid_df_price_fe.pkl').iloc[:, 3:]\n    keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    df = df[keep_cols]\n    df = df[df.index.isin(grid_df.index)]\n    grid_df = pd.concat([grid_df, df], axis=1)\n    del df\n    gc.collect() \n\n    ###### add calendar features ######\n    DATA_FEATURES_PATH = '..\/input\/m5-nb-fe-snap-and-event-features\/'\n    df = pd.read_pickle(f'{DATA_FEATURES_PATH}grid_df_cal_fe_2.pkl').iloc[:, 3:]\n    keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    df = df[keep_cols]\n    df = df[df.index.isin(grid_df.index)]\n    grid_df = pd.concat([grid_df, df], axis=1)\n    del df\n    gc.collect()\n\n\n\n    ################## Stats encodings #############################\n    DATA_FEATURES_PATH = '..\/input\/m5-fe-dow-encodings\/'\n    df = pd.read_pickle(f'{DATA_FEATURES_PATH}fe_stats_encodings.pkl')\n    keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    df = df[keep_cols]\n    df = df[df.index.isin(grid_df.index)]\n    grid_df = pd.concat([grid_df, df], axis=1)\n    del df\n    gc.collect()\n\n    ################################################################\n    ############ Features that need index to be shifted ############\n    # This way, we can use the same lagged features and be sure that\n    # we don't have leakage and everything is in line. \n    shift = 30490 * test_day\n\n    ##### Day of week means and last sale day ######\n    DATA_FEATURES_PATH = '..\/input\/m5-fe-dow-encodings\/'\n    df = pd.read_pickle(f'{DATA_FEATURES_PATH}fe_dow_means_4_20_and_last_sale_day.pkl')\n    keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    df = df[keep_cols]\n    df.index = df.index + shift\n    df = df[df.index.isin(grid_df.index)]\n    grid_df = pd.concat([grid_df, df], axis=1)\n    del df\n    gc.collect()\n\n    ############################ Normal lags part 1 ############################\n    DATA_FEATURES_PATH = '..\/input\/m5-fe-basic-lags-part-1-oos-fixed\/'\n\n    df = pd.read_pickle(F'{DATA_FEATURES_PATH}oos_fe_lags_1_14.pkl')\n    keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    df = df[keep_cols]\n    df.index = df.index + shift\n    df = df[df.index.isin(grid_df.index)]\n    grid_df = pd.concat([grid_df, df], axis=1)\n    del df\n\n    # df = pd.read_pickle(F'{DATA_FEATURES_PATH}oos_fe_lags_15_28.pkl')\n    # keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    # df = df[keep_cols]\n    # df.index = df.index + shift\n    # df = df[df.index.isin(grid_df.index)]\n    # grid_df = pd.concat([grid_df, df], axis=1)\n    # del df\n\n    ##################################################################################\n    DATA_FEATURES_PATH = '..\/input\/m5-fe-oos-ipca-with-nan-part-4\/'\n\n    df = pd.read_pickle(F'{DATA_FEATURES_PATH}fe_pca5_oos_lags_15_84.pkl')\n    keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    df = df[keep_cols]\n    df.index = df.index + shift\n    df = df[df.index.isin(grid_df.index)]\n    grid_df = pd.concat([grid_df, df], axis=1)\n    del df\n    gc.collect()\n\n    ########################## Rolling window features ###########################\n    DATA_FEATURES_PATH = '..\/input\/m5-oos-fe-00-rolling-windows-basic-stats\/'\n\n    df = pd.read_pickle(F'{DATA_FEATURES_PATH}oos_fe_rw_basic_3_7.pkl')\n    keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    df = df[keep_cols]\n    df.index = df.index + shift\n    df = df[df.index.isin(grid_df.index)]\n    grid_df = pd.concat([grid_df, df], axis=1)\n    del df\n    gc.collect()\n\n    df = pd.read_pickle(F'{DATA_FEATURES_PATH}oos_fe_rw_basic_14_30.pkl')\n    keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    df = df[keep_cols]\n    df.index = df.index + shift\n    df = df[df.index.isin(grid_df.index)]\n    grid_df = pd.concat([grid_df, df], axis=1)\n    del df\n    gc.collect()\n\n    df = pd.read_pickle(F'{DATA_FEATURES_PATH}oos_fe_rw_basic_60_140.pkl')\n    keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    df = df[keep_cols]\n    df.index = df.index + shift\n    df = df[df.index.isin(grid_df.index)]\n    grid_df = pd.concat([grid_df, df], axis=1)\n    del df\n    gc.collect()\n\n    ##################### Shifted rolling window features ######################\n    DATA_FEATURES_PATH = '..\/input\/m5-fe-shift-rw-and-momentum\/'\n\n    df = pd.read_pickle(F'{DATA_FEATURES_PATH}fe_rw_shifts_and_momentum_7.pkl')\n    keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    df = df[keep_cols]\n    df.index = df.index + shift\n    df = df[df.index.isin(grid_df.index)]\n    grid_df = pd.concat([grid_df, df], axis=1)\n    del df\n    gc.collect()\n\n    df = pd.read_pickle(F'{DATA_FEATURES_PATH}fe_rw_shifts_and_momentum_30.pkl')\n    keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    df = df[keep_cols]\n    df.index = df.index + shift\n    df = df[df.index.isin(grid_df.index)]\n    grid_df = pd.concat([grid_df, df], axis=1)\n    del df\n    gc.collect()\n\n    df = pd.read_pickle(F'{DATA_FEATURES_PATH}fe_rw_shifts_and_momentum_60.pkl')\n    keep_cols = [col for col in list(df) if col not in drop_features_from_training]\n    df = df[keep_cols]\n    df.index = df.index + shift\n    df = df[df.index.isin(grid_df.index)]\n    grid_df = pd.concat([grid_df, df], axis=1)\n    del df\n    gc.collect()\n\n    grid_df.info()\n\n\n    ########################## Save predictions #####################################\n    # prediction_df.to_csv(f'preds_{model_name}.csv')\n    display(list(grid_df))\n    display(grid_df.shape)\n\n    ####################### test set #########################\n    ##########################################################\n    remove_features = ['id', 'd', TARGET, 'item_id']\n    feature_cols = [col for col in list(grid_df) if col not in remove_features]\n    test_mask = (grid_df.d == (START_TEST + test_day)) \n    test_x = grid_df[test_mask][feature_cols]\n\n    ############ train, valid and test masks #################\n    start_valid = (START_TEST + test_day) - 28\n    valid_mask = (grid_df.d == start_valid) \n    train_mask = (grid_df.d >= START_TRAIN) & (grid_df.d < start_valid) & (grid_df[TARGET].notnull())\n\n    ################## Fit custom functions ##################\n    w_12_train = w_12.reindex(grid_df[train_mask].id)\n    w_12_valid = w_12.reindex(grid_df[valid_mask].id)\n\n    ################## Objective #############################\n    wrmsse = get_wrmsse(w_12_train)\n\n    ######################### Metrics ########################\n    oos_rmsse_metric = get_oos_rmsse_metric(w_12_valid)\n\n    ################################################################################\n    remove_features = ['id', 'd', TARGET, 'item_id']\n\n    lgb_params = {\n                        'boosting_type': 'gbdt',                      \n                        'subsample': 0.5,\n                        'metric': 'None',\n                        'subsample_freq': 1,\n                        'learning_rate': 0.03,           \n                        'num_leaves': 2**8-1,            \n                        'min_data_in_leaf': 2**8-1,     \n                        'feature_fraction': 0.8,\n                        'n_estimators': 5000,            \n                        'early_stopping_rounds': 50,     \n                        'seed': SEED,\n                        'verbose': -1,\n                    } \n\n    ######################## Same as above with item_id ##########################\n    # I removed item_id from remove features\n    estimator, preds, test_x, test_mask, feature_cols = train_lgbm(grid_df,             # All data, train and valid\n               lgb_params,     \n               remove_features,\n               test_day = START_TEST + test_day,\n               TARGET='sales',\n               objective=None,\n               fobj=wrmsse,           # Custom objective function\n               feval=oos_rmsse_metric,          # Custom metric function\n               verbose_eval=25, \n               categories=None,     # Must use if categories are int type!!!!!\n               w_12 = w_12\n               )\n\n    prediction_df[f'F{test_day + 1}'] = preds\nprediction_df.to_csv(f'sub{test_day}.csv')","400c42fc":"### Updated: Link to [notebook](https:\/\/www.kaggle.com\/chrisrichardmiles\/m5-wrmsse-custom-objective-and-custom-metric?scriptVersionId=38218310) that shows how to to create custom loss and metric functions. \n\nAbove, I have linked to a notebook that shows how to get everything for the custom loss and custom metric, but I wanted to post my final notebook in its raw form. I am super proud of how far I have come in the past months, and I definitely needed the help of everyones notebooks and ideas. If you posted any notebooks or comments in this competition, I read them and used them. Thank you.\n\n### Highlights and defining characteristics\n* **Custom loss function** - discovered at 11:37 pm, with about 17 hours to go. \n* **Custom metric** - essentially an RMSSE function, but with the scales calculated with out of stock taken into account.\n* **Training and validation setup:** 28 models, trained for each prediction day.  For a training set, I only used days that were the same day of the week as prediciton day. This allowed me to use all the trianing data, and a good amount of features. For early stopping validation, I used a single day 28 days before the prediction day. So for predicting day 1942, validation day was 1914, and last training days were ...1893, 1900, 1907. The inspiration for this setup was from the winners of the grocery favorita competition and [their source code](https:\/\/www.kaggle.com\/shixw125\/1st-place-lgb-model-public-0-506-private-0-511). I studied this code heavily, and also used a lot of the same features. I was a bit worried about not using the last month for trianing, and only using a single validation day, but this grocery solution gave me the confidence that this approach was possible.\n\n### Most obvious area for improvement: LGBM parameter tuning\nI literally just used the parameters that were in [Custom validation](https:\/\/www.kaggle.com\/kyakovlev\/m5-custom-validation) by [Konstantin](https:\/\/www.kaggle.com\/kyakovlev). With 10 hours left in the competition, I was about to start testing new parameters but thought \"dude, you better start running this notebook or it might not run in time\", so I just left them.  I have no idea really about tuning the parameters, so maybe theres a big boost that could be made here. \n\nActually, I changed learning rate to .03 from .05, but also ran the .05 version in case the .03 version didn't finish in time. I did this because I think it is true that lower learning rate cannot make results less accurate, but just slower training. Also, I ran two copies of this notebook, one for days 1-14 and the other for 15-28. \n"}}