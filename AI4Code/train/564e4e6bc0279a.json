{"cell_type":{"58207a52":"code","24fc6e44":"code","8239cb1e":"code","bfb7f23a":"code","a65d1fea":"code","d6b14b72":"code","23183845":"code","73b83c27":"code","751c49da":"code","316e8b78":"code","6be39d92":"code","b2a5c18e":"code","80f9f6f1":"code","f1936dd6":"code","af46876f":"code","6d096259":"code","8d2e5560":"code","2ec33f90":"code","51104add":"code","357f2857":"code","dd5aa125":"code","b94fca33":"code","3ccb81b8":"code","9952f3c0":"code","5f402a65":"code","f9581552":"code","7ec53473":"code","cbdc2e17":"code","50958c02":"code","42e8e9c7":"markdown","f33480fd":"markdown","a1718d1e":"markdown","269ee631":"markdown","c181e178":"markdown","b677d402":"markdown","5b189aa8":"markdown","9115f031":"markdown","3974ec2e":"markdown","e6afc629":"markdown","b5086e27":"markdown","0f543e6d":"markdown","a78170b4":"markdown","7b93d1d9":"markdown","4125dc00":"markdown","dac6bd7b":"markdown","69664eab":"markdown","df79e295":"markdown","545a06cd":"markdown","f3743cd3":"markdown","ee38f98f":"markdown","37f1c5e2":"markdown","87c72335":"markdown","263cea9e":"markdown","6d98b933":"markdown","b7f921ca":"markdown","ea76b366":"markdown"},"source":{"58207a52":"# Import of the useful libraries\nimport re\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\n\ntry:\n    # The following commands are to set the tpu\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master()) # Device: grpc:\/\/10.0.0.2:8470\n    tf.config.experimental_connect_to_cluster(tpu) # connection to the cloud\n    tf.tpu.experimental.initialize_tpu_system(tpu) # to initialize TPU\n    strategy = tf.distribute.experimental.TPUStrategy(tpu) # TPU is the optimal strategy\nexcept:\n    strategy = tf.distribute.get_strategy() # <tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x7fdfe9c18610>\nprint('Number of replicas:', strategy.num_replicas_in_sync) # When TPU works, it has 8 replicas\n    \nprint(tf.__version__)\n","24fc6e44":"# Defunution of the Global Variables\nAUTOTUNE = tf.data.experimental.AUTOTUNE # Optimitazion function # -1\nGCS_PATH = KaggleDatasets().get_gcs_path() # It comes out: gs:\/\/kds-a5e4cea8ad5ada2a7ddf4f4354e68ae67307221af80333cee9f55152\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync # When GPU is used, batch_size is fixed at 16\nIMAGE_SIZE = (180, 180)\nEPOCHS = 25","8239cb1e":"filenames = tf.io.gfile.glob(str(GCS_PATH + '\/chest_xray\/train\/*\/*')) # a list with the names of the Images of the Training Set is created\n# Example filenames[0]\n# gs:\/\/kds-a5e4cea8ad5ada2a7ddf4f4354e68ae67307221af80333cee9f55152\/chest_xray\/train\/NORMAL\/IM-0115-0001.jpeg\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH + '\/chest_xray\/val\/*\/*'))) # Validation Set is concatenated\n\ntrain_filenames, val_filenames = train_test_split(filenames, test_size=0.2) \n# Validation and Train are splitted in 80:20 ratio\n\n# Note: the division is not made in a stratified way!","bfb7f23a":"# Number of images for each class in the Training set.\nCOUNT_NORMAL = len([filename for filename in train_filenames if \"NORMAL\" in filename])\nprint(\"Normal images count in training set: \" + str(COUNT_NORMAL))\n\nCOUNT_PNEUMONIA = len([filename for filename in train_filenames if \"PNEUMONIA\" in filename])\nprint(\"Pneumonia images count in training set: \" + str(COUNT_PNEUMONIA))","a65d1fea":"# DATASET is created by splitting the names' lists into Train and Validation Set\ntrain_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames) # <TensorSliceDataset shapes: (), types: tf.string>\nval_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)\n\n# The first 5 are printed\nfor f in train_list_ds.take(5):\n    print(f.numpy())","d6b14b72":"# Number of images in Training & Validation sets.\nTRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy() # Returns the cardinality of the dataset\nprint(\"Training images count: \" + str(TRAIN_IMG_COUNT))\n\nVAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()\nprint(\"Validating images count: \" + str(VAL_IMG_COUNT))","23183845":"# Classes names are taken\n# item: variable which is stored in the following form (without extension in the end)\n# gs:\/\/kds-a5e4cea8ad5ada2a7ddf4f4354e68ae67307221af80333cee9f55152\/chest_xray\/train\/NORMAL \n# os.path.sep = \/\n# until [-1]= tf.Tensor(b'NORMAL', shape=(), dtype=string)\n# until  .numpy()= b'NORMAL'\n# Finally an array is used\nCLASS_NAMES = np.array([str(tf.strings.split(item, os.path.sep)[-1].numpy())[2:-1]\n                        for item in tf.io.gfile.glob(str(GCS_PATH + \"\/chest_xray\/train\/*\"))])\n\nCLASS_NAMES","73b83c27":"# Using the filepath given as an input, this function returns\n# 1 if \"PNEUMONIA\", 0 if \"NORMAL\"\ndef get_label(file_path):\n    # convert the path to a list of path components\n    parts = tf.strings.split(file_path, os.path.sep) # file_path is splitted using the separator \"\/\"\n    # The second to last is the class-directory\n    return parts[-2] == \"PNEUMONIA\" # returns 1 if \"PNEUMONIA\", 0 if \"NORMAL\"","751c49da":"# The string is turned into a resized and rescaled image\ndef decode_img(img):\n  # convert the compressed string to a 3D unit tensor a 3 canali.\n  img = tf.image.decode_jpeg(img, channels=3)\n  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n  img = tf.image.convert_image_dtype(img, tf.float32)\n  # resize the image to the desired size.\n  return tf.image.resize(img, IMAGE_SIZE)","316e8b78":"def process_path(file_path):\n    # file_path will be train_list_ds and val_list_ds\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    # the string is turned into a resized and rescaled image\n    img = decode_img(img)\n    return img, label","6be39d92":"# The new-built datasets (for Training and Validation) have the following structure: (image, label)\n# All the images are uploaded\ntrain_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE) # <ParallelMapDataset shapes: ((180, 180, 3), ()), types: (tf.float32, tf.bool)>\nval_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)","b2a5c18e":"# train_ds.take(1)= <TakeDataset shapes: ((180, 180, 3), ()), types: (tf.float32, tf.bool)>\nfor image, label in train_ds.take(1):\n    print(\"Image shape: \", image.numpy().shape)\n    print(\"Label: \", label.numpy())","80f9f6f1":"test_list_ds = tf.data.Dataset.list_files(str(GCS_PATH + '\/chest_xray\/test\/*\/*'))\nTEST_IMAGE_COUNT = tf.data.experimental.cardinality(test_list_ds).numpy()\n# With the Test Set, mages are mapped exactly as in the previous cell: (image (formatted), label)\ntest_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n# Test Set's elements are combined in batch of BATCH_SIZE dimension\ntest_ds = test_ds.batch(BATCH_SIZE)\n\nTEST_IMAGE_COUNT","f1936dd6":"# This function arranges the Dataset for the Training phase\ndef prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n    # This is a small dataset, only load it once, and keep it in memory.\n    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n    # fit in memory.\n    if cache:\n        if isinstance(cache, str):\n            ds = ds.cache(cache)\n        else:\n            ds = ds.cache()\n            \n    # ds is mixed \n    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n\n    # Repeat forever\n    ds = ds.repeat()\n    \n    # the dimension of the batch is introduced\n    ds = ds.batch(BATCH_SIZE)\n\n    # 'prefetch' allows the dataset to retrieve the batches (which are in background) while the model is being trained\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n\n    return ds","af46876f":"# Note: this cell takes a long time\ntrain_ds = prepare_for_training(train_ds) # <PrefetchDataset shapes: ((None, 180, 180, 3), (None,)), types: (tf.float32, tf.bool)>\nval_ds = prepare_for_training(val_ds)\n\n# iter() function returns an iterator for the given object.\n# iter(train_ds)= <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fdf38118cd0>\nimage_batch, label_batch = next(iter(train_ds))\n# image_batch:\n# tf.Tensor(\n# [[[[2.23529428e-01 2.23529428e-01 2.23529428e-01]\n#    [2.31372565e-01 2.31372565e-01 2.31372565e-01]...]]], shape=(128, 180, 180, 3), dtype=float32)\n\n# label_batch:\n# tf.Tensor(\n# [ True False  True  True  True...], shape=(128,), dtype=bool)\n\n\n# 128 is the MINIBATCHES' saccording to this setting\n\n","6d096259":"# Inputs: .numpy() of (image, label) connected to the first batch\ndef show_batch(image_batch, label_batch):\n    plt.figure(figsize=(10,10))\n    for n in range(15):\n        ax = plt.subplot(5,5,n+1) # nrows, ncols, index\n        plt.imshow(image_batch[n])\n        # Note: label_batch is a boolean variable\n        if label_batch[n]:\n            plt.title(\"PNEUMONIA\")\n        else:\n            plt.title(\"NORMAL\")\n        plt.axis(\"off\")","8d2e5560":"show_batch(image_batch.numpy(), label_batch.numpy())","2ec33f90":"def conv_block(filters):\n    block = tf.keras.Sequential([\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'), \n        # The SeparableConv2D is a variation of the traditional convolution that was proposed to compute it faster. \n        # It performs a depthwise spatial convolution followed by \n        # a pointwise convolution which mixes together the resulting output channels. \n        # MobileNet, for example, uses this operation to compute the convolutions faster.\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPool2D()\n    ]\n    )\n    \n    return block","51104add":"def dense_block(units, dropout_rate):\n    block = tf.keras.Sequential([\n        tf.keras.layers.Dense(units, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(dropout_rate)\n    ])\n    \n    return block","357f2857":"def build_model():\n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n        \n        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n        tf.keras.layers.MaxPool2D(),\n        \n        conv_block(32),\n        conv_block(64),\n        \n        conv_block(128),\n        tf.keras.layers.Dropout(0.2),\n        \n        conv_block(256),\n        tf.keras.layers.Dropout(0.2),\n        \n        tf.keras.layers.Flatten(),\n        dense_block(512, 0.7),\n        dense_block(128, 0.5),\n        dense_block(64, 0.3),\n        \n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    return model","dd5aa125":"initial_bias = np.log([COUNT_PNEUMONIA\/COUNT_NORMAL])\ninitial_bias #array([1.05106674])","b94fca33":"# Images belonging to the less numerous class are given an higher weight\nweight_for_0 = (1 \/ COUNT_NORMAL)*(TRAIN_IMG_COUNT)\/2.0 \nweight_for_1 = (1 \/ COUNT_PNEUMONIA)*(TRAIN_IMG_COUNT)\/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","3ccb81b8":"# 'with strategy.scope()' is needed to make the code run in the case of multiple GPU\/TPU \nwith strategy.scope():\n    model = build_model()\n\n    METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=METRICS\n    )","9952f3c0":"history = model.fit(\n    train_ds,\n    steps_per_epoch=TRAIN_IMG_COUNT \/\/ BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=val_ds,\n    validation_steps=VAL_IMG_COUNT \/\/ BATCH_SIZE,\n    class_weight=class_weight,\n)","5f402a65":"# Callbacks definition\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"xray_model.h5\",\n                                                    save_best_only=True)\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n                                                     restore_best_weights=True)","f9581552":"def exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1 **(epoch \/ s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(0.01, 20)\n\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n# At the beginning of every epoch, this callback gets the updated learning rate value from exponential_decay_fn function \n# with the current epoch and current learning rate, and applies the updated learning rate on the optimizer.","7ec53473":"history = model.fit(\n    train_ds,\n    steps_per_epoch=TRAIN_IMG_COUNT \/\/ BATCH_SIZE,\n    epochs=100,\n    validation_data=val_ds,\n    validation_steps=VAL_IMG_COUNT \/\/ BATCH_SIZE,\n    class_weight=class_weight,\n    callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler]\n)","cbdc2e17":"fig, ax = plt.subplots(1, 4, figsize=(20, 3))\nax = ax.ravel() # Flatten the 2D_numpy_array\n\nfor i, met in enumerate(['precision', 'recall', 'accuracy', 'loss']): \n    # enumerate:  adds a counter to an iterable and returns it (the enumerate object).\n    ax[i].plot(history.history[met])\n    ax[i].plot(history.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])","50958c02":"loss, acc, prec, rec = model.evaluate(test_ds)","42e8e9c7":"# 6. Train the model\n\nSince there are only two possible labels for the image, we will be using the `binary_crossentropy` loss. When we fit the model, identify the class weights. Because we are using a TPU, training will be relatively quick.\n\nFor our metrics, we want to include precision and recall as they will provide use with a more informed picture of how good our model is. Accuracy tells us what fractions are the labels are correct. Since our data is not balanced, accuracy might give a skewed sense of a good model (i.e. a model that always predicts PNEUMONIA will be 74% accurate but is not a good model).\n\nPrecision is the number of true positives (TP) over the sum of TP and false positives (FP). It shows what fraction of labeled positives are actually correct.\n\nRecall is the number of TP over the sum of TP and false negatves (FN). It shows what fraction of actual positives are correct.","f33480fd":"We see that our accuracy on our test data is 83%, which is lower than the accuracy for our validating set. This may indicate overfitting. Try finetuning the model further to decrease overfitting to the training and validation sets.\n\nOur recall is greater than our precision, indicating that almost all pneumonia images are correctly identified but some normal images are falsely identified. We should aim to increase our precision.","a1718d1e":"As the method takes in numpy arrays as its parameters, call the numpy function on the batches to return the tensor in numpy array form.","269ee631":"Call the next batch iteration of the training data.","c181e178":"Define the method to show the images in the batch.","b677d402":"Currently our dataset is just a list of filenames. We want to map each filename to the corresponding (image, label) pair. The following methods will help us do that.\n\nAs we only have two labels, we will rewrite the label so that `1` or `True` indicates pneumonia and `0` or `False` indicates normal.","5b189aa8":"The weight for class `0` (Normal) is a lot higher than the weight for class `1` (Pneumonia). Because there are less normal images, each normal image will be weighted more to balance the data as the CNN works best when the training data is balanced.","9115f031":"Run the following cell to see how many images we have in our training dataset and how many images we have in our validation set. Verify that the ratio of images is 80:20.","3974ec2e":"The images originally have values that range from [0, 255]. CNNs work better with smaller numbers so we will scale this down.","e6afc629":"# 2. Load the data\n\nThe Chest X-ray data we are using from [*Cell*](https:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia) divides the data into train, val, and test files. There are only 16 files in the validation folder, and we would prefer to have a less extreme division between the training and the validation set. We will append the validation files and create a new split that resembes the standard 80:20 division instead.","b5086e27":"As expected, we have two labels for our images.","0f543e6d":"Load and format the test data as well.","a78170b4":"From exploring the data and the model, I noticed that the training for the model has a slow start. However, after 25 epochs, the model slowly starts to converge.","7b93d1d9":"# 9. Predict and evaluate results\n\nLet's evaluate the model on our test data!","4125dc00":"Let's visualize the shape of an (image, label) pair.","dac6bd7b":"# 5. Correct for data imbalance\n\nWe saw earlier in this notebook that the data was imbalanced, with more images classified as pneumonia than normal. We will correct for that in this following section.","69664eab":"# 1. Introduction + Set-up\n\nMachine learning has a phenomenal range of applications, including in health and diagnostics. This tutorial will explain the complete pipeline from loading data to predicting results, and it will explain how to build an X-ray image classification model from scratch to predict whether an X-ray scan shows presence of pneumonia. This is especially useful during these current times as COVID-19 is known to cause pneumonia.\n\nThis tutorial will explain how to utilize TPUs efficiently, load in image data, build and train a convolution neural network, finetune and regularize the model, and predict results. Data augmentation is not included in the model because X-ray scans are only taken in a specific orientation, and variations such as flips and rotations will not exist in real X-ray images. For a tutorial on image data augmentation, check out this [tutorial](https:\/\/www.kaggle.com\/amyjang\/tensorflow-data-augmentation-efficientnet).\n\nRun the following cell to load the necessary packages. Make sure to change the Accelerator on the right to `TPU`.\n\nCloud TPU enables you to run your machine learning workloads on Google\u2019s TPU accelerator hardware using TensorFlow\n\n\nTrain:\n1341 N VS 3875 PNEU\n\nVal:\n8 N VS 8 PNEU\n\nTest: \n243 N VS 390 PNEU\n\nTotale:\n1592 N VS 4273 PNEU\n\n","df79e295":"# 3. Visualize the dataset\n\nFirst, let's use buffered prefetching so we can yield data from disk without having I\/O become blocking.","545a06cd":"The following method will define the function to build our model for us. The Dropout layers are important as they \"drop out,\" hence the name, certain nodes to reduce the likelikhood of the model overfitting. We want to end the model with a Dense layer of one node, as this will be the output that determines if an X-ray shows an image of pneumonia.","f3743cd3":"# 8. Visualizing model performance\n\nLet's plot the model accuracy and loss for the training and the validating set. These plots show the accuracy and loss values for the second round of training. Since we initially trained the model with 30 epochs, these would be epochs 31-45. Note that no random seed is specified for this notebook. For your notebook, there might be slight variance.","ee38f98f":"# 7. Finetune the model\n\nFinetuning is an art when it comes to Machine Learning, and there are many ways to adjust the model in efforts to improve it. Finetuning is beyond the scope of this notebook, but check out this [article](https:\/\/www.pyimagesearch.com\/2019\/06\/03\/fine-tuning-with-keras-and-deep-learning\/) for more information.\n\nFor our purposes, we'll use Keras callbacks to further finetune our model. The checkpoint callback saves the best weights of the model, so next time we want to use the model, we do not have to spend time training it. The early stopping callback stops the training process when the model starts becoming stagnant, or even worse, when the model starts overfitting. Since we set `restore_best_weights` to `True`, the returned model at the end of the training process will be the model with the best weights (i.e. low loss and high accuracy).","37f1c5e2":"We also want to finetune our learning rate. Too high of a learning rate will cause the model to diverge. Too small of a learning rate will cause the model to be too slow. We implement the exponential learning rate scheduling method below.","87c72335":"We need a Google Cloud link to our data to load the data using a TPU. While we're at it, we instantiate constant variables. It is generally better practice to use constant variables instead of hard-coding numbers.","263cea9e":"We see that the accuracy for our model is around 98%. Finetune the model further to see if we can achieve a higher accuracy.","6d98b933":"Run the following cell to see how many healthy\/normal chest X-rays we have and how many pneumonia chest X-rays we have in the Training set.","b7f921ca":"# 4. Build the CNN\n\nTo make our model more modular and easier to understand, let's define some blocks. As we're building a convolution neural network, we'll create a convolution block and a dense layer block.\n\nThe architecture for this CNN has been inspired by this [article](https:\/\/towardsdatascience.com\/deep-learning-for-detecting-pneumonia-from-x-ray-images-fc9a3d9fdba8).","ea76b366":"Notice that the there are way more images that are classified as pneumonia than normal. This shows that we have a imbalance in our data. We will correct for this imbalance later on in our notebook."}}