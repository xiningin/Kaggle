{"cell_type":{"1d6ce9ea":"code","28c77494":"code","1cea9cff":"code","a0e761c0":"code","ec43d951":"code","dfa5a033":"code","8622bf54":"code","b79d1b54":"code","d31b0271":"code","32ddc002":"code","f01014b1":"code","9e9e9c4f":"code","dacf9cf1":"code","8169635f":"code","963c5f4a":"code","1f869192":"code","f24c57f2":"code","01db5972":"code","5e7c5774":"code","915dcb00":"code","5d86b032":"code","845f22ae":"code","4b918f13":"code","7270cb82":"code","5e9d7c22":"code","8d7d5386":"code","b44f5687":"markdown","f8e3d671":"markdown","5401465e":"markdown","336fed80":"markdown","1ae202a7":"markdown","844a4f0b":"markdown","7e6fcfcb":"markdown","c54bfb0f":"markdown","3dc0822c":"markdown","20b462b0":"markdown","71307431":"markdown","c72a777f":"markdown","90de328f":"markdown","0c0fa15b":"markdown","1f9b5b35":"markdown","ea4c6c85":"markdown","e733db9b":"markdown","ccfe95bb":"markdown","7e28d277":"markdown","bb0a6607":"markdown","afb1396e":"markdown","e890ba7b":"markdown","ba348145":"markdown","7f1650b7":"markdown","9b16ef0a":"markdown"},"source":{"1d6ce9ea":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom scipy.signal import resample\nimport random\nnp.random.seed(42)\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\n\ntorch.manual_seed(42)\ntorch.backends.cudnn.benchmark = False","28c77494":"params = {'legend.fontsize': 'x-large',\n          'figure.figsize': (10, 5),\n         'axes.labelsize': 'x-large',\n         'axes.titlesize':'x-large',\n         'xtick.labelsize':'x-large',\n         'ytick.labelsize':'x-large',\n         'axes.grid':True,\n         'axes.grid.which':'both'\n         }\nplt.rcParams.update(params)","1cea9cff":"df = pd.read_csv(\"..\/input\/mitbih_train.csv\", header=None)\ndf2 = pd.read_csv(\"..\/input\/mitbih_test.csv\", header=None)\n# df = pd.concat([df, df2], axis=0)\ndf = df.reset_index().drop('index', axis='columns')","a0e761c0":"df.describe().T","ec43d951":"label_names = {0 : 'N',\n              1: 'S',\n              2: 'V',\n              3: 'F',\n              4 : 'Q'}","dfa5a033":"df.info()","8622bf54":"# show some data\nfor _ in range(3):\n    plt.figure()\n    sample = df.sample(1).values.flatten()\n    y = label_names[sample[-1]]\n    plt.plot(sample[:-1])\n    t = plt.title(y)","b79d1b54":"label=df[187].value_counts()\nlabel","d31b0271":"label.rename(label_names).plot.bar()","32ddc002":"label=df2[187].value_counts()\nlabel","f01014b1":"label.rename(label_names).plot.bar()","9e9e9c4f":"def stretch(x):\n    l = int(187 * (1 + (random.random()-0.5)\/3))\n    y = resample(x, l)\n    if l < 187:\n        y_ = np.zeros(shape=(187, ))\n        y_[:l] = y\n    else:\n        y_ = y[:187]\n    return y_\n\ndef amplify(x):\n    alpha = (random.random()-0.5)\n    factor = -alpha*x + (1+alpha)\n    return x*factor\n\n\nclass Stretch:\n    def __init__(self):\n        pass\n    \n    def __call__(self,x):\n        return stretch(x)\n    \n    def __repr__(self):\n        return 'Stretch'\n    \nclass Amplify:\n    def __init__(self):\n        pass\n    \n    def __call__(self,x):\n        return amplify(x)\n    \n    def __repr__(self):\n        return 'Amplify'\n\nclass Augment:\n    def __init__(self, augmentation_list, return_prints = False):\n        self.augmentation_list = augmentation_list\n        self.return_prints = return_prints\n        \n    def __call__(self, x):\n        augmentations_performed = ''\n        \n        for augmentation in self.augmentation_list:\n            if np.random.binomial(1, 0.5) == 1:\n                x = augmentation(x)\n                augmentations_performed += f'{augmentation} '\n                \n        if not self.return_prints:\n            return x\n        return x, augmentations_performed\n\n","dacf9cf1":"augment = Augment([Amplify(), Stretch()], True)\nfor _ in range(5):\n    plt.figure()\n    sample = df.sample(1).values.flatten()\n    y = label_names[sample[-1]]\n    plt.plot(sample[:-1])\n    \n    augmented, augmentations_performed = augment(sample[:-1])\n    plt.plot(augmented[:-1])\n    \n    title = plt.title(y + ' ' + augmentations_performed)","8169635f":"from sklearn.model_selection import train_test_split\n\n# train_df, test_val_df = train_test_split(df, train_size=0.9, stratify=df.iloc[:,-1])\n# val_df, test_df = train_test_split(test_val_df, test_size=0.5, stratify=test_val_df.iloc[:,-1])\n\ntrain_df, val_df = train_test_split(df, train_size=0.8, stratify=df.iloc[:,-1])\ntest_df = df2\n# Make sure the target distribution stays the same...\nfor temp_df in [train_df, val_df, test_df]:\n    print(temp_df.shape[0], temp_df.shape[0] \/ df.shape[0])\n    print(temp_df.iloc[:,-1].value_counts(normalize=True))\n    print('\\n')","963c5f4a":"col = train_df.groupby(187).size()\n(col \/ col.sum()).plot.bar()\ncol","1f869192":"non_normal_rows = train_df.loc[train_df.iloc[:,-1] != 0]\nnormal_rows = train_df.loc[train_df.iloc[:,-1] == 0].sample(5000)\n\ntrain_df = non_normal_rows.append(normal_rows)\n\ncol = train_df.groupby(187).size()\n(col \/ col.sum()).plot.bar()\ncol\n","f24c57f2":"class A(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.intro_bn = nn.BatchNorm1d(32)\n    \n        self.C11 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n        self.A11 = nn.ReLU()\n        self.C12 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n        self.A12 = nn.ReLU()\n        self.M11 = nn.MaxPool1d(kernel_size=5, stride=2)\n\n    def forward(self, x):\n        x = self.intro_bn(x)\n        C = x\n        x = self.C11(x)\n        x = self.A11(x)\n        x = self.C12(x)\n        x = x + C\n        x = self.A12(x)\n        x = self.M11(x)\n        \n        return x\n    \nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.conv_in = nn.Conv1d(1, 32, kernel_size=5)\n        \n        self.A_blocks = nn.ModuleList(A() for i in range(5))\n        \n        self.avg_pool = nn.AvgPool1d(2)\n        self.fc1 = nn.Linear(32,32)\n        self.acc1 = nn.ReLU()\n        self.fc2 = nn.Linear(32,5)\n        \n    def forward(self, x):\n        x = self.conv_in(x)\n        \n        for i in range(5):\n            x = self.A_blocks[i](x)\n        \n        x = self.avg_pool(x)\n        x = x.view(x.shape[0], -1)\n        x = self.fc1(x)\n        x = self.acc1(x)\n        x = self.fc2(x)\n        \n        return x\n    ","01db5972":"batch_size = 256\nepochs = 10 # 75\nlr = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nn_splits = 7","5e7c5774":"from torch.utils.data import Dataset, DataLoader\n\nclass ds(Dataset):\n  def __init__(self, x, y=None, transforms=None):\n    super().__init__()\n\n    self.X = x\n    self.Y = y\n    self.transforms = transforms\n\n  def __len__(self):\n    return self.X.shape[0]\n\n  def __getitem__(self,idx):\n    x = self.X.iloc[idx,:]\n    \n    if self.transforms is not None:\n        x = self.transforms(x)\n\n    if self.Y is not None:\n      return torch.Tensor(x).view(1,-1).float(), torch.Tensor([self.Y.iloc[idx]]).float().squeeze()\n\n    return torch.Tensor(x).float()\n\naugment = Augment([Amplify(), Stretch()])\n\ntrain_set = ds(train_df.iloc[:,:-1], train_df.iloc[:,-1], transforms=augment)\nval_set = ds(val_df.iloc[:,:-1], val_df.iloc[:,-1])\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=batch_size*4)","915dcb00":"# model = Model().to(device)\n\ncriterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n# lr_sched = ReduceLROnPlateau(optimizer, patience=3) # None for a manual LR regimen","5d86b032":"epoch_train_losses = []\nepoch_val_losses = []\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=n_splits)\nfor fold_n, (train_idx, val_idx) in enumerate(kf.split(df)):\n    train_set = ds(df.iloc[train_idx,:-1], df.iloc[train_idx,-1], transforms=augment)\n    val_set = ds(df.iloc[val_idx,:-1], df.iloc[val_idx,-1])\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size*4)\n    \n    model = Model().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    lr_sched = ReduceLROnPlateau(optimizer, patience=3)\n    for epoch in range(epochs):\n        epoch_train_loss = 0\n        epoch_val_loss = 0\n\n        model.train()\n        for batch_id, (x,y_true) in enumerate(train_loader):\n            y_pred = model(x.to(device))\n\n            optimizer.zero_grad()\n            loss = criterion(y_pred, y_true.long().to(device))\n            loss.backward()\n            optimizer.step()\n\n            epoch_train_loss += loss.item() \/ len(train_loader)\n\n        model.eval()\n        with torch.no_grad():\n            for batch_id, (x,y_true) in enumerate(val_loader):\n                y_pred = model(x.to(device))\n                loss = criterion(y_pred, y_true.long().to(device))\n\n                epoch_val_loss += loss.item() \/ len(val_loader)\n\n\n        epoch_train_losses.append(epoch_train_loss)\n        epoch_val_losses.append(epoch_val_loss)\n        print(f'Fold {fold_n} Epoch {epoch}:\\tTrain loss: {epoch_train_loss:0.2e}\\tVal loss: {epoch_val_loss:0.2e} \\tLR: {optimizer.param_groups[0][\"lr\"]:0.2e}')\n\n        if lr_sched is None:\n            if epoch % 10 == 0 and epoch > 0:\n                optimizer.param_groups[0]['lr'] \/= 10\n                print(f'Reducing LR to {optimizer.param_groups[0][\"lr\"]}')\n        else:\n            lr_sched.step(epoch_val_loss)\n    torch.save(model.state_dict(), f'model_fold_{fold_n}.pth')\n","845f22ae":"plt.plot(np.arange(len(epoch_train_losses)), epoch_train_losses)\nplt.plot(np.arange(len(epoch_val_losses)), epoch_val_losses)\nplt.legend(['training loss','validation loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.yscale('log')","4b918f13":"predictions = []\nlabels = []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch_id, (x,y_true) in enumerate(val_loader):\n        y_pred = model(x.to(device)).argmax(1)\n        \n        predictions.extend(y_pred.squeeze().tolist())\n        labels.extend(y_true.squeeze().tolist())\n        \nprediction_labels = pd.DataFrame(np.array([predictions, labels]).T, columns=['Prediction', 'Label'])\ncm = prediction_labels.groupby(['Prediction', 'Label']).size()\ncm.unstack(0)","7270cb82":"print(classification_report(predictions, labels))","5e9d7c22":"test_set = ds(test_df.iloc[:,:-1], test_df.iloc[:,-1])\ntest_loader = DataLoader(test_set, batch_size=batch_size*4, shuffle=False)\n\npredictions = []\n\nmodel.eval()\nwith torch.no_grad():\n    for fold in range(n_splits):\n        model = Model().to(device)\n        model.load_state_dict(torch.load(f'model_fold_{fold}.pth'))\n        model.eval()\n        \n        fold_predictions = []\n        labels = []\n        for batch_id, (x,y_true) in enumerate(test_loader):\n            y_pred = model(x.to(device)).argmax(1)\n\n            fold_predictions.extend(y_pred.squeeze().tolist())\n            labels.extend(y_true.squeeze().tolist())\n        predictions.append(fold_predictions)\n    \npredictions.append(labels)\nall_prediction_labels = pd.DataFrame(\n    np.array(predictions).T, \n    columns=[f'fold {n}' for n in range(n_splits)] + ['Label']\n#     columns=['fold 0', 'fold 1', 'fold 2','fold 3','fold 4', 'Label'],\n#     columns=['fold 0', 'fold 1', 'fold 3', 'Label']\n)\n\nfinal_predictions = all_prediction_labels.iloc[:,:-1].mode(1).iloc[:,0]\nprediction_labels = pd.DataFrame(\n    np.hstack([\n        final_predictions.values.reshape(-1,1), \n        all_prediction_labels.loc[:,'Label'].values.reshape(-1,1)]\n    ),\n    columns=['Prediction', 'Label']\n)\nprediction_labels\n\ncm = prediction_labels.groupby(['Prediction', 'Label']).size()\ncm.unstack(0)","8d7d5386":"predictions = prediction_labels.loc[:,'Prediction'].values\nlabels = prediction_labels.loc[:,'Label'].values\nprint(classification_report(predictions, labels))","b44f5687":"![image.png](attachment:image.png)","f8e3d671":"## Distribution of labels","5401465e":"### Undersample the over-represented\nThere is a huge imbalance. The model can easily diminish the loss by ignoring everything other than class 0.\nThere are several ways to handle this - oversample, undersample, bootstrapping etc.\nFirst, we look at the distribution of samples in the training set:","336fed80":"## Train Model\nAll that remains is to define the model (and transfer it to the right device), an optimizer and train.","1ae202a7":"For convenience, we define the recurring module as \"A\" and use it as a list of modules in the Model class.\nNow for the hyperparameters. These are chosen by the developer according to the performance on the validation set.","844a4f0b":"It is a good idea to look at the distribution of the data before working with it.","7e6fcfcb":"We take note of the following:\n- All the fields are between  0 and 1.\n- The last fields are mostly zeros.\n- count is always the same number. We deduce that there are no missing values.\n- The last row has a maximum of 4.... A look at the paper tells us that this is the target field. It is categorical with 4 categories:\n","c54bfb0f":"Notice the Use of 'cuda': this is the driver connecting us to the graphical processing unit (GPU). Moving a model to the GPU means that it uses the GPU memory and that any operation done will be done by the GPU (not the CPU). Any mathematical operation between two variables requires the two to be on the same device - RAM or GPU RAM.\n\nNow we define helper methods for data handling. These will be very helpfull to divide the data to batches, shuffle and randomize and do computations in parallel. Since this is very pytorch-specific, there is no need to dive into it.","3dc0822c":"Notice we decrease the learning rate throughout training. Learning rate modulation is an active area of research. \n\nLets see what we got.","20b462b0":"The following code will make graphs a bit easier to read and display.","71307431":"We should see the loss converge. The training set loss should be monotone (though it colud be not). Some amount of overfit is acceptable, but wometimes we need to use some ragularization (that was not used here).","c72a777f":"## Test the model - Validation set\n\nUp until now we discussed loss - not actual performane. Now is the time to consider accuracy. We first examine the different metrics on the validation set. ","90de328f":"For more detailed analysis of the categories, please refer to the paper.","0c0fa15b":"# ECG Classification\nThis notebook contain all the code necessary to handle data, define model and train it, and run tests. It uses data from MIT-BIH dataset (available online) and follows [1] for model definition and some hyper parameters selection.\n\nOne note is that notebooks are not intended for this purpose. It is well suited for short lines of code and displaying and transferring information between developers and users. For best results, define .py files and run experiments through console. \n\n[1] Kachuee, Mohammad, Shayan Fazeli, and Majid Sarrafzadeh. \"Ecg heartbeat classification: A deep transferable representation.\" 2018 IEEE International Conference on Healthcare Informatics (ICHI). IEEE, 2018.\n\n## Preliminary work\nWe first import all packages required for this code to work. As always, we use PyTorch.\nWe set manual randomization seeds to negate randomness between running. This makes random number generators predictable (but still random).\n","1f9b5b35":"Here we use Adam optimizer, which converges faster better than SGD. The learning rate is defined ahead.\n","ea4c6c85":"## Test the model - Test set","e733db9b":"## Build Model\nWe model the network from the article:","ccfe95bb":"In code, it looks like this:","7e28d277":"## Read Dataset\nThe default dataset comes in two sets - Training set and Testing set. We combine the two and redistribute to three sets.","bb0a6607":"## Split Dataset into Train-, Validation- and Test-set\n\nNext we devide the data to three sets - train set, validation set and test set - but we do so such that the probability of every class in each set is similar to the probability in the undivided set.","afb1396e":"Note the probabilities! ","e890ba7b":"Next we take the over represented class (N) and sample 5000 samples, randomly. These will be used for training and the rest are ignored.","ba348145":"Note we take special care to not accidently send the label to the model (column -1).","7f1650b7":"Lets look at the augmentations... In blue the original. In orange the augmented. It is always a good idea to know what the input is to the network exactly.","9b16ef0a":"## Data augmentation\n\nTo train properly the model, we sould have to augment all data to the same level. We define two augmentations and wrapper classes for easy use.\nNote that the augmentations are randomly random - so we get two levels of randomness in new input generation."}}