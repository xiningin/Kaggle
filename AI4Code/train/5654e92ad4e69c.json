{"cell_type":{"b8eef142":"code","6c5385dd":"code","57c20102":"code","a5e8ebc1":"code","1272c73c":"code","ac395188":"code","a52f55b2":"code","442e4515":"code","f22be25c":"code","b67a7283":"code","00a60663":"code","32ae9f45":"markdown","590f7b1d":"markdown","6f2ef198":"markdown","212af4b4":"markdown","f906a8c0":"markdown"},"source":{"b8eef142":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(style='darkgrid')\n\nfrom sklearn.feature_selection import RFECV, VarianceThreshold\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom ml_metrics import quadratic_weighted_kappa\nrandom_state = 42\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c5385dd":"train_df = pd.read_csv('\/kaggle\/input\/prudential-life-insurance-assessment\/train.csv.zip')\ntrain_df.set_index('Id', inplace=True)\ndisplay(train_df.head())\nX_train, y_train = train_df.iloc[:, :-1], train_df.iloc[:, -1]\nprint(X_train.shape)\nprint(y_train.shape)","57c20102":"ordinal_y_mean_dict = {}\n\n#transform the categorial columns into numeric\nfor col in X_train.select_dtypes(include='object').columns:\n    ordinal_y_mean_dict[col]  = {index:i for i, index in enumerate(train_df.groupby(col)['Response'].mean().sort_values().index)}\n    print(ordinal_y_mean_dict[col])\n    X_train[col] = X_train[col].map(ordinal_y_mean_dict[col])\n    \n#check missing data\nfor col in X_train:\n    if pd.isnull(X_train[col]).any():\n        print('containing NA values:', col)\n\nimputer = SimpleImputer(strategy='mean')\nX_train2 = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n#check missing data\nfor col in X_train2:\n    if pd.isnull(X_train2[col]).any():\n        print('containing NA values:', col)","a5e8ebc1":"X_train2_unsup  = X_train2.copy() #deep copy","1272c73c":"#Suppose you data now contains some missing data, you want to filter the data with very high missing rate.\nX_train2_unsup['dump'] = np.nan\nX_train2_unsup2 = X_train2_unsup.dropna(axis=1, thresh=0.8) #drop the columns with missing rate > 80%\nprint(X_train2_unsup.shape)\nprint(X_train2_unsup2.shape) #drop the demp column","ac395188":"selector = VarianceThreshold(0.7)\nselector.fit(X_train2_unsup2)\nX_train2_unsup3 = X_train2_unsup2[X_train2_unsup2.columns[selector.get_support(indices=True)]]\nprint('number of columns after dropping by variance threshold:', X_train2_unsup3.shape[1])","a52f55b2":"X_train2_sup = X_train2.copy() #deep copy","442e4515":"%%time\nX_model, X_valid, y_model, y_valid = train_test_split(X_train2_sup, y_train, stratify=y_train, random_state=random_state, test_size=.8)\n\nmodel_dict = {'LogisticRegression': LogisticRegression(penalty='l1', solver='saga', C=2, multi_class='multinomial', n_jobs=-1, random_state=random_state)\n             , 'ExtraTreesClassifier': ExtraTreesClassifier(n_estimators=200, max_depth=3, min_samples_leaf=.06, n_jobs=-1, random_state=random_state)\n              , 'RandomForestClassifier': RandomForestClassifier(n_estimators=20, max_depth=2, min_samples_leaf=.1, random_state=random_state, n_jobs=-1)\n             }\nestimator_dict = {}\nimportance_fatures_sorted_all = pd.DataFrame()\nfor model_name, model in model_dict.items():\n    print('='*10, model_name, '='*10)\n    model.fit(X_model, y_model)\n    print('Accuracy in training:', accuracy_score(model.predict(X_model), y_model))\n    print('Accuracy in valid:', accuracy_score(model.predict(X_valid), y_valid))\n    importance_values = np.absolute(model.coef_) if model_name == 'LogisticRegression' else model.feature_importances_\n    importance_fatures_sorted = pd.DataFrame(importance_values.reshape([-1, len(X_train2_sup.columns)]), columns=X_train2_sup.columns).mean(axis=0).sort_values(ascending=False).to_frame()\n    importance_fatures_sorted.rename(columns={0: 'feature_importance'}, inplace=True)\n    importance_fatures_sorted['ranking']= importance_fatures_sorted['feature_importance'].rank(ascending=False)\n    importance_fatures_sorted['model'] = model_name\n    print('Show top 10 important features:')\n    display(importance_fatures_sorted.drop('model', axis=1).head(10))\n    importance_fatures_sorted_all = importance_fatures_sorted_all.append(importance_fatures_sorted)\n    estimator_dict[model_name] = model\n\nplt.title('Feature importance ranked by number of features by model')\nsns.lineplot(data=importance_fatures_sorted_all, x='ranking', y='feature_importance', hue='model')\nplt.xlabel(\"Number of features selected\")","f22be25c":"selected_model = 'LogisticRegression'\nnumber_of_features = 60\nselect_features_by_model = importance_fatures_sorted_all[importance_fatures_sorted_all['model'] == selected_model].index[:number_of_features].tolist()","b67a7283":"%%time\n#it takes much more time comparing \nrfecv = RFECV(estimator=model_dict['LogisticRegression'].set_params(max_iter=150, C=1), step=1, cv=StratifiedShuffleSplit(1, test_size=.2, random_state=random_state), scoring='accuracy', n_jobs=-1)\nrfecv.fit(X_train2_sup[select_features_by_model], y_train)\nplt.figure()\nplt.title('Feature importance ranked by number of features by model')\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.plot(rfecv.n_features_, rfecv.grid_scores_[rfecv.n_features_-1], marker='o', label='Optimal number of feature')\nplt.legend(loc='best')\nplt.show()","00a60663":"rfecv_df = pd.DataFrame({'col': select_features_by_model})\nrfecv_df['rank'] = np.nan\nfor index, support in enumerate(rfecv.get_support(indices=True)):\n    rfecv_df.loc[support, 'rank'] = index\nfor index, rank in enumerate(rfecv.ranking_ -2):\n    if rank >= 0:\n        rfecv_df.loc[index, 'rank'] = rfecv.n_features_ + rank\nrfecv_df","32ae9f45":"## Drop by the missing rate","590f7b1d":"# 1. Unsupervised approach\nReference:\nhttps:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.feature_selection****","6f2ef198":"## Simple data cleansing","212af4b4":"## Drop by the variance","f906a8c0":"# 2. supervised approach"}}