{"cell_type":{"ef856420":"code","9929343e":"code","cd6cb9fc":"code","691d5dd2":"code","faf451b0":"code","b6ebc861":"code","55aa5654":"code","2a648c15":"code","e212bba5":"code","72ea1ba3":"code","aabd0391":"code","5bb666ed":"code","55599e54":"code","0d742235":"code","98b94da7":"code","01454600":"code","7a253e61":"code","9bdea5a8":"code","3b90dbaa":"code","2f7fc0a2":"code","3d1b9a76":"code","fea4af22":"code","0ed1efae":"code","f9d3e9d9":"code","1a4185fd":"code","a8a66ce9":"code","f472004c":"code","205bf64e":"markdown","922ef049":"markdown","59829786":"markdown","edfa3bbc":"markdown","8b70c7fa":"markdown","1cb87ddb":"markdown","f91d1c93":"markdown","f4a598b8":"markdown","a65cba20":"markdown","23778808":"markdown","2df8b6a6":"markdown","e75308fa":"markdown","024ebd37":"markdown","38e94665":"markdown","81d85ca6":"markdown","2e44d016":"markdown","9160155b":"markdown","e3ece996":"markdown","0fd0042d":"markdown","951c82e3":"markdown","34250512":"markdown","bb3a05a9":"markdown","9676bd11":"markdown","46786223":"markdown","f6a436b7":"markdown","c31f6bac":"markdown","daac7640":"markdown","d3b840c9":"markdown","3f7fe5dc":"markdown","e210d993":"markdown","fb92cd4f":"markdown","9bcf8ae2":"markdown","63c006b4":"markdown","e06ac562":"markdown","5f9bbdeb":"markdown","696955ae":"markdown","9931601e":"markdown","290eda30":"markdown","f17d54c0":"markdown"},"source":{"ef856420":"import numpy as np \nimport pandas as pd \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Graphics in SVG format are more sharp and legible\n%config InlineBackend.figure_format = 'svg'\n\n\nimport os\nprint(os.listdir(\"..\/input\/\"))","9929343e":"DATA_PATH = \"..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv\"","cd6cb9fc":"df = pd.read_csv(DATA_PATH)\ndf.info()","691d5dd2":"df.head()","faf451b0":"print(df['Attrition'].value_counts())\nsns.countplot(df['Attrition']);","b6ebc861":"# Filter Data\nnumerical, categorical = [], []\n\nfor col in df.columns:\n    if (df[col].dtype == 'O') or (len(df[col].value_counts()) < 10):\n        categorical.append(col)\n    else:\n        numerical.append(col)","55aa5654":"# Creating a correlation matrix\ncorr_matrix = df[numerical].corr()\n# Plotting heatmap\nsns.heatmap(corr_matrix);","2a648c15":"for col in categorical:\n    if len(df[col].value_counts()) < 2:\n        print(col)","e212bba5":"# Dropping the columns\ndf = df.drop(columns=['EmployeeCount', 'Over18','StandardHours'])\n# Updating our numerical features to not include the dropped columns\ncategorical = list(set(categorical) - set(['EmployeeCount', 'Over18','StandardHours']))","72ea1ba3":"categorical","aabd0391":"_, axes = plt.subplots(nrows=1, ncols=(len(categorical)), figsize=(200, 4))\nfor i,col in enumerate(categorical):\n    if col != 'Attrition':\n        sns.countplot(x=col, hue='Attrition', data=df, ax=axes[i])","5bb666ed":"df_processed = df.drop(columns=['Attrition'])","55599e54":"objects, numbers = [], []\n\nfor col in df_processed.columns:\n    if (df_processed[col].dtype == 'O'):\n        objects.append(col)\n    else:\n        numbers.append(col)","0d742235":"df_object = df_processed[objects]\ndf_object = pd.get_dummies(df_object)","98b94da7":"df_object.head()","01454600":"df_final = pd.concat([df_processed[numbers], df_object], axis=1)\ndf_final.head()","7a253e61":"target = df['Attrition'].map({'Yes': 1, 'No': 0})","9bdea5a8":"# Importing train-test split method from sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit","3b90dbaa":"train_X, test, train_y, val_y = train_test_split(df_final, target, train_size=0.9, random_state=0);","2f7fc0a2":"from imblearn.over_sampling import SMOTE","3d1b9a76":"oversampler = SMOTE(random_state=0)\nsmote_X, smote_y = oversampler.fit_sample(train_X, train_y)","fea4af22":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","0ed1efae":"params = {\n    'n_jobs': -1,\n    'n_estimators': 1000,\n    'max_features': 0.3,\n    'max_depth': 4,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'random_state' : 0,\n    'verbose': 0\n}","f9d3e9d9":"random_forest = RandomForestClassifier(**params)","1a4185fd":"random_forest.fit(smote_X, smote_y)","a8a66ce9":"predictions = random_forest.predict(test)","f472004c":"print(\"Accuracy = {}\".format(accuracy_score(val_y, predictions)))","205bf64e":"### Inspection of Categorical Data","922ef049":"### 4.3 Building a Model: The Random Forest Classifier\n\nA Random Forest is a type of machine learning method based on decision trees. Using the power of an ensemble of decision trees, a random forest can put together powerful learning algorithms for predicting data. \n\nFirst, we will import a Random Forest from sklearn and set up its parameters:","59829786":"Some observations from our plots above:\n\n- __Education Field__: Proportionally; Marketing, Technical Degree, and Human Resources employees have higher chances of leaving the company.\n- __Department__: Employees in the Sales department have the highest chance of leaving the company, roughly about a _25% chance_.\n- __Business Travel__: Those who travel frequently have a higher chance of leaving the company. Many of these employees could possibly be in the same group as the _Sales Department_, as sales positions usually require more travel.\n- __Stock Option Level__: The lower your stock option level, the higher chance you have of belonging to the attrition group.\n- __Work-Life Balance__: The minority group who rate poorly on Work-Life Balance have roughly a _50% chance_ of leaving IBM.\n- __Job Level__: The lower your job level, the higher chance you have of belonging to the attrition group. Seems to be highly correlated to _StockOptionLevel_.\n- __PerformanceRating__: Interestingly, there does not seem to be a higher probability of attrition for an employee who has a lower performance rating. It should be noted, however, that most employees who left the company do have lower performance ratings. Later we can take a second look at the ratios to confirm this.\n- __OverTime__: If an employee clocks in for overtime, they have a significant chance of eventually leaving the company. This seems to have a logical relationship with _WorkLifeBalance_ as well.\n- __JobRole__: With our current graph visualization, it is difficult to see the distribution of attrition per job role. We will take a look at this graph seperately.\n- __MaritalStatus__: Those employees who are _Single_ have a considerable chance of leaving IBM. It would be reasonable to assume that these individuals are also young who have a low _JobLevel_ and a low _StockOptionsLevel_. We can confirm this later with more exploration.\n- __TrainingTimesLastYear__: For those employees who engaged in some kind of training, they have a lower chance of leaving the company. However, those who did not engage in training seem to have a higher chance.\n- __RelationshipSatisfaction__: While not significant, those employees with a low relationship satisfaction tend to belong to the attrition group more than those who do not. This naturally goes hand-in-hand with _WorkLifeBalance_\n- __JobInvolvement__: Employees in the smallest group of Job Involvement have a significant chance of leaving IBM.\n\nWe just gained a lot of insight! While not a perfect picture, we can now form some hypothesis to determine the kinds of employees who leave IBM. ","edfa3bbc":"Our inference was correct! The columns have a single value across all employee entrees. Since these features are not providing us any further insight on attrition, we will drop them.\n\n__*Note on dropping features*__: The choice to get rid of valid data columns from your analysis should not be taken lightly. Usually, we need to ask practical and realistic questions around the features we are looking at: Will we receive more data in the future? If so, we could possible see varying data ranges in these columns which could affect our analysis. In this case, we know we will not receive any additiondal data, which is why we safely made the choice to get rid of these columns.","8b70c7fa":"Now that we have these forms of data seperates, let us take a further look into them and see if we need to do any cleaning or if there is some valuable information to be obtained","1cb87ddb":"__So no. Our dataset is inbalanced.__ Why do we care about this?\n\n1. Building machine learning predictions will be more difficult, as answers will be more biased towards 'no'.\n2. Our own inferences need to be carefully considered when looking for patterns in 'Yes' results for Attrition. We cannot mix up _causation_ for _correlation_","f91d1c93":"Now to initialize a random forest with the above parameters:","f4a598b8":"## __Defining our Objectives__\n\nWhile our data exploration will be largely open-ended, we should have some overall goals in mind as we tread into uncharted territory:\n\n- [x] Get a high-level understanding of our dataset and its features\n- [x] Conduct a multivariate analysis to look for patterns and corellations in the data\n- [x] Develop key insights on how the features affect an employee's attrition status\n- [x] Implement a machine learning model to see how effectively we can make predictions","a65cba20":"# __IBM Employee Attrition Study__\n\nThis notebook follows an exploration of the attrition of IBM Employees. Given a dataset of of employee information, can we make strong predictions wether they will stay at the company or not?","23778808":"Let us now split up our data:","2df8b6a6":"__SMOTE: Dealing with our inbalanced dataset__\n\nAs mentioned several times, our target data is inbalanced. A majority of the employees in the our data have not left IBM, making attrition easily a biased prediction to make for an algorithm\n\nThe Synthetic Minority Oversampling Technique, or SMOTE for short, will allow us to take this in account for our training data:","e75308fa":"Our work on this IBM Employee dataset was only the tip of the iceberg. We could choose to explore more deeply the relationships between certain features mentioned throughout our visualization process. We could also attempt to use other machine learning methods to achieve better accuracy scores, such as _Logistic Regression_. \n\nWith some initial data dissection, we developed key insights that could prove valuable not just to executives at IBM, but any employer intersted in determining their chances of keeping certain kinds of employees around in their companies. The Random Forest Classifier we put together could also serve as powerful tools for helping to recognize possible employee losses ahead of time to remedy their issues.\n\nThanks for joining me on this notebook and this exploration of the power of data science and machine learning for such a valuable topic of employee attrition :)","024ebd37":"### 4.2 Creating the Train-Test Split","38e94665":"## 0. Imports and Configurations","81d85ca6":"## __Afterword__","2e44d016":"In a supervised machine learning problem, such as this one, we have our input features, denoted by __X__, and a __target__ variable, usually denoted by __y__.\n\nWhat does this mean? Based on labeled information for each employee, we know ahead of time their _attrition_ status. In this case, since we are trying to predict _attrition_, that becomes our target variable. All other features in the data become our X.\n\nTo begin our ML work, we are going to create a duplicate dataframe and drop the _Attrition_ column, since this is what we will be attempting to predict:","9160155b":"### Inspection of Numerical Data\n\nA good starting point for our quantitaive data will be to create a heatmap that will look for correlations in our data. As stated above, we need to becareful in mixing up _causation_ with _correlation_, but it could provide some leads for further exploration\n\nWe have a lot of numerical columns to create correlations for, so its hard to visualize all of them in one grid. Instead, we can look at the majority first and see if there is anything peculiar about the data:","e3ece996":"For now, we will move forward with preparing our dataframe for machine learning predictions. This is actually a large process which we will need to break down into several smaller steps:\n\n1. _Features Engineering_: where we make our data easier to be processed for mathematical computation\n2. _Train-Test Data Split_: Divide our dataset into two sections, for training and testing ML models\n3. _Test_: Use different ML models and analyze the results","0fd0042d":"__Encoding our categorical object Features__\n\nTo easily process our data, we will transform our categorical object columns with a techqniue called __one hot encoding__. This will turn our categories into vector representations of data that a machine learning algorithm will be able to understand.\n\nAgain, we will filter out our columns that are object types verses the int types:","951c82e3":"__Thoughts So Far__\n\nWe are fortunate to work with a rather clean and straight-forward dataset. Some basic visualization techniques should help point out if we have any need for data cleaning","34250512":"### __Analysis__\n\nWe have 1,470 entries in our dataset and 35 columns\/features, including that of _Attrition_. \n\nOf the 35 features, most are __quantitative__ represented by the ```int64``` data type. The rest are __categorical__, but we can make some useful abstractions for most of them:\n\nFeatures such as _Arttrition_, _Gender_, _Over18_, and _OverTime_ represent __binary categorical__ data. These are easy to represent numerically as 1 or 0.\n\n_Business Travel_ is an object but also is __ordinal__, meaning it represents an ordered structure of data. We can make this inference because it represents 3 values of increasing amount in travel: _Non Travel_, _Travel Rarely_, and _Travel Frequently_.","bb3a05a9":"## 2. Preliminary Visualization and data cleaning","9676bd11":"The *get_dummies* method from Pandas will allow us to do one hot encoding","46786223":"__Fantastic!__ Given our basic feature engineering, we were able to build a model with close to __88% accuracy__ on identifying an employee's attrition status.","f6a436b7":"### Filtering Categorical and Numerical Features\n\n* We'll want to apply the right kind of visualizations depending on the type of feature, so its a good idea to create filters for them when accessing the dataframe:","c31f6bac":"Let us now see how some key features in our categorical set could affect attrition. Here is a refresher list for our categorical features:","daac7640":"And we can finally begin to fit our random forest to the training data:","d3b840c9":"## 1. Loading the Dataset and High-Level Overview\n\nOur first steps are to load in our csv file as a ```pandas``` DataFrame and to take a quick look at the entire dataset. We want to understand the features we will be working with as well as see how they are represented.","3f7fe5dc":"__Is this a balanced dataset?__\n\nOne of the first things that will be useful to understand right away is wether we have a balaned dataset. Essentially, we are checking if we gathered roughly equal data for both employees who have left the company and for those who have stayed. Lets take a quick look:","e210d993":"At the beginning of this visualization section, we already took a look at the distribution of our target categorical feature, _attrition_. We saw that we had an inbalanced dataset biased towards employees staying at the company. \n\nLets look at the distribution of data for our categorical features. Our first interest is to see if there are any columns where there is only 1 value. Why? If we find such a column, that would make that feature of no use to us: we don't know how it would impact _attrition_","fb92cd4f":"What makes an employee leave the company? As one could expect from the data, there is no one single decisive factor. However, we have spotted some key features that could play some influence in attrition. Mainly:\n\n- Does the employee have poor work-life balance? Do they work overtime, and have low relationship satisfaction?\n- Is the employee at a low-ranking position within the company, with little to no stock options?\n- Is the employee in a sales related job which contains much travel?\n- Is the employee young and single? If so, are they also in a low-ranking position?\n- Does the employee score low on Job Involement?\n\nHaving an individual answer _yes_ to one or many of these questions could potentially put them at very high risk of leaving the company. \n\n__*An Important Note on \"Attrition\":*__ While it may seem obvious we have not discussed the greatest weakness of our _Attrition_ metric, let us address it now. _Attrition_ could represent an individual _choosing_ to leave the company and could also represent them being _fired_ from the company. These two scenarios represent entirely different symptoms within an employee's data. With our questions on _Attrition_, we are making sure to cover both scenarios.","9bcf8ae2":"## __3. Key Insights__","63c006b4":"There are some logical conclusions we can draw from looking at our new heatmap:\n\n- _Monthly Income_, _Job Level_, and _TotalWorkingYears_ are strongly related. _Age_ is also related to _MonthlyIncome_.\n- _YearsWithCurrentManager_ is tied with _YearsWithCurrentRole_ and _YearsAtCompany_\n\nLater we can confirm wether or not they influence attrition by directly comparing features, as well as other categorical pieces of data.\n\nSpeaking of categorical data, lets move on to inspecting those features to be sure that we properly understand them.","e06ac562":"Fantastic! Now we have representations for our cateogircal data that an ML algorithm will be able to compute easily. Lets concat a final dataframe with our numerical values and categorical ones:","5f9bbdeb":"### 4.1 Features Engineering","696955ae":"## __4. Machine Learning Predictions__","9931601e":"__Setting Up the Target__\n\n_Attrition_ is defined by a string, 'Yes' or 'No'. As mentioned earlier in this study, we can transform binary features from their abstract concepts of 'Yes' or 'No' into 1 or 0. ","290eda30":"Even though we will be using the magic (actually it's just a lot of math) of machine learning to create our predictions, we will need a way to _test_ if we are creating accurate models! This is the purpose of the _train-test split_.","f17d54c0":"Our random forest has been fitted to the training data! Lets make some predictions based on the tests, and measure up how we compare to the actual results:"}}