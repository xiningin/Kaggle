{"cell_type":{"7820a198":"code","f1c4d196":"code","8f1bbcb0":"code","f3cdd64e":"code","83cd2955":"code","47003c16":"code","b44f7b33":"code","969b8dac":"code","ccd2f732":"code","3d3aa0f9":"code","00cc7744":"code","65abd706":"code","71091529":"code","97431104":"code","0f72c8ee":"code","c26f2cad":"code","027a386e":"code","563b2db1":"code","a9fedff1":"code","0a359a29":"code","05c1399e":"code","e6eb9a69":"code","fcebe3d7":"code","7bada0c2":"code","ba4a90d2":"code","5d42cd84":"code","6834caf7":"markdown"},"source":{"7820a198":"from __future__ import absolute_import, print_function, unicode_literals, division\nfrom builtins import input, range","f1c4d196":"import tensorflow as tf\nimport urllib","8f1bbcb0":"# data source: https:\/\/github.com\/Phylliida\/Dialogue-Datasets\nimport urllib; \nurllib.request.urlretrieve(\"https:\/\/raw.githubusercontent.com\/Phylliida\/Dialogue-Datasets\/master\/TwitterLowerAsciiCorpus.txt\", filename=\"TwitterLowerAsciiCorpus.txt\")","f3cdd64e":"with open('twitter_tab_format.txt', 'w') as f:\n  prev_line = None\n  for line in open('TwitterLowerAsciiCorpus.txt'):\n    line = line.rstrip()\n    if prev_line and line:\n      f.write(\"%s\\t%s\\n\" % (prev_line, line))\n    prev_line = line","83cd2955":"import os, sys\nimport gc\ngc.enable()\n\nimport numpy as np \nimport pandas as pd\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, GRU, Dense, Embedding, \\\nBidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import backend as K\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\")","47003c16":"def softmax_over_time(x):\n  assert(K.ndim(x)>2)\n  e = K.exp(x - K.max(x, axis=1, keepdims=True))\n  s = K.sum(e, axis=1, keepdims=True)\n  return e \/ s ","b44f7b33":"BATCH_SIZE = 128\nEPOCHS = 50\nLATENT_DIM = 512\n\nNUM_SAMPLES = 4500\nMAX_SEQUENCE_LEGTH = 75\nMAX_NUM_WORDS = 15000\nEMBEDDING_DIM = 50","969b8dac":"input_texts = []\ntarget_texts = []\ntarget_texts_inputs = []","ccd2f732":"#load in the data\nt = 0\nfor line in open('twitter_tab_format.txt'):\n  t+= 1\n  if t>NUM_SAMPLES:\n    print('Max Samples Exceeded. Breaking out of loop.')\n    break\n  if '\\t' not in line:\n    continue\n  input_text, translation = line.split('\\t')\n  target_text = translation + ' <eos>'\n  target_text_input = '<sos> ' + translation \n\n  input_texts.append(input_text)\n  target_texts.append(target_text)\n  target_texts_inputs.append(target_text_input)\nprint(\"num samples: \",len(input_texts))","3d3aa0f9":"tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer_inputs.fit_on_texts(input_texts)\ninput_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n\nword2idx_inputs = tokenizer_inputs.word_index\nprint('Found %s unique input tokens.' % len(word2idx_inputs))\n\nmax_len_input = max(len(s) for s in input_sequences)\n\ntokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\ntokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) \ntarget_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\ntarget_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n\nword2idx_outputs = tokenizer_outputs.word_index\nprint('Found %s unique output tokens.' % len(word2idx_outputs))\n\nnum_words_output = len(word2idx_outputs) + 1\n\nmax_len_target = max(len(s) for s in target_sequences)","00cc7744":"encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\nprint(\"encoder_data.shape:\", encoder_inputs.shape)\n","65abd706":"print(\"encoder_data[0]\")\nencoder_inputs[0]","71091529":"decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\nprint(\"decoder_data.shape:\", decoder_inputs.shape)","97431104":"print(\"decoder_data[0]\")\ndecoder_inputs[0]","0f72c8ee":"decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')","c26f2cad":"import urllib\nprint('Downloading pretrained embedding vectors..')\nurllib.request.urlretrieve(\"https:\/\/github.com\/kmr0877\/IMDB-Sentiment-Classification-CBOW-Model\/raw\/master\/glove.6B.50d.txt.gz\",filename=\"glove.6B.50d.txt.gz\")\n\nprint('Extracting pretrained embedding vectors...')\nimport gzip\nimport shutil\nwith gzip.open('glove.6B.50d.txt.gz', 'rb') as f_in:\n    with open('glove.6B.50d.txt', 'wb') as f_out:\n        shutil.copyfileobj(f_in, f_out)\n\nprint('Loading pretrained embedding...')\nword2vec = {}\nwith open('glove.6B.50d.txt') as f:\n  for line in f:\n    values = line.split()\n    word = values[0]\n    vec = np.asarray(values[1:],dtype='float32')\n    word2vec[word] = vec\nprint(f\"Found {len(word2vec)} word vectors\")\n\nprint('Filling pretrained embeddings...')\nnum_words = min(MAX_NUM_WORDS, len(word2idx_inputs)+1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word2idx_inputs.items():\n  if i < MAX_NUM_WORDS:\n    embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      embedding_matrix[i] = embedding_vector      \nprint('Done.!')\n\nembedding_layer = Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=max_len_input)","027a386e":"decoder_targets_one_hot = np.zeros((len(input_texts), max_len_target, num_words_output), dtype='float32')","563b2db1":"for i, d in enumerate(decoder_targets):\n  for t, word in enumerate(d):\n    decoder_targets_one_hot[i, t, word] = 1","a9fedff1":"encoder_inputs_placeholder = Input(shape=(max_len_input,))\nx = embedding_layer(encoder_inputs_placeholder)\nencoder = Bidirectional(LSTM(LATENT_DIM, return_sequences=True,   dropout=0.5))\nencoder_outputs = encoder(x)\n\ndecoder_inputs_placeholder = Input(shape=(max_len_target,))\ndecoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\ndecoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n\nattn_repeat_layer = RepeatVector(max_len_input)\nattn_concat_layer = Concatenate(axis=-1)\nattn_dense1 = Dense(10, activation='tanh')\nattn_dense2 = Dense(1, activation=softmax_over_time)\nattn_dot = Dot(axes=1)\n\ndef one_step_attention(h, st_1):\n  st_1 = attn_repeat_layer(st_1)\n  x = attn_concat_layer([h, st_1])\n  x = attn_dense1(x)\n  alphas = attn_dense2(x)\n  context = attn_dot([alphas, h])\n  return context\n\ndecoder_lstm = LSTM(LATENT_DIM, return_state=True,dropout=0.5)\ndecoder_dense = Dense(num_words_output, activation='softmax')\n\ninitial_s = Input(shape=(LATENT_DIM,), name='s0')\ninitial_c = Input(shape=(LATENT_DIM), name='c0')\ncontext_last_word_concat_layer = Concatenate(axis=2)\n\ns = initial_s\nc = initial_c\n\noutputs = []\nfor t in range(max_len_target):\n  context = one_step_attention(encoder_outputs, s)\n  selector = Lambda(lambda x: x[:,t:t+1])\n  xt = selector(decoder_inputs_x)\n  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s,c])\n  decoder_outputs = decoder_dense(o)\n  outputs.append(decoder_outputs)\n\ndef stack_and_permute(x):\n  x = K.stack(x)\n  x = K.permute_dimensions(x, pattern=(1,0,2))\n  return x\n\nstacker = Lambda(stack_and_permute)\noutputs = stacker(outputs)\n\nmodel = Model([encoder_inputs_placeholder, decoder_inputs_placeholder, initial_s, initial_c], outputs)\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])","0a359a29":"tf.keras.utils.plot_model(\n    model, to_file='model.png', show_shapes=False, show_dtype=False,\n    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n)","05c1399e":"model.summary()","e6eb9a69":"z = np.zeros((len(input_texts), LATENT_DIM))\nr = model.fit([encoder_inputs, decoder_inputs, z, z], \n              decoder_targets_one_hot, \n              batch_size=BATCH_SIZE, \n              epochs=EPOCHS, \n              validation_split=0.2, \n              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,verbose=2,mode='min',restore_best_weights=True),\n                         tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=5,verbose=2,mode='min')],\n              verbose=1)","fcebe3d7":"import seaborn as sns\n%config InlineBackend.figure_format = 'retina'\nsns.set_style('whitegrid')\nsns.set(rc={\"figure.figsize\":(12,10)})\n\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'],label='val_loss')\nplt.legend()\nplt.show()\n\nplt.plot(r.history['accuracy'], label='acc')\nplt.plot(r.history['val_accuracy'],label='val_acc')\nplt.legend()\nplt.show()","7bada0c2":"encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n\nencoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\ndecoder_inputs_single = Input(shape=(1,))\ndecoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n\ncontext = one_step_attention(encoder_outputs_as_input, initial_s)\n\ndecoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n\no, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\ndecoder_outputs = decoder_dense(o)\n\ndecoder_model = Model(\n  inputs=[\n    decoder_inputs_single,\n    encoder_outputs_as_input,\n    initial_s, \n    initial_c\n  ],\n  outputs=[decoder_outputs, s, c]\n)\n\nidx2word_eng = {v:k for k, v in word2idx_inputs.items()}\nidx2word_trans = {v:k for k, v in word2idx_outputs.items()}","ba4a90d2":"def decode_sequence(input_seq):\n  enc_out = encoder_model.predict(input_seq)\n  target_seq = np.zeros((1, 1))\n  target_seq[0, 0] = word2idx_outputs['<sos>']\n  eos = word2idx_outputs['<eos>']\n  s = np.zeros((1, LATENT_DIM))\n  c = np.zeros((1, LATENT_DIM))\n  output_sentence = []\n  for _ in range(max_len_target):\n    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n    idx = np.argmax(o.flatten())\n    if eos == idx:\n      break\n    word = ''\n    if idx > 0:\n      word = idx2word_trans[idx]\n      output_sentence.append(word)\n    target_seq[0, 0] = idx\n  return ' '.join(output_sentence)","5d42cd84":"while True:\n  i = np.random.choice(len(input_texts))\n  input_seq = encoder_inputs[i:i+1]\n  translation = decode_sequence(input_seq)\n  print('-')\n  print('Input sentence:', input_texts[i])\n  print('Predicted translation:', translation)\n  print('Actual translation:', target_texts[i])\n  ans = input(\"Continue? [Y\/n]\")\n  if ans and ans.lower().startswith('n'):\n    break","6834caf7":"This work is inspired by original work done by Rohit Singh (https:\/\/github.com\/sourcecode369) & hyperparameters are tunned for better results with limited resources of 13 RAM & 16 GB of GPU."}}