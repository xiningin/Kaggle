{"cell_type":{"ba38893d":"code","fd820844":"code","d5e003d9":"code","7c132622":"code","90c247da":"code","a2b374da":"code","24542fcc":"code","0a6c18d7":"code","fa518c23":"code","7cd1841d":"code","d6706090":"code","4c8142c6":"code","89266d70":"code","bb873960":"code","3ca1f252":"code","6ceadd2c":"code","173c93f5":"code","9499cd47":"code","5606c98f":"code","8c0de1cc":"code","76672e59":"code","fcaae599":"code","175120e6":"code","83a7948b":"code","5dfb1fdd":"code","5e95205d":"code","9fe88a6d":"code","dc20aff4":"code","ad0f3bfa":"code","da8ab9f8":"code","5b438d0c":"code","3b730cdf":"code","82679137":"code","2d601a1d":"code","47ad655e":"code","67558dec":"code","a3a84fa7":"code","a1500e16":"code","dff0bf7c":"code","3b54ccf7":"code","1f926cac":"code","61c0a860":"code","8a89e3ec":"code","561aa581":"code","9991351e":"code","f10b06f7":"code","3c80de19":"code","5cc9be51":"code","35a6dc25":"code","8e2bd1de":"code","14d2c303":"code","5cf27175":"code","25734173":"code","48cef041":"code","c0cebd73":"code","05c92d98":"code","26824787":"code","1764cfe2":"code","8bb903a1":"code","5293ef83":"code","e30a53a6":"code","05e7a90f":"code","df72fcbf":"code","60ba3f52":"code","76978634":"code","1e7dbe34":"code","209472c2":"code","6a5bf8b9":"code","9537702c":"code","37cd47d4":"code","9fc65a66":"code","41eed18c":"code","2f1e410a":"code","31fe0f14":"code","e120d01d":"code","3bceb432":"code","11661474":"code","bde783bb":"code","1eec6b3b":"code","7379e310":"code","70a1ed11":"code","6c53898b":"code","0c315d46":"code","680f65a5":"code","73ee05b3":"code","9093ee22":"code","3b46dcec":"code","82a0c1e8":"code","ac6fe98b":"code","25ab219b":"markdown","161d7db0":"markdown","6a636174":"markdown","fca1a123":"markdown","06d6fbdb":"markdown","4ffcb0f2":"markdown","a5d32a03":"markdown","f2f2f9d8":"markdown","471acd9a":"markdown","97b10a94":"markdown"},"source":{"ba38893d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fd820844":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings","d5e003d9":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata.head()","7c132622":"data.isnull().sum()","90c247da":"f, ax = plt.subplots(1, 2, figsize=(18,8))\ndata['Survived'].value_counts().plot.pie(explode=[0,0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived', data=data, ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","a2b374da":"data.groupby(['Sex', 'Survived'])['Survived'].count()","24542fcc":"f, ax = plt.subplots(1, 2, figsize=(18,8))\ndata[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","0a6c18d7":"pd.crosstab(data.Pclass, data.Survived, margins=True).style.background_gradient(cmap='summer_r')","fa518c23":"f, ax = plt.subplots(1, 2, figsize=(18,8))\ndata['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'], ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Pclass:Survived vs Dead')\nplt.show()","7cd1841d":"pd.crosstab([data.Sex,data.Survived], data.Pclass, margins=True).style.background_gradient(cmap='summer_r')","d6706090":"sns.factorplot('Pclass', 'Survived', hue='Sex', data=data)\nplt.show()","4c8142c6":"print('Oldest Passenger was of:', data['Age'].max(), 'Years')\nprint('Youngest Passenger was of:', data['Age'].min(), 'Years')\nprint('Average Age on the ship:', data['Age'].mean(), 'Years')","89266d70":"f, ax = plt.subplots(1, 2, figsize=(18,8))\nsns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=data, split=True, ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=data, split=True, ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","bb873960":"data['Initial'] = 0\nfor i in data:\n    data['Initial'] = data.Name.str.extract('([A-Za-z]+)\\.')","3ca1f252":"pd.crosstab(data.Initial, data.Sex).T.style.background_gradient(cmap='summer_r')","6ceadd2c":"data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'], ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'], inplace=True)","173c93f5":"data.groupby('Initial')['Age'].mean()","9499cd47":"data.loc[(data.Age.isnull()) & (data.Initial == 'Mr'), 'Age'] = 33\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Mrs'), 'Age'] = 36\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Master'), 'Age'] = 5\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Miss'), 'Age'] = 22\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Other'), 'Age'] = 46","5606c98f":"data.Age.isnull().any()","8c0de1cc":"f, ax = plt.subplots(1, 2, figsize=(20,10))\ndata[data['Survived'] == 0].Age.plot.hist(ax=ax[0], bins=20, edgecolor='black', color='red')\nax[0].set_title('Survived= 0')\nx1 = list(range(0,85,5))\nax[0].set_xticks(x1)\ndata[data['Survived'] == 1].Age.plot.hist(ax=ax[1], color='green', bins=20, edgecolor='black')\nax[1].set_title('Survived= 1')\nx2 = list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","76672e59":"data['Embarked'].fillna('S', inplace=True)\ndata.Embarked.isnull().any()","fcaae599":"data['Age_band'] = 0\ndata.loc[data['Age'] <= 16, 'Age_band'] = 0\ndata.loc[(data['Age'] > 16) & (data['Age'] <= 32), 'Age_band'] = 1\ndata.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age_band'] = 2\ndata.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age_band'] = 3\ndata.loc[data['Age'] > 64, 'Age_band'] = 4\ndata.head(2)","175120e6":"data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')","83a7948b":"sns.factorplot('Age_band', 'Survived', data=data, col='Pclass')\nplt.show()","5dfb1fdd":"data['Family_Size'] = 0\ndata['Family_Size'] = data['Parch'] + data['SibSp']\ndata['Alone'] = 0\ndata.loc[data.Family_Size==0,'Alone'] = 1\n\nf, ax = plt.subplots(1, 2, figsize=(18,6))\nsns.factorplot('Family_Size', 'Survived', data=data, ax=ax[0])\nax[0].set_title('Family_Size vs Survived')\nsns.factorplot('Alone','Survived', data=data, ax=ax[1])\nax[1].set_title('Alone vs Survived')\nplt.close(2)\nplt.close(3)\nplt.show()","5e95205d":"sns.factorplot('Alone', 'Survived', data=data, hue='Sex', col='Pclass')\nplt.show()","9fe88a6d":"data['Fare_Range'] = pd.qcut(data['Fare'], 4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","dc20aff4":"data['Fare_cat'] = 0\ndata.loc[data['Fare'] <= 7.91, 'Fare_cat'] = 0\ndata.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454), 'Fare_cat'] = 1\ndata.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31), 'Fare_cat'] = 2\ndata.loc[(data['Fare'] > 31) & (data['Fare'] <= 513),'Fare_cat'] = 3","ad0f3bfa":"sns.factorplot('Fare_cat', 'Survived', data=data, hue='Sex')\nplt.show()","da8ab9f8":"data['Sex'].replace(['male','female'], [0,1], inplace=True)\ndata['Embarked'].replace(['S','C','Q'], [0,1,2], inplace=True)\ndata['Initial'].replace(['Mr','Mrs','Miss','Master','Other'], [0,1,2,3,4], inplace=True)","5b438d0c":"data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'], axis=1, inplace=True)\nsns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidths=0.2, annot_kws={'size':20})\nfig = plt.gcf()\nfig.set_size_inches(18, 15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","3b730cdf":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix","82679137":"train, test = train_test_split(data,test_size=0.3, random_state=0, stratify=data['Survived'])\ntrain_X = train[train.columns[1:]]\ntrain_Y = train[train.columns[:1]]\ntest_X = test[test.columns[1:]]\ntest_Y = test[test.columns[:1]]\nX = data[data.columns[1:]]\nY = data['Survived']","2d601a1d":"model = svm.SVC(kernel='rbf', C=1, gamma=0.1)\nmodel.fit(train_X, np.ravel(train_Y))\nprediction1 = model.predict(test_X)\nprint('Accuracy for rbf SVM is ', metrics.accuracy_score(prediction1, test_Y))","47ad655e":"model=svm.SVC(kernel='linear', C=0.1, gamma=0.1)\nmodel.fit(train_X, np.ravel(train_Y))\nprediction2 = model.predict(test_X)\nprint('Accuracy for linear SVM is', metrics.accuracy_score(prediction2, test_Y))","67558dec":"model = LogisticRegression(solver='lbfgs')\nmodel.fit(train_X, np.ravel(train_Y))\nprediction3 = model.predict(test_X)\nprint('The accuracy of the Logistic Regression is', metrics.accuracy_score(prediction3, test_Y))","a3a84fa7":"model = KNeighborsClassifier() \nmodel.fit(train_X, np.ravel(train_Y))\nprediction5 = model.predict(test_X)\nprint('The accuracy of the KNN is', metrics.accuracy_score(prediction5, test_Y))","a1500e16":"a_index = list(range(1,11))\na = pd.Series()\nx = [0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model = KNeighborsClassifier(n_neighbors=i)\n    model.fit(train_X, np.ravel(train_Y))\n    prediction = model.predict(test_X)\n    a = a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig = plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:', a.values, 'with the max value as ', a.values.max())","dff0bf7c":"model = GaussianNB()\nmodel.fit(train_X, np.ravel(train_Y))\nprediction6 = model.predict(test_X)\nprint('The accuracy of the NaiveBayes is', metrics.accuracy_score(prediction6, test_Y))","3b54ccf7":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X, np.ravel(train_Y))\nprediction7 = model.predict(test_X)\nprint('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7, test_Y))","1f926cac":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz = []\naccuracy = []\nstd = []\nclassifiers = ['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels = [svm.SVC(kernel='linear', gamma='scale'), svm.SVC(kernel='rbf', gamma='scale'), LogisticRegression(solver='lbfgs'), KNeighborsClassifier(n_neighbors=9), DecisionTreeClassifier(), GaussianNB(), RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model, X, Y, cv=kfold, scoring=\"accuracy\")\n    cv_result = cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2 = pd.DataFrame({'CV Mean':xyz,'Std':std}, index=classifiers)       \nnew_models_dataframe2","61c0a860":"plt.subplots(figsize=(12,6))\nbox = pd.DataFrame(accuracy, index=[classifiers])\nbox.T.boxplot()","8a89e3ec":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig = plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","561aa581":"from sklearn.ensemble import VotingClassifier\n\nensemble_lin_rbf = VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF',svm.SVC(probability=True, kernel='rbf',C=0.5,gamma=0.1)),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR',LogisticRegression(solver='lbfgs', C=0.05)),\n                                              ('DT',DecisionTreeClassifier(random_state=0)),\n                                              ('NB',GaussianNB()),\n                                              ('svm',svm.SVC(kernel='linear',probability=True))], \n                                   voting='soft').fit(train_X, np.ravel(train_Y))\nprint('The accuracy for ensembled model is:', ensemble_lin_rbf.score(test_X, test_Y))\ncross = cross_val_score(ensemble_lin_rbf, X, Y, cv=10, scoring=\"accuracy\")\nprint('The cross validated score is', cross.mean())","9991351e":"#%matplotlib inline\n\n# for seaborn issue:\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport sklearn as sk\nimport itertools\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn import tree\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn import svm\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nsns.set(style='white', context='notebook', palette='deep')","f10b06f7":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ncombine = pd.concat([train.drop('Survived',1),test])","3c80de19":"surv = train[train['Survived']==1]\nnosurv = train[train['Survived']==0]\nsurv_col = \"blue\"\nnosurv_col = \"red\"\n\nprint(\"Survived: %i (%.1f percent), Not Survived: %i (%.1f percent), Total: %i\"\\\n      %(len(surv), 1.*len(surv)\/len(train)*100.0,\\\n        len(nosurv), 1.*len(nosurv)\/len(train)*100.0, len(train)))","5cc9be51":"warnings.filterwarnings(action=\"ignore\")\nplt.figure(figsize=[12,10])\nplt.subplot(331)\nsns.distplot(surv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=surv_col)\nsns.distplot(nosurv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=nosurv_col,\n            axlabel='Age')\nplt.subplot(332)\nsns.barplot('Sex', 'Survived', data=train)\nplt.subplot(333)\nsns.barplot('Pclass', 'Survived', data=train)\nplt.subplot(334)\nsns.barplot('Embarked', 'Survived', data=train)\nplt.subplot(335)\nsns.barplot('SibSp', 'Survived', data=train)\nplt.subplot(336)\nsns.barplot('Parch', 'Survived', data=train)\nplt.subplot(337)\nsns.distplot(np.log10(surv['Fare'].dropna().values+1), kde=False, color=surv_col)\nsns.distplot(np.log10(nosurv['Fare'].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nprint(\"Median age survivors: %.1f, Median age non-survivers: %.1f\"\\\n      %(np.median(surv['Age'].dropna()), np.median(nosurv['Age'].dropna())))","35a6dc25":"combine.where((combine['Embarked'] !='Q') & (combine['Pclass'] < 1.5) & \\\n    (combine['Sex'] == \"female\")).groupby(['Embarked','Pclass','Sex','Parch','SibSp']).size()","8e2bd1de":"train['Embarked'].iloc[61] = \"C\"\ntrain['Embarked'].iloc[829] = \"C\"","14d2c303":"test['Fare'].iloc[152] = combine['Fare'][combine['Pclass'] == 3].dropna().median()\nprint(test['Fare'].iloc[152])","5cf27175":"combine = pd.concat([train.drop('Survived',1),test])\nsurvived = train['Survived']\n\ncombine['Child'] = combine['Age']<=10\ncombine['Cabin_known'] = combine['Cabin'].isnull() == False\ncombine['Age_known'] = combine['Age'].isnull() == False\ncombine['Family'] = combine['SibSp'] + combine['Parch']\ncombine['Alone']  = (combine['SibSp'] + combine['Parch']) == 0\ncombine['Large_Family'] = (combine['SibSp']>2) | (combine['Parch']>3)\ncombine['Deck'] = combine['Cabin'].str[0]\ncombine['Deck'] = combine['Deck'].fillna(value='U')\ncombine['Ttype'] = combine['Ticket'].str[0]\ncombine['Title'] = combine['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ncombine['Fare_cat'] = pd.DataFrame(np.floor(np.log10(combine['Fare'] + 1))).astype('int')\ncombine['Bad_ticket'] = combine['Ttype'].isin(['3','4','5','6','7','8','A','L','W'])\ncombine['Young'] = (combine['Age']<=30) | (combine['Title'].isin(['Master','Miss','Mlle']))\ncombine['Shared_ticket'] = np.where(combine.groupby('Ticket')['Name'].transform('count') > 1, 1, 0)\ncombine['Ticket_group'] = combine.groupby('Ticket')['Name'].transform('count')\ncombine['Fare_eff'] = combine['Fare']\/combine['Ticket_group']\ncombine['Fare_eff_cat'] = np.where(combine['Fare_eff']>16.0, 2, 1)\ncombine['Fare_eff_cat'] = np.where(combine['Fare_eff']<8.5,0,combine['Fare_eff_cat'])\ntest = combine.iloc[len(train):]\ntrain = combine.iloc[:len(train)]\ntrain['Survived'] = survived\n\nsurv = train[train['Survived']==1]\nnosurv = train[train['Survived']==0]","25734173":"g = sns.factorplot(x=\"Sex\", y=\"Survived\", hue=\"Child\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)\ntab = pd.crosstab(train['Child'], train['Pclass'])\nprint(tab)\ntab = pd.crosstab(train['Child'], train['Sex'])\nprint(tab)","48cef041":"cab = pd.crosstab(train['Cabin_known'], train['Survived'])\nprint(cab)\ndummy = cab.div(cab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Cabin known')\ndummy = plt.ylabel('Percentage')","c0cebd73":"g = sns.factorplot(x=\"Sex\", y=\"Survived\", hue=\"Cabin_known\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","05c92d98":"tab = pd.crosstab(train['Deck'], train['Survived'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Deck')\ndummy = plt.ylabel('Percentage')","26824787":"stats.binom_test(x=12,n=12+35,p=24\/(24.+35.))","1764cfe2":"g = sns.factorplot(x=\"Deck\", y=\"Survived\", hue=\"Sex\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","8bb903a1":"tab = pd.crosstab(train['Ttype'], train['Survived'])\nprint(tab)\nsns.barplot(x=\"Ttype\", y=\"Survived\", data=train, ci=95.0, color=\"blue\")","5293ef83":"tab = pd.crosstab(train['Bad_ticket'], train['Survived'])\nprint(tab)\ng = sns.factorplot(x=\"Bad_ticket\", y=\"Survived\", hue=\"Sex\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","e30a53a6":"tab = pd.crosstab(train['Deck'], train['Bad_ticket'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Deck')\ndummy = plt.ylabel('Percentage')","05e7a90f":"tab = pd.crosstab(train['Age_known'], train['Survived'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Age known')\ndummy = plt.ylabel('Percentage')","df72fcbf":"stats.binom_test(x=424,n=424+290,p=125\/(125.+52.))","60ba3f52":"g = sns.factorplot(x=\"Sex\", y=\"Age_known\", hue=\"Embarked\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","76978634":"tab = pd.crosstab(train['Family'], train['Survived'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Family members')\ndummy = plt.ylabel('Percentage')","1e7dbe34":"tab = pd.crosstab(train['Alone'], train['Survived'])\nprint(tab)\nsns.barplot('Alone', 'Survived', data=train)","209472c2":"g = sns.factorplot(x=\"Sex\", y=\"Alone\", hue=\"Embarked\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","6a5bf8b9":"tab = pd.crosstab(train['Large_Family'], train['Survived'])\nprint(tab)\nsns.barplot('Large_Family', 'Survived', data=train)","9537702c":"g = sns.factorplot(x=\"Sex\", y=\"Large_Family\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","37cd47d4":"tab = pd.crosstab(train['Shared_ticket'], train['Survived'])\nprint(tab)\nsns.barplot('Shared_ticket', 'Survived', data=train)","9fc65a66":"combine = pd.concat([train.drop('Survived',1),test])\nsurvived = train['Survived']\n\ncombine[\"Sex\"] = combine[\"Sex\"].astype(\"category\")\ncombine[\"Sex\"].cat.categories = [0,1]\ncombine[\"Sex\"] = combine[\"Sex\"].astype(\"int\")\ncombine[\"Embarked\"] = combine[\"Embarked\"].astype(\"category\")\ncombine[\"Embarked\"].cat.categories = [0,1,2]\ncombine[\"Embarked\"] = combine[\"Embarked\"].astype(\"int\")\ncombine[\"Deck\"] = combine[\"Deck\"].astype(\"category\")\ncombine[\"Deck\"].cat.categories = [0,1,2,3,4,5,6,7,8]\ncombine[\"Deck\"] = combine[\"Deck\"].astype(\"int\")\n\ntest = combine.iloc[len(train):]\ntrain = combine.iloc[:len(train)]\ntrain['Survived'] = survived\n\ntrain.loc[:,[\"Sex\",\"Embarked\"]].head()","41eed18c":"ax = plt.subplots( figsize =( 12 , 10 ) )\nfoo = sns.heatmap(train.drop('PassengerId',axis=1).corr(), vmax=1.0, square=True, annot=True)","2f1e410a":"training, testing = train_test_split(train, test_size=0.2, random_state=0)\nprint(\"Total sample size = %i; training sample size = %i, testing sample size = %i\"\\\n     %(train.shape[0],training.shape[0],testing.shape[0]))","31fe0f14":"cols = ['Sex','Pclass','Cabin_known','Large_Family','Parch',\n        'SibSp','Young','Alone','Shared_ticket','Child']\ntcols = np.append(['Survived'],cols)\n\ndf = training.loc[:,tcols].dropna()\nX = df.loc[:,cols]\ny = np.ravel(df.loc[:,['Survived']])","e120d01d":"clf_log = LogisticRegression()\nclf_log = clf_log.fit(X,y)\nscore_log = clf_log.score(X,y)\nprint(score_log)","3bceb432":"pd.DataFrame(list(zip(X.columns, np.transpose(clf_log.coef_))))","11661474":"cols = ['Sex','Pclass','Cabin_known','Large_Family','Shared_ticket','Young','Alone','Child']\ntcols = np.append(['Survived'],cols)\n\ndf = training.loc[:,tcols].dropna()\nX = df.loc[:,cols]\ny = np.ravel(df.loc[:,['Survived']])\n\ndf_test = testing.loc[:,tcols].dropna()\nX_test = df_test.loc[:,cols]\ny_test = np.ravel(df_test.loc[:,['Survived']])","bde783bb":"clf_log = LogisticRegression()\nclf_log = clf_log.fit(X,y)\nscore_log = cross_val_score(clf_log, X, y, cv=5).mean()\nprint(score_log)","1eec6b3b":"clf_rf = RandomForestClassifier(\n    n_estimators=1000, \\\n    max_depth=None, \\\n    min_samples_split=10 \\\n    #class_weight=\"balanced\", \\\n    #min_weight_fraction_leaf=0.02 \\\n    )\nclf_rf = clf_rf.fit(X,y)\nscore_rf = cross_val_score(clf_rf, X, y, cv=5).mean()\nprint(score_rf)","7379e310":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nclf_gb = GradientBoostingClassifier(\n            #loss='exponential',\n            n_estimators=1000,\n            learning_rate=0.1,\n            max_depth=3,\n            subsample=0.5,\n            random_state=0).fit(X, y)\nclf_gb.fit(X,y)\nscore_gb = cross_val_score(clf_gb, X, y, cv=5).mean()\nprint(score_gb)","70a1ed11":"clf_xgb = xgb.XGBClassifier(\n    max_depth=2,\n    n_estimators=500,\n    subsample=0.5,\n    learning_rate=0.1\n    )\nclf_xgb.fit(X,y)\nscore_xgb = cross_val_score(clf_xgb, X, y, cv=5).mean()\nprint(score_xgb)","6c53898b":"clf_lgb = lgb.LGBMClassifier(\n    max_depth=2,\n    n_estimators=500,\n    subsample=0.5,\n    learning_rate=0.1\n    )\nclf_lgb.fit(X,y)\nscore_lgb = cross_val_score(clf_lgb, X, y, cv=5).mean()\nprint(score_lgb)","0c315d46":"clf = clf_lgb\ndf2 = test.loc[:,cols].fillna(method='pad')\nsurv_pred = clf.predict(df2)","680f65a5":"surv_pred.shape","73ee05b3":"submit = pd.DataFrame({'PassengerId' : test.loc[:,'PassengerId'],\n                       #'Survived': surv_pred.T})\n                       'Survived': surv_pred.T})\nsubmit.to_csv(\"..\/working\/submit.csv\", index=False)","9093ee22":"submit.head()","3b46dcec":"submit.shape","82a0c1e8":"# https:\/\/www.kaggle.com\/c\/titanic\/discussion\/10099\n  \n# imported library\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndataset1 = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndataset3 = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n#checking nan in columbs of dataset\ndataset.isnull().sum()\ndataset1.isnull().sum()\n#checking the datatype of series in dataframes\ndataset.dtypes\n#checking shape\ndataset.shape\ndataset.head(30)\ndataset.describe()\n#visualing the data\ndataset.plot(kind='box', subplots=True, layout=(2,7), sharex=False, sharey=False)\nplt.show()\n\ndataset.hist()\nplt.show()\n#making traing nad test datset\nX_train= dataset.iloc[:,[2,4,5,6,7,9]].values\ny_train= dataset.iloc[:,1:2].values\n\nX_test = dataset1.iloc[:,[1,3,4,5,6,8]].values\ny_test = dataset3.iloc[:,1:2].values\n# filling the NAN values withh mean \nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values=\"NaN\",strategy=\"mean\",axis=0)\nimputer = imputer.fit(X_train[:, 2:3])\nX_train[:, 2:3] = imputer.transform(X_train[:,2:3])\n\nimputer = Imputer(missing_values=\"NaN\",strategy=\"mean\",axis=0)\nimputer = imputer.fit(X_test[:, [2,5]])\nX_test[:, [2,5]] = imputer.transform(X_test[:,[2,5]])\n\n# making the dummy varaible of catagorical data\n\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X = LabelEncoder()\nX_train[:,1] = labelencoder_X.fit_transform(X_train[:, 1])\nonehotencoder = OneHotEncoder(categorical_features=[0])\nX_train = onehotencoder.fit_transform(X_train).toarray()\n\nlabelencoder_X1 = LabelEncoder()\nX_test[:,1] = labelencoder_X1.fit_transform(X_test[:, 1])\nonehotencoder1 = OneHotEncoder(categorical_features=[0])\nX_test = onehotencoder1.fit_transform(X_test).toarray()\n\n# performing standard scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.fit_transform(X_test)\n\n# performng PCA \nfrom sklearn.decomposition import PCA\npca = PCA(n_components = None)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\n\n#importing model\nfrom sklearn.linear_model import LinearRegression\n#maiing object\nregressor = LinearRegression()\n#trainnig the model\nregressor.fit(X_train, y_train)\n#predicting the model on test data ste\ny_pred = regressor.predict(X_test)\ny_pred = y_pred > 0.5\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nl = dataset3.iloc[:,0].values\n\ny_pred[1]\n\nsub =[]\nfor i in range(len(y_pred)):\n    if(y_pred[i]==False):\n        sub.append(0)\n    else:\n        sub.append(1)\nj = np.asarray(sub)\n# accuracy of 97.84%","ac6fe98b":"j.shape","25ab219b":"### References\n- https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\n- https:\/\/www.kaggle.com\/headsortails\/pytanic\n- https:\/\/www.kaggle.com\/chapagain\/titanic-solution-a-beginner-s-guide\n- https:\/\/www.kaggle.com\/c\/titanic\/discussion\/10099","161d7db0":"...","6a636174":"### Part 2: Feature Engineering and Data Cleaning","fca1a123":"## Pytanic","06d6fbdb":"### Part 1: EDA","4ffcb0f2":"...","a5d32a03":"...","f2f2f9d8":"### Contents of the Notebook:\n\nPart 1: Exploratory Data Analysis (EDA):\n- 1) Analysis of the features.\n- 2) Finding any relations or trends considering multiple features.\n\nPart 2: Feature Engineering and Data Cleaning:\n- 1) Adding any few features.\n- 2) Removing redundant features.\n- 3) Converting features into suitable form for modeling.\n\nPart 3: Predictive Modeling\n- 1) Running Basic Algorithms.\n- 2) Cross Validation.\n- 3) Ensembling.\n- 4) Important Features Extraction.\n\n### References\n- https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic","471acd9a":"## EDA to Pred","97b10a94":"### Part 3: Predictive Modeling"}}