{"cell_type":{"92879b4d":"code","7fab0355":"code","e92cbf1f":"code","58b5ca61":"code","e97043a4":"code","e91d8f29":"code","6f29a5fc":"code","2dd3085a":"code","199c6564":"code","098734ab":"code","3c771219":"code","ec45c157":"code","a0201853":"markdown","8464ebdb":"markdown","ea0fc5f9":"markdown","c4dd72df":"markdown","ea2236aa":"markdown","e1b32543":"markdown","41c0d960":"markdown","430e75e2":"markdown","83ad046c":"markdown","08768344":"markdown","d7642934":"markdown","830d4f9f":"markdown"},"source":{"92879b4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualisation purposes\nfrom sklearn import linear_model\nfrom sklearn.tree import DecisionTreeClassifier ,plot_tree, DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nprint('Set up complete')\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7fab0355":"trainfile='..\/input\/students-performance-in-exams\/StudentsPerformance.csv'\ndata = pd.read_csv(trainfile)\ndata.describe(include='all')\n#print(data)","e92cbf1f":"selected_columns=['gender','lunch','test preparation course']\nX=data[selected_columns]\nX=X.dropna(axis=0)\nX.describe()\nmath_y=data['math score']\nread_y=data['reading score']\nwrite_y=data['writing score']\nsns.pairplot(data[['math score', 'reading score', 'writing score']], height = 5)\nsns.relplot(x='math score', y='reading score', hue='lunch', data=data[['math score', 'reading score', 'lunch']])\nsns.relplot(x='math score', y='reading score', hue='test preparation course', data=data[['math score', 'reading score', 'test preparation course']])\nsns.relplot(x='math score', y='reading score', hue='parental level of education', data=data[['math score', 'reading score', 'parental level of education']])","58b5ca61":"sns.relplot(x='math score', y='reading score', hue='race\/ethnicity', data=data[['math score', 'reading score', 'race\/ethnicity']])\nsns.relplot(x='math score', y='reading score', hue='gender', data=data[['math score', 'reading score', 'gender']])","e97043a4":"selected_columns.append('gender')\nX=data[selected_columns]\nX=X.dropna(axis=0)","e91d8f29":"one_hot_X=pd.get_dummies(X)\none_hot_X.head()\nmath_train_X,math_val_X,math_train_y,math_val_y=train_test_split(one_hot_X, math_y, random_state=1)\nread_train_X,read_val_X,read_train_y,read_val_y=train_test_split(one_hot_X, read_y, random_state=1)\nwrite_train_X,write_val_X,write_train_y,write_val_y=train_test_split(one_hot_X, write_y, random_state=1)\nwrite_train_X.head(10)","6f29a5fc":"linear_math=linear_model.LinearRegression()\nlinear_math.fit(math_train_X,math_train_y)\nlinear_read=linear_model.LinearRegression()\nlinear_read.fit(read_train_X,read_train_y)\nlinear_write=linear_model.LinearRegression()\nlinear_write.fit(write_train_X,write_train_y)","2dd3085a":"math_score_predictor=DecisionTreeRegressor(max_depth=10,random_state=1)\nmath_score_predictor.fit(math_train_X,math_train_y)\nread_score_predictor=DecisionTreeRegressor(max_depth=10,random_state=1)\nread_score_predictor.fit(read_train_X,read_train_y)\nwrite_score_predictor=DecisionTreeRegressor(max_depth=10,random_state=1)\nwrite_score_predictor.fit(write_train_X,write_train_y)","199c6564":"linear_math_pred=linear_math.predict(one_hot_X)\nX['linear_math score']=math_y\nX['lm_Predicted']=linear_math_pred\nlinear_read_pred=linear_read.predict(one_hot_X)\nX['linear_reading score']=read_y\nX['lr_Predicted']=linear_read_pred\nlinear_write_pred=linear_write.predict(one_hot_X)\nX['linear_writing score']=write_y\nX['lw_Predicted']=linear_write_pred","098734ab":"math_pred=math_score_predictor.predict(one_hot_X)\nX['math score']=math_y\nX['m_Predicted']=math_pred\nread_pred=read_score_predictor.predict(one_hot_X)\nX['reading score']=read_y\nX['r_Predicted']=read_pred\nwrite_pred=write_score_predictor.predict(one_hot_X)\nX['writing score']=write_y\nX['w_Predicted']=write_pred\nprint(X)","3c771219":"lmc=linear_math.predict(math_val_X)\nlrc=linear_read.predict(read_val_X)\nlwc=linear_write.predict(write_val_X)\nlm_mae=mean_absolute_error(lmc,math_val_y)\nlr_mae=mean_absolute_error(lrc,read_val_y)\nlw_mae=mean_absolute_error(lwc,write_val_y)\nprint(f'Mean absolute error of linear regressor on math score prediction: {lm_mae}')\nprint(f'Mean absolute error of linear regressor on reading score prediction: {lr_mae}')\nprint(f'Mean absolute error of linear regressor on writing score prediction: {lw_mae}')","ec45c157":"mc=math_score_predictor.predict(math_val_X)\nm_mae=mean_absolute_error(mc,math_val_y)\nrc=read_score_predictor.predict(read_val_X)\nr_mae=mean_absolute_error(rc,read_val_y)\nwc=write_score_predictor.predict(write_val_X)\nw_mae=mean_absolute_error(wc,write_val_y)\nprint(f'Mean absolute error of decision tree regressor on math score prediction: {m_mae}')\nprint(f'Mean absolute error of decision tree regressor on reading score prediction: {r_mae}')\nprint(f'Mean absolute error of decision tree regressor on writing score prediction: {w_mae}')","a0201853":"# Fit the model\n### Linear Regression\n#### According to the hypothesis made above, I am going to use a Linear Regressor to make predictions.\n#### I will train three different Linear Tree Regressor models to train three different sets of data to predict math, reading and writing scores respectively.","8464ebdb":"### plot another two graphs for features of Gender and race\/ethnicity to have a look at their effects on data prediction. According to those two graphs, race\/ethnicity won't have to much effects on prediction as the scores seem to be distributed averagly, gender seems have a big impact on the scores, so I will add gender into the list of features that I will be using to train the model.","ea0fc5f9":"### Decision Tree Regressor\n#### In order to make sure I can get the most accurate predictions, I will train three other models using Decision Tree Rregressor","c4dd72df":"# Preparation\n#### Getting all the libraries that are going to be used ready.\n#### Set up the training data directory","ea2236aa":"# Get the data in\n#### Gives a general view of the data","e1b32543":"# Prepare the data\n#### Change those non-numerical data to numerical data\n#### Split three different sets of data into training and testing data as we are going to predict student's math, reading and writing scores.","41c0d960":"# Making predictions\n#### Make predictions with the two models which have been trained with the testing data.","430e75e2":"# Model selection\n#### According to the graph above, we can see that those data generally fits a line, and the features I chose to use to make predictions all have a big weighting on the outcome. Therefore, I can predict that a linear regression model may have a better performance on predicting the scores.","83ad046c":"# Gather and Explore the data\n#### define the features that the model will be trained with\n#### drop all the missing values in the data to get a more accurate outcome\n#### define student's exam score\n#### visualize all the scores students get","08768344":"# Evaluate the model\n#### Check the mean absolute errors of the predictions in order to evalute the accuracy of our model.\n#### One big improvements regards to Mean Absolute Errors is when I changed the model from Decision Tree Classifier to Decision Tree Regressor, the MAE droped from 20 to 10, I then changed the features that I am using to train the model, while the MAE was still around 10.\n#### The reason for this big improvement can be attributed to the features of models I am using. As Decision Tree Classifier is a model that was designed to make predictions on boolean values, while the Decision Tree Regressor was kind of designed to predict numerical data. Therefore, regressor can make better predictions on numerical data.","d7642934":"\n# Introduction of the Investigation\n### Purposes:\n#### In this project, I will be investigating the best machine learning model for students exam scores predicting.\n#### The data I will be using in this investigation is compromised of three different test scores: \"math\", \"reading\", \"writing\". It has five columns of different features which are: \"gender\", \"race\/ethnicity\", \"parental level of learning\", \"lunch\", \"test preparation course\".\n#### I hope to get the most accurate predictions I can get from this project.\n#### I chose this set of data for prediction because it had various features in it, most values in the data are non-numerical data, however, we can easily use them for prediction after a bit modification. \n\n### Hypothesis:\n#### I predict that the best model for this data will be Linear Regressor.\n#### I predict that features that have the strongest effects on predictions will be \"test preparation\"; \"lunch\"; \"parental level of education\" as they are generally affect students' performance more in real life.\n#### I am looking forward to minimise the Mean Absolute Errors as much as possible.\n#### According to the size of the data, I will aim to keep the Mean Absolute Errors below 15.","830d4f9f":"# Conclusion\n### The purposes of the investigation:\n#### In this investigation, I successfully implemented the Decision Tree Regerssor and Linear Regressor to make predictions based on students pre-exam features. \n#### The quality of the predictions is relatively not good, but is already the best I can get based on what I have learnt. There might be other methods which can help reduce the Mean Absolute Errors of my predicitions. By the way, I have also implemented a random forest regressor and got a similar mean absolute error as the Decision Tree Regressor.\n#### However, I do have some ideas that may help with lowering the Mean Absolute Error. I can draw more graphs and make more analysis on what features weight more than other features, and change the weighting of those features when training the model. Or, I can use some reinforement learning techinques, get some more data of student's exam performance to get a better outcome."}}