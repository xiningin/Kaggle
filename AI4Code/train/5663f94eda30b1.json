{"cell_type":{"f409b55d":"code","72ec51b8":"code","258ef95d":"code","c6054384":"code","cdc5cbd1":"code","ca12caaa":"code","45d9806c":"code","6c140b79":"code","00cd26aa":"code","509d11c8":"code","122470e9":"code","137d60a6":"code","5584608f":"code","f84fd5f3":"code","d424c14d":"code","24a5f8f9":"code","588c9558":"code","73085a47":"code","1d5fdc42":"code","d25cd7f4":"code","eba3dc27":"code","0fc5ef71":"code","1ca6d817":"code","886c5175":"code","8d773aaa":"code","65cc5d76":"code","70d4b23f":"code","147d0331":"code","67b6bb11":"code","1b31ae7d":"markdown","2295efd3":"markdown","72edb64e":"markdown"},"source":{"f409b55d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72ec51b8":"#importing libraries\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib  \nimport statsmodels.formula.api as smf    \nimport statsmodels.api as sm  \nfrom sklearn.preprocessing import robust_scale\nfrom sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n## for explainer    \nfrom lime import lime_tabular\nfrom mlxtend.preprocessing import minmax_scaling\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing.data import QuantileTransformer\nfrom sklearn.preprocessing.data import PowerTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.stats import skew\nfrom sklearn.tree import DecisionTreeRegressor\npd.set_option('display.max_rows', 1000)\n## for data\nimport pandas as pd\nimport numpy as np\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 8, 5\n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nSEED=2020\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nkf = KFold(n_splits = 5, random_state = 2)\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import RFECV\n## for statistical tests\nimport scipy\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n## for machine learning\nfrom sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n## for explainer\nfrom lime import lime_tabular\n## for machine learning\nfrom sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\nfrom collections import Counter\nfrom numpy import mean\nfrom IPython.core.interactiveshell import InteractiveShell \nInteractiveShell.ast_node_interactivity = \"all\"\nimport xgboost as xgb\n","258ef95d":"## importing and taking a look for dataset\ntrain= pd.read_csv('\/kaggle\/input\/mercedes-benz-greener-manufacturing\/train.csv.zip')\ntrain.sample(20)","c6054384":"train.shape","cdc5cbd1":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\nprint(f'There are {len(one_value_cols)} columns in train dataset with one unique value.')","ca12caaa":"# DF before tuning\ndef basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value'] = df.isnull().sum()\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(train)","45d9806c":"## drop features with 1 unique value\n# train.drop(['ID','X11','X107', 'X233', 'X235', 'X268','X289', 'X290','X293', 'X297', 'X330', 'X347'], axis=1, inplace=True)\ntrain.drop(one_value_cols, axis=1, inplace=True)\n","6c140b79":"## drop features with low variabiliry\nvariance_treshold = 0.9\nlow_var_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > variance_treshold]\ntrain=train.drop(low_var_cols, axis=1) ","00cd26aa":"## add some additional features, thanks for this script to @ Vitalii Mokin\ndef feature_creation(df):\n    for i in ['X0', 'X1', 'X2', 'X3', 'X5', 'X6', 'X8']:\n        for j in ['X0', 'X1', 'X2', 'X3', 'X5', 'X6', 'X8']:\n            df[i + \"_\" + j] = df[i].astype('str') + \"_\" + df[j].astype('str')\n\n    return df\n\n\ntrain = feature_creation(train)","509d11c8":"train.sample(5)","122470e9":"## encode object features to numeric\nlencoders = {}\nfor col in train.select_dtypes(include=['object']).columns:\n    lencoders[col] = LabelEncoder()\n    train[col] = lencoders[col].fit_transform(train[col])\n","137d60a6":"# and drop duplicates\ntrain = train.drop_duplicates()\n# train.T.drop_duplicates().T","5584608f":"## target with outliers\nsns.boxplot((train.y))","f84fd5f3":"# delete some outliers from target feature\ntrain = train[(train['y'] <= 136)].reset_index(drop=True)","d424c14d":"## target without outliers\nsns.boxplot((train.y))","24a5f8f9":"# target normalization between 0 and 1 values\ntrain[\"y\"]=((train[\"y\"]-train[\"y\"].min())\/(train[\"y\"].max()-train[\"y\"].min()))","588c9558":"## result\ntrain.y.describe()","73085a47":"## let's reduce memory for clear mind\n\ndef reduce_memory_usage(df):\n    \"\"\" The function will reduce memory of dataframe\n    Note: Apply this function after removing missing value\"\"\"\n    intial_memory = df.memory_usage().sum()\/1024**2\n    print('Intial memory usage:',intial_memory,'MB')\n    for col in df.columns:\n        mn = df[col].min()\n        mx = df[col].max()\n        if df[col].dtype != object:            \n            if df[col].dtype == int:\n                if mn >=0:\n                    if mx < np.iinfo(np.uint8).max:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < np.iinfo(np.uint16).max:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < np.iinfo(np.uint32).max:\n                        df[col] = df[col].astype(np.uint32)\n                    elif mx < np.iinfo(np.uint64).max:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n            if df[col].dtype == float:\n                df[col] =df[col].astype(np.float32)\n    \n    red_memory = df.memory_usage().sum()\/1024**2\n    print('Memory usage after complition: ',red_memory,'MB')\n    \nreduce_memory_usage(train)","1d5fdc42":"# Threshold for removing correlated variables, thanks to @ Vitalii Mokin\nthreshold = 0.9  ## optimal level 0.9\n# Absolute value correlation matrix\ncorr_matrix = train.corr().abs().round(2)\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Select columns with Pearson's correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\nfeatures_filtered = train.drop(columns = collinear_features)\nprint('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])\nfeatures_best = []\nfeatures_best.append(features_filtered.columns.tolist())","d25cd7f4":"# updated dataframe\ntrain=train[features_best[0]]\ntrain.sample(20)","eba3dc27":"X = train.drop('y', axis=1)\ny = train.y\nX['n0'] = (X == 0).sum(axis=1) ## one add feature\n# scale features between 1 and 0\nscaler = preprocessing.MinMaxScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\nX.sample(10)","0fc5ef71":"# baseline score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=SEED)\n# prepare the model with target scaling\nrf = RandomForestRegressor(max_depth=4, n_estimators=5, random_state=SEED)\n# evaluate model\ncv = KFold(n_splits=10, shuffle=True, random_state=SEED)\nscores = cross_val_score(rf, X, y, scoring='r2', cv=cv, n_jobs=-1)\n# summarize the result\ns_mean = mean(scores)\nprint('Mean R2: %.3f' % (s_mean))","1ca6d817":"## Feature ranking with recursive feature elimination and cross-validated selection of the best number of features\n# regressor =RandomForestRegressor(random_state=SEED)\n# regressor = DecisionTreeRegressor()\nregressor = xgb.XGBRegressor()\nselector = RFECV(regressor, step = 1, cv=cv, n_jobs=-1,verbose=1,  scoring='r2')\nselector.fit(X, y)\nprint('The optimal number of features is {}'.format(selector.n_features_))\nfeatures_rfecv = [f for f,s in zip(X, selector.support_) if s]\nprint('The selected features are:')\nprint ('{}'.format(features_rfecv)) ## optimal features list","886c5175":"plt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (R2)\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\nplt.savefig('feature_auc_nselected.png', bbox_inches='tight', pad_inches=1)","8d773aaa":"## let's compare RF after feature selection\nrf = RandomForestRegressor(max_depth=3, n_estimators=10, random_state=SEED)\nX_rfe = X[features_rfecv]\nrf.fit(X_rfe, y)\n# acc_log_train = round(rf.score(X_train_rfe, y_train)*100,2) \n# acc_log_test = round(rf.score(X_test[features_rfecv],y_test)*100,2)\nscores = cross_val_score(rf, X_rfe, y, cv=cv, scoring = 'r2')\n# print(\"Training Accuracy: % {}\".format(acc_log_train))\n# print(\"Testing Accuracy: % {}\".format(acc_log_test))\nprint(\"RF based on selected dataset\")\nprint(\"FR CV Accuracy Score after selection:\", scores.mean().round(3))","65cc5d76":"## feature importance with eli5 \nimport eli5 \nimport shap \nfrom eli5.sklearn import PermutationImportance\nrf.fit(X_rfe, y)\nperm = PermutationImportance(rf, random_state=SEED).fit(X_rfe, y)\neli5.show_weights(perm, feature_names = X_rfe.columns.tolist())","70d4b23f":"explainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X_rfe)\nshap.summary_plot(shap_values, X_rfe, plot_type=\"bar\")","147d0331":"\nshap.summary_plot(shap_values, X_rfe)","67b6bb11":"# Function to calculate mean absolute error\ndef mae(y_true, y_pred):\n    return np.mean(abs(y_true - y_pred))\n\nbaseline_guess = np.median(y)\n\nprint('The baseline guess based on Y-median value %0.2f' % baseline_guess)\nprint(\"Baseline Performance based on Y-median value: MAE = %0.4f\" % mae(y_test, baseline_guess))\nprint(\"Baseline Performance based on Random Forest RFECV-model: MAE = %0.4f\" % mae(y_test, rf.predict(X_test[features_rfecv])))","1b31ae7d":"So we got list of 4 best features with the same model score","2295efd3":" Some preliminary conclusions:\n  - no missing values in dataset\n  - 8 object features, rest - binary\n  - 12 features with 1 unique value","72edb64e":"So we can spend our time taking in account only this 4 features during car tests"}}