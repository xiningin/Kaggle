{"cell_type":{"7ccf31b2":"code","2613d656":"code","2f1c70ed":"code","f17a1fd1":"code","c227056d":"code","9d7b7952":"code","8a8fa46b":"code","b6c8c0d3":"code","b766eb78":"code","37b3d4b5":"code","c9d2011d":"code","d90c38b3":"code","ff4d0095":"code","278251a9":"code","abc675aa":"code","6fc92970":"code","1e123282":"markdown","95d5b0c8":"markdown","ddf14e2d":"markdown","0bff446a":"markdown","09b62d0c":"markdown"},"source":{"7ccf31b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2613d656":"import os # accessing directory structure\nimport pandas as pd\n\nreddit_200k_test = pd.read_csv(\"..\/input\/rscience-popular-comment-removal\/reddit_200k_test.csv\", delimiter=',', encoding = 'ISO-8859-1')\nreddit_200k_train = pd.read_csv(\"..\/input\/rscience-popular-comment-removal\/reddit_200k_train.csv\", delimiter=',', encoding = 'ISO-8859-1')\nreddit_test = pd.read_csv(\"..\/input\/rscience-popular-comment-removal\/reddit_test.csv\", delimiter=',', encoding = 'ISO-8859-1')\nreddit_train = pd.read_csv(\"..\/input\/rscience-popular-comment-removal\/reddit_train.csv\", delimiter=',', encoding = 'ISO-8859-1')\n\nprint(reddit_test.head())\nreddit_200k_test.head()\nprint('\\t reddit_200k_test')        # 55843 entries, 8 columns\nreddit_200k_test.info()\nprint('\\t reddit_200k_train')       # 167529 entries, 8 columns\nreddit_200k_train.info()\nprint('\\t reddit_test')\nreddit_test.info()                      # 7111 entries, 4 columns\nprint('\\t reddit_train')\nreddit_train.info()                      #21336 entries, 4 columns\n","2f1c70ed":"# Clean up unnecessary columns\nreddit_train = reddit_train.drop(columns=\"Unnamed: 0\")\nreddit_train = reddit_train.drop(columns=\"X\")\nreddit_test = reddit_test.drop(columns=\"Unnamed: 0\")\nreddit_test = reddit_test.drop(columns=\"X\")\n","f17a1fd1":"# Check composition of YES & NO\ngap_reddit_test= len(reddit_test[reddit_test['REMOVED'] == 0])- len(reddit_test[reddit_test['REMOVED'] == 1])\nprint(\"reddit_test :  0 vs 1\")\nprint(len(reddit_test[reddit_test['REMOVED'] == 0]),\" vs \", len(reddit_test[reddit_test['REMOVED'] == 1]), \" = \",gap_reddit_test)\nprint(\"\")\n\ngap_reddit_train= len(reddit_train[reddit_train['REMOVED'] == 0])- len(reddit_train[reddit_train['REMOVED'] == 1])\nprint(\"reddit_train : 0 vs 1\")\nprint(len(reddit_train[reddit_train['REMOVED'] == 0]),\" vs \", len(reddit_train[reddit_train['REMOVED'] == 1]),\" = \", gap_reddit_train)\n","c227056d":"# Delete rows to get balanced dataset\nreddit_train = reddit_train.sort_values(by=['REMOVED'])\nreddit_train = reddit_train.iloc[gap_reddit_train:,]\nprint(\"reddit_train : 0 vs 1\")\nprint(len(reddit_train[reddit_train['REMOVED'] == 0]),\" vs \", len(reddit_train[reddit_train['REMOVED'] == 1]))\n\nreddit_test = reddit_test.sort_values(by=['REMOVED'])\nreddit_test = reddit_test.iloc[gap_reddit_test:,]\nprint(\"reddit_test : 0 vs 1\")\nprint(len(reddit_test[reddit_test['REMOVED'] == 0]),\" vs \", len(reddit_test[reddit_test['REMOVED'] == 1]))\n","9d7b7952":"# Convert Pandas to Numpy\nreddit_test_numpy=reddit_test.to_numpy()\nprint(\"reddit_test_numpy.shape= \",reddit_test_numpy.shape)\nreddit_train_numpy=reddit_train.to_numpy()\nprint(\"reddit_train_numpy.shape= \",reddit_train_numpy.shape)","8a8fa46b":"#Collect Key Metrics in the training dataset\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsample_texts = reddit_train_numpy[:,0]\n\nnum_words = [len(s.split()) for s in sample_texts]\n\nprint(\"max = \",np.max(num_words))\nprint(\"min = \",np.min(num_words))\nprint(\"median = \",np.median(num_words)) # we can also get this from explore_data.py","b6c8c0d3":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","b766eb78":"from shutil import copyfile\ncopyfile(src = \"..\/input\/utilities\/util_text_classification.py\", dst = \"..\/working\/util_text_classification.py\")\n\nfrom util_text_classification import *\nplot_frequency_distribution_of_ngrams(sample_texts, ngram_range=(1, 2), num_ngrams=50)\nplot_sample_length_distribution(sample_texts)\n","37b3d4b5":"#Calculate ratio for model selection (no of samples\/no words per sample)\n\nratio = reddit_train_numpy.shape[0]\/np.median(num_words)\nprint(ratio)   # 320","c9d2011d":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nimport category_encoders as ce\nfrom sklearn.preprocessing import OneHotEncoder\n\nX_train = reddit_train_numpy[:,0]\nX_test = reddit_test_numpy[:,0]\ny_train=reddit_train_numpy[:,1]\ny_test=reddit_test_numpy[:,1]\n\nX= np.concatenate(( reddit_train_numpy[:,0],  reddit_test_numpy[:,0]))\nprint(X_train.shape, \"+\", X_test.shape, \" = \", X.shape)\n\nX_train_ngram, X_test_ngram = ngram_vectorize(X_train, y_train, X_test)\nprint(\"X_train_ngram.shape= \", X_train_ngram.shape)\nprint(\"X_test_ngram.shape= \", X_test_ngram.shape)\n\n#Categorical encoding is done after vectorization to avoid error on the method 'ngram_vectorize'\n# 0 = 1 0\n# 1 = 0 1\nohe =  ce.OneHotEncoder(handle_unknown='ignore')\ny_train=ohe.fit_transform(reddit_train_numpy[:,1])\ny_test=ohe.fit_transform(reddit_test_numpy[:,1])\nprint(y_train)\nprint(\"transformed into :\")\nprint(reddit_train_numpy[0:4,1])\nprint(reddit_train_numpy[13709:13713,1])","d90c38b3":"# Rename input & labels\nprint(type(X_train_ngram))\n\nX_train_ngram = X_train_ngram.todense()\n\nprint(type(X_train_ngram))\nprint(X_train_ngram.shape)\n\ninput_size=X_train_ngram.shape[1]","ff4d0095":"#Shallow NN\nfrom keras import layers, models, optimizers, losses\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.metrics import categorical_accuracy\n# ---------------------------------\n\n\n#Due to big input dimension, do not use lots of nodes & layers to avoid overfitting\nnet = Sequential()\nnet.add(Dense(1, input_dim=input_size, activation='relu'))\nnet.add(Dense(2, activation='softmax' ))\n\nnet.compile(loss='categorical_crossentropy' , optimizer=optimizers.Adam(), metrics=['accuracy'] )\n\nnet.summary()\nnet.fit(X_train_ngram, y_train, epochs=10, verbose=1)\nprint(\"training loss, acc: \" + str(net.evaluate(X_train_ngram, y_train, verbose=0)))\nprint(\"test loss, acc: \" + str(net.evaluate(X_test_ngram, y_test, verbose=0)))\n","278251a9":"import numpy as numpy\nfrom sklearn.metrics import accuracy_score\n\n# load the pre-trained word-embedding vectors \nembeddings_index = {}\nfor i, line in enumerate(open('..\/input\/wikinews300d1mvec\/wiki-news-300d-1M.vec')):\n    values = line.split()\n    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n\n# create a tokenizer \ntoken = text.Tokenizer()\n#token.fit_on_texts(trainDF['text'])    # change from pandas to numpy\ntoken.fit_on_texts(X_train)   # should be X_train + X_test\nword_index = token.word_index\n\n# convert text to sequence of tokens and pad them to ensure equal length vectors \nX_train_seq = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=70)\nX_test_seq = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=70)\nprint(\"X_train_seq.shape\", X_train_seq.shape)\nprint(\"X_test_seq.shape\", X_test_seq.shape)\n\n# create token-embedding mapping\nembedding_matrix = numpy.zeros((len(word_index) + 1, 300))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        \ndef create_rnn_lstm():\n    # Add an Input Layer\n    input_layer = layers.Input((70, ))\n\n    # Add the word embedding Layer\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n    # Add the LSTM Layer\n    lstm_layer = layers.LSTM(100)(embedding_layer)\n\n    # Add the output Layers\n    output_layer1 = layers.Dense(70, activation=\"relu\")(lstm_layer)\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\n    #output_layer12 = layers.Dense(70, activation=\"relu\")(output_layer1)\n    output_layer2 = layers.Dense(2, activation=\"softmax\")(output_layer1)\n\n    # Compile the model\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\n    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    \n    return model\n\nLSTM_classifier = create_rnn_lstm()\nLSTM_classifier.fit(X_train_seq, y_train, batch_size=128, epochs = 10, shuffle=True) # batch_size=64\nLSTM_classifier.summary()\npredictions = LSTM_classifier.predict(X_test_seq)\npredictions= np.argmax(predictions, axis=1)   # change from 2D array to 1D array\n\ny_test_int64=reddit_test_numpy[:,1].astype(int)  # change from numpy object to int64 as required by sklearn.accuracy\nprint (\"RNN-LSTM, Word Embeddings accuracy (Eout) = \",  accuracy_score(predictions, y_test_int64))\n\npredictions2 = LSTM_classifier.predict(X_train_seq)\npredictions2= np.argmax(predictions2, axis=1)   # change from 2D array to 1D array\n\ny_train_int64=reddit_train_numpy[:,1].astype(int)  # change from numpy object to int64 as required by sklearn.accuracy\nprint (\"RNN-LSTM, Word Embeddings accuracy (Ein)= \", accuracy_score(predictions2, y_train_int64))\n\n#Result will be bettter if number of epoch is increased but then started to overfit","abc675aa":"def create_rnn_gru():\n    # Add an Input Layer\n    input_layer = layers.Input((70, ))\n\n    # Add the word embedding Layer\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n    # Add the GRU Layer\n    lstm_layer = layers.GRU(100)(embedding_layer)\n\n    # Add the output Layers\n    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\n    output_layer2 = layers.Dense(2, activation=\"softmax\")(output_layer1)\n\n    # Compile the model\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\n    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    \n    return model\n\nGRU_classifier = create_rnn_gru()\nGRU_classifier.fit(X_train_seq, y_train, batch_size=128, epochs = 10, shuffle=True)\nGRU_classifier.summary()\n\npredictions = GRU_classifier.predict(X_test_seq)\npredictions= np.argmax(predictions, axis=1)   # change from 2D array to 1D array\n\npredictions2 = GRU_classifier.predict(X_train_seq)\npredictions2= np.argmax(predictions2, axis=1)   # change from 2D array to 1D array\n\nprint (\"RNN-GRU, Word Embeddings accuracy (Ein)= \", accuracy_score(predictions2, y_train_int64))\nprint (\"RNN-GRU, Word Embeddings accuracy (Eout) = \",  accuracy_score(predictions, y_test_int64))","6fc92970":"def create_bidirectional_rnn():\n    # Add an Input Layer\n    input_layer = layers.Input((70, ))\n\n    # Add the word embedding Layer\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n    # Add the LSTM Layer\n    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n\n    # Add the output Layers\n    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\n    output_layer2 = layers.Dense(2, activation=\"sigmoid\")(output_layer1)\n\n    # Compile the model\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\n    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    \n    return model\n\n#-------------------------------------------------\nBidirectional_classifier = create_bidirectional_rnn()\nBidirectional_classifier.fit(X_train_seq, y_train, batch_size=2000, epochs = 10, shuffle=True)\nBidirectional_classifier.summary()\n\npredictions = Bidirectional_classifier.predict(X_test_seq)\npredictions= np.argmax(predictions, axis=1)   # change from 2D array to 1D array\n\npredictions2 = Bidirectional_classifier.predict(X_train_seq)\npredictions2= np.argmax(predictions2, axis=1)   # change from 2D array to 1D array\n\nprint (\"RNN-Bidirectional, Word Embeddings accuracy (Ein)= \", accuracy_score(predictions2, y_train_int64))\nprint (\"RNN-Bidirectional, Word Embeddings accuracy (Eout) = \",  accuracy_score(predictions, y_test_int64))","1e123282":"We'll use the smaller dataset of 30k (`reddit_test` & `reddit_train`).\n\nWe will only need 2 columns (`BODY` & `REMOVED`)","95d5b0c8":"Training with Bidirectional RNN","ddf14e2d":"<font size=\"3\">**Training with LSTM**<\/font>","0bff446a":"**Training with GRU**","09b62d0c":"<font size=\"3\">**Text classification of 2 classes using various type of Neural Network**<\/font>\nResult:\n\n**Shallow NN**\n    <p> training loss, acc: [0.006, 0.99] => Overfit <\/p>\n    <p>  test loss, acc: [2.4, 0.72]<\/p>\n**LSTM**\n    <p>Training Accuracy 72%, Testing Accuracy 71% <\/p>\n    <p>Result will be bettter if number of epoch is increased but then start to overfit <\/p>\n**GRU**\n    <p>Training Accuracy=  0.76,  Testing Accuracy =  0.73 <\/p>\n**RNN-Bidirectional**\n<p> Trainings accuracy =  0.77, Test accuracy =  0.72 <\/p>\n\nReference :\nhttps:\/\/developers.google.com\/machine-learning\/guides\/text-classification\/step-2\n"}}