{"cell_type":{"07cbcb11":"code","d9cd32a6":"code","4922c17c":"code","3f487e98":"code","fe9f8ebd":"code","85008b50":"code","d983a304":"code","b0134b17":"code","f63263f6":"code","a50c610f":"code","56fd0753":"code","1ad63b45":"code","141c773f":"code","e835be2a":"code","a29ebfbc":"code","1501fc8f":"code","958eb1fc":"code","96715fcf":"code","b3e927a4":"code","c1f202ae":"code","57206f29":"code","0696e1c4":"code","426f3bae":"code","c287290a":"code","feaeafce":"markdown","f70cc325":"markdown","98938b69":"markdown","51091f6b":"markdown","64209097":"markdown","8931703f":"markdown","a68ec563":"markdown","ff5bb89e":"markdown","db85b7e4":"markdown","a77f2fd0":"markdown","c5d646c2":"markdown","fce97004":"markdown","170d8a90":"markdown","0511805b":"markdown","932ba6fc":"markdown","f8eea0e1":"markdown","d88a9149":"markdown","2f206dea":"markdown","cda6c079":"markdown","b2b737b0":"markdown","6110a8c5":"markdown","a1dfd212":"markdown"},"source":{"07cbcb11":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score","d9cd32a6":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","4922c17c":"df = pd.read_csv('\/kaggle\/input\/detecting-anomalies-in-wafer-manufacturing\/Train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/detecting-anomalies-in-wafer-manufacturing\/Train.csv')","3f487e98":"df.info()","fe9f8ebd":"duplicate=df.drop('Class',axis=1).T.drop_duplicates().T.columns\nClass = df['Class']\ndf_test = df_test[duplicate]\ndf = df[duplicate]\ndf['Class'] = Class","85008b50":"df.info()","d983a304":"df.head()","b0134b17":"sns.set(style=\"whitegrid\")\nax = sns.boxplot(x=\"Class\", y=\"feature_1\",data=df)","f63263f6":"ax = sns.boxplot(x=\"Class\", y=\"feature_2\",data=df)","a50c610f":"ax = sns.boxplot(x=\"Class\", y=\"feature_3\",data=df)","56fd0753":"pd.DataFrame(df[['feature_1','feature_2','feature_3','Class']].corr()['Class'])\n","1ad63b45":"zero = pd.DataFrame((df == 0).astype(int).sum(axis=0))","141c773f":"zero","e835be2a":"all_zero = zero[zero[0]>1761].index","a29ebfbc":"df.drop(all_zero,axis=1,inplace=True)","1501fc8f":"df_test.drop(all_zero,axis=1,inplace=True)","958eb1fc":"df.info()","96715fcf":"X = df.drop('Class',axis=1)\ny = df['Class'].values","b3e927a4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","c1f202ae":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier(silent=True,\n                      booster = 'gbtree',\n                      scale_pos_weight=5,\n                      learning_rate=0.01,  \n                      colsample_bytree = 0.7,\n                      subsample = 0.5,\n                      max_delta_step = 3,\n                      reg_lambda = 2,\n                     objective='binary:logistic',\n                      \n                      n_estimators=818, \n                      max_depth=8,\n                     )\n\neval_set = [(X_test, y_test)]\neval_metric = [\"logloss\"]\n%time model.fit(X_train, y_train,early_stopping_rounds=50, eval_metric=eval_metric, eval_set=eval_set)\n","57206f29":"predictions = model.predict_proba(X_test)[:,-1]","0696e1c4":"roc_auc_score(y_test, predictions)","426f3bae":"import matplotlib.pyplot as plt     \nmodel.feature_importances_\nfrom matplotlib import pyplot\nfrom xgboost import plot_importance\nfig, ax = plt.subplots(figsize=(18,10))\nplot_importance(model, max_num_features=35, height=0.8, ax=ax)\npyplot.show()","c287290a":"p1 = model.predict_proba(df_test)[:,-1]\nsubmission = pd.DataFrame(p1)\nsubmission = submission.rename(columns={0: \"Class\"})\nsubmission.index = submission['Class']\nsubmission.drop('Class',axis=1,inplace=True)\n#submission.to_csv('submissiom.csv',header=True, index=True)","feaeafce":"### Feature Importance","f70cc325":"Almost half of the columns were just duplicates. We just have half of the columns remained. Keeping the duplicate columns would have let to a less interpretable and slower model.","98938b69":"## Overview","51091f6b":"This dataframe gives us the number of zeroes every feature has. So out of 1763 values if a column has 1762 or 1763 zeroes itself than it is a pretty bad feature as the model won't be able to capture any pattern out if it. So let's drop them.","64209097":"\n## Steps :\n1. Overview\n2. Importing Libraries\n3. Data Analysis and Data Processing\n4. Model Building and Evaluation\n5. Leaderboard Results\n   ","8931703f":"As it is a very high dimensional dataset it is very easy to overfit. That's why hyperparameter tuning played a huge role. Let's go in-depth with some of the hyperparameters:\n\nscale_pos_weight : To counter the imbalanced nature of the dataset\n\ncolsample_bytree, subsample, reg_lambda : To counter overfitting","a68ec563":"### Let's check the results on the leaderboard","ff5bb89e":"### Data Analysis and Data Processing","db85b7e4":"So now we have now successfully removed 7 more useless features","a77f2fd0":"### Importing Libraries","c5d646c2":"![image.png](attachment:image.png)","fce97004":"Let's check again.","170d8a90":"I was able to secure 5th rank in the Final leaderboard. Feature Selection and Hyperparameter Tuning played a key role for me. I hope you liked my simple yet effective approach.","0511805b":"On looking at the data carefully we can see that only 'feature_1', 'feature_2', and 'feature_3' columns are numerical. All others are binary. Let's analyze the numerical columns.","932ba6fc":"Let's work more eliminating useless features. Let's check if there are any columns which only consists of zeroes.","f8eea0e1":"### Making Predictions and Submitting the results","d88a9149":"So we are getting an auc score of 0.93!!","2f206dea":"Detecting Anomalies can be a difficult task and especially in the case of labeled datasets due to some level of human bias introduced while labeling the final product as anomalous or good. These giant manufacturing systems need to be monitored every 10 milliseconds to capture their behavior which brings in lots of information and what we call the Industrial IoT (IIOT). Also, hardly a manufacturer wants to create an anomalous product. Hence, the anomalies are like a needle in a haystack which renders the dataset that is significantly Imbalanced. \n\nThe dataset collected was anonymized to hide the feature names, also there are 1558 features that would require some serious domain knowledge to understand them. \n\nLink: https:\/\/www.machinehack.com\/hackathons\/detecting_anomalies_in_wafer_manufacturing_weekend_hackathon_18","cda6c079":"We can say that these features definitely seem to be important for the model.","b2b737b0":"As we can see that this is a high dimensionality dataset. Let's drop the duplicate columns.","6110a8c5":"### Model Building","a1dfd212":"### Model Evaluation"}}