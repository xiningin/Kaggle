{"cell_type":{"1f316e13":"code","770dc14f":"code","fc9bce42":"code","8fe31f7a":"code","acdbf3fe":"code","e3038424":"code","58028f1d":"code","057bd931":"code","483e2e74":"code","1962e5e4":"code","0582ec83":"code","846d1bd1":"code","4d943da5":"code","cfead272":"code","3790e695":"code","689d8dff":"code","fd2ca825":"code","cf4397e9":"code","3b81ed23":"code","246869a4":"code","663cf8e9":"code","3ab0dbaf":"code","62df4992":"code","74156a00":"code","c9eae4f1":"code","5a717fc7":"code","5103ef26":"markdown","57af1e60":"markdown","be68f07b":"markdown","fc21d6a9":"markdown","87b6b156":"markdown","a8817043":"markdown","6b77136d":"markdown","c14cc57a":"markdown","7149c296":"markdown","e558e3c2":"markdown","2512951c":"markdown","015fd708":"markdown"},"source":{"1f316e13":"!wget \"https:\/\/raw.githubusercontent.com\/udacity\/deep-learning-v2-pytorch\/master\/convolutional-neural-networks\/conv-visualization\/data\/udacity_sdc.png\"","770dc14f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.image as mpimg\n\nimport cv2\nimport numpy as np\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\n# Any results you write to the current directory are saved as output.","fc9bce42":"\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# number of subprocesses to use for data loading\nnum_workers = 3 # lets try to parallelize the data loading\n# how many samples per batch to load\nbatch_size = 20\n# percentage of training set to use as validation\nvalid_size = 0.2\n\n# convert data to a normalized torch.FloatTensor\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n# choose the training and test datasets\ntrain_data = datasets.CIFAR10('input\/CIFAR', train=True,\n                              download=True, transform=transform)\ntest_data = datasets.CIFAR10('input\/CIFAR', train=False,\n                             download=True, transform=transform)\n\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n    num_workers=num_workers)\n# specify the image classes\nclasses = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck']","8fe31f7a":"import matplotlib.pyplot as plt\n# helper function to un-normalize and display an image\ndef imshow(img,label=None):\n    img = img \/ 2 + 0.5  # unnormalize\n    if label:\n        plt.title(classes[label])\n    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image","acdbf3fe":"dataiter = iter(train_loader)\nimages, labels = dataiter.next()","e3038424":"imshow(images[0],labels[0])","58028f1d":"# 3x3 array for edge detection\nsobel_y = np.array([[ -1, -2, -1], \n                   [ 0, 0, 0], \n                   [ 1, 2, 1]])\n# Filter the image using filter2D, which has inputs: (grayscale image, bit-depth, kernel)  \nfiltered_image = cv2.filter2D(images[0].numpy()[0], -1, sobel_y)\nplt.imshow(filtered_image, cmap='gray')","057bd931":"bgr_img = cv2.imread(\"udacity_sdc.png\") # we normalize the image to ly into 0-1 range so tat our filters and sGD will work better\ngray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY).astype(\"float32\")\/255\nplt.imshow(gray_img,cmap='gray')\ngray_img_tensor = torch.from_numpy(gray_img).unsqueeze(0).unsqueeze(1).float()","483e2e74":"right_V = np.array([[-1, -1, 1, 1],\n                    [-1, -1, 1, 1],\n                    [-1, -1, 1, 1],\n                    [-1, -1, 1, 1]])\nleft_V = -right_V\ndown_H = right_V.T\nup_H = -down_H\nfilters = np.array([right_V, left_V, down_H, up_H])","1962e5e4":"\"\"\"\nCite: taken from udacity\/deep-learning-v2-pytorch\n\"\"\"\ndef viz_filters(filters):\n    fig = plt.figure(figsize=(10, 5))\n    fig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)\n    for i in range(4):\n        ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n        ax.imshow(filters[i], cmap='gray')\n        ax.set_title('Filter %s' % str(i+1))\n        width, height = filters[i].shape\n        for x in range(width):\n            for y in range(height):\n                ax.annotate(str(filters[i][x][y]), xy=(y,x),\n                            horizontalalignment='center',\n                            verticalalignment='center',\n                            color='white' if filters[i][x][y]<0 else 'black')\nviz_filters(filters)","0582ec83":"import torch.nn.functional as F\n\nclass convtest(torch.nn.Module):\n    def __init__(self,weights):\n        super().__init__()\n        kernel_height,kernels_width=weights.shape[-2:] #the last two are the rows and columns of the kernels\n        #we input 1 \"feature map(image) 1 dimension\", we have 4 filters so we output 4 feature maps\n        self.convLayer1=nn.Conv2d(1, 4, kernel_size=(kernel_height, kernels_width), bias=False)\n        #we replace the weights with custom filters\n        self.convLayer1.weight = torch.nn.Parameter(weights)\n        # define a pooling layer\n        self.maxPool = nn.MaxPool2d(2, 2)#maxpooling with stride 2 and window of size 2x2 we speact to have the a half of the size o the input\n        self.averagePool=nn.AvgPool2d(2,2)#Avgpooling with stride 2 and window of size 2x2 we speact to have the a half of the size o the input\n    def forward(self,inputs):\n        convolved=self.convLayer1(inputs)\n        withActivation=F.relu(convolved)\n        maxPooled=self.maxPool(withActivation)\n        avgPooled=self.averagePool(withActivation)\n        return convolved,withActivation,maxPooled,avgPooled","846d1bd1":"weights = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)\nmodel = convtest(weights)\nmodel","4d943da5":"# helper function for visualizing the output of a given layer\n# default number of filters is 4\ndef viz_layer(layer, n_filters= 4,title=''):\n    fig = plt.figure(figsize=(20, 20))\n    fig.suptitle(title,y=0.6)\n    for i in range(n_filters):\n        ax = fig.add_subplot(1, n_filters, i+1, xticks=[], yticks=[])\n        # grab layer outputs\n        ax.imshow(np.squeeze(layer[0,i].data.numpy()), cmap='gray')\n        ax.set_title(f\"Output {i+1}\")","cfead272":"# plot original image\nplt.imshow(gray_img, cmap='gray')\n\nviz_filters(model.convLayer1.weight.squeeze(1).detach().int().numpy())\ngray_img_tensor = torch.from_numpy(gray_img).unsqueeze(0).unsqueeze(1)\n# get the convolutional layer (pre and post activation)\nconv_layer, activated_layer,maxPooled,avgPooled = model(gray_img_tensor)\n# visualize the output of a conv layer\nviz_layer(conv_layer,title='Conv Layer No Activation Function')\nviz_layer(activated_layer,title='Conv Layer with Activation Function')\nviz_layer(maxPooled,title='Max Pooling results')\nviz_layer(avgPooled,title='Avg Pooling results')\nprint(f\"Conv Layer Output Shape: {str(conv_layer.shape):>40}\")\nprint(f\"Activated Conv Output Shape: {str(activated_layer.shape):>36}\")\nprint(f\"MaxPooling Output Shape: {str(maxPooled.shape):>40}\")\nprint(f\"AveragePooling Output Shape: {str(avgPooled.shape):>36}\")\n\n","3790e695":"from tqdm.auto import tqdm,trange\nclass metrics():\n    def __init__(self):\n        self.loss=[]\n        self.accuracy=[]\n    def append(self,loss,accuracy):\n        self.loss.append(loss)\n        self.accuracy.append(accuracy)\n\nclass CIFARNET(torch.nn.Module):\n    def __init__(self,input_d,size):#size assuming H and W are the same (32x32)\n        super().__init__()\n        \"\"\"\n        the First Layer will have 16 kernels with size 3 \n        an stride of 1 to preserve te size and a padding of 1 to complete the missing pixels of the 3x3 kernels\n        \"\"\"\n        self.conv1=nn.Conv2d(input_d,16,kernel_size=3,stride=1,padding=1)#sees (32x32)x3 \n        self.conv2=nn.Conv2d(self.conv1.out_channels,32,kernel_size=3,stride=1,padding=1)#sees (16x16)x16 image tensor we've downsample the HxW with maxpooling a half\n        self.conv3=nn.Conv2d(self.conv2.out_channels,64,kernel_size=3,stride=1,padding=1)#sees (8x8)x32 image tensor  we've downsample the HxW with maxpooling a half\n        self.maxPooling=nn.MaxPool2d(2,2)#window size 2,stride 2 downsample the features height and weight to a half of their size\n        \"\"\"The fc network recieves a flatten tensor of size 64(feature maps)*height*width \n        where this H and w has been downsample by a factor of 2 each time we applied MaxPooling with stride 2\n        so the input image has a size of 32x32 pixels then the feature smaps of conv1 16x16, then 8x8 and finaly 4*4\n        \"\"\"\n        fc_input_f=self.conv3.out_channels*((size\/\/(self.maxPooling.stride**3))**2)\n        self.fc=nn.Sequential(#sees (4x4)x64 image tensor we've downsample the HxW with maxpooling a half\n                      nn.Linear(fc_input_f,512),\n                      nn.ReLU(),\n                      nn.Dropout(0.25),\n                      nn.Linear(512,10),\n                      nn.LogSoftmax(dim=1)\n                      )\n        \"\"\" we can also don't use activation an use crossentropy loss \n        but cross entropy loss applies logsoftmax and Nlloss so is more efficient to use Logsofmax \n        and NLLoss, and then just use torch.exp to get the actual probabilities\"\"\"\n    \n    def forward(self,inputs):\n        h_1=self.maxPooling(F.relu(self.conv1(inputs)))#feature maps by the conv1 max pooled (16x16)x16  \n        h_2=self.maxPooling(F.relu(self.conv2(h_1))) #feature maps by the conv2 max pooled (8x8)x32\n        h_3=self.maxPooling(F.relu(self.conv3(h_2))) #feature maps by the conv3 max pooled (4x4)x64  \n        h_3_flatten=h_3.view(-1,self.fc._modules['0'].in_features)# conv3 feature maps flattened (64 * 4 * 4) , we have already calculated in is the in-features of fc layer\n        return self.fc(h_3_flatten) #log probabilities for each class (we must use exp to get the actual probabilities\n    \n    \n    def fit(self,train_generator,val_generator,criterion,Optimizer,faccuracy,Epochs=10,device='cuda'):\n        self.to(device)\n        train_batches=len(train_generator)\n        val_batches=len(val_generator)\n        val_metrics,train_metrics = metrics(),metrics()\n        for epoch in trange(Epochs,desc='Epochs:'):\n            #Train steps\n            self.train()\n            train_accuracy,train_loss=0,0\n            for images,labels in tqdm(train_generator,desc='Train Steps:',leave=False):\n                images,labels=images.to(device),labels.to(device)\n                Optimizer.zero_grad()#clean the gradients of optimizer\n                logProbs=self.forward(images)#calculate logProbabilities \n                loss=criterion(logProbs,labels)#calculating loss\n                loss.backward()#Calculating loss gradient with respect the parameters\n                Optimizer.step()#Optimization step (backpropagation)\n                train_loss+=loss.item()\n                train_accuracy+=faccuracy(torch.exp(logProbs),labels).item()\n            train_metrics.append(train_loss\/train_batches,train_accuracy\/train_batches)\n            #Validation steps\n            self.eval()#turns off dropout \n            val_accuracy,val_loss=0,0\n            for images,labels in tqdm(val_generator,desc='Val Steps:',leave=False):\n                with torch.no_grad():\n                    images,labels=images.to(device),labels.to(device)\n                    logProbs=self.forward(images)\n                    val_loss+=criterion(logProbs,labels).item()\n                    val_accuracy+=faccuracy(torch.exp(logProbs),labels).item()\n            val_metrics.append(val_loss\/val_batches,val_accuracy\/val_batches)\n            print(f\"EPOCH: {epoch}\"\n                  f\"\\nTrain loss: {train_metrics.loss[-1]:.4f} Train accuracy: {train_metrics.accuracy[-1]:.4f}\"\n                  f\"\\nVal loss: {val_metrics.loss[-1]:.4f} Val accuracy: {val_metrics.accuracy[-1]:.4f}\")\n        return train_metrics,val_metrics","689d8dff":"images.shape","fd2ca825":"model=CIFARNET(images.shape[1],images.shape[2])\nprint(\"Model Description: \",model,\"\\nTest model\\n\",torch.sum(torch.exp(model(images)),dim=1))","cf4397e9":"import matplotlib.pyplot as plt\ndef plot_train_history(train_metrics,val_metrics):\n    train_loss,train_accuracy = train_metrics.loss,train_metrics.accuracy\n    val_loss,val_accuracy = val_metrics.loss,val_metrics.accuracy\n    plt.plot(train_loss, label='Training loss')\n    plt.plot(val_loss, label='Validation loss')\n    plt.legend(frameon=False)\n    plt.show()\n    plt.plot(train_accuracy, label='Training accuracy')\n    plt.plot(val_accuracy, label='Validation accuracy')\n    plt.legend(frameon=False)\n    plt.show()","3b81ed23":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef f_accuracy(predictions,labels):\n    top_p, top_class = predictions.topk(1, dim=1)\n    equals = top_class == labels.view(*top_class.shape)\n    return torch.mean(equals.type(torch.FloatTensor))\ncriterion=nn.NLLLoss()\nOptimizer=torch.optim.SGD(model.parameters(), lr=0.01)","246869a4":"train_metrics,val_metrics=model.fit(train_loader,valid_loader,criterion,Optimizer,f_accuracy)","663cf8e9":"plot_train_history(train_metrics,val_metrics)","3ab0dbaf":"from torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# number of subprocesses to use for data loading\nnum_workers = 3 # lets try to parallelize the data loading\n# how many samples per batch to load\nbatch_size = 20\n# percentage of training set to use as validation\nvalid_size = 0.2\n\n# let's perom data augmentation (sintetized samples)\ntransform_Augmented = transforms.Compose([\n    transforms.RandomRotation(10),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n# choose the training and test datasets\ntrain_data = datasets.CIFAR10('input\/CIFAR', train=True,\n                              download=True, transform=transform_Augmented) \n#we only apply data augmentation in our training set we didn't modify our test set cause we want ur test data to be the more similar to our real cases\ntest_data = datasets.CIFAR10('input\/CIFAR', train=False,\n                             download=True, transform=transform)\n\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader_aug = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\nvalid_loader_aug = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)\ntest_loader_aug = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n    num_workers=num_workers)","62df4992":"classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck']","74156a00":"dataiter = iter(train_loader_aug)\nimages, labels = dataiter.next()\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 4))\n# display 20 images\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20\/2, idx+1, xticks=[], yticks=[])\n    imshow(images[idx])\n    ax.set_title(classes[labels[idx]])","c9eae4f1":"model_aug=CIFARNET(images.shape[1],images.shape[2])\nOptimizer=torch.optim.SGD(model_aug.parameters(), lr=0.01)\ntrain_metrics_aug,val_metrics_aug=model_aug.fit(train_loader_aug,valid_loader_aug,criterion,Optimizer,f_accuracy,Epochs=28)","5a717fc7":"plot_train_history(train_metrics_aug,val_metrics_aug)","5103ef26":"To see it clear let's actually write a convolutional layer to see the outputs it's produce and also how the activation function affect the results\n\nSo let's say we have the following images and filters gray scale image","57af1e60":"CNNs bases their principles in Image feature extraction using Convolutional kernels let's see how this kernels works on and Image and then we will see how to contruct a CNN network using Pytorch","be68f07b":"## Visualizing Kernel effects\nlet's se how convolutions affects our Images ","fc21d6a9":"Let's train and test our mode","87b6b156":"Then we have 4 filters one for left vertical lines, other for right vertical lines and others 2 for horizontal lines","a8817043":"then let's see if our model valid loss and accuracy could be improved by using this resample technique","6b77136d":"we will create the layers, cast our filters to ","c14cc57a":"In the following cell you can be able to see how applying a particular filter (Convolutional kernel) will affect an image filtering out unrelevant features or emphasizing particular ones, in this case applying the sobel_y filter will emphasize horizontal lines and filtersout other features.\n\nfor edge detection it's important that all te elements of the kernel sum 0 in order to don't alterate the original brightness of the image, since we just want to detect the edges (high frequency regions in image (high-pass filter))\n\n- 0 no change\n- positive - more brighter\n- negative - less brighter","7149c296":"Then this is how our data looks now","e558e3c2":"then we will create a conv layer that uses this filters as conv kernels","2512951c":"## Building a CNN for the CIFAR dataset\nNow let's build a network to classify the images in the CIFAR dataset and see the kernels that our network will learn\n\nFor image classification tasks we usually want to increase the depth of our features but to decrease its size to create a bootleneck for compressing information obtaining a hicherical structure of the features going from general to particular\n\n- In our case we are going to **increase** by the double the number of kernels to get more feature maps trought each convLayer **16 features--> 32 features--> 64 features** \n- then we will **downsample** the features **HxW** by a half each layer using MaxPooling to extract the most important features for classification **size(32x32) --> size (16x16) --> size (8x8) --> size (4x4)**\n- At the end we will perfom the classification flattening our output feature maps and passing it throught a fully connected network so the input of the fc network will be \n$$n_{features} = depth*height*width$$\n\n    where depth is the number of feature maps coming and usually height and weight are the same value and could be calculated as\n   $$(height  | width)=\\frac{input_{(height|width)}}{P_{1}.stride*P_{2}.stride....*P_{n}.stride}$$\n  \n  **only when you mantain the same input-output size in each conv layer**\n  where P refers to the pooling layers applied until the desired output sequencially , remember pooling layers each time their are applied downsample our height|weight by a factor of the stride size (2 a half,1 maintain the same size, etc...)\n    ","015fd708":"## Data Augmentation  \nWe have several problems in this model the first one is that our maximum accuracy archieved whitout overfitting the model is around 70% that is not too god for this data set so there are several problems that could be happening in our. \n\nFirst we know that convNets have some properties by themselves for example they provide translation invariance (in a some way by the kernels working)  so they can found a pattern in an image (matrix) wherever they are but there are a lack of propierties that we need to correctly perfom our task (detect a kind of object and classify it) such as\n\n- Rotation Invariance (recognizes patterns no matter it rotation in the image)\n- Scale Invariance (recognizes patterns no metter it's size in the image)\n\nSo in order to produce better results and give our model rotation,scale and translation invariences propierties we can do several things one of the simplest way is just adding images that train our model to detect this patter in dificult situations so basically we perfom a preprocessing in our training set such as:\n- Image rotations: to give examples with diferent rotation\n- Shift images: to give examples where the object is not in a typical location\n- Scale the image: to give our image examples where the patterns appear in different scale(this is more harder so there is other ways to perfom this than Data Augmentation )\n\nData augmentation is considered a resampling techinique which helps to deal a lot of problems in ML algorithms in this case give our images better examples for classfication creating sintetic data.\n\n**we only apply data augmentation in our training set we didn't modify our test set cause we want ur test data to be the more similar to our real cases**\n"}}