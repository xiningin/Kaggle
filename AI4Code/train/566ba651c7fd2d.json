{"cell_type":{"de4bd9e1":"code","1a0609ba":"code","99e161a5":"code","5a946451":"code","9cfd498c":"code","b2ec812c":"code","05a9c553":"code","9c614882":"code","357a886e":"code","c5015e13":"code","45952bf7":"code","59f648d6":"code","f9d68bb8":"code","8bbc6fa7":"code","5612c728":"code","70fa34db":"code","49fc3f06":"code","5fd2b59c":"code","d6784dbd":"code","e1eeace3":"code","c1831243":"code","c0c7f621":"code","949c3174":"code","efc6a706":"code","fc6711bd":"markdown","febcb747":"markdown","da93cc55":"markdown","14f8a825":"markdown","89c417c0":"markdown","d843ee60":"markdown","894408e1":"markdown","4305b50a":"markdown","723ace96":"markdown","990faa40":"markdown","02a3a236":"markdown","0b3dfcc3":"markdown","1f6be5f4":"markdown","bad33ace":"markdown","e5273111":"markdown"},"source":{"de4bd9e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a0609ba":"import pandas as pd \nimport seaborn as sns\nimport nltk\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","99e161a5":"df_sample_sub = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/sample_submission.csv')\ndf_train = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')","5a946451":"# L\u1ea5y 10 d\u1eef li\u1ec7u \u0111\u1ea7u ti\u00ean c\u1ee7a t\u1eadp sample_sub\ndf_sample_sub.head(10)","9cfd498c":"# L\u1ea5y 10 d\u1eef li\u1ec7u \u0111\u1ea7u ti\u00ean c\u1ee7a t\u1eadp train\ndf_train.head(10)","b2ec812c":"# Hi\u1ec3n th\u1ecb th\u00f4ng tin t\u1eadp train\ndf_train.info()","05a9c553":"# \u0110\u1ebfm s\u1ed1 l\u01b0\u1ee3ng d\u1eef li\u1ec7u trong t\u1eadp train\ndf_train.target.value_counts()","9c614882":"import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nncount = df_train.shape[0]\n\nplt.figure(figsize=(7, 5))\n\nax = sns.countplot(data=df_train, x='target')\nplt.title('Bi\u1ec3u \u0111\u1ed3 ph\u00e2n ph\u1ed1i c\u00e2u h\u1ecfi')\n\nax2=ax.twinx()\n\nax2.yaxis.tick_left()\nax.yaxis.tick_right()\n\n# Also switch the labels over\nax.yaxis.set_label_position('right')\nax2.yaxis.set_label_position('left')\n\nax2.set_ylabel('Frequency [%]')\n\nfor p in ax.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax.annotate('{:.1f}%'.format(100.*y\/ncount), (x.mean(), y), \n            ha='center', va='bottom') \n\nax.yaxis.set_major_locator(ticker.LinearLocator(11))\n\n# Set frequency trong kho\u1ea3ng 0-100\nax2.set_ylim(0,100)\nax.set_ylim(0,ncount)\n\n# S\u1eed d\u1ee5ng MultipleLocator \u0111\u1ec3 kho\u1ea3ng c\u00e1ch gi\u1eefa t\u1eebng kho\u1ea3ng l\u00e0 10\nax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n\nax2.grid(None)","357a886e":"# T\u1ea3i xu\u1ed1ng stopwords trong ti\u1ebfng Anh\nnltk.download('stopwords')\nnltk_stopwords = stopwords.words('english')\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef lemSentence(sentence):\n    token_words = word_tokenize(sentence)\n    lem_sentence = []\n    for word in token_words:\n        lem_sentence.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\")) # X\u1eed l\u00fd c\u00e1c t\u1eeb b\u1ecb chia \u0111\u1ed9ng t\u1eeb chuy\u1ec3n v\u1ec1 nguy\u00ean th\u1ec3 v\u00ed d\u1ee5: was, were -> am.\n        lem_sentence.append(\" \")\n    return \"\".join(lem_sentence)","c5015e13":"def clean(message, lem = True):\n    # Lo\u1ea1i b\u1ecf d\u1ea5u c\u00e2u (v\u00ed d\u1ee5: c\u00e1c d\u1ea5u !\"#$%&'()*+, -.\/:;<=>?@[\\]^_`{|}~)\n    message = message.translate(str.maketrans('', '', string.punctuation))\n    \n    # Lo\u1ea1i b\u1ecf s\u1ed1\n    message = message.translate(str.maketrans('', '', string.digits))\n    \n    # Lo\u1ea1i b\u1ecf \"stopwords\"\n    message = [word for word in word_tokenize(message) if not word.lower() in nltk_stopwords]\n    message = ' '.join(message)\n    \n    if lem:\n        message = lemSentence(message)\n    \n    return message","45952bf7":"# Clean c\u00e1c c\u00e2u h\u1ecfi\ndf_train['question_text_cleaned'] = df_train.question_text.apply(lambda x: clean(x, True))","59f648d6":"from wordcloud import WordCloud\nfrom matplotlib import pyplot as plt\n\nplt.figure(figsize=(20, 20))\nwc = WordCloud(max_words=2000, height=800, width =1600, stopwords=nltk_stopwords).generate(\" \".join(df_train[df_train.target== 0].question_text_cleaned))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.title(\"Nh\u1eefng t\u1eeb th\u01b0\u1eddng xuy\u00ean xu\u1ea5t hi\u1ec7n trong c\u00e2u b\u00ecnh th\u01b0\u1eddng\")","f9d68bb8":"from wordcloud import WordCloud\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(20, 20))\nwc = WordCloud(max_words=2000, height=800, width =1600, stopwords=nltk_stopwords).generate(\" \".join(df_train[df_train.target== 1].question_text_cleaned))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.title(\"Nh\u1eefng t\u1eeb th\u01b0\u1eddng xuy\u00ean xu\u1ea5t hi\u1ec7n trong c\u00e2u toxic\")","8bbc6fa7":"df_train.head(10)","5612c728":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer()\nmodel = LogisticRegression(C=1, random_state=0)\n\nvectorize_model_pipeline = Pipeline([\n    ('count_vectorizer', count_vectorizer),\n    ('model', model)\n])","70fa34db":"X_train, X_test, y_train, y_test = train_test_split(df_train['question_text_cleaned'], df_train['target'], test_size = 0.3)\nvectorize_model_pipeline.fit(X_train, y_train)","49fc3f06":"predictions = vectorize_model_pipeline.predict(X_test)","5fd2b59c":"print('Accuracy :', accuracy_score(y_test, predictions))\nprint('F1 score :', accuracy_score(y_test, predictions))","d6784dbd":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, predictions))","e1eeace3":"# Cleaning the questions\ndf_test['question_text_cleaned'] = df_test.question_text.apply(lambda x: clean(x, True))","c1831243":"df_test['prediction'] = vectorize_model_pipeline.predict(df_test['question_text_cleaned'])","c0c7f621":"df_final = df_test[['qid','prediction']]\ndf_final.set_index('qid', inplace = True)","949c3174":"df_final.head(10)","efc6a706":"# Ghi l\u1ea1i k\u1ebft qu\u1ea3 ra file submission.csv\ndf_final.to_csv('submission.csv')","fc6711bd":"**D\u1eef li\u1ec7u sau khi x\u1eed l\u00fd \u0111\u00e3 \"s\u1ea1ch\" h\u01a1n so v\u1edbi ban \u0111\u1ea7u nh\u01b0 b\u1ea3ng d\u01b0\u1edbi**","febcb747":"**Ta xem th\u1eed t\u1eebng lo\u1ea1i d\u1eef li\u1ec7u s\u1ebd hi\u1ec3n th\u1ecb d\u1ea1ng ra sao**","da93cc55":"# M\u00f4 t\u1ea3 b\u00e0i to\u00e1n\n> ***B\u00e0i to\u00e1n \u0111\u1eb7t ra l\u00e0 v\u1edbi c\u00e1c c\u00e2u h\u1ecfi tr\u00ean Quora c\u00f3 ph\u1ea3i l\u00e0 c\u00e2u h\u1ecfi toxic hay kh\u00f4ng (Quora Insincere Questions Classification) ?***\n* Input: C\u00e1c c\u00e2u h\u1ecfi d\u01b0\u1edbi d\u1ea1ng text.\n* Output: Yes(1) or No(0) ?\n","14f8a825":"# 1. X\u1eed l\u00fd d\u1eef li\u1ec7u","89c417c0":"**D\u1eef li\u1ec7u v\u00e0o g\u1ed3m c\u00e1c question_text l\u00e0 ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean (c\u1ee5 th\u1ec3 l\u00e0 d\u1ea1ng text). N\u1ebfu \u0111\u1ec3 nguy\u00ean, th\u00ec s\u1ebd kh\u00f3 \u0111\u1ec3 train. V\u00ec v\u1eady ch\u00fang ta c\u1ea7n x\u1eed l\u00fd d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o(question_text) tr\u01b0\u1edbc khi ti\u1ebfn h\u00e0nh train.**\n> C\u1ee5 th\u1ec3 ta s\u1ebd ph\u1ea3i x\u1eed l\u00fd d\u1ea5u c\u00e2u, s\u1ed1, c\u00e1c \"stopword\", t\u1eeb \u0111\u1ed3ng ngh\u0129a v\u00e0 c\u00e1c t\u1ef1 b\u1ecb chia trong ti\u1ebfng anh ch\u00fang ta c\u1ea7n x\u1eed l\u00fd l\u1ea1i.","d843ee60":"# K\u1ebft qu\u1ea3 n\u1ed9p b\u00e0i tr\u00ean kaggle:","894408e1":"![image.png](attachment:07d1f159-f71c-4f98-9ad3-a823dadf90cd.png)","4305b50a":"**\u0110\u1ecdc d\u1eef li\u1ec7u v\u00e0o t\u1eeb \u0111\u1ecbnh d\u1ea1ng csv**\n\n**D\u1eef li\u1ec7u v\u00e0o g\u1ed3m 3 t\u1eadp: **\n\n* Sample submission\n* Train\n* Test\n","723ace96":"**M\u1eb7c d\u00f9 ta \u0111\u00e3 x\u1eed l\u00fd d\u1eef li\u1ec7u \u1edf tr\u00ean tuy nhi\u00ean, d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o v\u1eabn l\u00e0 d\u1ea1ng Text.\n\u0110\u1ec3 s\u1eed d\u1ee5ng d\u1eef li\u1ec7u v\u0103n b\u1ea3n cho m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n, v\u0103n b\u1ea3n ph\u1ea3i \u0111\u01b0\u1ee3c m\u00e3 h\u00f3a. \u0110\u1ea7u v\u00e0o c\u1ea7n \u0111\u01b0\u1ee3c m\u00e3 h\u00f3a d\u01b0\u1edbi d\u1ea1ng s\u1ed1 nguy\u00ean ho\u1eb7c gi\u00e1 tr\u1ecb d\u1ea5u ph\u1ea9y \u0111\u1ed9ng, \u0111\u1ec3 s\u1eed d\u1ee5ng l\u00e0m \u0111\u1ea7u v\u00e0o trong thu\u1eadt to\u00e1n h\u1ecdc m\u00e1y. Qu\u00e1 tr\u00ecnh n\u00e0y \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 tr\u00edch xu\u1ea5t \u0111\u1eb7c tr\u01b0ng (ho\u1eb7c vect\u01a1 h\u00f3a).**\n\n> Scikit-learning CountVectorizer \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 chuy\u1ec3n \u0111\u1ed5i m\u1ed9t b\u1ed9 s\u01b0u t\u1eadp c\u00e1c t\u00e0i li\u1ec7u v\u0103n b\u1ea3n th\u00e0nh m\u1ed9t vect\u01a1 c\u00f3 s\u1ed1 l\u01b0\u1ee3ng thu\u1eadt ng\u1eef \/ m\u00e3 th\u00f4ng b\u00e1o. N\u00f3 c\u0169ng cho ph\u00e9p x\u1eed l\u00fd tr\u01b0\u1edbc d\u1eef li\u1ec7u v\u0103n b\u1ea3n tr\u01b0\u1edbc khi t\u1ea1o bi\u1ec3u di\u1ec5n vect\u01a1. Ch\u1ee9c n\u0103ng n\u00e0y l\u00e0m cho n\u00f3 tr\u1edf th\u00e0nh m\u1ed9t m\u00f4-\u0111un bi\u1ec3u di\u1ec5n t\u00ednh n\u0103ng r\u1ea5t linh ho\u1ea1t cho v\u0103n b\u1ea3n.\n\n**T\u00f3m g\u1ecdn l\u1ea1i ta s\u1ebd s\u1eed d\u1ee5ng CountVectorizer \u0111\u1ec3 chuy\u1ec3n d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o t\u1eeb \u0111\u1ecbnh d\u1ea1ng Text th\u00e0nh Vect\u01a1****","990faa40":"**\u0110\u1ec3 x\u1eed l\u00fd d\u1eef li\u1ec7u v\u00e0o, ta s\u1eed d\u1ee5ng Natural Language Toolkit(NLTK) - l\u00e0 b\u1ed9 c\u00f4ng c\u1ee5 ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean, l\u00e0 m\u1ed9t th\u01b0 vi\u1ec7n \u0111\u01b0\u1ee3c vi\u1ebft b\u1eb1ng Python h\u1ed7 tr\u1ee3 x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean. B\u1eb1ng c\u00e1ch cung c\u1ea5p c\u00e1c c\u01a1 ch\u1ebf v\u00e0 k\u1ef9 thu\u1eadt x\u1eed l\u00fd ng\u00f4n ng\u1eef ph\u1ed5 bi\u1ebfn, n\u00f3 gi\u00fap cho vi\u1ec7c x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean tr\u1edf l\u00ean d\u1ec5 d\u00e0ng, nhanh ch\u00f3ng h\u01a1n v\u00e0 c\u00f3 t\u00e1c d\u1ee5ng l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u, x\u1eed l\u00fd d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o cho c\u00e1c thu\u1eadt to\u00e1n Machine Learning.**\n\n**Ngo\u00e0i ra c\u00f2n gi\u00fap x\u1eed l\u00fd c\u00e1c stopwords - l\u00e0 c\u00e1c t\u1eeb c\u00f3 t\u1ea7n s\u1ed1 xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u01b0 the, to... c\u00e1c t\u1eeb n\u00e0y th\u01b0\u1eddng mang \u00edt gi\u00e1 tr\u1ecb \u00fd ngh\u0129a v\u00e0 kh\u00f4ng kh\u00e1c nhau nhi\u1ec1u trong c\u00e1c v\u0103n b\u1ea3n kh\u00e1c nhau. V\u00ed d\u1ee5 t\u1eeb \"the\" hay \"to\" th\u00ec \u1edf v\u0103n b\u1ea3n n\u00e0o n\u00f3 c\u0169ng kh\u00f4ng b\u1ecb thay \u0111\u1ed5i v\u1ec1 \u00fd ngh\u0129a. \nV\u00ed d\u1ee5: i, the, my, me, ...**","02a3a236":"# 3. \u0110\u00e1nh gi\u00e1, k\u1ebft lu\u1eadn v\u00e0 c\u00e1c nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a m\u00f4 h\u00ecnh h\u1ed3i quy Logistic\n**K\u1ebft qu\u1ea3 khi th\u1ef1c hi\u1ec7n m\u00f4 h\u00ecnh Logistic Regression:**\n\nRun: **983.4s**\n\nPrivate Score: **0.52384**\n\nPublic Score: **0.52270**\n\n**Logistic Regression th\u1ef1c ra \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng nhi\u1ec1u trong c\u00e1c b\u00e0i to\u00e1n Classification.**\n\n**Nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a m\u00f4 h\u00ecnh: **\n> + Ph\u1ee5 thu\u1ed9c v\u00e0o m\u1ee9c \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a ngu\u1ed3n th\u00f4ng tin thu nh\u1eadp hay n\u00f3i c\u00e1ch kh\u00e1c l\u00e0 t\u1eadp d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o.\n> + H\u1ed3i quy logistic c\u0169ng d\u1ec5 b\u1ecb overfitting. N\u00f3 kh\u00f4ng th\u1ec3 \u0111\u01b0\u1ee3c \u00e1p d\u1ee5ng cho m\u1ed9t b\u00e0i to\u00e1n phi tuy\u1ebfn t\u00ednh, ho\u1ea1t \u0111\u1ed9ng k\u00e9m v\u1edbi c\u00e1c bi\u1ebfn \u0111\u1ed9c l\u1eadp kh\u00f4ng t\u01b0\u01a1ng quan v\u1edbi m\u1ee5c ti\u00eau v\u00e0 t\u01b0\u01a1ng quan v\u1edbi nhau. Do \u0111\u00f3, b\u1ea1n s\u1ebd ph\u1ea3i \u0111\u00e1nh gi\u00e1 c\u1ea9n th\u1eadn m\u1ee9c \u0111\u1ed9 ph\u00f9 h\u1ee3p c\u1ee7a h\u1ed3i quy logistic \u0111\u1ed1i v\u1edbi v\u1ea5n \u0111\u1ec1 m\u00e0 b\u1ea1n \u0111ang c\u1ed1 g\u1eafng gi\u1ea3i quy\u1ebft.\n> + B\u1ea3n ch\u1ea5t m\u00f4 h\u00ecnh Logistic l\u00e0 m\u00f4 h\u00ecnh kinh t\u1ebf l\u01b0\u1ee3ng, v\u00ec v\u1eady khi h\u1ec7 s\u1ed1 x\u00e1c \u0111\u1ecbnh \u1edf m\u1ee9c nh\u1ecf th\u00ec m\u00f4 h\u00ecnh c\u00f3 th\u1ec3 d\u1ef1 b\u00e1o k\u00e9m ch\u00ednh x\u00e1c (th\u1ec3 hi\u1ec7n qua c\u00e1c gi\u00e1 tr\u1ecb ph\u1ea7n d\u01b0).","0b3dfcc3":"****T\u1ec9 l\u1ec7 ch\u00eanh l\u1ec7ch l\u1edbn v\u00e0 c\u00e1c c\u00e2u h\u1ecfi toxic v\u1eabn xu\u1ea5t hi\u1ec7n kh\u00e1 nhi\u1ec1u chi\u1ebfm t\u1edbi 80810 c\u00e2u trong t\u1eadp d\u1eef li\u1ec7u.****\n\n**S\u1eed d\u1ee5ng Word Cloud \u0111\u1ec3 hi\u1ec3n th\u1ecb c\u00e1c t\u1eeb th\u01b0\u1eddng xuy\u00ean xu\u1ea5t hi\u1ec7n trong c\u00e2u h\u1ecfi b\u00ecnh th\u01b0\u1eddng v\u00e0 c\u00e2u h\u1ecfi toxic.**","1f6be5f4":"# Th\u1ef1c hi\u1ec7n b\u00e0i to\u00e1n theo c\u00e1c b\u01b0\u1edbc:\n**1. X\u1eed l\u00fd d\u1eef li\u1ec7u**\n> Do d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o l\u00e0 text n\u00ean ch\u00fang ta c\u1ea7n x\u1eed l\u00fd tr\u01b0\u1edbc khi train\n* lo\u1ea1i b\u1ecf d\u1ea5u c\u00e2u, s\u1ed1, stop_words,...\n* sau \u0111\u00f3 x\u1eed l\u00fd l\u1ea1i ngh\u0129a c\u1ee7a t\u1eeb\n\n**2. Training d\u1eef li\u1ec7u v\u1edbi m\u00f4 h\u00ecnh Logistic Regression**\n> \u0110\u1ed1i v\u1edbi b\u00e0i to\u00e1n n\u00e0y, ban \u0111\u1ea7u ta s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh **Logistic Regression** \n> M\u00f4 h\u00ecnh n\u00e0y gi\u1ed1ng v\u1edbi **Linear Regression** \u1edf kh\u00eda c\u1ea1nh \u0111\u1ea7u ra l\u00e0 s\u1ed1 th\u1ef1c, v\u00e0 gi\u1ed1ng v\u1edbi **PLA** \u1edf vi\u1ec7c \u0111\u1ea7u ra b\u1ecb ch\u1eb7n (trong \u0111o\u1ea1n 0 -> 1). M\u1eb7c d\u00f9 trong t\u00ean c\u00f3 ch\u1ee9a t\u1eeb \"regression\", tuy v\u1eady **Logistic Regression** th\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng nhi\u1ec1u cho c\u00e1c b\u00e0i to\u00e1n **classification**. Do v\u1eady, ban \u0111\u1ea7u ta l\u1ef1a ch\u1ecdn **Logistic Regression** cho b\u00e0i to\u00e1n n\u00e0y. Sau \u0111\u00f3 ta s\u1ebd l\u1ef1a ch\u1ecdn m\u1ed9t m\u00f4 h\u00ecnh kh\u00e1c \u0111\u1ec3 mong \u0111\u1ea1t \u0111\u01b0\u1ee3c k\u1ebft qu\u1ea3 cao h\u01a1n.\n\n**3. \u0110\u00e1nh gi\u00e1 v\u00e0 nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a m\u00f4 h\u00ecnh Training LR**","bad33ace":"**\u0110\u1ed1i v\u1edbi t\u1eadp d\u1eef li\u1ec7u Train**\n**T\u1eadp train g\u1ed3m 3 c\u1ed9t: qid, question_text v\u00e0 target**\n\n**C\u00f3 1225312 c\u00e2u h\u1ecfi b\u00ecnh th\u01b0\u1eddng v\u00e0 80810 c\u00e2u h\u1ecfi Toxic**\n\n**V\u00e0 bi\u1ec3u \u0111\u1ed3 d\u01b0\u1edbi \u0111\u00e2y th\u1ec3 hi\u1ec7n s\u1ef1 ch\u00eanh l\u1ec7ch % gi\u1eefa 2 lo\u1ea1i.**","e5273111":"# 2. Training d\u1eef li\u1ec7u v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic (Logistic Regression)\n> **Nh\u01b0 \u0111\u00e3 tr\u00ecnh b\u00e0y \u1edf tr\u00ean, ta s\u1eed d\u1ee5ng LR \u0111\u1ec3 train**\n\n> ***N\u00f3i qua v\u1ec1 Logistic Regression***\n>> Hai m\u00f4 h\u00ecnh tuy\u1ebfn t\u00ednh (linear models) **Linear Regression** v\u00e0 **Perceptron Learning Algorithm (PLA)**. Trong **Linear Regression**, ng\u01b0\u1eddi ta s\u1eed d\u1ee5ng \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n output y, m\u00f4 h\u00ecnh n\u00e0y ph\u00f9 h\u1ee3p \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n m\u1ed9t gi\u00e1 tr\u1ecb th\u1ef1c c\u1ee7a \u0111\u1ea7u ra kh\u00f4ng b\u1ecb ch\u1eb7n tr\u00ean v\u00e0 ch\u1eb7n d\u01b0\u1edbi. C\u00f2n trong **PLA**, \u0111\u1ea7u ra ch\u1ec9 nh\u1eadn m\u1ed9t trong hai gi\u00e1 tr\u1ecb 1 ho\u1eb7c \u22121, ph\u00f9 h\u1ee3p v\u1edbi c\u00e1c b\u00e0i to\u00e1n **Binary Classification**. \n\n>> Tuy v\u1eady, ta l\u1ea1i kh\u00f4ng s\u1eed d\u1ee5ng **Linear Regression**, hay th\u1eadm ch\u00ed l\u00e0 **PLA**, m\u1eb7c d\u00f9 **PLA** r\u1ea5t ph\u00f9 h\u1ee3p v\u1edbi b\u00e0i to\u00e1n. M\u00e0 ta l\u1ef1a ch\u1ecdn **Logistic Regression**. M\u00f4 h\u00ecnh n\u00e0y gi\u1ed1ng v\u1edbi **Linear Regression** \u1edf kh\u00eda c\u1ea1nh \u0111\u1ea7u ra l\u00e0 s\u1ed1 th\u1ef1c, v\u00e0 gi\u1ed1ng v\u1edbi **PLA** \u1edf vi\u1ec7c \u0111\u1ea7u ra b\u1ecb ch\u1eb7n (trong \u0111o\u1ea1n 0 -> 1). M\u1eb7c d\u00f9 trong t\u00ean c\u00f3 ch\u1ee9a t\u1eeb \"**regression**\", tuy v\u1eady **Logistic Regression** th\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng nhi\u1ec1u cho c\u00e1c b\u00e0i to\u00e1n **classification**. \u0110\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh th\u1ea5y n\u00f3 r\u1ea5t l\u00e0 linh ho\u1ea1t va d\u1ec5 s\u1eed d\u1ee5ng. Do v\u1eady, m\u00ecnh l\u1ef1a ch\u1ecdn **Logistic Regression** cho b\u00e0i to\u00e1n n\u00e0y.\n\n>> \u1ee8ng d\u1ee5ng c\u1ee7a m\u00f4 h\u00ecnh Logistic Regression: \n>> + D\u1ef1 \u0111o\u00e1n email c\u00f3 ph\u1ea3i spam hay kh\u00f4ng\n>> + D\u1ef1 \u0111o\u00e1n giao d\u1ecbch ng\u00e2n h\u00e0ng l\u00e0 gian l\u1eadn hay kh\u00f4ng\n>> + D\u1ef1 \u0111o\u00e1n kh\u1ed1i u l\u00e0nh hay \u00e1c t\u00ednh\n>> + D\u1ef1 \u0111o\u00e1n kho\u1ea3n vay c\u00f3 tr\u1ea3 \u0111\u01b0\u1ee3c kh\u00f4ng\n>> + D\u1ef1 \u0111o\u00e1n kho\u1ea3n \u0111\u1ea7u t\u01b0 v\u00e0o start-up c\u00f3 sinh l\u00e3i hay kh\u00f4ng.\n\n> T\u1ed5ng quan v\u1ec1 b\u00e0i to\u00e1n \u0111\u01b0\u1ee3c tr\u00ecnh b\u00e0y tr\u00ean link notion \u0111\u00e3 c\u00f3 \u1edf tr\u00ean l\u1edbp: https:\/\/tricky-tax-444.notion.site\/5acbc01b2a924fe6818d046339a68d23?v=6bc56f801d7c4ce1a1eefd4b60ef57cf&p=cf099245442c461aa36305456a17183e"}}