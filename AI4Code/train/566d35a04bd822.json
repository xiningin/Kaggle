{"cell_type":{"5d628a26":"code","59d10fb0":"code","f381eee1":"code","8dc53121":"code","5e47680f":"code","f7bc391f":"code","669147fd":"code","e910e8ff":"code","4ae32292":"code","29ddd394":"code","b1464e12":"code","1f57f6e8":"code","2634f5e0":"code","927ac195":"code","bbb149fc":"code","d017781c":"code","03f2a41f":"code","e6293a43":"code","4619edfb":"code","af973c5e":"code","ea7a3b54":"code","556dda86":"markdown","174a85e6":"markdown"},"source":{"5d628a26":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","59d10fb0":"#import neccessary libraries\nfrom keras.layers import LSTM,GRU,Dense,Bidirectional,Dropout\nfrom keras.callbacks import *\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop,Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\nfrom matplotlib import pyplot as plt","f381eee1":"#import data\nPATH=\"..\/input\/\"\ndata_train=pd.read_csv(f'{PATH}train.csv')\ndata_test=pd.read_csv(f'{PATH}test.csv')","8dc53121":"#see the shapes of data\ndata_train.shape,data_test.shape","5e47680f":"features=list(data_train.columns.values[2:])\ntarget=['target']","f7bc391f":"#normalizing data for nn\nscaler=StandardScaler()\ndata_train_scaled=data_train.copy()\ndata_test_scaled=data_test.copy()\ndata_train_scaled[features]=scaler.fit_transform(data_train[features].T).T\ndata_test_scaled[features]=scaler.transform(data_test[features].T).T","669147fd":"#standard split on train and validation\nX_train,X_valid,y_train,y_valid=train_test_split(data_train_scaled[features],\n                                              data_train_scaled[target])","e910e8ff":"#create testset and check sizes\nX_test=data_test_scaled[features]\nX_train.shape,X_valid.shape,X_test.shape","4ae32292":"#create custom auc metrics\nclass roc_callback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred = self.model.predict(self.x)\n        roc = roc_auc_score(self.y, y_pred)\n        y_pred_val = self.model.predict(self.x_val)\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","29ddd394":"#create callback procedures\nearlystopper = EarlyStopping(patience=8, verbose=1)\ncheckpointer = ModelCheckpoint(filepath = 'model_tranz.hdf5',\n                               verbose=1,\n                               save_best_only=True, save_weights_only = True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=2, min_lr=0.000001, verbose=1,cooldown=1)","b1464e12":"#create simple NN model\nmodel = Sequential()\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(rate=0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=Adam(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy'])","1f57f6e8":"#train the model\nhistory=model.fit(X_train.values,y_train.values,epochs=50,batch_size=2048,\n                 validation_data=(X_valid,y_valid.values),\n                 callbacks=[roc_callback(training_data=(\n                     X_train.values, y_train.values),\n                                         validation_data=(X_valid.values, y_valid.values)),\n                           earlystopper, checkpointer, reduce_lr])","2634f5e0":"#0.85 for simple nn... not so bad\n#next step is adding one more layer \nmodel = Sequential()\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(rate=0.2))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(rate=0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=Adam(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy'])","927ac195":"history_2=model.fit(X_train.values,y_train.values,epochs=50,batch_size=2048,\n                 validation_data=(X_valid,y_valid.values),\n                 callbacks=[roc_callback(training_data=(\n                     X_train.values, y_train.values),\n                                         validation_data=(X_valid.values, y_valid.values)),\n                           earlystopper, checkpointer, reduce_lr])","bbb149fc":"#then go to lstm\n#first of all edit Xs\nX_train_rnn=np.reshape(X_train.values,(X_train.shape[0],1,X_train.shape[1]))\nX_valid_rnn=np.reshape(X_valid.values,(X_valid.shape[0],1,X_valid.shape[1]))\nX_test_rnn=np.reshape(data_test_scaled[features].values,(data_test_scaled[features].shape[0],1,\n                                                     data_test_scaled[features].shape[1]))\n","d017781c":"X_train_rnn.shape,X_valid_rnn.shape","03f2a41f":"#create simple lstm\nmodel = Sequential()\nmodel.add(LSTM(32,batch_size=2048,batch_input_shape=(None,1,X_train.shape[-1]),\n              input_shape=(1,X_train_rnn.shape[-1])))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=Adam(lr=1e-2),loss='binary_crossentropy',metrics=['accuracy'])\nhistory_lstm=model.fit(X_train_rnn,y_train.values,epochs=50,batch_size=2048,\n                 validation_data=(X_valid_rnn,y_valid.values),\n                 callbacks=[roc_callback(training_data=(\n                     X_train_rnn, y_train.values),\n                                         validation_data=(X_valid_rnn, y_valid.values)),\n                           earlystopper, checkpointer, reduce_lr])","e6293a43":"#create simple gru\nmodel = Sequential()\nmodel.add(GRU(32,batch_size=2048,batch_input_shape=(None,1,X_train.shape[-1]),\n              input_shape=(1,X_train.shape[-1])))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=Adam(lr=1e-2),loss='binary_crossentropy',metrics=['accuracy'])\nhistory_gru=model.fit(X_train_rnn,y_train.values,epochs=50,batch_size=2048,\n                 validation_data=(X_valid_rnn,y_valid.values),\n                 callbacks=[roc_callback(training_data=(\n                     X_train_rnn, y_train.values),\n                                         validation_data=(X_valid_rnn, y_valid.values)),\n                           earlystopper, checkpointer, reduce_lr])","4619edfb":"#create bidirectional gru\nmodel = Sequential()\nmodel.add(Bidirectional(GRU(32,batch_size=1024,\n              input_shape=(1,X_train.shape[-1]))))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=Adam(lr=1e-2),loss='binary_crossentropy',metrics=['accuracy'])\nhistory_bidir=model.fit(X_train_rnn,y_train.values,epochs=50,batch_size=2048,\n                 validation_data=(X_valid_rnn,y_valid.values),\n                 callbacks=[roc_callback(training_data=(\n                     X_train_rnn, y_train.values),\n                                         validation_data=(X_valid_rnn, y_valid.values)),\n                           earlystopper, checkpointer, reduce_lr])","af973c5e":"#create stacked GRU\nmodel = Sequential()\nmodel.add(GRU(32,batch_size=2048,batch_input_shape=(None,1,X_train.shape[-1]),\n              return_sequences=True,dropout=0.1,recurrent_dropout=0.5,\n              input_shape=(1,X_train.shape[-1])))\nmodel.add(GRU(64,batch_size=2048,batch_input_shape=(None,1,X_train.shape[-1]),\n              return_sequences=False,dropout=0.1,recurrent_dropout=0.5,\n              input_shape=(1,X_train.shape[-1])))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=Adam(lr=1e-2),loss='binary_crossentropy',metrics=['accuracy'])\nhistory_gru_stacked=model.fit(X_train_rnn,y_train.values,epochs=50,batch_size=2048,\n                 validation_data=(X_valid_rnn,y_valid.values),\n                 callbacks=[roc_callback(training_data=(\n                     X_train_rnn, y_train.values),\n                                         validation_data=(X_valid_rnn, y_valid.values)),\n                           earlystopper, checkpointer, reduce_lr])","ea7a3b54":"#final plot for presenting results\nfig, ax = plt.subplots()\nval_loss_simple_nn=history.history['val_loss']\nval_loss_2layers_nn=history_2.history['val_loss']\nval_loss_lstm=history_lstm.history['val_loss']\nval_loss_gru=history_gru.history['val_loss']\nval_loss_bidir=history_bidir.history['val_loss']\nval_loss_gru2=history_gru_stacked.history['val_loss']\n\nepochs_1=range(1,len(val_loss_simple_nn)+1)\nepochs_2=range(1,len(val_loss_2layers_nn)+1)\nepochs_3=range(1,len(val_loss_lstm)+1)\nepochs_4=range(1,len(val_loss_gru)+1)\nepochs_5=range(1,len(val_loss_bidir)+1)\nepochs_6=range(1,len(val_loss_gru2)+1)\n\nax.plot(epochs_1,val_loss_simple_nn,'b',label='val_loss_simple_nn')\nax.plot(epochs_2,val_loss_2layers_nn,'r',label='val_loss_2layers_nn')\nax.plot(epochs_3,val_loss_lstm,'go',label='val_loss_lstm')\nax.plot(epochs_4,val_loss_gru,'yo',label='val_loss_gru')\nax.plot(epochs_5,val_loss_bidir,'bo',label='val_loss_bidir')\nax.plot(epochs_6,val_loss_gru2,'ro',label='val_loss_gru2layers')\n\nplt.title('Losses on models')\nplt.legend()\nplt.show()","556dda86":"In this kernel i tried to check several hypothesis:\n1. what quality of model based on simple NN\n2. can we get some uplift in auc by using LSTM, GRU, bidirectional GRU\n3. influence of stacking GRU\/LSTM on auc","174a85e6":"Conclusion:\n1. Stacking worsens auc\n2. Sequence models equal to simple nn.\n3. Optimal loss is reached quite fast (~10 epochs)\n\nTL,DR:\n1. Try autoencoders\n2. Try feature engineering\n3. Try GANs"}}