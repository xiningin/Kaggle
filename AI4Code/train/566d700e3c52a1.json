{"cell_type":{"9a279e69":"code","684e4ca3":"code","6b4ff179":"code","e2f7e736":"code","57bd11d4":"code","82b5d585":"code","a45add48":"markdown","3d096eef":"markdown","5698ce0a":"markdown","8680e04e":"markdown","4fb20e09":"markdown","ba4c98c9":"markdown","a78d2e9f":"markdown"},"source":{"9a279e69":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom perceptron import Perceptron\nfrom std_to_pts import std_to_pts\n\nN_DIMS = 2\nGROUND_TRUTH = np.array([2., 1.])\nGROUND_TRUTH_BIAS = -100.\n\nN_DATA = 50\nX_RANGE = np.array([-100., 100.])\n\nN_DATA_TEST = 500\nX_RANGE_TEST = np.array([-1000., 1000.])\n\nrng = np.random.default_rng()\nX = rng.uniform(*X_RANGE, (N_DATA, N_DIMS))\ny = np.where(\n    np.dot(X, GROUND_TRUTH) + GROUND_TRUTH_BIAS > 0,    # Condition\n    1,  # Truth value\n    0   # False value\n)","684e4ca3":"fig, ax = plt.subplots()\n\ncol = np.where(y==0, 'r', 'b')\nax.scatter(X[:,0], X[:,1], c=col)\n\ntruth_line = ax.plot(\n    *std_to_pts(*GROUND_TRUTH, GROUND_TRUTH_BIAS, ref=(X_RANGE * 1.2)),\n    linestyle='dashed',\n    label=\"Ground Truth\",\n    c='#00ff00'\n)\n\nax.set_xlim(*(X_RANGE * 1.2))\nax.set_ylim(*(X_RANGE * 1.2))\nax.set_xlabel(\"x_1\")\nax.set_ylabel(\"x_2\")\nfig.legend()\nplt.show()","6b4ff179":"perceptron = Perceptron(nu= 0.01, weights=[1., 2.], bias=1.)\n\nperceptron_line = ax.plot(\n    *std_to_pts(*perceptron.weights, perceptron.bias, ref=(X_RANGE * 1.1)),\n    label=\"Perceptron\",\n    c='#ffff00'\n)[0]\nfig.legend()\nfig","e2f7e736":"perceptron.learn(X, y)\nperceptron_line.set_data(*std_to_pts(*perceptron.weights, perceptron.bias, ref=(X_RANGE * 1.1)))\nprint(\"Learned Weights: {}, Learned Bias:{}\".format(perceptron.weights, perceptron.bias))\nprint(\"Ground Truth: {}, True Bias: {}\".format(GROUND_TRUTH, GROUND_TRUTH_BIAS))\nfig","57bd11d4":"X_test = rng.uniform(*X_RANGE_TEST, (N_DATA_TEST, N_DIMS))\ny_test = np.where(\n    np.dot(X_test, GROUND_TRUTH) + GROUND_TRUTH_BIAS > 0,    # Condition\n    1,  # Truth value\n    0   # False value\n)\n\nfig2, ax2 = plt.subplots()\n\ncol = np.where(y_test==0, 'r', 'b')\nax2.scatter(X_test[:,0], X_test[:,1], c=col)\n\nax2.set_xlim(*(X_RANGE_TEST * 1.2))\nax2.set_ylim(*(X_RANGE_TEST * 1.2))\nax2.set_xlabel(\"x_1\")\nax2.set_ylabel(\"x_2\")\nplt.show()","82b5d585":"perceptron_line = ax2.plot(\n    *std_to_pts(*perceptron.weights, perceptron.bias, ref=(X_RANGE_TEST * 1.1)),\n    label=\"Perceptron\",\n    c='#ffff00'\n)[0]\ny_test_predicted = perceptron.classify(X_test)\nmisclassifications = y_test_predicted != y_test\nax2.scatter(X_test[misclassifications][:,0], X_test[misclassifications][:,1], c='cyan', s=50)\naccuracy = np.count_nonzero(y_test_predicted == y_test)\/N_DATA_TEST\nprint(\"Accuracy: {}\".format(accuracy))\nfig2","a45add48":"## The Perceptron\nThe perceptron is one of the simplest forms of artificial neurons in the family of feed-forward neural networks. It is the precursor to the modern artificial neuron. Just by itself, a single perceptron can be used as a linear model to perform binary classifications.\n\n<img src=\"https:\/\/images.deepai.org\/glossary-terms\/perceptron-6168423.jpg\" width=500>\n\nIt performs a weighted sum of the given input vector and passes the sum through the Heaviside step function. Lets take a look at a simple implementation of a perceptron as a Python class.\n\n```python\nclass Perceptron:\n    def __init__(self, nu, weights, bias):\n        self.nu = nu # Learning Rate\n        self.weights = np.array(weights)\n        self.bias = bias\n        # Activation Function: Heaviside step function\n        self.activation = lambda x: np.where(x>0, 1, 0) if isinstance(x, np.ndarray) else (1 if x>0 else 0)\n```\nThe output of the Heaviside step function (1 or 0), determines the classification.\n```python\n    def classify(self, X):\n        fx = np.dot(X, self.weights) + self.bias\n        gfx = self.activation(fx)\n        return gfx\n```","3d096eef":"The figure above shows a perceptron that has trained on the given data. It accurately classifies the entire training data, but what about new data that it hasn't seen before? Let's create a test dataset using the same ground truth and evaluate our perceptron against it.","5698ce0a":"Let's demonstrate the working of this perceptron by training it on some sample data. I will set the size of the input vector to 2, to make it easy to visualize on an XY plane.\n\n`X = < x_1 , x_2 >`\n\nI use the Numpy random number generator to generate a set of randomized input vectors and classify them using a pre-set ground truth vector. Using a pre-set ground truth vector ensures that the data is linearly separable. The goal of the perceptron learning algorithm is to find this 'ground truth' from the generated dataset.","8680e04e":"Let's visualize the generated dataset.  ","4fb20e09":"After creating a perceptron, we need to train it on the data. To do this, we use the perceptron learning algorithm. It is a simple algorithm that incrementally tweaks the weights and bias of the perceptron until it fits the data.  \n```python\n    def learn(self, X, y):\n        _, dims = X.shape\n        self.weights = np.ones(dims)\n        self.bias = 0\n        \n        y_pred = self.classify(X)\n        misclassifications = y_pred != y\n        while np.any(misclassifications):\n            for i, x_i in enumerate(X):\n                if misclassifications[i]:\n                    delta = self.nu * (y[i] - y_pred[i])\n                    self.weights += delta * x_\n                    self.bias += delta\n\n            y_pred = self.classify(X)\n            misclassifications = y_pred != y\n```\nThis learning algorithm runs indefinitely until all the data is correctly classified. It assumes that the data is strictly linearly separable. While dealing with real-world data, this is not always the case. If there is a possibility of outliers, an alternative algorithm that runs for a fixed number of iterations would be more suitable.","ba4c98c9":"I will now initialize a perceptron with some arbitrary weights and a bias. In a 2D input space, we can use a straight line to visualize this perceptron. A straight line that divides the space into two parts.","a78d2e9f":"# Artificial Neural Networks from scratch\n\nArtificial Neural Networks or 'ANNs' are data structures that aim to solve complex computational problems that are often infeasible to solve using traditional algorithms. ANNs take inspiration from the structure of the biological animal brain. The fundamental unit of the brain is a neuron. An 'artificial neuron' is designed to loosely model a biological neuron and forms the fundamental unit of an ANN. \n\nIn this notebook, I attempt to model, train, and test some of the fundamental structures of ANNs from scratch using only the NumPy library and the Matplotlib library to plot the results."}}