{"cell_type":{"e14ea0a3":"code","35a995bc":"code","0cc400f7":"code","84b2ca18":"code","e3ac6689":"code","98a64269":"code","a7a4493a":"code","ecc6c0de":"code","a7b0c392":"code","6ee64b21":"code","dcb292fe":"code","ceb595a3":"code","c63faa6d":"code","a1f2ece1":"markdown","7a636e2c":"markdown","c20d978b":"markdown","af5703f4":"markdown","a12f626e":"markdown","b2515556":"markdown","3f762342":"markdown","a1bee49d":"markdown","29ea8653":"markdown","845021ef":"markdown","f54e638e":"markdown","6726d765":"markdown","fc79202f":"markdown","0498418a":"markdown"},"source":{"e14ea0a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\nimport os\nimport pandas as pd\nimport math\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nimport statsmodels.api as sm\n\n### CONSTANTS USED \nTRAIN_PERC=0.90\nPLT_WIDTH=5\nPLT_HEIGHT=9\nPLT_DPI=120\n###\ndata=[]\nfile=None\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        file=os.path.join(dirname,filename)\n\n#print(file)\n\n\n\n#load the data and generate the time series\n#interested only in the third column\ndf=pd.read_csv(file,delim_whitespace=True,header=None,names=['J1','J2','Y','J3','J4'])\ndf.head()\ntime_series=df.iloc[:,2]\n#remove Nan and non-digits\ntime_series.dropna(inplace=True)\ntime_series=time_series.drop([0,1])\n\n\ntime_series=time_series.astype(float)\ntime_series=time_series.reset_index(drop=True)\n##CREATE TRAIN AND TEST DATA \n###\ntrainlen=math.floor(time_series.size*TRAIN_PERC)\ntest_series=time_series[trainlen:]\ntrain_series=time_series[:trainlen]\n\n###\n\n\n#display the time series\n#https:\/\/machinelearningmastery.com\/time-series-data-visualization-with-python\/\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.figsize':(PLT_HEIGHT,PLT_WIDTH), 'figure.dpi':PLT_DPI})\nplt.rcParams.update({'font.size':9})\nplt.rc('figure', titlesize=9)\n#Line Plot\n\n\nfig,ax=plt.subplots(2,2)\ntrain_series.plot(ax=ax[0,0])\ntrain_series.hist(ax=ax[0,1])\nplot_acf(train_series,ax=ax[1,0],title='Auto')\nplot_pacf(train_series,ax=ax[1,1],title='')\n\nplt.show()\n\n\n","35a995bc":"def difference(series,order):\n    \"\"\"\n       Parameters:\n                    Input:\n                        series = input time series\n                        order= order of differencing\n                        \n                    Returns:\n                        returns a series consisting difference values of order=order\n        \n      \n    \"\"\"\n    \n    if order == 0:\n        return series\n    else:\n        diff=series.diff(-1)\n        order_=order-1\n        return difference(series=diff,order=order_)","0cc400f7":"\nzod=difference(train_series,0)   #zeroth-order differencing = original series\nzod=zod.rename('0')\nfod=difference(train_series,1)   #first-order\nfod=fod.rename('1')\nsod=difference(train_series,2)   #second-order\nsod=sod.rename('2')\ntod=difference(train_series,3)   #third-order\ntod=tod.rename('3')\n#print(type(zod))\nll=[zod,fod,sod,tod]\n\n#keys=[s.name for s in fod]\n#print(zod.name)\ndf=pd.concat([zod,fod,sod,tod],axis=1,keys=[s.name for s in ll])\nprint(df.head())","84b2ca18":"df.describe()","e3ac6689":"from pandas.plotting import lag_plot\n\ndef six_plots(sr):\n    \n    sr=sr.dropna()\n    plt.rcParams.update({'figure.figsize':(PLT_HEIGHT,PLT_WIDTH), 'figure.dpi':PLT_DPI})\n    fontdict={'fontsize':9,'verticalalignment':'bottom'}\n    fig,ax=plt.subplots(2,3)\n    sr.plot(ax=ax[0,0])  #plot the series\n    sr.hist(ax=ax[0,1]) #must be gaussian like\n    sm.qqplot(sr,ax=ax[0,2],line='45') # how close does the series fit the normal distribution\n    lag_plot(sr,ax=ax[1,0]) #lag-1 plot to see autocorrelations   \n    plot_acf(sr,ax=ax[1,1],title='') #acf plot\n    plot_pacf(sr,ax=ax[1,2],title='') #pacf plot\n    \n    #set the titles in the correct place. \n    #https:\/\/matplotlib.org\/gallery\/pyplots\/text_layout.html#sphx-glr-gallery-pyplots-text-layout-py\n    left = 0.45\n    bottom = -0.5\n    top = 1.2\n    \n    #for the top 3 plots, titles are on the top\n    ax[0,0].text(left, top, 'run sequence',\n        horizontalalignment='left',\n        verticalalignment='top',\n        transform=ax[0,0].transAxes)\n    ax[0,1].text(left, top, 'hist',\n        horizontalalignment='left',\n        verticalalignment='top',\n        transform=ax[0,1].transAxes)\n    ax[0,2].text(left, top, 'Q-Q',\n        horizontalalignment='left',\n        verticalalignment='top',\n        transform=ax[0,2].transAxes)\n    ax[0,2].set_xlabel('')\n    ax[0,2].set_ylabel('')\n    \n    #for the bottom 3 plots , titles are at the bottom\n    ax[1,0].text(left, bottom, 'Lag-plot',\n        horizontalalignment='left',\n        verticalalignment='bottom',\n        transform=ax[1,0].transAxes)\n    ax[1,1].text(left, bottom, 'ACF',\n        horizontalalignment='left',\n        verticalalignment='bottom',\n        transform=ax[1,1].transAxes)    \n    ax[1,2].text(left, bottom, 'PACF',\n        horizontalalignment='left',\n        verticalalignment='bottom',\n        transform=ax[1,2].transAxes)\n    \n    fig.tight_layout()\n    fig.suptitle('')\n    plt.show()\n    \n","98a64269":"import matplotlib.gridspec as gridspec\n\nsix_plots(df['0'])\nsix_plots(df['1'])\nsix_plots(df['2'])","a7a4493a":"from statsmodels.tsa.stattools import adfuller\n\ndef ADF(sr):\n    \"\"\"\n    Augmented Dickey Fuller Test.\n    \"\"\"\n    sr=sr.dropna() #remove any invalids\n    results=adfuller(sr)\n    print('ADF statistic: ',results[0])\n    print('p-value: ',results[1]) #probability that the null hypothesis is true\n    print('Critical vals: ')\n    for i,j in results[4].items():\n        print(i,j)\n\nprint('difference order = 1')        \nADF(df['1'])\nprint()\nprint('difference order = 2')      \nADF(df['2'])\n#print('Aresults)","ecc6c0de":"from statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.stats.diagnostic import acorr_ljungbox as ljungbox\n\nfrom scipy.stats import chi2\n\n\ndef chi_square_table(p,dof):\n    \"\"\"\n    https:\/\/stackoverflow.com\/questions\/32301698\/how-to-build-a-chi-square-distribution-table\n    \n    Parameters:\n            Input:\n                p= p-value \n                dof = degree of freedom\n            Returns:\n                chi-sq critical value corresponding to (p,dof)\n    \n    \"\"\"\n    return chi2.isf(p,dof)\n\n\ndef chi_sq_critical_val(alpha,dof):\n    \"\"\"\n    return the critical val (c) for chi-sq distrib parameterized by \n    probability(pr)=1-alpha and degrees of freedom=dof \n    c is the value at and below which pr% of data exists\n    \n    \"\"\"\n    pr=1-alpha\n    val=chi2.ppf(pr,dof)\n    return val\n\n    \n\n\ndef eval_arima(series,order,lags,dynamic=False,alpha=0.05):\n    \"\"\"\n    1.fit the model \n    2.get the residuals\n    3.plot the residuals\n    4.does it look like white noise? mean=0, normally distributed?\n    5.calculate Q on the residuals for number of lags\n    6.choose a level of significance\n    7.choose degrees of freedom\n    8.calculate the critical value of the chi-sq statistic\n    9.accept or reject null hypothesis\n\n    Parameters:\n            Input:\n                series          = time series to be fit by the ARIMA model\n                order           = 3-tuple of form (p,d,q) where p=AR terms. d=order of differencing, q= MA terms of an ARIMA(p,d,q) model\n                dynamic         = True ==> out-of-sample (predict unseen (test) data),\n                                  False ==> in-sample  (predict on the data trained on)\n                alpha           = significance level\n                lags            = number of lags used to calculate the Ljung-Box Q statistic\n            Return:\n                    fitted model\n    \"\"\"\n\n\n    plt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120})\n\n    #fit the model\n    model=ARIMA(series,order=order)   #ARIMA(0,1,1) model\n    model_fit=model.fit(disp=-1)\n\n    #print(type(model_fit))\n    print(model_fit.summary())\n    \n    #display the fit of the model\n    model_fit.plot_predict(dynamic=dynamic).suptitle(\"model fit on training data\")\n    plt.show()\n    \n   \n    #get the residuals\n    residuals=model_fit.resid\n    #plot the residuals\n    fig,ax=plt.subplots(1,2)\n\n    residuals.plot(title='Residuals',ax=ax[0])\n    residuals.plot(kind='kde',title='probability distribution of residuals',ax=ax[1])\n    #print(model_fit.)\n    plt.show()\n    \n    #are the residuals random?\n    print(residuals.describe())\n    #autocorrelation plots of residuals\n    six_plots(residuals)\n   \n    #Significance Level at 5%\n    #alpha=0.05\n\n    #The Ljung-Box Test \n    Q,p=ljungbox(residuals,range(1,lags),boxpierce=False)\n    c=[]\n    for i in range(len(Q)):\n        dof=i+1                \n        c.append(chi_sq_critical_val(alpha,dof))\n        #print('Chi-statistic(Q) :',Q[i],'  p-value:',p[i],'   critical value: ',c,\" KEEP H0\" if Q[i]<c else \"DNT KEEP H0\")\n    \n    #plot Q versus c\n    #accept if Q stays below the 45 deg line i.e Q<c\n    arstr=\"ARIMA\"+str(order)+\"\"\n    plt.plot(c,Q,label=arstr)\n    plt.plot(c,c,label='c=Q')\n    plt.xlabel('Q values')\n    plt.ylabel('critical values')\n    plt.title('Ljung - Box Test')\n    plt.legend()\n    plt.show()\n    return model_fit\n    \n\n","a7b0c392":"\narima_011=eval_arima(train_series,order=(0,1,1),lags=25)\narima_210=eval_arima(train_series,order=(2,1,0),lags=25)\narima_211=eval_arima(train_series,order=(2,1,1),lags=25)\n","6ee64b21":"def arima_forecast(model,test_sr,train_sr):\n    \"\"\"\n    Forecast arima models on the test series (test_sr)\n    Parameters:\n        Input:\n            model= arima model used for forecasting\n            test_sr = test series for forecasting\n            train_sr= training data used to build model\n        Returns:\n            dictionary containing metric values\n            \n    \"\"\"\n    fc,se,cf= model.forecast(test_sr.size,alpha=0.05)\n    #Convert to series\n\n    fc_series=pd.Series(fc,index=test_sr.index)\n    lower_cf=pd.Series(cf[:,0],index=test_sr.index)\n    upper_cf=pd.Series(cf[:,1],index=test_sr.index)\n\n    #plotting\n    plt.plot(train_sr,label='training')\n    plt.plot(test_sr,label='test')\n    plt.plot(fc_series,label='forecast')\n    plt.fill_between(lower_cf.index,lower_cf,upper_cf,\n                    color='k',alpha=0.15)\n    plt.legend()\n    plt.show()\n    \n    #forecast accuracies\n    actual=test_sr.values\n    forecast=fc_series\n    mape = np.mean(np.abs(forecast - actual)\/np.abs(actual))  # MAPE\n    me = np.mean(forecast - actual)             # ME\n    mae = np.mean(np.abs(forecast - actual))    # MAE\n    mpe = np.mean((forecast - actual)\/actual)   # MPE\n    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n    mins = np.amin(np.hstack([forecast[:,None], \n                              actual[:,None]]), axis=1)\n    maxs = np.amax(np.hstack([forecast[:,None], \n                              actual[:,None]]), axis=1)\n    minmax = 1 - np.mean(mins\/maxs)             # minmax\n    \n    return({'mape':mape, 'me':me, 'mae': mae, \n            'mpe': mpe, 'rmse':rmse,\n            'corr':corr, 'minmax':minmax})\n    \n    \n","dcb292fe":"arima011_metrics=arima_forecast(arima_011,test_sr=test_series,train_sr=train_series)\narima210_metrics=arima_forecast(arima_210,test_sr=test_series,train_sr=train_series)\narima211_metrics=arima_forecast(arima_211,test_series,train_series)","ceb595a3":"lm=[arima011_metrics,arima210_metrics,arima211_metrics]\ndlmdf=pd.DataFrame(lm)\ndlmdf.head()","c63faa6d":"   \nf,axx=plt.subplots(3,3)    \ndlmdf['mape'].plot(ax=axx[0,0])\ndlmdf['me'].plot(ax=axx[0,1])\ndlmdf['mae'].plot(ax=axx[0,2])\ndlmdf['mpe'].plot(ax=axx[1,0])\ndlmdf['rmse'].plot(ax=axx[1,1])\ndlmdf['corr'].plot(ax=axx[1,2])\ndlmdf['minmax'].plot(ax=axx[2,0])\n#axx[2,1].setvisible(False)\nf.delaxes(axx[2,1])\nf.delaxes(axx[2,2])\nf.tight_layout()","a1f2ece1":"how well do the models forecast the test data?  \n","7a636e2c":" We define a function to create [4 plots](https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/4plot.htm) which are a set of the following plots that are useful while evaluating a model.\n 1. run sequence plot  (time indices versus response variables)\n 2. lag plot (${y_t}$ versus ${y_{t-1}}$ . useful to check if there are any autocorrelations of lag-1)\n 3. normal probability distribution plot -(q-q-plot) (check if data can be approximated by the normal distribution [please read this ](https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/normprpl.htm)) . \n 4. histogram (freq distribution )  \n\n\n In addition to the above I also plot the ACF and PACF.  \n ","c20d978b":"We now evaluate each of our 3 models.","af5703f4":"\nNow we move on to parameters ${p}$ and ${q}$\nThe MA term ${q}$ is associated with the ACF plot.   \nWherever the ACF drops to within the confidence interval is candidate. Therefore ${q=1}$ seems like a good choice.  \n  \n${p}$ is the corresponding AR term and it is associated with the PACF plot.  \nThe value where the PACF value cuts to within the confidence interval is a reasonable choice. Hence ${p=2}$.   \n\nThe models ARIMA(2,1,0), ARIMA(0,1,1) and ARIMA(2,1,1) are models we will consider further for analysis\n\n","a12f626e":"**Beginners notes on selecting ARIMA parameters using the Box-Jenkins Method**","b2515556":"we select ${d=1}$ based on the ADF result.  Note we can run the same tests for ${\\:d=2,\\:d=3}$. But I will using the principle of parsimony (Occams Razor).  \n\nThe statistic is lower than the critical value at 1%, so we reject the null hypothesis. Our differenced series is stationary with ${d=1}$  ","3f762342":"The residuals appear to have Gaussian distribution (almost). The aim is to make the residuals follow a white noise type distribution. Completely random i.e. the more the residuals appear like a Gaussian distribution, the better.  \nIn conjuction with the fact that the first order difference has the least std dev , we may justify a choice of ${d=1}$.  \nHowever let's run a test to check this.  \n\nLet's see if the series after differencing is stationary using the **Augmented Dickey Fuller (ADF) Test**.  \nThe ADF lets us know if a series is stationary.  \n**The null hypothesis here is that the series is non-stationary**. \nIf the ADF statistic is greater than it's p-value (p-value is the probability in favor of the null-hypothesis, larger p-values imply that we cannot reject the null hypothesis) , then we accept the null hypothesis else reject it with an alternate i.e the series is stationary.    \nWe carry out the ADF for various orders of difference and then select the value of ${d}$ for which the series shows stationary behaviour\n","a1bee49d":"\nWe need to make forecasts for some time series data.  \n\n**The raw data and its description can be downloaded [here](https:\/\/www.itl.nist.gov\/div898\/handbook\/pmc\/section6\/pmc621.htm)**  \n\nTo make predictions we need a model. One such model is called **ARIMA** which stands for \"**A**utoRegressive **I**ntegrated **M**oving **A**verage\".  \nThe model is written as ARIMA(p,d,q) where   \n                        ${\\:\\:\\:\\:p\\:}$ is the number of lags from the time series used for autoregression(**AR**),  \n                                          ${\\:\\:\\:\\:d\\:}$ is the order of differencing (**I**) (explained below) and   \n                                           ${\\:\\:\\:\\:q\\:}$ is the moving average window(**MA**)   ${\\:\\:\\:}$i.e number of terms  to consider for performing the moving average operation.  \n\n**Modeling**  \nNow we move onto the modeling  i.e. We need to find decent values for parameters ${p}$,${\\:d}$ and ${q}$.  \nOne of the methods  is the **Box-Jenkins Method** which is what this post is about. :)  \n\n**Out-Of-Time Cross Validation** means that we split the data into training and test series.  \nForecasting requires the order of the series to stay intact. Therefore we **don't shuffle or randomize the order** like we do usually with other ML tasks.  \nWe build the ARIMA models on the training data and see how they fare on the test series using some metrics.\n\n\n\n","29ea8653":"**Evaluation of the models**  \nFor evaluating a model we need a test. The **Ljunge-Box Test** provides us with such a measure .  \nThe test is done using the residuals after the fit of the model and **NOT** the original time series.  \n\nThe test results in a statistic ${Q}$ defined ${Q=n(n+2){\\Huge\\sum}_{k=1}^{m}\\large\\frac{\\hat{r}_{k}^{2}}{n-k}}$  where  ${\\hat{r}_{k}}$ is the estimated autocorrelation of the series at lag ${k}$ and ${m}$ is the number of lags being tested and ${n}$ is the length of the time series.  \n\n\n\n\n**The null hypothesis ${H_{0}}$ is that our model ARIMA(${p,\\:d,\\:q}$) is a good fit**. i.e the residuals are random white noise. \n\nWe get to reject ${H_{0}}$ in the critical region which is the area of the distribution determined by      \n\n ${\\:\\: Q\\:>\\:\\:{\\chi^{2}_{1-\\alpha,h}}}\\:$   \n \n where ${\\:{\\chi^{2}_{1-\\alpha,h}}\\:\\:}$ is the chi-square distribution    \n ${\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:h}$ is the degrees of freedom  \n ${\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\alpha}$ is the level of significance  \n \n Extreme values of Q have less chances of occurence. We need to find the probability associated value of Q for this series. A high ${P}$-value indicates failure to reject the null hypothesis i.e. a good fit. A low ${P}$ value (${\\:P<0.05\\:@\\: 5\\%\\: significance\\:level\\:}$) is indicative of the alternate hypothesis i.e our fit is bad.     \n \n","845021ef":"from the plots of the metrics above, **ARIMA(2,1,0)** looks like a good choice.\n\nIn Summary:  \nWe chose ${d}$ by differencing the original series and then by using the ADF test.  \nWe chose ${p}$ by looking at the PACF plots of the stationary residuals.  \nWe chose ${q}$ by looking at the ACF plots of the stationary residuals.\n\nI will be covering automated arima modeling for the same dataset in the coming posts which will cover a wider range of the parameters ${\\:d\\:,p\\:,q\\:}$. \n\n\n\n\n\n\n\n\n","f54e638e":"We now have some metrics for each of the models which we wish to compare.  \nI will put them in a dataframe and plot each column and pick the model which looks the best. ","6726d765":"\n\nThere's trend in the data i.e. the time series is non-stationary as is visible from the autocorrelation plot.   \nBox Jenkings assumes a stationary time series where the mean and the standard deviation don't change with time.  \nSuch series shows no trend.  \n\n**Differencing**  \nConvertion of a non-stationary series to a stationary can be achieved using \"differencing\".   \n\nThere are orders to differencing.  \nFirst order  is ${\\:\\:\\Delta_{1} y_{t}=Y_{t}-Y_{t-1}}$.   \nSecond order is ${\\:\\:\\Delta_{2} y_{t}=\\Delta_{1} y_{t} - \\Delta_{1} y_{t-1}=Y_{t}-2Y_{t-1}+Y_{t-2}}$.  \n${\\vdots}$  \nN-th order is ${\\:\\:\\Delta_{i} y_{t}=\\Delta_{i-1} y_{t} - \\Delta_{i-1} y_{t-1}}$\n\nwhere ${Y_{t}}$ is the ${t}$-th sample in the time series.   \n\nThe task is to find the order that looks like white noise. White noise is random noise and therefore it's distribution is Gaussian.  \n\n\nWe can write a recursive function for getting the i-th order difference  \n\n \n\n","fc79202f":"The first order difference looks like the one with the least std dev.  \nFirst impression would be select ${d=1}$  \n","0498418a":"The order of difference with the lowest standard deviation is a good choice usually.\n"}}