{"cell_type":{"33c3f173":"code","72b154a3":"code","adae684c":"code","8bf2433f":"code","7a796cd0":"code","b0278fbe":"code","29d83ecb":"code","d7d1257d":"code","3d32cf99":"code","66cf1001":"code","7e1af9aa":"code","6757370e":"code","e0f12a56":"code","12544725":"code","7e19a970":"code","a5857b91":"code","33f5aa09":"code","e8ac246e":"code","b296d58f":"code","e1c429de":"code","71ba594d":"markdown","e7e7c6fb":"markdown","98d9e0ff":"markdown","d8aa4702":"markdown","26d8370b":"markdown","f02ec558":"markdown","57278404":"markdown","8456b445":"markdown","78626d5f":"markdown","9d4c91c9":"markdown","9a66d3d6":"markdown"},"source":{"33c3f173":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nholdout = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ntrain[\"origin\"] = \"train\"\nholdout[\"origin\"] = \"holdout\"\n\ncols_without_target = list(train.columns)\ncols_without_target.remove(\"Survived\")\n\ndf = pd.concat([train[cols_without_target], holdout])\n\ndf[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\ndf[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean())\n\ndf[\"Cabin\"] = df[\"Cabin\"].str.extract(r\"^.*([A-Z])[0-9]+$\")[0]\n\ndf = df.drop(\"Cabin\", axis=1)\n\ndf[\"Embarked\"] = df[\"Embarked\"].fillna('S')\n\ndf[\"PassengerId\"] = df[\"PassengerId\"].astype(\"uint16\")\n\ndf[\"Pclass\"] = df[\"Pclass\"].astype(\"category\")\n\ndf = df.drop(\"Name\", axis=1)\n\ndf[\"Sex\"] = df[\"Sex\"].astype(\"category\")\n\ndf[\"Age\"] = df[\"Age\"].astype(\"uint8\")\n\ndf[\"SibSp\"] = df[\"SibSp\"].astype(\"uint8\")\ndf[\"Parch\"] = df[\"Parch\"].astype(\"uint8\")\n\ndf[\"Ticket\"] = df[\"Ticket\"].str.extract(r\"([0-9]+)$\").astype('float')\n\ndf = df.dropna()\n\ndf[\"Fare\"] = df[\"Fare\"].astype(\"uint16\")\n\ndf[\"Embarked\"] = df[\"Embarked\"].astype(\"category\")\n\npclass_dummies = pd.get_dummies(df[\"Pclass\"], prefix=\"Pclass\", drop_first=True)\n\ndf = pd.concat([df, pclass_dummies], axis=1)\n\nsex_dummies = pd.get_dummies(df[\"Sex\"], prefix=\"Sex\", drop_first=True)\n\ndf = pd.concat([df, sex_dummies], axis=1)\n\nembarked_dummies = pd.get_dummies(df[\"Embarked\"], prefix=\"Embarked\", drop_first=True)\n\ndf = pd.concat([df, embarked_dummies], axis=1)\n\ntrain = pd.concat([df.loc[df[\"origin\"] == \"train\"], train[\"Survived\"]], axis=1)\nholdout = df.loc[df[\"origin\"] == \"holdout\"]\n\ntrain = train.dropna()\n\ntrain = train.drop(\"origin\", axis=1)\nholdout = holdout.drop(\"origin\", axis=1)\n\ntrain[\"Age_Group\"] = pd.cut(train[\"Age\"], [0,1,5,13,18,35,65,100], \n       labels=[\"infant\", \"toddler\", \"child\", \"teenager\", \"young adult\", \"middle aged\", \"seniors\"]).astype(\"category\")\nholdout[\"Age_Group\"] = pd.cut(holdout[\"Age\"], [0,1,5,13,18,35,65,100], \n       labels=[\"infant\", \"toddler\", \"child\", \"teenager\", \"young adult\", \"middle aged\", \"seniors\"]).astype(\"category\")\n\nageGroup_dummies = pd.get_dummies(train[\"Age_Group\"], prefix=\"Age_Group\", drop_first=True)\ntrain = pd.concat([train, ageGroup_dummies], axis=1)\n\nageGroup_dummies = pd.get_dummies(holdout[\"Age_Group\"], prefix=\"Age_Group\", drop_first=True)\nholdout = pd.concat([holdout, ageGroup_dummies], axis=1)\n\ntrain = train.drop([\"Pclass\", \"Sex\", \"Age\",  \"Embarked\", \"Age_Group\"], axis=1)\nholdout = holdout.drop([\"Pclass\", \"Sex\", \"Age\", \"Embarked\", \"Age_Group\"], axis=1)","72b154a3":"feature_cols = [\"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Pclass_2\", \"Pclass_3\", \n                \"Sex_male\", \"Embarked_Q\", \"Embarked_S\", \"Age_Group_toddler\", \"Age_Group_child\", \"Age_Group_teenager\",\n               \"Age_Group_young adult\", \"Age_Group_middle aged\", \"Age_Group_seniors\"]\ntarget_col = \"Survived\"\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmm_scaler = MinMaxScaler()\ntrain[feature_cols] = mm_scaler.fit_transform(train[feature_cols])\nholdout[feature_cols] = mm_scaler.fit_transform(holdout[feature_cols])\n\nX = train[feature_cols]\ny = train[target_col]","adae684c":"from sklearn.feature_selection import RFECV","8bf2433f":"from sklearn.metrics import roc_auc_score","7a796cd0":"from sklearn.model_selection import cross_val_score","b0278fbe":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","29d83ecb":"from sklearn.ensemble import RandomForestClassifier","d7d1257d":"rf = RandomForestClassifier(random_state=42)\nselector = RFECV(rf, step=1, cv=10, n_jobs=-1)\nselector = selector.fit(X, y)\nfeatures_mask = selector.support_","3d32cf99":"best_features = [col for col, select in zip(feature_cols, features_mask) if select]\nbest_features","66cf1001":"rf = RandomForestClassifier(random_state=42)\nscores = cross_val_score(rf, X[best_features], y, cv=10, n_jobs=-1)\nprint(np.mean(scores))","7e1af9aa":"from sklearn.model_selection import RandomizedSearchCV","6757370e":"rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Wighting of classification classes\nclass_weight=[None, \"balanced\", \"balanced_subsample\"]\n\n\nparams = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap,\n                'class_weight': class_weight}","e0f12a56":"clf = RandomizedSearchCV(rf, params, n_iter=10, cv=3, n_jobs=-1, verbose=5)\nclf.fit(X[best_features], y)\nbest_model = clf.best_estimator_\nbest_score = clf.best_score_","12544725":"(best_model, best_score)","7e19a970":"best_model.fit(X_train[best_features], y_train)\ny_pred = best_model.predict(X_train[best_features])\nprint(f\"training accuracy: {roc_auc_score(y_train, y_pred)}\")","a5857b91":"best_model.fit(X_train[best_features], y_train)\ny_pred = best_model.predict(X_test[best_features])\nprint(f\"testing accuracy: {roc_auc_score(y_test, y_pred)}\")","33f5aa09":"scores = cross_val_score(best_model, X[best_features], y, cv=10, n_jobs=-1)\nprint(f\"cross_val_score: {np.mean(scores)}\")","e8ac246e":"clf.best_params_","b296d58f":"!pwd","e1c429de":"best_model.fit(X[best_features], y)\ny_pred = best_model.predict(holdout[best_features])\nsubmission = pd.DataFrame({\"PassengerId\": holdout[\"PassengerId\"], \"Survived\": y_pred})\nsubmission.to_csv(\"submission.csv\", index=False)","71ba594d":"Feature selection will be done simulataneously with hyperparameter optimization.","e7e7c6fb":"### 4. Model Evaluation","98d9e0ff":"#### 3.2 Finding the best optimized model","d8aa4702":"**Kaggle Score**: 0.79425.","26d8370b":"##### k-fold cross-validation","f02ec558":"#### 3.1 Random Forest","57278404":"### 3. Model Training","8456b445":"#### Random Forest","78626d5f":"# Titanic Survival Prediction","9d4c91c9":"### 2. Feature Selection","9a66d3d6":"### 1. Data Preperation"}}