{"cell_type":{"1e7258d7":"code","e943c1b7":"code","10ea881f":"code","84f135d7":"code","d228d385":"code","5f46fc5e":"code","93678a9b":"code","ea64bfa9":"code","f258ac41":"code","f5b7e329":"code","e858fbce":"code","5af5c940":"code","1f9c8ee5":"code","c3e79d4c":"code","58c16b9c":"code","d88ba1f5":"code","c8d578b9":"code","cba2a174":"code","d900a0c9":"code","2699f4e0":"code","3af5c62a":"code","350a6807":"code","464c653f":"code","cb213f8e":"code","77445f35":"code","561c3ecd":"code","24d31aea":"code","70a7938c":"code","c26f23e1":"code","2d3b8390":"code","1296aabe":"code","b364b4ba":"code","7cd840a9":"code","08120d57":"code","928efa1d":"code","c8f30355":"code","841df2ec":"code","7b138632":"code","2f88bed6":"code","8fbbb75a":"code","fe7f49a0":"code","4fdc4128":"code","5ee27b60":"code","10144b81":"code","ee12acd3":"code","1e4a0d95":"code","22be6e2f":"code","346445b0":"code","4328abda":"code","fca55907":"code","a975e34f":"code","5167bfb9":"code","22c047b1":"code","f20563ba":"code","ab45749f":"code","d4b3da32":"code","1bedf0b6":"code","692c244e":"code","aa5e5862":"code","9dd0ea32":"code","51fd03a3":"code","db0d1fb0":"code","cb8c77c7":"code","7373a50b":"code","0b137c83":"code","a73eb4e4":"code","558d642c":"code","a222c2a0":"code","de1182f9":"code","92c1fec7":"code","9587bfaa":"code","bd4c4c69":"code","85496a6e":"code","b525e83a":"code","3bccdee4":"code","48339586":"code","508587b8":"code","6b8c92d2":"code","ed09b60e":"code","95fb0568":"code","6d57bba9":"code","223bada2":"code","1b17c1f9":"code","19dd0b7a":"code","2c264698":"code","ed12a54e":"code","83c03b7b":"code","5db20b21":"code","594a6c94":"code","a6b7d250":"code","7deb23e8":"code","361a3d1a":"code","bc815791":"code","5c79b591":"code","3504d41c":"code","61bb316c":"code","c25b8a06":"code","c10e658d":"code","3a7a6083":"code","1dbab422":"code","7e042c98":"code","ac8ad01e":"code","6c590a1d":"code","8c066e27":"code","a2f3c4ac":"code","c690f13b":"code","d1d16450":"code","185229c7":"code","a27236f6":"code","0728f6cb":"code","549c3c36":"code","dcdf24d4":"markdown","571f8270":"markdown","d1073641":"markdown","aae656bc":"markdown","acf30972":"markdown","339f962a":"markdown","6e25fd0c":"markdown","3acba3e1":"markdown","87b1a0ea":"markdown","beb5ebb3":"markdown","b2279600":"markdown","ec5adc03":"markdown","5ff28b20":"markdown","c20f11cc":"markdown","ebd7193b":"markdown","3689825e":"markdown","1189f3ee":"markdown","02e84cb2":"markdown","59c23055":"markdown","303b6fb0":"markdown","07478fb5":"markdown","7a2f7d23":"markdown","3375801b":"markdown","c386d43b":"markdown","b38f07f8":"markdown","c3204175":"markdown","af0dde48":"markdown","cc005a17":"markdown","af206fcd":"markdown","da0a7bbd":"markdown","213d0f20":"markdown","9a652b87":"markdown","ae0d5965":"markdown","d64a561f":"markdown","868f7198":"markdown","2cc06981":"markdown","ebb0c658":"markdown"},"source":{"1e7258d7":"import datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(style='darkgrid')\npd.set_option('display.float_format',lambda  x: '%.2f' %x)","e943c1b7":"#sales = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")","10ea881f":"sales = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_cats = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')","84f135d7":"print('sales:',sales.shape,'test:',test.shape,'items:',items.shape,'item_cats:',item_cats.shape,'shop:',shops.shape)","d228d385":"sales['date'] = pd.to_datetime(sales['date'], format='%d.%m.%Y')","5f46fc5e":"sales.head(3)","93678a9b":"test.head(3)","ea64bfa9":"items.head(3)","f258ac41":"item_cats.head(3)","f5b7e329":"shops.head(3)","e858fbce":"train = sales.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id',\n rsuffix='_').join(item_cats,on='item_category_id', rsuffix='_').drop(['item_id_', 'shop_id_', 'item_category_id_'], axis=1)","5af5c940":"train.head(3)","1f9c8ee5":"train.shape","c3e79d4c":"train.describe()","58c16b9c":"plt.figure(figsize=(35,10))\nsns.countplot(x='date_block_num',data=train)\nplt.show()","d88ba1f5":"plt.figure(figsize=(35,10))\nsns.countplot(x='shop_id', data=train)\nplt.show()","c8d578b9":"plt.figure(figsize=(35,10))\nsns.countplot(x='item_category_id',data= train)\nplt.show()","cba2a174":"monthwise_sale=pd.DataFrame(train.groupby(['date_block_num'])['item_cnt_day'].sum().reset_index())\nplt.figure(figsize=(35,10))\nsns.barplot(x='date_block_num',y='item_cnt_day',data= monthwise_sale,order=monthwise_sale['date_block_num'])\nplt.xlabel('Months')\nplt.ylabel('itemcount in a day')\nplt.title('item count in a day per month')\nplt.show()","d900a0c9":"ts = train.groupby(['date_block_num'])['item_cnt_day'].sum()\nplt.figure(figsize=(35,10))\nplt.xlabel = 'Time'\nplt.ylabel = 'Sales'\nplt.title = 'Total sale of the company'\nplt.plot(ts)","2699f4e0":"print('Min date from train set: %s'  %train['date'].min().date())\nprint('Max date from train set: %s' % train['date'].max().date())","3af5c62a":"shop_id_test = test['shop_id'].unique()\nitem_id_test = test['item_id'].unique()","350a6807":"# Only shop that exist in the test set\ntrain_k = train[train['shop_id'].isin(shop_id_test)]\n# Only item that exist in the test set\ntrain_k = train[train['item_id'].isin(item_id_test)]","464c653f":"print('Data set  size before leakage',train.shape[0])\nprint('Data set  size after leakage',train_k.shape[0])","cb213f8e":"train[train['item_price']<=0]","77445f35":"train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)]","561c3ecd":"median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&\n               (train.item_price>0)].item_price.median()","24d31aea":"median","70a7938c":"train.loc[train.item_price<0,'item_price'] =median","c26f23e1":"train_k.shape","2d3b8390":"train_monthly = train_k[['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']]","1296aabe":"train_monthly.shape","b364b4ba":"# Group by month in this case \"date_block_num\" and aggregate features.\ntrain_monthly = train_monthly.sort_values('date').groupby(['date_block_num', 'shop_id', 'item_category_id', 'item_id'], as_index=False)","7cd840a9":"train_monthly = train_monthly.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})","08120d57":"train_monthly.shape","928efa1d":"# Rename features.\ntrain_monthly.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt', 'transactions']","c8f30355":"train_monthly.head(3)","841df2ec":"train_monthly.isnull().sum()","7b138632":"# Build a data set with all the possible combinations of ['date_block_num','shop_id','item_id'] so we won't have missing records.\nshop_ids = train_monthly['shop_id'].unique()\nitem_ids = train_monthly['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])","2f88bed6":"empty_df.shape","8fbbb75a":"empty_df.head(3)","fe7f49a0":"train_monthly = pd.merge(empty_df, train_monthly, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_monthly.fillna(0, inplace=True)","4fdc4128":"train_monthly.describe()","5ee27b60":"#Extract Year and Month features\ntrain_monthly['year'] = train_monthly['date_block_num'].apply(lambda x: ((x\/\/12+2013)))","10144b81":"train_monthly['month'] = train_monthly['date_block_num'].apply(lambda x: (x%12))","ee12acd3":"gp_month_mean = train_monthly.groupby(['month'], as_index=False)['item_cnt'].mean()\ngp_month_sum = train_monthly.groupby(['month'],as_index=False)['item_cnt'].sum()\ngp_category_mean = train_monthly.groupby(['item_category_id'],as_index=False)['item_cnt'].mean()\ngp_category_sum = train_monthly.groupby(['item_category_id'],as_index=False)['item_cnt'].sum()\ngp_shop_mean = train_monthly.groupby(['shop_id'],as_index=False)['item_cnt'].mean()\ngp_shop_sum = train_monthly.groupby(['shop_id'],as_index=False)['item_cnt'].sum()","1e4a0d95":"f,axes = plt.subplots(2,1,figsize=(35,10))\nsns.barplot(x='shop_id',y='item_cnt',data= gp_shop_mean,ax= axes[0]).set_title('monthly average sale')\nsns.barplot(x='shop_id',y='item_cnt',data= gp_shop_sum,ax = axes[1]).set_title('monthly total sale')\nplt.show()","22be6e2f":"f,axes = plt.subplots(2,1,figsize=(35,10))\nsns.barplot(x='month',y='item_cnt',data= gp_month_mean,ax= axes[0]).set_title('monthly mean')\nsns.lineplot(x='month',y='item_cnt',data= gp_month_mean,ax= axes[0]).set_title('monthly mean')\nsns.barplot(x='month',y='item_cnt',data= gp_month_sum,ax = axes[1]).set_title('monthly sum')\nsns.lineplot(x='month',y='item_cnt',data= gp_month_sum,ax = axes[1]).set_title('monthly sum')\nplt.show()","346445b0":"f,axes = plt.subplots(2,1,figsize=(35,10))\nsns.barplot(x='item_category_id',y='item_cnt',data= gp_category_mean,ax= axes[0]).set_title('monthly mean sale by category')\nsns.barplot(x='item_category_id',y='item_cnt',data= gp_category_sum,ax = axes[1]).set_title('monthly total sale by category ')\nplt.show()","4328abda":"plt.subplots(figsize=(20,6))\n\nsns.boxplot(train_monthly['item_cnt'])\nplt.show()","fca55907":"plt.subplots(figsize=(20,6))\n\nsns.boxplot(train_monthly['item_price'])\nplt.show()","a975e34f":"train_monthly.shape","5167bfb9":"train_monthly = train_monthly.query('item_cnt >= 0 and item_cnt <= 20 and item_price < 400000')","22c047b1":"train_monthly.shape","f20563ba":"train_monthly['item_cnt_month'] = train_monthly.sort_values('date_block_num').groupby(['shop_id','item_id'])['item_cnt'].shift(-1)","ab45749f":"train_monthly.head(3)","d4b3da32":"train_monthly['item_price_unit'] = train_monthly['item_price']\/\/train_monthly['item_cnt']","1bedf0b6":"train_monthly.isnull().sum()","692c244e":"train_monthly['item_price_unit'].fillna(0, inplace=True)","aa5e5862":"gp_item_price = train_monthly.sort_values('date_block_num').groupby(['item_id'], as_index=False).agg({'item_price':[np.min, np.max]})\ngp_item_price.columns = ['item_id', 'hist_min_item_price', 'hist_max_item_price']\n\ntrain_monthly = pd.merge(train_monthly, gp_item_price, on='item_id', how='left')","9dd0ea32":"train_monthly.shape","51fd03a3":"train_monthly['price_increase'] = train_monthly['item_price'] - train_monthly['hist_min_item_price']\ntrain_monthly['price_decrease'] = train_monthly['hist_max_item_price'] - train_monthly['item_price']","db0d1fb0":"train_monthly.shape","cb8c77c7":"train_monthly.head()","7373a50b":"train_monthly.shape","0b137c83":"lag_list = [1, 2, 3]\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly[ft_name] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].shift(lag)\n    train_monthly[ft_name].fillna(0, inplace=True)","a73eb4e4":"train_monthly['item_cnt_shifted3'].max()","558d642c":"train_monthly.isnull().sum()","a222c2a0":"train_monthly['item_trend'] = train_monthly['item_cnt']\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly['item_trend'] -= train_monthly[ft_name]\n\ntrain_monthly['item_trend'] \/= len(lag_list) + 1","de1182f9":"train_monthly.describe()","92c1fec7":"train_set = train_monthly.query('date_block_num>=3 and date_block_num <28').copy()\nvalidation_set = train_monthly.query('date_block_num >= 28 and date_block_num < 33').copy()\ntest_set = train_monthly.query('date_block_num == 33').copy()\n\ntrain_set.dropna(subset= ['item_cnt_month'],inplace = True)\nvalidation_set.dropna(subset = ['item_cnt_month'],inplace = True)\ntrain_set.dropna(inplace = True)\nvalidation_set.dropna(inplace = True)\n\nprint('Train set records:',train_set.shape[0])\nprint('validation set records:',validation_set.shape[0])\nprint('Test set records:',test_set.shape[0])\n\nprint('Train set records: %s (%.f%% of complete data)'\n      % (train_set.shape[0],((train_set.shape[0]\/train_monthly.shape[0])*100)))\n\nprint('Validation set records: %s (%.f%% of complete data)'\n          % (validation_set.shape[0],((validation_set.shape[0]\/train_monthly.shape[0])*100)))","9587bfaa":"train_set.head(4)","bd4c4c69":"train_set.shape","85496a6e":"validation_set.head(4)","b525e83a":"validation_set.shape","3bccdee4":"test_set.head(4)","48339586":"test_set.shape","508587b8":"# Shop mean encoding.\ngp_shop_mean = train_set.groupby(['shop_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_mean.columns = ['shop_mean']\ngp_shop_mean.reset_index(inplace=True)\n# Item mean encoding.\ngp_item_mean = train_set.groupby(['item_id']).agg({'item_cnt_month': ['mean']})\ngp_item_mean.columns = ['item_mean']\ngp_item_mean.reset_index(inplace=True)\n# Shop with item mean encoding.\ngp_shop_item_mean = train_set.groupby(['shop_id', 'item_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_item_mean.columns = ['shop_item_mean']\ngp_shop_item_mean.reset_index(inplace=True)\n# Year mean encoding.\ngp_year_mean = train_set.groupby(['year']).agg({'item_cnt_month': ['mean']})\ngp_year_mean.columns = ['year_mean']\ngp_year_mean.reset_index(inplace=True)\n# Month mean encoding.\ngp_month_mean = train_set.groupby(['month']).agg({'item_cnt_month': ['mean']})\ngp_month_mean.columns = ['month_mean']\ngp_month_mean.reset_index(inplace=True)\n\n# Add meand encoding features to train set.\ntrain_set = pd.merge(train_set, gp_shop_mean, on=['shop_id'], how='left')\ntrain_set = pd.merge(train_set, gp_item_mean, on=['item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_year_mean, on=['year'], how='left')\ntrain_set = pd.merge(train_set, gp_month_mean, on=['month'], how='left')\n# Add meand encoding features to validation set.\nvalidation_set = pd.merge(validation_set, gp_shop_mean, on=['shop_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_item_mean, on=['item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_year_mean, on=['year'], how='left')\nvalidation_set = pd.merge(validation_set, gp_month_mean, on=['month'], how='left')","6b8c92d2":"train_set.head(4)","ed09b60e":"print('train shape:',train_set.shape,'validation shape:',validation_set.shape,'test shape:',test_set.shape)","95fb0568":"# Create train and validation sets and labels. \nX_train = train_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_train = train_set['item_cnt_month'].astype(int)\nX_validation = validation_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_validation = validation_set['item_cnt_month'].astype(int)","6d57bba9":"Y_train.head()","223bada2":"X_train.shape,Y_train.shape,X_validation.shape,Y_validation.shape","1b17c1f9":"test_set.shape","19dd0b7a":"latest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')","2c264698":"latest_records.shape","ed12a54e":"X_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])","83c03b7b":"X_test.head()","5db20b21":"X_train.head()","594a6c94":"X_train.shape","a6b7d250":"int_features = ['shop_id', 'item_id', 'year', 'month']\n\nX_train[int_features] = X_train[int_features].astype('int32')\nX_validation[int_features] = X_validation[int_features].astype('int32')","7deb23e8":"X_test['year'] = 2015\nX_test['month'] = 9\nX_test.drop('item_cnt_month', axis=1, inplace=True)\nX_test[int_features] = X_test[int_features].astype('int32')\nX_test = X_test[X_train.columns]","361a3d1a":"X_test.head()","bc815791":"X_test.isnull().sum()","5c79b591":"sets = [X_train, X_validation, X_test]\n# Median of each shop is used for filling missing value.            \nfor dataset in sets:\n    for shop_id in dataset['shop_id'].unique():\n        for column in dataset.columns:\n            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median\n            \n# Fill remaining missing values \nX_test.fillna(X_test.mean(), inplace=True)","3504d41c":"# Dropping \"item_category_id\",as given test dont have Item category\nX_train.drop(['item_category_id'], axis=1, inplace=True)\nX_validation.drop(['item_category_id'], axis=1, inplace=True)\nX_test.drop(['item_category_id'], axis=1, inplace=True)","61bb316c":"X_train.head()","c25b8a06":"X_validation.head()","c10e658d":"X_test.head()","3a7a6083":"X_test.describe()","1dbab422":"#X_train.to_pickle('..\/working\/X_train.pkl')\n#X_test.to_pickle('..\/working\/X_test.pkl')\n#Y_validation.to_pickle('..\/working\/Y_validation.pkl')\n#X_validation.to_pickle('..\/working\/X_validation.pkl')\n#Y_train.to_pickle('..\/working\/Y_train.pkl')\n","7e042c98":"#import pickle\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance","ac8ad01e":"#X_train = pd.read_pickle('..\/working\/X_train.pkl')\n#X_test = pd.read_pickle('..\/working\/X_test.pkl')\n#Y_validation = pd.read_pickle('..\/working\/Y_validation.pkl')\n#X_validation = pd.read_pickle('..\/working\/X_validation.pkl')\n#Y_train = pd.read_pickle('..\/working\/Y_train.pkl')","6c590a1d":"xg_reg = XGBRegressor(\n    n_jobs=-1,\n    tree_method='exact',\n    learning_rate=0.2,\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\n","8c066e27":"xg_reg.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_validation, Y_validation)], \n    verbose=10, \n    early_stopping_rounds = 3)","a2f3c4ac":"Y_pred = xg_reg.predict(X_validation).clip(0, 20)\nY_test = xg_reg.predict(X_test).clip(0, 20)","c690f13b":"submission = pd.DataFrame({\n    \"ID\": X_test.index, \n    \"item_cnt_month\": Y_test\n})","d1d16450":"#submission.to_csv('xgb_submission.csv', index=False)","185229c7":"def plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(xg_reg, (10,14))","a27236f6":"from sklearn.metrics import mean_squared_error","0728f6cb":"#xgboost_train_pred = xg_reg.predict(X_train)\nxgboost_val_pred = xg_reg.predict(X_validation)\n","549c3c36":"\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, xgboost_val_pred)))","dcdf24d4":"We need to forcast 'item_cnt',","571f8270":"Unit price for the item","d1073641":"**Joining Dataset **","aae656bc":"Create additional features","acf30972":"Dataset after feature engineering","339f962a":"**Data Exploration**","6e25fd0c":"**Period** <br>\n Let us check the data from  date till date.","3acba3e1":"We have 15 column now","87b1a0ea":"Merge the train set with the complete set (missing records will be filled with 0).","beb5ebb3":"More features","b2279600":"To get real behavior of the data we have to create the missing records from the loaded dataset, so for each month(date_block_num) we need to create the missing records for each shop and item.","ec5adc03":"How much each item's price changed from its (lowest\/highest) historical price","5ff28b20":"Change date form to DDMMYYYY","c20f11cc":"Only few category have more sales count","ebd7193b":"#### Create .pkl file <br>\n We are Saving the Train,Validation,Test data as .pkl file  to save the time. These file can be call any time when ever we required for create  model .","3689825e":"### Create Model\n #### Import required Libraries.","1189f3ee":"**Build test set** <br>\n We want to predict for \"date_block_num\" -34 .We use block 33 because we want to forecast values for block 34.","02e84cb2":"There is peak sale at begenning of the year and then decreasing trend .(In between 2 month there is peak in sale)","59c23055":"Pandas dataframe. shift() function Shift index by desired number of periods with an optional time freq. This function takes a scalar parameter called period, which represents the number of shifts to be made over the desired axis.","303b6fb0":"**Train\/validation split**<br>\n  Train set will be the  3~28 months, validation will be 29~32 months and test will be block 33.<br>","07478fb5":"Sales increases towards the end of the year<br>\n **Sales by Category**","7a2f7d23":"Ther is one item with one item below zero,fill it with median","3375801b":"**Data Pre processing**<br>\nSelect only numerical features as we are working with numerical features","c386d43b":"#### Treating Outliers","b38f07f8":"Data exploration with additional features<br>\n Sales by Shop","c3204175":"* Import Libraries\n* Load Data\n* Data Exploration\n* Data Pre processing\n* Feature Engineering\n* Checking Outliers\n* Train\/ Validation Split\n* Create .pkl file\n* Create Model\n* Submission","af0dde48":"#### Contents<br>","cc005a17":"**Load Data**","af206fcd":"Except 4 shpos most of the shops have similar sales rate <br>\n \n **Yearly sales behaviar**","da0a7bbd":"Replacing missing values.","213d0f20":"Monthly sales","9a652b87":"**Import Libraries**","ae0d5965":"**Data Leakage** <br>\nIn test we have the column shop_id,item_id","d64a561f":"#### Checking Outliers","868f7198":"**Mean Encoding** <br>\n done after the train\/validation split.<br>\n We are calculating Shop mean,Item mean,Shop-item Mean,Year mean,Month mean","2cc06981":"#### Feature Engineering","ebb0c658":"Item sales count trend."}}