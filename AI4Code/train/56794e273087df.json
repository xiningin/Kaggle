{"cell_type":{"2c018e0c":"code","bb743e88":"code","ea7d4b26":"code","6c87b513":"code","5c6a2020":"code","5f367f9a":"code","69ff50af":"code","d62c9d67":"code","f45008ea":"code","fe04710e":"code","f820c2cd":"code","0b3bcd01":"code","ffda0f39":"code","af526a11":"code","b01d712e":"code","e34cfd70":"code","dafce8f0":"code","f70d01d9":"code","6a2dc7fa":"code","84214562":"code","f90cb0a5":"code","e2178683":"code","689717e4":"code","2b77d68f":"code","2a16dc09":"code","31e59d65":"code","5be0723b":"code","7ad1910e":"code","bf022b17":"code","37829e7c":"code","e009eefc":"code","5a8e784b":"code","7cc5688f":"code","d0a5e65d":"code","915e384b":"code","5c93c504":"code","817f0633":"code","bf3df30c":"code","03bd77b5":"code","015e98c2":"code","d4c551dd":"code","cb4ff300":"code","d6072260":"code","49b47490":"code","8c5612e2":"code","fb839d27":"code","0ee00f05":"code","35ae330c":"code","a41f1e83":"code","faba1c3a":"code","c739aab9":"code","421c2951":"markdown","e7f6898d":"markdown","01ca2c31":"markdown","45b13112":"markdown","bc44bb5c":"markdown","68144add":"markdown","3cc3e7aa":"markdown","d3101dce":"markdown","5f3095f2":"markdown","22a0384b":"markdown","59795258":"markdown","9f68c5e4":"markdown","dae2d13e":"markdown","85fbf410":"markdown","4c9fec85":"markdown","98c7f8b1":"markdown","37f07c5c":"markdown","db9f757e":"markdown","534744f0":"markdown","1b70e385":"markdown","7210d49a":"markdown","e163c2c7":"markdown","18b2547e":"markdown","78c32f4f":"markdown","8f10b8b7":"markdown","b41c5a1f":"markdown","1a1eacf7":"markdown","6f86e177":"markdown","0f1ded76":"markdown","c32a1bc0":"markdown","775b86e6":"markdown","8625f452":"markdown","5a209a58":"markdown","efff0baf":"markdown","10ef8c9c":"markdown","44ce2e57":"markdown","fb4e3636":"markdown","2c9b63f3":"markdown","b7ac4a33":"markdown","15c1ab90":"markdown"},"source":{"2c018e0c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bb743e88":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","ea7d4b26":"df.describe()","6c87b513":"df.columns","5c6a2020":"import missingno as msno\n%matplotlib inline\nmsno.matrix(df)","5f367f9a":"print(\"Number of Null values present in the dataset are\", df.isnull().sum().sum())","69ff50af":"from bokeh.io import output_notebook\nfrom bokeh.io import show\nfrom bokeh.plotting import figure\nfrom bokeh.transform import cumsum\nfrom bokeh.palettes import Spectral6\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.layouts import gridplot","d62c9d67":"output_notebook()","f45008ea":"from math import pi\nfraud_and_not_fraud = {'Fraud' : df['Class'].value_counts()[1], \n                             'Not Fraud' : df['Class'].value_counts()[0]}\n\ndata = pd.Series(fraud_and_not_fraud).reset_index(name = 'value').rename(columns = {'index':'Fraud'})\ndata['angle'] = data['value']\/data['value'].sum() * 2 * pi\ndata['color'] = ['skyblue', 'salmon']","fe04710e":"p = figure(plot_height = 300, plot_width = 300, title = \"Pie Chart\", toolbar_location = None,\n           tools = \"hover\", tooltips = \"@Fraud: @value\", x_range=(-0.5, 1.0))\n\np.wedge(x = 0, y = 1, radius = 0.4,\n        start_angle = cumsum('angle', include_zero = True), end_angle = cumsum('angle'),\n        line_color = \"white\", fill_color = 'color', legend_field = 'Fraud', source = data)\np.legend.location = \"top_right\"\np.legend.label_text_font_size = '5pt'","f820c2cd":"unique = ['Fraud', 'Not Fraud']\ntop = [df['Class'].value_counts()[1], df['Class'].value_counts()[0]]\nsource = ColumnDataSource(data = dict(Fraud = unique, counts = top, color = Spectral6))","0b3bcd01":"p1 = figure(\n    x_range = unique,\n    plot_height= 300,\n    plot_width = 300,\n    title = 'Count of Fraud and Not Fraud',\n    tools = \"hover\", tooltips = \"@Fraud: @counts\"\n)\n\np1.vbar(\n    x = 'Fraud',\n    top = 'counts',\n    bottom = 0,\n    width = 0.9,\n    source = source,\n    color = 'color'\n)\n\nshow(gridplot([[p,p1]]))","ffda0f39":"from scipy.stats import skew \nhist, edges = np.histogram(df['Amount'], density = True, bins = 50)\np2 = figure(\n    x_axis_label = 'Amount',\n    title = 'Distribution of Amount'\n)\n\np2.quad(\n    bottom = 0,\n    top = hist,\n    left = edges[:-1],\n    right = edges[1:],\n    line_color = 'white'\n)","af526a11":"hist, edges = np.histogram(df['Time'], density = True, bins = 50)\np3 = figure(\n    x_axis_label = 'Time',\n    title = 'Distribution of Time'\n)\n\np3.quad(\n    bottom = 0,\n    top = hist,\n    left = edges[:-1],\n    right = edges[1:],\n    line_color = 'white'\n)","b01d712e":"show(gridplot([[p2,p3]]))","e34cfd70":"print(\"The skewness of the Amount feature is: \", df['Amount'].skew())\nprint(\"The skewness of the Time feature is: \", df['Time'].skew())","dafce8f0":"from sklearn.preprocessing import RobustScaler\n\nrs = RobustScaler()\n\ndf['scaled_amount'] = rs.fit_transform(df['Amount'].values.reshape(-1, 1))\ndf['scaled_time'] = rs.fit_transform(df['Time'].values.reshape(-1, 1))\n\ndf.drop(['Amount', 'Time'], axis = 1, inplace = True)","f70d01d9":"df.head(5)","6a2dc7fa":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis = 1, inplace = True)\n\ndf.insert(0,'Scaled_Amount', scaled_amount)\ndf.insert(1, 'Scaled_Time', scaled_time)\n\ndf.head(5)","84214562":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nX = df.drop(['Class'], axis = 1)\ny = df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0, \n                                                    stratify = y)","f90cb0a5":"lr = LogisticRegression(max_iter = 1000)\n\nlr.fit(X_train, y_train)\nprediction_lr = lr.predict(X_test)","e2178683":"from sklearn.metrics import confusion_matrix, precision_recall_curve, classification_report, roc_auc_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\ndef evaluation(y_actual, predicted):\n    cnf_matrix = confusion_matrix(y_actual, predicted)\n\n    sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = \"YlGnBu\", fmt = 'g')\n    plt.ylabel('Actual Label')\n    plt.xlabel('Predicted Label')\n    \n    labels = ['Non-Fraud', 'Fraud']\n    print(classification_report(y_actual, predicted, target_names = labels))","689717e4":"evaluation(y_test, prediction_lr)","2b77d68f":"y_pred_prob = lr.predict_proba(X_test)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","2a16dc09":"print(\"AUC score is: \", roc_auc_score(y_test, prediction_lr))","31e59d65":"from imblearn.over_sampling import SMOTE\n\nprint(\"Shape of the X_train before applying SMOTE is: \", X_train.shape)\nprint(\"Shape of the y_train data before applying SMOTE is: \", y_train.shape)\nprint(\"Shape of the X_test before applying SMOTE is: \", X_test.shape)\nprint(\"Shape of the y_test before applying SMOTE is: \", y_test.shape)\nprint()\nprint(\"Before applying SMOTE the count of the fraud cases are: \", y_train.value_counts().values[1])\nprint(\"Before applying SMOTE the count of the non-fraud cases are: \", y_train.value_counts().values[0])\n\nsm = SMOTE(random_state = 2)\nX_train_s, y_train_s = sm.fit_sample(X_train, y_train.ravel())\n\nprint(\"Shape of the X_train after applying SMOTE is: \", X_train_s.shape)\nprint(\"Shape of the y_train data after applying SMOTE is: \", y_train_s.shape)\nprint()\nprint(\"After applying SMOTE the count of the fraud cases are: \", sum(y_train_s == 1))\nprint(\"After applying SMOTE the count of the non-fraud cases are: \", sum(y_train_s == 0))\nprint(\"After applying SMOTE the percentage of the fraud cases are: \", sum(y_train_s == 1)\/ len(y_train_s) * 100.0)\nprint(\"After applying SMOTE the percentage of the non-fraud cases are: \", sum(y_train_s == 0)\/ len(y_train_s) * 100.0)\n","5be0723b":"unique = ['Fraud', 'Not Fraud']\ntop = [sum(y_train_s == 1), sum(y_train_s == 0)]\nsource = ColumnDataSource(data = dict(Fraud = unique, counts = top, color = Spectral6))","7ad1910e":"p5 = figure(\n    x_range = unique,\n    plot_height= 600,\n    plot_width = 600,\n    title = 'Count of Fraud and Not Fraud',\n    tools = \"hover\", tooltips = \"@Fraud: @counts\"\n)\n\np5.vbar(\n    x = 'Fraud',\n    top = 'counts',\n    bottom = 0,\n    width = 0.9,\n    source = source,\n    color = 'color'\n)\n\nshow(p5)","bf022b17":"lr_smote = LogisticRegression(max_iter = 1000)\nlr_smote.fit(X_train_s, y_train_s)\n\npredicted_lr_smote = lr_smote.predict(X_test)\nevaluation(y_test, predicted_lr_smote)","37829e7c":"y_pred_prob = lr_smote.predict_proba(X_test)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","e009eefc":"print(\"AUC score is: \", roc_auc_score(y_test, predicted_lr_smote))","5a8e784b":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators = 200)\n\nrfc.fit(X_train_s, y_train_s)\n\npredict_rfc_smote = rfc.predict(X_test)\n\nevaluation(y_test, predict_rfc_smote)","7cc5688f":"y_pred_prob = rfc.predict_proba(X_test)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","d0a5e65d":"print(\"AUC score is: \", roc_auc_score(y_test, predict_rfc_smote))","915e384b":"from xgboost import XGBClassifier\n\nxgbc = XGBClassifier(n_estimators = 1000, verbosity = 1, scale_pos_weight = 580)\n\nxgbc.fit(X_train, y_train)\n\npred_xgb = xgbc.predict(X_test)\n\nevaluation(y_test, pred_xgb)","5c93c504":"y_pred_prob = xgbc.predict_proba(X_test)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","817f0633":"print(\"AUC score is: \", roc_auc_score(y_test, pred_xgb))","bf3df30c":"df.var()","03bd77b5":"from sklearn.feature_selection import RFE","015e98c2":"model = XGBClassifier(n_estimators = 500, verbosity = 1, scale_pos_weight = 580)\nselector = RFE(model, n_features_to_select = 15, step=1)\nselector.fit(X_train, y_train)","d4c551dd":"selector.support_","cb4ff300":"a = selector.support_\ntrue_index = [i + 1 if a[i] == True else False for i in range(0, len(a))]\nselected_cols = []\ncols = df.columns\nfor i in true_index:\n    if i == False:\n        pass\n    else:\n        selected_cols.append(cols[i - 1])\n        \nprint(\"15 selected features are: \", selected_cols)","d6072260":"prediction_rfe = selector.predict(X_test)\nevaluation(y_test, prediction_rfe)","49b47490":"y_pred_prob = selector.predict_proba(X_test)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","8c5612e2":"print(\"AUC score is: \", roc_auc_score(y_test, prediction_rfe))","fb839d27":"model1 = XGBClassifier(n_estimators = 1000, verbosity = 1, scale_pos_weight = 580)\nselector1 = RFE(model1, n_features_to_select = 20, step = 1)\nselector1.fit(X_train, y_train)","0ee00f05":"selector1.support_","35ae330c":"b = selector1.support_\ntrue_index = [i + 1 if b[i] == True else False for i in range(0, len(b))]\nselected_cols = []\ncols = df.columns\nfor i in true_index:\n    if i == False:\n        pass\n    else:\n        selected_cols.append(cols[i - 1])\n        \nprint(\"20 selected features are: \", selected_cols)","a41f1e83":"prediction_rfe1 = selector1.predict(X_test)\nevaluation(y_test, prediction_rfe1)","faba1c3a":"y_pred_prob = selector1.predict_proba(X_test)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","c739aab9":"print(\"AUC score is: \", roc_auc_score(y_test, prediction_rfe1))","421c2951":"We will need to scale both the variables. We will be using Robust Scaler to preprocess the dataset. As the name suggests it is robust to outliers present in the dataset. \n\nOne approach to data scaling involves calculating the mean and standard deviation of each variable and using these values to scale the values to have a mean of zero and a standard deviation of one, a so-called \u201cstandard normal\u201d probability distribution. This process is called standardization and is most useful when input variables have a Gaussian probability distribution.\n\nStandardization is calculated by subtracting the mean value and dividing by the standard deviation.\n\n> value = (value \u2013 mean) \/ stdev\n\nSometimes an input variable may have outlier values. These are values on the edge of the distribution that may have a low probability of occurrence, yet are overrepresented for some reason. Outliers can skew a probability distribution and make data scaling using standardization difficult as the calculated mean and standard deviation will be skewed by the presence of the outliers.\n\nOne approach to standardizing input variables in the presence of outliers is to ignore the outliers from the calculation of the mean and standard deviation, then use the calculated values to scale the variable.\n\nThis is called robust standardization or robust data scaling.\n\nThis can be achieved by calculating the median (50th percentile) and the 25th and 75th percentiles. The values of each variable then have their median subtracted and are divided by the interquartile range (IQR) which is the difference between the 75th and 25th percentiles.\n\n> value = (value \u2013 median) \/ (p75 \u2013 p25)","e7f6898d":"From the above two plots we can observe that most of the people were not-fraud and only 492 people were identified as fraud. This indicates **high level of imbalance in the dataset**. Thus we would have to use methods like SMOTE, OverSampling, UnderSampling etc to make an efficient model.\n\nNow suppose that after applying the above methods, we want to evaluate our model, **will we be using accuracy to evaluate our model**?\n\nSuppose we have a dataset where there is a huge difference between the number of positive classes and negative classes or in other words the dataset is imbalanced or have skewed targets. When we train the model on such a dataset, it will not have seen enough examples for the class, which is less in number. What will happen is that while predicting the target, it will always predict the class which is larger in number.\n\nFor example, Imagine the dataset used for classifying the tumor as Malignant or Benign has 90 images where the tumor is benign, and only 10 images where the tumor is malignant, or in other words the dataset has skewed targets. As there are more negative or \u20180\u2019 class than positive or \u20181\u2019 class (difference of 80), while performing predictions it will always predict the class to be negative or \u20180\u2019 as there are not enough malignant images which can be used by the model to train itself. When we calculate the accuracy for such a model, the accuracy still comes out to be 90. Is the model good? No, it\u2019s not, because the model is not classifying any tumor to be malignant. We can conclude that even though accuracy is 90, the model is useless.\n\nHence for tackling such kinds of problems, other metrics such as **Precision, Recall, and F1 score** are recommended. In order to read more about evaluation metrics check out this [blog](https:\/\/medium.com\/analytics-vidhya\/even-models-need-to-pass-tests-part-2-540318f1d272).\n","01ca2c31":"# Importing Dataset","45b13112":"SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line. Specifically, a random example from the minority class is first chosen. This procedure can be used to create as many synthetic examples for the minority class as are required. The approach is effective because new synthetic examples from the minority class are created that are plausible, that is, are relatively close in feature space to existing examples from the minority class. A general downside of the approach is that synthetic examples are created without considering the majority class, possibly resulting in ambiguous examples if there is a strong overlap for the classes.\n\nBefore applying SMOTE, we should understand that SMOTE should be applied to only training dataset and not the whole dataset because we will need to test the model on the actual dataset. If we had applied SMOTE to test dataset, there would be many examples which are not real and are generated with help of SMOTE so we shouldn't test our model on those data points","bc44bb5c":"**Before performing any classification task, we should check the distribution of the classes.**\n\n**Why does the distribution of the class matter?**\n\n**Suppose that out of 1000 data points if 990 data points are non-fraud and only 10 data points are fraud. If we are to build a Machine Learing Model for this case, by training the model on such dataset, the model will predict non-fraud for all cases because it has learnt the parameters on non-fraud cases which are in abundance. There are very less data points for fraud cases. But is this ML model good? It's not, because one of the most important aim of building such kind of Machine Learning Model is to accurately detect Fraud Cases so that the company can take instant action. By classifying all data points as non-fraud, there can be huge loss for the company.**","68144add":"Now let's increase the number of features to be selected to 20.","3cc3e7aa":"# Random Forest with SMOTE","d3101dce":"We can observe that the XGB model trained on all the features was a little better than this model. This model have misclassified 30 data points and the F1-score is less by 0.01. ","5f3095f2":"WOW! Without using SMOTE, the precision value increased to 0.94 and recall to 0.80. FNs remained the same as with Random Forest with SMOTE but FPs decreased to 5, it's great isn't it? F1-score increased by 0.01 to 0.86.","22a0384b":"This model is better than the previous model which was trained on 15 features. It misclassifies 27 data points. The F1-score is similar to that of XGBoost. This indicates that the increase in number of features is benefitting the model.","59795258":"As you might observe that it is taking a lot of time to train a model on such a large dataset which was obtained after applying SMOTE algorithm. Instead of using SMOTE, there is a hyperparameter in XGBoost called scale_pos_weight. \n\nBy default, the scale_pos_weight hyperparameter is set to the value of 1.0 and has the effect of weighing the balance of positive examples, relative to negative examples when boosting decision trees. Gradients are used as the basis for fitting subsequent trees added to boost or correct errors made by the existing state of the ensemble of decision trees. \n\nThe scale_pos_weight value is used to scale the gradient for the positive class. This has the effect of scaling errors made by the model during training on the positive class and encourages the model to over-correct them. In turn, this can help the model achieve better performance when making predictions on the positive class. A sensible default value to set for the scale_pos_weight hyperparameter is the inverse of the class distribution. \n\nThus for XGBoost we will try to use this hyperparameter and not SMOTE.","9f68c5e4":"# Feature Selection","dae2d13e":"Till now we have trained our model on all the features of a dataset, there might be some correlated features which are decreasing the evaluation metrics score, let's try a method called 'Recursive Feature Elimination'. In this method, we start with all the features and keep removing one feature in every iteration that provides the least value to a given model. In each iteration, we can eliminate the least important features and keep eliminating it until we reach the number of features needed.","85fbf410":"From the above plot we can observe that total number of null values present in the dataset are zero. Believe me having zero null values in the dataset reduces quite a lot of work load. ","4c9fec85":"As compared to the previous model without SMOTE, we can observe that recall value have increased which means that our model is able to detect fraud transactions but the value of precision value have decreased  drastically which means that the model has classfied a lot of non-frauds as frauds whcih is not good for the institution.","98c7f8b1":"**In this notebook we are going to classify a given data point as Fraud and Non-Fraud by training Machine Learning Models on the given dataset. As it's clear from the question that this is a case of imbalanced dataset, we will also learn how to deal with such kind of datasets. We will be using Bokeh to plot all our graphs.**\n\n**Please upvote the notebook if you liked it!**\n\n**Most importantly please provide feeback as I am always ready to learn from others!**\n\n**Let's get started.**","37f07c5c":"# XGBoost","db9f757e":"Area Under the Curve(AUC) represents the degree or measure of separability or in other words it tells how much chance the model will be able to distinguish between '+' and '-' classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By using Logistic Regression without SMOTE the model will be able to distinguish between '+' and '-' classes 82% of times. ","534744f0":"# SMOTE","1b70e385":"## Recursive Feature Elimination","7210d49a":"# Null Values","e163c2c7":"Though the AUC score does not have major difference.","18b2547e":"# Credit Card Fraud Detection","78c32f4f":"**Hope you learned something new from the notebook!**\n\n**Please upvote the notebook if you liked it and please provide your valuable feedback!**","8f10b8b7":"# Logistic Regression with Original Data","b41c5a1f":"All the features are scaled.","1a1eacf7":"Let's move the 'scaled_amount' and 'scaled_time' features to the beginning of the dataset so that the 'Class' feature remains the last feature of the dataset.","6f86e177":"# Logistic Regression with SMOTE","0f1ded76":"AUC score has increased to 93%, but this is due to high number of correct predictions for the negative class as suggested by precision value. We need to take into consideration all the evaluation metrics(precision, recall and F1-score).","c32a1bc0":"The Precision Recall curve is quite different from the expected curve.","775b86e6":"![creditcardfraud.jpg](attachment:creditcardfraud.jpg)","8625f452":"# Class Distribution","5a209a58":"There are about 30 features given in the dataset, as we don't know the names for 25 columns, we can't judge whether those features are related to the Class in real life. Let's try out various methods which can help us to decide some important features.\n\nFirst, let's check out the variance of each column. The simplest form of selecting features would be to remove features with very low variance. If the features have a very low variance, they are close to being constant and thus do not add any value to any model at all. It would just be nice to get rid of them and hence lower the complexity.","efff0baf":"First we will retain 15 features out of 30.","10ef8c9c":"We can see that the model is performing poorly for detecting Fraud Transactions. Our model predicted non-fraud transactions 35 times when the transaction was fraud in actual and predicted fraud transactions 11 times where the transaction was indeed non-fraud. Let's have a look at the classification report to get a better understanding.\n\nWe can see from the above classification report that the model is able to detect Non_Fraud cases with perfect precision and recall or perfect f1-score. But the model performs very poorly for detecting 'Fraud' cases. Eventhough Precision is high but due to Precision-Recall tradeoff the recall is very poor. In other words, our model is only able to detect only 64 percent of the fraud cases and 85 percent of the times when our model says it's a fraud transaction, the transaction is fault in actual also. As F1 Score is Harmonic Mean of Precision and Recall, it solves the problem of Precision-Recall Tradeoff. The F1-score is pretty less for Fraud cases. Now keeping this as our base performance, let's try to increase our score.","44ce2e57":"I think we have found a better model than Logistic Regression, precision value increasead by a hug margin to 0.92 but the value of recall decreased to 0.80. F1-score becomes 0.85 which is the best we have got so far. False Negatives(20) decreased and False Positives(7) have also decreased which means that our model have made only 27 wrong predictions. Let's experiment more to get better results.","fb4e3636":"Recursive Feature Elimination is one of the ways through which we can select some of the most important features. There are other methods like SelectKBest,removing correlated features etc which I will explore in the furture versions. ","2c9b63f3":"# Standardizing Dataset","b7ac4a33":"Usually we delete those columns having variance less than 0.1, but here all features have column greater than 0.1 thus we will retain all the features","15c1ab90":"The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) (Except for time and amount).\n\nKeep in mind that in order to implement a PCA transformation features need to be previously scaled. (In this case, all the V features have been scaled) Therefore let's checkout the distribution of time and amount features."}}