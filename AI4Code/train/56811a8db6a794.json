{"cell_type":{"9acb1671":"code","7adf219e":"code","53afa350":"code","9c647a9b":"code","4cb322e0":"code","46433946":"code","9dc64f86":"code","7a4c54e4":"code","8bc1be69":"code","07721683":"code","df612e6c":"code","2874c008":"code","6c087a13":"code","e66cff1e":"code","3fdff46d":"code","3f9c3da5":"code","4eb16909":"code","08a3cfee":"code","0736ad7a":"code","744809dd":"code","4fb7f9c0":"code","59b7aea0":"markdown","9676c088":"markdown","6ae23a42":"markdown","4845d8e0":"markdown","43f271f0":"markdown","7fab9b70":"markdown","17988236":"markdown","f8095e4f":"markdown","44641132":"markdown","29fb3537":"markdown"},"source":{"9acb1671":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy  # linear algebra\nimport pandas  # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7adf219e":"import pathlib\nfire_image_path = \"..\/input\/forest-fire-images\/Data\/Train_Data\/Fire\"\nnon_fire_path = \"..\/input\/forest-fire-images\/Data\/Train_Data\/Non_Fire\"\nfire_image_path = pathlib.Path(fire_image_path)\nnon_fire_path = pathlib.Path(non_fire_path)","53afa350":"import glob","9c647a9b":"train_data_images = {\n    \"Fire\":list(fire_image_path.glob(\"*.jpg\")),\n    \"NonFire\":list(non_fire_path.glob(\"*.jpg\"))\n}\ntrain_labels = {\n    \"Fire\":0,\"NonFire\":1\n}","4cb322e0":"import PIL\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn \nimport tensorflow as tf\nfrom tensorflow import keras\n%matplotlib inline","46433946":"X, y = [], []\nfor label, images in train_data_images.items():\n    for image in images:\n        img = cv2.imread(str(image)) # Reading the image\n        if img is not None:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (180, 180))\n            X.append(img)\n            y.append(train_labels[label])","9dc64f86":"import numpy\nX_samp = numpy.array(X)\ny_samp = numpy.array(y)","7a4c54e4":"X_samp = (X_samp\/255)","8bc1be69":"X_samp[0].shape","07721683":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_samp, y_samp, test_size=0.2)","df612e6c":"data_argumentation = keras.Sequential([\n    keras.layers.experimental.preprocessing.RandomContrast(0.3),\n    keras.layers.experimental.preprocessing.RandomRotation(0.2),\n    keras.layers.experimental.preprocessing.RandomZoom(0.5)\n])","2874c008":"for i in range(5):\n    plt.imshow(data_argumentation(X_train[0]))\n    plt.show()","6c087a13":"model = keras.Sequential([\n    data_argumentation,\n    keras.layers.Conv2D(64, (3,3), padding='same', activation=\"relu\", input_shape=(180, 180, 3)),\n    keras.layers.MaxPooling2D(),\n    keras.layers.Conv2D(32, (3,3), padding='same', activation=\"softmax\"),\n    keras.layers.MaxPooling2D(),\n    keras.layers.Conv2D(16, (3,3), padding='same', activation=\"softmax\"),\n    keras.layers.MaxPooling2D(),\n    keras.layers.Dropout(0.2),\n    keras.layers.Flatten(),\n    keras.layers.Dense(10, activation=\"sigmoid\"),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=[\"accuracy\"])\nmodel.fit(X_train, y_train, epochs=20)","e66cff1e":"model.evaluate(X_test, y_test)","3fdff46d":"test_fire = \"..\/input\/forest-fire-images\/Data\/Test_Data\/Fire\"\ntest_non_fire = \"..\/input\/forest-fire-images\/Data\/Test_Data\/Non_Fire\"\ntest_fire = pathlib.Path(test_fire)\ntest_non_fire = pathlib.Path(test_non_fire)","3f9c3da5":"test = {\n    \"Fire\":list(test_fire.glob(\"*.jpg\")),\n    \"NonFire\":list(test_non_fire.glob(\"*.jpg\"))\n}","4eb16909":"test_array = []\nfor label, images in test.items():\n    for image in images:\n        img = cv2.imread(str(image)) # Reading the test image\n        if img is not None:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (180, 180))\n            test_array.append(img)\ntest_array = numpy.array(test_array)\n","08a3cfee":"test_array = test_array\/ 255\npred = model.predict(test_array)","0736ad7a":"preds = pred.round(decimals=0).flatten()\nresults = []\nfor i in preds:\n    if i==0:\n        results.append(\"Fire\")\n    elif i == 1:\n        results.append(\"Non Fire\")","744809dd":"fire = 0\nnonFire = 0\nfor i in results:\n    if i==\"Fire\":\n        fire += 1\n    else:\n        nonFire += 1\nprint(fire, \"Fire images out of \", 25)\nprint(nonFire , 'Non fire images out of', 25)","4fb7f9c0":"font1 = {'family':'serif','color':'blue','size':20} # Custom font for my title\nfor i in range(50):\n    plt.imshow(test_array[i])\n    plt.title(results[i], fontdict=font1)\n    plt.axis('off')\n    plt.show()","59b7aea0":"Data Argumentation is a process of increasing contrast, zooming to make the model unserstand better","9676c088":"# Evaluation","6ae23a42":"# Reading images and storing it in an array","4845d8e0":"# Preprocessing","43f271f0":"# Model Building","7fab9b70":"# Specifying path","17988236":"***Thanks codebasics for teaching me deep learning.***","f8095e4f":"# Data Argumentation","44641132":"# Testing Dataset","29fb3537":"***Feel free  to comment or upvote***"}}