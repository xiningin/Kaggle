{"cell_type":{"cf461bb7":"code","bdf6cfe2":"code","ce62f952":"code","9b04558a":"code","c42f4da4":"code","c8d1c6e9":"code","54688698":"code","cc917c45":"code","ee988867":"code","cef7e269":"code","860a1b02":"code","aae17b9b":"code","6a8ea75e":"code","f69c2e45":"code","d7236f9f":"code","b6d3ca76":"code","6a3bf3ca":"code","4b44c21b":"code","22bb0337":"code","8e9b83a9":"code","f4fa285d":"code","50e45e19":"code","d74d637a":"code","b93cc5a5":"markdown","f2ff06bb":"markdown","d8549527":"markdown","e367176c":"markdown","6a176749":"markdown"},"source":{"cf461bb7":"# Code previously used to load data and some libraries\n\n# The basic\nimport pandas as pd\nimport numpy as np\n\n# Helpers\nfrom sklearn.metrics import mean_absolute_error, make_scorer\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\n# Models\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n\n# Path of the file to read\niowa_file_path = '..\/input\/train.csv'\nhome_data = pd.read_csv(iowa_file_path)\n\n\n# path to file used for predictions\ntest_data_path = '..\/input\/test.csv'\ntest_data = pd.read_csv(test_data_path)","bdf6cfe2":"# Creating target object\ny = home_data.SalePrice\n\n# Defining X\nX_orig = home_data.drop('SalePrice', axis=1)\n\n# Maintaining only numeric features over here and...\nX_numeric_orig = X_orig.select_dtypes(exclude=['object'])\n\n# ...Establishing the categoricals now!\nX = pd.get_dummies(X_orig)\n\n# Also, establishing the categoricals on the test data\ntest_X_categoricals = pd.get_dummies(test_data)\n\n# creating test_X with only the numeric columns maintaining the previous pattern\ntest_X_numeric_orig = test_data.select_dtypes(exclude=['object'])","ce62f952":"# Listing features\n\nXColumns = X.columns.tolist()\nTestxColumns = test_X_categoricals.columns.tolist()","9b04558a":"print('Learning base columns count: \\n{}\\n'.format(len(XColumns)))\nprint('Testing base columns count: \\n{}\\n'.format(len(TestxColumns)))","c42f4da4":"Features = []\nDiscard = []\n\n# Note that the learning base have more columns than the testing base. Let's drop some of them.\nfor item in XColumns:\n    if item in TestxColumns:\n        Features.append(item)\n    else:\n        Discard.append(item)","c8d1c6e9":"# Redefining X\nX = X[Features]","54688698":"print(\"We've dropped the X columns on the learning data as below: \\n\")\nprint(Discard)","cc917c45":"# Safety checking!\n\nXColumns = X.columns.tolist()\nTestxColumns = test_X_categoricals.columns.tolist()\n\nprint('New learning base columns count: \\n{}\\n'.format(len(XColumns)))\nprint('Testing base columns count: \\n{}\\n'.format(len(TestxColumns)))\nprint('.\\n.\\n.\\n.\\n\\nGreat!!!')","ee988867":"# Imputing values\n\nimputer = SimpleImputer()\n\nX = pd.DataFrame(imputer.fit_transform(X))\ntest_X_categoricals = pd.DataFrame(imputer.fit_transform(test_X_categoricals))","cef7e269":"# Aaaannnnd some other variables here\nrandom_State_Seed = 1\nscoring_type = 'neg_mean_absolute_error'","860a1b02":"# Making scorer for the metric we'll use for the competition\ndef metric(val_predictions,val_y):\n    return mean_absolute_error(val_predictions, val_y)\n\n# Make scorer for scikit-learn\nscorer = make_scorer(metric)","aae17b9b":"dectree_model = DecisionTreeRegressor()\n\n# Grid for Decision Tree\ndectree_grid = {\n    'max_depth': [10, 20, 50, 80, 100],\n    'random_state': [random_State_Seed],\n    'max_leaf_nodes': [10, 20, 50, 80, 100, 500]\n}","6a8ea75e":"# Search parameter space\ndectree_gridsearch = GridSearchCV(estimator = dectree_model, \n                                      param_grid = dectree_grid, \n                                      cv = 3, \n                                      n_jobs = -1, \n                                      verbose = 1, \n                                      scoring=scorer)","f69c2e45":"rf_model = RandomForestRegressor()\n\n# Grid for Random Forest\nrf_grid = {\n    'n_estimators': [200, 500, 800, 1000],\n    'random_state': [random_State_Seed],\n    'max_leaf_nodes': [10, 20, 50, 80, 100, 500],\n    \n}","d7236f9f":"# Search parameter space\nrf_gridsearch = GridSearchCV(estimator = rf_model, \n                                      param_grid = rf_grid, \n                                      cv = 3, \n                                      n_jobs = -1, \n                                      verbose = 1, \n                                      scoring=scorer)","b6d3ca76":"xgb_model = XGBRegressor()\n\n# XGB Grid\nxgb_grid = {\n    'learning_rate': [0.05, 0.07, 0.10], \n    'n_estimators': [800, 1000, 1500],\n    'seed': [random_State_Seed],\n}\n\n# Split into validation and training data (XGB requires because of its evaluations)\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=random_State_Seed)\n\nevaluation_set = [(val_X, val_y)]\n\nfit_params={\"early_stopping_rounds\": 5, \n            \"eval_metric\" : \"mae\", \n            \"eval_set\" : evaluation_set}","6a3bf3ca":"# Search parameters\n\nxgb_gridsearch = GridSearchCV(estimator = xgb_model, \n                              param_grid = xgb_grid, \n                              #fit_params = fit_params, \n                              cv = 3,\n                              n_jobs = -1, \n                              verbose = 1, \n                              scoring=scorer)","4b44c21b":"dectree_gridsearch.fit(X, y)","22bb0337":"rf_gridsearch.fit(X, y)","8e9b83a9":"xgb_gridsearch.fit(train_X, train_y, **fit_params) # Explainability about the **","f4fa285d":"# So...\n\nprint(\"So... What are the best parameters for each model?\\n\\n\")\n\nprint('The best params for Decision Tree model:\\n{}\\n'.format(dectree_gridsearch.best_params_))\nprint('The best params for Random Forest model:\\n{}\\n'.format(rf_gridsearch.best_params_))\nprint('The best params for XBGBoost model:\\n{}\\n'.format(xgb_gridsearch.best_params_))","50e45e19":"# And now, predicting...\n\nprint(\"What's the mae for each model?\\n\\n\")\n\nprint('Error for Dec Tree:  {0:10.4f}'.format(metric(dectree_gridsearch.predict(X), y)))\nprint('Error for Random Forest: {0:10.4f}'.format(metric(rf_gridsearch.predict(X), y)))\nprint('Error for XGBoost:  {0:10.4f}'.format(metric(xgb_gridsearch.predict(X), y)))","d74d637a":"\"\"\"This is the original Submission cell. We've changed this one to the end of the notebook in order\nto send the best model for the competition\"\"\"\n\n# make predictions which we will submit. \ntest_preds = xgb_gridsearch.predict(test_X_categoricals)\n\n# The lines below shows you how to save your data in the format needed to score it in the competition\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\n\noutput.to_csv('submission.csv', index=False)","b93cc5a5":"# Testing the Work \/ Forking notebook\nAfter filling in the code above:\n1. Click the **Commit and Run** button. \n2. After the code has finished running, click the small double brackets **<<** in the upper left of your screen.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n3. Go to the output tab at top of your screen. Select the button to submit your file to the competition.  \n4. If you want to keep working to improve your model, select the edit button. Then you can change your model and repeat the process.\n","f2ff06bb":"# First of all\n\nFirstly, we've forked the Machine Learning Competitions notebook in order to check if we're able to achieve a better accuracy for our model, despite the bruteforcing process when using GridSearchCV. As before, we're creating and submiting predictions for a Kaggle competition, as per considered in the [Machine Learning Course \/ Housing Prices Competition](https:\/\/www.kaggle.com\/learn\/machine-learning).\n\nNotes and inspirations:\n\nhttps:\/\/www.kaggle.com\/mrshih\/here-s-a-house-salad-it-s-on-the-house\n\nhttps:\/\/www.kaggle.com\/carlolepelaars\/eda-and-ensembling\n\nhttps:\/\/www.kaggle.com\/learn\/machine-learning (The original learning track for this competition)","d8549527":"Now, let's predict using the best model over our testing base!","e367176c":"## Recap and reload!\nHere's the code we've written so far. Let's start by loading the libraries and databases, running it again and defining X, y and some other constants.\n\n*Note that we'll use the cross validation process throughout this notebook although we could also use train_test_split*","6a176749":"Our *learning base* doesn't have the same features as our *predicting base*. Let's consider the same features, only, or our **predict** command will fail later."}}