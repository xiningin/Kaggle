{"cell_type":{"6e56d7cb":"code","31f6c918":"code","16d6010b":"code","605c23dc":"code","50047821":"code","06659516":"code","2babcf7b":"code","8ea1a2d5":"code","1fdddb47":"code","ccd3319b":"code","20995b18":"code","0b4061b2":"code","21f9cddd":"code","b295a9a3":"code","6c0224eb":"code","6ad465e5":"code","6f6c54c2":"code","492dee2d":"code","6d1552e6":"code","734e2693":"code","7484d3a0":"code","1d743dd5":"code","bcb5d50a":"code","53bd395f":"code","ee444eae":"code","f4c62618":"code","82f0be11":"code","4f45ce53":"code","067f996c":"code","1d02ec1a":"code","13f6d98e":"code","59f97cef":"code","5bd6de58":"code","1f21191e":"code","226910c5":"code","5c8314ed":"code","b5214b13":"code","bbc92caa":"code","31e92f35":"code","7a30fb5d":"code","14c150f9":"code","65c3a732":"code","474341aa":"markdown","830ca93a":"markdown","976b2b88":"markdown","ad1781df":"markdown","83f515e1":"markdown","2cc15012":"markdown","bf024788":"markdown","94985bd3":"markdown","ea77bc27":"markdown","94523664":"markdown","a7c9b663":"markdown","54b5ad04":"markdown","9a2e07ee":"markdown","f4c76320":"markdown","f0691fdc":"markdown","e8a093ec":"markdown","1fecfb05":"markdown","41dab53c":"markdown"},"source":{"6e56d7cb":"pip install JPype1","31f6c918":"import re, os, pickle\nfrom collections import Counter\nfrom pathlib import Path\nimport nltk\nfrom nltk.corpus import stopwords\nfrom string import punctuation, digits\nimport itertools\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nfrom string import punctuation, digits\nimport jpype as jp\nfrom keras.layers import LSTM\nfrom keras.layers import Flatten\nfrom sklearn.preprocessing import scale\nfrom gensim.models import KeyedVectors\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf\nimport keras","16d6010b":"def lower(text):\n    text=re.sub(\"\u0130\",\"i\",text)\n    text = text.lower()\n    return text","605c23dc":"def resubComma(texts):\n    texts= re.sub(\",\",\" \",texts)\n    return texts","50047821":"WPT = nltk.WordPunctTokenizer()\nstop_word_list = nltk.corpus.stopwords.words('turkish')\nprint(stop_word_list)","06659516":"def replace_emoticon(word):\n    check_pos = re.findall(r'(?::\\)|:-\\)|=\\)|:D|:d|<3|\\(:|:\\'\\)|\\^\\^|;\\)|\\(-:)', word)\n    check_neg = re.findall(r'(:-\\(|:\\(|;\\(|;-\\(|=\\(|:\/|:\\\\|-_-|\\):|\\)-:)', word)\n    if check_pos:\n        #word = \":)\"\n        word = \"SMILEYPOSITIVE\"\n    elif check_neg:\n        #word = \":(\"\n        word = \"SMILEYNEGATIVE\"\n    return word","2babcf7b":"def vanish_punc(text):\n    regex = re.compile('[%s]' % re.escape(punctuation))\n    text = regex.sub(' ', text)\n    return text","8ea1a2d5":"def vanish_digits(text):\n    text=text.strip()\n    vanish_digits = str.maketrans('', '', digits)\n    text=text.translate(vanish_digits)\n    return text","1fdddb47":"def dup_vanish(s1):\n     return (''.join(i for i, _ in itertools.groupby(s1)))","ccd3319b":"def reverse(s): \n    if len(s) == 0: \n        return s \n    else: \n        return reverse(s[1:]) + s[0] \n\ndef checkOpennes(word):\n    vowels=['a','e','i','\u0131','o','\u00f6','u','\u00fc']\n    open_vowels=['e','i','\u00fc','\u00f6']\n    close_vowels=['a','\u0131','o','u']\n    for i in range(len(word)):\n        if reverse(word)[i] in vowels:\n            if reverse(word)[i] in open_vowels:\n                return True\n            else:\n                return False\n        else:\n            continue\ndef PresentCheck(word):\n        ei=['e','i']\n        a\u0131=['a','\u0131']\n        \u00fc\u00f6=['\u00fc','\u00f6']\n        uo=['u','o']\n        for i in range(len(word)):\n            if reverse(word)[i] in ei:\n                return 'ei'\n            elif reverse(word)[i] in \u00fc\u00f6:\n                return '\u00fc\u00f6'\n            elif reverse(word)[i] in a\u0131:\n                return 'a\u0131'\n            elif reverse(word)[i] in uo:\n                return 'uo'\n            else:\n                continue\n    \ndef StartCheck(word):\n        ei=['e','i']\n        a\u0131=['a','\u0131']\n        \u00fc\u00f6=['\u00fc','\u00f6']\n        uo=['u','o']\n        for i in range(len(word)):\n            if (word)[i] in ei:\n                return 'ei'\n            elif (word)[i] in \u00fc\u00f6:\n                return '\u00fc\u00f6'\n            elif (word)[i] in a\u0131:\n                return 'a\u0131'\n            elif (word)[i] in uo:\n                return 'uo'\n            else:\n                continue","20995b18":"WORDS = dict()\n\nspell_folder = Path(r\"..\/\/input\/\/turkish-spell-check\")\n\ndef words(text): return re.findall(r'\\w+', text.lower())\nwith open(os.path.expanduser(Path(spell_folder\/ \"big2.txt\")), \"r\", encoding = 'utf-8') as f:\n    for line in f:\n        splitted = line.split()\n        WORDS[splitted[0]] = int(splitted[1])","0b4061b2":"def replaceall(s, n,a):\n    occurence = s.count(n)\n    alt = []\n    temp = s\n    for i in range(occurence):\n        temp2 = temp\n        for j in range(i,occurence):\n            temp2 = temp2.replace(n,a,1)\n            alt.append(temp2)\n        temp = temp.replace(n,\"!\",1)\n    for i in range(len(alt)):\n        alt[i] = alt[i].replace(\"!\",n)\n\n    return alt\n\ndef P(word, N=sum(WORDS.values())):\n    \"Probability of `word`.\"\n    if  word in WORDS.keys():\n        number = WORDS[word]\n    else:\n        number = 1\n    if number == 0:\n        number = 1\n    return number \/ N\n\ndef correction(word):\n      return max(candi(word), key=P)\n\ndef candi(word):\n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\ndef known(words):\n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abc\u00e7defg\u011fh\u0131ijklmno\u00f6prs\u015ftu\u00fcvyzw'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n \n    sp     = replaceall(word,'\u0131','i')\n    sp2     = replaceall(word,'u','\u00fc')\n    sp3    = replaceall(word,'o','\u00f6')\n    sp4     = replaceall(word,'g','\u011f')\n    sp5     = replaceall(word,'c','\u00e7')\n    sp6     = replaceall(word,'s','\u015f')\n    sp7     = replaceall(word,'i','\u0131')\n    sp8     = replaceall(word,'\u00f6','o')\n    sp9     = replaceall(word,'\u015f','s')\n    sp10     = replaceall(word,'\u011f','g')\n    sp11     = replaceall(word,'\u00e7','c')\n    sp12     = replaceall(word,'\u00fc','u')\n    specials=[]\n    specials.extend(sp)\n    specials.extend(sp2)\n    specials.extend(sp3)\n    specials.extend(sp4)\n    specials.extend(sp5)\n    specials.extend(sp6)\n    specials.extend(sp7)\n    specials.extend(sp8)\n    specials.extend(sp9)\n    specials.extend(sp10)\n    specials.extend(sp11)\n    specials.extend(sp12)\n    return set(deletes+transposes+replaces+inserts+specials)\n\ndef edits2(word):\n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\ndef print_diff(word, s):\n    if not word == s:\n        print(word + \" --> \" + s)\ncounter = 0\n","21f9cddd":"ZEMBEREK_PATH = r'..\/input\/zemberek\/zemberek-full.jar'\njp.startJVM(jp.getDefaultJVMPath(), '-ea', '-Djava.class.path=%s' % (ZEMBEREK_PATH))","b295a9a3":"\nTurkishMorphology = jp.JClass('zemberek.morphology.TurkishMorphology')\nTurkishSpellChecker = jp.JClass('zemberek.normalization.TurkishSpellChecker')\nTurkishSentenceNormalizer = jp.JClass('zemberek.normalization.TurkishSentenceNormalizer')\nPaths = jp.JClass('java.nio.file.Paths')\n# Get the path to the (baseline) lookup files\nlookupRoot = Paths.get(r'..\/input\/zemberek2\/normalization')\n# Get the path to the compressed bi-gram language model\nlmPath = Paths.get(r'..\/input\/zemberek3\/lm.2gram.slm')\nmorphology = TurkishMorphology.createWithDefaults()\n# Initialize the TurkishSentenceNormalizer class\n# Instantiate the morphology class with the default RootLexicon\nmorph = TurkishMorphology.createWithDefaults()\n\n# Instantiate the spell checker class using the morphology instance\nspell = TurkishSpellChecker(morph)\n\n#normalizer = TurkishSentenceNormalizer(morphology, lookupRoot, lmPath)","6c0224eb":"morphology = TurkishMorphology.createWithDefaults()\n\n\ndef lemmatizer(word,texts):\n        wordList=[]\n        wordList = re.sub(\"[^\\w]\", \" \",  texts).split()\n        if '\ufffd' in word:\n            return 'question'\n        pos=wordList.index(word)\n        if word==\"SMILEYPOSITIVE\":\n            return word\n        if word==\"SMILEYNEGATIVE\":\n            return word\n        sakin=''\n        word=correction(word)\n        if len(wordList)-pos>3 and pos>2:\n            for i, kelime in enumerate(wordList[pos-3:pos+4]):\n                sakin=sakin+correction(kelime)+' '\n        elif pos<=2 and len(wordList)-pos>5:\n            for i, kelime in enumerate(wordList[pos:pos+5]):\n                sakin=sakin+correction(kelime)+' '\n        elif pos<=2 and len(wordList)-pos<=5:\n            for i, kelime in enumerate(wordList[pos:len(wordList)]):\n                sakin=sakin+correction(kelime)+' '\n        elif len(wordList)-pos<1 and pos>3:\n            for i, kelime in enumerate(wordList[pos-3:len(wordList)]):\n                sakin=sakin+correction(kelime)+' '\n        elif len(wordList)<3:\n            for i, kelime in enumerate(wordList):\n                sakin=sakin+correction(kelime)+' '\n        else:\n             for i, kelime in enumerate(wordList):\n                sakin=sakin+correction(kelime)+' '\n        results = morphology.analyze(word)\n        lemma=[]\n        form=[]\n        l=[]\n        m=[]\n        for i, result in enumerate(results):\n            form.append(str(result.formatLong()))\n            lemma.append(result.getLemmas()[0])\n        if len(lemma)>1:\n                analysis = morphology.analyzeSentence(sakin)\n                results = morphology.disambiguate(sakin, analysis).bestAnalysis()\n                for i, result in enumerate(results):\n                        l.append(result.getLemmas()[0])\n                        m.append(result.formatLong())\n                for i in range(len(m)):\n                    for j in range(len(form)):\n                        if m[i]==form[j]:\n                            lema=lemma[j]\n                            if lema=='de\u011fil':\n                                return 'de\u011fil'\n                            if 'Neg' in form[j] or 'WithoutHavingDoneSo' in form[j] or 'Unable' in form[j]:\n                                if checkOpennes(word):\n                                    return lema+'me'\n                                else:\n                                    return lema+'ma'\n                            if 'Without' in form[j]:\n                                if PresentCheck(word)=='ei':\n                                    return lema+'siz'\n                                elif PresentCheck(word)=='a\u0131':\n                                    return lema+'s\u0131z'\n                                elif PresentCheck(word)=='uo':\n                                    return lema+'suz'\n                                else:\n                                    return lema+'s\u00fcz'\n                            if 'With' in form[j]:\n                                if PresentCheck(word)=='ei':\n                                    return lema+'li'\n                                elif PresentCheck(word)=='a\u0131':\n                                    return lema+'l\u0131'\n                                elif PresentCheck(word)=='uo':\n                                    return lema+'lu'\n                                else:\n                                    return lema+'l\u00fc'\n                            else:\n                                return lema\n                    else:\n                        continue\n        elif len(lemma)==1:\n            if lemma[0]=='de\u011fil':\n                return lemma[0]\n            if 'Neg' in form[0] or 'WithoutHavingDoneSo' in form[0] or 'Unable' in form[0]:\n                 if checkOpennes(word):\n                    return lemma[0]+'me'\n                 else:\n                    return lemma[0]+'ma'\n            elif 'Without' in form[0]:\n                if PresentCheck(word)=='ei':\n                    return lemma[0]+'siz'\n                elif PresentCheck(word)=='a\u0131':\n                    return lemma[0]+'s\u0131z'\n                elif PresentCheck(word)=='uo':\n                    return lemma[0]+'suz'\n                else:\n                    return lemma[0]+'s\u00fcz'\n            elif 'With' in form[0]:\n                if PresentCheck(word)=='ei':\n                    return lemma[0]+'li'\n                elif PresentCheck(word)=='a\u0131':\n                    return lemma[0]+'l\u0131'\n                elif PresentCheck(word)=='uo':\n                    return lemma[0]+'lu'\n                else:\n                    return lemma[0]+'l\u00fc'\n            else:\n                return lemma[0]\n        else:\n            return word\n","6ad465e5":"def clean_messages(texts):\n    texts=lower(texts)\n    texts=resubComma(texts)\n    tokens=WPT.tokenize(texts)\n    text = [token for token in tokens if token not in stop_word_list]\n    for i, word in enumerate(text):\n        text[i]=dup_vanish(vanish_digits(vanish_punc(replace_emoticon((word)))))\n    text = list(filter(None,text))\n    metin=' '.join(text)\n    for i, word in enumerate(text):\n         text[i] = lemmatizer(word,metin)\n    test=' '.join(text)\n    return test","6f6c54c2":"data=pd.read_csv(r\"..\/input\/yemeksepeti\/yemeklerin_sepeti.csv\")","492dee2d":"data=data.dropna()\ndata = data.drop_duplicates(subset=['yorum'])\ndata=data.reset_index()\ndata=data.drop('index',axis=1)","6d1552e6":"for i in range(len(data.hiz)):\n    data.hiz[i]=re.sub(\"\\D\", \"\", data.hiz[i])\nfor i in range(len(data.servis)):\n    data.servis[i]=re.sub(\"\\D\", \"\", data.servis[i])\nfor i in range(len(data.lezzet)):\n    data.lezzet[i]=re.sub(\"\\D\", \"\", data.lezzet[i])\n       ","734e2693":"data['hiz'] = data['hiz'].astype(int)\ndata['servis'] = data['servis'].astype(int)\ndata['lezzet'] = data['lezzet'].astype(int)","7484d3a0":"data.loc[data['hiz']<4, 'hiz'] = 0\ndata.loc[data['hiz'] >7, 'hiz'] = 1\ndata.loc[data['lezzet']<4, 'lezzet'] = 0\ndata.loc[data['lezzet'] >7, 'lezzet'] = 1\ndata.loc[data['servis']<4, 'servis'] = 0\ndata.loc[data['servis'] >7, 'servis'] = 1\n\n","1d743dd5":"data=data[(data['hiz']<2) & (data['servis']<2) & (data['lezzet']<2)]","bcb5d50a":"data_positive=data[(data['hiz']==1) & (data['servis']==1) & (data['lezzet']==1)]\ndata_negative=data[(data['hiz']==0) & (data['servis']==0) & (data['lezzet']==0)]","53bd395f":"data_positive=data_positive[:600]","ee444eae":"data=pd.concat([data_positive,data_negative],axis=0)\ndata=data.drop_duplicates()","f4c62618":"print(data.head())\nprint(data.tail())","82f0be11":"data['isPositive']=np.where(((data['hiz']==1) & (data['servis']==1) & (data['lezzet']==1)),1,0)\ndata=data.drop([\"hiz\",\"servis\",\"lezzet\"],axis=1)","4f45ce53":"new=[]\nfor message in data['yorum']:\n      new.append(clean_messages(message))","067f996c":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(new, data['isPositive'], test_size=0.2 ,random_state=1)","1d02ec1a":"print(x_train[:5])\nprint(x_train[-5:])","13f6d98e":"word_vectors = KeyedVectors.load_word2vec_format(r\"..\/\/input\/\/wordvec\/\/trmodel\", binary=True)","59f97cef":"def buildWordVector(text, size=400):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0.\n    for word in text:\n        try:\n            vec += word_vectors[word].reshape((1, size))\n            count += 1.\n        except KeyError:\n            continue\n    if count != 0:\n        vec \/= count\n    return vec\n","5bd6de58":"tokensTrain=[]\nfor i in range(0,len(x_train)):\n    tokensTrain.append(WPT.tokenize(x_train[i]))\n\ntrain_vecs = np.concatenate([buildWordVector(z) for z in tokensTrain])\ntrain_vecs=scale(train_vecs)\n\ntokensTest=[]\nfor i in range(0,len(x_test)):\n    tokensTest.append(WPT.tokenize(x_test[i]))\n\ntest_vecs = np.concatenate([buildWordVector(z) for z in tokensTest])\ntest_vecs=scale(test_vecs)\n","1f21191e":"test_vecs = np.reshape(test_vecs, (test_vecs.shape[0], 1,test_vecs.shape[1]))\ntrain_vecs  = np.reshape(train_vecs , (train_vecs.shape[0],1,train_vecs.shape[1]))","226910c5":"model = Sequential()\nmodel.add(LSTM(200,input_shape=(1,train_vecs.shape[2])))\nmodel.add(Dense(units=1, activation='sigmoid'))\n\nmy_optimizer = tf.keras.optimizers.Adam(lr=0.0001)\nmodel.compile(loss='binary_crossentropy', optimizer=my_optimizer, metrics=['accuracy'])\nmodel.summary()\n\nmc = keras.callbacks.ModelCheckpoint(\"..\/working\/best_val_loss\", monitor='val_loss', verbose=0, save_best_only=True, mode='min',\n                                     save_weights_only=True)\n\nes = keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=10)","5c8314ed":"model.fit(train_vecs[:-300], y_train[:-300],\n          epochs=600,batch_size=2, verbose=2,\n         validation_data=(train_vecs[-300:], y_train[-300:]),\n          callbacks=[es,mc])","b5214b13":"(model.load_weights(\"..\/working\/best_val_loss\"))","bbc92caa":"y_pred=model.predict(test_vecs,\n                     batch_size=1,\n                     verbose=1, steps=None)","31e92f35":"y_pred=y_pred>0.5\n","7a30fb5d":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_pred=y_pred,y_true=y_test)\nprint('True Positives: ',cm[1,1])\nprint('False Positives: ',cm[0,1])\nprint('True Negatives: ',cm[0,0])\nprint('False Negatives: ',cm[1,0])\n","14c150f9":"from sklearn.metrics import accuracy_score\naccuracy=accuracy_score(y_test,y_pred)\nprint('Accuracy: %f' % accuracy)","65c3a732":"y_test = y_test.reset_index(drop = True)\ny_test=np.reshape(y_test,(len(y_test,)))\ny_pred=np.reshape(y_pred,(len(y_pred,)))\n\nx_test = pd.Series(x_test)\ny_test=pd.Series(y_test)\ny_pred=pd.Series(y_pred)\ninspection = pd.concat([x_test, y_test, y_pred], axis=1)\n\ninspection.columns=['Yorum', 'Test','Predictions']\n\nwrong_predictions = inspection[inspection[\"Predictions\"] != inspection[\"Test\"]]\nprint(wrong_predictions)","474341aa":"The lower and resubComma functions are pretty self-explanatory.\n\nStopwords are words that are used too often in sentences but they don't change the meaning of sentence. Because of this, we need to remove them all from our sentences. I used NLTK library's stopwords list to specify them.","830ca93a":"In some cases, digits can be useful. However, in this project, I don't think that they are worth to deal with. So I remove them all.","976b2b88":"The functions in the up cell is some kind of algorithm that tries to correct spelling. It tries many possible combinations of the word and then search these candidate words in Orhan Bilgin's dictionary and select the most probable one. ","ad1781df":"# Data Manipulation","83f515e1":"Reshaped the datas to fit LSTM model.","2cc15012":"Let's examine some of preprocessed comments.","bf024788":"We want to remove all punctuations because they don't have a direct meaning at all. But before, maybe we can use some of the punctuations in favor of us. The replace_emoticon function is doing that for us and it converts some emoticons to words. It is really game changing preprocessing step especially when it comes to sentiment analysis, because they directly filled with emotions. ","94985bd3":"# Analysis Of Predictions\n\n\nThese comments might seem meaningless because they are preprocessed comments and we can say that the wrong predictions are likely be more meaningless than true predictions. If we had a perfect word2vec model, we didn't have to make a complex lemmatizer like this. As I see, many wrong predictions caused by one positive and one negative word side by side that changes meaning of sentence. For example, \"iyi de\u011fil\" or \"g\u00fczel de\u011fil\". Again, this is caused by our pretrained word2vec model is not perfect. In my opinion, some comments are inconsistent with points. For instance, in this trial, the comment is \"sen ye eski sevgili gel ak\u0131l SMILEYNEGATIVE\" If I were the model and evaluate this comment I would give negative, too. However, commentor gave high points.\n\nThank you for reading.\nBest regards :)\n\nEge Ba\u015ft\u00fcrk","ea77bc27":"# The Model","94523664":"# Preprocess Functions","a7c9b663":"Let's examine some of the comments.","54b5ad04":"This step is for excited and angry writers who don't care about spelling rules. For example, think about this kind of comment. \"Yemeeeek harikayd\u0131\u0131\u0131\u0131.\" We can not expect this kind of comment can be interpreted just regularly by our neural nets. So we need to make it a regular comment. Like, \"Yemek harikayd\u0131\". The function named dup_vanish maks exactly this. However, there is a trade-off. Because of we vanish all the duplicates in words, some regular words are become misspelled. Like, \"Yemek m\u00fckemmeldi.\".After dup_vanish function it becomes \"Yemek m\u00fckemeldi\" which is not as we wanted. In the next steps, we are trying to solve this little problem","9a2e07ee":"# The Goal\nThe goal of this project is making a sentiment analysis on comments of the most popular online food order and delivery company of Turkey which is Yemeksepeti in Turkish Language. In the beginning of this study, I have read a lot of articles about NLP studies about English language and also some about Turkish Language. My main finding about this research was Turkish Language suffers scarcity of these type of NLP works. The structure of Turkish and English is really different and for that reason, the preprocessing steps should be different, too. So I decided to go deeper in NLP, understand the underlying logic behind it and what should be the correct way of preprocessing steps for sentiment analysis in Turkish. Let's begin.","f4c76320":"To avoid overfitting, we saved our model with best validation loss and now we will load it.","f0691fdc":"Lastly' let's inspect our wrong predictions.","e8a093ec":"# Word Embeddings in Turkish\n\nI used https:\/\/github.com\/akoksal\/Turkish-Word2Vec this rep and this pretrained model to convert words to vectors.","1fecfb05":"For spell correction dictionary I have used Orhan Bilgin's frequency dictionary. It includes lots of word and these frequences and selected the most probable word always.","41dab53c":"# The Lemmatizer\n\nIn English, there is very limited ways of making a sentence negative.(Simply, \"not\"). However, in Turkish there are many ways to do that like word \"de\u011fil\", \"-me\" afformative to the verb, word \"yok\" and many more ways. We should not remove this words and afformatives remark that negativity of sentence since we are making sentiment analysis. So I tried to this lemmatizer without removing this words and afformatives.\n\nIt is very basic and not optimized lemmatizer. Thank you for making this Zemberek library!"}}