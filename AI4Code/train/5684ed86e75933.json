{"cell_type":{"5b1833fe":"code","a3e50b79":"code","c5751e73":"code","80c94cb0":"code","a1016334":"code","c3f3b782":"code","887dad06":"code","39370224":"code","fa977ddb":"code","98d54a04":"code","2347b115":"code","15893445":"code","dcc74bde":"code","11619596":"code","ad44d3cb":"code","2ec1e98c":"code","fa224d6e":"code","54e30371":"code","3923da59":"code","6c5d6365":"code","f2acdeda":"code","6cb1216f":"code","b0ead504":"code","26d20192":"code","f3d550c1":"code","df1140bc":"markdown","8bae6cf7":"markdown","7f3af145":"markdown"},"source":{"5b1833fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a3e50b79":"from IPython.display import clear_output\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport datetime\nimport tensorflow as tf\nimport keras\nimport itertools\nimport matplotlib\nimport matplotlib.pyplot as plt\n#from pylab import rcParams\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization, Input, concatenate, Reshape\nfrom keras import layers, Input, Model\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras import models, regularizers, optimizers\nfrom keras.models import Sequential, save_model\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA\nfrom sklearn.utils import class_weight\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom sklearn.metrics import log_loss\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import Convolution1D\nfrom keras.layers.convolutional import MaxPooling1D\nimport pandas as pd\nimport random\nfrom tensorflow.keras.layers import Dense, Conv2DTranspose, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import ResNet50\nimport math\nfrom PIL import Image\nimport plotly.express as px\nimport math\nprint('Done..')","c5751e73":"train=pd.read_csv('..\/input\/emnist\/emnist-digits-train.csv')\ntest=pd.read_csv('..\/input\/emnist\/emnist-digits-test.csv')\na=['label']\nc=0\nfor i in range(784):\n    a.append('pixel'+str(i))\ntrain.columns=a\ntest.columns=a    ","80c94cb0":"df=pd.read_csv('..\/input\/digit-recognizer\/train.csv')\nX=df.drop('label', axis=1)\ny=df.label\ndf.head()","a1016334":"x_train=df.drop('label', axis=1)\nY_train=df.label\nx_train=x_train.values\nY_train=Y_train.values","c3f3b782":"import matplotlib.pyplot as plt\nprint('Some examples from training set...')\n# look at some of the digits from train_X\nplt.figure(figsize=(15,6))\nfor i in range(40):  \n    plt.subplot(4, 10, i+1)\n    plt.imshow(x_train[i].reshape((28,28)),cmap=plt.cm.binary)\n    plt.title(\"label=%d\" % Y_train[i],y=0.9)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","887dad06":"x_train=train.drop('label', axis=1)\nY_train=train.label\nx_train=x_train.values\nY_train=Y_train.values","39370224":"import matplotlib.pyplot as plt\nprint('Some examples from additional dataset...')\nplt.figure(figsize=(15,6))\nfor i in range(40):  \n    plt.subplot(4, 10, i+1)\n    plt.imshow(x_train[i].reshape((28,28)),cmap=plt.cm.binary)\n    plt.title(\"label=%d\" % Y_train[i],y=0.9)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","fa977ddb":"del x_train\ndel Y_train","98d54a04":"df=pd.concat([df,train,test])\ndf.reset_index()","2347b115":"train=pd.read_csv('..\/input\/emnist\/emnist-mnist-train.csv')\ntest=pd.read_csv('..\/input\/emnist\/emnist-mnist-test.csv')\ntrain.columns=a\ntest.columns=a  ","15893445":"df=pd.concat([df,train,test])\ndf.reset_index()","dcc74bde":"df = df.sample(frac = 1)\nX=df.drop('label', axis=1)\ny=df.label\ny=pd.get_dummies(y)","11619596":"from sklearn.model_selection import train_test_split\n\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.15, random_state=42)\ntest_df=pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nX_test = test_df","ad44d3cb":"del X\ndel y\ndel df","2ec1e98c":"def create_model(num_column):\n    model = Sequential()\n    model.add(tf.keras.layers.Reshape((28, 28, 1), input_shape=(784,)))\n    model.add(BatchNormalization())\n    model.add(layers.Conv2D(64, (3,3), activation='relu', input_shape=(28,28,1)))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n    model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n    model.add(BatchNormalization()) \n    model.add(Dropout(0.4))\n    \n    model.add(Flatten())\n    model.add(Dense(units=200, activation='relu'))\n    model.add(Dense(units=100, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(units=10, activation='softmax'))\n    model.compile(\n    optimizer='adam',loss='categorical_crossentropy',metrics=tf.keras.metrics.CategoricalAccuracy())\n    return model","fa224d6e":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    model=create_model(784)\n    #model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse', metrics=['mae'])\nmodel.summary()","54e30371":"es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=175)\nmc = ModelCheckpoint('best_model.h5', monitor='val_categorical_accuracy', mode='max', verbose=1, save_best_only=True, patience=175)","3923da59":"history=model.fit(X_train, y_train, epochs=1000, validation_data=(X_validation, y_validation), batch_size=20480, verbose=1,callbacks=[es,mc])","6c5d6365":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","f2acdeda":" model=tf.keras.models.load_model('.\/best_model.h5')","6cb1216f":"predi=model.predict(X_test)","b0ead504":"df=pd.DataFrame(predi)\ndf=df.apply(lambda x: x == x.max(), axis=1).astype(int)\ndf['ImageId']=df.index\ndf.set_index('ImageId',inplace=True)\ndf=df[df==1].stack().reset_index().drop(0,1)\ndf['Label']=df['level_1']\ndf=df.drop('level_1', axis=1)\ndf['ImageId']=df['ImageId']+1","26d20192":"df.head()","f3d550c1":"import gzip\ndf.to_csv('sub.gz', index=False, compression='gzip')","df1140bc":"# Import libraries \ud83d\udcda","8bae6cf7":"# Reading the CSV File \ud83d\udcc1","7f3af145":"# CNN model \ud83d\udc7b"}}