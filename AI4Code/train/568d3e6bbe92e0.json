{"cell_type":{"6236d5cc":"code","2f7a1625":"code","908b7553":"code","0a0fcb4c":"code","9f240aa3":"code","034d6008":"code","041fc365":"code","d69efbbf":"code","6437e4a3":"code","4a89f5b4":"code","837a9176":"code","db219aa9":"code","f36c4dcc":"code","711dbb0a":"code","521f8130":"code","b88f52e7":"code","0e433bf6":"code","4da87f8b":"code","a6c21f93":"code","685c7608":"code","2b47d572":"code","0c6c09f4":"code","f33af175":"code","72052946":"code","c468432a":"code","fc791629":"code","109729e5":"code","76410685":"code","30d7d2ba":"code","ecc66eeb":"code","1070d433":"code","b903ef7e":"code","b9391202":"code","c59173c8":"code","6f7580c1":"code","272cb5a3":"code","adacab83":"code","9c4f6437":"code","4ea2dee7":"code","a1294124":"markdown","29703f1e":"markdown","c3b20b9f":"markdown","fe869ee0":"markdown","7a0e8799":"markdown","7ba5c97a":"markdown","a0d6cb5b":"markdown","2453b2d0":"markdown","89dd8463":"markdown","9be72671":"markdown","cba0ed4f":"markdown","9657650a":"markdown","c4fe7bb5":"markdown","32f8af02":"markdown","08124552":"markdown","7a1c5cfb":"markdown","d58e6192":"markdown","3b4fdeae":"markdown","451f8018":"markdown","99fe8e19":"markdown","a9ed7de9":"markdown"},"source":{"6236d5cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2f7a1625":"# Basic EDA libraries\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('dark_background')\nsns.set_style(\"whitegrid\")\nfrom IPython.display import display\n%matplotlib inline\n\n# Basic ML Libraries\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ShuffleSplit\n\nimport xgboost as xgb\n\n# Deep Learning Libraries\n\n# some other libraries\nimport geopy.distance\nfrom geopy.geocoders import Nominatim","908b7553":"# Since the train data is pretty large, we will import only a random subset of rows to do our analysis.\n\n# The data to load\ntrain_file = \"..\/input\/train.csv\"\n\n# Take every N-th (in this case 10th) row\nn = 10\n\n# Count the lines or use an upper bound\nnum_lines = sum(1 for l in open(train_file))\n\n# The row indices to skip - make sure 0 is not included to keep the header!\nskip_idx = [x for x in range(1, num_lines) if x % n != 0]\n\n# Read the data\n# train = pd.read_csv(train_file, dtype={'fare_amount': 'float32', 'pickup_longitude' : 'float32', 'pickup_longitude' : 'float32', 'dropoff_longitude' : 'float32', 'dropoff_latitude' : 'float32', 'passenger_count' : 'int32'},\n#                    skiprows = skip_idx, parse_dates = ['pickup_datetime']).drop(columns = 'key')\n\ntrain = pd.read_csv(train_file, dtype={'fare_amount': 'float32', 'pickup_longitude' : 'float32', 'pickup_longitude' : 'float32', 'dropoff_longitude' : 'float32', 'dropoff_latitude' : 'float32', 'passenger_count' : 'int32'},\n                    nrows = 2_000_000, parse_dates = ['pickup_datetime']).drop(columns = 'key')\n\n\ntest = pd.read_csv(\"..\/input\/test.csv\", dtype={'fare_amount': 'float32', 'pickup_longitude' : 'float32', 'pickup_longitude' : 'float32', 'dropoff_longitude' : 'float32', 'dropoff_latitude' : 'float32', 'passenger_count' : 'int32'},\n                   parse_dates = ['pickup_datetime'])\n\n# To be used for creating the subission csv\ntest_id = list(test.pop('key'))\n\ndisplay(train.sample(n = 5))\n\ndisplay(test.sample(n = 5))\n\ndisplay(train.info())\n\ndisplay(test.info())","0a0fcb4c":"# Original Shape\nprint(train.shape)\nprint(test.shape)","9f240aa3":"# Lets have a peek at our data's descriptive statistics \ntrain.describe()","034d6008":"# Check for nulls in our dataset.\nprint(\"Training data has nulls? :\", train.isnull().values.any())\nprint(\"Testing data has nulls? :\", test.isnull().values.any())","041fc365":"print(train.isnull().sum())","d69efbbf":"# Since there aren't a lot of rows with nulls, we will drop them all\ntrain = train.dropna(how = 'any', axis = 'rows')","6437e4a3":"# Let's have a look at the fare's distribution\n\nsns.distplot(train.sample(n = 20000)[\"fare_amount\"], hist = True, kde = True)\nfig = plt.gcf()\nfig.set_size_inches(20, 8)\nplt.show()","4a89f5b4":"# Most of the fare amount lies b\/w 0 and 60. Lets remove some extreme outliers\n# Base Fare for NYC Cab : 2.5$ (http:\/\/nymag.com\/nymetro\/urban\/features\/taxi\/n_20286\/)\n\nq75, q25 = np.percentile(train[\"fare_amount\"], [75 ,25])\niqr = q75 - q25\n\nprint(\"Fare Iqr\", iqr)\n\ntrain = train[train[\"fare_amount\"].between(left = 2.5, right = (q75 + 10 * iqr))]","837a9176":"# Cleaning Latitudes & Longitudes\n# Latitudes range from -90 to 90.\n# Longitudes range from -180 to 180.\n# New york lat\/long =>40.730610, -73.935242\n\n# Lets have a look at all Invalid lat\/long ranges\n\ndisplay(train[np.logical_or(train[\"pickup_latitude\"] < -90, train[\"pickup_latitude\"] > 90)])\ndisplay(train[np.logical_or(train[\"dropoff_latitude\"] < -90, train[\"dropoff_latitude\"] > 90)])\n\ndisplay(train[np.logical_or(train[\"pickup_longitude\"] < -180, train[\"pickup_longitude\"] > 180)])\ndisplay(train[np.logical_or(train[\"dropoff_longitude\"] < -180, train[\"dropoff_longitude\"] > 180)])","db219aa9":"#There are a few rows that have invalid Latitudes & Longitudes. Will will discard them. \n#Plus, we will also discard rows that have lat\/long ranges not possible for the NY region (40.730610, -73.935242)\n\ntrain = train[np.logical_and(train[\"pickup_latitude\"] >= 40, train[\"pickup_latitude\"] <= 42)]\ntrain = train[np.logical_and(train[\"dropoff_latitude\"] >= 40, train[\"dropoff_latitude\"] <= 42)]\ntrain = train[np.logical_and(train[\"pickup_longitude\"] >= -75, train[\"pickup_longitude\"] <= -73)]\ntrain = train[np.logical_and(train[\"dropoff_longitude\"] >= -75, train[\"dropoff_longitude\"] <= -73)]","f36c4dcc":"# Lets have a look at the Passenger Count distribution\n\nsns.distplot(train.sample(n = 20000)[\"passenger_count\"], hist = True, kde = True)\nfig = plt.gcf()\nfig.set_size_inches(20, 8)\nplt.show()","711dbb0a":"# Most of the passenger counts are between 0 and 6. We will remove all others.\n\ntrain = train[train[\"passenger_count\"].between(left = 0, right = 6)]","521f8130":"print(train.shape)\nprint(test.shape)","b88f52e7":"# Example\ncoords_1 = (52.2296756, 21.0122287)\ncoords_2 = (52.406374, 16.9251681)\n\nprint(geopy.distance.vincenty(coords_1, coords_2).km)","0e433bf6":"def distanceCalculator(row) :\n    c1 = (row[\"pickup_latitude\"], row[\"pickup_longitude\"])\n    c2 = (row[\"dropoff_latitude\"], row[\"dropoff_longitude\"])\n    \n    return geopy.distance.vincenty(c1, c2).km\n\ntrain[\"distance\"] = train.apply(distanceCalculator, axis = 1)\ntest[\"distance\"] = test.apply(distanceCalculator, axis = 1)","4da87f8b":"train.sample(n = 5)","a6c21f93":"train[\"hour\"] = train[\"pickup_datetime\"].dt.hour\ntest[\"hour\"] = test[\"pickup_datetime\"].dt.hour\n\ntrain[\"dayOfWeek\"] = train[\"pickup_datetime\"].dt.dayofweek\ntest[\"dayOfWeek\"] = test[\"pickup_datetime\"].dt.dayofweek\n\ntrain['day'] = train['pickup_datetime'].dt.day\ntest['day'] = test['pickup_datetime'].dt.day\n\ntrain['month'] = train['pickup_datetime'].dt.month\ntest['month'] = test['pickup_datetime'].dt.month\n\ntrain[\"year\"] = train[\"pickup_datetime\"].dt.year\ntest[\"year\"] = test[\"pickup_datetime\"].dt.year","685c7608":"train.sample(n = 5)","2b47d572":"def dist(pickup_lat, pickup_long, dropoff_lat, dropoff_long):  \n    distance = np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n    \n    return distance\n\ndef transform(data):\n    # Distances to nearby airports, and city center\n    # By reporting distances to these points, the model can somewhat triangulate other locations of interest\n    nyc = (-74.0063889, 40.7141667)\n    jfk = (-73.7822222222, 40.6441666667)\n    ewr = (-74.175, 40.69)\n    lgr = (-73.87, 40.77)\n   \n    data['distance_to_center'] = dist(nyc[1], nyc[0],\n                                      data['pickup_latitude'], data['pickup_longitude'])\n    data['pickup_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                         data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    data['pickup_distance_to_ewr'] = dist(ewr[1], ewr[0], \n                                          data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_ewr'] = dist(ewr[1], ewr[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    data['pickup_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                          data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    \n    data['long_dist'] = data['pickup_longitude'] - data['dropoff_longitude']\n    data['lat_dist'] = data['pickup_latitude'] - data['dropoff_latitude']\n    \n    return data\n\n\ntrain = transform(train)\ntest = transform(test)","0c6c09f4":"train.sample(n = 5)","f33af175":"f, ax = plt.subplots(1, 2, figsize = (20, 8))\n\nsns.countplot(x = \"hour\", data = train.sample(n = 20000), ax = ax[0])\nplt.xlabel(\"Hour of the day\")\nplt.ylabel(\"Cab frequency\")\n\nsns.barplot(x = \"hour\", y = \"fare_amount\", data = train.sample(n = 20000), ax = ax[1])\nplt.xlabel(\"Hour of the day\")\nplt.ylabel(\"Average Fare amount\")\nplt.show()","72052946":"f, ax = plt.subplots(1, 2, figsize = (20, 8))\n\nsns.countplot(x = \"dayOfWeek\", data = train.sample(n = 20000), ax = ax[0])\nplt.xlabel(\"Day of the week\")\nplt.ylabel(\"Cab frequency\")\n\nsns.barplot(x = \"dayOfWeek\", y = \"fare_amount\", data = train.sample(n = 20000), ax = ax[1])\nplt.xlabel(\"Day of the week\")\nplt.ylabel(\"Average Fare amount\")\nplt.show()","c468432a":"sns.factorplot(x = \"hour\", y = \"fare_amount\", hue = \"dayOfWeek\", data = train.sample(n = 20000))\nfig = plt.gcf()\nfig.set_size_inches(20, 12)\nplt.show()","fc791629":"train[\"year\"].value_counts()","109729e5":"# The fare data contains data points for several years, The fares shoule depend on it(Inflation!)\n\nsns.barplot(x = \"year\", y = \"fare_amount\", data = train.sample(n = 20000))\nfig = plt.gcf()\nfig.set_size_inches(12, 8)\nplt.show()","76410685":"# Passenger Count\nsns.barplot(x = \"passenger_count\", y = \"fare_amount\", data = train.sample(n = 20000))\nfig = plt.gcf()\nfig.set_size_inches(12, 8)\nplt.show()","30d7d2ba":"display(train.sample(n = 5))\ndisplay(test.sample(n = 5))","ecc66eeb":"target = train[\"fare_amount\"].values\ntrain = train.drop(columns = [\"fare_amount\", \"pickup_datetime\"], axis = 1)\ntest = test.drop(columns = [\"pickup_datetime\"], axis = 1)\ndisplay(train.sample(n = 2))\ndisplay(test.sample(n = 2))","1070d433":"X_train, X_test, y_train, y_test = train_test_split(train, target, test_size = 0.15)","b903ef7e":"# Even though I have created the pipeline to optimise hyperparameters and save a list of model, I will not be using it\n# due to the sheer data size. \n\nmodelResults = pd.DataFrame(columns = ['Model_Name', 'Model', 'Params', 'Test_Score', 'CV_Mean', 'CV_STD'])\n\ndef save(grid, modelName, calFI):\n    global modelResults\n    cv_scores = cross_val_score(grid.best_estimator_, X_train, y_train, cv = 10, scoring = 'neg_mean_squared_error')\n    cv_mean = cv_scores.mean()\n    cv_std = cv_scores.std()\n    test_score = grid.score(X_test, y_test)\n    \n    print(\"Best model parameter are\\n\", grid.best_estimator_)\n    print(\"Saving model {}\\n\".format(modelName))\n    print(\"Mean Cross validation score is {} with a Standard deviation of {}\\n\".format(cv_mean, cv_std))\n    print(\"Test Score for the model is {}\\n\".format(test_score))\n    \n    if calFI:\n        pd.Series(grid.best_estimator_.feature_importances_, train.columns).sort_values(ascending = True).plot.barh(width = 0.6)\n        fig = plt.gcf()\n        fig.set_size_inches(12, 12)\n        plt.title(\"{} Feature Importance\".format(modelName))\n        plt.show()\n    \n    \n    modelResults = modelResults.append({'Model_Name' : modelName, 'Model' : grid.best_estimator_, 'Params' : grid.best_params_, 'Test_Score' : test_score, 'CV_Mean' : cv_mean, 'CV_STD' : cv_std}\n                                       , ignore_index=True)\n    \n    \ndef doGridSearch(classifier, params):\n    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n    score_fn = make_scorer(mean_squared_error)\n    grid = GridSearchCV(classifier, params, scoring = score_fn, cv = cv)\n    grid = grid.fit(X_train, y_train)\n    \n    return grid    ","b9391202":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nrandom_forest = RandomForestRegressor(max_features = None, oob_score = True, \n                                      bootstrap = True, verbose = 1, n_jobs = -1)\n\nrandom_forest.fit(X_train, y_train)\ny_test_preds = random_forest.predict(X_test)\n\nprint(\"RMSE score :\", np.sqrt(mean_squared_error(y_test, y_test_preds)))","c59173c8":"random_forest_preditions = random_forest.predict(test)","6f7580c1":"sub_rf = pd.DataFrame({'key': test_id, 'fare_amount': random_forest_preditions})\nsub_rf.to_csv('rf_nyc.csv', index = False)","272cb5a3":"#Cross-validation\n\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth': 8, #Result of tuning with CV\n    'eta':.03, #Result of tuning with CV\n    'subsample': 0.8, #Result of tuning with CV\n    'colsample_bytree': 0.8, #Result of tuning with CV\n    # Other parameters\n    'objective':'reg:linear',\n    'eval_metric':'rmse',\n    'silent': 1\n}\n\n#Block of code used for hypertuning parameters. Adapt to each round of parameter tuning.\n#Turn off CV in submission\n\nCV = False\nif CV:\n    dtrain = xgb.DMatrix(train,label=y)\n    gridsearch_params = [\n        (eta)\n        for eta in np.arange(.04, 0.12, .02)\n    ]\n\n    # Define initial best params and RMSE\n    min_rmse = float(\"Inf\")\n    best_params = None\n    for (eta) in gridsearch_params:\n        print(\"CV with eta={} \".format(\n                                 eta))\n\n        # Update our parameters\n        params['eta'] = eta\n\n        # Run CV\n        cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=1000,\n            nfold=3,\n            metrics={'rmse'},\n            early_stopping_rounds=10\n        )\n\n        # Update best RMSE\n        mean_rmse = cv_results['test-rmse-mean'].min()\n        boost_rounds = cv_results['test-rmse-mean'].argmin()\n        print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n        if mean_rmse < min_rmse:\n            min_rmse = mean_rmse\n            best_params = (eta)\n\n    print(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))\nelse:\n    #Print final params to use for the model\n    params['silent'] = 0 #Turn on output\n    print(params)","adacab83":"def XGBmodel(x_train,x_test,y_train,y_test,params):\n    matrix_train = xgb.DMatrix(x_train, label = y_train)\n    matrix_test = xgb.DMatrix(x_test, label = y_test)\n    model = xgb.train(params = params,\n                    dtrain = matrix_train,num_boost_round = 5000, \n                    early_stopping_rounds = 10,evals = [(matrix_test,'test')])\n    return model\n\nmodel = XGBmodel(X_train, X_test, y_train, y_test, params)","9c4f6437":"xgbpredictions = model.predict(xgb.DMatrix(test), ntree_limit = model.best_ntree_limit)","4ea2dee7":"sub_xgb = pd.DataFrame({'key': test_id, 'fare_amount': xgbpredictions})\nsub_xgb.to_csv('xgboost_nyc.csv', index = False)","a1294124":"Even though its not a result of direct causation, fares are a little higher when the usage is low ie. Odd hours (eg night 3- 6 in the morning) or 2 - 4 in the evening","29703f1e":"##### Cleaning Passenger Count","c3b20b9f":"##### Lets use XGBoost.","fe869ee0":"##### Lets create new TIME features using the pick up datetime","7a0e8799":"##### Lets use a simple Out of the box Random Forest Regressor","7ba5c97a":"Looks like there are more rides on Friday and Saturday but the fares are not affected much by day of the week","a0d6cb5b":"*  Does Hour of the day have any effect on the fare?","2453b2d0":"And it obvious that year should affect the fare price as well.","89dd8463":"### Dataset Import","9be72671":"##### Cleaning Fares","cba0ed4f":"##### Creating extra features based on Lat\/ Long ranges","9657650a":"Not much information","c4fe7bb5":"### Exploratory data analysis","32f8af02":"*  Does Day of the week have any effect on the fare?","08124552":"### Model Application\n","7a1c5cfb":"### Cleaning Data","d58e6192":"In this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used (see the starter code for an example of this approach in Kernels). Your challenge is to do better than this using Machine Learning techniques!","3b4fdeae":"Looking at the above information, there are a lot of bad data values :\n* The min fare amount is -ve, which can not happen. The maximum fare amount is also ridiculous. \n*   Latitude\/ Longitude ranges are also incorrect. (Since we are dealing with the NY region, we will limit ourselves to Lat\/Long ranges in this region and drop other data points that are potentially noise\/ bad data values)\n* The min passenger count is 0 and the max is 208. We will have to handle these values as well\n\n","451f8018":"##### Cleaning Latitude\/ Longitudes","99fe8e19":"### Feature Engg.","a9ed7de9":"##### Lets create a trip distance feature"}}