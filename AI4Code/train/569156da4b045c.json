{"cell_type":{"8689c726":"code","8308baec":"code","77b8efc6":"code","18287cf1":"code","9afff2b7":"code","622f9ad5":"code","a2f3ee5e":"code","ad0e5d26":"code","554f9760":"code","11562464":"code","088495dc":"code","ea5001bd":"code","ece34a2e":"code","dbd6c598":"code","635b5d79":"code","7e6c5850":"code","54ad1705":"code","e2606be0":"code","01e96d35":"code","afb92584":"code","07a1f351":"code","c9f915d8":"code","2982fbf7":"code","64ec7dde":"code","c776e367":"code","21e8d34b":"code","df5b8f7e":"code","03bc2312":"code","d12d5f33":"code","6817a393":"code","b6459d1c":"code","bf7c1dde":"code","323486e7":"code","9322e577":"code","49ed75ba":"code","cd0c1bc0":"code","8f1afdeb":"code","5f6f77e2":"code","2697bc91":"code","a1640b23":"markdown","7d2f9b5c":"markdown","57298564":"markdown","f8959b3f":"markdown","d42b58b7":"markdown","f0813a1b":"markdown","b4d1550e":"markdown","a20f635b":"markdown","0e8b4635":"markdown","8adbf358":"markdown","f31354ea":"markdown","98db6208":"markdown","3a57c990":"markdown","7de745f7":"markdown","8cb81b82":"markdown","1a9b2300":"markdown","d3f38cb3":"markdown","7d52f67b":"markdown","aec0019d":"markdown","c39d5d62":"markdown","9788374e":"markdown","d889c9ba":"markdown","605dc6a8":"markdown","e88862e5":"markdown","eb1108da":"markdown","1ba46c8a":"markdown","2e0027ae":"markdown","77830180":"markdown","44404746":"markdown","451d0bcf":"markdown","14e1e335":"markdown","7599c302":"markdown","c7e0a1cf":"markdown","ee6dc2f0":"markdown","c3e6cee9":"markdown","f0ab3b74":"markdown","33d11ae8":"markdown","185fa629":"markdown","29d0cd51":"markdown"},"source":{"8689c726":"import numpy as np\nimport pandas as pd\n# we don't like warnings\n# you can comment the following 2 lines if you'd like to\nimport warnings\nwarnings.filterwarnings('ignore')","8308baec":"df = pd.read_csv('..\/input\/telecom_churn.csv')\ndf.head()","77b8efc6":"print(df.shape)","18287cf1":"print(df.columns)","9afff2b7":"print(df.info())","622f9ad5":"df['Churn'] = df['Churn'].astype('int64')","a2f3ee5e":"df.describe()","ad0e5d26":"df.describe(include=['object', 'bool'])","554f9760":"df['Churn'].value_counts()","11562464":"df['Churn'].value_counts(normalize=True)","088495dc":"df.sort_values(by='Total day charge', ascending=False).head()","ea5001bd":"df.sort_values(by=['Churn', 'Total day charge'],\n        ascending=[True, False]).head()","ece34a2e":"df['Churn'].mean()","dbd6c598":"df[df['Churn'] == 1].mean()","635b5d79":"df[df['Churn'] == 1]['Total day minutes'].mean()","7e6c5850":"df[(df['Churn'] == 0) & (df['International plan'] == 'No')]['Total intl minutes'].max()","54ad1705":"df.loc[0:5, 'State':'Area code']","e2606be0":"df.iloc[0:5, 0:3]","01e96d35":"df[-1:]","afb92584":"df.apply(np.max) ","07a1f351":"df[df['State'].apply(lambda state: state[0] == 'W')].head()","c9f915d8":"d = {'No' : False, 'Yes' : True}\ndf['International plan'] = df['International plan'].map(d)\ndf.head()","2982fbf7":"df = df.replace({'Voice mail plan': d})\ndf.head()","64ec7dde":"columns_to_show = ['Total day minutes', 'Total eve minutes', \n                   'Total night minutes']\n\ndf.groupby(['Churn'])[columns_to_show].describe(percentiles=[])","c776e367":"columns_to_show = ['Total day minutes', 'Total eve minutes', \n                   'Total night minutes']\n\ndf.groupby(['Churn'])[columns_to_show].agg([np.mean, np.std, np.min, \n                                            np.max])","21e8d34b":"pd.crosstab(df['Churn'], df['International plan'])","df5b8f7e":"pd.crosstab(df['Churn'], df['Voice mail plan'], normalize=True)","03bc2312":"df.pivot_table(['Total day calls', 'Total eve calls', 'Total night calls'],\n               ['Area code'], aggfunc='mean')","d12d5f33":"total_calls = df['Total day calls'] + df['Total eve calls'] + \\\n              df['Total night calls'] + df['Total intl calls']\ndf.insert(loc=len(df.columns), column='Total calls', value=total_calls) \n# loc parameter is the number of columns after which to insert the Series object\n# we set it to len(df.columns) to paste it at the very end of the dataframe\ndf.head()","6817a393":"df['Total charge'] = df['Total day charge'] + df['Total eve charge'] + \\\n                     df['Total night charge'] + df['Total intl charge']\ndf.head()","b6459d1c":"# get rid of just created columns\ndf.drop(['Total charge', 'Total calls'], axis=1, inplace=True) \n# and here\u2019s how you can delete rows\ndf.drop([1, 2]).head() ","bf7c1dde":"pd.crosstab(df['Churn'], df['International plan'], margins=True)","323486e7":"# some imports to set up plotting \nimport matplotlib.pyplot as plt\n# pip install seaborn \nimport seaborn as sns","9322e577":"sns.countplot(x='International plan', hue='Churn', data=df);","49ed75ba":"pd.crosstab(df['Churn'], df['Customer service calls'], margins=True)","cd0c1bc0":"sns.countplot(x='Customer service calls', hue='Churn', data=df);","8f1afdeb":"df['Many_service_calls'] = (df['Customer service calls'] > 3).astype('int')\n\npd.crosstab(df['Many_service_calls'], df['Churn'], margins=True)","5f6f77e2":"sns.countplot(x='Many_service_calls', hue='Churn', data=df);","2697bc91":"pd.crosstab(df['Many_service_calls'] & df['International plan'] , df['Churn'])","a1640b23":"\n### DataFrame transformations\n\nLike many other things in Pandas, adding columns to a DataFrame is doable in many ways.\n\nFor example, if we want to calculate the total number of calls for all users, let\u2019s create the `total_calls` Series and paste it into the DataFrame:\n\n","7d2f9b5c":"\n### Indexing and retrieving data\n\nA DataFrame can be indexed in a few different ways. \n\nTo get a single column, you can use a `DataFrame['Name']` construction. Let's use this to answer a question about that column alone: **what is the proportion of churned users in our dataframe?**\n\n","57298564":"\nDataFrames can be indexed by column name (label) or row name (index) or by the serial number of a row. The `loc` method is used for **indexing by name**, while `iloc()` is used for **indexing by number**.\n\nIn the first case below, we say *\"give us the values of the rows with index from 0 to 5 (inclusive) and columns labeled from State to Area code (inclusive)\"*. In the second case, we say *\"give us the values of the first five rows in the first three columns\"* (as in a typical Python slice: the maximal value is not included).\n","f8959b3f":"It is possible to add a column more easily without creating an intermediate Series instance:","d42b58b7":"\n14.5% is actually quite bad for a company; such a churn rate can make the company go bankrupt.\n\n**Boolean indexing** with one column is also very convenient. The syntax is `df[P(df['Name'])]`, where `P` is some logical condition that is checked for each element of the `Name` column. The result of such indexing is the DataFrame consisting only of rows that satisfy the `P` condition on the `Name` column. \n\nLet\u2019s use it to answer the question:\n\n**What are average values of numerical features for churned users?**\n","f0813a1b":"\nLet\u2019s construct another contingency table that relates *Churn* with both *International plan* and freshly created *Many_service_calls*.\n\n","b4d1550e":"\n**What is the maximum length of international calls among loyal users (`Churn == 0`) who do not have an international plan?**\n\n","a20f635b":"Let\u2019s have a look at data dimensionality, feature names, and feature types.","0e8b4635":"\n1. First, the `groupby` method divides the `grouping_columns` by their values. They become a new index in the resulting dataframe.\n2. Then, columns of interest are selected (`columns_to_show`). If `columns_to_show` is not included, all non groupby clauses will be included.\n3. Finally, one or several functions are applied to the obtained groups per selected columns.\n\nHere is an example where we group the data according to the values of the `Churn` variable and display statistics of three columns in each group:","8adbf358":"In order to see statistics on non-numerical features, one has to explicitly indicate data types of interest in the `include` parameter.","f31354ea":"`bool`, `int64`, `float64` and `object` are the data types of our features. We see that one feature is logical (`bool`), 3 features are of type `object`, and 16 features are numeric. With this same method, we can easily see if there are any missing values. Here, there are none because each column contains 3333 observations, the same number of rows we saw before with `shape`.\n\nWe can **change the column type** with the `astype` method. Let\u2019s apply this method to the `Churn` feature to convert it into `int64`:\n","98db6208":"We can use the `info()` method to output some general information about the dataframe: ","3a57c990":"## 1. Demonstration of main Pandas methods\nWell... There are dozens of cool tutorials on Pandas and visual data analysis. If you are already familiar with these topics, you can wait for the 3rd article in the series, where we get into machine learning.  \n\n**[Pandas](http:\/\/pandas.pydata.org)** is a Python library that provides extensive means for data analysis. Data scientists often work with data stored in table formats like `.csv`, `.tsv`, or `.xlsx`. Pandas makes it very convenient to load, process, and analyze such tabular data using SQL-like queries. In conjunction with `Matplotlib` and `Seaborn`, `Pandas` provides a wide range of opportunities for visual analysis of tabular data.\n\nThe main data structures in `Pandas` are implemented with **Series** and **DataFrame** classes. The former is a one-dimensional indexed array of some fixed data type. The latter is a two-dimensional data structure - a table - where each column contains data of the same type. You can see it as a dictionary of `Series` instances. `DataFrames` are great for representing real data: rows correspond to instances (examples, observations, etc.), and columns correspond to features of these instances.","7de745f7":"\nThe `describe` method shows basic statistical characteristics of each numerical feature (`int64` and `float64` types): number of non-missing values, mean, standard deviation, range, median, 0.25 and 0.75 quartiles.","8cb81b82":"Although it's not so obvious from the summary table, it's easy to see from the above plot that the churn rate increases sharply from 4 customer service calls and above.\n\nNow let's add a binary feature to our DataFrame \u2013 `Customer service calls > 3`. And once again, let's see how it relates to churn. ","1a9b2300":"# <center> Topic 1. Exploratory data analysis with Pandas\n\n<img align=\"center\" src=\"https:\/\/habrastorage.org\/files\/10c\/15f\/f3d\/10c15ff3dcb14abdbabdac53fed6d825.jpg\"  width=50% \/>\n\n### Article outline\n1. [Demonstration of main Pandas methods](#1.-Demonstration-of-main-Pandas-methods)\n2. [First attempt at predicting telecom churn](#2.-First-attempt-at-predicting-telecom-churn)\n3. [Demo assignment](#3.-Demo-assignment)\n4. [Useful resources](#4-Useful-resources)","d3f38cb3":"If we need the first or the last line of the data frame, we can use the `df[:1]` or `df[-1:]` construct:","7d52f67b":"\nWe\u2019ll demonstrate the main methods in action by analyzing a [dataset](https:\/\/bigml.com\/user\/francisco\/gallery\/dataset\/5163ad540c0b5e5b22000383) on the churn rate of telecom operator clients. Let\u2019s read the data (using `read_csv`), and take a look at the first 5 lines using the `head` method:\n","aec0019d":"<details>\n<summary>About printing DataFrames in Jupyter notebooks<\/summary>\n<p>\nIn Jupyter notebooks, Pandas DataFrames are printed as these pretty tables seen above while `print(df.head())` is less nicely formatted.\nBy default, Pandas displays 20 columns and 60 rows, so, if your DataFrame is bigger, use the `set_option` function as shown in the example below:\n\n```python\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n```\n<\/p>\n<\/details>\n\nRecall that each row corresponds to one client, an **instance**, and columns are **features** of this instance.","c39d5d62":"\n### Applying Functions to Cells, Columns and Rows\n\n**To apply functions to each column, use `apply()`:**\n","9788374e":"Therefore, predicting that a customer is not loyal (*Churn*=1) in the case when the number of calls to the service center is greater than 3 and the *International Plan* is added (and predicting *Churn*=0 otherwise), we might expect an accuracy of 85.8% (we are mistaken only 464 + 9 times). This number, 85.8%, that we got through this very simple reasoning serves as a good starting point (*baseline*) for the further machine learning models that we will build. \n\nAs we move on in this course, recall that, before the advent of machine learning, the data analysis process looked something like this. Let's recap what we've covered:\n    \n- The share of loyal clients in the sample is 85.5%. The most naive model that always predicts a \"loyal customer\" on such data will guess right in about 85.5% of all cases. That is, the proportion of correct answers (*accuracy*) of subsequent models should be no less than this number, and will hopefully be significantly higher;\n- With the help of a simple forecast that can be expressed by the following formula: \"International plan = True & Customer Service calls > 3 => Churn = 1, else Churn = 0\", we can expect a guessing rate of 85.8%, which is just above 85.5%. Subsequently, we'll talk about decision trees and figure out how to find such rules **automatically** based only on the input data;\n- We got these two baselines without applying machine learning, and they\u2019ll serve as the starting point for our subsequent models. If it turns out that with enormous effort, we increase the share of correct answers by 0.5% per se, then possibly we are doing something wrong, and it suffices to confine ourselves to a simple model with two conditions;\n- Before training complex models, it is recommended to manipulate the data a bit, make some plots, and check simple assumptions. Moreover, in business applications of machine learning, they usually start with simple solutions and then experiment with more complex ones.\n\n## 3. Demo assignment\nTo practice with Pandas and EDA, you can complete [this assignment](https:\/\/www.kaggle.com\/kashnitsky\/a1-demo-pandas-and-uci-adult-dataset) where you'll be analyzing socio-demographic data. \n\n## 4. Useful resources\n\n* The same notebook as an interactive web-based [Kaggle Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-1-exploratory-data-analysis-with-pandas)\n* [\"Merging DataFrames with pandas\"](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse.ai\/blob\/master\/jupyter_english\/tutorials\/merging_dataframes_tutorial_max_palko.ipynb) - a tutorial by Max Plako within mlcourse.ai (full list of tutorials is [here](https:\/\/mlcourse.ai\/tutorials))\n* [\"Handle different dataset with dask and trying a little dask ML\"](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse.ai\/blob\/master\/jupyter_english\/tutorials\/dask_objects_and_little_dask_ml_tutorial_iknyazeva.ipynb) - a tutorial by Irina Knyazeva within mlcourse.ai\n* Main course [site](https:\/\/mlcourse.ai), [course repo](https:\/\/github.com\/Yorko\/mlcourse.ai), and YouTube [channel](https:\/\/www.youtube.com\/watch?v=QKTuw4PNOsU&list=PLVlY_7IJCMJeRfZ68eVfEcu-UcN9BbwiX)\n* Official Pandas [documentation](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/index.html)\n* Course materials as a [Kaggle Dataset](https:\/\/www.kaggle.com\/kashnitsky\/mlcourse)\n* Medium [\"story\"](https:\/\/medium.com\/open-machine-learning-course\/open-machine-learning-course-topic-1-exploratory-data-analysis-with-pandas-de57880f1a68) based on this notebook\n* If you read Russian: an [article](https:\/\/habrahabr.ru\/company\/ods\/blog\/322626\/) on Habr.com with ~ the same material. And a [lecture](https:\/\/youtu.be\/dEFxoyJhm3Y) on YouTube\n* [10 minutes to pandas](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/10min.html)\n* [Pandas cheatsheet PDF](https:\/\/github.com\/pandas-dev\/pandas\/blob\/master\/doc\/cheatsheet\/Pandas_Cheat_Sheet.pdf)\n* GitHub repos: [Pandas exercises](https:\/\/github.com\/guipsamora\/pandas_exercises\/) and [\"Effective Pandas\"](https:\/\/github.com\/TomAugspurger\/effective-pandas)\n* [scipy-lectures.org](http:\/\/www.scipy-lectures.org\/index.html) \u2014 tutorials on pandas, numpy, matplotlib and scikit-learn\n","d889c9ba":"The `map` method can be used to **replace values in a column** by passing a dictionary of the form `{old_value: new_value}` as its argument:","605dc6a8":"\nWe see that, with *International Plan*, the churn rate is much higher, which is an interesting observation! Perhaps large and poorly controlled expenses with international calls are very conflict-prone and lead to dissatisfaction among the telecom operator's customers.\n\nNext, let\u2019s look at another important feature \u2013 *Customer service calls*. Let\u2019s also make a summary table and a picture.","e88862e5":"To delete columns or rows, use the `drop` method, passing the required indexes and the `axis` parameter (`1` if you delete columns, and nothing or `0` if you delete rows). The `inplace` argument tells whether to change the original DataFrame. With `inplace=False`, the `drop` method doesn't change the existing DataFrame and returns a new one with dropped rows or columns. With `inplace=True`, it alters the DataFrame.","eb1108da":"\n### Summary tables\n\nSuppose we want to see how the observations in our sample are distributed in the context of two variables - `Churn` and `International plan`. To do so, we can build a **contingency table** using the `crosstab` method:\n\n","1ba46c8a":"We can also sort by multiple columns:","2e0027ae":"Let\u2019s do the same thing, but slightly differently by passing a list of functions to `agg()`:","77830180":"## 2. First attempt at predicting telecom churn\n\n\nLet's see how churn rate is related to the *International plan* feature. We\u2019ll do this using a `crosstab` contingency table and also through visual analysis with `Seaborn` (however, visual analysis will be covered more thoroughly in the next article).\n","44404746":"\n### Sorting\n\nA DataFrame can be sorted by the value of one of the variables (i.e columns). For example, we can sort by *Total day charge* (use `ascending=False` to sort in descending order):\n","451d0bcf":"From the output, we can see that the table contains 3333 rows and 20 columns.\n\nNow let\u2019s try printing out column names using `columns`:","14e1e335":"\n\n### Grouping\n\nIn general, grouping data in Pandas works as follows:\n","7599c302":"For categorical (type `object`) and boolean (type `bool`) features we can use the `value_counts` method. Let\u2019s have a look at the distribution of `Churn`:","c7e0a1cf":"The same thing can be done with the `replace` method:","ee6dc2f0":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \n\nAuthor: [Yury Kashnitskiy](https:\/\/yorko.github.io). Translated and edited by [Christina Butsko](https:\/\/www.linkedin.com\/in\/christinabutsko\/), [Yuanyuan Pao](https:\/\/www.linkedin.com\/in\/yuanyuanpao\/), [Anastasia Manokhina](https:\/\/www.linkedin.com\/in\/anastasiamanokhina), Sergey Isaev and [Artem Trunov](https:\/\/www.linkedin.com\/in\/datamove\/). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose.","c3e6cee9":"The `apply` method can also be used to apply a function to each row. To do this, specify `axis=1`. Lambda functions are very convenient in such scenarios. For example, if we need to select all states starting with W, we can do it like this:","f0ab3b74":"\n```python\ndf.groupby(by=grouping_columns)[columns_to_show].function()\n```","33d11ae8":"2850 users out of 3333 are *loyal*; their `Churn` value is `0`. To calculate fractions, pass `normalize=True` to the `value_counts` function.","185fa629":"We can see that most of the users are loyal and do not use additional services (International Plan\/Voice mail).\n\nThis will resemble **pivot tables** to those familiar with Excel. And, of course, pivot tables are implemented in Pandas: the `pivot_table` method takes the following parameters:\n\n* `values` \u2013 a list of variables to calculate statistics for,\n* `index` \u2013 a list of variables to group data by,\n* `aggfunc` \u2013 what statistics we need to calculate for groups, ex. sum, mean, maximum, minimum or something else.\n\nLet\u2019s take a look at the average number of day, evening, and night calls by area code:","29d0cd51":"**How much time (on average) do churned users spend on the phone during daytime?**"}}