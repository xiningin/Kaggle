{"cell_type":{"c2c98116":"code","f40c81a8":"code","d3259786":"code","a18ba23b":"code","b742dfcf":"code","deb1135e":"code","1b8255f2":"code","2131879f":"code","a5ca47cd":"markdown","f106411b":"markdown","feb74361":"markdown","4e254622":"markdown","93c2a89b":"markdown","923113d5":"markdown","1e40fcc1":"markdown"},"source":{"c2c98116":"import numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","f40c81a8":"!pip install autoviml","d3259786":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nnRowsRead = None # specify 'None' if want to read whole file\n# reduce_train.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndf2 = pd.read_csv('\/kaggle\/input\/reduce_train.csv', delimiter=',', nrows = nRowsRead)\ndf2.dataframeName = 'reduce_train.csv'\nnRow, nCol = df2.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf2.head()","a18ba23b":"nRowsRead = None # specify 'None' if want to read whole file\n# reduce_test.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndf1 = pd.read_csv('\/kaggle\/input\/reduce_test.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'reduce_test.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf1.head(5)","b742dfcf":"target = 'accuracy_group'\nsample_submission = ''\nscoring_parameter = 'balanced-accuracy'","deb1135e":"from autoviml.Auto_ViML import Auto_ViML","1b8255f2":"m, feats, trainm, testm = Auto_ViML(df2, target, df1, sample_submission,\n                                    scoring_parameter=scoring_parameter,\n                                    hyper_param='GS',feature_reduction=True,\n                                     Boosting_Flag='CatBoost',Binning_Flag=False,\n                                    Add_Poly=0, Stacking_Flag=False,Imbalanced_Flag=True, \n                                    verbose=2)  ","2131879f":"### Let's see how many features were selected by Auto_ViML to build this model \nprint(len(feats))\nprint('Selected Features:\\n%s' %feats)","a5ca47cd":"# Before you install Autoviml library, go to the right of this tab and see Kaggle Settings for \"Environment\". Click and change that to \"Latest Environment\". That will make your pip install smoother.\nLet's \"pip install autoviml\" library","f106411b":"## We need to setup some defaults for AutoViML","feb74361":"### Let's import the next file: \/kaggle\/input\/reduce_test.csv[](http:\/\/)","4e254622":"#### We are going to import Train and Test datasets and feed them to Autoviml :","93c2a89b":"## Auto_ViML took only 8 Mins to build a model after doing feature selection! Performance is not great unless we do some more feature engineering or try different models.","923113d5":"## Conclusion\nHope this Kaggle Kernel demonstrates the power of Auto_ViML. It took only 8 mins to clean the data, fill missing values, select features, add more features and then build a model and tune it. The data set was not very big only 17K rows and 900 columns. However, this shows that only   Happy Kaggling!","1e40fcc1":"## Introduction to autoviml Kernel: Quick Start!\nThis is a Kagggle Kernel meant to demonstrate the use of a new Python3 library called \"autoviml\". \n## Autoviml stands for \"Automatic Variant Interpretable ML\".\n\n1. First open this Kernel will do some feature engg to get you started (Running time: 10 mins):\nhttps:\/\/www.kaggle.com\/morenoh149\/autoviml-quickstart\n\n1. Then Use Auto_ViML to select best features and best model: (Running time: 8 mins)\nhttps:\/\/www.kaggle.com\/rsesha\/starter-autoviml-kernel-demo\/edit\n\n1. After that take the best features (about 79) and best model (CatBoost) and fit\/predict:\nhttps:\/\/www.kaggle.com\/rsesha\/2019-data-science-bowl-via-autoviml\/edit\n\nSo totally it should take you about 18 mins to get a LB Score in this completition!"}}