{"cell_type":{"14bb9582":"code","879b1b67":"code","9c546000":"code","bf310432":"code","022a5641":"code","d915bc2b":"code","41a75369":"code","94d35073":"code","88abcb0a":"code","c27c275d":"code","e054b5dc":"code","64806748":"code","af93dc19":"code","b47b8cf7":"code","e5d5b8b6":"code","b6dd85cb":"code","874aab9a":"code","7c8e64cd":"code","439045f9":"code","55592b39":"code","28b9f59e":"code","f282cf5a":"code","ad52ef3b":"code","ff52c407":"code","6b1c32bc":"code","2ca5d328":"code","130af3c2":"code","56ac4be0":"code","81257c81":"code","3e312f19":"code","73f3d9ed":"code","c6996c71":"code","396d1cab":"code","a0f32129":"code","7e574f1e":"code","9b09009a":"code","6ceaf250":"code","6d2dd92f":"code","d78ebcd5":"code","18527d4e":"code","fe32148c":"code","f044ce62":"code","ec59d6f5":"code","e90e53ef":"code","0a82559a":"code","54e0ecdc":"code","73a0d84a":"code","bf33c0ee":"code","29097746":"code","c4ed6554":"code","10462e87":"code","0a136d2f":"code","518c544a":"code","ab5776c0":"code","2044bf05":"code","7902fb1c":"code","782c3e00":"code","c0e12d67":"code","f5fcd3dd":"code","1413ae07":"code","84efc936":"code","e7c67a71":"code","88defa45":"code","30151b16":"code","803b3b59":"code","7b7f25c0":"code","2d406c9d":"code","2cf118f7":"code","0b9963ab":"code","20e79c84":"code","70576921":"code","5b9aa0c3":"code","799e5b30":"code","599026b6":"code","ff37e76e":"code","86d220a3":"code","4d84f734":"code","6614d8d7":"code","88166b0b":"markdown","941220bf":"markdown","97a444db":"markdown","4c962d26":"markdown","b6ba9c2d":"markdown","d17669e5":"markdown","16bbdee2":"markdown","b8952954":"markdown","dc764f16":"markdown","c01f0d33":"markdown","6e7fe932":"markdown","c11adb35":"markdown","f8e71683":"markdown","81a524c6":"markdown","de7a5185":"markdown","2d50783d":"markdown","28f00f42":"markdown","5e9fcd0d":"markdown","ffbf0df5":"markdown","9b801509":"markdown","196a84be":"markdown","85516238":"markdown","e0f68f63":"markdown","9e3d7e78":"markdown","4bc51877":"markdown","54e91acc":"markdown","a9739904":"markdown","25233819":"markdown","9cc912db":"markdown","08e7bb10":"markdown","a128e590":"markdown","b0501af7":"markdown","0dcc39ed":"markdown","b7f9f9ca":"markdown","e6863e94":"markdown","b91045b3":"markdown","44d49407":"markdown","ccbdab2d":"markdown","8bf15a9a":"markdown","72597290":"markdown","ed3c39b1":"markdown","9eb63cf4":"markdown","4bb48733":"markdown","91b1b617":"markdown","cad02109":"markdown","7cd7282d":"markdown","c5e1e301":"markdown"},"source":{"14bb9582":"#Basics\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Plotting libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nfrom PIL import Image as im\nfrom IPython.display import Image\nfrom scipy import stats\n\n#Wordcloud packages\nfrom os import path\nfrom PIL import Image\nfrom IPython.display import SVG, display\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport re \nimport collections as c\n\n#Modelling\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\n","879b1b67":"#Importing data\nny_df = pd.read_csv('\/Users\/emilykasa\/Desktop\/repos\/Capstone_Project\/capstone_data\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\nny_df.head()\n#ny_df.info()","9c546000":"#Take a look at naan value percentages\nmissing_data_summary = (ny_df.isnull().sum()\/len(ny_df))*100\nmissing_data_summary\n#looks like if there are no reviews then last review and reviews per month are both NaN","bf310432":"#dropping rows with nan would mean losing 20% of our data, but dropping the columns would be a loss of a possibly important predictor\nny_df[ny_df.isna()[\"reviews_per_month\"]]\nreviews_per_month_no_nan=ny_df[ny_df.notnull()[\"reviews_per_month\"]]\nreviews_per_month_no_nan.head()","022a5641":"#Lets replace the nan values in the reviews_per_month with 0 and drop the last_review  column\nny_df[\"reviews_per_month\"] = ny_df[\"reviews_per_month\"].fillna(0)\nny_df.drop('last_review', axis=1, inplace=True)","d915bc2b":"#Now lets deal with the missing names and host names. Since we might want to use names later, we wont drop\n#We also want to get rid of NaN values, so we'll replace NaN with Unkown \n\nny_df[\"name\"] = ny_df[\"name\"].fillna(\"Unknown\")\nny_df[\"host_name\"] = ny_df[\"name\"].fillna(\"Unknown\")\n","41a75369":"#making sure we don't have any nan values left \nmissing_data_summary = (ny_df.isnull().sum()\/len(ny_df))*100\nmissing_data_summary","94d35073":"#Moving price to the end of the dataframe\nny_df.columns\nny_df=ny_df[['id', 'name', 'host_id', 'host_name', 'neighbourhood_group',\n       'neighbourhood', 'latitude', 'longitude', 'room_type',\n       'minimum_nights', 'number_of_reviews', 'reviews_per_month',\n       'calculated_host_listings_count', 'availability_365','price']]","88abcb0a":"#Take a look at the continuous data in our dataframe\ndisplay(ny_df.describe())","c27c275d":"\n#Lets just take a look at our target, price, to see what kind of distribution it has\nfig = px.histogram(ny_df, x=\"price\", nbins=30, title='Histogram of Price (Whole Dataset)')\n\nprint(f'The mean price is {ny_df[\"price\"].mean()}')\nprint(f'The median price is {ny_df[\"price\"].median()}')\nprint(f'The max price is {ny_df[\"price\"].max()}')\nimg_bytes = pio.to_image(fig, format=\"svg\")\ndisplay(SVG(img_bytes))","e054b5dc":"#Lets just take a look at our target, price, to see what kind of distribution it has\nfig = px.histogram(ny_df[ny_df[\"price\"]<1000], x=\"price\", nbins=30, title='Histogram of Price (Price Less Than 1000)')\n\nprint(f'The mean price is {ny_df[ny_df[\"price\"]<1000][\"price\"].mean()}')\nprint(f'The median price is {ny_df[ny_df[\"price\"]<1000][\"price\"].median()}')\nprint(f'The max price is {ny_df[ny_df[\"price\"]<1000][\"price\"].max()}')\nprint(f'Dropping {len(ny_df[ny_df[\"price\"]>1000])} rows, {round((len(ny_df[ny_df[\"price\"]>1000]))\/len(ny_df), 3)*100} percent')\nimg_bytes = pio.to_image(fig, format=\"svg\")\ndisplay(SVG(img_bytes))","64806748":"#Lets just take a look at our target, price, to see what kind of distribution it has\nfig = px.histogram(ny_df[ny_df[\"price\"]<250], x=\"price\", nbins=30, title='Histogram of Price (Price Less Than 250)')\n\nprint(f'The mean price is {ny_df[ny_df[\"price\"]<250][\"price\"].mean()}')\nprint(f'The median price is {ny_df[ny_df[\"price\"]<250][\"price\"].median()}')\nprint(f'The max price is {ny_df[ny_df[\"price\"]<250][\"price\"].max()}')\nprint(f'Dropping {len(ny_df[ny_df[\"price\"]>250])} rows, {round((len(ny_df[ny_df[\"price\"]>250]))\/len(ny_df), 3)*100} percent')\nimg_bytes = pio.to_image(fig, format=\"svg\")\ndisplay(SVG(img_bytes))","af93dc19":"#Dropping rows with price above $250\nny_df=ny_df[ny_df[\"price\"]<250]\n\n#Resetting index after dropping prices above $250\nny_df=ny_df.reset_index()\nny_df.drop(\"index\", inplace=True, axis=1)","b47b8cf7":"#Lets just check some basic correllation properties\nny_df.corr().style.background_gradient(cmap='coolwarm')","e5d5b8b6":"#Lets build a wordcloud of common words used in the \"name\" column\n\ntext = \" \".join(name for name in ny_df.name)\n\nwhitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n\nnames_concat = ''.join(filter(whitelist.__contains__, text))\nnames_concat=names_concat.split(\" \")\n\n#Creating dictionary of word counts\nword_counts={}\nfor i in names_concat:\n    if i not in word_counts:\n        word_counts[i]=0\n    word_counts[i]+=1\n\n#Get rid of words with little meaning\nstopwords = set(STOPWORDS)\nstopwords.update([\"Room\", \"Bedroom\", \"Private\", \"In\", \"in\", \"NYC\", \"apartment\", \"room\", \"bedroom\", \"br\", \"Apartment\", \"BR\", \" \"])\n\n#Get rid of stopwords in our dictionary\nfor i in stopwords:\n    if i in word_counts:\n        del word_counts[i]\n\n#Use dictionary to build wordcloud \nwordcloud = WordCloud(background_color=\"white\", width=5000, height=3000, max_words=50).generate_from_frequencies(word_counts)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n","b6dd85cb":"#trying to see if we can get info on number of bedrooms from name\n\nbedrooms=[]\nfor i in ny_df[\"name\"]:\n    m=re.match(\"^\\d+\\s(?i)(br|bedroom)\", i)\n    bedrooms.append(m)    \n\nny_df[\"number_of_bedrooms\"]=bedrooms\nny_df[\"number_of_bedrooms\"] = ny_df[\"number_of_bedrooms\"].fillna(\"Unknown\")\nny_df[\"number_of_bedrooms\"] = ny_df[\"number_of_bedrooms\"].astype('category')\n\n\nny_df.groupby(\"number_of_bedrooms\")[\"price\"].mean()\n\n\n","874aab9a":"#Dropping this column since it won't be useful\nny_df.drop([\"number_of_bedrooms\"], inplace=True, axis=1)","7c8e64cd":"#sorting words_count dict by values (word counts)\n\nsorted_keys = sorted(word_counts, key=word_counts.get, reverse=True)\nsorted_keys\n\nsorted_list_wc=[]\nfor r in sorted_keys:\n    sorted_list_wc.append((r, word_counts[r]))\n\nsorted_list_wc[1:20]","439045f9":"#Lets look at prices based on room type\n\nfig = px.histogram(ny_df, x=\"price\", color=\"room_type\", nbins=30, title='Histogram of Price By Room Type')\nfig.update_layout(barmode='overlay')\nfig.update_traces(opacity=0.75)\nimg_bytes = pio.to_image(fig, format=\"svg\")\ndisplay(SVG(img_bytes))\n","55592b39":"ax = sns.swarmplot(x=\"room_type\", y=\"price\", data=ny_df)\n\n","28b9f59e":"fig = px.histogram(ny_df[ny_df[\"minimum_nights\"]<10], x=\"minimum_nights\", nbins=20, title='Histogram of Min Nights, 0-10 Min Nights')\nimg_bytes = pio.to_image(fig, format=\"svg\")\ndisplay(SVG(img_bytes))\n\nfig = px.histogram(ny_df[ny_df[\"minimum_nights\"]<30], x=\"minimum_nights\", nbins=20, title='Histogram of Min Nights, 0-30 Min Nights')\nimg_bytes = pio.to_image(fig, format=\"svg\")\ndisplay(SVG(img_bytes))\n\nfig = px.histogram(ny_df, x=\"minimum_nights\", nbins=20, title='Histogram of Min Nights, Entire Data')\nimg_bytes = pio.to_image(fig, format=\"svg\")\ndisplay(SVG(img_bytes))\n\nprint('the mode is ', ny_df[\"minimum_nights\"].mode())\nprint('the median is ', ny_df[\"minimum_nights\"].median())\nprint('the mean is ', ny_df[\"minimum_nights\"].mean())\n\n\n\n\n\n\n","f282cf5a":"#Lets look at prices based on latitude (note that Mahattan has greater Longitude values than Brooklyn\/Williamsburg)\n\nfig = px.scatter(ny_df, x=\"longitude\", y=\"price\", title='Longitude vs Price', trendline='ols')\nresults = px.get_trendline_results(fig)\nresults = px.get_trendline_results(fig)\nimg_bytes = pio.to_image(fig, format=\"svg\")\ndisplay(SVG(img_bytes))","ad52ef3b":"#Lets look at prices based on latitude (note that Mahattan has higher latitude values)\n\nfig = px.scatter(ny_df[ny_df[\"price\"]<1000], x=\"latitude\", y=\"price\", title='Latitude vs Price', trendline='ols')\nresults = px.get_trendline_results(fig)\nresults = px.get_trendline_results(fig)\nimg_bytes = pio.to_image(fig, format=\"svg\")\ndisplay(SVG(img_bytes))","ff52c407":"ax = sns.violinplot(x=\"neighbourhood_group\", y=\"price\", data=ny_df)","6b1c32bc":"#Map looking at most expensive neighborhoods\n\nneighborhoods=ny_df.groupby([\"neighbourhood\", \"room_type\"])[[\"price\", \"latitude\", \"longitude\", \"minimum_nights\"]].mean().reset_index()\nneighborhoods.sort_values('price', ascending=False, inplace=True)\nrich_neighborhoods=neighborhoods\n\n\nfig = px.scatter_mapbox(rich_neighborhoods, lat=\"latitude\", lon=\"longitude\",  size=\"price\", color=\"room_type\", size_max=15, zoom=10, hover_name=\"neighbourhood\", hover_data=[\"price\", \"minimum_nights\"], title=\"Map of Prices By Neighborhood and Roomtype\")\nfig.update_layout(mapbox_style=\"carto-positron\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n\nimg_bytes = fig.to_image(format=\"png\")\nimg_bytes = pio.to_image(fig, format=\"svg\")\ndisplay(SVG(img_bytes))","2ca5d328":"#Maps looking at prices for each Airbnb \n\n\nfig = px.scatter_mapbox(ny_df, lat=\"latitude\", lon=\"longitude\",  color=\"price\", size_max=15, zoom=10)\nfig.update_layout(mapbox_style=\"carto-positron\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nimg_bytes = pio.to_image(fig, format=\"svg\")\ndisplay(SVG(img_bytes))","130af3c2":"#Neighborhood Group Map\nfig = px.scatter_mapbox(ny_df, lat=\"latitude\", lon=\"longitude\",  color=\"neighbourhood_group\", size=\"price\", size_max=15, zoom=10)\nfig.update_layout(mapbox_style=\"carto-positron\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nimg_bytes = pio.to_image(fig, format=\"svg\")\ndisplay(SVG(img_bytes))\n","56ac4be0":"#Checking the median and mean price\nprint(ny_df[\"price\"].median())\nprint(ny_df[\"price\"].mean())","81257c81":"#adding dummies for roomtype\nroom_type_dummies=pd.get_dummies(ny_df['room_type'], prefix='room_type', drop_first=True).reset_index()\nroom_type_dummies.drop(\"index\", axis=1, inplace=True)\nny_df_added_features = pd.concat([room_type_dummies, ny_df], axis=1)\n\n#adding dist_from_midtown feature. After doing EDA on prices and location, a lot of the higher prices appeared to be very close to midtown. Lets construct a feature for this.\nny_df_added_features[\"dist_from_midtown\"]=abs(ny_df_added_features[\"latitude\"]-40.7069)+abs(ny_df_added_features[\"latitude\"]+74.0031)\n\n#adding dummies for neighborhood group\nny_df_added_features = pd.concat([ny_df_added_features ,pd.get_dummies(ny_df_added_features['neighbourhood_group'], prefix='neighbourhood_group', drop_first=True)],axis=1)\n\n#adding dummies for neighborhood\nny_df_added_features = pd.concat([ny_df_added_features ,pd.get_dummies(ny_df_added_features['neighbourhood'], prefix='neighbourhood', drop_first=True)],axis=1)\n\n","3e312f19":"#Dropping the original columns we just one hot encoded\nny_df_added_features.drop([\"neighbourhood_group\", \"neighbourhood\", \"room_type\", \"host_id\", \"host_name\", \"id\"], axis=1, inplace=True)","73f3d9ed":"ny_df_added_features.head()","c6996c71":"#Splitting data into remainder (train and validation) and test split of 20% (calling X datarame \"no words\" because I haven't count vectorized the name column yet).\n\n#I want all the columns except price in my X dataframe\nX=ny_df_added_features.loc[:, ny_df_added_features.columns != 'price']\n\n#Price is the target\ny=ny_df_added_features[\"price\"]\nsplit = 0.2\nX_remainder_no_words, X_test_no_words, y_remainder, y_test = train_test_split(X, y, test_size=split, random_state=6)\nfor i in [X_remainder_no_words, X_test_no_words, y_remainder, y_test]:\n    print(i.shape)\n\n","396d1cab":"#Resettig axis and dropping index columns\nX_remainder_no_words=X_remainder_no_words.reset_index()\nX_remainder_no_words.drop(\"index\", axis=1, inplace=True)\n\ny_remainder=y_remainder.reset_index()\ny_remainder.drop(\"index\", axis=1, inplace=True)\n\n\nX_test_no_words=X_test_no_words.reset_index()\nX_test_no_words.drop(\"index\", axis=1, inplace=True)\n\ny_test=y_test.reset_index()\ny_test.drop(\"index\", axis=1, inplace=True)","a0f32129":"#Scaling X data so we can compare the effect different coefficients are having\n\nX_remainder_no_words_scaled=X_remainder_no_words.copy()\nX_test_no_words_scaled=X_test_no_words.copy()\n\n\nscalerx = StandardScaler()\nscalerx.fit(X_remainder_no_words_scaled[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']])\nX_remainder_no_words_scaled[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']] = scalerx.transform(X_remainder_no_words_scaled[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']])\nX_test_no_words_scaled[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']] = scalerx.transform(X_test_no_words_scaled[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']])\n\n\n","7e574f1e":"#Baseline linear regression using scaled data so we can evaluate coefficients (not including latitude\/longitude\/dist_from_midtown)\nX = X_remainder_no_words_scaled[[ \"room_type_Private room\",\n\"room_type_Shared room\",\n\"neighbourhood_group_Manhattan\",\n\"neighbourhood_group_Brooklyn\",\n\"neighbourhood_group_Queens\",\n\"neighbourhood_group_Staten Island\"]]\ny = y_remainder\nX_withconstant = sm.add_constant(X)\n\n# 1. Instantiate Model\nmyregression = sm.OLS(y,X_withconstant)\n\n# 2. Fit Model (this returns a seperate object with the parameters)\nmyregression_results = myregression.fit()\n\n# Looking at the summary\nmyregression_results.summary()\n ","9b09009a":"print(X_remainder_no_words.shape)\nprint(X_test_no_words.shape)\nprint(y_remainder.shape)\nprint(y_test.shape)","6ceaf250":"#feed in the X_test dataframe, y_test, and the model\ndef get_errors(X, y_test, model):\n    \n    #predict the models \n    y_pred=model.predict(X)\n    \n    #create a dataframe to store the predictions, actual values, and errors\n    y_test_df=y_test.copy()\n    \n    #add predictions to the dataframe\n    y_test_df[\"pred\"]=y_pred\n    \n    #add the squared error of each prediction from the actual value to the dataframe\n    y_test_df[\"error\"]=(y_test_df[\"price\"]-y_test_df[\"pred\"])**(2)\n    \n    #add the absolute squared error to the dataframe\n    y_test_df[\"abs_error\"]=abs((y_test_df[\"price\"]-y_test_df[\"pred\"]))\n\n\n    #take the square root of the sum of the squared error column of the dataframe to get RMSE\n    print(f'RMSE={(((y_test_df[\"error\"].sum()))\/len(y_test_df))**(1\/2)}')\n    \n    #Plot the absolute errors in a histogram\n    fig=px.histogram(x=y_test_df['abs_error'], title=\"Abs Error\", nbins=30)\n\n\n    img_bytes = pio.to_image(fig, format=\"svg\")\n    display(SVG(img_bytes))","6d2dd92f":"#Try linear model using continuous data plust one hot encoded data for room_type and neighborhood_group\n\n\nX_remainder_baseline=X_remainder_no_words[['room_type_Private room', 'room_type_Shared room', 'latitude', 'latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'calculated_host_listings_count', 'availability_365', 'dist_from_midtown', 'neighbourhood_group_Brooklyn', 'neighbourhood_group_Manhattan', 'neighbourhood_group_Queens', 'neighbourhood_group_Staten Island']]\nX_test_baseline=X_test_no_words[['room_type_Private room', 'room_type_Shared room', 'latitude', 'latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'calculated_host_listings_count', 'availability_365', 'dist_from_midtown', 'neighbourhood_group_Brooklyn', 'neighbourhood_group_Manhattan', 'neighbourhood_group_Queens', 'neighbourhood_group_Staten Island']]\n\n\n# 1. Instantiate the model\nlinear_regression_model = LinearRegression()\n\n# 2. Fit the model\nlinear_regression_model.fit(X_remainder_baseline, y_remainder)\n\n#The intercept\nintercept = linear_regression_model.intercept_\n\n#The coefficient, notice it returns an array with one spot for each feature\ncoefficient = linear_regression_model.coef_\n\n\nmodel_prediction_train = linear_regression_model.predict(X_remainder_baseline)\nmodel_prediction_test = linear_regression_model.predict(X_test_baseline)\n\n\n\n# Evaluate the model on each set\nprint(f'The R2 score on the training set: {r2_score(y_remainder,model_prediction_train)}')\nprint(f'The R2 score on the testing set: {r2_score(y_test,model_prediction_test)}')\n\nprint(f'The RMSE score on the training set: {(mean_squared_error(y_remainder,model_prediction_train))**(1\/2)}')\nprint(f'The RMSE score on the testing set: {(mean_squared_error(y_test,model_prediction_test))**(1\/2)}')\n\n\n#looks like we are massively overfitting- negative R2 on test ","d78ebcd5":"X_remainder_added=X_remainder_no_words.loc[:, X_remainder_no_words.columns != 'name']\nX_test_added=X_test_no_words.loc[:, X_test_no_words.columns != 'name']\n\n# 1. Instantiate the model\nlinear_regression_model = LinearRegression()\n\n# 2. Fit the model\nlinear_regression_model.fit(X_remainder_added, y_remainder)\n\n#The intercept\nintercept = linear_regression_model.intercept_\n\n#The coefficient, notice it returns an array with one spot for each feature\ncoefficient = linear_regression_model.coef_\n\n\nmodel_prediction_train = linear_regression_model.predict(X_remainder_added)\nmodel_prediction_test = linear_regression_model.predict(X_test_added)\n\n\n\n# Evaluate the model on each set\nprint(f'The R2 score on the training set: {r2_score(y_remainder,model_prediction_train)}')\nprint(f'The R2 score on the testing set: {r2_score(y_test,model_prediction_test)}')\n\nprint(f'The RMSE score on the training set: {(mean_squared_error(y_remainder,model_prediction_train))**(1\/2)}')\nprint(f'The RMSE score on the testing set: {(mean_squared_error(y_test,model_prediction_test))**(1\/2)}')\n\n\n#looks like we are massively overfitting- negative R2 on test ","18527d4e":"get_errors(X_test_added, y_test, linear_regression_model)","fe32148c":"print(X_remainder_added.shape)\nprint(X_test_added.shape)\nprint(y_remainder.shape)\nprint(y_test.shape)\n\n\n\n","f044ce62":"#Now looking at the count vectorized name data \n\n\n# 1. Instantiate, setting min_df to 10 which means that the minimum times the word must appear in the corpus is 10\nbagofwords = CountVectorizer(min_df=10)\n\n# 2. Fit \nbagofwords.fit(X_remainder_no_words[\"name\"])\n\n# 3. Transform \nX_remainder_bagofwords = bagofwords.transform(X_remainder_no_words[\"name\"])\nX_test_bagofwords = bagofwords.transform(X_test_no_words[\"name\"])\n\n\n","ec59d6f5":"#Adding the bag of words data to a dataframe from the array that the transform method produced\nbagofwords.get_feature_names()\nX_remainder_bagofwords.toarray()\nX_test_bagofwords.toarray()\n\n\nbag_of_words_train = pd.DataFrame(columns=bagofwords.get_feature_names(), data=X_remainder_bagofwords.toarray())\nbag_of_words_test = pd.DataFrame(columns=bagofwords.get_feature_names(), data=X_test_bagofwords.toarray())\n\n\n","e90e53ef":"#Now we concat the count vectorized words to our X dataframes to get X_added_features\nX_remainder_added_features=pd.concat([bag_of_words_train, X_remainder_no_words], axis=1)\nX_test_added_features=pd.concat([bag_of_words_test, X_test_no_words], axis=1)\n\nprint(X_remainder_added_features.shape)\nprint(X_test_added_features.shape)\n\n","0a82559a":"X_remainder_added_features.drop([\"name\"], axis=1, inplace=True)\nX_test_added_features.drop([\"name\"], axis=1, inplace=True)","54e0ecdc":"print(X_remainder_added_features.shape)\nprint(X_test_added_features.shape)","73a0d84a":"\n\n# Create the lasso and ridge models\nlasso = Lasso()\nlasso.fit(X_remainder_added_features,y_remainder)\nridge = Ridge(alpha=20)\nridge.fit(X_remainder_added_features,y_remainder)\n\nprint(\"Coefficients:\")\nprint(\"Lasso:\", lasso.coef_)\nprint(\"Ridge:\", ridge.coef_)\nprint(\"\")\n\n\n# Compare R-squared\nprint(\"R-squared:\")\nprint(\"Lasso train:\", lasso.score(X_remainder_added_features,y_remainder))\nprint(\"Ridge train:\", ridge.score(X_remainder_added_features,y_remainder))\nprint(\"Lasso test:\", lasso.score(X_test_added_features,y_test))\nprint(\"Ridge test:\", ridge.score(X_test_added_features,y_test))\n\n\nprint(f'The RMSE score on the training set: {(mean_squared_error(y_remainder,model_prediction_train))**(1\/2)}')\nprint(f'The RMSE score on the testing set: {(mean_squared_error(y_test,model_prediction_test))**(1\/2)}')\n\nprint(\"Ridge Train and Test RMSE:\")\n\nprint(\"Ridge Train\")\nget_errors(X_remainder_added_features, y_remainder, ridge)\n\nprint(\"Ridge Test\")\nget_errors(X_test_added_features, y_test, ridge)\n\n\nprint(\"Lasso Train\")\nget_errors(X_remainder_added_features, y_remainder, lasso)\n\nprint(\"Lasso Test\")\nget_errors(X_test_added_features, y_test, lasso)","bf33c0ee":"#Running a logistic regression of bag of words on price to see which words have the most predictive power\n\n#Fit a logistic regression model\nlogreg_model = LogisticRegression()\nlogreg_model.fit(bag_of_words_train, y_remainder)\n\nprint(f'score on training: {logreg_model.score(X_remainder_bagofwords, y_remainder)}')\nprint(f'score on testing: {logreg_model.score(X_test_bagofwords, y_test)}')\n\n\nbag_of_words_train.head()","29097746":"#Getting list of indicies for the words\nindicies=[]\nfor i in bag_of_words_train:\n    index=bag_of_words_train.columns.get_loc(i)\n    indicies.append(index)\n\n#making a list of coefficients from the logistic regression    \ncoef_lst=logreg_model.coef_.tolist()\n\n#merging indicies and coefficients into a list of tuple \ntuples=list(zip(indicies, coef_lst[0]))\ntuples\n\n#sorting the tuples \ndef Sort_Tuple(tup):  \n  \n    # reverse = None (Sorts in Ascending order)  \n    # key is set to sort using second element of  \n    # sublist lambda has been used  \n    tup.sort(key = lambda x: x[1], reverse=True)  \n    return tup \n\nSorted_words=Sort_Tuple(tuples)\n\nindicies_2=[]\nfor i in Sorted_words:\n    indicies_2.append(i[0])\n\nwords_sorted=bag_of_words_train.columns[indicies_2]\n\n#Bottom 30 words \nfor i in words_sorted[0:30]:\n    print(i)","c4ed6554":"#redefine the sort function to sort in decending order  \ndef Sort_Tuple(tup):  \n  \n    # reverse = None (Sorts in Ascending order)  \n    # key is set to sort using second element of  \n    # sublist lambda has been used  \n    tup.sort(key = lambda x: x[1])  \n    return tup \n\nReverse_Sorted_words=Sort_Tuple(tuples)\n\nindicies_2=[]\nfor i in Reverse_Sorted_words:\n    indicies_2.append(i[0])\n\nwords_sorted_reverse=bag_of_words_train.columns[indicies_2]\n\n#Top 30 predictive words of high price\nfor i in words_sorted_reverse[0:30]:\n    print(i)","10462e87":"X_train_added_features, X_validation_added_features, y_train, y_validation = \\\n    train_test_split(X_remainder_added_features, y_remainder, test_size = 0.3,\n                     random_state=1)","0a136d2f":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\n\ncross_validation_scores_val = []\ncross_validation_scores_train = []\n\n\n\n\ndepths=range(1,10)\nfor depth in depths:\n    tree = DecisionTreeRegressor(max_depth=depth)\n    tree.fit(X_train_added_features, y_train)\n    cv_score_val = np.mean(cross_val_score(tree, X_validation_added_features, y_validation, cv = 5))\n    cross_validation_scores_val.append(cv_score_val)\n    cv_score_train = np.mean(cross_val_score(tree, X_train_added_features, y_train, cv = 5))\n    cross_validation_scores_train.append(cv_score_train)\n\n\n    print(f'depth = {depth}')\n    print(f\"DT R^2 score on training set: {tree.score(X_train_added_features, y_train):0.3f}\")\n    print(f\"DT R^2 score on validation set: {tree.score(X_validation_added_features, y_validation):0.3f}\")\n    \n","518c544a":"plt.figure()\nplt.plot(depths, cross_validation_scores_val,label=\"Cross Validation Score Val\",marker='.')\nplt.plot(depths, cross_validation_scores_train,label=\"Cross Validation Score Train\",marker='.')\nplt.legend()\nplt.xlabel('Max Depth')\nplt.ylabel('Cross Validation Score')\nplt.grid()\nplt.show()","ab5776c0":"tree = DecisionTreeRegressor(max_depth=8)\ntree.fit(X_train_added_features, y_train)\nprint(f'Validation score with optimal max depth of 8: {tree.score(X_validation_added_features, y_validation)}')\nprint(f'Test score with optimal max depth of 8: {tree.score(X_test_added_features, y_test)}')","2044bf05":"get_errors(X_test_added_features, y_test, tree)","7902fb1c":"cross_validation_scores_val = []\ncross_validation_scores_train = []\n\nmax_depth_lst=range(1,10)\nfor max_depth in max_depth_lst:\n    my_random_forest = RandomForestRegressor(n_estimators=100, max_depth=max_depth)\n    my_random_forest.fit(X_train_added_features, y_train.values.ravel())\n    cv_score_val = np.mean(cross_val_score(tree, X_validation_added_features, y_validation, cv = 5))\n    cross_validation_scores_val.append(cv_score_val)\n    cv_score_train = np.mean(cross_val_score(tree, X_train_added_features, y_train, cv = 5))\n    cross_validation_scores_train.append(cv_score_train)\n    \n\n    print(f'depth = {max_depth}')\n    print(f\"DT R^2 score on training set: {my_random_forest.score(X_train_added_features, y_train):0.3f}\")\n    print(f\"DT R^2 score on validation set: {my_random_forest.score(X_validation_added_features, y_validation):0.3f}\")\n    \n","782c3e00":"from sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [1,2,3,4,5,6,7,8,9,10],\n    'max_features': [1,2,3,4,5],\n    'min_samples_leaf': [1,2,3,4,5],\n    'min_samples_split': [1,2,3,4,5,6,7,8,9,10],\n    'n_estimators': [10,20,30,100,200]\n}\n\nmy_random_forest = RandomForestRegressor()\n\nclf = GridSearchCV(my_random_forest, param_grid)\n\nmy_random_forest.fit(X_remainder_added_features, y_remainder.values.ravel())\n","c0e12d67":"my_random_forest.score(X_test_added_features, y_test)","f5fcd3dd":"get_errors(X_test_added_features, y_test, my_random_forest)","1413ae07":"param_grid = {\n    'max_depth': [1,2,3,4,5,6,7,8,9,10],\n    'max_features': [180],\n    'min_samples_leaf': [1,2,3,4,5],\n    'min_samples_split': [1,2,3,4,5,6,7,8,9,10],\n    'n_estimators': [10,20,30,100,200],\n    'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1]\n}\n\ngb_model = GradientBoostingRegressor()\n\nclf = GridSearchCV(gb_model, param_grid)\n\ngb_model.fit(X_remainder_added_features, y_remainder.values.ravel())","84efc936":"gb_model.score(X_test_added_features, y_test)","e7c67a71":"get_errors(X_test_added_features, y_test, gb_model)","88defa45":"#Scaling data for KNN (only scaling continuous data and not the one hot encoded data)","30151b16":"from sklearn.preprocessing import StandardScaler\n\nscalerx = StandardScaler()\nscalerx.fit(X_train_added_features[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']])\nX_train_added_features[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']] = scalerx.transform(X_train_added_features[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']])\nX_validation_added_features[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']] = scalerx.transform(X_validation_added_features[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']])\nX_test_added_features[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']] = scalerx.transform(X_test_added_features[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']])\nX_remainder_added_features[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']] = scalerx.transform(X_remainder_added_features[['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']])\n\n","803b3b59":"from sklearn.neighbors import KNeighborsRegressor\n\nparam_grid = {'n_neighbors':[2,3,4,5,6,7,8,9]}\n\nKNN = KNeighborsRegressor()\n\nKNN_model = GridSearchCV(KNN, param_grid, cv=5)\nKNN_model.fit(X_remainder_added_features,y_remainder)\nKNN_model.best_params_\n","7b7f25c0":"KNN_model.score(X_test_added_features,y_test)","2d406c9d":"#Lets try KNN \nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\n\nKNNmodel = KNeighborsRegressor(weights='distance', n_neighbors=2)\nKNNmodel.fit(X_train_added_features, scaled_y_remainder)\n\nprint(f\"R^2 score on training set: {KNNmodel.score(X_train_added_features, scaled_y_remainder)}\")\nprint(f\"R^2 score on test set: {KNNmodel.score(X_test_added_features, scaled_y_test)}\")","2cf118f7":"get_errors(X_test_added_features, y_test, KNN_model)","0b9963ab":"y_train_scaled=np.log10(y_train+1)\ny_test_scaled=np.log10(y_test+1)\ny_validation_scaled=np.log10(y_validation+1)","20e79c84":"y_train_scaled.head()","70576921":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n#from tensorflow.metrics import RootMeanSquaredError\nmodel = Sequential([\n    Dense(512, activation='relu', input_shape=(1227,)),\n    Dense(512),\n    Dense(1),\n])\n\n\nmodel.compile(\n              loss='mse', optimizer='adam')\n\nhist = model.fit(X_train_added_features, y_train_scaled,\n          batch_size=32, epochs=2,\n          validation_data=(X_validation_added_features, y_validation_scaled))\n\n","5b9aa0c3":"print(hist.history.keys())","799e5b30":"# list all data in history\nprint(hist.history.keys())\n\n# summarize history for loss\nplt.plot(hist.history['loss'][1:])\nplt.plot(hist.history['val_loss'][1:])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","599026b6":"print(X_val.shape)\nprint(y_validation.shape)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","ff37e76e":"y_pred_log=model.predict(X_test_added_features)\ny_pred_normal=(10**y_pred_log)-1\ny_test_df=y_test.copy()\ny_test_df[\"pred\"]=y_pred_normal\ny_test_df[\"abs_error\"]=np.abs(y_test_df[\"pred\"]-y_test_df[\"price\"])\nRMSE=((((y_test_df[\"price\"]-y_test_df[\"pred\"])**(2))\/len(y_test_df)).sum())**(1\/2)\n\nprint(f'RMSE: {RMSE}')\nfig=px.histogram(x=y_test_df['abs_error'], nbins=30)\n\n\nfig.show()\n","86d220a3":"get_errors(X_test_added_features, y_test_scaled, model)","4d84f734":"#Compare Models\nRMSE=[37, 36 , 34, 40.47, 32.6, 40.19, 33.8]\nRMSE=sorted(RMSE)\n\nplt.figure()\nplt.bar(x=[\"RFR\", \"GBR\", \"Ridge\", \"DT\", \"LR2\", \"LR1\",  \"NN\"], height=RMSE)\nplt.xlabel(\"Model\")\nplt.ylabel(\"RMSE on Test\")\nplt.title(\"Model Comparison\")\nplt.show()","6614d8d7":"RMSE","88166b0b":"The main method of model comparison will be RMSE, the \"root mean squared error\" of our predictions from the actual values in the test set. It will also be helpful to visualize the errors as a histogram. I will define a function that will calculate the RMSE for any model.","941220bf":"It seems as though longitude has an inverse relationship with price and latitude has a postive relationship with price. This makes some sense as Manhattan has lower longitudinal coordinates and higher latitude coordinates. Longitude seems to be more predictive than latitude","97a444db":"## EDA##","4c962d26":"### Neighborhood EDA","b6ba9c2d":"It looks like max depth of 8 performed the best on the cross validation scores. ","d17669e5":"Now lets add the neighbourhood one hot encoding to the linear regression. This will bring our feautes up to over 200 since there are many neighbourhoods. Lets see what happens with the RMSE with these added regressors.","16bbdee2":"### Room Type EDA","b8952954":"Based on outside research, common sense, and EDA, it seems like price would mainly be a function of the room_type and general neighborhood group. Lets see how much of the variation can be explained by only these factors. I use statsmodels here so that the pvalues can easily be seen along with the coefficient values.","dc764f16":"Latitude and longitude describe the exact location of each Airbnb. Since Manhattan has lower longitudinal coordinates, I expect there will be a relationship there.","c01f0d33":"It might be helpful to see which neighborhoods tend to have more expensive listings. Lets assign color to price.","6e7fe932":"### A Little More Preprocessing and Added Features","c11adb35":"### Room Type and Neighborhood Maps","f8e71683":"### Neural Network ","81a524c6":"In this notebook, I perform exploratory data analysis to better understand the data. I then use machine learning techniques to predict price, using only the bottom 90% of the data (prices below $250).","de7a5185":"Now lets try a regression with all of the continuous data plus the room_type one hot encoded and neighbourhood group one hot encoded. This regression includes 14 features. ","2d50783d":"The models run in this section include linear regression, lasso\/ridge regression, decision tree regressor, random forest regressor, and a neural network. Again, this is modelling done on prices below $250 from the original data. ","28f00f42":"This column is highly skewed, with a few properties having minimum_nights set to over two years. The median is 2 nights and the vast majority are below a week. ","5e9fcd0d":"It looks like these distributions are fairly different between private and shared room and entire apartment, indicating that room_type might be a good predictor of price.","ffbf0df5":"It would make sense that the different neighborhood groups would have different price distribitions. I would expect Manhattan to be the most expensive, and possibly Brooklyn as second most expensive.","9b801509":"Some of the words here make sense, and some of them are less logical. For example, in the bottom 30 words, \"coliving\" and \"hostel\" are suggestive of cheaper prices, but \"space\" doesn't necessarily imply cheap. \nLooking at words most predictive of high prices, \"manhattan\" makes sense, while \"cozy\" is less suggestive. Overall, it seems like the count vectorized name should provide some additional predictive power for the model. ","196a84be":"First, lets look at the distrbution of minimum nights in the data.","85516238":"Looks like only 648 out of 40k have bedroom info. This is likely insufficient to do modelling with","e0f68f63":"### Minimum_nights EDA","9e3d7e78":"## Modeling ","4bc51877":"The room type column is split into private, entire home\/apartment, and shared room. I'd expect that these would have different prices associated with them. However, the data set is missing square footage and number of bedrooms which would be other (perhaps more useful) ways to measure the size of the space","54e91acc":"For this notebook, lets focus on prices that are below 250 dollars. The data is a lot less skewed for lower prices, and prices above 250 dollars only make up 10% of the data. ","a9739904":"It might be useful to count vectorize the names later on in the analysis, so lets see if there is common verbiage used by hosts in their name\/description.","25233819":"Now lets count vectorize the name column so the model can use this information. ","9cc912db":"### Longitude\/Latitude EDA ","08e7bb10":"Lets one hot encode room_type, neighborhood_group, and neighborhood. I'll also add a new feature for distance from midtown, since the EDA suggested that a lot of more expensive properties are near Midtown. Note, name is still in the dataframe-- I'll address this later on. ","a128e590":"## Data Cleaning ","b0501af7":"We can see that the R2 has increased from 0.477 to o.529 on the training set, meaning that more of the variation can be explained with the added regressors. From now on, lets pay attention to the RMSE on the test set as the main metric of model accuracy. This model has an RMSE of 37.","0dcc39ed":"In this section, I will visualize the features to better understand the relationships in the data.","b7f9f9ca":"### Name EDA","e6863e94":"Lets check out the word counts for the top most popular words in the name column. ","b91045b3":"Now that most of the preprocessing is done, let's check a scaled linear regression to see which columns are having the most effect on price. Note I am not scaling one hot encoded columns, only the continuous ones.","44d49407":"Minimum_nights refers to the minimum number of nights that must be booked. It is not obvious how this will impact the per night price; however, longer stays might have a lower per night cost due to economies of scale. ","ccbdab2d":"It would be interesting to see which words are most predictive of a higher prices and of a lower prices.","8bf15a9a":"When I get to modelling, the distribution of the target (price) will be important to the accuracy. Since the range of price looks high in the plot above, let's visualize what price looks like at different thresholds.  ","72597290":"It looks like the neighborhood one hot encoding brought the RMSE from 37 to 36 on the test set- a slight improvement.","ed3c39b1":"It looks like these neighbourhood groups do have different price distributions with Manhattan being the most expensive. This will probably be a highly predictive feature to use in the modelling.","9eb63cf4":"It looks like our R2 on the train plus validation is 0.477 with only including these regressors. We can see that having a private or shared room pulls down the price, and being in Manhattan increases the price. Note that the constant represents the average price for an entire home in the Bronx, since these are the categories that have been left out of the regression to avoid multicollinearity.","4bb48733":"One explanatory variable that would be helpful to our data would be number of bedrooms. Although we have a room type field, understanding the number of bedrooms might have more predictive power. Lets see if it's possible to strip out number of bedrooms from the name field.","91b1b617":"Lets try a decision tree regressor. I'll also use cross validation to select the correct max depth.","cad02109":"Lets plot the validation and training scores and select the depeth with the highest validation.","7cd7282d":"Next, lets try a KNN Regressor. (warning: the cell below takes a very long time to run )","c5e1e301":"To understand where the neighborhood group borders are, lets visualize them on a map. "}}