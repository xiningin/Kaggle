{"cell_type":{"c1be9072":"code","0c7a7084":"code","82262b6c":"code","6490ad97":"code","260ee1b4":"code","f517335e":"code","9cd32c60":"code","4bf44256":"code","567a46ac":"code","144a9bcf":"markdown","f26ac3d6":"markdown"},"source":{"c1be9072":"# ==================\n# \u30e9\u30a4\u30d6\u30e9\u30ea\n# ==================\nimport pandas as pd\nimport numpy as np\nimport torch\nimport transformers\nfrom transformers import BertTokenizer\nfrom tqdm import tqdm\nfrom sklearn.decomposition import TruncatedSVD\ntqdm.pandas()","0c7a7084":"df_wiki = pd.read_csv(\"..\/input\/data-science-summer2-osaka\/wiki.csv\") \ndf_wiki","82262b6c":"class BertSequenceVectorizer:\n    def __init__(self):\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.model_name = 'bert-base-uncased'\n        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n        self.bert_model = transformers.BertModel.from_pretrained(self.model_name)\n        self.bert_model = self.bert_model.to(self.device)\n        self.max_len = 128\n\n\n    def vectorize(self, sentence : str) -> np.array:\n        inp = self.tokenizer.encode(sentence)\n        len_inp = len(inp)\n\n        if len_inp >= self.max_len:\n            inputs = inp[:self.max_len]\n            masks = [1] * self.max_len\n        else:\n            inputs = inp + [0] * (self.max_len - len_inp)\n            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n\n        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n\n        bert_out = self.bert_model(inputs_tensor, masks_tensor)\n        seq_out, pooled_out = bert_out['last_hidden_state'], bert_out['pooler_output']\n\n        if torch.cuda.is_available():    \n            return seq_out[0][0].cpu().detach().numpy() # 0\u756a\u76ee\u306f [CLS] token, 768 dim \u306e\u6587\u7ae0\u7279\u5fb4\u91cf\n        else:\n            return seq_out[0][0].detach().numpy()","6490ad97":"BSV = BertSequenceVectorizer() # \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3057\u307e\u3059\ndf_wiki['wiki_description'] = df_wiki['wiki_description'].fillna(\"NaN\") # null \u306f\u4ee3\u308f\u308a\u306e\u3082\u306e\u3067\u57cb\u3081\u307e\u3059\ndf_wiki['wiki_description_feature'] = df_wiki['wiki_description'].progress_apply(lambda x: BSV.vectorize(x))","260ee1b4":"bert_array = np.zeros([len(df_wiki),768])\nfor n,i in enumerate(df_wiki['wiki_description_feature']):\n    bert_array[n,:] = i","f517335e":"svd = TruncatedSVD(n_components=50)\nX = svd.fit_transform(bert_array)\ndf = pd.DataFrame(X, columns=[f\"wiki_description_bert_svd_{i}\" for i in range(50)])","9cd32c60":"df[\"Japanese name\"] = df_wiki[\"Japanese name\"]","4bf44256":"df.head()","567a46ac":"df.to_csv(\"df_wiki_description_bert.csv\",index=False)","144a9bcf":"## \u7b2c2\u56de\u306b\u7d39\u4ecb\u3057\u3066\u9802\u3044\u305f\u5185\u5bb9\u306e\u30b3\u30d4\u30fc\n\n\u8a73\u7d30\u306f\u4e0b\u8a18URL\u3078\n\n\nhttps:\/\/www.kaggle.com\/takoihiraokazu\/bert-text","f26ac3d6":"### BERT\u3092\u4f7f\u3063\u3066\u3001text\u30c7\u30fc\u30bf\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3057\u307e\u3059\nhttps:\/\/huggingface.co\/transformers\/model_doc\/bert.html <br>\n\n\u30c7\u30fc\u30bf\u5206\u6790\u306e\u30b3\u30f3\u30da\u3067\u306f\u30c6\u30ad\u30b9\u30c8\u51e6\u7406\u306b\u3088\u304fBERT\u306f\u4f7f\u308f\u308c\u307e\u3059 <br>\n\u4e0b\u8a18URL\u3092\u53c2\u8003\u306b\u3057\u3066\u3044\u307e\u3059<br>\nhttps:\/\/www.guruguru.science\/competitions\/16\/discussions\/fb792c87-6bad-445d-aa34-b4118fc378c1\/\n\nSettings\u3067\u4e0b\u8a18\u8a2d\u5b9a\u306b\u5909\u66f4\u3057\u3066\u304f\u3060\u3055\u3044\n- Accelerator\u3092GPU\n- Internet\u3092ON"}}