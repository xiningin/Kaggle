{"cell_type":{"b68bcfa5":"code","e82a9bfa":"code","53e1d89d":"code","222f71ae":"code","d124d4bb":"code","c9a41396":"code","ed0ae3bf":"code","71560dfb":"code","b817e0d7":"code","71f90ff1":"code","66d64166":"code","cf7b699b":"code","f4700766":"code","436920ba":"code","1bcdc055":"code","22507b5e":"code","88234701":"code","386e95d5":"code","4dbcb937":"code","a8c74dea":"code","7dc53d22":"code","e38374c0":"code","3d7cc81a":"code","ea50e4bd":"code","e71c0e80":"code","fa2df7cf":"code","ce09e1d0":"markdown","81e2dcaa":"markdown","a8c26fde":"markdown","cafa915f":"markdown","dd11a4f5":"markdown","0652e6a2":"markdown","477a6109":"markdown","6ff0bce3":"markdown","2bdf5ff7":"markdown","6c59ea5c":"markdown","c2890627":"markdown","cab03d96":"markdown","4904c8f3":"markdown","2daf5235":"markdown","9b5fe8a4":"markdown"},"source":{"b68bcfa5":"#!pip install -U gensim\nimport warnings\nwarnings.filterwarnings('ignore')","e82a9bfa":"# import packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport sqlite3\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,classification_report,f1_score\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import cross_validation\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import expon","53e1d89d":"# creating sql connection string\ncon = sqlite3.connect('..\/input\/database.sqlite')","222f71ae":"#Positive Review - Rating above 3\n#Negative Review - Rating below 3\n#Ignoring Reviews with 3 Rating\n\nfiltered_data = pd.read_sql_query('SELECT * from Reviews WHERE Score != 3',con)","d124d4bb":"filtered_data.head(5)","c9a41396":"# mapping ratings above 3 as Positive and below 3 as Negative\n\nactual_scores = filtered_data['Score']\npositiveNegative = actual_scores.map(lambda x: 'Positive' if x>3 else 'Negative')\nfiltered_data['Score'] = positiveNegative","ed0ae3bf":"filtered_data.head(5)","71560dfb":"# Sorting values according to Time for Time Based Slicing\nsorted_values = filtered_data.sort_values('Time',kind = 'quicksort')","b817e0d7":"final = sorted_values.drop_duplicates(subset= { 'UserId', 'ProfileName', 'Time',  'Text'})","71f90ff1":"print('Rows dropped : ',filtered_data.size - final.size)\nprint('Percentage Data remaining after dropping duplicates :',(((final.size * 1.0)\/(filtered_data.size * 1.0) * 100.0)))","66d64166":"# Dropping rows where HelpfulnessNumerator < HelpfulnessDenominator\nfinal = final[final.HelpfulnessDenominator >= final.HelpfulnessNumerator]","cf7b699b":"print('Number of Rows remaining in the Dataset: ',final.size)","f4700766":"# Checking the number of positive and negative reviews\nfinal['Score'].value_counts()","436920ba":"# Data Sampling\nfinal = final.iloc[:5000,:]\nprint(final.shape)\nprint(final['Score'].value_counts())","1bcdc055":"# Function to Remove HTML Tags\ndef cleanhtml(sentence):\n    cleaner = re.compile('<.*?>')\n    cleantext = re.sub(cleaner,\"\",sentence)\n    return cleantext","22507b5e":"# Function to clean punctuations and special characters\n\ndef cleanpunct(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    return  cleaned\n","88234701":"#import nltk\n#nltk.download()","386e95d5":"# Initialize Stop words and PorterStemmer and Lemmetizer\nstop = set(stopwords.words('english'))\nsno = SnowballStemmer('english')\n\n\nprint(stop)\nprint('*' * 100)\nprint(sno.stem('tasty'))","4dbcb937":"# Cleaning HTML and non-Alphanumeric characters from the review text\ni=0\nstr1=' '\nfinal_string=[]\nall_positive_words=[] # store words from +ve reviews here\nall_negative_words=[] # store words from -ve reviews here.\ns=''\nfor sent in final['Text'].values:\n    filtered_sentence=[]\n    #print(sent);\n    sent=cleanhtml(sent) # remove HTMl tags\n    for w in sent.split():\n        for cleaned_words in cleanpunct(w).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n                if(cleaned_words.lower() not in stop):\n                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (final['Score'].values)[i] == 'Positive': \n                        all_positive_words.append(s) #list of all words used to describe positive reviews\n                    if(final['Score'].values)[i] == 'Negative':\n                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n                else:\n                    continue\n            else:\n                continue \n    #print(filtered_sentence)\n    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n    #print(\"***********************************************************************\")\n    \n    final_string.append(str1)\n    i+=1","a8c74dea":"final['CleanedText']=final_string\nfinal.head(5)","7dc53d22":"#Split data into Train and Test Set\nX_Train,X_Test,y_train,y_test = train_test_split(final['CleanedText'],final['Score'],random_state = 0,test_size = 0.3)\n","e38374c0":"# Function to run SVC with GridSearchCV and RandomSearchCV\ndef RunSVC(X_Train,X_Test,y_train,y_test,Search_Type):    \n    lb_make = LabelEncoder()\n    \n    y_train_encoded = lb_make.fit_transform(y_train)\n    y_test_encoded = lb_make.fit_transform(y_test)\n    \n    \n    if (Search_Type == 'grid'):\n        grid_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 100, 1000]}]\n        model = GridSearchCV(SVC(),grid_parameters,cv = 5,scoring = 'f1')\n        model.fit(X_Train,y_train_encoded)\n        print(model.best_estimator_)\n        print('The Score with '+ Search_Type+ 'search CV is: '+ str(model.score(X_Test, y_test_encoded)))\n    elif (Search_Type == 'random'):\n        random_parameters = dict(C=[1, 10, 100, 1000],gamma=[1e-3, 1e-4])  \n        model = RandomizedSearchCV(SVC(),random_parameters,cv = 5,scoring = 'f1',n_jobs= 1)\n        model.fit(X_Train,y_train_encoded)\n        print(model.best_estimator_)\n        print('The Score with '+ Search_Type+ 'search CV is: ' + str(model.score(X_Test, y_test_encoded)))","3d7cc81a":"# BoW Vectorization\n\nvect = CountVectorizer().fit(X_Train)\nX_Train_vectorised = vect.transform(X_Train)\nX_Test_vectorised = vect.transform(X_Test)\n\n\nRunSVC(X_Train_vectorised,X_Test_vectorised,y_train,y_test,'grid')\nRunSVC(X_Train_vectorised,X_Test_vectorised,y_train,y_test,'random')\n\n","ea50e4bd":"# Applying TFIDF\n\nvect_tfidf = TfidfVectorizer(min_df = 5).fit(X_Train)\nX_Train_vectorised = vect_tfidf.transform(X_Train)\nX_Test_vectorised = vect_tfidf.transform(X_Test)\nRunSVC(X_Train_vectorised,X_Test_vectorised,y_train,y_test,'grid')\nRunSVC(X_Train_vectorised,X_Test_vectorised,y_train,y_test,'random')","e71c0e80":"'''print(final['Text'].values[0])\nprint(\"*****************************************************************\")\n'''\nprint(list_of_sent[0])\n","fa2df7cf":"w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=5,size=50, workers=4)    \nwords = list(w2v_model.wv.vocab)\n#print(len(words))","ce09e1d0":"**1. Importing required packages**","81e2dcaa":"# Train your own Word2Vec model using your own text corpus\n\ni=0\nlist_of_sent=[]\nfor sent in X_Train:\n    filtered_sentence=[]\n    sent=cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunct(w).split():\n            if(cleaned_words.isalpha()):    \n                filtered_sentence.append(cleaned_words.lower())\n            else:\n                continue \n    list_of_sent.append(filtered_sentence)\n    ","a8c26fde":"X_Train.head()","cafa915f":"**4.1 Using Bag of Words**","dd11a4f5":"**4.4 Using TF-IDF Weighted Word2Vec**","0652e6a2":"**4.2 Using TF-IDF**","477a6109":"**2. Importing Dataset from database.sqlite and ignoring reviews with Score  = 3 as they represent a neutral view**","6ff0bce3":">    **KNN on Amazon Fine Foods Reviews**\n\n**Objective** - Run KNN Algorithms on Amazon Fine Foods Review Dataset using BoW,TF-IDF,Avg Word2Vec and TF-IDF weighed Word2Vec vectorization methods. Also to report the metrics for each iteration. Time based splitting to be followed.\n\n\n**KNN Algorithms to be used** - KD TREE and BRUTE\n\n**Kaggle Dataset Location** - https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews\/data\n\n","2bdf5ff7":"\n**4.3 Using Average Word2Vec**","6c59ea5c":"**4.  KNN with KD Tree and Brute Force Algorithm**","c2890627":"# TF-IDF weighted Word2Vec\ntfidf_feat = vect_tfidf.get_feature_names() # tfidf words\/col-names\n# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n\ntfidf_sent_vectors = []; # the tfidf-w2v for each sentence\/review is stored in this list\nrow=0;\nfor sent in list_of_sent: # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            # obtain the tf_idfidf of a word in a sentence\/review\n            tfidf = vect_tfidf[row, tfidf_feat.index(word)]\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n        except:\n            pass\n    \n    #print(type(sent_vec))\n    try:\n        sent_vec \/= weight_sum\n    except:\n        pass\n    \n    tfidf_sent_vectors.append(sent_vec)\n    row += 1\n    \n\n    \nX_1, X_test, y_1, y_test = cross_validation.train_test_split(tfidf_sent_vectors, final['Score'], random_state = 0,test_size = 0.3)\n#print('X_train first entry: \\n\\n', X_1[0])\n#print('\\n\\nX_train shape: ', X_1.shape)\n\n# split the train data set into cross validation train and cross validation test\nX_tr, X_cv, y_tr, y_cv = cross_validation.train_test_split(X_1, y_1, test_size=0.3)\n\nrunKNN(X_tr_vectorized,x_cv_vectorized,y_tr,y_cv,'TF-IDF weighted Word2Vec')\n    ","cab03d96":"# Code to read csv file into colaboratory:\n!pip install -U -q PyDrive\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n\n# 1. Authenticate and create the PyDrive client.\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\n#2. Get the file\ndownloaded = drive.CreateFile({'id':'1aeCGYtXU-YcvPMJfM9xxgFy2vooP-Xdk'}) # replace the id with id of file you want to access\ndownloaded.GetContentFile('database.sqlite')  \n\n#3. Read file as panda dataframe\n#import pandas as pd\n#mnist_data = pd.read_csv('xyz.csv') ","4904c8f3":"**3. Data Preprocessing**","2daf5235":"# average Word2Vec\n# compute average word2vec for each review.\nsent_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sent in list_of_sent: # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            pass\n    sent_vec \/= cnt_words\n    sent_vectors.append(sent_vec)\n#print(len(sent_vectors))\n#print(len(sent_vectors[0]))\n\n'''X_1, X_test, y_1, y_test = cross_validation.train_test_split(sent_vectors, final['Score'], random_state = 0,test_size = 0.3)\n#print('X_train first entry: \\n\\n', X_1[0])\n#print('\\n\\nX_train shape: ', X_1.shape)\n\n# split the train data set into cross validation train and cross validation test\nX_tr, X_cv, y_tr, y_cv = cross_validation.train_test_split(X_1, y_1, test_size=0.3)'''\n\n\n#runKNN(X_tr_vectorized,x_cv_vectorized,y_tr,y_cv,'Average Word2Vec')","9b5fe8a4":"#Split data into Train and Test Set\nX_Train,X_Test,y_train,y_test = train_test_split(final['Text'],final['Score'],random_state = 0,test_size = 0.3)"}}