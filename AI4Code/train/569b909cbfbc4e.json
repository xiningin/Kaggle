{"cell_type":{"7a61c3c3":"code","769d89e1":"code","556ec0fc":"code","38d59655":"code","787ecf09":"code","ce4e6a3f":"code","ef072153":"code","cab5a407":"code","d0695174":"code","79241944":"code","66586142":"code","1553caa9":"code","f2042cda":"code","143b3924":"code","045b4964":"code","d8275df7":"code","f89ac699":"code","e74dc1b5":"code","51fc9eb9":"code","53dbad82":"code","0b571446":"code","26392bb0":"markdown","7e6dd296":"markdown","dc23ca5c":"markdown","09f7a323":"markdown","fb1c942e":"markdown","f2ebdcbb":"markdown","07b51b7b":"markdown","854b5989":"markdown"},"source":{"7a61c3c3":"from functools import partial\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport warnings; warnings.filterwarnings('ignore')\nfrom sklearn.metrics import roc_auc_score\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\nfrom sklearn.cluster import KMeans","769d89e1":"def objective(params, pipeline, X_train, y_train, metric):\n    \"\"\"\n    Cross_validation with current hyperparameters\n    \n    params: hyperparameters\n    pipeline: model\n    X_train: features\n    y_train: target\n    \"\"\"    \n    pipeline.set_params(**params)\n    score = cross_val_score(estimator=pipeline, X=X_train, y=y_train, \n                            scoring=metric, cv=4, n_jobs=-1)\n        \n    return   {'loss': -score.mean(), 'params': params, 'status': STATUS_OK}\n# score.mean() with minus because hyperopt can only minimize loss","556ec0fc":"def model_filler(data, columns):\n    '''Divides the data into data with gaps and filled-in data.\n       Next, using hyperopt and XGBoost, \n       it learns on the filled ones and predicts the omissions.'''\n    for co in columns:\n        test_sample = data[data[co].isna()]\n        train_sample = data[data[co].isna() == False]\n        test_sample_features = np.array(test_sample.drop(columns, axis=1))\n        train_sample_target = train_sample[co].values\n        train_sample_features = np.array(train_sample.drop(columns, axis=1))\n        model = xgb.XGBRegressor()\n        trials = Trials()\n        best = fmin(fn=partial(objective, pipeline=model, X_train=train_sample_features, \n                               y_train=train_sample_target, metric='neg_mean_squared_error'),  \n                    space=params,\n                    algo=tpe.suggest,\n                    max_evals=7,\n                    trials=trials,\n                    rstate=np.random.RandomState(1),\n                    show_progressbar=True)\n        xgb_model = xgb.XGBRegressor(**trials.results[0]['params'])\n        xgb_model.fit(train_sample_features, train_sample_target)\n        xgb_prediction = xgb_model.predict(test_sample_features)\n        result = pd.DataFrame({co: xgb_prediction, \n                               'ID': test_sample['ID']})\n        result = result.merge(train_sample[['ID', co]], how='outer')\n        data = data.drop(co, axis = 1)\n        data = data.merge(result, on='ID')\n\n    return data","38d59655":"# load datasets\ntrain_res = pd.read_csv('..\/input\/rnd-42-welcome\/r42_welcome_train_res.csv', sep=';')\ntest_sample = pd.read_csv('..\/input\/rnd-42-welcome\/r42_welcome_test_sample.csv', sep=',')\ntest = pd.read_csv('..\/input\/rnd-42-welcome\/r42_welcome_test\/r42_welcome_test.csv', sep=';')\ntrain = pd.read_csv('..\/input\/rnd-42-welcome\/r42_welcome_train\/r42_welcome_train.csv', sep=';')","787ecf09":"display(train_res.head())\ntrain_res.info()\nprint()\nprint('Empty:', train_res.isna().sum().sum())\nprint('Doubled:', train_res['ID'].duplicated().sum().sum())","ce4e6a3f":"display(test_sample.head())\ntest_sample.info()\nprint()\nprint('Empty:', test_sample.isna().sum().sum())\nprint('Doubled:', test_sample['ID'].duplicated().sum().sum())","ef072153":"display(train.head())\ntrain.info()\nprint()\nprint('Empty:', train.isna().sum().sum())\nprint('Doubled:', train.duplicated().sum().sum())","cab5a407":"display(test.head())\ntest.info()\nprint()\nprint('Empty:', test.isna().sum())\nprint('Doubled:', test.duplicated().sum())","d0695174":"train['DT'] = pd.to_datetime(train['DT'])\ntest['DT'] = pd.to_datetime(test['DT'])\n# \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0433\u043e\u0434\ntrain['DT'].dt.year.value_counts()","79241944":"# train\ntrain['month'] = train['DT'].dt.month\ntrain['day'] = train['DT'].dt.day\ntrain['hour'] = train['DT'].dt.hour\n# remove source column\ntrain = train.drop('DT', axis=1)\n# test\ntest['month'] = test['DT'].dt.month\ntest['day'] = test['DT'].dt.day\ntest['hour'] = test['DT'].dt.hour\ntest = test.drop('DT', axis=1)","66586142":"emply_columns = []\nfor column in test.columns:\n    if True in test[column].isna().value_counts():\n        emply_columns.append(column)\n\nemply_columns","1553caa9":"for column in emply_columns:\n    print(column, train[column].value_counts())","f2042cda":"for column in emply_columns:\n    print(column, train[column].describe())","143b3924":"train_filled = train.copy()\ntest_filled = test.copy()\ntrain_filled['C_3'] = train_filled['C_3'].fillna(0)\ntest_filled['C_3'] = test_filled['C_3'].fillna(0)\nemply_columns.remove('C_3')\nemply_columns","045b4964":"params = {'max_depth' : hp.choice('max_depth', [i for i in range(3, 20)]),\n          'learning_rate': hp.uniform('learning_rate', 0.005, 0.1),\n          'n_estimators' : hp.choice('n_estimators', [i for i in range(100, 2000)]),\n          'gamma': hp.uniform('gamma', 0, 0.5),\n          'subsample': hp.uniform('subsample', 0.5, 1),\n          'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)}\n\ntrain_filled = model_filler(train_filled, emply_columns)\ntest_filled = model_filler(test_filled, emply_columns)","d8275df7":"distortion = []\nK = range(1, 8)\nfor k in K:\n    model = KMeans(n_clusters=k, random_state=12345)\n    model.fit(train_filled)\n    distortion.append(model.inertia_) ","f89ac699":"plt.figure(figsize=(12, 8))\nplt.plot(K, distortion, 'bx-')\nplt.xlabel ('Number of clusters')\nplt.ylabel('Target function')\nplt.show() ","e74dc1b5":"model = KMeans(n_clusters=4, random_state=12345)\ntrain_filled['claster'] = model.fit_predict(train_filled)\ntest_filled['claster'] = model.predict(test_filled)","51fc9eb9":"model = xgb.XGBClassifier()\n\ntrials = Trials()\nbest = fmin(   \n            fn=partial(objective, \n                       pipeline=model, \n                       X_train=train_filled, \n                       y_train=train_res['Y'], \n                       metric='roc_auc'),  \n            space=params,\n            algo=tpe.suggest,\n            max_evals=25,\n            trials=trials,\n            rstate=np.random.RandomState(1),\n            show_progressbar=True\n        )","53dbad82":"xgb_model = xgb.XGBClassifier(**trials.results[0]['params'])\nxgb_model.fit(train_filled, train_res['Y'])\nxgb_prediction = xgb_model.predict(test_filled)","0b571446":"submission = pd.DataFrame({'ID': test_sample['ID'],\n                           'Y': xgb_prediction})\nsubmission.to_csv('predict.csv', index=False)","26392bb0":"## Filling by logical and modeling.\nWe have already seen that C_3 has 4 options: None, 1, 2, 3. I think we could try to change None to 0.","7e6dd296":"## First look at the data","dc23ca5c":"Second step is fill gups by xgb.","09f7a323":"We can see that we have data for one year, and that this is a useless feature.","fb1c942e":"## Change type for date and creating new features.","f2ebdcbb":"UPD 03.25.21\n## Attempt to use the cluster to improve the quality.\n","07b51b7b":"## Let's see on empty values","854b5989":"## Searching final hyperparameter"}}