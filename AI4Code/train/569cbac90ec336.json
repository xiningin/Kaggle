{"cell_type":{"5ae44db9":"code","2e5417d2":"code","c9132a7d":"code","a7ae0820":"code","2a10ee14":"code","95eace67":"code","6f1f79dd":"code","90356aad":"code","4392f21a":"code","a142ba4c":"code","3c4cc91e":"code","aad921db":"code","49abbabd":"code","054e8d0d":"code","09cb5824":"code","2b2b6221":"code","249bd257":"code","a1b6db30":"code","55149613":"code","4d8f1e8f":"code","ed29469d":"code","9deb6dc6":"code","4ce19da6":"code","fcc89243":"markdown","2c30ae02":"markdown","86b5bdb3":"markdown","43f6a3fc":"markdown","04645761":"markdown","bb7dc5b8":"markdown","486d1769":"markdown","9bd3fb28":"markdown","8110b0d9":"markdown","4c3e61d6":"markdown","c79e2b00":"markdown","ef76d50c":"markdown","5e95a898":"markdown","dfc61f2c":"markdown","3a42446e":"markdown","d93b0fe7":"markdown","accf944b":"markdown","96be2e90":"markdown","76f54b21":"markdown","634611cb":"markdown"},"source":{"5ae44db9":"!pip install dabl\nimport dabl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport warnings\nwarnings.filterwarnings('ignore')\ncolorMap = sns.light_palette(\"blue\", as_cmap=True)","2e5417d2":"!wc -l ..\/input\/riiid-test-answer-prediction\/train.csv","c9132a7d":"import pandas as pd\ntrain_data = pd.read_csv('..\/input\/riiid-test-answer-prediction\/train.csv', nrows=100000)","a7ae0820":"dabl.detect_types(train_data)","2a10ee14":"train_data.describe().style.background_gradient(cmap=colorMap)","95eace67":"for col in train_data:\n    print(col,len(train_data[col].unique()))","6f1f79dd":"train_data.isnull().sum()","90356aad":"pd.set_option('display.max_columns', None)\npivot_table = pd.pivot_table(train_data, index=['user_answer']).style.background_gradient(cmap=colorMap)\npivot_table","4392f21a":"pivot_table = pd.pivot_table(train_data, index=['answered_correctly']).style.background_gradient(cmap=colorMap)\npivot_table","a142ba4c":"pivot_table = pd.pivot_table(train_data, index=['prior_question_had_explanation']).style.background_gradient(cmap=colorMap)\npivot_table","3c4cc91e":"dabl.plot(train_data, target_col=\"answered_correctly\")","aad921db":"plt.figure(figsize = (15,6))\n# set 300 bins, one bin for each of the 300 time values\nax = sns.distplot(train_data['prior_question_elapsed_time'], kde=False, bins=300)\nax.set_xlim(0,75000)\nax.set_xlabel(\"Histogram of 'prior_question_elapsed_time'\",fontsize=18)\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.show();","49abbabd":"print(\"The mean prior_question_elapsed_time is: %.1f\" % train_data['prior_question_elapsed_time'].mean())\nprint(\"The modal value is                     :\",train_data['prior_question_elapsed_time'].mode().squeeze())","054e8d0d":"print(\"Skew of prior_question_elapsed_time is:      %.2f\" %train_data['prior_question_elapsed_time'].skew() )\nprint(\"Kurtosis of prior_question_elapsed_time is: %.2f\" %train_data['prior_question_elapsed_time'].kurtosis() )","09cb5824":"plt.figure(figsize = (15,6))\nax = sns.distplot(train_data['task_container_id'], kde=False, bins=563)\nax.set_xlim(0,1000)\nax.set_xlabel(\"Histogram of 'task_container_id'\",fontsize=18)\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.show();","2b2b6221":"plt.figure(figsize = (15,6))\nax = sns.distplot(train_data['content_id'], kde=False, bins=341) # why 341? Because it is a factor of 32736 ;-)\nax.set_xlim(0,14000)\nax.set_xlabel(\"Histogram of 'content_id'\",fontsize=18)\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.show();","249bd257":"train_data.corr().style.background_gradient(cmap='Oranges')","a1b6db30":"how_good = train_data[train_data['answered_correctly'] != -1].groupby('user_id').mean()","55149613":"plt.figure(figsize = (15,6))\nax = sns.distplot(how_good['answered_correctly'], color='darkcyan',bins=50)\nax.set_xlabel(\"Plot of the ratio of correct to incorrect answers by user\",fontsize=18)\nax.set_xlim(0,1)\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.show();","4d8f1e8f":"print(\"The best score is: %.1f\" % (how_good['answered_correctly'].max()*100), \"%\")\nprint(\"The mean score is:  %.1f\" % (how_good['answered_correctly'].mean()*100), \"%\")","ed29469d":"fig_1 = px.scatter(how_good, x=how_good['prior_question_elapsed_time'], y=how_good['answered_correctly'], \n                   trendline=\"ols\", marginal_y=\"violin\", marginal_x=\"box\",\n                   title=(\"Scatter plot of results with respect to the prior question elapsed time\"))\nfig_1.show()","9deb6dc6":"X_train       = train_data.drop(\"answered_correctly\",axis=1)\nX_train       = X_train.fillna(X_train.mean())\ny_train       = train_data[\"answered_correctly\"]\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(max_features='auto', \n                                    min_samples_leaf=10)\nclassifier.fit(X_train, y_train)","4ce19da6":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm_import = PermutationImportance(classifier, random_state=1).fit(X_train, y_train)\n# visualize the results\neli5.show_weights(perm_import, top=None, feature_names = X_train.columns.tolist())","fcc89243":"## Unique values: How many unique values are there in the 100K rows?<a class=\"anchor\" id=\"unique\"><\/a>","2c30ae02":"I do not see any informative or useful correlations. There is a correlation between `task_container_id` (the Id code for the batch of questions or lectures) and `timestamp` which is the the time between this user interaction and the first event from that user. There is also a correlation between `content_type_id` (0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture) and `content_id` the ID code for the user interaction, but I am not sure this is useable.\n\n# Let's see how good the students are...<a class=\"anchor\" id=\"how_good\"><\/a>\nWe shall do this by grouping each student (by their `user_id`) and calculating the ratio of correct to incorrect answers. We have `lectures`, which correspond to `-1`, `0` for presumably `incorrect` answers, and `1`for a `correct` answer. So first we shall remove the lectures from the data, then group each `user_id`, then finally calculate the mean of their scores (`answered_correctly`):","86b5bdb3":"We can see that other than the `-1` row where thare was a lecture and no question was asked, the variables seem to be pretty much the same for each of the user answers. That said, if you did not know the answer to a particular question then choose `2` as it, albeit slimly, is the most frequent correct answer ;-)\n\n* `answered_correctly` is if the user responded correctly, which is either `0` or `1`, and is `-1` for no answer (i.e. watching lectures)","43f6a3fc":"## Create a Pearson correlation matrix<a class=\"anchor\" id=\"pearson\"><\/a>","04645761":"From this we can see that there are four **continuous** features:\n<font color=purple>\n* Time features:\n * `timestamp` which is the time between this user interaction and the first event from that user\n * `prior_question_elapsed_time` is the average time a user took to solve each question in the previous bundle. [I.e. if three questions are in a bundle, they share the same `prior_question_elapsed_time` and the value is the total time the user took to solve all three questions, divided by three.](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/189768)\n* Identification features:\n * `content_id` ID code for the user interaction \n * `task_container_id` ID code for the batch of questions or lectures\n<br><\/font>\n\nNote that `task_container_id` does not increase monotonically for each student, as was mentioned by [Gunes Evitan](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/189351) and [Zhao Chow](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/189465), but rather it seems to reflect the order in which the student starts the tasks.\n\nThere is one **low cardinality integer** feature, namely\n<font color=darkgreen>\n* `user_id` which is the ID code for the user.<br>\n<br><\/font>\n\nThere are three **categorical** features:<br>\n<font color=darkred>\nOur target feature:\n* `answered_correctly` if the user responded correctly, `0` or `1`, and `-1` for no answer (i.e. watching lectures)\n    \nand\n* `user_answer` the user's answer to the question: `0`, `1`, `2` or `3`, and `-1` for no answer (i.e. watching lectures)\n* `content_type_id` which is either:\n * `0` if the event was a question being posed to the user, \n * `1` if the event was the user watching a lecture\n<br><\/font>\n\nThere is one **Boolean** feature, namely:\n<font color=darkyellow>\n* `prior_question_had_explanation` (`True`\/`False`) which is whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between.\n<br><\/font>\n\n\n\n## pandas describe<a class=\"anchor\" id=\"pandas_describe\"><\/a>\n\nSome simple escriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding `NaN` values.","bb7dc5b8":"There seems to be a large class imbalance (maybe the [imbalanced-learn](https:\/\/github.com\/scikit-learn-contrib\/imbalanced-learn) package will come in useful?). The very small blue `-1` class corresponds to lectures, where no questions were asked, and can probabbly be excluded.\n\n## Histograms of the continuous features `prior_question_elapsed_time`, `task_container_id` and `content_id`:<a class=\"anchor\" id=\"histograms\"><\/a>\n\n`prior_question_elapsed_time` is the average time a user took to solve each question in the previous bundle. I.e. if three questions are in a bundle, they share the same `prior_question_elapsed_time` and the value is the total time the user took to solve all three questions, divided by three.","486d1769":"# What data types do we have?<a class=\"anchor\" id=\"data_types\"><\/a>","9bd3fb28":"## Null values: How many null values are there in the 100K rows?<a class=\"anchor\" id=\"null\"><\/a>","8110b0d9":"#### Notes: \n* Be careful how one uses the `answered_correctly` data as it is a potential source of leakage; see the discussion [\"Beware of target leakage\"](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/189437) by [Max Halford](https:\/\/www.kaggle.com\/maxhalford) for more details. \n* It has been found that some questions are repeated, and when this happens that students are more likely to answer correctly (!). See this excellent notebook [\"Statistical Analysis on Repeated questions\"](https:\/\/www.kaggle.com\/aravindpadman\/riiid-statistical-analysis-on-repeated-questions) by [Aravind Padmasanan](https:\/\/www.kaggle.com\/aravindpadman).\n* Somewhat unsurprisingly, students who listened to the lectures before answering the questions have a higher rate of `answered_correctly`, as was shown in the notebook [\"Utilize Lecture In Your Model & Before Answering\"](https:\/\/www.kaggle.com\/jsylas\/utilize-lecture-in-your-model-before-answering) by [Sylas](https:\/\/www.kaggle.com\/jsylas).","4c3e61d6":"Let us take a look at `task_container_id` and `content_id`:","c79e2b00":"There seems to be almost no correlation between the users score and how long it took the user to answer their previous question bundle.","ef76d50c":"# Visualisations<a class=\"anchor\" id=\"Visualisation\"><\/a>","5e95a898":"the biggest difference between those who answered correctly and those who did not seems to show in `task_container_id` which is the Id code for the batch of questions or lectures.\n\n* `prior_question_had_explanation` which is whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between","dfc61f2c":"We can see that there are null values in the columns `prior_question_elapsed_time` and `prior_question_had_explanation` that will need cleaning up.\n## Pivot tables of the categorical fearures `user_answer`, `answered_correctly` and the Boolean `prior_question_had_explanation`<a class=\"anchor\" id=\"pivot\"><\/a>\n\n* `user_answer` \"is the user's answer to the question, if any. Read -1 as null, for lectures.\"","3a42446e":"# Very quick Permutation Importance with our old friend the Random Forest<a class=\"anchor\" id=\"PermutationImportance\"><\/a>\nWe shall now perform a simple [permutation importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance) calculation, a basic way of seeing which features *may* be important. For a list of ideas regarding feature engineering and the Riiid dataset see the discussion [\"Any feature engineering ideas?\"](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/192137) by [Pavel Prokhorov](https:\/\/www.kaggle.com\/pavelvpster).","d93b0fe7":"We can see a distribution of times. (Note: It has not been explicitly stated what the units of time are, [but it has been suggested that it is milliseconds](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/189763)). \n\nLet us calculate the [skew](https:\/\/en.wikipedia.org\/wiki\/Skewness) and [kurtosis](https:\/\/en.wikipedia.org\/wiki\/Kurtosis):","accf944b":"We can see that `task_container_id` was deemed to be the \"most\" important. And the least important is `content_type_id`, as was indicated at the start when `dabl.detect_types` went as fas as to call it useless...\n\nNote: \n\n> Features that are deemed of low importance for a bad model (low cross-validation score) could be very important for a good model. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model. (Source: [scikit-learn permutation importance](https:\/\/scikit-learn.org\/stable\/modules\/permutation_importance.html)).\n\nAnother option for performing feature importance is by using the more sophisticated [Boruta-SHAP package](https:\/\/www.kaggle.com\/carlmcbrideellis\/feature-selection-using-the-borutashap-package). However, be advised that it does take quite a while to execute.\n\n# Useful links<a class=\"anchor\" id=\"useful_links\"><\/a>\n\n* [EdNet](https:\/\/github.com\/riiid\/ednet) on GitHub\n* [Chol *et al.* \"EdNet: A Large-Scale Hierarchical Dataset in Education\", arXiv:1912.03072\n](https:\/\/arxiv.org\/pdf\/1912.03072.pdf) (pdf)\n* [\"Papers on Knowledge Tracing and Education\"](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/188911) an excellent kaggle post by [charliec](https:\/\/www.kaggle.com\/crained) with links to many interesting papers.\n \n# Thanks for Riiiding this simple EDA!\n![image.png](attachment:image.png)","96be2e90":"Indeed we can see that it has 101,230,333 rows.\n\nThere are suggestions regarding how to load all of `train.csv` into a kaggle notebook in a [post by ryati](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/189499) in which they propose trying:\n* [Feather binary file format for storing data frames](https:\/\/github.com\/wesm\/feather)\n* [Apache Parquet storage format](https:\/\/parquet.apache.org\/)\n\nOther ideas to look at are\n* [Dask](https:\/\/docs.dask.org\/en\/latest\/): see the [notebook by  Rajnish Chauhan](https:\/\/www.kaggle.com\/rajnishe\/riiid-read-full-file-dask-api)\n* [Datatable](https:\/\/datatable.readthedocs.io\/en\/latest\/): see the [notebook by Vopani](https:\/\/www.kaggle.com\/rohanrao\/riiid-with-blazing-fast-rid) with the `.jay` binary format\n* [cuDF - GPU DataFrames](https:\/\/github.com\/rapidsai\/cudf) by RAPIDS: see the [notebook by ONODERA](https:\/\/www.kaggle.com\/onodera\/riiid-read-csv-in-cudf)\n\nVopani has also written an excellent [tutorial on reading large datasets](https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets).\n\nIt has been [suggested by tkm2261](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/190270) that [BigQuery](https:\/\/cloud.google.com\/bigquery) could come in useful in this competition, and it has been confirmed by the kaggle staff that the use of BigQuery is within the competition rules.\n\n\nThat said, for this little notebook we shall just read in the first 100K rows (which is less than 0.1% of the data) using vanilla pandas:","76f54b21":"# The train.csv file is big: How to read it in<a class=\"anchor\" id=\"train_file\"><\/a>\nThe `train.csv` is large: 5.45G (in fact so large it will not fit in the kaggle notebook memory). Let us see just how many rows it has:","634611cb":"![image.png](attachment:image.png)\n# Riiid!: An EDA of the train.csv file, and then a feature importance via the Random Forest\nWe shall undertake a very quick data exploration of the `train.csv` file in the [Riiid! Answer Correctness Prediction competition](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction). We shall then calculate the importance of the features using the random forest classifier.\n\n### Background reading\nBefore starting any project, background reading is essential. One should take a look at the paper [\"EdNet: A Large-Scale Hierarchical Dataset in Education\"](https:\/\/arxiv.org\/pdf\/1912.03072.pdf) as well as the excellent kaggle post by [charliec](https:\/\/www.kaggle.com\/crained) titled [\"Papers on Knowledge Tracing and Education\"](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/188911).\n## Contents\n* [The train.csv file](#train_file)\n* [What data types do we have?](#data_types)\n* [pandas describe](#pandas_describe)\n* [Unique values: How many unique values are there in the 100K rows?](#unique)\n* [Null values: How many null values are there in the 100K rows?](#null)\n* [Pivot tables of the categorical fearures](#pivot)\n* [Visualisations](#Visualisation)\n* [Histograms of the continuous features](#histograms)\n* [Create a Pearson correlation matrix](#pearson)\n* [Let's see how good the students are...](#how_good)\n* [Very quick Permutation Importance with our old friend the Random Forest](#PermutationImportance)\n* [Useful links](#useful_links)"}}