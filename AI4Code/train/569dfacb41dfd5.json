{"cell_type":{"542fd10f":"code","c1ae0973":"code","3477769a":"code","2ab459fe":"code","91cd1a3e":"code","4a03958d":"code","dd8d0310":"code","f868405b":"markdown","a0344524":"markdown","ce669c3e":"markdown","1e1b482a":"markdown","e37154c0":"markdown","79a755fa":"markdown","79db38f4":"markdown"},"source":{"542fd10f":"# This kernel is designed to produce the predicted values for the new dataset\n# Firstly, install some Vietnamese language toolkits and models\n\n# Perform all of this on Ubuntu terminal\n\n# Install 'vncorenlp', a Vietnamese language toolkit\n!pip3 install vncorenlp\n!mkdir -p vncorenlp\/models\/wordsegmenter\n!wget https:\/\/raw.githubusercontent.com\/vncorenlp\/VnCoreNLP\/master\/VnCoreNLP-1.1.1.jar\n!wget https:\/\/raw.githubusercontent.com\/vncorenlp\/VnCoreNLP\/master\/models\/wordsegmenter\/vi-vocab\n!wget https:\/\/raw.githubusercontent.com\/vncorenlp\/VnCoreNLP\/master\/models\/wordsegmenter\/wordsegmenter.rdr\n!mv VnCoreNLP-1.1.1.jar vncorenlp\/ \n!mv vi-vocab vncorenlp\/models\/wordsegmenter\/\n!mv wordsegmenter.rdr vncorenlp\/models\/wordsegmenter\/\n\n# Install the PhoBERT base model\n!wget https:\/\/public.vinai.io\/PhoBERT_base_transformers.tar.gz\n!tar -xzvf PhoBERT_base_transformers.tar.gz\n\n# Install fastBPE, fairseq\n!pip install fastBPE\n!pip uninstall typing -y\n!pip install fairseq\n\n########################################################################################################\n# This kernel is designed to produce the predicted values for the new dataset\n# Import necessary packages\nimport numpy as np\nimport pandas as pd\nimport os\nimport math\nfrom tqdm.notebook import tqdm\n\nfrom vncorenlp import VnCoreNLP\nrdrsegmenter = VnCoreNLP(\".\/vncorenlp\/VnCoreNLP-1.1.1.jar\", annotators = \"wseg\", max_heap_size = '-Xmx500m')\n\nfrom fairseq.data.encoders.fastbpe import fastBPE\nfrom fairseq.data import Dictionary\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n\nfrom catboost import Pool, CatBoostClassifier\n\nfrom hyperopt import hp, fmin, atpe, tpe, Trials\n\nimport argparse\n\nfrom transformers import RobertaConfig\nfrom transformers import RobertaModel\n\n########################################################################################################\n# Now, we prepare the data and embed each sample to the respective matrices\n\n# Load PhoBERT model\nconfig = RobertaConfig.from_pretrained(\n    \"PhoBERT_base_transformers\/config.json\"\n)\nphobert = RobertaModel.from_pretrained(\n    \"PhoBERT_base_transformers\/model.bin\",\n    config = config\n)\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--bpe-codes', \n    default = \".\/PhoBERT_base_transformers\/bpe.codes\",\n    required = False,\n    type = str,\n    help = 'path to fastBPE BPE'\n)\nargs, unknown = parser.parse_known_args()\nbpe = fastBPE(args)\n\n# Load the dictionary\nvocab = Dictionary()\nvocab.add_from_file(\".\/PhoBERT_base_transformers\/dict.txt\")\n\n########################################################################################################\n# CHANGE PATH HERE, \ndata_dir = r'..\/input\/isods-2020\/testing_data_sentiment'   # Where the testing data set is located\ntest_df = pd.read_csv(os.path.join(data_dir, 'testing_data_sentiment.csv'))\n########################################################################################################\n\n# Notice that, the test_df dataset must have columns named 'question' and 'answer'\n\n# Now, prepare data before feeding to the PhoBERT model\n\n# PhoBERT Tokenizer\ndef Pho_tokenizer(text):\n    # Firstly, split the text into sentences and join them by the dot\n    text = text.strip()\n    \n    # Secondly, tokenize\n    text = rdrsegmenter.tokenize(text)\n    text = ' '.join([' '.join(x) for x in text])\n    return text\n\n# Now, apply the method to all samples in the testing dataset\nprint('Tokenize...')\ntest_df['tokenized_question'] = list(map(Pho_tokenizer, test_df.question))\ntest_df['tokenized_answer'] = list(map(Pho_tokenizer, test_df.answer))\nprint('Tokenize: Done')","c1ae0973":"# Notice that, the test_df dataset must have columns named 'question' and 'answer'\n\n# Now, prepare data before feeding to the PhoBERT model\n\n# PhoBERT Tokenizer\ndef Pho_tokenizer(text):\n    # Firstly, split the text into sentences and join them by the dot\n    text = text.strip()\n    \n    # Secondly, tokenize\n    text = rdrsegmenter.tokenize(text)\n    text = ' '.join([' '.join(x) for x in text])\n    return text\n\n# Now, apply the method to all samples in the testing dataset\nprint('Tokenize...')\ntest_df['tokenized_question'] = list(map(Pho_tokenizer, test_df.question))\ntest_df['tokenized_answer'] = list(map(Pho_tokenizer, test_df.answer))\nprint('Tokenize: Done')","3477769a":"# Function to map each sample into matrices\ndef QA2mat(dataset, max_len = 256, masking = False):\n    \n    questions = []\n    answers = []\n    \n    # For each sample in the dataset, we have one tokenized question and one tokenized answer\n    # Then, for each sentence in the question or answer\n    print('Encoding...')\n    for i in tqdm(range(dataset.shape[0])):\n        q = dataset.tokenized_question.values[i].split(' | | | ')\n        a = dataset.tokenized_answer.values[i].split(' | | | ')\n\n        sub_q = []\n        sub_a = []\n\n        for m in q:\n            # Encoding\n            subwords_q = '<s> ' + bpe.encode(m) + ' <\/s>'\n            encoded_sent_q = vocab.encode_line(subwords_q, append_eos = True, add_if_not_exist = False).long().tolist()\n            sub_q.append(encoded_sent_q)\n\n        for n in a:\n            subwords_a = '<s> ' + bpe.encode(n) + ' <\/s>'\n            encoded_sent_a = vocab.encode_line(subwords_a, append_eos = True, add_if_not_exist = False).long().tolist()\n            sub_a.append(encoded_sent_a)\n\n        questions.append(sub_q)\n        answers.append(sub_a)\n    \n    # 'questions' and 'answers' are 2 lists of lists, \n    # each element of 'questions' (or 'answers') contains the encoded sentences of that question (or answer)\n    \n    # Next, padding each sentence within one question (or answer)\n    from tensorflow.keras.preprocessing.sequence import pad_sequences as pad_sequence\n    pad_que = []\n    pad_ans = []\n    \n    print('Padding...')\n    for i in tqdm(range(len(questions))):\n        # Padding\n        pad_que.append(pad_sequence(questions[i], maxlen = max_len, dtype = \"long\", value = 0, truncating = \"post\", padding = \"post\"))  # \n        pad_ans.append(pad_sequence(answers[i], maxlen = max_len, dtype = \"long\", value = 0, truncating = \"post\", padding = \"post\"))  # \n    \n    if masking == True:\n        mask_que = []\n        for sample in pad_que:\n            mask_sample = []\n            for sent in sample:\n                mask_q = [int(token_id > 0) for token_id in sent]\n                mask_sample.append(mask_q)\n            mask_que.append(mask_sample)\n\n        mask_ans = []\n        for sample in pad_ans:\n            mask_sample = []\n            for sent in sample:\n                mask_a = [int(token_id > 0) for token_id in sent]\n                mask_sample.append(mask_a)\n            mask_ans.append(mask_sample)\n    \n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    phobert.to(device)\n    \n    print('Embedding...')\n    with torch.no_grad():\n        output = []\n        for i in tqdm(range(len(pad_que))):\n            output_ques = phobert(input_ids = torch.tensor(pad_que[i]).to(device))[1]\n            output_ans = phobert(input_ids = torch.tensor(pad_ans[i]).to(device))[1]\n            output.append((output_ques @ output_ans.t()).cpu())\n    \n    return(output)\n\n# Import training embeddings\ntrain_embed = torch.load('..\/input\/embeddings\/train_embed.pt', map_location = torch.device('cpu'))\n\n# Convert to the embeddings for the testing set\ntest_embed = QA2mat(test_df[['tokenized_question', 'tokenized_answer']])\n\n# Load the training labels\ntrain_labels = np.load('..\/input\/embeddings\/train_labels.npy')","2ab459fe":"# Transform the matrix embeddings of testing samples into vectors\ndef embedding_transform(embed):\n    flatten_embed = []\n    for text in tqdm(embed):\n        flatten_embed.append(text.flatten())\n    return flatten_embed\n\n# Transform the embeddings and padding (and cutting...)\nmax_len = 512\ntrain_embed = embedding_transform(train_embed)\ntest_embed = embedding_transform(test_embed)\n\ntrain_embed = pad_sequence(train_embed, batch_first = True)[:,:max_len].numpy()\ntest_embed = pad_sequence(test_embed, batch_first = True)[:,:max_len].numpy()","91cd1a3e":"# Tuning parameters\ndef optimise(params):\n    # Define the device\n    device = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n    \n    pred_y = np.zeros(train_labels.shape)\n    print('=' * 50)\n    print('With the set of parameters')\n    print(params)\n    \n    # Split the data into folds\n    for n, (tr, te) in enumerate(KFold(n_splits = 5, \n                                       random_state = 0, \n                                       shuffle = True).split(train_embed, train_labels)):\n        print('Training fold:', n)\n        print('-' * 50)\n        x_tr, x_val = train_embed[tr], train_embed[te]\n        y_tr, y_val = train_labels[tr], train_labels[te]\n        \n        _trn = Pool(x_tr, label = y_tr)\n        _val = Pool(x_val, label = y_val)\n        \n        regressor = CatBoostClassifier(loss_function = \"Logloss\",\n                                       eval_metric = \"AUC\",\n                                       task_type = device,\n                                       grow_policy = 'Lossguide',\n                                       iterations = params['iter'],\n                                       l2_leaf_reg = params['l2_leaf'],\n                                       max_leaves = params['max_leaves'],\n                                       random_seed = 123456,\n                                       od_type = \"Iter\",\n                                       depth = params['max_depth'],\n                                       early_stopping_rounds = 15000,\n                                       border_count = params['border_count'],\n                                       verbose = 500)\n        # Fit and validate\n        fit_model = regressor.fit(_trn, \n                                  eval_set = _val,\n                                  use_best_model = True)\n        # Predict out-of-sample\n        pred_y[te] += fit_model.predict(x_val)\n        \n    print('The accuracy is: ', accuracy_score(pred_y, train_labels))\n    return 1-accuracy_score(pred_y, train_labels)","4a03958d":"param_space = {'iter': hp.choice('iter', [1000, 2000, 3000]), \n               'l2_leaf': hp.choice('l2_leaf', [1e-3, 1e-1, 1, 5]),\n               'max_leaves': hp.choice('max_leaves', [5, 10, 15, 20]),\n               'max_depth': hp.choice('max_depth', [5, 10, 15]),\n               'border_count': hp.choice('border_count', [32, 128, 256])\n              }\n\ntrials = Trials()\n\nprint('Optimizing hyperparameters...')\nhopt = fmin(fn = optimise,\n            space = param_space,\n            algo = tpe.suggest, \n            max_evals = 15, \n            timeout = 8.9 * 60 * 60, \n            trials = trials,\n           )","dd8d0310":"# Optimal hyperparameters (using Bayesian hyperparameter tuning technique)\nprint(hopt)","f868405b":"# Hyperparameter tuning","a0344524":"* Feature generation","ce669c3e":"* Objective function","1e1b482a":"# Prepare the dataset","e37154c0":"* Tuning","79a755fa":"* Tokenize the test dataset","79db38f4":"* Using PhoBERT to encode each (question-answer) pair to a pair of vectors"}}