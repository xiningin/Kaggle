{"cell_type":{"2ce9a56b":"code","fd6d3997":"code","86fca5a5":"code","01c667d2":"code","43e18125":"code","98fffab5":"code","77647b82":"code","872384b1":"code","12457633":"code","d004958b":"code","809e8010":"code","2020ac84":"code","2c5992bf":"code","d714c7cd":"code","7c88c23f":"code","58eb6c98":"code","e5e8fa0c":"code","5b6a8234":"code","a66ddc82":"code","e98996f6":"code","95f1882e":"code","206576c1":"code","32ffe359":"code","add99983":"code","02efe5e0":"code","83dea9cc":"code","59945ecb":"code","64a684f3":"code","686dd39b":"code","718f1849":"code","4cc8a1cc":"code","835064cc":"code","324cda6f":"code","f9f7546d":"code","4950d495":"markdown","f60973e8":"markdown","26ec32a7":"markdown","5587647a":"markdown","34e809a9":"markdown","0b3f4634":"markdown","27efec14":"markdown","fb98f535":"markdown","7a1b9380":"markdown","3b3ccacb":"markdown","5ef3f1a0":"markdown","f44f8baa":"markdown","acebb95c":"markdown","908a06bc":"markdown","4010c40d":"markdown","b12ddf5d":"markdown","8516fbaf":"markdown","24933967":"markdown","855a360e":"markdown","12ea1980":"markdown","48486a56":"markdown","3461247c":"markdown","c19e13c2":"markdown","03417522":"markdown","130c4003":"markdown","fbb31c2a":"markdown","b6172110":"markdown","adf975b6":"markdown","6e0de14f":"markdown","a618186d":"markdown","6af0af79":"markdown","ad2cbde3":"markdown","ffbeab4d":"markdown","d1a669a8":"markdown","8a4f0295":"markdown","a254762d":"markdown","735a2c21":"markdown","7aece421":"markdown","8ff3e331":"markdown","0e59cf7f":"markdown","051d2e37":"markdown","510cbdc6":"markdown","6559ac62":"markdown","942a71d4":"markdown","20b12df3":"markdown","1ca1862a":"markdown"},"source":{"2ce9a56b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd6d3997":"#Visualization\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nplt.style.use('dark_background')\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n#Data Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n#Basic Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport time\n\n#Metrics (Computation)\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import GridSearchCV\n\n#Boosting Algorithms\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\n#Neural Network Model\nfrom sklearn.neural_network import MLPClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","86fca5a5":"#data=pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/o2Saturation.csv\")\ndata1=pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\ndata1.head()","01c667d2":"display(data1.describe())\ndisplay(data1.info())","43e18125":"#Output variable\nprint(data1['output'].value_counts())\nfig = plt.figure(figsize = (10,6))\nsns.countplot(\"output\", data=data1, palette='flare')\nplt.show()\n\nax=px.pie(data1, names= \"output\" ,template= \"plotly_dark\",title=\"Output\",hole=0.8)\nax.show()","98fffab5":"#Classification columns\nclass_cols=[\"sex\",\"output\",'cp',\"fbs\",\"exng\",\"restecg\",\"thall\",\"caa\",\"slp\"]\nclass_data=data1[class_cols]\n\n#Continuous columns\ncontinuous_cols=[\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\ncontinuous_data=data1[continuous_cols]","77647b82":"#Divide the data based on sex variable\nX=data1[data1[\"sex\"]==1].reset_index()  \nY=data1[data1[\"sex\"]==0].reset_index()   \ndisplay(X.head(),Y.head())","872384b1":"ax= px.pie(data1, names= \"sex\",template= \"plotly_dark\",title= \"Gender distribution\",hole= 0.5)\nax.show()","12457633":"print(\"Percent of '1' at high risk of heart attack = {} %\" .format(round((len(X[X[\"output\"]==1])\/len(X)*100),2)))\nprint(\"'1' average high-risk age = {} yrs\\n\" .format(round(X[X[\"output\"]==1][\"age\"].mean())))\n\nprint(\"Percent of '0' at high risk of heart attack = {} %\" .format(round((len(Y[Y[\"output\"]==1])\/len(Y)*100),2)))\nprint(\"Average high-risk age = {} yrs\" .format(round(Y[Y[\"output\"]==1][\"age\"].mean())))","d004958b":"print(\"Female Value Counts: \\n{}\".format(X['output'].value_counts()))\nprint(\"Male Value Counts: \\n{}\".format(Y['output'].value_counts()))\n\nfig, ax1 = plt.subplots(1,2, figsize=(20,6))\nplt.suptitle(\"Female                                                                                                                      Male\")\nsns.countplot(\"output\", data=X, palette='gist_heat',ax=ax1[0])\nsns.countplot(\"output\", data=Y, palette='gist_heat',ax=ax1[1])\nfig.show()","809e8010":"fig=go.Figure()\nfig.add_trace(go.Box(y=Y[\"age\"],name=\"Male\",marker_color=\"blue\",boxpoints=\"all\",whiskerwidth=0.3))\nfig.add_trace(go.Box(y=X[\"age\"],name=\"Female\",marker_color=\"#e75480\",boxpoints=\"all\",whiskerwidth=0.3))\nfig.update_layout(template=\"plotly_dark\",title=\"Age Distribution\",height=600)\nfig.show()","2020ac84":"class_cols=[\"output\",'cp',\"fbs\",\"exng\",\"restecg\",\"thall\",\"caa\",\"slp\"]\nfor col in class_cols:\n    ax= px.sunburst(data1, names= col,path= [\"sex\",col],template= \"plotly_dark\", title= \"{} => Based on Sex\".format(col))\n    ax.show()","2c5992bf":"#Pie plots \nfor col in class_cols[2:]:\n    ax=px.pie(data1, names= col ,template= \"plotly_dark\",title=col,hole=0.7)\n    ax.show()\n    \n#Swarm Plots\nfor col in class_cols[2:]:\n    sns.catplot(kind=\"swarm\", data=data1, x=col, y=\"age\", palette=\"inferno\", hue=\"sex\")\nplt.show()\n    \n# fig, ax1 = plt.subplots(4,2, figsize=(15,22.5))\n# k = 0\n# for i in range(4):\n#     for j in range(2):\n#         sns.boxplot(data=data1,x=class_cols[k],y=\"age\",palette=\"autumn\",hue=\"sex\",saturation=1,ax=ax1[i][j])\n#         k+=1\n# plt.tight_layout()\n# plt.show()","d714c7cd":"columns=[\"age\",\"cp\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\nfig, ax1 = plt.subplots(3,2, figsize=(20,20))\nk = 0\nfor i in range(3):\n    for j in range(2):\n            sns.distplot(data1[columns[k]], ax = ax1[i][j], color = 'red')\n            k += 1\nplt.show()","7c88c23f":"#Distribution plots\nplt.figure(figsize=(20,6))\nsns.distplot(data1[\"age\"],color=\"green\",bins=\"auto\")\nplt.title(\"Total age distribution\")\nplt.show()\n\nax= px.histogram(data1,x= \"age\", template= \"plotly_dark\",color= \"output\",title='Output 0 or 1')\nax.show()\n\nax= px.histogram(data1,x= \"age\", template= \"plotly_dark\",color= \"sex\",title='Male vs Female age distribution')\nax.show()","58eb6c98":"X=data1[data1[\"sex\"]==1].reset_index()  #Females\nY=data1[data1[\"sex\"]==0].reset_index()   #Males\n\nHR=data1[data1[\"output\"]==1].reset_index()  #High Risk\nLR=data1[data1[\"output\"]==0].reset_index()  #Low Risk\n\n#Calculate general characteristics\nprint(\"----------------General Data------------------\")\nprint(\"Total Age =>\")\nprint(\"Mean: {}\".format(round(data1[\"age\"].mean())))\nprint(\"Median: {}\".format(round(data1[\"age\"].median())))\nprint(\"Variance: {}\".format(round(data1[\"age\"].var())))\nprint(\"Standard Deviation: {}\\n\".format(round((data1[\"age\"].std()),3)))\n\n# Calculate gender specific characteristics\nprint(\"--------------Gender Specific Data-------------------\")\nprint(\"For MEN =>\")\nprint(\"Mean: {}\".format(round(Y[\"age\"].mean())))\nprint(\"Median: {}\".format(round(Y[\"age\"].median())))\nprint(\"Variance: {}\".format(round(Y[\"age\"].var())))\nprint(\"Standard Deviation: {}\\n\\n\".format(round((Y[\"age\"].std()),3)))\n\nprint(\"For WOMEN =>\")\nprint(\"Mean: {}\".format(round(X[\"age\"].mean())))\nprint(\"Median: {}\".format(round(X[\"age\"].median())))\nprint(\"Variance: {}\".format(round(X[\"age\"].var())))\nprint(\"Standard Deviation: {}\\n\".format(round((X[\"age\"].std()),3)))\n\n# Calculate output specific characteristics\nprint(\"------------------Output Specific Data-----------------------\")\nprint(\"For High Risk =>\")\nprint(\"Mean: {}\".format(round(HR[\"age\"].mean())))\nprint(\"Median: {}\".format(round(HR[\"age\"].median())))\nprint(\"Variance: {}\".format(round(HR[\"age\"].var())))\nprint(\"Standard Deviation: {}\\n\\n\".format(round((HR[\"age\"].std()),3)))\n\nprint(\"For Low Risk =>\")\nprint(\"Mean: {}\".format(round(LR[\"age\"].mean())))\nprint(\"Median: {}\".format(round(LR[\"age\"].median())))\nprint(\"Variance: {}\".format(round(LR[\"age\"].var())))\nprint(\"Standard Deviation: {}\".format(round((LR[\"age\"].std()),3)))","e5e8fa0c":"new_df=data1\nplt.figure(figsize=(16,8))\ncorr=new_df.corr()\nsns.heatmap(abs(corr),lw=1,annot=True,cmap=\"Reds\")\nplt.show()\n\nC=abs(corr[\"output\"]).sort_values(ascending=False)[1:]\nprint(C)\nplt.figure(figsize=(15,6))\nplt.plot(C.index,C,color=\"red\")\nplt.scatter(C.index,C,color=\"red\",lw=5)\nplt.show()","5b6a8234":"continuous_cols=[\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\ncontinuous_data=data1[continuous_cols]\n\nfor k, v in continuous_data.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 \/ np.shape(data1)[0]\n        print(\"Column {} outliers = {} => {}%\".format(k,len(v_col),round((perc),3)))","a66ddc82":"fig, ax1 = plt.subplots(2,2, figsize=(20,12))\nk = 0\nfor i in range(2):\n    for j in range(2):\n        sns.boxplot(data=data1,x=data1[continuous_cols[1:][k]],saturation=1,ax=ax1[i][j],color=\"white\")\n        k+=1\nplt.tight_layout()\nplt.show()\n\n#Display the position of outliners.\nprint(\"Outliners Present at position: \\n\")\nprint(\"trtbps: {}\".format(np.where(data1['trtbps']>165)))\nprint(\"chol: {}\".format(np.where(data1['chol']>360)))\nprint(\"thalachh: {}\".format(np.where(data1['thalachh']<80)))\nprint(\"oldpeak: {}\".format(np.where(data1['oldpeak']>4)))","e98996f6":"#Using log transformation\ndata1[\"age\"]= np.log(data1.age)\ndata1[\"trtbps\"]= np.log(data1.trtbps)\ndata1[\"chol\"]= np.log(data1.chol)\ndata1[\"thalachh\"]= np.log(data1.thalachh)\nprint(\"---Log Transform performed---\")\n\n#Outliners again\ncontinuous_cols=[\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\ncontinuous_data=data1[continuous_cols]\n\nfor k, v in continuous_data.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 \/ np.shape(data1)[0]\n        print(\"Column {} outliers = {} => {}%\".format(k,len(v_col),round((perc),3)))","95f1882e":"#Data Splitting\nX=data1.iloc[:,:13]\nY=data1[\"output\"]\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=65) \n\n#MinMax Scaling \/ Normalization of data\nMM_scaler = MinMaxScaler()\nX_train = MM_scaler.fit_transform(X_train)\nX_test = MM_scaler.fit_transform(X_test)","206576c1":"def compute(Y_pred,Y_test):\n    #Output plot\n    plt.figure(figsize=(12,6))\n    plt.scatter(range(len(Y_pred)),Y_pred,color=\"yellow\",lw=5,label=\"Predictions\")\n    plt.scatter(range(len(Y_test)),Y_test,color=\"red\",label=\"Actual\")\n    plt.title(\"Prediction Values vs Real Values\")\n    plt.legend()\n    plt.show()\n\n    cm=confusion_matrix(Y_test,Y_pred)\n    class_label = [\"High-risk\", \"Low-risk\"]\n    df_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n    sns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\n    plt.title(\"Confusion Matrix\",fontsize=15)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.show()\n\n    #Calculate Metrics\n    acc=accuracy_score(Y_test,Y_pred)\n    mse=mean_squared_error(Y_test,Y_pred)\n    precision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=1, average='binary')\n    print('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {} %\\nMean Square Error: {}'.format(\n        round(precision, 3), round(recall, 3), round(fscore,3), round((acc*100),3), round((mse),3)))","32ffe359":"#Build Model\nstart = time.time()\n\nmodel_Log= LogisticRegression(random_state=10)\nmodel_Log.fit(X_train,Y_train)\nY_pred= model_Log.predict(X_test)\n\nend=time.time()\n\nmodel_Log_time=end-start\nmodel_Log_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\n\nprint(f\"Execution time of model: {round((model_Log_time),5)} seconds\\n\")\n#Plot and compute metrics\ncompute(Y_pred,Y_test)","add99983":"#Build Model\nstart=time.time()\n\nmodel_KNN = KNeighborsClassifier(n_neighbors=15)\nmodel_KNN.fit(X_train,Y_train)\nY_pred = model_KNN.predict(X_test)\n\nend=time.time()\n\nmodel_KNN_time = end-start\nmodel_KNN_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\n\nprint(f\"Execution time of model: {round((model_KNN_time),5)} seconds\")\n#Plot and compute metric\ncompute(Y_pred,Y_test)","02efe5e0":"#Build Model\nstart=time.time()\n\nmodel_svm=SVC(kernel=\"rbf\")\nmodel_svm.fit(X_train,Y_train)\nY_pred=model_svm.predict(X_test)\n\nend=time.time()\n\nmodel_svm_time=end-start\nmodel_svm_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\n\nprint(f\"Execution time of model: {round((model_svm_time),5)} seconds\")\n#Plot and compute metric\ncompute(Y_pred,Y_test)","83dea9cc":"#Build Model\nstart=time.time()\n\nmodel_tree=DecisionTreeClassifier(random_state=10,criterion=\"gini\",max_depth=100)\nmodel_tree.fit(X_train,Y_train)\nY_pred=model_tree.predict(X_test)\n\nend=time.time()\n\nmodel_tree_time=end-start\nmodel_tree_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\n\nprint(f\"Execution time of model: {round((model_tree_time),5)} seconds\")\n#Plot and compute metric\ncompute(Y_pred,Y_test)","59945ecb":"#Build Model\nstart=time.time()\n\nmodel_RF = RandomForestClassifier(n_estimators=300,criterion=\"gini\",random_state=5,max_depth=100)\nmodel_RF.fit(X_train,Y_train)\nY_pred=model_RF.predict(X_test)\n\nend=time.time()\n\nmodel_RF_time=end-start\nmodel_RF_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\n\nprint(f\"Execution time of model: {round((model_RF_time),5)} seconds\")\n#Plot and compute metric\ncompute(Y_pred,Y_test)","64a684f3":"#Build Model\nstart=time.time()\n\nmodel_ADA=AdaBoostClassifier(learning_rate= 0.15,n_estimators= 25)\nmodel_ADA.fit(X_train,Y_train)\nY_pred= model_ADA.predict(X_test)\n\nend=time.time()\n\nmodel_ADA_time=end-start\nmodel_ADA_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\n\nprint(f\"Execution time of model: {round((model_ADA_time),5)} seconds\")\n#Plot and compute metric\ncompute(Y_pred,Y_test)","686dd39b":"#Build Model\nstart=time.time()\n\nmodel_GB= GradientBoostingClassifier(random_state=10,n_estimators=20,learning_rate=0.29,loss=\"deviance\")\nmodel_GB.fit(X_train,Y_train)\nY_pred= model_GB.predict(X_test)\n\nend=time.time()\n\nmodel_GB_time=end-start\nmodel_GB_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\n\nprint(f\"Execution time of model: {round((model_GB_time),5)} seconds\")\n#Plot and compute metric\ncompute(Y_pred,Y_test)","718f1849":"#Build Model\nstart=time.time()\n\nmodel_xgb = XGBClassifier(objective='binary:logistic',learning_rate=0.1,\n                          max_depth=1,\n                          n_estimators = 50,\n                          colsample_bytree = 0.5)\nmodel_xgb.fit(X_train,Y_train)\nY_pred = model_xgb.predict(X_test)\n\nend=time.time()\n\nmodel_xgb_time=end-start\nmodel_xgb_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\n\nprint(f\"Execution time of model: {round((model_xgb_time),5)} seconds\")\n#Plot and compute metric\ncompute(Y_pred,Y_test)","4cc8a1cc":"#Build Model\nstart=time.time()\n\nmodel_MLP = MLPClassifier(random_state=48,hidden_layer_sizes=(150,100,50), max_iter=150,activation = 'relu',solver='adam')\nmodel_MLP.fit(X_train, Y_train)\nY_pred=model_MLP.predict(X_test)\n\nend=time.time()\n\nmodel_MLP_time=end-start\nmodel_MLP_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\n\nprint(f\"Execution time of model: {round((model_MLP_time),5)} seconds\")\n#Plot and compute metric\ncompute(Y_pred,Y_test)","835064cc":"def checkRS():\n\n    mse_dict={}  #Root mean square dictionary\n    acc_dict={}  #Accuracy dictionary\n\n    for n in range(25,400,25):\n        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,random_state=65)\n        model_MLP = MLPClassifier(random_state=48,hidden_layer_sizes=(150,100,50), max_iter=n,activation = 'relu',solver='adam')\n        model_MLP.fit(X_train, Y_train)\n        Y_pred=model_MLP.predict(X_test)\n        acc=accuracy_score(Y_test,Y_pred)\n        mse=mean_squared_error(Y_test,Y_pred)\n        mse_dict.update({n:round(mse,3)})\n        acc_dict.update({n:round((acc*100),3)})\n\n    #Mean Square Error\n    lowest=min(mse_dict.values())\n    res = [key for key in mse_dict if mse_dict[key] == lowest]\n    mse_list=mse_dict.items()\n    k,v = zip(*mse_list) \n    print(\"RMSE is lowest at {} for n: {} \".format(round((lowest),3),res))\n\n    #Plot RMSE values\n    plt.figure(figsize=(12,6))\n    #plt.scatter(res,lowest,color=\"red\",lw=5)\n    plt.plot(k,v)\n    plt.xlabel(\"Random State\")\n    plt.ylabel(\"RMSE\")\n    plt.grid(True)\n    plt.show()\n\n    #Accuracy\n    highest=max(acc_dict.values())\n    res1= [key for key in acc_dict if acc_dict[key] == highest]\n    acc_list=acc_dict.items()\n    k1,v1=zip(*acc_list)\n    print(\"Accuracy is highest at {} % for n: {} \".format(highest,res1))\n    #Plot Accuracy values\n    plt.figure(figsize=(12,6))\n    #plt.scatter(res1,highest,color=\"red\",lw=5)\n    plt.plot(k1,v1)\n    plt.xlabel(\"Random State\")\n    plt.ylabel(\"Accuracy\")\n    plt.grid(True)\n    plt.show()\n    \ncheckRS()","324cda6f":"accuracies={\"Logistic regression\": model_Log_accuracy,\n            \"KNN\": model_KNN_accuracy,\n            \"SVM\": model_svm_accuracy,\n            \"Decision Tree\": model_tree_accuracy,\n            \"Random Forest\": model_RF_accuracy,\n            \"Ada Boost\": model_ADA_accuracy,\n            \"Gradient Boosting\": model_GB_accuracy,\n            \"XG Boost\": model_xgb_accuracy,\n            \"MLP Classifier\": model_MLP_accuracy}\n\nacc_list=accuracies.items()\nk,v = zip(*acc_list) \ntemp=pd.DataFrame(index=k,data=v,columns=[\"Accuracy\"])\ntemp.sort_values(by=[\"Accuracy\"],ascending=False,inplace=True)\n\n#Plot accuracy for different models\nplt.figure(figsize=(20,7))\nACC=sns.barplot(y=temp.index,x=temp[\"Accuracy\"],label=\"Accuracy\",edgecolor=\"violet\",linewidth=3,orient=\"h\",palette=\"twilight_r\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Accuracy Comparison\")\nplt.xlim(80,98)\n\nACC.spines['left'].set_linewidth(3)\nfor w in ['right', 'top', 'bottom']:\n    ACC.spines[w].set_visible(False)\n    \n#Write text on barplots\nk=0\nfor ACC in ACC.patches:\n    width = ACC.get_width()\n    plt.text(width+0.1, (ACC.get_y() + ACC.get_height()-0.3),s=\"{}%\".format(temp[\"Accuracy\"][k]),fontname = 'monospace', fontsize = 14, color = 'violet') \n    k+=1\n    \nplt.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.show()","f9f7546d":"exe_time={\"Logistic regression\": model_Log_time,\n            \"KNN\": model_KNN_time,\n            \"SVM\": model_svm_time,\n            \"Decision Tree\": model_tree_time,\n            \"Random Forest\": model_RF_time,\n            \"Ada Boost\": model_ADA_time,\n            \"Gradient Boosting\": model_GB_time,\n            \"XG Boost\": model_xgb_time,\n            \"MLP Classifier\": model_MLP_time }\n\ntime_list=exe_time.items()\nk,v = zip(*time_list) \ntemp1=pd.DataFrame(index=k,data=v,columns=[\"Time\"])\ntemp1.sort_values(by=[\"Time\"],ascending=False,inplace=True)\n\n#Plot accuracy for different models\nplt.figure(figsize=(20,7))\nET=sns.barplot(y=temp1.index,x=temp1[\"Time\"],label=\"Time\",edgecolor=\"violet\",linewidth=3,orient=\"h\",palette=\"twilight_r\")\nplt.ylabel(\"Model\")\nplt.title(\"Execution Time Comparison\")\nET.spines['left'].set_linewidth(3)\nfor w in ['right', 'top', 'bottom']:\n    ET.spines[w].set_visible(False)\n\n#Write text on barplots\nk=0\nfor ET in ET.patches:\n    width = ET.get_width()\n    plt.text(width+0.01, (ET.get_y() + ET.get_height()-0.3),s=\"{}s\".format(round((temp1[\"Time\"][k]),3)),fontname = 'monospace', fontsize = 14, color = 'violet') \n    k+=1\n\nplt.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.show()","4950d495":"<hr>","f60973e8":"Clearly, there arent many outliners but we still have to try to minimize them as much as we can. We can take the log of these columns to bring all values within a range and minimize the outliners.","26ec32a7":"# Data Pre-processing\nWe will start off by observing the correalation between every column and the specific correlation with the output.","5587647a":"> Below is a box-cum-swarm plot for the two genders. It is a great way to observe the data and it's stats like **median, max, min, and the quartiles** by just hovering on the plots.\n\nAs we can see more females took part in the research as compared to males and also the range of women was higher (29-77 years) as opposed to (34-76 years) in men.","34e809a9":"# 4 => Decision Tree Classifier","0b3f4634":"<hr>","27efec14":"<hr>","fb98f535":"**Plot 1: The first plot shows the total distribution of age for all the individuals in the survey. Most of them were between 50-60 age group and very few on either side.<br>\nPlot 2: The second plot shows the distribution of age separately for people with higher risk (output = 1) and for peoplw with lower risk (output = 0)<br>\nPlot 3: the third plot shows the distribution of age separately for women (sex = 1) and men (sex = 0)<br>**","7a1b9380":"# 5 => Random Forest Classifier","3b3ccacb":"**Clealy, there are no null values in the dataset as observed above so we don't need to treat any columns for null values.**","5ef3f1a0":"**Display Function to plot and compute the data.**<br>\nWe will use this function for each model separately.","f44f8baa":"<hr>","acebb95c":"![image.png](attachment:a9b4858e-8044-4c7e-8556-43ecb358cc5b.png)","908a06bc":"These plots look okay to as none of them except \"oldpeak\" are skewed too much to the left or right. We will take care of these columns later while pre-processing the data.<br><br>\n<hr>\n\n# \"Age\" EDA\nLet's analyze the age column more closely now. We will plot the variation of age with respect to other variables like \"sex\" and \"output\".","4010c40d":"# Male vs Female\nThe dataset has a column \"sex\" to classify the patients in the category 1 or 0 based on their gender. However, it does not mention if 1 represents male or female and the same for 0. We can not bother and build the model anyway without knowing which label stands for which gender, but it's always better to know the gender for better analysis. <br><br>\n\nJust by little research we can find this out^ ,\n\n> __According to the Harvard Health department:__ <br>\n> Researchers found that throughout life, men were about twice as likely as women to have a heart attack. That higher risk persisted even after they accounted for traditional risk factors for heart disease, including high cholesterol, high blood pressure, diabetes, body mass index, and physical activity. <br><br>\n\nWe can use this information to do find out which label stands for which gender. This basically means that the gender which has higher average risk of experiencing an heart-attack is more likely to be a male. We have the data to find this risk percentage, we only have to compute it as below.","b12ddf5d":"# 6 => AdaBoost Classifier","8516fbaf":"> The best parameters and random state for every model was calculated using the following **checkRS()** function.","24933967":"So clearly, the boosting algorithms have dominated the accuracy when it comes to the model comparison (Unsurprisingly). <br>\n* XGBoost: 95.08%\n* AdaBoost: 93.44%\n* MLPClassifier: 93.44%\n* Random Forest: 91.8%\n* Gradient Boosting: 91.8%\n* Logistic Regression: 90.16%\n* SVM: 90.16%\n* KNN: 88.52%\n* Decision Tree: 81.97%","855a360e":"Here the columns \"*age - thall*\" are the independent variables and the last column \"*output*\" contains the dependent variable.<br> The meaning of every column is given below (pulled from the original dataset description).","12ea1980":"Now let's plot the categorized values, namely **[\"output\",'cp',\"fbs\",\"exng\",\"restecg\",\"thall\",\"caa\",\"slp\"]**","48486a56":"# 2 => K-Nearest Neighbours","3461247c":"When it comes to the execution time, decision tree is again at the bottom (which is a good thing this time) and is the fastest algorithm of the lot.\n* MLPClassifier: 1.211s\n* Random Forest: 0.59s\n* AdaBoost: 0.051s\n* XGBoost: 0.044s\n* Gradient Boosting: 0.025s\n* Logistic Regression: 0.011s\n* SVM: 0.009s\n* KNN: 0.008s\n* Decision Tree: 0.003s\n","c19e13c2":"<hr>","03417522":"We can see that no column is highly correlated with the output, with a maximum correlation of **0.436757** for \"exng\" and a minimum correlation of **0.028046** for \"fbs\". Anyway, we will go ahead and use them all for our model.\n\n**Next, we have to check the data for outliners and treat them.**<br>\nWe will use the IQR (Inter-Quartile Range) method to detect outliners and then visualize them with the help of boxplots.","130c4003":"Let's observe the output variable for the data.","fbb31c2a":"<hr>","b6172110":"<hr>","adf975b6":"# Compare Accuracy and Execution Time","6e0de14f":"# 3 => Support Vector Machines","a618186d":"This shows that for the gender \"1\", 44.93% are at a high risk of having an heart attack whereas for gender \"0\", 75.0% are at a higher risk.<br>\n\n> **Based on this calculation and the survey by harvard researchers, we can conclude that the sex label \"0\" in the dataset is for male whereas the label \"1\" is for females since the label \"0\" is more prone to having an heart attack (75.0%)**\n\nSince we managed to find out which label stood for which gender, we can now visualize the data for Male vs Female.","6af0af79":"# 1 => Logistic Regression","ad2cbde3":"**Data Splitting and scaling:**<br>\nWe will split the data into training and testing sets using train_test_split from *sklearn.preprocessing*. After splitting we will scale our data using the MinMax scaler before using it for training our model.","ffbeab4d":"\"1\" shows that the person is at a higher risk of experiencing an heart attack and similarly \"0\" shows a lower risk.<br>\nSo in the given dataset, more people are in the high risk (54.5%) and less people in the low risk (45.5%) as observed in the pie plot.\n\n<hr>\nAs you must have noticed this dataset has 2 typical types of data, categorized data and continuous data. We cannot analyze this data together as categorization and continuous values have different properties. Let's first store these column names separately so that we can access them easily.<br><br>","d1a669a8":"<hr>","8a4f0295":"__Import Datasets__","a254762d":"Plotting the pie charts and swarm plots for the continuous variables will not get us anywhere, so instead we plot the density distribution for these variables and analyze them. **[\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]**","735a2c21":"<hr>","7aece421":"<hr>","8ff3e331":"# 7 => Gradient Boosting Classifier","0e59cf7f":"<hr>","051d2e37":"**The sunburst plots from Plotly show more specific visualizations. They visualize hierarchical data spanning outwards radially from root to leaves. The sunburst sector hierarchy is determined by the entries in labels ( names in px. sunburst ) and in parents . The root starts from the center and children are added to the outer rings.**<br>\n> In this case, the parent column is \"Sex\".","510cbdc6":"<hr><hr>\n\n# __Conclusion:__\n\n1. We started off by understanding the columns of the Heart Attack dataset and performing EDA on the it.\n2. The dataset didn't mention which label (0 or 1) stood for which gender, hence we also found that  with a little extra research.\n3. We then preprocessed the data based on the correlation and outliners.\n4. The preprocessed data was used to train 9 ML models who's accuracy and execution time was stored and later plotted to compare the models.\n\n<hr>\n\n**If you found the notebook useful please upvote!**<br>\n**If you have any doubts or suggestions, feel free to comment down below!**\n\n# Thank You!!\n\n<hr><hr>","6559ac62":"# 9 => MLPClassifier","942a71d4":"**After performing the log transform a significant number of outliners was reduced as observed above.**\n\nThat's all the preprocessing we need, we will now build our models.<br>","20b12df3":"# 8 => XGBoost Classifier","1ca1862a":"This pie chart depicts that more than 68% patients belong to one gender (male or female) while the rest to the other gender (31.7%) <br><br>\n**We just have to find which one is which.  For that lets calculate the the percent of high risk for both labels (1 and 0)**"}}