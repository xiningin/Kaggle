{"cell_type":{"24150af7":"code","1d583395":"code","57e30cc0":"code","ae00da20":"code","929d07d5":"code","30d8117a":"code","f2aa40a8":"code","10ccaba3":"code","2298aee2":"code","05cd9b47":"code","5520ce21":"code","6b1377fe":"markdown","544253fa":"markdown","73d7cd18":"markdown","6ff52b12":"markdown","096a89aa":"markdown","be952808":"markdown","ba067a05":"markdown","0c8a8a77":"markdown","51e624c0":"markdown","c0cb67f1":"markdown","42ce965e":"markdown","608f3494":"markdown","617c9018":"markdown","2a0618b8":"markdown","b88a4829":"markdown"},"source":{"24150af7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1d583395":"train_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv')","57e30cc0":"train_data.info()\ntrain_data.head()","ae00da20":"test_data.info()\ntest_data.head()","929d07d5":"# Counting the number of zeros in training dataset\nzeros = 0\nfor field in train_data.drop(['id', 'target'], axis=1).columns:\n    zeros += train_data[field].value_counts().loc[0]\n\nprint('Training data')\nprint('Zero Count: ', zeros)\nprint(f'Percent of Zeros: {zeros\/(75*200000)*100}%')\n\n# Counting the number of zeros in test dataset\nzeros = 0\nfor field in test_data.drop(['id'], axis=1).columns:\n    zeros += test_data[field].value_counts().loc[0]\n\nprint('\\nTest data')\nprint('Zero Count: ', zeros)\nprint(f'Percent of Zeros: {zeros\/(75*100000)*100}%')","30d8117a":"# removing id as it will not provide any statistical insights\ntrain_data.drop(['id'], axis=1).describe()","f2aa40a8":"# removing id as it will not provide any statistical insights\ntest_data.drop(['id'], axis=1).describe()","10ccaba3":"# Finding correlation between features\ncorr = train_data.drop(['id'], axis=1).corr() # we are again removing the id column due to the same reason\ncorr.info()\ncorr","2298aee2":"# Creating a correlation plot of size 20x20\nfig, ax =plt.subplots(figsize=(20, 20))\n\nplt.title(\"Correlation Plot\")\nsns.heatmap(corr,\n            cmap=sns.diverging_palette(230, 10, as_cmap=True),\n            square=True,\n            ax=ax)\nplt.show()","05cd9b47":"class_counts = train_data.target.value_counts().sort_index()\nclass_percents = class_counts\/len(train_data)*100\nclass_percents","5520ce21":"plt.figure(figsize=(10,6))\nplt.bar(class_percents.index, class_percents.values)\n\nplt.show()","6b1377fe":"From above two cells, we can conclude that there are **no null values** in training data or in test data both. In total, there are 75 features and all of these contain integer values.\n\nThere are **200000 records** in training data and **100000 records** in test data. Since the dataset is huge, models which are computationally expensive like Support Vector Machine cannot be used for this problem.\n\nAnother observation we can make is that a lot of values in the dataset are **zero**. To confirm it we can count the number of zeros in both datasets and find the percentage of zeros.","544253fa":"**Class_6** and **Class_8** each occur for about **26%** of the cases. On the other hand, **Class_4** and **Class_5** each occur for about **2%** of the cases.\n\nSo, in such case a good strategy would be to maintain a good ratio of precision and recall values.","73d7cd18":"Now, we will check whether the occurence of each target class in the training data is equal or not.\n\nIf the number of examples of each class in the training data are not close to each other, then a poorly trained model might generalize this as the property of the feature set, which might not be the actual case. So, in such cases it is beneficial to measure **precision** and **recall** values also in addition to accuracy.","6ff52b12":"# Feature Engineering and EDA","096a89aa":"# Initialization","be952808":"In both training data and test data, the percent of zero values is **about 65%**. So, the dataset is sparse.","ba067a05":"In this notebook I will try to gain insights from data. This notebook will help in later deciding what model would be best for this problem.","0c8a8a77":"Next, we will find correlation between features to check for any redundant features.\n\nThe **basic concept** is that if the correlation value between two features is **close to -1 or 1**, the more features are like each other. So, keeping both such features provides no extra information and we can remove one of these features.","51e624c0":"Since, the correlation matrix is of size 75x75, it is not possible gain any insights by observing it.\n\nSo, we will be creating a **correlation plot**.","c0cb67f1":"First, we will observe the dataset by printing\n - features info\n - some dataset rows\n - statistical information\n \nThen, we will try to gather insights using these.","42ce965e":"## Finding Correlation between features","608f3494":"As expected, the 25 percentile and 50 percentile values are 0.\n\nThe maximum value of each feature is a **lot greater** than the 75 percentile.\n\nBoth of these observations are valid for both training data and test data. So, considering the huge values as outliers and removing them might degrade the model performance.\n\nWe can also observe that all statistical values for all features for both the datasets **are quite similar** to each other, so it might not be necessary to apply regularization in the model.","617c9018":"The scale on the right of the plot shows that values **close to 0 are plotted as blue** and the **close to 1 are plotted as red**.\n\nThe only red boxes in the plot are along the diagonal which is expected as each feature will definitely be like itself. So, correlation between a feature with itself will be equal to 1.\n\nThe whole plot is blue, which suggests that the correlation values between each features are close to 0. Thus, we can conclude that the features are **not correlated** to each other at all.\n\nSo, all the features should be used for model training and feature reduction techniques will do harm to model accuracy.","2a0618b8":"## Observing datasets","b88a4829":"## Finding class occurence percentage"}}