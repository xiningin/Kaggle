{"cell_type":{"2f989b04":"code","e9ac6f73":"code","27131a0f":"code","7e3485a9":"code","ebb5afb2":"code","64d45578":"code","9b06a800":"code","61595674":"code","e756f93d":"code","11f95bff":"code","d420c07a":"code","5ef5050b":"code","1020c5c6":"code","4f1cedb9":"code","877449fd":"code","8ca7ad66":"code","60ef18df":"code","811e9b6f":"code","e588d6c9":"code","48e0cb76":"code","0ccb46fe":"code","7198b34f":"code","e1c0d319":"code","c33e8681":"code","fe05591a":"code","e666f919":"code","cb5db029":"code","6973112a":"code","d1d58341":"code","11ebbf58":"code","6f11c9e0":"code","ae82d32a":"code","6d6234cf":"code","8d891c3a":"code","33d06d83":"code","2076ab3e":"code","a53dc075":"code","152a17d3":"code","869e9eb6":"code","c8ebf342":"code","99f799e3":"code","c424be2e":"code","a12df25d":"code","fcf01794":"code","2a3aa498":"code","732da6a8":"code","93307648":"code","0a4c61cb":"code","26e56269":"code","80e23ab4":"code","58ab641c":"code","4d94293d":"code","c9ea2d8e":"code","b8de7c73":"code","8c498604":"code","81dc1d12":"code","848e33aa":"code","4b3aec16":"code","546fa72a":"code","c74c4163":"code","a1525f52":"code","c263dd8b":"code","adb0d1b1":"code","0803553f":"code","56371aaa":"code","472f45ce":"code","f33726f8":"markdown","260cc273":"markdown","26b4b9df":"markdown","244f3973":"markdown","754db237":"markdown","e7902103":"markdown","0ac9b60f":"markdown","ba9e5d80":"markdown","55d9b716":"markdown","c95f88f0":"markdown","2c656b1b":"markdown","6bffbb3f":"markdown","5f709673":"markdown","c4f6c011":"markdown","3bbe34da":"markdown","564b9487":"markdown","9066b122":"markdown","5ffdf23d":"markdown","56bded45":"markdown"},"source":{"2f989b04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e9ac6f73":"# Regular EDA(Exploratory Data Analysis) and plotting libraries\nimport pandas as pd # Data\nimport numpy as np # Linear Algebra\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for plots to appear inside the notebook\n%matplotlib inline \n\n# Models from Scikit-learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model evaluation\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\n","27131a0f":"# Load Data\ndf = pd.read_csv('..\/input\/heartdisease\/heart-disease.csv')\ndf.shape # (rows, columns)","7e3485a9":"df.head()","ebb5afb2":"df['target'].value_counts()","64d45578":"df['target'].value_counts().plot(kind='bar', color=['salmon', 'lightblue']);\nplt.title('Heart Disease')","9b06a800":"df.info()","61595674":"# lets check total null values\ndf.isna().sum()\n","e756f93d":"# No null values in any columns\n# lets check some stats\ndf.describe()","11f95bff":"### Heart Disease frequency according to 'Sex' variable:\n\ndf.sex.value_counts()","d420c07a":"# Male value 1 \/\/ 0 = Female\n\n# Lets compare 'sex' column to the 'target' column:\n\npd.crosstab(df.target, df.sex)","5ef5050b":"# Above shows 72 women has postive for heart disease i.e 1 and it also shows les\n# male have less number of heart disease i.e. 93.\n# Base on exsiting data, we may infer that female have high chance of heart disease. \n\n## Lets visualize the above comparison\n\npd.crosstab(df.target, df.sex).plot(kind='bar', figsize=(10,6), color=(['salmon', 'lightblue']));","1020c5c6":"## Lets add details to the above bar graph for better visualization\npd.crosstab(df.target, df.sex).plot(kind='bar', figsize=(10,6), color=(['salmon', 'lightblue']));\nplt.title('Heart Disease Frequency based on Sex')\nplt.xlabel('0 = No Disease, 1 = Disease')\nplt.ylabel('Amount')\nplt.legend(['Female', 'Male'])\nplt.xticks(rotation=0);","4f1cedb9":"#Bar graph shows only few points, however we need some more information which cannot be represened only in BAR graph\n\n# Now lets check Age vs MAX heart rate for Heart Disease\nplt.figure(figsize=(10,6))\n\n#Scatter with positive examples\nplt.scatter(df.age[df.target==1], df.thalach[df.target==1], c='salmon');\n\n#Scatter with negative examples\nplt.scatter(df.age[df.target==0], df.thalach[df.target==0], c='blue');\n\n#Add some helpful info\nplt.title('Heart Disease in function of Age and Max Heart Rate')\nplt.xlabel('Age')\nplt.ylabel('Max Heart Rate')\nplt.legend(['Disease', 'No Disease']);","877449fd":"#Check the distribution of age column using histogram (spread of data)\ndf.age.plot.hist();\n    \n    ","8ca7ad66":"#From histogram, we can see maximum dataset is focusing on 50-65 years\n# Moreover, there seems no outlier in age column\n\n# Lets compare another (Chest pain = cp) with target column\n# Heart Disease Frequency with Chest Pain type\n\npd.crosstab(df.cp, df.target)\n","60ef18df":"# Odd information from dataset shows that patients with CP = 2 (non heart related) have more heart disease\n# lets make this more visual\n\npd.crosstab(df.cp, df.target).plot(kind='bar', figsize=(10,6), color=['Salmon', 'lightblue'])\n\n#Add some more info to plot\nplt.title('Heart Disease Frequency as per Chest Pain')\nplt.xlabel('Chest Pain Type')\nplt.ylabel('Amount')\nplt.legend(['No Disease', 'Disease'])\nplt.xticks(rotation=0);\n","811e9b6f":"df.head()","e588d6c9":"## Now we will use correlation matrix to find relation between different variable\ndf.corr()\n","48e0cb76":"# let's make correlation matrix better for visualization\ncorr_matrix = df.corr()\nfig,ax = plt.subplots(figsize=(15,10))\nax=sns.heatmap(corr_matrix, annot=True, linewidths=0.5, fmt='.2f', cmap='YlGnBu')","0ccb46fe":"df.head()","7198b34f":"# Split data into X and y\nX = df.drop('target', axis=1)\n\ny = df['target']","e1c0d319":"X","c33e8681":"y","fe05591a":"# Split data in to train and test\n\nnp.random.seed(42)\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)","e666f919":"X_train","cb5db029":"y_train, len(y_train)","6973112a":"# Lets put modesl in dictionary\nmodels = {'Logistic Regression': LogisticRegression(), 'KNN': KNeighborsClassifier(),\n         'Random Forest' : RandomForestClassifier()}\n\n# Create a function to fit and score models\n\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    #set random seed\n    np.random.seed(42)\n    #Make a dictionay to keep model scores\n    model_scores = {}\n    #Loop through models\n    for name,model in models.items():\n        #fit model to data\n        model.fit(X_train, y_train)\n        #Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test,y_test)\n    return model_scores\n    ","d1d58341":"model_scores = fit_and_score(models = models,\n                            X_train=X_train,\n                            X_test=X_test,\n                            y_train=y_train,\n                             y_test=y_test)\nmodel_scores","11ebbf58":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot.bar();","6f11c9e0":"## Training KNN\ntrain_scores = []\ntest_scores = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1,21)\n\n# Setup KNN instance\n\nknn = KNeighborsClassifier()\n\n# Loop through different n_neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    \n    #Fit the algorithm\n    knn.fit(X_train, y_train)\n    \n    # Update the training scores list\n    \n    train_scores.append(knn.score(X_train,y_train))\n    \n    # Update test scores list\n    \n    test_scores.append(knn.score(X_test, y_test))","ae82d32a":"train_scores","6d6234cf":"plt.plot(neighbors, train_scores, label='Train Score')\nplt.plot(neighbors, test_scores, label='Test Score')\nplt.xticks(np.arange(1,21,1))\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Model Score')\nplt.legend()\n\nprint(f'Maxinum Knn score on the test data: {max(test_scores)*100:.2f}%')","8d891c3a":"# Create a hyperparameter grid for LogisticRegression\n\nlog_reg_grid = {'C': np.logspace(-4,4,20),\n               'solver': ['liblinear']}\n\n# Create a hyperparameter grid for RandomForestClassifier\n\nrf_grid = {'n_estimators': np.arange(10,1000,50),\n           \"max_depth\" : [None,3,5,10],\n           'min_samples_split': np.arange(2,20,2),\n           'min_samples_leaf':np.arange(1,30,2)}","33d06d83":"#Tune LogisticRegression\n\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LR\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                               param_distributions=log_reg_grid,\n                               cv=5,\n                               n_iter=20,\n                               verbose=True)\n\n# Fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(X_train, y_train)","2076ab3e":"rs_log_reg.best_params_","a53dc075":"rs_log_reg.score(X_test, y_test)\n","152a17d3":"# Setup random seed\nnp.random.seed(42)\n\n#Setup random hyperparameter search for RandomForestClassifier()\n\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                          param_distributions=rf_grid,\n                          cv=5,\n                          n_iter = 30,\n                          verbose=True)\n\n# Fit random hyperparameter search model for RandomForest\nrs_rf.fit(X_train, y_train)","869e9eb6":"# Find the best hyperparameters\nrs_rf.best_params_","c8ebf342":"# Evalute the randomized search RandomForestClassifier model\nrs_rf.score(X_test, y_test)","99f799e3":"model_scores","c424be2e":"# Different Hyperparameter for our LogisticRegression model\n\nlog_reg_grid = {'C' : np.logspace(-4,4,30),\n               'solver' : ['liblinear']}\n\n# Setup grid hyperparameter search for LR\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid=log_reg_grid,\n                         cv=5,\n                         verbose=True)\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train);","a12df25d":"#Check the best hyperparameters\ngs_log_reg.best_params_","fcf01794":"# Evaluate the grid search LR model\ngs_log_reg.score(X_test, y_test)","2a3aa498":"model_scores","732da6a8":"# Make predictions with tuned model\ny_preds = gs_log_reg.predict(X_test)","93307648":"y_preds\n","0a4c61cb":"y_test","26e56269":"# Import ROC curve function from the sklearn.metrics already imported under\n# Model evalutaion at the starting\n\n# Plot ROC curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test);","80e23ab4":"# Confusion matrix\nprint(confusion_matrix(y_test, y_preds))","58ab641c":"sns.set(font_scale=1.5)\ndef plot_conf_mat(y_test,y_preds):\n    '''\n    Plots a nice looking confusion matrix using seaborns heatmap\n    '''\n    fig,ax= plt.subplots(figsize=(3,3))\n    ax = sns.heatmap(confusion_matrix(y_test,y_preds),\n                    annot=True,\n                    cbar=False)\n    plt.xlabel('True Label')\n    plt.ylabel('Predicted Label')\n\nplot_conf_mat(y_test, y_preds)","4d94293d":"print(classification_report(y_test, y_preds))","c9ea2d8e":"# Check best hyperparamaters\ngs_log_reg.best_params_","b8de7c73":"# Create a new classified with best parameters\nclf=LogisticRegression(C=0.20433597178569418,\n                      solver='liblinear')","8c498604":"# Cross-Validated Accuracy\ncv_acc= cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring='accuracy')\ncv_acc =np.mean(cv_acc)\ncv_acc\n","81dc1d12":"# Cross-Validated precision\ncv_precision= cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring='precision')\ncv_precision=np.mean(cv_precision)\ncv_precision","848e33aa":"# Cross-Validated recall\ncv_recall=cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring='recall')\ncv_recall=np.mean(cv_recall)\ncv_recall","4b3aec16":"# Cross-Validated F1-score\ncv_f1 =cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring='f1')\ncv_f1 = np.mean(cv_f1)\ncv_f1\n","546fa72a":"# Visualize our cross-validated metrics\ncv_metrics = pd.DataFrame({'Accuracy': cv_acc,\n                          'Precision': cv_precision,\n                          'Recall': cv_recall,\n                          'F1':cv_f1},\n                         index=[0])\ncv_metrics.T.plot.bar(title='Cross-validated classification matrix',\n                     legend=False);","c74c4163":"# Fit an instance of LogisticRegression\ngs_log_reg.best_params_\n\nclf = LogisticRegression(C=0.20433597178569418,\n                        solver='liblinear')\n\nclf.fit(X_train, y_train);","a1525f52":"# Check coef_\nclf.coef_","c263dd8b":"df.head()","adb0d1b1":"# Match coef's of feature to columns\nfeature_dict=dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","0803553f":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title='Feature Importance', legend=False);\n","56371aaa":"pd.crosstab(df['sex'], df['target'])","472f45ce":"pd.crosstab(df['slope'], df['target'])","f33726f8":"### Lets start with MODELLING","260cc273":"# Experimentation\n\nIf you haven't hit your evaluation metric yet... ask yourself..\n\n* Could you collect more data?\n* Could you try a better model? Like Catboost or XGBoost?\n* Could you improve the current models? (beyond what has been done so far)\n* If your model is good enough (you have hit your evaluation metric) how would you export it and share it with others?\n","26b4b9df":"# Looking at above one can figure out that there are 165 people with heart-disease and 138 with no heart disease\n# This looks like nearly a balance classification problem\n\n# Lets visualize the above","244f3973":"## Lets check type of data in our dataset","754db237":"# Data Exploration (EDA)\nThe goal is to find details about the data:\n\nWhat queston(s) are you trying to solve?\nWhat kind of data do we have and how do we treat different types?\nwhat's missing from the data and how do you deal with it?\nwhere are the outliers and why should you care about them?\nHow can you add, change or remove features to get more out of your date?","e7902103":"## Our target is to find if someone has heart disease or not\n## Since we already have a target variable, we will consider that as our dependent variable\n\n### lets check first how many classes are there in this 'target' variable:\n","0ac9b60f":"#### Now we've got our data split in to training and test set. Let's build ML model\n#### We will train on training set and test on test set\n#### Since this is a classification problem, we will use various classification method. For details, visit scikit learn machine_learning_map\n\nWe will try 3 Different models\n\n1. Logistic Regression\n2. K-Nearest Neighbors Classifier\n3. RandomForest Classifier","ba9e5d80":"### We can see that chestpain has positvie correlation(0.43) with target variable\n#### As CP type goes up (from 1-4) target value also increases \n#### Negative correlation = a relationship between two variables in which one variable increases as other decrease\n#### We can see a negative correlation between 'exang' variable and 'target' variable","55d9b716":"### Let's now try GridSearch Cross Validation technique and see if this improves score.\n\nSince our LogisticRegression model provides best score so fare, we will try and improve\nthem again using GridSearchCV","c95f88f0":"### Feature Importance\n\nFeature importance is another way of asking, which feature is contributing\nmost to the outcomes of model and how did they contribute?\n\nFinding feature importance is different for each machine learning models. One way to find feature importance is to search for (Model Name) feature importance.\n\nLet's find the feature importance for our Logistic Regression model","2c656b1b":"### Now we have tuned LogisticRegression(), let's do the same for RandomForestClassifier()","6bffbb3f":"### Now we have got ROC curve, AUC metric and a confusion matrix. Lets get a \nclassification report as well as cross valid report and f1-score","5f709673":"## MODEL COMPARISION","c4f6c011":"#### Above figure shows improvement in KNN model score, however it is still not so good so we will drop this model","3bbe34da":"## Evalutaing our tuned machine learning classifier, beyond accuracy\n\n* ROC curve and AUC score\n* Confusion matrix\n* Classification report\n* Precision \n* Recall\n* F1 Score\n\n.... it would be great if cross-validation was used where possible\n\nTo make comparisons and evalute our trained model, first we need to make predictions","564b9487":"### Hyperparameter tuning with RandomizedSearchCV\n\nWe are going to tune:\n* LogisticRegression()\n* RandomForestClassifier()\n.... using RandominzedSearchCV`","9066b122":"### Now we have got hyperparameter grids setup for each of our models. Lets tune them using RandomizedSearchCV","5ffdf23d":"### Now we have got a baseline model and we know our model's first prediction aren't always accurate.\n\nWhat should we do?\n\nLet's look at the following:\n* Hyperparameter tuning\n* Feature Importance\n* Confusion matrix\n* Cross-validation\n* Precision\n* Recall\n* F1 score\n* Classification report\n* ROC curve\n* Area under the curve (AUC)\n\n## Hyperparameter tuning","56bded45":"### Calculate evaluation matrix using cross-validation\n\nWe are goind to calculate accuracy, precision, recall and f1-score of our model using cross-validation score cross_val_score()."}}