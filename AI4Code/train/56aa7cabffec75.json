{"cell_type":{"abc21ffb":"code","1ea1820d":"code","3940778f":"code","dfee7a69":"code","417696fc":"code","84ce604c":"code","882393fd":"code","cbef8f41":"code","a4badd04":"code","aa7180b8":"code","7ed50905":"code","f0a06299":"code","3c8b13a9":"code","d257096f":"code","cda33acc":"code","4ee2129a":"code","0d395c11":"code","aecb7aba":"code","fba01831":"code","69e38263":"code","ca47e002":"code","d27881d3":"code","79f019e6":"code","5191bb84":"code","aeca79e5":"code","3ed46860":"code","528212b1":"code","f82e80ea":"code","82c42291":"code","178306be":"code","12e293cc":"code","6df77d23":"code","728b3bbd":"code","4e5f7fc1":"code","e2f7447d":"code","09704c5b":"code","01acea60":"code","d620766f":"code","208d22f6":"code","fe6065b0":"code","1f8d51ba":"code","8c0f226e":"code","ee19d553":"code","3af8726a":"code","add29355":"code","f0b4dacd":"code","b8607527":"code","ae77f860":"code","03048e8c":"code","a3743e1a":"code","a49afcd4":"code","2ed75d9c":"markdown","1b518db9":"markdown","e704e116":"markdown","7c46a1e4":"markdown","dcbec175":"markdown","7d8c8be2":"markdown","664e6ad2":"markdown","65cfbbf3":"markdown","56f73bdc":"markdown","029acce7":"markdown","a6987e41":"markdown","59186f7d":"markdown","6d708de5":"markdown","421e67da":"markdown","5c37f089":"markdown","b100fc53":"markdown"},"source":{"abc21ffb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ea1820d":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","3940778f":"np.shape(df)","dfee7a69":"df.head(10)","417696fc":"df.describe()","84ce604c":"df.mode()","882393fd":"sns.countplot(df['Survived'], label='number of survivors')","cbef8f41":"df.groupby('Sex')[['Survived']].mean()","a4badd04":"df.groupby('SibSp')[['Survived']].mean()","aa7180b8":"sns.countplot(df['Pclass'], label='Passengers')","7ed50905":"labels = ['First Class', 'Second Class', 'Third Class']\nsizes = [202, 190, 500]\ncolors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\nexplode = (0.1, 0, 0)\n\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=140)\n\nplt.axis('equal')\nplt.show()","f0a06299":"cols = ['Sex', 'SibSp', 'Parch', 'Embarked']\n\nn_rows = 2\nn_cols = 2\n\nfig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*5.0,n_rows*5.0))\n\nfor r in range(0,n_rows):\n    for c in range(0,n_cols):  \n        \n        i = r*n_cols+ c      \n        ax = axs[r][c] \n        sns.countplot(df[cols[i]], hue=df[\"Survived\"], ax=ax)\n        ax.set_title(cols[i])\n        ax.legend(title=\"survived\", loc='upper right') \n        \nplt.tight_layout()","3c8b13a9":"df.head()","d257096f":"X = df.iloc[:, [2, 4, 5, 6, 7, 9, 11]].values\ny = df.iloc[:, 1].values","cda33acc":"print(X)","4ee2129a":"print(pd.unique(X[:, 1]))","0d395c11":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX[:, 1] = le.fit_transform(X[:, 1])","aecb7aba":"print(pd.unique(X[:, 1]))","fba01831":"print(X)","69e38263":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X[:, :-1])\nX[:, :-1] = imputer.transform(X[:, :-1])","ca47e002":"print(X)","d27881d3":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputer.fit(X)\nX = imputer.transform(X)","79f019e6":"print(X)","5191bb84":"pd.isnull(X)","aeca79e5":"print(X)","3ed46860":"print(pd.unique(X[:, -1]))","528212b1":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))","f82e80ea":"print(X)","82c42291":"print(y)","178306be":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","12e293cc":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit_transform(X_train, y_train)\nsc.fit_transform(X_test)","6df77d23":"print(X)","728b3bbd":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)","4e5f7fc1":"from sklearn.metrics import accuracy_score, confusion_matrix \ny_pred_xgb = xgb.predict(X_test)\ncm = confusion_matrix(y_test, y_pred_xgb)\nprint(cm)\naccuracy_score(y_test, y_pred_xgb)","e2f7447d":"from sklearn.svm import SVC\nsvm = SVC(kernel = 'rbf', random_state = 0)\nsvm.fit(X_train, y_train)","09704c5b":"from sklearn.metrics import accuracy_score, confusion_matrix \ny_pred_svm = svm.predict(X_test)\ncm = confusion_matrix(y_test, y_pred_svm)\nprint(cm)\naccuracy_score(y_test, y_pred_svm)","01acea60":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknn.fit(X_train, y_train)","d620766f":"from sklearn.metrics import confusion_matrix, accuracy_score\ny_pred_knn = knn.predict(X_test)\ncm = confusion_matrix(y_test, y_pred_knn)\nprint(cm)\naccuracy_score(y_test, y_pred_knn)","208d22f6":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = xgb, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","fe6065b0":"test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","1f8d51ba":"X_td = test_data.iloc[:, [1, 3, 4, 5, 6, 8, 10]].values","8c0f226e":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX_td[:, 1] = le.fit_transform(X_td[:, 1])","ee19d553":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X_td[:, :-1])\nX_td[:, :-1] = imputer.transform(X_td[:, :-1])","3af8726a":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputer.fit(X_td)\nX_td = imputer.transform(X_td)","add29355":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\nX_td = np.array(ct.fit_transform(X_td))","f0b4dacd":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit_transform(X_td)","b8607527":"y_pred = xgb.predict(X_td)","ae77f860":"print(y_pred)","03048e8c":"submission_predictions = pd.DataFrame({\"PassengerId\": test_data['PassengerId'], \"Survived\": y_pred})","a3743e1a":"print(submission_predictions)","a49afcd4":"submission_predictions.to_csv(r'C:\\Users\\adity.LAPTOP-F6A6F39F.000\\Desktop\\submission_predictions.csv', index = False, header=True)","2ed75d9c":"# Implement a K-Nearest Neighbors model","1b518db9":"# Implement Cross Validation #\n\nUsing cross validation to ensure taht there is no overfitting i.e. a variable that would cause the test set to be overtly dependent on that variable whcih would be counter intuitive.","e704e116":"# Encoding the Independent Variable *'Sex'*","7c46a1e4":"The code below fills in the missing value for the last one with the mode or the most recurring value of the column.","dcbec175":"The code below fills in the missing value for every column except the last one with the mean or average of each column.","7d8c8be2":"# Creating the matrix of features of the independent variable and the vector of the dependent variable\n\nHere, a bit of feature selection was implemented and what remained were 8 main columns excluding the Passenger ID column which served as an index. The prominent columns that remained are:\n\nPclass: This represents the class a passenger belonged to. It is a categorical variable with three values.\u20191\u2019 which stands for First Class. \u20182\u2019 which stands for Second Class and finally \u20183\u2019 which stands for Third Class.\nSex: This column represents the gender of a passenger and it is a categorical variable as well. The values that the two variables represent are self-explanatory.\nAge: This column represents the age of a passenger and it is occupied by numerical values.\nSibSp: This column represents the number of siblings or spouses a passenger has aboard the Titanic with them. It also has numerical values.\nParch: These values represent the number of parents or children a passenger has aboard.\n\nHere X represents the matrix of features of the independent variable and y represents the vector of the dependent variable.\n","664e6ad2":"# Filling in the Missing Values ","65cfbbf3":"# Final Predictions with the Test Values #","56f73bdc":"This data shows that women were significantly more likely to survive the peril more than men.","029acce7":"# Implementing the XGBoost model.\n\nWe are fitting the XGBoost model on the training set and the test set to train the model.","a6987e41":"# Importing the Dataset\n","59186f7d":"This data shows that individual passengers with 1 to 2 siblings or a spouse aboard were more likely to survive.","6d708de5":"# Feature Scaling #\n\nWe are scaling the independent and the dependent variable to prevent any inaccuracies. We are implementing Standardization.","421e67da":"# Titanic Survival Prediction #\n\nThe aim of this project is to predict if an individual died on the Titanic or lived.\n\nThis program begins with importing the required libraries for the project. The libraries that have been imported are as follows: Pandas which is a premier data analysis and manipulation library used in Python, Numpy which is one of the most popular libraries for data collection and storage. The last two libraries that have been imported are Seaborn and Matplotlib which are optimal data visualisation and graphing libraries.","5c37f089":"A barplot shows that most individuals on the Titanic died in the tragedy.","b100fc53":"# Implementing a Kernel SVM model"}}