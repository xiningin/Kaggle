{"cell_type":{"0c8ed19d":"code","e2216e95":"code","39431590":"code","609fd71c":"code","f0e11938":"code","3d607da1":"code","689e9f4e":"code","2572ec49":"code","d1c603b2":"code","d818b1cf":"code","1f48c9be":"markdown","05e58c5b":"markdown","f9a2bf94":"markdown","d50a4fde":"markdown","9b69dec7":"markdown","ff8e6ad5":"markdown","99d24295":"markdown","4b755648":"markdown","142e0add":"markdown","ea051034":"markdown"},"source":{"0c8ed19d":"##Import the required packages. These include pandas, numpy,scikit-learn and optuna\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import KFold\nimport optuna\nimport optuna.integration.lightgbm as lgb\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e2216e95":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')\n\ntrain.columns.to_list()","39431590":"conts = ['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10']\ncats = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9','cat10','cat11','cat12','cat13','cat14','cat15','cat16','cat17','cat18','target']\nfor c in train.columns:\n    col_type = train[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        train[c] = train[c].astype('category')\n\nfor c in test.columns:\n    col_type = test[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        test[c] = test[c].astype('category')","609fd71c":"X= train[['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9','cat10','cat11','cat12','cat13','cat14','cat15','cat16','cat17','cat18']]\nY = train[['target']]","f0e11938":"from sklearn.model_selection import StratifiedKFold\nkfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","3d607da1":"dtrain = lgb.Dataset(X,Y,categorical_feature = 'auto')\n\nparams = {\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"verbosity\": -1,\n    \"boosting_type\": \"gbdt\",\n}\n\ntuner = lgb.LightGBMTunerCV(\n    params, dtrain, verbose_eval=100, early_stopping_rounds=1000000, folds=kfolds\n)\n\ntuner.run()\n\nprint(\"Best score:\", tuner.best_score)\nbest_params = tuner.best_params\nprint(\"Best params:\", best_params)\nprint(\"  Params: \")\nfor key, value in best_params.items():\n    print(\"    {}: {}\".format(key, value))","689e9f4e":"tuner.best_score","2572ec49":"params = tuner.best_params","d1c603b2":"import lightgbm as lgb\nid_test = test.id.to_list()\nmodel = lgb.train(params, dtrain, num_boost_round=1000)","d818b1cf":"X_test = test[['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9','cat10','cat11','cat12','cat13','cat14','cat15','cat16','cat17','cat18']]\npreds = model.predict(X_test)\nresultf = pd.DataFrame()\nresultf['id'] = id_test\nresultf['target'] = preds\nresultf.to_csv('submission.csv',index=False)","1f48c9be":"LightGBM is a really convenient to use, fast to train and usually accurate implementation of boosted trees. Here I use optuna for hyperparameter search using Bayesian optimization methods, with 5-fold cross validation, to gain a fairly accurate model. Also I leverage some seemingly minor but very useful built in features of the LightGBM library to handle categorical variables.","05e58c5b":"Specify dependent and independent variables and create a lgb Dataset object","f9a2bf94":"Inspect the best score for AUC value","d50a4fde":"Predict on the test set and save file. Make sure you set index=False ","9b69dec7":"Assign the best params to a variable","ff8e6ad5":"Notice how I've specified auc as the metric.","99d24295":"Convert the categorical data into the category type such that lightgbm can handle the categorical variables. Unless you leverage learned embeddings for categorical variables, this fares better that one hot encoding or label encoding","4b755648":"Use these parameters to train a LightGBM model on the entire training dataset \n","142e0add":"Read the train and test csv's into variables and list out the column names","ea051034":"Use the optuna lightGBM integration to do hyperparamater optimization with 5 fold cross validation. Make sure to pass in the argument 'auto' for categorical_feature for automated feature engineering for categorical input features."}}