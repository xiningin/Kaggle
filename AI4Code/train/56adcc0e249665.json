{"cell_type":{"41335fdc":"code","45121b9b":"code","185eadc2":"code","6a956b38":"code","aa1200ae":"code","9c3c5a8f":"code","1dadf8a6":"code","bfa4dc28":"code","0e638337":"code","4173a522":"code","2e7aafca":"code","86bc90cb":"code","922772df":"code","c6e0ce38":"code","571ec05c":"code","b899f0fe":"code","26dc3c7f":"markdown","4f694900":"markdown","42767896":"markdown","12ba9e9a":"markdown"},"source":{"41335fdc":"from sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nimport pickle\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\n\nseed = 0","45121b9b":"dataset = load_iris()\ndataset.keys()","185eadc2":"# Vemos la descripci\u00f3n del dataset.\nprint(dataset['DESCR'])","6a956b38":"# Extraemos del diccionario los objetos que nos interesan.\ndata = dataset['data']\ntarget = dataset['target']\ntarget_names = dataset['target_names']\nfeature_names = dataset['feature_names']","aa1200ae":"print('Clases:', target_names)\nprint('Variables:', feature_names)","9c3c5a8f":"# Para manejar los datos m\u00e1s f\u00e1cilmente en la exploraci\u00f3n, creamos un dataframe con los datos.\niris_df = pd.DataFrame(data, columns=feature_names)\niris_df['target'] = list(map(lambda x: target_names[x], target))\niris_df","1dadf8a6":"# Sacamos con describe un resumen de algunas estad\u00edsticas del dataset,\ndisplay(iris_df.describe())\n# vemos el balance de etiquetas\ndisplay(iris_df.target.value_counts())\n# y si hay valores nulos\nprint('\u00bfHay valores nulos?', pd.isna(iris_df).any().any())","bfa4dc28":"# Sacamos el histograma de cada variable separada por clase. \n\ncolor_dic = {target_name: c for target_name, c in zip(target_names, 'rgb')}\n\nfig, ax = plt.subplots(nrows=3, ncols=4, figsize=(15,15))\nfor n_row, class_name in enumerate(target_names):\n    for n_col, feature in enumerate(feature_names):\n        ax[n_row, n_col].hist(iris_df[iris_df.target == class_name][feature], \n                              range=(iris_df[feature].min(), iris_df[feature].max()),\n                              color=color_dic[class_name],\n                              label=class_name)\n        ax[n_row, n_col].set_title(feature)\n        ax[n_row, n_col].legend()\nplt.title('Histogramas de las cuatro variables separadas por clase')\nplt.show()","0e638337":"# Vemos la distribuci\u00f3n de las variables dos a dos seg\u00fan clase:\n\nscatter_matrix(iris_df, alpha=0.7, figsize=(15, 15), diagonal=\"hist\", \n               c=list(map(lambda x: color_dic[x], iris_df['target'])))\nhandles = [plt.plot([],[],color=\"rgb\"[i], ls=\"\", marker=\".\", \n                    markersize=8)[0] for i in range(3)]\nlabels = iris_df['target'].drop_duplicates()\nplt.legend(handles, labels, loc=(1.02,0))\nplt.title('Scatter plots de las variables dos a dos')\nplt.show()","4173a522":"# Cambiamos la columna de target de textual a num\u00e9rica y barajamos los datos.\niris_df['target'] = target\niris_df = iris_df.sample(frac=1, random_state=seed).sort_values('target').reset_index(drop=True)\n\n# Hacemos el split de los datos en train y test de forma estratificada \n# (vamos a usar un 80% de los datos para el entrenamiento y un 20% para testear). \n# Posteriormente, entrenaremos con un 5-fold cross-validation.\ntest_index = pd.RangeIndex(start=0, stop=10).union(\n    pd.RangeIndex(start=50, stop=60)).union(\n    pd.RangeIndex(start=100, stop=110))\ntest_df = iris_df.loc[test_index].reset_index(drop=True)\n\ntrain_index = pd.RangeIndex(start=10, stop=50).union(\n    pd.RangeIndex(start=60, stop=100)).union(\n    pd.RangeIndex(start=110, stop=150))\ntrain_df = iris_df.loc[train_index].reset_index(drop=True)\n\nX_train = train_df.drop(columns='target').values\ny_train = train_df.target.values\nX_test = test_df.drop(columns='target').values\ny_test = test_df.target.values\n\n# Escalamos los datos y guardamos el normalizador para usarlo en las futuras predicciones.\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nwith open('scaler.pkl', 'wb') as f:\n    pickle.dump(scaler, f)","2e7aafca":"def grid_search_cv(search_space, model_name, return_result=False):\n    \"\"\"\n    Recibe un espacio de b\u00fasqueda de hiperpar\u00e1metros y una clase de un predictor, realiza la b\u00fasqueda con CV e \n    imprime los resultados del mejor modelo en train y en test y la mejor combinaci\u00f3n de hiperpar\u00e1metros \n    (en cuanto a accuracy).\n    \"\"\"\n    model = model_name(random_state=seed)\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=seed)\n    grid_search = GridSearchCV(model, search_space, scoring='accuracy', n_jobs=-1, cv=cv)\n    search_result = grid_search.fit(X_train, y_train)\n    print('Mejor accuracy de la b\u00fasqueda ({}):'.format(model.__class__.__name__), \n          search_result.best_score_)\n    print('Mejor combinaci\u00f3n de hiperpar\u00e1metros ({}):'.format(model.__class__.__name__),  \n          search_result.best_params_)\n    best_model = model_name(**search_result.best_params_, random_state=seed)\n    best_model = best_model.fit(X_train, y_train)\n    print('Accuracy del mejor modelo en test ({}):'.format(model.__class__.__name__), \n          (best_model.predict(X_test) == y_test).mean())\n    print('----------------------------------------------')\n    if return_result:\n        return search_result.best_score_, search_result.best_params_, model_name\n    \ndef full_grid_search_cv(spaces_list, models_list):\n    \"\"\"\n    Recibe una lista de espacios de b\u00fasqueda y otra de clases de modelos, aplica grid_search_cv a cada par de \n    (espacio, modelo), reentrena el mejor de todos los modelos con todos los datos (para ponerlo en \n    producci\u00f3n despu\u00e9s), lo eval\u00faa y lo guarda.\n    \"\"\"\n    scores = [grid_search_cv(space, model, return_result=True) for space, model in zip(spaces_list, models_list)]\n    _, params, model_name = max(scores, key=lambda x: x[0])\n    model = model_name(**params, random_state=seed).fit(np.concatenate([X_train, X_test]), np.concatenate([y_train, y_test]))\n    print('Modelo seleccionado:', model.__class__.__name__)\n    print('Accuracy del mejor modelo en train:', (model.predict(X_train) == y_train).mean())\n    print('Accuracy del mejor modelo en test:', (model.predict(X_test) == y_test).mean())\n    print('Accuracy del mejor modelo en todo el conjunto:', (model.predict(np.concatenate([X_train, X_test])) == np.concatenate([y_train, y_test])).mean())\n    with open('mejor_modelo.pkl', 'wb') as f:\n        pickle.dump(model, f)\n","86bc90cb":"# Definimos los espacios de b\u00fasqueda y modelos.\n\nspaces_list = []\nmodels_list = []\n\nspace_1 = dict()\nspace_1['solver'] = ['liblinear']\nspace_1['penalty'] = ['l1', 'l2']\nspace_1['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000]\nspace_1['max_iter'] = [50, 100, 200]\n\nspace_2 = dict()\nspace_2['solver'] = ['lbfgs', 'newton-cg']\nspace_2['penalty'] = ['l2']\nspace_2['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000]\nspace_2['max_iter'] = [50, 100, 200]\n\nsearch_space = [space_1, space_2]\n\nspaces_list.append(search_space)\nmodels_list.append(LogisticRegression)\n\nsearch_space = {\n    'n_estimators': [20, 50, 150, 250],\n    'min_samples_split': [2, 4, 6],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4, 5, 6, 7],\n    'criterion' :['gini', 'entropy'],\n    'bootstrap': [True, False]\n}\n\nspaces_list.append(search_space)\nmodels_list.append(RandomForestClassifier)\n\nsearch_space = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5],\n        'eta': [0.01, 0.05, 0.1, 0.2],\n        'verbosity': [0]\n        }\n\nspaces_list.append(search_space)\nmodels_list.append(XGBClassifier)\n\nfull_grid_search_cv(spaces_list, models_list)","922772df":"results = pd.read_csv('results.csv')\n","c6e0ce38":"results","571ec05c":"results = pd.read_csv('results.csv')\n\nresults['best_epoch'] = results['val_loss'].map(lambda x: np.argmin(list(map(float, x[1:-1].split(',')))))\nresults['val_loss'] = results['val_loss'].map(lambda x: list(map(float, x[1:-1].split(','))))\nresults['loss'] = results['loss'].map(lambda x: list(map(float, x[1:-1].split(','))))\nresults['val_loss'] = results.apply(lambda row: row['val_loss'][row['best_epoch']], axis=1)\nresults['loss'] = results.apply(lambda row: row['loss'][row['best_epoch']], axis=1)\n\nresults = results.sort_values('val_loss', ascending=True)","b899f0fe":"results","26dc3c7f":"### Importaci\u00f3n de librer\u00edas","4f694900":"Se observa una gran separabilidad entre la clase setosa y las otras dos, no tanto entre versicolor y virg\u00ednica. Aun as\u00ed, parece un problema bastante asequible para un clasificador sencillo.\n\nPor ello, vamos a probar con Regresi\u00f3n Log\u00edstica, Random Forest y XGBoost (estos dos \u00faltimos no son espec\u00edficos para datasets peque\u00f1os pero suelen dar muy buenos resultados).","42767896":"### Carga y exploraci\u00f3n de datos","12ba9e9a":"### Modelado"}}