{"cell_type":{"1383f99b":"code","d30bc6d1":"code","37172b31":"code","220314be":"code","87ec05c4":"code","81f787a4":"code","24283ef3":"code","4e5d736e":"code","4f6c5bb5":"code","a06b2b52":"code","0efbb552":"code","bff459d1":"code","8ffdf262":"code","caaa438f":"code","c83d6814":"code","04bcaf79":"code","ff92e2db":"code","a625475e":"code","dd3ddd89":"code","c62804eb":"code","4c3c87d4":"code","bc05df49":"code","a69e4274":"code","61a7944f":"code","bea1973d":"code","5c7e5581":"code","76866bde":"code","8e9cad0d":"code","162e5eb9":"code","be694bf6":"code","0fb50cf9":"code","216ae3e1":"code","9ffab434":"code","879cba9d":"code","f026a735":"code","1d15ae9b":"code","a14137fa":"code","e77df3cd":"code","21ed51b0":"code","2c8a1406":"code","ce6a3fcb":"code","3f331f2f":"code","b3e039cd":"code","bd138e10":"code","7f1e1f3b":"code","59dc5485":"code","9efde840":"code","4681b7f9":"code","45412ce3":"code","741fd85f":"code","c9cfb4af":"code","f83b0c6a":"code","28fa8168":"code","389fafde":"code","decb0a9a":"code","a37ba55e":"code","4dbcab71":"code","7dd189ff":"code","695b8e13":"code","46f53f4d":"code","4d39d2e4":"markdown","8ca5cb7b":"markdown","6fd8ddcc":"markdown","ac7f3301":"markdown","e69618ef":"markdown","6ac9ec84":"markdown","eb9a3f45":"markdown","13ff24fa":"markdown","70d82429":"markdown","80629820":"markdown","39a4d612":"markdown","dc506fa4":"markdown","3cff861c":"markdown","507bc2fc":"markdown","1ac9c432":"markdown","d6817a6e":"markdown","8c54d91c":"markdown","9ec9c532":"markdown","a28ef4fe":"markdown","c71f8f67":"markdown","442dba3b":"markdown","d55ad5a3":"markdown"},"source":{"1383f99b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d30bc6d1":"import re #regex applies a regular expression to a string and returns the matching substrings. \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport nltk \nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\nimport nltk.corpus\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import BlanklineTokenizer\nfrom nltk.tokenize import TweetTokenizer\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nimport scikitplot as skplt\nfrom nltk.tokenize import word_tokenize\nimport gensim\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pickle\nfrom multiprocessing import Pool\nfrom textblob import TextBlob\nimport seaborn as sns","37172b31":"pip install emojis","220314be":"# Reading .csv file \nimport pandas as pd\ndata = pd.read_csv('..\/input\/large-random-tweets-from-pakistan\/Random Tweets from Pakistan- Cleaned- Anonymous.csv', encoding='ISO-8859-1')","87ec05c4":"from PIL import Image\nimport requests","81f787a4":"data.head(10)","24283ef3":"# To get the dimentions of the dataset.\ndata.shape","4e5d736e":"# To get the column names of the dataset\ndata.columns","4f6c5bb5":"# to print the consise summary of the data set\ndata.info()","a06b2b52":"# to compute a summary of statistics pertaining to the DataFrame columns\ndata.describe()","0efbb552":"# to return the first five rows of the data frame\ndata.head()","bff459d1":"# To get the data types of the different features\/ columns\ndata.dtypes","8ffdf262":"# to check the missing or null values in the data set \ndata.isnull().sum()                                                     \nmiss_val = data.isnull().sum().sort_values(ascending=False)\nmiss_val = pd.DataFrame(data=data.isnull().sum().sort_values(ascending=False), columns=['MissvalCount'])\n\n# Add a new column to the dataframe and fill it with the percentage of missing values\nmiss_val['Percent'] = miss_val.MissvalCount.apply(lambda x : '{:.2f}'.format(float(x)\/data.shape[0] * 100)) \nmiss_val = miss_val[miss_val.MissvalCount > 0]\nmiss_val","caaa438f":"# to check the the text of tweet at specified row\ndata['full_text'][10]","c83d6814":"# to compute a summary of statistics pertaining to the DataFrame column full_text\ndata['full_text'].describe()","04bcaf79":"# to compute a summary of statistics pertaining to the DataFrame location\ndata['location'].describe()","ff92e2db":"text = data['full_text']\nlocation = data['location']\n\nfor i in np.random.randint(1000, size=10):\n    print(f'Tweet # {i}: ', text[i], '=> Location: ', location[i], end='\\n' * 3)","a625475e":"location = data.groupby('location')\nlocation.head()","dd3ddd89":"# drop rows with any missing values\ndata.dropna(inplace=True)","c62804eb":"# Removing duplicates\ndata.drop_duplicates()","4c3c87d4":"# drop the columns with highest missing values \ndata = data.drop(['Unnamed: 0', 'created_at_tweet', 'retweet_count', 'favorite_count','reply_count', 'location'], axis=1)\ndata.head()","bc05df49":"from sklearn import decomposition\nfrom scipy import linalg\nimport re\n\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport string\nimport nltk\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline","a69e4274":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt ","61a7944f":"# remove twitter handles (@user)\ndata['tidy_text'] = np.vectorize(remove_pattern)(data['full_text'], \"@[\\w]*\")","bea1973d":"# remove special characters, numbers, punctuations\ndata['tidy_text'] = data['tidy_text'].str.replace(\"[^a-zA-Z#]\", \" \")","5c7e5581":"#Removing Short Words\ndata['tidy_text'] = data['tidy_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","76866bde":"def cleaning_URLs(data):\n    return re.sub('((www.[^s]+)|(https?:\/\/[^s]+))',' ',data)","8e9cad0d":"data['tidy_text'] = data['tidy_text'].apply(lambda x: cleaning_URLs(x))","162e5eb9":"from textblob import TextBlob","be694bf6":"def getSubjectivity(text):\n    return TextBlob(text).sentiment.subjectivity","0fb50cf9":"def getpolarity(text):\n    return TextBlob(text).sentiment.polarity","216ae3e1":"data['Subjectivity'] = data['tidy_text'].apply(getSubjectivity)","9ffab434":"data['Polarity'] = data['tidy_text'].apply(getpolarity)","879cba9d":"data.head(10)","f026a735":"def getPositiveNegativeWordCount(score):\n    if score < 0:\n        return 'Negative'\n    else:\n        return 'Positive'","1d15ae9b":"data['Positive Negative Word Count'] =  data['Polarity'].apply(getPositiveNegativeWordCount)","a14137fa":"import matplotlib.pyplot as plt \nimport seaborn as sns\nimport string\nimport nltk\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline\nfrom wordcloud import WordCloud ","e77df3cd":"negative_words =' '.join([text for text in data['tidy_text'][data['Positive Negative Word Count'] == 'Negative']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n","21ed51b0":"positive_words =' '.join([text for text in data['tidy_text'][data['Positive Negative Word Count'] == 'Positive']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(positive_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","2c8a1406":"def wordcount_extract(x):\n    wordCount = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        wordCount.append(ht)\n\n    return wordCount","ce6a3fcb":"negativeWordCount = wordcount_extract(data['tidy_text'][data['Positive Negative Word Count'] == 'Negative'])\n\n# extracting hashtags from racist\/sexist tweets\npositiveWordCount = wordcount_extract(data['tidy_text'][data['Positive Negative Word Count'] == 'Positive'])\n\n# unnesting list\nnegativeWordCount = sum(negativeWordCount,[])\npositiveWordCount = sum(positiveWordCount,[])","3f331f2f":"neg = nltk.FreqDist(negativeWordCount)\nnegWordCount = sum(list(neg.values()))","b3e039cd":"pos = nltk.FreqDist(positiveWordCount)\nposWordCount = sum(list(pos.values()))","bd138e10":"print(\"Negative Word Count: \",negWordCount)","7f1e1f3b":"print(\"Positive Word Count: \",posWordCount)","59dc5485":"data['Positive Negative Word Count'].value_counts()\n\nplt.title('Positive Negative Word Count')\nplt.xlabel('')\nplt.ylabel('Counts')\ndata['Positive Negative Word Count'].value_counts().plot(kind='bar')\nplt.show()","9efde840":"all_words = ' '.join([text for text in data['tidy_text']])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","4681b7f9":"all_words = ' '.join([text for text in data['tidy_text']])\nfrom wordcloud import WordCloud\n\nnormal_word = r\"(?:\\w[\\w']+)\"\nemoji = r\"(?:[^\\s])(?<![\\w{ascii_printable}])\".format(ascii_printable=string.printable)\nregexp = r\"{normal_word}|{emoji}\".format(normal_word=normal_word,emoji=emoji)\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, regexp=regexp ).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","45412ce3":"data.columns","741fd85f":"words = data[data.Subjectivity==0].full_text.apply(lambda x: [word.lower() for word in x.split()])\nh_words = Counter()\n\nfor Tweets_ in words:\n    h_words.update(Tweets_)\n    \nprint(h_words.most_common(50))","c9cfb4af":"words = data[data.Subjectivity==1].full_text.apply(lambda x: [word.lower() for word in x.split()])\nh_words = Counter()\n\nfor Tweets_ in words:\n    h_words.update(Tweets_)\n    \nprint(h_words.most_common(50))","f83b0c6a":"def  clean_text(df, text_field, new_text_field_name):\n    df[new_text_field_name] = df[text_field].str.lower() #Convert strings in the Series\/Index to lowercase.\n    \n    # remove numbers\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n    #remove url\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"https?:\/\/\\S+|www\\.\\S+\", \"\", elem))\n    #remove HTML tags\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"<.*?>\", \"\", elem))\n    #remove emojis \n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"[\"\n                                                                                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", \"\", elem))\n    return df\ndata_clean = clean_text(data, 'full_text', 'text_clean')\ndata_clean_test = clean_text(data,'full_text', 'text_clean')\ndata_clean.head(10)\n","28fa8168":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\ndata_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndata_clean.head()","389fafde":"from nltk.tokenize import sent_tokenize, word_tokenize\ndata_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\ndata_clean.head()","decb0a9a":"import nltk\nfrom nltk.stem.porter import PorterStemmer\nporter_stemmer  = PorterStemmer()\ntext = \"but this was one of my first shoots, about six years\"\ntokenization = nltk.word_tokenize(text)\nfor w in tokenization:\n    print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))","a37ba55e":"#Lemmatization\nimport nltk\nfrom nltk.stem import \tWordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\ntext = \"studies studying cries cry\"\ntokenization = nltk.word_tokenize(text)\nfor w in tokenization:\n\tprint(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  ","4dbcab71":"from nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\ndef word_stemmer(text):\n    stem_text = [PorterStemmer().stem(i) for i in text]\n    return stem_text\ndata_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\ndata_clean.head()","7dd189ff":"X_train, X_test, Y_train, Y_test = train_test_split(data_clean['text_clean'], \n                   \n                                                    data_clean['full_text'], \n                                                    test_size = 0.2,\n                                                    random_state = 10)","695b8e13":"vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\nvectorized = vectorizer.fit_transform(X_train)\npd.DataFrame(vectorized.toarray(), \n            index=['sentence '+str(i) \n                   for i in range(1, 1+len(X_train))],\n            columns=vectorizer.get_feature_names())","46f53f4d":"#Only alphabet, contains at least 3 letters\nvectorizer = CountVectorizer(analyzer='word', \n                              token_pattern=r'\\b[a-zA-Z]{3,}\\b',  \n                              ngram_range=(1, 1))\nvectorized = vectorizer.fit_transform(X_train)\npd.DataFrame(vectorized.toarray(), \n             index=['sentence '+str(i) \n                    for i in range(1, 1+len(X_train))],\n             columns=vectorizer.get_feature_names())","4d39d2e4":"# Feature Extraction","8ca5cb7b":"# Emoji Cloud","6fd8ddcc":"# Removing Stop Words","ac7f3301":"# Tokenizing","e69618ef":"# Remove Short Words","6ac9ec84":"# Droping rows with missing Values","eb9a3f45":"# Subjectivity in Detail","13ff24fa":"**Negative Words**","70d82429":"# EXPLORATORY DATA ANALYSIS","80629820":"# Assignment 2 Preprocessing of Tweats","39a4d612":"# Removing Missing Values","dc506fa4":"# A plot between positive and negative word counts","3cff861c":"# Remove special characters, numbers, punctuations","507bc2fc":"#  Claening Tweets","1ac9c432":"# Stemming","d6817a6e":"# Lemmatization","8c54d91c":"# Remove twitter handles","9ec9c532":"**Positive Words**","a28ef4fe":"# Remove URLs","c71f8f67":"# Word Cloud","442dba3b":"# HashTag Analysis of Tweats","d55ad5a3":"# Removing Duplicates"}}