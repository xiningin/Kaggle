{"cell_type":{"b01778a6":"code","25e111ab":"code","7719ee27":"code","f2619ae4":"code","2f81d131":"code","e68b2bb7":"code","e751868e":"code","79362d3c":"code","593cf724":"code","00c780d6":"code","f66bb67b":"code","3e9aaa98":"code","166b2735":"code","3849f9f7":"code","9e38372c":"code","b2d038ff":"code","42e60efc":"markdown","1262dc15":"markdown","9ae89be3":"markdown","a2ca592a":"markdown","02e8b97a":"markdown","a5144c8e":"markdown","fa6ae10d":"markdown","ce26b9e6":"markdown","8fe6cd24":"markdown","bbabe295":"markdown","b28fc5aa":"markdown","88566e5f":"markdown","10a0aa8e":"markdown","4017009b":"markdown","029855d5":"markdown","c9a64620":"markdown","57c64d69":"markdown","72901f1b":"markdown","86a4da51":"markdown","69db6930":"markdown","082746dd":"markdown","41c7db61":"markdown","712f621d":"markdown","8031abe1":"markdown"},"source":{"b01778a6":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","25e111ab":"img=Image.open('\/kaggle\/input\/parrot-image\/Grayscale_8bits_palette_sample_image.png')\nimg_array=-np.array(img)\/255.0\nplt.imshow(img_array,cmap='Greys',  interpolation='nearest')\nplt.show()","7719ee27":"edge_kernal=np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])\nsharpen_kernal=np.array([[0,-1,0],[-1,5,-1],[0,-1,0]])\nboxblur_kernal=np.ones((3,3))","f2619ae4":"# Creating to store resulting images in \nres=np.zeros(img_array.shape)\nres_sharpen=np.zeros(img_array.shape)\nboxblur_res=np.zeros(img_array.shape)\n\n# Logic is to iterate over every possible pixel on which convolution operation is possible\nfor i in range(1,img_array.shape[0]-2):\n    for j in range(1,img_array.shape[1]-2):\n        res[i,j]=(img_array[i:i+3,j:j+3]*edge_kernal).sum()\n        res_sharpen[i,j]=(img_array[i:i+3,j:j+3]*sharpen_kernal).sum()\n        boxblur_res[i,j]=(img_array[i:i+3,j:j+3]*boxblur_kernal).sum()\/9\nfig,ax=plt.subplots(1,3,figsize=(10,7))a\nax[0].imshow(res,cmap='Greys')\nax[0].set_title('edge detection')\nax[1].imshow(res_sharpen,cmap='Greys')\nax[1].set_title('sharpen image')\nax[2].imshow(boxblur_res,cmap='Greys')\nax[2].set_title('blur')\nplt.show()\n        ","2f81d131":"import torchvision\nimport torchvision.transforms as transforms\nimport torch\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='\/kaggle\/input\/cifar10-python', train=True,transform=transform)\ntestset = torchvision.datasets.CIFAR10(root='\/kaggle\/input\/cifar10-python', train=False,transform=transform)\nbatch_size = 128\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)","e68b2bb7":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","e751868e":"import torch.nn as nn\nclass LeNet(nn.Module):\n    def __init__(self): \n        super(LeNet, self).__init__()\n        self.cnn_model = nn.Sequential(\n            nn.Conv2d(3, 6, 5),         # (N, 3, 32, 32) -> (N,  6, 28, 28) |in_channel=3 (rgb) out_channel=6 kernel=5|\n            nn.ReLU(),\n            nn.AvgPool2d(2, stride=2),  # (N, 6, 28, 28) -> (N,  6, 14, 14)\n            nn.Conv2d(6, 16, 5),        # (N, 6, 14, 14) -> (N, 16, 10, 10)  \n            nn.ReLU(),\n            nn.AvgPool2d(2, stride=2)   # (N,16, 10, 10) -> (N, 16, 5, 5)\n        )\n        self.fc_model = nn.Sequential(\n            nn.Linear(400,120),         # (N, 400) -> (N, 120)\n            nn.ReLU(),\n            nn.Linear(120,84),          # (N, 120) -> (N, 84)\n            nn.ReLU(),\n            nn.Linear(84,10)            # (N, 84)  -> (N, 10)\n        )\n        \n    def forward(self, x):\n        x = self.cnn_model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_model(x)\n        return x","79362d3c":"def evaluation(dataloader):\n    total, correct = 0, 0\n    for data in dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = net(inputs)\n        _, pred = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (pred == labels).sum().item()\n    return 100 * correct \/ total","593cf724":"import torch.optim as optim\nnet = LeNet().to(device)\nloss_fn = nn.CrossEntropyLoss()\nopt = optim.Adam(net.parameters(),eps=1e-08,weight_decay=0.001)","00c780d6":"%%time\nloss_train=[]\nloss_test=[]\nmax_epochs = 20                                                      \n\nfor epoch in range(max_epochs):\n\n    for i, data in enumerate(trainloader, 0):\n\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        opt.zero_grad()\n\n        outputs = net(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        opt.step()\n\n    loss_train.append(evaluation(trainloader))\n    loss_test.append(evaluation(testloader))\n\n\n    print('Epoch: %d\/%d' % (epoch, max_epochs))","f66bb67b":"print('Test acc: %0.2f, Train acc: %0.2f' % (evaluation(testloader), evaluation(trainloader)))","3e9aaa98":"import seaborn as sns\nsns.set()\nplt.plot(loss_train,label='train')\nplt.plot(loss_test)\nplt.legend()","166b2735":"net","3849f9f7":"classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\nnet = net.to('cpu')\nimg,labels=iter(testloader).next()","9e38372c":"fig,ax=plt.subplots(1,2,figsize=(15,7))\nimgindx=9\nax[0].imshow(np.transpose(img[imgindx,:,:,:], (1, 2, 0)),cmap='viridis' )\nax[0].set_title(classes[labels[imgindx]])\nax[1].bar(classes,torch.nn.functional.softmax(net(img),dim=1)[imgindx].tolist())\nplt.show()","b2d038ff":"conv_channel=1\nfig,ax=plt.subplots(int(net.cnn_model[conv_channel-1].out_channels\/2),2,\n                    figsize=(15,7*int(net.cnn_model[conv_channel-1].out_channels\/2)))\nax=ax.flatten()\nfor i in range(net.cnn_model[conv_channel-1].out_channels):\n    ax[i].imshow( net.cnn_model[0:conv_channel](img)[imgindx,i,:,:].detach().numpy() )\n    ax[i].set_title('channel {}'.format(i))\nplt.show()","42e60efc":"### Padding and Stride\nHowever to allow convolutional opertation to be performed for all pixels of the image, the practice of Padding is taken into image <br\/>\nLet us consider adding extra rows+columns of zeros so that we can access all the image pixels\n![cnn2.PNG](attachment:cnn2.PNG)\n1. We can see that we must apply padding to preserve the output size\n2. The bigger the kernel size, the larger the padding required.\n3. Thus, the formulae from the last section can be updates as follows\n     $$W_0=W_I-F+1+2P$$\n     $$H_0=H_I-F+1+2P$$\n     \nAnother term that we use is called stride (S). It also affects the size of the output image.The intuition behind stride is that when we have images of huge sizes, taking a convolutional operation at steps does not reult in loss of variational information.\n\n\n$$W_0=\\frac{W_I-F+2P}{S}+1$$\n\n$$H_0=\\frac{H_I-F+2P}{S}+1$$\n\n\n    ","1262dc15":"### Some Well known kernels","9ae89be3":"# Contents :\n* [Fully Connected Neural Networks](#Fully-Connected-Neural-Networks)\n* [Understanding Convolution](#Understanding-Convolution)\n    * [1D Convolution](#1D-Convolution)\n        * [Going on a tangent , some interesting applications of 1d convolutions](#Going-on-a-tangent-,-some-interesting-applications-of-1d-convolutions)\n    * [2D convolution](#2D-convolution)\n        * [Understanding images](#Understanding-images)\n        * [2D convolution operation](#2D-convolution-operation)\n        * [Examples of 2D convolution](#Examples-of-2D-convolution)\n    * [Applying some well known convolutional kernels](#Applying-some-well-known-convolutional-kernels)\n    * [Understanding 2D convolution for a colored image](#Understanding-2D-convolution-for-a-colored-image)\n    * [Convolutional-Terminologies-and-relationship](#Convolutional-Terminologies-and-relationship)\n        * [Padding and Stride](#Padding-and-Stride)\n* [From Convolution operations to neural networks](#From-Convolution-operations-to-neural-networks)\n* [CNN Architectures](#CNN-Architectures)\n    * [LeNet](#LeNet)\n    * [Understanding components of LeNet](#Understanding-components-of-LeNet)\n    * [LeNet in Pytorch](#LeNet-in-Pytorch)\n    * [LeNet Class](#LeNet-Class)\n    * [Visualizing intermediate layers](#Visualizing-intermediate-layers)\n    ","a2ca592a":"![cnn-min.PNG](attachment:cnn-min.PNG)\n1. Input Width ($W_I$), Height ($H_I$) Depth ($D_I$)\n2. Output Width ($W_0$), Height ($H_0$) and Depth ($D_0$)\n3. The spatial extent of a filter (F), a single number to denote width and height as they are equal\n4. Filter depth is always the same as the Input Depth ($D_I$)\n5. The number of filters (K)\n\nPerforming the convolutional operation, shrinks down the image size because the kernal cannot be placed on the edges of the image i.e the kernal must overlapp with the image at all points<br\/>\nHence the resultant relationship is\n$$W_0=W_I-F+1$$\n$$H_0=H_I-F+1$$","02e8b97a":"# Understanding Convolution","a5144c8e":"allocating the cuda gpu","fa6ae10d":"![download.jpg](attachment:download.jpg)","ce26b9e6":"### LeNet in Pytorch","8fe6cd24":"## 2D convolution \n### Understanding images","bbabe295":"## Visualizing intermediate layers","b28fc5aa":"# From Convolution operations to neural networks\nWhile one way of image classification would be we apply some predefined convolutional filters like edge dectors and then feed the pixel valued to some sort of classification algorithm however in this method the transformation performed on the input image are static, without any learning per se, making it a hand-crafted set of features. The only learning that happens is inthe classifier.<br\/>\n\nHowever, in a deep Neural Network, the input features are not directly fed to the\nclassification\/output layer, instead they are passed through hidden\/representation layers, where\nthey are distilled down to more relevant features, before being passed into the classification layer.\n\nThis is why Deep Learning is also called Deep Representation Learning.<br\/>\nIn the above case, we allow the DNN to learn the representation weights and apply it to the\nfeatures in steps, before finally passing it onto the output layer.\n\nThe question arises that Why not let the network learn the feature representation also? <br\/>Why must we leave the choice of transformation to our\nhuman intuition? Why can\u2019t we let the DNN learn for itself the best transformation to apply?\n\nThis is where we join the two dots<br\/>\nThe output of a convolutional operation becomes a neural layer and the weights of the multiple kernals become weights of the network , which will be changed with backpropagation with the goal of reducing loss<br\/>\nHere, we are not concerned with the type of transformation that occurs, so long as the overall\nloss\/error is reduced. \n\nThe only difference between a DNN and what we\u2019re looking at now (CNN) is that for a CNN\nwe only consider a small localised neighborhood of inputs when calculating the output, instead\nof the entire input layer as in the case of DNNs.\n\nIn a nutshell, for CNNs, instead of learning the final classifier weights directly, we should also\nlearn to transform the input into a suitable representation through multiple layers of\nrepresentations and learn the kernel weights for all of those representations instead of using\nhand-crafted kernels.\n\nWith CNNs we have achived the goal of more complex non linearitites with lesser weights and sparse connections\n\n#### Some important points\n1. In CNNs, weights are nothing but the kernel, and the kernel remains constant as we passover the entire image, creating different output values based on the neighborhood of inputs. One complete pass of the kernel over the image constitutes one Convolutional output.\n2. As we have seen earlier, it is possible to have multiple convolutional operations by usingmultiple kernels over the input.\n3. For each of these convoluted outputs, the kernel is constant, thereby the weights get sharedfor all the neurons in that output area. Only the inputs vary.\n4. By combining all of these convolutional outputs, we get one convolutional layer\n5. Instead of treating this Convolutional Layer as a flat vector, we treat it as a volume, whosedepth is given by the number of kernels used to process the input.\n6. By this practice, we are effectively reducing the number of parameters yet still retaining modelcomplexity, thereby overcoming one of the shortcomings of DNNs.\n\n","88566e5f":"### LeNet Class\nParameters of Conv2d\n> torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')","10a0aa8e":"As per definition\nThe convolution of f and g is written f\u2217g, denoting the operator with the symbol '\u2217'.<br\/>It is defined as the integral of the product of the two functions after one is reversed and shifted. As such, it is a particular kind of integral transform:\n![Capture.PNG](attachment:Capture.PNG)\n\nLet us try to understand this better, with an example <br\/>\n\nAssume that we are trying to measure the distance of a vehicle at regular intervals<br\/>\n$$x_0\\ at\\ t_0$$\n$$x_1\\ at\\ t_1$$\n$$x_2\\ at\\ t_2$$\nbut it so happens, that I am not really confident about my device, and to compenstate for that I take the average at multiple points resulting in distance at $t_2$ being $\\frac{1}{3}*{(t_2+t_1+t_0)}$<br\/>\n\nHowever, if we think about it furthur ,we should try giving the most importance to the current reading, and a progressively decreasing level of importance to every reading preceding the current one. To implement this idea of importance we will assosiate weights to the distances<br\/>\n\n$$x_2 \u2212> w_{0}\\ (\\ 0\\ indicates\\ current\\ reference\\ point\\ )$$\n$$x_1 \u2212> w_{-1}\\ (\\ 1\\ reading\\ before\\ reference\\ point\\ )$$\n$$x_0 \u2212> w_{-2}\\ (\\ 2\\ reading\\ before\\ reference\\ point\\ )$$\n$$such\\ that\\ w_0>w_{-1}>w_{-2} $$\n\nSo the new overall new distance  $xnew_{2}$ would be calculated by $w_{-2}x_0+w_{-1}x_{1}+w_{0}x_{2}$ where the weights\nare decreasing from $w_0$<br\/>\n\nHence the formula now becomes $$x_t=\\sum_{0}^{3}{w_{-a}x_{t-a}}$$\nNow I want to include more than 3 previous reading for calculating my present new reading and it so happens that\nI have been recording the values for a really long time, so i make the appropriate adjustment to my formula\n$$x_{t}=\\sum_{0}^{\\infty}{w_{-a}x_{t-a}}$$\n\nCongratulations !! we have reached to a disrcreate representation of a convolution operation. If we are to take readings in a continuous manner we would obtain the integral form of operation\n$$x_t=\\int_{0}^{\\infty}{w_{-a}x_{t-a}}da$$\n\n### Going on a tangent , some interesting applications of 1d convolutions\n1. [1D Convolutional Neural Networks for Time Series Modeling - Nathan Janos, Jeff Roach](https:\/\/www.youtube.com\/watch?v=nMkqWxMjWzg)\n2. [End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network](https:\/\/www.groundai.com\/project\/end-to-end-environmental-sound-classification-using-a-1d-convolutional-neural-network\/1)","4017009b":"# Fully Connected Neural Networks \nBefore jumping into CNNs, it is important to understand what came before them, the fully Connected Neural Networks.<br\/>\nFully Connected Neural Networks: Any neuron in a given layer is fully connected to all the\nneurons in the previous layer\n\nPros :\n1. The Universal Approximation Theorem says that DNNs are power function approximators We can come up with a neural network of output f(x) which is very close to the true output f(x)\n2. Easy to train \n\nCons :\n1. DNNs are prone to overfitting (too many parameters).Even a slight change in the training set could cause the model to arrive at very different weight configurations\n2. Gradients can vanish due to long chains. Vanishing gradient problem could occur in the case of saturated neurons\n\nIt was because of these cons that reseach was put into better Optimization Algorithms, Better Activation Functions, and we came to questioning if we could make DNNs which are complex (many non-linearities) but have fewer parameters and\nhence less prone to overfitting?\n","029855d5":"### Importing stuff","c9a64620":"## Convolutional Terminologies and relationship","57c64d69":"## 1D Convolution ","72901f1b":"## Applying some well known convolutional kernels","86a4da51":"## Understanding 2D convolution for a colored image\nAs we have discussed a colored image has 3 channels , a reg green and blue color channel. It is hence important to understand that our kernal will also have a 3 layer depth to it.\n![ab.gif](attachment:ab.gif)\n![1_CYB2dyR3EhFs1xNLK8ewiA.gif](attachment:1_CYB2dyR3EhFs1xNLK8ewiA.gif)","69db6930":"#### Loading Data\n[Guide to dataset and trainloader in Pytorch](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html)","082746dd":"All Images are composed of pixels which can take a range of values between 0 to 255 <br\/>\nWhile a black and white image can be represented in a 2 dimensional arrangement of pixels\n![0.jpg](attachment:0.jpg)\n\nA colored RGB image ,has three 2D arrangement of pixels of red , green and blue color\n![1_lRpx5pTrVewFTD8YXjhIKA.png](attachment:1_lRpx5pTrVewFTD8YXjhIKA.png)\n","41c7db61":"# CNN Architectures\n## LeNet\nLeNet5 was one of the earliest convolutional neural networks and promoted the development of deep learning. Since 1988, after years of research and many successful iterations, the pioneering work has been named LeNet5.\n\nThe LeNet5 architecture was fundamental, in particular the insight that image features are distributed across the entire image, and convolutions with learnable parameters are an effective way to extract similar features at multiple location with few parameters. At the time there was no GPU to help training, and even CPUs were slow. Therefore being able to save parameters and computation was a key advantage. This is in contrast to using each pixel as a separate input of a large multi-layer neural network. LeNet5 explained that those should not be used in the first layer, because images are highly spatially correlated, and using individual pixel of the image as separate input features would not take advantage of these correlations.\n![1_1TI1aGBZ4dybR6__DI9dzA-min.png](attachment:1_1TI1aGBZ4dybR6__DI9dzA-min.png)\n\n### Understanding components of LeNet\n\n1. Convolution Layer 1\n\n    1. Here, the filter size F = 5, and the central cell is the pixel of interest\n    2. Stride length S = 1\n    3. We use a total of 6 filters, i.e. K = 6\n    4. No padding is used, i.e. P = 0\n    5. Each of the filters generate 28x28 output (calculated using $W_0$, $H_0$ formula)\n        1. $W_0 = \\frac{32 +2*0 - 5}{1}+1$=28\n        2. $H_0 = \\frac{32 +2*0 - 5}{1}+1$=28\n        3. $D_0=K=6$\n    6. Non-linearity like tanh or ReLU(preferred for CNN) \n    \n    \n2. Max Pooling Layer 1\n\n    1. Filter size F = 2\n    2. Stride length S = 2\n    3. No. of filters K = 6\n    4. Padding P = 0\n    5. $W_0 = \\frac{28 +2*0 - 2}{2}+1$=14\n    6. $H_0 = \\frac{28 +2*0 - 2}{2}+1$=14\n    7. $D_0=K=6$\n    \n    \n3. Convolutional Layer 2\n\n    1. Filter size F = 5\n    2. Stride length S = 1\n    3. No. of filters K = 16\n    4. Padding P = 0\n    5. $W_0 = \\frac{14 +2*0 - 5}{1}+1$=10\n    6. $H_0 = \\frac{14 +2*0 - 5}{1}+1$=10\n    7. $D_0=K=16$\n    \n4. Max Pooling Layer 2\n    1. Filter size F = 2\n    2. Stride length S = 2\n    3. No. of filters K = 16\n    4. $W_0 = \\frac{10 +2*0 - 2}{2}+1$=5\n    5. $H_0 = \\frac{10 +2*0 - 2}{2}+1$=5\n    6. $D_0=K=16$\n    \n5. Fully connected layer \n    1. Result after Max Pooling Layer 2 is flattened resulting in 5*5*16 =400 inputs\n    2. Number of neurons: 120\n    3. Number of neurons: 84\n    4. Number of neurons: 10\n\n\n","712f621d":"In a nutshell, the convolution operation boils down to taking a given input and re-estimating it as\na weighted average of all the inputs around it.\nThe above definition is easy to visualise in 1D, but what about 2D?<br\/>\n\nSince images are presented in a 2D representation of pixels, the weights we will use will also have a 2D representation.<br\/>\nWe go from a 1 D representation<br\/>\n\n|     |  |\n| ----------- | ----------- |\n| $w_1$      | $w_2$        | $w_3$ |\n\nto a 2D representation of weights known as \"kernal\"\n\n|     |  |\n| ----------- | ----------- |\n| $w_{11}$      | $w_{12}$        | $w_{13}$ |\n| $w_{21}$      | $w_{22}$        | $w_{23}$ |\n| $w_{31}$      | $w_{32}$        | $w_{33}$ |\n<br\/>\nIn 2D, we would consider neighbors along the rows and columns, using the following formula :\n$$\\sum_{a=0}^{m-1}\\sum_{b=0}^{n-1}I_{i+a,j+b}*K_{a,b}$$\n$$Where\\ K\\ refers\\ to\\ kernel\\ or\\ weights\\ and\\ I\\ refers\\ to\\ the\\ input\\  and\\ *\\ refers\\ to\\ the\\ convolution\\ operation$$\n\n|   K kernal  |  |\n| ----------- | ----------- |\n| $w_{00}$      | $w_{01}$        |\n| $w_{10}$      | $w_{11}$        | \n\n1. Let a be the number of rows and b be the number of columns\n2. m & n specify the size of the matrix, in this case we consider them to be 2 each. So it\u2019s a 2x2 matrix. Therefore a & b range from 0-1 each.\n3. Now, to calculate the new value at a particular pixel Ii,j, we simply need to fill in the values ino the formula.\n\n$$s_{ij} = I_{i+0,j+0}K_{0,0} + I_{i+0,j+1}K_{0,1} + I_{i+1,j+0}K_{1,0} + I_{i+1,j+1}K_{1,1}$$\n\n\n\n\n\nOur kernal will slide across an image, to calculate the required value <br\/>\n\n![nb.gif](attachment:nb.gif)\n\n\n### Examples of 2D convolution\nKernals are used for variety of tasks in image processing, and we will now look at some well known kernals and their results and try to perform the operation in python\n![kernals-min.PNG](attachment:kernals-min.PNG)\n","8031abe1":" ### 2D convolution operation "}}