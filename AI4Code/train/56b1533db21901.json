{"cell_type":{"0397ffa1":"code","8a78d72e":"code","ed3b25a9":"code","84637dee":"code","ee4d56e0":"code","26ce8418":"code","ce424573":"code","20824b61":"code","3835e032":"code","424b70ff":"code","cf8c5900":"code","3abf54e8":"code","c0e0e985":"code","a1688a82":"code","112881d0":"code","3d4cac58":"code","8c1cc5e3":"code","d402fc87":"code","3eb1a39c":"code","0c78313b":"code","9f40dda2":"code","ab615c3b":"code","5b00b4b4":"code","5c3242be":"code","8d75616e":"code","e52c86c3":"code","f3452548":"code","b7c329cb":"code","27164c87":"code","30f9ab0e":"code","63970a8e":"code","2aa98114":"code","8de3703b":"code","cedec19b":"code","55fa1346":"code","2a607d32":"code","8cd04692":"code","68ca4e49":"code","9041351e":"code","3ea5dec1":"code","443fd8fd":"code","d2df7e1a":"code","aedff8d2":"code","5faeecc7":"code","c26964be":"code","5e1733af":"code","f8e79ce9":"code","f802ec7e":"code","5362fb83":"code","eb8531f4":"code","300320c1":"markdown","467938e8":"markdown","f778ab58":"markdown","5e97492d":"markdown","08544c76":"markdown","2749f501":"markdown","4626b167":"markdown","6059168e":"markdown","18e3c288":"markdown","d3e8b9fa":"markdown","ba2d752c":"markdown","e2e742d2":"markdown","b29da9cc":"markdown","7c81ea10":"markdown","73238e64":"markdown","a672129a":"markdown","1543ad09":"markdown","d23d0ab1":"markdown","7ad1ab23":"markdown"},"source":{"0397ffa1":"import warnings\nwarnings.filterwarnings(\"ignore\")","8a78d72e":"import pandas as pd\ndataset=pd.read_csv(\"..\/input\/dataset.csv\")","ed3b25a9":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np\nimport seaborn as sns\n\n# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","84637dee":"plotPerColumnDistribution(dataset, 10, 5)","ee4d56e0":"#split the data into train and test set\nfrom sklearn.model_selection         import train_test_split\ntext = dataset.Text\nlanguage = dataset.language\ntrain_features, test_features, train_labels, test_labels = train_test_split(text, language, test_size=0.20, random_state=5)","26ce8418":"print('lenght of training data = ',len(train_features))\nprint('lenght of test data = ', len(test_features))","ce424573":"from sklearn.feature_extraction.text import*\nfrom sklearn                         import preprocessing\nfrom sklearn.preprocessing           import LabelEncoder","20824b61":"#uni gram\nuni_vector =   CountVectorizer( strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', \n                          stop_words=None, ngram_range=(1,1), max_features=1000)\nbag_of_words_uni = uni_vector.fit_transform(train_features)\nbag_of_words_uni.shape","3835e032":"#bigram\nbi_vector =   CountVectorizer( strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', \n                          stop_words=None, ngram_range=(2,2), max_features=1000)\nbag_of_words_bi = bi_vector.fit_transform(train_features)","424b70ff":"#trigram\ntri_vector =   CountVectorizer( strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', \n                          stop_words=None, ngram_range=(3,3), max_features=1000)\nbag_of_words_tri = tri_vector.fit_transform(train_features)","cf8c5900":"#3chargram\nchar3_vector =   CountVectorizer( strip_accents='unicode', analyzer='char', token_pattern=r'\\w{1,}', \n                          stop_words=None, ngram_range=(3,3), max_features=1000)\nbag_of_words_char3 = char3_vector.fit_transform(train_features)","3abf54e8":"#4chargram\nchar4_vector =   CountVectorizer( strip_accents='unicode', analyzer='char', token_pattern=r'\\w{1,}', \n                          stop_words=None, ngram_range=(4,4), max_features=1000)\nbag_of_words_char4 = char4_vector.fit_transform(train_features)","c0e0e985":"#5chargram\nchar5_vector =   CountVectorizer( strip_accents='unicode', analyzer='char', token_pattern=r'\\w{1,}', \n                          stop_words=None, ngram_range=(5,5), max_features=1000)\nbag_of_words_char5 = char5_vector.fit_transform(train_features)","a1688a82":"#6chargram\nchar6_vector =   CountVectorizer( strip_accents='unicode', analyzer='char', token_pattern=r'\\w{1,}', \n                          stop_words=None, ngram_range=(6,6), max_features=1000)\nbag_of_words_char6 = char6_vector.fit_transform(train_features)","112881d0":"#7chargram\nchar7_vector =   CountVectorizer( strip_accents='unicode', analyzer='char', token_pattern=r'\\w{1,}', \n                          stop_words=None, ngram_range=(7,7), max_features=1000)\nbag_of_words_char7 = char7_vector.fit_transform(train_features)","3d4cac58":"#8chargram\nchar8_vector =   CountVectorizer( strip_accents='unicode', analyzer='char', token_pattern=r'\\w{1,}', \n                          stop_words=None, ngram_range=(1,1), max_features=1000)\nbag_of_words_char8 = char8_vector.fit_transform(train_features)","8c1cc5e3":"#9chargram\nchar9_vector =   CountVectorizer( strip_accents='unicode', analyzer='char', token_pattern=r'\\w{1,}', \n                          stop_words=None, ngram_range=(9,9), max_features=1000)\nbag_of_words_char9 = char9_vector.fit_transform(train_features)","d402fc87":"#10chargram\nchar10_vector =   CountVectorizer( strip_accents='unicode', analyzer='char', token_pattern=r'\\w{1,}', \n                          stop_words=None, ngram_range=(10,10), max_features=1000)\nbag_of_words_char10 = char10_vector.fit_transform(train_features)","3eb1a39c":"# Get feature names\nuni_feature_names = uni_vector.get_feature_names()\nbi_feature_names = bi_vector.get_feature_names()\ntri_feature_names = tri_vector.get_feature_names()\nchar3_feature_names = char3_vector.get_feature_names()\nchar4_feature_names = char4_vector.get_feature_names()\nchar5_feature_names = char5_vector.get_feature_names()\nchar6_feature_names = char6_vector.get_feature_names()\nchar7_feature_names = char7_vector.get_feature_names()\nchar8_feature_names = char8_vector.get_feature_names()\nchar9_feature_names = char9_vector.get_feature_names()\nchar10_feature_names = char10_vector.get_feature_names()","0c78313b":"uni_train_features=pd.DataFrame(bag_of_words_uni.toarray(), columns=uni_feature_names)\nuni_train_features[:5]","9f40dda2":"bi_train_features=pd.DataFrame(bag_of_words_bi.toarray(), columns=bi_feature_names)\ntri_train_features=pd.DataFrame(bag_of_words_tri.toarray(), columns=tri_feature_names)\nchar3_train_features=pd.DataFrame(bag_of_words_char3.toarray(), columns=char3_feature_names)\nchar4_train_features=pd.DataFrame(bag_of_words_char4.toarray(), columns=char4_feature_names)\nchar5_train_features=pd.DataFrame(bag_of_words_char5.toarray(), columns=char5_feature_names)\nchar6_train_features=pd.DataFrame(bag_of_words_char6.toarray(), columns=char6_feature_names)\nchar7_train_features=pd.DataFrame(bag_of_words_char7.toarray(), columns=char7_feature_names)\nchar8_train_features=pd.DataFrame(bag_of_words_char8.toarray(), columns=char8_feature_names)\nchar9_train_features=pd.DataFrame(bag_of_words_char9.toarray(), columns=char9_feature_names)\nchar10_train_features=pd.DataFrame(bag_of_words_char10.toarray(), columns=char10_feature_names)","ab615c3b":"from sklearn.ensemble                import RandomForestClassifier\n\n#apply RandomForestClassifier on train dataset\nrfc_uni = RandomForestClassifier()                      \nrfc_uni.fit(uni_train_features, train_labels);\n\nrfc_bi = RandomForestClassifier()                      \nrfc_bi.fit(bi_train_features, train_labels);\n\nrfc_tri = RandomForestClassifier()                      \nrfc_tri.fit(tri_train_features, train_labels);\n\nrfc_char3 = RandomForestClassifier()                      \nrfc_char3.fit(char3_train_features, train_labels);\n\nrfc_char4 = RandomForestClassifier()                      \nrfc_char4.fit(char4_train_features, train_labels);\n\nrfc_char5 = RandomForestClassifier()                      \nrfc_char5.fit(char5_train_features, train_labels);\n\nrfc_char6 = RandomForestClassifier()                      \nrfc_char6.fit(char6_train_features, train_labels);\n\nrfc_char7 = RandomForestClassifier()                      \nrfc_char7.fit(char7_train_features, train_labels);\n\nrfc_char8 = RandomForestClassifier()                      \nrfc_char8.fit(char8_train_features, train_labels);\n\nrfc_char9 = RandomForestClassifier()                      \nrfc_char9.fit(char9_train_features, train_labels);\n\nrfc_char10 = RandomForestClassifier()                      \nrfc_char10.fit(char10_train_features, train_labels);","5b00b4b4":"from sklearn.svm                     import LinearSVC\n#apply LinearSVC() on train dataset\nlsvc_uni = LinearSVC()                      \nlsvc_uni.fit(uni_train_features, train_labels);\n\nlsvc_bi = LinearSVC()                      \nlsvc_bi.fit(bi_train_features, train_labels);\n\nlsvc_tri = LinearSVC()                      \nlsvc_tri.fit(tri_train_features, train_labels);\n\nlsvc_char3 = LinearSVC()                     \nlsvc_char3.fit(char3_train_features, train_labels);\n\nlsvc_char4 = LinearSVC()                     \nlsvc_char4.fit(char4_train_features, train_labels);\n\nlsvc_char5 = LinearSVC()                     \nlsvc_char5.fit(char5_train_features, train_labels);\n\nlsvc_char6 = LinearSVC()                     \nlsvc_char6.fit(char6_train_features, train_labels);\n\nlsvc_char7 = LinearSVC()                     \nlsvc_char7.fit(char7_train_features, train_labels);\n\nlsvc_char8 = LinearSVC()                     \nlsvc_char8.fit(char8_train_features, train_labels);\n\nlsvc_char9 = LinearSVC()                     \nlsvc_char9.fit(char9_train_features, train_labels);\n\nlsvc_char10 = LinearSVC()                     \nlsvc_char10.fit(char10_train_features, train_labels);","5c3242be":"from sklearn.linear_model            import LogisticRegression\n#apply LogisticRegression() on train dataset\nlr_uni = LogisticRegression()                      \nlr_uni.fit(uni_train_features, train_labels);\n\nlr_bi = LogisticRegression()                      \nlr_bi.fit(bi_train_features, train_labels);\n\nlr_tri = LogisticRegression()                      \nlr_tri.fit(tri_train_features, train_labels);\n\nlr_char3 = LogisticRegression()                     \nlr_char3.fit(char3_train_features, train_labels);\n\nlr_char4 = LogisticRegression()                     \nlr_char4.fit(char4_train_features, train_labels);\n\nlr_char5 = LogisticRegression()                     \nlr_char5.fit(char5_train_features, train_labels);\n\nlr_char6 = LogisticRegression()                     \nlr_char6.fit(char6_train_features, train_labels);\n\nlr_char7 = LogisticRegression()                     \nlr_char7.fit(char7_train_features, train_labels);\n\nlr_char8 = LogisticRegression()                     \nlr_char8.fit(char8_train_features, train_labels);\n\nlr_char9 = LogisticRegression()                     \nlr_char9.fit(char9_train_features, train_labels);\n\nlr_char10 = LogisticRegression()                     \nlr_char10.fit(char10_train_features, train_labels);","8d75616e":"uni_test_features = uni_vector.transform(test_features)\nuni_test_features=pd.DataFrame(uni_test_features.toarray(), columns=uni_feature_names)\nuni_test_features[:5]","e52c86c3":"bi_test_features = bi_vector.transform(test_features)\nbi_test_features=pd.DataFrame(bi_test_features.toarray(), columns=bi_feature_names)","f3452548":"tri_test_features = tri_vector.transform(test_features)\ntri_test_features=pd.DataFrame(tri_test_features.toarray(), columns=tri_feature_names)","b7c329cb":"char3_test_features = char3_vector.transform(test_features)\nchar3_test_features=pd.DataFrame(char3_test_features.toarray(), columns=char3_feature_names)","27164c87":"char4_test_features = char4_vector.transform(test_features)\nchar4_test_features=pd.DataFrame(char4_test_features.toarray(), columns=char4_feature_names)","30f9ab0e":"char5_test_features = char5_vector.transform(test_features)\nchar5_test_features=pd.DataFrame(char5_test_features.toarray(), columns=char5_feature_names)","63970a8e":"char6_test_features = char6_vector.transform(test_features)\nchar6_test_features=pd.DataFrame(char6_test_features.toarray(), columns=char6_feature_names)","2aa98114":"char7_test_features = char7_vector.transform(test_features)\nchar7_test_features=pd.DataFrame(char7_test_features.toarray(), columns=char7_feature_names)","8de3703b":"char8_test_features = char8_vector.transform(test_features)\nchar8_test_features=pd.DataFrame(char8_test_features.toarray(), columns=char8_feature_names)","cedec19b":"char9_test_features = char9_vector.transform(test_features)\nchar9_test_features=pd.DataFrame(char9_test_features.toarray(), columns=char9_feature_names)","55fa1346":"char10_test_features = char10_vector.transform(test_features)\nchar10_test_features=pd.DataFrame(char10_test_features.toarray(), columns=char10_feature_names)","2a607d32":"from sklearn.metrics                 import accuracy_score","8cd04692":"predictions_uni_rfc = rfc_uni.predict(uni_test_features)         #prediction\nuni_rfc = accuracy_score(test_labels, predictions_uni_rfc)           #accuracy\n\npredictions_bi_rfc = rfc_bi.predict(bi_test_features)         #prediction\nbi_rfc = accuracy_score(test_labels, predictions_bi_rfc)           #accuracy\n\npredictions_tri_rfc = rfc_tri.predict(tri_test_features)         #prediction\ntri_rfc = accuracy_score(test_labels, predictions_tri_rfc)           #accuracy\n\npredictions_char3_rfc = rfc_char3.predict(char3_test_features)         #prediction\nchar3_rfc = accuracy_score(test_labels, predictions_char3_rfc)           #accuracy\n\npredictions_char4_rfc = rfc_char4.predict(char4_test_features)         #prediction\nchar4_rfc = accuracy_score(test_labels, predictions_char4_rfc)           #accuracy\n\npredictions_char5_rfc = rfc_char5.predict(char5_test_features)         #prediction\nchar5_rfc = accuracy_score(test_labels, predictions_char5_rfc)           #accuracy\n\npredictions_char6_rfc = rfc_char6.predict(char6_test_features)         #prediction\nchar6_rfc = accuracy_score(test_labels, predictions_char6_rfc)           #accuracy\n\npredictions_char7_rfc = rfc_char7.predict(char7_test_features)         #prediction\nchar7_rfc = accuracy_score(test_labels, predictions_char7_rfc)           #accuracy\n\npredictions_char8_rfc = rfc_char8.predict(char8_test_features)         #prediction\nchar8_rfc = accuracy_score(test_labels, predictions_char8_rfc)           #accuracy\n\npredictions_char9_rfc = rfc_char9.predict(char9_test_features)         #prediction\nchar9_rfc = accuracy_score(test_labels, predictions_char9_rfc)           #accuracy\n\npredictions_char10_rfc = rfc_char10.predict(char10_test_features)         #prediction\nchar10_rfc = accuracy_score(test_labels, predictions_char10_rfc)           #accuracy","68ca4e49":"predictions_uni_lsvc= lsvc_uni.predict(uni_test_features)         #prediction\nuni_lsvc = accuracy_score(test_labels, predictions_uni_lsvc)           #accuracy\n\npredictions_bi_lsvc = lsvc_bi.predict(bi_test_features)         #prediction\nbi_lsvc = accuracy_score(test_labels, predictions_bi_lsvc)           #accuracy\n\npredictions_tri_lsvc = lsvc_tri.predict(tri_test_features)         #prediction\ntri_lsvc = accuracy_score(test_labels, predictions_tri_lsvc)           #accuracy\n\npredictions_char3_lsvc = lsvc_char3.predict(char3_test_features)         #prediction\nchar3_lsvc = accuracy_score(test_labels, predictions_char3_lsvc)           #accuracy\n\npredictions_char4_lsvc = lsvc_char4.predict(char4_test_features)         #prediction\nchar4_lsvc = accuracy_score(test_labels, predictions_char4_lsvc)           #accuracy\n\npredictions_char5_lsvc = lsvc_char5.predict(char5_test_features)         #prediction\nchar5_lsvc = accuracy_score(test_labels, predictions_char5_lsvc)           #accuracy\n\npredictions_char6_lsvc = lsvc_char6.predict(char6_test_features)         #prediction\nchar6_lsvc = accuracy_score(test_labels, predictions_char6_lsvc)           #accuracy\n\npredictions_char7_lsvc = lsvc_char7.predict(char7_test_features)         #prediction\nchar7_lsvc = accuracy_score(test_labels, predictions_char7_lsvc)           #accuracy\n\npredictions_char8_lsvc = lsvc_char8.predict(char8_test_features)         #prediction\nchar8_lsvc = accuracy_score(test_labels, predictions_char8_lsvc)           #accuracy\n\npredictions_char9_lsvc = lsvc_char9.predict(char9_test_features)         #prediction\nchar9_lsvc = accuracy_score(test_labels, predictions_char9_lsvc)           #accuracy\n\npredictions_char10_lsvc = lsvc_char10.predict(char10_test_features)         #prediction\nchar10_lsvc = accuracy_score(test_labels, predictions_char10_lsvc)           #accuracy","9041351e":"predictions_uni_lr = lr_uni.predict(uni_test_features)         #prediction\nuni_lr = accuracy_score(test_labels, predictions_uni_lr)           #accuracy\n\npredictions_bi_lr = lr_bi.predict(bi_test_features)         #prediction\nbi_lr = accuracy_score(test_labels, predictions_bi_lr)           #accuracy\n\npredictions_tri_lr = lr_tri.predict(tri_test_features)         #prediction\ntri_lr = accuracy_score(test_labels, predictions_tri_lr)           #accuracy\n\npredictions_char3_lr = lr_char3.predict(char3_test_features)         #prediction\nchar3_lr = accuracy_score(test_labels, predictions_char3_lr)           #accuracy\n\npredictions_char4_lr = lr_char4.predict(char4_test_features)         #prediction\nchar4_lr = accuracy_score(test_labels, predictions_char4_lr)           #accuracy\n\npredictions_char5_lr = lr_char5.predict(char5_test_features)         #prediction\nchar5_lr = accuracy_score(test_labels, predictions_char5_lr)           #accuracy\n\npredictions_char6_lr = lr_char6.predict(char6_test_features)         #prediction\nchar6_lr = accuracy_score(test_labels, predictions_char6_lr)           #accuracy\n\npredictions_char7_lr = lr_char7.predict(char7_test_features)         #prediction\nchar7_lr = accuracy_score(test_labels, predictions_char7_lr)           #accuracy\n\npredictions_char8_lr = lr_char8.predict(char8_test_features)         #prediction\nchar8_lr = accuracy_score(test_labels, predictions_char8_lr)           #accuracy\n\npredictions_char9_lr = lr_char9.predict(char9_test_features)         #prediction\nchar9_lr = accuracy_score(test_labels, predictions_char9_lr)           #accuracy\n\npredictions_char10_lr = lr_char10.predict(char10_test_features)         #prediction\nchar10_lr = accuracy_score(test_labels, predictions_char10_lr)           #accuracy","3ea5dec1":"!pip install prettytable","443fd8fd":"from prettytable                     import PrettyTable\nfrom astropy.table                   import Table, Column\nModel_Table = PrettyTable()\nModel_Table.field_names = [\" \", \"   Random forest Classififier  \", \"     Linear SVC       \",\"   Logistic Regression   \"]\nModel_Table.add_row([\"  Uni Gram  \", round(uni_rfc,2),round(uni_lsvc,2),round(uni_lr,2)])\nModel_Table.add_row([\"  Bi Gram  \", round(bi_rfc,2),round(bi_lsvc,2),round(bi_lr,2)])\nModel_Table.add_row([\"  Tri Gram  \", round(tri_rfc,2),round(tri_lsvc,2),round(tri_lr,2)])\nModel_Table.add_row([\"  3 Char Gram  \", round(char3_rfc,2),round(char3_lsvc,2),round(char3_lr,2)])\nModel_Table.add_row([\"  4 Char Gram  \", round(char4_rfc,2),round(char4_lsvc,2),round(char4_lr,2)])\nModel_Table.add_row([\"  5 Char Gram  \", round(char5_rfc,2),round(char5_lsvc,2),round(char5_lr,2)])\nModel_Table.add_row([\"  6 Char Gram  \", round(char6_rfc,2),round(char6_lsvc,2),round(char6_lr,2)])\nModel_Table.add_row([\"  7 Char Gram  \", round(char7_rfc,2),round(char7_lsvc,2),round(char7_lr,2)])\nModel_Table.add_row([\"  8 Char Gram  \", round(char8_rfc,2),round(char8_lsvc,2),round(char8_lr,2)])\nModel_Table.add_row([\"  9 Char Gram  \", round(char9_rfc,2),round(char9_lsvc,2),round(char9_lr,2)])\nModel_Table.add_row([\"  10 Char Gram  \", round(char10_rfc,2),round(char10_lsvc,2),round(char10_lr,2)])\nprint(\"Detailed performance of all models:\")\nprint(Model_Table)","d2df7e1a":"Best_Model = PrettyTable()\nBest_Model.field_names = [\" \", \"   Random forest Classififier  \", \"     Linear SVC          \",\"   Logistic Regression   \"]\nBest_Model.add_row([\"  Uni Gram  \", round(uni_rfc,2),round(uni_lsvc,2),round(uni_lr,2)])\nBest_Model.add_row([\"  3 Char Gram  \", round(char3_rfc,2),round(char3_lsvc,2),round(char3_lr,2)])\nBest_Model.add_row([\"  8 Char Gram  \", round(char8_rfc,2),round(char8_lsvc,2),round(char8_lr,2)])\nprint(\"Best Model\")\nprint(Best_Model)","aedff8d2":"features = uni_vector.transform(dataset.Text)\ntarget=language","5faeecc7":" #apply random forest on train dataset\nmodel=lr_uni.fit(features, target);","c26964be":"# import pickle\n# filename = 'unigram_model.sav'\n# pickle.dump(model, open(filename, 'wb'))","5e1733af":"# # load the model from disk\n# model = pickle.load(open(filename, 'rb'))","f8e79ce9":"# comment =input(\"please enter a text paragraph: \\n\")\n\ncomment = '\u0639\u062f\u0645 \u0639\u0644 \u0639\u064f\u0642\u0631 \u0648\u064a\u062a\u0651\u0641\u0642 \u0627\u0631\u062a\u0643\u0628\u0647\u0627. \u0634\u0639\u0627\u0631 \u0633\u0642\u0637\u062a \u0648\u0641\u064a \u0663\u0660, \u0630\u0627\u062a \u0628\u0627\u0644\u0641\u0634\u0644 \u0648\u0645\u062d\u0627\u0648\u0644\u0629 \u0648\u0627\u0644\u0645\u0639\u062f\u0627\u062a \u0647\u0648. \u0628\u064a\u0646\u0645\u0627 \u0627\u0644\u0623\u0633\u064a\u0648\u064a \u0628\u0631\u064a\u0637\u0627\u0646\u064a\u0627-\u0641\u0631\u0646\u0633\u0627 \u062a\u062d\u062a \u0628\u0644, \u0627\u0646\u0647 \u0623\u0633\u064a\u0627 \u062a\u062d\u0631\u0651\u0643\u062a \u0648\u0627\u0646\u062f\u0648\u0646\u064a\u0633\u064a\u0627\u060c \u0643\u0644. \u0645\u0639 \u0627\u0644\u0625\u0646\u0632\u0627\u0644 \u0627\u0644\u0623\u0633\u064a\u0648\u064a \u0636\u0631\u0628.'\n","f802ec7e":"user_input= uni_vector.transform([comment])\na=user_input.toarray()\nuser_input=pd.DataFrame(a, columns=uni_feature_names)","5362fb83":"# fit the model on input\nlanguage = model.predict(user_input)  ","eb8531f4":"a=[ language[i] for i in [0] ]\nprint(\"Language : \",a)","300320c1":"### Step 6: Evaluate Machine Learning Algorithms using Test Data","467938e8":"## Phase 2 Testing","f778ab58":"#### Step 8.3: Save the Trained Model as Pickle File","5e97492d":"### Step 2: Import preprocessed dataset","08544c76":"## Step 5: Train Machine Learning Algorithms using Train Data\n### Apply RandomForestClassifier on train dataset","2749f501":"### Apply LinearSVC() on train dataset","4626b167":"### Feature Extraction\nThe features were extracted using \n> sklearn CountVectorizer","6059168e":"#           Language Identification\n##                                             >    MultiLingual Text Classification using Machine Learning Algorithms \n## PHASES 1: TRAINING\n### Step 1: Import Libraries","18e3c288":"### Step 4: Seperate features and target","d3e8b9fa":"### Step 9: Make prediction on unseen\/new data\n#### Step 9.1: Load the Trained Model","ba2d752c":"### Apply LogisticRegression() on train dataset","e2e742d2":"#### Step 9.4: Apply Trained Model on Feature Vector of Unseen Data and Output Prediction to user","b29da9cc":"#### Logistic Regression","7c81ea10":"## Phase 3 Application Phase\n### Step 8: Application Phase\n\n#### Step 8.1: Combine Data (Train + Test )","73238e64":"#### Step 9.3: Convert User Input into Feature Vector","a672129a":"#### Step 9.2: Take Input from User","1543ad09":"#### LinearSVC","d23d0ab1":"#### Random forest Classififier","7ad1ab23":"### Step 7: Selection of Best Model\n"}}