{"cell_type":{"ce87ffda":"code","786d29a0":"code","c06761c9":"code","61b3fe21":"code","05d06f8f":"code","2bf22fd2":"code","33dd1dd2":"code","00f6ddec":"code","6023139d":"code","b46b9408":"code","06fc3573":"code","e3c87c72":"code","566735f6":"code","1ce0cd8c":"code","a178993b":"code","4bca5d1d":"code","1f056d46":"markdown","b7e1fb46":"markdown","bacc1326":"markdown","4f7b5226":"markdown","3b67f7bb":"markdown","7750a7f1":"markdown","f235c5cb":"markdown","9a4ba602":"markdown","84293093":"markdown","2acdf0f3":"markdown","69315b50":"markdown"},"source":{"ce87ffda":"%matplotlib inline","786d29a0":"import numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c06761c9":"# np.random.rand gives us uniformly distributed random numbers [0, 1], so multiplying by 10 gives us [0, 10]\nX = 10*np.random.rand(50) \n# np.random.randn gives us standard normal random numbers, so multiplying by 2 gives us N(0, 2)\neps = 2*np.random.randn(50)\n# our final simulated dataset\ny = 8*X+eps","61b3fe21":"plt.scatter(X, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()","05d06f8f":"def loss_function(preds, y):\n    sq_residuals = (y-preds)**2\n    rss = np.sum(sq_residuals)\/len(sq_residuals)\n    return rss","2bf22fd2":"def predict_linear_model(b0, b1, x_to_pred):\n    preds = b0+b1*x_to_pred\n    return preds","33dd1dd2":"def plot_data_and_preds(b0, b1, x_to_pred):\n    preds = predict_linear_model(b0, b1, x_to_pred)\n\n    plt.scatter(X, y)\n    plt.plot(X, preds, c=\"red\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.legend([\"Regression Line\", \"Raw Data\"])\n    plt.show()\n\n    the_loss = loss_function(preds, y)\n    print(\"loss=%s\" % the_loss)\n    return the_loss","00f6ddec":"plot_data_and_preds(0, 2, X)","6023139d":"def plot_loss_function(y, X, n_points=5):\n    # equally spaced array of 5 values between -20 and 20, like the seq function in R\n    beta1s = np.linspace(-20, 20, n_points)\n    losses = []\n    for beta1 in beta1s:\n        print(\"beta1=%s \" % beta1)\n        loss = plot_data_and_preds(0, beta1, X)\n        losses.append(loss)\n    plt.scatter(beta1s, losses)\n    plt.xlabel(\"beta1\")\n    plt.ylabel(\"J\")\n    plt.show()","b46b9408":"#plot_loss_function(y, X)\nplot_loss_function(y, X, 20)","06fc3573":"def gradient_b1(b0, b1, y, X):\n    grad = np.sum(-2.0*X*(y-b0-b1*X))\/len(X)\n    return grad","e3c87c72":"# the gradient at 20 is positive...\ngradient_b1(0, 20, y, X)\n# and at -10 is negative...\ngradient_b1(0, -10, y, X)","566735f6":"def gradient_descent(b0_start, b1_start, y, X, learning_rate=0.01, n_steps=25):\n    b1 = b1_start\n    print(\"b1=%s\" % b1)\n    for i in range(n_steps):\n        grad = gradient_b1(b0_start, b1, y, X)\n        #print \"gradient=%s\" % grad\n        b1 = b1-learning_rate*grad\n        print(\"b1=%s\" % b1)","1ce0cd8c":"# play with different learning rates: 0.00001, 0.1, 10\ngradient_descent(0, 15, y, X, learning_rate=0.01)","a178993b":"# things can go terribly wrong and diverge if the learning rate is too high\ngradient_descent(0, 20, y, X, learning_rate=10)","4bca5d1d":"# a tiny learning rate with converge slowly\ngradient_descent(0, 20, y, X, learning_rate=0.00001, n_steps=100)","1f056d46":"Let's define a function that will take coefficients and a set of x values and return the predictions for a linear model:","b7e1fb46":"# Lecture 2: Gradient Descent","bacc1326":"And finally, a function that will plot our data, a linear model, and return the loss function value:","4f7b5226":"Here, we construct a simulated dataset:\n$$y = 8x+ \\epsilon$$\nwhere\n$$\\epsilon \\sim N(0, 2)$$","3b67f7bb":"This function will return the value of the gradient of the loss function with respect to $\\beta_1$:","7750a7f1":"This notebook will illustrate gradient descent in a simple linear regression example with a single predictor.  It will also illustrate how to construct functions in python.","f235c5cb":"Let's start by importing all of our plotting functionality and numpy:","9a4ba602":"This function will create a bunch of beta values, plot the linear regression line for each, calculate the loss, and make a plot of the loss as a function of beta.  Notice how to specify a default value for a parameter to a function:","84293093":"Let's define a function that takes two arguments-a set of predictions and a set of true y values-and returns the average squared loss:","2acdf0f3":"And finally, this function will run our gradient descent algorithm to get to the minumum value of $\\beta_1$.","69315b50":"Let's take a look at our dataset:"}}