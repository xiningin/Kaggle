{"cell_type":{"671fbcc3":"code","842c771e":"code","81218003":"code","68ae7e12":"code","60b8326b":"code","a02f74bb":"code","f78a4a68":"code","cc5d45d2":"code","fb1a1bef":"code","fdffd350":"code","b8110776":"code","bf95783c":"code","6e75e24a":"code","8c501732":"code","08d65088":"code","0370e162":"code","eb50bd88":"code","5776471a":"code","ac0024bd":"code","83969a6d":"code","018b6e41":"code","fab26543":"code","eb328b1c":"code","294cc4af":"code","fc50bb9f":"code","0a169a29":"code","675cb0ea":"code","6f03856c":"code","47f321ae":"code","52898d5d":"code","5eff0e54":"code","3dad591f":"code","151e4166":"markdown","92be0431":"markdown","1ae343c9":"markdown","0a3ddb95":"markdown","f5c45c09":"markdown","a37ede89":"markdown","b46b7236":"markdown","2a082d52":"markdown","ae076a20":"markdown","efd89845":"markdown"},"source":{"671fbcc3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","842c771e":"# Libraries\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error,r2_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","81218003":"# Reading data\ntrain_data = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","68ae7e12":"train_data.head(5)","60b8326b":"train_data.info()","a02f74bb":"train_data.shape,test_data.shape","f78a4a68":"#Seperating target variable from independent variables\ny_train = train_data[\"target\"]\nX_train = train_data.drop(['target'],axis = 1)","cc5d45d2":"#Seperating continuous and categorical varaibles \n#Train_data\ntrain_col_int = [cname for cname in X_train.columns if (((X_train[cname].dtype)  ==  \"int64\") | ((X_train[cname].dtype)  ==  \"float\")) ]\ntrain_col_obj = [cname for cname in X_train.columns if (X_train[cname].dtype) == \"object\"]\n\n#Test_data\ntest_col_int = [cname for cname in test_data.columns if (((test_data[cname].dtype)  ==  \"int64\") | ((test_data[cname].dtype)  ==  \"float\")) ]\ntest_col_obj = [cname for cname in test_data.columns if (test_data[cname].dtype) == \"object\"]","fb1a1bef":"#Converting Categorical columns into Continuous variable columns\nX_train_cat_to_con = pd.get_dummies(X_train[train_col_obj])","fdffd350":"#TOTAL COLUMNS IN TRAINING  = TOTAL \"int\" COLUMNS(15) + TOTAL \"cat_to_conv\"(56) COLUMNS = 71(15+56)\nprint(len(train_col_int))\nprint(len(train_col_obj))\nprint(len(X_train_cat_to_con.columns))\nprint(len(train_data.columns))\nprint(len((X_train.columns)))","b8110776":"(X_train_cat_to_con).shape","bf95783c":"#combinig int coloumns and converted categorical clumns\nX_train = pd.concat([X_train[train_col_int], X_train_cat_to_con],axis = 1,join = \"inner\")","6e75e24a":"print(len((X_train.columns)))","8c501732":"X_test = pd.concat([test_data[test_col_int],X_train_cat_to_con], axis = 1, join = \"inner\")","08d65088":"X_train.head(5)","0370e162":"X_train.columns","eb50bd88":"\"\"\"Total 75 columns are available.We can move ahead to build model\"\"\"\n#Verifying the X_train shape\nprint(X_train.shape)\n","5776471a":"model = LinearRegression()\nmodel.fit(X_train,y_train)\ny_train_predictions = model.predict(X_train)","ac0024bd":"mae = mean_absolute_error(y_train,y_train_predictions)\nprint(mae)\nprint(r2_score(y_train,y_train_predictions))","83969a6d":"sns.scatterplot(x = train_data.cont1,y = train_data.target )\nplt.show()","018b6e41":"train_data.target.value_counts().sum()","fab26543":"\"\"\"test_predictions = model.predict(X_test)\ny_test = pd.DataFrame({\"id\"  : X_test.index , \"target\": test_predictions})\ny_test.reset_index(inplace =True)\ny_test\"\"\"","eb328b1c":"plt.figure(figsize = (15.,10))\nsns.heatmap(train_data.corr(),annot =True)\nplt.show()","294cc4af":"corr_data = train_data.corr().target","fc50bb9f":"corr_data.index","0a169a29":"top_5_cont_variables = [(corr_data.sort_values(ascending = False)[1:6]).index]","675cb0ea":"print(top_5_cont_variables)","6f03856c":"XX = train_data[['cont11','cont12']]","47f321ae":"XX.head()","52898d5d":"XX_test = test_data[['cont11','cont12']]","5eff0e54":"mdl = LinearRegression()\nmdl.fit(XX,y_train)\npredictions = mdl.predict(XX)\nmae = mean_absolute_error(y_train,predictions)\nprint(mae)\nprint(r2_score(y_train,predictions))","3dad591f":"test_predictions =mdl.predict(XX_test)\noutput = pd.DataFrame({\"id\" : test_data.id, \"target\" : test_predictions})\noutput.to_csv(\".\/submissions.csv\",index = False)","151e4166":"mae = mean_absolute_error(y_train,y_train_predictions)\nprint(\"XGBRegression\")\nprint(\"Mean absolute error\" , mae)\nprint(\"R2_Score Value\" ,r2_score(y_train,y_train_predictions))","92be0431":"## Features Understanding","1ae343c9":"# FUTURE STEPS\nTill now, ran the models(Linear and XGBoost) without any feature reduction(either continuous or categorical) ,scaling etc\nModel \"Mean absolute Error\" is low,However R2_Score is pretty less\nSo I thought better to select the appropriate variables required for model building.For that we need to investigate the fetures of the given data.","0a3ddb95":"**Observation**:\nNo null\/Missing data\n26 columns in training data including target variable","f5c45c09":"output = pd.DataFrame({\"id\": test_data.id,\"target\" : mdl.predict(X_test)})\noutput.to_csv(\".\/submissions.csv\",index = False)","a37ede89":"#Trying RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor","b46b7236":"from xgboost import XGBRegressor\nmdl = XGBRegressor()\nmdl.fit(X_train,y_train)\ny_train_predictions = mdl.predict(X_train)","2a082d52":"y_train_predictions = mdl.predict(X_train)","ae076a20":"mae = mean_absolute_error(y_train,y_train_predictions)\nprint(\"RandomForestClassifier\")\nprint(\"Mean absolute error\" , mae)\nprint(\"R2_Score Value\" ,r2_score(y_train,y_train_predictions))","efd89845":"model = RandomForestRegressor(n_estimators = 100 , max_depth = 5,random_state = 1)\nmodel.fit(X_train,y_train)\ny_train_predictions = model.predict(X_train)"}}