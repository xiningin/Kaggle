{"cell_type":{"22d75d5b":"code","6b45cf4a":"code","5de47c99":"code","82791a59":"markdown","0c610ed2":"markdown","a7cb6b5a":"markdown","c1cb5a0f":"markdown","08c2f5bf":"markdown"},"source":{"22d75d5b":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\npd.set_option('max.columns',80)\n\npreprocessed_train = pd.read_csv('..\/input\/preprocessed-train-data\/preprocessed_train_data.csv')\ntest = pd.read_csv('..\/input\/preprocessed-test-data\/preprocessed_test_data.csv')\ndisplay(preprocessed_train.head())\n'''Split the train data into features and target'''\nx_train, y_train = preprocessed_train[preprocessed_train.columns[:-1]], preprocessed_train[preprocessed_train.columns[-1]]","6b45cf4a":"'''Kernel initializer denotes the distribution in which the weights of the neural networks are initialized'''\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, kernel_initializer='normal', activation='relu'),\n    tf.keras.layers.Dense(256, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(256, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(256, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(384, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(384, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(1, kernel_initializer='normal',activation='linear')\n])\n\nmsle = tf.keras.losses.MeanSquaredLogarithmicError()\nmodel.compile(loss= msle, optimizer='adam', metrics=[msle])\nmodel.fit(x_train.values, y_train.values, epochs=15, batch_size=64, validation_split = 0.2)","5de47c99":"predictions = model.predict(test.values)\npredictions","82791a59":"# Artificial Neural Network (ANN)\nANN is a deep learning algorithm that emerged and evolved from the idea of **Biological Neural Networks of human brains**. An attempt to simulate the workings of the humain brain resulted in ANN. ANN works very similar to the biological neural networks but doesn't exactly resemble it's workings. \n\nANN algorithm would accept only numeric and structured data. To accept unstructured and non-numeric data formats such as Image and Text, **Convolutional Neural Networks (CNN)** and **Recursive Neural Networks (RNN)** are used respectively.\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/4\/46\/Colored_neural_network.svg\/280px-Colored_neural_network.svg.png)\n[Image source](https:\/\/en.wikipedia.org\/wiki\/Artificial_neural_network#History)\n* Structure of ANN consists of an input layer, an output layer and many hidden layers. \n* Each layer could have any number of neurons\/ units greater than or equal to one. Each layer could have different activation functions as well.\n* ANN consists of two phases **Forward propagation and Back propagation**. The **forward propagation** involves multiplying weights, adding bias and applying activation function to the inputs and propagating it forward.\n* The **back propagation** step is the most important step which usually involves finding optimal parameters for the model by propagating in the backward direction of Neural network. The back propagation requires optimization function to find the optimal weights for the model.\n* ANN can be applied to both **Regression and Classification tasks** by changing the output layers accordingly.\n\n## Biological neurons vs Artificial neurons\n### Structure of Biological neurons\n* **Dendrites** receive incoming signals.\n* **Soma** (cell body) is responsible for processing the input and carries biochemical information. \n* **Axon** is a tubular in structure responsible for transmission of signals.\n* **Synapse** is present at the end of axon and is responsible for connecting other neurons.\n![](https:\/\/miro.medium.com\/max\/1400\/1*K1ee1SzB0lxjIIo7CGI7LQ.png)\n\n[Image source](https:\/\/miro.medium.com\/max\/1400\/1*K1ee1SzB0lxjIIo7CGI7LQ.png)\n\n### Structure of Artificial neurons\n* A neural network with single layer is called **perceptron**.\n* Each of the input features is multiplied with corresponding weights and bias is added.\n* The results are produced by applying an **Activation function**.\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*6lRzmYq4CSjtIGmaIyHRkg.jpeg)\n\n[Image source](https:\/\/towardsdatascience.com\/what-is-the-relation-between-artificial-and-biological-neuron-18b05831036)\n\n\n## Why Neural Networks?\n* Traditional Machine Learning algorithms tend to perform in the same level when the data size increases but ANN outperforms traditional ML algorithms when the **data size is huge** as shown in the graph below.\n![](https:\/\/www.researchgate.net\/profile\/Benoit_Gallix\/publication\/324457640\/figure\/fig1\/AS:622298201595905@1525378861825\/Graph-illustrating-the-impact-of-data-available-on-performance-of-traditional-machine.png)\n[Image source](https:\/\/www.researchgate.net\/figure\/Graph-illustrating-the-impact-of-data-available-on-performance-of-traditional-machine_fig1_324457640)\n* Feature Learning. The ANN tries to learn hierarchically in an incremental manner layer by layer. Due to this reason it is not necessary to perform feature engineering explicitly.\n\n","0c610ed2":"# Neural Architecture Used\n* The ANN model below consists of seven layers including one input layer, one output layer and five hidden layers.\n* The first layer (input layer) consists of 128 units\/ neurons with ReLU activation function.\n* The second, third and fourth layer consists of 256 hidden units\/ neurons with ReLU activation function.\n* The fifth and sixth layer consists of 384 hidden units with ReLU activation function.\n* The last layer (output layer) consists of one single neuron which outputs an array with the shape (1, N) where N is the number of features.\n\n## Metrics\n* A metric is used to assess the performance of the model.\n* Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. \nNote that you may use any loss function as a metric.\n* We have used Mean Squared Logarithmic Error as a metric and loss function.\n\n$MSLE = \\frac{1}{n} \\sum\\limits_{i=1}^{n}((\\log{y_{i}} +1) - (\\log{\\hat{y}} +1))^2$\n\n$RMSLE = \\sqrt{\\frac{1}{n} \\sum\\limits_{i=1}^{n}((\\log{y_{i}} +1) - (\\log{\\hat{y}} +1))^2}$\n\n## Hyperparameters\n\n* Hyperparameters are the **tunable parameters** that are not produced by a model which means the users must provide a value for these parameters.\n* Hyperparameters values that we provide affect the training process so Hyperparameter optimization comes to the rescue.\n* The Hyperparameters used in this ANN model are,\n    * Number of layers\n    * Number of units\/ neurons in a layer\n    * Activation function\n    * Initialization of weights\n    * Loss function\n    * Metric\n    * Optimizer\n    * Number of epochs\n    * Batch size\n    \n## Epochs\n* A single pass through the training data is called an **epoch**. The training data is fed to the model in mini batches and when all the mini batches of the training data are fed to the model that constitutes to an epoch.","a7cb6b5a":"# How ANN works\nThe working of ANN consists of mainly two phases,\n* **Forward Propagation**\n* **Back Propagation**\n\n## Forward Propagation\n* Forward propagation involves multiplying features with weights, adding bias and then applying an activation function to each neuron in the neural network. \n* Multiplying features with weights and adding bias to each neuron is basically applying **Linear Regression**. If we apply Sigmoid\/Logistic function to it then each neuron is basically performing a **Logistic Regression**.\n\n$\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .... + \\beta_nx_n$\n\n$\\beta_0 $ = *Bias*\n\n$\\beta_i $ = *Weights\/ coefficients*\n\n$x_i$ = *Features*\n\n* If we apply **Sigmoid activation function** to $\\hat{y}$ then the resultant function would look like,\n\n$\\sigma{(\\hat{y})} = \\frac{1}{1+ e^{-{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .... + \\beta_nx_n}}} =\\frac{1}{1+ e^{-\\hat{y}}}$\n\n\n### Activation functions\n* An activation function is used to introduce non-linearity to the data to identify the underlying patterns. It is also used to scale the value to a particular interval. For example sigmoid activation function scales the value between 0 and 1. \n\n#### Logistic or Sigmoid function\n* Logistic\/ Sigmoid function scales the values between 0 and 1. \n* It is used in the output layer for Binary classification.\n* It may cause vanishing gradient problem during backpropagation and slows the training time.\n\n$f(x) = \\frac{1}{1+ e^{-x}}$\n\n![](https:\/\/miro.medium.com\/max\/4000\/1*JHWL_71qml0kP_Imyx4zBg.png)\n\n#### Tanh function\n* Tanh function scales the values between -1 and 1.\n\n$f(x) = tanh(x) = \\frac{(e^x - e^{-x})}{(e^x + e^{-x})}$\n\n![](https:\/\/i.stack.imgur.com\/vxfdW.png)\n\n\n#### ReLU function\n* ReLU (Rectified Linear Unit) outputs the same number if x>0 and outputs 0 if x<0.\n* It prevents vanishing gradient problem but introduces exploding gradient problem during back propagation.\n\n$f(x) =\n\\left\\{\n    \\begin{array}{ll}\n        0  & \\mbox{for } x < 0 \\\\\n        { x} & \\mbox{for } x \\geq 0\n    \\end{array}\n\\right.$\n\n![](https:\/\/static.packt-cdn.com\/products\/9781786469786\/graphics\/B05478_03_11.png)\n\n#### Leaky ReLU function\n* Leaky ReLU is very much similar to ReLU but when x<0 it returns (0.01 * x) instead of 0.\n* If the data is normalized using Z-Score it may contain negative values and ReLU would fail to consider it but leaky ReLU overcomes this problem.\n\n$f(x) =\n\\left\\{\n    \\begin{array}{ll}\n        0.01x  & \\mbox{for } x < 0 \\\\\n        { x} & \\mbox{for } x \\geq 0\n    \\end{array}\n\\right.$\n\n","c1cb5a0f":"## Back propagation\n\n* Back propagation is done to find the **optimal parameters** for the model by iteratively updating parameters by differentiating gradients of loss function with respect to the parameters.\n* Any of the optimizers could be used like,\n    * Gradient Descent\n    * Adam optimizer\n    * Gradient Descent with momentum\n    * RMS Prop\n    \n* Chain rule of Calculus plays an important role in back propagation. The formula below denotes partial differentiation of Loss (L) with respect to Weights (w). A small change in weights w affects the equation z ($\\frac{\\partial{z}}{\\partial{w}}$)\n\n$\\frac{\\partial{L}}{\\partial{w}} = \\frac{\\partial{z}}{\\partial{w}} \\times \\frac{\\partial{a}}{\\partial{z}} \\times \\frac{\\partial{L}}{\\partial{a}} $\n\n* L - Loss function\n    * **Example:** **Cross entropy loss** L = -y log($\\hat{y}$) + (1 - y) log(1 - $\\hat{y}$)\n* w - weights\n* z - Linear regression \n    * **Example**: **z = $\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .... + \\beta_nx_n$**\n* a - activation function used.\n    * **Example**: **a = $\\sigma{(z)} = \\sigma{(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .... + \\beta_nx_n)}$**\n","08c2f5bf":"# Load the preprocessed data\nThe data you feed to the **ANN** must be preprocessed thouroughly to yield reliable results. The training data has been preprocessed already. The preprocessing steps involved are,\n\n* MICE Imputation\n* Log transformation\n* Square root transformation\n* Ordinal Encoding\n* Target Encoding\n* Z-Score Normalization\n\nFor detailed implementation of the above mentioned steps refer my notebook on data preprocessing:\n\n[Notebook Link](https:\/\/www.kaggle.com\/srivignesh\/data-preprocessing-for-house-price-prediction)"}}