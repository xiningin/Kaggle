{"cell_type":{"0c226aaf":"code","1be2a0e5":"code","e7b813ea":"code","e5088402":"code","cf23bfaa":"code","87affd44":"code","01d867a2":"code","2a41c42f":"code","7960350d":"code","e2ae5255":"code","83ed9d29":"code","c1a02747":"code","2200c016":"code","517bcd3b":"code","fb2c2954":"code","06f9c336":"code","6c42d0c1":"code","efc02adc":"code","65232b9c":"code","08bd70c3":"code","b2831d31":"code","fe761aec":"code","2df35600":"code","597fb8f5":"code","69e72cc8":"code","8111d5f8":"code","7185d578":"code","6fcb1d5f":"code","0d057c56":"code","24424cdc":"code","a054597d":"code","e5cd0ba0":"code","534cae47":"code","c063e075":"code","88c685b8":"code","b297afa9":"code","e4315976":"code","a9caf3dc":"code","684f4f0a":"code","d87953cc":"code","7717e9bf":"code","0ce7afdb":"code","ff5a4a0c":"code","71e5f886":"code","1075ac8d":"code","c2efc185":"code","76020ef9":"code","04031a14":"code","ba3fa02b":"code","d009564b":"code","825f96f4":"markdown","07ebc9d0":"markdown","6b13229c":"markdown","67eb8180":"markdown","73d2d658":"markdown","34ca7575":"markdown","d0b7631a":"markdown","f30c7f46":"markdown","0be33f16":"markdown","655a2654":"markdown","251cbd13":"markdown","2bb6effe":"markdown","fa4a3d99":"markdown","667a01d4":"markdown","215186d0":"markdown","a936b1b4":"markdown","71e515f3":"markdown","3de578c2":"markdown","edda5786":"markdown","418fdbb4":"markdown","29dbeef4":"markdown","2ccbfe2d":"markdown","d29923c2":"markdown","85d18444":"markdown","eafa57ef":"markdown","94ae5878":"markdown"},"source":{"0c226aaf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1be2a0e5":"!pip install seaborn --upgrade","e7b813ea":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set()\nsns.__version__","e5088402":"df = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","cf23bfaa":"df.set_index('id', inplace=True)","87affd44":"for col in df.columns[:-1]:\n    sns.displot(df, x=col, hue='stroke')\n    plt.show()","01d867a2":"for col in df.columns:\n    if df[col].isnull().sum() > 0:\n        print(col,':', df[col].isnull().sum(), 'nan values')","2a41c42f":"for col in df[['age', 'avg_glucose_level']].columns:\n    print(f'bmi corr with {col}: ', df['bmi'].corr(df[col]))","7960350d":"df['bmi'].describe()","e2ae5255":"sns.displot(df[df['bmi'].isnull()], x='stroke')","83ed9d29":"p1 = df[df['bmi'].isnull()]['stroke'].sum() \/ len(df[df['bmi'].isnull()]) * 100\np2 = df['stroke'].sum() \/ len(df) * 100\nprint(f'{p1.round()}% of subjects with unreported bmi had a stroke')\nprint(f'{p2.round()}% of subjects in data set had a stroke')","c1a02747":"from sklearn.model_selection import train_test_split\n\nX = df.drop('stroke', axis=1)\ny = df['stroke']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2, stratify=y)","2200c016":"train = X_train.join(y_train)\ntest = X_test.join(y_test)\ndfs = [train, test]","517bcd3b":"train['bmi_unreported'] = np.where(train['bmi'].isnull(), 1, 0)\ntest['bmi_unreported'] = np.where(test['bmi'].isnull(), 1, 0)","fb2c2954":"print('train bmi median : ', train['bmi'].median())\nprint('test bmi median : ', test['bmi'].median())","06f9c336":"train['bmi'].fillna(train['bmi'].median(), inplace=True)\ntest['bmi'].fillna(test['bmi'].median(), inplace=True)\ntrain.head()","6c42d0c1":"for df in dfs:\n    for col in df.columns:\n        if df[col].isnull().sum() > 0:\n            print(col,':', df[col].isnull().sum(), 'nan values')","efc02adc":"for df in dfs:\n    df['ever_married'].replace({'Yes': 1, 'No': 0}, inplace=True)\n    df['Residence_type'].replace({'Urban': 1, 'Rural': 0}, inplace=True)","65232b9c":"def dummies(df):\n    df = pd.get_dummies(df, columns=['work_type'], prefix='work')\n    df = pd.get_dummies(df, columns=['smoking_status'], prefix='smoking')\n    df = pd.get_dummies(df, columns=['gender'], prefix='gender')\n    return df","08bd70c3":"train = dummies(train)\ntest = dummies(test)\ntrain.head()","b2831d31":"train.columns = train.columns.str.lower()\ntest.columns = test.columns.str.lower()","fe761aec":"test[test['gender_other']==1]['gender_other'].count()","2df35600":"test = test.drop('gender_other', axis=1)","597fb8f5":"for df in dfs:\n    print(df['avg_glucose_level'].describe(), '\\n')","69e72cc8":"def add_features(df):\n    df['obese'] = np.where(df['bmi'] >= 30, 1, 0)\n    df['hyperglycemic'] = np.where(df['avg_glucose_level'] >=108, 1, 0)\n    df['over_65'] = np.where(df['age'] >=65, 1, 0)\n    return df","8111d5f8":"add_features(train)\nadd_features(test)","7185d578":"risk_factors = ['obese', 'hyperglycemic', 'over_65', 'gender_female', 'heart_disease', \n                'hypertension', 'smoking_formerly smoked', 'smoking_smokes']\ntrain['risk_factors'] = train[risk_factors].sum(axis=1)\ntest['risk_factors'] = test[risk_factors].sum(axis=1)\ntrain.head()","6fcb1d5f":"from sklearn.decomposition import PCA\n\ndef pca(X):\n    Xp = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    pca = PCA(random_state=0)\n    X_pca = pca.fit_transform(Xp)\n    comps = [f'PC{1+i}' for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=comps, index=X.index)\n    \n    loadings = pd.DataFrame(pca.components_.T, columns=comps, index=X.columns)\n\n    return pca, X_pca, loadings","0d057c56":"pca_features = ['age', 'avg_glucose_level', 'bmi', 'risk_factors']\npca_train, X_pca_train, loadings_train = pca(train[pca_features])\npca_test, X_pca_test, loadings_test = pca(test[pca_features])\nloadings_train","24424cdc":"loadings_test","a054597d":"train['age*risk_factors'] = train['age'] * (train['risk_factors'])\ntest['age*risk_factors'] = test['age'] * (test['risk_factors'])","e5cd0ba0":"train = train.join(X_pca_train)\ntest = test.join(X_pca_test)\ntest['PC2'] = test['PC2'] * (-1)\ntest['PC4'] = test['PC4'] * (-1)","534cae47":"features = train.columns\nadded_features = ['obese', 'hyperglycemic', 'over_65', 'risk_factors', \n                  'bmi_unreported', 'PC1', 'PC2', 'PC3', 'PC4', 'age*risk_factors']\nog_features = [x for x in features if x not in added_features]","c063e075":"# train = train[og_features]\n# test = test[og_features]","88c685b8":"X_train = train.drop('stroke', axis=1)\ny_train = train['stroke']\n\nX_test = test.drop('stroke', axis=1)\ny_test = test['stroke']","b297afa9":"print([x for x in X_test.columns if x not in X_train.columns])\nprint([x for x in X_train.columns if x not in X_test.columns])","e4315976":"print('y_train mean : ', y_train.mean())\nprint('y_test mean : ', y_test.mean())","a9caf3dc":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","684f4f0a":"from imblearn.over_sampling import SMOTE\n\nX_train_smote, y_train_smote = SMOTE().fit_resample(X_train_scaled, y_train)","d87953cc":"print('y_train_smote mean : ', y_train_smote.mean())","7717e9bf":"from sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, auc, RocCurveDisplay\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression","0ce7afdb":"models = dict()\nrs = 0\nmodels['xgb clf'] = XGBClassifier(n_estimators=300, learning_rate=0.05, random_state=rs)\nmodels['gbd tree'] = GradientBoostingClassifier(random_state=rs)\nmodels['random forests'] = RandomForestClassifier(random_state=rs)\nmodels['log reg'] = LogisticRegression(random_state=rs)","ff5a4a0c":"for model in models:\n    models[model].fit(X_train_smote, y_train_smote)\n    print(f'{model} : \u2714')","71e5f886":"from sklearn.metrics import confusion_matrix, classification_report","1075ac8d":"def model_test(model):\n    model = models[m]\n    pred = model.predict(X_test_scaled)\n    pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n    auc = roc_auc_score(y_test, pred_proba)\n    cf = confusion_matrix(y_test, pred)\n    cr = classification_report(y_test, pred)\n    \n    print('- - - - - -\\n', m, '\\n')\n    print('roc auc: ', auc)\n    print(cf)\n    print(cr, '\\n')","c2efc185":"for m in models:\n   model_test(m)","76020ef9":"fig = plt.figure(figsize=(8,8))\nfor m in models:\n    model = models[m]\n    prediction = models[m].predict_proba(X_test_scaled)[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_test, prediction)\n    plt.plot(fpr, tpr, label=m)\nplt.xlabel('false positive rate')\nplt.ylabel('true positive rate')\nplt.legend()\nplt.show()","04031a14":"log_params = [{'solver':['liblinear'], 'penalty':['l1', 'l2'], 'C':[1.0, 10.0, 100.0]},\n              {'solver':['lbfgs'], 'penalty':['none', 'l2'], 'C':[1.0, 10.0, 100.0]}]\nlog_grid = GridSearchCV(models['log reg'], param_grid=log_params, scoring='recall').fit(X_train_smote, y_train_smote)\nprint(log_grid.best_score_)\nprint(log_grid.best_params_)\nprint(log_grid.best_estimator_)","ba3fa02b":"model_test(log_grid.best_estimator_)","d009564b":"final_predicted = log_grid.best_estimator_.predict(X_test_scaled)\nsub = pd.DataFrame(data={'id': X_test.index, 'stroke':final_predicted})\nsub.to_csv('stroke_sub.csv', index=False)","825f96f4":"only 1 sample of gender_other so let's delete it","07ebc9d0":"time to train and analyze some models","6b13229c":"make sure the columns match between train and test sets","67eb8180":"we'll continue with the logistic regressor since it has the highest recall score by far","73d2d658":"add pca features to the data sets","34ca7575":"scale our data","d0b7631a":"multiply test pc2 and pc4 by -1 to get into same orientation as training data","f30c7f46":"looks like age * risk_factors may be a promising feature ","0be33f16":"Thanks to samsatp's notebook for guidance on the modeling structure. Their notebook can be found here : [https:\/\/www.kaggle.com\/sathianpong\/stroke-eda-visualization-prediction](http:\/\/)","655a2654":"# classifier","251cbd13":"add a total risk factors feature","2bb6effe":"nan check","fa4a3d99":"heavily skewed towards non stroke subjects","667a01d4":"split data for processing","215186d0":"look into bmi","a936b1b4":"by replacing the missing bmi with the median, we won't pumping up the obese numbers ( >= 30 )","71e515f3":"the recall score is important in this analysis since we want to catch all the possible strokes even if we misclassify some","3de578c2":"add risk factors according to research","edda5786":"further outside research found that these are all risk factors for stroke : \n\n* high blood pressure\n* obesity : bmi >= 30\n* diabetic\n* smoking history\n* heart disease\n* gender : female\n* age : over 65 (70% strokes over age 65)\n* prior stroke\n* stress\n* hyperglycemia : >= 108 mg\/dL (observed in 2\/3 of ischemic strokes)\n\n* unreported bmi? could just be sample specific ","418fdbb4":"get original features just for reference","29dbeef4":"# data processing","2ccbfe2d":"# check out some data","d29923c2":"those that don't report bmi are more likely to have a stroke","85d18444":"use smote to oversample the stroke data since it is heavily skewed","eafa57ef":"nan check 2","94ae5878":"PCA analysis of some features"}}