{"cell_type":{"b28b38e3":"code","b4668f8e":"code","bdca2a4c":"code","40c34587":"code","dad0d8d6":"code","151b9f51":"code","bd4dfa11":"code","847b6572":"code","d52c86b6":"code","bea6af9b":"code","dbdd199b":"code","cbeb28d6":"code","3491a441":"code","2fa07479":"code","d88b5782":"code","9fdfa1a4":"code","a7b86287":"code","d3bf92d2":"code","2904212b":"code","98d70676":"code","134ffe56":"code","97aae175":"code","403324b9":"markdown","87016dda":"markdown","7a308ca6":"markdown","7c588fbf":"markdown","01126d5b":"markdown","db6583f5":"markdown","468605bf":"markdown","55ef53fe":"markdown","a2cf79de":"markdown","372b8af7":"markdown","dca05890":"markdown","a2ff3578":"markdown","c8e05919":"markdown","68239fbe":"markdown","6e57209c":"markdown","0e5de192":"markdown","e394683b":"markdown","e3a9af0c":"markdown","62222d36":"markdown"},"source":{"b28b38e3":"import numpy as np \nimport pandas as pd \nimport os\nimport json\nfrom pandas.io.json import json_normalize\nimport ast\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport seaborn as sns\n%matplotlib notebook\nfrom scipy.stats import skew, boxcox\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom mpl_toolkits.mplot3d import Axes3D\nimport ast\nimport re\nimport yaml\nimport json\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, KFold\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nimport eli5\nimport time\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings  \nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n","b4668f8e":"print(os.listdir(\"..\/input\"))","bdca2a4c":"%%time \n\ntrain_new= pd.read_csv('..\/input\/train-new\/train_new.csv')\ntest_new = pd.read_csv('..\/input\/train-new\/test_new.csv')\nsam_sub = pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')\nprint( \"train dataset:\", train_new.shape,\"\\n\",\"test dataset: \",test_new.shape,\"\\n\",\"sample_submission dataset:\", sam_sub .shape)","40c34587":"#append train and test and replace NAs with mean of each column\n\ntrain_new_agg=train_new\ntest_new_agg=test_new\n\ntrain_new_agg['dataset']='train'\ntest_new_agg['dataset']='test'\n\nbig_and_beautiful_data=train_new_agg.append(test_new_agg)\n\ndel train_new_agg\ndel test_new_agg\n\n#numerical mean replacement \nnumerical_cols_df =[c for c in big_and_beautiful_data.columns if big_and_beautiful_data[c].dtype in [np.float, np.int] and c not in ['revenue', 'id']]\ncategorical_cols_df = [c for c in big_and_beautiful_data.columns if big_and_beautiful_data[c].dtype in [np.object]]\nbig_and_beautiful_data_num=big_and_beautiful_data[numerical_cols_df].fillna(big_and_beautiful_data[numerical_cols_df].mean()) \nbig_and_beautiful_data= pd.concat([big_and_beautiful_data_num, big_and_beautiful_data[categorical_cols_df],big_and_beautiful_data[['revenue', 'id']]], axis=1)\n","dad0d8d6":"#replace by mean of train and test \nnumerical_cols_train =[c for c in train_new.columns if train_new[c].dtype in [np.float, np.int] and c not in ['revenue', 'id']]\ncategorical_cols_train = [c for c in train_new.columns if train_new[c].dtype in [np.object]]\ntrain_new_num=train_new[numerical_cols_train].fillna(train_new[numerical_cols_train].mean()) \ntrain_new= pd.concat([train_new_num, train_new[categorical_cols_train],train_new[['revenue', 'id']]], axis=1)\n\nnumerical_cols_test =[c for c in test_new.columns if test_new[c].dtype in [np.float, np.int] and c not in ['id']]\ncategorical_cols_test = [c for c in test_new.columns if test_new[c].dtype in [np.object]]\ntest_new_num=test_new[numerical_cols_test].fillna(test_new[numerical_cols_test].mean()) \ntest_new= pd.concat([test_new_num, test_new[categorical_cols_test],test_new[['id']]], axis=1)","151b9f51":"#dropping unnecessary columns \ndrop_columns=['homepage','imdb_id','poster_path','status','title', 'tagline', 'overview', 'original_title','all_genres','all_cast',\n             'original_language','collection_name','all_crew']\ntrain_new=train_new.drop(drop_columns,axis=1)\ntest_new=test_new.drop(drop_columns,axis=1)","bd4dfa11":"big_and_beautiful_data=big_and_beautiful_data.drop(drop_columns,axis=1)\n\ntrain_mean_agg=big_and_beautiful_data.loc[big_and_beautiful_data['dataset']=='train']\ntest_mean_agg=big_and_beautiful_data.loc[big_and_beautiful_data['dataset']=='test']\n\ntrain_mean_agg=train_mean_agg.drop('dataset',axis=1)\ntest_mean_agg=test_mean_agg.drop(['dataset','revenue'],axis=1)\ntrain_new=train_new.drop('dataset',axis=1)\ntest_new=test_new.drop('dataset',axis=1)\n\n\nprint( \"updated train dataset:\", train_new.shape,\"\\n\",\"updated test dataset: \",test_new.shape)\nprint( \"updated train agg dataset:\", train_mean_agg.shape,\"\\n\",\"updated test agg dataset: \",test_mean_agg.shape)\n\n\n# Just double checking the difference of variables between train and test \nprint(train_new.columns.difference(test_new.columns)) # good to go! \nprint(train_mean_agg.columns.difference(test_mean_agg.columns)) # good to go! ","847b6572":"%%time \n\nX = train_new.drop(['id', 'revenue','release_date'], axis=1)\ny = np.log1p(train_new['revenue'])\nX_test = test_new.drop(['id','release_date'], axis=1)\n\ndtrain = xgb.DMatrix(X, label=y)\ndtest = xgb.DMatrix(X_test)\n\ndef xgb_evaluate(max_depth, subsample, eta, min_child_weight,colsample_bytree):\n    params = {'eval_metric': 'rmse',\n              'max_depth': int(max_depth),\n              'subsample':subsample,\n              'eta': eta,\n              'min_child_weight': int(min_child_weight),\n              'colsample_bytree': colsample_bytree}\n\n    cv_result = xgb.cv(params, dtrain, num_boost_round=5000, nfold=3, early_stopping_rounds=50)    \n    \n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1] #because Bayesian Optimization function maximizes, we have to flip the number by multiplying by -1\n\n\n\nxgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (0, 6), \n                                             'subsample': (0.6, 1),\n                                             'eta':(0.01, 0.4),\n                                             'min_child_weight':(1, 30),\n                                             'colsample_bytree':(0.6, 1)})\n\nxgb_bo.maximize(init_points=10, n_iter=15, acq='ei')\n\n\nmodel_rmse=[]\nfor model in range(len(xgb_bo.res)):\n    model_rmse.append(xgb_bo.res[model]['target'])\n    \nxgb_bo.res[pd.Series(model_rmse).idxmax()]['target']\nxgb_opt_params = xgb_bo.res[pd.Series(model_rmse).idxmax()]['params']\nxgb_opt_params['max_depth']= int(round(xgb_opt_params['max_depth']))\nxgb_opt_params['objective']='reg:linear'\nxgb_opt_params['eval_metric']='rmse'\n    \n","d52c86b6":"xgb_opt_params ","bea6af9b":"%%time \n\nX = train_new.drop(['id', 'revenue','release_date'], axis=1)\ny = np.log1p(train_new['revenue'])\n\ntrain_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n   \n\ndef lgb_eval(learning_rate, num_leaves, bagging_fraction, max_depth, min_data_in_leaf, lambda_l1, lambda_l2):\n    \n    params = {\n            'application': 'regression',\n            'metric': 'rmse',\n            'learning_rate': learning_rate,\n            'num_leaves':int(round(num_leaves)),\n            'bagging_fraction':bagging_fraction,   \n            'max_depth': int(round(max_depth)),\n            'min_data_in_leaf':int(round(min_data_in_leaf)),\n            'lambda_l1':lambda_l1,\n            'lambda_l2':lambda_l2}\n    \n    \n    cv_result = lgb.cv(params, train_data, num_boost_round=5000, nfold=3,stratified =False,early_stopping_rounds=50)    \n    \n    return -1.0 * min(cv_result['rmse-mean'])\n\n#put min\/max of hyperparameter that you want to test \nlgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 0.2),\n                                            'num_leaves': (2,31 ),\n                                            'bagging_fraction': (0.8, 1),\n                                            'max_depth': (-1, 5),\n                                            'min_data_in_leaf': (2, 20),\n                                           'lambda_l1': (0, 5),\n                                           'lambda_l2': (0, 5)}, random_state=200)\n    \nlgbBO.maximize(init_points=10, n_iter=15, acq='ei')\nmodel_rmse=[]\nfor model in range(len(lgbBO.res)):\n    model_rmse.append(lgbBO.res[model]['target'])\n    \nlgbBO.res[pd.Series(model_rmse).idxmax()]['target']\nlgb_opt_params = lgbBO.res[pd.Series(model_rmse).idxmax()]['params']\nlgb_opt_params['max_depth']= int(round(lgb_opt_params['max_depth']))\nlgb_opt_params['num_leaves']= int(round(lgb_opt_params['num_leaves']))\nlgb_opt_params['min_data_in_leaf']= int(round(lgb_opt_params['min_data_in_leaf']))\n\nlgb_opt_params['application']='regression'\nlgb_opt_params['metric']='rmse'\n    ","dbdd199b":"lgb_opt_params","cbeb28d6":"n_fold = 10\nrandom_seed=2222\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n\n\nX = train_new.drop(['id', 'revenue','release_date'], axis=1)\ny = np.log1p(train_new['revenue'])\nX_test = test_new.drop(['id','release_date'], axis=1)\n","3491a441":"def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=True, model=None):\n\n    oof = np.zeros(X.shape[0])\n    prediction = np.zeros(X_test.shape[0])\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        if model_type == 'sklearn':\n            X_train, X_valid = X[train_index], X[valid_index]\n        else:\n            X_train, X_valid = X.values[train_index], X.values[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 10000, nthread = 4, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n                    verbose=100, early_stopping_rounds=100)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=10000, evals=watchlist, early_stopping_rounds=100, verbose_eval=100, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)\n\n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_squared_error(y_valid, y_pred_valid)\n            \n            y_pred = model.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=10000,  eval_metric='RMSE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n        \n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction\n        return oof, prediction\n    \n    else:\n        return oof, prediction","2fa07479":"%%time \noof_xgb, prediction_xgb = train_model(X, X_test, y, params=xgb_opt_params, model_type='xgb')","d88b5782":"%%time \n\noof_lgb, prediction_lgb = train_model(X, X_test, y, params=lgb_opt_params, model_type='lgb')","9fdfa1a4":"%%time \n\nX = train_mean_agg.drop(['id', 'revenue','release_date'], axis=1)\ny = np.log1p(train_mean_agg['revenue'])\nX_test = test_mean_agg.drop(['id','release_date'], axis=1)\n\ndtrain = xgb.DMatrix(X, label=y)\ndtest = xgb.DMatrix(X_test)\n\ndef xgb_evaluate(max_depth, subsample, eta, min_child_weight,colsample_bytree):\n    params = {'eval_metric': 'rmse',\n              'max_depth': int(max_depth),\n              'subsample':subsample,\n              'eta': eta,\n              'min_child_weight': int(min_child_weight),\n              'colsample_bytree': colsample_bytree}\n\n    cv_result = xgb.cv(params, dtrain, num_boost_round=5000, nfold=3, early_stopping_rounds=50)    \n    \n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\n\n\n\nxgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (0, 6), \n                                             'subsample': (0.6, 1),\n                                             'eta':(0.01, 0.4),\n                                             'min_child_weight':(1, 30),\n                                             'colsample_bytree':(0.6, 1)})\n\nxgb_bo.maximize(init_points=10, n_iter=15, acq='ei')\n\nmodel_rmse=[]\nfor model in range(len(xgb_bo.res)):\n    model_rmse.append(xgb_bo.res[model]['target'])\n    \nxgb_bo.res[pd.Series(model_rmse).idxmax()]['target']\nxgb_opt_params = xgb_bo.res[pd.Series(model_rmse).idxmax()]['params']\nxgb_opt_params['max_depth']= int(round(xgb_opt_params['max_depth']))\nxgb_opt_params['objective']='reg:linear'\nxgb_opt_params['eval_metric']='rmse'\n    \n","a7b86287":"xgb_opt_params","d3bf92d2":"%%time \n\nX = train_mean_agg.drop(['id', 'revenue','release_date'], axis=1)\ny = np.log1p(train_mean_agg['revenue'])\nX_test = test_mean_agg.drop(['id','release_date'], axis=1)\n\ntrain_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n   \n\ndef lgb_eval(learning_rate, num_leaves, bagging_fraction, max_depth, min_data_in_leaf, lambda_l1, lambda_l2):\n    \n    params = {\n            'application': 'regression',\n            'metric': 'rmse',\n            'learning_rate': learning_rate,\n            'num_leaves':int(round(num_leaves)),\n            'bagging_fraction':bagging_fraction,   \n            'max_depth': int(round(max_depth)),\n            'min_data_in_leaf':int(round(min_data_in_leaf)),\n            'lambda_l1':lambda_l1,\n            'lambda_l2':lambda_l2}\n    \n    \n    cv_result = lgb.cv(params, train_data, num_boost_round=5000, nfold=3,stratified =False,early_stopping_rounds=50)    \n    \n    return -1.0 * min(cv_result['rmse-mean'])\n\n#put min\/max of hyperparameter that you want to test \nlgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 0.2),\n                                            'num_leaves': (2,31 ),\n                                            'bagging_fraction': (0.8, 1),\n                                            'max_depth': (-1, 5),\n                                            'min_data_in_leaf': (2, 20),\n                                           'lambda_l1': (0, 5),\n                                           'lambda_l2': (0, 5)}, random_state=200)\n    \n    \n    \nlgbBO.maximize(init_points=10, n_iter=15, acq='ei')\nmodel_rmse=[]\nfor model in range(len(lgbBO.res)):\n    model_rmse.append(lgbBO.res[model]['target'])\n    \nlgbBO.res[pd.Series(model_rmse).idxmax()]['target']\nlgb_opt_params = lgbBO.res[pd.Series(model_rmse).idxmax()]['params']\nlgb_opt_params['max_depth']= int(round(lgb_opt_params['max_depth']))\nlgb_opt_params['num_leaves']= int(round(lgb_opt_params['num_leaves']))\nlgb_opt_params['min_data_in_leaf']= int(round(lgb_opt_params['min_data_in_leaf']))\n\nlgb_opt_params['application']='regression'\nlgb_opt_params['metric']='rmse'\n    \n","2904212b":"lgb_opt_params","98d70676":"%%time \noof_xgb_agg, prediction_xgb_agg = train_model(X, X_test, y, params=xgb_opt_params, model_type='xgb')","134ffe56":"%%time \n\noof_lgb_agg, prediction_lgb_agg = train_model(X, X_test, y, params=lgb_opt_params, model_type='lgb')","97aae175":"sam_sub['revenue'] = np.expm1(prediction_lgb)\nsam_sub.to_csv(\"lgb.csv\", index=False)\nsam_sub['revenue'] = np.expm1(prediction_xgb)\nsam_sub.to_csv(\"xgb.csv\", index=False)\n\nsam_sub['revenue'] = np.expm1(prediction_lgb_agg)\nsam_sub.to_csv(\"lgb_agg.csv\", index=False)\nsam_sub['revenue'] = np.expm1(prediction_xgb_agg)\nsam_sub.to_csv(\"xgb_agg.csv\", index=False)\n\n\n\nsam_sub['revenue'] = np.expm1((prediction_lgb + prediction_xgb) \/ 2)\nsam_sub.to_csv(\"blend_lgb_xgb.csv\", index=False)\n\nsam_sub['revenue'] = np.expm1((prediction_lgb_agg + prediction_xgb_agg) \/ 2)\nsam_sub.to_csv(\"blend_lgb_xgb_agg.csv\", index=False)\n\nsam_sub['revenue'] = np.expm1(( prediction_lgb + prediction_xgb +prediction_lgb_agg + prediction_xgb_agg) \/ 4)\nsam_sub.to_csv(\"put_them_all.csv\", index=False)","403324b9":"# Dear tree \ud83c\udf34, why do you require so many parameters to grow?\nAnd you would probably react with my parameter choice like...\n","87016dda":"# **XGB Optmiziation**\n## First dataset (non-agg mean filled one)\n","7a308ca6":"# **LGB Optimization**\n## First dataset (non-agg mean filled one)","7c588fbf":"----\n<a id=\"5\"><\/a> \n# **5. Training Xgboost and Lightgbm**  <br>\n\nI used 10 folds cross-validation. ","01126d5b":"<br>\n# ** CONTENTS**\n\n1. [Lightgbm vs. Xgboost and Why Do We Care?](#1)\n2. [What Is Bayesian Optimization and Why Do We Care?](#2)\n3. [Loading Library and Dataset](#3)\n4. [Bayesian Optimization with XGBOOST and LightGBM](#4)\n5. [Training XGBOOST and LightGBM](#5)\n\n\n\n","db6583f5":"----\n<a id=\"1\"><\/a> \n# **1. Lightgbm vs. Xgboost and Why Do We Care?**  <br>\n\n**Lightgbm** is  a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning task. [(source)](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/which-algorithm-takes-the-crown-light-gbm-vs-xgboost\/) Like Xgboost, it is based on decision tree algorithms, but <u>Lightgbm splits the tree leaf wise with the best fit while Xgboost splits the tree level wise.<\/u> The upper figure demonstrates Lightgbm, and the lower figure demonstrates Xgboost. <br><br>\n\n\n<img src=\"https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2017\/06\/11194227\/depth.png\"  alt=\"Drawing\" style=\"width: 600px;\"\/>\n\n<img src=\"https:\/\/t4.ftcdn.net\/jpg\/00\/95\/63\/41\/240_F_95634124_ticAi4oZV4gHnVVOK5KbhA29SJnypKGY.jpg\"  alt=\"Drawing\" style=\"width: 200px;\"\/>\n\n<img src=\"https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2017\/06\/11194110\/leaf.png\"  alt=\"Drawing\" style=\"width: 600px;\"\/>\n\n<br> \n\n\n<br>\n* **Well..why do we care about it?** <br><br>\nThe biggest adventage of Lightgbm is <span style=\"color: red; font-family: sans-serif; font-size: 1.1em;\">**Speed!**<\/span> Faster speed allows faster hyperparameter searches when optimizing the model. We can create more possible combinations to be tested than if using XGBoost in the same time period. The speed also allows for adding more cross-validation testing to get more accurate metrics of how your model will generalize. <br><br>\n* **Why is it faster?** <br><br>\nLightGBM uses a novel technique of Gradient-based One-Side Sampling (GOSS) to filter out the data instances for finding a split value while XGBoost uses pre-sorted algorithm & Histogram-based algorithm for computing the best split. GOSS excludes a significant proportion of data instances with small gradients, only use the rest to estimate the information gain, and obtain quite accurate estimation of the information gain with a much smaller data size. [(source)](https:\/\/papers.nips.cc\/paper\/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf) I will compare the time difference between xgboost and lightgbm at the end! <br><br>\n* **Time Comparison between them** <br><br>\n<u>On average, Xgboost took twice or more longer than lightgbm! <\/u> \n---","468605bf":"# Before starting the pain, here is a fun meme about decision tree <br>\n\n<center> <img src=\"https:\/\/www.catster.com\/wp-content\/uploads\/2015\/06\/4d7b24e783483f4092e303c48d1a7953.png\"  alt=\"Drawing\" style=\"width: 600px;\"\/> <\/center>\n","55ef53fe":"## Let's do some experiments right here \ud83d\udca1\n\nI made two datasets, which I used two different methods of filling NAs. For the first dataset (train dataset name: train_new, test dataset name: test_new), I filled out NAs with each column's mean of train for train and each column's mean of test for test.  For the second dataset (train dataset name: train_new_agg, test dataset name: test_new_agg), I appended train and test and filled out NAs with each column's mean of the whole dataset, which I demonstarated below. **Let's see which one scores better at the end!**","a2cf79de":"# <center> Thank you for reading my kernel and keep working hard \ud83d\udc4d <\/center>\n<center> <img src=\"https:\/\/media.giphy.com\/media\/jaXDDTuKmeJvwI56kV\/giphy.gif \"  height=\"600px\" width=\"600px\"> <\/center>\n\n","372b8af7":"-----","dca05890":"# **XGB Optimization**\n## Second dataset (agg mean filled one)","a2ff3578":"----\n<a id=\"4\"><\/a> \n# **4. Bayesian Optimization with XGBOOST and LightGBM**  <br>\n\n\n<img src=\"https:\/\/github.com\/fmfn\/BayesianOptimization\/raw\/master\/examples\/func.png\"  alt=\"Drawing\" style=\"width: 600px;\"\/>\n<br>\n\nFinally, we get to use Bayesian Optimization here! Just in case you are interested in, I put the [official github page for Bayesian Optimization](https:\/\/github.com\/fmfn\/BayesianOptimization) <br><br>\n\n* **First**, prepare the dataset for optimization. Divide it as X (independent variable) and target (dependent variable) \n\n\n\n* **Second**, put hyperparameters that you want to put as input of lgb_eval function and specify the minimum and maximum values of hyperparameters in BayesianOptimization function <br> \n> For example, 'learning_rate': (0.01, 0.2) means that the optimization function will find the optimal value of learning rate in between 0.01 to 0.2 \n\n* **Second and a half**, Here are the explanation for parameters that I used. There are lots of parameters to explore for Lightgbm (you can even use xgboost dart mode by setting xgboost_dart_mode=True \ud83d\udc49 [more info](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html) ) <br> \n\n### [For Xgboost](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html) \n\n> **max_depth**: limit the max depth for tree model. *default=6* <br>\n**min_child_weight**: Minimum sum of instance weight (hessian) needed in a child.  *default=1* <br>\n**subsample**:subsample ratio of the training instances. *default = 1* <br>\n**eta**: step size shrinkage used in update to prevents overfitting *default = 0.1* <br>\n**colsample_bytree**: subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.  *default = 1* <br> <br>\n\n\n### [For Lightgbm](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html) \n\n> **max_depth**: limit the max depth for tree model. *default=-1 (no limit)* <br>\n**min_data_in_leaf**: minimal number of data in one leaf.  *default=20* <br>\n**num_leaves**:max number of leaves in one tree.It controls the complexity of the tree moodel. *default = 31* <br>\n**learning_rate**: shrinkage rate *default = 0.1* <br>\n**bagging_fraction**: work like feature_fraction, but this will randomly select part of data without resampling.  *default = 1* <br>\n**lambda_l1**: L1 regularization. *default = 0* <br>\n**lambda_l2**: L2 regularization. *default = 0* <br> <br>\n\n<span style=\"color: blue; font-family: sans-serif; font-size: 1.1em;\">In order to avoid overfitting:<\/span> <br> <br> \u25aa\ufe0f Use small max_bin \ud83d\udd3d<br> \u25aa\ufe0f Use small num_leaves \ud83d\udd3d <br> \u25aa\ufe0f Use min_data_in_leaf and min_sum_hessian_in_leaf <br> \u25aa\ufe0f Use bagging by set bagging_fraction and bagging_freq <br> \u25aa\ufe0f Use feature sub-sampling by set feature_fraction <br> \u25aa\ufe0f Try lambda_l1, lambda_l2 and min_gain_to_split for regularization <br> \u25aa\ufe0f Try max_depth to avoid growing deep tree\n<br> <br> <br> <span style=\"color: blue; font-family: sans-serif; font-size: 1.1em;\">For better accuracy:<\/span> <br> <br> \u25aa\ufe0f Use large max_bin (may be slower) \ud83d\udd3c <br> \u25aa\ufe0f Use small learning_rate \ud83d\udd3d with large num_iterations \ud83d\udd3c <br> \u25aa\ufe0f Use large num_leaves (may cause over-fitting) \ud83d\udd3c <br> \u25aa\ufe0f Try dart <br> <br> **Now, we are ready to use the optimization function \ud83d\ude0a**\n\n<br>\n\n* **Third**, the BayesianOptimization object will work out of the box without much tuning needed. Maximize the function! Here are some explanation of parameters <br> \n> **init_points**: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space <br> **n_iter**: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n\n-----\n","c8e05919":"While waiting, feel free to enjoy this amazing panda video \u2764\ufe0f\u2764\ufe0f <br> \n\nYou can also watch [live cam panda video](https:\/\/nationalzoo.si.edu\/webcams\/panda-cam) from the Smithsonian's National Zoo website. So cool! \n\n<center><iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/WwHOGLqT0OE\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>\n\n","68239fbe":"<center> <img src=\"https:\/\/media.giphy.com\/media\/FVIflnMFdGoMw\/giphy.gif\"  height=\"500px\" width=\"500px\"> <\/center>\n\n","6e57209c":"<a id=\"3\"><\/a> \n# **3. Loading Library and Dataset** <br>\n\nI already did feature engineering with [a different kernel](https:\/\/www.kaggle.com\/somang1418\/eda-lgb-xgb-modelings-with-a-cute-panda-meme). Feel free to use the dataset!","0e5de192":"# **LGM Optimization**\n## Second dataset (agg mean filled one)","e394683b":"# Final step: don't forget to submit your prediction and upvote if this kernel was useful :)","e3a9af0c":"# Introduction\n\nHi guys! \n\nI hope that your Kaggle journey is going well! While working on my kernel\/stalking what others worked on, I thought that it would be helpful to some people if I made a kernel about the comparison between Xgboost and Lightgbm and Bayesian Optimization (regression). Although there are many kernels that implement these techniques, I infrequently saw kernels that explained about them. So, I tried my best to explain these concepts by putting things together from different resources! Hope that it will be helpful to you guys \ud83d\ude03\n\n# Special Thanks to: <br>\n\n\nThanks to [Andrew Lukyanenko's kernel](https:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-model-interpretation) and [btyuhas's kernel](https:\/\/www.kaggle.com\/btyuhas\/bayesian-optimization-with-xgboost)!! <br>\nAnd if you are interested in seeing Bayesian Optimization tuning on binary classification problems, here is [my kernel](https:\/\/www.kaggle.com\/somang1418\/tuning-hyperparameters-under-10-minutes-lgbm). Also, I worked on some extensive EDA work on [this kernel](https:\/\/www.kaggle.com\/somang1418\/eda-lgb-xgb-modelings-with-a-cute-panda-meme). \n\n","62222d36":"<a id=\"2\"><\/a> \n# **2. What Is Bayesian Optimization and Why Do We Care?** <br>\n\n**Bayesian Optimization** is a probabilistic model based approach for finding the minimum of any function that returns a real-value metric. [(source)](https:\/\/towardsdatascience.com\/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0) It is very effective with real-world applications in high-dimensional parameter-tuning for complex machine learning algorithms. Bayesian optimization utilizes the Bayesian technique of setting a prior over the objective function and\ncombining it with evidence to get a posterior function.<br> <br>\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Michael_Gutmann\/publication\/270906114\/figure\/fig2\/AS:614141605707791@1523434177308\/The-first-iterations-of-Bayesian-optimization-to-estimate-the-mean-of-a-Gaussian-The.png\"  alt=\"Drawing\" style=\"width: 600px;\"\/>\n<br> <br> \n The prior belief is our belief in parameters before modeling process. The posterior belief is our belief in our parameters after observing the evidence. <br>\n<br> Another way to demonstrate Bayesian Optimization is: \n\n<img src=\"http:\/\/www.resibots.eu\/limbo\/_images\/bo_concept.png\"  alt=\"Drawing\" style=\"width: 600px;\"\/> <br> \n\nFor continuous functions, Bayesian optimization typically works by assuming the unknown function was sampled from a Gaussian process and maintains a posterior distribution for this function as observations are made, which means that we need to give range of values of hyperparameters (ex. learning rate range from 0.1 to 1).  So, in our case, the Gaussian process gives us a prior distribution on functions. Gaussian process approach is a non-parametric approach, in that it finds a distribution over the possible functions \nf(x) that are consistent with the observed data. Gaussian processes have proven to be useful surrogate models for computer experiments and good\npractices have been established in this context for sensitivity analysis, calibration and prediction While these strategies are not considered in the context of optimization, they can be useful to researchers in machine learning who wish to understand better the sensitivity of their models to various hyperparameters. [(source)](http:\/\/papers.nips.cc\/paper\/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)\n\n<br>\n* **Well..why do we care about it?** <br><br>\nAccording to the [study](http:\/\/proceedings.mlr.press\/v28\/bergstra13.pdf), hyperparameter tuning by Bayesian Optimization of machine learning models is more efficient than Grid Search and Random Search. Bayesian Optimization has better overall performance on the test data and takes less time for optimization. Also, we do not need to set a certain values of parameters like we do in Random Search and Grid Search. For Bayesian Optimization tuning, we just give a range of a hyperparameter. \n\n<br>\n* **Time Comparison to Grid Search\/Random Search** <br><br>\nI tried to run Grid Search for the model in [a different kernel](https:\/\/www.kaggle.com\/somang1418\/eda-lgb-xgb-modelings-with-a-cute-panda-meme), but after running for 3 hours, I decided to stop it because it took too long (when you do not have anything else to do and want to feel productive while watching some videos, I suggest you to run it \ud83d\ude02). For Bayesian Optimization, it takes significantly less than these two methods! Depending on the number of iteration and exploration and model type (xgboost vs. lightgbm) time varies. I used 10 random exploration and 15 iteration to tune 5 hyperparameters (xgboost) and 7 hyperparameters (lightgbm). <u> It took 30 min for xgboost and 12 min for lightgbm to tune hyperparameters by Bayesian Optimization! <\/u>\n\n-----"}}