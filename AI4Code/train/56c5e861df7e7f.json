{"cell_type":{"b3e6e63e":"code","19441f0b":"code","9f571c5a":"code","53be123b":"code","7c6b0bd5":"code","ca0497c1":"code","319572e1":"code","a61449be":"code","3b088df3":"code","dd7bf338":"code","5363576a":"code","fadfde06":"code","5178ec7c":"code","47bc4f54":"code","fd32c1ba":"code","94ad8b9d":"code","5e2e5c5e":"code","696e1647":"code","be2fb86c":"code","007cab69":"code","683aac11":"code","1b80397d":"code","91564399":"code","945ea4d8":"markdown","6cd149d8":"markdown","36ac2f30":"markdown","9442c7e7":"markdown","1ff954d2":"markdown","dce198f6":"markdown","53e7cbfb":"markdown","5cdd7816":"markdown","d19dc26f":"markdown","6cdc47eb":"markdown","821b062a":"markdown","aadf3118":"markdown","ad6ec1b0":"markdown","fd7a41a4":"markdown","fdfc9d8d":"markdown","0b2af66b":"markdown","81bf4664":"markdown","3a8e22b6":"markdown","b66036d1":"markdown","1d0478d9":"markdown","8c6a0759":"markdown","3f1e77b8":"markdown","b9597bd4":"markdown","2666c0aa":"markdown","660d2a98":"markdown"},"source":{"b3e6e63e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","19441f0b":"medium_colored_data = pd.read_csv('..\/input\/hmnist_28_28_RGB.csv')\nmedium_colored_data.head()","9f571c5a":"medium_colored_data.shape","53be123b":"example = medium_colored_data.drop(\"label\", axis=1).values[0]\nto_show = example.reshape((28,28,3))\n\nfig, ax = plt.subplots(1,4,figsize=(20,5))\nfor channel in range(3):\n    ax[channel].imshow(to_show[:,:,channel], cmap=\"gray\")\n    ax[channel].set_title(\"Channel {}\".format(channel+1))\n    ax[channel].set_xlabel(\"Width\")\n    ax[channel].set_ylabel(\"Height\")\nax[3].imshow(to_show)\nax[3].set_title(\"All channels together\")\nax[3].set_xlabel(\"Width\")\nax[3].set_ylabel(\"Height\")","7c6b0bd5":"order_example = np.arange(0,12)\norder_example","ca0497c1":"show_order = order_example.reshape(2,2,3)\nprint(show_order[:,:,0])\nprint(show_order[:,:,1])\nprint(show_order[:,:,2])","319572e1":"medium_data = pd.read_csv('..\/input\/hmnist_28_28_L.csv')\nmedium_data.head()","a61449be":"print(medium_data.shape)","3b088df3":"def show_inarow(data, row_shape):\n    example = data.drop(\"label\", axis=1).values[0:4]\n    to_show = example.reshape(row_shape)\n    fig, ax = plt.subplots(1,4,figsize=(20,5))\n    for image_example in range(4):\n        ax[image_example].imshow(to_show[image_example,:,:], cmap=\"gray\")\n        ax[image_example].set_title(\"Grayscaled image {}\".format(image_example))\n        ax[image_example].set_xlabel(\"Widht\")\n        ax[image_example].set_ylabel(\"Height\")","dd7bf338":"show_inarow(medium_data, (4,28,28))","5363576a":"big_data = pd.read_csv(\"..\/input\/hmnist_64_64_L.csv\")\nbig_data.head()","fadfde06":"big_data.shape","5178ec7c":"show_inarow(big_data, (4,64,64))","47bc4f54":"small_data = pd.read_csv(\"..\/input\/hmnist_8_8_L.csv\")\nsmall_colored_data = pd.read_csv(\"..\/input\/hmnist_8_8_RGB.csv\")\nsmall_data.head()","fd32c1ba":"print(small_data.shape)\nprint(small_colored_data.shape)","94ad8b9d":"show_inarow(small_data, (4,8,8))","5e2e5c5e":"show_inarow(small_colored_data, (4,8,8,3))","696e1647":"from os import listdir\n\nclasses_dir = listdir(\"..\/input\/kather_texture_2016_image_tiles_5000\/Kather_texture_2016_image_tiles_5000\")\nclasses_dir","be2fb86c":"files = listdir(\"..\/input\/kather_texture_2016_image_tiles_5000\/Kather_texture_2016_image_tiles_5000\/01_TUMOR\")\nfor n in range(5):\n    print(files[n])","007cab69":"from scipy.misc import imread\n\ndef show_set(basepath, classes_dir, num_file):\n    fig, ax = plt.subplots(2,4,figsize=(20,10))\n    for n in range(4):\n        for m in range(2):\n            class_idx = m * 4 + n\n            path = basepath + classes_dir[class_idx] + \"\/\"\n            files = listdir(path)\n            image = imread(path + files[num_file])\n            ax[m,n].imshow(image)\n            ax[m,n].set_title(classes_dir[class_idx])","683aac11":"basepath = \"..\/input\/kather_texture_2016_image_tiles_5000\/Kather_texture_2016_image_tiles_5000\/\"\nshow_set(basepath, classes_dir, num_file=0)","1b80397d":"basepath = \"..\/input\/kather_texture_2016_larger_images_10\/Kather_texture_2016_larger_images_10\/\"\nfiles = listdir(basepath)\nfiles","91564399":"fig, ax = plt.subplots(1,2,figsize=(20,10))\nax[0].imshow(imread(basepath + files[0]))\nax[1].imshow(imread(basepath + files[1]))","945ea4d8":"You can browse through the files by setting different values for num_file. Currently you will obtain the first file in each class folder:","6cd149d8":"### Peak at the image folder 150x150\n\nIn the images folder *'kather_texture_2016_image_tiles_5000'* we should find 5000 images of size 150x150 pixels. Let's have a look at them to compare with our candidate of 64x62 grayscaled images. Within this folder (one step further) we should see 8 different directories that correspond to the 8 different class labels of cancer.","36ac2f30":"Ah, ok, we can see that each sample belongs to one image with a size of 28x28x3 (width times hight times color channel). This yields 2352 pixels in total per image. And we have one label that holds the class target. One question remains: How are the pixels ordered? Are three consequtive columns given by weight, height and color? For me it's not very intuitive even if one reads the intro kernels. Hmm... ","9442c7e7":"What does empty and adipose classes mean?!","1ff954d2":"Well actually I'm a bit confused about the meanings of all files. Let's dive into each file and folder to obtain an impression of what we can find there: ","dce198f6":"Uhh yes, this is far better! We can see more details of the tissues and it could be worth it to try classification with them. ","53e7cbfb":"## Take-Away\n\n* All images with a size higher or equal than 64x64 could be a good choice for deep learning.\n* 8x8 images are far too small to obtain meaningful insights. \n* A good starting point could be 64x64 grayscale, especially if one is limited to hardware resources. \n* There is an empty and an adipose class that is somehow strange and we have to find out their meanings. ","5cdd7816":"### Peek at the large images","d19dc26f":"Great! The first image is the same from above without color. :-) This medium resolution could still be too low to gain nice classification results. ","6cdc47eb":"### Short overview\n\n* 8 classes of cancer tissues\n* multiclass classification\n* Kather_texture_2016_image_tiles_5000 \n    * 150 x 150 pixel in size\n    * 5000 samples","821b062a":"Ok, same but with less pixels.","aadf3118":"Ok :-D This could be everything...","ad6ec1b0":"This doesn't look better. Ok, 8x8 is not worth it.","fd7a41a4":":-) That's nice! To gain an impression it's sufficient to look at only some examples. ","fdfc9d8d":"Let's take a look at the grayscaled ones first:","0b2af66b":"### Medium - 28 x 28 MNIST like RGB images","81bf4664":"Ok let's wirte a small method that collects examples - one per class folder: ","3a8e22b6":"Ok, this looks fine. Each channel has the same spatial structure. Our reshaping worked well, but even though we don't know the order yet. Let's try to untersand, what reshape does with our flattened image row:","b66036d1":"### Small - 8 x 8 MNIST like RGB and L images\n\nJust for completeness we should look at the smallest images with size 8x8 even though I assume that they are far too small to gain deep insights. But perhaps they are useful somehow... we will see.","1d0478d9":"Ok, we can see that the first 3 numbers are all of the same spatial coordinate but with different colors. Hence the first order-quantity is color. After that the width of the images is filled up and than the heigth. Hence the order is: color, width, height. And this should be true for our dataframe as well.","8c6a0759":"Ah, these are grayscaled images. Cool, this way we can compare the image we obtained above with its grayscaled counterpart. ","3f1e77b8":"Ok, 5000 samples as given by the dataset description.","b9597bd4":"### Medium - 28 x 28 MNIST like L images","2666c0aa":"Perhaps this rosolution is still better for us than 28x28 even though colors are missing. ","660d2a98":"### Big - 64 x 64 MNIST like L images\n\n....Why aren't here colored ones as well?.... "}}