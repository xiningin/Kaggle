{"cell_type":{"7beaad13":"code","ed3ba7dc":"code","7d495ddb":"code","682d88d1":"code","52156e8b":"code","be5cdea9":"code","16a6e856":"code","20072271":"code","bcc21aa3":"code","a29f58ce":"code","1f719029":"code","8ea5b4c2":"code","d854f6ff":"code","a9c424d2":"code","705a6422":"code","ae30637e":"code","bdab0af0":"code","598808ed":"code","4f699e20":"code","a4d9136c":"code","d227093b":"code","1a33c417":"code","77686435":"code","095ae4c1":"code","32a50c8c":"code","889794ea":"code","1ffb3490":"code","f1fd7c81":"code","41b54a96":"code","4c993fea":"code","683e65c5":"code","c414b2a7":"code","fbf7de0b":"code","916f4a8f":"code","66f3e9c6":"code","2228a2f4":"code","8030e29d":"code","0f924c21":"code","8e8d8b8b":"code","6aeba8b0":"code","f5e0c5a9":"code","5a0ba039":"code","ebf3c21f":"code","f65fc3eb":"code","2d326faf":"code","df613e31":"code","706c6d3b":"code","54cf4184":"code","c6c8a11a":"code","23e68694":"code","74f2a3ff":"code","d67eadf8":"code","a95835ea":"code","34aca007":"code","398b5931":"code","71d8a983":"code","c03e531c":"code","02eee067":"code","c8ae94cc":"code","b4c30be4":"code","813ab834":"code","b6651c53":"code","7e59a30b":"code","d8768ef6":"code","23d708af":"code","bb9333ad":"code","b0cfb1f2":"code","b5fd0456":"code","c5d45230":"code","4c86c2a4":"markdown","8d0b7fd3":"markdown","a6a7d76f":"markdown","c5b4dbfb":"markdown","7b243fb0":"markdown","d3c1578c":"markdown","3484115d":"markdown","95c5a245":"markdown","e59834c1":"markdown","fa567343":"markdown","ccf032e9":"markdown"},"source":{"7beaad13":"# remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n# ---\n\n%matplotlib inline\nimport pandas as pd\npd.options.display.max_columns = 100\nfrom matplotlib import pyplot as plt\nimport matplotlib\nmatplotlib.style.use('ggplot')\nimport numpy as np\n\nimport os\nfrom sklearn import metrics, preprocessing\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\npd.options.display.max_rows = 100\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\/titanic\"]).decode(\"utf8\"))\n","ed3ba7dc":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain_data.head()","7d495ddb":"test_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_data.head()","682d88d1":"train_data.info()","52156e8b":"test_data.info()","be5cdea9":"survived_sex = train_data[train_data['Survived']==1]['Sex'].value_counts()\ndead_sex = train_data[train_data['Survived']==0]['Sex'].value_counts()\ndf = pd.DataFrame([survived_sex,dead_sex])\ndf","16a6e856":"df.index = ['Survived','Dead']","20072271":"df.plot(kind='bar', stacked=True, figsize=(15,8))","bcc21aa3":"missing =round(100*train_data.select_dtypes(include='object').isnull().sum()[train_data.select_dtypes(include='object').isnull().sum()>0]\/len(train_data.PassengerId),2)\nmissing.loc[missing > 0]","a29f58ce":"missing =round(100*test_data.select_dtypes(include='object').isnull().sum()[test_data.select_dtypes(include='object').isnull().sum()>0]\/len(test_data.PassengerId),2)\nmissing.loc[missing > 0]\n","1f719029":"test_data['Cabin']=test_data['Cabin'].fillna('U') #unknown\ntrain_data['Cabin']=train_data['Cabin'].fillna('U')\n","8ea5b4c2":"train_data['Embarked']=train_data['Embarked'].fillna('S')","d854f6ff":"missing =train_data.select_dtypes(include='object').isnull().sum()[train_data.select_dtypes(include='object').isnull().sum()>0]\nmissing.loc[missing > 0]","a9c424d2":"# check the null values in the numerical data\nmissing =round(100*(train_data.select_dtypes(include=['int64','float']).isnull().sum()\/len(train_data.PassengerId)),2)\nmissing.loc[missing > 0]","705a6422":"# check the null values in the numerical data\nmissing =round(100*(test_data.select_dtypes(include=['int64','float']).isnull().sum()\/len(test_data.PassengerId)),2)\nmissing.loc[missing > 0]","ae30637e":"test_data['Fare'] = test_data['Fare'].replace(np.nan, test_data['Fare'].median())","bdab0af0":"   data = [train_data, test_data]\n\nfor dataset in data:\n    # we extract the title from each name\n    dataset['Title'] = dataset['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n    \n    # a map of more aggregated titles\n    Title_Dictionary = {\n                        \"Capt\":       \"Officer\",\n                        \"Col\":        \"Officer\",\n                        \"Major\":      \"Officer\",\n                        \"Jonkheer\":   \"Royalty\",\n                        \"Don\":        \"Royalty\",\n                        \"Sir\" :       \"Royalty\",\n                        \"Dr\":         \"Officer\",\n                        \"Rev\":        \"Officer\",\n                        \"the Countess\":\"Royalty\",\n                        \"Dona\":       \"Royalty\",\n                        \"Mme\":        \"Mrs\",\n                        \"Mlle\":       \"Miss\",\n                        \"Ms\":         \"Mrs\",\n                        \"Mr\" :        \"Mr\",\n                        \"Mrs\" :       \"Mrs\",\n                        \"Miss\" :      \"Miss\",\n                        \"Master\" :    \"Master\",\n                        \"Lady\" :      \"Royalty\"\n\n                        }\n    \n    # we map each title\n    dataset['Title'] = dataset.Title.map(Title_Dictionary)","598808ed":"# now let's drop Name column because it won't help more in analysis\ntrain_data = train_data.drop(['Name'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)","4f699e20":"train_data.head()","a4d9136c":"grouped_train = train_data.groupby(['Sex','Pclass','Title'])\ngrouped_median_train = grouped_train.median()\n\ngrouped_test = test_data.groupby(['Sex','Pclass','Title'])\ngrouped_median_test = grouped_test.median()","d227093b":"def fillAges(row, grouped_median):\n    if row['Sex']=='female' and row['Pclass'] == 1:\n        if row['Title'] == 'Miss':\n            return grouped_median.loc['female', 1, 'Miss']['Age']\n        elif row['Title'] == 'Mrs':\n            return grouped_median.loc['female', 1, 'Mrs']['Age']\n        elif row['Title'] == 'Officer':\n            return grouped_median.loc['female', 1, 'Officer']['Age']\n        elif row['Title'] == 'Royalty':\n            return grouped_median.loc['female', 1, 'Royalty']['Age']\n\n    elif row['Sex']=='female' and row['Pclass'] == 2:\n        if row['Title'] == 'Miss':\n            return grouped_median.loc['female', 2, 'Miss']['Age']\n        elif row['Title'] == 'Mrs':\n            return grouped_median.loc['female', 2, 'Mrs']['Age']\n\n    elif row['Sex']=='female' and row['Pclass'] == 3:\n        if row['Title'] == 'Miss':\n            return grouped_median.loc['female', 3, 'Miss']['Age']\n        elif row['Title'] == 'Mrs':\n            return grouped_median.loc['female', 3, 'Mrs']['Age']\n\n    elif row['Sex']=='male' and row['Pclass'] == 1:\n        if row['Title'] == 'Master':\n            return grouped_median.loc['male', 1, 'Master']['Age']\n        elif row['Title'] == 'Mr':\n            return grouped_median.loc['male', 1, 'Mr']['Age']\n        elif row['Title'] == 'Officer':\n            return grouped_median.loc['male', 1, 'Officer']['Age']\n        elif row['Title'] == 'Royalty':\n            return grouped_median.loc['male', 1, 'Royalty']['Age']\n\n    elif row['Sex']=='male' and row['Pclass'] == 2:\n        if row['Title'] == 'Master':\n            return grouped_median.loc['male', 2, 'Master']['Age']\n        elif row['Title'] == 'Mr':\n            return grouped_median.loc['male', 2, 'Mr']['Age']\n        elif row['Title'] == 'Officer':\n            return grouped_median.loc['male', 2, 'Officer']['Age']\n\n    elif row['Sex']=='male' and row['Pclass'] == 3:\n        if row['Title'] == 'Master':\n            return grouped_median.loc['male', 3, 'Master']['Age']\n        elif row['Title'] == 'Mr':\n            return grouped_median.loc['male', 3, 'Mr']['Age']\n    \n    ","1a33c417":"train_data['Age'] = train_data.apply(lambda r : fillAges(r, grouped_median_train) if np.isnan(r['Age']) else r['Age'], axis=1)\n    ","77686435":"test_data['Age'] = test_data.apply(lambda r : fillAges(r, grouped_median_test) if np.isnan(r['Age']) else r['Age'], axis=1)\n","095ae4c1":"grp=train_data.groupby(\"Sex\")[\"Age\"]\ngrp.describe()","32a50c8c":"missing =round(100*(test_data.select_dtypes(include=['int64','float']).isnull().sum()\/len(test_data.PassengerId)),2)\nmissing.loc[missing > 0]","889794ea":"missing =round(100*(train_data.select_dtypes(include=['int64','float']).isnull().sum()\/len(train_data.PassengerId)),2)\nmissing.loc[missing > 0]","1ffb3490":"# mapping each Cabin value with the cabin letter\ntrain_data['Cabin'] = train_data['Cabin'].map(lambda c : c[0])\n    \n# dummy encoding ...\ncabin_dummies = pd.get_dummies(train_data['Cabin'], prefix='Cabin')\n    \ntrain_data = pd.concat([train_data,cabin_dummies], axis=1)\n    \ntrain_data.drop('Cabin', axis=1, inplace=True)\n    \n# on Test\ntest_data['Cabin'] = test_data['Cabin'].map(lambda c : c[0])\n    \n# dummy encoding ...\ncabin_dummies = pd.get_dummies(test_data['Cabin'], prefix='Cabin')\n    \ntest_data = pd.concat([test_data,cabin_dummies], axis=1)\n    \ntest_data.drop('Cabin', axis=1, inplace=True)\n    ","f1fd7c81":"genders = {\"male\": 0, \"female\": 1}\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)\n\ntrain_data['Ticket'].describe()","41b54a96":"train_data = train_data.drop(['Ticket'], axis=1)\ntest_data = test_data.drop(['Ticket'], axis=1)","4c993fea":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","683e65c5":"data = [train_data, test_data]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n","c414b2a7":"train_data.head(10)","fbf7de0b":"data = [train_data, test_data]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)","916f4a8f":"data = [train_data, test_data]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']","66f3e9c6":"data = [train_data, test_data]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\ntrain_data['not_alone'].value_counts()","2228a2f4":"train_data = train_data.drop(['Title'], axis=1)\ntest_data = test_data.drop(['Title'], axis=1)","8030e29d":"data = [train_data, test_data]\nfor dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n# Let's take a last look at the training set, before we start training the models.\ntrain_data.head(20)","0f924c21":"train_data=train_data.drop(['Cabin_T'], axis=1)\nX_train = train_data.drop(['PassengerId' ,'Survived'], axis=1)\ny_train = train_data[\"Survived\"]\nX_test  = test_data.drop(\"PassengerId\", axis=1).copy()","8e8d8b8b":"X_train.head(5)","6aeba8b0":"X_test.head(5)","f5e0c5a9":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","5a0ba039":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 100)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nclassifier.score(X_train, y_train)\nacc_random_forest = round(classifier.score(X_train, y_train) * 100, 2)","ebf3c21f":"y_pred","f65fc3eb":"acc_random_forest","2d326faf":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, y_train) \nY_pred = decision_tree.predict(X_test) \nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)","df613e31":"Y_pred","706c6d3b":"acc_decision_tree","54cf4184":"from sklearn.svm import SVC, LinearSVC\nlinear_svc = LinearSVC(max_iter= 100, dual=False)\nlinear_svc.fit(X_train, y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)","c6c8a11a":"Y_pred","23e68694":"acc_linear_svc","74f2a3ff":"results = pd.DataFrame({\n    'Model': ['Random Forest','Decision Tree','Support Vector Machines'],\n    'Score': [acc_random_forest, acc_decision_tree, acc_linear_svc]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","d67eadf8":"#K-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","a95835ea":"# Feature Importance\nimportances = pd.DataFrame({'feature':train_data.drop(['PassengerId' , 'Survived'], axis=1).columns.values,'importance':np.round(classifier.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)","34aca007":"importances.plot.bar()","398b5931":"train_data = train_data.drop(\"not_alone\", axis=1)\ntest_data = test_data.drop(\"not_alone\", axis=1)\n\ntrain_data  = train_data.drop(\"Parch\", axis=1)\ntest_data  = test_data.drop(\"Parch\", axis=1)\n\ntrain_data = train_data.drop(\"Fare\", axis=1)\ntest_data = test_data.drop(\"Fare\", axis=1)\n\ntrain_data  = train_data.drop(\"Fare_Per_Person\", axis=1)\ntest_data  = test_data.drop(\"Fare_Per_Person\", axis=1)","71d8a983":"#Retranning the model\n\nclassifier = RandomForestClassifier(n_estimators=100, oob_score = True)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nclassifier.score(X_train, y_train)\nacc_random_forest = round(classifier.score(X_train, y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","c03e531c":"print(\"oob score:\", round(classifier.oob_score_, 4)*100, \"%\")","02eee067":"param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10], \"min_samples_split\" : [2,10,30,35,40], \"n_estimators\": [100, 300, 500]}\n","c8ae94cc":"\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\nclf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\nclf.fit(X_train, y_train)\nclf.best_params_","b4c30be4":"classifier = RandomForestClassifier(criterion = \"gini\", min_samples_leaf = 5, min_samples_split = 2, n_estimators=40, \n                                       max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\nclassifier.fit(X_train, y_train)\ny_prediction = classifier.predict(X_test)\n\nclassifier.score(X_train, y_train)\n\nprint(\"oob score:\", round(classifier.oob_score_, 4)*100, \"%\")","813ab834":"#Confussion Matrics\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(classifier, X_train, y_train, cv=3)\nconfusion_matrix(y_train, predictions)","b6651c53":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(y_train, predictions))\nprint(\"Recall:\",recall_score(y_train, predictions))","7e59a30b":"from sklearn.metrics import f1_score\nf1_score(y_train, predictions)","d8768ef6":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores =classifier.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(y_train, y_scores)\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","23d708af":"from sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_scores)\n# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","bb9333ad":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","b0cfb1f2":"prediction_submission = pd.DataFrame({'PassengerId':test_data['PassengerId'],'Survived':y_pred})","b5fd0456":"prediction_submission","c5d45230":"prediction_submission.to_csv('prediction_submission.csv', sep=\",\")","4c86c2a4":"So now both dataset have no missing value, now we can move further for analysis","8d0b7fd3":"## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we are going to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","a6a7d76f":"let's impute age with there median of sex and class according to title","c5b4dbfb":"we assume NaN data as None,so impute Nan as None","7b243fb0":"Impute Embarked as mode which is 'S'","d3c1578c":"In test data we have some missing value in Fare and we beleave that there is some fare, so i am going to impute missing value with median of fare","3484115d":"#### Let's examine Categorical variable","95c5a245":"Now look into numerical missing value","e59834c1":"##### let's convert Categorical column \n","fa567343":"### Data Preparation","ccf032e9":"### Load the data and understand data\n"}}