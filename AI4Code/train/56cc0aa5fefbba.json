{"cell_type":{"8860cfac":"code","ec78b859":"code","85783d3f":"code","4c55ef81":"code","a6729176":"code","c05f736d":"code","eca02ab8":"code","d0d3d516":"code","7b3fe22a":"code","45cce222":"code","dad724ec":"code","cd9efbf9":"markdown","b085bc22":"markdown","9e1b0c82":"markdown","f07bcbc5":"markdown","f0ed72bd":"markdown","d7be84dd":"markdown","c433ca98":"markdown","697db4fb":"markdown","d64bdd95":"markdown","25b8572f":"markdown","7176833c":"markdown","867a34f3":"markdown","1d5fe8a1":"markdown","526321b5":"markdown","670b3b28":"markdown"},"source":{"8860cfac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport time\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torchvision import models\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec78b859":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","85783d3f":"transform = transforms.Compose(\n    [transforms.Resize((224, 224)),\n     transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntrainset = torchvision.datasets.CIFAR10(root='.\/data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n                                          shuffle=True)\ntestset = torchvision.datasets.CIFAR10(root='.\/data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=32,\n                                         shuffle=False)","4c55ef81":"vgg16 = models.vgg16(pretrained=True)\nvgg16.to(device)\nprint(vgg16)","a6729176":"# change the number of classes \nvgg16.classifier[6].out_features = 10\n# freeze convolution weights\nfor param in vgg16.features.parameters():\n    param.requires_grad = False","c05f736d":"# optimizer\noptimizer = optim.SGD(vgg16.classifier.parameters(), lr=0.001, momentum=0.9)\n# loss function\ncriterion = nn.CrossEntropyLoss()","eca02ab8":"def validate(model, test_dataloader):\n    model.eval()\n    val_running_loss = 0.0\n    val_running_correct = 0\n    for int, data in enumerate(test_dataloader):\n        data, target = data[0].to(device), data[1].to(device)\n        output = model(data)\n        loss = criterion(output, target)\n        \n        val_running_loss += loss.item()\n        _, preds = torch.max(output.data, 1)\n        val_running_correct += (preds == target).sum().item()\n    \n    val_loss = val_running_loss\/len(test_dataloader.dataset)\n    val_accuracy = 100. * val_running_correct\/len(test_dataloader.dataset)\n    \n    return val_loss, val_accuracy","d0d3d516":"def fit(model, train_dataloader):\n    model.train()\n    train_running_loss = 0.0\n    train_running_correct = 0\n    for i, data in enumerate(train_dataloader):\n        data, target = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        train_running_loss += loss.item()\n        _, preds = torch.max(output.data, 1)\n        train_running_correct += (preds == target).sum().item()\n        loss.backward()\n        optimizer.step()\n    train_loss = train_running_loss\/len(train_dataloader.dataset)\n    train_accuracy = 100. * train_running_correct\/len(train_dataloader.dataset)\n    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}')\n    \n    return train_loss, train_accuracy","7b3fe22a":"train_loss , train_accuracy = [], []\nval_loss , val_accuracy = [], []\nstart = time.time()\nfor epoch in range(10):\n    train_epoch_loss, train_epoch_accuracy = fit(vgg16, trainloader)\n    val_epoch_loss, val_epoch_accuracy = validate(vgg16, testloader)\n    train_loss.append(train_epoch_loss)\n    train_accuracy.append(train_epoch_accuracy)\n    val_loss.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)\nend = time.time()\nprint((end-start)\/60, 'minutes')","45cce222":"plt.figure(figsize=(10, 7))\nplt.plot(train_accuracy, color='green', label='train accuracy')\nplt.plot(val_accuracy, color='blue', label='validataion accuracy')\nplt.legend()\nplt.savefig('accuracy.png')\nplt.show()","dad724ec":"plt.figure(figsize=(10, 7))\nplt.plot(train_loss, color='orange', label='train loss')\nplt.plot(val_loss, color='red', label='validataion loss')\nplt.legend()\nplt.savefig('loss.png')\nplt.show()","cd9efbf9":"# VGG16\n### We will be downloading the VGG16 from PyTorch models and it uses the weights of ImageNet.","b085bc22":"## Optimizer and Loss Function","9e1b0c82":"One may observe that one of the transforms is resizing the images to 224\u00d7224 size. Well, this is because the VGG network takes an input image of size 224\u00d7224 by default. So, it is best to resize the CIFAR10 images as well.\n\nAnother thing to take care of here is the batch size. Remember that, if the CUDA device is being used, then we will be loading all the data and the VGG16 model into the CUDA GPU memory. This may require a lot of GPU RAM. If you face OOM (Out Of Memory) error, then consider reducing the batch size. It is best to choose the batch size as a multiple of 2. So, you may choose either 16, 8, or 4 according to your requirement.","f07bcbc5":"## Summary and Conclusion\n\nWe are getting fairly good results, but we can do even better. We can fine-tune the features model values of VGG16 and try to get even more accuracy. One way to get started is to freeze some layers and train some others. We have only tried freezing all of the convolution layers.\n\nI hope that you learned something from this kernel. You can comment and leave your thoughts and queries in the comment section.","f0ed72bd":"## Importing Required Libraries","d7be84dd":"The models module from torchvision will help us to download the VGG16 neural network.\n\nThe next block of code is for checking the CUDA availability. If you have a dedicated CUDA GPU device, then it will be used. Else, further on, your CPU will be used for the neural network operations.","c433ca98":"We are now going to download the VGG16 model from PyTorch models. The following code loads the VGG16 model.","697db4fb":"## Training and Validation Functions","d64bdd95":"## Loading VGG16 Network","25b8572f":"After each epoch, we are saving the training accuracy and loss values in train_accuracy, train_loss and val_accuracy, val_loss. We can see that by the end of the training, our training accuracy is 98.32%.","7176833c":"We need to classify the images into 10 classes only. So, we will change the the output features to 10. Also, we will freeze all the weights of the convolutional blocks. The model as already learned many features from the ImageNet dataset. So, freezing the Conv2d() weights will make the model to use all those pre-trained weights.","867a34f3":"## Freeze Convolution Weights","1d5fe8a1":"We can see that the validation accuracy was more at the beginning. But with advancing epochs, finally, the model was able to learn the important features. By the end of the training, the training accuracy is much higher than the validation accuracy. Specifically, we are getting about 98% training and 87% validation accuracy.","526321b5":"## Loading and Preparing Data","670b3b28":"## If you found this kernel informative Please do upvote."}}