{"cell_type":{"7f8185d4":"code","d679cd00":"code","d00ae92d":"code","8f5b2204":"code","4f7b87df":"code","e5e9eef7":"code","b43c1b4b":"code","d11074fc":"code","9ab0c942":"code","f0ec8354":"code","eb91503e":"code","22cd5bbe":"code","bd4c7169":"code","a3298abb":"code","2e8cb153":"code","3a825495":"code","5e1043d3":"code","7ca7bdfb":"code","c738470e":"code","040c8c04":"code","1ccec51a":"code","55399108":"code","6f8dd0e1":"code","e1e72668":"code","85a62f62":"code","a4cc7035":"code","1bdc75fd":"code","c8086ee4":"markdown","48adf667":"markdown","55c3d28c":"markdown","eb382280":"markdown","6e78d794":"markdown","f9b0ec9d":"markdown","987ea5bc":"markdown","7af79c4b":"markdown","16884940":"markdown","cf22c6c7":"markdown","3335ab8a":"markdown","ca65bf4d":"markdown","cc52c4e4":"markdown","39ae59d9":"markdown"},"source":{"7f8185d4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold, GroupKFold\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nplt.style.use('ggplot')\nimport warnings\nwarnings.filterwarnings('ignore')","d679cd00":"train = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/train.parquet')\ntest = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/test.parquet')\nss = pd.read_csv('..\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')","d00ae92d":"cfg = {\n    'TARGET' : 'target',\n    'N_FOLDS' : 5,\n    'RANDOM_STATE': 69,\n    'N_ESTIMATORS' : 15_000,\n    'LEARNING_RATE': 0.05\n}\n\ntrain_vids = train['video_id'].unique()","8f5b2204":"kf = KFold(n_splits=cfg['N_FOLDS'],\n           shuffle=True,\n           random_state=cfg['RANDOM_STATE'])\n\ng_kf = GroupKFold(n_splits=cfg['N_FOLDS'])\n           #shuffle=True,\n           #random_state=cfg['RANDOM_STATE'])\n\n#g_kf.get_n_splits(X, y, groups)\n\n# Create Folds\nfold = 1\nfor tr_idx, val_idx in g_kf.split(train_vids, groups=train_vids):\n    fold_vids = train_vids[val_idx]\n    train.loc[train['video_id'].isin(fold_vids), 'fold'] = fold\n    fold += 1\ntrain['fold'] = train['fold'].astype('int')","4f7b87df":"g_kf.get_n_splits(train_vids, groups=train_vids)","e5e9eef7":"fd_2_ids=train[train['fold']==2]['video_id'].unique()\nfd_2_ids=list(fd_2_ids)","b43c1b4b":"fd_1=train[train['fold']==1]\nfd_1[fd_1['video_id'].isin(fd_2_ids)]","d11074fc":"def create_across_folds_features(df, train=True):\n    \"\"\"\n    Adds features to training or test set.\n    \"\"\"\n    df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n    df['trending_date'] = pd.to_datetime(df['trending_date'], utc=True)\n    \n    # Feature 1 - Age of video\n    df['video_age_seconds'] = (df['trending_date'] - df['publishedAt']).dt.total_seconds().astype('int')\n    \n    # Trending day of week As a category\n    df['trending_dow'] = df['trending_date'].dt.day_name()\n    df['trending_dow']= df['trending_dow'].astype('category')\n    \n    df['published_dow'] = df['publishedAt'].dt.day_name()\n    df['published_dow']= df['published_dow'].astype('category')\n    \n    \n   \n    \n    #should be calculated across train and test\n    df['channel_occurance'] = df['channelId'].map(df['channelId'].value_counts().to_dict())\n    \n    #should be calculated across train and test\n    df['channel_unique_video_count'] = df['channelId'].map(df.groupby('channelId')['video_id'].nunique().to_dict()) \n    \n    #should be calculated across train and test\n    df['video_occurance_count'] = df.groupby('video_id')['trending_date'].rank().astype('int')\n    \n    #should be calculated across train and test\n    df['Total_Trending_Days']=df['video_id'].map(df.groupby('video_id')['id'].count().to_dict())\n    #should be calculated across train and test\n    df['min_trending_dt']=df['video_id'].map(df.groupby('video_id')['trending_date'].min().to_dict())\n    df['days_since_first_trend']=(df['trending_date']-df['min_trending_dt']).dt.days\n    \n    \n    \n    \n    \n    \n    \n    df['categoryId_cat'] = df['categoryId'].astype('category')\n    df['channelId_cat'] = df['channelId'].astype('category')\n    df['video_id_cat'] = df['video_id'].astype('category')\n    return df","9ab0c942":"def create_within_folds_train_features(df, train=True):   \n#Video_initial_ratio - NaN         5498 and 0.000000     426\n    \n    df['video_initial_ratio']=df['video_id'].map(df[df['min_trending_dt']==df['trending_date']][['video_id','target']].set_index('video_id').squeeze(axis=1).to_dict())\n    \n    #channel_mean_initial_ratio - NaN         1202\n    df['channel_mean_initial_ratio']=df['channelId'].map(df.groupby('channelId')['video_initial_ratio'].mean().to_dict())\n    \n    df['channel_target_overall_avg']=df['channelId'].map(df.groupby('channelId')['target'].mean().to_dict())\n    \n    #0.06650316611115485    19762\n    df['categoryId_mean_initial_ratio']=df['categoryId'].map(df.groupby('categoryId')['video_initial_ratio'].mean().to_dict())\n    \n    df['category_target_overall_avg']=df['categoryId'].map(df.groupby('categoryId')['target'].mean().to_dict())\n    \n    #if Video_initial_ratio is null impute with channel_mean_initial_ratio, \n        #if still null impute with categoryId_mean_initial_ratio\n    \n    df.loc[df['video_initial_ratio'].isnull(),'video_initial_ratio']=df['channel_mean_initial_ratio']    \n    df.loc[df['video_initial_ratio'].isnull(),'video_initial_ratio']=df['categoryId_mean_initial_ratio']\n    \n    #if channel_mean_initial_ratio is null impute with categoryId_mean_initial_ratio\n    df.loc[df['channel_mean_initial_ratio'].isnull(),'channel_mean_initial_ratio']=df['categoryId_mean_initial_ratio']\n    \n    \n    \n    #16,077 where days trending =1. \n    #Provide the scaffolding for\n        #Use video's day=0 Ratio to predict day=1 ratio\n        #Use video's day=1 Ratio to predict day=3 ratio\n        #Use video's day=2 Ratio to predict day=4 ratio etc....\n    df['video_1_days_since_trend_ratio']=df['video_id'].map(df[df['days_since_first_trend']==1][['video_id','target']].set_index('video_id').squeeze(axis=1).to_dict())\n    df['video_2_days_since_trend_ratio']=df['video_id'].map(df[df['days_since_first_trend']==2][['video_id','target']].set_index('video_id').squeeze(axis=1).to_dict())\n    df['video_3_days_since_trend_ratio']=df['video_id'].map(df[df['days_since_first_trend']==3][['video_id','target']].set_index('video_id').squeeze(axis=1).to_dict())\n    df['video_4_days_since_trend_ratio']=df['video_id'].map(df[df['days_since_first_trend']==4][['video_id','target']].set_index('video_id').squeeze(axis=1).to_dict())\n    df['video_5_days_since_trend_ratio']=df['video_id'].map(df[df['days_since_first_trend']==5][['video_id','target']].set_index('video_id').squeeze(axis=1).to_dict())\n    df['video_6_days_since_trend_ratio']=df['video_id'].map(df[df['days_since_first_trend']==6][['video_id','target']].set_index('video_id').squeeze(axis=1).to_dict())\n    df['video_7_days_since_trend_ratio']=df['video_id'].map(df[df['days_since_first_trend']==7][['video_id','target']].set_index('video_id').squeeze(axis=1).to_dict())\n    \n    df['channel_trend_day_1_mean_ratio']=df['channelId'].map(df.groupby('channelId')['video_1_days_since_trend_ratio'].mean().to_dict())\n    df['channel_trend_day_2_mean_ratio']=df['channelId'].map(df.groupby('channelId')['video_2_days_since_trend_ratio'].mean().to_dict())\n    df['channel_trend_day_3_mean_ratio']=df['channelId'].map(df.groupby('channelId')['video_3_days_since_trend_ratio'].mean().to_dict())\n    df['channel_trend_day_4_mean_ratio']=df['channelId'].map(df.groupby('channelId')['video_4_days_since_trend_ratio'].mean().to_dict())\n    df['channel_trend_day_5_mean_ratio']=df['channelId'].map(df.groupby('channelId')['video_5_days_since_trend_ratio'].mean().to_dict())\n    df['channel_trend_day_6_mean_ratio']=df['channelId'].map(df.groupby('channelId')['video_6_days_since_trend_ratio'].mean().to_dict())\n    df['channel_trend_day_7_mean_ratio']=df['channelId'].map(df.groupby('channelId')['video_7_days_since_trend_ratio'].mean().to_dict())\n    \n    df['category_trend_day_1_mean_ratio']=df['categoryId'].map(df.groupby('categoryId')['video_1_days_since_trend_ratio'].mean().to_dict())\n    df['category_trend_day_2_mean_ratio']=df['categoryId'].map(df.groupby('categoryId')['video_2_days_since_trend_ratio'].mean().to_dict())\n    df['category_trend_day_3_mean_ratio']=df['categoryId'].map(df.groupby('categoryId')['video_3_days_since_trend_ratio'].mean().to_dict())\n    df['category_trend_day_4_mean_ratio']=df['categoryId'].map(df.groupby('categoryId')['video_4_days_since_trend_ratio'].mean().to_dict())\n    df['category_trend_day_5_mean_ratio']=df['categoryId'].map(df.groupby('categoryId')['video_5_days_since_trend_ratio'].mean().to_dict())\n    df['category_trend_day_6_mean_ratio']=df['categoryId'].map(df.groupby('categoryId')['video_6_days_since_trend_ratio'].mean().to_dict())\n    df['category_trend_day_7_mean_ratio']=df['categoryId'].map(df.groupby('categoryId')['video_7_days_since_trend_ratio'].mean().to_dict())\n    \n    \n    #if channel_trend_day_1_mean_ratio is null impute with category_trend_day_1_mean_ratio\n    df.loc[df['channel_trend_day_1_mean_ratio'].isnull(),'channel_trend_day_1_mean_ratio']=df['category_trend_day_1_mean_ratio']\n    df.loc[df['channel_trend_day_2_mean_ratio'].isnull(),'channel_trend_day_2_mean_ratio']=df['category_trend_day_2_mean_ratio']\n    df.loc[df['channel_trend_day_3_mean_ratio'].isnull(),'channel_trend_day_3_mean_ratio']=df['category_trend_day_3_mean_ratio']\n    df.loc[df['channel_trend_day_4_mean_ratio'].isnull(),'channel_trend_day_4_mean_ratio']=df['category_trend_day_4_mean_ratio']\n    df.loc[df['channel_trend_day_5_mean_ratio'].isnull(),'channel_trend_day_5_mean_ratio']=df['category_trend_day_5_mean_ratio']\n    df.loc[df['channel_trend_day_6_mean_ratio'].isnull(),'channel_trend_day_6_mean_ratio']=df['category_trend_day_6_mean_ratio']\n    df.loc[df['channel_trend_day_7_mean_ratio'].isnull(),'channel_trend_day_7_mean_ratio']=df['category_trend_day_7_mean_ratio']\n    \n\n    return df","f0ec8354":"def create_within_folds_test_features(df_train,df_test, train=True):\n    \n    train_chID_avg=df_train[['channelId','channel_target_overall_avg']].set_index('channelId').squeeze(axis=1).to_dict()\n    df_test['channel_target_overall_avg']=df_test['channelId'].map(train_chID_avg)\n    \n    \n    train_catID_avg=df_train[['categoryId','category_target_overall_avg']].set_index('categoryId').squeeze(axis=1).to_dict()\n    df_test['category_target_overall_avg']=df_test['categoryId'].map(train_catID_avg)\n    \n    train_chID_to_CH_day_0=df_train[['channelId','channel_mean_initial_ratio']].set_index('channelId').squeeze(axis=1).to_dict()\n    df_test['channel_mean_initial_ratio']=df_test['channelId'].map(train_chID_to_CH_day_0)\n\n\n    train_chID_to_CH_day_1=df_train[['channelId','channel_trend_day_1_mean_ratio']].set_index('channelId').squeeze(axis=1).to_dict()\n    df_test['channel_trend_day_1_mean_ratio']=df_test['channelId'].map(train_chID_to_CH_day_1)\n    train_chID_to_CH_day_2=df_train[['channelId','channel_trend_day_2_mean_ratio']].set_index('channelId').squeeze(axis=1).to_dict()\n    df_test['channel_trend_day_2_mean_ratio']=df_test['channelId'].map(train_chID_to_CH_day_2)\n    train_chID_to_CH_day_3=df_train[['channelId','channel_trend_day_3_mean_ratio']].set_index('channelId').squeeze(axis=1).to_dict()\n    df_test['channel_trend_day_3_mean_ratio']=df_test['channelId'].map(train_chID_to_CH_day_3)\n    train_chID_to_CH_day_4=df_train[['channelId','channel_trend_day_4_mean_ratio']].set_index('channelId').squeeze(axis=1).to_dict()\n    df_test['channel_trend_day_4_mean_ratio']=df_test['channelId'].map(train_chID_to_CH_day_4)\n    train_chID_to_CH_day_5=df_train[['channelId','channel_trend_day_5_mean_ratio']].set_index('channelId').squeeze(axis=1).to_dict()\n    df_test['channel_trend_day_5_mean_ratio']=df_test['channelId'].map(train_chID_to_CH_day_5)\n    train_chID_to_CH_day_6=df_train[['channelId','channel_trend_day_6_mean_ratio']].set_index('channelId').squeeze(axis=1).to_dict()\n    df_test['channel_trend_day_6_mean_ratio']=df_test['channelId'].map(train_chID_to_CH_day_6)\n    train_chID_to_CH_day_7=df_train[['channelId','channel_trend_day_7_mean_ratio']].set_index('channelId').squeeze(axis=1).to_dict()\n    df_test['channel_trend_day_7_mean_ratio']=df_test['channelId'].map(train_chID_to_CH_day_7)\n\n    train_catID_to_CAT_day_0=df_train[['categoryId','categoryId_mean_initial_ratio']].set_index('categoryId').squeeze(axis=1).to_dict()\n    df_test['categoryId_mean_initial_ratio']=df_test['categoryId'].map(train_catID_to_CAT_day_0)\n\n    train_catID_to_CAT_day_1=df_train[['categoryId','category_trend_day_1_mean_ratio']].set_index('categoryId').squeeze(axis=1).to_dict()\n    df_test['category_trend_day_1_mean_ratio']=df_test['channelId'].map(train_catID_to_CAT_day_1)\n    train_catID_to_CAT_day_2=df_train[['categoryId','category_trend_day_2_mean_ratio']].set_index('categoryId').squeeze(axis=1).to_dict()\n    df_test['category_trend_day_2_mean_ratio']=df_test['channelId'].map(train_catID_to_CAT_day_2)\n    train_catID_to_CAT_day_3=df_train[['categoryId','category_trend_day_3_mean_ratio']].set_index('categoryId').squeeze(axis=1).to_dict()\n    df_test['category_trend_day_3_mean_ratio']=df_test['channelId'].map(train_catID_to_CAT_day_3)\n    train_catID_to_CAT_day_4=df_train[['categoryId','category_trend_day_4_mean_ratio']].set_index('categoryId').squeeze(axis=1).to_dict()\n    df_test['category_trend_day_4_mean_ratio']=df_test['channelId'].map(train_catID_to_CAT_day_4)\n    train_catID_to_CAT_day_5=df_train[['categoryId','category_trend_day_5_mean_ratio']].set_index('categoryId').squeeze(axis=1).to_dict()\n    df_test['category_trend_day_5_mean_ratio']=df_test['channelId'].map(train_catID_to_CAT_day_5)\n    train_catID_to_CAT_day_6=df_train[['categoryId','category_trend_day_6_mean_ratio']].set_index('categoryId').squeeze(axis=1).to_dict()\n    df_test['category_trend_day_6_mean_ratio']=df_test['channelId'].map(train_catID_to_CAT_day_6)\n    train_catID_to_CAT_day_7=df_train[['categoryId','category_trend_day_7_mean_ratio']].set_index('categoryId').squeeze(axis=1).to_dict()\n    df_test['category_trend_day_7_mean_ratio']=df_test['channelId'].map(train_catID_to_CAT_day_7)\n\n\n    \n# #     #if video_2_days_since_trend_ratio is null impute with video_2_days_since_trend_ratio\n#     df.loc[df['video_1_days_since_trend_ratio'].isnull(),'video_1_days_since_trend_ratio']=df['channel_trend_day_1_mean_ratio']\n#     df.loc[df['video_2_days_since_trend_ratio'].isnull(),'video_2_days_since_trend_ratio']=df['channel_trend_day_2_mean_ratio']\n#     df.loc[df['video_3_days_since_trend_ratio'].isnull(),'video_3_days_since_trend_ratio']=df['channel_trend_day_3_mean_ratio']\n#     df.loc[df['video_4_days_since_trend_ratio'].isnull(),'video_4_days_since_trend_ratio']=df['channel_trend_day_4_mean_ratio']\n#     df.loc[df['video_5_days_since_trend_ratio'].isnull(),'video_5_days_since_trend_ratio']=df['channel_trend_day_5_mean_ratio']\n#     df.loc[df['video_6_days_since_trend_ratio'].isnull(),'video_6_days_since_trend_ratio']=df['channel_trend_day_6_mean_ratio']\n#     df.loc[df['video_7_days_since_trend_ratio'].isnull(),'video_7_days_since_trend_ratio']=df['channel_trend_day_7_mean_ratio']\n    \n\n    \n#     df['categoryId'] = df['categoryId'].astype('category')\n#     df['channelId'] = df['channelId'].astype('category')\n#     df['video_id'] = df['video_id'].astype('category')\n    return df_test","eb91503e":"# #Test Set & Train Set\n\n# #18,187 videos with days_since_first_trend ==0\n# tt[tt['days_since_first_trend']==0]\n\n# #16,088 videos with days_since_first_trend ==1\n# tt[tt['days_since_first_trend']==1]\n\n# #16,184 videos with days_since_first_trend ==2\n# tt[tt['days_since_first_trend']==2]\n\n# #15761 videos with days_since_first_trend ==3\n# tt[tt['days_since_first_trend']==3].count()[1]\n","22cd5bbe":"train['isTrain'] = True\ntest['isTrain'] = False\ntt = pd.concat([train, test]).reset_index(drop=True).copy()\ntt = create_across_folds_features(tt)\n\ntrain_feats = tt.query('isTrain').reset_index(drop=True).copy()\ntest_feats = tt.query('isTrain == False').reset_index(drop=True).copy()","bd4c7169":"#For Final Test\/Submission Feature engineering, create a 'leaked train Feature Engineered' dataframe\n#to feed values into final test\/submission data set\n\ndf_leaked_train_fe=train_feats.copy()\ndf_leaked_train_fe = create_within_folds_train_features(df_leaked_train_fe)\n\ntest_feats = create_within_folds_test_features(df_leaked_train_fe,test_feats)","a3298abb":"FEATURES = ['video_age_seconds',\n            'trending_dow',\n            'channelId_cat',\n            'video_id_cat',\n            'published_dow',\n            'duration_seconds',\n            'categoryId_cat',\n            'comments_disabled',\n            'ratings_disabled',\n            'channel_occurance',\n            'channel_unique_video_count',\n            'video_occurance_count',\n            'Total_Trending_Days',\n            'days_since_first_trend',\n            #'video_initial_ratio',\n#             'channel_target_overall_avg',\n#             'category_target_overall_avg',\n            'channel_mean_initial_ratio',\n            'categoryId_mean_initial_ratio',\n            'channel_trend_day_1_mean_ratio',\n'channel_trend_day_2_mean_ratio',\n'channel_trend_day_3_mean_ratio',\n'channel_trend_day_4_mean_ratio',\n'channel_trend_day_5_mean_ratio',\n'channel_trend_day_6_mean_ratio',\n'channel_trend_day_7_mean_ratio',\n#             'category_trend_day_1_mean_ratio',\n# 'category_trend_day_2_mean_ratio',\n# 'category_trend_day_3_mean_ratio',\n# 'category_trend_day_4_mean_ratio',\n# 'category_trend_day_5_mean_ratio',\n# 'category_trend_day_6_mean_ratio',\n# 'category_trend_day_7_mean_ratio',\n#             'video_1_days_since_trend_ratio',\n# 'video_2_days_since_trend_ratio',\n# 'video_3_days_since_trend_ratio',\n# 'video_4_days_since_trend_ratio',\n# 'video_5_days_since_trend_ratio',\n# 'video_6_days_since_trend_ratio',\n# 'video_7_days_since_trend_ratio'\n]\n\nTARGET = ['target']","2e8cb153":"X_test = test_feats[FEATURES]\noof = train_feats[['id','target','fold']].reset_index(drop=True).copy()\nsubmission_df = test[['id']].copy()","3a825495":"#https:\/\/towardsdatascience.com\/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5","5e1043d3":"regs = []\nfis = []\n# Example Fold 1\nfor fold in range(1, 6):\n    print(f'===== Running for fold {fold} =====')\n    # Split train \/ val\n    X_tr = train_feats.query('fold != @fold')\n    X_tr = create_within_folds_train_features(X_tr)\n    y_tr = train_feats.query('fold != @fold')[TARGET]\n    X_val = train_feats.query('fold == @fold')\n    X_val = create_within_folds_test_features(X_tr,X_val)\n    y_val = train_feats.query('fold == @fold')[TARGET]\n    \n    X_tr = X_tr[FEATURES]\n    X_val = X_val[FEATURES]\n    print(X_tr.shape, y_tr.shape, X_val.shape, y_val.shape)\n\n    # Create our model\n    reg = lgb.LGBMRegressor(n_estimators=cfg['N_ESTIMATORS'],\n                            learning_rate=cfg['LEARNING_RATE'],\n                            objective='mae',\n                            metric=['mae'],\n                            importance_type='gain',\n                            boosting='dart'\n                            #max_depth= 7, min_child_weight= 1\n                           )\n    # Fit our model\n    reg.fit(X_tr, y_tr,\n            eval_set=(X_val, y_val),\n            early_stopping_rounds=500,\n            verbose=200,\n           )\n\n    # Predicting on validation set\n    fold_preds = reg.predict(X_val,\n                             num_iteration=reg.best_iteration_)\n    oof.loc[oof['fold'] == fold, 'preds'] = fold_preds\n    # Score validation set\n    fold_score = mean_absolute_error(\n        oof.query('fold == 1')['target'],\n            oof.query('fold == 1')['preds']\n    )\n\n    # Creating a feature importance dataframe\n    fi = pd.DataFrame(index=reg.feature_name_,\n                 data=reg.feature_importances_,\n                 columns=[f'{fold}_importance'])\n\n    # Predicting on test\n    fold_test_pred = reg.predict(X_test,\n                num_iteration=reg.best_iteration_)\n    submission_df[f'pred_{fold}'] = fold_test_pred\n\n    print(f'Score of this fold is {fold_score:0.6f}')\n    regs.append(reg)\n    fis.append(fi)","7ca7bdfb":"oof_score = mean_absolute_error(oof['target'], oof['preds'])\nprint(f'Out of fold score {oof_score:0.6f}')","c738470e":"fis_df = pd.concat(fis, axis=1)\nfis_df.sort_values('1_importance').plot(kind='barh', figsize=(12, 15),\n                                       title='Feature Importance Across Folds')\nplt.show()","040c8c04":"train_unique_channel_ct=tt[tt['isTrain']]['channel_unique_video_count']\ntrain_channel_occurance=tt[tt['isTrain']]['channel_occurance']\ntest_unique_channel_ct=tt[~(tt['isTrain'])]['channel_unique_video_count']\ntest_channel_occurance=tt[~(tt['isTrain'])]['channel_occurance']","1ccec51a":"f = plt.figure()\n\nf.add_subplot(2, 2, 1)\nsns.distplot(train_unique_channel_ct).set(title='Train Set - Channel Id'+'\\n # of Unique Videos')\n\nf.add_subplot(2, 2, 2)\nsns.distplot(train_channel_occurance).set(title='Train Set - Channel Id'+'\\n Total # of Videos')\n\nf.add_subplot(2, 2, 3)\nsns.distplot(test_unique_channel_ct).set(title='Test Set - Channel Id'+'\\n # of Unique Videos')\n\nf.add_subplot(2, 2, 4)\nsns.distplot(test_channel_occurance).set(title='Test Set - Channel Id'+'\\n Total # of Videos')\n\nplt.gcf().set_size_inches(18, 15)\nplt.show()","55399108":"tt[tt['isTrain']]['channel_unique_video_count'].mean()","6f8dd0e1":"tt[tt['isTrain']]['channel_occurance'].mean()","e1e72668":"tt[~(tt['isTrain'])]['channel_unique_video_count'].mean()","85a62f62":"tt[~(tt['isTrain'])]['channel_occurance'].mean()","a4cc7035":"pred_cols = [c for c in submission_df.columns if c.startswith('pred_')]\n\nsubmission_df['target'] = submission_df[pred_cols].mean(axis=1)\n# Visually check correlation between fold predictions\nsns.heatmap(submission_df[pred_cols].corr(), annot=True)","1bdc75fd":"submission_df[['id','target']].to_csv('submission.csv', index=False)","c8086ee4":"## Create Folds\n- This is how we will later split when validating our models","48adf667":"## Model 1 - Train LGBM Model","55c3d28c":"## Model 1- Evaluation out of all out of fold predictions","eb382280":"## Model 1- Look at Fold Feature Importances","6e78d794":"## Model 1- Create Submission","f9b0ec9d":"# Pog Competition 1\n- Notes - to Ensure accurate CV:\n    - If creating a test feature such as Channel Id's Mean, ensure its only created from Train Video Ids\n\n","987ea5bc":"## Further EDA- Channel Id's Cardinality for Train and Test","7af79c4b":"### Categorical Predictor Overlap between Test and Training Datasets\n-Test - category Ids - 5800 of 5800 rows in found in Training Datase\n\n-Test - channell Ids - 4598 of 5800 rows in found in Training Dataset\n\n-Test - Video Ids - 300 of 5800 rows in found in Training Dataset","16884940":"## Setup KFold","cf22c6c7":"## Model 1 - Set Target and Features","3335ab8a":"### Video's Initial Ratio is Best Feature for Predicting Ratio at Trend Day=2, Trend Day=3, etc... \n-(did not work) if there No Video Initial Ratio exists, impute with Channel's Mean Initial Ratio \n\n-Build a second model for those Test Video Ids found in Training Data Set","ca65bf4d":"#### Days Since First Trending - Further Investigation\n- days_since_first_trend==2 has a higher count than days_since_first_trend==1","cc52c4e4":"### Feature- Video_id x days since Trend Ratio\n- Use video's day=0 Ratio to predict day=1 ratio\n- Use video's day=1 Ratio to predict day=3 ratio\n- Use video's day=2 Ratio to predict day=4 ratio etc....\n","39ae59d9":"## Feature Engineering"}}