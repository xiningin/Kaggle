{"cell_type":{"9dc3e585":"code","7f1ead31":"code","e34a97b5":"code","25012274":"code","7a3304a9":"code","67675655":"code","3728ebe1":"code","45fa6232":"code","89005ec8":"code","d64a6914":"markdown","f8dec965":"markdown","3f6db103":"markdown","49791bed":"markdown","54405bc9":"markdown","0c49f966":"markdown","af785644":"markdown","9ec0ed64":"markdown","3f43e8b3":"markdown"},"source":{"9dc3e585":"import pandas as pd\nspam = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\", encoding = \"ISO-8859-1\",names=[\"label\",\"message\",\"v3\",\"v4\",\"v5\"])\ndel spam[\"v3\"]\ndel spam[\"v4\"]\ndel spam[\"v5\"]\nspam = spam.drop(spam.index[0])","7f1ead31":"spam.label.unique()","e34a97b5":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\n ","25012274":"# we are interating from index 1 because the zero index row is already deleted in previous step.\ncorpus = []\nfor i in range(1, len(spam)+1):\n    review  = re.sub('[^a-zA-Z]',' ',spam['message'][i])\n    review  = review.lower()\n    review  = review.split()\n    review  = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review  = ' '.join(review)\n    corpus.append(review)\n    \nprint(corpus[:10])  \n \n    ","7a3304a9":" from sklearn.feature_extraction.text import CountVectorizer\n cv = CountVectorizer(max_features=5000)\n x = cv.fit_transform(corpus).toarray()\n","67675655":"y = pd.get_dummies(spam[\"label\"])\ny = y.iloc[:,1].values","3728ebe1":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state=0)","45fa6232":"from sklearn.naive_bayes import MultinomialNB\nspam_detect_model = MultinomialNB().fit(x_train, y_train)","89005ec8":"y_pred = spam_detect_model.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_m = confusion_matrix(y_test,y_pred)\nprint(confusion_m)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","d64a6914":"<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">The target column &quot;label&quot; contains text data which is &quot;ham&quot; or &quot;spam&quot;,&nbsp;<\/span><\/span><\/p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">we need to convert this to numerical value for processing.&nbsp;<\/span><\/span><\/p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">get_dummies will do this for us, creates two columns for each category.<\/span><\/span><\/p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">We can choose any one of them using iloc<\/span><\/span><\/p>\n","f8dec965":"<p><u><span style=\"color:#a52a2a\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Iterate through all the messages and do the following:<\/span><\/span><\/u><\/p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">a) Remove all the characters except a to z and A to Z<\/span><\/span><\/p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">b) Convert all the characters to lower case so that there cant be any duplicates<\/span><\/span><\/p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">c) Split the sentences to get the list of words&nbsp;<\/span><\/span><\/p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">d) for each of the word in a Sentense, stem that (get the base word) and remove all the words if that exists in stop words &nbsp;<\/span><\/span><\/p>\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">   (Stop words are like the, is, was, those, these,... e.t.c) &nbsp;<\/span><\/span><\/p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">&nbsp;e) The final output will be the messages that contains the&nbsp;result of step a to d&nbsp;<\/span><\/span><\/p>\n\n<p>&nbsp; &nbsp; &nbsp;&nbsp;<\/p>\n","3f6db103":"<p>&nbsp; <span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Create vectorizer that takes the top 5000 most frequent words<\/span><\/span><\/p>","49791bed":"<p>&nbsp; <span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Split the data into training and test sets<\/span><\/span><\/p>","54405bc9":"<p>&nbsp; <span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Use the test data to predict , check the confusion matrix and observe the accuracy<\/span><\/span><\/p>","0c49f966":"<p><u><span style=\"color:#a52a2a\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Below are the steps that we are going to do:<\/span><\/span><\/u><\/p>\n\n<p style=\"margin-left: 40px;\"><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">1. Read data from the dataset<\/span><\/span><\/p>\n\n<p style=\"margin-left: 40px;\"><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">2. Remove the empty columns v3,v4,v5&nbsp;<\/span><\/span><\/p>\n\n<p style=\"margin-left: 40px;\"><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">3. Remove the first row which contains v1, v2 which is not releveant to our dataset<\/span><\/span><\/p>\n","af785644":"<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Just to confirm if there are any invalid data, analyze the unique target variable( in our case the target column is the &quot;label&quot;)&nbsp;, and the result is the expected &quot;ham&quot; and &quot;spam&quot;; &nbsp;so we can proceed further<\/span><\/span><\/p>\n","9ec0ed64":"<p>&nbsp; <span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Fit using Naive Bayes<\/span><\/span><\/p>","3f43e8b3":"<p style=\"text-align: center;\"><span style=\"font-size:18px\"><span style=\"color:#2f4f4f\"><strong><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Spam detection - Multinomial Naive Bayes<\/span><\/strong><\/span><\/span><\/p>"}}