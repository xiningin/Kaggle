{"cell_type":{"d95555e3":"code","22958fcc":"code","0de3e4e9":"code","89d4e759":"code","ae689d9b":"code","6e2752b2":"markdown","1716685c":"markdown","968f5710":"markdown","d4615963":"markdown","77b334f5":"markdown","065b58bd":"markdown"},"source":{"d95555e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","22958fcc":"from google.cloud import bigquery\nfrom time import time\n\nclient = bigquery.Client()\n\ndef show_amount_of_data_scanned(query):\n    # dry_run lets us see how much data the query uses without running it\n    dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n    query_job = client.query(query, job_config=dry_run_config)\n    print('Data processed: {} GB'.format(round(query_job.total_bytes_processed \/ 10**9, 3)))\n    \ndef show_time_to_run(query):\n    time_config = bigquery.QueryJobConfig(use_query_cache=False)\n    start = time()\n    query_result = client.query(query, job_config=time_config).result()\n    end = time()\n    print('Time to run: {} seconds'.format(round(end-start, 3)))","0de3e4e9":"star_query = \"SELECT * FROM `bigquery-public-data.github_repos.contents`\"\nshow_amount_of_data_scanned(star_query)\n\nbasic_query = \"SELECT size, binary FROM `bigquery-public-data.github_repos.contents`\"\nshow_amount_of_data_scanned(basic_query)","89d4e759":"more_data_query = \"\"\"\n                  SELECT MIN(start_station_name) AS start_station_name,\n                      MIN(end_station_name) AS end_station_name,\n                      AVG(duration_sec) AS avg_duration_sec\n                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n                  WHERE start_station_id != end_station_id \n                  GROUP BY start_station_id, end_station_id\n                  LIMIT 10\n                  \"\"\"\nshow_amount_of_data_scanned(more_data_query)\n\nless_data_query = \"\"\"\n                  SELECT start_station_name,\n                      end_station_name,\n                      AVG(duration_sec) AS avg_duration_sec                  \n                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n                  WHERE start_station_name != end_station_name\n                  GROUP BY start_station_name, end_station_name\n                  LIMIT 10\n                  \"\"\"\nshow_amount_of_data_scanned(less_data_query)","ae689d9b":"big_join_query = \"\"\"\n                 SELECT repo,\n                     COUNT(DISTINCT c.committer.name) as num_committers,\n                     COUNT(DISTINCT f.id) AS num_files\n                 FROM `bigquery-public-data.github_repos.commits` AS c,\n                     UNNEST(c.repo_name) AS repo\n                 INNER JOIN `bigquery-public-data.github_repos.files` AS f\n                     ON f.repo_name = repo\n                 WHERE f.repo_name IN ( 'tensorflow\/tensorflow', 'facebook\/react', 'twbs\/bootstrap', 'apple\/swift', 'Microsoft\/vscode', 'torvalds\/linux')\n                 GROUP BY repo\n                 ORDER BY repo\n                 \"\"\"\nshow_time_to_run(big_join_query)\n\nsmall_join_query = \"\"\"\n                   WITH commits AS\n                   (\n                   SELECT COUNT(DISTINCT committer.name) AS num_committers, repo\n                   FROM `bigquery-public-data.github_repos.commits`,\n                       UNNEST(repo_name) as repo\n                   WHERE repo IN ( 'tensorflow\/tensorflow', 'facebook\/react', 'twbs\/bootstrap', 'apple\/swift', 'Microsoft\/vscode', 'torvalds\/linux')\n                   GROUP BY repo\n                   ),\n                   files AS \n                   (\n                   SELECT COUNT(DISTINCT id) AS num_files, repo_name as repo\n                   FROM `bigquery-public-data.github_repos.files`\n                   WHERE repo_name IN ( 'tensorflow\/tensorflow', 'facebook\/react', 'twbs\/bootstrap', 'apple\/swift', 'Microsoft\/vscode', 'torvalds\/linux')\n                   GROUP BY repo\n                   )\n                   SELECT commits.repo, commits.num_committers, files.num_files\n                   FROM commits \n                   INNER JOIN files\n                       ON commits.repo = files.repo\n                   ORDER BY repo\n                   \"\"\"\n\nshow_time_to_run(small_join_query)","6e2752b2":"In this case, we used `start_station_name` and `end_station_name` instead of `id` since there is a 1:1 relationship between the two and including `start_station_id` or `end_station_id` ends up becoming redundant.","1716685c":"### SELECT statements\n\nIt can be tempting to select all the data from a given table to showcase everything that is available, however this can lead to high inefficiency as displayed in the code snippet below:","968f5710":"## Advanced SQL Practice 4\n\nJust some code to learn using SQL integrated within the Kaggle environment.\n\nThis notebook will be focues on writing efficient queries, although this might not matter when working with smaller datasets, what if we are trying to query a database or feeding information into a website. We are going to learn to use two important functions that will help us optimize query efficiency (one of them coming from the **time** package).\n\n* `show_amount_of_data_scanned()` will show the amount of data that the query is using.\n* `show_time_to_run()` will print how long in a time measurement it takes to run the query.\n\nAs always, we will need to set up our client and connection to *bigquery* as well as all necessary APIs to fetch the data.","d4615963":"### N:N joins\n\nIn all the practice exercises that we have completed so far all *joins* have had a corresponding value in another table. However, it is a possbility that joins such as **N:1** joins as well as **N:N** joins will have multiple values from a single table correspoding to multiple rows in another or have multiple rows in one table correspond to multiple rows in another table respectively. \n\nBy rewriting our queries keeping this in mind, we can decrease our run time as well as data that we are working on to better streamline our ouputs as rep-roducable code.","77b334f5":"Looking at the results, you can see that we were able to cut down on the time that it took to run the query by creating a more efficient join!","065b58bd":"### Read less data\n\nSometimes we can tell the query to process less data by taking a closer look at table structures before running the queries."}}