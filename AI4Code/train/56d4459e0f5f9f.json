{"cell_type":{"066882a1":"code","7cf0a532":"code","e875acad":"code","a82957f4":"code","b752f7cd":"code","50daad74":"code","b32c3cfd":"code","af89408e":"code","f318ee99":"code","0896683f":"code","c4745abd":"code","b9222959":"code","0c7f684e":"code","0883e14c":"code","5a2febca":"code","dc6d5cba":"code","792c9480":"code","78de9d3a":"code","d836df00":"code","efb48b51":"code","47a1ec97":"code","6577ea7f":"code","7c5acf47":"code","09d80df2":"code","a74a79f1":"code","3fbbe987":"code","0240295d":"code","29389981":"code","a7ff5277":"code","b1c948a1":"code","2aad742c":"markdown","8ae8a2e8":"markdown","8726f942":"markdown","368bdd62":"markdown","92a146fb":"markdown","63eccf43":"markdown"},"source":{"066882a1":"import os\nos.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/')","7cf0a532":"os.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/')","e875acad":"os.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/test\/')","a82957f4":"train_dir='..\/input\/chest-xray-pneumonia\/chest_xray\/train\/'\ntest_dir='..\/input\/chest-xray-pneumonia\/chest_xray\/test\/'\nval_dir='..\/input\/chest-xray-pneumonia\/chest_xray\/val\/'","b752f7cd":"# train \nos.listdir(train_dir)\ntrain_n = train_dir+'NORMAL\/'\ntrain_p = train_dir+'PNEUMONIA\/'","50daad74":"#Normal pic \nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint(len(os.listdir(train_n)))\nrand_norm= np.random.randint(0,len(os.listdir(train_n)))\nnorm_pic = os.listdir(train_n)[rand_norm]\nprint('normal picture title: ',norm_pic)\n\nnorm_pic_address = train_n+norm_pic\n\n#Pneumonia\nrand_p = np.random.randint(0,len(os.listdir(train_p)))\n\nsic_pic =  os.listdir(train_p)[rand_norm]\nsic_address = train_p+sic_pic\nprint('pneumonia picture title:', sic_pic)\n\n# Load the images\nnorm_load = Image.open(norm_pic_address)\nsic_load = Image.open(sic_address)\n\n#Let's plt these images\nf = plt.figure(figsize= (10,6))\na1 = f.add_subplot(1,2,1)\nimg_plot = plt.imshow(norm_load)\na1.set_title('Normal')\n\na2 = f.add_subplot(1, 2, 2)\nimg_plot = plt.imshow(sic_load)\na2.set_title('Pneumonia')","b32c3cfd":"import cv2\nimport matplotlib.pyplot as plt\nfor dire in os.listdir(train_dir):\n    path = os.path.join(train_dir,dire)\n    for img in os.listdir(path):\n        paths=os.path.join(path,img)\n        \n        img_array = cv2.imread(paths,cv2.IMREAD_COLOR)\n        plt.imshow(img_array)\n        break","af89408e":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.layers import Input, Lambda, Dense, Flatten\nfrom tensorflow.python.keras.layers import   Dropout, MaxPooling2D, ZeroPadding2D,BatchNormalization","f318ee99":"train_generator_2=ImageDataGenerator(preprocessing_function=preprocess_input,featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip = False,  # randomly flip images\n        vertical_flip=False)\ntraining_generator_2=train_generator_2.flow_from_directory(train_dir,target_size=(224,224),batch_size=4,class_mode='binary')","0896683f":"val_generator_2=ImageDataGenerator(preprocessing_function=preprocess_input)\nvalidation_generator_2=val_generator_2.flow_from_directory(val_dir,target_size=(224,224),batch_size=12,class_mode='binary')","c4745abd":"from tensorflow.keras.applications import InceptionV3","b9222959":"inception=InceptionV3(input_shape=[224,224,3],weights='imagenet',include_top=False)","0c7f684e":"inception.summary()","0883e14c":"#train the layers of the model\nfor layers in inception.layers[:50]:\n    layers.trainable=False","5a2febca":"x = Flatten()(inception.output)","dc6d5cba":"prediction = Dense(1, activation='sigmoid')(x)\n# create a model object\nmodel = Model(inputs=inception.input, outputs=prediction)","792c9480":"model.summary()","78de9d3a":"pip install visualkeras","d836df00":"import visualkeras\nvisualkeras.layered_view(model, type_ignore=[ ZeroPadding2D,BatchNormalization,Flatten,Dropout])","efb48b51":"# let's visualize layer names and layer indices to see how many layers\nfrom keras import layers\nfor i, layer in enumerate(model.layers):\n   print(i, layer.name)","47a1ec97":"from keras import optimizers\nmodel.compile(optimizer=optimizers.Adam(lr=0.001, decay=0.005),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","6577ea7f":"#CallBacks Function\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ReduceLROnPlateau\nearly_stop=EarlyStopping(monitor=\"val_acc\",\n                         patience=10,\n                         mode=\"auto\",)\nLearning_rate_reduction=ReduceLROnPlateau(monitor='val_acc',patience=2,verbose=1,factor=0.5,min_lr=0.001)\n\ncallbacks=[early_stop,Learning_rate_reduction]","7c5acf47":"history = model.fit_generator(training_generator_2,validation_data = validation_generator_2,epochs = 20, verbose = 1,callbacks=callbacks)","09d80df2":"accuracy_3=history.history['accuracy']\nloss_3=history.history['loss']\nval_accuracy_3=history.history['val_accuracy']\nval_loss_3=history.history['val_loss']","a74a79f1":"epochs = range(len(accuracy_3))\nepochs","3fbbe987":"import matplotlib.pyplot as plt\nplt.plot(epochs,accuracy_3,'r',label='training_accuracy')\nplt.plot(epochs,val_accuracy_3,'g',label='val_accuracy')\nplt.legend()\nplt.show()","0240295d":"#plotting training values\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='green', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='pink', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","29389981":"class_dict = training_generator_2.class_indices\nprint(class_dict)\n","a7ff5277":"li = list(class_dict.keys())\nprint(li)","b1c948a1":"# predicting an image\n\nfrom keras.preprocessing import image\nimport numpy as np\nimage_path = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test\/PNEUMONIA\/person96_bacteria_464.jpeg\"\nnew_img = image.load_img(image_path, target_size=(224, 224))\nimg = image.img_to_array(new_img)\nimg = np.expand_dims(img, axis=0)\nimg = img\/255\n\nprint(\"Following is our prediction:\")\nprediction = model.predict(img)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nd = prediction.flatten()\nj = d.max()\nfor index,item in enumerate(d):\n    if item == j:\n        class_name = li[index]\nplt.figure(figsize = (4,4))\nplt.imshow(new_img)\nplt.axis('off')\nplt.title(class_name)\nplt.show()","2aad742c":"# Image Preprocessing","8ae8a2e8":"**After Adding the dense and output layer, **","8726f942":"# InceptionV3 Architecture\n**Inception V3 by Google is the 3rd version in a series of Deep Learning Convolutional Architectures. Inception V3 was trained using a dataset of 1,000 classes (See the \nlist of classes here) from the original ImageNet dataset which was trained with over 1 million training images, the Tensorflow version has 1,001 classes which is due to \nan additional \"background' class not used in the original ImageNet. Inception V3 was trained for the ImageNet Large Visual Recognition Challenge where it was a first \nrunner up.**","368bdd62":"# Importing Pretrained InceptionV3 model","92a146fb":"**After Removing the hidden layers, inception model looks like this**","63eccf43":"**Now we need to add the dense layers and the output layers to our model**"}}