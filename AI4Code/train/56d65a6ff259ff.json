{"cell_type":{"72cf0b3c":"code","d3f48867":"code","d46a159a":"code","ab904a96":"code","f66c5bd2":"code","4d4c8a77":"code","348032b3":"code","c9172c21":"code","64af9512":"code","2d634c70":"code","bedbf71c":"code","736a4738":"code","d6a37c1e":"code","8ca91be0":"code","7be51085":"code","ec10ac8e":"code","f647f341":"code","b498bb4e":"code","20900a71":"code","aaaa3208":"code","3337013d":"code","e7d9116a":"code","96b2d5f8":"code","9cb49f4c":"code","64d816b0":"code","1074abae":"code","0f14bcfa":"code","96d47fe6":"code","2d9f687f":"code","139eefae":"code","553d1f5c":"code","a8460c59":"code","2b868a7a":"markdown","09f45189":"markdown","507300af":"markdown","665217b3":"markdown"},"source":{"72cf0b3c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = [16, 8]\n\nprint('Using Tensorflow version:', tf.__version__)","d3f48867":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","d46a159a":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('shopee-product-detection-open')\n\n# Configuration\nEPOCHS = 110\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\nimage_size=(299, 299)","ab904a96":"train_df = pd.read_csv('\/kaggle\/input\/shopee-product-detection-open\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/shopee-product-detection-open\/test.csv')\n\ntrain_df.shape, test_df.shape","f66c5bd2":"train_df.head()","4d4c8a77":"def show_train_img(category):\n    \n    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(24, 10))\n    \n    train_path = '\/kaggle\/input\/shopee-product-detection-open\/train\/train\/train\/'\n    ten_random_samples = pd.Series(os.listdir(os.path.join(train_path, category))).sample(10).values\n    \n    for idx, image in enumerate(ten_random_samples):\n        final_path = os.path.join(train_path, category, image)\n        img = cv2.imread(final_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        axes.ravel()[idx].imshow(img)\n        axes.ravel()[idx].axis('off')\n    plt.tight_layout()","348032b3":"show_train_img('38')","c9172c21":"show_train_img('12')","64af9512":"show_train_img('32')","2d634c70":"def show_test_img():\n    \n    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(24, 10))\n    \n    test_path = '\/kaggle\/input\/shopee-product-detection-open\/test\/test\/test\/'\n    ten_random_samples = pd.Series(os.listdir(test_path)).sample(10).values\n    \n    for idx, image in enumerate(ten_random_samples):\n        final_path = os.path.join(test_path, image)\n        img = cv2.imread(final_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        axes.ravel()[idx].imshow(img)\n        axes.ravel()[idx].axis('off')\n    plt.tight_layout()","bedbf71c":"show_test_img()","736a4738":"# pick random samples\n\ndataset_path = {}\n\ncategories = np.sort(train_df['category'].unique())\n\nfor cat in categories:\n#     try:\n#         dataset_path[cat] = train_df[train_df['category'] == cat]['filename'].sample(2100)\n#     except:\n    dataset_path[cat] = train_df[train_df['category'] == cat]['filename'].sample(frac=1.)","d6a37c1e":"category_list = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09',\n                 '10', '11', '12', '13', '14', '15', '16', '17', '18', '19',\n                 '20', '21', '22', '23', '24', '25', '26', '27', '28', '29',\n                 '30', '31', '32', '33', '34', '35', '36', '37', '38', '39',\n                 '40', '41']\n\nclass_weight = {v:1.0 for v in range(42)}\nclass_weight[0]=2.0\nclass_weight[1]=2.0\nclass_weight[2]=2.0\nclass_weight[3]=2.0\nclass_weight[20]=2.0\nclass_weight[36]=2.0\nclass_weight[41]=2.0","8ca91be0":"train_paths = []\n\nfor idx, key in enumerate(dataset_path.keys()):\n    if key == idx:\n        for path in dataset_path[idx]:\n            train_paths.append(os.path.join(GCS_DS_PATH, 'train', 'train', 'train', category_list[idx], path))","7be51085":"labels = []\n\nfor label in dataset_path.keys():\n    labels.extend([label] * len(dataset_path[label]))","ec10ac8e":"from tensorflow.keras.utils import to_categorical\n\n# convert to numpy array\ntrain_paths = np.array(train_paths)\n\n# convert to one-hot-encoding-labels\ntrain_labels = to_categorical(labels)","f647f341":"from sklearn.model_selection import train_test_split\n\ntrain_paths, valid_paths, train_labels, valid_labels = train_test_split(train_paths, \n                                                                        train_labels, \n                                                                        stratify=train_labels,\n                                                                        test_size=0.1, \n                                                                        random_state=2020)\n\ntrain_paths.shape, valid_paths.shape, train_labels.shape, valid_labels.shape","b498bb4e":"test_paths = []\n\nfor path in test_df['filename']:\n    test_paths.append(os.path.join(GCS_DS_PATH,  'test', 'test', 'test', path))\n    \ntest_paths = np.array(test_paths)","20900a71":"def decode_image(filename, label=None, image_size=image_size):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n    \ndef decode_aug_image(filename, label=None, image_size=image_size):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, image_size)\n    #augment\n    image = tf.image.random_brightness(image, 0.4)\n    image = tf.image.random_contrast(image, 0.2, 0.5)\n    image = tf.image.random_crop(image, [200, 200, 3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n    \n","aaaa3208":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_aug_image, num_parallel_calls=AUTO)\n    .cache()\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","3337013d":"!pip install -q efficientnet","e7d9116a":"from tensorflow.keras.layers import Dense, Dropout\nimport efficientnet.tfkeras as efn \nfrom functools import partialmethod\nimport transformers\nfrom transformers.optimization_tf import AdamWeightDecay, create_optimizer","96b2d5f8":"initial_lr = 1e-4\nLABEL_SMOOTHING = 0.10\ndef loss_with_ls(y_true, y_pred):\n    return tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, \\\n                                                    label_smoothing=LABEL_SMOOTHING)\ndef build_model(update_model=None):\n    with strategy.scope():\n        lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=initial_lr, \n                        decay_steps = 320, end_learning_rate=3e-6, power=1.0,cycle=True)\n    \n        lr_schedule = transformers.optimization_tf.WarmUp(initial_learning_rate=initial_lr, \n                                                          decay_schedule_fn=lr_schedule, warmup_steps=50)\n        \n        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n\n        if update_model != None:\n            update_model.trainable = True\n            for layer in update_model.layers:\n                layer.trainable = False\n                if (layer.name=='stem_conv'):\n                    layer.trainable = True\n                    break\n            update_model.compile(optimizer=optimizer, loss=loss_with_ls, metrics=['accuracy'])\n            print(\"trainable layers\",len(update_model.trainable_variables))\n            return update_model\n\n        base_model = efn.EfficientNetB5(weights='noisy-student',\n                                        include_top=False,\n                                        input_shape=image_size+(3,),\n                                        pooling='avg')   \n        base_model.trainable = False\n        x = tf.keras.layers.Dense(42,activation=\"softmax\")(base_model.output)\n\n        model = tf.keras.Model(base_model.input, x)  \n        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n        print(\"trainable layers\",len(model.trainable_variables))\n        return model","9cb49f4c":"model_load = \"..\/input\/efficient-netb5\/eff_b5.h5\"\nmodel_save= \".\/eff_b5.h5\"\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001,\n                                   patience=3, verbose=1, mode='auto')    \nsv = tf.keras.callbacks.ModelCheckpoint(model_save, \\\n                                        monitor='val_loss', verbose=1, save_best_only=True,\\\n                                        save_weights_only=True, mode='auto', save_freq='epoch')\n\n\nmodel = build_model()\nn_steps = train_labels.shape[0] \/\/ BATCH_SIZE\n\nmodel.fit(\n    train_dataset, \n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    class_weight=class_weight,\n    epochs=10,\n)\n# model = build_model(model)\n# model.load_weights(model_load)\n\nprint(\"unfreezing few more layers and finetuning\")\n# unfreeze a few more layers and train till convergence\nmodel = build_model(model)\nhistory = model.fit(\n    train_dataset, \n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    class_weight=class_weight,\n    initial_epoch=10,callbacks=[early_stopping,sv],\n    epochs=EPOCHS,\n)\n\nprint('Loading best model...')\nmodel.load_weights(model_save)\n","64d816b0":"# Get training and test loss histories\ntraining_loss = history.history['loss']\ntest_loss = history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","1074abae":"training_loss","0f14bcfa":"test_loss","96d47fe6":"history.history['val_accuracy']","2d9f687f":"pred = model.predict(test_dataset, verbose=1)\nnp.save(\"pred_prob.npy\",pred)","139eefae":"# drop existing feature\ntest_df = test_df.drop('category', axis=1)\n\n# change with prediction\ntest_df['category'] = pred.argmax(axis=1)\n\n# then add zero-padding\ntest_df['category'] = test_df['category'].apply(lambda x: str(x).zfill(2))","553d1f5c":"test_df.to_csv('submission.csv', index=False)","a8460c59":"pred_train = model.predict(test_dataset, verbose=1, )\npred_val = model.predict(test_dataset, verbose=1)\nnp.save(\"pred_train_prob.npy\",pred_train)\nnp.save(\"pred_val_prob.npy\",pred_val)\ntest_df.head()","2b868a7a":"## Efficient-Net\n\nIn May 2019, Google published both a very exciting paper and source code for a newly designed CNN called EfficientNet, that set new records for both accuracy and computational efficiency. Here\u2019s the results of EfficientNet, scaled to different block layers (B1, B2, etc) vs. most other popular CNN\u2019s.\n\n![Architecture](https:\/\/miro.medium.com\/max\/985\/1*nQ5HYZ1xiIGn092Y5H5SIQ.jpeg)\n\nAs the image shows, EfficientNet tops the current state of the art both in accuracy and in computational efficiency. How did they do this?\n\n### Model scaling\n\nThey learned that CNN\u2019s must be scaled up in **depth, width, and input image resolution together** to improve the performance of the model. The scaling method is named **compound scaling** and suggests that instead of scaling only one model attribute out of depth, width, and resolution; strategically scaling all three of them together delivers better results.\n\nThere is a synergy in scaling depth, width and image-resolution together, and after an extensive grid search derived the theoretically optimal formula of \u201ccompound scaling\u201d using the following co-efficients:\n\n* Depth = 1.20\n* Width = 1.10\n* Resolution = 1.15\n\nDepth simply means how deep the networks is which is equivalent to the number of layers in it. Width simply means how wide the network is. One measure of width, for example, is the number of channels in a Conv layer whereas Resolution is simply the image resolution that is being passed to a CNN.\n\nIn other words, to scale up the CNN, the depth of layers should increase 20%, the width 10% and the image resolution 15% to keep things as efficient as possible while expanding the implementation and improving the CNN accuracy. This compound scaling formula is used to scale up the EfficientNet from B0-B7\n\n### Swish Activation\n\n![swish-activation](https:\/\/miro.medium.com\/max\/1400\/0*EhAHcCmGOzQUgQ0k)\n\nReLu works pretty well but it got a problem, it nullifies negative values and thus derivatives are zero for all negative values. There are many known alternatives to tackle this problem like leaky ReLu, Elu, Selu etc., but none of them has proven consistent.\n\nGoogle Brain team suggested a newer activation that tends to work better for deeper networks than ReLU which is a Swish activation. They proved that if we replace Swish with ReLu on InceptionResNetV2, we can achieve 0.6% more accuracy on ImageNet dataset.\n\n> Swish(x) = x * sigmoid(x)\n\nThere are other things like MBConv Block etc. If you want to know more details, you can read the articles in reference below","09f45189":"## Make a submission","507300af":"## Show images in test dataset","665217b3":"# Reference:\n\n[EfficientNet from Google \u2014 Optimally Scaling CNN model architectures with \u201ccompound scaling\u201d](https:\/\/medium.com\/@lessw\/efficientnet-from-google-optimally-scaling-cnn-model-architectures-with-compound-scaling-e094d84d19d4)\n\n[Image Classification with EfficientNet: Better performance with computational efficiency](https:\/\/medium.com\/analytics-vidhya\/image-classification-with-efficientnet-better-performance-with-computational-efficiency-f480fdb00ac6)\n\n[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https:\/\/medium.com\/@nainaakash012\/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-92941c5bfb95)"}}