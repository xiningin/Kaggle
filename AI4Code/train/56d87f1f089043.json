{"cell_type":{"208bd196":"code","1ce66ab2":"code","94f0e070":"code","d0ab6216":"code","f1d34896":"code","823b23d4":"code","fd393258":"code","7acf3aa3":"code","c7c845ae":"code","ae1bbeb2":"code","5fb7bf7c":"code","59ba11f2":"code","8d1f5e87":"code","f3b1673c":"code","53f82365":"code","02f30c05":"code","6fd7e2c0":"code","b8b59b04":"code","45202465":"code","3c1c2bb7":"code","f4d79665":"code","b595f371":"code","7f3dd8a6":"code","cf363618":"code","7afcf38b":"code","779f69f3":"code","8456fb46":"code","0f80cefe":"code","5829d7cd":"markdown","3dd6381d":"markdown","b738fee2":"markdown","674bbc31":"markdown"},"source":{"208bd196":"!pip install pillow","1ce66ab2":"import os\nimport torch\nimport torchvision\nimport tarfile\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torchvision.datasets.utils import download_url\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as tt\nfrom torch.utils.data import random_split\nfrom PIL import Image\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nimport torchvision.models as models\n\n%matplotlib inline","94f0e070":"DATA_DIR = '..\/input\/cat-and-dog'\nprint(os.listdir(DATA_DIR))","d0ab6216":"classes = os.listdir(DATA_DIR + '\/test_set\/test_set')\nclasses","f1d34896":"len(os.listdir(DATA_DIR + '\/test_set\/test_set\/cats'))","823b23d4":"# transformation\nstats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\ntrain_transform = tt.Compose([tt.Resize(360),\n                              tt.RandomCrop(360, padding=4, padding_mode='reflect'),\n                              tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                              tt.RandomHorizontalFlip(),\n                              tt.RandomRotation(10),\n                              tt.ToTensor(),\n                              tt.Normalize(*stats, inplace=True),\n                              tt.RandomErasing(inplace=True)])\n\n#normalizing the validation image size before transforming to tensor\nvalid_transform = tt.Compose([tt.Resize(360),\n                              tt.CenterCrop(360),\n                              tt.ToTensor(),\n                              tt.Normalize(*stats)])","fd393258":"# Pytorch datasets\n'''\nImageFolder is one of best the function for data that is separated by classes. Along with transformations, it also appends label by itself. \nThe label is index of class in the working directory. (It follows Zero-Indexing Scheme)\n'''\ntrain_ds = ImageFolder(DATA_DIR + '\/training_set\/training_set', valid_transform)\ntest_ds = ImageFolder(DATA_DIR+'\/test_set\/test_set' ,train_transform)\n\n    ","7acf3aa3":"# Checking some labels of the datasets\n\n#For Dogs\nc = 0\nfor i in reversed(train_ds):\n    print(i[1])\n    c += 1\n    if c > 10:\n        break\n        \n# For Cats\nc = 0\nfor i in (test_ds):\n    print(i[1])\n    c += 1\n    if c > 10:\n        break\n        \n# Since, they're ordered in folders(Classes).The data is contiguous for respective classes.\n# We'll shuffle them by using DataLoaders very soon.","c7c845ae":"torch.manual_seed(664)\ntest_size = int(len(test_ds)\/2)\nval_size = len(test_ds) - test_size\ntest_ds, val_ds = random_split(test_ds, [test_size, val_size])","ae1bbeb2":"print(len(train_ds))\nprint(len(test_ds))\nprint(len(val_ds))","5fb7bf7c":"# DataLoaders will help the model train efficiently by loading by groups.\n# shuffle = True is very important for our ordered data\n\nbatch_size = 18\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\nvalid_dl = DataLoader(val_ds, batch_size*2, num_workers=3, pin_memory=True)\ntest_dl = DataLoader(test_ds, batch_size*2, num_workers=3, pin_memory=True)\n","59ba11f2":"for img, _ in train_dl:\n    print(img.size())\n    break","8d1f5e87":"def show_sample(dl):\n    for img, lbl in dl:\n        print('images.shape:', img.shape)\n        print('example label:', lbl)\n        fig, ax = plt.subplots(figsize=(12, 12))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(img[:batch_size], nrow=8).permute(1, 2, 0))\n        break","f3b1673c":"show_sample(train_dl)","53f82365":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","02f30c05":"device = get_default_device()\ntorch.cuda.empty_cache()","6fd7e2c0":"if device == torch.device('cuda'):\n    train_dl = DeviceDataLoader(train_dl, device)\n    valid_dl = DeviceDataLoader(valid_dl, device)\n#   test_dl = DeviceDataLoader(test_dl, device)","b8b59b04":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\ndef evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, model, train_loader, val_loader, max_lr, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), max_lr)\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    for epoch in range(epochs):\n        lrs = []\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            sched.step()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))","45202465":"class CatsNDogs(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 360, kernel_size=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(360, 720, kernel_size=1, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(720, 1024, kernel_size=1, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(1024, 720, kernel_size=1, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(720, 360, kernel_size=1, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(360, 256, kernel_size=1, stride=1, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1),\n\n            nn.Flatten(), \n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 2),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, xb):\n        return self.network(xb)\n    \nmodel = CatsNDogs()\n","3c1c2bb7":"class CatDogRes(ImageClassificationBase):\n    \n    def __init__(self):\n        super().__init__()\n        \n        self.network = models.resnet18(pretrained = True)\n        num_ftrs = self.network.fc.in_features\n        self.network.fc = nn.Linear(num_ftrs, 2)\n        \n    def forward(self, batch):\n        return torch.sigmoid(self.network(batch))\n        \n        \n        \nmodel = CatDogRes()       \n    ","f4d79665":"model","b595f371":"torch.cuda.empty_cache()\nto_device(model, device)","7f3dd8a6":"max_lr = 0.01","cf363618":"history = fit(4, model, train_dl, valid_dl, max_lr)","7afcf38b":"def show_sample(img, target, invert=True):\n    if invert:\n        plt.imshow(1 - img.permute((1, 2, 0)))\n    else:\n        plt.imshow(img.permute(1, 2, 0))\n    ","779f69f3":"def predict_single(image, lbl):\n    xb = image.unsqueeze(0)\n    xb = to_device(xb, device)\n    preds = model(xb)\n    prediction = preds\n    print(prediction)\n    show_sample(image, prediction)\n    if prediction[0][0] > prediction[0][1]:\n        print(prediction[0][0], lbl, 'Cat') \n    else:\n        print(prediction[0][1], lbl, 'Dog')\n    \n#     print(\"Prediction: \", prediction)\n#     show_sample(image, prediction)","8456fb46":"img, lbl = test_ds[11]\n\nimg.size()","0f80cefe":"predict_single(img, lbl)","5829d7cd":"## Let's push the tensors on CUDA...\ud83d\ude4c","3dd6381d":"## **Setting Up GPU Suport**\n\n","b738fee2":"## Following pipeline is same for most of the basic models.","674bbc31":"## Implementing Conv2D"}}