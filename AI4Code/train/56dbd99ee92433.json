{"cell_type":{"d315c738":"code","81dddf6e":"code","0563112c":"code","6e5334bb":"code","72e4eec8":"code","1f57f9df":"code","9cb14cc6":"code","d5feb192":"code","af53699b":"code","5f792135":"code","70f8421b":"code","888a83aa":"code","59da53ca":"code","9e31294f":"code","66eec41b":"code","701d7c5b":"code","ee8b1ecc":"code","08b990f0":"code","f1f9d920":"code","a6c6d76b":"code","769e1aed":"code","e01aa032":"code","52988506":"code","b3e1b439":"code","d8885df4":"code","7bf99be0":"code","f1b0de01":"code","d0200917":"code","2a6a8b4f":"code","2d4ea93d":"code","cb2692c0":"code","610c65f6":"markdown"},"source":{"d315c738":"'''\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n'''","81dddf6e":"import math\nimport os\nimport shutil\nimport sys\nfrom keras.preprocessing.image import array_to_img\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.patches as ptc\nimport matplotlib\nimport seaborn as sns\nimport glob\nimport pydicom\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","0563112c":"'''\nlistfile=[]\nfiles=os.listdir(\"..\/input\/dicom-to-512jpg-train\") \nfor file in files: \n    if os.path.isfile(\"..\/input\/dicom-to-512jpg-train\/\" + file):\n        listfile.append(file)\nlen(listfile)\n'''","6e5334bb":"'''\n#load data from dicom\nmy_annots=pd.read_csv(\"..\/input\/dicom-to-jpg\/2000_data_dim_convert.csv\")\nmy_annots=my_annots.iloc[:,1:]\nmy_annots.image_id= my_annots.image_id.apply(lambda x: '..\/..\/input\/dicom-to-jpg\/'+  x + '.jpg')\nannotations=my_annots[['image_id','x_min', 'y_min', 'x_max', 'y_max', 'class_name']]\nannotations.to_csv('annotations.csv')\nclasses=my_annots.class_name.unique()\nclass_id=my_annots.class_id.unique()\n\n\nwith open('classes.csv', mode='w') as file:\n    for  clss ,i in zip(classes,class_id ):\n        file.write('{},{}\\n'.format(clss, i))\namount_80 = int(0.8*len(annotations))\ntrain_data = annotations[:amount_80]\ntest_data = annotations[amount_80:]\nprint(len(train_data))\nprint(len(test_data))\n\ntrain_data.to_csv(path_or_buf='train_annotations.csv', index=False, header=None)\ntest_data.to_csv(path_or_buf='val_annotations.csv', index=False, header=None)\n\n'''\n\n","72e4eec8":"%ls","1f57f9df":"\ntrain_data=pd.read_csv(\"\/kaggle\/input\/img-dim-train\/train_img_dim.csv\")\nannots=train_data.copy()\nannots= annots[annots.class_id != 14] #drop NO Finding class\nIMG_SIZE=512\ndef norm_df(df):\n    df.x_min= ((df.x_min\/ df.width)* IMG_SIZE).astype('int32')\n    df.x_max=( (df.x_max\/ df.width)* IMG_SIZE).astype('int32')\n    \n    df.y_min= ((df.y_min\/ df.height)* IMG_SIZE).astype('int32')\n    df.y_max= ((df.y_max\/ df.height)* IMG_SIZE).astype('int32')\n    return df\nalldata=norm_df(annots.copy()).reset_index(drop = True)\n#smalldata = df[0:20000] # first 20000 \n#smalldata.to_csv('20000_data_dim_convert.csv')","9cb14cc6":"from sklearn.model_selection import StratifiedKFold\nfrom matplotlib.pyplot import figure\nimport warnings\n\n#load data from dicom\n#my_annots=pd.read_csv(\"..\/..\/input\/dicom-to-512jpg-train\/20000_data_dim_convert.csv\")\n#my_annots=pd.read_csv(\"\/kaggle\/input\/dicom-to-512jpg-train\/20000_data_dim_convert.csv\")\nmy_annots=alldata\n#my_annots=my_annots.iloc[:,1:]\nmy_annots.image_id= my_annots.image_id.apply(lambda x: '..\/input\/dicom-to-512jpg-train\/'+  x + '.jpg')\nannotations=my_annots[['image_id','x_min', 'y_min', 'x_max', 'y_max', 'class_name']]\nannotations.to_csv('annotations.csv')\nclasses=my_annots.class_name.unique()\nclass_id=my_annots.class_id.unique()\n\nwith open('classes.csv', mode='w') as file:\n    for  clss ,i in zip(classes,class_id ):\n        file.write('{},{}\\n'.format(clss, i))\n#amount_80 = int(0.85*len(annotations))\n#train_data = annotations[:amount_80]\n#test_data = annotations[amount_80:]\n\n\nwarnings.filterwarnings('ignore')\n\n#df = pd.read_csv(r'..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train.csv')\ndf = pd.DataFrame(my_annots)\ndf = df[df['class_name'] != 'No finding']\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=101)\ndf_folds = df[['image_id']].copy()\n\ndf_folds.loc[:, 'bbox_count'] = 1\ndf_folds = df_folds.groupby('image_id').count()\ndf_folds.loc[:, 'object_count'] = df.groupby('image_id')['class_id'].nunique()\n\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['object_count'].values.astype(str),\n    df_folds['bbox_count'].apply(lambda x: f'_{x \/\/ 15}').values.astype(str)\n)\n\ndf_folds.loc[:, 'fold'] = 0\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number\n\n# example with fold 0\ndf_folds.reset_index(inplace=True)\n\ndf_valid = pd.merge(df, df_folds[df_folds['fold'] == 0], on='image_id')\n\ndf_train = pd.merge(df, df_folds[df_folds['fold'] != 0], on='image_id')\n\nfigure(num=None, figsize=(30, 8))\ndf_train['class_name'].hist()\ndf_valid['class_name'].hist()\nplt.show()\n\n'''\ntrain_data,test_data =train_test_split (annotations,test_size=0.15, random_state=42)\nprint(len(train_data))\nprint(len(test_data))\n\n'''\n","d5feb192":"\ntrain_data=df_train[['image_id','x_min', 'y_min', 'x_max', 'y_max', 'class_name']]\ntest_data=df_valid[['image_id','x_min', 'y_min', 'x_max', 'y_max', 'class_name']]","af53699b":"#train_data= train_data.drop(train_data[train_data.y_max== train_data.y_min].index[0])\ntest_data= test_data.drop(test_data[test_data.y_max== test_data.y_min].index[0])\ntrain_data.to_csv(path_or_buf='train_annotations.csv', index=False, header=None)\ntest_data.to_csv(path_or_buf='val_annotations.csv', index=False, header=None)","5f792135":"'''\n'https:\/\/github.com\/fizyr\/keras-retinanet\/releases\/download\/0.5.1\/resnet50_coco_best_v2.1.0.h5',\n'https:\/\/github.com\/fizyr\/keras-retinanet\/releases\/download\/0.5.1\/resnet101_oid_v1.0.0.h5',\n'https:\/\/github.com\/fizyr\/keras-retinanet\/releases\/download\/0.5.1\/resnet152_oid_v1.0.0.h5'\n'''","70f8421b":"\n!mkdir weights\n!wget -O \/kaggle\/working\/weights\/resnet50_coco_best_v2.h5 https:\/\/github.com\/fizyr\/keras-retinanet\/releases\/download\/0.5.1\/resnet50_coco_best_v2.1.0.h5","888a83aa":"!mkdir \/kaggle\/working\/snapshots\n!mkdir \/kaggle\/working\/tensorboard","59da53ca":"!cd ~\n!git clone https:\/\/github.com\/fizyr\/keras-retinanet","9e31294f":"#!git clone 'https:\/\/github.com\/fizyr\/keras-retinanet.git'\n%cd keras-retinanet\/\n!pip install .\n!python setup.py build_ext --inplace\n\n\n","66eec41b":"train_data.shape","701d7c5b":"!cd ~\n%ls","ee8b1ecc":"cd \/kaggle\/working\/keras-retinanet","08b990f0":"!keras_retinanet\/bin\/train.py --freeze-backbone \\\n  --random-transform \\\n  --workers 0 \\\n  --weights \/kaggle\/working\/weights\/resnet50_coco_best_v2.h5 \\\n  --batch-size 10 \\\n  --steps 352 \\\n  --image-min-side 512\\\n  --image-max-side 512\\\n  --epochs 50 \\\n  csv \/kaggle\/working\/train_annotations.csv \/kaggle\/working\/classes.csv\n","f1f9d920":"from glob import glob\nmodel_paths = glob('.\/snapshots\/resnet50_csv_*.h5')\nlatest_path = sorted(model_paths)[-1]\nprint(\"path:\", latest_path)\n\n\n\n    \n","a6c6d76b":"from keras_retinanet import models\nfrom keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\nfrom keras_retinanet.utils.visualization import draw_box, draw_caption\nfrom keras_retinanet.utils.colors import label_color\n\n#\n#model = models.load_model(latest_path, backbone_name='resnet50')\n#model = models.convert_model(model)","769e1aed":"!retinanet-convert-model .\/snapshots\/resnet50_csv_50.h5 \\\n\/kaggle\/working\/weights\/resnet50_csv_final.h5","e01aa032":"import keras\n\n# import keras_retinanet\nfrom keras_retinanet import models\nfrom keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\nfrom keras_retinanet.utils.visualization import draw_box, draw_caption\nfrom keras_retinanet.utils.colors import label_color\n#from keras_retinanet.keras_retinanet.utils.gpu import setup_gpu\n\n# import miscellaneous modules\nimport matplotlib.pyplot as plt\nimport cv2\nimport csv\nimport os\nimport numpy as np\nimport time","52988506":"#%cd .\/keras-retinanet\n%ls\n","b3e1b439":"model_path = os.path.join('\/kaggle\/working\/weights', 'resnet50_csv_final.h5')\n\n# load retinanet model\nmodel = models.load_model(model_path, backbone_name='resnet50')\n\n\n##########\n\nlabels_to_names = {\n    0: 'Aortic enlargement', \n    1: 'Atelectasis',\n    2: 'Calcification',\n    3: 'Cardiomegaly',\n    4: 'Consolidation',\n    5: 'ILD',\n    6: 'Infiltration',\n    7: 'Lung Opacity',\n    8: 'Nodule\/Mass',\n    9:'Other lesion',\n    10: 'Pleural effusion',\n    11: 'Pleural thickening',\n    12: 'Pneumothorax',\n    13:'Pulmonary fibrosis'}\n","d8885df4":"\n%cd \/kaggle\/\n","7bf99be0":"sub1=pd.read_csv('.\/input\/sample\/sample_submission .csv')\nsub2=pd.read_csv('.\/input\/test-img-dim\/test.csv')\nsub=pd.DataFrame.merge(sub1,sub2,on='image_id')\nmysub=sub.copy()\nmysub['thispath']='.\/input\/dicom-to-512jpg-test\/'+mysub.image_id+'.jpg'\nmysub['score']=''\nmysub.head()\n#mysub['predictionstring']='14 1 0 0 1 1'\n\n","f1b0de01":"thr=0.5\n#mysub=mysub[:50].copy() \nfields = ['image_id', 'PredictionString','score'] \nwith open('submission.csv', mode='w') as csvfile:\n    csvwriter = csv.writer(csvfile) \n    csvwriter.writerow(fields)\n\n    for row in tqdm(mysub.values) :\n        imgpath=row[4]\n        width=row[2]\n        height=row[3]\n        #predictionstring=row[4]\n        \n        image = read_image_bgr(imgpath)\n        boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n        \n        x= int(width)\/512\n        y= int(height)\/512\n        for box, score, label in zip(boxes[0], scores[0], labels[0]):\n            row[5]=score\n            if score < thr:\n                #row[1]=\"14 1 0 0 1 1\"\n                csvfile.write('{},{},{}\\n'.format(row[0], row[1] ,row[5]))\n                break\n           \n            row[1]= '{} {:.2f} {} {} {} {} '.format(label ,score , int(box[0]*x), int(box[1]*y),int((box[2])*x), int((box[3])*y))\n            \n            csvfile.write('{},{},{}\\n'.format(row[0], row[1],row[5]))\npresubmission=pd.read_csv('submission.csv')\n\n","d0200917":"presubmission.to_csv('\/kaggle\/working\/presubmission.csv',index = False)","2a6a8b4f":"submission=presubmission.groupby('image_id')['PredictionString'].apply(' '.join).reset_index()","2d4ea93d":"submission.to_csv('\/kaggle\/working\/submission.csv',index = False)\n\n","cb2692c0":"!retinanet-evaluate csv \/kaggle\/working\/val_annotations.csv \/kaggle\/working\/classes.csv \/kaggle\/working\/weights\/resnet50_csv_final.h5\n\n","610c65f6":"## predict and submission"}}