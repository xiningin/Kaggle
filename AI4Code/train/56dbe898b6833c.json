{"cell_type":{"8bbfcc20":"code","7fd0a6d0":"code","70085b42":"code","67d7c58b":"code","2326ae67":"code","b3e39a3e":"code","3ca79f4b":"code","5c320946":"code","def6e324":"code","bcc6c2ac":"code","6d3e58b5":"code","923a783f":"code","39ae2d5f":"code","4403ee81":"code","a233fdf5":"code","23a5c811":"code","922dd62f":"code","5e33daec":"code","f5835b53":"code","aa305162":"code","dc1c2c61":"code","3a57a7b6":"code","0eab6245":"code","5b6f46fc":"code","97da0d2e":"code","31066c03":"code","57894604":"code","d88f9c8c":"code","32cb5d21":"code","b90f25ae":"code","160b2bf3":"code","23fd0d77":"code","8767d2dd":"code","0fe7d2f6":"code","ccb4e67d":"code","a954625e":"code","b36271ff":"code","6022829d":"code","5923c785":"code","5f65ef80":"code","a761bc32":"code","06cf9456":"code","1fa1f002":"code","1969ba25":"markdown","d1107e68":"markdown","29ede8e9":"markdown","9ff14093":"markdown","a7b44c68":"markdown","a382d30c":"markdown","8de674d2":"markdown","e494d223":"markdown","e6f30d9e":"markdown","eed9e7ca":"markdown"},"source":{"8bbfcc20":"'''Main'''\nimport numpy as np\nimport pandas as pd\nimport os\n\n'''Data Viz'''\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\n'''Data Prep'''\nfrom sklearn import preprocessing as pp \nfrom scipy.stats import pearsonr \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold \nfrom sklearn.metrics import log_loss \nfrom sklearn.metrics import precision_recall_curve, average_precision_score \nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report \n\n'''Algos'''\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb","7fd0a6d0":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.shape","70085b42":"data.Class.value_counts()","67d7c58b":"2*284315","2326ae67":"data.head()","b3e39a3e":"data.drop(columns = ['Time'], inplace = True)","3ca79f4b":"data.head()","5c320946":"print(\"Number of fraudulent transactions:\", data['Class'].sum())","def6e324":"np.isnan(data).sum()","bcc6c2ac":"# Very huge unbalenced\ndata.Class.value_counts() \/ data.shape[0] * 100","6d3e58b5":"X = data.iloc[:, 0:29].values\ny = data.iloc[:, 29].values","923a783f":"plt.figure(figsize = (12, 6))\nplt.scatter(x = X[y == 0, 6], y = X[y == 0, 3], label = 'Class #0', alpha = .2)\nplt.scatter(x = X[y == 1, 6], y = X[y == 1, 3], label = 'Class #1', alpha = .2)\nplt.legend()","39ae2d5f":"from imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.pipeline import Pipeline ","4403ee81":"method = SMOTE()\nX_SMOTE_Sampling, y_SMOTE_Sampling = method.fit_resample(X, y)","a233fdf5":"X_SMOTE_Sampling.shape","23a5c811":"y_SMOTE_Sampling.shape","922dd62f":"plt.figure(figsize = (12, 6))\nplt.scatter(x = X_SMOTE_Sampling[y_SMOTE_Sampling == 0, 6], y = X_SMOTE_Sampling[y_SMOTE_Sampling == 0, 3], alpha = .5, label = 'Class #0')\nplt.scatter(x = X_SMOTE_Sampling[y_SMOTE_Sampling == 1, 6], y = X_SMOTE_Sampling[y_SMOTE_Sampling == 1, 3], color = 'black', label = 'Class #1', alpha = .3)\nplt.legend()","5e33daec":"X_train, X_test, y_train, y_test = train_test_split(X, \n                                    y, test_size=0.33, \n                                    random_state = 2111, stratify = y)","f5835b53":"pd.Series(y_train).value_counts() \/ X_train.shape[0] * 100","aa305162":"pd.Series(y_test).value_counts() \/ X_test.shape[0] * 100","dc1c2c61":"pd.Series(y_train).value_counts()","3a57a7b6":"pd.Series(y_test).value_counts()","0eab6245":"# Apply resampling to the training dataset only\nmethod = SMOTE()\nX_SMOTE_Sampling, y_SMOTE_Sampling = method.fit_resample(X_train, y_train)","5b6f46fc":"penalty = 'l2'\nC = 1.0\nclass_weight = 'balanced'\nrandom_state = 2111\nsolver = 'liblinear'\nn_jobs = 1\n\nlogReg = LogisticRegression(penalty = penalty, C = C, \n            class_weight = class_weight, random_state = random_state, \n                            solver = solver, n_jobs = n_jobs)\n\nlogReg.fit(X_SMOTE_Sampling, y_SMOTE_Sampling)","97da0d2e":"y_pred = logReg.predict(X_test)\nconfusion_matrix(y_pred, y_test)","31066c03":"n_estimators = 10\nmax_features = 'auto'\nmax_depth = None\nmin_samples_split = 2\nmin_samples_leaf = 1\nmin_weight_fraction_leaf = 0.0\nmax_leaf_nodes = None\nbootstrap = True\noob_score = False\nn_jobs = -1\nrandom_state = 2111\nclass_weight = 'balanced'\n\nRFC = RandomForestClassifier(n_estimators=n_estimators, \n        max_features=max_features, max_depth=max_depth,\n        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n        min_weight_fraction_leaf=min_weight_fraction_leaf, \n        max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap, \n        oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, \n        class_weight=class_weight)\n\nRFC.fit(X_SMOTE_Sampling, y_SMOTE_Sampling)","57894604":"y_pred = RFC.predict(X_test)\nconfusion_matrix(y_test, y_pred)","d88f9c8c":"from catboost import CatBoostClassifier, Pool\nmodel3 = CatBoostClassifier(iterations = 1000,\n                           depth = 10,\n                           learning_rate = 0.1,\n                           loss_function = 'Logloss',\n                           verbose=True)\n# train the model\nmodel3.fit(X_SMOTE_Sampling, y_SMOTE_Sampling)\n","32cb5d21":"# make the prediction using the resulting model\npreds_class = model3.predict(X_test)\nconfusion_matrix(y_test, preds_class)","b90f25ae":"array([[93757,    68],\n       [   18,   144]])","160b2bf3":"from lightgbm import LGBMClassifier\nmodel14 = LGBMClassifier(max_depth = 10, learning_rate = 0.12)\nmodel14.fit(X_SMOTE_Sampling, y_SMOTE_Sampling)\ny_pred_model4 = model14.predict(X_test)\nconfusion_matrix_model14 = confusion_matrix(y_pred_model4, y_test)\nconfusion_matrix_model14","23fd0d77":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense","8767d2dd":"n_features = X_SMOTE_Sampling.shape[1]","0fe7d2f6":"y_SMOTE_Sampling = keras.utils.to_categorical(y_SMOTE_Sampling)\ny_SMOTE_Sampling","ccb4e67d":"# He initialization: sigma_2 = 2 \/ N (N = the number of input neurons to a particular layer) - for ReLU\n#Xavier initialization: sigma_2 = 1 \/ N (N = the number of input neurons to a particular layer) - for Sigmoid and tanh\nmodel = Sequential()\nmodel.add(Dense(4, activation = 'relu', input_dim = n_features, kernel_initializer = tf.keras.initializers.RandomNormal(mean = 0, stddev = np.sqrt(2\/29)))) # First hidden layer\nmodel.add(Dense(4, activation = 'relu', kernel_initializer = tf.keras.initializers.RandomNormal(mean = 0, stddev = np.sqrt(2\/4))))# second hidden layer\nmodel.add(Dense(4, activation = 'relu', kernel_initializer = tf.keras.initializers.RandomNormal(mean = 0, stddev = np.sqrt(2\/4))))# second hidden layer\nmodel.add(Dense(4, activation = 'relu', kernel_initializer = tf.keras.initializers.RandomNormal(mean = 0, stddev = np.sqrt(2\/4))))# third hidden layer\nmodel.add(Dense(2, activation = 'softmax')) # Output layer\nmodel.summary()","a954625e":"X_SMOTE_Sampling.shape","b36271ff":"y_SMOTE_Sampling.shape","6022829d":"model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0024, beta_1=0.9, beta_2=0.999, epsilon=1e-06, amsgrad=False, name=\"Adam\"), loss = 'CategoricalCrossentropy', metrics = ['accuracy'])\nmodel.fit(X_SMOTE_Sampling, y_SMOTE_Sampling, epochs = 300, batch_size = 512, verbose = 1, validation_split = .1)","5923c785":"#Loss - Epochs\nplt.figure(figsize = (8, 6))\nplt.plot(model.history.history['loss'], label = 'train')\nplt.plot(model.history.history['val_loss'], alpha = 0.7, label = 'test')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = 'upper left')\nplt.grid()","5f65ef80":"#Loss - Epochs\nplt.figure(figsize = (8, 6))\nplt.plot(model.history.history['accuracy'], label = 'train')\nplt.plot(model.history.history['val_accuracy'], alpha = 0.7, label = 'test')\nplt.ylabel('Accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = 'upper left')\nplt.grid()","a761bc32":"y_pred = model.predict(X_test)\ny_pred","06cf9456":"y_pred = np.argmax(y_pred, axis = 1)\ny_pred","1fa1f002":"confusion_matrix(y_pred, y_test)","1969ba25":"## Deep learning","d1107e68":"## Sampling for balenced data","29ede8e9":"# SMOTE Sampling","9ff14093":"### Model 2: RandomForestClassifier","a7b44c68":"### Model 4: Light GBM","a382d30c":"### Model1: Logistic Regression","8de674d2":"### Model 3: CatBoost\n","e494d223":"# Let's Build Models","e6f30d9e":"**Machine learning algorithms have problems with huge unbalanced datasets, I want to use SMOTE Sampling to unbalanced our dataset**","eed9e7ca":"#  When to use resmapling methods\u00b6\n* Use resampling methods on the training set, not on the test set\n* The goal is to produce a better model by providing balanced data, \nThe goal is not to predict the synthetic samples\n* Test data should be free of duplicates and synthetic data\n* Only test the model on real data\n### First, split the data into train and test sets"}}