{"cell_type":{"ab36b9c0":"code","dd570628":"code","8d828d41":"code","cc9d589f":"code","7b8c2fcd":"code","04fe1e1a":"code","62289189":"code","c8dbd88c":"code","faa886cf":"code","dc2f6ca1":"code","0c45a82c":"code","e5eba197":"code","2651731e":"code","80ffd65a":"code","98b8619d":"code","0c39478a":"code","3392626a":"code","aa424434":"code","28ce0037":"code","02a7a9b5":"code","cda53a5b":"code","ca7f48a1":"code","701e3e59":"code","fec7a11b":"code","579a0352":"code","ccadecf6":"code","f19772aa":"code","942cd661":"code","ceb98fa7":"code","8197b013":"code","8952ffef":"code","58005e57":"code","c29ad266":"code","b59503ea":"code","2db9dd5d":"code","d23d105a":"code","2ab7ab5f":"code","37f44801":"code","6aff0840":"code","1fa4fff2":"code","64893b8f":"code","69377365":"code","e7f97a38":"code","f93edc13":"code","a5cb270e":"code","0646660d":"code","b839afec":"code","6c21ddd5":"code","fd2b6d2e":"code","4d44ba5d":"code","0d089b8a":"markdown","35ac05f7":"markdown","0e1723fb":"markdown","7b079b0c":"markdown","8737b96e":"markdown","5ba6a80b":"markdown","aaf11dc1":"markdown","1d761d06":"markdown","6ae55ba0":"markdown","e8f4cf27":"markdown"},"source":{"ab36b9c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dd570628":"df = pd.read_csv(\"\/kaggle\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv\")","8d828d41":"df","cc9d589f":"df.info()","7b8c2fcd":"df.isnull().sum()","04fe1e1a":"df.describe()","62289189":"print(df[\"Message\"][:20])","c8dbd88c":"import re\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE","faa886cf":"def preprocessing_text(text):\n    text = re.sub(\"[^a-zA-Z]\",\" \",text) # \u304b\u306a\u308a\u5927\u80c6\u306asub\n    text = text.lower()\n    \n    text = nltk.word_tokenize(text)\n    \n    lemma = nltk.WordNetLemmatizer()\n    text= [lemma.lemmatize(word) for word in text]\n    \n    text = \" \".join(text)\n    return text","dc2f6ca1":"df[\"Message\"] = df[\"Message\"].apply(preprocessing_text)","0c45a82c":"df[\"Message\"]","e5eba197":"count_vectorizer = CountVectorizer(analyzer='word', stop_words = \"english\", ngram_range=(1,2))\ntfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words = \"english\", ngram_range=(1,2))\n\ncount_mat = count_vectorizer.fit_transform(df[\"Message\"])\ntfidf_mat = tfidf_vectorizer.fit_transform(df[\"Message\"])","2651731e":"print(count_mat.shape)","80ffd65a":"%%time\nn_comp_SVD = 20\nn_comp_TSNE = 2\n\n# SVD\ntext_svd = TruncatedSVD(n_components=n_comp_SVD,random_state=0)\ndf_count_svd = pd.DataFrame(text_svd.fit_transform(count_mat))\ndf_count_svd.columns = ['count_vec_svd_'+str(j+1) for j in range(n_comp_SVD)]\n\ntext_svd = TruncatedSVD(n_components=n_comp_SVD,random_state=0)\ndf_tfidf_svd = pd.DataFrame(text_svd.fit_transform(tfidf_mat))\ndf_tfidf_svd.columns = ['tfidf_vec_svd_'+str(j+1) for j in range(n_comp_SVD)]\n\n# TSNE(\u3061\u3087\u3063\u3068\u91cd\u3044\u306e\u3067\u3001sklearn\u3067\u306f\u306a\u3044\u65b9\u306emodule\u4f7f\u3046\u3068\u304b(\u3053\u3063\u3061\u306e\u65b9\u304c\u901f\u3044\u3089\u3057\u3044))\ntsne = TSNE(n_components=n_comp_TSNE, random_state = 0)\ndf_count_tsne = pd.DataFrame(tsne.fit_transform(count_mat))\ndf_count_tsne.columns = ['count_vec_tsne_'+str(j+1) for j in range(n_comp_TSNE)]\n\ntsne = TSNE(n_components=n_comp_TSNE, random_state = 0)\ndf_tfidf_tsne = pd.DataFrame(tsne.fit_transform(tfidf_mat))\ndf_tfidf_tsne.columns = ['tfidf_vec_tsne_'+str(j+1) for j in range(n_comp_TSNE)]\n\ndf = pd.concat([df, df_count_svd,df_tfidf_svd,df_count_tsne,df_tfidf_tsne],axis=1)","98b8619d":"df.columns","0c39478a":"pd.set_option(\"display.max_columns\", 80)\npd.set_option(\"display.max_rows\", 80)\ndf.head()","3392626a":"import matplotlib.pyplot as plt\nimport seaborn as sns","aa424434":"sns.scatterplot(x = \"count_vec_tsne_1\", y = \"count_vec_tsne_2\", hue = \"Category\", data = df)","28ce0037":"sns.scatterplot(x = \"tfidf_vec_tsne_1\", y = \"tfidf_vec_tsne_2\", hue = \"Category\", data = df)","02a7a9b5":"y = df.iloc[:,0].values\nx = df.drop([\"Category\", \"Message\"], axis = 1)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 42)","cda53a5b":"#We make model for predict\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"the accuracy of our model: {}\".format(nb.score(x_test,y_test)))","ca7f48a1":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter = 200)\nlr.fit(x_train,y_train)\nprint(\"our accuracy is: {}\".format(lr.score(x_test,y_test)))","701e3e59":"from sklearn.feature_selection import SelectKBest, mutual_info_classif","fec7a11b":"x_train_tfidf, x_test_tfidf, y_train, y_test = train_test_split(tfidf_mat,y, test_size = 0.2, random_state = 42)","579a0352":"# \u91cd\u3044\nselector = SelectKBest(k = 10000, score_func = mutual_info_classif)\nselector.fit(x_train_tfidf, y_train)\nx_train_new = selector.transform(x_train_tfidf)\nx_test_new = selector.transform(x_test_tfidf)","ccadecf6":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter = 200)\nlr.fit(x_train_new,y_train)\nprint(\"our accuracy is: {}\".format(lr.score(x_test_new,y_test)))","f19772aa":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb","942cd661":"le = LabelEncoder()\ndf[\"Category\"] = le.fit_transform(df[\"Category\"])","ceb98fa7":"y = df[\"Category\"]\nx = df.drop([\"Category\", \"Message\"], axis = 1)","8197b013":"# \u5b66\u7fd2\ny_preds = []\nmodels = []\noof_train = np.zeros(len(x))\n\nkf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\n\nparams = {\n    \"boosting_type\" : \"gbdt\",\n    \"objective\" : \"binary\",\n    \"metric\" : \"binary_logloss\",\n    \"max_depth\" : -1, # \u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u306f-1\u30670\u4ee5\u4e0b\u306e\u5024\u306f\u5236\u9650\u306a\u3057\u3092\u610f\u5473\u3059\u308b\uff0e\n    #\"num_leaves\" : 31,  #\u6728\u306b\u3042\u308b\u5206\u5c90\u306e\u500b\u6570\uff0esklearn\u306emax_depthsklearn\u306emax_depth\u3068\u306e\u95a2\u4fc2\u306fnum_leaves=2^max_depth\n                                    #\u30c7\u30d5\u30a9\u30eb\u30c8\u306f31\uff0e\u5927\u304d\u304f\u3059\u308b\u3068\u7cbe\u5ea6\u306f\u4e0a\u304c\u308b\u304c\u904e\u5b66\u7fd2\u304c\u9032\u3080\uff0e\n    \"learning_rate\": 0.03, \n    \"feature_fraction\" : 0.7, #\u5b66\u7fd2\u306e\u9ad8\u901f\u5316\u3068\u904e\u5b66\u7fd2\u306e\u6291\u5236\u306b\u4f7f\u7528\u3055\u308c\u308b\uff0e\u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u91cf\u306efeature_fraction * 100 % \u3060\u3051\u4f7f\u7528\u3059\u308b\uff0e\n    'bagging_fraction': 0.7, #like feature_fraction, but this will randomly select part of data without resampling\n    \"bagging_freq\" : 5, #frequency for bagging. 0 means disable bagging; k means perform bagging at every k iteration\n                                    # Note: to enable bagging, bagging_fraction should be set to value smaller than 1.0 as well\n    \"early_stopping_rounds\" : 100,\n    'n_estimators':2000, # aliases: num_iteration, n_iter, num_tree, num_trees, num_round, num_rounds, num_boost_round, n_estimators. \n                                        # number of boosting iterations\n     \"seed\" : 42,\n    \"reg_alpha\":0.1,\n    \"reg_lambda\":0.1 #0.1\n}\n\n\nfor fold, (train_index, val_index) in enumerate(kf.split(x, y)):\n    tr_x = x.iloc[train_index, :]\n    tr_y = y[train_index]\n    val_x = x.iloc[val_index, :]\n    val_y = y[val_index]\n    \n    lgb_train = lgb.Dataset(tr_x, tr_y)\n    lgb_eval = lgb.Dataset(val_x, val_y, reference = lgb_train)\n    \n    model = lgb.train(params, \n                      lgb_train,\n                      valid_sets = [lgb_train, lgb_eval], \n                      verbose_eval= 10\n                     )\n    oof_train[val_index] = model.predict(val_x, num_iteration = model.best_iteration)\n    #y_pred = model.predict(test, num_iteration = model.best_iteration)\n    \n    #y_preds.append(y_pred)\n    models.append(model)","8952ffef":"accuracy_score(np.round(oof_train), y)","58005e57":"fi = pd.DataFrame(index = x.columns)\nfor i, m in enumerate(models):\n    fi[\"model_\"+str(i+1)] = m.feature_importance(importance_type = \"gain\")\nfi[\"fi_ave\"] = fi.mean(axis = 1)\nfi.sort_values(by = \"fi_ave\", inplace = True, ascending = False)","c29ad266":"plt.figure(figsize = (15,12))\nsns.barplot(x = fi[\"fi_ave\"][:30], y = fi.index[:30])\nplt.show()","b59503ea":"fi","2db9dd5d":"sns.scatterplot(x = \"tfidf_vec_tsne_1\", y = \"tfidf_vec_svd_5\", data = df, hue = \"Category\")","d23d105a":"from sklearn.decomposition import LatentDirichletAllocation","2ab7ab5f":"lda = LatentDirichletAllocation(n_components=10,random_state=0)\ncount_lda = lda.fit_transform(count_mat)","37f44801":"count_features = count_vectorizer.get_feature_names()\nfor tn in range(10):\n    print(\"topic #\"+str(tn))\n    row = lda.components_[tn]\n    words = ', '.join([count_features[i] for i in row.argsort()[:-20-1:-1]])\n    print(words, \"\\n\")","6aff0840":"tfidf_lda = lda.fit_transform(tfidf_mat)\ntfidf_features = tfidf_vectorizer.get_feature_names()\nfor tn in range(10):\n    print(\"topic #\"+str(tn))\n    row = lda.components_[tn]\n    words = ', '.join([tfidf_features[i] for i in row.argsort()[:-20-1:-1]])\n    print(words, \"\\n\")","1fa4fff2":"lda_count_df = pd.DataFrame(count_lda)\nlda_count_df = lda_count_df.add_prefix(\"lda_count_\")","64893b8f":"lda_tfidf_df = pd.DataFrame(tfidf_lda)\nlda_tfidf_df = lda_tfidf_df.add_prefix(\"lda_tfidf_\")","69377365":"df_lda = pd.concat([df,lda_count_df, lda_tfidf_df], axis = 1)","e7f97a38":"df_lda","f93edc13":"y = df_lda[\"Category\"]\nx = df_lda.drop([\"Category\", \"Message\"], axis = 1)","a5cb270e":"# \u5b66\u7fd2\ny_preds = []\nmodels = []\noof_train = np.zeros(len(x))\n\nkf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\n\nparams = {\n    \"boosting_type\" : \"gbdt\",\n    \"objective\" : \"binary\",\n    \"metric\" : \"binary_logloss\",\n    \"max_depth\" : -1, # \u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u306f-1\u30670\u4ee5\u4e0b\u306e\u5024\u306f\u5236\u9650\u306a\u3057\u3092\u610f\u5473\u3059\u308b\uff0e\n    #\"num_leaves\" : 31,  #\u6728\u306b\u3042\u308b\u5206\u5c90\u306e\u500b\u6570\uff0esklearn\u306emax_depthsklearn\u306emax_depth\u3068\u306e\u95a2\u4fc2\u306fnum_leaves=2^max_depth\n                                    #\u30c7\u30d5\u30a9\u30eb\u30c8\u306f31\uff0e\u5927\u304d\u304f\u3059\u308b\u3068\u7cbe\u5ea6\u306f\u4e0a\u304c\u308b\u304c\u904e\u5b66\u7fd2\u304c\u9032\u3080\uff0e\n    \"learning_rate\": 0.03, \n    \"feature_fraction\" : 0.7, #\u5b66\u7fd2\u306e\u9ad8\u901f\u5316\u3068\u904e\u5b66\u7fd2\u306e\u6291\u5236\u306b\u4f7f\u7528\u3055\u308c\u308b\uff0e\u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u91cf\u306efeature_fraction * 100 % \u3060\u3051\u4f7f\u7528\u3059\u308b\uff0e\n    'bagging_fraction': 0.7, #like feature_fraction, but this will randomly select part of data without resampling\n    \"bagging_freq\" : 5, #frequency for bagging. 0 means disable bagging; k means perform bagging at every k iteration\n                                    # Note: to enable bagging, bagging_fraction should be set to value smaller than 1.0 as well\n    \"early_stopping_rounds\" : 100,\n    'n_estimators':2000, # aliases: num_iteration, n_iter, num_tree, num_trees, num_round, num_rounds, num_boost_round, n_estimators. \n                                        # number of boosting iterations\n     \"seed\" : 42,\n    \"reg_alpha\":0.1,\n    \"reg_lambda\":0.1 #0.1\n}\n\n\nfor fold, (train_index, val_index) in enumerate(kf.split(x, y)):\n    tr_x = x.iloc[train_index, :]\n    tr_y = y[train_index]\n    val_x = x.iloc[val_index, :]\n    val_y = y[val_index]\n    \n    lgb_train = lgb.Dataset(tr_x, tr_y)\n    lgb_eval = lgb.Dataset(val_x, val_y, reference = lgb_train)\n    \n    model = lgb.train(params, \n                      lgb_train,\n                      valid_sets = [lgb_train, lgb_eval], \n                      verbose_eval= 10\n                     )\n    oof_train[val_index] = model.predict(val_x, num_iteration = model.best_iteration)\n    #y_pred = model.predict(test, num_iteration = model.best_iteration)\n    \n    #y_preds.append(y_pred)\n    models.append(model)","0646660d":"accuracy_score(np.round(oof_train), y)","b839afec":"fi = pd.DataFrame(index = x.columns)\nfor i, m in enumerate(models):\n    fi[\"model_\"+str(i+1)] = m.feature_importance(importance_type = \"gain\")\nfi[\"fi_ave\"] = fi.mean(axis = 1)\nfi.sort_values(by = \"fi_ave\", inplace = True, ascending = False)","6c21ddd5":"fi[:50]","fd2b6d2e":"plt.figure(figsize = (15,12))\nsns.barplot(x = fi[\"fi_ave\"][:30], y = fi.index[:30])\nplt.show()","4d44ba5d":"fi","0d089b8a":"# LatentDirichletAllocation","35ac05f7":"### \u751f\u306etfidf\u3092\u76f8\u4e92\u60c5\u5831\u91cf\u3067\u7279\u5fb4\u9078\u629e\u3057\u3066\u304b\u3089\u6295\u5165","0e1723fb":"## \u304f\u3063\u3064\u3051\u3066\u5b66\u7fd2","7b079b0c":"# import\u3068df\u78ba\u8a8d","8737b96e":"- https:\/\/www.kaggle.com\/team-ai\/spam-text-message-classification\n\n- https:\/\/www.kaggle.com\/mdmahmudferdous\/nlp-tutorial-spam-msg-classification","5ba6a80b":"# lgb\u3067cv","aaf11dc1":"- tfidf\u306e\u65b9\u304c\u660e\u3089\u304b\u306b\u5206\u96e2\u3057\u3084\u3059\u305d\u3046\u3002\n- tfidf_vec_tsne1\u3060\u3051\u3067spam\u306f\u7d50\u69cb\u62bd\u51fa\u3067\u304d\u3066\u308b","1d761d06":"# \u3084\u308a\u305f\u3044\u3053\u3068\n- spam\u3068ham\u306e\u5206\u985e(\u30e2\u30c7\u30eb\u306f\u306a\u3093\u3067\u3082\u826f\u3044)\n- nlp\u4f7f\u3063\u3066\u7279\u5fb4\u751f\u6210\n- \u53c2\u8003notebook\n - https:\/\/www.kaggle.com\/nihalbey\/spam-detection-and-deep-nlp","6ae55ba0":"# \u7c21\u5358\u306a\u5b66\u7fd2","e8f4cf27":"- message\u306f\u524d\u51e6\u7406\u5fc5\u8981\n    - \u5927\u6587\u5b57\u5c0f\u6587\u5b57\n    - \u6570\u5b57\n    - \u8a18\u53f7\n    - \n- \u524d\u51e6\u7406\u7cfb\u95a2\u6570\u4f5c\u3063\u3066\u304a\u304f\u3068\u4fbf\u5229\u306a\u306e\u3067\u3001\u3044\u308d\u3044\u308d\u96c6\u3081\u3066\u304a\u304f\n    - https:\/\/github.com\/upura\/probspace-youtube\/blob\/master\/ayniy\/preprocessing\/text.py"}}