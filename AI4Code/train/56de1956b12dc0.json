{"cell_type":{"85515c82":"code","fd26a001":"code","a17f77bf":"code","3be4c617":"code","afa15d5d":"code","67006df7":"code","dd6fecca":"code","9fa2cbc6":"code","5e89b74e":"code","dc567cc8":"code","05ef3041":"code","b3fbc5be":"code","294d749c":"code","750f482e":"code","42826f58":"code","63148d83":"code","145cf7b1":"code","8c7ee313":"code","a25eb3c4":"code","283a7a7d":"code","67ee3289":"code","474849d1":"code","c2aacfd2":"code","bc38c9e2":"code","8eade6e6":"code","b38eb67f":"code","1c5ee98a":"code","3108e3e6":"code","07999fea":"code","aa1df30c":"code","f04fb62e":"code","17384830":"code","0137a0ff":"code","ee439c8f":"markdown","d61c126f":"markdown","fb63ad43":"markdown","b1b114f5":"markdown","5c13895c":"markdown","34fb5b21":"markdown","e11982f9":"markdown","d093a95c":"markdown","f15a9928":"markdown","da01580e":"markdown","7bb45967":"markdown","6b337038":"markdown","7c653265":"markdown","64573307":"markdown","d3bf361f":"markdown","e1f17f39":"markdown","4b576b8f":"markdown","9a57d3bf":"markdown","260f0c8c":"markdown"},"source":{"85515c82":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fd26a001":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n# The following two lines determines the number of visible columns and \n#the number of visible rows for dataframes and that doesn't affect the code\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","a17f77bf":"# Dataframe.head(n) function enables you to show the first n rows in the dataframe\n\ntrain.head()","3be4c617":"train.describe()","afa15d5d":"test.head()","67006df7":"print(\"The number of traning examples(data points) = %i \" % train.shape[0])\nprint(\"The number of features we have = %i \" % train.shape[1])","dd6fecca":"train.describe()","9fa2cbc6":"train.isnull().sum()","5e89b74e":"train.drop([\"PoolQC\",\"PavedDrive\",'Fence','MiscFeature','FireplaceQu','GarageFinish', 'Alley'], inplace = True, axis = 1 )\ntest.drop([\"PoolQC\",\"PavedDrive\",'Fence','MiscFeature','FireplaceQu','GarageFinish' , 'Alley' ], inplace = True, axis = 1 )","dc567cc8":"train.head()","05ef3041":"train = pd.get_dummies(train)\ntest = pd.get_dummies(test)","b3fbc5be":"y = train['SalePrice']\ntrain = train.drop([\"SalePrice\"], axis = 1)","294d749c":"missing_cols = set( train.columns ) - set( test.columns )\nprint( missing_cols )","750f482e":"train.drop( missing_cols , inplace = True, axis = 1 )","42826f58":"train.head()","63148d83":"mean = train.mean().astype(np.int32)\ntrain.fillna( mean , inplace = True)\n\nmean = test.mean().astype(np.int32)\ntest.fillna( mean , inplace = True)","145cf7b1":"train.isnull().sum()","8c7ee313":"import seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\ncorr = train.corr()\nf, ax = plt.subplots(figsize=(25, 25))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5)","a25eb3c4":"train['LotFA'] = train['LotArea'] + train['LotFrontage']\ntrain['LotFA2'] = train['LotArea'] - train['LotFrontage']\n\ntrain['OverallRate'] = train['OverallQual'] + train['OverallCond']\ntrain['OverallRate2'] = train['OverallQual'] - train['OverallCond']\n\ntrain['yearAvg'] = train['YearBuilt'] + train['YearRemodAdd']\ntrain['yearAvg2'] = train['YearBuilt'] - train['YearRemodAdd']\n\ntrain['fsFeet'] = train['1stFlrSF'] + train['2ndFlrSF']\ntrain['fsFeet2'] = train['1stFlrSF'] - train['2ndFlrSF']\n\ntrain['bath'] = train['BsmtFullBath'] + train['BsmtHalfBath']\ntrain['bath2'] = train['BsmtFullBath'] - train['BsmtHalfBath']\n\ntrain['areaPerCar'] = train['GarageArea'] \/ train['GarageCars']\n\ntrain['OpenEnclosedPorch'] = train['OpenPorchSF'] + train['EnclosedPorch']\ntrain['OpenEnclosedPorch2'] = train['OpenPorchSF'] - train['EnclosedPorch']\n\ntrain['yearSANEG'] = (train['YrSold']**2 - train['yearAvg']**2)**0.5\ntrain['yearSAPOS'] = (train['YrSold']**2 + train['yearAvg']**2)**0.5\n\n\ntest['LotFA'] = test['LotArea'] + test['LotFrontage']\ntest['LotFA2'] = test['LotArea'] - test['LotFrontage']\n\ntest['OverallRate'] = test['OverallQual'] + test['OverallCond']\ntest['OverallRate2'] = test['OverallQual'] - test['OverallCond']\n\ntest['yearAvg'] = test['YearBuilt'] + test['YearRemodAdd']\ntest['yearAvg2'] = test['YearBuilt'] - test['YearRemodAdd']\n\ntest['fsFeet'] = test['1stFlrSF'] + test['2ndFlrSF']\ntest['fsFeet2'] = test['1stFlrSF'] - test['2ndFlrSF']\n\ntest['bath'] = test['BsmtFullBath'] + test['BsmtHalfBath']\ntest['bath2'] = test['BsmtFullBath'] - test['BsmtHalfBath']\n\ntest['areaPerCar'] = test['GarageArea'] \/ test['GarageCars']\n\ntest['OpenEnclosedPorch'] = test['OpenPorchSF'] + test['EnclosedPorch']\ntest['OpenEnclosedPorch2'] = test['OpenPorchSF'] - test['EnclosedPorch']\n\ntest['yearSANEG'] = (test['YrSold']**2 - test['yearAvg']**2)**0.5\ntest['yearSAPOS'] = (test['YrSold']**2 + test['yearAvg']**2)**0.5","283a7a7d":"train.fillna( 0 , inplace = True)\ntest.fillna( 0 , inplace = True)","67ee3289":"train.isnull().sum()","474849d1":"from sklearn.cluster import KMeans\ncluster = KMeans(n_clusters= 800, max_iter=300, tol=0.0001, verbose=0, random_state = 0, n_jobs=-1)","c2aacfd2":"kmeans_train = cluster.fit(train)\nlabels_train = kmeans_train.labels_\nlabels_train","bc38c9e2":"kmeans_testing = cluster.predict(test)\nkmeans_testing","8eade6e6":"train['cluster'] = labels_train\ntest ['cluster'] = kmeans_testing","b38eb67f":"print(\"The number of traning examples(data points) = %i \" % train.shape[0])\nprint(\"The number of features we have = %i \" % train.shape[1])","1c5ee98a":"id = test['Id']\ntest.drop(['Id'], axis = 1, inplace = True)\ntrain.drop(['Id'], axis = 1, inplace = True)\n","3108e3e6":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split( train.values , y.values, test_size=0.05, random_state=42 )","07999fea":"from sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error  \nfrom sklearn.ensemble import ExtraTreesRegressor\n\net  = ExtraTreesRegressor(n_estimators=950 ,  max_features = 'auto', max_leaf_nodes=None, n_jobs= -1, random_state = 0, verbose = 0)\ngbr = GradientBoostingRegressor()\nlasso = Lasso()\nxgbr = XGBRegressor()\nsvr = SVR(kernel= 'rbf', gamma= 'auto', tol=0.001, C=100.0, max_iter=-1)\nrf = RandomForestRegressor(n_estimators=900,  random_state=0)\nlr = LinearRegression(fit_intercept=True, normalize=True, copy_X=True, n_jobs=-1)\nknnR = KNeighborsRegressor(n_neighbors=20, n_jobs=-1)\nreg = StackingCVRegressor(regressors=[  lasso , xgbr , et],meta_regressor=lasso)\n\nreg.fit(x_train, y_train, groups = None)\n","aa1df30c":"reg.score(x_test,y_test)","f04fb62e":"test = test[train.columns]","17384830":"sub = pd.DataFrame({\"Id\": id ,\"SalePrice\": (reg.predict(test.values)).round(decimals=2)})\nsub.to_csv(\"stackcv_linearsvc.csv\", index=False) ","0137a0ff":"sub.head(10)","ee439c8f":"Based on the relations between features, I will create new features to increase the accuracy of my tree-based model","d61c126f":"# Data Exploration\nLet's now see how the training data and the test data look like","fb63ad43":"# Fitting the Regression Model","b1b114f5":"I will use Train test split from Scikit-learn to get an estimate for the result before submission. I used a very small test set because the data is too small.","5c13895c":"Let's check if any of the columns contains NaNs or Nulls so that we can fill those values if they are insignificant or drop them. We may drop a whole column if most of its values are NaNs or fill its value according to its relation with other columns in the dataframe. Nones can also be 0 in some datasets and that is why i am going to use the describe of the train to see if the range of numbers is not reasonable or not. if you are dropping rows with NaNs and you notice that you need to drop a large portion of your dataset then you should think about filling the NaN values or drop a column that has most of its values missing.","34fb5b21":"We can check the score of the model on the x_test before predicting the output of the test set","e11982f9":"Now let's fit the model and using Random forests, ExtraTreesRegressor and support vector regressor ensemble learning using [Stacking regressor](https:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingCVRegressor\/) with the lasso regressor as meta regressor","d093a95c":"Let's now explore the dataset that we have. First we will load the training set and test set and show all the columns using pandas.","f15a9928":"Since all the NaNs were removed, We should now check the correlation matrix to see how features relate to the target feature.","da01580e":"Now we will predict the output and making an output CSV","7bb45967":"Let's now check again the dimentions of our training set after engineering the catagorical features using the get_dummies function.","6b337038":"Let's now see how many data points we have for training.","7c653265":"Now i will make sure that the traing and the test set cols are in the same order","64573307":"We will fill NAN values in the rest of the columns using the mean values","d3bf361f":"It seems we have some columns with most of the values null values, column Alley has 1369 null values out of 1460 total values and so as FireplaceQu, PoolQC, MiscFeature and Fence. I will remove those columns because filling it won't be an ease task as we can't find a relationship between most of them and the other features due to the huge numbers of missing values in those columns. \n<lb>Let's now discover the correlation matrix for this dataset and see if we can combine features or drop some according to its correlation with the output labels after removing the mentioned columns.","e1f17f39":"### NOTE: \nWhen using get dummies it is important to notice that it may give different results when applied on training other than when applied on test sets\nthat is because if you have training = | c1 , c2 |           and test = | c1 , c2 |          then you will get new columns in training named [ a, b, c, d] and only in test [a, b]\n                                                                 |a ,    b  |                              |a ,    b  |\n                                                                 |c ,    d  |\nThat is why we will removing columns in training sets that are not in test set","4b576b8f":"# ExtraTrees with Feature Engineering \nIn this Tutorial I am going to explain in details how to use regression with Ensemble learning. In many cases machine learning algorithms don't perform well without feature engineering which is the process of filling NaNs and missing values , creating new features and etc... . I will also be performing some exploratory data analysis to perform feature engineering before implementing the suitable model.","9a57d3bf":"now we get the label from the training set and put it in a seperate series then we drop it from the training set for future use.","260f0c8c":"Now we will check again to see if all the NaN values have been removed"}}