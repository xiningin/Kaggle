{"cell_type":{"b48f5c44":"code","5df03ad1":"code","a38fdb01":"code","1e992cd6":"code","56909c39":"code","adc63c6d":"code","763e60e7":"code","ca0c2919":"code","2f984100":"code","f61eca74":"code","b85b05c7":"code","a45cdc04":"code","3a12aa55":"code","4ac35837":"code","8bc72cc3":"code","eb25845b":"code","35e45a24":"code","f3282cbd":"code","07f1aa43":"code","cdecb848":"code","cfbe0e64":"code","246dd01a":"code","9d50cf84":"code","f5090b9f":"code","827d4400":"code","27a3298b":"markdown","f5d66a56":"markdown","94258a80":"markdown","ca6d3c7e":"markdown","19ac8be4":"markdown","3c42dd5f":"markdown","f4822d3b":"markdown","875b4318":"markdown","d766224e":"markdown","8ebbbc8e":"markdown","ff14baba":"markdown","5ec7ca13":"markdown","7af5c552":"markdown","43054571":"markdown","ddccd22d":"markdown","c95bbef6":"markdown","6c027f5b":"markdown","eac6b5ae":"markdown","eafb8281":"markdown"},"source":{"b48f5c44":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nimport tensorflow as tf\n\nfrom sklearn.metrics import explained_variance_score, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import Lasso, LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR","5df03ad1":"df = pd.read_csv(\"..\/input\/predict-test-scores-of-students\/test_scores.csv\")\ndf.head()","a38fdb01":"df.info()","1e992cd6":"df.isna().sum()","56909c39":"df.describe()","adc63c6d":"df.columns","763e60e7":"df.drop([\"school\", \"classroom\", \"student_id\"], axis=1, inplace=True)\ndf.columns","ca0c2919":"for x in [\"school_setting\", \"school_type\", \"teaching_method\", \"gender\"]:\n    for val in df[x].unique():\n        count = df[x].value_counts()[val]\n        percent = df[x].value_counts(normalize=True)[val] * 100\n        print(f\"{val} - Count: {count}, Percentage: {percent:.2f}%\")\n    print()","2f984100":"corr = df.drop(\"posttest\", axis=1).corr()\n \nsns.heatmap(corr, annot=True, linewidth=.6, linecolor=\"black\")\nplt.show()","f61eca74":"sns.pairplot(df, hue=\"gender\", palette=\"Set2\")\nplt.show()","b85b05c7":"df = pd.get_dummies(df)\ndf.head()","a45cdc04":"# Renaming last two column to not have spaces between words \ndf.rename(columns={\n                    df.columns[-2]: '_'.join(df.columns[-2].split()),\n                    df.columns[-1]: '_'.join(df.columns[-1].split()),\n                   }, inplace=True)\ndf.head()","3a12aa55":"X = df.drop(\"posttest\", axis=1)\ny = df[\"posttest\"]","4ac35837":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","8bc72cc3":"def score(y_test, y_pred):\n    \"\"\"Helper function for evaluation metrics.\"\"\"\n    explained_variance = explained_variance_score(y_test, y_pred) * 100\n    mae = round(mean_absolute_error(y_test, y_pred), 2)\n    print(f\"\"\"Explained Variance: {explained_variance:.2f}%\nMAE: {mae:.2f}\"\"\")\n    \n    return explained_variance","eb25845b":"accuracy_scores = np.zeros(6, dtype=\"float64\")","35e45a24":"reg = LinearRegression().fit(X_train, y_train)\ny_pred = reg.predict(X_test)\naccuracy_scores[0] = score(y_test, y_pred)","f3282cbd":"reg1 = Lasso().fit(X_train, y_train)\ny_pred1 = reg1.predict(X_test)\naccuracy_scores[1] = score(y_test, y_pred1)","07f1aa43":"reg2 = DecisionTreeRegressor().fit(X_train, y_train)\ny_pred2 = reg2.predict(X_test)\naccuracy_scores[2] = score(y_test, y_pred2)","cdecb848":"reg3 = SVR().fit(X_train, y_train)\ny_pred3 = reg3.predict(X_test)\naccuracy_scores[3] = score(y_test, y_pred3)","cfbe0e64":"reg4 = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_train, y_train)\ny_pred4 = reg4.predict(X_test)\naccuracy_scores[4] = score(y_test, y_pred4)","246dd01a":"tf.random.set_seed(42)\n\nreg5 = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation=\"relu\"),\n    tf.keras.layers.Dense(64, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n])\n\nreg5.compile(loss=tf.keras.losses.mae,\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n              metrics=[\"mae\"])\n\nhistory = reg5.fit(X_train, y_train, epochs=100, verbose=0)","9d50cf84":"reg5.evaluate(X_test, y_test)","f5090b9f":"y_pred5 = reg5.predict(X_test)\naccuracy_scores[5] = score(y_test, y_pred5)","827d4400":"models = [\n          \"Linear Regression\", \"Lasso Regressor\", \"Decision Tree Regressor\",\n          \"Support Vector Regressor\", \"Random Forest Regressor\", \"Neural Network Regression\",\n         ]\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=models, y=accuracy_scores)\n\nplt.xlabel(\"Model Name\")\nplt.xticks(rotation = -90)\nplt.ylabel(\"Accuracy\")\n\nplt.show()","27a3298b":"# Visualizing","f5d66a56":"## Splitting the data in training and testing set\n\n- Training data set is used for fitting our model to learn the patterns.\n- Testing data set is used for prediction and unbiased evaluation of our final model\n\nWe can do this by using [sklearn.model_selection.train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html)\n\nTraining data set - 80% of the total data\n\nTesting data set - 20% of the total data","94258a80":"# Importing the libraries","ca6d3c7e":"# One Hot Encoding\n\nMost of the machine learning models don't like text. What do they like? Numbers. **One Hot Encoding** is an important part of **feature engineering** which is used to convert categorial data so that they can be provided to our machine learning model.\n\nExample: If we have two colors `red` and `green` and we want to represent `red` we could do something like\n\n```\n+---+-----+\n|red|green|\n+---------+\n| 1 |  0  |\n+---+-----+\n```\nThese are often referred to as \"dummy variables\".\n\n\nWe can do this by using [pandas.get_dummies](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.get_dummies.html)\n\nAlternate: [sklearn.preprocessing.OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html)\n\nExtra Resource: https:\/\/www.educative.io\/blog\/one-hot-encoding","19ac8be4":"# Exploring the data\n\nKnowledge about the data you are working on is very important for data analysis.\n\nWhat I have used:\n\n- [pandas.DataFrame.info](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.info.html)\n\n- [pandas.DataFrame.isna](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.isna.html)\n\n- [pandas.DataFrame.describe](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.describe.html)\n\n- [pandas.DataFrame.columns](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.columns.html)","3c42dd5f":"# Conclusion","f4822d3b":"# Dropping columns\n\nBy looking at the data we can see that the features `school`, `classroom` and `student_id` doesn't have any effect on our label `posttest`, so we can safely drop them using [pandas.DataFrame.drop](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.drop.html)","875b4318":"## Linear Regression","d766224e":"# Building Regression Models","8ebbbc8e":"# Loading the data","ff14baba":"## Creating features (X) and label (y)\n\nFeatures are often referred to as \"independent variables\" and Label is often referred to as \"dependent variable\".\n\nHere `posttest` is our label because it depends on other features.","5ec7ca13":"# Exploring Categorical Data","7af5c552":"## Support Vector Regressor","43054571":"Accuracy: 94.76%\n\nMAE: 2.54","ddccd22d":"# Inspiration\n\nInspired and motivated by: https:\/\/www.kaggle.com\/oscardavidperilla\/regression-for-note-predictions","c95bbef6":"## Neural Network Regression","6c027f5b":"## Decision Tree Regressor","eac6b5ae":"## Lasso Regressor","eafb8281":"## Random Forest Regressor"}}