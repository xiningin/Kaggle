{"cell_type":{"3e75be4a":"code","71a136d8":"code","dde79265":"code","0b4d23cf":"code","92da101e":"code","4f636e33":"code","52695706":"code","70205839":"code","c628874a":"code","c96e87e9":"code","08ad0243":"code","c830942c":"code","9316bed0":"code","449825fd":"code","adff51a9":"code","4942da0a":"code","077b0146":"code","f0af59d2":"code","5dfcf04c":"code","a46b17d0":"code","4dd9167b":"code","e17ef9a2":"code","6d2c6803":"code","b4c09efb":"code","bdf66cb0":"code","ce0d3549":"code","0fd656f8":"code","f0ac372e":"code","4833b0ff":"code","d0a15fb6":"code","dc24b36d":"code","14433d30":"code","02d66987":"code","420f030a":"code","4f9b2729":"code","208e495c":"code","27832e91":"code","ae988aee":"code","dc36dee2":"code","8ea6e4d2":"code","01fddbca":"code","429e7cf4":"code","25270863":"code","7ccb57cf":"code","d580c248":"code","f6a6a1ed":"code","e0390919":"code","487f11c8":"code","3d3e6b7a":"code","3a7e8883":"code","54997ac4":"code","827aa6cd":"markdown","3fd63d1c":"markdown","859722fa":"markdown","72d8f596":"markdown","f9929ff7":"markdown"},"source":{"3e75be4a":"import matplotlib.pyplot as plt\nimport cv2","71a136d8":"img = cv2.imread('..\/input\/model-architecture\/Architecture.png')","dde79265":"fig = plt.figure(figsize=(20,15))\nplt.imshow(img);","0b4d23cf":"import os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport numpy as np\nimport pandas as pd\nimport csv\nimport json\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom sklearn.metrics import confusion_matrix\n\nprint(torch.__version__)","92da101e":"os.listdir('..\/input\/beauty-weights\/')","4f636e33":"class Pipeline:\n    def __init__(self, name, data, profiles, emb_dim):\n        self.name = name\n        self.data = data\n        self.profiles = profiles\n        self.emb_dim = emb_dim\n        \n        self.corpus= self.data['title']\n        self.occurences = self.count_words(self.corpus)\n        self.vocab, self.vocab_list = self.build_condensed_vocab(self.occurences)\n        self.labels = self.build_labels(profiles)\n        self.vocab, self.vocab_list, self.int_labels = self.process_labels(self.labels, self.vocab, self.vocab_list)\n        self.vocab_size = len(self.vocab_list) + 1 # for 0 to emb to [0 ... 0]\n        self.int_corpus = self.integerize(self.corpus)\n        self.padded_corpus, self.padded_labels = self.pad_sequences(self.int_corpus, self.int_labels)\n        self.seq_len = len(self.int_corpus[0])\n        \n        self.embedding_matrix = self.build_embedding_matrix(self.vocab_list)\n        \n        self.num_attributes = len(profiles)\n        self.attributes = list(self.profiles.keys())\n        self.num_classes = [len(self.profiles[key])+1 for key in self.attributes]\n       \n        self.training_labels = [np.array(self.data[col].fillna(-1)) for i, col in enumerate(self.attributes)]\n    \n    \n    def count_words(self, corpus):\n        occurences = {}\n        for sentence in corpus:\n            tokens = sentence.split()\n            for word in tokens:\n                if word not in occurences:\n                    occurences[word] = 1\n                else:\n                    occurences[word] += 1\n        return occurences\n    \n    def build_condensed_vocab(self, occurences):\n        for word in occurences:\n            if occurences[word] < 3:\n                occurences[word] = '<UNKNOWN>'\n        \n        vocab = {}    \n        vocab_list = []\n        count = 1 #Starts at 1, where 0 is for padding\n        for word in occurences:\n            if occurences[word] != '<UNKNOWN>':\n                vocab[word] = count\n                vocab_list.append(word)\n                count += 1\n        vocab['<UNKNOWN>'] = count\n        vocab_list.append('<UNKNOWN>')\n        return vocab, vocab_list\n    \n    def build_labels(self, profiles):    \n        att = profiles.keys()\n        labels = []\n        for key in att:\n            temp_dict = profiles[key]\n            temp_labels = [k for k in sorted(temp_dict, key=temp_dict.get, reverse=False)]\n            labels.append(temp_labels)\n        return labels\n            \n    def process_labels(self, labels, vocab, vocab_list):\n        int_labels = []\n        count = len(self.vocab) + 1\n        for line in labels:\n            temp_labels = []\n            for string in line:\n                try:\n                    splitted = string.split(\" \")\n                    temp = []\n                    for word in splitted:\n                        try:\n                            temp.append(self.vocab[word])\n                        except KeyError:\n                            self.vocab[word] = count\n                            self.vocab_list.append(word)\n                            count += 1\n                            temp.append(self.vocab[word])\n                    temp_labels.append(temp)\n                except:\n                    try:\n                        temp_labels.append([self.vocab[string]])\n                    except KeyError:\n                        self.vocab[string] = count\n                        self.vocab_list.append(string.lower())\n                        count += 1\n                        temp_labels.append([self.vocab[string]])\n            int_labels.append(temp_labels)\n        return vocab, vocab_list, int_labels                      \n    \n    def integerize(self, corpus, text=True):\n        features = []\n        for sentence in corpus:\n            temp = []\n            if text:\n                tokens = sentence.split()\n            else:\n                tokens = sentence\n            for word in tokens:\n                try:\n                    temp.append(self.vocab[word])\n                except KeyError:\n                    temp.append(self.vocab['<UNKNOWN>'])\n            features.append(temp)\n        return features\n    \n    def pad_sequences(self, corpus, labels):\n        #Corpus\n        maximum = 0\n        padded_corpus = []\n        temp = corpus\n        for sentence in temp:\n            if len(sentence) > maximum:\n                maximum = len(sentence)\n        if maximum % 2 != 0:\n            maximum += 1 #Make it Even\n        \n        for sentence in temp:\n            while len(sentence) < maximum:\n                sentence.insert(0, 0)\n            padded_corpus.append(np.array(sentence))\n            \n        #Labels\n        \n        padded_labels = []\n        temp = labels\n        for line in labels:\n            maximum = 0\n            temp_items = []\n            for item in line:\n                if len(item) > maximum:\n                    maximum = len(item)\n            for item in line:\n                while len(item) < maximum:\n                    item.insert(0, 0)\n                temp_items.append(np.array(item))\n            padded_labels.append(np.array(temp_items))\n        \n        return np.array(padded_corpus), np.array(padded_labels)\n        \n    def build_embedding_matrix(self, vocab_list):\n        glove = pd.read_table('..\/input\/glove6b200d\/glove.6B.200d.txt', \\\n                sep=' ', index_col=0, header=None, quoting=csv.QUOTE_NONE)\n\n        embedding_matrix = {0 : np.zeros(self.emb_dim)}\n        start = time.time()\n        for i, word in enumerate(vocab_list):\n            try:\n                embedding_matrix[i+1] = glove.loc[word].values\n            except KeyError:\n                embedding_matrix[i+1] = np.random.normal(scale=0.6, size=(self.emb_dim))\n            if i % 1000 == 0 and i != 0:\n                print('{}\/{} done in {}s'.format(i, len(vocab_list), time.time()-start))\n                start = time.time()\n        return embedding_matrix","52695706":"class RNN(nn.Module):\n    def __init__(self, name, device, emb_weights, seq_len, V, vocab_size, emb_dim, filter_size):\n        super(RNN, self).__init__()\n        self.name = name\n        self.device = device\n        self.conv_layers = filter_size\n        self.num_layers = 1\n\n        self.emb, self.emb_dim = self.create_embedding_layer(emb_weights, vocab_size, emb_dim, non_trainable=False)\n        self.V = V\n        \n        self.gru = nn.GRU(self.emb_dim, hidden_size=filter_size, num_layers=self.num_layers, \\\n                          bidirectional=True, dropout=0.5, batch_first=True) #Dropout for if num_layers > 1\n        \n        self.dense_1 = nn.Linear(filter_size*2, emb_dim)\n        self.dense_2 = nn.Linear(filter_size*2, emb_dim)\n        \n        self.bn_1 = nn.BatchNorm1d(int(seq_len))\n        self.bn_2 = nn.BatchNorm1d(V.size(0))\n        \n        self.dropout = nn.Dropout(0.4)\n\n    def forward(self, x):\n        \n        #########\n        ### V ###\n        #########\n        \n        # Create V in (L, E)\n        V_len = self.V.size(1)\n        true_len = torch.sum((self.V != 0), 1).float().to(self.device)\n        \n        V_emb = self.emb(self.V)\n        \n        V = torch.sum(V_emb, 1)\n        \n    \n        #########\n        ### X ###\n        #########\n        \n        #Embedding Layer (B x S) --> (B x S x E)\n        x = self.emb(x)\n        \n        ## D ##        \n        #GRU Layer (B x S x E) --> (B x S x F)\n        d, _ = self.gru(x)\n        \n        ## D2 ##\n        #Dense Layer (B x S, F) --> (B x S x E)\n        d2 = F.relu(self.bn_1(self.dense_1(d)))\n        d2 = self.dropout(d2)\n        \n        ## a ##\n        #Matrix Multiplication (B x S x E) matmul inv(L, E) --> (B x L x S)\n        for i, Vi in enumerate(V):\n            if i == 0:\n                a = torch.tanh(torch.matmul(d2, Vi)) # a (B, S)\n                a = a.view(a.size(0), 1, a.size(1)) # a (B, 1, S)\n            else:\n                b = torch.tanh(torch.matmul(d2, Vi)) # b (B, S)\n                b = b.view(b.size(0), 1, b.size(1)) # b (B, 1, S)\n                a = torch.cat([a, b], 1) #a (B, L, S)\n                \n        ## c ##\n        #Matrix Multiplication (B x L x S) matmul (B x S x F) --> (B x L x F)\n        c = F.relu(torch.matmul(a, d))\n        \n        ## e ##\n        #Dense Layer (B x L x F) --> (B x L x E)\n        e = torch.tanh(self.bn_2(self.dense_2(c)))\n        e = self.dropout(e)\n        \n        ## Output ##\n        #Cosine Similarity (B x L x E) vs. (1 x L x E) --> (B x L)\n        V_comp  = V.view(1, V.size(0), V.size(1))        \n        y_hat = F.cosine_similarity(e, V_comp, dim=2) * 10\n            \n        return y_hat\n    \n    \n    def create_embedding_layer(self, emb_weights, num_embedding, embedding_dim, non_trainable=False):\n        emb_layer = nn.Embedding(num_embedding, embedding_dim)\n        emb_layer.load_state_dict({'weight': emb_weights})\n        if non_trainable:\n            emb_layer.weight.requires_grad = False\n\n        return emb_layer, embedding_dim ","70205839":"class Algorithm:\n    def __init__(self, pipeline, filter_size, split):\n        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n        self.pipeline = pipeline\n        self.cats = pipeline.num_classes\n        self.seq_length = self.pipeline.padded_corpus.shape[1]\n        self.learning_rate = 0.01\n        \n        #Instantiate Models and Criterions\n        self.weights_matrix = np.array([ray for ray in pipeline.embedding_matrix.values()])\n        self.weights_matrix = torch.tensor(self.weights_matrix, dtype=torch.float, device=self.device)\n        \n        self.V = [torch.tensor(pipeline.padded_labels[i], dtype=torch.long, device=self.device) \\\n                  for i in range(len(pipeline.num_classes))]\n\n        self.models = [RNN(pipeline.attributes[i], self.device, self.weights_matrix, pipeline.seq_len, self.V[i], \\\n                          pipeline.vocab_size, pipeline.emb_dim, filter_size\\\n                          ).to(self.device) \\\n                       for i in range(len(pipeline.num_classes))]\n\n        self.criterions = [nn.CrossEntropyLoss() for i in range(len(self.cats))]\n        self.init_optimizers(self.learning_rate)\n    \n        #Load Data\n        self.text = torch.tensor(pipeline.padded_corpus, dtype=torch.long, device=self.device)\n\n        self.Y = [torch.tensor(pipeline.training_labels[i], dtype=torch.long, device=self.device) \\\n                  for i in range(len(pipeline.num_classes))]\n\n        #Shuffle Data\n        np.random.seed(369)\n        s = np.random.shuffle(np.arange(len(self.text)))\n        self.text = self.text[s]\n        self.text = self.text.view(self.text.size(1), self.text.size(2))\n        self.Y = [y[s].view(-1) for y in self.Y]\n        print(self.text.size())\n        print(self.Y[0].size())\n        \n        \n        #Split Data\n        self.split = int(split*self.text.size(0))\n\n        self.text_train = self.text[self.split:int(0.5*self.text.size(0))] #ONLY USE HALF FOR DEMO\n        self.text_val = self.text[:self.split]\n\n        self.Y_train = [y[self.split:int(0.5*self.text.size(0))] for y in self.Y] #ONLY USE HALF FOR DEMO\n        self.Y_val = [y[:self.split] for y in self.Y]\n    \n    def init_optimizers(self, learning_rate):\n        self.optimizers = [optim.SGD(self.models[i].parameters(), lr=learning_rate, momentum=0.9) \\\n                          for i in range(len(self.cats))]\n        \n    def set_learning_rate(self, learning_rate, which=None):\n        if which == None:\n            for optimizer in self.optimizers:\n                for group in optimizer.param_groups:\n                    group['lr'] = learning_rate\n                    print(\"Learning rate lowered\")\n        else:\n            try:\n                for group in self.optimizers[which].param_groups:\n                    group['lr'] = learning_rate\n                    print(\"Learning rate lowered\")\n            except IndexError:\n                print('When specifying which, specify integer within range of number of models.')\n                    \n    def not_equal(self, x, y):\n        return x != y\n    \n    def masked_loss(self, loss_func, output, true_y):\n        mask = self.not_equal(true_y, -1)\n        try:\n            return loss_func(output[mask], true_y[mask])\n        except:\n            return loss_func(torch.tensor([[0., 15.]]), torch.tensor([1]))\n                    \n            \n    def Train(self, index, epochs, batch_size, verbose=1):\n        categories = self.cats[index]\n        list_cat = [i for i in range(categories)]\n        list_cat.append('NaN')\n        count = 0\n        best_acc = 0\n        best_loss = 10\n        Y_train = self.Y_train[index]\n        Y_val = self.Y_val[index]\n        for epoch in range(epochs):\n            self.models[index].train()\n            et = time.time() #Epoch Time\n            bt = time.time() #Batch Time\n            #Train Step\n            train_iters = (self.text_train.size(0) \/\/ batch_size) + 1\n            running_loss = 0.\n            print(\"\\nEpoch {}\/{} for {}\".format(epoch+1, epochs, self.models[index].name))\n            for batch in range(train_iters):\n                text = self.text_train[batch*batch_size : min((batch+1)*batch_size, self.text_train.size(0))]\n                y = Y_train[batch*batch_size : min((batch+1)*batch_size, Y_train.size(0))]\n\n                #zero the parameter gradients\n                self.optimizers[index].zero_grad()\n                \n                #forward + backward + optimizer(update gradients)\n                output = self.models[index](text)\n                    \n                loss = self.masked_loss(self.criterions[index], output, y)\n\n                loss.backward()\n                self.optimizers[index].step()\n\n                running_loss += loss\n\n                if batch % 500 == 0 and batch != 0:\n                    print(\"[Epoch %d: Batch%5d\/%s] Loss: %.6f - Time Taken: %.2fs\" %(epoch+1, batch, train_iters, running_loss\/batch, time.time()-bt))\n                    bt = time.time()\n                    \n            #Validation Step\n            val_iters = (self.text_val.size(0) \/\/ batch_size) + 1\n            class_correct = list(0. for i in range(self.pipeline.num_classes[index])) \n            class_correct_2 = list(0. for i in range(self.pipeline.num_classes[index]))\n            class_total = list(0. for i in range(self.pipeline.num_classes[index]))\n            class_predicted = list(0. for i in range(self.pipeline.num_classes[index]))\n            class_predicted_2 = list(0. for i in range(self.pipeline.num_classes[index]))\n            correct = 0\n            correct_2 = 0\n            total = 0\n            val_loss = 0.0\n            true = []\n            pred = []\n            self.models[index].eval()\n            with torch.no_grad():\n                for batch in range(val_iters):\n                    text = self.text_val[batch*batch_size : min((batch+1)*batch_size, self.text_val.size(0))]\n                    y = Y_val[batch*batch_size : min((batch+1)*batch_size, Y_val.size(0))]\n\n                    output = self.models[index](text)\n\n                    loss = self.masked_loss(self.criterions[index], output, y)\n\n                    val_loss += loss\n                    _, predicted = torch.max(output, 1)\n                    \n                    _, summary = torch.max(output.data, 1)\n                    _, top_2 = torch.topk(output.data, 2)\n                    \n                    second = torch.tensor([pair[1] for pair in top_2], dtype=torch.long, device=self.device)\n                    \n                    c = (predicted == y).squeeze()\n                    c_2 = (second == y).squeeze()\n                    \n                    \n                    for i in range(len(y)):\n                        label = y[i]\n                        class_correct[label] += c[i].item()\n                        class_correct_2[label] += c[i].item()\n                        class_correct_2[label] += c_2[i].item()\n                        class_total[label] += 1\n                    \n                    \n                    for p in summary:\n                        class_predicted[p] += 1 \n                    for p in second:\n                        class_predicted_2[p] += 1\n              \n                    total += y.size(0)\n                    correct += (summary == y).sum().item()\n                    correct_2 += (summary == y).sum().item()\n                    correct_2 += (second == y).sum().item()\n                    \n                    for t in y:\n                        true.append(t.item())\n\n                    for p in predicted:\n                        pred.append(p.item())\n\n            if verbose:\n                for i in range(categories):\n                    try:\n                        accuracy = 100 * class_correct[i]\/class_total[i]\n                    except ZeroDivisionError:\n                        accuracy = 0\n                    print(\"Accuracy of %5s : %4d%% [Top 1 -(%4d\/%4d)] | [Top 2 - (%4d\/%4d)] | %4d, %4d\" % \\\n                          (list_cat[i], accuracy, class_correct[i], class_total[i], class_correct_2[i], class_total[i], \\\n                           class_predicted[i], class_predicted_2[i]))\n            \n            last_correct = class_correct[i]\n            last_total = class_total[i]\n                    \n            acc = 100 * correct\/total\n            true_acc = 100 * (correct-last_correct)\/ (total-last_total)\n            top_2_acc = 100 * (correct_2-last_correct)\/(total-last_total)\n            \n            \n            print(\"\\nValidation Accuracy: %.2f%%, Validation Loss: %.6f\" % (acc, val_loss\/batch))\n            print(\"True Accuracy: %.2f%%\" % (true_acc))\n            print(\"Top 2 Accuracy: %.2f%%\" % (top_2_acc))\n                \n            if true_acc < best_acc:\n                count += 1\n            else:\n                count = 0\n    \n            if count > 0:\n                for group in self.optimizers[index].param_groups:\n                    lr = group['lr']\n                self.set_learning_rate(lr\/2, index)\n                \n            print(\"Epoch Time: %.2fs\" % (time.time()-et))\n\n        print(\"Finished Training\")\n        \n    def load_state(self, index, path):\n        state = torch.load('.\/{}\/{}\/{}'.format(path, self.pipeline.name, self.pipeline.attributes[index]))\n        self.models[index].load_state_dict(state)\n        self.models[index].to(self.device)","c628874a":"epochs = 1\nbatch_size = 64","c96e87e9":"beauty_data = pd.read_csv('..\/input\/ndsc-advanced\/beauty_data_info_train_competition.csv', index_col=0)\n\nwith open('..\/input\/ndsc-advanced\/beauty_profile_train.json') as f:\n    beauty_profiles = json.load(f)\n    \nbeauty = Pipeline('beauty', beauty_data, beauty_profiles, 200)","08ad0243":"# beauty_algo = Algorithm(beauty, filter_size=128, split=0.05)","c830942c":"# beauty_algo.Train(0, epochs, batch_size)","9316bed0":"# beauty_algo.Train(1, epochs, batch_size)","449825fd":"# beauty_algo.Train(2, epochs, batch_size)","adff51a9":"# beauty_algo.Train(3, epochs, batch_size)","4942da0a":"# beauty_algo.Train(4, epochs, batch_size)","077b0146":"fashion_data = pd.read_csv('..\/input\/ndsc-advanced\/fashion_data_info_train_competition.csv', index_col=0)\n\nwith open('..\/input\/ndsc-advanced\/fashion_profile_train.json') as f:\n    fashion_profiles = json.load(f)\n    \nfashion = Pipeline('fashion', fashion_data, fashion_profiles, 200)","f0af59d2":"# fashion_algo = Algorithm(fashion, filter_size=128, split=0.05)","5dfcf04c":"# fashion_algo.Train(0, epochs, batch_size)","a46b17d0":"# fashion_algo.Train(1, epochs, batch_size)","4dd9167b":"# fashion_algo.Train(2, epochs, batch_size)","e17ef9a2":"# fashion_algo.Train(3, epochs, batch_size)","6d2c6803":"# fashion_algo.Train(4, epochs, batch_size)","b4c09efb":"mobile_data = pd.read_csv('..\/input\/ndsc-advanced\/mobile_data_info_train_competition.csv', index_col=0)\n\nwith open('..\/input\/ndsc-advanced\/mobile_profile_train.json') as f:\n    mobile_profiles = json.load(f)\n    \nmobile = Pipeline('mobile', mobile_data, mobile_profiles, 200)","bdf66cb0":"# mobile_algo = Algorithm(mobile, filter_size=128, split=0.05)","ce0d3549":"# mobile_algo.Train(0, epochs, batch_size)","0fd656f8":"# mobile_algo.Train(1, epochs, batch_size)","f0ac372e":"# mobile_algo.Train(2, epochs, batch_size)","4833b0ff":"# mobile_algo.Train(3, epochs, batch_size)","d0a15fb6":"# mobile_algo.Train(4, epochs, batch_size)","dc24b36d":"# mobile_algo.Train(5, epochs, batch_size)","14433d30":"# mobile_algo.Train(6, epochs, batch_size)","02d66987":"# mobile_algo.Train(7, epochs, batch_size)","420f030a":"# mobile_algo.Train(8, epochs, batch_size)","4f9b2729":"# mobile_algo.Train(9, epochs, batch_size)","208e495c":"# mobile_algo.Train(10, epochs, batch_size)","27832e91":"class TestPipe:\n    def __init__(self, name, data, vocab, profiles, emb_dim, seq_len):\n        self.name = name\n        self.data = data\n        self.vocab = vocab\n        self.vocab_size = len(vocab)+1\n        self.profiles = profiles\n        self.emb_dim = emb_dim\n        self.seq_len = seq_len\n        \n        self.attributes = list(self.profiles.keys())\n        self.attributes = list(self.profiles.keys())\n        self.num_classes = [len(self.profiles[key])+1 for key in self.attributes]\n        \n        self.corpus = self.data['title']\n        self.int_corpus, self.unknown_words = self.integerize_corpus(self.corpus, self.vocab)\n        \n        self.labels = self.build_labels(profiles)\n        self.int_labels = self.process_labels(self.labels, self.vocab)\n        \n        self.padded_corpus, self.padded_labels = self.pad_sequences(self.int_corpus, self.int_labels)\n        self.seq_len = len(self.int_corpus[0])\n        \n        \n    def build_labels(self, profiles):    \n        att = profiles.keys()\n        labels = []\n        for key in att:\n            temp_dict = profiles[key]\n            temp_labels = [k for k in sorted(temp_dict, key=temp_dict.get, reverse=False)]\n            labels.append(temp_labels)\n            \n#             labels.append(list(profiles[key].keys()))\n        return labels\n            \n    def process_labels(self, labels, vocab):\n        int_labels = []\n        count = len(vocab) + 1\n        for line in labels:\n            temp_labels = []\n            for string in line:\n                try:\n                    splitted = string.split(\" \")\n                    temp = []\n                    for word in splitted:\n                        temp.append(vocab[word])\n                    temp_labels.append(temp)\n                except:\n                    temp_labels.append([vocab[string]])\n            int_labels.append(temp_labels)\n        return int_labels\n     \n    def integerize_corpus(self, corpus, vocab):\n        features = []\n        unknown_words = []\n        for sentence in corpus:\n            temp = []\n            tokens = sentence.split()\n            for word in tokens:\n                try:\n                    temp.append(vocab[word])\n                except KeyError:\n                    temp.append(vocab['<UNKNOWN>'])\n                    unknown_words.append(word)\n            features.append(temp)\n        return features, unknown_words\n    \n    def pad_sequences(self, corpus, labels):\n        #Corpus\n        padded_corpus = []\n        temp = corpus\n\n        for sentence in temp:\n            while len(sentence) < self.seq_len:\n                sentence.insert(0, 0)\n            \n            padded_corpus.append(np.array(sentence))\n        #Labels\n        maximum = 0\n        padded_labels = []\n        temp = labels\n        for line in labels:\n            temp_items = []\n            for item in line:\n                if len(item) > maximum:\n                    maximum = len(item)\n            for item in line:\n                while len(item) < maximum:\n                    item.insert(0, 0)\n                temp_items.append(np.array(item))\n            padded_labels.append(np.array(temp_items))\n        \n        return np.array(padded_corpus), np.array(padded_labels)","ae988aee":"class RNN(nn.Module):\n    def __init__(self, name, device, seq_len, V, vocab_size, emb_dim, filter_size):\n        super(RNN, self).__init__()\n        self.name = name\n        self.device = device\n        self.conv_layers = filter_size\n        self.num_layers = 1\n\n        self.emb, self.emb_dim = self.create_embedding_layer(vocab_size, emb_dim, non_trainable=False)\n        self.V = V\n        \n        self.gru = nn.GRU(self.emb_dim, hidden_size=filter_size, num_layers=self.num_layers, bidirectional=True, dropout=0.5, batch_first=True)\n        \n        self.dense_1 = nn.Linear(filter_size*2, emb_dim)\n        self.dense_2 = nn.Linear(filter_size*2, emb_dim)\n        \n        self.bn_1 = nn.BatchNorm1d(int(seq_len))\n        self.bn_2 = nn.BatchNorm1d(V.size(0))\n        \n        self.dropout = nn.Dropout(0.4)\n\n    def forward(self, x):\n        \n        #########\n        ### V ###\n        #########\n        \n        # Create V in (L, E)\n        V_len = self.V.size(1)\n        true_len = torch.sum((self.V != 0), 1).float().to(self.device)\n        \n        V_emb = self.emb(self.V)\n        \n        V = torch.sum(V_emb, 1)\n        \n    \n        #########\n        ### X ###\n        #########\n        \n        #Embedding Layer (B x S) --> (B x S x E)\n        x = self.emb(x)\n        \n        ## D ##        \n        #GRU Layer (B x S x E) --> (B x S x F)\n        d, _ = self.gru(x)\n        \n        ## D2 ##\n        #Dense Layer (B x S, F) --> (B x S x E)\n        d2 = F.relu(self.bn_1(self.dense_1(d)))\n        \n        ## a ##\n        #Matrix Multiplication (B x S x E) matmul inv(L, E) --> (B x L x S)\n        for i, Vi in enumerate(V):\n            if i == 0:\n                a = torch.tanh(torch.matmul(d2, Vi)) # a (B, S)\n                a = a.view(a.size(0), 1, a.size(1)) # a (B, 1, S)\n            else:\n                b = torch.tanh(torch.matmul(d2, Vi)) # b (B, S)\n                b = b.view(b.size(0), 1, b.size(1)) # b (B, 1, S)\n                a = torch.cat([a, b], 1) #a (B, L, S)\n                \n        ## c ##\n        #Matrix Multiplication (B x L x S) matmul (B x S x F) --> (B x L x F)\n        c = F.relu(torch.matmul(a, d))\n        \n        ## e ##\n        #Dense Layer (B x L x F) --> (B x L x E)\n        e = torch.tanh(self.bn_2(self.dense_2(c)))\n        \n        ## Output ##\n        #Cosine Similarity (B x L x E) vs. (1 x L x E) --> (B x L)\n        V_comp  = V.view(1, V.size(0), V.size(1))\n        y_hat = F.cosine_similarity(e, V_comp, dim=2) * 10\n        \n        return y_hat\n    \n    \n    def create_embedding_layer(self, num_embedding, embedding_dim, non_trainable=False):\n        emb_layer = nn.Embedding(num_embedding, embedding_dim)\n        return emb_layer, embedding_dim ","dc36dee2":"class Predictor:\n    def __init__(self, pipeline, seq_len):\n        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\n        self.pipeline = pipeline\n        self.cats = pipeline.attributes\n        \n        self.V = [torch.tensor(pipeline.padded_labels[i], dtype=torch.long, device=self.device) \\\n                  for i in range(len(pipeline.num_classes))]\n        \n        #Instantiate Models      \n        self.models = [RNN(pipeline.attributes[i], self.device, pipeline.seq_len, self.V[i], \\\n                          pipeline.vocab_size, pipeline.emb_dim, 128\\\n                          ).to(self.device) \\\n                       for i in range(len(self.cats))]\n        \n        #Load State Dicts\n        states = {}\n        for attribute in self.pipeline.attributes:\n            states[attribute] = torch.load('..\/input\/{}-weights\/{}'.format(self.pipeline.name, attribute))\n        for model in self.models:\n            model.load_state_dict(states[model.name])\n        \n        #Load Data\n        self.X = torch.tensor(pipeline.padded_corpus, dtype=torch.long, device=self.device)\n        \n    def predict(self, index, batch_size):\n        predict_iters = self.X.size(0)\/\/ batch_size + 1\n        \n        prediction = []\n        first = []\n        second = []\n        third = []\n        st = time.time()\n        tt = time.time()\n\n        self.models[index].eval()\n\n        with torch.no_grad():\n            for batch in range(predict_iters):\n                x = self.X[batch*batch_size : min((batch+1)*batch_size, self.X.size(0))]\n                \n                outputs = self.models[index](x)\n                \n                _, predicted = torch.max(outputs, 1)\n                _, top_2 = torch.topk(outputs, 2)\n                        \n                predicted = predicted.to('cpu').numpy()\n\n                top1 = np.array([pair[0].item() for pair in top_2])\n                top2 = np.array([pair[1].item() for pair in top_2])\n                    \n                \n                for pred in predicted:\n                    prediction.append(pred)\n                \n                for item in top1:\n                    first.append(item)\n                    \n                for item in top2:\n                    second.append(item)\n\n                if batch % 1000 == 0 and batch != 0:\n                    print(\"Prediction Done for {} items.\".format(batch*batch_size))\n                    print(\"Done in %.2fs\" % (time.time() - st))\n                    st = time.time()\n\n        print(\"Total Time Taken: %.2fs\" % (time.time()-tt))\n\n        prediction = np.array(prediction)\n        print(prediction)\n        print(prediction.shape)\n\n        return prediction, first, second","8ea6e4d2":"def export(predictor, batch_size=512):\n    results = []\n    results_first = []\n    results_second = []\n    results_third = []\n    for i in range(len(predictor.models)):\n        out, first, second = predictor.predict(i, batch_size)\n        results.append(np.array([out]))\n        results_first.append(np.array([first]))\n        results_second.append(np.array([second]))\n        \n    together = np.concatenate([result for result in results], axis=0)\n    together_first = np.concatenate([result for result in results_first], axis=0)\n    together_second = np.concatenate([result for result in results_second], axis=0)\n    \n    transposed = np.transpose(together, [1, 0])\n    transposed_first = np.transpose(together_first, [1, 0])\n    transposed_second = np.transpose(together_second, [1, 0])\n    \n    output = []\n    output_first = []\n    output_second = []\n    \n    for ray in transposed:\n        for item in ray:\n            output.append(item)\n            \n    for ray in transposed_first:\n        for item in ray:\n            output_first.append(item)\n            \n    for ray in transposed_second:\n        for item in ray:\n            output_second.append(item)\n    \n    df = pd.DataFrame()\n    \n    ids = []\n    category = []\n    attributes = predictor.pipeline.attributes\n    for identity in predictor.pipeline.data.index:\n        for attribute in attributes:\n            ids.append(\"{}_{}\".format(identity, attribute))\n            category.append(attribute)\n                            \n    print(\"ids:\", len(ids))\n    print(\"outputs:\", len(output))\n            \n    df['id'] = ids\n    \n    df['tagging'] = output\n    df['category'] = category\n    df['first_1'] = output_first\n    df['second_2'] = output_second\n            \n    return df","01fddbca":"beauty_test = pd.read_csv('..\/input\/ndsc-advanced\/beauty_data_info_val_competition.csv', index_col = 0)\nfashion_test = pd.read_csv('..\/input\/ndsc-advanced\/fashion_data_info_val_competition.csv', index_col = 0)\nmobile_test = pd.read_csv('..\/input\/ndsc-advanced\/mobile_data_info_val_competition.csv', index_col = 0)","429e7cf4":"seq_lens = [beauty.seq_len, fashion.seq_len, mobile.seq_len]","25270863":"beauty = TestPipe('beauty', beauty_test, beauty.vocab, beauty_profiles, 200, seq_lens[0])\nfashion = TestPipe('fashion', fashion_test, fashion.vocab, fashion_profiles, 200, seq_lens[1])\nmobile = TestPipe('mobile', mobile_test, mobile.vocab, mobile_profiles, 200, seq_lens[2])","7ccb57cf":"beauty_pred = Predictor(beauty, seq_lens[0])\nfashion_pred = Predictor(fashion, seq_lens[1])\nmobile_pred = Predictor(mobile, seq_lens[2])","d580c248":"beauty_output = export(beauty_pred, 512)","f6a6a1ed":"fashion_output = export(fashion_pred, 512)","e0390919":"mobile_output = export(mobile_pred, 512)","487f11c8":"output = pd.concat([beauty_output, fashion_output, mobile_output], axis=0)\noutput = output.drop(['tagging', 'category'], axis=1)\noutput[\"tagging\"] = output[\"first_1\"].map(str) + \" \" + output[\"second_2\"].map(str)\noutput = output.drop(['first_1', 'second_2'], axis=1)","3d3e6b7a":"len(output)","3a7e8883":"output","54997ac4":"output.to_csv('output.csv', index=False)","827aa6cd":"**Beauty**","3fd63d1c":"Model for Prediction","859722fa":"**Model**","72d8f596":"**Code**","f9929ff7":"**Predictor**"}}