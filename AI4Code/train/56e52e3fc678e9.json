{"cell_type":{"ad9b1484":"code","0f5e9002":"code","841309f4":"code","d270af3a":"code","fae93ad1":"code","1b0dfdc1":"code","d149b526":"code","abe2f32a":"code","ff416d9a":"code","a2e11f1d":"code","58f14e96":"code","2d542959":"code","6f120713":"code","c5cf45c5":"code","d6d489f1":"code","006f0a3b":"code","b2ec2278":"code","e0bb2252":"code","9e4f24a8":"code","f0aba98b":"code","5238dd33":"code","b271b39d":"code","9328eef6":"code","d9be99c6":"code","b6605da3":"code","1813a421":"code","4be0fe11":"code","f4693abb":"code","af04fe34":"code","90fe2170":"code","9fb0a59e":"code","b6f768ca":"code","cc02d3c4":"code","3384039c":"code","962b53ca":"code","ea4d0384":"code","80d91cad":"code","038b3ccd":"code","fc534a01":"code","c6f12e31":"code","50016949":"code","7fdbd003":"code","d9cf8485":"code","48d787a9":"code","e82bb3c1":"code","76fe27c3":"code","07e4539e":"code","07e8a57f":"code","a07883ff":"code","058240a6":"code","966417e0":"code","42072f86":"code","385b08de":"code","a0acfdbe":"code","72a41657":"code","10823c44":"code","2be98116":"code","d9f96646":"code","a1c3efe5":"code","3676672a":"code","2d00a38b":"code","1731efb1":"code","9f65a069":"code","14eb0b9f":"code","ca0c9d51":"code","958799e2":"markdown","1bccde2b":"markdown","6a53fa6a":"markdown","2ca6dbcf":"markdown"},"source":{"ad9b1484":"import pandas as pd\nfrom pathlib import Path\n\nimport optuna\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\ndata_dir = Path('..\/input\/tabular-playground-series-nov-2021\/')\n\ndf_train = pd.read_csv(\n    data_dir \/ \"train.csv\",\n    index_col='id',\n    #nrows=540000,# comment this row to use the full dataset\n)\n\ndf_train.head(20)","0f5e9002":"df_train.shape","841309f4":"df_train.info()","d270af3a":"# Downcasting the traind dataset.\nfor col in df_train.columns:\n    \n    if df_train[col].dtype == \"float64\":\n        df_train[col] = pd.to_numeric(df_train[col], downcast=\"float\")\n        \n    if df_train[col].dtype == \"int64\":\n        df_train[col] = pd.to_numeric(df_train[col], downcast=\"integer\")","fae93ad1":"# Heatmap to View Missing Values by Variable\nplt.figure(figsize = (14,6))\np = sns.heatmap(df_train.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')\np.axes.set_title(\"Valores Ausentes\", fontsize = 20)","1b0dfdc1":"# Check the unique values\n{df_train[col].nunique():col for col in df_train.columns if df_train[col].nunique() > 0}","d149b526":"df_train.describe().T.style.background_gradient(cmap='Blues')","abe2f32a":"corr = df_train.corr()\ncorr[['target']].sort_values(by = 'target',ascending = False).style.background_gradient()","ff416d9a":"# View Dataset Class Distribution\n\nsns.set(style=\"whitegrid\")\n\n# Using a bar chart to show the distribution of classes\nbp = sns.countplot(x=df_train['target'])\nplt.title(\"Dataset Class Distribution\")\nbp.set_xticklabels([\"0\",\"1\"])\nplt.show()","a2e11f1d":"# function for applying recursive scaling\ndef recursive_scaler(x, n_scaler, scaler):\n    for e in range(1,n_scaler):\n        x = scaler.transform(x)\n    return x","58f14e96":"# Drop Cols\ndropcols = ['f2','f35','target']","2d542959":"# Features and target\nFEATURES = df_train.drop(dropcols, axis = 1)\nTARGET = df_train['target'].astype(int).astype(str)","6f120713":"features = FEATURES.columns","c5cf45c5":"from sklearn.model_selection import train_test_split","d6d489f1":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(FEATURES, TARGET, \n                                                      train_size=0.67, test_size=0.33, random_state=42, shuffle=True)","006f0a3b":"from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\nimport gc","b2ec2278":"import os\nimport random\nimport numpy as np\nfrom sklearn.model_selection import RepeatedKFold\nfrom optuna import create_study\nfrom optuna.samplers import TPESampler\nfrom optuna.integration import XGBoostPruningCallback\nfrom catboost import Pool\nfrom sklearn.utils import resample\nimport multiprocessing\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, MaxAbsScaler","e0bb2252":"#%%time\n#n_trials = int(9)\n#SEED = 143","9e4f24a8":"#%%time\n#Function to seed everything\n#def seed_everything(seed):\n#    random.seed(seed)\n#    np.random.seed(seed)\n#    os.environ['PYTHONHASHSEED'] = str(seed)\n#seed_everything(SEED)","f0aba98b":"#n_scaler = 4\n\n#X = np.array(X_train)\n#y = np.array(y_train)\n\n#scaler = MinMaxScaler()\n#scaler.fit(X)\n#X = scaler.transform(X)\n\n#X  = recursive_scaler(X, n_scaler, scaler)\n\n#X = pd.DataFrame(X, columns = features)\n\n#X['sum']  = X[features].sum(axis=1)\n#X['mean'] = X[features].mean(axis=1)\n#X['std']  = X[features].std(axis=1)\n#X['max']  = X[features].max(axis=1)\n#X['min']  = X[features].min(axis=1)\n#X['kurt'] = X[features].kurtosis(axis=1)\n\n#X = np.array(X)\n","5238dd33":"#def objective(trial):\n    # Parameters\n#    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.33, random_state=int(SEED), shuffle=True)\n#    train_pool = Pool(train_x, train_y)\n#    test_pool = Pool(test_x, test_y)    \n#    params = {'iterations' : trial.suggest_int('iterations', 50, 1600),                         \n#            'depth' : trial.suggest_int('depth', 2, 10),                                       \n#            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n#            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n#            'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n#            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter'])\n#        }\n    # Learning\n#    model = CatBoostClassifier(\n#            loss_function=\"Logloss\",\n#            eval_metric=\"AUC\",\n#            task_type=\"GPU\",\n#            l2_leaf_reg=0.2,\n#            random_seed=SEED,\n#            border_count=64,\n#            **params\n#    )        \n#    model.fit(train_pool)\n    # Predict\n#    preds = model.predict_proba(test_x)\n   # Evaluation\n#    ROC_AUC_Score = roc_auc_score(test_y, preds[:, 1])\n#    print('ROC AUC Score of CatBoost =', ROC_AUC_Score)\n#    return ROC_AUC_Score","b271b39d":"#%%time\n#study = optuna.create_study(direction = \"maximize\", sampler = TPESampler(seed=int(SEED)))\n#study.optimize(objective, n_trials = n_trials)","9328eef6":"# This is nice handy constant to turn on and off the GPU. When `False`\n# the notebook will ignore the GPU even when present.\n#GPU_ENABLED = True","d9be99c6":"#X = np.array(X_train)\n#y = np.array(y_train)\n\n#scaler = QuantileTransformer()\n#scaler.fit(X)\n#X  = recursive_scaler(X, 2, scaler)\n#X = scaler.transform(X)\n\n#X = pd.DataFrame(X, columns = features)\n\n#X['sum']  = X[features].sum(axis=1)\n#X['mean'] = X[features].mean(axis=1)\n#X['std']  = X[features].std(axis=1)\n#X['max']  = X[features].max(axis=1)\n#X['min']  = X[features].min(axis=1)\n#X['kurt'] = X[features].kurtosis(axis=1)\n\n#X = np.array(X)","b6605da3":"#def train_model_for_study(X, y, model):\n#    X_train, X_valid, y_train, y_valid = train_test_split(\n#        X, \n#        y, \n#        test_size=0.33, \n#        random_state=42,\n#        shuffle=True\n#    )\n\n\n#    model.fit(\n#       X_train, \n#       y_train,\n     #  sample_weight=classes_weights,\n#        early_stopping_rounds=15,\n#        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n#        eval_metric=\"auc\",\n#        verbose=True\n#    )\n\n#    yhat = model.predict_proba(X_valid)\n#    score = roc_auc_score(y_valid, yhat[:, 1])\n#    return score ","1813a421":"#def objective_xgb(trial):\n#   \"\"\"\n#    Objective function to tune an `XGBRegressor` model.\n#    \"\"\"\n\n#    params = {\n#        'n_estimators': trial.suggest_int(\"n_estimators\", 1000, 10000),\n#        'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n#        'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n#        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.1),\n#        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 2.0, log=True),\n#        'max_depth': trial.suggest_int(\"max_depth\", 2, 9),\n#        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n#    }\n\n#    if GPU_ENABLED:\n#        params[\"tree_method\"] = \"gpu_hist\"\n#        params[\"predictor\"] = \"gpu_predictor\"\n\n#    model = XGBClassifier(\n#        booster=\"gbtree\",\n#        objective=\"binary:logistic\",\n#        n_jobs=-1, \n#        random_state=42,\n#        **params\n#    )\n\n#    return train_model_for_study(X, y, model)","4be0fe11":"#study_xgb = optuna.create_study(direction=\"maximize\")\n#study_xgb.optimize(objective_xgb, n_trials=9)\n#study_xgb.best_params","f4693abb":"#%%time\n#study = optuna.create_study()\n#study.optimize(objective, n_trials = 100)","af04fe34":"import pickle","90fe2170":"#%%time\n# Save\n#pickle.dump(study.best_trial.params, open('CatBoost_Hyperparameter.pickle', 'wb'))\n#print('CatBoost Hyperparameter:', study.best_trial.params)","9fb0a59e":"# CastBoost model configuration\nctb = CatBoostClassifier(iterations=1462, \n                        learning_rate = 0.08775145655977466, \n                        random_strength = 15, \n                        l2_leaf_reg = 0.2, \n                        depth = 3,\n                        bagging_temperature=1.0190751501900164,\n                        od_type='IncToDec',\n                        loss_function=\"Logloss\",\n                        eval_metric='AUC:type=Ranking',\n                        random_state=42)","b6f768ca":"# XGboost model configuration\nxgb = XGBClassifier(\n        learning_rate= 0.02003527792413422,\n        reg_alpha = 0.004448966694735556,\n        reg_lambda = 2.610959038520974,\n        max_depth=3,\n        subsample=0.8,\n        colsample_bytree=0.6183087610995104,\n        objective='binary:logistic',\n        n_estimators=4884,\n        eval_metric='auc',\n        n_jobs=-1,\n        tree_method='gpu_hist',\n        predictor = \"gpu_predictor\",\n        # Uncomment if you want to use GPU. Recommended for whole training set.\n        #tree_method='gpu_hist',\n        random_state=42,\n        )","cc02d3c4":"kf = StratifiedShuffleSplit(n_splits=9, test_size=0.33, random_state=42)\n#kf = StratifiedKFold(n_splits=6, shuffle=False)\n#cv_score = []\ncv_score = {}\ni=1\nn_scaler1 = 4\nn_scaler2 = 2\n\nX_1 = np.array(X_train)\nX_2 = np.array(X_train)\ny = np.array(y_train)\n\nscaler1 = MinMaxScaler()\nscaler1.fit(X_1)\n\nscaler2 = QuantileTransformer()\nscaler2.fit(X_2)\n\nX_1  = recursive_scaler(X_1, n_scaler1, scaler1)\nX_2  = recursive_scaler(X_2, n_scaler2, scaler2)\n#X_2  = scaler2.transform(X_2)\n\nX_1 = pd.DataFrame(X_1, columns = features)\n\nX_1['sum']  = X_1[features].sum(axis=1)\nX_1['mean'] = X_1[features].mean(axis=1)\nX_1['std']  = X_1[features].std(axis=1)\nX_1['max']  = X_1[features].max(axis=1)\nX_1['min']  = X_1[features].min(axis=1)\nX_1['kurt'] = X_1[features].kurtosis(axis=1)\n\nX_1 = np.array(X_1)\n\nX_2 = pd.DataFrame(X_2, columns = features)\n\nX_2['sum']  = X_2[features].sum(axis=1)\nX_2['mean'] = X_2[features].mean(axis=1)\nX_2['std']  = X_2[features].std(axis=1)\nX_2['max']  = X_2[features].max(axis=1)\nX_2['min']  = X_2[features].min(axis=1)\nX_2['kurt'] = X_2[features].kurtosis(axis=1)\n\nX_2 = np.array(X_2)\n\nX_valid_1 = recursive_scaler(X_valid, n_scaler1, scaler1)\nX_valid_2 = recursive_scaler(X_valid, n_scaler2, scaler2)\n#X_valid_2  = scaler2.transform(X_valid)\n\nX_valid_1 = pd.DataFrame(X_valid_1, columns = features)\nX_valid_2 = pd.DataFrame(X_valid_2, columns = features)\n\n\nX_valid_1['sum']  = X_valid_1[features].sum(axis=1)\nX_valid_1['mean'] = X_valid_1[features].mean(axis=1)\nX_valid_1['std']  = X_valid_1[features].std(axis=1)\nX_valid_1['max']  = X_valid_1[features].max(axis=1)\nX_valid_1['min']  = X_valid_1[features].min(axis=1)\nX_valid_1['kurt'] = X_valid_1[features].kurtosis(axis=1)\n\nX_valid_1 = np.array(X_valid_1)\n\nX_valid_2['sum']  = X_valid_2[features].sum(axis=1)\nX_valid_2['mean'] = X_valid_2[features].mean(axis=1)\nX_valid_2['std']  = X_valid_2[features].std(axis=1)\nX_valid_2['max']  = X_valid_2[features].max(axis=1)\nX_valid_2['min']  = X_valid_2[features].min(axis=1)\nX_valid_2['kurt'] = X_valid_2[features].kurtosis(axis=1)\n\nX_valid_2 = np.array(X_valid_2)\n\nfor train_index, test_index in kf.split(X_1, y):\n    print(train_index)  \n    print('{} of KFold {}'.format(i,kf.n_splits)) \n    xtr_1,xvl_1 = X_1[train_index],X_1[test_index]\n    xtr_2,xvl_2 = X_2[train_index],X_2[test_index]\n    ytr,yvl = y[train_index],y[test_index]\n\n    #model\n    eval_set_1 = [(xtr_1,ytr), (xvl_1,yvl)]\n    eval_set_2 = [(xtr_2,ytr), (xvl_2,yvl)]\n    \n    \n    ctb.fit(xtr_1,ytr, early_stopping_rounds=15, eval_set=eval_set_1, verbose=False)\n    xgb.fit(xtr_2,ytr, early_stopping_rounds=15, eval_set=eval_set_2, verbose=False)\n    score1 = roc_auc_score(np.array(y_valid),ctb.predict_proba(X_valid_1)[:, 1])\n    score2 = roc_auc_score(np.array(y_valid),xgb.predict_proba(X_valid_2)[:, 1])\n    print('ROC AUC score1:',score1)\n    print('ROC AUC score2:',score2)\n    \n    print(train_index.shape)\n    print(test_index.shape)\n    if score1 > score2:\n        #cv_score.append(score1) \n        cv_score.update({'ctb'+str(i): score1})\n        pickle.dump(ctb, open(\"model\"+str(i)+\".pickle.dat\", \"wb\"))\n    else:\n        #cv_score.append(score2)\n        cv_score.update({'xgb'+str(i): score2})\n        pickle.dump(xgb, open(\"model\"+str(i)+\".pickle.dat\", \"wb\"))\n    i+=1","3384039c":"# Check Scores\ncv_score","962b53ca":"# load model from file\nmdl1 = pickle.load(open(\"model1.pickle.dat\", \"rb\"))\nmdl2 = pickle.load(open(\"model2.pickle.dat\", \"rb\"))\nmdl3 = pickle.load(open(\"model3.pickle.dat\", \"rb\"))\nmdl4 = pickle.load(open(\"model4.pickle.dat\", \"rb\"))\nmdl5 = pickle.load(open(\"model5.pickle.dat\", \"rb\"))\nmdl6 = pickle.load(open(\"model6.pickle.dat\", \"rb\"))\nmdl7 = pickle.load(open(\"model7.pickle.dat\", \"rb\"))\nmdl8 = pickle.load(open(\"model8.pickle.dat\", \"rb\"))\nmdl9 = pickle.load(open(\"model9.pickle.dat\", \"rb\"))","ea4d0384":"#y_pred1 = ctb1.predict_proba(X_valid_1)\n#y_pred2 = ctb2.predict_proba(X_valid_1)\n#y_pred3 = ctb3.predict_proba(X_valid_1)\n#y_pred4 = ctb4.predict_proba(X_valid_1)\n#y_pred5 = ctb5.predict_proba(X_valid_1)\n#y_pred6 = ctb6.predict_proba(X_valid_1)\n#y_pred7 = ctb7.predict_proba(X_valid_1)\n#y_pred8 = ctb8.predict_proba(X_valid_1)\n#y_pred9 = ctb9.predict_proba(X_valid_1)","80d91cad":"#y_pred_1 = np.mean([y_pred1, y_pred2, y_pred3, y_pred4, y_pred5, y_pred6, y_pred7, y_pred8, y_pred9], axis=0)","038b3ccd":"modelapply = [ mdl1, mdl2, mdl3, mdl4, mdl5, mdl6, mdl7, mdl8, mdl9 ]","fc534a01":"modelapply2 = [[], []]\ni = 0\nfor item in cv_score.keys():\n    if 'ctb' in item:\n        modelapply2[0].append('X_valid_1')\n    else:\n        modelapply2[0].append('X_valid_2')\n    modelapply2[1].append(modelapply[i])\n    i = i + 1","c6f12e31":"X_valid_1.shape","50016949":"#def predict_subset(X_1, X_2, X1_name, ModelApply):\n#    y_pred = [[],[],[],[],[],[],[],[],[]]\n#    start = 0\n#    for i in range(0,9):\n#        print(ModelApply[0][i])\n#        model = ModelApply[1][i]\n\n#        if ModelApply[0][i] == 'X_valid_1':\n#            chunk_size = int(X_1.shape[0] \/ 9)\n#            subset = X_1[start:start + chunk_size]\n#            y_pred[i] = model.predict_proba(subset)\n#        else:\n#            chunk_size = int(X_2.shape[0] \/ 9)\n#            subset = X_2[start:start + chunk_size]\n#            y_pred[i] = model.predict_proba(subset)\n        #start = start + chunk_size\n#    return y_pred","7fdbd003":"def predict_model(X_1, X_2, X1_name, ModelApply):\n    y_pred = [[],[],[],[],[],[],[],[],[]]\n    for i in range(0,9):\n        print(ModelApply[0][i])\n        model = ModelApply[1][i]\n\n        if ModelApply[0][i] == 'X_valid_1':\n            y_pred[i] = model.predict_proba(X_1)\n        else:\n            y_pred[i] = model.predict_proba(X_2)\n    return y_pred","d9cf8485":"y_pred = predict_model(X_valid_1, X_valid_2, 'X_valid_1', modelapply2)","48d787a9":"y_pred_1 = np.mean([y_pred[0], y_pred[1], y_pred[2], y_pred[3], y_pred[4], y_pred[5], y_pred[6], y_pred[7], y_pred[8]], axis=0)","e82bb3c1":"y_pred_1","76fe27c3":"#flatlist=[element.tolist() for sublist in y_pred for element in sublist]","07e4539e":"#y_pred_1 = np.array(flatlist)","07e8a57f":"# retrieve just the probabilities for the positive class\npos_probs = y_pred_1[:, 1]\n# plot no skill roc curve\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n# calculate roc curve for model\nfpr, tpr, _ = roc_curve(y_valid.astype(str).astype(int), pos_probs)\n# plot model roc curve\nplt.plot(fpr, tpr, marker='.', label='Logistic')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","a07883ff":"precisions, recalls, thresholds = precision_recall_curve(y_valid.astype(str).astype(int), y_pred_1[:,1])\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.ylim([0, 1])\n    \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","058240a6":"import scikitplot as skplt\nskplt.metrics.plot_roc(y_valid.astype(str).astype(int), y_pred_1, figsize=(10, 8))","966417e0":"# reading test data\nX_test = pd.read_csv(data_dir \/ \"test.csv\", index_col='id')","42072f86":"# Downcasting the test dataset.\nfor col in X_test.columns:\n    \n    if X_test[col].dtype == \"float64\":\n        X_test[col] = pd.to_numeric(X_test[col], downcast=\"float\")\n        \n    if X_test[col].dtype == \"int64\":\n        X_test[col] = pd.to_numeric(X_test[col], downcast=\"integer\")","385b08de":"X_test.shape","a0acfdbe":"X_test.describe().T.style.background_gradient(cmap='Blues')","72a41657":"# get predictions\ndropcols = ['f2','f35']\nXt = X_test.drop(dropcols, axis = 1)\ncolumns = Xt.columns\n\nXt1  = recursive_scaler(Xt, n_scaler1, scaler1)\nXt2  = recursive_scaler(Xt, n_scaler2, scaler2)\n#Xt2  = scaler2.transform(Xt)\n\nXt1 = pd.DataFrame(Xt1, columns = features, )\nXt2 = pd.DataFrame(Xt2, columns = features)\n\nXt1['sum']  = Xt1[features].sum(axis=1)\nXt1['mean'] = Xt1[features].mean(axis=1)\nXt1['std']  = Xt1[features].std(axis=1)\nXt1['max']  = Xt1[features].max(axis=1)\nXt1['min']  = Xt1[features].min(axis=1)\nXt1['kurt'] = Xt1[features].kurtosis(axis=1)\n\nXt1 = np.array(Xt1)\n\nXt2['sum']  = Xt2[features].sum(axis=1)\nXt2['mean'] = Xt2[features].mean(axis=1)\nXt2['std']  = Xt2[features].std(axis=1)\nXt2['max']  = Xt2[features].max(axis=1)\nXt2['min']  = Xt2[features].min(axis=1)\nXt2['kurt'] = Xt2[features].kurtosis(axis=1)\n\nXt2 = np.array(Xt2)\n#y_pred = ctb.predict_proba(np.array(Xt))","10823c44":"y_test = predict_model(Xt1, Xt2, 'Xt1', modelapply2)","2be98116":"y_test_1 = np.mean([y_test[0], y_test[1], y_test[2], y_test[3], y_test[4], y_test[5], y_test[6], y_test[7], y_test[8]], axis=0)","d9f96646":"#y_test = predict_subset(Xt1, Xt2, 'Xt1', modelapply2)","a1c3efe5":"#flatlist=[element.tolist() for sublist in y_test for element in sublist]","3676672a":"#y_test_1 = np.array(flatlist)","2d00a38b":"#y_pred1 = ctb1.predict_proba(Xt1)\n#y_pred2 = ctb2.predict_proba(Xt1)\n#y_pred3 = ctb3.predict_proba(Xt1)\n#y_pred4 = ctb4.predict_proba(Xt1)\n#y_pred5 = ctb5.predict_proba(Xt1)\n#y_pred6 = ctb6.predict_proba(Xt1)\n#y_pred7 = ctb7.predict_proba(Xt1)\n#y_pred8 = ctb8.predict_proba(Xt1)\n#y_pred9 = ctb9.predict_proba(Xt1)","1731efb1":"#y_test_1 = np.mean([y_pred1, y_pred2, y_pred3, y_pred4, y_pred5, y_pred6, y_pred7, y_pred8, y_pred9], axis=0)","9f65a069":"y_pred_test = pd.Series(\n    y_test_1[:, 1],\n    index=X_test.index,\n    name='target',\n)","14eb0b9f":"y_pred_test","ca0c9d51":"# Create submission file\ny_pred_test.to_csv(\"submission.csv\")","958799e2":"## Applying the models with the best scores to the validation data then I extract the average of the prediction results.","1bccde2b":"## Applying the models with the best scores to the test data then I extract the average of the prediction results.","6a53fa6a":"# Exploratory data analysis","2ca6dbcf":"# Applying cross-validation using two models in parallel and storing the best results of each model."}}