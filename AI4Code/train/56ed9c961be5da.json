{"cell_type":{"4d07323b":"code","51ddd462":"code","887edfe9":"code","68024473":"code","bcccb01a":"code","e5f5fd5e":"code","991c7e23":"code","81659932":"code","f04304e2":"code","90896862":"code","503c8e62":"code","95264563":"code","ca76e352":"code","fe78fc55":"code","7a1e2184":"code","5500fc48":"code","23a1e5a5":"code","6289a6d8":"code","47a810fb":"code","42fe64f2":"code","ce76e8fc":"code","abf304e7":"code","282e12a3":"code","61be5782":"code","d2c1789d":"code","3aa8ca59":"code","a2bd78d4":"code","e0b421eb":"code","ba35718c":"code","d329dd17":"code","bf9da515":"code","bf45047e":"markdown","772c7600":"markdown","925c76c6":"markdown","b4aa43d6":"markdown","60010b06":"markdown","402f439f":"markdown","b59da591":"markdown"},"source":{"4d07323b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.linear_model import BayesianRidge\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# Plot settings\nplt.style.use('ggplot')\n%config InlineBackend.figure_format = 'svg'","51ddd462":"# BERT-as-service\n!pip install bert-serving-server\n!pip install bert-serving-client","887edfe9":"!pip install git+http:\/\/github.com\/brendanhasz\/dsutils.git\n    \nfrom dsutils.encoding import LambdaTransformer\nfrom dsutils.encoding import LambdaFeatures\nfrom dsutils.encoding import NullEncoder\nfrom dsutils.encoding import DateEncoder\nfrom dsutils.encoding import JsonEncoder\nfrom dsutils.encoding import NhotEncoder\nfrom dsutils.encoding import JoinTransformer\nfrom dsutils.encoding import MultiTargetEncoderLOO\n\nfrom dsutils.ensembling import BaggedRegressor\nfrom dsutils.ensembling import StackedRegressor\nfrom dsutils.models import InterpolatingPredictor\n\nfrom dsutils.evaluation import permutation_importance_cv\nfrom dsutils.evaluation import plot_permutation_importance\nfrom dsutils.evaluation import top_k_permutation_importances\n\nfrom dsutils.transforms import Scaler, Imputer\n\nfrom dsutils.cleaning import DeleteCols\nfrom dsutils.cleaning import KeepOnlyCols\nfrom dsutils.external import BertEncoder","68024473":"# Load training data\ndtypes = {\n  'id':                    'uint16',\n  'belongs_to_collection': 'str',\n  'budget':                'float32',\n  'genres':                'str',\n  'homepage':              'str',\n  'imdb_id':               'str',\n  'original_language':     'str',\n  #'original_title':        'str',\n  'overview':              'str',\n  'popularity':            'float32',\n  #'poster_path':           'str',\n  'production_companies':  'str',\n  'production_countries':  'str',\n  'release_date':          'str',\n  'runtime':               'float32',\n  'spoken_languages':      'str',\n  #'status':                'str',\n  'tagline':               'str',\n  'title':                 'str',\n  'Keywords':              'str',\n  'cast':                  'str',\n  'crew':                  'str',\n  'revenue':               'float32',\n}\ntrain = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv',\n                    usecols=dtypes.keys(),\n                    dtype=dtypes)\ndel dtypes['revenue']\ntest = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv',\n                   usecols=dtypes.keys(),\n                   dtype=dtypes)\ndf = pd.concat([train, test], axis=0)","bcccb01a":"# Load imdb scores\ndtypes = {\n  'imdb_id':    'str',\n  'avg_rating': 'float32',\n  'num_rating': 'float32',\n}\nimdb_df = pd.read_csv('..\/input\/box-office-prediction-imbd-scores\/imdb_scores.csv',\n                      usecols=dtypes.keys(),\n                      dtype=dtypes)","e5f5fd5e":"# Histogram of release year (sans century)\nyear_fn = lambda x: int(x[-2:]) if isinstance(x, str) else np.nan\nyears = df['release_date'].apply(year_fn)\nplt.hist(years, bins=np.arange(100), color='#648FFF')\nplt.xlabel('year (sans century)')\nplt.ylabel('count')\nplt.show()","991c7e23":"def fix_dates(date_str):\n    if isinstance(date_str, str):\n        if int(date_str[-2:]) < 20:\n            return date_str[:-2]+'20'+date_str[-2:]\n        else:\n            return date_str[:-2]+'19'+date_str[-2:]\n    else:\n        return np.nan\n    \n# Fix dates\ntrain['release_date'] = train['release_date'].apply(fix_dates)\ntest['release_date'] = test['release_date'].apply(fix_dates)","81659932":"plt.hist(df['runtime'], bins=np.arange(250), color='#FFB000')\nplt.xlabel('runtime (min)')\nplt.ylabel('count')\nplt.show()","f04304e2":"plt.hist(df['runtime'], bins=np.arange(50), color='#FFB000')\nplt.xlabel('runtime (min)')\nplt.ylabel('count')\nplt.show()","90896862":"df.loc[df['runtime']==0, :]","503c8e62":"# Set runtimes of 0 to nan\ntrain.loc[train['runtime']<1, 'runtime'] = np.nan\ntest.loc[test['runtime']<1, 'runtime'] = np.nan","95264563":"# Make id the index\ntrain.set_index('id', inplace=True)\ntest.set_index('id', inplace=True)","ca76e352":"# Split into X and Y\ntrain_y = train['revenue']\ntrain_X = train\ndel train_X['revenue']","fe78fc55":"plt.hist(train_y, color='#785EF0')\nplt.xlabel('revenue')\nplt.ylabel('count')\nplt.show()","7a1e2184":"plt.hist(np.log1p(train_y), color='#785EF0')\nplt.xlabel('log(1+revenue)')\nplt.ylabel('count')\nplt.show()","5500fc48":"# Transform target\ntrain_y = np.log1p(train_y)","23a1e5a5":"plt.hist(df['budget'], color='#FE6100')\nplt.xlabel('budget')\nplt.ylabel('count')\nplt.show()","6289a6d8":"plt.hist(np.log1p(df['budget']), color='#FE6100')\nplt.xlabel('log(1+budget)')\nplt.ylabel('count')\nplt.show()","47a810fb":"plt.hist(df['popularity'], color='#DC267F', bins=np.linspace(0, 100, 50))\nplt.xlabel('popularity')\nplt.ylabel('count')\nplt.show()","42fe64f2":"plt.hist(np.log1p(df['popularity']), color='#DC267F')\nplt.xlabel('log(1+popularity)')\nplt.ylabel('count')\nplt.show()","ce76e8fc":"# Transforms to apply to numeric columns\ntransforms = {\n    'budget': lambda x: np.log1p(x),\n    'popularity': lambda x: np.log1p(x),\n}\n\n# Columns to null-encode\nnull_encode_cols = [\n    'belongs_to_collection',\n    'homepage',\n]\n\n# Date encoder\ndate_cols = {\n    'release_date': ('%m\/%d\/%Y', ['year', 'month', 'day', 'dayofyear', 'dayofweek'])\n}\n\n# JSON fields to extract\njson_fields = {\n    'genres': 'name',\n    'production_companies': 'name',\n    'production_countries': 'iso_3166_1',\n    'spoken_languages': 'iso_639_1',\n    'Keywords': 'name',\n    'cast': 'name',\n    'crew': [('name', 'job', 'Director'),\n             ('name', 'job', 'Producer'),\n             ('name', 'job', 'Writer'),\n             ('name', 'job', 'Editor'),],\n}\n\n# Columns to N-hot encode\nnhot_cols = [\n    'genres_name',\n    'original_language',\n    'production_countries_iso_3166_1',\n    'spoken_languages_iso_639_1',\n]\n\n# Columns to target encode\nbayesian_c = 5 #regularization\nte_cols = [\n    'production_companies_name',\n    'cast_name',\n    'crew_job_Director_name',\n    'crew_job_Producer_name',\n    'crew_job_Writer_name',\n    'crew_job_Editor_name',\n]\n\n# Columns to BERT encode\nn_pc = 5 #keep top 5 principal components of BERT embeddings\nbert_cols = [\n    'overview',\n    'tagline',\n    'title',\n    'Keywords_name',\n]\n\n# Feature engineering\nword_count = lambda e: len(e.split(' ')) if isinstance(e, str) else 0\nkeyword_count = lambda e: len(e.split(',')) if isinstance(e, str) else 0\nnew_features = {\n    'budget_runtime_ratio': lambda x: x['budget']\/x['runtime'],\n    'budget_popularity_ratio': lambda x: x['budget']\/(x['popularity']+1),\n    'budget_year_ratio': lambda x: x['budget']\/np.square(x['release_date_year']),\n    'popularity_year_ratio': lambda x: x['popularity']\/np.square(x['release_date_year']),\n    'rating_to_votes_ratio': lambda x: x['avg_rating']\/(x['num_rating']+1),\n    'runtime_rating_ratio': lambda x: x['runtime']\/(x['avg_rating']+1),\n    'overview_word_count': lambda x: x['overview'].apply(word_count),\n    'tagline_word_count': lambda x: x['tagline'].apply(word_count),\n    'keyword_count': lambda x: x['Keywords_name'].apply(keyword_count),\n}","abf304e7":"# Create the pipeline\npreprocessing = Pipeline([\n    ('transforms',   LambdaTransformer(transforms)),\n    ('join_imbd',    JoinTransformer(imdb_df, 'imdb_id', 'imdb_id')),\n    ('null_encoder', NullEncoder(null_encode_cols, delete_old=True)),\n    ('date_encoder', DateEncoder(date_cols)),\n    ('json_encoder', JsonEncoder(json_fields)),\n    ('nhot_encoder', NhotEncoder(nhot_cols, top_n=10)),\n    ('add_features', LambdaFeatures(new_features)),\n    ('targ_encoder', MultiTargetEncoderLOO(te_cols, bayesian_c=bayesian_c)),\n    ('bert_encoder', BertEncoder(bert_cols, n_pc=n_pc)),\n    ('scaler',       Scaler()),\n    ('imputer',      Imputer()),\n])","282e12a3":"\"\"\"\n# Model w\/ just catboost + no BERT encoding\nmodel = Pipeline([\n    ('transforms',   LambdaTransformer(transforms)),\n    ('join_imbd',    JoinTransformer(imdb_df, 'imdb_id', 'imdb_id')),\n    ('null_encoder', NullEncoder(null_encode_cols, delete_old=True)),\n    ('date_encoder', DateEncoder(date_cols)),\n    ('json_encoder', JsonEncoder(json_fields)),\n    ('nhot_encoder', NhotEncoder(nhot_cols, top_n=10)),\n    ('add_features', LambdaFeatures(new_features)),\n    ('targ_encoder', MultiTargetEncoderLOO(te_cols, bayesian_c=bayesian_c)),\n    ('delete_cols',  DeleteCols(bert_cols)),\n    ('scaler',       Scaler()),\n    ('imputer',      Imputer()),\n    ('regressor',    CatBoostRegressor(verbose=False))\n])\n\"\"\"\n\n# Model w\/ just catboost\nmodel = Pipeline([\n    ('preprocessing', preprocessing),\n    ('regressor',     CatBoostRegressor(verbose=False))\n])\n\n# Fit + make predictions\nfit_model = model.fit(train_X, train_y)\npreds = fit_model.predict(test)\n\n# Save predictions to file\npreds_df = pd.DataFrame(index=test.index)\npreds_df['revenue'] = np.maximum(0, np.expm1(preds))\npreds_df.to_csv('predictions_baseline.csv')","61be5782":"# Preprocess both test and train data\ntrain_pp = preprocessing.fit_transform(train_X, train_y)\ntest_pp = preprocessing.transform(test)","d2c1789d":"train_pp","3aa8ca59":"%%time\n\n# CatBoost\ncatboost = Pipeline([\n    ('regressor', CatBoostRegressor(verbose=False))\n])\n\n# Compute permutation-based feature importances\nimp_df = permutation_importance_cv(train_pp,\n                                   train_y.copy(),\n                                   estimator=catboost,\n                                   metric='rmse',\n                                   n_jobs=1)","a2bd78d4":"# Show the feature importances\nplt.figure(figsize=(6, 20))\nplot_permutation_importance(imp_df)\nplt.show()","e0b421eb":"# Get a list of the top 30 most important features\ncols_to_keep = top_k_permutation_importances(imp_df, k=30)","ba35718c":"# Add feature selection to preprocessing\npreprocessing = Pipeline([\n    ('preprocessing', preprocessing),\n    ('col_filter',    KeepOnlyCols(cols_to_keep)),\n])","d329dd17":"# Base learner models\nbase_learners = [\n    BayesianRidge(),\n    XGBRegressor(),\n    CatBoostRegressor(verbose=False),\n    LGBMRegressor()\n]\n\n# Stacked model\nmodel = StackedRegressor(base_learners,\n                         meta_learner=BayesianRidge(),\n                         preprocessing=preprocessing,\n                         n_splits=5, n_jobs=1)","bf9da515":"%%time\n\n# Fit model and predict on test data\nfit_model = model.fit(train_X, train_y)\npreds = fit_model.predict(test)\n\n# Save predictions to file\npreds_df = pd.DataFrame(index=test.index)\npreds_df['revenue'] = np.maximum(0, np.expm1(preds))\npreds_df.to_csv('predictions.csv')","bf45047e":"## Model","772c7600":"## Processing Pipeline","925c76c6":"## Baseline","b4aa43d6":"## Data Loading","60010b06":"Load the IMDB average ratings and number of ratings per movie (scraped in this kernel: http:\/\/www.kaggle.com\/brendanhasz\/box-office-prediction-imbd-scores)","402f439f":"## Feature Selection","b59da591":"## EDA \/ Data Cleaning"}}