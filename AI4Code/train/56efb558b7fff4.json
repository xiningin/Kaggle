{"cell_type":{"d435b694":"code","55bf2d6a":"code","ab8950ee":"code","602eaf2b":"code","fb942034":"code","607a24b2":"code","a6542c54":"code","c7a10aa0":"code","5edb3573":"code","7c6d9fc1":"code","9fdd697a":"code","1add3d08":"code","56c4dbfa":"code","0c2da899":"code","a2dcaf15":"code","7f203f3b":"code","c5cf1b22":"code","e182d0b3":"code","88b6ef1a":"code","b0f4a312":"code","b96a1dbf":"code","40f98e33":"code","66f8fb5a":"code","e19b337f":"code","1e6bdc93":"code","505062e7":"code","1086cd99":"code","ac1a34d7":"code","80fa13b1":"code","b21bc3bc":"code","30dcec36":"code","33088af0":"code","c2a32d1a":"code","0f326957":"code","ca91c159":"code","867932d4":"code","8ab234d1":"code","ae2d8f4a":"code","29cfe8ed":"code","c617dffa":"code","7cc6fe98":"code","5e7215f1":"code","6ea6c1f5":"code","36bbb722":"code","09f9d859":"code","40cad8d3":"code","cfa1e99d":"code","10662465":"code","0851c121":"code","94935e07":"code","5567c0c8":"code","e6da4eb6":"code","abe0e1be":"code","3f239655":"code","c7427559":"code","480f66c3":"code","fd7a1a5b":"code","99e44fac":"code","f8d9a224":"code","37f0b932":"code","48e6c2e7":"code","087699e8":"code","842a3ccf":"code","3ce86481":"code","8adcc8bf":"code","ca206d9a":"code","a565502f":"code","72a664a1":"code","81ad046c":"code","694f2ebf":"code","ef397110":"code","b5ce4a6c":"code","ece655d4":"code","ed1e5f93":"code","902d51d2":"code","98dfacb6":"code","47a59220":"code","82099378":"code","35376bc1":"code","805bb6c0":"code","6c280ea2":"code","a6b8461a":"code","a6c87917":"code","25252a96":"code","cf50e450":"code","72669e95":"code","002f80f5":"code","6098ef42":"code","52a6493c":"code","c4ed4ffb":"code","81eb40cf":"code","52e7a1a0":"code","926b345b":"code","cf99220b":"code","8c3a8d57":"code","cf27721d":"code","d7ef05ed":"code","2448c4c9":"code","62926981":"code","6d71b265":"code","06786ac0":"code","ac426480":"code","87a8944b":"code","6c51956a":"code","2f9bc75e":"code","e23e6ab0":"code","fa815142":"code","c9725919":"code","6cb40445":"code","fcdab7b2":"code","a9ef534a":"code","4970f2ea":"code","ba1014e8":"code","56359ccf":"code","17848e02":"code","cebd4e87":"code","03babebd":"code","8b877c80":"code","1353c0b2":"code","6575c5ab":"code","e5facb11":"code","68b6fe5c":"code","76bd9626":"code","9348d413":"code","c798b6dd":"code","87941cb1":"code","f089bb97":"code","c0cb8127":"code","a644d707":"code","fabbb951":"code","1d4ccabb":"code","bb11954c":"code","8657d56e":"code","901d6caa":"code","bd88c6da":"code","9e9431b7":"code","0f020772":"code","2c8fdfc4":"code","bc0fcdde":"code","ce237fde":"code","10abf865":"code","cf50dc02":"code","e10265bc":"code","65e6bdf1":"code","5ae7c0c5":"code","049b60e9":"markdown","0fdd6942":"markdown","ba4b2d4e":"markdown","a1c0b00d":"markdown","45a5682f":"markdown","caad217e":"markdown","9c821b76":"markdown","7e27fd22":"markdown","7ae04c73":"markdown","5829ec86":"markdown","2d0161fb":"markdown","95217aed":"markdown","cae0c29e":"markdown","fd2b8f17":"markdown","4ac65387":"markdown","74a818bd":"markdown","250b29f9":"markdown","820f99a7":"markdown","9b439161":"markdown","6486421a":"markdown","c276aca2":"markdown","34436fab":"markdown","627fb9e8":"markdown","8c0b40af":"markdown","5b6b3244":"markdown","11dfa54d":"markdown","30f25578":"markdown","54588ce4":"markdown","45bcd2fb":"markdown","a04d4562":"markdown","33eb53f5":"markdown","f8c9469c":"markdown","ad7a8511":"markdown","33e0f0bb":"markdown","831b7846":"markdown","7390cff1":"markdown","2e28673d":"markdown","10eb151a":"markdown","a4223583":"markdown","2c2ceb0e":"markdown","f30a4702":"markdown","e1689b5f":"markdown","ff25ccd7":"markdown","e49d3c43":"markdown","49017f5c":"markdown","c858c63d":"markdown","4dbda818":"markdown","c2dc9981":"markdown","58e381b9":"markdown","59b07842":"markdown","b24b7e6b":"markdown","bad0391b":"markdown","56678f52":"markdown","6d08aad7":"markdown","85ade08a":"markdown","995bf7d0":"markdown","c2b2a2ff":"markdown","c4513c27":"markdown","ac931ba3":"markdown","2a2cb597":"markdown","41f76b92":"markdown","ae8ae94b":"markdown","02452389":"markdown","14e21b78":"markdown","2650d88b":"markdown","029c5e38":"markdown","b565f33a":"markdown","04eee90c":"markdown","ab62831a":"markdown","0394719b":"markdown","43c1aefa":"markdown","6e0fb892":"markdown","50ec3236":"markdown","8ae3e292":"markdown","2c2040ad":"markdown","0d5e5978":"markdown","5e3e1b84":"markdown","c72d500d":"markdown","5afa9020":"markdown","1114bc06":"markdown","44d5ac09":"markdown","aed9e5de":"markdown","b234a823":"markdown","6e6c15ee":"markdown","fccbaedd":"markdown","7a1c93ed":"markdown"},"source":{"d435b694":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import accuracy_score,mean_squared_error,roc_curve,roc_auc_score,classification_report,r2_score,confusion_matrix\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import scale \nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import BaggingRegressor\n\n\n# For data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns; sns.set()\n# Plotly for interactive graphics \nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\n# Disabling warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport warnings\nwarnings.filterwarnings(\"ignore\")","55bf2d6a":"data=pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndf=data.copy()\ndf.head()","ab8950ee":"data.sample(5)  #chose randon sample from row","602eaf2b":"df.info()","fb942034":"df.target.unique()","607a24b2":"df.isnull().sum()  ","a6542c54":"df[\"target\"].value_counts()","c7a10aa0":"df.describe()","5edb3573":"df.corr()","7c6d9fc1":"sns.countplot(df.target, palette=['green', 'red'])\nplt.title(\"[0] == Not Disease, [1] == Disease\");","9fdd697a":"plt.scatter(x=df.age[df.target==1], y=df.thalach[(df.target==1)], c=\"red\")\nplt.scatter(x=df.age[df.target==0], y=df.thalach[(df.target==0)])\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","1add3d08":"f, ax = plt.subplots(figsize=(10,6)) #DISTRUBUTION OF AGE WITH DISTPLOT\nx = df['age']\nax = sns.distplot(x, bins=10)\nplt.show()","56c4dbfa":"f, ax = plt.subplots(figsize=(8, 6))   #DISTRUBUTION OF AGE WITH BOXPLOT\nsns.boxplot(x=df[\"age\"])\nplt.show()","0c2da899":"young_ages=df[(df.age>=29)&(df.age<40)] \nmiddle_ages=df[(df.age>=40)&(df.age<55)]\nelderly_ages=df[(df.age>55)]\nprint('Young Ages :',len(young_ages))\nprint('Middle Ages :',len(middle_ages))\nprint('Elderly Ages :',len(elderly_ages))","a2dcaf15":"sns.barplot(x=['young ages','middle ages','elderly ages'],y=[len(young_ages),len(middle_ages),len(elderly_ages)])\nplt.xlabel('Age Range')\nplt.ylabel('Age Counts')\nplt.title('Ages State in Dataset')\nplt.show()","7f203f3b":"colors = ['blue','green','yellow']  #we can see in pie.\nexplode = [0,0,0.1]\nplt.figure(figsize = (10,10))\n#plt.pie([target_0_agerang_0,target_1_agerang_0], explode=explode, labels=['Target 0 Age Range 0','Target 1 Age Range 0'], colors=colors, autopct='%1.1f%%')\nplt.pie([len(young_ages),len(middle_ages),len(elderly_ages)],labels=['young ages','middle ages','elderly ages'],explode=explode,colors=colors, autopct='%1.1f%%')\nplt.title('Age States',color = 'blue',fontsize = 15)\nplt.show()","c5cf1b22":"plt.figure(figsize=(15,7))\nsns.violinplot(x=df.age,y=df.target)\nplt.xticks(rotation=90)\nplt.legend()\nplt.title(\"Age & Target System\")\nplt.show()","e182d0b3":"df.columns","88b6ef1a":"plt.figure(figsize=(10,7))\nsns.barplot(x=\"sex\",y = 'ca',hue = 'target',data=df);","b0f4a312":"plt.figure(figsize=(10,7))\nsns.barplot(x=\"sex\",y = 'oldpeak',hue = 'restecg',data=df);","b96a1dbf":"sns.countplot(df.target,hue=df.sex)\nplt.xlabel('Target')\nplt.ylabel('Count')\nplt.title('Target & Sex Counter 1 & 0')\nplt.show()","40f98e33":"plt.figure(figsize=(15,6))\nsns.countplot(x='age',data = df, hue = 'target',palette='GnBu')\nplt.show()#Number of people who have heart disease according to age ","66f8fb5a":"# Let's make our correlation matrix a little prettier\ncorr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(15, 15))\nax = sns.heatmap(corr_matrix,\n                 annot=True,\n                 linewidths=0.5,\n                 fmt=\".2f\",\n                 cmap=\"YlGnBu\");\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","e19b337f":"df.groupby('cp',as_index=False)['target'].mean()","1e6bdc93":"df.groupby('slope',as_index=False)['target'].mean()","505062e7":"df.groupby('target').mean()","1086cd99":"num_var = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'target' ]\nsns.pairplot(df[num_var], kind='scatter', diag_kind='hist')\nplt.show()","ac1a34d7":"num_var = ['cp', 'slope', 'exang', 'thalach', 'oldpeak','ca','thal', 'target' ]\nsns.pairplot(df[num_var], kind='scatter', diag_kind='hist')\nplt.show()","80fa13b1":"df.drop('target', axis=1).corrwith(df.target).plot(kind='bar', grid=True, figsize=(12, 8), \n                                                   title=\"Correlation with target\")","b21bc3bc":"f, ax = plt.subplots(figsize=(8, 6))\nsns.stripplot(x=\"target\", y=\"thalach\", data=df)\nplt.show()","30dcec36":"f, ax = plt.subplots(figsize=(8, 6))# with jitter\nsns.stripplot(x=\"target\", y=\"thalach\", data=df, jitter = 0.01)\nplt.show()","33088af0":"f, ax = plt.subplots(figsize=(8, 6))  #with boxplot\nsns.boxplot(x=\"target\", y=\"thalach\", data=df)\nplt.show()","c2a32d1a":"y = df.target.values\nx_dat = df.drop(['target'], axis = 1)\nx=(x_dat-np.min(x_dat))\/(np.max(x_dat)-np.min(x_dat)).values","0f326957":"y=df.target.values\nx_dat=df.drop([\"target\"],axis=1)","ca91c159":"from sklearn.model_selection import train_test_split,cross_val_score,ShuffleSplit,GridSearchCV\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=42)","867932d4":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(solver = 'liblinear')\nlr.fit(x_train,y_train)\ny_pred=lr.predict(x_test)","8ab234d1":"lr #We can see what there is in lr(icinde hangi secenekler vargormek icin) \n\n#LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   #intercept_scaling=1, l1_ratio=None, max_iter=100,\n                  # multi_class='auto', n_jobs=None, penalty='l2',\n                  # random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                  # warm_start=False)","ae2d8f4a":"lr","29cfe8ed":"lr.intercept_  #sabit katsayi","c617dffa":"lr.coef_   #degisken katsayilari","7cc6fe98":"l_score=accuracy_score(y_test,y_pred)\nl_score\n#The y predicted by the y in the test are compared(test deki y ile tahmin edilen yler karsilastiriliyor.Dogru tahmin etme yuzdesi bulunuyor)","5e7215f1":"c_l=confusion_matrix(y_test,y_pred)# We found the numbers of guessing with confusion matrix, 31 for 1 correct guess, 0 for 35 correct guess\nc_l                               #The top was imported.\n#confusion matrixle tahmin etme sayilarini bulduk,1 icin 31 i dogru tahmin,0 icin 35 i dogru tahmin\n#En ustte import edildi.","6ea6c1f5":"from sklearn.metrics import confusion_matrix   #Hepsi icin yapilabilir\ny_true=y_test\ny_pred=lr.predict(x_test)\ncmlr=confusion_matrix(y_true, y_pred)\nf,ax=plt.subplots(figsize=(6,6))\nsns.heatmap(cmlr, annot=True)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","36bbb722":"#print(classification_report(y_test,y_pred)) #yukarda import edildi","09f9d859":"lr.predict(x_test)[0:10] #ilk 10 datatest deki tahminlerimiz","40cad8d3":"lr.predict_proba(x_test)[0:10] #1.si 0 olma 2.si 1 olma olasiligi oranlari","cfa1e99d":"y_probs = lr.predict_proba(x_test)[:,1]","10662465":"y_pred = [1 if i>0.52 else 0 for i in y_probs]\ny_pred[-10:]","0851c121":"confusion_matrix(y_test,y_pred)","94935e07":"accuracy_score(y_test,y_pred)","5567c0c8":"y = df.target\nx = df.drop('target',axis = 1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,\n                                                test_size = 0.25,\n                                                random_state = 42)","e6da4eb6":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\ny_pred=nb.predict(x_test)\ny_pred[:10]","abe0e1be":"nb  #we can look at which option is there in GaussionNB","3f239655":"#?nb","c7427559":"n_score=accuracy_score(y_test,y_pred)\nn_score","480f66c3":"c_nb=confusion_matrix(y_test,y_pred)\nc_nb\n#confusion matrixle tahmin etme sayilarini bulduk,1 icin 32 i dogru tahmin,0 icin 30 i dogru tahmin\n#En ustte import edildi.","fd7a1a5b":"from sklearn.metrics import confusion_matrix   #Hepsi icin yapilabilir\ny_true=y_test\ny_pred=nb.predict(x_test)\ncmnb=confusion_matrix(y_true, y_pred)\nf,ax=plt.subplots(figsize=(6,6))\nsns.heatmap(cmnb, annot=True)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","99e44fac":"#print(classification_report(y_test,y_pred)) #yukarda import edildi","f8d9a224":"nb.predict(x_test)[0:10] #ilk 10 datatest deki tahminlerimiz","37f0b932":"nb.predict_proba(x_test)[0:10] #1.si 0 olma 2.si 1 olma olasiligi oranlari","48e6c2e7":"y_probs = nb.predict_proba(x_test)[:,1]\ny_pred = [1 if i>0.45 else 0 for i in y_probs]\ny_pred[0:10]","087699e8":"nb_tuned_bestscore=accuracy_score(y_test,y_pred)\nnb_tuned_bestscore","842a3ccf":"\ncmnb_best=confusion_matrix(y_test,y_pred) \ncmnb_best","3ce86481":"from sklearn.neighbors import KNeighborsClassifier\ny=df.target\nx=df.drop(\"target\",axis=1)\n","8adcc8bf":"x_train,x_test,y_train,y_test = train_test_split(x,y,\n                                                test_size = 0.25,\n                                                random_state = 42)","ca206d9a":"knn = KNeighborsClassifier(n_neighbors=3).fit(x_train,y_train)\nknn.fit(x_train,y_train)\ny_pred = knn.predict(x_test)\ny_pred","a565502f":"knn  ##we can look at which option is there in KNeighborsClassifier\n#KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     #metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n                     #weights='uniform')","72a664a1":"knn_score=accuracy_score(y_test,y_pred)\nknn_score","81ad046c":"c_knn=confusion_matrix(y_test,y_pred)\nc_knn","694f2ebf":"from sklearn.metrics import confusion_matrix   #Hepsi icin yapilabilir\ny_true=y_test\ny_pred=knn.predict(x_test)\ncmknn=confusion_matrix(y_true, y_pred)\nf,ax=plt.subplots(figsize=(6,6))\nsns.heatmap(cmknn, annot=True)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","ef397110":"#print(classification_report(y_test,y_pred)) #yukarda import edildi","b5ce4a6c":"knn.predict(x_test)[0:10] #ilk 10 datatest deki tahminlerimiz","ece655d4":"knn.predict_proba(x_test)[0:10] #1.si 0 olma 2.si 1 olma olasiligi oranlari","ed1e5f93":"RMSE = []   # ERROR ON TRAIN DATA\n\nfor k in range(30):\n    k = k+1\n    knn = KNeighborsRegressor(n_neighbors = k).fit(x_train, y_train)\n    y_pred = knn.predict(x_train) \n    rmse = np.sqrt(mean_squared_error(y_train,y_pred)) \n    RMSE.append(rmse) \n    print(\"k =\" , k , \"i\u00e7in RMSE de\u011feri: \", rmse)","902d51d2":"from sklearn.model_selection import GridSearchCV  \n#We use Grid for tuning","98dfacb6":"knn_params = {'n_neighbors': np.arange(1,30,1)} #we obta\nknn = KNeighborsRegressor()","47a59220":"knn_cv_model = GridSearchCV(knn, knn_params, cv = 10) #cross validation yontemi kullaniliyor.nesnesi tanimlandi\nknn_cv_model.fit(x_train, y_train)","82099378":"print(\"Best Score:\"+str(knn_cv_model.best_score_))\nprint(\"Best Parameters:\"+str(knn_cv_model.best_params_))","35376bc1":"knn_tuned =KNeighborsClassifier(n_neighbors = 21)\nknn_tuned = knn_tuned.fit(x_train,y_train)\ny_pred = knn_tuned.predict(x_test)\nknn_tuned_score=accuracy_score(y_test,y_pred)\nknn_tuned_score","805bb6c0":"#np.sqrt(mean_squared_error(y_test, knn_tuned.predict(x_test)))","6c280ea2":"knn_tune2 =KNeighborsClassifier(n_neighbors = 21,metric='hamming')\nknn_tune2.fit(x_train,y_train)\ny_pred = knn_tune2.predict(x_test)\nknn_tuned_bestscore=accuracy_score(y_test,y_pred)\nknn_tuned_bestscore","a6b8461a":"from sklearn.metrics import confusion_matrix   #Hepsi icin yapilabilir\ny_true=y_test\ny_pred=knn_tune2.predict(x_test)\ncmknn_best=confusion_matrix(y_true, y_pred)\nf,ax=plt.subplots(figsize=(6,6))\nsns.heatmap(cmknn_best, annot=True)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","a6c87917":"from sklearn.svm import SVC\ny=df.target\nx=df.drop(\"target\",axis=1)","25252a96":"y = df.target\nx = df.drop('target',axis = 1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,\n                                                test_size = 0.25,\n                                                random_state = 42)","cf50e450":"svm = SVC(C=5,degree=9,kernel = 'poly')\nsvm.fit(x_train,y_train)\ny_pred = svm.predict(x_test)","72669e95":"#?svm","002f80f5":"svm\n#SVC(C=5, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    #decision_function_shape='ovr', degree=9, gamma='scale', kernel='poly',\n    #max_iter=-1, probability=False, random_state=None, shrinking=True,\n    #tol=0.001, verbose=False)","6098ef42":"y_pred","52a6493c":"svm_score1 = accuracy_score(y_test,y_pred)\nsvm_score1","c4ed4ffb":"c_svm=confusion_matrix(y_test,y_pred)\nc_svm","81eb40cf":"from sklearn.metrics import confusion_matrix   #Hepsi icin yapilabilir\ny_true=y_test\ny_pred=svm.predict(x_test)\ncmsvm=confusion_matrix(y_true, y_pred)\nf,ax=plt.subplots(figsize=(6,6))\nsns.heatmap(cmsvm, annot=True)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","52e7a1a0":"#print(classification_report(y_test,y_pred)) #yukarda import edildi","926b345b":"#EN UYGUN C VE GAMMA DEGERI BULMA\nsvc_params ={\"C\":[0.00001,0.001,0.01,5,10,50,100],\n            \"gamma\":[0.0001,0.001,0.01,1,5,10,50,100]}\nsvc =SVC()\nsvc_cv_model = GridSearchCV(svc,svc_params,\n                           cv = 10,\n                           n_jobs = -1,\n                           verbose = 2)\nsvc_cv_model.fit(x_train,y_train)\nprint(\"Best Parameters:\"+str(svc_cv_model.best_params_))","cf99220b":"?svc_cv_model","8c3a8d57":"# svm_tune1= SVC(C=100,gamma= 0.0001,degree=9,kernel = 'poly')\n# svm_tune1.fit(x_train,y_train)\n# y_pred = svm.predict(x_test)  # cok uzun suruyor","cf27721d":"svm_score2 = accuracy_score(y_test,y_pred)\nsvm_score2","d7ef05ed":"#we changed the kernel,We can use linear,poly,rbf...\nsvm_tune2 = SVC(C=100,degree=9,kernel = 'linear')\nsvm_tune2.fit(x_train,y_train)\ny_pred = svm_tune2.predict(x_test)\naccuracy_score(y_test,y_pred)","2448c4c9":"#we changed the kernel,We can use linear,poly,rbf...\nsvm_tune3 = SVC(C=100,degree=9,kernel = 'rbf')\nsvm_tune3.fit(x_train,y_train)\ny_pred = svm_tune3.predict(x_test)\naccuracy_score(y_test,y_pred)","62926981":"# svc_tuned=SVC(C=100,gamma=0.0001,kernel = 'linear')\n# svc_tuned.fit(x_train,y_train)\n# y_pred = svc_tuned.predict(x_test)\n# accuracy_score(y_test,y_pred)    #uzun suruyor","6d71b265":"from sklearn.ensemble import RandomForestClassifier\ny=df.target\nx=df.drop(\"target\",axis=1)","06786ac0":"x_train,x_test,y_train,y_test = train_test_split(x,y,\n                                                test_size = 0.25,\n                                                random_state = 42)","ac426480":"rf=RandomForestClassifier()\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\ny_pred","87a8944b":"rf\n#RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       #criterion='gini', max_depth=None, max_features='auto',\n                       #max_leaf_nodes=None, max_samples=None,\n                      # min_impurity_decrease=0.0, min_impurity_split=None,\n                      # min_samples_leaf=1, min_samples_split=2,\n                      # min_weight_fraction_leaf=0.0, n_estimators=100,\n                      # n_jobs=None, oob_score=False, random_state=None,\n                      # verbose=0, warm_start=False)","6c51956a":"rf_score=accuracy_score(y_test,y_pred)\nrf_score","2f9bc75e":"c_rf=confusion_matrix(y_test,y_pred)\nc_rf","e23e6ab0":"from sklearn.metrics import confusion_matrix   #Hepsi icin yapilabilir\ny_true=y_test\ny_pred=rf.predict(x_test)\ncmlr=confusion_matrix(y_true, y_pred)\nf,ax=plt.subplots(figsize=(6,6))\nsns.heatmap(cmlr, annot=True)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","fa815142":"print(classification_report(y_test,y_pred)) #yukarda import edildi","c9725919":"rf.predict(x_test)[0:10] #ilk 10 datatest deki tahminlerimiz","6cb40445":"rf.predict_proba(x_test)[0:10] #1.si 0 olma 2.si 1 olma olasiligi oranlari","fcdab7b2":"from sklearn.ensemble import RandomForestClassifier  #n_estimotors=11 is best\nscore_list=[]\nfor each in range(1,75):\n    rf2=RandomForestClassifier(n_estimators=each, random_state=42)\n    rf2.fit(x_train, y_train)\n    score_list.append(100*rf2.score(x_test, y_test))\n    print(\"n_estimators=\", each, \"--> Accuracy:\", 100*rf2.score(x_test, y_test), \"%\")\n\nplt.plot([*range(1,75)], score_list)\nplt.xlabel(\"n_estimators Value\")\nplt.ylabel(\"Accuracy %\")\nplt.show()","a9ef534a":"Importance = pd.DataFrame({\"Importance\": rf.feature_importances_*100},\n                         index = x_train.columns)\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"g\")\n\nplt.xlabel(\"Variable Severity Levels\");","4970f2ea":"y=df.target\nx=df[['ca','oldpeak','thal','cp','thalach','age']]","ba1014e8":"\nx_train, x_test, y_train, y_test = train_test_split(x, y, \n                                                    test_size=0.25, \n                                                    random_state=42)","56359ccf":"rf_2 = RandomForestClassifier().fit(x_train, y_train)\ny_pred = rf_2.predict(x_test)\nrf_2_score=accuracy_score(y_test, y_pred)\nrf_2_score","17848e02":"c_rf2=confusion_matrix(y_test,y_pred)\nc_rf2","cebd4e87":"rf_params = {\"max_depth\": [2,5,8,10],\n            \"max_features\": [2,5,8],\n            \"n_estimators\": [10,500,1000],\n            \"min_samples_split\": [2,5,10]}","03babebd":"rf_model1 = RandomForestClassifier()\n\nrf_cv_model1 = GridSearchCV(rf_model1, \n                           rf_params, \n                           cv = 10, \n                           n_jobs = -1, \n                           verbose = 2)","8b877c80":"#rf_cv_model1.fit(x_train, y_train)    #uzun suruyor","1353c0b2":"#print(\"Best Parameters: \" + str(rf_cv_model1.best_params_))  #uzun suruyor","6575c5ab":"rf_tuned1 = RandomForestClassifier(max_depth = 2, \n                                  max_features = 2, \n                                  min_samples_split = 2,\n                                  n_estimators = 500)\n\nrf_tuned1.fit(x_train, y_train)","e5facb11":"y_pred = rf_tuned1.predict(x_test)\nrf_tuned_score=accuracy_score(y_test, y_pred)\nrf_tuned_score","68b6fe5c":"from sklearn.tree import DecisionTreeClassifier\ny=df.target\nx=df.drop(\"target\",axis=1)","76bd9626":"x_train,x_test,y_train,y_test = train_test_split(x,y,\n                                                test_size = 0.25,\n                                                random_state = 42)","9348d413":"dtc = DecisionTreeClassifier()\ndtc.fit(x_train, y_train)","c798b6dd":"y_pred = dtc.predict(x_test)\ny_pred","87941cb1":"dtc_score=accuracy_score(y_test,y_pred)\ndtc_score","f089bb97":"c_dtc=confusion_matrix(y_test,y_pred)\nc_dtc","c0cb8127":"from sklearn.metrics import confusion_matrix   #Hepsi icin yapilabilir\ny_true=y_test\ny_pred=dtc.predict(x_test)\ncmdtc=confusion_matrix(y_true, y_pred)\nf,ax=plt.subplots(figsize=(6,6))\nsns.heatmap(cmdtc, annot=True)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","a644d707":"print(classification_report(y_test,y_pred))","fabbb951":"tree_grid = {\"max_depth\": range(1,10),\n            \"min_samples_split\" : list(range(2,50)) }","1d4ccabb":"tree1 = DecisionTreeClassifier()\ntree_cv = GridSearchCV(tree1, tree_grid, cv = 10, n_jobs = -1, verbose = 2)\ntree_cv_model = tree_cv.fit(x_train, y_train)","bb11954c":"#?tree_cv_model","8657d56e":"print(\"Best Parameters: \" + str(tree_cv_model.best_params_))","901d6caa":"tree1 = DecisionTreeClassifier(max_depth = 3, min_samples_split = 2)\ntree_tuned1 = tree1.fit(x_train, y_train)","bd88c6da":"y_pred = tree_tuned1.predict(x_test)\ndtc_tuned_bestscore=accuracy_score(y_test, y_pred)\ndtc_tuned_bestscore","9e9431b7":"Importance = pd.DataFrame({\"Importance\": dtc.feature_importances_*100},\n                         index = x_train.columns)\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"g\")\n\nplt.xlabel(\"Variable Severity Levels\");","0f020772":"y=df.target\nx=df[['ca','oldpeak','thal','cp','thalach','age']]","2c8fdfc4":"\nx_train, x_test, y_train, y_test = train_test_split(x, y, \n                                                    test_size=0.25, \n                                                    random_state=42)","bc0fcdde":"dtc2 = RandomForestClassifier().fit(x_train, y_train)\ny_pred = dtc2.predict(x_test)\ndtc2_score=accuracy_score(y_test, y_pred)\ndtc2_score","ce237fde":"c_dtc2=confusion_matrix(y_test,y_pred)\nc_dtc2","10abf865":"dtc_tuned1 = RandomForestClassifier(max_depth = 2, \n                                  max_features = 2, \n                                  min_samples_split = 2,\n                                  n_estimators = 500)\ndtc_tuned1.fit(x_train, y_train)","cf50dc02":"y_pred = dtc_tuned1.predict(x_test)\ndtc_tuned_bestscore=accuracy_score(y_test, y_pred)\ndtc_tuned_bestscore","e10265bc":"c_bestdtc=confusion_matrix(y_test,y_pred)\nc_bestdtc","65e6bdf1":"indexx = [\"Log\",\"KNN\",\"SVM\",\"NB\",\"RF\",\"DT\"]\nregressions = [l_score,knn_tuned_bestscore,svm_score1,nb_tuned_bestscore,rf_2_score,dtc_tuned_bestscore]\n\nplt.figure(figsize=(8,6))\nsns.barplot(x=indexx,y=regressions)\nplt.xticks()\nplt.title('Model Comparision',color = 'orange',fontsize=20);","5ae7c0c5":"plt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(c_l,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cmknn_best,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(c_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cmnb_best,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(c_bestdtc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(c_rf2,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.show()","049b60e9":"#### CONCLUSION OF VISUALIZATION\nFindings of Bivariate Analysis are as follows:\n\n* There is no variable which has strong positive correlation with target variable.\n\n* There is no variable which has strong negative correlation with target variable.\n\n* There is no correlation between target and fbs.\n\n* The cp and thalach variables are mildly positively correlated with target variable.\n\n* We can see that the thalach variable is slightly negatively skewed.\n\n* The people suffering from heart disease (target = 1) have relatively higher heart rate (thalach) as compared to people who are not suffering from heart disease (target = 0).","0fdd6942":"Tuning3-changing kernel=rbf,c=100","ba4b2d4e":"### Conclusion:\n\n* svm_score1=84 is the best score and c_svm is the best confusion matrix","a1c0b00d":"### E ) TUNING THE PREDICTION: WE can tune our prediction\n","45a5682f":"## READ DATA AND EXPLORING DATA","caad217e":"### A ) Train-test splitting","9c821b76":"#### CLASSICICATION REPORT: we can also see classification report.","7e27fd22":"### C ) Lets control the succes(score) prediction(accuracy_score,confusion m.) on test_data","7ae04c73":"### Lets look at importance 6 variables","5829ec86":"#### CLASSICICATION REPORT: we can also see classification report.","2d0161fb":"### Count of target with hue=sex","95217aed":"### E ) TUNING THE PREDICTION: WE can tune our prediction\n* n_estimators,importance variables,","cae0c29e":"#### LOOK AT ALL PREDICTION VALUE ON TEST DATA.","fd2b8f17":"#### CLASSICICATION REPORT: we can also see classification report.","4ac65387":"### Conclusion KNN:\n* knn_tuned_bestscore= % 85  and cmknn_best are our best best score and our best confusion matrix","74a818bd":"## 1)LOGISTIC REGRESSION:\n * Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution.\n * Lojistik regresyon, k\u00fcm\u00fclatif lojistik da\u011f\u0131l\u0131m olan bir lojistik fonksiyon kullanarak olas\u0131l\u0131klar\u0131 tahmin ederek kategorik ba\u011f\u0131ml\u0131 de\u011fi\u015fken (\u00f6zellik) ile bir veya daha fazla ba\u011f\u0131ms\u0131z de\u011fi\u015fken (\u00f6zellik) aras\u0131ndaki ili\u015fkiyi \u00f6l\u00e7er.","250b29f9":"### Normalization","820f99a7":"#### LOOK AT ALL PREDICTION VALUE ON TEST DATA.","9b439161":"### Distrubution of Age and Target with violinplot","6486421a":"### E ) TUNING THE PREDICTION: WE can tune our prediction\n* Look at c,kernel,gamma","c276aca2":"## 6 ) DECISION TREE METHOD\nThis model uses a Decision Tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.\n* Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed\".\n\n* According to \u0131nformation entropy, we can determine which feature is the most important. And we put the most important one to the top of the related tree.\n\n* Decision tree classification can be used for both binary and multi classes\n\n* Coding is the same for all supervised classes and we jus need to change the last part of the code.","34436fab":"### Distribution of disease and not disease with scatter","627fb9e8":"### correlation only with target and other variables","8c0b40af":"#### LOOK AT ALL PREDICTION VALUE ON TEST DATA.","5b6b3244":"#### HEATMAP IN CONFUSION MATRIX: We can see the confusion matrix in Heatmap.","11dfa54d":"### A ) Train-test splitting","30f25578":"* If we tune our data for nb, it increase a little.\n* nb_tuned_bestscore= % 89 and cmnb_best are our best best score and our best confusion matrix","54588ce4":"### Count of disease and not desease","45bcd2fb":"## LETS SEE ALL SCORE OF CLASSIFICATIONS METHODS ","a04d4562":"### Interpretation\n* We can see that those people suffering from heart disease (target = 1) have relatively higher heart rate (thalach) as compared to people who are not suffering from heart disease (target = 0).\n","33eb53f5":"#### HEATMAP IN CONFUSION MATRIX: We can see the confusion matrix in Heatmap.","f8c9469c":"### A ) Train-test splitting","ad7a8511":"### Dividing into age groups","33e0f0bb":"## SOME VISUALIZATION","831b7846":"## 2)NAIVE BAYES METHOD\n* In machine learning, Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. \n* Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem.\n","7390cff1":"# LETS TRY CLASSIFICATIONS METHODS\n* Now we've got our data split into training and test sets, it's time to build a machine learning model.\n\n* We'll train it (find the patterns) on the training set.\n\n* And we'll test it (use the patterns) on the test set.\n\n* We're going to try  machine learning models:\n   * 1)Logistic Regression\n   * 2)K-Nearest Neighbours Classifier\n   * 3)Support Vector machine\n   * 4)Decision Tree Classifier\n   * 5)Random Forest Classifier","2e28673d":"#### Tuning2-changing kernel=linear,c=100","10eb151a":"### C ) Lets control the succes(score) prediction(accuracy_score,confusion m.) on test_data","a4223583":"### B ) Modeling of Decision Tree","2c2ceb0e":"### Dividing into age groups with pieplot","f30a4702":"## Correlation matrix(heatmap)","e1689b5f":"## Target and Thalech","ff25ccd7":"#### LOOK AT ALL PREDICTION VALUE ON TEST DATA:","e49d3c43":"### B ) Modeling of Naive B. Method","49017f5c":"#### CLASSICICATION REPORT: we can also see classification report.","c858c63d":"#### CLASSICICATION REPORT: we can also see classification report.","4dbda818":"### C ) Lets control the succes(score) prediction(accuracy_score,confusion m.) on test_data","c2dc9981":"### E ) TUNING THE PREDICTION: WE can tune our prediction","58e381b9":"### A ) Train-test splitting","59b07842":"### Lets look at 6 importance variables","b24b7e6b":"## FINISH","bad0391b":"### sex and ca (hue=target) with barplot","56678f52":"### B ) Modeling of SVM Medhod","6d08aad7":"## Data contains;\n\n* age - age in years\n* sex - (1 = male; 0 = female)\n* cp - chest pain type(gogus agrisi tipi)\n* trestbps - resting blood pressure (in mm Hg on admission to the hospital) (kan basinci)\n* chol - serum cholestoral in mg\/dl (mg\/dl cinsinden serum kolesterol\u00fc)\n* fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* restecg - resting electrocardiographic results(dinlenme elektrokardiyografik sonu\u00e7lar\u0131)\n* thalach - maximum heart rate achieved (ula\u015f\u0131lan maksimum kalp at\u0131\u015f h\u0131z\u0131)\n* exang - exercise induced angina (1 = yes; 0 = no) (egzersize ba\u011fl\u0131 anjina (1 = evet; 0 = hay\u0131r))\n* oldpeak - ST depression induced by exercise relative to rest (dinlenmeye g\u00f6re egzersizin neden oldu\u011fu ST depresyonu)\n* slope - the slope of the peak exercise ST segment (en y\u00fcksek egzersiz ST segmentinin e\u011fimi)\n* ca - number of major vessels (0-3) colored by flourosopy\n* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n* target - have disease or not (1=yes, 0=no) (hastal\u0131\u011f\u0131 var m\u0131 yok mu (1=evet, 0=hay\u0131r))","85ade08a":"### C ) Lets control the succes(score) prediction(accuracy_score,confusion m.) on test_data","995bf7d0":"#### Tuning1-change C and gamma","c2b2a2ff":"* we tune the knn ,than our score increase.","c4513c27":"## LETS NORMALIZE THE VARIABLES ","ac931ba3":"#### Conclusion:\n\n* rf2_score=84 is the best score and c_rf2 is the best confusion matrix","2a2cb597":"## 3)KNN METHOD\n* In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. \n* A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). \n* In this method we need to choose k value.It means that we chose k number of points of classes which are nearest to the out test point. We can call this small data set. We count the number of classes in the small dataset and determine the highest number of class. Finally we can say our test point belongs to the class.\n\n* While choosing k number we have to be carefull because small k value causes overfitting while big k value causes underfitting.\n\n* Coding is the same for all supervised classes and we jus need to change the last part of the code.\n* K=1 SECERSEK OVERFITTING OLABILIR, K= BUYUK SECERSEK UNDERFITTING OLABILIR","41f76b92":"#### HEATMAP IN CONFUSION MATRIX: We can see the confusion matrix in Heatmap.","ae8ae94b":"### D ) Model tuning","02452389":"### B ) Modeling of SVM Medhod","14e21b78":"### Sex and Oldpeak(hue=restecg)","2650d88b":"### Dividing into age groups with barplot","029c5e38":"### Number of people who have heart disease according to age ","b565f33a":"### B ) Modeling of Logistic R. Method","04eee90c":"#### Look at accuracy_score","ab62831a":"### Distribution of age with boxplot","0394719b":"### Interpretation of heatmap\nFrom the above correlation heat map, we can conclude that :-\n\n* target and cp variable are mildly positively correlated (correlation coefficient = 0.43).\n\n* target and thalach variable are also mildly positively correlated (correlation coefficient = 0.42).\n\n* target and slope variable are weakly positively correlated (correlation coefficient = 0.35).\n\n* target and exang variable are mildly negatively correlated (correlation coefficient = -0.44).\n\n* target and oldpeak variable are also mildly negatively correlated (correlation coefficient = -0.43).\n\n* target and ca variable are weakly negatively correlated (correlation coefficient = -0.39).\n\n* target and thal variable are also waekly negatively correlated (correlation coefficient = -0.34).","43c1aefa":"* If we use n_neighbors=21, we can obtain best score...","6e0fb892":"* We can see, If we change our condition for probobilty,our prediction and confusion matrix and accuracy_score change","50ec3236":"* If we change metric and use tuned n-neigbors,acurracy_score is best..There are many kinds of metric in KNN. minkowski,hamming,.... ","8ae3e292":"### E ) TUNING THE PREDICTION: WE can tune our prediction\n* we can tune (n_neigbors,metric,..)","2c2040ad":"## Problem Definition\n* Given clinical parameters about a patient, can we predict whether or not they have heart disease?","0d5e5978":"### C ) Lets control the succes(score) prediction(accuracy_score,confusion m.) on test_data","5e3e1b84":"## If you find this kernel helpful, Please UPVOTES.","c72d500d":"### C ) Lets control the succes(score) prediction(accuracy_score,confusion m.) on test_data","5afa9020":"### B ) Modeling of KNN Medhod","1114bc06":"### A ) Train-test splitting","44d5ac09":"## 4)SVM(SUPPORT VECTOR MACHINES)\n* Support Vector Machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier.\n* SVM is used fo both regression and classification problems, but generally for classification. There is a C parameter inside the SVM algoritma and the default value of C parameter is 1. If C is small, it causes the misclassification. If C is big, it causes ovetfitting. So we need to try C parameter to find best value.\n* SVM, hem regresyon hem de s\u0131n\u0131fland\u0131rma problemleri i\u00e7in kullan\u0131l\u0131r, ancak genellikle s\u0131n\u0131fland\u0131rma i\u00e7in kullan\u0131l\u0131r. SVM i\u00e7erisinde C parametresi vard\u0131r ve C parametresinin default de\u011feri 1'dir. C'nin k\u00fc\u00e7\u00fck olmas\u0131 yanl\u0131\u015f s\u0131n\u0131fland\u0131rmaya neden olur. C b\u00fcy\u00fckse overfitting e neden olur. Bu y\u00fczden en iyi de\u011feri bulmak i\u00e7in C parametresini denememiz gerekiyor.","aed9e5de":"### A ) Train -test splitting","b234a823":"## 5)RANDOM FOREST METHOD\nRandom Forests is one of the most popular model. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators= [100, 300]) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees\n* This methods basically use multiple number of decision trees and take the avarage of the results of these decision trees. And we use this avarage to determine the class of the test point.\n\n* This is one of ensamble method which uses multiple classes to predict the target, and very powerfull technique.","6e6c15ee":"* There are a few young ages","fccbaedd":"#### HEATMAP IN CONFUSION MATRIX: We can see the confusion matrix in Heatmap.","7a1c93ed":"### Distrbution of age with distplot*"}}