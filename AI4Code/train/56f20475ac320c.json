{"cell_type":{"3020a2bf":"code","0544394a":"code","f906b0d3":"code","dca4f669":"code","930bf044":"code","fc30ea2d":"code","6a4813d1":"code","5dce2ce2":"code","9cb8bbcf":"code","2485793d":"code","b9f9bf5e":"code","6a84a2fa":"code","d00eba96":"code","2ae2ce92":"code","8d6aa0e8":"code","c5f6cb83":"code","da81e6c5":"code","ca397f5a":"code","3fefb8b6":"code","3432f9b1":"code","22914e7c":"code","4d3139bf":"code","abb50de2":"markdown","7e558877":"markdown","cc4a0b24":"markdown","435d8859":"markdown","e342db3e":"markdown","2802d02d":"markdown","b6e1ecaa":"markdown","17fbd211":"markdown"},"source":{"3020a2bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0544394a":"# train \u0438 test csv \u0444\u0430\u0439\u043b\u044b \ntrain_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain_df.head(3)","f906b0d3":"print('Length of Data')\nprint('Training data',len(train_df))\nprint('Testing data',len(test_df))","dca4f669":"# \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u0443\u043b\u0435\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\n# \u0432\u044b\u0432\u043e\u0434 \u0438\u043c\u044f \u0441\u0442\u043e\u043b\u0431\u0446\u0430, \u0442\u0438\u043f \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\ndef check_null_values(df):\n    s = df.isna().sum()\n    cols, na_vals = s.index, s.values\n    missing_cols=[]\n    for c, n in zip(cols, na_vals):\n        if n>0:\n            print(f'{c:15}  {train_df[c].dtype !s:10} {n :5} missing values ({n\/len(df) :.2f}% missing)')\n            missing_cols.append({'column':c, 'missing':n, 'missing_percentage':n\/len(df)})\n    return missing_cols","930bf044":"# \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430\u0430 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u043d\u0443\u043b\u0435\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\ntrain_missing_cols = check_null_values(train_df)","fc30ea2d":"# \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430\u0430 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u043d\u0443\u043b\u0435\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\ntest_missing_cols = check_null_values(test_df)","6a4813d1":"set1 = set([obj['column'] for obj in train_missing_cols])\nset2 = set([obj['column'] for obj in test_missing_cols])\n\nintersection = set1.intersection(set2)\nprint('Common Missing values \\n')\nintersection","5dce2ce2":"# \u0438\u0437\u0443\u0447\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n# \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\n# \u0442\u0438\u043f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438\n# \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043f\u0440\u043e\u0446\u0435\u043d\u0442 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432\nu = train_df.nunique()\nm = train_df.isna().sum()\n\nprint('Total number of records',len(train_df))\nfor cols, unique, missing in zip(u.index, u.values, m.values):\n    print(f'{cols :15}{unique !s:5}unique {train_df[cols].dtype !s:10} ({missing\/len(train_df) :.3f}% missing)')","9cb8bbcf":"# \u0421\u0442\u043e\u043b\u0431\u0446\u044b \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0439\ndef get_cat_cols(type='O', threshold=15):\n    cat = []\n\n    # \u0412\u044b\u0431\u043e\u0440 \u0444\u0443\u043d\u043a\u0446\u0438\u0439\n    for col in train_df.columns:\n        if train_df[col].dtype == type:\n            if train_df[col].nunique()<=threshold:\n                cat.append(col)\n    return cat","2485793d":"# \u0427\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b  (int64\/float64)\ndef get_numerical_cols(threshold=15):\n    num_cols =[]\n    num_df = train_df.select_dtypes(exclude='O')\n    for num_col in num_df.columns:\n        if num_col != 'SalePrice':\n            if train_df[num_col].nunique() > 15:\n                num_cols.append(num_col)\n    return num_cols","b9f9bf5e":"# \u0435\u0441\u043b\u0438 \u0432 \u0441\u0442\u043e\u043b\u0431\u0446\u0435 \u043c\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0445 \u043f\u043e\u043b\u0435\u0439, \u0442\u043e \u043e\u043d \u0431\u0435\u0441\u043f\u043e\u043b\u0435\u0437\u0435\u043d\n# \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0443 'MiscFeature' \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u043d\u0430 97 %\nfield_missing = train_df['MiscFeature'].isna().sum()\nprint(f'Missing field in MiscFeature column {field_missing} ({field_missing\/len(train_df):.2f}%)')","6a84a2fa":"def feature_selection_util(type='O', cat_threshold=15, missing_threshold=100, is_cat=True):\n    features=[]\n    columns = get_cat_cols(type=type, threshold=cat_threshold) if is_cat else get_numerical_cols(threshold=cat_threshold)\n    for feature in columns:\n        if train_df[feature].isna().sum() <100:\n            features.append(feature)\n    return features","d00eba96":"# \u041f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\n# \u043f\u0435\u0440\u0435\u043a\u0440\u0435\u0441\u0442\u043d\u0430\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430\nfrom sklearn.model_selection import cross_val_score\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\n\n# \u043a\u043e\u043b\u043e\u043d\u043d\u044b\u0439 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0442\u043e\u0440\nfrom sklearn.compose import ColumnTransformer\n\n# \u041c\u043e\u0434\u0435\u043b\u0438\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor","2ae2ce92":"# \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f \u0441\u0442\u0440\u043e\u043a\nstr_cat_features = feature_selection_util()\nstr_cat_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                      ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n\n# \u0446\u0435\u043b\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u0430\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f\nint_cat_features = feature_selection_util(type='int64')\nint_cat_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                      ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n\n# \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b\nnumerical_features = feature_selection_util(is_cat=False)\nnumerical_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                        ('scale',StandardScaler())])\n\n\n# \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435\npreprocessing = ColumnTransformer(transformers=[('str_cat',str_cat_transformer,str_cat_features),\n                                                ('int_cat',int_cat_transformer, int_cat_features),\n                                                ('num_col', numerical_transformer, numerical_features)])\n\n\n# pipeline \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u044d\u0442\u0430\u043f\u0430 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\npipe = Pipeline(steps=[('preprocessing', preprocessing),\n                       ('model', RandomForestRegressor(random_state=42))])","8d6aa0e8":"# \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\nX = train_df.drop(columns=['SalePrice'])\ny = train_df['SalePrice'] \n# \u043e\u0446\u0435\u043d\u043a\u0430 \u043f\u0435\u0440\u0435\u043a\u0440\u0435\u0441\u0442\u043d\u043e\u0439 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438\ncv_score = cross_val_score(pipe, X, y, scoring='neg_root_mean_squared_error')","c5f6cb83":"print('Cross validation score\\n',cv_score)\nprint('Mean cross validation score',cv_score.mean())","da81e6c5":"def build_model_pipeline(model):\n    # \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f \u0441\u0442\u0440\u043e\u043a\n    str_cat_features = feature_selection_util()\n    str_cat_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                          ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n\n    # \u0446\u0435\u043b\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u0430\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f\n    int_cat_features = feature_selection_util(type='int64')\n    int_cat_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                          ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n\n    # \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b\n    numerical_features = feature_selection_util(is_cat=False)\n    numerical_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                            ('scale',StandardScaler())])\n\n\n    # \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435\n    preprocessing = ColumnTransformer(transformers=[('str_cat',str_cat_transformer,str_cat_features),\n                                                    ('int_cat',int_cat_transformer, int_cat_features),\n                                                    ('num_col', numerical_transformer, numerical_features)])\n\n\n    # pipeline \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u044d\u0442\u0430\u043f\u0430 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\n    pipe = Pipeline(steps=[('preprocessing', preprocessing),\n                           ('model', model)])\n    \n    return pipe\n\ndef cross_validate(model,scoring='neg_root_mean_squared_error'):\n    # training features and lables\n    X = train_df.drop(columns=['SalePrice'])\n    y = train_df['SalePrice'] \n    # cross validation score\n    cv_score = cross_val_score(model, X, y, scoring=scoring)\n    return cv_score.mean()","ca397f5a":"model_list = [('RandomForestRegressor',RandomForestRegressor(random_state=42)),\n              ('Lasso',Lasso(tol=1e-2)),\n              ('Ridge',Ridge()),\n              ('ElasticNet',ElasticNet()),\n              ('XGBRegressor',XGBRegressor())\n             ]\n\nmodel_mean_rmse = {}\nfor name, model in model_list:\n    pipeline = build_model_pipeline(model)\n    mean_cv_score = cross_validate(pipeline)\n    model_mean_rmse[name]=mean_cv_score","3fefb8b6":"for k,v in model_mean_rmse.items():\n    print(f'{k :25} RMSE {-v}')","3432f9b1":"xgb_pipe = build_model_pipeline(XGBRegressor(n_estimators=350, learning_rate=0.05, max_depth=5, subsample = 0.7, colsample_bytree = 0.5))\ncv_mean_score = cross_validate(xgb_pipe)\nprint('cv mean score',cv_mean_score)","22914e7c":"final_model = XGBRegressor(n_estimators=350, learning_rate=0.05, max_depth=5, subsample = 0.7, colsample_bytree = 0.5)\nmodel_pipe = build_model_pipeline(final_model)\n# \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\nX = train_df.drop(columns=['SalePrice'])\ny = train_df['SalePrice'] \n# \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\nmodel_pipe.fit(X,y)\n# \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u043e\u0432\npredictions = model_pipe.predict(test_df)","4d3139bf":"output = pd.DataFrame({'Id':test_df['Id'], 'SalePrice':predictions})\noutput = output.to_csv('submission.csv',index=False)\nprint('Submission saved successfully')","abb50de2":"\u0423\u0442\u0438\u043b\u0438\u0442\u0430 \u0434\u043b\u044f \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u044f \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u043c\u043d\u043e\u0433\u043e \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043f\u043e\u043b\u0435\u0439","7e558877":"# **\u0418\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445**","cc4a0b24":"# **\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u043e\u0432 \u043d\u0430 \u043d\u0430\u0431\u043e\u0440\u0435 \u0442\u0435\u0441\u0442\u043e\u0432**","435d8859":"# **\u0421\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u0435 \u041c\u043e\u0434\u0435\u043b\u0438**","e342db3e":"**\u041f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430**\n\n* \u0426\u0435\u043b\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u0430\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f \n * \u0412\u043c\u0435\u043d\u044f\u0442\u044c \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f, \u0430 \u0437\u0430\u0442\u0435\u043c \u043e\u0434\u043d\u0443 \u0433\u043e\u0440\u044f\u0447\u0443\u044e \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u043a\u0443\n\n* \u041a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f \u0441\u0442\u0440\u043e\u043a\n * \u0412\u043c\u0435\u043d\u044f\u0442\u044c \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f, \u0430 \u0437\u0430\u0442\u0435\u043c \u043e\u0434\u043d\u0443 \u0433\u043e\u0440\u044f\u0447\u0443\u044e \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u043a\u0443\n\n* \u0427\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b\n * \u0412\u043c\u0435\u043d\u0438\u0442\u0435 \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f, \u0437\u0430\u0442\u0435\u043c \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u0443\u0439\u0442\u0435 (0 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0439 \u0432 \u0435\u0434\u0438\u043d\u0438\u0446\u0430\u0445 \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u044f)","2802d02d":"**\u0423\u0442\u0438\u043b\u0438\u0442\u0430 \u0434\u043b\u044f \u0432\u044b\u0431\u043e\u0440\u0430 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0439**","b6e1ecaa":"# **\u041c\u043e\u0434\u0435\u043b\u044c Pipeline**","17fbd211":"**\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0443\u0442\u0438\u043b\u0438\u0442\u044b \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439**"}}