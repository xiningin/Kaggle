{"cell_type":{"8e0d30f1":"code","4a7de1bb":"code","e6addbca":"code","3a65776c":"code","46b89379":"code","6e8b1f7f":"code","17ace2de":"code","39eca491":"code","ea9112be":"code","a23e61fd":"code","3f003bcc":"code","2019305c":"code","ebc20d57":"code","b1436a82":"markdown","e69d6419":"markdown","57f8262a":"markdown","9ec06fb8":"markdown","4e0d4e5e":"markdown","9deb70e4":"markdown","f277ffc9":"markdown","97921e92":"markdown","2a6f3b09":"markdown","4ececa1b":"markdown","0e4faf00":"markdown","ae873c7c":"markdown","70fcdc20":"markdown","e47d8a7d":"markdown","d3f32178":"markdown","ba33ab75":"markdown","973ba362":"markdown"},"source":{"8e0d30f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4a7de1bb":"#Create dataframe with just the title, description, and genre list\ndf = pd.read_csv(\"..\/input\/rotten-tomatoes-movies-and-critics-datasets\/rotten_tomatoes_movies.csv\", na_filter=False)\ndf = df[{\"movie_title\",\"genre\",\"movie_info\"}]\n\n#drop blank rows\ndf= df[df['genre'] != ''].reset_index(drop = True) \n\ndf.tail(8)\n","e6addbca":"import re\nimport nltk\nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize \nfrom nltk.corpus import stopwords\n\n# Load stopwords, common words such as  \"a,\" \"the,\" \"it,\" etc.\nstop_words = stopwords.words('english')\n    \n#Initialize stemmer, which will take words and convert words to their \"stem,\" e.g. Playing-> Play\nps = PorterStemmer() \n\n# Removes non-alphabetical characters, whitespaces, and converts all letters to lowercase\n# References: https:\/\/www.analyticsvidhya.com\/blog\/2019\/04\/predicting-movie-genres-nlp-multi-label-classification\/\ndef clean_text(txt): \n    txt= txt.lower()   #lowercase\n    txt= re.sub(\"[^a-zA-Z]\",\" \",txt) #Remove everything except alphabetical characters \n    txt= word_tokenize(txt) #tokenize (split into list and remove whitespace)\n    \n    #initialize list to store clean text\n    clean_text=\"\"\n    \n    #iterate over each word\n    for w in txt:      \n        #remove stopwords\n        if w not in stop_words:\n            #stem=ps.stem(w) #stem \n            stem=w\n            clean_text += stem +\" \" \n    return clean_text\n\n\nmovie_info_new=[] #declare a list to hold new movies\n\nfor cell in df['movie_info']:    \n    txt= clean_text(cell)\n    movie_info_new.append(txt)\n    \n#add new info column to the dataframe\ndf['movie_info_new'] = movie_info_new \ndf.tail()","3a65776c":"from sklearn.preprocessing import MultiLabelBinarizer\nimport json\n\n## format genre column to be in the form of a list rather than string\n\ngenre_new=[] #declare a list\nfor cell in df['genre']:\n    cell=cell.replace(\" \", \"\") #remove whitespace\n    cell=cell.replace(\"&\", \"& \") #add whitespace back in for ampersands\n    genre_new.append(cell.split(\",\")) #for each genre cell, create a list of items from the original string, using a comma as a delimeter\n    \n#add new genre column to the dataframe\ndf['genre_new'] = genre_new \n\n## MultiLabelBinarizer takes an iterable list and turns it into columns with binary values that represent the list.\n## For example, [Comedy, Drama] -> Comedy and Drama columns with a value of 1, all other columns with a value of 0\n\n#initialize MultiLabelBinarizer \nmlb = MultiLabelBinarizer() \n\n#transform the genre_new column to a series of columns with binary values\nbinary_labels=pd.DataFrame(mlb.fit_transform(df['genre_new']),columns=mlb.classes_) \n\n#order columns alphabetically\nbinary_labels=binary_labels.sort_index(axis=1) \n\nbinary_labels.tail()\n\n","46b89379":"#bring data frames together\nmovies = df.merge(binary_labels, how='inner', left_index=True, right_index=True)\n\n#Drop non-properly formatted columns\nmovies= movies.drop(columns=['genre', 'movie_info','genre_new'])\n\nmovies.tail(7)","6e8b1f7f":"import seaborn as sns\nimport matplotlib.pyplot as plt\ncategories = list(binary_labels.columns.values)\nax= sns.barplot(binary_labels.sum().values, categories)\n\nplt.title(\"Movies for each genre\", fontsize=24)\nplt.ylabel('Genre', fontsize=18)\nplt.xlabel('Number of movies tagged with genre', fontsize=18)\n#adding the text labels\nrects = ax.patches\nlabels = binary_labels.sum().values\nplt.show()","17ace2de":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\n# split dataset into training and validation set\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)\nxtrain, xval, ytrain, yval = train_test_split(movies['movie_info_new'], binary_labels, test_size=0.2, random_state=9)\n\n# create TF-IDF features\n# TF-IDF = Term frequency - inverse document frequency\n# Used to predict how important a word is for a document\n# https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf\nxtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)\nxval_tfidf = tfidf_vectorizer.transform(xval)","39eca491":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import accuracy_score\n\n#Run Logistic Regrssion\nlogreg = LogisticRegression()\nlogreg_classifier = OneVsRestClassifier(logreg)\n\n# fit model on train data\nlogreg_classifier.fit(xtrain_tfidf, ytrain)\n\n# make predictions for validation set\npredictions = logreg_classifier.predict(xval_tfidf)\n\n# evaluate performance\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy score for Logistic Regression:\")\nprint(accuracy_score(yval, predictions))","ea9112be":"from sklearn.metrics import classification_report\n\n#Show precision and recall per genre\nprint(classification_report(yval, predictions, target_names=binary_labels.columns))","a23e61fd":"# Using Gaussian Naive Bayes \nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import GaussianNB\n\n\n# initialize binary relevance multi-label classifier\n# with a gaussian naive bayes base classifier\nclassifier = BinaryRelevance(GaussianNB())\n# train\nclassifier.fit(xtrain_tfidf, ytrain)\n# predict\npredictions = classifier.predict(xval_tfidf)\n\nprint(\"Accuracy score for Gaussian Naive Bayes:\")\nprint(accuracy_score(yval, predictions))\n\nprint(\"Individual genre predictions:\")\nprint(classification_report(yval, predictions, target_names=binary_labels.columns))","3f003bcc":"#Make smaller dataset with only the 4 most common genres\nmovies_02= movies[['movie_info_new', 'Action& Adventure', 'Drama', 'Comedy', 'Mystery& Suspense']]\nbinary_labels_02=movies[['Action& Adventure', 'Drama', 'Comedy', 'Mystery& Suspense',]]\nmovies_02.tail(7)\n","2019305c":"tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)\n\n# split dataset into training and validation set\nxtrain_02, xval_02, ytrain_02, yval_02 = train_test_split(movies_02['movie_info_new'], binary_labels_02, test_size=0.2, random_state=9)\n\n# create TF-IDF features\nxtrain_tfidf_02 = tfidf_vectorizer.fit_transform(xtrain_02)\nxval_tfidf_02 = tfidf_vectorizer.transform(xval_02)\n\n#Run Logistic Regression\nlog_reg_02 = LogisticRegression()\nlogreg_classifier_02 = OneVsRestClassifier(log_reg_02)\n\n# fit model on train data\nlogreg_classifier_02.fit(xtrain_tfidf_02, ytrain_02)\n\n# make predictions for validation set\npredictions_02 = logreg_classifier_02.predict(xval_tfidf_02)\n\n# evaluate performance\nprint(\"Accuracy score for Logistic Regression with only 4 genres:\")\nprint(logreg_classifier_02.score(xval_tfidf_02, yval_02))\nprint(classification_report(yval_02, predictions_02, target_names=binary_labels_02.columns))","ebc20d57":"# Using Gaussian Naive Bayes \nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\n# initialize binary relevance multi-label classifier\n# with a gaussian naive bayes base classifier\nclassifier = BinaryRelevance(GaussianNB())\n# train\nclassifier.fit(xtrain_tfidf_02, ytrain_02)\n# predict\npredictions_03 = classifier.predict(xval_tfidf_02)\n\nprint(\"Accuracy score for Naive Bayes with only 4 genres:\")\nprint(accuracy_score(yval_02, predictions_03))\nprint(classification_report(yval_02, predictions_03, target_names=binary_labels_02.columns))","b1436a82":"## Step 1: Data Preparation & Cleaning\n\nIn this section, I will format the data to prepare it to be used by the text categorization algorithm.\nI will also clean up the \"movie info\" column to turn all the text lowercase and remove punctuation.","e69d6419":"As expected, some genres did much better than others. You can see that genres with small sample sizes such as Faith& Spirituality and Sports& Fitness have scores of 0 in both precision and recall (https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall). \nOthers with higher sample sizes such as Comedy and Drama did better. This makes sense - the more samples the algorithm has, the better it can train the model.\n\nNext, I will try a different classifier - Gaussian Naive Bayes (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html)","57f8262a":" ## What is the predicted accuracy per genre?\n \nSince the accuracy score counts a correct prediction as one that correctly identifies every genre lebel for a particular movie, I am also curious to see the prediction accuracy per genre.","9ec06fb8":"The overall accuracy score for Naive Bayes went up as well from 0.036 to 0.186, which is much better but still not great. As you can see, we are still getting a lot of false positives and false negatives.","4e0d4e5e":"# Run Logistic Regression and Naive Bayes classifiers\n\nHere I will  split the data into test and training data, then run two different types of classifiers and see the results.\n\n","9deb70e4":"# Trying with a smaller subset of movies\nSince I believe that these poor accuracy scores are due to an uneven distribution of all the genres within the dataset, I am curious to see what happens when only the most common genres in the dataset are used. \n\nI will look at the four most common genres in the dataset: Action & Adventure, Drama, Comedy, and Mystery & Suspense.","f277ffc9":"As you can see, the Gaussian Naive Bayes classifier did even worse than the Logistic Regression and got an accuracy score of .036 - not good! Individual genres seemed to have fared worse than in the Logistic Regression as well.","97921e92":"## Clean movie description text\n\nBelow I will clean movie description text including removing common words, converting it to lowercase, and stemming the words.\n\nreferences:\nhttps:\/\/towardsdatascience.com\/journey-to-the-center-of-multi-label-classification-384c40229bff\nhttps:\/\/chrisalbon.com\/machine_learning\/preprocessing_text\/remove_stop_words\/","2a6f3b09":"Seems like this model was not able to accurately predict the labels very well as it got a low score of 0.198 accuracy. This score evaluates exact matches (all labels are correctly predicted for a single item) for multi-label classification, and the highest value is 1.\nreference: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html","4ececa1b":"First, I will try out the Logistic Regression classifier.","0e4faf00":"That's better! By only using genres that had a large enough representation in the dataset, the accuracy score improved from 0.198 to 0.454. As expected, the per-genre precision and recall did not change (since all labels are predicted independently from one another).","ae873c7c":"# References\n\n\n\nI used several articles to guide me in this project, often pulling code directly from the examples:\n* Followed this overall process: https:\/\/towardsdatascience.com\/multi-label-text-classification-with-scikit-learn-30714b7819c5\n* Followed a lot of the data cleaning techniques here: https:\/\/towardsdatascience.com\/journey-to-the-center-of-multi-label-classification-384c40229bff\n* https:\/\/www.analyticsvidhya.com\/blog\/2019\/04\/predicting-movie-genres-nlp-multi-label-classification\/\n* https:\/\/machinelearningmastery.com\/what-is-imbalanced-classification\/","70fcdc20":"# Learnings & Reflection\n\nAlthough I was not able to get very accurate predictions from these classifiers, I still feel great that I was able to apply them to this project. I believe that with a large set of properly tagged survey comment data, I could easily take this same code and apply it to that problem.\n\nI am also not sure why the Logistic Regression did so much better than the Gaussian Naive Bayes classifier. I ran out of time to research these more in depth for this project, but in the future I think I should choose classifiers more carefully. I chose these because they came up most often in multi-label text classification examples I found online.\n\nAlso, maybe Rotten Tomatoes movie descriptions are just that useful for predicting genre. I would be interested to see if these classifiers would do better given full movie scripts. I suspect they would.","e47d8a7d":"# Overview\nIn this project, I will attempt to use Machine Learning techniques to cateorize text data with multiple labels. I will use multi-label classification techniques, following some of the techniques found in these articles:\nhttps:\/\/towardsdatascience.com\/multi-label-text-classification-with-scikit-learn-30714b7819c5\nhttps:\/\/www.analyticsvidhya.com\/blog\/2019\/04\/predicting-movie-genres-nlp-multi-label-classification\/\nhttps:\/\/medium.com\/coinmonks\/multi-label-classification-blog-tags-prediction-using-nlp-b0b5ee6686fc\n\nThe UX application of this would be to categorize qualitative survey data from open-ended text box answers. However, I currently don't have a categorized dataset from work that is large enough to train and test a model. So instead, I will use a proxy: a movie dataset from Rotten Tomatoes which has movie descriptions (analogous to comments) and genre tags (analogous to the tags I would have in survey data). \n\nDataset: https:\/\/www.kaggle.com\/stefanoleone992\/rotten-tomatoes-movies-and-critics-datasets\n\n","d3f32178":"## Turning genre list into set of columns of binary data\nSince I will be using models that predict binary values, I will turn the genre list column into multiple binary columns.","ba33ab75":"# 2. Visualize Existing Data","973ba362":"The data set is imbalanced, meaning that not all the categories are equally represented.\nFrom, https:\/\/machinelearningmastery.com\/what-is-imbalanced-classification\/:\n\"Most machine learning algorithms for classification predictive models are designed and demonstrated on problems that assume an equal distribution of classes. This means that a naive application of a model may focus on learning the characteristics of the abundant observations only, neglecting the examples from the minority class...\"\n\nTherefore, I suspect that the classifiers will have a difficult time classifying less common genres such as \"Cult Movies\" and will be more accurate for well-represented genres like Drama and Action & Adventure."}}