{"cell_type":{"e5349f51":"code","a2141fc1":"code","62cf85e0":"code","dedf51c1":"code","652ed42e":"code","4de65147":"code","07dd1c24":"code","cffb5e15":"code","32c6f71c":"code","8b6badbd":"code","efa9aca3":"code","9ca50fe7":"code","cc71b52c":"code","45a0035e":"code","6390784a":"code","4e9e1f19":"code","23c6f602":"code","b116e3ff":"code","23ba37ac":"code","1c3bbe44":"code","e7139297":"code","0f37b57b":"code","34d4ea88":"code","5e52fdc0":"code","649d660c":"code","f65edb47":"markdown","436937ff":"markdown","13ccf31a":"markdown","ed212a9c":"markdown","9ef0db4b":"markdown","b5b873ec":"markdown","a7db69ed":"markdown","5915d9b6":"markdown","b5429786":"markdown","92642547":"markdown"},"source":{"e5349f51":"from sklearn.model_selection import train_test_split\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport torchvision\nfrom PIL import Image, ImageOps\nfrom torchvision import datasets, models, transforms\nfrom torch.optim import lr_scheduler\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\nimport copy\nimport matplotlib.image as mpimg\n\n#metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix","a2141fc1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","62cf85e0":"data = pd.DataFrame()\ndata[\"path\"] = os.listdir(\"\/kaggle\/input\/ships-in-satellite-imagery\/shipsnet\/shipsnet\")\nlabels = [1 if x[0]==\"1\" else 0 for x in data[\"path\"].values]\ndata[\"labels\"] = labels\n\n# split_data\ndata_train, data_test = train_test_split(data, test_size=0.2, random_state=1234)\ndata_train = data_train.reset_index()\ndata_test = data_test.reset_index()\n\n\nclass ShipDataset(Dataset):\n    \n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        # load data from dataframe\n        img_path = data.iloc[idx, 0]\n        label = torch.from_numpy(np.array(data.iloc[idx, 1]))\n        \n        img = Image.open(\"\/kaggle\/input\/ships-in-satellite-imagery\/shipsnet\/shipsnet\/\" + img_path)\n        \n        # apply transformations\n        if self.transform:\n            img = self.transform(img)\n\n        sample = (img_path, img, label)\n        \n        return sample\n\n    \n# create transformers for ResNet and a simple CNN (greyscale - gs)\n\ntransformers = {\"train\": transforms.Compose([ \n                    transforms.RandomHorizontalFlip(),\n                    transforms.RandomVerticalFlip(),\n                    transforms.ToTensor(),\n                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n               \"val\": transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n\n\n# grey scale transformer\ntransformers_gs = {\"train\": transforms.Compose([\n                        transforms.Grayscale(num_output_channels=1),\n                        transforms.RandomHorizontalFlip(),\n                        transforms.RandomVerticalFlip(),\n                        transforms.ToTensor(), \n                        transforms.Normalize((0.5, ), (0.5, ))]),\n                   \"val\": transforms.Compose([\n                        transforms.Grayscale(num_output_channels=1),\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.5, ), (0.5, ))])}\n\n\n# create train and validation dataset\ntrain_dataset = ShipDataset(data_train, transform=transformers[\"train\"])\nval_dataset = ShipDataset(data_test, transform=transformers[\"val\"])\n\ntrain_dataset_gs = ShipDataset(data_train, transform=transformers_gs[\"train\"])\nval_dataset_gs = ShipDataset(data_test, transform=transformers_gs[\"val\"])\n\nnp.random.seed(0)\n\n# create dataloaders\ntorch.manual_seed(0)\ndataloaders = {\"train\": torch.utils.data.DataLoader(train_dataset, batch_size=32,\n                                             shuffle=True, num_workers=4),\n               \"val\": torch.utils.data.DataLoader(val_dataset, batch_size=32,\n                                             shuffle=False, num_workers=4)}\ntorch.manual_seed(0)\ndataloaders_gs = {\"train\": torch.utils.data.DataLoader(train_dataset_gs, batch_size=32,\n                                             shuffle=True, num_workers=4),\n                  \"val\": torch.utils.data.DataLoader(val_dataset_gs, batch_size=32,\n                                             shuffle=False, num_workers=4)}\n\n\ndataset_sizes = {\"train\": len(train_dataset), \"val\": len(val_dataset)}","dedf51c1":"# set device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","652ed42e":"def train_model(model, criterion, optimizer, scheduler, dataloader, num_epochs=25):\n    \"\"\"\n    train a CNN\n    \n    model (torch model): CNN to train\n    \n    criterion (torch loss): loss function\n    \n    scheduler (lr_scheduler): Scheduler used during training \n    \n    dataloader (dict containing \"train\" and \"val\" dataloader): dataloaders used during training\/evaluation\n    \n    num_epochs (int): specifies number of training epochs\n    \"\"\"\n    \n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for path, inputs, labels in dataloader[phase]:\n\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    \n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model if best performance on val_dataset\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","4de65147":"\n# Pretrained Resnet 18 \n\nmodel_conv = torchvision.models.resnet18(pretrained=True)\n\n# replace output layer output layer (two classes: ship - no ship)\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_ftrs, 2)\n\nmodel_conv = model_conv.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Small lr, we only have to fine tune the weights\noptimizer_conv = optim.SGD(model_conv.parameters(), lr=0.0001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n\n\n\n# simple cnn model from scratch\n\nclass SimpleCnn(nn.Module):\n    \n    def __init__(self, in_channel):\n        \n        super(SimpleCnn, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channel, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=4))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc1 = nn.Linear(5 * 5 * 128, 128)\n        self.fc2 = nn.Linear(128, 2)\n    \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out\n\n# train simple cnn on greyscale\nsimpleCNN_gs = SimpleCnn(in_channel = 1)\nsimpleCNN_gs = simpleCNN_gs.to(device)\n\n# train simple cnn on color\nsimpleCNN = SimpleCnn(in_channel = 3)\nsimpleCNN = simpleCNN.to(device)\n\n# Loss and optimizer\ncriterion_gs = nn.CrossEntropyLoss()\noptimizer_conv_gs = optim.SGD(simpleCNN_gs.parameters(), lr=0.01, momentum=0.9)\nexp_lr_scheduler_gs = lr_scheduler.StepLR(optimizer_conv_gs, step_size=10, gamma=0.1)\n\ncriterion_col = nn.CrossEntropyLoss()\noptimizer_conv_col = optim.SGD(simpleCNN.parameters(), lr=0.01, momentum=0.9)\nexp_lr_scheduler_col = lr_scheduler.StepLR(optimizer_conv_col, step_size=10, gamma=0.1)","07dd1c24":"# Train models, uncomment the model that should be trained\n\nmodel_conv = train_model(model_conv, criterion, optimizer_conv,\n                        exp_lr_scheduler,dataloaders, num_epochs=15)\n\n","cffb5e15":"# train simple cnn on grey scale images\n\nsimple_cnn_gs = train_model(simpleCNN_gs, criterion_gs, optimizer_conv_gs,\n                        exp_lr_scheduler_gs, dataloaders_gs, num_epochs=15)","32c6f71c":"# train simple cnn on colored images\n\nsimple_cnn = train_model(simpleCNN, criterion_col, optimizer_conv_col,\n                        exp_lr_scheduler_col, dataloaders, num_epochs=15)","8b6badbd":"pytorch_total_params = sum(p.numel() for p in simple_cnn_gs.parameters() if p.requires_grad)\npytorch_total_params","efa9aca3":"def calculate_metrics(model, dataloader):\n    \n    \"\"\"\n    calculate Acc, Precission, Recall, F1 and Confusion matrix of one models predictions\n    \n    model (torch model): the model that is evaluated\n    \n    dataloader (dict containing \"train\" and \"val\" dataloader\"): dataloader belonging to the stated model\n    \n    \n    \"\"\"\n    \n    \n    \n    # set model to evaluation mode\n    model.eval()\n    \n    all_labels = []\n    all_preds = []\n    incorrect_examples = []\n    incorrect_paths = []\n    \n    for paths, inputs, labels in dataloader[\"val\"]:\n        all_labels.append(labels.numpy())\n        \n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        paths = np.array(paths)\n        paths_np = paths\n\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.append(preds.cpu().numpy())\n        \n        \n        # save incorrect images\n        idxs_mask = (preds != labels).view(-1)\n        wrong_images = inputs[idxs_mask].cpu().numpy()\n        wrong_paths = paths_np[idxs_mask.cpu()]\n        \n        if wrong_images.size!=0:\n            incorrect_paths.append(wrong_paths)\n        \n    \n    all_labels = np.concatenate(all_labels)\n    all_preds = np.concatenate(all_preds)\n    \n    print(\"Accuracy: {}\".format(accuracy_score(all_labels, all_preds)))\n    print(\"Precission: {}\".format(precision_score(all_labels, all_preds)))\n    print(\"Recall: {}\".format(recall_score(all_labels, all_preds)))\n    print(\"F1: {}\".format(f1_score(all_labels, all_preds)))\n    \n    conf_mat = confusion_matrix(all_labels, all_preds)\n    print(conf_mat)\n    \n    \n    wrong_sample_paths = []\n    for arr in incorrect_paths:\n        for path in arr:\n            wrong_sample_paths.append(path)\n    \n    return wrong_sample_paths\n\n# calculate metrics for simple cnn\n\nprint(\"simple CNN greyscale\")\nwrong_paths_gs= calculate_metrics(simple_cnn_gs, dataloaders_gs)\nprint(\"----------------------\")\n\nprint(\"simple CNN color\")\nwrong_paths_color= calculate_metrics(simple_cnn, dataloaders)\nprint(\"----------------------\")\n\nprint(\"ResNet\")\nwrong_paths_ResNet= calculate_metrics(model_conv, dataloaders)\nprint(\"----------------------\")\n        ","9ca50fe7":"# function to plot incorrectly classified images\n\ndef plot_wrong(wrong_paths):\n    \n    fig, axes = plt.subplots(len(wrong_paths), figsize=((10,10)))\n    fig.tight_layout()\n    \n    for idx, img in enumerate(wrong_paths):\n        \n        pltimg = mpimg.imread(\"\/kaggle\/input\/ships-in-satellite-imagery\/shipsnet\/shipsnet\/\" + img)\n        \n        if len(wrong_paths) == 1:\n            axes.imshow(pltimg)\n        else:\n            axes[idx].imshow(pltimg)\n            axes[idx].title.set_text(\"Predicted: \" + str(np.abs(int(img[0])-1)) + \" - truth: \" + img[0])","cc71b52c":"# look at wrong images of gs model (sometimes the model makes no mistakes, therefore the plot may be empty - see confusion matrix)\nplot_wrong(wrong_paths_gs)","45a0035e":"# look at wrong images of color model(sometimes the model makes no mistakes, therefore the plot may be empty - see confusion matrix)\nplot_wrong(wrong_paths_color)","6390784a":"# look at wrong images of ResNet model(sometimes the model makes no mistakes, therefore the plot may be empty - see confusion matrix)\nplot_wrong(wrong_paths_ResNet)","4e9e1f19":"import sklearn\nimport shap\nfrom sklearn.model_selection import train_test_split\nimport json\nimport cv2","23c6f602":"shap_loader = {\"train\": torch.utils.data.DataLoader(train_dataset, batch_size=128,\n                                             shuffle=True, num_workers=4),\n                  \"val\": torch.utils.data.DataLoader(val_dataset, batch_size=256,\n                                             shuffle=False, num_workers=4)}\n\nbatch = next(iter(shap_loader[\"val\"]))\npaths, images, labels = batch\n\n# shap background\nbackground = images[:200]\n\n#look at images containing ship (Not in background !)\nships = np.where(labels.numpy() == 1)[0]\nships = ships[ships > 199]\n\nshap_plot = []\n\nfor i in ships:\n    \n    shap_plot.append(mpimg.imread(\"\/kaggle\/input\/ships-in-satellite-imagery\/shipsnet\/shipsnet\/\" + paths[i]))\n\nplt_imgs = np.stack(shap_plot, axis = 0)\n\ntest_images = images[ships]\n\nbackground = background.to(device)\ntest_images = test_images.to(device)\n\ne = shap.DeepExplainer(simple_cnn, background)\nshap_values = e.shap_values(test_images)\n\nshap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\n\n# plot shap results\nplt.figsize=(20,30)\nshap.image_plot(shap_numpy, plt_imgs)","b116e3ff":"# shap on gs\n\nshap_loader = {\"train\": torch.utils.data.DataLoader(train_dataset_gs, batch_size=128,\n                                             shuffle=True, num_workers=4),\n                  \"val\": torch.utils.data.DataLoader(val_dataset_gs, batch_size=256,\n                                             shuffle=False, num_workers=4)}\n\nbatch = next(iter(shap_loader[\"val\"]))\npaths, images, labels = batch\n\n# shap background\nbackground = images[:200]\n\n#look at images containing ship (Not in background !)\nships = np.where(labels.numpy() == 1)[0]\nships = ships[ships > 199]\n\nshap_plot = []\n\ntest_images = images[ships]\n\nbackground = background.to(device)\ntest_images = test_images.to(device)\n\ne = shap.DeepExplainer(simple_cnn_gs, background)\n\nshap_values = e.shap_values(test_images)\n\nshap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\ntest_numpy = np.swapaxes(np.swapaxes(test_images.cpu().numpy(), 1, -1), 1, 2)\n# plot shap results\nplt.figsize=(20,30)\nshap.image_plot(shap_numpy, -test_numpy)","23ba37ac":"def rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\nshap_loader = {\"train\": torch.utils.data.DataLoader(train_dataset_gs, batch_size=128,\n                                             shuffle=True, num_workers=4),\n                  \"val\": torch.utils.data.DataLoader(val_dataset_gs, batch_size=256,\n                                             shuffle=False, num_workers=4)}\n\nbatch = next(iter(shap_loader[\"val\"]))\npaths, images, labels = batch\n\n# shap background\nbackground = images[:200]\n\n#look at images containing ship (Not in background !)\nships = np.where(labels.numpy() == 1)[0]\nships = ships[ships > 199]\n\n\nshap_plot = []\n\nfor i in ships:\n    \n    shap_plot.append(rgb2gray(mpimg.imread(\"\/kaggle\/input\/ships-in-satellite-imagery\/shipsnet\/shipsnet\/\" + paths[i])))\n\nplt_imgs = np.stack(shap_plot, axis = 0)\n\ntest_images = images[ships]\n\nbackground = background.to(device)\ntest_images = test_images.to(device)\n\ne = shap.DeepExplainer(simple_cnn_gs, background)\nshap_values = e.shap_values(test_images)\n\nclass_names = {\"0\": \"no_ship\",\n               \"1\": \"ship\"}\n\nshap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\n\n\n# plot shap results\nshap.image_plot(shap_numpy, plt_imgs)","1c3bbe44":"import cv2\nimport math","e7139297":"large_images = os.listdir(\"\/kaggle\/input\/ships-in-satellite-imagery\/scenes\/scenes\")","0f37b57b":"def sliding_window(image, stepSize, windowSize):\n    # slide a window across the image\n    \n    for y in range(0, image.shape[0], stepSize):\n        for x in range(0, image.shape[1], stepSize):\n            # yield the current window\n            yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])","34d4ea88":"def non_max_suppression(boxes, o_tresh=0.3):\n    \n    \"\"\"\n    execute non max supression on list of potential bounding boxes\n    \n    \"\"\"\n    \n    \n    if len(boxes) == 0:\n        return []\n    if boxes.dtype.kind == \"i\":\n        boxes = boxes.astype(\"float\")\n\n    pick = []\n    #[[x,y],[x+size,y+size]]\n    x1 = boxes[:,0,0]\n    y1 = boxes[:,0,1]\n    x2 = boxes[:,1,0]\n    y2 = boxes[:,1,1]\n\n    overlapThresh = 0.3\n\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n    idxs = np.argsort(y2)\n\n    while len(idxs) > 0:\n        last = len(idxs) - 1\n        i = idxs[last]\n        pick.append(i)\n\n        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n\n        w = np.maximum(0, xx2 - xx1 + 1)\n        h = np.maximum(0, yy2 - yy1 + 1)\n\n        overlap = (w * h) \/ area[idxs[:last]]\n\n        idxs = np.delete(idxs, np.concatenate(([last],\n            np.where(overlap > overlapThresh)[0])))\n\n    return boxes[pick].astype(\"int\")\n","5e52fdc0":"def recognize_ships(model, transformer, image_path, image_name, o_tresh=0.3):\n    # load image\n    base_image = Image.open(image_path).convert('RGB')\n    base_image = np.array(base_image)\n    \n    #print(base_image.shape)\n    \n    image_bbs = []\n    \n    model.eval()\n    \n    for factor in [0.9, 1, 1.1]:\n        \n        #print(factor)\n        \n        scaled_image = cv2.resize(base_image, (0,0), fx=factor, fy=factor) \n        \n        # sliding windows over image. calculate predictions at each position and save bounding boxes associated with label \"ship\"\n        for (x, y, window) in sliding_window(scaled_image, stepSize=10, windowSize=(80, 80)):\n            if window.shape[0] != 80 or window.shape[1] != 80:\n                continue\n            \n            torch_image = Image.fromarray(window)\n            \n            torch.set_grad_enabled(False)\n            \n            \n            # add \"batch dimension\"\n            transformed_image = transformer[\"val\"](torch_image).unsqueeze(0)\n            inputs = transformed_image.to(device)\n            \n            # calculate model output \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            preds = preds.cpu().numpy()[0]\n            \n            color = (255, 0, 0)\n\n            # only save bounding box if model recognizes ship\n            if preds == 1:\n                image_bbs.append([[int(x * 1\/factor), int(y * 1\/factor)], [int((x+80) * 1\/factor), int((y+80) * 1\/factor)]])\n                \n    \n    \n    final_bbs = non_max_suppression(np.array(image_bbs), o_tresh)\n    \n    \n    image = cv2.imread(image_path)\n    \n    for coords in final_bbs:\n        \n        x1 = int(coords[0][0])\n        y1 = int(coords[0][1])\n        x2 = int(coords[1][0])\n        y2 = int(coords[1][1])\n        \n        # draw rectangles of final bbs\n        image = cv2.rectangle(image, (x1, y1), (x2,y2), (0,255,0), 2)\n\n    cv2.imwrite(image_name + \".png\", image) \n    \n\n    \n    return image_bbs","649d660c":"# simple_cnn - transformers for color model\n# simple_cnn_gs - transformers_gs for greyscale\n\n\n# set treshold for non max supression\ntresh = 0.1\nmodel = simple_cnn_gs\ntransformers_ = transformers_gs\n\n# results are in output\/kaggle\/working\ni = 0\nfor name in large_images[0:]:\n    print(i)\n    bounding_boxes = recognize_ships(model, transformers_, \"\/kaggle\/input\/ships-in-satellite-imagery\/scenes\/scenes\/\"+name, \"image_\" + str(i), o_tresh = tresh)\n    i = i+1\n    ","f65edb47":"# Large satellite images\n\nIn this section we try to detect ships in large satellite images","436937ff":"**Create Dataloaders**","13ccf31a":"Shap for cnn with gs input","ed212a9c":"We can see that the image region containing the ship \"pushes\" the model towards the decision \"ship\" (positive values in right column - output 1) and away from the decision \"no-ship\" (negative values in the left column - output 0)","9ef0db4b":"Train models","b5b873ec":"# Shap tests\n\nIn this section we take a quick look at the explainability of the network. We use the Shap algorithm.\nSee [here](https:\/\/github.com\/slundberg\/shap)\nModel: simlpleCNN (color)","a7db69ed":"Shap for cnn with colored input","5915d9b6":"All models archive similar acc, with the simple cnn architecture archieving a lower loss.\nWe decide to use the Simlpe_cnn due to its simple architecture (Occams razor). We report the results on the large satellite images with both simple_cnn models","b5429786":"# calculate metrics\n\nIn this section we take an in-depth look at the classification results of the model","92642547":"# Training\nIn this section the used models are trained"}}