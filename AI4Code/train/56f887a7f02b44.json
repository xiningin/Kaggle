{"cell_type":{"d346206e":"code","ce38d33c":"code","0aa96fb2":"code","db3a8b41":"code","a29e5416":"code","5b1053de":"code","650c3e36":"code","f7e1af7b":"code","60b99299":"code","513d8688":"code","a6a75f64":"code","6a547933":"code","1b9e5889":"code","f566fde8":"code","0abc10fd":"code","470971cd":"code","5e7b3305":"code","fc74d89c":"code","fd028899":"code","a2f1c67f":"code","c5389889":"code","be92dfc1":"code","dd566529":"code","f10934bc":"code","056b797c":"code","b612b533":"code","64b3dc5e":"code","36628ccc":"markdown","cab7a783":"markdown","8b5e1bd4":"markdown","ca29b3d0":"markdown","d1fb5096":"markdown","13eb4fdc":"markdown","6eccc1a1":"markdown","4535d459":"markdown","51d6d901":"markdown","e3a4db99":"markdown","3f15c90d":"markdown","79d5df21":"markdown","44f4779c":"markdown","6efc82c4":"markdown","e8df43f2":"markdown","024e81ef":"markdown"},"source":{"d346206e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image= https=\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only ..\/input\/ directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using Save & Run All \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce38d33c":"# EDA libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","0aa96fb2":"# read train data\ndata = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata.head()","db3a8b41":"# edit train data\ndata_edited = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\ndata_edited.head()","a29e5416":"# Check if there are data missing, and how much of them.\ndata_edited.isna().sum()","5b1053de":"# Check ratio of missing data\ndata_edited.isna().sum()\/len(data_edited)","650c3e36":"# Delete missing Embarked value\ndata_edited.dropna(subset=['Embarked'], inplace = True)\n\n# Fill missing age with median age\ndata_edited['Age'].fillna(data_edited['Age'].median(), inplace = True)","f7e1af7b":"# Check if there are still missing values.\ndata_edited.isna().sum()","60b99299":"# Check the datatypes of features.\ndata_edited.info()","513d8688":"# Convert String Objects to categories and then convert to numbers.\nfor label, content in data_edited.items():\n    if pd.api.types.is_string_dtype(content):\n        data_edited[label] = content.astype('category').cat.as_ordered()\n        data_edited[label] = pd.Categorical(content).codes","a6a75f64":"# Recheck feature dtype.\ndata_edited.info()","6a547933":"# Correlation Matrix Visualization\ncorr_matrix = data_edited.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(\n    corr_matrix,\n    annot=True,\n    fmt='.2f',\n    cmap='YlGnBu'\n);","1b9e5889":"# Models\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Model Evaluators\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV","f566fde8":"# Split data into training and validation\nX = data_edited.drop('Survived', axis=1)\ny = data_edited['Survived']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)","0abc10fd":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)\nmodel.score(X_val, y_val)","470971cd":"# Create model dictionary for looping fit\nmodels = {\n    'LinearSVC': LinearSVC(),\n    'SVC': SVC(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'RandomForestClassifier':RandomForestClassifier(),\n    'LogisticRegression':LogisticRegression()\n}","5e7b3305":"# # Fit and evaluate models\n# def fit_and_score(models, X_train, X_test, y_train, y_test):\n#     np.random.seed(1)\n#     model_scores={}\n#     for name, model in models.items():\n#         model.fit(X_train, y_train)\n#         model_scores[name]=model.score(X_test, y_test)\n#     return model_scores\n","fc74d89c":"# model_scores = fit_and_score(models, X_train, X_val, y_train, y_val)\n# model_scores","fd028899":"# # Visualize the score\n# model_compare = pd.DataFrame(model_scores, index=['accuracy'])\n# model_compare.T.plot.bar();","a2f1c67f":"# New models and model grid dictionary\nmodels = {\n    'RandomForestClassifier': RandomForestClassifier(),\n    'LogisticRegression': LogisticRegression()\n}\n\nlog_reg_grid = {\n    'penalty':['l2', 'none'],\n    'dual': [True, False],\n    'tol': np.logspace(-4, 4, 10),\n    'C': np.logspace(-4, 4, 10),\n    'solver': ['liblinear'],\n}\n\nrf_grid = {\n    'n_estimators': np.arange(10, 1000, 50),\n    'max_depth': [None, 3, 5, 10],\n    'min_samples_split': np.arange(2, 20, 2),\n    'min_samples_leaf': np.arange(1, 20, 2)\n}","c5389889":"# RandomizedSearchCV on RandomForest\n# np.random.seed(1)\n\n# rs_rf_reg = RandomizedSearchCV(\n#     RandomForestClassifier(),\n#     param_distributions=rf_grid,\n#     cv=5,\n#     n_iter=20,\n#     verbose=True\n# )\n\n# rs_rf_reg.fit(X_train, y_train);\n# rs_rf_reg.score(X_val, y_val)","be92dfc1":"# rs_rf_reg.best_params_","dd566529":"# Copy down the best params in RF\n# {'n_estimators': 760,\n#  'min_samples_split': 6,\n#  'min_samples_leaf': 3,\n#  'max_depth': None}","f10934bc":"# RandomizedSearchCV on LogisticRegression\n# np.random.seed(3)\n\n# rs_log_reg = RandomizedSearchCV(\n#     LogisticRegression(),\n#     param_distributions=log_reg_grid,\n#     cv=5,\n#     n_iter=50,\n#     verbose=True\n# )\n\n# rs_log_reg.fit(X_train, y_train);\n# rs_log_reg.score(X_val, y_val)","056b797c":"# rs_log_reg.best_params_","b612b533":"# Comment RandomizedSearchCV about and use the model to predict the test data.\n\npred = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\npred.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace = True)\npred['Age'].fillna(pred['Age'].median(), inplace = True)\npred['Fare'].fillna(pred['Fare'].mean(), inplace = True)\n\nfor label, content in pred.items():\n    if pd.api.types.is_string_dtype(content):\n        pred[label] = content.astype('category').cat.as_ordered()\n        pred[label] = pd.Categorical(content).codes\n\n\nmodel = RandomForestClassifier(n_estimators=760,\n                           min_samples_split=6,\n                           min_samples_leaf=3,\n                           max_depth=None)\n\nmodel.fit(X_train, y_train)\nlabels = model.predict(pred)\n","64b3dc5e":"test = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n\noutput = pd.DataFrame(\n    {'PassengerId': test.PassengerId, 'Survived': labels}\n)\noutput.to_csv('submission.csv', index=False)","36628ccc":"According to common sense, PassengerId, Name, Ticket, Cabin has nothing to do with survival rate, so we just delete these columns and put the edited dataframe into a new variable.","cab7a783":"## 1.3 Data Visualization","8b5e1bd4":"\n## 2.1 Tools","ca29b3d0":"**According to the scores, we will use RandomForestClassifer and LogisticRegression and tune their hyperparameters to get the highest score**","d1fb5096":"Great, now let's make sure all features are numerical","13eb4fdc":"# 0.3 Evaluation\n\n95% Accuracy on validation data.","6eccc1a1":"# 1. EDA","4535d459":"The column `Embarked` has only 2 values missing, we just delete them.\n\nThe column `Age` has about 20% of the missing value, we cannot just delete them, instead, we fill the NaN with median age of the data.","51d6d901":"# 0.1 Problem Def\nUse machine learning to create a model that predicts which passengers survived the Titanic shipwreck.","e3a4db99":"## 1.1 Tools","3f15c90d":"## 1.2 Preprocess data","79d5df21":"`0.741 < 0.80`, we will keep the RandomForestClassifer with the best params\n\n`{'n_estimators': 760,\n 'min_samples_split': 6,\n 'min_samples_leaf': 3,\n 'max_depth': None}`","44f4779c":"# 2. Modeling\n\nhttps=\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html\n\nAccording to Scikit-Learn map, we will try=\n* LinearSVC\n* KNeighboursClassifier\n* RandomForestClassifier\n* SVC\n\nWe will also try LinearRegression which is not in the map but is also a common classifier.\n\nSteps=\n* Prepare Tools\n* Split Data\n* Create Model Library\n* Fit model and evaluate accuracy\n* Choose two models with highest score\n* Parameter tuning with RandomizedSearchCV\n* Choose the model and related params with highest score\n* Make predictions and submit","6efc82c4":"According to correlation matrix, sex effects most on survival rate.","e8df43f2":"# 0.2 Data\nhttps=\/\/www.kaggle.com\/c\/titanic\/data\n\nThe data has been split into two groups=\n\ntraining set (train.csv)\ntest set (test.csv)\n\nSince test labels are not given, I will split training set into training and validation datas.(ratio = 0.2)","024e81ef":"# 0.4 Features\n\n**Data dictionary for knowing some details**\n* survival, Survival, 0 = No, 1 = Yes\n* pclass, Ticket class, 1 = 1st, 2 = 2nd, 3 = 3rd\n* sex, Sex, \n* Age, Age in years, \n* sibsp, # of siblings \/ spouses aboard the Titanic, \n* parch, # of parents \/ children aboard the Titanic, \n* ticket, Ticket number, \n* fare, Passenger fare, \n* cabin, Cabin number, \n* embarked, Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton\n"}}