{"cell_type":{"41b04d6a":"code","f51a548f":"code","369e612e":"code","84d1d84a":"code","71e663b3":"code","76b9d458":"code","ad78b796":"code","6e3c47be":"code","510cbf57":"code","fff12e86":"code","2be5be2b":"code","e7e4f0a3":"code","d2af2248":"code","bbd469b1":"code","b0969ea6":"code","1ac841ae":"code","40e5d789":"code","a542de42":"code","4088c7df":"code","2da53559":"code","1bc191dc":"code","c2311b04":"code","8b27820d":"code","29f1d763":"code","5567c1dd":"code","fecd28f6":"code","7cfdf8e2":"code","ee437732":"code","d5e13f89":"code","b6cd732f":"code","fe1faa84":"code","caf58b90":"code","8873c324":"code","00bcfabe":"markdown","2ba07ee7":"markdown","3b9f3d73":"markdown","6bb92378":"markdown","4a48b956":"markdown","6f935b17":"markdown","8dfe8d43":"markdown","73a4296b":"markdown","33a19a0d":"markdown","15fcb424":"markdown","53c2a515":"markdown","f7400c7c":"markdown","2c6171ae":"markdown","d9f82019":"markdown","77ac21e1":"markdown","89aed3c4":"markdown","6002df08":"markdown","de4ddef9":"markdown","5bbaac6e":"markdown","3e1eff00":"markdown","cbc7bcf2":"markdown","363b4d4a":"markdown"},"source":{"41b04d6a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline \nsns.set_theme(style = \"darkgrid\")\nsns.set_context(\"paper\")\nplt.figure(figsize=(8,6))","f51a548f":"#pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","369e612e":"raw_heart_df = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")","84d1d84a":"print(raw_heart_df)","71e663b3":"print(raw_heart_df.info())","76b9d458":"print(raw_heart_df.describe())","ad78b796":"sns.relplot(x = \"age\", y = \"platelets\", hue = \"DEATH_EVENT\", data = raw_heart_df);","6e3c47be":"raw_heart_df","510cbf57":"sns.relplot(x = \"creatinine_phosphokinase\", y = \"age\", hue = \"DEATH_EVENT\", data = raw_heart_df)","fff12e86":"sns.relplot(x = \"serum_creatinine\", y = \"age\", hue = \"DEATH_EVENT\", data = raw_heart_df)","2be5be2b":"sns.countplot(x = \"DEATH_EVENT\", data = raw_heart_df)","e7e4f0a3":"sns.countplot(x = \"DEATH_EVENT\", hue=\"smoking\", data = raw_heart_df)","d2af2248":"sns.catplot(x = \"smoking\", hue=\"DEATH_EVENT\", col = \"DEATH_EVENT\",data = raw_heart_df, kind=\"count\", height=4, aspect=.7);","bbd469b1":"sns.catplot(x = \"DEATH_EVENT\", y = \"age\", data = raw_heart_df)","b0969ea6":"sns.catplot(x = \"smoking\", y = \"age\", hue = \"DEATH_EVENT\", kind = \"box\", data = raw_heart_df)","1ac841ae":"raw_heart_df.corr()","40e5d789":"plt.figure(figsize = (15,8))\nsns.heatmap(raw_heart_df.corr(), annot=True)","a542de42":"# X -> Dataset with features, y = target column or label\nX = raw_heart_df.drop('DEATH_EVENT', axis = 1)\ny = raw_heart_df['DEATH_EVENT']","4088c7df":"print(\"Shape of X - \", X.shape)\nprint(\"Shape of y - \", y.shape)","2da53559":"print(\"Type of X - \", type(X))\nprint(\"Type of y - \", type(y))","1bc191dc":"print(raw_heart_df)","c2311b04":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.2)","8b27820d":"print(\"Type of X_train - \", type(X_train))\nprint(\"Type of X_test - \", type(X_test))\nprint(\"Type of y_train - \", type(y_train))\nprint(\"Type of y_test - \", type(y_test))","29f1d763":"print(\"Shape of X_train - \", X_train.shape)\nprint(\"Shape of X_test - \", X_test.shape)\nprint(\"Shape of y_train - \", y_train.shape)\nprint(\"Shape of y_test - \", y_test.shape)","5567c1dd":"from sklearn.tree import DecisionTreeClassifier\nhousing_model = DecisionTreeClassifier(max_depth = 2)\nhousing_model.fit(X_train, y_train)","fecd28f6":"predicted_values = housing_model.predict(X_train)\nprint(\"Shape of Predicted values - \", predicted_values.shape)\nprint(\"Shape of true values - \", y_train.shape)","7cfdf8e2":"from sklearn.metrics import accuracy_score\naccuracy_score(predicted_values, y_train)","ee437732":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train, predicted_values)","d5e13f89":"predicted_values = housing_model.predict(X_test)\nprint(\"Shape of Predicted values - \", predicted_values.shape)\nprint(\"Shape of true values - \", y_test.shape)","b6cd732f":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, predicted_values)","fe1faa84":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nhousing_model = DecisionTreeClassifier(random_state = 42)\nhousing_model.fit(X_train, y_train)","caf58b90":"predicted_values = housing_model.predict(X_test)\nscore = accuracy_score(y_test, predicted_values)\nprint(\"Accuracy score is = \", score)\n\nmax_depth_of_tree = housing_model.tree_.max_depth\nprint(\"Max depth of our tree is - \", housing_model.tree_.max_depth)","8873c324":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfor i in range(1, max_depth_of_tree + 1):\n  \n  #Create a model object with different values of max_depth\n  housing_model = DecisionTreeClassifier(max_depth = i, random_state = 42)\n  housing_model.fit(X_train, y_train)\n\n  #Predict values of X_depth for each max_depth\n  predicted_values = housing_model.predict(X_test)\n  score = accuracy_score(y_test, predicted_values)\n\n  print(\"Accuracy for max_depth {} is {}\".format(i, score))\n  print(\"\")","00bcfabe":"Let us get the accuracy for our prediction on test data ","2ba07ee7":"# Predict for test data","3b9f3d73":"Let us check the shape and type of both these parameters","6bb92378":"Our accuracy is gone down with this approach. Max depth of our tree is 9 so let us get accuracy values for all these depths ","4a48b956":"# Preparing data\nPreparing data can have multiple activities like - \n- Clean missing values by replacing by mean, mode, etc. or by removing rows \n- Separate features and labels \n- Perform scaling in case numericals values in different columns are in a huge range which can impact the error calculation \n- Separate train, test and validation data \n\n\nWe have a very small dataset with no missing data and hence we need not perform any operation for missing values. We will perform all other activities as mentioned above ","6f935b17":"# Improving model","8dfe8d43":"# Visualization to understand basic relationships ","73a4296b":"<a href=\"https:\/\/colab.research.google.com\/github\/AMMLRepos\/heart-failure-detection\/blob\/main\/heart_failure_prediction.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","33a19a0d":"# Train the model","15fcb424":"Let us know split the data into train and test dataset","53c2a515":"# Objective \nTo predict a possible death event in a case of heart failure of a person \n\n# Dataset \nWe will use public dataset available on [Kaggle](https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data)\n\n# Major Steps \n- Download data \n- Setup libraries \n- Perform basic analysis \n- Split train and test set \n- Train model \n- Predict \n- Publish and expose as an API\n\n\n","f7400c7c":"Clearly we have 299 rows in X and 12 feature columns. We have 299 rows in y with only 1 column which is the label","2c6171ae":"Other ways to improve the model are - \n- Separate a few features by removing them. Certain features which does not practically impact the target must be removed as they are just a garbage to the model and can reduce its accuracy \n- We have smokers and non smokers and male and female in the model. Sepearating such columns and having two models for them could boost our accuracy.\n- Trying another alogorithm like KNN or random forest to check if the accuracy acheived is much better ","d9f82019":"Our accuracy is just 68% which is not a good model at all. We need to improve our model by changing hyperparameters like depth of the tree","77ac21e1":"Best accuracy in this case is 0.75 which is for depth of 1 and 5. We will choose 5 as a tree of depth one would surely not make good decisions with just 1 depth. ","89aed3c4":"# Predict for training data itself","6002df08":"# Prepare your data for training \n- Split your data in train, validation and testing dataset \n- Seperate features and target label\n\nOur target label is DEATH_EVENT and features are all other parameters available in the dataset. Please note that based on proper analysis and expert advice, we can remove certain columns which domain experts thinks will have no impact on heart failure but since we have no one, we will just go ahead and train our model with all available features. \n\n# Which model to use ?\nThis is a classification problem where we need to detect if patient will be dead(1) or will not be dead (0). There are multiple algorithms for classification problems - \n- Logistic Regression \n- KNN Algorithm\n- Decision Trees\n- Random Forest \n- etc. \n\nWe will use Decision Trees at first place to train the model and see how it works in terms of accuracy.","de4ddef9":"Let us not provide the max depth parameter. In that case it will try to get the max possible depth. Let us see if we have a better accuracy for our test data with no max_depth parameter","5bbaac6e":"Separate features and labels ","3e1eff00":"Let's make a prediction for our train data. Please note that we have trained our model using the same data and hence this should perform well ","cbc7bcf2":"# Let us identify the correlation between all features ","363b4d4a":"Let us get the accuracy score for our predictions made on X_train data"}}