{"cell_type":{"d2f04e1e":"code","d5544f41":"code","de3bf103":"code","557e09f4":"code","2e1ec8ac":"code","d63ffe71":"code","e3956bf0":"code","324ff564":"code","b271dd9d":"code","673ada52":"code","808f805d":"code","d53c7de1":"code","f3b0e0b7":"code","369a7713":"code","fe74ec4b":"code","97e0eae5":"code","447b76f5":"code","17c11f7c":"code","45850b14":"code","9f0e88b2":"code","378e3757":"code","ed249b5c":"code","0f0bb283":"code","5b592cd8":"code","bfd8bde1":"code","84f0985a":"code","26a73ac5":"code","f17abc42":"code","5581fcc0":"code","aa818d32":"code","9d0d0ea3":"markdown","dabbae52":"markdown","8424bbbe":"markdown","bbd5a701":"markdown","b55bc32b":"markdown","83487fca":"markdown","6eae9e7c":"markdown","d5a16716":"markdown"},"source":{"d2f04e1e":"# imports\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split","d5544f41":"input_path = Path('\/kaggle\/input\/tabular-playground-series-nov-2021\/')\nlist(input_path.iterdir())","de3bf103":"train_df = pd.read_csv(input_path\/'train.csv')\n\n# import the 5-fold data that is prepared\ntrain_5fold_df = pd.read_csv('\/kaggle\/input\/tpsnovember-5-fold-data-split\/train_df_5fold.csv')\ntest_df = pd.read_csv(input_path\/'test.csv')\nsubmission_df = pd.read_csv(input_path\/'sample_submission.csv')\n\ntrain_df.shape, train_5fold_df.shape, test_df.shape","557e09f4":"# null values\ntrain_df.isnull().sum().sum(), test_df.isnull().sum().sum()","2e1ec8ac":"# duplicates check\nlen(train_df) - len(train_df.drop(['id', 'target'], axis=1).drop_duplicates())","d63ffe71":"test_id = test_df.loc[:, 'id']\ntrain_target = train_df.loc[:, 'target']\n# train_df.drop(['id', 'target'], axis=1, inplace=True)","e3956bf0":"train_target_counts = train_target.value_counts()\nlabels = train_target_counts.index\ncounts = train_target_counts.values\n\nplt.bar(labels, counts, width=0.4)\nplt.xticks(labels)\nplt.show()","324ff564":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport optuna\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report, auc, roc_auc_score","b271dd9d":"X_train, X_test, y_train, y_test = train_test_split(train_df.drop(['id', 'target'], axis=1), train_target, test_size=0.25,\n                                                    stratify=train_target, random_state=13)\nX_train.shape, X_test.shape","673ada52":"def objective(trial):\n    \"\"\"\n    Objective function to tune XGBoost classifier\n    \"\"\"\n    params = {\n        'tree_method': 'gpu_hist',\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        # 'eta': trial.suggest_float('eta', 1e-8, 1., log=True),\n        'eta': trial.suggest_float('eta', 0.01, 0.3),\n        'gamma': trial.suggest_float('gamma', 1e-8, 1., log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 9, step=2),\n        'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n        'alpha': trial.suggest_float('alpha', 1e-8, 1., log=True),\n        'subsample': trial.suggest_float('subsample', 0.2, 1.),\n        'colsample_bytree': trial.suggest_float('colsample_bytree',0.2, 1.),\n        'use_label_encoder': False,\n    }\n    \n    # KFold split\n    skf = StratifiedKFold(n_splits=5, random_state=13, shuffle=True)\n    cv_scores = []\n    \n    for train_ix, test_ix in skf.split(X_train, y_train):\n        X_train_k, X_test_k = X_train.iloc[train_ix], X_train.iloc[test_ix]\n        y_train_k, y_test_k = y_train.iloc[train_ix], y_train.iloc[test_ix]\n    \n        booster = XGBClassifier(**params)\n        booster.fit(X_train_k, y_train_k, eval_metric='auc', eval_set=[(X_test_k, y_test_k)], verbose=0, early_stopping_rounds=100)\n        preds = booster.predict_proba(X_test)\n        preds = preds[:, 1]\n        cv_scores.append(preds)\n    cv_score = np.mean(cv_scores, axis=0)\n    return roc_auc_score(y_test, cv_score)","808f805d":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50, timeout=600)","d53c7de1":"print(f'Best score: {study.best_value}')\nprint(f'Best params: {study.best_params}')\nxgboost_best_params = study.best_params","f3b0e0b7":"def get_valid_test_preds(model, best_params):\n    \"\"\"\n    build models on separate folds, and predict on hold-out set, using parameters studied from optuna\n    return:\n    valid_preds(pd dataframe): All the hold-out dataset predictions, concatenated\n    test_preds(pd dataframe): Mean of Test predictions by all the models\n    \"\"\"\n    # save X_test predictions by 'id'\n    X_test_preds_final = dict()\n    # save actual test_preds\n    test_preds_final = []\n    n_folds = 5\n\n    for fold_no in range(n_folds):\n        # X_train data prep\n        X_train_ix = train_5fold_df[train_5fold_df.fold_no != fold_no].index\n        X_train = train_df.loc[X_train_ix, :]\n        y_train = X_train.target\n        X_train.drop(['id', 'target'], axis=1, inplace=True)  # drop id & target\n\n        # X_test data prep\n        X_test_ix = train_5fold_df[train_5fold_df.fold_no == fold_no].index\n        X_test = train_df.loc[X_test_ix, :]\n        y_test = X_test.target\n        X_test_ids = X_test.id  # save ids of each test id in each fold\n        X_test.drop(['id', 'target'], axis=1, inplace=True)\n\n        # train model \n        if model == 'xgboost':\n            booster = XGBClassifier(**best_params, use_label_encoder=False, tree_method='gpu_hist',\n                                    objective='binary:logistic', eval_metric='auc')\n            booster.fit(X_train, y_train, eval_metric='auc', eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)\n        elif model == 'lightgbm':\n            booster = LGBMClassifier(**best_params, eval_metric='auc')\n            booster.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)\n        elif model == 'catboost':\n            booster = CatBoostClassifier(**best_params, loss_function='Logloss', eval_metric='AUC')\n            booster.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0)\n\n        # predict on X_test\n        X_test_preds = booster.predict_proba(X_test)\n        X_test_preds = X_test_preds[:, 1]\n        X_test_preds_final.update(zip(X_test_ids, X_test_preds))\n        print(f'Fold no: {fold_no}: roc_score: {roc_auc_score(y_test, X_test_preds)}')\n\n        # predict on test_df\n        test_preds = booster.predict_proba(test_df.drop('id', axis=1))\n        test_preds = test_preds[:, 1]\n        test_preds_final.append(test_preds)\n\n    test_preds_final = np.mean(test_preds_final, axis=0)\n    X_test_preds_df = pd.DataFrame.from_dict(X_test_preds_final, orient='index').reset_index()\n    X_test_preds_df.columns = ['id', 'target']\n    test_preds_df = pd.DataFrame()\n    test_preds_df['id'] = test_id\n    test_preds_df['target'] = test_preds_final\n    return X_test_preds_df, test_preds_df","369a7713":"xgboost_valid_df, xgboost_test_df = get_valid_test_preds('xgboost', xgboost_best_params)\nxgboost_valid_df.shape, xgboost_test_df.shape","fe74ec4b":"def objective(trial):\n    \"\"\"\n    Objective function to tune LGBM classifier\n    \"\"\"\n    params = {\n        'objective': 'binary',\n        'device': 'gpu',\n        'metric': 'auc',\n        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 4, 15),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n        'subsample': trial.suggest_float('subsample', 0.2, 0.95),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.95)\n    }\n    \n    # KFold split\n    skf = StratifiedKFold(n_splits=5, random_state=13, shuffle=True)\n    cv_scores = []\n    \n    for train_ix, test_ix in skf.split(X_train, y_train):\n        X_train_k, X_test_k = X_train.iloc[train_ix], X_train.iloc[test_ix]\n        y_train_k, y_test_k = y_train.iloc[train_ix], y_train.iloc[test_ix]    \n        booster = LGBMClassifier(**params)\n        booster.fit(X_train_k, y_train_k, eval_metric='auc', early_stopping_rounds=30, eval_set=[(X_test_k, y_test_k)], verbose=0)\n        preds = booster.predict_proba(X_test)\n        preds = preds[:, 1]\n        cv_scores.append(preds)\n    cv_score = np.mean(cv_scores, axis=0)\n    return roc_auc_score(y_test, cv_score)","97e0eae5":"study = optuna.create_study(direction='maximize', study_name='lgbm')\nstudy.optimize(objective, n_trials=50, timeout=600)","447b76f5":"print(f'Best study score: {study.best_value}')\nprint(f'Best params: {study.best_params}')\nlgbm_best_params = study.best_params","17c11f7c":"lgbm_valid_df, lgbm_test_df = get_valid_test_preds('lightgbm', lgbm_best_params)\nlgbm_valid_df.shape, lgbm_test_df.shape","45850b14":"def objective(trial):\n    params = {\n        'objective': 'Logloss',\n        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.01, .1),\n        'depth': trial.suggest_int('depth', 1, 12),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'iterations': trial.suggest_int('iterations', 50, 300),\n    }\n    \n    # KFold split\n    skf = StratifiedKFold(n_splits=5, random_state=13, shuffle=True)\n    cv_scores = []\n    \n    for train_ix, test_ix in skf.split(X_train, y_train):\n        X_train_k, X_test_k = X_train.iloc[train_ix], X_train.iloc[test_ix]\n        y_train_k, y_test_k = y_train.iloc[train_ix], y_train.iloc[test_ix]\n    \n        booster = CatBoostClassifier(**params, eval_metric='AUC')\n        booster.fit(X_train_k, y_train_k, eval_set=[(X_test_k, y_test_k)], verbose=0)\n        preds = booster.predict_proba(X_test)\n        preds = preds[:, 1]\n        cv_scores.append(preds)\n    cv_score = np.mean(cv_scores, axis=0)\n    return roc_auc_score(y_test, cv_score)","9f0e88b2":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50, timeout=600)","378e3757":"print(f'Best score: {study.best_value}')\nprint(f'Best params: {study.best_params}')\ncatboost_best_params = study.best_params","ed249b5c":"cb_valid_df, cb_test_df = get_valid_test_preds('catboost', catboost_best_params)\ncb_valid_df.shape, cb_test_df.shape","0f0bb283":"xgboost_valid_df.head()","5b592cd8":"# concatenate valid features\nvalid_features_df = pd.merge(xgboost_valid_df, lgbm_valid_df, on='id', how='inner')\nvalid_features_df = pd.merge(valid_features_df, cb_valid_df, on='id', how='inner', sort='id')\nvalid_features_df.columns = ['id', 'xgb', 'lgbm', 'cb']\nvalid_features_df.head()","bfd8bde1":"# concatenate test features\ntest_features_df = pd.merge(xgboost_test_df, lgbm_test_df, on='id', how='inner')\ntest_features_df = pd.merge(test_features_df, cb_test_df, on='id', how='inner', sort='id')\ntest_features_df.columns = ['id', 'xgb', 'lgbm', 'cb']\ntest_features_df.head()","84f0985a":"def objective(trial):\n    \"\"\"\n    Objective function to tune LGBM classifier\n    \"\"\"\n    params = {\n        'objective': 'binary',\n        'device': 'gpu',\n        'metric': 'auc',\n        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 4, 15),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n        'subsample': trial.suggest_float('subsample', 0.2, 0.95),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.95)\n    }\n    \n    # KFold split\n    skf = StratifiedKFold(n_splits=5, random_state=13, shuffle=True)\n    cv_scores = []\n    \n    X_train, X_test, y_train, y_test = train_test_split(valid_features_df.drop(['id'], axis=1), train_target, test_size=0.25,\n                                                    stratify=train_target, random_state=13)\n    \n    for train_ix, test_ix in skf.split(X_train, y_train):\n        X_train_k, X_test_k = X_train.iloc[train_ix], X_train.iloc[test_ix]\n        y_train_k, y_test_k = y_train.iloc[train_ix], y_train.iloc[test_ix]  \n        booster = LGBMClassifier(**params)\n        booster.fit(X_train_k, y_train_k, eval_metric='auc', early_stopping_rounds=30, eval_set=[(X_test_k, y_test_k)], verbose=0)\n        preds = booster.predict_proba(X_test)\n        preds = preds[:, 1]\n        cv_scores.append(preds)\n    cv_score = np.mean(cv_scores, axis=0)\n    return roc_auc_score(y_test, cv_score)","26a73ac5":"study = optuna.create_study(direction='maximize', study_name='lgbm_meta_model')\nstudy.optimize(objective, n_trials=50, timeout=600)","f17abc42":"best_meta_params = study.best_params\nbest_meta_params","5581fcc0":"test_df_preds = []\nskf = StratifiedKFold(n_splits=5, random_state=13, shuffle=True)\nfor train_ix, test_ix in skf.split(valid_features_df, train_target):\n    X_train, X_test = valid_features_df.drop('id', axis=1).iloc[train_ix], valid_features_df.drop('id', axis=1).iloc[test_ix]\n    y_train, y_test = train_target.iloc[train_ix], train_target.iloc[test_ix]\n    # meta model build\n    booster = LGBMClassifier(**best_meta_params, device='gpu')\n    booster.fit(X_train, y_train, verbose=0, eval_set=[(X_test, y_test)], eval_metric='auc')\n    print(booster.best_score_)\n    test_preds = booster.predict_proba(test_features_df.drop('id', axis=1))\n    test_preds = test_preds[:, 1]\n    test_df_preds.append(test_preds)","aa818d32":"submission_df.target = np.mean(test_df_preds, axis=0)\nsubmission_df.to_csv('submission.csv', index=False)","9d0d0ea3":"# Meta model building \nFollowing models are going to be used for building meta model\n1. XGBoost\n2. CatBoost\n3. LGBM\n\n### I have created a dataset, with 5 folds split. \n### Check it out here -> https:\/\/www.kaggle.com\/tharunreddy\/tpsnovember-5-fold-data-split","dabbae52":"# CatBoost model build","8424bbbe":"# Concatenate the predictions from xgboost, lgbm, catboost","bbd5a701":"## Data sanity check","b55bc32b":"## EDA\n\n1. It is said that, all the variables are continuous\n2. Target variable is binary","83487fca":"# LGBM model build","6eae9e7c":"# XGBoost model build","d5a16716":"## Meta model building"}}