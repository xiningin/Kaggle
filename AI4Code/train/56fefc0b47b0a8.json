{"cell_type":{"ce26b94e":"code","0e49afb4":"code","b76f686b":"code","6337216d":"code","e1504c05":"code","4c63cccc":"code","46742b5f":"code","f5baf9e8":"code","73abd1e6":"code","c00c6114":"code","e9c07a3b":"code","9d7126f1":"code","864a5dda":"code","42691557":"code","8fb2781e":"code","4150b420":"code","0aaf1fad":"code","4e33da6f":"code","00e05122":"code","b7e34534":"code","ea834869":"code","586c4516":"code","5ab66fce":"code","65851cc6":"code","6d4fcda5":"code","ddccfc4c":"markdown","7bdfb725":"markdown","2e8d18e5":"markdown","dfc6854f":"markdown","5092bf5e":"markdown","b748cff7":"markdown","0f397034":"markdown","b9915a17":"markdown","e1bddb8c":"markdown","4848fc90":"markdown","2f164719":"markdown","92d6553f":"markdown","c54ae872":"markdown","d73ee55d":"markdown","e13c23a6":"markdown","f862cfb9":"markdown","3bd2863b":"markdown","6124cf1a":"markdown","1d4c3309":"markdown","900e1435":"markdown","9a7b786d":"markdown","16084b2e":"markdown","2bf6991a":"markdown","7ceb0bc5":"markdown","14ca6fb7":"markdown","79d7a3ea":"markdown","0a49f16c":"markdown","519e9e84":"markdown","e2845dcc":"markdown","fcd561dc":"markdown","18043b57":"markdown","f804e4ff":"markdown","d1fe4ec9":"markdown","b5d6cb17":"markdown","5beb0f5b":"markdown","8fc73de9":"markdown","7d7bc40b":"markdown","5d055a01":"markdown","d308a8fd":"markdown","68f86a21":"markdown","9294eb94":"markdown","8807434f":"markdown","d711b91a":"markdown","dd058c76":"markdown","b8cd0de7":"markdown","20f97987":"markdown"},"source":{"ce26b94e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0e49afb4":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import imagenet_utils\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport os\nimport shutil\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline","b76f686b":"mobile = tf.keras.applications.mobilenet.MobileNet()","6337216d":"def prepare_image(file):\n    img_path = '\/kaggle\/input\/horseimage\/'\n    img = image.load_img(img_path + file, target_size=(224, 224))\n    img_array = image.img_to_array(img)\n    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n    return tf.keras.applications.mobilenet.preprocess_input(img_array_expanded_dims)\n","e1504c05":"from IPython.display import Image\nImage(filename='\/kaggle\/input\/horseimage\/horse.jpg', width=300,height=200) ","4c63cccc":"preprocessed_image = prepare_image('horse.jpg')\npredictions = mobile.predict(preprocessed_image)","46742b5f":"results = imagenet_utils.decode_predictions(predictions)","f5baf9e8":"results","73abd1e6":"train_path = '\/kaggle\/input\/cat-and-dog\/training_set\/training_set'\ntest_path = '..\/input\/cat-and-dog\/test_set\/test_set'","c00c6114":"train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n    .flow_from_directory(directory=train_path, target_size=(224,224), classes=['cats', 'dogs'], batch_size=10)\ntest_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n    .flow_from_directory(directory=test_path, target_size=(224,224), classes=['cats', 'dogs'], batch_size=10, shuffle=False)","e9c07a3b":"mobile = tf.keras.applications.mobilenet.MobileNet()","9d7126f1":"x = mobile.layers[-6].output","864a5dda":"mobile.summary()","42691557":"output = Dense(units=2, activation='softmax')(x)","8fb2781e":"model = Model(inputs=mobile.input, outputs=output)","4150b420":"model.summary()","0aaf1fad":"for layer in model.layers[:-5]:\n    layer.trainable = False","4e33da6f":"model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])","00e05122":"model.fit(x=train_batches,\n          steps_per_epoch=len(train_batches),\n          epochs=10,\n          verbose=2\n)","b7e34534":"test_labels = test_batches.classes","ea834869":"predictions = model.predict(x=test_batches, verbose=0)","586c4516":"def plot_confusion_matrix(cm, classes,\n        normalize=False,\n        title='Confusion matrix',\n        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","5ab66fce":"cm = confusion_matrix(y_true=test_labels, y_pred=predictions.argmax(axis=1))","65851cc6":"cm_plot_labels = ['cat','dog']","6d4fcda5":"plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","ddccfc4c":"We can conclude our fine-tuned model overall did a really great job on the task of classifying images as cats or dogs.","7bdfb725":"On this model, we\u2019re first calling compile and specifying the Adam optimizer with a `learning rate of .0001`. We\u2019re setting the loss to `categorical_crossentropy`, and our metrics just include `accuracy`.\n\nAfter our model is compiled, we\u2019re now going to train the model by calling fit().","2e8d18e5":"Calling classes on test_batches is going to give us the class names (i.e. the labels), for each sample in the test set. This is the reason we specified `shuffle=False` for test_batches","dfc6854f":"Since we\u2019ll be using these static labels that are returned from test_batches.classes, we can\u2019t shuffle our test data each time we use it for predicting because then the labels won\u2019t map correctly to the data.\n\nBy calling `class_indices` on test_batches, we can see the mapping from the underlying class names, cat and dog, to the 0s and 1s. The output of `test_batches.class_indices` looks like this.\n\n* {'cat': 0, 'dog': 1}","5092bf5e":"# Building A Fine-Tuned MobileNet Model","b748cff7":"Since cat was first and then dog in the class indices dictionary, that\u2019s how we specify the order for the confusion matrix labels.\n\nThen, we call the `plot_confusion_matrix()` function that we referenced above, and we pass in our confusion matrix `cm`, the labels `cm_plot_labels`, and we give it the title of 'Confusion Matrix'","0f397034":"We first make a call to `tf.keras.applications.mobilenet.MobileNet()` to obtain a copy of a single pretrained MobileNet with weights that were saved from being trained on ImageNet images. We\u2019re assigning this model to the variable mobile","b9915a17":"Now, we append an output layer that we\u2019re calling output, which will just be a `Dense layer` with 2 output nodes, for cat and dog, and we\u2019ll use the `softmax` activation function.\n\n","e1bddb8c":"Next, we have a function called `prepare_image()` that accepts an image file, and processes the image to get it in a format that the model expects. We\u2019ll be passing each of our images to this function before we use MobileNet to predict on it, so let\u2019s see what exactly this function is doing.","4848fc90":"# Using The Model For Inference","2f164719":"Next, we\u2019re going to use our model to predict on images from our test set that it hasn\u2019t already seen during training or validation.\n\nBefore we run the predictions, we\u2019re going to get and format the labels for the test set, and we need these just in order to plot the confusion matrix we\u2019ll see in a few moments. We don\u2019t actually need the labels to get predictions from the model.","92d6553f":"First, we define `test_labels` to be equal to `test_batches.classes`.","c54ae872":"By training only the last five layers, all the weights in the remaining earlier layers will not be updated during training and instead will be saved with the ImageNet weights from the original MobileNet.\n\nNote that the number of layers that you choose to retrain is, again, one of those things that <font color = 'orange'>varies by situtation<\/font>. Since the original MobileNet model has already generally learned about cats and dogs, we\u2019re not really needing to retrain many layers.","d73ee55d":"Notice the preprocessing_function parameter we\u2019re supplying to ImageDataGenerator. We\u2019re setting this equal to `keras.applications.mobilenet.preprocess_input()`. This is going to do the necessary MobileNet preprocessing on the images obtained from `flow_from_directory()`.\n\nRecall, we talked about this exact function in the above cells and its role in regards to preprocessing images for MobileNet.","e13c23a6":"We\u2019ll be using this to build a new model. This new model will consist of the original MobileNet up to the sixth to last layer. We\u2019re not including the last five layers of the original MobileNet.","f862cfb9":"To flow_from directory(), we\u2019re passing in the path to the data set, the target_size for the images, and the batch_size we\u2019re choosing to use for training.\n\nFor the test_batches variable, we\u2019re also supplying one additional parameter, `shuffle=False`, so that we can later access the corresponding non-shuffled test lables to plot a confusion matrix.\n\nThe data portion is now done. Next, let\u2019s move on to modifying the model.","3bd2863b":"Let\u2019s now get some predictions from MobileNet and see how it performs. We\u2019ll be using some random sample images","6124cf1a":"Note that this code was pulled directly off of [scikit-learn\u2019s](https:\/\/scikit-learn.org\/stable\/) website, so we\u2019re not going to go over the details here.\n\nNext, we create this confusion_matrix object called `cm`, and we set it equal to scikit-learn\u2019s confusion_matrix that we imported at the start of our code in the last episode.","1d4c3309":"Now, we need to choose how many layers we actually want to be trained when we train on cats and dogs.\n\nHere, we are freezing the weights of all the layers except for the last five layers in our new model, meaning that only the last five layers of the model will be trained.","900e1435":"Similar to what we previously implemented with VGG16, we\u2019re going to be fine-tuning MobileNet on images of cats and dogs. The implementation will be pretty similar, but you\u2019ll notice there will be a few differences.\n\nMany different breeds of cats and dogs were included in the ImageNet data set for which MobileNet was originally trained on, so the original model has already learned a lot about cats and dogs in general. Because of this, it won\u2019t take much tuning to get the model to perform well on this specific, more narrow classification task.\n\nIn a later cells, however, we\u2019ll be fine-tuning MobileNet on a completely new data set made up of classes that the model hasn\u2019t already learned about in it\u2019s original training, so stay tuned for that.","9a7b786d":"Now that we\u2019ve seen what MobileNet is all about in above cells, let\u2019s now talk about how we can fine-tune the model and and use transfer learning to train it on another dataset.\nIf you\u2019re not already familiar with the concept of fine-tuning, that\u2019s alright because we have [another notebook](https:\/\/www.kaggle.com\/bavalpreet26\/vgg-16-keras-nb3) on fine-tuning using the VGG16 model with Keras","16084b2e":"What we\u2019re going to do is download a MobileNet model, and then use it for inference just on a few random images to see how well it classifies these images according to ImageNet classes.","2bf6991a":"With the minor tuning we did to the model, it is performing very well on this new task.","7ceb0bc5":"Then, we\u2019re using an ImageNet utility function provided by Keras called `decode_predictions()`. It returns the top five ImageNet class predictions with the ImageNet class ID, the class label, and the probability. With this, we\u2019ll be able to see the five ImageNet classes with the highest prediction probabilities from our model on this given image. Recall that there are 1000 total ImageNet classes.","14ca6fb7":"We now define the path variables for where the training and test set reside on directory.","79d7a3ea":"libraries","0a49f16c":"![](https:\/\/nitheshsinghsanjay.github.io\/images\/mobtiny_fig.PNG)","519e9e84":"We define this predictions variable to be equal to `model.predict()`, and we\u2019re passing in our test_batches.\n\nLastly, we set the verbosity equal to 0, which is just not going to print out any output.","e2845dcc":"Now, we construct the new fine-tuned model, which we\u2019re calling `model`.","fcd561dc":"We\u2019re going to process this image by passing it to our `prepare_image()` function and assign the result to this preprocessed_image variable. We\u2019re then having MobileNet predict on this image by calling `mobile.predict()` and passing it our preprocessed_image.","18043b57":"Then, we create directory iterators for each dataset using Keras\u2019 `ImageDataGenerator.flow_from_directory()` function, which yeilds batches of image data from the directory that we pass in with our first parameter.","f804e4ff":"To our confusion matrix, we pass the labels of our test set as well as the prediction results stored in `predictions`. Calling `argmax` on the predictions is going to return the indices that contain the maximum values from the list of predictions. So, because we only have two classes, it will return a 0 or 1 for each prediction in the predictions list.\n\nNext, we define the labels for our `confusion matrix`, and these need to be in the order that the class indices are in. Recall the class indices from `test_batches` we checked out above.","d1fe4ec9":"Now, our new model is now built, tuned, and ready to be trained on cats and dogs","b5d6cb17":"# Model Modification","5beb0f5b":"First, we have a function called `plot_confusion_matrix()`, and this is what we\u2019ll be calling in a few moments to do the plotting.","8fc73de9":"# Preparing The Data","7d7bc40b":"# Training The Model","5d055a01":"Checking out the results, we have Saluki with 25%, then Great Dane at 19%, whippet at 13%, and a couple of other types.","d308a8fd":"# Predicting with MobileNet","68f86a21":"Next, we\u2019re going to <font color='green'>grab the output from the sixth to last layer<\/font> of the model and store it in this variable <font color ='red'> x <\/font>.","9294eb94":"# Visualize Predictions In Confusion Matrix","8807434f":"So, we can see that a cat label corresponds to `0`, and a dog label corresponds to `1`.\n\nWe\u2019ve now got our labels taken care of. Let\u2019s now get some predictions.","d711b91a":"Within this function, we first define the relative path to the images. I have all the sample images we\u2019ll be using stored in kaggle\/input.\n\nWe then call the Keras function `image.load_img()` which accepts the image file and a target_size for the image, which we\u2019re setting to (224,224) (which is the default size for MobileNet). `load_img()` returns an instance of a PIL image.\n\nWe then convert the PIL image into an array with the Keras img_to_array() function, and then we expand the dimensions of that array by using numpy\u2019s `expand_dims()`.\n\nLastly, we\u2019re calling `preprocess_input()` from `tf.keras.applications.mobilenet`, which preprocesses the given image data to be in the same format as the images that MobileNet was originally trained on. Specifically, it\u2019s scaling the pixel values in the image between -1 and 1, and this function will return the preprocessed image data as a numpy array.\n\nThis result is what we\u2019re returning within this overall `prepare_image()` function.","dd058c76":"Note, you can see by the Model constructor used to create our model, that this is a model that is being created with the Keras Functional API, not the Sequential API that we\u2019ve worked with in previous notebook. That\u2019s why this format that we\u2019re using to create the model may look a little different than what you\u2019re used to.\n\n*To build the new model, we create an instance of the Model class and specify the inputs to the model to be equal to the input of the original MobileNet, and then we define the outputs of the model to be equal to the output variable we created directly above.*\n\nThis creates a new model, which is identical to the original MobileNet up to the original model\u2019s sixth to last layer. We don\u2019t have the last five original MobileNet layers included, but instead we have a new layer, the output layer we created with two output nodes.\n\nYou can compare the summary of the new model here with the summary of the original MobileNet to verifiy these differences using by calling summary() on both the old and new models","b8cd0de7":"We\u2019ll be using the scikit-learn library to do that. If you\u2019re not generally familiar with confusion matrices, or you want to learn more about working with them, check out the earlier [notebook](https:\/\/www.kaggle.com\/bavalpreet26\/cnn-tutorial-keras-nb2) where we go into more detail on using scikit-learn\u2019s confusion matrix.","20f97987":"By looking at the summary of the original model, we can see that by not including the last five layers, we\u2019ll be including everything up to and including the last` global_average_pooling` layer.\n\nNote that the amount of layers that you choose to cut off when you\u2019re fine-tuning a model will vary for each scenario, but I\u2019ve found through experimentation that just removing the last `5 layers` here works out well for this particular task. So with this setup, we\u2019ll be keeping the vast majority of the original MobileNet architecutre, which has 88 layers total."}}