{"cell_type":{"754b75ae":"code","65a045c2":"code","47eef0fe":"code","b1b0ce38":"code","7842f4ac":"code","9c4a67c1":"code","d6d5a3f6":"code","079aff7e":"code","abb4b3f6":"code","3169cea2":"code","2d8ff0ad":"code","cf9427ec":"code","19dfc94b":"code","70c94f4a":"code","44baa8d4":"code","2771741c":"code","9b871c19":"code","b8ca8bc0":"markdown","7a8611e5":"markdown","32589bf6":"markdown","60dbca7e":"markdown","43683847":"markdown","6369ce6d":"markdown","cfc88bc6":"markdown","dc2e90f0":"markdown","5d2f65a3":"markdown"},"source":{"754b75ae":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf_train = pd.read_csv(\"..\/input\/mobile-price-classification\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/mobile-price-classification\/test.csv\")","65a045c2":"df_train.shape #2000 records with 20 features.\n#(This really reminds me of the markstrat game in business school)\ndf_test.shape #1000 records 20 features with ID. ","47eef0fe":"#Lets take a look of battery power in different price ranges\nimport plotly.express as px\nfig = px.histogram(df_train[\"battery_power\"], color = df_train[\"price_range\"], width=600, height=400)\nfig.update_layout(yaxis_range=[0,80])\nfig.show()\n#Cannot tell much but class 3 does have larger battery power. ","b1b0ce38":"#Lets take a look of bluetooth feature in different price ranges\ndf_blue = df_train.groupby([\"price_range\",\"blue\"])[\"blue\"].count()\npd.DataFrame(df_blue)\n#I dont think I can see much correlation between bluetooth and price ranges. ","7842f4ac":"#Similarly, lets take a look on all the features. \ndf_cs = df_train.groupby([\"price_range\"],as_index=False).mean()\n#Most values are really close. px_height and px_width are important features. \n#Normalization is definitely necessary for related models. \ndf_cs","9c4a67c1":"#Correlation plot\n#Beautiful Correlation Plot\nfrom string import ascii_letters\n\nsns.set(style=\"white\")\n\n# Generate a large random dataset\nrs = np.random.RandomState(33)\n\n# Compute the correlation matrix\ncorr = df_train.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","d6d5a3f6":"num_columns = []\nfor i in df_train.columns:\n    if df_train[i].nunique()>=3 and i!=\"price_range\":\n        num_columns.append(i)","079aff7e":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\n\n# 20 features with 2000 records, multi-classification problem with low Collinearity, \n# the suitable models came in my mind are RF, Logistic Regression, XGBoost, SVM and GBM.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import plot_tree, plot_importance\n\n#CrossValidation and Metrics\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\ndf_train[num_columns] = stats.zscore(df_train[num_columns])\nX = df_train.drop(\"price_range\", axis=1)\ny = df_train[\"price_range\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.3, random_state=42)\n\n\ncro_val_acc, train_scores, test_scores = [], [], []\n\n\nmodel_names = [\"RandomForestClassifier\", \"LogisticRegression\", \"XGBoost\",\"SVM\",\"GradientBoostingClassifer\"]\nmodels = {\n    \"RandomForestClassifier\": RandomForestClassifier(),\n    \"LogisticRegression\": LogisticRegression(),\n    \"XGBClassifier\": XGBClassifier(verbosity = 0),\n    \"SVM\": SVC(),\n    \"GradientBoostingClassifer\": GradientBoostingClassifier()\n}\n\nfor i in models:\n    print(i)\n    print(\"\\n\")\n    \n    model = models[i]\n    cv = KFold(n_splits=5, shuffle=True)\n    cv_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv)\n    cv_mean_score = np.mean(cv_scores)\n    print(i,\"  \",\"cross validation accuracy\", cv_mean_score)\n    cro_val_acc.append(cv_mean_score)\n    \n    model.fit(X_train, y_train)\n    train_score = model.score(X_train, y_train)\n    train_scores.append(train_score)\n    print(f\"Train Score:{train_score*100}\")\n    print(\"\\n\")\n    \n    test_score = model.score(X_test, y_test)\n    print(f\"Test Score:{test_score*100}\")\n    print(\"\\n\")\n    test_scores.append(test_score)\n    \n    y_pred = model.predict(X_test)\n    conf_matrix = confusion_matrix(y_pred, y_test)\n    print(conf_matrix)\n    print(\"\\n\")\n    \n    cla_report = classification_report(y_test, y_pred, output_dict=True)\n    print(pd.DataFrame(cla_report).transpose())\n    print(\"\\n\")","abb4b3f6":"import seaborn as sns\nplt.figure(figsize=(16,8))\nsns.set_style('darkgrid')\nplt.title('Model Performance', fontweight='bold', size=20)\n\nbarWidth = 0.20\n \nb1 = train_scores\nb2 = test_scores\nb3 = cro_val_acc\n \nr1 = np.arange(len(b1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \nplt.bar(r1, b1, color='blue', width=barWidth, edgecolor='white', label='train',capsize=18)\nplt.bar(r2, b2, color='red', width=barWidth, edgecolor='white', label='test',capsize=18)\nplt.bar(r3, b3, color='grey', width=barWidth, edgecolor='white', label='cv_accuracy',capsize=18)\n\nplt.ylim([0.8,1])\n \n\nplt.xlabel('Models', fontweight='bold', size = 20)\nplt.ylabel('Scores', fontweight='bold', size = 20)\nplt.xticks([r + barWidth for r in range(len(b1))], model_names)\n \nplt.legend()\nplt.show()","3169cea2":"for i in range(len(model_names)):\n    print(f'Accuracy {model_names[i]}'.ljust(80, ' '))\n    print(round(test_scores[i],3))","2d8ff0ad":"# Lets give XGBoost another try with Hyperparater Tuning\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    'learning_rate': [0.02, 0.1],\n    'max_depth': [5, 8, 11],\n    'colsample_bytree': [0.7],\n    'n_estimators' : [500, 800,1000],\n    'objective': ['multi:softmax']\n}\n\nxgb_model = XGBClassifier(verbosity = 0)\n\nclf = GridSearchCV(estimator = xgb_model,\n                       param_grid = params,\n                       cv=5, \n                       scoring = \"accuracy\")\nclf.fit(X_train, y_train)\nclf.best_params_","cf9427ec":"xgb_model = XGBClassifier(colsample_bytree=0.7,\n learning_rate=0.1,\n max_depth=5,\n n_estimators=1000,\n verbosity = 0)\nxgb_model.fit(X_train, y_train)\ny_pred = xgb_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n#Not much improvement. ","19dfc94b":"from xgboost import plot_importance\nplot_importance(xgb_model, max_num_features=10)","70c94f4a":"h2o.cluster().shutdown()","44baa8d4":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()\nh2o_df = h2o.H2OFrame(df_train)\nh2o_df[\"price_range\"] = h2o_df[\"price_range\"].asfactor()\ntrain, test = h2o_df.split_frame(ratios=[.7])\n# Identify predictors and response\nx = train.columns\ny = \"price_range\"\nx.remove(y)","2771741c":"train[y] = train[y].asfactor()\ntest[y] = test[y].asfactor()\n\naml = H2OAutoML(max_runtime_secs=600,\n                exclude_algos=['DeepLearning'],\n                seed=1,\n                stopping_metric='mean_per_class_error',\n                sort_metric='mean_per_class_error',\n                project_name='Price_Range_Prediction'\n)\n\n%time aml.train(x=x, y=y, training_frame=train)\n# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(5) ","9b871c19":"model_id = aml.leader.model_id\nmodel = h2o.get_model(model_id)\nperf = model.model_performance(test)\nperf.confusion_matrix()","b8ca8bc0":"#### Tree based models all have overfitting problems","7a8611e5":"# Exploratory Data Analysis","32589bf6":"# Hyperparameter Tuning for XGBoost","60dbca7e":"\nIn case H2O bug comes up, I copied the result here. \n\nStackedEnsemble_BestOfFamily_AutoML_20210506_034057\t0.0476932\nGLM_1_AutoML_20210506_034057\t0.0484978\t0.185706\t0.220634\t0.0486792\tnan\tnan\n\n\nStackedEnsemble_AllModels\t0.0906175\t0.383616\t0.340883\t0.116201\tnan\tnan\n\nGBM_grid__1_1\t0.0954304\t0.242366\t0.267777\t0.0717045\tnan\tnan\n\nXGBoost_grid__1_0.0962082\t0.238954\t0.267441\t0.0715246\tnan\tnan\n\n\t0\t1\t2\t3\tError\tRate\n    \n0  \t147.0\t3.0\t0.0\t0.0\t0.020000\t3 \/ 150\n\n1  \t7.0\t148.0\t1.0\t0.0\t0.051282\t8 \/ 156\n\n2  \t0.0\t4.0\t133.0\t6.0\t0.069930\t10 \/ 143\n\n3  \t0.0\t0.0\t0.0\t148.0\t0.000000\t0 \/ 148\n\n4  \t154.0\t155.0\t134.0\t154.0\t0.035176\t21 \/ 597","43683847":"Logistic Regression performs the best. \nThe rest of the models have overfitting problems. ","6369ce6d":"# Machine Learning Modeling","cfc88bc6":"### No strong correlation was found.\n### Weak correlations found between target and the following vars: battery power, px height and wildth,  ram. \n","dc2e90f0":"# Model Performance Analysis","5d2f65a3":"# AutoML comparison"}}