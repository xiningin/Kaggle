{"cell_type":{"6fb97d30":"code","fe2e6e6b":"code","769909c7":"code","35626230":"code","160a08f3":"code","1eb99769":"code","5d408c7d":"code","3fe81b02":"code","05bdddc9":"code","b6999948":"code","f1b30d48":"code","cf10babf":"code","d193ce1e":"code","1e955aa1":"code","2fe55d0f":"code","8d96d109":"code","28e0cafe":"code","bb818e79":"code","04ab4aca":"markdown","ce98e9cb":"markdown","d5b796c8":"markdown","5eb4a236":"markdown","1f458ef7":"markdown","9a8f352e":"markdown","039d605a":"markdown","83355484":"markdown","cb7f0a64":"markdown","37fd04bc":"markdown"},"source":{"6fb97d30":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, LeakyReLU\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint \nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport os\nfrom glob import glob","fe2e6e6b":"WEIGHTS_FOLDER = '\/kaggle\/working\/weights\/'\nDATA_FOLDER = '\/kaggle\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/'\nZ_DIM = 200\n\nif not os.path.exists(WEIGHTS_FOLDER):\n  os.makedirs(os.path.join(WEIGHTS_FOLDER,\"AE\"))\n  os.makedirs(os.path.join(WEIGHTS_FOLDER,\"VAE\"))\n\nfilenames = np.array(glob(os.path.join(DATA_FOLDER, '*.jpg')))\nNUM_IMAGES = len(filenames)\nprint(\"Total number of images : \" + str(NUM_IMAGES))","769909c7":"def build_decoder(test=False, out_size=(128, 128)):\n    def decoder(path):\n        img = file_bytes = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(file_bytes, channels=3)  \n        img = tf.image.resize(img, (128, 128))\n        img = tf.cast(img, tf.float32) \/ 255.0\n        return img\n    def decoder_train(path):\n        return decoder(path), decoder(path)\n\n    return decoder if test else decoder_train\n\ndef build_dataset(paths, test=False, shuffle=1, batch_size=1):\n    AUTO = tf.data.experimental.AUTOTUNE\n    decoder = build_decoder(test)\n\n    dset = tf.data.Dataset.from_tensor_slices(paths)\n    dset = dset.map(decoder, num_parallel_calls=AUTO)\n    \n    dset = dset.shuffle(shuffle)\n    dset = dset.batch(batch_size)\n    return dset","35626230":"train_paths, valid_paths, _, _ = train_test_split(filenames, filenames, test_size=0.2, shuffle=True)\n\ntrain_dataset = build_dataset(train_paths, batch_size=128)\nvalid_dataset = build_dataset(valid_paths, batch_size=128)","160a08f3":"class ConvAutoencoder:\n    def __init__(self):\n        self.input_dim = (128,128,3)\n        self.batch_size = 512\n        self.latentDim = 200\n        self.z_dim = 200 # Dimension of the latent vector (z)\n        self.autoencoder_model = None\n        self.encoder_model = None\n        self.decoder_model = None\n\n    #@staticmethod\n    def build(self):\n        inputs = Input(shape = self.input_dim)\n        x = inputs\n        \n        filters=(32, 64, 64, 64)\n        \n        for index, f in enumerate(filters):\n            x = Conv2D(f, (3,3), strides=2, padding=\"same\", name=\"conv2dtranspose_\" + str(index))(x)\n            x = LeakyReLU()(x)\n        \n        volumeSize = K.int_shape(x)\n        x = Flatten()(x)\n        latent = Dense(self.latentDim)(x)\n        self.encoder_model = Model(inputs, latent, name = \"encoder\")\n        \n        latentInputs = Input(shape=(self.latentDim,))\n        x = Dense(np.prod(volumeSize[1:]))(latentInputs)\n        x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n        \n        for f in [64, 64, 32]:\n            x = Conv2DTranspose(f, (3, 3), strides=2, padding=\"same\")(x)\n            x = LeakyReLU()(x)\n\n        x = Conv2DTranspose(3, (3, 3), strides=2, padding=\"same\")(x)\n        outputs = Activation(\"sigmoid\")(x)\n        self.decoder_model = Model(latentInputs, outputs, name=\"decoder\")\n        self.autoencoder_model = Model(inputs, self.decoder_model(self.encoder_model(inputs)),name=\"autoencoder\")\n        \n        return None\n    \n    def get_encoder(self):\n        if self.encoder_model is not None:\n            return self.encoder_model\n        else:\n            print(\"Encoder model has not been defined!\")\n            return None\n\n    def get_decoder(self):\n        if self.decoder_model is not None:\n            return self.decoder_model\n        else:\n            print(\"Decoder model has not been defined!\")\n            return None\n    \n    def get_autoencoder(self):\n        if self.autoencoder_model is not None:\n            return self.autoencoder_model\n        else:\n            print(\"Autoencoder model has not been defined!\")\n            return None","1eb99769":"model = ConvAutoencoder()\nmodel.build()\nencoder = model.get_encoder()\ndecoder = model.get_decoder()\nautoencoder = model.get_autoencoder()\nencoder.summary()\ndecoder.summary()\nautoencoder.summary()","5d408c7d":"LEARNING_RATE = 0.0005\nN_EPOCHS = 20\n\noptimizer = Adam(lr = LEARNING_RATE)\n\ndef r_loss(y_true, y_pred):\n    return K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n\nautoencoder.compile(optimizer=optimizer, loss = r_loss)\n\ncheckpoint_ae_best = ModelCheckpoint(os.path.join(WEIGHTS_FOLDER, 'AE\/autoencoder_best_weights.h5'), \n                                     monitor='val_loss',\n                                     mode='min',\n                                     save_best_only=True,\n                                     save_weights_only = False, \n                                     verbose=1)\n\ncheckpoint_ae_last = ModelCheckpoint(os.path.join(WEIGHTS_FOLDER, 'AE\/autoencoder_last_weights.h5'), \n                                     monitor='val_loss',\n                                     mode='min',\n                                     save_best_only=True,\n                                     save_weights_only = False, \n                                     verbose=1)","3fe81b02":"autoencoder.fit(train_dataset,\n                epochs=10,\n                callbacks=[checkpoint_ae_best, checkpoint_ae_last],\n                validation_data=valid_dataset)","05bdddc9":"test_dataset = build_dataset(valid_paths, test=True)\nautoencoder.load_weights('\/kaggle\/working\/weights\/AE\/autoencoder_last_weights.h5')","b6999948":"data = list(test_dataset.take(20))\n\nfig = plt.figure(figsize=(30, 10))\nfor n in range(0, 20, 2):\n    image = autoencoder.predict(data[n])\n    \n    plt.subplot(2, 10, n + 1)\n    plt.imshow(np.squeeze(data[n]))\n    plt.title('original image')\n    \n    plt.subplot(2, 10, n + 2)\n    plt.imshow(np.squeeze(image))\n    plt.title('reconstruct')\n    \nplt.show()","f1b30d48":"data = list(test_dataset.take(20))\n\nfig = plt.figure(figsize=(30, 10))\nfor n in range(0, 20, 2):\n    image = encoder.predict(data[n])\n    image += np.random.normal(0.0, 1.0, size = (Z_DIM)) #inject noise to the encoded vector\n    reconst_images = decoder.predict(image)\n    \n    plt.subplot(2, 10, n + 1)\n    plt.imshow(np.squeeze(data[n]))\n    plt.title('original image')\n    \n    plt.subplot(2, 10, n + 2)\n    plt.imshow(np.squeeze(reconst_images))\n    plt.title('Reconstruct noisy image')\n    \nplt.show()","cf10babf":"class VariableAutoencoder:\n    def __init__(self):\n        self.input_dim = (128,128,3)\n        self.batch_size = 512\n        self.z_dim = 200 # Dimension of the latent vector (z)\n        self.learning_rate = 0.0005\n        self.var_autoencoder_model = None\n        self.var_encoder_model = None\n        self.var_decoder_model = None\n\n    def build(self):\n        #Encoder\n        input_encoder = Input(shape=(self.input_dim))\n        x = Conv2D(32, kernel_size=(3, 3), strides = 2, padding='same', name='encoder_conv2d_1')(input_encoder)\n        x = LeakyReLU()(x)\n        x = Conv2D(64, kernel_size=(3, 3), strides = 2, padding='same', name='encoder_conv2d_2')(x)\n        x = LeakyReLU()(x)\n        x = Conv2D(64, kernel_size=(3, 3), strides = 2, padding='same', name='encoder_conv2d_3')(x)\n        x = LeakyReLU()(x)\n        x = Conv2D(64, kernel_size=(3, 3), strides = 2, padding='same', name='encoder_conv2d_4')(x)\n        volumeSize = K.int_shape(x)\n        x = Flatten()(x)\n\n        latent_mu = Dense(self.z_dim, name='latent_mean')(x)\n        latent_log_var = Dense(self.z_dim, name='latent_log_var')(x)\n        \n        def sampling(args=None):\n            z_mean, z_log_var = args\n            batch = K.shape(z_mean)[0]\n\n            epsilon = K.random_normal(shape=(batch, self.z_dim))\n            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n        \n        latent_sample = Lambda(sampling)([latent_mu, latent_log_var])\n        self.var_encoder_model = Model(input_encoder, latent_sample, name='encoder')\n\n        latent_input = Input(shape=(self.z_dim,), name='decoder_input')\n        x = Dense(np.prod(volumeSize[1:]))(latent_input)\n        x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n        x = Conv2DTranspose(64, kernel_size=(3, 3), strides=2, padding='same', name='conv2d_1')(x)\n        x = LeakyReLU()(x)\n        x = Conv2DTranspose(64, kernel_size=(3, 3), strides=2, padding='same', name='conv2d_2')(x)\n        x = LeakyReLU()(x)\n        x = Conv2DTranspose(32, kernel_size=(3, 3), strides=2, padding='same', name='conv2d_3')(x)\n        x = LeakyReLU()(x)\n        x = Conv2DTranspose(3, kernel_size=(3, 3), strides=2, padding='same', name='conv2d_4')(x)\n        output_decoder = Activation('sigmoid')(x)\n\n        self.var_decoder_model = Model(latent_input, output_decoder, name='decoder')\n\n        output_vae = self.var_decoder_model(self.var_encoder_model(input_encoder))\n        self.var_autoencoder_model = Model(input_encoder, output_vae, name ='variable_autoencoder')\n\n        reconstruction_loss = binary_crossentropy(input_encoder, output_vae) * (128 * 128)\n        reconstruction_loss = K.mean(reconstruction_loss)\n\n        kl_loss = 1 + latent_log_var - K.square(latent_mu) - K.exp(latent_log_var)\n        kl_loss = K.sum(kl_loss, axis=-1)\n        kl_loss *= -0.5\n\n        vae_loss = K.mean(reconstruction_loss + kl_loss)\n\n        self.var_autoencoder_model.add_loss(vae_loss)  \n        self.var_autoencoder_model.add_metric(reconstruction_loss, name='reconstruction_loss')\n        self.var_autoencoder_model.add_metric(kl_loss, name='kl_divergence_loss')\n\n        optimizer = Adam(lr = self.learning_rate)\n\n        self.var_autoencoder_model.compile(optimizer=optimizer)\n\n        return None\n\n    def get_varencoder(self):\n        if self.var_encoder_model is not None:\n            return self.var_encoder_model\n        else:\n            print(\"Variable Encoder model has not been defined!\")\n            return None\n\n    def get_vardecoder(self):\n        if self.var_decoder_model is not None:\n            return self.var_decoder_model\n        else:\n            print(\"Variable Decoder model has not been defined!\")\n            return None\n    \n    def get_varautoencoder(self):\n        if self.var_autoencoder_model is not None:\n            return self.var_autoencoder_model\n        else:\n            print(\"Variable Autoencoder model has not been defined!\")\n            return None    ","d193ce1e":"var_model = VariableAutoencoder()\nvar_model.build()\n\nvar_autoencoder = var_model.get_varautoencoder()\nvar_encoder = var_model.get_varencoder()\nvar_decoder = var_model.get_vardecoder()\nvar_autoencoder.summary()","1e955aa1":"VAE_N_EPOCHS = 10\n\ncheckpoint_vae_best = ModelCheckpoint(os.path.join(WEIGHTS_FOLDER, 'VAE\/vae_best_model.h5'), \n                                      monitor='val_loss',\n                                      mode='min',\n                                      save_best_only=True,\n                                      save_weights_only = True, \n                                      verbose=1)\n    \ncheckpoint_vae_last = ModelCheckpoint(os.path.join(WEIGHTS_FOLDER, 'VAE\/vae_last_model.h5'),\n                                      monitor='val_loss',\n                                      mode='min',\n                                      verbose=1,\n                                      save_best_only=False,\n                                      save_weights_only=True)\n\nvar_autoencoder.fit(train_dataset,\n                    epochs=VAE_N_EPOCHS,\n                    callbacks=[checkpoint_vae_best, checkpoint_vae_last],\n                    validation_data=valid_dataset)","2fe55d0f":"test_dataset = build_dataset(valid_paths, test=True)\nvar_autoencoder.load_weights(os.path.join(WEIGHTS_FOLDER, 'VAE\/vae_last_model.h5'))","8d96d109":"data = list(test_dataset.take(20))\n\nfig = plt.figure(figsize=(30, 10))\nfor n in range(0, 20, 2):\n    image = var_autoencoder.predict(data[n])\n    \n    plt.subplot(2, 10, n + 1)\n    plt.imshow(np.squeeze(data[n]))\n    plt.title('original image')\n    \n    plt.subplot(2, 10, n + 2)\n    plt.imshow(np.squeeze(image))\n    plt.title('reconstruct')\n    \nplt.show()","28e0cafe":"def vae_generate_images(new_to_show=10):\n    random_codes = np.random.normal(size=(new_to_show, 200))\n    new_faces = var_decoder.predict(np.array(random_codes))\n\n    fig = plt.figure(figsize=(30, 15))\n\n    for i in range(new_to_show):\n        ax = fig.add_subplot(6, 10, i+1)\n        ax.imshow(new_faces[i])\n        ax.axis('off')\n    plt.show()","bb818e79":"vae_generate_images(30)","04ab4aca":"## Limitations of autoencoders for content generation\n\nOnce the autoencoder network has been trained, we have both an encoder and a decoder but there is no real way to produce any new content. This is because a generation of meaningful content depends on the regularity of the output latent space by the encoder for the decoder to generate the new content. Pretty much, is to get the decoder to work more or less like the generator of a Generative Adversarial Network.\n\nIn a conventional autoencoder network, this regularity of the out latent space from the encoder is almost impossible. It depends on the distribution of the data in the initial space, the dimension of the latent space and the architecture of the encoder. There's no way that the encoder will organize the latent space in a smart way compatible with the generative process to produce new content ","ce98e9cb":"# Generating new faces with Variational Autoencoders (VAE)\nThis script demonstrates the process of building and training a VAE using Keras to generate new faces. We shall be using the CelebFaces Attributes (CelebA) Dataset from Kaggle for training the VAE model. Some parts of the Markdown notes are extracted from [Understanding Variational Autoencoders (VAEs)](https:\/\/towardsdatascience.com\/understanding-variational-autoencoders-vaes-f70510919f73) \n\n## Simple Autoencoder\nI first built a simple Autoencoder. A simple Autoencoder consists of two neural networks - an Encoder and a Decoder. An Encoder is responsible for converting an image into a compact lower dimensional vector (or latent vector). This latent vector is a compressed representation of the image. The Encoder maps an input from the higher dimensional input space to the lower dimensional latent space, a concept that is similar to a CNN classifier. The outputs of the encoder are then fed into the Decoder. The Decoder is a different neural network that tries to reconstruct the image from the lower dimensional latent space to the higher dimensional output space. The Encoder and Decoder perform mappings that are exactly opposite to each other, as shown in the image.\n\n![image.png](attachment:c52fdd45-049a-4c95-8b25-6533cd5995df.png)","d5b796c8":"## Autoencoder Decoding with noise\n\nWe can observe that the reconstructed images are becoming more distorted then noise is added to their encodings.","5eb4a236":"## Variable Autoencoder\n\nA variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process. A variational autoencoder is an architecture composed of both an encoder and a decoder and is trained to minimise the reconstruction error between the encoded-decoded data and the initial data. In addition, regularisation of the latent space is introduced within the process of encoding-decoding. This is done via a slight modification where instead of encoding an input as a single point, we encode it as a distribution over the latent space. The model is then trained as follows:\n1. The input is encoded as distribution over the latent space\n2. A point from the latent space is sampled from that distribution\n3. The sampled point is decoded and the reconstruction error can be computed\n4. The reconstruction error is backpropagated through the network\n\nThe loss function used is the sum of RMSE and KL Divergence. A weight is assigned to the RMSE loss, known as the loss factor. The loss factor is multiplied with the RMSE loss. If we use a high loss factor, the drawbacks of a Simple Autoencoder start to appear. However, if we use a loss factor too low, the quality of the reconstructed images will be poor. Hence the loss factor is a hyperparameter that needs to be tuned.\n\nThe section covers the definition of the Variable Autoencoder defined within a VariableAutoencoder Class.","1f458ef7":"## Decoding by Variable Autoencoder\n\nLikewise, the first image shows images directly from the dataset and the second images shows the outcomes after passing through the trained Variable Autoencoder. Evidently, the model is still able to encode and decode with observable degradation in quality.","9a8f352e":"## Decoding\n\nThe first image shows images directly from the dataset and the second images shows the outcomes after passing through Autoencoder. Evidently, the model has learned to encode and decode (reconstruct) but with observable degradation in quality.","039d605a":"### Compilation and Training\n\nBefore the autoencoder is trained, it must be compiled. I compile the model with Adam optimiser with Root Mean Square Error (RMSE). I also compile the model with keras ModelCheckpoint to save the model weights for reuse. It saves 2 sets of model outputs - one output is the fresh set of weights after every epoch and one output is the best weights from all the epochs.","83355484":"## Data preparation\n\nI am not using Kera's flow_from_directory method because the function seems to hang on Kaggle. I took reference from Github the function of build_decoder to prepare the dataset for training. ","cb7f0a64":"## Generate new faces using the Variable Autoencoder \n\nThe Variable Autoencoder is capable enough of producing new faces from vectors samped from a standard normal distribution - although the output of the generated faces are not as good quality.","37fd04bc":"## Model Architecture - Autoencoder\n\nThe section covers the definition of the simple Autoencoder defined within a ConvAutoencoder Class."}}