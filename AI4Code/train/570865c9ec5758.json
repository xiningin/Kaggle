{"cell_type":{"e2613ba4":"code","34aaf10d":"code","5eb1bb2f":"code","e9926b24":"code","0826efce":"code","93a88da5":"code","d444608c":"code","c01007e8":"code","d24441c5":"code","edcd6457":"code","a9dbbf2f":"code","b7531eb2":"code","e9050e91":"code","ef4bb633":"code","1c3761ca":"code","facde6dd":"code","8f1569bb":"code","747aad25":"code","adc3bb13":"code","16bec238":"code","d6f2e41c":"code","5f14bc67":"code","fbe32dec":"markdown","17a886f6":"markdown","a18576b2":"markdown","014f6378":"markdown","7e6829f5":"markdown","42d9e9e3":"markdown","16d595c4":"markdown","12c3672b":"markdown","90924bc7":"markdown","042fc1ba":"markdown","8424e3bd":"markdown","4bdc29e9":"markdown","52f32128":"markdown","feb32f71":"markdown","a99fd9e4":"markdown","912b518b":"markdown"},"source":{"e2613ba4":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom PIL import Image\n\n# Imports for Building CNN\nfrom keras.layers import Input, Lambda, Dense, Flatten, Activation,Dropout,BatchNormalization\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model\nfrom tensorflow.keras.utils import plot_model\n\n!pip install git+https:\/\/github.com\/paulgavrikov\/visualkeras\n\nimport visualkeras\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n\nfrom keras import applications\n\nimport tensorflow as tf\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","34aaf10d":"# re-size all the images to this\nIMAGE_SIZE = [224, 224]\n\ncategories = [\"ABNORMAL\",'NORMAL']\n\n# File Path to all Required Folders\ntrain_path = '..\/input\/xraydata\/train'\ntest_path = '..\/input\/xraydata\/test'","5eb1bb2f":"train_normal_images = glob(train_path+\"\/NORMAL\/*\")\ntrain_abnormal_images = glob(train_path+\"\/ABNORMAL\/*\")\n\nplt.figure(figsize=(12, 8), dpi=80)\ninit_subplot = 330\nfor i in range(1, 7):\n    plt.subplot(2,3 ,i)\n    \n    if i < 4:\n        img = Image.open(np.random.choice(train_normal_images)).resize((244, 244))\n        plt.title(\"Normal Image\")  \n    else:\n        img = Image.open(np.random.choice(train_abnormal_images)).resize((244, 244))\n        plt.title(\"Abnormal Image\")\n        \n    img = np.asarray(img)\n    plt.axis('off')\n    plt.imshow(img)\n\n","e9926b24":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)","0826efce":"training_set = train_datagen.flow_from_directory(train_path,\n                                                 target_size = (224, 224),\n                                                 batch_size = 32,\n                                                 class_mode = 'categorical')\n\ntest_set = test_datagen.flow_from_directory(test_path,\n                                            target_size = (224, 224),\n                                            batch_size = 64,\n                                            class_mode = 'categorical')","93a88da5":"# add preprocessing layer to the front of VGG\ndef vgg16():\n    vgg16 = VGG16(input_shape = IMAGE_SIZE + [3],\n                weights = 'imagenet',\n                include_top=False)\n\n    # don't train existing weights\n    for layer in vgg16.layers:\n      layer.trainable = False\n    \n    vgg16_1 = vgg16.layers[-3]\n    vgg16_2 = vgg16.layers[-2]\n    lastlayer = vgg16.layers[-1]\n\n    # Creating the dropout layers\n    dropout1 = Dropout(0.85)\n    dropout2 = Dropout(0.85)\n\n    # Reconnecting the layers\n    vgg16x = dropout1(vgg16_1.output)\n    vgg16x = vgg16_2(vgg16x)\n    vgg16x = dropout2(vgg16x)\n    vgg16x = lastlayer(vgg16x)\n    \n    vgg16x = Flatten()(vgg16x)\n    vgg16x = Dense(1000, activation='relu')(vgg16x)\n\n    # This is the Last Layer with softmax activation for binary outputs\n    prediction_vgg16 = Dense(len(categories), activation='softmax')(vgg16x)\n    \n    model_vgg16 = Model(inputs=vgg16.input, outputs=prediction_vgg16)\n    \n    model_vgg16.compile(\n          loss='categorical_crossentropy',\n          optimizer='adam',\n          metrics=['accuracy']\n    )\n    \n    return model_vgg16\n    \ndef vgg19():\n    vgg19 = applications.vgg19.VGG19(weights = \"imagenet\", include_top=False, input_shape = IMAGE_SIZE + [3])\n    for layer in vgg19.layers:\n      layer.trainable = False\n    \n    # Storing the fully connected layers\n    vgg19_1 = vgg19.layers[-3]\n    vgg19_2 = vgg19.layers[-2]\n    lastlayer = vgg19.layers[-1]\n\n    # Creating the dropout layers\n    dropout1 = Dropout(0.85)\n    dropout2 = Dropout(0.85)\n\n    # Reconnecting the layers\n    vgg19x = dropout1(vgg19_1.output)\n    vgg19x = vgg19_2(vgg19x)\n    vgg19x = dropout2(vgg19x)\n    vgg19x= lastlayer(vgg19x)\n\n    vgg19x = Flatten()(vgg19x)\n    vgg19x = Dense(1000, activation='relu')(vgg19x)\n\n    # This is the Last Layer with softmax activation for binary outputs\n    prediction_vgg19 = Dense(len(categories), activation='softmax')(vgg19x)\n\n    model_vgg19 = Model(inputs=vgg19.input, outputs=prediction_vgg19)\n    \n    model_vgg19.compile(\n          loss='categorical_crossentropy',\n          optimizer='adam',\n          metrics=['accuracy']\n    )\n    \n    return model_vgg19\n\ndef inceptionv3():\n    inception_model = applications.inception_v3.InceptionV3(weights = \"imagenet\", include_top=False, input_shape = IMAGE_SIZE + [3])\n    for layer in inception_model.layers[:5]:\n        layer.trainable = False\n\n    #Adding custom Layers \n    x = inception_model.output\n    x = Flatten()(x)\n    x = Dense(512, activation=\"relu\")(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1024, activation=\"relu\")(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(len(categories), activation='softmax')(x)\n\n    # creating the final model \n    model_v3=  Model(inputs=inception_model.input, outputs=predictions)\n    model_v3.compile(\n      loss='categorical_crossentropy',\n      optimizer='adam',\n      metrics=['accuracy']\n    )\n    \n    return model_v3\n\ndef xception():\n    xception_model = applications.xception.Xception(weights = \"imagenet\", include_top=False, input_shape = IMAGE_SIZE + [3])\n    for layer in xception_model.layers[:5]:\n        layer.trainable = False\n\n    #Adding custom Layers \n    x = xception_model.output\n    x = Flatten()(x)\n    x = Dense(1024,activation=\"relu\")(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1024, activation=\"relu\")(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(len(categories), activation='softmax')(x)\n\n    # creating the final model \n    model_Xception =  Model(inputs=xception_model.input, outputs=predictions)\n    model_Xception.compile(\n      loss='categorical_crossentropy',\n      optimizer='adam',\n      metrics=['accuracy']\n    )\n    \n    return model_Xception\n\ndef mobilenet():\n    mobile_model = applications.mobilenet.MobileNet(weights = \"imagenet\", include_top=False, input_shape =  IMAGE_SIZE + [3])\n    for layer in mobile_model.layers[:5]:\n        layer.trainable = False\n\n    #Adding custom Layers \n    x = mobile_model.output\n    x = Flatten()(x)\n    x = Dense(512, activation=\"relu\")(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1024, activation=\"relu\")(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(len(categories), activation='softmax')(x)\n\n    # creating the final model \n    model_MobileNet =  Model(inputs=mobile_model.input, outputs=predictions)\n    model_MobileNet.compile(\n      loss='categorical_crossentropy',\n      optimizer='adam',\n      metrics=['accuracy']\n    )\n    return model_MobileNet\ndef resnet():\n    resnet_model = applications.resnet.ResNet50(weights = \"imagenet\", include_top=False, input_shape =  IMAGE_SIZE + [3])\n    for layer in resnet_model.layers[:5]:\n        layer.trainable = False\n\n    #Adding custom Layers \n    x = resnet_model.output\n    x = Flatten()(x)\n    x = Dense(512, activation=\"relu\")(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1024, activation=\"relu\")(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(len(categories), activation='softmax')(x)\n\n    # creating the final model \n    model_resnet =  Model(inputs=resnet_model.input, outputs=predictions)\n    model_resnet.compile(\n      loss='categorical_crossentropy',\n      optimizer='adam',\n      metrics=['accuracy']\n    )\n    return model_resnet\ndef densenet():\n    densenet_model = applications.densenet.DenseNet169(weights = \"imagenet\", include_top=False, input_shape =  IMAGE_SIZE + [3])\n    for layer in densenet_model.layers[:5]:\n        layer.trainable = False\n\n    #DenseNet169 Model\n    x=Flatten()(densenet_model.output)\n    #Fully Connection Layers\n    # FC1\n    x=Dense(1024, activation=\"relu\")(x)\n    x=BatchNormalization()(x)\n    x=Dense(512, activation=\"relu\")(x)\n    #Dropout to avoid overfitting effect\n    x=Dropout(0.2)(x)\n    # FC2\n    x=Dense(256, activation=\"relu\")(x)\n    x=Dense(128, activation=\"relu\")(x)\n\n    #output layer\n    predictions=Dense(len(categories), activation='softmax')(x)\n\n    # creating the final model \n    modelDenseNet =  Model(inputs=densenet_model.input, outputs=predictions)\n    modelDenseNet.compile(\n      loss='categorical_crossentropy',\n      optimizer='adam',\n      metrics=['accuracy']\n    )\n    return modelDenseNet","d444608c":"model_vgg16,vgg16H=None,None\nmodel_vgg19,vgg19H=None,None\nmodel_v3,model_v3H=None,None\nmodel_Xception,model_xH=None,None\nmodel_MobileNet,model_mobileH=None,None\nmodel_resnet,model_resnetH=None,None\nmodelDenseNet,model_denseH=None,None\n\ntitles=[\"VGG16\",\"VGG19\",\"InceptionV3\",\"Xception\",\"MobileNet\",\"ResNet\",\"DenseNet\"]\nmodelsH=[vgg16H,vgg19H,model_v3H,model_xH,model_mobileH,model_resnetH,model_denseH]\nmodels=[model_vgg16,model_vgg19,model_v3,model_Xception,model_MobileNet,model_resnet,modelDenseNet]\nmodelsMethod=[vgg16(),vgg19(),inceptionv3(),xception(),mobilenet(),resnet(),densenet()]\n\nfor i in range(len(models)):\n    models[i]=modelsMethod[i]","c01007e8":"visualkeras.layered_view(models[0],to_file=\"layeredView.png\",legend=True,scale_xy=1)","d24441c5":"plot_model(models[0], to_file='model.png',dpi=50)","edcd6457":"early_stop = EarlyStopping(monitor='val_accuracy',patience=10)\n\nfor i in range(len(models)):\n    checkpoint = ModelCheckpoint(titles[i]+\"_tuned_2.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n\n    modelsH[i] = models[i].fit_generator(\n      training_set,\n      validation_data=test_set,\n      epochs=20,\n        callbacks=[early_stop,checkpoint]\n    )","a9dbbf2f":"for i in range(len(modelsH)):\n    fig, ax = plt.subplots(figsize=(20, 4), nrows=1, ncols=2)\n    fig.suptitle(titles[i])\n    ax[0].plot(modelsH[i].history['loss'], color='b', label=\"Training Loss\")\n    ax[0].plot(modelsH[i].history['val_loss'], color='r', label=\"Testing Loss\",axes =ax[0])\n    legend = ax[0].legend(loc='best', shadow=True)\n    ax[1].plot(modelsH[i].history['accuracy'], color='b', label=\"Training Accuracy\")\n    ax[1].plot(modelsH[i].history['val_accuracy'], color='r',label=\"Testing Accuracy\")\n    legend = ax[1].legend(loc='best', shadow=True)","b7531eb2":"for i in range(len(models)):\n    models[i].save(titles[i]+'_tuned_model.h5')","e9050e91":"for i in range(len(models)):\n    models[i] = load_model(titles[i]+'_tuned_2.h5')","ef4bb633":"normal_images_list = glob(\"..\/input\/xraydata\/test\/NORMAL\/*\")\nabnormal_images_list = glob(\"..\/input\/xraydata\/test\/ABNORMAL\/*\")","1c3761ca":"check_set = test_datagen.flow_from_directory(test_path,\n                                            target_size = (224, 224),\n                                            batch_size = 150,\n                                            class_mode = 'categorical')","facde6dd":"#Predicting on Test images \nx,y=check_set.next()\ny_true=y.argmax(axis=1)\n\npreds16=None\npreds19=None\npredsv3=None\npredsX=None\npredsM=None\npredsR=None\npredsD=None\n\noutputs=[preds16,preds19,predsv3,predsX,predsM,predsR,predsD]\n\nfor i in range(len(models)):\n    outputs[i]=models[i].predict(x).argmax(axis=1)","8f1569bb":"#Printing Metrics for model evaluation\nfor i in range(len(models)):\n    cm=confusion_matrix(y_true,outputs[i])\n    acc=accuracy_score(y_true,outputs[i])\n    cr=classification_report(y_true,outputs[i])\n    print(titles[i]+'\\nAccuracy Score:\\n',acc)\n    print('\\nConfusion Matrix:\\n',cm)\n    print('\\nClassification Report:\\n',cr)","747aad25":"for i in range(len(outputs)):\n    print(titles[i])\n    cm  = confusion_matrix(y_true, np.round(outputs[i]))\n    plt.figure()\n    plot_confusion_matrix(cm,figsize=(15,10), hide_ticks=True, cmap=plt.cm.Oranges)\n    plt.xticks(range(len(categories)), categories, fontsize=16)\n    plt.yticks(range(len(categories)), categories, fontsize=16)\n    plt.show()","adc3bb13":"# !pip install anvil-uplink\n# import anvil.server\n# anvil.server.disconnect()\n# anvil.server.connect(\"JSQS77C6HTPIAJGDFT2CSCZW-5F3FVTJ3RB7U4RJR\")\n\n# @anvil.server.callable\n\n\n\n# anvil.server.wait_forever()","16bec238":"def predict_image(image_path):\n    for i in range(len(models)):\n        img = image.load_img(image_path, target_size=(224, 224))\n\n        x = image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n\n        img_data = preprocess_input(x)\n        classes = models[i].predict(img_data)\n\n        pred_class = classes[0]\n        indx=np.argmax(pred_class)\n\n        if indx==1:\n            print(titles[i],\" Normal Xray\")\n        else:\n            print(titles[i],\" ABNORMAL XRay\")\n#         ans=input(\"Are you currently expirencing weight loss?(yes or no)\")\n#         if ans==\"yes\":\n#             print(\"TB\")\n#         else:\n#             ans=input(\"Are you currently expirencing loss of taste or smell,dry cough,diarrhoea or your PCR is positive?(yes or no)\")\n#             if ans==\"yes\":\n#                 print(\"Covid19\")\n#             else:\n#                 print(\"Pneumonia\")","d6f2e41c":"ramdom_noraml_img = normal_images_list[np.random.randint(len(normal_images_list))]\nprint(ramdom_noraml_img)\npredict_image(ramdom_noraml_img)\nimage.load_img(ramdom_noraml_img, target_size=(224, 224))","5f14bc67":"ramdom_noraml_img = abnormal_images_list[np.random.randint(len(abnormal_images_list))]\nprint(ramdom_noraml_img)\npredict_image(ramdom_noraml_img)\nimage.load_img(ramdom_noraml_img, target_size=(224, 224))","fbe32dec":"## Saving the Trained Model for Futher Use","17a886f6":"## Viewing Some Random Images","a18576b2":"## Visualizing Model","014f6378":"## Model Evaluation","7e6829f5":"## Method that Predicts the Class to which the Image Belongs","42d9e9e3":"## Kersas Model for Image Classification","16d595c4":"## Getting the Images in Batches and Scaling them","12c3672b":"## Preparing Images for Model","90924bc7":"## Loading The Best Saved Model and Making Predictions","042fc1ba":"## Testing the Model with Random Images","8424e3bd":"## Gettting Random Images and Testing","4bdc29e9":"## Making Prediction on Testing images","52f32128":"## Importing Libraries","feb32f71":"## Data Preprocessing","a99fd9e4":"## Training our Models","912b518b":"## Ploting Graphs for better understanding"}}