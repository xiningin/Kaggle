{"cell_type":{"cdd741da":"code","ddc97159":"code","92f7d0ff":"code","172a9402":"code","1f1230ab":"code","78a0c2e4":"code","f8a0d8c9":"code","08b18ab9":"code","5455be73":"code","25e5e7b3":"code","ffaafb6e":"code","f05364ad":"code","b4e3c9ac":"code","37ecd292":"code","fab80aa4":"code","d3caccf6":"markdown","1745af3f":"markdown","f88b4fc8":"markdown","4fd93843":"markdown","82459250":"markdown","90021ccb":"markdown","91c67a26":"markdown","ea0ed8fd":"markdown"},"source":{"cdd741da":"import numpy as np \nimport pandas as pd\n\nimport os\nprint(os.listdir(\"..\/input\"))","ddc97159":"print(os.listdir(\"..\/input\/frame-sample\/frame\"))","92f7d0ff":"# Loading libraries & datasets\nimport tensorflow as tf\nimport numpy as np\nfrom IPython.display import YouTubeVideo\n\nframe_lvl_record = \"..\/input\/frame-sample\/frame\/train00.tfrecord\"","172a9402":"vid_ids = []\nlabels = []\n\nfor example in tf.python_io.tf_record_iterator(frame_lvl_record):\n    tf_example = tf.train.Example.FromString(example)\n    vid_ids.append(tf_example.features.feature['id']\n                   .bytes_list.value[0].decode(encoding='UTF-8'))\n    labels.append(tf_example.features.feature['labels'].int64_list.value)","1f1230ab":"print('Number of videos in this tfrecord: ',len(vid_ids))\nprint('Picking a youtube video id:',vid_ids[13])","78a0c2e4":"# With that video id, we can play the video\nYouTubeVideo('UzXQaOLQVCU')","f8a0d8c9":"# due to execution time, we're only going to read the first video\n\nfeat_rgb = []\nfeat_audio = []\n\nfor example in tf.python_io.tf_record_iterator(frame_lvl_record):  \n    tf_seq_example = tf.train.SequenceExample.FromString(example)\n    n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n    sess = tf.InteractiveSession()\n    rgb_frame = []\n    audio_frame = []\n    # iterate through frames\n    for i in range(n_frames):\n        rgb_frame.append(tf.cast(tf.decode_raw(\n                tf_seq_example.feature_lists.feature_list['rgb']\n                  .feature[i].bytes_list.value[0],tf.uint8)\n                       ,tf.float32).eval())\n        audio_frame.append(tf.cast(tf.decode_raw(\n                tf_seq_example.feature_lists.feature_list['audio']\n                  .feature[i].bytes_list.value[0],tf.uint8)\n                       ,tf.float32).eval())\n        \n        \n    sess.close()\n    \n    feat_audio.append(audio_frame)\n    feat_rgb.append(rgb_frame)\n    break","08b18ab9":"print('The first video has %d frames' %len(feat_rgb[0]))","5455be73":"from matplotlib import pyplot as plt\n%matplotlib inline\nfrom sklearn.manifold import TSNE\nimport numpy as np","25e5e7b3":"vocab = pd.read_csv('..\/input\/vocabulary.csv')\nprint(\"we have {} unique labels in the dataset\".format(len(vocab['Index'].unique())))","ffaafb6e":"n = 30 # although, we'll only show those that appear in the 1,000 for this competition\nfrom collections import Counter\n\nlabel_mapping =  vocab[['Index', 'Name']].set_index('Index', drop=True).to_dict()['Name']\n\ntop_n = Counter([item for sublist in labels for item in sublist]).most_common(n)\ntop_n_labels = [int(i[0]) for i in top_n]\ntop_n_label_names = [label_mapping[x] for x in top_n_labels if x in label_mapping] # filter out the labels that aren't in the 1,000 used for this competition\nprint(top_n_label_names)","f05364ad":"import networkx as nx\nfrom itertools import combinations\n\nG = nx.Graph()\n\nG.clear()\nfor list_of_nodes in labels:\n    filtered_nodes = set(list_of_nodes).intersection(set(top_n_labels) & \n                                                     set(vocab['Index'].unique()))  \n    for node1,node2 in list(combinations(filtered_nodes,2)): \n        node1_name = label_mapping[node1]\n        node2_name = label_mapping[node2]\n        G.add_node(node1_name)\n        G.add_node(node2_name)\n        G.add_edge(node1_name, node2_name)\n\nplt.figure(figsize=(9,9))\nnx.draw_networkx(G, font_size=\"12\")","b4e3c9ac":"validate_record = \"..\/input\/validate-sample\/validate\/validate00.tfrecord\"","37ecd292":"val_vid_ids = []\nval_vid_labels = []\nsegment_start_times = []\nsegment_end_times = []\nsegment_labels = []\nsegment_scores = []\n\nfor example in tf.python_io.tf_record_iterator(validate_record):\n    tf_example = tf.train.Example.FromString(example)\n    val_vid_ids.append(tf_example.features.feature['id']\n                   .bytes_list.value[0].decode(encoding='UTF-8'))\n    val_vid_labels.append(tf_example.features.feature['labels'].int64_list.value)\n    segment_start_times.append(tf_example.features.feature['segment_start_times'].int64_list.value)\n    segment_end_times.append(tf_example.features.feature['segment_end_times'].int64_list.value)\n    segment_labels.append(tf_example.features.feature['segment_labels'].int64_list.value)\n    segment_scores.append(tf_example.features.feature['segment_scores'].float_list.value)\n    ","fab80aa4":"print(val_vid_ids[0])\nprint(val_vid_labels[0])\nprint(segment_start_times[0])\nprint(segment_end_times[0])\nprint(segment_labels[0])\nprint(segment_scores[0])","d3caccf6":"## Read the validate data","1745af3f":"As described on the YouTube8M download page, for privacy reasons, the video id has been randomly generated and does not directly correspond to the actual YouTube video id. To convert the id into the actua YouTube video id, we follow link: http:\/\/data.yt8m.org\/2\/j\/i\/UL\/UL00.js","f88b4fc8":"This notebook explores the data (TFRecord format) using a subsample of the YouTube-8M `frame-level` and `validate` data. \nTo work with the entire dataset, please refer to the Starter code on the [YouTube-8M github repo](https:\/\/github.com\/google\/youtube-8m)","4fd93843":"By inspecting the labels and segment score data, you can see that label 1036 was in the segment 135:140, but _not_ in the other segments!","82459250":"## Video-level information (extracted from the frame-level files)","90021ccb":"## Plot the relationships between each of these top labels...\n","91c67a26":"## Read the frame-level data","ea0ed8fd":"## Now let's explore the labels\nFirst, we'll find the most commonly used labels..."}}