{"cell_type":{"07d300e3":"code","4951f71d":"code","dab512fb":"code","0a95593b":"code","00c902db":"code","817bfde5":"code","2f2dd801":"code","3baed6eb":"code","a97f7f81":"code","399b63c9":"code","7e299893":"code","e3ebe866":"code","42e7292f":"code","9e2d2ee4":"code","7d4bf01b":"markdown","67bb7006":"markdown","f3f816ec":"markdown","bb089dc0":"markdown","23f8ebed":"markdown","e509c7a6":"markdown","f05a1c5d":"markdown"},"source":{"07d300e3":"# necessary imports \nimport numpy as np\nimport pandas as pd \nimport re\nimport string\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","4951f71d":"# load the data\ndf = pd.read_csv('..\/input\/nlp-getting-started\/train.csv',index_col='id')\ndf.head()","dab512fb":"# check null, type, size \ndf.info()","0a95593b":"# class distribution\nsns.catplot(kind='count',data=df,x='target',aspect=3)\nplt.show()","00c902db":"# drop unnecessary columns\ndf.drop(['location','keyword'],inplace=True,axis=1)\ndf.head","817bfde5":"def process_tweet(tweet):\n    stopwords_english = set(stopwords.words('english'))\n    stemmer = PorterStemmer()\n    \n    tweet = re.sub(r'\\$\\w*','',tweet) # remove words with pattern as $word\n    tweet = re.sub(r'^RT[\\s]+','',tweet) # remove retweet text \"RT\"\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*','',tweet) # remove hyperlinks\n    tweet = re.sub(r'#','',tweet) # remove #tag sign\n    \n    tokenizer = TweetTokenizer(preserve_case=False,strip_handles=True, reduce_len=True)\n    tokens = tokenizer.tokenize(tweet)\n    \n    clean = []\n    for word in tokens:\n        if( word not in stopwords_english and word not in string.punctuation):\n            stem_word = stemmer.stem(word)\n            clean.append(stem_word)\n            \n    return clean","2f2dd801":"# Instead of using tf-idf vectorizer or count vectorizer directly, we want to bring all this in 3 dimensional features\n# Hence, we want features like (bias term, word in positive tweet, word in negative tweet)\n# For that, we can get help from build_freqs() function given below.\ndef build_freqs(tweets,targets):\n    yslist= np.squeeze(targets).tolist()\n    \n    freqs = {}\n    for target, tweet in zip(targets,tweets):\n        for word in process_tweet(tweet):\n            pair = (word, target)\n            freqs[pair] = freqs.get(pair,0) + 1\n    return freqs","3baed6eb":"X = df['text'].values\ny = df['target'].values\nX_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.1,stratify=y,random_state=43) # stratify makes sure distribution is even","a97f7f81":"freqs = build_freqs(X_train,y_train) # frequencies of word associated with label","399b63c9":"# transform word in tweet to (bias, word associated with +ve, word associated with -ve label)\n\ndef extract_features(tweet,freqs):\n    word_l = process_tweet(tweet)\n    x = np.zeros((1,3)) # feature vector\n    x[0,0] = 1 # for bias\n    \n    for word in word_l:\n        x[0,1] += freqs.get((word,1),0)\n        x[0,2] += freqs.get((word,0),0)\n    return x","7e299893":"def train(x,y,xVal,yVal,freqs):\n    xTRAIN = np.zeros((len(x),3))\n    for i in range(len(x)):\n        xTRAIN[i,:] = extract_features(x[i],freqs)\n        \n    model = SGDClassifier(loss='log',n_jobs=-1,max_iter=800,random_state=31) # 31\n    model.fit(xTRAIN,y)\n    \n    # training f1-score: \n    preds = model.predict(xTRAIN)\n    print('Training f1-score is {}'.format(f1_score(y,preds)))\n    \n    # validation f1-score:\n    xVAL = np.zeros((len(xVal),3))\n    for i in range(len(xVal)):\n        xVAL[i,:] = extract_features(xVal[i],freqs)\n    \n    preds = model.predict(xVAL)\n    print('Validation f1-score is {}'.format(f1_score(yVal,preds)))\n    return model\n    \nmodel = train(X_train,y_train,X_val, y_val, freqs)","e3ebe866":"# load test set\ntf = pd.read_csv('..\/input\/nlp-getting-started\/test.csv',index_col='id')\ntf.drop(['keyword','location'],axis=1,inplace=True)\n\ndef predict_tweet(model,tweet, freqs):\n    x = extract_features(tweet,freqs)\n    pred = model.predict(x)\n    return pred\n\ntarget = []\nxTest = tf['text'].values\nfor i in range(len(xTest)):\n    target.append(predict_tweet(model,xTest[i],freqs)[0])","42e7292f":"sub = pd.DataFrame({'id':tf.index, 'target':target}) \n# Prediction counts\nsns.catplot(kind='count',y='target',data=sub)\nplt.show()","9e2d2ee4":"# save submission file\nsub.to_csv('submission.csv', index=False)","7d4bf01b":"# Preprocess data","67bb7006":"# Split dataset ","f3f816ec":"# Train the Model","bb089dc0":"# Feature Extraction","23f8ebed":"Build a machine learning model, which can predict that the given tweet discussing disasters, is real or fake. ","e509c7a6":"# Prediction on Test Set","f05a1c5d":"# Detection of Real Disasters using Simple Model"}}