{"cell_type":{"726d750e":"code","aa2e7b39":"code","79ea4489":"code","40d59a3b":"code","300ec992":"code","1cd8338d":"code","f67397dc":"code","c090b864":"code","4204299f":"code","cbc8cc06":"code","1e72532b":"code","c7080ba2":"code","c911ebfa":"code","b6eca5be":"code","877bdaf8":"code","4a8c1fd3":"code","e24f2f17":"code","2dc7e6a8":"code","f046cc90":"code","eabb0a7c":"code","c87df19d":"code","e48257e5":"code","18371c60":"code","c757dfc9":"code","c0b02d99":"code","3c4cab5d":"code","9bdb9ef7":"code","dde30cd9":"code","92b8be01":"code","12cdb0c0":"code","fcebca94":"code","4a3bd4fc":"code","8f1254be":"code","ae8db222":"markdown","36685d2d":"markdown"},"source":{"726d750e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","aa2e7b39":"df_train = pd.read_csv(\"..\/input\/quora-question-pairs\/train.csv.zip\")","79ea4489":"df_train.head(5)","40d59a3b":"len(df_train)","300ec992":"df_train = df_train[df_train['question1'].apply(lambda x: isinstance(x, str))]\ndf_train = df_train[df_train['question2'].apply(lambda x: isinstance(x, str))]","1cd8338d":"len(df_train)","f67397dc":"import re, string, six\n\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport numpy as np\n\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\n\ndef tokenize(s): \n    return re_tok.sub(r' \\1 ', s).split()\n\ndef clean_text(s):\n    try:\n        return re.sub(r'[^A-Za-z0-9,?\"\\'. ]+', '', s).encode('utf-8').decode('utf-8').lower()\n    except:\n        return \"\"\n\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    try:\n        for word in tokenize(row['question1']):\n            if word not in stops:\n                q1words[word] = 1\n        for word in tokenize(row['question2']):\n            if word not in stops:\n                q2words[word] = 1\n        if len(q1words) == 0 or len(q2words) == 0:\n            return 0\n        shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n        shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n        return (len(shared_words_in_q1) + len(shared_words_in_q2))\/(len(q1words) + len(q2words))\n    except:\n        return 0\n\ndef word_count_diff(row):\n    try:\n        q1words = len(list(filter(lambda x: x.lower() not in stops, tokenize(row['question1']))))\n        q2words = len(list(filter(lambda x: x.lower() not in stops, tokenize(row['question2']))))\n        return abs(q1words - q2words)\n    except:\n        return 50","c090b864":"df_train['wms'] = df_train.apply(word_match_share, axis=1)\ndf_train['wcd'] = df_train.apply(word_count_diff, axis=1)","4204299f":"df_train.head()","cbc8cc06":"df_train.groupby(['is_duplicate']).agg({'wcd': np.mean}).reset_index()","1e72532b":"df_train['question1'] = df_train['question1'].apply(lambda x: clean_text(x))\ndf_train['question2'] = df_train['question2'].apply(lambda x: clean_text(x))","c7080ba2":"import tensorflow as tf\nimport tensorflow_hub as hub\n\nhub_url = \"https:\/\/tfhub.dev\/google\/nnlm-en-dim128-with-normalization\/2\"\nembed = hub.KerasLayer(hub_url, trainable=False)","c911ebfa":"def euc_dist(x, y):\n    return np.sqrt(np.dot((x-y), (x-y)))","b6eca5be":"X_train_q1 = df_train['question1'].tolist()\nX_train_q2 = df_train['question2'].tolist()\nX_wms = df_train['wms'].tolist()\nX_wcd = df_train['wcd'].tolist()\ny_train = (1-df_train['is_duplicate']).tolist()","877bdaf8":"len(X_train_q1)","4a8c1fd3":"from sklearn.model_selection import train_test_split\n\nX_train_q1, X_test_q1, X_train_q2, X_test_q2, X_wms_train, X_wms_test, X_wcd_train, X_wcd_test, y_train, y_test = train_test_split(X_train_q1, X_train_q2, X_wms, X_wcd, y_train, test_size=0.2, random_state=42, stratify=y_train)","e24f2f17":"X_wcd_train[:4]","2dc7e6a8":"import tensorflow as tf\n# tf.config.run_functions_eagerly(False)\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K \nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler","f046cc90":"input1 = Input(shape=(), dtype=tf.string)\ninput2 = Input(shape=(), dtype=tf.string)\ninput_wms = Input(shape=(1,), dtype=tf.float16)\ninput_wcd = Input(shape=(1,), dtype=tf.float16)\n\nembed1 = embed(input1)\nembed2 = embed(input2)\n\ndist = Lambda(lambda x: K.sqrt(K.sum(K.square(x[0] - x[1]), axis=-1, keepdims=True)))([embed1,embed2])\n\nconcat = Concatenate(axis=1)([dist, input_wms, input_wcd])\n\nhidden = Dense(9, activation=\"relu\", kernel_regularizer=l2(1e-4))(concat)\n\nout = Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(1e-4))(hidden)\nmodel = Model(inputs=[input1, input2, input_wms, input_wcd], outputs=out)","eabb0a7c":"model.summary()","c87df19d":"model.compile(optimizer=Adam(1e-3), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])","e48257e5":"# callbacks defined\n\n# learning rate schedule\ndef step_decay(epoch):\n    initial_lrate = 0.003\n    drop = 0.5\n    epochs_drop = 3\n    lrate = initial_lrate * (drop**((1 + epoch)\/epochs_drop))\n    return lrate\n\nlrate_scheduler = LearningRateScheduler(step_decay)\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\nmodel_chkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n\nmodel.fit(x=[np.array(X_train_q1), np.array(X_train_q2), np.array(X_wms_train), np.array(X_wcd_train)],\n          y=np.array(y_train),\n          batch_size=128,\n          epochs=5,\n          validation_data=([np.array(X_test_q1), np.array(X_test_q2), np.array(X_wms_test), np.array(X_wcd_test)], np.array(y_test)),\n          callbacks=[lrate_scheduler, early_stop, model_chkpoint])","18371c60":"df_test = pd.read_csv(\"..\/input\/quora-question-pairs\/test.csv\")","c757dfc9":"df_test.head()","c0b02d99":"df_test['wms'] = df_test.apply(word_match_share, axis=1)\ndf_test['wcd'] = df_test.apply(word_count_diff, axis=1)","3c4cab5d":"df_test['question1'] = df_test['question1'].apply(lambda x: clean_text(x))\ndf_test['question2'] = df_test['question2'].apply(lambda x: clean_text(x))","9bdb9ef7":"df_test.head()","dde30cd9":"X_test_q1 = df_test['question1'].tolist()\nX_test_q2 = df_test['question2'].tolist()\nX_test_wms = df_test['wms'].tolist()\nX_test_wcd = df_test['wcd'].tolist()","92b8be01":"from tqdm import tqdm\npreds = []\nbatch_size = 512\nsteps = len(X_test_q1) \/\/ batch_size + 1\nfor i in tqdm(range(0, steps)):\n    X_test_q1_batch = np.array(X_test_q1[i*batch_size: i*batch_size+batch_size])\n    X_test_q2_batch = np.array(X_test_q2[i*batch_size: i*batch_size+batch_size])\n    X_test_wms_batch = np.array(X_test_wms[i*batch_size: i*batch_size+batch_size])\n    X_test_wcd_batch = np.array(X_test_wcd[i*batch_size: i*batch_size+batch_size])\n    preds.extend(model.predict([X_test_q1_batch, X_test_q2_batch, X_test_wms_batch, X_test_wcd_batch]))","12cdb0c0":"preds = [1 - x[0] for x in preds]","fcebca94":"df_test['is_duplicate'] = preds","4a3bd4fc":"df_test = df_test.drop(['question1', 'question2', 'wms', 'wcd'], axis=1)","8f1254be":"df_test.set_index('test_id').to_csv(\"submission.csv\")","ae8db222":"### meta-features\n* number of common tokens(non-stopwords) in both the questions\n* tokens count difference","36685d2d":"## Please Upvote this notebook if you like the approach "}}