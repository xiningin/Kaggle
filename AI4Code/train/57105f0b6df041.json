{"cell_type":{"b750d2be":"code","06bfe921":"code","49788b74":"code","61a235fa":"code","1a97463e":"code","8711a9c4":"code","7a6b7ef9":"code","cfe5d705":"code","234fe75d":"code","519e1cb9":"code","df660b6b":"code","94d50161":"code","e2b4fb5c":"code","1324619d":"code","31eeead5":"code","23abf89f":"code","bf6e09e8":"code","1770a2a6":"code","7116daf5":"code","2f34129d":"code","9ba8e4dd":"code","c351eccd":"code","b6547e88":"code","49ecd54e":"code","4721867e":"code","a6ae43b7":"code","3e8c0852":"code","984059cd":"code","d5c76f5e":"code","53252f4e":"code","e25ef5a1":"code","72d9880c":"code","9a792032":"code","c3ecb8e5":"code","7fa57f5d":"code","746bb98a":"code","799df820":"code","f43214dd":"code","71434402":"code","f9c8317b":"code","f7c7c52b":"code","693d12ad":"code","fb09f0f4":"code","64dfb220":"code","b599fa02":"code","740f2819":"markdown","15e49fd2":"markdown","828019a5":"markdown","88e23697":"markdown","615b3337":"markdown","b9497ed0":"markdown","49647a5c":"markdown","9d67ef67":"markdown","6176c985":"markdown","9936f516":"markdown","83f3e3ca":"markdown","a7742a27":"markdown","2176932f":"markdown","1115367a":"markdown","4d12cc14":"markdown","d7b75b1c":"markdown","ca0d4d58":"markdown","575d895d":"markdown"},"source":{"b750d2be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","06bfe921":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","49788b74":"train.head()","61a235fa":"train_df = train.copy()","1a97463e":"train.info()","8711a9c4":"train.describe()","7a6b7ef9":"train.isnull().sum()","cfe5d705":"# Checking for categorical features\n\ncategorical_col = []\nfor column in train.columns:\n    if train[column].dtype == object and len(train[column].unique()) <= 50:\n        categorical_col.append(column)\n        print(f\"{column} : {train[column].unique()}\")\n        print(\"====================================\")","234fe75d":"numerical_col = []\nfor column in train.columns:\n    if train[column].dtype != object and len(train[column].unique()) <= 50:\n        numerical_col.append(column)\n        print(f\"{column} : {train[column].unique()}\")\n        print(\"====================================\")","519e1cb9":"# Visulazing the distibution of the data for every feature\ntrain.hist(edgecolor='black', linewidth=1.2, figsize=(20, 20));","df660b6b":"plt.figure(figsize=(20,10))\nsns.heatmap(train.isnull(), cmap='viridis')","94d50161":"# Extracting the columns which have missing values from the dataset\nmissing_values = [feature for feature in train.columns if train[feature].isnull().sum() >1]\nprint(\"The features having the missing values are\",missing_values,end='')","e2b4fb5c":"for feature in missing_values:\n    print(feature, 'has', np.round(train[feature].isnull().mean(),2), '% of missing values')","1324619d":"sns.distplot(train['SalePrice'])","31eeead5":"train['SalePrice'] = np.log(train['SalePrice'] + 1)\nsns.distplot(train['SalePrice'])","23abf89f":"print(categorical_col,end='')","bf6e09e8":"for feature in categorical_col:\n    temp = train.groupby(feature)['SalePrice'].count()\/len(train) #Calculating the percentage\n    temp_df = temp[temp>0.01].index\n    train[feature] = np.where(train[feature].isin(temp_df), train[feature], 'Rare_var')\n","1770a2a6":"train.head()","7116daf5":"# Label encoder basically converts categorical values into numerical values\n\nfrom sklearn.preprocessing import LabelEncoder\n\nsc=LabelEncoder()\n\nfor feature in categorical_col:\n\n    train[feature]=sc.fit_transform(train[feature])","2f34129d":"train.head()","9ba8e4dd":"for feature in missing_values:\n    print(feature, 'has', np.round(train[feature].isnull().mean(),2), '% of missing values')","c351eccd":"train['LotFrontage'] = train['LotFrontage'].fillna(train['LotFrontage'].mean())\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(train['MasVnrArea'].mean())\ntrain['GarageYrBlt'] = train['GarageYrBlt'].fillna(train['GarageYrBlt'].mean())","b6547e88":"train.head()","49ecd54e":"# Splitting the features into independent and dependent variables\n\nx = train.drop(['SalePrice'], axis = 1)\ny = train['SalePrice']","4721867e":"from sklearn.ensemble import ExtraTreesRegressor\n\nmodel = ExtraTreesRegressor()\nmodel.fit(x,y)","a6ae43b7":"print(model.feature_importances_)","3e8c0852":"#plotting graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","984059cd":"sns.distplot(train['SalePrice'])","d5c76f5e":"#Spliting data into test and train\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.20)","53252f4e":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\n\nlr.fit(x_train, y_train)\n\nlr_pred = lr.predict(x_test)","e25ef5a1":"r2 = r2_score(y_test,lr_pred)\nprint('R-Square Score: ',r2*100)","72d9880c":"# Calculate the absolute errors\nlr_errors = abs(lr_pred - y_test)\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(lr_pred), 2), 'degrees.')","9a792032":"# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (lr_errors \/ y_test)\n# Calculate and display accuracy\nlr_accuracy = 100 - np.mean(mape)\nprint('Accuracy for Logistic Regression is :', round(lr_accuracy, 2), '%.')","c3ecb8e5":"from sklearn.metrics import mean_absolute_error,mean_squared_error\n\nprint('mse:',metrics.mean_squared_error(y_test, lr_pred))\nprint('mae:',metrics.mean_absolute_error(y_test, lr_pred))","7fa57f5d":"sns.distplot(y_test-lr_pred)","746bb98a":"# plotting the Linear Regression values predicated Rating\n\nplt.figure(figsize=(12,7))\n\nplt.scatter(y_test,x_test.iloc[:,2],color=\"blue\")\nplt.title(\"True rate vs Predicted rate\",size=20,pad=15)\nplt.xlabel('Sale Price',size = 15)\nplt.scatter(lr_pred,x_test.iloc[:,2],color=\"yellow\")","799df820":"from sklearn.tree import DecisionTreeRegressor\n\ndtree = DecisionTreeRegressor(criterion='mse')\ndtree.fit(x_train, y_train)","f43214dd":"dtree_pred = dtree.predict(x_test)","71434402":"r2 = r2_score(y_test,dtree_pred)\nprint('R-Square Score: ',r2*100)\n\n# Calculate the absolute errors\ndtree_errors = abs(dtree_pred - y_test)\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(dtree_pred), 2), 'degrees.')\n\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (dtree_errors \/ y_test)\n# Calculate and display accuracy\ndtree_accuracy = 100 - np.mean(mape)\nprint('Accuracy for Decision tree regressor is :', round(dtree_accuracy, 2), '%.')","f9c8317b":"#plotting the Decision Tree values predicated Rating\n\nplt.figure(figsize=(12,7))\n\nplt.scatter(y_test,x_test.iloc[:,2],color=\"blue\")\nplt.title(\"True rate vs Predicted rate\",size=20,pad=15)\nplt.xlabel('Sale Price',size = 15)\nplt.scatter(dtree_pred,x_test.iloc[:,2],color=\"yellow\")\nplt.legend()","f7c7c52b":"from sklearn.ensemble import RandomForestRegressor\n\nrandom_forest_regressor = RandomForestRegressor()\nrandom_forest_regressor.fit(x_train, y_train)\nrf_pred = random_forest_regressor.predict(x_test)","693d12ad":"r2 = r2_score(y_test,rf_pred)\nprint('R-Square Score: ',r2*100)\n\n# Calculate the absolute errors\nrf_errors = abs(rf_pred - y_test)\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(rf_pred), 2), 'degrees.')\n\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (rf_errors \/ y_test)\n# Calculate and display accuracy\nrf_accuracy = 100 - np.mean(mape)\nprint('Accuracy for random forest regressor is :', round(rf_accuracy, 2), '%.')\n","fb09f0f4":"#plotting the Random forest values predicated Rating\n\nplt.figure(figsize=(12,7))\n\nplt.scatter(y_test,x_test.iloc[:,2],color=\"blue\")\nplt.title(\"True rate vs Predicted rate\",size=20,pad=15)\nplt.xlabel('Sale Price',size = 15)\nplt.scatter(rf_pred,x_test.iloc[:,2],color=\"yellow\")","64dfb220":"pred_y = (lr_pred*0.45 + dtree_pred*0.55 + rf_pred*0.65)","b599fa02":"pred_y","740f2819":"# Data fields\n\nHere's a brief version of what you'll find in the data description file.\n\n* **SalePrice** - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* **MSSubClass**: The building class\n* **MSZoning**: The general zoning classification\n* **LotFrontage**: Linear feet of street connected to property\n* **LotArea**: Lot size in square feet\n* **Street**: Type of road access\n* **Alley**: Type of alley access\n* **LotShape**: General shape of property\n* **LandContour**: Flatness of the property\n* **Utilities**: Type of utilities available\n* **LotConfig**: Lot configuration\n* **LandSlope**: Slope of property\n* **Neighborhood**: Physical locations within Ames city limits\n* **Condition1**: Proximity to main road or railroad\n* **Condition2**: Proximity to main road or railroad (if a second is present)\n* **BldgType**: Type of dwelling\n* **HouseStyle**: Style of dwelling\n* **OverallQual**: Overall material and finish quality\n* **OverallCond**: Overall condition rating\n* **YearBuilt**: Original construction date\n* **YearRemodAdd**: Remodel date\n* **RoofStyle**: Type of roof\n* **RoofMatl**: Roof material\n* **Exterior1st**: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: $Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale","15e49fd2":"* Now if the percentage is greater than 0.015 then only i am going to take the categorical feature otherwise i am going to say it as rare variable ('Rare_var')","828019a5":"### Building the model","88e23697":"* We can see that our dependent feature is slightly right skewed this can affect the accuracy of our model so we need to convert it to normal distribution.","615b3337":"### Feature Selection","b9497ed0":"* We can see that our dataset contains lot of missing values so we need to handle them accordingly","49647a5c":"#### Label Encoding the categorical features\n\n* **Label Encoding** : Label encoder basically converts categorical values into numerical values","9d67ef67":"### Checking for numerical features","6176c985":"### Visualizing the Dependent feature","9936f516":"### Visualizing the missing values with the help of heatmap.","83f3e3ca":"### Converting all the categorical columns into numerical","a7742a27":"### Checking for categorical features","2176932f":"### Applying Random Forest Regressor Algorithm","1115367a":"* In our dataset excluding the dependent feature we have 80 indenpendent feature If we consider all the 80 columns as our independent feature our model accuracy will decrease, as the number of features increases the accuracy decreases this is called as the **Curse Of Dimentionality**\n\n* In order to solve this problem there are several ways to eliminate this problem like PCA, dropping the useless columns etc.\n\n* But in our case we will use a library under sklearn called as **Extra Tree Regressor**, what it does is that it returns use only those features which are important for model building, prediction and the features which helps us it increase the accuracy of the model.\n\n* Feature importance gives you a score for each feature of your data, the higher the score the more important or relevant is the feature towards your output variable\n\n* Feature importance is an in built class that comes with Tree Based Regressor, we will be using Extra Tree Regressor for extracting the top 10 features for the dataset\n","4d12cc14":"### Filling the missing values","d7b75b1c":"### Applying Linear Regression Algorithm","ca0d4d58":"### Applying Decision tree Regressor","575d895d":"* We have normalized our dependent feature into Gaussian Distribution to fit our model properly"}}