{"cell_type":{"ca0d0890":"code","1f07f914":"code","cd86a81f":"code","34616125":"code","6bb488cc":"code","128bae15":"code","1b49feb0":"code","9482cf6f":"code","3a237425":"code","1d5dd61c":"code","c0bdabec":"code","158c3ed9":"code","b418fb22":"code","b9faa911":"code","94bc45dd":"code","c7473092":"code","7ec00175":"code","78761ae0":"code","df07aa3b":"code","6bbfed8f":"code","6e1e7ec7":"code","d4178ffa":"code","ba279584":"code","905ceb51":"markdown","fed1b5d4":"markdown","cc76d23a":"markdown","295ad3a0":"markdown","38aae108":"markdown"},"source":{"ca0d0890":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\ndf_train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ndf_test = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')\ndf_SS = pd.read_csv('..\/input\/cat-in-the-dat\/sample_submission.csv')","1f07f914":"#importing libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.preprocessing import OneHotEncoder \nfrom sklearn import preprocessing \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \n\n\n\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn import svm\n\nfrom sklearn.model_selection import GridSearchCV","cd86a81f":"df_train[\"target\"].value_counts()#No missing values","34616125":"#encoding categorical columns\ndf_train_bin= df_train.iloc[:,1:6]\n\n#USING CUSTOM MAP FOR BIN_3 AND BIN_4\nbin_3_mapper = {'F':0, \n                'T':1}\n\ndf_train_bin['bin_3C'] = df_train_bin['bin_3'].replace(bin_3_mapper)\n\nbin_4_mapper = {'N':0, \n                'Y':1}\n\ndf_train_bin['bin_4C'] = df_train_bin['bin_4'].replace(bin_4_mapper)\ndf_train_bin_cleaned=  df_train_bin.iloc[:,[0,1,2,5,6]]\n\n#USING ONE HOT ENCODING FOR NOMINAL FEATURES\ndf_train_nom=df_train.iloc[:,6:16]\n\nonehotencoder = OneHotEncoder() \n  \ndf_train_nom_cleaned = pd.get_dummies(df_train_nom.iloc[:,[0,1,2,3,4,5,6,7,8,9]])  ##col 5,6,7,8,9 removed\n\n#df_train_nom_cleaned['nom_7C'] = label_encoder.fit_transform(df_train_nom['nom_7'])  \n#df_train_nom_cleaned['nom_8C'] = label_encoder.fit_transform(df_train_nom['nom_8'])  \n#df_train_nom_cleaned['nom_9C'] = label_encoder.fit_transform(df_train_nom['nom_9'])  \ndf_train_nom_cleaned\n","6bb488cc":"#USING CUSTOM MAP LABELS FOR ORDINAL FEATURES\ndf_train_ord_time = df_train.iloc[:,16:24]\n\n#ord_1 mapping\nord_1_mapper = {'Novice':0, \n                'Contributor':1,\n               'Expert':2,\n                'Master':3,\n               'Grandmaster':4}\n\ndf_train_ord_time['ord_1C'] = df_train_ord_time['ord_1'].replace(ord_1_mapper)\n\n#ord_2 mapping\nord_2_mapper = {'Freezing':0, \n                'Cold':1,\n               'Warm':2,\n                'Hot':3,\n               'Boiling Hot':4,\n                'Lava Hot':5\n               }\n\ndf_train_ord_time['ord_2C'] = df_train_ord_time['ord_2'].replace(ord_2_mapper)\n\n#ord_3 mapping\nord_3_mapper = {'a':0, 'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14\n               }\n\ndf_train_ord_time['ord_3C'] = df_train_ord_time['ord_3'].replace(ord_3_mapper)\n\n#ord_4 mapping\ndf_train_ord_time['ord_4C'] = df_train_ord_time['ord_4'].apply(lambda x: ord(x)-65)\n\n#ord_5 mapping\nlabel_encoder = preprocessing.LabelEncoder() \ndf_train_ord_time['ord_5C'] = label_encoder.fit_transform(df_train_ord_time['ord_5'])   \n\ndf_train_ord_time_cleaned = df_train_ord_time.iloc[:,[0,6,7,8,9,10,11,12]]#6,7 time\n\ndf_train_all =  pd.concat([df_train_bin_cleaned,df_train_ord_time_cleaned], axis=1) #, df_train_nom_cleaned\n\ndf_train_all\n","128bae15":"X_train, X_test, y_train, y_test = train_test_split(df_train_all,df_train.iloc[:,24].values , test_size=0.2, random_state=42)","1b49feb0":"X_train1= X_train.iloc[0:1000,:] # (240000, 13)\ny_train1=y_train[0:1000]\n","9482cf6f":"clf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train1,y_train1)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)","3a237425":"y_train","1d5dd61c":"import graphviz\n\ndata = export_graphviz(clf,out_file=None,feature_names=X_train.columns,class_names=['0','1'],   \n                         filled=True, rounded=True,  \n                         special_characters=True)\ngraph = graphviz.Source(data)\ngraph","c0bdabec":"\"\"\"#create an instance and fit the model \nlogmodel = LogisticRegression(C=0.1338,\n                        solver=\"lbfgs\",\n                        tol=0.0003,\n                        max_iter=5000)# cv: if integer then it is the numbe\nlogmodel.fit(X_train, y_train)\ny_pred = logmodel.predict_proba(X_test)\nroc_auc_score(y_test,y_pred[:, 1]) #7830551086693487 ,0.7829327708379799\n","158c3ed9":"\"\"\"ogistic = LogisticRegression()\n# Create regularization penalty space\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 2)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\n\n# Create grid search using 5-fold cross validation\nclf = GridSearchCV(logistic, hyperparameters, cv=2, verbose=1,scoring='roc_auc')\n\nbest_model = clf.fit(X_train, y_train)","b418fb22":"\"\"\"p=best_model.predict_proba(X_test)\nroc_auc_score(y_test,p[:, 1]) #0.7831027261785495","b9faa911":"\"\"\"logit_roc_auc = roc_auc_score(y_test,y_pred )\nfpr, tpr, thresholds = roc_curve(y_test, logmodel.predict_proba(X_test))\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()\"\"\"\n","94bc45dd":"\"\"\"\nmodel_xg = XGBClassifier(show_progress=True,verbose_eval=50,silent =1)\nmodel_xg.fit(X_train, y_train)\ny_pred_xg = model_xg.predict_proba(X_test)\nroc_auc_score(y_test,y_pred_xg[:, 1])\"\"\"\n","c7473092":"\"\"\"SVC_model = svm.SVC(kernel='linear', C = 1.0)\nSVC_model.fit(X_train, y_train)\n\ny_pred_svc = SVC_model.predict_proba(X_test)\nroc_auc_score(y_test,y_pred_svc[:, 1])\"\"\"","7ec00175":"\"\"\"from keras import Sequential\nfrom keras.layers import Dense\n","78761ae0":"\"\"\"classifier = Sequential()\n#First Hidden Layer\nclassifier.add(Dense(8, activation='relu', kernel_initializer='random_normal', input_dim=13))\n#Second  Hidden Layer\nclassifier.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n#3  Hidden Layer\nclassifier.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n#4  Hidden Layer\nclassifier.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n#5  Hidden Layer\nclassifier.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n#6  Hidden Layer\nclassifier.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n#Output Layer\nclassifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n\nclassifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])","df07aa3b":"\"\"\"classifier.fit(X_train,y_train, batch_size=10, epochs=2) # 0.7261\n\n","6bbfed8f":"\"\"\"pred_neural = classifier.predict(X_test)\n\nroc_auc_score(y_test,pred_neural) #0.7831027261785495","6e1e7ec7":"#Predicting values\n\"\"\"\n#encoding categorical columns\ndf_test_bin= df_test.iloc[:,1:6]\n\n#USING CUSTOM MAP FOR BIN_3 AND BIN_4\n\n\ndf_test_bin['bin_3C'] = df_test_bin['bin_3'].replace(bin_3_mapper)\n\n\n\ndf_test_bin['bin_4C'] = df_test_bin['bin_4'].replace(bin_4_mapper)\ndf_test_bin_cleaned=  df_test_bin.iloc[:,[0,1,2,5,6]]\n\n#USING ONE HOT ENCODING FOR NOMINAL FEATURES\ndf_test_nom=df_test.iloc[:,6:16]\n\nonehotencoder = OneHotEncoder() \n  \ndf_test_nom_cleaned = pd.get_dummies(df_test_nom.iloc[:,[0,1,2,3,4,5,6]] )  ##col 7,8,9 removed\n\n#USING CUSTOM MAP LABELS FOR ORDINAL FEATURES\ndf_test_ord_time = df_test.iloc[:,16:24]\n\n#ord_1 mapping\n\n\ndf_test_ord_time['ord_1C'] = df_test_ord_time['ord_1'].replace(ord_1_mapper)\n\n#ord_2 mapping\n\n\ndf_test_ord_time['ord_2C'] = df_test_ord_time['ord_2'].replace(ord_2_mapper)\n\n#ord_3 mapping\n\ndf_test_ord_time['ord_3C'] = df_test_ord_time['ord_3'].replace(ord_3_mapper)\n\n#ord_4 mapping\ndf_test_ord_time['ord_4C'] = df_test_ord_time['ord_4'].apply(lambda x: ord(x)-65)\n\n#ord_5 mapping\nlabel_encoder = preprocessing.LabelEncoder() \ndf_test_ord_time['ord_5C'] = label_encoder.fit_transform(df_test_ord_time['ord_5'])   \n\ndf_test_ord_time_cleaned = df_test_ord_time.iloc[:,[0,6,7,8,9,10,11,12]]\n\ndf_test_all =  pd.concat([df_test_bin_cleaned,df_test_nom_cleaned,df_test_ord_time_cleaned], axis=1) \n\n","d4178ffa":"\"\"\"y_pred_actual = logmodel.predict_proba(df_test_all)\ny_pred_actual ","ba279584":"\"\"\"Id =  pd.DataFrame(df_test['id'] , columns=['id'])\npredictions_Log  = pd.DataFrame(y_pred_actual[:,1]  , columns=['target']) \nid_predictions_Log = pd.concat([Id,predictions_Log],axis=1)\nid_predictions_Log.head()\npd.DataFrame(id_predictions_Log, columns=['id','target']).to_csv('id_predictions_Log.csv',index = False)","905ceb51":"# Logistic regression","fed1b5d4":"# SVM","cc76d23a":"# Decision trees","295ad3a0":"# XG BOOST","38aae108":"# DEEP"}}