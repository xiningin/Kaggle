{"cell_type":{"49b2cf53":"code","be28bb25":"code","10ce9079":"code","8b605ece":"code","0c1d0fec":"code","db03c87f":"code","34a20720":"code","ebac83df":"code","e07cc252":"code","5087942d":"code","d10d7dfc":"code","6c5548dc":"code","f0066839":"code","35c152a1":"code","4a49f0f3":"code","f7325dbd":"code","7dd47a81":"code","8b6d0f82":"code","f8df5447":"code","3670a4eb":"code","9ca55f88":"code","c4950c16":"code","cd79d75b":"code","be43d7c2":"code","113cbde2":"code","1e6e98a1":"code","d6843ef5":"code","7110e585":"code","1f09527b":"code","3600ca00":"code","15a3c24c":"code","6c2ba6ae":"code","60f77c91":"code","81ceced5":"code","39ce8916":"code","171ae3a9":"code","1d5278d5":"code","b2668bbb":"code","d8dbefc9":"code","1bb3a518":"code","6a20abb8":"code","75253a0c":"code","e434e971":"code","a7508e17":"code","81db983c":"code","9d65783c":"code","3ea70a7f":"code","e1c00ac5":"code","a9554327":"code","eaecb0fe":"markdown","d571374b":"markdown","d620aa5c":"markdown","01dd829a":"markdown","a5d5460d":"markdown","3fe06a65":"markdown","859cea8c":"markdown","e6ed504e":"markdown","48f65122":"markdown","ce6783c6":"markdown","ff5600cb":"markdown","35474ea7":"markdown","72e4a9ca":"markdown","b1ddac3a":"markdown","693d10c3":"markdown","d1207d83":"markdown","2d84acfb":"markdown","174ae7fa":"markdown","ac0963bc":"markdown","00e73e27":"markdown","2aec22af":"markdown","c938892f":"markdown","ce25c054":"markdown","f45eba45":"markdown","9102f934":"markdown","cf266bc7":"markdown","cde3062a":"markdown","1cc5f898":"markdown","209d6a6b":"markdown","8a4b6442":"markdown","02e232d6":"markdown","9757fd91":"markdown","dcca6d71":"markdown","87f76ca3":"markdown","c3e166f9":"markdown","96c1d70c":"markdown","5b41627d":"markdown","246c8e74":"markdown","7e550737":"markdown","4ec16634":"markdown","b22d63b1":"markdown","70fa8b5f":"markdown"},"source":{"49b2cf53":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm.notebook import tqdm\nfrom IPython.display import YouTubeVideo\ntqdm.pandas()","be28bb25":"df = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='latin8')\ndf.head()","10ce9079":"def create_folds(X,y):\n    \n    df['kfold'] = -1\n    \n    splitter = StratifiedKFold(n_splits=5)\n    \n    for f, (t_, v_) in enumerate(splitter.split(X, y)):\n        \n        X.loc[v_, 'kfold'] = f\n        \n    return X","8b605ece":"df = create_folds(df, df['Sentiment'])\ndf.head()","0c1d0fec":"df = df[['OriginalTweet', 'Sentiment', 'kfold']]\ndf.head(2)","db03c87f":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom nltk.tokenize import TweetTokenizer, word_tokenize","34a20720":"sentences = df['OriginalTweet'][:5]","ebac83df":"for i in sentences[2:3]:\n    print(\"Original:\\n\")\n    print(i)\n    print('\\nTensorflow Tokenizer\\n:')\n    a = Tokenizer()\n    a.fit_on_texts([i])\n    print(a.word_index)\n    print(\"\\nTweet Tokenizer:\\n\")\n    print(TweetTokenizer().tokenize(i))\n    print('\\nNLTK word_tokenizer:\\n')\n    print(word_tokenize(i))","e07cc252":"tweets = []\n\nfor i in tqdm(df['OriginalTweet']):\n    \n    tweet = TweetTokenizer().tokenize(i)\n    tweet = ' '.join(tweet)\n    tweets.append(tweet)","5087942d":"for i in tweets[:3]:\n    print(i, '\\n')","d10d7dfc":"from gensim.models import KeyedVectors\nfrom gensim import downloader\n\nembedding_file = '..\/input\/embeddings\/GoogleNews-vectors-negative-300d.bin'\n\nembedding_model =  KeyedVectors.load_word2vec_format(embedding_file, binary=True)","6c5548dc":"def build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","f0066839":"vocab = build_vocab([tweet.split() for tweet in tweets])\nprint({k: vocab[k] for k in list(vocab)[:5]})","35c152a1":"import operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","4a49f0f3":"oov = check_coverage(vocab,embedding_model)","f7325dbd":"oov[:20]","7dd47a81":"def clean_text(x):\n\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019':\n        x = x.replace(punct, '')\n    return x","8b6d0f82":"for index, tweet in enumerate(tweets):\n    \n    tweets[index] = clean_text(tweet)\n\n\nvocab = build_vocab([tweet.split() for tweet in tweets])","f8df5447":"oov = check_coverage(vocab,embedding_model)","3670a4eb":"oov[:20]","9ca55f88":"\"crisis\" in embedding_model","c4950c16":"\"distancing\" in embedding_model","cd79d75b":"len(oov)","be43d7c2":"count = 0\nindex = 0\n\nwhile((count != 30) and count < len(oov)):\n    \n    if len(oov[index][0]) > 3:\n        print(oov[index])\n        count += 1\n        \n    index += 1","113cbde2":"to_replace = [('COVID', 'health crisis'),\n            ('COVID19', 'health crisis'),\n            ('Covid19', 'health crisis'),\n            ('Covid', 'health crisis'),\n            ('COVID2019', 'health crisis'),\n            ('covid19', 'health crisis'),\n            ('toiletpaper', 'toilet paper'),\n            ('covid', 'health crisis'),\n            ('CoronaCrisis', 'health crisis'),\n            ('CoronaVirus', 'health crisis'),\n            ('SocialDistancing', 'Social distancing'),\n            ('2020', 'this year'),\n            ('CoronavirusPandemic', 'health crisis'),\n            ('CoronavirusOutbreak', 'health crisis'),\n            ('StayHomeSaveLives', 'Stay Home Save Lives'),\n            ('StayAtHome', 'Stay At Home'),\n            ('StayHome', 'Stay Home'),\n            ('panicbuying', 'Panic Buying'),\n            ('socialdistancing', 'Social Distancing'),\n            ('CoronaVirusUpdate', 'health crisis update'),\n            ('StopHoarding', 'Stop Hoarding'),\n            ('realDonaldTrump', 'real Donald Trump'),\n            ('StopPanicBuying', 'Stop Panic Buying'),\n            ('covid19UK', 'health crisis'),\n            ('QuarantineLife', 'Quarantine life'),\n            ('behaviour', 'behave')]","1e6e98a1":"to_replace_dict = {}\n\nfor i in to_replace:\n    \n    to_replace_dict[i[0]] = i[1]","d6843ef5":"for index, tweet in tqdm(enumerate(tweets)):\n    \n    cleaned_tweet = []\n    \n    for word in tweet.split():\n        \n        if len(word) > 2:\n            \n            if word in to_replace_dict:              \n                cleaned_tweet.append(to_replace_dict[word])\n            else:\n                cleaned_tweet.append(word)\n                \n    tweets[index] = ' '.join(cleaned_tweet)","7110e585":"vocab = build_vocab([tweet.split() for tweet in tweets])","1f09527b":"oov = check_coverage(vocab,embedding_model)","3600ca00":"count = 0\nindex = 0\n\nwhile((count != 30) and count < len(oov)):\n    \n    if len(oov[index][0]) > 3:\n        print(oov[index])\n        count += 1\n        \n    index += 1","15a3c24c":"to_check = ['fuck', 'motherfucker', ':)', \":{\", 'bastard', ':(']\n\nfor i in to_check:\n    if i in embedding_model:\n        print('yes')\n    else:\n        print('no')","6c2ba6ae":"TweetTokenizer().tokenize('This word has a :) face')","60f77c91":"from nltk.stem import SnowballStemmer, WordNetLemmatizer\n\nword = 'elegant'\nstem_word = SnowballStemmer('english').stem(word)\nlemma = WordNetLemmatizer().lemmatize(word)\n\nprint(\"Stem word: \", stem_word)\nprint(\"\\nLemma: \", lemma)\n\nprint(\"\\nIs stemmed word present in embedding :\", stem_word in embedding_model)\nprint(\"\\nIs lemma present in embedding :\", lemma in embedding_model)","81ceced5":"word1 = 'feet'\nword2 = 'foot'\n\nprint(WordNetLemmatizer().lemmatize(word1))\n\nprint(word1 in embedding_model)\nprint(word2 in embedding_model)","39ce8916":"!pip install nlpaug","171ae3a9":"import nlpaug.augmenter.word as naw","1d5278d5":"sent = 'All month there hasn been crowding the supermarkets restaurants however reducing all the hours and closing the malls means everyone now using the same entrance and dependent single supermarket manila lockdown covid2019 Philippines https tco HxWs9LAnF9'\nprint('original: ', sent)\nprint('\\nAugmented: ', naw.SynonymAug(aug_src='wordnet').augment(sent))","b2668bbb":"YouTubeVideo('BBR3J2HI5xI')","d8dbefc9":"YouTubeVideo('VpLAjOQHaLU')","1bb3a518":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","6a20abb8":"'not' in stop_words","75253a0c":"words = ['break the rules', 'free time', 'draw a conclusion', 'keep in mind', 'get ready']\n\nfor i in words:\n    \n    print(i in embedding_model)","e434e971":"from fuzzywuzzy import fuzz\nfrom fuzzywuzzy import process","a7508e17":"fuzz.ratio(\"this is a test\", \"this is a test!\")","81db983c":"fuzz.partial_ratio(\"this is a test\", \"this is a test!\")","9d65783c":"fuzz.token_sort_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")","3ea70a7f":"fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")","e1c00ac5":"fuzz.token_sort_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")","a9554327":"fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")","eaecb0fe":"We will remove the punctuation which is not in the embeddings","d571374b":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"5\" > <\/a>\n## 5.Traditional Data prep and where to use it\n\nBy now you could be wondering where is all the Stemming, Lemmatization and etc etc. <br>\nWell the thing is we generally don't need those when using pretrained embeddings. Why so? Well let's have a look. ","d620aa5c":"Ok. Now what ?? <br>\nWell, Now we can do data cleaning but before that we have to see how we should do that. And for that I have a very good kernel which I will take insights from. - https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings. I will still write the code but for better explanation and deeper understanding I would highly recommend that notebook and also a couple more from the same author.","01dd829a":"## What is this Notebook about?","a5d5460d":"???? Why are we only at 88%. Let's  look again at our oov words.","3fe06a65":"In short, we will load the embeddings and see how much vocablary is covered by the embeddings.","859cea8c":"Now for converting into different language you should see this - https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/discussion\/48038","e6ed504e":"#### Using tf-idf weighted embeddings you you an extra edge in most of the cases. At least I find so.","48f65122":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"3\" > <\/a>\n## 3.Cleaning (Like thoughtful cleaning)","ce6783c6":"OH MAN I HATE THIS. Why can't they just use normal english and use Space between characters. Also as you can see \"can't\" is written as \"can\u00c2\" and thanks to that I can't use someone else cleaning code. Well, I just want to show you that there are things that you need to take care of.","ff5600cb":"USE CASE - A couple years back there was a Competition \"Quora Question Pair Similarity..\" in which we had to predict given 2 questions whether they are simmilar or not and the following features could be very useful.","35474ea7":"So, we did improve a lot but as we can see we still have a significat portion of vocab which still has no embeddings. <br>\nThe main reason is our use case that is \"Covid-19\" which itself is a new term and hence the previously trained word embeddings will be useless. So, what can we possibly do in this case. Well, In my opinion we do have one option and that to replace every \"COVID\" occurance with \"crisis\" ( Just a word for which we have embedding) also we can replace the \"SocialDistancing\" with distancing.","72e4a9ca":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"8\" > <\/a>\n## 8.A look at Collection Extraction\n\nA good read - https:\/\/medium.com\/@datamonsters\/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908","b1ddac3a":"# Table of Contents\n1. [Prepairing the data](#1)\n2. [Tokenization](#2)\n3. [Problem based Cleaning](#3)\n4. [Where embedding might fail](#4)\n5. [Traditional data prep and where to use it](#5)\n6. [Augmentation](#6)\n7. [Resolving StopWords](#7)\n8. [A look at Collection extraction](#8)\n9. [Similarity Analysis Among Sentences and feature extraction](#9)","693d10c3":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"2\" > <\/a>\n## 2.Tokenization\nWhile tokenizing the tweets we have many tokenizers to choose from. Here we have to wise and to see what gives what?","d1207d83":"In this notebook I will try to share different aspects of a NLP problem and well see what difference one makes. This is not a traditional approach to a pproblem but a compairative one.","2d84acfb":"### I would recommend these 2 videos:","174ae7fa":"A good look - https:\/\/github.com\/seatgeek\/fuzzywuzzy#usage","ac0963bc":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"6\" > <\/a>\n## 6.Augmentation\n\nWell, this part is really less discussed and I am also not too sure how to deal with it. For all the video and articles\/blogs I have read regarding this I found 2 very useful.","00e73e27":"### Token Sort Ration - After sorting tokens how much match","2aec22af":"As you can see eleg is not a word and isn't present in the embedding and hence it will only reduce our coverage. let's have a few more examples for understanding it better.","c938892f":"### Simple Ratio - How much macthing.","ce25c054":"As you can see, we may or may not have embeddings for all the words and especially emoticons and nowdays emoticons are really popular. I would recommend to make your own embeddings as it will have better coverage. One more thing is that you could use the pretrained embeddings and then just finetune them. <br> Also let me show you the good thing about tweet_tokenizer of nltk.","f45eba45":"You see these are not individual words but have a meaning due to continuty. You will find the solution in the above mentioned read. <br>\nAlso, the n-gram approach of BOW and Tf-idf could be very helpful in this context.","9102f934":"### Token Set Ratio - Matching Ratio after making a set of tokens","cf266bc7":"<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>","cde3062a":"As you can see these all yield different results and you have to see which works best for your use case. For now we will use NLTK Tweet-Tokenizer.","1cc5f898":"<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>","209d6a6b":"You see. it recognizes the emoticons and that's really helpful while making embeddings.","8a4b6442":"Before we make these changes let's look at which oov words have a significant length.","02e232d6":"Oh man I hate cleaning. But well what can I say it's damn important.","9757fd91":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"1\" > <\/a>\n## 1.Prepairing the data","dcca6d71":"#### Now the question is where to use this Stemming then ?\n\nWell, the answer to that question is not so simple. I will give you 2 places wher you could use Stemming and Lemmmatization. More likely stemming.\n    * 1) We have small data and the pretrained embeddings doesn't have good vocab coverage ( Also you can search for domain wise\n    pretrained embeddings and you will likey find one.) or you just don't want to use pretrained embeddings,in such a case stemming will be \n    very useful as it will provide better vectors for words as after stemming the occurance will be incresed significantly.\n    \n    * 2) Instead of using simple embeddings you could use Tf-idf weighted embeddings and can use stemming in the creation of Tf-idf vectors.","87f76ca3":"* 1) Use Synonyms of words and make new sentences by replacing the word by its synonyms.\n* 2) Convert text into another language and then convert it back again to the original language.","c3e166f9":"Before doing anything else let's first create folds for our dataset.","96c1d70c":"I apolozise in advance for the mention of these words. I am using them simply to show the importance.","5b41627d":"Well as you can see both feet and foot are present in our embedding and so if do lemmatize the word we will lose the availabe variance and hence we should not do that. <br>\nTo be truthful you could try lemmatization and see what results it yields and then choose wheather to use it or not.","246c8e74":"### Partial Ration - Does it have a partial match","7e550737":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"7\" > <\/a>\n## 7.Resolving StopWords\nWell, there's nothing much to say here but a small remainder \"Be careful as 'not' is also a stopword\"","4ec16634":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"4\" > <\/a>\n## 4.Some other examples where embeddings might fail\n\nWhat we have seen is only one use case. Some time ago there was this competition \"Toxic comment classification\" which used many emoticons and some vulgur words and so let's have a look at our embedding and see if we have anything related to that.","b22d63b1":"Pretty good write!!! <br>\nIt would be interseting to explore this library further. I will list a few more such libraries : <br>\n    * textattack\n    * textaugment","70fa8b5f":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"9\" > <\/a>\n## 9.Similarity Analysis among sentences"}}