{"cell_type":{"c8ab2770":"code","b16c9cd6":"code","21d32208":"code","35cc0738":"code","38f4bb45":"code","c14a21b4":"code","1578f0a1":"code","357f519d":"code","f8531eed":"code","f6fd80c5":"code","d7601c75":"code","bed0e6ec":"code","2cf4669b":"code","dfe4b14f":"code","f9b4211f":"code","f17ee542":"code","0a98c98b":"code","67830452":"code","f6891fff":"code","b96abd17":"code","daae4b2f":"code","bed35b67":"code","282deb24":"code","827cffd4":"code","5aacd2ff":"code","fed1eff8":"code","21ddf874":"code","deea4c49":"code","9528e49d":"code","7095f416":"code","e70f728c":"code","2cb356c8":"code","b471cfae":"code","22922ac4":"code","a3220c91":"code","a6fc60e7":"code","f3a238c1":"code","4e2a98bd":"code","b5b3a903":"code","2805577d":"markdown","eb90228a":"markdown","463792fd":"markdown","783fe2cb":"markdown","60898dcf":"markdown","32a444ba":"markdown","5d49d4bd":"markdown","2b6d49fe":"markdown"},"source":{"c8ab2770":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b16c9cd6":"pd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.max_rows\", 200)","21d32208":"country_info = pd.read_csv(\"\/kaggle\/input\/countryinfo\/covid19countryinfo.csv\")\ncountry_info = country_info.rename({\"region\": \"state\"}, axis=1)\ncountry_info.loc[country_info[\"state\"].isna(), \"state\"] = \"Unknown\"\ncountry_info = country_info.drop([col for col in country_info.columns if \"Unnamed\" in col], axis=1)\ncountry_info[\"pop\"] = country_info[\"pop\"].str.replace(',', '').astype(float)\n\npollution = pd.read_csv(\"\/kaggle\/input\/pollution-by-country-for-covid19-analysis\/region_pollution.csv\")\npollution = pollution.rename({\"Region\": \"country\",\n                             \"Outdoor Pollution (deaths per 100000)\": \"outdoor_pol\",\n                             \"Indoor Pollution (deaths per 100000)\": \"indoor_pol\"}, axis=1)\n\neconomy = pd.read_csv(\"\/kaggle\/input\/the-economic-freedom-index\/economic_freedom_index2019_data.csv\", engine='python')\neconomy_cols = [col for col in economy.columns if economy[col].dtype == \"float64\"] + [\"Country\"]\neconomy = economy[economy_cols]\neconomy = economy.rename({\"Country\": \"country\"}, axis=1)\n\ndef append_external_data(df):\n    df = pd.merge(df, country_info, on=[\"country\", \"state\"], how=\"left\")\n    df = pd.merge(df, pollution, on=\"country\", how=\"left\")\n    df = pd.merge(df, economy, on=\"country\", how=\"left\")\n    return df","35cc0738":"country_info.head()","38f4bb45":"country_info[\"publicplace\"] = np.where(country_info[\"publicplace\"].str.contains(\"\/\"), country_info[\"publicplace\"], np.nan)","c14a21b4":"country_info[\"publicplace\"].value_counts()","1578f0a1":"country_info.columns","357f519d":"list_rel_columns = ['state', 'country', 'pop', 'tests',\n       'testpop', 'density', 'medianage', 'urbanpop', 'quarantine', 'schools',\n       'publicplace', 'gatheringlimit', 'gathering', 'nonessential',\n       'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54', 'sex64',\n       'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung', 'gdp2019',\n       'healthexp', 'healthperpop', 'fertility']","f8531eed":"country_info = country_info[list_rel_columns]","f6fd80c5":"country_info.loc[country_info[\"country\"] == \"China\"]","d7601c75":"def aggregate_label(df):\n    country_df = df[[\"country\", \"Date\", \"ConfirmedCases\", \"Fatalities\"]].groupby([\"country\", \"Date\"], as_index=False).sum()\n    country_df = country_df.rename({\"ConfirmedCases\": \"country_cases\", \"Fatalities\": \"country_fatalities\"}, axis=1)\n    df = pd.merge(df, country_df, on=[\"country\", \"Date\"], how=\"left\")\n    return df","bed0e6ec":"def calculate_days_since_event(df, feature_name, casualties, casualties_amount, groupby=[\"country\"]):\n    cases_df = df.loc[df[casualties] > casualties_amount][groupby + [\"Date\"]].groupby(groupby, as_index=False).min()\n    cases_df = cases_df.rename({\"Date\": \"relevant_date\"}, axis=1)\n    df = pd.merge(df, cases_df, on=groupby, how=\"left\")\n    df[feature_name] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(df[\"relevant_date\"])).dt.days\n    df.loc[df[feature_name] < 0, feature_name] = 0\n    df = df.drop(\"relevant_date\", axis = 1)\n    return df","2cf4669b":"def generate_time_features(df):\n    df = calculate_days_since_event(df, \"days_from_first_death\", \"Fatalities\", 0, [\"country\"])\n    df = calculate_days_since_event(df, \"days_from_first_case\", \"ConfirmedCases\", 0, [\"country\"])\n    df = calculate_days_since_event(df, \"days_from_first_case_province\", \"ConfirmedCases\", 0, [\"country\", \"state\"])\n    df = calculate_days_since_event(df, \"days_from_first_death_province\", \"Fatalities\", 0, [\"country\", \"state\"])\n    df = calculate_days_since_event(df, \"days_from_centenary_case\", \"ConfirmedCases\", 99, [\"country\"])\n    df = calculate_days_since_event(df, \"days_from_centenary_case_province\", \"Fatalities\", 99, [\"country\", \"state\"])\n    df = calculate_days_since_event(df, \"days_from_centenary_daily_cases_province\", \"ConfirmedCases_daily\", 99, [\"country\", \"state\"])\n    df = calculate_days_since_event(df, \"days_from_centenary_daily_cases\", \"ConfirmedCases_daily\", 99, [\"country\"])\n    \n    # Days from first detected case\n    df[\"days_from_first_ever_case\"] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(\"2019-12-01\")).dt.days\n    df.loc[df[\"days_from_first_ever_case\"] < 0, \"days_from_first_ever_case\"] = 0\n    \n    #Days from quarantine, school closures and restrictions\n    df[\"days_from_quarantine\"] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(df[\"quarantine\"])).dt.days\n    df[\"days_from_quarantine\"].fillna(0)\n    df.loc[df[\"days_from_quarantine\"] < 30, \"days_from_quarantine\"] = 0\n    df.loc[df[\"days_from_quarantine\"] >= 30, \"days_from_quarantine\"] = 1\n\n    df[\"days_from_school\"] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(df[\"schools\"])).dt.days\n    df[\"days_from_school\"].fillna(df[\"days_from_quarantine\"])\n    df.loc[df[\"days_from_school\"] < 30, \"days_from_school\"] = 0\n    df.loc[df[\"days_from_school\"] >= 30, \"days_from_school\"] = 1\n\n    df[\"days_from_publicplace\"] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(df[\"publicplace\"])).dt.days\n    df[\"days_from_publicplace\"].fillna(df[\"days_from_quarantine\"])\n    df.loc[df[\"days_from_publicplace\"] < 30, \"days_from_publicplace\"] = 0\n    df.loc[df[\"days_from_publicplace\"] >= 30, \"days_from_publicplace\"] = 1\n    \n    df[\"days_from_gathering\"] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(df[\"gathering\"])).dt.days\n    df[\"days_from_gathering\"].fillna(df[\"days_from_quarantine\"])\n    df.loc[df[\"days_from_gathering\"] < 30, \"days_from_gathering\"] = 0\n    df.loc[df[\"days_from_gathering\"] >= 30, \"days_from_gathering\"] = 1\n    \n    df[\"days_from_nonessential\"] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(df[\"nonessential\"])).dt.days\n    df[\"days_from_nonessential\"].fillna(df[\"days_from_quarantine\"])\n    df.loc[df[\"days_from_nonessential\"] < 30, \"days_from_nonessential\"] = 0\n    df.loc[df[\"days_from_nonessential\"] >= 30, \"days_from_nonessential\"] = 1\n    \n    return df","dfe4b14f":"def generate_ar_features(df, group_by_cols, value_cols):\n    \n    # Daily cases\n    diff_df = df.groupby(group_by_cols)[value_cols].diff().fillna(0)\n    diff_df.columns = [col + \"_daily\" for col in value_cols]\n    value_cols += [col + \"_daily\" for col in value_cols]\n    df = pd.concat([df, diff_df], axis=1)\n    \n    # Daily percentage increase\n    pct_df = df.groupby(group_by_cols)[value_cols].pct_change().fillna(0)\n    pct_df.columns = [col + \"_pct_change\" for col in value_cols]\n    value_cols += [col + \"_pct_change\" for col in value_cols]\n    df = pd.concat([df, pct_df], axis=1)\n\n    # Shift to yesterday's data\n    yesterday_df = df.groupby(group_by_cols)[value_cols].shift()\n    value_cols = [col + \"_yesterday\" for col in value_cols]\n    yesterday_df.columns = value_cols\n    df = pd.concat([df, yesterday_df], axis=1)\n\n    # Average of the percentage change in the last 3 days\n    three_days_avg = df.groupby(group_by_cols)[value_cols].rolling(3).mean()\n    three_days_avg = three_days_avg.reset_index()[value_cols]\n    three_days_avg.columns = [col + \"_3_day_avg\" for col in three_days_avg.columns]\n    df = pd.concat([df, three_days_avg], axis=1)\n\n    # Average of the percentage change in the last 7 days\n    seven_days_avg = df.groupby(group_by_cols)[value_cols].rolling(7).mean()\n    seven_days_avg = seven_days_avg.reset_index()[value_cols]\n    seven_days_avg.columns = [col + \"_7_day_avg\" for col in seven_days_avg.columns]\n    df = pd.concat([df, seven_days_avg], axis=1)\n    \n    df = df.replace([np.inf, -np.inf], 0)\n    \n    return df","f9b4211f":"def generate_features(df):\n    group_by_cols = [\"state\",\"country\"]\n    value_cols = [\"ConfirmedCases\", \"Fatalities\", \"country_cases\", \"country_fatalities\"]\n    \n    df = aggregate_label(df)\n    df = append_external_data(df)\n    df = generate_ar_features(df, group_by_cols, value_cols)\n    df = generate_time_features(df)\n    df[\"dow\"] = pd.to_datetime(df[\"Date\"]).dt.dayofweek\n    df.loc[df[\"ConfirmedCases_yesterday\"]<0, \"ConfirmedCases_yesterday\"] = 0\n    df.loc[df[\"Fatalities_yesterday\"]<0, \"Fatalities_yesterday\"] = 0\n    return df","f17ee542":"train = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv\")\ntrain = train.rename({\"Province_State\": \"state\", \"Country_Region\": \"country\"}, axis=1)\ntrain.loc[train[\"state\"].isna(), \"state\"] = \"Unknown\"\ntrain = generate_features(train)\nprint(train[\"Date\"].min(), \"-\", train[\"Date\"].max())\ntrain.loc[train[\"country\"] == \"Italy\"].tail()","0a98c98b":"train.loc[train[\"state\"] == \"Hubei\"]","67830452":"test = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv\")\ntest = test.rename({\"Province_State\": \"state\", \"Country_Region\": \"country\"}, axis=1)\ntest.loc[test[\"state\"].isna(), \"state\"] = \"Unknown\"\nprint(test[\"Date\"].min(), \"-\", test[\"Date\"].max())\ntest.head()","f6891fff":"train.loc[train[\"Date\"]<\"2020-03-22\", \"split\"] = \"train\"\ntrain.loc[train[\"Date\"]>=\"2020-03-22\", \"split\"] = \"test\"","b96abd17":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nfeatures = [col for col in train.columns if (\"yesterday\" in col) | (\"days_from\" in col)]\nfeatures += country_info.select_dtypes(include=numerics).columns.tolist()\nfeatures += pollution.select_dtypes(include=numerics).columns.tolist()\nfeatures += economy.select_dtypes(include=numerics).columns.tolist()\nfeatures += [\"dow\"]","daae4b2f":"len(features)","bed35b67":"import lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error","282deb24":"def train_model(df, label, base_label, features=features, **kwargs):\n    X_train = df.loc[df[\"split\"] == \"train\"][features]\n    y_train = np.log(df.loc[df[\"split\"] == \"train\"][label] + 1)\n    b_train = np.log(df.loc[df[\"split\"] == \"train\"][base_label] + 1)\n    X_test = df.loc[df[\"split\"] == \"test\"][features]\n    y_test = np.log(df.loc[df[\"split\"] == \"test\", label] + 1)\n    b_test = np.log(df.loc[df[\"split\"] == \"test\", base_label] + 1)\n    print(kwargs)\n    model = lgb.LGBMRegressor(**kwargs)\n    model.fit(X_train, y_train, init_score = b_train)\n    y_pred = model.predict(X_test)\n    print(np.sqrt(mean_squared_error(y_test, y_pred + b_test)))\n    return model","827cffd4":"lgb_model_cases = train_model(train, \"ConfirmedCases\", \"ConfirmedCases_yesterday\",\n                                   max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8)","5aacd2ff":"import shap\nexplainer = shap.TreeExplainer(lgb_model_cases)\nsample = train.loc[train[\"split\"] == \"test\"].sample(500)\nshap_values = explainer.shap_values(sample[features])\nshap.summary_plot(\n    shap_values,\n    sample[features],\n    max_display=110,\n    show=True,\n)","fed1eff8":"lgb_model_fatalities = train_model(train, \"Fatalities\", \"Fatalities_yesterday\", features,\n                                   max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8\n                                  )","21ddf874":"import shap\nexplainer = shap.TreeExplainer(lgb_model_fatalities)\nsample = train.loc[train[\"split\"] == \"test\"].sample(500)\nshap_values = explainer.shap_values(sample[features])\nshap.summary_plot(\n    shap_values,\n    sample[features],\n    max_display=110,\n    show=True,\n)","deea4c49":"X_train = train[features]\ny_train = np.log(train[\"ConfirmedCases\"] + 1)\nb_train = np.log(train[\"ConfirmedCases_yesterday\"] + 1)\ncases_model = lgb.LGBMRegressor(max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8\n                               )\ncases_model.fit(X_train, y_train, init_score = b_train)\n\nX_train = train[features]\ny_train = np.log(train[\"Fatalities\"] + 1)\nb_train = np.log(train[\"Fatalities_yesterday\"] + 1)\nfatalities_model = lgb.LGBMRegressor(max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8)\nfatalities_model.fit(X_train, y_train, init_score = b_train)","9528e49d":"base_df = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv\")\nbase_df = base_df.rename({\"Province_State\": \"state\", \"Country_Region\": \"country\"}, axis=1)\nbase_df.loc[base_df[\"state\"].isna(), \"state\"] = \"Unknown\"\nscoring_dates = test[\"Date\"].unique()","7095f416":"scoring_dates","e70f728c":"from datetime import datetime as dt, timedelta","2cb356c8":"pred_df = pd.DataFrame(columns=base_df.columns)\nfor date in scoring_dates.tolist():\n    print(date)\n    new_df = base_df.loc[base_df[\"Date\"] < date].copy()\n    curr_date_df = test.loc[test[\"Date\"] == date].copy()\n    curr_date_df[\"ConfirmedCases\"] = 0\n    curr_date_df[\"Fatalities\"] = 0\n    new_df = new_df.append(curr_date_df).reset_index(drop=True)\n    new_df = generate_features(new_df)\n    new_df[features] = new_df[features]\n    predictions = cases_model.predict(new_df[features]) + np.log(new_df[\"ConfirmedCases_yesterday\"] + 1)\n    new_df[\"predicted_cases\"] = round(np.maximum(np.exp(predictions) - 1, new_df[\"ConfirmedCases_yesterday\"]))\n    predictions = fatalities_model.predict(new_df[features]) + np.log(new_df[\"Fatalities_yesterday\"] + 1)\n    new_df[\"predicted_fatalities\"] = np.maximum(np.exp(predictions) - 1, new_df[\"Fatalities_yesterday\"])\n    new_df[\"predicted_fatalities\"] = round(np.minimum(new_df[\"predicted_fatalities\"], new_df[\"predicted_cases\"]*0.2))\n    new_df.loc[new_df[\"Date\"] == date, \"ConfirmedCases\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_cases\"]\n    new_df.loc[new_df[\"Date\"] == date, \"Fatalities\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_fatalities\"]\n    pred_df = pred_df.append(new_df.loc[new_df[\"Date\"] == date][pred_df.columns.tolist()])\n    if date not in base_df[\"Date\"].unique():\n        base_df = base_df.append(new_df.loc[new_df[\"Date\"] == date][base_df.columns.tolist()])","b471cfae":"pred_df.loc[pred_df[\"state\"] == \"Hubei\"]","22922ac4":"pred_df.loc[pred_df[\"country\"] == \"Italy\"]","a3220c91":"pred_df.loc[pred_df[\"country\"] == \"Israel\"]","a6fc60e7":"pred_df.loc[pred_df[\"country\"] == \"Argentina\"]","f3a238c1":"pred_df.loc[pred_df[\"country\"] == \"Uruguay\"]","4e2a98bd":"test = pd.merge(test, pred_df[[\"state\", \"country\", \"Date\", \"ConfirmedCases\", \"Fatalities\"]], on=[\"state\", \"country\", \"Date\"], how=\"left\")","b5b3a903":"test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].to_csv(\"submission.csv\", index=False)","2805577d":"# Feature Engineering","eb90228a":"## Predict submission dates","463792fd":"# Data Gathering","783fe2cb":"## Fatalities","60898dcf":"## Train on full data","32a444ba":"## Cases","5d49d4bd":"## LightGBM","2b6d49fe":"# Modeling"}}