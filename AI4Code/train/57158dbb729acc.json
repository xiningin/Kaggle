{"cell_type":{"28cfd244":"code","6f003e06":"code","1fb98b96":"code","da30a15e":"code","57c9d41c":"code","b25df91b":"code","05938f2a":"code","b54af56f":"code","8ee5c22f":"code","f2533f42":"code","17e565f3":"code","ea75e0ca":"code","01dc59e8":"code","358d0296":"code","9d5996a3":"markdown","aa080474":"markdown"},"source":{"28cfd244":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split #Splitting data into test and validation data\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import average_precision_score, roc_auc_score, f1_score, precision_score, \\\nrecall_score, cohen_kappa_score, classification_report,confusion_matrix\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6f003e06":"#Read the data\nX = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nX_test_full = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","1fb98b96":"#Overview\nprint(X.isnull().sum())\nprint(X_test_full.isnull().sum())\nX.head()","da30a15e":"X.columns","57c9d41c":"# MISSING VALUES\n\n#Starting with age, we can impute median values grouping by sex and class.\n#This will have a higher accuracy than simply using a global median.\nX['Age'] = X.groupby(['Pclass','Sex'])['Age'].apply(lambda x: x.fillna(x.median()))\nX_test_full['Age'] = X_test_full.groupby(['Pclass','Sex'])['Age'].apply(lambda x: x.fillna(x.median()))\n\n#There are 2 missing values in the training data for embarked. \n#Thanks to some detective work (https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial)\n#We know that these two people embarked from Southampton, so we can impute 'S'\nX['Embarked'] = X['Embarked'].fillna('S')\n\n#There is one missing value in the test data for fare.\n#Fare will be linked to Pclass, Sibsp and Parch\nX_test_full['Fare'] = X_test_full.groupby(['Pclass', 'SibSp', 'Parch'])['Fare'].apply(lambda x: x.fillna(x.median()))\n#Alternative method:\n# fare = X_test_full[(X_test['Pclass']==3) & (X_test['Parch']==0) & (X_test['SibSp']==0)]['Fare'].median()\n# X_test_full['Fare'] = X_test_full['Fare'].fillna(fare)\n\n#For cabin data, where passengers didn't survive they couldn't retrieve cabin information\n#The letter at the beggining of the cabin data related to the deck and has a direct impact on outcome\n#Extract letter, group decks by similar class distribtuion and fill in missing values with 'M'\nX['Deck'] = X['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'M')\nX['Deck'] = X['Deck'].replace(['A', 'B', 'C', 'T'], 'ABC')\nX['Deck'] = X['Deck'].replace(['D', 'E'], 'DE')\nX['Deck'] = X['Deck'].replace(['F', 'G'], 'FG')\nX.drop(['Cabin'], axis=1, inplace=True)\nX_test_full['Deck'] = X_test_full['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'M')\nX_test_full['Deck'] = X_test_full['Deck'].replace(['A', 'B', 'C', 'T'], 'ABC')\nX_test_full['Deck'] = X_test_full['Deck'].replace(['D', 'E'], 'DE')\nX_test_full['Deck'] = X_test_full['Deck'].replace(['F', 'G'], 'FG')\nX_test_full.drop(['Cabin'], axis=1, inplace=True)","b25df91b":"X.head()","05938f2a":"#DATA ENGINEERING\n\n#Extracting the useful information from name, ie. the title of the passenger\n#There are several different titles so we can group them\nX['Title'] = X['Name'].str.split(',', expand=True)[1].str.split('. ', expand=True)[0]\nX['Title'] = X['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms', regex=True)\nX['Title'] = X['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev','th'], 'Professional', regex=True)\nX.drop(['Name'], axis=1, inplace=True)\nX_test_full['Title'] = X_test_full['Name'].str.split(',', expand=True)[1].str.split('. ', expand=True)[0]\nX_test_full['Title'] = X_test_full['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms', regex=True)\nX_test_full['Title'] = X_test_full['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev','th'], 'Professional', regex=True)\nX_test_full.drop(['Name'], axis=1, inplace=True)\n\n#There is no information in ticket that wont already be in fare or cabin, so this column can be dropped\n\n#Splitting test data\ny = X.Survived\nX.drop(['Survived'], axis=1, inplace=True)\n\n#Dividing into test and validation data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.6, \n                                                               test_size=0.4, random_state=0)\n\n#Columns we will use:\nmy_cols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Deck', 'Title']\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data\nX_train = pd.get_dummies(X_train, columns=['Sex','Embarked','Deck','Title'])\nX_valid = pd.get_dummies(X_valid, columns=['Sex','Embarked','Deck','Title'])\nX_test = pd.get_dummies(X_test, columns=['Sex','Embarked','Deck','Title'])\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","b54af56f":"X_train.shape","8ee5c22f":"#MODEL\n\nproto_model = GradientBoostingClassifier(random_state=0)\nproto_model.fit(X_train,y_train)\nproto_score = proto_model.score(X_valid,y_valid)\nprint(\"Model Score:\" , proto_score)","f2533f42":"#OPTIMIZING\n#Using Bayesian Optimisation\n\ndef hyperopt_train_test(params): \n    model = GradientBoostingClassifier(\n                        learning_rate=params['learning_rate'],\n                        min_samples_leaf=params['min_samples_leaf'],\n                        max_depth = params['max_depth'],\n                        max_features = params['max_features'],\n                        random_state=0)\n\n    model.fit(X_train, y_train)\n    return {\n        'learning_rate': params['learning_rate'],\n        'min_samples_leaf': params['min_samples_leaf'],\n        'max_depth': params['max_depth'],\n        'max_features': params['max_features'],     \n        'train_ROCAUC': roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]),\n        'valid_ROCAUC': roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1]),\n        'train_PRAUC': average_precision_score(y_train, model.predict_proba(X_train)[:, 1]),\n        'valid_PRAUC': average_precision_score(y_valid, model.predict_proba(X_valid)[:, 1]),\n        'recall': recall_score(y_valid, model.predict(X_valid)),\n        'precision': precision_score(y_valid, model.predict(X_valid)),\n        'f1_score': f1_score(y_valid, model.predict(X_valid)),\n        'train_accuracy': model.score(X_train, y_train),\n        'validation_accuracy': model.score(X_valid, y_valid),\n    }\n\ndef f(params):\n    res = hyperopt_train_test(params)\n    res['loss'] = - res['valid_ROCAUC']\n    res['status'] = STATUS_OK \n    return res \n\nspace4gbc = {\n        'learning_rate': hp.uniform('learning_rate', 0.05, 0.3),\n        'min_samples_leaf': hp.choice('min_samples_leaf', range(15, 200)),\n        'max_depth': hp.choice('max_depth', range(2, 20)),\n        'max_features': hp.choice('max_features', range(5, 18))\n}","17e565f3":"trials = Trials() #Showing trials as they are run\nbest = fmin(f, space4gbc, algo=tpe.suggest, max_evals=50, trials=trials) #minimises our function across parameter space\nprint(best)","ea75e0ca":"#We can use the validation accuracy score to sort trials\npd.DataFrame(trials.results).sort_values(by='validation_accuracy', ascending=False).head(5)","01dc59e8":"#SUBMISSION\n\n#We choose our parameters from above:\n\nmodel = GradientBoostingClassifier(learning_rate=0.234658,\n                                   min_samples_leaf=54,\n                                   max_depth =2,\n                                   max_features =11,\n                                   random_state=0)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\noutput = pd.DataFrame({'PassengerId': X_test_full.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","358d0296":"#FEATURE IMPORTANCE\nimport eli5\neli5.explain_weights(model, feature_names=list(X_train.columns))","9d5996a3":"* PassengerID = Unique ID, no relevance to 'Survived' \n\n* **Survived = Target variable, 0\/1**\n\n* Pclass = Passenger class, categorical ordinal feature that can be left alone:\n   * 1 = Upper Class\n   * 2 = Middle Class\n   * 3 = Lower Class\n   \n* Name = Passenger name, only relevance to outcome could be in the title (Mr,Mrs,Miss). Otherwise drop\n\n* Sex = Male or Female. Should be OH encoded\n\n* Age = Passenger age. Around 25% of values are missing and can be imputed\n\n* SibSp = Total number of passengers' siblings and spouses. Can be left alone\n\n* Parch = Total number of passengers' parents and children. Can be left alone\n\n* Ticket = Ticket number of the passenger\n\n* Fare = Passenger fare. Has missing value to be imputed but can otherwise be left alone\n\n* Cabin = Cabin number of the passenger. Majority of values are missing so needs work\n\n* Embarked = Port of embarkation, categorical non-ordinal feature with missing values to be imputed, then should be OH encoded:\n   * C = Cherbourg\n   * Q = Queenstown\n   * S = Southampton","aa080474":"If you have any comments, suggestions, questions etc. please comment below!"}}