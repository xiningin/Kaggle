{"cell_type":{"2db8ac0b":"code","95344f12":"code","2b789c8f":"code","a5919fc4":"code","8c4758dc":"code","796456a1":"code","598ba39e":"code","0524a27e":"code","cfc2b91d":"code","af41a30b":"code","4973e96c":"code","fcc52302":"code","4b22d7ee":"code","82dd2795":"code","457e9e99":"code","64d9ee10":"code","774921f2":"code","0c2b4606":"code","9bf6610c":"code","babe4618":"code","70d61c1a":"code","63c46acc":"code","911cd80f":"code","a8c3c899":"code","ba808565":"code","10a29536":"code","9c143877":"code","260bccbd":"code","f3b2799d":"code","cf7e2c15":"code","8933e8c2":"code","204557a6":"code","6cc8cedc":"code","2167f060":"code","11734360":"code","02442efc":"markdown","522419f0":"markdown","5fce5193":"markdown","815e5dbc":"markdown","ef545d7b":"markdown","92ad6bb5":"markdown","ea36a5eb":"markdown","2560d4f8":"markdown","6ed34d05":"markdown","21b222f5":"markdown","845bd9b5":"markdown","f1538bbc":"markdown","646cbe27":"markdown","69e641d7":"markdown","f522073f":"markdown","b314b9df":"markdown"},"source":{"2db8ac0b":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport numpy as np","95344f12":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","2b789c8f":"data.shape","a5919fc4":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(data, data['Class']):\n    df_train = data.loc[train_index]\n    df_test = data.loc[test_index]\n\nprint(df_train.shape)\nprint(df_test.shape)","8c4758dc":"print(\"Train Set Class column:\")\nprint(df_train.Class.value_counts(normalize=True))\n\nprint(\"\\nTest Set Class column:\")\nprint(df_test.Class.value_counts(normalize=True))","796456a1":"df_train.info()","598ba39e":"df_train.columns[df_train.isnull().any()]","0524a27e":"df_train[[\"Time\", \"Amount\", \"Class\"]].describe()","cfc2b91d":"df_train.Time.nunique()","af41a30b":"# Setting some parameters to plot better graphs\ncustom_params = {\"axes.spines.right\": False, \n                 \"axes.spines.top\": False,  \n                 \"font.family\": \"arial\", \n                 \"figure.figsize\": (18, 6)}\nsns.set_theme(style=\"ticks\", rc=custom_params)","4973e96c":"g = sns.histplot(df_train.Time, bins=100, label=\"Time\", )\ng.set_xlabel(\"Time\")\ng.set_ylabel(\"Frequency\")\ng.set_title(\"Time Distribution\")\nplt.show()","fcc52302":"g = sns.boxplot(x=\"Class\", y=\"Time\", data=df_train)\ng.set_title(\"Time Distribution by Class\")\nplt.show()","4b22d7ee":"g = sns.histplot(df_train.Amount, bins=100, label=\"Amount\")\ng.set_ylabel(\"Frequency\")\ng.set_title(\"Amount Distribution\")\nplt.show()","82dd2795":"g = sns.boxplot(x=\"Class\", y=\"Amount\", data=df_train)\ng.set_title(\"Amount Distribution by Class\")\nplt.show()","457e9e99":"from sklearn.preprocessing import RobustScaler\n\nrob_amount = RobustScaler()\nrob_time = RobustScaler()\ndf_train['scaled_amount'] = rob_amount.fit_transform(df_train['Amount'].values.reshape(-1,1))\ndf_train['scaled_time'] = rob_time.fit_transform(df_train['Time'].values.reshape(-1,1))\n\ndf_train.drop(['Time','Amount'], axis=1, inplace=True)","64d9ee10":"corr = df_train.corr()\ng = sns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values,\n            cmap='coolwarm_r',\n            \n            )\ng.set_title(\"Linear Correlation Heatmap\")\nplt.show()","774921f2":"fig, ax = plt.subplots(3, 1, figsize=(18,10))\ng0 = sns.scatterplot(x=\"scaled_amount\", y=\"V2\", data=df_train, ax=ax[0], hue=\"Class\", alpha=0.5)\ng1 = sns.scatterplot(x=\"scaled_amount\", y=\"V5\", data=df_train, ax=ax[1], hue=\"Class\", alpha=0.5)\ng2 = sns.scatterplot(x=\"scaled_time\", y=\"V3\", data=df_train, ax=ax[2], hue=\"Class\", alpha=0.5)\nplt.tight_layout()\nplt.show()","0c2b4606":"fig, ax = plt.subplots(1, 2, figsize=(18,6))\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=df_train, ax=ax[0])\nsns.boxplot(x=\"Class\", y=\"V5\", data=df_train, ax=ax[1])\nplt.show()","9bf6610c":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df_train, df_train['Class']):\n    df_train_ml = data.loc[train_index]\n    df_valid = data.loc[test_index]\n\nX_train = df_train_ml.drop(['Class'], axis=1)\nX_valid = df_valid.drop(['Class'], axis=1)\ny_train = df_train_ml['Class']\ny_valid = df_valid['Class']","babe4618":"from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.base import clone\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\nimport xgboost as xgb","70d61c1a":"params_grid = {\n                \"n_estimators\": [100, 500, 1000],\n                \"max_depth\": [2, 5, 10],\n                \"learning_rate\": [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1.0],\n                \"gamma\": stats.reciprocal(0.001, 0.1),\n                \"subsample\": np.arange(0.1, 1.0, 0.1),\n                \"colsample_bytree\": np.arange(0.1, 1.0, 0.1),\n                \"scale_pos_weight\": [5, 10, 20, 50, 100],\n                \"n_jobs\": [-1],\n                \"use_label_encoder\": [False],\n                \"random_state\": [42]\n        }\n\nfit_params = {\n                \"early_stopping_rounds\": 5,\n                \"eval_metric\":[\"auc\"],\n                \"eval_set\": [(X_valid, y_valid)],\n                \"verbose\":0\n        }","63c46acc":"xgb_clf = xgb.XGBClassifier()\nrand_grid = RandomizedSearchCV(xgb_clf, \n                               params_grid, \n                               n_iter=10, \n                               cv=5, \n                               scoring=\"recall\", \n                               random_state=42,\n                               verbose=1,\n                               n_jobs=-1)\npipe = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_grid)\nrand_grid.fit(X_train, y_train, **fit_params)\npreds = rand_grid.predict(X_valid)\n\nf1 = f1_score(y_valid, preds)\nprecision = precision_score(y_valid, preds)\nrecall = recall_score(y_valid, preds)\nprint(\"F1 Score: %.3f\" %f1)\nprint(\"Precision: %.3f\" %precision)\nprint(\"Recall: %.3f\" %recall)","911cd80f":"cm = confusion_matrix(y_valid, preds, normalize='true', labels=[0,1])\nplt.figure(figsize=(6,6))\ng = sns.heatmap(cm, annot=True, fmt=\".2%\", cmap=\"Blues\")\ng.set_title(\"Validation Confusion Matrix\")\ng.set_xlabel(\"Predicted Class\")\ng.set_ylabel(\"True Class\")\nplt.show()","a8c3c899":"best_xgb = rand_grid.best_estimator_","ba808565":"best_xgb","10a29536":"X = df_train.drop(['Class'], axis=1)\ny = df_train['Class']\n\nsmote = SMOTE(sampling_strategy='minority')\nX_res, y_res = smote.fit_resample(X, y)\nbest_xgb.fit(X_res, y_res)","9c143877":"!pip install shap","260bccbd":"import shap \n\nexplainer = shap.Explainer(best_xgb)\n\nshap_values = explainer(X, check_additivity=False)","f3b2799d":"shap.summary_plot(shap_values, X)","cf7e2c15":"df_test['scaled_amount'] = rob_amount.transform(df_test['Amount'].values.reshape(-1,1))\ndf_test['scaled_time'] = rob_time.transform(df_test['Time'].values.reshape(-1,1))\n\ndf_test.drop(['Time','Amount'], axis=1, inplace=True)","8933e8c2":"X_test = df_test.drop([\"Class\"], axis=1)\ny_test = df_test[\"Class\"]","204557a6":"test_preds = best_xgb.predict(X_test)\ny_test.index = range(len(y_test))\n\ny_test_1 = y_test[y_test == 1]\ntest_preds_1 = test_preds[y_test_1.index]\n\ny_test_0 = y_test[y_test == 0]\ntest_preds_0 = test_preds[y_test_0.index]","6cc8cedc":"from sklearn.metrics import accuracy_score\n\nacc = accuracy_score(y_test, test_preds)\nacc_1 = accuracy_score(y_test_1, test_preds_1)\nacc_0 = accuracy_score(y_test_0, test_preds_0)\n\nprint(\"Total Accuracy: %.1f%%\" %(acc*100))\nprint(\"Fraud Accuracy: %.1f%%\" %(acc_1*100))\nprint(\"Non-Fraud Accuracy: %.1f%%\" %(acc_0*100))","2167f060":"cm = confusion_matrix(y_test, test_preds, normalize='true', labels=[0,1])\nplt.figure(figsize=(6,6))\ng = sns.heatmap(cm, annot=True, fmt=\".2%\", cmap=\"Blues\")\ng.set_title(\"Test Confusion Matrix\")\ng.set_xlabel(\"Predicted Class\")\ng.set_ylabel(\"True Class\")\nplt.show()","11734360":"output = pd.DataFrame({\"Id\": y_test.index, \"Class\": test_preds})\noutput.to_csv(\"output.csv\", index=False)","02442efc":"# Exploratory Data Analysis (EDA)","522419f0":"# Model Explainability","5fce5193":"# Training ML Models","815e5dbc":"Now that we have split our dataset, let's forget about Test Set and focus on our Training Set. Let's explore its data!","ef545d7b":"# Predicting Test Labels","92ad6bb5":"Now we will use RandomizedSearchCV to find the best params for our model. Given that our dataset is imbalanced, we will use a technique called Oversampling, using the SMOTE algorithm to create a dataset with synthetic positive instances.","ea36a5eb":"### Dependencies","2560d4f8":"As we can see, we don't have any columns with null values, and also every one of them is numerical. As I said earlier, the columns were scaled and transformed by a PCA. But I was lying... not all of them! The columns `Time` and `Amount` aren't scaled. Let's explore them to scale in the best possible way.","6ed34d05":"# Reading Data","21b222f5":"As we can see, `V2` and `V5` are very negatively correlated with `scaled_amount`, and `V3` with `scaled_time`.","845bd9b5":"### Acknowledges\nSpecial regards to the authors of these notebooks, which helped me a lot to write this script!\n\n- https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets ","f1538bbc":"Wow, how many outliers! We can remove them, but we could remove rows with Class==1 (which we saw earlier that they are pretty rare and important). Another way is to use a scaling technique not very sensible to outliers, A.K.A RobustScaler! Check this amazing article by Jeff Hale about different scaling, normalizing, and standardizing techniques: https:\/\/towardsdatascience.com\/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02","646cbe27":"# Introduction\n\nHello there! In this project, we will try to predict a very imbalanced dataset - Credit Cards Frauds. As it is said in the description of the dataset, the features are transformed by a PCA (Principal Component Analysis, a Dimensionality Reduction technique) and have their names hidden for privacy reasons. We will try to explore some data and apply different classification models to come to the best solution. Let's start!","69e641d7":"Given that the dataset is extremely imbalanced, we will use StatifiedShuffleSplit to split the train and test sets with similar proportions of our target class.","f522073f":"## Training On Full Data","b314b9df":"If you come this far, thank you! I hope I could help you in some way with my solution. Please let me know how can I improve it in the comments :)"}}