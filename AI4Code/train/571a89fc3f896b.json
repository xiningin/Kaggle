{"cell_type":{"a8b3edc5":"code","57c8d608":"code","f0425ed6":"code","f5a5bdae":"code","63d7b019":"code","efda18b6":"code","2fdb037f":"code","dc43e468":"code","e3fb217f":"code","aba8f80f":"code","630d0e98":"code","cd87acb6":"code","340b4a98":"code","e37307b0":"code","baf394b9":"code","bf92ad14":"code","776d8527":"markdown"},"source":{"a8b3edc5":"# Import labraries\nfrom os import listdir\nfrom pickle import dump\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model","57c8d608":"# extract features from each photo in the directory\ndef extract_features(directory):\n    # load the model\n    model = VGG16()\n    # re-structure the model\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    # summarize\n    print(model.summary())\n    # extract features from each photo\n    features = dict()\n    for name in listdir(directory):\n        # load an image from file\n        filename = directory + '\/' + name\n        image = load_img(filename, target_size=(224, 224))\n        # convert the image pixels to a numpy array\n        image = img_to_array(image)\n        # reshape data for the model\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        # prepare the image for the VGG model\n        image = preprocess_input(image)\n        # get features\n        feature = model.predict(image, verbose=0)\n        # get image id\n        image_id = name.split('.')[0]\n        # store feature\n        features[image_id] = feature\n        print('>%s' % name)\n    return features","f0425ed6":"!ls \/kaggle\/input\/flickr8k\/Images | wc -l","f5a5bdae":"# extract features from all images\ndirectory = '\/kaggle\/input\/flickr8k\/Images'\nfeatures = extract_features(directory)\nprint('Extracted Features: %d' % len(features))\n# save to file\ndump(features, open('features.pkl', 'wb'))","63d7b019":"# Preparing Text Data\n\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text\n\nfilename = '\/kaggle\/input\/flickr8k\/captions.txt'\n# load descriptions\ndoc = load_doc(filename)","efda18b6":"!ls \/kaggle\/input","2fdb037f":"# extract descriptions for images\ndef load_descriptions(doc):\n    mapping = dict()\n    # process lines\n    for line in doc.split('\\n'):\n        # split line by white space\n        tokens = line.split()\n        if len(line) < 2:\n            continue\n        # take the first token as the image id, the rest as the description\n        image_id, image_desc = tokens[0], tokens[1:]\n        # remove filename from image id\n        image_id = image_id.split('.')[0]\n        # convert description tokens back to string\n        image_desc = ' '.join(image_desc)\n        # create the list if needed\n        if image_id not in mapping:\n            mapping[image_id] = list()\n        # store description\n        mapping[image_id].append(image_desc)\n    return mapping\n\n# parse descriptions\ndescriptions = load_descriptions(doc)\nprint('Loaded: %d ' % len(descriptions))","dc43e468":"import string\n\ndef clean_descriptions(descriptions):\n    # prepare translation table for removing punctuation\n    table = str.maketrans('', '', string.punctuation)\n    for key, desc_list in descriptions.items():\n        for i in range(len(desc_list)):\n            desc = desc_list[i]\n            # tokenize\n            desc = desc.split()\n            # convert to lower case\n            desc = [word.lower() for word in desc]\n            # remove punctuation from each token\n            desc = [w.translate(table) for w in desc]\n            # remove hanging 's' and 'a'\n            desc = [word for word in desc if len(word)>1]\n            # remove tokens with numbers in them\n            desc = [word for word in desc if word.isalpha()]\n            # store as string\n            desc_list[i] =  ' '.join(desc)\n\n# clean descriptions\nclean_descriptions(descriptions)","e3fb217f":"# convert the loaded descriptions into a vocabulary of words\ndef to_vocabulary(descriptions):\n    # build a list of all description strings\n    all_desc = set()\n    for key in descriptions.keys():\n        [all_desc.update(d.split()) for d in descriptions[key]]\n    return all_desc\n\n# summarize vocabulary\nvocabulary = to_vocabulary(descriptions)\nprint('Vocabulary Size: %d' % len(vocabulary))","aba8f80f":"# save descriptions to file, one per line\ndef save_descriptions(descriptions, filename):\n    lines = list()\n    for key, desc_list in descriptions.items():\n        for desc in desc_list:\n            lines.append(key + ' ' + desc)\n    data = '\\n'.join(lines)\n    file = open(filename, 'w')\n    file.write(data)\n    file.close()\n\n# save descriptions\nsave_descriptions(descriptions, 'descriptions.txt')","630d0e98":"!ls  \/kaggle\/input\/flickr8k\/Images\/ | head -7000  > \/kaggle\/working\/train.txt\n!ls  \/kaggle\/input\/flickr8k\/Images\/ | tail -1091  > \/kaggle\/working\/test.txt\n","cd87acb6":"from numpy import array\nfrom pickle import load\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers.merge import add\nfrom keras.callbacks import ModelCheckpoint\n\n# load doc into memory\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text\n\n# load a pre-defined list of photo identifiers\ndef load_set(filename):\n    doc = load_doc(filename)\n    dataset = list()\n    # process line by line\n    for line in doc.split('\\n'):\n        # skip empty lines\n        if len(line) < 1:\n            continue\n        # get the image identifier\n        identifier = line.split('.')[0]\n        dataset.append(identifier)\n    return set(dataset)\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n    # load document\n    doc = load_doc(filename)\n    descriptions = dict()\n    for line in doc.split('\\n'):\n        # split line by white space\n        tokens = line.split()\n        # split id from description\n        image_id, image_desc = tokens[0], tokens[1:]\n        # skip images not in the set\n        if image_id in dataset:\n            # create list\n            if image_id not in descriptions:\n                descriptions[image_id] = list()\n            # wrap description in tokens\n            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n            # store\n            descriptions[image_id].append(desc)\n    return descriptions\n\n# load photo features\ndef load_photo_features(filename, dataset):\n    # load all features\n    all_features = load(open(filename, 'rb'))\n    # filter features\n    features = {k: all_features[k] for k in dataset}\n    return features\n\n# covert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n    all_desc = list()\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n\n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n    lines = to_lines(descriptions)\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n# calculate the length of the description with the most words\ndef max_length(descriptions):\n    lines = to_lines(descriptions)\n    return max(len(d.split()) for d in lines)\n\n# create sequences of images, input sequences and output words for an image\ndef create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n    X1, X2, y = list(), list(), list()\n    # walk through each description for the image\n    for desc in desc_list:\n        # encode the sequence\n        seq = tokenizer.texts_to_sequences([desc])[0]\n        # split one sequence into multiple X,y pairs\n        for i in range(1, len(seq)):\n            # split into input and output pair\n            in_seq, out_seq = seq[:i], seq[i]\n            # pad input sequence\n            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n            # encode output sequence\n            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n            # store\n            X1.append(photo)\n            X2.append(in_seq)\n            y.append(out_seq)\n    return array(X1), array(X2), array(y)\n\n# define the captioning model\ndef define_model(vocab_size, max_length):\n    # feature extractor model\n    inputs1 = Input(shape=(4096,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(256, activation='relu')(fe1)\n    # sequence model\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n    se3 = LSTM(256)(se2)\n    # decoder model\n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n    # tie it together [image, seq] [word]\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    # compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    # summarize model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model\n\n# data generator, intended to be used in a call to model.fit_generator()\ndef data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n    # loop for ever over images\n    while 1:\n        for key, desc_list in descriptions.items():\n            # retrieve the photo feature\n            photo = photos[key][0]\n            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n            yield [in_img, in_seq], out_word\n\n# load training dataset (6K)\nfilename = '\/kaggle\/working\/train.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# photo features\ntrain_features = load_photo_features('features.pkl', train)\nprint('Photos: train=%d' % len(train_features))\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n# determine the maximum sequence length\nmax_length = max_length(train_descriptions)\nprint('Description Length: %d' % max_length)\n\n# define the model\nmodel = define_model(vocab_size, max_length)\n# train the model, run epochs manually and save after each epoch\nepochs = 20\nsteps = len(train_descriptions)\nfor i in range(epochs):\n    # create the data generator\n    generator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n    # fit for one epoch\n    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n    # save model\n    model.save('model_' + str(i) + '.h5')","340b4a98":"from numpy import argmax\nfrom pickle import load\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# load doc into memory\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text\n\n# load a pre-defined list of photo identifiers\ndef load_set(filename):\n    doc = load_doc(filename)\n    dataset = list()\n    # process line by line\n    for line in doc.split('\\n'):\n        # skip empty lines\n        if len(line) < 1:\n            continue\n        # get the image identifier\n        identifier = line.split('.')[0]\n        dataset.append(identifier)\n    return set(dataset)\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n    # load document\n    doc = load_doc(filename)\n    descriptions = dict()\n    for line in doc.split('\\n'):\n        # split line by white space\n        tokens = line.split()\n        # split id from description\n        image_id, image_desc = tokens[0], tokens[1:]\n        # skip images not in the set\n        if image_id in dataset:\n            # create list\n            if image_id not in descriptions:\n                descriptions[image_id] = list()\n            # wrap description in tokens\n            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n            # store\n            descriptions[image_id].append(desc)\n    return descriptions\n\n# load photo features\ndef load_photo_features(filename, dataset):\n    # load all features\n    all_features = load(open(filename, 'rb'))\n    # filter features\n    features = {k: all_features[k] for k in dataset}\n    return features\n\n# covert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n    all_desc = list()\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n\n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n    lines = to_lines(descriptions)\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n# calculate the length of the description with the most words\ndef max_length(descriptions):\n    lines = to_lines(descriptions)\n    return max(len(d.split()) for d in lines)\n\n# map an integer to a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n    # seed the generation process\n    in_text = 'startseq'\n    # iterate over the whole length of the sequence\n    for i in range(max_length):\n        # integer encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # pad input\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        # predict next word\n        yhat = model.predict([photo,sequence], verbose=0)\n        # convert probability to integer\n        yhat = argmax(yhat)\n        # map integer to word\n        word = word_for_id(yhat, tokenizer)\n        # stop if we cannot map the word\n        if word is None:\n            break\n        # append as input for generating the next word\n        in_text += ' ' + word\n        # stop if we predict the end of the sequence\n        if word == 'endseq':\n            break\n    return in_text\n\n# evaluate the skill of the model\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\n    actual, predicted = list(), list()\n    # step over the whole set\n    for key, desc_list in descriptions.items():\n        # generate description\n        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n        # store actual and predicted\n        references = [d.split() for d in desc_list]\n        actual.append(references)\n        predicted.append(yhat.split())\n    # calculate BLEU score\n    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n\n# prepare tokenizer on train set\n\n# load training dataset (6K)\nfilename = '\/kaggle\/working\/train.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n# determine the maximum sequence length\nmax_length = max_length(train_descriptions)\nprint('Description Length: %d' % max_length)\n\n# prepare test set\n\n# load test set\nfilename = '\/kaggle\/working\/test.txt'\ntest = load_set(filename)\nprint('Dataset: %d' % len(test))\n# descriptions\ntest_descriptions = load_clean_descriptions('descriptions.txt', test)\nprint('Descriptions: test=%d' % len(test_descriptions))\n# photo features\ntest_features = load_photo_features('features.pkl', test)\nprint('Photos: test=%d' % len(test_features))\n\n# load the model\nfilename = 'model_17.h5'\nmodel = load_model(filename)\n# evaluate model\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","e37307b0":"from keras.preprocessing.text import Tokenizer\nfrom pickle import dump\n\n# load doc into memory\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text\n\n# load a pre-defined list of photo identifiers\ndef load_set(filename):\n    doc = load_doc(filename)\n    dataset = list()\n    # process line by line\n    for line in doc.split('\\n'):\n        # skip empty lines\n        if len(line) < 1:\n            continue\n        # get the image identifier\n        identifier = line.split('.')[0]\n        dataset.append(identifier)\n    return set(dataset)\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n    # load document\n    doc = load_doc(filename)\n    descriptions = dict()\n    for line in doc.split('\\n'):\n        # split line by white space\n        tokens = line.split()\n        # split id from description\n        image_id, image_desc = tokens[0], tokens[1:]\n        # skip images not in the set\n        if image_id in dataset:\n            # create list\n            if image_id not in descriptions:\n                descriptions[image_id] = list()\n            # wrap description in tokens\n            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n            # store\n            descriptions[image_id].append(desc)\n    return descriptions\n\n# covert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n    all_desc = list()\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n\n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n    lines = to_lines(descriptions)\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n# load training dataset (6K)\nfilename = '\/kaggle\/working\/train.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\n# save the tokenizer\ndump(tokenizer, open('tokenizer.pkl', 'wb'))","baf394b9":"from pickle import load\nfrom numpy import argmax\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nfrom keras.models import load_model\n\n# extract features from each photo in the directory\ndef extract_features(filename):\n    # load the model\n    model = VGG16()\n    # re-structure the model\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    # load the photo\n    image = load_img(filename, target_size=(224, 224))\n    # convert the image pixels to a numpy array\n    image = img_to_array(image)\n    # reshape data for the model\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    # prepare the image for the VGG model\n    image = preprocess_input(image)\n    # get features\n    feature = model.predict(image, verbose=0)\n    return feature\n\n# map an integer to a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n    # seed the generation process\n    in_text = 'startseq'\n    # iterate over the whole length of the sequence\n    for i in range(max_length):\n        # integer encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # pad input\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        # predict next word\n        yhat = model.predict([photo,sequence], verbose=0)\n        # convert probability to integer\n        yhat = argmax(yhat)\n        # map integer to word\n        word = word_for_id(yhat, tokenizer)\n        # stop if we cannot map the word\n        if word is None:\n            break\n        # append as input for generating the next word\n        in_text += ' ' + word\n        # stop if we predict the end of the sequence\n        if word == 'endseq':\n            break\n    return in_text\n\n# load the tokenizer\ntokenizer = load(open('tokenizer.pkl', 'rb'))\n# pre-define the max sequence length (from training)\nmax_length = 33\n# load the model\nmodel = load_model('model_17.h5')\n\n","bf92ad14":"# load and prepare the photograph\nphoto = extract_features('..\/input\/flickr8k\/Images\/1003163366_44323f5815.jpg')\n# generate description\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(description)","776d8527":"# Prepare Photo Data for processing"}}