{"cell_type":{"d9c7ca82":"code","9f125c04":"code","c75cc665":"code","7abaf01f":"code","005daadc":"code","5246c9cd":"code","74bf0470":"code","5ab8439a":"code","abc96174":"code","840e7626":"code","66182de3":"code","1a71cde1":"code","559bbe7f":"code","86b72c8f":"code","748383b5":"code","980bf47a":"code","45c372b2":"code","8eeeda1b":"code","d9f8f98f":"code","54127a6e":"code","0da2728b":"code","4524302d":"code","6058d86a":"code","6a890b4d":"markdown","32ecca7d":"markdown","87a3762e":"markdown","66b186bf":"markdown","1282d79b":"markdown","bbda68aa":"markdown","73c8594a":"markdown","3a4758e6":"markdown","c537ed23":"markdown","4197dad1":"markdown","888d9851":"markdown","63146850":"markdown","0241b2c8":"markdown","2df69311":"markdown","adf2e833":"markdown","67dc813c":"markdown","04ee46c1":"markdown","9cb57611":"markdown","d9ea3d3b":"markdown","3a241d9c":"markdown","23410113":"markdown","cdb76ad1":"markdown","b1ee3f2c":"markdown","d5b18ffe":"markdown","5e087f8c":"markdown","3dd58731":"markdown","add6db71":"markdown","7fc4f088":"markdown","78df6b05":"markdown","fca1dc25":"markdown","d7c2fea1":"markdown","976ee7dc":"markdown"},"source":{"d9c7ca82":"# Core\nimport numpy as np\nimport pandas as pd\npd.set_option('display.float_format', lambda x: '%.1f' % x)\npd.get_option(\"display.max_columns\", 55)\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import combinations\nimport statistics\nimport time\n\n# Sklearn\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\n# Tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","9f125c04":"# Save to df\ntrain_data=pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv', index_col='Id')\ntest_data=pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv', index_col='Id')\n\n# save for submission\ntest_index=test_data.index\n\n# Shape and preview\nprint('Training data df shape:',train_data.shape)\nprint('Test data df shape:',test_data.shape)\ntrain_data.head()","c75cc665":"# Save to df\npseudo_label_df=pd.read_csv('..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv', index_col='Id')\n\n# Concatenate\nnew_train_data=pd.concat([train_data, pseudo_label_df], axis=0)\n\n# Remove pseudolabel samples from test set\npseudo_label_index=pseudo_label_df.index\nnew_test_data=test_data.drop(pseudo_label_index, axis=0)\n\n# Save for submission\nnew_test_data_index=new_test_data.index\npseudo_label_preds_df=pd.DataFrame({'Id': pseudo_label_index,\n                       'Cover_Type': pseudo_label_df['Cover_Type']}).reset_index(drop=True)","7abaf01f":"new_train_data.drop(new_train_data[new_train_data.Cover_Type==5].index, axis=0, inplace=True)","005daadc":"# Specify features to clip\nmask_features=['Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n              'Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']\n\n# Clip negative values\nnew_train_data[mask_features]=new_train_data[mask_features].clip(lower=0)\nnew_test_data[mask_features]=new_test_data[mask_features].clip(lower=0)","5246c9cd":"# Project training aspect angles onto [0,360]\nnew_train_data['Aspect'][new_train_data['Aspect'] < 0] += 360\nnew_train_data['Aspect'][new_train_data['Aspect'] >= 360] -= 360\n\n# Project test aspect angles onto [0,360]\nnew_test_data['Aspect'][new_test_data['Aspect'] < 0] += 360\nnew_test_data['Aspect'][new_test_data['Aspect'] >= 360] -= 360","74bf0470":"# l1 (aka Manhattan) distance to Hydrology\nnew_train_data['l1_Hydrology'] = np.abs(new_train_data['Horizontal_Distance_To_Hydrology']) + np.abs(new_train_data['Vertical_Distance_To_Hydrology'])\nnew_test_data['l1_Hydrology'] = np.abs(new_test_data['Horizontal_Distance_To_Hydrology']) + np.abs(new_test_data['Vertical_Distance_To_Hydrology'])","5ab8439a":"# Euclidean distance to Hydrology (training set)\nnew_train_data[\"ED_to_Hydrology\"] = np.sqrt((new_train_data['Horizontal_Distance_To_Hydrology'].astype(np.int32))**2 + \n                                        (new_train_data['Vertical_Distance_To_Hydrology'].astype(np.int32))**2)\n\n# Euclidean distance to Hydrology (test set)\nnew_test_data[\"ED_to_Hydrology\"] = np.sqrt((new_test_data['Horizontal_Distance_To_Hydrology'].astype(np.int32))**2 + \n                                       (new_test_data['Vertical_Distance_To_Hydrology'].astype(np.int32))**2)","abc96174":"# Clips hillshades 0 to 255 index\nhillshades = [col for col in train_data.columns if col.startswith('Hillshade')]\n\n# Clip df's\nnew_train_data[hillshades] = new_train_data[hillshades].clip(0, 255)\nnew_test_data[hillshades] = new_test_data[hillshades].clip(0, 255)","840e7626":"# Soil type count\nsoil_features = [x for x in new_train_data.columns if x.startswith(\"Soil_Type\")]\nnew_train_data[\"Soil_Type_Count\"] = new_train_data[soil_features].sum(axis=1)\nnew_test_data[\"Soil_Type_Count\"] = new_test_data[soil_features].sum(axis=1)\n\n# Wilderness area count\nwilderness_features = [x for x in new_train_data.columns if x.startswith(\"Wilderness_Area\")]\nnew_train_data[\"Wilderness_Area_Count\"] = new_train_data[wilderness_features].sum(axis=1)\nnew_test_data[\"Wilderness_Area_Count\"] = new_test_data[wilderness_features].sum(axis=1)","66182de3":"# Train df\nnew_train_data.drop('Soil_Type7', axis=1, inplace=True)\nnew_train_data.drop('Soil_Type15', axis=1, inplace=True)\n\n# Test df\nnew_test_data.drop('Soil_Type7', axis=1, inplace=True)\nnew_test_data.drop('Soil_Type15', axis=1, inplace=True)","1a71cde1":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","559bbe7f":"new_train_data=reduce_mem_usage(new_train_data)\nnew_test_data=reduce_mem_usage(new_test_data)","86b72c8f":"# Labels\ny=new_train_data.Cover_Type\n\n# Features\nX=new_train_data.drop('Cover_Type', axis=1)","748383b5":"scaler = StandardScaler()\nX=scaler.fit_transform(X)\ntest_data_preprocessed = scaler.transform(new_test_data)","980bf47a":"# Encode labels to lie in range 0 to 5\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)","45c372b2":"del train_data, test_data, scaler\ndel pseudo_label_df, new_train_data, new_test_data\ndel mask_features, hillshades\ndel soil_features,wilderness_features","8eeeda1b":"# Define model\ndef build_model():\n    model = keras.Sequential([\n\n        # hidden layer 1\n        layers.Dense(units=256, activation='relu', input_shape=[X.shape[1]], kernel_initializer='lecun_normal'),\n        layers.Dropout(rate=0.3),\n\n        # hidden layer 2\n        layers.Dense(units=256, activation='relu', kernel_initializer='lecun_normal'),\n        layers.Dropout(rate=0.3),\n\n        # hidden layer 3\n        layers.Dense(units=128, activation='relu', kernel_initializer='lecun_normal'),\n        layers.Dropout(rate=0.2),\n        \n        # hidden layer 4\n        layers.Dense(units=64, activation='relu', kernel_initializer='lecun_normal'),\n        layers.Dropout(rate=0.2),\n\n        # output layer\n        layers.Dense(units=6, activation='softmax')\n    ])\n    \n    # Define loss, optimizer and metric\n    model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n    \n    return model","d9f8f98f":"# Define early stopping callback on validation loss\nearly_stopping = callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=20,\n    restore_best_weights=True,\n)\n\n# Reduce learning rate when validation loss plateaus\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=5\n)","54127a6e":"FOLDS = 8\nEPOCHS = 100\nBATCH_SIZE = 250\n\ntest_preds = np.zeros((1, 1))\nscores = []\n\ncv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=0)\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n    # Start timer\n    start = time.time()\n    \n    # get training and validation sets\n    X_train, X_valid = X[train_idx], X[val_idx]\n    y_train, y_valid = y[train_idx], y[val_idx]\n\n    # Build and train model on tpu\n    model = build_model()\n    model.fit(\n        X_train,\n        y_train,\n        validation_data=(X_valid, y_valid),\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        callbacks=[early_stopping, reduce_lr],\n        verbose=False\n    )\n\n    # Make predictions and get measure accuracy\n    y_pred = np.argmax(model.predict(X_valid), axis=1)\n    score = accuracy_score(y_valid, y_pred)\n    scores.append(score)\n    \n    # Store predictions\n    test_preds = test_preds + model.predict(test_data_preprocessed)\n    \n    # Stop timer\n    stop = time.time()\n    \n    # Print accuracy and time\n    print(f\"Fold {fold} - Accuracy: {score}, Time: {round((stop - start)\/60,1)} mins\")\n    \nprint('')\nprint(f\"Mean Accuracy: {np.mean(scores)}\")","0da2728b":"# Soft voting to ensemble predictions\ntest_preds = np.argmax(test_preds, axis=1)\n\n# Recover class labels\npred_classes = encoder.inverse_transform(test_preds)","4524302d":"# Save new predictions to df\nnew_test_preds_df=pd.DataFrame({'Id': new_test_data_index, \n                                'Cover_Type': pred_classes})\n\n# Concatenate with pseudolabels\nfinal_preds=pd.concat([new_test_preds_df, pseudo_label_preds_df])\n\n# Sort by id\nfinal_preds=final_preds.sort_values(by='Id', ascending=True)\n\n# Check format\nfinal_preds.head(10)","6058d86a":"# Save to csv\nfinal_preds.to_csv('submission.csv', index=False)","6a890b4d":"**Distance to Hydrology**","32ecca7d":"# Feature engineering","87a3762e":"**Drop label 5**","66b186bf":"# Introduction","1282d79b":"# Pseudolabeling","bbda68aa":"# Cross validation","73c8594a":"**Label encoding**","3a4758e6":"Credit: [Craig Thomas](https:\/\/www.kaggle.com\/craigmthomas).","c537ed23":"**Soft voting**","4197dad1":"Credit: [Gulshan](https:\/\/www.kaggle.com\/gulshanmishra\/tps-dec-21-tensorflow-nn-feature-engineering).","888d9851":"**Acknowledgments:**\n* [Confusion matrices](https:\/\/www.kaggle.com\/ambrosm\/tpsdec21-01-keras-quickstart) by [AmbrosM](https:\/\/www.kaggle.com\/ambrosm).\n* [Feature engineering](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373) by [Gulshan Mishra](https:\/\/www.kaggle.com\/gulshanmishra).\n* [Memory usage](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/291844) by [Luca Massaron](https:\/\/www.kaggle.com\/lucamassaron).\n* [Ensembling](https:\/\/www.kaggle.com\/odins0n\/tps-dec-eda-modeling\/notebook#Modeling) by [Sanskar Hasija\n](https:\/\/www.kaggle.com\/odins0n).\n* [Pseudolabelling](https:\/\/www.kaggle.com\/remekkinas\/tps-12-nn-tpu-pseudolabeling-0-95661\/notebook) by [Remek Kinas](https:\/\/www.kaggle.com\/remekkinas).","63146850":"**Aspect**","0241b2c8":"**Callbacks**","2df69311":"**Number of soil & wilderness types**","adf2e833":"For the features below it does not make physical sense to include negative numbers.","67dc813c":"This notebook will be essentially the same to my other notebook, except we won't do EDA here to save memory. This will allow use to use more folds in the cross validation stage. \n\nSee below for my main notebook:\n* [EDA, Feature Engineering and Pseudolabelling](https:\/\/www.kaggle.com\/samuelcortinhas\/tps-dec-eda-feat-eng-pseudolab)","04ee46c1":"**Save memory**","9cb57611":"Aspect values represent angles between 0 and 360 degrees so we should project them onto [0,360] to make any patterns easier to learn.","d9ea3d3b":"From [ArcMap](https:\/\/desktop.arcgis.com\/en\/arcmap\/10.3\/manage-data\/raster-and-images\/hillshade-function.htm): \"A hillshade is a grayscale 3D representation of the surface, with the sun's relative position taken into account for shading the image.\" \n\nThis means all Hillshade values should lie in the range [0, 255] because it corresponds to a greyscale image.","3a241d9c":"**Hillshade**","23410113":"# Memory","cdb76ad1":"# Neural network","b1ee3f2c":"# Submission","d5b18ffe":"**Remove unwanted negative values**","5e087f8c":"The objective to predict the cover type of a forest given features like elavation, soil type etc. There are 7 different cover types to predict in total.\n\n![https:\/\/th.bing.com\/th\/id\/OIP.PcAN1kc44gDpHowTie715gHaD4?pid=ImgDet&rs=1](https:\/\/th.bing.com\/th\/id\/OIP.PcAN1kc44gDpHowTie715gHaD4?pid=ImgDet&rs=1)","3dd58731":"# Data","add6db71":"**Scale data**","7fc4f088":"**Drop features with 0 variance**","78df6b05":"**Labels and features:**","fca1dc25":"# Pre-process data","d7c2fea1":"# Libraries","976ee7dc":"We have the horizontal and vertical distances to Hydrology so we can use these to calculate the l1 or euclidean distance."}}