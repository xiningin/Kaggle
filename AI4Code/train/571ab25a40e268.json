{"cell_type":{"7ed7efae":"code","c3d25277":"code","a7fe9b93":"code","9436409b":"code","30a2ae50":"code","ed8dd9c8":"code","6920fe0b":"code","0697e273":"code","8ecc2fc8":"code","9ed686e8":"code","a920b2b8":"code","88f6385d":"code","a2bbb976":"code","80f58a31":"code","daa0eb85":"code","78f59e2e":"code","9540595f":"code","0a13c376":"code","b9543d78":"code","af0389e3":"code","c6d303cc":"code","cf12208e":"code","ece531f9":"code","9228898b":"code","2de1e760":"code","aba5d433":"code","4ab24807":"code","cdfea5f0":"code","afd039e0":"code","7edd482b":"code","7183f5e7":"code","f680ffe0":"code","0db5b61b":"code","829bdc9f":"code","22455771":"code","c17af59d":"code","3c310d17":"code","0ad98b0d":"code","ba7b018b":"code","2b70b3b8":"code","8e21b4d0":"code","03a305a9":"code","28d2d3fa":"code","dc43d3a9":"code","9ac933f6":"code","36844699":"code","2f525046":"code","306aa0b4":"code","1adf1487":"code","f4b1ad5c":"code","2aa7b6f0":"code","fae6a5ee":"code","d15f6dca":"code","a250bf6f":"code","4348dcc5":"code","d1ade6f4":"code","9b283f5d":"code","3b14920f":"code","b0e1fbb3":"code","a6f34667":"code","b97eb0c6":"code","553319c5":"code","41326d06":"code","89c6a01c":"code","f6773d24":"code","552c57d1":"code","71800c6d":"code","0aa76d02":"code","95ac9fad":"code","fb5a05d3":"code","9cddaa32":"code","29db055e":"code","40977435":"code","b41c5ac8":"code","610ce089":"code","d29aad57":"code","139afc35":"code","dc49d80d":"code","27b1fa7a":"code","e57101bf":"code","0ebd5aac":"code","6dd7adf6":"code","490e9a18":"code","9ff64991":"code","78f2edc8":"code","d288e225":"code","26ebdb5b":"code","b82e2904":"markdown","28ce9825":"markdown","6351571b":"markdown","ea9b60a9":"markdown","191eee36":"markdown","b183e907":"markdown","c7f4bf3d":"markdown","9e1a34d6":"markdown","afc9fecb":"markdown","c4cae8f6":"markdown","69c934d9":"markdown","ed0d3d96":"markdown","4c1a2606":"markdown","8edc4f1f":"markdown","087374da":"markdown","fa70134a":"markdown","33d60488":"markdown","cb785cca":"markdown","17a8f50d":"markdown","88f0866f":"markdown","8834784d":"markdown","e7382ba5":"markdown","f609d515":"markdown","6955cbc6":"markdown","111a5d37":"markdown","ef02606e":"markdown","099ee7aa":"markdown","eb02e401":"markdown","139ad196":"markdown","7fd5f8cc":"markdown","1e700f68":"markdown","62996603":"markdown","c01d5548":"markdown","71740248":"markdown","2bf51f35":"markdown","c213a6c6":"markdown","175ca663":"markdown","98dc13ea":"markdown"},"source":{"7ed7efae":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('ggplot')\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, KFold\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nstop = set(stopwords.words('english'))\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nimport json\nimport ast\nimport eli5\nimport shap\nfrom catboost import CatBoostRegressor\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model","c3d25277":"import os\nIS_LOCAL = False\nif (IS_LOCAL):\n    location = \"..\/input\/tmdb-box-office\/\"\nelse:\n    location = \"..\/input\/\"\nos.listdir(location)","a7fe9b93":"%%time\nxtrain = pd.read_csv(os.path.join(location, 'train.csv'))\nxtest = pd.read_csv(os.path.join(location, 'test.csv'))\nsam_sub = pd.read_csv(os.path.join(location, 'sample_submission.csv'))","9436409b":"print(\"Xtrain: {}\\nXtest: {}:\".format(xtrain.shape, xtest.shape))","30a2ae50":"def show_head(data):\n    return(data.head())","ed8dd9c8":"show_head(xtrain)\nxtrain.columns\n","6920fe0b":"show_head(xtest)\nxtest.columns","0697e273":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (total\/data.isnull().count()*100)\n    miss_column = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    miss_column['Types'] = types\n    return(np.transpose(miss_column))","8ecc2fc8":"missing_data(xtrain)","9ed686e8":"missing_data(xtest)","a920b2b8":"dict_features = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_dict(df):\n    for col in dict_features:\n        df[col] = df[col].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \nxtrain = text_dict(xtrain)\nxtest = text_dict(xtest)","88f6385d":"for i, e in enumerate(xtrain['belongs_to_collection'][:5]):\n    print(i, e)","a2bbb976":"xtrain['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0).value_counts()","80f58a31":"xtrain['CollectionName'] = xtrain['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\nxtrain['BelongCollection'] = xtrain['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n\nxtest['CollectionName'] = xtest['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\nxtest['BelongCollection'] = xtest['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n\nxtrain = xtrain.drop(['belongs_to_collection'], axis=1)\nxtest = xtest.drop(['belongs_to_collection'], axis=1)","daa0eb85":"show_head(xtrain)","78f59e2e":"for i, e in enumerate(xtrain['genres'][:5]):\n    print(i, e)","9540595f":"print('Number of Genres per movie')\nxtrain['genres'].apply(lambda x: len(x) if x != {} else 0).value_counts()","0a13c376":"GenresList = list(xtrain['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)","b9543d78":"plt.figure(figsize = (16, 12))\ntext = ' '.join([i for j in GenresList for i in j])\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('NumberGenres')\nplt.axis(\"off\")\nplt.show()","af0389e3":"xtrain['GenresNumb'] = xtrain['genres'].apply(lambda x: len(x) if x != {} else 0)\nxtrain['AllGenres'] = xtrain['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_genres = [m[0] for m in Counter([i for j in GenresList for i in j]).most_common(10)]\nfor g in top_genres:\n    xtrain['genre_' + g] = xtrain['AllGenres'].apply(lambda x: 1 if g in x else 0)\n    \nxtest['GenresNumb'] = xtest['genres'].apply(lambda x: len(x) if x != {} else 0)\nxtest['AllGenres'] = xtest['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_genres:\n    xtest['genre_' + g] = xtest['AllGenres'].apply(lambda x: 1 if g in x else 0)\n\nxtrain = xtrain.drop(['genres'], axis=1)\nxtest = xtest.drop(['genres'], axis=1)","c6d303cc":"xtrain.columns\nshow_head(xtrain)","cf12208e":"for i, e in enumerate(xtrain['production_companies'][:5]):\n    print(i, e)\nprint('number of producing company')\nxtrain['production_companies'].apply(lambda x: len(x) if x!= {} else 0).value_counts()","ece531f9":"xtrain[xtrain['production_companies'].apply(lambda x: len(x) if x!= {} else 0)>7]\n\nProdCompList = list(xtrain['production_companies'].apply(lambda x: [i['name'] for i in x] if x!= {}\n                                                         else []).values)\n\nxtrain['ProdCompNumb'] = xtrain['production_companies'].apply(lambda x: len(x) if x!= {} else 0)\nxtrain['AllProdComp'] = xtrain['production_companies'].apply(lambda x: ''.join(sorted([i['name'] for i in x])) if x!= {} else '')\ntop_ProdComp = [m[0] for m in Counter([i for j in ProdCompList for i in j]).most_common(10)]\nfor g in top_ProdComp:\n    xtrain['production_company_' + g] = xtrain['production_companies'].apply(lambda x: 1 if g in x else 0)\n    \n\n\nxtest['ProdCompNumb'] = xtest['production_companies'].apply(lambda x: len(x) if x!= {} else 0)\nxtest['AllProdComp'] = xtest['production_companies'].apply(lambda x: ''.join(sorted([i['name'] for i in x])) if x!= {} else '')\ntop_ProdComp = [m[0] for m in Counter([i for j in ProdCompList for i in j]).most_common(10)]\nfor g in top_ProdComp:\n    xtest['production_company_' + g] = xtest['production_companies'].apply(lambda x: 1 if g in x else 0)\n    \nxtrain = xtrain.drop(['production_companies'], axis=1)\nxtest = xtest.drop(['production_companies'], axis=1)","9228898b":"xtrain.columns","2de1e760":"for i, e in enumerate(xtrain['production_countries'][:5]):\n    print(i, e)\nprint('number of producing countries')\nxtrain['production_countries'].apply(lambda x: len(x) if x!= {} else 0).value_counts()","aba5d433":"ProdCountryList = list(xtrain['production_countries'].apply(lambda x: [i['name'] for i in x] if x!= {}\n                                                         else []).values)\nplt.figure(figsize = (16, 12))\ntext = ' '.join([i for j in ProdCountryList for i in j])\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('NumberProdCountry')\nplt.axis(\"off\")\nplt.show()","4ab24807":"xtrain['ProdCountryNumb'] = xtrain['production_countries'].apply(lambda x: len(x) if x!= {} else 0)\nxtrain['AllProdCountry'] = xtrain['production_countries'].apply(lambda x: ''.join(sorted([i['name'] for i in x])) if x!= {} else '')\ntop_ProdCountry = [m[0] for m in Counter([i for j in ProdCountryList for i in j]).most_common(5)]\nfor g in top_ProdCountry:\n    xtrain['production_country_' + g] = xtrain['production_countries'].apply(lambda x: 1 if g in x else 0)\n    \n\n\nxtest['ProdCountryNumb'] = xtest['production_countries'].apply(lambda x: len(x) if x!= {} else 0)\nxtest['AllProdCountry'] = xtest['production_countries'].apply(lambda x: ''.join(sorted([i['name'] for i in x])) if x!= {} else '')\ntop_ProdCountry = [m[0] for m in Counter([i for j in ProdCountryList for i in j]).most_common(5)]\nfor g in top_ProdCountry:\n    xtest['production_country_' + g] = xtest['production_countries'].apply(lambda x: 1 if g in x else 0)\n    \nxtrain = xtrain.drop(['production_countries'], axis=1)\nxtest = xtest.drop(['production_countries'], axis=1)","cdfea5f0":"xtrain.columns","afd039e0":"for i, e in enumerate(xtrain['spoken_languages'][:5]):\n    print(i, e)\n    \nprint('Number of spoken languages')\nxtrain['spoken_languages'].apply(lambda x: len(x) if x != {} else 0).value_counts()\n\nLanguageList = list(xtrain['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n\nplt.figure(figsize = (16, 12))\ntext = ' '.join([i for j in LanguageList for i in j])\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('NumberOfLanguage')\nplt.axis(\"off\")\nplt.show()","7edd482b":"xtrain['LangNumb'] = xtrain['spoken_languages'].apply(lambda x: len(x) if x!= {} else 0)\nxtrain['AllLang'] = xtrain['spoken_languages'].apply(lambda x: ''.join(sorted([i['name'] for i in x])) if x!= {} else '')\ntop_Lang = [m[0] for m in Counter([i for j in LanguageList for i in j]).most_common(10)]\nfor g in top_Lang:\n    xtrain['spoken_language_' + g] = xtrain['spoken_languages'].apply(lambda x: 1 if g in x else 0)\n    \n\n\nxtest['LangNumb'] = xtest['spoken_languages'].apply(lambda x: len(x) if x!= {} else 0)\nxtest['AllLang'] = xtest['spoken_languages'].apply(lambda x: ''.join(sorted([i['name'] for i in x])) if x!= {} else '')\ntop_Lang = [m[0] for m in Counter([i for j in LanguageList for i in j]).most_common(10)]\nfor g in top_Lang:\n    xtest['spoken_language_' + g] = xtest['spoken_languages'].apply(lambda x: 1 if g in x else 0)\n    \nxtrain = xtrain.drop(['spoken_languages'], axis=1)\nxtest = xtest.drop(['spoken_languages'], axis=1)","7183f5e7":"xtrain.columns","f680ffe0":"for i, e in enumerate(xtrain['Keywords'][:5]):\n    print(i, e)\n    \nprint('Number of Keywords in films')\nxtrain['Keywords'].apply(lambda x: len(x) if x != {} else 0).value_counts().head(10)\n\nKeywordsList = list(xtrain['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nplt.figure(figsize = (16, 12))\ntext = ' '.join(['_'.join(i.split(' ')) for j in KeywordsList for i in j])\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top keywords')\nplt.axis(\"off\")\nplt.show()","0db5b61b":"xtrain['KeywordsNumb'] = xtrain['Keywords'].apply(lambda x: len(x) if x!= {} else 0)\nxtrain['AllKeywords'] = xtrain['Keywords'].apply(lambda x: ''.join(sorted([i['name'] for i in x])) if x!= {} else '')\ntop_Keyword = [m[0] for m in Counter([i for j in KeywordsList for i in j]).most_common(40)]\nfor g in top_Keyword:\n    xtrain['Keyword_' + g] = xtrain['Keywords'].apply(lambda x: 1 if g in x else 0)\n    \n\n\nxtest['KeywordsNumb'] = xtest['Keywords'].apply(lambda x: len(x) if x!= {} else 0)\nxtest['AllKeywords'] = xtest['Keywords'].apply(lambda x: ''.join(sorted([i['name'] for i in x])) if x!= {} else '')\ntop_Keyword = [m[0] for m in Counter([i for j in KeywordsList for i in j]).most_common(40)]\nfor g in top_Keyword:\n    xtest['Keyword_' + g] = xtest['Keywords'].apply(lambda x: 1 if g in x else 0)\n    \nxtrain = xtrain.drop(['Keywords','AllKeywords','AllLang','AllProdCountry','AllProdComp','AllGenres'], axis=1)\nxtest = xtest.drop(['Keywords','AllKeywords','AllLang','AllProdCountry','AllProdComp','AllGenres'], axis=1)","829bdc9f":"xtrain.columns","22455771":"for i, e in enumerate(xtrain['cast'][:1]):\n    print(i, e)\n\nprint('Number of casted persons in films')\nxtrain['cast'].apply(lambda x: len(x) if x != {} else 0).value_counts().head(10)\n\nCastNameList = list(xtrain['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n\nCastGenderList = list(xtrain['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\n\nCastCharacterList = list(xtrain['cast'].apply(lambda x: [i['character'] for i in x] if x != {} else []).values)","c17af59d":"xtrain['num_cast'] = xtrain['cast'].apply(lambda x: len(x) if x != {} else 0)\ntop_cast_names = [m[0] for m in Counter([i for j in CastNameList for i in j]).most_common(15)]\nfor g in top_cast_names:\n    xtrain['cast_name_' + g] = xtrain['cast'].apply(lambda x: 1 if g in str(x) else 0)\nxtrain['genders_0_cast'] = xtrain['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\nxtrain['genders_1_cast'] = xtrain['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\nxtrain['genders_2_cast'] = xtrain['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_cast_characters = [m[0] for m in Counter([i for j in CastCharacterList for i in j]).most_common(15)]\nfor g in top_cast_characters:\n    xtrain['cast_character_' + g] = xtrain['cast'].apply(lambda x: 1 if g in str(x) else 0)\n    \nxtest['num_cast'] = xtest['cast'].apply(lambda x: len(x) if x != {} else 0)\nfor g in top_cast_names:\n    xtest['cast_name_' + g] = xtest['cast'].apply(lambda x: 1 if g in str(x) else 0)\nxtest['genders_0_cast'] = xtest['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\nxtest['genders_1_cast'] = xtest['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\nxtest['genders_2_cast'] = xtest['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\nfor g in top_cast_characters:\n    xtest['cast_character_' + g] = xtest['cast'].apply(lambda x: 1 if g in str(x) else 0)\n\nxtrain = xtrain.drop(['cast'], axis=1)\nxtest = xtest.drop(['cast'], axis=1)","3c310d17":"xtrain.columns","0ad98b0d":"for i, e in enumerate(xtrain['crew'][:1]):\n    print(i, e[:10])\n    \nprint('Number of casted persons in films')\nxtrain['crew'].apply(lambda x: len(x) if x != {} else 0).value_counts().head(10)\n\nlist_of_crew_names = list(xtrain['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n\nlist_of_crew_jobs = list(xtrain['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)\n\nlist_of_crew_genders = list(xtrain['crew'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\n\nlist_of_crew_departments = list(xtrain['crew'].apply(lambda x: [i['department'] for i in x] if x != {} else []).values)\n","ba7b018b":"xtrain['num_crew'] = xtrain['crew'].apply(lambda x: len(x) if x != {} else 0)\ntop_crew_names = [m[0] for m in Counter([i for j in list_of_crew_names for i in j]).most_common(15)]\nfor g in top_crew_names:\n    xtrain['crew_name_' + g] = xtrain['crew'].apply(lambda x: 1 if g in str(x) else 0)\nxtrain['genders_0_crew'] = xtrain['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\nxtrain['genders_1_crew'] = xtrain['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\nxtrain['genders_2_crew'] = xtrain['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_cast_characters = [m[0] for m in Counter([i for j in CastCharacterList for i in j]).most_common(15)]\nfor g in top_cast_characters:\n    xtrain['crew_character_' + g] = xtrain['crew'].apply(lambda x: 1 if g in str(x) else 0)\ntop_crew_jobs = [m[0] for m in Counter([i for j in list_of_crew_jobs for i in j]).most_common(15)]\nfor j in top_crew_jobs:\n    xtrain['jobs_' + j] = xtrain['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\ntop_crew_departments = [m[0] for m in Counter([i for j in list_of_crew_departments for i in j]).most_common(15)]\nfor j in top_crew_departments:\n    xtrain['departments_' + j] = xtrain['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n    \nxtest['num_crew'] = xtest['crew'].apply(lambda x: len(x) if x != {} else 0)\nfor g in top_crew_names:\n    xtest['crew_name_' + g] = xtest['crew'].apply(lambda x: 1 if g in str(x) else 0)\nxtest['genders_0_crew'] = xtest['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\nxtest['genders_1_crew'] = xtest['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\nxtest['genders_2_crew'] = xtest['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\nfor g in top_cast_characters:\n    xtest['crew_character_' + g] = xtest['crew'].apply(lambda x: 1 if g in str(x) else 0)\nfor j in top_crew_jobs:\n    xtest['jobs_' + j] = xtest['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\nfor j in top_crew_departments:\n    xtest['departments_' + j] = xtest['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n\nxtrain = xtrain.drop(['crew'], axis=1)\nxtest = xtest.drop(['crew'], axis=1)","2b70b3b8":"show_head(xtrain)","8e21b4d0":"fig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(xtrain['revenue']);\nplt.title('Distribution of revenue');\nplt.subplot(1, 2, 2)\nplt.hist(np.log1p(xtrain['revenue']));\nplt.title('Distribution of log of revenue');","03a305a9":"xtrain['log_revenue'] = np.log1p(xtrain['revenue'])","28d2d3fa":"fig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(xtrain['budget']);\nplt.title('Distribution of budget');\nplt.subplot(1, 2, 2)\nplt.hist(np.log1p(xtrain['budget']));\nplt.title('Distribution of log of budget');","dc43d3a9":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(xtrain['budget'], xtrain['revenue'])\nplt.title('Revenue vs budget');\nplt.subplot(1, 2, 2)\nplt.scatter(np.log1p(xtrain['budget']), xtrain['log_revenue'])\nplt.title('Log Revenue vs log budget');","9ac933f6":"xtrain['log_budget'] = np.log1p(xtrain['budget'])\nxtest['log_budget'] = np.log1p(xtest['budget'])","36844699":"xtrain['homepage'].value_counts().head()","2f525046":"xtrain['has_homepage'] = 0\nxtrain.loc[xtrain['homepage'].isnull() == False, 'has_homepage'] = 1\nxtest['has_homepage'] = 0\nxtest.loc[xtest['homepage'].isnull() == False, 'has_homepage'] = 1","306aa0b4":"sns.catplot(x='has_homepage', y='revenue', data=xtrain);\nplt.title('Revenue for film with and without homepage');","1adf1487":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='original_language', y='revenue', data=xtrain.loc[xtrain['original_language'].isin(xtrain['original_language'].value_counts().head(10).index)]);\nplt.title('Mean revenue per language');\nplt.subplot(1, 2, 2)\nsns.boxplot(x='original_language', y='log_revenue', data=xtrain.loc[xtrain['original_language'].isin(xtrain['original_language'].value_counts().head(10).index)]);\nplt.title('Mean log revenue per language');","f4b1ad5c":"plt.figure(figsize = (12, 16))\ntext = ' '.join(xtrain['original_title'].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in titles')\nplt.axis(\"off\")\nplt.show()","2aa7b6f0":"plt.figure(figsize = (12, 12))\ntext = ' '.join(xtrain['overview'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in overview')\nplt.axis(\"off\")\nplt.show()","fae6a5ee":"vectorizer = TfidfVectorizer(\n            sublinear_tf=True,\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2),\n            min_df=5)\n\noverview_text = vectorizer.fit_transform(xtrain['overview'].fillna(''))\nlinreg = LinearRegression()\nlinreg.fit(overview_text, xtrain['log_revenue'])\neli5.show_weights(linreg, vec=vectorizer, top=20, feature_filter=lambda x: x != '<BIAS>')","d15f6dca":"print('Target value:', xtrain['log_revenue'][1000])\neli5.show_prediction(linreg, doc=xtrain['overview'].values[1000], vec=vectorizer)","a250bf6f":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(xtrain['popularity'], xtrain['revenue'])\nplt.title('Revenue vs popularity');\nplt.subplot(1, 2, 2)\nplt.scatter(xtrain['popularity'], xtrain['log_revenue'])\nplt.title('Log Revenue vs popularity');","4348dcc5":"xtest.loc[xtest['release_date'].isnull() == True, 'release_date'] = '01\/01\/98'\n\ndef fix_date(x):\n    \"\"\"\n    Fixes dates which are in 20xx\n    \"\"\"\n    year = x.split('\/')[2]\n    if int(year) <= 19:\n        return x[:-2] + '20' + year\n    else:\n        return x[:-2] + '19' + year\n    \nxtrain['release_date'] = xtrain['release_date'].apply(lambda x: fix_date(x))\nxtest['release_date'] = xtest['release_date'].apply(lambda x: fix_date(x))\nxtrain['release_date'] = pd.to_datetime(xtrain['release_date'])\nxtest['release_date'] = pd.to_datetime(xtest['release_date'])\n\ndef process_date(df):\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n    for part in date_parts:\n        part_col = 'release_date' + \"_\" + part\n        df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n    \n    return df\n\nxtrain = process_date(xtrain)\nxtest = process_date(xtest)","d1ade6f4":"d1 = xtrain['release_date_year'].value_counts().sort_index()\nd2 = xtest['release_date_year'].value_counts().sort_index()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='train'), go.Scatter(x=d2.index, y=d2.values, name='test')]\nlayout = go.Layout(dict(title = \"Number of films per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","9b283f5d":"d1 = xtrain['release_date_year'].value_counts().sort_index()\nd2 = xtrain.groupby(['release_date_year'])['revenue'].sum()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='total revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and total revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Total revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","3b14920f":"d1 = xtrain['release_date_year'].value_counts().sort_index()\nd2 = xtrain.groupby(['release_date_year'])['revenue'].mean()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='mean revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and average revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Average revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","b0e1fbb3":"sns.catplot(x='release_date_weekday', y='revenue', data=xtrain);\nplt.title('Revenue on different days of week of release');","a6f34667":"plt.figure(figsize=(20, 6))\nplt.subplot(1, 3, 1)\nplt.hist(xtrain['runtime'].fillna(0) \/ 60, bins=40);\nplt.title('Distribution of length of film in hours');\nplt.subplot(1, 3, 2)\nplt.scatter(xtrain['runtime'].fillna(0), xtrain['revenue'])\nplt.title('runtime vs revenue');\nplt.subplot(1, 3, 3)\nplt.scatter(xtrain['runtime'].fillna(0), xtrain['popularity'])\nplt.title('runtime vs popularity');","b97eb0c6":"xtrain['status'].value_counts()","553319c5":"xtest['status'].value_counts()","41326d06":"plt.figure(figsize = (18, 16))\ntext = ' '.join(xtrain['tagline'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in tagline')\nplt.axis(\"off\")\nplt.show()","89c6a01c":"show_head(xtrain)","f6773d24":"sns.boxplot(x='BelongCollection', y='revenue', data=xtrain);","552c57d1":"sns.catplot(x='GenresNumb', y='revenue', data=xtrain);\nplt.title('Revenue for different number of genres in the film');","71800c6d":"f, axes = plt.subplots(3, 5, figsize=(24, 12))\nplt.suptitle('Violinplot of revenue vs genres')\nfor i, e in enumerate([col for col in xtrain.columns if 'genre_' in col]):\n    sns.violinplot(x=e, y='revenue', data=xtrain, ax=axes[i \/\/ 5][i % 5]);","0aa76d02":"f, axes = plt.subplots(6, 5, figsize=(24, 32))\nplt.suptitle('Violinplot of revenue vs production company')\nfor i, e in enumerate([col for col in xtrain.columns if 'production_company' in col]):\n    sns.violinplot(x=e, y='revenue', data=xtrain, ax=axes[i \/\/ 5][i % 5]);","95ac9fad":"sns.catplot(x='ProdCountryNumb', y='revenue', data=xtrain);\nplt.title('Revenue for different number of countries producing the film');","fb5a05d3":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(xtrain['num_cast'], xtrain['revenue'])\nplt.title('Number of cast members vs revenue');\nplt.subplot(1, 2, 2)\nplt.scatter(xtrain['num_cast'], xtrain['log_revenue'])\nplt.title('Log Revenue vs number of cast members');","9cddaa32":"f, axes = plt.subplots(6, 5, figsize=(24, 32))\nplt.suptitle('Violinplot of revenue vs keyword')\nfor i, e in enumerate([col for col in xtrain.columns if 'keyword_' in col]):\n    sns.violinplot(x=e, y='revenue', data=xtrain, ax=axes[i \/\/ 5][i % 5]);","29db055e":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(xtrain['num_crew'], xtrain['revenue'])\nplt.title('Number of crew members vs revenue');\nplt.subplot(1, 2, 2)\nplt.scatter(xtrain['num_crew'], xtrain['log_revenue'])\nplt.title('Log Revenue vs number of crew members');","40977435":"show_head(xtrain)","b41c5ac8":"for col in ['original_language', 'CollectionName']:\n    le = LabelEncoder()\n    le.fit(list(xtrain[col].fillna('')) + list(xtest[col].fillna('')))\n    xtrain[col] = le.transform(xtrain[col].fillna('').astype(str))\n    xtest[col] = le.transform(xtest[col].fillna('').astype(str))","610ce089":"show_head(xtrain)","d29aad57":"train_texts = xtrain[['title', 'tagline', 'overview', 'original_title']]\ntest_texts = xtest[['title', 'tagline', 'overview', 'original_title']]","139afc35":"for col in ['title', 'tagline', 'overview', 'original_title']:\n    xtrain['len_' + col] = xtrain[col].fillna('').apply(lambda x: len(str(x)))\n    xtrain['words_' + col] = xtrain[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    xtrain = xtrain.drop(col, axis=1)\n    xtest['len_' + col] = xtest[col].fillna('').apply(lambda x: len(str(x)))\n    xtest['words_' + col] = xtest[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    xtest = xtest.drop(col, axis=1)","dc49d80d":"show_head(xtrain)","27b1fa7a":"xtrain.fillna(xtrain.mean(), inplace=True)\nxtest.fillna(xtest.mean(), inplace=True)","e57101bf":"ytrain = np.log1p(xtrain['revenue'])\nxtrain = xtrain.drop(['id', 'revenue', 'homepage', 'imdb_id', 'poster_path', 'release_date', 'status', 'log_revenue'], axis=1)\nxtest = xtest.drop(['id', 'homepage', 'imdb_id', 'poster_path', 'release_date', 'status'], axis=1)","0ebd5aac":"\nxtrain.shape","6dd7adf6":"x_train, x_valid, y_train, y_valid = train_test_split(xtrain, ytrain, test_size=0.1, random_state=4, shuffle=True)","490e9a18":"from sklearn.ensemble import RandomForestRegressor as rfr\n\nclf_rfr = rfr(n_estimators=1000, random_state=4)\nclf_rfr.fit(x_train, y_train)","9ff64991":"predictions = clf_rfr.predict(xtest)\npred = clf_rfr.predict(x_valid)","78f2edc8":"errors = abs(pred - y_valid)\nprint('Mean Absolute Error:',round(np.mean(errors), 2), 'degrees.')","d288e225":"mape = 100 * (errors\/y_valid)\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","26ebdb5b":"sam_sub['revenue'] = np.expm1(predictions)\nsam_sub.to_csv(\"RFC.csv\", index=False)","b82e2904":"A fim genres is a motion-picture category based om similarities in emotional responses. From defination, this column will show the level of how film will influence viewers base on their similarities. We should focus on this column.\n\nFrom the look of things, it shows that a some of this films are much alike that the others. Those ones with number of genres will have more viewers I guess. Those films with 6, 7, 0 genres could be counted as outliers.\n\nWe should check on the list of the genres to know how many are they and to see if we can create columns for them. ","28ce9825":"**Crew**","6351571b":"this shows that films release on wednesday have higher revenue. This makes this column important in our prediction","ea9b60a9":"**Keywords**","191eee36":"**Production Company**\n\nI believe this column is less important because many viewers knows absolute nothing about the company producing the movie.\n\nLet us explore it first and extract some content then decision on it will be later things. Intrestingly the movies have more than one production company.","b183e907":"This confirms my guess. The USA top the list followed by Kingdom and france.\n\nWe will create column for the first five","c7f4bf3d":"**Genres**","9e1a34d6":"**Belong_to_Collection**\n\nWe will extract datas of the films that belongs to collection\n\nThe useful info here is the collection names and the fact that movie belongs to a collection. I will creat a column for the names of the collections and another for belonging to a collection. the column for belonging to a collection will be a binary classification 0 or 1. 0 for not belonging to a collection(empty) and 1 fpr belonging to collection.","afc9fecb":"**Modelling the Game**\n\n\nNow the boring and the mindless lifting is over.....Let us model some demons our of this sucker files","c4cae8f6":"Evaluating the clf itself","69c934d9":"**Production Countries**","ed0d3d96":"Importing Libraries","4c1a2606":"**Production Companies**","8edc4f1f":"In this competition, you're presented with metadata on over 7,000 past films from The Movie Database to try and predict their overall worldwide box office revenue. Data points provided include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries.\n\nWe are predicting the worldwide revenue for 4398 movies in the test file.","087374da":"**About The Data**","fa70134a":"**Production Countries**\n\nThe country of production may have impact depending on the social nature of the country and the language. We all know most movie seen by even we are 80% american movie. Let us see. some movies were produced in more than one countries. Most movies are produced in one country.","33d60488":"I noticed that we are having many missing values in this train file. The belonging to coliection column has about 80 percent missing values. This has already negate the column. If we fill the column we will be at the danger of incorrect data.\n\ngenres is good column we need to focus on because the type of genres can determine customers. Language is a good features too. Title and so on \n","cb785cca":"**Random Forest Regression**\n\nWe will build our model first with RFC.","17a8f50d":"I guess we should remove the AllGenres column to avoid noise and overfitting. I will remove it latter.","88f0866f":"**Spoken Language**\n\nThis also attracts our focus base on the country that mostly appear in our data.","8834784d":"2396 does not belong to any collection and no movie belongs to more than 1 collection. That makes it easy for us","e7382ba5":"From the wordcloud we see that DRAMA, COMEDY AND THRILLER often appear in the films. This is real because this three genres are most watch in reality. Any film that belongs to of this genres could have the potential of high revenue due to high number of customers.\n\nI will create a column for top 10. This top 10 will have a great impact in out prediction.","f609d515":"**Genres**","6955cbc6":"**Crew **","111a5d37":"We have 3000 train sample with 23 features and 4398 test samples with 22 feature","ef02606e":"**Cast**","099ee7aa":"**Tagline**","eb02e401":"Fun and finanace arnt friends and they are in most cases used paradoxically. thimgs like ...hard works pay with no or little fun, all day movies are the streanght of the weak, burn the candles overnight so as not to burn the volts at old age..... I am a strong fan of the sage. Statements like this are in most cases my watch word.  For further thought and reflections, are this words truely true?\n\ncant we find furtune in fun? purpose in jokes, career in movies..... if we are to ponder over this kernel topic, we may at the end find an ace up our sleeve. As a friend rightly define popcorn movie has the movie we watch for entartainment but he has speedily forgoten that the fun and pleasure derived from the movie helps in better reflection to the cores of rightful defination of what life exactly is. We can think on these things \n\nBox office are places where ticket are sold and revenue predicted for a perticular movies. As i know, not only the box office but also cinemas, internet and so on.... Revenue are largly generated from internet this days. Let us use our analysis of data to predict movie revenue and help the industries.\n\nLet us begin!!!!!!!!!!!!","139ad196":"**Data Exploration**\n\nWe will take the columns one after the other and do healing to the sick parts lol.....","7fd5f8cc":"**Runtime**","1e700f68":"**Keywords**","62996603":"**Collections**","c01d5548":"to be continued......\nIf you like my kernel please upvote and leave a comment\n\nMany thanks to  Andrew Lukyanenko\n\nhttps:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-model-interpretation","71740248":"**Introduction**","2bf51f35":"**Cast**","c213a6c6":"This shows that films with more than one hour have high revenues but popular films are with short time. Comedy are popular and are short. This makes us have faith on this column","175ca663":"**Status**","98dc13ea":"So our films with collection has more income that is interesting"}}