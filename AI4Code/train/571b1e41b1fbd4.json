{"cell_type":{"fe5843f2":"code","ff18e0be":"code","3eea3b78":"code","de88a58a":"code","292cdbec":"code","9d61d32c":"code","7b81d44a":"code","d6d05732":"code","b3e7ecbd":"code","b0e192b7":"code","2930681a":"code","57dc77e2":"code","eccae0a3":"code","d4aca65e":"code","656f919e":"code","cdd0b95e":"code","1c3ddc0c":"code","996dd7f0":"code","e7ac5e65":"code","e86c16f0":"code","63e3182b":"code","fcecad35":"code","9619e29f":"code","b5356cff":"code","02a528dc":"code","991f8833":"code","a41a8670":"code","82e8682d":"code","eb56a549":"code","6b4d2ad7":"code","84988194":"code","8ea5a53c":"markdown","5a83567c":"markdown","a007298a":"markdown","83877fa2":"markdown","7475393d":"markdown","4d9481b4":"markdown","e2f0e7e2":"markdown","d0db5815":"markdown","e4c44009":"markdown","6fddcb97":"markdown","30114157":"markdown","6e1fa481":"markdown","8372920b":"markdown","097688cc":"markdown","9ec05027":"markdown","2e57edfc":"markdown","b4062f03":"markdown","8267b2b9":"markdown","58f516d1":"markdown","957550c2":"markdown","6e6cbc65":"markdown","ac692793":"markdown","e5dbd4bd":"markdown","6b338a15":"markdown","7784d477":"markdown","f8e92305":"markdown"},"source":{"fe5843f2":"!pip install datasets --no-index --find-links=file:\/\/\/kaggle\/input\/coleridge-packages\/packages\/datasets\n!pip install ..\/input\/coleridge-packages\/seqeval-1.2.2-py3-none-any.whl\n!pip install ..\/input\/coleridge-packages\/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ..\/input\/coleridge-packages\/transformers-4.5.0.dev0-py3-none-any.whl\n\nfrom IPython.display import clear_output\nclear_output()","ff18e0be":"import numpy as np\nimport pandas as pd \nimport json\nimport os \nimport re\nimport string\nimport random\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nfrom wordcloud import WordCloud\n\n#Text Color\nfrom termcolor import colored\n\n#NLP\nimport spacy\n\nfrom tqdm.auto import tqdm\n\nimport pathlib\nimport glob\n\n\nfrom datasets import load_dataset\nimport torch\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling, \\\nAutoModelForMaskedLM, Trainer, TrainingArguments, pipeline\n\nfrom typing import List\nimport string\nfrom functools import partial\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nplt.rcParams['figure.figsize']=(8,6)","3eea3b78":"def SeedEverything(seed: int):\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f'Setted Pipeline SEED = {SEED}')\n\nSEED=2021\nSeedEverything(SEED)","de88a58a":"DATA_PATH = pathlib.Path('..\/input\/coleridgeinitiative-show-us-the-data')","292cdbec":"#reading train.csv\ntrain=pd.read_csv(DATA_PATH \/'train.csv')\ntrain.sample(5)","9d61d32c":"sampleSubmission=pd.read_csv(DATA_PATH \/'sample_submission.csv')\nsampleSubmission.head()","7b81d44a":"print('Dimension of the training Dataset : {}'.format(colored(train.shape,'blue')))","d6d05732":"train.info()","b3e7ecbd":"train.isnull().sum().to_frame('NaN Values')","b0e192b7":"print('All rows :',colored(train.shape[0],'red'))\nfor column in train.columns:\n    print(\"{} : {}\".format(column,colored(len(train[column].unique()),'blue')))","2930681a":"trainFilesPath =DATA_PATH \/'train'\ntestFilesPath = DATA_PATH \/'test'","57dc77e2":"def ReadJsonFiles(fileName, InputPath):\n    \"\"\"\n    This Function get the Publication text from Json file without Section titles\n    \"\"\"\n    \n    JsonPATH = os.path.join(InputPath, (fileName+'.json'))\n    \n    publicationSections = []\n    \n    with open(JsonPATH, 'r') as file:\n        json_decode = json.load(file)\n        for data in json_decode:\n            publicationSections.append(data.get('text'))\n    \n    publicationText = ' '.join(publicationSections)\n    \n    return publicationText","eccae0a3":"#Extract text from json file and plus its column to train and sampleSubmission csv file:\ntqdm.pandas()\ntrain['publicationText']=train['Id'].progress_apply(lambda x:ReadJsonFiles(x,trainFilesPath))\nsampleSubmission['publicationText']=sampleSubmission['Id'].progress_apply(lambda x:ReadJsonFiles(x,testFilesPath))","d4aca65e":"train.sample(5)","656f919e":"sampleSubmission.head()","cdd0b95e":"def TextCleaning(text):\n    \n   \n    text = ''.join([k for k in text if k not in string.punctuation])#Delete punctuation\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()  #Convert all text to lower case\n    text = re.sub(' +', ' ', text)\n\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","1c3ddc0c":"#Example :\nTextCleaning('Hello World \ud83d\ude00')","996dd7f0":"tqdm.pandas()\ntrain['publicationText']=train['publicationText'].progress_apply(lambda x:TextCleaning(x))\nsampleSubmission['publicationText']=sampleSubmission['publicationText'].progress_apply(lambda x: TextCleaning(x))","e7ac5e65":"sampleSubmission","e86c16f0":"papers = {}\nfor paper_id in sampleSubmission['Id']:\n    with open(f'{testFilesPath}\/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","63e3182b":"adnl_govt_labels = pd.read_csv('..\/input\/bigger-govt-dataset-list\/data_set_800.csv')\n\nliteral_preds = []\nto_append = []\nfor index, row in tqdm(sampleSubmission.iterrows()):\n    to_append = [row['Id'],'']\n    \n    clean_string = row['publicationText']\n    for index, row2 in adnl_govt_labels.iterrows():\n        query_string = str(row2['title'])\n        if query_string in clean_string:\n            if to_append[1] != '' and query_string not in to_append[1]:\n                to_append[1] = to_append[1] + '|' + query_string\n            if to_append[1] == '':\n                to_append[1] = query_string\n    literal_preds.append(*to_append[1:])","fcecad35":"literal_preds","9619e29f":"PRETRAINED_PATH = '..\/input\/coleridge-mlm-model\/output-mlm\/checkpoint-48000'\nTOKENIZER_PATH = '..\/input\/coleridge-mlm-model\/model_tokenizer'\n\nMAX_LENGTH = 64\nOVERLAP = 20\n\nPREDICT_BATCH = 32\n\nDATASET_SYMBOL = '$' # this symbol represents a dataset name\nNONDATA_SYMBOL = '#' # this symbol represents a non-dataset name","b5356cff":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=True)\nmodel = AutoModelForMaskedLM.from_pretrained(PRETRAINED_PATH)\n\nmlm = pipeline(\n    'fill-mask', \n    model=model,\n    tokenizer=tokenizer,\n    device=0 if torch.cuda.is_available() else -1\n)","02a528dc":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) \/ union\n\ndef clean_paper_sentence(s):\n    \"\"\"\n    This function is essentially clean_text without lowercasing.\n    \"\"\"\n    s = re.sub('[^A-Za-z0-9]+', ' ', str(s)).strip()\n    s = re.sub(' +', ' ', s)\n    return s\n\ndef shorten_sentences(sentences):\n    \"\"\"\n    Sentences that have more than MAX_LENGTH words will be split\n    into multiple sentences with overlappings.\n    \"\"\"\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences\n\nconnection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'data', 'dataset'}\ndef find_mask_candidates(sentence):\n    \"\"\"\n    Extract masking candidates for Masked Dataset Modeling from a given $sentence.\n    A candidate should be a continuous sequence of at least 2 words, \n    each of these words either has the first letter in uppercase or is one of\n    the connection words ($connection_tokens). Furthermore, the connection \n    tokens are not allowed to appear at the beginning and the end of the\n    sequence.\n    \"\"\"\n    def candidate_qualified(words):\n        while len(words) and words[0].lower() in connection_tokens:\n            words = words[1:]\n        while len(words) and words[-1].lower() in connection_tokens:\n            words = words[:-1]\n        \n        return len(words) >= 2\n    \n    candidates = []\n    \n    phrase_start, phrase_end = -1, -1\n    for id in range(1, len(sentence)):\n        word = sentence[id]\n        if word[0].isupper() or word in connection_tokens:\n            if phrase_start == -1:\n                phrase_start = phrase_end = id\n            else:\n                phrase_end = id\n        else:\n            if phrase_start != -1:\n                if candidate_qualified(sentence[phrase_start:phrase_end+1]):\n                    candidates.append((phrase_start, phrase_end))\n                phrase_start = phrase_end = -1\n    \n    if phrase_start != -1:\n        if candidate_qualified(sentence[phrase_start:phrase_end+1]):\n            candidates.append((phrase_start, phrase_end))\n    \n    return candidates","991f8833":"mask = mlm.tokenizer.mask_token","a41a8670":"all_test_data = []\n\n\nfor paper_id in sampleSubmission['Id']:\n    # load paper\n    paper = papers[paper_id]\n    \n    # extract sentences\n    sentences = set([clean_paper_sentence(sentence) for section in paper \n                     for sentence in section['text'].split('.')\n                    ])\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n    sentences = [sentence.split() for sentence in sentences] # sentence = list of words\n    \n    # mask\n    test_data = []\n    for sentence in sentences:\n        for phrase_start, phrase_end in find_mask_candidates(sentence):\n            dt_point = sentence[:phrase_start] + [mask] + sentence[phrase_end+1:]\n            test_data.append((' '.join(dt_point), ' '.join(sentence[phrase_start:phrase_end+1]))) # (masked text, phrase)\n    \n    all_test_data.append(test_data)\n    ","82e8682d":"pred_labels = []\n\npbar = tqdm(total = len(all_test_data))\nfor test_data in all_test_data:\n    pred_bag = set()\n    \n    if len(test_data):\n        texts, phrases = list(zip(*test_data))\n        mlm_pred = []\n        for p_id in range(0, len(texts), PREDICT_BATCH):\n            batch_texts = texts[p_id:p_id+PREDICT_BATCH]\n            batch_pred = mlm(list(batch_texts), targets=[f' {DATASET_SYMBOL}', f' {NONDATA_SYMBOL}'])\n            \n            if len(batch_texts) == 1:\n                batch_pred = [batch_pred]\n            \n            mlm_pred.extend(batch_pred)\n        \n        for (result1, result2), phrase in zip(mlm_pred, phrases):\n            if (result1['score'] > result2['score']*1.5 and result1['token_str'] == DATASET_SYMBOL) or\\\n               (result2['score'] > result1['score']*1.5 and result2['token_str'] == NONDATA_SYMBOL):\n                pred_bag.add(clean_text(phrase))\n    \n    # filter labels by jaccard score \n    filtered_labels = []\n    \n    for label in sorted(pred_bag, key=len, reverse=True):\n        if len(filtered_labels) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered_labels):\n            filtered_labels.append(label)\n            \n    pred_labels.append('|'.join(filtered_labels))\n    pbar.update(1)","eb56a549":"pred_labels","6b4d2ad7":"final_predictions=[]\nfor literal_match, mlm_pred in zip(literal_preds, pred_labels):\n        if literal_match!='' and mlm_pred not in literal_match:\n            final_predictions.append(literal_match +'|'+mlm_pred)\n        else:\n            if literal_match:\n                final_predictions.append(literal_match)\n            else :\n                final_predictions.append(mlm_pred)\n            \n\n            \nfinal_predictions\n            \nsampleSubmission['PredictionString'] = final_predictions\nsample_submission=sampleSubmission[['Id','PredictionString']]\nsample_submission.to_csv('submission.csv', index=False)","84988194":"sample_submission['PredictionString'].head().to_list()","8ea5a53c":"## Load model and tokenizer","5a83567c":"## Transform :","a007298a":"We don't have any duplicated value on the train set","83877fa2":"<a id='5'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 10px 25px;\">Masked Modling Language \ud83e\udd17<\/p>","7475393d":"## Install packages :","4d9481b4":"<a id='4'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 10px 25px;\">Matching \ud83d\udcd1<\/p>","e2f0e7e2":"## Predict :","d0db5815":"* Data Description :","e4c44009":"* we have 14316 unique `Publication Id` that's mean there's some publication mentioning more than one data set.\n* for the `publication title` there's less unique values than `Publication Id` So there's diffirent publication with same title.\n* 45 `Dataset title` and 130 `Dataset Label` that's mean there some dataset with multiple labels","6fddcb97":"Let's do some data cleaning\n\n`TextCleaning` function will help us to convert all text to lower case, remove special charecters, emojis and multiple spaces","30114157":"* Let's take a look to the `Sample_submission.csv `:","6e1fa481":"* Training set shape :","8372920b":"> \ud83d\udcd1 Context : This competition challenges data scientists to show how publicly funded data are used to serve science and society. Evidence through data is critical if government is to address the many threats facing society, including; pandemics, climate change, Alzheimer\u2019s disease, child hunger, increasing food production, maintaining biodiversity, and addressing many other challenges. Yet much of the information about data necessary to inform evidence and science is locked inside publications.\n\n> In this competition, you'll use natural language processing (NLP) to automate the discovery of how scientific data are referenced in publications. Utilizing the full text of scientific publications from numerous research areas gathered from CHORUS publisher members and other sources, you'll identify data sets that the publications' authors used in their work.\n\n> \ud83d\udccc Goal : The objective of the competition is to identify the mention of datasets within scientific publications. ","097688cc":"<a id='2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 10px 25px;\">Loading the data \u231b<\/p>","9ec05027":"# <p style=\"font-family:newtimeroman; text-align:center; fontsize:150%\">Coleridge Initiative - Show US the Data<br>Discover how data is used for the public good<\/p>","2e57edfc":"### 1 CSV files :","b4062f03":"* `train.csv` : Labels and metadata for the training set from scientific publications in the train folder ;\n* `train` - the full text of the training set's publications in JSON format, broken into sections with section titles\n* `test` - the full text of the test set's publications in JSON format, broken into sections with section titles\n* The `sample_subimission.csv` : a sample submission file in the correct format.","8267b2b9":"## Importing Libraries:","58f516d1":"**Columns Description :**\n\n* `id `- publication id - note that there are multiple rows for some training documents, indicating multiple mentioned datasets\n* `pub_title` - title of the publication (a small number of publications have the same title)\n* `dataset_title` - the title of the dataset that is mentioned within the publication\n* `dataset_label` - a portion of the text that indicates the dataset\n* `cleaned_label` - the dataset_label, as passed through the clean_text function from the Evaluation page\n* `PredictionString`- To be filled with equivalent of cleaned_label of train data (just in sample submission).","957550c2":"### 3 Reading JSON format :","6e6cbc65":"<a id='0'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 10px 25px;\">Table of Content<\/p>\n* [1. Importing necessary packages and libraries\ud83d\udcda](#1)\n* [2. Loading the data \u231b](#2)\n* [3. Data Pre-Processing\ud83d\udd27](#4)\n* [4. Matching \ud83d\udcd1](#4)\n* [5. Masked Language Modling  \ud83e\udd17](#5)\n","ac692793":"### 2 Basic Analysis :","e5dbd4bd":"Let's apply `ReadJsonFile` to train and Submission Set (Kaggle Test Set):","6b338a15":"The publications that we will use in train and test are provided  in JSON format, broken up into sections with section titles.","7784d477":"<a id='3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 10px 25px;\">Data Pre-Processing \ud83d\udd27<\/p>","f8e92305":"<a id='1'><\/a>\n## <p style=\"background-color:skyblue ; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 10px 25px;\">Importing necessary packages and libraries\ud83d\udcda<\/p>"}}