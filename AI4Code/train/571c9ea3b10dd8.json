{"cell_type":{"d2414908":"code","4b3ed26d":"code","c67ac9d7":"code","2235baa5":"code","099178f7":"code","e4b9163c":"code","d700c100":"code","62569289":"code","95c2ab96":"code","4c2e7547":"code","54c51530":"code","c67f47df":"code","932b3d0c":"code","cdac89ba":"code","ce27d6a1":"code","c52902bc":"code","f4a81c80":"code","378e246d":"code","aa36375b":"code","3f356882":"code","1229b960":"code","54fc4039":"code","9fb8f7fd":"code","c34cc389":"code","59a4fcf9":"code","f37a0e03":"code","47e7e993":"code","41fe65ee":"code","7ec7148f":"code","e2d87955":"code","ba511db9":"code","4ca0e585":"code","0c9a3b6b":"code","253446f7":"code","08f1a575":"code","6efe9fe7":"code","d21ab3ba":"code","0e30c035":"code","a4f2f50d":"code","9d3bdda4":"code","d88c759b":"code","ad49b2c3":"code","cfc16515":"code","54519278":"code","a73f92c4":"code","a16382fe":"code","f64b5d5e":"code","bfcb84b3":"code","da1594c3":"code","993804fb":"code","489e642f":"code","91151787":"code","824043b5":"code","4f1d7319":"code","6e0c18e5":"code","eeb71325":"code","be47740b":"code","b48310b7":"code","6a265e19":"code","a83d6abf":"code","0c0fee94":"code","a9c41b93":"code","b2a2854c":"code","56e1c03b":"code","6b13310b":"code","490ed9d7":"code","5f327463":"code","4303688a":"code","66d93134":"code","23750600":"code","1bded521":"code","d03a3228":"code","98a29753":"code","4687a1ab":"code","41891378":"code","f5698402":"code","d1dc7481":"code","6dcff71c":"code","bb02a2b5":"code","bf2cf101":"code","eb2811b1":"code","fdb3eb99":"code","b5b7ae9a":"code","2d0c688c":"code","63c36766":"code","aacfc275":"code","8a3dfacf":"code","e960bcfb":"code","7e836c95":"code","55723ae3":"code","a9ac850e":"code","dc89c4ad":"code","604a119e":"code","c486a839":"code","b57d5e39":"code","8a3bc503":"code","b8b3b3db":"code","5b3349fa":"markdown","d4621211":"markdown","9c758b1c":"markdown","357ba706":"markdown","e752cf15":"markdown","be84b86a":"markdown","674dff36":"markdown","65c79c25":"markdown","a3e86b16":"markdown","6bdad000":"markdown","779ca773":"markdown","1be984ad":"markdown","49c97b93":"markdown","0a98ad55":"markdown","d95787b5":"markdown","15d59ef3":"markdown","62811972":"markdown","bfdb5dde":"markdown","4faa3dc1":"markdown","992613c9":"markdown","ac1aa4dc":"markdown","74a44a4c":"markdown","f3b6bc7d":"markdown","9297fde7":"markdown","ed2416f1":"markdown","cf9611a0":"markdown","a00b2471":"markdown","5a0021ea":"markdown","665f6a43":"markdown","f72acb91":"markdown","6871c2ba":"markdown","7f01f8db":"markdown","ba470221":"markdown","e5da322b":"markdown","6a265c44":"markdown","4abfdf77":"markdown","937b6063":"markdown","07e9e906":"markdown","4fa38434":"markdown","6404f520":"markdown","a2573856":"markdown","d24b7e4d":"markdown","746a3da1":"markdown","a5ecb840":"markdown","778afa87":"markdown","83e24c89":"markdown","1ec75a8f":"markdown","b2c4d0e2":"markdown","30e1a395":"markdown","3a293c63":"markdown","84eba051":"markdown","f33bee80":"markdown","cf3838fe":"markdown","718bac04":"markdown","9cd38b02":"markdown","8a6f4dee":"markdown","58424a4a":"markdown","31df91ae":"markdown","7ee76e02":"markdown","67e101d4":"markdown","7a9cd2ad":"markdown","192a39df":"markdown"},"source":{"d2414908":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, StandardScaler, PowerTransformer, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_predict, train_test_split, GridSearchCV, cross_val_score, cross_validate\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge,ElasticNet\nfrom sklearn.metrics import plot_confusion_matrix, r2_score, mean_absolute_error, mean_squared_error, classification_report, confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, plot_roc_curve, roc_auc_score, roc_curve\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostClassifier\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.impute import SimpleImputer, KNNImputer\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\n# pd.set_option('display.max_rows', 100) # if you wish to see more rows rather than default, just uncomment this line.\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.7f' % x)","4b3ed26d":"# Function for determining the number and percentages of missing values\n\ndef missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values","c67ac9d7":"# Function for comparing different approaches\n\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = SVC(class_weight = \"balanced\", random_state=42)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return accuracy_score(y_valid, preds)","2235baa5":"df0 = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","099178f7":"df = df0.copy()","e4b9163c":"df.head()","d700c100":"df.info()","62569289":"df.describe().T","95c2ab96":"df.describe(include=object).T","4c2e7547":"print(f\"Data has {df.shape[0]} instances and {df.shape[1] - 1} attributes.\")","54c51530":"df.nunique()","c67f47df":"df.duplicated().value_counts()","932b3d0c":"missing(df)","cdac89ba":"df['stroke'].value_counts()","ce27d6a1":"df['stroke'].value_counts(normalize=True)*100","c52902bc":"y = df['stroke']\nprint(f'Percentage of patient has a stroke: % {round(y.value_counts(normalize=True)[1]*100,2)} -->\\\n     ({y.value_counts()[1]} patient)\\nPercentage of patient does not have a stroke: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} patient)')","f4a81c80":"df.drop('id', axis=1, inplace=True)","378e246d":"df.columns","aa36375b":"df.columns = df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')","3f356882":"df.columns","1229b960":"sns.heatmap(df.corr(), annot=True);","54fc4039":"numerical= df.drop(['stroke'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint(\"\\033[1m\", \"Numerical Columns:\", \"\\033[0;0m\",  numerical)\nprint(\"------------------------------------------------------------------------------------------------------------------\")\nprint(\"\\033[1m\", \"Categorical Columns:\", \"\\033[0;0m\", categorical)","9fb8f7fd":"df[numerical].shape","c34cc389":"df['stroke'].describe().T","59a4fcf9":"df['stroke'].value_counts()","f37a0e03":"print( f\"Skewness: {df['stroke'].skew()}\")","47e7e993":"df['stroke'].iplot(kind='hist')","41fe65ee":"df[numerical].describe().T","7ec7148f":"df[numerical].iplot(kind='hist');","e2d87955":"df[numerical].iplot(kind='histogram',subplots=True,bins=50)","ba511db9":"df.columns","4ca0e585":"for i in df.drop(columns=[\"stroke\", 'gender', 'hypertension', 'heart_disease', 'ever_married',\n       'work_type', 'residence_type', 'smoking_status']).columns:\n    df[i].iplot(kind=\"box\", title=i, boxpoints=\"all\")","0c9a3b6b":"index = 0\nplt.figure(figsize=(20,20))\nfor feature in df.drop(columns=['gender', 'hypertension', 'heart_disease', 'ever_married',\n       'work_type', 'residence_type', 'smoking_status']).columns:\n    if feature != \"stroke\":\n        index += 1\n        plt.subplot(4,3,index)\n        sns.boxplot(x='stroke', y=feature, data=df)","253446f7":"df[(df['age'] <= 14) & (df['stroke'] == 1)] ","08f1a575":"df.age.value_counts()","6efe9fe7":"df[df['age'] < 2]","d21ab3ba":"skew_limit = 0.75 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models. \nskew_vals = df[numerical].skew()\nskew_cols = skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols ","0e30c035":"df[skew_cols.index].iplot(kind='hist');","a4f2f50d":"df[skew_cols.index].iplot(kind='histogram',subplots=True,bins=50)","9d3bdda4":"skew_limit = 0.75 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models.\nskew_vals = df[numerical].skew()\nskew_cols = skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols = skew_cols.drop(['heart_disease', 'hypertension'])\nskew_cols","d88c759b":"df_try = df.copy()\nfor col in skew_cols.index.values:\n    df_try[col] = df_try[col].apply(np.log1p)\nprint(df_try[skew_cols.index].skew())\nprint()\ndf_try[skew_cols.index].iplot(kind='histogram',subplots=True,bins=50);","ad49b2c3":"df_trans = df[skew_cols.index].copy()\npt = PowerTransformer(method='yeo-johnson')\ntrans= pt.fit_transform(df_trans)\ndf_trans = pd.DataFrame(trans, columns =skew_cols.index )\nprint(df_trans.skew())\nprint()\ndf_trans.iplot(kind='histogram',subplots=True,bins=50);","cfc16515":"df.shape","54519278":"df.drop([\"avg_glucose_level\", \"bmi\"], axis=1, inplace=True)","a73f92c4":"df.shape","a16382fe":"df_trans.shape","f64b5d5e":"df = pd.concat([df, df_trans], axis=1)","bfcb84b3":"df.shape","da1594c3":"df","993804fb":"df.gender.value_counts()","489e642f":"df.drop(df[df['gender'] == 'Other'].index, inplace = True)","91151787":"df.shape","824043b5":"df.gender.value_counts()","4f1d7319":"df[categorical].head().T","6e0c18e5":"df[categorical].describe()","eeb71325":"# To view summary information about the column\n\ndef first_looking(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"per_of_nulls   : \", \"%\", round(df[col].isnull().sum()\/df.shape[0]*100, 2))\n    print(\"num_of_nulls   : \", df[col].isnull().sum())\n    print(\"num_of_uniques : \", df[col].nunique())\n    print(df[col].value_counts(dropna = False))\n","be47740b":"first_looking(\"gender\")","b48310b7":"print(df.groupby('gender')['stroke'].mean().sort_values())\nprint()\ndf.groupby('gender')['stroke'].mean().iplot(kind='histogram',subplots=True,bins=50)","6a265e19":"first_looking(\"ever_married\")","a83d6abf":"print(df.groupby('ever_married')['stroke'].mean().sort_values())\nprint()\ndf.groupby('ever_married')['stroke'].mean().iplot(kind='histogram',subplots=True,bins=50)","0c0fee94":"first_looking(\"work_type\")","a9c41b93":"print(df.groupby('work_type')['stroke'].mean().sort_values())\nprint()\ndf.groupby('work_type')['stroke'].mean().iplot(kind='histogram',subplots=True,bins=50)","b2a2854c":"first_looking(\"residence_type\")","56e1c03b":"print(df.groupby('residence_type')['stroke'].mean().sort_values())\nprint()\ndf.groupby('residence_type')['stroke'].mean().iplot(kind='histogram',subplots=True,bins=50)","6b13310b":"first_looking(\"smoking_status\")","490ed9d7":"print(df.groupby('smoking_status')['stroke'].mean().sort_values())\nprint()\ndf.groupby('smoking_status')['stroke'].mean().iplot(kind='histogram',subplots=True,bins=50)","5f327463":"sns.pairplot(df, hue=\"stroke\");","4303688a":"sns.scatterplot(x = 'avg_glucose_level', y = 'age', hue = 'stroke', data = df);","66d93134":"sns.scatterplot(x = 'bmi', y = 'age', hue = 'stroke', data = df);","23750600":"sns.scatterplot(x = 'avg_glucose_level', y = 'bmi', hue = 'stroke', data = df);","1bded521":"df.columns","d03a3228":"df = pd.get_dummies(df, columns=['gender', 'ever_married',\n       'work_type', 'residence_type', 'smoking_status',], drop_first=True)","98a29753":"df.head()","4687a1ab":"plt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), annot=True)\nplt.xticks(rotation=45);","41891378":"X = df.drop([\"stroke\"], axis=1)\ny = df[\"stroke\"]","f5698402":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","d1dc7481":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer(missing_values=np.nan, strategy=\"median\")\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_test = pd.DataFrame(my_imputer.transform(X_test))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_test.columns = X_test.columns","6dcff71c":"imputed_X_train.isnull().sum()","bb02a2b5":"imputed_X_test.isnull().sum()","bf2cf101":"imputed_X_train.age.value_counts().head(10)","eb2811b1":"X_train.age.value_counts().head(10)","fdb3eb99":"X_test = imputed_X_test","b5b7ae9a":"X_train = imputed_X_train","2d0c688c":"from sklearn.preprocessing import MinMaxScaler","63c36766":"scaler = MinMaxScaler()","aacfc275":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","8a3dfacf":"svm_model_scaled = SVC(class_weight = \"balanced\")\nsvm_model_scaled.fit(X_train_scaled, y_train)\ny_pred = svm_model_scaled.predict(X_test_scaled)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","e960bcfb":"plot_confusion_matrix(svm_model_scaled, X_test_scaled, y_test);","7e836c95":"model = SVC(class_weight = \"balanced\")\n\nscores = cross_validate(model, X_train_scaled, y_train, scoring = ['accuracy', 'precision','recall','f1'], cv = 10)\ndf_scores = pd.DataFrame(scores, index = range(1, 11))\ndf_scores.mean()[2:]","55723ae3":"from sklearn.model_selection import GridSearchCV","a9ac850e":"param_grid = {'C': [0.1,1, 10, 100, 1000],\n              'gamma': [\"scale\", \"auto\", 1,0.1,0.01,0.001,0.0001],\n              'kernel': ['rbf', 'linear']}","dc89c4ad":"model = SVC(class_weight = \"balanced\")\nsvm_model_grid = GridSearchCV(model, param_grid, verbose=3, refit=True)","604a119e":"svm_model_grid.fit(X_train_scaled, y_train)","c486a839":"svm_model_grid.best_params_","b57d5e39":"svm_model_grid.best_estimator_","8a3bc503":"y_pred = svm_model_grid.predict(X_test_scaled)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","b8b3b3db":"plot_confusion_matrix(svm_model_grid, X_test_scaled, y_test);","5b3349fa":"## 3) The Implementation of Support Vector Machine (SVM) <a class=\"anchor\" id=\"svm\"><\/a>   ","d4621211":"The proportion of target variable is one of the most important things in a classification problem. So let's a close look at how its values are distributed.","9c758b1c":"### 2.5 - Examination of Skewness <a class=\"anchor\" id=\"skewness\"><\/a>","357ba706":"### residence_type & stroke <a class=\"anchor\" id=\"residence_type&stroke\"><\/a>  ","e752cf15":"The first step is to import the required libraries.\n\nFor the visualization, the study will use both Seaborn and Plotly's interactive environment for making a better and meaningful comparison with related subjects.\n\nIf your plotly module is not in your working environment, please download plotly and run the following codes.\n\nFirst --> pip install plotly==5.1.0","be84b86a":"Let's split our features into two part, numerical and categorical, for easing our further examination. ","674dff36":"### ever_married & stroke <a class=\"anchor\" id=\"married&stroke\"><\/a> ","65c79c25":"## TABLE OF CONTENTS\n\n[A - PREFACE & PROBLEM DEFINED](#preface)\n* [Atribute Information](#atributeinformation)\n\n[B - LIBRARIES NEEDED IN THE STUDY](#libraries)\n* [User Defined Functions](#userdefinedfunctions)\n\n[C - ANALYSIS](#analysis)\n* [1) Reading the Dataset](#readingthedataset)\n* [2) Exploratory Data Analysis (EDA) & Visualization](#eda)\n    * [2.1 - A General Looking at the Data](#generallook)\n    * [2.2 - Data Cleaning](#datacleaning)\n    * [2.3 - The Examination of Target Variable](#targetvariable)\n    * [2.4 - Numerical Features](#numericalvariables)    \n    * [2.5 - Examination of Skewness](#skewness)     \n    * [2.6 - Handling with Skewness](#handlinghskewness)    \n        * [The Application of np.log](#np.log)\n        * [The Aplication of PowerTransformer](#powertransformer)        \n    * [2.7 - Categorical Features](#categoricalfeatures)          \n        * [gender & stroke](#gender&stroke)\n        * [ever_married & stroke](#married&stroke)\n        * [work_type & stroke](#work_type&stroke)\n        * [residence_type & stroke](#residence_type&stroke) \n        * [smoking_status & stroke](#smoking_status&stroke)\n    * [2.8 - Getting Dummies](#dummies)          \n* [3) The Implementation of Support Vector Machine (SVM)](#svm)        \n    * [3.1 - Train | Test Split & Handling with Missing Values](#tts)\n    * [3.2 - Scalling](#scalling)\n    * [3.3 - Modelling](#modelling)        \n        * [SVM Modelling by Default Parameters](#svmmodelling)      \n        * [SVM With Best Parameters (GridsearchCV)](#GridsearchCV) \n* [4) Conclusion](#Conclusion)  ","a3e86b16":"**The Aplication of PowerTransformer** <a class=\"anchor\" id=\"powertransformer\"><\/a> ","6bdad000":"![stroke.jpg](attachment:stroke.jpg)","779ca773":"### 2.1 - A General Looking at the Data <a class=\"anchor\" id=\"generallook\"><\/a>","1be984ad":"### gender & stroke  <a class=\"anchor\" id=\"gender&stroke\"><\/a> ","49c97b93":"**SVM Modelling by Default Parameters**  <a class=\"anchor\" id=\"svmmodelling\"><\/a>  ","0a98ad55":"Skewness is a key statistics concept you must know in the data science and analytics fields.\n\nIn simple words, skewness is the measure of how much the probability distribution of a random variable deviates from the normal distribution. \n\nThe primary reason skewness is important is that any analysis based on normal distribution incorrectly estimates expected returns and risk.\n\nFor more information please refer to [external link text](https:\/\/en.wikipedia.org\/wiki\/Skewness) [external link text](https:\/\/www.analyticsvidhya.com\/blog\/2020\/07\/what-is-skewness-statistics\/) [external link text](http:\/\/www.fusioninvesting.com\/2010\/09\/what-is-skew-and-why-is-it-important\/)","d95787b5":"**Before constructing our model, we should convert some features into dummies.**","15d59ef3":"**The Application of np.log** <a class=\"anchor\" id=\"np.log\"><\/a>","62811972":"**SPECIAL NOTE:**To prevent data leakage we prefer to handle with 201 missing values stored in \"bmi\" column after train-test split.","bfdb5dde":"### 2.4 - Numerical Features <a class=\"anchor\" id=\"numericalvariables\"><\/a>","4faa3dc1":"### 2.7 - Categorical Features <a class=\"anchor\" id=\"categoricalfeatures\"><\/a> ","992613c9":"## A - PREFACE & PROBLEM DEFINED <a class=\"anchor\" id=\"preface\"><\/a>\n\nIn this Exploratory Data Analysis (EDA) and Model Classification using Support Vector Machine (SVM), we will examine the dataset named as \"Stroke Prediction Dataset\" under the 'stroke-prediction-dataset' file at Kaggle website [external link text](https:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset).\n\nThis study, in general, will cover what any beginner in Machine Learning can do as much as possible for a better understanding with the given dataset not only by examining its various aspects but also visualising it. Later S\/he will be familiar with SVM Classification modelling. \n\nContext emphasizes that \"According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\"","ac1aa4dc":"### 2.8 - Getting Dummies <a class=\"anchor\" id=\"dummies\"><\/a> ","74a44a4c":"Having imputed train and test sets seperately so that we did not have any data leakage.","f3b6bc7d":"Now it's time to jump on the analysis of dataset.","9297fde7":"**GridSearchCV couldn't make any contribution to our model. Contrary, it decreased our True Positive predictions while increasing True Negatives predictions. What a shame...** ","ed2416f1":"## 2) Exploratory Data Analysis (EDA) & Visualization <a class=\"anchor\" id=\"eda\"><\/a>","cf9611a0":"## 4) Conclusion : <a class=\"anchor\" id=\"Conclusion\"><\/a> \n\n- In this study respectively,\n\n- We try to predict classification problem in Stroke Dataset by developing a Support Vector Machine (SVM) model.\n\n- We made the detailed exploratory analysis (EDA).\n\n- We have decided which metrics will be used.\n\n- We have analyzed both target and features in detail.\n\n- We have transformed categorical variables into dummies so we can use them in the model.\n\n- We have handled with skewness problem for make them closer to normal distribution.\n\n- We have decided to use SVC in our study.\n\n- We have cross-checked the model obtained from train set by applying cross validation for the model evaluation.\n\n- We have handled with missing values after train-split for avoiding data leakage.\n\n- We have examined the results of the  model and selected the best one for the problem in hand.\n\n- Further studies will hopefully examine this dataset by using KNN, Logistic Regression, Decision Tree, Random Forests seperatley. And then an evaluation notebook will be submitted for selecting the best model by comparison of each model one another.\n\n- Any contribution will be appriciated.","a00b2471":"It would be more sensible to fill the missing values in \"bmi\" column with the median by taking the effects of outliers on mean into consideration.","5a0021ea":"### User Defined Functions <a class=\"anchor\" id=\"userdefinedfunctions\"><\/a>","665f6a43":"- We have binary classification problem.\n\n- So we will apply one of the classification model which is Support Vector Classification (SVC) for the target variable of \"stroke\".\n\n- SVC is a nonparametric clustering algorithm that does not make any assumption on the number or shape of the clusters in the data. In general it works best for low-dimensional data.\n\nFor more information please refer to: [external link1](https:\/\/pythonprogramming.net\/linear-svc-example-scikit-learn-svm-python\/) and [external link2](http:\/\/www.scholarpedia.org\/article\/Support_vector_clusteringl)","f72acb91":"### 3.1 - Train | Test Split & Handling with Missing Values <a class=\"anchor\" id=\"tts\"><\/a> ","6871c2ba":"### work_type & stroke <a class=\"anchor\" id=\"work_type&stroke\"><\/a> ","7f01f8db":"**What can we learn from this matrix?**\n\nThere are two possible predicted classes: \"yes\" and \"no\". If we were predicting the presence of a disease, for example, \"yes\" would mean they have the disease, and \"no\" would mean they don't have the disease.\n\nThe classifier made a total of 1533 predictions (e.g., 1533 patients were being tested for the presence of that disease).\n\nOut of those 1533 cases, the classifier predicted \"yes\" 428 times, and \"no\" 1125 times.\n\nIn reality, 89 patients in the sample have the disease, and 1444 patients do not.\n\n\nPlease for a better understanding, refer to <a href=\"https:\/\/towardsdatascience.com\/understanding-confusion-matrix-a9ad42dcfd62\">HERE<\/a> and <a href=\"https:\/\/www.dataschool.io\/simple-guide-to-confusion-matrix-terminology\/\">HERE<\/a>","ba470221":"In the given dataset, we have no duplicated rows.","e5da322b":"### Have fun while... \n\n![AL1331618_Original_1800x900-1132x670.jpg](attachment:AL1331618_Original_1800x900-1132x670.jpg)","6a265c44":"## B - LIBRARIES NEEDED IN THE STUDY <a class=\"anchor\" id=\"libraries\"><\/a>","4abfdf77":"## C -  ANALYSIS <a class=\"anchor\" id=\"analysis\"><\/a>","937b6063":"Before deeping into the analysis it would be benefical to examine the correlation among variables using heatmap.","07e9e906":"**describe()** function gives us a general descriptive summary of each continuous attribute such as the count, mean, the min, max values and some percentiles as well.","4fa38434":"Some features have skewness. After visualizing them we will try to handle with their skewness by \"np.log\" and \"PowerTransformer\". Let's first take a close look at those in details.","6404f520":"### 3.2 - Scalling <a class=\"anchor\" id=\"scalling\"><\/a>  ","a2573856":"In \"id\" column, all the values are unique. Let's drop the \"id\" column.","d24b7e4d":"### Attribute Information <a class=\"anchor\" id=\"atributeinformation\"><\/a>","746a3da1":"Similarly, it's clear that the proportinate distribution for each class is not the case here. So we should assume an imbalanced data we have in the given case.","a5ecb840":"In the \"gender\" column there has been an undefined classification which makes no contribution to understand stroke. So let's discard this row from the analysis.","778afa87":"## 1) Reading the Dataset <a class=\"anchor\" id=\"readingthedataset\"><\/a>","83e24c89":"**1) id:** unique identifier\n\n**2) gender:** \"Male\", \"Female\" or \"Other\"\n\n**3) age:** age of the patient\n\n**4) hypertension:** 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n\n**5) heart_disease:** 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n\n**6) ever_married:** \"No\" or \"Yes\"\n\n**7) work_type:** \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n\n**8) Residence_type:** \"Rural\" or \"Urban\"\n\n**9) avg_glucose_level:** average glucose level in blood\n\n**10) bmi:** body mass index\n\n**11) smoking_status:** \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n\n**12) stroke:** 1 if the patient had a stroke or 0 if not\n\n**Note:** \"Unknown\" in smoking_status means that the information is unavailable for this patient\n\nAcknowledgements (Confidential Source) - Use only for educational purposes If you use this dataset in your research, please credit the author.","1ec75a8f":"### smoking_status & stroke <a class=\"anchor\" id=\"smoking_status&stroke\"><\/a>","b2c4d0e2":"### 2.3 - The Examination of Target Variable <a class=\"anchor\" id=\"targetvariable\"><\/a>","30e1a395":"### 3.3 - Modelling <a class=\"anchor\" id=\"modelling\"><\/a> ","3a293c63":"**StandardScaler:** Assumes that data has normally distributed features and will scale them to zero mean and 1 standard deviation. Use StandardScaler() if you know the data distribution is normal. For most cases StandardScaler would do no harm. Especially when dealing with variance (PCA, clustering, logistic regression, SVMs, perceptron's, neural networks) in fact Standard Scaler would be very important. On the other hand it will not make much of a difference if you are using tree based classifiers or regressors.\n\n**MinMaxScaler :** This will transform each value in the column proportionally within the range [0,1]. This is quite acceptable in cases where we are not concerned about the standardization along the variance axes. e.g. image processing or neural networks expecting values between 0 to 1.\n\nPlease for more information, refer to <a href=\"https:\/\/www.kaggle.com\/discdiver\/guide-to-scaling-and-standardizing\">Visit HERE<\/a>","84eba051":"With normalize set to True, we can obtain the relative frequencies by dividing all values by the sum of values.\n\nIn this sense, almost %95 of the instances in our target variable haven't experienced with 'stroke' representing 4861 patients. \n\nOn the other hand %5 of the instances in our target variable go through 'Stroke' representing 249 patient.","f33bee80":"The column of 'Residence_type' begins with uppercase while others are not. To make a standardize grammer to prevent mistake we will change all column names into lowercase","cf3838fe":"**SVM With Best Parameters (GridsearchCV)** <a class=\"anchor\" id=\"GridsearchCV\"><\/a> \n","718bac04":"Now it's time to get rid of these skewed features from our dataset and concatenate unskewed ones with the dataset. ","9cd38b02":"For more information on PowerTransformer please refer to [external link text](https:\/\/machinelearningmastery.com\/power-transforms-with-scikit-learn\/)","8a6f4dee":"### 2.2 - Data Cleaning <a class=\"anchor\" id=\"datacleaning\"><\/a>","58424a4a":"For more information on np.log please refer to [external link text](https:\/\/www.javatpoint.com\/numpy-log)","31df91ae":"Our dataset demonstrates;\n\n- 7 numeric variable including (4) int64 and (3) float64 data types out of 7.\n\n- 5 non-numeric variable including (5) object types out of 5.","7ee76e02":"### 2.6 - Handling with Skewness <a class=\"anchor\" id=\"handlinghskewness\"><\/a>","67e101d4":"In the given dataset, there have been 201 missing values in the column of \"bmi\". These missing values will be handled with after Train & Split process for preventing data leakage.","7a9cd2ad":"How to read and assign the dataset as df. [external link text](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_csv.html) (You can define it as what you want instead of df)","192a39df":"- In our dataset, we have both numerical and categorical variables.\n\n- It is critical to determine if the columns are correctly designed.\n\n- For the analysis, it is critical to determine our target (label) variable which is \"stroke\" in the given study.\n\n- It is critical to determine if stroke is an integer\/binary type or not.\n\n- In this dataset, target variable is coded as 1 for positive cases (having stroke) and 0 for negative cases (not having stroke).\n\n- Both Hypertension and heart disease have integer data types, not as an object.\n\n- Like our Target variable (stroke), both hypertension and heart_disease are coded as 1 for the positive cases and 0 for negative cases.\n\n- In addition, we have 5 categorical variables, which needs to be converted to dummies."}}