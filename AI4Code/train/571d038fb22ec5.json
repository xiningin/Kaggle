{"cell_type":{"bce5f077":"code","682729d3":"code","11b0d8ff":"code","b2b48e92":"code","1a507720":"code","25815e01":"code","ac1866a8":"code","301e920d":"code","c6823548":"code","1138de12":"code","fb36d99a":"code","75657173":"code","d6aefe18":"code","d1e9b669":"code","a2098232":"code","5177e63d":"code","0228e82e":"code","2453c9e1":"code","cebb010c":"code","b3446248":"code","2c0cae9e":"code","e70537ae":"code","b2044895":"code","fba2f30a":"code","ed0ac414":"code","5092f793":"code","7fe7709a":"code","c96e7caf":"code","5f461c78":"code","3702f10c":"code","9088346e":"code","942b2db9":"code","247e6421":"code","4050df66":"code","e360d758":"code","1410df69":"code","47219a2d":"code","fda79581":"code","8d3da132":"code","4e12a783":"code","8a25c238":"code","e4614769":"code","dfd41552":"code","892c905c":"code","987efad6":"code","304567c8":"code","d9830baa":"code","1a0f5fb9":"code","be6440b0":"code","d11b06c3":"code","a215d530":"code","21806de2":"code","8ef47ce6":"code","58d57b77":"code","88442060":"code","3b143afc":"code","47d411c1":"code","1b19e5c3":"code","06593369":"code","a6121089":"code","684525ab":"code","56dad2c3":"code","ad6fdef2":"code","58778481":"code","889ceea0":"code","8ea0c34d":"code","86a8f73d":"code","542af1e2":"code","e2194211":"code","d1254c3c":"code","d24e25ec":"code","dd5a8abf":"code","f0e6f49f":"code","c3bea8fa":"code","8b197d9c":"code","86b8d975":"code","488cd559":"code","c8752eeb":"code","8fd180e1":"code","8da20608":"code","330ce7b9":"code","347d688e":"code","b37482ce":"code","95f7076d":"code","45e9cd82":"code","a1ffd957":"code","8be16d15":"code","ff366fde":"code","244ae28d":"code","4cafcd2c":"code","b9d47ea3":"code","192a0926":"code","6710c462":"code","92e6a50c":"code","d2baf416":"code","403bdc20":"code","72c027dc":"code","30c4f913":"code","e999240c":"code","5109eecb":"markdown"},"source":{"bce5f077":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","682729d3":"# load packages\n\nimport sys              # access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nimport pandas as pd        # collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nimport matplotlib as mpl     # collection of fun for scientific and publication-ready visualization\nimport numpy as np                 # foundational package for scientific computing\nimport scipy as sp                 # collection of functions for scientific computing and advance mathematics\nimport IPython\nfrom IPython import display        # pretty printing of dataframes in Jupyter notebook\nimport sklearn                     # collection of machine learning algorithms)\nimport plotly.graph_objs as go\nfrom scipy.stats import norm,skew  # for some statistics\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom scipy import stats\nimport ast\nimport plotly.offline as py\npy.init_notebook_mode(connected=True) # for make plot as notebook editable\n\n\n#misc libraries\nimport random\nimport time\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('always')\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","11b0d8ff":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n","b2b48e92":"print(train_df.shape)\nprint(test_df.shape)","1a507720":"## memory reducer","25815e01":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df\n\ntrain_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","ac1866a8":"train_df.isna().sum()","301e920d":"# transforming dictionary columns to proper format( Nan to {})\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \ntrain_df = text_to_dict(train_df)\ntest_df = text_to_dict(test_df)","c6823548":"train_df.info()","1138de12":"# removing outliers \n\n# movies with budget and runtime 0\ntrain_df.drop(train_df[train_df['budget'] == 0].index, inplace=True)\ntrain_df.drop(train_df[train_df['runtime'] == 0].index, inplace=True)","fb36d99a":"## looking all movies with runlength > 3.5hrs\ntrain_df.loc[train_df['runtime'].fillna(0) \/ 60 > 3.5 ]","75657173":"## belongs_to_collection","d6aefe18":"for i,j in enumerate(train_df.belongs_to_collection[:6]):\n    print(i,j) ","d1e9b669":"a=b=0\nfor j in train_df.belongs_to_collection:\n    if j != {}: \n        a=a+1\n    else:\n        b=b+1\nprint(f\"len not 0 :{a}    len = 0 :{b} \")","a2098232":"train_df['collection_name'] = train_df['belongs_to_collection'].apply(lambda x: x[0]['name'] if x!= {} else 0)\ntrain_df['has_collection'] = train_df['belongs_to_collection'].apply(lambda x: len(x) if x!={} else 0)\n\ntest_df['collection_name'] = test_df['belongs_to_collection'].apply(lambda x: x[0]['name'] if x!= {} else 0)\ntest_df['has_collection'] = test_df['belongs_to_collection'].apply(lambda x: len(x) if x!={} else 0)\n\ntrain_df.drop('belongs_to_collection', axis =1, inplace= True)\ntest_df.drop('belongs_to_collection', axis =1, inplace= True)","5177e63d":"sns.swarmplot(x='has_collection', y='revenue', data=train_df);\nplt.title('Revenue for film with and without collection');","0228e82e":"# there are lot's of movies with doesn't have collection but earn large revenues","2453c9e1":"## genres","cebb010c":"for i,j in enumerate(train_df.genres[:10]):\n    print(f\"{i} {j}\")","b3446248":"## finding list of genres\nprint('Number of genres in films')\ntrain_df['genres'].apply(lambda x: len(x) if x != {} else 0).value_counts()","2c0cae9e":"list_of_genres = list(train_df['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)","e70537ae":"## finding top genres\nplt.figure(figsize = (12, 8))\ntext = ' '.join([i for j in list_of_genres for i in j])\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text)\n\nplt.imshow(wordcloud)\nplt.title('Top genres')\nplt.axis(\"off\")\nplt.show()","b2044895":"Counter([i for j in list_of_genres for i in j]).most_common()[:4]","fba2f30a":"## creating seperate column for top 15 genre\n\ntrain_df['num_genres'] = train_df['genres'].apply(lambda x: len(x) if x != {} else 0)\ntrain_df['all_genres'] = train_df['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_genres = [m[0] for m in Counter([i for j in list_of_genres for i in j]).most_common(15)]\nfor g in top_genres:\n    train_df['genre_' + g] = train_df['all_genres'].apply(lambda x: 1 if g in x else 0)\n    \ntest_df['num_genres'] = test_df['genres'].apply(lambda x: len(x) if x != {} else 0)\ntest_df['all_genres'] = test_df['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_genres:\n    test_df['genre_' + g] = test_df['all_genres'].apply(lambda x: 1 if g in x else 0)\n    \ntrain_df = train_df.drop(['genres'], axis=1)\ntest_df = test_df.drop(['genres'], axis=1)","ed0ac414":"## production_companies","5092f793":"for i,j in enumerate(train_df.production_companies[:5]):\n    print(i,j)","7fe7709a":"print('Number of production companies in films')\ntrain_df['production_companies'].apply(lambda x: len(x) if x != {} else 0).value_counts()[:5]","c96e7caf":"# lets look movies with more than 10 production companies\ntrain_df[train_df['production_companies'].apply(lambda x: len(x) if x != {} else 0) > 10].head()","5f461c78":"## finding list of companies\n\nlist_of_companies = list(train_df['production_companies'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nlist_of_companies[:5]","3702f10c":"## taking top 30 diffrent companies\nCounter([i for j in list_of_companies for i in j]).most_common(30)[:5]","9088346e":"## binary col for top 20 production companies\n\ntrain_df['num_companies'] = train_df['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntrain_df['all_production_companies'] = train_df['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_companies = [m[0] for m in Counter([i for j in list_of_companies for i in j]).most_common(30)]\nfor g in top_companies:\n    train_df['production_company_' + g] = train_df['all_production_companies'].apply(lambda x: 1 if g in x else 0)\n    \ntest_df['num_companies'] = test_df['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntest_df['all_production_companies'] = test_df['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_companies:\n    test_df['production_company_' + g] = test_df['all_production_companies'].apply(lambda x: 1 if g in x else 0)\n\n    \ntrain_df = train_df.drop(['production_companies', 'all_production_companies'], axis=1)\ntest_df = test_df.drop(['production_companies', 'all_production_companies'], axis=1)","942b2db9":"## production_countries","247e6421":"for i, e in enumerate(train_df['production_countries'][:5]):\n    print(i, e)","4050df66":"print('Number of production countries in films')\ntrain_df['production_countries'].apply(lambda x: len(x) if x != {} else 0).value_counts().head()","e360d758":"list_of_countries = list(train_df['production_countries'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n\nCounter([i for j in list_of_countries for i in j]).most_common(25)[:5]","1410df69":"train_df['num_countries'] = train_df['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntrain_df['all_countries'] = train_df['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_countries = [m[0] for m in Counter([i for j in list_of_countries for i in j]).most_common(25)]\nfor g in top_countries:\n    train_df['production_country_' + g] = train_df['all_countries'].apply(lambda x: 1 if g in x else 0)\n    \ntest_df['num_countries'] = test_df['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntest_df['all_countries'] = test_df['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_countries:\n    test_df['production_country_' + g] = test_df['all_countries'].apply(lambda x: 1 if g in x else 0)\n\n    \ntrain_df = train_df.drop(['production_countries', 'all_countries'], axis=1)\ntest_df = test_df.drop(['production_countries', 'all_countries'], axis=1)","47219a2d":"## spoken_languages\n\nfor i, e in enumerate(train_df['spoken_languages'][:5]):\n    print(i, e)","fda79581":"print('Number of spoken languages in films')\ntrain_df['spoken_languages'].apply(lambda x: len(x) if x != {} else 0).value_counts()","8d3da132":"list_of_languages = list(train_df['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_languages for i in j]).most_common(30)[:5]","4e12a783":"train_df['num_languages'] = train_df['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntrain_df['all_languages'] = train_df['spoken_languages'].apply(lambda x: ' '.join(sorted([i['iso_639_1'] for i in x])) if x != {} else '')\ntop_languages = [m[0] for m in Counter([i for j in list_of_languages for i in j]).most_common(30)]\nfor g in top_languages:\n    train_df['language_' + g] = train_df['all_languages'].apply(lambda x: 1 if g in x else 0)\n    \ntest_df['num_languages'] = test_df['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntest_df['all_languages'] = test_df['spoken_languages'].apply(lambda x: ' '.join(sorted([i['iso_639_1'] for i in x])) if x != {} else '')\nfor g in top_languages:\n    test_df['language_' + g] = test_df['all_languages'].apply(lambda x: 1 if g in x else 0)\n\n    \ntrain_df = train_df.drop(['spoken_languages', 'all_languages'], axis=1)\ntest_df = test_df.drop(['spoken_languages', 'all_languages'], axis=1)","8a25c238":"## keywords","e4614769":"for i, e in enumerate(train_df['Keywords'][:5]):\n    print(i, e)","dfd41552":"print('Number of Keywords in films')\ntrain_df['Keywords'].apply(lambda x: len(x) if x != {} else 0).value_counts().head(10)","892c905c":"list_of_keywords = list(train_df['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nplt.figure(figsize = (16, 12))\ntext = ' '.join(['_'.join(i.split(' ')) for j in list_of_keywords for i in j])\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text)\n\nplt.imshow(wordcloud)\nplt.title('Top keywords')\nplt.axis(\"off\")\nplt.show()","987efad6":"train_df['num_Keywords'] = train_df['Keywords'].apply(lambda x: len(x) if x != {} else 0)\ntrain_df['all_Keywords'] = train_df['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_keywords = [m[0] for m in Counter([i for j in list_of_keywords for i in j]).most_common(30)]\nfor g in top_keywords:\n    train_df['keyword_' + g] = train_df['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n    \ntest_df['num_Keywords'] = test_df['Keywords'].apply(lambda x: len(x) if x != {} else 0)\ntest_df['all_Keywords'] = test_df['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_keywords:\n    test_df['keyword_' + g] = test_df['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n\n    \ntrain_df = train_df.drop(['Keywords', 'all_Keywords'], axis=1)\ntest_df = test_df.drop(['Keywords', 'all_Keywords'], axis=1)","304567c8":"## cast","d9830baa":"for i, e in enumerate(train_df['cast'][:1]):\n    print(i, e)","1a0f5fb9":"print('Number of casted persons in films')\ntrain_df['cast'].apply(lambda x: len(x) if x != {} else 0).value_counts().head(10)","be6440b0":"## taking most common names...top 15\nlist_of_cast_names = list(train_df['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_cast_names for i in j]).most_common(15)[:5]","d11b06c3":"train_df['num_cast'] = train_df['cast'].apply(lambda x: len(x) if x != {} else 0)\ntop_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(15)]\nfor g in top_cast_names:\n    train_df['cast_name_' + g] = train_df['cast'].apply(lambda x: 1 if g in x else 0)\n    \ntest_df['num_cast'] = test_df['cast'].apply(lambda x: len(x) if x != {} else 0)\nfor g in top_cast_names:\n    test_df['cast_name_' + g] = test_df['cast'].apply(lambda x: 1 if g in x else 0)\n    \ntrain_df = train_df.drop(['cast'], axis=1)\ntest_df = test_df.drop(['cast'], axis=1)","a215d530":"## release date","21806de2":"test_df.loc[test_df['release_date'].isnull() == True, 'release_date'] = '01\/01\/98'","8ef47ce6":"def fix_date(x):\n    \"\"\"\n    Fixes dates which are in 20xx\n    \"\"\"\n    year = x.split('\/')[2]\n    if int(year) <= 19:\n        return x[:-2] + '20' + year\n    else:\n        return x[:-2] + '19' + year","58d57b77":"train_df['release_date'] = train_df['release_date'].apply(lambda x: fix_date(x))\ntest_df['release_date'] = test_df['release_date'].apply(lambda x: fix_date(x))\n\ntrain_df['release_date'] = pd.to_datetime(train_df['release_date'])\ntest_df['release_date'] = pd.to_datetime(test_df['release_date'])","88442060":"# creating features based on dates\ndef process_date(df):\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n    for part in date_parts:\n        part_col = 'release_date' + \"_\" + part\n        df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n    \n    return df\n\ntrain_df = process_date(train_df)\ntest_df = process_date(test_df)\n\ntrain_df.drop('release_date',axis=1, inplace=  True)\ntest_df.drop('release_date',axis=1, inplace=  True)","3b143afc":"d1 = train_df['release_date_year'].value_counts().sort_index()\nd2 = test_df['release_date_year'].value_counts().sort_index()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='train'), go.Scatter(x=d2.index, y=d2.values, name='test')]\nlayout = go.Layout(dict(title = \"Number of films per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","47d411c1":"d1 = train_df['release_date_year'].value_counts().sort_index()\nd2 = train_df.groupby(['release_date_year'])['revenue'].mean()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='mean revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and average revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Average revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))\n","1b19e5c3":"# The number of films and total revenue are growing, which is to be expected. But there were some years in the past with a high number of successful films, which brought high revenue.","06593369":"sns.swarmplot(x='release_date_weekday', y='revenue', data=train_df);\nplt.title('Revenue on different days of week of release');","a6121089":"## surprisingly films releases on Wednesdays and on Thursdays tend to have a higher revenue\n## also, there is large %age of movies released on Friday","684525ab":"sns.swarmplot(x='release_date_month', y='revenue', data=train_df);\nplt.title('Revenue on different days of week of release');","56dad2c3":"## jan and aug are not that great for movies as compared to other months","ad6fdef2":"## crew\n\ntrain_df.drop('crew', axis = 1, inplace= True)\ntest_df.drop('crew', axis =1, inplace = True)","58778481":"## released","889ceea0":"train_df.status.value_counts()","8ea0c34d":"## let's check those rumored movies\n\ntrain_df.loc[train_df.status == \"Rumored\"]","86a8f73d":"train_df.drop(['imdb_id','status'], axis = 1, inplace=True)\ntest_df.drop(['imdb_id','status'], axis = 1, inplace=True)","542af1e2":"## revenue","e2194211":"fig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(train_df['revenue']);\nplt.title('Distribution of revenue');\n\nplt.subplot(1, 2, 2)\nplt.hist(np.log1p(train_df['revenue']));\nplt.title('Distribution of log of revenue');","d1254c3c":"train_df['revenue'] = np.log1p(train_df['revenue'])","d24e25ec":"## homepage ","dd5a8abf":"train_df['has_homepage'] = 0\ntrain_df.loc[train_df['homepage'].isnull() == False, 'has_homepage'] = 1\ntest_df['has_homepage'] = 0\ntest_df.loc[test_df['homepage'].isnull() == False, 'has_homepage'] = 1","f0e6f49f":"sns.catplot(x='has_homepage', y='revenue', data=train_df);\nplt.title('Revenue for film with and without homepage');","c3bea8fa":"# films with homepage likely to generate more revenue\ntrain_df.has_homepage.value_counts()","8b197d9c":"train_df.drop('homepage', axis=1, inplace=True)\ntest_df.drop('homepage', axis=1, inplace=True)","86b8d975":"## poster_path","488cd559":"train_df['has_posterpath'] = 0\ntrain_df.loc[train_df['poster_path'].isnull() == False, 'has_posterpath'] = 1\ntest_df['has_posterpath'] = 0\ntest_df.loc[test_df['poster_path'].isnull() == False, 'has_posterpath'] = 1","c8752eeb":"sns.catplot(x='has_posterpath', y='revenue', data=train_df);\nplt.title('Revenue for film with and without poster');","8fd180e1":"train_df.has_posterpath.value_counts()\n## every one has poster path so we can drop col","8da20608":"train_df.drop(['poster_path','has_posterpath'], axis=1, inplace= True)\ntest_df.drop(['poster_path','has_posterpath'], axis=1, inplace= True)","330ce7b9":"## original_language","347d688e":"# analysing top 10 languages\nfig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='original_language', y='revenue',data=train_df.loc[train_df['original_language'].isin(train_df['original_language'].value_counts().head(10).index)])\nplt.title('Distribution of language');\n\nplt.subplot(1, 2, 2)\nsns.countplot(data=train_df.loc[train_df['original_language'].isin(train_df['original_language'].value_counts().head(10).index)], x='original_language')\nplt.title('Count of Language')","b37482ce":"## majority of the movie are in english with higher revenues but there are other languages too with higher revenues ","95f7076d":"train_df.original_language.value_counts()","45e9cd82":"## original_title","a1ffd957":"plt.figure(figsize = (12, 12))\ntext = ' '.join(train_df['original_title'].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in titles')\nplt.axis(\"off\")\nplt.show()","8be16d15":"# counting the title of movie having word \"Man\"\ntrain_df['original_title'].apply(lambda x: 1 if \"Man\" in x else 0).value_counts()\n# checking th title of movie having word \"Man\"\n# train_df['original_title'].apply(lambda x: print(x) if \"Man\" in x else 0)","ff366fde":"## overview","244ae28d":"plt.figure(figsize = (12, 12))\ntext = ' '.join(train_df['overview'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in overview')\nplt.axis(\"off\")\nplt.show()","4cafcd2c":"## popularity","b9d47ea3":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(x= train_df['popularity'],y= train_df['revenue'])\nplt.title('Revenue vs popularity');","192a0926":"## runtime\n\nplt.figure(figsize=(20, 6))\nplt.subplot(1, 3, 1)\nplt.hist(train_df['runtime'].fillna(0) \/ 60, bins=40);\nplt.title('Distribution of length of film in hours');\n\nplt.subplot(1, 3, 2)\nplt.scatter(train_df['runtime'].fillna(0), train_df['revenue'])\nplt.title('runtime vs revenue');\n\nplt.subplot(1, 3, 3)\nplt.scatter(train_df['runtime'].fillna(0), train_df['popularity'])\nplt.title('runtime vs popularity');","6710c462":"## genres\nf, axes = plt.subplots(3, 5, figsize=(30, 15))\nplt.suptitle('Violinplot of revenue vs genres')\nfor i, e in enumerate([col for col in train_df.columns if 'genre_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train_df, ax=axes[i \/\/ 5][i % 5]);","92e6a50c":"## basic modelling","d2baf416":"train_df.drop(['collection_name','all_genres'], axis=1 ,inplace= True)\ntest_df.drop(['collection_name','all_genres'], axis=1 ,inplace= True)","403bdc20":"for col in train_df.columns:\n    if train_df[col].nunique() == 1:\n        print(col)\n        train_df = train_df.drop([col], axis=1)\n        test_df = test_df.drop([col], axis=1)","72c027dc":"for col in ['original_language']:\n    le = LabelEncoder()\n    le.fit(list(train_df[col].fillna('')) + list(test_df[col].fillna('')))\n    train_df[col] = le.transform(train_df[col].fillna('').astype(str))\n    test_df[col] = le.transform(test_df[col].fillna('').astype(str))","30c4f913":"train_df.sample(3)","e999240c":"train_df.to_csv('train_cleaned.csv',index=False)\ntest_df.to_csv('test_cleaned.csv',index=False)","5109eecb":"So, Drama, Thriller and comedy are the top 3 genres"}}