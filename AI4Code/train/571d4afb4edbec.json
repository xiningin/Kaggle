{"cell_type":{"92bac58a":"code","0eed1d90":"code","1fe2d376":"code","e73c2514":"code","1b70d84f":"code","f4aedbd7":"code","821fe9ad":"code","87179a27":"code","8d0ef4b0":"code","fe3ca4af":"code","ee71ae44":"code","32647950":"code","ffb6ce32":"markdown","e3babf94":"markdown","3c9be456":"markdown","4d1bb4b4":"markdown","92a31d4a":"markdown","4c863953":"markdown","fae5a13e":"markdown","78a58f09":"markdown","ca26c925":"markdown","718ebe58":"markdown","03a3041a":"markdown"},"source":{"92bac58a":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport optuna         # for tunning\nimport xgboost as xgb ","0eed1d90":"train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\ny = train['target']\nfeatures = train.drop(['target'], axis=1)","1fe2d376":"cat_cols = [col for col in features.columns if 'cat' in col]\nnum_cols = [col for col in features.columns if 'cont' in col]\n\nX = features.copy()\nX_test = test.copy()\n\nordinal_encoder = OrdinalEncoder()\n\nX[cat_cols] = ordinal_encoder.fit_transform(features[cat_cols])\nX_test[cat_cols] = ordinal_encoder.transform(test[cat_cols])","e73c2514":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)\nprint(X_train.shape)\nprint(X_valid.shape)\nprint(y_train.shape)\nprint(y_valid.shape)","1b70d84f":"def objective(trial):\n    param = { \n        'verbosity': 1,\n        'n_estimators': trial.suggest_int('n_estimators', 7000,18000,step=250),\n        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n        'subsample': trial.suggest_categorical('subsample', [0.9,0.95,1.0]),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.45,0.5,0.55,0.6,0.65,0.7]),\n        'max_depth' : trial.suggest_int('max_depth', 1, 3, step=1),\n        'min_child_weight' : trial.suggest_int('min_child_weight', 2, 10),\n        'learning_rate': trial.suggest_float('learning_rate',0.10,0.45, step=0.05),  \n        'gamma' : trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n        'tree_method': 'gpu_hist'\n    }\n    \n    if param['booster'] == 'dart':\n        param['sample_type'] = trial.suggest_categorical('sample_type', ['uniform', 'weighted'])\n        param['normalize_type'] = trial.suggest_categorical('normalize_type', ['tree', 'forest'])\n        param['rate_drop'] = trial.suggest_float('rate_drop', 1e-8, 1.0, log=True)\n        param['skip_drop'] = trial.suggest_float('skip_drop', 1e-8, 1.0, log=True)\n        \n    xgb_model = xgb.XGBRegressor(**param)\n    xgb_model.fit(X_train, y_train, \n             early_stopping_rounds=10, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\n\n    xgb_preds = xgb_model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, xgb_preds, squared=False)\n    return rmse\n\n# Run tuning process (50 times in order to minimize RMSE)\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)","f4aedbd7":"print(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"Value: {}\".format(trial.value))\n\nprint(\"Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","821fe9ad":"study.trials_dataframe()","87179a27":"optuna.visualization.plot_optimization_history(study)","8d0ef4b0":"optuna.visualization.plot_param_importances(study)","fe3ca4af":"params = study.best_params \nparams['tree_method']='gpu_hist'\nparams","ee71ae44":"xgb_best_model = xgb.XGBRegressor(**params)\nxgb_best_model.fit(X_train, y_train, \n         early_stopping_rounds=10, \n         eval_set=[(X_valid, y_valid)], \n         verbose=False)\n\nxgb_preds = xgb_best_model.predict(X_valid)\nprint(mean_squared_error(y_valid, xgb_preds, squared=False))","32647950":"predictions = xgb_best_model.predict(X_test)\n\noutput = pd.DataFrame({'Id': X_test.index,'target': predictions})\noutput.to_csv('submission.csv', index=False)","ffb6ce32":"# Split Data","e3babf94":"# Train Model With Best Parameters","3c9be456":"## Visualize parameter importances","4d1bb4b4":"# Categorical Columns Encode","92a31d4a":"# Prepare Data","4c863953":"# Tune XGBoost","fae5a13e":"## RMSE in each trial","78a58f09":"## Print Optimization Results","ca26c925":" Set Search Space And Run XGBoost on it","718ebe58":"# Submit Results","03a3041a":"## Trials info"}}