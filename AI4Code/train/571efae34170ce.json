{"cell_type":{"9645f864":"code","204c3b9f":"code","557d3cf8":"code","fadc56be":"code","cbe30696":"code","5a57162e":"code","8a615ed4":"code","13840c29":"code","6523179a":"code","a5bb0e13":"code","81c757db":"code","7f52c8c7":"code","fc969c5b":"code","bfff0dbf":"code","f96a6ea1":"code","b3501090":"code","9e62d6d6":"code","033cc159":"code","cd120ca6":"code","09a63643":"code","0f67c63f":"code","6903b4bc":"code","0e7526c5":"code","b3396005":"code","8a8dbc0c":"code","86470c91":"code","c1c5b7cb":"code","bd7367a8":"code","b25c0cca":"code","8d1b77e9":"code","62d5e8cd":"code","a598917a":"code","6b25f0d6":"code","bd1c7bb9":"code","d2488d81":"code","45042623":"code","fe3043ec":"code","0f00f359":"code","ba90915e":"code","315003de":"code","1538ea60":"code","acc0a616":"code","5474d115":"markdown","5584b14a":"markdown","bbd1a598":"markdown","c8c7fc3c":"markdown","36b5d903":"markdown","868737ec":"markdown","8813d170":"markdown","93134be2":"markdown","26ef3eea":"markdown","4c4664f8":"markdown","0868bc33":"markdown","cbcbab41":"markdown","ea18e0a2":"markdown","14614146":"markdown","20eb7727":"markdown","1f8e1962":"markdown","e0f6f46d":"markdown","751421ed":"markdown"},"source":{"9645f864":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# Ignore warnings so that the notebook will be tidy\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Numerical, Statistical, and Visual Packages\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# sklearn ML Packages\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom mlxtend.plotting import plot_confusion_matrix\n\n# Tensorflow Packages\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential # Sequential is the easiest to learn\nfrom tensorflow.keras.layers import Dense # Dense Layers are common in neural networks (fully-connected)\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport keras_tuner as kt\n\n# k-Fold Cross-Validation\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom tensorflow.keras.layers import Dropout","204c3b9f":"# Supress scientific notations for a dataframe\npd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n\n# Define maximum number of columns to be displayed in a dataframe\npd.set_option(\"display.max_columns\", None)\n\n# Limit logging to informational messages\nnp.set_printoptions(threshold=np.inf)","557d3cf8":"titanic_train = pd.read_csv('..\/input\/d\/azeembootwala\/titanic\/train_data.csv')\ntitanic_test = pd.read_csv('..\/input\/d\/azeembootwala\/titanic\/test_data.csv')","fadc56be":"train = titanic_train.copy()\ntest = titanic_test.copy()","cbe30696":"train.head()","5a57162e":"train.shape","8a615ed4":"train.describe().T","13840c29":"train.isna().any()","6523179a":"train.nunique()","a5bb0e13":"# drop columns that will not add any value to the model and work with a copy of the dataset\ntrain = train.drop(['PassengerId', 'Unnamed: 0'], axis = 1)\ntest = test.drop(['PassengerId', 'Unnamed: 0'], axis = 1)\ntrain.head()","81c757db":"train.info()","7f52c8c7":"def pie_chart(data, dependent_variable, labels):\n  sizes = [data[dependent_variable][data[dependent_variable]==1].count(), data[dependent_variable][data[dependent_variable]==0].count()]\n  explode = (0, 0.1)\n  fig1, ax1 = plt.subplots(figsize=(7, 6))\n  ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n          shadow=True, startangle=90)\n  ax1.axis('equal')\n  title = 'Proportion of ' + labels[0] + ' to ' + labels[1]\n  plt.title(title, size = 15)\n  plt.show()","fc969c5b":"# function to plot a boxplot and a histogram along the same scale.\n\ndef histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n    \"\"\"\n    Boxplot and histogram combined\n\n    data: dataframe\n    feature: dataframe column\n    figsize: size of figure (default (12,7))\n    kde: whether to the show density curve (default False)\n    bins: number of bins for histogram (default None)\n    \"\"\"\n    f2, (ax_box2, ax_hist2) = plt.subplots(\n        nrows=2,  # Number of rows of the subplot grid= 2\n        sharex=True,  # x-axis will be shared among all subplots\n        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n        figsize=figsize,\n    )  # creating the 2 subplots\n    sns.boxplot(\n        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n    )  # boxplot will be created and a star will indicate the mean value of the column\n    sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n    ) if bins else sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2\n    )  # For histogram\n    ax_hist2.axvline(\n        data[feature].mean(), color=\"green\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax_hist2.axvline(\n        data[feature].median(), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram","bfff0dbf":"labels = ['Survived', 'Lost Souls']\npie_chart(train, 'Survived', labels)","f96a6ea1":"# The distribution of the customer's age\nhistogram_boxplot(train, \"Age\")","b3501090":"labels = ['Male', 'Female']\npie_chart(train, 'Sex', labels)","9e62d6d6":"# The distribution of the customer's age\nhistogram_boxplot(train, \"Fare\")","033cc159":"# The distribution of the customer's age\nhistogram_boxplot(train, \"Family_size\")","cd120ca6":"# function to plot stacked bar chart\n\n\ndef stacked_barplot(data, predictor, target):\n    \"\"\"\n    Print the category counts and plot a stacked bar chart\n\n    data: dataframe\n    predictor: independent variable\n    target: target variable\n    \"\"\"\n    count = data[predictor].nunique()\n    sorter = data[target].value_counts().index[-1]\n    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n        by=sorter, ascending=False\n    )\n    print(tab1)\n    print(\"-\" * 120)\n    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n        by=sorter, ascending=False\n    )\n    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 6))\n    plt.legend(\n        loc=\"lower left\", frameon=False,\n    )\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n    plt.show()","09a63643":"stacked_barplot(train, 'Sex', \"Survived\")","0f67c63f":"stacked_barplot(train, 'Fare', \"Survived\")","6903b4bc":"sns.set(rc={'figure.figsize':(16,10)})\nsns.heatmap(train.corr(),\n            annot=True,\n            linewidths=.5,\n            center=0,\n            cbar=False,\n            cmap=\"Spectral\")\nplt.show()","0e7526c5":"num_cols = train.select_dtypes(include=np.number).columns.tolist()\nsns.pairplot(data=train[num_cols], diag_kind=\"kde\", hue='Survived')\nplt.show()","b3396005":"print(train.shape, test.shape)\nX_train = train.drop(columns = 'Survived')\ny_train = train['Survived']\nX_test = test.drop(columns = 'Survived')\ny_test = test['Survived']","8a8dbc0c":"# Model 1\nmodel = Sequential()\nmodel.add(Dense(X_train.shape[1], activation='relu', input_dim = X_train.shape[1]))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))","86470c91":"model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])","c1c5b7cb":"history = model.fit(X_train, y_train.to_numpy(), validation_data = (X_test, y_test), batch_size = 20, epochs = 10, verbose = 1)","bd7367a8":"# Training and Validation Loss (loss vs epochs)\nplt.clf()\nplt.plot(history.history['loss'], label = 'Training Loss')\nplt.plot(history.history['val_loss'], label = 'Validation Loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","b25c0cca":"# Training and Validation Accuracy (accuracy vs epochs)\nplt.clf()\nplt.plot(history.history['accuracy'], label = 'Training Accuracy')\nplt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","8d1b77e9":"# Evaluate model performance\nmodel.evaluate(X_test, y_test.to_numpy())","62d5e8cd":"# Model 1 - predict the churn values\nypred_1 = model.predict(X_test)\n\n# unscale ypred\nypred_list_1 = []\nfor i in ypred_1:\n  if i > 0.5:\n    ypred_list_1.append(1)\n  else:\n    ypred_list_1.append(0)   \n\n# Model 1 - dataframe for comparing the orignal and predict values\ndata_1 = {'orignal_survivors':y_test, 'predicted_survivors':ypred_list_1}\ndf_check_1 = pd.DataFrame(data_1)\n\n# Model 1 - print classification_report\nprint(classification_report(y_test,ypred_list_1))\n\n# Model 1 - ploting the confusion metrix plot\nconf_mat = tf.math.confusion_matrix(labels=y_test,predictions=ypred_list_1)\nplt.figure(figsize = (17,7))\nsns.heatmap(conf_mat, annot=True,fmt='d')\nplt.xlabel('Predicted_number')\nplt.ylabel('True_number')","a598917a":"# Re-training the model with epochs = 50\nhistory_2 = model.fit(X_train, y_train.to_numpy(), validation_data = (X_test, y_test), batch_size = 200, epochs = 50, verbose = 1)","6b25f0d6":"# Training and Validation Accuracy (accuracy vs epochs)\nplt.clf()\nplt.plot(history_2.history['accuracy'], label = 'Training Accuracy')\nplt.plot(history_2.history['val_accuracy'], label = 'Validation Accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","bd1c7bb9":"# predict the survival values\nypred_2 = model.predict(X_test)\n\n# unscale ypred\nypred_list_2 = []\nfor i in ypred_2:\n  if i > 0.5:\n    ypred_list_2.append(1)\n  else:\n    ypred_list_2.append(0)\n\n# create a dataframe of orignal and predict values\ndata_2 = {'orignal_survived':y_test, 'predicted_survived':ypred_list_2}\ndf_check_2 = pd.DataFrame(data_2)\n\n# print the classification_report for model 2\nprint(classification_report(y_test, ypred_list_2))\n\n# plot the confusion metrix for model 2\nconf_mat = tf.math.confusion_matrix(labels = y_test, predictions = ypred_list_2)\nplt.figure(figsize = (17,7))\nsns.heatmap(conf_mat, annot=True,fmt='d')\nplt.xlabel('Predicted_number')\nplt.ylabel('True_number')","d2488d81":"# Function for building the model\ndef model_builder(hp):\n  model = keras.Sequential()\n  model.add(keras.layers.Flatten(input_shape=(X_train.shape[1], 1)))\n\n  # Tune the number of units in the first Dense layer\n  # Choose an optimal value between 1-512\n  hp_units = hp.Int('units', min_value=8, max_value=200, step=5)\n  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n  # Tune the learning rate for the optimizer\n  # Choose an optimal value from 0.01, 0.001, or 0.0001\n  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n\n  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n\n  return model\n\ntuner = kt.Hyperband(model_builder,\n                     objective='val_accuracy',\n                     max_epochs=100,\n                     factor=3,\n                     directory='google_drive',\n                     project_name='titanic_kt')\n\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)","45042623":"tuner.search(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete. The optimal number of units in the first densely-connected\nlayer is {best_hps.get('units')} and the optimal learning rate for the optimizer\nis {best_hps.get('learning_rate')}.\n\"\"\")","fe3043ec":"# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\nmodel_7 = tuner.hypermodel.build(best_hps)\nhistory_7 = model.fit(X_train, y_train, epochs=100, validation_split=0.2)\n\nval_acc_per_epoch = history_7.history['val_accuracy']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))","0f00f359":"hypermodel = tuner.hypermodel.build(best_hps)\n\n# Retrain the model\nhistory_8 = hypermodel.fit(X_train, y_train, epochs=best_epoch, validation_split=0.2)","ba90915e":"eval_result = hypermodel.evaluate(X_test, y_test)\nprint(\"[test loss, test accuracy]:\", eval_result)","315003de":"# Hypermodel 1 - Training and Validation Loss (loss vs epochs)\nplt.clf()\nplt.plot(history_8.history['loss'], label = 'Training Loss')\nplt.plot(history_8.history['val_loss'], label = 'Validation Loss')\nplt.title('Hypermodel 1 - Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","1538ea60":"# Hypermodel 1 - Training and Validation Accuracy (accuracy vs epochs)\nplt.clf()\nplt.plot(history_8.history['accuracy'], label = 'Training Accuracy')\nplt.plot(history_8.history['val_accuracy'], label = 'Validation Accuracy')\nplt.title('Hypermodel 1 - Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","acc0a616":"X_test.head()","5474d115":"306 of the 792 passengers survived. Of the 306 survivors, 209 were female. Only 70 of the 486 males survived the disaster. ","5584b14a":"Here we have 88% accuracy","bbd1a598":"The Titanic had 792 passengers on board on her last voyage. These passengers are represented in the dataset with seventeen characteristics.","c8c7fc3c":"The data types are already correct - and the data has been standardized beforehand","36b5d903":"Passengers paying the lowest fares all perished, whilst passengers paying the highest fares all survived.","868737ec":"the model predicted 88 passengers correctly and only 12 incorrectly. The model is 97% accurate at predicting survivors","8813d170":"We are left with 14 features and the dependent variable","93134be2":"The model performs slightly better on the validation data","26ef3eea":"We have 17 columns of data about Titanic passengers:\nUnnamed: A row number\nPassengerId: Identifies the sample\nSurvived: The dependent variable - we want to predict whether or not a passenger would survive based on the data\nAge: The standardized Age of the passenger\nFare: The standardized fare\nPassenger Class: 1, 2, 3\nFamily Size: standardized z-scores\nTitle: 1 - 4\nEmbarked: 1 - 3 locations\n","4c4664f8":"The model can predict survivors with an accuracy of 87% - but we can train it for more epochs and analyze performance","0868bc33":"Most passengers were single","cbcbab41":"We see that 38% of the passengers survived the Titanic disaster. 64% of the passengers were male, so it is likely only Women and children survived. 24% first class, 20% second, and 54% third class - so the majority of passengers were traveling third-class. 72% of passengers embarked from the same location.","ea18e0a2":"There's a big difference in fare on the Titanic. Wealthier travellers are less common and pay a lot more","14614146":"Roughly the same percentage of females survived","20eb7727":"There are no missing values","1f8e1962":"We see that Survived, Age, and the one hot encoded fields are all binary classifications\nWe have 792 unique passengers - no duplicates\nThere's a wide range of Ages\nThere's a wide range of fares","e0f6f46d":"We see that 61.4% of passengers were lost on the fated Titanic voyage","751421ed":"We see from the distribution that the average age of passengers was around 28 and 65% of passengers were aged 21 - 33\nWe also have some outliers - babies and seniors - but not many."}}