{"cell_type":{"3f753d4b":"code","53bf5183":"code","7d6df402":"code","9e83094e":"code","b9997528":"code","ca8d41a3":"code","a9b029f6":"code","26cade11":"code","32620b25":"code","72ca3bc5":"code","fa47829b":"code","358f312c":"code","2d673626":"code","dbe696c7":"code","9629487b":"code","3f66f218":"code","cdf891ed":"code","c374d85e":"code","d65ac459":"code","be00d535":"code","f487ad2b":"code","3347253f":"code","8c1a2dc3":"code","9477acbe":"code","6f008a7b":"code","c5c88773":"code","a82a4294":"code","78c218d3":"code","c5c156d7":"code","ad5306ae":"code","03b4a4ca":"code","522c0801":"code","ba9db66b":"code","1584791f":"code","a324e08d":"code","ec337aaa":"code","2888885d":"code","b6f7eee4":"code","02467180":"code","4986b60e":"code","c3028e30":"code","b6fda06c":"code","b59110ba":"code","a63a3296":"code","270ddc12":"code","23d9571e":"code","a8eacde6":"code","93a8fbbc":"code","ebc54fda":"code","478ec09d":"code","df5414d3":"code","df0b1db9":"code","dd3f8da1":"code","42d4dec8":"code","17302ef3":"code","30ae352c":"code","ce10f7ec":"code","f67aac62":"code","0eb6f48d":"code","a6cbb0dc":"code","bda69cb3":"markdown","e720b6c6":"markdown","519d8ea9":"markdown","2feb500c":"markdown","8e86f628":"markdown","72915264":"markdown","986250b6":"markdown","c5f06886":"markdown","c04b2577":"markdown"},"source":{"3f753d4b":"# importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline","53bf5183":"# importing data\ndata = pd.read_csv('..\/input\/fifa19\/data.csv')","7d6df402":"data.head()","9e83094e":"data.columns","b9997528":"data.info()","ca8d41a3":"# assigning dataframe to 'df' and droping unnecessary columns \ndf = data.drop(columns=['Unnamed: 0', 'ID', 'Photo', 'Flag', \n                     'Club Logo', 'Real Face', 'Jersey Number', \n                     'Loaned From', 'Contract Valid Until', 'Release Clause'], axis=1)","a9b029f6":"df.head()","26cade11":"df.columns","32620b25":"# looking how distributed are null values\nplt.figure(figsize=(12,8))\nsns.heatmap(df[['Overall', 'Crossing',\n       'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n       'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n       'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n       'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n       'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n       'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n       'GKKicking', 'GKPositioning', 'GKReflexes']].isnull(), cbar=False, cmap='viridis', yticklabels=False)\nplt.show()","72ca3bc5":"# assign 'df' to name 'properties' and droping null values\nproperties = df[['Overall', 'Crossing',\n       'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n       'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n       'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n       'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n       'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n       'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n       'GKKicking', 'GKPositioning', 'GKReflexes']]\nproperties.dropna(inplace=True)\nproperties['Crossing'].isnull().sum()","fa47829b":"plt.figure(figsize=(12,8))\nsns.heatmap(properties[['Overall', 'Crossing',\n       'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n       'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n       'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n       'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n       'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n       'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n       'GKKicking', 'GKPositioning', 'GKReflexes']].isnull(), cbar=False, cmap='viridis', yticklabels=False)\nplt.show()","358f312c":"properties.describe()","2d673626":"# when I have clean dataframe with all important properties of a player I can start investigate \n# which of them is most important for overall performance.","dbe696c7":"# from correlation matrix taking just overall performance column and sort it\nproperties.corr()['Overall'].sort_values(ascending=False)","9629487b":"# as we can see in the graph below there are plenty of properties that correlate. \n# In order to avoid collinearity we have to exclude one from correlating pair \n# (except when high (>.8) correlation appears with target feature this case 'Overall')\nplt.figure(figsize=(12,8))\nsns.heatmap(properties.corr(), cmap='viridis')\nplt.show()","3f66f218":"# after excluding collinear properties I got these features\n# for those of whom this and previous steps are confusing I suggest reading about collinearity \nregModel = properties[['Overall', 'Strength', 'Stamina', 'Jumping', 'Composure', 'Reactions', 'ShortPassing', 'GKKicking']]","cdf891ed":"regModel.corr()[\"Overall\"].sort_values(ascending=False).head(12)\n\n# here I explain features:\n# Reactions: measures how quickly a player responds to a situation happening around him. \n# It has nothing to do with the player\u2019s speed.\n\n\n# Composure: this attribute determines at what distance the player \n# with the ball starts feeling the pressure from the opponent. \n# This then affects the chances of the player making an error when he shoots, \n# passes and crosses.","c374d85e":"# last time checking to avoid collinearity\nplt.figure(figsize=(8,6))\nsns.heatmap(regModel.corr(), cmap='viridis', annot=True)\nplt.show()","d65ac459":"# assign x and y\nX = regModel[['Strength', 'Stamina', 'Jumping', 'Composure', 'Reactions', 'ShortPassing', 'GKKicking']]\ny = regModel['Overall']","be00d535":"from sklearn.model_selection import train_test_split","f487ad2b":"# spliting variables into train and test, setting test_size and random state\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","3347253f":"from sklearn.linear_model import LinearRegression","8c1a2dc3":"# instantiate LinearRegression model and assign it to 'lm'\nlm = LinearRegression()","9477acbe":"# fitting my training data to the model\nlm.fit(X_train, y_train)","6f008a7b":"# here I'm creating dataframe from my model coefficients\ncoefs = pd.DataFrame(lm.coef_, X_train.columns, columns=[\"Coefficients\"])","c5c88773":"print(f'rSquared: {round(lm.score(X_train, y_train), 3)}')\ncoefs\n# rSquared is a metric which describe how good is your model.\n# coefficients interpretation: if you hold other features fixed and increase 'Reactions'\n# in one unit you get increase in 'Overall' by 0.379","a82a4294":"# in order to get predicions I input 'X_test' values to the model's predict method \n# and assign to the variable 'predictions'\npredictions = lm.predict(X_test)","78c218d3":"# plot actual vs. predicted values\nplt.rcParams.update({'font.size': 12})\nplt.title('Actual vs. Predicted')\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.scatter(y_test, predictions)\nplt.show()","c5c156d7":"from sklearn import metrics","ad5306ae":"# here I use a metric (Root Mean Squared Error) which represents difference between real and predicted values.\n# this difference is expressed in the same units as predicted value (in this case 'Overall')\n# other way to test your model is to plot residuals distribution. If it visually seems normally distributed and mean around 0 \n# it indicates that your model is the right decision for this data\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\nprint(f\"Root Mean Squared Error: {round(rmse, 3)}\")\nplt.title('Residuals')\nsns.distplot((y_test-predictions),bins=50)\nplt.show()","03b4a4ca":"# creating separate dataframe with \"Position\" column\nfeatures = df[['Crossing',\n       'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n       'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n       'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n       'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n       'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n       'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n       'GKKicking', 'GKPositioning', 'GKReflexes', 'Position']]","522c0801":"# cheching for null values\nfeatures.isnull().sum()","ba9db66b":"# droping null values\nfeatures.dropna(inplace=True)","1584791f":"features.head()","a324e08d":"# checking how many unique positions I have\nfeatures.Position.nunique()","ec337aaa":"# function which changes position from goolkeeper to 1, defender to 2, midfielder to 3, striker to 4.\ndef simplePosition(col):\n    if (col == 'GK'):\n        return 1\n    elif ((col == 'RB') | (col == 'LB') | (col == 'CB') | (col == 'LCB') | (col == 'RCB') | (col == 'RWB') | (col == 'LWB') ):\n        return 2\n    elif ((col == 'LDM') | (col == 'CDM') | (col == 'RDM') | (col == 'LM') | (col == 'LCM') | \n          (col == 'CM') | (col == 'RCM') | (col == 'RM') | (col == 'LAM') | (col == 'CAM') | \n          (col == 'RAM') | (col == 'LW') | (col == 'RW')):\n        return 3\n    elif ((col == 'RS') | (col == 'ST') | (col == 'LS') | (col == 'CF') | (col == 'LF') | (col == 'RF')):\n        return 4\n    else:\n        return 'error'","2888885d":"# applying that funcion to position column\nfeatures[\"Position\"] = features.Position.apply(simplePosition)","b6f7eee4":"features.Position.unique()","02467180":"features.head()","4986b60e":"from sklearn.preprocessing import StandardScaler","c3028e30":"# instantiating StandardScaler object\nscaler = StandardScaler()","b6fda06c":"# fitting data to the scaler object except position column\nscaler.fit(features.drop('Position', axis=1))","b59110ba":"# perform actual scaling\nscaled_fetures = scaler.transform(features.drop('Position', axis=1))","a63a3296":"# and now we have dataframe with scaled features\ndf_features = pd.DataFrame(scaled_fetures, columns=features.columns[:-1])","270ddc12":"from sklearn.model_selection import train_test_split","23d9571e":"# assign scaled features dataframe and position column to the varibles.\nfeat = df_features\ntarg = features.Position\n# spliting data into train and test\nxTrain, xTest, yTrain, yTest = train_test_split(feat, targ, test_size=0.2, random_state=42)","a8eacde6":"from sklearn.neighbors import KNeighborsClassifier","93a8fbbc":"knn = KNeighborsClassifier(n_neighbors=1)","ebc54fda":"knn.fit(xTrain, yTrain)","478ec09d":"pred = knn.predict(xTest)","df5414d3":"from sklearn.metrics import classification_report, confusion_matrix","df0b1db9":"# confusion matrix and classification report explains how good our classification algorith performs\nprint(confusion_matrix(yTest, pred))\nprint('\\n')\nprint(classification_report(yTest, pred))","dd3f8da1":"confMatrix = confusion_matrix(yTest, pred)","42d4dec8":"# this function plots good looking confusion matrix, accuracy and error rates. \ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","17302ef3":"# here is same, but a bit better looking confusion matrix \nplot_confusion_matrix(cm = confMatrix, normalize = False, \n                      target_names = ['Goolkeeper', 'Defender', 'Midfielder', 'Striker'],\n                      title= \"Confusion Matrix K=1\")","30ae352c":"# here I am looping through same classification algorithm with different n_neighbors values (it takes some time)\nerror_rate = []\nfor i in range(1,30):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(xTrain, yTrain)\n    pred_i = knn.predict(xTest)\n    error_rate.append(np.mean(pred_i != yTest))","ce10f7ec":"# error rate for different number neighbors (K)\n# as we can see around 8 or 9 neighbors error rate reach plateau\nplt.plot(range(1,30),error_rate,color='blue', linestyle='--', marker='o', markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","f67aac62":"# rerun classification algorith with n_neighbors where error rate was smallest\n# it is always better to choose odd number of neighbors\nKnn = KNeighborsClassifier(n_neighbors=9)\nKnn.fit(xTrain, yTrain)\npred_9 = Knn.predict(xTest)\nprint(confusion_matrix(yTest, pred_9))\nprint('\\n')\nprint(classification_report(yTest, pred_9))","0eb6f48d":"conf9Matrix = confusion_matrix(yTest, pred_9)","a6cbb0dc":"# model prediction improved from ~86% to ~90%\nplot_confusion_matrix(cm = conf9Matrix, normalize = False, \n                      target_names = ['Goolkeeper', 'Defender', 'Midfielder', 'Striker'],\n                      title = \"Confusion Matrix with K=9\")","bda69cb3":"#  Fifa 2019 Analysis\n## Main questions:\n### 1. What physical or technical properties of a player increase the overall performance most?\n### 2. How accurately can we tell the position of the player just knowing physical and technical features of a player?","e720b6c6":"## Multiple Linear Regression","519d8ea9":"## Model Evaluation","2feb500c":"# 1. What physical or technical properties of a player increase the overall performance most?","8e86f628":"## Model Evaluation","72915264":"## Model Predicions","986250b6":"# 2. How accurately can we tell the position of the player just knowing physical and technical features of a player?","c5f06886":"# Conclusion\n## 1. Player reactions and composure increase overall performance most\n## 2. With ~90% accuracy we can predict players position just knowing physical and technical features. ","c04b2577":"# KNN Classification"}}