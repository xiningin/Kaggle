{"cell_type":{"851d2226":"code","6c0a576d":"code","bb597d7e":"code","df9e8216":"code","72dcad02":"code","784ae368":"code","77c6481d":"code","8fa74dda":"code","2e4d32e2":"code","8968479d":"code","abd03e89":"code","1f970eb8":"code","43d5f6d6":"code","53c14540":"code","87e17bfd":"code","e7f52a59":"code","8f4a885f":"code","6e90c6bd":"code","0e2ba033":"code","6bf8b70f":"code","09fe3161":"code","f9e8fd7b":"code","e4ef732e":"code","1ec70a38":"code","e5b3c67c":"code","2362bef8":"code","a698ea04":"code","92d86a33":"code","7a71439e":"code","9cf466f3":"code","5e883aed":"code","d99e6cd4":"code","d0cf676c":"code","a06000ec":"code","4dcef2b6":"code","d2a4e29d":"code","25b9786a":"code","23aebfdf":"code","bbdfe94c":"code","cf0d2a37":"code","9aa71a50":"code","fc0ad3c2":"code","65427ee0":"code","1afc533d":"code","353bc168":"code","200928da":"code","3e162099":"code","1be22f8c":"markdown","ee586f24":"markdown","7e78c255":"markdown","4fc870e0":"markdown","250ae27e":"markdown"},"source":{"851d2226":"# Project name used for jovian.commit\nproject_name = 'music-classification-using-deep-learning-with-pytorch'","6c0a576d":"# Uncomment and run the commands below if imports fail\n# !conda install numpy pytorch torchaudio cpuonly -c pytorch -y\n# !pip install matplotlib --upgrade --quiet\n# !conda install -c conda-forge librosa\n!pip install jovian --upgrade --quiet","bb597d7e":"# !mkdir genres && wget http:\/\/opihi.cs.uvic.ca\/sound\/genres.tar.gz  && tar -xf genres.tar.gz genres\/","df9e8216":"# !rmdir genres\n# !rm genres.tar.gz\n# !rm -rf img_data","72dcad02":"import jovian\nimport os\nimport pathlib\nimport random\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport librosa.display\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch\nimport torchaudio\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.transforms import ToTensor,transforms\nfrom torchvision.utils import make_grid\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom tqdm.autonotebook import tqdm\nfrom skimage.io import imread, imsave\n%matplotlib inline","784ae368":"data_path = '..\/input\/genres-data-for-music-classification\/genres'\nimg_path = 'img_data'","77c6481d":"cmap = plt.get_cmap('inferno') # this is for img color\nplt.figure(figsize=(8,8)) # img size\ngenres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split() # all possible  music class\nfor g in genres:\n    pathlib.Path(f'{img_path}\/{g}').mkdir(parents=True, exist_ok=True)\n    for filename in os.listdir(f'{data_path}\/{g}'):\n        songname = f'{data_path}\/{g}\/{filename}'\n#         print(songname)\n#         break\n        y, sr = librosa.load(songname, mono=True, duration=5)\n        plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB');\n        plt.axis('off');\n        plt.savefig(f'{img_path}\/{g}\/{filename[:-3].replace(\".\", \"\")}.png')\n        plt.clf()","8fa74dda":"audio_data = data_path+'\/classical\/classical.00009.wav'\nx , sr = librosa.load(audio_data)\nprint(type(x), type(sr))\n#<class 'numpy.ndarray'> <class 'int'>print(x.shape, sr)#(94316,) 22050","2e4d32e2":"plt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sr)","8968479d":"import IPython.display as ipd\nipd.Audio(audio_data)","abd03e89":"import matplotlib.image as mpimg\nimg=mpimg.imread(img_path+'\/blues\/blues00093.png')\nimgplot = plt.imshow(img)\nplt.show()\nprint('shape of image is:',img.shape)","1f970eb8":"#parameters\nbatch_size = 32\nim_size = 576","43d5f6d6":"def normalization_parameter(dataloader):\n    mean = 0.\n    std = 0.\n    nb_samples = len(dataloader.dataset)\n    for data,_ in tqdm(dataloader):\n        batch_samples = data.size(0)\n        data = data.view(batch_samples, data.size(1), -1)\n        mean += data.mean(2).sum(0)\n        std += data.std(2).sum(0)\n    mean \/= nb_samples\n    std \/= nb_samples\n    return mean.numpy(),std.numpy()\n\ntrain_transforms = transforms.Compose([transforms.Resize((im_size,im_size)),transforms.ToTensor()])\ntrain_data = torchvision.datasets.ImageFolder(root = img_path, transform = train_transforms)\ntrain_loader =  DataLoader(train_data, batch_size = batch_size , shuffle = True)\nmean,std = normalization_parameter(train_loader)","53c14540":"\ntrain_transforms = transforms.Compose([transforms.Resize((im_size,im_size)),\n                                        transforms.RandomResizedCrop(size=315, scale=(0.95, 1.0)),\n                                        transforms.RandomRotation(degrees=10),\n                                        transforms.RandomHorizontalFlip(),\n                                        transforms.CenterCrop(size=299),  # Image net standards\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean,std)])\ntest_transforms = transforms.Compose([\n                                        transforms.Resize((im_size,im_size)),\n                                        transforms.CenterCrop(size=299),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean,std)\n])","87e17bfd":"#data loader\ndataset = torchvision.datasets.ImageFolder(root = img_path, transform = train_transforms)\ntest_dataset = torchvision.datasets.ImageFolder(root = img_path, transform =  test_transforms)","e7f52a59":"#encoder and decoder to convert classes into integer\ndef encoder(data):\n    #label of classes\n    classes = data.classes\n    encoder = {}\n    for i in range(len(classes)):\n        encoder[i] = classes[i]\n    return encoder\n\ndef decoder(data):\n    #label of classes\n    classes = data.classes\n    \n    decoder = {}\n    for i in range(len(classes)):\n        decoder[classes[i]] = i\n    return decoder","8f4a885f":"#plotting rondom images from dataset\ndef class_plot(data,n_figures = 4):\n    n_row = int(n_figures\/4)\n    fig,axes = plt.subplots(figsize=(24, 20), nrows = n_row, ncols=4)\n    for ax in axes.flatten():\n        a = random.randint(0,len(data))\n        (image,label) = data[a]\n#         print(type(image))\n        label = int(label)\n        encoders = encoder(data)\n        l = encoders[label]\n        image = image.numpy().transpose(1,2,0)\n        im = ax.imshow(image)\n        ax.set_title(l)\n        ax.axis('off')\n    plt.show()\n","6e90c6bd":"class_plot(dataset)","0e2ba033":"class_plot(train_data)","6bf8b70f":"torch.manual_seed(43)\nval_size = int(len(dataset)*0.2)\ntrain_size = len(dataset) - val_size","09fe3161":"from torch.utils.data import random_split\ntrain_ds, val_ds = random_split(dataset, [train_size,val_size])\nlen(train_ds), len(val_ds)","f9e8fd7b":"train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size*2, num_workers=4, pin_memory=True)","e4ef732e":"for images, _ in train_loader:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n    break","1ec70a38":"def accuracy(outputs,labels):\n    _,preds =torch.max(outputs,dim=1)\n    return torch.tensor(torch.sum(preds==labels).item()\/len(preds))","e5b3c67c":"arch = \"5 layer ( 3*299*299,1024,512,128,32,10)\"","2362bef8":"class ClassifyMusic(nn.Module):\n    def __init__(self,input_size,output_size):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size,1024)\n        self.linear2 = nn.Linear(1024,512)\n        self.linear3 = nn.Linear(512,128)\n        self.linear4 = nn.Linear(128,32)\n        self.linear5 = nn.Linear(32,output_size)\n    def forward(self, xb):\n        out = xb.view(xb.size(0), -1)\n        out = self.linear1(out)\n        out = F.relu(out)\n        out = self.linear2(out)\n        out = F.relu(out)\n        out = self.linear3(out)\n        out = F.relu(out)\n        out = self.linear4(out)\n        out = F.relu(out)\n        out = self.linear5(out)\n        return out\n    def training_step(self,batch):\n        image,labels =batch\n        out = self(image)\n        loss =F.cross_entropy(out,labels)\n        return loss\n   \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n\n        \n","a698ea04":"def evaluate(model,val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs,lr,model,train_loader,val_loader,opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(),lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","92d86a33":"def plot_losses(history):\n    losses = [x['val_loss'] for x in history]\n    plt.plot(losses, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title('Loss vs. No. of epochs');\ndef plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","7a71439e":"torch.cuda.is_available()","9cf466f3":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","5e883aed":"device = get_default_device()\ndevice","d99e6cd4":"def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","d0cf676c":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)\ntest_loader = DeviceDataLoader(test_loader, device)","a06000ec":"input_size = 3*299*299\noutput_size = 10\ninput_size","4dcef2b6":"model = to_device(ClassifyMusic(input_size,output_size), device)","d2a4e29d":"history = [evaluate(model, val_loader)]\nhistory","25b9786a":"# epochs = {0.1:50,0.01:50,0.001:50,0.0001:50}\nepochs = {1e-2:50,1e-2:50,1e-3:50,1e-5:50,1e-6:50}","23aebfdf":"for lr,epoch in epochs.items():\n    print(f'epoch:{epoch},lr:{lr}')\n    history += fit(epoch,lr, model, train_loader, val_loader)","bbdfe94c":"plot_losses(history)","cf0d2a37":"plot_accuracies(history)","9aa71a50":"test = evaluate(model, test_loader)","fc0ad3c2":"test_acc = test['val_acc']\ntest_loss = test['val_loss']\ntest_loss,test_acc","65427ee0":"torch.save(model.state_dict(), project_name+'.pth')","1afc533d":"# Clear previously recorded hyperparams & metrics\njovian.reset()","353bc168":"jovian.log_hyperparams(arch=arch,lrs=list(epochs.keys()),epochs=list(epochs.values()))","200928da":"jovian.log_metrics(test_loss=test['val_loss'], test_acc=test['val_acc'])","3e162099":"jovian.commit(project=project_name,output=[project_name+'.pth'], environment=None)","1be22f8c":"## Download Music Data\nDownload data from data scource and unzip tar file into genres folder \n\n!mkdir: for creating  directoty\n\nwget url : data source url\n\ntar -xvf tag_file_name -d extracted_dir\/  : this command for extract tar zip ","ee586f24":"befor augmentation ","7e78c255":"one batch","4fc870e0":"## Create Data","250ae27e":"after augmentation"}}