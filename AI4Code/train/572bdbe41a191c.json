{"cell_type":{"4292b8fa":"code","a9d3c2db":"code","1d840926":"code","99338f51":"code","ba5df7f5":"code","cb4ccece":"code","4fb75b4b":"code","c6fe11f1":"code","73c998ed":"code","b95aa14d":"code","dc6e8b3d":"code","b4c7a095":"code","349eb2a5":"code","9b703116":"code","7afe125f":"code","09ca51af":"code","2673e424":"code","fded7449":"code","5488160b":"code","15a6fa60":"code","716c83fa":"code","c7a46037":"code","98bc34a2":"code","4a9500f7":"code","e98eb0bb":"code","49b4cf02":"code","3930bda9":"code","6cb608c0":"code","9efc9976":"code","89e29dd2":"code","66c267c7":"code","af2d20f8":"code","db2c46be":"code","24532838":"code","4612d95b":"code","92c1fd78":"code","8ce4594c":"code","0f61296c":"code","76b0f1e8":"code","c8fb6098":"code","ec4629b0":"code","f6d704ba":"code","e327ca49":"code","517acd3b":"code","f2938c6b":"code","ab602caa":"code","eaa583d8":"code","77255989":"code","c1d96166":"code","7877360b":"code","a94813a8":"code","e0fdbf28":"code","216fe6f3":"code","d94a7379":"code","7a3dcb1e":"code","6bcd36ed":"code","df78f241":"code","737cbf43":"code","2384fbcd":"code","476df086":"code","28a23bc5":"markdown","6b30cb2b":"markdown","1f3b0b88":"markdown","559e55cb":"markdown","4b19800c":"markdown","fdb5cd16":"markdown","cee0c486":"markdown","a6bfbdc2":"markdown","797048e7":"markdown","b67c5aa7":"markdown","51922c70":"markdown","02c3aed6":"markdown","d7b9e0e9":"markdown","d6706874":"markdown","96573154":"markdown","5d593694":"markdown","e469a2c2":"markdown","b06c5d0e":"markdown","8e9bd5f4":"markdown","c566bfba":"markdown","41d4fd5b":"markdown","9dfeb9a6":"markdown","ed81ec3b":"markdown","bd535f8e":"markdown","160b3852":"markdown","c202a867":"markdown","61dbdc76":"markdown","d7cfffac":"markdown","173453e0":"markdown","4afe91b5":"markdown","2fec0ea1":"markdown","bf6d0da5":"markdown","9c8a71df":"markdown","0504f65d":"markdown","e9820242":"markdown","e9bbaa2f":"markdown","d2bb3b0f":"markdown","238aab5e":"markdown","f25af9be":"markdown","69e4fba4":"markdown","20d75952":"markdown","da29537d":"markdown","64c1bef7":"markdown","dedef14f":"markdown","8f9e39a7":"markdown","a3e7c6c1":"markdown","7aec2024":"markdown","483d79c1":"markdown","e6557bcc":"markdown"},"source":{"4292b8fa":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","a9d3c2db":"df = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')","1d840926":"# Look some row of the data\ndf.head()\n#df.head(10) # top 10 row\n#df.tail(10) # last 10 row","99338f51":"# All the features used in the data set\nfeatures = []\nfetures = list(df.columns) \nprint('#'*5,'No of features in training data', len(fetures))\nprint()\nprint(fetures)\nprint('-'*100)\nprint()\n\n#The numeric features\nnum_fet = []\nnum_fet = list(df.select_dtypes(exclude = 'object').columns)\nprint('#'*5, 'No of numerical features in training data', len(num_fet))\nprint()\nprint(num_fet)\nprint('-'*100)\nprint()\n\n#The Categorical features\ncat_fet = []\ncat_fet = list(df.select_dtypes(include = 'object').columns)\nprint('#'*5,'No of categorical features in training data', len(cat_fet))\nprint()\nprint(cat_fet)","ba5df7f5":"# Some statiscs for numerical cols\ndf.describe().T","cb4ccece":"# Some statiscs for all cols\ndf.describe(include= 'all').T","4fb75b4b":"# Printing a specific column\nprint(df['SalePrice'][:7]) # Printing top 7 rows\n\nprint('-'*50)\n\n# Printing some specific rows and columns\nprint(df.iloc[10:14,-3:]) # The last three features and 10 to 13 rows\n# Here iloc stands for Inverted Letter Of Credit a mehod to Select row or col\n\nprint('-'*50)\n\n# Printing some rows and cols using specific cols name\nprint(df.loc[90:95,'SalePrice'])\n# using loc Letter of Credit one can use the col name directly for selecion","c6fe11f1":"# Lets see some row by applying condition\n\ndf.loc[(df.SaleType == 'New') & (df.YrSold == 2007) & (df.MSZoning=='FV')]","73c998ed":"import seaborn as sns\nimport matplotlib.pyplot as  plt","b95aa14d":"# Lets see the scatter plot\n# Lets create scatterplot of GrLivArea and SalePrice\n\nplt.figure(figsize=(8,8))\nsns.scatterplot(x='GrLivArea',y='SalePrice',data=df)\nplt.show()","dc6e8b3d":"# Lets see the line plot \/ regression plot\n# Lets create regression plot of GrLivArea and SalePrice\n\nplt.figure(figsize=(10,8))\nsns.regplot(x='GrLivArea',y='SalePrice',data=df,line_kws={\"color\": \"red\"})\nplt.show()","b4c7a095":"# Lets see the box plot\n# It is very useful to handle the ouliers\n\n# Lets create scatterplot of LotShape and SalePrice\nplt.figure(figsize=(8,8))\nsns.boxplot(x='LotShape',y='SalePrice',data=df)\nplt.show()","349eb2a5":"# Lets see a bar plot of a categorical features\n# We pick 'MSZoning' features for bar plot\n\n# First list all the category in MSZoning features\n\nmszoning_cats =df['MSZoning'].unique().tolist()\nmszoning_cats","9b703116":"\n# We convert the catregorical variables into numeric one. We will see more in the next step\n\ndf['MSZoning'] = df['MSZoning'].astype('category').cat.codes + 1\ncat_val = df['MSZoning'].value_counts()\n\nplt.figure(figsize=(8,8))\nsns.barplot(x = cat_val.index, y = cat_val.values)\nplt.title('Values', fontsize=14)\nplt.xlabel('Type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(cat_val.index)),mszoning_cats)\nplt.show()","7afe125f":"# Now it is high time to see the histogram\n# Let see the histogram on YrSold\n\nplt.figure(figsize=(10,6))\nsns.distplot(a = df['YrSold'],kde = False)\nplt.show()","09ca51af":"# Lets see distribution plot on log of SalePrice\n\nplt.figure(figsize=(7,7))\nsns.kdeplot(data = np.log(df['SalePrice']), shade = True)\nplt.show()","2673e424":"# Now it is time to see heatmap\n# Before doing heatmap first see the relational dependency \/ relation on salePrice with the other features\n\ndependency = df.drop('Id',1).corr().sort_values(by='SalePrice',ascending=False).round(4)\nprint(dependency['SalePrice'])\n\n# The link how heatmap and corelation matrix work , will be provided","fded7449":"# Now lets see the heatmap of the features\n\nplt.subplots(figsize=(12, 12))\nsns.heatmap(dependency, square=True);","5488160b":"# Now lets create heatmap for top 10 correlated features\n\ncols = dependency['SalePrice'].head(10).index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1)\nplt.figure(figsize=(9,9))\nhm = sns.heatmap(cm, annot=True, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","15a6fa60":"# Now turn into the swarmplot\n\ndict_of_col={i:cat_fet[i] for i in range(len(cat_fet))}\n\ntemp_list=list(i for i in range(3))\nf,axes=plt.subplots(1, 3,figsize=(16,7))\nf.subplots_adjust(hspace=0.5)\n\nfor j in temp_list:\n        g = sns.swarmplot(x = df[dict_of_col[j+1]],y = df['SalePrice'],ax = axes[j]) \n        g.set_title(label = dict_of_col[j+1].upper(),fontsize = 17)\n        g.set_xlabel(str(dict_of_col[j+1]),fontsize = 13)\n        g.set_ylabel('SalePrice',fontsize = 17)\n        plt.tight_layout() # to increase gapping between subplots","716c83fa":"df['log_SalePrice'] = np.log(df['SalePrice'])","c7a46037":"df[['SalePrice','log_SalePrice']].head()","98bc34a2":"df.drop(columns = 'log_SalePrice', inplace = True)\n# If you donot use inplace = True ,\n# Then the column will be deleted temporalily but not permanatly","4a9500f7":"df.head()","e98eb0bb":"# Normalization\ndf['normalized_SalePrice'] = (df['SalePrice'] - df['SalePrice'].min())\/ (df['SalePrice'].max() - df['SalePrice'].min())","49b4cf02":"# Standardization\ndf['standardized_SalePrice'] = (df['SalePrice'] - df['SalePrice'].std())\/ df['SalePrice'].mean()","3930bda9":"df[['SalePrice','normalized_SalePrice','standardized_SalePrice']].head(5)","6cb608c0":"df.drop(columns = ['normalized_SalePrice','standardized_SalePrice'], inplace = True)","9efc9976":"for col in fetures:\n    if df[col].isnull().sum() > 0:\n        print(str(col+' '*2)+str('-'*7)+str('->  ')+ str(df[col].isnull().sum()))","89e29dd2":"for col in cat_fet:\n    if df[col].isnull().sum() > 0:\n        df[col] = df[col].fillna('Unknown')","66c267c7":"for col in num_fet:\n    if df[col].isnull().sum() > 0:\n        feature_mean = df[col].mean()\n        df[col].replace(np.nan,feature_mean,inplace = True)","af2d20f8":"df.isnull().values.any()","db2c46be":"df.tail()","24532838":"for col in cat_fet:\n    df[col] = df[col].astype('category').cat.codes + 1","4612d95b":"df.head()","92c1fd78":"dup = df[df.duplicated()]\n\nif len(dup) == 0:\n    print('There are no duplicates')\nelse:\n    print('There are ',len(dup), ' duplicates')","8ce4594c":"# Choosing Features for building modeling\n# You may also add other features and also some new features\n# To make simple, we choose the features and do not do any feature engr\n\ncols = ['Alley', 'Fence', 'FireplaceQu', 'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType',\n        'ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n        'Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType',\n        'GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition']","0f61296c":"X_train = df[cols]\ny_train = df['SalePrice']","76b0f1e8":"# first import the dcsn tree\nfrom sklearn.tree import DecisionTreeRegressor","c8fb6098":"# look at the parameter of the building fucntion\n# It is very important to have a good knowledge about parameter and how they work\n\n?DecisionTreeRegressor","ec4629b0":"# Now, look into the source code of the building function of sklearn\n\n??DecisionTreeRegressor","f6d704ba":"# Create a very simple model\n\nDT_model = DecisionTreeRegressor(criterion='mse',max_depth = 13)\n\nDT_model.fit(X_train,y_train)\nprint(DT_model.score(X_train,y_train))","e327ca49":"from sklearn.tree import export_graphviz  \nimport graphviz\n\ntree_graph = export_graphviz(DT_model, out_file= None,max_depth = 2,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = cols)\n\ngraphviz.Source(tree_graph)","517acd3b":"from sklearn.ensemble import RandomForestRegressor","f2938c6b":"# Lets look into the parameter\n\n?RandomForestRegressor","ab602caa":"# Lets Look into the source code of RF model in Sklearn\n\n??RandomForestRegressor","eaa583d8":"# Lets build our rf model\n\nRF_model = RandomForestRegressor(n_estimators = 700, max_features='log2', max_samples = 0.9, n_jobs = -1)\nRF_model.fit(X_train,y_train)\nprint(RF_model.score(X_train,y_train))","77255989":"df_test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')","c1d96166":"df_test.head()","7877360b":"features_test = []\nfetures_test = list(df_test.columns) \n\n#The numeric features\nnum_fet_test = []\nnum_fet_test = list(df_test.select_dtypes(exclude = 'object').columns)\n\n#The Categorical features\ncat_fet_test = []\ncat_fet_test = list(df_test.select_dtypes(include = 'object').columns)","a94813a8":"for col in cat_fet_test:\n    if df_test[col].isnull().sum() > 0:\n        df_test[col] = df_test[col].fillna('Unknown')\n\n\nfor col in num_fet_test:\n    if df_test[col].isnull().sum() > 0:\n        feature_mean = df_test[col].mean()\n        df_test[col].replace(np.nan,feature_mean,inplace = True)","e0fdbf28":"for col in cat_fet_test:\n    df_test[col] = df_test[col].astype('category').cat.codes + 1","216fe6f3":"df_test.head()","d94a7379":"X_test = df_test[cols]\n\nX_test.shape","7a3dcb1e":"prediction_DT = DT_model.predict(X_test)","6bcd36ed":"submission = pd.read_csv('..\/input\/home-data-for-ml-course\/sample_submission.csv')","df78f241":"submission['SalePrice'] = prediction_DT\nsubmission.to_csv('submission_DT.csv', index=False)","737cbf43":"prediction_RF = RF_model.predict(X_test)","2384fbcd":"submission = pd.read_csv('..\/input\/home-data-for-ml-course\/sample_submission.csv')","476df086":"submission['SalePrice'] = prediction_RF\nsubmission.to_csv('submission_RF.csv', index=False)","28a23bc5":"Now, finally check if there is anymore missing values.","6b30cb2b":"Lets import a RF Model from sklearn","1f3b0b88":"#### 5. Checking Duplicates Rows\n\nSometimes there are duplicated rows in dataset. Those duplicates have no importance to design a good model. In the opposite , those rows take memory space. So, it is better to remove the rows. Follow [this link](https:\/\/www.geeksforgeeks.org\/find-duplicate-rows-in-a-dataframe-based-on-all-or-selected-columns\/) for more.","559e55cb":"#### 3. Handling the Missing Values\n\nWhen one is working with **real-world data**, then he often sees the **missing values**. It is very common phenomona in real world. The reason behind having missing value is that data can have missing values for a number of reasons such as observations that were not recorded and data corruption. But any ML\/DL algo does not support any missing values. So, handling missing data is important.\n\nThere are several ways to handle the missing values. Go to the following links for more details. \n1. [Link 1](https:\/\/medium.com\/analytics-vidhya\/why-it-is-important-to-handle-missing-data-and-10-methods-to-do-it-29d32ec4e6a)\n2. [Link 2](https:\/\/towardsdatascience.com\/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e)\n\nWe first check the features that have the missing values with number of the missing values. For simplicity, for the categorical varibales that have missing values, we replce those by giving a category **Unknown** . And for numerical variables that have missing values, in this case we impute this type of missing values using the **features mean** .  \n\n\nLets look into the features which have missing values","4b19800c":"## 2. Random Forest\n\nIf you tell me in any ML competition ,which is my baseline model ? I will definitely reply , the model will be **Random Forest (RF)**. It is very much popular for its simplicity. It is not only a tree based model but also a ensemble model. If you understand the **Decision Tree** algorithm , then the algorithm behind **RF model** will be easier one for you.<br>\n\nA random forest consists of multiple random decision trees. Two types of randomnesses are built into the trees. First, each tree is built on a random sample from the original data. Second, at each tree node, a subset of features are randomly selected to generate the best split.<br>\n\nRandom forests are popularly applied to both data science competitions and practical problems. They are often accurate, do not require feature scaling, categorical feature encoding, and need little parameter tuning. They can also be more interpretable than other complex models such as neural networks. Lets derive into RF\n\n1. [A Great Introduction](https:\/\/towardsdatascience.com\/understanding-random-forest-58381e0602d2)\n2. [All about RF](https:\/\/towardsdatascience.com\/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76)\n3. [RF Regressor](https:\/\/towardsdatascience.com\/random-forest-and-its-implementation-71824ced454f)\n4. [To Build RF from Scratch in Python](https:\/\/towardsdatascience.com\/building-a-random-forest-classifier-c73a4cae6781)\n5. [Advantage of RF ](https:\/\/towardsdatascience.com\/quick-intro-to-random-forest-3cb5006868d8)\n6. [Decison Tree and RF](https:\/\/www.analyticsvidhya.com\/blog\/2020\/05\/decision-tree-vs-random-forest-algorithm\/)\n7. [More on Decision Tree and RF](https:\/\/www.kdnuggets.com\/2020\/01\/decision-tree-algorithm-explained.html)","fdb5cd16":"### How to read the kernel\nToday to solve the ML problems there are so many frameworks used. Infact , you do not need the math behind the algo or others to work with the framework. You need to know some basic level coding and fucntions and their usage to work with the framework. But it is not good. As the function acts like **Black Box** in this case. In the begining, it feels great but one can not go further without having a strong knowledge about what is happening behind the algo.\n\nIn this kernel I will put some links and tutorials to understand about the algo and the others. After understanding what is happening behind the ML algo via the links and tutorials, go through the other section of this kernel.  ","cee0c486":"![](https:\/\/miro.medium.com\/max\/875\/0*pZ0pgQT3i-CFPyaf.)","a6bfbdc2":"To learn about boxplot , you may follow the links\n\n1. [ Simple Overview on BoxPlot ](https:\/\/www.khanacademy.org\/math\/statistics-probability\/summarizing-quantitative-data\/box-whisker-plots\/a\/box-plot-review)\n2. [ Great Article On Boxplot ](https:\/\/towardsdatascience.com\/understanding-boxplots-5e2df7bcbd51)\n3. [ With SeaBorn ](https:\/\/cmdlinetips.com\/2018\/03\/how-to-make-boxplots-in-python-with-pandas-and-seaborn\/)","797048e7":"![](https:\/\/cdn.educba.com\/academy\/wp-content\/uploads\/2019\/12\/Six-Different-Steps-Involved-in-Machine-Learning.png)","b67c5aa7":"![](https:\/\/fiverr-res.cloudinary.com\/images\/t_main1,q_auto,f_auto,q_auto,f_auto\/gigs\/155900526\/original\/191941adae3024838d463cf1ddccdf97edf34e69\/make-you-a-creative-and-unique-logo-design.png)","51922c70":"For more about the **parameters and details** [follow this link](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html)","02c3aed6":"Lets see some visualization using **seaborn** <br>","d7b9e0e9":"To learn more about **sklearn DT parameters** what they stand for and how influence the model [follow this link](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeRegressor.html)","d6706874":"Lets see how our decision tree is growing","96573154":"![](https:\/\/www.indiumsoftware.com\/blog\/wp-content\/uploads\/2020\/01\/Machine-Learning-algorithms-1536x1410.png)","5d593694":"![](https:\/\/miro.medium.com\/max\/1434\/1*ZFuMI_HrI3jt2Wlay73IUQ.png)","e469a2c2":"Lets create a DT Regressor model","b06c5d0e":"The code for swarmplot is from [this kaggle kernel ](https:\/\/www.kaggle.com\/sahib12\/housing-price-problem-top-20) ","8e9bd5f4":"# Weclome to the world of Machine Learning (ML)","c566bfba":"Read the csv file","41d4fd5b":"#### 2. Rescaling ( Normalization \/ Standardization )\nTo gain basic knowledge about the rescaling [this link](https:\/\/towardsai.net\/p\/data-science\/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff) may help you. <br>\nIf we want to rescale a column or normalize a column , this also can be done with pandas. For example, we want **salePrice** columns value range from 0 to 1. Lets see, how this can be done","9dfeb9a6":"# Step - 4: Modeling\n\nIn this step, one\/more ML model\/models is\/are used for solving the problem. A machine learning model is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data.Once you have trained the model, you can use it to reason over data that it hasn't seen before, and make predictions about those data.<br>\n\nBut there is a little cofusion about the **ML algo** and **ML model**. Follow [this article](https:\/\/machinelearningmastery.com\/difference-between-algorithm-and-model-in-machine-learning\/#:~:text=Machine%20learning%20involves%20the%20use%20of%20machine%20learning%20algorithms%20and%20models.&text=Machine%20learning%20models%20are%20output,learning%20models%20represent%20the%20program.) for clarification. <br>\n\nAnyway, there are two types of **ML model**. Those are **Regression** and **Classification**. For learning better about the **ML model** [follow this great article](https:\/\/towardsdatascience.com\/all-machine-learning-models-explained-in-6-minutes-9fe30ff6776a) ","ed81ec3b":"### I am very much beginner in ML field \nIf you are very much beginner in this field then you should take the great course by Andrew Ng on Coursera.<br>\n[Course Link](https:\/\/www.coursera.org\/learn\/machine-learning)\n\n### I have taken the course. What next ? \nIf you have taken the course then follow this notebook. It may help you.\n\n### But wait I want to write on markdown cell. How can I do? \nIf you want to learn about the markdown cell, you may follow the tutorial links below\n\n1. **Markdown Text Writing**: https:\/\/medium.com\/ibm-data-science-experience\/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\n2. **More Markdown** : https:\/\/www.datacamp.com\/community\/tutorials\/markdown-in-jupyter-notebook","bd535f8e":"### Prediction\nNow, lets make the prediction based on the model.<br>\nBefore doing prediciton, we need to clear the test data also (like handling missing values, encoding the categorical values). Lets do that....","160b3852":"![](https:\/\/images.assetsdelivery.com\/compings_v2\/handiniatmodiwiryo\/handiniatmodiwiryo1909\/handiniatmodiwiryo190900054.jpg)","c202a867":"If you want to delete any features , this can also be done easily with pandas","61dbdc76":"# Step - 1 : Understand the problem\nIn this step, one should understand the problem. What is the competiton about ? What are the features ? What need to predict ?  What is the evaluation metric and so on .. <br>\n\nSo many questions in this step. But don't worry. All of those are basic questions. You will get the answer of all the questions in the  **competition description**. But to learn about the features , you should google it. As because another question arises *What the features stand for ?* Moreover to understand the problem more deeply, one should have a proper knowledge about the features. You can learn about the features in more details by googling by their name or by your knowledge <br>\n\nBut some competitons [like this](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/data?select=sales_train_evaluation.csv) have no appropiate features name. The features name are like f1,f2,f3.. or something like this. In this case, use your prior knowledge by knowing what should need to predict the target and think about what features should need to solve the problem. \n\nTo have a good start with ML, [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course) is a great competition. This a regression problem where you should  predcit the price of the houses. The evaluation metrics for this competition is **Root Mean Squared Error**. For more details about the competition and data follow [the link](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/overview).\n\nNow, it is high time to look into the data and understand the problem","d7cfffac":"# Solve the Machine Learning Problem\nAlmost all ML problem can be solved into 5 few steps. The steps are\n1. Understand the problem [ What is the problem about ? ]\n2. Data Visualization  [ Understand the data ]\n3. Data Preprocessing  [ Looking into the data ]\n4. Modeling     [ Use of ML algo ]\n5. Deployment  [ Less use in Competition]\n\nThough different problem has differnet procedure but you can categorize the procedure into the steps. In this notebook I will discuss about all the steps above except the *Deployment* step.","173453e0":"####  Pandas\n**Pandas** is a great library for machine learning. It is very useful not only loading the **csv** file but also doing some basic data preprocessing. It is very popular library in Machine Learning. If you do not know about the pandas, the following link may help you <br>\n\n1.  Kaggle Tutorial : https:\/\/www.kaggle.com\/learn\/pandas\n2.  Github Excerise : https:\/\/github.com\/guipsamora\/pandas_exercises\n3.  DataCamp  : https:\/\/www.datacamp.com\/community\/tutorials\/pandas-tutorial-dataframe-python\n4.  Towards data Sciencce : https:\/\/towardsdatascience.com\/in-depth-pandas-tutorial-5d896483ba8a","4afe91b5":"# Step - 3: Data Preprocessing \nData preprocessing is very very much important not only in ML but also in DL as well as RL. One subsector of this step is also known as **Feature Engineering**. The efficiency of the models depend on the preprocssing. For example, in any CNN problem resizing the images with 1024 X 1024 may give worse result than by resizing wiht 512 X 512. Models may have much low accuracy without handling missing values . Outliers may affect the model efficiency. and so on. <br>\n\nThere are basically some few steps in Data Preprocessing.\n1. Feature Engineering [ Adding Features\/Deleting Features\/Selecting most important features (Feature Selection) and so on ]\n2. Rescaling and others [ Noramlizing\/Outliers\/Data Fomatting ]\n3. Handling the Missing Values\n4. Checking Categorical Values \n5. Checking duplicates\n6. Binning [ Numeric value to categorical ]","2fec0ea1":"### Let's Start\nIn this section, I want to cover some very much popular method that is used for preprocessing, some basic visualization and a very simple model. Gradually, I will drive into more ohter basic and advanced models. I want to cover all kind of ML models and techniques that are used in any ML competition. But for part 1, I want to make it simple. ","bf6d0da5":"The **HeatMap** visualization is from [ this kaggle kernel ](https:\/\/www.kaggle.com\/vishalvanpariya\/top-5-on-leaderboard\/data).<br>\n\nIf you want to learn more about heatmap and the correalation matrix you may see following links\n\n1. [ Correlation Matrix ](https:\/\/www.displayr.com\/what-is-a-correlation-matrix\/#:~:text=A%20correlation%20matrix%20is%20a,a%20diagnostic%20for%20advanced%20analyses.)\n2. [ Correlation and Covariance ](https:\/\/towardsdatascience.com\/let-us-understand-the-correlation-matrix-and-covariance-matrix-d42e6b643c22)  \n3. [ About HeatMap ](https:\/\/towardsdatascience.com\/heatmap-basics-with-pythons-seaborn-fb92ea280a6c)\n4. [ Annoted Heatmap ](https:\/\/www.kdnuggets.com\/2019\/07\/annotated-heatmaps-correlation-matrix.html)","9c8a71df":"Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves. In fact, nowadays ML has touched almost every field of technology. ML has brought a big change in the mordern technology. So, nowadays it one of the hottest topic for learning. As a result, many of us want to learn ML. But a big question araises for new learners is that where to learn. In this notebook, I will try to answer the question. So, lets start","0504f65d":"Lets create prediciton on **DT Model**","e9820242":"First create training data","e9bbaa2f":"For handling the numercial missing values we will use the **features mean**","d2bb3b0f":"Now, lets give an another category named **Unknown** for every categorical features that have missing values","238aab5e":"### Which libaray will we use for visualization and why ?\nThere are many more libraries for doing the data visualization. Some of them \n\n1. **Matplotlib** [ Most Popular ]\n2. **Seaborn** [ Most commonly used in competition ]\n3. **ggplot** [ Best for geographic Plot ]\n4. **Bokeh** [ Many of Data scientists are using ]\n5. **pygal** [ used for creating SVG (Scalable Vector Graphics) charts]\n6. **Plotly** [ Finest library for advance visualization ]\n7. **geoplotlib** [ Same as ggplot ]\n\nNow, we choose the **Seaborn** library because it very simple to use and very efficient for doing the basic visulaization. ","f25af9be":"### Here '?' is a magical one which works while you work with Python Code in Jupyter Notebook\n#### For any python class\/function you can use '?' to learn about the parameters","69e4fba4":"Now, this is a regression problem. We will apply different **ML models** that are used for regression. We can divide those models into several categories.\n\n\n1. **Tree Based Models** [ Decision Tree , RandomForest (this is also ensemble model]\n2. **Linear Models** [ LinerRegression, SVM, Ridge, Lasso, ElasticNet and so on]\n3. **Ensemble Models** [ Xgboost, Catboost, LightGBM, GradientBoosting, and so on]\n4. **Ensemble of Linear Models** [ Bagging Regressor ]\n5. **Blending and Stacking** [ StackingRegressor, StackNet ]\n6. **Neural Net** \n\nIn this notebook. we will go through the first one. **Tree Based Models**. We will build a simple **Decision Tree** and then for getting better result we will move to the **Random Forest Regressor**. In other part of this notebook, we will see other models and also explore how they work, how to build those models <br>\n\nLets build our **Machine Learning Model**","20d75952":"# Final Word Befor We Go\n\nIt takes more than two months to learn all kind of things above. Take your time to learn everything and increase your **Machine Learning** knowledge. In this competition , do not focus on rank rather than learning.<br>\nWell, this is the basic model, we have created. In other part, we will see more advanced **Machine Leaning Models and techniques**. Stay with the other parts for more models in future. Hope that this notebook may help you. **Upvote** this kernel if you like it.\n\n![](https:\/\/thumbs.dreamstime.com\/b\/badge-thank-you-graphics-design-elements-vector-label-logo-gratitude-branding-advertisement-189529344.jpg)","da29537d":"### Lets submit\n\nFeel free to build your own model and submit. Improve your rank trying your own model. Explore the tutorials and links for better understadning. Make a change to the model\n","64c1bef7":"#### 1. Feature Engnieering \nTo learn about the **feature engineering** [this kaggle course](https:\/\/www.kaggle.com\/learn\/feature-engineering) can help you. We will do a few of feature engineering in this notebook. <br>\n\nLets say, we want to create an extra feature which will be the log of the **salePrice**. This can be easily done using panda","dedef14f":"#### 4. Handling categorical features\nAny ML\/DL model can not be handled with the categorical variable. So, we need to convert those variables \/ features into numeric one. There are few ways to do so. Some of them\n1. Label Encoding \n2. One Hot Encoding \n3. Target Encoding\n4. Count Encoding\n\nTo know about the label and categorical encoding [this link](https:\/\/towardsdatascience.com\/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd) can be followed. <br>\n[This kaggle notebook](https:\/\/www.kaggle.com\/matleonard\/categorical-encodings) helps you to learn about the other endcoing.\n\nTo make this notebook simple, I will use **Label Encoding** using pandas. You may choose other encoding methods if you want. I","8f9e39a7":"1. If you want to learn about **seaborn** [this kaggle course](https:\/\/www.kaggle.com\/learn\/data-visualization) will help you a lot.<br>\n2. Another [Coursera Course by IBM](https:\/\/www.coursera.org\/learn\/python-for-data-visualization) is great for learning about data visualization. <br>\n3. For learning the **ploty** [this blog](https:\/\/towardsdatascience.com\/the-easy-way-to-do-advanced-data-visualisation-for-data-scientists-bbc852e26fb6) is great. <br>\n4. You may browse [this medium homage ](https:\/\/towardsdatascience.com\/data-visualization\/home) for learning more","a3e7c6c1":"Lets create prediction on **RF Model**","7aec2024":"# Step - 2: Data Visualization\nNow, we are in the second step towards entering the ML world. It is  ***Data Visualization***  step. Actually, ***Data Visualization*** and ***Data Preprocessing*** are very much related to each other. There is no rule of thumb that which step will come before. One may need the *Data Preprocessing* before then the ***Data Visualization***. Or another may need ***Data Visualization*** before and the ***Data Preprocessing*** after. Very often we need to switch the two steps like the **Data Preprocessing** first and then ***Data Visualization*** and then again ***Data Preprocessing***. These type of combining also known as **EDA** **(Explatory Data Analysis)**. By doing ***EDA*** we actually look into the data and understand the data in more depth. It also helps us to know about the features like what the features actually doing in the dataset, the feature distribution and many more. Finally, ***EDA*** helps us very much not only figuring out which model we will use but also improving the model. For example, if one can see that the data is in linear form (after doing visualization), the linear model will work better in this case. Another one has found that there are few outliers through doing box plot and he removes the outliers, then obviously he may creates better model than before.  \n\nSome bacis Visualization techniques are \n1. Scatter plots\n2. Bar charts and Histograms\n3. Line plots \/ Regression Plot\n4. Distribution Plot\n5. BoxPlot\n6. Heat Map \/ Correlation Matrix \n7. Pie charts\n8. Stem plots\n9. SwarmPlot\n\nWe will apply some of them","483d79c1":"![](https:\/\/www.aquatechtrade.com\/-\/media\/websites\/aquatech\/aqd\/images\/news\/aqd_machine-learning_1140x400.ashx?mw=900&mh=615)","e6557bcc":"## 1. Decision Tree\n\nWell, **Decision Tree (DT)** is a great ML model where a tree is build based on the decision. It is very popular for the beginner to start their journey with ML as the **Decision Tree** model is easy to understand and also very efficient to solve the ML problem. A decision tree is a flowchart-like tree structure where an internal node represents feature (or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition based on the attribute value. It partitions the tree in a recursive manner called recursive partitioning. This flowchart-like structure helps you in decision making. It\u2019s visualization like a flowchart diagram which easily mimics the human-level thinking. That is why decision trees are easy to understand and interpret.<br>\nNow, it is high time to derive into the **Decision Tree**\n\n1. [A Great Introduction on DT](https:\/\/towardsdatascience.com\/decision-trees-in-machine-learning-641b9c4e8052)\n2. [To Understand the Math Behind DT -  1](https:\/\/www.vruttitanna.com\/post\/understanding-the-mathematics-behind-the-decision-tree-algorithm-part-i)\n3. [To Understand the Math Behind DT -  2](https:\/\/medium.com\/@ankitnitjsr13\/math-behind-decision-tree-algorithm-2aa398561d6d)\n4. [To Understand the Math Behind DT -  3](https:\/\/medium.com\/@ankitnitjsr13\/decision-tree-algorithm-id3-d512db495c90)\n5. [To Build DT From Scratch in Python](https:\/\/www.kaggle.com\/nathanlauga\/mathematics-of-decision-tree)\n6. [About the Algo behind DT](https:\/\/heartbeat.fritz.ai\/introduction-to-decision-tree-learning-cd604f85e236)\n7. [How the Values are Splitted?](https:\/\/www.coursera.org\/lecture\/ml-classification\/optional-picking-the-best-threshold-to-split-on-sKrGp)\n8. [Pros and Cons of DT](https:\/\/towardsdatascience.com\/decision-trees-explained-3ec41632ceb6)\n9. [A complete Overview](https:\/\/towardsdatascience.com\/the-complete-guide-to-decision-trees-28a4e3c7be14)\n10. [Sklearn Documentation](https:\/\/scikit-learn.org\/stable\/modules\/tree.html#)"}}