{"cell_type":{"bbf4039f":"code","bd9154f1":"code","dbc41d4f":"code","8a116d9b":"code","e2643302":"code","656f3c8c":"code","95b60371":"code","b66530a8":"code","b80e6fd1":"code","dbbb99be":"code","8d929944":"code","d6ea99ea":"code","0fb67a37":"code","f1bece74":"code","67b43e81":"code","d2c504ab":"code","11ddd1d6":"code","d8827876":"code","9d33606b":"code","c888fee6":"code","3591090e":"code","83eae224":"code","e9f82674":"code","afddaebe":"code","9d7110d7":"markdown","3f3f1421":"markdown","63220472":"markdown","f2c0a6d9":"markdown","acc82812":"markdown","536afa2c":"markdown","d66c6a72":"markdown","48049c68":"markdown","b6cd9364":"markdown","a36d2ee2":"markdown","5f868ef1":"markdown","384b5289":"markdown","d84d6a9f":"markdown","d7f5f986":"markdown"},"source":{"bbf4039f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport sklearn\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nimport re\nfrom nltk.tokenize import TweetTokenizer\nimport gensim\nfrom gensim.models import Word2Vec\nimport re\nimport random","bd9154f1":"path='..\/input\/nlp-getting-started'\ntrain=pd.read_csv(path+'\/train.csv')","dbc41d4f":"train.info()","8a116d9b":"train.isnull().sum()\/len(train)","e2643302":"glove_loc='..\/input\/d\/fullmetal26\/glovetwitter27b100dtxt\/glove.twitter.27B.200d.txt'","656f3c8c":"def custom_tokenize(seq):\n    url_regex=r\"(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))\"\n    #replaces all the links with url\n    seq=re.sub(url_regex,'<URL><sp>',seq) \n    \n    # removes words which cannot be encoded\n    seq=re.sub('[^\\x00-\\x7F]+.','',seq)\n    \n    # replaces all the numbers with a general token NUMBER\n    seq=re.sub('[0-9]+','<NUMBER><sp>',seq)\n    \n    # Replaces all the usernames with generalized token USER\n    seq=re.sub('@\\w+', \"<USER><sp>\",seq)\n    \n    #Replaces the hashtags before words with token HASHTAG\n    hash_words=re.findall('#[A-z]{1,}',seq)\n    for i in hash_words:\n        seq=re.sub(i,'<HASHTAG><sp>'+i.split('#')[1],seq)\n    reg_split=r'\\s|<sp>'\n    seq=re.split(reg_split,seq)\n    \n    \n    for i,j in enumerate(seq):\n\n        non_cap=re.findall('[A-Z]{1}[a-z]+',j)\n        #replaces all the capital letter words with this ALLCAPS token followed with \n        #the original words\n        if len(non_cap)>0:\n            cap_in_noncap=re.split(non_cap[0],j)\n            if len(non_cap[0])\/\/len(j)==0:\n                if len(cap_in_noncap)>0:\n                    #replaces all the capital letter words with this ALLCAPS token followed with \n                    #the original words\n                    seq[i]=' '.join([cap_in_noncap[0].lower(),'<ALLCAPS>',non_cap[0].lower()])\n        \n        all_cap=re.findall('^([A-Z]{2,}[^a-z]+)$',j)\n        if len(all_cap)>0:\n            seq[i]=re.findall('[^(.,`;@_!#$%^&*()<>?\/|}{~:\\'\\-)]+',j)[0].lower()+' <ALLCAPS>'\n\n\n        repeat=re.findall('[.,!?:\\'\\-]',j)\n        \n        # Repeating special characters will be replaced with this token REPEAT\n        \n        if len(repeat)>1:\n            seq[i]=' '.join([seq[i],repeat[0]+' <REPEAT>'])\n        else:\n            if repeat:\n                seq[i]=' '.join([re.split('[.,!?:\\'\\-]',j)[0],re.findall('[.,!?:\\'\\-]',j)[0]])\n        \n        spe_words=re.split('[.,`;_!$%^&*()?\/|}{~:\\'\\-]',j)\n        expre=re.split('[^(.,`;_!$%^&*()?\/|}{~:)\\'\\-]',j) \n        \n        if len(spe_words)>2:\n            temp1=[z for z in spe_words if z]\n            temp2=[z for z in expre if z][-1]\n            temp3=re.findall('[.,`;_!$%^&*()?\/|}{~:\\'\\-]',temp2)\n            if len(temp3)>1:\n                seq[i]=' '.join(temp1+[temp3[0],'<REPEAT>'])\n        \n        exp=re.findall('[.,`;_!$%^&*()?\/|}{~:\\'\\-]',j)\n        if len(exp)==1:\n            seq[i]=re.sub('[.,`;_!$%^&*()?\/|}{~:\\'\\-]',' '+exp[0]+' ',j)\n    seq=' '.join(seq)\n    seq=seq.split(' ')\n    return [i.lower() for i in seq if i]","95b60371":"def text_processing(seq,glove_loc,len_vec=200):\n    stpwds_set=set(stopwords.words('english'))\n    \n    embeddings_dict = {}\n    with open(glove_loc, 'r') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:], \"float32\")\n            embeddings_dict[word] = vector\n    \n    seq=seq.apply(lambda x:custom_tokenize(x))\n    seq=seq.apply(lambda x:[i for i in x if i not in stpwds_set])\n    for i,j in enumerate(seq):\n        temp=[]\n        for l,k in enumerate(j):\n            \n            try:\n                temp.append(embeddings_dict[k])\n            except(Exception):\n                continue\n        seq[i]=temp\n    return seq","b66530a8":"train_data=text_processing(train.text,glove_loc)\ntrain_data=pd.DataFrame(train_data).join(train.target)","b80e6fd1":"train_data.target.value_counts()\/len(train_data)","dbbb99be":"test=pd.read_csv(path+'\/test.csv')\ntest_data=pd.DataFrame(text_processing(test.text,glove_loc))","8d929944":"max_seq=train_data.text.apply(lambda x:len(x)).max()\nmax_seq","d6ea99ea":"min_seq=train_data.text.apply(lambda x:len(x)).min()\nmin_seq","0fb67a37":"import keras\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,BatchNormalization,Dropout,Conv1D, MaxPooling1D, Activation, Flatten\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nimport tensorflow as tf","f1bece74":"def tweet_gen(data,batch_size,max_seq,min_seq,aug):\n    \n    while True:\n        data=np.random.permutation(data)\n        num_batches = data.shape[0]\/\/batch_size\n        rem_d=data.shape[0]%batch_size\n        \n        for batch in range(num_batches): # we iterate over the number of batches\n            if aug==True:\n                maxlen=random.choice(range(min_seq,max_seq))\n                padding=random.choice(['pre','post'])\n                truncating=random.choice(['pre','post'])\n                temp=tf.keras.preprocessing.sequence.pad_sequences(data[batch*batch_size:(batch+1)*batch_size][:,0], maxlen=maxlen, dtype='float32', padding=padding,truncating=truncating, value=0.0)\n                yield tf.keras.preprocessing.sequence.pad_sequences(temp, maxlen=max_seq, dtype='float32', padding='pre',truncating='pre', value=0.0),data[batch*batch_size:(batch+1)*batch_size][:,1].astype('float32')\n            else:\n                yield tf.keras.preprocessing.sequence.pad_sequences(data[batch*batch_size:(batch+1)*batch_size][:,0], maxlen=max_seq, dtype='float32', padding='pre',truncating='pre', value=0.0),data[batch*batch_size:(batch+1)*batch_size][:,1].astype('float32')\n    \n        if rem_d!=0:\n            fm=num_batches*batch_size\n            to=fm+rem_d\n            if aug==True:\n                maxlen=random.choice(range(min_seq,max_seq))\n                padding=random.choice(['pre','post'])\n                truncating=random.choice(['pre','post'])\n                \n                temp=tf.keras.preprocessing.sequence.pad_sequences(data[fm:to][:,0], maxlen=maxlen, dtype='float32', padding=padding,truncating=truncating, value=0.0)\n                yield tf.keras.preprocessing.sequence.pad_sequences(temp, maxlen=max_seq, dtype='float32', padding='pre',truncating='pre', value=0.0),data[fm:to][:,1].astype('float32')\n            else:\n                yield tf.keras.preprocessing.sequence.pad_sequences(data[fm:to][:,0], maxlen=max_seq, dtype='float32', padding='pre',truncating='pre', value=0.0),data[fm:to][:,1].astype('float32')","67b43e81":"batch_size=8\n\ntrain_gen=tweet_gen(data=train_data.iloc[:int(len(train_data)*0.8),:],batch_size=batch_size,max_seq=max_seq,min_seq=min_seq,aug=True)\nval_gen=tweet_gen(data=train_data.iloc[int(len(train_data)*0.8):,:],batch_size=batch_size,max_seq=max_seq,min_seq=min_seq,aug=False)","d2c504ab":"def plot(history):\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n    axes[0].plot(history.history['loss'])   \n    axes[0].plot(history.history['val_loss'])\n    axes[0].legend(['loss','val_loss'])\n\n    axes[1].plot(history.history['accuracy'])   \n    axes[1].plot(history.history['val_accuracy'])\n    axes[1].legend(['accuracy','val_accuracy'])","11ddd1d6":"# create the model\nmodel = Sequential()\nmodel.add(keras.layers.InputLayer(input_shape=(max_seq,200)))\n\n\nmodel.add(Conv1D(128, 5, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(pool_size=4))\n\nmodel.add(Conv1D(256, 5, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(pool_size=4))\n\nmodel.add(LSTM(512))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","d8827876":"es = EarlyStopping(monitor='val_loss', mode='min',verbose=1, patience=20)\n#mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\nnum_epochs=100\nnum_train_sequences=int(len(train_data)*0.8)\nnum_val_sequences=len(train_data)-int(len(train_data)*0.8)\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences\/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences\/\/batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences\/batch_size)\nelse:\n    validation_steps = (num_val_sequences\/\/batch_size) + 1\n\nhistory1=model.fit(train_gen, steps_per_epoch=steps_per_epoch,callbacks=es, epochs=num_epochs, verbose=1, validation_data=val_gen,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\nplot(history1)","9d33606b":"test_data=tf.keras.preprocessing.sequence.pad_sequences(test_data.text, maxlen=max_seq, dtype='float32', padding='pre',truncating='pre', value=0.0)","c888fee6":"pre_var=model.predict(test_data)","3591090e":"submission=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","83eae224":"submission.target=pre_var","e9f82674":"submission['target']=submission.target.apply(lambda x:1 if x>0.5 else 0)","afddaebe":"submission.to_csv('submission.csv', index=False)","9d7110d7":"Loading the Test data","3f3f1421":"Since, here Twitter Data is being used, the glove embeddings for twitter will be used. Loading the path for the embeddings","63220472":"Checking the data for any class imbalance","f2c0a6d9":"Converting the twitter text data into numerical embeddings","acc82812":"Importing the keras modules","536afa2c":"Building  the ANN model based on 1 D Convolutional Neural Network and LSTM","d66c6a72":"Checking the type of training data provided","48049c68":"Checking for null values in the train data","b6cd9364":"Designing Custom Tokenizer which has special tokens like 'user','number','hashtag','allcaps' and 'repeat'which replaces many of words which are specific like username, url link, hashtag before a word.","a36d2ee2":"Calculating maximum sequence length among all the tweets","5f868ef1":"Function for plotting the loss and accuracies","384b5289":"Loading the Tweets Data","d84d6a9f":"Function for converting the twitter data into numerical embeddings based on glovetwitter","d7f5f986":"Importing the Text Processing Libraries"}}