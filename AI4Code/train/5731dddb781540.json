{"cell_type":{"1f999974":"code","5ea1b21a":"code","1de8478f":"code","591e1b30":"code","74308b49":"code","5af74c32":"code","b5580105":"code","5ceadc62":"code","01bc4e98":"code","97b47494":"code","1346f317":"code","4ecb4b21":"markdown","9a139557":"markdown","41025525":"markdown","3daf8ff9":"markdown","51a73d93":"markdown","6a4c75fd":"markdown","2370246a":"markdown","97592b18":"markdown"},"source":{"1f999974":"pip install opencv-contrib-python","5ea1b21a":"import cv2\nimport os\nimport numpy as np\nfrom PIL import Image","1de8478f":"recognizer=cv2.face.LBPHFaceRecognizer_create()","591e1b30":"#display the images used in the dataset\n\n# import  matplotlib.pyplot as plt\n# sample = cv2.imread('\/kaggle\/input\/myface\/dataset\/User.5.10.jpg')\n# plt.imshow(sample)","74308b49":"path='..\/input\/myface\/dataset'\ndef getImagesWithID(path):\n    imagePaths=[os.path.join(path,f) for f in os.listdir(path)]\n    faces=[]\n    IDs=[]\n\n    for imagepath in imagePaths:\n        faceImg=Image.open(imagepath).convert('L')\n        faceNp=np.array(faceImg,'uint8')\n        print(imagepath)\n        ID=int(os.path.split(imagepath)[-1].split(\".\")[1])\n        #dataset\/User.1.3\n        faces.append(faceNp)\n        IDs.append(ID)\n#         cv2.imshow(\"training\",faceNp)\n        cv2.waitKey(10)\n    return np.array(IDs),faces\n","5af74c32":"Ids,faces=getImagesWithID(path)","b5580105":"recognizer.train(faces,Ids)","5ceadc62":"# recognizer.save('trainingData.yml')","01bc4e98":"face_cascade = cv2.CascadeClassifier('..\/input\/haarcascade\/haarcascade_frontalface_default.xml')","97b47494":"def detect_faces(our_image):\n    img = np.array(our_image.convert('RGB'))\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    # Detect faces\n    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n    # Draw rectangle around the faces\n    name='Unknown'\n    for (x, y, w, h) in faces:\n        # To draw a rectangle in a face\n        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 255, 0), 2)\n        id, uncertainty = recognizer.predict(gray[y:y + h, x:x + w])\n        print(id, uncertainty)\n\n        if (uncertainty< 53):\n            if (id in [1,2,3,4,5]):\n                name = \"Nandini\"\n                cv2.putText(img, name, (x, y + h), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2.0, (0, 0, 255))\n        else:\n            cv2.putText(img, 'Unknown', (x, y + h), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2.0, (0, 0, 255))\n\n\n    return name","1346f317":"test_image = cv2.imread('..\/input\/test-image\/test_image.jpg')\ntest_image =  Image.fromarray(test_image)\n\n# img = cv2.cvtColor(img, cv2.IMREAD_GRAYSCALE)\ndetect_faces(test_image)","4ecb4b21":"Here, my model is able to detect me in the image, hence the output 'Nandini'.\n\nHere it detected ID = 5 and uncertainity = 25%.\n\nThis is pretty good as I only trained the model over 105 images where in a few images I was wearing my glasses and in most of the images I  was not wearing my glasses.\n\n20 images - with glasses, 85 images without glasses.\n\nI used a without glasses RGB test image.\n\n","9a139557":"**Saving the model as a YAML file.** YAML is like a better version of JSON.","41025525":"# A Simple Face Recognition Model","3daf8ff9":"### This notebook is a continuation of the notebook : https:\/\/www.kaggle.com\/nandinibagga\/simple-guide-to-store-data-for-facial-recognition","51a73d93":"In the dataset we have, cropped images of my face in grayscale. ","6a4c75fd":"You can read about LBPH Recognizer here: [LBPH Recognizer](https:\/\/towardsdatascience.com\/face-recognition-how-lbph-works-90ec258c3d6b)","2370246a":"I have labelled my photos as IDs 1,2,3,4,5.\n\n","97592b18":"![image.png](attachment:6c639370-202a-4bcd-8fe9-67025142eca5.png)"}}