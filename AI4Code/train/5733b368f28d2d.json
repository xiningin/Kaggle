{"cell_type":{"fa34b7a1":"code","e9ed2081":"code","23ee1628":"code","60ba6afc":"code","802b599d":"code","e18daeef":"code","f3c5436b":"code","375e7f52":"code","ed2b40d1":"code","3a33afbe":"code","84cc4336":"code","40395c77":"code","c59c364a":"code","6c1b7cf6":"code","83d6abe1":"code","48163ef6":"code","e662ef79":"code","1f492bcf":"code","e9998905":"code","5287d049":"code","2413a961":"code","393b32be":"markdown","0b44f253":"markdown","48ac1580":"markdown","ec4dde15":"markdown","2acf5615":"markdown","ef874b89":"markdown"},"source":{"fa34b7a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport zipfile\nwith zipfile.ZipFile('..\/input\/platesv2\/plates.zip', 'r') as zip_obj:\n   # Extract all the contents of zip file in current directory\n   zip_obj.extractall('\/kaggle\/working\/')\n    \nprint('After zip extraction:')\nprint(os.listdir(\"\/kaggle\/working\/\"))","e9ed2081":"data_root = '\/kaggle\/working\/plates\/'\nprint(os.listdir(data_root))","23ee1628":"import shutil \nfrom tqdm import tqdm\n\ntrain_dir = 'train'\nval_dir = 'val'\n\nclass_names = ['cleaned', 'dirty']\n\nfor dir_name in [train_dir, val_dir]:\n    for class_name in class_names:\n        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n\nfor class_name in class_names:\n    source_dir = os.path.join(data_root, 'train', class_name)\n    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        if i % 5 != 0:\n            dest_dir = os.path.join(train_dir, class_name) \n        else:\n            dest_dir = os.path.join(val_dir, class_name)\n        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))","60ba6afc":"!ls train","802b599d":"import torch\nimport torchvision\nimport matplotlib.pyplot as plt\nimport time\nimport copy\n\nfrom torchvision import transforms, models","e18daeef":"train_transforms_1 = transforms.Compose([\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])  # Normalize for ResNet input\n])\n\ntrain_transforms_2 = transforms.Compose([\n    transforms.CenterCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(0.6, 0.6, 0.3, 0.3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ntrain_transforms_3 = transforms.Compose([\n    transforms.CenterCrop(224),\n    transforms.RandomRotation(degrees=60),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset_1 = torchvision.datasets.ImageFolder(train_dir, train_transforms_1)\ntrain_dataset_2 = torchvision.datasets.ImageFolder(train_dir, train_transforms_2)\ntrain_dataset_3 = torchvision.datasets.ImageFolder(train_dir, train_transforms_3)\n\n\n\ntrain_dataset = torch.utils.data.ConcatDataset([train_dataset_1, train_dataset_2, train_dataset_3])\nval_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)","f3c5436b":"batch_size = 15\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=10)\nval_dataloader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=10)","375e7f52":"len(train_dataloader), len(train_dataset)","ed2b40d1":"X_batch, y_batch = next(iter(train_dataloader))\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\nplt.imshow(X_batch[0].permute(1, 2, 0).numpy() * std + mean);","3a33afbe":"def show_input(input_tensor, title=''):\n    image = input_tensor.permute(1, 2, 0).numpy()\n    image = std * image + mean\n    plt.imshow(image.clip(0, 1))\n    plt.title(title)\n    plt.show()\n    plt.pause(0.001)\n\nX_batch, y_batch = next(iter(train_dataloader))\n\nfor x_item, y_item in zip(X_batch, y_batch):\n    show_input(x_item, title=class_names[y_item])","84cc4336":"def train_model(model, loss, optimizer, scheduler, num_epochs):\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}:'.format(epoch, num_epochs - 1), flush=True)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                dataloader = train_dataloader\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                dataloader = val_dataloader\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.\n            running_acc = 0.\n\n            # Iterate over data.\n            for inputs, labels in tqdm(dataloader):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                # forward and backward\n                with torch.set_grad_enabled(phase == 'train'):\n                    preds = model(inputs)\n                    loss_value = loss(preds, labels)\n                    preds_class = preds.argmax(dim=1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss_value.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss_value.item()\n                running_acc += (preds_class == labels.data).float().mean()\n\n            epoch_loss = running_loss \/ len(dataloader)\n            epoch_acc = running_acc \/ len(dataloader)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n\n    return model","40395c77":"model = models.resnet34(pretrained=True)\n\n# Disable grad for all conv layers\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),amsgrad=True, lr=1.0e-3)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","c59c364a":"train_model(model, loss, optimizer, scheduler, num_epochs=50);","6c1b7cf6":"test_dir = 'test'\nshutil.copytree(os.path.join(data_root, 'test'), os.path.join(test_dir, 'unknown'))","83d6abe1":"class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path\n    \ntest_dataset = ImageFolderWithPaths('\/kaggle\/working\/test', val_transforms)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)","48163ef6":"test_dataset","e662ef79":"model.eval()\n\ntest_predictions = []\ntest_img_paths = []\n\nfor inputs, labels, paths in tqdm(test_dataloader):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    \n    with torch.set_grad_enabled(False):\n        preds = model(inputs)\n    test_predictions.append(\n        torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n    test_img_paths.extend(paths)\n    \ntest_predictions = np.concatenate(test_predictions)","1f492bcf":"inputs, labels, paths = next(iter(test_dataloader))\n\nfor img, pred in zip(inputs, test_predictions):\n    show_input(img, title=pred)","e9998905":"submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})","5287d049":"submission_df.to_csv('submission.csv')","2413a961":"!rm -rf train val test # return the directory to its original state","393b32be":"# Test data preprocessing","0b44f253":"# Building a model","48ac1580":"# Dividing the training dataset into training and validation parts","ec4dde15":"**Visualization of data**","2acf5615":"# Train data preprocessing","ef874b89":"# Evaluation"}}