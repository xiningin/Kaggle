{"cell_type":{"81488f6e":"code","914ffa37":"code","80192772":"code","9c264119":"code","2f1be5f5":"code","6f91e718":"code","1ba6e50c":"code","805f0291":"code","6501204d":"code","c98d1ec1":"code","ac42797a":"code","53f2a72f":"code","f82f3171":"code","69e91fe2":"code","f439d995":"code","755cb120":"code","670e0d2d":"code","454235ee":"code","cbb53904":"code","03782670":"code","8f25072b":"markdown"},"source":{"81488f6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","914ffa37":"# Importing the Required Library\nimport pandas as  pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout,Dense,Activation,Conv2D,MaxPooling2D,Flatten, BatchNormalization, MaxPool2D\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","80192772":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\nx_test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntrain.head()","9c264119":"# taking the independent feature and the dependent feature in xtrain and ytrain of the traning data\nx_train = train.drop(columns = 'label')\ny_train = train['label']","2f1be5f5":"x_test.shape,x_train.shape,y_train.shape","6f91e718":"sns.countplot(y_train)","1ba6e50c":"#data processing\nx_train = x_train.values.reshape(-1, int(np.sqrt(784)), int(np.sqrt(784)), 1)\/255.0\nx_test =  x_test.values.reshape(-1, int(np.sqrt(784)), int(np.sqrt(784)), 1)\/255.0","805f0291":"x_test.shape,x_train.shape,y_train.shape","6501204d":"rows = 5\ncols = 6\ncounter = 0\nfig = plt.figure(figsize=(15,7))\nfor i in range(1, rows*cols+1):\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(np.squeeze(x_train[counter + i-1]), cmap='gray')\n    plt.title(y_train[counter + i-1], fontsize=16)\n    plt.axis(False)\n    fig.add_subplot\ncounter += rows*cols","c98d1ec1":"y_train = to_categorical(y_train, num_classes=10)","ac42797a":"x_test.shape,x_train.shape,y_train.shape","53f2a72f":"x_train_trim ,x_valid , y_train_trim, y_valid = train_test_split(x_train, y_train, test_size= 0.1 ,random_state = 1455)\nprint(f'Training Set size: {x_train_trim.shape[0]}')\nprint(f'Validation Set size: {x_valid.shape[0]}')","f82f3171":"model = Sequential()#69\ninput_shape = (28,28,1)\nmodel.add(Conv2D(32, (5, 5), input_shape=input_shape,activation='relu', padding='same'))\nmodel.add(Conv2D(32, (5, 5), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(128, (3, 3),activation='relu',padding='same'))\nmodel.add(Conv2D(128, (3, 3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\nmodel.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))","69e91fe2":"from keras.utils import plot_model\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\nfrom IPython.display import Image\nImage(\"model.png\")","f439d995":"#Optimizer & model compile\n#optimizer_rmsprop = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)\nadam = Adam(lr = .00001)\nmodel.compile(optimizer=adam, \n              loss = 'categorical_crossentropy',  \n              metrics = ['accuracy'])","755cb120":"# With data augmentation to prevent overfitting (accuracy 0.99286)\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=23,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.3, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ndatagen.fit(x_train_trim)","670e0d2d":"def build_lrfn(lr_start=1e-4, lr_max=1e-3, \n               lr_min=0, lr_rampup_epochs=16, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n                                - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn\n\nlrfn = build_lrfn()\nlr_schedule = LearningRateScheduler(lrfn, verbose=True)\n\n#Usually monitor='val_accuracy' should be tracked here. Since the training set is smaller let keep it limited to accuracy\ncheckpoint = ModelCheckpoint(\n    filepath='best_weights.hdf5',\n    save_weights_only=True,\n    monitor='accuracy',\n    mode='max',\n    save_best_only=True)","454235ee":"train_history = model.fit_generator(datagen.flow(x_train_trim, y_train_trim, \n                                                 batch_size=512),\n                                    epochs = 50,\n                                    validation_data = (x_valid,y_valid),\n                                    verbose = 2, \n                                    steps_per_epoch=x_train_trim.shape[0] \/\/512,\n                                    callbacks=[lr_schedule,checkpoint])","cbb53904":"y_pred = model.predict(x_test)\ny_pred = np.argmax(y_pred,axis=1)","03782670":"my_submission = pd.DataFrame({'ImageId': list(range(1, len(y_pred)+1)), 'Label': y_pred})\nmy_submission.to_csv('submission.csv', index=False)","8f25072b":"Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\nEpoch 1\/50\n147\/147 - 9s - loss: 0.5003 - accuracy: 0.8416 - val_loss: 2.4651 - val_accuracy: 0.1012 - lr: 1.0000e-04\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 0.00015625.\nEpoch 2\/50\n147\/147 - 9s - loss: 0.1386 - accuracy: 0.9583 - val_loss: 1.9740 - val_accuracy: 0.2471 - lr: 1.5625e-04\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 0.00021250000000000002.\nEpoch 3\/50\n147\/147 - 9s - loss: 0.0981 - accuracy: 0.9701 - val_loss: 0.4611 - val_accuracy: 0.8886 - lr: 2.1250e-04\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 0.00026875.\nEpoch 4\/50\n147\/147 - 9s - loss: 0.0852 - accuracy: 0.9740 - val_loss: 0.0495 - val_accuracy: 0.9876 - lr: 2.6875e-04\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 0.000325.\nEpoch 5\/50\n147\/147 - 10s - loss: 0.0680 - accuracy: 0.9794 - val_loss: 0.0466 - val_accuracy: 0.9874 - lr: 3.2500e-04\n\nEpoch 00006: LearningRateScheduler reducing learning rate to 0.00038124999999999997.\nEpoch 6\/50\n147\/147 - 9s - loss: 0.0651 - accuracy: 0.9806 - val_loss: 0.0603 - val_accuracy: 0.9824 - lr: 3.8125e-04\n\nEpoch 00007: LearningRateScheduler reducing learning rate to 0.0004375.\nEpoch 7\/50\n147\/147 - 9s - loss: 0.0532 - accuracy: 0.9840 - val_loss: 0.0416 - val_accuracy: 0.9876 - lr: 4.3750e-04\n\nEpoch 00008: LearningRateScheduler reducing learning rate to 0.00049375.\nEpoch 8\/50\n147\/147 - 9s - loss: 0.0477 - accuracy: 0.9859 - val_loss: 0.0410 - val_accuracy: 0.9883 - lr: 4.9375e-04\n\n\nEpoch 00009: LearningRateScheduler reducing learning rate to 0.00055.\nEpoch 9\/50\n147\/147 - 9s - loss: 0.0470 - accuracy: 0.9858 - val_loss: 0.1555 - val_accuracy: 0.9540 - lr: 5.5000e-04\n\nEpoch 00010: LearningRateScheduler reducing learning rate to 0.00060625.\nEpoch 10\/50\n147\/147 - 9s - loss: 0.0484 - accuracy: 0.9851 - val_loss: 0.0368 - val_accuracy: 0.9912 - lr: 6.0625e-04\n\nEpoch 00011: LearningRateScheduler reducing learning rate to 0.0006625.\nEpoch 11\/50\n147\/147 - 9s - loss: 0.0425 - accuracy: 0.9882 - val_loss: 0.0506 - val_accuracy: 0.9864 - lr: 6.6250e-04\n\nEpoch 00012: LearningRateScheduler reducing learning rate to 0.00071875.\nEpoch 12\/50\n147\/147 - 9s - loss: 0.0372 - accuracy: 0.9890 - val_loss: 0.0984 - val_accuracy: 0.9750 - lr: 7.1875e-04\n\nEpoch 00013: LearningRateScheduler reducing learning rate to 0.0007750000000000001.\nEpoch 13\/50\n147\/147 - 9s - loss: 0.0402 - accuracy: 0.9879 - val_loss: 0.1463 - val_accuracy: 0.9598 - lr: 7.7500e-04\n\nEpoch 00014: LearningRateScheduler reducing learning rate to 0.0008312500000000001.\nEpoch 14\/50\n147\/147 - 9s - loss: 0.0336 - accuracy: 0.9897 - val_loss: 0.0246 - val_accuracy: 0.9921 - lr: 8.3125e-04\n\nEpoch 00015: LearningRateScheduler reducing learning rate to 0.0008875.\nEpoch 15\/50\n147\/147 - 9s - loss: 0.0369 - accuracy: 0.9891 - val_loss: 0.0789 - val_accuracy: 0.9790 - lr: 8.8750e-04\n\nEpoch 00016: LearningRateScheduler reducing learning rate to 0.00094375.\nEpoch 16\/50\n147\/147 - 9s - loss: 0.0340 - accuracy: 0.9901 - val_loss: 0.0334 - val_accuracy: 0.9907 - lr: 9.4375e-04\n\nEpoch 00017: LearningRateScheduler reducing learning rate to 0.001.\nEpoch 17\/50\n147\/147 - 9s - loss: 0.0308 - accuracy: 0.9905 - val_loss: 0.0454 - val_accuracy: 0.9867 - lr: 0.0010\n\nEpoch 00018: LearningRateScheduler reducing learning rate to 0.0008.\nEpoch 18\/50\n147\/147 - 9s - loss: 0.0268 - accuracy: 0.9919 - val_loss: 0.0313 - val_accuracy: 0.9895 - lr: 8.0000e-04\n\nEpoch 00019: LearningRateScheduler reducing learning rate to 0.0006400000000000002.\nEpoch 19\/50\n147\/147 - 9s - loss: 0.0201 - accuracy: 0.9940 - val_loss: 0.0332 - val_accuracy: 0.9929 - lr: 6.4000e-04\n\nEpoch 00020: LearningRateScheduler reducing learning rate to 0.0005120000000000001.\nEpoch 20\/50\n147\/147 - 9s - loss: 0.0167 - accuracy: 0.9950 - val_loss: 0.0324 - val_accuracy: 0.9898 - lr: 5.1200e-04\n\nEpoch 00021: LearningRateScheduler reducing learning rate to 0.0004096000000000001.\nEpoch 21\/50\n147\/147 - 9s - loss: 0.0130 - accuracy: 0.9961 - val_loss: 0.0272 - val_accuracy: 0.9921 - lr: 4.0960e-04\n\nEpoch 00022: LearningRateScheduler reducing learning rate to 0.0003276800000000001.\nEpoch 22\/50\n147\/147 - 9s - loss: 0.0112 - accuracy: 0.9968 - val_loss: 0.0203 - val_accuracy: 0.9950 - lr: 3.2768e-04\n\n\nEpoch 00023: LearningRateScheduler reducing learning rate to 0.0002621440000000001.\nEpoch 23\/50\n147\/147 - 9s - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0226 - val_accuracy: 0.9938 - lr: 2.6214e-04\n\nEpoch 00024: LearningRateScheduler reducing learning rate to 0.0002097152000000001.\nEpoch 24\/50\n147\/147 - 9s - loss: 0.0083 - accuracy: 0.9972 - val_loss: 0.0181 - val_accuracy: 0.9945 - lr: 2.0972e-04\n\nEpoch 00025: LearningRateScheduler reducing learning rate to 0.0001677721600000001.\nEpoch 25\/50\n147\/147 - 9s - loss: 0.0090 - accuracy: 0.9974 - val_loss: 0.0201 - val_accuracy: 0.9940 - lr: 1.6777e-04\n\nEpoch 00026: LearningRateScheduler reducing learning rate to 0.00013421772800000008.\nEpoch 26\/50\n147\/147 - 9s - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.0179 - val_accuracy: 0.9948 - lr: 1.3422e-04\n\nEpoch 00027: LearningRateScheduler reducing learning rate to 0.00010737418240000006.\nEpoch 27\/50\n147\/147 - 9s - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.0219 - val_accuracy: 0.9940 - lr: 1.0737e-04\n\nEpoch 00028: LearningRateScheduler reducing learning rate to 8.589934592000005e-05.\nEpoch 28\/50\n147\/147 - 9s - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0209 - val_accuracy: 0.9950 - lr: 8.5899e-05\n\nEpoch 00029: LearningRateScheduler reducing learning rate to 6.871947673600005e-05.\nEpoch 29\/50\n147\/147 - 9s - loss: 0.0059 - accuracy: 0.9980 - val_loss: 0.0185 - val_accuracy: 0.9955 - lr: 6.8719e-05\n\nEpoch 00030: LearningRateScheduler reducing learning rate to 5.497558138880004e-05.\nEpoch 30\/50\n147\/147 - 9s - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0179 - val_accuracy: 0.9952 - lr: 5.4976e-05\n\nEpoch 00031: LearningRateScheduler reducing learning rate to 4.398046511104004e-05.\nEpoch 31\/50\n147\/147 - 9s - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0202 - val_accuracy: 0.9952 - lr: 4.3980e-05\n\nEpoch 00032: LearningRateScheduler reducing learning rate to 3.518437208883203e-05.\nEpoch 32\/50\n147\/147 - 9s - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.0199 - val_accuracy: 0.9952 - lr: 3.5184e-05\n\nEpoch 00033: LearningRateScheduler reducing learning rate to 2.8147497671065623e-05.\nEpoch 33\/50\n147\/147 - 9s - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0191 - val_accuracy: 0.9955 - lr: 2.8147e-05\n\nEpoch 00034: LearningRateScheduler reducing learning rate to 2.2517998136852502e-05.\nEpoch 34\/50\n147\/147 - 9s - loss: 0.0042 - accuracy: 0.9985 - val_loss: 0.0192 - val_accuracy: 0.9955 - lr: 2.2518e-05\n\nEpoch 00035: LearningRateScheduler reducing learning rate to 1.8014398509482003e-05.\nEpoch 35\/50\n147\/147 - 9s - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0192 - val_accuracy: 0.9955 - lr: 1.8014e-05\n\nEpoch 00036: LearningRateScheduler reducing learning rate to 1.4411518807585603e-05.\nEpoch 36\/50\n147\/147 - 9s - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0193 - val_accuracy: 0.9957 - lr: 1.4412e-05\n\nEpoch 00037: LearningRateScheduler reducing learning rate to 1.1529215046068483e-05.\nEpoch 37\/50\n147\/147 - 9s - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0186 - val_accuracy: 0.9957 - lr: 1.1529e-05\n\nEpoch 00038: LearningRateScheduler reducing learning rate to 9.223372036854787e-06.\nEpoch 38\/50\n147\/147 - 9s - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.0188 - val_accuracy: 0.9955 - lr: 9.2234e-06\n\nEpoch 00039: LearningRateScheduler reducing learning rate to 7.37869762948383e-06.\nEpoch 39\/50\n147\/147 - 9s - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.0185 - val_accuracy: 0.9955 - lr: 7.3787e-06\n\nEpoch 00040: LearningRateScheduler reducing learning rate to 5.902958103587064e-06.\nEpoch 40\/50\n147\/147 - 9s - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0187 - val_accuracy: 0.9957 - lr: 5.9030e-06\n\nEpoch 00041: LearningRateScheduler reducing learning rate to 4.722366482869652e-06.\nEpoch 41\/50\n147\/147 - 9s - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0185 - val_accuracy: 0.9957 - lr: 4.7224e-06\n\nEpoch 00042: LearningRateScheduler reducing learning rate to 3.7778931862957216e-06.\nEpoch 42\/50\n147\/147 - 9s - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.0185 - val_accuracy: 0.9957 - lr: 3.7779e-06\n\nEpoch 00043: LearningRateScheduler reducing learning rate to 3.0223145490365774e-06.\nEpoch 43\/50\n147\/147 - 9s - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0186 - val_accuracy: 0.9957 - lr: 3.0223e-06\n\n\nEpoch 00044: LearningRateScheduler reducing learning rate to 2.417851639229262e-06.\nEpoch 44\/50\n73\/73 - 9s - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0237 - val_accuracy: 0.9960 - lr: 2.4179e-06\n\nEpoch 00045: LearningRateScheduler reducing learning rate to 1.93428131138341e-06.\nEpoch 45\/50\n73\/73 - 9s - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0237 - val_accuracy: 0.9960 - lr: 1.9343e-06\n\nEpoch 00046: LearningRateScheduler reducing learning rate to 1.547425049106728e-06.\nEpoch 46\/50\n73\/73 - 9s - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0238 - val_accuracy: 0.9960 - lr: 1.5474e-06\n\nEpoch 00047: LearningRateScheduler reducing learning rate to 1.2379400392853823e-06.\nEpoch 47\/50\n73\/73 - 9s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0238 - val_accuracy: 0.9960 - lr: 1.2379e-06\n\nEpoch 00048: LearningRateScheduler reducing learning rate to 9.903520314283058e-07.\nEpoch 48\/50\n73\/73 - 9s - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0238 - val_accuracy: 0.9960 - lr: 9.9035e-07\n\nEpoch 00049: LearningRateScheduler reducing learning rate to 7.922816251426449e-07.\nEpoch 49\/50\n73\/73 - 9s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0238 - val_accuracy: 0.9960 - lr: 7.9228e-07\n\nEpoch 00050: LearningRateScheduler reducing learning rate to 6.338253001141158e-07.\nEpoch 50\/50\n73\/73 - 9s - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0237 - val_accuracy: 0.9960 - lr: 6.3383e-07"}}