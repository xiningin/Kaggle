{"cell_type":{"b20a07fc":"code","5f99fd32":"code","24008061":"code","24cdc1cf":"code","2f114126":"code","33e65508":"code","d329cdbc":"code","d026e19e":"code","600ccf8f":"markdown","4f896b62":"markdown","a0be8591":"markdown","d15f5ce3":"markdown","f60578dd":"markdown","313e48b7":"markdown"},"source":{"b20a07fc":"import numpy as np, pandas as pd \nimport os\nimport zipfile\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline","5f99fd32":"!wget https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-development.tsv\nnrows = 10\ndata = pd.read_csv(\"gap-development.tsv\", sep = '\\t', nrows = nrows)","24008061":"#downloading weights and cofiguration file for bert\n!wget https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-24_H-1024_A-16.zip\nwith zipfile.ZipFile(\"uncased_L-24_H-1024_A-16.zip\",\"r\") as zip_ref:\n    zip_ref.extractall()\n!rm \"uncased_L-24_H-1024_A-16.zip\"","24cdc1cf":"!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/modeling.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/extract_features.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/tokenization.py\n\nimport modeling\nimport extract_features\nimport tokenization\nimport tensorflow as tf","2f114126":"def compute_offset_no_spaces(text, offset):\n\tcount = 0\n\tfor pos in range(offset):\n\t\tif text[pos] != \" \": count +=1\n\treturn count\n\ndef count_chars_no_special(text):\n\tcount = 0\n\tspecial_char_list = [\"#\"]\n\tfor pos in range(len(text)):\n\t\tif text[pos] not in special_char_list: count +=1\n\treturn count\n\ndef count_length_no_special(text):\n\tcount = 0\n\tspecial_char_list = [\"#\", \" \"]\n\tfor pos in range(len(text)):\n\t\tif text[pos] not in special_char_list: count +=1\n\treturn count","33e65508":"text = data[\"Text\"]\ntext.to_csv(\"input.txt\", index = False, header = False)\n\nos.system(\"python3 extract_features.py \\\n  --input_file=input.txt \\\n  --output_file=output.jsonl \\\n  --vocab_file=uncased_L-24_H-1024_A-16\/vocab.txt \\\n  --bert_config_file=uncased_L-24_H-1024_A-16\/bert_config.json \\\n  --init_checkpoint=uncased_L-24_H-1024_A-16\/bert_model.ckpt \\\n  --layers=-1,-5 \\\n  --max_seq_length=256 \\\n  --batch_size=8\")\n\nbert_output = pd.read_json(\"output.jsonl\", lines = True)\nos.system(\"rm output.jsonl\")\nos.system(\"rm input.txt\")","d329cdbc":"emb_2d = {}\nfor row in range(nrows):\n    P = data.loc[row,\"Pronoun\"].lower()\n    A = data.loc[row,\"A\"].lower()\n    B = data.loc[row,\"B\"].lower()\n    P_offset = compute_offset_no_spaces(data.loc[row,\"Text\"], data.loc[row,\"Pronoun-offset\"])\n    A_offset = compute_offset_no_spaces(data.loc[row,\"Text\"], data.loc[row,\"A-offset\"])\n    B_offset = compute_offset_no_spaces(data.loc[row,\"Text\"], data.loc[row,\"B-offset\"])\n    # Figure out the length of A, B, not counting spaces or special characters\n    A_length = count_length_no_special(A)\n    B_length = count_length_no_special(B)\n    \n    # Get the BERT embeddings for the current line in the data file\n    features = pd.DataFrame(bert_output.loc[row,\"features\"]) \n    \n    span = range(2,len(features)-2)\n    emb1, emb5 = {}, {}\n    count_chars = 0\n    \n    # Make a list with the text of each token, to be used in the plots\n    texts = []\n\n    for j in span:\n        token = features.loc[j,'token']\n        texts.append(token)\n        emb1[j] = np.array(features.loc[j,'layers'][0]['values'])\n        emb5[j] = np.array(features.loc[j,'layers'][1]['values'])\n        if count_chars == P_offset:\n            texts.pop()\n            texts.append(\"@P\" + token)\n        if count_chars in range(A_offset, A_offset + A_length): \n            texts.pop()\n            if data.loc[row,\"A-coref\"]:\n                texts.append(\"@G\" + token)\n            else:\n                texts.append(\"@R\" + token)\n        if count_chars in range(B_offset, B_offset + B_length): \n            texts.pop()\n            if data.loc[row,\"B-coref\"]:\n                texts.append(\"@G\" + token)\n            else:\n                texts.append(\"@R\" + token)\n        count_chars += count_length_no_special(token)\n    \n    X1 = np.array(list(emb1.values()))\n    X5 = np.array(list(emb5.values()))\n    if row == 0: print(\"Shape of embedding matrix: \", X1.shape)\n\n    # Use PCA to reduce dimensions to a number that's manageable for t-SNE\n    pca = PCA(n_components = 50, random_state = 7)\n    X1 = pca.fit_transform(X1)\n    X5 = pca.fit_transform(X5)\n    if row == 0: print(\"Shape after PCA: \", X1.shape)\n\n    # Reduce dimensionality to 2 with t-SNE.\n    # Perplexity is roughly the number of close neighbors you expect a\n    # point to have. Our data is sparse, so we chose a small value, 10.\n    # The KL divergence objective is non-convex, so the result is different\n    # depending on the seed used.\n    tsne = TSNE(n_components = 2, perplexity = 10, random_state = 6, \n                learning_rate = 1000, n_iter = 1500)\n    X1 = tsne.fit_transform(X1)\n    X5 = tsne.fit_transform(X5)\n    if row == 0: print(\"Shape after t-SNE: \", X1.shape)\n    \n    # Recording the position of the tokens, to be used in the plot\n    position = np.array(list(span)) \n    position = position.reshape(-1,1)\n    \n    X = pd.DataFrame(np.concatenate([X1, X5, position, np.array(texts).reshape(-1,1)], axis = 1), \n                     columns = [\"x1\", \"y1\", \"x5\", \"y5\", \"position\", \"texts\"])\n    X = X.astype({\"x1\": float, \"y1\": float, \"x5\": float, \"y5\": float, \"position\": float, \"texts\": object})\n\n    # Remove a few outliers based on zscore\n    X = X[(np.abs(stats.zscore(X[[\"x1\", \"y1\", \"x5\", \"y5\"]])) < 3).all(axis=1)]\n    emb_2d[row] = X","d026e19e":"for row in range(nrows):\n    X = emb_2d[row]\n    \n    # Plot for layer -1\n    plt.figure(figsize = (20,15))\n    p1 = sns.scatterplot(x = X[\"x1\"], y = X[\"y1\"], hue = X[\"position\"], palette = \"coolwarm\")\n    p1.set_title(\"development-\"+str(row+1)+\", layer -1\")\n    \n    # Label each datapoint with the word it corresponds to\n    for line in X.index:\n        text = X.loc[line,\"texts\"]\n        if \"@P\" in text:\n            p1.text(X.loc[line,\"x1\"]+0.2, X.loc[line,\"y1\"], text[2:], horizontalalignment='left', \n                    size='medium', color='blue', weight='semibold')\n        elif \"@G\" in text:\n            p1.text(X.loc[line,\"x1\"]+0.2, X.loc[line,\"y1\"], text[2:], horizontalalignment='left', \n                    size='medium', color='green', weight='semibold')\n        elif \"@R\" in text:\n            p1.text(X.loc[line,\"x1\"]+0.2, X.loc[line,\"y1\"], text[2:], horizontalalignment='left', \n                    size='medium', color='red', weight='semibold')\n        else:\n            p1.text(X.loc[line,\"x1\"]+0.2, X.loc[line,\"y1\"], text, horizontalalignment='left', \n                    size='medium', color='black', weight='semibold')\n    \n    # Plot for layer -5\n    plt.figure(figsize = (20,15))\n    p1 = sns.scatterplot(x = X[\"x5\"], y = X[\"y5\"], hue = X[\"position\"], palette = \"coolwarm\")\n    p1.set_title(\"development-\"+str(row+1)+\", layer -5\")\n    \n    for line in X.index:\n        text = X.loc[line,\"texts\"]\n        if \"@P\" in text:\n            p1.text(X.loc[line,\"x5\"]+0.2, X.loc[line,\"y5\"], text[2:], horizontalalignment='left', \n                    size='medium', color='blue', weight='semibold')\n        elif \"@G\" in text:\n            p1.text(X.loc[line,\"x5\"]+0.2, X.loc[line,\"y5\"], text[2:], horizontalalignment='left', \n                    size='medium', color='green', weight='semibold')\n        elif \"@R\" in text:\n            p1.text(X.loc[line,\"x5\"]+0.2, X.loc[line,\"y5\"], text[2:], horizontalalignment='left', \n                    size='medium', color='red', weight='semibold')\n        else:\n            p1.text(X.loc[line,\"x5\"]+0.2, X.loc[line,\"y5\"], text, horizontalalignment='left', \n                    size='medium', color='black', weight='semibold') ","600ccf8f":"I intend to play with these tools more, and update the kernel if I have any new insights. But let me end with a disclaimer: here's a great [explanation](https:\/\/distill.pub\/2016\/misread-tsne\/) of some of the pitfalls of t-SNE visualizations.","4f896b62":"A lot of us used BERT embeddings in this competition. I think I'm not wrong when I say that, for most of us, it's a bit mysterious precisely what information about the text gets encoded in BERT's layers. So I decided to make some visualizations of the embeddings, using a dimensional reduction algorithm called [t-SNE](https:\/\/lvdmaaten.github.io\/tsne\/). Here's a sketch:\n1. For BERT-large, each word is represented as a vector in a 1024-dimensional space;\n2. Using a good old PCA, we reduce this to a 50-dimensional space, hopefully without losing too much information;\n3. Using PCA to further reduce from 50 to 2 dimensions would probably kill a lot of useful information, so we want a more refined method. What t-SNE does, roughly, is create vectors in a 2-dimensional space, such that if two vectors have small distance in the 50-dimensional space, they also have small distance in the 2-dimensional space. We get a 2-dimensional plot, which offers a little insight into how BERT embeddings for various words are distributed.\n\nI have a concrete question that I'd like to answer using these plots. I think a lot of people noticed that you can concatenate different layers of BERT, not necessarily the last ones. For my team, what worked best was concatenating layers -4, -5, -6. We will talk more about our solution elsewhere. But just to give you an idea, here are some experiments which I did with the model from [my previous kernel](https:\/\/www.kaggle.com\/mateiionita\/taming-the-bert-a-baseline). After replacing BERT-base with BERT-large, and concatenating embeddings coming from two layers only, I get the following results:\n\nWith layers -5, -6:\nCV mean score: 0.4666, std: 0.0278.\nTest score: 0.41730251922932554\n\nWith layers -3, -4:\nCV mean score: 0.4929, std: 0.0267.\nTest score: 0.45579418221937407\n\nWith layers -1, -2:\nCV mean score: 0.5311, std: 0.0205.\nTest score: 0.49026846792574713\n\nIt's pretty clear that layers -5, -6 are much better suited for this problem than the first 4. So in the graphs below, I took the first 10 examples from gap-development, and I'm plotting the result of t-SNE for layer -1, and separately for layer -5. Hopefully staring long enough at plots like these can reveal something about the different ways in which BERT layers encode information.","a0be8591":"Passing the 10 GAP examples through BERT, and saving layers -1, -5","d15f5ce3":"There's some useful information in these plots. Notice two reasons why points are close:\n1. They represent the same word, or similar words, independently of context, such as \"girlfriend\" and \"boyfriend\" in development-1.\n2. They represent tokens which have close positions in the sentence, such as \"episode\" and \"final\" in development-1.\n\nIn some cases, you can see directly from these plots that BERT has learned some information that's very useful for coreference resolution. For example, in development-5, \"she\" and \"rivera\" are very close together.","f60578dd":"Reading just 10 examples from the gap-development file.","313e48b7":"Finally, plot the 2-dimensional representations output by t-SNE. I labeled each datapoint by the token it represents, using blue text for the pronoun, green text for the correct coreferent, and red text for incorrect correferents. The color of the points represents the position of the token in the sentence: blue is towards the beginning, red towards the end."}}