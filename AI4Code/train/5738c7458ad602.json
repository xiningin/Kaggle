{"cell_type":{"a925b9fb":"code","82abecd9":"code","fac9b66b":"code","c9159659":"code","1ce66cf4":"code","08185fcd":"code","02cdbfea":"code","c9e545e3":"code","e0391d84":"code","203708df":"code","cba27a71":"code","4054a873":"code","2aa478af":"code","c70ab2ad":"code","4f803bdd":"code","eb415067":"code","74fd3530":"code","a8da7a27":"code","c47f6baf":"code","5c525074":"code","69ee0d6b":"code","d1948f36":"code","338b057f":"code","61f826bf":"code","32f0c5d9":"code","13164187":"code","5bee7cc6":"code","a16b6f57":"code","2d559622":"code","6331d70e":"code","5d70d1f2":"code","53b503bd":"code","ff089fc9":"code","02dcf9bf":"code","20359509":"code","db5e6586":"code","de8633ad":"code","716e2a36":"code","030f1eab":"code","1896f0e2":"code","9d9c4d8c":"code","b6cd4df6":"code","6d582beb":"code","b5934e31":"code","2eccfc42":"code","15db15b0":"code","9593e435":"code","9f56ac47":"code","b4732008":"code","4a300966":"code","1ab05b22":"code","63f4a783":"code","8484abce":"markdown","287eecc9":"markdown","31a441c3":"markdown","701789b0":"markdown","f7efc56d":"markdown"},"source":{"a925b9fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport gensim\n\nimport nltk\nimport re\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82abecd9":"from nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport string","fac9b66b":"def extract_kw(text):\n    text_tokens = word_tokenize(text.lower())\n    kw = [word for word in text_tokens if word in uniq_kw_lst]\n    if kw == []:\n        return np.nan\n    else:\n        return kw[0]\n\ndef text_preprocess(text):\n    # clean text\n    text = re.sub(r'https?:\\\/\\\/\\S*', '', text, flags=re.MULTILINE) #get rid of urls\n    text = re.sub(r\"[^a-zA-Z]+\", \" \", text) #replace all numbers and any letter that is not a character (a-z or A-Z) with a space ' '\n    text = text.strip('\\n') #strip off all new line characters\n    text = text.strip('\\t') #strip off all tab characters\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text) #remove all (including punctuations) except word and spaces\n    return text\n\ndef remove_stop_words(text):\n    text = text.split()\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops]\n    text = \" \".join(text)\n    return text\n\ndef lemmatization(text):\n    text = [word.lemma_ for word in sp(text)]\n    text=\" \".join(text)\n    return text\n        \ndef tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            if len(word) < 2:\n                continue\n            tokens.append(word.lower())\n    text = \" \".join(tokens)\n    return text\n","c9159659":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_train.head()","1ce66cf4":"df_test=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndf_test.head()","08185fcd":"df_train.info() #7613 rows, 5 cols","02cdbfea":"df_test.info() #3263 rows and 4 columns","c9e545e3":"df_train.groupby('target')['target'].count() #3271 rows where tweet was tagged as '1' and therefore 4342 tweets marked '0'\n\n#Clearly an unbalanced data","e0391d84":"df_train.keyword.nunique()","203708df":"df_train.keyword.value_counts().tail(10)","cba27a71":"#There are a few %20 characters in the keyword. I will drop those to make them clean \ndf_train['keyword'] = df_train['keyword'].str.replace('%20', ' ')\ndf_train.keyword.nunique()","4054a873":"df_test['keyword'] = df_test['keyword'].str.replace('%20', ' ')\ndf_test.keyword.nunique()","2aa478af":"len(df_train[df_train.keyword.isnull()]), len(df_test[df_test.keyword.isnull()])\n#61 nulls in df_train,  #26 nulls in df_test","c70ab2ad":"uniq_kw_lst=list(df_train.keyword.unique())\nuniq_kw_lst.pop(0)\nlen(uniq_kw_lst)","4f803bdd":"uniq_kw_tst_lst=list(df_test.keyword.unique())\nuniq_kw_tst_lst.pop(0)\nlen(uniq_kw_tst_lst)","eb415067":"uniq_kw_tst_lst==uniq_kw_lst #True","74fd3530":"#Fill na of keyword in df_train with keywords extracted from the text field\ndf_train['keyword']=df_train['keyword'].fillna(df_train['text'].apply(lambda x:extract_kw(x)))","a8da7a27":"df_train.head() #a lot fo NaN in keyword field have been filled","c47f6baf":"len(df_train[df_train.keyword.isnull()]) #23 NaN keywords remain out of 61 before","5c525074":"df_train_nan=df_train[df_train.keyword.isnull()]\nlist(df_train_nan['text'])","69ee0d6b":"#dropping NaN keyword rows. \nidx=df_train[df_train.keyword.isnull()].index\ndf_train.drop(idx, inplace=True)\ndf_train[df_train.keyword.isnull()]","d1948f36":"df_train.reset_index(drop=True, inplace=True)\ndf_train.info() #7590 entries","338b057f":"#same operation  with df_test\ndf_test['keyword']=df_test['keyword'].fillna(df_test['text'].apply(lambda x:extract_kw(x)))","61f826bf":"len(df_test[df_test.keyword.isnull()]) #9 left after applying extract_kw","32f0c5d9":"list(df_test[df_test.keyword.isnull()]['text'])","13164187":"df_test.info()","5bee7cc6":"df_test.drop('location', axis=1, inplace=True)\ndf_test.drop_duplicates(inplace=True)\ndf_test.info()","a16b6f57":"df_train[(df_train.location.isnull()) & (df_train.target==1)] #1069 rows","2d559622":"df_train.location.nunique() #3341 unique locations\nlist(df_train.location.unique())\n#I think this list is not right. Majority of the locations are not really locations e.g 'milky way'. Deleting\n#this col \ndf_train.drop('location', inplace=True, axis=1)\ndf_train.head()","6331d70e":"list(df_train.text[:12])","5d70d1f2":"df_train['text']=df_train.text.apply(lambda x: text_preprocess(x))\nlist(df_train.text[:20])","53b503bd":"#applying text_preprocess to testdf as well\ndf_test['text']=df_test.text.apply(lambda x: text_preprocess(x))\nlist(df_test.text[:20])","ff089fc9":"df_train['text']=df_train.text.apply(lambda x: remove_stop_words(x))\ndf_train['text']=df_train.text.apply(lambda x: tokenize_text(x))\ndf_train","02dcf9bf":"df_test['text']=df_test.text.apply(lambda x: remove_stop_words(x))\ndf_test['text']=df_test.text.apply(lambda x: tokenize_text(x))\ndf_test","20359509":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(stop_words={'english'})\nX_text = vectorizer.fit_transform(df_train['text'])\nfeature_names=vectorizer.get_feature_names()\nlen(feature_names)","db5e6586":"X_test_text=vectorizer.transform(df_test['text'])\nfeature_names_test=vectorizer.get_feature_names()","de8633ad":"len(feature_names_test)","716e2a36":"df_temp=pd.DataFrame(X_text.toarray(), columns=feature_names)\ndf_temp.head()","030f1eab":"df_temp_test=pd.DataFrame(X_test_text.toarray(), columns=feature_names_test)\ndf_temp_test.head()","1896f0e2":"one_hot_encoded_training_predictors = pd.get_dummies(df_train.keyword)\none_hot_encoded_test_predictors = pd.get_dummies(df_test.keyword)\ndf_kw_train, df_kw_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)","9d9c4d8c":"X_train=pd.concat([df_kw_train, df_temp], axis=1)\nX_train.head()","b6cd4df6":"X_test=pd.concat([df_kw_test,df_temp_test], axis=1)\nX_test.head()","6d582beb":"y_train=df_train['target']\ny_train.head()","b5934e31":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nclf = LogisticRegression(random_state=0)\nclf.fit(X_train,y_train)","2eccfc42":"pred = clf.predict(X_train)","15db15b0":"accuracy_score(y_train, pred)","9593e435":"print(classification_report(y_train, pred))","9f56ac47":"X_test = X_test.reindex(labels=X_train.columns,axis=1)","b4732008":"y_test_pred = clf.predict(X_test)\n","4a300966":"y_test_pred","1ab05b22":"df_test['target']=y_test_pred\ndf_final=df_test[['id','target']]\ndf_final","63f4a783":"df_final.to_csv('final_submission.csv',index=False)","8484abce":"## Functions","287eecc9":"## Vectorizing and Model application","31a441c3":"## Submission","701789b0":"### Dealing with NaNs in keyword\/clean_kw columns\n\n##### Strategy: Replace NaN with the words in the unique KWs if they are present in the 'text'\n","f7efc56d":"### EDA"}}