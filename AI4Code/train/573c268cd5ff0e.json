{"cell_type":{"ab9635e5":"code","1abd649e":"code","999f1ef4":"code","6568fadf":"code","a44b147e":"code","dfd3ec05":"code","115c5f01":"code","48ff1cf1":"code","8e75bb61":"code","d5587be6":"code","f5706664":"code","327aee19":"code","6207c5fb":"code","39024093":"code","1bed27ad":"code","ad07fdea":"code","6cf2d7a1":"code","9e4e8c89":"markdown","e617c569":"markdown","20391043":"markdown","3f18caa3":"markdown","01a19282":"markdown","830548ce":"markdown","6dd1b7e0":"markdown","12be0759":"markdown","23b28804":"markdown","50c86ecb":"markdown","b6dd7de0":"markdown","54addb5a":"markdown","6b8b3c0f":"markdown","913ea17e":"markdown","529e6f30":"markdown","95e10d6a":"markdown"},"source":{"ab9635e5":"# upgrade transformers and datasets to latest versions\n!pip install --upgrade transformers\n!pip install --upgrade datasets\nimport transformers\nimport datasets\nprint(transformers.__version__)\nprint(datasets.__version__)","1abd649e":"# Make necessary imports\n\n# for array operations \nimport numpy as np \n# PyTorch framework\nimport torch\n# for pretty printing\nfrom pprint import pprint\n# plotting\nfrom matplotlib import pyplot as plt\n# reproducibility\nimport random\n\n# HuggingFace ecosystem\n# tokenizer\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n# model\nfrom transformers import AutoModelForSequenceClassification\n# trainer\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom transformers import AdamW\n# dataset\nfrom datasets import load_dataset, load_metric\n\n# disable WandB defaults\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\n# a seed for reproducibility\nSEED = 42\n# set seed\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nrandom.seed(SEED)\n\n# check for GPU device\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Device available:', device) ","999f1ef4":"raw_data = load_dataset(\"glue\", \"cola\")\n# how does it look like?\nraw_data","6568fadf":"# Sample a data\nraw_data[\"train\"][0]","a44b147e":"# what features are there in data?\n# What are the label names?\nraw_data[\"train\"].features","dfd3ec05":"checkpoint = 'bert-base-uncased'\n# bert tokenizer\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# data collator for dynamic padding as per batch\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","115c5f01":"# cache a pre-trained BERT model for two-class classification\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)","48ff1cf1":"# define a tokenize function\ndef Tokenize_function(example):\n    return tokenizer(example['sentence'], truncation=True)","8e75bb61":"# tokenize entire data\ntokenized_data = raw_data.map(Tokenize_function, batched=True)","d5587be6":"tokenized_data = tokenized_data.remove_columns(['idx','sentence'])\ntokenized_data = tokenized_data.rename_column('label','labels')\ntokenized_data.with_format('pt')","f5706664":"training_args = TrainingArguments('bert-finetuning-cola', \n                                  evaluation_strategy='epoch',\n                                  num_train_epochs=2,\n                                  learning_rate=5e-5,\n                                  weight_decay=0.005,\n                                  per_device_train_batch_size=8,\n                                  per_device_eval_batch_size=8,\n                                  report_to = 'none'\n                                 )","327aee19":"# use the pre-built metrics \ndef compute_metrics(eval_preds):\n    metric = load_metric(\"glue\", \"cola\")\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","6207c5fb":"# formulate a trainer with necessary data, metrics, tokenizer and arguments\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_data[\"train\"],\n    eval_dataset=tokenized_data[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)","39024093":"# Train the model\ntrainer.train()","1bed27ad":"# prepare test data by removing labels\ntest_data = tokenized_data['test'].remove_columns(['labels'])","ad07fdea":"# make predictions\nyhat = trainer.predict(test_data)\nyhat","6cf2d7a1":"# classify labels\npreds = np.argmax(yhat.predictions, axis=1)\npreds","9e4e8c89":"#### ------------------------------------------------ \n#### *Articles So Far In This Series*\n#### -> [[NLP Tutorial] Finish Tasks in Two Lines of Code](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-finish-tasks-in-two-lines-of-code)\n#### -> [[NLP Tutorial] Unwrapping Transformers Pipeline](https:\/\/www.kaggle.com\/rajkumarl\/nlp-unwrapping-transformers-pipeline)\n#### -> [[NLP Tutorial] Exploring Tokenizers](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-exploring-tokenizers)\n#### -> [[NLP Tutorial] Fine-Tuning in TensorFlow](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-in-tensorflow) \n#### -> [[NLP Tutorail] Fine-Tuning in Pytorch](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-in-pytorch) \n#### -> [[NLP Tutorail] Fine-Tuning with Trainer API](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-with-trainer-api) \n#### ------------------------------------------------ ","e617c569":"# Prepare Environment and Data\n\nIn this article we discuss fine-tuning a BERT model on the famous COLA dataset using Trainer API. This requires a GPU environment for faster training and inference.","20391043":"### Thank you for your valuable time!","3f18caa3":"define training arguments","01a19282":"![Image](https:\/\/raw.githubusercontent.com\/RajkumarGalaxy\/dataset\/master\/Images\/z0003.jpg)\n\n> Image by [Author](https:\/\/raw.githubusercontent.com\/RajkumarGalaxy\/dataset\/master\/Images\/z0003.jpg)\n### How to finetune a BERT model on a custom dataset using Trainer API?","830548ce":"# Prediction \n\nPredict the labels for the test data. Remove the `labels` column as expected by the trainer.","6dd1b7e0":"Each data point contains a sentence, its index and its label. What labels are there? What are their positions?","12be0759":"# Tokenizer and Data Collator\n\nWe are about to use a pre-trained Bert_base_uncased model for our fine-tuning. A tokenizer function associated with a data collator can ensure efficient memory usage and quick data handling during training.","23b28804":"# Effortless NLP using HuggingFace's Tranformers Ecosystem","50c86ecb":"### That's the end. We got a good understanding of fine-tuning a BERT model on COLA dataset for a sentiment analysis task with Trainer API!\n\n##### Key reference: [HuggingFace's NLP Course](https:\/\/huggingface.co\/course)","b6dd7de0":"Data is ready now for efficient data loading and faster training.","54addb5a":"Load the COLA Dataset from GLUE benchmark","6b8b3c0f":"How does tokenized data look like?","913ea17e":"`attention_mask`, `input_ids`, `token_type_ids` are the necessary input features and `labels` is the target. Other features are useless in the view of modeling.","529e6f30":"We understand that this dataset consists of the supervised task - *Sequence Classification* with 2 classes: unacceptable [0] and acceptable [1] ","95e10d6a":"# Model Fine-tuning"}}