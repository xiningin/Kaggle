{"cell_type":{"60b804c6":"code","46668a34":"code","7e34478d":"code","ddffec0e":"code","6cff1096":"code","82d6f131":"code","d085cb5c":"code","1b708fb4":"code","ff658e8a":"code","11833520":"code","da73fb03":"code","629f0eea":"code","9864fb83":"code","63864a6a":"code","0ee04f58":"code","fd03e4de":"code","ae7500f6":"code","4453fd87":"code","97e30a54":"code","5ecbf942":"code","df4a762e":"code","05067fa2":"code","bc9f5a4a":"code","33f9c607":"code","d9e4c803":"code","dff67969":"code","aff237d5":"code","e0a78c76":"code","7a4cd4c5":"code","f026870d":"code","ada1d49d":"code","b9ae8bdc":"code","7e4f8dd8":"code","646bad52":"code","5b28def8":"markdown","a4766181":"markdown","e05f2ec0":"markdown","df2dcf72":"markdown","d428d19c":"markdown","183daa66":"markdown","56bf03c1":"markdown","7f60f9d8":"markdown","c6cca5f0":"markdown","e2f45f57":"markdown","deb44ea8":"markdown","12bb5a7d":"markdown","d5a3af82":"markdown","3c839c77":"markdown","ff0c918e":"markdown","b300d05c":"markdown","90ae86c9":"markdown","2fd67eeb":"markdown","fcbd7ca0":"markdown","d66f7153":"markdown","5c4874b3":"markdown","73e21143":"markdown","b882e903":"markdown","985a1af9":"markdown","34f360de":"markdown","46249552":"markdown","0cc685d7":"markdown","83ee5abe":"markdown","41d36a25":"markdown","53fdc766":"markdown","8db5b07d":"markdown","1a3a051e":"markdown","c4ad5d30":"markdown","862874fd":"markdown","516d8d00":"markdown","47efea6d":"markdown","64a0cd6a":"markdown","006f83cd":"markdown","8e12a9a5":"markdown","b5e67aa9":"markdown","3d7daa93":"markdown","589e1d69":"markdown","8aee4c2b":"markdown","e2c67b96":"markdown","e4d2eb02":"markdown","b1684518":"markdown","d0537d24":"markdown","0517b1bd":"markdown"},"source":{"60b804c6":"import pandas as pd # load and manipulate data and for One-Hot Encoding\nimport numpy as np # calculate the mean and standard deviation\nimport matplotlib.pyplot as plt # drawing graphs\nfrom sklearn.tree import DecisionTreeClassifier # a classification tree\nfrom sklearn.tree import plot_tree # draw a classification tree\nfrom sklearn.model_selection import train_test_split # split  data into training and testing sets\nfrom sklearn.model_selection import cross_val_score # cross validation\nfrom sklearn.metrics import confusion_matrix # creates a confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix # draws a confusion matrix\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n#You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","46668a34":"dataset = pd.read_csv('\/kaggle\/input\/Heart_disease_prediction.csv' , header = None)","7e34478d":"dataset.head()","ddffec0e":"dataset.columns = ['age', \n              'sex', \n              'cp', \n              'restbp', \n              'chol', \n              'fbs', \n              'restecg', \n              'thalach', \n              'exang', \n              'oldpeak', \n              'slope', \n              'ca', \n              'thal', \n              'hd']\ndataset.head()","6cff1096":"dataset.dtypes","82d6f131":"dataset['ca'].unique()","d085cb5c":"dataset['thal'].unique()","1b708fb4":"len(dataset.loc[(dataset['ca'] == '?')\n                |\n                (dataset['thal'] == '?')])\n","ff658e8a":"dataset.loc[(dataset['ca'] == '?')\n                |\n                (dataset['thal'] == '?')]\n","11833520":"len(dataset)","da73fb03":"df_no_missing = dataset.loc[(dataset['ca'] != '?')\n                &\n                (dataset['thal'] != '?')]","629f0eea":"len(df_no_missing)","9864fb83":"df_no_missing['ca'].unique()","63864a6a":"df_no_missing['thal'].unique()","0ee04f58":"X = df_no_missing.drop('hd', axis=1).copy()\nX.head()","fd03e4de":"y = df_no_missing['hd'].copy()\ny.head()","ae7500f6":"X.dtypes","4453fd87":"X['cp'].unique()","97e30a54":"pd.get_dummies(X , columns=['cp']).head()","5ecbf942":"X_encoded = pd.get_dummies(X , columns=[ 'cp' , 'restecg', 'slope', 'thal' , 'oldpeak'])\nX_encoded.head()","df4a762e":"y.unique()","05067fa2":"y_not_zero_index = y>0\ny[y_not_zero_index] = 1\ny.unique()","bc9f5a4a":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nclf_dt = DecisionTreeClassifier(random_state=42)\nprint(clf_dt)\nclf_dt = clf_dt.fit(X_train, y_train)","33f9c607":"plot_confusion_matrix(clf_dt, X_test, y_test, display_labels=[\"Does not have HD\", \"Has HD\"])","d9e4c803":"path = clf_dt.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\nccp_alphas = ccp_alphas[:-1]\n\nclf_dts = []\nfor ccp_alpha in ccp_alphas:\n    clf_dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n#     print(clf_dt)\n    clf_dt.fit(X_train, y_train)\n    clf_dts.append(clf_dt)","dff67969":"train_scores = [clf_dt.score(X_train, y_train) for clf_dt in clf_dts]\ntest_scores = [clf_dt.score(X_test, y_test) for clf_dt in clf_dts]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()","aff237d5":"clf_dt = DecisionTreeClassifier(random_state=42, ccp_alpha=0.016)\nscores = cross_val_score(clf_dt, X_train, y_train, cv=5)\ndataset = pd.DataFrame(data={'tree': range(5), 'accuracy': scores})\n\ndataset.plot(x='tree', y='accuracy', marker='o', linestyle='--')","e0a78c76":"alpha_loop_values = []\nfor ccp_alpha in ccp_alphas:\n    clf_dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    scores = cross_val_score(clf_dt, X_train, y_train, cv=5)\n    alpha_loop_values.append([ccp_alpha, np.mean(scores), np.std(scores)])\n    \nalpha_results = pd.DataFrame(alpha_loop_values, \n                             columns=['alpha', 'mean_accuracy', 'std'])\n\nalpha_results.plot(x='alpha', \n                   y='mean_accuracy', \n                   yerr='std', \n                   marker='o', \n                   linestyle='--')","7a4cd4c5":"alpha_results [(alpha_results['alpha'] > 0.014)\n              &\n              (alpha_results['alpha'] < 0.015)]","f026870d":"ideal_ccp_alpha = alpha_results[(alpha_results['alpha'] > 0.014)\n                               &\n                               (alpha_results['alpha'] < 0.015)]['alpha']\nideal_ccp_alpha","ada1d49d":"ideal_ccp_alpha = float(ideal_ccp_alpha)\nideal_ccp_alpha","b9ae8bdc":"clf_dt_pruned = DecisionTreeClassifier(random_state=42, \n                                       ccp_alpha=ideal_ccp_alpha)\nclf_dt_pruned = clf_dt_pruned.fit(X_train, y_train) ","7e4f8dd8":"plot_confusion_matrix(clf_dt_pruned, \n                      X_test, \n                      y_test, \n                      display_labels=[\"Does not have HD\", \"Has HD\"])","646bad52":"plt.figure(figsize=(15,7.5))\nplot_tree(clf_dt_pruned, \n          filled=True, \n          rounded=True, \n          class_names=[\"No HD\", \"Yes HD\"], \n          feature_names=X.columns) \n\npred = clf_dt_pruned.predict([[ 63 , 0 , 1, 130, 235 , 0 , 0 , 150, 0 , 1 , 0 , 1 , 1]])\nprint(pred)","5b28def8":"----","a4766181":"<a id=\"draw-tree\"><\/a>\n# Step 10: Building, Evaluating, Drawing, and Interpreting the Final Classification Tree\n\nNow that we have the ideal value for `alpha` we can build the final **Classification Tree** by setting `ccp_alpha`:","e05f2ec0":"-----","df2dcf72":"<!-- Now we need to deal with **cp** (chest pain), **restecg** (resting electrocardiographic results), **slope** (the slope of the peak exercise ST segment) and **thal** (thalium heart scan).\n -->\n**NOTE:** There are many different ways to do **One-Hot Encoding** in Python. Two of the more popular methods are `ColumnTransformer()` (from **scikit-learn**) and `get_dummies()` (from **pandas**), and the both methods have pros and cons. `ColumnTransformer()` has a very cool feature where it creates a persistent function that can validate data that you get in the future. For example, if you build your **Decision Tree** using a categorical variable **favorite color** that has **red**, **blue** and **green** options, then `ColumnTransformer()` can remember those options and later on when your **Decision Tree** is being used in a production system, if someone says their favorite color is **orange**, then `ColumnTransformer()` can throw an error or handle the situation in some other nice way. The downside of `ColumnTransformer()` is that it turns your data into an array and looses all of the column names, making it harder to verify that your usage of `ColumnTransformer()` worked as you intended it to. In contrast, `get_dummies()` leaves your data in a dataframe and retains the column names, making it much easier to verify that it worked as intended. However, it does not have the persistent behavior that `ColumnTransformer()` has. So, for the sake of learning how **One-Hot Encoding** works, I prefer to use `get_dummies()`. However, once you are comfortable with **One-Hot Encoding**, I encourage you to investigate using `ColumnTransformer()`.\n\nFirst, before we commit to converting **cp** with **One-Hot Encoding**, let's just see what happens when we convert **cp** without saving the results. This will just make it easy to see how `get_dummies()` works.","d428d19c":"We have replaced the column numbers with nice, easy to remember names. Now that we have the data in a **data frame** called **dataset**, we are ready to identify and deal with **Missing Data**.\n","183daa66":"We have finally finished formatting the data for making a **Classification Tree**, so let's do it!!!\n","56bf03c1":"Now let's draw another confusion matrix to see if the pruned tree does better.","7f60f9d8":"The last thing we are going to do is draw the pruned tree and discuss how to interpret it.","c6cca5f0":"So, the good news is that **cp** only contains the values it is supposed to contain, so we will convert it, using **One-Hot Encoding**, into a series of columns that only contains **0s** and **1s**.","e2f45f57":"Hooray!!! We see that the pruned tree is better at classifying patients than the full sized tree. \n\nOf the **34 + 8 = 42** people that did not have heart disease, **34 (81%)** were correctly classified. This is a big improvement over the full sized tree, which only correctly classified **25 (59%)** of the patients without heart disease. Of the **5 + 28 = 33** people with heart disease, **28 (85%)** were correctly classified. Again, this is an improvement over the full sized tree, which only correctly classified **24 (73%)** of the patients with heart disease. Yay for pruning!","deb44ea8":"Hooray!!! Now we have the ideal value for `alpha` and we can build, evaluate and draw the final **Classification Tree**.\n\n----","12bb5a7d":"We see that that they are almost all `float64`, however, two columns, **ca** and **thal**, have the `object` type and one column, **hd** has `int64`.\n\nThe fact that the **ca** and **thal** columns have `object` data types suggests there is something funny going on in them. `object` datatypes are used when there are mixtures of things, like a mixture of numbers and letters. In theory, both **ca** and **thal** should just have a few values representing different categories, so let's investigate what's going on by printing out their unique values. We'll start with **ca**:","d5a3af82":"We see that instead of nice column names, we just have column numbers.  Since nice column names would make it easier to know how to format the data, let's replace the column numbers with the following column names:\n- **age**,\n- **sex**,\n- **cp**, chest pain\n- **restbp**, resting blood pressure (in mm Hg)\n- **chol**, serum cholesterol in mg\/dl\n- **fbs**, fasting blood sugar\n- **restecg**, resting electrocardiographic results\n- **thalach**,  maximum heart rate achieved\n- **exang**, exercise induced angina\n- **oldpeak**, ST depression induced by exercise relative to rest\n- **slope**, the slope of the peak exercise ST segment.\n- **ca**, number of major vessels (0-3) colored by fluoroscopy\n- **thal**, this is short of thalium heart scan.\n- **hd**, diagnosis of heart disease, the predicted attribute","3c839c77":"We have verified that `df_no_missing` does not contain any missing values. **NOTE:** **ca** and **thal** still have the `object` data type. That's OK. Now we are ready to format the data for making a **Classification Tree**.\n\n----","ff0c918e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","b300d05c":"So 6 of the **303** rows, or **2%**, contain missing values. Since **303 - 6 = 297**, and **297** is plenty of data to build a classification tree, we will remove the rows with missing values, rather than try to impute their values. We do this by selecting all of the rows that do not contain question marks in either the **ca** or **thal** columns:","90ae86c9":"Now let's graph the accuracy of the trees using the **Training Dataset** and the **Testing Dataset** as a function of alpha.","2fd67eeb":"Now let's store the ideal value for alpha so that we can use it to build the best tree.","fcbd7ca0":"In the confusion matrix, we see that of the **25 + 17 = 42** people that did not have **Heart Disease**, **25 (59%)** were correctly classified. And of the **9 + 24 = 33** people that have **Heart Disease**, **24 (73%)** were correctly classified. Can we do better? One thing that might be holding this **Classification Tree** back is that it may have **over fit** the training dataset. So let's prune the tree. Pruning, in theory, should solve the over fitting problem and give us better results.\n\n----","d66f7153":"# Step 9: Cost Complexity Pruning Part 2: Cross Validation For Finding the Best Alpha\n\nThe graphs we just drew suggest one value for alpha, **0.016**, but another\nset of data might suggest another optimal value. \n\n<!-- **Terminology Alert!!!** Since, ultimately, we have to decide on one value for `alpha`, and\nthe **Decision Tree** algorithm will not do this for us, `alpha` is called a **Hyperparameter** to differentiate it from the parameters that the **Decision Tree** algorithm can take care of on its own. -->\n\nFirst, let's demonstrate that different training and testing datasets result in trees with different accuracies:","5c4874b3":"Now let's discuss how to interpret the tree.\nIn each node, we have:\n- The variable (column name) and the threshold for splitting the observations. For example, in the tree's root, we use **ca** to split the observations. All\nobservations with **ca <= 0.5** go to the **left** and all observations with **ca > 0.5** go to the **right**.\n- **gini** is the gini index or score for that node\n- **samples** tell us how many samples are in that node\n- **value** tells us how many samples in the node are in each category. In this example, we have two categories, **No** and **Yes**, referring to whether or not a patient has heart disease. The number of patients with **No** comes first because the categories are in alphabetical order. Thus, in the root, 118 patients have **No** and 104 patients have **Yes**.\n- **class** tells us whichever category is represented most in the node. In the root, since 118 people have **No** and only 104 people have **Yes**, class is set to **No**.\n\nThe leaves are just like the nodes, except that they do not contain a variable and threshold for splitting the observations.\n\nThe nodes and leaves are colored by the **class**. In this case **No** is different shades of orange-ish and **Yes** is different shades of blue. The the darker the shade, the lower the **gini** score, and that tells us how much the node or leaf is skewed towards one class.","73e21143":"Now we need to talk about the **3** categorical columns that only contain **0**s and **1**s: **sex**, **fbs** (fasting blood sugar), and **exang** (exercise induced angina). As we can see, **One-Hot Encoding** converts a column with more than **2** categories, like **cp** (chest pain) into multiple columns of **0**s and **1**s. Since **sex**, **fbs**, and **exang** only have **2** categories and only contain **0**s and **1**s to begin with, we do not have to do anything special to them, so we're done formatting the data for the **Classification Tree**.\n\n**NOTE:** In practice we would use `unique()` to verify that they only contain **0**s and **1**s.","b882e903":"<a id=\"format-the-data\"><\/a>\n# Step 5: Format Data Part 1: Split the Data into Dependent and Independent Variables\n\nNow that we have taken care of the missing data, we are ready to start formatting the data for making a **Classification Tree**.\n\nThe first step is to split the data into two parts:\n1. The columns of data that we will use to make classifications\n2. The column of data that we want to predict.\n\nWe will use the conventional notation of `X` (capital **X**) to represent the columns of data that we will use to make classifications and `y` (lower case **y**) to represent the thing we want to predict. In this case, we want to predict **hd** (heart disease).\n\nThe reason we deal with missing data before splitting it into **X** and **y** is that if we remove rows, splitting after ensures that each row in **X** correctly corresponds with the appropriate value in **y**.\n\n**NOTE:** In the code below we are using `copy()` to copy the data *by value*. By default, pandas uses copy *by reference*. Using `copy()` ensures that the original data `df_no_missing` is not modified when we modify `X` or `y`. In other words, if we make a mistake when we are formatting the columns for classification trees, we can just re-copy `df_no_missing`, rather than have to reload the original data and  remove the missing values etc.","985a1af9":"Pruning a decision tree is all about finding the right value for the pruning parameter, `alpha`, which controls how little or how much pruning happens. One way to find the optimal value for `alpha` is to plot the accuracy of the tree as a function of different values for `alpha`.  We'll do this for both the **Training Dataset** and the **Testing Dataset**.\n\nFirst, let's extract the different values of `alpha` that are available for this tree and build a pruned tree for each value for `alpha`. **NOTE:** We omit the maximum value for alpha because it would prune all leaves, leaving us with only a root instead of a tree, with the following: `ccp_alphas = ccp_alphas[:-1]`.","34f360de":"# Step 1: Import the modules that will do all the work\nThe very first thing we do is load in a bunch of python modules. Python, itself, just gives us a basic programming language. These modules give us extra functionality to import the data, clean it up and format it, and then build, evaluate and draw the classification tree. \n\n**NOTE:** You will need **Python 3** and have at least these versions for each of the following modules: \n- pandas >= 0.25.1\n- numpy >= 1.17.2\n- sklearn >= 0.22.1\n \nIf you installed **Python 3** with [Anaconda](https:\/\/www.anaconda.com\/) can check which version you have with the command: `conda list`. If, for example, your version of `scikit-learn` is older than 0.22.1, then the easiest thing to do is just update all of your **Anaconda** packages with the following command: `conda update --all`. However, if only want to update `scikit-learn`, then you can run this command: `conda install scikit-learn=0.22.1`.","46249552":"Now that we have loaded the data into a **data frame** called **dataset**, let's look at the first five rows using the `head()` function:","0cc685d7":"Since only 6 rows have missing values, let's look at them.","83ee5abe":"We see that **ca** contains numbers (0.0, 3.0, 2.0 and 1.0) and questions marks (?). The numbers represent the number of blood vessels that we lit up by fluoroscopy and the question marks represent missing data.\n\nNow let's look at the unique values in **thal**.","41d36a25":"<a id=\"build-tree\"><\/a>\n# Step 7 : Build A Preliminary Classification Tree\nAt long last, the data is correctly formatted for making a **Classification Tree**. Now we simply split the data into **training** and **testing** sets and build the tree.","53fdc766":"So we see that using different **Training** and **Testing** data results in different accuracies.\n\nNow let's use **cross validation** to find the optimal value for `ccp_alpha`.","8db5b07d":"So, we see that **age**, **restbp**, **chol** and **thalach** are all `float64`, which is good, because we want them to be floating point numbers. All of the other columns, however, need to be inspected to make sure they only contain reasonable values, and some of them need to change. This is because, while **scikit learn Decision Trees** natively support continuous data, like resting blood preasure (**restbp**) and maximum heart rate (**thalach**), they do not natively support categorical data, like chest pain (**cp**), which contains 4 different categories. Thus, in order to use categorical data with **scikit learn Decision Trees**, we have to use a trick that converts a column of categorical data into multiple columns of binary values. This trick is called **One-Hot Encoding**.\n\nAt this point you may be wondering, \"what's wrong with treating categorical data like continuous data?\" To answer that question, let's look at an example: For the **cp** (chest pain) column, we have 4 options:\n1. typical angina,\n2. atypical angina,\n3. non-anginal pain,\n4. asymptomatic\n\nIf we treated these values, 1, 2, 3 and 4, like continuous data, then we would assume that 4, which means \"asymptomatic\", is more similar to 3, which means \"non-anginal pain\", than it is to 1 or 2, which are other types of chest pain. Thus, the decision tree would be more likely to cluster the patients with 4s and 3s together than the patients with 4s and 1s together. In contrast, if we treat these numbers like categorical data, then we treat each one a separate category that is no more or less similar to any of the other categories. Thus, the likelihood of clustering patients with 4s with 3s is the same as clustering 4s with 1s, and that approach is more reasonable.\n\nNow let's inspect and, if needed, convert the columns that contain categorical and integer data into the correct datatypes. We'll start with **cp** (chest pain) by inspecting all of its unique values:\n<!-- We'll start with the three colunms that should only contain 0s and 1s. **sex**. First, let's make sure it only contains `0` (for **female**) and `1` (for **male**). -->","1a3a051e":"Since `df_no_missing` has **6** fewer rows than the original `df`, it should have **297** rows.","c4ad5d30":"Again, **thal** also contains a mixture of numbers, representing the different diagnoses from the thalium heart scan, and question marks, which represent missing values.\n\n----\n\n# Step 4: Missing Data Part 2: Dealing With Missing Data\n\nSince scikit-learn's classification trees do not support datasets with missing values, we need to figure out what to do these question marks. We can either delete these patients from the training dataset, or impute values for the missing data. First let's see how many rows contain missing values.","862874fd":"In the graph above, we see that the accuracy for the **Testing Dataset** hits its maximum value when `alpha` is about **0.016**. After this value for `alpha`, the accuracy of the **Training Dataset** drops off and that suggest we should set `ccp_alpha=0.016`.\n\nHowever, since there are many ways we could have divided the original dataset into **Training** and **Testing** datasets, how do we know we used the best **Training Dataset** and how do we know we used the best **Testing Dataset**? Typically, we answer this question with **10-Fold Cross Validation**. So that's what we're going to do now, and we'll do it with the `cross_val_score()` function.\n\n----","516d8d00":"<a id=\"prune-tree\"><\/a>\n# Step 8: Cost Complexity Pruning Part 1: Visualize alpha\n\n**Decision Trees** are notorious for being **overfit** to the **Training Dataset**, so let's prune this tree in hopes that we can improve the accuracy with the **Testing Dataset**.","47efea6d":"----","64a0cd6a":"Using cross validation, we can see that, over all, instead of setting `ccp_alpha=0.016`,  we need to set it to something closer to **0.014**. We can find the exact value with:","006f83cd":"Now let's count the number of rows in the full dataset.","8e12a9a5":"<a id=\"download-the-data\"><\/a>\n# Step 2: Import the data\nNow we load in a dataset from the **[UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/index.php)**.\nSpecifically, we are going to use the **[Heart Disease Dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease)**. This dataset will allow us to predict if someone has heart disease based on their sex, age, blood pressure and a variety of other metrics.\n\n**NOTE:** When **pandas** (**pd**) reads in data, it returns a **data frame**, which is a lot like a spreadsheet. The data are organized in rows and columns and each row can contain a mixture of text and numbers. The standard variable name for a **data frame** is the initials **df**, and that is what we will use here:","b5e67aa9":"<a id=\"one-hot-encoding\"><\/a>\n# Step 6: Format the Data Part 2: One-Hot Encoding\n\nNow that we have split the data frame into two pieces, `X`, which contains the data we will use to make, or predict, classifications, and `y`, which contains the known classifications in our training dataset, we need to take a closer look at the variables in `X`. The list bellow tells us what each variable represents and the type of data (**float** or **categorical**) it should contain:\n\n- **age**, **Float**\n- **sex** - **Category**\n  - 0 = female\n  - 1 = male\n- **cp**, chest pain, **Category**\n  - 1 = typical angina,\n  - 2 = atypical angina,\n  - 3 = non-anginal pain,\n  - 4 = asymptomatic\n- **restbp**, resting blood pressure (in mm Hg), **Float**\n- **chol**, serum cholesterol in mg\/dl, **Float**\n- **fbs**, fasting blood sugar, **Category**\n  - 0 = >=120 mg\/dl\n  - 1 = <120 mg\/dl\n- **restecg**, resting electrocardiographic results, **Category**\n  - 1 = normal\n  - 2 = having ST-T wave abnormality\n  - 3 = showing probable or definite left ventricular hypertrophy\n- **thalach**,  maximum heart rate achieved, **Float**\n- **exang**, exercise induced angina, **Category**\n  - 0 = no\n  - 1 = yes\n- **oldpeak**, ST depression induced by exercise relative to rest. **Float**\n- **slope**, the slope of the peak exercise ST segment, **Category**\n  - 1 = upsloping\n  - 2 = flat\n  - 3 = downsloping\n- **ca**, number of major vessels (0-3) colored by fluoroscopy, **Float**\n- **thal**, thalium heart scan, **Category**\n  - 3 = normal (no cold spots)\n  - 6 = fixed defect (cold spots during rest and exercise)\n  - 7 = reversible defect (when cold spots only appear during exercise)\n\nNow, just to review, let's look at the data types in `X` to remember how python is seeing the data right now.","3d7daa93":"**NOTE** At this point Python thinks that `ideal_ccp_alpha` is a `series`. We can tell because when we printed `ideal_ccp_alpha` out, we got two bits of stuff. The first one was `20`, which is the index in the series, the second one, `0.014225`, is the value we want. So we can convert this from a series to a float with the following command:","589e1d69":"And we can also do the same thing for **thal**:","8aee4c2b":"Now, one last thing before we build a Classification Tree.  `y` doesn't just contain **0**s and **1**s. Instead, it has **5** different levels of heart disease. **0 =** no heart disease and **1-4** are various degrees of heart disease. We can see this with `unique()`:","e2c67b96":"Since we're only making a tree that does simple classification and only care if someone has heart disease or not, we need to convert all numbers **> 0** to **1**.","e4d2eb02":"As we can see in the printout above, `get_dummies()` puts all of the columns it does not process in the front and it puts **cp** at the end. It also split **cp** into **4** columns, just like we expected it. **cp_1.0** is `1` for any patient that scored a **1** for chest pain and `0` for all other patients. **cp_2.0** is `1` for any patient that scored **2** for chest pain and `0` for all other patients. **cp_3.0** is `1` for any patient that scored **3** for chest pain and **cp_4.0** is `1` for any patient that scored **4** for chest pain.\n\nNow that we see how `get_dummies()` works, let's use it on the four categorical columns that have more than 2 categories and save the result.\n\n**NOTE:** In practice you should verify all 5 of these columns to make sure they only contain\nthe accepted categories.","b1684518":"The math works out. However, we can also make sure **ca** no longer contains question marks by printing its unique values:","d0537d24":"<a id=\"identify-and-deal-with-missing-data\"><\/a>\n# Step 3: Missing Data Part 1: Identifying Missing Data\nUnfortunately, the biggest part of any data analysis project is making sure that the data is correctly formatted and fixing it when it is not. The first part of this process is dealing with **Missing Data**.\n\n**Missing Data** is simply a blank space or surrogate value that indicates that we failed to collect data for one of the features. For example, if we forgot to ask someone's age, or forgot to write it down, then we would have a blank space in the dataset for that person's **age**.\n\nThere are two main ways to deal with missing data:\n1. We can remove the rows that contain missing data from the dataset. This is relatively easy to do, but it wastes all of the other values that we collected. How a big of a waste this is depends on how important this missing value is for classification. For example, if we are missing a value for **age**, and **age** is not useful for classifying if people have heart disease or not, then it would be a shame to throw out all of someone's data just because we do not have their **age**.\n2. We can **impute** the values that are missing. In this context **impute** is just a fancy way of saying \"we can make an educated guess about about what the value should be\". Continuing our example where we are missing a value for **age**, instead of throwing out the entire row of data, we can fill the missing value with the average age or the median age, or use some other, more sophisticated approach, to guess at an appropriate value.\n\nIn this section, we'll focus on identifying missing values in the dataset and dealing with them. \n\nFirst, let's see what sort of data is in each column.","0517b1bd":"OK, we've built a **Classification Tree** for classification. Let's see how it performs on the **Testing Dataset** by running the **Testing Dataset** down the tree and drawing a **Confusion Matrix**."}}