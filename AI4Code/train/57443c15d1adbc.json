{"cell_type":{"f0499856":"code","3e066385":"code","c416c68c":"code","1454f3c8":"code","7a039d95":"code","6fa1822c":"code","fcfe60d0":"code","ae60c1b7":"code","49af3bed":"code","5d3e5381":"code","24babe7b":"code","396541c5":"code","9f91e889":"code","f117c938":"code","00d08ab0":"code","64b95d83":"code","6cd9e4c5":"code","d4b7114b":"code","7a099e6b":"code","785b442c":"code","bda5849e":"code","36d856c6":"code","fb15cda9":"code","63186116":"code","c338c0de":"code","e0110838":"code","c2c0c3e6":"code","473495cf":"code","9a51e21e":"code","9b06b569":"code","350ecfb9":"code","77e77b72":"code","5c070cb6":"code","b44e4fdc":"code","773007ba":"code","5e236932":"code","5b0ef7f6":"code","9175867c":"code","0eb0ad96":"code","ed01e90d":"code","0d047af2":"code","80755ec6":"code","8abcbf8a":"code","3a3b22b4":"code","7e2c83cb":"code","b4839809":"code","9346beaa":"code","bd4043c2":"code","0931285f":"code","e948865e":"code","1e0b165c":"code","1b0b7fdb":"code","dc33e842":"code","3f92d398":"code","497e1a3e":"code","777854db":"code","79f8bad0":"code","2318b55c":"code","a65f57f1":"code","1d9b55a4":"code","015e7290":"code","dc4ef1a3":"code","70acfb97":"code","85f8070b":"code","7afe6808":"code","47acebdf":"code","f4410819":"code","ebd0f684":"code","68556e67":"code","e83d8132":"code","fcacea60":"code","387dbf3c":"code","3c085412":"code","2b152528":"code","2b0a4bd3":"code","6f315d4f":"code","2370996a":"code","5189b045":"code","c91af259":"code","d0e9b877":"code","768595f5":"code","ee4cfee6":"code","ee86ff66":"markdown","c95608e0":"markdown","7fc45597":"markdown","044ca061":"markdown","93c806db":"markdown"},"source":{"f0499856":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e066385":"import numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom sklearn.impute import SimpleImputer as Imputer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split","c416c68c":"df = pd.read_csv(\"\/kaggle\/input\/xAPI-Edu-Data\/xAPI-Edu-Data.csv\") #at first we import the dataset","1454f3c8":"df.head()","7a039d95":"print (df.shape)","6fa1822c":"df.isnull().sum() #so we can consider that we don't have null values, so we can go fast through dataset","fcfe60d0":"df.info()","ae60c1b7":"df.describe()","49af3bed":"df.columns","5d3e5381":"df.rename(index=str, columns={'gender':'Gender', \n                              'NationalITy':'Nationality',\n                              'raisedhands':'RaisedHands',\n                              'VisITedResources':'VisitedResources'},\n                               inplace=True)\ndf.columns #here we want to make the dataset neat as some words had capital and small alphabets together","24babe7b":"df.dtypes","396541c5":"for i in range(1,17):\n    print(df.iloc[:,i].value_counts())\n    print(\"*\"*20)","9f91e889":"print(\"Class Unique Values : \", df[\"Class\"].unique())\nprint(\"Topic Unique Values : \", df[\"Topic\"].unique())\nprint(\"StudentAbsenceDays Unique Values : \", df[\"StudentAbsenceDays\"].unique())\nprint(\"ParentschoolSatisfaction Unique Values : \", df[\"ParentschoolSatisfaction\"].unique())\nprint(\"Relation Unique Values : \", df[\"Relation\"].unique())\nprint(\"SectionID Unique Values : \", df[\"SectionID\"].unique())\nprint(\"Gender Unique Values : \", df[\"Gender\"].unique())","f117c938":"df.groupby('Class').size()","00d08ab0":"# plot missing data:\ndf.isnull().sum().plot(kind='bar')\n# Add a title and show the plot.\nplt.title('Number of Missing Values Per Column')\n# Create tick mark labels on the Y axis and rotate them.\nplt.xticks(rotation = 45)\n# Create X axis label.\nplt.xlabel(\"Columns\")\n# Create Y axis label.\nplt.ylabel(\"NaN Values\");","64b95d83":"# drop any duplicate value if exists\ndf.drop_duplicates()","6cd9e4c5":"df.shape\n# so no duplicate value, no missing\n","d4b7114b":"#we want to visualize how many times students raised their hand in a time=x\nplt.subplots(figsize=(20, 8))\ndf[\"RaisedHands\"].value_counts().sort_index().plot.bar()\nplt.title(\"Number of times vs number of students raised their hands in a certain time\", fontsize=20)\nplt.xlabel(\"Number of times\", fontsize=14)\nplt.ylabel(\"Number of student\", fontsize=14)\nplt.show()","7a099e6b":"df.columns","785b442c":"#we want to visualize how many times students visited certin resources in a particular time\nplt.subplots(figsize=(20, 8))\nplt.title(\"Number of times vs number of student visted resource in a certain time\", fontsize=20)\ndf[\"VisitedResources\"].value_counts().sort_index().plot.bar()\nplt.xlabel(\"Number  of times, student visted resource\", fontsize=14)\nplt.ylabel(\"Number  of student, on particular visit\", fontsize=14)\nplt.show()","bda5849e":"# we want to analysis the relation between gender and performance\ndf_edu=df.groupby(['Gender','Class']).size()\n\ndf_edu","36d856c6":"df_edu.unstack().plot(kind='bar', stacked=True)","fb15cda9":"#From this bar chart we visualize that male students are more on \"medium\" and \"lower\" category while girls show better performance","63186116":"fig, (axis1, axis2)  = plt.subplots(2, 1,figsize=(10,10))\nsns.countplot(x='Topic', hue='Gender', data=df, ax=axis1)\nsns.countplot(x='Nationality', hue='Gender', data=df, ax=axis2)","c338c0de":"# We see the gender wise performance at country level and by each subject\n\n# According to those charts Although it shows girls have better performance but when we analyzie the topic diagram, we see that, girls took less technical subjects\n\n# As we see in the Nationality diagram, there is Gender disparity at a country level so maybe its another reason that girls  participated less than boys\n\n\n","e0110838":"# illustrate the percentage of participants from each country\ndf.Nationality.value_counts(normalize= True).plot(kind='bar')","c2c0c3e6":"# illustrate the percentage of participants according to their education level\ndf.StageID.value_counts(normalize=True).plot(kind='bar')","473495cf":"fig, axarr  = plt.subplots(2,2,figsize=(10,10))\nsns.barplot(x='Class', y='VisitedResources', data=df, order=['L','M','H'], ax=axarr[0,0])\nsns.barplot(x='Class', y='AnnouncementsView', data=df, order=['L','M','H'], ax=axarr[0,1])\nsns.barplot(x='Class', y='RaisedHands', data=df, order=['L','M','H'], ax=axarr[1,0])\nsns.barplot(x='Class', y='Discussion', data=df, order=['L','M','H'], ax=axarr[1,1])","9a51e21e":"#As expected, those that participated more (higher counts in Discussion, raisedhands, AnnouncementViews, RaisedHands), performed better.\n#that thing about correlation and causation\n","9b06b569":"fig, (axis1, axis2)  = plt.subplots(1, 2,figsize=(10,5))\nsns.pointplot(x='Semester', y='VisitedResources', hue='Gender', data=df, ax=axis1)\nsns.pointplot(x='Semester', y='AnnouncementsView', hue='Gender', data=df, ax=axis2)","350ecfb9":"#As we see both visiting resources and viewing announcements, students were more vigilant in the second semester, perhaps that last minute need to boost your final grade.","77e77b72":"#Create a new dataframe for processing data in decision tree.\ndf_dt=df.copy()","5c070cb6":"df_dt.head(10)","b44e4fdc":"replace_map = {'Gender': {'F': 1, 'M': 2}}\nreplace_map1 = {'Nationality': {'Egypt': 1, 'Iran': 2, 'Iraq': 3, 'Jordan': 4, 'KW':5,'Lybia': 6, 'Morocco': 7, 'Palestine': 8 , 'SaudiArabia': 9 , 'Syria': 10,'Tunis': 11,'USA': 12,'lebanon': 13,'venzuela': 14}}\nreplace_map2 = {'PlaceofBirth': {'Egypt': 1, 'Iran': 2, 'Iraq': 3, 'Jordan': 4, 'KuwaIT':5,'Lybia': 6, 'Morocco': 7, 'Palestine': 8 , 'SaudiArabia': 9 , 'Syria': 10,'Tunis': 11,'USA': 12,'lebanon': 13,'venzuela': 14}}\nreplace_map3 = {'StageID': {'HighSchool': 1, 'MiddleSchool': 2, 'lowerlevel': 3}}\nreplace_map4 = {'GradeID': {'G-02':1,'G-04':2, 'G-05':3,'G-06':4,'G-07':5,'G-08':6,'G-09':7,'G-10':8,'G-11':9,'G-12':10}}\nreplace_map5 = {'SectionID': {'A': 1, 'B': 2, 'C': 3}}\nreplace_map6 = {'Topic': {'Arabic': 1, 'Biology': 2, 'Chemistry': 3, 'English': 4, 'French':5,'Geology': 6, 'History': 7, 'IT': 8 , 'Math': 9 , 'Quran': 10,'Science': 11,'Spanish': 12}}\nreplace_map7 = {'Semester': {'F': 1, 'S': 2}}\nreplace_map8 = {'Relation': {'Father': 1, 'Mum': 2}}\nreplace_map9 = {'ParentAnsweringSurvey': {'Yes': 1, 'No': 2}}\nreplace_map10 = {'ParentschoolSatisfaction': {'Bad': 1, 'Good': 2}}\nreplace_map11 = {'StudentAbsenceDays': {'Above-7': 1, 'Under-7': 2}}\nreplace_map12 = {'Class': {'M': 1, 'L': 2,'H':3}}","773007ba":"df_dt.replace(replace_map,inplace=True)\ndf_dt.replace(replace_map1,inplace=True)\ndf_dt.replace(replace_map2,inplace=True)\ndf_dt.replace(replace_map3,inplace=True)\ndf_dt.replace(replace_map4,inplace=True)\ndf_dt.replace(replace_map5,inplace=True)\ndf_dt.replace(replace_map6,inplace=True)\ndf_dt.replace(replace_map7,inplace=True)\ndf_dt.replace(replace_map8,inplace=True)\ndf_dt.replace(replace_map9,inplace=True)\ndf_dt.replace(replace_map10,inplace=True)\ndf_dt.replace(replace_map11,inplace=True)\ndf_dt.replace(replace_map12,inplace=True)\n\n","5e236932":"df_dt.head()","5b0ef7f6":"# Great now lets start decision tree learning\n\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","9175867c":"feature_columns=['Gender','Nationality','PlaceofBirth','StageID','SectionID','Topic','Semester','Relation','RaisedHands','VisitedResources','AnnouncementsView','Discussion','ParentAnsweringSurvey','ParentschoolSatisfaction','StudentAbsenceDays']\nX=df_dt[feature_columns]\ny=df_dt.Class","0eb0ad96":"X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=1)\nindextoCheckDecisionTree=y_test.index","ed01e90d":"clf=DecisionTreeClassifier()","0d047af2":"clf=clf.fit(X_train,y_train)","80755ec6":"y_pred = clf.predict(X_test)","8abcbf8a":"# now lets test accuracy of our model\nAccuracy_dt=metrics.accuracy_score(y_test,y_pred)\nprint(\"Accuracy : \",Accuracy_dt)","3a3b22b4":"from sklearn import tree\nimport graphviz","7e2c83cb":"dot_data = tree.export_graphviz(clf, out_file=None)","b4839809":"graph = graphviz.Source(dot_data)\ngraph.render(\"xAPI-Edu\")","9346beaa":"dot_data = tree.export_graphviz(clf, out_file=None,\nfeature_names=feature_columns,\nclass_names=['Low-level','Middle-level','High-level'],\nfilled=True, rounded=True,\nspecial_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph","bd4043c2":"# Work on modifying criterion to see the accuracy.\nclf=DecisionTreeClassifier(criterion=\"entropy\")\nclf=clf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)","0931285f":"Accuracy_dt=metrics.accuracy_score(y_test,y_pred)\nprint(\"Accuracy : \",Accuracy_dt)\n","e948865e":"# The accuracy decreased by a certain margin. so entropy is not a good criterion. Now lets tune parameter of max_depth\nclf=DecisionTreeClassifier(criterion=\"entropy\",max_depth=13)\nclf=clf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nAccuracy_dt=metrics.accuracy_score(y_test,y_pred)\nprint(\"Accuracy : \",Accuracy_dt)","1e0b165c":"#By combining entropy and max_depth accuracy increases to 0.73 which is very good for this model\n","1b0b7fdb":"# Now lets draw our modified decision tree\ndot_data = tree.export_graphviz(clf, out_file=None)\ngraph = graphviz.Source(dot_data)\ngraph.render(\"xAPI-Edu2\")","dc33e842":"dot_data = tree.export_graphviz(clf, out_file=None,\nfeature_names=feature_columns,\nclass_names=['Low-level','Middle-level','High-level'],\nfilled=True, rounded=True,\nspecial_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph","3f92d398":"# we draw the classifier for the decision tree labels and show the number of TP,TN,FP and FN. so We use Confusion metrix\n#to calculate the TP,TN,FP,FN\nfrom sklearn.model_selection  import cross_val_score\nfrom sklearn.metrics import classification_report\nprint(\"Confusion Metrix\",metrics.confusion_matrix(y_test,y_pred))\ncnf=metrics.confusion_matrix(y_test,y_pred)\nprint(\"Accuracy\",metrics.accuracy_score(y_test,y_pred))\n","497e1a3e":"from sklearn.metrics import classification_report, accuracy_score\n\ntarget_names = ['Low-level', 'Middle-level', 'High-level']\nprint(classification_report(y_test, y_pred, target_names=target_names))","777854db":"from sklearn.naive_bayes import GaussianNB","79f8bad0":"df_naive = df_dt.copy()","2318b55c":"feature_columns=['Gender','Nationality','PlaceofBirth','StageID','SectionID','Topic','Semester','Relation','RaisedHands','VisitedResources','AnnouncementsView','Discussion','ParentAnsweringSurvey','ParentschoolSatisfaction','StudentAbsenceDays']\nX=df_naive[feature_columns]\ny=df_naive.Class","a65f57f1":"X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=109) # 70 % training and 30 % testing\n","1d9b55a4":"gnb = GaussianNB()","015e7290":"gnb.fit(X_train, y_train)","dc4ef1a3":"#Predict the response for test dataset\ny_pred = gnb.predict(X_test)","70acfb97":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","85f8070b":"# Lets try with some different split for data same as decision trees\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=1) ","7afe6808":"gnb.fit(X_train, y_train)","47acebdf":"y_pred = gnb.predict(X_test)","f4410819":"Accuracy_naive=metrics.accuracy_score(y_test,y_pred)\nprint(\"Accuracy : \",Accuracy_naive)","ebd0f684":"# To conclude on the same split , naive bayes has a better accuracy.\nfrom sklearn.model_selection  import cross_val_score\nfrom sklearn.metrics import classification_report\nprint(\"Confusion Metrix\",metrics.confusion_matrix(y_test,y_pred))\ncnf=metrics.confusion_matrix(y_test,y_pred)\nprint(\"Accuracy\",metrics.accuracy_score(y_test,y_pred))\n","68556e67":"from sklearn.metrics import classification_report, accuracy_score\n\ntarget_names = ['Low-level', 'Middle-level', 'High-level']\nprint(classification_report(y_test, y_pred, target_names=target_names))","e83d8132":"#Lets use Randomn Forest Now\n\nfrom sklearn.ensemble import RandomForestClassifier","fcacea60":"df_rf = df_naive.copy()","387dbf3c":"feature_columns=['Gender','Nationality','PlaceofBirth','StageID','SectionID','Topic','Semester','Relation','RaisedHands','VisitedResources','AnnouncementsView','Discussion','ParentAnsweringSurvey','ParentschoolSatisfaction','StudentAbsenceDays']\nX=df_rf[feature_columns]\ny=df_rf.Class","3c085412":"# Lets try with some different split for data same as decision trees and naive bayes\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=1) ","2b152528":"clf=RandomForestClassifier()","2b0a4bd3":"clf.fit(X_train,y_train)","6f315d4f":"y_pred=clf.predict(X_test)","2370996a":"Accuracy_rf = metrics.accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", Accuracy_rf)","5189b045":"#Can we improve this ?\nclf=RandomForestClassifier(bootstrap=True,criterion='gini',n_estimators=300)","c91af259":"clf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\nAccuracy_rf = metrics.accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", Accuracy_rf)","d0e9b877":"# Lets calculate feature importance using Randomn Forest Classifier\ndn = {'features':feature_columns,'score':clf.feature_importances_}\ndf = pd.DataFrame.from_dict(data=dn).sort_values(by='score',ascending=False)\n      ","768595f5":"plot= sns.barplot(x='score',y='features',data=df,orient='h')\nplot.set(xlabel=\"Score\",yLabel=\"features\",title='Feature Importance of Randomn Forest Classifier')\nplt.setp(plot.get_xtickLabels(), rotation = 90)\nplt.show()","ee4cfee6":"# So finally by the metric of accuracy our results are : \n\nprint(\"Accuracy of decision Tree is : \", Accuracy_dt * 100,\"%\")\nprint(\"Accuracy of Naive Bayes is  : \", Accuracy_naive * 100, \"%\")\nprint(\"Accuracy of RandomnForest is : \", Accuracy_rf * 100 ,\"%\")","ee86ff66":"# Decision Tree Learning on Data\n","c95608e0":"# Naive Bayes Implementation","7fc45597":"# visualizing categorical features with numerical features\n","044ca061":"Our Goal is to predit the Class label which is the level of the student as predicted by the model. The level of student is divided into three categories by their score\/grade in the semester as following : \n* Low-Level: interval includes values from 0 to 69,\n* Middle-Level: interval includes values from 70 to 89,\n* High-Level: interval includes values from 90-100.\n\nWe conclude it is a supervised classification task where we need to predict the categorical label. \nSo we use the following three algorithms to test on the dataset and make 3 models.\n1. Decision Trees\n2. Naive Bayes\n3. Randomn Forest Classifier\n\nFinally , we will conclude with the model which has the most accuracy on the test test. \n\n","93c806db":"# Random Forest Classifier Learning"}}