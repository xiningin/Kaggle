{"cell_type":{"70c08f8c":"code","8b3cc7f8":"code","1e5d748e":"code","dbccddd2":"code","e64be598":"code","cf6160bd":"code","2857cf89":"code","9eefdeb0":"code","12396a81":"code","711f212d":"code","1913ca6f":"code","f90f7392":"code","2546064b":"code","c14bad45":"code","22d5a0cf":"code","0bcd5e53":"code","64448f2e":"markdown","d9ffafa6":"markdown","7dfec15c":"markdown","bb474d43":"markdown","c8e03606":"markdown","72a538a9":"markdown","c3d26ae0":"markdown","87ba7bc1":"markdown","4508c389":"markdown","ac20f53f":"markdown","87e5dcff":"markdown","5f63cdec":"markdown","8eea257f":"markdown","ae4e33b2":"markdown","095b3a21":"markdown","08b68a57":"markdown","b0c8b7ea":"markdown","74ba0b38":"markdown","7491e35e":"markdown","5c97de74":"markdown"},"source":{"70c08f8c":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \npd.set_option('max_columns',None)\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8b3cc7f8":"train_data = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\nprint('The train data has {} training observations with {} features'.format(train_data.shape[0],train_data.shape[1]))\ntrain_data.head(3)","1e5d748e":"test_data = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\nprint('The test data has {} test observations with {} features'.format(test_data.shape[0],test_data.shape[1]))\ntest_data.head(3)","dbccddd2":"train_data['TARGET'].value_counts().plot.pie()","e64be598":"pd.options.display.float_format = '{:,.2f}'.format\ndf_missing_val = pd.DataFrame(columns=['column','number of missing values'])\nfor column in train_data.columns: \n    missing_val = train_data[column].isnull().sum()\n    x = {'column':column,'number of missing values':missing_val}\n    df_missing_val = df_missing_val.append(x,ignore_index=True)\n    \n\ndf_missing_val = df_missing_val[df_missing_val['number of missing values']!=0]\ndf_missing_val['number of missing values'] = df_missing_val['number of missing values'].astype(int)\ndf_missing_val = df_missing_val.sort_values(by='number of missing values',ascending=False)\n\ndf_missing_val.head(10)\n","cf6160bd":"print(train_data.dtypes.value_counts())","2857cf89":"print(train_data.select_dtypes('object').apply(pd.Series.nunique,axis=0).sort_values(ascending=False))","9eefdeb0":"le = LabelEncoder()\nle_count = 0\n\nfor col in train_data.columns:\n    if train_data[col].dtype == 'object':\n        if len(list(train_data[col].unique()))<=2:\n            le.fit(train_data[col])\n            train_data[col] = le.transform(train_data[col])\n            test_data[col] = le.transform(test_data[col])\n            \n            le_count +=1\n                   \nprint('{} columns were transformed'.format(le_count))","12396a81":"train_data = pd.get_dummies(train_data)\ntest_data = pd.get_dummies(test_data)\n\nprint('The new train data shape after one-hot encoding is {} features and {} variables'.format(train_data.shape[1],train_data.shape[0]))\nprint('The new test data shape after one-hot encoding is {} features and {} variables'.format(test_data.shape[1],test_data.shape[0]))","711f212d":"train_labels = train_data.TARGET\n\ntrain_data,test_data= train_data.align(test_data,join='inner', axis=1)\ntrain_data['TARGET'] = train_labels\n\nprint('The new train data shape after one-hot aligning is {} features and {} variables'.format(train_data.shape[1],train_data.shape[0]))\nprint('The new test data shape after one-hot aligning is {} features and {} variables'.format(test_data.shape[1],test_data.shape[0]))\n","1913ca6f":"train_data[['DAYS_EMPLOYED']].describe().plot.hist()\ntrain_data[['DAYS_EMPLOYED']].describe()","f90f7392":"# Create an anomalous flag column\ntrain_data['DAYS_EMPLOYED_ANOM'] = train_data[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\ntrain_data['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\ntrain_data['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');\n\ntest_data['DAYS_EMPLOYED_ANOM'] = test_data[\"DAYS_EMPLOYED\"] == 365243\ntest_data[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (test_data[\"DAYS_EMPLOYED_ANOM\"].sum(), len(test_data)))","2546064b":"corelations = train_data.corr()['TARGET'].sort_values()\nhigh_corelation = corelations.tail(15)\nlow_corelations =  corelations.head(15)\n\nprint('most positive corelations:\\n', high_corelation)\nprint('most negative corelations:\\n', low_corelations)","c14bad45":"most_corr=train_data[['NAME_INCOME_TYPE_Working','REGION_RATING_CLIENT',\n                      'REGION_RATING_CLIENT_W_CITY','DAYS_EMPLOYED','DAYS_BIRTH','TARGET']]\nmost_corr_corr = most_corr.corr()\n\nsns.set_style(\"dark\")\nsns.set_context(\"notebook\", font_scale=2.0, rc={\"lines.linewidth\": 1.0})\nfig, axes = plt.subplots(figsize = (20,10),sharey=True)\nsns.heatmap(most_corr_corr,cmap=plt.cm.RdYlBu_r,vmin=-0.25,vmax=0.6,annot=True)\nplt.title('Correlation Heatmap for features with highest correlations with target variables')\n","22d5a0cf":"\n\n# iterate through the sources\nfor i, source in enumerate(['NAME_INCOME_TYPE_Working','REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','DAYS_EMPLOYED','DAYS_BIRTH']):\n    \n    # create a new subplot for each source\n    sns.set_style(\"dark\")\n    sns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.0})\n    fig, axes = plt.subplots(figsize = (10,5))\n    # plot repaid loans\n    sns.kdeplot(train_data.loc[train_data['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(train_data.loc[train_data['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \n","0bcd5e53":"sns.set_style(\"dark\")\nsns.set_context(\"notebook\", font_scale=2.0, rc={\"lines.linewidth\": 1.0})\nfig, axes = plt.subplots(figsize = (20,10),sharey=True)\nleast_corr = train_data[['EXT_SOURCE_3','EXT_SOURCE_2','EXT_SOURCE_1','NAME_EDUCATION_TYPE_Higher education','CODE_GENDER_F']]\nleast_corr_corr = least_corr.corr()\nsns.heatmap(least_corr_corr,cmap=plt.cm.RdYlBu_r,vmin=-0.25,vmax=0.6,annot=True)\nplt.title('Correlation Heatmap for features with lowest correlations with target variables')","64448f2e":"<h4 style=\"color:brown;\">One hot necoding has created more variables in the train data than test data since not all data in the test dataset is in the train dataset.\n    The two will need to align the train data to the test dataset in order to have the same number of columns.\n    However, as we align, we will ensure that the target variable is not excluded\n<\/h4>","d9ffafa6":"<h4 style=\"color:brown;\">\n    Let's look at other data anomalies.\n    The days employed column seems unreasonable with the maximum number being 365243 which is unreasonable.\n    Anomalies have different ways of being dealt with. I will chose to exclude the same from our dataset.\n<\/h4>","7dfec15c":"<h4 style=\"color:brown;\">\nLet's get a snippet of the most negatively and positiely correlations with the target variable.\n    From there, we will look at their distributions.\n    We will then proceed to visualize the correlations\n<\/h4>","bb474d43":"<h4 style=\"color:brown;\">\nWe have two main methds of dealing with categorical variables ie\n    <ul>\n        <li> One hot encoding <\/li>\n        <li> Label encoding <\/li>\n    <\/ul>\nWe will apply label encoding for features with two or less variables and for those with more than two variables  we'll use one-hot encoding. <br>\nWe will then apply a dimension reduction method known as *PCA* to reduce the number of dimensions while still preserving the information\n<\/h4>","c8e03606":"<h4 style=\"color:brown;\">\n    The training and testing datasets now have the same features which is required for machine learning\n<\/h4>","72a538a9":"<h4 style=\"color:brown;\">\ni. Label Encodig\n<\/h4>","c3d26ae0":"<hr>\n<h2 style=\"text-decoration:bold;color:Navy;\" >Exploratory Data Analysis (EDA) &#128202;; <\/h2>\n<p style = \"color:brown;font-size:16px;\">\n    The goal is to analyze our data to find patterns and correlations as well as calculate statistics that will enable us better understand our data\n<\/p>\n\n<hr>","87ba7bc1":"<h2 style = \"color:brown;font-size:18px;\" >\nIn this notebook, we will stick to using only the main application training and testing data. \n We will apply a standard classification model\n<\/h2>\n<ul style = \"color:brown;font-size:16px;\">\n    <li>\nSupervised: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features\n    <\/li>    \n    <li>    \nClassification: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)\n    <\/li>    \n<\/ul>","4508c389":"<html>\n    <h2 style=\"text-decoration:italics;color:red;\">Work in Progress. 4 Days Left to Complete<\/h2>    \n<\/html>","ac20f53f":"<h3 style=\"color:brown;\">\nLets look at the Categorical variables\n<\/h3>","87e5dcff":"<h2 style=\"color:brown;\">\nDealing with Data Types\n<\/h2>\n","5f63cdec":"<hr>\n<h2 style = \"text-decoration:bold;color:Navy;\">Import required packages and load data &#128451;<\/h2>\n<hr>","8eea257f":"<h3 style=\"color:brown;\">\nThe table Below shows the datatypes in our data and the number of features per each datatype\n<\/h3>\n","ae4e33b2":"<h3 style=\"color:brown;\">\nCorrelations\n<\/h3>","095b3a21":"<h3 style=\"color:brown;\">\nAnomalies\n<\/h3>","08b68a57":"<h4 style=\"color:brown;\">\nii. One-Hot Encodig\n<\/h4>","b0c8b7ea":"<h2 style=\"color:brown;\">\nDetermine Missing Values\n<\/h2>\n\n\n<p style = \"color:brown;font-size:16px;\">\n    No action will be taken for now on missing values. However, in subsequent procedures, we will review as to whether to drop the columns with high number of missing values or fill in with other values.\n<\/p>\n","74ba0b38":"<hr>\n<h2 style=\"text-decoration:bold;color:Navy;\" >Introduction &#127908; <\/h2>\n<hr>","7491e35e":"<html>\n    <img src=\"https:\/\/cdn.standardmedia.co.ke\/images\/friday\/top_10_most_attracti5ea2fa735cf45.jpeg\" style=\"width:1200px;height:300px;background-color:lightgrey;\" alt=\"\">\n    <p style = \"color:brown;font-size:16px;\" >\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\nThe data uses is as below:\n        <\/p> \n                <img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/home-credit\/home_credit.png\" style=\" display:block; margin-left:auto;margin-right:auto; width:1400px;height:650px;background-color:lightgrey;\" alt=\"\">\n        \n     \n<\/html>","5c97de74":"<h2 style=\"color:brown;\">\nTarget Variable Distribution\n<\/h2>\n\n<p style=\"color:brown;font-size:16px;\">\nFrom this information, we see this is an imbalanced class problem. There are far more loans that were repaid on time than loans that were not repaid.\n<\/p>"}}