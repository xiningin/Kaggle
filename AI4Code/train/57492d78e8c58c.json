{"cell_type":{"d8fb16b0":"code","bfe0582c":"code","1ce06fa1":"code","6b8dfa7d":"code","d6531105":"code","d2a6624f":"code","39eb7281":"code","28d1c38c":"code","fd41503d":"code","d476361e":"code","5049dc27":"code","a8684ee4":"code","54e24147":"code","3513a6bb":"code","c920b29e":"code","8edf3523":"code","dcaa79f7":"code","85d9b5e5":"code","f4eb864c":"code","5055f9a2":"code","7f8d6e17":"code","892def95":"code","3df5885e":"code","84679704":"code","3167f41d":"code","8fab85e4":"code","cc76a6a0":"code","e8851029":"code","b5b1cf80":"code","4b382465":"code","7c5755c0":"code","ca32a470":"code","99d6b963":"code","2bcd9615":"code","1c77c7cc":"code","6ad3f6b5":"code","b0022b81":"code","68f01b25":"code","39f1be0e":"code","5e5b9de7":"code","3a8bc686":"code","80279101":"code","4dc1a144":"code","ae43d8b2":"code","f094fa0c":"code","66d368de":"code","2e6a71e0":"code","6c9dfe37":"code","89e1d13a":"code","17255e12":"code","296d684a":"code","4d84c4ca":"code","df181baf":"code","9d3f0c08":"code","ebadc451":"code","3906f6c1":"code","bc87feb1":"code","54f589a5":"code","b93b987b":"code","c48e8c47":"code","0050ccd9":"code","93d77998":"code","113c86ae":"code","8bff9432":"code","c9bc153b":"code","a1ee393e":"code","10d0dc27":"code","e381d5e0":"code","238414f4":"code","dd4216f6":"code","6bb06d2c":"code","1e335db0":"code","ed9e3cd4":"code","8f58dc68":"code","07a0e88a":"code","0f9a0108":"code","527ea023":"code","4d8bc9a6":"code","665b07e7":"code","bc5c867a":"code","9997615d":"code","3956566e":"code","3dbd924f":"code","8f496aee":"code","57ce140a":"code","aa1bdd8d":"code","aa5eef98":"code","8b8b8035":"code","2464c2bb":"code","f447df44":"code","def6a65d":"code","b31f4a36":"code","f992f18f":"code","3f82ec82":"code","29cc3b07":"code","542df074":"code","395cfae9":"code","129e3400":"code","91311037":"code","e0ee9e1e":"code","234e752e":"code","1cef1145":"code","dba4ca27":"code","0ec697b5":"code","44793a8f":"code","86d4e2a9":"code","d253794e":"code","293bdffb":"code","197b286f":"code","dbc04bed":"code","ff9680b6":"code","6588ca7d":"code","67d0b78a":"code","1218ec59":"code","20c581e6":"code","d113bc9e":"code","ffec816a":"code","fb7f9fe2":"code","66b592b2":"code","b33ed51a":"code","a69ec00a":"code","ff06228b":"code","71732925":"code","208d041a":"code","11e0ed6e":"code","0f726284":"code","ecacd514":"code","8676a7f9":"code","8731158d":"code","df1abc9d":"code","cbeba82a":"code","67bfad48":"code","7a8653af":"code","b0513981":"markdown","9b741698":"markdown","69627762":"markdown","44ecdc4e":"markdown","621a75dc":"markdown","baa314f1":"markdown","f29ce868":"markdown","3ccb5715":"markdown","71994b20":"markdown","50cee046":"markdown","be5a362b":"markdown","6de45f49":"markdown","dad527bd":"markdown","cae69fbc":"markdown","bc372baf":"markdown","79664753":"markdown","bead3f14":"markdown","b4e6135f":"markdown","409d203a":"markdown","295a78f1":"markdown","4b84055a":"markdown","8d417419":"markdown","1721ce85":"markdown","e539aeba":"markdown","688972fb":"markdown","211d0bc7":"markdown","81a04328":"markdown","e9b5f99b":"markdown","a39f17a1":"markdown","d1a8087b":"markdown","37ce9dc5":"markdown","b21efd30":"markdown","dc82cab6":"markdown","3bc95589":"markdown","9ffbdfa3":"markdown","840a58cb":"markdown","322664e1":"markdown","55796732":"markdown","e4e4cc7f":"markdown","f7a38fea":"markdown","a8827a69":"markdown","cc8a10b2":"markdown","5d7e00c0":"markdown","0e53d2aa":"markdown","d7afa313":"markdown","34bc52cb":"markdown","e94ad28a":"markdown","c90357a7":"markdown","eddf4b72":"markdown","14839c71":"markdown","75da3d4e":"markdown","899b006a":"markdown","01c1fadd":"markdown","88ad717d":"markdown","33b326a3":"markdown","2b495f2d":"markdown","68975dad":"markdown","99ea30b9":"markdown","680fba35":"markdown","a5c415f4":"markdown","13d2d622":"markdown","0c374391":"markdown","f1df1dd5":"markdown","f779bf5e":"markdown"},"source":{"d8fb16b0":"import numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nimport missingno as msno\nimport pickle","bfe0582c":"df = pd.read_csv('..\/input\/titanic\/train.csv')","1ce06fa1":"df.head()","6b8dfa7d":"df.shape","d6531105":"df.dtypes","d2a6624f":"df.info()","39eb7281":"df.describe()","28d1c38c":"df.describe(include=['object'])","fd41503d":"sns.countplot(x = df.Survived, data = df)\nplt.show()","d476361e":"df['Survived'].value_counts()","5049dc27":"sns.countplot(x =df.Sex, data= df)\nplt.show()","a8684ee4":"df.Sex.value_counts()","54e24147":"sns.countplot(x = df.Sex, hue = df['Survived'], data = df)\nplt.show()","3513a6bb":"survived_df = df[df.Survived == 1]\n\nmale_survived = (survived_df[survived_df.Sex == 'male'].Survived.value_counts()  \/ survived_df.Survived.value_counts() ) * 100\nfemale_survived = (survived_df[survived_df.Sex == 'female'].Survived.value_counts() \/ survived_df.Survived.value_counts()) * 100\n\nprint(f\"Out of the Total Survived Population, Male Survived Population % = {male_survived}\")\nprint(f\"Out of the Total Survived Population, Female Survived Population % = {female_survived}\")","c920b29e":"sns.countplot(x = df.Pclass, data = df)","8edf3523":"sns.countplot(x = df.Pclass, hue=df.Survived, data = df)\nplt.show()","dcaa79f7":"sns.countplot(x = df.Embarked, data = df)\nplt.show()","85d9b5e5":"sns.countplot(x = df.Embarked, hue = df.Survived, data = df)\nplt.show()","f4eb864c":"grouped_data = df.groupby(['Embarked', 'Survived']).agg({'Survived': 'count'})\ngrouped_data['%'] = grouped_data.apply(lambda x: x*100\/float(x.sum()))\ngrouped_data","5055f9a2":"df.Age.hist()","7f8d6e17":"sns.boxplot(x = df.Age, data = df)","892def95":"sns.boxplot(x = df.Sex, y = df.Age, hue= \"Survived\", data = df)\nplt.show()","3df5885e":"sns.boxplot(x = df.SibSp, data = df)\nplt.show()","84679704":"df.SibSp.value_counts()","3167f41d":"sns.countplot(x = df.SibSp, hue = df.Survived, data = df)","8fab85e4":"df.Parch.value_counts()","cc76a6a0":"sns.countplot(x = df.Parch, data = df)","e8851029":"sns.countplot(x = df.Parch, hue = df.Survived, data = df)\nplt.show()","b5b1cf80":"msno.matrix(df)","4b382465":"msno.bar(df)","7c5755c0":"df.isnull().mean() * 100","ca32a470":"df.duplicated().sum()","99d6b963":"correlations = df.corr()\ncorrelations","2bcd9615":"sns.heatmap(data = correlations, cmap = 'RdBu_r')\nplt.show()","1c77c7cc":"df.head()","6ad3f6b5":"sns.boxplot(x = df.Age)\nplt.show()","b0022b81":"sns.violinplot(x = df.SibSp)\nplt.show()","68f01b25":"df.groupby('SibSp').agg(['count'])","39f1be0e":"sns.violinplot(x = df.Parch)","5e5b9de7":"df.groupby('Parch').agg(['count'])","3a8bc686":"sns.boxplot(x = df.Fare)\nplt.show()","80279101":"sns.violinplot(x = df.Pclass , y = df.Age, hue = 'Survived', data = df)\nplt.show()","4dc1a144":"df.isnull().mean() * 100","ae43d8b2":"df.isnull().sum()\/ len(df) ","f094fa0c":"df.isnull().sum()","66d368de":"df.head()","2e6a71e0":"df[df.Embarked.isnull()]","6c9dfe37":"first_class = df['Pclass'] == 1\nfemale_people = df['Sex'] == 'female'\nno_siblings = df['SibSp'] == 0\nno_parents = df['Parch'] == 0","89e1d13a":"filtered_df = df[first_class & female_people & no_siblings & no_parents]\nfiltered_df.head()","17255e12":"filtered_df.Embarked.value_counts()","296d684a":"df_copy = df.drop('PassengerId', axis=1)","4d84c4ca":"df_copy.fillna({\n    'Embarked': 'C'\n}, inplace=True)","df181baf":"df_copy.isnull().sum()","9d3f0c08":"def plot_correlationmap(df):\n  # Set the Plot Size\n  plt.figure(figsize=(10, 10))\n  sns.set_style('white')\n  # Fetch the Correlation Matrix\n  correlations = df_copy.corr()\n  # Display 1 Half of the Map to Avoid Duplicate of HeatMap and Screen Clutter\n  boolean_mask = np.zeros_like(correlations)\n  upperTriangle = np.triu_indices_from(boolean_mask)\n  boolean_mask[upperTriangle] = 1\n\n  sns.heatmap(correlations * 100, cmap='RdBu_r', annot= True, fmt='0.0f', mask=boolean_mask)","ebadc451":"plot_correlationmap(df_copy)","3906f6c1":"df_copy.Age = df.groupby('Pclass')['Age'].apply(lambda x: x.fillna(x.mean()))","bc87feb1":"df_copy.isnull().sum()","54f589a5":"df_copy.describe(include=['object'])","b93b987b":"df_copy.fillna(method='ffill', inplace=True)\ndf_copy.fillna(method='bfill', inplace=True)","c48e8c47":"df_copy.isnull().sum()","0050ccd9":"df_copy.describe(include=[\"object\"])","93d77998":"df_copy.head()","113c86ae":"numeric_features = df_copy.dtypes[df_copy.dtypes != object].index\nnumeric_features = np.delete(numeric_features, [0, 1])","8bff9432":"for feature in numeric_features:\n  sns.boxplot(x = feature, data = df)\n  plt.show()","c9bc153b":"df_copy.head()","a1ee393e":"df_copy = df_copy.drop('Cabin', axis= 1)","10d0dc27":"df_copy = df_copy.drop('Name', axis=1)","e381d5e0":"df_copy.Ticket.describe()","238414f4":"df_copy = df_copy.drop('Ticket', axis=1)","dd4216f6":"df_copy.head()","6bb06d2c":"from sklearn.model_selection import train_test_split","1e335db0":"features = df_copy.drop('Survived', axis=1)\ntarget = df_copy['Survived']","ed9e3cd4":"features_Train, features_Test, target_Train, target_Test = train_test_split(features, target, test_size= 0.30, random_state=987)","8f58dc68":"print('Features Train - ', features_Train.shape)\nprint('Features Test - ', features_Test.shape)\nprint('Target Train - ', target_Train.shape)\nprint('Target Test - ', target_Test.shape)","07a0e88a":"target_Train.value_counts(normalize=True) * 100","0f9a0108":"target_Test.value_counts(normalize=True) * 100","527ea023":"df_copy.dtypes","4d8bc9a6":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder","665b07e7":"labelEncoder = LabelEncoder()\ndf_copy['Sex'] = labelEncoder.fit_transform(df_copy['Sex'])\ndf_copy.dtypes","bc5c867a":"df_copy.Sex.unique()","9997615d":"OHEncoder = OneHotEncoder()\nembarked_columns_encoded = OHEncoder.fit_transform(df_copy['Embarked'].values.reshape(-1, 1)).toarray()\nembarked_coded = pd.DataFrame(embarked_columns_encoded, columns=[\"Embarked_\" + str(int(i)) for i in range(embarked_columns_encoded.shape[1])])\ndf_copy = df_copy.join(embarked_coded)\ndf_copy.head()","3956566e":"df_copy = df_copy.drop('Embarked', axis=1)","3dbd924f":"df_copy.head()","8f496aee":"df.to_csv('final_titatic_data.csv', index=False)","57ce140a":"df_copy = pd.read_csv('final_titatic_data.csv')","aa1bdd8d":"from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, plot_confusion_matrix, roc_curve, confusion_matrix, plot_roc_curve\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB","aa5eef98":"df = pd.read_csv('final_titatic_data.csv')\ndf.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\nfeatures = df.drop('Survived', axis=1)\ntarget = df.Survived\nX_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size=0.30, random_state=1234)","8b8b8035":"df.head()","2464c2bb":"print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\nprint(Y_train.value_counts(normalize=True), Y_test.value_counts(normalize=True))","f447df44":"pipelines = {\n    'tree': make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=1234)),\n    'rf': make_pipeline(StandardScaler(), RandomForestClassifier(random_state=1234)),\n    'lr': make_pipeline(StandardScaler(), LogisticRegression(random_state=1234)),\n    'svm': make_pipeline(StandardScaler(), svm.SVC(random_state=1234)),\n    'gb': make_pipeline(StandardScaler(), GaussianNB())\n}","def6a65d":"\ntree_hyperparameters = {\n    'decisiontreeclassifier__splitter': ['best', 'random'],\n    'decisiontreeclassifier__max_features': list(range(1, X_train.shape[1])),\n    'decisiontreeclassifier__min_samples_split': np.linspace(1, 10, 10, endpoint=True),\n    'decisiontreeclassifier__min_samples_leaf': np.linspace(0.1, 0.5, 10, endpoint=True),\n    'decisiontreeclassifier__max_depth': np.linspace(1, 32, 32, endpoint=True)\n}\nrf_hyperparameters = {\n    'randomforestclassifier__min_samples_split': np.linspace(1, 10, 10, endpoint=True),\n    'randomforestclassifier__min_samples_leaf': np.linspace(0.1, 0.5, 10, endpoint=True),\n    'randomforestclassifier__max_depth': np.linspace(10, 110, 11),\n    'randomforestclassifier__max_features': ['auto', 'sqrt'],\n    'randomforestclassifier__bootstrap': [True, False]\n}\nsvm_hyperparameters = {\n    'svc__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n    'svc__kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n    'svc__C': [0.1, 1, 10, 100, 1000]\n}\n\nlr_hyperparameters = {\n    'logisticregression__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n    'logisticregression__C': np.linspace(-4, 4, 20),\n    'logisticregression__max_iter': [100, 1000, 10000],\n    'logisticregression__solver': ['lbfgs','newton-cg','liblinear','sag','saga']\n}\n\ngb_hyperparameters = {\n    \n}\n\nhyperparameters = {\n    'tree': tree_hyperparameters,\n    'rf': rf_hyperparameters,\n    'svm': svm_hyperparameters,\n    'lr': lr_hyperparameters,\n    'gb': gb_hyperparameters\n}","b31f4a36":"fitted_models = {}\n\nfor name, pipeline in pipelines.items():\n  model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n  model.fit(X_train, Y_train)\n  print(name, \"model is fitted\")\n  fitted_models[name] = model","f992f18f":"# Fit the LR Model\n\nmodel = GridSearchCV(pipelines['lr'], hyperparameters['lr'], cv=10, n_jobs=-1)\nmodel.fit(X_train, Y_train)\nprint(\"Logistic Regression Model is fitted\")","3f82ec82":"predicted = model.predict(X_test)\nprint(accuracy_score(Y_test, predicted))","29cc3b07":"svmModel = GridSearchCV(pipelines['svm'], hyperparameters['svm'], cv=10, n_jobs=-1)\nsvmModel.fit(X_train, Y_train)\nprint(\"SVM Model is fitted\")","542df074":"prdicted = svmModel.predict(X_test)\nprint(accuracy_score(Y_test, prdicted))","395cfae9":"import pickle\n\nwith open('fitted_models.pkl', 'wb') as f:\n  pickle.dump(fitted_models, f)","129e3400":"for name, model in fitted_models.items():\n  print('Model - ', name)\n  pred = model.predict(X_test)\n  print(\"Accuracy Score -\", accuracy_score(Y_test, pred))\n  print()","91311037":"model_performance = {}\n\nfor name, model in fitted_models.items():\n  model_performance[name] = {}\n  pred = model.predict(X_test)\n  model_performance[name]['Accuracy Score'] = accuracy_score(Y_test, pred)\n  model_performance[name]['Precision Score'] = precision_score(Y_test, pred)\n  model_performance[name]['Recall Score'] = precision_score(Y_test, pred)\n  model_performance[name]['F1 Score'] = f1_score(Y_test, pred)\n\n ","e0ee9e1e":"performance_list = [['Model', 'Accuracy Score', 'Precision Score', 'Recall Score', 'F1 Score']]\nfor name, performance in model_performance.items():\n  metrics = list()\n  metrics.append(name)\n  metrics.append(performance['Accuracy Score'])\n  metrics.append(performance['Precision Score'])\n  metrics.append(performance['Recall Score'])\n  metrics.append(performance['F1 Score'])\n  performance_list.append(metrics)\n\ncolumn_names = performance_list.pop(0)\n\ndf_performance = pd.DataFrame(performance_list, columns=column_names)\ndf_performance.Model.replace('tree', 'Decision Tree', inplace=True)\ndf_performance.Model.replace('rf', 'Random Forest', inplace=True)\ndf_performance.Model.replace('lr', 'Logistic Regression', inplace=True)\ndf_performance.Model.replace('svm', 'Support Vector Machines', inplace=True)\ndf_performance.Model.replace('gb', 'Gaussian Naive Bayes', inplace=True)\ndf_performance.head()","234e752e":"def model_train(model, model_name):\n  model.fit(X_train, Y_train)\n  predicted = model.predict(X_test)\n  confusion = confusion_matrix(Y_test, predicted)\n  plot_confusion_matrix(model, X_test, Y_test, display_labels=['Survived', 'Not Survived'], cmap=plt.cm.Blues)\n  print(f\"{model_name} - \")\n  print(\"Accuracy - \", accuracy_score(Y_test, predicted))\n  print(\"Sensitivity\/ Recall Score - \", recall_score(Y_test, predicted))\n  print(\"Precision Score - \", precision_score(Y_test, predicted))\n  print(\"F1 Score - \", f1_score(Y_test, predicted))\n  print(\"ROC Curve - \")\n  plot_roc_curve(model, X_test, Y_test)\n  plt.show()\n  return model","1cef1145":"target_Test.value_counts(normalize=True) * 100","dba4ca27":"unrestricted_model = DecisionTreeClassifier(random_state=123, criterion='gini', splitter='best')\ndt_model = model_train(unrestricted_model, \"UnRestricted Decision Tree\")","0ec697b5":"predicted = dt_model.predict(X_train)\nplot_roc_curve(dt_model, X_train, Y_train)\nprint(accuracy_score(Y_train, predicted))","44793a8f":"random_forest = RandomForestClassifier(random_state=123, criterion='gini')\nrandom_forest = model_train(random_forest, \"Random Forest Classifier\")","86d4e2a9":"model_train(LogisticRegression(max_iter=100), \"Logistic Regression\")","d253794e":"model_train(LogisticRegression(max_iter=10000), \"Logistic Regression\")","293bdffb":"model_train(LogisticRegression(max_iter=1000000), \"Logistic Regression\")","197b286f":"model_train(svm.SVC(kernel='linear'), 'SVM - Linear Kernel')","dbc04bed":"model_train(svm.SVC(kernel='poly'), 'SVM - Polynomial Kernel')","ff9680b6":"model_train(svm.SVC(kernel='rbf'), 'SVM - RBF Kernel')","6588ca7d":"model_train(svm.SVC(kernel='sigmoid'), 'SVM - Sigmoid Kernel')","67d0b78a":"model_train(GaussianNB(), 'Gaussian NB Model')","1218ec59":"test = pd.read_csv('..\/input\/titanic\/train.csv')\ntest.head()","20c581e6":"test.drop(['PassengerId', 'Name', 'Cabin'], axis=1, inplace=True)","d113bc9e":"test.head()","ffec816a":"OHEncoder = OneHotEncoder()\nembarked_columns_encoded = OHEncoder.fit_transform(test['Embarked'].values.reshape(-1, 1)).toarray()\nembarked_coded = pd.DataFrame(embarked_columns_encoded, columns=[\"Embarked_\" + str(int(i)) for i in range(embarked_columns_encoded.shape[1])])\ntest = test.join(embarked_coded)\ntest.head()","fb7f9fe2":"labelEncoder = LabelEncoder()\ntest['Sex'] = labelEncoder.fit_transform(test['Sex'])\ntest.head()","66b592b2":"test = test.drop('Ticket', axis=1)","b33ed51a":"test.head()\ntest = test.drop('Embarked', axis=1)","a69ec00a":"test.head()","ff06228b":"test.isnull().sum()","71732925":"test.Age = test.groupby('Pclass')['Age'].apply(lambda x: x.fillna(x.mean()))","208d041a":"test.isnull().sum()","11e0ed6e":"test[test.Fare.isnull()]","0f726284":"test.groupby('Pclass').Fare.describe()","ecacd514":"df.head()","8676a7f9":"df.groupby('Pclass').Fare.describe()","8731158d":"test['Fare'].fillna(13.675550, inplace=True)","df1abc9d":"test_copy = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_copy.head()","cbeba82a":"predicted = model.predict(test)","67bfad48":"predicted_df = pd.DataFrame({'PassengerId': test_copy['PassengerId'], 'Survived': predicted})\npredicted_df.head()","7a8653af":"predicted_df.to_csv('final_submission.csv', index=False)","b0513981":"## Data Modelling and Hyper Parameters Tuning and Performance Evaluation","9b741698":"For the Logistic Regression, We can observe that, for 1000 Observations it failed to get the best fit and as we increase 10000 we get the best fit with Accuracy fo 83% and after that even if we increase the iterations, it no longer has effect on the performance of the model\n\nSo far, Logistic Regression has the best Accuracy and F1 Score ","69627762":"#### SVM","44ecdc4e":"#### Observations: After Filtering the Data based on the Common criteria for the Missing Enbarked Values, we can Observe that -  \n\nClass - **C** is the most common category for Embarked. So, we'll Impute Embarked with Class **C**","621a75dc":"## Categorical Data EDA","baa314f1":"### Let's Check people's survival based on Parents\/ Children onboard","f29ce868":"#### Let's Check the Missing Values Percentages for each feature","3ccb5715":"### Checking for Duplicated Observations","71994b20":"From the Above, we can observe that % of Target Variables in Both Train and Test Set is Similar which is around 60%","50cee046":"## Importing the required libraries","be5a362b":"From the Above we can Infer that - \n\n- Passenger Class played a role on Survival with 1st class people survival much better compared to the 2nd class and 3rd class people.\n\n- People with Age over 60 Outlier Survival rate is also better in the First Class\n\n- Infants in the Class 2 and CLass 3 Survival is much better though elderly survival rate suffered in the Class 2 and Class 3","6de45f49":"Modelling the Data using Logistic Regression, we get Accuracy of 83%. Though we increase iterations from 1k to 100k we arn't getting any better accuracy. ","dad527bd":"#### Let's Impute Missing Values for Embarked","cae69fbc":"### Feature Selection","bc372baf":"For the SVM, Among all the Kernels we have modelled, we can observe that Linear Kernel has the Best Accuracy and F1 Score and SVM Model came close to the Logistic Regression model in terms of performance though it lost by a little to Logistic Regression","79664753":"### From the Above, we can infer that there are no duplicate observations in our data","bead3f14":"## Identifying Missing Values","b4e6135f":"From the Features we selected for final modelling, we have 2 Features which are categorical which we'll need to convert into numerical since most models accept numeric input features.","409d203a":"#### From the above, we can infer that we have few people above the Age 60+ who are outliers","295a78f1":"#### Logistic Regression Classifier Modelling","4b84055a":"Name is also another feature which doesn't help us in the predition of our survival. so, we'll drop the Name feature also","8d417419":"## Loading the dataset","1721ce85":"### Summary Statistics of the Data","e539aeba":"### Let's Check Survival Against Embarked","688972fb":"#### From the Above Correlation Matrix, we can Observe that Age is't much Corelated to Other Criteria. The Only Inverse Correlation we can see is that with Passenger Class. So, we'll Impute the Age value with the Mean value by the group of Passenger Class","211d0bc7":"From All the Above models, We can Observe that, \n\nLogistic Regression performed the Best followed by SVM Model\n\nWe have Taken the Following into consideration for the Model Performance -\n\n- Accuracy Score since our Test Data Set has a good mix of Target Variable of 57% & 43% of classes\n\n- F1 Score since we are interested in both Precision and Recall of the Model","81a04328":"For the Sex Column, Since we have Only 2 Values Either Male or Female we can use LabelEncoder and convert the Column.\n\nFor the Embarked Column, since we have 3 values, we'll use OneHotEncoder to convert it.","e9b5f99b":"### Let's do numeric data EDA","a39f17a1":"We can see that Our Test Data Set has a reasonably good balance of Target Feature classes.  ","d1a8087b":"### Let's Check the Survival Data based on Gender","37ce9dc5":"From the Above, we can observe that Cabin has too many missing values and the Categorical Feature doesn't have that many limited classes too. So, we'll Just Impute the Cabin with the random value. Here we'll forward fill without any sorting order","b21efd30":"## Missing Value Imputations","dc82cab6":"### Observations So Far:\n\nWe have missing values for the features - \n\n*   Age\n*   Cabin\n*   Embarked\n\nLet's Check the percentage of missing Values for the Data - \n\n","3bc95589":"#### Naive Bayes Modelling","9ffbdfa3":"## Data Distributions","840a58cb":"### Let's Check the distribution of the Target Variable - How many Survived Vs Not","322664e1":"### Let's Check the Age Group and Survival","55796732":"Ticket Column is also not a useful column and we can remove it","e4e4cc7f":"From the Data Above, we can observe that -\n\n- Age has around 20% missing data\n- Cabin has around 77% missing data\n- Embarked has less than 1% missing data\n\nWhile Embarked missing data % is well within the limits, Age and Cabin Data have more missing values than we can accomodate.","f7a38fea":"Let's Check the Outliers Interms of Survival","a8827a69":"### From the CoRelation map, we can make the following observations - \n\n- Survival isn't much correlated to the Sibling\/Sposes onboard the ship\n- Survival isn't much correlated to the Fare also as much\n- Survival shows Inversely Co-related to the Passenger Class but since Passenger Class is more of a Categorical Feature, we can ignore it.","cc8a10b2":"### Train and Test Data Split","5d7e00c0":"From the available features, Cabin isn't giving much information and also has too many missing values which we imputed forwardfilling the data. So, we can remove the Cabin Feature","0e53d2aa":"### Let's Check the Gender Distribution","d7afa313":"### Observations So Far:\n\n- Generally based on the data more no of people didn't survive. Of the 891 Observations, 541 people didn't survive and 349 people did survive\n- Most of the People are of the Age group 20-40\n- Gender distribution is skewed towards Male population\n- Female population is more likely to survive compared to Male population\n- Of the Passenger Classes, 1st Class % is having more chance to survive compared to 2nd class % compared to 3rd class %","34bc52cb":"#### Let's Recheck the Data to confirm if Embarked Missing values have been Imputed -","e94ad28a":"### Let's Check Survival for People with Siblings\/Spouses","c90357a7":"From the Above, we can observe that - \n\n- People with Parents\/Children onboard are very few with most of the poeple without any parents\/children onboard the ship.\n\n- People with Siblings\/Spouses onboard are also very few with most of the poeple without any siblings\/spouses onboard the ship.\n\n- When we observe the Data Distribution, we can see that people with more than 2 People with Parents\/Children, Siblings\/Spouses obboard the ship are very few but since they are valid observation i.e. people can have those many family members we will keep the observations.\n\n- We can even Observe that People Survival rate is also better for these people who have more family members onboard.","eddf4b72":"### Let's Check Survival Data based on the Ticket Class","14839c71":"Finally, we have the Cleaned Data and ENcoded Data which can be used for Modelling the Data","75da3d4e":"### Age Distribution","899b006a":"### Observations So Far:\n\n- Training Data has 891 Observations with 12 Features (Including the target variable)\n- Of these features 5 are Categorical features - Name, Sex, Ticket, Cabin, Embarked","01c1fadd":"#### Random Forest Modelling","88ad717d":"#### From the Above, we can infer, people's survival chances didn't increase if they don't have any spouses or siblings","33b326a3":"Gaussian NB model has also performed pretty well but still Logistic Regression and SVM have better performance","2b495f2d":"## Let's See the Outliers in the Data","68975dad":"For an UnRestricted Decision Tree Model, we have a decent F1 Score of 0.7 and Accuracy of 75%","99ea30b9":"For an UnRestricted RandomForest Classifier, we have an F1 Score of 0.75 and Accuracy of 80% which is much better compared to the UnRestricted Decision Tree model","680fba35":"### Outlier Treatment","a5c415f4":"#### Decision Tree Modelling","13d2d622":"## Let's See the Corelation Between the Features","0c374391":"#### From the Above, we can infer people's survival chances didn't improve if they didn't had any parents or children. InFact their chances are better if they had a parent or child","f1df1dd5":"### From the Above Summary Data, we can infer the following - \n\n- Male Population is High compared to the Femal Population\n- People Embarked from Location S is Higher compared to Other Locations\n- Age is missing some values\n- Most of the Poeple don't have a Parent\/Children onboard the Ship\n- People didnt bring Siblings\/ Spouses onboard the train but more pople had brought compared to the Parents\/Children","f779bf5e":"### From the Above, we can infer the following -\n\n- People Age is between 20- 40 mostly with a few outliers above 60\n- In Male Population Younger Chances of Survival are more compared to the female population by slight marign.\n"}}