{"cell_type":{"296e398a":"code","f7b0b0de":"code","e697daeb":"code","1ec594c3":"code","0047086f":"code","b7173541":"code","5b111ee4":"code","404157bf":"code","ad7add6d":"code","49882079":"code","69dd43c3":"code","8d4b6186":"code","c1960103":"code","16e5e767":"code","f472f36b":"code","13151257":"code","588627e6":"code","37bb2c8a":"code","0693ac4d":"code","88a50550":"code","5a8d4a85":"code","314a3932":"code","30f4a2a8":"code","9a2a88d0":"code","d6ef6fbe":"code","f538e2a5":"code","9994eaae":"code","19440d30":"code","453f0ac3":"code","c3df52e5":"code","63c155a2":"code","0f198724":"code","ed1e7503":"code","e0b5e48c":"code","da25caf6":"code","d458fa84":"code","f0cad3f5":"code","364a3b02":"code","9ae8af9c":"code","c38cfd18":"code","53fa286f":"code","0774ede5":"code","a3e44010":"code","626da5af":"code","3decec32":"code","d49b88bb":"code","4b536b43":"code","9138cfa1":"code","8a33efb7":"code","39f8cdaa":"code","40ace741":"code","229a1296":"code","a9c28f12":"code","02cf4a02":"code","36473448":"code","8d14be63":"code","8b6946ba":"code","220b85ca":"code","811e3494":"code","33f3710f":"code","d2b93d1d":"code","d086a2b0":"code","e9a3ea6e":"code","03eb85ab":"markdown","ec00fd8b":"markdown","789af90d":"markdown","4c052b99":"markdown","0e16307d":"markdown","8ceb2cbf":"markdown","ab2c8a44":"markdown","654969c1":"markdown","940112fd":"markdown","5921dfc2":"markdown","5e5d1369":"markdown","b32d6b18":"markdown","50af0aff":"markdown","2fc4301b":"markdown","bc323d14":"markdown"},"source":{"296e398a":"!pip install keras-tuner","f7b0b0de":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom collections import Counter, OrderedDict\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import multilabel_confusion_matrix, precision_score, average_precision_score\nimport kerastuner\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom kerastuner.tuners import RandomSearch\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e697daeb":"# Load the data\ntrain = pd.read_csv(\"..\/input\/amexpert-2021-challenge\/train_go05W65.csv\")\ntest = pd.read_csv(\"..\/input\/amexpert-2021-challenge\/test_VkM91FT.csv\")","1ec594c3":"train.head()","0047086f":"train.head()","b7173541":"def convert_string_to_list(s):\n  return re.sub(r'[^\\w,]', '', s).split(',')\n\ntrain['Product_Holding_B1'] = train['Product_Holding_B1'].apply(lambda x : convert_string_to_list(x))\ntest['Product_Holding_B1'] = test['Product_Holding_B1'].apply(lambda x : convert_string_to_list(x))\n\ntrain['Product_Holding_B2'] = train['Product_Holding_B2'].apply(lambda x : convert_string_to_list(x))","5b111ee4":"# Extract all the possible product holdings\nproduct_holdings_B1 = []\nfor holdings in train['Product_Holding_B1'].values.tolist():\n  for holding in holdings:\n    if holding not in product_holdings_B1:\n      product_holdings_B1.append(holding)\n\nproduct_holdings_B2 = []\nfor holdings in train['Product_Holding_B2'].values.tolist():\n  for holding in holdings:\n    if holding not in product_holdings_B2:\n      product_holdings_B2.append(holding)","404157bf":"train.head()","ad7add6d":"# Compute the count of each holding in and B1\nholding_count_B1 = {}\nfor holding in product_holdings_B1:\n\n  # initialise the count for each holding as 0\n  holding_count_B1[holding] = 0\n\nholding_count_B2 = {}\nfor holding in product_holdings_B2:\n\n  # initialise the count for each holding as 0\n  holding_count_B2[holding] = 0","49882079":"# Iterate over both holdings 1 and 2 and increment the count of each holding in the dictionaries\nfor holdings in train['Product_Holding_B1'].values.tolist():\n  for holding in holdings:\n    if holding in holding_count_B1:\n      holding_count_B1[holding] += 1\n\nfor holdings in train['Product_Holding_B2'].values.tolist():\n  for holding in holdings:\n    if holding in holding_count_B2:\n      holding_count_B2[holding] += 1","69dd43c3":"# PLot the count for each holding\nplt.style.use('seaborn')\nholding_count_B1 = OrderedDict(sorted(holding_count_B1.items(), key = lambda x : (x[1], x[0]), reverse=False))\nholding_count_B2 = OrderedDict(sorted(holding_count_B2.items(), key = lambda x : (x[1], x[0]), reverse=False))\n\nfigure, ax = plt.subplots(1, 2, figsize=(15,8))\nax[0].barh(y=list(holding_count_B1.keys()), width=list(holding_count_B1.values()))\nax[0].set_ylabel(\"B1 Holdings\")\nax[0].set_xlabel(\"Count\")\nax[0].set_title(\"B1 Holdings Count\")\n\nax[1].barh(y=list(holding_count_B2.keys()), width=list(holding_count_B2.values()))\nax[1].set_ylabel(\"B2 Holdings\")\nax[1].set_xlabel(\"Count\")\nax[1].set_title(\"B2 Holdings Count\")\n\nfigure.show()","8d4b6186":"train.head()","c1960103":"# Visulaise the Gender Count\nfig, ax = plt.subplots(1, 2, figsize=(15,8))\nax[0].pie(train['Gender'].value_counts(), labels = ['Male', 'Female'], autopct=\"%.0f%%\")\nsns.countplot(data=train, x='Gender', ax=ax[1])\nfig.show()","16e5e767":"# Plot the City Category\nfig, ax = plt.subplots(1, 2, figsize=(15,8))\nax[0].pie(train['City_Category'].value_counts(), labels = train['City_Category'].value_counts().index, autopct=\"%.0f%%\")\nsns.countplot(data=train, x='City_Category', ax=ax[1])\nfig.show()","f472f36b":"# Plot the Customer Category\nfig, ax = plt.subplots(1, 2, figsize=(15,8))\nax[0].pie(train['Customer_Category'].value_counts(), labels = train['Customer_Category'].value_counts().index, autopct=\"%.0f%%\")\nsns.countplot(data=train, x='Customer_Category', ax=ax[1])\nfig.show()","13151257":"# Plot the Is_Active\nfig, ax = plt.subplots(1, 2, figsize=(15,8))\nax[0].pie(train['Is_Active'].value_counts(), labels = train['Is_Active'].value_counts().index, autopct=\"%.0f%%\")\nsns.countplot(data=train, x='Is_Active', ax=ax[1])\nfig.show()","588627e6":"# Plot the Age histogram\nsns.histplot(data=train, x='Age', kde=True)\nplt.show()","37bb2c8a":"# Plot the Vintage histogram\nsns.histplot(data=train, x='Vintage', kde=True)\nplt.show()","0693ac4d":"train.head()","88a50550":"# Group by Gender and plot the mean Age and mean Vintage\ngender_data = train.groupby('Gender').mean().reset_index()\nplt.bar(x=gender_data['Gender'], height=gender_data['Age'])\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Age\")\nplt.title(\"Mean Age across Gender\")\nplt.show()","5a8d4a85":"# Group by Gender and plot the mean Age and mean Vintage\ngender_data = train.groupby('Gender').mean().reset_index()\nplt.bar(x=gender_data['Gender'], height=gender_data['Vintage'])\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Vintage\")\nplt.title(\"Mean Vintage across Gender\")\nplt.show()","314a3932":"# Insights across City Category\ntrain.groupby('City_Category').mean()","30f4a2a8":"# Insights across Customer_Category\ntrain.groupby('Customer_Category').mean()","9a2a88d0":"# Create a mapping for Product Holding B1\nb1_columns = ['P00', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21']\nb1_matrix_train = np.zeros(shape=(train.shape[0], len(product_holdings_B1)), dtype='int')\nb1_matrix_train_df = pd.DataFrame(b1_matrix_train, columns=b1_columns)\nb1_matrix_train_df.set_index(train['Customer_ID'], inplace=True)\n\nfor holding in product_holdings_B1:\n  for i in range(train.shape[0]):\n\n    # Check if the holding is in the train\n    cust_id = train['Customer_ID'].iloc[i]\n    curr_holdings = train['Product_Holding_B1'].iloc[i]\n\n    if holding in curr_holdings:\n\n      # Update the b1_matrix_train_df for the particular ID\n      b1_matrix_train_df.loc[cust_id][holding] = 1\n\n\n# Join with the train dataframe\ntrain_final = pd.merge(train, b1_matrix_train_df, on='Customer_ID')\n\n# Drop the Product_Holding_B2 category\ntrain_final.drop(['Product_Holding_B1', 'Product_Holding_B2'], axis=1, inplace=True)\n\n'''Prepare the target variable'''\nmlb = MultiLabelBinarizer()\ntarget = mlb.fit_transform(train['Product_Holding_B2'])","d6ef6fbe":"# Prepare the test data\n\n# Create a mapping for Product Holding B1\nb1_columns = ['P00', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21']\nb1_matrix_test = np.zeros(shape=(test.shape[0], len(product_holdings_B1)), dtype='int')\nb1_matrix_test_df = pd.DataFrame(b1_matrix_test, columns=b1_columns)\nb1_matrix_test_df.set_index(test['Customer_ID'], inplace=True)\n\nfor holding in product_holdings_B1:\n  for i in range(test.shape[0]):\n\n    # Check if the holding is in the train\n    cust_id = test['Customer_ID'].iloc[i]\n    curr_holdings = test['Product_Holding_B1'].iloc[i]\n\n    if holding in curr_holdings:\n\n      # Update the b1_matrix_train_df for the particular ID\n      b1_matrix_test_df.loc[cust_id][holding] = 1\n\n\n# Join with the train dataframe\ntest_final = pd.merge(test, b1_matrix_test_df, on='Customer_ID')\n\n# Drop the Product_Holding_B2 category\ntest_final.drop(['Product_Holding_B1'], axis=1, inplace=True)","f538e2a5":"train_final.head()","9994eaae":"test_final.head()","19440d30":"# Drop the Customer ID and encode the Is_Active, City_Category, Customer_Category\ntrain_final.drop('Customer_ID', axis=1, inplace=True)\ntest_final.drop('Customer_ID', axis=1, inplace=True)","453f0ac3":"# Encode the Categories\ncols_to_encode = ['Gender', 'City_Category', 'Customer_Category']\nfor col in cols_to_encode:\n  le = LabelEncoder()\n  train_final[col] = le.fit_transform(train_final[col])\n  test_final[col] = le.transform(test_final[col])","c3df52e5":"train_final.head()","63c155a2":"# Create new feature --> Age\/Vintage ratio\ntrain_final['Age_Vintage_ratio'] = train_final['Age']\/train_final['Vintage']\ntest_final['Age_Vintage_ratio'] = test_final['Age']\/test_final['Vintage']","0f198724":"# Scale the continuous variable\ncols_to_scale = [\"Gender\", \"Age\", \"Vintage\", \"Is_Active\", \"City_Category\", \"Customer_Category\", \"Age_Vintage_ratio\"]\nfor col in cols_to_scale:\n  mms = MinMaxScaler()\n  train_final[col] = mms.fit_transform(train_final[col].values.reshape(-1,1))\n  test_final[col] = mms.transform(test_final[col].values.reshape(-1,1))","ed1e7503":"train_final.head()","e0b5e48c":"train_final.head()","da25caf6":"test_final.head()","d458fa84":"# Plot the Age Vintage Ratio\ntrain_final['Age_Vintage_ratio'].hist(bins=50)\nplt.xlabel(\"Age Vintage Ratio\")\nplt.title(\"Age Vintage Ratio Distribution\")\nplt.show()","f0cad3f5":"# Print the final shape of train and test shape\ntrain_final.shape, test_final.shape","364a3b02":"def extract_top_k_predictions(prediction_df, k):\n\n  # Dictionary to map ID and recommendations\n  recommendations = []\n  \n  #iterate over the frame\n  for i in range(prediction_df.shape[0]):\n    recommendations.append(list(prediction_df.loc[i].sort_values(ascending=False)[ : k].index))\n\n  return recommendations","9ae8af9c":"def generate_submission(model_recommendations, customer_id):\n\n  submission = (('[') +\n                 (\"'\") + \n                 (pd.DataFrame(np.array(model_recommendations))[0]) +\n                 (\"'\") + \n                 (', ') + \n                 (\"'\" ) + \n                 (pd.DataFrame(np.array(model_recommendations))[1]) + \n                 (\"'\") + \n                 (', ') + \n                 (\"'\") + \n                 (pd.DataFrame(np.array(model_recommendations))[2]) + \n                 (\"'\") +\n                 (']'))\n  \n\n  final_submission = pd.concat((pd.DataFrame(customer_id, columns=['Customer_ID']), submission), axis=1)\n  final_submission.rename(columns = {0 : 'Product_Holding_B2'}, inplace=True)\n  \n  return final_submission","c38cfd18":"# Split the data\nX_train, X_test, y_train, y_test = train_test_split(train_final, target, test_size=0.2, random_state=42)","53fa286f":"# Baseline\n\n# 1. KNN\nknn_clf = KNeighborsClassifier()\nknn_ml = OneVsRestClassifier(knn_clf, n_jobs=-1)\nknn_ml.fit(X_train, y_train)","0774ede5":"# Make Predictions and compute the precision at K\nknn_predictions = knn_ml.predict_proba(X_test)\nknn_predictions_2 = knn_ml.predict(X_test)","a3e44010":"# Print the micro and macro precision score\nmacro_precision_knn = precision_score(y_test, knn_predictions_2, average='macro')\nmicro_precision_knn = precision_score(y_test, knn_predictions_2, average='micro')\n\nprint(micro_precision_knn, macro_precision_knn)","626da5af":"# Make Predictions\nknn_ml.fit(train_final, target)\ntest_cust_id = test['Customer_ID'].values\nknn_test_predictions = pd.DataFrame(knn_ml.predict(test_final), columns = mlb.classes_)\nknn_recommendations = extract_top_k_predictions(knn_test_predictions, 3)","3decec32":"# Make Submissions (Baseline)\nknn_recommendations_sub = generate_submission(knn_recommendations, test_cust_id)\nknn_recommendations_sub.to_csv('knn_recommendations_1.csv', index=False)","d49b88bb":"# Model Object :\nlr_clf = LogisticRegression(solver='liblinear')\nlr_ml = OneVsRestClassifier(lr_clf, n_jobs=-1)\nlr_ml.fit(X_train, y_train)","4b536b43":"# Make Predictions and compute the precision at K\nlr_predictions = lr_ml.predict_proba(X_test)\nlr_predictions_2 = lr_ml.predict(X_test)","9138cfa1":"# Print the micro and macro precision score\nmacro_precision_lr = precision_score(y_test, lr_predictions_2, average='macro')\nmicro_precision_lr = precision_score(y_test, lr_predictions_2, average='micro')\n\nprint(micro_precision_lr, macro_precision_lr)","8a33efb7":"# Make Predictions\nlr_ml.fit(train_final, target)\ntest_cust_id = test['Customer_ID'].values\nlr_test_predictions = pd.DataFrame(lr_ml.predict(test_final), columns = mlb.classes_)\nlr_recommendations = extract_top_k_predictions(lr_test_predictions, 3)","39f8cdaa":"# Make Submissions \nlr_recommendations_sub = generate_submission(lr_recommendations, test_cust_id)\nlr_recommendations_sub.to_csv('lr_recommendations_1.csv', index=False)","40ace741":"# Model Object :\nsvm_clf = SVC(probability=True)\nsvm_ml = OneVsRestClassifier(svm_clf, n_jobs=-1)\nsvm_ml.fit(X_train, y_train)\n\n# Make Predictions and compute the precision at K\nsvm_predictions = svm_ml.predict_proba(X_test)\nsvm_predictions_2 = svm_ml.predict(X_test)\n\n# Print the micro and macro precision score\nmacro_precision_svm = precision_score(y_test, svm_predictions_2, average='macro')\nmicro_precision_svm = precision_score(y_test, svm_predictions_2, average='micro')\n\nprint(micro_precision_svm, macro_precision_svm)\n\n# Make Predictions\ntest_cust_id = test['Customer_ID'].values\nsvm_test_predictions = pd.DataFrame(svm_ml.predict(test_final), columns = mlb.classes_)\nsvm_recommendations = extract_top_k_predictions(svm_test_predictions, 3)\n\n# Make Submissions \nsvm_ml.fit(train_final, target)\nsvm_recommendations_sub = generate_submission(svm_recommendations, test_cust_id)\nsvm_recommendations_sub.to_csv('svm_recommendations_1.csv', index=False)","229a1296":"# Model Object :\ndt_clf = DecisionTreeClassifier()\ndt_ml = OneVsRestClassifier(dt_clf, n_jobs=-1)\ndt_ml.fit(X_train, y_train)\n\n# Make Predictions and compute the precision at K\ndt_predictions = dt_ml.predict_proba(X_test)\ndt_predictions_2 = dt_ml.predict(X_test)\n\n# Print the micro and macro precision score\nmacro_precision_dt = precision_score(y_test, dt_predictions_2, average='macro')\nmicro_precision_dt = precision_score(y_test, dt_predictions_2, average='micro')\n\nprint(micro_precision_dt, macro_precision_dt)\n\n# Make Predictions\ntest_cust_id = test['Customer_ID'].values\ndt_test_predictions = pd.DataFrame(dt_ml.predict(test_final), columns = mlb.classes_)\ndt_recommendations = extract_top_k_predictions(dt_test_predictions, 3)\n\n# Make Submissions \ndt_ml.fit(train_final, target)\ndt_recommendations_sub = generate_submission(dt_recommendations, test_cust_id)\ndt_recommendations_sub.to_csv('dt_recommendations_1.csv', index=False)","a9c28f12":"# Model Object :\nrf_clf = RandomForestClassifier()\nrf_ml = OneVsRestClassifier(rf_clf, n_jobs=-1)\nrf_ml.fit(X_train, y_train)\n\n# Make Predictions and compute the precision at K\nrf_predictions = rf_ml.predict_proba(X_test)\nrf_predictions_2 = rf_ml.predict(X_test)\n\n# Print the micro and macro precision score\nmacro_precision_rf = precision_score(y_test, rf_predictions_2, average='macro')\nmicro_precision_rf = precision_score(y_test, rf_predictions_2, average='micro')\n\nprint(micro_precision_rf, macro_precision_rf)\nrf_ml.fit(train_final, target)\n\n# Make Predictions\ntest_cust_id = test['Customer_ID'].values\nrf_test_predictions = pd.DataFrame(rf_ml.predict(test_final), columns = mlb.classes_)\nrf_recommendations = extract_top_k_predictions(rf_test_predictions, 3)\n\n# Make Submissions \nrf_recommendations_sub = generate_submission(rf_recommendations, test_cust_id)\nrf_recommendations_sub.to_csv('rf_recommendations_1.csv', index=False)","02cf4a02":"# Model Object :\next_clf = ExtraTreesClassifier()\next_ml = OneVsRestClassifier(ext_clf, n_jobs=-1)\next_ml.fit(X_train, y_train)\n\n# Make Predictions and compute the precision at K\next_predictions = ext_ml.predict_proba(X_test)\next_predictions_2 = ext_ml.predict(X_test)\n\n# Print the micro and macro precision score\nmacro_precision_ext = precision_score(y_test, ext_predictions_2, average='macro')\nmicro_precision_ext = precision_score(y_test, ext_predictions_2, average='micro')\n\nprint(micro_precision_ext, macro_precision_ext)\next_ml.fit(train_final, target)\n\n# Make Predictions\ntest_cust_id = test['Customer_ID'].values\next_test_predictions = pd.DataFrame(ext_ml.predict(test_final), columns = mlb.classes_)\next_recommendations = extract_top_k_predictions(ext_test_predictions, 3)\n\n# Make Submissions \next_recommendations_sub = generate_submission(ext_recommendations, test_cust_id)\next_recommendations_sub.to_csv('ext_recommendations_1.csv', index=False)","36473448":"# Model Object :\nxgb_clf = XGBClassifier()\nxgb_ml = OneVsRestClassifier(xgb_clf, n_jobs=-1)\nxgb_ml.fit(X_train, y_train)\n\n# Make Predictions and compute the precision at K\nxgb_predictions = xgb_ml.predict_proba(X_test)\nxgb_predictions_2 = xgb_ml.predict(X_test)\n\n# Print the micro and macro precision score\nmacro_precision_xgb = precision_score(y_test, xgb_predictions_2, average='macro')\nmicro_precision_xgb = precision_score(y_test, xgb_predictions_2, average='micro')\n\nprint(micro_precision_xgb, macro_precision_xgb)\nxgb_ml.fit(train_final, target)\n\n# Make Predictions\ntest_cust_id = test['Customer_ID'].values\nxgb_test_predictions = pd.DataFrame(xgb_ml.predict(test_final), columns = mlb.classes_)\nxgb_recommendations = extract_top_k_predictions(xgb_test_predictions, 3)\n\n# Make Submissions \nxgb_recommendations_sub = generate_submission(xgb_recommendations, test_cust_id)\nxgb_recommendations_sub.to_csv('xgb_recommendations_1.csv', index=False)","8d14be63":"# Model Object :\nlgbm_clf = LGBMClassifier()\nlgbm_ml = OneVsRestClassifier(lgbm_clf, n_jobs=-1)\nlgbm_ml.fit(X_train, y_train)\n\n# Make Predictions and compute the precision at K\nlgbm_predictions = lgbm_ml.predict_proba(X_test)\nlgbm_predictions_2 = lgbm_ml.predict(X_test)\n\n# Print the micro and macro precision score\nmacro_precision_lgbm = precision_score(y_test, lgbm_predictions_2, average='macro')\nmicro_precision_lgbm = precision_score(y_test, lgbm_predictions_2, average='micro')\n\nprint(micro_precision_lgbm, macro_precision_lgbm)\nlgbm_ml.fit(train_final, target)\n\n# Make Predictions\ntest_cust_id = test['Customer_ID'].values\nlgbm_test_predictions = pd.DataFrame(lgbm_ml.predict(test_final), columns = mlb.classes_)\nlgbm_recommendations = extract_top_k_predictions(lgbm_test_predictions, 3)\n\n# Make Submissions \nlgbm_recommendations_sub = generate_submission(lgbm_recommendations, test_cust_id)\nlgbm_recommendations_sub.to_csv('lgbm_recommendations_1.csv', index=False)","8b6946ba":"# Build the Deep Learning model (BASELINE)\n\nmodel = Sequential()\nmodel.add(Dense(32, activation='elu', input_shape = [X_train.shape[1]]))\nmodel.add(layers.BatchNormalization())\nmodel.add(Dense(32, activation='elu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(Dense(32, activation='elu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(Dense(32, activation='elu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(Dense(32, activation='elu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.10))\nmodel.add(Dense(target.shape[1], activation='sigmoid'))\nmodel.summary()","220b85ca":"# Compile and fit the model\nmodel.compile(optimizer=\"adam\", loss='binary_crossentropy')\nhist = model.fit(X_train, y_train, epochs = 100, batch_size=64, callbacks=EarlyStopping(patience=5), validation_data=(X_test, y_test))","811e3494":"model.evaluate(X_test, y_test)","33f3710f":"h = hist.history\nplt.plot(h['loss'], label='Training Loss')\nplt.plot(h['val_loss'], label='Validation Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","d2b93d1d":"# Make Predictions and compute precision (Macro and Micro)\nnn_predictions_baseline = model.predict(X_test)\nnn_predictions_baseline_2 = nn_predictions_baseline.round()\nnn_predictions_df = pd.DataFrame(nn_predictions_baseline, columns=mlb.classes_)\n\nmacro_precision_nn = precision_score(y_test, nn_predictions_baseline_2, average='macro')\nmicro_precision_nn = precision_score(y_test, nn_predictions_baseline_2, average='micro')\n\nprint(micro_precision_nn, macro_precision_nn)","d086a2b0":"# Make Predictions\ntest_cust_id = test['Customer_ID'].values\nnn_test_predictions = pd.DataFrame(model.predict(test_final), columns = mlb.classes_)\nnn_recommendations = extract_top_k_predictions(nn_test_predictions, 3)","e9a3ea6e":"# Make Predictions\ntest_cust_id = test['Customer_ID'].values\nnn_recommendations_sub = generate_submission(nn_recommendations, test_cust_id)\nnn_recommendations_sub.to_csv('nn_recommendations_12.csv', index=False)","03eb85ab":"## Data Analysis","ec00fd8b":"## Machine Learning Modelling","789af90d":"**Both distributions have equal distribution of mean Vintage.**","4c052b99":"### KNN (BASELINE)","0e16307d":"### XGBoost","8ceb2cbf":"## DeepLearning Modelling","ab2c8a44":"**Almost equal distributions of Age, Vintage and Is_Active observed across both the Customer Cateogry.**","654969c1":"## Data Preparation","940112fd":"**Both the genders have equal distribution in mean Age.**","5921dfc2":"### Decision Tree","5e5d1369":"**Almost equal distributions of Age, Vintage and Is_Active observed across both the cities.**","b32d6b18":"### RandomForest","50af0aff":"### Logistic Regression","2fc4301b":"# Amex Hackathon\n","bc323d14":"### ExtraTrees "}}