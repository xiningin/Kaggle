{"cell_type":{"cffb4373":"code","83c6e62f":"code","06c26395":"code","081d3cc3":"code","3f53c1ed":"code","5facf8d4":"code","60ca3380":"code","4d049f41":"code","8d3f7737":"code","93b00ce3":"code","ea9a8864":"code","9bdc0dad":"code","15cdd2b5":"code","2d465219":"code","a61892d1":"code","b248fc87":"code","ae2bb5eb":"code","4fa1e990":"code","016b3360":"code","ded36664":"code","fa84a5b5":"code","de82ca0c":"code","6e768965":"code","6c3112b5":"code","da492b76":"code","74c1d31f":"code","106d420e":"code","29c97bb0":"code","faecf074":"code","328b7e15":"code","8cbd2fd8":"code","b2d4023e":"code","13f3ff70":"code","66fce2bb":"code","51d84786":"code","b9b579b7":"code","38779542":"code","72ca5dbe":"code","fa072447":"code","13d9c58a":"code","44d7e089":"code","32a89fb8":"code","6761d8a6":"code","8799110f":"code","ed556066":"code","a1129572":"code","de521cff":"code","94ef83aa":"code","34d09ce8":"code","36af3940":"code","77c43fc3":"code","0b652fa3":"code","d89a30ae":"code","1ff5d149":"code","af261694":"code","054dc06a":"code","36947ff3":"code","79dceca9":"code","ec4d271d":"code","204b583c":"code","4f23190e":"code","a426211c":"code","d95dab04":"code","cdc92cc8":"code","f5bbe365":"code","e1a0e3ae":"code","56d05f01":"code","a306a7fb":"code","ee98cd22":"code","faa46fd8":"code","d78b5cfd":"markdown","ae675a31":"markdown","2d97723b":"markdown","941eb9f4":"markdown","689953a4":"markdown","2a58aba9":"markdown","ef69906e":"markdown","3c2a5b90":"markdown","0dd1e913":"markdown","2c16bf6b":"markdown"},"source":{"cffb4373":"import numpy as np \nimport pandas as pd \nimport datetime\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nimport warnings\nimport itertools\nimport statsmodels.api as sm\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom sklearn.metrics import mean_squared_error\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport seaborn as sns\nsns.set_context(\"paper\", font_scale=1.3)\nsns.set_style('white')\nimport math\nfrom sklearn.preprocessing import MinMaxScaler","83c6e62f":"crude = pd.read_csv('..\/input\/ntt-data-global-ai-challenge-06-2020\/Crude_oil_trend.csv')\n","06c26395":"crude.head(3)","081d3cc3":"crude.tail(3)","3f53c1ed":"crude.info()","5facf8d4":"crude['date'] = pd.to_datetime(crude.Date)","60ca3380":"crude.info()","4d049f41":"crude.set_index('date')","8d3f7737":"crude.info()","93b00ce3":"crude=crude.drop(['Date'], axis=1)","ea9a8864":"crude.set_index(['date'], inplace=True)","9bdc0dad":"y = crude['Price'].resample('MS').mean()","15cdd2b5":"y.plot(figsize=(15, 6))\nplt.show()","2d465219":"rcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(y, model='additive')\nfig = decomposition.plot()\nplt.show()","a61892d1":"data=pd.DataFrame()","b248fc87":"data=crude","ae2bb5eb":"sc = MinMaxScaler(feature_range = (0, 1))\ndf = sc.fit_transform(data)\n","4fa1e990":"train_size = int(len(df) * 0.70)\ntest_size = len(df) - train_size\ntrain, test = df[0:train_size, :], df[train_size:len(df), :]","016b3360":"def create_data_set(_data_set, _look_back=1):\n    data_x, data_y = [], []\n    for i in range(len(_data_set) - _look_back - 1):\n        a = _data_set[i:(i + _look_back), 0]\n        data_x.append(a)\n        data_y.append(_data_set[i + _look_back, 0])\n    return np.array(data_x), np.array(data_y)","ded36664":"look_back =75\nX_train,Y_train,X_test,Ytest = [],[],[],[]\nX_train,Y_train=create_data_set(train,look_back)\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1]))\nX_test,Y_test=create_data_set(test,look_back)\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1]))","fa84a5b5":"X_train.shape","de82ca0c":"Y_train.shape","6e768965":"from sklearn.linear_model import LinearRegression","6c3112b5":"reg = LinearRegression().fit(X_train,Y_train)","da492b76":"reg.score(X_train,Y_train)","74c1d31f":"y_test=reg.predict(X_test)","106d420e":"df1=pd.DataFrame(Y_test, columns=['True'])","29c97bb0":"df1['Prediction']=y_test","faecf074":"df1['Diference']=df1['True']-df1['Prediction']\n","328b7e15":"df1","8cbd2fd8":"from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(df1['True'], df1['Prediction'],  multioutput='raw_values')\nmse","b2d4023e":"data.info()","13f3ff70":"sc = MinMaxScaler(feature_range = (0, 1))\ndf = sc.fit_transform(data)","66fce2bb":"df.shape","51d84786":"train_size = int(len(df) * 0.70)\ntest_size = len(df) - train_size\ntrain, test = df[0:train_size, :], df[train_size:len(df), :]","b9b579b7":"def create_data_set(_data_set, _look_back=1):\n    data_x, data_y = [], []\n    for i in range(len(_data_set) - _look_back - 1):\n        a = _data_set[i:(i + _look_back), 0]\n        data_x.append(a)\n        data_y.append(_data_set[i + _look_back, 0])\n    return np.array(data_x), np.array(data_y)","38779542":"look_back = 75\nX_train,Y_train,X_test,Ytest = [],[],[],[]\nX_train,Y_train=create_data_set(train,look_back)\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test,Y_test=create_data_set(test,look_back)\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))","72ca5dbe":"regressor = Sequential()\n\nregressor.add(LSTM(units = 60, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nregressor.add(Dropout(0.1))\n\nregressor.add(LSTM(units = 60, return_sequences = True))\nregressor.add(Dropout(0.1))\n\nregressor.add(LSTM(units = 60))\nregressor.add(Dropout(0.1))\n\nregressor.add(Dense(units = 1))\n\n\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss',patience=5)\nhistory =regressor.fit(X_train, Y_train, epochs = 20, batch_size = 15,validation_data=(X_test, Y_test), callbacks=[reduce_lr],shuffle=False)\n","fa072447":"train_predict = regressor.predict(X_train)\ntest_predict = regressor.predict(X_test)","13d9c58a":"train_predict = sc.inverse_transform(train_predict)\nY_train = sc.inverse_transform([Y_train])\ntest_predict = sc.inverse_transform(test_predict)\nY_test = sc.inverse_transform([Y_test])","44d7e089":"print('Train Mean Absolute Error:', mean_absolute_error(Y_train[0], train_predict[:,0]))\nprint('Train Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0])))\nprint('Test Mean Absolute Error:', mean_absolute_error(Y_test[0], test_predict[:,0]))\nprint('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0])))\nplt.figure(figsize=(8,4))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Test Loss')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(loc='upper right')\nplt.show();","32a89fb8":"aa=[x for x in range(180)]\nplt.figure(figsize=(8,4))\nplt.plot(aa, Y_test[0][:180], marker='.', label=\"actual\")\nplt.plot(aa, test_predict[:,0][:180], 'r', label=\"prediction\")\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('Price', size=15)\nplt.xlabel('Time step', size=15)\nplt.legend(fontsize=15)\nplt.show()","6761d8a6":"test_predict.shape","8799110f":"df=pd.DataFrame()\ndf=data\nultimosDias = df['2020-01-01':'2020-07-04']\nultimosDias\n","ed556066":"ultimosDias.shape","a1129572":"sc = MinMaxScaler(feature_range = (0, 1))\ndf_p = sc.fit_transform(ultimosDias)","de521cff":"testX, testY = create_data_set(df_p, _look_back=75)\ntestX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))","94ef83aa":"testX","34d09ce8":"test_predict = regressor.predict(testX, batch_size=1)","36af3940":"test_predict.shape","77c43fc3":"test_predict = sc.inverse_transform(test_predict)","0b652fa3":"test_predict","d89a30ae":"Date1 = pd.date_range('2020-07-16', periods=740, freq='D')\ncolumns = ['Date','Price']    \nTest2 = pd.DataFrame(columns=columns)\nTest2['Price'] = pd.to_numeric(Test2['Price'])\nTest2[\"Date\"] = pd.to_datetime(Date1)\nTest2 = Test2.fillna(0)\nTest2.set_index(['Date'], inplace=True)\n","1ff5d149":"from random import randrange","af261694":"Test2.head(3)","054dc06a":"Test2['Price']=np.random.uniform(39,41, Test2.shape[0])","36947ff3":"Test2.head(3)","79dceca9":"test=sc.fit_transform(Test2)","ec4d271d":"test.shape","204b583c":"test.shape[0]","4f23190e":"def create_dataset(dataset, look_back=1):\n  dataX, dataY = [], []\n  for i in range(len(dataset)-look_back-1):\n    a = dataset[i:(i+look_back), 0]\n    dataX.append(a)\n    dataY.append(dataset[i + look_back, 0])\n  return np.array(dataX), np.array(dataY)","a426211c":"testX, testY = create_dataset(test, look_back=75)\ntestX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))","d95dab04":"test_predict = regressor.predict(testX, batch_size=15)","cdc92cc8":"test_predict = sc.inverse_transform(test_predict)","f5bbe365":"df_f=pd.DataFrame(Test2['Price'][:37], columns=['Price'])","e1a0e3ae":"df_f['Price']=test_predict[:37]","56d05f01":"df_f.head(3)","a306a7fb":"df_f.shape","ee98cd22":"df_f.to_csv('sampleSubmission.csv', index=True)","faa46fd8":"y=df_f['Price']\ny.plot(figsize=(15, 6))\nplt.show()","d78b5cfd":"# SELECT THE DATA","ae675a31":"# NTT CHALLENGE ","2d97723b":"# IMPORT THE LIBRERIES","941eb9f4":"This Jupyter-notebook was create for Ing. Africa Sahara Rodriguez Verduzco and Ing. Carlos Omar Chavez Sanchez, both are consultores at Everis M\u00e9xico.","689953a4":"We generate a linear regression, to have as a parameter to overcome the mean squared error that this generates, so as to also know, at what point to stop the training of the LSTM network that we decided will be the model that will create the predictions for us.","2a58aba9":"# LSTM","ef69906e":"# Predictions ","3c2a5b90":"# LINEAR REGRESSION","0dd1e913":"# WE STANDARDIZE","2c16bf6b":"# DO A QUICKLY RESEARCH ABOUT IT "}}