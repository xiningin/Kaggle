{"cell_type":{"2728f587":"code","10423996":"code","2297e40d":"code","89ba4543":"code","a3c62bfa":"code","c895314f":"code","0f0e7289":"code","8aa79732":"code","a4aae7a7":"code","f8dc877e":"code","b47c317e":"code","6eca7f3b":"code","e73c73f6":"code","4f7c062b":"code","aabff61c":"code","9a880223":"code","ca55cded":"code","18a50107":"code","d3c8f9e5":"code","be8f9664":"code","4a5e8c0a":"code","fc3654a0":"code","b9215411":"code","6e65562f":"code","1eb7f9f2":"code","54695b95":"code","5e32dbe7":"code","b1f6f858":"code","96a93423":"code","a79b263f":"code","73f66680":"code","44debb6f":"code","b5bec46e":"code","653eb076":"code","ac35dea5":"code","2c665320":"code","0078c9bd":"code","46493863":"code","2e3351b1":"code","877603bb":"code","f8a45fd7":"code","85825c23":"markdown","671e8bcb":"markdown","651b8ba0":"markdown","c77b4e97":"markdown","f4c70946":"markdown","13350e22":"markdown","a27127b2":"markdown","54bc8be2":"markdown","7894bc72":"markdown","8c24cea1":"markdown","4956d4c3":"markdown","7cf1f1bd":"markdown","2332f732":"markdown","1dcc2fb3":"markdown","50cfd94d":"markdown","992a25d2":"markdown","fb429101":"markdown","07e7b509":"markdown","8911a2c9":"markdown","5dd6c8c5":"markdown","8c69a7f5":"markdown","2cba7cfc":"markdown","bcf332cc":"markdown","a8f78943":"markdown","fca1b8dc":"markdown","27841003":"markdown","858c7c3a":"markdown","5fb1ef48":"markdown","fcde1771":"markdown","fd898443":"markdown","5168e1bc":"markdown","a072268b":"markdown","7ca44f12":"markdown","3758cbd2":"markdown","39fc1a38":"markdown","839414d3":"markdown","1f5258a8":"markdown","85679757":"markdown","ddd58284":"markdown","c094267e":"markdown","f2558bc5":"markdown","2338373d":"markdown","32f969e7":"markdown","f8878c60":"markdown","20965bb9":"markdown","d33611c5":"markdown","6175d219":"markdown"},"source":{"2728f587":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\npd.set_option('precision', 3)\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\n\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nmpl.rcParams['axes.labelsize'] = 14\nmpl.rcParams['axes.titlesize'] = 15\nmpl.rcParams['xtick.labelsize'] = 12\nmpl.rcParams['ytick.labelsize'] = 12\nmpl.rcParams['legend.fontsize'] = 12\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew \nfrom scipy.special import boxcox1p\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, learning_curve\nfrom mlxtend.regressor import StackingCVRegressor\n\nrandom_state = 42\n\nprint ('Libraries Loaded!')","10423996":"train_df = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ntest_df = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\n\nprint ('Dataframes loaded successfully!\\n')\nprint ('The train set contains {} rows and {} columns'.format(train_df.shape[0], train_df.shape[1]))\nprint (' The test set contains {} rows and {} columns'.format(test_df.shape[0], test_df.shape[1]))","2297e40d":"common_cols = train_df.columns & test_df.columns\nprint (\"Column not in common: '{}'\".format(np.setdiff1d(train_df.columns, common_cols)[0]))","89ba4543":"all_data = pd.concat([train_df, test_df]).reset_index(drop = True)\n\nprint ('The combined dataset has {} rows and {} columns'.format(all_data.shape[0], all_data.shape[1]))\nprint (\"Number of NaN values in 'SalePrice': \", all_data['SalePrice'].isnull().sum())","a3c62bfa":"all_data.drop('Id', axis = 1, inplace = True)\nprint(\"'Id' dropped!\")","c895314f":"sns.scatterplot(x = 'GrLivArea', y = 'SalePrice', data = train_df);","0f0e7289":"outliers_idx = train_df[train_df['GrLivArea'] > 4500].index\nall_data.drop(outliers_idx, inplace = True)\n\nprint ('Outliers dropped!')\n\n# Uncomment the following line to check the new plot\n# sns.scatterplot(x = 'GrLivArea', y = 'SalePrice', data = train_df)","8aa79732":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\n\nax1.set_title('Raw - Distplot')\nsns.distplot(train_df['SalePrice'], color = 'darkslateblue',  ax = ax1)\nax2.set_title('Raw - Boxplot')\nsns.boxplot(train_df['SalePrice'], color = 'slateblue', ax = ax2)\nax2.set_xticks([0, 200000, 400000, 600000])\n\nprint ('Skewness: ', np.round(train_df['SalePrice'].skew(), 2))\nprint ('Kurtosis: ', np.round(train_df['SalePrice'].kurt(), 2))","a4aae7a7":"train_df['SalePrice-log'] = np.log1p(train_df['SalePrice'])\nall_data['SalePrice'] = np.log1p(all_data['SalePrice'])\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\n\nax1.set_title('Log Transformed - Distplot')\nsns.distplot(train_df['SalePrice-log'], color = 'darkslateblue', ax = ax1)\nax2.set_title('Log Transformed - Boxplot')\nsns.boxplot(train_df['SalePrice-log'], color = 'darkslateblue', ax = ax2)\n\nprint ('Skewness: ', np.round(train_df['SalePrice-log'].skew(), 2))\nprint ('Kurtosis: ', np.round(train_df['SalePrice-log'].kurt(), 2))","f8dc877e":"missing_counts = all_data.isnull().sum().sort_values(ascending = False)\nmissing_percent = (all_data.isnull().sum()*100\/all_data.shape[0]).sort_values(ascending = False)\n\nmissing_df = pd.concat([missing_counts, missing_percent], axis = 1, keys = ['Counts', '%'])\n\ndisplay(missing_df.head(36).style.background_gradient(cmap = 'Reds', axis = 0))","b47c317e":"for column in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']:\n    all_data[column] = all_data[column].fillna('None')\n\nprint (\"Columns 'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu': Imputation complete!\")","6eca7f3b":"all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nprint (\"Column 'LotFrontage': Imputation complete!\")","e73c73f6":"for column in ['GarageQual', 'GarageFinish', 'GarageCond', 'GarageType']:\n    all_data[column] = all_data[column].fillna('None')\n\nprint (\"Columns 'GarageQual', 'GarageFinish', 'GarageCond', 'GarageType': Imputation complete!\")","4f7c062b":"for column in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n    all_data[column] = all_data[column].fillna(0)\n    \nprint (\"Columns 'GarageYrBlt', 'GarageArea', 'GarageCars': Imputation complete!\")","aabff61c":"for column in ['BsmtCond', 'BsmtExposure', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1']:\n    all_data[column] = all_data[column].fillna('None')\n    \nprint (\"Columns 'BsmtCond', 'BsmtExposure', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1': Imputation complete!\")","9a880223":"for column in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtHalfBath', 'BsmtFullBath']:\n    all_data[column] = all_data[column].fillna(0)\n    \nprint (\"Columns 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtHalfBath', 'BsmtFullBath': Imputation complete!\")","ca55cded":"all_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)\n\nfor column in ['MSZoning', 'Functional', 'Utilities', 'SaleType', \n              'Exterior1st', 'Exterior2nd', 'KitchenQual', 'Electrical']:\n    all_data[column] = all_data[column].fillna(train_df[column].mode()[0])\n    \nprint (\"Remaining columns: Imputation complete!\")","18a50107":"cols_dtype = ['MSSubClass', 'YrSold', 'MoSold', 'OverallCond']\nall_data[cols_dtype] = all_data[cols_dtype].astype(str)\n\nprint ('Data type changed successfully!')","d3c8f9e5":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\nprint (\"Feature 'TotalSF' has been created!\")","be8f9664":"numeric_feats = all_data.iloc[:1458].drop('SalePrice', axis = 1).dtypes[all_data.iloc[:1458].dtypes != 'object'].index\n\nskewed_feats = all_data.iloc[:1458][numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew': skewed_feats})\n\nskewness.head(10)","4a5e8c0a":"skewness = skewness[abs(skewness) > 0.75]\nprint('There are {} skewed numerical features that need Box-Cox transformation!'.format(skewness.shape[0]))","fc3654a0":"skewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n\n    all_data.loc[:1458, feat] = boxcox1p(all_data.loc[:1458, feat], lam)\n    all_data.loc[1458:, feat] = boxcox1p(all_data.loc[1458:, feat], lam)\n    \nprint ('Box-Cox transformation applied successfully!')","b9215411":"thd = 0.95\ncols_drop = []\n\nfor column in train_df.drop('SalePrice', axis = 1):\n    \n    most_freq_value = train_df[column].value_counts(normalize = True).iloc[0]\n    if (most_freq_value > thd):\n        cols_drop.append(column)\n        print ('{}: {}% same value'.format(column, np.round(most_freq_value, 3)))\n        \nall_data.drop(cols_drop, axis = 1, inplace = True)\nprint ('\\nFeatures with low variance dropped successfully!')","6e65562f":"all_data.drop(['GarageCars', 'GarageYrBlt', 'TotRmsAbvGrd', 'TotalBsmtSF'], axis = 1, inplace = True)\nprint ('Features dropped successfully!')","1eb7f9f2":"all_data_before = all_data.copy()\n\nall_data = pd.get_dummies(all_data)\nprint ('One-hot encoding completed!\\n')\nprint ('Shape before: ', all_data_before.shape)\nprint (' Shape after: ', all_data.shape)","54695b95":"train_df = all_data[:1458]\n\nX_train = train_df.drop('SalePrice', 1)\ny_train = train_df['SalePrice']\n\n#######################################################\n\ntest_df = all_data[1458:]\n\nX_test = test_df.copy()\nX_test.drop('SalePrice', axis = 1, inplace = True)\nprint ('Splitting: Done!')","5e32dbe7":"# Step 1: create a list containing all estimators with their default parameters\nest_list = [ElasticNet(alpha = 0.01, l1_ratio = 0.5, random_state = random_state),\n            RandomForestRegressor(random_state = random_state), \n            GradientBoostingRegressor(random_state = random_state),\n            XGBRegressor(random_state = random_state),\n            LGBMRegressor(random_state = random_state)]\n\n\n# Step 2: calculate the cv mean and standard deviation for each one of them\ncv_base_mean, cv_std = [], []\nfor est in est_list:  \n    \n    cv = cross_val_score(est, X_train, y = y_train, scoring = 'neg_root_mean_squared_error', cv = 5, n_jobs = -1)\n    cv_base_mean.append(-cv.mean())\n    cv_std.append(-cv.std())\n\n    \n# Step 3: create a dataframe and plot the mean with error bars\ncv_total = pd.DataFrame({'Algorithm': ['Elastic Net', 'Random Forest','GradientBoostingRegressor', 'XGBRegressor', 'LGBMRegressor'],\n                         'CV-Means': cv_base_mean, \n                         'CV-Errors': cv_std})\n\nsns.barplot('CV-Means', 'Algorithm', data = cv_total, palette = 'Set3', orient = 'h', **{'xerr': cv_std})\nplt.xlabel('RMS Error')\nplt.title('Cross Validation Scores')\nplt.axvline(x = 0.15, color = 'firebrick', linestyle = '--');","b1f6f858":"cv_tuned = []\n\ndef Model_Performance(model_name, model):\n    \n    print(model_name)\n    print('-----------------------------------------')\n    print('        Best Score: ', str(-model.best_score_)) # best_score_: Mean cross-validated score of the best_estimator\n    print('   Best Parameters: ', str(model.best_params_)) \n    \n    arg_min = np.argmin(model.cv_results_['rank_test_score'])\n    scores_list = []\n    for i in ['split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score']:\n        scores_list.append(-model.cv_results_[i][arg_min])\n    \n    cv_tuned.append({'Model': model_name, \n                     'CV Scores': scores_list})\n    \nprint ('Function ready!')","96a93423":"en = ElasticNet(random_state = random_state)\n\nparam_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1], # constant that multiplies the penalty terms, default = 1.0\n              'l1_ratio': [0.25, 0.5, 0.75]} # the mixing parameter, default = 0.5\n\nen_grid = GridSearchCV(en, param_grid = param_grid, cv = 5,\n                       scoring = 'neg_root_mean_squared_error', verbose = True, n_jobs = -1)\nbest_grid_en = en_grid.fit(X_train, y_train)\nModel_Performance('Elastic Net', best_grid_en)","a79b263f":"rf = RandomForestRegressor(random_state = random_state)\nparam_grid = {'n_estimators': [300],    # the number of trees in the forest, default = 100\n              'bootstrap': [True],  # whether bootstrap samples are used when building trees\n              'max_depth': [10, 25, 50],  # the maximum depth of the tree, default = None\n              'max_features': ['auto','sqrt'],  # the number of features to consider when looking for the best split , default = \u201dauto\u201d\n              'min_samples_leaf': [1, 2, 3],  # default = 1\n              'min_samples_split': [2, 3]}  # default = 2\n\ngrid_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5,\n                       scoring = 'neg_root_mean_squared_error', verbose = True, n_jobs = -1)\nbest_grid_rf = grid_rf.fit(X_train, y_train)\nModel_Performance('Random Forest Regressor', best_grid_rf)","73f66680":"gbr = GradientBoostingRegressor(random_state = random_state)\n\nparam_grid = {'loss': ['ls', 'huber'],  # loss function to be optimized (default = \u2019ls\u2019)\n              'n_estimators': [200],    # The number of boosting stages to perform (default = 100)\n              'learning_rate': [0.01, 0.1, 1],  # (default = 0.1)\n              'min_samples_split': [3, 5, 10],  # The minimum number of samples required to split an internal node (default = 2)\n              'max_depth': [3, 5, 10]}  # maximum depth of the individual regression estimators (default = 3)\n\ngrid_gbr = GridSearchCV(gbr, param_grid, cv = 5,\n                        scoring = 'neg_root_mean_squared_error', verbose = True, n_jobs = -1)\nbest_grid_gbr = grid_gbr.fit(X_train, y_train)\nModel_Performance('GradientBoostingRegressor', best_grid_gbr)","44debb6f":"xgb = XGBRegressor(random_state = random_state)\nparam_grid = {'learning_rate' : [0.01, 0.1, 0.5], # Boosting learning rate\n              'n_estimators' : [200],       # Number of gradient boosted trees\n              'max_depth' : [3, 6, 10],     # Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n              'min_child_weight' : [1, 5, 10],  # Increasing this value will make model more conservative.\n              'reg_alpha' : [0.01, 0.1, 1],  # Increasing this value will make model more conservative.\n              'reg_lambda' : [0.01, 0.1, 1]}  # Increasing this value will make model more conservative.\n\ngrid_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5,\n                        scoring = 'neg_root_mean_squared_error', return_train_score = True,\n                        verbose = True, n_jobs = -1)\nbest_grid_xgb = grid_xgb.fit(X_train, y_train)\nModel_Performance('XGBRegressor', best_grid_xgb)","b5bec46e":"lgbm = LGBMRegressor(random_state = random_state)\nparam_grid = {'max_depth' : [5, 10, 15],    # Maximum tree depth, <=0 means no limit (default = -1)\n              'learning_rate' : [0.01, 0.1],    # Boosting learning rate (default = 0.1)\n              'n_estimators' : [250, 500],   # Number of boosted trees to fit (default = 100)\n              'feature_fraction' : [0.4, 0.6, 0.8], # \n              'min_child_samples' : [5, 10, 20]} # Minimum number of data needed in a leaf (default = 20)\n\ngrid_lgbm = GridSearchCV(lgbm, param_grid, cv = 5,\n                         scoring = 'neg_root_mean_squared_error', verbose = True, n_jobs = -1)\n\nbest_grid_lgbm = grid_lgbm.fit(X_train, y_train)\nModel_Performance('LGBMRegressor', best_grid_lgbm)","653eb076":"cv_tuned_mean = [np.round(np.mean(cv_tuned[i]['CV Scores']), 4) for i in range(len(cv_tuned))]\n\ncv_total = pd.DataFrame({'Algorithm': ['Elastic Net', 'Random Forest', 'GradientBoostingRegressor', 'XGBRegressor', 'LGBMRegressor'],\n                         'Baseline': cv_base_mean, \n                         'Tuned Performance': cv_tuned_mean})\n\ncv_total.style.highlight_min(color = 'lightskyblue', axis = 1)","ac35dea5":"colors = ['#7798AB', '#4D5061', '#FFA987', '#724E91', '#C297B8', '#FE4A49']\nmodel_names = [cv_tuned[i]['Model'] for i in range(len(cv_tuned))]\nscores = [cv_tuned[i]['CV Scores'] for i in range(len(cv_tuned))]\nscore_medians = [np.round(np.median(cv_tuned[i]['CV Scores']), 4) for i in range(len(cv_tuned))]\n\nfig, ax = plt.subplots(figsize = (12, 6))\n\nax.set_title('Model Score Comparison', size = 24)\nsns.boxplot(x = model_names, y = scores, palette = colors, ax = ax)\n\nfor xtick in ax.get_xticks():\n    ax.text(xtick, score_medians[xtick] - 0.0015, score_medians[xtick],\n            horizontalalignment = 'center', size = 18, color = 'w', weight = 'semibold')\n\nplt.xlabel('Model', size = 20, labelpad = 12)\nplt.tick_params(axis = 'x', labelsize = 15, rotation = 20)\nplt.tick_params(axis = 'y', labelsize = 15);","2c665320":"boosting_reg = [{'Name': 'GradientBoostingRegressor', 'Model': best_grid_gbr.best_estimator_, 'Color': 'darkgray'},\n                {'Name': 'XGBRegressor', 'Model': best_grid_xgb.best_estimator_, 'Color': 'cadetblue'},\n                {'Name': 'LGBMRegressor', 'Model': best_grid_lgbm.best_estimator_, 'Color': 'steelblue'}]\n\nn_features_vis = 15\n\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize = (18, 6))\n\nfor i in range(len(boosting_reg)):\n    importances = pd.DataFrame({'Feature': X_train.columns,\n                                'Importance': np.round(boosting_reg[i]['Model'].feature_importances_, 4)})\n    importances = importances.sort_values('Importance', ascending = False).set_index('Feature')\n    \n    most_important = importances.head(n_features_vis).iloc[::-1]\n    \n    ax[i].set_title(boosting_reg[i]['Name'])\n    most_important.plot.barh(color = boosting_reg[i]['Color'], edgecolor = 'firebrick', legend = False, ax = ax[i])\n      \nplt.tight_layout();","0078c9bd":"boosting_reg = [{'Name': 'Elastic Net', 'Model': best_grid_en.best_estimator_},\n#                 {'Name': 'Random Forest', 'Model': best_grid_rf.best_estimator_},\n                {'Name': 'GradientBoostingRegressor', 'Model': best_grid_gbr.best_estimator_},\n                {'Name': 'XGBRegressor', 'Model': best_grid_xgb.best_estimator_},\n                {'Name': 'LGBMRegressor', 'Model': best_grid_lgbm.best_estimator_,}]\n\ndef plot_learning_curve(estimators, X, y, nrows, ncols, cv = None, train_sizes = np.linspace(0.1, 1.0, 5)):\n\n    plt.figure(1)\n    fig, axes = plt.subplots(nrows, ncols, figsize = (15, 12))\n    \n    index = 0\n    for row in range(nrows):\n        for col in range(ncols):\n            if (index == len(boosting_reg)): continue\n            \n            estimator = estimators[index]\n            title = estimators[index]['Name']\n            axes[row, col].set_title(title)\n            \n            axes[row, col].set_xlabel('Training Examples')\n            axes[row, col].set_ylabel('Score')\n            \n            train_sizes, train_scores, test_scores = learning_curve(estimators[index]['Model'], X, y, cv = cv, n_jobs = -1, \n                                                                    train_sizes = train_sizes, scoring = 'neg_mean_squared_error')\n    \n            train_scores, test_scores = np.sqrt(-train_scores), np.sqrt(-test_scores)\n    \n            train_scores_mean, train_scores_std = np.mean(train_scores, axis = 1), np.std(train_scores, axis = 1)\n            test_scores_mean, test_scores_std = np.mean(test_scores, axis = 1), np.std(test_scores, axis = 1)\n            \n            axes[row, col].grid()\n            axes[row, col].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n                                     alpha = 0.1, color = 'r')\n            axes[row, col].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n                                     alpha = 0.1, color = 'g')\n            axes[row, col].plot(train_sizes, train_scores_mean, 'ro-', label = 'Training Score')\n            axes[row, col].plot(train_sizes, test_scores_mean, 'go-', label = 'Cross-validation Score')\n            axes[row, col].legend(loc = 'best')\n            \n            index += 1\n            \n#     fig.delaxes(axes[1, 2])\n            \n    plt.tight_layout()\n    plt.show()\n    \nplot_learning_curve(boosting_reg, X_train, y_train, nrows = 2, ncols = 2, cv = 5, train_sizes = np.linspace(0.1, 1.0, 5));","46493863":"stack = StackingCVRegressor(regressors = (best_grid_en.best_estimator_,\n#                                           best_grid_rf.best_estimator_,\n                                          best_grid_gbr.best_estimator_,\n#                                           best_grid_xgb.best_estimator_,\n                                          best_grid_lgbm.best_estimator_),\n                            meta_regressor = best_grid_xgb.best_estimator_, \n                            use_features_in_secondary = True, random_state = random_state)\n\nstack.fit(X_train, y_train);\nstack_score = -cross_val_score(stack, X_train, y_train, scoring = 'neg_root_mean_squared_error', cv = 5)\nprint('StackingCVRegressor trained and validated!')\nprint('Mean RMSErrors: {:.4f} \u00b1 {:.4f}\\n'.format(stack_score.mean(), \n                                                 stack_score.std()))\n\ncv_tuned_mean.append(stack_score.mean())","2e3351b1":"models = ['Elastic Net','Random Forest Regressor', 'GradientBoostingRegressor', 'XGBRegressor', 'LGBMRegressor', 'StackingCVRegressor']\n\nfig = plt.figure(figsize = (14, 6))\n\nsns.pointplot(x = models, y = cv_tuned_mean, color = 'firebrick', markers = 'o', linestyles = '-')\n\nfor index, score in enumerate(cv_tuned_mean):\n    plt.text(index + 0.08, score - 0.0001, '{:.4f}'.format(score), \n             color = 'black', fontsize = 15, weight = 'semibold')\n\nplt.title('Mean Score Comparison', size = 24)\nplt.xlabel('Model', size = 20, labelpad = 12)\nplt.ylabel('Mean Score (RMSE)', size = 20, labelpad = 12)\nplt.tick_params(axis = 'x', labelsize = 15, rotation = 25)\nplt.tick_params(axis = 'y', labelsize = 15);","877603bb":"scores.append(stack_score)\nscore_medians.append(np.round(np.median(stack_score), 4))\n\nfig, ax = plt.subplots(figsize = (12, 6))\n\nax.set_title('Model Score Comparison', size = 24)\nsns.boxplot(x = models, y = scores, palette = colors, ax = ax)\n\nfor xtick in ax.get_xticks():\n    ax.text(xtick, score_medians[xtick] - 0.0015, score_medians[xtick], \n             horizontalalignment = 'center', size = 18, color = 'w', weight = 'semibold')\n\nplt.xlabel('Model', size = 20, labelpad = 12)\nplt.tick_params(axis = 'x', labelsize = 15, rotation = 25)\nplt.tick_params(axis = 'y', labelsize = 15);","f8a45fd7":"model_names = ['Elastic_Net', 'Random_Forest', 'GradientBoostingRegressor', 'XGBRegressor', 'LGBMRegressor', 'StackingRegressor']\nmodels = [best_grid_en, best_grid_rf, best_grid_gbr, best_grid_xgb, best_grid_lgbm, stack]\n\nsub_df = pd.read_csv('..\/input\/home-data-for-ml-course\/sample_submission.csv')\n\nfor i in range(len(models)):\n    y_pred = models[i].predict(X_test)\n    y_pred_trans = np.expm1(y_pred)\n\n    sub_df['SalePrice'] = y_pred_trans\n    sub_df.to_csv('{}-{}.csv'.format(i, model_names[i]), index = False)\n\nprint ('Submission files created!')","85825c23":"Additionally, [EDA](https:\/\/www.kaggle.com\/korfanakis\/housing-prices-clear-and-concise-eda) showed that some features exhibit **multicollinearity**. We can drop them now:","671e8bcb":"## Dealing with Skewed Features\n\nI didn't know much about dealing with skewed features before this competition. Serigne's [notebook](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) helped me a lot!\n\nSkewness is a measure of the asymmetry of the distribution of a variable, and if there is too much of it in our data, some models may perform poorly. \n\nWe will use **Box-Cox transformation** to fix our data. I am still learning this techniques so there might be mistakes (please tell me in the comment section \ud83e\uddd0). I won't go into detail about it, but you can learn more [here](https:\/\/towardsdatascience.com\/box-cox-transformation-explained-51d745e34203) and [here](http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html).\n\nLet's have a look at the most skewed features of our **training set** (again, we have to be careful for data leakage):","651b8ba0":"## Missing Values\n\nUnfortunately, many attributes contain missing values. We can see both the count and the percentage of missing values for each column:","c77b4e97":"## Changing Data Type\n\nThe attribute 'MSSubClass' identifies the type of dwelling involved in the sale and is written so that it maps categories to integer values (such as '1-STORY 1946 & NEWER ALL STYLES' to 20, etc.) We can change its data type to string by using the `astype()` method.\n\nWe will also use the same method for changing three other columns to string.","f4c70946":"### XGBRegressor\n\nWe can also use `XGBRegressor()` from the [XGBoost library](https:\/\/xgboost.readthedocs.io\/en\/latest\/) which is 'an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable'.","13350e22":"# Conclusions\n\nCongratulations! Our kernel just came to an end! \ud83c\udf89\ud83c\udf89\n\nAs a brief conclusion, we could say that:\n\n- We used quite a few techniques for processing our data (imputation, feature engineering, etc). It might feel tedious and less enjoyable than training ML models, however, it's  an important step in any Machine Learning workflow. \n- We spent a lot of time (and computing power) tweaking hyperparameters for each model. This enabled us to improve their predictive performance. We also used StackingCVRegressor to build an ensemble of all our individual models.\n\n---\n\nFeel free to ask me anything in the comment section. I would also like to hear <font size = 3 color = \"forestgreen\"><b> suggestions <\/b><\/font> for improving my analysis!\n\nPlease <font size = 3 color = \"orangered\"><b> upvote <\/b><\/font> if you liked this notebook! Thank you!\ud83d\ude09\ud83e\uddd0","a27127b2":"# Libraries\n\nWe start by importing the necessary libraries and setting some parameters for the whole notebook (such as parameters for the plots, etc.). We will mainly use:\n\n- Pandas for handling and analysing data,\n- Seaborn and Matplotlib for data visualization, and\n- Scikit-learn for building Machine Learning models.","54bc8be2":"### LotFrontage\t\n\nThis attribute represents the 'Linear feet of street connected to property'. I have decided to follow this [notebook](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) and replace missing values with the median 'LotFrontage' of the neighborhood. For this purpose, we need the `groupby` method:","7894bc72":"### GarageQual, GarageFinish, GarageCond, and GarageType\n\nFor this set of categorical attributes, NA indicates 'No Garage'.","8c24cea1":"### GradientBoostingRegressor\n\nBoosting is an Ensemble technique in which predictors are not made independently of each other, but sequentially. The general idea is to train predictors sequentially, each trying to correct its predecessor. \n\nGradient boosting in particular works by fitting each new predictor to the residual errors made by the previous predictor. You can have a look at page 205 of [this book](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/) to see how Gradient Boosting can improve an ensemble's prediction.\n\nWe can use Scikit-learn's `GradientBoostingRegressor` class:","4956d4c3":"### Remaining Attributes\n\nI won't go into details about the remaing attributes that need to be imputed. You can read the description to understand why I choose the particular value in each case.","7cf1f1bd":"We can visually observe that our target variable appears more normally distributed now. Both the skewness (measure of the asymmetry) and kurtosis (measure of the tailedness) of the distribution decreased after the transformation.\n\n**Note**: we should not forget to take the **inverse** of this transformation (np.expm1) before submitting our predictions.","2332f732":"Our target variable deviates from the symmetrical bell curve we would expect from a normal distribution. Specifically, it is right-skewed (similar terms: right-tailed or skewed to the right) since the right tail is longer.\n\nThis could be a problem since many ML algorithms don't do well with data that are not normally distributed. We can correct for this by performing a **log-transformation** of the target variable:","1dcc2fb3":"**Note**: For the last 8 attributes, we fill the missing values with the most frequent entry of the training set only. This ensures that information from outside the training set won't be used to create ML modes, thus protecting our models from data leakage.","50cfd94d":"## Learning Curves\n\nLearning curves are plots of a model\u2019s performance on the training set and the validation set as a function of the training set size. They can help us visualize overfitting\/underfitting and the effect of the training size on a model's error.\n\nTo generate these curves, simply train the model several times on different sized subsets of the training set:","992a25d2":"## Stacking\n\nIn the last section of this notebook, we will try to improve predictive performance by building an ensemble. I have chosen Stacking Regression for this task (as many other participants in this competition).\n\nSpecifically, we will use the `StackingCVRegressor` from the `mlxtend` library. In the [documentation](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingRegressor\/) we read that:\n\n<blockquote>Stacking regression is an ensemble learning technique to combine multiple regression models via a <b>meta-regressor<\/b>. The individual regression models are trained based on the complete training set; then, the meta-regressor is fitted based on the outputs -- meta-features -- of the individual regression models in the ensemble.<\/blockquote>\n\n<img src=\"http:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingRegressor_files\/stackingregression_overview.png\" width = 300>\n<center>Schematic overview of Stacking regression<\/center>\n\nLet's start by building our stack. I have excluded the Random Forest Regressor (didn't help much) and used the XGBRegressor as the meta-regressor:","fb429101":"<br>\n<br>\n\nAfter performing cross-validation on the best set of parameter for each algorithm, we can now compare theis performance **before** and **after** tuning: ","07e7b509":"### GarageYrBlt, GarageArea and GarageCars\n\nThese attributes are numeric and their missing values are related to the absence of a garage. Therefore, we could replace them with 0:","8911a2c9":"StackingCVRegressor has a lower mean RMSE compared to the other models and similar scores with the second best model. It will also generalize better to new data, thus reducing the impact of overfitting.\n\nTherefore, we will use the **StackingCVRegressor** for submission, which scores 13694 on the Leaderboard (<font size = 3 color = \"crimson\"><b>Top 2%<\/b><\/font>)!","5dd6c8c5":"## Target Variable: 'SalePrice'\n\nLet's take a quick look at 'SalePrice', the value we want to predict:","8c69a7f5":"We note that:\n\n- The error on the training set is always lower compared to the validation set (no surprise there, all models perform worse on unseen data),\n- There is a significant gap between the two curves for all models, meaning that they perform better on the instances of the training set than on the validation set. Unfortunately, this indicates overfitting.","2cba7cfc":" # Building Machine Learning Models\n \nWe are ready to have some fun! Before we to that, we need to select a **performance measure**. I have picked the **Root Mean Square Error** (**RMSE**), a typical performance measure for\nregression problems that measures the standard deviation of the errors the system makes in its predictions:\n\n<center> $RMSE = \\sqrt{\\sum_{i=1}^{n}\\frac{(\\hat{y}_{i} - y_{i} )^{2}}{n}}$ <\/center>\n\nwhere, $y_{1}, y_{2}, . . . , y_{n} $ are the observed values, <br>\n$\\hat{y}_{1}, \\hat{y}_{2}, . . . , \\hat{y}_{n}$ are the predicted values, and <br>\n$n$ is the total number of observations.\n\n<font size=+0 color='#009E0C'><b>Let the fun begin!<\/b><\/font>\n \n ## Baseline Models\n \nThe aim of this subsection is to calculate the **baseline performance** of 5 different estimators\/regressors on the training set. This will enable us to later see how tuning improves each of these models.\n\nThe regressors are:\n\n1) Elastic Net, <br>\n2) Random Forest Regressor, <br>\n3) GradientBoostingRegressor, <br>\n4) XGBRegressor, and <br>\n5) LGBMRegressor.\n\nI will give a little bit more information about these models in the next subsection, when we will tune their performance.\n\nFor the baseline models, we will use the default parameters for each regressor and evaluate performance by calculating the room mean squared error using **k-fold cross validation**.\n\nThe idea behind k-fold cross validation, which is illustrated in the following figure, is simple: it splits the (training) set into k subsets\/folds, trains the models using k-1 folds and evaluates the model on the remaining one fold. This process is repeated until every fold is tested once.\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png\" width = 400>\n<center>Taken from the official documentation on scikit-learn's website<\/center>\n\nK-fold cross validation leads to less biased models since it ensures that every instance from the original training set will be used both for training and testing.\n\nWe can implement cross validation by using the `cross_val_score()` method from scikit-learn. We will use k = 5 folds.","bcf332cc":"# Submission","a8f78943":"### Elastic Net\n\nThe first model is Elastic Net, a regularized regression model. In general, we prefer having a little bit of regularization in our models (to reduce overfitting), hence why we should\navoid plain Linear Regression.\n\nElastic Net is the middle ground between two other regularized linear regression models: Ridge and Lasso regression. The difference between each model lies in the regularization term of the cost function (please read [Chapter 4](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/) for more info).\n\nThe parameter 'alpha' controls how much we want to regularize the model, while 'l1_ratio' controls the mix ratio between Ridge and Lasso's regularization terms (0 for 100% Ridge, and 1 for %100 Lasso).","fca1b8dc":"## One-hot Encoding\n\nAlmost there! Machine Learning models can only get trained on numerical data; that's why we need to convert categorical variables into a form that is suitable for ML algorithms. One-hot encoding does exactly that: it substitutes every categorical column with $n$ columns ($n$: the number of categories), one for each of its categories. Every instance\/row get the value of 1 for the column it corresponds to, and 0 for the remaining $n -1 $ columns.\n\nAs an example, image we have an attribute 'Color' with three categories:\n\n| Color |\n|-------|\n| red   |\n| green |\n| blue  |\n\nOne-hot encoding will transform this attribute into the following matrix:\n\n| Color_red | Color_green | Color_blue |\n|:---------:|:-----------:|:----------:|\n|     1     |      0      |      0     |\n|     0     |      1      |      0     |\n|     0     |      0      |      1     |\n\n<br>\n\nIn our case, we will use the `get_dummies()` method:","27841003":"We need to take care of each attrbitute individually:\n\n### PoolQC, MiscFeature, Alley, Fence and FireplaceQu\n\nSomeone would argue that we need to discard these attributes since they have a large number of missing values. However,  after reading the description, it turns out that these values are not actually missing:\n\n- 'PoolQC': NA indicates 'No Pool',\n- 'MiscFeature': NA indicates 'No miscellaneous feature',\n- 'Alley': NA represents 'No alley access',\n- 'Fence': NA indicates 'No Fence', and\n- 'FireplaceQu': NA means 'No Fireplace'.\n\nTherefore, we should fill missing values with a string indicating the absence of what is been described:","858c7c3a":"Notice how important 'TotalSF' is! Feature engineering proved quite useful in our case. You could try engineering new features and see if they make it to the top15 list. \n\nAdditionally, it's not a surprise that features such as 'OverallQual', 'GrLivArea' and 'GarageCars' play a significant role in predicting the final price.","5fb1ef48":"# Getting the Data\n\nThe data has already been split into a training set ('train.csv') and a test set ('test.csv'). We can use the `read_csv()` method to load them as Pandas dataframes:","fcde1771":"## Feature Engineering\n\nWe goal of Feature Engineering is to create useful new features from our existing ones. Specifically, we will merge the area-related features ('TotalBsmtSF', '1stFlrSF' and '2ndFlrSF') into one new features titled 'TotalSF'. Some models can calculate the importance of each feature in the final prediction, and 'TotalSF' is indeed a very important\/useful feature (as we will see later).\n\nI also tried other ideas such as a 'TotalLot' feature ('LotArea' + 'LotFrontage'), but they weren't as important as 'TotalSF'.","fd898443":"### LGBMRegressor\n\n[LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/index.html) is another gradient boosting framework (developed by Microsoft) that uses tree based learning algorithms.","5168e1bc":"The difference in the number of columns\/attributes is due to the fact that the training set is labeled, i.e. it contains a column for the price of each house. This set should be used to build our machine learning models, while the test set should be used to see how well our model performs on unseen\/unlabeled data.\n\nWe are going to **merge** the two dataframes into one. The new dataframe will have NaN in the 'SalePrice' column for instances of the test set:","a072268b":"# Data Preprocessing \n\nI will mention again that there is seperate notebook for EDA, which you can find [here](https:\/\/www.kaggle.com\/korfanakis\/housing-prices-clear-and-concise-eda). For the shake of completion, part of my EDA will be shown here too.\n\n## Outliers\n\nThe documentation for the [Ames Housing Data](http:\/\/jse.amstat.org\/v19n3\/decock.pdf) states that there are **outliers present** in the training set and suggests we plot 'SalePrice' against 'GrLivArea' (i.e. 'Above ground living area square feet') to see them:","7ca44f12":"## Model Tuning\n\nWe are ready to tune hyperparameters using **grid search** and see if performance improves. Grid search (provided by scikit-learn's `GridSeachCV`) 'exhaustively generates candidate models from a grid of parameter values specified with the `param_grid` parameter'. It then evaluates each model using cross validation, thus allowing us to determine the optimal values for all hyperparameters.\n\nI won't go into detail about the hyperparameters of each model. If you want to know more, please visit the corresponding documentation.\n\n<br>\n\nWe write a simple performance reporting function (inspired from [Ken](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example)'s kernel). The function will:\n- Print the best score,\n- Print the best parameters (i.e. the parameters that lead to the best score), and\n- Append each CV score to a list (we will use it for plotting later).","3758cbd2":"Tuning indeed **improved** performance for all models! Apart from the Random Forest Regressor, all other models have a **similar mean RMSE around 0.125**.\n\n<br>\n\nWe can also use a boxplot to compare the CV scores for all models (Orhan's [notebook](https:\/\/www.kaggle.com\/orhankaramancode\/ensemble-stacked-regressors-92-acc\/notebook#Final-Preprocessing-Steps) gave me this idea). The shape doesn't look perfect, but that's because we only have 5 CV scores for each model. The number on top of each box indicates the median.","39fc1a38":"## Feature Selection\n\n[EDA](https:\/\/www.kaggle.com\/korfanakis\/housing-prices-clear-and-concise-eda) can confirm that some features display **low variance** which means that they are close to being constant. Therefore, they do not provide any information to a ML model for learning the patterns in the data and they **should be removed**.\n\nI have writen a simple for loop for identifying such features based on a threshold value (0.95 in our case, i.e. features in which 95% of instances have the same value):","839414d3":"##  Feature Importance\n\nWe can visualize feature importance for each of the three last regressors (I will only show the 15th most important features, but you can show more by changing the 'n_features_vis' variable):","1f5258a8":"\n\nLastly, we need to split the combined dataset back into train and test set:","85679757":"### BsmtCond, BsmtExposure, BsmtQual, BsmtFinType2, BsmtFinType1\n\nThese features are categorical and an NA value indicates that there is 'no basement'.","ddd58284":"We can visually compare its mean RMSE with the remaining models:","c094267e":"All models score below 0.15. Also, the three Boosting regressors perform better.","f2558bc5":"### BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtHalfBath and BsmtFullBath \n\nWe will replace missing values for this set with 0.","2338373d":"# Bibliography\n\nI used several sources, but most notably:\n\n1) [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/) by Aur\u00e9lien G\u00e9ron\n\n2) [Practical Statistics for Data Scientists, 2nd Edition](https:\/\/www.oreilly.com\/library\/view\/practical-statistics-for\/9781492072935\/), by Peter Bruce, Andrew Bruce and Peter Gedeck\n\n<br>\n\nThe following kernels helped me a lot in my analysis (especially during processing):\n\n3) [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) by Serigne, \n\n4) [Data Science Workflow TOP 2% (with Tuning)](https:\/\/www.kaggle.com\/angqx95\/data-science-workflow-top-2-with-tuning) by aqx,\n\n5) [Ensemble | Stacked Regressors | ~92% Acc. |](https:\/\/www.kaggle.com\/orhankaramancode\/ensemble-stacked-regressors-92-acc\/output) by Orhan <br>","32f969e7":"We should note that there are other outliers present in the dataset. We will tackle this problem by making our models **robust** to them.","f8878c60":"<font size = +3 color='#1E1E1E'><center><b>Housing Prices: A Simple Approach to Top 2% \ud83c\udfd8\ufe0f\ud83d\udcb0\ud83d\udcc8<\/b><\/center><\/font>\n\n<img src=\"https:\/\/images.unsplash.com\/photo-1508404140693-f3a1a3e2a85d?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1338&q=80\" width = 550>\n<center>Photo by Annie Spratt (Unsplash)<\/center>\n\n# Overview\n\nHello, readers! This notebook is my attempt at the [Housing Prices Competition](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course), one of the most famous competitions on Kaggle. \n\nThe dataset, compiled by Dean De Cock, contains information about (almost) every aspect of residential homes in Ames, Iowa.  Our **goal** is to predict the final price of a house based on its characteristics. Therefore, it is typical **regression** task, since we need to predict a continuous value.\n\nSpecifically, \n\n- The first step is **processing** our data. In other words, we will convert our (raw) data to a format that is suitable for Machine Learning algorithms.\n- Then, we will **build** several Machine Learning models and **evaluate** their predictive performance.\n\nThe **skills** we will learn: imputation, feature engineering, dealing with skewed features, feature selection, encoding, machine learning regressors, hyperparameter tuning, feature importance, stacking and more.\n\n<br>\n\nI have a seperate notebook for <font size = +0 color = 'mediumblue'><b> Exploratory Data Analysis<\/b><\/font> of this dataset, which you can find [here](https:\/\/www.kaggle.com\/korfanakis\/housing-prices-clear-and-concise-eda). It's clear and concise, so please have a look at it if you are not familiar with the dataset. Similar to my kernel for the [Titanic](https:\/\/www.kaggle.com\/korfanakis\/titanic-a-beginner-friendly-approach-to-top-3) competition, I have included text to explain my reasoning\/workflow and make this kernel as <font size = +0 color = 'forestgreen'><b>beginner friendly<\/b><\/font> as possible.\n\n<br>\n\nMy analysis is <font size = +0 color = 'firebrick'><b> not complete yet<\/b><\/font>, so it might have omissions and possibly a few mistakes. Please do let me know in the comment section if you spot any.\n\n<br>\n\nPlease consider <font size=+0 color='#E27400'><b>upvoting<\/b><\/font> if you found it useful! \ud83e\uddd0\n    \n<br>\n\n**Table of Contents**\n\n1. [Overview](#Overview)\n2. [Libraries](#Libraries)\n3. [Getting the Data](#Getting-the-Data)\n4. [Data Preprocessing ](#Data-Preprocessing)\n    1. [Outliers](#Outliers)\n    2. [Target Variable: 'SalePrice'](#Target-Variable:-'SalePrice')\n    3. [Missing Values](#Missing-Values)\n    4. [Changing Data Type](#Changing-Data-Type)\n    5. [Feature Engineering](#Feature-Engineering)\n    6. [Dealing with Skewed Features](#Dealing-with-Skewed-Features)\n    7. [Feature Selection](#Feature-Selection)\n    8. [One-hot Encoding](#One-hot-Encoding)\n5. [Building Machine Learning Models \ud83d\udcc8](#Building-Machine-Learning-Models)\n    1. [Baseline Models](#Baseline-Models)\n    2. [Model Tuning](#Model-Tuning)\n    3. [Feature Importance](#Feature-Importance)\n    4. [Learning Curves](#Learning-Curves)\n    5. [Stacking](#Stacking)\n6. [Submission](#Submission)\n7. [Bibliography](#Bibliography)\n8. [Conclusions](#Conclusions)","20965bb9":"### RandomForestRegressor\n\nTo understand Random Forest, we first need to examine Decision Trees. [Decision trees](https:\/\/scikit-learn.org\/stable\/modules\/tree.html) are versatile Machine Learning algorithms that make predictions by learning simple decision rules inferred from the features in our data.\n\nA [Random Forest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html), as the name implies, consists of a large number of individual Decision Trees that are trained independently on a random subset of our data. This is a perfect example of **[Ensemble Learning](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/)**.\n\nWe should note that a Random Forest Regressor has all the hyperparameters of a Decision Tree (to control how trees grow), plus hyperparameters to control the ensemble itself (such as 'n_estimators').","d33611c5":"The two points at the lower right part of the plot (extremely large area for a low price) could be classified as outliers. We will follow the documentation and remove points 'more than 4500 square feet from the data set' (I have decided to change the threshold to 4500 instead of 4000):","6175d219":"'Id' is unique to each instance and can be dropped:"}}