{"cell_type":{"e95beb9e":"code","5129256c":"code","fbc61973":"code","16d90a21":"code","183615db":"code","bb4d549f":"code","c5a28e55":"code","0e0d4ce5":"code","f16a45f6":"code","4ae8b6f0":"code","e173b557":"code","bba0a62c":"code","c974737e":"code","c06f8977":"code","ce555ce7":"code","7109180e":"code","25297206":"code","1976992d":"code","5cbee77b":"code","c3c0658d":"code","8decdce6":"code","e4fb3393":"code","0ee4df22":"code","6ec021f5":"code","79348a95":"markdown","464614e3":"markdown","1fbcc9c7":"markdown"},"source":{"e95beb9e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5129256c":"# !pip install -U tensorflow-gpu==1.15.2\n# !pip install keras==2.3.0\n# !pip install scikit-learn==0.22.1\n!pip install pymystem3\n# !pip install numpy==1.18.2\n# !pip install pandas==0.25.3\n# !pip install seaborn==0.10.0\n# !pip install matplotlib==3.2.1\n!pip install missingno==0.4.2","fbc61973":"# Load libraries\nimport tensorflow\nprint(tensorflow.__version__) # make sure the version of tensorflow\nimport numpy as np # for scientific computing\nimport pandas as pd # for data analysis\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns # for data visualization\nimport missingno as msno # for missing data visualization\nimport collections\nimport nltk\nimport codecs\nimport string\nimport re\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom collections import Counter \nfrom keras.initializers import Constant\nfrom keras.callbacks import ModelCheckpoint\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, SpatialDropout1D, Embedding, LSTM, Conv1D, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Dropout\nplt.style.use('ggplot')\nnp.random.seed(42) # set the random seeds","16d90a21":"train = pd.read_csv('\/kaggle\/input\/kinopoisk-remove-stopwords\/train_proc.csv', low_memory = False)\ntest = pd.read_csv('\/kaggle\/input\/kinopoisk-remove-stopwords\/test_proc.csv', low_memory = False)\ntrain['target'] = train[['positive', 'negative', 'neutral']].idxmax(axis=1)\n\ntrain = train[['preprocess_text', 'target']]","183615db":"train.head()","bb4d549f":"test.head()","c5a28e55":"sns.countplot(x='target', data=train)","0e0d4ce5":"train = train[train['preprocess_text'] != '']","f16a45f6":"max_len = 881\nmax_fatures = 500000","4ae8b6f0":"tokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(train['preprocess_text'].values)\n\ntrain_converted = tokenizer.texts_to_sequences(train['preprocess_text'].values)\ntrain_converted = pad_sequences(train_converted, maxlen=max_len)\n\ntest = tokenizer.texts_to_sequences(test['preprocess_text'].values)\ntest = pad_sequences(test, maxlen=max_len)","e173b557":"target_converted = pd.get_dummies(train['target']).values","bba0a62c":"pd.get_dummies(train['target'])","c974737e":"print('The shape of train data :', train_converted.shape)\nprint('The shape of test data :', test.shape)\nprint('The shape of target of the training :', target_converted.shape)","c06f8977":"X_train, X_test, Y_train, Y_test = train_test_split(train_converted, target_converted, test_size = 0.1, random_state = 42)\n\ntrain_converted_shape = train_converted.shape[1]\ndel(train_converted)\ndel(target_converted)\n\nvalidation_size = 5000\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\n\nprint('The shape of train data :', X_train.shape)\nprint('The shape of labels of train data :', Y_train.shape)\nprint('The shape of test data :', X_test.shape)\nprint('The shape of test label data :', Y_test.shape)","ce555ce7":"# embed_dim = 256\n# lstm_out = 196\n# batch_size = 4\n# EPOCHS = 10\n\n# model = Sequential()\n# model.add(Embedding(max_fatures, embed_dim,input_length = train_converted_shape))\n# model.add(SpatialDropout1D(0.3))\n# model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))\n# model.add(Dense(3,activation='softmax'))\n# model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = [tensorflow.keras.metrics.AUC()])\n# print(model.summary())","7109180e":"# history = model.fit(X_train, Y_train, epochs = EPOCHS, batch_size=batch_size, \n#                     validation_data=(X_validate, Y_validate), verbose = 2)\n\n# train_acc = history.history['auc']\n# test_acc = history.history['val_auc']\n# x = np.arange(len(train_acc))\n# plt.plot(x, train_acc, label = 'train auc')\n# plt.plot(x, test_acc, label = 'test auc')\n# plt.title('Train and validation auc')\n# plt.xlabel('Number of epochs')\n# plt.ylabel('auc')\n# plt.legend()","25297206":"NUM_FILTERS = 1024\nNUM_WORDS = 7\nembed_dim = 256\nbatch_size = 32\nEPOCHS = 1\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim, input_length = train_converted_shape))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS, activation=\"relu\"))\nmodel.add(GlobalMaxPooling1D())\n#model.add(Dense(256, activation=\"linear\"))\nmodel.add(Dense(3, activation=\"softmax\"))\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics=[tensorflow.keras.metrics.AUC()])\nprint(model.summary())","1976992d":"history = model.fit(X_train, Y_train, batch_size=batch_size,\n                    epochs=EPOCHS, validation_data=(X_validate, Y_validate),\n                    callbacks=[ModelCheckpoint('bm.h5', monitor='val_loss', verbose=1, save_best_only=True)])\n\n# train_acc = history.history['auc']\n# test_acc = history.history['val_auc']\n# x = np.arange(len(train_acc))\n# plt.plot(x, train_acc, label = 'train auc')\n# plt.plot(x, test_acc, label = 'test auc')\n# plt.title('Train and validation auc')\n# plt.xlabel('Number of epochs')\n# plt.ylabel('auc')\n# plt.legend()","5cbee77b":"submit = pd.read_csv('\/kaggle\/input\/ml-guild-classification-task\/sample_submission.csv')\nsubmit.head()","c3c0658d":"pred = model.predict(test, batch_size=8)\npred[:10]","8decdce6":"submit[['negative', 'neutral', 'positive']] = pred","e4fb3393":"submit[['positive', 'negative', 'neutral']] = submit[['positive', 'negative', 'neutral']].apply(round)","0ee4df22":"submit[(submit.negative == 0) & (submit.neutral == 0) & (submit.positive == 0)]","6ec021f5":"submit.to_csv('submission.csv', index=False)","79348a95":"## 1D CNN","464614e3":"## LSTM","1fbcc9c7":"* Submit writing\n* Imblearn - for class balance\n* Blending LSTM & CNN if it reasonable\n* Working with other input data"}}