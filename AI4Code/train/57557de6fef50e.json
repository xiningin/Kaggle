{"cell_type":{"fb531825":"code","49752632":"code","a58f1787":"code","0e2a30d8":"code","cf8b4a8c":"code","1894587c":"code","d703922c":"code","96d9e45c":"code","ed1ce5d1":"code","89b507c9":"code","26d4b2f8":"code","5196dadd":"code","97467fd1":"code","7e8c9785":"code","02186da2":"markdown","cf363f92":"markdown","3471c3ec":"markdown"},"source":{"fb531825":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os, gc\n\nimport pandas as pd\nimport numpy as np\n\nimport janestreet\nimport xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom scipy.optimize import curve_fit\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom joblib import dump, load\n\nimport tensorflow as tf\ntf.random.set_seed(41)\nimport tensorflow.keras.backend as K\n\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom typing import List","49752632":"def create_mlp(num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate):\n\n    inp = tf.keras.layers.Input(shape = (num_columns, ))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)): \n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)    \n\n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation('sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = label_smoothing), \n                  metrics = tf.keras.metrics.AUC(name = 'AUC'), \n                 )\n    \n    return model","a58f1787":"def create_mlp_wrapper(train, params, filename, features):\n    batch_size = params['batch_size']\n    hidden_units = [params['hidden_unit_1'], params['hidden_unit_2'], params['hidden_unit_3'], params['hidden_unit_4']]\n    dropout_rates = [params['dropout_rate_1'], params['dropout_rate_2'], params['dropout_rate_3'], params['dropout_rate_4'], params['dropout_rate_5']]\n    label_smoothing = params['label_smoothing']\n    learning_rate = params['learning_rate']\n    time='all'\n#     features = [c for c in train.columns if 'feature' in c]\n    \n    oof = []\n    gkf = GroupKFold(n_splits = 5)\n    scores = []\n    utility_noisy = pd.DataFrame(columns=features,index=range(5))\n    scores_noisy = pd.DataFrame(columns=features,index=range(5))\n    for fold, (tr, te) in enumerate(gkf.split(train['resp'].values, train['resp'].values, train['date'].values)):\n        X_tr, X_val = train.iloc[tr][features].values, train.iloc[te][features].values\n        y_tr, y_val = train.iloc[tr]['action'].values, train.iloc[te]['action'].values\n\n        ckp_path = f'JSModel_{filename}_{fold}.hdf5'\n        model = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, label_smoothing, learning_rate)\n        rlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = 0, \n                                min_delta = 1e-4, mode = 'max')\n        ckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n                              save_best_only = True, save_weights_only = True, mode = 'max')\n        es = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n                           baseline = None, restore_best_weights = True, verbose = 0)\n        model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 1000, \n                  batch_size = batch_size, callbacks = [rlr, ckp, es], verbose = 0)\n        rng = np.random.default_rng()\n        for i, feature in enumerate(features):\n            gc.collect()\n            X_val_noisy = X_val.copy()\n            X_val_noisy[:,i] = rng.permutation(X_val_noisy[:,i])\n            pred = model.predict(X_val_noisy, batch_size = batch_size * 4).ravel()\n            scores_noisy.loc[fold, feature] = roc_auc_score(y_val, (pred>.5).astype(int))\n            utility_noisy.loc[fold, feature] = utility_score(train.iloc[te]['date'].values, train.iloc[te]['weight'].values, train.iloc[te]['resp'], (pred>.5).astype(int))\n        oof.append(model.predict(X_val, batch_size = batch_size * 4).ravel())\n        score = roc_auc_score(y_val, oof[fold])\n        print(f'Fold {fold} ROC AUC:\\t', round(score, 4))\n\n        K.clear_session()\n        del model\n        rubbish = gc.collect()\n\n    return oof, scores_noisy, utility_noisy\n","0e2a30d8":"def utility_score(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u","cf8b4a8c":"train = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\nfeatures = [c for c in train.columns if 'feature' in c]\nf_mean = train[features[1:]].mean()\ntrain = train.query('weight > 0').reset_index(drop = True)\ntrain[features[1:]] = train[features[1:]].fillna(f_mean)\ntrain['action'] = (train['resp'] > 0).astype('int')\nparams = {'batch_size': 4096, 'dropout_rate_1': 0.08949964830076425, 'dropout_rate_2': 0.09214922878769727, 'dropout_rate_3': 0.10007316489415027, 'dropout_rate_4': 0.2416444299467454,  'dropout_rate_5': 0.1338760637313039,   'hidden_unit_1': 245, 'hidden_unit_2': 678,  'hidden_unit_3': 759, 'hidden_unit_4': 471, 'label_smoothing': 0.0007423928500131863, 'learning_rate': 0.0003772663214527269}","1894587c":"gc.collect()\noof_pred, scores_noisy, utility_noisy = create_mlp_wrapper(train, params, 'filename', features)\ngc.collect()","d703922c":"def mutate(x, p):\n    mutate_idx = np.random.choice(x.shape[0], int(x.shape[0]*p),replace=False)\n    x[mutate_idx] = np.where(x[mutate_idx] == 1, 0, 1)\n    return x","96d9e45c":"gkf = GroupKFold(n_splits = 5)\nutilities = []\nfor fold, (tr, te) in enumerate(gkf.split(train['resp'].values, train['resp'].values, train['date'].values)):\n    oof_pred_, oof_real = oof_pred[fold], train.iloc[te]['action'].values\n    fold_utilities = []\n    ps = np.linspace(0.0, 0.7, num=100)\n    for p in ps:\n        mutated_pred = mutate((oof_pred_>0.5).astype(int), p)\n        fold_utilities.append(utility_score(train.iloc[te]['date'].values, train.iloc[te]['weight'].values, train.iloc[te]['resp'], mutated_pred))\n    utilities.append(fold_utilities)","ed1ce5d1":"def sigmoid(x, L ,x0, k, b):\n    y = -L \/ (1 + np.exp(-k*(x-x0)))+b\n    return (y)","89b507c9":"for i, fold_utilities in enumerate(utilities):\n#     https:\/\/stackoverflow.com\/questions\/55725139\/fit-sigmoid-function-s-shape-curve-to-data-using-python\n    p0 = [max(fold_utilities), np.median(ps),20,max(fold_utilities)]\n    popt, pcov = curve_fit(sigmoid, ps, fold_utilities, p0, method='dogbox')\n\n    fig, ax = plt.subplots()\n    ax.scatter(ps, fold_utilities)\n    ax.plot(ps, sigmoid(ps, *popt), 'g--')\n    ax.set_xlabel('probability of mutation')\n    ax.set_ylabel('utility')\n    ax.set_title(f'Fold {i+1} resilience to noise')\n","26d4b2f8":"((scores_noisy.mean()-scores_noisy.mean().max()).sort_values()).plot.bar()\nplt.ylabel('ROC AUC decrease from noise')\nplt.show()\n((scores_noisy.mean()-scores_noisy.mean().max()).sort_values())[:5].plot.bar()\nplt.ylabel('ROC AUC decrease from noise')\nplt.show()\n((scores_noisy.mean()-scores_noisy.mean().max()).sort_values())[-5:].plot.bar()\nplt.ylabel('ROC AUC decrease from noise')\nplt.show()","5196dadd":"((utility_noisy.mean()-utility_noisy.mean().max()).sort_values()).plot.bar()\nplt.ylabel('utility decrease from noise')\nplt.show()\n((utility_noisy.mean()-utility_noisy.mean().max()).sort_values())[:5].plot.bar()\nplt.ylabel('utility decrease from noise')\nplt.show()\n((utility_noisy.mean()-utility_noisy.mean().max()).sort_values())[-5:].plot.bar()\nplt.ylabel('utility decrease from noise')\nplt.show()","97467fd1":"print((utility_noisy.mean()-utility_noisy.mean().max()).sort_values().corr((scores_noisy.mean()-scores_noisy.mean().max()).sort_values()))","7e8c9785":"plt.scatter((utility_noisy.mean()-utility_noisy.mean().max()), scores_noisy.mean()-scores_noisy.mean().max())\nplt.xlabel('utility decrease from noise')\nplt.ylabel('ROC AUC decrease from noise')","02186da2":"## Methods\n\nWe randomly permute all the values in a feature column, as input to a trained prediction model. We then graph the relative decrease in utility, and ROC AUC, among the features. We also scatter plotted the decreases in utility and ROC AUC\n\n## Results\n\nWe see the random permutations of features 39, 0, 44, 3, and 41 as the most detrimental to our ROC AUC score, and features 46, 50, 80, 86, and 30 as the least detrimental.\nWe see the random permutations of features 3, 83, 39, 44, and 8 as the most detrimental to our utility score, and features 72, 21, 70, 17, and 35 as the least detrimental.\n\n## Discussion\n\nThe neural net model appears to rely heavily on the inputs of 3, 39, and 44, as permuting these features resulted in large decreases in both ROC AUC and utility.\nROC AUC and utility are loosely tied.","cf363f92":"## Methods\n\nWe randomly mutate the predicted best actions, flipping some percentage of actions if 1 to 0, if 0 to 1. We then plot the output. A s-curve is fit for visual clarity. \n\n## Results\n\nWe see most utility fully decay upwards 40% chance of mutation. Before then, utility steadily declines linearly with an increase in probablity of mutation.\n\n## Discussion\n\nSmall permutations in the output of our actions, <1%, have a noticable affect on our utility score. Radically large permutations can completely decay our utility. \n\n### Future work\n\n- investigate weight's relationship on utility decay from noise\n- investigate feature dependence for neural net models. ","3471c3ec":"# Resilience to noise\n\nSpawned from discussion here: https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/203312\nUsing @yirun zhang's NN model as a benchmark"}}