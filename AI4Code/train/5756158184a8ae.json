{"cell_type":{"c24ee0a7":"code","3243e2eb":"code","d8c41012":"code","95623c9b":"code","028ef8f6":"code","29773e11":"code","9fca4541":"code","d5510574":"code","a2e1d1fc":"code","eff3472f":"code","9f2cfd73":"code","4922364c":"code","8917ad1a":"code","92d54436":"code","98341403":"code","4db68df1":"code","0c7a5ecd":"code","bc0246d7":"code","67daf6b2":"code","a547ea7d":"code","aa22073b":"code","1189d6b7":"code","293633e2":"code","a1b24dc0":"code","627ae94b":"code","b30beaf4":"code","68871759":"code","c8027aaf":"code","4cac3149":"code","f87e58fa":"code","a32b49a6":"code","f7f41297":"code","d7e5451d":"code","bffeb3e5":"code","c4f5df54":"code","bb704e78":"code","b26c5c01":"code","1650c9a5":"code","789e9c60":"code","9341cc3f":"code","7f8bdbf0":"code","251c9e0a":"code","1fcd3f53":"code","d25a4deb":"code","c2b2775f":"code","c262d171":"code","ec036959":"code","ff3d0ac3":"code","455bfc76":"code","9f8218a0":"code","1f829b68":"code","e5e71910":"code","9c0fd971":"code","b40b69c9":"code","0c29d91f":"code","01fb51fd":"code","6c47b419":"code","2dbfba1d":"code","bde8f758":"code","9218a3f8":"code","e6027a9a":"code","3447d3c2":"code","e80e4995":"code","2e9ee751":"code","b0c90ca5":"code","1a2d22ce":"code","33d1f956":"code","ae848562":"code","350119ad":"code","09ef4bd5":"code","9db342e7":"code","8228234c":"code","40b17a07":"code","01fbbef7":"code","979a8f32":"code","6299374d":"code","28a765cd":"code","b73d861e":"code","f0c714ed":"code","fae546e9":"code","4c0dae6f":"code","798df266":"code","64fdb003":"code","7155c2de":"code","23595168":"code","d309b6f3":"code","4dd842e5":"code","96a2d223":"code","e060e07d":"code","2be878ae":"code","51f85f79":"code","795705e3":"code","6c28a524":"code","804e154c":"code","f96e824b":"code","29a3e99c":"code","03747a96":"code","a01e8107":"code","8f470ae7":"code","ac3ee48b":"code","6ca0d154":"code","057df619":"code","73a55b66":"code","56552d41":"code","3e583f86":"code","26f7aaec":"code","8482d344":"code","04e1d963":"code","c6d7ce33":"code","5b746294":"code","6a761506":"code","e1593258":"code","af6b70a7":"code","7e8e5c3d":"code","b918d812":"code","19b8d3a6":"code","e31fe0ba":"code","9b3133b5":"code","973589ef":"code","d714d9e3":"code","077327e3":"code","b8854e45":"code","d2af1008":"code","b0b689b8":"code","5d8fb8dc":"code","7675583e":"code","f6ad6b1b":"code","ead0cfb4":"code","c22ce213":"code","b864b4df":"code","3bdd39e9":"code","36929961":"markdown","50db9bd7":"markdown","d7683983":"markdown","6063a40b":"markdown","6e1ee85c":"markdown","423f0c4d":"markdown","73fde942":"markdown","ad442a24":"markdown","35c4576f":"markdown","ae61bc03":"markdown","88b25ee3":"markdown","3e90db4c":"markdown","0a869774":"markdown","2e366159":"markdown","b1df2a18":"markdown","4731e599":"markdown","c00eaf97":"markdown","e8a2ef3e":"markdown","38f2bd57":"markdown","5a266141":"markdown","68f63385":"markdown","2aaafe05":"markdown","7c575461":"markdown","1fe67a75":"markdown","2e67dc24":"markdown","e55b21a9":"markdown","1aa59f67":"markdown","acbd343d":"markdown","f66deeb9":"markdown","a01d4412":"markdown","3f171ee1":"markdown","4c14015a":"markdown","05c28f12":"markdown","c67517d6":"markdown"},"source":{"c24ee0a7":"import keras\nimport tensorflow as tf\nimport keras.layers as layers\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import array_to_img\nimport numpy as np\nfrom math import floor, ceil\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow import math, random, shape\nimport os\nfrom keras.losses import MeanSquaredError, BinaryCrossentropy\nfrom keras.optimizers import Nadam, SGD, Adam, Adamax\nfrom keras.activations import sigmoid\nfrom tensorflow import convert_to_tensor as tens\nfrom keras import backend as K\nfrom cv2 import getGaborKernel as Gabor\nfrom functools import reduce\nfrom matplotlib import pyplot as plt\nfrom math import sqrt\nimport itertools\nimport re\nfrom random import shuffle, seed\nfrom tensorflow.keras.utils import Sequence\nfrom keras.constraints import NonNeg\nfrom keras.regularizers import l1,l2,l1_l2\nfrom keras.initializers import RandomNormal\nimport tensorflow_addons as tfa","3243e2eb":"N_EXC = 639\nBATCH_SIZE = 16\nEPOCHS = 250\nRESOLUTION = 8\nEXCITATORY_SYNAPSES_WANTED = 8\nINHIBITORY_SYNAPSES_WANTED = 6\nTHRESHOLD = 0.2    # 0.06379306316375732\nLR_DECAY = False","d8c41012":"directory = '\/kaggle\/input\/cat-and-dog\/'\nLABELS = ['Cat', 'Dog']\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    directory+'training_set\/training_set',\n    labels='inferred',\n    color_mode='grayscale',\n    validation_split=0.2,\n    subset=\"training\",\n    seed=1337,\n    batch_size=BATCH_SIZE,\n)\nvalid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    directory+'training_set\/training_set',\n    labels='inferred',\n    color_mode='grayscale',\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=1337,\n    batch_size=BATCH_SIZE,\n)\n\ntest_ds = keras.preprocessing.image_dataset_from_directory(directory+'test_set\/test_set', labels='inferred', color_mode='grayscale', batch_size=BATCH_SIZE)","95623c9b":"plt.figure(figsize=(10, 10))\nfor i, (images, labels) in enumerate(train_ds.take(9)):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[0].numpy()[:,:,0], cmap='gray')\n    plt.title(LABELS[int(labels[0])])\n    plt.axis(\"off\")","028ef8f6":"class Module:\n    def __init__(self, path, module_name, name):\n        self.name = name\n        self.module = self.create_module(path, module_name)\n        self.input = self.module.input\n    \n    def __call__(self, inp):\n        return self.module(inp)\n    \n    def create_module(self, path, module_name):\n        models_folder  = os.path.join(path, 'Models')\n        model_filename  = os.path.join(models_folder, module_name)\n        old_model = keras.models.load_model(model_filename)\n        inp = keras.Input(shape=old_model.layers[0].input.shape[1:])\n        x = old_model.layers[1](inp)\n        for layer in old_model.layers[2:-2-1]:\n            x = layer(x)\n        output = old_model.layers[-3](x)\n        module = keras.Model(inp, output)\n#         L5PC_model.compile(optimizer=Nadam(lr=0.0001), loss='binary_crossentropy', loss_weights=[1.])\n\n        for layer in module.layers:\n            layer.trainable = False\n\n        module.summary()\n        return module       ","29773e11":"class Modules:\n    def __init__(self, neurons):\n        self.multiple = isinstance(neurons, list)\n        self.neurons = neurons\n        self.input = neurons[0].input if self.multiple else neurons.input\n        if not self.multiple:\n            print(\"Neuron module is\", self.neurons.name)\n        else:\n            message = f\"Averaging between {len(self.neurons)} neurons:\\n\"\n            for neuron in self.neurons:\n                message += f\"\\t- {neuron.name}\\n\"\n            print(message)\n                \n    def __call__(self, inputs):\n        if self.multiple:\n            outputs = math_ops.reduce_mean(tf.stack([module(inputs) for module in self.neurons], axis=-1), axis=-1)\n        else:\n            outputs = self.neurons(inputs)\n        return outputs  ","9fca4541":"multiple_neurons = False\nbigger_network = True\ndataset_folder = '\/kaggle\/input\/single-neurons-as-deep-nets-nmda-test-data'","d5510574":"if multiple_neurons:\n    neurons = Modules([Module(dataset_folder, \"NMDA_TCN__DWT_8_224_217__model.h5\", \"Module_8_layers\"), \n                       Module(dataset_folder, \"NMDA_TCN__DWT_7_292_169__model.h5\", \"Module_7_layers\"), \n                       Module(dataset_folder, \"NMDA_TCN__DWT_9_256_241__model.h5\", \"Module_9_layers\")])\nelse:\n    if bigger_network:\n        neurons = Module(dataset_folder, \"NMDA_TCN__DWT_9_256_241__model.h5\", \"Module_9_layers\")\n    else: neurons = Module(dataset_folder, \"NMDA_TCN__DWT_7_128_153__model.h5\", \"module_7_layers\")","a2e1d1fc":"class ToBoolLayer(keras.layers.Layer):\n  def __init__(self, threshold=0.5, mult=15, use_sigmoid=True, use_special_sigmoid=None, name=\"ToBoolLayer\"):\n      super().__init__(name=name)\n      self.use_sigmoid = use_sigmoid\n      self.special_sigmoid = use_special_sigmoid\n      self.threshold = threshold\n      self.mult = mult\n\n  def build(self, shape):\n    if self.special_sigmoid is not None:\n      if not len(self.special_sigmoid) == 2:\n        raise Exception(\"Special sigmoid should be in format (pos_mult, neg_mult).\")\n      else:\n        self.sigmoid = SigmoidThreshold(*self.special_sigmoid, self.threshold)\n    elif self.use_sigmoid:\n      self.sigmoid = SigmoidThresholdEasier(threshold=self.threshold, mult=self.mult)\n    \n  def call(self, inputs, training=None):\n    if training is False:\n        inputs = inputs - self.threshold\n        inputs = math_ops.ceil(inputs)\n    elif self.use_sigmoid:\n        inputs = self.sigmoid(inputs)\n    return inputs","eff3472f":"class SpikeProcessor(keras.layers.Layer):\n    def __init__(self, nSynapses, start=None, end=None, name=\"SpikeProcessor\"):\n        super().__init__(name=name)\n        self.nSynapses = nSynapses\n        self.start = start\n        self.end = end\n\n    def build(self, shape):\n        self.dims = len(shape)\n        pass\n    \n    def call(self, inputs):\n        if self.start is not None:\n            if self.end is not None:\n                inputs = inputs[:,:,:,self.start:self.end] if self.dims==4 else inputs[:,:,self.start:self.end]\n            else:\n                inputs = inputs[:,:,:,self.start:] if self.dims==4 else inputs[:,:,self.start:]\n        elif self.end is not None:\n            inputs = inputs[:,:,:,:self.end] if self.dims==4 else inputs[:,:,:self.end]\n        preds_sum = math.reduce_sum(inputs, axis=-1, keepdims=True)\n        return_value = preds_sum\/self.nSynapses\n        return layers.Flatten()(return_value)","9f2cfd73":"class OneTimeStamp2Many(keras.layers.Layer):\n    def __init__(self, units):\n        super().__init__()\n        self.units = units\n    \n    def build(self, input_shape):\n        self.w = self.add_weight(shape=(self.units, input_shape[1]),\n                               initializer='random_normal',\n                               trainable=True)\n        self.b = self.add_weight(shape=(1,input_shape[1]),\n                               initializer='zeros',\n                               trainable=True)\n    def call(self, inputs):\n        inputs = K.expand_dims(inputs, axis=1)\n        return tf.multiply(inputs, self.w) + self.b","4922364c":"def noise_init(shape, dtype=None):\n    return tf.cast(\n        tf.concat(\n        [tf.random.categorical(tf.math.log([[0.92, 0.08]]), shape[-1]) \n         for _ in range(shape[-2])], axis=0), \n        dtype)","8917ad1a":"class AddNoise(keras.layers.Layer):\n    def __init__(self, dtype=None):\n        super().__init__()\n        self.datatype=dtype\n\n    def build(self, shape):\n        self.noise = noise_init(shape, self.datatype)\n    \n    def call(self, inputs):\n        return math.multiply(inputs, self.noise)","92d54436":"def loss_for_me(y_true, y_preds):\n    return MeanSquaredError()(1, y_preds)","98341403":"def spikes_preds_processing(preds, n_spikes=50):\n    preds_sum = math.reduce_sum(preds, axis=-1, keepdims=True)\n    return preds_sum\/(n_spikes)","4db68df1":"def SigmoidThreshold(pos_mult=1, neg_mult=1, threshold=0):\n    \"\"\"returns a sigmoid function [0,1]->[0,1] with the center at threshold given, and slope multified\"\"\"\n    def sigmoid_threshold(x):\n        new_x = -x+threshold\n        multiplier = (pos_mult + neg_mult + (-pos_mult + neg_mult)*K.sign(new_x)) \/ 2 \n        return 1\/(1+math.exp(multiplier*(new_x)))\n    return sigmoid_threshold","0c7a5ecd":"x = np.arange(0,1,0.01)\nthreshold = 1\/16\npos_mult = 10\nneg_mult = 50\nplt.plot(x, SigmoidThreshold(pos_mult, neg_mult, threshold)(x))\nplt.ylim(0,1)\nplt.axhline(0.5, color='r', linestyle='--')\nplt.axvline(threshold, color='g', linestyle='--')\nplt.show()","bc0246d7":"def SigmoidThresholdEasier(mult=1, threshold=0.5):\n  \"\"\"returns a sigmoid function [0,1]->[0,1] with the center at threshold given, and slope multified\"\"\"\n  def sigmoid_threshold(x):\n      return 1\/(1+math.exp(mult*(threshold-x)))\n  return sigmoid_threshold","67daf6b2":"x = np.arange(0,1,0.01)\nthreshold = 0.9\nmult = 50\nplt.plot(x, SigmoidThresholdEasier(mult, threshold)(x))\nplt.ylim(0,1)\nplt.xlim(0,1)\nplt.axhline(0.5, color='r', linestyle='--')\nplt.axvline(threshold, color='g', linestyle='--')\nplt.show()","a547ea7d":"def set_num_syn_loss(syns_wanted_per_ms=50):\n    def num_syn_loss(y_true, y_preds):\n        return MeanSquaredError()(1,y_preds\/syns_wanted_per_ms)\n    return num_syn_loss","aa22073b":"class GetRandomInt():\n    def __init__(self, maximum):\n        self.maximum = maximum\n    def call(self):\n        return np.random.randint(0, self.maximum, 1)[0]","1189d6b7":"class ToNeuronInput(keras.layers.Layer):\n    \"\"\"flattens the temporal dimensions, order by new_order and pads with 0's to fill\"\"\"\n    def __init__(self, full, padding=0, new_order=None, name=\"NeuronInput\", part=200, batch_size=BATCH_SIZE):\n        super().__init__(name=name)\n        self.full = full\n        self.new_order = None\n        self.zero_padding = isinstance(padding, int) and padding == 0\n        self.padding = padding\n        self.part = part\n        self.batch_size = batch_size\n    \n    def build(self, shape):\n        self.ms = shape[1]*shape[2]\n        self.times = self.part \/\/ self.ms\n        self.padding_amount = self.full-self.ms*self.times\n    \n    def call(self, inputs, padding):\n                \n        new_inp = layers.Reshape((self.ms, inputs.shape[-1]))(inputs)\n        if self.new_order is not None:\n            new_inp = self.gather(new_inp, self.new_order, axis=-2)\n        if self.times > 1:\n            new_inp = layers.Concatenate(axis=1)([new_inp for _ in range(self.times)])\n        if self.zero_padding and False:\n            new_inp = layers.ZeroPadding1D(padding=(self.full - self.ms*self.times, 0))(new_inp)\n        else:\n            padding = tf.reshape(padding, (padding.shape[0]*padding.shape[1]*padding.shape[2], padding.shape[-1]))\n            starting_time = layers.Lambda(lambda x: self.ranInt(x))(padding)\n            padding = padding[np.newaxis, starting_time:starting_time+self.padding_amount]\n            new_inp = layers.Concatenate(axis=-2)([tf.tile(padding, [tf.shape(new_inp)[0], 1, 1]), new_inp])\n        return new_inp\n\n    def ranInt(self, x):\n        return K.random_uniform((1,), 0, 1024-self.padding_amount, dtype=tf.int32)[0]#.numpy()\n    \n    @tf.function\n    def gather(x, ind):\n        return tf.gather(x+0, ind)","293633e2":"class PredictNeuron(keras.layers.Layer):\n    \"\"\"flattens the temporal dimensions, order by new_order and pads with 0's to fill\"\"\"\n    def __init__(self, neuron_module, name=\"NeuronPrediction\"):\n        super().__init__(name=name)\n        self.neurons = neuron_module\n        self.single = not isinstance(self.neurons, list)\n        if self.single:\n            print(\"Neuron module is\", self.neurons.name)\n        else:\n            message = f\"Averaging between {len(self.neurons)} neurons:\\n\"\n            for neuron in self.neurons:\n                message += f\"\\t- {neuron.name}\\n\"\n            print(message)\n                \n    def call(self, inputs):\n        if not self.single:\n            outputs = math_ops.reduce_mean(tf.stack([module.module(inputs) for module in self.neurons], axis=-1), axis=-1)\n        else:\n            outputs = self.neurons.module(inputs)\n        return outputs","a1b24dc0":"def MeanSquaredErrorSynapsesPerMS(batch_size=32):\n  def mean_squared_error_synapses_per_ms(y_true, y_preds):\n    squared_difference = tf.square(1.-y_preds)\n    mean = tf.reduce_mean(squared_difference, axis=-1)\n    return mean\n  return mean_squared_error_synapses_per_ms","627ae94b":"def MSE_RMS_SynapsesPerMS(wanted, size=N_EXC, batch_size=32, eps=1e-3):\n    def mse_rms(X, mu):\n        return tf.math.reduce_mean(tf.math.reduce_sum(tf.square(tf.math.sqrt(tf.math.reduce_mean(tf.square(X+eps), axis=-1)) - mu), axis=-1))\n    real_wanted = np.sqrt(wanted \/ size)\n    zeros_wanted = np.sqrt(1. - wanted \/ size)\n    def mse_rms_synapses_per_ms(y_true, y_preds):\n        result = tf.math.sqrt((mse_rms(y_preds, real_wanted)**2 + mse_rms(1. - y_preds, zeros_wanted)**2) \/ 2)\n        return result\n    return mse_rms_synapses_per_ms","b30beaf4":"def NonBooleanLoss(*args):\n    def non_boolean_loss(y_true, y_preds):\n        return -tf.math.reduce_mean((y_preds-0.5)**2)\n    return non_boolean_loss","68871759":"class MeanSynapsesPerMsMetric:\n    def __init__(self, name=\"nSynapses\"):\n        self.name=name\n    def __call__(self, y_true, y_pred):\n        booleans = tf.cast(tf.math.greater(y_pred, 0.5), tf.float32)\n        summed = tf.math.reduce_sum(booleans, axis=-1)\n        mean_by_ms = tf.math.reduce_mean(summed, axis=-1)\n        meaned_batches = tf.math.reduce_mean(mean_by_ms, axis=-1)\n        return meaned_batches\n# def MeanSynapsesPerMsMetric(y_true, y_pred):\n#     booleans = tf.cast(tf.math.greater(y_pred, 0.5), tf.float32)\n#     summed = tf.math.reduce_sum(booleans, axis=-1)\n#     mean_by_ms = tf.math.reduce_mean(summed, axis=-1)\n#     meaned_batches = tf.math.reduce_mean(mean_by_ms, axis=-1)\n#     return meaned_batches","c8027aaf":"# class MeanSynapsesPerMsMetric(tf.keras.metrics.Metric):\n#     def __init__(self, name=\"SynapsesPerMs\", dtype=tf.float32):\n#         super().__init__(name=name, dtype=dtype)\n#         self.curr_result = self.add_weight(name='nSyns', initializer='zeros')\n#         self.dtype=dtype\n        \n#     def update_state(self, y_true, y_pred, sample_weight=None):\n#         booleans = tf.cast(tf.math.greater(y_pred, 0.5), self.dtype)\n#         summed = tf.math.reduce_sum(booleans, axis=-1)\n#         mean_by_ms = tf.math.reduce_mean(summed, axis=-1)\n#         meaned_batches = tf.math.reduce_mean(mean_by_ms, axis=-1)\n#         if sample_weight is not None:\n#             sample_weight = tf.cast(sample_weight, self.dtype)\n#             meaned_batches = tf.multiply(meaned_batches, sample_weight)\n#         self.curr_result.assign_add(meaned_batches)\n        \n#     def result(self):\n#         return tf.cast(self.curr_result)\n#     def reset_states(self):\n#         self.curr_result.assign(0.)\n#         self.n.assign(0)","4cac3149":"mean = 8\nsamples = 10\nsynapses = 100\nmu = mean \/ synapses\n\nf = lambda x, y: MSE_RMS_SynapsesPerMS(mean, size=synapses)(x, np.tile(y[np.newaxis], (BATCH_SIZE,1,1)))\nmetric = lambda x: MeanSynapsesPerMsMetric()(None, np.tile(x[np.newaxis], (BATCH_SIZE,1,1)))#()\nupdate = lambda x: metric.update_state(None, np.tile(x[np.newaxis], (BATCH_SIZE,1,1)))\n\nroun = lambda x: str(round(x.numpy(), 3))\n\ngood = np.array([[1.]*mean + [0]*(synapses - mean)]*samples)\nfor i in range(samples): np.random.shuffle(good[i])\n\n# update(good)\nplt.figure(figsize=(10,5))    \nplt.suptitle(f\"Loss with wanted mean {mean}, {samples} samples and {synapses} synapses\")\nplt.subplot(3,2,1)\nplt.title(f\"perfect: {roun(f(None, good))}; nSyns {metric(good)}\")\nplt.imshow(good, cmap=\"gray\", vmin=0, vmax=1)\n# metric.reset_states()\n\nplt.subplot(3,2,2)\npretty_good = np.clip(good + np.random.normal(0, .3, good.shape), 0 ,1)\n# update(pretty_good)\nplt.title(f\"good: {roun(f(None, pretty_good))}; nSyns {metric(pretty_good)}\")\nplt.imshow(pretty_good, cmap=\"gray\", vmin=0, vmax=1)\n# metric.reset_states()\n\nplt.subplot(3,2,3)\nopposite = np.array([[not i for i in row] for row in good.astype(np.int8)], dtype=np.float32) #np.clip(good + np.random.normal(0, .6, good.shape), 0, 1)\n# update(opposite)\nplt.title(f\"opposite: {roun(f(None, opposite))}; nSyns {metric(opposite)}\")\nplt.imshow(opposite, cmap=\"gray\", vmin=0, vmax=1)\n# metric.reset_states()\n\nplt.subplot(3,2,4)\nbad = np.clip(np.random.normal(.5, .25, samples*synapses).reshape((samples, synapses)), 0, 1)\n# update(bad)\nplt.title(f\"random: {roun(f(None, bad))}; nSyns {metric(bad)}\")\nplt.imshow(bad, cmap=\"gray\", vmin=0, vmax=1)\n# metric.reset_states()\n\n\nplt.subplot(3,2,5)\nzeros = np.zeros((samples, synapses))\n# update(zeros)\nplt.title(f\"zeros: {roun(f(None, zeros))}; nSyns {metric(zeros)}\")\nplt.imshow(zeros, cmap=\"gray\", vmin=0, vmax=1)\n# metric.reset_states()\n\n\nplt.subplot(3,2,6)\nones = np.ones((samples, synapses))\n# update(ones)\nplt.title(f\"ones: {roun(f(None, ones))}; nSyns {metric(ones)}\")\nplt.imshow(ones, cmap=\"gray\", vmin=0, vmax=1)  \n# metric.reset_states()\n\nplt.show()","f87e58fa":"def MaxErrorSynapsesPerMS(batch_size=32, y_true_for_real=False):\n    def max_error_synapses_per_ms(y_true, y_preds):\n        maxes = tf.reduce_max(y_preds, axis=-1)\n        squared_difference = tf.square(1-y_preds)\n        mean = tf.reduce_mean(squared_difference, axis=-1)\n        return mean\n    return max_error_synapses_per_ms","a32b49a6":"def EntropySynapseLoss(batch_size=32):\n    def entropy_synapse_loss(y_true, y_preds):\n        flattened = layers.Flatten()(y_preds)\n        return -math_ops.reduce_sum(flattened * math_ops.log(flattened), -1)\n    return entropy_synapse_loss","f7f41297":"def VarianceSynapseLoss(k=1, batch_size=32):\n    def variance_synapse_loss(y_true, y_preds):\n        flattened = layers.Flatten()(y_preds)\n        mean = layers.Flatten()(layers.RepeatVector(flattened.shape[-1])(K.expand_dims(math_ops.reduce_mean(flattened, axis=-1), axis=-1)))\n        variance = math_ops.reduce_sum((flattened - mean)**2, -1) \/ (flattened.shape[-1] - 1)\n        return math_ops.exp(-k * variance)\n    return variance_synapse_loss","d7e5451d":"max_acceptable_spikes_per_ms = 3.0\nmax_acceptable_spikes_deviation = 20.0\nactivity_reg_constant = 0.2 * 0.0028\n\n\ndef pre_synaptic_spike_regularization(activation_map):\n    # sum over all dendritic locations\n    x = K.sum(activation_map, axis=2)\n\n    # ask if it's above 'max_acceptable_spikes_per_ms'\n    x = K.relu(x - max_acceptable_spikes_per_ms)\n\n    # if above threshold, apply quadratic penelty\n    x = K.square(x \/ max_acceptable_spikes_deviation)\n\n    # average everything\n    x = activity_reg_constant * K.mean(x)\n\n    return x","bffeb3e5":"n_filters = 64\nn_orientations = 4\n\nksizes = [(i, i) for i in range(7,38, 2)]\nthetas = [0 , (45 \/ 180) * np.pi, (90 \/ 180) * np.pi, (135 \/ 180) * np.pi]\ngammas = [0.3] * 16\nsigmas = [2.8, 3.6, 4.5, 5.4, 6.3, 7.3, 8.2, 9.2, 10.2, 11.3, 12.3, 13.4, 14.6, 15.8, 17., 18.2]\nlambdas = [3.5, 4.6, 5.6, 6.8, 7.9, 9.1, 10.3, 11.5, 12.7, 14.1, 15.4, 16.8, 18.2, 19.7, 21.2, 22.8]\n\nall_filters = [[(size, sigma, theta, lambd, gamma) for theta in thetas] \n               for size, gamma, sigma, lambd in zip(ksizes, gammas, sigmas, lambdas)]\nall_filters = reduce(lambda x,y: x+y, all_filters, [])\nreoredering_inds = [ 0,0+4,   1, 1+4,  2, 2+4,  3, 3+4,\n                     8,8+4,   9, 9+4, 10,10+4, 11,11+4,\n                    16,16+4, 17,17+4, 18,18+4, 19,19+4,\n                    24,24+4, 25,25+4, 26,26+4, 27,27+4,\n                    32,32+4, 33,33+4, 34,34+4, 35,35+4,\n                    40,40+4, 41,41+4, 42,42+4, 43,43+4,\n                    48,48+4, 49,49+4, 50,50+4, 51,51+4,\n                    56,56+4, 57,57+4, 58,58+4, 59,59+4]\n\nall_filters_reordered = [all_filters[k] for k in reoredering_inds]","c4f5df54":"class GaborInitializer(tf.keras.initializers.Initializer):\n    def __init__(self, size, sigma, theta, lambd, gamma):\n        self.ksize = size\n        self.sigma = sigma\n        self.theta = theta\n        self.lambd = lambd\n        self.gamma = gamma\n\n    def __call__(self, dtype=None):\n        return tens(Gabor(self.ksize, self.sigma, self.theta, self.lambd, self.gamma))\n\n    def get_config(self):  # To support serialization\n        return {'ksize': self.ksize, 'sigma': self.sigma, 'theta': self.theta, 'lambda': self.lambd, 'gamma': self.gamma}","bb704e78":"max_filter_shape = all_filters_reordered[-1][0]\ncenter_pixel_ind = int((max_filter_shape[0] - 1 ) \/ 2)\n\n# place to store all filter activations\nfilters_matrix = np.zeros((max_filter_shape[0], max_filter_shape[1], len(all_filters)))\n\nplt.figure(figsize=(18,22))\nplt.subplots_adjust(left=0.04, right=0.96, bottom=0.04, top=0.96, hspace=0.25, wspace=0.1)\nfor i, filt in enumerate(all_filters_reordered):\n    filter_size  = filt[0][0]\n    oritentation = (filt[2] \/ np.pi ) * 180\n    curr_sigma   = filt[1]\n    curr_lambda  = filt[3]\n    \n    half_filter_size = int((filter_size -1 ) \/ 2)\n    upper_left_start = center_pixel_ind - half_filter_size\n    \n    curr_small_filter = GaborInitializer(*filt)().numpy()\n    curr_full_filter = np.zeros((max_filter_shape))\n    curr_full_filter[upper_left_start:upper_left_start + filter_size, upper_left_start:upper_left_start + filter_size] = curr_small_filter\n    \n    # store the filter and the activations for later\n    filters_matrix[:,:,i] = curr_full_filter\n    \n    plt.subplot(8,8,i+1);\n    plt.title('%dx%d, $\\Theta=%d$, \\n$\\sigma=%.1f$, $\\lambda=%.1f$' %(filter_size, filter_size, oritentation, curr_sigma, curr_lambda))\n    plt.imshow(curr_full_filter, cmap='gray')\n    plt.axis(\"off\")","b26c5c01":"def simple_cells_module(filters_matrix, strides=(1,1), input_shape=(256,256)):\n    # input is a single channel gray scale image\n    input_tensor = keras.Input(shape=(input_shape[0],input_shape[1],1))\n    \n    # initializer with predefined weights\n    def gabor_filters_init(shape, dtype=None):\n        return -filters_matrix[:,:,np.newaxis,:]\n\n    num_filters = filters_matrix.shape[2]\n    kernel_size = (filters_matrix.shape[0], filters_matrix.shape[1])\n\n    # single conv2d layer with all weights\n    conv_2d_layer = layers.Conv2D(num_filters, kernel_size, strides=strides, kernel_initializer=gabor_filters_init, padding='same')\n    conv_2d_layer.trainable=False\n    simple_cell_activations = conv_2d_layer(input_tensor)\n    simple_cell_module = keras.Model(input_tensor, simple_cell_activations)\n    \n    return simple_cell_module","1650c9a5":"max_pooling_size = list(range(8,23,2))\npooling_size_list = [[(s,s,2)] for s in max_pooling_size]\nnum_orientations = 4","789e9c60":"def pre_process_module(filter_matrix, pooling_size_list, strides=(1,1), num_orientations=4, input_shape=(256,256)):\n    \n    print('num gabor filters must be %d' %(sum([x[0][-1] for x in pooling_size_list]) * num_orientations))\n\n    # input is a single channel gray scale image\n    input_tensor = keras.Input(shape=(input_shape[0],input_shape[1],1))\n    \n    # calc simple cell activations\n    simple_cells_activations = K.expand_dims(simple_cells_module(filters_matrix, strides=(1,1), input_shape=(256,256))(input_tensor), axis=-1)\n    \n    # add support for subsampling\n    strides_to_use = (strides[0], strides[1], 2)\n\n    # apply max pooling\n    maxpool_3d_layers = []\n    for k, pool_size in enumerate(pooling_size_list):\n        start_ind = num_orientations * pool_size[0][-1] * k\n        end_ind   = num_orientations * pool_size[0][-1] * (k + 1)\n        curr_pool_input_slice  = simple_cells_activations[:,:,:,start_ind:end_ind]\n        curr_pool_output_slice = layers.MaxPooling3D(pool_size=pool_size[0], strides=strides_to_use, padding='same')(curr_pool_input_slice)\n        maxpool_3d_layers.append(curr_pool_output_slice)\n    \n    # squeeze the last dimention\n    concatenated_pooled_layers = K.squeeze(layers.Concatenate(axis=-2)(maxpool_3d_layers), axis=-1)\n    \n    # wrap as module and return\n    complex_cell_module = keras.Model(input_tensor, concatenated_pooled_layers)\n    \n    return complex_cell_module","9341cc3f":"def preprocessor(filters_matrix, pooling_sizes, n_orientations=4, conv_strides=(1,1), max_pooling_strides=(1,1), input_shape=(256,256)):\n    return pre_process_module(filters_matrix, pooling_sizes, strides=max_pooling_strides, num_orientations=n_orientations, input_shape=input_shape)\n","7f8bdbf0":"processor = preprocessor(filters_matrix, pooling_size_list, max_pooling_strides=(RESOLUTION,RESOLUTION))","251c9e0a":"def conv2d_with_gabor(filters, trainable=False):\n    layer = layers.Conv2D(len(filters), filters[0][0], kernel_initializer='zeros', padding='same')\n    initalize = layer(np.zeros((1,256,256,1)))\n    new_weights = np.stack([GaborInitializer(*filt)() for filt in filters]).transpose((1,2,0))[:,:,np.newaxis,:]\n    layer.set_weights([new_weights, layer.get_weights()[-1]])\n    layer.trainable = trainable\n    return layer","1fcd3f53":"def preprocess(inp):\n    normalized = inp \/ 255\n    pre_process_s1 = [layers.Concatenate(axis=-2)\n                  ([K.expand_dims(conv2d_with_gabor(all_filters[n_orientations*j:n_orientations*(j+1)], \n                                                    trainable=False)(inp),\n                                  axis=-2) for j in (i, i+1)]) \n                  for i in range(0,n_filters\/\/n_orientations,2)]\n    pre_process_c1 = layers.Concatenate(axis=-2)([(layers.MaxPooling3D(pool_size=(ksize, ksize, 2), strides=(1,1,2), padding=\"same\")(i)) \n                  for i, ksize in zip(pre_process_s1, max_pooling_size)])\n    return pre_process_c1","d25a4deb":"plt.figure(figsize=(10, 10))\nfor _, (image, label) in enumerate(train_ds.take(1)):\n    image = image[0].numpy()[:,:,0]\n    label = int(label[0])\n    processed = processor(image[np.newaxis,:,:,np.newaxis]).numpy()[0]\n    for i in range(32):\n        ax = plt.subplot(8, 4, i+1)\n        plt.imshow(processed[:,:,i], cmap='gray')\n        plt.title(LABELS[label])\n        plt.axis(\"off\")","c2b2775f":"import inspect\nimport importlib\n\ndef add_gradient_noise(BaseOptimizer, keras=None):\n    \"\"\"\n    Given a Keras-compatible optimizer class, returns a modified class that\n    supports adding gradient noise as introduced in this paper:\n    https:\/\/arxiv.org\/abs\/1511.06807\n    The relevant parameters from equation 1 in the paper can be set via\n    noise_eta and noise_gamma, set by default to 0.3 and 0.55 respectively.\n    By default, tries to guess whether to use default Keras or tf.keras based\n    on where the optimizer was imported from. You can also specify which Keras\n    to use by passing the imported module.\n    \"\"\"\n    if keras is None:\n        # Import it automatically. Try to guess from the optimizer's module\n        if hasattr(BaseOptimizer, '__module__') and BaseOptimizer.__module__.startswith('keras'):\n            keras = importlib.import_module('keras')\n        else:\n            keras = importlib.import_module('tensorflow.keras')\n\n    K = keras.backend\n\n    if not (\n        inspect.isclass(BaseOptimizer) and\n        issubclass(BaseOptimizer, keras.optimizers.Optimizer)\n    ):\n        raise ValueError(\n            'add_gradient_noise() expects a valid Keras optimizer'\n        )\n\n    def _get_shape(x):\n        if hasattr(x, 'dense_shape'):\n            return x.dense_shape\n\n        return K.shape(x)\n\n    class NoisyOptimizer(BaseOptimizer):\n        def __init__(self, noise_eta=0.3, noise_gamma=0.55, **kwargs):\n            super(NoisyOptimizer, self).__init__(**kwargs)\n            with K.name_scope(self.__class__.__name__):\n                self.noise_eta = K.variable(noise_eta, name='noise_eta')\n                self.noise_gamma = K.variable(noise_gamma, name='noise_gamma')\n\n        def get_gradients(self, loss, params):\n            grads = super(NoisyOptimizer, self).get_gradients(loss, params)\n\n            # Add decayed gaussian noise\n            t = K.cast(self.iterations, K.dtype(grads[0]))\n            variance = self.noise_eta \/ ((1 + t) ** self.noise_gamma)\n\n            grads = [\n                grad + K.random_normal(\n                    _get_shape(grad),\n                    mean=0.0,\n                    stddev=K.sqrt(variance),\n                    dtype=K.dtype(grads[0])\n                )\n                for grad in grads\n            ]\n\n            return grads\n\n        def get_config(self):\n            config = {'noise_eta': float(K.get_value(self.noise_eta)),\n                      'noise_gamma': float(K.get_value(self.noise_gamma))}\n            base_config = super(NoisyOptimizer, self).get_config()\n            return dict(list(base_config.items()) + list(config.items()))\n\n    NoisyOptimizer.__name__ = 'Noisy{}'.format(BaseOptimizer.__name__)\n\n    return NoisyOptimizer","c262d171":"NoisySGD = add_gradient_noise(SGD)\nNoisyNadam = add_gradient_noise(Nadam)\nNoisyAdamax = add_gradient_noise(Adamax)","ec036959":"eta = 0.03\ngamma = 0.55\n\nn = 150\n\nvar = []\nfor t in range(n):\n    var.append(eta\/(1+t)**gamma)\nplt.plot(np.arange(n), var)\nplt.title(r'Noise $\\sigma^{2} $ through epochs')\nplt.xlabel('ephocs')\nplt.ylabel(r'noise $\\sigma^{2}$')\nplt.show()","ff3d0ac3":"plt.figure(figsize=(5,20))\ndct = {\"Dog\": [], \"Cat\": []}\ntotal = 32*5\ncur_how_many = 0\nfor _, (image, label) in enumerate(train_ds.take(5)):\n    if cur_how_many > total: break\n    for img, lbl in zip(image, label):\n        if cur_how_many > total: break\n        else:\n            cur_how_many += 1\n        dct[LABELS[lbl]].append(img[np.newaxis,])\n#     boolean = False\n#     if len(dogs) < 400\/\/64:\n#         boolean = True\n#         dogs += [img[np.newaxis,] for img,lbl in zip(images,label) if LABELS[lbl]==\"Dog\"]\n#     if len(cats) < 400\/\/64:\n#         boolean = True   \n#         cats += [img[np.newaxis,] for img,lbl in zip(images,label) if LABELS[lbl]==\"Cat\"]\n#     if not boolean: break\nall_images = np.concatenate(dct[\"Dog\"] + dct[\"Cat\"], axis=0)\nplt.subplot(5,1,1)\nplt.title(\"Original Image\")\nplt.imshow(all_images[0,:,:,0], cmap=\"gray\")\n\n# Import data\nblocksize = 64\n\n# Create blocks\nshuffled_images = all_images.copy()\nfor j in range(0, all_images.shape[2], blocksize):\n    for i in range(0, all_images.shape[1], blocksize):\n        indxs = np.random.permutation(all_images.shape[0]).tolist()\n        for orig,new in zip(indxs, range(all_images.shape[0])):\n            shuffled_images[orig,i:i+blocksize, j:j+blocksize] = all_images[new, i:i+blocksize, j:j+blocksize]\nplt.subplot(5,1,2)\nplt.title(\"Hybrid Image\")\nplt.imshow(shuffled_images[0,:,:,0], cmap=\"gray\")\n\n# plt.subplot(5,1,3)\n# plt.title(\"Smoothed Hybrid\")\n# smoothed = smoothing_layer(shuffled_images)\n# plt.imshow(smoothed[0,:,:,0], cmap=\"gray\")\n\n\n# image_list = [smoothed[i] for i in range(smoothed.shape[0])]\n# PADDING_HYBRID = np.concatenate(image_list, axis=-2)[np.newaxis]\n\n# processed_images = processor(smoothed)\n\n# plt.subplot(5,1,4)\n# plt.title(\"Processed Smoothened Hybrid Image (Channel 0)\")\n# plt.imshow(processed_images[0,:,:,0], cmap=\"gray\")\n\n# image_list = [processed_images[i] for i in range(processed_images.shape[0])]\n# PADDING = np.concatenate(image_list, axis=-2)[np.newaxis]\n# print(PADDING.shape)\n\nprocessed_images = processor(shuffled_images)\n\nplt.subplot(5,1,4)\nplt.title(\"Processed Hybrid Image (Channel 0)\")\nplt.imshow(processed_images[0,:,:,0], cmap=\"gray\")\n\nimage_list = [processed_images[i] for i in range(processed_images.shape[0])]\nPADDING = np.concatenate(image_list, axis=-2)[np.newaxis]\nprint(PADDING.shape)\n\nplt.subplot(5,1,5)\nplt.title(\"Concatenated Processed Hybrid Image (Channel 0)\")\nplt.imshow(PADDING[0,:,:,0], cmap=\"gray\")\n\nplt.show()","455bfc76":"class NumpyDirectoryGeneratorSequence(Sequence):\n    def __init__(self, dct_label, dtype='.npy', randomize=True, random_seed=1331, validation_split=None, is_validation=False, batch_size=32):\n        self.directories = dct_label\n        self.dtype = dtype\n        self.randomize = randomize\n        self.seed = random_seed\n        self.validation_split = validation_split\n        self.is_validation = is_validation\n        self.batch_size = batch_size\n        self.x = self.y = None\n        self._create_files()\n        \n    def _create_files(self):\n        files = []\n        for directory,label in self.directories.items():\n            files += [(directory+ ('' if directory[-1] == '\/' else '\/') +i, label) \n                           for i in os.listdir(directory) \n                           if re.findall(self.dtype, i)]\n        if self.randomize:\n            seed(self.seed)\n            shuffle(files)\n        if self.validation_split:\n            if self.is_validation:\n                files = files[floor(len(files) - len(files)*self.validation_split):]\n            else:\n                files = files[:floor(len(files) - len(files)*self.validation_split)]\n                \n        self.x, self.y = zip(*files)\n        self.y = np.array(self.y)\n    \n    def __len__(self):\n        return ceil(self.y.shape[0] \/ self.batch_size)\n    \n    def __getitem__(self, idx):\n        batch_x_pre, batch_y = self.x[idx * self.batch_size:(idx + 1) * self.batch_size], self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_x = [np.load(file) for file in batch_x_pre]\n        return np.array(batch_x), batch_y","9f8218a0":"class NumpyDirectoryGenerator:\n    def __init__(self, dct_label, dtype='.npy', randomize=True, random_seed=1331, validation_split=None, is_validation=False):\n        self.directories = dct_label\n        self.dtype = dtype\n        self.files = []\n        self.randomize = randomize\n        self.seed = random_seed\n        self.validation_split = validation_split\n        self.is_validation = is_validation\n        self._create_files()\n        \n    def _create_files(self):\n        for directory,label in self.directories.items():\n            self.files += [(directory+ ('' if directory[-1] == '\/' else '\/') +i, label) \n                           for i in os.listdir(directory) \n                           if re.findall(self.dtype, i)]\n        if self.randomize:\n            seed(self.seed)\n            shuffle(self.files)\n        if self.validation_split:\n            if self.is_validation:\n                self.files = self.files[floor(len(self.files) - len(self.files)*self.validation_split):]\n            else:\n                self.files = self.files[:floor(len(self.files) - len(self.files)*self.validation_split)]\n    \n    def __call__(self):\n        return iter(self)\n    \n    def __iter__(self):\n        for file, label in self.files:\n            with open(file, 'rb') as data:\n                yield (np.load(data), label)","1f829b68":"def saccades_per1ms(shape, noise_mult=0, radius=0, radial_q=0, return_values_and_grades=False):\n    \"\"\"creates a new order, supposed to be simulating microsaccades but i didn't really check how they work\"\"\"\n    m, n = shape\n    size = m * n\n    center = np.array([m \/ 2, n \/ 2])-0.5 + np.mod(shape,2)*0.5\n    keys = np.arange(size)\n    if radius and radial_q:\n        keys = {(k, (i, j)): np.minimum((1-radial_q)*np.abs(np.sqrt(((np.array([i,j]) - center)**2).sum())-radius), \n                                        radial_q*np.sqrt(((np.array([i,j]) - center)**2).sum()))+np.random.rand(1)*noise_mult \n                for k, i, j in zip(keys, keys\/\/m, np.mod(keys, m))}\n    else: \n        keys = {(k, (i, j)): np.abs(np.sqrt(((np.array([i,j]) - center)**2).sum())-radius)+np.random.rand(1)*noise_mult for k, i, j in zip(keys, keys\/\/m, np.mod(keys, m))}\n\n    sort = sorted(keys.keys(), key=lambda x: keys[x])\n    sorted_lst = [i[0] for i in sort]\n    if not return_values_and_grades: return sorted_lst\n    as_array = np.zeros((m,n))\n    for k in range(len(sort)):\n        curr_i, curr_j = sort[k][1]\n        as_array[curr_i, curr_j] = k\n    return keys, as_array, sorted_lst","e5e71910":"m = n = 8\nnoise_mult = 0.2\n\nvalue, grading, SACCADES_ORDER = saccades_per1ms((m,n), noise_mult, 6, radial_q=.1, return_values_and_grades=True)\nplt.figure(figsize=(8, 8))\ngrades = np.zeros((m,n))\nfor key, grade in value.items():\n    grades[key[1][0], key[1][1]] = grade\nplt.imshow(grades, cmap='gray')\nfor i in range(m):\n    for j in range(n):\n        text = plt.text(j, i, int(grading[i, j]), ha=\"center\", va=\"center\", color=\"w\")","9c0fd971":"def saccades_blocks(shape, noise_mult=0, small_mult=0, rows=2, cols=2, radius=0, radial_q=0, return_values_and_grades=False):\n    arr = saccades_per1ms((m\/\/rows, n\/\/cols), noise_mult, radius, radial_q, True)[1]\n    small_arr = saccades_per1ms((rows, cols), small_mult, radius, radial_q, return_values_and_grades=True)[1]\n    size = m * n\n    keys = np.arange(size).reshape(shape)\n    sorted_dct = {}\n    for row in range(arr.shape[0]):\n        for col in range(arr.shape[1]):\n            value = arr[row, col]\n            k = 0\n            for subrow in range(rows):\n                for subcol in range(cols):\n                    sorted_dct[keys[rows*row+subrow, cols*col+subcol]] = rows*cols*value + small_arr[subrow, subcol]\n                    k += 1\n            keys[rows*row: rows*row +rows, cols*col: cols*col +cols] = rows*cols*value + small_arr\n    if not len(sorted_dct) == size: print(\"PROBLEM!\")\n    sorted_by_value = sorted(sorted_dct.keys(), key=lambda x: sorted_dct[x])\n    if not return_values_and_grades: return sorted_by_value\n    else:\n        return keys, sorted_by_value, arr, small_arr","b40b69c9":"m = n = 16\nnoise_mult = 0.6\nsmall_noise_mult = 1\n\nto_plot, SACCADES_ORDER, bigarr, smallarr = saccades_blocks((m,n), noise_mult, small_noise_mult, rows=2, cols=2, radius=3, radial_q=0.4, return_values_and_grades=True)\n_, (ax0, ax1, ax2) = plt.subplots(3,1, figsize=(20, 20), gridspec_kw={'height_ratios': [4,2,20]})\nax0.imshow(bigarr, cmap='gray')\nax1.imshow(smallarr, cmap='gray')\nax2.imshow(to_plot, cmap='gray')\nfor i in range(m):\n    for j in range(n):\n        text = ax2.text(j, i, int(to_plot[i, j]), ha=\"center\", va=\"center\", color=\"w\")\nplt.show()","0c29d91f":"def data_augmentation(rotate=0.1):\n    return tf.keras.Sequential([layers.experimental.preprocessing.RandomFlip(\"horizontal\"), \n                                layers.experimental.preprocessing.RandomRotation(rotate)])","01fb51fd":"plt.figure(figsize=(30, 10))\nfor _, (images, labels) in enumerate(train_ds.take(1)):\n    augmented = data_augmentation()(images)\n    for i in range(9):\n        plt.subplot(2, 9, i + 1)\n        plt.imshow(images[i,:,:,0], cmap='gray')\n        plt.title(LABELS[int(labels[i])])\n        plt.axis(\"off\")\n        \n        plt.subplot(2, 9, 9 + i + 1)\n        plt.imshow(augmented[i,:,:,0], cmap='gray')\n        plt.title(\"Augmented \" + LABELS[int(labels[i])])\n        plt.axis(\"off\")","6c47b419":"def benchmark(depth=1, width=32, augment=True, optimizer=Nadam(5e-3), l2_reg=1e-3, drop_rate=0.2):\n    inp = keras.Input(shape=(256,256,1))\n    if augment:\n        x = data_augmentation(inp)\n    x = processor(x if augment else inp)\n    # x = layers.BatchNormalization()(x)\n    x = layers.Flatten()(x)\n    for d in range(depth):\n        if l2_reg: x = layers.Dense(units=width, activation='linear', kernel_regularizer=keras.regularizers.l2(l2_reg), name='FC_layer_%d' %(d + 1))(x)\n        else: x = layers.Dense(units=width, activation='linear', name='FC_layer_%d' %(d + 1))(x)\n        x = layers.LeakyReLU(alpha=0.3, name='LReLU_%d' %(d + 1))(x)\n        x = layers.BatchNormalization(name='BN_layer_%d' %(d + 1))(x)\n        if drop_rate: x = layers.Dropout(drop_rate, name=\"DROP_layer_%d\" %(d + 1))(x)\n    output = layers.Dense(1, activation='sigmoid', name='logits')(x)\n    model = keras.Model(inp, output)\n    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=keras.metrics.BinaryAccuracy())\n    return model","2dbfba1d":"bench = benchmark(width=32, depth=6, optimizer=SGD(momentum=.9), l2_reg=1e-3, drop_rate=.1, augment=False)\nbench.summary()","bde8f758":"# bench.fit(train_ds, epochs=50, validation_data=valid_ds)","9218a3f8":"# archives = {}\n# for depth in (1,2,3):\n#   archives[depth] = {}\n#   for width in [1,2,4,8,16]:\n#     archives[depth][width] = set()\n#     for _ in range(3):\n#       bench = benchmark(depth=depth, width=width, optimizer=Nadam(1e-3), augment=True)\n#       archives[depth][width].add(bench.fit(train_ds, epochs=12, validation_data=valid_ds))","e6027a9a":"# pickle.dump(archives, open('benchmark_archives.pickle', \"wb\"))","3447d3c2":"# mean_dct = {nLayer: {nDepth: np.mean([np.mean(i[-3]) for i in nDepthValue]) for nDepth, nDepthValue in depths.items()} for nLayer, depths in archives.items()}","e80e4995":"# mean_archives_df = pandas.DataFrame.from_dict(archives[1])\n# for i in (2,3):\n#   mean_archives_df.append(pandas.DataFrame.from_dict(archives[i]))\n# mean_archives_df","2e9ee751":"# best_dct = {nLayer: {nDepth: np.max([i[-1] for i in nDepthValue]) for nDepth, nDepthValue in depths.items()} for nLayer, depths in archives.items()}","b0c90ca5":"# best_archives_df = pandas.DataFrame.from_dict(archives[1])\n# for i in (2,3):\n#   best_archives_df.append(pandas.DataFrame.from_dict(archives[i]))\n# best_archives_df","1a2d22ce":"# for depth in (1,2,3):\n#   curDepth = archives[depth]\n#   for width in [1,2,4,8,16]:\n#     archives[depth][width] = set()","33d1f956":"# for i, col in zip(range(3), ('b', 'r', 'g')):\n#   for history, boolean in zip(archives[i+1], (True, False, False)):\n#     if boolean:\n#       plt.plot(np.arange(50), history.history['val_binary_accuracy'], color=col, label=f\"nLayers={i+1}\")\n#     else:\n#       plt.plot(np.arange(50), history.history['val_binary_accuracy'], color=col)\n# plt.ylim(0,1)\n# plt.legend()\n# plt.show()","ae848562":"def pre_conv3d_conv2d_max_dense(lr, n_conv):\n    inp = keras.Input(shape=(256,256,1))\n    x = processor(inp)\n    x = layers.Conv2D(32, 3, activation='relu', strides=(3,3))(x)\n    x = layers.BatchNormalization()(x)\n    for _ in range(n_conv):\n        x = layers.Conv2D(32, 3, activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.MaxPooling2D((2,2))(x)\n    x = layers.Flatten()(x)\n#     x = layers.Conv3D(1278, (16,16,8), activation='sigmoid', strides=(16,16,8))(x)\n#     x = conv_to_neuron_input(x, 400, saccades)\n#     x = layers.Flatten()(x)\n    output = layers.Dense(1, activation='sigmoid')(x)\n    model = keras.Model(inp, output)\n    model.compile(optimizer=SGD(lr=lr), loss=MeanSquaredError(), metrics=keras.metrics.BinaryAccuracy())\n    # model_gabor = keras.Model(inp, [output, spikes_per_ml])\n    return model","350119ad":"def preprocess_linear(lr, loss_func=SGD):\n    inp = keras.Input(shape=(256,256,1))\n    x = processor(inp)\n    x = layers.Flatten()(x)\n#     x = layers.Conv3D(1278, (16,16,8), activation='sigmoid', strides=(16,16,8))(x)\n#     x = conv_to_neuron_input(x, 400, saccades)\n#     x = layers.Flatten()(x)\n    output = layers.Dense(1, activation='sigmoid')(x)\n    model = keras.Model(inp, output)\n    model.compile(optimizer=loss_func(lr=lr), loss=MeanSquaredError(), metrics=keras.metrics.BinaryAccuracy())\n    # model_gabor = keras.Model(inp, [output, spikes_per_ml])\n    return model","09ef4bd5":"FULL = 400\ndef block_initializer(*windows):\n    rate = sum([j-i for i,j in windows])\n    def block(shape, dtype=None):\n        weights = np.zeros(FULL)\n        for i,j in windows:\n            weights[i:j] = 1 \/ (rate)\n        plt.plot(weights, label=\"spikes wanted\")\n        return weights[:,np.newaxis]\n    return block","9db342e7":"FULL = 400\ndef block_initializer(*windows):\n    rate = sum([j-i for i,j in windows])\n    def block(shape, dtype=None):\n        weights = np.zeros(FULL)\n        for i,j in windows:\n            weights[i:j] = 1\/np.sqrt(2*np.pi)*np.exp(-0.5*(np.arange(i-(i+j)\/\/2,j-(i+j)\/\/2) \/ (j-i)*1.5)**2) \/ len(windows)\n        weights = weights \/ sum(weights)\n        plt.plot(weights, label=\"spikes wanted\")\n        return weights[:,np.newaxis]\n    return block","8228234c":"bloop = block_initializer((250,280), (200, 230))(1)","40b17a07":"def dense_initializer_AP(cycle_time, full=400, sin_intializer=False, calc_init=False, norm_calc=False):\n    times = full \/\/ cycle_time\n    starting_time = full % cycle_time\n    def sinus_initializer(shape, dtype=None):\n        actual = (np.sin(np.arange(0,cycle_time*(times))*2*np.pi\/cycle_time + np.pi) + 1) \/ times\n        padding = np.zeros(starting_time)\n        weights = np.concatenate([padding, actual])\n        return weights[:,np.newaxis]\n    def calcium_initializer(shape, dtype=None):\n        weights = np.zeros(full)\n        weights[starting_time:starting_time+3*cycle_time] = (np.sin(np.arange(0,3*cycle_time)*2\/3*np.pi\/cycle_time-np.pi\/2)+1) \/ np.pi#cycle_time * 10\n        return weights[:, np.newaxis]\n    def normal_calcium(shape, dtype=None):\n        sigma = cycle_time\n        weights =  1\/np.sqrt(2*np.pi)*np.exp(-((np.arange(full) - (starting_time + cycle_time)) \/ sigma)**2 * 0.5)\n        return weights[:, np.newaxis]\n    return sinus_initializer if sin_intializer else calcium_initializer if calc_init else normal_calcium","01fbbef7":"plt.plot(np.arange(400), dense_initializer_AP(32, calc_init=True)(1), color='b', label=\"calcium spike initializer\")\nplt.plot(np.arange(400), dense_initializer_AP(32, sin_intializer=True)(1),  color='r', label=\"pulses initializer\")\nplt.plot(np.arange(400), dense_initializer_AP(32)(1), color='g', label=\"normal calcium initializater\")\nplt.plot(np.arange(400), block_initializer((250,280), (200, 230))(1), color='k', label=\"block\")\nplt.ylim(0,1)\nplt.legend()\nplt.show()","979a8f32":"def identity_init_1d_conv_layer(shape, dtype=None):\n    if shape[0] != 1:\n        raise ValueError('Can only be used with keranel size of 1')\n    if shape[1] != shape[2]:\n        raise ValueError('Can only be used with same number of filters for input and output')\n\n    return np.identity(shape[1])[np.newaxis]","6299374d":"def playable(saccades=None, xaxis=False, use_sigmoid=True, sigmoid_threshold=0.9, sigmoid_mult=15, to_bool=True, \n             useSynapse=False, qSynapse=0.2, augment=False, nSynapse=50, optimizer=SGD, conv_shape=(16,16)):\n    inp = keras.Input(shape=(256,256,1))\n    if augment:\n        x = data_augmentation(inp)\n    x = processor(x if augment else inp)\n    conv_shape = (1,x.shape[2]) if xaxis else conv_shape\n    x = layers.Conv2D(1278, conv_shape, strides=conv_shape, activity_regularizer=pre_synaptic_spike_regularization)(x)\n    x = layers.BatchNormalization()(x)\n    x = sigmoid(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, use_sigmoid=use_sigmoid, mult=sigmoid_mult, name='Neuron_Input_Bool')(x)  # only in validation and test\n    if useSynapse:\n        nSynapsesPerMS = SpikeProcessor(nSynapse, name='nSynapses')(x)\n#     x= layers.UpSampling()\n    x = ToNeuronInput(400, saccades, name=\"NeuronInput\")(x)\n    x = L5PC_model(x)   # run through david's model\n    \n    x = layers.Flatten()(x)\n    if to_bool:\n        x = ToBoolLayer(threshold=0.1, use_sigmoid=False, name='postNeuronBool')(x)  # only in validation and test\n    output = layers.Dense(1, activation='sigmoid', name='postNeuron')(x)\n    if useSynapse:\n        model = keras.Model(inp, [output, nSynapsesPerMS])\n        model.compile(optimizer=optimizer, loss={'postNeuron': MeanSquaredError(), 'nSynapses': loss_for_me}, \n                      metrics={'postNeuron': keras.metrics.BinaryAccuracy()}, loss_weights=[1-qSynapse, qSynapse])\n    else:\n        model = keras.Model(inp, output)#, output_after_sigmoid, output_train])\n        model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=keras.metrics.BinaryAccuracy())\n    return model","28a765cd":"l1_reg = 1e-7\nl2_reg = 1e-6","b73d861e":"def playable_initialized_dense(saccades=None, xaxis=False, use_sigmoid=True, \n                       sigmoid_threshold=0.9, sigmoid_mult=15, to_bool=True, \n                       useSynapse=False, qSynapse=0.2, augment=False, \n                       nSynapse=50, optimizer=SGD(5e-3), conv_shape=(16,16),\n                       dense_init=block_initializer):\n    inp = keras.Input(shape=(256,256,1))\n    if augment:\n        x = data_augmentation(inp)\n    x = processor(x if augment else inp)\n    # x = layers.BatchNormalization()(x)\n    conv_shape = (1,x.shape[2]) if xaxis else conv_shape\n    x = layers.Conv2D(1278, conv_shape, strides=conv_shape,\n                      activity_regularizer=pre_synaptic_spike_regularization, name=\"WiringLayer\")(x)\n    x = layers.BatchNormalization()(x)\n    x = sigmoid(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, \n                    use_sigmoid=use_sigmoid, \n                    mult=sigmoid_mult, \n                    name='preNeuronBool')(x)\n    if useSynapse:\n        nSynapsesPerMS = SpikeProcessor(nSynapse, name='nSynapses')(x)\n    one_cycle = 200\/\/x.shape[1]\n    full_time = one_cycle*x.shape[1]\n\n    x = ToNeuronInput(400, new_order=saccades, name=\"NeuronInput\", padding=NOISE)(x)\n    x = L5PC_model(x)   # run through david's model\n    \n    x = layers.Flatten()(x)\n    if to_bool:\n        x = ToBoolLayer(threshold=0.2, use_sigmoid=False, mult=25, name='postNeuronBool')(x)  # only in validation and test\n    \n    output = layers.Dense(1, \n                          kernel_initializer=dense_init(start=400-full_time, end=400), \n                          bias_initializer=lambda shape, dtype: np.array([0.]), \n                          trainable=False,\n                          name=\"nSpikes\")(x)\n\n    if useSynapse:\n        model = keras.Model(inp, [output, nSynapsesPerMS])\n        model.compile(optimizer=optimizer, \n                      loss={'nSpikes': MeanSquaredError(), 'nSynapses': MeanSquaredErrorSynapsesPerMS()}, \n                      metrics={'nSpikes': keras.metrics.BinaryAccuracy()}, \n                      loss_weights=[1-qSynapse, qSynapse])\n    else:\n        model = keras.Model(inp, output)#, output_after_sigmoid, output_train])\n        model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=keras.metrics.BinaryAccuracy())\n    return model","f0c714ed":"def playable_dense(saccades=None, xaxis=False, use_sigmoid=True, \n                       sigmoid_threshold=0.9, sigmoid_mult=15, to_bool=True, \n                       useSynapse=False, qSynapse=0.2, augment=False, \n                       nSynapse=50, optimizer=SGD(5e-3), conv_shape=(16,16),\n                       dense_init=block_initializer):\n    inp = keras.Input(shape=(256,256,1))\n    if augment:\n        x = data_augmentation(inp)\n    x = processor(x if augment else inp)\n    # x = layers.BatchNormalization()(x)\n    conv_shape = (1,x.shape[2]) if xaxis else conv_shape\n    x = layers.Conv2D(1278, conv_shape, strides=conv_shape,\n                      activity_regularizer=pre_synaptic_spike_regularization, name=\"WiringLayer\")(x)\n    x = layers.BatchNormalization()(x)\n    x = sigmoid(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, \n                    use_sigmoid=use_sigmoid, \n                    mult=sigmoid_mult, \n                    name='preNeuronBool')(x)\n    if useSynapse:\n        nSynapsesPerMS = SpikeProcessor(nSynapse, name='nSynapses')(x)\n    one_cycle = 400\/\/x.shape[1]\n    full_time = once_cycle*x.shape[1]\n\n    x = ToNeuronInput(400, new_order=saccades, name=\"NeuronInput\", padding=NOISE)(x)\n    x = L5PC_model(x)   # run through david's model\n    \n    x = layers.Flatten()(x)\n    if to_bool:\n        x = ToBoolLayer(threshold=0.2, use_sigmoid=False, mult=25, name='postNeuronBool')(x)  # only in validation and test\n    \n    output = layers.Dense(1, name=\"nSpikes\")(x[:, -full_time:])\n\n    if useSynapse:\n        model = keras.Model(inp, [output, nSynapsesPerMS])\n        model.compile(optimizer=optimizer, \n                      loss={'nSpikes': MeanSquaredError(), 'nSynapses': MeanSquaredErrorSynapsesPerMS()}, \n                      metrics={'nSpikes': keras.metrics.BinaryAccuracy()}, \n                      loss_weights=[1-qSynapse, qSynapse])\n    else:\n        model = keras.Model(inp, output)#, output_after_sigmoid, output_train])\n        model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=keras.metrics.BinaryAccuracy())\n    return model","fae546e9":"def around_spikes_loss(windows, r, full=400):\n    weights = np.zeros(full)\n    for start,end in windows:\n        weights[start-r:start] = 1\n        weights[end:end+r] = 1\n    for start,end in windows:\n        weights[start:end] = 0\n    weights = tens(weights[:,np.newaxis], dtype=tf.float32)\n    def loss(y_true, y_preds):\n        return mean_squared_error_synapses_per_ms(tf.matmul(y_preds, weights))\n    \n    def mean_squared_error_synapses_per_ms(y_preds):\n        squared_difference = tf.square(y_preds)\n        mean = tf.reduce_mean(squared_difference, axis=-1)\n        return mean\n    plt.plot(weights[:,0], label=\"silence wanted\")\n    return loss","4c0dae6f":"def different_nSynapses_initialized_dense(saccades=None, xaxis=False, use_sigmoid=True, \n                        sigmoid_threshold=0.9, sigmoid_mult=15, to_bool=True, dropout=.2,\n                        excitatory_wanted=EXCITATORY_SYNAPSES_WANTED, inhibitory_wanted=INHIBITORY_SYNAPSES_WANTED,\n                        qSynapse=(0.1, 0.1), augment=False, \n                        optimizer=SGD(5e-3), conv_shape=(16,16),\n                        dense_init=block_initializer, spike_wanted = [(260,280)], loss_radius=20,\n                                          padding=PADDING):\n    inp = keras.Input(shape=(256,256,1))\n    if augment:\n        x = data_augmentation(inp)\n    x = processor(x if augment else inp)\n    # x = layers.BatchNormalization()(x)\n    conv_shape = (1,x.shape[2]) if xaxis else conv_shape\n    wiringLayer = layers.Conv2D(1278, conv_shape, strides=conv_shape,#kernel_constraint=keras.constraints.non_neg(),\n                      activity_regularizer=pre_synaptic_spike_regularization, name=\"WiringLayer\")\n    x = wiringLayer(x)\n    x = layers.BatchNormalization()(x)\n    x = sigmoid(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, \n                    use_sigmoid=use_sigmoid, \n                    mult=sigmoid_mult, \n                    name='preNeuronBool')(x)\n    nExcitatorySynapsesPerMS = SpikeProcessor(excitatory_wanted, name='nExcitatory', end=639)(x)\n    nInhibitorySynapsesPerMS = SpikeProcessor(inhibitory_wanted, name='nInhibitory', start=639)(x)\n    cycle_time = x.shape[1]\n    cycles = 200\/\/x.shape[1]\n    full_time = cycles*cycle_time\n    convertedPadding = wiringLayer(padding)\n    convertedPadding = layers.BatchNormalization()(convertedPadding)\n    convertedPadding = sigmoid(convertedPadding)\n    convertedPadding = ToBoolLayer(threshold=sigmoid_threshold, \n                    use_sigmoid=use_sigmoid, \n                    mult=sigmoid_mult, \n                    name='preNeuronBool')(convertedPadding)\n\n    x = ToNeuronInput(400, new_order=saccades, name=\"NeuronInput\", padding=convertedPadding)(x)\n    if dropout: layers.Dropout(dropout)(x)\n    x = L5PC_model(x)   # run through david's model\n    \n    x = layers.Flatten()(x)\n    if to_bool:\n        spike_train = ToBoolLayer(threshold=0.2, use_sigmoid=False, mult=25, name='SpikeTrain')(x)  # only in validation and test\n    \n    output = layers.Dense(1, \n                          kernel_initializer=dense_init(*spike_wanted), \n                          bias_initializer=lambda shape, dtype: np.array([0.]), \n                          trainable=False,\n                          name=\"nSpikes\")(x)\n    \n    model = keras.Model(inp, [output, spike_train, nExcitatorySynapsesPerMS, nInhibitorySynapsesPerMS])\n    model.compile(optimizer=optimizer, \n                  loss={'nSpikes': MeanSquaredError(), 'SpikeTrain': around_spikes_loss(spike_wanted, loss_radius), 'nExcitatory': MeanSquaredErrorSynapsesPerMS(), 'nInhibitory': MeanSquaredErrorSynapsesPerMS()}, \n                  metrics={'nSpikes': keras.metrics.BinaryAccuracy()}, \n                  loss_weights=[0.95-sum(qSynapse),0.05, *qSynapse])\n    return model","798df266":"def different_nSynapses(saccades=None, xaxis=False, use_sigmoid=True, \n                        sigmoid_threshold=0.9, sigmoid_mult=15, to_bool=True, \n                        excitatory_wanted=EXCITATORY_SYNAPSES_WANTED, inhibitory_wanted=INHIBITORY_SYNAPSES_WANTED,\n                        qSynapse=(0.1, 0.1), augment=False, \n                        optimizer=SGD(5e-3), conv_shape=(16,16),\n                        dense_init=block_initializer, padding=PADDING):\n    inp = keras.Input(shape=(256,256,1))\n    if augment:\n        x = data_augmentation(inp)\n    x = processor(x if augment else inp)\n    # x = layers.BatchNormalization()(x)\n    conv_shape = (1,x.shape[2]) if xaxis else conv_shape\n    wiringLayer = layers.Conv2D(1278, conv_shape, strides=conv_shape,\n                      activity_regularizer=pre_synaptic_spike_regularization, name=\"WiringLayer\")\n    x = wiringLayer(x)\n    x = layers.BatchNormalization()(x)\n    x = sigmoid(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, \n                    use_sigmoid=use_sigmoid, \n                    mult=sigmoid_mult, \n                    name='preNeuronBool')(x)\n    nExcitatorySynapsesPerMS = SpikeProcessor(excitatory_wanted, name='nExcitatory', end=639)(x)\n    nInhibitorySynapsesPerMS = SpikeProcessor(inhibitory_wanted, name='nInhibitory', start=639)(x)\n    one_cycle = 200\/\/x.shape[1]\n    full_time = one_cycle*x.shape[1]\n    convertedPadding = wiringLayer(padding)\n    convertedPadding = layers.BatchNormalization()(convertedPadding)\n    convertedPadding = sigmoid(convertedPadding)\n    convertedPadding = ToBoolLayer(threshold=sigmoid_threshold, \n                    use_sigmoid=use_sigmoid, \n                    mult=sigmoid_mult, \n                    name='preNeuronBool')(convertedPadding)\n\n    x = ToNeuronInput(400, new_order=saccades, name=\"NeuronInput\", padding=convertedPadding)(x)\n    x = L5PC_model(x)   # run through david's model\n    \n    x = layers.Flatten()(x)\n    if to_bool:\n        x = ToBoolLayer(threshold=0.2, use_sigmoid=False, mult=25, name='postNeuronBool')(x)  # only in validation and test\n    \n    output = layers.Dense(1, name=\"nSpikes\")(x[:, -full_time:])\n\n    model = keras.Model(inp, [output, nExcitatorySynapsesPerMS, nInhibitorySynapsesPerMS])\n    model.compile(optimizer=optimizer, \n                  loss={'nSpikes': MeanSquaredError(), 'nExcitatory': MeanSquaredErrorSynapsesPerMS(), 'nInhibitory': MeanSquaredErrorSynapsesPerMS()}, \n                  metrics={'nSpikes': keras.metrics.BinaryAccuracy()}, \n                  loss_weights=[1-sum(qSynapse), *qSynapse])\n    return model","64fdb003":"def different_nSynapses_max_loss(window, saccades=None, xaxis=False, use_sigmoid=True, \n                        sigmoid_threshold=0.9, sigmoid_mult=15, to_bool=True, \n                        excitatory_wanted=EXCITATORY_SYNAPSES_WANTED, inhibitory_wanted=INHIBITORY_SYNAPSES_WANTED,\n                        qSynapse=(0.1, 0.1), augment=False, \n                        optimizer=SGD(5e-3), conv_shape=(16,16),\n                        dense_init=block_initializer, padding=PADDING):\n    inp = keras.Input(shape=(256,256,1))\n    if augment:\n        x = data_augmentation(inp)\n    x = processor(x if augment else inp)\n    # x = layers.BatchNormalization()(x)\n    conv_shape = (1,x.shape[2]) if xaxis else conv_shape\n    wiringLayer = layers.Conv2D(1278, conv_shape, strides=conv_shape,\n                      activity_regularizer=pre_synaptic_spike_regularization, name=\"WiringLayer\")\n    x = wiringLayer(x)\n    x = layers.BatchNormalization()(x)\n    x = sigmoid(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, \n                    use_sigmoid=use_sigmoid, \n                    mult=sigmoid_mult, \n                    name='preNeuronBool')(x)\n    nExcitatorySynapsesPerMS = SpikeProcessor(excitatory_wanted, name='nExcitatory', end=639)(x)\n    nInhibitorySynapsesPerMS = SpikeProcessor(inhibitory_wanted, name='nInhibitory', start=639)(x)\n    one_cycle = 200\/\/x.shape[1]\n    full_time = one_cycle*x.shape[1]\n    convertedPadding = wiringLayer(padding)\n    convertedPadding = layers.BatchNormalization()(convertedPadding)\n    convertedPadding = sigmoid(convertedPadding)\n    convertedPadding = ToBoolLayer(threshold=sigmoid_threshold, \n                    use_sigmoid=use_sigmoid, \n                    mult=sigmoid_mult, \n                    name='preNeuronBool')(convertedPadding)\n\n    x = ToNeuronInput(400, new_order=saccades, name=\"NeuronInput\", padding=convertedPadding)(x)\n    x = L5PC_model(x)   # run through david's model\n    \n    output = layers.Flatten(name=\"SpikeTrain1\")(x)\n    if to_bool:\n        output = ToBoolLayer(threshold=0.2, use_sigmoid=True, mult=25, name='SpikeTrain')(output)  # only in validation and test\n    \n    model = keras.Model(inp, [output, nExcitatorySynapsesPerMS, nInhibitorySynapsesPerMS])\n    model.compile(optimizer=optimizer, \n                  loss={'SpikeTrain': max_loss(window), 'nExcitatory': MeanSquaredErrorSynapsesPerMS(), 'nInhibitory': MeanSquaredErrorSynapsesPerMS()}, \n                  metrics={'SpikeTrain': keras.metrics.BinaryAccuracy()}, \n                  loss_weights=[1-sum(qSynapse), *qSynapse])\n    return model","7155c2de":"def max_loss(windows, full=400):\n    \"\"\"not in use\"\"\"\n    weights = np.zeros(full)\n    for start,end in windows:\n        weights[start:end] = 1\n    weights = tens(weights, dtype=tf.float32)\n    def loss(y_true, y_preds):\n        loss_value =  (tf.cast(y_true, y_preds.dtype) - K.max(y_preds*weights, axis=-1))**2\n        return loss_value\n    return loss","23595168":"class SynapseLossDecay(keras.callbacks.Callback):\n    def __init__(self, alpha, beta, decay_alpha=.9, decay_beta=.9, low=.03):\n        super().__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.decay_alpha = decay_alpha\n        self.decay_beta = decay_beta\n        self.low = low\n\n#     @tf.autograph.experimental.do_not_convert\n    def on_epoch_end(self, batch, logs=None):\n        if self.alpha > 0:\n            if self.alpha < self.low: self.alpha = self.alpha * 0\n            else: self.alpha = self.alpha * self.decay_alpha\n        if self.beta > 0:\n            if self.beta < self.low: self.beta = self.beta * 0\n            else: self.beta = self.beta * self.decay_beta","d309b6f3":"class SynapsePruner(keras.callbacks.Callback):\n    def __init__(self, kmax1=5, kmax2=15, iterations=8, axis=-1, splitPoint = N_EXC, prune_layer=\"WiringLayer\"):\n        super().__init__()\n        self.kmax1 = kmax1\n        self.kmax2 = kmax2\n        self.iterations = iterations\n        self.curIterations = iterations\n        self.split = splitPoint\n        self.layer_to_prune = prune_layer\n        print(self)\n    \n    def __str__(self):\n        string = f\"\\nSynapsePruner callback:\\nkmax1 = {self.kmax1}\\n\"\n        string += f\"kmax2 = {self.kmax2}\\n\"\n        string += f\"every {self.iterations} iterations\"\n        return string\n        \n    def build(self, shape):\n        pass\n        \n#     @tf.autograph.experimental.do_not_convert\n    def on_train_batch_end(self, batch, logs=None):\n        if self.iterations:\n            if self.curIterations: self.curIterations -= 1\n            else:\n                self.prune()\n                self.curIterations = self.iterations\n\n    def prune(self):\n        layer = self.model.get_layer(self.layer_to_prune)\n        weights = layer.get_weights()\n        is_bias = len(weights) == 2\n        if is_bias: kernels, bias = layer.get_weights()\n        else: kernels = layer.get_weights()[0]\n        kernels = kernels[:, 0]\n        p1_x = kernels[:, :, :self.split]\n        p2_x = kernels[:, :, self.split:]\n        kmax1 = np.partition(p1_x, -self.kmax1, axis=-1)[:,:,-self.kmax1][:,:,np.newaxis]\n        kmax2 = np.partition(p2_x, -self.kmax2, axis=-1)[:,:,-self.kmax2][:,:,np.newaxis]\n\n\n        arr1 = (p1_x * (p1_x >= kmax1))\n        arr2 = (p2_x * (p2_x >= kmax2))\n        arr = np.concatenate([arr1, arr2], axis=-1)\n        arr = np.concatenate([arr1, arr2], axis=-1)\n        assert arr.shape == kernels.shape, tf.print(\"Oh no. something went wrong\")\n\n        layer.set_weights([arr[:, np.newaxis], bias] if is_bias else [arr[:, np.newaxis]])","4dd842e5":"nFilters = 32\nnSyn = 10\nnPixels = 32  # coloumn\na_kmax1 = 1   # how many excitatory synapses per filter per pixel\na_kmax2 = 1   # how many inhibitory synapses per filter per pixel\n\n\nkernels = np.random.rand(nPixels*nFilters*nSyn).reshape((nPixels,nFilters,nSyn))\nsplat = nSyn \/\/2\n\n# Split\np1_x = kernels[:, :, :splat]\np2_x = kernels[:, :, splat:]\n\n# find k-highest\nkmax1 = np.partition(p1_x, -a_kmax1,axis=-1)[:,:,-a_kmax1][:,:,np.newaxis]\nkmax2 = np.partition(p2_x, -a_kmax2,axis=-1)[:,:,-a_kmax2][:,:,np.newaxis]\n\n# Prune all lower\narr1 = (p1_x * (p1_x >= kmax1))\narr2 = (p2_x * (p2_x >= kmax2))\narr = np.concatenate([arr1, arr2], axis=-1)\n\n# Plot\nplt.figure(figsize=(50,10))\nfor i in range(nFilters):\n    plt.subplot(2,nFilters,i+1)\n    plt.title(f\"Filter {i}\")\n    plt.imshow(kernels[:,i,:], cmap=\"gray\")\n    plt.xlabel(\"Synapse\")\n    plt.ylabel(\"Pixel\")\n    plt.axvline(splat, color=\"r\", linestyle=\"--\")\n    \n    plt.subplot(2,nFilters,nFilters +i+1)\n    plt.title(f\"Pruned {i}\")\n    plt.imshow(arr[:,i,:], cmap=\"gray\")\n    plt.xlabel(\"Synapse\")\n    plt.ylabel(\"Pixel\")\n    plt.axvline(splat, color=\"r\", linestyle=\"--\")\n\n\nplt.show()","96a2d223":"def printModule(dropout1, dropout2, sigmoid_threshold, sigmoid_mult, exWanted, inWanted, qSynapse, augment, synLoss):\n    print(\"~*~ Visual Module ~*~\")\n    if augment: print(f\"Images are augmented (rotated by {augment})\")\n    print(f\"Image Dropout Rate: {dropout1}\")\n    print(f\"Synapse Threshold: {sigmoid_threshold}\")\n    print(f\"Synapse Training Sigmoid Multiplication: {sigmoid_mult}\")\n    print(f\"Synapses Wanted: Exc-{exWanted}; Inh-{inWanted}; Sum-{exWanted+inWanted}\")\n    print(f\"Synapses Loss Rate: Exc-{qSynapse[0]}; Inh-{qSynapse[1]}\")\n    print(f\"Synapses Dropout Rate: {dropout2}\")\n    print(f\"Synapse Loss Function: {synLoss.__name__}\")","e060e07d":"def different_nSynapses(saccades=None, xaxis=True, use_sigmoid=True, pruning=True, dropout1=.2, dropout2=False,\n                        sigmoid_threshold=0.9, sigmoid_mult=15, to_bool=True, \n                        excitatory_wanted=EXCITATORY_SYNAPSES_WANTED, inhibitory_wanted=INHIBITORY_SYNAPSES_WANTED,\n                        qSynapse=(0.1, 0.1), augment=False, \n                        optimizer=SGD(5e-3), conv_shape=(16,16),\n                        padding=PADDING, synLoss=MeanSquaredErrorSynapsesPerMS):\n    printModule(dropout1, dropout2, sigmoid_threshold, sigmoid_mult, excitatory_wanted, inhibitory_wanted, qSynapse, augment, synLoss)\n    padding = tf.Variable(lambda: padding, trainable=False)\n    inp = keras.Input(shape=(256,256,1))\n    if augment:\n        x = data_augmentation(inp)\n    x = processor(x if augment else inp)\n    # x = layers.BatchNormalization()(x)\n    if dropout1:\n        x = layers.Dropout(dropout1)(x)\n    conv_shape = (x.shape[2], 1) if xaxis else conv_shape\n    f = tf.keras.Sequential([layers.Conv2D(1278, conv_shape, strides=conv_shape, #kernel_constraint=keras.constraints.NonNeg(), \n                                activity_regularizer=pre_synaptic_spike_regularization, name=\"WiringLayer\"),\n                             layers.BatchNormalization(name=\"BatchNorm\"),\n                             layers.Activation(sigmoid),\n                             ToBoolLayer(threshold=sigmoid_threshold, use_sigmoid=use_sigmoid, mult=sigmoid_mult, name='preNeuronBool')])\n    x = f(x)\n    pad = f(padding)\n        \n    cycles = 200\/\/x.shape[-2]\n    full_time = cycles*x.shape[-2]\n    print(\"how many cycles:\", cycles)\n\n    x = ToNeuronInput(400, new_order=saccades, name=\"NeuronInput\")(x, padding=pad if padding is not 0 else padding)\n    \n    nExcitatorySynapsesPerMS = SpikeProcessor(excitatory_wanted, name='nExcitatory', end=639)(x)\n    nInhibitorySynapsesPerMS = SpikeProcessor(inhibitory_wanted, name='nInhibitory', start=639)(x)\n\n    if dropout2:\n        x = layers.Dropout(dropout2)(x)\n    x = L5PC_model(x)[:,-full_time:,:]   # run through david's model\n    x = layers.MaxPooling1D(x.shape[-2], strides=x.shape[-2], name=\"MaxPooling\")(x)\n    output = layers.Flatten(name=\"nSpikes\")(x)\n#     if to_bool:\n#         x = ToBoolLayer(threshold=0.2, use_sigmoid=False, mult=25, name='postNeuronBool')(x)  # only in validation and test\n    \n#     output = layers.Dense(1, name=\"nSpikes\")(x[:, -full_time:])\n#     model = keras.Model(inp, output)\n#     model.compile(optimizer=optimizer, loss={'nSpikes': MeanSquaredError()}, metrics={'nSpikes': keras.metrics.BinaryAccuracy()})\n    model = keras.Model(inp, [output, nExcitatorySynapsesPerMS, nInhibitorySynapsesPerMS])\n    model.compile(optimizer=optimizer, \n                  loss={'nSpikes': MeanSquaredError(), 'nExcitatory': synLoss(), 'nInhibitory': synLoss()}, \n                  metrics={'nSpikes': keras.metrics.BinaryAccuracy()}, \n                  loss_weights=[1-sum(qSynapse), *qSynapse])\n    return model","2be878ae":"class Pad(keras.layers.Layer):\n    \"\"\"flattens the temporal dimensions, order by new_order and pads with 0's to fill\"\"\"\n    def __init__(self, padding, full=400, times=6, reverse=True, new_order=None, name=\"PadLayer\"):\n        super().__init__(name=name)\n        self.full = full\n        self.padding = padding\n        self.times = times\n        self.reverse = reverse\n        self.new_order = new_order\n        self.shape = None\n        self.pad_to_add = None\n    \n    def build(self, shape):\n        self.shape = shape\n        self.pad_to_add = self.full - self.shape[-2]*self.times\n    \n    def call(self, inputs):\n        if self.new_order is not None:\n            inputs = self.gather(inputs, self.new_order, axis=-2)\n        if self.times > 1:\n            if self.reverse: new_inp = layers.Concatenate(axis=-2)([inputs, K.reverse(inputs,axes=-2)] * (self.times \/\/ 2) + [inputs] * (self.times % 2))\n            else: new_inp = layers.Concatenate(axis=-2)([inputs] * self.times)\n        else: new_inp = inputs\n        starting_time = layers.Lambda(lambda x: self.ranInt(x))(self.padding)\n        padding = self.padding[:,:, starting_time:starting_time+self.pad_to_add]\n        new_inp = layers.Concatenate(axis=-2)([tf.tile(padding, [tf.shape(new_inp)[0], 1, 1, 1]), new_inp])\n        new_inp = K.reshape(new_inp, (tf.shape(new_inp)[0], self.shape[-3], self.shape[-2]*self.times + self.pad_to_add, self.shape[-1]))\n        return new_inp\n\n    def ranInt(self, x):\n        return K.random_uniform((1,), 0, self.padding.shape[-2]-self.pad_to_add, dtype=tf.dtypes.int32)[0]#.numpy()\n    \n    @tf.function\n    def gather(self, x, ind, axis):\n        return tf.gather(x+0, ind, axis)","51f85f79":"class SliceLayer(tf.keras.layers.Layer):\n    def __init__(self, start=None, end=None, name=\"SliceLayer\"):\n        super().__init__(name=name)\n        self.start = start\n        self.end = end\n\n    def build(self, shape):\n        self.dims = len(shape)\n        pass\n    \n    def call(self, inputs):\n        if self.start is not None:\n            if self.end is not None:\n                return inputs[:,:,:,self.start:self.end] if self.dims==4 else inputs[:,:,self.start:self.end]\n            else:\n                return inputs[:,:,:,self.start:] if self.dims==4 else inputs[:,:,self.start:]\n        elif self.end is not None:\n            return inputs[:,:,:,:self.end] if self.dims==4 else inputs[:,:,:self.end]","795705e3":"class DropoutActivateOnEval(layers.Dropout):\n    def __init__(self, rate, activate_on_eval=True, **kwargs):\n        super().__init__(rate, **kwargs)\n        self.activate_on_eval = activate_on_eval\n        \n    def call(self, inputs, training=True):\n        return super().call(inputs, training=training or self.activate_on_eval)","6c28a524":"def different_nSynapses(saccades=None, xaxis=True, use_sigmoid=True, pruning=True, dropout1=False, dropout2=False,\n                        sigmoid_threshold=0.9, sigmoid_mult=15, to_bool=True, renorm=False,\n                        excitatory_wanted=EXCITATORY_SYNAPSES_WANTED, inhibitory_wanted=INHIBITORY_SYNAPSES_WANTED,\n                        qSynapse=(tf.Variable(0.1, trainable=False), tf.Variable(0.1, trainable=False)), augment=False, \n                        optimizer=SGD(momentum=.9), conv_shape=(16,16), threshold=.2, times=6,\n                        padding=PADDING, synLoss=MeanSquaredErrorSynapsesPerMS, nSynapse=True, neurons=neurons, \n                        regular_sigmoid=True, non_neg=False, reverse=False, mult2=1, activate_dropout_on_eval=False):\n    printModule(dropout1, dropout2, sigmoid_threshold, sigmoid_mult, excitatory_wanted, inhibitory_wanted, qSynapse, augment, synLoss)\n    module_name = \"module_syn_\"\n    for i in [dropout1, dropout2, sigmoid_threshold, sigmoid_mult, threshold, excitatory_wanted, inhibitory_wanted, qSynapse, augment, synLoss.__name__]:\n        module_name += str(i)\n           \n    inp = keras.Input(shape=(256,256,1))\n    \n    padding = tf.Variable(lambda: padding, trainable=False)\n    x = inp\n    if augment: x = data_augmentation(augment)(x)\n    x = processor(x)\n    if dropout1: x = DropoutActivateOnEval(dropout1, activate_dropout_on_eval)(x)\n    conv_shape = (x.shape[-3], 1) if xaxis else conv_shape\n    real_input_time = x.shape[-2]*times\n    x = Pad(padding, full=neurons.input.shape[-2], times=times, reverse=reverse)(x)\n    if non_neg: x = layers.Conv2D(2*N_EXC, conv_shape, strides=conv_shape, use_bias=True, kernel_constraint=keras.constraints.NonNeg(), name=\"WiringLayer\")(x)#, activity_regularizer=pre_synaptic_spike_regularization)(x)\n    else: x = layers.Conv2D(2*N_EXC, conv_shape, strides=conv_shape, use_bias=True, name=\"WiringLayer\")(x)#, activity_regularizer=pre_synaptic_spike_regularization)(x)\n    x = layers.BatchNormalization(name=\"BatchNorm\", renorm=renorm)(x)\n    if regular_sigmoid: x = layers.Activation(sigmoid)(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, use_sigmoid=use_sigmoid, mult=sigmoid_mult, name='preNeuronBool')(x)\n    \n    x = K.squeeze(x, axis=-3)\n        \n    ExcitatorySynapses = SliceLayer(end=N_EXC, name=\"ExcSyns\")(x)#[:, :, :N_EXC]  #SpikeProcessor(excitatory_wanted, name='nExcitatory', end=639)(x)\n    InhibitorySynapses = SliceLayer(start=N_EXC, name=\"InhSyns\")(x)#[:, :, N_EXC:]  #SpikeProcessor(inhibitory_wanted, name='nInhibitory', start=639)(x)\n\n    if dropout2: x = DropoutActivateOnEval(dropout2, activate_dropout_on_eval)(x)\n    x = neurons(x)[:,-real_input_time:,:]   # run through david's model, take only the post-noise time (32 ms X 6 times)\n    x = layers.MaxPooling1D(x.shape[-2], strides=x.shape[-2], name=\"MaxPooling\")(x)\n    x = ToBoolLayer(threshold=threshold, use_sigmoid=True, mult=mult2, name='postNeuronBool')(x)\n    output = layers.Flatten(name=\"nSpikes\")(x)\n    \n    model = keras.Model(inp, [output, ExcitatorySynapses, InhibitorySynapses])\n    if not nSynapse:\n        model.compile(optimizer=optimizer, loss={'nSpikes': MeanSquaredError()}, \n                      metrics={\"nSpikes\": keras.metrics.BinaryAccuracy(), \"ExcSyns\": MeanSynapsesPerMsMetric(), \"InhSyns\": MeanSynapsesPerMsMetric()})\n    else:\n        model.compile(optimizer=optimizer, \n                  loss=[MeanSquaredError(), synLoss(excitatory_wanted), synLoss(inhibitory_wanted)], \n                  metrics={\"nSpikes\": keras.metrics.BinaryAccuracy(), \"ExcSyns\": MeanSynapsesPerMsMetric(), \"InhSyns\": MeanSynapsesPerMsMetric()},\n                  loss_weights=[1.0 -qSynapse[0] - qSynapse[1], qSynapse[0], qSynapse[1]])\n    return model, module_name","804e154c":"# ex_synapse_weight = tf.Variable(0.05, trainable=False)\n# inh_synapse_weight = tf.Variable(0.05, trainable=False)\n\nmodel_synapses, module_name = different_nSynapses(reverse=True, renorm=True, activate_dropout_on_eval=False, mult2=1, threshold=THRESHOLD, non_neg=False, nSynapse=False, dropout1=.4, dropout2=False, use_sigmoid=True, regular_sigmoid=True, sigmoid_mult=50, sigmoid_threshold=.9, augment=0.075, optimizer=Nadam(), qSynapse=(.2, .2), synLoss=NonBooleanLoss)","f96e824b":"lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=5e-3, decay_steps=100, decay_rate=0.3)","29a3e99c":"initial_learning_rate=5e-3\ndecay_steps=100\ndecay_rate=0.3\n\ndef decayed_learning_rate(step):\n    return initial_learning_rate * decay_rate ** (step \/ decay_steps)\nplt.plot(decayed_learning_rate(np.arange(EPOCHS)))\nplt.xlabel(\"epoch\")\nplt.ylabel(\"lr\")","03747a96":"# model_synapses, module_name = different_nSynapses(threshold=.2, non_neg=False, nSynapse=True, dropout1=0.8, dropout2=0.8, use_sigmoid=True, regular_sigmoid=True, sigmoid_mult=50, sigmoid_threshold=.9, augment=0.5, qSynapse=(.01, .01), synLoss=MSE_RMS_SynapsesPerMS)","a01e8107":"model_synapses.summary()","8f470ae7":"if LR_DECAY: history_syn = model_synapses.fit(train_ds, epochs=EPOCHS, validation_data=valid_ds, callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)])#, SynapseLossDecay(ex_synapse_weight, inh_synapse_weight)])#, tf.keras.callbacks.LearningRateScheduler(tfa.optimizers.CyclicalLearningRate(1e-3, 5.1e-2, 1e-2, lambda x: 1))]) )#, callbacks=[SynapsePruner(50, 50, iterations=1)])#, \nelse: history_syn = model_synapses.fit(train_ds, epochs=EPOCHS, validation_data=valid_ds)","ac3ee48b":"# model_synapses, module_name = different_nSynapses(threshold=.2, non_neg=True, nSynapse=True, dropout1=0.2, dropout2=0.1, use_sigmoid=True, regular_sigmoid=True, sigmoid_mult=50, sigmoid_threshold=.95, augment=True, optimizer=tf.keras.optimizers.Nadam(3e-3), qSynapse=(.05, .05), synLoss=MSE_RMS_SynapsesPerMS)","6ca0d154":"# history_syn = model_synapses.fit(train_ds, epochs=100, validation_data=valid_ds)#, callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)])#, SynapseLossDecay(ex_synapse_weight, inh_synapse_weight)])#, tf.keras.callbacks.LearningRateScheduler(tfa.optimizers.CyclicalLearningRate(1e-3, 5.1e-2, 1e-2, lambda x: 1))]) )#, callbacks=[SynapsePruner(50, 50, iterations=1)])#, ","057df619":"# model_synapses.compile(optimizer=SGD(momentum=0.9), \n#                   loss=[MeanSquaredError(), MSE_RMS_SynapsesPerMS(8), MSE_RMS_SynapsesPerMS(6)], \n#                   metrics={\"nSpikes\": keras.metrics.BinaryAccuracy(), \"ExcSyns\": MeanSynapsesPerMsMetric(), \"InhSyns\": MeanSynapsesPerMsMetric()},\n#                   loss_weights=[1.0 -0.02 -0.02, 0.02, 0.02])\n# model_synapses.fit(train_ds, epochs=50, validation_data=valid_ds)","73a55b66":"# model_synapses.fit(train_ds, epochs=50, validation_data=valid_ds)","56552d41":"model = model_synapses\nhistory = history_syn","3e583f86":"def printModule(dropout1, dropout2, sigmoid_threshold, sigmoid_mult, augment):\n    print(\"~*~ Visual Module ~*~\")\n    if augment: print(\"Images are augmented\")\n    print(f\"Image Dropout Rate: {dropout1}\")\n    print(f\"Synapse Threshold: {sigmoid_threshold}\")\n    print(f\"Synapse Training Sigmoid Multiplication: {sigmoid_mult}\")\n    print(f\"Synapses Dropout Rate: {dropout2}\")","26f7aaec":"def create_module(saccades=None, xaxis=True, use_sigmoid=True, pruning=True, dropout1=.2, dropout2=False,\n                        sigmoid_threshold=0.9, sigmoid_mult=15, to_bool=True, \n                        augment=False, optimizer=SGD(5e-3), conv_shape=(16,16),\n                        padding=PADDING, neurons=neurons, model_name=\"L5PC\", non_neg=False):\n    \n    printModule(dropout1, dropout2, sigmoid_threshold, sigmoid_mult, augment)\n    module_name = f\"module_{model_name}_{dropout1}_{dropout2}_{sigmoid_threshold}_{sigmoid_mult}_{augment}\"\n    \n    inp = keras.Input(shape=(256,256,1))\n    \n    padding = tf.Variable(lambda: padding, trainable=False)\n    x = inp\n    if augment: x = data_augmentation(augment)(x)\n    x = processor(x)\n    if dropout1: x = layers.Dropout(dropout1)(x)\n    conv_shape = (x.shape[-3], 1) if xaxis else conv_shape\n    x = Pad(padding)(x)\n    if non_neg: x = layers.Conv2D(1278, conv_shape, strides=conv_shape, activity_regularizer=pre_synaptic_spike_regularization, kernel_constraint=keras.constraints.non_neg(),use_bias=True, name=\"WiringLayer\")(x)\n    else: x = layers.Conv2D(1278, conv_shape, strides=conv_shape, activity_regularizer=pre_synaptic_spike_regularization, use_bias=True, name=\"WiringLayer\")(x)\n\n    x = layers.BatchNormalization(name=\"BatchNorm\")(x)\n    x = layers.Activation(sigmoid)(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, use_sigmoid=use_sigmoid, mult=sigmoid_mult, name='preNeuronBool')(x)\n    \n    x = K.squeeze(x, axis=-3)\n\n    if dropout2: x = layers.Dropout(dropout2)(x)\n    x = PredictNeuron(neurons)(x)[:,-198:,:]   # run through david's model, take only the post-noise time (32 ms X 6 times)\n    x = layers.MaxPooling1D(x.shape[-2], strides=x.shape[-2], name=\"MaxPooling\")(x)\n    output = layers.Flatten(name=\"nSpikes\")(x)\n\n    model = keras.Model(inp, output)\n    model.compile(optimizer=optimizer, loss={'nSpikes': MeanSquaredError()}, metrics={'nSpikes': keras.metrics.BinaryAccuracy()})\n    return model, module_name","8482d344":"# model, module_name = create_module(use_sigmoid=True, dropout1=False, dropout2=False, sigmoid_mult=75, sigmoid_threshold=0.9, augment=.8, optimizer=Nadam())","04e1d963":"# model.summary()","c6d7ce33":"# history = model.fit(train_ds, epochs=100, validation_data=valid_ds, callbacks=[SynapsePruner(100, 100, iterations=0)])","5b746294":"hist = history.history\n\nplt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\nplt.title(\"Loss\")\nplt.plot(hist[\"loss\"], color=\"k\", label=\"train\")\nplt.plot(hist[\"val_loss\"], color=\"r\", label=\"validation\")\nplt.axhline(0, color=\"b\", linestyle=\"--\")\nplt.xlabel(\"epochs\")\nplt.legend()\n\naccuracy = \"nSpikes_binary_accuracy\"\n# accuracy = \"binary_accuracy\"\n\nplt.subplot(1,2,2)\nplt.title(\"Accuracy\")\nplt.plot(hist[accuracy], color=\"k\", label=f\"train (last: {str(round(hist[accuracy][-1], 2))})\")\nplt.plot(hist[\"val_\"+accuracy], color=\"r\", label=f\"validation (last: {str(round(hist['val_'+accuracy][-1], 2))})\")\nplt.axhline(.5, color=\"b\", linestyle=\"--\", label=\"chance\")\nplt.ylim((0,1))\nplt.xlabel(\"epochs\")\nplt.legend()\n\nplt.show()","6a761506":"model.evaluate(test_ds)","e1593258":"def plot_examples(model, startFrom=200, plot_cycle=False, plot_spikes=True, plot_start=None, write_last=False, weights_start=None, preNeuron=\"NeuronInput\", spikeTrain=\"SpikeTrain\", postNeuron=\"nSpikes\"):\n    plt.figure(figsize=(200, 250))\n    how_many = 10\n    for img, label in train_ds.take(1):\n#         inputs = K.function(model.input, model.get_layer(bool_layers[0]).output)([img])\n        inputs = K.function(model.input, model.get_layer(preNeuron).output)([img])\n        outputs = K.function(model.input, model.get_layer(spikeTrain).output)([img])\n        if write_last: nAP = K.function(model.input, model.get_layer(postNeuron).output)([img])\n        for i in range(how_many):\n            plt.subplot(how_many, 3, 3*i+1)\n            plt.imshow(img[i,:,:,0]\/255, cmap='gray')\n            plt.title(LABELS[label[i]], fontdict={'fontsize':200})\n            plt.axis(\"off\")\n            plt.subplot(how_many, 3, 3*i+2)\n            if len(inputs[i].shape)==3:\n                curr_input = np.squeeze(inputs[i], axis=-3)[startFrom:]\n            else:\n                curr_input = inputs[i][startFrom:]\n            num_of_synapses = np.sum(curr_input, axis=-1)\n            mean_synapses = round(num_of_synapses.mean(), 2)\n            std_synapses = round(num_of_synapses.std(), 2)\n            plt.title(f\"\\u03BC: {str(mean_synapses)},  \\u03C3: {str(std_synapses)}\", fontdict={'fontsize':200})\n            plt.imshow(curr_input[-32:,:] if plot_cycle else curr_input, cmap='binary', vmin=0, vmax=1)\n            plt.axis(\"off\")\n            plt.subplot(how_many, 3, 3*i+3)\n            curr_output = outputs[i]            \n            spikes = []\n            curr_index = startFrom\n            while True:\n                start = np.where(curr_output[curr_index:] > 0.25)[0]\n                if not start.shape[0]: break\n                start = curr_index+start[0]\n                end = np.where(curr_output[start:] < 0.1)[0]\n                if not end.shape[0]: break\n                end = end[0] + start\n                spikes.append((start,end))\n                curr_index = end + 1\n            plt.title((f\"Score: {str(round(nAP[i][0],2))},\" if write_last else \"\") +f\"\\u03A3: {str(round(curr_output.sum(),2))}\" + (f\", spikes:{spikes}\" if plot_spikes else \"\"), fontdict={'fontsize':150})\n            plt.plot(curr_output, linewidth=5)\n            if weights_start is not None:\n                plt.plot(np.arange(weights_start, 400), model.get_layer(\"nSpikes\").get_weights()[0][:,0], color='r')\n            plt.ylim(0,1)\n            if plot_start: plt.axvline(plot_start, color='g', linestyle=':', linewidth=10.)\n            plt.axis('on')\n        break","af6b70a7":"def plot_statistics(model, starting_time=0, cycle_time=32, preNeuron=\"NeuronInput\", spikeTrain=\"SpikeTrain\"):\n    \n    all_inputs = []\n    n_synapses = []\n    synapses_mu = []\n    synapses_std = []\n    synapses_max = []\n    synapses_min = []\n#     sum_output = []\n    all_outputs = []\n#     excitatory_synapses = []\n#     inhibitory_synapses = []\n    label_dct = {}\n    synapses_mean_per_label = {}\n    \n    for img, label in train_ds.take(25):\n        inputs = K.function(model.input, model.get_layer(preNeuron).output)([img])\n        outputs = K.function(model.input, model.get_layer(spikeTrain).output)([img])\n        all_inputs.append(inputs)\n        labels = [LABELS[lbl] for lbl in label]\n        for j in range(len(inputs)):\n            if len(inputs[j].shape)==3:\n                curr_input = np.squeeze(inputs[j], axis=-3)\n            else:\n                curr_input = inputs[j]\n            \n            num_of_synapses = np.sum(curr_input[starting_time:starting_time+cycle_time, :N_EXC], axis=-1)\n#             excitatory_synapses.append(np.sum(curr_input[:, :N_EXC], axis=-1).mean())\n#             inhibitory_synapses.append(np.sum(curr_input[:, N_EXC:], axis=-1).mean())\n            n_synapses.append(num_of_synapses)\n            synapses_mu.append(num_of_synapses.mean())\n            synapses_std.append(num_of_synapses.std())\n            synapses_max.append(num_of_synapses.max())\n            synapses_min.append(num_of_synapses.min())\n            curr_output = outputs[j]\n#             sum_output.append(curr_output.sum())\n            all_outputs.append(curr_output)\n            if labels[j] not in label_dct:\n                label_dct[labels[j]] = []\n                synapses_mean_per_label[labels[j]] = {\"Sum\": [], \"Ex\":[], \"Inh\":[]}\n            label_dct[labels[j]].append(curr_output)\n            synapses_mean_per_label[labels[j]][\"Sum\"].append(np.sum(curr_input[starting_time:starting_time+cycle_time], axis=-1).mean())\n            synapses_mean_per_label[labels[j]][\"Ex\"].append(np.sum(curr_input[starting_time:starting_time+cycle_time, :N_EXC], axis=-1).mean())\n            synapses_mean_per_label[labels[j]][\"Inh\"].append(np.sum(curr_input[starting_time:starting_time+cycle_time, N_EXC:], axis=-1).mean())\n\n    mean_dct = {}\n    for lbl in label_dct.keys():\n        mean_dct[lbl] = np.stack(label_dct[lbl]).mean(axis=0)\n\n    plt.figure(figsize=(15,10))\n\n    plt.subplot(3,3,1)\n    plt.title('input values')\n    plt.hist(np.array(all_inputs).ravel())\n\n    plt.subplot(3,3,2)\n    plt.title('mean of sum synapses')\n    plt.hist(synapses_mu)\n\n    plt.subplot(3,3,3)\n    plt.title('std of sum synapses')\n    plt.hist(synapses_std)\n\n    plt.subplot(3,3,4)\n    plt.title('max sum synapses')\n    plt.hist(synapses_max)\n\n    plt.subplot(3,3,5)\n    plt.title('min sum synapses')\n    plt.hist(synapses_min)\n\n    plt.subplot(3,3,6)\n    plt.title(f'mean output of neuron (AP)')\n    for lbl, value in mean_dct.items():\n        plt.plot(value, label=lbl, linewidth=2.)\n    plt.ylim(0,1)\n    plt.legend()\n\n    plt.subplot(3,3,7)\n    plt.title('Excitatory Synapses Mean per Label')\n    plt.hist([synapses_mean_per_label[lbl][\"Ex\"] for lbl in synapses_mean_per_label.keys()], label=list(synapses_mean_per_label.keys()))\n    plt.legend()\n    plt.xlim()\n\n\n    plt.subplot(3,3,8)\n    plt.title('Inhibitory Synapses Mean per Label')\n    plt.hist([synapses_mean_per_label[lbl][\"Inh\"] for lbl in synapses_mean_per_label.keys()], label=list(synapses_mean_per_label.keys()))\n    plt.legend()\n    plt.xlim()\n\n    \n    plt.subplot(3,3,9)\n    plt.title('All Synapses Mean per Label')\n    plt.hist([synapses_mean_per_label[lbl][\"Sum\"] for lbl in synapses_mean_per_label.keys()], label=list(synapses_mean_per_label.keys()))\n    plt.legend()\n\n    plt.show()","7e8e5c3d":"import re\nmoduleLayer = r\"tf_op_layer_strided_slice_\\d+\"\nselectedLayer = None\nfor layer in model.layers:\n    if bool(re.search(moduleLayer, layer.name)):\n        selectedLayer = layer.name\n        break\n    else: print(layer.name)\nprint(selectedLayer)","b918d812":"plot_examples(model, startFrom=0, plot_cycle=False, plot_spikes=False, plot_start=None, write_last=True, preNeuron=\"preNeuronBool\", spikeTrain=selectedLayer)","19b8d3a6":"plot_statistics(model, 200 + 200%32, preNeuron=\"preNeuronBool\", spikeTrain=selectedLayer)","e31fe0ba":"plt.figure(figsize=(5,15))\nweights = model.get_layer(\"WiringLayer\").get_weights()[0][0]\nplt.suptitle(f\"Weights Examples (8 synapses out of {2*N_EXC})\")\nfor i in range(8):\n    for j in range(4):\n        plt.subplot(8, 4, 4*i+j+1)\n        x = np.zeros((32,8))\n        for l in range(32\/\/4):\n            x[:,l] = weights[:,l*4+j,i]\n        plt.imshow(x, vmin=tf.reduce_min(weights), vmax=tf.reduce_max(weights))\n        if not i:\n            plt.title(r\"$\\Theta={}\\pi$\".format(thetas[j]\/np.pi))\n        if not j:\n            plt.ylabel(f\"Syn {i}\\nyaxis pixels\")\n        if i == 7:\n            plt.xlabel(f\"filter\")\n        plt.xticks([])\n        plt.yticks([])","9b3133b5":"plt.plot(weights.mean(axis=(0,2)), label=\"mean\")\nplt.plot(np.abs(weights).mean(axis=(0,2)), label=\"abs mean\")\nplt.title(\"Mean and Abs Mean of Synapse Weights by Channel (after processing)\")\nplt.legend()\nplt.show()","973589ef":"plt.figure(figsize=(20,20))\n\nplt.subplot(2,2,1)\nplt.title(r\"Exc Synapses Weights (std by Synapse)\")\nplt.hist(weights[:,:,:N_EXC].std(axis=(0,1)))\n\nplt.subplot(2,2,3)\nplt.title(r\"Exc Synapses Weights (sum by Synapse)\")\nplt.hist(weights[:,:,:N_EXC].sum(axis=(0,1)))\n\nplt.subplot(2,2,2)\nplt.title(r\"Inh Synapses Weights (std by Synapse)\")\nplt.hist(weights[:,:,N_EXC:].std(axis=(0,1)))\n\nplt.subplot(2,2,4)\nplt.title(r\"Inh Synapses Weights (sum by Synapse)\")\nplt.hist(weights[:,:,N_EXC:].sum(axis=(0,1)))\n\nplt.show()","d714d9e3":"# plt.figure(figsize=(10,20))\nweights = model.get_layer(\"WiringLayer\").get_weights()[0][0]\nprint(f\"min: {weights.min()}, max: {weights.max()}\\nmean: {weights.mean()}, sd: {weights.std()}\\n\")\nexc = weights[:,:,:N_EXC]\ninh = weights[:,:,N_EXC:]\n# xExc = K.flatten(tf.transpose(exc) \/ tf.reduce_max(exc, axis=-1))\n# xInh = K.flatten(tf.transpose(inh) \/ tf.reduce_max(exc, axis=-1))\n# print(f\"Exc: mean per pre-synaptic: {xExc.mean()}, mean binary: {(xExc>PRESYNAPTIC_THRESHOLD).sum(axis=-1).mean()}\")\n# print(f\"Inh: mean per pre-synaptic: {weights[:,N_EXC:].mean()}, mean binary: {(weights[:,N_EXC:]>PRESYNAPTIC_THRESHOLD).sum(axis=-1).mean()}\")\namountExc = [(exc>0.01).sum(axis=(0,1))]\namountInh = [(inh>0.01).sum(axis=(0,1))]\n\nplt.subplot(1,2,1)\nplt.hist(amountExc)\nplt.title(\"Exc Synapses per Pixel\")\nplt.ylabel(\"Synapses per Pixel\")\n\nplt.subplot(1,2,2)\nplt.hist(amountInh)\nplt.title(\"Inh Synapses per Pixel\")\n# plt.ylabel(\"Synapses per Neuron\")\n\n\n\n# plt.subplot(3,1,1)\n# plt.title(\"Histogram Synapses per Presynaptic Neuron (Exc synapse)\")\n# plt.hist((weights[:,:N_EXC]>PRESYNAPTIC_THRESHOLD).sum(axis=-1))\n# plt.ylabel(\"nSynapses\")\n\n# plt.subplot(3,1,2)\n# plt.title(\"Histogram Synapses per Presynaptic Neuron (Inh synapse)\")\n# plt.hist((weights[:,N_EXC:]>PRESYNAPTIC_THRESHOLD).sum(axis=-1))\n# plt.ylabel(\"nSynapses\")\n\n# plt.subplot(3,1,3)\n# plt.title(\"Weights\")\n# plt.imshow(weights, cmap='binary', vmin=0, vmax=weights.max())\n# plt.axvline(N_EXC, color='r')\n# plt.xlabel(\"synapse\")\n# plt.ylabel(\"neuron\")\n\nplt.show()","077327e3":"serial = 0","b8854e45":"with open(f\"{module_name}_weights.npy\", 'wb') as f:\n    np.save(f, model.get_layer(\"WiringLayer\").get_weights()[0])\nwith open(f\"{module_name}_bias.npy\", 'wb') as f:\n    np.save(f, model.get_layer(\"WiringLayer\").get_weights()[1])\nwith open(f\"{module_name}_batchnorm.npy\", 'wb') as f:\n    np.save(f, model.get_layer(\"BatchNorm\").get_weights())","d2af1008":"# model.save(f\".\/{module_name}\")","b0b689b8":"class EvolutionModule:\n    generations = {}\n    results = {}\n    alive = set()\n    \n    def __init__(self, ID, generation, create_func, noise=.1, parent=None):\n        self.module = None\n        \n        self.generation = generation\n        self.cur_generation = generation\n        self.ID = ID\n        self.parent = parent\n        self.name = \"Module\" + (f\"({parent.__str__()[6:]})-\" if parent else \"\") + f\"[{generation}.{ID}]\"\n        \n        if generation not in EvolutionModule.generations: EvolutionModule.generations[generation] = [self]\n        else: EvolutionModule.generations[generation].append(self)\n        EvolutionModule.alive.add(self)\n                   \n        self.history = None\n        \n        self.children = set()\n        self.status = True\n        \n        self.create_func = create_func\n        self.original_weights = None\n        self.noise = noise\n        \n        self._create_model()\n        \n    \n    def _create_model(self):\n        self.module = self.create_func()\n        if self.parent:\n            self.original_weights = [weight for weight in self.parent.get_weights()]\n            self.original_weights[2] = self.original_weights[2] + np.random.rand(*self.original_weights[2].shape) * self.original_weights[2].std() * self.noise\n            self.module.set_weights(self.original_weights)\n    \n    def give_birth(self, n):\n        self.cur_generation += 1\n        for i in range(n):\n            self.children.add(EvolutionModule(i, self.cur_generation, self.create_func, self.noise, self))\n    \n    def is_alive(self):\n        return self.status\n    \n    def fit(self, training_ds, epochs, validation_data):\n        self.to_print()\n        history = self.module.fit(train_ds, epochs=epochs, validation_data=validation_data).history\n        if not self.history: self.history = history\n        else: \n            for key, value in history.items(): \n                self.history[key].extend(value)\n    \n    def die(self):\n        self.status = False\n        EvolutionModule.alive.remove(self)\n    \n    def value(self, n):\n        if not self.history: return None\n        return np.mean(self.history['loss'][-n:])\n    \n    def accuracy(self):\n        return self.history['val_logits_binary_accuracy'][-1]\n    \n    def value_to_print(self, n):\n        return self.history['loss'][-1], np.mean(self.history['loss'][-n:]), self.history['val_loss'][-1], np.mean(self.history['val_loss'][-n:]), self.history['val_logits_binary_accuracy'][-1], np.mean(self.history['val_logits_binary_accuracy'][-n:])\n    \n    def to_print(self):\n        print(f\"\\n Beginning fitting for {self.name}...\\n\")\n    \n    def __str__(self):\n        return self.name\n    \n    def get_weights(self):\n        return self.module.get_weights()","5d8fb8dc":"def create_model():\n    return playable(saccades=None, xaxis=True, use_sigmoid=True, sigmoid_mult=50, to_bool=True, useSynapse=True, qSynapse=0.1, sigmoid_threshold=0.8, augment=False, optimizer=SGD(5e-3))","7675583e":"def run_evolution(nStart=10, nEpochs=10, nMax=2, nPerMax=3, nGenerations=5, rand=0.1, measure_by=2):\n    EvolutionModule.generations.clear()\n    EvolutionModule.results.clear()\n    EvolutionModule.alive.clear()\n    \n    print(\"\\nGeneration 0:\\n\")\n    for i in range(nStart):\n        module = EvolutionModule(i, 0, create_model, rand)\n        module.fit(train_ds, epochs=nEpochs, validation_data=valid_ds)\n    EvolutionModule.results[0] = sorted(EvolutionModule.alive, key=lambda x: x.value(measure_by))\n    print(\"\\n\" + \"* \"*10)\n    print(f'Generation: 0:\\n Best accuracy (by loss): {EvolutionModule.results[0][0].accuracy()} by {EvolutionModule.results[0][0]}\\n')\n    for module in EvolutionModule.alive:\n        print(module, end=': ')\n        print(\"Last loss: {0}, Mean loss: {1}, Last val_loss: {2}, Mean val_loss: {3}, Last val accuracy: {4}, Mean val acc: {5}\".format(*module.value_to_print(measure_by)))\n    print(\"\\n\" + \"* \"*10+'\\n')\n    \n    \n    for generation in range(1, nGenerations):    \n        print(f\"\\nGeneration {generation}:\\n\")\n        for module, indicator in zip(EvolutionModule.results[generation-1], [True]*nMax+[False]*len(EvolutionModule.alive)):\n            if indicator: module.give_birth(nPerMax)\n            else: module.die()\n        for module in EvolutionModule.alive:\n            module.fit(train_ds, epochs=nEpochs, validation_data=valid_ds)\n        \n        EvolutionModule.results[generation] = sorted(EvolutionModule.alive, key=lambda x: x.value(measure_by))\n        print(\"\\n\" + \"* \"*10)\n        print(f'Generation: {generation}:\\n Best accuracy (by loss): {EvolutionModule.results[generation][0].accuracy()}\\n')\n        for module in EvolutionModule.alive:\n            print(module, end=': ')\n            print(\"Last loss: {0}, Mean loss: {1}, Last val_loss: {2}, Mean val_loss: {3}, Last val accuracy: {4}, Mean val acc: {5}\".format(*module.value_to_print(measure_by)))\n        print(\"\\n\" + \"* \"*10+'\\n')\n        \n    print(\"\\n~ ~ ~ ~ ~ ~ Done! ~ ~ ~ ~ ~ ~\\n\")\n    return EvolutionModule.results[-1][0], EvolutionModule.generations, EvolutionModule.results, EvolutionModule.alive","f6ad6b1b":"def evolution(result_dct={}, nStart=10, nEpochs=10, nMax=2, nPerMax=3, nGenerations=5, rand=0.1, measure_by=3):\n    result_dct[0] = []\n    for i in range(nStart):\n        print(\"\\nStarting first generation module number\", i, \"...\\n\")\n        model = create_model()\n        history = model.fit(train_ds, epochs=nEpochs, validation_data=valid_ds)\n        result_dct[0].append((model, history.history['loss'], history.history['val_loss'], history.history['val_logits_binary_accuracy']))\n    for generation in range(1,nGenerations):\n        result_dct[generation] = []\n        max_chosen = sorted(result_dct[generation-1], key=lambda x: sum(x[1][-measure_by:]))[:nMax]\n        \n        print(\"\\n\" + \"* \"*10)\n        print(f'Generation: {generation}:\\n Best accuracy (by loss): {max_chosen[0][-1][-1]}\\n')\n        for result in result_dct[generation-1]:\n            print(\"last loss:\", result[1][-1], \" loss mean:\", np.mean(result[1][-measure_by:]),\"   last accuracy:\", result[-1][-1], \"  accuracy mean:\", np.mean(result[-1][-measure_by:]))\n        print(\"\\n\" + \"* \"*10+'\\n')\n        \n        print(\"Starting generation \", generation+1,\"...\\n\")\n        i = 1\n        for max_model, _, _ in max_chosen:\n            old_weights = max_model.get_weights()\n            for _ in range(nPerMax):\n                print(\"Starting module number \", i, \" out of\", nMax*(nPerMax+1), \"\\n\")\n                new_weights = [weight for weight in old_weights]\n                old_conv_weights = new_weights[2]\n                new_weights[2] = old_conv_weights + np.random.rand(*old_conv_weights.shape) * old_conv_weights.std() * rand\n                model = create_model()\n                model.set_weights(new_weights)\n                history = model.fit(train_ds, epochs=nEpochs, validation_data=valid_ds)\n                result_dct[generation].append((model, history.history['loss'], history.history['val_loss'], history.history['val_logits_binary_accuracy']))\n                i+=1\n            print(\"\\nStarting father...\\n\")\n            history = max_model.fit(train_ds, epochs=nEpochs, validation_data=valid_ds)\n            result_dct[generation].append((model, history.history['loss'], history.history['val_loss'], history.history['val_logits_binary_accuracy']))\n    max_chosen = sorted(result_dct[generation-1], key=lambda x: sum(x[1][-measure_by:]))[:nMax]\n        \n    print(\"\\n\" + \"* \"*10)\n    print(f'Generation: {nGenerations}:\\n Best accuracy (by loss): {max_chosen[0][-1][-1]}\\n')\n    for result in result_dct[nGenerations-1]:\n        print(\"last loss:\", result[2][-1], \" loss mean:\", np.mean(result[2][-measure_by:]),\"   last accuracy:\", result[-1][-1], \"  accuracy mean:\", np.mean(result[-1][-measure_by:]))\n    print(\"\\n\" + \"* \"*10+'\\n')\n    return result_dct","ead0cfb4":"# best_module, generations, results, alive = run_evolution(nStart=10, nEpochs=10, nMax=3, nPerMax=2, nGenerations=6)\n# print(best_module.value_to_print(1))","c22ce213":"# plot_examples(best_module)","b864b4df":"# plot_statistics(best_module)","3bdd39e9":"# cur_model = best_module\n# while cur_model:\n#     print(\" * * * * * * *\"*2)\n#     print(\"Current module:\", cur_model)\n#     print(\"current weights:\", best_module.get_weights()[2].std())\n#     print(\"Original weights:\", best_module.original_weights[2].std())\n#     cur_model = cur_model.parent","36929961":"# Plot","50db9bd7":"# Preprocess + Conv3D + Nx(Conv2D + MaxPooling2D) + Dense","d7683983":"# BenchMark","6063a40b":"# Lowering Synapse loss Callback","6e1ee85c":"# Noisy Optimizer (not used)\nhttps:\/\/arxiv.org\/abs\/1511.06807\n\nhttps:\/\/github.com\/cpury\/keras_gradient_noise","423f0c4d":"# Start noise","73fde942":"Show how the microsaccades look. The ints are the order, and the paint is the grade the order is decided by. the noise_mult can control how much the order is centered (the higher it is, the noisier it gets).","ad442a24":"# Model","35c4576f":"# Save Weights","ae61bc03":"# Evolution (not used)","88b25ee3":"will be in use later","3e90db4c":"# Pruning (not in use)","0a869774":"# Previous Modules and Funcs","2e366159":"Gabor initializer","b1df2a18":"# Use David Beniaguev's (selfishgene) trained L5PC model","4731e599":"# Some custom layers and funcs to use","c00eaf97":"Function to create Conv2D layer waith gabor filter","e8a2ef3e":"# Custom Funcs","38f2bd57":"# **Preprocessing**","5a266141":"Parameters for gabor filters and max pooling kernel sizes are from \"Robust Object Recognition with Cortex-Like Mechanisms\" (Serre et al.)","68f63385":"show some photos","2aaafe05":"# Previous funcs and layers I don't use anymore","7c575461":"# Synapse Loss","1fe67a75":"using SGD - learn","2e67dc24":"# Microsaccades (not used)\nAfter preprocessing, the model will use a 3d convolutional layer with 1278 filters (that will be the input vector for a synapse) that slices the image to 256 blocks. Each one of these blocks will represent one ms, but in order to keep the \"eyesight\" none-linear, the vectors are reorganized in an order that represents microsaccades.\nThe following functions define the blocks order.","e55b21a9":"how the new sigmoid looks like","1aa59f67":"# Transform data to dataset","acbd343d":"# Convert Images to 400X1278 matrices","f66deeb9":"> # Gabor filters - Simple Cells","a01d4412":"# Convert V1-like images to Dataset (not used)","3f171ee1":"One Conv2D - learns","4c14015a":"Show preprocessing on one image from dataset","05c28f12":"# Dense Initializer","c67517d6":"# Best Module so Far"}}