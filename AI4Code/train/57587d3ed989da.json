{"cell_type":{"43878c81":"code","9cf33e64":"code","a1be09ac":"code","14f17963":"code","780c531e":"code","6b0c30cc":"code","8b663ca5":"code","50b1916f":"code","6b587b00":"code","ca7abf36":"code","449ebeff":"code","98cbade9":"code","b7f59b90":"code","64ed14ba":"code","cbdb277d":"code","305d44db":"code","ffa735fd":"code","2de3a7c5":"code","501dabf5":"code","ffa7fc31":"code","1ef2ece4":"code","065a046b":"code","31f6ff64":"code","2af1fc57":"code","874a116d":"code","1f35459e":"code","d2e7c50e":"code","d47d4f3f":"code","7a13a422":"code","3637975a":"code","7153fcf5":"code","e00a2eca":"code","5ac01159":"code","01e5c30f":"code","0930c321":"code","439008ea":"code","ae290e3a":"code","f16bd983":"code","56bebb88":"code","9e7ab95b":"code","12c6b0c1":"code","c07f2429":"code","6da5417b":"code","227799ca":"code","3f5a16a7":"code","f580cd85":"code","a5074b81":"code","fbb1b0b7":"markdown","ee9ddafa":"markdown","97f30bde":"markdown","0427cddf":"markdown","d4aa2362":"markdown","1ed70af9":"markdown","7ec31f07":"markdown","b90828ee":"markdown","abea151e":"markdown","e7ac882b":"markdown","b716beb6":"markdown","995edcb7":"markdown","d7ccefea":"markdown","0af9997d":"markdown","63bca2b9":"markdown","12f8ce89":"markdown","c84910a1":"markdown","b82cb507":"markdown","9863a59f":"markdown","60f0950a":"markdown","f73bed94":"markdown","7dda875c":"markdown","144b700b":"markdown","9c3f9e12":"markdown","6ac3149c":"markdown","e8da85bd":"markdown","e240b11f":"markdown","365a9dde":"markdown"},"source":{"43878c81":"# Import needed libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nfrom mlxtend.frequent_patterns import apriori, association_rules\nimport re\n\npd.options.display.max_rows = None","9cf33e64":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a1be09ac":"# Read data in the excel file\n\ndf = pd.read_excel('..\/input\/online-retail-data-set-from-uci-ml-repo\/Online Retail.xlsx')\ndf.head()","14f17963":"df.shape","780c531e":"df.info()","6b0c30cc":"df.describe()","8b663ca5":"# Check null values\ndf.isnull().sum()","50b1916f":"# Check number of unique values\ndf.nunique()","6b587b00":"# Check each stock code has only one description\ndf.groupby('StockCode').apply(lambda x: x['Description'].unique())","ca7abf36":"# Number of invoices for each country\ndf.groupby(['Country']).count() ['InvoiceNo']","449ebeff":"# Delete rows with null CustomerID\nclean_df = df.dropna(subset = ['CustomerID'])\n\n# Check null values\nclean_df.isnull().sum()","98cbade9":"# Removing the price and quantity that are less than or equal to 0\nclean_df = clean_df[(clean_df.Quantity >= 0) & (clean_df.UnitPrice >= 0)]","b7f59b90":"clean_df.describe()","64ed14ba":"# Check the number of invoices that starts with letter 'c', cancellation.\nclean_df['InvoiceNo'] = clean_df['InvoiceNo'].astype('str')\nclean_df[clean_df['InvoiceNo'].str.contains(\"c\")].shape[0]","cbdb277d":"# Check the stock code\n\ndef has_right_scode(input):\n    \n    \"\"\"\n    Function: check the if the stock code is wirtten in a right way,\n            The function check if the code contains 5-digit number or 5-digit number with a letter.\n    Args:\n      input(String): Stock code\n    Return:\n      Boolean: True or False\n    \"\"\"\n    \n    x = re.search(\"^\\d{5}$\", input)\n    y = re.search(\"^\\d{5}[a-zA-Z]{1}$\", input)\n    if (x or y):\n        return True\n    else:\n        return False\n\n    \nclean_df['StockCode'] = clean_df['StockCode'].astype('str')\nclean_df = clean_df[clean_df['StockCode'].apply(has_right_scode) == True]\nclean_df.head()","305d44db":"# One discription for each stock code\n\n# Put all Descriptions of each StockCode in a list \ndf_itms = pd.DataFrame(clean_df.groupby('StockCode').apply(lambda x: x['Description'].unique())).reset_index()\ndf_itms.rename(columns = { 0: 'Description2'}, inplace = True)\n\n# StockCode that have more than one Description\ndf_itms[df_itms['Description2'].str.len() != 1]","ffa735fd":"# Take one Description for each StockCode\ndf_itms.loc[:, 'Description2'] = df_itms.Description2.map(lambda x: x[0])\n\n# StockCode that have more than one Description\ndf_itms[df_itms['Description2'].str.len() != 1]","2de3a7c5":"# Merge clean_df with df_itms\nclean_df = pd.merge(clean_df, df_itms, on = 'StockCode')\nclean_df = clean_df.drop('Description', axis = 1)\nclean_df.head()","501dabf5":"clean_df.rename(columns = { 'Description2': 'Description'}, inplace = True)\nclean_df.head()","ffa7fc31":"df_itms_togthr = clean_df.groupby(['InvoiceNo','Description'])['Quantity'].sum()\ndf_itms_togthr.head()","1ef2ece4":"df_itms_togthr = df_itms_togthr.unstack().fillna(0)\ndf_itms_togthr.head()","065a046b":"# Encode the frequency of description to 0 or 1\nencode = lambda x : 1 if (x >= 1) else 0\ndf_itms_togthr = df_itms_togthr.applymap(encode)\ndf_itms_togthr.head()","31f6ff64":"df_itms_togthr.shape","2af1fc57":"nl_df = clean_df[clean_df['Country'] == 'Netherlands']\nspain_df = clean_df[clean_df['Country'] == 'Spain']\nfrance_df = clean_df[clean_df['Country'] == 'France']","874a116d":"nl_df.head()","1f35459e":"spain_df.head()","d2e7c50e":"france_df.head()","d47d4f3f":"df_itms_togthr_nl = nl_df.groupby(['InvoiceNo','Description'])['Quantity'].sum()\n\ndf_itms_togthr_nl = df_itms_togthr_nl.unstack().fillna(0)\n\nencode = lambda x : 1 if (x >= 1) else 0\ndf_itms_togthr_nl = df_itms_togthr_nl.applymap(encode)\ndf_itms_togthr_nl.head()","7a13a422":"df_itms_togthr_spain = spain_df.groupby(['InvoiceNo','Description'])['Quantity'].sum()\n\ndf_itms_togthr_spain = df_itms_togthr_spain.unstack().fillna(0)\n\nencode = lambda x : 1 if (x >= 1) else 0\ndf_itms_togthr_spain = df_itms_togthr_spain.applymap(encode)\ndf_itms_togthr_spain.head()","3637975a":"df_itms_togthr_france = france_df.groupby(['InvoiceNo','Description'])['Quantity'].sum()\n\ndf_itms_togthr_france = df_itms_togthr_france.unstack().fillna(0)\n\nencode = lambda x : 1 if (x >= 1) else 0\ndf_itms_togthr_france = df_itms_togthr_france.applymap(encode)\ndf_itms_togthr_france.head()","7153fcf5":"# Build the Apriori model\nrep_items = apriori(df_itms_togthr, min_support = 0.02, use_colnames = True, verbose = 1)\nrep_items.head()","e00a2eca":"# Generate the association rules dataframe\nrules = association_rules(rep_items, metric = \"confidence\", min_threshold = 0.6)\nrules","5ac01159":"# The number of rules\nrules.shape[0]","01e5c30f":"# Show confidence distribution\nplt.hist(rules['confidence'])\nplt.show()","0930c321":"# Show the rules that have confidance > 0.75\nhigh_confidance = rules[rules['confidence'] > 0.75]\nhigh_confidance [['antecedents', 'consequents']]","439008ea":"# Build the Apriori model\nrep_items_nl = apriori(df_itms_togthr_nl, min_support = 0.1, use_colnames = True, verbose = 1)\nrep_items_nl.head()","ae290e3a":"# Generate the association rules dataframe\nrules_nl = association_rules(rep_items_nl, metric = \"confidence\", min_threshold = 0.6)\nrules_nl.head()","f16bd983":"# The number of rules\nrules_nl.shape[0]","56bebb88":"high_confidance_nl = rules_nl[rules_nl['confidence'] > 0.75]\nhigh_confidance_nl [['antecedents', 'consequents']]","9e7ab95b":"# Build the Apriori model\nrep_items_spain = apriori(df_itms_togthr_spain, min_support = 0.1, use_colnames = True, verbose = 1)\nrep_items_spain.head()","12c6b0c1":"# Generate the association rules dataframe\nrules_spain = association_rules(rep_items_spain, metric = \"confidence\", min_threshold = 0.6)\nrules_spain","c07f2429":"# The number of rules\nrules_spain.shape[0]","6da5417b":"high_confidance_spain = rules_spain[rules_spain['confidence'] > 0.75]\nhigh_confidance_spain [['antecedents', 'consequents']]","227799ca":"# Build the Apriori model\nrep_items_france = apriori(df_itms_togthr_france, min_support = 0.05, use_colnames = True, verbose = 1)\nrep_items_france.head()","3f5a16a7":"# Generate the association rules dataframe\nrules_france = association_rules(rep_items_france, metric = \"confidence\", min_threshold = 0.6)\nrules_france","f580cd85":"# The number of rules\nrules_france.shape[0]","a5074b81":"high_confidance_france = rules_france[rules_france['confidence'] > 0.75]\nhigh_confidance_france [['antecedents', 'consequents']]","fbb1b0b7":"As we can see from the previous section, there are null data. So, I will remove the rows that contain null values.  ","ee9ddafa":"### Dataset","97f30bde":"Apriori algorithm gave us some rules about the item that is frequently purchased with other items. The result of association analysis using the whole dataset different than the result of association analysis when using a dataset of a specific country. These rules are helpful for items recommendation in the e-commerce websites.   ","0427cddf":"This step builds the association rule model from the dataset to answer the questions, and discusses the results.","d4aa2362":"### 2.1. Missing Data","1ed70af9":"#### 2.3.1 All Countries Data","7ec31f07":"## 2. Data Preparation","b90828ee":"## 4. Summary","abea151e":"This section includes:\n* Removing the price and quantity that are less than or equal to 0. \n* Removing inconsistent Stock code.\n* Keeping only one description for each Stock codes.","e7ac882b":"As we can see, the items in the each Description list are the same item but they are written in different ways. So, I will keep only one describtion.","b716beb6":"### 3.1 All Countries Data","995edcb7":"This step includes cleaning data and preparing it before using Apyori algorithm.","d7ccefea":"To build the association rule model, the `Description` categorical variables should be converted to dummy variables.","0af9997d":"### 3.3 Results Discussion\n\nThe results of association analysis show which item is frequently purchased with other items. The result of association analysis using the whole dataset is differnt than the result of association analysis when using a dataset of a spacific country. ","63bca2b9":"The project is organized as follows. Section 1 explores and visualize the data. Section 2 includes data preperation and Section 3 implements the solution. Finally, Section 4 presents the summary.","12f8ce89":"#### 2.3.2 Netherlands, Spain, France Countries Data","c84910a1":"* **Online Retail.xlsx** - The file contains all transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail. In thecdataset, there are eight columns:\n\n    * InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n    * StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n    * Description: Product (item) name. Nominal.\n    * Quantity: The quantities of each product (item) per transaction. Numeric.\n    * InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n    * UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n    * CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n    * Country: Country name. Nominal, the name of the country where each customer resides.\n    ","b82cb507":"# Finding Items Bought Together","9863a59f":"## 1. Data Understanding","60f0950a":"## Overview\n\nCustomers are important to the survival and success of any business because they are the source of the revenue. The success of a business is the ability to satisfy customers and make them happy, and therefore turn a profit from them. Giving the customer recommendation about items that can buy it with items in their online basket may attract them.\nUsing retail transactional data can help to build models for finding the items that are bought together and recommending customers about items that can buy it. \n\nIn this project, I will analyze an online retail transactional dataset to answer the following questions:\n* Which items are bought together?\n* Which items are bought together in different specific countries?\n\nTo answer the questions, I will build an association rules model using Apriori Algorithm to find the items that are usually bought together by the customers. I will also find the items that are usually bought together in different specific countries.\n","f73bed94":"I will select the minimum support based on the size of the dataset. Small support value for the big dataset and bigger support value for the smaller dataset. The confidence will be 60%. ","7dda875c":"## 3. Modeling","144b700b":"This step explores the dataset using different functions such as `head()`, `shape`, `describe()` and checks the null values using `isnull()`.","9c3f9e12":"### 3.2 Netherlands, Spain, France Countries Data","6ac3149c":"Now, the data is ready to be used to build the association rule model.","e8da85bd":"### 2.3. Creating Dummy Variables","e240b11f":"### 2.2. Dealing with Inconsistent Data","365a9dde":"**Observations:**\n\nFrom the data, we can see that:\n* The minimum price and quantity are negative, and this is impossible.\n* There are null values in CustomerID and Description columns.\n* Some Stock Codes are not a 5-digit integral number.\n* Some Stock Codes has more than one item description.\n"}}