{"cell_type":{"d110d186":"code","6637693d":"code","19f73c94":"code","966ef781":"code","abc3030d":"code","cd0a5cb2":"code","b68937c7":"code","4a8c48d6":"code","94b13dd2":"code","9a85f280":"code","d451cf91":"code","c2693e58":"code","40e5a286":"code","aa105f6c":"code","23b7de09":"code","ac5921fd":"code","7a813d89":"code","faaef8d9":"code","b612c738":"code","26b0e2d1":"code","8a6a4e4f":"code","6c8e311b":"code","c17dcf4e":"code","d437f43d":"code","f2175fad":"code","c805a574":"code","648179a0":"code","3e592f4d":"code","c5578a74":"code","67e00a23":"code","42953019":"code","5e3c9827":"code","596f458d":"code","7ecfad6b":"code","0ee6e07c":"code","bf778b49":"code","47d75d0e":"code","9096d103":"code","13cfe587":"code","dc8cc470":"code","8ca44975":"code","f2994c79":"code","ca826d22":"code","dc30d9c5":"code","53d38b34":"code","e4fabba1":"code","a5b8ef29":"code","50bebda3":"code","04424fe5":"code","5d31997e":"code","d9f3b06c":"code","415fc42a":"code","e30d8da1":"code","927fce1e":"code","5fa49e5d":"code","a4a0d908":"code","a48b6258":"code","c0132a6d":"code","a34f1d77":"code","a11e347f":"code","689368ba":"code","8616b7c7":"code","574a9bf2":"code","ab59865f":"code","294ed5b9":"code","1dfd861e":"code","721c8123":"code","87c28989":"code","c02089f1":"code","f344096e":"code","67e55534":"markdown","93ee1a6a":"markdown","4b60095c":"markdown","d5b6df4f":"markdown","cf4d9cf6":"markdown","5d0efaed":"markdown","8ac16513":"markdown","1e937d10":"markdown","d1c39804":"markdown","d1fe278a":"markdown","b8ba4b10":"markdown","2db92733":"markdown","80fec78d":"markdown","2ba3b12b":"markdown","288bc94e":"markdown","330c3a7c":"markdown","0199af97":"markdown","d658cc6e":"markdown","b692b5c3":"markdown","30b4a5de":"markdown","042b7205":"markdown","ff3f1960":"markdown","7f8d141c":"markdown","da76f340":"markdown","8f3d0813":"markdown","a5806e67":"markdown","325234cf":"markdown","3682b2ea":"markdown","bd848379":"markdown","f6453316":"markdown","bbae6fd1":"markdown","37456890":"markdown","82d11c30":"markdown","7efdec39":"markdown","cb62477c":"markdown","84fe1e4e":"markdown","bfdd0c15":"markdown","7b765500":"markdown","2360c677":"markdown","bd581379":"markdown","ba940028":"markdown","e2a2aa5d":"markdown","c9bf30e9":"markdown","d901cf64":"markdown","623a8586":"markdown","76fdacbb":"markdown","d1ac76a5":"markdown","45b6882a":"markdown","e5cdb235":"markdown","a4de364a":"markdown","63dcbb3c":"markdown","601adf63":"markdown","47d51ec2":"markdown","20bec8fb":"markdown","06ef6b7e":"markdown","78bc020b":"markdown","faf31786":"markdown","72f5d622":"markdown","fdaa7f3d":"markdown","beb560e4":"markdown","9bbbd5b8":"markdown","6490e828":"markdown","8fb10290":"markdown","b4a91c0b":"markdown","a6b5c05d":"markdown","96413f50":"markdown","793a9751":"markdown","b0c33e70":"markdown","18a4ac13":"markdown"},"source":{"d110d186":"# numpy\nimport numpy as np\n\n# pandas stuff\nimport pandas as pd\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# plotting stuff\nfrom pandas.plotting import lag_plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\ncolorMap = sns.light_palette(\"blue\", as_cmap=True)\n#plt.rcParams.update({'font.size': 12})\n\n\n# install dabl\n!pip install dabl > \/dev\/null\nimport dabl\n# install datatable\n!pip install datatable > \/dev\/null\nimport datatable as dt\n\n# misc\nimport missingno as msno\n\n# system\nimport warnings\nwarnings.filterwarnings('ignore')\n# for the image import\nimport os\nfrom IPython.display import Image\n# garbage collector to keep RAM in check\nimport gc  ","6637693d":"!wc -l ..\/input\/jane-street-market-prediction\/train.csv","19f73c94":"%%time\n\ntrain_data_datatable = dt.fread('..\/input\/jane-street-market-prediction\/train.csv')","966ef781":"%%time\n\ntrain_data = train_data_datatable.to_pandas()","abc3030d":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_data['resp']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"Cumulative resp\", fontsize=18);\nbalance.plot(lw=3);\ndel balance\ngc.collect();","cd0a5cb2":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_data['resp']).cumsum()\nresp_1= pd.Series(train_data['resp_1']).cumsum()\nresp_2= pd.Series(train_data['resp_2']).cumsum()\nresp_3= pd.Series(train_data['resp_3']).cumsum()\nresp_4= pd.Series(train_data['resp_4']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\nplt.legend(loc=\"upper left\");\ndel resp_1\ndel resp_2\ndel resp_3\ndel resp_4\ngc.collect();","b68937c7":"plt.figure(figsize = (12,5))\nax = sns.distplot(train_data['resp'], \n             bins=3000, \n             kde_kws={\"clip\":(-0.05,0.05)}, \n             hist_kws={\"range\":(-0.05,0.05)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of the resp values\", size=14)\nplt.show();\ngc.collect();","4a8c48d6":"min_resp = train_data['resp'].min()\nprint('The minimum value for resp is: %.5f' % min_resp)\nmax_resp = train_data['resp'].max()\nprint('The maximum value for resp is:  %.5f' % max_resp)","94b13dd2":"print(\"Skew of resp is:      %.2f\" %train_data['resp'].skew() )\nprint(\"Kurtosis of resp is: %.2f\"  %train_data['resp'].kurtosis() )","9a85f280":"from scipy.optimize import curve_fit\n# the values\nx = list(range(len(values)))\nx = [((i)-1500)\/30000 for i in x]\ny = values\n\ndef Lorentzian(x, x0, gamma, A):\n    return A * gamma**2\/(gamma**2+( x - x0 )**2)\n\n# seed guess\ninitial_guess=(0, 0.001, 3000)\n\n# the fit\nparameters,covariance=curve_fit(Lorentzian,x,y,initial_guess)\nsigma=np.sqrt(np.diag(covariance))\n\n# and plot\nplt.figure(figsize = (12,5))\nax = sns.distplot(train_data['resp'], \n             bins=3000, \n             kde_kws={\"clip\":(-0.05,0.05)}, \n             hist_kws={\"range\":(-0.05,0.05)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\n#norm = plt.Normalize(values.min(), values.max())\n#colors = plt.cm.jet(norm(values))\n#for rec, col in zip(ax.patches, colors):\n#    rec.set_color(col)\nplt.xlabel(\"Histogram of the resp values\", size=14)\nplt.plot(x,Lorentzian(x,*parameters),'--',color='black',lw=3)\nplt.show();\ndel values\ngc.collect();","d451cf91":"percent_zeros = (100\/train_data.shape[0])*((train_data.weight.values == 0).sum())\nprint('Percentage of zero weights is: %i' % percent_zeros +\"%\")","c2693e58":"min_weight = train_data['weight'].min()\nprint('The minimum weight is: %.2f' % min_weight)","40e5a286":"max_weight = train_data['weight'].max()\nprint('The maximum weight was: %.2f' % max_weight)","aa105f6c":"train_data[train_data['weight']==train_data['weight'].max()]","23b7de09":"plt.figure(figsize = (12,5))\nax = sns.distplot(train_data['weight'], \n             bins=1400, \n             kde_kws={\"clip\":(0.001,1.4)}, \n             hist_kws={\"range\":(0.001,1.4)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of non-zero weights\", size=14)\nplt.show();\ndel values\ngc.collect();","ac5921fd":"train_data_nonZero = train_data.query('weight > 0').reset_index(drop = True)\nplt.figure(figsize = (10,4))\nax = sns.distplot(np.log(train_data_nonZero['weight']), \n             bins=1000, \n             kde_kws={\"clip\":(-4,5)}, \n             hist_kws={\"range\":(-4,5)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of the logarithm of the non-zero weights\", size=14)\nplt.show();\ngc.collect();","7a813d89":"from scipy.optimize import curve_fit\n# the values\nx = list(range(len(values)))\nx = [(i\/110)-4 for i in x]\ny = values\n\n# define a Gaussian function\ndef Gaussian(x,mu,sigma,A):\n    return A*np.exp(-0.5 * ((x-mu)\/sigma)**2)\n\ndef bimodal(x,mu_1,sigma_1,A_1,mu_2,sigma_2,A_2):\n    return Gaussian(x,mu_1,sigma_1,A_1) + Gaussian(x,mu_2,sigma_2,A_2)\n\n# seed guess\ninitial_guess=(1, 1 , 1,    1, 1, 1)\n\n# the fit\nparameters,covariance=curve_fit(bimodal,x,y,initial_guess)\nsigma=np.sqrt(np.diag(covariance))\n\n# the plot\nplt.figure(figsize = (10,4))\nax = sns.distplot(np.log(train_data_nonZero['weight']), \n             bins=1000, \n             kde_kws={\"clip\":(-4,5)}, \n             hist_kws={\"range\":(-4,5)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of the logarithm of the non-zero weights\", size=14)\n# plot gaussian #1\nplt.plot(x,Gaussian(x,parameters[0],parameters[1],parameters[2]),':',color='black',lw=2,label='Gaussian #1', alpha=0.8)\n# plot gaussian #2\nplt.plot(x,Gaussian(x,parameters[3],parameters[4],parameters[5]),'--',color='black',lw=2,label='Gaussian #2', alpha=0.8)\n# plot the two gaussians together\nplt.plot(x,bimodal(x,*parameters),color='black',lw=2, alpha=0.7)\nplt.legend(loc=\"upper left\");\nplt.show();\ndel values\ngc.collect();","faaef8d9":"train_data['weight_resp']   = train_data['weight']*train_data['resp']\ntrain_data['weight_resp_1'] = train_data['weight']*train_data['resp_1']\ntrain_data['weight_resp_2'] = train_data['weight']*train_data['resp_2']\ntrain_data['weight_resp_3'] = train_data['weight']*train_data['resp_3']\ntrain_data['weight_resp_4'] = train_data['weight']*train_data['resp_4']\n\nfig, ax = plt.subplots(figsize=(15, 5))\nresp    = pd.Series(1+(train_data.groupby('date')['weight_resp'].mean())).cumprod()\nresp_1  = pd.Series(1+(train_data.groupby('date')['weight_resp_1'].mean())).cumprod()\nresp_2  = pd.Series(1+(train_data.groupby('date')['weight_resp_2'].mean())).cumprod()\nresp_3  = pd.Series(1+(train_data.groupby('date')['weight_resp_3'].mean())).cumprod()\nresp_4  = pd.Series(1+(train_data.groupby('date')['weight_resp_4'].mean())).cumprod()\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Cumulative daily return for resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\nresp.plot(lw=3, label='resp x weight')\nresp_1.plot(lw=3, label='resp_1 x weight')\nresp_2.plot(lw=3, label='resp_2 x weight')\nresp_3.plot(lw=3, label='resp_3 x weight')\nresp_4.plot(lw=3, label='resp_4 x weight')\n# day 85 marker\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\nax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nplt.legend(loc=\"lower left\");","b612c738":"train_data_no_0 = train_data.query('weight > 0').reset_index(drop = True)\ntrain_data_no_0['wAbsResp'] = train_data_no_0['weight'] * (train_data_no_0['resp'])\n#plot\nplt.figure(figsize = (12,5))\nax = sns.distplot(train_data_no_0['wAbsResp'], \n             bins=1500, \n             kde_kws={\"clip\":(-0.02,0.02)}, \n             hist_kws={\"range\":(-0.02,0.02)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of the weights * resp\", size=14)\nplt.show();","26b0e2d1":"trades_per_day = train_data.groupby(['date'])['ts_id'].count()\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.plot(trades_per_day)\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Total number of ts_id for each day\", fontsize=18)\n# day 85 marker\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\nax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\nplt.show()","8a6a4e4f":"fig, ax = plt.subplots(figsize=(15, 5))\nplt.plot(23400\/trades_per_day)\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_ylabel (\"Av. time between trades (s)\", fontsize=18)\nax.set_title (\"Average time between trades for each day\", fontsize=18)\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\nax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\nax.set_ylim(ymin=0)\nax.set_ylim(ymax=12)\nplt.show()","6c8e311b":"plt.figure(figsize = (12,4))\n# the minimum has been set to 1000 so as not to draw the partial days like day 2 and day 294\n# the maximum number of trades per day is 18884\n# I have used 125 bins for the 500 days\nax = sns.distplot(trades_per_day, \n             bins=125, \n             kde_kws={\"clip\":(1000,20000)}, \n             hist_kws={\"range\":(1000,20000)},\n             color='darkcyan', \n             kde=True);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Number of trades per day\", size=14)\nplt.show();","c17dcf4e":"volitile_days = pd.DataFrame(trades_per_day[trades_per_day > 9000])\nvolitile_days.T","d437f43d":"train_data['feature_0'].value_counts()","f2175fad":"fig, ax = plt.subplots(figsize=(15, 4))\nfeature_0 = pd.Series(train_data['feature_0']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"feature_0 (cumulative)\", fontsize=18);\nfeature_0.plot(lw=3);","c805a574":"feature_0_is_plus_one  = train_data.query('feature_0 ==  1').reset_index(drop = True)\nfeature_0_is_minus_one = train_data.query('feature_0 == -1').reset_index(drop = True)\n# the plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\nax1.plot((pd.Series(feature_0_is_plus_one['resp']).cumsum()), lw=3, label='resp')\nax1.plot((pd.Series(feature_0_is_plus_one['resp']*feature_0_is_plus_one['weight']).cumsum()), lw=3, label='return')\nax2.plot((pd.Series(feature_0_is_minus_one['resp']).cumsum()), lw=3, label='resp')\nax2.plot((pd.Series(feature_0_is_minus_one['resp']*feature_0_is_minus_one['weight']).cumsum()), lw=3, label='return')\nax1.set_title (\"feature 0 = 1\", fontsize=18)\nax2.set_title (\"feature 0 = -1\", fontsize=18)\nax1.legend(loc=\"lower left\")\nax2.legend(loc=\"upper left\");\n\ndel feature_0_is_plus_one\ndel feature_0_is_minus_one\ngc.collect();","648179a0":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(20,10))\n\nax1.plot((pd.Series(train_data['feature_1']).cumsum()), lw=3, color='red')\nax1.set_title (\"Linear\", fontsize=22);\nax1.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\nax1.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax1.set_xlim(xmin=0)\nax1.set_ylabel (\"feature_1\", fontsize=18);\n\nax2.plot((pd.Series(train_data['feature_3']).cumsum()), lw=3, color='green')\nax2.set_title (\"Noisy\", fontsize=22);\nax2.axvline(x=514052, linestyle='--', alpha=0.3, c='red', lw=2)\nax2.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax2.set_xlim(xmin=0)\nax2.set_ylabel (\"feature_3\", fontsize=18);\n\nax3.plot((pd.Series(train_data['feature_55']).cumsum()), lw=3, color='darkorange')\nax3.set_title (\"Hybryd (Tag 21)\", fontsize=22);\nax3.set_xlabel (\"Trade\", fontsize=18)\nax3.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\nax3.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax3.set_xlim(xmin=0)\nax3.set_ylabel (\"feature_55\", fontsize=18);\n\nax4.plot((pd.Series(train_data['feature_73']).cumsum()), lw=3, color='blue')\nax4.set_title (\"Negative\", fontsize=22)\nax4.set_xlabel (\"Trade\", fontsize=18)\nax4.set_ylabel (\"feature_73\", fontsize=18);\ngc.collect();","3e592f4d":"day_0 = train_data.loc[train_data['date'] == 0]\nday_1 = train_data.loc[train_data['date'] == 1]\nday_3 = train_data.loc[train_data['date'] == 3]\nthree_days = pd.concat([day_0, day_1, day_3])\nthree_days.plot.scatter(x='ts_id', y='feature_41', s=0.5, figsize=(15,3));\nthree_days.plot.scatter(x='ts_id', y='feature_42', s=0.5, figsize=(15,3));\nthree_days.plot.scatter(x='ts_id', y='feature_43', s=0.5, figsize=(15,3));\ndel day_1\ndel day_3\ngc.collect();","c5578a74":"fig, ax = plt.subplots(1, 3, figsize=(17, 4))\nlag_plot(day_0['feature_41'], lag=1, s=0.5, ax=ax[0])\nlag_plot(day_0['feature_42'], lag=1, s=0.5, ax=ax[1])\nlag_plot(day_0['feature_43'], lag=1, s=0.5, ax=ax[2])\nax[0].title.set_text('feature_41')\nax[0].set_xlabel(\"ts_id (n)\")\nax[0].set_ylabel(\"ts_id (n+1)\")\nax[1].title.set_text('feature_42')\nax[1].set_xlabel(\"ts_id (n)\")\nax[1].set_ylabel(\"ts_id (n+1)\")\nax[2].title.set_text('feature_43')\nax[2].set_xlabel(\"ts_id (n)\")\nax[2].set_ylabel(\"ts_id (n+1)\")\n\nax[0].plot(0, 0, 'r.', markersize=15.0)\nax[1].plot(0, 0, 'r.', markersize=15.0)\nax[2].plot(0, 0, 'r.', markersize=15.0);\ngc.collect();","67e00a23":"three_days.plot.scatter(x='ts_id', y='feature_44', s=0.5, figsize=(15,3));\nthree_days.plot.scatter(x='ts_id', y='feature_45', s=0.5, figsize=(15,3));\ngc.collect();","42953019":"fig, ax = plt.subplots(1, 2, figsize=(15, 4))\nlag_plot(day_0['feature_44'], lag=1, s=0.5, ax=ax[0])\nlag_plot(day_0['feature_45'], lag=1, s=0.5, ax=ax[1])\nax[0].title.set_text('feature_44')\nax[0].set_xlabel(\"ts_id (n)\")\nax[0].set_ylabel(\"ts_id (n+1)\")\nax[1].title.set_text('feature_45')\nax[1].set_xlabel(\"ts_id (n)\")\nax[1].set_ylabel(\"ts_id (n+1)\")\n\nax[0].plot(0, 0, 'r.', markersize=15.0)\nax[1].plot(0, 0, 'r.', markersize=15.0);\ngc.collect();","5e3c9827":"fig, ax = plt.subplots(figsize=(15, 5))\nfeature_60= pd.Series(train_data['feature_60']).cumsum()\nfeature_61= pd.Series(train_data['feature_61']).cumsum()\nfeature_62= pd.Series(train_data['feature_62']).cumsum()\nfeature_63= pd.Series(train_data['feature_63']).cumsum()\nfeature_64= pd.Series(train_data['feature_64']).cumsum()\nfeature_65= pd.Series(train_data['feature_65']).cumsum()\nfeature_66= pd.Series(train_data['feature_66']).cumsum()\nfeature_67= pd.Series(train_data['feature_67']).cumsum()\nfeature_68= pd.Series(train_data['feature_68']).cumsum()\n#feature_69= pd.Series(train_data['feature_69']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative plot for feature_60 ... feature_68 (Tag 22).\", fontsize=18)\nfeature_60.plot(lw=3)\nfeature_61.plot(lw=3)\nfeature_62.plot(lw=3)\nfeature_63.plot(lw=3)\nfeature_64.plot(lw=3)\nfeature_65.plot(lw=3)\nfeature_66.plot(lw=3)\nfeature_67.plot(lw=3)\nfeature_68.plot(lw=3)\n#feature_69.plot(lw=3)\nplt.legend(loc=\"upper left\");\ndel feature_60, feature_61, feature_62, feature_63, feature_64, feature_65, feature_66 ,feature_67, feature_68\ngc.collect();","596f458d":"sns.set_palette(\"bright\")\n\nfig, axes = plt.subplots(2,2,figsize=(8,8))\n\nsns.distplot(train_data[['feature_60']], hist=True, bins=200,  ax=axes[0,0])\nsns.distplot(train_data[['feature_61']], hist=True, bins=200,  ax=axes[0,0])\naxes[0,0].set_title (\"features 60 and 61\", fontsize=18)\naxes[0,0].legend(labels=['60', '61'])\n\nsns.distplot(train_data[['feature_62']], hist=True,  bins=200, ax=axes[0,1])\nsns.distplot(train_data[['feature_63']], hist=True,  bins=200, ax=axes[0,1])\naxes[0,1].set_title (\"features 62 and 63\", fontsize=18)\naxes[0,1].legend(labels=['62', '63'])\n\nsns.distplot(train_data[['feature_65']], hist=True,  bins=200, ax=axes[1,0])\nsns.distplot(train_data[['feature_66']], hist=True,  bins=200, ax=axes[1,0])\naxes[1,0].set_title (\"features 65 and 66\", fontsize=18)\naxes[1,0].legend(labels=['65', '66'])\n\n\nsns.distplot(train_data[['feature_67']], hist=True,  bins=200, ax=axes[1,1])\nsns.distplot(train_data[['feature_68']], hist=True,  bins=200, ax=axes[1,1])\naxes[1,1].set_title (\"features 67 and 68\", fontsize=18)\naxes[1,1].legend(labels=['67', '68'])\n\nplt.show();\ngc.collect();","7ecfad6b":"plt.figure(figsize = (12,5))\nax = sns.distplot(train_data['feature_64'], \n             bins=1200, \n             kde_kws={\"clip\":(-6,6)}, \n             hist_kws={\"range\":(-6,6)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of feature_64\", size=14)\nplt.show();\ndel values\ngc.collect();","0ee6e07c":"day_0 = train_data.loc[train_data['date'] == 0]\nday_1 = train_data.loc[train_data['date'] == 1]\nday_3 = train_data.loc[train_data['date'] == 3]\nthree_days = pd.concat([day_0, day_1, day_3])\n\n# plot\nfig, ax = plt.subplots(2, 1, figsize=(15, 6), sharex=True)\nax[0].scatter(three_days.ts_id, three_days.feature_64, s=0.5, color='b')\nax[0].set_xlabel('')\nax[0].set_ylabel('value')\nax[0].set_title('feature_64 (days 0, 1 and 3)')\nax[1].scatter(three_days.ts_id, pd.Series(three_days['feature_64']).cumsum(), s=0.5, color='r')\nax[1].set_xlabel('ts_id')\nax[1].set_ylabel('cumulative sum')\nax[1].set_title('')\nplt.show();","bf778b49":"x = np.arange(-1,1,0.01) \ny = 2 * np.arcsin(x) +1\nfig, ax = plt.subplots(1, 1, figsize=(7, 4))\nax.plot(x,y, lw=3)\nax.set(xticklabels=[]) \nax.set(yticklabels=['9:00','10:00','11:00','12:00','13:00','14:00','15:00' ,'16:00'])  \nax.set_title(\"2$\\it{arcsin}$(t) +1\", fontsize=18)\nax.set_xlabel (\"'tick' time\", fontsize=18)\nax.set_ylabel (\"Clock time\", fontsize=18)\nplt.show();","47d75d0e":"x_dash = np.arange(-0.98,0.99,0.01) \ny_dash = 2 \/ np.sqrt(1-(x_dash**2))\nfig, ax = plt.subplots(1, 1, figsize=(7, 4))\nax.plot(x_dash,y_dash, lw=3)\nax.set(yticklabels=[])\nax.xaxis.set_ticks(np.arange(-1, 1, 0.28))\n#ax.set(xticklabels=['9:00','10:00','11:00','12:00','13:00','14:00','15:00' ,'16:00'])\nax.set(xticklabels=['9:00','10:00','11:00','12:00','13:00','14:00','15:00' ,'16:00'])  \nax.set_title(\"d\/dt (2$\\it{arcsin}$(t) +1)\", fontsize=18)\nax.set_xlabel (\"Clock time\", fontsize=18)\nax.set_ylabel (\"'tick' frequency\", fontsize=18)\nplt.show();","9096d103":"three_days.plot.scatter(x='ts_id', y='feature_65', s=0.5, figsize=(15,4));","13cfe587":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(16,8))\nax1.set_title (\"features 3 and 4 (+Tag 9)\", fontsize=18);\nax1.plot((pd.Series(train_data['feature_3']).cumsum()), lw=2, color='blue')\nax1.plot((pd.Series(train_data['feature_4']).cumsum()), lw=2, color='red')\n\nax2.set_title (\"features 5 and 6 (+Tag 9)\", fontsize=18);\nax2.plot((pd.Series(train_data['feature_5']).cumsum()), lw=2, color='blue')\nax2.plot((pd.Series(train_data['feature_6']).cumsum()), lw=2, color='red')\n\nax3.set_title (\"features 37 and 38 (+Tag 9)\", fontsize=18);\nax3.plot((pd.Series(train_data['feature_37']).cumsum()), lw=2, color='blue')\nax3.plot((pd.Series(train_data['feature_38']).cumsum()), lw=2, color='red')\nax3.set_xlabel (\"Trade\", fontsize=18)\n\nax4.set_title (\"features 39 and 40 (+Tag 9)\", fontsize=18);\nax4.plot((pd.Series(train_data['feature_39']).cumsum()), lw=2, color='blue')\nax4.plot((pd.Series(train_data['feature_40']).cumsum()), lw=2, color='red')\nax4.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\nax4.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax4.set_xlabel (\"Trade\", fontsize=18)\n#ax4.axvline(x=514052, linestyle='--', alpha=0.3, c='black', lw=1)\n#ax4.axvspan(0,  514052, color=sns.xkcd_rgb['grey'], alpha=0.1);\ngc.collect();","dc8cc470":"fig, ax = plt.subplots(figsize=(15, 4))\nax.scatter(train_data_nonZero.weight, train_data_nonZero.feature_51, s=0.1, color='b')\nax.set_xlabel('weight')\nax.set_ylabel('feature_51')\nplt.show();","8ca44975":"fig, ax = plt.subplots(figsize=(15, 3))\nfeature_0 = pd.Series(train_data['feature_52']).cumsum()\nax.set_xlabel (\"ts_id\", fontsize=18)\nax.set_ylabel (\"feature_52 (cumulative)\", fontsize=12);\nfeature_0.plot(lw=3);","f2994c79":"fig, ax = plt.subplots(1,1, figsize=(4, 4))\nlag_plot(day_0['feature_52'], s=0.5, ax=ax)\nax.title.set_text('feature_52')\nax.set_xlabel(\"ts_id (n)\")\nax.set_ylabel(\"ts_id (n+1)\")\nax.plot(0, 0, 'r.', markersize=15.0);","ca826d22":"fig, ax = plt.subplots(figsize=(15, 4))\nax.scatter(train_data_nonZero.feature_52, train_data_nonZero.resp, s=0.1, color='b')\nax.set_xlabel('feature_52')\nax.set_ylabel('resp')\nplt.show();","dc30d9c5":"fig, ax = plt.subplots(figsize=(15, 5))\nfeature_55= pd.Series(train_data['feature_55']).cumsum()\nfeature_56= pd.Series(train_data['feature_56']).cumsum()\nfeature_57= pd.Series(train_data['feature_57']).cumsum()\nfeature_58= pd.Series(train_data['feature_58']).cumsum()\nfeature_59= pd.Series(train_data['feature_59']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative plot for the 'Tag 21' features (55-59)\", fontsize=18)\nax.axvline(x=514052, linestyle='--', alpha=0.3, c='black', lw=1)\nax.axvspan(0,  514052, color=sns.xkcd_rgb['grey'], alpha=0.1)\nfeature_55.plot(lw=3)\nfeature_56.plot(lw=3)\nfeature_57.plot(lw=3)\nfeature_58.plot(lw=3)\nfeature_59.plot(lw=3)\nplt.legend(loc=\"upper left\");\ngc.collect();","53d38b34":"# Note: I have had to import this png image from another kaggle notebook \n# since producing it took up almost *all* of the notebook memory. Right click to enlarge.\nImage(filename=\"..\/input\/jane-17-plots\/17_plots.png\", width= \"95%\")","e4fabba1":"feature_tags = pd.read_csv(\"..\/input\/jane-street-market-prediction\/features.csv\" ,index_col=0)\n# convert to binary\nfeature_tags = feature_tags*1\n# plot a transposed dataframe\nfeature_tags.T.style.background_gradient(cmap='Oranges')","a5b8ef29":"plt.figure(figsize=(32,14))\nsns.heatmap(feature_tags.T,\n            cbar=False,\n            xticklabels=False,\n            yticklabels=False,\n            cmap=\"Oranges\");","50bebda3":"tag_sum = pd.DataFrame(feature_tags.T.sum(axis=0),columns=['Number of tags'])\ntag_sum.T","04424fe5":"train_data['action'] = ((train_data['resp'])>0)*1","5d31997e":"train_data['action'].value_counts()","d9f3b06c":"daily_action_sum   = train_data['action'].groupby(train_data['date']).sum()\ndaily_action_count = train_data['action'].groupby(train_data['date']).count()\ndaily_ratio        = daily_action_sum\/daily_action_count\n# now plot\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.plot(daily_ratio)\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_ylabel (\"ratio\", fontsize=18)\nax.set_title (\"Daily ratio of action to inaction\", fontsize=18)\nplt.axhline(0.5, linestyle='--', alpha=0.85, c='r');\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\nplt.show();","415fc42a":"daily_ratio_mean = daily_ratio.mean()\nprint('The mean daily ratio is %.3f' % daily_ratio_mean)","e30d8da1":"daily_ratio_max = daily_ratio.max()\nprint('The maximum daily ratio is %.3f' % daily_ratio_max)","927fce1e":"day_0 = train_data.loc[train_data['date'] == 0]","5fa49e5d":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(day_0['resp']).cumsum()\nresp_1= pd.Series(day_0['resp_1']).cumsum()\nresp_2= pd.Series(day_0['resp_2']).cumsum()\nresp_3= pd.Series(day_0['resp_3']).cumsum()\nresp_4= pd.Series(day_0['resp_4']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative values for resp and time horizons 1, 2, 3, and 4 for day 0\", fontsize=18)\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\nplt.legend(loc=\"upper left\");","a4a0d908":"day_0.describe().style.background_gradient(cmap=colorMap)","a48b6258":"msno.matrix(day_0, color=(0.35, 0.35, 0.75));","c0132a6d":"feats_7_11 = day_0.iloc[:, [14,18]]\nmsno.matrix(feats_7_11, color=(0.35, 0.35, 0.75), width_ratios=(1, 3));","a34f1d77":"#missing_data = pd.DataFrame(train_data.isna().sum().sort_values(ascending=False),columns=['Total missing'])\n#missing_data.T\n\ngone = train_data.isnull().sum()\npx.bar(gone, color=gone.values, title=\"Total number of missing values for each column\").show()","a11e347f":"missing_features = train_data.iloc[:,7:137].isnull().sum(axis=1).groupby(train_data['date']).sum().to_frame()\n# now make a plot\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.plot(missing_features)\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Total number of missing values in all features for each day\", fontsize=18)\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=2)\nax.axvspan(0,  85, color=sns.xkcd_rgb['grey'], alpha=0.1)\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\nplt.show()","689368ba":"count_weights    = train_data[['date', 'weight']].groupby('date').agg(['count'])\nresult = pd.merge(count_weights, missing_features, on = \"date\", how = \"inner\")\nresult.columns = ['weights','missing']\nresult['ratio'] = result['missing']\/result['weights']\nmissing_per_trade = result['ratio'].mean()\n\n# now make a plot\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.plot(result['ratio'])\nplt.axhline(missing_per_trade, linestyle='--', alpha=0.85, c='r');\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Average number of missing feature values per trade, for each day\", fontsize=18)\nplt.show()","8616b7c7":"day_1 = train_data.loc[train_data['date'] == 1]\nday_2 = train_data.loc[train_data['date'] == 2]\nday_3 = train_data.loc[train_data['date'] == 3]\nthree_days = pd.concat([day_1, day_2, day_3])\n\n#td = three_days.plot.scatter(x='ts_id', y='feature_64', s=0.5, figsize=(15,4), color='blue')\n#day_2.plot.scatter(x='ts_id', y='feature_64', s=0.5, figsize=(15,4), color='red', ax=td);\n\nfig, ax = plt.subplots(figsize=(15, 3))\nax.scatter(three_days.ts_id, three_days.feature_64, s=0.5, color='b')\nax.scatter(day_2.ts_id, day_2.feature_64, s=0.5, color='r')\nax.scatter(15150, 5.2, s=1800, facecolors='none', edgecolors='black', linestyle='--', lw=2)\nax.set_xlabel('feature_64')\nax.set_ylabel('ts_id')\nax.set_title('feature_64 for days 1, 2 and 3')\nplt.show();","574a9bf2":"dabl.plot(day_0, target_col=\"action\")","ab59865f":"dabl.plot(day_0, target_col=\"resp\")","294ed5b9":"fig_1 = px.scatter(day_0, x=day_0['ts_id'], y=day_0['resp'], \n                   trendline=\"ols\", marginal_y=\"violin\",\n                   title=(\"Scatter plot of resp with respect to ts_id for day 0\"))\nfig_1.show()","1dfd861e":"X_train = day_0.loc[:, day_0.columns.str.contains('feature')]\nX_train = X_train.fillna(X_train.mean())\n# our target is the action\ny_train = day_0['resp']\n\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(max_features='auto')\nregressor.fit(X_train, y_train)","721c8123":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm_import = PermutationImportance(regressor, random_state=1).fit(X_train, y_train)\n# visualize the results\neli5.show_weights(perm_import, top=15, feature_names = X_train.columns.tolist())","87c28989":"day_100  = train_data.loc[train_data['date'] == 100]\nday_200  = train_data.loc[train_data['date'] == 200]\nday_100_and_200 = pd.concat([day_100, day_200])\nday_100_and_200.corr(method='pearson').style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)","c02089f1":"subset = day_100_and_200[[\"feature_120\",\"feature_121\",\"feature_122\",\"feature_123\",\"feature_124\",\"feature_125\",\"feature_126\",\"feature_127\",\"feature_128\",\"feature_129\"]]\nsubset.corr(method='pearson').style.background_gradient(cmap='coolwarm', low=1, high=0, axis=None).set_precision(2)","f344096e":"features_day_100   = day_100.iloc[:,7:137]\nfeatures_day_200   = day_200.iloc[:,7:137]\nfeatures_100_and_200 = pd.concat([features_day_100, features_day_200])\n\n# code from: https:\/\/izziswift.com\/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\/\ndef corrFilter(x: pd.DataFrame, bound: float):\n    xCorr = x.corr()\n    xFiltered = xCorr[((xCorr >= bound) | (xCorr <= -bound)) & (xCorr !=1.000)]\n    xFlattened = xFiltered.unstack().sort_values().drop_duplicates()\n    return xFlattened\n\ncorrFilter(features_100_and_200, .992).to_frame()","67e55534":"We can see a correlation of only 0.54 between our simple definition of `action` and the value of `resp`. It is interesting to note that no one single feature shows a strong correlation to `resp`.\n\nIt has been mentioned that the [Spearman's rank correlation coefficient](https:\/\/en.wikipedia.org\/wiki\/Spearman%27s_rank_correlation_coefficient) is more apropo for financial data. One can simply change `method='pearson'` to `method='spearman'` in the [pandas.DataFrame.corr](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.corr.html) to see such correlations.\n\n### Tag 28 section\nNavigating around there do appear to be some rather curious regions, for example between features 120 through to 129:","93ee1a6a":"Also, `feature_0` is the *only* feature in the `features.csv` file that has no `True` tags.","4b60095c":"with the following lag plots","d5b6df4f":"graphically we can see that indeed there are chunks of missing data (in white) in some of the columns, and there appears to be a pattern. Let us take a look only at `feature_7` (the first `resp_1` feature) and `feature_11` (the first `resp_2` feature): ","cf4d9cf6":"Just for fun let us re-plot the above data, but now in '8-bit' mode; totally illegible, but may perhaps serve as an overall visual aid...","5d0efaed":"<a class=\"anchor\" id=\"missing_values\"><\/a>\n## <center style=\"background-color:Gainsboro; width:60%;\">Are there any missing values?<\/center>\n\nTo start with let us look at day 0 ","8ac16513":"### Tag 18 features: 44 (+ tag 15) and 45 (+ tag 17)\nThese are similar to the Tag 14 features seen above, but are now much more centred around 0","1e937d10":"### `feature_52` (Tag 19)","d1c39804":"as well as four [time horizons](https:\/\/www.investopedia.com\/terms\/t\/timehorizon.asp)\n> \"*The longer the Time Horizon, the more aggressive, or riskier portfolio, an investor can build. The shorter the Time Horizon, the more conservative, or less risky, the investor may want to adopt.*\"","d1fe278a":"Could these represent offer prices, and those with with **Tag 9** bid prices? That said, we can see that after day 85 the value of `feature_40` actually becomes greater than the value of `feature_39`.\n\n### `feature_51` (Tag 19)\nIn the Topic [\"*Weight and feature_51 de-anonymized*\"](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/202014) by [marketneutral](https:\/\/www.kaggle.com\/marketneutral) it is suggested that `feature_51` is the (log of) the average daily volume of the stock.\nHere I reproduce the plot of `feature_51` w.r.t. `weight` for non-zero weights:","b8ba4b10":"![image.png](attachment:image.png)\n\n# Jane Street Market Prediction: A simple EDA\n\n> \"*Machine learning (ML) at Jane Street begins, unsurprisingly, with data. We collect and store around 2.3TB of market data every day. Hidden in those petabytes of data are the relationships and statistical regularities which inform the models inside our strategies. But it\u2019s not just awesome models. ML work in a production environment like Jane Street\u2019s involves many interconnected pieces.*\" -- [Jane Street Tech Blog \"*Real world machine learning*\"](https:\/\/blog.janestreet.com\/real-world-machine-learning-part-1\/).\n\nThis notebook is a simple exploratory data analysis (EDA) of the files provided for the kaggle [Jane Street Market Prediction](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction) competition. Here we shall...\n\n> \"**Explore the data:** *It\u2019s hard to know what techniques to throw at a problem before we understand what the data looks like, and indeed figure out what data to use. Spending the time to visualize and understand the structure of the problem helps pick the right modeling tools for the job. Plus, pretty plots are catnip to traders and researchers!*\"\n\n## <center style=\"background-color:Gainsboro; width:40%;\">Contents<\/center>\n* [The train.csv file is big](#train_csv)\n* [resp](#resp)\n* [weight](#weight)\n* [Cumulative return](#return)\n* [Time](#time)\n* [The features](#features)\n* [The `features.csv` file](#features_file)\n* [Action](#action)\n* [The first day (\"day 0\")](#day_0)\n* [Are there any missing values?](#missing_values)\n* [Is there any missing data: Days 2 and 294](#missing_data)\n* [DABL plots (targets: action and resp)](#DABL)\n* [Permutation Importance using the Random Forest](#permutation)\n* [Is there any correlation between day 100 and day 200?](#Pearson)\n* [The test data](#test_data)\n* [Evaluation](#evaluation)","2db92733":"Here is a histogram of the number of trades per day (it has been [suggested](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/201930#1125847) that the number of trades per day is an indication of the [volatility](https:\/\/www.investopedia.com\/terms\/v\/volatility.asp) that day)","80fec78d":"we can see that with the above formula overall we are very slightly more proactive (0.4%) than inactive. How does this look daily?","2ba3b12b":"We can see that `resp` (in blue) most closely follows time horizon 4 (`resp_4` is the uppermost curve, in purple). \n\nIn the notebook [\"*Jane Street: time horizons and volatilities*\"](https:\/\/www.kaggle.com\/pcarta\/jane-street-time-horizons-and-volatilities) written by [pcarta](pcarta), if I understand correctly, by using [maximum likelihood estimation](https:\/\/en.wikipedia.org\/wiki\/Maximum_likelihood_estimation) it is calculated that if the time horizon $(T_j$) for `resp_1` (*i.e.* $T_1$) is 1, then \n* $T_j($ `resp_2` $) ~\\approx 1.4 ~T_1$\n* $T_j($ `resp_3` $) ~\\approx 3.9 ~T_1$ \n* $T_j($ `resp_4` $) ~\\approx 11.1 ~T_1$\n\nwhere $T_1$ could correspond to 5 trading days.\n\nLet us now plot a histogram of all of the `resp` values (here only shown for values between -0.05 and 0.05)","288bc94e":"We can see that the daily action is fairly consistent; no obvious weekly\/monthly\/seasonal changes *etc*.","330c3a7c":"and the following curious relationship with `resp`","0199af97":"We can see that the shortest time horizons, `resp_1`, `resp_2` and `resp_3`, representing a more conservative strategy, result in the lowest return.\n\nWe shall now plot a histogram of the `weight` multiplied by the value of `resp` (after removing the 0 weights)","d658cc6e":"We now have loaded `train.csv` in less than 17 seconds.\n\n<a class=\"anchor\" id=\"return\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">resp<\/center>\n\nThere are a total of 500 days of data in `train.csv` (*i.e.* two years of trading data). Let us take a look at the cumulative values of `resp` over time","b692b5c3":"and we can now try to fit a pair of Gaussian functions to this distribution","30b4a5de":"The same goes for **day 294**, which has only 29 `ts_id`.\nThis would also explain why days 2 and 294 have none of the missing values that we usually find during breakfast and lunch the other days.\nIt is possibly worth treating these two days as outliers and dropping them. \n\n\n<a class=\"anchor\" id=\"DABL\"><\/a>\n## <center style=\"background-color:Gainsboro; width:20%;\">DABL plots<\/center>\n\nLet us run **day 0** through the *data analysis baseline library* [dabl](https:\/\/github.com\/amueller\/dabl). First using the `action` as the target:","042b7205":"and in between them is `feature_64`","ff3f1960":"### Plot of `resp` values with respect to time (`ts_id`) for day 0","7f8d141c":"with limited success; the narrower left hand peak seems to be some other distribution. (Just in case, the $\\mu$ of the small Gaussian is located at -1.32, and the large Gaussian at 0.4).\n<a class=\"anchor\" id=\"return\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">Cumulative return<\/center>\n\nLet us take a look at the cumulative daily return over time, which is given by `weight` multiplied by the value of `resp`","da76f340":"What if these are associated with the five `resp` values? Perhaps: \n* `feature_55` is related to `resp_1`\n* `feature_56` is related to `resp_4` \n* `feature_57` is related to `resp_2` \n* `feature_58` is related to `resp_3` \n* `feature_59` is related to `resp`\n\nIf that *is* the case then \n* **Tag 0** represents `resp_4` features\n* **Tag 1** represents `resp` features\n* **Tag 2** represents `resp_3` features\n* **Tag 3** represents `resp_2` features\n* **Tag 4** represents `resp_1` features\n\n*i.e.*\n* `resp_1` related features: 7, 8, 17, 18, 27, 28, 55, 72, 78, 84, 90, 96, 102, 108, 114, 120, and 121 <font color=\"red\">(Note: 79.6% of all of the missing data is found within this set of features).<\/font>\n* `resp_2` related features: 11, 12, 21, 22, 31, 32, 57, 74, 80, 86, 92, 98, 104, 110, 116, 124, and 125 <font color=\"red\">(Note: 15.2% of all of the missing data is found within this set of features).<\/font>\n* `resp_3` related features: 13, 14, 23, 24, 33, 34, 58, 75, 81, 87, 93, 99, 105, 111, 117, 126, and 127\n* `resp_4` related features: 9, 10, 19, 20, 29, 30, 56, 73, 79, 85, 91, 97, 103, 109, 115, 122, and 123\n* `resp` related features: 15, 16, 25, 26, 35, 36, 59, 76, 82, 88, 94, 100, 106, 112, 118, 128, and 129\n\nLet us take a look at a plot of each of these 17 features for each of the resp (Note: This is an image; right click to view and enlarge)","8f3d0813":"Let us take a look at a histogram of the non-zero weights","a5806e67":"We can see in this very simple example that for the first day (**day 0**) the top 5 most important features appear to be 39, 43, 37, 5 and 42.\n\nNote:\n\n> Features that are deemed of low importance for a bad model (low cross-validation score) could be very important for a good model. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model. (Source: [scikit-learn permutation importance](https:\/\/scikit-learn.org\/stable\/modules\/permutation_importance.html)).\n\nIt goes without saying that a serious study of the feature importance is essential (and will use *a lot* of CPU). For a much more advanced approach may I suggest taking a look at [\"*Feature selection using the Boruta-SHAP package*\"](https:\/\/www.kaggle.com\/carlmcbrideellis\/feature-selection-using-the-boruta-shap-package). \n\nHowever, the global feature importance ranking does not tell the whole story, and in the notebook [\"*TabNet and interpretability: Jane Street example*\"](https:\/\/www.kaggle.com\/carlmcbrideellis\/tabnet-and-interpretability-jane-street-example) thanks to [TabNet](https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-tabnet-3-0-0-starter-notebook) one can inspect which features were important for each and every calculation, and one can see that the process is much more dynamic than a static overall ranking would suggest.\n\n<a class=\"anchor\" id=\"Pearson\"><\/a>\n## <center style=\"background-color:Gainsboro; width:90%;\">Is there any correlation between day 100 and day 200?<\/center>\nAre the days independent? For the moment let us take a look at day(100) and day(200) using a [Pearson pairwise correlation](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient) matrix (this is a **big** matrix!). Why days 100 and 200? Because they are far apart in time, thus reducing any temporal leakage.\nWe shall use a [diverging colormap](https:\/\/matplotlib.org\/3.1.0\/tutorials\/colors\/colormaps.html) where red indicates positive linear correlation, and blue indicates linear anti-correlation:","325234cf":"An now to find the maximum weight used","3682b2ea":"and then [convert to a pandas dataframe](https:\/\/datatable.readthedocs.io\/en\/latest\/api\/frame\/to_pandas.html)","bd848379":"which occured on day 446","f6453316":"This distribution has very long tails","bbae6fd1":"These are all **Tag 28** features. \n* `feature_120` and `feature_121` both also have **Tag 4** which I suggest are related to `resp_1`\n* `feature_122` and `feature_123` both also have **Tag 0** which I suggest are related to `resp_4`\n* `feature_124` and `feature_125` both also have **Tag 3** which I suggest are related to `resp_2`\n* `feature_126` and `feature_127` both also have **Tag 2** which I suggest are related to `resp_3`\n* `feature_128` and `feature_129` both also have **Tag 1** which I suggest are related to `resp`\n\nfrom this we can see that (for days 100 and 200) for the Tag 28 features between\n* `resp` and `resp_4` there is a linear correlation of 0.98\n* `resp` and `resp_3` there is a linear correlation of 0.97\n* `resp` and `resp_2` there is a linear correlation of 0.94\n* `resp` and `resp_1` there is a linear correlation of 0.89\n\n\n\n### High correlations\nWe shall now find the pairs of features with a correlation > |0.992|:","37456890":"Note that a Cauchy distribution can be generated from the ratio of two independent normally distributed random variables with mean zero. The paper by [David E. Harris \"*The Distribution of Returns*\"](https:\/\/www.scirp.org\/pdf\/JMF_2017083015172459.pdf) goes into detail regarding the use of a Cauchy distribution to model returns.\n\n<a class=\"anchor\" id=\"weight\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">weight<\/center>\n\n> *Each trade has an associated `weight` and `resp`, which together represents a return on the trade.\nTrades with `weight = 0` were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.*","82d11c30":"It is interesting to note that almost all of the days having a large volume of trades are before and up to day 85.\n\nAlso related to time is `feature_64` which seems to be some sort of daily clock as we shall see below.\n\n<a class=\"anchor\" id=\"features\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">The features<\/center>\n\n> \"*This dataset contains an anonymized set of features, `feature_{0...129}`, representing real stock market data.*\"\n\n### feature_0\nFirst of all, `feature_0` seems to be a little unusual, as it is composed solely of the integers `+1` or `-1`:","7efdec39":"Let us see if there are any negative weights. A negative weight would be meaningless, but you never know...","cb62477c":"(vertical dashed line represents day 85)\n\nWe are told that `resp_1`, `resp_2`, `resp_3` and `resp_4`  [*... are provided just in case some people want some alternative objective metrics to regularize their model training*](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/198965#1088950). If we do not intend to do this, then we can probably drop all these 4 x 17 = 68 features, and just keep the `resp` related features. \n\n## <center style=\"background-color:Gainsboro; width:40%;\">t-SNE plots<\/center>\n\nI have made some t-distributed stochastic neighbor embedding (t-SNE) plots for some of the feature groups in a separate notebook [\"*Jane Street: t-SNE using RAPIDS cuML*\"](https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-t-sne-using-rapids-cuml) as they take a long time to calculate.\n\n<a class=\"anchor\" id=\"features_file\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">The features.csv file<\/center>\n\nWe are also provided with a `features.csv` file which contains \"*metadata pertaining to the anonymized features*\". Let us take a quick look at it, where `1` is `True` and `0` is `False`. The file has 29 \"tags\" associated with each feature.","84fe1e4e":"<a class=\"anchor\" id=\"test_data\"><\/a>\n# <center style=\"background-color:Gainsboro; width:40%;\">The test data<\/center>\nThe wall time taken for a submission to return a score in this competition is around 3\u00bd hours, so testing a script before submitting is (as always) *very* important. We are provided with some test data in the smaller (36 MB) file `example_test.csv`. This file contains over 15k rows, and covers three days of data. It consists of the 130 `features` as in the `train.csv`, as well as the `weight` for each trade. It is interesting to note that `example_test.csv` contains no `resp` data.\n\nThe three days of data in the test file correspond to day 0, day 1 and day 2. Note that day 2 only contains data from the very end of the day, as with day 2 in the `train.csv` file, so should be treated with caution.\n\n<a class=\"anchor\" id=\"evaluation\"><\/a>\n# <center style=\"background-color:Gainsboro; width:40%;\">Evaluation<\/center>\nThis competition is evaluated via a utility score $(u)$. For a detailed description of this score and its calculation see the excellent notebooks:\n* [\"*Understanding the Utility Score Function*\"](https:\/\/www.kaggle.com\/renataghisloti\/understanding-the-utility-score-function) written by [Renata Ghisloti Duarte de Souza](https:\/\/www.kaggle.com\/renataghisloti)\n* [\"*Utility Function and Patterns in Missing Values*\"](https:\/\/www.kaggle.com\/iamleonie\/utility-function-and-patterns-in-missing-values) written by [Leonie](https:\/\/www.kaggle.com\/iamleonie)\n* [\"*Jane Street: Super Fast Utility Score Function*\"](https:\/\/www.kaggle.com\/gogo827jz\/jane-street-super-fast-utility-score-function) written by [Yirun Zhang](https:\/\/www.kaggle.com\/gogo827jz)\n\nWhen it comes to 'off-line' evaluation with cross-validation may I also suggest the notebooks \n* [\"*Found the Holy Grail: GroupTimeSeriesSplit*\"](https:\/\/www.kaggle.com\/jorijnsmit\/found-the-holy-grail-grouptimeseriessplit) written by [Jorijn Jacko Smit](https:\/\/www.kaggle.com\/jorijnsmit)\n* [\"*Purged Rolling Time Series CV Split*\"](https:\/\/www.kaggle.com\/marketneutral\/purged-rolling-time-series-cv-split) written by [marketneutral](https:\/\/www.kaggle.com\/marketneutral)\n\n### <center style=\"background-color:LightGreen; width:40%;\">Thank you for your time.<\/center>\n\nPostscript:\n\n> \"*If you have been asked to develop ML strategies on your own, the odds are stacked\nagainst you. It takes almost as much effort to produce one true investment strategy\nas to produce a hundred, and the complexities are overwhelming*\". ([Marcos Lopez de Prado in \"*Advances in Financial Machine Learning*\"](https:\/\/www.wiley.com\/en-es\/Advances+in+Financial+Machine+Learning-p-9781119482109))\n\n> \"*...It looks just a little more mathematical and regular than it is; its exactitude is obvious, but its inexactitude is hidden; its wildness lies in wait.*\" (G. K. Chesterton (1908))\n\n### <center style=\"background-color:LightGreen; width:40%;\">Good luck to everyone!<\/center>\n","bfdd0c15":"We can see that the classes 0 and 1 for `action` are reasonably well balanced.\n\nNow we shall use `resp` as the target:","7b765500":"### <center style=\"background-color:Gainsboro; width:80%;\">Descriptive statistics of the `train.csv` file for day 0<\/center>\nSome simple [descriptive statistics](https:\/\/en.wikipedia.org\/wiki\/Descriptive_statistics) of the day 0 data:","2360c677":"<a class=\"anchor\" id=\"train_csv\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">The train.csv file is big<\/center>\n\nThe train.csv is large: 5.77G. Let us see just how many rows it has:","bd581379":"### 'Linear' features\n* 1 \n* 7, 9, 11, 13, 15\n* 17, 19, 21, 23, 25\n* 18,  20,  22,  24, 26\n* 27, 29, 21, 33, 35\n* 28, 30, 32, 34, 36\n* 84, 85, 86, 87, 88\n* 90, 91, 92, 93, 94\n* 96, 97, 98, 99, 100\n* 102 (strong change in gradient), 103, 104, 105, 106\n\nas well as\n41, 46, 47, 48, 49, 50, 51, 53, 54, 69, 89, 95 (strong change in gradient), 101, 107 (strong change in gradient), 108, 110, 111, 113, 114, 115, 116, 117, 118, 119 (strong change in gradient), 120, 122, and 124.\n### Features 41, 42 and 43 (Tag 14)\nThe **Tag 14** set are interesting as they appear to be \"stratified\"; only adopting discrete values throughout the day (could these be a value of a [security](https:\/\/en.wikipedia.org\/wiki\/Security_(finance%29)?).\nHere are scatter plots of these three features for days 0, 1 and 3 (Note that I have omitted day 2, which I shall discuss in the *missing data* section below):","ba940028":"It is also very interesting to plot the cumulative `resp` and return (`resp`\\*`weight`) for `feature_0 = +1` and `feature_0 = -1` individually (Credit: [\"*An observation about feature_0*\"](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/204963) by [therocket290](https:\/\/www.kaggle.com\/therocket290))","e2a2aa5d":"If this were so, then perhaps the period of missing values seen at the start of the day for some of the  features (see the section below on missing values) is actually similar to the period of missing values seen during the middle of the day? Also perhaps the higher tick frequency at the beginning and end of the day is due to a lot of buying when the day opens, and a lot of selling towards the close of the day so as to have no significant position overnight?\n\nIt was first suggested (if I am not mistaken) by [marketneutral](https:\/\/www.kaggle.com\/marketneutral) in a [post](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/201264#1101507) that the data *may* correspond to equities traded on the [Tokyo Stock Exchange](https:\/\/www.jpx.co.jp\/english\/derivatives\/rules\/trading-hours\/index.html), whose trading hours are from 9:00 until 11:30, a break for lunch, and then from 12:30 until 15:00. This could explain the central discontinuity in the Tag 22 features.\n\nWe shall now also look at `feature 65`:","c9bf30e9":"where, for some reason, the tick time is more frequent at the start and end of the day than in the middle. Also for fun let us plot the hypothetical tick frequency, *i.e.*\n\n$$ \\frac{d}{dt} (2 \\arcsin(t) +1) = \\frac{2}{\\sqrt{1-t^2}}$$","d901cf64":"having the following lag plot","623a8586":"We can see that on average there are $\\approx$ 3 missing feature values per trade, per day, except for two spikes located on days 2 and 294 where there are no missing values at all. (The most missing values are on day 14).\n\nThis raises the question of [what to do with missing data in the unseen test data?](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/200691). Whatever one decides to do, in this competition time is of the essence, so we have to do it fast, and [Yirun Zhang](https:\/\/www.kaggle.com\/gogo827jz) has made an exhaustive study of the time taken in various filling methods in the notebook [\"*Optimise Speed of Filling-NaN Function*\"](https:\/\/www.kaggle.com\/gogo827jz\/optimise-speed-of-filling-nan-function).\n\n<a class=\"anchor\" id=\"missing_data\"><\/a>\n## <center style=\"background-color:Gainsboro; width:80%;\">Is there any missing data: Days 2 and 294<\/center>\nIf we produce scatter plots of `feature_64` we see that each day has the same sweeping pattern. However we see that **day 2** has only 231 `ts_id`  which all seem to originate from the very end of the day. Here is a plot of day 1 (in blue), day 2 (in red) and day 3 (blue again). Day 2 has been encircled as a visual aid.","76fdacbb":"The global minimum value of `feature_64` is \\\\( \\approx -6.4 \\\\) and the global maximum value is \\\\( \\approx 8 \\\\) (not all days reach these limits). It is curious that a trading day on the New York Stock Exchange spans from 9:30 until 16:00. What if the units of `feature_64` were \\\\( \\approx 30 \\\\) minutes, and `feature_64 = 0` corresponds to 12:00? Just for fun let us make a plot of the *arcsin* function, renaming the *y*-axis as the hours of the day...","d1ac76a5":"Let us sum the number of tags for each feature:","45b6882a":"which has a big gap for values in the range 0.7 to 1.38. (Incidentally,  $\\ln(2) \\approx 0.693...$ and $\\ln(4) \\approx 1.386...$, I do not know if there is any significance to this at all).\n\nThe **Tag 22** features also have a very interesting daily pattern. For example, here are scatter and cumulative plots over three days for feature 64","e5cdb235":"Indeed `feature_60` and `feature_61` (both having Tags 22 & 12) are virtually coincident. \nThe same goes for `feature_62` and `feature_63` (both having Tags 22 & 13), \n`feature_65` and `feature_66` (both having Tags 22 & 12) and\n`feature_67` and `feature_68` (both having Tags 22 & 13). Let us plot these features as distributions","a4de364a":"which occurred on day **294** (we shall hear more about day 294 in the missing data section below).\n\nObviously the above is a very simplistic target for such a complex set of data. For a more detailed look my I suggest the notebook [\"*Target Engineering; CV; \u26a1 Multi-Target*\"](https:\/\/www.kaggle.com\/marketneutral\/target-engineering-cv-multi-target) written by [marketneutral](https:\/\/www.kaggle.com\/marketneutral), where other options are explored.\n<a class=\"anchor\" id=\"day_0\"><\/a>\n## <center style=\"background-color:Gainsboro; width:90%;\">Now let us take a look at the first day (\"day 0\")<\/center>\nTo do this we shall make a new dataframe called `day_0`","63dcbb3c":"For a very interesting look at the Tag 22 features see the notebook [\"*Important and Hidden Temporal Data*\"](https:\/\/www.kaggle.com\/lachlansuter\/important-and-hidden-temporal-data) written by [Lachlan Suter](https:\/\/www.kaggle.com\/lachlansuter).\n### 'Noisy' features\n* 3, 4, 5, 6\n* 8, 10, 12, 14, 16\n* 37, 38, 39, 40\n* 72, 73, 74, 75, 76\n* 78, 79, 80, 81, 82\n* 83\n\nHere are cumulative plots of some of these features","601adf63":"<a class=\"anchor\" id=\"time\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">Time<\/center>\nLet us plot the number of `ts_id` per day. Note: I have taken to drawing a vertical dashed line in my plots because I started to wonder [did Jane Street modify their trading model around day 85?](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/201930) Thanks to comments on that forum the general consenus seems to be that a change in the market took place around that time (perhaps a mean reverting market changing to a momentum market, or *vice versa*).","47d51ec2":"These three features also have very interesting lag plots, where we plot the value of the feature at `ts_id` $(n)$ with respect to the next value of the feature, *i.e.*  at `ts_id` $(n+1)$, (here for day 0). Red markers have been placed at (0,0) as a visual aid.","20bec8fb":"we can see that the missing data does not appear to be random, indeed there appear to be two big chunks missing at the start and in the middle of each column. Let us assume that a [trading day](https:\/\/en.wikipedia.org\/wiki\/Trading_day) spans from 9:30 until 16:00. Let us also assume that the trades occur at regular intervals (which is almost certainly *not* the case) then `feature_7` has chunks of missing data from 9:30 until 10:03, and is missing \u224816 minutes from 13:17 until 13:33. `feature_11` has missing data from 9:30 until 9:35, and is missing \u22485\u00bd minutes from 13:17 until 13:22.\n\nNow let us look at the sum of the number of missing data in each column for the whole `train.csv` file:","06ef6b7e":"### Features 60 to 68 (Tag 22)\nWe have the **Tag 22** set:\n* 60, 61, 62, 63, 64, 65, 66, 67, 68","78bc020b":"It can be seen that \"+1\" and the \"-1\" projections describe very different return dynamics.\nIn the notebook [\"*Feature 0, beyond feature 0*\"](https:\/\/www.kaggle.com\/nanomathias\/feature-0-beyond-feature-0) written by [NanoMathias](https:\/\/www.kaggle.com\/nanomathias) a [uniform manifold approximation and projection (UMAP)](https:\/\/arxiv.org\/abs\/1802.03426) is performed and shows that `feature_0`  effectively classifies two distributions of features.\nThere have been many suggestions made regarding the nature of this feature on the discussion topic [\"*What is \"feature_0\" ?*\"](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/199462) such as `feature_0` representing the direction of the trade or things like bid\/ask, long\/short, or call\/put.\n\nOne possibility is that `feature_0` represents something similar to the [Lee and Ready 'Tick' model](https:\/\/onlinelibrary.wiley.com\/doi\/epdf\/10.1111\/j.1540-6261.1991.tb02683.x) for classifying individual trades as market buy or market sell orders, using intraday trade and quote data.\nA buy initiated trade is labeled as \"1\", and a sell-initiated trade is labeled as \"-1\" (*Source*: \u00a7 19.3.1 of [\"*Advances in Financial Machine Learning*\"](https:\/\/www.wiley.com\/en-es\/Advances+in+Financial+Machine+Learning-p-9781119482109) by Marcos Lopez de Prado)\n\n$$\nb_t = \n\\begin{cases} \n  1  & \\mbox{if }\\Delta p_t > 0\\\\\n  -1 & \\mbox{if }\\Delta p_t < 0\\\\\n  b_{t-1} & \\mbox{if }\\Delta p_t = 0\n\\end{cases}\n$$\n\nwhere $p_t$ is the price of the trade indexed by $t = 1,\\ldots , T$, and $b_0$ is arbitrarily set to\n1.\n\nIf we look at the correlation matrix (see below) it can be seen that there is a strong positive correlation between `feature_0` and the **Tag 12** features, a strong negative correlation with the **Tag 13** features. There is also a negative correlation with the **Tag 25** and **Tag 27** features, and a positive correlation with  the **Tag 24** features.\n\nOther than features 37, 38, 39 and 40 all of the above features are `resp` related features (see below) with the strongest correlation being with the `resp_4` features.\n\n### feature_{1...129}\nThere seem to be four general 'types' of features, here is a plot of an example of one of each:","faf31786":"There appear to be two peaks, one situated at `weight` $\\approx$ 0.17, and a lower, broader peak at `weight` $\\approx$ 0.34. Could this be indicative of two underlying distributions that we see here, superimposed on each other? Maybe one distribution of weights correspond to selling, and the other to buying?\n\nWe can plot the logarithm of the weights (*Credit*: [\"*Target Engineering; CV; \u26a1 Multi-Target*\"](https:\/\/www.kaggle.com\/marketneutral\/target-engineering-cv-multi-target) by [marketneutral](https:\/\/www.kaggle.com\/marketneutral))","72f5d622":"Finally, let us fit a [Cauchy distribution](https:\/\/en.wikipedia.org\/wiki\/Cauchy_distribution) to this data","fdaa7f3d":"Let us now compare the overall action to inaction","beb560e4":"If we assume a [trading day](https:\/\/en.wikipedia.org\/wiki\/Trading_day) is 6\u00bd hours long (*i.e.* 23400 seconds) then","9bbbd5b8":"### 'Negative' features\nFeatures 73, 75, 76, 77 (noisy), 79, 81(noisy), 82. These are all found in the **Tag 23** section.\n\n### 'Hybrid' features (Tag 21): \n55, 56, 57, 58, 59.\n\nThese start off noisy, with prominent almost discontinuous steps around the 0.2M, 0.5M, and 0.8M trade marks, then go linear. These five features form the \"**Tag 21**\" set:","6490e828":"<a class=\"anchor\" id=\"permutation\"><\/a>\n## <center style=\"background-color:Gainsboro; width:100%;\">Very quick Permutation Importance using the Random Forest<\/center>\nWe shall now perform a simple [permutation importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance) calculation, a basic way of seeing which features may be important. We shall perform a regression, with `resp` as the target.","8fb10290":"We can see that all of the features have at least one tag, and some as many a four. All except, that is, for `feature_0`, which has no tags at all.\nThere seem to be 5 (ish) regions that seem different to each other:\n\n| '*Region*' | features | Tags | <font color=\"red\">missing values?<\/font> | observations |\n| :--- | :--- | :--- | :--- | :--- |\n| 0 | feature_0 | none | none | -1 or +1 |\n| 1  | 1...6 | **Tag 6** |  |\n| 2  | 7-36  | **Tag 6** |  |\n| 2a |  7..16 | + 11 | <font color=\"red\">7, 8 and 11, 12<\/font> | |\n| 2b | 17...26 | + 12 | <font color=\"red\">17, 18 and 21, 22 <\/font>| |\n| 2c | 27...36 | + 13 | <font color=\"red\">27, 28 and 31, 32 <\/font>| |\n| 3  | 37...72 | various | |\n| 3a | 55...59 | Tag 21 | | All hybrid |\n| 3b | 60...68 | Tag 22 | | Clock + time features? |\n| 4  | 72-119 | **Tag 23** | |\n| 4a | 72...77  |  + 15 & 27 | <font color=\"red\"> 72 and 74<\/font> | |\n| 4b | 78...83  |  + 17 & 27 |<font color=\"red\"> 78 and 80<\/font>| |\n| 4c | 84...89  |  + 15 & 25 | <font color=\"red\"> 84 and 86<\/font>| |\n| 4d | 90...95  |  + 17 & 25 | <font color=\"red\"> 90 and 92<\/font>| |\n| 4e | 96...101 |  + 15 & 24 | <font color=\"red\"> 96 and 98<\/font>| |\n| 4f | 102...107 | + 17 & 24 | <font color=\"red\"> 102 and 104<\/font>| |\n| 4g | 108...113 | + 15 & 26 | <font color=\"red\"> 108 and 110<\/font>| |\n| 4h | 114...119 | + 17 & 26 | <font color=\"red\"> 114 and 116<\/font>| |\n| 5  | 120...129 |**Tag 28** | |\n| 5a | 120 |+ 4      | <font color=\"red\">missing data<\/font> | |\n| 5b | 121 |+ 4 & 16 |<font color=\"red\">missing data<\/font> | |\n| 5c | 122 |+ 0|| |\n| 5d | 123 |+ 0 & 16|| |\n| 5e | 124 |+ 3|| |\n| 5f | 125 |+ 3 & 16|| |\n| 5g | 126 |+ 2|| |\n| 5h | 127 |+ 2 & 16|| |\n| 5i | 128 |+ 1|| |\n| 5j | 129 |+ 1 & 16|||\n\n\nHere, more than anywhere, merits the classic: \"*To be continued...*\". \n\nIt is very difficult to unravel what is going on in a traditional tabular format, and [quillio](https:\/\/www.kaggle.com\/quillio) has written a very interesting notebook [\"\ud83c\udf10*EDA: Tag Network Analysis (networkx + gephi)*\ud83c\udf10\"](https:\/\/www.kaggle.com\/quillio\/eda-tag-network-analysis-networkx-gephi) in which a graph analysis is performed showing relationships between the tags and the features.\n\nAlso, [Greg Calvez](https:\/\/www.kaggle.com\/gregorycalvez) has produced an excellent series of notebooks dedicated to understanding the tags:\n\n* [\"*De-anonymization: Time Aggregation Tags*\"](https:\/\/www.kaggle.com\/gregorycalvez\/de-anonymization-time-aggregation-tags), looking at `tag_{0, 1, 2, 3, 4, 5}`\n* [\"*De-anonymization: Price, Quantity, Stocks*\"](https:\/\/www.kaggle.com\/gregorycalvez\/de-anonymization-price-quantity-stocks), suggestinging that `tag_6` is related to prices, `tag_23` is realted to volume, and `tag_20` could be related to spread\n* [\"*De-anonymization: Min, Max and Time*\"](https:\/\/www.kaggle.com\/gregorycalvez\/de-anonymization-min-max-and-time), suggestinging that `tag_12` is related to minima, `tag_13` is related to maxima, and `tag_22` is related to time\n\n<a class=\"anchor\" id=\"action\"><\/a>\n## <center style=\"background-color:Gainsboro; width:40%;\">Action<\/center>\n\nThe target of this competition is the `action`: 1 to make the trade and 0 to pass on it. In view of this let us add a new 'binary' column to our test dataset called `action` such that if `resp` is positive then `action=1` else `action=0`, *i.e.*","b4a91c0b":"First of all, \n\n* **79.6%** of all the missing data is located in the **Tag 4** group, which represent the `resp_1` features\n* **15.2%** of the missing data is in the **Tag 3** group, which represent the `resp_2` features\n* In total, the features associated with `resp_1` and `resp_2` make up **> 95%** of all the missing data.\n\nWe can see that features 7 and 8 both have exactly the same number of missing values (393135). \n17 and 18, and 27 and 28 all have 395535 missing values each. These are all `resp_1` features.\n\nNext we have features 72, 78, 84, 90, 96, 102, 108, 114 all with 351426 missing values each. These too are all `resp_1` features.\n\nFeatures 21, 22, 31, 32 have 81444 missing values, closely followed by features 11 and 12. These are all `resp_2` features.\n\nThere are more features with even less missing values. I think the interesting thing is not so much the quantity of missing values in so much as it may tell us which features represent similar measures\/metrics.\n\nIs day 0 special, or does every day have missing data?","a6b5c05d":"It looks like we can certainly cut down on the number of eventual features in our model. A good place to start will be to look at the features in the region of the **60's** (*i.e.* **Tag 22**) as we have seen before there seems to be a lot of correlation between them.","96413f50":"Let us also calculate the [skew](https:\/\/en.wikipedia.org\/wiki\/Skewness) and [kurtosis](https:\/\/en.wikipedia.org\/wiki\/Kurtosis) of this distribution:","793a9751":"If that is the case, then 'volitile' days, say with more than 9k trades (*i.e.* `ts_id`) per day, are the following ","b0c33e70":"Indeed we can see that there is missing data *almost* every day, with no discernible pattern (weekly, monthly, *etc*). The exceptions are days **2** and **294**, which we shall look at in the next section.\n\nIn the notebook [\"*Jane Street EDA Market Regime*\"](https:\/\/www.kaggle.com\/marketneutral\/jane-street-eda-market-regime) written by [marketneutral](https:\/\/www.kaggle.com\/marketneutral) a plot is made of the number of trades per day, and is strikingly similar to the above plot. In view of this, for curiosity, we shall plot the number of missing values in the features with respect to the number of trades, for each day.","18a4ac13":"We can see that it has a total of 2,390,492 rows. I recommend reading this magnificent [Tutorial on reading large datasets](https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets) by [Vopani](https:\/\/www.kaggle.com\/rohanrao).\n\nI have used pandas to load in the `train.csv` and it took almost 2 minutes. To speed things up here I shall use [datatable](https:\/\/datatable.readthedocs.io\/en\/latest\/):"}}