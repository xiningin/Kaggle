{"cell_type":{"98d70a8c":"code","00250a19":"code","361b7d34":"code","88d1fa94":"code","58857e89":"code","d18beac5":"code","3673157a":"code","5ff97b5f":"code","3ff11cb8":"code","18dfb985":"code","53b9a3d7":"code","77c33ccb":"code","c623d7d0":"code","11c700db":"code","8973501e":"code","9d7e4280":"code","f0e76f56":"code","bcba1ecc":"code","bb4ea95e":"code","b350b92e":"code","d4402382":"code","f2407058":"code","4042598b":"code","1319ccc6":"code","2f592653":"code","2bf3ace7":"code","d2421078":"code","2bf89a7e":"code","71701bf8":"code","a8421adc":"code","44bc11b2":"code","58359991":"code","67701725":"code","45151667":"code","05a8c0cf":"code","dbd5a602":"code","0422fbc8":"code","abef26a7":"code","280b9c03":"code","e7a549e2":"code","93fefb68":"code","2dc677f9":"code","a2796a6f":"code","ec567fae":"code","e9e85217":"code","7dafcd0c":"code","aa94c3e1":"code","4b3c869c":"code","addf91f6":"code","8de44c8d":"code","e033bc08":"code","e4d37fa6":"code","9476f718":"code","59e3e520":"code","5281e00f":"code","5eece3dd":"code","bcdd24e5":"code","e00d21d6":"markdown","4f017b3a":"markdown","d4fabc62":"markdown","27b88a48":"markdown","a34a400e":"markdown","d8108bb9":"markdown","4fbee287":"markdown","6ea43079":"markdown","036f6808":"markdown","d3f45231":"markdown","a1521247":"markdown","1167abb7":"markdown","4bc7f10c":"markdown","d25e80f3":"markdown","fb2c44e1":"markdown","ab34ee72":"markdown","ae25e3c2":"markdown","d97c6225":"markdown","496478e2":"markdown","57a5b254":"markdown","90dd8e6d":"markdown","ca4abe31":"markdown","e1f9fc68":"markdown","52468b4a":"markdown","975bf3ed":"markdown","0f0b3892":"markdown","2bd69ed9":"markdown","87409238":"markdown","2c497ae3":"markdown","a476e8ac":"markdown","1a4e665b":"markdown","b8ded8a9":"markdown","86d19d85":"markdown","2d8fe42b":"markdown","3fd94ed2":"markdown","58df5f1f":"markdown","3d3a7154":"markdown","40091058":"markdown","113d828b":"markdown","b18ac8b3":"markdown","6fba3020":"markdown","a548571d":"markdown","8c0901ee":"markdown","681fa864":"markdown","30b8c3e5":"markdown","e80cb108":"markdown","82a111ee":"markdown","4f896444":"markdown","46e519b7":"markdown","32ac7503":"markdown"},"source":{"98d70a8c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","00250a19":"df_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train.info()","361b7d34":"df_train.describe()","88d1fa94":"df_train.head(10)","58857e89":"df_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)\ndf_train.head()","d18beac5":"df_train.columns","3673157a":"sns.histplot(df_train['SalePrice'])","5ff97b5f":"from scipy import stats\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","3ff11cb8":"df_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","18dfb985":"df_train_cor = df_train.corr()\ndf_train_cor[df_train_cor['PoolArea']>0.7]","53b9a3d7":"#df_train.GarageYrBit.astype('float64')\nsns.scatterplot(data=df_train, x ='YearBuilt', y='GarageYrBlt')","77c33ccb":"var = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=df_train)","c623d7d0":"corrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(13, 9))\nsns.heatmap(corrmat, vmax=.9, square=True,cmap=\"YlGnBu\");","11c700db":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nplt.figure(figsize=(6,4),dpi=150)\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();","8973501e":"sns.scatterplot(x = 'OverallQual', y= 'SalePrice', data = df_train)","9d7e4280":"sns.scatterplot(x = 'GrLivArea', y= 'SalePrice', data = df_train)","f0e76f56":"df_train[(df_train['SalePrice']<12.5) & (df_train['OverallQual'] > 8) & (df_train['GrLivArea']>4000)]","bcba1ecc":"dropouts = df_train[(df_train['SalePrice']<12.5) & (df_train['OverallQual'] > 8) & (df_train['GrLivArea']>4000)]\ndf_train = df_train.drop(df_train[(df_train['SalePrice']<12.5) & (df_train['OverallQual'] > 8) & (df_train['GrLivArea']>4000)].index)\nsns.scatterplot(x = 'GrLivArea', y= 'SalePrice', data = df_train)","bb4ea95e":"print(f'There are {df_train.isnull().sum().sum()} missing values')\ndf_train.isnull().sum().sort_values(ascending=False)","b350b92e":"df = df_train\ny = df[\"SalePrice\"]\ndf.drop(['SalePrice'], axis=1, inplace=True)\ndf.head()","d4402382":"df.isnull().sum().sort_values(ascending = False).head(20)","f2407058":"df[\"PoolQC\"] = df[\"PoolQC\"].fillna(\"None\")","4042598b":"df[\"MiscFeature\"] = df[\"MiscFeature\"].fillna(\"None\")","1319ccc6":"df[\"Alley\"] = df[\"Alley\"].fillna(\"None\")","2f592653":"df[\"Fence\"] = df[\"Fence\"].fillna(\"None\")","2bf3ace7":"df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(\"None\")","d2421078":"df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].apply(\n    lambda x: x.fillna(x.median()))","2bf89a7e":"df['GarageType'] = df['GarageType'].fillna('None')\ndf['GarageFinish'] = df['GarageFinish'].fillna('None')\ndf['GarageQual'] = df['GarageQual'].fillna('None')\ndf['GarageCond'] = df['GarageCond'].fillna('None')","71701bf8":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    df[col] = df[col].fillna(0)","a8421adc":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    df[col] = df[col].fillna(0)","44bc11b2":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df[col] = df[col].fillna('None')","58359991":"df[\"MasVnrType\"] = df[\"MasVnrType\"].fillna(\"None\")\ndf[\"MasVnrArea\"] = df[\"MasVnrArea\"].fillna(0)","67701725":"df['MSZoning'] = df['MSZoning'].fillna(df['MSZoning'].mode()[0])","45151667":"df = df.drop(['Utilities'], axis=1)","05a8c0cf":"df[\"Functional\"] = df[\"Functional\"].fillna(\"Typ\")","dbd5a602":"df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])","0422fbc8":"df['KitchenQual'] = df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])","abef26a7":"df['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\ndf['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])","280b9c03":"df['SaleType'] = df['SaleType'].fillna(df['SaleType'].mode()[0])","e7a549e2":"df['MSSubClass'] = df['MSSubClass'].fillna(\"None\")","93fefb68":"df.isnull().sum().sort_values(ascending=False)","2dc677f9":"categorical_data = df.dtypes[(df.dtypes == \"object\")].index\ncategorical_data","a2796a6f":"df[categorical_data].head()","ec567fae":"df_numerical = df.select_dtypes(exclude='object')\ndf_categorical = df.select_dtypes(include='object')\ndff = pd.get_dummies(df_categorical, drop_first=True)\ndf= pd.concat([df_numerical, dff], axis=1)\ndf.head()","e9e85217":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.3, random_state=101)\nX_train.head()","7dafcd0c":"from sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nlinearModel = LinearRegression()\nlinearModel.fit(X_train, y_train)\nlinearPred = linearModel.predict(X_test)\n\nMAE_linear = metrics.mean_absolute_error(y_test, linearPred)\nMSE_linear = metrics.mean_squared_error(y_test, linearPred)\nRMSE_linear = np.sqrt(MSE_linear)\n\nresults_df = pd.DataFrame(data=[[\"Linear Regression\", MAE_linear, MSE_linear, RMSE_linear]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE'])\nresults_df","aa94c3e1":"from sklearn.preprocessing import PolynomialFeatures\npolynomial_convertor = PolynomialFeatures(degree = 2, include_bias = False)\npolynomial_convertor.fit(df)\npoly_features = polynomial_convertor.transform(df)\npoly_features.shape","4b3c869c":"X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(poly_features, y, test_size=0.3, random_state=101)\npolyLinearModel = LinearRegression()\npolyLinearModel.fit(X_train_poly, y_train_poly)\npolyLinearPred = polyLinearModel.predict(X_test_poly)\n\nMAE_poly_linear = metrics.mean_absolute_error(y_test_poly, polyLinearPred)\nMSE_poly_linear = metrics.mean_squared_error(y_test_poly, polyLinearPred)\nRMSE_poly_linear = np.sqrt(MSE_poly_linear)\nresults_df = pd.DataFrame(data=[[\"Linear Regression\", MAE_linear, MSE_linear, RMSE_linear],\n                                [\"Polynomial Regression\", MAE_poly_linear, MSE_poly_linear, RMSE_poly_linear ]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE'])\nresults_df","addf91f6":"from sklearn.linear_model import Ridge, RidgeCV\nridgeModel = Ridge(alpha = 10)\nridgeModel.fit(X_train, y_train)\nridgePred = ridgeModel.predict(X_test)\n\nMAE_ridge = metrics.mean_absolute_error(y_test, ridgePred)\nMSE_ridge = metrics.mean_squared_error(y_test, ridgePred)\nRMSE_ridge = np.sqrt(MSE_ridge)\n\nresults_df = pd.DataFrame(data=[[\"Linear Regression\", MAE_linear, MSE_linear, RMSE_linear],\n                                [\"Polynomial Regression\", MAE_poly_linear, MSE_poly_linear, RMSE_poly_linear],\n                                [\"Ridge Regression\", MAE_ridge, MSE_ridge, RMSE_ridge]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE'])\nresults_df","8de44c8d":"ridgeCvModel = RidgeCV(alphas=(0.1,1.0,10.0,100.0))\nridgeCvModel.fit(X_train, y_train)\nridgeCvModel.alpha_","e033bc08":"from sklearn.linear_model import LassoCV\nlassoCvModel = LassoCV(eps=0.1, n_alphas=10000,cv=5)\nlassoCvModel.fit(X_train,y_train)\nlassoCvModel.alpha_","e4d37fa6":"lassoPred = lassoCvModel.predict(X_test)\nMAE_lasso = metrics.mean_absolute_error(y_test, lassoPred)\nMSE_lasso = metrics.mean_squared_error(y_test, lassoPred)\nRMSE_lasso = np.sqrt(MSE_lasso)\n\nresults_df = pd.DataFrame(data=[[\"Linear Regression\", MAE_linear, MSE_linear, RMSE_linear],\n                                [\"Polynomial Regression\", MAE_poly_linear, MSE_poly_linear, RMSE_poly_linear],\n                                [\"Ridge Regression\", MAE_ridge, MSE_ridge, RMSE_ridge],\n                                [\"LASSO Regression\", MAE_lasso, MSE_lasso, RMSE_lasso]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE'])\nresults_df","9476f718":"lassoCvModel.coef_","59e3e520":"from sklearn.linear_model import ElasticNetCV\nelasticModel = ElasticNetCV(l1_ratio=[.1,.5,.7,.7,.9,.95,.99,1],\n                           eps = 0.001, n_alphas=100,max_iter=1000000)\nelasticModel.fit(X_train, y_train)","5281e00f":"elasticPred = elasticModel.predict(X_test)\nprint(elasticModel.l1_ratio_)","5eece3dd":"MAE_elastic = metrics.mean_absolute_error(y_test, lassoPred)\nMSE_elastic = metrics.mean_squared_error(y_test, lassoPred)\nRMSE_elastic = np.sqrt(MSE_elastic)\nresults_df = pd.DataFrame(data=[[\"Linear Regression\", MAE_linear, MSE_linear, RMSE_linear],\n                                [\"Polynomial Regression\", MAE_poly_linear, MSE_poly_linear, RMSE_poly_linear],\n                                [\"Ridge Regression\", MAE_ridge, MSE_ridge, RMSE_ridge],\n                                [\"LASSO Regression\", MAE_lasso, MSE_lasso, RMSE_lasso],\n                                [\"Elastic Net\", MAE_elastic, MSE_elastic, RMSE_elastic]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE'])\nresults_df","bcdd24e5":"fig, ax = plt.subplots()\nfig.set_size_inches(10,7)\nregressors = [\"Linear\", \"Polynomial\", \"Ridge\", \"LASSO\", \"Elastic Net\"]\nrmses = [RMSE_linear, RMSE_poly_linear, RMSE_ridge, RMSE_lasso, RMSE_elastic]\nsns.barplot(x=regressors, y=rmses, ax= ax)\nplt.ylabel('RMSE')\nplt.show()","e00d21d6":"* Electrical : we will repeat the same apprach we used for MSZoning. It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","4f017b3a":"# \ud83d\udfe2 Ridge Regression","d4fabc62":"In the first plot we can see that two points in the 10th bar that are being soled with low prices and we can say these two points are probably our outliers. In the second plot we see two points for GrLivArea that are highly off-priced. we can assume that these two points in the first and second plot are the same.","27b88a48":"# Feature Engineering and Data Prepration\nWe have three issues to deal with:\n1. Outliers\n2. Missing Data\n3. Categorical Data \n","a34a400e":"* Fence : data description says NA means \"no fence\"\n","d8108bb9":"# \ud83d\udcca Data Gathering and EDA","4fbee287":"The skew seems now corrected and the data appears more normally distributed.","6ea43079":"## what we're gonna do is first take a look at what we're predicting! That would be the SalePrice. So let's see what we got. :)","036f6808":"We found them. these are the two points that will really screw up our regression. Obviously we can not check all the features one by one in order to find outliers. The corrolations hepled us to find the most corrolative features wich are OverallQual and GrLivArea and by checking them we found two points that will really screw our prediction up.","d3f45231":"This means since l1_ratio is 1 and that's the alpha parameter in the formula, it only considered lasso.","a1521247":"For MSZoning we hava 4 missing data. We can either delete them or another approach is to fill missing with the most common data is this column. ","1167abb7":"Let's concatinate the train and test data, because the data preparation such as data missing procedure must apply on both train and test data.","4bc7f10c":"Now let's take look at our categorical features.","d25e80f3":"* Functional : data description says NA means typical","fb2c44e1":"Ok it seems we've got rid of the missing data succesfully. Let's move on to categorical data.","ab34ee72":"* SaleType : Fill in again with most frequent which is \"WD\"","ae25e3c2":"We have a linear regression between GrLiveArea and ToralBsmtSF and we can see a nice positive corrolation between SalesPrice and GrLiveArea. Also realtion between SalePrice and YearBuilt can make us think!(kinda exponential regression)","d97c6225":"* BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.","496478e2":"# House Price Prediction using diffrent regression methods\n* EDA \n* Data preparation and Feature Engineering \n    * outliers\n    * missing data\n    * categorical data\n* Liniear regression\n* Polynomial regression\n* L1 regression \n* L2 regression\n* Elastic Net\n* Conclusion","57a5b254":"# \ud83d\udfe2 Elastic Net","90dd8e6d":"Up to now we just kept going after our gut to find the corrolations! we can take a look at all corrolations with df.corr. But I preffer to find them by a heatplot because it's graphical and instead of numbers we can find them by colors.","ca4abe31":"# \ud83d\udcc8 Training a Regression Model","e1f9fc68":"* Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string","52468b4a":"As you can see the result for elastic and LASSO regression are exactly the same.","975bf3ed":"Notice that median is diffrent from mean it's actually the middle of the values in the list of numbers.","0f0b3892":"# \ud83d\udfe2 Linear Regression","2bd69ed9":"# Outliers ","87409238":"* MSSubClass : Na most likely means No building class. We can replace missing values with None","2c497ae3":"* MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.","a476e8ac":"# Missing Data","1a4e665b":"# \ud83d\udfe2 Polynomial Regression","b8ded8a9":"Find the dark blues. They show us big corrolations. The relation that our intuition told us was true. Look at the SalePrice on axis X and OverallQual on the Y axis. Yep that's dark dark blue. Besides two other thing got my attention. First it's the relation between GrLiveArea and TotRmsAbvGrd. Second intersting relation is GarageYrBit and YearBuilt.","86d19d85":"# Categorical Data","2d8fe42b":"* KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.","3fd94ed2":"* BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement","58df5f1f":"* GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None","3d3a7154":"* MiscFeature : data description says NA means \"no misc feature\"","40091058":"* PoolQC: data description says NA means \"No Pool\". Ok then it's reasonable to fill the NA by None beacause majority of houses have no pool.","113d828b":"# \ud83d\udfe2 LASSO Regression (Least Absolute Shrinking and Selection Operator)","b18ac8b3":"To be sure about our corrolations we'll use scatterplots. Because we have a lot of features we will choose noncategorical features that we think they may have some corrolations with one another.","6fba3020":"* Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.","a548571d":"As you saw in the table LASSO Regression didn't do a good a job predecting the data and that's because lasso regression allow the coeficient to be zero. All coeficients are zero execpt two of them that means we only consider two of our feature to predict our label.\n* **Ok then why do we even use this model?!!!**\n    * In some cases the trade-off in resault may worth it to consider less feature. Considering only two feature would make out job too easy beacause as then we should only be worried about those two although we should consider the error rate it has given two us.    ","8c0901ee":"* FireplaceQu : data description says NA means \"no fireplace\"","681fa864":"* GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)","30b8c3e5":"![width = 500](https:\/\/ctp-media.imigino.com\/image\/1\/process\/nullxnull?source=https:\/\/d3cx3ub94vxukq.cloudfront.net\/wp-content\/uploads\/sites\/30\/2018\/06\/The-Tembisan-Gauteng-property-market-showing-signs-of-early-recovery.jpeg)","e80cb108":"LotFrontage : I actually didn't know what to do with this column and since it has almost 500 missing data, we cann't drop them. So I got help from https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard notebook to find a way. Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.","82a111ee":"* Alley : data description says NA means \"no alley access\"","4f896444":"Fnally It's time build out model and start predecting.","46e519b7":"**This part was kanda tricky and honestly I choose the easiest way to deal with the categorical data. Another way to deal with them was checking their corrolation with SalePrice and assigning a number to that feature target according to its corrolation. The more domain knowlege we use in this part the more precise data we will extract.**","32ac7503":"The target variable is right skewed. To make it more like a normal distro with assign a log function to it because especially linear models like normal distributions."}}