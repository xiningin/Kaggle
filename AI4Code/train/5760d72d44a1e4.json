{"cell_type":{"1b3476b8":"code","785613e7":"code","c5a06c94":"code","fad87281":"code","7e928b04":"code","990fde88":"code","717c29d8":"code","eb2f2c1e":"code","69378cf4":"code","7fd59d3e":"code","0a05353d":"code","6c80620a":"code","f1fc9bef":"code","f68edbf5":"code","cb861060":"code","90689982":"code","d9702ccc":"code","890be29c":"code","a161ca61":"code","690f9853":"code","eed54d0e":"code","b98ddfab":"code","14af260d":"code","63d235d1":"code","02780ca7":"code","6aa98969":"code","e2f55eed":"code","1d15f2c1":"code","ed6ccea8":"code","d3172380":"code","81c5c485":"code","64bed897":"code","801301d2":"code","9db96887":"code","7abfae43":"markdown","667cb697":"markdown","3d65c723":"markdown","34069040":"markdown","6b65f127":"markdown","d2788b24":"markdown","a6c71712":"markdown","0be979e4":"markdown","453a4371":"markdown","e22e8617":"markdown","c94c80b9":"markdown","63918888":"markdown","4d0e13ae":"markdown"},"source":{"1b3476b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","785613e7":"#Import Data\n\ndf = pd.read_csv('..\/input\/Iris.csv')","c5a06c94":"# First 5 rows of Data\n\ndf.head()","fad87281":"# Lowercase  all column\ndf.columns = map(str.lower, df.columns)\n# See which classes we have\ndf.species.unique()","7e928b04":"# As we will create model for 2 different classes we drop 'Iris-virginica'\n\ndf = df[df.species != 'Iris-setosa'].drop('id',axis=1)","990fde88":"#Summary of data\n\ndf.info()","717c29d8":"# plot the data with Species Label\n\nsns.pairplot(df, hue='species', size=2.5)","eb2f2c1e":"# We need to change Species column data type to categorical to apply .cat.codes method\n\ndf.species = df.species.astype('category')\n\n# We assign 0 for Iris-setosa and 1 for Iris-versicolor\n\ndf[\"species\"] = df[\"species\"].cat.codes","69378cf4":"df.head()","7fd59d3e":"# Create x variable for our feature. Drop species and species_type as they are not features \n\nx = df.drop('species',axis = 1)\n\n# Create y variable for label of x features\n\ny = df.species.values","0a05353d":"# Check x\nx.head()","6c80620a":"# Whitening Normalization\nx_norm = (x-np.mean(x))\/np.std(x)","f1fc9bef":"#Normalized x\nx_norm.head()","f68edbf5":"# Split the data\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm, y, test_size=0.2, random_state=42)","cb861060":"# Pandas Dataframe create arrays with features on column and observations on rows.\n# We need to transpose these arrays to fit Logistic Regression Equation   \nx_test = x_test.T\nx_train = x_train.T","90689982":"# weight initialization function\ndef weight_init(dimension) :\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b","d9702ccc":"# We use sigmoid function to have 1-0 prediction.\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","890be29c":"def logistic_forward_propagation(x_train):\n    \n    dimension =  x_train.shape[0]\n    \n    w,b = weight_init(dimension)\n    \n    z = np.dot(w.T,x_train) + b\n    \n    y_head = sigmoid(z)     \n    \n    # To calculate how far our prediction from real value we use Log likehood Function\n    # Log Likehood = (y * log(y_head)) + ((1\u2212y) * log(1\u2212y_head))\n    # Difference between real values and predicted values is Cost Function our aim is to minimize Cost Function\n    # Cost Function = \u2211-(Log Likehood)\n    \n    inv_loss = (y_train * np.log(y_head)) + ((1-y_train) * np.log(1-y_head))\n    \n    cost = (-1\/x_train.shape[1]) * (np.sum(inv_loss))\n    \n    return cost","a161ca61":"logistic_forward_propagation (x_train)","690f9853":"def grading_decent(w, b, x_train, learning_rate, iteration_num):\n    \n    # we use grading desenct to minimize Cost function\n    # Grading Descent = First Value - Learning_Rate * \u2202Cost_Function \/ \u2202Parameter\n    weights = []\n    bias = []\n    costs = []\n    for i in range(iteration_num):\n        weights.append(w[0])\n        bias.append(b)\n        y_head,cost = logistic_backward_propagation(x_train, y_train, w, b)\n        costs.append(cost)\n        \n        w = w - learning_rate*((np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1])\n        \n        b = b - learning_rate*((np.sum(y_head-y_train))\/x_train.shape[1])\n    return weights,bias,costs,w,b","eed54d0e":"def logistic_backward_propagation(x_train, y_train, w, b):\n    \n    z = np.dot(w.T,x_train) + b\n    \n    y_head = sigmoid(z)     \n    \n    # To calculate how far our prediction from real value we use Log likehood Function\n    # Log Likehood = (y * log(y_head)) + ((1\u2212y) * log(1\u2212y_head))\n    # Difference between real values and predicted values is Cost Function our aim is to minimize Cost Function\n    # Cost Function = \u2211-(Log Likehood)\n    \n    inv_loss = (y_train * np.log(y_head)) + ((1-y_train) * np.log(1-y_head))\n    \n    cost = (-1\/x_train.shape[1]) * (np.sum(inv_loss))\n    \n    return y_head,cost","b98ddfab":"def predict(w,b,x_test):\n    \n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    \n    return Y_prediction","14af260d":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate , iteration_num):\n    # initialize\n    dimension =  x_train.shape[0]\n    \n    w,b = weight_init(dimension)\n    \n    weights,bias,costs,w,b = grading_decent (w, b, x_train, learning_rate, iteration_num)\n    \n    y_prediction_test = predict(w, b, x_test)\n    y_prediction_train = predict(w, b, x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    return weights,costs","63d235d1":"weights,costs = logistic_regression(x_train, y_train, x_test, y_test, learning_rate = 0.01, iteration_num = 150)","02780ca7":"plt.scatter(weights,costs)\nplt.show()","6aa98969":"weights,costs = logistic_regression(x_train, y_train, x_test, y_test, learning_rate = 0.01, iteration_num = 300)\nplt.scatter(weights,costs) \nplt.show()","e2f55eed":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(max_iter=150)","1d15f2c1":"lr.fit(x_train.T,y_train.T)","ed6ccea8":"lr.predict(x_test.T)","d3172380":"score = lr.score(x_test.T, y_test.T)\nprint(score)","81c5c485":"x_norm = (x-np.min(x))\/(np.max(x)-np.min(x))","64bed897":"# Split the data\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm, y, test_size=0.2, random_state=42)","801301d2":"# Pandas Dataframe create arrays with features on column and observations on rows.\n# We need to transpose these arrays to fit Logistic Regression Equation   \nx_test = x_test.T\nx_train = x_train.T","9db96887":"weights,costs = logistic_regression(x_train, y_train, x_test, y_test, learning_rate = 0.01, iteration_num = 150)","7abfae43":"Prediction Function with 0.5 Threshold","667cb697":"Calculate the Cost function for one Iteration","3d65c723":"Plot for Weight - Cost","34069040":"Lets Change normalization method from Whitening to Min Max Normalization","6b65f127":"Our Cost is go low after 150 iteration.","d2788b24":"Logistic regression equation\n\nw.T = [[w1],[w2],[w3],[w4]]\n\nx_train = [[(x1,1),(x1,2),...,(x1,80)], [(x2,1),(x2,2),...,(x2,80)], [(x3,1),(x3,2),...,(x3,80)], [(x4,1),(x4,2),...,(x4,80)]]\n\nz = w.T * x_train + b\n\ny_head = sigmoid(z)","a6c71712":"Conclusion\n\nThe accuracy goes down. Our data has only possitive value. Thats why if we apply min max normalization the output of the sigmoid function is always 0.5 as a result y_train matrix is all 1 in every case. Consequently, test accuracy goes down.","0be979e4":"Grading Decent equation\n\nw = [[w1], [w2], [w3], [w4]]\n\nx_train = [[(x1,1),(x1,2),...,(x1,80)], [(x2,1),(x2,2),...,(x2,80)], [(x3,1),(x3,2),...,(x3,80)], [(x4,1),(x4,2),...,(x4,80)]]\n\ny_train = [[(y1,1)], ., ., [(y80,1)]]\n\ny_head = [[(y1,1)], ., ., [(y80,1)]]\n\nw_new = w_old - Learning_rate*np.dot(x_train,((y_head-y_train).T))","453a4371":"If we increase the iteration Cost stays same level after it reaches the optimum level.","e22e8617":"As features have different distrubution we apply Min-Max normalization to every column.","c94c80b9":"Main Function that we use for Logistic Regression","63918888":"Introduction\n\nIn this notebook, we will use Logistic Regression to make classification.","4d0e13ae":"Our data has 100 different measurement and has 4 feature.\n\nAll features are float64 so we don't need to make any conversion."}}