{"cell_type":{"d1a6d7dd":"code","7a916036":"code","bbd48c8c":"code","7175a2e4":"code","e05c7f08":"code","780c7496":"code","6e9ee75d":"code","c55e3809":"code","f7cc915b":"code","ad8c9611":"code","a36970cc":"code","b6423d5a":"code","8edee73d":"code","afc8a877":"code","83f0e479":"code","b32267bc":"code","3fcfedba":"code","fb2cbcf1":"code","2be8b7e6":"code","1cdec07e":"code","043b7dba":"code","822a9174":"code","32f2ae70":"code","31246972":"code","23676e12":"code","f7141863":"code","0d6c6dd9":"code","351ca521":"code","daee5903":"code","dce50509":"code","781b3344":"code","4a69fd7e":"code","04f41f12":"code","f471e351":"code","bca6e870":"code","6ae9003a":"code","0913b42c":"code","1a8ec584":"code","fb592b5c":"code","a65c0257":"code","e50a8f1b":"code","c1568dda":"code","9437138b":"code","56ea0509":"code","d6d43eb8":"code","4041b6db":"code","220f971b":"code","b6bea626":"code","9ac51486":"code","4c748fe8":"code","bed03fda":"code","938ac361":"code","05ea78a6":"code","a22d013a":"code","2236c5b8":"code","656790e2":"markdown","eab50a63":"markdown","103ffe9f":"markdown","ae2bcef0":"markdown","5aa4b3a9":"markdown","d7b92de5":"markdown","fd080b02":"markdown","db3193d7":"markdown","f5ff2e2e":"markdown","38edef74":"markdown","0465f781":"markdown","7bf76ad5":"markdown","ce9f0f97":"markdown","23e8b828":"markdown","b4f70b30":"markdown","69dbeb8e":"markdown","8fcbe051":"markdown","f0245ac8":"markdown","14f6fffe":"markdown","b1f90215":"markdown","6a9e784c":"markdown","2c304d9f":"markdown","9bb742b6":"markdown","7f5c1913":"markdown","89615d7d":"markdown","bd0462e5":"markdown","9e2ddc01":"markdown","444fb25f":"markdown","c63d7e37":"markdown","ecc4ba87":"markdown","9e94ea87":"markdown","043820c7":"markdown","2af3238e":"markdown","45d302e0":"markdown","54c5a009":"markdown","2916bd91":"markdown","57d086a4":"markdown","998f6ca7":"markdown","bf59e27f":"markdown","dedba069":"markdown","61eaef4e":"markdown","a0806dcd":"markdown","6abc09b2":"markdown","c9e1d07c":"markdown","a9610a91":"markdown","9d5ef340":"markdown","bd60b135":"markdown","10973e52":"markdown","22b1a6e3":"markdown","5955e2fa":"markdown"},"source":{"d1a6d7dd":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate,train_test_split","7a916036":"df = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf.head()","bbd48c8c":"df.shape","7175a2e4":"df.dropna(inplace=True)\ndf.shape","e05c7f08":"#Distribution of dependent variable\nsns.distplot(df.Salary);","780c7496":"# According to the league variable, groupby is made and how much salary is taken in which league on average.\ndf.groupby(\"League\").agg({\"Salary\": \"mean\"})","6e9ee75d":"# What is the average salary the players receive based on the region they play in?\ndf.groupby([\"Division\",\"Years\"]).agg({\"Salary\": \"mean\"})","c55e3809":"df = df.drop([\"League\", \"Division\", \"NewLeague\"], axis= 1).astype(\"float64\")","f7cc915b":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car\n\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","ad8c9611":"cat_cols","a36970cc":"def num_cols(df):\n    numeric_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\n    return numeric_cols\n\nfor i in num_cols(df):\n\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 4))\n    sns.histplot(df[i], bins=10, ax=axes[0])\n    axes[0].set_title(i)\n    \n    sns.boxplot(df[i], ax=axes[1])\n    axes[1].set_title(i)\n   \n    sns.kdeplot(df[i], ax=axes[2])\n    axes[2].set_title(i)\n    plt.show()","b6423d5a":"y = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)","8edee73d":"models = [\n          ('CART', DecisionTreeRegressor(random_state=17)),\n          ('RF', RandomForestRegressor(random_state=17)),\n          ('GBM', GradientBoostingRegressor(random_state=17)),\n          (\"XGBoost\", XGBRegressor(random_state=17, objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor(random_state=17)),\n          (\"CatBoost\", CatBoostRegressor(random_state=17, verbose=False))\n          ]","afc8a877":"for name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","83f0e479":"df = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf.head()","b32267bc":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","3fcfedba":"cat_cols","fb2cbcf1":"def label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\nfor col in cat_cols:\n    df.loc[:, col] = label_encoder(df, col)","2be8b7e6":"def hitters_feature(df):\n\n    df[\"New_BattingAverage\"] = df[\"CHits\"] \/ df[\"CAtBat\"]\n    df[\"New_TotalBases\"] =  ((df[\"CHits\"] * 2) + (4 * df[\"CHmRun\"]))\n    df[\"New_SluggingPercentage\"] = df[\"New_TotalBases\"] \/ df[\"CAtBat\"]\n    df[\"New_IsolatedPower\"] = df[\"New_SluggingPercentage\"] - df[\"New_BattingAverage\"]\n    df[\"New_TripleCrown\"] = (df[\"CHmRun\"] * 0.4) + (df[\"CRBI\"] * 0.25) + (df[\"New_BattingAverage\"] * 0.35)\n    df[\"New_BattingAverageOnBalls\"] = (df[\"CHits\"] - df[\"CHmRun\"]) \/ (df[\"CAtBat\"] - df[\"CHmRun\"])\n    df[\"New_RunsCreated\"] = df[\"New_TotalBases\"] * (df[\"CHits\"] + df[\"CWalks\"]) \/ (df[\"CAtBat\"] + df[\"CWalks\"])\n    df[\"New_FieldingPercentage\"] = 1 - ((df[\"PutOuts\"] + df[\"Assists\"]) \/ (df[\"PutOuts\"] + df[\"Assists\"] + df[\"Errors\"] + 1))\n\n    df[\"New_CRunsYearsRatio\"] = df[\"CRuns\"] \/ df[\"Years\"]\n    df['New_PutOutsYears'] = df['PutOuts'] * df['Years']\n    df[\"New_RBIWalks\"] = df[\"RBI\"] * df[\"Walks\"]\n    df[\"New_CHmRunCAtBatRatio\"] = df[\"CHmRun\"] \/ df[\"CAtBat\"]\n    \n    df['AtBat_new'] = df['AtBat'] \/ df['CAtBat']\n    df['Hits_new'] = df['Hits'] \/ df['CHits']\n    df['HmRun_new'] = (df['HmRun'] \/ df['CHmRun']).fillna(0)\n    df['Runs_new'] = df['Runs'] \/ df['CRuns']\n    df['RBI_new'] = (df['RBI'] \/ df['CRBI']).fillna(0)\n    df['Walks_new'] = (df['Walks'] \/ df['CWalks']).fillna(0)\n\n    df[\"CAtBat_rate\"] = df[\"CAtBat\"] \/ df[\"Years\"]\n    df[\"CHits_rate\"] = df[\"CHits\"] \/ df[\"Years\"]\n    df[\"CHmRun_rate\"] = df[\"CHmRun\"] \/ df[\"Years\"]\n    df[\"Cruns_rate\"] = df[\"CRuns\"] \/ df[\"Years\"]\n    df[\"CRBI_rate\"] = df[\"CRBI\"] \/ df[\"Years\"]\n    df[\"CWalks_rate\"] = df[\"CWalks\"] \/ df[\"Years\"]\n    \n    return df","1cdec07e":"df = hitters_feature(df)","043b7dba":"# Based on some trials and correlation results, we subtract variables that do not contribute to the model from our data set.\ndf = df.drop(['AtBat','Hits','HmRun','Runs','RBI','Walks','Assists','Errors',\"PutOuts\",'League','NewLeague', 'Division'], axis=1)","822a9174":"null_df = df[df[\"Salary\"].isnull()]\nnull_df = null_df.drop([\"Salary\"], axis=1)\ndf.dropna(inplace=True)","32f2ae70":"null_df.shape","31246972":"df.head","23676e12":"df.isnull().sum()","f7141863":"y = df['Salary']\nX = df.drop(\"Salary\", axis=1)","0d6c6dd9":"models = [\n          ('CART', DecisionTreeRegressor(random_state=17)),\n          ('RF', RandomForestRegressor(random_state=17)),\n          ('GBM', GradientBoostingRegressor(random_state=17)),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror',random_state=17)),\n          (\"LightGBM\", LGBMRegressor(random_state=17)),\n          (\"CatBoost\", CatBoostRegressor(verbose=False,random_state=17))\n          ]","351ca521":"for name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","daee5903":"print('RandomForestRegressor:',RandomForestRegressor().get_params())\nprint('XGBRegressor:',XGBRegressor(objective='reg:squarederror').get_params())\nprint('LGBMRegressor:',LGBMRegressor().get_params())\nprint('CatBoostRegressor:',CatBoostRegressor().get_params())\nprint('GradientBoostingRegressor:',GradientBoostingRegressor().get_params())\nprint('DecisionTreeRegressor:',DecisionTreeRegressor(random_state=1).get_params())","dce50509":"cart_params = {'max_depth': range(1, 20),\n               \"min_samples_split\": range(2, 30)}\n\n\nrf_params = {\"max_depth\": [5, 8, 15, None],\n             \"max_features\": [2, 5,7, 9, \"auto\"],\n             \"min_samples_split\": [2, 4,5,7],\n             \"n_estimators\": [100, 200,300,400,500]}\n\nxgboost_params = {\"learning_rate\": [0.1, 0.01, None],\n                  \"max_depth\": [5, 8,10 ,None],\n                  \"n_estimators\": [100, 200, 500,600],\n                  \"colsample_bytree\": [0.5, 0.7, 1,None]}\n\nlightgbm_params = {\"learning_rate\": [0.01, 0.1],\n                   \"n_estimators\": [100, 500,1500],\n                   \"colsample_bytree\": [0.5, 0.7, 1.0]}\n\nregressors = [(\"CART\", DecisionTreeRegressor(random_state=17).fit(X,y), cart_params),\n              (\"RF\", RandomForestRegressor(random_state=17).fit(X,y), rf_params),\n              ('XGBoost', XGBRegressor(random_state=17, objective='reg:squarederror').fit(X,y), xgboost_params),\n              ('LightGBM', LGBMRegressor(random_state=17).fit(X,y), lightgbm_params)]\n\nbest_models = {}","781b3344":"for name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n    gs_best = GridSearchCV(regressor, params, cv=5, n_jobs=-1, verbose=False).fit(X, y)\n\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model","4a69fd7e":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')","04f41f12":"plot_importance(best_models[\"XGBoost\"], X)","f471e351":"plot_importance(best_models[\"LightGBM\"], X)","bca6e870":"plot_importance(best_models[\"RF\"], X)","6ae9003a":"voting_reg = VotingRegressor(estimators=[('RF',best_models[\"RF\"]),\n                                         ('LightGBM', best_models[\"LightGBM\"]),\n                                         ('XGBoost', best_models[\"XGBoost\"])])","0913b42c":"voting_reg.fit(X, y)","1a8ec584":"np.mean(np.sqrt(-cross_val_score(voting_reg, X, y, cv=10, scoring=\"neg_mean_squared_error\")))","fb592b5c":"X.columns","a65c0257":"random_user = X.sample(1, random_state=15)\nuser_index = random_user.index\nprint(user_index[0])\nvoting_reg.predict(random_user)","e50a8f1b":"df[df.index==user_index[0]]['Salary']","c1568dda":"null_df.isnull().sum()","9437138b":"null_df['Salary']=voting_reg.predict(null_df)","56ea0509":"final_df = df.merge(null_df, how='outer')","d6d43eb8":"final_df.shape","4041b6db":"def outlier_thresholds(dataframe, col_name, q1=0.1, q3=0.90):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","220f971b":"def replace_with_thresholds(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if low_limit > 0:\n        dataframe.loc[(dataframe[col_name] < low_limit), col_name] = low_limit\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit\n    else:\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit\n        \n    return dataframe","b6bea626":"final_df=replace_with_thresholds(df, 'Salary')","9ac51486":"sns.boxplot(final_df['Salary'])","4c748fe8":"y = final_df['Salary']\nX = final_df.drop(\"Salary\", axis=1)","bed03fda":"cart_params = {'max_depth': range(1, 20),\n               \"min_samples_split\": range(2, 30)}\n\n\nrf_params = {\"max_depth\": [5, 8, 15, None],\n             \"max_features\": [2, 5,7, 9, \"auto\"],\n             \"min_samples_split\": [2, 4,5,7],\n             \"n_estimators\": [100, 200,300,400,500]}\n\nxgboost_params = {\"learning_rate\": [0.1, 0.01, None],\n                  \"max_depth\": [5, 8,10 ,None],\n                  \"n_estimators\": [100, 200, 500,600],\n                  \"colsample_bytree\": [0.5, 0.7, 1,None]}\n\nlightgbm_params = {\"learning_rate\": [0.01, 0.1],\n                   \"n_estimators\": [100, 500,1500],\n                   \"colsample_bytree\": [0.5, 0.7, 1.0]}\n\nregressors = [(\"CART\", DecisionTreeRegressor(random_state=17).fit(X,y), cart_params),\n              (\"RF\", RandomForestRegressor(random_state=17).fit(X,y), rf_params),\n              ('XGBoost', XGBRegressor(random_state=17, objective='reg:squarederror').fit(X,y), xgboost_params),\n              ('LightGBM', LGBMRegressor(random_state=17).fit(X,y), lightgbm_params)]\n\nbest_models = {}","938ac361":"best_models = {}\n\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (Before): {round(rmse, 4)} ({name}) \")\n\n    gs_best = GridSearchCV(regressor, params, cv=5, n_jobs=-1, verbose=False).fit(X, y)\n\n    final_model = regressor.set_params(**gs_best.best_params_)\n\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n    best_models[name] = final_model","05ea78a6":"voting_reg = VotingRegressor(estimators=[('XGBoost', best_models['XGBoost']),\n                                         ('RF', best_models[\"RF\"]),\n                                         ('LightGBM',best_models['LightGBM'])])\n\nvoting_reg.fit(X, y)\n\nnp.mean(np.sqrt(-cross_val_score(voting_reg, X, y, cv=10,\n                                 scoring=\"neg_mean_squared_error\")))","a22d013a":"random_user = X.sample(1, random_state=15)\nuser_index = random_user.index\nprint(user_index[0])\nvoting_reg.predict(random_user)","2236c5b8":"user_index = random_user.index\ndf[df.index==user_index[0]]['Salary']","656790e2":"**Quick Data Preprocessing**","eab50a63":"A random forest is a group of decision trees. However, there are some differences between the two. A decision tree tends to create rules, which it uses to make decisions. A random forest will randomly choose features and make observations, build a forest of decision trees, and then average out the results.\n\n**Bagging, otherwise known as bootstrap aggregation, allows individual decision trees to randomly sample from the dataset and replace data, creating very different outcomes in individual trees. This means that instead of including all the available data, each tree takes only some of the data. These individual trees then make decisions based on the data they have and predict outcomes based only on these data points.**\n \n**That means that in each random forest, there are trees that are trained on different data and have used different features in order to make decisions. This provides a buffer for the trees, protecting them from errors and incorrect predictions.**\n \n**The process of bagging only uses about two-thirds of the data, so the remaining third can be used as a test set.**","103ffe9f":"![image.png](attachment:07e23807-e901-4458-a4fc-5d03918ae33a.png)","ae2bcef0":"Linear models are not the only type of regression models. Another powerful technique is to use regression trees. Regression trees are based on the idea of a decision tree. A decision tree is a bit like a flowchart, where at each step you ask whether a variable is greater than or less than some value. After flowing through several of these steps, you reach the end of the tree and receive an answer for what value the prediction should be. In this article, using certain tree methods; We will try to reduce the error in the salary values that we estimated in the Hitters data set.","5aa4b3a9":"While doing Feature Engineering, we add +1 to each column to avoid errors in division operations. We leave salary and the year variable out of this process.","d7b92de5":"# Gradient Boosting Machines","fd080b02":"# Random Forest","db3193d7":"The representation for the CART model is a binary tree.\n\nThis is your binary tree from algorithms and data structures, nothing too fancy. Each root node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).\n\nThe leaf nodes of the tree contain an output variable (y) which is used to make a prediction.\n\nGiven a dataset with two inputs (x) of height in centimeters and weight in kilograms the output of sex as male or female, below is a crude example of a binary decision tree (completely fictitious for demonstration purposes only).","f5ff2e2e":"![](https:\/\/miro.medium.com\/max\/596\/1*N0pcLAiD1zR6OHHYbhwAdg.png)","38edef74":"**Tree Based Regression Models**\n\n\n* CART (Classification and Regression)\n\n* Random Forest\n\n* Gradient Boosting Machines\n\n* XGBoost\n\n* Light GBM\n\n* CatBoost (Categorical Boosting)","0465f781":"![](https:\/\/machinelearningmastery.com\/wp-content\/uploads\/2016\/02\/Example-Decision-Tree.png)","7bf76ad5":"# DATA PREPROCESSING, FEATURE ENGINEERING AND MODEL TUNING ","ce9f0f97":"**Feature Engineering**","23e8b828":"**The two reasons to use XGBoost:**\n\nExecution Speed\n\nModel Performance","b4f70b30":"The term \u201crandom decision forest\u201d was first proposed in 1995 by Tin Kam Ho. Ho developed a formula to use random data to create predictions. Then in 2006, Leo Breiman and Adele Cutler extended the algorithm and created random forests as we know them today. This means this technology, and the math and science behind it, are still relatively new.","69dbeb8e":"# Hyperparameter Tuning And Feature Engineering","8fcbe051":"# Light GBM","f0245ac8":"In contrast to bagging, you use very simple classifiers as base classifiers, so-called \u201cweak learners.\u201d Picture these weak learners as \u201cdecision tree stumps\u201d \u2013 decision trees with only 1 splitting rule. Below, we will refer to the probably most popular example of boosting, AdaBoost. Here, we start with one decision tree stump (1) and \u201cfocus\u201d on the samples it got wrong. In the next round, we train another decision tree stump that attempts to get these samples right (2); we achieve this by putting a larger weight on these training samples. Again, this 2nd classifier will likely get some other samples wrong, so you\u2019d re-adjust the weights. In a nutshell, we can summarize \u201cAdaboost\u201d as \u201cadaptive\u201d or \u201cincremental\u201d learning from mistakes. Eventually, we will come up with a model that has a lower bias than an individual decision tree (thus, it is less likely to underfit the training data). [1]","14f6fffe":"# REFERENCES","b1f90215":"# Stacking & Ensemble Learning","6a9e784c":"Finding ideal values in hyperparameter optimization was very challenging in Kaggle, so I did a Grid Search among very few values just to be an example, you can run it for ideal values in wider ranges in your own local environments.","2c304d9f":"![image.png](attachment:e4384695-27ee-4779-9f46-16a42478a858.png)","9bb742b6":"![image.png](attachment:9ab3c294-3203-4da6-b6f7-2c80bf75bdb9.png)![image.png](attachment:0f4dfa3d-f1ad-4caf-93c1-00cca13d59f8.png)","7f5c1913":"# Model Creation with Default Hyperparameters\nNow, we will make direct predictions in various algorithms without any operation, so that we can see how much our error has decreased after the operations we have done.\nBut first, we have to do a few operations for NaN, that is, empty values and categorical variables (League, Division, NewLeague variables) so that all our algorithms can work properly.","89615d7d":"**Boosting:**","bd0462e5":"# Feature Importance","9e2ddc01":"The most important point to consider here is that the default values of the models must be added to the set of values to be calculated while performing Grid Search.","444fb25f":"**Benefits of Random Forest:**\n\nEasy to Measure Relative Importance\n\nVersatile\n\nNo Overfitting\n\nHighly Accurate\n\nReduces Time Spent on Data Management\n\nQuick Training Speed\n\n**Challenges of Random Forest:**\n\nSlower Results\n\nUnable to Extrapolate\n\nLow Interpretability [3]","c63d7e37":"# Final Model with after Prediction of Null Target Variables ","ecc4ba87":"![](https:\/\/miro.medium.com\/max\/700\/1*YGkAuYiey8zJA2IMvHBASA.png)","9e94ea87":"The first realization of boosting that saw great success in application was Adaptive Boosting or AdaBoost for short. Boosting refers to this general problem of producing a very accurate prediction rule by combining rough and moderately inaccurate rules-of-thumb.The weak learners in AdaBoost are decision trees with a single split, called decision stumps for their shortness. AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. \n\nThis framework was further developed by Friedman and called Gradient Boosting Machines. Later called just gradient boosting or gradient tree boosting. The statistical framework cast boosting as a numerical optimization problem where the objective is to minimize the loss of the model by adding weak learners using a gradient descent like procedure. This class of algorithms were described as a stage-wise additive model. This is because one new weak learner is added at a time and existing weak learners in the model are frozen and left unchanged.\n\n\n**How Gradient Boosting Works**\n\nGradient boosting involves three elements:\n\n1. A loss function to be optimized.\n\n2. A weak learner to make predictions.\n\n3. An additive model to add weak learners to minimize the loss function.","043820c7":"# The evolution of tree-based algorithms over time","2af3238e":"# Prediction for a New Observation","45d302e0":"It is very, very important to know and understand the data set well before making predictions. It is also necessary to have some knowledge of the subject the dataset is related to. Hitters.csv is a dataset that contains statistics on the careers of baseball players, and the actions we will take without knowing anything about baseball will not work to reduce our error. So we should start by getting to know our dataset.","54c5a009":"# Hitters Dataset\nThe Hitters dataset is a dataset that contains certain statistics and salaries of Major league baseball players for the years 1986\u201387.","2916bd91":"XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. In fact, both XGBoost and Gradient-Boosting Machines (GBMs) use community-based weak learners, weak learners are supported by the gradient-descent method.\n\n\n**The advantage of XGBoost over GBM is both system (hardware and software) optimization and algorithmic enhancements.**","57d086a4":"**Bagging:**","998f6ca7":"The random forest algorithm is actually a bagging algorithm: also here, we draw random bootstrap samples from your training set. However, in addition to the bootstrap samples, we also draw random subsets of features for training the individual trees; in bagging, we provide each tree with the full set of features. Due to the random feature selection, the trees are more independent of each other compared to regular bagging, which often results in better predictive performance (due to better variance-bias trade-offs), and I\u2019d say that it\u2019s also faster than bagging, because each tree learns only from a subset of features.[1]","bf59e27f":"# CART (Classification andd Regresson Tree)","dedba069":"# Bagging (Bootstrap Aggregation) and Boosting Methods","61eaef4e":"**Reading the Dataset**","a0806dcd":"# Predicting NULL Values With Voting Regression Model","6abc09b2":"**Importing Libraries**","c9e1d07c":"![image.png](attachment:1e4b702c-783d-498f-8b50-4b9a6ca862ac.png)","a9610a91":"Data is growing day by day and the need for new methods that are faster, focused on accuracy and high performance results have increased. The LGB algorithm was developed by Microsoft for these purposes. The LGB algorithm is a decision tree-based ensemble learning algorithm.\nMost decision tree algorithms use level-wise division.\n\nLGB uses leaf-wise splitting as follows. Leaf-wise finds and selects the leaves that will reduce loss the most according to level-wise. It just splits that leaf. It uses Depth-First Search (DFS) instead of Breadth - First Search (BFS). BFS visits the nearest neighbors. DFS, on the other hand, aims to go as far as it can go, then return and wander around the graph.","9d5ef340":"![image.png](attachment:2e076370-1a99-4157-afc6-2972c2f251b2.png)","bd60b135":"# XGBoost Algorithm","10973e52":"**Automated Hyperparameter Optimization**","22b1a6e3":"[1] https:\/\/sebastianraschka.com\/faq\/docs\/bagging-boosting-rf.html\n\n[2] https:\/\/machinelearningmastery.com\/bagging-and-random-forest-ensemble-algorithms-for-machine-learning\/\n\n[3] https:\/\/www.tibco.com\/reference-center\/what-is-a-random-forest\n\n[4] https:\/\/machinelearningmastery.com\/gentle-introduction-gradient-boosting-algorithm-machine-learning\/","5955e2fa":"Here, we train a number (ensemble) of decision trees from bootstrap samples of your training set. Bootstrap sampling means drawing random samples from our training set with replacement. E.g., if our training set consists of 7 training samples, our bootstrap samples (here: n=7) can look as follows, where C1, C2, \u2026 Cm shall symbolize the decision tree classifiers:"}}