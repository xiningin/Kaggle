{"cell_type":{"67e584f7":"code","454ad1a3":"code","40409784":"code","3db657a8":"code","7985a731":"code","71493588":"code","8b1e4997":"code","65dde8b6":"code","99be1420":"code","d57757fc":"code","2ed7d929":"code","110907f3":"code","606759ac":"code","621f6aa7":"code","d33a17bf":"code","0ab3ebfd":"code","c5cc1f0e":"code","50bb72ea":"code","d2e55808":"code","dc3cf671":"code","654a02ac":"code","4a054713":"markdown","c7890133":"markdown","1f77d198":"markdown","c5f26a06":"markdown","815051c5":"markdown","8957a40b":"markdown","03df6f15":"markdown","e14ee8e2":"markdown","fa255fd0":"markdown","1021d873":"markdown","87254fba":"markdown","5d9ac581":"markdown","cd09f8c4":"markdown","a14e8e47":"markdown","592aa088":"markdown","c3ea46b7":"markdown","0420a72e":"markdown","29f0ca90":"markdown","a049f974":"markdown","6cce5616":"markdown","2ff8abdc":"markdown","f2767393":"markdown","0c789b66":"markdown","6e8bef5d":"markdown"},"source":{"67e584f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n# !pip install altair vega_datasets notebook vega\nimport altair as alt\nalt.renderers.enable('kaggle')\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.tree import DecisionTreeClassifier\n!pip install dtreeviz\nfrom dtreeviz.trees import *\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","454ad1a3":"#passing index_col = 0 specifies that the first column in the data is the index, so just skip it \nspotify_df = pd.read_csv(\"..\/input\/spotifyclassification\/data.csv\", index_col = 0)","40409784":"\ntrain_df, test_df = train_test_split(spotify_df, test_size=0.2, random_state=123)","3db657a8":"train_df.head()","7985a731":"train_df.shape, test_df.shape","71493588":"train_df.info()","8b1e4997":"train_df.describe()","65dde8b6":"numeric_features = list(train_df.select_dtypes('number').columns)","99be1420":"# Let's plot each feature's distribution with respect to the predicted class\n\nalt.Chart(train_df).mark_bar(opacity = 0.6).encode(\n     alt.X(alt.repeat(), type='quantitative', bin=alt.Bin(maxbins=60)),\n     y=alt.Y('count()', stack=False),\n    color = 'target:N'\n).properties(\n    width=300,\n    height=200\n).repeat(\n    numeric_features, columns = 3\n) ","d57757fc":"corr_df = train_df.corr('spearman').stack().reset_index(name='corr')\ncorr_df.loc[corr_df['corr'] == 1, 'corr'] = 0  # Remove diagonal\ncorr_df['abs'] = corr_df['corr'].abs()\ncorr_df","2ed7d929":"alt.Chart(corr_df).mark_circle().encode(\n    x='level_0',\n    y='level_1',\n    size='abs',\n    color=alt.Color('corr', scale=alt.Scale(scheme='blueorange', domain=(-1, 1))))","110907f3":"X_train,y_train = train_df.drop(columns=[\"song_title\", \"artist\", \"target\"]), train_df['target']\nX_test, y_test = test_df.drop(columns=[\"song_title\", \"artist\", \"target\"]), test_df['target']\n","606759ac":"model = DecisionTreeClassifier()\nmodel_scores = pd.DataFrame(cross_validate(model, X_train, y_train, cv=10, return_train_score=True))\nmodel_scores","621f6aa7":"round(np.mean(model_scores['test_score']),3), round(np.mean(model_scores['train_score']),3)","d33a17bf":"results_dict = {\n    \"max_depth\": [],\n    \"mean_train_score\": [],\n    \"mean_cv_score\": []\n}\n\nfor depth in range(1, 26):\n    model = DecisionTreeClassifier(max_depth=depth)\n    cv_score = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n    results_dict[\"max_depth\"].append(depth)\n    results_dict[\"mean_cv_score\"].append( np.mean(cv_score[\"test_score\"]))\n    results_dict[\"mean_train_score\"].append( np.mean(cv_score[\"train_score\"]))\n\n    \nresult_df = pd.DataFrame(results_dict)\nresult_df\n\n","0ab3ebfd":"results = pd.melt(result_df, id_vars=['max_depth'], value_vars=['mean_train_score', 'mean_cv_score'])\nresults.head(4)","c5cc1f0e":"alt.Chart(results).mark_line().encode(\n    x=alt.X('max_depth', title= 'Depth of tree'),\n    y= alt.Y('value',title = 'Score', scale=alt.Scale(zero=False)),\n    color=alt.Color('variable', title='Score type')\n)","50bb72ea":"optimised_model = DecisionTreeClassifier(max_depth=4)\noptimised_scores = pd.DataFrame(cross_validate(optimised_model, X_train, y_train, cv =10, return_train_score = True))\n\n","d2e55808":"optimised_scores","dc3cf671":"optimised_model.fit(X_train, y_train)\ndt_viz = dtreeviz(optimised_model, \n               x_data=X_train,\n               y_data=y_train,\n               target_name='class',\n               feature_names=X_train.columns, \n               title=\"Decision Tree for Spotify Dataset\")\ndt_viz","654a02ac":"optimised_model = DecisionTreeClassifier(max_depth=4)\noptimised_model.fit(X_train, y_train)\n\nprint(f\"Score for test data: {round(optimised_model.score(X_test, y_test), 3)}\")","4a054713":"#### Some Observations\n- We can see that the training score has now reduced from 0.99 to around 0.75\n- Let's see how our model did the splitting on features with `max_depth`=4","c7890133":"### 5. Optimise Depth of Tree \n","1f77d198":"### 2. Split Data","c5f26a06":"We divide our data into training and test set in the ratio 80:20 respectively\n","815051c5":"Let's view high level summary statistics of our data","8957a40b":"### 6. Model with best depth\nNow, we will observe the scores with our best `max_depth` and ideally, it should now decrease the overfitting","03df6f15":"- Below, we visualise the distribution of each feature in our data. From the distributions we can find out which features are continuous, which are discrete, their skewness, modality, etc which helps us understand the data better\n- Decision Trees use histograms under the hood for to identify the best features and thresholds that are able to distinguish between the classes\n","e14ee8e2":"### 3. EDA","fa255fd0":"From the `optimised_scores` table above, we can see that the `validation score` is 0.714286 and the score on `test data` is 0.676, which provides us an estimate of how well our model will generalise on the deployment data","1021d873":"- In the plot above, we observe that for the feature `energy`, the threshold can be considered as 0.2 as it is clearly visible that below 0.2 energy value, more people tend to dislike songs(indicated by blue bars and very less orange bars). Similarly, we can observe for the feature `loudness`, a reasonable threshold would be something around -5 approximately, we can predict that more users like songs when `loudness` > -5 and dislike songs with `loudness` < -5\n- We can see bimodality for `tempo` when the user dislikes the songs(blue color bars), other features are mostly unimodal in nature","87254fba":"Let's try to see the validation and traininig scores for depths 1 to 25","5d9ac581":"Looking at `train_df.info()` gives us a rough idea about the features being numeric or textual and the number of `NaNs` in each feature","cd09f8c4":"#### Some Observations","a14e8e47":"#### Select the best max_depth of the tree\nAbove, we can see that the training score keeps increasing and gets stabilised when `max_depth` = 25, while the validation increases to a point and then starts decreasing. So, the best depth for the tree would be 4 as at `max_depth`=4, the vaidation score is highest","592aa088":"### 7. Model evaluation on Test set\n","c3ea46b7":"### 4. Model Building","0420a72e":"In this notebook, we try to understand how Decision Trees work on classification of the Spotify Attributes Dataset, how to determine features which are able to distinguish between classes, optimise the depth of the tree and visualise the optimised tree","29f0ca90":"#### Relationship between features\n- For finding out the relationship among features, we will use `Spearman Correlation`, instead of `Pearson`, as `Spearman correlation` helps in finding non-linear relationships as well","a049f974":"Luckily, we don't have any null values in our data, as we can see above","6cce5616":"#### Some Observations\n- We can see a lot of variance in the validation score from 0.61 to 0.73, so we looked at the mean value of the validation score, which is 0.676 and the mean of training score is 0.99. There is a lot of gap between the validation score and training score, which means our model is overfitting on the training data and also the training error is almost negligible, thus it is highly overfitting the data\n- So, let's try to prune the tree a but by changing the depth of the tree and try to optimise it","2ff8abdc":"### 1. Read Data","f2767393":"Let's plot a graph to see how the scores change with depth","0c789b66":"For simplicity, let's remove the text features from the data and just understand decision trees using numeirc features for now, as textual features need some preprocessing ","6e8bef5d":"#### Some Observations\n- We can see that there is no strong relation of any feature with our `target` feature\n- But, there is a positive correlation between `energy` and `loudness`, which seems quite reasonable and we have a negative correlation between acousticness and energy, which also makes sense, as music which is more acoustic tends to be a soft melody with less energy and misuc with high energy have less acousticness in it, but more loudness"}}