{"cell_type":{"249e016a":"code","3b1ee9fb":"code","bec4ed20":"code","3632be1e":"code","8dd76f94":"code","233cfdd0":"code","d6f8f4e4":"code","65893cdd":"code","9c2874ce":"code","6dec4390":"code","85da887c":"code","db34f5a3":"code","037c581b":"code","a3cd075e":"code","726c4958":"code","7de98a43":"code","999a954b":"code","f63ac889":"code","abd4145f":"code","38257778":"code","1845c09f":"code","81719ae3":"code","0837efd1":"code","dee2f732":"code","a04f77ff":"code","22e056c8":"code","b951125d":"code","5e23f77b":"code","da1d4628":"code","b0a1edb6":"code","44d7f1ab":"code","622d86dd":"code","b695e50f":"code","6e9e72a7":"code","117b2965":"code","689d7b77":"code","ac5168bf":"code","8dc925e6":"code","7a5eeabb":"code","a0ad39f5":"code","8bb4329e":"code","a851a011":"code","37b62358":"code","aab76a2f":"code","4a4c5a63":"code","3bc4b2d0":"code","61e38a63":"code","3275931a":"code","0d1a70f2":"code","d58c086e":"markdown","a0e80a20":"markdown","b504e41b":"markdown","881f7604":"markdown","da933858":"markdown","2e9fb7ff":"markdown","00930b27":"markdown","04973152":"markdown","f3189ee7":"markdown","60f70cbf":"markdown","af583e24":"markdown","60317458":"markdown","6bcafbb3":"markdown"},"source":{"249e016a":"# importing the required libraries \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom datetime import datetime # to tackle the date time features\nfrom datetime import date\nfrom sklearn.cluster import MiniBatchKMeans\nimport seaborn as sns #pretty plots\nimport warnings\nsns.set()\n\n","3b1ee9fb":"#reading the training and testing data and storing them as data and test respectively.\ndata =  pd.read_csv('..\/input\/new-york-city-taxi-with-osrm\/train.csv', parse_dates =['pickup_datetime'])\ndates=['pickup_datetime']\ntest = pd.read_csv('..\/input\/new-york-city-taxi-with-osrm\/test.csv',parse_dates = ['pickup_datetime'])\ndata.head(5)","bec4ed20":"# checking the types of columns and entries in the data and test.\ndata.info()\ntest.info()\n#using the describe() function to get and idea about different features and their stats\ndata.describe()","3632be1e":"#getting individual components of date time\n#converting store_and_fwd_flag to a 0 and 1 from a categorical data as only 2 possible values.\nfor df in (data,test):\n    df['year'] = df['pickup_datetime'].dt.year\n    df['month'] = df['pickup_datetime'].dt.month\n    df['day'] = df['pickup_datetime'].dt.day\n    df['hr'] = df['pickup_datetime'].dt.hour\n    df['minute'] = df['pickup_datetime'].dt.minute\n    #converting flag into 0 or 1\n    df['store_and_fwd_flag'] = 1 * (df.store_and_fwd_flag.values== 'Y')","8dd76f94":"\n \nprint(data[data['pickup_longitude']<-74.2].shape[0]) #points with longitude out of range, possible outliers\nprint((data['vendor_id']==2).mean())  #Distribution by vendor id\nprint((data['store_and_fwd_flag']==1).mean()) #distribution by store_and_fwd_flag\nprint(data[data['store_and_fwd_flag']==1].shape[0])\n","233cfdd0":"#converting trip duration to log(trip_duration +1) for rmsle\ndata = data.assign(log_trip_duration = np.log(data.trip_duration+1)) \n","d6f8f4e4":"#since ride duration will depend on holiday or workday etc importing nyc 2016 holiday dataset\nholiday = pd.read_csv('..\/input\/nyc2016holidays\/NYC_2016Holidays.csv', sep = ';') # secondary dataset for restdays\nholiday['Date'] = holiday['Date'].apply(lambda x: x+ '2016')\nholidays = [datetime]","65893cdd":"#storing restday features into new data frames (will merge them later)\ntime_data = pd.DataFrame(index = range(len(data)))  \ntime_test = pd.DataFrame(index = range(len(test)))","9c2874ce":"# we will list out the weekends and rest days(holidays)\n# will return if its a holiday or weekend\n\nfrom datetime import date\ndef find_holiday(yr, month, day, holidays):\n    holiday =  [None]*len(yr)\n    weekend =  [None]*len(yr)\n    i = 0 \n    for yy,mm,dd in zip(yr, month, day):\n        #checking for saturday sunday(6,7), date.isoweekday() returns the day\n        weekend[i] = date(yy,mm,dd).isoweekday() in (6,7)\n        holiday[i] = weekend[i] or date(yy,mm,dd)  in holidays\n        i+=1\n    return holiday, weekend\n    ","6dec4390":"#for training data, creating rest_day and weekend columns in a new dataframe (will merge later)\nrest_day,weekend = find_holiday(data.year, data.month, data.day, holidays)\ntime_data = time_data.assign(rest_day = rest_day)\ntime_data = time_data.assign(weekend = weekend)\n\n#replicating for test data\nrest_day,weekend = find_holiday(test.year, test.month, test.day, holidays)\ntime_test = time_test.assign(rest_day = rest_day)\ntime_test = time_test.assign(weekend = weekend)","85da887c":"#changing time into floats \ntime_data = time_data.assign(pickup_time = data.hr+data.minute\/60)\ntime_test = time_test.assign(pickup_time = test.hr+ test.minute\/60)","db34f5a3":"#using OSRM feature of fastroute (its split into 2 files)\nfastrout1 = pd.read_csv('..\/input\/new-york-city-taxi-with-osrm\/fastest_routes_train_part_1.csv', usecols=['id','total_distance','total_travel_time','number_of_steps','step_direction'])\nfastrout2 = pd.read_csv('..\/input\/new-york-city-taxi-with-osrm\/fastest_routes_train_part_2.csv', usecols=['id','total_distance','total_travel_time','number_of_steps','step_direction'])\n#merging data from the 2 fastrout files\nfastrout = pd.concat((fastrout1,fastrout2))\nfastrout.head()","037c581b":"#from the data calculating the number for strict lefts and rights\nright_turn = []\nleft_turn = []\nright_turn+= list(map(lambda x:x.count('right')-x.count('slight right'), fastrout.step_direction))\nleft_turn+= list(map(lambda x:x.count('left')-x.count('slight left'), fastrout.step_direction))","a3cd075e":"# storing the osrm features in a new dataframe (will merge later)\nosrm_data = fastrout[['id', 'total_distance', 'total_travel_time','number_of_steps']]\nosrm_data = osrm_data.assign(right_steps = right_turn)\nosrm_data = osrm_data.assign(left_steps = left_turn)\nosrm_data.head(3)","726c4958":"# making osrm data compatible to use by mapping ids\ndata = data.join(osrm_data.set_index('id'), on = 'id')\ndata.head(3)\n\n","7de98a43":"#replicating for test data\nosrm_test = pd.read_csv('..\/input\/new-york-city-taxi-with-osrm\/fastest_routes_test.csv')\nright_turn= list(map(lambda x:x.count('right')-x.count('slight right'),osrm_test.step_direction))\nleft_turn = list(map(lambda x:x.count('left')-x.count('slight left'),osrm_test.step_direction))\n\nosrm_test = osrm_test[['id','total_distance','total_travel_time','number_of_steps']]\nosrm_test = osrm_test.assign(right_steps=right_turn)\nosrm_test = osrm_test.assign(left_steps=left_turn)\nosrm_test.head(3)","999a954b":"test = test.join(osrm_test.set_index('id'), on='id')","f63ac889":"osrm_test.head(3)","abd4145f":"# final osrm features\nosrm_data = data[['total_distance','total_travel_time','number_of_steps','right_steps','left_steps']]\nosrm_test = test[['total_distance','total_travel_time','number_of_steps','right_steps','left_steps']]","38257778":"# DISTANCE METRICS\n#haversine manhattan and bearing distance\ndef haversine_array(lat1, lng1, lat2, lng2):\n    lat1,lng1,lat2,lng2 = map(np.radians, (lat1,lng1,lat2,lng2))\n    earth_radius = 6371 #km\n    lat = lat2-lat1\n    lng = lng2-lng1\n    d = np.sin(lat*0.5) **2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng *0.5)**2\n    h = 2 * earth_radius * np.arcsin(np.sqrt(d))\n    return h\n\ndef manhattan_distance(lat1, lat2, lng1, lng2):\n    a = haversine_array(lat1,lng1,lat1,lng2)\n    b = haversine_array(lat1,lng1,lat2,lng1)\n    return a+b\n\ndef bearing_array(lat1, lng1, lat2, lng2):\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))\n    \n    ","1845c09f":"#calculating and saving distance features in a new dataframe\n\nList_dist = []\nfor df in (data,test):\n    lat1, lng1, lat2, lng2 = (df['pickup_latitude'].values, df['pickup_longitude'].values, \n                              df['dropoff_latitude'].values,df['dropoff_longitude'].values)\n    dist = pd.DataFrame(index=range(len(df)))\n    dist = dist.assign(haversine_dist = haversine_array(lat1, lng1, lat2, lng2))\n    dist = dist.assign(manhattan_dist = manhattan_distance(lat1, lng1, lat2, lng2))\n    dist = dist.assign(bearing = bearing_array(lat1, lng1, lat2, lng2))\n    List_dist.append(dist)\nOther_dist_data,Other_dist_test = List_dist","81719ae3":"##### using k means to make clusters of locations\ncoord_pickup = np.vstack((data[['pickup_latitude', 'pickup_longitude']].values, test[['pickup_latitude','pickup_longitude']].values))\ncoord_dropoff = np.vstack((data[['dropoff_latitude', 'dropoff_longitude']].values, test[['dropoff_latitude','dropoff_longitude']].values))\n\n# creating a 4d data for k means\ncoords = np.hstack((coord_pickup,coord_dropoff))\nsample_ind = np.random.permutation(len(coords))[:500000]\nkmeans = MiniBatchKMeans(n_clusters=10, batch_size =10000).fit(coords[sample_ind])\nfor df in (data,test):\n    df.loc[:,'pickup_dropoff_loc'] = kmeans.predict(df[['pickup_latitude', 'pickup_longitude',\n                                                         'dropoff_latitude','dropoff_longitude']])","0837efd1":"# saving location clusters\nkmean_data = data[['pickup_dropoff_loc']]\nkmean_test = test[['pickup_dropoff_loc']]\n","dee2f732":"# WEATHER DATA\nweather = pd.read_csv('..\/input\/knycmetars2016\/KNYC_Metars.csv', parse_dates=['Time'])\nweather.head(5)","a04f77ff":"#possible events\nprint(set(weather.Events))\n#assuming snow to play a role with or without fog ","22e056c8":"weather['snow'] = 1*(weather.Events == 'Snow') + 1*(weather.Events == 'Fog\\n\\t,\\nSnow')\nweather['year'] = weather['Time'].dt.year\nweather['month'] = weather['Time'].dt.month\nweather['day'] = weather['Time'].dt.day\nweather['hr'] = weather['Time'].dt.hour\nweather = weather[weather['year']== 2016][['month','day','hr','Temp.','Precip','snow', 'Visibility']]","b951125d":"weather.head(3)","5e23f77b":"# merging weather data by mapping the datetime stamps\ndata = pd.merge(data, weather, on=['month', 'day', 'hr'], how = 'left')\ntest = pd.merge(test, weather, on=['month', 'day', 'hr'], how = 'left')","da1d4628":"#weather features stored in a new data frame\nweather_data = data[['Temp.','Precip','snow','Visibility']]\nweather_test = test[['Temp.','Precip','snow','Visibility']]","b0a1edb6":"weather_data.head()","44d7f1ab":"# Calculate the manhattan distance and speed\nmanhattan = manhattan_distance(data['pickup_latitude'],data['dropoff_latitude'],data['pickup_longitude'],data['dropoff_longitude'])\nmanhattan_speed = manhattan \/ data['trip_duration']\n\n# Plot speed against the time of day\nsns.set(font_scale=1.2)\nplt.figure(figsize=(12, 5))\nsns.boxplot(data['hr'], manhattan_speed, showfliers=False)\nplt.title('Average speeds vs. time of day')\nplt.ylabel('Average Speed')\nplt.xlabel('Hour')\nplt.show()","622d86dd":"# distributionof trip duration\nsns.set(font_scale=1.2)\nplt.figure(figsize=(6, 5))\nplt.hist(sorted(data['log_trip_duration']), bins=100, normed=True)\nplt.title('Distribution curve of rides w.r.t Duration(in log)')\nplt.ylabel('Ride Distribution')\nplt.xlabel('log_trip_duration')\nplt.show()","b695e50f":"#Workhours time\ntempdata = data\ntempdata = pd.concat([tempdata, time_data], axis=1)","6e9e72a7":"#fig = plt.figure(figsize = (18,8))\nsns.set(font_scale=1.2)\nplt.figure(figsize=(18, 8))\nsns.boxplot(x = \"hr\", y=\"log_trip_duration\", data = data, showfliers = False)\nplt.title(\"log_trip_duration vs hr of the day\")","117b2965":"#month vs trip_duration\nsns.violinplot(x = \"month\", y=\"log_trip_duration\", hue = \"rest_day\", data=tempdata, split=True, inner = \"quart\")\nplt.title(\"Comparision of log_trip_duration vs month for rest_day and working day\")","689d7b77":"#THE XGBOOST\n# dropping off the target variables\nmydf = data[['vendor_id','passenger_count','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','store_and_fwd_flag']]\ntestdf = test[['vendor_id','passenger_count','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','store_and_fwd_flag']]","ac5168bf":"#converting kmeans data into one hot encoding for XGBOOSTING\nkmeans_data = pd.get_dummies(kmean_data.pickup_dropoff_loc, prefix='loc', prefix_sep = '_')\nkmeans_test = pd.get_dummies(kmean_test.pickup_dropoff_loc, prefix='loc', prefix_sep = '_')","8dc925e6":"#merging the entire feature space\nmydf  = pd.concat([mydf  ,time_data,weather_data,osrm_data,Other_dist_data,kmeans_data],axis=1)\ntestdf= pd.concat([testdf,time_test,weather_test,osrm_test,Other_dist_test,kmeans_test],axis=1)","7a5eeabb":"# checking if test and training sets are similar\nprint(np.all(mydf.keys()==testdf.keys()))","a0ad39f5":"\nX = mydf\nz = data['log_trip_duration'].values\n","8bb4329e":"# verifying if the keys are similar\nprint(X.keys() == testdf.keys())","a851a011":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split","37b62358":"# 80-20 split for training and validation\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state =42)\n#50-50 validation set split\nXcv,Xv,Zcv,Zv = train_test_split(Xval, Zval, test_size=0.5, random_state=1)\n#DMatrix conversion for xgb\ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xcv   , label=Zcv)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]\n","aab76a2f":"parms = {'max_depth':15, #maximum depth of a tree 8 12\n         'objective':'reg:linear',\n         'eta'      :0.05, #0.3\n         'subsample':0.9,#SGD will use this percentage of data 0.8 0.99\n         'lambda '  :3, #L2 regularization term,>1 more conservative 4 \n         'colsample_bytree ':0.6, #0.9\n         'colsample_bylevel':0.7, #1 0.7\n         'min_child_weight': 0.5, #10 0.5\n         #'nthread'  :3 ... default is max cores\n         'eval_metric':'rmse'}  #number of cpu core to use\n# running for 2k iterations \nmodel = xgb.train(parms, data_tr, num_boost_round=2000, evals = evallist,\n                  early_stopping_rounds=50, maximize=False, \n                  verbose_eval=100)\n\n\n# The optimization function replacing GridSearchCV implemented with the help of the kaggle kernel of beluga\n'''                  \n\nFlag = True\nxgb_pars = []\nfor min_child_weight in [0.5, 1, 2, 4]:\n    for ETA in [0.05, 0.1, 0.15]:\n        for colsample_bytree in [0.5,0.6,0.7,0.8,0.9]:\n            for max_depth in [6, 8, 10, 12, 15]:\n                for subsample in [0.5, 0.6, 0.7, 0.8, 0.9]:\n                    for lambda in [0.5, 1., 1.5,  2., 3.,4.]:\n                        xgb_pars.append({'min_child_weight': min_child_weight, 'eta': ETA,\n                                         'colsample_bytree': colsample_bytree, 'max_depth': max_depth,\n                                         'subsample': subsample, 'lambda': lambda, \n                                         'eval_metric': 'rmse','silent': 1, 'objective': 'reg:linear'})\n\nwhile Flag:\n    xgb_par = np.random.choice(xgb_pars, 1)[0]\n    print(xgb_par)\n    model = xgb.train(xgb_par, data_tr, 2000, evallist, early_stopping_rounds=50,maximize=False, verbose_eval=100)\n    print('Modeling RMSLE %.6f' % model.best_score)\n'''\n#the final score achieved by the set parameters and the iterations required for achieveing it (early stopping) \n\nprint('score = %1.5f, n_boost_round =%d.'%(model.best_score,model.best_iteration))","4a4c5a63":"# Feature imporatnce graph using the xgb.plot_importance\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(model,ax = axes,height =0.5)\nplt.show();plt.close()","3bc4b2d0":"# prediction using the model from the test data\ndata_test = xgb.DMatrix(testdf)\nztest = model.predict(data_test)\n","61e38a63":"\nytest = np.exp(ztest)-1\nprint(ytest[:10])","3275931a":"#creating submission for kaggle\nsubmission = pd.DataFrame({'id': test.id, 'trip_duration': ytest})\nsubmission.to_csv('submission.csv', index=False)","0d1a70f2":"# model evaluation predicted vs actual values on validation set\npred_val = model.predict(xgb.DMatrix(Xval))\n\nsns.set(font_scale=1)\nplt.figure(figsize=(7, 7))\nplt.scatter(Zval, pred_val, s=3, color='red', alpha=0.025)\nplt.plot([1,9],[1,9], color='blue')\nplt.title('Predicted values vs actual values on the validation set')\nplt.xlabel('log(trip_duration+1)')\nplt.ylabel('Model predictions')\naxes = plt.gca()\naxes.set_xlim([2, 9])\naxes.set_ylim([2, 9])\nplt.show()\nplt.show()","d58c086e":"#### Distance Metrics","a0e80a20":"#### Considering the impact of OSRM features","b504e41b":"## PROBLEM STATEMENT\nThe dataset being used for the competition \u2018New York City Taxi Trip Duration\u2019 consist of the trip details\nfor about 1.4 million taxi rides between the years 2009 and 2015 in New York city.\nThe main aim of the challenge is to predict the duration of a taxi ride, which makes it a supervised regression problem to begin with. For this contest the evaluation metric is the Root Mean Squared Log Error.","881f7604":"#### Using k means to make clusters of locations","da933858":"## DATA VISUALIZATION\n","2e9fb7ff":"### MODEL EVALUATION","00930b27":"### TRAINING AND HYPERPARAMETER TUNING","04973152":"#### Considering the impact of holidays ","f3189ee7":"## LETS MODEL AWAY","60f70cbf":"## APPROACH \nMy approach to the problem is multifold in nature, \n- The first step being exploratory data analysis (EDA) where I explore the data to get an insight of the features and inputs existing which will lead to some visualizations for more information. \n\n- Post the EDA comes the step of feature engineering, this is the step where I use the secondary datasets and transform the data present in all datasets to create a relevant featurespace for my model. This also involves transforming the features to make it compatible with the chosen model.\n\n- Once the final feature space is obtained I will model a regressor (XGBoost here) to generate a score and a submission file for the competition. \n\n- This first draft will now undergo fine tuning and hyper-parameter tuning in order to further reduce the error and create my final submission file.","af583e24":"#### Considering Impact of Weather","60317458":"## FEATURE IMPORTANCE","6bcafbb3":"## EXPLORING THE DATA & FEATURE ENGINEERING"}}