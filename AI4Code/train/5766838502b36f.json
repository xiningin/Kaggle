{"cell_type":{"0ea89785":"code","27bb4dde":"code","699dabb1":"code","e96d4d46":"code","f055bd0e":"code","3c475d3a":"markdown"},"source":{"0ea89785":"from io import StringIO\nimport requests\nimport bs4\nfrom bs4 import BeautifulSoup\nimport pandas as pd","27bb4dde":"CSV_FOLDER='.\/'\n#These are a subset of books on the site, that I hoped would be similar enough to the contest data \n#to prove useful\nid_list=[\"2284\",\"22925\",\"13347\",\"18790\",\"18525\",\"22420\",\"10834\",\"24263\",\"22766\",\"24656\",\"24884\",\"24598\",\"24222\",\"16140\",\"20522\",\"18217\",\"16667\",\"20698\",\"23576\",\"20107\",\"1874\",\"17314\",\"770\",\"23625\",\"53386\",\"21687\",\"5128\",\"21752\",\"18019\",\"18505\",\"21748\",\"21737\",\"6952\",\"23388\",\"20163\",\"19140\",\"21750\",\"7882\",\"19988\",\"14964\",\"11151\",\"19154\",\"23267\",\"22224\",\"14890\",\"22464\",\"20043\",\"21731\",\"24124\",\"1952\",\"58585\",\"61\",\"58866\",\"1837\",\"21279\",\"146\",\"1228\",\"852\",\"65238\",\"61085\",\"22381\",\"910\",\"1754\",\"29558\",\"10947\",\"45631\",\"202\",\"23128\",\"45502\",\"32032\",\"65804\",\"65876\",\"2397\",\"65949\",\"242\",\"15489\",\"986\",\"37423\",\"3189\",\"25282\",\"56795\",\"4240\"]\nurl_list=[]\nurl_list_alt=[]\n#as of when this code was written, the site followed a naming pattern for its URL, \n#whereby valid URL formats for a given book could be either of the below.\nfor i in range(len(id_list)):\n  url_list.append(\"https:\/\/www.gutenberg.org\/files\/\" + id_list[i] + \"\/\" + id_list[i] + \".txt\")  \n  url_list_alt.append(\"https:\/\/www.gutenberg.org\/files\/\" + id_list[i] + \"\/\" + id_list[i] + \"-0.txt\")\n  ","699dabb1":"excerpts=[]\nweb_content=[]\nctr=0\nfor site in url_list:\n try:\n  req = requests.get(site)\n  print(f\"got text from {site} site {ctr+1}\")\n  html_str=str(req.content)\n  soup = BeautifulSoup(html_str,features=\"html.parser\")\n  url_content = soup.get_text()\n  #clean up text \n  url_content=url_content.replace(\"\\\\r\", \"\")\n  url_content=url_content.replace(\"\\\\n\", \"\")\n  url_content=url_content.replace(\"\\\\xef\", \"\")\n  url_content=url_content.replace(\"\\\\xbf\", \"\")\n  url_content=url_content.replace(\"\\\\\", \"\")\n  #replace multiple spaces with 1\n  url_content =(' '.join(url_content.split()))\n  words = url_content.split()\n  content_len=len(words)\n  #if len==148 this indicates that an error was returned so try\n    #alternate site\n  if(content_len==148):\n     print(f\"trying alternate site {url_list_alt[ctr]}\")\n     req = requests.get(url_list_alt[ctr])\n     print(f\"got text from {url_list_alt[ctr]}\")\n     html_str=str(req.content)\n     soup = BeautifulSoup(html_str,features=\"html.parser\")\n     url_content = soup.get_text()\n     url_content=url_content.replace(\"\\\\r\", \"\")\n     url_content=url_content.replace(\"\\\\n\", \"\")\n     url_content=url_content.replace(\"\\\\xef\", \"\")\n     url_content=url_content.replace(\"\\\\xbf\", \"\")\n     url_content=url_content.replace(\"\\\\\", \"\")\n     #replace multiple spaces with 1\n     url_content =(' '.join(url_content.split()))\n     words = url_content.split()\n     content_len=len(words)\n     soup = BeautifulSoup(html_str,features=\"html.parser\")\n     url_content = soup.get_text()\n     url_content=url_content.replace(\"\\\\r\", \"\")\n     url_content=url_content.replace(\"\\\\n\", \"\")\n     url_content=url_content.replace(\"\\\\xef\", \"\")\n     url_content=url_content.replace(\"\\\\xbf\", \"\")\n     url_content=url_content.replace(\"\\\\\", \"\")\n  #replace multiple spaces with 1\n     url_content =(' '.join(url_content.split()))\n     words = url_content.split()\n     content_len=len(words)\n  #divide into chunks of 300 and discard first two and final two chunks, since\n    #the beginning and end of text usually contain verbiage not in the book\n  for i in range(2, content_len-2, 300):\n      if(i+300 < content_len-2):\n           #print(\"got excerpt in middle\")\n           excerpt=(' '.join(words[i:i+300]))\n          \n      else:\n           excerpt=(' '.join(words[i:content_len-2]))\n      excerpts.append(excerpt) \n      \n except:\n   print(f\"fail for {site}\")\n ctr+=1","e96d4d46":"CSV_FOLDER='.\/'\n\n#These are unique ids of books that based on a quick review, I thought to be most\n#relevant to the CLRP competition.\n\nid_list=[\"2284\",\"22925\",\"13347\",\"18790\",\"18525\",\"22420\",\"10834\",\"24263\",\"22766\",\"24656\",\"24884\",\"24598\",\"24222\",\"16140\",\"20522\",\"18217\",\"16667\",\"20698\",\"23576\",\"20107\",\"1874\",\"17314\",\"770\",\"23625\",\"53386\",\"21687\",\"5128\",\"21752\",\"18019\",\"18505\",\"21748\",\"21737\",\"6952\",\"23388\",\"20163\",\"19140\",\"21750\",\"7882\",\"19988\",\"14964\",\"11151\",\"19154\",\"23267\",\"22224\",\"14890\",\"22464\",\"20043\",\"21731\",\"24124\",\"1952\",\"58585\",\"61\",\"58866\",\"1837\",\"21279\",\"146\",\"1228\",\"852\",\"65238\",\"61085\",\"22381\",\"910\",\"1754\",\"29558\",\"10947\",\"45631\",\"202\",\"23128\",\"45502\",\"32032\",\"65804\",\"65876\",\"2397\",\"65949\",\"242\",\"15489\",\"986\",\"37423\",\"3189\",\"25282\",\"56795\",\"4240\"]\nurl_list=[]\nurl_list_alt=[]\n#I found that the books were available in one location or the other on the remote site.\nfor i in range(len(id_list)):\n  url_list_alt.append(\"https:\/\/www.gutenberg.org\/files\/\" + id_list[i] + \"\/\" + id_list[i] + \"-0.txt\")\n  url_list.append(\"https:\/\/www.gutenberg.org\/files\/\" + id_list[i] + \"\/\" + id_list[i] + \".txt\")","f055bd0e":"df = pd.DataFrame(excerpts)\ndf.columns=['excerpt']\n#BELOW IS UNCOMMENTED IN PRODUCTION VERSION OF THE SCRIPT\n#df.to_csv(CSV_FOLDER + 'books.csv')\n","3c475d3a":"This is a utility script used as part of a submission to the [Common Lit Readability Prize contest](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize). It generates additional data for training transfomer models that are in turn used to generate predicted scores for that contest. Also, perhaps, it is useful as an example of how to fetch and save data from web sites.\n\nThe data generated by this notebook is used by:\n\n* [Train Roberta Base Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-train-robertabase-maskedlm-model)\n* [Train Roberta Large Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-train-robertalarge-masked-lm-model\/)\n\nI chose not to make the data generated by this script public so as not to violate any copyrights.  "}}