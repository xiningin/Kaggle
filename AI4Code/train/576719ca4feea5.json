{"cell_type":{"38f68257":"code","c8a941e3":"code","98b93092":"code","95a68c7e":"code","b4857238":"code","b022aad7":"code","85d24114":"code","e9f1f9f9":"code","268cac96":"code","5b199b39":"code","e3787e4c":"code","aaa473fa":"code","ce442f73":"code","e190040b":"code","34506777":"code","b46e13d1":"code","f1d8eafc":"code","a1071793":"code","23caeb10":"markdown","524da4c4":"markdown","4401c67b":"markdown","d287dc1f":"markdown","71bab0b7":"markdown","38ef31ba":"markdown","08d1e6f8":"markdown"},"source":{"38f68257":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn import linear_model\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport utils\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c8a941e3":"df = pd.read_csv('..\/input\/train.csv')\ndf_t = pd.read_csv('..\/input\/test.csv')","98b93092":"df.count()","95a68c7e":"df.head(3)","b4857238":"df_t.head(3)","b022aad7":"import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(18, 6)) # create figure\n\nplt.subplot2grid((2,3), (0,0))\ndf.Survived.value_counts(normalize=True).plot(kind='bar', alpha=0.5) # data that we want to plot\nplt.title(\"Survived\")\n\nplt.subplot2grid((2,3), (0,1))\nplt.scatter(df.Survived, df.Age, alpha=0.1)\nplt.title(\"Age-Survived\")\n\nplt.subplot2grid((2,3), (0,2))\ndf.Pclass.value_counts(normalize=True).plot(kind='bar', alpha = 0.5)\nplt.title(\"Classes\")\n\nplt.subplot2grid((2,3), (1,0), colspan=2)\nfor x in [1,2,3]:\n    # find list of age where Pclass value is 1,2,3\n    df.Age[df.Pclass == x].plot(kind=\"kde\")\nplt.title(\"Class-Age\")\nplt.legend((\"1\", \"2\", \"3\"))\n\n# where did the passengers got on the ship (3 different locations)\nplt.subplot2grid((2,3), (1,2))\ndf.Embarked.value_counts(normalize=True).plot(kind='bar', alpha=0.5)\nplt.title(\"Embarked\")\n\nplt.show() # show figure","85d24114":"df.Survived.value_counts()","e9f1f9f9":"fig = plt.figure(figsize=(18,6))\n\nplt.subplot2grid((3,4), (0,0))\ndf.Survived.value_counts(normalize=True).plot(kind='bar', alpha=0.5)\nplt.title(\"Survived (regardless of sex)\")\n\nplt.subplot2grid((3,4), (0,1))\ndf.Survived[df.Sex==\"male\"].value_counts(normalize=True).plot(kind=\"bar\", alpha=0.5)\nplt.title(\"men survived\")\n\nplt.subplot2grid((3,4), (0,2))\ndf.Survived[df.Sex==\"female\"].value_counts(normalize=True).plot(kind=\"bar\", alpha=0.5)\nplt.title(\"women survived\")\n\nplt.subplot2grid((3,4), (0,3))\ndf.Sex[df.Survived==1].value_counts(normalize=True).plot(kind=\"bar\", alpha=0.5)\nplt.title(\"sex of survived\")\n\nplt.subplot2grid((3,4), (1,0), colspan=3)\nfor x in [1,2,3]:\n    df.Survived[df.Pclass == x].plot(kind=\"kde\")\nplt.title(\"class-survived\")\nplt.legend((\"1\", \"2\", \"3\"))\n\nplt.subplot2grid((3,4), (2,0))\ndf.Survived[(df.Sex =='male') & (df.Pclass == 1)].value_counts(normalize=True).plot(kind=\"bar\")\nplt.title(\"Rich men survived\")\n\nplt.subplot2grid((3,4), (2,1))\ndf.Survived[(df.Sex =='male') & (df.Pclass == 3)].value_counts(normalize=True).plot(kind=\"bar\")\nplt.title(\"Poor men survived\")\n\nplt.subplot2grid((3,4), (2,2))\ndf.Survived[(df.Sex =='female') & (df.Pclass == 1)].value_counts(normalize=True).plot(kind=\"bar\")\nplt.title(\"Rich women survived\")\n\nplt.subplot2grid((3,4), (2,3))\ndf.Survived[(df.Sex =='female') & (df.Pclass == 3)].value_counts(normalize=True).plot(kind=\"bar\")\nplt.title(\"Poor women survived\")\n\nplt.show()","268cac96":"df_t.head()","5b199b39":"# clean data and fill empty columns\ndf[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].dropna().median())\ndf[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].dropna().median())\n\n# encode categorical data into one hot vectors\nlabelencoder_1 = LabelEncoder()\ndf['Sex'] = labelencoder_1.fit_transform(df['Sex'])\n# label data : 0,1,2,..\nlabelencoder_2 = LabelEncoder()\ndf['Embarked'] = labelencoder_2.fit_transform(df['Embarked'].astype(str))\n# slect X, Y\nY = df.iloc[:,1]\nX = df.iloc[:, [2,4,5,6,7,9,11]]\n# encode the data\nonehotencoder = OneHotEncoder(categorical_features=[-1])\nX = onehotencoder.fit_transform(X).toarray()\n\n# scale the data\nsc = StandardScaler()\nX = sc.fit_transform(X)","e3787e4c":"classifier = linear_model.LogisticRegression()\nclassifier.fit(X, Y)","aaa473fa":"classifier.score(X, Y)","ce442f73":"print(classifier.coef_)\n# pd.DataFrame(classifier.coef_, columns=['Coeff'])","e190040b":"dtree = DecisionTreeClassifier()\ndtree.fit(X, Y)\ndtree.score(X, Y)\n\naccuracies = cross_val_score(estimator = dtree, X = X, y = Y, cv=10, n_jobs=1, scoring='accuracy')\nprint(accuracies)\n\nmean = accuracies.mean()\n# if we have high variance between different K-fold sets its a sign of overfitting in our training set\nvariance = accuracies.std()  \nprint(mean, variance)","34506777":"# use gridsearch to find better parameters\ndepths = np.arange(1, 10)\nnum_leafs = [2, 3, 5, 10, 20, 25]\n\nparam_grid = [{'max_depth':depths,\n              'max_leaf_nodes':num_leafs,\n               'min_samples_split':num_leafs\n              }]\n\ngrid = GridSearchCV(DecisionTreeClassifier(), param_grid, scoring=\"accuracy\", cv=3)\ngrid.fit(X, Y)\n\nbest_accuracy = grid.best_score_\nbest_parameters = grid.best_params_\n\nprint(best_parameters, best_accuracy)","b46e13d1":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout \nfrom keras.wrappers.scikit_learn import KerasClassifier","f1d8eafc":"classifier = Sequential()\nclassifier.add(Dense(output_dim=100, init='uniform', activation='relu', input_dim=10))\nclassifier.add(Dropout(p=0.4))\nclassifier.add(Dense(output_dim=50, init='uniform', activation='relu'))\nclassifier.add(Dropout(p=0.2))\nclassifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\nclassifier.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n","a1071793":"classifier.fit(X, Y,batch_size=10, epochs=100, validation_split=0.2)","23caeb10":"Positive means when this value increses, we also increase chace of being survived,<br>\nnegative means when this value increses, we decrease chace of being survived","524da4c4":"* **Logistic Regression**","4401c67b":"* **Neural Network**","d287dc1f":"We can see thst 40% of people survived while around 60% dies <br>\n<br>\nand bulk of both survived and died people were between 20-60 while some younger people<br>\nsurvived and older people died, but there isn't major age difference here.<br>\n<br>\nHalf of passengers where third class and around 25% were first and second class","71bab0b7":"We can see that while only 25% of men survived, for women the number is around 70%<br>\nAlso while most of people in passenger class 3 didn't survive,","38ef31ba":"* **Decision Tree**","08d1e6f8":"* As it can be seen with a ANN network we get the result of around 89% accuracy"}}