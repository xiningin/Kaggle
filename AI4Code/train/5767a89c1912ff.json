{"cell_type":{"cff49689":"code","3545f0e3":"code","d3e2016c":"code","ee396ef4":"code","bd592a00":"code","dcc1b0b6":"code","60fb2a40":"code","0cc0572d":"code","f0b13955":"code","dc8a4ee9":"code","e0f89b10":"code","eeb088a3":"code","3df6f32c":"code","d5cd15a1":"code","0f9a316c":"code","aff8bd53":"code","eea6ef18":"code","c2cc2c49":"code","c37b404c":"code","23f79314":"code","032cb5fb":"code","b9688bb2":"code","b85adae4":"code","4b24d895":"code","e555cf34":"code","c7c1103b":"code","691e9b9c":"code","8e9cfb75":"code","a940f239":"code","f86bef40":"code","36f284c6":"code","b84bc939":"code","ae48086d":"code","246f36d3":"code","e27652fd":"code","f30e3262":"code","e92103cd":"code","0eef59b2":"code","c499498d":"code","807fd45a":"code","81ecb720":"code","c19052d8":"code","00a37db0":"markdown","ac5f9f74":"markdown","7e535f05":"markdown","d838119d":"markdown","70da1ad8":"markdown","f791cacd":"markdown","18b73df0":"markdown","8561116b":"markdown"},"source":{"cff49689":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd \n\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3545f0e3":"data = pd.read_csv(\"\/kaggle\/input\/diabetes\/diabetes.csv\")","d3e2016c":"data.head()","ee396ef4":"data.info()","bd592a00":"data.nunique()   #Unique degerler","dcc1b0b6":"data.describe().T","60fb2a40":"print(\"satir ve sutun = \", data.shape)\nprint(\"Boyut sayisi = \",data.ndim)\nprint(\"eleman sayisi = \",data.size)    # neden farkli bilmiyorum? ","0cc0572d":"data.corr()","f0b13955":"# Heatmap correlation cizimi\ndata.corr()\n\n#correlation map\nf,ax = plt.subplots(figsize=(10, 6))\nsns.heatmap(data.corr(), annot=True, linewidths=0.5,linecolor=\"yellow\", fmt= '.1f',ax=ax) #ftm noktadan sonra kac hane olacak onu verir\nplt.show()","dc8a4ee9":"#kac adet 1 ve 0 var\n\nsns.countplot(data.Outcome)\n#sns.countplot(kill.manner_of_death)\nplt.title(\"Outcome\",color = 'blue',fontsize=15)\nplt.show()","e0f89b10":"data.isnull().sum()   # eksik gozlem var mi? ","eeb088a3":"import missingno as msno\nmsno.matrix(data)\nplt.show()","3df6f32c":"x = data.drop([\"Outcome\"], axis=1)  #\"Outcome\" disindaki sutunlar bagimsiz degisken_\ny = data[\"Outcome\"]                 #\"Outcome\" ise bagimli degiskendir,   ","d5cd15a1":"#x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","0f9a316c":"x.describe().T","aff8bd53":"# Test Train \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)","eea6ef18":"from sklearn.linear_model import LogisticRegression\nloj_reg = LogisticRegression(penalty='l1').fit(x_train,y_train)\nloj_reg\n\n#Not penalty= default olursa skor dusuyor.","c2cc2c49":"#?loj_reg","c37b404c":"print(\"b0 = \",loj_reg.intercept_)   # b0 yani sabit degerimizi aldik\nprint(\"Coefs = \",loj_reg.coef_)        # bagimsiz degiskenlerin katsayi degerlerini aldik","23f79314":"y_pred = loj_reg.predict(x_test)     # model uzerinden tahmin etme yaptik","032cb5fb":"loj_reg.predict_proba(x_train)[0:10]  #ikili cikti uretir, 0 ve 1 oranlarini verir.\n\n# sadece 1 ve sifir olarak donmesini degilde, bunlarin olasilik degerlerini ogrenmek istedik.","b9688bb2":"log_score = accuracy_score(y_test, y_pred)\nprint(\"Logistic_reg_class_SCORE = \",log_score)       \n# modeldeki gercek 0-1 ile tahmindeki 0-1 oranlarini karsilastirip, dogru siniflandirma oranimizdir.","b85adae4":"from sklearn.ensemble import RandomForestClassifier","4b24d895":"rf_model = RandomForestClassifier(n_estimators=22).fit(x_train, y_train)\nrf_model\n\n#n_estimators=10 defaultur. 22 da daha yuksek skor aldim","e555cf34":"y_pred = rf_model.predict(x_test)\nrf_score = accuracy_score(y_test, y_pred)\nprint(\"random_forest_class_SCORE = \",rf_score)       \n# modeldeki gercek 0-1 ile tahmindeki 0-1 oranlarini karsilastirip, dogru siniflandirma oranimizdir.","c7c1103b":"Importance = pd.DataFrame({\"Importance\": rf_model.feature_importances_*100},\n                         index = x_train.columns)\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"g\")\n\nplt.xlabel(\"De\u011fi\u015fken \u00d6nem D\u00fczeyleri\")","691e9b9c":"knn = KNeighborsClassifier( n_neighbors=4).fit(x_train, y_train)\nknn","8e9cfb75":"y_pred = knn.predict(x_test)\nknn_score = accuracy_score(y_test, y_pred)\nprint(\"KNN_class_SCORE = \", knn_score)       \n# modeldeki gercek 0-1 ile tahmindeki 0-1 oranlarini karsilastirip, dogru siniflandirma oranimizdir.","a940f239":"y_probs = knn.predict_proba(x_test)     # tekrardan 0 ve 1'in olasilik degerlerini bu degiskene atadik.\ny_probs = y_probs[:,1] \ny_pred = [1 if i > 0.75 else 0 for i in y_probs] \n\n\n# if ve for donguleri ile \"1\" in 0.5'den buyukse 1 diye siniflandirmasini, degilse \"0\" diye siniflandirmasini istedik. \n# ..ve tekrar y_pred seklinde yeni tahminlerimizi atadik\n","f86bef40":"y_pred = [1 if i > 0.75 else 0 for i in y_probs] ","36f284c6":"accuracy_score(y_test, y_pred)    # degistirilen diagnosisden sonraki skorumuz","b84bc939":"svm_model = SVC(C=5, degree=7, kernel='linear' ).fit(x_train, y_train)\nsvm_model\n\n# C=5, degree=7, kernel='linear'yaptim. def degil","ae48086d":"y_pred = svm_model.predict(x_test)\nsvm_score = accuracy_score(y_test, y_pred)\nprint(\"SVM_class_SCORE = \", svm_score)       \n# modeldeki gercek 0-1 ile tahmindeki 0-1 oranlarini karsilastirip, dogru siniflandirma oranimizdir.","246f36d3":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb_model = nb.fit(x_train, y_train)\nnb_model","e27652fd":"nb_model.predict(x_test)[:10]","f30e3262":"nb_model.predict_proba(x_test)[0:10]  \n\n# ! Tahmini olasilik degerleridir. Ilk sutun \"0\", ikinci sutun \"1\" i ifade eder.","e92103cd":"y_pred = nb_model.predict(x_test)\nnb_score = accuracy_score(y_test, y_pred)\nprint(\"NB_class_SCORE = \", nb_score)       \n# modeldeki gercek 0-1 ile tahmindeki 0-1 oranlarini karsilastirip, dogru siniflandirma oranimizdir.","0eef59b2":"cross_val_score(nb_model, x_test, y_test, cv = 20).mean()  \n\n#Dogrulanmis test hatamiz, 20 katmanli cross valide edilmis test hatasi ortalamasidir.","c499498d":"from sklearn.tree import DecisionTreeClassifier\ndec_tree = DecisionTreeClassifier().fit(x_train, y_train)\ndec_tree\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\ny_pred = dec_tree.predict(x_test)\ndt_score = accuracy_score(y_test, y_pred)\nprint(\"DT_class_SCORE = \", dt_score)       \n# modeldeki gercek 0-1 ile tahmindeki 0-1 oranlarini karsilastirip, dogru siniflandirma oranimizdir.","807fd45a":"# Model Skorlarinin  Seaborn ile gorsellestirilmesi\n\nindexx = [\"Log\",\"RF\",\"KNN\",\"SVM\",\"NB\"]\nregressions = [log_score,rf_score,knn_score,svm_score,nb_score]\n\nplt.figure(figsize=(8,6))\nsns.barplot(x=indexx,y=regressions)\nplt.xticks()\nplt.title('Model Compare',color = 'orange',fontsize=20)\nplt.show()","81ecb720":"## conda install -c plotly plotly chart-studio","c19052d8":"from plotly.plotly import iplot\nimport plotly.graph_objs as go\nimport chart_studio.plotly as py\n\n\nindexx = [\"Log\",\"RF\",\"KNN\",\"SVM\",\"NB\"]\nregressions = [log_score,rf_score,knn_score,svm_score,nb_score]\n# creating trace1\ntrace1 =go.Scatter(\n                    x = indexx,\n                    y = regressions,\n                    mode = \"lines+markers+text\",\n                    name = \"#\",\n                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n                    text= indexx) #uzerine geldiginde ne gorunsun.\n\ndata = [trace1]  # olusturdugumuz veriler listeye atadik\n\n# konumlandirmayi yapar ve isimlendirir.(layout)\nlayout = dict(title = 'Model Compare',\n              xaxis= dict(title= 'Models',ticklen= 15,zeroline= True),\n              yaxis= dict(title= 'Scores',ticklen= 15,zeroline= True)\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","00a37db0":"En yuksek skoru veren model random foresttir","ac5f9f74":"# 1) Logistic Regression","7e535f05":"n_estimators=10 >> fit edilecek agac sayisidir, yani 10 adet agac tahmini olusturulacak,\nmax_features='auto'> > Bolunme islemlerinde goz onunde bulundurulacak olan maximum degisken sayisini verir,\nmin_samples_split=2 > > Bu bir node bolunmeden once minimum gozlem sayisini ifade eder,\nmin_samples_leaf=1 > > leaf node ' taki minimum gozlem sayisini ifade eder,","d838119d":"# 4) SVM - Support Vector Model\n\nAmac, iki sinif arasrindaki ayrimin optimum olmasini saglayacak hiper-duzlemi bulmaktir.\n\nyani 1 ve 0 arasindaki mesafenin maximum tutmaya calisir. ayrimin belirgin olmasini saglamaya calisir.","70da1ad8":"# 5) Gaussian Naive Bayes Model\nOlasilik temelli bir modelleme teknigidir. Amac belirli bir ornegin her bir sinifa ait olma olasiliginin kosullu olasilik temelli hesaplanmasidir. Cok sinifli degiskenlerde bu model daha iyi sonuclar verebilir. cok daha fazla katogorik degisken varsa kullanilmasi uygun olabilir","f791cacd":" # 3)KNN Model\n Tahminler gozlem benzerligine gore yapilir\n\nk = komsu sayisini belirler\nBilinmeyen noktalar ile diger tum noktalar arasindaki mesafeyi hesaplar\nbelirlenen k ile kendisine en yakin k kadar gozlemi secer.","18b73df0":"#  6) Decision Tree Siniflandirma****","8561116b":" # 2) Random Forest\n*  Temeli birden cok karar agacinin urettigi tahminlerin bir araya getirilerek degerlendirilmesine dayanir. Gozlemlerde ve degiskenlerde rastsallik saglar, "}}