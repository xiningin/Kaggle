{"cell_type":{"2a8da50a":"code","8ba6d9c8":"code","32131dd5":"code","7b90d90f":"code","4b92fcbb":"code","e35144b9":"code","0c49d313":"code","654da7a6":"code","f2fae392":"code","6657107e":"code","fb9aac9d":"code","ec90b99a":"code","542bcff6":"code","ddfe6fe9":"code","196c8439":"code","0258a25b":"code","e46860ff":"code","3dc63835":"code","99cb6a7e":"code","b2927073":"code","852b140f":"code","567df9ec":"code","6c25c39c":"code","ed0633e9":"code","9266374e":"code","8fd0095e":"code","7c3686df":"code","3dc211dc":"code","0bf6da04":"code","f3f12387":"code","94f73d8b":"code","26cbf606":"code","938f0cb4":"code","04d5afa8":"code","c539ba3b":"code","decefe21":"code","484ce9af":"code","4fd455e8":"code","7e491963":"code","7ef61dfe":"code","6b318f1a":"code","fc75893a":"code","4815a64e":"code","410d744e":"code","729ea0b5":"code","97938ccd":"code","3090cb77":"code","e04ea560":"markdown","a50a6aa3":"markdown","d5743b4c":"markdown","c55531d5":"markdown","33eb4ce5":"markdown"},"source":{"2a8da50a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ba6d9c8":"import seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","32131dd5":"train_data=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\",encoding=\"latin1\")\ntest_data=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\",encoding=\"latin1\")","7b90d90f":"train_data.shape","4b92fcbb":"test_data.shape","e35144b9":"train_data.head()","0c49d313":"test_data.head()","654da7a6":"Id=test_data.id","f2fae392":"train_data.isnull().sum()","6657107e":"test_data.isnull().sum()","fb9aac9d":"train_data.describe()","ec90b99a":"import re \ntrain_data['text'] = [re.sub(\"[^a-zA-Z]\",\" \",text).lower() for text in train_data['text']]\ntest_data['text'] = [re.sub(\"[^a-zA-Z]\",\" \",text).lower() for text in test_data['text']]","542bcff6":"train_data.head()","ddfe6fe9":"test_data.head()","196c8439":"from textblob import TextBlob\n\ndef polarity_check_label(dataframe):\n    polarity_list = []\n    \n    for text in dataframe['text']:\n        polarity_point = TextBlob(text).sentiment.polarity\n        if polarity_point < 0 : polarity_state= 'Negative'\n        elif polarity_point == 0 : polarity_state = 'Neutral'\n        else : polarity_state = 'Positive'\n        polarity_list.append(polarity_state)\n        \n    dataframe['polarity_state'] = polarity_list\n    \npolarity_check_label(train_data)\npolarity_check_label(test_data)","0258a25b":"import nltk # natural language tool kit for word_tokenize ...\nnltk.download(\"stopwords\")      # stopwords is download in corpus directory\nfrom nltk.corpus import stopwords  # import stopwords","e46860ff":"train_data['text'] = [nltk.word_tokenize(text) for text in train_data['text']]\ntest_data['text'] = [nltk.word_tokenize(text) for text in test_data['text']]\n\ntrain_data.text.head()","3dc63835":"test_data.text.head()","99cb6a7e":"def lemma_and_join(dataframe):\n    lemma = nltk.WordNetLemmatizer()\n    text_list = []\n    for text in dataframe['text']:\n        text = [ word for word in text if not word in set(stopwords.words(\"english\"))]\n        text = [lemma.lemmatize(word) for word in text]\n        text = \" \".join(text)\n        text_list.append(text)\n    return text_list","b2927073":"train_data['text'] = lemma_and_join(train_data)\ntest_data['text'] = lemma_and_join(test_data)\ntrain_data.head()","852b140f":"raw_loc = train_data.location.value_counts()\ntop_loc_disaster = list(raw_loc[raw_loc>=10].index)\ntop_only_disaster = train_data[train_data.location.isin(top_loc_disaster)]\n\ntop_location = top_only_disaster.groupby('location')['target'].mean().sort_values(ascending=False)\nsns.barplot(x=top_location.index, y=top_location)\nplt.xticks(rotation=90)\nplt.show()","567df9ec":"len(set(train_data['location'])) ","6c25c39c":"def clean_location(x):\n    if x == 'None':\n        return 'None'\n    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    elif x in top_location:\n        return x\n    else: \n        return 'Others'\n    \ntrain_data['location'] = train_data['location'].apply(lambda x: clean_location(str(x)))\ntest_data['location'] = test_data['location'].apply(lambda x: clean_location(str(x)))","ed0633e9":"top_location = train_data.groupby('location')['target'].mean().sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_location.index, y=top_location)\nplt.xticks(rotation=90)\nplt.show()","9266374e":"len(set(train_data['location'])) ","8fd0095e":"len(set(test_data['location'])) ","7c3686df":"freq_df = train_data['text'].apply(lambda x:pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\nfreq_df.columns = ['words', 'frequences']\nfreq_df.sort_values('frequences',ascending=False)","3dc211dc":"from sklearn.feature_extraction.text import CountVectorizer\nmax_features=5000\n\ncount_vectorizer = CountVectorizer(max_features=max_features)\n\nsparce_matrix_train=count_vectorizer.fit_transform(train_data['text'])\nsparce_matrix_test=count_vectorizer.fit_transform(test_data['text'])\n\nprint(\"{} most used words: {} \".format(max_features,count_vectorizer.get_feature_names()))","0bf6da04":"from textblob import TextBlob\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble","f3f12387":"import category_encoders as ce\n\nfeatures = ['keyword', 'location']\nencoder = ce.TargetEncoder(cols=features)\nencoder.fit(train_data[features],train_data['target'])\n\ntrain_data = train_data.join(encoder.transform(train_data[features]).add_suffix('_target'))\ntest_data = test_data.join(encoder.transform(test_data[features]).add_suffix('_target'))\n","94f73d8b":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \ntext_vec = vec_text.fit_transform(train_data['text'])\ntext_vec_test = vec_text.transform(test_data['text'])\nX_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\nX_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())","26cbf606":"train_data = train_data.join(X_train_text, rsuffix='_text')\ntest_data=test_data.join(X_test_text,rsuffix='_text')","938f0cb4":"from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,precision_score,recall_score\n","04d5afa8":"y = train_data.target\nx = train_data.drop(columns = ['id','keyword', 'location', 'text','polarity_state','target'])\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3)","c539ba3b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier","decefe21":"f1_score_list=[]\ntrain_accuracy_list=[]\ntest_accuracy_list = []\nclassifier_list = []\ndef fit_and_predict(model,x_train,x_test,y_train,y_test):\n    \n    classifier = model\n    classifier.fit(x_train,y_train)\n    y_pred = classifier.predict(x_test)\n    cmatrix = confusion_matrix(y_test,y_pred)\n    \n    \n    f,ax = plt.subplots(figsize=(3,3))\n    sns.heatmap(cmatrix,annot=True,linewidths=0.5,cbar=False,linecolor=\"red\",fmt='.0f',ax=ax)\n    plt.xlabel(\"y_predict\")\n    plt.ylabel(\"y_true\")\n    ax.set(title=str(classifier))\n    plt.show()\n    \n    \n    f1score = f1_score(y_test,y_pred,average='weighted')\n    train_accuracy = round(classifier.score(x_train,y_train)*100)\n    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)\n    \n    classifier_list.append(str(classifier))\n    train_accuracy_list.append(str(train_accuracy))\n    test_accuracy_list.append(str(test_accuracy))\n    f1_score_list.append(str(round(f1score*100)))\n    \n    \n    print(classification_report(y_test,y_pred))\n    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))\n    print('-'*50)\n    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))\n","484ce9af":"x_train.shape","4fd455e8":"x_test.shape","7e491963":"random_state = 29\nmodels=[\n        LogisticRegression(random_state=29),\n        SVC(random_state=random_state),\n        MultinomialNB(),\n        DecisionTreeClassifier(random_state = 29),\n        KNeighborsClassifier(),\n        RandomForestClassifier(random_state=29),\n       ]","7ef61dfe":"for model in models:\n    fit_and_predict(model,x_train,x_test,y_train,y_test)","6b318f1a":"columns=x_train.columns\ncolumns","fc75893a":"test = test_data.reindex(columns = columns, fill_value=0)","4815a64e":"test.shape","410d744e":"test.columns","729ea0b5":"pred = model.predict(test)\npred.shape","97938ccd":"#submission = pd.DataFrame(data=pred,columns=['Traget'])","3090cb77":"submission = pd.DataFrame({\"id\":Id, \"target\": pred})\nsubmission.to_csv(\"submission.csv\", index=False)","e04ea560":"As above, RandomForestClassifier has the best accuracy so far. Hurrayy!!!","a50a6aa3":"# Importing","d5743b4c":"# Data Cleaning ","c55531d5":"# Model Building and Prediction","33eb4ce5":"# Stopping Irrelavnt Words"}}