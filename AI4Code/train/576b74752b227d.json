{"cell_type":{"300c707a":"code","156caf8e":"code","82c4443f":"code","cc74c007":"code","7dc8ca45":"code","15b63d72":"code","e823f938":"code","bad2da89":"code","ac0a901a":"code","c6778ea0":"code","b20a372b":"code","8558ec88":"code","1a20b32c":"code","4c82ed60":"code","2d77b87d":"code","f7321a65":"code","3f3810db":"code","13f41628":"markdown","71c76d02":"markdown","014b21bf":"markdown","e98dcf6f":"markdown","457cdcdf":"markdown","1a81c857":"markdown","fd2766fd":"markdown","ca159d85":"markdown","133c6141":"markdown","a595cfeb":"markdown"},"source":{"300c707a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","156caf8e":"df = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","82c4443f":"df.info()","cc74c007":"df.head()","7dc8ca45":"y = df.fbs.values\nx_data = df.drop([\"sex\",\"restecg\",\"exang\",\"slope\",\"ca\",\"thal\",\"target\",\"oldpeak\"],axis =  1)","15b63d72":"x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","e823f938":"#%% train test split\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 42)\n#train_test_split methodu rastgele b\u00f6ler, random_state kullan\u0131rsak random b\u00f6ler yine ama her seferinde farkl\u0131 de\u011ferler yok.\n\n#x_train2 = x_train.T #rowlarla columnlar\u0131n yerini de\u011fi\u015ftirdik.\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","bad2da89":"#dimension = 6(feature)\ndef initialize_weight_and_bias(dimension):\n   \n    w = np.full((dimension,1),0.01) #row say\u0131s\u0131 dimension boyutunda i\u00e7inde 0.1 de\u011ferleri olan matrix olu\u015ftur. column say\u0131s\u0131 1\n    b = 0.0\n    return w,b\n\n# w,b = initialize_weight_and_bias(30)\n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n# print(sigmoid(0))\n","ac0a901a":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling. Bir toplam\u0131 sample a b\u00f6l\u00fcyorsak o normalizasyon\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    # gradints = weightlerin ve biaslar\u0131n t\u00fcrevi\n    return cost,gradients","c6778ea0":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = [] #t\u00fcm costlar\u0131 depolamak i\u00e7in\n    cost_list2 = [] #her 10 ad\u0131mda bir costlar\u0131 depolamak i\u00e7in\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","b20a372b":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","8558ec88":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weight_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 5, num_iterations = 15)    \n","1a20b32c":"a=20\nb=25\nwhile a<b:\n    logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = a)\n    a += 1","4c82ed60":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 50)","2d77b87d":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 70)","f7321a65":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 300)","3f3810db":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))\n","13f41628":"<a id=\"4\"><\/a> <br>\n## Parameter Initializing and Sigmoid Function","71c76d02":"<a id=\"6\"><\/a> <br>\n## Updating Parameters","014b21bf":"<a id=\"5\"><\/a> <br>\n## Forward and Backward Propogation","e98dcf6f":"<a id=\"3\"><\/a> <br>\n## Train Test Split","457cdcdf":"<a id=\"9\"><\/a> <br>\n## Sklearn with LR","1a81c857":"<a id=\"7\"><\/a> <br>\n## Prediction","fd2766fd":"<a id=\"1\"><\/a> <br>\n## Loading and Explanation Data\n* age = age in years\n\n* sex = sort\n(1 = male; 0 = female)\n\n* cp =\nchest pain type\n\n* trestbps =\nresting blood pressure (in mm Hg on admission to the hospital)\n* chol\nsort\nserum cholestoral in mg\/dl\n\n* fbs =\n(fasting blood sugar &gt; 120 mg\/dl) (1 = true; 0 = false)\n\n* restecg =\nresting electrocardiographic results\n\n* thalach =\nmaximum heart rate achieved\n\n* exang =\nexercise induced angina (1 = yes; 0 = no)\n\n* oldpeak =\nST depression induced by exercise relative to rest","ca159d85":"# INTRODUCTION\n\n* [Loading and Explanation Data](#1)\n* [Normalization](#2)\n* [Train Test Split](#3)\n* [Parameter Initializing and Sigmoid Function](#4)\n* [Forward and Backward Propogation](#5)\n* [Updating Parameters](#6)\n* [Prediction](#7)\n* [Logistic Regression](#8)\n* [Sklearn with LR](#9)","133c6141":"<a id=\"8\"><\/a> <br>\n## Logistic Regression","a595cfeb":"<a id=\"2\"><\/a> <br>\n## Normalization"}}