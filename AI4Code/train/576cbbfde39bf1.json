{"cell_type":{"05c5d676":"code","473da669":"code","3dd0651d":"code","ebe857f5":"code","6a4cfe20":"code","6dce85eb":"code","6539a6db":"code","3687d350":"code","b5c87caf":"code","ed69c73f":"code","f75d880b":"code","05c99a61":"code","a36fce4c":"code","f59411c6":"code","305c1534":"code","67306b84":"markdown","6fe56c4c":"markdown","b6aa884c":"markdown","6cc51e54":"markdown","b5d2015a":"markdown","2a806a33":"markdown","4ae79a17":"markdown","8c8fb876":"markdown","1f84598f":"markdown","fc7f8bf7":"markdown"},"source":{"05c5d676":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom keras.callbacks import Callback\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport re\n\nfrom keras import backend as K\nimport keras.layers as layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dropout, Dense, concatenate, Embedding, Flatten, Activation, SpatialDropout1D\nfrom keras.layers import Bidirectional, GRU, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.utils import np_utils\nfrom keras.engine import Layer\nfrom keras import initializers, regularizers, constraints\nfrom keras.layers import *\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import LSTM, CuDNNGRU, CuDNNLSTM, Add, Reshape\nfrom keras.layers import MaxPooling1D, Conv1D, MaxPooling1D, Conv2D, MaxPooling2D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nos.environ['OMP_NUM_THREADS'] = '4'\n\n\nimport re\nimport math\n# set seed\nnp.random.seed(123)","473da669":"train = pd.read_csv('..\/input\/stanford-natural-language-inference-corpus\/snli_1.0_train.csv')\ntest = pd.read_csv('..\/input\/stanford-natural-language-inference-corpus\/snli_1.0_test.csv')\nvalid = pd.read_csv('..\/input\/stanford-natural-language-inference-corpus\/snli_1.0_dev.csv')","3dd0651d":"print(\"Training on\", train.shape[0], \"examples\")\nprint(\"Validating on\", test.shape[0], \"examples\")\nprint(\"Testing on\", valid.shape[0], \"examples\")\ntrain[:10]","ebe857f5":"train.isnull().sum()","6a4cfe20":"train = train.dropna(subset = ['sentence2'])\ntrain = train[train[\"gold_label\"] != \"-\"]\ntest = test[test[\"gold_label\"] != \"-\"]\nvalid = valid[valid[\"gold_label\"] != \"-\"]","6dce85eb":"train.nunique()","6539a6db":"%%time\n\ndef get_rnn_data(df):\n    x = {\n        'sentence1': df[\"sentence1\"],\n        #\n        'sentence2': df[\"sentence2\"],\n        }\n    return x\n\nle = LabelEncoder()\n\nX_train = get_rnn_data(train)\nY_train = np_utils.to_categorical(le.fit_transform(train[\"gold_label\"].values)).astype(\"int64\")\n\nX_valid = get_rnn_data(valid)\nY_valid = np_utils.to_categorical(le.fit_transform(valid[\"gold_label\"].values)).astype(\"int64\")\n\nX_test = get_rnn_data(test)\nY_test = np_utils.to_categorical(le.fit_transform(test[\"gold_label\"].values)).astype(\"int64\")","3687d350":"class ElmoEmbeddingLayer(Layer):\n    def __init__(self, **kwargs):\n        self.dimensions = 1024\n        self.trainable=True\n        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.elmo = hub.Module('https:\/\/tfhub.dev\/google\/elmo\/2', trainable=self.trainable,\n                               name=\"{}_module\".format(self.name))\n\n        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module\/.*\".format(self.name))\n        super(ElmoEmbeddingLayer, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n                      as_dict=True,\n                      signature='default',\n                      )['default']\n        return result\n\n    def compute_mask(self, inputs, mask=None):\n        return K.not_equal(inputs, '--PAD--')\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.dimensions)\n    \n#     def get_config(self):\n#         config = {'output_dim': self.output_dim}\n    \nclass NonMasking(Layer):   \n    def __init__(self, **kwargs):   \n        self.supports_masking = True  \n        super(NonMasking, self).__init__(**kwargs)   \n  \n    def build(self, input_shape):   \n        input_shape = input_shape   \n  \n    def compute_mask(self, input, input_mask=None):   \n        # do not pass the mask to the next layers   \n        return None   \n  \n    def call(self, x, mask=None):   \n        return x   \n  \n    def get_output_shape_for(self, input_shape):   \n        return input_shape\n    \n#     def get_config(self):\n#         config = {'output_dim': self.output_dim}\n        \ncustom_ob={'ElmoEmbeddingLayer': ElmoEmbeddingLayer, 'NonMasking': NonMasking}","b5c87caf":"#### Elmo attempt\ndef get_model():\n    model = Sequential()\n    inp1 = Input(shape=(1,), dtype=\"string\", name=\"sentence1\")\n    inp2 = Input(shape=(1,), dtype=\"string\", name=\"sentence2\")\n    \n    def emb_layer(inp, col):\n        x = ElmoEmbeddingLayer()(inp)\n        return x\n\n    x = concatenate([\n                    emb_layer(inp1,\"sen_1\"),\n                    emb_layer(inp2,\"sen_2\"),\n                     ])\n    \n    x = NonMasking()(x)\n    x = Reshape((1, 1024*2), input_shape=(1024*2,))(x)\n    x = Bidirectional(LSTM(128, return_sequences=True,recurrent_dropout=0.2))(x)\n    x = Bidirectional(LSTM(128, return_sequences=True,recurrent_dropout=0.2))(x)\n    x = Bidirectional(LSTM(128, return_sequences=True,recurrent_dropout=0.2))(x)\n    x = Bidirectional(LSTM(128, return_sequences=True,recurrent_dropout=0.2))(x)\n    x = Bidirectional(LSTM(128, return_sequences=True,recurrent_dropout=0.2))(x)\n    x = Bidirectional(LSTM(128, return_sequences=True,recurrent_dropout=0.2))(x)\n\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    x = concatenate([avg_pool, max_pool])\n\n    outp = Dense(3, activation=\"softmax\", name=\"final_output\")(x)\n    \n    model = Model(inputs=[inp1,inp2], outputs=outp)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=Adam(lr=0.001),\n                  metrics=['accuracy'],\n                 )\n\n    return model\n\nmodel = get_model()\n\nmodel.summary()","ed69c73f":"\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=1, \n                                            verbose=1, \n                                            factor=0.5,\n                                            min_lr=0.00001)\nfile_path=\"checkpoint_SNLI_weights.hdf5\"\ncheckpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\nearly = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=1)\n\nmodel_callbacks = [checkpoint, early, learning_rate_reduction]","f75d880b":"# model = load_model(\"..\/input\/snli-model-and-weights\/SNLI_6_LSTM_model.h5\", custom_objects=custom_ob)\n# model.load_weights('..\/input\/snli-model-and-weights\/SNLI_6_LSTM_weights.hdf5')","05c99a61":"%%time\n \nmodel.fit(X_train, Y_train,\n          batch_size=128,\n          epochs=10,\n          verbose=2,\n          validation_data=(X_valid, Y_valid),\n          callbacks = model_callbacks\n         )","a36fce4c":"model.save_weights(\"SNLI_weights.hdf5\")\nmodel.save(\"SNLI_model.h5\")","f59411c6":"%%time\ntest_pred = model.predict(X_test, batch_size=128)","305c1534":"test_acc = (np.argmax(test_pred, axis=1) == np.argmax(Y_test, axis=1)).sum()\/Y_test.shape[0] * 100\n\nprint(\"Accuracy on test set is: %\"+str(test_acc))","67306b84":"# Stanford Natural Language Inference\n\nThis is a straight forward attempt at the SNLI dataset. NLI is particularly and even Kaggle's free GPU with it's recently expanded 9 hour time limit is insufficient to optimize a model for it. This model uses Elmo embeddings followed by stacked bidirectional LSTM layers to achieve ~75% test accuracy. Using more complicated model structures and expanded time limits, the results can be much better than this but this serves as a good baseline and an easier to follow model. Many papers have been written on the SNLI dataset and I find to be a very fulfilling to study after becoming acquainted with this dataset. Enjoy.\n\n# The Corpus\n\n*The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE). We aim for it to serve both as a benchmark for evaluating representational systems for text, especially including those induced by representation learning methods, as well as a resource for developing NLP models of any kind.\n\nRead the rest here: https:\/\/nlp.stanford.edu\/projects\/snli\/ \n\nIn short, the dataset provides one line of text and one hypothesis for the text. The goal of the model is to decide if the hypothesis contradicts, entails or is neutral to the text. So the example text \"*A man inspects the uniform of a figure in some East Asian country.*\" with a hypothesis of \"*The man is sleeping*\" is a contradiction because the man cannot inspect if he is asleep.","6fe56c4c":"# Train the Model","b6aa884c":"If you enjoyed this notebook, please like, comment, and check out some of my other notebooks on Kaggle: \n\nMaking AI Dance Videos: https:\/\/www.kaggle.com\/valkling\/how-to-teach-an-ai-to-dance\n\nImage Colorization: https:\/\/www.kaggle.com\/valkling\/image-colorization-using-autoencoders-and-resnet\/notebook\n\nStar Wars Steganography: https:\/\/www.kaggle.com\/valkling\/steganography-hiding-star-wars-scripts-in-images","6cc51e54":"# Read in data","b5d2015a":"Load any model weights here. ","2a806a33":"# Imports\n","4ae79a17":"# Make the NLI model\n\n## Custom Layers","8c8fb876":"# Prediction","1f84598f":"# Callbacks ","fc7f8bf7":"# Preprocessing the data\n\nThere are a few NA values to drop in sentence2 and the gold_label has a few \"-\". The \"-\" values are when the 5 votes from the turk participants came out tied, usually caused by very confusingly worded rows, so it is best to remove these as well.\n"}}