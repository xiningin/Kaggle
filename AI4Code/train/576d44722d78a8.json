{"cell_type":{"22fc5194":"code","035f327f":"code","d6f7355e":"code","2b888b8a":"code","853c099a":"code","58898057":"code","48f690bf":"code","b98570b5":"code","d34e950e":"code","7b82df48":"code","b883179f":"markdown","9ebe0c12":"markdown"},"source":{"22fc5194":"import numpy as np\nimport pandas as pd\nimport spacy\nfrom spacy.util import minibatch\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nimport itertools\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","035f327f":"train_tweets = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_tweets = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\n#Load the large model to get the vectors\nnlp = spacy.load('en_core_web_lg')\n\ntrain_tweets.head()","d6f7355e":"# Create a copy of the data in case it needs to be edited,without editing the original\ntrain_data = train_tweets\n\n# Disabling other pipes because we don't need them and it'll speed up this part \nwith nlp.disable_pipes():\n    doc_vectors = np.array([nlp(text).vector for text in train_data.text])\n    \n\n#Some examples use .iterrows, is this preferable?\n#with nlp.disable_pipes():\n#    vectors = np.array([nlp(tweet.text).vector for idx, tweet in train_data.iterrows()])\n\n#Repeat for test data\ntest_data = test_tweets\n\nwith nlp.disable_pipes():\n    test_doc_vectors = np.array([nlp(text).vector for text in test_data.text])","2b888b8a":"# Split the data into train and test, to assess the performance of different models\n\nX_train, X_test, y_train, y_test = train_test_split(doc_vectors, train_data.target,\n                                                    test_size=0.2, random_state=1)","853c099a":"#We will loop through and fit models using different values for parameters. Creating these lists to loop through.\n\nestimators_list = [50, 100, 150, 200, 250]\nlearning_rate_list = [0.3, 0.4, 0.5, 0.6, 0.7]\nXGB_estimators_list = [500, 1000, 1500, 2000, 2500]","58898057":"# SVM Classifier Model\n\ndef SVM_score_dataset():\n    model = LinearSVC(random_state=0, dual=False, max_iter=10000)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    f1score = f1_score(y_test, preds)\n    return(f1score)\n\nSVM_score_dataset()","48f690bf":"#Random Forest Model\n\ndef RF_score_dataset(n_estimators):\n    model = RandomForestClassifier(n_estimators=n_estimators, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    f1score = f1_score(y_test, preds)\n    return(f1score)\n\n#RF_f1scores will hold a list of different f1 scores, as we iterate through greater values for n_estimators.\nRF_f1scores = []\n\nfor i in estimators_list:\n    f1 = RF_score_dataset(i)\n    RF_f1scores.append(f1)\n\nprint('Max f1 is %f when n_estimators is %s.' % (max(RF_f1scores), estimators_list[RF_f1scores.index(max(RF_f1scores))]))\nprint(max(RF_f1scores))","b98570b5":"# Commented out because it takes a long time to run! Results in next cell\n\n#XGBoost Model\n\n#def XGB_score_dataset(n_estimators, learning_rate):\n#    model = XGBClassifier(n_estimators=n_estimators, learning_rate=learning_rate)\n#    model.fit()\n#    preds = model.predict(X_test)\n#    f1score = f1_score(y_test, preds)\n#    return(f1score)\n\n#XGB_f1scores = []\n\n#for i in itertools.product(XGB_estimators_list, learning_rate_list):\n#    f1 = XGB_score_dataset(i[0], i[1])\n#    XGB_f1scores.append(f1)\n\n#print('Max f1 is %f when n_estimators is %s and learning_rate is %s.' % (max(XGB_f1scores), \n#                                                                        XGB_estimators_list[(XGB_f1scores.index(max(XGB_f1scores))) \/\/ len(XGB_estimators_list)],\n#                                                                        learning_rate_list[(XGB_f1scores.index(max(XGB_f1scores))) % len(learning_rate_list)]))\n","d34e950e":"#Selecting XGBoost as it returns the highest f1 score\n#Fit a new XGB Classifier using the total training data, with the predetermined optimal parameters\n\nmy_model = XGBClassifier(n_estimators=1000, learning_rate=0.4)\nmy_model.fit(X_train, y_train)\npreds = my_model.predict(test_doc_vectors)","7b82df48":"# Create output to submit to competition\n\noutput = pd.DataFrame({'id': test_tweets.id,\n                       'target': preds})\noutput.to_csv('submission.csv', index=False)","b883179f":"Max f1 is 0.763245 when n_estimators is 1000 and learning_rate is 0.4","9ebe0c12":"Greatest f1 scores: <br>\nSVM: 0.7590 <br>\nRandom Forest: 0.7317 (n_estimators = 50) <br>\nXGBoost: 0.7632 (n_estimators = 1000 and learning_rate = 0.4)"}}