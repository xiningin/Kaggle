{"cell_type":{"9c326c0e":"code","e63aeb86":"code","7ce0fee7":"code","f61a23fe":"code","fbf98f65":"code","f9278e7f":"code","201fb120":"code","2763c0f4":"code","4d6eb5e5":"code","8c604d45":"code","0a2279f9":"code","65e9a2a1":"code","a8024e74":"code","3cc02d6c":"code","7e227594":"code","635a73ab":"code","b98fd53a":"code","cade2ff4":"code","28160edf":"code","ffac702d":"code","a680d97a":"code","deaa9923":"code","93b1f5d8":"code","e14b56cd":"code","e212c95f":"code","ce17a096":"code","2b4774a5":"code","51da38bd":"markdown","ce4f8e57":"markdown","ae661a01":"markdown","12fec20b":"markdown","bfd0d6d4":"markdown","b8057a1d":"markdown","0f825c74":"markdown"},"source":{"9c326c0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\n\nimport os\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e63aeb86":"PATH = \"..\/input\/bluebook-for-bulldozers\/\"","7ce0fee7":"! ls {PATH}","f61a23fe":"df_raw = pd.read_csv(f'{PATH}TrainAndValid.csv', low_memory=False, parse_dates=[\"saledate\"])\ntest = pd.read_csv(f'{PATH}Test.csv', low_memory=False)","fbf98f65":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","f9278e7f":"\n\ndisplay_all(df_raw.tail().T)","201fb120":"display_all(df_raw.describe(include='all').T)","2763c0f4":"df_raw.SalePrice = np.log(df_raw.SalePrice)","4d6eb5e5":"df_raw.isna().sum()","8c604d45":"features_to_consider = ['YearMade', 'datasource', 'state', 'fiBaseModel', 'fiProductClassDesc' , 'fiModelDesc']","0a2279f9":"model_performance = {\n    \"model_name\":[],\n    \"performance\":[],\n    \"model_score\":[]\n}","65e9a2a1":"def model_score(model, X_trn, y_trn, X_val, y_val):\n    model.fit(X_trn, y_trn)\n    pred = model.predict(X_val)\n    model_performance['model_name'].append(type(model).__name__)\n    model_performance['performance'].append(np.sqrt(mse(pred,y_val)))\n    print(model.score(X_val,y_val)*100)\n    model_performance['model_score'].append(model.score(X_val,y_val))\n    return np.sqrt(mse(pred, y_val))","a8024e74":"X = df_raw[features_to_consider]\ny = df_raw.SalePrice","3cc02d6c":"LabelEnc = LabelEncoder()\nX['state']=LabelEnc.fit_transform(X.state)\nX['fiBaseModel']= LabelEnc.fit_transform(X.fiBaseModel)\nX['fiProductClassDesc']= LabelEnc.fit_transform(X.fiProductClassDesc)\nX['fiModelDesc']= LabelEnc.fit_transform(X.fiModelDesc)","7e227594":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","635a73ab":"# Random Forest Regressor\nmodel = RandomForestRegressor(max_depth=30, min_samples_split=20, n_estimators=110, n_jobs= -1)\nmodel_score(model, X_train, y_train, X_test, y_test)","b98fd53a":"# Linear Regression\nmodel = LinearRegression()\nmodel_score(model, X_train, y_train, X_test, y_test)","cade2ff4":"# MLP \nmodel = MLPRegressor(hidden_layer_sizes=(100), activation=\"relu\", solver=\"adam\", alpha=0.0001, verbose=True)\nmodel_score(model, X_train, y_train, X_test, y_test)","28160edf":"# K Neighbours \nfrom sklearn.neighbors import KNeighborsRegressor\nmodel = KNeighborsRegressor(weights='distance', algorithm='auto')\nmodel_score(model, X_train, y_train, X_test, y_test)","ffac702d":"# SVM \nfrom sklearn.svm import SVR\nmodel = SVR(max_iter=1000)\nmodel_score(model, X_train, y_train, X_test, y_test)","a680d97a":"# SGD Regressor \nfrom sklearn.linear_model import SGDRegressor\nmodel = SGDRegressor(max_iter=1000, tol=1e-3)\nmodel_score(model, X_train, y_train, X_test, y_test)","deaa9923":"# Gradient Boosting Regressor \nfrom sklearn.ensemble import GradientBoostingRegressor\nmodel = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=10, random_state=0, loss='ls')\nmodel_score(model, X_train, y_train, X_test, y_test)","93b1f5d8":"model_df = pd.DataFrame(model_performance)\nmodel_df","e14b56cd":"model_df['model_score'] = model_df['model_score']*100\nmodel_df","e212c95f":"model_df = model_df.sort_values(\"model_score\", ascending=False)\nmodel_df","ce17a096":"# Random Forest Regressor\nmodel = RandomForestRegressor(max_depth=30, min_samples_split=20, n_estimators=110, n_jobs= -1)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)","2b4774a5":"model.score(X_test, y_test)*100","51da38bd":"Seeing the number of NaNs, we need to choose the column which are really important","ce4f8e57":"Generally selcting the metric(s) is an important part of the project setup. However, this case Kaggle tells us what metric to use. RMSLE(root mean squared log error) between the actual and predicted auction pries. Therefore we take log of the prices so that RMSE will give us what we need","ae661a01":"### Splitting the data in training and validation set","12fec20b":"#### We will be using the Label Encoder as the number of unique values in each column are a lot","bfd0d6d4":"### DataPrep and preprocessing ","b8057a1d":"Since this file has lot of columns, checking the number of NaN(s)","0f825c74":"## Inference \nwe observe that the best performance is observed when we use the <b>Random Forest Regressor<\/b>"}}