{"cell_type":{"dadc022e":"code","497ba393":"code","b562fe41":"code","3cf28597":"code","6cd05535":"code","9d1eecdc":"code","11b2eee6":"code","39530bee":"code","a6b6932e":"code","34f5e224":"code","a3d77523":"code","0177405e":"code","c903c91b":"code","24611e82":"markdown","91d9011e":"markdown","7debe527":"markdown","ca1803f9":"markdown","3420e53a":"markdown","556c5536":"markdown","1b8e5b92":"markdown","8642db26":"markdown","7d7bdaad":"markdown","d0fd3f80":"markdown","854b9709":"markdown"},"source":{"dadc022e":"df = spark.read.csv('s3:\/\/ui-spark-social-science-public\/data\/diamonds_nulls.csv', \n                    inferSchema=True, header=True, sep=',', nullValue='')","497ba393":"df.show()","b562fe41":"df.where(df['price'].isNull()).show(50)","3cf28597":"from pyspark.sql import Window","6cd05535":"window = Window.partitionBy('cut', 'clarity').orderBy('price').rowsBetween(-3, 3)","9d1eecdc":"window","11b2eee6":"from pyspark.sql.functions import mean","39530bee":"moving_avg = mean(df['price']).over(window)\nmoving_avg","a6b6932e":"df = df.withColumn('moving_avg', moving_avg)\ndf.show()","34f5e224":"from pyspark.sql.functions import when, col\n\ndef replace_null(orig, ma):\n    return when(orig.isNull(), ma).otherwise(orig)","a3d77523":"df_new = df.withColumn('imputed', \n                       replace_null(col('price'), col('moving_avg')) \n                      )","0177405e":"df_new.show()","c903c91b":"df_new.where(df['price'].isNull()).show(50)","24611e82":"***","91d9011e":"And we see that in all cases, the `'imputed'` column has the moving average value listed.","7debe527":"What this creates is a *column object* that contains the set of SQL instructions needed to create the data.  It hasn't been discussed in these tutorials, but pySpark is capable of taking SQL formatted instructions for most operations, and in the case of windowing, SQL is what underlies the Python code.\n\n**Remember that pySpark dataframes are immutable, so we cannot just fill in missing values.**  Instead we have to create a new column, then recast the dataframe to include it:","ca1803f9":"**_pySpark Basics: Moving Average Imputation_**","3420e53a":"_**Abstract:** In this guide we will demonstrate pySpark's windowing function by imputing missing values using a moving average._\n\n_**Main operations used:** `window`, `partitionBy`, `orderBy`, `over`, `when`, `otherwise`_","556c5536":"# Defining a Window\n\nThe first step to windowing is to **define the window parameters.**  We do this by combining three elements: the grouping (`partitionBy`), the ordering (`orderBy`) and the range (`rowsBetween`).  The window we define is then assigned to a variable, which we will use to perform computations.","1b8e5b92":"We can also show the subset where the value for `'price'` has been replaced with `null`:","8642db26":"**We now have an object of type `WindowSpec` that knows what the window should look like.  **\n\nThe first portion, `partionBy('cut', 'clarity')` is somewhat misleadningly named, as it is not related to *partitions* in Spark, which are segments of the distributed data, roughly analogous to individual computers within the cluster.  It is much more closely related to `groupBy`, as discussed for example in the *basics 2.ipynb* tutorial.  It tells pySpark that the windows should only be computed within each grouping of the columns `'cut'` and `'clarity'`.  Like `groupBy`, `partitionBy` can take one or more criteria.\n\nThe second portion, `orderBy('price')` simply sorts the data by price *within each partitionBy column*.\n\nAnd finally, `rowsBetween(-3, 3)` specifies the size of the window.  In this case it includes seven rows in each window - the current row plus the three before and the three after.\n\n# Operations Over a Window\n\nThe next step is to apply this window to an operation, which we can do using the `over` method.  Here we will use `mean` as our aggregator, but you can do this with any valid aggregator function.","7d7bdaad":"Think of windowing as **defining a range around any given row that counts as the window**, then **performing an operation just within that window.**  So if we define our window range as -1 to +1, then it will go over the data and examine every row along with its preceeding and following rows.  If we define it as 0 to +3, it will look at every row and the three following rows.  Obviously for this to make sense **the rows must be in a meaningful order**, and we have to **deliniate any groups within the data.**  That is to say, if we have panel data with 12 monthly observations for all 50 states, and we don't group before we define a window, then a -1 to +1 window might include November and December for one state, then January for the next state.\n\nThere are many ways to handle imputing missing data, and many uses for pySpark's windowing function.  We will demonstrate the intersection of these two concepts using a modified version of the dimaonds dataset, where several values in the `'price'` column have been deleted:","d0fd3f80":"And it returns a dataframe sorted by the specifications from our window function with the new column fully calculated.  Note that the first entry computes a window of 0, +3, the second entry a window of -1, +3, the third -2, +3 and the fourth finally -3, +3.  It would be reasonable to expect it to compute `null` values where the full window range can't be operated over; neither way is necessarily wrong, but make sure you note how pySpark handles it.\n\n# Imputation\n\nDue to immutability, we will recast the dataframe with yet another column that takes the value from the `'price'` column if it **is not `null`**, and fills in the value from the `'moving_avg'` column if it **is `null`**.  We will do this using pySpark's built in *`when... otherwise`* conditionals.  It is an intuitive, if not very Pythonic, formulation.  Then we cast the condition into a new column named `'imputed'`.  Note that the `withColumn` code is split onto multiple lines just for readability; the open brackets tell Python automatically that the command is meant to continue seamlessly to the next line, ignoring leading whitespace.","854b9709":"We can see in the above, on the first row the price is `null`, and the imputed column shows the moving average value.  On all the other rows the price has an actual value, and the imputed column uses those values.  Below we can look again at all the rows where price is `null`:"}}