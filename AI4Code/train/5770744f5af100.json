{"cell_type":{"42b1e20e":"code","54a5e40c":"code","a67882a0":"code","c41f361a":"code","5a056761":"code","4288025c":"code","6ee080c0":"code","a0224fd1":"code","f60a1ec3":"code","1ab459e6":"code","5b99eba9":"code","9101a705":"code","59342bc4":"code","1cb53dec":"code","2b0c2a00":"code","40062587":"code","cebc68b8":"code","779a4af1":"code","778cbe38":"code","64af598e":"code","74fcf0e1":"code","5439eb7a":"code","1148dc9e":"code","f4d7cd13":"code","f0ce1661":"code","dcaad9f8":"code","d52a21b6":"code","2e9f1509":"code","73ec8c90":"code","031cb295":"code","da290b0b":"code","1badd1be":"code","6abe9d37":"code","371b408e":"code","582e7863":"code","15f39185":"code","467bbc19":"code","e7774359":"code","48d4a781":"code","a4785dfc":"code","4d5a5c71":"code","57d5a121":"code","c6c34811":"code","66e25eb7":"code","5495958d":"code","f65c5b67":"code","0592bdd4":"code","89ff45ba":"code","9740c019":"code","dcfed692":"code","a574bf5d":"code","c8f74825":"markdown","fe545a72":"markdown","eea0b378":"markdown","1932f2f7":"markdown","1c523832":"markdown","f2a0801c":"markdown","917b524b":"markdown","5338d413":"markdown","c69f26de":"markdown","8fb1f51e":"markdown","d550d83c":"markdown","8271766b":"markdown","a7f44bdc":"markdown","acd000ec":"markdown","3a810659":"markdown","1993bd16":"markdown","2ade7cd7":"markdown","0fa2488f":"markdown","42321729":"markdown","a9e7ed74":"markdown","38b931b3":"markdown","bba050ca":"markdown","b2d89f7a":"markdown","2b03c982":"markdown","dc43d957":"markdown","36217a4f":"markdown","4ba41c1e":"markdown","1ea08f30":"markdown","2896bb01":"markdown"},"source":{"42b1e20e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54a5e40c":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","a67882a0":"df = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","c41f361a":"df.head()","5a056761":"df.info()","4288025c":"df.describe()","6ee080c0":"plt.figure(figsize=(20,15))\nsns.heatmap(df.corr())","a0224fd1":"df.corr()['DEATH_EVENT'].sort_values(ascending=False)","f60a1ec3":"plt.figure(figsize = (20, 25))\nfeature_num = 1\nfor i in df:\n    if feature_num < 13:\n        ax = plt.subplot(4, 4, feature_num)\n        sns.distplot(df[i])\n        plt.xlabel(i, fontsize = 12)\n        \n    feature_num += 1\nplt.show()","1ab459e6":"df.shape","5b99eba9":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\ny = df['DEATH_EVENT']\ndf_scaled = ss.fit_transform(df.drop(('DEATH_EVENT'),axis=1))\ndf_scaled = pd.DataFrame(data=df_scaled,columns=df.columns[:-1])\ndf_scaled = pd.concat([df_scaled,y],axis=1)","9101a705":"df_scaled","59342bc4":"fig, ax = plt.subplots(figsize = (15, 10))\nsns.boxplot(data = df_scaled, width = 0.5, ax = ax, fliersize = 3)\nplt.show()","1cb53dec":"pip install pca","2b0c2a00":"from pca import pca\nmodel = pca(n_feat=12,n_components=12) #considering all the 12 features as I do not want to eliminate columns\ndf_scaled_x = model.fit_transform(df_scaled.drop(('DEATH_EVENT'),axis=1))\n","40062587":"df_scaled_x['topfeat']","cebc68b8":"outliers = df_scaled_x['outliers']\noutliers_ = outliers[outliers['y_bool_spe']==True]","779a4af1":"outliers_.reset_index(inplace=True)","778cbe38":"outliers_index = outliers_['index']","64af598e":"outliers_index #gives the index of the outliers in the dataset","74fcf0e1":"plt.figure(figsize=(20,10))\nmodel.biplot(legend=True, SPE=True, hotellingt2=True)","5439eb7a":"df_new = df_scaled.drop(outliers_index,axis=0)","1148dc9e":"df_new","f4d7cd13":"fig, ax = plt.subplots(figsize = (15, 10))\nsns.boxplot(data = df_new, width = 0.5, ax = ax, fliersize = 3)\nplt.show()","f0ce1661":"df_exp = df_scaled[df_scaled['platelets'] < df_scaled['platelets'].quantile(0.95)]\ndf_exp = df_exp[df_exp['platelets'] > df_exp['platelets'].quantile(.1)]\ndf_exp = df_exp[df_exp['serum_sodium'] > df_exp['serum_sodium'].quantile(.1)]\ndf_exp = df_exp[df_exp['serum_creatinine'] < df_exp['serum_creatinine'].quantile(0.9)]\ndf_exp = df_exp[df_exp['creatinine_phosphokinase'] < df_exp['creatinine_phosphokinase'].quantile(0.91)]\nfig, ax = plt.subplots(figsize = (15, 10))\nsns.boxplot(data = df_exp, width = 0.5, ax = ax, fliersize = 3)\nplt.show()","dcaad9f8":"df_exp.shape","d52a21b6":"df_exp['DEATH_EVENT'].value_counts()","2e9f1509":"from sklearn.model_selection import train_test_split","73ec8c90":"X_train, X_test, y_train, y_test = train_test_split(df_exp.drop(('DEATH_EVENT'),axis=1), df_exp['DEATH_EVENT'], test_size=0.3, random_state=42,stratify=df_exp['DEATH_EVENT'])","031cb295":"y_train.value_counts()","da290b0b":"sns.countplot(y_train)","1badd1be":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(sampling_strategy={0:150,1:150})\nX,y = sm.fit_resample(X_train,y_train)","6abe9d37":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, balanced_accuracy_score\nfrom sklearn.model_selection import KFold\nmodels=[(\"XGboost\", XGBClassifier()),\n        (\"Stochastic Gradient Descent\", SGDClassifier()),\n        (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n        (\"Decision Tree\", DecisionTreeClassifier()),\n        (\"Random Forest\", RandomForestClassifier()),\n        (\"Extra Trees\", ExtraTreesClassifier()),\n        (\"Gradient Boosting\", GradientBoostingClassifier()),\n        (\"KNeighbors\", KNeighborsClassifier()),\n        (\"SVM\", SVC()),\n        (\"Naive Bayes\", GaussianNB()),\n        (\"Cat Boost\", CatBoostClassifier(verbose=False)),\n        (\"Ada Boost\", AdaBoostClassifier())]\n\nf1 = []\nvariance = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10)\n    results = cross_val_score(model, X, y, cv=kfold, scoring='f1')\n    f1.append(results.mean())\n    variance.append(results.std())\n    names.append(name)\n    print('Model name : {}, F1 score : {},  variance : {}'.format(name,results.mean(),results.std()))\n\nf1 = pd.Series(data=f1,name='f1-score')\nvariance = pd.Series(data=variance,name='variance')\nnames = pd.Series(data=names,name='names')\ndf_f1 = pd.concat([f1,variance],axis=1)\n\ndf_f1.set_index(keys=names,inplace=True)","371b408e":"df_f1.sort_values('f1-score',ascending=False,inplace=True)","582e7863":"plt.figure(figsize=(15,10))\nsns.barplot(y=df_f1.index,x=df_f1['f1-score'])","15f39185":"from sklearn.metrics import confusion_matrix,classification_report\next = ExtraTreesClassifier(criterion='entropy', max_depth=8)\next.fit(X,y)\npred_ext = ext.predict(X_test)\nprint(confusion_matrix(pred_ext,y_test))\nprint(classification_report(pred_ext,y_test))\nsns.barplot(x=ext.feature_importances_,y=X_test.columns)","467bbc19":"X.columns","e7774359":"X.drop(['anaemia', 'creatinine_phosphokinase', 'diabetes',\n        'high_blood_pressure', 'platelets','serum_sodium', 'sex', 'smoking'],inplace=True,axis=1)\nX_test.drop(['anaemia', 'creatinine_phosphokinase', 'diabetes',\n        'high_blood_pressure', 'platelets','serum_sodium', 'sex', 'smoking'],inplace=True,axis=1)","48d4a781":"print(X.shape,X_test.shape,y.shape,y_test.shape)","a4785dfc":"from sklearn.model_selection import GridSearchCV\nparams_ext = [{'criterion' : [\"gini\", \"entropy\"],\n              'min_samples_split': [2,4,6,8], \n              'max_depth': [2,4,6,8],\n              'max_features' : [\"auto\", \"sqrt\", \"log2\"],\n              'n_estimators': [100,200,400,600],\n          }]\nclassifier_ = ExtraTreesClassifier()\ngrid_search_ext = GridSearchCV(classifier_,params_ext,cv=3,n_jobs=150,scoring='f1',verbose=10)\ngrid_search_ext.fit(X,y)\nprint(grid_search_ext.best_params_)","4d5a5c71":"grid_search_ext.best_estimator_","57d5a121":"ext = ExtraTreesClassifier(max_depth=8, max_features='sqrt', min_samples_split=4)\next.fit(X,y)\npred_ext = ext.predict(X_test)","c6c34811":"from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(pred_ext,y_test))\nprint(classification_report(pred_ext,y_test))","66e25eb7":"from sklearn.ensemble import RandomForestClassifier\nparams_rfc = [{'criterion' : [\"gini\", \"entropy\"],\n              'min_samples_split': [2,4,6,8], \n              'max_depth': [2,4,6,8],\n              'max_features' : [\"auto\", \"sqrt\", \"log2\"],\n              'n_estimators': [100,200,400,600],\n          }]\nclassifier_ = RandomForestClassifier()\ngrid_search_rfc = GridSearchCV(classifier_,params_rfc,cv=3,n_jobs=150,scoring='f1',verbose=10)\ngrid_search_rfc.fit(X,y)\nprint(grid_search_rfc.best_params_)","5495958d":"grid_search_rfc.best_estimator_","f65c5b67":"rfc = RandomForestClassifier(criterion='entropy', max_depth=8)\nrfc.fit(X,y)","0592bdd4":"pred_rfc = rfc.predict(X_test)","89ff45ba":"print(confusion_matrix(pred_rfc,y_test))\nprint(classification_report(pred_rfc,y_test))","9740c019":"cat = CatBoostClassifier(verbose=False)\ncat.fit(X,y)","dcfed692":"pred_cat = cat.predict(X_test)","a574bf5d":"print(confusion_matrix(pred_cat,y_test))\nprint(classification_report(pred_cat,y_test))","c8f74825":"**Conclusions from the heatmap :**\n* Death event looks to be highly correlated with serum_creatinine and age\n* Survival event looks to be highly correlated with time, ejection_fraction, serum_sodium\n* Sex and smoking have the least correlation with DEATH_EVENT, we can consider dropping these features","fe545a72":"# Hyper parameter tuning for Extra trees classifier","eea0b378":"**Inference from the distplot** :<br>\nThere is a notable skew in certain features like platelets, creatinine_phosphokinase <br>\nWe can overcome this using log transformation but will skip the same as these features have less impact on survival","1932f2f7":"# Plot Boxplots to find outliers","1c523832":"# Training the Cat Boost Classifier","f2a0801c":"# Feature Selection\nLet us use extra trees to do some feature selection and eliminate unwanted features to boost model accuracy","917b524b":"The most important features are **age, ejection_fraction, serum_creatinine,time**<br>\nWe will be dropping all the other features","5338d413":"The output labels are imbalanced and we have overcome this via oversampling in order to avoid model bias towards majority class","c69f26de":"# Scaling the features using Standard Scaler<br>\nML algorithms are very sensitive differences in scales of various features<br>\nIn this particular case the platelets feature has a huge magnitude when compared to other features<br>\nThis can considerably offset the accuracy of our results<br>\nHence scaling is necessary<br>","8fb1f51e":"# Prediction using Extra trees","d550d83c":"Looks like a very ideal dataset<br>\nAll the values are numeric<br>\nThere are no null values to impute<br>\nThe platelets column needs scaling before applying ML algorithms, but lets find out if that feature actually matters<br>\nLets proceed to visualization<br>","8271766b":"Let us see if there are some more outliers in the dataset after PCA","a7f44bdc":"# Test train split","acd000ec":"# Choosing the best model based on F1-score","3a810659":"Let us drop the outliers","1993bd16":"# Conclusion","2ade7cd7":"We will use **Cat boost , Extra trees, Random forests** as they seem to have the best F1-scores","0fa2488f":"# Prediction using cat boost","42321729":"# Treating Outliers with Principal Component Analysis<br>\nPrincipal component analysis can be used to find the features that explain the most of the variance(95-100%) in the dataset.<br>\nMost of the times we do not need all the features in the input dataset to explain the variance.<br>\n\nHence this can be used to :<br>\n1. Drop unwanted features <br>\n2. Drop unwanted rows that act as outliers<br>\n\nThe inbuilt sklearn pca does not give us the top features(best features to use), the location of outliers in the dataset<br>\nThere is this cool library called pca that does both and hence we will be using the same.","a9e7ed74":"**Please upvote if this notebook was helpful and if you liked it<br>\nComments for improvements are welcome**","38b931b3":"We have cleaned up most of the outliers, do not want to drop more samples as it may reduce the amount of data input<br>\nto ML algorithms for classification","bba050ca":"So the best model to use would be **Extra trees and cat boost** , because they both have an **F1-score of 0.82** <br>\nAnd also an **accuracy of 0.87**","b2d89f7a":"Let us plot the pca model<br>\nAll the points outside the green zone gives us the outliers","2b03c982":"# Distplot of all the features\nTo understand how all the features are distributed","dc43d957":"# Exploratory data analysis","36217a4f":"# Prediction using random forests","4ba41c1e":"# Oversampling with SMOTE","1ea08f30":"# Hyper parameter tuning for random forests","2896bb01":"Looks like there are some more outliers and we have to clean them up"}}