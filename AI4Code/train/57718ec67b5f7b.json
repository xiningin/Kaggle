{"cell_type":{"aa0bd5de":"code","3e452410":"code","5034510c":"code","edd2ea81":"code","a85e3313":"code","1415b6e1":"code","152251c8":"code","eb9916ca":"code","a99e0ae9":"code","32e75827":"code","fe4e649f":"code","88214d64":"code","9f0b9814":"code","3de6da39":"code","6e1d56bc":"code","c4d4a24c":"code","f825a2eb":"code","939d213f":"code","6996183b":"code","79d48a08":"code","54c72a58":"code","565fccc7":"code","c9909e8b":"code","d407ca78":"code","9327be34":"code","5dd76167":"code","67ffbba0":"code","a0ff3ff8":"code","7bf708e4":"code","475bb654":"code","74259c30":"code","7548bd96":"code","ddda9113":"code","55ea93ab":"code","9b70e101":"code","76a839d8":"code","68558d6c":"code","563959df":"code","2027b5d3":"code","053f8c75":"code","75807026":"code","16760deb":"code","f93fa1d2":"code","d3f75101":"code","6e281c3e":"code","615d2a1b":"code","75f08a3b":"code","02108b9c":"code","cd294956":"code","9f23d58a":"code","7d310f3a":"code","0f68e43e":"code","5f1f800f":"code","f263ec8f":"code","c340d451":"code","22e1817c":"code","0bcbc74f":"code","14fc728c":"code","c9c4400c":"code","aa01d8e7":"code","06130ed3":"code","08012746":"code","9743fffa":"code","05a23c1c":"code","52dc88c1":"code","efc77342":"code","860a0f09":"code","53b41a56":"code","c740a82a":"code","62f847f7":"code","597cec4c":"code","757e949d":"code","5f4a364a":"code","0dabfcd5":"code","1a506c44":"code","1a9280ed":"code","de2323d4":"code","1fd35d11":"code","1b9c38fe":"code","60dfadb2":"code","0a9d0bef":"code","2e06365a":"code","3c3a2d17":"code","6bde3f35":"code","b5de9206":"markdown","344b54f3":"markdown","ad1c88e9":"markdown","58d4285e":"markdown","fdd53c1b":"markdown","e643186c":"markdown","14ccbfd6":"markdown","400cc4ec":"markdown","a65fdbe6":"markdown","7aa27bcd":"markdown","083db31b":"markdown","5beedc71":"markdown","df71f0fb":"markdown","039560b5":"markdown","f81f3939":"markdown","3380b4c9":"markdown","83b6f9e5":"markdown","bdf19ee2":"markdown","a3df5595":"markdown","48687200":"markdown","5b18b42e":"markdown"},"source":{"aa0bd5de":"# Importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nplt.style.use('ggplot')","3e452410":"df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","5034510c":"df.head()","edd2ea81":"df.drop(['id', 'Unnamed: 32'], axis = 1, inplace = True)","a85e3313":"df.diagnosis.unique()","1415b6e1":"df['diagnosis'] = df['diagnosis'].apply(lambda val: 1 if val == 'M' else 0)","152251c8":"df.head()","eb9916ca":"df.describe()","a99e0ae9":"df.info()","32e75827":"# checking for null values\n\ndf.isna().sum()","fe4e649f":"# visualizing null values\n\nmsno.bar(df)","88214d64":"plt.figure(figsize = (20, 15))\nplotnumber = 1\n\nfor column in df:\n    if plotnumber <= 30:\n        ax = plt.subplot(5, 6, plotnumber)\n        sns.distplot(df[column])\n        plt.xlabel(column)\n        \n    plotnumber += 1\n\nplt.tight_layout()\nplt.show()","9f0b9814":"# heatmap \n\nplt.figure(figsize = (20, 12))\n\ncorr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nsns.heatmap(corr, mask = mask, linewidths = 1, annot = True, fmt = \".2f\")\nplt.show()","3de6da39":"# removing highly correlated features\n\ncorr_matrix = df.corr().abs() \n\nmask = np.triu(np.ones_like(corr_matrix, dtype = bool))\ntri_df = corr_matrix.mask(mask)\n\nto_drop = [x for x in tri_df.columns if any(tri_df[x] > 0.92)]\n\ndf = df.drop(to_drop, axis = 1)\n\nprint(f\"The reduced dataframe has {df.shape[1]} columns.\")","6e1d56bc":"# creating features and label \n\nX = df.drop('diagnosis', axis = 1)\ny = df['diagnosis']","c4d4a24c":"# splitting data into training and test set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)","f825a2eb":"# scaling data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","939d213f":"# fitting data to model\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)","6996183b":"# model predictions\n\ny_pred = log_reg.predict(X_test)","79d48a08":"# accuracy score\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nprint(accuracy_score(y_train, log_reg.predict(X_train)))\n\nlog_reg_acc = accuracy_score(y_test, log_reg.predict(X_test))\nprint(log_reg_acc)","54c72a58":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","565fccc7":"# classification report\n\nprint(classification_report(y_test, y_pred))","c9909e8b":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)","d407ca78":"# model predictions \n\ny_pred = knn.predict(X_test)","9327be34":"# accuracy score\n\nprint(accuracy_score(y_train, knn.predict(X_train)))\n\nknn_acc = accuracy_score(y_test, knn.predict(X_test))\nprint(knn_acc)","5dd76167":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","67ffbba0":"# classification report\n\nprint(classification_report(y_test, y_pred))","a0ff3ff8":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nsvc = SVC()\nparameters = {\n    'gamma' : [0.0001, 0.001, 0.01, 0.1],\n    'C' : [0.01, 0.05, 0.5, 0.1, 1, 10, 15, 20]\n}\n\ngrid_search = GridSearchCV(svc, parameters)\ngrid_search.fit(X_train, y_train)","7bf708e4":"# best parameters\n\ngrid_search.best_params_","475bb654":"# best accuracy \n\ngrid_search.best_score_","74259c30":"svc = SVC(C = 10, gamma = 0.01)\nsvc.fit(X_train, y_train)","7548bd96":"# model predictions \n\ny_pred = svc.predict(X_test)","ddda9113":"# accuracy score\n\nprint(accuracy_score(y_train, svc.predict(X_train)))\n\nsvc_acc = accuracy_score(y_test, svc.predict(X_test))\nprint(svc_acc)","55ea93ab":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","9b70e101":"# classification report\n\nprint(classification_report(y_test, y_pred))","76a839d8":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nparameters = {\n    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1],\n    'loss' : ['hinge', 'log'],\n    'penalty' : ['l1', 'l2']\n}\n\ngrid_search = GridSearchCV(sgd, parameters, cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)","68558d6c":"# best parameter \n\ngrid_search.best_params_","563959df":"sgd = SGDClassifier(alpha = 0.001, loss = 'log', penalty = 'l2')\nsgd.fit(X_train, y_train)","2027b5d3":"# model predictions \n\ny_pred = sgd.predict(X_test)","053f8c75":"# accuracy score\n\nprint(accuracy_score(y_train, sgd.predict(X_train)))\n\nsgd_acc = accuracy_score(y_test, sgd.predict(X_test))\nprint(sgd_acc)","75807026":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","16760deb":"# classification report\n\nprint(classification_report(y_test, y_pred))","f93fa1d2":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()\n\nparameters = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : range(2, 32, 1),\n    'min_samples_leaf' : range(1, 10, 1),\n    'min_samples_split' : range(2, 10, 1),\n    'splitter' : ['best', 'random']\n}\n\ngrid_search_dt = GridSearchCV(dtc, parameters, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search_dt.fit(X_train, y_train)","d3f75101":"# best parameters\n\ngrid_search_dt.best_params_","6e281c3e":"# best score\n\ngrid_search_dt.best_score_","615d2a1b":"dtc = DecisionTreeClassifier(criterion = 'entropy', max_depth = 28, min_samples_leaf = 1, min_samples_split = 8, splitter = 'random')\ndtc.fit(X_train, y_train)","75f08a3b":"y_pred = dtc.predict(X_test)","02108b9c":"# accuracy score\n\nprint(accuracy_score(y_train, dtc.predict(X_train)))\n\ndtc_acc = accuracy_score(y_test, dtc.predict(X_test))\nprint(dtc_acc)","cd294956":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","9f23d58a":"# classification report\n\nprint(classification_report(y_test, y_pred))","7d310f3a":"from sklearn.ensemble import RandomForestClassifier\n\nrand_clf = RandomForestClassifier(criterion = 'entropy', max_depth = 11, max_features = 'auto', min_samples_leaf = 2, min_samples_split = 3, n_estimators = 130)\nrand_clf.fit(X_train, y_train)","0f68e43e":"y_pred = rand_clf.predict(X_test)","5f1f800f":"# accuracy score\n\nprint(accuracy_score(y_train, rand_clf.predict(X_train)))\n\nran_clf_acc = accuracy_score(y_test, y_pred)\nprint(ran_clf_acc)","f263ec8f":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","c340d451":"# classification report\n\nprint(classification_report(y_test, y_pred))","22e1817c":"from sklearn.ensemble import VotingClassifier\n\nclassifiers = [('Logistic Regression', log_reg), ('K Nearest Neighbours', knn), ('Support Vector Classifier', svc),\n               ('Decision Tree', dtc)]\n\nvc = VotingClassifier(estimators = classifiers)\n\nvc.fit(X_train, y_train)","0bcbc74f":"y_pred = vc.predict(X_test)","14fc728c":"# accuracy score\n\nprint(accuracy_score(y_train, vc.predict(X_train)))\n\nvc_acc = accuracy_score(y_test, y_pred)\nprint(vc_acc)","c9c4400c":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","aa01d8e7":"# classification report\n\nprint(classification_report(y_test, y_pred))","06130ed3":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator = dtc)\n\nada = AdaBoostClassifier(dtc, n_estimators = 180)\nada.fit(X_train, y_train)","08012746":"y_pred = ada.predict(X_test)","9743fffa":"# accuracy score\n\nprint(accuracy_score(y_train, ada.predict(X_train)))\n\nada_acc = accuracy_score(y_test, y_pred)\nprint(ada_acc)","05a23c1c":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","52dc88c1":"# classification report\n\nprint(classification_report(y_test, y_pred))","efc77342":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier()\n\nparameters = {\n    'loss': ['deviance', 'exponential'],\n    'learning_rate': [0.001, 0.1, 1, 10],\n    'n_estimators': [100, 150, 180, 200]\n}\n\ngrid_search_gbc = GridSearchCV(gbc, parameters, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search_gbc.fit(X_train, y_train)","860a0f09":"# best parameters \n\ngrid_search_gbc.best_params_","53b41a56":"# best score\n\ngrid_search_gbc.best_score_","c740a82a":"gbc = GradientBoostingClassifier(learning_rate = 1, loss = 'exponential', n_estimators = 200)\ngbc.fit(X_train, y_train)","62f847f7":"y_pred = gbc.predict(X_test)","597cec4c":"# accuracy score\n\nprint(accuracy_score(y_train, gbc.predict(X_train)))\n\ngbc_acc = accuracy_score(y_test, y_pred)\nprint(gbc_acc)","757e949d":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","5f4a364a":"# classification report\n\nprint(classification_report(y_test, y_pred))","0dabfcd5":"sgbc = GradientBoostingClassifier(max_depth=4, subsample=0.9, max_features=0.75, n_estimators=200, random_state=0)\n\nsgbc.fit(X_train, y_train)","1a506c44":"y_pred = sgbc.predict(X_test)","1a9280ed":"# accuracy score\n\nprint(accuracy_score(y_train, sgbc.predict(X_train)))\n\nsgbc_acc = accuracy_score(y_test, y_pred)\nprint(sgbc_acc)","de2323d4":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","1fd35d11":"# classification report\n\nprint(classification_report(y_test, y_pred))","1b9c38fe":"from xgboost import XGBClassifier \n\nxgb = XGBClassifier(objective = 'binary:logistic', learning_rate = 0.5, max_depth = 5, n_estimators = 180)\n\nxgb.fit(X_train, y_train)","60dfadb2":"y_pred = xgb.predict(X_test)","0a9d0bef":"# accuracy score\n\nprint(accuracy_score(y_train, xgb.predict(X_train)))\n\nxgb_acc = accuracy_score(y_test, y_pred)\nprint(xgb_acc)","2e06365a":"# confusion matrix\n\nprint(confusion_matrix(y_test, y_pred))","3c3a2d17":"# classification report\n\nprint(classification_report(y_test, y_pred))","6bde3f35":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'KNN', 'SVC', 'SGD Classifier', 'Decision Tree Classifier', 'Random Forest Classifier', 'Voting Classifier', 'Ada Boost Classifier',\n             'Gradient Boosting Classifier', 'Stochastic Gradient Boosting', 'XgBoost'],\n    'Score': [log_reg_acc, knn_acc, svc_acc, sgd_acc, dtc_acc, ran_clf_acc, vc_acc, ada_acc, gbc_acc, sgbc_acc, xgb_acc]\n})\n\nmodels.sort_values(by = 'Score', ascending = False)","b5de9206":"## Load the data","344b54f3":"# Logistic Regression","ad1c88e9":"## If you like my work, please do a upvote.","58d4285e":"## Data Preprocessing","fdd53c1b":"## Exploratory Data Analysis (EDA)","e643186c":"## Attribute Information:\n\n-  ID number \n- Diagnosis (M = malignant, B = benign)\n\n### Ten real-valued features are computed for each cell nucleus:\n\n- radius (mean of distances from center to points on the perimeter)\n- texture (standard deviation of gray-scale values)\n- perimeter\n- area\n- smoothness (local variation in radius lengths)\n- compactness (perimeter^2 \/ area - 1.0)\n- concavity (severity of concave portions of the contour)\n- concave points (number of concave portions of the contour)\n- symmetry\n- fractal dimension (\"coastline approximation\" - 1)","14ccbfd6":"### We can see that there are many columns which are very highly correlated which causes multicollinearity so we have to remove highly correlated features.","400cc4ec":"# Gradient Boosting Classifier","a65fdbe6":"# Extreme Gradient Boosting","7aa27bcd":"# Breast Cancer Classification","083db31b":"# SGD Classifier","5beedc71":"### Best model for diagnosing breast cancer is \"Gradient Boosting Classifier\" with an accuracy of 98.8%.","df71f0fb":"# Voting Classifier","039560b5":"##  Importing libraries","f81f3939":"# Support Vector Classifier (SVC)","3380b4c9":"# Random Forest Classifier","83b6f9e5":"#### There are no missing values in the data.","bdf19ee2":"# Decision Tree Classifier","a3df5595":"# Stochastic Gradient Boosting (SGB)","48687200":"# Ada Boost Classifier","5b18b42e":"# K Neighbors Classifier (KNN)"}}