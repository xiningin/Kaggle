{"cell_type":{"628192ba":"code","35c4bf99":"code","d4c13523":"code","3480da5f":"code","9ee57bc3":"code","71dd31ff":"code","df75aaff":"code","a4afea71":"code","d19bc096":"code","16ed9937":"code","657ea43b":"code","2d4d0fab":"code","27d43e23":"code","a0bcea45":"code","191947d1":"code","5af09b71":"code","2c025582":"code","e041632b":"code","d3d9a147":"code","56b86a57":"code","5ffa10df":"code","17e3668a":"code","73b4885b":"code","e1f621f9":"code","dfc4b87d":"code","b921db7b":"code","629a798e":"code","433d2082":"code","75c1b1ae":"code","d3d1776e":"code","97fdebaa":"code","c9afe0ba":"code","50e6324d":"code","d1c52844":"code","2ce5469e":"code","347c97ec":"code","47bdbb7f":"code","e5ef8159":"code","ef178a22":"code","3718cd43":"code","037ca625":"code","4b2d0fc0":"code","0f2fd55a":"code","0eb47f05":"code","545af719":"code","865d996b":"code","47c7aa74":"code","565a88a3":"code","84d69ed3":"code","47da2817":"code","b42164fa":"code","c4235d9f":"code","67fbf6f1":"code","ae8513e9":"code","ef8ccd9d":"code","78e5c04e":"code","819ca7d2":"code","767205e8":"code","af21fd75":"code","4742d4a2":"code","0c811e5c":"code","d4518d68":"code","4d296abb":"code","dc7a627a":"code","d00efc02":"code","b5dafa84":"code","b4c14cea":"code","9a6239cf":"code","6be29fe2":"code","a5086891":"code","df3c76b9":"markdown","39048450":"markdown","44bba86b":"markdown","b55b440e":"markdown","ffd74ddc":"markdown","d77cfbf9":"markdown","00ba3a47":"markdown","09de30de":"markdown","4052de89":"markdown","bfa7a677":"markdown","ab71164a":"markdown","227715fe":"markdown","c882da67":"markdown","0ce19a42":"markdown","84e370bf":"markdown","2a05857a":"markdown","96ce6b02":"markdown","29778341":"markdown","fa3a9c99":"markdown"},"source":{"628192ba":"import pandas as pd\nfrom tqdm.autonotebook import tqdm\nimport os\nimport matplotlib.pyplot as plt\nimport json\nimport nltk\nimport re\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import OrderedDict, Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split \nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n# nltk.download('stopwords')\n# nltk.download('punkt')\n# nltk.download('wordnet')","35c4bf99":"train_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","d4c13523":"train_df = train_df.sample(frac=1).reset_index(drop=True)","3480da5f":"def Concatenate(filename, files_path=train_files_path, output='text'):\n    json_path = os.path.join(train_files_path, (filename + '.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n\n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n\n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data\ntqdm.pandas()\ntrain_df['text'] = train_df['Id'].progress_apply(Concatenate)\ntqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(Concatenate)","9ee57bc3":"train_df.head(3)","71dd31ff":"sample_sub","df75aaff":"# train_df.drop(columns = ['pub_title','dataset_title', 'dataset_label','Id'], inplace = True)","a4afea71":"len(train_df['cleaned_label'].value_counts())","d19bc096":"train_df.isna().sum()","16ed9937":"train_df['dataset_title'].value_counts().plot(kind='bar', color='red', figsize = (24,20), fontsize = 10)\nplt.xlabel('labels')\nplt.ylabel('Total label count')\nplt.title('label count of each label type')","657ea43b":"def cleaning_text(text): \n    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n    text = re.sub('[!@#$_]', '', text)\n    text = text.replace(\"co\",\"\")\n    text = text.replace(\"http\",\"\")\n    text = ' '.join(text.split()) \n    text = text.lower() \n    return text\n\ntrain_df['text'] = train_df['text'].progress_apply(lambda x: cleaning_text(x))\nsample_sub['text'] = sample_sub['text'].progress_apply(lambda x: cleaning_text(x))","2d4d0fab":"# df = train_df.copy()","27d43e23":"def tokenize(text):\n    token_words= word_tokenize(str(text))\n    return \" \".join(token_words)\ntrain_df['text'] = train_df['text'].progress_apply(lambda x: tokenize(x))\nsample_sub['text'] = sample_sub['text'].progress_apply(lambda x: tokenize(x))","a0bcea45":"def stopwords_clean(text):\n    stop_words = set(stopwords.words('english'))\n    no_stopword_text = [w for w in str(text).split() if not w in stop_words]\n    return \" \".join(no_stopword_text)\n\ntrain_df['text'] = train_df['text'].progress_apply(lambda x: stopwords_clean(x))\nsample_sub['text'] = sample_sub['text'].progress_apply(lambda x: stopwords_clean(x))","191947d1":"# import nltk\n# from nltk.stem import WordNetLemmatizer\n# lemma= WordNetLemmatizer() \n\n# def lemmatize_text(text):\n#     lemma_text = [lemma.lemmatize(word) for word in text]\n#     return \"\".join(lemma_text)\n\n# train_df['texts_cleaned_lemmatized'] = train_df['text_cleaned_nostop'].progress_apply(lambda x: lemmatize_text(x))\n# sample_sub['texts_cleaned_lemmatized'] = sample_sub['text_cleaned_nostop'].progress_apply(lambda x: lemmatize_text(x))","5af09b71":"train_df['text'] = (train_df['text'].str.split().progress_apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' '))\nsample_sub['text'] = (sample_sub['text'].str.split().progress_apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' '))","2c025582":"train_df.head(3)","e041632b":"sample_sub","d3d9a147":"Train_df = train_df.copy() #ML\n# df = train_df.copy()       #BERT","56b86a57":"Train_df.info()","5ffa10df":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\ntext_docs = [TaggedDocument(doc.split(' '), [i]) \n             for i, doc in enumerate(Train_df.text)]\nmodel = Doc2Vec(vector_size=128, min_count=3, epochs = 30)\n#instantiate model\nmodel = Doc2Vec(vector_size=128, window=2, min_count=3, workers=8, epochs = 50)\n#build vocab\nmodel.build_vocab(text_docs)\n#train model\nmodel.train(text_docs, total_examples=model.corpus_count\n            , epochs=model.epochs)","17e3668a":"text2vec = [model.infer_vector((Train_df['text'][i].split(' '))) \n            for i in range(0,len(Train_df['text']))]","73b4885b":"test_text2vec = [model.infer_vector((sample_sub['text'][i].split(' '))) \n            for i in range(0,len(sample_sub['text']))]","e1f621f9":"dtv= np.array(text2vec).tolist()\nTrain_df['text2vec'] = dtv\nTrain_df.head(2)","dfc4b87d":"ttv= np.array(test_text2vec).tolist()\nsample_sub['text2vec'] = ttv\nsample_sub.head(2)","b921db7b":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())","629a798e":"# sample_sub.drop(columns = ['text','cleaned_text', 'cleaned_text_tokenized'], inplace = True)","433d2082":"sparce_matrix = text2vec\ny = Train_df.iloc[:,2].values \nX = sparce_matrix","75c1b1ae":"encoder = LabelEncoder()\ny = encoder.fit_transform(y)\ny[:10]\n# X[:5]","d3d1776e":"decoded = encoder.inverse_transform(y)\ndecoded[:10]","97fdebaa":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=45)","c9afe0ba":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nlogistic = LogisticRegression(penalty = 'l1', solver='liblinear', C= 0.1, random_state = 45)\npipeline = Pipeline(steps=[('sc', sc),('logistic', logistic)])\npipeline.fit(X_train, y_train)","50e6324d":"y_pred = pipeline.predict(X_test)\nprint('Accuracy:',accuracy_score(y_test,y_pred))\n# print('Confusion matrix:\\n',confusion_matrix(y_test,y_pred))\n# print('Classification report:\\n',classification_report(y_test,y_pred))","d1c52844":"predicts = []\nfor i in range (4):\n    lol  = np.array(test_text2vec[i]).reshape(1, -1)\n    pred_test =  pipeline.predict(lol)\n    X = encoder.inverse_transform(pred_test)\n    X = X.tolist()[0]\n    predicts.append(X)\npredicts","2ce5469e":"temp_1 = [x.lower() for x in Train_df['dataset_label'].unique()]\ntemp_2 = [x.lower() for x in Train_df['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in Train_df['cleaned_label'].unique()]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3)\nid_list = []\nlables_list = []\nfor index, row in tqdm(sample_sub.iterrows()):\n    sample_text = row['text']\n    row_id = row['Id']\n    temp_df = Train_df[Train_df['text'] == clean_text(sample_text)]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    for known_label in existing_labels:\n        if known_label in sample_text.lower():\n            cleaned_labels.append(clean_text(known_label))\n    cleaned_labels = [clean_text(x) for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n    lables_list.append('|'.join(cleaned_labels))\n    id_list.append(row_id)","347c97ec":"lables_list","47bdbb7f":"sample_subf = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')","e5ef8159":"sample_subf.PredictionString = lables_list","ef178a22":"sample_subf['PredictionString'] = sample_subf['PredictionString'].progress_apply(lambda x: cleaning_text(x))","3718cd43":"sample_subf","037ca625":"sample_subf.to_csv('submission.csv', index=False)","4b2d0fc0":"# from sklearn.ensemble import RandomForestClassifier\n# rfc1=RandomForestClassifier(n_estimators= 50, max_depth=15, bootstrap=True, random_state=45)\n# crfc1.fit(X_train, y_train)","0f2fd55a":"# y_pred = rfc1.predict(X_test)\n# print('Accuracy:',accuracy_score(y_test,y_pred))\n# print('Confusion matrix:\\n',confusion_matrix(y_test,y_pred))\n# print('Classification report:\\n',classification_report(y_test,y_pred))","0eb47f05":"# predicts = []\n# for i in range (4):\n#     lol  = np.array(test_text2vec[i]).reshape(1, -1)\n#     pred_test =  rfc1.predict(lol)\n#     X = encoder.inverse_transform(pred_test)\n#     X = X.tolist()[0]\n#     predicts.append(X)\n# predicts","545af719":"# from sklearn.svm import SVC\n# from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n# from sklearn.pipeline import Pipeline\n# from sklearn.preprocessing import StandardScaler\n\n# sc = StandardScaler()\n# svm = SVC(C=1,gamma='scale', kernel='linear')\n# pipe = Pipeline(steps=[('sc', sc),\n#                        ('SVM', svm)])\n\n# pipe.fit(X_train, y_train)","865d996b":"# y_pred = pipe.predict(X_test)\n# print('Accuracy:',accuracy_score(y_test,y_pred))\n# print('Confusion matrix:\\n',confusion_matrix(y_test,y_pred))\n# print('Classification report:\\n',classification_report(y_test,y_pred))","47c7aa74":"# predicts = []\n# for i in range (4):\n#     lol  = np.array(test_text2vec[i]).reshape(1, -1)\n#     pred_test =  pipe.predict(lol)\n#     X = encoder.inverse_transform(pred_test)\n#     X = X.tolist()[0]\n#     predicts.append(X)\npredicts","565a88a3":"# from sklearn.naive_bayes import GaussianNB\n# gnb = GaussianNB()\n# gnb.fit(X_train, y_train)","84d69ed3":"# y_pred = gnb.predict(X_test)\n# from sklearn import metrics\n# print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","47da2817":"# predicts = []\n# for i in range (4):\n#     lol  = np.array(test_text2vec[i]).reshape(1, -1)\n#     pred_test =  gnb.predict(lol)\n#     X = encoder.inverse_transform(pred_test)\n#     X = X.tolist()[0]\n#     predicts.append(X)\n# predicts","b42164fa":"# df.head(5)","c4235d9f":"# possible_labels = df.cleaned_label.unique()\n\n# label_dict = {}\n# for index, possible_label in enumerate(possible_labels):\n#     label_dict[possible_label] = index\n# label_dict","67fbf6f1":"# df['label'] = df.cleaned_label.replace(label_dict)","ae8513e9":"# from sklearn.model_selection import train_test_split\n\n# X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n#                                                   df.label.values, \n#                                                   test_size=0.15, \n#                                                   random_state=42)\n\n# df['data_type'] = ['not_set']*df.shape[0]\n\n# df.loc[X_train, 'data_type'] = 'train'\n# df.loc[X_val, 'data_type'] = 'val'\n\n# df.groupby(['cleaned_label', 'label', 'data_type']).count()","ef8ccd9d":"# from tokenizers import Tokenizer\n# from tokenizers.models import BPE\n# from tokenizers.trainers import BpeTrainer\n# from tokenizers.pre_tokenizers import Whitespace","78e5c04e":"# !pip install transformers\n# !pip install torchvision \n# import transformers\n# import torch\n# import torchvision\n# from torch.utils.data import TensorDataset \n# from transformers import BertForSequenceClassification","819ca7d2":"# tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', \n#                                           do_lower_case=True)\n                                          \n# encoded_data_train = tokenizer.batch_encode_plus(\n#     df[df.data_type=='train'].cleaned_text.values, \n#     add_special_tokens=True, \n#     return_attention_mask=True, \n#     pad_to_max_length=True, \n#     max_length=256, \n#     return_tensors='pt'\n# )\n\n# encoded_data_val = tokenizer.batch_encode_plus(\n#     df[df.data_type=='val'].cleaned_text.values, \n#     add_special_tokens=True, \n#     return_attention_mask=True, \n#     pad_to_max_length=True, \n#     max_length=256, \n#     return_tensors='pt'\n# )\n\n\n# input_ids_train = encoded_data_train['input_ids']\n# attention_masks_train = encoded_data_train['attention_mask']\n# labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n\n# input_ids_val = encoded_data_val['input_ids']\n# attention_masks_val = encoded_data_val['attention_mask']\n# labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n\n# dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n# dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)","767205e8":"# len(dataset_train), len(dataset_val)","af21fd75":"# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n#                                                       num_labels=len(label_dict),\n#                                                       output_attentions=False,\n#                                                       output_hidden_states=False)","4742d4a2":"# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n# batch_size = 3\n\n# dataloader_train = DataLoader(dataset_train, \n#                               sampler=RandomSampler(dataset_train), \n#                               batch_size=batch_size)\n\n# dataloader_validation = DataLoader(dataset_val, \n#                                    sampler=SequentialSampler(dataset_val), \n#                                    batch_size=batch_size)","0c811e5c":"# from transformers import AdamW, get_linear_schedule_with_warmup\n\n# optimizer = AdamW(model.parameters(),\n#                   lr=1e-5, \n#                   eps=1e-8)\n                  \n# epochs = 5\n\n# scheduler = get_linear_schedule_with_warmup(optimizer, \n#                                             num_warmup_steps=0,\n#                                             num_training_steps=len(dataloader_train)*epochs)","d4518d68":"# from sklearn.metrics import f1_score\n\n# def f1_score_func(preds, labels):\n#     preds_flat = np.argmax(preds, axis=1).flatten()\n#     labels_flat = labels.flatten()\n#     return f1_score(labels_flat, preds_flat, average='weighted')\n\n# def accuracy_per_class(preds, labels):\n#     label_dict_inverse = {v: k for k, v in label_dict.items()}\n    \n#     preds_flat = np.argmax(preds, axis=1).flatten()\n#     labels_flat = labels.flatten()\n\n#     for label in np.unique(labels_flat):\n#         y_preds = preds_flat[labels_flat==label]\n#         y_true = labels_flat[labels_flat==label]\n#         print(f'Class: {label_dict_inverse[label]}')\n#         print(f'Accuracy: {len(y_preds[y_preds==label])}\/{len(y_true)}\\n')","4d296abb":"# import random\n\n# seed_val = 17\n# random.seed(seed_val)\n# np.random.seed(seed_val)\n# torch.manual_seed(seed_val)\n# torch.cuda.manual_seed_all(seed_val)","dc7a627a":"# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model.to(device)\n\n# print(device)","d00efc02":"# def evaluate(dataloader_val):\n\n#     model.eval()\n    \n#     loss_val_total = 0\n#     predictions, true_vals = [], []\n    \n#     for batch in dataloader_val:\n        \n#         batch = tuple(b.to(device) for b in batch)\n        \n#         inputs = {'input_ids':      batch[0],\n#                   'attention_mask': batch[1],\n#                   'labels':         batch[2],\n#                  }\n\n#         with torch.no_grad():        \n#             outputs = model(**inputs)\n            \n#         loss = outputs[0]\n#         logits = outputs[1]\n#         loss_val_total += loss.item()\n\n#         logits = logits.detach().cpu().numpy()\n#         label_ids = inputs['labels'].cpu().numpy()\n#         predictions.append(logits)\n#         true_vals.append(label_ids)\n    \n#     loss_val_avg = loss_val_total\/len(dataloader_val) \n    \n#     predictions = np.concatenate(predictions, axis=0)\n#     true_vals = np.concatenate(true_vals, axis=0)\n            \n#     return loss_val_avg, predictions, true_vals","b5dafa84":"# for epoch in tqdm(range(1, epochs+1)):\n    \n#     model.train()\n    \n#     loss_train_total = 0\n\n#     progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n#     for batch in progress_bar:\n\n#         model.zero_grad()\n        \n#         batch = tuple(b.to(device) for b in batch)\n        \n#         inputs = {'input_ids':      batch[0],\n#                   'attention_mask': batch[1],\n#                   'labels':         batch[2],\n#                  }       \n\n#         outputs = model(**inputs)\n        \n#         loss = outputs[0]\n#         loss_train_total += loss.item()\n#         loss.backward()\n\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n#         optimizer.step()\n#         scheduler.step()\n        \n#         progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()\/len(batch))})\n         \n        \n#     torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n        \n#     tqdm.write(f'\\nEpoch {epoch}')\n    \n#     loss_train_avg = loss_train_total\/len(dataloader_train)            \n#     tqdm.write(f'Training loss: {loss_train_avg}')\n    \n#     val_loss, predictions, true_vals = evaluate(dataloader_validation)\n#     val_f1 = f1_score_func(predictions, true_vals)\n#     tqdm.write(f'Validation loss: {val_loss}')\n#     tqdm.write(f'F1 Score (Weighted): {val_f1}')","b4c14cea":"# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n#                                                       num_labels=len(label_dict),\n#                                                       output_attentions=False,\n#                                                       output_hidden_states=False)\n\n# model.to(device)","9a6239cf":"# model.load_state_dict(torch.load('finetuned_BERT_epoch_1.model', map_location=torch.device('cpu')))","6be29fe2":"# _, predictions, true_vals = evaluate(dataloader_validation)","a5086891":"# accuracy_per_class(predictions, true_vals)","df3c76b9":"BERT Pre-trained Model","39048450":"Accuracy","44bba86b":"**RandomForest Classifier**","b55b440e":"Accuracy","ffd74ddc":"*Note - Accuracy is a bad metric of evaluation for the models as the classes are heavily imbalanced*","d77cfbf9":"**Support Vector Machine Classifier**","00ba3a47":"Encoding the Labels","09de30de":"**Naive Bayes Classifier**","4052de89":"BertTokenizer and Encoding the Data","bfa7a677":"Performance Metrics","ab71164a":"Accuracy","227715fe":"**BERT**","c882da67":"Data Loaders","0ce19a42":"Optimizer & Scheduler","84e370bf":"Training Loop","2a05857a":"Accuracy","96ce6b02":"Train and Validation Split","29778341":"**Logistic Regression Classifier**","fa3a9c99":"Loading and Evaluating the Model"}}