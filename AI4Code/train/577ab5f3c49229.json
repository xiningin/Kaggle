{"cell_type":{"0c6622bf":"code","594344b4":"code","e74c92dd":"code","9c16b0cc":"code","44019fef":"code","67b834ae":"code","609a9f0d":"code","659cd5d2":"code","d2694c8b":"code","6844a48f":"code","d85b6342":"code","095a7b4d":"markdown"},"source":{"0c6622bf":"!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html","594344b4":"import os\nimport torch \nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as tt\nimport torch.nn.functional as F\nimport tarfile\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom torchvision.datasets import ImageFolder\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# check versions of torch and confirm detected device\nprint(torch.__version__)\nprint(torch.cuda.is_available())","e74c92dd":"# download the CIFAR-10 dataset\ndownload_url(\"https:\/\/s3.amazonaws.com\/fast-ai-imageclas\/cifar10.tgz\", '.')\n\n# extract images and convert to tensors\nwith tarfile.open('.\/cifar10.tgz', 'r:gz') as tar:\n    tar.extractall(path='.\/data')","9c16b0cc":"# data augmentation to prevent overfitting\ntrain_transform = tt.Compose([tt.RandomCrop(32, padding=8, padding_mode='edge'), \n                         tt.RandomHorizontalFlip(),\n                         tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                         tt.ToTensor()])","44019fef":"train_ds = ImageFolder('.\/data\/cifar10\/train', transform=train_transform)\ntest_ds = ImageFolder('.\/data\/cifar10\/test', transform=tt.ToTensor())","67b834ae":"# set device to cpu or gpu depending on availability\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Device: ', device)","609a9f0d":"# examine dataset and print an image\nimg, label = train_ds[0]\nplt.imshow(img.permute(1,2,0))","659cd5d2":"# Hyperparameters\nepochs = 10\nbatch_size = 128\nlearning_rate = 0.001\n#val_size = 5000\nworkers = 2\n\n'''\n# Splitting Dataset to create a Validation Set\ntorch.manual_seed(40)\ntrain_size = len(dataset) - val_size\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\n'''\n\n# DataLoader\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=workers, pin_memory=True)\ntest_dl = DataLoader(test_ds, batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n\n# Design Convolutional Neural Network model\nclass CnnModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Flatten(), \n            nn.Linear(256*4*4, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10))\n\n    def forward(self, xb):\n        return self.network(xb)\n\n# Instantiate CNN\nmodel = CnnModel().to(device)\n# Instantiating Loss Function\ncriterion = nn.CrossEntropyLoss()\n# Instantiating Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","d2694c8b":"# Train the model\ntotal_step = len(train_dl)\nhistory = []\nfor epoch in range(epochs):\n    for i, (images, labels) in enumerate(train_dl):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        ##FORWARD PASS##\n        # Make Prediction using CNN\n        yhat = model(images)\n        # Calculate Loss using Cross Entropy\n        loss = criterion(yhat, labels)\n        \n        ##BACKPROPAGATION##\n        # Calculate Gradient\n        loss.backward()\n        # Take Step\n        optimizer.step()\n        # Zero Gradients\n        optimizer.zero_grad()\n        \n        # Print Current State of Model\n        if (i+1) % 100 == 0:\n            print ('Epoch [{}\/{}], Step [{}\/{}], Loss: {:.4f}' \n                   .format(epoch+1, epochs, i+1, total_step, loss.item()))\n    history.append(loss.item())","6844a48f":"# Test the model\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_dl:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Test Accuracy of the model on test images: {} %'.format(100 * correct \/ total))\n\n# Save the model checkpoint\ntorch.save(model.state_dict(), 'model_ckpt.pth')","d85b6342":"# Plot the model loss as a function of epochs\nplt.plot(history, '-bx')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['Training'])\nplt.title('Loss vs. No. of epochs');","095a7b4d":"PyTorch CNN Image Classifier for CIFAR-10\n---\n---\n```\nStep 1: Examine Dataset\nStep 2: Split Dataset between Train and Val\nStep 3: Create Dataloaders and Move Data to GPU\nStep 4: Define Model\nStep 5: Set Hyperparameters\nStep 6: Train Model\n   -Generate Predictions\n   -Calculate Loss (Cross Entropy)\n   -Compute Gradient (Adam or SGD)\n   -Step\n   -Reset Gradient to Zero\nStep 7: Evaluate Model with Validation Set\nStep 8: Decide if Further Training is Necessary\nStep 9: Save Weights\n```"}}