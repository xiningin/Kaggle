{"cell_type":{"6bb60443":"code","26a89c0a":"code","f4db39e8":"code","4f847541":"code","bea50529":"code","afe64baf":"code","cf3b1af9":"code","b4c41cc1":"code","de783603":"code","fa23a182":"code","5f29f99e":"code","ae1d6d84":"code","1d1b79d4":"code","55ab65a4":"code","a7245ac6":"code","2a47da50":"code","fc98c611":"code","7d552500":"code","4b0da955":"code","607594cf":"code","a82a36bc":"code","624d98ae":"code","28521276":"code","4b607a6b":"code","a155f958":"code","304017f6":"code","513a4984":"code","fcb66897":"code","96fdb752":"code","af917444":"code","5772a1d4":"code","d5e57560":"code","ad1d7420":"code","9478a41b":"code","8635c59e":"code","3287c2ad":"code","ca27f5f8":"code","798899fd":"code","fb39541b":"code","94f128f0":"code","44a030e8":"code","071d1a52":"code","769c5ac0":"code","410296aa":"code","52b012c2":"code","25fcbdfc":"markdown","624792a4":"markdown"},"source":{"6bb60443":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26a89c0a":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom scipy.sparse import hstack\nimport gc\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nfrom gensim.models.phrases import Phraser,Phrases \nfrom gensim.models.word2vec import Word2Vec \nfrom gensim.models import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument","f4db39e8":"# Loading data\n\ntrain_en = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain_en['lang'] = 'en'\n\ntrain_es = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-es-cleaned.csv')\ntrain_es['lang'] = 'es'\n\ntrain_fr = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-fr-cleaned.csv')\ntrain_fr['lang'] = 'fr'\n\ntrain_pt = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-pt-cleaned.csv')\ntrain_pt['lang'] = 'pt'\n\ntrain_ru = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-ru-cleaned.csv')\ntrain_ru['lang'] = 'ru'\n\ntrain_it = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-it-cleaned.csv')\ntrain_it['lang'] = 'it'\n\ntrain_tr = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-tr-cleaned.csv')\ntrain_tr['lang'] = 'tr'\n\n#train2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\n#train2.toxic = train2.toxic.round().astype(int)\n#train2['lang'] = 'en'\n\ndata = pd.concat([\n    \n    train_en[['comment_text', 'lang', 'toxic']],\n    train_es[['comment_text', 'lang', 'toxic']],\n    train_tr[['comment_text', 'lang', 'toxic']],\n    train_fr[['comment_text', 'lang', 'toxic']],\n    train_pt[['comment_text', 'lang', 'toxic']],\n    train_ru[['comment_text', 'lang', 'toxic']],\n    train_it[['comment_text', 'lang', 'toxic']]\n    \n]).sample(n=100000).reset_index(drop=True)\n\ndel train_en, train_es, train_fr, train_pt, train_ru, train_it, train_tr\ngc.collect()","4f847541":"validation = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')","bea50529":"validation.describe()","afe64baf":"data.shape,validation.shape,submission.shape","cf3b1af9":"data.head()","b4c41cc1":"data['comment_text'][0]","de783603":"data['comment_text'][100]","fa23a182":"lens = data.comment_text.str.len()\nlens.min(),lens.mean(), lens.std(), lens.max()","5f29f99e":"len(data),len(validation)","ae1d6d84":"data['comment_text'].fillna(\"unknown\", inplace=True)\nvalidation['comment_text'].fillna(\"unknown\", inplace=True)","1d1b79d4":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","55ab65a4":"#clean txt function \n#write a class function for cleaning and preprocss the doctoVec attributes \n#clean the text using genism \nfrom gensim import utils\nimport gensim.parsing.preprocessing as gsp\n\n#the filters are doing the following \n#remove tags \n#remove punctuation\n#standarized the spaces \n#stop words \n#stemming \n#lower case for all words \nfilters = [\n           gsp.strip_tags, \n           gsp.strip_punctuation,\n           gsp.strip_multiple_whitespaces,\n           gsp.strip_numeric,\n           gsp.remove_stopwords, \n           gsp.strip_short, \n           gsp.stem_text\n          ]\n\ndef clean_text(s):\n    s = str(s).lower()\n    s = utils.to_unicode(s)\n    for f in filters:\n        s = f(s)\n    return s","a7245ac6":"submission.head()","2a47da50":"n = data.shape[0]\n\n#clean the text first \ndata[\"clean_comment\"] = data.comment_text.apply(clean_text)\nvalidation[\"clean_comment\"] = validation.comment_text.apply(clean_text)\nsubmission[\"clean_comment\"] = submission.content.apply(clean_text)\n\n#split the train and test from the whole table \nx_train, x_test, y_train, y_test = train_test_split(data[\"clean_comment\"],data.toxic, test_size=0.3)\n\n#vectorization of the model \nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )","fc98c611":"train_term_doc = vec.fit_transform(x_train)\ntest_term_doc = vec.transform(x_test)\nvalid_term_doc = vec.transform(validation['clean_comment'])\nsubmission_term_doc = vec.transform(submission['clean_comment'])","7d552500":"train_term_doc.shape","4b0da955":"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import roc_auc_score","607594cf":"logreg = LogisticRegression(n_jobs=1, C=4)\nlogreg.fit(train_term_doc, y_train)\ny_pred = logreg.predict(test_term_doc)\n\nprint('Testing accuracy %s' % accuracy_score(y_test, y_pred))\nprint('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\nprint('Testing AUC score %s' % roc_auc_score(y_test, y_pred))","a82a36bc":"#score the validation dataset\ny_valid = validation['toxic']\ny_pred_valid = logreg.predict(valid_term_doc)\nprint('Testing accuracy %s' % accuracy_score(y_valid, y_pred_valid))\nprint('Testing F1 score: {}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\nprint('Testing AUC score %s' % roc_auc_score(y_valid, y_pred_valid))","624d98ae":"#score the submission file \ny_pred_submission = logreg.predict(submission_term_doc)","28521276":"#load the sample submission file \nsample_sub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')","4b607a6b":"submid = pd.DataFrame({'id': sample_sub[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(y_pred_submission,columns=['toxic'])], axis=1)\n\nsubmission.to_csv('submission.csv', index=False)","a155f958":"#run the xgboost \nxgboost_model = xgb.train({'max_depth': 5, \"seed\": 123,'objective':'binary:logistic',\n                   'learning_rate':0.23,'min_child_weight':4}, \n                  xgb.DMatrix(train_term_doc, label=y_train), num_boost_round=500)\n\ny_pred_xgboost = xgboost_model.predict(xgb.DMatrix(test_term_doc, label=y_test))\n\nprint('Testing accuracy %s' % accuracy_score(y_test, y_pred_xgboost.round()))\nprint('Testing F1 score: {}'.format(f1_score(y_test, y_pred_xgboost.round(), average='weighted')))\nprint('Testing AUC score %s' % roc_auc_score(y_test, y_pred_xgboost))","304017f6":"#validation set\ny_valid = validation['toxic']\ny_pred_valid = xgboost_model.predict(xgb.DMatrix(valid_term_doc, label=y_valid))\n#print('Testing accuracy %s' % accuracy_score(y_valid, y_pred_valid))\n#print('Testing F1 score: {}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\nprint('Testing AUC score %s' % roc_auc_score(y_valid, y_pred_valid))","513a4984":"#score the submissint file \ny_pred_submission = xgboost_model.predict(xgb.DMatrix(submission_term_doc))","fcb66897":"sample_sub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')\nsubmid = pd.DataFrame({'id': sample_sub[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(y_pred_submission,columns=['toxic'])], axis=1)\n\nsubmission.to_csv('submission.csv', index=False)","96fdb752":"#use the English Version\ndata = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\nvalidation = pd.read_csv('..\/input\/translated\/validation_translate.csv')\nsubmission = pd.read_csv('..\/input\/translated\/test_translate.csv')","af917444":"n = data.shape[0]\n\n#clean the text first \ndata[\"clean_comment\"] = data.comment_text.apply(clean_text)\nvalidation[\"clean_comment\"] = validation.comment_text.apply(clean_text)\nsubmission[\"clean_comment\"] = submission.content.apply(clean_text)\n\n#split the train and test from the whole table \nx_train, x_test, y_train, y_test = train_test_split(data[\"clean_comment\"],data.toxic, test_size=0.3)\n\n#vectorization of the model \nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )","5772a1d4":"train_term_doc = vec.fit_transform(x_train)\ntest_term_doc = vec.transform(x_test)\nvalid_term_doc = vec.transform(validation['clean_comment'])\nsubmission_term_doc = vec.transform(submission['clean_comment'])","d5e57560":"logreg = LogisticRegression(n_jobs=1, C=4)\nlogreg.fit(train_term_doc, y_train)\ny_pred = logreg.predict(test_term_doc)\n\nprint('Testing accuracy %s' % accuracy_score(y_test, y_pred))\nprint('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\nprint('Testing AUC score %s' % roc_auc_score(y_test, y_pred))","ad1d7420":"#score the validation dataset\ny_valid = validation['toxic']\ny_pred_valid = logreg.predict(valid_term_doc)\nprint('Testing accuracy %s' % accuracy_score(y_valid, y_pred_valid))\nprint('Testing F1 score: {}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\nprint('Testing AUC score %s' % roc_auc_score(y_valid, y_pred_valid))","9478a41b":"sample_sub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')\nsubmid = pd.DataFrame({'id': sample_sub[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(y_pred_submission,columns=['toxic'])], axis=1)\n\nsubmission.to_csv('submission.csv', index=False)","8635c59e":"x_train.shape","3287c2ad":"#train the xgboost model \n#run the xgboost \nxgboost_model = xgb.train({'max_depth': 5, \"seed\": 123,'objective':'binary:logistic',\n                   'learning_rate':0.23,'min_child_weight':4}, \n                  xgb.DMatrix(train_term_doc, label=y_train), num_boost_round=500)\n\ny_pred_xgboost = xgboost_model.predict(xgb.DMatrix(test_term_doc, label=y_test))\n\nprint('Testing accuracy %s' % accuracy_score(y_test, y_pred_xgboost.round()))\nprint('Testing F1 score: {}'.format(f1_score(y_test, y_pred_xgboost.round(), average='weighted')))\nprint('Testing AUC score %s' % roc_auc_score(y_test, y_pred_xgboost))","ca27f5f8":"#validation set\ny_valid = validation['toxic']\ny_pred_valid = xgboost_model.predict(xgb.DMatrix(valid_term_doc, label=y_valid))\n#print('Testing accuracy %s' % accuracy_score(y_valid, y_pred_valid))\n#print('Testing F1 score: {}'.format(f1_score(y_valid, y_pred_valid, average='weighted')))\nprint('Testing AUC score %s' % roc_auc_score(y_valid, y_pred_valid))","798899fd":"#score the submissint file \ny_pred_submission = xgboost_model.predict(xgb.DMatrix(submission_term_doc))","fb39541b":"sample_sub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')\nsubmid = pd.DataFrame({'id': sample_sub[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(y_pred_submission,columns=['toxic'])], axis=1)\n\nsubmission.to_csv('submission.csv', index=False)","94f128f0":"#Navie-Bayes-SVM version- use multi-language\n# Loading data\n\ntrain_en = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain_en['lang'] = 'en'\n\ntrain_es = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-es-cleaned.csv')\ntrain_es['lang'] = 'es'\n\ntrain_fr = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-fr-cleaned.csv')\ntrain_fr['lang'] = 'fr'\n\ntrain_pt = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-pt-cleaned.csv')\ntrain_pt['lang'] = 'pt'\n\ntrain_ru = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-ru-cleaned.csv')\ntrain_ru['lang'] = 'ru'\n\ntrain_it = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-it-cleaned.csv')\ntrain_it['lang'] = 'it'\n\ntrain_tr = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-tr-cleaned.csv')\ntrain_tr['lang'] = 'tr'\n\n#train2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\n#train2.toxic = train2.toxic.round().astype(int)\n#train2['lang'] = 'en'\n\ntrain = pd.concat([\n    \n    train_en[['comment_text', 'lang', 'toxic']],\n    train_es[['comment_text', 'lang', 'toxic']],\n    train_tr[['comment_text', 'lang', 'toxic']],\n    train_fr[['comment_text', 'lang', 'toxic']],\n    train_pt[['comment_text', 'lang', 'toxic']],\n    train_ru[['comment_text', 'lang', 'toxic']],\n    train_it[['comment_text', 'lang', 'toxic']]\n    \n]).sample(n=500000).reset_index(drop=True)\n\ndel train_en, train_es, train_fr, train_pt, train_ru, train_it, train_tr\ngc.collect()","44a030e8":"test = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\nsub= pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')","071d1a52":"label_cols = ['toxic']\ntrain['none'] = 1-train[label_cols].max(axis=1)\ntrain.describe()","769c5ac0":"n = train.shape[0]\n\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\n\n\ntrn_term_doc = vec.fit_transform(train['comment_text'])\ntest_term_doc = vec.transform(test['content'])","410296aa":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)\n\nx = trn_term_doc\ntest_x = test_term_doc\n\ndef get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) \/ pr(0,y))\n    m = LogisticRegression(C=4, dual=False)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r\n\npreds = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    m,r = get_mdl(train[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]","52b012c2":"submid = pd.DataFrame({'id': sub[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n\nsubmission.to_csv('submission.csv', index=False)","25fcbdfc":"### Naive-Bayes Code Reference \nhttps:\/\/medium.com\/towards-artificial-intelligence\/naive-bayes-support-vector-machine-svm-art-of-state-results-hands-on-guide-using-fast-ai-13b5d9bea3b2","624792a4":"### Takeaway of DIfference Baselines\n\n(1) Multi-language training is more beneficial than using English only as a training. <br> \n(2) Vectorization is more better than Doc2Vec processing. Using the 'clean' data would also result in higher performance <br>\n(3) The performance of the different models ranked as followed: NB-SVM > xgboost > logistic regression "}}