{"cell_type":{"32b88e3c":"code","3ba528e9":"code","5146c87f":"code","25b01144":"code","e428660c":"code","6a70e1c8":"code","36486506":"code","f265a3e4":"code","e5d8ba42":"code","9c63f783":"markdown","a5cd8d6a":"markdown","287cc878":"markdown"},"source":{"32b88e3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\nprint(os.getcwd())\n\n# reading the input data\ndf_train = pd.read_csv(\"..\/input\/comp_train.csv\")\ndf_test = pd.read_csv(\"..\/input\/comp_test.csv\")\nprint(df_train.head())\nprint(df_test.head())\n\n# removing the id column from the df_train\ndf_train = df_train.drop(columns=['Id'])\nprint(df_train.head())","3ba528e9":"sns.set(rc={'figure.figsize':(11.7,8.27)})\n\n# separating the \ndf_trainfeatures = df_train.drop(columns=['SalePrice'])\ndf_trainpredict = pd.DataFrame(df_train[['SalePrice']])\ndisplay(df_trainfeatures.head())\ndisplay(df_trainpredict.head())","5146c87f":"def pairplot(x, y, **kwargs):\n    ax = plt.gca()\n    ts = pd.DataFrame({'time': x, 'val': y})\n    ts.plot.scatter('time', 'val',ax=ax)\n\n\nf = pd.melt(df_train, id_vars=[\"SalePrice\"], value_vars=df_trainfeatures)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False,height=7)\ng = g.map(pairplot, \"value\", \"SalePrice\")","25b01144":"# Calculate correlations and multiply it with 100 to increase the range from [-1 , +1 ] to [-100 , +100]\ncorr = df_train.corr()*100\n \n# Visualising the correlations using Heatmap\nsns.heatmap(corr , annot=True , vmin=-100 , vmax=100 , fmt=\".1f\")","e428660c":"# adding the quadratic features\ndf_trainfeatures['YearBuiltSqr'] = np.square(df_trainfeatures['YearBuilt'])\ndf_trainfeatures['2ndFlrSFSqr'] = np.square(df_trainfeatures['2ndFlrSF'])\ndf_trainfeatures['TotRmsAbvGrdSqr'] = np.square(df_trainfeatures['TotRmsAbvGrd'])\ndisplay(df_trainfeatures.head())\n\n# removing the BsmtFinSF2 feature\ndf_trainfeatures = df_trainfeatures.drop(columns=['BsmtFinSF2'])\ndisplay(df_trainfeatures.head())","6a70e1c8":"# First convert this pandas dataframe to a numpy array.\nX = np.array(df_trainfeatures , dtype=np.float64)\nY = np.array(df_trainpredict , dtype=np.float64)\n\n# let's confirm the shape of the X and Y\nprint (\"Shape of df_trainfeatures: {0} \\nShape of X: {1}\".format(df_trainfeatures.shape , X.shape))\nprint (\"Shape of df_trainpredict: {0} \\nShape of Y: {1}\".format(df_trainpredict.shape , Y.shape))\n\n# X is a matrix where each row contains features for a certain house in the training set\n# Y is a column matrix where each element in a row is sale price of a corresponding house described by corresponding row in X\n\n# let's find out the weightage of each feature in the model.\n# before doing that first stack the a columns of ones to the X matrix\nX = np.column_stack( ( np.ones(X.shape[0]) , X ) )\nprint(\"Shape of X after stacking a column vector containing ones: {0}\".format(X.shape))","36486506":"# to finding the weights\nw = np.linalg.pinv(X).dot(Y)\nprint(w)","f265a3e4":"# let's try a prediction on test data\nprint(\"Test Data:\")\ndisplay(df_test.head())\n\n# extracting the features out\ndf_testfeatures = df_test.drop(columns=['Id'])\nprint(\"Numerical Features of the Test Data:\")\ndisplay(df_testfeatures.head())\n\n# adding the quadratic features\ndf_testfeatures['YearBuiltSqr'] = np.square(df_testfeatures['YearBuilt'])\ndf_testfeatures['2ndFlrSFSqr'] = np.square(df_testfeatures['2ndFlrSF'])\ndf_testfeatures['TotRmsAbvGrdSqr'] = np.square(df_testfeatures['TotRmsAbvGrd'])\nprint(\"Adding Quadratic Features on test data:\")\ndisplay(df_testfeatures.head())\n\n# removing the BsmtFinSF2 feature\ndf_testfeatures = df_testfeatures.drop(columns=['BsmtFinSF2'])\nprint(\"Removing the BsmtFinSF2 feature on test data:\")\ndisplay(df_testfeatures.head())","e5d8ba42":"X_test = np.array(df_testfeatures , dtype=np.float64)\nX_test = np.column_stack( ( np.ones(X_test.shape[0]) , X_test ) )\nY_test = X_test.dot(w)\n\ndf_testprediction = pd.DataFrame()\ndf_testprediction['Id'] = df_test['Id']\ndf_testprediction['SalePrice'] = Y_test\ndf_testprediction.to_csv('submission.csv' , index=False)","9c63f783":"**Comments on Observations**\n1. SalePrice depends quadratically on OverallQual, YearBuilt\n> **A correlation value of 0.779 means higher linear dependence and hence OverallQual holds linear relation. But the Correlation values for YearBuilt is around 0.5 which means that this quantity has average linear relation and may hold  some non-linear relations.**\n2. SalePrice seems to have linear relationship with GrLivArea,  GarageArea , TotalBsmtSF, 1stFlrSF, 2ndFlrSF, TotRmsAbvGrd\n> **For GrLivArea,  GarageArea , TotalBsmtSF, 1stFlrSF we can assume linear relationship but for 2ndFlrSF and TotRmsAbvGrd we need to introduce some non-linear relations.**\n3. No linear relationship seems to have between SalePrice and BsmtFinSF2\n> **BsmtFinSF2 has a correlation value almost equal to zero, meaning that no linear relation existes between it and SalePrice. Infact on looking the graph plot it seems that the house prices doesn't even depen on it. Hence I will assume that SalePrice is not much dependent on this feature.**\n\n**Feature Selection**\n* OverallQual --> linear\n* GrLivArea --> linear\n* GarageArea --> linear\n* TotalBsmtSF --> linear\n* YearBuilt --> linear + quadratic\n* BsmtFinSF2 --> discard this feature\n* 1stFlrSF --> linear\n* 2ndFlrSF --> linear + quadratic\n* TotRmsAbvGrd --> linear + quadratic","a5cd8d6a":"All the quadratic features are introduced and no relationship(either linear or non-linear) features are removed. \nLet's proceed to next step i.e., find an optimal weights for these features.","287cc878":"**Observations**\n1. SalePrice depends quadratically on OverallQual, YearBuilt\n2. SalePrice seems to have linear relationship with GrLivArea,  GarageArea , TotalBsmtSF, 1stFlrSF, 2ndFlrSF, TotRmsAbvGrd\n3. No linear relationship seems to have between SalePrice and BsmtFinSF2\n\nLet's find out more internal relationships among them using correlation values.\n\n**But, what is Correlation?**\n> A number between +1 and \u22121 calculated so as to represent the linear interdependence of two variables or sets of data.\n> So, let assume two features x1 and x2, the higher the **magnitude** of the correlation the higher they are linearly dependent.\n> By linear dependent, I meant that x1 and x2 satisfy equation of the form: **x1 = m*x2 + c**.\n\n> The linear relation can be of two types: \n    > 1. As we increase the value of x1, the value of x2 increases and vice-versa. For this case the correlation values are positive.\n    > 2. As we increase the value of x1, the value of x2 decreases and vice-versa. For this case the correlation values are negative.\n\n> As I already mentioned that the **magnitude** of the correlation tells us how much these features are **linearly dependent**, a correlation value very close to zero simply means that they are having a weak **linear dependence**, and if the correlation coefficient is exactly 0 this means there is **no linear relationship**. Two features having correlation value equal to zero might be quadratically related for eg., x1 = x2*x2."}}