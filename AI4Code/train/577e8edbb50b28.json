{"cell_type":{"9ae17dcb":"code","76791fcd":"code","bc1ddfd3":"code","0fa139f5":"code","71835b12":"code","260c8a3b":"code","a1fe9c77":"code","258c65c4":"code","e105ec6a":"code","d32f6724":"code","47bcaa29":"code","a600d69b":"code","168b6d42":"code","93f43549":"code","889c36ed":"code","ad67280f":"code","a10d7b87":"code","ab9f6c2c":"code","9bf4eba0":"code","24daee84":"code","bd4a5ff7":"code","144e2dc0":"code","ab4e50cc":"code","3d579d6b":"code","59e421e0":"code","3bd4a6fe":"code","8626fb0f":"code","42a01949":"code","01bbd255":"code","6363d3a3":"code","0a236762":"code","aec8199b":"code","23905116":"code","5d07fb7c":"code","39af97c5":"code","e0f729df":"markdown"},"source":{"9ae17dcb":"import os\nos.environ[\"WANDB_API_KEY\"] = \"0\" # silence the warning","76791fcd":"import transformers\nimport torch\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport textwrap as wrap\nfrom torch.utils import data\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\nfrom collections import defaultdict\nimport re, random, html","bc1ddfd3":"# constants - config params\nRANDOM_SEED = 42\nMAX_LEN = 84\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nEPOCHS=6\nLR=5e-5\nTEST_SIZE = 0.15\nTRAINING_FILE = \"..\/input\/nlp-getting-started\/train.csv\"\nSAMPLE_FILE = \"..\/\/nlp-getting-started\/sample_submission.csv\"\nTEST_FILE = \"..\/input\/nlp-getting-started\/test.csv\"\n\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)\nMODEL_TYPE = \"bert-large-uncased\"\nMODEL_FILENAME = 'model.bin'\nDO_LOWER = True #False When the classification is sentiment then CASED pre-trained model really helps improving Accuracy score\nCLEANSE_DATA = True\n\n# Seed it - it can be a function or part of config when packaged\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True","0fa139f5":"# Download tokenizerfrom S3 and cache.\n\nTOKENIZER = transformers.BertTokenizer.from_pretrained(\n    MODEL_TYPE,\n    do_lower_case=DO_LOWER,\n)","71835b12":"#clean the data - some basic cleaning of commonly known characters, words and symbols \ndef cleanse_text(t):\n    _re_rep = re.compile(r'(\\S)(\\1{2,})')\n    _re_wrep = re.compile(r'(?:\\s|^)(\\w+)\\s+((?:\\1\\s+)+)\\1(\\s|\\W|$)')\n\n    def replace_rep(t):\n        \"Replace repetitions at the character level, however there are exceptions\"\n        def _replace_rep(m):\n            c,cc = m.groups()\n            return f'{c}{c}'\n        w = _re_rep.sub(_replace_rep, t)\n        w = w.replace('LOOL','LOL').replace('lool','lol').replace('LooL','LoL').replace('Lool','Lol')\n        return w\n\n    def fix_html(x):\n        \"messy characters\"\n        x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace('nbsp;', ' ').replace('.',' ').replace('\"','').replace('~',' ').replace(\n            '#36;', '$').replace('\\\\n', \"\").replace('quot;', \"'\").replace('<br \/>', \"\").replace('<br>', \"\").replace('<\/br>', \"\").replace('&gt;','').replace('&lt;','').replace('|','').replace(\n            '\\\\\"', '\"').replace('<unk>','').replace(' @.@ ','.').replace(' @-@ ','-').replace('...',' \u2026').replace('=>','').replace('..','').replace('\u00db_','').replace(\n            'gooaal','goal').replace('!','').replace(']','').replace('\u0089\u00db\u00ca','').replace(' - ','').replace('\u00e5\u00ca',' ').replace('\u00e5\u00ca\u0089\u00db\u00d2',' ').replace('\u00db\u00aa',' ').replace(\n            '[','').replace('_','').replace(';)', ':)').replace('?','').replace('+','').replace('\u00db\u00d2',' ').replace('{','').replace('}','').replace('%20',' ').replace('*',' ')\n        return html.unescape(x)\n\n    def replace_wrep(t):\n        \"Replace word repetitions: w1 w1 w1 w1\"\n        def _replace_wrep(m):\n            c,cc,e = m.groups()\n            return f'{c} {e}'\n        return _re_wrep.sub(_replace_wrep, t)\n    \n    def remove_url(t):\n        return re.split('https?:\\\/\\\/.*', str(t))[0]\n        \n    return remove_url(fix_html(replace_wrep(replace_rep(t))))","260c8a3b":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\",\n    # Typos\n    \"kno\" : \"know\",\n    \"fab\" : \"fabulous\",\n    \"oli\" : \"oil\",\n    \"tren\" : \"trend\", \n    \"swea\" : \"swear\", \n    \"stil\" : \"still\",\n    \"diff\" : \"different\",\n    \"appx\" : \"approximately\",\n    \"srsly\" : \"seriously\",\n    \"epicente\" : \"epicenter\",\n    \"evng\" : \"evening\",\n    \"lookg\" : \"looking\",\n    \"sayin\" : \"saying\",\n    \"tryin\" : \"trying\",\n    \"comin\" : \"Coming\",  \n    \"jumpin\" : \"jumping\",\n    \"nothin\" : \"nothing\", \n    \"burnin\" : \"burning\", \n    \"killin\" : \"killing\",\n    \"thinkin\" : \"thinking\",\n    \"throwin\" : \"throwing\",\n    \"newss\" : \"news\",\n    \"memez\" : \"memes\",\n    \"fforecast\" : \"Forecast\",\n}\n\ndef convert_abbrev(w):\n    return abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w\n","a1fe9c77":"print(cleanse_text('good good good good looooooool :)'))\nprint(cleanse_text('@bbcmtd Wholesale Markets ablaze http:\/\/t.co\/lHYXEOHY6C'))\nprint(\" \".join(cleanse_text(\"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh... **** *\").split()))\nprint(cleanse_text('#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires.. Crash_______,] chemical%20emergency'))\nprint(convert_abbrev('lookg'))","258c65c4":"df_train = pd.read_csv(TRAINING_FILE)\ndf_test = pd.read_csv(TEST_FILE)\ndf_train['classes'] = np.where(df_train['target'] == 0, 'Not Disaster', 'Disaster')","e105ec6a":"if CLEANSE_DATA:\n    df_train['text'] = df_train['text'].apply(lambda x: convert_abbrev(cleanse_text(x)))\ndf_train['tlen'] = df_train['text'].apply(len)\ndf_train.head()","d32f6724":"if CLEANSE_DATA:\n    df_test['text'] = df_test['text'].apply(lambda x: convert_abbrev(cleanse_text(x)))\ndf_test['tlen'] = df_test['text'].apply(len)\ndf_test.head()","47bcaa29":"df_train.describe()","a600d69b":"df_test.describe()","168b6d42":"sns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"]=(8,6)\nsns.countplot(df_train.classes);\nsns.set(style=\"dark\")\nclass_names = ['Not Disaster', 'Disaster'];","93f43549":"plt.rcParams[\"figure.figsize\"]=(8,6)\nsns.distplot(df_train.tlen,kde=True, rug=False,color='red');","889c36ed":"plt.rcParams[\"figure.figsize\"]=(8,6)\nsns.distplot(df_test.tlen,kde=True, rug=False, color='green');","ad67280f":"#combine text and tokenize\nlabels = df_train['target'].values\nidx = len(labels)\ndf = pd.concat([df_train, df_test])\ndf = df.text.values\n\n#### Splitting the train\/test data after tokenizing.\ntrain= df[:idx]\ntest = df[idx:]\ntrain.shape, test.shape","a10d7b87":"#find max_len. This is a small dataset and keeping it 96\/80\/72\/68 really not making much difference.\nmax_len=0\nfor t in df:\n    # Tokenize the text and add special tokens - `[CLS]` and `[SEP]` \n    input_ids = TOKENIZER.encode(t, add_special_tokens=True)\n\n    # Update the maximum input_ids length.\n    max_len = max(max_len, len(input_ids))\n\nprint('Max input_ids length = MAX_LEN: ', max_len)\nMAX_LEN = max_len","ab9f6c2c":"train[11:25]","9bf4eba0":"df_train[11:22]","24daee84":"class DisasterTweetsDataset(data.Dataset):\n    def __init__(self,text,target,tokenizer,max_len):\n        self.text = text\n        self.target = target\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        text = \" \".join(text.split()) # basic cleansing to remove unwanted spaces\n\n        # Encoding\n        encoding = TOKENIZER.encode_plus(\n            text, \n            add_special_tokens = True,\n            max_length = self.max_len,\n            truncation='longest_first',\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_attention_mask = True,\n            return_tensors='pt',\n        )\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding['token_type_ids'].flatten(),\n            'targets': torch.tensor(self.target[item],dtype=torch.long)\n        }","bd4a5ff7":"class DisasterTweetsModel(nn.Module):\n    def __init__(self, n_classes):\n        super(DisasterTweetsModel,self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(MODEL_TYPE)\n        self.bert_drop_1 = nn.Dropout(0.15)\n        self.bn = nn.BatchNorm1d(self.bert.config.hidden_size)\n        self.relu = nn.ReLU()\n        self.out1 =  nn.utils.weight_norm(nn.Linear(self.bert.config.hidden_size,2048))\n        self.out2 =  nn.utils.weight_norm(nn.Linear(2048,self.bert.config.hidden_size))\n        self.out3 =  nn.utils.weight_norm(nn.Linear(self.bert.config.hidden_size,n_classes)) # (768,2) or (1024,2)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input_ids, attention_mask,token_type_ids):\n        _,pooled_output = self.bert(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n            token_type_ids = token_type_ids,\n        )\n        output = self.bert_drop_1(pooled_output)\n        output = self.relu(output)\n        output = self.out1(output)\n        output = self.out2(output)\n        output = self.bn(output)\n        output = self.relu(output)\n        output = self.out3(output)\n        return self.softmax(output)","144e2dc0":"df_train, df_val = train_test_split(df_train, test_size=TEST_SIZE,random_state=RANDOM_SEED)\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_train.shape, df_val.shape","ab4e50cc":"def create_data_loader(df, tokenizer, max_len, bsz, shuffle):\n    ds = DisasterTweetsDataset(\n        text=df.text.to_numpy(),\n        target=df.target.to_numpy(),\n        tokenizer=TOKENIZER,\n        max_len = MAX_LEN,\n    )\n    return data.DataLoader(\n        ds,\n        batch_size=bsz,\n        num_workers=4,\n        shuffle=shuffle\n    )\n\ntrain_dl = create_data_loader(df_train, tokenizer=TOKENIZER, max_len=MAX_LEN, bsz=TRAIN_BATCH_SIZE,shuffle=True)\nval_dl = create_data_loader(df_val, tokenizer=TOKENIZER, max_len=MAX_LEN, bsz=VALID_BATCH_SIZE,shuffle=False)","3d579d6b":"model = DisasterTweetsModel(len(class_names))\nmodel = model.to(DEVICE)\n\n#optimizer parameters\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay':0.001},\n                        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay':0.0}]\n\n#optimizer \noptimizer = AdamW(optimizer_parameters, lr=LR)\nsteps = len(train_dl) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = steps\n)\n\n#loss function\nloss_fn = nn.CrossEntropyLoss().to(DEVICE)\n","59e421e0":"def train_fn(model, dl, loss_fn, optimizer, device, scheduler, n_examples):\n    model.train()\n    losses = []\n    predictions = 0\n\n    #iterate each from dl\n    for d in tqdm(dl, total=len(dl), position=0, leave=True):\n        input_ids = d['input_ids'].to(DEVICE)\n        attention_mask = d['attention_mask'].to(DEVICE)\n        token_type_ids = d['token_type_ids'].to(DEVICE, dtype=torch.long)\n        targets = d['targets'].to(DEVICE)\n\n        outputs = model(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n            token_type_ids = token_type_ids\n        )\n\n        _,preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        predictions += torch.sum(preds==targets)\n        losses.append(loss.item())\n\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    return predictions.double() \/ n_examples, np.mean(losses)\n\ndef eval_fn(model, dl, loss_fn, device, n_examples):\n    model.eval()\n    losses = []\n    predictions = 0\n\n    with torch.no_grad():\n        for d in tqdm(dl, total=len(dl), position=0, leave=True):\n            input_ids = d['input_ids'].to(DEVICE)\n            attention_mask = d['attention_mask'].to(DEVICE)\n            token_type_ids = d['token_type_ids'].to(DEVICE, dtype=torch.long)\n            targets = d['targets'].to(DEVICE)\n\n            outputs = model(\n                input_ids = input_ids,\n                attention_mask = attention_mask,\n                token_type_ids = token_type_ids\n            )\n\n            _,preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n\n            predictions += torch.sum(preds==targets)\n            losses.append(loss.item())\n    return predictions.double() \/ n_examples, np.mean(losses)","3bd4a6fe":"%%time\nhist = defaultdict(list)\nbest_acc = 0\nfor epoch in range(EPOCHS):\n    print(f'\\nEpoch {epoch + 1} \/ {EPOCHS}')\n    train_acc, train_loss = train_fn(model,train_dl,loss_fn,optimizer,DEVICE,scheduler,len(df_train))\n    print(f'Train loss {train_loss} Accuracy {train_acc}')\n\n    val_acc, val_loss = eval_fn(model,val_dl,loss_fn,DEVICE,len(df_val))\n    print(f'Validation loss {val_loss} Accuracy {val_acc}')\n    print()\n\n    hist['train_acc'].append(train_acc)\n    hist['train_loss'].append(train_loss)\n    hist['val_acc'].append(val_acc)\n    hist['val_loss'].append(val_loss)\n\n    if val_acc > best_acc:\n        torch.save(model.state_dict(),MODEL_FILENAME)\n        best_acc = val_acc","8626fb0f":"plt.figure(figsize=(8,6))\nplt.gca().title.set_text(f'Accuracy Chart')\nplt.plot(np.arange(EPOCHS),hist['train_acc'],label='Training')\nplt.plot(np.arange(EPOCHS),hist['val_acc'],label='Validation')\nplt.legend();","42a01949":"plt.figure(figsize=(8,6))\nplt.gca().title.set_text(f'Loss Chart')\nplt.plot(np.arange(EPOCHS),hist['train_loss'],label='Training')\nplt.plot(np.arange(EPOCHS),hist['val_loss'],label='Validation')\nplt.legend();","01bbd255":"def get_preds(model, data_loader):\n    model.eval()\n    predictions = []\n    prediction_proba = []\n \n    with torch.no_grad():\n        for d in tqdm(data_loader, total=len(data_loader)):\n            input_ids = d['input_ids'].to(DEVICE)\n            attention_mask = d['attention_mask'].to(DEVICE)\n            token_type_ids = d['token_type_ids'].to(DEVICE, dtype=torch.long)\n            outputs = model(\n                input_ids = input_ids,\n                attention_mask = attention_mask,\n                token_type_ids = token_type_ids\n            )\n\n            _,preds = torch.max(outputs, dim=1)\n            predictions.extend(preds)\n            prediction_proba.extend(outputs)\n    predictions = torch.stack(predictions).cpu()\n    prediction_proba = torch.stack(prediction_proba).cpu()\n\n    return predictions, prediction_proba","6363d3a3":"model = DisasterTweetsModel(len(class_names))\nmodel.load_state_dict(torch.load(MODEL_FILENAME))\nmodel = model.to(DEVICE)\ndf_test['target']=-1\ntest_dl = create_data_loader(df_test, tokenizer=TOKENIZER, max_len=MAX_LEN, bsz=TRAIN_BATCH_SIZE,shuffle=False)","0a236762":"preds, proba = get_preds(model,test_dl)\nlen(preds)","aec8199b":"df_sample = pd.read_csv(SAMPLE_FILE)","23905116":"preds[:10], proba[:10]","5d07fb7c":"df_sample['target']=preds;df_sample.head()","39af97c5":"df_sample.to_csv(\"submission_final.csv\",index=False)","e0f729df":"import emoji\n\ndef extract_emojis(s):\n  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI)"}}