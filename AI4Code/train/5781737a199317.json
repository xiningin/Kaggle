{"cell_type":{"e7032b73":"code","d44516f6":"code","b9b2c44f":"code","21794295":"code","a30de9d1":"code","73b68620":"code","3be0df22":"code","34be909c":"code","71aa4e1b":"code","5aabbc2c":"code","ffe12cb9":"code","701bf694":"code","647bd789":"code","76dd19dd":"code","e510094e":"code","8e457d85":"code","b2de7724":"code","853d5fef":"code","346c568e":"code","7d50a6de":"code","730f4c88":"code","584bf12b":"code","c920f18d":"code","d0ffd9a4":"code","5906cf8f":"code","42100f76":"code","f9e3b7ef":"code","22e8dd1f":"code","c49d969d":"code","62f002d4":"code","39315e18":"code","5047c194":"code","029af944":"code","32e2e11f":"code","de04dff2":"code","e2ac970d":"code","3d0c02fc":"code","fa4bb09e":"code","8e92408e":"code","ebbf0bfc":"code","abf3b750":"code","7a5c8c2c":"code","cb9b684e":"code","3acb3798":"code","d2cc0732":"code","6a7074a7":"code","4bec1993":"code","eaed06f0":"code","12142905":"code","8e9b954e":"code","84de6fa0":"code","4fe2a0d8":"code","3af0eda4":"code","f96438d0":"code","54c1f704":"markdown","be12b84e":"markdown","8590e10e":"markdown","72779f07":"markdown","4580b64f":"markdown","610b2dec":"markdown","88b5c3e4":"markdown","bb50b66c":"markdown","007ee9f5":"markdown","a0340892":"markdown","19c8d1d0":"markdown","87351076":"markdown","96ac3438":"markdown","660a02da":"markdown","4fc5932f":"markdown","cebb4922":"markdown","c63b88b9":"markdown","98cbd547":"markdown"},"source":{"e7032b73":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport re as re\n\n# Data Load\ntrain = pd.read_csv('..\/input\/train.csv', header = 0, dtype={'Age': np.float64})\ntest  = pd.read_csv('..\/input\/test.csv' , header = 0, dtype={'Age': np.float64})\nfull_data = [train, test] # for Data Preprocessing","d44516f6":"from IPython.display import Image\nImage(url= \"https:\/\/static1.squarespace.com\/static\/5006453fe4b09ef2252ba068\/t\/5090b249e4b047ba54dfd258\/1351660113175\/TItanic-Survival-Infographic.jpg?format=1500w\")","b9b2c44f":"train.describe?","21794295":"print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True).mean())\n# 1\ub4f1\uce78\uc5d0 \ud0c4\uc0ac\ub78c\ub4e4\uc758 \uc0dd\uc874\ub960\uc774 \ub192\uc73c\uba70, 3\ub4f1\uce78 \ud0d1\uc2b9\uac1d\ub4e4\uc740 \ub300\ubd80\ubd84 \uc8fd\uc5c8\ub2e4.","a30de9d1":"print (train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=True).mean())\n# \ub0a8\uc131\ubcf4\ub2e4 \uc5ec\uc131\uc774 \uc0c1\ub300\uc801\uc73c\ub85c \ub9ce\uc774 \uc0b4\uc544\ub0a8\uc558\ub2e4.","73b68620":"for dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\nprint (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())\n# \uc0dd\uc874\uc774 1\ub85c \ud45c\uae30\ub418\uc5b4 \uc788\uc5b4\uc11c \ube44\uc728\uacc4\uc0b0\uc744 \ubc14\ub85c \ud574\ub3c4\ub41c\ub2e4.\n# \uc0dd\uc874\uc774 0\uc774 \uc5c8\ub2e4\uba74, 1-value","3be0df22":"for dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\nprint (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())","34be909c":"total_df_ = pd.concat([train, test])\ntotal_df_['Embarked'].value_counts()","71aa4e1b":"train['Embarked'].isnull().sum(), test['Embarked'].isnull().sum()","5aabbc2c":"for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\nprint (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())","ffe12cb9":"fare_series = train['Fare'].copy()","701bf694":"fare_series.sort_values(inplace=True)\nfare_series.reset_index(inplace=True, drop=True)","647bd789":"fare_series[int(len(fare_series)\/2)]","76dd19dd":"fare_series.median()","e510094e":"fare_series.plot()","8e457d85":"for dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median()) # \uc911\uc559\uac12\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\nprint (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())","b2de7724":"for dataset in full_data:\n    age_avg \t   = dataset['Age'].mean()\n    age_std \t   = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    \n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count) ## ??\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n    \ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n\nprint (train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())","853d5fef":"def get_title(name):\n\ttitle_search = re.search(' ([A-Za-z]+)\\.', name)\n\t# If the title exists, extract and return it.\n\tif title_search: ## \uc608\uc678 Series\uc758 \ub0b4\uc7a5\ud568\uc218\ub294 \uc608\uc678\ucc98\ub9ac \ud558\uae30 \ub09c\ud574\n\t\treturn title_search.group(1)\n\treturn \"\"\n\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n\nprint(pd.crosstab(train['Title'], train['Sex']))","346c568e":"for dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nprint (train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())","7d50a6de":"for dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4\n    # Feature Selection\n    \ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp',\\\n                 'Parch', 'FamilySize']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n\ntest  = test.drop(drop_elements, axis = 1)\n\ntrain_x = train.drop(['Survived'],axis=1).values\ntrain_y = train['Survived'].values\n\ntest_x = test.values","730f4c88":"train_x[0,:], train_y[0]","584bf12b":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(max_iter=5, random_state=42)\nsgd_clf.fit(train_x,train_y)","c920f18d":"sgd_clf.predict([test_x[0]])","d0ffd9a4":"from sklearn.model_selection import cross_val_score\nfold_score = cross_val_score(sgd_clf, train_x, train_y, cv=5, scoring=\"accuracy\")\nprint(fold_score)\nprint(np.mean(fold_score))","5906cf8f":"# \ubaa8\ub450 \uc0b4\uc558\ub2e4\uace0 \ub0b4\ubcb9\ub294 \ubaa8\ub378\nfrom sklearn.base import BaseEstimator\n\nclass MyClassifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1),dtype=bool) # \ubaa8\ub450 \uc0b4\uc558\ub2e4\uace0 \ub0b4\ubcb9\ub294 \ubaa8\ub378","42100f76":"my_classifier = MyClassifier()\nfold_score = cross_val_score(my_classifier, train_x, train_y, cv=5, scoring=\"accuracy\") # \ubaa8\ub450 \uc0b4\uc558\ub2e4\uace0 \ub0b4\ubc49\uc5b4\ub3c4 \uc815\ud655\ub3c4\uac00 61%\uc815\ub3c4 \ub098\uc628\ub2e4.\nprint(fold_score)\nprint(np.mean(fold_score))","f9e3b7ef":"from sklearn.model_selection import cross_val_predict\n# cross_val_score vs cross_val_predict\ntrain_pred = cross_val_predict(sgd_clf, train_x, train_y, cv=5)\ntrain_pred[:10]","22e8dd1f":"train_pred.sum(), train_y.size, train_y.sum()","c49d969d":"import time","62f002d4":"from sklearn.metrics import confusion_matrix\n%time confusion_matrix(train_y, train_pred)\n# 452 : \uc8fd\uc740\uc0ac\ub78c\uc744 \uc8fd\uc5c8\ub2e4\uace0 \uc608\uce21 ( TN )\n# 97 : \uc0b0\uc0ac\ub78c\uc744 \uc8fd\uc5c8\ub2e4\uace0 \uc608\uce21 ( FP )\n# 165 : \uc0b0\uc0ac\ub78c\uc744 \uc8fd\uc5c8\ub2e4\uace0 \uc608\uce21 ( FN )\n# 177 : \uc0b0\uc0ac\ub78c\uc744 \uc0b4\uc558\ub2e4\uace0 \uc608\uce21 ( TP )","39315e18":"import numpy as np\n%time np.bincount(train_y * 2 + train_pred)","5047c194":"%time np.bincount(train_y * 2 + train_pred).reshape(2,2)","029af944":"from sklearn.metrics import precision_score, recall_score, f1_score\nscore_info = train_y, train_pred\nprint(\"precision : {:.2f}%\".format(precision_score(*score_info)))\nprint(\"recall : {:.2f}%\".format(recall_score(*score_info)))\nprint(\"f1-score : {:.2f}%\".format(f1_score(*score_info)))","32e2e11f":"y_scores = sgd_clf.decision_function(test_x)\ny_scores_df = pd.DataFrame(y_scores,columns=['Threshold'])\ny_scores_df.describe() # min -77 \/ max 32 \/ mean -31","de04dff2":"y_scores = cross_val_predict(sgd_clf, train_x, train_y, cv=5, method=\"decision_function\")\ny_scores","e2ac970d":"from sklearn.metrics import precision_recall_curve\nprecisions, recalls, thresholds  = precision_recall_curve(train_y, y_scores)","3d0c02fc":"import matplotlib.pyplot as plt\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"precision\")\n    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"recall\")\n    plt.xlabel(\"thresholds\")\n    plt.legend(loc=\"center left\")\n    plt.ylim([0,1])","fa4bb09e":"plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","8e92408e":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(train_y, y_scores)","ebbf0bfc":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.utils.fixes import signature\nprecisions, recalls, thresholds  = precision_recall_curve(train_y, y_scores)\n\n# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\nstep_kwargs = ({'step': 'post'}\n               if 'step' in signature(plt.fill_between).parameters\n               else {})\nplt.step(recalls, precisions, color='b', alpha=0.2, where='post')\nplt.fill_between(recalls, precisions, alpha=0.2, color='b', **step_kwargs)\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))","abf3b750":"pd.DataFrame({\n    'precisions':precisions,\n    'recalls':recalls\n}).plot.line(x=\"recalls\",y=\"precisions\")\nplt.show()","7a5c8c2c":"train_y_90 = ( y_scores > 40 )","cb9b684e":"train_y_90","3acb3798":"precision_score(train_y, train_y_90)","d2cc0732":"recall_score(train_y, train_y_90)","6a7074a7":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(train_y, y_scores)","4bec1993":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1],[0,1],'k--')\n    plt.axis([0,1,0,1])\n    plt.xlabel(\"fpr\")\n    plt.ylabel(\"tpr\")\nplot_roc_curve(fpr, tpr)\nplt.show()","eaed06f0":"pd.DataFrame({\n    'fpr':fpr,\n    'tpr':tpr\n}).plot(x=\"fpr\",y=\"tpr\")","12142905":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(train_y, y_scores)\naverage_precision","8e9b954e":"from sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(random_state=42)\ny_probas_forest = cross_val_predict(forest_clf, train_x, train_y, cv=5, method=\"predict_proba\")\n# An array containing the probabilities of belonging to a given class","84de6fa0":"y_probas_forest","4fe2a0d8":"y_probas_forest = y_probas_forest[:,-1] # Use probability for positive class as score\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(train_y, y_probas_forest)","3af0eda4":"plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\nplot_roc_curve(fpr_forest, tpr_forest, \"RandomForest\")\nplt.legend(loc=\"lower right\")\nplt.show()","f96438d0":"from sklearn.metrics import roc_auc_score\nprint(\"SGD : {:.2f}\".format(roc_auc_score(train_y,y_scores)))\nprint(\"RandomForest : {:.2f}\".format(roc_auc_score(train_y,y_probas_forest)))","54c1f704":"### Threshold","be12b84e":"## Confusion Matrix\n- https:\/\/www.waytoliah.com\/1222","8590e10e":"## Workflow goals\u00b6\n- The data science solutions workflow solves for <font color=\"red\">seven major goals.<\/font><br\/>\n\n### Classifying<br\/>\n- We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.<br\/>\n\n### Correlating. <br\/>\n- One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? <br\/>\nStatistically speaking is there a correlation among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? <br\/>\nThis can be tested both for numerical and categorical features in the given dataset. <br\/>\nWe may also want to determine correlation among features other than survival for subsequent goals and workflow stages.\n<font color=\"red\">Correlating certain features may help in creating, completing, or correcting features.<\/font><br\/><br\/>\n\n### Converting.( Name -> Title -> numeric )<br\/>\n- For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values.<br\/>\nSo for instance converting <font color=\"red\">text categorical values to numeric values<\/font>.<br\/><br\/>\n\n### Completing. ( Age, Cabin, Embarked ) <br\/>\n- Data preparation may also require us to estimate any <font color=\"red\">missing values<\/font>within a feature. Model algorithms may work best when there are no missing values.<br\/><br\/>\n\n### Correcting. <br\/>\n- We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. <br\/>\nOne way to do this is to detect any outliers among our samples or features. <font color=\"red\">We may also completely discard a feature if it is not contribting to the analysis<\/font> or may significantly skew the results.<br\/><br\/>\n\n### Creating. ( SibSp + Parch -> FamilySize ) <br\/>\n- Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.<br\/><br\/>\n\n### Charting.( plot )<br\/>\n- How to select the right visualization plots and charts depending on nature of the data and the solution goals.","72779f07":"## How can you determine the appropriate threshold value?","4580b64f":"# Fare","610b2dec":"```\uc911\uc559\uac12\uc744 \ucc3e\ub294 \ubc29\ubc95\n\ub370\uc774\ud130\ub97c \uc791\uc740 \uc218\ubd80\ud130 \ud070 \uc218 \uc21c\uc11c\ub300\ub85c \uc815\ub82c\ud569\ub2c8\ub2e4.\n\ub370\uc774\ud130\uc758 \uac1c\uc218\uac00 \ud640\uc218\ub77c\uba74 \uc911\uc559\uac12\uc740 \uc815\ub82c\ub41c \uacb0\uacfc\uc758 \uac00\uc6b4\ub370 \uc218\uc785\ub2c8\ub2e4\n\ub370\uc774\ud130\uc758 \uac1c\uc218\uac00 \uc9dd\uc218\ub77c\uba74 \uc911\uc559\uac12\uc740 \uac00\uc6b4\ub370 \ub450 \uc218\uc758 \ud3c9\uade0\uc785\ub2c8\ub2e4.```","88b5c3e4":"## If the goal is to achieve precision of 90%, a threshold of 40","bb50b66c":"# Story\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, <font color=\"red\">killing 1502 out of 2224<\/font> passengers and crew. Translated 32% survival rate.\n- One of the reasons that the shipwreck led to such loss of life was that there were <font color=\"red\">not enough lifeboats<\/font> for the passengers and crew.\n- Although there was some element of <font color=\"red\">luck<\/font> involved in surviving the sinking, some groups of people were more likely to survive than others, such as <font color=\"red\">women, children, and the upper-class.<\/font>","007ee9f5":"## IsAlone","a0340892":"# SGDClassfier\n - https:\/\/seamless.tistory.com\/38","19c8d1d0":"## Family Size ( SibSp + Parch ) + 1","87351076":"## Embarked","96ac3438":"# RandomForestClassifier vs SGDClassifier","660a02da":"# Data\n- Survived: \ub808\uc774\ube14 (0, 1) -> binary classification\n- pclass : N \ub4f1\uae09 ( categorical, ordinal )\n- sibsp : \ud615\uc81c \ub4f1 \ud0d1\uc2b9 \uc5ec\ubd80 ( numeric, discrete )\n- parch : \ubd80\ubaa8\/\uc790\uc2dd \ud0d1\uc2b9 \uc5ec\ubd80 ( numeric, discrete )\n- ticket: \ud2f0\ucf13\ubc88\ud638 ( String -> categorical )\n- cabin: \uc120\uc2e4\ubc88\ud638 ( String -> categorical )\n- embarked : \ud0d1\uc2b9 \uc120\ucc29\uc7a5 ( String -> categorical )\n   * (C = Cherbourg, Q = Queenstown, S = Southampton)","4fc5932f":"## AUC ( area under th curve )","cebb4922":"## precision & recall & F1-score","c63b88b9":"## precision % recall trade off","98cbd547":"# ROC CURVE"}}