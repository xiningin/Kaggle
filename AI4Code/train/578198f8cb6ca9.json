{"cell_type":{"2ade0dc6":"code","f7661d72":"code","59b66fe8":"code","0fc870f4":"code","e2a82919":"code","6b56007f":"code","c2795397":"code","7af72a19":"code","68658ac8":"code","02c8271c":"code","0314d972":"code","3ab157b4":"code","bb4804a4":"code","10ebe1c4":"code","3e6db494":"code","ba7f8870":"code","7418e75c":"code","08fd5917":"code","1808f620":"code","e00c9f5e":"code","0438f878":"code","238df4f4":"code","bb1dc94e":"markdown","f6dc078a":"markdown","67c60bd9":"markdown","464cf794":"markdown","4eede1c3":"markdown","04dc1969":"markdown","876e06d0":"markdown","2dd89556":"markdown","bf839402":"markdown","ae8c5345":"markdown","1b772d1b":"markdown","2ffc6afe":"markdown","3ef18ec4":"markdown","73829c57":"markdown","26ee4c4a":"markdown","8ba9ead0":"markdown","90cd8334":"markdown","1f694229":"markdown"},"source":{"2ade0dc6":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport numpy as np","f7661d72":"train_data=pd.read_csv('..\/input\/iba-ml1-mid-project\/train.csv')\ntest_data=pd.read_csv('..\/input\/iba-ml1-mid-project\/test.csv')\ntrain_data.head(10)","59b66fe8":"train_data.info()","0fc870f4":"train_data['credit_line_utilization'].replace(',','.',regex=True, inplace=True)\ntest_data['credit_line_utilization'].replace(',','.',regex=True, inplace=True)\n\ntrain_data[\"credit_line_utilization\"] = pd.to_numeric(train_data[\"credit_line_utilization\"])\ntest_data[\"credit_line_utilization\"] = pd.to_numeric(test_data[\"credit_line_utilization\"])","e2a82919":"train_data.info()","6b56007f":"test_data.info()","c2795397":"train_data.describe()","7af72a19":"train_data.drop(labels='Id', axis=1, inplace=True)\ntest_data.drop(labels='Id', axis=1, inplace=True)","68658ac8":"train_data.head(10)","02c8271c":"train_data.hist(train_data.columns.values.tolist(), figsize=(18,10))\nplt.show()","0314d972":"pd.value_counts(train_data['defaulted_on_loan']).plot.bar()\nplt.title('Defaulted on Loan Bar chart')\nplt.ylabel('Frequency')\nplt.xlabel('0-No, 1-Yes')\nplt.show()","3ab157b4":"corr_matrix = train_data.corr()\nprint(corr_matrix[\"defaulted_on_loan\"].sort_values(ascending=False))\n(corr_matrix[\"defaulted_on_loan\"]).plot.bar()","bb4804a4":"train_data.drop(train_data.index[train_data['number_of_previous_late_payments_up_to_59_days'] >7], inplace = True)\ntrain_data.drop(train_data.index[train_data['number_of_previous_late_payments_up_to_89_days'] >4], inplace = True)\ntrain_data.drop(train_data.index[train_data['number_of_previous_late_payments_90_days_or_more'] >4], inplace = True)\ntrain_data.drop(train_data.index[train_data['number_dependent_family_members'] >6], inplace = True)\ntrain_data.drop(train_data.index[train_data['credit_line_utilization'] >2.5], inplace = True)\ntrain_data.drop(train_data.index[train_data['monthly_income'] >30000], inplace = True)\ntrain_data.drop(train_data.index[train_data['real_estate_loans'] >6], inplace = True)\ntrain_data.drop(train_data.index[train_data['ratio_debt_payment_to_income'] >2], inplace = True)","10ebe1c4":"train_data.hist(train_data.columns.values.tolist(), figsize=(18,10))\nplt.show()","3e6db494":"corr_matrix = train_data.corr()\nprint(corr_matrix[\"defaulted_on_loan\"].sort_values(ascending=False))\n(corr_matrix[\"defaulted_on_loan\"]).plot.bar()\nplt.show()","ba7f8870":"scaler = StandardScaler()\ntrain_data[['number_of_previous_late_payments_up_to_59_days']]= scaler.fit_transform(train_data[['number_of_previous_late_payments_up_to_59_days']])\ntrain_data[['number_of_previous_late_payments_90_days_or_more']]= scaler.fit_transform(train_data[['number_of_previous_late_payments_90_days_or_more']])\ntrain_data[['monthly_income']]= scaler.fit_transform(train_data[['monthly_income']])\ntrain_data[['number_dependent_family_members']]= scaler.fit_transform(train_data[['number_dependent_family_members']])\ntrain_data[['number_of_credit_lines']]= scaler.fit_transform(train_data[['number_of_credit_lines']])\ntrain_data[['real_estate_loans']]= scaler.fit_transform(train_data[['real_estate_loans']])\ntrain_data[['credit_line_utilization']]= scaler.fit_transform(train_data[['credit_line_utilization']])\ntrain_data[['ratio_debt_payment_to_income']]= scaler.fit_transform(train_data[['ratio_debt_payment_to_income']])\n","7418e75c":"train_data.hist(train_data.columns.values.tolist(), figsize=(18,10))\nplt.show()","08fd5917":"corr_matrix = train_data.corr()\nprint(corr_matrix[\"defaulted_on_loan\"].sort_values(ascending=False))\n(corr_matrix[\"defaulted_on_loan\"]).plot.bar()\nplt.show()","1808f620":"test_data.head(10)","e00c9f5e":"test_data.hist(test_data.columns.values.tolist(), figsize=(18,10))\nplt.show()","0438f878":"test_data['number_of_previous_late_payments_up_to_59_days'].mask(test_data['number_of_previous_late_payments_up_to_59_days']>7, inplace=True)\ntest_data['number_of_previous_late_payments_up_to_89_days'].mask(test_data['number_of_previous_late_payments_up_to_89_days']>4, inplace=True)\ntest_data['number_of_previous_late_payments_90_days_or_more'].mask(test_data['number_of_previous_late_payments_90_days_or_more']>4, inplace=True)\ntest_data['number_dependent_family_members'].mask(test_data['number_dependent_family_members']>6, inplace=True)\ntest_data['credit_line_utilization'].mask(test_data['credit_line_utilization']>2.5, inplace=True)\ntest_data['monthly_income'].mask(test_data['monthly_income']>30000, inplace=True)\ntest_data['real_estate_loans'].mask(test_data['real_estate_loans']>6, inplace=True)\ntest_data['ratio_debt_payment_to_income'].mask(test_data['ratio_debt_payment_to_income']>2, inplace=True)","238df4f4":"test_data.hist(test_data.columns.values.tolist(), figsize=(18,10))\nplt.show()","bb1dc94e":"now, lets get some information about these data","f6dc078a":"### Reading the data","67c60bd9":"### Importing Libraries","464cf794":"as we said before, since we don't need ID column, let's drop it","4eede1c3":"now let's try using StandardScaler to normalize the data","04dc1969":"now let's check again","876e06d0":"we can clearly see that the correlation in some columns increased tremendously.","2dd89556":"now to get a better understanding of distributions let's plot histograms of each column","bf839402":"we can see here that there is an imbalance at the results. However, this imbalance is not an error in getting the data, but mostly showing the real life situation. Using oversampling or undersampling techniques will negatively effect model predictions then","ae8c5345":"great, let's check other parameters","1b772d1b":"it is obvious that distributions are very bad and there are a lot of outliers.However, let us first check the correlation of each column to our target value.","2ffc6afe":"# Elnur Shahbalayev EDA","3ef18ec4":"as can be seen the correlation is very low. Number of previous late payments up to 59 days being the highest at 0.1242 which is not that great correlation as well. However, given that we have around 70,000 data, it will help us to get more accurate results. Now let us see by manually treating outliers, if we can increase the correlation","73829c57":"it can be seen that in credit_line_utilization column standard deviation is much bigger than the mean of it.","26ee4c4a":"in the first look we can say that the ID column will not be that important for us to predict the results. Also it is weird that above we can see that 'credit line utilization column' has numbers only but it is recognized as object type. After checking the dataset manually, we find that there is ',' sign in some numbers. We need to remove them and change the datatype to float64.","8ba9ead0":"now let us check the distributions on the test data so that we can later adjust it as well","90cd8334":"let us check the correlations now","1f694229":"obviously, StandardScaler or MinMaxScaler did not give any difference in results. Usage of IsolationForest also had negative effects on the correlation."}}