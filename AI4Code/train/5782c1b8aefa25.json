{"cell_type":{"f74b67d5":"code","697f7d15":"code","e962e8d4":"code","37cc3ca3":"code","892ea76f":"code","4ebc9df8":"code","ba4d549d":"code","9f90eadd":"code","d18d0082":"code","130ff0b4":"code","6100ac34":"code","35e922fa":"code","420f7773":"code","8cd8f80a":"code","a031a000":"code","72478661":"code","bd6c735c":"markdown"},"source":{"f74b67d5":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import trange\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nfrom IPython.display import clear_output","697f7d15":"def compute_modified_confusion_matrix_nonorm(labels, outputs):\n    # Compute a binary multi-class, multi-label confusion matrix, where the rows\n    # are the labels and the columns are the outputs.\n    num_recordings, num_classes = np.shape(labels)\n    A = np.zeros((num_classes, num_classes))\n\n    # Iterate over all of the recordings.\n    for i in range(num_recordings):\n        # Calculate the number of positive labels and\/or outputs.\n        #####normalization = float(max(np.sum(np.any((labels[i, :], outputs[i, :]), axis=0)), 1))\n        # Iterate over all of the classes.\n        for j in range(num_classes):\n            # Assign full and\/or partial credit for each positive class.\n            if labels[i, j]:\n                for k in range(num_classes):\n                    if outputs[i, k]:\n                        A[j, k] += 1.0#\/normalization\n\n    return A\n\ndef load_dataset(flatten=False):\n    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n    # normalize x\n    X_train = X_train.astype(float) \/ 255.\n    X_test = X_test.astype(float) \/ 255.\n    # we reserve the last 10000 training examples for validation\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n    if flatten:\n        X_train = X_train.reshape([X_train.shape[0], -1])\n        X_val = X_val.reshape([X_val.shape[0], -1])\n        X_test = X_test.reshape([X_test.shape[0], -1])\n    return X_train, y_train, X_val, y_val, X_test, y_test\n\n\ndef compute_accuracy(labels, outputs):\n    num_recordings, num_classes = np.shape(labels)\n\n    num_correct_recordings = 0\n    for i in range(num_recordings):\n        if np.all(labels[i, :]==outputs[i, :]):\n            num_correct_recordings += 1\n\n    return float(num_correct_recordings) \/ float(num_recordings)","e962e8d4":"X_train, y_train, _, _, X_test, y_test = load_dataset(flatten=True)","37cc3ca3":"\n%matplotlib inline\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    plt.title(\"Label: %i\"%y_train[i])\n    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');","892ea76f":"\nohe = OneHotEncoder()\nohe.fit(y_train.reshape(-1, 1))","4ebc9df8":"y_train_ohe = ohe.transform(y_train.reshape(-1, 1)).toarray()","ba4d549d":"y_test_ohe = ohe.transform(y_test.reshape(-1, 1)).toarray()","9f90eadd":"\nclass NN_clf:\n    def __init__(\n            self,\n            X_data,\n            Y_data,\n            n_hidden_neurons=50,\n            n_categories=10,\n            epochs=10,\n            batch_size=100,\n            eta=0.1,\n            lmbd=0.0):\n\n        self.X_data_full = X_data\n        self.Y_data_full = Y_data\n\n        self.n_inputs = X_data.shape[0]\n        self.n_features = X_data.shape[1]\n        self.n_hidden_neurons = n_hidden_neurons\n        self.n_categories = n_categories\n\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.iterations = self.n_inputs \/\/ self.batch_size\n        self.eta = eta\n        self.lmbd = lmbd\n\n        self.create_biases_and_weights()\n\n    def create_biases_and_weights(self):\n        self.hidden_weights = np.random.randn(self.n_features, self.n_hidden_neurons)\n        self.hidden_bias = np.zeros(self.n_hidden_neurons) + 0.01\n\n        self.output_weights = np.random.randn(self.n_hidden_neurons, self.n_categories)\n        self.output_bias = np.zeros(self.n_categories) + 0.01\n\n    def feed_forward(self):\n\n        def sigmoid(Z):\n            A = 1 \/ (1 + np.exp(-Z))\n            return A\n        # feed-forward for training\n        self.z_h = np.matmul(self.X_data, self.hidden_weights) + self.hidden_bias\n        self.a_h = sigmoid(self.z_h)\n\n        self.z_o = np.matmul(self.a_h, self.output_weights) + self.output_bias\n\n        exp_term = np.exp(self.z_o)\n        self.probabilities = exp_term \/ np.sum(exp_term, axis=1, keepdims=True)\n\n    def feed_forward_out(self, X):\n\n        def relu(Z):\n            return np.maximum(0,Z)\n\n        def sigmoid(Z):\n            A = 1 \/ (1 + np.exp(-Z))\n            return A\n\n        # feed-forward for output\n        z_h = np.matmul(X, self.hidden_weights) + self.hidden_bias\n        a_h = sigmoid(z_h)\n\n        z_o = np.matmul(a_h, self.output_weights) + self.output_bias\n        \n        exp_term = np.exp(z_o)\n        probabilities = exp_term \/ np.sum(exp_term, axis=1, keepdims=True)\n        return probabilities\n\n    def backpropagation(self):\n        error_output = self.probabilities - self.Y_data\n        error_hidden = np.matmul(error_output, self.output_weights.T) * self.a_h * (1 - self.a_h)\n\n        self.output_weights_gradient = np.matmul(self.a_h.T, error_output)\n        self.output_bias_gradient = np.sum(error_output, axis=0)\n\n        self.hidden_weights_gradient = np.matmul(self.X_data.T, error_hidden)\n        self.hidden_bias_gradient = np.sum(error_hidden, axis=0)\n\n        if self.lmbd > 0.0:\n            self.output_weights_gradient += self.lmbd * self.output_weights\n            self.hidden_weights_gradient += self.lmbd * self.hidden_weights\n\n        self.output_weights -= self.eta * self.output_weights_gradient\n        self.output_bias -= self.eta * self.output_bias_gradient\n        self.hidden_weights -= self.eta * self.hidden_weights_gradient\n        self.hidden_bias -= self.eta * self.hidden_bias_gradient\n\n    def predict(self, X):\n        probabilities = self.feed_forward_out(X)\n        return np.argmax(probabilities, axis=1)\n\n    def predict_probabilities(self, X):\n        probabilities = self.feed_forward_out(X)\n        return probabilities\n            \n\n    def train(self,X_val, y_val, transformer):\n\n        def compute_accuracy(labels, outputs):\n            num_recordings, num_classes = np.shape(labels)\n\n            num_correct_recordings = 0\n            for i in range(num_recordings):\n                if np.all(labels[i, :]==outputs[i, :]):\n                    num_correct_recordings += 1\n\n            return float(num_correct_recordings) \/ float(num_recordings)\n        data_indices = np.arange(self.n_inputs)\n\n        train_log = []\n        val_log = []\n\n        for i in range(self.epochs):\n            for j in range(self.iterations):\n                # pick datapoints with replacement\n                chosen_datapoints = np.random.choice(\n                    data_indices, size=self.batch_size, replace=False\n                )\n\n                # minibatch training data\n                self.X_data = self.X_data_full[chosen_datapoints]\n                self.Y_data = self.Y_data_full[chosen_datapoints]\n\n                self.feed_forward()\n                self.backpropagation()\n            pred_val = self.predict(X_val)\n            pred_val=transformer(pred_val.reshape(-1, 1)).toarray()\n            \n            pred_train = self.predict(self.X_data_full)\n            pred_train = transformer(pred_train.reshape(-1, 1)).toarray()\n            #print(np.argmax(y_val, axis=1).shape)\n\n            train_log.append(compute_accuracy(self.Y_data_full,pred_train))\n            val_log.append(compute_accuracy(y_val,pred_val))\n            #print(compute_accuracy(y_val,pred_val))\n\n            clear_output()\n            print(\"Epoch\",i)\n            print(\"Train accuracy:\",train_log[-1])\n            print(\"Val accuracy:\",val_log[-1])\n            plt.plot(train_log,label='train accuracy')\n            plt.plot(val_log,label='val accuracy')\n            plt.legend(loc='best')\n            plt.grid()\n            plt.show()\n\ndef sigmoid(Z):\n    A = 1 \/ (1 + np.exp(-Z))\n    return A\n","d18d0082":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5)\nepochs = 10\nbatch_size = 500\neta = 0.001\nlmbd = 0.001\nn_hidden_neurons = 50\nval_res = []\ntrain_res = []\n\n\nfor train_index, test_index in kf.split(X_train):\n    X_train_, X_val_ = X_train[train_index], X_train[test_index]\n    y_train_, y_val_ = y_train_ohe[train_index], y_train_ohe[test_index]\n\n    dnn = NN_clf(X_train_, y_train_,eta=eta, lmbd=lmbd, epochs=epochs, batch_size=batch_size,\n                        n_hidden_neurons=n_hidden_neurons, n_categories=y_train_ohe.shape[1])\n    dnn.train(X_val_, y_val_,ohe.transform)\n    val_predict = dnn.predict(X_val_)\n    train_predict = dnn.predict(X_train_)\n    print(compute_accuracy(y_val_.argmax(axis=1).reshape(y_val_.argmax(axis=1).shape[0],1),val_predict.reshape(val_predict.shape[0],1)))\n    train_res.append(compute_accuracy(y_train_.argmax(axis=1).reshape(y_train_.argmax(axis=1).shape[0],1),train_predict.reshape(train_predict.shape[0],1)))\n    val_res.append(compute_accuracy(y_val_.argmax(axis=1).reshape(y_val_.argmax(axis=1).shape[0],1),val_predict.reshape(val_predict.shape[0],1)))","130ff0b4":"val_res","6100ac34":"plt.style.use('seaborn-whitegrid')\nplt.style.use('seaborn-paper')\nplt.boxplot([train_res,val_res])\nplt.xticks(ticks = [1,2], labels = [\"Train data\",\"Validation data\"], fontsize=14, rotation=0)\nplt.yticks(fontsize=14)\nplt.ylabel(\"Accuracy\",fontsize=14)\nplt.title(\"Training vs validation data\",fontsize=14)\nplt.show()","35e922fa":"test_predict = dnn.predict(X_test)","420f7773":"print(compute_accuracy(y_test.reshape(y_test.shape[0],1),test_predict.reshape(test_predict.shape[0],1)))","8cd8f80a":"y_pred_ohe = ohe.transform(test_predict.reshape(-1, 1)).toarray()","a031a000":"conf_matrix = compute_modified_confusion_matrix_nonorm(y_test_ohe, y_pred_ohe)","72478661":"plt.figure(figsize = (20,20))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matrix, cmap=\"rocket_r\", annot=True,annot_kws={\"size\": 20}, fmt=\".2f\", cbar=False)\nplt.title(\"Predict MNIST data using own Neural Network\", fontsize = 40, color= \"black\")\nplt.xlabel(\"y predicted\",fontsize=30, color= \"black\")\nplt.ylabel(\"y true\",fontsize = 30, color= \"black\")\nplt.yticks(fontsize=30, rotation=0, color= \"black\")\nplt.xticks(fontsize=30, rotation=0, color= \"black\")\nplt.savefig(\"MNIST_confMatrix_ownNN.png\",dpi=300)\nplt.show()","bd6c735c":"# <center> Develop a Feed Forward Neural Network from scratch <\/center>\n\n![Neural Network](https:\/\/www.researchgate.net\/profile\/Arnaud-Nguembang-Fadja\/publication\/329586439\/figure\/fig1\/AS:702963681087490@1544611010266\/Feedforward-neural-network_W640.jpg)\n\nFigure 1:  Feedforward neural network. The image is redistributed from [Fadja et al 2018](https:\/\/www.researchgate.net\/publication\/329586439_Vision_Inspection_with_Neural_Networks)"}}