{"cell_type":{"90e664ee":"code","79ef678b":"code","d4d45209":"code","57176ac4":"code","eb053e78":"code","6141594a":"code","796e6775":"code","738e1964":"code","7cbad062":"code","abb54063":"code","d23f856a":"code","b42fbb53":"code","60cba13c":"code","038e4099":"code","5bdaf85f":"code","9a50cc53":"code","825e346a":"code","7c19d249":"code","5e77561b":"code","3b610812":"code","08bfd4ca":"code","31901ff9":"code","e08b371b":"code","b69d747d":"markdown","3f69709d":"markdown","e717181f":"markdown"},"source":{"90e664ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#for visualising\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","79ef678b":"dataset=pd.read_csv(\"..\/input\/spam.csv\",encoding=\"latin-1\")","d4d45209":"dataset.head()","57176ac4":"dataset.columns","eb053e78":"dataset = dataset.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)","6141594a":"dataset.tail()","796e6775":"dataset.info()","738e1964":"#for text preprocessing\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n","7cbad062":"corpus=[]\nfor i in range(0,len(dataset)):\n    review=re.sub(\"[^a-zA-Z]\",\" \",dataset.iloc[i][1])\n    review=review.lower()\n    review=review.split()\n    ps=PorterStemmer()\n    review=[ps.stem(word) for word in review if not word in set(stopwords.words(\"english\"))]\n    review=\" \".join(review)\n    corpus.append(review)","abb54063":"#before and after text preprocessing\nprint(dataset.iloc[6,1])\nprint(30*\"*\")\nprint(corpus[6])","d23f856a":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer()\nX=cv.fit_transform(corpus).toarray()\ny=dataset.iloc[:,0].values\ny=[0 if each==\"ham\" else 1 for each in y]","b42fbb53":"X","60cba13c":"#Classification algorithms\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n#These are for visualising\nalgoritma=[]\naccuracy=[]","038e4099":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier=DecisionTreeClassifier(random_state=0,criterion=\"entropy\")\nclassifier.fit(X_train,y_train)\n\n#predict\ny_pred=classifier.predict(X_test)\n\n\nfrom sklearn.metrics import confusion_matrix\ncm_dt=confusion_matrix(y_test,y_pred)\ndtscore=classifier.score(X_test,y_test)*100\nprint(\"Decision Tree accuracy=\",classifier.score(X_test,y_test)*100)\nalgoritma.append(\"Decision Tree\")\naccuracy.append(dtscore)\n\n#plot confusion matrix\nf,ax=plt.subplots(figsize=(4,4))\nsns.heatmap(cm_dt,annot=True,fmt=\".0f\",ax=ax,linewidths=2,linecolor=\"blue\")\nplt.title(\"Decision Tree Confusion Matrix\")\nplt.xlabel(\"predicted Values\")\nplt.ylabel(\"true values\")\nplt.show()","5bdaf85f":"#Find the optimal  k-value\n\n#error=[]\n#knn_accuracy=[]\n#for i in range(3, 9): \n#    classifier=KNeighborsClassifier(n_neighbors=i)\n#    classifier.fit(X_train,y_train)\n#    pred_i = classifier.predict(X_test)\n#    error.append(np.mean(pred_i != y_test))\n#    knn_accuracy.append(classifier.score(X_test,y_test)*100)\n#plt.figure(figsize=(12, 6))  \n#plt.plot(range(3, 9), error, color='red', linestyle='dashed', marker='o',  \n#         markeclassifieracecolor='blue', markersize=10)\n#plt.title('Error Rate K Value')  \n#plt.xlabel('K Value')  \n#plt.ylabel('Mean Error')  \n#\n#plt.figure(figsize=(12, 6))  \n#plt.plot(range(3, 9), knn_accuracy, color='red', linestyle='dashed', marker='o',  \n#         markeclassifieracecolor='blue', markersize=10)\n#plt.title('Accuracy of K Value')  \n#plt.xlabel('K Value')  \n#plt.ylabel('Accuracy')","9a50cc53":"from sklearn.neighbors import KNeighborsClassifier\n\nclassifier=KNeighborsClassifier(n_neighbors=3)\nclassifier.fit(X_train,y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_knn= confusion_matrix(y_test, y_pred)\nknn=classifier.score(X_test,y_test)*100\nprint(\"KNN accuracy=\",classifier.score(X_test,y_test)*100)\nalgoritma.append(\"KNN\")\naccuracy.append(knn)\n\n#plot confusion matrix\nf,ax=plt.subplots(figsize=(4,4))\nsns.heatmap(cm_knn,annot=True,fmt=\".0f\",ax=ax,linewidths=2,linecolor=\"blue\")\nplt.title(\"KNN Confusion Matrix\")\nplt.xlabel(\"predicted Values\")\nplt.ylabel(\"true values\")\nplt.show()","825e346a":"#Find the optimal n_estimators value\n\n#n_estimators = np.arange(1,200,5)\n#train_results = []\n#test_results = []\n#from sklearn.metrics import roc_curve, auc\n#for estimator in n_estimators:\n#   classifier = RandomForestClassifier(n_estimators=estimator)\n#   classifier.fit(X_train, y_train)\n#   train_pred = classifier.predict(X_train)\n#   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n#   roc_auc = auc(false_positive_rate, true_positive_rate)\n#   train_results.append(roc_auc)\n#   y_pred = classifier.predict(X_test)\n#   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n#   roc_auc = auc(false_positive_rate, true_positive_rate)\n#   test_results.append(roc_auc)\n#\n#plt.plot(n_estimators, train_results, \"b\", label=\"Train AUC\")\n#plt.plot(n_estimators, test_results, \"r\", label=\"Test AUC\")\n#plt.legend()\n#plt.ylabel(\"AUC score\")\n#plt.xlabel(\"n_estimators\")\n#plt.show()","7c19d249":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier=RandomForestClassifier(n_estimators=67,criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train,y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_rf= confusion_matrix(y_test, y_pred)\nrandom_forest=classifier.score(X_test,y_test)*100\nprint(\"Random Forest accuracy=\",classifier.score(X_test,y_test)*100)\nalgoritma.append(\"Random Forest\")\naccuracy.append(random_forest)\n\n#plot confusion matrix\nf,ax=plt.subplots(figsize=(4,4))\nsns.heatmap(cm_rf,annot=True,fmt=\".0f\",ax=ax,linewidths=2,linecolor=\"blue\")\nplt.title(\"Random Forest Confusion Matrix\")\nplt.xlabel(\"predicted Values\")\nplt.ylabel(\"true values\")\nplt.show()","5e77561b":"#SVM\nfrom sklearn.svm import SVC\nclassifier=SVC(kernel=\"linear\",random_state=0)\nclassifier.fit(X_train,y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_svm = confusion_matrix(y_test, y_pred)\nsvm=classifier.score(X_test,y_test)*100\nprint(\"SVM accuracy=\",classifier.score(X_test,y_test)*100)\nalgoritma.append(\"SVM\")\naccuracy.append(svm)\n\n#plot confusion matrix\nf,ax=plt.subplots(figsize=(4,4))\nsns.heatmap(cm_svm,annot=True,fmt=\".0f\",ax=ax,linewidths=2,linecolor=\"blue\")\nplt.title(\"SVM Confusion Matrix\")\nplt.xlabel(\"predicted Values\")\nplt.ylabel(\"true values\")\nplt.show()","3b610812":"#Navie Bayes\nfrom sklearn.naive_bayes import GaussianNB\nclassifier=GaussianNB()\nclassifier.fit(X_train,y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_nb= confusion_matrix(y_test, y_pred)\nnb=classifier.score(X_test,y_test)*100\nprint(\"Navie Bayes accuracy=\",classifier.score(X_test,y_test)*100)\nalgoritma.append(\"Navie Bayes\")\naccuracy.append(nb)\n\n#plot confusion matrix\nf,ax=plt.subplots(figsize=(4,4))\nsns.heatmap(cm_nb,annot=True,fmt=\".0f\",ax=ax,linewidths=2,linecolor=\"blue\")\nplt.title(\"Navie Bayes Confusion Matrix\")\nplt.xlabel(\"predicted Values\")\nplt.ylabel(\"true values\")\nplt.show()","08bfd4ca":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier=LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_lr = confusion_matrix(y_test, y_pred)\nlr=classifier.score(X_test,y_test)*100\nprint(\"Logistic Regression accuracy=\",classifier.score(X_test,y_test)*100)\nalgoritma.append(\"Logistic Regression\")\naccuracy.append(lr)\n\n\n#plot confusion matrix\nf,ax=plt.subplots(figsize=(4,4))\nsns.heatmap(cm_lr,annot=True,fmt=\".0f\",ax=ax,linewidths=2,linecolor=\"blue\")\nplt.title(\"Logistic Regression Confusion Matrix\")\nplt.xlabel(\"predicted Values\")\nplt.ylabel(\"true values\")\nplt.show()","31901ff9":"plt.figure(figsize=(24,24))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\n#Logistic Regression Confusion Matrix\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#Decision Tree Confusion Matrix\nplt.subplot(2,3,2)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dt,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#K Nearest Neighbors Confusion Matrix\nplt.subplot(2,3,3)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#Naive Bayes Confusion Matrix\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#Random Forest Confusion Matrix\nplt.subplot(2,3,5)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rf,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#Support Vector Machine Confusion Matrix\nplt.subplot(2,3,6)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(cm_svm,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\"d\")\n\nplt.show()","e08b371b":"f=plt.subplots(figsize=(20,20))\nplt.bar(algoritma,accuracy,color=\"orange\")\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Accuracy\")\nplt.show()","b69d747d":"**If you're wondering how I found the n_estimators value, you can run the above code.\nThis process takes a very long time.**","3f69709d":"**If you're wondering how I found the k value, you can run the above code.\nThis process takes a very long time.**","e717181f":"<h2>CONCLUSION<\/h2>\n    \n**    According to this dataset SVM algorithm has found the best result.The SVM algorithm mis-classified only 19 data and showed 98.29% success.** \n\n**Waiting for your questions and suggestions.**"}}