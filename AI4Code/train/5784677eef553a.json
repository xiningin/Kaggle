{"cell_type":{"2c37d804":"code","74eae7ff":"code","75325211":"code","7fb3da05":"code","206f488c":"code","468bab3f":"code","fd3a8652":"code","3de6b939":"code","448f0fa5":"code","8f3b95de":"code","8f3f3031":"code","782e473e":"code","27f52255":"code","5aa8a1a8":"code","a83efa52":"code","cdc03d2d":"code","db64ee22":"code","b15c2206":"code","45e6ee5f":"code","0ac706b4":"code","c901f95c":"code","3873192c":"code","21b85c65":"code","96582720":"code","70254487":"code","e2fc72f9":"code","eea83206":"code","2d6e40f7":"code","a5a13d8d":"code","2585d161":"code","b9e2cff3":"code","0ef8402c":"code","e1949002":"code","bb2ea54d":"code","c1261cff":"code","4d191864":"code","7f87154a":"code","dc6b2c60":"code","95b55cb7":"code","5272ddcb":"code","bfa663ba":"code","3032c7db":"code","509b7cb9":"code","7c4858d2":"code","f4a0591b":"code","0300a3a5":"code","b83fd2f4":"code","623c3aed":"code","1831a54d":"markdown","2ee23b95":"markdown","ff32af4e":"markdown","213454af":"markdown","b0822bac":"markdown","a829abbf":"markdown","352b5b74":"markdown"},"source":{"2c37d804":"import numpy as np # linear algebra\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('max_colwidth', None)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","74eae7ff":"# train=pd.read_csv('\/kaggle\/input\/ames-housing-dataset\/AmesHousing.csv')\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain2=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","75325211":"train2.head()","7fb3da05":"train2.columns = train2.columns.str.replace(' ', '')\ntrain2=train2.rename(columns={\"YearRemod\/Add\": \"YearRemodAdd\"})","206f488c":"train2.head()","468bab3f":"test.head()","fd3a8652":"# print(\"Size of the Ames Dataset\",len(train))\nprint(\"Size of the Housing Dataset\",len(train2))\nprint(\"Size of the Housing Test Dataset\",len(test))","3de6b939":"print(train2.shape)\ntrain2 = train2.drop_duplicates()\nprint(train2.shape)\n","448f0fa5":"useless = ['Id'] \ntrain2 = train2.drop(useless, axis = 1)\ntrain2.shape","8f3b95de":"from scipy.stats import norm\n(mu, sigma) = norm.fit(train2['SalePrice'])\nplt.figure(figsize = (12,6))\nsns.distplot(train2['SalePrice'], kde = True, hist=True, fit = norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel(\"House's sale Price in $\", fontsize = 12)\nplt.legend(['Normal dist ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma),'actual price dist'],loc='best')\nplt.show()","8f3f3031":"from scipy import stats\nshap = stats.shapiro(train2['SalePrice'])\nprint('Skewness : %f' % abs(train2['SalePrice']).skew())\nprint('Kurtosis : %f' % abs(train2['SalePrice']).kurt())\nprint('Shapiro_Test_statistic : %f' % shap.statistic )\nprint('Shapiro_Test_pvalue : %f' % shap.pvalue )","782e473e":"f, ax = plt.subplots(figsize=(50, 35))\nmat = train2.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","27f52255":"# OverallQuall - SalePrice [Pearson = 0.8]\nfig,ax=plt.subplots(1,3,figsize=(20,10))\nsns.stripplot(data=train2,x='OverallQual',y='SalePrice',ax=ax[1])\nsns.violinplot(data=train2,x='OverallQual',y='SalePrice',ax=ax[2])\nsns.boxplot(data=train2,x='OverallQual',y='SalePrice',ax=ax[0])\nplt.show()","5aa8a1a8":"# BsmtFinSF2 - SalePrice [Pearson = -0.011\nfig,ax=plt.subplots(1,3,figsize=(20,10))\nsns.stripplot(data=train2,x='BsmtFinSF2',y='SalePrice',ax=ax[1])\nsns.violinplot(data=train2,x='BsmtFinSF2',y='SalePrice',ax=ax[2])\nsns.boxplot(data=train2,x='BsmtFinSF2',y='SalePrice',ax=ax[0])\nplt.show()","a83efa52":"# GrLivArea vs SalePrice [corr = 0.71]\n\nPearson_GrLiv = 0.71\nplt.figure(figsize = (12,6))\nsns.regplot(data=train2, x = 'GrLivArea', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('GrLivArea vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_GrLiv)], loc = 'best')\nplt.show()","cdc03d2d":"# YearBuilt vs SalePrice\n\nPearson_YrBlt = 0.56\nplt.figure(figsize = (12,6))\nsns.regplot(data=train2, x = 'YearBuilt', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('YearBuilt vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_YrBlt)], loc = 'best')\nplt.show()","db64ee22":"plt.figure(figsize=(15,10))\nsns.barplot(x='YrSold',y='SalePrice',data=train2,estimator=np.median)\nplt.title('Median of Sale Price by Year')\nplt.xlabel('Year of Selling')\nplt.ylabel('Median of Price')\nplt.show()","b15c2206":"# Separating Target and Features\n\ntarget = train2['SalePrice']\ntest_id = test['Id']\ntest = test.drop(['Id'],axis = 1)\ntrain2_1 = train2.drop(['SalePrice'],axis = 1)\nprint(\"train_datasets shape\",train2.shape)\nprint(\"test_datasets shape\",test.shape)\n\ntrain_test = pd.concat([train2_1,test], axis=0, sort=False)\nprint(train_test.shape)","45e6ee5f":"nan=pd.DataFrame(train_test.isna().sum(),columns=['Nan_sum'])\nnan['feat']=nan.index\nnan=nan[nan['Nan_sum']>0]\nnan['Percentage']=(nan['Nan_sum']\/1460)*100\n\nnan=nan.sort_values(by=['Nan_sum'])\nnan.insert(0,'Serial No.',range(1,len(nan)+1))\nnan","0ac706b4":"plt.figure(figsize=(20,10))\nsns.barplot(x=nan['feat'],y=nan['Percentage'])\nplt.xticks(rotation=40)\nplt.title('Features Containing Nan')\nplt.xlabel('Features')\nplt.ylabel('% of Missing Data')\nplt.show()","c901f95c":"# Converting non-numeric predictors stored as numbers into string\n\ntrain_test['MSSubClass'] = train_test['MSSubClass'].apply(str)\ntrain_test['YrSold'] = train_test['YrSold'].apply(str)\ntrain_test['MoSold'] = train_test['MoSold'].apply(str)\ntrain_test['OverallQual'] = train_test['OverallQual'].apply(str)\ntrain_test['OverallCond'] = train_test['OverallCond'].apply(str)","3873192c":"# Filling Categorical NaN (That we know how to fill due to the description file )\n\ntrain_test['Functional'] = train_test['Functional'].fillna('Typ')\ntrain_test['Electrical'] = train_test['Electrical'].fillna(\"SBrkr\")\ntrain_test['KitchenQual'] = train_test['KitchenQual'].fillna(\"TA\")\ntrain_test['Exterior1st'] = train_test['Exterior1st'].fillna(train_test['Exterior1st'].mode()[0])\ntrain_test['Exterior2nd'] = train_test['Exterior2nd'].fillna(train_test['Exterior2nd'].mode()[0])\ntrain_test['SaleType'] = train_test['SaleType'].fillna(train_test['SaleType'].mode()[0])\ntrain_test[\"PoolQC\"] = train_test[\"PoolQC\"].fillna(\"None\")\ntrain_test[\"Alley\"] = train_test[\"Alley\"].fillna(\"None\")\ntrain_test['FireplaceQu'] = train_test['FireplaceQu'].fillna(\"None\")\ntrain_test['Fence'] = train_test['Fence'].fillna(\"None\")\ntrain_test['MiscFeature'] = train_test['MiscFeature'].fillna(\"None\")\nfor col in ('GarageArea', 'GarageCars'):\n    train_test[col] = train_test[col].fillna(0)\n        \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    train_test[col] = train_test[col].fillna('None')\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train_test[col] = train_test[col].fillna('None')\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea','BsmtUnfSF', 'TotalBsmtSF'):\n    train_test[col] = train_test[col].fillna(0)\n\ntrain_test['LotFrontage'] = train_test['LotFrontage'].fillna(train2['LotFrontage'].median())\n    \n    # Checking the features with NaN remained out\n\nfor col in train_test:\n    if train_test[col].isna().sum() > 0:\n        print(train_test[col].isna().sum(),'::::',train_test[col].name)","21b85c65":"train_test[\"SqFtPerRoom\"] = train_test[\"GrLivArea\"] \/ (train_test[\"TotRmsAbvGrd\"] +\n                                                       train_test[\"FullBath\"] +\n                                                       train_test[\"HalfBath\"] +\n                                                       train_test[\"KitchenAbvGr\"])\n\ntrain_test['Total_Home_Quality'] = train_test['OverallQual'] + train_test['OverallCond']\n\ntrain_test['Total_Bathrooms'] = (train_test['FullBath'] + (0.5 * train_test['HalfBath']) +\n                               train_test['BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))\n\ntrain_test[\"HighQualSF\"] = train_test[\"1stFlrSF\"] + train_test[\"2ndFlrSF\"]\ntrain_test['renovated']=train_test['YearRemodAdd']+train_test['YearBuilt']","96582720":"# Removing the useless variables\n\nuseless = ['GarageYrBlt'] \ntrain_test = train_test.drop(useless, axis = 1)","70254487":"# Creating dummy variables from categorical features\n\ntrain_test_dummy = pd.get_dummies(train_test)\nfrom scipy.stats import skew\nnumeric_features = train_test_dummy.dtypes[train_test_dummy.dtypes != object].index\nskewed_features = train_test_dummy[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index\n","e2fc72f9":"# Normalize skewed features using log_transformation\n    \nfor i in skew_index:\n    train_test_dummy[i] = np.log1p(train_test_dummy[i] )","eea83206":"nan=pd.DataFrame(train_test_dummy.isna().sum(),columns=['Nan_sum'])\nnan['feat']=nan.index\nnan=nan[nan['Nan_sum']>0]\nnan['Percentage']=(nan['Nan_sum']\/1460)*100\nnan=nan.sort_values(by=['Nan_sum'])\nnan.insert(0,'Serial No.',range(1,len(nan)+1))\nnan","2d6e40f7":"inf=pd.DataFrame(np.isinf(train_test_dummy).sum() ,columns=['Inf_sum'])\ninf['feat']=inf.index\ninf=inf[inf['Inf_sum']>0]\ninf=inf.sort_values(by=['Inf_sum'])\ninf.insert(0,'Serial No.',range(1,len(inf)+1))\ninf","a5a13d8d":"import statsmodels.api as sm\n# SalePrice before transformation\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\" qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\n#research sm \nsns.distplot(target, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","2585d161":"# SalePrice after transformation\n\ntarget_log = np.log1p(target)\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\"qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target_log, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\nsns.distplot(target_log, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","b9e2cff3":"import shap\nfrom xgboost import XGBRegressor\nfrom catboost import Pool\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error","0ef8402c":"# Train-Test separation\n\nX_train = train_test_dummy[0:train2.shape[0]]\nX_test = train_test_dummy[train2.shape[0]:]\nprint(X_train.shape)\nprint(X_test.shape)\n\n# Creation of the RMSE metric:\n    \ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, X_train, target_log, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","e1949002":"nan=pd.DataFrame(X_train.isna().sum(),columns=['Nan_sum'])\nnan['feat']=nan.index\nnan=nan[nan['Nan_sum']>0]\nnan['Percentage']=(nan['Nan_sum']\/1460)*100\nnan=nan.sort_values(by=['Nan_sum'])\nnan.insert(0,'Serial No.',range(1,len(nan)+1))\nnan","bb2ea54d":"nan=pd.DataFrame(X_test.isna().sum(),columns=['Nan_sum'])\nnan['feat']=nan.index\nnan=nan[nan['Nan_sum']>0]\nnan['Percentage']=(nan['Nan_sum']\/1460)*100\nnan['Perc']=(nan['Nan_sum']\/2919)*100\nnan=nan.sort_values(by=['Nan_sum'])\nnan.insert(0,'Serial No.',range(1,len(nan)+1))\nnan","c1261cff":"# 10 Fold Cross validation\n\nkf = KFold(n_splits=11, random_state=42, shuffle=True)\n\ncv_scores = []\ncv_std = []\n\n# baseline_models = ['Linear_Reg.','Bayesian_Ridge_Reg.','LGBM_Reg.','SVR',\n#                    'Dec_Tree_Reg.','Random_Forest_Reg.', 'XGB_Reg.',\n#                    'Grad_Boost_Reg.','Cat_Boost_Reg.','Stacked_Reg.','Stacked_Reg2']\n\nbaseline_models = ['Linear_Reg.','XGB_Reg.',\n                   'Grad_Boost_Reg.','Cat_Boost_Reg.','Stacked_Reg.','Stacked_Reg2']","4d191864":"# Linear Regression\n\nlreg = LinearRegression()\nscore_lreg = cv_rmse(lreg)\ncv_scores.append(score_lreg.mean())\ncv_std.append(score_lreg.std())\n\n# # Bayesian Ridge Regression\n\n# brr = BayesianRidge(compute_score=True)\n# score_brr = cv_rmse(brr)\n# cv_scores.append(score_brr.mean())\n# cv_std.append(score_brr.std())\n\n# # Light Gradient Boost Regressor\n\n# l_gbm = LGBMRegressor(objective='regression')\n# score_l_gbm = cv_rmse(l_gbm)\n# cv_scores.append(score_l_gbm.mean())\n# cv_std.append(score_l_gbm.std())\n\n# # Support Vector Regression\n\n# svr = SVR()\n# score_svr = cv_rmse(svr)\n# cv_scores.append(score_svr.mean())\n# cv_std.append(score_svr.std())\n\n# # Decision Tree Regressor\n\n# dtr = DecisionTreeRegressor()\n# score_dtr = cv_rmse(dtr)\n# cv_scores.append(score_dtr.mean())\n# cv_std.append(score_dtr.std())\n\n# # Random Forest Regressor\n\n# rfr = RandomForestRegressor()\n# score_rfr = cv_rmse(rfr)\n# cv_scores.append(score_rfr.mean())\n# cv_std.append(score_rfr.std())\n\n# XGB Regressor\n\nxgb = XGBRegressor()\nscore_xgb = cv_rmse(xgb)\ncv_scores.append(score_xgb.mean())\ncv_std.append(score_xgb.std())\n\n# Gradient Boost Regressor\n\ngbr = GradientBoostingRegressor()\nscore_gbr = cv_rmse(gbr)\ncv_scores.append(score_gbr.mean())\ncv_std.append(score_gbr.std())\n\n# Cat Boost Regressor\n\ncatb = CatBoostRegressor()\nscore_catb = cv_rmse(catb)\ncv_scores.append(score_catb.mean())\ncv_std.append(score_catb.std())\n\n# Stacked Regressor\n\nstack_gen = StackingRegressor(regressors=(CatBoostRegressor(),\n                                          BayesianRidge()),\n                              meta_regressor = CatBoostRegressor(),\n                              use_features_in_secondary = True)","7f87154a":"score_stack_gen = cv_rmse(stack_gen)\ncv_scores.append(score_stack_gen.mean())\ncv_std.append(score_stack_gen.std())\n","dc6b2c60":"# Stacked Regressor\n\nstack_gen2 = StackingRegressor(regressors=(CatBoostRegressor(),\n                                          XGBRegressor()),\n                              meta_regressor = CatBoostRegressor(),\n                              use_features_in_secondary = True)\n\nscore_stack_gen2 = cv_rmse(stack_gen2)\ncv_scores.append(score_stack_gen2.mean())\ncv_std.append(score_stack_gen2.std())\n\n","95b55cb7":"final_cv_score = pd.DataFrame(baseline_models, columns = ['Regressors'])\nfinal_cv_score['RMSE_mean'] = cv_scores\nfinal_cv_score['RMSE_std'] = cv_std","5272ddcb":"final_cv_score","bfa663ba":"plt.figure(figsize = (12,8))\nsns.barplot(final_cv_score['Regressors'],final_cv_score['RMSE_mean'])\nplt.xlabel('Regressors', fontsize = 12)\nplt.ylabel('CV_Mean_RMSE', fontsize = 12)\nplt.xticks(rotation=40)\nplt.show()","3032c7db":"cat = CatBoostRegressor()\ncat_model = cat.fit(X_train,target_log,\n                     plot=True,\n                     verbose = 0)","509b7cb9":"feat_imp = cat_model.get_feature_importance(prettified=True)\nfeat_imp.head()","7c4858d2":"# Plotting top 30 features' importance\n\nplt.figure(figsize = (12,8))\nsns.barplot(feat_imp['Importances'][:30],feat_imp['Feature Id'][:30], orient = 'h')\nplt.show()","f4a0591b":"params = {'iterations': 6000,\n          'learning_rate': 0.005,\n          'depth': 4,\n          'l2_leaf_reg': 1,\n          'eval_metric':'RMSE',\n          'early_stopping_rounds': 200,\n          'verbose': 200,\n          'random_seed': 42}\n         \ncat_f = CatBoostRegressor(**params)\ncat_model_f = cat_f.fit(X_train,target_log,\n                     plot=True,\n                     verbose = False)","0300a3a5":"test_pred = cat_f.predict(X_test)\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pred = np.expm1(test_pred)\nsubmission['SalePrice'] = test_pred \nsubmission.head()\nsubmission.to_csv(r\"C:\\Users\\Administrator\\Desktop\\cat.csv\", index = False, header = True)","b83fd2f4":"stack_f=stack_gen.fit(X_train,target_log)\ntest_stack = stack_gen.predict(X_test)\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pre = np.expm1(test_stack)\nsubmission['SalePrice'] = test_pre\n\nsubmission.to_csv(r\"C:\\Users\\Administrator\\Desktop\\stack.csv\", index = False, header = True)","623c3aed":"stack_f2=stack_gen2.fit(X_train,target_log)\ntest_stack = stack_gen2.predict(X_test)\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pre = np.expm1(test_stack)\nsubmission['SalePrice'] = test_pre\n\nsubmission.to_csv(\"C:\\Users\\Administrator\\Desktop\\stack2.csv\", index = False, header = True)","1831a54d":"checking for nan values in test set","2ee23b95":"In literature, acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis. Looking at the plot, we can clearly see how the distribution does not seem to be normal, but highly right-skewed. The non-normality of our distribution is also supported by the Shapiro test for normality (p-value really small that allows us to reject the hypotesis of normality). Despite that, let's leave it like that for now, we'll deal with that later in the notebook.","ff32af4e":"checking for nan values in training set","213454af":"Finding duplicates in data","b0822bac":"transforming the sale price ","a829abbf":"checking if the values are in infinity or not after log transformation","352b5b74":"Checking for Nan values after dummy"}}