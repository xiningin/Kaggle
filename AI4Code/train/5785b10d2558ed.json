{"cell_type":{"dd197500":"code","f4ceb1fc":"code","80cad87f":"code","a44ada2b":"code","cf1f04d0":"code","3b5d4006":"code","b24e150b":"code","c3e5d8b6":"code","2652eacb":"code","1cab0aee":"code","9f859468":"code","26c551c9":"code","f7523474":"code","a6edaa25":"code","71b0ae62":"code","3e7ecdb6":"code","4eed22e7":"code","6ef50f87":"code","7e5e835c":"code","c03a1c69":"code","56f48b7d":"code","8c18d31b":"code","611d738f":"markdown","cf895f4a":"markdown","1faa5c4f":"markdown","eb158161":"markdown","23b032ba":"markdown","1683dd2f":"markdown","740bc2f7":"markdown","1d56d412":"markdown","cd8de2ba":"markdown","fc0e2f69":"markdown","09c1e622":"markdown","738d87b0":"markdown","2cfbe504":"markdown","aa724704":"markdown","f2214a3f":"markdown","c9a4391f":"markdown"},"source":{"dd197500":"import pandas as pd                 # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px         # plotting for EDA\nimport re                           # Regex library for preserving\/removing substrings\nfrom typing import List             # List type for heler functions\nimport chardet                      # For detecting the encoding of a file\n\nimport base64\nimport requests\n# import tweepy\n\n#---------------------------------------All nltk imports-----------------------------------------------\nimport nltk\nfrom nltk.corpus import twitter_samples # For working with a twitter dataset\nfrom nltk.tokenize import word_tokenize # For breaking down a sentence string into a list of words\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer  # For converting tokens back into strings\nfrom nltk.probability import FreqDist # Frequency Distribution\nfrom nltk import regexp_tokenize # For tokenization of regular expressions\nfrom nltk.text import Text # For concordance\nfrom nltk.stem import PorterStemmer # For stemming the words\nfrom nltk.stem.wordnet import WordNetLemmatizer # For Lemmatization\nfrom nltk.sentiment import SentimentIntensityAnalyzer # For default sentiment analysis\n#------------------------------------------------------------------------------------------------------\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f4ceb1fc":"# def get_tweet_by_id(idd):\n#     collected_tweets = []\n    \n#     client_key = 'CLIENT_KEY'\n#     client_secret = 'CLIENT_SECRET'\n\n#     key_secret = f'{client_key}:{client_secret}'.encode('ascii')\n\n#     b64_encoded_key = base64.b64encode(key_secret)\n#     b64_encoded_key = b64_encoded_key.decode('ascii')\n    \n#     base_url = 'https:\/\/api.twitter.com\/'\n#     auth_endpoint = base_url+'oauth2\/token'\n#     auth_headers = {'Authorization': f'Basic {b64_encoded_key}'}\n#     auth_data = {'grant_type': 'client_credentials'}\n    \n#     response = requests.post(auth_endpoint, headers=auth_headers, data=auth_data)\n\n#     json_data = response.json()\n#     access_token = json_data['access_token']\n#     search_headers = {'Authorization': f\"Bearer {access_token}\"}\n    \n#     for i in range(len(idd)):\n#         parameters = {'ids': idd[i]}\n#         search_url = 'https:\/\/api.twitter.com\/2\/tweets'\n#         response = requests.get(search_url, headers=search_headers, params=parameters)\n#         response_tweet = response.json()\n#         collected_tweets.append(response_tweet['data'][0]['text'])\n        \n#     return collected_tweets","80cad87f":"# client = tweepy.Client(bearer_token='BEARER_TOKEN')\n# get_tweets = []\n\n# # We will have 1000 tweets to work with, due to tweepy's limit of number of tweets allowed per request\n# for tweet in tweepy.Paginator(client.get_users_tweets, id='50393960', max_results=100).flatten(limit=1000):\n#     get_tweets.append(str(tweet))\n\n# for tweet in tweepy.Paginator(client.get_users_tweets, id='21401934', max_results=100).flatten(limit=1000):\n#     get_tweets.append(str(tweet))","a44ada2b":"# Converted the resulting tweet list to csv\n\n# btdf = pd.DataFrame(bill_tweets)\n# btdf.to_csv(\"bill_gates_tweets.csv\")","cf1f04d0":"def encode_check(f):\n    \"\"\"\n    Helper function to check encoding of incoming csv file\n    \"\"\"\n    \n    file = open(f, \"rb\").read(10000)\n    a = chardet.detect(file)\n    return a['encoding']\n\n\ndef parse_csv_tweets(fcsv: str) -> List[str]:\n    \"\"\"\n    A little helper function to parse a csv file into a list in order to work for this whole project\n    \"\"\"\n    \n    dcsv = pd.read_csv(fcsv, encoding=encode_check(fcsv))\n    \n    lst = [dcsv[col].tolist() for col in dcsv.columns \n           if((dcsv[col].dtype == object) and \n           (dcsv[col].str.contains('@|#').any()))]\n    print(\"File Parsed!\")\n    return lst[0]","3b5d4006":"# tweet_lst = parse_csv_tweets(\"..\/input\/tweetutf\/tweetsutf.csv\")\ntweet_lst = twitter_samples.strings(\"tweets.20150430-223406.json\")\n\n# Get an extensive list of stopwords from nltk\nstopwords = nltk.corpus.stopwords.words(\"english\")","b24e150b":"tweet_lst[:5]","c3e5d8b6":"# Removing any non-utf8 abnormalities\n\ntemp = []\nfor tweet in tweet_lst:\n    tweet = re.sub(r'[^\\x00-\\x7f]',r'', tweet)\n    temp.append(tweet)\n\ntweet_lst = temp","2652eacb":"tweet_lst[:5]","1cab0aee":"def tok_stem(lst):\n    \"\"\"\n    Removes stopwords, '@', hyperlinks, hashtags (not the letters following them) \n    and punctuations from all tweets and then stemming every word.\n    \"\"\"\n    \n    pst = PorterStemmer()\n    cleaned = []\n    for t in lst:\n        #tokenize with special characters and hyperlinks removed\n        tokens = regexp_tokenize(t, r\"\\s|[^a-zA-Z]|http\\S+\", gaps=True)\n\n        # Remove stopwords and the redundant 'RT'\n        cleanlst = [pst.stem(w.lower()) for w in tokens if ((w not in stopwords) and (\"RT\" not in w))]\n        cleanstr = ' '.join(cleanlst)\n        cleaned.append(cleanstr)\n    return cleaned\n    \nclean_tweets = tok_stem(tweet_lst)\nclean_tweets[:10]","9f859468":"def lemmatizer(lst):\n    \"\"\"\n    Removes inflected words from the clean tweets list\n    \"\"\"\n    \n    lem = WordNetLemmatizer()\n    cleaned = []\n    for t in lst:\n        tokens = word_tokenize(t)\n        lem_tokens = [lem.lemmatize(w) for w in tokens]\n        lem_str = ' '.join(lem_tokens)\n        cleaned.append(lem_str)\n    return cleaned\n    \nclean_tweets = lemmatizer(clean_tweets)\nclean_tweets[:10]","26c551c9":"def count_words(lst):\n    \"\"\"\n    A helper function to count words in a list of sentences (in this case, tweets)\n    \"\"\"\n    count = 0\n    for sentence in lst:\n        tks = sentence.split()\n        count += len(tks)\n    return count","f7523474":"print(\"Word Count before removal: \" + str(count_words(tweet_lst)) + \"\\n\" + \n      \"Word Count after removal: \" + str(count_words(clean_tweets)))","a6edaa25":"fd = FreqDist()\n\nfor t in clean_tweets:\n    tokens = word_tokenize(t)\n    for words in tokens:\n        fd[words] += 1\n        \nfd_ten = fd.most_common(10) # top 10 most common words\nfd_ten","71b0ae62":"fd_t = fd.tabulate(10)","3e7ecdb6":"sen = SentimentIntensityAnalyzer()\n\nsentiment_scores = []\nfor tweet in clean_tweets:\n    sentiment_scores.append(sen.polarity_scores(tweet))\n    \n# Created a resultant list of score values\nsentiment_scores[:10]","4eed22e7":"# Compound score is the sum of positive, negative & neutral scores which is then \n# normalized between -1 and +1. For the scope of this EDA, which is the comparision \n# between positive, negative and neutral values, the compound score is not relevant.\n\ndf_sent = pd.DataFrame(sentiment_scores)\ndf_sent.drop(['compound'], axis=1, inplace=True)\n\nd = {'polarity': ['negatives', 'neutrals', 'positives'], \n     'scores': [int(df_sent['neg'].sum()), int(df_sent['neu'].sum()), int(df_sent['pos'].sum())]}\n\nscores_df = pd.DataFrame(d, columns = ['polarity', 'scores'])\nscores_df","6ef50f87":"fig = px.bar(x = scores_df['polarity'], color = scores_df['polarity'], y = scores_df.scores)\nfig.update_layout(height=500, width=700, plot_bgcolor = \"white\")\nfig.update_xaxes(title_text = 'polarity')\nfig.update_yaxes(title_text = 'Scores')\nfig.show()","7e5e835c":"class sent_analyser():\n    \"\"\"\n    Initializes the dictionary of positive\/negatives words, and the dictionary of preprocessed words\n    \"\"\"\n    \n    def __init__(self):\n        # Dictionary of normal positive\/negative words\n        self.emo_dict = {}\n        \n        # Dictionary of tokenized, stemmed and lemmatized pos\/neg words\n        self.emo_dict_tsl = {}\n        \n        self._parseCSV_PN(\"..\/input\/positivesnegatives\/PNW.csv\")\n        \n        \n    \"\"\"\n    Protected method to parse the csv file of positive\/negative words\n    \"\"\"\n    \n    def _parseCSV_PN(self, f):\n        fpd = pd.read_csv(f, encoding=encode_check(f))\n        neglst = fpd[\"negative words\"].tolist()\n        poslst = fpd[\"positive words\"].tolist()\n        \n        neglst = [str(w) for w in neglst]  # The obtained list had some non-string values, so converted\n        poslst = [str(w) for w in poslst]  # everything into a string\n        \n        negstr = [\" \".join(neglst)]  # Converted back to string for preprocessing\n        posstr = [\" \".join(poslst)] \n        \n        self.emo_dict[0] = neglst\n        self.emo_dict[1] = poslst\n        \n        neglst_tsl = lemmatizer(tok_stem(negstr))\n        poslst_tsl = lemmatizer(tok_stem(posstr))\n        \n        self.emo_dict_tsl[0] = neglst_tsl\n        self.emo_dict_tsl[1] = poslst_tsl\n        \n    \n    \"\"\"\n    Protected helper method to calculate scores and give weightage\n    \"\"\"\n    \n    def _calculate_score(self, dct):\n        scores_dict = {'neg': 0, 'neu': 0, 'pos': 0}\n        \n        if dct['neg'] == 0 and dct['pos'] > 0:\n            scores_dict['pos'] = 100\n        elif dct['neg'] > 0 and dct['pos'] == 0:\n            scores_dict['neg'] = 100\n        elif dct['neg'] == 0 and dct['pos'] == 0:\n            scores_dict['neu'] = 100\n        else:\n            n = dct['neg']\n            p = dct['pos']\n            \n            n_per = (n * 100) \/ (n + p)\n            p_per = (p * 100) \/ (n + p)\n            \n            scores_dict['neg'] = int(n_per)\n            scores_dict['pos'] = int(p_per)\n            \n        return scores_dict\n        \n    \"\"\"\n    Calls the _calculate_score method to do sentimental analysis\n    \"\"\"\n    \n    def do_sent_an(self, lst):\n        scores_lst = []\n        \n        for t in lst:\n            scores_raw = {'neg': 0, 'neu': 0, 'pos': 0}\n            tokens = word_tokenize(t)\n            for tok in tokens:\n                if tok in self.emo_dict_tsl[0][0]:\n                    scores_raw['neg'] += 1\n                elif tok in self.emo_dict_tsl[1][0]:\n                    scores_raw['pos'] += 1\n                    \n            calculated_scores = self._calculate_score(scores_raw)\n            scores_lst.append(calculated_scores)\n            \n        return scores_lst","c03a1c69":"myanalyzer = sent_analyser()\nscores = myanalyzer.do_sent_an(clean_tweets)","56f48b7d":"df_sen = pd.DataFrame(scores)\n\nd = {'polarity': ['negatives', 'neutrals', 'positives'], \n     'scores': [df_sen['neg'].mean(), df_sen['neu'].mean(), df_sen['pos'].mean()]}\n\nmyscores_df = pd.DataFrame(d, columns = ['polarity', 'scores'])\nmyscores_df","8c18d31b":"fig = px.bar(x = myscores_df['polarity'], color = myscores_df['polarity'], y = myscores_df.scores)\nfig.update_layout(height=500, width=700, plot_bgcolor = \"white\")\nfig.update_xaxes(title_text = 'polarity')\nfig.update_yaxes(title_text = 'Scores')\nfig.show()","611d738f":"## Frequency Distribution","cf895f4a":"#### A little EDA","1faa5c4f":"# Preprocessing","eb158161":"## Creating my own sentimental analysis algorithm","23b032ba":"## Tokenization & Stemming","1683dd2f":"A more efficient and accurate approach by using concordance and collocation:\n\n- Use the frequency distribution above to find out the most repeated positive\/negative words\n  by matching them up with the dataset we have. \n  \n- Use concordance to find the occurrance of a particular positive\/negative word in a tweet.\n\n- Use collocation to find consecutive words; this could be useful for checking the context for\n   a particular word and, to some extent (for example 'very nice' or 'overwhelmingly useful', \n   quantify how how positive or negative it may be (helps with weightage). Most importantly, \n   it helps in identifying negated positive words, such as 'not great'. This is probably the biggest\n   drawback of my custom analyzer because it can wrongfully identify a negative group of words with\n   positive connotation.\n\n- This technique could also be helpful in identifying named entities and their frequency to give\n   a sentence more structure and meaning.","740bc2f7":"## Working with the List dataset obtained\n\nFound three ways to work with datasets. One way is to use the tweets extracted from Bill Gates' twitter account, second is to use twitter_samples from nltk, and the third way is a third party dataset of endgame tweets from kaggle.","1d56d412":"## Lemmatization","cd8de2ba":"# Limitations of the current notebook and model\n\n- The model is not nearly as good for evaluating sarcastic statements\n- Model does not use machine learning so it's very bit primitive\n- The model does not parse emojis, something which is very helpful in the analysis\n- The custom sentimental analysis program does not take advantage of frequency distribution and concordance\/collocation however, should not be difficult to implement in the future.\n- The custom analysis program does not utilize named entity recognition.","fc0e2f69":"#### Some EDA on the obtained data","09c1e622":"## Extracting Tweets using tweepy\n\nHere we will be using multiple accounts, for instance, tweets from Bill Gates, Crestron, Elon Musk to extract tweets from.","738d87b0":"Conclusion: The tweets with negative connotations are, on an average, 25.5% more in frequency.\n\nError Margins:\n - The bias in resultant data could be due to some misinterpretations of stemmed words.\n - The dataset obtained has more negative words than positive words which could have caused the bias.\n - Neutrality was not properly calculated, as it is not relative to the positivity and negativity of tweets","2cfbe504":"Conclusion: As you can see, the amount of neutral tweets is overwhelmingly high as compared to the negative and positive tweets, making this dataset neutral as a whole.","aa724704":"## Extracting tweets from Twitter\n\n### This method takes pre-defined ids from twitter and fetches them from the API in list format.","f2214a3f":"#### Some analyzation on word count before and after the preprocessing","c9a4391f":"# Sentiment Analysis\n\n## Using an already trained model from nltk\nHere we're going to first see nltk's own sentiment analysis too, VADER for testing purposes and then make our own algorithm."}}