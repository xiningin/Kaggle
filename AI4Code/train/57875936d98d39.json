{"cell_type":{"b0970696":"code","5fc3e10e":"code","3577fd3f":"code","2fe0b9b0":"code","ee4ff3c0":"code","f5c23b07":"code","a500a68c":"code","364ccca4":"code","d7699b05":"code","0dccae90":"code","42b47988":"code","59cab893":"code","ae93aad8":"code","cde0cdce":"code","f6402ae8":"markdown","5b97ee40":"markdown","29de149d":"markdown","cd116029":"markdown","d1fc311e":"markdown","1b215522":"markdown","739d921c":"markdown","759c8a55":"markdown","dd7134d9":"markdown","005478de":"markdown","01f8d393":"markdown","a408ce18":"markdown","5eddfab8":"markdown","3da5644c":"markdown","01688718":"markdown","a1087281":"markdown"},"source":{"b0970696":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\npd.set_option('chained_assignment',None)\n\nseed=47\n\ntrain = pd.read_csv('\/kaggle\/input\/prohack-hackathon\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/prohack-hackathon\/test.csv')\nsample_submit = pd.read_csv('\/kaggle\/input\/prohack-hackathon\/sample_submit.csv')\n\ntotal_df_fillna = pd.concat([train, test], ignore_index=True)\n\nprint(train.shape, test.shape)","5fc3e10e":"total_df_fillna","3577fd3f":"total_df_fillna.describe()","2fe0b9b0":"print('Unique galaxies:', total_df_fillna['galaxy'].nunique())\nprint('Number of years:', total_df_fillna['galactic year'].nunique())\n\ntotal_df_fillna.drop(['galactic year','galaxy','y'],axis=1, inplace=True)","ee4ff3c0":"import missingno as msno \nmsno.matrix(total_df_fillna[list(total_df_fillna.columns)[:]])\nprint('Train missing values:', train.isna().sum().sum())\nprint('Test missing values:', test.isna().sum().sum())","f5c23b07":"import pickle\nfrom sklearn.linear_model import ElasticNet\n\ntest_na = test.dropna()\ntrain_na = train.dropna()\ntotal_df = pd.concat([train_na, test_na], ignore_index=True)\ntotal_df.drop(['galactic year','galaxy','y'],axis=1, inplace=True)\n\ncorrMatrix = total_df.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(corrMatrix, annot=False)\nplt.show()\n\nkoef_corr = 0.65\nkoef_na = 0.8\ntop_features = 5\nnum_model = 1\nlist_target_features = []\nlist_corr_features = []\n\nfor big_col in list(total_df.columns):\n    list_correlated=[]\n    correlations=[]\n    na_rows=[]\n    na_big_col= total_df_fillna[big_col].isna().sum()\n\n    for col in list(total_df.columns):\n        cor = total_df[big_col].corr(total_df[col])\n\n        if np.absolute(cor)>koef_corr and np.absolute(cor)<0.99:\n            list_correlated.append(col)\n            correlations.append(np.absolute(cor))\n            na_rows.append(total_df_fillna[col].isna().sum())\n            df = pd.DataFrame(list(zip(list_correlated, correlations,na_rows)),columns =['features', 'correlation', 'na']) \n\n    df = df[df.na<koef_na*na_big_col].sort_values(by='correlation', ascending=False)\n    list_correlated = list(df[:top_features].features)\n\n    if len(list_correlated)>0:\n        list_target_features.append(big_col)\n        list_corr_features.append(list_correlated)\n        model = ElasticNet(random_state = seed)     \n        model.fit(total_df[list_correlated], total_df[big_col])\n        pkl_filename = \"pickle_model_%i.pkl\"%len(list_target_features)\n        with open(pkl_filename, 'wb') as file:\n            pickle.dump(model, file)\n        num_model = num_model + 1","a500a68c":"for i in range(len(list_target_features)):\n    target = list_target_features[i]\n    corr_feat = list_corr_features[i]\n\n    with open(\"pickle_model_%i.pkl\"%(i+1), 'rb') as file:\n        pickle_model = pickle.load(file)\n\n    for i in range(len(total_df_fillna.index)):\n        time_corr_features = []\n        \n        for j in range(len(corr_feat)):\n            if len(total_df_fillna[corr_feat[j]][i].shape) == 1:\n                feat = total_df_fillna[corr_feat[j]][i].values[0]\n            else:\n                feat = total_df_fillna[corr_feat[j]][i]\n            time_corr_features.append(feat)\n\n        if np.isnan(time_corr_features).sum() == 0:\n            if np.isnan(total_df_fillna[target][i]) == True:\n                pred = pickle_model.predict(np.asarray(time_corr_features).reshape(1,-1))\n                total_df_fillna[target][i] = pred\n\nprint(total_df_fillna.isna().sum().sum())","364ccca4":"new_train = total_df_fillna[:len(train.index)].reset_index()\nnew_test = total_df_fillna[len(train.index):].reset_index()\n\nnew_train = new_train.join(train[['galactic year','galaxy','y']])\nnew_test = new_test.join(test[['galactic year','galaxy']])\n\ndf = pd.concat([new_train, new_test], ignore_index=True)\n\ndf_sort = df.sort_values(by=['galaxy','galactic year'], ascending=True, ignore_index=True)\ntitle_galaxy = list(df['galaxy'].unique())\n\nnum_col = df.iloc[:,:-3].columns\n\ntotal_df = pd.DataFrame()\nfor name in title_galaxy:\n    temp = df[df['galaxy']==name]\n    for column in num_col:\n        if temp[column].isna().sum()+4<=len(temp):\n            temp[column] = temp[column].interpolate(method='linear', limit_direction ='backward')\n        if temp[column].isna().sum()<len(temp):\n            m = temp[column].mean()\n            temp[column]=temp[column].fillna(m)\n    total_df = pd.concat([total_df, temp], ignore_index=True)\n    \nprint(total_df.isna().sum().sum())","d7699b05":"for col in total_df.iloc[:,:-3].columns.values:\n    mean = total_df[col].mean()\n    total_df[col] = total_df[col].fillna(mean)","0dccae90":"train = total_df[:len(train.index)].reset_index()\ntest = total_df[len(train.index):].reset_index()\n\ntrain = train.drop(['level_0', 'index'], axis=1)\ntest = test.drop(['y','index', 'level_0'], axis=1)","42b47988":"total_df['galaxy'].unique()","59cab893":"import category_encoders as ce\ncat_cols=['galaxy']\ntarget_enc = ce.CatBoostEncoder(cols=cat_cols)\ntarget_enc.fit(train[cat_cols], train['y'])\ntrain = train.join(target_enc.transform(train[cat_cols]).add_suffix('_cb'))\n\ntest = test.join(target_enc.transform(test[cat_cols]).add_suffix('_cb'))\ntrain[['galaxy', 'galaxy_cb']]","ae93aad8":"sns.distplot(train['Gross income per capita'], label='train')\nsns.distplot(test['Gross income per capita'], label='test')\nplt.title('Train & Test Distribution')\nplt.legend()\nplt.show()","cde0cdce":"sns.distplot(train['y'], label='train')\nplt.title('Target Distribution')","f6402ae8":"*Perhaps we underestimated the importance of working with galaxies. We tried to group by galaxy names (highlight the first word in the name, it is the key), but the result has become worse.*","5b97ee40":"We decided to use **ElasticNet** to fill in the missing data. \n\n**The algorithm is as follows:**\n1. Delete the missing data and build the correlation matrix\n2. We fix each feature (this will be our target in this case) and select high correlated feature. We will use them for training model and predicting target.\n3. We run through all the features","29de149d":"# Train and Test distribution\n\nAfter we filled in the missing data, we need to **check the distribution of the training and test dataset**. Perhaps there will be discrepancies. (It turned out too many graphs, because we will show an example of one function).","cd116029":"The first part is over! Continued [here](https:\/\/www.kaggle.com\/imgremlin\/prohack-part-2-fe-modeling-optimization)","d1fc311e":"# Data Exploration\n\nSo let's turn to the code","1b215522":"<img src=\"https:\/\/kenya.ai\/wp-content\/uploads\/2020\/05\/Mckinsey-prohack.png\" width=\"1000px\"> \n\n\n","739d921c":"On many charts, you can see quite long right tails. We will talk about this in the second part. Also take a look at the distribution of the target and see a similar picture. This is bad for the model learning phase. We will try to avoid these problems in the second part.","759c8a55":"Great job! The remaining values are filled with the **mean**. You can also try re-doing model training or try to find formulas for calculating these functions. So if you don't want to overfit, we recommend that you do an mean for the all set, and not just for training data.","dd7134d9":"*It was also not possible to try to fill in the missing values directly using the formulas of various indices, but many functions could not be calculated using ready-made formulas (for example, the number of women in the Senate). Moreover, if we substitute the known data into the formulas, then they did not always coincide.*","005478de":"**In total**:\n* we have 79 features (in the second part we will select features)\n* many unique galaxies\n* and 27 years during which statistics was kept\n* different scales of features. It will be necessary to use scalers\n* our data contains many outliers","01f8d393":"Since there are a lot of galaxies, using get_dummies is rather irrational, as there will be a significant increase in dimension. To convert categorical functions, we used the **CatBoost Encoder**.\n\n> Catboost is a recently created target-based categorical encoder. It is intended to overcome target leakage problems inherent in LOO. In order to do that, the authors of Catboost introduced the idea of \u201ctime\u201d: the order of observations in the dataset. Clearly, the values of the target statistic for each example rely only on the observed history. To calculate the statistic for observation j in train dataset, we may use only observations, which are collected before observation j, i.e. i\u2264j:\n\n![image.png](attachment:image.png)\n\n> To prevent overfitting, the process of target encoding for train dataset is repeated several times on shuffled versions of the dataset and results are averaged.","a408ce18":"Fine! The amount of missing data has decreased **by more than 3 times**. To be sure, you need to check the updated data for cross-validation, but now let's miss it. For the remaining missing values, we used **linear interpolation**. In this situation, due to the fact that the data are given at certain intervals, it best illustrates the dynamics of the features.","5eddfab8":"The first problem we are facing: **there is a lot of missing data**. In the initial stages of the solution, perhaps, many teams tried to use **interpolation**. We also started with her, but she did not give a good result.\n\nMoreover, there are many countries in which entire columns of data are missing *(example below)*, so interpolation there will not help exactly.\n\n![image.png](attachment:image.png)","3da5644c":"# Missing values","01688718":"# Encoding galaxy names\n\nAn underrated step is the encoding of categorical features. There are many unique names for galaxies in the dataset, we need to do something with them.","a1087281":"# International Data Science Hackathon by McKinsey & Company\n\n\n\n**by team GORNYAKI (Tsepa Oleksii and Samoshin Andriy [Ukraine, KPI, IASA])**\n\nThanks to the organizers for this hackathon and everyone for participating! In this notebook we will go through the first part of our team solution:\n\n* **data exploration**\n* **missing values**\n* **categorical features**\n* **and other issues**\n\nFor those who hear about this competition for the first time, you can familiarize yourself with the condition [here](https:\/\/prohack.org\/). All data is located [here](https:\/\/www.kaggle.com\/mrmorj\/prohack-hackathon).\n\n[The second part, where we talk about our models and the optimization task](https:\/\/www.kaggle.com\/imgremlin\/prohack-part-2-fe-modeling-optimization)"}}