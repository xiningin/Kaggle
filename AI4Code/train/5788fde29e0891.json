{"cell_type":{"f806e2b1":"code","96d6b8fe":"code","a519a29d":"code","c6c4e247":"code","915430b4":"code","0f265f50":"code","b96573d5":"code","b96af54b":"code","4b027b61":"code","bfd0c601":"code","b93f4e84":"code","ae75b2e3":"code","5f4a77ca":"code","0e20fd15":"code","d138d720":"code","190d3001":"code","7548552e":"code","6d05eb48":"code","871a2635":"code","990c8cff":"code","ff712476":"code","d44f5d23":"code","50a427fc":"code","c06fcffe":"code","0c5230c5":"code","128f7943":"code","c79f5506":"code","38938a37":"code","881061cd":"code","5508204e":"code","87ddd5f2":"code","99b7ac10":"markdown","529c7c06":"markdown","a3895291":"markdown","420d6534":"markdown","0594ecdd":"markdown","8d3dd34a":"markdown","b0cb8b6e":"markdown","ef4f05c2":"markdown","933a28bf":"markdown","c18d6ce7":"markdown","a9e430bc":"markdown","f7c1e136":"markdown","87dc8f80":"markdown","c4230849":"markdown","d8e92f4b":"markdown","e39e99e8":"markdown","05b1c4d1":"markdown","e6862476":"markdown","26cdca01":"markdown","0e9afcc7":"markdown","af0f1392":"markdown","99b08e97":"markdown","a08bd3fc":"markdown","f15882e5":"markdown","85b8ab9d":"markdown","8e338bbc":"markdown","d8d2ebdd":"markdown","a92bfbaa":"markdown","9ddb3c3c":"markdown","03626268":"markdown","013e1a94":"markdown","fe518b43":"markdown","2822bc85":"markdown"},"source":{"f806e2b1":"# data manipulation\nimport pandas as pd\nimport numpy as np\n\n# sklearn helper functions\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit, cross_validate\n\n# Dimensionality Reduction algorithms\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.manifold import LocallyLinearEmbedding, MDS, Isomap, TSNE\nfrom sklearn.random_projection import johnson_lindenstrauss_min_dim, GaussianRandomProjection, SparseRandomProjection\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# sklearn ML algorithms\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\n\n# Viz\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\n\n# Hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","96d6b8fe":"# read train set file\ndf_train_raw = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n\n# check dimensions of train set\nprint(\"Train Set has\", df_train_raw.shape[0], \"rows and\", df_train_raw.shape[1], \"columns.\")","a519a29d":"# splitting method which keeps 20% data for validation set\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in sss.split(df_train_raw, df_train_raw['label']):\n    strat_train_set = df_train_raw.loc[train_index]\n    df_validation = df_train_raw.loc[test_index]\n    \n# creating a copy of target labels and removing them from validation set\nvalidation_labels = df_validation['label'].copy()\ndf_validation.drop(columns=['label'], inplace=True)","c6c4e247":"scaler = StandardScaler()    # Standardize features by removing the mean and scaling to unit variance\ndf_validation = scaler.fit_transform(df_validation)","915430b4":"# custom cross-validation for comparing multiple estimators\ndef cross_validation(data, target_labels, mla_list, split_method):\n    \n    MLA_columns = ['MLA Name', 'MLA Parameters', 'Train Accuracy Mean', 'Test Accuracy Mean', 'Test Accuracy 3*STD', 'Training Time']\n    MLA_compare = pd.DataFrame(columns = MLA_columns)\n\n    row_index = 0\n    for alg in mla_list:\n\n        # set name and parameters\n        MLA_name = alg.__class__.__name__\n        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n\n        # cross validation\n        cv_results = cross_validate(alg, data, target_labels, cv=split_method, scoring=['accuracy'], return_train_score=True)\n\n        MLA_compare.loc[row_index, 'Training Time'] = cv_results['fit_time'].sum()\n        MLA_compare.loc[row_index, 'Train Accuracy Mean'] = cv_results['train_accuracy'].mean()\n        MLA_compare.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_accuracy'].mean()\n        MLA_compare.loc[row_index, 'Test Accuracy 3*STD'] = cv_results['test_accuracy'].std()*3\n\n        row_index+=1\n\n    # print and sort table\n    MLA_compare.sort_values(by = ['Test Accuracy Mean'], ascending = False, inplace = True)\n    \n    return MLA_compare","0f265f50":"# list of classifiers we want to test\nmla_list = [\n    RandomForestClassifier(n_jobs=-1),\n    \n    SGDClassifier(early_stopping=True, n_iter_no_change=5, n_jobs=-1),\n    \n    LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10, n_jobs=-1),    # Softmax Regression\n]\n\n# splitting method for use in 'cross_validate'\ncv_split_method = ShuffleSplit(n_splits=10, test_size=0.3, train_size=0.6, random_state=0)","b96573d5":"cross_validation(data=df_validation, target_labels=validation_labels, mla_list=mla_list, split_method=cv_split_method)","b96af54b":"n_components = 100","4b027b61":"pca = PCA(n_components=n_components)\n\ndf_validation_pca = df_validation.copy()\ndf_validation_pca = pca.fit_transform(df_validation_pca)","bfd0c601":"cross_validation(data=df_validation_pca, target_labels=validation_labels, mla_list=mla_list, split_method=cv_split_method)","b93f4e84":"kpca = KernelPCA(n_components=n_components, kernel='rbf')\n\ndf_validation_kpca = df_validation.copy()\ndf_validation_kpca = kpca.fit_transform(df_validation_kpca)","ae75b2e3":"cross_validation(data=df_validation_kpca, target_labels=validation_labels, mla_list=mla_list, split_method=cv_split_method)","5f4a77ca":"# multiple distortion values will give multiple components value\njohnson_lindenstrauss_min_dim(df_validation.shape[0], eps=[0.5, 0.6, 0.7, 0.8, 0.9])","0e20fd15":"grp = GaussianRandomProjection(n_components='auto', eps=0.9, random_state=0)\n\ndf_validation_grp = df_validation.copy()\ndf_validation_grp = grp.fit_transform(df_validation_grp)","d138d720":"cross_validation(data=df_validation_grp, target_labels=validation_labels, mla_list=mla_list, split_method=cv_split_method)","190d3001":"srp = SparseRandomProjection(n_components='auto', eps=0.9, random_state=0)\n\ndf_validation_srp = df_validation.copy()\ndf_validation_srp = srp.fit_transform(df_validation_srp)","7548552e":"cross_validation(data=df_validation_srp, target_labels=validation_labels, mla_list=mla_list, split_method=cv_split_method)","6d05eb48":"lle = LocallyLinearEmbedding(n_components=n_components, n_neighbors=10)\n\ndf_validation_lle = df_validation.copy()\ndf_validation_lle = lle.fit_transform(df_validation_lle)","871a2635":"cross_validation(data=df_validation_lle, target_labels=validation_labels, mla_list=mla_list, split_method=cv_split_method)","990c8cff":"iso = Isomap(n_components=n_components, n_neighbors=10, n_jobs=-1)\n\ndf_validation_iso = df_validation.copy()\ndf_validation_iso = iso.fit_transform(df_validation_iso)","ff712476":"cross_validation(data=df_validation_iso, target_labels=validation_labels, mla_list=mla_list, split_method=cv_split_method)","d44f5d23":"tsne = TSNE(n_components=2, random_state=0, n_jobs=-1)\n\ndf_validation_tsne = df_validation.copy()\ndf_validation_tsne = tsne.fit_transform(df_validation_tsne)","50a427fc":"cross_validation(data=df_validation_tsne, target_labels=validation_labels, mla_list=mla_list, split_method=cv_split_method)","c06fcffe":"lda = LinearDiscriminantAnalysis(n_components=9)\n\ndf_validation_lda = df_validation.copy()\ndf_validation_lda = lda.fit_transform(df_validation_lda, validation_labels)","0c5230c5":"cross_validation(data=df_validation_lda, target_labels=validation_labels, mla_list=mla_list, split_method=cv_split_method)","128f7943":"# using only 2 components\npca = PCA(n_components=2)\nkpca = KernelPCA(n_components=2, kernel='rbf')\ngrp = GaussianRandomProjection(n_components=2, random_state=0)\nsrp = SparseRandomProjection(n_components=2, random_state=0)\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\nmds = MDS(n_components=2, max_iter=2, random_state=0)\niso = Isomap(n_components=2, n_neighbors=10)\nlda = LinearDiscriminantAnalysis(n_components=2)\n\n# transform data for vizualization\ndf_reduced_pca = df_validation.copy()\ndf_reduced_pca = pca.fit_transform(df_reduced_pca)\n\ndf_reduced_kpca = df_validation.copy()\ndf_reduced_kpca = kpca.fit_transform(df_reduced_kpca)\n\ndf_reduced_grp = df_validation.copy()\ndf_reduced_grp = grp.fit_transform(df_reduced_grp)\n\ndf_reduced_srp = df_validation.copy()\ndf_reduced_srp = srp.fit_transform(df_reduced_srp)\n\ndf_reduced_lle = df_validation.copy()\ndf_reduced_lle = lle.fit_transform(df_reduced_lle)\n\ndf_reduced_mds = df_validation.copy()\ndf_reduced_mds = mds.fit_transform(df_reduced_mds)\n\ndf_reduced_iso = df_validation.copy()\ndf_reduced_iso = iso.fit_transform(df_reduced_iso)\n\ndf_reduced_tsne = df_validation_tsne\n\ndf_reduced_lda = df_validation.copy()\ndf_reduced_lda = lda.fit_transform(df_reduced_lda, validation_labels)","c79f5506":"titles = [\"PCA\", \"KernelPCA\", \"Gaussian Random Projection\", \"Sparse Random Projection\", \"LLE\", \"MDS\", \"Isomap\", \"t-SNE\", \"LDA\"]\n\nfig, axs = plt.subplots(3, 3, figsize=(20,20))\n\naxs = axs.ravel()\n\nfor subplot, title, df_reduced in zip(axs, titles, (df_reduced_pca, df_reduced_kpca, df_reduced_grp, df_reduced_srp, df_reduced_lle, df_reduced_mds, df_reduced_iso, df_reduced_tsne, df_reduced_lda)):\n    plt.subplot(subplot)\n    plt.title(title, fontsize=14)\n    plt.scatter(df_reduced[:, 0], df_reduced[:, 1], c=validation_labels, cmap=plt.cm.hot)\n    plt.xlabel(\"$z_1$\", fontsize=18)\n    if subplot == 101:\n        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n    plt.grid(True)\n\nplt.show()","38938a37":"plt.figure(figsize=(13,10))\nplt.scatter(df_reduced_tsne[:, 0], df_reduced_tsne[:, 1], c=validation_labels, cmap=\"jet\")\nplt.axis('off')\nplt.colorbar()\nplt.show()","881061cd":"def plot_digits(X, y, min_distance=0.05, images=None, figsize=(13, 10)):\n    \n    # scale the input features so that they range from 0 to 1\n    X_normalized = MinMaxScaler().fit_transform(X)\n    \n    # Now we create the list of coordinates of the digits plotted so far.\n    # We pretend that one is already plotted far away at the start, to\n    # avoid `if` statements in the loop below\n    neighbors = np.array([[10., 10.]])\n    \n    plt.figure(figsize=figsize)\n    cmap = mpl.cm.get_cmap(\"jet\")\n    digits = np.unique(y)\n    \n    for digit in digits:\n        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1], c=[cmap(digit \/ 9)])\n    \n    plt.axis(\"off\")\n    ax = plt.gcf().gca()  # get current axes in current figure\n    \n    for index, image_coord in enumerate(X_normalized):\n        closest_distance = np.linalg.norm(np.array(neighbors) - image_coord, axis=1).min()\n        if closest_distance > min_distance:\n            neighbors = np.r_[neighbors, [image_coord]]\n            if images is None:\n                plt.text(image_coord[0], image_coord[1], str(int(y[index])),\n                         color=cmap(y[index] \/ 9), fontdict={\"weight\": \"bold\", \"size\": 16})\n            else:\n                image = images[index].reshape(28, 28)\n                imagebox = AnnotationBbox(OffsetImage(image, cmap=\"binary\"), image_coord)\n                ax.add_artist(imagebox)","5508204e":"plot_digits(df_reduced_tsne, validation_labels, images=df_validation, figsize=(35,25))","87ddd5f2":"from sklearn.datasets import make_swiss_roll\nX, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n\naxes = [-11.5, 14, -2, 23, -12, 15]\n\nx2s = np.linspace(axes[2], axes[3], 10)\nx3s = np.linspace(axes[4], axes[5], 10)\nx2, x3 = np.meshgrid(x2s, x3s)\n\nfig = plt.figure(figsize=(6, 5))\nax = plt.subplot(111, projection='3d')\n\npositive_class = X[:, 0] > 5\nX_pos = X[positive_class]\nX_neg = X[~positive_class]\nax.view_init(10, -70)\nax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], \"y^\")\nax.plot_wireframe(5, x2, x3, alpha=0.5)\nax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], \"gs\")\nax.set_xlabel(\"$x_1$\", fontsize=18)\nax.set_ylabel(\"$x_2$\", fontsize=18)\nax.set_zlabel(\"$x_3$\", fontsize=18)\nax.set_xlim(axes[0:2])\nax.set_ylim(axes[2:4])\nax.set_zlim(axes[4:6])\n\nplt.show()\n\nfig = plt.figure(figsize=(5, 4))\nax = plt.subplot(111)\n\nplt.plot(t[positive_class], X[positive_class, 1], \"gs\")\nplt.plot(t[~positive_class], X[~positive_class, 1], \"y^\")\nplt.axis([4, 15, axes[2], axes[3]])\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\nplt.grid(True)\n\nplt.show()\n\nfig = plt.figure(figsize=(6, 5))\nax = plt.subplot(111, projection='3d')\n\npositive_class = 2 * (t[:] - 4) > X[:, 1]\nX_pos = X[positive_class]\nX_neg = X[~positive_class]\nax.view_init(10, -70)\nax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], \"y^\")\nax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], \"gs\")\nax.set_xlabel(\"$x_1$\", fontsize=18)\nax.set_ylabel(\"$x_2$\", fontsize=18)\nax.set_zlabel(\"$x_3$\", fontsize=18)\nax.set_xlim(axes[0:2])\nax.set_ylim(axes[2:4])\nax.set_zlim(axes[4:6])\n\nplt.show()\n\nfig = plt.figure(figsize=(5, 4))\nax = plt.subplot(111)\n\nplt.plot(t[positive_class], X[positive_class, 1], \"gs\")\nplt.plot(t[~positive_class], X[~positive_class, 1], \"y^\")\nplt.plot([4, 15], [0, 22], \"b-\", linewidth=2)\nplt.axis([4, 15, axes[2], axes[3]])\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\nplt.grid(True)\n\nplt.show()","99b7ac10":"In short, reducing the dimensionality of your training set before training a\nmodel will usually speed up training, but it may not always lead to a better\nor simpler solution; it all depends on the dataset.\n","529c7c06":"* In the last 2 images the Swiss roll is split into two classes: in the 3D space the decision boundary would be fairly complex, but in the 2D unrolled manifold space, the decision boundary is a straight line.\n\n* In the first 2 images the decision boundary is located at x = 5. This decision boundary looks very simple in the original 3D space (a vertical plane), but it looks more complex in the unrolled manifold (a collection of four independent line segments)","a3895291":"### KernelPCA\n\nNon-linear dimensionality reduction through the use of kernels.\n\nReference: [scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.KernelPCA](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.KernelPCA.html?highlight=kernelpca)","420d6534":"Now, lets see how the different techniques transform data into 2D form.","0594ecdd":"### LDA (Linear Discriminant Analysis)\n\nIs a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data.\n\nReference: [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/)","8d3dd34a":"*The Johnson-Lindenstrauss lemma*\n\nFind a \u2018safe\u2019 number of components to randomly project to. It just needs maximum distortion rate aka *'eps'* value and then it will calculate safe number of components to randomly project to. \n\nReference: [https:\/\/scikit-learn.org\/stable\/modules\/random_projection.html](https:\/\/scikit-learn.org\/stable\/modules\/random_projection.html)","b0cb8b6e":"*Sparse Random Projection*","ef4f05c2":"### LLE (Locally Linear Embedding)\n\nSeeks a lower-dimensional projection of the data which preserves distances within local neighborhoods.\n\nReference: [scikit-learn.org\/stable\/modules\/manifold.html](https:\/\/scikit-learn.org\/stable\/modules\/manifold.html?highlight=lle#locally-linear-embedding)","933a28bf":"# Manifold Learning","c18d6ce7":"<font size=5>There are 2 main approaches to reducing dimensionality -<\/font>\n<font size=3> \n* Projection\n* Manifold Learning\n<\/font>","a9e430bc":"# Data Preparation","f7c1e136":"This plot tells us which numbers are easily distinguishable from the others (e.g., 0s, 6s, and most 8s are rather well separated clusters), and it also tells us which numbers are often hard to distinguish (e.g., 4s and 9s, 5s and 3s, and so on).","87dc8f80":"# Dimensionality Reduction For Vizualization","c4230849":"### PCA (Principal Component Analysis)\n\nLinear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.\n\nReference: [scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA)","d8e92f4b":"*Gaussian Random Projection*","e39e99e8":"While the training times has decreased for LogisticRegression and SGDClassifier, RandomForestClassifier training time has increased. Dimensionality reduction does not always lead to faster training time: it depends on the dataset, the model and the training algorithm.\n\nLogistic Regression sees a huge decrease in training time while improving its test accuracy.\n\nJust for reference, I get 93% accuracy in the leaderboards with PCA applied and 97% without applying any reduction. So, obviously there are trade-offs with using Dimensionality reduction techniques.","05b1c4d1":"Both the Gaussian and Sparse Random Projection methods uses The Johnson-Lindenstrauss lemma to automatically calculate the number of components.","e6862476":"Using KernelPCA gives another improvement in LogisticRegression scores while greatly improving training times.","26cdca01":"# Projection","0e9afcc7":"# Introduction\n\nIn this notebook we will anlayze Effects of Several Dimensionality Reduction techniques on the MNIST Handwritten Digits dataset. We will compare classification accuracy scores and check which methods work better for this type of dataset.","af0f1392":"Surprisingly, with t-SNE, we are able to get very decent scores even though only 2 components are used.","99b08e97":"### t-SNE (t-Distributed Stochastic Neighbor Embedding)\n\nReduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space.\n\nReference: [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/)","a08bd3fc":"We need to scale the data first.","f15882e5":"Lets create a function that will plot Digit numbers or scaled down Digit images(if provided) with the instances of those digits.\n<br>\nCredit: [https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/08_dimensionality_reduction.ipynb](https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/08_dimensionality_reduction.ipynb)","85b8ab9d":"### Isomap (Isometric Mapping)\n\nSeeks a lower-dimensional embedding which maintains geodesic distances between all points.\n\nReference: [scikit-learn.org\/stable\/modules\/manifold.html](https:\/\/scikit-learn.org\/stable\/modules\/manifold.html?highlight=lle#isomap)","8e338bbc":"Again, we are able to get above 90% accuracy by only using 9 components.","d8d2ebdd":"We need to first create a validation set so that cross-validation can be done quickly.","a92bfbaa":"Some techniques are very good at seperating the individual classes like *t-SNE* while others are overlapping severly.\nLets view the *t-SNE* plot while plotting each class with different colors.","9ddb3c3c":"# Without any reduction\nBefore we start comparing the dimensionality reduction techniques, lets check the accuracy score and the training times on the vanilla dataset.","03626268":"# Why Dimensionality Reduction sometimes increases training times?\n\nFinally we will see why RandomForestClassier saw increased training times after applying dimensionality reduction techniques. To demonstrate this we will use Swiss Roll dataset. This example is inspired from the book [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/)","013e1a94":"### Random Projections\n\nProjects the data to a lower-dimensional space using a random linear projection. The quality of the dimensionality reduction depends on the number of instances and the target dimensionality, but surprisingly not on the initial dimensionality.\n\nReference: [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/)","fe518b43":"For the sake of comparison, lets keep number of components constant across different techniques.","2822bc85":"These are the scores and timings for the Data without any reduction."}}