{"cell_type":{"c6ef7df3":"code","98e6e9b0":"code","b85c3636":"code","6cad684a":"code","fb45061e":"code","a8b30dab":"code","627118fd":"code","befb4c57":"code","f9616229":"code","969ea1fb":"code","fcee1e53":"code","cf293f39":"code","2195498e":"code","89e57b1e":"code","fc401b5e":"code","7c50b8f0":"code","90d4e75a":"code","9e013100":"markdown","5d3ecd82":"markdown","8c3fcd8a":"markdown","dbb28e88":"markdown","566077c7":"markdown","e1b181a2":"markdown","facf281e":"markdown","9a92a4a0":"markdown","e3f8e16f":"markdown","1b8bbd25":"markdown","d4a13b60":"markdown","bd9aebd7":"markdown","a0fd7dba":"markdown","6dc4c015":"markdown","07c87bf8":"markdown"},"source":{"c6ef7df3":"import pandas as pd\nimport numpy as np","98e6e9b0":"def transform_dataset(metadata_path, weather_path, energy_path, site_id):\n    \"\"\"\n    metadata_path: path to metadata data set\n    weather_path: path to weather data set\n    energy_path: path to energy data set\n    site_id: selected site id\n    \"\"\"\n    # Load data\n    metadata = pd.read_csv(metadata_path)\n    weather = pd.read_csv(weather_path, parse_dates=[\"timestamp\"])\n    energy = pd.read_csv(energy_path, parse_dates=[\"timestamp\"])\n    \n    # Filter\n    metadata = metadata.loc[metadata.site_id == site_id, [\"building_id\", \"site_id\", \"primaryspaceusage\", \"sqm\"]]\n    weather = weather.loc[weather.site_id == site_id]\n    cols = [\"timestamp\"] + [col for col in energy.columns if site_id in col]\n    energy = energy[cols]\n    \n    # Melt\n    energy = energy.melt(id_vars=\"timestamp\", var_name=\"building_id\", value_name=\"meter_reading\")\n    \n    # Merge\n    energy = pd.merge(energy, metadata, how=\"left\", on=\"building_id\")\n    energy = pd.merge(energy, weather, how=\"left\", on=[\"timestamp\",\"site_id\"])\n    \n    return energy","b85c3636":"def encode_categorical(df):\n    from sklearn.preprocessing import OrdinalEncoder\n    from sklearn.preprocessing import OneHotEncoder\n    \n    ordinal_cols = [\"building_id\",\"site_id\"]\n    cols = [\"primaryspaceusage\"] # only one here, but you could add more\n    \n    # Ordinal encoder\n    # Define the encoder\n    encoder = OrdinalEncoder()\n\n    # This returns an array\n    encoded_columns = encoder.fit_transform(df[ordinal_cols])\n\n    # We can convert it to a dataframe\n    encoded_columns = pd.DataFrame(encoded_columns)\n\n    # And we asign the column names (the encoder remove them)\n    encoded_columns.columns = ordinal_cols\n\n    # And replace it in your data\n    df[ordinal_cols] = encoded_columns\n    \n    # One Hot Encoder\n    for col in cols:\n        # Get columns with less than 10% of data\n        s = (df[col].value_counts() < len(df)*0.1)\n        #\u00a0Convert to list\n        categories_under_10pct = list(s[s].index)\n        #\u00a0Replace for category \"Other\"\n        df = df.replace(categories_under_10pct, \"Other\")\n        \n    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    \n    # Transform\n    OH_cols = pd.DataFrame(OH_encoder.fit_transform(df[cols]))\n    \n    # Rename\n    OH_cols.columns = [\"primary_use_0\", \"primary_use_1\", \"primary_use_2\", \"primary_use_3\"]\n    \n    # Remove categorical columns (will be replaced with one-hot encoding)\n    df = df.drop(cols, axis=1)\n\n    # Add one-hot encoded columns to numerical features\n    df = pd.concat([df, OH_cols], axis=1)\n    \n    return df","6cad684a":"def handle_missings(df):\n    \"\"\"\n    Handle missing values in dataframe.\n    \"\"\"\n    # Columns to drop\n    to_drop = [\"cloudCoverage\", \"precipDepth6HR\"]\n\n    # Drop columns\n    df.drop(to_drop, axis=1, inplace=True)\n\n    from sklearn.impute import SimpleImputer\n\n    # Columns to input\n    to_input = [\"airTemperature\", \"dewTemperature\", \"precipDepth1HR\", \"seaLvlPressure\", \"windDirection\"]\n\n    # Imputation\n    my_imputer = SimpleImputer()\n    input_df = pd.DataFrame(my_imputer.fit_transform(df[to_input]))\n\n    # Imputation removed column names; put them back\n    input_df.columns = to_input\n\n    # And now we replace the inputed column in the original data set\n    df[to_input] = input_df\n\n    # Drop missing in meter_reading\n    df.dropna(inplace=True)\n    \n    return df","fb45061e":"metadata_path = \"\/kaggle\/input\/buildingdatagenomeproject2\/metadata.csv\"\nweather_path = \"\/kaggle\/input\/buildingdatagenomeproject2\/weather.csv\"\nenergy_path = \"\/kaggle\/input\/buildingdatagenomeproject2\/electricity.csv\"\nsite_id = \"Panther\"\n\ndf = transform_dataset(metadata_path, weather_path, energy_path, site_id)\ndf = encode_categorical(df)\ndf = handle_missings(df)\ndf.head()","a8b30dab":"# Months, from 1 to 12\ndf[\"month\"] = df.timestamp.dt.month\n\n# Day, from 1 to 31\ndf[\"day\"] = df.timestamp.dt.day\n\n# Week day, from 0 (monday) to 6 (sunday)\ndf[\"weekday\"] = df.timestamp.dt.weekday\n\n# Hour\ndf[\"hour\"] = df.timestamp.dt.hour\n\n# check it out\ndf.head()","627118fd":"# 5 and 6 are saturday and sunday\ndf[\"is_weekend\"] = [weekday in [5,6] for weekday in df.weekday]\n\n# Check it out\ndf.head()","befb4c57":"df[\"RH\"] =  100 - 5*(df.airTemperature - df.dewTemperature)","f9616229":"# Group by month and aggregate by the median (returns a series)\nmedian_temp = df.groupby(\"month\")[\"airTemperature\"].median()\n\n# Convert it to a dataframe\nmedian_temp = pd.DataFrame(median_temp)\n\n# And rename the column (keeps the aggregation column name, airTemperature)\nmedian_temp.rename(columns={\"airTemperature\": \"month_median_temp\"}, inplace=True)\n\n# Check it out\nmedian_temp","969ea1fb":"# Left merge on month\ndf = pd.merge(df, median_temp, how=\"left\", on=\"month\")\n\n# Check it out\ndf.head()","fcee1e53":"# 1. Select a time window\nwindow=3\n\n# 2. Select the columns\ncols = [\"airTemperature\"]\n\n# 3. First, create the grouby object\ngroup_df = df.groupby('site_id')\n\n# 4. Create the rolling window object\nrolled = group_df[cols].rolling(window=window, min_periods=0)\n\n# 5. Calculate the summary metrics\nlag_mean = rolled.mean().reset_index(drop=True)\n\n# Check the values obtained\nlag_mean","cf293f39":"# 6. And add them to the train data set\nfor col in cols:\n    df[f'{col}_mean_lag{window}'] = lag_mean[col]\n    \n# 7. Replace missing values\ndf.replace(np.nan, 0, inplace=True)\n\n# Check it out\ndf.head()","2195498e":"def add_lag_feature(df, cols, group_col, window=3):\n    group_df = df.groupby(group_col)\n    rolled = group_df[cols].rolling(window=window, min_periods=0)\n    lag_mean = rolled.mean().reset_index()\n    for col in cols:\n        df[f'{col}_mean_lag{window}'] = lag_mean[col]\n    df.replace(np.nan, 0, inplace=True)\n    \n    return df","89e57b1e":"df = add_lag_feature(df, [\"airTemperature\"], \"site_id\", window=12)\ndf.head()","fc401b5e":"def train_model(df):\n    # Imports\n    from xgboost import XGBRegressor\n    from sklearn.metrics import mean_squared_log_error\n    \n    # Metric\n    def RMSLE(y_true, y_pred):\n        return np.sqrt(mean_squared_log_error(y_true, y_pred))\n    \n    # Split data\n    # Train set\n    X_train = df[df.timestamp < \"2017-01-01\"].set_index(\"timestamp\").drop(\"meter_reading\", axis=1)\n    y_train = df[df.timestamp < \"2017-01-01\"].set_index(\"timestamp\").meter_reading\n    # Validation set\n    X_val = df[df.timestamp >= \"2017-07-01\"].set_index(\"timestamp\").drop(\"meter_reading\", axis=1)\n    y_val =df[df.timestamp >= \"2017-07-01\"].set_index(\"timestamp\").meter_reading\n    \n    # Train\n    # This is the model we are going to train (a simple one)\n    my_model = XGBRegressor(n_estimators=50, learning_rate=0.1, n_jobs=4, random_state=55, objective=\"reg:squaredlogerror\")\n    # Train\n    my_model.fit(X_train, y_train, verbose=False)\n    \n    # Predict\n    prediction = pd.DataFrame({\"y_pred\": my_model.predict(X_val)})\n    \n    # Process prediction\n    # In case there is a negative or infinite prediction (could happens) we replace them with 0\n    prediction.replace([np.inf, -np.inf], np.nan, inplace=True) # replace inf\n    prediction[prediction <0] = np.nan # replace negative values\n    prediction.fillna(0, inplace=True) # replace all missings with 0\n\n    # Calculate the metric\n    rmsle = RMSLE(y_val, prediction.y_pred)\n    \n    return rmsle","7c50b8f0":"train_model(df)","90d4e75a":"# define our lag function\ndef add_lag_feature(df, cols, group_col, window=3):\n    group_df = df.groupby(group_col)\n    rolled = group_df[cols].rolling(window=window, min_periods=0)\n    lag_mean = rolled.mean().reset_index()\n    for col in cols:\n        df[f'{col}_mean_lag{window}'] = lag_mean[col]\n    df.replace(np.nan, 0, inplace=True)\n\n    return df\n\ndef create_simple_features(df):\n    \n    # Time features\n    df[\"month\"] = df.timestamp.dt.month\n    df[\"day\"] = df.timestamp.dt.day\n    df[\"weekday\"] = df.timestamp.dt.weekday\n    df[\"hour\"] = df.timestamp.dt.hour\n    df[\"is_weekend\"] = [weekday in [5,6] for weekday in df.weekday]\n    df[\"RH\"] = 100 - 5*(df.airTemperature - df.dewTemperature)\n    \n    # Group features\n    median_temp = pd.DataFrame(df.groupby(\"month\")[\"airTemperature\"].median()).rename(columns={\"airTemperature\": \"month_median_temp\"})\n    df = pd.merge(df, median_temp, how=\"left\", on=\"month\")\n    \n    # Lag features\n    cols = [\"airTemperature\"]\n    df = add_lag_feature(df, cols, \"site_id\", window=3)\n    df = add_lag_feature(df, cols, \"site_id\", window=12)\n    \n    return df","9e013100":"And now we have to add it to our dataframe. We have to [*merge*](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.merge.html) them:","5d3ecd82":"## Lag features\nAs you now, a version of this dataset was used in the [ASHRAE - Great Energy Predictor III](https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/) competition. One of the most voted notebooks is [this one](https:\/\/www.kaggle.com\/corochann\/ashrae-training-lgbm-by-meter-type), which, among other techniques, creates some lag features from the weather data. This is a simple but really strong strategy when working with time series: you are giving your model some \"memory\" with this information from the past. Let's try a simpler version of that.\n\nA \"lag feature\" is a summary feature that contains data from the past, considering a certain lag. For example, the max value of the last three days, the mean value of the last week, and so on. Here we will calculate the mean (you could use any summary metric: min, max, median, standard deviation, or even all of them); and we will consider a time window of 3 and 12 hours. Let's do it for the weather feature `airTemperature`. Remember that the weather data was collected by site, so the metrics should be calculate by site too. In this case we have only one site, but we will include the site grouping anyway.\n\nThe steps to follow are:\n\n1. Select a time window\n2. Select the columns\n3. Identify your grouping feature\n4. Create a rolling windows with the selected value\n5. Calculate the metric\n6. Add it to your data\n7. Replace missing values (this always happens at the beginning of your data, when you don't have enough rows to create the rolling window)\n\nTry it out, step by step for a window of 3:","8c3fcd8a":"## Mathematical transforms\nA ratio is one of the simplest mathematical transformations, it consist on dividing one value over another. In general one does not simply creates a ratio feature at random, they usually are known equations related to the data field. This apply to more complex equations that can be useful depending on the problema to solve.\n\nWith our data we could calculate the [simple aproximation for relative humidity](https:\/\/en.wikipedia.org\/wiki\/Dew_point#Simple_approximation):\n\n$ RH \\approx 100 - 5(T - T_{dp})$","dbb28e88":"An once you have this information in numeric values some new options open: you can create categorical features based on them. For example:\n\n- You could get the name on the season, based on the month and day (and don't forget the hemisphere, northern seasons are opposite to southern)\n- You could get a boolean feature named `working_hour`, based on the time (and don't forget the timezone, is pretty common to get UTC timestamps)\n- You could get a boolean feature named `is_weekend`, based on the weekday.\n\nLet's create the last one mentioned above using list comprehension:","566077c7":"# Notebook goal\nThe goal of this notebook is is to add new features to out data set and run a simple predictive model at the end. Here will be shown only one of the many possibilities in machine learning, feel free to experiment and play around with the data on your own. We will be focusing on the techniques and not on the model performance.","e1b181a2":"As usual, you can create a function to do it automatically. This is written for the mean, but you can try out differente metrics just changing it","facf281e":"# Model\nWe are working with a two years data. Let's split it like this:\n\n- **Training**: first year of data\n- **Validation**: second year of data","9a92a4a0":"# Simple feature engineering","e3f8e16f":"## Break down timestamp\nIn our dataset we have a timestamp, which is a feature to identify each meter reading (along with `building_id`) but also a rich source of information. The problema is that machine learning models can not correctly \"understand\" datetime object. If we want to feed our model with date ant time information some simple feature engineering is required: we have to transform our date\/time columns into numeric columns.\n\nLuckyly, there is a [pandas method to access datetime object](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.dt.html) and transform it in a simple step. Follow this sintax:\n```\nSeries.dt.aTimePeriod\n```\nWhere `aTimePeriod` is the time period you want: month, day, date, hour, seconds. In our case, we have a timestamp with information of the year, month, day, hour, minutes and seconds. Let's extract some of them and create new columns:","1b8bbd25":"## Summary metrics\nSummary metrics are also important in feature engineering. With this kind of features we are helping our model with information already processed: instead to work in finding, for example, the mean for each category in some categorical feature, the model already have it there.\n\nLet's find out the median temperature by month. First, we have to use the pandas [*groupby*](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.groupby.html) method:","d4a13b60":"# Introduction\n\nThis is a serie of notebooks thar should be visit in order, they are all linked in the table of content. In this notebook we are going to create new -simple- features that are based on another ones. At the end we will run a simple XGBoost model. Before going on with this notebook, make sure you have finished these courses:\n\n- [Python](https:\/\/www.kaggle.com\/learn\/python)\n- [Pandas](https:\/\/www.kaggle.com\/learn\/pandas)\n- [Intermediate machine learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)\n- [Feature Engineering](https:\/\/www.kaggle.com\/learn\/feature-engineering)\n\n#### Content table\n- [Preprocessing 1: Data Transformation](https:\/\/www.kaggle.com\/ponybiam\/preprocessing-1-data-transformation)\n- [Preprocessing 2: Encoding Categorical Variables](https:\/\/www.kaggle.com\/ponybiam\/preprocessing-2-encoding-categorical-variable\/)\n- [Preprocessing 3: Handling Missing Values](https:\/\/www.kaggle.com\/ponybiam\/preprocessing-3-handling-missing-values)\n- **Feature Engineering 1: Simple Features** (you are here)\n    - [Load and transform data](#Load-and-transform-data)\n    - [Simple feature engineering](#[Simple-feature-engineering)\n      - [Break down timestamp](#Break-down-timestamp)\n      - [Mathematical transforms](#Mathematical-transforms)\n      - [Summary metrics](#Summary-metrics)\n      - [Lag features](#Lag-features)\n    - [Model](#Model)\n- [Feature Engineering 2: Clustering & PCA](https:\/\/www.kaggle.com\/ponybiam\/feature-engineering-2-clustering-pca)\n- [Feature Engineering 3: Target Encoding](https:\/\/www.kaggle.com\/ponybiam\/feature-engineering-3-target-encoding)","bd9aebd7":"# Load and transform data\nIn our previous notebooks we [transformed our data](https:\/\/www.kaggle.com\/ponybiam\/preprocessing-pt-1-data-transformation), [encoded the categorical variables](https:\/\/www.kaggle.com\/ponybiam\/preprocessing-pt-2-encoding-categorical-variable) and [handled missing values](https:\/\/www.kaggle.com\/ponybiam\/preprocessing-pt-3-handling-missing-values) to be used in a predictive model. At the end of them we wrote a function to perform all the process explained there, at once. Don't worry about these functions, just make sure you checked and understood the transformation and encoding process in the previous notebooks.","a0fd7dba":"And let's use it to create a feature with a time windows of 12:","6dc4c015":"# (Optional) A function to do it all\nThis part is optional. We are going to write a function that follows all the steps we performed here. This function will be used in the following notebooks.","07c87bf8":"| Model         | Categorical | Missing values | Feature Engineering | RMSLE  |\n|---------------|-------------|----------------|---------------------|--------|\n| Preproc. pt.2 | Encoded     | Replaced by 0  | -                   | 2.4752 |\n| Preproc. pt.3 | Encoded     | Input          | -                   | 2.3726 |\n| FE pt.1       | Encoded     | Input          | Simple              | 1.0668 |"}}