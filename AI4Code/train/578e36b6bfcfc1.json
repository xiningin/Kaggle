{"cell_type":{"ba30d867":"code","9d7b6002":"code","671dc12b":"code","5fd02e9f":"code","eedd4335":"code","1a7a28f4":"code","57b679a1":"code","c1245f63":"code","6870abb1":"code","59edf821":"code","3bf757e2":"code","5373dd8c":"code","8c600a8e":"code","5f2b5408":"code","363ae03d":"code","89e31cb8":"code","a097bfb9":"code","ac593a9b":"code","8afb7fc0":"code","2c960333":"code","5d79d4fc":"code","b40a39c1":"code","b828c175":"code","62915bb1":"code","abf7a5e1":"code","08dc6ba6":"code","ffe1856e":"code","3c0c6e20":"code","71d26999":"code","46d28bef":"code","6ded15bf":"code","31e6340d":"code","b7bcf0bb":"code","69f57d2c":"code","667706a6":"code","4f53e983":"code","887c9ce4":"code","25f4dd25":"code","9864c398":"code","db3dd72e":"code","2d39f3e9":"code","4632d597":"code","6fef6db9":"code","16a5819d":"code","2056ccfc":"code","b9c4a2f1":"code","576db02a":"code","514a5a51":"code","74abeb6b":"code","600a6461":"code","22d8c249":"markdown","9b1791e1":"markdown","3ac23916":"markdown","51c7e712":"markdown","fe692410":"markdown","1aeb26c6":"markdown","723ebe27":"markdown","cea1a55f":"markdown","f22c79f7":"markdown","a0fcf347":"markdown","91a48851":"markdown"},"source":{"ba30d867":"import sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport gensim\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nnltk.download('stopwords')\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Flatten, Dropout, Dense, LSTM, Embedding\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score","9d7b6002":"base_path = '..\/input\/'","671dc12b":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import rankdata\n\ndef ridge_cv (vec, X, y, X_test, folds, stratified ):\n    kf = StratifiedKFold(n_splits=FOLDS,shuffle=True,random_state=123)\n    val_scores = []\n    rmse_scores = []\n    X_less_toxics = []\n    X_more_toxics = []\n\n    preds = []\n    for fold, (train_index,val_index) in enumerate(kf.split(X,stratified)):\n        X_train, y_train = X[train_index], y[train_index]\n        X_val, y_val = X[val_index], y[val_index]\n        model = Ridge()\n        model.fit(X_train, y_train)\n\n        rmse_score = mean_squared_error ( model.predict (X_val), y_val, squared = False) \n        rmse_scores.append (rmse_score)\n\n        X_less_toxic = vec.transform(df_val['less_toxic'])\n        X_more_toxic = vec.transform(df_val['more_toxic'])\n\n        p1 = model.predict(X_less_toxic)\n        p2 = model.predict(X_more_toxic)\n\n        X_less_toxics.append ( p1 )\n        X_more_toxics.append ( p2 )\n\n        # Validation Accuracy\n        val_acc = (p1< p2).mean()\n        val_scores.append(val_acc)\n\n        pred = model.predict (X_test)\n        preds.append (pred)\n\n        print(f\"FOLD:{fold}, rmse_fold:{rmse_score:.5f}, val_acc:{val_acc:.5f}\")\n\n    mean_val_acc = np.mean (val_scores)\n    mean_rmse_score = np.mean (rmse_scores)\n\n    p1 = np.mean ( np.vstack(X_less_toxics), axis=0 )\n    p2 = np.mean ( np.vstack(X_more_toxics), axis=0 )\n\n    val_acc = (p1< p2).mean()\n\n    print(f\"OOF: val_acc:{val_acc:.5f}, mean val_acc:{mean_val_acc:.5f}, mean rmse_score:{mean_rmse_score:.5f}\")\n    \n    preds = np.mean ( np.vstack(preds), axis=0 )\n    \n    return p1, p2, preds","5fd02e9f":"df_val = pd.read_csv(base_path + 'jigsaw-toxic-severity-rating\/validation_data.csv')\ndf_test = pd.read_csv(base_path + 'jigsaw-toxic-severity-rating\/comments_to_score.csv')","eedd4335":"FOLDS = 5\ndef TfidfVec(df):\n    \n    vec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(4, 6) )\n    X = vec.fit_transform(df['text'])\n    y = df[\"y\"].values\n    X_test = vec.transform(df_test['text'])\n    \n    return vec, X, y, X_test","1a7a28f4":"jf_train_df = pd.read_csv(base_path + \"jigsaw-toxic-comment-classification-challenge\/train.csv\")\nprint(f\"jf_train_df:{jf_train_df.shape}\")","57b679a1":"toxic = 1.0\nsevere_toxic = 2.0\nobscene = 1.0\nthreat = 1.0\ninsult = 1.0\nidentity_hate = 2.0\n\ndef create_train (df):\n    df['y'] = df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].max(axis=1)\n    df['y'] = df[\"y\"]+df['severe_toxic']*severe_toxic\n    df['y'] = df[\"y\"]+df['obscene']*obscene\n    df['y'] = df[\"y\"]+df['threat']*threat\n    df['y'] = df[\"y\"]+df['insult']*insult\n    df['y'] = df[\"y\"]+df['identity_hate']*identity_hate\n    \n    \n    \n    df = df[['comment_text', 'y', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].rename(columns={'comment_text': 'text'})\n\n    #undersample non toxic comments  on Toxic Comment Classification Challenge\n    min_len = (df['y'] >= 1).sum()\n    df_y0_undersample = df[df['y'] == 0].sample(n=int(min_len*1.5),random_state=201)\n    df = pd.concat([df[df['y'] >= 1], df_y0_undersample])\n                                                \n    return df\n \njf_train_df = create_train (jf_train_df)\nprint(jf_train_df['y'].value_counts())","c1245f63":"vec, X, y, X_test = TfidfVec(jf_train_df)\nstratified = np.around ( y )\njf_p1, jf_p2, jf_preds =  ridge_cv (vec, X, y, X_test, FOLDS, stratified )","6870abb1":"js_train_df = pd.read_csv(base_path + \"jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\nprint(f\"js_train_df:{js_train_df.shape}\")\njs_train_df = js_train_df.query (\"toxicity_annotator_count > 5\")\nprint(f\"juc_train_df:{js_train_df.shape}\")\n\njs_train_df['y'] = js_train_df[[ 'severe_toxicity', 'obscene', 'sexual_explicit','identity_attack', 'insult', 'threat']].sum(axis=1)\n\njs_train_df['y'] = js_train_df.apply(lambda row: row[\"target\"] if row[\"target\"] <= 0.5 else row[\"y\"] , axis=1)\njs_train_df = js_train_df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\nmin_len = (js_train_df['y'] > 0.5).sum()\ndf_y0_undersample = js_train_df[js_train_df['y'] <= 0.5].sample(n=int(min_len*1.5),random_state=201)\njs_train_df = pd.concat([js_train_df[js_train_df['y'] > 0.5], df_y0_undersample])\n\nprint(js_train_df['y'].value_counts())","59edf821":"vec, X, y, X_test = TfidfVec(js_train_df)\n\nstratified = (np.around (y, decimals=1)*10).astype(int)\njs_p1, js_p2, js_preds =  ridge_cv (vec, X, y, X_test, FOLDS, stratified )","3bf757e2":"rud_df = pd.read_csv(base_path + \"ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv\")\nprint(f\"rudd_df:{rud_df.shape}\")\nrud_df['y'] = rud_df['offensiveness_score'].map(lambda x: 0.0 if x <=0 else x)\nrud_df = rud_df[['txt', 'y']].rename(columns={'txt': 'text'})\nmin_len = (rud_df['y'] < 0.5).sum()\nprint(rud_df['y'].value_counts())","5373dd8c":"vec, X, y, X_test = TfidfVec(rud_df)\n\nstratified = (np.around ( y, decimals = 1  )*10).astype(int)\nrud_p1, rud_p2, rud_preds =  ridge_cv (vec, X, y, X_test, FOLDS, stratified )","8c600a8e":"jf_train_df = pd.read_csv(base_path + 'jigsaw-toxic-comment-classification-challenge\/train.csv')\njf_train_df.head()","5f2b5408":"jf_train_df.shape","363ae03d":"jf_train_df['toxicity'] = (jf_train_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0).astype(int)\njf_train_df = jf_train_df[['comment_text', 'toxicity']].rename(columns={'comment_text': 'text'})\njf_train_df.toxicity.value_counts()","89e31cb8":"jf_train_df = jf_train_df[['text', 'toxicity']]\njf_train_df.head()","a097bfb9":"js_train_df = pd.read_csv(base_path + 'jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\njs_train_df.head()","ac593a9b":"js_train_df.shape","8afb7fc0":"js_train_df['toxicity'] = (js_train_df[['target', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']].sum(axis=1) > 0).astype(int)\njs_train_df = js_train_df[['comment_text', 'toxicity']].rename(columns={'comment_text': 'text'})\njs_train_df.toxicity.value_counts()","2c960333":"js_train_df = js_train_df[['text', 'toxicity']]\njs_train_df.head()","5d79d4fc":"rud_df = pd.read_csv(\"..\/input\/ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv\")\nrud_df.head()","b40a39c1":"rud_df.shape","b828c175":"rud_df['toxicity'] = rud_df['offensiveness_score'].map(lambda x: 0 if x <=0 else 1)\nrud_df = rud_df[['txt', 'toxicity']].rename(columns={'txt': 'text'})\nrud_df.toxicity.value_counts()","62915bb1":"rud_df = rud_df[['text', 'toxicity']]\nrud_df.head()","abf7a5e1":"df = pd.concat([jf_train_df, js_train_df, rud_df])\ndf.toxicity.value_counts()","08dc6ba6":"df.shape","ffe1856e":"# sampling\nmin_len = (df['toxicity'] == 1).sum()\ndf_undersample = df[df['toxicity'] == 0].sample(n=min_len, random_state=201)\ndf = pd.concat([df_undersample, df[df['toxicity'] == 1]])\ndf = shuffle(df)","3c0c6e20":"df.toxicity.value_counts()","71d26999":"df.text = df.text.map(lambda x:x.replace('\\n', ' '))\ndf.text[:2]","46d28bef":"y = df.toxicity\nx = df.drop('toxicity', axis=1)","6ded15bf":"texts = x.copy()\ntexts.reset_index(inplace=True, drop=True)\ntexts.head()","31e6340d":"print(sys.getrecursionlimit())","b7bcf0bb":"sys.setrecursionlimit(6000)","69f57d2c":"ps = PorterStemmer()\ncorpus = []\n\nfor i in tqdm(range(0, len(texts))):\n    cleaned = re.sub('[^a-zA-Z]', ' ', texts['text'][i])\n    cleaned = cleaned.lower().split()\n    \n    cleaned = [ps.stem(word) for word in cleaned if not word in stopwords.words('english')]\n    cleaned = ' '.join(cleaned)\n    corpus.append(cleaned)","667706a6":"DIM = 100\n\nX = [d.split() for d in corpus]\nw2v_model = gensim.models.Word2Vec(sentences = X, vector_size = DIM, window = 10, min_count = 1)","4f53e983":"len(w2v_model.wv.key_to_index.keys())","887c9ce4":"w2v_model.wv.most_similar('toxic')","25f4dd25":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X)","9864c398":"X = tokenizer.texts_to_sequences(X)\nX[:3]","db3dd72e":"X = pad_sequences(X, padding='pre', maxlen=20)\nX[:3]","2d39f3e9":"vocab_size = len(tokenizer.word_index) + 1\nvocab = tokenizer.word_index","4632d597":"def get_weights_matrix(model):\n    weights_matrix = np.zeros((vocab_size, DIM))\n    \n    for word, i in vocab.items():\n        weights_matrix[i] = model.wv[word]\n        \n    return weights_matrix\n\nembedding_vectors = get_weights_matrix(w2v_model)","6fef6db9":"model = Sequential()\n\nmodel.add(Embedding(vocab_size, output_dim=DIM, weights=[embedding_vectors], input_length=20))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(64))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1, activation='linear'))","16a5819d":"model.compile(loss='mean_squared_error', optimizer='adam', metrics='accuracy')\nmodel.summary()","2056ccfc":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n\nes = EarlyStopping(patience=3,\n                  monitor='loss',\n                  restore_best_weights=True,\n                  mode='min',\n                  verbose=1)\n\nhist = model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs=10, callbacks=es, batch_size=32)","b9c4a2f1":"plt.style.use('fivethirtyeight')\n\n# visualize the models accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","576db02a":"# tokenizer for LSTM\nX_less_toxic = tokenizer.texts_to_sequences(df_val['less_toxic'])\nX_less_toxic = pad_sequences(X_less_toxic, maxlen=20)\nX_more_toxic = tokenizer.texts_to_sequences(df_val['more_toxic'])\nX_more_toxic = pad_sequences(X_more_toxic, maxlen=20)\nnew_text = tokenizer.texts_to_sequences(df_test.text)\nnew_text = pad_sequences(new_text, maxlen=20)","514a5a51":"# make predict value to list for Ensemble\nlstm_p1 = model.predict(X_less_toxic)\nlstm_p2 = model.predict(X_more_toxic)\nlstm_preds = np.hstack(model.predict(new_text))","74abeb6b":"jf_max = max(jf_p1.max() , jf_p2.max())\njs_max = max(js_p1.max() , js_p2.max())\nrud_max = max(rud_p1.max() , rud_p2.max())\nlstm_max = max(lstm_p1.max(), lstm_p2.max())\n\n\np1 = jf_p1\/jf_max + js_p1\/js_max + rud_p1\/rud_max + lstm_p1\/lstm_max\np2 = jf_p2\/jf_max + js_p2\/js_max + rud_p2\/rud_max + lstm_p2\/lstm_max\n\nval_acc = (p1 < p2).mean()\nprint(f\"Ensemble: val_acc:{val_acc:.5f}\")","600a6461":"score = jf_preds\/jf_max + js_preds\/js_max + rud_preds\/rud_max + lstm_preds\/lstm_max\n## to enforce unique values on score\ndf_test['score'] = rankdata(score, method='ordinal')\n\ndf_test[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\n\ndf_test.head()","22d8c249":"# Modeling & Training","9b1791e1":"# Processing predict value for Ensemble","3ac23916":"# Embedding","51c7e712":"# jigsaw-toxic-comment-classification-challenge Dataset","fe692410":"# Submission","1aeb26c6":"# Model Ruddit: Norms of Offensiveness for English Reddit Comments Dataset","723ebe27":"# Make one train data set","cea1a55f":"# Stemming","f22c79f7":"# Model Unintended Bias in Toxicity Classification Dataset","a0fcf347":"# RidgeRegression Ensemble","91a48851":"# Test Pre-Processing"}}