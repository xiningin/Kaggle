{"cell_type":{"e9633524":"code","2c1281d0":"code","6f740577":"code","372d0c5b":"code","d2f31999":"code","be67cc43":"code","3f92078a":"code","999aa969":"code","6cfe6a52":"code","131667a9":"code","cd8d9cde":"code","8cab50b2":"code","53b2e9a1":"code","c429f920":"code","b5a59c03":"code","8833988f":"code","fd0ad435":"code","836c5dee":"code","fa571fa0":"code","01d66780":"code","74903a7d":"code","4637ff81":"markdown"},"source":{"e9633524":"import pandas as pd","2c1281d0":"test_data = pd.read_csv('..\/input\/train.tsv', sep='\\t')\n#print(test_data)\n#pd.read_csv('data\/train.tsv', sep='\\t')\ntestdf = test_data.values\n\nXtrain = testdf[:,2] #This will have all rows with index 2 col (Our reviews)\nprint(Xtrain)\n\nytrain = testdf[:,3] #This will all rows with index 3 col (reviews label)\n#print(ytrain)","6f740577":"from numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding","372d0c5b":"t = Tokenizer() #Technique to convert text list to text list index","d2f31999":"t.fit_on_texts(Xtrain) ","be67cc43":"vocab_size = len(t.word_index) + 1\nprint(vocab_size)","3f92078a":"# integer encode the documents\nencoded_docs = t.texts_to_sequences(Xtrain)\nprint(encoded_docs[0])","999aa969":"item = max(Xtrain, key=len)\nprint(len(item))","6cfe6a52":"# pad documents to a max length of 4 words\nmax_length = 20\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","131667a9":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open('..\/input\/glove.6B.100d.txt', encoding=\"utf8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","cd8d9cde":"print('Loaded %s word vectors.' % len(embeddings_index))","8cab50b2":"# create a weight matrix for words in training docs\nembedding_matrix = zeros((vocab_size, 100))\nfor word, i in t.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","53b2e9a1":"# define model\n#model = Sequential()\n#e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n#model.add(e)\n#model.add(Flatten())\n#model.add(Dense(1, activation='sigmoid'))\n# compile the model\n#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n# summarize the model\n#print(model.summary())\n\nmodel = Sequential()\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=20, trainable=False)\nmodel.add(e)\nmodel.add(Flatten())\nmodel.add(Dense(32, input_dim=20, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(5, activation='softmax'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","c429f920":"print(ytrain)","b5a59c03":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\n# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(ytrain)\nencoded_Y = encoder.transform(ytrain)\n# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y = np_utils.to_categorical(encoded_Y)","8833988f":"# fit the model\nmodel.fit(padded_docs, dummy_y, epochs=500, verbose=1)","fd0ad435":"from keras.models import load_model","836c5dee":"model.save('sentiment.h5')","fa571fa0":"# integer encode the documents\nXtest = ['bad movie ever in the history', 'awesome movie, very good', 'bad', 'what a awesome movie', 'movie was good']\n\ntest_docs = t.texts_to_sequences(Xtest)\n#print(test_docs)\n\ntest_padded_docs = pad_sequences(test_docs, maxlen=max_length, padding='post')\n#print(test_padded_docs)\n\na = model.predict(test_padded_docs)\n#print(a)\n\nfor x in a:\n    m = max(x)\n    print(m, [i for i, j in enumerate(x) if j == m][0])","01d66780":"testing_data = pd.read_csv('..\/input\/test.tsv', sep='\\t')\ntestingdf = testing_data.values\n\nXphraseID = testingdf[:,0]\nXtest = testingdf[:,2]\nprint(Xtest)\n#print(XphraseID)","74903a7d":"import csv\ntest_docs = t.texts_to_sequences(Xtest)\n#print(test_docs)\n\ntest_padded_docs = pad_sequences(test_docs, maxlen=max_length, padding='post')\n#print(test_padded_docs)\n\na = model.predict(test_padded_docs)\n#print(a)\n\nsubmission = open('..\/input\/Submission.csv','w')\ncolumnTitleRow = \"PhraseId, Sentiment\\n\"\nsubmission.write(columnTitleRow)\n\nfor counter, x in enumerate(a):\n    m = max(x)\n    print(XphraseID[counter], m, [i for i, j in enumerate(x) if j == m][0])\n    #submission.write(str(XphraseID[counter])+','+str([i for i, j in enumerate(x) if j == m][0])+'\\n')\n    PhraseId = str(XphraseID[counter])\n    Sentiment = str([i for i, j in enumerate(x) if j == m][0])\n    row = PhraseId + \",\" + Sentiment + \"\\n\"\n    submission.write(row)\n    \nsubmission.close()    \n    ","4637ff81":"The sentiment labels are:\n\n0 - negative, 1 - somewhat negative, 2 - neutral, 3 - somewhat positive, 4 - positive"}}