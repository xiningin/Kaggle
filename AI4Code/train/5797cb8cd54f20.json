{"cell_type":{"b7d4216f":"code","a736c648":"code","0970d1b2":"code","244fe011":"code","632020b0":"code","b2717fe5":"code","f9f7602f":"code","081f4940":"code","2ea82649":"code","d47342f4":"code","5923e68d":"code","14109ed4":"code","5f268d77":"code","a2887c67":"code","c8bb9dc2":"code","f65113a0":"code","985f1e3c":"code","b02fe560":"code","70a8a524":"code","db247406":"code","38ce32d7":"code","58d082a2":"code","efaf1922":"code","6e9183f2":"code","f3278ba4":"markdown","608720db":"markdown","fd449085":"markdown","cfee0945":"markdown","f0d13fa1":"markdown","fef97bc7":"markdown","8dd64c39":"markdown","6444b0ab":"markdown","efd77865":"markdown","2ec4608f":"markdown","3dc100fe":"markdown","d38734a2":"markdown","339ca854":"markdown","dbe5ed51":"markdown","e72ddf66":"markdown"},"source":{"b7d4216f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom lightgbm import LGBMClassifier\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom transformers import BertTokenizer\n\nprint(tf.__version__)","a736c648":"df = pd.read_csv('..\/input\/disaster-tweets-cleaned\/df.csv')\ntest_df = pd.read_csv('..\/input\/disaster-tweets-cleaned\/test_df.csv')\nprint(df.shape, test_df.shape)","0970d1b2":"def plot_sent_len(dataf, col, title):\n    dataf['len_' + col] = dataf[col].apply(lambda txt: len(txt.split()))\n    plt.hist(dataf['len_' + col], bins = 100)\n    plt.title('Train sentences length')\n    plt.show()\n    return dataf\n\ncol = 'ctext'\ndf = plot_sent_len(df, col, 'sentence lengths')","244fe011":"train_txts, val_txts, y_train, y_val = train_test_split(\n    df[col].values, df['target'].values,\n    shuffle = True, test_size = 0.15,\n    stratify = df['target'].values,\n)\ntest_txts = test_df[col].values\ny_test = test_df['target'].values\nprint('Train size:', train_txts.shape)\nprint('Validation size:', val_txts.shape)\nprint('Test size:', test_txts.shape)","632020b0":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","b2717fe5":"def tokenize_txts(txts, max_len = 40):\n    res = tokenizer(\n        text = [tokenizer.tokenize(txt) for txt in txts],\n        max_length = max_len,\n        padding = 'max_length',\n        truncation = True,\n        is_split_into_words = True,\n    )\n    return {\n        'input_word_ids': res['input_ids'],\n        'input_mask': res['attention_mask'],\n        'input_type_ids': res['token_type_ids'],\n    }\n\ndef detokenize_txt(token):\n    res = tokenizer.decode(token)\n    return res","f9f7602f":"MAX_LEN = 35\nBATCH_SIZE = 32","081f4940":"train_tokens = tokenize_txts(train_txts, MAX_LEN)\nval_tokens = tokenize_txts(val_txts, MAX_LEN)\ntest_tokens = tokenize_txts(test_txts, MAX_LEN)\n","2ea82649":"print('Orginal txt: ', train_txts[0])\nprint()\nsample = train_tokens['input_word_ids'][0]\nprint('Tokenized txt:', sample)\nprint()\nprint('Detokenizd txt:', detokenize_txt(sample))","d47342f4":"train_ds = tf.data.Dataset.from_tensor_slices((train_tokens, y_train))\nval_ds = tf.data.Dataset.from_tensor_slices((val_tokens, y_val))\ntest_ds = tf.data.Dataset.from_tensor_slices((test_tokens, y_test))\n\ntrain_ds = train_ds.batch(BATCH_SIZE)\nval_ds = val_ds.batch(BATCH_SIZE)\ntest_ds = test_ds.batch(BATCH_SIZE)","5923e68d":"class MyF1(tf.keras.metrics.Metric):\n    def __init__(self, name = 'mf1_score'):\n        super(MyF1, self).__init__(name)\n        self.p = tf.metrics.Precision()\n        self.r = tf.metrics.Recall()\n        self.f1 = self.add_weight(name=\"f1\", initializer=\"zeros\")\n    \n    def update_state(self, actual, predicted, sample_weight = None):\n        self.p.update_state(actual, predicted)\n        self.r.update_state(actual, predicted)\n        self.f1.assign(2 * self.p.result() * self.r.result() \/ (self.p.result() + self.r.result()))\n        \n    def reset_states(self, ):\n        self.p.reset_states()\n        self.r.reset_states()\n        self.f1.assign(0.0)\n    def result(self,):\n        return self.f1    ","14109ed4":"class CSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, lr, freeze_epoch ,batch_size, data_size):\n        super(CSchedule, self).__init__()\n        self.lr = lr\n        self.bs = batch_size\n        self.ds = data_size\n        self.freeze_epoch = freeze_epoch\n\n    def __call__(self, step):\n        epoch = step \/(self.ds \/ self.bs) + 1\n        if not self.freeze_epoch or epoch < self.freeze_epoch:\n            return self.lr \/ (tf.cast(epoch, tf.float32)+1)\n        else:\n            return self.lr * 10 \/ (tf.cast(epoch, tf.float32)+1)\n        \nclr = CSchedule(2e-5, 3, BATCH_SIZE, len(train_txts))\nplt.plot([clr(x) for x in tf.range(1, 1457, dtype=tf.float32)])\nplt.xlabel('steps')\nplt.ylabel('learning rate')\nplt.title('Learning Rate Schedule')\nplt.show()","5f268d77":"bert_handler = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3'","a2887c67":"class MClassifier(tf.keras.Model):\n    def __init__(self, dropout_rate):\n        super(MClassifier, self).__init__()\n        \n        self.bert_layer = hub.KerasLayer(\n            bert_handler,\n            name = 'feature_ext',\n            trainable = True,\n        )\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        self.proba = tf.keras.layers.Dense(1, activation = 'sigmoid')\n\n    def call(self, X_in, training):\n        x = self.bert_layer(X_in, training = training)\n        x = x['pooled_output']\n        x = self.dropout(x, training = training)\n        output = self.proba(x)\n        return output","c8bb9dc2":"LEARNING_RATE = 2e-5\nEPOCHS = 5\nDP_RATE = 0.3\n\nloss_objective = tf.keras.losses.BinaryCrossentropy()","f65113a0":"def model_evaluation(model, ds, name):\n    acc = tf.keras.metrics.BinaryAccuracy()\n    f1 = MyF1()\n    total_loss = []\n    y_hats = []\n    for X, y in ds:\n        y_hat = model(X, training = False)\n        loss = loss_objective(y, y_hat)\n        acc.update_state(y, y_hat)\n        f1.update_state(y, y_hat)\n        total_loss.append(loss.numpy())\n        y_hats.append(y_hat)\n    y_hats = tf.concat(y_hats, axis = 0).numpy()\n    print(f'{name}, loss {np.mean(total_loss):.3f}, acc {acc.result().numpy():.3f}, f1 {f1.result().numpy():.3f}')\n    return y_hats, acc.result().numpy(), f1.result().numpy()","985f1e3c":"@tf.function\ndef train_step(model, tr_vars, X, y):\n    with tf.GradientTape() as tape:\n        y_hat = model(X, training = True)\n        loss = loss_objective(y, y_hat)\n    grads = tape.gradient(loss, tr_vars)\n    return loss, grads, y_hat\n\ndef train_model(model, epochs, freeze_bert_on_epoch = None):\n    clr = CSchedule(LEARNING_RATE, freeze_bert_on_epoch, BATCH_SIZE, len(train_txts))\n    optimizer = tf.keras.optimizers.Adam(clr)\n    acc = tf.keras.metrics.BinaryAccuracy()\n    f1 = MyF1()\n    for epoch in range(1, epochs + 1):\n        \n        if freeze_bert_on_epoch is not None and epoch == freeze_bert_on_epoch:\n            model.get_layer('feature_ext').trainable = False\n            \n        acc.reset_states()\n        f1.reset_states()\n        total_loss = []\n        for X, y in train_ds:\n            tr_vars = model.trainable_variables\n            loss, grads, y_hat = train_step(model, tr_vars, X, y)\n            optimizer.apply_gradients(zip(grads, tr_vars))\n            acc.update_state(y, y_hat)\n            f1.update_state(y, y_hat)\n            total_loss.append(loss.numpy())\n            \n        print(f'epoch {epoch}, loss {np.mean(total_loss):.3f}, acc {acc.result().numpy():.3f}, f1 {f1.result().numpy():.3f}')\n        model_evaluation(model, val_ds, 'val')\n        print()\n        \n    y_test_hat, _, _ = model_evaluation(model, test_ds, 'test')\n    return y_test_hat","b02fe560":"model = MClassifier(DP_RATE,)\ny_test_hat = train_model(model, EPOCHS, freeze_bert_on_epoch = 3)","70a8a524":"y_model_hat = np.array([1 if x[0] >0.5 else 0 for x in y_test_hat])\nprint(classification_report(y_test, y_model_hat))","db247406":"def plot_embedding(X, y):\n    colors = ['green', 'red']\n    labels = ['NoDisastor', 'Disastor']\n    proj = PCA(2)\n    proj_X = proj.fit_transform(X)\n    for y_id in np.unique(y):\n        plt.scatter(\n            x = proj_X[y == y_id, 0], \n            y = proj_X[y == y_id, 1],\n            s = 4,\n            label = labels[y_id],\n            c = colors[y_id], \n            alpha = 0.4\n        )\n        plt.xlabel('X1')\n        plt.ylabel('X2')\n        plt.legend()\n\ndef plot_embeddings(pooled, seq, mean_max, y, title):\n    fig = plt.figure(figsize = (12, 4))\n    plt.subplot(1, 3 ,1 )\n    plot_embedding(pooled, y)\n    plt.title(f'Pooled output in {title} dataset')\n    \n    plt.subplot(1, 3 ,2)\n    plot_embedding(seq, y)\n    plt.title(f'Mean hidden state in {title} dataset')\n    \n    plt.subplot(1, 3 ,3)\n    plot_embedding(mean_max, y)\n    plt.title(f'MeanMax hidden state in {title} dataset')\n    plt.show()\n    \ndef accumulate_embeddings(model, ds):\n    embedder = model.get_layer('feature_ext')\n    pooled_list = []\n    seq_list = []\n    for X, y in ds:\n        output = embedder(X,)\n        pooled_output = output['pooled_output'].numpy()\n        seq_output = output['sequence_output'].numpy()\n        pooled_list.append(pooled_output)\n        seq_list.append(seq_output)\n    pooled_list = tf.concat(pooled_list, axis = 0).numpy()\n    seq_list = tf.concat(seq_list, axis = 0).numpy()\n    return pooled_list, np.mean(seq_list, axis = 1), np.mean(seq_list, axis = 1) + np.max(seq_list, axis = 1)","38ce32d7":"tr_pooled, tr_seq, tr_mean_max = accumulate_embeddings(model, train_ds,)\nplot_embeddings(tr_pooled, tr_seq,tr_mean_max, y_train, 'Train')\n\nval_pooled, val_seq, val_mean_max = accumulate_embeddings(model, val_ds,)\nplot_embeddings(val_pooled, val_seq,val_mean_max, y_val, 'Valid')\n\ntest_pooled, test_seq, test_mean_max = accumulate_embeddings(model, test_ds,)\nplot_embeddings(test_pooled, test_seq, test_mean_max, y_test, 'Test')","58d082a2":"def train_gbm_cls(X_tr, y_tr, X_val, y_val, X_test, y_test):\n    gbm_cls = LGBMClassifier(\n        objective = 'binary',\n        class_weight = 'balanced'\n    )\n    gbm_cls.fit(\n        X_tr, y_tr,\n        eval_set = (X_val, y_val),\n        early_stopping_rounds = 20,\n        verbose = 0,\n    )\n    print('Train')\n    print(classification_report(y_train, gbm_cls.predict(X_tr)))\n    print('Validation')\n    print(classification_report(y_val, gbm_cls.predict(X_val)))\n    print('Test')\n    gbm_y_hat = gbm_cls.predict(X_test)\n    print(classification_report(y_test, gbm_y_hat))\n    return gbm_cls, gbm_y_hat\n\ntrain_feat = tr_mean_max\nval_feat = val_mean_max\ntest_feat = test_mean_max\n\ngbm_cls, gbm_y_hat = train_gbm_cls(train_feat, y_train, val_feat, y_val , test_feat, y_test)","efaf1922":"sub = pd.DataFrame(columns = ['id', 'target'])\nsub['id'] = test_df.id\nsub['target'] = gbm_y_hat\nsub.to_csv('sub.csv', index = False)","6e9183f2":"nsub = pd.DataFrame(columns = ['id', 'target'])\nnsub['id'] = test_df.id\nnsub['target'] = y_model_hat\nnsub.to_csv('nsub.csv', index = False)","f3278ba4":"# Plot pooled output vs last hidden state","608720db":"# Train\/Val split","fd449085":"# Read Data","cfee0945":"# Train LGBM on the MeanMax hidden state","f0d13fa1":"# Batch the data","fef97bc7":"# Download Huggingface Tokenizer","8dd64c39":"# Define Metrics","6444b0ab":"# Tokenize Data","efd77865":"# Plot sentences Lengths","2ec4608f":"# Define Learning rate schedule","3dc100fe":"# Define the Model","d38734a2":"# Model Trainig\n\n- Fine tune Bert for x epochs\n- Freeze Bert after x epochs and train the rest of the network for another y epochs","339ca854":"# Model Evaluate on the testset","dbe5ed51":"# Exploratory Data Analysis\nhttps:\/\/www.kaggle.com\/bkassem\/disaster-tweets-eda\n\n# Data Cleaning\nhttps:\/\/www.kaggle.com\/bkassem\/disastor-tweets-cleaning","e72ddf66":"# Import Libs"}}