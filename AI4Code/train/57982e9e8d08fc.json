{"cell_type":{"160b75c6":"code","74b16539":"code","5232910b":"code","a6559680":"code","16db16a9":"code","165be712":"code","65bac626":"code","a06839ef":"code","f9750bb2":"markdown","f95064c0":"markdown","6f6dbe4c":"markdown","66b03c9e":"markdown"},"source":{"160b75c6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","74b16539":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\")","5232910b":"y = train.target\ntrain.drop(['target', 'id'], axis = 1, inplace = True)\ntest.drop(['id'], axis = 1, inplace = True)\ntrain.head()","a6559680":"from sklearn.preprocessing import OrdinalEncoder\ncat_cols = [col for col in train.columns if 'cat' in col]\n\nX = train.copy()\nX_test = test.copy()\nenc = OrdinalEncoder()\nX[cat_cols] = enc.fit_transform(train[cat_cols])\nX_test[cat_cols] = enc.transform(test[cat_cols])\nX.head()","16db16a9":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nxgb_params = {\n    'random_state': 1, \n    'booster': 'gbtree',\n    'n_estimators': 10000,\n    'learning_rate': 0.03628302216953097,\n    'reg_lambda': 0.0008746338866473539,\n    'reg_alpha': 23.13181079976304,\n    'subsample': 0.7875490025178415,\n    'colsample_bytree': 0.11807135201147481,\n    'max_depth': 3\n}\n\n\n# My own Optuna tuning parameters\n# xgb_params = {\n# 'lambda': 67.79737006663706,\n# 'alpha': 40.12405005448161,\n# 'colsample_bytree': 0.061613774851329205,\n# 'subsample': 0.9556736521337416,\n# 'learning_rate': 0.17024722721525629,\n# 'n_estimators': 9489,\n# 'max_depth': 3,\n# 'booster': 'gbtree',\n# 'min_child_weight': 155,\n# 'seed' : 38\n# }\n# Model hyperparameters\n# xgb_params = {\n#     'n_estimators': 5000,\n#     'learning_rate': 0.1235,\n#     'subsample': 0.95,\n#     'colsample_bytree': 0.11,\n#     'max_depth': 2,\n#     'booster': 'gbtree', \n#     'reg_lambda': 66.1,\n#     'reg_alpha': 15.9,\n#     'random_state':42\n# }\n# xgb_params = {\n#       'n_estimators': 10000,\n#       'learning_rate': 0.35,\n#       'subsample': 0.926,\n#       'colsample_bytree': 0.84,\n#       'max_depth': 2,\n#       'booster': 'gbtree', \n#       'reg_lambda': 35.1,\n#       'reg_alpha': 34.9,\n#       'random_state': 42,\n#       'n_jobs': 4\n# }","165be712":"#Setting the kfold parameters\nkf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(kf.split(X)):\n    X_train, X_valid = X.loc[train_id], X.loc[valid_id]\n    y_train, y_valid = y.loc[train_id], y.loc[valid_id]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 100)\n    \n    #Mean of the predictions\n    preds += model.predict(X_test) \/ 10 # Splits\n    \n    #Mean of feature importance\n    model_fi += model.feature_importances_ \/ 10 #splits\n    \n    #Out of Fold predictions\n    oof_preds[valid_id] = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_id]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ 10\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","65bac626":"sub.target = preds\nsub.head()","a06839ef":"sub.to_csv(\"submission.csv\", index = False)\nprint(\"Sent\")","f9750bb2":"### I ran same code on local machine but gets different result. It seems that by running on the Kaggle notebook will generate a bettwe result. However, it does not supprot featur_engine package or other features. Also, it appears that tuning with Optuna generate better score than what's carried from forked parameters.","f95064c0":"# Encoding categorical data","6f6dbe4c":"In this notebook I'm only using a few things I learned on the lasts days of \"30 Days of ML\" so I'm keeping things as simple as I can.\n# Load and visualize dataset","66b03c9e":"# Modeling"}}