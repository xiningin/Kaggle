{"cell_type":{"d682a362":"code","98ec98ab":"code","291d803a":"code","d1e560c0":"code","c21709b7":"code","e84049b4":"code","e7952802":"code","8f6f7589":"code","c35d7c08":"code","4bad314c":"code","0445f059":"code","1d07c7a9":"code","5f098c6f":"markdown","d62d8771":"markdown","d8f67171":"markdown","965d6336":"markdown","7e498231":"markdown","cf323664":"markdown","16868900":"markdown","568eb569":"markdown","b1e0c9f7":"markdown","002a0ad9":"markdown"},"source":{"d682a362":"#Importing Libraries \n#basics and Visualization\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n#ML libraries\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\n\n#metrics\nfrom statistics import mean\nfrom sklearn.metrics import accuracy_score as score\nfrom sklearn.metrics import explained_variance_score as evs\n\n\n#Ignore Warning \nimport warnings as wrn\nwrn.filterwarnings('ignore')","98ec98ab":"df = pd.read_csv(r'..\/input\/balance-scale.csv')\ndf.head()","291d803a":"df.info()","d1e560c0":"#Visualization after doing label encoding\ndf['Class'] = LabelEncoder().fit_transform(df['Class'].tolist())\n#pairplot\nsns.pairplot(data=df)\n\n#Heatmap\nnum_feat = df.select_dtypes(include=np.number).columns\nplt.figure(figsize= (15, 15))\nsns.heatmap(df.corr())\n","c21709b7":"#Dividing X and y\ny = df[['Class']]\nX = df.drop(['Class'], axis = 1)\n\nprint(y.head())\nprint(X.head())","e84049b4":"train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 20)","e7952802":"#Classification and prediction\n#ExtraTreeClassifier\n\nclf = ExtraTreesClassifier(n_estimators=1000)\nclf.fit(train_X, train_y)\npred = clf.predict(test_X)\nprint('Accuracy in percent = ',score(pred, test_y)*100)","8f6f7589":"#Classification and prediction\n#XGBoost\n\nclf = XGBClassifier(learning_rate=0.5, n_jobs=-1, n_estimators=1000)\nclf.fit(train_X, train_y)\npred = clf.predict(test_X)\nprint('Accuracy in percent = ',score(pred, test_y)*100)","c35d7c08":"#Classification and prediction\n#Random Forest\n\nclf = RandomForestClassifier(n_estimators=1000, n_jobs=-1, )\nclf.fit(train_X, train_y)\npred = clf.predict(test_X)\nprint('Accuracy in percent = ',score(pred, test_y)*100)","4bad314c":"#Classification and prediction\n#DT\n\nclf = DecisionTreeClassifier()\nclf.fit(train_X, train_y)\npred = clf.predict(test_X)\nprint('Accuracy in percent = ',score(pred, test_y)*100)","0445f059":"#Classification and prediction\n#SVM\n\nclf = SVC()\nclf.fit(train_X, train_y)\npred = clf.predict(test_X)\nprint('Accuracy in percent = ',score(pred, test_y)*100)","1d07c7a9":"#Classification and prediction\n#KNN\n\nclf = KNeighborsClassifier(n_neighbors=9)\nclf.fit(train_X, train_y)\npred = clf.predict(test_X)\nprint('Accuracy in percent = ',score(pred, test_y)*100)","5f098c6f":"Spliting the X and y in four DataFrame :\ntrain_X, test_X, train_y and test_y using rain_test_split","d62d8771":"Importing Libraries ","d8f67171":"Classification and prediction using following Classifiers :\n\n- Extra Tree    Classifier\n- Xgboost       Classifier\n- Random Forest Classifier\n- Decision Tree Classifier\n- Support Vector Machine\n- K-Nearest Neighbor","965d6336":"Result :     Accuracy score of each classifier in percentage \n\n\n- Extra Tree    Classifier  : 90\n- Xgboost       Classifier  : 95\n- Random Forest Classifier  : 90\n- Decision Tree Classifier  : 85\n- Support Vector Machine    : 95\n- K-Nearest Neighbor        : 95\n\n\nConclusion :\nXGBoost Classifier, SVM, and KNN have best accuracy with 95%\n\nNote : Accuracy will variate at each run of kernel","7e498231":"Conclusion : Plots show that the other attributes are equally correlated to Class. \n- Hence, droping other columns except for 'Class' is not required","cf323664":"Reading the csv file","16868900":"Label Encoding the 'Class' column so that heatmap can be plotted\n\nPlotting pairplot and heatmap to see the correlation among the attributes","568eb569":"Calling info to check the number of missing values and datatypes","b1e0c9f7":"Seperating features as X and target value as y : \n- 'Class' in y\n- 'L-Weight', 'R-Weight', 'L-Distance', 'R-Distance' in X","002a0ad9":"Dataset Desciption :\nSource: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Balance+Scale\n\nGenerated to model psychological experiments reported by Siegler, R. S. (1976).\nThree Aspects of Cognitive Development. Cognitive Psychology, 8, 481-520.\n\nDonor:\n\nTim Hume (hume '@' ics.uci.edu)\n\nData Set Information:\n\nThis data set was generated to model psychological experimental results. Each example is classified as having the balance scale tip to the right, tip to the left, or be balanced. The attributes are the left weight, the left distance, the right weight, and the right distance. The correct way to find the class is the greater of (left-distance * left-weight) and (right-distance * right-weight). If they are equal, it is balanced.\n\nAttribute Information:\n\n1. Class Name: 3 (L, B, R)\n2. Left-Weight: 5 (1, 2, 3, 4, 5)\n3. Left-Distance: 5 (1, 2, 3, 4, 5)\n4. Right-Weight: 5 (1, 2, 3, 4, 5)\n5. Right-Distance: 5 (1, 2, 3, 4, 5)\n"}}