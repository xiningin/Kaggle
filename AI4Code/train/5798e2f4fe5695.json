{"cell_type":{"05484490":"code","17afcc67":"code","6e43da6f":"code","8a5c2b56":"code","21340c8d":"code","35ddd31f":"code","c4541140":"code","dcbd80e8":"code","946d9247":"code","c2b3fb01":"code","710c1c8d":"code","07e3c8c1":"code","50b046d6":"code","ac65f0b2":"code","6c59ae59":"code","06e11ea3":"code","8cff9b2b":"code","34e14076":"code","6b1d800b":"markdown","a22def6b":"markdown","5081a72f":"markdown","f2edab2a":"markdown","336abb3d":"markdown","d25df3f8":"markdown","85fef135":"markdown","2663035a":"markdown","25ede893":"markdown","935c13da":"markdown","635fcbed":"markdown","b67d32c7":"markdown","eff39cce":"markdown"},"source":{"05484490":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","17afcc67":"# General\nimport numpy as np\nimport pandas as pd\nimport os\nfrom time import time\n\n# Data Exploration\nimport pandas_profiling \n\n# Visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdate\n\n# Data Transformation\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# Encoders\nfrom sklearn.preprocessing import LabelEncoder\n\n# Split Data\nfrom sklearn.model_selection import train_test_split\n\n# Models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Metrics\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n","6e43da6f":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","8a5c2b56":"def dataframe_details(df):\n  '''\n  Input: Dataframe\n  Output: Get basic information on the dataframe \n  '''\n\n  first_five_rows = df.head()\n  data_shape = df.shape\n  data_types = df.dtypes\n  n_rows = data_shape[0]\n  n_columns = data_shape[1]\n  null_values = df.isnull().sum()\n  numeric_columns = df.select_dtypes([np.number]).columns.tolist()\n  columns = list(df)\n  categorical_columns = []\n\n  for i in columns:\n    if i not in numeric_columns:\n      categorical_columns.append(i)\n    else:\n      None\n\n\n  print('The shape of the dataframe:',data_shape)\n  print('\\nCategorical columns:',len(categorical_columns))\n  print(categorical_columns)\n  print('\\nNumeric columns:',len(numeric_columns))\n  print(numeric_columns)\n  print('\\nThe percentage of null values in columns:')\n  print(round((null_values\/n_rows)*100,0))\n  print('\\nFirst five rows of the dataframe:')\n  print(first_five_rows)\n  print('\\nThe data types of columns:')\n  print(data_types)\n  print('\\nThe number of unique values in a column:')\n  for col in df:\n        print(col,':',df[col].nunique())","21340c8d":"def missing_data_basic_analysis(df):\n  '''\n  Input: Dataframe\n  Output: Get information on missing data\n  '''\n  data_shape = df.shape\n  data_types = df.dtypes\n  n_rows = data_shape[0]\n  n_columns = data_shape[1]\n  \n  null_values = df.isnull().sum()\n  \n  columns_nans = df.isnull().sum()[df.isnull().sum() > 0]\n  \n  n_columns_nans = len(columns_nans)\n  \n  percent_nan_columns = round((columns_nans\/n_rows)*100,2).sort_values()\n  \n  total_missing = df.isna().sum().sum()\n  percent = percent_nan_columns\n\n  drop_columns = percent.where(percent > 30)\n  drop_columns = drop_columns.dropna()\n  n_drop_columns = len(drop_columns)\n\n\n    \n  percent_nan_columns.plot(kind='bar',figsize=(18, 8))\n  print('\\nColumns with null values:')\n  print('There are',n_columns_nans,'columns with null values which is',round((n_columns_nans\/n_columns)*100,0),'percent of the features')\n  print('\\nTotal number of missing values:')\n  print(total_missing,'which is',round((n_rows\/total_missing)*100,0),'percent of total data')\n  print('\\nColumns with NaNs:')\n  print(percent_nan_columns)\n  print('\\nColumns with NaNs greater than 30%:')\n  print(drop_columns)\n  print('\\nNumber of columns to be dropped:')\n  print(n_drop_columns,'needs to be dropped, that is,',round((n_drop_columns\/n_columns)*100,0),'of total features')\n  print('\\nDescriptive analysis of columns with missing values')\n  print(percent_nan_columns.describe())\n","35ddd31f":"def categorical_columns_list(df):\n  '''\n  Input: Dataframe\n  Output: Get a list of categorical columns \n  '''\n  \n  null_values = df.isnull().sum()\n  numeric_columns = df.select_dtypes([np.number]).columns.tolist()\n  columns = list(df)\n  categorical_columns = []\n\n  for i in columns:\n    if i not in numeric_columns:\n      categorical_columns.append(i)\n    else:\n      None\n  return categorical_columns","c4541140":"def class_balance(df, target_attribute):\n      '''\n      Input: Dataframe, target attribute\n      Output: Get class balance in percentages\n      '''\n  n_rows = df.shape[0]\n  classes = df[target_attribute].value_counts()\n  rows = len(classes)\n  cols = 2\n  class_balance = [[0]*cols]*rows\n  \n  for idx, i in enumerate(classes):\n    class_name = idx\n    class_percent = i\/n_rows\n    print('Class:',idx)\n    print('Percentage:',i\/n_rows)\n    print('\\n')\n","dcbd80e8":"# Get dataframe details\ndataframe_details(train)","946d9247":"# Get information on missing data \nmissing_data_basic_analysis(train)","c2b3fb01":"# Find class balance\nclass_balance(train,'Survived')","710c1c8d":"# Drop 'Cabin' feature since it has a large amount of missing values\ntrain.drop(['Cabin'],axis=1)","07e3c8c1":"# Using One-Hot encoding, encode data\ntrain_prepared = pd.get_dummies(train)","50b046d6":"# Handle missing data\nimputer = SimpleImputer(strategy='median')\ntrain_prepared[:] = imputer.fit_transform(train_prepared)","ac65f0b2":"# Split data to train and test data\nX = train_prepared.drop(['Survived'], axis=1)\ny = train_prepared['Survived']\nX_train, X_val, y_train, y_val = train_test_split(X,\n                                                  y,\n                                                  test_size = 0.2, \n                                                  random_state = 0)","6c59ae59":"model_rf = RandomForestClassifier(n_jobs = -1)\nparameters = {'n_estimators' : [20,40,60,80,100], 'min_samples_leaf' : [1,2,3,4,5]}\nscorer = make_scorer(fbeta_score, beta=0.5)\ngrid_obj = GridSearchCV(model_rf, parameters, scoring = scorer)\nstart = time() \ngrid_fit = grid_obj.fit(X_train, y_train)\nend = time() \nbest_rf = grid_fit.best_estimator_\ntime_rf = end - start\nval_predictions = (model_rf.fit(X_train, y_train)).predict(X_val)\nbest_predictions = best_rf.predict(X_val)\nimportances = model_rf.fit(X_train, y_train).feature_importances_","06e11ea3":"print('Best estimators:',best_rf)\nprint('Total time taken in seconds:', time_rf)\nprint('Feature importances:',importances)\nprint(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_val, best_predictions)))\nprint(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_val, best_predictions, beta = 0.5)))","8cff9b2b":"model_ada = AdaBoostClassifier(base_estimator = None)\nparameters = {'n_estimators': [20,40,60,80,100], 'learning_rate': [0.01, 0.1,0.5,1.0]}\nscorer = make_scorer(fbeta_score, beta=0.5)\ngrid_obj = GridSearchCV(model_ada, parameters, scoring = scorer)\nstart = time() \ngrid_fit = grid_obj.fit(X_train, y_train)\nend = time() \nbest_ada = grid_fit.best_estimator_\ntime = end - start\npredictions = (model_ada.fit(X_train, y_train).predict(X_val))\nbest_predictions = best_ada.predict(X_val)","34e14076":"print('Best estimators:',best_rf)\nprint('Total time taken in seconds:', time)\nprint(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_val, best_predictions)))\nprint(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_val, best_predictions, beta = 0.5)))","6b1d800b":"### 2. Encode Data","a22def6b":"## 1. Introduction","5081a72f":"### 3. Handle Missing Data","f2edab2a":"## 2. Data Exploration","336abb3d":"### 1. Drop Columns","d25df3f8":"### 1. Libraries","85fef135":"## 4. Model Development","2663035a":"## 3. Data Preparation","25ede893":"### 2. AdaBoost","935c13da":"### 2. Data","635fcbed":"### 4. Split Data to Train and Test","b67d32c7":"### 1. Random Forest","eff39cce":"### 3. Functions"}}