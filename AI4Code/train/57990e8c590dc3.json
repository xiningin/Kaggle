{"cell_type":{"e85c0bfa":"code","98d44340":"code","1852eb6e":"code","5b401893":"code","df823fc0":"code","96471046":"code","e9a1aba6":"code","25c00800":"code","09a187ca":"code","db7aeb34":"code","f4a87eea":"code","20f5f17f":"code","edbb053f":"code","c9a7dc39":"markdown","4d723575":"markdown","0d41f218":"markdown","c01cb8c1":"markdown","7a30b801":"markdown","63679d08":"markdown","3af0a5b6":"markdown"},"source":{"e85c0bfa":"import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport matplotlib.pyplot as plt","98d44340":"import os\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\ntrain = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv', index_col='Id')\ndataX = train[ train.columns[train.columns.str.contains('Cm$')] ]\ndatay = OrdinalEncoder().fit_transform(train[['Species']]).flatten().astype(int)\ntrainSet = lgb.Dataset(dataX, datay)\n\nparam = {'objective'       : 'multiclass',\n         'metric'          : 'multi_logloss',\n         'num_class'       : train['Species'].nunique(),\n         'num_leaves'      : 50,   # set ridiculously high for demo purpose\n         'min_data_in_leaf': 2,    # set dangerously low for demo purpose\n         'learning_rate'   : .15,\n         'num_boost_round' : 30}\nmodel = lgb.train(param, trainSet)","1852eb6e":"def grabdict(tisdict, tree_index, split_index, depth, splits, leaves):\n# recursive function to unravel nested dictionaries\n    depth += 1\n    if 'split_index' in tisdict.keys():\n        tis = tisdict.copy()\n        del tis['left_child']\n        del tis['right_child']\n        tis['tree_index'] = tree_index\n        split_index = tis['split_index']\n        splits = pd.concat([splits, pd.DataFrame(tis, index=[len(splits)])])\n        splits, leaves = grabdict(tisdict['left_child'], tree_index, split_index, depth, splits, leaves)\n        splits, leaves = grabdict(tisdict['right_child'], tree_index, split_index, depth, splits, leaves)\n    else:\n        tis = tisdict.copy()\n        tis['tree_index'] = tree_index\n        tis['split_index'] = split_index\n        tis['depth'] = depth\n        leaves = pd.concat([leaves, pd.DataFrame(tis, index=[len(leaves)])])\n    return splits, leaves\n\ndef grabtrees(model):\n# wrapper function to call grabdict\n    splits, leaves = pd.DataFrame(), pd.DataFrame()\n    tree_info = model.dump_model()['tree_info']\n    for tisdict in tree_info:\n        splits, leaves = grabdict(tisdict['tree_structure'], tisdict['tree_index'], 0, 0, splits, leaves)\n    leaves = leaves.merge(splits, left_on=['tree_index', 'split_index'], right_on=['tree_index', 'split_index'], how='left')\n    return tree_info, leaves\n\ntree_info, leaves = grabtrees(model)\nleaves   # all leaves from all trees","5b401893":"# plot the final tree\nlgb.plot_tree(model, tree_index=len(tree_info)-1, figsize=(15, 10))","df823fc0":"# find the maximum number of leaves per tree in existing model\nmax_leaves_per_tree = leaves['leaf_index'].max() + 1\n# we need to plus one because leaf indexing starts from zero\nmax_leaves_per_tree","96471046":"# now reduce num_leaves from 50 to 43, which we got from the previous cell\n# no effect expected\nparam.update({'num_leaves': 43})\nmodel_test = lgb.train(param, trainSet)\n_, leaves_test = grabtrees(model_test)\nmax_leaves_per_tree = leaves_test['leaf_index'].max() + 1\nprint(max_leaves_per_tree, (leaves_test==leaves).all().all())\n# output confirms no change indeed","e9a1aba6":"# now decrease num_leaves by 1\n# we do expect some clipping effects\nparam.update({'num_leaves': 42})\nmodel_test = lgb.train(param, trainSet)\n_, leaves_test = grabtrees(model_test)\nmax_leaves_per_tree = leaves_test['leaf_index'].max() + 1\nmax_leaves_per_tree\n# output confirms the maximum number of leaves per tree is now reduced from 43 down to 42","25c00800":"# next, try chopping further: decrease num_leaves to 3 and see what happens\nparam.update({'num_leaves': 3})\nmodel_test = lgb.train(param, trainSet)\ntree_info_test, leaves_test = grabtrees(model_test)\nmax_leaves_per_tree = leaves_test['leaf_index'].max() + 1\nmax_leaves_per_tree\n# voila, output as expected: this is how parameter *num_leaves* gives us the full handle","09a187ca":"# plot and see\n# we expect a tree far smaller than the original\nlgb.plot_tree(model_test, tree_index=len(tree_info_test)-1, figsize=(15, 10))","db7aeb34":"# find the minimum data per leaf in our original model\nmin_data_per_leaf = leaves['leaf_count'].min()\nmin_data_per_leaf","f4a87eea":"# now increase min_data_in_leaf from 2 to 3\n# we expect some clipping of our original model\nparam.update({'min_data_in_leaf': 3})\nmodel_test = lgb.train(param, trainSet)\ntree_info_test, leaves_test = grabtrees(model_test)\nmin_data_per_leaf = leaves_test['leaf_count'].min()\nmin_data_per_leaf\n# voila, the minimum data per leaf is now increased from 2 to 3 i.e. more conservative and less overfitting","20f5f17f":"# next, try increasing min_data_in_leaf further\n# we expect some drastic changes\nparam.update({'min_data_in_leaf': 30})\nmodel_test = lgb.train(param, trainSet)\ntree_info_test, leaves_test = grabtrees(model_test)\nmin_data_per_leaf = leaves_test['leaf_count'].min()\nmin_data_per_leaf\n# output as expected: this is how parameter *min_data_in_leaf* gives us the full handle","edbb053f":"# plot and see\n# we expect a tree far smaller than the original\nlgb.plot_tree(model_test, tree_index=len(tree_info_test)-1, figsize=(15, 10))","c9a7dc39":"# Put trees in Pandas DataFrame\nSkip the next cell unless you are particularly interested! The original notebook where I wrote these[](http:\/\/) functions is available from https:\/\/www.kaggle.com\/marychin\/lightgbm-trees-to-pandas-dataframe.","4d723575":"# 5. Sister notebooks: the Leaf-by-leaf series\nDecision trees: a leaf-by-leaf demo\n\nhttps:\/\/www.kaggle.com\/marychin\/decision-trees-a-leaf-by-leaf-demo\n\n**num_leaves** and **min_data_in_leaf**: a LightGBM demo (we are here)\n\nhttps:\/\/www.kaggle.com\/marychin\/num-leaves-min-data-in-leaf-a-lightgbm-demo\n\nmin_sum_hessian: a LightGBM demo\n\nhttps:\/\/www.kaggle.com\/marychin\/min-sum-hessian-a-lightgbm-demo\n\nfeature_importances split vs gain: a demo\n\nhttps:\/\/www.kaggle.com\/marychin\/feature-importances-split-vs-gain-a-demo\n\n# 6. Cheers, Kagglers & Kaggle!\nTogether we democratise learning and skills.","0d41f218":"# 3. num_leaves","c01cb8c1":"# 1. Initiation rite\nInvocations we can't go on without.","7a30b801":"# 2. Toy data & toy model \nBorrowing data from https:\/\/www.kaggle.com\/uciml\/iris.\n*Replace next cell with your own data.*","63679d08":"# 4. min_data_in_leaf","3af0a5b6":"# num_leaves & min_data_in_leaf\nIn this notebook we exercise\/explore the 2 main parameters controlling tree *bushiness*:\n* **num_leaves**: the maximum number of leaves per tree (default is 31; higher num_leaves means less conservative\/control, potentially overfitting);\n* **min_data_in_leaf**: the minimum number of data\/sample\/count per leaf (default is 20; lower min_data_in_leaf means less conservative\/control, potentially overfitting).\n\n*num_leaves* is also understood by LightGBM as: *num_leaf*, *max_leaves*, *max_leaf*.\n\n*min_data_in_leaf* is also understood by LightGBM as: *min_data_per_leaf*, *min_data*, *min_child_samples*.\n\nThis notebook draws connections between:\n* user-defined parameters: **num_leaves**, **min_data_in_leaf**;\n* attributes from *booster.dump_model()['tree_info']*: **leaf_index**, **leaf_count**;\n* *lgb.plot_tree* output."}}