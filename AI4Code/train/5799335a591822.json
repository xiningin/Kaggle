{"cell_type":{"8053c0cc":"code","2d548bdc":"code","5819b833":"code","9a713e9a":"code","bd7ccbc1":"code","1958b907":"code","ebb0ff6c":"code","4d91342e":"code","2e6e4939":"code","0f93e515":"code","d334bf02":"code","77cba214":"code","5676979b":"code","7793d0e3":"code","781e4248":"code","2d5ef80a":"code","cdc58ffc":"code","85dfcb87":"markdown","9c4eef1a":"markdown","0c74327b":"markdown","5efaa4a0":"markdown","1cef01aa":"markdown","6af4aec3":"markdown","b2b1e7f6":"markdown","6c6e8491":"markdown","1e131372":"markdown"},"source":{"8053c0cc":"import shutil\nimport re, math, os, cv2, random, warnings\nimport math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\n\nimport glob, torch, imagehash\nfrom tqdm.auto import tqdm\nfrom PIL import Image\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nGOAL = 'valid'\n#GOAL = 'train'\nseed = 2020\nseed_everything(seed)\nwarnings.filterwarnings('ignore')","2d548bdc":"base_path = '..\/input\/cassavapreprocessed'\nimages_dir = '..\/input\/cassavapreprocessed\/train_images\/train_images\/'\ngen_dir = '.\/gen_images\/'\n#working_dir = '.\/working\/'\ntrain_tfrecords_dir = '.\/train_data_tf\/'\nvalid_tfrecords_dir = '.\/valid_data_tf\/'\n\nif GOAL == 'train':\n    if not os.path.exists(train_tfrecords_dir):\n        os.mkdir(train_tfrecords_dir)\n    if not os.path.exists(gen_dir):\n        os.mkdir(gen_dir)\n        for i in range(N_FILES):\n            os.mkdir(gen_dir+f'{i}\/')\n        \n    # Resampling\n    # if 0.6, all classes will be over\/undersampled\n    # to the 0.6 of the major class\n    cut_ratio = 0.4\n\nelif GOAL == 'valid':\n    if not os.path.exists(valid_tfrecords_dir):\n        os.mkdir(valid_tfrecords_dir)","5819b833":"DIMS = (800, 600)\nN_FILES = 30\nWIDTH, HEIGHT  = DIMS\nIMG_QUALITY = 100","9a713e9a":"train = pd.read_csv(base_path + '\/merged_data.csv')\ntrain.head()\nprint('Train samples: %d' % len(train))","bd7ccbc1":"# Remove duplicates from train data\nremove_pd = ['train-cmd-2399.jpg']\ntrain = train[~train['image_id'].isin(remove_pd)]\ntrain.reset_index(inplace=True)\nprint('Train samples: %d' % len(train))","1958b907":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n                      \n    # image = tf.image.resize(image, [HEIGHT, WIDTH])\n    # image = tf.reshape(image, [HEIGHT, WIDTH, 3])\n    return image\n\ndef read_tfrecord(example):\n    TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string), \n        'target': tf.io.FixedLenFeature([], tf.int64), \n        'image_name': tf.io.FixedLenFeature([], tf.string), \n    }\n    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n    image = decode_image(example['image'])\n    target = example['target']\n    name = example['image_name']\n    return image, target, name\n\ndef load_dataset(filenames, HEIGHT, WIDTH, CHANNELS=3):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n    return dataset\n\ndef display_samples(ds, row, col):\n    ds_iter = iter(ds)\n    plt.figure(figsize=(15, int(15*row\/col)))\n    for j in range(row*col):\n        image, label, name = next(ds_iter)\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(image[0])\n        plt.title(f\"{label[0]}: {name[0].numpy().decode('utf-8')}\", fontsize=12)\n    plt.show()\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\n\n# Create TF Records\ndef _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef serialize_example(image, target, image_name):\n  feature = {\n      'image': _bytes_feature(image),\n      'target': _int64_feature(target),\n      'image_name': _bytes_feature(image_name),\n  }\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n  return example_proto.SerializeToString()","ebb0ff6c":"folds = StratifiedKFold(n_splits=N_FILES, shuffle=True, random_state=seed)\ntrain['file'] = -1\n\nfor fold_n, (train_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n    print('File: %s has %s samples' % (fold_n+1, len(val_idx)))\n    train['file'].loc[val_idx] = fold_n\n    \ndisplay(train.head())\ndisplay(train.describe())\ntrain.to_csv('train.csv', index=False)","4d91342e":"def gen_cutmix(df, label = None, file = None, probability = 0.2):\n    \n    f_uni = df.file.unique()\n    l_uni = df.label.unique()\n    print(f'DataFrame has {len(f_uni)} files!')\n    max_label = df.label.value_counts().idxmax()\n    sampling_num = max(df.label.value_counts())*cut_ratio \/\/ len(f_uni)\n    print('sampling_num is', sampling_num)\n    rows = []    \n    \n    if label:\n        df = df.loc[df.label == label]\n        \n    if file:\n        df = df.loc[df.file == file]\n    f_counter = 0    \n    for f in f_uni:\n        f_counter += 1\n        path = f\"{gen_dir}{f}\/\"\n        for l in l_uni:\n            \n            n = len(df.loc[(df.file == f) & (df.label == l)])\n            i = 0\n            while (n + i) < sampling_num:\n                \n                P = np.random.rand()                \n                if (P < probability) & (P > (1 - probability)):\n                    continue\n                \n                row = []    \n                # CHOOSE RANDOM PHOTOS OF THE SAME FILE AND CLASS\n                img1_name = df.iloc[np.random.randint(len(df))].image_id\n                img1_path = f'{images_dir}{img1_name}'\n                img1 = cv2.imread(img1_path)\n                if img1.shape[:2] != (HEIGHT, WIDTH):\n                    img1 = cv2.resize(img1, (WIDTH, HEIGHT))              \n\n                img2_name = df.iloc[np.random.randint(len(df))].image_id\n                img2_path = f'{images_dir}{img2_name}'\n                img2 = cv2.imread(img2_path)\n                if img2.shape[:2] != (HEIGHT, WIDTH):\n                    img2 = cv2.resize(img2, (WIDTH, HEIGHT))\n\n                # CHOOSE RANDOM LOCATION\n                x = (np.random.rand() * WIDTH)\/\/1\n                y = (np.random.rand() * HEIGHT)\/\/1\n                width = (WIDTH * P)\/\/1\n                height = (HEIGHT * P)\/\/1\n\n                ya = int(max(0,y-height\/\/2))\n                yb = int(max(HEIGHT,y+height\/\/2))\n                xa = int(max(0,x-width\/\/2))\n                xb = int(max(WIDTH,x+width\/\/2))\n\n                # MAKE CUTMIX IMAGE\n                one = img1[ya:yb,0:xa,:]\n                two = img2[ya:yb,xa:xb,:]\n                three = img1[ya:yb,xb:WIDTH,:]\n                middle = np.concatenate((one,two,three),axis=1)\n                img = np.concatenate((img1[0:ya,:,:],middle,img1[yb:HEIGHT,:,:]),axis=0)\n                i = i + 1    \n                img_name = f\"x{i}_{img1_name}\"\n                cv2.imwrite(f\"{path}{img_name}\",img)\n\n                row.append(img_name)\n                row.append(l)\n                row.append(f)\n                rows.append(row)\n        print(f\"{f_counter} files are done!\")\n    new_df = pd.DataFrame(rows, columns = ['image_id','label','file'])\n    return new_df","2e6e4939":"if GOAL == 'train':\n    if os.path.exists(gen_dir):\n        shutil.rmtree(gen_dir)\n    else:\n        os.mkdir(gen_dir)\n        for i in range(N_FILES):\n            os.mkdir(gen_dir+f'{i}\/')\n    imgs = gen_cutmix(train)\n    imgs.head()","0f93e515":"if GOAL == 'train':\n    _, ax = plt.subplots(10, figsize = (10,50))\n    for i in range(10):\n        img = plt.imread(f\"{gen_dir}{imgs.file[i]}\/{imgs.image_id[i]}\")\n        ax[i].imshow(img)","d334bf02":"if GOAL == 'train':\n    bl_train = pd.concat([train[['image_id','label','file']],imgs],ignore_index=True)\n    bl_train.label.value_counts()","77cba214":"if GOAL == 'train':\n    for tfrec_num in range(N_FILES):\n        print('\\nWriting TFRecord %i of %i...'%(tfrec_num, N_FILES))\n        samples = bl_train[bl_train['file'] == tfrec_num]\n        n_samples = len(samples)\n        print(f'{n_samples} samples')\n        with tf.io.TFRecordWriter(train_tfrecords_dir + 'Id_train%.2i-%i.tfrec'%(tfrec_num, n_samples)) as writer:\n            for row in samples.itertuples():\n                label = row.label\n                image_name = row.image_id\n                if image_name[0] == 'x':\n                    img_path = f'{gen_dir}{tfrec_num}\/{image_name}'\n                else:\n                    img_path = f'{images_dir}{image_name}'\n\n                img = cv2.imread(img_path)\n                img = cv2.resize(img, (WIDTH, HEIGHT))\n                img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, IMG_QUALITY))[1].tostring()\n\n                example = serialize_example(img, label, str.encode(image_name))\n                writer.write(example)\n        shutil.rmtree(f\"{gen_dir}{tfrec_num}\")","5676979b":"if os.path.exists(gen_dir):\n    shutil.rmtree(gen_dir)","7793d0e3":"if GOAL == 'valid':\n    for tfrec_num in range(N_FILES):\n        print('\\nWriting TFRecord %i of %i...'%(tfrec_num, N_FILES))\n        samples = train[train['file'] == tfrec_num]\n        n_samples = len(samples)\n        print(f'{n_samples} samples')\n        with tf.io.TFRecordWriter(valid_tfrecords_dir + 'Id_valid%.2i-%i.tfrec'%(tfrec_num, n_samples)) as writer:\n            for row in samples.itertuples():\n                label = row.label\n                image_name = row.image_id\n                img_path = f'{images_dir}{image_name}'\n\n                img = cv2.imread(img_path)\n                img = cv2.resize(img, (WIDTH, HEIGHT))\n                img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, IMG_QUALITY))[1].tostring()\n\n                example = serialize_example(img, label, str.encode(image_name))\n                writer.write(example)","781e4248":"if GOAL == 'train':\n    path = train_tfrecords_dir + \nelif GAL == 'valid':\n    path = valid_tfrecords_dir\n    \nAUTO = tf.data.experimental.AUTOTUNE\nFILENAMES = tf.io.gfile.glob(path + '*.tfrec')\nprint(f'TFRecords files: {FILENAMES}')\nprint(f'Created image samples: {count_data_items(FILENAMES)}')\n\ndisplay_samples(load_dataset(FILENAMES, WIDTH, HEIGHT).batch(1), 6, 6)","2d5ef80a":"CLASSES = ['Cassava Bacterial Blight', \n           'Cassava Brown Streak Disease', \n           'Cassava Green Mottle', \n           'Cassava Mosaic Disease', \n           'Healthy']\nif GOAL == 'train':\n    df = bl_train\nelif GOAL == 'valid':\n    df = train\n\nlabel_count = df.groupby('label', as_index=False).count()\nlabel_count.rename(columns={'image_id': 'Count', 'label': 'Label'}, inplace=True)\nlabel_count['Label'] = label_count['Label'].apply(lambda x: CLASSES[x])\n\nfig, ax = plt.subplots(1, 1, figsize=(14, 8))\nax = sns.barplot(x=label_count['Count'], y=label_count['Label'], palette='viridis')\nax.tick_params(labelsize=16)\n\nplt.show()","cdc58ffc":"for fold_n in range(folds.n_splits):\n    label_count = df[df['file'] == fold_n].groupby('label', as_index=False).count()\n    label_count.rename(columns={'image_id': 'Count', 'label': 'Label'}, inplace=True)\n    label_count['Label'] = label_count['Label'].apply(lambda x: CLASSES[x])\n\n    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n    fig.suptitle(f'File {fold_n+1}', fontsize=22)\n    ax = sns.barplot(x=label_count['Count'], y=label_count['Label'], palette='viridis')\n    ax.tick_params(labelsize=16)\n\n    plt.show()","85dfcb87":"# Helper functions","9c4eef1a":"# Cutmix function","0c74327b":"# Generate TF records","5efaa4a0":"## Labels distribution for each file","1cef01aa":"# Create TFRecords\n\n--- Hamed\/Ali --- Jan 10\n- 1) oversample with cutmix\n- 2) cut ratio 0.4\n\n--- Hamed --- Jan 02\n- 1) not copying images to working directory\n- 2) no undersampling for major class\n- 3) cut off 70%\n\n--- Hamed --- Jan 01\n- 1) shuffled images in each file\n\n--- Hamed --- Dec 31\n- 1) fixed the empty TFR files\n- 2) changed the N_FILE to 30 for HDD problem\n- 3) fixed size 600x800\n- 4) cut-off to 0.6 for HDD problem\n\n--- Hamed --- Dec 27\n- 1) added merged dataset 2019-20 from Tom's notebook\n- 2) from merged_data.csv only Images to remove: ['train-cmd-2399.jpg']\n- 3) \"N_FILES\" TFRecord files are ready (over\/undersampled with no common images between files, no duplicates)\n- 4) sizes vary! not 600*800!\n\n--- Hamed --- Dec 24\n- 1) kfold\n- 2) Random over\/undersampling on each fold (cut_ratio can be defined)\n\n\n- Based on: https:\/\/www.kaggle.com\/dimitreoliveira\/cassava-leaf-disease-stratified-tfrecords-256x256\/data\n- Discussion [thread](https:\/\/www.kaggle.com\/c\/cassava-leaf-disease-classification\/discussion\/198744)\n- Reference: [How To Create TFRecords](https:\/\/www.kaggle.com\/cdeotte\/how-to-create-tfrecords)","6af4aec3":"# Visualize created TF records\n\n## Class map\n\n```\n0: Cassava Bacterial Blight (CBB)\n1: Cassava Brown Streak Disease (CBSD)\n2: Cassava Green Mottle (CGM)\n3: Cassava Mosaic Disease (CMD)\n4: Healthy\n```","b2b1e7f6":"## Split samples into \"N_FILES\" different files","6c6e8491":"# LOAD DATA","1e131372":"# Complete set label distribution"}}