{"cell_type":{"56dd46e9":"code","cc07d5fc":"code","016bb7d0":"code","5de4c615":"code","2cb4dd7f":"code","628a8794":"code","294be77e":"code","8acbf1cc":"code","0e570a47":"code","307344b2":"code","ec5d6d0d":"code","adbbdd2d":"code","20983bc0":"code","cafad117":"code","d4e9b465":"code","97e480d7":"code","7f168727":"code","6d7a4fff":"code","b6b0590a":"code","b4a0d65d":"markdown","ebcdf553":"markdown","d8b33ddd":"markdown","92cd1cb0":"markdown"},"source":{"56dd46e9":"import numpy as np\nimport pandas as pd\n\nseed = 72\nnp.random.seed(seed)","cc07d5fc":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\ntrain_features.shape, train_targets_scored.shape","016bb7d0":"#drop rows with cp_type = ctl_vehicle\n\nkp_cl = train_features[train_features['cp_type'] != 'ctl_vehicle'].index\ntrain_features = train_features.loc[kp_cl]\ntrain_targets_scored = train_targets_scored.loc[kp_cl]\n\ntrain_features.drop(['sig_id', 'cp_type'], axis=1, inplace=True)\ntrain_targets_scored.drop('sig_id', axis=1, inplace=True)\ntest_features.drop(['sig_id'], axis=1, inplace=True)","5de4c615":"#three processing functions\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\n\ncols = train_features.columns\ncl_g = cols[cols.str.contains('g-')]\ncl_c = cols[cols.str.contains('c-')]\n\ndef tra_cp_dose(val):\n    if val == 'D1':\n        return 1.\n    elif val == 'D2':\n        return 0.\n\ndef tra_cp_time(val):\n    return val \/ 24.\n\ndef process1():\n    train = train_features.copy()\n    test = test_features.drop('cp_type', axis=1)\n    \n    train['cp_dose'] = train['cp_dose'].apply(tra_cp_dose)\n    train['cp_time'] = train['cp_time'].apply(tra_cp_time)\n    test['cp_dose'] = test['cp_dose'].apply(tra_cp_dose)\n    test['cp_time'] = test['cp_time'].apply(tra_cp_time)\n    \n    return train, test\n\ndef process2(n_c=60, n_g=480, seed=seed, scale=True):\n    pca_c = PCA(n_components=n_c, random_state=seed)\n    pca_g = PCA(n_components=n_g, random_state=seed)\n    SS = StandardScaler()\n    \n    train_pca_g = pd.DataFrame(pca_g.fit_transform(train_features[cl_g]),\n                           columns=[f'g-pca{i}' for i in range(n_g)], \n                           index=train_features.index)\n    train_pca_c = pd.DataFrame(pca_c.fit_transform(train_features[cl_c]), \n                           columns=[f'c-pca{i}' for i in range(n_c)], \n                           index=train_features.index)\n    \n    train = pd.concat([train_pca_g, train_pca_c], axis=1)\n    train['g-mean'] = np.mean(train_features[cl_g], axis=1)\n    train['g-std'] = np.std(train_features[cl_g], axis=1)\n    train['c-mean'] = np.mean(train_features[cl_c], axis=1)\n    train['c-std'] = np.std(train_features[cl_c], axis=1)\n    \n    test_pca_g = pd.DataFrame(pca_g.transform(test_features[cl_g]),\n                           columns=[f'g-pca{i}' for i in range(n_g)], \n                           index=test_features.index)\n    test_pca_c = pd.DataFrame(pca_c.fit_transform(test_features[cl_c]), \n                           columns=[f'c-pca{i}' for i in range(n_c)], \n                           index=test_features.index)\n    \n    test = pd.concat([test_pca_g, test_pca_c], axis=1)\n    test['g-mean'] = np.mean(test_features[cl_g], axis=1)\n    test['g-std'] = np.std(test_features[cl_g], axis=1)\n    test['c-mean'] = np.mean(test_features[cl_c], axis=1)\n    test['c-std'] = np.std(test_features[cl_c], axis=1)\n    if scale:\n        train = pd.DataFrame(SS.fit_transform(train), columns=train.columns, index=train.index)\n        test = pd.DataFrame(SS.transform(test), columns=test.columns, index=test.index)\n     \n    return train, test\n\ndef process3(n_c=80, n_g=660, tr=.8):\n    trn1, tst1 = process1()\n    trn2 ,tst2 = process2(n_c, n_g, scale=False)\n    \n    train = pd.concat([trn1, trn2], axis=1)\n    test = pd.concat([tst1, tst2], axis=1)\n    \n    QS = QuantileTransformer()\n    VT = VarianceThreshold(tr)\n    \n    train = pd.DataFrame(QS.fit_transform(VT.fit_transform(train)), index=train.index)\n    test = pd.DataFrame(QS.transform(VT.transform(test)), index=test.index)\n    \n    return train, test\n    ","2cb4dd7f":"#shuffling the data\nper = np.random.permutation(train_features.index)\ntarget = train_targets_scored.loc[per]\n\ntrain1, test1 = process1()\ntrain1 = train1.loc[per]\n\ntrain1","628a8794":"train2, test2 = process2()\ntrain2 = train2.loc[per]\n\ntrain2","294be77e":"train3, test3 = process3()\ntrain3 = train3.loc[per]\n\ntrain3","8acbf1cc":"#import libraries to build model\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.metrics import binary_crossentropy","0e570a47":"#define logloss, early stopping, learning rate decay, maximum epoch\np_min = 0.001\np_max = 0.999\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return binary_crossentropy(y_true, y_pred)\n\nmax_epoch = 50\nlr_decay = ReduceLROnPlateau(monitor='val_logloss', patience=1, verbose=1, min_lr=1e-8)\nearly_stop = EarlyStopping(monitor='val_logloss', patience=5, verbose=1, restore_best_weights=True)","307344b2":"#build three models\ndef build_model1(inp_shape1, inp_shape2, tar_shape):\n    inp1 = Input((inp_shape1,), name='inp1')\n    inp2 = Input((inp_shape2,), name='inp2')\n    \n    X1 = BatchNormalization()(inp1)\n    X1 = Dropout(.2)(X1)\n    X1 = Dense(1024)(X1)\n    X1 = BatchNormalization()(X1)\n    X1 = LeakyReLU()(X1)\n    X1 = Dense(512)(X1)\n    X1 = BatchNormalization()(X1)\n    X1 = Activation('relu')(X1)\n    \n    X2 = Dropout(.3)(inp2)\n    X2 = Dense(1024)(X2)\n    X2 = BatchNormalization()(X2)\n    X2 = LeakyReLU()(X2)\n    X2 = Dense(512)(X2)\n    X2 = BatchNormalization()(X2)\n    X2 = LeakyReLU()(X2)\n    \n    inp3 = Concatenate(name='concat')([X1, X2])\n    \n    X3 = Dropout(.3)(inp3)\n    X3 = Dense(2048)(X3)\n    X3 = BatchNormalization()(X3)\n    X3 = Activation('relu')(X3)\n    X3 = Dense(512)(X3)\n    X3 = BatchNormalization()(X3)\n    X3 = Activation('relu')(X3)\n    X3 = Dropout(.4)(X3)\n    X3 = Dense(inp_shape1)(X3)\n    X3 = BatchNormalization()(X3)\n    X3 = Activation('relu')(X3)\n    \n    inp4 = Add(name='add')([X3, inp1])\n    \n    out = Dropout(.3)(inp4)\n    out = Dense(1024)(out)\n    out = BatchNormalization()(out)\n    out = LeakyReLU()(out)\n    out = Dropout(.4)(out)\n    out = Dense(512)(out)\n    out = BatchNormalization()(out)\n    out = LeakyReLU()(out)\n    out = Dropout(.5)(out)\n    out = Dense(tar_shape)(out)\n    out = Activation('sigmoid')(out)\n    \n    model = Model(inputs=[inp1, inp2], outputs=out, name='model1')\n    return model\n\n\ndef build_model2(inp_shape, tar_shape):\n    inp = Input((inp_shape,), name='inp')\n    \n    out = BatchNormalization()(inp)\n    out = Dropout(.3)(out)\n    out = Dense(1024)(out)\n    out = BatchNormalization()(out)\n    out = Activation('elu')(out)\n    out = Dropout(.3)(out)\n    out = Dense(512)(out)\n    out = BatchNormalization()(out)\n    out = Activation('elu')(out)\n    out = Dropout(.3)(out)\n    out = Dense(256)(out)\n    out = BatchNormalization()(out)\n    out = Activation('elu')(out)\n    out = Dropout(.3)(out)\n    out = Dense(tar_shape)(out)\n    out = BatchNormalization()(out)\n    out = Activation('sigmoid')(out)\n    \n    model = Model(inputs=inp, outputs=out, name='model2')\n    return model\n\ndef build_model3(inp_shape, tar_shape):\n    inp = Input((inp_shape,), name='inp')\n    \n    out = BatchNormalization()(inp)\n    out = Dropout(.3)(out)\n    \n    out = Dense(2048)(out)\n    out = BatchNormalization()(out)\n    out = Activation('elu')(out)\n    \n    out = Dropout(.5)(out)\n    \n    out = Dense(256)(out)\n    out = BatchNormalization()(out)\n    out = Activation('elu')(out)\n    \n    out = Dropout(.3)(out)\n    \n    out = Dense(1024)(out)\n    out = BatchNormalization()(out)\n    out = Activation('elu')(out)\n    \n    out = Dropout(.5)(out)\n    \n    out = Dense(tar_shape)(out)\n    out = BatchNormalization()(out)\n    out = Activation('sigmoid')(out)\n    \n    model = Model(inputs=inp, outputs=out, name='model3')\n    return model","ec5d6d0d":"model1 = build_model1(train1.shape[1], train2.shape[1], target.shape[1])\nmodel1.compile(optimizer=Adam(3 * 1e-3), loss='binary_crossentropy', metrics=[logloss])\nmodel1.summary()","adbbdd2d":"his1 = model1.fit([train1, train2], target, batch_size=64, epochs=max_epoch, validation_split=.2,\n                  shuffle=False, callbacks=[lr_decay, early_stop], verbose=2)","20983bc0":"model2 = build_model2(train3.shape[1], target.shape[1])\nmodel2.compile(optimizer=Adam(1e-1), loss='binary_crossentropy', metrics=[logloss])\nmodel2.summary()","cafad117":"his2 = model2.fit(train3, target, batch_size=64, epochs=max_epoch, validation_split=.2,\n                  shuffle=False, callbacks=[lr_decay, early_stop], verbose=2)","d4e9b465":"model3 = build_model3(train3.shape[1], target.shape[1])\nmodel3.compile(optimizer=Adam(.1), loss='binary_crossentropy', metrics=[logloss])\nmodel3.summary()","97e480d7":"his3 = model3.fit(train3, target, batch_size=128, epochs=max_epoch, validation_split=.2,\n                  shuffle=False, callbacks=[lr_decay, early_stop], verbose=2)","7f168727":"pred1 = model1.predict([test1, test2])\npred2 = model2.predict(test3)\npred3 = model3.predict(test3)\n\npred = (pred1 + pred2 + pred3) \/ 3.0\npred.shape","6d7a4fff":"out = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nout.iloc[:,1:] = np.clip(pred, p_min, p_max)\n\nout.iloc[test_features['cp_type'] == 'ctl_vehicle', 1:] = 0\nout","b6b0590a":"out.to_csv('submission.csv', index=False)","b4a0d65d":"# Load Data","ebcdf553":"# Feature Engineering and Preprocessing","d8b33ddd":"# Build Model","92cd1cb0":"# Making Predictions"}}