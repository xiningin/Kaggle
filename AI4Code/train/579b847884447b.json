{"cell_type":{"a2bf618e":"code","cb045c47":"code","1e08d431":"code","02a3a729":"code","e1360f67":"code","8f7f56cd":"code","ddb9a828":"code","ac9a34bc":"code","616b99a4":"code","008a9df2":"code","69ce078c":"code","7b9781da":"code","639bb551":"code","4afc9bd0":"code","32209199":"code","a34fa64b":"code","a55d7172":"code","88ec6f33":"code","9d73d7f4":"code","86e47877":"code","6278061e":"code","ae04340d":"code","02598af8":"code","1ab2055e":"markdown","4a4f0bc4":"markdown","9d4c7270":"markdown","9ce6e742":"markdown","520f775c":"markdown","a8da4e87":"markdown","5ab48333":"markdown","70c264e1":"markdown","020f1485":"markdown","f2cc84da":"markdown","1f33de3e":"markdown","5a677baa":"markdown","78a9d6af":"markdown"},"source":{"a2bf618e":"from pathlib import Path \nimport arviz as az\nimport numpy as np\nimport pandas as pd\nimport pymc3 as pm\nimport seaborn as sns\nimport theano.tensor as tt\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.patches as mpatches\nimport cufflinks as cf\nimport matplotlib.pyplot as plt","cb045c47":"plt.rcParams[\"figure.figsize\"] = (16, 8)\ncf.set_config_file(theme='pearl', offline=True)","1e08d431":"path = Path(\"..\/input\/bayesian-methods-for-hackers\/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers\/\")\nchallenger = (pd.read_csv(path\/'Chapter2_MorePyMC'\/'data'\/'challenger_data.csv', parse_dates=['Date'], index_col='Date')\n              .sort_index())\nchallenger.tail()","02a3a729":"## Cleaning Data\n\ndf = challenger.dropna(subset=['Damage Incident'])\ndf = df.drop(index=pd.to_datetime('1986-01-28')) # Challenger Accident row\ndf['Damage Incident'] = df['Damage Incident'].astype(int)","e1360f67":"df.plot.scatter(x='Temperature', y='Damage Incident', s=200);","8f7f56cd":"p_threshold = 0.5\nfeature = ['Temperature']","ddb9a828":"with pm.Model() as log_reg_model:\n\n    X_transformed = StandardScaler().fit_transform(df[feature].values)\n\n#     X = pm.Data('X', df[feature].values)\n    X = pm.Data('X', X_transformed)\n    y = pm.Data('y', df['Damage Incident'].values)\n\n    # priors\n    \u03b1 = pm.Normal('\u03b1', sigma=5)\n    \u03b2 = pm.Normal('\u03b2', sigma=5, shape=X.shape.eval()[1])\n    \n    # decision boundary\n    lo_threshold = np.log(p_threshold\/(1 - p_threshold))\n\n    if X.ndim > 1 and X.shape.eval()[1] > 1:\n        \n        # (-2, 2) range taken because x are standardised to (0, 1)\n        x_temp = np.expand_dims(np.linspace(-2, 2, 100), axis=1) \n        X_temp = np.repeat(x_temp, repeats=X.shape.eval()[1] - 1, axis=1)\n        \u03a3 = pm.math.matrix_dot(X_temp, \u03b2[:-1])\n    else:\n        \u03a3 = 0\n        \n    # values of the decision boundary for the last feature \n    x_k = (lo_threshold - \u03b1 - \u03a3)\/\u03b2[-1] \n#     X_bd = np.stack([X_temp, x_k], axis=1)\n    x_k = pm.Deterministic('x_k', x_k)\n\n    # log-odds\n    \u03c0 = pm.Deterministic('\u03c0', \u03b1 + pm.math.dot(X, \u03b2))\n\n    # logistic transformation\n    \u00df = pm.Deterministic('\u00df', pm.math.sigmoid(\u03c0))\n\n    # liklihood\n    pm.Bernoulli('liklihood', p=\u00df, observed=y)\n    pm.model_to_graphviz().save('log_reg_model_challenger.png')\n    \npm.model_to_graphviz(log_reg_model)","ac9a34bc":"with log_reg_model:\n    trace = pm.sample()\n    ppc = pm.sample_posterior_predictive(trace)","616b99a4":"sampled_vars = ['\u03b2', '\u03b1']\nlog_reg = az.from_pymc3(model=log_reg_model, trace=trace, posterior_predictive=ppc)","008a9df2":"az.plot_trace(log_reg, var_names=sampled_vars);","69ce078c":"# for legend\nhandles = {}\n\n# Data Points\nx = log_reg.constant_data['X'].data.squeeze(-1)\ny = log_reg.constant_data['y'].data\nscatter = plt.scatter(x=x, y=y, c=y, s=150);\nhandles.update(dict(zip([0, 1], scatter.legend_elements()[0])))\n\n\n# Decison Boundary\n\u03b4_hpd = az.hpd(log_reg.posterior['x_k'], credible_interval=0.95).mean(axis=0);\nplt.fill_betweenx(y=[0, 1], x1=\u03b4_hpd[0], x2=\u03b4_hpd[1], color='C1', alpha=0.2);\nhandles['Decision Boundary Spread'] = mpatches.Patch(color=f'C1', alpha=0.2)\n\n# Mean Decision Boundary\nline = plt.vlines(x=log_reg.posterior['x_k'].mean(), ymin=0.0, ymax=1.0, color='C1');\n# handles['Mean Decision Boundary'] = mpatches.Patch(color=f'C1')\nhandles.update({'Mean Decision Boundary': line})\n\n\n\n# Labelling\nplt.xlabel(f'{feature[0]} Z Score');\nplt.ylabel('Damage Incident');\nplt.title('Decision Boundary');\nplt.legend(handles=list(handles.values()), labels=list(handles.keys()));\n# plt.legend()","7b9781da":"# az.plot_hpd(x=x, y=log_reg.posterior['\u00df'], credible_interval=0.95, fill_kwargs={'alpha':0.2}, color='C1');","639bb551":"iris = pd.read_csv('..\/input\/iris\/Iris.csv', index_col='Id')\niris.head()","4afc9bd0":"# Choosing only two categories\n\ndf = iris.query(\"Species in ['Iris-setosa', 'Iris-versicolor']\").reset_index(drop=True)\ndf.head()","32209199":"idx, group_names = df['Species'].factorize()","a34fa64b":"features = df.set_index('Species').columns.to_list()\nfeatures","a55d7172":"df.boxplot(by='Species');","88ec6f33":"p_threshold = 0.5\nfeature = ['SepalLengthCm', 'SepalWidthCm']","9d73d7f4":"## SAME MODEL AS EXAMPLE I\n\nwith pm.Model() as log_reg_model:\n\n    X_transformed = StandardScaler().fit_transform(df[feature].values)\n\n#     X = pm.Data('X', df[feature].values)\n    X = pm.Data('X', X_transformed)\n    y = pm.Data('y', idx)\n\n    # priors\n    \u03b1 = pm.Normal('\u03b1', sigma=5)\n    \u03b2 = pm.Normal('\u03b2', sigma=5, shape=X.shape.eval()[1])\n    \n    # decision boundary\n    lo_threshold = np.log(p_threshold\/(1 - p_threshold))\n\n    if X.ndim > 1 and X.shape.eval()[1] > 1:\n        \n        # (-2, 2) range taken because x are standardised to (0, 1)\n        x_temp = np.expand_dims(np.linspace(-2, 2, 100), axis=1) \n        X_temp = np.repeat(x_temp, repeats=X.shape.eval()[1] - 1, axis=1)\n        \u03a3 = pm.math.matrix_dot(X_temp, \u03b2[:-1])\n    else:\n        \u03a3 = 0\n        \n    # values of the decision boundary for the last feature \n    x_k = (lo_threshold - \u03b1 - \u03a3)\/\u03b2[-1] \n#     X_bd = np.stack([X_temp, x_k], axis=1)\n    x_k = pm.Deterministic('x_k', x_k)\n\n    # log-odds\n    \u03c0 = pm.Deterministic('\u03c0', \u03b1 + pm.math.dot(X, \u03b2))\n\n    # logistic transformation\n    \u00df = pm.Deterministic('\u00df', pm.math.sigmoid(\u03c0))\n\n    # liklihood\n    pm.Bernoulli('liklihood', p=\u00df, observed=y)\n    pm.model_to_graphviz().save('log_reg_model_iris.png')\n    \npm.model_to_graphviz(log_reg_model)","86e47877":"with log_reg_model:\n    trace = pm.sample()\n    ppc = pm.sample_posterior_predictive(trace)","6278061e":"sampled_vars = ['\u03b2', '\u03b1']\nlog_reg = az.from_pymc3(model=log_reg_model, trace=trace, posterior_predictive=ppc)","ae04340d":"az.plot_trace(log_reg, var_names=sampled_vars);","02598af8":"# for legend\nhandles = {}\n\n# Decison Boundary\nhpd = az.plot_hpd(x=np.linspace(-2, 2, 100), y=log_reg.posterior['x_k'], \n                  credible_interval=0.95, fill_kwargs={'alpha':0.2}, color='C1');\n\nhandles['Decision Boundary Spread'] = mpatches.Patch(color=f'C1', alpha=0.2)\n\n# Mean Decison Boundary\nmean_x_k = log_reg.posterior['x_k'].mean(dim=['chain', 'draw'])\nline = plt.plot(np.linspace(-2, 2, 100), mean_x_k, c='C1')\nhandles['Mean Decision Boundary'] = line[0]\n\n\n# Data Points\nx1, x2 = np.split(log_reg.constant_data['X'].data, axis=1, indices_or_sections=len(feature))\nscatter = plt.scatter(x=x1, y=x2, c=idx);\nhandles.update(dict(zip(group_names.to_list(), scatter.legend_elements()[0])))\n\n# Labelling\nplt.xlabel(f'{feature[0]} Z Score');\nplt.ylabel(f'{feature[1]} Z Score');\nplt.title('Decision Boundary');\nplt.legend(handles=list(handles.values()), labels=list(handles.keys()));","1ab2055e":"# Example II - 2D Data","4a4f0bc4":"## Analysis","9d4c7270":"## Model","9ce6e742":"## Model","520f775c":"# Theory","a8da4e87":"## Note\nThe model presented above can be used for multi-dimensional data as is. \nMajor changes might be only required for plotting the data and its decision boundary.","5ab48333":"# Experiments\/ To-Do's\n\n1. Run the same experiment on non-standardrised data (original)\n2. Run the same experiment on mean-centered data (using the transformer `StandardScaler(with_std=False)`)\n3. Try classying either one of the 3 classes in the original dataset (`iris`) in one-versus-rest (OVR) fashion.","70c264e1":"## Analysis","020f1485":"# Example I - 1D Data","f2cc84da":"**Note**: Requires some basic familiarity with logistic regression.\n\n### Recap \nA logistic regression model can be expressed as:\n$$\nP(y = 1) = p = \\frac{1}{1 + e^{-z}}\n$$\n\nWhere $z$ is the log-odds given the features $X$\n$$\nz = \\alpha + X\\beta\n$$\n\n### Decision Boundary\nA decision boundary is a hyperplane in the feature space which separated the two classes.\n\n![db](https:\/\/i.stack.imgur.com\/V0kf4.png)\n\nIt can be calculated simply by setting a classification threshold and doing some matrix multiplication.\n\nThe logistic equation can be inverse tranformed to the logit equation:\n$$\nz = ln(\\frac{p}{1-p})\n$$\n\n$\\implies \\alpha + X\\beta = ln(\\frac{p}{1-p})$\n\n$\\implies X\\beta = ln(\\frac{p}{1-p}) - \\alpha$\n\n$\\implies \\sum_{i=1}^{k} \\beta_{i}*x_{i} = ln(\\frac{p}{1-p}) - \\alpha$\n\n$\\implies \\beta_{k}*x_{k} = ln(\\frac{p}{1-p}) - \\alpha - \\sum_{i=1}^{k-1} \\beta_{i}*x_{i}$\n\n$\\implies x_{k} = \\frac{ln(\\frac{p}{1-p}) - \\alpha - \\sum_{i=1}^{k-1} \\beta_{i}*x_{i}}{\\beta_{k}}$\n\nwhere,\n\n* $p$ is the threshold probablity.\n* $\\alpha$ and $\\beta$ are the learned parameters\n* $k$ is the number of features\n\nThis gives us the equation of a $k$ dimension hyperplane. \n\nWe can obtain the values of the $k^{th}$ dimension, i.e. $x_{k}$ by iterating over all the possible (realistic) values of each feature, $x_{i}$.\n\nThis is what I have implemented in the code below.","1f33de3e":"## Reading Data","5a677baa":"## Objective\nTo implement a method for calculating decision boundary of a logistic classifier trained on multi-dimensional data.","78a9d6af":"## Reading Data"}}