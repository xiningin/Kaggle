{"cell_type":{"4f4ec5ba":"code","5632fd07":"code","2c4124b2":"code","8964aff5":"code","433c1214":"code","b977c51f":"code","61a64759":"code","63b9597e":"code","ac78f5f6":"code","bd458118":"code","2adac4e6":"code","0d8045c0":"code","62c136fd":"markdown","b7892343":"markdown","9c413706":"markdown","9dfe03ad":"markdown","ec1675dc":"markdown","dac2a1b9":"markdown","f27ce18f":"markdown","5ebf72ce":"markdown","00d2db29":"markdown","2d3bc3d6":"markdown","b965f921":"markdown","7732d5fa":"markdown","9def20f6":"markdown"},"source":{"4f4ec5ba":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndataset1 = pd.read_csv('..\/input\/Social_Network_Ads.csv')\nx = dataset1.iloc[:, [2,3]].values\ny = dataset1.iloc[:, 4].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = sc_x.fit_transform(X_train)\nX_test = sc_x.transform(X_test)\n# Y is not scaled, because it contians categorical values.","5632fd07":"from sklearn.linear_model import LogisticRegression #Logistic regression  is still a linear model\n\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, Y_train)\ny_pred = classifier.predict(X_test)\ny_pred, Y_test","2c4124b2":"#Making the Confusion Matrix --> to assess accuracy\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\ncm","8964aff5":"#65 + 24 = 89 correct results, 3+8 = 11 incorrect results.\n\n#Visualizing the Training Set results\nplt.figure(1)\nfrom matplotlib.colors import ListedColormap\nX_set, Y_set = X_train, Y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() -1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() -1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(Y_set)):\n    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j , 1], c = ListedColormap(('red', 'green'))(i), label = j, linewidths = 1, edgecolor = 'black')\nplt.title('Prediction boundary and training examples plotted')\nplt.legend()\nplt.show()","433c1214":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndataset1 = pd.read_csv('..\/input\/Social_Network_Ads.csv')\nx = dataset1.iloc[:, [2,3]].values\ny = dataset1.iloc[:, 4].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = sc_x.fit_transform(X_train)\nX_test = sc_x.transform(X_test)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) #n_neighbors parameter can be tuned, but will learn later.\n#minkowsky - 2 --> Euclidean distance\nclassifier.fit(X_train, Y_train)\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)","b977c51f":"#visualizing the result\n\nplt.figure (2)\nfrom matplotlib.colors import ListedColormap\nX_set, Y_set = X_train, Y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() -1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() -1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(Y_set)):\n    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j , 1], c = ListedColormap(('red', 'green'))(i), label = j, linewidths = 1, edgecolor = 'black')\nplt.title('Prediction boundary and training examples plotted (KNN)')\nplt.legend()\nplt.show()","61a64759":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndataset1 = pd.read_csv('..\/input\/Social_Network_Ads.csv')\nx = dataset1.iloc[:, [2,3]].values\ny = dataset1.iloc[:, 4].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = sc_x.fit_transform(X_train)\nX_test = sc_x.transform(X_test)\n\nfrom sklearn.svm import SVC #Support vecture classifier\nclassifier = SVC(kernel = 'linear', random_state = 0) #other options for kernel: 'poly', 'rbf' (gaussean), 'sigmoid', 'precomputed'\nclassifier.fit(X_train, Y_train)\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\ncm","63b9597e":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndataset1 = pd.read_csv('..\/input\/Social_Network_Ads.csv')\nx = dataset1.iloc[:, [2,3]].values\ny = dataset1.iloc[:, 4].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = sc_x.fit_transform(X_train)\nX_test = sc_x.transform(X_test)\n\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, Y_train)\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\nprint(cm)\n\nplt.figure (2)\nfrom matplotlib.colors import ListedColormap\nX_set, Y_set = X_train, Y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() -1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() -1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(Y_set)):\n    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j , 1], c = ListedColormap(('red', 'green'))(i), label = j, linewidths = 1, edgecolor = 'black')\nplt.title('Prediction boundary and training examples plotted (rbf SVM Kernel)')\nplt.legend()\nplt.show()","ac78f5f6":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndataset1 = pd.read_csv('..\/input\/Social_Network_Ads.csv')\nx = dataset1.iloc[:, [2,3]].values\ny = dataset1.iloc[:, 4].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = sc_x.fit_transform(X_train)\nX_test = sc_x.transform(X_test)\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, Y_train)\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\n\nplt.figure (2)\nfrom matplotlib.colors import ListedColormap\nX_set, Y_set = X_train, Y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() -1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() -1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(Y_set)):\n    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j , 1], c = ListedColormap(('red', 'green'))(i), label = j, linewidths = 1, edgecolor = 'black')\nplt.title('Prediction boundary and training examples plotted (Naive Bayes)')\nplt.legend()\nplt.show()\n","bd458118":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndataset1 = pd.read_csv('..\/input\/Social_Network_Ads.csv')\nx = dataset1.iloc[:, [2,3]].values\ny = dataset1.iloc[:, 4].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = sc_x.fit_transform(X_train)\nX_test = sc_x.transform(X_test)\n\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, Y_train)\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\n\nplt.figure (2)\nfrom matplotlib.colors import ListedColormap\nX_set, Y_set = X_train, Y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() -1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() -1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(Y_set)):\n    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j , 1], c = ListedColormap(('red', 'green'))(i), label = j, linewidths = 1, edgecolor = 'black')\nplt.title('Prediction boundary and training examples plotted (DecisionTree Classification)')\nplt.legend()\nplt.show()","2adac4e6":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndataset1 = pd.read_csv('..\/input\/Social_Network_Ads.csv')\nx = dataset1.iloc[:, [2,3]].values\ny = dataset1.iloc[:, 4].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = sc_x.fit_transform(X_train)\nX_test = sc_x.transform(X_test)\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, Y_train)\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\n\nplt.figure (2)\nfrom matplotlib.colors import ListedColormap\nX_set, Y_set = X_train, Y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() -1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() -1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(Y_set)):\n    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j , 1], c = ListedColormap(('red', 'green'))(i), label = j, linewidths = 1, edgecolor = 'black')\nplt.title('Prediction boundary and training examples plotted (Random Forest Classification)')\nplt.legend()\nplt.show()","0d8045c0":"cm","62c136fd":"# Support Vector Machine (SVM)\n\n![i3](https:\/\/i.imgur.com\/HzHqaGA.png)\n\nSupport vectores are those who do not have the clearest characteristics of each category. This makes SVM special. For example, support vectors might be an orange that looks like an apple, or an apple that resembles an orange, if the problem was to classify between oranges and apples.","b7892343":"# Random Forest Classification\n\nThis is a team of tree models, so this will be more powerful.\nEmsemble learning: combining many machine learning models to produce one better result. ","9c413706":"# K-nearest Neighbor (KNN)\n\n![i2](https:\/\/i.imgur.com\/mA3sq9N.png) \n\nIn above case, 5 neighbors were chosen.\n\nStep 1: Choose the number K of neighbors\nStep 2: Take the K nearest neighbors of the new data point, according to the Euclidean distance\nStep 3: Among these K neighbors, count the number of data points in each category\nStep 4: Assign the new data point to the category where I counted the most neighbors.","9dfe03ad":"Prediction boundary determines the boundary between categories. \nAs above model was a linear classifier, the prediction boundary can only be a straight line. Non-linear boundaries can be created as well.","ec1675dc":"# Decision Tree Classification","dac2a1b9":"# Logistic Regression \nUse the same library that was used in Regression notebook.\n","f27ce18f":"# Naive Bayes Classification algorithm","5ebf72ce":"Created by: Sangwook Cheon\n\nDate: Dec 23, 2018\n\nThis is step-by-step guide to Classification using scikit-learn, which I created for reference. I added some useful notes along the way to clarify things. I am excited to move onto more advanced concepts, such as deep learning, using frameworks like Keras and Tensorflow.\nThis notebook's content is from A-Z Datascience course, and I hope this will be useful to those who want to review materials covered, or anyone who wants to learn the basics of classification.\n\n# Content:\n\n### 1. Logistic Regression\n### 2. K-nearest Neighbors (KNN)\n### 3. Support Vector Machine (SVM)\n### 4. Kernel SVM\n### 5. Naive Bayes algorithm\n### 6. Decision Tree Classification\n### 7. Random Forest Classification\n### 8. Evaluating Classification models\n\n__________\n_________","00d2db29":"Here, 63 and 28 are correct values. 5 is the false positive values, and 4 is the false nagative values.\n\nAccuracy: (91\/100) = 0.91\n\n## Cumulative Accuracy Profile (CAP)\nthis is used to analyze the performance of the model\n![i6](https:\/\/i.imgur.com\/PFXbIfS.png)\nIf the area between the performance curve and a straight line (generated by a random sample) is higher, it shows that the model is better. If it approaches the perfect line (ideal line), it is better.\n\n#### How to quantify CAP\nFirst option: Area between the trained model and the random model line\/Area between the random model line and perfect line. Closer to 1, better the model\n\nSecond option: take the halfway point of the x-axis, then see what that value projects to (the corresponding y_value). If the y_value is:\n\n* < 60% ---> Rubbish\n* 60% < y < 70% --> Poor\n* 70% < y < 80% ---> good\n* 80% < y < 90% ---> Very good\n* y > 90% --> too good","2d3bc3d6":"This is a linear classification. Kernel SVM should be used for better fitting.\n\n# Kernel SVM\n### Mapping data to a higher dimension to make non-linearly-separable to separable\nThis is possible, by using certain mapping functions, such as 1d line to 2d parabola, but it requries a lot of computing and processing power. Therefore, a different approach should be explored: \n\n### Gaussian RBF Kernel\nThis allows transformation of each data point without actually mapping it into a higher dimension. Using the specific formula, computer only computes values while not creating an entirely new higher-dimensional array, which saves processing power. A landmark should be placed within the data, and distance from each point to the landmark is used to determine its position. If it is close to the landmark, it has the value greater than 0. The maximum value is 1, which would be the landmark itself. \n\nFormula: ![i4](https:\/\/i.imgur.com\/Moe3GSG.png) \n\n#### Different types of kernels\n* Gaussian RBF Kernel\n* Sigmoid Kernel\n* Polynomial Kernel --> popular\n","b965f921":"### Interpreting the result\n\nThis seems to have the characteristics of overfitting. ","7732d5fa":"# Naive Bayes\n\n## Bayes Theorem\nTerms:\n\nP(Something | condition) = 50% --> Probability of something happening given a condition is 50%\n![i5](https:\/\/i.imgur.com\/UX47tMA.png)","9def20f6":"Testing the classfier on test set is needed to detect overfitting.\n\n# Evaluating Classification models \nFalse Negative Errors (Type II error) : When the model predicted a value that is rounded to be a higher category, but when the result is actually lower category. \nFalse Positive Errors (Type I error): When the model predicted a value that is rounded to be a lower category, but when the result is actually the higher category. #When the model says it will happen, but it actually doesn't happen.\n\n## Confusion matrix\n"}}