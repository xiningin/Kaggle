{"cell_type":{"369fc86d":"code","ac7a3c8f":"code","76e56361":"code","940a14d7":"code","94c3c569":"code","c5e5063a":"code","43b98515":"code","d1267f2d":"code","ca0a1615":"code","89ca844e":"code","68868f33":"code","b232e8c2":"code","f752aa0e":"code","4486f780":"code","bf0a4cbe":"code","e3e6fa88":"code","7e4dbd8b":"code","207f35e0":"code","1a1c8fa5":"code","a5ec938c":"code","89f93662":"code","241329f2":"code","0df3ec9d":"code","f5d9c193":"code","286a3b72":"code","e4121e4b":"code","28d128e8":"code","c2dfe9e2":"code","ddf216cd":"code","bdaa4a4b":"code","4ec3d234":"code","c20d9de8":"code","ec93743f":"code","85233202":"code","b8a42592":"code","eec7be42":"code","cee3f50f":"markdown","40e02448":"markdown","25bac423":"markdown"},"source":{"369fc86d":"# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\n# SeaBorn : librairie de graphiques avanc\u00e9s\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import model_selection\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import datasets\n\nfrom tensorflow.keras.preprocessing import sequence\nfrom sklearn.datasets import fetch_20newsgroups\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, MaxPool1D, Dropout, SimpleRNN, LSTM\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.preprocessing import sequence\nimport numpy as np\nimport string\nimport re","ac7a3c8f":"df = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\n\ndf.sentiment = df.sentiment.map({ 'negative': 0, 'positive': 1 })\n\ntext = df.review.tolist()\nlabel = df.sentiment.tolist()","76e56361":"X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=0.2, random_state=1)","940a14d7":"translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space","94c3c569":"X_train_clean = []\nX_test_clean = []\nclean = re.compile(r'<[^>]+>')\nfor i, test in enumerate(X_train):\n    tmp_text = test.lower()\n    tmp_text = tmp_text.replace('\\n', '')\n    tmp_text = clean.sub('', tmp_text)\n    tmp_text = tmp_text.translate(translator)\n    X_train_clean.append(tmp_text)\n\nfor i, test in enumerate(X_test):\n    tmp_text = test.lower()\n    tmp_text = tmp_text.replace('\\n', '')\n    tmp_text = clean.sub('', tmp_text)\n    tmp_text = tmp_text.translate(translator)\n    X_test_clean.append(tmp_text)\n\nX_train_clean = np.array(X_train_clean)\nX_test_clean = np.array(X_test_clean)\n\nX_train = X_train_clean\nX_test = X_test_clean","c5e5063a":"X_train.shape","43b98515":"top_words = 40000\ntokenizer = Tokenizer(num_words=top_words)\ntokenizer.fit_on_texts(X_train)\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","d1267f2d":"max_words = 100\nX_train = sequence.pad_sequences(X_train, maxlen=max_words, padding='post')\nX_test = sequence.pad_sequences(X_test, maxlen=max_words, padding='post')","ca0a1615":"X_train[0]","89ca844e":"y_train = np.array(y_train)\ny_test = np.array(y_test)","68868f33":"X_train.shape","b232e8c2":"model = Sequential()\nmodel.add(Embedding(20000,32, input_length=100))\nmodel.add(Conv1D(256, 3, activation='relu', padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv1D(128, 3, activation='relu', padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(250, activation='relu'))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","f752aa0e":"model.summary()","4486f780":"model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=100, batch_size=128, verbose=2)","bf0a4cbe":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nprint(stopwords.words('english'))","e3e6fa88":"X_train = X_train_clean\nX_test = X_test_clean\n\nstop_words = set(stopwords.words('english'))\n\nX_train_stopwords = []\nX_test_stopwords = []\n\nfor i, text in enumerate(X_train):\n    word_tokens = word_tokenize(text)\n    filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n    filtered_text = []\n    for w in word_tokens:\n        if w not in stop_words:\n            filtered_text.append(w)\n    string = ' '.join(filtered_text)\n    X_train_stopwords.append(string)\n    \nfor i, text in enumerate(X_test):\n    word_tokens = word_tokenize(text)\n    filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n    filtered_text = []\n    for w in word_tokens:\n        if w not in stop_words:\n            filtered_text.append(w)\n    string = ' '.join(filtered_text)\n    X_test_stopwords.append(string)","7e4dbd8b":"X_train_stopwords[0]","207f35e0":"X_train = np.array(X_train_stopwords)\nX_test = np.array(X_test_stopwords)","1a1c8fa5":"X_train.shape","a5ec938c":"top_words = 40000\ntokenizer = Tokenizer(num_words=top_words)\ntokenizer.fit_on_texts(X_train)\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","89f93662":"max_words = 100\nX_train = sequence.pad_sequences(X_train, maxlen=max_words, padding='post')\nX_test = sequence.pad_sequences(X_test, maxlen=max_words, padding='post')","241329f2":"model = Sequential()\nmodel.add(Embedding(20000,32, input_length=100))\nmodel.add(Conv1D(256, 3, activation='relu', input_shape=(178, 1), padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv1D(128, 3, activation='relu', padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64, return_sequences=True))\nmodel.add(LSTM(32))\nmodel.add(Flatten())\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","0df3ec9d":"model.summary()","f5d9c193":"model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=100, batch_size=128, verbose=2)","286a3b72":"from nltk.stem import PorterStemmer","e4121e4b":"ps = PorterStemmer()","28d128e8":"X_train = X_train_stopwords\nX_test = X_test_stopwords","c2dfe9e2":"X_train","ddf216cd":"X_train_stemming = []\nX_test_stemming = []\nword_list = []\nfor text in X_train:\n    stem_text = []\n    word_list = text.split()\n    for word in word_list:\n        stem_text.append(ps.stem(word))\n    string = \" \".join(stem_text)\n    X_train_stemming.append(string)\nfor text in X_test:\n    stem_text = []\n    word_list = text.split()\n    for word in word_list:\n        stem_text.append(ps.stem(word))\n    string = \" \".join(stem_text)\n    X_test_stemming.append(string)","bdaa4a4b":"X_train = np.array(X_train_stemming)\nX_test = np.array(X_test_stemming)","4ec3d234":"X_train","c20d9de8":"top_words = 40000\ntokenizer = Tokenizer(num_words=top_words)\ntokenizer.fit_on_texts(X_train)\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","ec93743f":"max_words = 100\nX_train = sequence.pad_sequences(X_train, maxlen=max_words, padding='post')\nX_test = sequence.pad_sequences(X_test, maxlen=max_words, padding='post')","85233202":"model = Sequential()\nmodel.add(Embedding(20000,32, input_length=100))\nmodel.add(Conv1D(256, 3, activation='relu', input_shape=(178, 1), padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv1D(128, 3, activation='relu', padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64, return_sequences=True))\nmodel.add(LSTM(32))\nmodel.add(Flatten())\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","b8a42592":"model.summary()","eec7be42":"model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=100, batch_size=128, verbose=2)","cee3f50f":"## Conv1D","40e02448":"## Conv1D et LTSM sans Stopwords","25bac423":"## Stemming"}}