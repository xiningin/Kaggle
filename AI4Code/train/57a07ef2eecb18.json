{"cell_type":{"ea330f84":"code","5b34bf47":"code","e7c07ab8":"code","197fedb7":"code","5c4c67af":"code","c49b1587":"code","15e61092":"code","5a5608b8":"code","ad4f2fc2":"code","dbedd846":"code","8af0f68d":"code","9e166179":"code","be0478a5":"code","ba917430":"code","bb179ce3":"code","2dc49193":"code","ce6db66c":"code","808b9bd8":"code","f31a0e23":"code","20ea234c":"code","fd9ae767":"code","e9099179":"code","7d32d39b":"code","377b6693":"code","062fa62a":"code","0dd3bb87":"code","8b0b1888":"code","47c1a2a4":"code","8bbd348a":"code","c15bb78d":"code","0e7b474f":"code","463067b5":"code","8f0cfb4b":"code","3cd76edd":"code","2c1e6657":"code","24244ad1":"code","d9f32616":"code","c1b37915":"code","21fa1fb1":"code","591d0159":"code","81c0dd7c":"code","5de63bae":"code","3ce50612":"code","36187baf":"code","84feb7f3":"code","a98a7297":"code","fd8b4afd":"code","321bfb37":"code","5d9ea407":"code","2fa16457":"code","fb74057b":"code","1fc849e5":"code","adb86dfb":"code","c41f80f3":"code","444b939c":"code","a99a8d48":"code","f4adab82":"code","e777fe48":"code","80bc4629":"code","aebb4d71":"code","c4cdc790":"code","a4090313":"code","1f81c97a":"code","1274ecc6":"code","2e0bbae8":"code","4598bc42":"code","cb028520":"code","56e84db8":"code","ac388f47":"code","5ff0f7fb":"code","4b394d19":"code","ef43210e":"code","0fea9b9b":"code","273d996c":"code","a8c64c48":"code","d625b090":"code","66d478c9":"code","a4fa3929":"code","c78e4c59":"code","349875b4":"code","a3a36923":"code","be715b59":"code","b42463a5":"code","5a53931f":"code","e124147e":"code","fd2aa30a":"code","2b7208a9":"code","f20b7583":"code","11b2bd1a":"code","d1aede8d":"code","43988934":"code","a8d7bc75":"code","13ace314":"code","023c345b":"code","ee6284b6":"code","769fb648":"code","5ad1c818":"code","5eb52151":"code","bf519c11":"code","018f2120":"code","648f16a8":"code","1d340ae0":"code","334357f9":"code","c55a0e8d":"code","8df07fda":"code","e6689efa":"code","d68e5b78":"code","d551f053":"code","f0390067":"markdown","6df73cfa":"markdown","7c7f6a7c":"markdown","0ce24dd2":"markdown","71171884":"markdown","8a714506":"markdown","5f40f4a1":"markdown","5abc1b8e":"markdown","2d9e1132":"markdown","574c8c5d":"markdown","cdac0336":"markdown","6dd6b702":"markdown","c24006c8":"markdown","0e355b6c":"markdown","e10a4e73":"markdown","d9b5abc8":"markdown","d4ecf1f9":"markdown","f5cc14f4":"markdown","4bccbd9d":"markdown","636909b1":"markdown","d5d81751":"markdown","04ad1198":"markdown","cdb6b4eb":"markdown","bacd05c0":"markdown","dfe38362":"markdown","35f2db75":"markdown","e4a19db9":"markdown","ab5c014f":"markdown","40a81610":"markdown","909fe60a":"markdown","00a06bd5":"markdown","5e1a1e99":"markdown","99e001ab":"markdown","902e9403":"markdown","ced7b70a":"markdown","d060f138":"markdown","9282826c":"markdown","d874feb7":"markdown","05eb1d33":"markdown","855fc168":"markdown","58a2df56":"markdown","32593af9":"markdown","2a64c979":"markdown","e17d41ba":"markdown","3ae04f11":"markdown","d4163cc0":"markdown","b5732d6c":"markdown","1d67d691":"markdown","60cb9911":"markdown","330c7ab8":"markdown","6cd76886":"markdown","8e1cc68b":"markdown","bb481bc6":"markdown","38d67e3d":"markdown","20209822":"markdown","005c1120":"markdown","cf2f4c5b":"markdown","53e9612e":"markdown","0e8546d4":"markdown","098e14a5":"markdown","9f78beba":"markdown","b93497b9":"markdown","d0bef52f":"markdown","902d7cce":"markdown","0b1301b0":"markdown","0a487271":"markdown","73ca8043":"markdown","8bf9b80b":"markdown","9a3bb2bc":"markdown","1ee74e14":"markdown","00b8c6a3":"markdown","20d6167d":"markdown","0d0e83f5":"markdown","4ac2605d":"markdown","c27ba42a":"markdown","7870283c":"markdown","b2b35742":"markdown","0434e8cf":"markdown","752777bb":"markdown","ac3be8fd":"markdown","150c1378":"markdown"},"source":{"ea330f84":"!pip install lightgbm","5b34bf47":"!pip install catboost","e7c07ab8":"!pip install sklearn_pandas","197fedb7":"!pip install imbalanced-learn","5c4c67af":"import numpy as np\n\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\n\nimport seaborn as sns\n\nimport scipy.stats as st\n\nimport pylab\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, average_precision_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, cross_validate\nfrom sklearn.utils import resample\nfrom sklearn.ensemble import VotingClassifier, StackingClassifier\n\nfrom sklearn_pandas import DataFrameMapper, gen_features\n\nfrom imblearn.over_sampling import SMOTENC\n\nfrom catboost import CatBoostClassifier\n\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c49b1587":"sns.set_context(\"paper\", rc={\"font.size\":12, \n                             \"figure.titlesize\":18, \n                             \"axes.titlesize\":15, \n                             \"axes.labelsize\":13, \n                             \"xtick.labelsize\": 13,\n                             \"ytick.labelsize\": 13,\n                             \"legend.fontsize\": 9,\n                             \"legend.title_fontsize\": 11}) ","15e61092":"train_raw = pd.read_csv('..\/input\/hranalysis\/train.csv', index_col='employee_id')\ntest_raw = pd.read_csv('..\/input\/hranalysis\/test.csv', index_col='employee_id')","5a5608b8":"train_raw.head(10)","ad4f2fc2":"train_raw.info()","dbedd846":"test_raw.info()","8af0f68d":"train_raw.describe()","9e166179":"TARGET = 'is_promoted'\nTARGET_PALETTE = ['#ff6969', '#69ff8e'] # Used when we consider the graphs in relation to the target variable. Red - not promoted, Green - promoted\nORDINARY_PALETTE = sns.color_palette(\"Set2\")","be0478a5":"sizes = [train_raw.query(f'{TARGET} == 0').shape[0],\n         train_raw.query(f'{TARGET} == 1').shape[0]]\n\nplt.figure(figsize=(12, 8))\nplt.title('Target variable proportion')\nplt.pie(sizes, labels=['No', 'Yes'], colors=TARGET_PALETTE, autopct=\"%.1f%%\", pctdistance=0.8, explode=[0,0.1], shadow=True)\nplt.legend(title='Has the employee been promoted', labels=['No', 'Yes'], bbox_to_anchor=(1, 1))\nplt.show()","ba917430":"train_raw['no_of_trainings'].value_counts()","bb179ce3":"data = pd.crosstab(train_raw['no_of_trainings'], train_raw[TARGET])\nax = data.div(data.sum(1).astype('float'), \n              axis = 0\n             ).sort_values(by=0).plot(kind = 'bar', stacked = True, figsize = (18, 8), color=TARGET_PALETTE)\nax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n\nplt.title('Proportion of promoted employees by number of trainings')\nplt.xlabel('Number of trainings')\nplt.legend(title='Has the employee been promoted', bbox_to_anchor=(1.01, 1.01), labels=['No', 'Yes'])\nplt.show()","2dc49193":"def custom_displot(df: pd.DataFrame, col: str) -> None:\n    rp = sns.displot(data=df, x=col, hue=TARGET, palette=TARGET_PALETTE, kde=True, height=6, aspect=3)\n    rp._legend.remove()\n    plt.legend(title=f'Has the employee been promoted', loc='upper right', labels=['Yes', 'No'])\n    rp.fig.suptitle(f'{col} distribution')","ce6db66c":"def custom_boxplot(df: pd.DataFrame, col: str) -> None:\n    plt.figure(figsize=(14,6))\n    plt.title(f'{col} boxplot with target variable')\n    ax = sns.boxplot(data=df, x=TARGET, y=col, palette=TARGET_PALETTE)\n    ax.set_xticklabels(['No', 'Yes'])\n    ax.set_xlabel('Has the employee ben promoted')\n    plt.show()","808b9bd8":"custom_displot(train_raw, 'age')","f31a0e23":"custom_boxplot(train_raw, 'age')","20ea234c":"st.probplot(train_raw['age'], dist='norm', plot=pylab)\npylab.show()","fd9ae767":"sns.distplot(np.log(train_raw['age']), color=ORDINARY_PALETTE[0])\nplt.title('age log-transformed')\nplt.show()","e9099179":"st.probplot(np.log(train_raw['age']), dist='norm', plot=pylab)\npylab.show()","7d32d39b":"data = pd.crosstab(train_raw['previous_year_rating'], train_raw[TARGET])\nax = data.div(data.sum(1).astype('float'), \n              axis = 0\n             ).sort_values(by=0).plot(kind = 'bar', stacked = True, figsize = (18, 8), color=TARGET_PALETTE)\nax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n\nplt.title('Proportion of promoted employees by number of trainings')\nplt.xlabel('Number of trainings')\nplt.legend(title='Has the employee been promoted', bbox_to_anchor=(1.01, 1.01), labels=['No', 'Yes'])\nplt.show()","377b6693":"custom_displot(train_raw, 'avg_training_score')","062fa62a":"custom_boxplot(train_raw, 'avg_training_score')","0dd3bb87":"plt.figure(figsize=(18,6))\ngraph = sns.boxplot(data=train_raw, x='department', y='avg_training_score', palette=ORDINARY_PALETTE)\nplt.title('Average training score across departments')\nplt.show()","8b0b1888":"high_avg_departments = ['Technology', 'Analytics', 'R&D'] # high average training score departments\nmid_avg_departments = ['Operations', 'Finance', 'Procurement', 'Legal'] # medium average training score departments\nlow_avg_departments = ['Sales & Marketing', 'HR'] # low average training score departments\n\ntrain_raw['department_avg'] = train_raw['department']\ntrain_raw['department_avg'].replace(high_avg_departments, 2, inplace=True)\ntrain_raw['department_avg'].replace(mid_avg_departments, 1, inplace=True)\ntrain_raw['department_avg'].replace(low_avg_departments, 0, inplace=True)\n\ntest_raw['department_avg'] = test_raw['department']\ntest_raw['department_avg'].replace(high_avg_departments, 2, inplace=True)\ntest_raw['department_avg'].replace(mid_avg_departments, 1, inplace=True)\ntest_raw['department_avg'].replace(low_avg_departments, 0, inplace=True)","47c1a2a4":"sizes = [train_raw.loc[train_raw['KPIs_met >80%'] == 0].shape[0],\n         train_raw.loc[train_raw['KPIs_met >80%'] == 1].shape[0]]\n\nplt.figure(figsize=(12, 8))\nplt.title('Employees proportion in terms of KPIs')\nplt.pie(sizes, labels=['Below 80%', 'Above 80%'], autopct=\"%.1f%%\", pctdistance=0.8, explode=[0,0.1], shadow=True, colors=ORDINARY_PALETTE)\nplt.legend(title='KPI metrics', bbox_to_anchor=(1, 1))\nplt.show()","8bbd348a":"unique_column_labels = train_raw['KPIs_met >80%'].dropna().unique()\ntitles = ['KPIs above 80%', 'KPIs below 80%']\nunique_target_labels = ['Not promoted', 'Promoted']\n\nfig, axes= plt.subplots(1, len(unique_column_labels), figsize=(18, 6))\n\nfor i, ax in enumerate(axes.flatten()):\n    sizes = [train_raw.loc[train_raw['KPIs_met >80%'] == unique_column_labels[i]].query(f'{TARGET} == 0').shape[0],\n             train_raw.loc[train_raw['KPIs_met >80%'] == unique_column_labels[i]].query(f'{TARGET} == 1').shape[0]]\n    ax.pie(sizes, labels=unique_target_labels, colors=TARGET_PALETTE, autopct=\"%.1f%%\", pctdistance=0.8, explode=[0,0.1], shadow=True)\n    ax.set_title(titles[i])\n\nfig.suptitle('A piechart representing gap in employees promotion in terms of KPIs')\n\nplt.tight_layout()\nplt.show()","c15bb78d":"sizes = [train_raw.loc[train_raw['awards_won?'] == 0].shape[0],\n         train_raw.loc[train_raw['awards_won?'] == 1].shape[0]]\n\nplt.figure(figsize=(18, 8))\nplt.title('Employees proportion in terms of awards won')\nplt.pie(sizes, labels=['No', 'Yes'], colors=ORDINARY_PALETTE, autopct=\"%.1f%%\", pctdistance=0.85, shadow=True)\n\nplt.legend(title='Has the employee ever won an award', labels=['No', 'Yes'], bbox_to_anchor=(1, 1))\n\n# add a circle at the center to transform it in a donut chart\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle)\n\nplt.show()","0e7b474f":"unique_column_labels = train_raw['awards_won?'].dropna().unique()\ntitles = ['Awards not won', 'Awards won']\nunique_target_labels = ['Not promoted', 'Promoted']\n\nfig, axes= plt.subplots(1, len(unique_column_labels), figsize=(18, 6))\n\nfor i, ax in enumerate(axes.flatten()):\n    sizes = [train_raw.loc[train_raw['awards_won?'] == unique_column_labels[i]].query(f'{TARGET} == 0').shape[0],\n             train_raw.loc[train_raw['awards_won?'] == unique_column_labels[i]].query(f'{TARGET} == 1').shape[0]]\n    ax.pie(sizes, labels=unique_target_labels, colors=TARGET_PALETTE, autopct=\"%.1f%%\", pctdistance=0.8, explode=[0,0.1], shadow=True)\n    ax.set_title(titles[i])\n\nfig.suptitle('A piechart representing gap in employees promotion in terms of awards won')\n\nplt.tight_layout()\nplt.show()","463067b5":"plt.figure(figsize=(18, 6))\nplt.title('Number of employees by department')\nax = sns.countplot(train_raw['department'], order = train_raw['department'].value_counts().index, palette=ORDINARY_PALETTE)\nplt.show()","8f0cfb4b":"data = pd.crosstab(train_raw['department'], train_raw['is_promoted'])\nax = data.div(data.sum(1).astype('float'), \n              axis = 0\n             ).sort_values(by=0).plot(kind = 'bar', stacked = True, figsize = (18, 8), color=TARGET_PALETTE)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n\nplt.title('Proportion of promoted employees in every department')\nplt.xlabel('Department')\nplt.legend(title='Has the employee been promoted', bbox_to_anchor=(1.01, 1.01), labels=['No', 'Yes'])\nplt.show()","3cd76edd":"plt.figure(figsize=(18, 6))\nplt.title('Number of employees by region')\nax = sns.countplot(train_raw['region'], order = train_raw['region'].value_counts().index)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nplt.show()","2c1e6657":"data = pd.crosstab(train_raw['region'], train_raw['is_promoted'])\nax = data.div(data.sum(1).astype('float'), \n              axis = 0\n             ).sort_values(by=0).plot(kind = 'bar', stacked = True, figsize = (18, 8), color=TARGET_PALETTE)\n\nplt.title('Proportion of promoted employees in every region')\nplt.xlabel('region')\nplt.legend(title='Has the employee been promoted', bbox_to_anchor=(1.01, 1.01), labels=['No', 'Yes'])\nplt.show()","24244ad1":"unique_column_labels = train_raw['education'].dropna().unique()\nsizes = []\n\nfor i, label in enumerate(unique_column_labels):\n    sizes.append(train_raw.loc[train_raw['education'] == label].shape[0])\n\nplt.figure(figsize=(12, 8))\nplt.title(\"Proportion of employees education types\")\nplt.pie(sizes, labels=unique_column_labels, autopct=\"%.1f%%\", pctdistance=0.85, shadow=True, colors=ORDINARY_PALETTE)\nplt.legend(title=\"Employee's education\", labels=unique_column_labels, bbox_to_anchor=(1, 1))\n\n# add a circle at the center to transform it in a donut chart\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle)\n\nplt.show()","d9f32616":"sizes = [train_raw.loc[train_raw['gender'] == 'f'].shape[0],\n         train_raw.loc[train_raw['gender'] == 'm'].shape[0]]\n\nplt.figure(figsize=(12, 8))\nplt.title('Employees proportion in terms of gender')\nplt.pie(sizes, labels=['Female', 'Male'], autopct=\"%.1f%%\", pctdistance=0.8, explode=[0,0.1], shadow=True, colors=ORDINARY_PALETTE)\nplt.legend(title='Gender', bbox_to_anchor=(1, 1))\nplt.show()","c1b37915":"unique_column_labels = train_raw['recruitment_channel'].dropna().unique()\nsizes = []\n\nfor i, label in enumerate(unique_column_labels):\n    sizes.append(train_raw.loc[train_raw['recruitment_channel'] == label].shape[0])\n\nplt.figure(figsize=(12, 8))\nplt.title(\"Proportion of employees recruitment channels\")\nplt.pie(sizes, labels=unique_column_labels, autopct=\"%.1f%%\", pctdistance=0.85, shadow=True, colors=ORDINARY_PALETTE)\nplt.legend(title=\"Employee's recruitment channel\", labels=unique_column_labels, bbox_to_anchor=(1, 1))\n\n# add a circle at the center to transform it in a donut chart\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle)\n\nplt.show()","21fa1fb1":"CATEGORICAL_FEATURES = ['department', 'region', 'education', 'gender', 'recruitment_channel']\nBINARY_FEATURES = ['KPIs_met >80%', 'awards_won?']\nNUMERICAL_FEATURES = ['no_of_trainings', 'age', 'previous_year_rating', 'length_of_service', 'avg_training_score']","591d0159":"g = sns.pairplot(data=train_raw[NUMERICAL_FEATURES + [TARGET]], \n                 hue=TARGET, kind='scatter', plot_kws={'alpha':0.25}, palette=ORDINARY_PALETTE[1:3][::-1])\n\nhandles = g._legend_data.values()\ng._legend.remove()\ng.fig.suptitle('Numerical features pairplot')\ng.fig.legend(title='Has the employee been promoted', handles=handles, labels=['No', 'Yes'], bbox_to_anchor=(1, 0.5), loc=2)\n\nplt.tight_layout()\nplt.show()","81c0dd7c":"print('Minimum age and length of service difference - ',\n      min((train_raw['age'] - train_raw['length_of_service'])))","5de63bae":"g = sns.jointplot(x=train_raw['age'], y=train_raw['length_of_service'], kind='hex', color=ORDINARY_PALETTE[0])\ng.fig.suptitle('Correlation of Age and Length of service')\n\nplt.tight_layout()\nplt.show()","3ce50612":"corr_matr = train_raw[NUMERICAL_FEATURES].corr(method='pearson')\nplt.figure(figsize=(10,10))\nsns.heatmap(corr_matr, annot=True, cmap='coolwarm', square=True)\nplt.title(\"Pearson's correlation heatmap\")\nplt.show()","36187baf":"train_raw.isnull().sum(axis=0)","84feb7f3":"train_raw[train_raw['previous_year_rating'].isna()]['length_of_service'].value_counts()","a98a7297":"train_raw[train_raw['education'].isna()]","fd8b4afd":"fill_values = {'education': 'missing',\n               'previous_year_rating': 0}\ntrain_filled = train_raw.fillna(value=fill_values)\ntest_filled = test_raw.fillna(value=fill_values)","321bfb37":"train_filled['gender'] = train_filled['gender'].apply(lambda x: 1 if x == 'm' else 0)\ntest_filled['gender'] = test_filled['gender'].apply(lambda x: 1 if x == 'm' else 0)","5d9ea407":"fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\nsns.distplot(train_filled['age'], kde=True, ax=axes[0], color=ORDINARY_PALETTE[0])\naxes[0].set_title('Age distribution')\n\nsns.distplot(np.log(train_filled['age']), kde=True, ax=axes[1], color=ORDINARY_PALETTE[0])      \naxes[1].set_title('Age log-transformed distribution')\naxes[1].set_xlabel('log(age)')\n\nplt.show()","2fa16457":"train_log = train_filled.apply(lambda x: np.log(x) if x.name == 'age' else x, axis=0)","fb74057b":"custom_displot(train_log, 'age')","1fc849e5":"X = train_log.drop(columns=TARGET)\ny = train_log[TARGET]\nRANDOM_STATE = 43\nTEST_SIZE = 0.2","adb86dfb":"X_train, X_val, y_train, y_val = train_test_split(X,\n                                                  y,\n                                                  random_state=RANDOM_STATE,\n                                                  shuffle=True, \n                                                  test_size=TEST_SIZE,\n                                                  stratify=y)","c41f80f3":"X_train","444b939c":"education_encoder = gen_features(\n    columns=[['education']],\n    classes=[\n        {'class': OrdinalEncoder, 'categories': [['missing',\n                                                 'Below Secondary',\n                                                 \"Bachelor's\",\n                                                 \"Master's & above\"]]\n        }\n    ]\n)","a99a8d48":"education_mapper = DataFrameMapper(education_encoder, default=None, df_out=True)\nX_train_education_encoded = education_mapper.fit_transform(X_train)\nX_val_education_encoded = education_mapper.transform(X_val)","f4adab82":"X_train_education_encoded","e777fe48":"not_to_encode_features = ['gender', 'education']\nto_encode_features = list(filter(lambda x: x not in not_to_encode_features, CATEGORICAL_FEATURES))\n\ncategorical_ohe_encoder = gen_features(\n    columns=[[c] for c in to_encode_features],\n    classes=[{'class': OneHotEncoder}]\n)","80bc4629":"categorical_ohe_encoder","aebb4d71":"categorical_mapper = DataFrameMapper(categorical_ohe_encoder, df_out=True)","c4cdc790":"X_train_cat_encoded = categorical_mapper.fit_transform(X_train_education_encoded)\nX_val_cat_encoded = categorical_mapper.transform(X_val_education_encoded)\n\n# have to join dataframes because DataFrameMapper tries to convert remaining columns to object \n# Issue https:\/\/github.com\/scikit-learn-contrib\/sklearn-pandas\/issues\/171\nX_train_cat_encoded = X_train_education_encoded.drop(columns=to_encode_features, axis=1).join(X_train_cat_encoded)\nX_val_cat_encoded = X_val_education_encoded.drop(columns=to_encode_features, axis=1).join(X_val_cat_encoded)","a4090313":"minmax_scaler = gen_features(\n    columns = [[c] for c in X_train_cat_encoded.columns.values],\n    classes=[{'class': MinMaxScaler}]\n)","1f81c97a":"numeric_mapper = DataFrameMapper(minmax_scaler, default=None, df_out=True)\nX_train = numeric_mapper.fit_transform(X_train_cat_encoded)\nX_val = numeric_mapper.transform(X_val_cat_encoded)","1274ecc6":"X_train","2e0bbae8":"X_val","4598bc42":"class ModelEvaluator():\n    def __init__(self,\n                 X_train: pd.DataFrame, \n                 y_train: np.ndarray,\n                 X_val: pd.DataFrame, \n                 y_val: pd.Series,\n                 model):\n        \"\"\"\n        Parameters\n        X_train: dataset to train the model\n        y_train: train dataset labels\n        X_val: dataset to validate the model\n        y_val: validation dataset labels\n        model: model to evaluate\n        \"\"\"\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_val = X_val\n        self.y_val = y_val\n        self.model = model\n        \n        self.target_names = sorted(self.y_train.unique())\n\n    def _fit_predict(self):\n        \"\"\"\n        This method fits the model with train data and \n        gets the predictions on train and validation\n        \"\"\"\n        # model fitting\n        self.model.fit(self.X_train, self.y_train)\n\n        # predicting\n        self.pred_val = self.model.predict(self.X_val)\n        self.pred_train = self.model.predict(self.X_train)\n        self.probas_val = self.model.predict_proba(self.X_val)[:,1]\n        self.probas_train = self.model.predict_proba(self.X_train)[:,1]\n\n    def evaluate_model(self):\n        \"\"\"\n        This method builds a dlassification report on train and validation data\n        \"\"\"\n        self._fit_predict()\n    \n        print(\"Train:\")\n        print(classification_report(self.y_train, self.pred_train, \n                                    labels=self.target_names))\n        print(f'PR-AUC: {average_precision_score(self.y_train, self.probas_train)}\\n')\n        \n        print(\"Validation:\")\n        print(classification_report(self.y_val, self.pred_val, \n                                    labels=self.target_names))\n        print(f'PR-AUC: {average_precision_score(self.y_val, self.probas_val)}')\n\n    def plot_confusion_matrix(self):\n        \"\"\"\n        This method plots confusion matrix\n        \"\"\"\n        conf = confusion_matrix(self.y_val, self.pred_val)\n        sns.heatmap(conf,         \n                    yticklabels=self.target_names,\n                    xticklabels=self.target_names,\n                    annot=True,\n                    fmt=\".1f\")\n        plt.title('Confusion matrix')\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.show()","cb028520":"me_logreg = ModelEvaluator(X_train, y_train, X_val, y_val, LogisticRegression())","56e84db8":"me_logreg.evaluate_model()","ac388f47":"me_logreg.plot_confusion_matrix()","5ff0f7fb":"cat_features_indicies = range(10, 53)\nclasses_proportion = 0.3\n\nsmotenc = SMOTENC(cat_features_indicies, random_state=RANDOM_STATE, sampling_strategy=classes_proportion)\n\nX_upsampled, y_upsampled = smotenc.fit_resample(X_train, y_train)","4b394d19":"me_logreg_upsampled = ModelEvaluator(X_upsampled, y_upsampled, X_val, y_val, LogisticRegression())","ef43210e":"me_logreg_upsampled.evaluate_model()","0fea9b9b":"me_logreg_upsampled.plot_confusion_matrix()","273d996c":"def resample_(df, amount: int, feature: str) -> pd.DataFrame:\n    df_majority = df[df[feature]==0]\n    df_minority = df[df[feature]==1]\n\n    # Upsample minority class\n    df_minority_resampled = resample(df_minority, \n                                     replace=True,\n                                     n_samples=amount,\n                                     random_state=RANDOM_STATE)\n\n    # Combine majority class with upsampled minority class\n    df_resampled = pd.concat([df_majority, df_minority_resampled])\n    df_resampled = df_resampled.sample(frac=1)\n    return df_resampled","a8c64c48":"train = X_train.copy()\ntrain[TARGET] = y_train.copy()","d625b090":"train","66d478c9":"train_resampled = resample_(resample_(train, 25000, 'awards_won?'), 20000, 'KPIs_met >80%')","a4fa3929":"train_resampled","c78e4c59":"sizes = [train_resampled.loc[train_resampled['awards_won?'] == 0].shape[0],\n         train_resampled.loc[train_resampled['awards_won?'] == 1].shape[0]]\n\nplt.figure(figsize=(18, 8))\nplt.title('Employees proportion in terms of awards won')\nplt.pie(sizes, labels=['No', 'Yes'], colors=ORDINARY_PALETTE, autopct=\"%.1f%%\", pctdistance=0.85, shadow=True)\n\nplt.legend(title='Has the employee ever won an award', labels=['No', 'Yes'], bbox_to_anchor=(1, 1))\n\n# add a circle at the center to transform it in a donut chart\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle)\n\nplt.show()","349875b4":"sizes = [train_resampled.loc[train_resampled['KPIs_met >80%'] == 0].shape[0],\n         train_resampled.loc[train_resampled['KPIs_met >80%'] == 1].shape[0]]\n\nplt.figure(figsize=(12, 8))\nplt.title('Employees proportion in terms of KPIs')\nplt.pie(sizes, labels=['Below 80%', 'Above 80%'], autopct=\"%.1f%%\", pctdistance=0.8, explode=[0,0.1], shadow=True, colors=ORDINARY_PALETTE)\nplt.legend(title='KPI metrics', bbox_to_anchor=(1, 1))\nplt.show()","a3a36923":"X_resampled = train_resampled.drop(columns=TARGET)\ny_resampled = train_resampled[TARGET]","be715b59":"me_logreg_resampled = ModelEvaluator(X_resampled, y_resampled, X_val, y_val, LogisticRegression())","b42463a5":"me_logreg_resampled.evaluate_model()","5a53931f":"me_logreg_resampled.plot_confusion_matrix()","e124147e":"train_filled.info()","fd2aa30a":"train_filled[CATEGORICAL_FEATURES] = train_filled[CATEGORICAL_FEATURES].astype('category')","2b7208a9":"train_filled.info()","f20b7583":"X_filled = train_filled.drop(columns=TARGET)\ny_filled = train_filled[TARGET]\n\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\n\nX_train_filled, X_val_filled, y_train_filled, y_val_filled = train_test_split(X_filled,\n                                                                              y_filled,\n                                                                              random_state=RANDOM_STATE,\n                                                                              shuffle=True, \n                                                                              test_size=TEST_SIZE,\n                                                                              stratify=y)","11b2bd1a":"X_train_filled","d1aede8d":"lgb_clf = LGBMClassifier(random_state=RANDOM_STATE)","43988934":"me_lgb = ModelEvaluator(X_train_filled, y_train_filled, X_val_filled, y_val_filled, lgb_clf)","a8d7bc75":"me_lgb.evaluate_model()","13ace314":"lgbm_scores = cross_validate(me_lgb.model, X_filled, y_filled, \n                             scoring=['recall', 'precision'], cv=5)\n\n# report precision\nprint('Precision scores by folds: ', lgbm_scores['test_precision'])\nprint('Mean precision score: ', np.mean(lgbm_scores['test_precision']))\n\n# report recall\nprint('Recall scores by folds: ', lgbm_scores['test_recall'])\nprint('Mean recall score: ', np.mean(lgbm_scores['test_recall']))","023c345b":"catboost_clf = CatBoostClassifier(random_seed=RANDOM_STATE, \n                                  cat_features=X_train_filled.select_dtypes(['category']).columns.values,\n                                  verbose=False)","ee6284b6":"me_catboost = ModelEvaluator(X_train_filled, y_train_filled, X_val_filled, y_val_filled, catboost_clf)","769fb648":"me_catboost.evaluate_model()","5ad1c818":"catboost_scores = cross_validate(me_catboost.model, X_filled, y_filled, \n                                 scoring=['recall', 'precision'], cv=5)\n\n# report precision\nprint('Precision scores by folds: ', catboost_scores['test_precision'])\nprint('Mean precision score: ', np.mean(catboost_scores['test_precision']))\n\n# report recall\nprint('Recall scores by folds: ', catboost_scores['test_recall'])\nprint('Mean recall score: ', np.mean(catboost_scores['test_recall']))","5eb52151":"param_grid = {\n    'num_iterations':[200,300,400,500,600],\n    'max_depth': list(range(6, 10)),\n    'num_leaves': list(range(20, 150)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'scale_pos_weight': list(np.linspace(1, 5, 20))\n}","bf519c11":"classifiers = {'Recall LGBM' : LGBMClassifier(**{'boosting_type': 'gbdt',\n                                                'class_weight': None,\n                                                'colsample_bytree': 1.0,\n                                                'importance_type': 'split',\n                                                'learning_rate': 0.1,\n                                                'max_depth': -1,\n                                                'min_child_samples': 20,\n                                                'min_child_weight': 0.001,\n                                                'min_split_gain': 0.0,\n                                                'n_estimators': 107,\n                                                'n_jobs': -1,\n                                                'num_leaves': 31,\n                                                'objective': None,\n                                                'random_state': 42,\n                                                'reg_alpha': 0.0,\n                                                'reg_lambda': 0.0,\n                                                'silent': True,\n                                                'subsample': 1.0,\n                                                'subsample_for_bin': 200000,\n                                                'subsample_freq': 0,\n                                                'scale_pos_weight': 7}),\n                                   \n               'F1 LGBM' : LGBMClassifier(**{'boosting_type': 'gbdt',\n                                             'class_weight': None,\n                                             'colsample_bytree': 1.0,\n                                             'importance_type': 'split',\n                                             'learning_rate': 0.1,\n                                             'max_depth': -1,\n                                             'min_child_samples': 20,\n                                             'min_child_weight': 0.001,\n                                             'min_split_gain': 0.0,\n                                             'n_estimators': 200,\n                                             'n_jobs': -1,\n                                             'num_leaves': 31,\n                                             'objective': None,\n                                             'random_state': 42,\n                                             'reg_alpha': 7,\n                                             'reg_lambda': 0.0,\n                                             'silent': True,\n                                             'subsample': 1.0,\n                                             'subsample_for_bin': 200000,\n                                             'subsample_freq': 0,\n                                             'scale_pos_weight': 2}),\n               \n               'Precision LGBM' : LGBMClassifier(**{'boosting_type': 'gbdt',\n                                                     'class_weight': None,\n                                                     'colsample_bytree': 1.0,\n                                                     'importance_type': 'split',\n                                                     'learning_rate': 0.1,\n                                                     'max_depth': -1,\n                                                     'min_child_samples': 20,\n                                                     'min_child_weight': 0.001,\n                                                     'min_split_gain': 0.0,\n                                                     'n_estimators': 100,\n                                                     'n_jobs': -1,\n                                                     'num_leaves': 31,\n                                                     'objective': None,\n                                                     'random_state': 42,\n                                                     'reg_alpha': 0,\n                                                     'reg_lambda': 0.0,\n                                                     'silent': True,\n                                                     'subsample': 1.0,\n                                                     'subsample_for_bin': 200000,\n                                                     'subsample_freq': 0,\n                                                     'scale_pos_weight': 1})\n}","018f2120":"voting_model = VotingClassifier(estimators=[('Recall_LGBM_Best', list(classifiers.values())[0]), \n                                            ('F1_LGBM_Best', list(classifiers.values())[1]),\n                                            ('Precision_LGBM_Best', list(classifiers.values())[2])],\n                                voting='soft', \n                                weights=[2, 2.35, 1.25])","648f16a8":"voting_scores = cross_validate(voting_model, X_filled, y_filled, \n                               scoring=['precision', 'recall'], cv=5)\n\n# report precision\nprint('Precision scores by folds: ', voting_scores['test_precision'])\nprint('Mean precision score: ', np.mean(voting_scores['test_precision']))\n\n# report recall\nprint('Recall scores by folds: ', voting_scores['test_recall'])\nprint('Mean recall score: ', np.mean(voting_scores['test_recall']))","1d340ae0":"stacked_model = StackingClassifier(estimators=[('Recall_LGBM_Best', list(classifiers.values())[0]), \n                                               ('F1_LGBM_Best', list(classifiers.values())[1]),\n                                               ('Precision_LGBM_Best', list(classifiers.values())[2])],                                           \n                                   final_estimator=LGBMClassifier(scale_pos_weight=2.5, n_estimators=93),\n                                   cv=5,\n                                   stack_method='predict_proba')","334357f9":"stacking_scores = cross_validate(stacked_model, X_filled, y_filled, \n                                 scoring=['recall', 'precision'], cv=5)\n\n# report precision\nprint('Precision scores by folds: ', stacking_scores['test_precision'])\nprint('Mean precision score: ', np.mean(stacking_scores['test_precision']))\n\n# report recall\nprint('Recall scores by folds: ', stacking_scores['test_recall'])\nprint('Mean recall score: ', np.mean(stacking_scores['test_recall']))","c55a0e8d":"test_filled","8df07fda":"test_filled[CATEGORICAL_FEATURES] = test_filled[CATEGORICAL_FEATURES].astype('category')","e6689efa":"voting_model.fit(X_filled, y_filled)","d68e5b78":"predictions = voting_model.predict(test_filled)","d551f053":"print(f'Number of employees in test dataset: {predictions.size}\\n',\n      f'Predicted not to be promoted: {(predictions == 0).sum()}\\n',\n      f'Predicted to be promoted: {(predictions == 1).sum()}')","f0390067":"We can't see the difference in age distribution between promoted and not promoted employees\n","6df73cfa":"Pearsons's correlation coefficients shows us is there linear correlation between variables or not. And the assumption about `age` and `length_of_service` correlation is confirmed\n\nBy the way, all other features are not correlated and that is good","7c7f6a7c":"We got models based on boosted trees:\n- If we want to identify employees worthy of promotion as best as possible, we should choose the voting classifier.\n- If it is not permissible to make a mistake in a promotion - not tuned CatBoost\/LightGBM","0ce24dd2":"#### Upsampling","71171884":"2 insights from these graphs:\n1. from the boxplot we can guess that target does not depends on age\n2. from displot and probplot: `age` is __positive skewed__. We can try to fix that with log transformation if we use parametric models","8a714506":"As we see, log-transformation reduces the skewness of `age`","5f40f4a1":"##### department","5abc1b8e":"#### LightGBM","2d9e1132":"# Results","574c8c5d":"#### Resampling","cdac0336":"#### CatBoost","6dd6b702":"LightGBM has better scores than logistic regression, precision is pretty high, but recall is still not good","c24006c8":"#### Categorical features","0e355b6c":"##### Preparing the data","e10a4e73":"# Data preparation","d9b5abc8":"We can see that our data is almost perfect, because there are not a lot of Nulls and the types are correct for each column. I can guess that this is synthetic dataset or it is already cleaned","d4ecf1f9":"`avg_training_score` well delimits our target variable\n\n`length_of_service` and `age` are correlated in some form, because `length_of_service` < `age` in any case. ","f5cc14f4":"Now let's try resample methods to increase our scores. I will add more objects with `KPIs_met >80%`=1 and `awards_won?`=1","4bccbd9d":"##### recruitment_channel","636909b1":"##### previous_year_rating","d5d81751":"I would try more accurate models, that will give us better scores","04ad1198":"#### Model Evaluator class","cdb6b4eb":"##### Categorical features OHE","bacd05c0":"A little bit better","dfe38362":"As we see, the more trainings an employee has completed, the less likely he will be promoted. That's __negative correlation__ with target variable","35f2db75":"It is hard to tell is this model good or not, because of Precision-Recall tradeoff. But in my opinion, this proportion (67 - 43) is better for the problem, than 95 - 35 for instance. Better not to miss out on someone who deserves a promotion. At the same time, the precision is still at a good level. 2\/3 of those who predicted to be promoted should really be promoted","e4a19db9":"Same thing. The ratio of those who are promoted is higher in the sample of awarded employees","ab5c014f":"##### Target variable (is_promoted)","40a81610":"##### education","909fe60a":"So, our model gives us not really good scores. Using upsampling, we are increasing our scores a bit, but we are overfitting (train and validation difference)","00a06bd5":"##### awards_won?","5e1a1e99":"# Dataset description","99e001ab":"#### Preprocessing","902e9403":"##### Splitting the data","ced7b70a":"I'd guess there are employees with no education or the information about it could be missing. In this case we can try to introduce new category - `missing` and then perform ordinal encoding on `education` feature ","d060f138":"##### avg_training_score","9282826c":"##### gender","d874feb7":"# EDA","05eb1d33":"# Voting classifier","855fc168":"There is some dependency: the higher __previous_year_rating__ -> more likely employees are getting promoted","58a2df56":"##### no_of_trainings","32593af9":"Let's encode `education` using OrdinalEncoder","2a64c979":"Now let's scale our  features to one range [0, 1]","e17d41ba":"I have chosen SMOTE-NC method to perform upsampling, because SMOTE-NC supports both nominal and continuous data, while SMOTE doesn't.\n\nLink: https:\/\/towardsdatascience.com\/5-smote-techniques-for-oversampling-your-imbalance-data-b8155bdbe2b5","3ae04f11":"# Boosted trees","d4163cc0":"I've already tried different sets of hyperparameters and this is the one i selected","b5732d6c":"##### Gender binarization","1d67d691":"###  Univariate analysis","60cb9911":"##### age","330c7ab8":"I decided to stack 3 LightGBMs, which are tuned respectively on: Recall, F1, Precision\n\nI used Random Search with manual adjustment of the parameters after it\n\nThe grid is:","6cd76886":"#### Numerical features","8e1cc68b":"#### Training model on the whole dataset","bb481bc6":"#### Missing values","38d67e3d":"##### Modeling","20209822":"`avg_training_score` seems like a good feature for us. When the value is more than 90, employees are gettin prompoted more often.\n\nAnd from the boxplot we can see that the medians are different","005c1120":"##### MinMax scaling","cf2f4c5b":"# Logistic Regression","53e9612e":"All other categorical features i will encode using OneHotEncoder","0e8546d4":"The dataset is __imbalanced__ and we will have to keep that in mind further","098e14a5":"As we see, the tradeoff is ~ 74 - 40, but i would still choose 67 - 43 as the final model ","9f78beba":"##### Filling Nulls","b93497b9":"Not good result, model works bad with 1 class","d0bef52f":"And we got three LGBM models with parameters that are specified below ","902d7cce":"We would normally start exploring our data with the target variable, but as long as our __target is binary__, there are not a lot of descriptive statistics.","0b1301b0":"#### Baseline model","0a487271":"# Import required libraries","73ca8043":"##### region","8bf9b80b":"__Context__\n\nProblem Statement\n\nThis is the HR datasets. In our dataset 50000 rows and 14 columns. Every year, around 5% of its employees have promoted in the company. so, we have the check employee is promoted or not?\n\n__Columns Name__\n- employee_id\n- department\n- region\n- education\n- gender\n- recruitment_channel\n- nooftrainings\n- age \n- previousyearrating\n- lengthofservice\n- KPIs_met >80%\n- awards_won?\n- avgtrainingscore\n- is_promoted","9a3bb2bc":"Recall is a little bit lower than upsampled model, but precision increased a lot. Aside from that, we are still overfitting and the model is bad","1ee74e14":"##### Education ordinal encoding","00b8c6a3":"Average training score really differs across departments, so let's try to introduce nef feature `department_avg` by dividing departments into groups","20d6167d":"# Stacking classifier","0d0e83f5":"That's right. We can encode these NaNs with zeros further, because there is definetly an order in values.","4ac2605d":"##### Modeling","c27ba42a":"And it's logical to assume that older age leads to more length of service (and the correlation is linear)","7870283c":"##### KPIs_met >80%","b2b35742":"CatBoost is a little bit better out of box","0434e8cf":"### Multivariate analysis","752777bb":"Employees who have KPIs metrics > 80% are likely to be promoted","ac3be8fd":"##### Skewed features transformation","150c1378":"Employees with NaN `previous_year_rating` would have `length_of_service` $= 1$, let's check"}}