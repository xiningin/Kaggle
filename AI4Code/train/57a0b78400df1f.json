{"cell_type":{"af5d90ef":"code","aa1df8e5":"code","0f27655e":"code","cdc4f810":"code","792186e8":"code","f211462a":"code","c11916ee":"code","42fb313e":"code","b0960652":"code","fae0543d":"code","6e579458":"code","68d26b57":"code","41714c68":"code","444d986d":"code","ec4dde15":"code","e0c79bc7":"code","ea83588f":"code","8bf1aeba":"code","269fba69":"code","14cdb046":"markdown","9967ee50":"markdown","9a7e6014":"markdown","714c6872":"markdown","151dc7fc":"markdown","a698f967":"markdown","141952b5":"markdown","0cc7c123":"markdown","f4cb87b4":"markdown","d5a0b93c":"markdown","648e30a5":"markdown","ea3d7d96":"markdown","ffe511d7":"markdown","ef7f5105":"markdown"},"source":{"af5d90ef":"import numpy as np\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom string import punctuation\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","aa1df8e5":"real = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")\nfake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")","0f27655e":"real.head()","cdc4f810":"fake.head()","792186e8":"real['category']=1\nfake['category']=0","f211462a":"df = pd.concat([real,fake])","c11916ee":"df.isna().sum()","42fb313e":"df['title'].count()","b0960652":"df.subject.value_counts()","fae0543d":"df['text'] = df['text'] + \" \" + df['title'] + \" \" + df['subject']\ndel df['title']\ndel df['subject']\ndel df['date']","6e579458":"stop = set(stopwords.words('english'))\npnc = list(punctuation)\nstop.update(pnc)","68d26b57":"stemmer = PorterStemmer()\ndef stem_text(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            word = stemmer.stem(i.strip())\n            final_text.append(word)\n    return \" \".join(final_text)","41714c68":"df['text'] = df['text'].apply(stem_text)","444d986d":"X_train,X_test,y_train,y_test = train_test_split(df['text'],df['category'])","ec4dde15":"cv = CountVectorizer(min_df=0,max_df=1,ngram_range=(1,2))\n\ncv_train = cv.fit_transform(X_train)\ncv_test = cv.transform(X_test)\n\nprint('Train shape: ',cv_train.shape)\nprint('Test shape: ',cv_test.shape)","e0c79bc7":"nb = MultinomialNB()","ea83588f":"nb.fit(cv_train, y_train)","8bf1aeba":"pred_nb = nb.predict(cv_test)","269fba69":"score = metrics.accuracy_score(y_test, pred_nb)\nprint(\"Accuracy Score: \",score)","14cdb046":"## Define Model","9967ee50":"## Fit Model","9a7e6014":"## Files\n\n* Fake.csv\n* True.csv","714c6872":"## Data Description\nFake.csv file contains a list of articles considered as \"fake\" news. True.csv contains a list of articles considered as \"real\" news. Both the files contain\n\n* The title of the article\n* The text of the article\n* The subject of the article\n* The date that this article was posted at","151dc7fc":"We will add a new column for both real and fake dataframe. This column will have 0 and 1. 1 for real news and 0 for fake news.","a698f967":"We now concatenate Text, Title and Subject in Text.","141952b5":"## Overview\n\nCan you use this data set to make an algorithm able to determine if an article is fake news or not ?","0cc7c123":"#### Accuracy","f4cb87b4":"## Load Data","d5a0b93c":"Splitting dataset in train set and test set","648e30a5":"## So let\u2019s begin here\u2026","ea3d7d96":"We will concatenate both the dataframe in a single dataframe and we will use this for training.","ffe511d7":"# Fake and Real News","ef7f5105":"## Predict"}}