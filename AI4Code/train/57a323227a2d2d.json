{"cell_type":{"a6c4a6fe":"code","c4252210":"code","2d569ad2":"code","0f34ad00":"code","07e7e71e":"code","91e9dcf9":"code","dd11f15f":"code","488474a3":"code","e91d71f5":"code","7f465626":"code","cb45f31b":"code","d857a37c":"code","e70e4624":"code","e0722c88":"code","ef08aaac":"code","81d0d71f":"code","f3d2724d":"code","d6bef964":"code","792509f6":"code","a4c65833":"code","33878667":"code","6b480b01":"code","cb448b6d":"code","985e1ac9":"code","e529d4ac":"code","706654fa":"code","266388fb":"code","f87fdbfd":"code","a5a39af3":"code","37c04edd":"code","6123f4b1":"code","cbaec11a":"code","a467359f":"code","9a0051e4":"code","78bf4c1d":"code","bf65fc55":"code","122c5c56":"code","80893be1":"code","e890d7eb":"code","d8a113e9":"code","3de8407d":"code","cfab5fe3":"code","497711e9":"code","15389036":"code","3787c246":"code","72d8d3c4":"code","8895c0bf":"code","38959bca":"code","9ba0f5bf":"code","9aab501d":"code","6c8f9d1f":"code","b11c90b0":"code","1accd072":"code","45787e42":"code","a0dcf73a":"code","9d75fe30":"code","a611d2d9":"code","dca0694d":"code","deb8ca01":"code","7faa0b48":"code","f31522b9":"code","24e2719d":"code","d909b67b":"code","674ce1aa":"code","355ec88c":"code","db36cf95":"code","3f3ad532":"code","4f98458e":"code","a5325c14":"code","c8ba464e":"code","e83f088e":"code","c1eb50da":"code","09fc2d3c":"markdown","e9981821":"markdown","86b40857":"markdown","63fe5383":"markdown","d032c77b":"markdown","fb4d7fbb":"markdown","2443753b":"markdown","756baaf4":"markdown","82e170aa":"markdown","4830ce6c":"markdown","d9ba49be":"markdown","4f1114f7":"markdown","2fceb5d0":"markdown","21774a72":"markdown","f5da81e1":"markdown","8cf8105d":"markdown","84515fc8":"markdown","3e96d4e3":"markdown","d303b928":"markdown"},"source":{"a6c4a6fe":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.metrics import f1_score\nimport graphviz\nfrom sklearn import tree\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c4252210":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(1)","2d569ad2":"# from hmmlearn import hmm\n# model = hmm.GaussianHMM(n_components=3, covariance_type=\"full\")\n# model.startprob_ = np.array([0.6, 0.3, 0.1])\n# model.transmat_ = np.array([[0.7, 0.1,0.2],[0.3, 0.5, 0.2],[0.3, 0.3, 0.4]])\n# model.means_ = np.array([[0.0, 0.0], [3.0, -3.0], [5.0, 10.0]])\n# model.covars_ = np.tile(np.identity(2), (3, 1, 1))\n# X, Z = model.sample(100)\n# X.shape, Z.shape","0f34ad00":"%%time\ntest = pd.read_csv('..\/input\/data-without-drift\/test_clean.csv')[['time', 'signal']]\ntrain = pd.read_csv('..\/input\/data-without-drift\/train_clean.csv')[['time', 'signal', 'open_channels']]\n# train['signal'] = train['signal'].apply(lambda x:np.round(x,4))\n# test['signal']  = test['signal'].apply(lambda x:np.round(x,4))\n\n#Normalizing\ntrain_input_mean = train.signal.mean()\ntrain_input_sigma = train.signal.std()\ntrain['signal'] = (train.signal-train_input_mean)\/train_input_sigma\ntest['signal'] = (test.signal-train_input_mean)\/train_input_sigma\n\ntrain.shape,test.shape","07e7e71e":"# def f(row):\n#     if row['open_channels'] == 0:\n#         val = np.inf\n#     else:\n#         val = row['signal']\n#     return val\n# train['vol'] = train.apply(f, axis=1)","91e9dcf9":"#Adding Previous signal values to make markovian\ntrain['prev'] = 0\ntrain['prev'][0+1:500000] = train['signal'][0:500000-1]\ntrain['prev'][500000+1:1000000] = train['signal'][500000:1000000-1]\ntrain['prev'][1000000+1:1500000] = train['signal'][1000000:1500000-1]\ntrain['prev'][1500000+1:2000000] = train['signal'][1500000:2000000-1]\ntrain['prev'][2000000+1:2500000] = train['signal'][2000000:2500000-1]\ntrain['prev'][2500000+1:3000000] = train['signal'][2500000:3000000-1]\ntrain['prev'][3000000+1:3500000] = train['signal'][3000000:3500000-1]\ntrain['prev'][3500000+1:4000000] = train['signal'][3500000:4000000-1]\ntrain['prev'][4000000+1:4500000] = train['signal'][4000000:4500000-1]\ntrain['prev'][4500000+1:5000000] = train['signal'][4500000:5000000-1]\n\n#Adding Previous signal values to make markovian\n\ntest['prev'] = 0\ntest['prev'][0+1:500000] = test['signal'][0:500000-1]\ntest['prev'][500000+1:1000000] = test['signal'][500000:1000000-1]\ntest['prev'][1000000+1:1500000] = test['signal'][1000000:1500000-1]\ntest['prev'][1500000+1:2000000] = test['signal'][1500000:2000000-1]","dd11f15f":"# 3d plot of data------without rounding off\n# res = 100\n# import matplotlib.pyplot as plt\n# from mpl_toolkits import mplot3d\n# fig = plt.figure(figsize=(30,25))\n# ax = plt.axes(projection='3d')\n# ax.set_xlabel('prev_signal',size=16)\n# ax.set_ylabel('curr_signal',size=16)\n# ax.set_zlabel('#channels',size=16);\n# zdata = train.open_channels[0::res]\n# xdata = train.prev[0::res]\n# ydata = train.signal[0::res]\n# ax.scatter3D(xdata, ydata, zdata, c=zdata,cmap='plasma');\n","488474a3":"# from tqdm.notebook import tqdm\n# train1 = np.asarray(train[['signal','open_channels']])\n# train_dict = {}#sig_val:list_of_open_channels rendered\n# for sig,chan in tqdm(train1):\n#     temp = []\n#     try:\n#         temp = train_dict[sig]\n#         temp.append(chan)\n#     except KeyError:\n#         #if signal value occurs for the 1st time\n#         temp.append(chan)\n#     finally:\n#         train_dict[sig] = temp\n# print(\"unique signal values in the training data:\",len(train_dict))\n# #-------------------------------------------------\n# sns.distplot(list(train_dict.keys()))\n# plt.show()\n# #----------------------------------------------------\n# train_info_dict={}#signal:Counter of open_channels\n# from collections import Counter\n# for key,value in tqdm(train_dict.items()):\n#     train_info_dict[key] = Counter(value)","e91d71f5":"# counter = 0\n# overlapping_signals={}#A signal value corresponding to more than 1 open_channels in the data\n# for key,value in tqdm(train_info_dict.items()):\n#     if len(value)>1:\n#         overlapping_signals[key] = value\n#         counter+=1\n#         #print(key,value)\n# print(\"No of signal values that overlap:\",counter)\n# #--------------------------------------------------------------------------\n# channel_probs = []#probabilities for overlapping signals ONLY.\n# for sig,chan_dict in tqdm(overlapping_signals.items()):\n#     for i,j in chan_dict.items():\n#         #print(sig,i,np.round(j\/sum(chan_dict.values()),4))\n#         channel_probs.append((sig,i,np.round(j\/sum(chan_dict.values()),4)))","7f465626":"# df_channel_probs = pd.DataFrame(channel_probs)\n# df_channel_probs.columns=['signal','open_channels','prob']#renaming columns\n# df_channel_probs.shape","cb45f31b":"# df_channel_probs.head()","d857a37c":"# result = pd.merge(train, df_channel_probs, how='left', on=['signal','open_channels'])\n# result = result.fillna(1)\n# result.shape","e70e4624":"# train_clean = result\n# train_clean.head()","e0722c88":"# test['prob'] = 0.5\n# test_clean = test\n# test_clean.head()","ef08aaac":"# REMOVING OUTLIERS\n#credits :- https:\/\/www.kaggle.com\/miklgr500\/ghost-drift-and-outliers\nFIRST_EMISSION = (47.857, 47.863)\nSECOND_EMISSION = (364.229, 382.343)\n\ntrain_clean=train\ntest_clean=test\n\ntrain_cwe = train_clean_without_emission = train_clean.loc[(train_clean.time < FIRST_EMISSION[0]) | (train_clean.time > FIRST_EMISSION[1]), :]\ntrain_cwe = train_clean_without_emission = train_cwe.loc[(train_cwe.time < SECOND_EMISSION[0]) | (train_cwe.time > SECOND_EMISSION[1]), :]\n\nSGNAL_SHIFT_CONSTANT = np.exp(1)\n\n#removing the \"Ghost drift\"\n\ntrain_cwe.loc[2000000:2500000, 'signal'] += SGNAL_SHIFT_CONSTANT\ntrain_cwe.loc[4500000:, 'signal'] += SGNAL_SHIFT_CONSTANT\n\ntest_clean.loc[500000:600000, 'signal'] += SGNAL_SHIFT_CONSTANT\ntest_clean.loc[700000:800000, 'signal'] += SGNAL_SHIFT_CONSTANT\n\ntrain = train_cwe\ntest  = test_clean\n\n\ntrain.shape,test.shape","81d0d71f":"sns.distplot(train['signal'])","f3d2724d":"sns.distplot(test['signal'])","d6bef964":"train.head()","792509f6":"from sklearn.mixture import GaussianMixture\nX = np.array(train[['prev','signal']])\ngmm = GaussianMixture(n_components=11,random_state=1).fit(X)\n# labels = gmm.predict(X)\n# plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='plasma');\nprobs = pd.DataFrame(gmm.predict_proba(X)).round(4).mul(100)#------------------------CHANGE HERE\ntemp = pd.concat([train.reset_index(),probs.reset_index()],axis=1)\ntrain = temp\nimport gc\ngc.collect()\ntrain.shape","a4c65833":"X = np.array(test[['prev','signal']])\nprobs = pd.DataFrame(gmm.predict_proba(X)).round(4).mul(100)#------------------------CHANGE HERE\ntemp = pd.concat([test.reset_index(),probs.reset_index()],axis=1)\ntest = temp\nimport gc\ngc.collect()\ntest.shape","33878667":"train=train.drop(columns=['index'],axis=1)\ntest=test.drop(columns=['index'],axis=1)\ntrain.shape,test.shape","6b480b01":"# #Prints the overlap between signals w.r.t no of open channels\n# from tqdm.notebook import tqdm\n# train1 = np.asarray(train[['signal','open_channels']])\n# train_dict = {}\n# for sig,chan in tqdm(train1):\n#     temp = []\n#     try:\n#         temp = train_dict[chan]\n#         temp.append(sig)\n#     except KeyError:\n#         temp.append(sig)\n#     finally:\n#         train_dict[chan] = temp\n# for no_of_channels in range(11):\n#     print('no_of_channels:',no_of_channels,'| percent of overlapped signal',100*np.round(len(np.unique(train_dict[no_of_channels]))\/len(train_dict[no_of_channels]),4),'%')","cb448b6d":"plt.figure(figsize=(30,25)); res = 10\nplt.xticks(np.arange(2000000, 5500000, step=500000))\nplt.yticks(np.arange(-5, 12.5, step=1))\n\nplt.plot(range(0,train.shape[0],res),train.signal[0::res])\n#plt.plot(range(0,train.shape[0],res),train.open_channels[0::res],'magenta') \nplt.plot(range(0,test.shape[0],res),test.signal[0::res],'brown')\n\n\nfor i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'g--')\nfor i in range(10): plt.plot([i*500000,i*500000],[-5,12.5],'black')\n    \n# plt.plot([0,5000000],[0,0],'r--')\n# plt.plot([0,5000000],[1,1],'r--')\n\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal & open channels',size=16); \nplt.title('Training Data Signal and open channels in 10 batches',size=20)\nplt.show()","985e1ac9":"# plt.plot(range(0,1000000,1),test.signal[1000000:],'brown')\n# plt.plot([0,1000000],[0.5,0.5],'r--')\n# plt.plot([0,1000000],[1,1],'r--')\n# plt.plot([0,1000000],[-0.65,-0.65],'r--')","e529d4ac":"# plt.plot(range(0,600000,1),train.signal[1000000:1600000],'brown')\n# plt.plot([0,250000],[0.5,0.5],'r--')\n# plt.plot([0,250000],[1,1],'r--')","706654fa":"# sns.distplot(train.signal.values[1500000:2000000])\n# plt.show()\n\n# sns.distplot(test.signal.values[1000000:])\n# plt.show()","266388fb":"# train_orig = pd.read_csv('..\/input\/data-without-drift\/train_clean.csv')[['time', 'signal', 'open_channels']]\n# condition0 = (train_orig.open_channels.values==0) \n# condition1 = (train_orig.open_channels.values==1) \n# condition2 = (train_orig.open_channels.values==2) \n# condition3 = (train_orig.open_channels.values==3) \n# condition4 = (train_orig.open_channels.values==4) \n# condition5 = (train_orig.open_channels.values==5) \n# condition6 = (train_orig.open_channels.values==6) \n# condition7 = (train_orig.open_channels.values==7) \n# condition8 = (train_orig.open_channels.values==8) \n# condition9 = (train_orig.open_channels.values==9) \n# condition10 = (train_orig.open_channels.values==10) \n\n# plt.figure(figsize=(20,10))\n\n# k=sns.distplot(train_orig[condition0].signal.values,color='magenta',bins=2000)\n# a=sns.distplot(train_orig[condition1].signal.values,color='black',bins=2000)\n# b=sns.distplot(train_orig[condition2].signal.values,color='red',bins=2000)\n# c=sns.distplot(train_orig[condition3].signal.values,color='blue',bins=2000)\n# d=sns.distplot(train_orig[condition4].signal.values,color='green',bins=2000)\n# e=sns.distplot(train_orig[condition5].signal.values,color='yellow',bins=2000)\n# f=sns.distplot(train_orig[condition6].signal.values,color='pink',bins=2000)\n# g=sns.distplot(train_orig[condition7].signal.values,color='grey',bins=2000)\n# h=sns.distplot(train_orig[condition8].signal.values,color='purple',bins=2000)\n# i=sns.distplot(train_orig[condition9].signal.values,color='cyan',bins=2000)\n# j=sns.distplot(train_orig[condition10].signal.values,color='brown',bins=2000)\n# plt.title(\"Original dataset\")\n# plt.show()\n# train_orig = []","f87fdbfd":"\n# # test = pd.read_csv('..\/input\/data-without-drift\/test_clean.csv')[['time', 'signal']]\n# # train = pd.read_csv('..\/input\/data-without-drift\/train_clean.csv')[['time', 'signal', 'open_channels']]\n# # #Normalizing\n# # train_input_mean = train.signal.mean()\n# # train_input_sigma = train.signal.std()\n# # train['signal'] = (train.signal-train_input_mean)\/train_input_sigma\n# # test['signal'] = (test.signal-train_input_mean)\/train_input_sigma\n# # plt.figure(figsize=(30,10))\n# # plt.xticks(np.arange(-2.0, 1.40, step=0.1))\n# # test1=sns.distplot(test.signal.values[0:1000000,],color='red',bins=2000)\n# # test2=sns.distplot(test.signal.values[1000000:,],color='black',bins=2000)\n\n# condition0 = (train.open_channels.values==0) \n# condition1 = (train.open_channels.values==1) \n# condition2 = (train.open_channels.values==2) \n# condition3 = (train.open_channels.values==3) \n# condition4 = (train.open_channels.values==4) \n# condition5 = (train.open_channels.values==5) \n# condition6 = (train.open_channels.values==6) \n# condition7 = (train.open_channels.values==7) \n# condition8 = (train.open_channels.values==8) \n# condition9 = (train.open_channels.values==9) \n# condition10 = (train.open_channels.values==10) \n\n\n# k=sns.distplot(train[condition0].signal.values,color='magenta',bins=2000)\n# a=sns.distplot(train[condition1].signal.values,color='black',bins=2000)\n# b=sns.distplot(train[condition2].signal.values,color='red',bins=2000)\n# c=sns.distplot(train[condition3].signal.values,color='blue',bins=2000)\n# d=sns.distplot(train[condition4].signal.values,color='green',bins=2000)\n# e=sns.distplot(train[condition5].signal.values,color='yellow',bins=2000)\n# f=sns.distplot(train[condition6].signal.values,color='pink',bins=2000)\n# g=sns.distplot(train[condition7].signal.values,color='grey',bins=2000)\n# h=sns.distplot(train[condition8].signal.values,color='purple',bins=2000)\n# i=sns.distplot(train[condition9].signal.values,color='cyan',bins=2000)\n# j=sns.distplot(train[condition10].signal.values,color='brown',bins=2000)\n# plt.title(\"After cleaning dataset\")\n# plt.show()","a5a39af3":"# trainvals = np.array(train[['prev','signal']].apply(lambda x:np.round(x,4)))\n# testvals  = np.array(test[['prev','signal']].apply(lambda x:np.round(x,4)))\n# train_set = set(map(lambda x: frozenset(tuple(x)), trainvals))\n# test_set = set(map(lambda x: frozenset(tuple(x)), testvals))\n# len(train_set),len(test_set),len(train_set.intersection(test_set))","37c04edd":"#2:(132707, 91066, 86313)\n#4:(4623885, 1881919, 219496)\n","6123f4b1":"plt.figure(figsize=(25,15))\nplt.yticks(np.arange(-3, 8, step=1))\nplt.xticks(np.arange(0, 2500000, step=100000));res = 1; \nlet = ['1f', '3', '5', '1f','1f','10','5','10','1f','3']\nplt.plot(range(0,test.shape[0],res),test.signal[0::res])\nplt.plot([0,2000000],[0,0],'r--')\nplt.plot([0,2000000],[1,1],'r--')\n\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(21): plt.plot([j*100000,j*100000],[-5,12.5],'y:')\nfor k in range(4): plt.text(k*500000+200000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7,let[k],size=16)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Test Data Signal - 4 batches - 10 subsamples',size=20)\nplt.show()","cbaec11a":"'channel-1:',min(train[train['open_channels']==2]['signal']),max(train[train['open_channels']==1]['signal'])","a467359f":"# train2 = train.copy()","9a0051e4":"# X_train = np.asarray(train2[['signal','prev']][0:1000000]).reshape((-1,2))\n# y_train = np.asarray(train2.open_channels.values[0:1000000]).reshape((-1,1))\n\n# X_train, y_train = rus.fit_resample(X_train, y_train)\n# print('X_train.shape,y_train.shape:',X_train.shape,y_train.shape)\n\n# plt.hist(y_train)\n","78bf4c1d":"# clf1s = tree.DecisionTreeClassifier(max_depth=1,criterion='entropy')\n# clf1s = clf1s.fit(X_train,y_train)\n# print('Training model low-probability channel')\n# preds = clf1s.predict(X_train)\n# print('f1 validation score =',f1_score(y_train,preds,average='macro'))\n# tree_graph = tree.export_graphviz(clf1s, out_file=None, max_depth = 10,\n#     impurity = False, feature_names = ['signal','prev'], class_names = ['0', '1'],\n#     rounded = True, filled= True )\n# graphviz.Source(tree_graph)  ","bf65fc55":"# #UNDERSAMPLING\n# s=pd.concat([train2[1000000:2000000],train2[2500000:4500000]])\n# from imblearn.under_sampling import RandomUnderSampler\n# rus = RandomUnderSampler(random_state=0)\n# X_resampled, y_resampled = rus.fit_resample(np.asarray(s[['signal','prev']]).reshape(-1,2), np.asarray(s['open_channels']))\n# sns.countplot(y_resampleda\n# plt.show()\n# X_resampled.shape","122c5c56":"\n# %%time\n# import pandas\n# import xgboost\n# from sklearn import model_selection\n# from sklearn.metrics import accuracy_score\n# from sklearn.preprocessing import LabelEncoder\n\n# X = X_sm#np.asarray(train2[['signal','prev']][1000000:]).reshape((-1,2))\n# Y = y_sm#np.asarray(train2.open_channels.values[1000000:])\n# # encode string class values as integers\n# label_encoder = LabelEncoder()\n# label_encoder = label_encoder.fit(Y)\n# label_encoded_y = label_encoder.transform(Y)\n# seed = 1\n# test_size = 0.30\n# X_train, X_test, y_train, y_test = model_selection.train_test_split(X, label_encoded_y, test_size=0.25, random_state=seed)\n# # X_train, X_test, y_train, y_test = model_selection.train_test_split(X_test, y_test, test_size=0.5, random_state=seed)\n# # X_train, X_test, y_train, y_test = model_selection.train_test_split(X_test, y_test, test_size=test_size, random_state=seed)\n# # fit model no training data\n# print('X_train.shape,y_train.shape:',X_train.shape,y_train.shape)\n\n# model2 = xgboost.XGBClassifier(objective='multi:softmax',num_classes=10)\n# model2.fit(X_train, y_train)\n# print(model2)\n# # make predictions for test data\n# y_pred = model2.predict(X_test)\n# predictions = [round(value) for value in y_pred]\n# # evaluate predictions\n# accuracy = accuracy_score(y_test, predictions)\n# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","80893be1":"# from numpy import argmax\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import LabelEncoder\n# from imblearn.over_sampling import SMOTE, ADASYN\n\n# X, y = SMOTE().fit_resample(np.asarray(train[['prev','signal',0,1,2,3,4,5,6,7,8,9,10]]), np.asarray(train['open_channels']))\n# sns.countplot(y)\n# plt.show()\n\n# print('X.shape,y.shape:',X.shape,y.shape)","e890d7eb":"# seed = 1\n# #,0,1,2,3,4,5,6,7,8,9,10\n# #y = LabelEncoder().fit_transform(y)\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10,random_state=seed,shuffle=True)\n# X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.50,random_state=seed,shuffle=True)\n# X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.3,random_state=seed,shuffle=True)\n\n# print(\"After splitting:\",X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n# sns.countplot(y_train)\n# plt.show()\n# x = X.shape[1]#no of features in data matrix\n# X_train = X_train.reshape((-1,1,1,x))\n# X_test = X_test.reshape((-1,1,1,x))\n# y_train=y_train.reshape(-1,1)\n# y_test=y_test.reshape(-1,1)\n# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","d8a113e9":"import tensorflow as tf\nimport keras.backend as K\nfrom sklearn.metrics import f1_score\nimport time \n#,0,1,2,3,4,5,6,7,8,9,10\nclass CustomCallback(tf.keras.callbacks.Callback):\n    def __init__(self, training_data, validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        self.start=0.0\n        self.end=0.0\n        \n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        self.start = time.time()\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        pred = np.array([*map(np.argmax,y_pred_val)]).reshape(-1)\n        target = self.y_val.reshape(-1)\n        score = f1_score(target, pred, average=\"macro\")\n        print(f' F1Macro: {score:.5f}')\n        self.end = time.time()\n        print((self.end-self.start))","3de8407d":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import constraints\n\nclass Attention1(Layer):\n    \"\"\"\n    Multi-headed attention layer.\n    \"\"\"\n    \n    def __init__(self, hidden_size, \n                 num_heads = 8, \n                 attention_dropout=.1,\n                 trainable=True,\n                 name='Attention1'):\n        \n        if hidden_size % num_heads != 0:\n            raise ValueError(\"Hidden size must be evenly divisible by the number of heads.\")\n            \n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.trainable = trainable\n        self.attention_dropout = attention_dropout\n        self.dense = tf.keras.layers.Dense(self.hidden_size, use_bias=False)\n        super(Attention1, self).__init__(name=name)\n\n    def split_heads(self, x):\n        \"\"\"\n        Split x into different heads, and transpose the resulting value.\n        The tensor is transposed to insure the inner dimensions hold the correct\n        values during the matrix multiplication.\n        Args:\n          x: A tensor with shape [batch_size, length, hidden_size]\n        Returns:\n          A tensor with shape [batch_size, num_heads, length, hidden_size\/num_heads]\n        \"\"\"\n        with tf.name_scope(\"split_heads\"):\n            batch_size = tf.shape(x)[0]\n            length = tf.shape(x)[1]\n\n            # Calculate depth of last dimension after it has been split.\n            depth = (self.hidden_size \/\/ self.num_heads)\n\n            # Split the last dimension\n            x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n\n            # Transpose the result\n            return tf.transpose(x, [0, 2, 1, 3])\n    \n    def combine_heads(self, x):\n        \"\"\"\n        Combine tensor that has been split.\n        Args:\n          x: A tensor [batch_size, num_heads, length, hidden_size\/num_heads]\n        Returns:\n          A tensor with shape [batch_size, length, hidden_size]\n        \"\"\"\n        with tf.name_scope(\"combine_heads\"):\n            batch_size = tf.shape(x)[0]\n            length = tf.shape(x)[2]\n            x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n            return tf.reshape(x, [batch_size, length, self.hidden_size])        \n        \n    def call(self, inputs):\n        \"\"\"\n        Apply attention mechanism to inputs.\n        Args:\n          inputs: a tensor with shape [batch_size, length_x, hidden_size]\n        Returns:\n          Attention layer output with shape [batch_size, length_x, hidden_size]\n        \"\"\"\n    \n        q = self.dense(inputs)\n        k = self.dense(inputs)\n        v = self.dense(inputs)\n\n        q = self.split_heads(q)\n        k = self.split_heads(k)\n        v = self.split_heads(v)\n        \n        # Scale q to prevent the dot product between q and k from growing too large.\n        depth = (self.hidden_size \/\/ self.num_heads)\n        q *= depth ** -0.5\n        \n        logits = tf.matmul(q, k, transpose_b=True)\n        # logits += self.bias\n        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n        \n        if self.trainable:\n            weights = tf.nn.dropout(weights, 1.0 - self.attention_dropout)\n        \n        attention_output = tf.matmul(weights, v)\n        attention_output = self.combine_heads(attention_output)\n        attention_output = self.dense(attention_output)\n        return attention_output\n        \n    def compute_output_shape(self, input_shape):\n        return tf.TensorShape(input_shape)","cfab5fe3":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.optimizers import Adam,SGD\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.layers import LSTM, GRU\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import TimeDistributed\n# instantiating the model in the strategy scope creates the model on the TPU\nx= 13\ndef create_model():\n    model = Sequential([\n    TimeDistributed(Conv1D(filters=128, kernel_size=1,activation='relu'), input_shape=(None,1, 13)),\n    TimeDistributed(MaxPooling1D(pool_size=1)),\n    TimeDistributed(Flatten()),\n    Bidirectional(LSTM(128, return_sequences=True)),\n    #Attention1(512),\n    BatchNormalization(),\n    Dropout(0.20),\n    Bidirectional(LSTM(128, return_sequences=True)),\n    BatchNormalization(),\n    Dropout(0.30),\n    Bidirectional(LSTM(128, return_sequences=False)),\n    BatchNormalization(),\n    Dropout(0.20),\n    Dense(16,activation=\"relu\"),\n    Dropout(0.20),\n    Dense(11, activation='softmax')\n    ])\n    return model","497711e9":"create_model().summary()","15389036":"# from IPython.display import SVG\n# SVG(tf.keras.utils.model_to_dot(create_model(), dpi=70).create(prog='dot', format='svg'))","3787c246":"# from tensorflow.keras.callbacks import ModelCheckpoint\n# sv = ModelCheckpoint(best_model_path, monitor='val_loss', verbose=1, save_best_only=True,save_weights_only=True, mode='auto', save_freq='epoch')","72d8d3c4":"test.head()","8895c0bf":"test2=test.copy()\ntest2=np.asarray(test2[['prev','signal',0,1,2,3,4,5,6,7,8,9,10]])\nX_test = test2.reshape((-1,1,1,x))\nX_test.shape","38959bca":"\n# print(\"Tensorflow version \" + tf.__version__)\n# AUTO = tf.data.experimental.AUTOTUNE\n# from kaggle_datasets import KaggleDatasets\n# gcs_path = KaggleDatasets().get_gcs_path('data-without-drift') \n#************************************************************************\n# import tensorflow as tf\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     tpu = None\n\n# if tpu:\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# else:\n#     strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n#************************************************************************\n#with strategy.scope():\n#     model = create_model()\n#     optimizer=Adam(lr=0.001)\n#     model.compile(optimizer=optimizer,loss=SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n# model.summary()\n# model.fit(X_train, y_train, epochs=20,batch_size=128,shuffle=False,validation_data=(X_test, y_test),\n#               callbacks=[CustomCallback((X_train, y_train),(X_test, y_test)),lr_schedule])","9ba0f5bf":"# sys.path.insert(, \"..\/input\/multikfold\/\")\n# from ml_stratifiers import MultilabelStratifiedKFold\n#kf = MultilabelStratifiedKFold(n_splits = 5, random_state = 1)\n","9aab501d":"import pickle\nwith open('..\/input\/iondata\/X_train.pickle', 'rb') as handle1:\n    X_train = pickle.load(handle1)\nwith open('..\/input\/iondata\/y_train.pickle', 'rb') as handle2:\n    y_train = pickle.load(handle2)\n","6c8f9d1f":"# %%time\n\n# from hyperopt import tpe\n# from hyperopt import STATUS_OK\n# from hyperopt import Trials\n# from hyperopt import hp\n# from hyperopt import fmin\n# import warnings\n# warnings.filterwarnings(\"ignore\")  \n# MAX_EVALS = 10\n# def objective(params):\n#     model = create_model(**params)\n#     optimizer=Adam(lr=0.001)\n#     model.compile(optimizer=optimizer,loss=SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n#     model.fit(X_train_opt, y_train_opt, epochs=20, batch_size=256, verbose=False)\n#     loss,accuracy = model.evaluate(X_eval_opt,y_eval_opt, steps=2, verbose=2)\n#     return {'loss': loss, 'params': params, 'status': STATUS_OK}\n# space = {\n#     'a': hp.choice('a', range(128,800)),\n#     'b': hp.choice('b', range(128,800)),\n#     'c': hp.choice('c', range(64,800)),\n#     #'dropout2': hp.uniform('dropout2', 0.2,0.4),\n# }\n# tpe_algorithm = tpe.suggest\n# bayes_trials = Trials()\n# best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials)","b11c90b0":"# train\nall_predictions = []\n\nimport math\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5, random_state=1, shuffle=True)\nlr_schedule = LearningRateScheduler(lambda epoch: 0.001 * math.pow(0.001, math.floor((1+epoch)\/3.0)))\n\n\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    model = create_model()\n    optimizer=Adam(lr=0.001)\n    model.compile(optimizer=optimizer,loss=SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n\n    print( X_tr.shape,y_tr.shape,X_vl.shape,y_vl.shape)\n    model.fit(X_tr, y_tr, epochs=30,batch_size=128,shuffle=True,validation_data=(X_vl, y_vl),\n              callbacks=[CustomCallback((X_tr, y_tr),(X_vl, y_vl)),lr_schedule])\n    print(\"Done training! Now predicting\")\n    all_predictions.append(model.predict(X_test))","1accd072":"# %%time\n# from bayes_opt import BayesianOptimization\n# import numpy as np\n# def fit_with(a,b,c):\n#     model = create_model(a,b,c)\n#     optimizer=Adam(lr=0.001)\n#     model.compile(optimizer=optimizer,loss=SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n#     model.fit(X_train_opt, y_train_opt, epochs=33, batch_size=256, verbose=False)\n#     loss,accuracy = model.evaluate(X_eval_opt,y_eval_opt, steps=12, verbose=0)\n#     print('loss:', np.round(loss,5))\n#     return np.round(accuracy,5)#rounding so that the optimizer converges for 5 decimal places \n# pbounds = {\n#         'a':(256,512),\n#         'b': (256,512),\n#         'c':(256,512)\n#           }\n# optimizer = BayesianOptimization(f=fit_with,pbounds=pbounds,verbose=2,random_state=1)\n# optimizer.maximize()\n# for i, res in enumerate(optimizer.res):\n#     print(\"Iteration {}: \\n\\t{}\".format(i, res))","45787e42":"all_predictions[0].shape","a0dcf73a":"## Predictions for k-fold TRAINING\nsub = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')\navged = sum(all_predictions)\/5.0\npredictions =[*map(np.argmax,avged)]\nsub.iloc[:,1] = np.asarray(predictions)\nsub.to_csv('submission.csv',index=False,float_format='%.4f')\nsub['open_channels'].hist()","9d75fe30":"# ##FOR SIMPLE TRAINING\n# ans=model.predict(X_test)\n# sub = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')\n# predictions =[*map(np.argmax,ans)]\n# sub.iloc[:,1] = np.asarray(predictions)\n# sub.to_csv('submission.csv',index=False,float_format='%.4f')\n# sub['open_channels'].hist()","a611d2d9":"# loss, acc, f1 = model.evaluate(X_test, y_test, verbose=1)\n# print(\"loss={}, acc={}, f1={}\".format(loss, acc,f1))","dca0694d":"\n# import os\n# import numpy\n# import time\n# import random\n# import math\n\n# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import tensorflow as tf\n# from imblearn.over_sampling import SMOTE\n# from sklearn.metrics import f1_score\n\n# from sklearn.preprocessing import MinMaxScaler\n# from sklearn.metrics import roc_curve, auc, confusion_matrix\n# from sklearn.utils import shuffle\n\n# from tensorflow.keras.models import Sequential, load_model\n# from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, Activation, LSTM, BatchNormalization, TimeDistributed, Conv1D, MaxPooling1D\n# from tensorflow.keras.metrics import Precision, Recall\n# from tensorflow.keras.callbacks import LearningRateScheduler\n# from tensorflow.keras import optimizers\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.utils import to_categorical\n# from tensorflow.keras.metrics import Precision, Recall\n# from tensorflow_addons.metrics import F1Score\n\n\n# def mcor(y_true, y_pred):\n#     # Matthews correlation\n#     y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n#     y_pred_neg = 1 - y_pred_pos\n\n#     y_pos = K.round(K.clip(y_true, 0, 1))\n#     y_neg = 1 - y_pos\n\n#     tp = K.sum(y_pos * y_pred_pos)\n#     tn = K.sum(y_neg * y_pred_neg)\n\n#     fp = K.sum(y_neg * y_pred_pos)\n#     fn = K.sum(y_pos * y_pred_neg)\n\n#     numerator = (tp * tn - fp * fn)\n#     denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n#     return numerator \/ (denominator + K.epsilon())\n\n\n# def make_roc(true, predicted):\n\n#     # roc curve plotting for multiple\n\n#     n_classesi = predicted.shape[1]\n\n#     fpr = {}\n#     tpr = {}\n#     roc_auc = {}\n\n#     for i in range(n_classesi):\n#         fpr[i], tpr[i], _ = roc_curve(true[:, i], predicted[:, i])\n#         roc_auc[i] = auc(fpr[i], tpr[i])\n\n#     plt.figure()\n#     lw = 2\n#     plt.plot(fpr[2], tpr[2], color='darkorange',\n#              lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n#     plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n#     plt.xlim([0.0, 1.0])\n#     plt.ylim([0.0, 1.0])\n#     plt.xlabel('False Positive Rate')\n#     plt.ylabel('True Positive Rate')\n#     plt.title('Receiver operating characteristic example')\n#     plt.legend(loc=\"lower right\")\n#     plt.show()\n\n#     plt.figure(2)\n#     plt.xlim(0, 1)\n#     plt.ylim(0, 1)\n#     colors = ['aqua', 'darkorange', 'cornflowerblue',\n#                     'red', 'black', 'yellow']\n#     for i in range(n_classesi):\n#         plt.plot(fpr[i], tpr[i], color=color[i], lw=lw,\n#                  label='ROC curve of class {0} (area = {1:0.2f})'\n#                  ''.format(i, roc_auc[i]))\n\n#     plt.xlabel('False Positive Rate (1 - Specificity)')\n#     plt.ylabel('True Positive Rate (Sensitivity)')\n#     plt.title('Zooom in View: Some extension of ROC to multi-class')\n#     plt.legend(loc=\"lower right\")\n#     plt.show()\n\n\n# def step_decay(epoch):\n#     # Learning rate scheduler object\n#     initial_lrate = 0.001\n#     drop = 0.001\n#     epochs_drop = 3.0\n#     lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)\/epochs_drop))\n#     return lrate\n\n\n# '''\n# ############# SET UP RUN HERE ####################\n# '''\n\n# batch_size = 256\n\n\n\n# df = pd.read_csv('outfinaltest161.csv', header=None)\n# dataset = df.values.astype('float64')\n# timep = dataset[:, 0]\n# maxer = np.amax(dataset[:, 2])\n# maxeri = maxer.astype('int')\n# maxchannels = maxeri\n# idataset = dataset[:, 2].astype(int)\n# scaler = MinMaxScaler(feature_range=(0, 1))\n# dataset = scaler.fit_transform(dataset)\n\n# # train and test set split and reshape:\n# train_size = int(len(dataset) * 0.80)\n# modder = math.floor(train_size\/batch_size)\n# train_size = int(modder*batch_size)\n# test_size = int(len(dataset) - train_size)\n# modder = math.floor(test_size\/batch_size)\n# test_size = int(modder*batch_size)\n\n# print(f'training set = {train_size}')\n# print(f'test set = {test_size}')\n# print(f'total length = {test_size + train_size}')\n\n\n# x_train = dataset[:, 1]\n# y_train = idataset[:]\n# x_train = x_train.reshape((len(x_train), 1))\n# y_train = y_train.reshape((len(y_train), 1))\n\n\n# sm = SMOTE(sampling_strategy='auto', random_state=42)\n# X_res, Y_res = sm.fit_sample(x_train, y_train)\n\n# yy_res = Y_res.reshape((len(Y_res), 1))\n# yy_res = to_categorical(yy_res, num_classes=maxchannels+1)\n# xx_res, yy_res = shuffle(X_res, yy_res)\n\n\n# trainy_size = int(len(xx_res) * 0.80)\n# modder = math.floor(trainy_size\/batch_size)\n# trainy_size = int(modder*batch_size)\n# testy_size = int(len(xx_res) - trainy_size)\n# modder = math.floor(testy_size\/batch_size)\n# testy_size = int(modder*batch_size)\n\n# print('training set= ', trainy_size)\n# print('test set =', testy_size)\n# print('total length', testy_size+trainy_size)\n\n\n# in_train, in_test = xx_res[0:trainy_size,\n#                            0], xx_res[trainy_size:trainy_size+testy_size, 0]\n# target_train, target_test = yy_res[0:trainy_size,\n#                                    :], yy_res[trainy_size:trainy_size+testy_size, :]\n# in_train = in_train.reshape(len(in_train), 1, 1, 1)\n# in_test = in_test.reshape(len(in_test), 1, 1, 1)\n\n\n# # validation set!!\n# df_val = pd.read_csv('outfinaltest78.csv', header=None)\n# data_val = df_val.values.astype('float64')\n\n# idataset2 = data_val[:, 2].astype(int)\n\n# val_set = data_val[:, 1]\n# scaler = MinMaxScaler(feature_range=(0, 1))\n# val_set = scaler.fit_transform(val_set.reshape(-1,1))\n# val_set = val_set.reshape(len(val_set), 1, 1, 1)\n# val_target = data_val[:, 2]\n# val_target = to_categorical(val_target, num_classes=maxchannels+1)\n\n\n# # model starts..\n\n# newmodel = Sequential()\n# timestep = 1\n# input_dim = 1\n# newmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=1,\n#                                     activation='relu'), input_shape=(None, timestep, input_dim)))\n# newmodel.add(TimeDistributed(MaxPooling1D(pool_size=1)))\n# newmodel.add(TimeDistributed(Flatten()))\n\n# newmodel.add(LSTM(256, activation='relu', return_sequences=True))\n# newmodel.add(BatchNormalization())\n# newmodel.add(Dropout(0.2))\n\n# newmodel.add(LSTM(256, activation='relu', return_sequences=True))\n# newmodel.add(BatchNormalization())\n# newmodel.add(Dropout(0.2))\n\n# newmodel.add(LSTM(256, activation='relu'))\n# newmodel.add(BatchNormalization())\n# newmodel.add(Dropout(0.2))\n\n# newmodel.add(Dense(maxchannels+1))\n# newmodel.add(Activation('softmax'))\n\n\n# newmodel.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.001, momentum=0.9, nesterov=False), metrics=[\n#                  'accuracy', Precision(), Recall(), F1Score(num_classes=maxchannels+1, average='micro')])\n\n\n# lrate = LearningRateScheduler(step_decay)\n\n\n# epochers = 2\n# history = newmodel.fit(x=in_train, y=target_train, initial_epoch=0, epochs=epochers, batch_size=batch_size, callbacks=[\n#                        lrate], verbose=1, shuffle=False, validation_data=(in_test, target_test))\n\n\n# # prediction for test set\n# predict = newmodel.predict(in_test, batch_size=batch_size)\n\n# # prediction for val set\n# predict_val = newmodel.predict(val_set, batch_size=batch_size)\n\n\n# class_predict = np.argmax(predict, axis=-1)\n# class_predict_val = np.argmax(predict_val, axis=-1)\n# class_target = np.argmax(target_test, axis=-1)\n# class_target_val = np.argmax(val_target, axis=-1)\n\n\n# cm_test = confusion_matrix(class_target, class_predict)\n# cm_val = confusion_matrix(idataset2, class_predict_val)\n\n# rnd = 1\n# # summarize history for accuracy\n# plt.plot(history.history['accuracy'])\n# plt.plot(history.history['val_accuracy'])\n# plt.title('model accuracy')\n# plt.ylabel('accuracy')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='lower right')\n# plt.savefig(str(rnd)+'acc.png')\n# plt.show()\n\n# # summarize history for loss\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('model loss')\n# plt.ylabel('loss')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper right')\n# plt.savefig(str(rnd)+'loss.png')\n# plt.show()\n\n\n# plotlen = test_size\n# lenny = 2000\n\n# plt.figure(figsize=(30, 6))\n# plt.subplot(2, 1, 1)\n# # temp=scaler.inverse_transform(dataset)\n# plt.plot(xx_res[trainy_size:trainy_size+lenny, 0],\n#          color='blue', label=\"some raw data\")\n# plt.title(\"The raw test\")\n\n# plt.subplot(2, 1, 2)\n# plt.plot(class_target[:lenny], color='black', label=\"the actual idealisation\", drawstyle='steps-mid')\n\n# line, = plt.plot(class_predict[:lenny], color='red',\n#                  label=\"predicted idealisation\", drawstyle='steps-mid')\n# plt.setp(line, linestyle='--')\n# plt.xlabel('timepoint')\n# plt.ylabel('current')\n# # plt.savefig(str(rnd)+'data.png')\n# plt.legend()\n# plt.show()\n\n\n# # newmodel.save('nmn_oversampled_deepchanel6_5.h5')\n\n# make_roc(val_target, predicted_val)","deb8ca01":"# train2 = train.copy()","7faa0b48":"# test2.tail()","f31522b9":"# test2=test.copy()\n# test2['prev']=0\n","24e2719d":"# test2['prev'][0+1:500000] = test2['signal'][0:500000-1]\n# test2['prev'][500000+1:1000000] = test2['signal'][500000:1000000-1]\n# test2['prev'][1000000+1:1500000] = test2['signal'][1000000:1500000-1]\n# test2['prev'][1500000+1:2000000] = test2['signal'][1500000:2000000-1]\n","d909b67b":"# test2['prob']=0.5\n","674ce1aa":"# #For neural-network\n# sub = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')\n# test2=np.asarray(test2[['signal']])\n# test2 = test2.reshape((-1,1,1))\n# test2.shape\n# predictions =[*map(np.argmax,newmodel.predict(test2))]\n# sub.iloc[:,1] = np.asarray(predictions)","355ec88c":"# sub = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')\n\n# a = 0 # SUBSAMPLE A, Model 1f\n# sub.iloc[100000*a:100000*(a+1),1] = newmodel.predict(np.asarray(test2[['signal']][100000*a:100000*(a+1)]))\n\n# a = 1 # SUBSAMPLE B, Model 3\n# sub.iloc[100000*a:100000*(a+1),1] = newmodel.predict(np.asarray(test2[['signal']][100000*a:100000*(a+1)]))\n\n\n# a = 2 # SUBSAMPLE C, Model 5\n# sub.iloc[100000*a:100000*(a+1),1] = newmodel.predict(np.asarray(test2[['signal']][100000*a:100000*(a+1)]))\n\n# a = 3 # SUBSAMPLE D, Model 1f\n# sub.iloc[100000*a:100000*(a+1),1] = newmodel.predict(np.asarray(test2[['signal']][100000*a:100000*(a+1)]))\n\n# a = 4 # SUBSAMPLE E, Model 1f\n# sub.iloc[100000*a:100000*(a+1),1] = newmodel.predict(np.asarray(test2[['signal']][100000*a:100000*(a+1)]))\n\n# a = 5 # SUBSAMPLE F, Model 10\n# sub.iloc[100000*a:100000*(a+1),1] = newmodel.predict(np.asarray(test2[['signal']][100000*a:100000*(a+1)]))\n\n\n# a = 6 # SUBSAMPLE G, Model 5\n# sub.iloc[100000*a:100000*(a+1),1] = newmodel.predict(np.asarray(test2[['signal']][100000*a:100000*(a+1)]))\n\n# a = 7 # SUBSAMPLE H, Model 10\n# sub.iloc[100000*a:100000*(a+1),1] = newmodel.predict(np.asarray(test2[['signal']][100000*a:100000*(a+1)]))\n\n# a = 8 # SUBSAMPLE I, Model 1s\n# sub.iloc[100000*a:100000*(a+1),1] = newmodel.predict(np.asarray(test2[['signal']][100000*a:100000*(a+1)]))\n\n# a = 9 # SUBSAMPLE J, Model 3\n# sub.iloc[100000*a:100000*(a+1),1] = newmodel.predict(np.asarray(test2[['signal']][100000*a:100000*(a+1)]))\n\n#  # BATCHES 3 AND 4 seem to be generated from Model 1s\n# #sub.iloc[1000000:2000000,1] = clf1s.predict(test2.signal.values[1000000:2000000].reshape((-1,1)))\n# sub.iloc[1000000:2000000,1] = newmodel.predict(np.asarray(test2[['signal']][1000000:2000000]))","db36cf95":"# sub = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')\n\n# a = 0 # SUBSAMPLE A, Model 1s\n# #sub.iloc[100000*a:100000*(a+1),1] = clf1s.predict(test2[['signal','prev']][100000*a:100000*(a+1)].reshape((-1,1)))\n# sub.iloc[100000*a:100000*(a+1),1] =clf1s.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n\n# a = 1 # SUBSAMPLE B, Model 3\n# y_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n# predictions = [*map(np.argmax,y_pred)]\n# sub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\n# a = 2 # SUBSAMPLE C, Model 5\n# y_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n# predictions =[*map(np.argmax,y_pred)]\n# sub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\n# a = 3 # SUBSAMPLE D, Model 1s\n# #sub.iloc[100000*a:100000*(a+1),1] = clf1s.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\n# sub.iloc[100000*a:100000*(a+1),1] =clf1s.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n\n# a = 4 # SUBSAMPLE E, Model 1f\n# y_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n# predictions = [*map(np.argmax,y_pred)]\n# sub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\n# a = 5 # SUBSAMPLE F, Model 10\n# y_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n# predictions = [*map(np.argmax,y_pred)]\n# sub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\n# a = 6 # SUBSAMPLE G, Model 5\n# y_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n# predictions = [*map(np.argmax,y_pred)]\n# sub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\n# a = 7 # SUBSAMPLE H, Model 10\n# y_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n# predictions = [*map(np.argmax,y_pred)]\n# sub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\n# a = 8 # SUBSAMPLE I, Model 1s\n# #sub.iloc[100000*a:100000*(a+1),1] = clf1s.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\n# sub.iloc[100000*a:100000*(a+1),1] =clf1s.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n\n# a = 9 # SUBSAMPLE J, Model 3\n# y_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n# predictions = [*map(np.argmax,y_pred)]\n# sub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\n#  # BATCHES 3 AND 4 seem to be generated from Model 1s\n# #sub.iloc[1000000:2000000,1] = clf1s.predict(test2.signal.values[1000000:2000000].reshape((-1,1)))\n# sub.iloc[1000000:2000000,1] =clf1s.predict(np.asarray(test2[['signal','prev']][1000000:2000000]))","3f3ad532":"# plt.figure(figsize=(20,5))\n# plt.ylim(bottom=-1,top=12)\n# plt.yticks(np.arange(-1, 12, step=1))\n# plt.ylabel('Channels Open',size=16)\n# res = 1000\n# plt.plot(range(0,test.shape[0],res),sub.open_channels[0::res])\n# for i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\n# for i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\n# for k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\n# for k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\n# plt.title('Test Data Predictions',size=16)\n# plt.show()","4f98458e":"# sub.to_csv('submission.csv',index=False,float_format='%.4f')","a5325c14":"# sub['open_channels'].hist()\n","c8ba464e":"# sub['open_channels'].hist()","e83f088e":"# import pickle\n# # save model to file\n# with open('y_test.pickle', 'wb') as handle:\n#     pickle.dump(y_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n# #X_train.shape, X_test.shape, y_train.shape, y_test","c1eb50da":"# # # some time later...\n# import pickle\n# with open('X_test.pickle', 'rb') as handle:\n#     b = pickle.load(handle)\n# # # load model from file\n# # loaded_model = pickle.load(open(\"pima.pickle.dat\", \"rb\"))","09fc2d3c":"### 1) low probability model","e9981821":"## What data was collected?  \nelectrical signal  \nmaximum number of open channels  \ntime of collection  \n,constitute a data instance.\n## How was the data collected? \nOne data instance is recorded every 0.1 millisecond. So, in 1 second there are 10,000 data instances recorded.  \nThe data was recorded in batches of 50 seconds. Therefore, one batch contains 500,000 rows.  \nTraining data contains 10 batches: 5 million rows  \nTest data contains 4 batches: 2 million rows  ","86b40857":"#### 1553707(1.5M) signal values in test data correspond to channel 0 and channel 1 of the training data's 697764 unique signal values","63fe5383":"# Make Five Simple Models\nWe will make one model for each different type of signal we observed above.","d032c77b":"### Conclusion: The overlap has increased by 1% from original to clean data set","fb4d7fbb":"## Analysis from above EDA:-  \nFrom the plots above, it looks like they used 5 different synthetic models.   \nOne model produced maximum 1 open channel with low probability (batches 1 and 2).   \nOne model produced maximum 1 open channel with high probability (batches 3 and 7).   \nOne model produced maximum 3 open channels (batches 4 and 8).   \nOne model produced maximum 5 open channels (batches 6 and 9) and  \nOne model produced maximum 10 open channels (batches 5 and 10).   \n\nAccording to the paper [here][1], the data is synthesized. Also \"electrophysiological\" noise and drift were added.  \nDrift is a signal bias causing the signal to no longer be a horizontal line like batches 2, 7, 8, 9, 10.\n\n> Data description and dataset construction. Ion channel dwell-times were\nsimulated using the method of Gillespie 43 from published single channel models.\nChannels are assumed to follow a stochastic Markovian process and transition\nfrom one state to the next simulated by randomly sampling from a lifetime\nprobability distribution calculated for each state. Authentic \u201celectrophysiological\u201d\nnoise was added to these events by passing the signal through a patch-clamp\namplifier and recording it back to file with CED\u2019s Signal software via an Axon\nelectronic \u201cmodel cell\u201d. In some datasets additional drift was applied to the final\ndata with Matlab. Two different stochastic gating models, (termed M1 and M2)\nwere used to generate semi-synthetic ion channel data. M1 is a low open probability model from ref. 41 (Fig. 3a, b), typically no more than one ion channel opens\nsimultaneously. Model M2 is from refs. 42,44 and has a much higher open probability (Fig. 3c, d), consequently up to five channels opened simultaneously and there are few instances of zero channels open.\n\n\n[1]: https:\/\/www.nature.com\/articles\/s42003-019-0729-3\n","2443753b":"### HMM\nApplies per batch.  \nThere are 11 states, cannot observe them directly.  \na 3-state HMM with a 2-d Gaussian emission model. ","756baaf4":"## My approach: Make 2 models:-  \n1.for low-probability channels : batch1,batch2 in training data  \n2.for high-probability channels: batch3 to batch10 in training data","82e170aa":"## Overlap after cleaning the dataset  \nno_of_channels: 0 | percent of overlapped signal 19.22%    \nno_of_channels: 1 | percent of overlapped signal 48.55 %  \nno_of_channels: 2 | percent of overlapped signal 44.29 %  \nno_of_channels: 3 | percent of overlapped signal 46.66%  \nno_of_channels: 4 | percent of overlapped signal 54.57%  \nno_of_channels: 5 | percent of overlapped signal 58.04%  \nno_of_channels: 6 | percent of overlapped signal 57.86%  \nno_of_channels: 7 | percent of overlapped signal 55.45%  \nno_of_channels: 8 | percent of overlapped signal 55.47%  \nno_of_channels: 9 | percent of overlapped signal 58.2%  \nno_of_channels: 10 | percent of overlapped signal 73.24%","4830ce6c":"### glossary:-\nunique(signal) = 2173186  \nunique(signal,open_channels) = 2173186 - 49149 = 2124037 --- prob is assigned to this. For 5000000-2124037 rows, prob=1.0.  \nnonunique(signal,open_channels) = no of overlapping signals = 49149 ","d9ba49be":"# Display Test Predictions","4f1114f7":"The channels are classified broadly into 2 categories depending on whether they have a low-probability of opening(low conductance) or a high-probability of opening(high conductance):-  \n1.Batch1 and Batch2 represent low-probability channels: binary classification.>>>>Model1  \n2.Other Batches represnet high-probabaility channels: multi-class classification.>>>>Model2","2fceb5d0":"### 10channels: 2000000:2500000 and 4500000:5000000","21774a72":"### Magenta--channels  Signal---Blue","f5da81e1":"### 2)high probability model","8cf8105d":"# Predict Test\n","84515fc8":"## Overlap present in the original dataset\nno_of_channels: 0 | percent of overlapped signal 18.4%    \nno_of_channels: 1 | percent of overlapped signal 46.43%  \nno_of_channels: 2 | percent of overlapped signal 44.11%  \nno_of_channels: 3 | percent of overlapped signal 45.91%  \nno_of_channels: 4 | percent of overlapped signal 52.98%  \nno_of_channels: 5 | percent of overlapped signal 57%  \nno_of_channels: 6 | percent of overlapped signal 56.46%  \nno_of_channels: 7 | percent of overlapped signal 53.6%    \nno_of_channels: 8 | percent of overlapped signal 53.78%  \nno_of_channels: 9 | percent of overlapped signal 56.61%  \nno_of_channels:10 | percent of overlapped signal 72.58%  \n\n**inference:** for 0 channels we can only be 72% sure during prediction, for 1 channels we can only be 54% sure during prediction, so on and so forth","3e96d4e3":"# Test Data\nLet's display the test data signal","d303b928":"1s ---> atmost 1 open channel with low prob  \n1f ---> atmost 1 open channel with high prob  \n 3 ---> atmost 3 open channels with high prob  \n 5 ---> atmost 5 open channels with high prob  \n10 ---> atmost 10 open channels with high prob  "}}