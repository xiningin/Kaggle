{"cell_type":{"da40102d":"code","ec3955eb":"code","2d9b0eb9":"code","033e6f42":"code","b2c3cdab":"code","a2ff5c62":"code","a16863e4":"code","763c102b":"code","f6515272":"code","cadbef23":"code","701456de":"code","87111de9":"markdown","5edc74e9":"markdown","5ab83977":"markdown","db5b2a8d":"markdown","80c737b6":"markdown","c84b8cfe":"markdown","59c401c5":"markdown","64477fbf":"markdown","e53e9f94":"markdown","8ad499a2":"markdown","269ab9ad":"markdown"},"source":{"da40102d":"import cv2\nimport imageio\nfrom PIL import Image\nimport numpy as np\nfrom keras.preprocessing.image import img_to_array\nfrom keras import backend as K\nfrom keras.layers import Layer\nfrom keras.layers import Input\nfrom keras.layers.convolutional import Convolution2D\nfrom keras.layers.core import Activation, Reshape\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model\nimport argparse\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","ec3955eb":"class MaxPoolingWithArgmax2D(Layer):\n\n    def __init__(\n            self,\n            pool_size=(2, 2),\n            strides=(2, 2),\n            padding='same',\n            **kwargs):\n        super(MaxPoolingWithArgmax2D, self).__init__(**kwargs)\n        self.padding = padding\n        self.pool_size = pool_size\n        self.strides = strides\n\n    def call(self, inputs, **kwargs):\n        padding = self.padding\n        pool_size = self.pool_size\n        strides = self.strides\n        ksize = [1, pool_size[0], pool_size[1], 1]\n        padding = padding.upper()\n        strides = [1, strides[0], strides[1], 1]\n        output, argmax = tf.nn.max_pool_with_argmax(\n            inputs,\n            ksize=ksize,\n            strides=strides,\n            padding=padding)\n\n        argmax = K.cast(argmax, K.floatx())\n        return [output, argmax]\n\n    def compute_output_shape(self, input_shape):\n        ratio = (1, 2, 2, 1)\n        output_shape = [\n            dim \/\/ ratio[idx]\n            if dim is not None else None\n            for idx, dim in enumerate(input_shape)]\n        output_shape = tuple(output_shape)\n        return [output_shape, output_shape]\n\n    def compute_mask(self, inputs, mask=None):\n        return 2 * [None]\n\n\nclass MaxUnpooling2D(Layer):\n    def __init__(self, size=(2, 2), **kwargs):\n        super(MaxUnpooling2D, self).__init__(**kwargs)\n        self.size = size\n\n    def call(self, inputs, output_shape=None):\n        updates, mask = inputs[0], inputs[1]\n        with tf.compat.v1.variable_scope(self.name):\n            mask = K.cast(mask, 'int32')\n            input_shape = tf.shape(updates, out_type='int32')\n\n            if output_shape is None:\n                output_shape = (\n                    input_shape[0],\n                    input_shape[1] * self.size[0],\n                    input_shape[2] * self.size[1],\n                    input_shape[3])\n\n            ret = tf.scatter_nd(K.expand_dims(K.flatten(mask)),\n                                  K.flatten(updates),\n                                  [K.prod(output_shape)])\n\n            input_shape = updates.shape\n            out_shape = [-1,\n                         input_shape[1] * self.size[0],\n                         input_shape[2] * self.size[1],\n                         input_shape[3]]\n        return K.reshape(ret, out_shape)\n\n    def compute_output_shape(self, input_shape):\n        mask_shape = input_shape[1]\n        return (\n                mask_shape[0],\n                mask_shape[1]*self.size[0],\n                mask_shape[2]*self.size[1],\n                mask_shape[3]\n                )","2d9b0eb9":"    \ndef segnet(input_shape, n_labels, kernel=3, pool_size=(2, 2), output_mode=\"softmax\"):\n    # encoder\n    inputs = Input(shape=input_shape)\n\n    conv_1 = Convolution2D(64, (kernel, kernel), padding=\"same\")(inputs)\n    conv_1 = BatchNormalization()(conv_1)\n    conv_1 = Activation(\"relu\")(conv_1)\n    conv_2 = Convolution2D(64, (kernel, kernel), padding=\"same\")(conv_1)\n    conv_2 = BatchNormalization()(conv_2)\n    conv_2 = Activation(\"relu\")(conv_2)\n\n    pool_1, mask_1 = MaxPoolingWithArgmax2D(pool_size)(conv_2)\n\n    conv_3 = Convolution2D(128, (kernel, kernel), padding=\"same\")(pool_1)\n    conv_3 = BatchNormalization()(conv_3)\n    conv_3 = Activation(\"relu\")(conv_3)\n    conv_4 = Convolution2D(128, (kernel, kernel), padding=\"same\")(conv_3)\n    conv_4 = BatchNormalization()(conv_4)\n    conv_4 = Activation(\"relu\")(conv_4)\n\n    pool_2, mask_2 = MaxPoolingWithArgmax2D(pool_size)(conv_4)\n\n    conv_5 = Convolution2D(256, (kernel, kernel), padding=\"same\")(pool_2)\n    conv_5 = BatchNormalization()(conv_5)\n    conv_5 = Activation(\"relu\")(conv_5)\n    conv_6 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_5)\n    conv_6 = BatchNormalization()(conv_6)\n    conv_6 = Activation(\"relu\")(conv_6)\n\n\n    pool_3, mask_3 = MaxPoolingWithArgmax2D(pool_size)(conv_6)\n\n    conv_8 = Convolution2D(512, (kernel, kernel), padding=\"same\")(pool_3)\n    conv_8 = BatchNormalization()(conv_8)\n    conv_8 = Activation(\"relu\")(conv_8)\n    \n    \n    pool_4, mask_4 = MaxPoolingWithArgmax2D(pool_size)(conv_8)\n\n    conv_11 = Convolution2D(512, (kernel, kernel), padding=\"same\")(pool_4)\n    conv_11 = BatchNormalization()(conv_11)\n    conv_11 = Activation(\"relu\")(conv_11)\n   \n\n    pool_5, mask_5 = MaxPoolingWithArgmax2D(pool_size)(conv_11)\n    print(\"Build enceder done..\")\n\n    # decoder\n\n    unpool_1 = MaxUnpooling2D(pool_size)([pool_5, mask_5])\n\n    conv_14 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_1)\n    conv_14 = BatchNormalization()(conv_14)\n    conv_14 = Activation(\"relu\")(conv_14)\n    \n\n    unpool_2 = MaxUnpooling2D(pool_size)([conv_14, mask_4])\n\n    conv_17 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_2)\n    conv_17 = BatchNormalization()(conv_17)\n    conv_17 = Activation(\"relu\")(conv_17)\n    conv_19 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_17)\n    conv_19 = BatchNormalization()(conv_19)\n    conv_19 = Activation(\"relu\")(conv_19)\n\n    unpool_3 = MaxUnpooling2D(pool_size)([conv_19, mask_3])\n\n    conv_20 = Convolution2D(256, (kernel, kernel), padding=\"same\")(unpool_3)\n    conv_20 = BatchNormalization()(conv_20)\n    conv_20 = Activation(\"relu\")(conv_20)\n    conv_21 = Convolution2D(128, (kernel, kernel), padding=\"same\")(conv_20)\n    conv_21 = BatchNormalization()(conv_21)\n    conv_21 = Activation(\"relu\")(conv_21)\n   \n    unpool_4 = MaxUnpooling2D(pool_size)([conv_21, mask_2])\n\n    conv_23 = Convolution2D(128, (kernel, kernel), padding=\"same\")(unpool_4)\n    conv_23 = BatchNormalization()(conv_23)\n    conv_23 = Activation(\"relu\")(conv_23)\n    conv_24 = Convolution2D(64, (kernel, kernel), padding=\"same\")(conv_23)\n    conv_24 = BatchNormalization()(conv_24)\n    conv_24 = Activation(\"relu\")(conv_24)\n\n    unpool_5 = MaxUnpooling2D(pool_size)([conv_24, mask_1])\n\n    conv_25 = Convolution2D(64, (kernel, kernel), padding=\"same\")(unpool_5)\n    conv_25 = BatchNormalization()(conv_25)\n    conv_25 = Activation(\"relu\")(conv_25)\n\n    conv_26 = Convolution2D(n_labels, (1, 1), padding=\"valid\")(conv_25)\n    conv_26 = BatchNormalization()(conv_26)\n    conv_26 = Reshape(\n        (input_shape[0] * input_shape[1], n_labels),\n        input_shape=(input_shape[0], input_shape[1], n_labels),\n    )(conv_26)\n\n    outputs = Activation(output_mode)(conv_26)\n    print(\"Build decoder done..\")\n\n    model = Model(inputs=inputs, outputs=outputs, name=\"SegNet\")\n\n    return model\n\n\nmodel = segnet((224,224,3), n_labels=5 ,kernel=3, pool_size=(2,2), output_mode=\"softmax\")\n\nprint(model.summary())\n\nprint(\"tf.__version__ is\", tf.__version__)\nprint(\"tf.keras.__version__ is:\", tf.keras.__version__)","033e6f42":"def exr_to_jpg(path):\n    im = imageio.imread(path)\n    im_gamma_correct = np.clip(np.power(im, 0.45), 0, 1)\n    im_fixed = Image.fromarray(np.uint8(im_gamma_correct*255))\n    return im_fixed\n\ndef category_label(labels, dims, n_labels):\n    x = np.zeros([dims[0], dims[1], n_labels])\n    for i in range(dims[0]):\n        for j in range(dims[1]):\n            f=int(labels[i,j])\n            x[i, j, f] = 1\n    x = x.reshape(dims[0] * dims[1], n_labels)\n    return x\ndef colorize(img):\n    w=img.shape[0]\n    h=img.shape[1]\n    z=img.shape[2]\n    l=np.zeros((w,h,3))\n    for i in range(w):\n        for j in range(h):\n            if img[i,j,0]==1:\n                l[i,j,0]=0\n                l[i,j,1]=0\n                l[i,j,2]=0\n            elif img[i,j,1]==1:\n                l[i,j,0]=255\n                l[i,j,1]=0\n                l[i,j,2]=0\n            elif img[i,j,2]==1:\n                l[i,j,0]=0\n                l[i,j,1]=255\n                l[i,j,2]=0\n            elif img[i,j,3]==1:\n                l[i,j,0]=0\n                l[i,j,1]=0\n                l[i,j,2]=255\n            elif img[i,j,4]==1:\n                l[i,j,0]=238\n                l[i,j,1]=197\n                l[i,j,2]=145\n    return l\n\ndef class_pixels(img):\n    w=img.shape[0]\n    h=img.shape[1]\n    z=img.shape[2]\n    l=np.zeros((w,h,z))\n    for i in range(w):\n        for j in range(h):\n            for f in range(z-1):\n                if img[i,j,f]==np.max([img[i,j,0],img[i,j,1],img[i,j,2],img[i,j,3],img[i,j,4]]):\n                    l[i,j,f]=1\n    return l\n\ndef data_gen_small(img_dir,mask_dir,depth_dir,liste, batch_size, dims=(224,224), n_labels=5):\n    while True:\n        ix = np.random.choice(liste, batch_size)\n        imgs = []\n        labels = []\n        for index in ix:\n            # images\n            img_path = img_dir[index]\n            original_img = exr_to_jpg(img_path)\n            array_img = img_to_array(original_img)\/255\n            imgs.append(array_img)\n            \n            # masks\n            mask_path = mask_dir[index]\n            original_mask=cv2.imread(mask_path,cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH)\n            array_mask = category_label(original_mask[:, :, 0], dims, n_labels)\n            labels.append(array_mask)\n            \n        imgs = np.array(imgs)\n        labels = np.array(labels)\n        yield imgs, labels","b2c3cdab":"import os\nrgb=[]\ndepth=[]\nmask=[]\nnode=[]\n\nfor dirs,subdir,files in os.walk('..\/input\/synthetic-rgbd-images-of-plants\/dataset of synthetic rgb-d plants\/rgb_map'):\n    for file_name in files:\n        if file_name.endswith(\".exr\"):\n            path_file=dirs+os.sep+file_name\n            depth_file='..\/input\/synthetic-rgbd-images-of-plants\/dataset of synthetic rgb-d plants\/depth_map\/profondeur_map\/'+file_name\n            mask_file='..\/input\/synthetic-rgbd-images-of-plants\/dataset of synthetic rgb-d plants\/semantic_map\/segmentation2_map\/'+file_name\n            node_file='..\/input\/synthetic-rgbd-images-of-plants\/dataset of synthetic rgb-d plants\/nodes_map\/internoeuds_map\/'+file_name\n            rgb.append(path_file)\n            depth.append(depth_file)\n            mask.append(mask_file)\n            node.append(node_file)\n            \nliste=np.arange(1,10000)\nnp.random.shuffle(liste)\n\ntrain_list=liste[0:8000]\nval_list=liste[8000:9000]\ntest_list=liste[9000:9999]\n\n\ntrain_gen = data_gen_small(rgb\n,mask,depth,liste=train_list,batch_size=16,dims=(224,224),n_labels=5)\nval_gen=data_gen_small(rgb\n,mask,depth,liste=val_list,batch_size=16,dims=(224,224),n_labels=5)\ntest_gen = data_gen_small(rgb\n,mask,depth,liste=test_list,batch_size=1,dims=(224,224),n_labels=5)","a2ff5c62":"def dice_coef(y_true, y_pred): \n    \n    epsilon=1e-6\n    axes = tuple(range(1, len(y_pred.shape)-1)) \n    numerator = 2. * K.sum(y_pred * y_true, axes)\n    denominator = K.sum(K.square(y_pred) + K.square(y_true), axes)\n    \n    return K.mean((numerator + epsilon) \/ (denominator + epsilon))\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)","a16863e4":"model.compile(loss=dice_coef_loss, optimizer='adam', metrics=[\"accuracy\",dice_coef])","763c102b":"history=model.fit_generator(\n        train_gen,\n        steps_per_epoch=500,\n        epochs=50,\n        validation_data=val_gen,\n        validation_steps=62\n    )","f6515272":"model.load_weights('..\/input\/pre-model-2\/only_rgb_dice.hdf5')","cadbef23":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# plotting of training and validation loss curves\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","701456de":"h=0\nfor i in test_list:\n    if h<50:\n        \n        img_path = rgb[i]\n        original_img = exr_to_jpg(img_path)\n            \n        \n        plt.figure(figsize=(15,15))\n        plt.subplot(1,2,1)    \n        plt.imshow(original_img)\n        array_img=img_to_array(original_img)\/255\n       \n        array_img2 = np.reshape(array_img, (1,224,224,3))\n        y_pred=model.predict(array_img2)\n        y_pred=np.reshape(y_pred,(224,224,6))\n        c=class_pixels(y_pred)\n        o=colorize(c)\n        plt.subplot(1,2,2)    \n        plt.imshow(o)\n        plt.show()\n        h+=1\n    else:\n        break","87111de9":"# Libraries needed for this notebook","5edc74e9":"# Data generator","5ab83977":"# Customized Maxpooling\/Unpooling Layers\n\nReference: https:\/\/github.com\/ykamikawa\/tf-keras-SegNet","db5b2a8d":"# Data preprocessing and splitting","80c737b6":"# Metrics\n\nBecause the dataset contains imbalanced number of classes in each image, it's better to use a proper metric for that, which is Dice Coefficient.","c84b8cfe":"# Segnet model with some customized layers\n\nreference : https:\/\/github.com\/ykamikawa\/tf-keras-SegNet","59c401c5":"# Training and validation curves\n\nOnce again I uploaded the history of my external training.","64477fbf":"It took me while to train the model ( 24h on four external GPUs ). So will just write the code for training and load the trained model after.","e53e9f94":"# Training","8ad499a2":"# Post-training\n\nHere is the model trained on the same training set but externally in a server of 4 GPUs. Couldn't finish the training here because of the GPU quota and the successive shutdowns of the session.","269ab9ad":"# ***Inference on test data***"}}