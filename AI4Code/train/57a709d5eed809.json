{"cell_type":{"8e7f1c73":"code","6d2b46b7":"code","9c8ac3a0":"code","c92d09b5":"code","c58318fa":"code","c3d993d6":"code","f823c59e":"code","fefa3585":"code","6c84ab3b":"code","793e09db":"code","9754f746":"code","3a08eff2":"code","80e8455c":"code","fdefa416":"code","c6c7fd4c":"code","e6e5757a":"code","653456f6":"code","d29dc8eb":"markdown","970c9fb0":"markdown","cfb69c88":"markdown","2182e24f":"markdown","5068afc3":"markdown","deafc052":"markdown","ce217e5c":"markdown","f19c5a62":"markdown","5c493281":"markdown","11d4acbf":"markdown","888f9407":"markdown"},"source":{"8e7f1c73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error as RMSE\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import cross_val_score\npd.options.mode.chained_assignment = None\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6d2b46b7":"train_data = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-1\/train.csv', dtype={'Year': 'object', 'Week-Ending Date': 'object'})\ntrain_data.head()","9c8ac3a0":"test_data = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-1\/test.csv', dtype={'Year': 'object', 'Week-Ending Date': 'object'})\ntest_data.head()","c92d09b5":"train_data.info()\nprint('\\nThe Shape for the train data', train_data.shape, '\\n')\ntrain_data.describe()","c58318fa":"test_data.info()\nprint('\\nThe Shape for the test data', test_data.shape, '\\n')\ntest_data.describe()","c3d993d6":"msno.matrix(train_data)","f823c59e":"train_data.drop(['Footnote', 'MMWR Week', 'Data As Of', 'Total Deaths', 'End Date','Start Date'], axis = 1, inplace = True)\n\ntest_data.drop(['MMWR Week', 'Data As Of', 'Total Deaths', 'End Date', 'Start Date'],axis = 1, inplace = True)\n\ntrain_data = train_data[train_data['HHS Region'] == 'United States']\ntest_data = test_data[test_data['HHS Region'] == 'United States']\n\n\ntrain_data['Week-Ending Date'] = pd.to_datetime(train_data['Week-Ending Date'])\ntest_data['Week-Ending Date'] = pd.to_datetime(test_data['Week-Ending Date'])\n\nweekly_data = train_data[train_data['Group'] == 'By Week']\n\nweekly_data.info()\nweekly_data.describe()","fefa3585":"def is_outlier(s):\n    lower_limit = s.mean() - (s.std() * 3)\n    upper_limit = s.mean() + (s.std() * 3)\n    return ~s.between(lower_limit, upper_limit)\n\n\ndef replace(group):\n    mean, std = group.mean(), group.std()\n    outliers = (group - mean).abs() > 3*std\n    group[outliers] = 0        # or \"group[~outliers].mean()\"\n    return group","6c84ab3b":"le =LabelEncoder()\n\nweekly_data['Week Date'] = le.fit_transform(weekly_data['Week-Ending Date'])\ntest_data['Week Date'] = le.fit_transform(test_data['Week-Ending Date'])\n\nweekly_data['Race'] = weekly_data['Race and Hispanic Origin Group']\ntest_data['Race'] = test_data['Race and Hispanic Origin Group']\n\ng = sns.FacetGrid(weekly_data, col = 'Race', row='Age Group',legend_out = True, margin_titles = True)\ng.map(sns.regplot, 'Week Date', 'COVID-19 Deaths')\n","793e09db":"#creating a set of plots to examine the relationship between race, age, and the covid deaths.\nrace_age_plot = sns.relplot(data = weekly_data, x = 'Week-Ending Date', y = 'COVID-19 Deaths', hue = 'Race and Hispanic Origin Group',\n                            col = 'Age Group', col_wrap = 3, kind = 'line')\nrace_age_plot.set_xticklabels(rotation = 90)","9754f746":"\ndataframe = pd.concat([weekly_data, test_data])\n\ndataframe.drop('Race', axis = 1, inplace = True)\n\ndataframe.drop(['HHS Region', 'Group'], axis = 1, inplace = True)\n\nle =LabelEncoder()\nfeatures = ['id', 'Week-Ending Date', 'Month', 'Age Group', 'COVID-19 Deaths', 'Week Date']\n\ndataframe['Ages'] = dataframe['Age Group'].apply(lambda x: ['0-4 years', '5-17 years', '18-29 years', '30-39 years',\n       '40-49 years', '50-64 years', '65-74 years', '75-84 years',\n       '85 years and over'].index(x))\n        \n        \ndataframe['Month'] = pd.DatetimeIndex(dataframe['Week-Ending Date']).month\ndataframe['Quarter'] = pd.DatetimeIndex(dataframe['Week-Ending Date']).quarter\ndataframe['Year'] = pd.DatetimeIndex(dataframe['Week-Ending Date']).year\n\n\ndataframe = pd.get_dummies(dataframe, columns=['Race and Hispanic Origin Group', 'Year', 'Quarter'], drop_first=True)\n\nweekly_data = dataframe[dataframe['id'].isin(weekly_data['id'])]\ntest_data = dataframe[dataframe['id'].isin(test_data['id'])]\n\ny = weekly_data['COVID-19 Deaths']\nX = weekly_data.drop(columns = ['id', 'Week-Ending Date', 'Age Group', 'COVID-19 Deaths'])\ntest = test_data.drop(columns = ['id', 'Week-Ending Date', 'Age Group', 'COVID-19 Deaths'])\n\ntrain_scaler = StandardScaler()\ntest_scaler = StandardScaler()\n\nX.info()\n\nX = pd.DataFrame(train_scaler.fit_transform(X), index=X.index, columns=X.columns)\ntest = pd.DataFrame(test_scaler.fit_transform(test), index=test.index, columns=test.columns)","3a08eff2":"lreg = LinearRegression()\n\ntime_kfold = TimeSeriesSplit(n_splits = 30)\n\n\nfold_metrics = []\n\nfor train_index, test_index in time_kfold.split(X):\n    X_train = X.iloc[train_index]\n    y_train = y.iloc[train_index]\n    X_test = X.iloc[test_index]\n    y_test = y.iloc[test_index]\n    \n    lreg.fit(X_train, y_train)\n    y_pred = lreg.predict(X_test)\n    fold_metrics.append(RMSE(y_test, y_pred, squared = False))\n    \nprint(fold_metrics)\nprint(np.mean(fold_metrics))","80e8455c":"ridge_params = {'alpha': [1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 0.1]}\n\nridge_gsc = GridSearchCV(Ridge(), param_grid = ridge_params, scoring ='neg_root_mean_squared_error', cv = time_kfold.split(X))\nridge_gsc.fit(X, y)\n#ridge_gsc = gsc.best_estimator_\n","fdefa416":"lasso_params = {'alpha': [1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 0.1, 1]}\n\nlasso_gsc = GridSearchCV(Lasso(tol = 1), param_grid = lasso_params, scoring = 'neg_root_mean_squared_error', cv = time_kfold.split(X))\nlasso_gsc.fit(X, y)\n#lasso_gsc = gsc.best_estimator_\n\n","c6c7fd4c":"elastic_params = {'alpha': [1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 0.1, 1], 'l1_ratio': [0, 0.5, 1]}\n\nelastic_gsc = GridSearchCV(ElasticNet(tol = 1), param_grid = elastic_params, scoring = 'neg_root_mean_squared_error', cv = time_kfold.split(X))\nelastic_gsc.fit(X, y)\n#elastic_gsc = gsc.best_estimator_\n\n","e6e5757a":"lreg_gsc = GridSearchCV(LinearRegression(), param_grid = {}, scoring ='neg_root_mean_squared_error', cv = time_kfold.split(X))\nlreg_gsc.fit(X, y)\n\nlregdf = pd.DataFrame.from_dict(lreg_gsc.cv_results_)\nridgedf = pd.DataFrame.from_dict(ridge_gsc.cv_results_)\nlassodf = pd.DataFrame.from_dict(lasso_gsc.cv_results_)\nelasticdf = pd.DataFrame.from_dict(elastic_gsc.cv_results_)\n\nscores = {}\nscores['Linear'] = -lregdf.iloc[lreg_gsc.best_index_, 5:29]\nscores['Ridge'] = -ridgedf.iloc[ridge_gsc.best_index_, 6:30]\nscores['Lasso'] = -lassodf.iloc[lasso_gsc.best_index_, 7:31]\nscores['ElasticNet'] = -elasticdf.iloc[elastic_gsc.best_index_, 8:32]\n\ndf = pd.DataFrame.from_dict(scores, dtype=np.float64)\n\nfor column in df.columns:\n    plt.figure()\n    sns.displot(df[column], bins=6)\n    \n    \ndf.info()\ndf.describe()","653456f6":"predictions = elastic_gsc.predict(test)\npredictions[predictions < 0] = 0\n\noutput = pd.DataFrame({'id': test_data.id, 'COVID-19 Deaths': predictions})\noutput.to_csv('submission.csv', index=False)","d29dc8eb":"# Least Squares Model","970c9fb0":"After trying to find the outliers, replacing them with the mean or dropping them didn't improve the models at all, so I'm going to keep them in","cfb69c88":"From looking at the Data:\n* The data is repeated 3 times, by week, by month, by year. so they must be split in order get a more managable data set\n* footnote is just notes, so it is irrelevant\n* test data is in weeks, so building the models off of the weekly training data would be more sensible\n* since HHS region is 11 regions and test data is only for the U.S., only data pertaining to the 'United States' region will be kept.\n* Date as of is just saying the most recent day the data was updated, so it's not needed.\n* Since Total Deaths just says how many people died in general and not how many died from COVID-19 specifically, and since we have a column for that specifically, that can be dropped as well.","2182e24f":"# Lasso Regression","5068afc3":"# Ridge Regression","deafc052":"# Elastic Net Regression","ce217e5c":"# Exploratory Data Analysis","f19c5a62":"# Importing Data","5c493281":"# Visualizing the Models","11d4acbf":"# building the models","888f9407":"# Building A Submission"}}