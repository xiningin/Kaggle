{"cell_type":{"ca7189d3":"code","183b1d6e":"code","7d540425":"code","c1aeb657":"code","2781cda0":"code","d0bff0c1":"code","331af39d":"code","072dbf01":"code","27c94abd":"code","5d258338":"code","2b6cdbf8":"code","dc646fa3":"code","8d61fb2f":"code","7c106fa0":"code","40290249":"code","71de8f44":"code","b805e191":"code","ce3a39a0":"code","32c32dab":"code","0ac95f81":"code","8f9b6364":"markdown","18d7bad6":"markdown","b98288e5":"markdown","7383ef86":"markdown","7eef152f":"markdown","f9ffa7cd":"markdown","a7c647af":"markdown","56542c5d":"markdown","bac093ea":"markdown","b3376a13":"markdown"},"source":{"ca7189d3":"import librosa\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport IPython\nimport scipy\n\n\nfrom IPython.display import Audio\nfrom IPython.core.display import HTML\nfrom pandas_profiling import ProfileReport\nfrom scipy.io import wavfile","183b1d6e":"# no of bird classes\n! ls \/kaggle\/input\/birdsong-recognition\/train_audio\/ | wc -l","7d540425":"train = pd.read_csv('\/kaggle\/input\/birdsong-recognition\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/birdsong-recognition\/test.csv')","c1aeb657":"print(train.shape)\ntrain.head()","2781cda0":"train_summary = pd.read_csv('\/kaggle\/input\/birdsong-recognition\/example_test_audio_summary.csv')\ntrain_summary.head()","d0bff0c1":"train_profile = ProfileReport(train)","331af39d":"train_profile.to_widgets()","072dbf01":"# Press the output to see the complete report\ntrain_profile","27c94abd":"Audio('\/kaggle\/input\/birdsong-recognition\/train_audio\/eawpew\/XC148566.mp3')","5d258338":"y, sr = librosa.load('\/kaggle\/input\/birdsong-recognition\/train_audio\/eawpew\/XC148566.mp3')\ntempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n\nprint('Estimated tempo: {:.2f} beats per minute'.format(tempo))\n\nbeat_times = librosa.frames_to_time(beat_frames, sr=sr)","2b6cdbf8":"print(f\"Sample rate  :\", sr)\nprint(f\"Signal Length:{len(y)}\")\nprint(f\"Duration     : {len(y)\/sr}seconds\")","dc646fa3":"plt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(y, sr=sr)","8d61fb2f":"sg0 = librosa.stft(y)\nsg_mag, sg_phase = librosa.magphase(sg0)\ndisplay(librosa.display.specshow(sg_mag))","7c106fa0":"sg1 = librosa.feature.melspectrogram(S=sg_mag, sr=sr)\ndisplay(librosa.display.specshow(sg1))","40290249":"sg2 = librosa.amplitude_to_db(sg1, ref=np.min)\nlibrosa.display.specshow(sg2)","71de8f44":"# code adapted from the librosa.feature.melspectrogram documentation\nlibrosa.display.specshow(sg2, sr=16000, y_axis='mel', fmax=8000, x_axis='time')\nplt.colorbar(format='%+2.0f dB')\nplt.title('Mel spectrogram')\n\n","b805e191":"sg2.min(), sg2.max(), sg2.mean()","ce3a39a0":"\n# Code adapted from https:\/\/musicinformationretrieval.com\/fourier_transform.html and the original\n# implementation of fastai audio by John Hartquist at https:\/\/github.com\/sevenfx\/fastai_audio\/\ndef fft_and_display(signal, sr):\n    ft = scipy.fftpack.fft(signal, n=len(signal))\n    ft = ft[:len(signal)\/\/2+1]\n    ft_mag = np.absolute(ft)\n    f = np.linspace(0, sr\/2, len(ft_mag)) # frequency variable\n    plt.figure(figsize=(13, 5))\n    plt.plot(f, ft_mag) # magnitude spectrum\n    plt.xlabel('Frequency (Hz)')","32c32dab":"fft_and_display(y, sr)","0ac95f81":"sub = pd.read_csv('\/kaggle\/input\/birdsong-recognition\/sample_submission.csv')\nsub.to_csv('submission.csv', index=False)","8f9b6364":"## Spectogram\n\n- checking what is characteristics of frequency","18d7bad6":"## What's inside a spectogram","b98288e5":"## Importing libraries","7383ef86":"- find sample_rate\n- find signal_length\n- duration","7eef152f":"## Quick EDA\n\nusing pandas profile report","f9ffa7cd":"- loading audio with librosa\n- finding beat_time","a7c647af":"## Making a submission","56542c5d":"## Analysing the sound with EDA","bac093ea":"## Fourier Transformation","b3376a13":"## Quick peek at the markdown"}}