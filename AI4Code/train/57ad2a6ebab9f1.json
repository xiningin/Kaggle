{"cell_type":{"f380cae5":"code","320ecae7":"code","44c29d37":"code","735eccc4":"code","f8891c02":"code","f1ee93d5":"code","59452ad2":"code","15202037":"code","2e200cfd":"code","75b994c9":"code","62303452":"code","7dfc9177":"code","a699d043":"code","9e11f764":"code","7adb1b8b":"code","a7542040":"code","b6789143":"code","8205a6af":"code","bc7c8618":"code","476c3514":"code","c07ae7dc":"code","8144662b":"code","499958c9":"code","4a573205":"code","d3d8da60":"code","0f648f0e":"code","55445f44":"code","5b694261":"code","78b1d949":"code","02724774":"code","432b0700":"code","bf048233":"code","07b01336":"code","2c37f5db":"code","63188c92":"code","857abb82":"code","db659dda":"code","2dd955f9":"code","a071e8b9":"markdown","fcce5bc7":"markdown","f0d8944e":"markdown","65685787":"markdown","f5e1c742":"markdown","d9a82594":"markdown","0af1011a":"markdown","ab39836d":"markdown","206fe567":"markdown","bf4c6f91":"markdown","1645b8ab":"markdown","eb5ef9eb":"markdown","0c63f94e":"markdown","b7649fbd":"markdown","0cd09509":"markdown","ad9887e9":"markdown","b45ba852":"markdown","8d1219b8":"markdown","b1226bbe":"markdown","30227f2d":"markdown"},"source":{"f380cae5":"is_kaggle = True","320ecae7":"!git clone https:\/\/github.com\/ofjpostema\/bert.git","44c29d37":"output_dir_name = 'bert_output'    \nif is_kaggle:\n    QUESTION_DIR = \"\/kaggle\/input\/elasticsearchquestions\"\n    from kaggle_datasets import KaggleDatasets\n    BUCKET_NAME = KaggleDatasets().get_gcs_path(\"bertanswers\")\n    BUCKET = BUCKET_NAME.split(\"\/\")[-1]\n    OUTPUT_DIR = '\/kaggle\/input\/bertanswers'\nelse:\n    QUESTION_DIR = os.path.join(\"..\", \"data\", \"interim\", \"questions\")\n    if not os.path.isdir(QUESTION_DIR):\n        os.mkdir(QUESTION_DIR)\n\n    BUCKET = 'of-covid-19-clean'\n    BUCKET_NAME = 'gs:\/\/{}'.format(BUCKET)\n    OUTPUT_DIR = 'gs:\/\/{}\/{}'.format(BUCKET, output_dir_name)\n\nGS_QUESTION_DIR_BASE = \"questions\"\nGS_QUESTION_DIR = 'gs:\/\/{}\/{}'.format(BUCKET, GS_QUESTION_DIR_BASE)\nANSWER_DIR = \"answers\"\nGS_ANSWER_DIR = '{}\/{}'.format(BUCKET_NAME, ANSWER_DIR)\nGCP_PROJECT = 'covid-19-271609'","735eccc4":"# NOTE: This is only relevant for Google Colab. On Kaggle it uses the Kaggle's OAuth authentication\nif not is_kaggle:\n    from google.colab import auth\n    auth.authenticate_user()","f8891c02":"if not is_kaggle:\n    # You can't import these by default on Google Colab\n    from elasticsearch_dsl import connections, Index, Search\n    from elasticsearch_dsl import Document, Text, Boolean\n    from elasticsearch import Elasticsearch","f1ee93d5":"import os\nimport pandas as pd\nimport json\nimport pprint\npp = pprint.PrettyPrinter(indent=2)\nfrom collections import Counter\nimport re\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport datetime\nimport random\nimport string\nimport sys\nimport tensorflow as tf\nimport collections","59452ad2":"# Create a connection to ElasticSearch\nif not is_kaggle:\n    connections.create_connection(hosts=['localhost'], timeout=20)","15202037":"datasets = ['biorxiv_medrxiv', 'comm_use_subset', 'custom_license', 'noncomm_use_subset']\nif not is_kaggle:\n    # Define the paths to the data\n    dir_data_raw = os.path.join(\"..\", \"data\", \"raw\")\n    data_dir_interim = os.path.join(\"..\", \"data\", \"interim\")\nelse:\n    dir_data_raw = os.path.join(\"..\", \"input\", \"CORD-19-research-challenge\")\n    data_dir_interim = os.path.join(\"interim\")\n    if not os.path.isdir(data_dir_interim):\n        os.mkdir(data_dir_interim)","2e200cfd":"def format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body","75b994c9":"def parse_article(full_path, file_path):\n    \"\"\"\n    Parse an article's body text and extract the full text, the results and the conclusion.\n    \n    full_path: str: The fully qualified path to the file\n    file_path: str: The file path starting from the data_raw dir\n    \"\"\"\n    section_headings = {\n        \"results\": [\"results and discussion\", \"results\"],\n        \"conclusion\": [\"conclusion\", \"conclusions\", \"discussion and conclusions\"],\n        #TODO: Intro\n    }\n    with open(full_path) as file:\n        json_article = json.load(file)[\"body_text\"]\n        article_sections = []\n        # For extracting the main body we \n        metadata.loc[index, 'full_text'] = format_body(json_article)\n        for body_text in json_article:\n            # Clean the section headings, lowercase and trim them\n            section_heading = re.sub(r'[^a-zA-Z0-9 ]', '', body_text[\"section\"]).lower().strip()\n            for section, headings in section_headings.items():\n                if section_heading in headings:\n                    metadata.loc[index, section] =  article[section] + body_text[\"text\"]","62303452":"if not is_kaggle:\n    # Load the metadata and initialize the new, empty, columns\n    metadata = pd.read_csv(os.path.join(dir_data_raw, \"metadata.csv\"))\n    metadata[\"full_text\"] = \"\"\n    metadata[\"file_path\"] = None\n    metadata[\"results\"] = \"\"\n    metadata[\"conclusion\"] = \"\"","7dfc9177":"if not is_kaggle:\n    for index, article in tqdm(metadata.iterrows()):\n        # We only need to update if there's a full text\n        if article[\"has_full_text\"]:\n            for dataset in datasets:\n                file_path = os.path.join(dataset, dataset, str(article[\"sha\"]) + \".json\")\n                metadata.loc[index, \"file_path\"] = file_path\n                full_path = os.path.join(dir_data_raw, file_path)\n                if os.path.exists(full_path):\n                    parse_article(full_path, file_path)","a699d043":"if not is_kaggle:\n    metadata.to_csv(os.path.join(data_dir_interim, \"1_full_data.csv\"))","9e11f764":"if not is_kaggle:\n    metadata = pd.read_csv(os.path.join(data_dir_interim, \"1_full_data.csv\"))","7adb1b8b":"if not is_kaggle:\n    class Paper(Document):\n        id = Text(required=True, index='covid')\n        title = Text(required=True)\n        authors = Text(required=True)\n        abstract = Text(required=True)\n        text = Text(required=True)\n        results = Text(required=True)\n        conclusion = Text(required=True)\n        bibliography = Text(required=False)\n\n        class Meta:\n            name = 'covid'","a7542040":"if not is_kaggle:\n    # First create an index\n    index = Index(\"covid\")\n\n    for index, paper in tqdm(metadata.iterrows()):\n        # We'll only upload the document if it has a full text.\n        if paper[\"has_full_text\"]:\n            paper_doc = Paper(\n                id=paper[\"sha\"] if type(paper[\"sha\"]) == str else \"\",\n                title=paper[\"title\"] if type(paper[\"title\"]) == str else \"\",\n                authors=paper[\"authors\"] if type(paper[\"authors\"]) == str else \"\",\n                abstract=paper[\"abstract\"] if type(paper[\"abstract\"]) == str else \"\",\n                text=paper[\"full_text\"] if type(paper[\"full_text\"]) == str else \"\",\n                results=paper[\"results\"] if type(paper[\"results\"]) == str else \"\",\n                conclusion=paper[\"conclusion\"] if type(paper[\"conclusion\"]) == str else \"\",\n                bibliography=\"\"\n            )\n            paper_doc.save(index=\"covid\")","b6789143":"if not is_kaggle:\n    client = Elasticsearch()","8205a6af":"queries = [\n    {\n        \"id\": 1,\n        \"question\": \"What is the clinical effectiveness of antiviral agents?\",\n        \"keywords\": [\"clinical effectiveness\", \"therapeutic\", \"antiviral agents\"],\n    },\n    {\n        \"id\": 2,\n        \"question\": \"What is the effectiveness of drugs being developed and tried to treat COVID-19 patients?\",\n        \"keywords\": [\"clinical trials\", \"bench trials\", \"viral inhibitors\", \"naproxen\", \"clarithromycin\", \"minocyclinethat\", \"viral replication\"],\n    },\n    {\n        \"id\": 3,\n        \"question\": \"Are there potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients?\",\n        \"keywords\": [\"complications\", \"Antibody-Dependent Enhancement\", \"vaccine\", \"antiviral proteins\"],\n    },\n    {\n        \"id\": 4,\n        \"question\": \"Are there animal models that offer predictive value for a human vaccine?\",\n        \"keywords\": [\"animal models\", \"predictive\", \"vaccine\"],\n    },\n    {\n        \"id\": 5,\n        \"question\": \"How to distribute scarces therapeutics?\",\n        \"keywords\": [\"distribution\", \"therapeutics\", \"antiviral agents\", \"decision making\", \"prioritizing\"],\n    },\n    {\n        \"id\": 6,\n        \"question\": \"How to expand production capacity of antiviral agents?\",\n        \"keywords\": [\"production capacity\", \"therapeutic\", \"antiviral agents\"],\n    },\n    {\n        \"id\": 7,\n        \"question\": \"Are there universal coronavirus vaccines?\",\n        \"keywords\": [\"coronavirus vaccine\", \"universal vaccine\"],\n    },\n    {\n        \"id\": 8,\n        \"question\": \"Which animal models are there?\",\n        \"keywords\": [\"animal models\", \"challenge studies\"],\n    },\n    {\n        \"id\": 9,\n        \"question\": \"Which prophylaxis clinical studies are there?\",\n        \"keywords\": [\"prevention\", \"prophylaxis\", \"clinical study\"],\n    },\n    {\n        \"id\": 10,\n        \"question\": \"What is the clinical effectiveness of antiviral agents?\",\n        \"keywords\": [\"clinical effectiveness\", \"therapeutic\", \"antiviral agents\"],\n    },\n]","bc7c8618":"def search_get_results(question, limit_from, limit_size):\n    should = [{\"match\": {\"text\": keyword}} for keyword in question[\"keywords\"]]\n    response = client.search(\n        index=\"covid\",\n        body={\n          \"from\": limit_from,\n          \"size\": limit_size,\n          \"query\": {\n                \"bool\": {\n                  \"should\": [{\"match\": {\"text\": \"covid\"}}, {\"match\": {\"text\": \"ncov\"}}],\n                  \"should\": should,\n                }\n          },\n        }\n    )\n    return response\n\n\ndef get_all_results(question, min_score):\n    \"\"\"\n    Get all results from elasticsearch that have a minimum score. This method continues going \n    through the pages (using the limit and size parameters) until it has found an article that \n    has a score below the min score.\n    \n    question: dict: The question\n    min_score: float: The minimum score an article should have.\n    \"\"\"\n    last_score = 10000\n    limit_size = 50\n    limit_from = 0\n    hits = []\n    while last_score > min_score:\n        search_results = search_get_results(question, limit_from, limit_size)\n        hits = hits + search_results[\"hits\"][\"hits\"]\n        limit_from += limit_size\n        last_score = hits[-1][\"_score\"]\n        \n    # We'll delete articles with a score under the min_score.\n    while hits[-1][\"_score\"] < min_score:\n        hits.pop()\n        \n    return hits","476c3514":"if not is_kaggle:\n    for query in queries:\n        hits = get_all_results(query, 11)\n        print(\"{} hits for query {}\".format(len(hits), query[\"id\"]))\n        input_questions = {\n            \"version\": \"v0.1\",\n            \"data\": [\n                {\n                    \"title\": hit[\"_source\"][\"title\"],\n                    \"paragraphs\": []\n                }\n            ]\n        }\n        # Get the query\n        for hit in hits:\n            input_questions[\"data\"][0][\"paragraphs\"].append({\n                \"qas\": [{\n                    \"question\": query[\"question\"],\n                    \"id\": \"q_{}_h_{}\".format(query[\"id\"], hit[\"_source\"][\"id\"]),\n                    \"is_impossible\": \"\"\n                }],\n                \"context\": hit[\"_source\"][\"text\"].lower()\n            })\n        with open(os.path.join(QUESTION_DIR, \n                                \"q_{}.json\".format(\n                                    query[\"id\"]\n                                )), 'w') as outfile:\n            json.dump(input_questions, outfile)","c07ae7dc":"if not is_kaggle:\n    !gsutil -m cp -r $QUESTION_DIR $GS_QUESTION_DIR","8144662b":"if is_kaggle:\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n        TPU_ADDRESS = tpu.master()\n    except ValueError:\n        TPU_ADDRESS = None\nelse:\n    assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n    TPU_ADDRESS = 'grpc:\/\/' + os.environ['COLAB_TPU_ADDR']\nprint('TPU address is => ', TPU_ADDRESS)","499958c9":"if not is_kaggle:\n    !wget https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-research\/scibert\/tensorflow_models\/scibert_scivocab_uncased.tar.gz\n    !tar -xf scibert_scivocab_uncased.tar.gz","4a573205":"if not is_kaggle:\n    !gsutil mv \/content\/scibert_scivocab_uncased $BUCKET_NAME","d3d8da60":"if not is_kaggle:\n    !wget https:\/\/rajpurkar.github.io\/SQuAD-explorer\/dataset\/train-v2.0.json\n    !wget https:\/\/rajpurkar.github.io\/SQuAD-explorer\/dataset\/dev-v2.0.json","0f648f0e":"if not is_kaggle:\n    !pip install -U tensorflow==1.15.0","55445f44":"if not is_kaggle:\n    !python bert\/run_squad.py \\\n      --vocab_file=$BUCKET_NAME\/scibert_scivocab_uncased\/vocab.txt \\\n      --bert_config_file=$BUCKET_NAME\/scibert_scivocab_uncased\/bert_config.json \\\n      --init_checkpoint=$BUCKET_NAME\/scibert_scivocab_uncased\/bert_model.ckpt \\\n      --do_train=True \\\n      --train_file=train-v2.0.json \\\n      --do_predict=True \\\n      --predict_file=dev-v2.0.json \\\n      --train_batch_size=24 \\\n      --learning_rate=3e-5 \\\n      --num_train_epochs=2.0 \\\n      --use_tpu=True \\\n      --tpu_name=$TPU_ADDRESS \\\n      --max_seq_length=512 \\\n      --doc_stride=128 \\\n      --version_2_with_negative=True \\\n      --output_dir=$OUTPUT_DIR","5b694261":"if not is_kaggle:\n    from google.cloud import storage\n    storage_client = storage.Client(project=GCP_PROJECT)\n    bucket = storage_client.get_bucket(BUCKET)","78b1d949":"if not is_kaggle:\n    question_blobs = bucket.list_blobs(\n        prefix=GS_QUESTION_DIR_BASE\n    )","02724774":"if not is_kaggle:\n    for question in tqdm(question_blobs):\n        question_name = question.name\n        output_dir_answer = question.name.split(\".\")[0].split(\"\/\")[-1]\n        !python bert\/run_squad.py \\\n          --vocab_file=$BUCKET_NAME\/scibert_scivocab_uncased\/vocab.txt \\\n          --bert_config_file=$BUCKET_NAME\/scibert_scivocab_uncased\/bert_config.json \\\n          --init_checkpoint=$BUCKET_NAME\/scibert_scivocab_uncased\/bert_model.ckpt \\\n          --do_train=False \\\n          --max_query_length=30  \\\n          --do_predict=True \\\n          --predict_file=$BUCKET_NAME\/$question_name \\\n          --use_tpu=True \\\n          --tpu_name=$TPU_ADDRESS \\\n          --predict_batch_size=8 \\\n          --n_best_size=3 \\\n          --max_seq_length=512 \\\n          --doc_stride=128 \\\n          --output_dir=$BUCKET_NAME\/answers\/$output_dir_answer\/","432b0700":"import nltk\nnltk.download('punkt')","bf048233":"from bert import tokenization","07b01336":"class InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self,\n                 unique_id,\n                 example_index,\n                 doc_span_index,\n                 tokens,\n                 token_to_orig_map,\n                 token_is_max_context,\n                 input_ids,\n                 input_mask,\n                 segment_ids,\n                 start_position=None,\n                 end_position=None,\n                 is_impossible=None):\n        self.unique_id = unique_id\n        self.example_index = example_index\n        self.doc_span_index = doc_span_index\n        self.tokens = tokens\n        self.token_to_orig_map = token_to_orig_map\n        self.token_is_max_context = token_is_max_context\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.start_position = start_position\n        self.end_position = end_position\n        self.is_impossible = is_impossible","2c37f5db":"tokenizer = tokenization.BasicTokenizer(do_lower_case=True)","63188c92":"def parse_questions(paragraphs):\n    \"\"\"\n    Parse the questions from the paragraphs in the questions file.\n    This will extract the article texts and the question.\n    args\n    paragraphs: list: A list of paragraph dicts.\n\n    return \n    articles: dict\n    question: str\n    \"\"\"\n    qa_id_text = {}\n    for paragraph in original_json[\"data\"][0][\"paragraphs\"]:\n        for qas in paragraph[\"qas\"]:\n            qa_id_text[qas[\"id\"]] = paragraph[\"context\"]\n        question_text = qas[\"question\"]\n    return qa_id_text, question_text\n\ndef get_sentence_index(word_index, sentences):\n    \"\"\"\n    Get the index of a sentence given the index of a word in the text.\n\n    args\n    word_index: int: The index of the word\n    sentences: list<str>: The list of sentences\n\n    return\n    sentence_index: int: The index of the sentence\n    \"\"\"\n    i = 0\n    for idx_sentence, sentence in enumerate(sentences):\n        sententence_tokens = sentence.split(\" \")\n        i += len(sententence_tokens)\n        if i > word_index:\n            return idx_sentence\n\ndef get_passage(word_index, sentences, text_to_find):\n    \"\"\"\n    Get a passage from the text, given a list of sentences, the text to find and \n    an index of the word.\n    \"\"\"\n    selected_sentence = get_sentence_index(word_index, sentences)\n    distance = 1\n    found = False\n    while not found and distance < 6:\n        combined_sentences = \" \".join([\n            sentence \n            for idx_sentence, sentence in enumerate(sentences) \n            if  idx_sentence >= selected_sentence - distance and \n                idx_sentence <= selected_sentence + distance\n            ]\n        )\n        if text_to_find in combined_sentences:\n            return combined_sentences\n        else:\n            distance += 1\n    return None\n\ndef get_questions():\n    \"\"\"\n    Get a list or iterator of all the questions.\n    \"\"\"\n    if is_kaggle:\n        return [(f, os.path.join(QUESTION_DIR, f)) \n                for f in os.listdir(QUESTION_DIR) \n                if os.path.isfile(os.path.join(QUESTION_DIR, f))]\n    else:\n        question_blobs = bucket.list_blobs(\n            prefix=GS_QUESTION_DIR_BASE\n        )\n        return [(question_blob.name, question_blob) \n                for question_blob in question_blobs]\n    \ndef get_answers():\n    \"\"\"\n    Get a list or iterator of all the answers.\n    \"\"\"\n    if is_kaggle:\n        return [(f, os.path.join(OUTPUT_DIR, f)) for f in os.listdir(OUTPUT_DIR) \n                if os.path.isfile(os.path.join(OUTPUT_DIR, f))]\n    else:\n        answer_blobs = bucket.list_blobs(\n            prefix=ANSWER_DIR\n        )\n        return [(answer_blob.name, answer_blob) \n                for answer_blob in answer_blobs \n                if answer_blob.name.endswith(\"nbest_predictions.json\")]\n\n    \ndef get_qa_content(qa):\n    \"\"\"\n    Get the contents of a questions or answers file.\n    \"\"\"\n    if is_kaggle:\n        with open(qa, \"r\") as in_file:\n            return json.load(in_file)\n    else:\n        if type(qa) == str:\n            # This means that it's only a path to the file on GCS.\n            qa = storage.blob.Blob(qa, bucket)\n\n        return json.loads(qa.download_as_string())","857abb82":"answers = get_answers()\n\nqa_overview = collections.defaultdict(list)\n\nfor answer_name, answer in tqdm(answers):\n    # Load the question file, to get the original texts\n    print(answer_name)\n    if is_kaggle:\n        question_key = answer_name.split(\".\")[0]\n    else:\n        question_key = answer_name.split(\"\/\")[-2]\n\n    # We'll first load the original questions file that was used to predict on\n    original_json = get_qa_content(QUESTION_DIR + \"\/\"+question_key+\".json\")\n    qa_id_text, question_text = parse_questions(original_json[\"data\"][0][\"paragraphs\"])\n\n    # Now we'll get the predicted answers\n    question_results = get_qa_content(answer)\n\n    for question, results in tqdm(question_results.items()):\n        if results[0][\"text\"] != \"empty\":\n            text_tokenized = qa_id_text[question].split(\" \")\n            text = tokenizer._clean_text(qa_id_text[question])\n            text = re.sub(' +', ' ', text)\n\n            sentences = nltk.sent_tokenize(text)\n            passage = get_passage(results[0][\"start_orig_doc\"], sentences, results[0][\"text\"])\n\n            if passage:\n                id_question = question.split(\"_\")[1]\n                id_article = question.split(\"_\")[3].split(\".\")[0]\n                qa_overview[id_question].append({\n                    \"id\": id_article,\n                    \"passage\": passage, \n                    \"probability\": results[0][\"probability\"]\n                })\n\nfor id_question, articles in qa_overview.items():\n    qa_overview[id_question] = sorted(articles, key = lambda i: i['probability']) ","db659dda":"from tabulate import tabulate\nfrom IPython.display import HTML, display\nimport ipywidgets as widgets","2dd955f9":"for query in queries:\n    display(HTML(\"<h3>{}<\/h3>\".format(query[\"question\"])))\n    data = [[article[\"id\"], article[\"passage\"]] for article in qa_overview[str(query[\"id\"])]]\n    display(HTML(tabulate(data, \n                        tablefmt='html', \n                        headers=[\"Paper SHA\",\"Passage\"])))","a071e8b9":"## Training\nWe first need to re-train SciBERT to actually answer questions.","fcce5bc7":"### Collecting data\nWe'll collect all relevant data from ElasticSearch. To do this we\n\n1. Create a search query based on the question\n2. Get all nouns from the query\n3. Get synonyms for all nouns\n4. Search using this search query (in the abstract and the keywords)\n5. Collect the top 50 results\n6. Create a train.json file for this question, posing it to each article","f0d8944e":"### Formatting\nThese formatting helper functions are courtesy of [xhlulu](https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv). It formats the body of a paper using the different section headings, etc. This is used to format the document body before it's ingested into ElasticSearch and SciBERT.","65685787":"We are going to go through each of the previously defined queries and retrieve all documents that match the keywords. These articles are then combined in a JSON file that follows the input format for [SQuAD](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/). There is one file for each question with one paragraph for each paper.","f5e1c742":"In order to train the BERT model you need a Google Cloud Storage bucket to which you've got access. You should enter that below.","d9a82594":"We'll load the metadata file and add the full text, file path and the results and conclusions sections to it, using the previously defined methods.","0af1011a":"$\\{ \\text{query} = (\\text{all articles},\\text{keywords}) \\} \\xrightarrow{\\text{Elasticsearch}} \\{ (50 \\text{ candidate articles}) \\} \\xrightarrow{\\text{BERT}} \\text{Answer}$ ","ab39836d":"Usually these two methods might not be optimal, namely because Elasticsearch would provide non-relevant results and BERT would cost lots of time. Similar to \\[[BERTserini](https:\/\/arxiv.org\/pdf\/1902.01718.pdf)] which integrates BERT with another toolkit, we combined Elasticsearch and BERT, hoping to answer the queries in an efficient and accurate way.","206fe567":"## Predicting\nThis assumes that there is already a trained model in the previously mentioned directory in the Google Cloud bucket. If you want to train using a TPU you also need to enter the TPU's address. ","bf4c6f91":"To begin with, Elasticsearch is one of the most popular and powerful enterprise search engines, and BERT outperforms other models in tasks of question answering and language inference. For each query, we determined its keywords, and used Elasticsearch to output 50 most relevant articles from the dataset. Then we re-trained the BERT model with plain texts in Google Cloud, after which BERT would figure out the most suitable answer for the query. The procedure is as follows.","1645b8ab":"# Question answering using ElasticSearch and SciBERT\nThis notebook attempts to answer the most questions in the vaccines and [therapeutics tasks](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks?taskId=561) using a combination of ElasticSearch for the initial information retrieval and SciBERT for the further answering of the questions. It is loosly based on [this paper by David R. Cheriton](https:\/\/arxiv.org\/pdf\/1902.01718.pdf). \n\nRoughly what it does is the following:\n\n1.   Retrieve relevant papers based on keywords (this is annotated by humans)\n2.   Train a SciBERT model on the SQuAD 2.0 set for Question and Answering\n3.   Predict the answer for each of the questions based on each of the relevant articles and display the results.\n\nFor easy reading some of the sections have been hidden. In order to retrain the BERT model you need a Google Cloud Storage bucket to which you've write access. Since Kaggle doesn't provide this, the output of the model has been added as a data set. If you want to run the model yourself you should set the `is_kaggle` parameter below to `False` and enter the name of your Google Cloud Storage bucket below (in the Prerequisites section).","eb5ef9eb":"#### Checkpointing\nOptional: Store the results in a CSV file.","0c63f94e":"In order to make predictions, you need a trained model and access to a GCS bucket to which you've got write access.","b7649fbd":"## ElasticSearch\nThis section of the code processes all of the documents and reads them into the ElasticSearch index. This requires an Elasticsearch server to be up-and-running. Since we ran the server locally, we won't run the code below on Kaggle, however, all the code to preprocess the data, insert it and then search in the resulting index is here. If you're new to ElasticSearch I'd recommend first reading the [getting started guide](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/getting-started.html).","0cd09509":"## Abstract\n\nAs Coronavirus is spreading across the world, there is a rapid acceleration in related literature, making it difficult for the medical research community to keep up. In this notebook, we combined \\[[Elasticsearch](https:\/\/github.com\/elastic\/elasticsearch)] and \\[[BERT model](https:\/\/arxiv.org\/pdf\/1810.04805.pdf)] to mine useful text information from the article dataset and answer related queries.","ad9887e9":"# Postprocessing\nAfter the predictions have been made, we should do some post processing. This is mainly matching the position of the found answer to the original text and extracting passages from it that contain the answer.","b45ba852":"### Preprocessing\nWe'll attempt to extract the results and conclusion sections from the articles. This is done based on the heading titles. We first collected the most occuring section titles and then used these to determine which parts belong to the results and conclusions sections respectively.","8d1219b8":"### Ingestion\nCreate a document type using the ElasticSearch DSL package for the data and upload all papers that have a full text to the ElasticSearch server.","b1226bbe":"## Prerequisites\nThis code depends on Google's BERT implementation and the training script they've written to retrain the BERT model for the SQuAD challenge.\nIt also requires an ElasticSearch server as well as an Google TPU for faster training and prediction. Alternatively you can also run this code locally, though a GPU is highly recommended.  ","30227f2d":"# Answers\nThis section attempts to provide answers to the questions that were asked before."}}