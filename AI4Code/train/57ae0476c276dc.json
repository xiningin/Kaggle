{"cell_type":{"85d651dd":"code","ce986c61":"code","23239314":"code","14ea5c93":"code","04441d00":"code","c0416215":"code","e4d60cc4":"code","b1fc32c6":"code","5c4827fc":"code","cc19ffa3":"code","4051f80a":"code","2829de7c":"code","d029747c":"code","4680bc7b":"code","fd8a9255":"code","e9d42c81":"code","5d347dad":"code","eeab32e5":"code","6415028a":"code","4e1aaa61":"code","20eecc79":"code","1348b448":"code","0539c083":"code","48993bc4":"code","e9f55d6c":"code","f464abb5":"code","545ddbfe":"code","24f865e4":"code","38915cb1":"code","a4ab5569":"code","011a433b":"code","b37dac5d":"code","9611afd7":"code","dfeb4ac0":"code","9f2c3461":"code","8ef332c7":"code","40d31195":"code","84621c9a":"code","6594b9f6":"code","8258d4db":"code","c3b31829":"code","8815241f":"code","0fcacb25":"code","866d137a":"code","7e1928f1":"code","add211e9":"code","9a72a2ea":"code","5cf041f2":"code","aeac5248":"code","8c8bcdd2":"code","9698e7a1":"code","857269c2":"code","753329cf":"code","9c4638ee":"code","d7778376":"code","d686f951":"code","29b775eb":"code","ffff9019":"code","25b8759b":"code","da9a07dc":"code","5a7e1fe6":"markdown","3bd36e12":"markdown","b038e759":"markdown","39a1e0af":"markdown","90556707":"markdown","e8755d71":"markdown","686ce7ce":"markdown","ba38cd50":"markdown","ad1fd617":"markdown","adb17cde":"markdown","76db1012":"markdown","04c736a3":"markdown","beb5208c":"markdown","f4453650":"markdown"},"source":{"85d651dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#support python 2&3\nfrom __future__ import division, print_function, unicode_literals\n\n#common imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#make notebook's output stable across runs\nnp.random.seed(42)\n\n#Pretty Figures\n%matplotlib inline\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\n#Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action='ignore',message='^internal gelsd')\npd.options.mode.chained_assignment = None\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ce986c61":"# function to load data\ndataset_path = os.path.join(\"..\/input\",'forestfires.csv')\ndef load_data(path):\n        return pd.read_csv(dataset_path)\n    \nfire = load_data(dataset_path)\nfire.head()","23239314":"fire.FFMC[np.random.choice(fire.index,15)] = np.nan\nfire.info() #now we have 15 missing values for attribute FFMC","14ea5c93":"fire.describe() #some attributes skewed to the right(mean>median) some to the left(mean<median)","04441d00":"#let's name the categorical and numeical attributes \ncategorical_attributes = list(fire.select_dtypes(include=['object']).columns)\nnumerical_attributes = list(fire.select_dtypes(include=['float64', 'int64']).columns)\nprint('categorical_attributes:', categorical_attributes)\nprint('numerical_attributes:', numerical_attributes)","c0416215":"#Here we face an uniqe issue where a few months have only 1 or 2 data points. I chose to get rid of these so that the glm models\n#can handle the tests properly. Let me know how you would have hanlded.\nprint('months', fire.month.value_counts(), sep='\\n')\nprint('\\n')\nprint('days', fire.day.value_counts(), sep='\\n')","e4d60cc4":"months_to_remove = ['nov','jan','may']\nforest_fire = fire.drop(fire[fire.month.isin(months_to_remove)].index ,axis=0)\nforest_fire.month.value_counts()","b1fc32c6":"#visualizing distributions \nforest_fire.hist(bins=50, figsize=(15,10), ec='w')\nplt.show()\n#target-area-is heavily skewed, we have extreme outliers.","5c4827fc":"plt.hist(forest_fire.area, ec='w', bins=100, color='red')\nplt.text(800,100, 'max: '+str(forest_fire.area.max()), color='black', fontsize=14)\n#Burnt area attribute ranges from 0 to 1091.","cc19ffa3":"#Grouping the the burnt area to get a better understanding\nforest_fire['area_cat'] = pd.cut(forest_fire['area'], bins=[0,5, 10, 50, 100, 1100], include_lowest=True, \n                                 labels=['0-5', '5-10', '10-50', '50-100', '>100'])\nforest_fire.area_cat.value_counts()","4051f80a":"#Interquartile range\nQ1 = forest_fire.area.quantile(.25)\nQ3 = forest_fire.area.quantile(.75)\nIQR = 1.5*(Q3-Q1)\nIQR","2829de7c":"#we are loosing quite a number of data points in already a small data set if we remove all outliers\nforest_fire.query('(@Q1 - 1.5 * @IQR) <= area <= (@Q3 + 1.5 * @IQR)').area_cat.value_counts(sort=False)","d029747c":"#remove outliers\nforest_fire.drop(forest_fire[forest_fire.area>100].index,axis=0, inplace=True)\nforest_fire.area_cat.value_counts()","4680bc7b":"#Let's understand what temp ranges we have here.\nplt.hist(forest_fire.temp, ec='w', bins=50, color='red')\nplt.show()","fd8a9255":"forest_fire['temp_bins'] = pd.cut(forest_fire.temp, bins=[0, 15, 20, 25, 40], include_lowest=True, \n                                 labels=['0-15', '15-20', '20-25', '>25'])\nforest_fire.temp_bins.value_counts(sort=False)\n#so we have from very cold 0-15 degrees to hot >25 degree.","e9d42c81":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(forest_fire.values, forest_fire.temp_bins.values):\n    st_train_set = forest_fire.iloc[train_index]\n    st_test_set = forest_fire.iloc[test_index]","5d347dad":"#this works like magic.\nprint(st_test_set.temp_bins.value_counts(sort=False)\/len(st_test_set), sep='\\n')\nprint(forest_fire.temp_bins.value_counts(sort=False)\/len(forest_fire), sep='\\n')","eeab32e5":"#now lets drop the area and temp categories \nfor _ in (st_train_set, st_test_set):\n    _.drop(['area_cat','temp_bins'], axis=1, inplace=True)\n    \nforest_fire = st_train_set.copy()\nforest_fire.head()","6415028a":"#December had a few incidents but all on the higher burnt area side. Is it becuase dry weather or because more tourists? \nax = plt.figure(figsize=(12,8))\nax = sns.boxplot(x='month', y='area', data=forest_fire, color='lightgrey', )\nax = sns.stripplot(x='month', y='area', data=forest_fire, color='red', jitter=0.4, size=4)","4e1aaa61":"#There are more incidents on weekends - Friday\/Sat\/Sun, it might mean that campers vactioning might have caused\/spotted fires.\nax = plt.figure(figsize=(12,8))\nax = sns.boxplot(x='day', y='area', data=forest_fire, color='lightgrey', )\nax = sns.stripplot(x='day', y='area', data=forest_fire, color='red', jitter=0.4, size=4)","20eecc79":"#I am checking the temparature distribution as per the forest cordinates during the month of december\n#it looks like the temps are low, so it could be a combinaton of dry weather and human made fire. But as data scientists,\n#we must make important discoveries to arrive at any conclusive evidence. Else the above statments are just my mind made \n#fantsies\nforest_fire[forest_fire.month=='dec'].plot(kind='scatter', x='X', y='Y', c='temp', cmap=plt.get_cmap('coolwarm'), colorbar=True)\nplt.show()","1348b448":"corr_matrix = forest_fire.corr(method='spearman')\ncorr_matrix","0539c083":"ax = plt.figure(figsize=(12,8))\nax = sns.heatmap(corr_matrix, cmap='PiYG')","48993bc4":"#corrleation with area\ncorr_matrix.area.sort_values(ascending=False)","e9f55d6c":"#visualizing relations of most related attributes\nattributes = ['area', 'wind', 'temp', 'rain', 'RH']\nsns.pairplot(forest_fire[attributes])\nplt.show()","f464abb5":"#create a fresh copy of train to preprocess\nforest_fire = st_train_set.drop('area', axis=1)\nforest_fire_labels = st_train_set.area.copy()","545ddbfe":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass AttributeDeleter(BaseEstimator, TransformerMixin):\n    def __init__(self, delete=True):\n        self.delete = delete\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return np.delete(X,[fire.columns.get_loc(i) for i in['X','Y','area']],axis=1)\n            ","24f865e4":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\ncategorical_attributes = list(forest_fire.select_dtypes(include=['object']).columns)\nnumerical_attributes = list(forest_fire.select_dtypes(include=['float64', 'int64']).columns)\n\nnum_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')),\n                         ('drop_attributes', AttributeDeleter()),\n                         ('std_scaler', StandardScaler()),\n                        ])\nfull_pipeline = ColumnTransformer([('num', num_pipeline, numerical_attributes),\n                                   ('cat', OneHotEncoder(), categorical_attributes),\n                                  ])\n\ntrain = full_pipeline.fit_transform(forest_fire)\ntrain_labels = forest_fire_labels\ntrain.shape ,forest_fire.shape","38915cb1":"#check the train data\ntrain_df = pd.DataFrame(train, columns= numerical_attributes[2:] + list(full_pipeline.named_transformers_.cat.categories_[0]) +\n             list(full_pipeline.named_transformers_.cat.categories_[1]))\ntrain_df.head()","a4ab5569":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nlin_reg = LinearRegression()\nlin_reg.fit(train, train_labels)\n\narea_predictions = lin_reg.predict(train)\nlin_mse = mean_squared_error(train_labels, area_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint('linear_train_rmse', lin_rmse) #model might be underfitting\n\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(lin_reg, train, train_labels, scoring='neg_mean_squared_error', cv=10)\nlin_rmse_scores = np.sqrt(-scores)\n\ndef explain_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n \nexplain_scores(lin_rmse_scores)","011a433b":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(train, train_labels)\n\narea_predictions = tree_reg.predict(train)\ntree_mse = mean_squared_error(train_labels, area_predictions)\ntree_rmse = np.sqrt(tree_mse)\nprint('tree_train_rmse', tree_rmse) #model obviously overfitting\n\nscores = cross_val_score(tree_reg, train, train_labels, scoring='neg_mean_squared_error', cv=10)\ntree_rmse_scores = np.sqrt(-scores)\nexplain_scores(tree_rmse_scores)","b37dac5d":"from sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor()\nrf_reg.fit(train, train_labels)\n\narea_predictions = rf_reg.predict(train)\nrf_mse = mean_squared_error(train_labels, area_predictions)\nrf_rmse = np.sqrt(rf_mse)\nprint('rf_train_rmse', rf_rmse) #model is overfitting \n\nscores = cross_val_score(rf_reg, train, train_labels, scoring='neg_mean_squared_error', cv=10)\nrf_rmse_scores = np.sqrt(-scores)\nexplain_scores(rf_rmse_scores)","9611afd7":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel='linear')\nsvm_reg.fit(train, train_labels)\n\narea_predictions = svm_reg.predict(train)\nsvm_mse = mean_squared_error(train_labels, area_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nprint('svm_train_rmse', svm_rmse) #svm is generalizing well to crossvalidation set\n\nscores = cross_val_score(svm_reg, train, train_labels, scoring='neg_mean_squared_error', cv=10)\nsvm_rmse_scores = np.sqrt(-scores)\nexplain_scores(svm_rmse_scores)","dfeb4ac0":"from sklearn.neighbors import KNeighborsRegressor\n\nknn_reg = KNeighborsRegressor()\nknn_reg.fit(train, train_labels)\n\narea_predictions = knn_reg.predict(train)\nknn_mse = mean_squared_error(train_labels, area_predictions)\nknn_rmse = np.sqrt(knn_mse)\nprint('knn_train_rmse', knn_rmse) #overfiiting\n\nscores = cross_val_score(knn_reg, train, train_labels, scoring='neg_mean_squared_error', cv=10)\nknn_rmse_scores = np.sqrt(-scores)\nexplain_scores(knn_rmse_scores)","9f2c3461":"#lets improve the models with hyperparameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [{'bootstrap':[False,True],'n_estimators':[75,100,125,150,200], 'max_features':[1,2,4,6]}]\n\nforest_reg = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n\ngrid_search.fit(train, train_labels)","8ef332c7":"grid_search.best_params_ ","40d31195":"grid_search.best_estimator_","84621c9a":"feature_importances = grid_search.best_estimator_.feature_importances_\nattributes = numerical_attributes + list(full_pipeline.named_transformers_.cat.categories_[0]) +\\\n             list(full_pipeline.named_transformers_.cat.categories_[1])\n    \nsorted(zip(feature_importances, attributes), reverse=True)","6594b9f6":"indices = np.argsort(feature_importances)\nplt.figure(figsize=(12,8))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), feature_importances[indices], color='lightblue', align='center')\nplt.yticks(range(len(indices)), [attributes[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","8258d4db":"#lets try with RandomizedSearchCV\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_dist = {'n_estimators': randint(low=10, high=250),\n              'max_features': randint(low=1, high=24),\n             }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_dist, n_iter=40, cv=5, scoring='neg_mean_squared_error', random_state=42)\n\nrnd_search.fit(train, train_labels)\n","c3b31829":"rnd_search.best_params_","8815241f":"np.sqrt(-rnd_search.best_score_), np.sqrt(-grid_search.best_score_) #both have very similar scores even though they came up with \n#different best parameters. which one to use, I leave to you explain","0fcacb25":"#tuning svr\n\nparam_grid = [{'kernel': ['linear'], 'C':[0.5,1,5,10,30]},\n              {'kernel':['rbf'], 'C':[5,10,15,20], 'gamma':[0.5,1.0,1.5,2.0]},\n             ]\n\nsvm_reg = SVR()\ngrid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=4)\ngrid_search.fit(train,train_labels)\n\nnegative_mse = grid_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse","866d137a":"grid_search.best_params_","7e1928f1":"from scipy.stats import expon, reciprocal\n\nparam_dist = {'kernel':['linear','rbf'],\n                  'C':reciprocal(1,100),\n                  'gamma':expon(scale=1.0)}\nsvm_reg = SVR()\nrnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_dist, n_iter=100, cv=5,\n                               scoring='neg_mean_squared_error', verbose=2, n_jobs=4, random_state=42)\nrnd_search.fit(train, train_labels)\n\nnegative_mse = rnd_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse","add211e9":"rnd_search.best_params_ #randomized search is able to find better parameters for rbf kernel in same number of iterations","9a72a2ea":"def indices_top_features(impotance, top):\n    return np.sort(np.argpartition(np.array(impotance), -top)[-top:])\n\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importnaces, top):\n        self.feature_importnaces = feature_importances\n        self.top = top\n    def fit(self, X, y=None):\n        self.feature_indcies_ = indices_top_features(self.feature_importnaces, self.top)\n        return self\n    def transform(self,X):\n        return X[:, self.feature_indcies_]","5cf041f2":"data_prep_feature_seletion_pipe = Pipeline([('prep', full_pipeline),\n                                            ('fe_select', TopFeatureSelector(feature_importances,5)) #here am choosing top 5 features you can choose others depending on \n                                           ])                                                        #on what you want to keep  ","aeac5248":"train_fe_selected = data_prep_feature_seletion_pipe.fit_transform(forest_fire)\ntrain_fe_selected.shape","8c8bcdd2":"#now let's try knn with these reduced dimensions \n\nknn_reg = KNeighborsRegressor()\nknn_reg.fit(train_fe_selected, train_labels)\n\narea_predictions = knn_reg.predict(train_fe_selected)\nknn_mse = mean_squared_error(train_labels, area_predictions)\nknn_rmse = np.sqrt(knn_mse)\nprint('knn_train_rmse', knn_rmse) #knn is generalizing well to crossvalidation set\n\nscores = cross_val_score(knn_reg, train_fe_selected, train_labels, scoring='neg_mean_squared_error', cv=10)\nknn_rmse_scores = np.sqrt(-scores)\nexplain_scores(knn_rmse_scores)","9698e7a1":"#lets tune KNN \nparam_grid = {'weights': ['uniform', 'distance'], 'n_neighbors': list(range(1,36,5))}\n\nknn_reg= KNeighborsRegressor()\nknn_grid_search = GridSearchCV(knn_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=4)\nknn_grid_search.fit(train_fe_selected,train_labels)\n\nknn_negative_mse = knn_grid_search.best_score_\nknn_rmse = np.sqrt(-knn_negative_mse)\nknn_rmse","857269c2":"#and for final model I want to try GBM\nfrom xgboost import XGBRegressor\n\nxgb_reg = XGBRegressor()\nxgb_reg.fit(train_fe_selected, train_labels)\n\narea_predictions = xgb_reg.predict(train_fe_selected)\nxgb_mse = mean_squared_error(train_labels, area_predictions)\nxgb_rmse = np.sqrt(xgb_mse)\nprint('xgb_train_rmse', xgb_rmse) #overfitting\n\nscores = cross_val_score(xgb_reg, train_fe_selected, train_labels, scoring='neg_mean_squared_error', cv=10)\nxgb_rmse_scores = np.sqrt(-scores)\nexplain_scores(xgb_rmse_scores)","753329cf":"param_grid = {'objective':['reg:linear'],\n              'learning_rate': [0.02,0.03,0.04], \n              'max_depth': [1,2],\n              'min_child_weight': [2,3,4],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.2,0.3,0.4],\n              'n_estimators': [50,60,70,100]}\n\nxgb_reg = XGBRegressor()\n\nxgb_grid_search = GridSearchCV(xgb_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=4)\nxgb_grid_search.fit(train_fe_selected,train_labels)\n\nxgb_negative_mse = grid_search.best_score_\nxgb_rmse = np.sqrt(-xgb_negative_mse)\nxgb_rmse","9c4638ee":"xgb_grid_search.best_params_","d7778376":"final_model = knn_grid_search.best_estimator_\n\nX_test = st_test_set.drop(['area'], axis=1)\ny_test = st_test_set['area'].copy()\n\nX_test_prepared = data_prep_feature_seletion_pipe.transform(X_test)\n\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","d686f951":"prepare_select_and_predict_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, 5)),\n    ('final_model', XGBRegressor(**xgb_grid_search.best_params_))\n])","29b775eb":"prepare_select_and_predict_pipeline.fit(forest_fire,forest_fire_labels)","ffff9019":"final_predictions = prepare_select_and_predict_pipeline.predict(X_test)","25b8759b":"some_data = forest_fire[:4]\nsome_labels = forest_fire_labels[:4]\n\nprint(\"Predictions:\\t\", prepare_select_and_predict_pipeline.predict(some_data))\nprint(\"Labels:\\t\\t\", list(some_labels))","da9a07dc":"#Confidence Interval of our Predictions will help us better understand the ouput of our model\nfrom scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions-y_test)**2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors)-1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))","5a7e1fe6":"#### With same reasoning, you need to do a lot of transformations depending on the task, so having a custom pipeline that helps you automate these tasks will simplify your deployment process in the end.\n\n#### To handle missing data, I chose to fill them with median values of the attribute you can choose to do differently, or once you finalize your model you can do a hyperparameter tuning to find out which imputing strategy works best using a grid search. Imputing simply means what value are you going to put in place of null values.\n\n#### I chose to create of dummies out of my categorical attributes, but you can do a low-level vector representation of them using embeddings. OneHotEncoder does that for me.\n\n#### And lastly, we have data attributes in many different scales. So, it's always a good idea to either standardize or normalize the scales. Reason being many instances based and model based learning have a distance based cost function to minimize to get the best possible similarity or model fit parameters. Having a standardized scale helps us find the best possible parameters in an efficient way.","3bd36e12":"#### Now we can finally start the model selection process where we try out bunch of different models. Choose some and fine tune them before finalizing one.","b038e759":"#### Spearman rank correlation is more robust to the effect of outliers than Pearson\u2019s correlations coefficient. If you need to know why I will load a new kernel sometime. But do remember we are just checking linear correlations here.","39a1e0af":"#### Now the time for our assumptions. I remind you many assumptions we make during the iterative process might not make to the final model; As I already said, we make some assumptions, then during the model building process we might prove them right or wrong. But it always to helps to list the assumptions and take opinions of several domain experts but this is the luxury not every data scientist has.\n\n#### Assumption1- Temperature might be a great indicator of wildfires and it might be the case that high temperature may lead to more area burnt.","90556707":"#### the dataset doesn't have any missing values, so I am randomly removing some data points from FFMC attribute to showcase how to handle missing data problems.\n","e8755d71":"#### As, I have assumed temp is a critical indicator, to have a golden test dataset I would prefer that both train and test has equally distributed temperature conditions. Stratified shuffle split will help us maintain the required strata compositions here. My strata set is temperature categories I have created above. Do note while creating groups make sure that all the groups are well represented.","686ce7ce":"#### I chose to remove any area greater than 100. You could do it differently, maybe with log transformations or simply removing every outlier example. It all depends how you state your problem. For me it is a matter of showcasing several tricks of Scikit-Learn, but I would have gone for a log-log model otherwise. Maybe in part2, where I will do a Regression framework using TensorFlow in eager.","ba38cd50":"#### I chose to remove coordinates X & Y before model building. The reason being here we are building a model that will use weather conditions to determine the severity of a wildfire based on area burnt, so keeping X&Y locations of one forest does \u2018t generalize well to others.\n#### Now in the process of data prep, you might need to create or drop several columns. So, it's a good idea to automate the process. Below an example of what an Attribute\/Feature deleter will look like.","ad1fd617":"### Once you create the test data remember to not touch it until you finalize your model. Any exploration of test data would result in a biased perspective that might not generalize well to new data once the system is live.","adb17cde":"### An example showing a small use case. Let's assume some_data here are the new data that our system will predict for.","76db1012":"#### Not enough opportunity for feature engineering. we might take help of external data sources but need domain experts' input.","04c736a3":"#### At times it is a good idea to use only your best features in the model, but it is context dependent. Below sample code helps you add another transformer to your pipeline that chooses the best feature based on an estimated feature importance","beb5208c":"### Let's say, I determined to deploy XGB model. I will add a predictor to my previously defined Pipeline as the last element., which will take care of the entire process from preparing the data to predicting area.","f4453650":"# Prediction of Burn Area using Meteorological Data\n\n### My goal with this kernel is to provide a reusable end-to-end Regression framework that is deployable in real world. It is imperative that the users adapt it to the dataset in hand and make small necessary changes, but the overall process and class-objects given can adapt to any regression system.\n\n### Also, this kernel showcases how Scikit-Learn library can simplify and automate data preprocessing, model building and predicting for practical deployment.\n\n### I chose this dataset because this problem showcases several fundamental challenges in data science, that are not always easy to overcome: - such as highly left-right skewed data, extreme outliers, underfitting, overfitting, lack of enough examples etc. Many of these problems can be dealt with by collecting more data, meaningful feature engineering, and may be removing rare cases or modelling with different techniques that can handle outliers much better, but that is not the motivation behind this kernel.\n\n#### This dataset covers meteorological and spatiotemporal data for forest fires (between 2000 and 2003) in Portugal\u2019s Montesinho Natural Park, with 13 attributes each for 517 such incidents. Our target attribute from these 13 is \u2018area\u2019 - total burned area in hectares (ha). The corresponding weather data is that which is registered by sensors when the fire was detected (or first broke out). We have: temp, RH (relative humidity), wind (speed), rain (accumulated precipitation over the last 30 minutes).\n\n#### We also have DMC, DC, ISI and FFMC are components of the Canadian Forest Fire Weather Index (FWI) System, which measures the effects of fuel moisture and wind on fire behavior. These have been calculated using consecutive daily observations of temperature, relative humidity, wind speed, and 24-hour rainfall - i.e. these are time-lagged and not instantaneous values, unlike the four weather variables. Roughly, the higher these components, the more the expected severity of the fire.\n\n#### Spatiotemporal data includes the month and day of the week that the incident occurred, and X and Y co-ordinates of the incident with respect to the park. Smaller fires are much more frequent. This is the case in this dataset, as well as with incidences of wildfires around the world, making this a difficult regression problem.\n\n"}}