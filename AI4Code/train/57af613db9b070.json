{"cell_type":{"6e31c950":"code","0049bc3e":"code","5249cfa9":"code","7c200abf":"code","edabacb0":"code","42a70414":"code","79f2af46":"code","5d0fd7db":"code","69426397":"code","87309db8":"code","886322b5":"code","1a8b6fd6":"code","bf1db359":"code","4f256c65":"code","1fd69a1d":"code","5878ead1":"code","a9bbf57c":"code","20ecd648":"code","a9f0d24f":"code","c6558def":"code","5201ebc8":"code","5a969d88":"code","ea7ed8a8":"code","74cd32bb":"code","a888a0f6":"code","ba82e2b5":"code","e7251895":"code","73dfb89c":"code","6f63b955":"code","1945a3f2":"code","3af82cca":"code","641b12c8":"code","0d488c3c":"code","5457744d":"code","e71650db":"code","5db27544":"code","fceb428c":"code","7a23d3e8":"code","8bd998a0":"code","1b0d01f4":"markdown","b4d22c1b":"markdown","fcae26bb":"markdown","27799cb3":"markdown","ccec673b":"markdown","e469073c":"markdown","522573f4":"markdown","5dbaf08b":"markdown","8093e7a3":"markdown","2bc12970":"markdown","f37e4fc6":"markdown","05461668":"markdown","d8092e4f":"markdown","949e0a71":"markdown","4477401d":"markdown","2e13f391":"markdown","6d196d9a":"markdown"},"source":{"6e31c950":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0049bc3e":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns","5249cfa9":"#Load our dataset\ndf= pd.read_csv('..\/input\/iris-data-set\/iris_dataset.csv')","7c200abf":"df.head()","edabacb0":"#dropping the Id column \ndf.drop('Id',axis=1,inplace=True)","42a70414":"df.head()","79f2af46":"#encoding categorical column values in species to numercial values using label encoder\n#create an instance of label encoder \nlabelencoder=LabelEncoder()","5d0fd7db":"#create a column label with numerical values tranformed from species categorical values\ndf['label']=labelencoder.fit_transform(df['species'])","69426397":"#view dimensions of our dataset \ndf.shape","87309db8":"#preview summary of our dataset - first top 5 rows\ndf.head()","886322b5":"#preview summary of our dataset - last bottom 5 rows\ndf.tail()","1a8b6fd6":"#view column names\ndf.columns","bf1db359":"#checking the distribution of our target column \ndf.species.value_counts()","4f256c65":"#view the percentage distribution of our target variable\ndf.species.value_counts()\/np.float(len(df))","1fd69a1d":"#checking missing values in our dataset\ndf.isnull().sum()","5878ead1":"#getting to view summary of the data\ndf.info()","a9bbf57c":"#getting to view a statistical summary of our data rounded two decimal places\nround(df.describe(),2)","20ecd648":"#drawing box plots to check for outliers\nplt.figure(figsize=(24,20))\n\nplt.subplot(4,2,1)\nfig=df.boxplot(column='sepal_length')\nfig.set_title('')\nfig.set_ylabel('sepal_length')\n\nplt.subplot(4,2,2)\nfig=df.boxplot(column='sepal_width')\nfig.set_title('')\nfig.set_ylabel('sepal_width')\n\nplt.subplot(4,2,3)\nfig=df.boxplot(column='petal_length')\nfig.set_title('')\nfig.set_ylabel('petal_length')\n\nplt.subplot(4,2,4)\nfig=df.boxplot(column='petal_width')\nfig.set_title('')\nfig.set_ylabel('petal_width')\n","a9f0d24f":"#plotting histograms to check the distributions of our variables\nplt.figure(figsize=(20,24))\n\nplt.subplot(4,2,1)\nfig=df['sepal_length'].hist(bins=20)\nfig.set_xlabel('sepal_length')\nfig.set_ylabel('no of sepal_length')\n\n\nplt.subplot(4,2,2)\nfig=df['sepal_width'].hist(bins=20)\nfig.set_xlabel('sepal_width')\nfig.set_ylabel('no of sepal_width')\n\nplt.subplot(4,2,3)\nfig=df['petal_length'].hist(bins=20)\nfig.set_xlabel('petal_length')\nfig.set_ylabel('no of petal_length')\n\nplt.subplot(4,2,4)\nfig=df['petal_width'].hist(bins=20)\nfig.set_xlabel('petal_width')\nfig.set_ylabel('no of petal_width')","c6558def":"sns.set_style('darkgrid')\nsns.FacetGrid(df,hue='species',height=4).map(plt.scatter,\n                                            'sepal_length',\n                                            'petal_length').add_legend()","5201ebc8":"sns.set_style('darkgrid')\nsns.FacetGrid(df,hue='species',height=4).map(plt.scatter,\n                                            'sepal_width',\n                                            'petal_width').add_legend()","5a969d88":"#Creating our feature vector\nX=df.drop(['species','label'],axis=1)","ea7ed8a8":"#Creating our target variable\ny=df.label","74cd32bb":"#spliting X and y into training and testing sets\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)","a888a0f6":"#check the shape of X_train and X_test\nX_train.shape,X_test.shape","ba82e2b5":"#Assigning cols the values in our X_train data set\ncols=X_train.columns","e7251895":"#checking the values in the cols variable\ncols","73dfb89c":"scaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)","6f63b955":"X_train=pd.DataFrame(X_train,columns=[cols])\nX_test=pd.DataFrame(X_test,columns=[cols])","1945a3f2":"#checking the statitical summary of our X_train data\nX_train.describe()","3af82cca":"#Support Vector Machine Classifier\nfrom sklearn.svm import SVC","641b12c8":"#import metrics to calculate accuracy of our model\nfrom sklearn.metrics import accuracy_score","0d488c3c":"#instanciate SVC classifier with default hyperparameters\nmodel_svc=SVC(kernel='rbf',C=1000)","5457744d":"#fitting our model to the training dataset\nmodel_svc.fit(X_train,y_train)","e71650db":"#make prediction on our test dataset\npred=model_svc.predict(X_test)\ntrain=model_svc.predict(X_train)","5db27544":"#compute the accuracy score of our model\nprint('Model accuracy with default hyperparamters:{0:0.4f}'.format(accuracy_score(y_test,pred)))","fceb428c":"#Using Linear Kernel on our SVC classifier model \nmodel_svc=SVC(kernel='linear',C=1000)\nmodel_svc.fit(X_train,y_train)\npred=model_svc.predict(X_test)\nprint('Model accuracy with kernel function as linear on training data set: %.4f'% accuracy_score(y_train,train))\nprint('Model accuracy with kernel function as linear on testing data set : %.4f'% accuracy_score(y_test,pred))","7a23d3e8":"#Using Polynomial Kernel on our SVC classifier model \nmodel_svc=SVC(kernel='poly',C=1000)\nmodel_svc.fit(X_train,y_train)\npred=model_svc.predict(X_test)\nprint('Model accuracy with kernel function as polynomial on training data set: %.2f'% accuracy_score(y_train,train))\nprint('Model accuracy with kernel function as polynomial  on testing data set : %.2f'% accuracy_score(y_test,pred))","8bd998a0":"#Using Sigmoid Kernel on our SVC classifier model \nmodel_svc=SVC(kernel='sigmoid',C=1000)\nmodel_svc.fit(X_train,y_train)\npred=model_svc.predict(X_test)\nprint('Model accuracy with kernel function as sigmoid on training data set: %.2f'% accuracy_score(y_train,train))\nprint('Model accuracy with kernel function as sigmoid on testing data set : %.2f'% accuracy_score(y_test,pred))","1b0d01f4":"### Exploratory Data Analysis","b4d22c1b":"The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example which translates less outliers from a higher value of C\nFor large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly.","fcae26bb":"### Model Building ","27799cb3":"There are 150 instances and 6 variables . (150 rows 6 columns)","ccec673b":"Feature scaling through standardization can be an important preprocessing step for many machine learning algorithms.A feature scaling method called standardization, gives our data the property of a standard normal distribution.\n\n\"One of the reasons why standardization helps with gradient descent learning is that\nthe optimizer has to go through fewer steps to find a good or optimal solution.\" - Sebastian Raschka\nVahid Mirjalili (Machine Learning and Deep Learning with Python,scikit-learn, and TensorFlow)","e469073c":"### Feature Scaling","522573f4":"From the various types of SVM Kernel functions  the linear kernel works fine this shows that from the  iris dataset it's  linearly separable.","5dbaf08b":"There are no missing values in our data all the feature variables are of float data type the target in an object and the one we changed is an integer.","8093e7a3":"There is an equal distribution from the values in our target column all have the same values.","2bc12970":"There are 4 continous variables with one categorical variable which is our outcome.","f37e4fc6":"#### Declare feature vector  and target variable","05461668":"From the visualization of the box plots we find out that there are outliers in our sepal_width column values from our data set","d8092e4f":"### Visualizing our target values","949e0a71":"From the graph we see that Iris_setosa can be linearly separated from the other two flower types based on sepal and petal length and width.","4477401d":"Support Vector Machine (SVM)Algorithm is a type of supervised learning algorithm that can be used for both classification and regression purposes. \n\nUsing the public data set of Iris we shall use SVM  classifier to try and  classify the iris species that falls into 3 catogories namely 'Iris-setosa','Iris-versicolor','Iris-virginica' with measurement features that include: 'sepal_length','sepal_width','petal_length','petal_width','species','label'.\n\nRef image source: http:\/\/www.lac.inpe.br\/~rafael.santos\/Docs\/CAP394\/WholeStory-Iris.html\n![image.png](attachment:08896da1-e826-40aa-9272-994a7ccc4af6.png)\n\n **I hope you find this Kernel helpful ,comment with suggestions as I am on a learning journey.Don't forget to  UPVOTE** ","2e13f391":"Increaing our C values increases the accuracy score based on testing data and our linear kernel produces the best score at 0.93 compared to rbf at 0.90 accuracy,the poly kernel follows with 0.87 and lastly sigmoid it 0.77","6d196d9a":"### Spliting our data into training and testing set"}}