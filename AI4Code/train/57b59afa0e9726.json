{"cell_type":{"f67586b2":"code","de328481":"code","0ff0f235":"code","2f5dd249":"code","5e44e291":"code","ca1b4d3a":"code","67d8ced2":"code","db49f4e8":"code","ef83fd81":"code","a2d49362":"code","ebdd157f":"code","179affcb":"code","132391ed":"code","51ee971f":"code","ecb23d9f":"code","a3a0553b":"code","b8331ebb":"code","6b9b3ae4":"code","b4a653a0":"code","08ec5af7":"code","9f524774":"code","ac6862d2":"code","f6881423":"code","30fdfa77":"markdown","2d67e70a":"markdown","f1c0d7a0":"markdown","948b5fe5":"markdown","8771c165":"markdown","b7fbf07c":"markdown","7442606f":"markdown","1a8f7303":"markdown","2a99529e":"markdown","1e40de1e":"markdown","7a5c4593":"markdown","4ec75384":"markdown","c1e844b4":"markdown","9633ad2d":"markdown","5d6606b1":"markdown","4f42bee6":"markdown"},"source":{"f67586b2":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing","de328481":"train_path = r'..\/input\/glass-imbalanced\/glass (Imbalanced).xlsx'\ndata_train = pd.read_excel(train_path)\ndata_train.head()","0ff0f235":"# PALETTE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom matplotlib.colors import LinearSegmentedColormap\nimport numpy as np\nimport matplotlib\n\ndef colorFader(c1,c2,mix=0): #fade (linear interpolate) from color c1 (at mix=0) to c2 (mix=1)\n    c1=np.array(matplotlib.colors.to_rgb(c1))\n    c2=np.array(matplotlib.colors.to_rgb(c2))\n    return matplotlib.colors.to_hex((1-mix)*c1 + mix*c2)\n\nmeshPalette = []\nmeshPalette_rev = []\nnCol = 50\n\nfor i in range(nCol):\n    meshPalette.append(colorFader(\"#71706E\", \"#990303\", i\/nCol))\n    meshPalette_rev.append(colorFader(\"#990303\",\"#9C9999\", i\/nCol))\n\ncm = LinearSegmentedColormap.from_list(\"cmap_name\", meshPalette, N=nCol)\ncm_rev = LinearSegmentedColormap.from_list(\"cmap_name\", meshPalette_rev, N=nCol)","2f5dd249":"plt.figure(figsize=(8, 7))\nsns.heatmap(data_train.corr(method='pearson'), vmin=-1, vmax=1, cmap='viridis', annot=True, fmt='.2f');","5e44e291":"np.unique(np.array(data_train['Class']))","ca1b4d3a":"features = data_train.iloc[:,:9].columns.tolist()\nplt.figure(figsize=(18, 27))\n\nfor i, col in enumerate(features):\n    plt.subplot(6, 4, i*2+1)\n    plt.subplots_adjust(hspace =.25, wspace=.3)\n    \n#     plt.grid(True)\n#     plt.title(col)\n#     sns.kdeplot(data_train.loc[data_train[\"Class\"]=='negative', col], label=\"negative\", color = \"blue\", shade=True, kernel='gau', cut=0)\n#     sns.kdeplot(data_train.loc[data_train[\"Class\"]=='positive', col], label=\"positive\",  color = \"yellow\", shade=True, kernel='gau', cut=0)\n#     plt.subplot(6, 4, i*2+2) \n    sns.boxplot(y = col, data = data_train, x=\"Class\", palette = [\"blue\", \"yellow\"]) ","67d8ced2":"label_train = data_train.iloc[:,-1].to_numpy()\nfitur_train = data_train.iloc[:,:9].to_numpy()","db49f4e8":"scaler = MinMaxScaler(feature_range=(0, 1))\nscaler.fit(fitur_train)\nfitur_train_normalize = scaler.transform(fitur_train)","ef83fd81":"label_train = label_train.ravel() \nlb = preprocessing.LabelBinarizer()\nlabel_train = lb.fit_transform(label_train)\n\nX_train, X_test, y_train, y_test = train_test_split(fitur_train_normalize, label_train, random_state=8, stratify=label_train, test_size=0.3  )","a2d49362":"inputSize = X_train.shape[1]\nweight = np.matrix(np.random.uniform(0, 1, (5, inputSize)))\nbias = np.matrix(np.random.uniform(0, 1, (1, 5)))\noutputSize = 1\nhiddenSize = 5\nH = 0\nbeta = 0","ebdd157f":"X = np.matrix(X_train)\ny = np.matrix(y_train)        \nH = (X * weight.T) + bias","179affcb":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-1 * x))\n\nH = sigmoid(H) ","132391ed":"H_moore_penrose = np.linalg.inv(H.T * H) * H.T","51ee971f":"beta = H_moore_penrose * y\nbeta","ecb23d9f":"result_train = H * beta","a3a0553b":"y_pred = (result_train > 0.5).astype(int)","b8331ebb":"y_test_new = lb.fit_transform(y_train)\nprint('Accuracy Training: ', accuracy_score(y_test_new, y_pred))","6b9b3ae4":"X = np.matrix(X_test)\nresul_test = sigmoid((X * weight.T) + bias) * beta\ny_test_new = lb.fit_transform(y_test)\ny_pred = (resul_test > 0.5).astype(int)\nscore = accuracy_score(y_test_new, y_pred)\nprint('Accuracy Testing: ', score)","b4a653a0":"class ELM(object):  \n  def __init__(self, inputSize, outputSize, hiddenSize, weight, bias):\n    self.inputSize = inputSize\n    self.outputSize = outputSize\n    self.hiddenSize = hiddenSize  \n    self.weight = weight\n    self.bias = bias\n    self.H = 0\n    self.beta = 0\n\n  def sigmoid(self, x):\n    return 1 \/ (1 + np.exp(-1 * x))\n\n  def predict(self, X):\n    X = np.matrix(X)\n    y = self.sigmoid((X * self.weight.T) + self.bias) * self.beta\n    return y\n\n  def train(self, X, y):\n    X = np.matrix(X)\n    y = np.matrix(y)        \n    self.H = (X * self.weight.T) + self.bias\n    self.H = self.sigmoid(self.H)      \n    H_moore_penrose = np.linalg.inv(self.H.T * self.H) * self.H.T\n    self.beta = H_moore_penrose * y\n    return self.H * self.beta","08ec5af7":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=10, random_state=1, shuffle=True) \nkf.get_n_splits(fitur_train_normalize)","9f524774":"arrHiddenLayer = []\nfor h in range(1,21):\n    acc_train = [] \n    acc_test = []\n    for train_index, test_index in kf.split(fitur_train_normalize):\n        X_train, X_test = fitur_train_normalize[train_index], fitur_train_normalize[test_index]\n        y_train, y_test = label_train[train_index], label_train[test_index]\n\n        inputSize = X_train.shape[1]\n        weight = np.matrix(np.random.uniform(0, 1, (h, inputSize)))\n        bias = np.matrix(np.random.uniform(0, 1, (1, h)))\n        elm = ELM(inputSize, 1, h, weight, bias )\n\n        hasilTrain = elm.train(X_train, y_train)\n        y_test_new = lb.fit_transform(y_train)\n        y_pred = (hasilTrain > 0.5).astype(int)\n        acc = accuracy_score(y_test_new, y_pred)\n#         print('Accuracy Training: ', acc)\n        acc_train.append(acc)\n\n        # ------------ Testing -----------\n        y_pred = elm.predict(X_test)\n        y_test_new = lb.fit_transform(y_test)\n        y_pred = (y_pred > 0.5).astype(int)\n        score = accuracy_score(y_test_new, y_pred)\n#         print('Accuracy Testing: ', score)\n        acc_test.append(score)\n    print('Mean Accuracy Training : ',np.mean(acc_train))\n    print('Mean Accuracy Testing :', np.mean(acc_test))\n    print('-------------------------------')\n    arrHiddenLayer.append(np.mean(acc_test))","ac6862d2":"import matplotlib.pyplot as plt\nimport numpy as np\n\ns = np.arange(1,21)\n\nplt.plot(s, np.array(arrHiddenLayer), color='red', marker='o')\nplt.title('Hidden Neuron')\nplt.xlabel('hidden neuron')\nplt.ylabel('Accuracy')\nplt.show()\n\nplt.show()","f6881423":"print('The best hidden neuron: ', np.argmax(arrHiddenLayer)+1)","30fdfa77":"## Training","2d67e70a":"So that when combined, it will be like this.","f1c0d7a0":"# Introduction\nExtreme Learning Machine (ELM) is one of neural network algorithms which used single hidden layer feed-forward neural networks (SLFNs). The network topology of ELM has three laters: input layer, hidden layer, and output layer as follow on image below.\n\nI try to classify the simple dataset with imbalanced data, where I get from the KEEL repository.","948b5fe5":"And to get the output layer (target), use this equation.\n![image.png](attachment:image.png)","8771c165":"![image.png](attachment:image.png)","b7fbf07c":"Calculate the Moore-Penrose generalized inverse matrix\n![image.png](attachment:image.png)","7442606f":"# Dataset","1a8f7303":"# Preprocessing","2a99529e":"# Table of Content\n> **Introduction** <br\/><br\/>\n> **Dataset** <br\/><br\/>\n> **Preprocessing** <br\/><br\/>\n> **Extreme Learning Machine (ELM)** <br\/><br\/>\n> **Result** <br\/><br\/>","1e40de1e":"And then, calculate the output weight (Beta)\n![image.png](attachment:image.png)","7a5c4593":"# Result\n","4ec75384":"Find the optimal number of hidden neuron in range [1, 20] and to generalized the performance of the ELM, i used k-fold cross validation with 10 fold","c1e844b4":"Calculate the initialize of the matrix hidden layer.\n\n![image.png](attachment:image.png)\n","9633ad2d":"## Testing","5d6606b1":"# Extreme Learning Machine (ELM)","4f42bee6":"Then, calculate the output hidden layer with activation function. I used the sigmoid activation function\n![image.png](attachment:image.png)"}}