{"cell_type":{"44df179e":"code","ab11e4bf":"code","41380bbf":"code","15240fce":"code","54d6a158":"code","fc9bc00a":"code","664e7093":"code","df8070a2":"code","3ff6b00d":"code","124b1a80":"code","bfb990b6":"code","fa14393e":"code","c04a577f":"code","ffe47cc8":"code","dd3bf113":"code","aa323598":"code","68de1b79":"code","75dab53a":"code","ed0e54e7":"code","20e29e2a":"code","9e11158f":"code","f2f73ba7":"code","9ed008f3":"code","f01f2abe":"code","1888161f":"code","ab2a39bf":"code","5b3a2748":"code","579f3286":"code","4c28f8f3":"code","fe9a5d8b":"code","7623b808":"code","1a051991":"markdown"},"source":{"44df179e":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.metrics import make_scorer","ab11e4bf":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\nprint(df_train.columns)\ndf_train.describe(include='all')","41380bbf":"df_train.info()","15240fce":"null_data = (df_train.isnull().sum()\/df_train.isnull().count())*100\nnull_data.sort_values(ascending=False).head(4)","54d6a158":"null_data = (df_test.isnull().sum()\/df_test.isnull().count())*100\nnull_data.sort_values(ascending=False).head(4)","fc9bc00a":"print(df_train.columns.tolist())\ndf_train.hist(figsize=(12,8))\nplt.show()","664e7093":"print(df_test.columns.tolist())\ndf_test.hist(figsize=(12,8))\nplt.show()","df8070a2":"sns.catplot(x=\"Embarked\", y=\"Survived\",data=df_train)\nsns.catplot(x=\"Sex\", y=\"Survived\",data=df_train);","3ff6b00d":"sns.boxplot(x='SibSp', data=df_train)","124b1a80":"sns.boxplot(x='Fare', data=df_train)","bfb990b6":"sns.boxplot(x='Fare', data=df_test)","fa14393e":"# z = np.abs(stats.zscore(df_train['SibSp']))\n# df_train.iloc[np.where(z>3)]['SibSp']","c04a577f":"# def removing_outliers(df)\n# df_num = df_train.select_dtypes(include=[np.number])    ## selecting dataframe columns with numerical values\n# z = np.abs(stats.zscore(df_num))                      ## Getting Z score for the numerical values columns \n# outlier_rows = np.where(z>3)[0]\n# df_train.drop(outlier_rows, axis=0, inplace= True)\n# df_train.reset_index(drop=True, inplace=True)","ffe47cc8":"df_train[\"Embarked\"].fillna(df_train[\"Embarked\"].mode()[0], inplace=True)\ndf_test[\"Fare\"].fillna(df_test[\"Fare\"].median(), inplace=True)\n\ndef age_transform(df):\n    df[\"Age\"].fillna(df['Age'].median(), inplace=True)\n    bins = (0, 5, 12, 18, 25, 35, 60, 120)\n    group_names = ['Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    df[\"Age\"] = pd.cut(df[\"Age\"], bins, labels=group_names) \n    return df\n\ndef fare_transform(df):\n    df['Fare'] = pd.cut(df['Fare'], bins=[0, 7.91, 14.45, 31, 120, 200], \n                        labels=['Low_fare', 'median_fare', 'Average_fare','high_fare', 'v_high_fare'])\n    \n    return df\n\ndef name_transform(df):\n    df[\"Lname\"] = df[\"Name\"].apply(lambda x: x.split(',')[0])\n    df[\"prefix\"] = df[\"Name\"].apply(lambda x: x.split(' ')[1])\n    df['prefix'] =  df['prefix'].apply(lambda x: x.split('.')[0])\n    df['prefix'] = df['prefix'].apply(lambda x: x if x in ['Mr', 'Mrs', 'Miss', 'Master']  else 'Misc')\n    return df\n\ndef feature_transform(df):\n    df = name_transform(df)\n    df = age_transform(df)\n    df = fare_transform(df)\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    df.drop([\"PassengerId\",\"Name\", \"Ticket\", \"Cabin\", \"SibSp\", \"Parch\"], axis=1, inplace=True)\n    return df","dd3bf113":"df_train = feature_transform(df_train)\n\ndf_test = feature_transform(df_test)","aa323598":"def get_dummy_data(df):\n    cols = ['Age', 'Embarked', 'prefix', 'Sex', 'Fare']\n    df = pd.get_dummies(df, columns=cols, prefix=cols)\n    return df\ndf_train = get_dummy_data(df_train)\ndf_test = get_dummy_data(df_test)","68de1b79":"sns.heatmap(df_train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","75dab53a":"X = df_train.iloc[:, 1:]\ny = df_train.iloc[:, 0]\nX_test =  df_test","ed0e54e7":"X.head()","20e29e2a":"def features_encode(df):\n    features = df.columns\n    for feature in features:\n        le = LabelEncoder()\n        le = le.fit(df[feature])\n        df[feature] = le.transform(df[feature])\n    return df\n\nX= features_encode(X)\nX_test =  features_encode(X_test)","9e11158f":"scaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.fit_transform(X_test)","f2f73ba7":"X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size = 0.2, random_state = 0)","9ed008f3":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\n\nkfold = KFold(n_splits=10, random_state=22)\nmodel = XGBClassifier()\nparam_grid = {'n_estimators' : [100,210,300,400],\n              'gamma': [0.1, 0.05, 0.01,0.001],\n              'max_depth': [4,6, 8]\n              }\n\nmodelf = GridSearchCV(model, param_grid = param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodelf.fit(X_train,y_train)\n\n# Best score\nmodelf.best_score_\n\n# Best Estimator\nmodelf.best_estimator_","f01f2abe":"modelf.best_score_","1888161f":"clf = XGBClassifier(n_estimators=210, max_depth=6,gamma=0.05)\n# clf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)","ab2a39bf":"xgb_pred = clf.predict(X_cv)\naccuracy = accuracy_score(y_cv, xgb_pred)\nclf_report = classification_report(y_cv, xgb_pred)\nprint(\"XGB Classifier Accuracy: \", accuracy*100, \"%\", \"\\n\\n\",  \"XGB Classification Report: \\n \", clf_report)","5b3a2748":"clf = XGBClassifier(n_estimators=210, max_depth=6,gamma=0.05)\nclf.fit(X, y)","579f3286":"y_pred = modelf.predict(X_test)\ndf = pd.read_csv(\"..\/input\/gender_submission.csv\")\ndf[\"Survived\"] = y_pred\ndf.set_index(['PassengerId', 'Survived'], drop=True, inplace=True, )\ndf.to_csv(\"titanic_disaster.csv\")","4c28f8f3":"print('cheers !')","fe9a5d8b":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nrandom_forest.fit(X_train, y_train)\nY_pred_rf = random_forest.predict(X_test)\nrandom_forest.score(X_cv,y_cv)\nacc_random_forest = round(random_forest.score(X_cv, y_cv) * 100, 2)\n\nprint(\"Important features\")\n# pd.Series(random_forest.feature_importances_,df_train.columns).sort_values(ascending=True).plot.barh(width=0.8)\nprint('__'*30)\nprint(acc_random_forest)","7623b808":"df = pd.read_csv(\"..\/input\/gender_submission.csv\")\ndf[\"Survived\"] = Y_pred_rf\ndf.set_index(['PassengerId', 'Survived'], drop=True, inplace=True, )\ndf.to_csv(\"titanic_disaster.csv\")","1a051991":"## Filling Missing data and tranforming features"}}