{"cell_type":{"e3c07bdd":"code","0fe63807":"code","696ec217":"code","a1ad32d3":"code","c501954d":"code","d1694edc":"code","07f22d9e":"code","553a7d64":"code","03d83fb1":"code","851182ae":"code","f9ac1d05":"code","8f506b7a":"code","2dca6b3c":"code","553328e9":"code","3cf4be51":"code","321a89f0":"code","149ed540":"code","0626affd":"code","4fb76f16":"code","eba4d4f5":"code","aef6fb33":"code","7787b69a":"code","7193c764":"code","6b237394":"code","83c33f74":"code","d4605b76":"code","53b345d8":"code","0b8c070d":"code","cbbf6f90":"code","e98707fe":"code","a1052808":"code","f1afacc9":"code","bf51cbe2":"code","1feb2470":"code","82e0e28e":"code","cdf8a5d3":"code","aa8a52e1":"code","b09773b6":"code","32901120":"code","1940554b":"markdown","610b7389":"markdown","be6dcdc8":"markdown","5f5ab395":"markdown","749e59be":"markdown","32bee8e5":"markdown","41cfc111":"markdown","bfa404ff":"markdown","4170eef5":"markdown","3ef31a9a":"markdown","ca92a345":"markdown","8158da3c":"markdown","e7b4a383":"markdown","c1981cb2":"markdown","7ac21fd1":"markdown","a98e122f":"markdown","f481317c":"markdown","da3ed9a0":"markdown","1cdede5f":"markdown","60ba28cc":"markdown","d5587712":"markdown","f033e798":"markdown","d81ecd71":"markdown","11cb0213":"markdown","d36b0aad":"markdown","6a10363e":"markdown"},"source":{"e3c07bdd":"! pip install -q dabl","0fe63807":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport dabl\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\npd.options.display.max_columns = 8000","696ec217":"train_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain_data.head()","a1ad32d3":"train_data.shape","c501954d":"train_data.isna().sum()","d1694edc":"test_data.isna().sum()","07f22d9e":"# Remove outliers from OverallQual, GrLivArea and SalesPrice\ntrain_data.drop(train_data[(train_data['OverallQual']<5) & (train_data['SalePrice']>200000)].index, inplace=True)\ntrain_data.drop(train_data[(train_data['GrLivArea']>4500) & (train_data['SalePrice']<300000)].index, inplace=True)\ntrain_data.reset_index(drop=True, inplace=True)","553a7d64":"train_labels = train_data['SalePrice']\ntrain_features = train_data.drop(['SalePrice'], axis=1)\n\ndata = pd.concat([train_features, test_data]).reset_index(drop=True)","03d83fb1":"# These columns have a lot of Null values, so we drop them\ndata = data.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\ndata.head()","851182ae":"plt.style.use(\"fivethirtyeight\")\nplt.figure(figsize=(16, 9))\nsns.distplot(data['MSSubClass'])\nplt.xlabel(\"Type of Dwelling\")\nplt.ylabel(\"Count\")\nplt.title(\"Dwelling Type Count\")\nplt.show()","f9ac1d05":"plt.style.use(\"ggplot\")\nplt.figure(figsize=(16, 9))\nsns.countplot(data['MSZoning'])\nplt.xlabel(\"Type of Zoning of the property\")\nplt.ylabel(\"Count\")\nplt.title(\"Zone Type Count\")\nplt.show()","8f506b7a":"plt.style.use(\"classic\")\nplt.figure(figsize=(16, 9))\nsns.distplot(data['LotFrontage'])\nplt.xlabel(\"Lot Frontage (in ft)\")\nplt.ylabel(\"Count\")\nplt.title(\"Lot Frontage Distribution\")\nplt.show()","2dca6b3c":"plt.style.use(\"classic\")\nplt.figure(figsize=(16, 9))\nsns.distplot(train_labels, color='red')\nplt.xlabel(\"Price (in $)\")\nplt.ylabel(\"Count\")\nplt.title(\"Sales Price Distribution\")\nplt.show()","553328e9":"sns.pairplot(data.corr())","3cf4be51":"plt.figure(figsize=(16, 9))\nsns.heatmap(data.corr())\nplt.show()","321a89f0":"dabl.plot(train_data, target_col='SalePrice')","149ed540":"train_labels = train_labels.apply(lambda x: np.log(1+x))","0626affd":"plt.style.use(\"classic\")\nplt.figure(figsize=(16, 9))\nsns.distplot(train_labels, color='red')\nplt.xlabel(\"Price (in $)\")\nplt.ylabel(\"Count\")\nplt.title(\"Sales Price Distribution\")\nplt.show()","4fb76f16":"data['MSSubClass'] = data['MSSubClass'].apply(str)\ndata['YrSold'] = data['YrSold'].astype(str)\ndata['MoSold'] = data['MoSold'].astype(str)\n\n# the data description states that NA refers to typical ('Typ') values\ndata['Functional'] = data['Functional'].fillna('Typ')\n# Replace the missing values in each of the columns below with their mode\ndata['Electrical'] = data['Electrical'].fillna(\"SBrkr\")\ndata['KitchenQual'] = data['KitchenQual'].fillna(\"TA\")\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\ndata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])\ndata['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n# Replacing the missing values with 0, since no garage = no cars in garage\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    data[col] = data[col].fillna(0)\n# Replacing the missing values with None\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    data[col] = data[col].fillna('None')\n# NaN values for these categorical basement features, means there's no basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    data[col] = data[col].fillna('None')\n\n# Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\ndata['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# We have no particular intuition around how to fill in the rest of the categorical features\n# So we replace their missing values with None\nobjects = []\nfor i in data.columns:\n    if data[i].dtype == object:\n        objects.append(i)\ndata.update(data[objects].fillna('None'))\n\n# And we do the same thing for numerical features, but this time with 0s\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in data.columns:\n    if data[i].dtype in numeric_dtypes:\n        numeric.append(i)\ndata.update(data[numeric].fillna(0))","eba4d4f5":"data.isna().sum()","aef6fb33":"data = data.drop(['Id'], axis=1)","7787b69a":"data.head()","7193c764":"# Make a list of all categorical columns\ncat_cols = ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']\n\n# Get the dummy variables from them\ndata = pd.get_dummies(data, columns=cat_cols)","6b237394":"# Recheck the shape of the data\ndata.shape","83c33f74":"# Remove any repeated columns\ndata = data.iloc[:, ~data.columns.duplicated()]","d4605b76":"# Identify the split percent and split the data\ntrain = data[:len(train_labels)]\ntest = data[len(train_labels):]","53b345d8":"train.head()","0b8c070d":"# Define some metrics\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, train=train, train_labels=train_labels):\n    rmse = np.sqrt(-cross_val_score(model, train.values, train_labels.values, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","cbbf6f90":"# First Make 5-Folds for cross validation\nkf = KFold(n_splits=10, shuffle=True)","e98707fe":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","a1052808":"# List of all regressors\nregs = [(lightgbm, \"Light Gradient Boosting Regressor\"), (xgboost, \"X-Gradient Boosting Regressor\"), (ridge, \"Ridge Regressor\"), (svr, \"Support Vector Regressor\"), (gbr, \"Gradient Boosting Regressor\"), (rf, \"Random Forest Regressor\"), (stack_gen, \"All Model Stacked\")]\n\n# We will store all the scores in here\ncv_scores = {}\n\n# Calculate CV-RMSE Scores for all regressors\nfor reg, reg_name in regs:\n    sc = cv_rmse(reg)\n    cv_scores[reg_name] = (sc.mean(), sc.std())\n    print(f\"Calculating CV-RMSE for {reg_name} ==> Score Mean: {sc.mean():.2f} | Score Std: {sc.std():.2f}\")","f1afacc9":"# Now we fit all the above models and then get the final model which we will use to blend our predictions\nfor model, model_name in regs:\n    print('='*40)\n    print(f\"Fitting {model_name}...\")\n    model.fit(train.values, train_labels.values)\n    val_score_temp = model.score(train.values, train_labels.values)\n    print(f\"val_acc: {val_score_temp:.2f}\")","bf51cbe2":"def blended_predictions(X):\n    return ((0.1 * regs[2][0].predict(X)) + \\\n            (0.2 * regs[3][0].predict(X)) + \\\n            (0.1 * regs[4][0].predict(X)) + \\\n            (0.1 * regs[1][0].predict(X)) + \\\n            (0.1 * regs[0][0].predict(X)) + \\\n            (0.05 * regs[5][0].predict(X)) + \\\n            (0.35 * regs[6][0].predict(np.array(X))))","1feb2470":"blended_score = rmsle(train_labels.values, blended_predictions(train.values))\ncv_scores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","82e0e28e":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(cv_scores.keys()), y=[score for score, _ in cv_scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(cv_scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","cdf8a5d3":"# First we load the submission file\nsub = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","aa8a52e1":"# Now we make predictions on the test data\npreds = blended_predictions(test.values)","b09773b6":"sub['SalePrice'] = np.floor(np.expm1(preds))","32901120":"sub.to_csv(\"submission_fixed.csv\", index=False)","1940554b":"As we can see, the Blended Model Prediction has the lowest of all losses. So we will use it to predict.","610b7389":"## Sales Price\nLet's jump directly to sales price, since it will take a lot of time to visualize every single feature all by itself!","be6dcdc8":"Now I will join both train and test data so that we can process all the data at once.","5f5ab395":"## DABL\nLet's also look at DABL Plot","749e59be":"## MSZoning\nThis feature identifies general zoning classification of the sale.","32bee8e5":"Also remove outliers from traning data before joining","41cfc111":"## Pairplot\nLet's see the correlation pair plot","bfa404ff":"# EDA\nLet's start with EDA and keep the note of things along the way","4170eef5":"New let's do the predictions on this data","3ef31a9a":"## Encode Categorical Features\nLet's encode categorical features in our data to make them suitable for our models.","ca92a345":"## Split the Data\nLet's now finally split the data back into their respective sets","8158da3c":"# Data Preprocessing","e7b4a383":"Now let's blend the predictions of all of our models and make blended prediction.","c1981cb2":"As we can see, the data is now centered in the middle and the skewness is gone.","7ac21fd1":"As we can see, the columns: `Alley`, `PoolQC`, `Fence` and `MiscFeature` have more than 1000 NULL Values, when the data itself has 1460 total samples. For the sake of simplicity, I will just drop them and not include in the main dataset.","a98e122f":"As we can see, no other columns now have null values in them.","f481317c":"## Sales price\nTo get rid of the data skewness, we have to log shift the data.\nWe can apply `log(1+x)` to out data to shift it at center.","da3ed9a0":"## Dealing with NuLL Values\nSince the data still had a lot of Null values, we are dealing with them here.","1cdede5f":"## Correlation Heatmap\nAlso see the heatmap of correlation of different features","60ba28cc":"## MSSubClass\nThis feature Identifies the type of dwelling involved in the sale.","d5587712":"We can see one very important thing from above which is that our target column (SalesPrice) is shifted to the left. In other word, it's **skewed to the left**.","f033e798":"Let's replot Sales price, to see if the skewness is gone","d81ecd71":"## Lot Frontage\nThis feature tells about the Linear feet of Street Connected to the Property","11cb0213":"Let's start with Cross Validation Scores","d36b0aad":"# Modelling\nLet's now get to modelling our data.","6a10363e":"There are now `286` features in this dataset!"}}