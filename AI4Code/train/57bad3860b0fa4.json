{"cell_type":{"1538cba1":"code","040384e5":"code","49ddc5f6":"code","a290ed83":"code","055704a8":"code","9413e392":"code","641f4706":"code","be20d99b":"code","f4ea9025":"code","6e89adcf":"code","f4c81d10":"code","87333ad7":"code","d1e0f66e":"code","74a0f2fc":"code","27b4cab2":"markdown","bc929abf":"markdown","77ad16fc":"markdown","4d75f86b":"markdown","10b3c93b":"markdown","3ed03997":"markdown","249f447a":"markdown","0f73546a":"markdown","68d7fb45":"markdown"},"source":{"1538cba1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.optimizers import RMSprop\nfrom sklearn.model_selection import train_test_split\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","040384e5":"train_data = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-4\/train.csv')\ntrain_data.head()","49ddc5f6":"test_data = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-4\/eval.csv')\ntest_data.head()","a290ed83":"print(train_data.shape)\n\ntrain_data.describe()","055704a8":"print(test_data.shape)\n\ntest_data.describe()","9413e392":"train_data.info()","641f4706":"test_data.info()","be20d99b":"train_data.isnull().values.any()","f4ea9025":"train_data.isnull().values.any()","6e89adcf":"X = train_data.iloc[:,2:].to_numpy().reshape(len(train_data), 28, 28, 1)\/255\ny = pd.get_dummies(train_data['label']).to_numpy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 420)#blaze it, as the hip kids would say","f4c81d10":"adam = Adam(learning_rate=0.0001, decay=1e-6, amsgrad = True)\nsgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\nrms = RMSprop(learning_rate = 0.001)","87333ad7":"model = keras.models.Sequential([\nkeras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\",\n\ninput_shape=[28, 28, 1]),\n\nkeras.layers.MaxPooling2D(2),\nkeras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\nkeras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\nkeras.layers.MaxPooling2D(2),\nkeras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\nkeras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\nkeras.layers.MaxPooling2D(2),\nkeras.layers.Conv2D(512, 3, activation=\"relu\", padding=\"same\"),\nkeras.layers.Conv2D(512, 3, activation=\"relu\", padding=\"same\"),\nkeras.layers.MaxPooling2D(2),\nkeras.layers.Flatten(),\n\nkeras.layers.Dense(128, activation=\"relu\"),\nkeras.layers.Dropout(0.3),\nkeras.layers.Dense(64, activation=\"relu\"),\nkeras.layers.Dropout(0.3),\nkeras.layers.Dense(10, activation=\"softmax\")\n])\n\n\nmodel.compile(optimizer=adam,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\nmodel.fit(X_train, y_train, epochs = 50, validation_split = .2,)\nprint('Evaluating' + str(model.evaluate(X_test, y_test)))\n\nprobability_model = tf.keras.Sequential([model, \n                                         tf.keras.layers.Softmax()])","d1e0f66e":"model_2 = Sequential([\n        keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', \n               input_shape=(28, 28, 1)),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n        keras.layers.BatchNormalization(),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        keras.layers.Dropout(0.3),\n\n        keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n        keras.layers.BatchNormalization(),        \n        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        keras.layers.Dropout(0.3),\n        \n        keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu'),\n        keras.layers.BatchNormalization(),    \n        keras.layers.MaxPooling2D(pool_size=(2, 2)),  \n        keras.layers.Dropout(0.3),\n        \n        keras.layers.Flatten(),\n        \n        Dense(128, activation='relu'),\n        keras.layers.Dropout(0.3),\n        Dense(64, activation='relu'),\n        keras.layers.Dropout(0.3),\n        Dense(32, activation='relu'),\n        keras.layers.Dropout(0.3),\n        Dense(10, activation='softmax')\n    ])\n    \n\nmodel_2.compile(optimizer=adam,\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\nmodel_2.summary()\nmodel_2.fit(X_train, y_train, epochs = 100, validation_split = .2,)\nprint('Evaluating' + str(model_2.evaluate(X_test, y_test)))\n\nprobability_model_2 = tf.keras.Sequential([model_2, \n                                         tf.keras.layers.Softmax()])","74a0f2fc":"print(test_data.head())\ntest = test_data.iloc[:,1:].to_numpy().reshape(len(test_data), 28, 28, 1)\/255\n\npredictions =  probability_model_2.predict(test)\n\npredictions = np.argmax(predictions, axis = 1)\n\n\noutput = pd.DataFrame({'id':test_data['id'], 'label':predictions})\noutput.to_csv('submission.csv', index = False)\n\nprint(output.to_string())","27b4cab2":"# Batch Normalization Model","bc929abf":"# Building The Model","77ad16fc":"I found out that the optimizers have their own parameters, so I made a code block to test with them to try and get the best from them.","4d75f86b":"# Building A Submission","10b3c93b":"# Loading In The Data","3ed03997":"# Exploratory Data Analysis","249f447a":"Basically model 1, however I put in some batch normalizations and some drop outs to help improve it, usually gives an accuracy of about 93.5%.","0f73546a":"This is the model found in Chapter 14 of the textbook I recreated, usually tests around 9.15% - 92% accuracy.","68d7fb45":"# Model 1"}}