{"cell_type":{"a0f3adb8":"code","7fa7af12":"code","1a0bc4dd":"code","472c85ee":"code","258329d0":"code","87efbc19":"code","74d2e71c":"code","cfb5f1bc":"code","ad42f8b8":"code","0dd582cf":"code","fd855b7a":"code","6307d24e":"code","971c68b2":"code","961c2826":"code","0a2d210f":"code","850eaf63":"code","d543a73f":"code","c6e98916":"code","24d9ec45":"markdown","4ac94de9":"markdown","b76965e7":"markdown","2b03a104":"markdown","30c9a681":"markdown","3d1069e1":"markdown","dca48405":"markdown","02238a08":"markdown","43640a47":"markdown","432bcc80":"markdown"},"source":{"a0f3adb8":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport librosa.display\nimport os\n\nfrom sklearn.model_selection import train_test_split","7fa7af12":"path = '..\/input\/cremad\/AudioWAV\/'\naudio_path = []\naudio_emotion = []","1a0bc4dd":"# collects all the audio filename in the variable 'path'\ndirectory_path = os.listdir(path)","472c85ee":"for audio in directory_path:\n    audio_path.append(path + audio)\n    emotion = audio.split('_')\n    if emotion[2] == 'SAD':\n        audio_emotion.append(\"sad\")\n    elif emotion[2] == 'ANG':\n        audio_emotion.append(\"angry\")\n    elif emotion[2] == 'DIS':\n        audio_emotion.append(\"disgust\")\n    elif emotion[2] == 'NEU':\n        audio_emotion.append(\"neutral\")\n    elif emotion[2] == 'HAP':\n        audio_emotion.append(\"happy\")\n    elif emotion[2] == 'FEA':\n        audio_emotion.append(\"fear\")\n    else:\n        audio_emotion.append(\"unknown\")","258329d0":"emotion_dataset = pd.DataFrame(audio_emotion, columns=['Emotions'])\naudio_path_dataset = pd.DataFrame(audio_path, columns=['Path'])\ndataset = pd.concat([audio_path_dataset, emotion_dataset], axis= 1)\n#print(len(dataset))\nprint(dataset.head())\n# print(dataset['File Path'][55])","87efbc19":"# counting audio categorized by emotions\nplt.figure(figsize=(6,6), dpi=80)\nplt.title(\"Emotion Count\", size=16)\nplt.xlabel('Emotions', size = 12)\nplt.ylabel('Count', size = 12)\nsns.histplot(dataset.Emotions, color='#F19C0E')\n#plt.show()","74d2e71c":"emotion_sad = dataset[dataset['Emotions']=='sad']['Path']\nprint(type(emotion_sad))","cfb5f1bc":"#choosing a file to plot wave and spectogram\n#print(emotion_sad.values[65])\ndata_path = emotion_sad.values[542]\ndata, sampling_rate = librosa.load(data_path)","ad42f8b8":"plt.figure(figsize=(10,6))\nplt.title(\"Waveplot for a particular audio representing SAD emotion\", size=16)\nlibrosa.display.waveplot(data, sr=sampling_rate)\n#plt.show()","0dd582cf":"\nplt.figure(figsize=(10,4))\nplt.title(\"Spectogram for a particular audio representing SAD emotion\", size=16)\nD = librosa.stft(data)\nS_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\nlibrosa.display.specshow(S_db, sr = sampling_rate, x_axis='time', y_axis='hz')\n#plt.show()","fd855b7a":"# for audio processing accuracy\n# add noise to audio and check how the waveplot changes\n# also the observing the change in audio quality\n\n## Augmentation (Noise Injection)\nnoise_amp = 0.035*np.random.uniform()*np.amax(data)\naudio_injected_data = data + noise_amp*np.random.normal(size=data.shape[0])\n\n# waveplot view after noise injection:\nplt.figure(figsize=(10,6))\nplt.title(\"Waveplot for a particular audio representing SAD emotion after noise injection\", size=16)\nlibrosa.display.waveplot(audio_injected_data, sr=sampling_rate)\nplt.show()","6307d24e":"X, Y = [], []\nprint(\"Feature processing...\")\n\nfor path, emo, index in zip(dataset.Path, dataset.Emotions, range(len(dataset))):\n    value, sample = librosa.load(path)\n    # noise injection\n    noise_amp = 0.035 * np.random.uniform() * np.amax(value)\n    value = value + noise_amp * np.random.normal(size=value.shape[0])\n    # mfcc\n    mfcc = librosa.feature.mfcc(y=value, sr= sample, n_mfcc=13, n_fft=200, hop_length=512)\n    mfcc = np.ravel(mfcc.T)\n    # mel\n    mel = librosa.feature.melspectrogram(y=value, sr=sample, hop_length = 256, n_fft = 512, n_mels=64)\n    mel = librosa.power_to_db(mel ** 2)\n    mel = np.ravel(mel).T\n    result = np.array([])\n    result = np.hstack((result, mfcc, mel))\n    #print(result)\n    result = np.array(result)\n    X.append(result)\n    Y.append(emo)","971c68b2":"# print(X)\n# print(Y)\nextracted_audio_df = pd.DataFrame(X)\nextracted_audio_df[\"emotion_of_audio\"] = Y\nprint(extracted_audio_df.shape)\nprint(extracted_audio_df.tail(10))\nextracted_audio_df = extracted_audio_df.fillna(0)\n#print(extracted_audio_df.isna().any())","961c2826":"# preparing to train\nX = extracted_audio_df.drop(labels='emotion_of_audio', axis= 1)\nY = extracted_audio_df['emotion_of_audio']\n\nx_train, x_test, y_train, y_test = train_test_split(np.array(X), Y, test_size=0.2)","0a2d210f":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","850eaf63":"mlp_model = MLPClassifier(activation='relu',\n                         solver='sgd',\n                         hidden_layer_sizes=100,\n                         alpha=0.839903176695813,\n                         batch_size=150,\n                         learning_rate='adaptive',\n                         max_iter=100000)\n# Fit mlp model\nmlp_model.fit(x_train,y_train)","d543a73f":"y_pred = mlp_model.predict(x_test)\naccuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n\n# the accuracy didn't turn out to be that good :(\nprint(\"\\nModel:{}    Accuracy: {:.2f}%\".\n          format(type(mlp_model).__name__ , accuracy*100))","c6e98916":"# the prediction made by the model:\nprint(\"The Prediction Made By Model: \")\nprint(\"<<<===========================================>>>\")\ndf = pd.DataFrame({'Actual': y_test, 'Predict': y_pred})\nprint(df.head())","24d9ec45":"# Visualization","4ac94de9":"# Prediction Verification","b76965e7":"# Showing spectogram and waveplot","2b03a104":"# Augmentation (Noise Injection)","30c9a681":"# Waveplot","3d1069e1":"# Spectogram","dca48405":"# Model Creation and Fitting","02238a08":"# Accuracy Calculation","43640a47":"# Training ","432bcc80":"# Fearure Extraction\n## Creating a DF with extracted Feautures"}}