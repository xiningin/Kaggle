{"cell_type":{"d205425f":"code","e2cbf609":"code","a59b6c7e":"code","980e0da6":"code","5aa063a3":"code","b535dc99":"code","8ef513da":"code","c275c5b6":"code","ae9af6a4":"code","a821e121":"code","06234270":"code","71878b53":"code","f0bb6bc9":"code","4444c96c":"code","bd7b88be":"code","7cbc50d4":"code","89d5b767":"code","73236a4a":"code","935507d9":"code","1d8fa12e":"code","16651415":"code","afb96bbc":"code","ba3b5980":"code","24eaa627":"code","f2456e3a":"code","3a0acffc":"code","2f07dc35":"code","caacd832":"code","352a38a8":"code","c902d82b":"code","a2d66334":"code","d194f5e3":"code","fe9858be":"code","e4c79ef9":"code","7a84b108":"code","208dbff9":"code","fc298316":"code","fa92ca1e":"code","1faacf26":"code","cfd9db26":"code","21f66977":"code","0e413b5f":"code","726ba361":"code","9d00e316":"code","453a414b":"code","48ebf959":"code","f6bb8eba":"code","99197812":"code","f98a206d":"code","00557fdc":"code","6f300efa":"code","6b888136":"markdown","3e708314":"markdown","09998ae6":"markdown","de6c90c2":"markdown","8767860d":"markdown","6b622d0b":"markdown","8fc62747":"markdown","6465bbe7":"markdown","fe3fa660":"markdown","e4f986af":"markdown","c0813943":"markdown","51338337":"markdown","ce4be819":"markdown","c05189d4":"markdown","d905db87":"markdown","45562260":"markdown","af87af11":"markdown","44b50b1e":"markdown","9a1b5ab4":"markdown","8f411e8e":"markdown","871eb86d":"markdown","2b5d6c89":"markdown","59422084":"markdown","ca5944c8":"markdown","f0d19b8a":"markdown","5e5afb68":"markdown","1802c949":"markdown","503a28bf":"markdown","80bf285b":"markdown","6d692288":"markdown","b6c3dab7":"markdown","db176860":"markdown","4848fc00":"markdown","02392c96":"markdown","17c40a5d":"markdown","e5dc06ab":"markdown","40f1ad10":"markdown","6aee65bf":"markdown","a7e22180":"markdown","fe67c7a8":"markdown","d6a825bf":"markdown","e4f63af0":"markdown"},"source":{"d205425f":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport gc\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\n\npd.set_option('display.max_columns', 100)\n\n\nRFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier\n\n\n#TRAIN\/VALIDATION\/TEST SPLIT\n#VALIDATION\nVALID_SIZE = 0.20 # simple validation using train_test_split\nTEST_SIZE = 0.20 # test size using_train_test_split\n\n#CROSS-VALIDATION\nNUMBER_KFOLDS = 5 #number of KFolds for cross-validation\n\n\n\nRANDOM_STATE = 2018\n\nMAX_ROUNDS = 1000 #lgb iterations\nEARLY_STOP = 50 #lgb early stop \nOPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\nVERBOSE_EVAL = 50 #Print out metric result\n\nIS_LOCAL = False\n\nimport os\n\nPATH = \"\/kaggle\/input\/creditcardfraud\/creditcard.csv\"","e2cbf609":"data = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","a59b6c7e":"print(\"Credit Card Fraud Detection data -  rows:\",data.shape[0],\" columns:\", data.shape[1])","980e0da6":"data.head()","5aa063a3":"data.describe()","b535dc99":"total = data.isnull().sum().sort_values(ascending = False)\npercent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()","8ef513da":"temp = data[\"Class\"].value_counts()\ndf = pd.DataFrame({'Class': temp.index,'values': temp.values})\n\ntrace = go.Bar(\n    x = df['Class'],y = df['values'],\n    name=\"Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)\",\n    marker=dict(color=\"Blue\"),\n    text=df['values']\n)\ntemp_data = [trace]\nlayout = dict(title = 'Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)',\n          xaxis = dict(title = 'Class', showticklabels=True), \n          yaxis = dict(title = 'Number of transactions'),\n          hovermode = 'closest',width=600\n         )\nfig = dict(data=temp_data, layout=layout)\niplot(fig, filename='class')","c275c5b6":"class_0 = data.loc[data['Class'] == 0][\"Time\"]\nclass_1 = data.loc[data['Class'] == 1][\"Time\"]\n\nhist_data = [class_0, class_1]\ngroup_labels = ['Not Fraud', 'Fraud']\n\nfig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\nfig['layout'].update(title='Credit Card Transactions Time Density Plot', xaxis=dict(title='Time [s]'))\niplot(fig, filename='dist_only')","ae9af6a4":"fig, ax1 = plt.subplots(ncols=1, figsize=(6,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data, palette=\"PRGn\",showfliers=False)\nplt.show();","a821e121":"plt.hist(data[\"Amount\"], bins=20)\nplt.gca().set(title='Frequency Histogram', ylabel='Frequency');","06234270":"plt.hist(np.log(data[\"Amount\"] +1), bins=50)\nplt.gca().set(title='Frequency Histogram', ylabel='Frequency');","71878b53":"plt.figure(figsize = (14,14))\nplt.title('Credit Card Transactions features correlation plot (Pearson)')\ncorr = data.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","f0bb6bc9":"orig_data = data.copy()\n\ndata[\"Amount\"] = np.log(data[\"Amount\"] + 1)","4444c96c":"train_df, test_df = train_test_split(data, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True )\ntrain_df, valid_df = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )","bd7b88be":"print(train_df.shape)\nprint(test_df.shape)\nprint(valid_df.shape)","7cbc50d4":"do_balancing = False\n\nif do_balancing :\n\n    # Lets make the event rate as 1% \n\n    train_fraud_df  = train_df[train_df['Class'] ==1]\n    no_of_fraud = train_fraud_df.shape[0]\n    print(\"Total Fraud in Train Data :\" ,no_of_fraud)\n\n    no_of_non_fraud = no_of_fraud * 99\n    train_non_fraud_df = train_df[train_df['Class'] ==0].sample( no_of_non_fraud , random_state =2021)\n    no_of_non_fraud = train_non_fraud_df.shape[0]\n    print(\"Total non Fraud in Train Data :\" ,no_of_non_fraud)\n\n    # join the data \n\n    train_df = pd.concat([train_fraud_df, train_non_fraud_df] , axis =0 ) # concat  row wise\n    train_df = train_df.sample(frac = 1)\n","89d5b767":"target = 'Class'\npredictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']\n","73236a4a":"clf = DecisionTreeClassifier(random_state=RANDOM_STATE,\n                             max_depth= 2)","935507d9":"%%time\nclf.fit(train_df[predictors], train_df[target].values)","1d8fa12e":"\nfrom sklearn import tree\nfrom sklearn.tree import export_graphviz\n\ntree.export_graphviz(clf,out_file='tree.dot',feature_names = predictors,\nclass_names = ['Non-Fraud' ,'Fraud'],rounded = True, proportion = False, precision = 2, filled = True)  \n\n!dot -Tpng tree.dot -o tree.png\nfrom IPython.display import Image\nImage(filename = 'tree.png')","16651415":"def model_function(row):\n    \n    if row['V17'] <= -2.94:\n        return 1\n    elif row['V14'] <= -7.75:\n        return 1\n    else :\n        return 0\n    \n","afb96bbc":"valid_df['prediction'] = valid_df.apply(model_function , axis = 1)","ba3b5980":"valid_df['prediction'].value_counts()","24eaa627":"cm = pd.crosstab(valid_df[target].values, valid_df['prediction'], rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(7,7))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\" , fmt='d')\nplt.title('Confusion Matrix', fontsize=16)\nplt.show()","f2456e3a":"metric_data = pd.DataFrame(columns =['Model Name','Detection Rate' ,'AUROC','F1 Score','Accuracy','Fraud Loss Saved'])\nmetric_data.shape","3a0acffc":"# we will use original data as Amount is transformed for modelling \nfrom sklearn.metrics import accuracy_score , f1_score ,roc_auc_score\ndef fraud_loss_saved ( dataset , key) :\n\n    df = dataset.copy()\n    df['Amount']  = np.exp(df['Amount'])\n    total_fraud_amt = df[df['Class'] ==1]['Amount'].sum()\n    print(\"Total Fraud Amount in Validation Data : \" +  str(round(total_fraud_amt,2)))\n    total_fraud_amt_detected = df.loc[(df['prediction'] ==1) & (df['Class']==1) ]['Amount'].sum()\n    print(\"Total Fraud Amount Detected in Validation Data : \" +  str(round(total_fraud_amt_detected,2)))\n    print(\"Fraud Loss Saved (%): \" + str(round(100*total_fraud_amt_detected\/total_fraud_amt ,2)))\n    detection_rate  = 100 * (df[df['prediction']==1]['Class'].sum())\/df['Class'].sum()\n    print(\"Detection Rate (%) : \" + str(round(detection_rate , 2)))\n    accuracy = 100*accuracy_score(df['Class'] ,df['prediction'])\n    print(\"Accuracy : \" + str(round(accuracy ,2)))\n    f1 = f1_score(df['Class'] ,df['prediction'])\n    print(\"F1 Score : \" + str(round(f1 ,4)))   \n    auc_score = roc_auc_score(df['Class'],df['prediction'])\n    print(\"AUROC Score : \" + str(round(auc_score,4)))\n    values = []\n    values.append(key)\n    values.append(detection_rate)\n    values.append(auc_score)\n    values.append(f1)\n    values.append(accuracy)\n    values.append(round(100*total_fraud_amt_detected\/total_fraud_amt ,2))\n    \n    final_values =[]\n    final_values.append(values)\n    temp_df = pd.DataFrame(final_values ,columns =['Model Name','Detection Rate' ,'AUROC','F1 Score','Accuracy','Fraud Loss Saved'])\n    \n    global metric_data\n    \n    metric_data = pd.concat([metric_data,temp_df ] , axis = 0 )\n    \n    \n    ","2f07dc35":"fraud_loss_saved(valid_df ,'Decision Tree - Valid Data')","caacd832":"%%time \n\nfrom xgboost import XGBClassifier\n\nxgb_clf = XGBClassifier()\n\nxgb_clf.fit(train_df[predictors], train_df[target].values)\n\nvalid_df['prediction'] = xgb_clf.predict(valid_df[predictors])\n\ncm = pd.crosstab(valid_df[target].values, valid_df['prediction'], rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(7,7))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\" , fmt='d')\nplt.title('Confusion Matrix', fontsize=16)\nplt.show()","352a38a8":"fraud_loss_saved(valid_df ,'XGBOOST - Valid Data')","c902d82b":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression()\n\nlr_clf.fit(train_df[predictors], train_df[target].values)\n\nvalid_df['prediction'] = lr_clf.predict(valid_df[predictors])\n\ncm = pd.crosstab(valid_df[target].values, valid_df['prediction'], rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(7,7))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\" , fmt='d')\nplt.title('Confusion Matrix', fontsize=16)\nplt.show()","a2d66334":"fraud_loss_saved(valid_df ,'Logistic Regression - Valid Data')","d194f5e3":"%%time \nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators = 20)\n\nrf_clf.fit(train_df[predictors], train_df[target].values)\n\nvalid_df['prediction'] = rf_clf.predict(valid_df[predictors])\n\ncm = pd.crosstab(valid_df[target].values, valid_df['prediction'], rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(7,7))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\" , fmt='d')\nplt.title('Confusion Matrix', fontsize=16)\nplt.show()","fe9858be":"fraud_loss_saved(valid_df ,'Random Forest - Valid Data')","e4c79ef9":"metric_data","7a84b108":"\n\ntest_df['prediction'] = test_df[predictors].apply(model_function, axis = 1)\n\ncm = pd.crosstab(test_df[target].values, test_df['prediction'], rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(7,7))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\" , fmt='d')\nplt.title('Confusion Matrix', fontsize=16)\nplt.show()\n\nfraud_loss_saved(test_df, 'Decision Tree - Test Data')","208dbff9":"test_df['prediction'] = xgb_clf.predict(test_df[predictors])\n\n\ncm = pd.crosstab(test_df[target].values, test_df['prediction'], rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(7,7))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\" , fmt='d')\nplt.title('Confusion Matrix', fontsize=16)\nplt.show()\n\nfraud_loss_saved(test_df ,'XGBOOST - Test Data')","fc298316":"metric_data","fa92ca1e":"# Lets Hyper Tune XGBOOST \n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nmodel = xgb.XGBClassifier()\nn_estimators = range(50, 100, 50)\nparam_grid = dict(n_estimators=n_estimators)\n\nmax_depth = range(5, 8, 2)\nparam_grid['max_depth'] = max_depth\nprint(param_grid)","1faacf26":"%%time\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold,verbose = 3)\ngrid_result = grid_search.fit(train_df[predictors], train_df[target])\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","cfd9db26":"means = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n# plot\n","21f66977":"grid_result.best_params_","0e413b5f":"valid_df['prediction'] = grid_result.best_estimator_.predict(valid_df[predictors])\n\n\ncm = pd.crosstab(valid_df[target].values, valid_df['prediction'], rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(7,7))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\" , fmt='d')\nplt.title('Confusion Matrix', fontsize=16)\nplt.show()\n\nfraud_loss_saved(valid_df ,'XGBOOST - Hypertune - Valid Data')","726ba361":"metric_data","9d00e316":"test_df['prediction'] = grid_result.best_estimator_.predict(test_df[predictors])\n\n\ncm = pd.crosstab(test_df[target].values, test_df['prediction'], rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(7,7))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\" , fmt='d')\nplt.title('Confusion Matrix', fontsize=16)\nplt.show()\n\nfraud_loss_saved(test_df , 'XGBOOST - Hypertune - Test Data')","453a414b":"metric_data","48ebf959":"total_fraud_in_data = orig_data.loc[orig_data['Class'] ==1]['Amount'].sum()\nprint(\"Total Fraud Amount in the Dataset :\" + str(round(total_fraud_in_data , 2)))\nprint(\"Fraud Loss per day : \" + str(round(total_fraud_in_data\/2,2)))\nprint(\"Fraud Loss per year : \" + str(round(365*total_fraud_in_data\/2,2)))\n","f6bb8eba":"%%time\nmetric_list  =[]\nfor threshold in np.linspace(0.0001, 0.1,200 ) :\n    \n    threshold = round(threshold,5)\n    df = valid_df.copy()\n    probs = grid_result.best_estimator_.predict_proba(df[predictors])\n    prob_of_fraud = probs[:,1]\n    preds = prob_of_fraud >= threshold\n    df['prediction'] = preds\n    df['prediction'] = df['prediction'].astype(int)\n    df['Amount']  = np.exp(df['Amount'])\n    total_fraud_amt = df[df['Class'] ==1]['Amount'].sum()\n    total_fraud_amt_detected = df.loc[(df['prediction'] ==1) & (df['Class']==1) ]['Amount'].sum()\n    fraud_saving = round(100*total_fraud_amt_detected\/total_fraud_amt ,2)\n    detection_rate  = 100 * (df[df['prediction']==1]['Class'].sum())\/df['Class'].sum()\n    detection_rate = round(detection_rate , 2)\n    metric =[]\n    metric.append(threshold)\n    metric.append(round(f1_score(df[target] ,df['prediction']),4))\n    metric.append(detection_rate)\n    metric.append(fraud_saving)\n    \n    metric_list.append(metric)\n\nbrute_force_df =pd.DataFrame(metric_list , columns = ['Threshold', 'F1 Score' , 'Detection Rate' ,'Fraud Loss Saved'])\nbrute_force_df = brute_force_df.sort_values('Fraud Loss Saved' , ascending = False)                  \nbrute_force_df.head(20)","99197812":"cut_off_selected = 0.00161\nprobs = grid_result.best_estimator_.predict_proba(valid_df[predictors])\nprob_of_fraud = probs[:,1]\npreds = prob_of_fraud >= cut_off_selected\nvalid_df['prediction'] = preds\n\nvalid_df['prediction'] = valid_df['prediction'].astype(int)\n\ncm = pd.crosstab(valid_df[target].values, valid_df['prediction'], rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(7,7))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\" , fmt='d')\nplt.title('Confusion Matrix', fontsize=16)\nplt.show()\n\n\nfraud_loss_saved(valid_df ,'XGBOOST Optimized Cut off - Valid Data')\n","f98a206d":"cut_off_selected = 0.00161\nprobs = grid_result.best_estimator_.predict_proba(test_df[predictors])\nprob_of_fraud = probs[:,1]\npreds = prob_of_fraud >= cut_off_selected\ntest_df['prediction'] = preds\n\ntest_df['prediction'] = test_df['prediction'].astype(int)\n\ncm = pd.crosstab(test_df[target].values, test_df['prediction'], rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(7,7))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\" , fmt='d')\nplt.title('Confusion Matrix', fontsize=16)\nplt.show()\n\n\nfraud_loss_saved(test_df ,'XGBOOST Optimized Cut off - Test Data')\n","00557fdc":"metric_data","6f300efa":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': grid_result.best_estimator_.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (10,8))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","6b888136":"# Using Decision Tree : ###\n\n### Detect 70% of Fraudulent Transaction ###\n\n### Save Approx 5.9 million dollars on Fraud Losses ###","3e708314":"## Features correlation ##","09998ae6":"### Feature Importance ###","de6c90c2":"### Data is highly skewed and has a long tail towards the right side ","8767860d":"## Introduction ##\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have **492 frauds** out of **284,807** transactions. The dataset is highly unbalanced, the positive class (frauds) account for **0.172% of all transactions**.\n\nIt contains only numerical input variables which are the result of a PCA transformation.\n\nDue to **confidentiality** issues, there are not provided the original features and more background information about the data.\n\n**Features V1, V2, ... V28 are the principal components obtained with PCA;**\nThe only features which have not been transformed with PCA are Time and Amount. Feature Time contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature Amount is the transaction Amount, this feature can be used for example-dependant **cost-senstive learning**.\n\nFeature Class is the response variable and it takes value 1 in case of fraud and 0 otherwise.","6b622d0b":"# Using XGBOOST : ###\n\n### Detect 80% of Fraudulent Transaction ###\n\n### Save Approx 6.4 million dollars on Fraud Losses ###\n\n### Incremental benefit of 500k dollars ###","8fc62747":"### Log transformation - reduces the skewness ","6465bbe7":"### Plot the tree ###","fe3fa660":"<img src= \"https:\/\/ai-journey.com\/wp-content\/uploads\/2019\/06\/fraud-EMV-chip-credit-card.jpg\" alt =\"Credit Card Fraud Detection\" style='width: 600px;'>","e4f986af":"### Simple Model  - Decision Tree ###","c0813943":"<img src= \"http:\/\/static.financialexpress.com\/m-images\/M_Id_459416_Savings.jpg\" alt =\"Fraud Loss Saved\" style='width: 600px;'>","51338337":"Looking to the Time feature, we can confirm that the data contains **284,807** transactions, **during 2 consecutive days (or 172792 seconds).**","ce4be819":"### Wait  !!!!!!! ###\n\n### Business wants to how much money is saved ###","c05189d4":"## Transactions amount ##","d905db87":"**Only 492 (or 0.172%) of transaction are fraudulent**. That means the data is highly unbalanced with respect with target variable Class. ","45562260":"### Caution !!!!! - Check Accuracy everyone doing 99% , we cant figure good vs bad model ###\n\n### Perhaps  AUROC and F1 Score is a better model performance metric for final model selection ###","af87af11":"# Read the Data #","44b50b1e":"# Using Performance Tuning we have saved 25K dollars additionally !!!!!\n\n# As compares to Decision Trees 5.88 million  , we have saved 6.42 million dollars for the bank using Machine Learning model ( XGBOOST)\n\n# Incremental benefit of 520K Dollars","9a1b5ab4":"## Let us check Decision Tree & Machine Learning Algorithm XGBOOST performance on test data set ","8f411e8e":"### Since the data points are PCA i.e. Principal Component Analysis - they are **uncorrelated** ###\n\n### Note : if we have 2 feature with high correlation then we need to select 1 feature and drop another feature based on statistics significance.\n\n### You can find Information Value of both the features and select the feature which has high IV","871eb86d":"### Lets Understand the Business Impact of implementing machine learning model for Fraud Detection ###","2b5d6c89":"### 1) 45447 ---- True Negative --> These were non fraud and model predicted them as non fraud \n\n### 2) 71 ---- True Positive --> These were fraud txn and model predicted them as fraud txn\n\n### 3) 31 ----  False Negative --> These were fraud txn but our model failed to predict them as fraud \n\n### 4) 20 ------ False Positive ---> These were non fraud txn but our model predicted them as fraud \n","59422084":"### Lets try Machine Learning and see if it can help ###","ca5944c8":"## Credit Card Fraud Detection Using Predictive Machine Learning Models ##\n","f0d19b8a":"# Glimpse of the data #","5e5afb68":"## Amazing ... isn't it!!!! ##\n\n### With a Simple Decision Tree - we were able to detect 70% of Fraud ###\n\n### We Know what the model is doing internally ###\n\n### implementing this model is very cost effective as it can be coded in any of the existing language like Java , C# or even Mainframe ###","1802c949":"### Balancing the train data","503a28bf":"### Note : Importance of Validation Data set \n\n### It is used to fine tune the model , here we are training 4 models and evaluating their performance on the validation data set \n\n### We can use the result on this data set to select the model ","80bf285b":"### Wow - No Missing Data !!!!! ###","6d692288":"# Check the Data #","b6c3dab7":"### Note V17 is the most useful feature for XGBOOST \n\n### Even Decision Tree algorithm started with V17 as the root node - indicating it is the most important feature","db176860":"## Insights ##\n\n1) Low Real i.e Non Fraud Transactions during night times \n\n2) Fraudulent transactions have more even distributions\n\n3) Fraudulent transactions happen consistently over night time","4848fc00":"# Check missing data #","02392c96":"## Advance Technique ##\n\n### Lets us try to use the probabilities which are given by model predict_proba method ####\n\n### For every instance model with 2 probabilities i.e [ 0.3, 0.7] \n\n### where 0.3 means probability of it being Class = 0 i.e. Non Fraud \n\n### where 0.7 means probability of it being Class = 1 i.e. Fraud ","17c40a5d":"# Split data in train, test and validation set #","e5dc06ab":"### For Simplicity - we are taking Dollar Saving as the criteria to choose the model in Practise many parameters are there like AUROC , F1 Score , False Positive Ratio , PSI , CSI , Customer Impact , Concordance - Discordance , Gini ###","40f1ad10":"## Future Scope ##\n\n### 1. Try Deep Learning \n### 2. Try Unsupervised Learning\n### 3 . Create and Ensemble of Supervised & Unsupervised Learning ","6aee65bf":"# Load packages #","a7e22180":"# Check for Data Imbalance #","fe67c7a8":"<img src= \"https:\/\/thumbs.dreamstime.com\/b\/machine-learning-technology-artificial-intelligence-modern-manufacturing-144923304.jpg\" alt =\"Machine Learning\" style='width: 600px;'>","d6a825bf":"## Check the performance ##","e4f63af0":"### Data Exploration ###"}}