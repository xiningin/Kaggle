{"cell_type":{"119128b4":"code","635b7788":"code","b1ae849c":"code","75aff2f9":"code","a7201a85":"code","5ac361fc":"code","e0444483":"code","2fad702d":"code","f23365ad":"code","1339757d":"code","74b90e35":"code","0bfbab55":"code","9427f8ca":"code","4c6b5d01":"code","da80f227":"code","d6d8c771":"code","6719848a":"code","63a95309":"code","b6d8f6d7":"code","5087873a":"code","a93e5c58":"code","c718b2c7":"code","56d54ac1":"code","764633fc":"code","37c28843":"code","50416d95":"code","e30f7110":"code","e0013440":"code","98b275dd":"code","e9e29b6c":"code","740547a9":"code","99188bf3":"code","b8dd7d3a":"code","fb75bf7b":"code","c2aefaab":"code","35b8d0d0":"code","a6a72017":"code","5598c69f":"code","bbbe6992":"code","db38d58f":"markdown","42d6421d":"markdown","0596a3a9":"markdown","a646c119":"markdown","b22db519":"markdown","e15f60f7":"markdown","66cf7e5a":"markdown","ec68c634":"markdown","0cb6d9c3":"markdown","90d75cf9":"markdown","4d674c27":"markdown","47c3f651":"markdown","493b7480":"markdown","375cca22":"markdown","2a66f697":"markdown","e1d84ce4":"markdown","76eb483e":"markdown","333c3fe1":"markdown","8f3f4744":"markdown","67d6a70e":"markdown","b3d9604a":"markdown","6d92a2f2":"markdown","240b7dd2":"markdown","0bfb8bc5":"markdown","b36f332d":"markdown","59f77206":"markdown","2559b3e0":"markdown","9ed9ad8c":"markdown","f259ae6c":"markdown","f24e8437":"markdown","c7560422":"markdown"},"source":{"119128b4":"%config IPCompleter.greedy=True","635b7788":"!pip uninstall statsmodels --yes\n!pip install statsmodels==0.10.0rc2 --pre","b1ae849c":"!pip install --upgrade scipy==1.1.0\n!pip install statsmodels==0.10.0rc2 --pre","75aff2f9":"import pandas as pd\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","a7201a85":"df=pd.read_excel('..\/input\/Churn_data.xlsx')\ndf.head()","5ac361fc":"#checking columns datatypes\ndf.dtypes","e0444483":"#data summary, checking for outliers \ndf.describe().transpose()","2fad702d":"#checking for any null values\ndf.isnull().any().any()","f23365ad":"#checking dependent variable classes\nimport numpy as np\nclass_freq=np.bincount(df.Churn)\npChurn=class_freq[1]\/sum(class_freq) #will use it later\nprint(\"probabilities:\")\nprint(\"No Churn: \"+str(class_freq[0]\/sum(class_freq)))\nprint(\"Churn: \"+str(class_freq[1]\/sum(class_freq)))","1339757d":"#Writing a function to calculate the VIF values\nimport statsmodels.formula.api as sm\ndef vif_cal(input_data, dependent_col):\n    x_vars=input_data.drop([dependent_col], axis=1)\n    xvar_names=x_vars.columns\n    for i in range(0,xvar_names.shape[0]):\n        y=x_vars[xvar_names[i]] \n        x=x_vars[xvar_names.drop(xvar_names[i])]\n        rsq=sm.ols(formula=\"y~x\", data=x_vars).fit().rsquared  \n        vif=round(1\/(1-rsq),2)\n        print (xvar_names[i], \" VIF = \" , vif)","74b90e35":"#correlation graph\nsns.set(style='darkgrid',palette=\"muted\")\n#fig, ax=plt.subplots(figsize=dims)\ndf.corr()['Churn'][1:].sort_values(ascending = False).plot(kind='bar')\nplt.xlabel(\"Dependent Variables\")\nplt.ylabel(\"Correlation to Churn\")\nplt.title(\"Correlation to Churn\")","0bfbab55":"#Calculating VIF values using that function\nvif_cal(input_data=df, dependent_col=\"Churn\")","9427f8ca":"# Acceptable vif columns: AccountWeeks, Contract Renewal, CustServCalls, DayCalls, RoamMins\nvif_cal(df.drop(columns=['MonthlyCharge','DataUsage']),dependent_col=\"Churn\")\n#Removing Monthly Charge and DataUsage leads to very good improvement in vif.","4c6b5d01":"ax=sns.kdeplot(df.CustServCalls[(df['Churn']==1)],color=\"blue\",shade=True)\nax=sns.kdeplot(df.CustServCalls[(df['Churn']==0)],color=\"red\",shade=True)\nax.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('Customer case calls')\nax.set_title('Distribution of customer service calls by churn')","da80f227":"ax=sns.kdeplot(df.AccountWeeks[(df['Churn']==1)],color=\"blue\",shade=True)\nax=sns.kdeplot(df.AccountWeeks[(df['Churn']==0)],color=\"red\",shade=True)\nax.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('AccountWeeks')\nax.set_title('Distribution of AccountWeeks by churn')","d6d8c771":"ax=sns.countplot(\"ContractRenewal\",data=df,palette=\"rainbow\",hue=\"Churn\")\nax.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('ContractRenewal')\nax.set_title('Distribution of ContractRenewal by churn')","6719848a":"ax=sns.countplot(\"DataPlan\",data=df,palette=\"rainbow\",hue=\"Churn\")\nax.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('DataPlan')\nax.set_title('Distribution of DataPlan by churn')","63a95309":"ax=sns.kdeplot(df.DayMins[(df['Churn']==1)],color=\"blue\",shade=True)\nax=sns.kdeplot(df.DayMins[(df['Churn']==0)],color=\"red\",shade=True)\nax.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('DayMins')\nax.set_title('Distribution of DayMins by churn')","b6d8f6d7":"ax=sns.kdeplot(df.DayCalls[(df['Churn']==1)],color=\"blue\",shade=True)\nax=sns.kdeplot(df.DayCalls[(df['Churn']==0)],color=\"red\",shade=True)\nax.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('DayCalls')\nax.set_title('Distribution of DayCalls by churn')","5087873a":"ax=sns.kdeplot(df.OverageFee[(df['Churn']==1)],color=\"blue\",shade=True)\nax=sns.kdeplot(df.OverageFee[(df['Churn']==0)],color=\"red\",shade=True)\nax.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('OverageFee')\nax.set_title('Distribution of OverageFee by churn')","a93e5c58":"ax=sns.kdeplot(df.RoamMins[(df['Churn']==1)],color=\"blue\",shade=True)\nax=sns.kdeplot(df.RoamMins[(df['Churn']==0)],color=\"red\",shade=True)\nax.legend([\"Not Churn\",\"Churn\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('RoamMins')\nax.set_title('Distribution of RoamMins by churn')","c718b2c7":"#logistic regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","56d54ac1":"X = df.drop(columns = ['Churn','MonthlyCharge','DataUsage'])\ny = df['Churn'].values","764633fc":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","37c28843":"model = LogisticRegression()\nresult = model.fit(X_train, y_train)","50416d95":"prediction_test = model.predict(X_test)\n# Print the prediction accuracy\nmetrics.accuracy_score(y_test, prediction_test)","e30f7110":"model.coef_[0]","e0013440":"weights = pd.Series(model.coef_[0],\n                 index=X.columns.values)\nprint (weights.sort_values(ascending = False)[:10].plot(kind='bar'))","98b275dd":"arr=metrics.confusion_matrix(y_test,prediction_test)\ndf_cm = pd.DataFrame(arr, range(2),range(2))\n#plt.figure(figsize = (10,7))\nsns.set(font_scale=1)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 10},fmt=\"d\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix for threshold: .5\")","e9e29b6c":"metrics.precision_recall_fscore_support(y_test,prediction_test)","740547a9":"predicted_proba=model.predict_proba(X_test)\npredicted_proba","99188bf3":"#plot precicion, recall and thresholds\n#predicted_proba[:,1]\ndef plotPrecisionRecallThreshold(y_test, pred_prob):\n    precision, recall, thresholds = metrics.precision_recall_curve(y_test, pred_prob) \n   #retrieve probability of being 1(in second column of probs_y)\n    pr_auc = metrics.auc(recall, precision)\n    plt.title(\"Precision-Recall vs Threshold Chart\")\n    plt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\n    plt.ylabel(\"Precision, Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"lower left\")\n    plt.ylim([0,1])\n    \ndef plotROC(y_test,pred_prob):\n    fpr, tpr, threshold=metrics.roc_curve(y_test,pred_prob)\n    plt.title(\"ROC Curve\")\n    sns.lineplot(x=fpr,y=tpr,palette=\"muted\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.xlabel(\"False Positive Rate\")\n    \ndef areaUnderROC(y_test, pred_prob):\n    precision, recall, thresholds = metrics.precision_recall_curve(y_test, pred_prob) \n    return metrics.auc(recall, precision)","b8dd7d3a":"plotPrecisionRecallThreshold(y_test, predicted_proba[:,1])","fb75bf7b":"plotROC(y_test, predicted_proba[:,1])","c2aefaab":"areaUnderROC(y_test, predicted_proba[:,1])","35b8d0d0":"import numpy as np\nimport math\npred=np.empty(1000)\nprobsChurn= predicted_proba[:,1]\npred=np.empty(1000)\nthresh=pChurn\nfor i in range(0, probsChurn.size):\n    if probsChurn[i]>thresh:\n        pred[i]=1\n    else:\n        pred[i]=0\n        ","a6a72017":"metrics.precision_recall_fscore_support(y_test,pred)","5598c69f":"arr=metrics.confusion_matrix(y_test,pred)\ndf_cm = pd.DataFrame(arr, range(2),\n                  range(2))\n#plt.figure(figsize = (10,7))\nsns.set(font_scale=1.2)#for label size\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 15},fmt=\"d\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix for threshold: .145\")","bbbe6992":"metrics.accuracy_score(y_test, pred)","db38d58f":"Now time to tweek the Model.\nRemember the data that we have, only 14.5% of the records are Churn. Its unfair to have prediction threshold of .5\nUpdating threshold to pChurn\n","42d6421d":"Lets look at Precision and Recall","0596a3a9":"For import error with scipy 1.3. downgraging to scipy 1.1","a646c119":"Prediction with New threshold","b22db519":"Lets check Interaction of Churn with these variables","e15f60f7":"The accuracy of our model has gone down to 76%, but that should not be a matter of much concern because predicting No Churns doesnot help. What we are interested in is predicting Churns.","66cf7e5a":"Predicting on the Test Set","ec68c634":"Area Under ROC Curve: .37, Not impressive but again given unbalanced dataset, thats what we got.","0cb6d9c3":"VIF(Multicollinearity) is high for a lot of variables.\nVIF should ideally be 1, but anythin under 5 is good and under 10 can be considered okay.\nBut here they are quite high.\nWe will try and lower it down","90d75cf9":"All datatypes are floor and int","4d674c27":"CustServCalls, DayMins have high positive correlation with Churn where as Contract renewal has high negative correlation.\nWe will investigate further.","47c3f651":"The results are similar to our Exploratory Data Analysis. DayMins, DayCalls and AccountWeeks have less weightage.","493b7480":"Model have an accuracy of 86.6% that seems good, But as the set is imbalanced and we are interested in predicting Churns, this may be a bad way of accessing the model.","375cca22":"Hope, you find this helpful.\nThis is my first Kernel !!\nAny feedback appreciated.","2a66f697":"Yoohoo!\nRecall: 77% (Model is able to predict 77% of the actual Churns correctly)\nPrecision: 33% (Out of all Churns we predict, 33% are actually Churns)\nHowever the Precision is not quite upto the mark, but we do manage to Predict 77% of Churns, that is pretty good number as compared to the default rate of 14% in the given dataset.","e1d84ce4":"Plotting the coefficients","76eb483e":"Precision : .468\nRecall: .16\nWe are more interested in imporving Recall(% of Churns predicted out of actual Churns)","333c3fe1":"Lets draw some graphs","8f3f4744":"there is less interaction with AccountWeeks, as seen from correlation plot","67d6a70e":"Out of the given Customer Churn dataset, Build a Logistic Regression model to predict Customer Churns.","b3d9604a":"Checking for Correlation of independent variables with Churn","6d92a2f2":"   Running the model on Training data","240b7dd2":"And The Confusion Matrix","0bfb8bc5":"**Moving to Logistic regression**","b36f332d":"Creating Test\/Train sets in 70-30","59f77206":"Cheking out predicted probabilities","2559b3e0":"All VIFs are 1 now. Thats ideal, we can proceed with these variables","9ed9ad8c":"Removing Columns with high VIF from the model","f259ae6c":"Dataset is very unbalanced.\n14.45% of the total customers churn.","f24e8437":"Lets look at the confusion matrix","c7560422":"Looking for Model Coefficients(Betas)"}}