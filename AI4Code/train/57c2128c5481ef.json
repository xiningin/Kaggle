{"cell_type":{"2b550745":"code","0dfa7fcd":"code","79e807a9":"code","830282a4":"code","0726fef9":"code","6096f871":"code","c9d7de00":"code","a763752f":"code","2d6ea6be":"code","74f84578":"code","8fc2d188":"code","f5e22218":"code","99403a63":"code","4babfe98":"code","3db2b56b":"code","c7d3bf53":"code","42bd8a99":"code","7df80660":"code","db5da096":"code","1d509663":"code","77350256":"code","e630de92":"code","cc3d106b":"code","2a766ae1":"code","977e6ebc":"code","a682313d":"code","02857987":"code","7663e569":"code","8e9c939d":"code","6ba88647":"code","4636ab8f":"code","6320a10d":"markdown","c14a7456":"markdown","9fcae9fa":"markdown","0ddff81d":"markdown","d675174b":"markdown","3139f45d":"markdown","d18bed99":"markdown","6a0716a4":"markdown","592741c8":"markdown","d4cb259a":"markdown","d728df88":"markdown"},"source":{"2b550745":"#Directories present\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n        print(dirname)","0dfa7fcd":"import os \nimport sys\nimport random\nimport math\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nimport pydicom\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\nimport pandas as pd \nimport glob ","79e807a9":"import tensorflow as tf\ntf.__version__","830282a4":"DATA_DIR = '\/kaggle\/input'\n\n# Directory to save logs and trained model\nROOT_DIR = '\/kaggle\/working'","0726fef9":"mkdir data","6096f871":"!git clone https:\/\/www.github.com\/matterport\/Mask_RCNN.git\nos.chdir('Mask_RCNN')\n!pip install -r requirements.txt\n!python setup.py -q install","c9d7de00":"# Import Mask RCNN\nsys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","a763752f":"!pip install git+https:\/\/github.com\/waleedka\/coco.git#subdirectory=PythonAPI","2d6ea6be":"from mrcnn import utils\nimport numpy as np\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom pycocotools import mask as maskUtils\n","74f84578":"class FoodChallengeDataset(utils.Dataset):\n    def load_dataset(self, dataset_dir, load_small=False, return_coco=True):\n        \"\"\" Loads dataset released for the AICrowd Food Challenge\n            Params:\n                - dataset_dir : root directory of the dataset (can point to the train\/val folder)\n                - load_small : Boolean value which signals if the annotations for all the images need to be loaded into the memory,\n                               or if only a small subset of the same should be loaded into memory\n        \"\"\"\n        self.load_small = load_small\n        if self.load_small:\n            annotation_path = os.path.join(dataset_dir, \"annotation-small.json\")\n        else:\n            annotation_path = os.path.join(dataset_dir, \"annotations.json\")\n\n        image_dir = os.path.join(dataset_dir, \"images\")\n        print(\"Annotation Path \", annotation_path)\n        print(\"Image Dir \", image_dir)\n        assert os.path.exists(annotation_path) and os.path.exists(image_dir)\n\n        self.coco = COCO(annotation_path)\n        self.image_dir = image_dir\n\n        # Load all classes (Only Building in this version)\n        classIds = self.coco.getCatIds()\n\n        # Load all images\n        image_ids = list(self.coco.imgs.keys())\n\n        # register classes\n        for _class_id in classIds:\n            self.add_class(\"crowdai-food-challenge\", _class_id, self.coco.loadCats(_class_id)[0][\"name\"])\n\n        # Register Images\n        for _img_id in image_ids:\n            assert(os.path.exists(os.path.join(image_dir, self.coco.imgs[_img_id]['file_name'])))\n            self.add_image(\n                \"crowdai-food-challenge\", image_id=_img_id,\n                path=os.path.join(image_dir, self.coco.imgs[_img_id]['file_name']),\n                width=self.coco.imgs[_img_id][\"width\"],\n                height=self.coco.imgs[_img_id][\"height\"],\n                annotations=self.coco.loadAnns(self.coco.getAnnIds(\n                                            imgIds=[_img_id],\n                                            catIds=classIds,\n                                            iscrowd=None)))\n\n        if return_coco:\n            return self.coco\n\n    def load_mask(self, image_id):\n        \"\"\" Loads instance mask for a given image\n              This function converts mask from the coco format to a\n              a bitmap [height, width, instance]\n            Params:\n                - image_id : reference id for a given image\n\n            Returns:\n                masks : A bool array of shape [height, width, instances] with\n                    one mask per instance\n                class_ids : a 1D array of classIds of the corresponding instance masks\n                    (In this version of the challenge it will be of shape [instances] and always be filled with the class-id of the \"Building\" class.)\n        \"\"\"\n\n        image_info = self.image_info[image_id]\n        assert image_info[\"source\"] == \"crowdai-food-challenge\"\n\n        instance_masks = []\n        class_ids = []\n        annotations = self.image_info[image_id][\"annotations\"]\n        # Build mask of shape [height, width, instance_count] and list\n        # of class IDs that correspond to each channel of the mask.\n        for annotation in annotations:\n            class_id = self.map_source_class_id(\n                \"crowdai-food-challenge.{}\".format(annotation['category_id']))\n            if class_id:\n                m = self.annToMask(annotation,  image_info[\"height\"],\n                                                image_info[\"width\"])\n                # Some objects are so small that they're less than 1 pixel area\n                # and end up rounded out. Skip those objects.\n                if m.max() < 1:\n                    continue\n\n                # Ignore the notion of \"is_crowd\" as specified in the coco format\n                # as we donot have the said annotation in the current version of the dataset\n\n                instance_masks.append(m)\n                class_ids.append(class_id)\n        # Pack instance masks into an array\n        if class_ids:\n            mask = np.stack(instance_masks, axis=2)\n            class_ids = np.array(class_ids, dtype=np.int32)\n            return mask, class_ids\n        else:\n            # Call super class to return an empty mask\n            return super(FoodChallengeDataset, self).load_mask(image_id)\n\n\n    def image_reference(self, image_id):\n        \"\"\"Return a reference for a particular image\n\n            Ideally you this function is supposed to return a URL\n            but in this case, we will simply return the image_id\n        \"\"\"\n        return \"crowdai-food-challenge::{}\".format(image_id)\n    # The following two functions are from pycocotools with a few changes.\n\n    def annToRLE(self, ann, height, width):\n        \"\"\"\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        \"\"\"\n        segm = ann['segmentation']\n        if isinstance(segm, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, height, width)\n            rle = maskUtils.merge(rles)\n        elif isinstance(segm['counts'], list):\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, height, width)\n        else:\n            # rle\n            rle = ann['segmentation']\n        return rle\n\n    def annToMask(self, ann, height, width):\n        \"\"\"\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        \"\"\"\n        rle = self.annToRLE(ann, height, width)\n        m = maskUtils.decode(rle)\n        return m","8fc2d188":"class FoodChallengeConfig(Config):\n    \"\"\"Configuration for training on data in MS COCO format.\n    Derives from the base Config class and overrides values specific\n    to the COCO dataset.\n    \"\"\"\n    # Give the configuration a recognizable name\n    NAME = \"crowdai-food-challenge\"\n\n    # We use a GPU with 12GB memory, which can fit two images.\n    # Adjust down if you use a smaller GPU.\n    IMAGES_PER_GPU = 4\n\n    # Uncomment to train on 8 GPUs (default is 1)\n    GPU_COUNT = 1\n    BACKBONE = 'resnet50'\n    # Number of classes (including background)\n    NUM_CLASSES = 62  # 1 Background + 61 classes\n\n    STEPS_PER_EPOCH=150\n    VALIDATION_STEPS=50\n\n    LEARNING_RATE=0.001\n    IMAGE_MAX_DIM=256\n    IMAGE_MIN_DIM=256\n","f5e22218":"config = FoodChallengeConfig()\nconfig.display()","99403a63":"PRETRAINED_MODEL_PATH = os.path.join(ROOT_DIR,\"data\", \"mask_rcnn_coco.h5\")\nLOGS_DIRECTORY = os.path.join(ROOT_DIR, \"logs\")","4babfe98":"if not os.path.exists(PRETRAINED_MODEL_PATH):\n    utils.download_trained_weights(PRETRAINED_MODEL_PATH)","3db2b56b":"from keras import backend as K\nK.tensorflow_backend._get_available_gpus()","c7d3bf53":"import keras.backend\nK = keras.backend.backend()\nif K=='tensorflow':\n    keras.backend.common.image_dim_ordering()\nmodel = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=LOGS_DIRECTORY)\nmodel_path = PRETRAINED_MODEL_PATH\nmodel.load_weights(model_path, by_name=True, exclude=[\n        \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n        \"mrcnn_bbox\", \"mrcnn_mask\"])","42bd8a99":"dataset_train = FoodChallengeDataset()\ndataset_train.load_dataset(dataset_dir=\"\/kaggle\/input\/food-recognition-challenge\/train\/train\", load_small=False)\ndataset_train.prepare()","7df80660":"dataset_val = FoodChallengeDataset()\nval_coco = dataset_val.load_dataset(dataset_dir=\"\/kaggle\/input\/food-recognition-challenge\/val\/val\", load_small=False, return_coco=True)\ndataset_val.prepare()","db5da096":"class_names = dataset_train.class_names\n# If you don't have the correct classes here, there must be some error in your DatasetConfig\nassert len(class_names)==62, \"Please check DatasetConfig\"\nclass_names","1d509663":"print(\"Training network\")\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE,\n            epochs=15,\n            layers='heads')","77350256":"model_path = model.find_last()\nmodel_path","e630de92":"class InferenceConfig(FoodChallengeConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    NUM_CLASSES = 62  # 1 Background + 61 food classes\n    IMAGE_MAX_DIM=256\n    IMAGE_MIN_DIM=256\n    NAME = \"food\"\n    DETECTION_MIN_CONFIDENCE=0\n\ninference_config = InferenceConfig()\ninference_config.display()","cc3d106b":"# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=ROOT_DIR)\n\n# Load trained weights (fill in path to trained weights here)\nassert model_path != \"\", \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","2a766ae1":"# Show few example of ground truth vs. predictions on the validation dataset \ndataset = dataset_val\nfig = plt.figure(figsize=(10, 30))\n\nfor i in range(4):\n\n    image_id = random.choice(dataset.image_ids)\n    \n    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n        modellib.load_image_gt(dataset_val, inference_config, \n                               image_id, use_mini_mask=False)\n    \n    print(original_image.shape)\n    plt.subplot(6, 2, 2*i + 1)\n    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n                                dataset.class_names, ax=fig.axes[-1])\n    \n    plt.subplot(6, 2, 2*i + 2)\n    results = model.detect([original_image]) #, verbose=1)\n    r = results[0]\n    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'], ax=fig.axes[-1])","977e6ebc":"import json\nwith open('\/kaggle\/input\/food-recognition-challenge\/val\/val\/annotations.json') as json_file:\n    data = json.load(json_file)","a682313d":"d = {}\nfor x in data[\"categories\"]:\n    d[x[\"name\"]]=x[\"id\"]","02857987":"id_category = [0]\nfor x in dataset.class_names[1:]:\n    id_category.append(d[x])\n#id_category","7663e569":"import tqdm\nimport skimage","8e9c939d":"files = glob.glob(os.path.join('\/kaggle\/input\/food-recognition-challenge\/val\/val\/test_images\/images', \"*.jpg\"))\n_final_object = []\nfor file in tqdm.tqdm(files):\n    images = [skimage.io.imread(file) ]\n    #if(len(images)!= inference_config.IMAGES_PER_GPU):\n    #    images = images + [images[-1]]*(inference_config.BATCH_SIZE - len(images))\n    predictions = model.detect(images, verbose=0)\n    #print(file)\n    for _idx, r in enumerate(predictions):\n        \n            image_id = int(file.split(\"\/\")[-1].replace(\".jpg\",\"\"))\n            for _idx, class_id in enumerate(r[\"class_ids\"]):\n                if class_id > 0:\n                    mask = r[\"masks\"].astype(np.uint8)[:, :, _idx]\n                    bbox = np.around(r[\"rois\"][_idx], 1)\n                    bbox = [float(x) for x in bbox]\n                    _result = {}\n                    _result[\"image_id\"] = image_id\n                    _result[\"category_id\"] = id_category[class_id]\n                    _result[\"score\"] = float(r[\"scores\"][_idx])\n                    _mask = maskUtils.encode(np.asfortranarray(mask))\n                    _mask[\"counts\"] = _mask[\"counts\"].decode(\"UTF-8\")\n                    _result[\"segmentation\"] = _mask\n                    _result[\"bbox\"] = [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]]\n                    _final_object.append(_result)\n\nfp = open('\/kaggle\/working\/output.json', \"w\")\nimport json\nprint(\"Writing JSON...\")\nfp.write(json.dumps(_final_object))\nfp.close()","6ba88647":"import random\nimport json\nimport numpy as np\nimport argparse\nimport base64\nimport glob\nimport os\nfrom PIL import Image\n\nfrom pycocotools.coco import COCO\nGROUND_TRUTH_ANNOTATION_PATH = \"\/kaggle\/input\/food-recognition-challenge\/val\/val\/annotations.json\"\nground_truth_annotations = COCO(GROUND_TRUTH_ANNOTATION_PATH)\nsubmission_file = json.loads(open(\"\/kaggle\/working\/output.json\").read())\nresults = ground_truth_annotations.loadRes(submission_file)\ncocoEval = COCOeval(ground_truth_annotations, results, 'segm')\ncocoEval.evaluate()\ncocoEval.accumulate()\ncocoEval.summarize()\n","4636ab8f":"# remove files to allow committing (hit files limit otherwise)\n!rm -rf \/kaggle\/working\/Mask_RCNN","6320a10d":"## Installation\n","c14a7456":"You can change other values in the `FoodChallengeConfig` as well and try out different combinations for best results!","9fcae9fa":"#### Lets start training!!","0ddff81d":"In this Notebook, we will first do an analysis of the Food Recognition Dataset and then use maskrcnn for training on the dataset.","d675174b":"### **BONUS :** Resources to Read\n\n\n* [An Introduction to Image Segmentation](https:\/\/www.analyticsvidhya.com\/blog\/2019\/04\/introduction-image-segmentation-techniques-python\/)\n* [Blog introducing Mask RCNN in COCO dataset](https:\/\/www.analyticsvidhya.com\/blog\/2019\/07\/computer-vision-implementing-mask-r-cnn-image-segmentation\/)\n* [A good blog by matterport on Mask RCNN and it's implementation](https:\/\/engineering.matterport.com\/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46)\n* [Using mmdetection library in Pytorch](https:\/\/github.com\/open-mmlab\/mmdetection\/blob\/master\/docs\/GETTING_STARTED.md)\n","3139f45d":"To train MaskRCNN, two things we have to define `FoodChallengeDataset` that implements the `Dataset` class of MaskRCNN and `FoodChallengeConfig` that implements the `Config` class.\n\nThe `FoodChallengeDataset` helps define certain functions that allow us to load the data. \n\nThe `FoodChallengeConfig` gives the information like `NUM_CLASSES`, `BACKBONE`, etc.","d18bed99":"![AIcrowd-Logo](https:\/\/raw.githubusercontent.com\/AIcrowd\/AIcrowd\/master\/app\/assets\/images\/misc\/aicrowd-horizontal.png)","6a0716a4":"## The Challenge\n\n\n*   Given Images of Food, we are asked to provide Instance Segmentation over the images for the food items.\n*   The Training Data is provided in the COCO format, making it simpler to load with pre-available COCO data processors in popular libraries.\n*   The test set provided in the public dataset is similar to Validation set, but with no annotations.\n*   The test set after submission is much larger and contains private images upon which every submission is evaluated.\n*   Pariticipants have to submit their trained model along with trained weights. Immediately after the submission the AICrowd Grader picks up the submitted model and produces inference on the private test set using Cloud GPUs.\n*   This requires Users to structure their repositories and follow a provided paradigm for submission.\n*   The AICrowd AutoGrader picks up the Dockerfile provided with the repository, builds it and then mounts the tests folder in the container. Once inference is made, the final results are checked with the ground truth.\n\n***For more submission related information, please check [the AIcrowd Challenge page](https:\/\/www.aicrowd.com\/challenges\/food-recognition-challenge) and [the starter kit](https:\/\/github.com\/AIcrowd\/food-recognition-challenge-starter-kit\/).***","592741c8":"## MaskRCNN","d4cb259a":"This dataset and notebook correspond to the [Food Recognition Challenge](https:\/\/www.aicrowd.com\/challenges\/food-recognition-challenge) being held on [AICrowd](https:\/\/www.aicrowd.com\/).","d728df88":"## The Notebook\n> *  Installation of MaskRCNN\n> *  Using MatterPort MaskRCNN Library and Making local inference with it\n> *  Local Evaluation Using Matterport MaskRCNN\n\n***A bonus section on other resources to read is also added!***"}}