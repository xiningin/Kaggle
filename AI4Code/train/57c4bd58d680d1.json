{"cell_type":{"9aed2fb8":"code","d933d770":"code","8b98cc71":"code","1f974b47":"code","82e56155":"code","2f17895f":"markdown","72ae7c3a":"markdown"},"source":{"9aed2fb8":"!pip install kaggle-environments --upgrade","d933d770":"%%writefile my-sub-file.py\n\nimport json\nimport numpy as np\nimport pandas as pd\n\nbasic_state = None\nreward_full = 0\nstep_ending = None\n\ndef basic_mab (observation, configuration):\n\n\n    no_reward_step = 0.1\n    decay_rate = 0.99\n\n    global basic_state,reward_full,step_ending\n\n    if observation.step == 0:\n        basic_state = [[1,1] for i in range(configuration[\"banditCount\"])]\n    else:\n        reward_final = observation[\"reward\"] - reward_full\n        reward_full = observation[\"reward\"]\n\n        player = int(step_ending == observation.lastActions[1])\n\n        if reward_final > 0:\n            basic_state[observation.lastActions[player]][0] += reward_final\n        else:\n            basic_state[observation.lastActions[player]][1] += no_reward_step\n\n        basic_state[observation.lastActions[0]][0] = (basic_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\n        basic_state[observation.lastActions[1]][0] = (basic_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\n\n#     implementing Beta distribution to generate random number, for each agent and select the most lucky one\n    best_probability = -1\n    agent_optimal = None\n    for k in range(configuration[\"banditCount\"]):\n        probability = np.random.beta(basic_state[k][0],basic_state[k][1])\n        if probability > best_probability:\n            best_probability = probability\n            agent_optimal = k\n\n    step_ending = agent_optimal\n    return agent_optimal","8b98cc71":"%%writefile random_agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","1f974b47":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\nenv.reset()\nenv.run([\"random_agent.py\", \"my-sub-file.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","82e56155":"env.reset()\nenv.run([\"my-sub-file.py\", \"my-sub-file.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","2f17895f":"#### Forked from [Ilia Larchenko](https:\/\/www.kaggle.com\/ilialar\/simple-multi-armed-bandit) and then made changes.\n\nPlease upvote the original Notebook as well.\n\n### Multi-armed bandit problems are some of the simplest reinforcement learning (RL) problems to solve. We have an agent which we allow to choose actions, and each action has a reward that is returned according to a given, underlying probability distribution. The game is played over many episodes (single actions in this case) and the goal is to maximize your reward.\n\nTo exaplain further, how do you most efficiently identify the best machine to play, whilst sufficiently exploring the many options in real time? This problem is not an exercise in theoretical abstraction, it is an analogy for a common problem that organisations face all the time, that is, how to identify the best message to present to customers (message is broadly defined here i.e. webpages, advertising, images) such that it maximises some business objective (e.g. clickthrough rate, signups).\n\nThe classic approach to making decisions across variants with unknown performance outcomes is to perform multiple A\/B tests. These are typically run by evenly directing a percentage of traffic across each of the variants over a number of weeks, then performing statistical tests to identify which variant is the best. This is perfectly fine when there are a small number of variations of the message (e.g. 2\u20134), but can be quite inefficient in terms of both time and opportunity cost when there are many.\n\nOne simple example is in the optimization of click-through rates (CTR) of online ads. Perhaps you have 10 ads that essentially say the same thing\n(maybe the words and designs are slightly different from one another). At first, you want to know which ad performs best and yields the highest CTR.\n\n\nAnother similar problem, let\u2019s say you have a limited resource (e.g.,advertising budget) and some choices (10 ad variants). How will you allocate your resource among those choices so you can maximize your gain?\n\nFirst, you have to \u201cexplore\u201d and try the ads one by one. Of course, if you\u2019re seeing that Ad 1 performs unusually well, you\u2019ll \u201cexploit\u201d it and run it for the rest of the campaign. You don\u2019t need to waste your money on underperforming ads. Stick to the winner and continuously exploit its performance. There\u2019s one catch, though. Early on, Ad 1 might be performing well, so we\u2019re tempted to use it again and again. But what if Ad 2 catches up\nand if we let things unfold Ad 2 will produce higher gains? We\u2019ll never know because the performance of Ad 1 was already exploited. There will always be tradeoffs in many data analysis and machine learning\nprojects. That\u2019s why it\u2019s always recommended to set performance targets beforehand instead of wondering about the what-ifs later. Even in the most sophisticated techniques and algorithms, tradeoffs and constraints are always there.\n\nThis is where Reinforcement Learning (RL) comes in. In a nutshell, RL is about reinforcing the correct or desired behaviors as time passes. A reward\nfor every correct behavior and a punishment otherwise. \n\nThe general reinforcement learning problem is a very general setting. Actions affect subsequent observations. Rewards are only observed corresponding to the chosen actions. The environment may be either fully or partially observed. Accounting for all this complexity at once may ask too much of researchers. Moreover, not every practical problem exhibits all this complexity. As a result, researchers have studied a number of special cases of reinforcement learning problems.\nWhen the environment is fully observed, we call the RL problem a Markov Decision Process (MDP). When the state does not depend on the previous actions, we call the problem a contextual bandit problem. When there is no state, just a set of available actions with initially unknown rewards, this problem is the classic multi-armed bandit problem.\n\nWhile in most learning problems we have a continuously parametrized function f where we want to learn its parameters (e.g., a deep network), in a bandit problem we only have a finite number of arms that we can pull, i.e., a finite number of actions that we can take.\n\n\n---\n\n#### From this [post](https:\/\/lilianweng.github.io\/lil-log\/2018\/01\/23\/the-multi-armed-bandit-problem-and-its-solutions.html), a more mathematical explanation...\n\nA Bernoulli multi-armed bandit can be described as a tuple of \u27e8A,R\u27e9, where:\n\nWe have K machines with reward probabilities, ${\u03b81,\u2026,\u03b8K}$.\nAt each time step t, we take an action a on one slot machine and receive a reward r.\nA is a set of actions, each referring to the interaction with one slot machine. The value of action a is the expected reward, $Q(a)=E[r|a]=\u03b8$. If action at at the time step t is on the i-th machine, then Q(at)=\u03b8i.\nR is a reward function. In the case of Bernoulli bandit, we observe a reward r in a stochastic fashion. At the time step t, rt=R(at) may return reward 1 with a probability Q(at) or 0 otherwise.\nIt is a simplified version of Markov decision process, as there is no state S.\n\nThe goal is to maximize the cumulative reward $\u2211Tt=1rt$. If we know the optimal action with the best reward, then the goal is same as to minimize the potential regret or loss by not picking the optimal action.\n\nThe optimal reward probability \u03b8\u2217 of the optimal action a\u2217 is:\n\n$\u03b8\u2217=Q(a\u2217)=maxa\u2208AQ(a)=max1\u2264i\u2264K\u03b8i$\n\nOur loss function is the total regret we might have by not selecting the optimal action up to the time step T:\n\n$LT=E[\u2211t=1T(\u03b8\u2217\u2212Q(at))]$\n\n---\n\n### Bandit Strategies\n\nBased on how we do exploration, there several ways to solve the multi-armed bandit.\n\nNo exploration: the most naive approach and a bad one.\nExploration at random\nExploration smartly with preference to uncertainty\n\n#### numpy.random.beta - has the probability distribution function\n\n![img](https:\/\/i.imgur.com\/TRlNMCd.png)\n\n---\n\nThe below function is inspired by this [Kernel](https:\/\/www.kaggle.com\/ilialar\/simple-multi-armed-bandit#Competition-specific-changes)","72ae7c3a":"#### References\n\n- CS229 Supplemental Lecture notes: [Hoeffding\u2019s inequality](http:\/\/cs229.stanford.edu\/extra-notes\/hoeffding.pdf).\n\n- RL Course by David Silver - Lecture 9: E[xploration and Exploitation](https:\/\/youtu.be\/sGuiWX07sKw)\n\n- Olivier Chapelle and Lihong Li. \u201cAn empirical evaluation of thompson sampling.\u201d NIPS. 2011.\n\n-  https:\/\/web.eecs.umich.edu\/~teneket\/pubs\/MAB-Survey.pdf\n\n-  https:\/\/arxiv.org\/pdf\/1904.07272.pdf"}}