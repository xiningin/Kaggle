{"cell_type":{"74b91a8c":"code","da1186eb":"code","76ad43d2":"code","c0714d0c":"code","631dfaf8":"code","e15d8acc":"code","3d05dfb4":"code","78aac902":"code","03fc531a":"code","12e51ae0":"code","13a26a3c":"code","e7e4d421":"code","3689de20":"code","4fd0ef47":"code","fb6d5198":"code","80e3b0b8":"code","91bd0362":"code","771826ae":"code","b1f5e90f":"code","6eb34221":"code","4a860383":"code","2470be38":"code","5cc1c1ab":"code","b66bf6af":"code","cc9db599":"code","07bf2ebf":"code","444d4024":"code","d761e7e8":"code","65ab9f2a":"code","2c225f6f":"code","651fff13":"code","1f660080":"code","7c61ff21":"code","4f90bcf3":"code","2e9076db":"code","07171d86":"code","c8d2ee1c":"code","c8b236d9":"code","2b3a5061":"code","f7f33b52":"code","ed12831e":"code","73e68f42":"code","61ee0254":"code","848fb9fb":"code","0ec32436":"code","c5d85583":"code","d4b35c25":"code","ee53097e":"code","fbc6e00c":"code","2e3cfaa1":"code","b3c9d0ec":"code","f9076b14":"code","1b0e358e":"code","a0016fe5":"code","84a35ce1":"code","3d5fc5ce":"code","4812c8d6":"code","fdb381f1":"code","426208e7":"code","e880d16c":"code","ceb86e3c":"code","4478a7a8":"code","2cc219c3":"code","5582e0a1":"code","e3c3b7d4":"code","4328b162":"code","80ac0403":"code","98398ba3":"code","542a7b58":"code","113fab58":"code","15b0e5e3":"code","71a3c0a6":"code","3c16fcca":"code","aa1a89b6":"code","8818c8e1":"code","fbc307f3":"code","404204dd":"code","51c6e8ff":"code","2052bd85":"code","8d348688":"code","210dd891":"code","d61e52f4":"code","7f6193b2":"code","96613c87":"code","756460f4":"code","05d4c31b":"code","abd3cb41":"code","8dfe2d6d":"code","4b243b08":"code","c14f2c34":"code","fc64befc":"code","fd19fdd2":"code","015aac5e":"code","45695f9c":"code","b14b96a5":"code","6697f794":"code","ba088a8f":"code","d8729b99":"code","0c7d23ec":"code","dc753e79":"code","126593d6":"code","16890bb1":"code","57f35295":"code","3daa80e7":"code","d902475f":"code","74214f41":"code","b329c6eb":"code","02489561":"code","91be2bc8":"code","3a342825":"code","2d8833ca":"code","fac7bdb5":"code","20073cd2":"code","05a9824c":"code","1b22ac53":"code","c843aa8f":"markdown","9f10111f":"markdown","6c99ac94":"markdown","4e00d921":"markdown","cab15c2e":"markdown","73326cfe":"markdown","163283fb":"markdown","53e83396":"markdown","3db1b394":"markdown","c2b677a0":"markdown","197bb389":"markdown","44659c7a":"markdown","93005ae4":"markdown","1a1cfbc9":"markdown","59657ff3":"markdown","c6d36948":"markdown","e2d07007":"markdown","8f7742be":"markdown","a7d87472":"markdown","c9ded256":"markdown","a1cd945a":"markdown","d9182fcb":"markdown","48b30368":"markdown","fd0ef395":"markdown","f208545b":"markdown","15abad06":"markdown","2a46d6ce":"markdown","428f58b9":"markdown","4378e5eb":"markdown","bfa87382":"markdown","b4ce7b72":"markdown","42649211":"markdown","eb5c19b1":"markdown","47f9432a":"markdown","61fa224b":"markdown","4dd2dd97":"markdown","b4430ba3":"markdown","a5bc6839":"markdown","c61b5d12":"markdown","5718bef0":"markdown","6df05634":"markdown","7e6f7f2b":"markdown","e1703194":"markdown","270dfd74":"markdown","f23601d2":"markdown","cb0f2bd2":"markdown","0015d690":"markdown","4afe7c76":"markdown","caefac33":"markdown","468e47e3":"markdown","03d9a63e":"markdown","83881ec1":"markdown","5b287fe2":"markdown","27eeb9ac":"markdown","46ae2aec":"markdown","a3fe9539":"markdown","60c46770":"markdown","024405e5":"markdown","48b321b7":"markdown","d9367233":"markdown"},"source":{"74b91a8c":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\npd.set_option('display.max_columns',None)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn import preprocessing","da1186eb":"# reading the data and displaying top 5 rows\n# ..\/input\/creditcardfraud\/creditcard.csv\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","76ad43d2":"print('Shape of data frame : ',df.shape)","c0714d0c":"# checking type of data for each column\ndf.info()","631dfaf8":"# checking the count of null values in dataframe\ndf.isnull().sum()","e15d8acc":"# describing the distribution of dataset\ndf.describe()","3d05dfb4":"# distrubution of classes\nclasses=df['Class'].value_counts()\nnormal_share=classes[0]\/df['Class'].count()*100\nfraud_share=classes[1]\/df['Class'].count()*100\n\nprint('Percentage of fraud in given dataset : ', fraud_share)\nprint('Number of fraud in dataset : ',classes[1])\nprint('Number of non-fraud in dataset : ',classes[0])","78aac902":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,8))\nf.suptitle('Amount per transaction by class')\n\nbins = 50\n\nax1.hist(df[df.Class == 1].Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(df[df.Class == 0].Amount, bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","03fc531a":"# Do fraudulent transactions occur more often during certain time?\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,8))\nf.suptitle('Time of transaction vs Amount by class')\n\nax1.scatter(df[df.Class == 1].Time, df[df.Class == 1].Amount)\nax1.set_title('Fraud')\n\nax2.scatter(df[df.Class == 0].Time, df[df.Class == 0].Amount)\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","12e51ae0":"var = df.columns.tolist()\ni = 0\nt0 = df.loc[df['Class'] == 0]\nt1 = df.loc[df['Class'] == 1]\nfig, ax = plt.subplots(8,4,figsize=(16,28))\n\nfor feature in var:\n    i += 1\n    plt.subplot(8,4,i)\n    sns.kdeplot(t0[feature], bw=0.3,label=\"Class 0\")\n    sns.kdeplot(t1[feature], bw=0.3,label=\"Class 1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","13a26a3c":"X = df.drop(labels='Class', axis=1) # Features\ny = df.loc[:,'Class']               # Response\n                \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)","e7e4d421":"# distribution of amount with respect to class\nfig, ax = plt.subplots(1,2,figsize=(18,5))\nsns.boxplot(x = \"Class\", y = \"Amount\", data = df, ax=ax[0])\nsns.boxplot(x = \"Class\", y = \"Time\", data = df, ax=ax[1])\nplt.show()","3689de20":"fig, ax = plt.subplots(1,2,figsize=(18,5))\nsns.distplot(X_train['Amount'],ax = ax[0])\nsns.distplot(X_train['Time'],ax = ax[1])\nplt.show()","4fd0ef47":"X_train.loc[:,'Time'] = X_train.Time \/ 3600\nX_test.loc[:,'Time'] = X_test.Time \/ 3600","fb6d5198":"plt.figure(figsize=(12,5), dpi=80)\nsns.distplot(X_train['Time'], bins=48, kde=False)\nplt.xlim([0,48])\nplt.xticks(np.arange(0,54,6))\nplt.xlabel('Time After First Transaction (hr)')\nplt.ylabel('Count')\nplt.title('Transaction Times')","80e3b0b8":"print('Skewness of Amount column :',X_train['Amount'].skew())","91bd0362":"# converting zero amount to very small values\nX_train.loc[:,'Amount'] = X_train['Amount'] + 1e-9\nX_train.loc[:,'Amount'], maxlog, (min_ci, max_ci) = sp.stats.boxcox(X_train['Amount'], alpha=0.01)","771826ae":"plt.figure(figsize=(12,5))\nsns.distplot(X_train['Amount'])\nplt.xlabel('Transformed Amount')\nplt.ylabel('Count')\nplt.title('Transaction Amounts (Box-Cox Transformed)')","b1f5e90f":"print('Skewness of modified Amount Column : ',X_train['Amount'].skew())","6eb34221":"# converting the test 'amount' data using Box-Cox Transformation\nX_test.loc[:,'Amount'] = X_test['Amount'] + 1e-9\nX_test.loc[:,'Amount'] = sp.stats.boxcox(X_test['Amount'], lmbda=maxlog)","4a860383":"pca_vars = ['V%i' % k for k in range(1,29)]\nX_train[pca_vars].describe()","2470be38":"plt.figure(figsize=(12,5))\nsns.barplot(x=pca_vars, y=X_train[pca_vars].mean(), color='green')\nplt.xlabel('PCA Column')\nplt.ylabel('Mean')\nplt.title('V1-V28 Means')","5cc1c1ab":"plt.figure(figsize=(12,5))\nsns.barplot(x=pca_vars, y=X_train[pca_vars].std(), color='blue')\nplt.xlabel('PCA Column')\nplt.ylabel('Standard Deviation')\nplt.title('V1-V28 Standard Deviations')","b66bf6af":"plt.figure(figsize=(12,5))\nsns.barplot(x=pca_vars, y=X_train[pca_vars].skew(), color='green')\nplt.xlabel('PCA Column')\nplt.ylabel('Skewness')\nplt.title('V1-V28 Skewnesses')","cc9db599":"fig, ax = plt.subplots(2,2,figsize=(18,10))\nsns.distplot(X_train['V28'],ax = ax[0][0])\nsns.distplot(X_train['V8'],ax = ax[0][1])\nsns.distplot(X_train['V23'],ax = ax[1][0])\nsns.distplot(X_train['V2'],ax = ax[1][1])\nplt.title('Histogram')\n# plt.show()","07bf2ebf":"# Note the log scale on the y-axis in the plot below\nplt.figure(figsize=(12,5))\nplt.yscale('log')\nsns.barplot(x=pca_vars, y=X_train[pca_vars].kurtosis(), color='darkorange')\nplt.xlabel('Column')\nplt.ylabel('Kurtosis')\nplt.title('V1-V28 Kurtoses')","444d4024":"# Medians of all principal transforned component\nplt.figure(figsize=(12,5))\nsns.barplot(x=pca_vars, y=X_train[pca_vars].median(), color='darkblue')\nplt.xlabel('Column')\nplt.ylabel('Median')\nplt.title('V1-V28 Medians')","d761e7e8":"# IQR of all prinipal transformed columns\nplt.figure(figsize=(12,5))\nsns.barplot(x=pca_vars, y=X_train[pca_vars].quantile(0.75) - X_train[pca_vars].quantile(0.25), color='darkred')\nplt.xlabel('Column')\nplt.ylabel('IQR')\nplt.title('V1-V28 IQRs')","65ab9f2a":"# heat map to eliminate highly corelated variable\nplt.figure(figsize=(25,25))\nsns.heatmap(X_train.corr(), annot=False, vmin=-1, vmax=1, cmap=\"YlGnBu\", linewidths=.5)","2c225f6f":"# Importing all the necessary liabraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.utils import resample\nfrom imblearn.over_sampling import SMOTE","651fff13":"# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score,roc_auc_score, recall_score, roc_curve\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV","1f660080":"# defining the params for Logistic Regression\nparam_grid = {'C': [0.01, 0.1, 1, 10, 20],\n             'penalty':['l1', 'l2']}\n\nlogreg = LogisticRegression(random_state=2)\n\ngrid_search_lr = GridSearchCV(logreg, param_grid=param_grid, scoring='recall', cv=5)\n\ngrid_search_lr.fit(X_train, y_train)","7c61ff21":"print(grid_search_lr.best_score_)\n\nprint(grid_search_lr.best_params_)","4f90bcf3":"logreg = LogisticRegression(C=1, penalty='l2',random_state=2)\n\n#Fiting the model\nlogreg.fit(X_train, y_train)\n           \n# Printing the Training Score\nprint(\"ROC AUC score X and y Train: \")\nprint(roc_auc_score(y_train, logreg.predict(X_train)))\n\nprint(\"ROC AUC score X and y Test: \")\nprint(roc_auc_score(y_test, logreg.predict(X_test)))","2e9076db":"# AUC-ROC curve for training data\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Generate precision recall curve values: precision, recall, thresholds\ntpr, fpr, thresholds = roc_curve(y_train, logreg.predict_proba(X_train)[:,1])\n\n# Plot ROC curve\nplt.plot(tpr, fpr)\nplt.xlabel('True Positive')\nplt.ylabel('False Positive')\nplt.title('ROC Curve')\nplt.show()","07171d86":"# Running the random forest with default parameters.\nrfc = RandomForestClassifier()\n\n# fiting the model on training data\nrfc.fit(X_train,y_train)","c8d2ee1c":"# Making predictions\npredictions_rfc = rfc.predict(X_test)","c8b236d9":"tmp = pd.DataFrame({'Feature': X_train.columns.tolist(), 'Feature importance': rfc.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (10,5))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show() ","2b3a5061":"print('Confusion Matrix : ')\nconfusion_matrix(y_test,predictions_rfc)","f7f33b52":"print('AU score : ',roc_auc_score(y_test, predictions_rfc))","ed12831e":"# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(2, 150, 10)}\n\n# instantiate the model\nrf_1 = RandomForestClassifier()\n\n\n# fit tree on training data\nrf_1 = GridSearchCV(rf_1, parameters, \n                    cv=n_folds, \n                   scoring=\"recall\", return_train_score=True)\nrf_1.fit(X_train, y_train)","73e68f42":"scored = rf_1.cv_results_\npd.DataFrame(rf_1.cv_results_).head()","61ee0254":"plt.figure()\nplt.plot(scored['param_max_depth'],\n        scored['mean_train_score'],\n        label='train_recall')\nplt.plot(scored['param_max_depth'],\n        scored['mean_test_score'],\n        label='test_recall')\nplt.xlabel('Depth of tree')\nplt.ylabel('Mean Recall')\nplt.legend()\nplt.show()","848fb9fb":"# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(200, 500, 50)}\n\n# instantiate the model\nrf_2 = RandomForestClassifier()\n\n# fit tree on training data\nrf_2 = GridSearchCV(rf_2, parameters, \n                    cv=n_folds, \n                   scoring=\"recall\",\n                   return_train_score=True)\nrf_2.fit(X_train, y_train)","0ec32436":"scored = rf_2.cv_results_\npd.DataFrame(rf_2.cv_results_).head()","c5d85583":"plt.figure()\nplt.plot(scored['param_min_samples_split'],\n        scored['mean_train_score'],\n        label='train_recall')\nplt.plot(scored['param_min_samples_split'],\n        scored['mean_test_score'],\n        label='test_recall')\nplt.xlabel('Number of Split')\nplt.ylabel('Mean Recall')\nplt.legend()\nplt.show()","d4b35c25":"# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(15, 400, 50)}\n\n# instantiate the model\nrf_3 = RandomForestClassifier()\n\n\n# fit tree on training data\nrf_3 = GridSearchCV(rf_3, parameters, \n                    cv=n_folds, \n                   scoring=\"recall\",\n                     n_jobs=-1, pre_dispatch='2*n_jobs',\n                     return_train_score=True)\nrf_3.fit(X_train, y_train)","ee53097e":"scored = rf_3.cv_results_\npd.DataFrame(rf_3.cv_results_).head()","fbc6e00c":"plt.figure()\nplt.plot(scored['param_min_samples_leaf'],\n        scored['mean_train_score'],\n        label='train_recall')\nplt.plot(scored['param_min_samples_leaf'],\n        scored['mean_test_score'],\n        label='test_recall')\nplt.xlabel('Number of Sample leaf')\nplt.ylabel('Mean Recall')\nplt.legend()\nplt.show()","2e3cfaa1":"# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_features': [4, 8, 14, 20, 24, 28]}\n\n# instantiate the model\nrf_4 = RandomForestClassifier()\n\n\n# fit tree on training data\nrf_4 = GridSearchCV(rf_4, parameters, \n                    cv=n_folds, \n                   scoring=\"recall\",\n                     n_jobs=-1, pre_dispatch='2*n_jobs',\n                     return_train_score=True)\nrf_4.fit(X_train, y_train)","b3c9d0ec":"scored = rf_4.cv_results_\npd.DataFrame(rf_4.cv_results_).head()","f9076b14":"plt.figure()\nplt.plot(scored['param_max_features'],\n        scored['mean_train_score'],\n        label='train_recall')\nplt.plot(scored['param_max_features'],\n        scored['mean_test_score'],\n        label='test_recall')\nplt.xlabel('Number of Features')\nplt.ylabel('Mean Recall')\nplt.legend()\nplt.show()","1b0e358e":"# Create the parameter grid based on the results of random search\nparam_grid = {\n    'max_depth': [8,10,15,17],\n    'min_samples_leaf': range(20,180,40),\n    'min_samples_split': range(200,300,20),\n    'n_estimators': [100, 200, 300],\n    'max_features': [8,10,15,20]\n}\n# Create a based model\nrf_5 = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf_5, param_grid = param_grid,\n                          cv = 3, n_jobs = -1,verbose = 1)\n\ngrid_search.fit(X_train, y_train)","a0016fe5":"grid_search.best_params_","84a35ce1":"scores = grid_search.cv_results_\npd.DataFrame(grid_search.cv_results_).to_csv('randomforest_gridsearch.csv', index=False)","3d5fc5ce":"# instantiate the model\nrf_final = RandomForestClassifier(n_estimators=100, \n                                  max_depth=10,\n                                  max_features=10, \n                                  min_samples_leaf=20, \n                                  min_samples_split=240)\nrf_final.fit(X_train, y_train)","4812c8d6":"# storing the results on test set\nprediction_rf = rf_final.predict(X_test)","fdb381f1":"# analysing the resuts of confusion matrix\nconfusion_matrix(y_test, prediction_rf)","426208e7":"# 18 fraud transtions are not able to predict by the model","e880d16c":"# roc score when restricting the parameters of tree\nroc_auc_score(y_test, prediction_rf)","ceb86e3c":"y_pred_prob = rf_final.predict_proba(X_test)[:,1]\n\n# Generate precision recall curve values: precision, recall, thresholds\ntpr, fpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot(tpr, fpr)\nplt.xlabel('True Positive')\nplt.ylabel('False Positive')\nplt.title('ROC Curve')\nplt.show()","4478a7a8":"# Prepare the train and valid datasets\ndtrain = xgb.DMatrix(X_train, y_train)\ndtest = xgb.DMatrix(X_test, y_test)\n#What to monitor (in this case, *train*)\nwatchlist = [(dtrain, 'train')]\n# Set xgboost parameters\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.039\nparams['silent'] = True\nparams['max_depth'] = 4\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.9\nparams['eval_metric'] = 'auc'\nmodel = xgb.train(params,\n                dtrain,\n                1000,\n                watchlist,\n                early_stopping_rounds=50,\n                maximize=True,\n                verbose_eval=50)","2cc219c3":"fig,(ax) = plt.subplots(ncols=1, figsize=(18,12))\nxgb.plot_importance(model, height=0.8, title=\"Features importance (XGBoost)\", ax=ax, color=\"green\") ","5582e0a1":"prediction_xgb = model.predict(dtest)\nprint('Auc Roc Score: ')\nroc_auc_score(y_test, prediction_xgb)","e3c3b7d4":"# analysing the resuts of confusion matrix\nprint(confusion_matrix(y_test, np.where(prediction_xgb<0.55, 0,1)))","4328b162":"print(classification_report(y_test,  np.where(prediction_xgb<0.55, 0,1)))","80ac0403":"# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\nX.head()","98398ba3":"not_fraud = X[X.Class==0]\nfraud = X[X.Class==1]\n\n# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_fraud, fraud_upsampled])\n\n# check new class counts\nupsampled.Class.value_counts()","542a7b58":"y_train = upsampled.Class\nX_train = upsampled.drop('Class', axis=1)\n\nlr_upsampled = LogisticRegression(C=1, penalty='l2',random_state=2).fit(X_train, y_train)\n\nupsampled_pred = lr_upsampled.predict(X_test)","113fab58":"# analysing the resuts of confusion matrix\nconfusion_matrix(y_test, upsampled_pred)","15b0e5e3":"recall_score(y_test, upsampled_pred)","71a3c0a6":"print('Auc Roc Score: ')\nroc_auc_score(y_test, upsampled_pred)","3c16fcca":"rf_upsample = RandomForestClassifier(n_estimators=100, \n                                  max_depth=10,\n                                  max_features=10, \n                                  min_samples_leaf=20, \n                                  min_samples_split=240)\nrf_upsample.fit(X_train, y_train)\n\nupsampled_pred = rf_upsample.predict(X_test)","aa1a89b6":"# analysing the resuts of confusion matrix\nconfusion_matrix(y_test, upsampled_pred)","8818c8e1":"recall_score(y_test, upsampled_pred)","fbc307f3":"print('Auc Roc Score: ')\nroc_auc_score(y_test, upsampled_pred)","404204dd":"not_fraud = X[X.Class==0]\nfraud = X[X.Class==1]\nconfusion_matrix(y_test, upsampled_pred)\n\nrecall_score(y_test, upsampled_pred)\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\n# checking counts\ndownsampled.Class.value_counts()","51c6e8ff":"y_train = downsampled.Class\nX_train = downsampled.drop('Class', axis=1)\n\nlr_undersampled = LogisticRegression(C=1, penalty='l2',random_state=2).fit(X_train, y_train)\n\nundersampled_pred = lr_undersampled.predict(X_test)","2052bd85":"confusion_matrix(y_test, undersampled_pred)","8d348688":"recall_score(y_test, undersampled_pred)","210dd891":"rf_undersampled = RandomForestClassifier(n_estimators=100, \n                                  max_depth=10,\n                                  max_features=10, \n                                  min_samples_leaf=20, \n                                  min_samples_split=240)\nrf_undersampled.fit(X_train, y_train)\n\nundersampled_pred = rf_upsample.predict(X_test)","d61e52f4":"confusion_matrix(y_test, undersampled_pred)","7f6193b2":"recall_score(y_test, undersampled_pred)","96613c87":"OS = SMOTE(random_state=12)\nX_train_OS, y_train_OS = OS.fit_sample(X_train, y_train)","756460f4":"print(\"Label 1, After using SMOTE: {}\".format(sum(y_train_OS==1)))\nprint(\"Label 0, After using SMOTE: {}\".format(sum(y_train_OS==0)))","05d4c31b":"lr_os1 = LogisticRegression(solver='liblinear').fit(X_train_OS, y_train_OS)\n\ny_lr_os_predict = lr_os1.predict(X_test)","abd3cb41":"confusion_matrix(y_test, y_lr_os_predict)","8dfe2d6d":"recall_score(y_test, y_lr_os_predict)","4b243b08":"lr_os = LogisticRegression(C=1, penalty='l2',random_state=2).fit(X_train_OS, y_train_OS)\n\ny_lr_os_predict = lr_os.predict(X_test)","c14f2c34":"confusion_matrix(y_test, y_lr_os_predict)","fc64befc":"recall_score(y_test, y_lr_os_predict)","fd19fdd2":"# instantiate the model\nrf_os = RandomForestClassifier(n_estimators=100, \n                                  max_depth=10,\n                                  max_features=10, \n                                  min_samples_leaf=20, \n                                  min_samples_split=240)\nrf_os.fit(X_train_OS, y_train_OS)","015aac5e":"confusion_matrix(y_train, rf_os.predict(X_train))","45695f9c":"confusion_matrix(y_test, rf_os.predict(X_test))","b14b96a5":"recall(y_test.values, rf_os.predict(X_test))","6697f794":"(array([0, 0, 0, ..., 0, 0, 0], dtype=int64), array([0, 0, 0, ..., 0, 0, 0], dtype=int64))","ba088a8f":"print(classification_report(y_test,  rf_os.predict(X_test)))","d8729b99":"# y_test has 98 fraud transaction, out of which 89 are predicted correctly\n# and 9 are being missed, but there are 178 non-fraud transaction which has been marked as fraud","0c7d23ec":"# roc-au score\nprint('ROC -AOC score : ')\nroc_auc_score(y_test, rf_os.predict(X_test))","dc753e79":"# Prepare the train and valid datasets\ndtrain_os= xgb.DMatrix(X_train_OS, y_train_OS)\ndtest_os = xgb.DMatrix(X_test, y_test)\n#What to monitor (in this case, *train*)\nwatchlist = [(dtrain_os, 'train')]\n# Set xgboost parameters\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.039\nparams['silent'] = True\nparams['max_depth'] = 5\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.9\nparams['eval_metric'] = 'auc'\nmodel = xgb.train(params,\n                dtrain_os,\n                1000,\n                watchlist,\n                early_stopping_rounds=50,\n                maximize=True,\n                verbose_eval=50)","126593d6":"confusion_matrix(y_train_OS, np.where(model.predict(dtrain_os)<0.55, 0,1))","16890bb1":"confusion_matrix(y_train_OS, np.where(model.predict(dtrain_os)<0.57, 0,1))","57f35295":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()","3daa80e7":"draw_roc(y_train_OS, model.predict(dtrain_os))","d902475f":"pred_xgb_os = model.predict(dtest_os)","74214f41":"print('Auc Roc Score: ')\nroc_auc_score(y_test, pred_xgb_os)","b329c6eb":"y_test_pred = np.where(pred_xgb_os<0.5, 0,1)","02489561":"confusion_matrix(y_test, y_test_pred)","91be2bc8":"# y_test has 98 fraud transaction, out of which 89 are predicted correctly\n# and 9 are being missed, but there are 50 non-fraud transaction which has been marked as fraud","3a342825":"# XG Boost performs much better than Random Forest","2d8833ca":"confusion_matrix(y_test, np.where(pred_xgb_os<0.4, 0,1))","fac7bdb5":"confusion_matrix(y_test, np.where(pred_xgb_os<0.6, 0,1))","20073cd2":"confusion_matrix(y_test, np.where(pred_xgb_os<0.55, 0,1))","05a9824c":"print(classification_report(y_test,  np.where(pred_xgb_os<0.55, 0,1)))","1b22ac53":"print('Confusion Matrix : ')\nconfusion_matrix(y_test, np.where(pred_xgb_os<0.55, 0,1))","c843aa8f":"That's a strong right skew. Let's use a power transform to bring the transaction amounts closer to a normal distribution","9f10111f":"**Defining threshold as 0.4**","6c99ac94":"-----\n# 7. Over Sampling","4e00d921":"**Defining threshold as 0.55**","cab15c2e":"**Best parameter on training set**","73326cfe":"**Feature Importance**","163283fb":"let's now look at the list of hyperparameters which we can tune to improve model performance","53e83396":"- **Memory usage is 67.4 MB**\n- **all principle component tranformed columns are of type float64**\n- **total 31 columns**\n    - **30 Independent Variables**\n        - 28 principal transformed columns\n        - 1 time specific column\n        - 1 amount column, describing amount of transaction\n    - **1 Dependent Variable**\n        -1 class column, as target variable","3db1b394":"------\n## 4. Removing Skewness of Amount Column\n\nUsing box-cox transform","c2b677a0":"## 9.2. Random Forest on Modified Data\n\nTree having best param is been trained","197bb389":"So our power transform removed most of the skewness in the Amount variable. Now we need to compute the Box-Cox transform on the test data amounts as well, using the  \u03bb  value estimated on the training data.","44659c7a":"**Best Hyperparameter for tree**\n\n### Caution - avoid runiing this cell, consumes lot of time","93005ae4":"## 3.2 Histogram Plots","1a1cfbc9":"- Plotting the means of all principal transformed variables","59657ff3":"**1.3. Visualizing Distribution of all columns**","c6d36948":"### 6.2.1 Random Forest Hyperparameter Tuning\n\n**Max Depth of Tree**","e2d07007":"# 6 Model Building\n","8f7742be":"The most important features are V17, V12, V14, V10, V16, V11.","a7d87472":"## 6.1 Logistic regression\n**Defining the params for Logistic Regression**\n\n\n**(Hyperparameter Tuning)**","c9ded256":"From above box plot we can easily infer that there are no fraud transactions occur above the transaction amount of 3000. All of the fraud transactions have transaction amount less than 3000. However, there are many transactions which have a transaction amount greater than 3000 and all of them are genuine.","a1cd945a":"# 11. Result\n\nAnalysing all the result we can conclude that **XGBoost** model trained after applying **SMOTE** is best ML model to be used in future\n\n\nWe can finilised the threshold of 0.55\n- Recall = 0.91\n- Auc Roc = 0.9894\n- F1 score = 0.77\n    - Confusion Matrix : [[56820,  44]\n                     [9,    89]]","d9182fcb":"recall rate has improved after oversampling the data but at the same time Flase Positive number has also increased","48b30368":"### Main task is to reduce 13 as low as possible\n\n- **Our model should idealy capture all fraud transaction i.e False Negative = 0**","fd0ef395":"## 9.3. XG Boost on Modified Data","f208545b":"- Standard devation is ranging from 1.9 to 0.30, thus we must check for distribution of data","15abad06":"------\n# 1. Class Distribution","2a46d6ce":"## 8.1. Logistic Regression with the undersampled dataset","428f58b9":"------\n# 3. EDA\n\n### 3.1 Box Plot","4378e5eb":"- All the principal transformed columns have approx 0 mean","bfa87382":"**Distribution of Amount variable based on Fraud and Normal Transaction**\n\n**1.1. Bar Chart**","b4ce7b72":"Features that are important for XGBoost algorithm are **V14, V4, V17, V12, V26, V7**","42649211":"## 7.1 Logistic Regression with the oversample dataset","eb5c19b1":"-------\n## 5. Analysis of PCA Transformed variable","47f9432a":"- **Here no such column are highly corelated to each other as they are already principal transformed**","61fa224b":"By looking at the above box plot we can say that both fraud & genuine transactions occur throughout time and there is no distinction between them.","4dd2dd97":"**Training the Random Forest Model having parameter with best score**","b4430ba3":"**Feature Importance**","a5bc6839":"## 7.2 Random Forest with the oversample dataset","c61b5d12":"------\n# 8. Under Sampling","5718bef0":"---","6df05634":"Only 0.172% fraudulent transaction out all the transactions. The data is highly Unbalanced","7e6f7f2b":"**Minimum Number of sample in leaf**","e1703194":"----\n# 10. Observations\n\n\n**10.1. Using Raw Data**\n\n- Logistic Regression - Lesser Recall Rate as we are left with 28 fraud transaction unidentified.\n    - Recall = 0.71\n    - AUC-ROC = 0.90\n        - Confusion Matrix : [[56855,    9],\n                          [   28,    70]]\n                          \n                          \n- Random Forest - Recall Rate increases as compared to Logistic Regression and AUC-ROC remains same, still lookinf forward for more enchancement in results\n    - Recall = 0.81\n    - AUC-ROC = 0.90\n        - Confusion Matrix : [[56850,    14],\n                          [   18,    80]]\n    \n    \n- XG Boost - Its the best performing algorithm compared to above two having larger Recall and AUC-Roc\n    - Recall = 0.86\n    - AUC-ROC = 0.99\n        - Confusion Matrix : [[56857,    7],\n                       [   13,    85]]\n\n\n**Comment - Try to oversampling the data for better performance of algorithm**\n\n\n**10.2. Over Sampling**\n\n- Logistic Regression - Recall rate has improved after oversampling the data but at the same time Flase Positive number has also increased\n    - Recall  = 0.91\n    - Auc-ROC = 0.94\n        - Confusion Matrix :[[55530,  1334]\n                       [8,    90]]\n     \n    \n- Random Forest - Recall rate has decreased but still more than model trained without performing oversampling on data but at the same time Flase Positive number has also decreased. Still try to reduce the False Negative from 11 to much lesser\n    - Recall  = 0.88\n    - Auc-ROC = 0.94 \n    - Confusion Matrix : [[56815,    49]\n                            [11,    87]]\n\n\n**Comment - Try to undersample the data to analyse the performance of same algorithm**\n\n\n**10.3. Under Sampling**\n\n- Logistic Regression - Recall rate remains the same as compared to oversampled data and AUC-ROC score also\n    - Recall  = 0.91\n    - Auc-ROC = 0.94\n        - Confusion Matrix : [[55606,  1258]\n                           [8,    90]]\n    \n- Random Forest - Recall rate remains the same as compared to oversampled data and AUC-ROC score also\n    - Recall  = 0.88\n    - Auc-ROC = 0.943\n        - Confusion Matrix : [[56815,    49]\n                          [11,    87]]\n                          \n**Comment - Will not prefer UnderSampling because there is loss of data and information in this process**\n\n    \n**10.4. SMOTE**\n\n- Logistic Regression - Recall rate remains the same as compared to oversampled or undersampled data and AUC-ROC score also\n    - Recall  = 0.91\n    - Auc-ROC = 0.94\n        - Confusion Matrix : [[55492,  1372]\n                          [ 8,    90]]\n    \n- Random Forest - Recall rate remains the same as compared to oversampled or undersampled data but  AUC-ROC score increases slightly\n    - Recall  = 0.91\n    - Auc-ROC = 0.95\n        - Confusion Matrix : [[56695,  169]\n                           [9,    89]]\n    \n- XGBoost - Recall rate remains the same as compared to oversampled or undersampled data but  AUC-ROC score increases by a significant amount, as we see only 44 non-fradualent transactions are been predicted to be fraud\n    - Recall  = 0.91\n    - Auc-ROC = 0.98\n        - Confusion Matrix : [[56820,  44]\n                           [9,    89]]","270dfd74":"**Optimal depth of tree to be tuned is 2-16**","f23601d2":"**Maximum number of features to split**","cb0f2bd2":"### 6.1.1 Best Logistic Regression Model\n\n","0015d690":"**1.2. Scatter Chart**","4afe7c76":"- Descriptive statisticss of the PCA variables ","caefac33":"## 8.2. Random Forest with the undersample dataset","468e47e3":"-----\n# 9. SMOTE","03d9a63e":"## 6.2 Random Forest","83881ec1":"-----\n# 2. Train \/ Test Split","5b287fe2":"**Number of samples in leaf nodes lies less than 150**","27eeb9ac":"**Defining threshold as 0.6**","46ae2aec":"## 9.1. Logistic Regression with the undersampled dataset","a3fe9539":"**Optimal number of sample splits for the algorithm are between 200-300**","60c46770":"- Few of the principal transformed variables are significantly skewed. Ploting the histogram of one of the particularly skewed variables, V8, to see the distribution in detail.","024405e5":"## 6.3 XG Bosot Classifier","48b321b7":"**Minimum Sample Split**","d9367233":"**Defining threshold as 0.55**\n\n**Classification Report**"}}