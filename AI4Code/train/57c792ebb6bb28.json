{"cell_type":{"d088af38":"code","1bcfda7a":"code","85cc5683":"code","88ad720f":"code","dc9cb440":"code","b6abed40":"code","2c03234f":"code","a894ecdd":"code","50510336":"code","4da321b7":"code","e9150805":"code","0dfb5314":"code","d2ec4a2e":"code","0a2af894":"code","bd2ca649":"code","d238f417":"code","1b4e5335":"code","3d7e5d6b":"code","11f91b09":"code","0937740c":"code","24b224b7":"code","24b37093":"code","063591cb":"code","6448d7e2":"code","a170bc61":"code","1212c253":"code","924aaa99":"code","7dc1f08c":"code","a7ac2e32":"code","ae1698ff":"code","3e9b6c60":"code","1199d1b5":"code","2b9c3597":"code","2992c9c2":"code","9f51acc8":"code","dfec78d5":"code","33dbfd00":"code","6dcd9469":"code","af0a516d":"code","b489f742":"code","40509b33":"code","83e9a8a3":"code","44accee3":"code","381a7abe":"code","3b31b4d5":"code","cd00de3e":"code","21938292":"markdown","4fb43796":"markdown","096ee2fa":"markdown","28717ea2":"markdown","6d3a4155":"markdown","72c6ddcd":"markdown","fe3b1804":"markdown","68303bef":"markdown","96635805":"markdown","a95f9511":"markdown","67df6b06":"markdown","804ec2b2":"markdown","3db3f033":"markdown","1f2259b7":"markdown","304ad039":"markdown","8ace4720":"markdown","51075fe4":"markdown","49344372":"markdown","786c76e8":"markdown","da8c107d":"markdown","c65388d5":"markdown","de31e520":"markdown","cd563a32":"markdown","fa35d3c0":"markdown","7bd4e068":"markdown","ae40b4fb":"markdown","c6931495":"markdown","143064ef":"markdown","f256d51a":"markdown","6dab27d5":"markdown","8ad07666":"markdown","caca68de":"markdown","02cedf50":"markdown","3c944638":"markdown","d29ce06d":"markdown","b2c9290e":"markdown","0523998f":"markdown","0670656d":"markdown","3b4c256b":"markdown","98ad46e2":"markdown","5d7a8efa":"markdown","66cbd706":"markdown","4e2a0f41":"markdown","58762be9":"markdown"},"source":{"d088af38":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom catboost import CatBoostClassifier, Pool, cv\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, f1_score, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","1bcfda7a":"df = pd.read_csv('..\/input\/bank-customer-churn-modeling\/Churn_Modelling.csv')\ndf.head()","85cc5683":"df.info()","88ad720f":"df1 = df.drop(['RowNumber','Surname', 'CustomerId'],axis=1)","dc9cb440":"df1.columns = ['credit_score', 'geography', 'gender', 'age', 'tenure',\n       'balance', 'num_of_products', 'has_cr_card', 'is_active_member',\n       'estimated_salary', 'exited']","b6abed40":"df1.duplicated().sum()","2c03234f":"df1['exited'].value_counts(normalize=True)","a894ecdd":"cat_data = ['geography','gender', 'num_of_products', 'has_cr_card', 'is_active_member']\nnum_data = ['credit_score','age','estimated_salary','balance','tenure']","50510336":"df1 = pd.get_dummies(data=df1,columns=cat_data, drop_first=True)","4da321b7":"df_train, df_temp = train_test_split(df1, test_size=0.4, random_state=101)\ndf_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=101)","e9150805":"display(df_train.shape)\ndisplay(df_valid.shape)\ndisplay(df_test.shape)","0dfb5314":"features_train = df_train.drop(['exited'], axis=1)\ntarget_train = df_train['exited']","d2ec4a2e":"features_valid = df_valid.drop(['exited'], axis=1)\ntarget_valid = df_valid['exited']","0a2af894":"features_test = df_test.drop(['exited'], axis=1)\ntarget_test = df_test['exited']","bd2ca649":"scaler = StandardScaler()\nscaler.fit(features_train[num_data]) \nfeatures_train[num_data] = scaler.transform(features_train[num_data])\nfeatures_valid[num_data] = scaler.transform(features_valid[num_data])\nfeatures_test[num_data] = scaler.transform(features_test[num_data])","d238f417":"tuned_parameters = {\"n_estimators\": list(range(60,121,20)), \n                    \"max_depth\": list(range(5,13,1)), \n                    \"criterion\": ['gini', 'entropy']}\n\nrand_forest = GridSearchCV(RandomForestClassifier(random_state=101), \n                   tuned_parameters, scoring='f1', cv=5)\nrand_forest.fit(features_train, target_train)\nprint(\"Best parameters set found on development set:\")\nprint(rand_forest.best_params_)\nprint()\nprint(\"Grid scores on development set:\")\nmeans = rand_forest.cv_results_[\"mean_test_score\"]\nstds = rand_forest.cv_results_[\"std_test_score\"]\nfor mean, std, params in zip(means, stds, rand_forest.cv_results_[\"params\"]):\n    print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))\nprint()\n\nprint(\"Detailed classification report:\")\ntarget_pred = rand_forest.predict(features_valid)\nprint(classification_report(target_valid, target_pred))","1b4e5335":"rand_forest_report = classification_report(target_valid, target_pred, output_dict=True)\nresult_df = pd.DataFrame(columns=['name_ml', 'f1_valid_NB', 'f1_valid_B', 'f1_test_B']) #NB - not balanced, B - balanced\nresult_df.loc[0]=['RandomForest', round(rand_forest_report['1']['f1-score'], 2), 0, 0]\nresult_df","3d7e5d6b":"tuned_parameters = {\"C\": np.logspace(-3, 3, num=7), \n                    \"gamma\": np.logspace(-5, 2, num=8)}\n\nsvm = GridSearchCV(SVC(kernel='rbf', random_state=101), \n                    tuned_parameters, cv=5)\nsvm.fit(features_train, target_train)\nprint(\"Best parameters set found on development set:\")\nprint(svm.best_params_)\nprint()\nprint(\"Grid scores on development set:\")\nmeans = svm.cv_results_[\"mean_test_score\"]\nstds = svm.cv_results_[\"std_test_score\"]\nfor mean, std, params in zip(means, stds, svm.cv_results_[\"params\"]):\n    print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))\nprint()\n\nprint(\"Detailed classification report:\")\ntarget_pred = svm.predict(features_valid)\nprint(classification_report(target_valid, target_pred))","11f91b09":"svm_report = classification_report(target_valid, target_pred, output_dict=True)\nresult_df.loc[1]=['SVM', round(svm_report['1']['f1-score'], 2), 0, 0]\nresult_df","0937740c":"cat_data = ['geography_Germany', 'geography_Spain', 'gender_Male',\n       'num_of_products_2', 'num_of_products_3', 'num_of_products_4',\n       'has_cr_card_1', 'is_active_member_1']","24b224b7":"model_cat = CatBoostClassifier(random_seed=101, iterations=200, learning_rate=0.15, early_stopping_rounds=20)\n\nmodel_cat.fit(features_train, target_train, eval_set=(features_valid, target_valid))\n\nparams = model_cat.get_params()\nparams.update({'loss_function': 'Logloss'})\n\ncv_data = cv(\n    params=params,\n    pool=Pool(features_train, target_train, cat_features=cat_data),\n    fold_count=5, \n    shuffle=True,\n    partition_random_seed=0,\n    stratified=True\n)\n\nprint(\"Detailed classification report:\")\ntarget_pred = model_cat.predict(features_valid)\nprint(classification_report(target_valid, target_pred))","24b37093":"cat_report = classification_report(target_valid, target_pred, output_dict=True)\nresult_df.loc[2]=['CatBoost', round(cat_report['1']['f1-score'], 2), 0, 0]\nresult_df","063591cb":"target_train.value_counts()","6448d7e2":"def updownsample(features, target, up_rate, down_rate):\n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n\n    features_updownsampled = pd.concat([features_zeros.sample(frac=1\/down_rate, \n                                        random_state=101)] + [features_ones] * up_rate)\n    target_updownsampled = pd.concat([target_zeros.sample(frac=1\/down_rate, random_state=101)] \n                             + [target_ones] * up_rate)\n    \n    features_updownsampled, target_updownsampled = shuffle(features_updownsampled, \n                                                    target_updownsampled, random_state=101)\n       \n    return features_updownsampled, target_updownsampled","a170bc61":"features_mixed, target_mixed = updownsample(features_train, target_train, 2, 2)","1212c253":"target_mixed.value_counts()","924aaa99":"rand_forest.best_estimator_.fit(features_mixed, target_mixed)\nprint(\"Detailed classification report:\")\ntarget_pred = rand_forest.predict(features_valid)\nprint(classification_report(target_valid, target_pred))","7dc1f08c":"rand_forest_report = classification_report(target_valid, target_pred, output_dict=True)\nresult_df.loc[0, 'f1_valid_B'] = round(rand_forest_report['1']['f1-score'],2)\nresult_df","a7ac2e32":"svm.best_estimator_.fit(features_mixed, target_mixed)\nprint(\"Detailed classification report:\")\ntarget_pred = svm.predict(features_valid)\nprint(classification_report(target_valid, target_pred))","ae1698ff":"svm_report = classification_report(target_valid, target_pred, output_dict=True)\nresult_df.loc[1, 'f1_valid_B'] = round(svm_report['1']['f1-score'],2)\nresult_df","3e9b6c60":"model_cat.fit(features_mixed, target_mixed, eval_set=(features_valid, target_valid))\nprint(\"Detailed classification report:\")\ntarget_pred = model_cat.predict(features_valid)\nprint(classification_report(target_valid, target_pred))","1199d1b5":"cat_report = classification_report(target_valid, target_pred, output_dict=True)\nresult_df.loc[2, 'f1_valid_B'] = round(cat_report['1']['f1-score'],2)\nresult_df","2b9c3597":"print(\"Detailed classification report:\")\ntarget_pred = rand_forest.predict(features_test)\nprint(classification_report(target_test, target_pred))","2992c9c2":"rand_forest_report = classification_report(target_test, target_pred, output_dict=True)\nresult_df.loc[0, 'f1_test_B'] = round(rand_forest_report['1']['f1-score'],2)\nresult_df","9f51acc8":"probabilities_test = rand_forest.best_estimator_.predict_proba(features_test)\nprobabilities_one_test = probabilities_test[:, 1]\n\nfpr, tpr, thresholds = roc_curve(target_test, probabilities_one_test)\n\nplt.figure()\n\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.plot([0, 1], [0, 1], linestyle='--')\n\nplt.show()\n\nprint()\nauc_roc = roc_auc_score(target_test, probabilities_one_test)\nprint('Square under the ROC curve, AUC-ROC =', auc_roc)","dfec78d5":"print(\"Detailed classification report:\")\ntarget_pred = svm.predict(features_test)\nprint(classification_report(target_test, target_pred))","33dbfd00":"svm_report = classification_report(target_test, target_pred, output_dict=True)\nresult_df.loc[1, 'f1_test_B'] = round(svm_report['1']['f1-score'],2)\nresult_df","6dcd9469":"svm = SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n    max_iter=-1, probability=True, random_state=101, shrinking=True, tol=0.001,\n    verbose=False)","af0a516d":"svm.fit(features_mixed, target_mixed)\nprobabilities_test = svm.predict_proba(features_test)\nprobabilities_one_test = probabilities_test[:, 1]\n\nfpr, tpr, thresholds = roc_curve(target_test, probabilities_one_test)\n\nplt.figure()\n\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.plot([0, 1], [0, 1], linestyle='--')\n\nplt.show()\n\nprint()\nauc_roc = roc_auc_score(target_test, probabilities_one_test)\nprint('Square under the ROC curve, AUC-ROC =', auc_roc)","b489f742":"print(\"Detailed classification report:\")\ntarget_pred = model_cat.predict(features_test)\nprint(classification_report(target_test, target_pred))","40509b33":"cat_report = classification_report(target_test, target_pred, output_dict=True)\nresult_df.loc[2, 'f1_test_B'] = round(cat_report['1']['f1-score'],2)\nresult_df","83e9a8a3":"probabilities_test = model_cat.predict_proba(features_test)\nprobabilities_one_test = probabilities_test[:, 1]\n\nfpr, tpr, thresholds = roc_curve(target_test, probabilities_one_test)\n\nplt.figure()\n\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.plot([0, 1], [0, 1], linestyle='--')\n\nplt.show()\n\nprint()\nauc_roc = roc_auc_score(target_test, probabilities_one_test)\nprint('Square under the ROC curve, AUC-ROC =', auc_roc)","44accee3":"dummy1 = DummyClassifier(strategy='uniform', random_state=101).fit(features_mixed, target_mixed)\nprint(\"Detailed classification report:\")\ntarget_pred = dummy1.predict(features_test)\nprint(classification_report(target_test, target_pred))","381a7abe":"dummy2 = DummyClassifier(strategy='constant', constant=[1],\n                         random_state=101).fit(features_train, target_train)\nprint(\"Detailed classification report:\")\ntarget_pred = dummy2.predict(features_test)\nprint(classification_report(target_test, target_pred))","3b31b4d5":"result_df","cd00de3e":"result_df.plot(kind='bar')\nplt.title('F-1 measure value')\nplt.xlabel('Algoritms')\nplt.ylabel('Value')\nplt.xticks(range(len(result_df['name_ml'])), labels=result_df['name_ml'], rotation=0)\nplt.ylim(0.55,0.65)\nplt.grid()\nplt.show()","21938292":"Additionally, we will build and calculate **AUC-ROC**.","4fb43796":"Scaling quantitative features","096ee2fa":"Good we haven't in our dataset **NA** and haven't duplicating rows. So, we can start to prepare our data for next steps.","28717ea2":"## SVM","6d3a4155":"On the face of the imbalance of classes. According to the conditions of the study, we will make predictions without paying attention to it. But first we will prepare the data for analysis. \n\nFor this:\n* transform categorical features, \n* divide the sample into 3 parts: training (60%), validation (20%) and test (20%),\n* we scale quantitative features.","72c6ddcd":"Check","fe3b1804":"# Fighting imbalance\n\nLet's balance the classes and compare with the previous values of **F-1**. We will manually balance the classes by **upsampling** and **downsampling**, the number of negative answers will be reduced by 2 times and the number of positive answers will be increased by 2 times. Thus, we bring the number of 0 and 1 to an approximately equal number.","68303bef":"## RandomForest\nTo select hyperparameters, use **GridSearchCV**","96635805":"# Conclusion\nBased on historical data provided by the bank, we trained models to predict customer outflow, the **randomForest** algorithm coped best with this task with the result **F-1 = 0.62**, surprisingly, the backup model **CatBoost** on unbalanced data showed the result **F-1 = 0.61**.","a95f9511":"## CatBoost","67df6b06":"As can be seen from the final table of metrics, recall is less than 0.5 for class \"1\". That is, the model weakly predicts this class. Which in turn reduces the value of the final metric **F-1**. Let's see further how it will change for the case with balanced classes.\n\nLet's create a summary table with key metrics and fill it in as the research progresses","804ec2b2":"Great, our models are much better than a random model with equal classes","3db3f033":"As can be seen from the final table of metrics, recall is less than 0.5 for class \"1\". That is, the model weakly predicts this class. Which in turn reduces the value of the final metric **F-1**. Let's see further how it will change for the case of balanced classes. However, the metric value already satisfies the research condition. The result will be entered in the final table","1f2259b7":"Data for train","304ad039":"Data for validation","8ace4720":"## CatBoost\nLet's check the last algorithm on the test set","51075fe4":"## (Bonus) Adequacy test\nAdditionally, we check our models for adequacy\nFirst check for the case of equal classes","49344372":"Additionally, we will build and calculate **AUC-ROC**.\nEnable the **probability** option and retrain the model as it is disabled by default","786c76e8":"After balancing, the value of the metric has not changed","da8c107d":"# First view \n****Check the data****","c65388d5":"And now let's check for the case of dominance of one of the classes. In this case, we will train the model on the original data, and not on balanced mixed tables.","de31e520":"Let's divide the sample into three parts: training (60%), validation (20%) and test (20%)","cd563a32":"## RandomForest","fa35d3c0":"After balancing, the value of the metric improved by 0.02.","7bd4e068":"The value of the metric has deteriorated and, unfortunately, it is less than the required value.","ae40b4fb":"## SVM\nSimilarly, to select the best parameters, we will use **GridSearchCV**","c6931495":"Additionally, we will build and calculate **AUC-ROC**","143064ef":"Now it remains to check how our manual balancing influenced the change in the **F-1** metric","f256d51a":"Let's consider how to deal with unbalanced classes of different machine learning algorithms - **Random Forest, SVM, CatBoost**","6dab27d5":"# Model testing\n## RandomForest","8ad07666":"### CatBoost\nConsider the last algorithm","caca68de":"Since we specified **1** for predictions, then **0** was not filled in and *Warning* is issued","02cedf50":"We transform categorical features","3c944638":"Check for duplicate","d29ce06d":"Excellent result, the model on unfamiliar data improved its metric.","b2c9290e":"## SVM","0523998f":"## Brief conclusion\nThe classes were balanced by changing the quantitative composition. We trained the models on the new balanced data. **F-1** metric values improved for RandomForest and Catboost. Let's check how our models behave on the test set.","0670656d":"It is necessary to predict whether the client will leave the bank in the near future or not. You are presented with historical data on the behavior of customers and the termination of contracts with the bank.\n\nBuild a model with an extremely large value of the F1-measure.\n\nSo, lets begin.","3b4c256b":"## Brief conclusion\nWe checked how the models predict the value with unbalanced classes, and the results were entered in the table. The best so far is **CatBoost**. In the next paragraph, we will make a balance of classes, and see how the value of the **F-1 measure changes**","98ad46e2":"After balancing, the value of the metric improved by 0.01.","5d7a8efa":"# Preparing data\nLet's check how the target feature is distributed, which we will predict using machine learning","66cbd706":"Rename the column names for convenience, and also delete some of the columns **Row Number**, **Surname**, **CustomerID**, which for our study do not carry a semantic load.","4e2a0f41":"Data for test","58762be9":"Check"}}