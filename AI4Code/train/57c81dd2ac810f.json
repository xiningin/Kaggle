{"cell_type":{"7ec3fb79":"code","1650ff04":"code","804a1e1c":"code","8282fc6c":"code","8ed056eb":"code","a42cd485":"code","36335d4c":"code","607fd14b":"code","0a999d95":"code","8edcc8b8":"code","c916fa1c":"code","bb0b4816":"code","0e090ae0":"code","4b82e9da":"code","8880374e":"code","dd98788e":"code","b16a884d":"code","c00d835c":"code","18ccec2c":"code","cd8371fb":"code","5f821627":"code","178389f2":"code","4591bbd4":"code","3d615ada":"code","c25d5cec":"code","df780e49":"code","8b512acc":"code","7c6326c2":"code","addf7192":"code","e0091bda":"code","cd6519e3":"code","608d06d7":"markdown","5c88d00b":"markdown","8ede12fa":"markdown","08d1bc6c":"markdown","7dc35d81":"markdown","98d45e77":"markdown","62aa9c95":"markdown","29a21d4c":"markdown","3dd1db71":"markdown","4be6dbf9":"markdown","d3a380a9":"markdown","cce25eb3":"markdown","a2427fe9":"markdown","d03ec774":"markdown","03ead041":"markdown","aaf20471":"markdown","4f243278":"markdown","dc8cfc0c":"markdown","504a5328":"markdown","aa0f3139":"markdown","7856c445":"markdown","ae36b35f":"markdown","4509b060":"markdown","cd344436":"markdown","b7fcd228":"markdown","307cbd84":"markdown","a08c325c":"markdown"},"source":{"7ec3fb79":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.linear_model import LinearRegression as lr\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score as cvs","1650ff04":"#importing dataset\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndataset = pd.read_csv('\/kaggle\/input\/boston-house-prices\/housing.csv', delimiter=r'\\s+', names=column_names)","804a1e1c":"#Top 5 rows of dataset\ndataset.head()","8282fc6c":"#Shape of dataset (rows, columns)\ndataset.shape","8ed056eb":"#describing the dataste to see distribution of data\ndataset.describe()","a42cd485":"#removing variables 'ZN' and 'CHAS' form data\ndataset = dataset.drop(['ZN', 'CHAS'], axis=1)","36335d4c":"dataset.isnull().sum()","607fd14b":"#Plotting boxplots to see if there are any outliers in our data (considering data betwen 25th and 75th percentile as non outlier)\nfig, ax = plt.subplots(ncols=6, nrows=2, figsize=(15, 5))\nax = ax.flatten()\nindex = 0\nfor i in dataset.columns:\n  sns.boxplot(y=i, data=dataset, ax=ax[index])\n  index +=1\nplt.tight_layout(pad=0.4)\nplt.show()","0a999d95":"#checking percentage\/ amount of outliers\nfor i in dataset.columns:\n  dataset.sort_values(by=i, ascending=True, na_position='last')\n  q1, q3 = np.nanpercentile(dataset[i], [25,75])\n  iqr = q3-q1\n  lower_bound = q1-(1.5*iqr)\n  upper_bound = q3+(1.5*iqr)\n  outlier_data = dataset[i][(dataset[i] < lower_bound) | (dataset[i] > upper_bound)] #creating a series of outlier data\n  perc = (outlier_data.count()\/dataset[i].count())*100\n  print('Outliers in %s is %.2f%% with count %.f' %(i, perc, outlier_data.count()))\n  #----------------------code below is for comming sections----------------------\n  if i == 'B':\n    outlierDataB_index = outlier_data.index\n    outlierDataB_LB = dataset[i][(dataset[i] < lower_bound)]\n    outlierDataB_UB = dataset[i][(dataset[i] > upper_bound)]\n  elif i == 'CRIM':\n    outlierDataCRIM_index = outlier_data.index\n    outlierDataCRIM_LB = dataset[i][(dataset[i] < lower_bound)]\n    outlierDataCRIM_UB = dataset[i][(dataset[i] > upper_bound)]\n  elif i == 'MEDV':\n    lowerBoundMEDV = lower_bound\n    upperBoundMEDV = upper_bound","8edcc8b8":"dataset2 = dataset.copy() # I copied the data in another variable just for an ease of coding, but this is not required","c916fa1c":"#removing extreme outliers form B and CRIM (removing those observations)\nremoved=[]\noutlierDataB_LB.sort_values(ascending=True, inplace=True)\noutlierDataB_UB.sort_values(ascending=False, inplace=True)\ncounter=1\nfor i in outlierDataB_LB.index:\n  if counter<=19:\n    dataset2.drop(index=i, inplace=True)\n    counter+=1\n    removed.append(i)\nfor i in outlierDataB_UB.index:\n  if counter<=38:\n    dataset2.drop(index=i, inplace=True)\n    counter+=1\n    removed.append(i)\nfor i in outlierDataB_LB.index:\n  if counter<=38 and i not in removed:\n    dataset2.drop(index=i, inplace=True)\n    counter+=1\n    removed.append(i)\n\n\noutlierDataCRIM_LB.sort_values(ascending=True, inplace=True)\noutlierDataCRIM_UB.sort_values(ascending=False, inplace=True)\ncounter=1\nfor i in outlierDataCRIM_LB.index:\n  if counter<=16 and i not in removed:\n    dataset2.drop(index=i, inplace=True)\n    counter+=1\n    removed.append(i)\nfor i in outlierDataCRIM_UB.index:\n  if counter<=33 and i not in removed:\n    dataset2.drop(index=i, inplace=True)\n    counter+=1\n    removed.append(i)\nfor i in outlierDataCRIM_LB.index:\n  if counter<=33 and i not in removed:\n    dataset2.drop(index=i, inplace=True)\n    counter+=1\n    removed.append(i)\n\ndataset2.shape","bb0b4816":"dataset3 = dataset2.copy() # I copied the data in another variable just for an ease of coding, but this is not required","0e090ae0":"#replacing remaning outliers by mean\nfor i in dataset.columns:\n  dataset.sort_values(by=i, ascending=True, na_position='last')\n  q1, q3 = np.nanpercentile(dataset[i], [25,75])\n  iqr = q3-q1\n  lower_bound = q1-(1.5*iqr)\n  upper_bound = q3+(1.5*iqr)\n  mean = dataset3[i].mean()\n  if i != 'MEDV':\n    dataset3.loc[dataset3[i] < lower_bound, [i]] = mean\n    dataset3.loc[dataset3[i] > upper_bound, [i]] = mean\n  else:\n    dataset3.loc[dataset3[i] < lower_bound, [i]] = mean\n    dataset3.loc[dataset3[i] > upper_bound, [i]] = 50","4b82e9da":"dataset3.describe()","8880374e":"#independent variable(X) and dependent variable(Y)\nX = dataset3.iloc[:, :-1]\nY = dataset3.iloc[:, 11]","dd98788e":"#Feature selection using P-Value\/ Backward elimination\ndef BackwardElimination(sl, w):\n    for i in range(0, len(w.columns)):\n        regressor_OLS = sm.OLS(endog=Y, exog=w).fit()\n        max_pvalue = max(regressor_OLS.pvalues)\n        pvalues = regressor_OLS.pvalues\n        if max_pvalue > SL:\n            index_max_pvalue = pvalues[pvalues==max_pvalue].index\n            w = w.drop(index_max_pvalue, axis = 1) #delete the valriable for that p value\n    return w,pvalues,index_max_pvalue\n\nSL = 0.05\nones = np.ones((435,1))  #adding a columns of ones to X as it is required by statsmodels library\nW = X\nW.insert(0, 'Constant', ones, True)\nW_optimal = W.iloc[:, [0,1,2,3,4,5,6,7,8,9,10,11]]\n\nW_optimal,pvalues,index_max_pvalue = BackwardElimination(SL, W_optimal)\nX = W_optimal.drop('Constant', axis=1)","b16a884d":"#remaning variabls after backward elimination\nX.columns","c00d835c":"#Ploting heatmap using pearson correlation among independent variables\nplt.figure(figsize=(8, 8))\nax = sns.heatmap(X.corr(method='pearson').abs(), annot=True, square=True)\nplt.show()","18ccec2c":"#dropping TAX and NOX\nX.drop('TAX', axis=1, inplace=True)\nX.drop('NOX', axis=1, inplace=True)\n\n#remaning columns after removing multicollinearity\nX.columns","cd8371fb":"#now checking correlation of each variable with MEDV by pearson method and dropping the one with least correlation with MEDV\nfor i in X.columns:\n  corr, _ = pearsonr(X[i], Y)\n  print(i,corr)","5f821627":"X.drop(['DIS', 'RAD'], axis=1, inplace=True)","178389f2":"#remaning variables\/ features that can predict the MEDV most\nX.columns","4591bbd4":"#spliting data into traning set and test set\nX_train, X_test, Y_train, Y_test = tts(X, Y, test_size=0.2, random_state=0)","3d615ada":"linear = lr()\nlinear.fit(X_train, Y_train)\nY_pred = linear.predict(X_test)\nY_compare_linear = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nY_compare_linear.head() #displaying the comparision btween actual and predicted values of MEDV","c25d5cec":"polyRegressor = PolynomialFeatures(degree=3)\nX_train_poly = polyRegressor.fit_transform(X_train)\nX_test_poly = polyRegressor.fit_transform(X_test)\npoly = lr()\npoly.fit(X_train_poly, Y_train)\nY_pred = poly.predict(X_test_poly)\nY_compare_poly = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nY_compare_poly.head() #displaying the comparision btween actual and predicted values of MEDV","df780e49":"svr = SVR(kernel= 'poly', gamma='scale')\nsvr.fit(X_train,Y_train)\nY_pred = svr.predict(X_test)\nY_compare_svr = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nY_compare_svr.head() #displaying the comparision btween actual and predicted values of MEDV","8b512acc":"rf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train,Y_train)\nY_pred = rf.predict(X_test)\nY_compare_randomforrest = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nY_compare_randomforrest.head() #displaying the comparision btween actual and predicted values of MEDV","7c6326c2":"knn = KNeighborsRegressor(n_neighbors=13)\nknn.fit(X_train,Y_train)\nY_pred = knn.predict(X_test)\nY_compare_knn = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nY_compare_knn.head() #displaying the comparision btween actual and predicted values of MEDV","addf7192":"fig, ax = plt.subplots(ncols=5, nrows=1, figsize=(25, 4))\nax = ax.flatten()\nY_compare_linear.head(10).plot(kind='bar', title='Linear Regression', grid=True, ax=ax[0])\nY_compare_poly.head(10).plot(kind='bar', title='Polynomial Regression', grid=True, ax=ax[1])\nY_compare_svr.head(10).plot(kind='bar', title='Support Vector Regression', grid=True, ax=ax[2])\nY_compare_randomforrest.head(10).plot(kind='bar', title='Random Forrest Regression', grid=True, ax=ax[3])\nY_compare_knn.head(10).plot(kind='bar', title='KNN Regression', grid=True, ax=ax[4])\nplt.show()","e0091bda":"print('According to R squared scorring method we got below scores for out machine learning models:')\nmodelNames = ['Linear', 'Polynomial', 'Support Vector', 'Random Forrest', 'K-Nearest Neighbour']\nmodelRegressors = [linear, poly, svr, rf, knn]\nmodels = pd.DataFrame({'modelNames' : modelNames, 'modelRegressors' : modelRegressors})\ncounter=0\nscore=[]\nfor i in models['modelRegressors']:\n  if i is poly:\n    accuracy = cvs(i, X_train_poly, Y_train, scoring='r2', cv=5)\n    print('Accuracy of %s Regression model is %.2f' %(models.iloc[counter,0],accuracy.mean()))\n    score.append(accuracy.mean())\n  else:\n    accuracy = cvs(i, X_train, Y_train, scoring='r2', cv=5)\n    print('Accuracy of %s Regression model is %.2f' %(models.iloc[counter,0],accuracy.mean()))\n    score.append(accuracy.mean())\n  counter+=1","cd6519e3":"pd.DataFrame({'Model Name' : modelNames,'Score' : score}).sort_values(by='Score', ascending=True).plot(x=0, y=1, kind='bar', figsize=(15,5), title='Comparison of R2 scores of differnt models', )\nplt.show()","608d06d7":"# **Final summary**\n\n**From above data engineering and machine learning techniques we can conclude that:**\n\n1.   Features RM, PTRATIO and LSAT are alone capable of predicting MEDV to a good accuracy\n2.   Random Forrest regression model (with 100 estimators) can be considered as a good model for predictiong MEDV using the above mentioned three features.\n3.   However Polynomial and KNN regression models can also be used as an alternative to Random Forrest.\n4.   Linear and Support Vector regression models shows the least r2 score, which can be considered bad models for predicting MEDV.\n\nI would like to close it by mentioning an important fact, that no Data Science technique is perfect and there is always scope for imporvement.","5c88d00b":"**Linear regression model:**","8ede12fa":"**K-Nearest Neighbour regression model:**","08d1bc6c":"# **Checking null values**","7dc35d81":"**Support vector regression model:**","98d45e77":"From the above feature selection process we conclude that features *RM*, *PTRATIO* and *LSAT* can alone predict MEDV the best","62aa9c95":"Variable 'CRIM' and 'B' have high percentage of outlier data which can adversely affect the accuracy of our model.\n\nTo get rid of this we can either drop the observations or replace with some apporach like mean or median. But dropping all the outlier observations is not a good idea as we will be left with very fewer observations due to higher percentage of outliers to train our model on, also if we replace such a big percentage of the outliers with some approach (mean, median...etc.) then it might result into less accurate or biased model.\n\nWe can use an alternative : let's drop the extreme outliers and replace the remaning by some approach (mean, median.....etc.)","29a21d4c":"**Scores (R squared) of different machine learning models using K-fold cross validation:**","3dd1db71":"From the above visualiation we can summarise that Random Forrest (r2 = 0.72) machine learning model gives the best score and we can use it to predict the values of MEDV the best.\n\nHowever other models like Polynomial (r2 = 0.64) regression model and KNN (r2 = 0.64)regression model also have comparable score to Random Forrest and hence can also be used to make predictions of MEDV.","4be6dbf9":"**Polynomial regression model:**","d3a380a9":"**Using pearson correlation to remove any highly correlated independent variables to avoid multicollinearity :**","cce25eb3":"# **About the dataset:**\n\nThis dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass.\n\nThere are 14 attributes in each case of the dataset. They are:\n\n* CRIM - per capita crime rate by town\n* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS - proportion of non-retail business acres per town.\n* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n* NOX - nitric oxides concentration (parts per 10 million)\n* RM - average number of rooms per dwelling\n* AGE - proportion of owner-occupied units built prior to 1940\n* DIS - weighted distances to five Boston employment centres\n* RAD - index of accessibility to radial highways\n* TAX - full-value property-tax rate per \\$10,000\n* PTRATIO - pupil-teacher ratio by town\n* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n* LSTAT - % lower status of the population\n* MEDV - Median value of owner-occupied homes in $1000's\n\nVariable #14 seems to be censored at 50.00 (corresponding to a median price of \\$50,000); Censoring is suggested by the fact that the highest median price of exactly \\$50,000 is reported in 16 cases, while 15 cases have prices between \\$40,000 and $50,000, with prices rounded to the nearest hundred.\n\nOur goal is to select the valiables which predicts the MEDV best, also to suggest a machine learning model to predict MEDV.","a2427fe9":"There are no null values in our dataset","d03ec774":"**Checking correlation of remaning independent variables with MEDV using Pearson correlation method**","03ead041":"Columns CRIM, RM, DIS, PTRATIO, B, LSTAT and MEDV have outliers.","aaf20471":"**Decission tree regression model:**","4f243278":"From above correlation heatmap we can see that:\n1.   TAX and RAD are highly correlated with score 0.86. As per my personal understandig RAD (index of accessibility to radial highways) will be more important in predicting MEDV as commpared to TAX (full-value property-tax rate per $10,000), so I am considering to drop TAX\n2.   DIS and NOX are highly correlated with score 0.75. As per my personal understandig DIS (weighted distances to five Boston employment centres) will be more important in predicting MEDV as commpared to NOX (nitric oxides concentration (parts per 10 million)), so I am considering to drop NOX\n\n","dc8cfc0c":"Below is the description of our dataset after treating the outliers:","504a5328":"From the above distribution we can see that:\n1.   Variable 'ZN' is 0 for 25th and 50th percentile that will result in skweed data. This is a result of 'ZN' being a conditional variable.\n2.   Also for variable 'CHAS' it's 0 for 25th, 50th and 75th percentile that will also show us that data is highly skweed. This is a result of 'CHAS' being a categorical data, contaning vaules 0 and 1 only.\n\nAnother important fact we can derive form above description is tha max value of 'MEDV' which is 50, goes along with the original data description which says : Variable #14 seems to be censored at 50.00 (corresponding to a median price of $50,000)\n\nFor a start we can derive an asumption that 'ZN' and 'CHAS' variables may not be useful in predicting MEDV as they will result in biased model, so let's remove them.","aa0f3139":"# **Checking and treating outliers in the data**","7856c445":"**Using p-Value to to select the optimal features:**\n\nDropping all the variables whose p-value is less than significance level of 0.05 using backward elimination method","ae36b35f":"**Plotting compariasion of actual and predicted values of MEDV that we got using different machine learning models**","4509b060":"# **Machine learning**\n\nThis is a regression a problem as we have to predict a continous (non catagorical) value.\n\nImplementing regression machine learning models to out our dataset (using the remaing independent variables) to predict MEDV","cd344436":"**Please comment your suggestions**\n\n**Please upvote if this notebook is helpful**","b7fcd228":"# **Selecting the features which can predict MEDV the best**","307cbd84":"We can see that DIS and RAD are least correlated to MEDV, so dropping DIS and RAD","a08c325c":"We have dropped 71 observations from our dataset, now we are left with 435 observations and 12 columns.\n\n**Now replacing the remaning outliers with mean of each variable.**"}}