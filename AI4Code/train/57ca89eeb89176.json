{"cell_type":{"f33c4ae9":"code","147a4791":"code","6e995043":"code","92281399":"code","07713293":"code","430c9b03":"code","ce2c8d40":"code","1be33851":"code","39e2887d":"code","f1b15903":"code","03bff49e":"code","5dea46c2":"code","34e3776e":"code","7083b436":"code","a2fb9168":"code","8a79136e":"code","058223a3":"code","ad1f775a":"code","009ceb6e":"code","56eea2e5":"code","c0968842":"code","e65de237":"code","ef1d32f3":"code","1e605f25":"code","e16b5127":"code","a790b202":"code","37982aa7":"code","d42ce25b":"code","de0aa158":"code","9035437a":"code","59139333":"code","1a8a7ddf":"code","bed25b25":"code","b25255c1":"code","d1f4046f":"code","315329a1":"code","034539c8":"code","2bcccbf0":"code","04b23835":"code","82c4754f":"code","f4b9aff1":"code","913c2ee6":"code","2ad46d64":"code","b04c1cd5":"code","0044e945":"code","e99267f4":"code","ed4d5145":"code","67088d55":"code","78a30f34":"code","5de14b7f":"code","d9507ed6":"code","5b89c91b":"markdown","68fbec30":"markdown","e37c1bd7":"markdown","3820f79b":"markdown","19c6a9a1":"markdown","cd3b9656":"markdown","26345d3b":"markdown","8f25acac":"markdown","53ecb243":"markdown","b3ce170d":"markdown","fea9f398":"markdown","0ba12d10":"markdown","8aa5fe48":"markdown","bd0e11b7":"markdown","f5fdc2ff":"markdown","a9afaf30":"markdown","85f9dc82":"markdown","e4fda8bd":"markdown","0cbe2eaf":"markdown"},"source":{"f33c4ae9":"from fastai.vision.all import *\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\nimport matplotlib.image as immg\nfrom joblib import Parallel, delayed\nimport PIL,cv2,gc,os,sys,torch","147a4791":"Path('\/root\/.cache\/torch\/hub\/checkpoints\/').mkdir(exist_ok=True, parents=True)\n!cp '..\/input\/resnet34\/resnet34.pth' '\/root\/.cache\/torch\/hub\/checkpoints\/resnet34-b627a593.pth' \n!cp \"..\/input\/resnet50\/resnet50.pth\" '\/root\/.cache\/torch\/hub\/checkpoints\/resnet50-0676ba61.pth'\n!cp '..\/input\/resnet18\/resnet18.pth' '\/root\/.cache\/torch\/hub\/checkpoints\/resnet18-f37072fd.pth' ","6e995043":"path = Path('..\/input\/sartoriuscellinstancesegmentationmaskpng')","92281399":"def label_func(fn): return f\"\/kaggle\/input\/sartoriuscellinstancesegmentationmaskpng\/TrainMask2x2\/{fn.stem}_mask.png\"","07713293":"img_files = get_image_files(path\/'TrainImage2x2')","430c9b03":"img_files_clean = [] \nfor f in tqdm(img_files):\n    loc = label_func(f)\n    img = np.unique(np.array(Image.open(loc)))\n    if len(img)!=1:\n        img_files_clean.append(f)","ce2c8d40":"len(img_files),len(img_files_clean)","1be33851":"img_files = img_files_clean ","39e2887d":"n = np.random.randint(0,100)\nimg = PIL.Image.open(img_files[n])\nmask = PIL.Image.open(label_func(img_files[n]))","f1b15903":"plt.figure(1,figsize=(18,8))\nplt.subplot(121)\nplt.imshow(img)\nplt.title('raw image')\nplt.subplot(122)\nplt.imshow(img)\nplt.imshow(mask,alpha=0.5);\nplt.title('image + mask');","03bff49e":"img_path = Path('..\/input\/sartoriuscellinstancesegmentationmaskpng\/TrainImage2x2')","5dea46c2":"def get_classes(fnames):\n    class_codes=[]\n    for i in tqdm(range(len(fnames))):\n        class_codes += list(np.unique(np.asarray(Image.open(label_func(fnames[i])))))\n    return np.array(list(set(class_codes)))","34e3776e":"codes = get_classes(img_files);codes","7083b436":"mask.shape","a2fb9168":"img_files_big = get_image_files('..\/input\/sartorius-cell-instance-segmentation\/train')","8a79136e":"def label_func2(fn): \n    fn = Path(fn)\n    img = np.array(Image.open(f\"..\/input\/sartoriuscellinstancesegmentationmaskpng\/TrainMask2x2\/{fn.stem}_mask.png\"))\n    img = img.clip(0,1)\n    return img","058223a3":"def label_func_big(fn): \n    fn = Path(fn)\n    img = np.array(Image.open(f\"..\/input\/cell-train-mask-big\/{fn.stem}.png\"))\n    img = img.clip(0,1)\n    return img","ad1f775a":"dls = SegmentationDataLoaders.from_label_func(img_path, bs=12, \n                                                    fnames = img_files,\n                                                    label_func = label_func2, \n                                                    codes = [0,1])","009ceb6e":"dls.show_batch(max_n=8,figsize=(20,8))","56eea2e5":"len(dls.train_ds),len(dls.valid_ds)","c0968842":"name2id = {v:k for k,v in enumerate(codes)}\nvoid_code = -1\n\ndef cell_mask_accuracy(input, target):\n    target = target.squeeze(1)\n    mask = target != void_code\n    return (input.argmax(dim=1)[mask]==target[mask]).float().mean()","e65de237":"acc = cell_mask_accuracy","ef1d32f3":"# https:\/\/forums.fast.ai\/t\/multi-class-semantic-segmentation-metrics-and-accuracy\/74665\/4\n# Return Jaccard index, or Intersection over Union (IoU) value\ndef IoU(preds:Tensor, targs:Tensor, eps:float=1e-8):\n    \"\"\"Computes the Jaccard loss, a.k.a the IoU loss.\n    Notes: [Batch size,Num classes,Height,Width]\n    Args:\n        targs: a tensor of shape [B, H, W] or [B, 1, H, W].\n        preds: a tensor of shape [B, C, H, W]. Corresponds to\n            the raw output or logits of the model. (prediction)\n        eps: added to the denominator for numerical stability.\n    Returns:\n        iou: the average class intersection over union value \n             for multi-class image segmentation\n    \"\"\"\n    num_classes = preds.shape[1]\n    \n    # Single class segmentation?\n    if num_classes == 1:\n        true_1_hot = torch.eye(num_classes + 1)[targs.squeeze(1)]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n        pos_prob = torch.sigmoid(preds)\n        neg_prob = 1 - pos_prob\n        probas = torch.cat([pos_prob, neg_prob], dim=1)\n        \n    # Multi-class segmentation\n    else:\n        # Convert target to one-hot encoding\n        # true_1_hot = torch.eye(num_classes)[torch.squeeze(targs,1)]\n        true_1_hot = torch.eye(num_classes)[targs.squeeze(1)]\n        \n        # Permute [B,H,W,C] to [B,C,H,W]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        \n        # Take softmax along class dimension; all class probs add to 1 (per pixel)\n        probas = F.softmax(preds, dim=1)\n        \n    true_1_hot = true_1_hot.type(preds.type())\n    \n    # Sum probabilities by class and across batch images\n    dims = (0,) + tuple(range(2, targs.ndimension()))\n    intersection = torch.sum(probas * true_1_hot, dims) # [class0,class1,class2,...]\n    cardinality = torch.sum(probas + true_1_hot, dims)  # [class0,class1,class2,...]\n    union = cardinality - intersection\n    iou = (intersection \/ (union + eps)).mean()   # find mean of class IoU values\n    return iou","1e605f25":"learn = unet_learner(dls, resnet34,  model_dir='\/kaggle\/working\/',metrics=[acc,Dice(),IoU]).to_fp16()","e16b5127":"learn.lr_find()","a790b202":"gc.collect()","37982aa7":"cb1 = SaveModelCallback(monitor='IoU',fname='best_model',comp=np.greater) # Callbacks\ncb2 = ReduceLROnPlateau(monitor='IoU', patience=1,factor=0.2)\nlearn.fit_one_cycle(2, 1e-3,cbs = [cb1,cb2])","d42ce25b":"dls_big = SegmentationDataLoaders.from_label_func(img_path, bs=4, \n                                              fnames = img_files_big,\n                                              label_func = label_func_big, \n                                              codes = [0,1])","de0aa158":"learn.dls = dls_big","9035437a":"cb1 = SaveModelCallback(monitor='IoU',fname='best_model_big',comp=np.greater) # Callbacks\ncb2 = ReduceLROnPlateau(monitor='IoU', patience=1,factor=0.2)\nlearn.fit_one_cycle(1, 1e-5, cbs = [cb1,cb2])","59139333":"learn.load('\/kaggle\/working\/best_model_big');","1a8a7ddf":"try:\n    learn.export('\/kaggle\/working\/export.pkl')\nexcept:\n    pass","bed25b25":"learn.show_results(max_n = 8, figsize = (10,16) )","b25255c1":"interp = SegmentationInterpretation.from_learner(learn)\ninterp.plot_top_losses(k=3)","d1f4046f":"submission = pd.read_csv('..\/input\/sartorius-cell-instance-segmentation\/sample_submission.csv')\nsubmission.head()","315329a1":"test_data_path = submission['id'].apply(lambda x:f'..\/input\/sartorius-cell-instance-segmentation\/test\/{x}.png').tolist()","034539c8":"tst_dl = learn.dls.test_dl(test_data_path)\npreds = learn.get_preds(dl = tst_dl)[0]","2bcccbf0":"prediction_masks = [x.argmax(axis=0) for x in preds]","04b23835":"im_num = 2\nts_img = PIL.Image.open(test_data_path[im_num])\nts_mask = prediction_masks[im_num]","82c4754f":"plt.figure(1,figsize=(18,8))\nplt.subplot(121)\nplt.imshow(ts_img)\nplt.title('Test Image')\nplt.subplot(122)\nplt.imshow(ts_img)\nplt.imshow(ts_mask,alpha=0.5);\nplt.title('Test Image + Predicted Mask');","f4b9aff1":"def CCL(img_arr):\n    img = img_arr\n    # Converting those pixels with values 1-127 to 0 and others to 1\n    #img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)[1]\n    # Applying cv2.connectedComponents() \n    num_labels, labels = cv2.connectedComponents(img)\n    # Map component labels to hue val, 0-179 is the hue range in OpenCV\n    label_hue = np.uint8(179*labels\/np.max(labels))\n    blank_ch = 255*np.ones_like(label_hue)\n    labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])\n    ret_lbl = labeled_img.copy()\n    return ret_lbl[:,:,0]","913c2ee6":"is_mask = np.expand_dims(prediction_masks[im_num].numpy(),axis=-1).astype(np.uint8)\nis_img = CCL(is_mask)","2ad46d64":"plt.figure(1,figsize=(18,8))\nplt.subplot(131)\nplt.imshow(ts_img)\nplt.title('Test Image')\nplt.subplot(132)\nplt.imshow(is_img)\nplt.title('Instance Converted mask')\nplt.subplot(133)\nplt.imshow(ts_img)\nplt.imshow(is_mask,alpha=0.5);\nplt.title('Test Image upon Instance Converted Mask');","b04c1cd5":"# From https:\/\/www.kaggle.com\/stainsby\/fast-tested-rle\ndef rle_decode(mask_rle, shape=(520, 704)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction\n\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","0044e945":"def convert_seg_ins(img_f):\n    lbl_img1 = img_f.copy()\n    grps = list(np.unique(lbl_img1))\n    grps.remove(0)\n    all_masks = []\n    shape = (520,704)\n    for g in grps:\n        a = np.where(((lbl_img1!=0)&(lbl_img1!=g)),np.zeros(shape),lbl_img1)\n        all_masks.append(a.clip(0,1))\n    all_masks = np.array(all_masks)\n    return all_masks","e99267f4":"sub_ids = submission['id'].values","ed4d5145":"res = []\nn = 0\nfor i in tqdm(range(len(prediction_masks))):\n    chk_mask = np.expand_dims(prediction_masks[i].numpy(),axis=-1).astype(np.uint8)\n    lbl_img = CCL(chk_mask)\n    pred_masks = convert_seg_ins(lbl_img)\n    for mask in pred_masks:\n        ts = np.unique(mask, return_counts=True)[1][1]\n        #removing blocks with very small areas\n        if ts>50:\n            res.append([sub_ids[i],rle_encode(mask)])","67088d55":"sub_df = pd.DataFrame(res,columns=['id', 'predicted'])","78a30f34":"sub_df.head()","5de14b7f":"sub_df.to_csv('submission.csv',index=False)","d9507ed6":"sub_df['id'].value_counts()","5b89c91b":"## A look at test predictions","68fbec30":"## Loading Submission files and predicting results","e37c1bd7":"* Start your training","3820f79b":"## Creating a UNet Learner","19c6a9a1":"**We will use an algorithm called connected components algorithm to convert semantic mask to instance mask**\n\n**Connected Component Labeling (CCL)** is a basic algorithm in image processing and an essential step in nearly every application dealing with object detection. It groups together pixels belonging to the same connected component\n\n![ccl](https:\/\/homepages.inf.ed.ac.uk\/rbf\/HIPR2\/labelb.gif)","cd3b9656":"<center><h1>Sartorius - Cell Instance Segmentation.<\/h1><\/center>\n\n![img](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/30201\/logos\/header.png)","26345d3b":"### Writing masks to rle","8f25acac":"## Converting predicted semantic masks to instance masks and then to run length encodings\n\n**Since fastai only provides semantic segmentation we will use a hack to convert it into intance segmentation masks**","53ecb243":"## Data already broken down into 2x2 tiles to increase dataset, and faster training\n\nhttps:\/\/www.kaggle.com\/robertlangdonvinci\/sartorius-cell-segmentation-data-gen\/notebook","b3ce170d":"### **Semantic Segmentation: each pixel of an image is linked to a class label.**\n\n![img](https:\/\/raw.githubusercontent.com\/WaterKnight1998\/SemTorch\/develop\/readme_images\/semantic_segmentation.png)\n\n### **Instance Segmentation: is similar to semantic segmentation, but goes a bit deeper, it identifies , for each pixel, the object instance it belongs to.**\n\n![img](https:\/\/raw.githubusercontent.com\/WaterKnight1998\/SemTorch\/develop\/readme_images\/instance_segmentation.png)","fea9f398":"## IoU metrics","0ba12d10":"**This module builds a dynamic U-Net from any backbone pretrained on ImageNet, automatically inferring the intermediate sizes.**\n\n***\n\n![dynamicUnet](https:\/\/fastai1.fast.ai\/imgs\/u-net-architecture.png)\n\n***\n\n**This is the original U-Net. The difference here is that the left part is a pretrained model.**\n\nhttps:\/\/fastai1.fast.ai\/vision.models.unet.html","8aa5fe48":"## our top 3 losses","bd0e11b7":"* Launch a mock training to find a good learning rate","f5fdc2ff":"### Since data is broked into 2x2 tile some tiles contain no masks we will clean them up.","a9afaf30":"## Creating A dataloader","85f9dc82":"## Please don't forget to Upvote if like the work.\n\n### if you fork the notebook please try to upvote it too.\n**It keeps me motivated**\n\n","e4fda8bd":"## Please don't forget to Upvote if like the work.","0cbe2eaf":"**See how CCL algorithm has colored each mask with a different color**"}}