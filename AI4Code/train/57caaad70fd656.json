{"cell_type":{"488f7189":"code","79b79279":"code","1346b308":"code","a5aba7c3":"code","a4c00a32":"code","5af7edd5":"code","ae6a8ea7":"code","5082dfc2":"code","684b4b05":"code","78816ba1":"code","81c92d20":"code","477b5b50":"code","e2d1e326":"code","3ced679d":"code","6cf5c940":"code","9233c43b":"code","56a00782":"code","2ecfbcab":"code","da9d63f9":"code","346f3977":"code","c8e8e8ed":"code","4f882d05":"code","27033fc0":"code","f501b1a7":"code","7fd9422c":"code","08d385f3":"code","3cd53c53":"code","cc274d6e":"code","940a96f9":"code","839efbf6":"code","139d591f":"code","06b9b03f":"markdown","b6647df6":"markdown","f3b90eb8":"markdown","d2b1f349":"markdown","46cf6e54":"markdown","ec71365d":"markdown","74ba787a":"markdown","3c463dde":"markdown","4c3b5b10":"markdown","190df146":"markdown","2e6cd07e":"markdown","89104b00":"markdown","650c9ff3":"markdown","144c179d":"markdown"},"source":{"488f7189":"!pip install ftfy regex tqdm\n!pip install git+https:\/\/github.com\/openai\/CLIP.git","79b79279":"# Handling data\nimport glob\nimport json\nfrom PIL import Image\nimport os\nimport random\n\n# Modeling and training\nimport torch\nimport numpy\nfrom transformers import AutoConfig, AutoTokenizer, GPT2PreTrainedModel, GPT2Model, AdamW, get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n\nfrom torch.utils.tensorboard import SummaryWriter\nimport tqdm\n\n# Pretrained CLIP models\nimport clip\n\n# Evaluation\nimport torchtext\nimport numpy as np\n","1346b308":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntorch.manual_seed(21)\n\nencoder_name = \"ViT-B\/32\"\ndecoder_name = \"distilgpt2\"\n\nprint(\"Available CLIP variants: {}\".format(clip.available_models()))\nencoder, preprocess = clip.load(encoder_name, device=device)\n\ndecoder_config = AutoConfig.from_pretrained(decoder_name)\ntext_tokenizer = AutoTokenizer.from_pretrained(decoder_name)\ntext_tokenizer.add_special_tokens({'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '[pad]'})\ndecoder_config.pad_token_id = text_tokenizer.pad_token_id\ndecoder_config.bos_token_id = text_tokenizer.bos_token_id\ndecoder_config.eos_token_id = text_tokenizer.eos_token_id\n\nbatch_size = 16","a5aba7c3":"img_paths = glob.glob('..\/input\/meme-project-raw\/*.jpg')\nprint(\"total num_imgs:\",len(img_paths))\nrandom.shuffle(img_paths)\ntrain_imgs = img_paths[:2400]\nval_imgs = img_paths[2400:2700]\ntest_imgs = img_paths[2700:]\nprint(\"train\/val\/test:\", len(train_imgs), len(val_imgs), len(test_imgs))","a4c00a32":"class MemeDataset(torch.utils.data.Dataset):\n    def __init__(self, img_paths):\n        self.img_paths = img_paths\n        self.pairs = []\n        \n        for img_path in self.img_paths:\n            \n            json_path = \"..\/input\/meme-project-clean-json\/\" + img_path[26:-3] + \"json\"\n            with open(json_path, 'r') as fp:\n                captions = json.loads(fp.read())\n                \n            for i, row in enumerate(captions):\n                if row != '':\n                    caption = \"<bos> \"+\" \".join(row)+\" <eos>\"\n                    self.pairs.append([img_path, caption])\n\n        \n    def __getitem__(self, idx):\n        pair = self.pairs[idx]\n        \n        img_path = pair[0]\n        caption = pair[1].lower()\n        \n        img = Image.open(img_path)\n        img = preprocess(img) # Using CLIP's preprocessing pipeline\n        \n        return img, caption, img_path\n        \n    def __len__(self):\n        return len(self.pairs)","5af7edd5":"train_set = MemeDataset(train_imgs)\nval_set = MemeDataset(val_imgs)\ntest_set = MemeDataset(test_imgs)\nprint(\"train\/val\/test set length (num_captions):\", len(train_set), len(val_set), len(test_set))","ae6a8ea7":"train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n\nos.makedirs('.\/outputs_dataloader', exist_ok=True)\ntorch.save(train_loader, '.\/outputs_dataloader\/train_loader.pt')\ntorch.save(val_loader, '.\/outputs_dataloader\/val_loader.pt')\ntorch.save(test_loader, '.\/outputs_dataloader\/test_loader.pt')","5082dfc2":"# Look at a sample\ntest_sample_idx = 4\n\ncap = text_tokenizer(train_set[test_sample_idx][1], padding=True, truncation=True)\ncap = torch.tensor(cap[\"input_ids\"])\n\nprint(train_set[test_sample_idx][0].shape) # Image\nprint(cap.shape) # Caption\nprint(text_tokenizer.decode(cap))\nprint(cap)\nImage.open(train_set[test_sample_idx][2])","684b4b05":"# Modified GPT2 with z added to the hidden states before the LM head\nclass Meme_decoder(torch.nn.Module):\n    def __init__(self, name, config, text_tokenizer):\n        super(Meme_decoder, self).__init__()\n        self._config = config\n        \n        self._transformer = GPT2Model(config).from_pretrained(name)\n        self._transformer.resize_token_embeddings(len(text_tokenizer))\n        \n        self._lm_head = torch.nn.Linear(config.n_embd, len(text_tokenizer), bias=False)\n        \n        self.layer_norm_h = torch.nn.LayerNorm(config.n_embd)\n        self.layer_norm_z = torch.nn.LayerNorm(config.n_embd)\n        \n    def forward(self, cap, z): \n        h = self._transformer(cap)\n        h = h.last_hidden_state\n        h = self.layer_norm_h(h)\n        \n        z = self.layer_norm_z(z)\n        \n        h = h + z\n        \n        logits = self._lm_head(h)\n        return logits","78816ba1":"class Meme_variation(torch.nn.Module):\n    def __init__(self, encoder, decoder_name, decoder_config, text_tokenizer):\n        super(Meme_variation, self).__init__()\n        self.encoder = encoder\n        \n        self.decoder = Meme_decoder(decoder_name, decoder_config, text_tokenizer)\n        \n        self.linear_mean = torch.nn.Linear(512, 768)\n        self.linear_logvar = torch.nn.Linear(512, 768)\n\n\n    def forward(self, img, cap):\n        img_emb = self.encoder.encode_image(img).float()\n        \n        mean = self.linear_mean(img_emb)\n        logvar = self.linear_logvar(img_emb)\n        \n        eps = torch.randn_like(mean)\n        z = mean + torch.exp(logvar) ** 0.5 * eps\n        z = z.unsqueeze(1)\n        \n        out = self.decoder(cap, z)\n        \n        return (out, (mean, logvar))","81c92d20":"def Meme_decode(model, img, text_tokenizer = text_tokenizer, max_len = 50, min_len = 1):\n    model.eval()\n    logits, _ = model(img.unsqueeze(0).to(device), torch.tensor([text_tokenizer.bos_token_id]).unsqueeze(0).to(device))\n    probs = torch.nn.functional.softmax(logits, dim=-1)[-1]\n    prev = int(torch.multinomial(probs, num_samples=1))\n    \n    while prev in [text_tokenizer.eos_token_id]:\n        logits, _ = model(img.unsqueeze(0).to(device), torch.tensor([text_tokenizer.bos_token_id]).unsqueeze(0).to(device))\n        probs = torch.nn.functional.softmax(logits, dim=-1)[-1]\n        prev = int(torch.multinomial(probs, num_samples=1))\n    \n    out = [prev]\n    \n    while len(out) < max_len:\n        logits, _ = model(img.unsqueeze(0).to(device), torch.tensor(out).unsqueeze(0).to(device))\n        probs = torch.nn.functional.softmax(logits, dim=-1)[-1,-1]\n        prev = int(torch.multinomial(probs, num_samples=1))\n        \n        if prev in [text_tokenizer.eos_token_id, 1279]:\n            if len(out) < min_len:\n                continue\n            break\n            \n        \n        out.append(prev)\n    \n    if len(out) > 0:\n        while out[0] in [39565, 29]: # To fix a bug with the preprocessing post-hoc (affets only the decoding of bos\/eos tokens) so we won't have to train again\n            try:\n                out.pop(0)\n            except:\n                break\n                \n            if len(out) == 0:\n                break\n        \n    return out","477b5b50":"def Meme_evaluate(model, loader, get_bleu=1):\n    \n    model.eval()\n    n_samples = 0\n    total_loss = 0.0\n    bleu_x = {}\n    bleu_y = {}\n\n    \n    for step, batch in enumerate(tqdm.tqdm(loader)):\n        \n        batch_size = batch[0].shape[0]\n        n_samples += batch_size\n        \n        img, cap, img_path = batch[0], batch[1], batch[2]\n        \n        cap = list(cap)\n        cap = text_tokenizer(cap, padding=True, truncation=True, max_length=50)\n\n        cap = torch.tensor(cap[\"input_ids\"])\n        \n        cap_x = cap[:,:-1].long().to(device)\n        cap_y = cap[:,1:].long().to(device)\n\n        logits, _ = model(img, cap_x)\n\n        B, N, V = logits.size()\n\n        loss = criterion_CE(logits.view((-1, V)), cap_y.view(-1))\n\n        total_loss += loss.item()*batch_size\n        \n        \n        # BLEU\n        if get_bleu:\n            for i in range(batch_size):\n                bleu_img = img[i]\n                bleu_text_real = batch[1][i].lower().split(\" \")[1:-1]\n                bleu_img_path = img_path[i]\n                \n                if bleu_img_path not in bleu_x.keys():\n                    generated_ids = Meme_decode(model, bleu_img)\n                    generated_text = text_tokenizer.decode(generated_ids).lower().split(\" \")\n                    bleu_x[bleu_img_path] = generated_text\n\n                if bleu_img_path not in bleu_y.keys():\n                    bleu_y[bleu_img_path] = [bleu_text_real]\n                else:\n                    bleu_y[bleu_img_path].append(bleu_text_real)\n                    \n    perplexity  = torch.exp(torch.tensor(total_loss \/ n_samples))\n    \n    if get_bleu:\n        bleu_x_lst, bleu_y_lst = [], []\n        for key in bleu_x.keys():\n            bleu_x_lst.append(bleu_x[key])\n            bleu_y_lst.append(bleu_y[key])\n        BLEU = torchtext.data.metrics.bleu_score(bleu_x_lst, bleu_y_lst, max_n=get_bleu, weights=[1\/get_bleu]*get_bleu) # Max_len = 4\n        \n    else:\n        BLEU = 0\n    \n    return (perplexity, BLEU, bleu_x_lst)\n","e2d1e326":"model = Meme_variation(encoder, decoder_name, decoder_config, text_tokenizer).to(device)","3ced679d":"num_train_epochs = 4\nKL_weight = 0.1\n\ncriterion_CE = torch.nn.CrossEntropyLoss(ignore_index=text_tokenizer.pad_token_id)\n\ndef criterion_KL(mean, logvar):\n    return torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n\noptimizer = AdamW(model.parameters(), lr=1e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, 10, num_train_epochs*len(train_loader))","6cf5c940":"# SKIP ME WHEN RUNNING INFERENCE\n# REMOVE THE 2 BREAKS FOR TRAINING\noutput_root = \".\/outputs\"\n\nmodel.train()\nglobal_step = 0\ntr_loss = 0\nloss_lst = []\neval_lst = []\n\nfor epoch in range(num_train_epochs):\n    \n    for step, batch in enumerate(train_loader):\n        \n        global_step += 1\n        optimizer.zero_grad()\n\n        img, cap = batch[0], batch[1]\n        \n        cap = list(cap)\n        cap = text_tokenizer(cap, padding=True, truncation=True, max_length=50)\n\n        cap = torch.tensor(cap[\"input_ids\"])\n        \n        cap_x = cap[:,:-1].long().to(device)\n        cap_y = cap[:,1:].long().to(device)\n\n        logits, (mean, logvar) = model(img, cap_x)\n\n        B, N, V = logits.size()\n\n        loss_CE = criterion_CE(logits.reshape((-1, V)), cap_y.reshape(-1))\n        loss_KL = criterion_KL(mean, logvar)\n        \n        loss = loss_CE - KL_weight * loss_KL\n\n        tr_loss += loss.item()\n\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n        \n        if global_step % 1000 == 0: # Loss printed every 1000 batches\n            print(global_step, loss.item())\n            \n        if global_step % 5000 == 0: # Checkpoint saved every 5000 batches\n            loss_lst.append(tr_loss\/5000)\n            eval_lst.append(Meme_evaluate(model, val_loader, get_bleu=False)[0])\n            output_dir = output_root + \"\/\" + str(global_step)\n            os.makedirs(output_dir, exist_ok=True)\n            state_dict = model.state_dict()\n            torch.save(state_dict, os.path.join(output_dir, \"pytorch_model.bin\"))\n            tr_loss = 0\n            model.train()\n            \n        break\n    break\n\n        \n","9233c43b":"print(\"training set loss (CE):\", loss_lst)\nprint(\"validation set perplexity:\", eval_lst)","56a00782":"# Load dataloader and model checkpoint\nval_loader = torch.load('..\/input\/variation-best\/val_loader.pt')\ntest_loader = torch.load('..\/input\/variation-best\/test_loader.pt')\n\nmodel.load_state_dict(torch.load(\"..\/input\/variation-best\/variation_best.bin\"))","2ecfbcab":"_, bleu1, _ = Meme_evaluate(model, test_loader, get_bleu=1)\n_, bleu2, _ = Meme_evaluate(model, test_loader, get_bleu=2)\n_, bleu3, generated = Meme_evaluate(model, test_loader, get_bleu=3)\n\nprint(\"test set blue1:\", bleu1)\nprint(\"test set blue2:\", bleu2)\nprint(\"test set blue3:\", bleu3)","da9d63f9":"import nltk, re, string, collections\nfrom nltk.util import ngrams\n\nsentences = generated\nBigramCtr = collections.Counter()\nUnigramCtr = collections.Counter()\nfor sentence in sentences:\n    BigramCtr.update(nltk.ngrams(sentence, 2))\n    UnigramCtr.update(nltk.ngrams(sentence, 1))\nprint(\"Unigram count:\",len(BigramCtr))\nprint(\"Bigram count:\",len(UnigramCtr))","346f3977":"for step, batch in enumerate(val_loader):\n    if  step == 210:\n        img = batch[0][0]\n        img_path = batch[2][0]\n        generated_ids = Meme_decode(model, img, text_tokenizer = text_tokenizer, max_len = 50, min_len = 1)\n        print(text_tokenizer.decode(generated_ids).lower().split(\" \"))\n        print(\"Reference Caption:\", batch[1][0])\n        break\nImage.open(img_path)","c8e8e8ed":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\nlm_tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # the tokenizers for lmgpt2 and distilgpt2 are the same (except the special tokens)\nlm_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)","4f882d05":"def antilm_prob_last_word(input_ids, tokenizer=text_tokenizer, lm_model=lm_model):\n    input_v_size = len(tokenizer)\n    logits = lm_model(torch.tensor([input_ids]).to(device)).logits\n    probs = torch.nn.functional.softmax(logits, dim=-1)[-1,-1]\n    probs = torch.cat((probs, probs[-1:], probs[-1:] ,probs[-1:]))\n    return probs","27033fc0":"def score_key(item):\n    return item[1]\n\ndef Meme_decode_MMI(model, img, text_tokenizer=text_tokenizer, antilm_tokenizer=lm_tokenizer,max_len = 20, antilm_weight=0.1, beam_size=10):\n    beams = [] # [[ids], score], [], ...\n    logits, _ = model(img.unsqueeze(0).to(device), torch.tensor([text_tokenizer.bos_token_id]).unsqueeze(0).to(device))\n    logits[0, -1, 39565] -= 1e20\n    model_probs = torch.nn.functional.softmax(logits, dim=-1)[-1]\n    antilm_probs = antilm_prob_last_word(antilm_tokenizer(antilm_tokenizer.bos_token).input_ids).unsqueeze(0)\n    score = torch.log(model_probs) - antilm_weight * torch.log(antilm_probs)\n    ids = torch.argsort(score, descending=True)[0][:beam_size]\n    for id in ids:\n        beams.append([[int(id)], float(score[0, id])])\n    \n    while True:\n        modified = False\n        new_beams = []\n        for beam in beams:\n            ids = beam[0]\n            score = beam[1]\n            if ids[-1] in [text_tokenizer(text_tokenizer.eos_token), 1279] or len(ids) >= max_len:\n                new_beams.append([ids, score])\n            else:\n                modified = True\n                logits, _ = model(img.unsqueeze(0).to(device), torch.tensor(ids).unsqueeze(0).to(device))\n                logits[0, -1, 39565] -= 1e20\n                model_probs = torch.nn.functional.softmax(logits, dim=-1)[-1]\n                model_probs[0, 39565] = 0\n                antilm_probs = antilm_prob_last_word(ids).unsqueeze(0)\n                score = torch.log(model_probs) + torch.tensor(score) - antilm_weight * torch.log(antilm_probs)\n                new_ids = torch.argsort(score, descending=True)[0][:beam_size]\n                for new_id in new_ids:\n                    new_beams.append([ids + [int(new_id)], float(score[0, new_id])])\n        beams = sorted(new_beams, key=score_key, reverse=True)[:beam_size]\n                               \n        if not modified:\n            best_beam = beams[0]\n            for beam in beams:\n                print(text_tokenizer.decode(beam[0]))\n            print(best_beam[0])\n            return text_tokenizer.decode(best_beam[0]), best_beam[1]\n    \n    ","f501b1a7":"Meme_decode_MMI(model, test_set[0][0])","7fd9422c":"def Meme_evaluate_MMI(model, loader, get_bleu=1):\n    \n    model.eval()\n    n_samples = 0\n    total_loss = 0.0\n    bleu_x = {}\n    bleu_y = {}\n\n    \n    for step, batch in enumerate(tqdm.tqdm(loader)):\n        \n        batch_size = batch[0].shape[0]\n        n_samples += batch_size\n        \n        img, cap, img_path = batch[0], batch[1], batch[2]\n        \n        cap = list(cap)\n        cap = text_tokenizer(cap, padding=True, truncation=True, max_length=50)\n\n        cap = torch.tensor(cap[\"input_ids\"])\n        \n        cap_x = cap[:,:-1].long().to(device)\n        cap_y = cap[:,1:].long().to(device)\n\n        logits, _ = model(img, cap_x)\n\n        B, N, V = logits.size()\n\n        loss = criterion_CE(logits.view((-1, V)), cap_y.view(-1))\n\n        total_loss += loss.item()*batch_size\n        \n        \n        # BLEU\n        if get_bleu:\n            for i in range(batch_size):\n                bleu_img = img[i]\n                bleu_text_real = batch[1][i].lower().split(\" \")[1:-1]\n                bleu_img_path = img_path[i]\n                \n                if bleu_img_path not in bleu_x.keys():\n                    generated_ids = Meme_decode_MMI(model, bleu_img)\n                    generated_text = text_tokenizer.decode(generated_ids).lower().split(\" \")\n                    bleu_x[bleu_img_path] = generated_text\n\n                if bleu_img_path not in bleu_y.keys():\n                    bleu_y[bleu_img_path] = [bleu_text_real]\n                else:\n                    bleu_y[bleu_img_path].append(bleu_text_real)\n                    \n    perplexity  = torch.exp(torch.tensor(total_loss \/ n_samples))\n    \n    if get_bleu:\n        bleu_x_lst, bleu_y_lst = [], []\n        for key in bleu_x.keys():\n            bleu_x_lst.append(bleu_x[key])\n            bleu_y_lst.append(bleu_y[key])\n        BLEU = torchtext.data.metrics.bleu_score(bleu_x_lst, bleu_y_lst, max_n=get_bleu, weights=[1\/get_bleu]*get_bleu) # Max_len = 4\n        \n    else:\n        BLEU = 0\n    \n    return (perplexity, BLEU, bleu_x_lst)\n","08d385f3":"def find_nearest_text(img_path, txts, clip_model, preprocess):\n\n    image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n    text = clip.tokenize(txts).to(device)\n\n    with torch.no_grad():\n        logits_per_image, logits_per_text = clip_model(image, text)\n        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n        probs = probs.squeeze(0)\n    return np.argmax(probs), np.max(probs), txts[np.argmax(probs)]","3cd53c53":"clip_decode, preprocess = clip.load(\"ViT-B\/32\", device=device, jit=False)\nclip_decode.load_state_dict(torch.load(\"..\/input\/clipfinetuneweights\/best_model.pt\"))\nfind_nearest_text(\"..\/input\/meme-project-raw\/-minho.jpg\", [\"blablabla bla\", \"hey camper\", \"some other text\"], clip_decode, preprocess)","cc274d6e":"def CLIP_gen(image, image_path, n=10):\n    candidates = []\n    for i in range(n):\n        generated_ids = Meme_decode(model, image, text_tokenizer = text_tokenizer, max_len = 50, min_len = 1)\n        text = text_tokenizer.decode(generated_ids)\n        candidates.append(text)\n    best_match = find_nearest_text(image_path, candidates, clip_decode, preprocess)[2]\n    return best_match\n\nimport random\nimport requests\npath = \"..\/input\/meme-project-raw\/conspiracy-keanu.jpg\"\nprint(CLIP_gen(preprocess(Image.open(path)),path))\nImage.open(path)","940a96f9":"def Meme_evaluate_CLIP(model, loader, get_bleu=1):\n    \n    model.eval()\n    n_samples = 0\n    total_loss = 0.0\n    bleu_x = {}\n    bleu_y = {}\n\n    \n    for step, batch in enumerate(tqdm.tqdm(loader)):\n        \n        batch_size = batch[0].shape[0]\n        n_samples += batch_size\n        \n        img, cap, img_path = batch[0], batch[1], batch[2]\n        \n        cap = list(cap)\n        cap = text_tokenizer(cap, padding=True, truncation=True, max_length=50)\n\n        cap = torch.tensor(cap[\"input_ids\"])\n        \n        cap_x = cap[:,:-1].long().to(device)\n        cap_y = cap[:,1:].long().to(device)\n\n        logits, _ = model(img, cap_x)\n\n        B, N, V = logits.size()\n\n        loss = criterion_CE(logits.view((-1, V)), cap_y.view(-1))\n\n        total_loss += loss.item()*batch_size\n        \n        \n        # BLEU\n        if get_bleu:\n            for i in range(batch_size):\n                bleu_img = img[i]\n                bleu_text_real = batch[1][i].lower().split(\" \")[1:-1]\n                bleu_img_path = img_path[i]\n                \n                if bleu_img_path not in bleu_x.keys():\n                    generated_text = CLIP_gen(bleu_img, bleu_img_path).lower().split(\" \")\n                    bleu_x[bleu_img_path] = generated_text\n\n                if bleu_img_path not in bleu_y.keys():\n                    bleu_y[bleu_img_path] = [bleu_text_real]\n                else:\n                    bleu_y[bleu_img_path].append(bleu_text_real)\n    perplexity  = torch.exp(torch.tensor(total_loss \/ n_samples))\n    \n    if get_bleu:\n        bleu_x_lst, bleu_y_lst = [], []\n        for key in bleu_x.keys():\n            bleu_x_lst.append(bleu_x[key])\n            bleu_y_lst.append(bleu_y[key])\n        BLEU = torchtext.data.metrics.bleu_score(bleu_x_lst, bleu_y_lst, max_n=get_bleu, weights=[1\/get_bleu]*get_bleu) # Max_len = 4\n        \n    else:\n        BLEU = 0\n    \n    return (perplexity, BLEU, bleu_x_lst)\n","839efbf6":"_, bleu1, _ = Meme_evaluate_CLIP(model, test_loader, get_bleu=1)\n_, bleu2, _ = Meme_evaluate_CLIP(model, test_loader, get_bleu=2)\n_, bleu3, generated = Meme_evaluate_CLIP(model, test_loader, get_bleu=3)\n\nprint(\"test set bleu1:\", bleu1)\nprint(\"test set bleu2:\", bleu2)\nprint(\"test set bleu3:\", bleu3)","139d591f":"import nltk, re, string, collections\nfrom nltk.util import ngrams\n\nsentences = generated\nBigramCtr = collections.Counter()\nUnigramCtr = collections.Counter()\nfor sentence in sentences:\n    BigramCtr.update(nltk.ngrams(sentence, 2))\n    UnigramCtr.update(nltk.ngrams(sentence, 1))\nprint(\"Unigram count:\",len(BigramCtr))\nprint(\"Bigram count:\",len(UnigramCtr))","06b9b03f":"# Meme Variation\nTransformer-based encoder-decoder with a VAE-like middle layer, hybrid loss (CE + KL)\n\n* Preprocessing\n* Modeling (Enc = CLIP_ViT-B\/32, Dec = distilgpt2)\n* Training\n* Generation and Evaluation\n\nTo run inference: run every cell in order and skip the onr training cell","b6647df6":"## Preprocessing\n* Image: CLIP's preprocessing pipeline, no augmentation\n* Captions: linebreak ignored","f3b90eb8":"## CLIP-Score Decoding","d2b1f349":"## Evaluation","46cf6e54":"#### Evaluation\n* Perplexity and BLEU","ec71365d":"## Model\n* Image encoder: CLIP-ViT-B\/32, output_size = (batch_size, 512)\n* Linear layers to project the image embedding to (batch_size, 768) for mean and log variance\n* Reparameterisation trick to sample from the learned distribution\n* Text decoder: distilgpt2 with the image embedding added to the hidden states before the LM head, output_size =  (batch_size, seq_length, vocal_size)","74ba787a":"## Install and Import Things","3c463dde":"#### Import things","4c3b5b10":"#### Generation\n* Random sampling based on the probs","190df146":"## MMI Decoding","2e6cd07e":"#### Split training\/validation\/test sets by image\n* train\/val\/test ratio = 8:1:1 (by #image, not #captions)","89104b00":"#### CLIP","650c9ff3":"#### Set up","144c179d":"## Training"}}