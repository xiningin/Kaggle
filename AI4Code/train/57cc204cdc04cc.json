{"cell_type":{"2b32d191":"code","da1995fd":"code","d5ac380e":"code","3e33df64":"code","ed1cf866":"code","5301b363":"code","3fffa9d2":"code","a21f8bb6":"code","e477e45c":"code","eefbf2ad":"code","7a32c167":"code","dc2d0d54":"code","79aff1fb":"code","17944101":"code","c8bfba4c":"code","c84798e2":"code","9646c89f":"code","e4f3f03a":"markdown","9dc37563":"markdown","79e9e8af":"markdown","45e96bce":"markdown","a67dc70a":"markdown","d34acbb9":"markdown","6fb1642d":"markdown"},"source":{"2b32d191":"from IPython.display import display\nimport gc\nfrom tqdm.auto import tqdm, trange\n\n# data manipulation\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing, model_selection, pipeline\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# supervised learning\nfrom tensorflow.keras import models, layers, losses, optimizers","da1995fd":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col=0)\n\ndisplay(df.head())\nprint(f'\\nData shape: \\t{df.shape}\\n')","d5ac380e":"(df == 'None').sum().loc[lambda x: x!=0]","3e33df64":"df = df.replace('None', np.nan)","ed1cf866":"null_frac = df.isnull().sum().loc[lambda x: x!=0] \/ df.shape[0]\n\nfig,ax = plt.subplots(figsize=(10,5))\nax.set_title('Missing values')\nax.set_xlabel('feature')\nax.set_ylabel('relative fraction')\nax.grid(axis='y')\nax.tick_params(axis='x', rotation=45)\nax = sns.barplot(x=null_frac.index, y=null_frac.values, color='C0')","5301b363":"df.dtypes.loc[null_frac.index]","3fffa9d2":"# fill missing values with `0`\ndf = df.fillna(value=0)","a21f8bb6":"print(f'Missing values: \\t{df.isnull().sum().sum():d}')","e477e45c":"def preprocess_missing(df, plot=True, figsize=(10,5), **kwargs):\n    \n    '''Remove missing values.\n    \n    Parameters:\n    - df: pandas.DataFrame to be preprocessed;\n    - plot: boolean value to plot or not missing values relative fraction;\n    - figsize: plot's figure size;\n    - **kwargs: additional keyword arguments are passed to matplotlib.pyplot.subplots when creating the plot.\n    \n    Categorical missing values are filled with `'None'` label; numeric ones are filled with value `0`.\n    \n    Returns:\n    - df: preprocessed dataframe.'''\n    \n    # avoid modifying original dataframe\n    df = df.copy()\n    \n    df = df.replace('None', np.nan)\n    null_frac = df.isnull().sum().loc[lambda x: x!=0] \/ df.shape[0]\n    \n    if plot:\n        fig,ax = plt.subplots(figsize=figsize, **kwargs)\n        ax.set_title('Missing values')\n        ax.set_xlabel('feature')\n        ax.set_ylabel('relative fraction')\n        ax.grid(axis='y')\n        ax.tick_params(axis='x', rotation=45)\n        ax = sns.barplot(x=null_frac.index, y=null_frac.values, color='C0')\n        plt.show()\n        \n    # fill missing values\n    df = df.fillna(value=0)\n        \n    return df","eefbf2ad":"def encode_labels(df, encoders=None):\n    \n    '''Encode dataframe labels.\n    \n    Parameters:\n    - df: pandas.DataFrame to encode;\n    - encoders: dict containing (col, encoder: sklearn.preprocessing.LabelEncoder) pairs.\n      If `None` (default value) encoders are created for each object column and fitted on dataframe's values.\n    \n    Returns:\n    - df: encoded dataframe;\n    - encoders: dict with used encoders.\n    \n    0 values are not encoded and encoding is done using values greater or equal than 1.'''\n    \n    # avoid modifying original df\n    df = df.copy()\n\n    # create and fit encoders\n    if encoders == None:\n        encoders = {}\n        for col in df.dtypes.loc[lambda x: x == object].index:\n            encoders[col] = preprocessing.LabelEncoder().fit(df.loc[df.loc[:,col] != 0, col])\n\n    # encode labels\n    for col, encoder in encoders.items():\n        try:\n            df.loc[df.loc[:,col] != 0, col] = encoder.transform(df.loc[df.loc[:,col] != 0, col]) + 1\n        except ValueError as err:\n            raise ValueError(f'col: {col} \\tValueError: {err}')\n    \n    return df, encoders\n\ndf, encoders = encode_labels(df)","7a32c167":"targets = [\"SalePrice\"]\ntrain, valid = model_selection.train_test_split(df.astype(float), train_size=0.8, random_state=5)\nx_train, y_train = train.drop(columns=targets), train.loc[:,targets]\nx_valid, y_valid = valid.drop(columns=targets), valid.loc[:,targets]\n\nprint(f'Train shape: \\t{x_train.shape}')","dc2d0d54":"def build_model(features, targets, n_layers=3, units=1024, divide_units=2, activation='relu', normalize_input=True, normalize_dense=True, dropout_rate=0, learning_rate=1e-3, **kwargs):\n    \n    '''Build MLP model.\n    \n    Parameters:\n    - `features`: input layers shape is set as `(features,)`;\n    - `targets`: output layer units;\n    - `n_layers`: hidden layers number;\n    - `units`: first dense hidden layers units;\n    - `divide_units`: dense hidden layers contain a number of units given by the previous layer one divided by `divide_units`;\n    - `activation`: dense hidden layers activation function;\n    - `normalize_input`: normalize input values;\n    - `normalize_dense`: add a batch normalization layer after dense;\n    - `dropout`: dropout probability for dense hidden layers;\n    - `learning_rate`: optimizer learning rate;\n    - **kwargs: additional keyword arguments, they are passed to dense layers.\n    \n    Loss used by model is mean squared logarithmic error (MSLE).\n    Function could raise an InvalidArgument exception if `units`, `divide_units` and `n_layers` are not compatible.\n    \n    Returns:\n    - Built and compiled model.'''\n    \n    # units, divide_units, n_layers exceptions\n    for i in range(n_layers):\n        if units % divide_units**i != 0:\n            raise ValueError(f'''invalid units number: layer {i} would have {units\/divide_units**i:.2f} units\n            (n_layers: {n_layers}, units: {units}, divide_units: {divide_units})'''\n                                 )\n        \n    # input layer\n    if normalize_input:\n        model = [layers.experimental.preprocessing.Normalization(input_shape=(features,), name='input')]\n    else:\n        model = [layers.Input(shape=(features,), name='input')]\n    \n    # hidden layer\n    for i in range(1,n_layers+1):\n        if dropout_rate != 0:\n            model += [layers.Dropout(rate=dropout_rate, seed=i, name=f'drop_{i}')]\n        model += [layers.Dense(units=units, activation=activation, name=f'dense_{i}', **kwargs)]\n        if normalize_dense:\n            model += [layers.BatchNormalization(name=f'norm_{i}')]\n        units \/= divide_units\n    \n    # output layer\n    model += [layers.Dense(units=targets, name='dense_out')]\n    \n    # build and compile\n    model = models.Sequential(model, name = 'MLP')\n    model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), loss='msle')\n    \n    return model\n\ngc.collect()\nmodel = build_model(features=x_train.shape[1], targets=y_train.shape[1],\n                    n_layers=3, units=1024*1, divide_units=1, activation='relu', normalize_input=False, normalize_dense=False, learning_rate=3e-4\n                   )\nmodel.summary()","79aff1fb":"# training pars\nepochs = 50\nbatch_size = 1000\n\n# training\ntraining_history = model.fit(\n    x=x_train, y=y_train,\n    validation_data=(x_valid, y_valid),\n    batch_size=batch_size, epochs=epochs,\n    verbose=0\n)\n\n# training loss plot\nfig,ax = plt.subplots(figsize=(10,5))\nax.set_title('MLP training')\nax.set_xlabel('epoch')\nax.set_ylabel('RMSLE')\nax.grid(axis='y')\nax = sns.lineplot(x=np.arange(1,epochs+1), y=np.sqrt(training_history.history['loss']), label='loss', ax=ax)\nax = sns.lineplot(x=np.arange(1,epochs+1), y=np.sqrt(training_history.history['val_loss']), label='val_loss', ax=ax)\n\nprint(f'Last epoch (RMSLE): \\tloss: {np.sqrt(training_history.history[\"loss\"][-1]):01.3f} \\tval_loss: {np.sqrt(training_history.history[\"val_loss\"][-1]):01.3f}')","17944101":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col=0)\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv', index_col=0)\n\ndisplay(test.head())\nprint(f'Test shape: \\t{test.shape}')","c8bfba4c":"test = preprocess_missing(test, figsize=(12,5))","c84798e2":"# encode test labels using train dataset's encoders\ntest, encoders = encode_labels(test, encoders)","9646c89f":"# predict test target\nsubmission.loc[:,targets] = model.predict(test.astype(float))\nsubmission.to_csv('submission.csv')","e4f3f03a":"# Data preprocessing","9dc37563":"# Supervised Learning","79e9e8af":"Following function summarize previous operations and it will be used to preprocess test data.","45e96bce":"To include those values in missing ones they are replaced with `NaN`.","a67dc70a":"Numeric features with missing values can be filled with $0$: missing `LotFrontage` could mean there is no frontage, the same is valid fot `MasVnrArea`; missing `GarageYrBlt` can be filled with $0$ since no garage was built in year $0$.\nIn the same way, the missing categorical variables can be replaced with 0, later avoiding the use of this value when encoding the labels.","d34acbb9":"Some categorical features contain label `'None'` as a valid value.","6fb1642d":"# Submission"}}