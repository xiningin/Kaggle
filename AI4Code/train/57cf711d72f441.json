{"cell_type":{"4d7fef23":"code","2f10c53e":"code","6a2900bb":"code","0152cad9":"code","019fad36":"code","2a35b242":"code","f82fc57c":"code","c45877b9":"code","f72baf89":"code","cd8a1648":"code","34031142":"code","14c6997c":"code","96307b79":"code","9efab2f8":"code","94dfc78d":"code","9985c89e":"code","a19e2daf":"code","f9043b90":"code","137cf570":"code","5541aff9":"code","a26c557d":"code","87cf7431":"code","a3ddda21":"code","33e90bcc":"code","b7ca3982":"code","1814972f":"code","b22724c0":"code","b806c917":"code","73c62c42":"code","fea5c50a":"code","35787224":"code","f8a68064":"code","78102206":"code","b34f42c7":"code","4572290e":"code","306f86e1":"code","40a854e8":"code","1a517a9e":"code","84a14102":"code","0519c44b":"code","c85fbef7":"code","89c39b5e":"code","cbb7b15a":"code","a3035377":"code","7334a37a":"code","50997c39":"code","d9fac3a4":"code","bf6fd7e8":"code","11fb7333":"code","138b944a":"code","bce7d514":"code","9af0df70":"code","593566d9":"code","18459426":"code","8b0a6756":"code","b35e1393":"markdown","6d34368e":"markdown","17ed72eb":"markdown","55bbbe56":"markdown","d3e5c338":"markdown","3db841e1":"markdown","34eedabf":"markdown"},"source":{"4d7fef23":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n","2f10c53e":"#load data on dataframe\ntel_df = pd.read_csv(\"\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n\n#display dataframe\ntel_df.head()","6a2900bb":"#count of rows and columns\ntel_df.shape","0152cad9":"# Get data frame info\ntel_df.info()","019fad36":"# Change Dtype of TotalCharges from object to numeric\ntel_df.TotalCharges = pd.to_numeric(tel_df.TotalCharges, errors='coerce')\ntel_df.info()","2a35b242":"# Get the statistics of the data frame\ntel_df.describe()","f82fc57c":"# TotalCharges colums has 7032 values, Total values suppose to be 7043 value. There is 11  missing values. This small quantity of missing values are insignificant, thus deleting rows with null values is the prefer choice.\n# There are 11 missing values out of 7043 values in TotalCharges column. I prefer deleting rows with null values \ntel_df.dropna(inplace = True)","c45877b9":"# Drop customerID column. It is not required\ntel_df.drop(['customerID'], axis =1, inplace =True)","f72baf89":"# Assign 1 to Male and 0 for Female\ntel_df.gender = [1 if each == \"Male\" else 0 for each in tel_df.gender]\n\n# Assign 1 to Yes, and 0 to No for the columns below\n\ncolumns_to_convert = ['Partner', \n                      'Dependents', \n                      'PhoneService', \n                      'OnlineSecurity',\n                      'OnlineBackup',\n                      'DeviceProtection',\n                      'TechSupport',\n                      'StreamingTV',\n                      'StreamingMovies',\n                      'PaperlessBilling', \n                      'Churn',\n                      'MultipleLines']\n\n# Assign 1 to Yes, and 0 to No for the \n\nfor item in columns_to_convert:\n    tel_df[item] = [1 if each == \"Yes\" else 0 if each == \"No\" else -1 for each in tel_df[item]]\n\ntel_df.head()","cd8a1648":"tel_df.info()","34031142":"sns.countplot(x=\"Churn\",data=tel_df);\ncounts = tel_df['Churn'].value_counts()\ncounts\n              ","14c6997c":"fig, ax = plt.subplots()\nax.pie(counts, autopct='%1.1f%%')\nax.legend(labels=['0(Stay)', '1(Left)'], title='Customer Churn',loc='lower right')\nax.set_title(\"Customer Churn Percentage\")\nplt.show()","96307b79":"sns.pairplot(tel_df,vars = ['TotalCharges','MonthlyCharges','tenure'], hue=\"Churn\",diag_kind=\"hist\")","9efab2f8":"counts = tel_df['InternetService'].value_counts()\ncounts","94dfc78d":"sns.set\nsns.countplot(x=\"InternetService\", data = tel_df)","9985c89e":"sns.set(style=\"whitegrid\")\ng1=sns.catplot(x=\"InternetService\", y=\"Churn\", data=tel_df,kind=\"bar\")\ng1.set(xlabel='InternerService', ylabel = 'Churn Probability')\nplt.show()","a19e2daf":"sns.set(style=\"whitegrid\")\ng1=sns.catplot(x=\"Contract\", y=\"Churn\", data=tel_df,kind=\"bar\")\ng1.set(xlabel=\"Contract\", ylabel = 'Churn Probability')\nplt.show()","f9043b90":"sns.set(style=\"whitegrid\")\ng1=sns.catplot(x=\"PaymentMethod\", y=\"Churn\", data=tel_df,kind=\"bar\")\ng1.set(xlabel=\"PaymentMethod\", ylabel = 'Churn Probability')\nplt.show()","137cf570":"# Let's create new columns by using pandas get_dummies function.\ntel_df = pd.get_dummies(data=tel_df)\ntel_df.head()","5541aff9":"# Now see the correlation between churn and all of the columns.\ntel_df.corr()['Churn'].sort_values()","a26c557d":"# Assigning X as input and y as output\ny = tel_df['Churn']\nX = tel_df.drop(['Churn'], axis=1)\n","87cf7431":"#Split data into Train and Test \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state =1)","a3ddda21":"# RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rf=RandomForestClassifier()\nmodel_rf.fit(X_train,y_train)","33e90bcc":"y_predict1=model_rf.predict(X_test)","b7ca3982":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,y_predict1))","1814972f":"cm=confusion_matrix(y_test,y_predict1)\nsns.heatmap(cm,annot=True)\n","b22724c0":"# SUPPORT VECTOR MACHINE CLASSIFICATION\nfrom sklearn.calibration import CalibratedClassifierCV # For probability score output\nfrom sklearn.svm import LinearSVC\n\nmodel_svm=LinearSVC(max_iter=10000)\nmodel_svm=CalibratedClassifierCV(model_svm)\nmodel_svm.fit(X_train,y_train)\n\n#Alternative method\n#from sklearn import svm\n#clf = svm.SVC(kernel='rbf')\n#clf.fit(X_train, y_train)\n\n","b806c917":"y_predict2=model_svm.predict(X_test)","73c62c42":"print(classification_report(y_test,y_predict2))","fea5c50a":"cm=confusion_matrix(y_test,y_predict2)\nsns.heatmap(cm, annot=True)","35787224":"yhat_prob = model_svm.predict_proba(X_test)","f8a68064":"# Method 1 for KNN (simple model)\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel_knn=KNeighborsClassifier()\nmodel_knn.fit(X_train,y_train)","78102206":"y_predict3=model_knn.predict(X_test)","b34f42c7":"print(classification_report(y_test,y_predict3))","4572290e":"cm=confusion_matrix(y_test,y_predict3)\nsns.heatmap(cm,annot=True)","306f86e1":"# Method II for KNN\n# Import library\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Training\nk = 4\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh\n\n#Predicting\nyhat = neigh.predict(X_test)\nyhat[0:5]\n\n#Accuracy evaluation\nfrom sklearn import metrics\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))\n\n\n#Calculate the accuracy of KNN for different values of k\nKs = 12\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  (Look for test set accuracy from Ks=1 to Ks=12)\n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n\nmean_acc","40a854e8":"#Calculate the accuracy of KNN for different values of k\nKss = 12\nmean_acc1 = np.zeros((Kss-1))\nstd_acc1 = np.zeros((Kss-1))\n\nfor n in range(1,Kss):\n    \n    #Train Model and Predict  (Look for train set accuracy from Kss=1 to Kss=12)\n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat_train=neigh.predict(X_train)\n    mean_acc1[n-1] = metrics.accuracy_score(y_train, yhat_train)\n\n    \n    std_acc1[n-1]=np.std(yhat_train==y_train)\/np.sqrt(yhat_train.shape[0])\n\nmean_acc1","1a517a9e":"#Plot the model test accuracy for a different number of neighbors\nplt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\nplt.legend(('Accuracy ', '+\/- 1xstd','+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()\n\nprint( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1)\n\n\n#Training\nk = 8\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh\n\n#Predicting\nyhat = neigh.predict(X_test)\nyhat[0:5]\n\n#Accuracy evaluation\nfrom sklearn import metrics\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))","84a14102":"# NAIVE BAYES CLASSIFIER\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel_gnb=GaussianNB()\nmodel_gnb.fit(X_train, y_train)","0519c44b":"y_predict4=model_gnb.predict(X_test)","c85fbef7":"print(classification_report(y_test, y_predict4))\n","89c39b5e":"cm = confusion_matrix(y_test, y_predict4)\nsns.heatmap(cm, annot = True)","cbb7b15a":"# Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\nmodel_dt = DecisionTreeClassifier()\nmodel_dt.fit(X_train,y_train)","a3035377":"y_predict5=model_dt.predict(X_test)","7334a37a":"print(classification_report(y_test, y_predict5))","50997c39":"cm = confusion_matrix(y_test, y_predict5)\nsns.heatmap(cm, annot = True)","d9fac3a4":"# Logistic Regression\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR\n\n\n","bf6fd7e8":"y_predict6 = LR.predict(X_test)","11fb7333":"print(classification_report(y_test, y_predict6))","138b944a":"cm = confusion_matrix(y_test, y_predict6)\nsns.heatmap(cm, annot = True)","bce7d514":"# Print all results of each algorithm\nprint(\"Random Forest Classifier accuracy :\", model_rf.score(X_test,y_test))\nprint(\"SVM accuracy :\", model_svm.score(X_test,y_test))\nprint(\"KNN accuracy :\", model_knn.score(X_test,y_test))\nprint(\"Naive Bayes accuracy :\", model_gnb.score(X_test,y_test))\nprint(\"Decision Tree accuracy :\", model_dt.score(X_test,y_test))\nprint(\"Logistic Regression :\", LR.score(X_test,y_test))","9af0df70":"# Comparison of Classication_report across vaious machine leanirnig method\nprint(\"Random Forest Classifier(RFC):\")\nprint(\"RFC:\",classification_report(y_test, y_predict1))\nprint()\nprint(\"SVM:\", classification_report(y_test, y_predict2))\nprint()\nprint(\"KNN:\")\nprint(classification_report(y_test, y_predict3))\nprint()\nprint(\"Naive Bayes:\")\nprint(classification_report(y_test, y_predict4))\nprint()\nprint(\"Decision Tree:\")\nprint(classification_report(y_test, y_predict5))\nprint(\"Logistic Regression:\")\nprint(classification_report(y_test, y_predict6))","593566d9":"# ROC curve\nfrom sklearn.metrics import roc_curve\nfpr1, tpr1, thresh1 = roc_curve(y_test, model_rf.predict_proba(X_test)[:, 1], pos_label = 1)\nfpr2, tpr2, thresh2 = roc_curve(y_test, model_svm.predict_proba(X_test)[:, 1], pos_label = 1)\nfpr3, tpr3, thresh3 = roc_curve(y_test, model_knn.predict_proba(X_test)[:, 1], pos_label = 1)\nfpr4, tpr4, thresh4 = roc_curve(y_test, model_gnb.predict_proba(X_test)[:, 1], pos_label = 1)\nfpr5, tpr5, thresh5 = roc_curve(y_test, model_dt.predict_proba(X_test)[:, 1], pos_label = 1)\nfpr6, tpr6, thresh6 = roc_curve(y_test, LR.predict_proba(X_test)[:, 1], pos_label = 1)","18459426":"# AUC score\n\nfrom sklearn.metrics import roc_auc_score\nauc_score1 = roc_auc_score(y_test, model_rf.predict_proba(X_test)[:, 1])\nauc_score2 = roc_auc_score(y_test, model_svm.predict_proba(X_test)[:, 1])\nauc_score3 = roc_auc_score(y_test, model_knn.predict_proba(X_test)[:, 1])\nauc_score4 = roc_auc_score(y_test, model_gnb.predict_proba(X_test)[:, 1])\nauc_score5 = roc_auc_score(y_test, model_dt.predict_proba(X_test)[:, 1])\nauc_score6 = roc_auc_score(y_test, LR.predict_proba(X_test)[:, 1])\n\n\n\nprint(\"Random Forest: \", auc_score1) \nprint(\"Support Vector Machine: \", auc_score2)\nprint(\"K-Nearest Neighbors: \", auc_score3) \nprint(\"Naive Bayes: \", auc_score4) \nprint(\"Decision Tree: \", auc_score5) \nprint(\"Logistric Regression: \", auc_score6) ","8b0a6756":"plt.plot(fpr1, tpr1, linestyle = \"--\", color = \"orange\", label = \"Random Forest\")\nplt.plot(fpr2, tpr2, linestyle = \"--\", color = \"black\", label = \"SVM\")\nplt.plot(fpr3, tpr3, linestyle = \"--\", color = \"purple\", label = \"KNN\")\nplt.plot(fpr4, tpr4, linestyle = \"--\", color = \"green\", label = \"Naive bayes\")\nplt.plot(fpr5, tpr5, linestyle = \"--\", color = \"red\", label = \"Decision Tree\")\nplt.plot(fpr6, tpr6, linestyle = \"--\", color = \"blue\", label = \"Logistic Regressoin\")\n\n#plt.title(\"Receiver Operator Characteristics ROC\")\n\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive rate\")\n\nplt.legend(loc = 'best')\nplt.savefig('ROC', dpi = 300)\nplt.show()","b35e1393":"Phone Service, Online Security, TchSupport, MultipleLines, OnlineBackip, DeviceProtection, Srnior Citizen, StreamingMovies, StreamingTV, PaperlessBilling, MonthlyChrges, PaymentMethod_Electronic check, InternetService_Fiber optic and Contract_Month-to-month have a positive relationship.","6d34368e":"# Machine Learning Method","17ed72eb":"# Data preparation for training","55bbbe56":"# Conclusion\n\nSupport Vector Machine model is the best machine learning model because it produces the best AUC score and F1 Score. Contrastly, the Decision Tree model produces the worst AUC score\n\nCustomers having fiber optic internet service, using electronic payment and having month-to-month contract are tend to churn away from telco.\n","d3e5c338":"Customers who have lhigher monthly charges and shorter tenure are tend to churn more.","3db841e1":"# PLOT ROC CURVES & AUC SCORE LOOK UP FOR THE 5 MODELS AND FIND AUC SCORES","34eedabf":"Various machine learning methods are applied on Telco customers churn dataset (WAFn-UseC-Telco-Customer-Churn.csv)\n\n* Random Forest Classifier\n* Support Vector Machine Classifier\n* K-Nearest Neighbour(KNN)\n* Naive Bayes\n* Decision Tree Classification"}}