{"cell_type":{"ab5e5179":"code","3db93343":"code","e77fc85c":"code","133a846b":"markdown","91a2dd66":"markdown","bf30147a":"markdown","a7e26abf":"markdown","380d3209":"markdown","bd821f9e":"markdown"},"source":{"ab5e5179":"# This is an example. \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n\ncust_df = pd.read_csv('..\/input\/santander-customer-satisfaction\/train.csv')\n\ncust_df['var3'].value_counts()\ncust_df['var3'].replace(-999999, 2, inplace = True)\ncust_df.drop('ID', axis = 1, inplace = True)\n\nX_feature = cust_df.iloc[: , :-1]\ny_label = cust_df.iloc[:,-1]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_feature, y_label, test_size = 0.2, random_state = 56)\nxgb_clf = XGBClassifier(n_estimators = 100, random_state = 156)\nxgb_clf.fit(X_train, Y_train, early_stopping_rounds = 100,eval_metric ='auc', eval_set = [(X_test, Y_test)])  ","3db93343":"from xgboost import plot_importance\nimport matplotlib.pyplot as plt\nfig , ax = plt.subplots(1, 1, figsize = (10, 10))\nplot_importance(xgb_clf, max_num_features= 20, ax = ax, height = 0.4)","e77fc85c":"from xgboost import plot_tree\nfig, ax = plt.subplots(figsize=(80, 80))\nxgboost.plot_tree(xgb_clf, num_trees=4, ax=ax)\nplt.show()","133a846b":"# XGBoost \n\n- <h1>Hyperparameter<\/h1>\n\n    - eta,learning_rate : Learning Rate (0~1)\n        - (\ud559\uc2b5\ub960 (0~1))\n    - n_estimators : Number of Weak Learner (ex. Decision Tree), (default: 10)\n        - (\uc57d\ud55c \ud559\uc2b5\uae30\uc758 \uac1c\uc218)\n    - min_child_weight : Minimum sum of weights for all observations needed in child. (default: 1)\n        - (child \uc5d0 \ud544\uc694\ud55c \ubaa8\ub4e0 \uad00\uce21\uc9c0\uc5d0 \ub300\ud55c \uad00\uce21\uce58\uc5d0 \ub300\ud55c \uac00\uc911\uce58\uc758 \ucd5c\uc18c \ud569)\n    - max_depth : Tree max_depth (default: 6)\n        - (\ud2b8\ub9ac\uc758 \uae4a\uc774 \ub0ae\uc73c\uba74 \ub0ae\uc744\uc218\ub85d \uacfc\uc801\ud569\uc744 \ubc29\uc9c0\ud569\ub2c8\ub2e4.)\n    - subsample : Data Sampling rate for tree\n        - (\uac01 \ud2b8\ub9ac\ubcc4 \ub370\uc774\ud130 \uc0d8\ud50c\ub9c1 \ube44\uc728)\n    - colsample_bytree :  feature sampling rate for tree\n        - (\uac01 \ud2b8\ub9ac\ubcc4 feature \uc0d8\ud50c\ub9c1 \ube44\uc728)\n    - gamma : It is the minmum loss reduction value that will determine the further division of leaf node\n        - (\ub9ac\ud504\ub178\ub4dc\uc758 \ucd94\uac00\ubd84\ud560\uc744 \uacb0\uc815\ud560 \ucd5c\uc18c\uc190\uc2e4 \uac12\uc774\ub2e4.)\n    - reg_lambda : L2 Regulation\n        - (L2 \uac00\uc911\uce58)\n    - reg_alpha : L1 Regulation\n        - (L1 \uac00\uc911\uce58)\n    - scale_pos_weight : Balancing unbalanced datasets\n        - (\ubd88\uade0\ud615 \ub370\uc774\ud130\uc14b\uc758 \uade0\ud615 \uc720\uc9c0)\n\n\n<h3>If you need to use GridSearchCV to find best_params_<\/h3>\n\n- I'm recommend that you start with the largest range and work your way down to finding best_params_.\n    - \ud07c \ubc94\uc704\ubd80\ud130 \uc2dc\uc791\ud574\uc11c \ubc94\uc704\ub97c \uc881\ud600\uac00\ub294 \ubc29\uc2dd\uc73c\ub85c best_params_\uc744 \ucc3e\ub294 \ubc29\uc2dd\uc744 \ucd94\ucc9c\ub4dc\ub9bd\ub2c8\ub2e4.\n\n<h3>If you make visualization about xgboost<\/h3>\n\n- XGB_model.plot_importance : indicate the importance of a characteristic\n    - \uac01 feature\uc758 \uc911\uc694\ub3c4\ub97c \uc2dc\uac01\ud654 \ud558\uc5ec \ub098\ud0c0\ub0bc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n- XGB_model.plot_tree : indicate the Decision Tree\n    - \uc758\uc0ac \uacb0\uc815\ud2b8\ub9ac\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n\n\n* Reference : https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html    ","91a2dd66":"# XGBoost Plot_imortance\n- you can visualize the importance of features.\n    - (plot_importance\ub97c \ud65c\uc6a9\ud558\uc5ec \ud2b9\uc131\uc758 \uc911\uc694\ub3c4\ub97c \uc2dc\uac01\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.)","bf30147a":"# Before We discussing XGBoost, let's briefly talk about Boosting\n- (XGBoost\uc5d0 \uad00\ud558\uc5ec \uc124\uba85\ub4dc\ub9ac\uae30 \uc804\uc5d0 Boosting\uc5d0 \uad00\ud558\uc5ec \uac04\ub7b5\ud558\uac8c \uc9da\uace0 \ub118\uc5b4\uac00\uaca0\uc2b5\ub2c8\ub2e4.)\n\n### What's Boosting?\n- One of the ensemble techniques is to combine several Week learner's to improve performance.\n    - (\uc559\uc0c1\ube14 \uae30\ubc95\uc911 \ud558\ub098\ub85c \uc57d\ud55c \ud559\uc2b5\uae30 \uc5ec\ub7ec\uac1c\ub97c \ud569\ud558\uc5ec \uc131\ub2a5\uc744 \ub192\uc774\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4.)\n\n<img src = https:\/\/itwiki.kr\/images\/4\/45\/%EB%B6%80%EC%8A%A4%ED%8C%85%28Boosting%29.png>\n\n","a7e26abf":"# A brief description of XGBoost\n<strong><h4>Please Upovote This kernel, if it was useful or helpful!<\/h4><\/strong>\n<img src = https:\/\/www.h2o.ai\/wp-content\/uploads\/2018\/07\/xgboost-narrow.png>","380d3209":"# What's XGBoost?\n\n- XGBoost stands for Extreme Gradient Boosting.\n    - (XGBoost\ub294 Extend Graident Boosting\uc758 \uc57d\uc790\uc785\ub2c8\ub2e4.)\n- The algorithm implemented using the boosting technique is representative of Gradient Boost. XGBoost is the implementation of this algorithm in parllel operation.\n    - (\ubd80\uc2a4\ud305 \uae30\ubc95\uc744 \uc774\uc6a9\ud558\uc5ec \uad6c\ud604\ub41c \uc54c\uace0\ub9ac\uc998\uc740 Gradient Boost\uc785\ub2c8\ub2e4. \uc774 \uc54c\uace0\ub9ac\uc998\uc5d0 \ubcd1\ub82c \ud559\uc2b5\uc744 \uc774\uc6a9 \uac00\ub2a5\ud558\ub3c4\ub85d \ud55c \uac83\uc774 XGBoost \uc785\ub2c8\ub2e4.)\n- It's the leading machine learning library for regression, classification.\n    - (\uba38\uc2e0\ub7ec\ub2dd \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c \ubd84\ub958, \ud68c\uadc0\uc5d0 \uad00\ud55c \ubaa8\ub378\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.)\n\n<img src = https:\/\/diya-blogpost.s3.us-east-1.amazonaws.com\/imgs_2020NLP\/xgboost\/XGBoost-feature.png>\n\n## Advantages of XGBoost(XGBoost\uc758 \uc7a5\uc810)\n- A large and growing list of data scientist globally that are actively contributing to XGBoost open source development.\n    - (\ub9ce\uc740 \ub370\uc774\ud130 \uacfc\ud559\uc790\ub4e4\uc774 XGBoost\uc758 \uc624\ud508 \uc18c\uc2a4 \uac1c\ubc1c\uc5d0 \uc801\uadf9\uc801\uc73c\ub85c \uae30\uc5ec\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.)\n- Usage on a wide range of applications, including solving problems in regression, classification and user-defined prediction challenges.\n    - (XGBoost\ub294 \uad11\ubc94\uc704\ud558\uac8c applications\uc5d0\uc11c \uc0ac\uc6a9\ub418\uace0 \uc788\uc73c\uba70, \ud68c\uadc0, \ubd84\ub958\ub97c \ud3ec\ud568\ud558\uc5ec \ub2e4\uc591\ud55c \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294\ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.)\n- A library that was built from the ground up to be efficient, felxtible and portable\n    - (\ud6a8\uc728\uc801\uc774\uba70, \uc720\uc5f0\ud558\uac8c \ubcc0\uacbd \uac00\ub2a5\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\uc785\ub2c8\ub2e4.)\n- By using Greedy-Algorithm, Automatic pruning is possible. Therefore, Overfitting rarely occurs.\n    - (\uadf8\ub9ac\ub514 \uc54c\uace0\ub9ac\uc998(\ud0d0\uc695\ubc95)\uc744 \uc0ac\uc6a9\ud568\uc73c\ub85c \uc368, \uc790\ub3d9 \uac00\uc9c0\uce58\uae30\uac00 \uac00\ub2a5\ud569\ub2c8\ub2e4. \uadf8\ub807\uac8c \ub428\uc73c\ub85c\uc368 \uacfc\uc801\ud569\uc774 \ub4dc\ubb3c\uac8c \ubc1c\uc0dd\ud569\ub2c8\ub2e4.)\n     \n\n## Disadvantages of XGBoost(XGBoost\uc758 \ub2e8\uc810)\n- Although the speed problem, which is a disadvantage of GBM, has been solved to extent, but it is still slow.\n    - (GBM\uc758 \ub2e8\uc810\uc778 \uc18d\ub3c4\ubb38\uc81c\ub97c \ud574\uacb0\ud558\uc600\uc9c0\ub9cc, \uadf8\ub798\ub3c4 \ub290\ub9bd\ub2c8\ub2e4.)\n- If Hyperparameter modify use GridSearchCV, Speed is very slow.\n    - (\ub9cc\uc57d GridSearchCV\ub97c \uc0ac\uc6a9\ud558\uc5ec, \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \ubcc0\uacbd\ud560 \uacbd\uc6b0, \uc18d\ub3c4\ub294 \ud604\uc800\ud788 \ub290\ub9bd\ub2c8\ub2e4.)    ","bd821f9e":"# XGBoost plot_tree\n- you can visualize Decision tree\n    - (\uc758\uc0ac\uacb0\uc815\ud2b8\ub9ac\ub97c \ud65c\uc6a9\ud558\uc5ec \ubd84\ub958\ub97c \uc5b4\ub5bb\uac8c \ud558\uc600\ub294\uc9c0 \uc2dc\uac01\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.)"}}