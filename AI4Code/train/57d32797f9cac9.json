{"cell_type":{"36dc1b2c":"code","8ab90dcc":"code","45eb714d":"code","19a981b1":"code","9a7fc7bb":"code","a720434a":"code","fb4dc226":"code","b2e9615f":"code","9f79e56f":"code","7f7361c2":"code","9679875f":"code","9fda9806":"code","edb73ca8":"code","bc9f1890":"code","d337c5c5":"code","d026c637":"code","dccd9e8b":"code","76b016f4":"markdown","8e5ccbf9":"markdown"},"source":{"36dc1b2c":"#\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\nimport os\nimport torch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchvision.transforms as transforms\n\nfrom torch.utils.data import Dataset, DataLoader\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport tqdm\n\n# \u0423\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c random seed'\u044b \u0434\u043b\u044f \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.backends.cudnn.deterministic = True","8ab90dcc":"# \u0421\u043a\u0430\u0447\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 MNIST (\u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0440\u0443\u043a\u043e\u043f\u0438\u0441\u043d\u044b\u0435 \u0446\u0438\u0444\u0440\u044b \u043e\u0442 0 \u0434\u043e 9 )\nsample = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv') # \u041e\u0431\u0440\u0430\u0437\u0435\u0446 \u043f\u043e\u0441\u044b\u043b\u043a\u0438 \u043d\u0430 kaggle\ntrain_df = pd.read_csv('..\/input\/digit-recognizer\/train.csv') # \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\ntest_df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')  # \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0439 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u0434\u043b\u044f \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438 \u043d\u0430 kaggle\nprint(f\"Number of training samples: {len(train_df)}\")\nprint(\"Any missing data? \", train_df.isnull().values.any())\nprint(f\"Dataframe shape: {train_df.shape}\")","45eb714d":"X, y = train_df.drop(labels = 'label', axis=1), train_df['label']","19a981b1":"# \u0420\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u0438 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0443\u044e\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","9a7fc7bb":"# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c DataFrame \u0432 Tensor\nX_train = torch.tensor(X_train.values)\nX_test = torch.tensor(X_test.values)\ny_train = torch.tensor(y_train.values)\ny_test = torch.tensor(y_test.values)","a720434a":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0442\u0438\u043f \u043d\u0430\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\nX_train.dtype, y_train.dtype","fb4dc226":"# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u0434\u0440\u043e\u0431\u043d\u044b\u0435 \u0447\u0438\u0441\u043b\u0430\nX_train = X_train.float()\nX_test = X_test.float()","b2e9615f":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430. \u041f\u0435\u0440\u0432\u043e\u0435 \u0447\u0438\u0441\u043b\u043e - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439, 2-\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0438\u043a\u0441\u0435\u043b\u0435\u0439(28\u044528)\nX_train.shape, X_test.shape","9f79e56f":"y_train.shape, y_test.shape","7f7361c2":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0447\u0438\u0441\u043b\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 matplotlib\nimport matplotlib.pyplot as plt\nplt.imshow(X_train.reshape(-1, 28, 28)[4,:,:])\nplt.show()\nprint(y_train[4])\n","9679875f":"# \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u043a\u0443\nclass MNISTNet(torch.nn.Module):\n    def __init__(self, n_hidden_neurons):\n        super(MNISTNet, self).__init__()\n        self.fc1 = torch.nn.Linear(28 * 28, n_hidden_neurons)\n        self.ac1 = torch.nn.Sigmoid()\n        self.fc2 = torch.nn.Linear(n_hidden_neurons, 10) \n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.ac1(x)\n        x = self.fc2(x)\n        return x\n    \nmnist_net = MNISTNet(100)","9fda9806":"# \u041f\u0435\u0440\u0435\u0432\u0435\u0434\u0435\u043c \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043d\u0430 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0443\ntorch.cuda.is_available()  # \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u0430 \u043b\u0438 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0430","edb73ca8":"# \u041a\u043e\u043d\u0441\u043e\u043b\u044c\u043d\u0430\u044f \u0443\u043a\u043e\u043c\u0430\u043d\u0434\u0430 \u0434\u043b\u044f \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u0430 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0435\n!nvidia-smi","bc9f1890":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmnist_net = mnist_net.to(device)  # \u041f\u0435\u0440\u0435\u043b\u043e\u0436\u0438\u043c \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u043d\u0430 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0443\nlist(mnist_net.parameters())  # \u0423\u0431\u0435\u0434\u0438\u043c\u0441\u044f \u0447\u0442\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u044e\u0442\u0441\u044f \u043d\u0430 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u044b","d337c5c5":"# \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043b\u043e\u0441\u0441 \u0444\u0443\u043d\u043a\u0446\u0438\u044e - \u043a\u0440\u043e\u0441\u0441\u044d\u043d\u0442\u0440\u043e\u043f\u0438\u044f (\u0441\u043e\u0444\u0442\u043c\u0430\u043a\u0441 \u0443\u0436\u0435 \u0432\u0441\u0442\u0440\u043e\u0435\u043d \u0432 \u043d\u0435\u0435)\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(mnist_net.parameters(), lr=1.0e-3) # \u0412\u044b\u0431\u043e\u0440 \u043c\u0435\u0442\u043e\u0434\u0430 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 loss \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 Adam","d026c637":"# \u0420\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u043f\u043e\u0431\u0430\u0442\u0447\u0435\u0432\u044b\u0439 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a\nbatch_size = 100 # \u0420\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430 100\n\ntest_accuracy_history = []\ntest_loss_history = []\n\nX_test = X_test.to(device)\ny_test = y_test.to(device)\n\nfor epoch in range(200): # \u0414\u043b\u044f \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u0438 \u0432\u043e\u0437\u044c\u043c\u0443 \u0442\u043e\u043b\u044c\u043a\u043e 200 \u044d\u043f\u043e\u0445, \u043d\u043e \u043b\u0443\u0447\u0448\u0435 \u0432\u0437\u044f\u0442\u044c \u043f\u043e\u0431\u043e\u043b\u044c\u0448\u0435\n    order = np.random.permutation(len(X_train))\n    \n    for start_index in range(0, len(X_train), batch_size):\n        optimizer.zero_grad()\n        \n        batch_indexes = order[start_index:start_index+batch_size]\n        \n        X_batch = X_train[batch_indexes].to(device)\n        y_batch = y_train[batch_indexes].to(device)\n        \n        preds = mnist_net.forward(X_batch) \n        \n        loss_value = loss(preds, y_batch)\n        loss_value.backward()\n        \n        optimizer.step() \n\n    test_preds = mnist_net.forward(X_test)\n    test_loss_history.append(loss(test_preds, y_test).data.cpu())\n    \n    accuracy = (test_preds.argmax(dim=1) == y_test).float().mean().data.cpu()\n    test_accuracy_history.append(accuracy)\n    print(accuracy)","dccd9e8b":"# \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0444\u0430\u0439\u043b \u043f\u043e\u0441\u044b\u043b\u043a\u0438 \u0434\u043b\u044f kaggle:\ntest_df = torch.tensor(test_df.values)\ntest_df = test_df.float()\npreds = mnist_net.forward(test_df.to(device))\npreds = preds.argmax(dim=1).cpu().numpy()\nsample['Label'] = pd.DataFrame(preds)\nsample.to_csv('submission.csv',index=False)\n#\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432 \u0446\u0435\u043b\u043e\u043c \u043d\u0435\u043f\u043b\u043e\u0445\u043e\u0439 (0.95764), \u043e\u0434\u043d\u0430\u043a\u043e \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u043c\u043d\u043e\u0433\u043e \u043b\u0443\u0447\u0448\u0435 (\u0443 \u043c\u0435\u043d\u044f \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 CNN \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0431\u044b\u043b 0.98771)","76b016f4":"# \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0440\u0443\u043a\u043e\u0438\u0441\u043d\u044b\u0445 \u0446\u0438\u0444\u0440 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438(FNN)","8e5ccbf9":"\u0417\u0430 \u043e\u0441\u043d\u043e\u0432\u0443 \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u043a\u0435\u0440\u043d\u0435\u043b\u0430 \u0432\u0437\u044f\u0442 \u043e\u0434\u0438\u043d \u0438\u0437 \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u043e\u0432 \u0438\u0437 \u043a\u0443\u0440\u0441\u0430 \u043f\u043e \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u043e\u043c\u0443 \u0437\u0440\u0435\u043d\u0438\u044e \u043e\u0442 Samsung \u043d\u0430 \u0441\u0430\u0439\u0442\u0435 Stepik.org \u0421\u043e\u0432\u0435\u0442\u0443\u044e \u043f\u0435\u0440\u0435\u0434 \u043f\u0440\u043e\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435\u043c \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0439 \u043f\u043e \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u043e\u043c\u0443 \u0437\u0440\u0435\u043d\u0438\u044e \u043e\u0437\u043d\u0430\u043a\u043e\u043c\u0438\u0442\u044c\u0441\u044f \u0441 \u0434\u0430\u043d\u043d\u044b\u043c \u043a\u0443\u0440\u0441\u043e\u043c, \u043e\u0447\u0435\u043d\u044c \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e(\u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u044d\u0442\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e) \u043e\u0431\u044a\u044f\u0441\u043d\u044f\u044e\u0442 \u0440\u0435\u0431\u044f\u0442\u0430: https:\/\/stepik.org\/course\/50352\/info  \n\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u043a\u0435\u0440\u043d\u0435\u043b\u0435 \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442\u0441\u044f \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u0430\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c. \u041e\u043d\u0430 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u0430 \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a, \u043d\u043e \u0434\u043b\u044f \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0438 \u043f\u043e\u0434\u043e\u0439\u0434\u0435\u0442. \u0423 \u043c\u0435\u043d\u044f \u0442\u0430\u043a\u0436\u0435 \u0435\u0441\u0442\u044c \u043a\u0435\u0440\u043d\u0435\u043b, \u0433\u0434\u0435 \u0440\u0435\u0430\u043b\u0438\u0438\u0437\u0443\u0435\u0442\u0441\u044f \u0431\u043e\u043b\u0435\u0435 \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c CNN \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439, \u0432\u043e\u0442 \u0441\u0441\u044b\u043b\u043a\u0430: https:\/\/www.kaggle.com\/alekseysergeev\/pytorch-cnn-lenet5-russian-version  \n\u0431\u0443\u0434\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u0442\u0435\u043b\u0435\u043d \u0437\u0430 \u0432\u0430\u0448\u0438 \u0430\u043f\u0432\u043e\u0443\u0442\u044b)"}}