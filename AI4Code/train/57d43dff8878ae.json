{"cell_type":{"2ddfb077":"code","fad5bf12":"code","61bc4ce8":"code","b5702971":"code","6841f086":"code","427b1eaf":"code","ac82a774":"code","76aec4f2":"code","dbc32b21":"code","f73873f4":"code","4456ed0a":"code","0607dfeb":"code","653c7a12":"code","68c530be":"code","20539c3b":"code","26b39508":"code","9ffe98a8":"code","71a6b79b":"code","7f0c2867":"code","99a174ff":"code","8a615ed0":"code","8dc972ca":"code","08f97cf4":"code","721f96ce":"code","715c7310":"code","0b2292e1":"code","83a2fa27":"code","b8cbe8d0":"code","136f1cff":"code","02db743f":"code","d6efb164":"code","faf1f5f1":"code","83ff75ff":"code","b370d94e":"code","e07c30ad":"code","796855df":"code","a90f2668":"code","c5929518":"code","97af63a1":"code","d93727b9":"code","ebd72d11":"code","ac95179d":"code","26ce6816":"code","ca840805":"code","8c467d4d":"code","25e8d9c4":"code","f273fce5":"code","981986f1":"code","0425035e":"code","8a50150a":"code","aeda35d1":"code","3bd6ca39":"code","8adca6a5":"code","8b494efb":"code","0957883f":"code","3de07aed":"code","b5ba8d80":"code","20e9f557":"code","b6d6b221":"code","f5d83274":"code","dcfc279d":"code","46c2db3f":"code","8ff08eac":"code","7e43c969":"code","f1b49794":"code","3737e234":"markdown","b6161e0d":"markdown","1f7c2366":"markdown","f47c5da9":"markdown","1a577a48":"markdown","12f910fc":"markdown","bf03c2e9":"markdown"},"source":{"2ddfb077":"!pip install pyspellchecker","fad5bf12":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom gensim.models import KeyedVectors\nimport nltk\n\nimport re\nimport time\nimport gc\nfrom string import punctuation\nfrom pprint import pprint\nfrom functools import reduce\nfrom ast import literal_eval\n\nimport json\nimport scipy\nimport numpy as np\nimport pandas as pd\nimport spacy\nfrom nltk.corpus import stopwords\n# import spellchecker\nfrom spellchecker import SpellChecker\nfrom matplotlib import pyplot\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nnlp = spacy.load('en')\n\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","61bc4ce8":"sample_subm = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n\ndef make_submission(predictions, filename='.\/sumbission.csv'):\n    pd.DataFrame({\n        'id': sample_subm.id.values,\n        'target': predictions\n    }).to_csv(filename, index=False, header=True)","b5702971":"EMBEDDINGS_PATH = '..\/input\/glovetwitter100d\/glove.twitter.27B.100d.txt'\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDINGS_PATH))","6841f086":"# embeddings.get('goooooaaaaal', None)","427b1eaf":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n# train.head()","ac82a774":"print(f'Number of samples: {train.shape[0]}')\nprint(f'Target_distribution: {train.target.value_counts(normalize=True).round(5).to_dict()}')\nprint('NaN ratio per column:')\nfor column in train.columns:\n    print(f'\\t{column}: {train[column].isna().sum()}\/{train.shape[0]}')\nfor target in [0, 1]:\n    print(f'5 texts for {target} target:')\n    for text in train[train.target == target]['text'].head().to_list():\n        print(f'\\t{text}')\n","76aec4f2":"CONTRACTIONS = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n'&amp': 'and'\n} #contractions_dict from https:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\n\nHTTP_PATTERN = re.compile('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\nEMOJI_PATTERN = re.compile(\"[\"\n                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                       u\"\\U00002702-\\U000027B0\"\n                       u\"\\U000024C2-\\U0001F251\"\n                       \"]+\", flags=re.UNICODE)\nSPECWORDS_PATTERN = re.compile('((?:(?:[^a-z\\s\\#\\@]+?[a-z]+[^a-z\\s\\#\\@]*?)|(?:[^a-z\\s\\#\\@]*?[a-z]+[^a-z\\s\\#\\@]+))+)', flags=re.IGNORECASE)#|(?:.*[a-zA-Z]+.+)\nCONTRACTIONS_PATTERN = re.compile('(%s)' % '|'.join(CONTRACTIONS.keys()))\nSTOPWORDS = stopwords.words('english')\n\ndef remove_html(text):\n    return HTTP_PATTERN.sub('',text)\n\ndef remove_emoji(txt):\n    return EMOJI_PATTERN.sub('', txt)\n\ndef remove_punct(txt):\n    table = str.maketrans('', '', punctuation)\n    return txt.translate(table)\n\ndef remove_stopwords(txt):\n    txt = txt.split()\n    return ' '.join([word for word in txt if word not in STOPWORDS])\n\ndef contract_text(txt):\n    txt = re.sub('([^\\s])[\\.\\:,\\(\\)\\?\\!\\^\\;\\=]+', '\\g<1> ', txt.lower())\n\n    def expand_contractions(s):\n        def replace(match):\n            return CONTRACTIONS[match.group(0)]\n        return CONTRACTIONS_PATTERN.sub(replace, s)\n    \n    txt = re.sub('(?:w\/|-\\s|\\=\\>)', '', expand_contractions(txt).replace('&', ' and '))\n    txt = re.sub('(\\s+)', ' ', txt)\n    \n    return txt.strip()\n\ndef extract_hashtag(txt):\n    return ' '.join([x.strip() for x in re.findall(r'\\#(.*?)(?:\\s|$)', txt)])\n\ndef extract_citations(txt):\n    return [x.strip() for x in re.findall(r'\\'(.*?)\\'', txt)]\n\n\ndef lemmatize(txt):\n    return ' '.join([token.lemma_ for token in nlp(txt)]).strip()\n\ndef lemmatize_hashtags(lst):\n    return [token.lemma_ for txt in lst for token in nlp(txt)]\n\ndef lemmatize_citations(lst):\n    return [' '.join([token.lemma_ for token in nlp(txt)]).strip() for txt in lst]\n\n\ndef drop_hashtags(txt):\n    txt = re.sub(r'\\#.*?(?:\\s|$)', '', txt).strip()\n    txt = re.sub(r'\\s+', ' ', txt)\n    return txt\n\ndef drop_at_words(txt):\n    txt = re.sub(r'\\@.*?(?:\\s|$)', '', txt).strip()\n    txt = re.sub(r'\\s+', ' ', txt)\n    return txt\n\ndef drop_citations(txt):\n    txt = re.sub(r'\\'.*?\\'', '', txt).strip()\n    txt = re.sub(r'\\s+', ' ', txt)\n    return txt\n\ndef word_count(txt):\n    return len(txt.split())\n\n\nspell = SpellChecker()\ndef unknown_words_fraction(text):\n    uknown_words = 0\n    txt_split = text.split()\n    misspelled_words = spell.unknown(txt_split)\n    for word in txt_split:\n        if word in misspelled_words:\n            uknown_words += 1\n    return uknown_words \/ len(txt_split) if len(txt_split) > 1 else 0","dbc32b21":"class BaseFeatureGenerator:\n    \n    def __init__(self):\n        self.isTrain = True\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        X['contain_http'] = X['text'].str.contains('http')\n\n        X['text'] = X.text.apply(lambda x: remove_html(x))\n        X['text'] = X.text.apply(lambda x: remove_emoji(x))\n\n        if self.isTrain:\n            print(f'Before droping duplicates: {X.shape[0]}')\n            X.drop_duplicates('text', inplace=True)\n            print(f'After droping duplicates: {X.shape[0]}')\n\n        X['contain_at'] = X['text'].str.contains('@')\n        X['contain_hashtag'] = X['text'].str.contains('#')\n        X['at_count'] = X.text.apply(lambda x: x.count('@'))\n        #drop @ words\n        X.loc[X['at_count']>0, 'text'] = X.loc[X['at_count']>0, 'text'].apply(lambda x: drop_at_words(x))\n\n        X['text_len'] = X.text.str.len()\n\n        X['clean_text'] = X.text.apply(lambda x: contract_text(x))\n\n        #extract hashtags\n        X['hashtag'] = X.clean_text.apply(lambda x: extract_hashtag(x))\n        X['hashtag_count'] = X.text.apply(lambda x: x.count('#'))\n        #extract citations\n        X['citations'] = X.clean_text.apply(lambda x: extract_citations(x))\n        X['citations_count'] = X.citations.apply(len)\n\n        #drop hashtags\n        X['text_no_hashtags'] = X.clean_text.values\n        X.loc[X['hashtag_count']>0, 'text_no_hashtags'] = X.loc[X['hashtag_count']>0, 'text_no_hashtags'].apply(lambda x: drop_hashtags(x))\n\n        #drop citations\n        X['text_no_citations'] = X.clean_text.values\n        X.loc[X['citations_count']>0, 'text_no_citations'] = X.loc[X['citations_count']>0, 'text_no_citations'].apply(lambda x: drop_citations(x))\n\n        #drop citations and hashtags\n        X['text_no_hashtags_no_citations'] = X.text_no_hashtags.values\n        X.loc[X['citations_count']>0, 'text_no_hashtags_no_citations'] = X.loc[X['citations_count']>0, 'text_no_hashtags_no_citations'].apply(lambda x: drop_citations(x))\n        return X\n\nclass SpecificFeatureGenerator:\n    \n    def __init__(self, remove_punctuation=True, is_remove_stopwords=True, is_lemmatize=True):\n        self.remove_punctuation = remove_punctuation\n        self.is_remove_stopwords = is_remove_stopwords\n        self.is_lemmatize = is_lemmatize\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        \n        if self.remove_punctuation:\n            for cl in ['clean_text', 'text_no_hashtags', 'text_no_citations', 'text_no_hashtags_no_citations', 'hashtag']:\n                X[cl] = X[cl].apply(lambda x: remove_punct(x))\n            X.loc[X['citations_count']>0, 'citations'] = X.loc[X['citations_count']>0, 'citations'].apply(lambda x: [remove_stopwords(y) for y in x])\n    #         for cl in ['hashtag', 'citations']:\n    #             X.loc[X['%s_count'%cl]>0, cl] = X.loc[X['%s_count'%cl]>0, cl].apply(lambda x: [remove_punct(y) for y in x])\n\n        if self.is_remove_stopwords:\n            for cl in ['clean_text', 'text_no_hashtags', 'text_no_citations', 'text_no_hashtags_no_citations', 'hashtag']:\n                X[cl] = X[cl].apply(lambda x: remove_stopwords(x))\n            X.loc[X['citations_count']>0, 'citations'] = X.loc[X['citations_count']>0, 'citations'].apply(lambda x: [remove_stopwords(y) for y in x])\n    #         for cl in ['hashtag', 'citations']:\n    #             X.loc[X['%s_count'%cl]>0, cl] = X.loc[X['%s_count'%cl]>0, cl].apply(lambda x: [remove_stopwords(y) for y in x])\n\n        #word counting\n        for cl in ['clean_text', 'text_no_hashtags', 'text_no_citations', 'text_no_hashtags_no_citations']:\n            X[cl + '_word_count'] = X[cl].apply(lambda x: word_count(x))\n\n        if self.is_lemmatize:\n            for cl, func in [\n                               ('clean_text', lemmatize), \n                               ('text_no_hashtags', lemmatize), \n                               ('text_no_citations', lemmatize), \n                               ('text_no_hashtags_no_citations', lemmatize),\n                               ('hashtag', lemmatize),\n                               ('citations', lemmatize_citations)\n                            ]:\n                print('lemmatize %s...'%cl)\n                X['lemma_%s'%cl] = X[cl].apply(lambda x: func(x))\n                if cl not in ['hashtag', 'citations']:\n                    X['lemma_%s_unknown_words_fraction'%cl] = X['lemma_%s'%cl].apply(lambda x: unknown_words_fraction(x))\n\n        else:\n            for cl in ['clean_text', 'text_no_hashtags', 'text_no_citations', 'text_no_hashtags_no_citations']:\n                X['lemma_%s_unknown_words_fraction'%cl] = X[cl].apply(lambda x: unknown_words_fraction(x))\n\n        return X\n    \n    def set_params(self, remove_punctuation=True, is_remove_stopwords=True, is_lemmatize=True):\n        self.remove_punctuation = remove_punctuation\n        self.is_remove_stopwords = is_remove_stopwords\n        self.is_lemmatize = is_lemmatize","f73873f4":"def cross_validation(cv, model, X, y, metrics=[f1_score], verbose=True):\n    \n    scores = {}\n    for metric in metrics:\n        scores[metric.__name__] = {'train': [], 'val': []}\n    \n    for train_index, val_index in cv.split(X, y):\n        X_train, X_val, y_train, y_val = X.loc[train_index], X.loc[val_index], y[train_index], y[val_index]\n\n        model.fit(X_train, y_train)\n        \n        train_predictions_proba = model.predict_proba(X_train).T[1]\n        val_predictions_proba = model.predict_proba(X_val).T[1]\n\n        train_predictions = np.round(train_predictions_proba)\n        val_predictions = np.round(val_predictions_proba)\n\n        # metric calculation\n        for index, metric in enumerate(metrics):\n            if metric.__name__ in ['precision_recall_curve', 'roc_curve']:\n                train_score = auc(*metric(y_train, train_predictions_proba)[:2][::-1])\n                val_score = auc(*metric(y_val, val_predictions_proba)[:2][::-1])\n            else:\n                train_score = metric(y_train, train_predictions)\n                val_score = metric(y_val, val_predictions)\n\n            scores[metric.__name__]['train'].append(train_score)\n            scores[metric.__name__]['val'].append(val_score)\n            \n    for metric in metrics:\n        for key in ['train', 'val']:\n            scores[metric.__name__][f'{key}_mean'] = np.mean(scores[metric.__name__][key]).round(5)\n            scores[metric.__name__][key] = np.round(scores[metric.__name__][key], 5)\n        if verbose:\n            print(metric.__name__)\n            print(f\"Train: {scores[metric.__name__]['train']}, mean: {scores[metric.__name__]['train_mean']}\")\n            print(f\"Val: {scores[metric.__name__]['val']}, mean: {scores[metric.__name__]['val_mean']}\\n\")\n    \n    return scores\n    ","4456ed0a":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntm = time.time()\nfeature_generator = Pipeline([\n    ('baseFeatureGenerator', BaseFeatureGenerator()),\n    ('specificFeatureGenerator', SpecificFeatureGenerator()),\n])\nfeature_generator.fit(train, None)\n\ntrain = feature_generator[0].transform(train)\nfor remove_punctuation in [True]:\n    for is_remove_stopwords in [True]:\n        for is_lemmatize in [True]:\n# for remove_punctuation in [True, False]:\n#     for is_remove_stopwords in [True, False]:\n#         for is_lemmatize in [True, False]:\n            print('remove_punctuation=%s, is_remove_stopwords=%s, is_lemmatize=%s'%(remove_punctuation, is_remove_stopwords, is_lemmatize))\n            feature_generator[1].set_params(remove_punctuation, is_remove_stopwords, is_lemmatize)\n            feature_generator[1].transform(train).to_csv('.\/train_%s_%s_%s.csv'%(remove_punctuation, is_remove_stopwords, is_lemmatize), header=True, index=False)\nprint(time.time() - tm)","0607dfeb":"# train = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nfeature_generator[0].isTrain = False\nfeature_generator[1].set_params(True, True, True)\n\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest = feature_generator.transform(test)\ntest.clean_text = test.clean_text.fillna('')","653c7a12":"train = pd.read_csv('.\/train_%s_%s_%s.csv'%(True, True, True))\ntrain.clean_text = train.clean_text.fillna('')","68c530be":"def twit2vec(df):\n    vectors = []\n    nums = 0\n    for text in df['clean_text'].values:\n        valid_tokens = [token for token in nltk.tokenize.word_tokenize(text) if token in embeddings]\n#         valid_tokens = [token for token in text.split(' ') if token in embeddings]\n        if valid_tokens:\n            tokens_embeddings = [embeddings.get(token) for token in valid_tokens]\n            vectors.append(np.mean(tokens_embeddings, axis=0))\n        else:\n            nums += 1\n            vectors.append(np.zeros(100))\n    print('number of empty twits:', nums)\n    return vectors\n\n# twit_vectors = twit2vec(train)","20539c3b":"logreg = LogisticRegression(solver='lbfgs', max_iter=2000, C=1)\n\ncv_params = {\n    'n_splits': 5,\n    'shuffle': True,\n    'random_state': 123\n}\ncv = StratifiedKFold(**cv_params)\n\n\ncross_validation(cv, logreg, pd.DataFrame(twit_vectors), train.target)","26b39508":"logreg = LogisticRegression(solver='lbfgs', max_iter=2000, C=1)\ncv_params = {\n    'n_splits': 5,\n    'shuffle': True,\n    'random_state': 123\n}\ncv = StratifiedKFold(**cv_params)\n\nfor remove_punctuation in [True, False]:\n    for is_remove_stopwords in [True, False]:\n        for is_lemmatize in [True, False]:\n            print(remove_punctuation, is_remove_stopwords, is_lemmatize)\n            train = pd.read_csv('.\/train_%s_%s_%s.csv'%(remove_punctuation, is_remove_stopwords, is_lemmatize))\n            train.clean_text = train.clean_text.fillna('')\n            twit_vectors = twit2vec(train)\n            cross_validation(cv, logreg, pd.DataFrame(twit_vectors), train.target)","9ffe98a8":"import torch\nimport torch.nn as nn\nimport torch.optim as optim","71a6b79b":"train = pd.read_csv('.\/train_%s_%s_%s.csv'%(True, True, True))\ntrain.clean_text = train.clean_text.fillna('')","7f0c2867":"EMBEDDINGS_FILE = '..\/input\/glovetwitter100d\/glove.twitter.27B.100d.txt'\nWORD2IDX = {}\nWORD2VEC = {}\nEMBEDDING_MATRIX = []\nwith open(EMBEDDINGS_FILE, 'r') as f:\n    for idx, row in enumerate(f.readlines()):\n        row = row.strip().split(' ')\n        values = np.asarray(row[1:], dtype='float32')\n        if values.shape[0] == 99:\n            print(row[0], row)\n            continue\n#             break\n        WORD2IDX[row[0]] = idx\n        EMBEDDING_MATRIX.append(values)\n        WORD2VEC[row[0]] = EMBEDDING_MATRIX[-1]\n        \nEMBEDDING_MATRIX = np.array(EMBEDDING_MATRIX)","99a174ff":"def text2idx(texts):\n    idx = []\n    corpus = []\n    for text in texts:\n        idx.append([])\n        corpus.append([])\n        for word in text.split(' '):\n            if word in WORD2IDX:\n                idx[-1].append(WORD2IDX[word])\n                corpus[-1].append(word)\n    return idx, corpus\n\ndef groupby_token_count(idx, y):\n    dic_X, dic_y = {}, {}\n    for index, text in enumerate(idx):\n        dic_X[len(text)] = dic_X.get(len(text), []) + [text]\n        dic_y[len(text)] = dic_y.get(len(text), []) + [y[index]]\n    return dic_X, dic_y\n\ndef train_val_split(grouped_X, grouped_y):\n    grouped_train_X, grouped_val_X, grouped_train_y, grouped_val_y = {}, {}, {}, {}\n    for token_count in grouped_X:\n        if token_count == 0:\n            continue\n        indicator = False # whether minority class count is less than 2\n        zero_cnt = one_count = 0\n        for val in grouped_y[token_count]:\n            if val == 1:\n                one_count += 1\n            else:\n                zero_cnt += 1\n            if one_count > 1 and zero_cnt > 1:\n                break\n        else:\n            indicator = True\n        if indicator:\n            grouped_train_X[token_count] = grouped_X[token_count]\n            grouped_train_y[token_count] = grouped_y[token_count]\n        else:\n            grouped_train_X[token_count], grouped_val_X[token_count], \\\n            grouped_train_y[token_count], grouped_val_y[token_count] = train_test_split(grouped_X[token_count], grouped_y[token_count], test_size=0.2, \n                                                                                        shuffle=True, random_state=123, stratify=grouped_y[token_count])\n    return grouped_train_X, grouped_val_X, grouped_train_y, grouped_val_y\n        \n\ntrain_text2idx, train_corpus = text2idx(train.clean_text.values)\ngrouped_X, grouped_y = groupby_token_count(train_text2idx, train.target.values)\ngrouped_train_X, grouped_val_X, grouped_train_y, grouped_val_y = train_val_split(grouped_X, grouped_y)\nprint(grouped_X.keys())","8a615ed0":"def create_emb_layer(requires_grad=False):\n    num_embeddings, embedding_dim = EMBEDDING_MATRIX.shape\n    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n    emb_layer.from_pretrained(torch.FloatTensor(EMBEDDING_MATRIX), freeze=~requires_grad)\n#     print(emb_layer.weight.requires_grad)\n#     emb_layer.weight.requires_grad = requires_grad\n#     print(emb_layer.weight.requires_grad)\n\n    return emb_layer, num_embeddings, embedding_dim\n\nclass RNNNet(nn.Module):\n    def __init__(self, rnn_params={'hidden_size': 64, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2},):\n        super().__init__()\n        self.rnn_params = rnn_params\n        self.embedding, num_embeddings, embedding_dim = create_emb_layer(requires_grad=False)\n        self.drop = nn.Dropout(0.2)\n        self.gru = nn.GRU(embedding_dim, batch_first=True, **rnn_params)\n        if rnn_params['bidirectional']:\n            self.fc1 = nn.Linear(2*rnn_params['hidden_size'], 1)\n        else:\n            self.fc1 = nn.Linear(rnn_params['hidden_size'], 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, inp):\n        x = self.embedding(inp)\n        x = self.drop(x)\n#         x = self.gru(x, torch.zeros((1, x.shape[0], self.rnn_params['hidden_size'],)))[0][:, -1, :]\n        x = self.gru(x)[0][:, -1, :]\n        x = self.fc1(x)\n        x = self.sigmoid(x)\n        return x\n    \n\nnet = RNNNet()\nopt = optim.Adam(net.parameters(), lr=3e-4)#, lr=0.1)\ncriterion = nn.BCELoss()\n\n# with torch.no_grad():\n# #     inputs = prepare_sequences(train.clean_text.values[:1])#.reshape(1, 1, -1)\n#     inputs = torch.tensor(grouped_X[7], dtype=torch.long)\n#     tag_scores = net(inputs)\n#     print(tag_scores)","8dc972ca":"def train_net(net, criterion, optimizer, train_X, val_X, train_y, val_y, n_epochs=10, batch_size=64):\n    for epoch in range(n_epochs):\n        net.train()\n        train_loss = []\n        train_target = []\n#         for index in range(0, X.shape[0], batch_size):\n        for token_count in train_X:\n            for index in range(0, len(train_X[token_count]), batch_size):\n                optimizer.zero_grad()\n    #             if index % 100 == 0:\n    #                 print(index, X.shape[0])\n    #             train_X = prepare_sequences(X[index : index+batch_size])\n    #             if train_X.shape[-1] == 0:\n    #                 running_loss += [0]*batch_size\n    #                 true_target += y[index : index+batch_size]\n    #                 continue\n    #             train_y = torch.tensor(y[index : index+batch_size], dtype=torch.float32).unsqueeze(0)\n                if not token_count:\n                    continue\n                batch_X = torch.tensor(train_X[token_count][index : index+batch_size], dtype=torch.long)\n                batch_y = torch.tensor(train_y[token_count][index : index+batch_size], dtype=torch.float32).unsqueeze(0)\n\n                output = net(batch_X)\n                loss = criterion(output, batch_y)\n                loss.backward()\n                optimizer.step()\n                \n                train_loss += output.data.numpy().ravel().tolist()\n                train_target += batch_y.data.numpy().ravel().astype(int).tolist()\n    \n        val_loss = []\n        val_target = []\n        net.eval()\n        for token_count in val_X:\n            for index in range(0, len(val_X[token_count]), batch_size):\n                optimizer.zero_grad()\n                if not token_count:\n                    continue\n                batch_X = torch.tensor(val_X[token_count][index : index+batch_size], dtype=torch.long)\n                batch_y = torch.tensor(val_y[token_count][index : index+batch_size], dtype=torch.float32).unsqueeze(0)\n\n                output = net(batch_X)\n                val_loss += output.data.numpy().ravel().tolist()\n                val_target += batch_y.data.numpy().ravel().astype(int).tolist()\n        print('n_epoch: %s, train_f1_score:%s val_f1_score:%s train_accuracy:%s val_accuracy:%s'%(epoch, \n                                                                                                  round(f1_score(train_target, np.round(train_loss).astype(int), average='binary'), 5), \n                                                                                                  round(f1_score(val_target, np.round(val_loss).astype(int), average='binary'), 5),\n                                                                                                 round(accuracy_score(train_target, np.round(train_loss).astype(int)), 5), \n                                                                                                 round(accuracy_score(val_target, np.round(val_loss).astype(int)), 5)))\n                \n    return net\n    \nnet = train_net(net, criterion, opt, grouped_train_X, grouped_val_X, grouped_train_y, grouped_val_y, n_epochs=20, batch_size=64)\n","08f97cf4":"# del net\ngc.collect()\n","721f96ce":"import tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, Dense, SpatialDropout1D, LSTM, GRU, Dropout, Conv1D, Concatenate, GlobalAvgPool1D, GlobalMaxPool1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras import backend as K","715c7310":"train = pd.read_csv('.\/train_%s_%s_%s.csv'%(True, True, True))\ntrain.clean_text = train.clean_text.fillna('')\n","0b2292e1":"EMBEDDINGS_FILE = '..\/input\/glovetwitter100d\/glove.twitter.27B.100d.txt'\nWORD2IDX = {}\nWORD2VEC = {}\nEMBEDDING_MATRIX = []\nwith open(EMBEDDINGS_FILE, 'r') as f:\n    for idx, row in enumerate(f.readlines()):\n        row = row.strip().split(' ')\n        values = np.asarray(row[1:], dtype='float32')\n        if values.shape[0] == 99:\n            print(row[0], row)\n            continue\n#             break\n        WORD2IDX[row[0]] = idx\n        EMBEDDING_MATRIX.append(values)\n        WORD2VEC[row[0]] = EMBEDDING_MATRIX[-1]\n        \nEMBEDDING_MATRIX = np.array(EMBEDDING_MATRIX)","83a2fa27":"def text2idx(texts):\n    idx = []\n    corpus = []\n    for text in texts:\n        idx.append([])\n        corpus.append([])\n        for word in text.split(' '):\n            if word in WORD2IDX:\n                idx[-1].append(WORD2IDX[word])\n                corpus[-1].append(word)\n    return idx, corpus\n\ntrain_text2idx, train_corpus = text2idx(train.clean_text.values)\ntest_text2idx, test_corpus = text2idx(test.clean_text.values)\n\ndef detirmineMaxLen(corpus):\n    maxlen = 0\n    for text in corpus:\n        text_len = len(text)\n        if text_len > maxlen:\n            maxlen = text_len\n    return maxlen\n\ntrain_maxlen = detirmineMaxLen(train_corpus)\ntest_maxlen = detirmineMaxLen(test_corpus)\nmaxlen = max(train_maxlen, test_maxlen)\nprint(train_maxlen, test_maxlen)\n\n# def groupby_token_count(idx, y):\n#     dic_X, dic_y = {}, {}\n#     for index, text in enumerate(idx):\n#         dic_X[len(text)] = dic_X.get(len(text), []) + [text]\n#         dic_y[len(text)] = dic_y.get(len(text), []) + [y[index]]\n#     return dic_X, dic_y\n\n# def train_val_split(grouped_X, grouped_y):\n#     grouped_train_X, grouped_val_X, grouped_train_y, grouped_val_y = {}, {}, {}, {}\n#     for token_count in grouped_X:\n#         if token_count == 0:\n#             continue\n#         indicator = False # whether minority class count is less than 2\n#         zero_cnt = one_count = 0\n#         for val in grouped_y[token_count]:\n#             if val == 1:\n#                 one_count += 1\n#             else:\n#                 zero_cnt += 1\n#             if one_count > 1 and zero_cnt > 1:\n#                 break\n#         else:\n#             indicator = True\n#         if indicator:\n#             grouped_train_X[token_count] = grouped_X[token_count]\n#             grouped_train_y[token_count] = grouped_y[token_count]\n#         else:\n#             grouped_train_X[token_count], grouped_val_X[token_count], \\\n#             grouped_train_y[token_count], grouped_val_y[token_count] = train_test_split(grouped_X[token_count], grouped_y[token_count], test_size=0.2, \n#                                                                                         shuffle=True, random_state=123, stratify=grouped_y[token_count])\n#     return grouped_train_X, grouped_val_X, grouped_train_y, grouped_val_y\n        \n\n# grouped_X, grouped_y = groupby_token_count(train_text2idx, train.target.values)\n# grouped_train_X, grouped_val_X, grouped_train_y, grouped_val_y = train_val_split(grouped_X, grouped_y)\n# print(max(grouped_X.keys()))","b8cbe8d0":"train_padded_sequences = pad_sequences(train_text2idx, maxlen=maxlen, dtype='int32', padding='pre', truncating='pre', value=0.0)\ntest_padded_sequences = pad_sequences(test_text2idx, maxlen=maxlen, dtype='int32', padding='pre', truncating='pre', value=0.0)","136f1cff":"train_padded_sequences = pad_sequences(train_text2idx, maxlen=maxlen, dtype='int32', padding='post', truncating='post', value=0.0)\ntest_padded_sequences = pad_sequences(test_text2idx, maxlen=maxlen, dtype='int32', padding='post', truncating='post', value=0.0)","02db743f":"class F1Score(keras.metrics.Metric):\n    \n    def __init__(self, name='f1_score', **kwargs):\n        super(F1Score, self).__init__(**kwargs)\n        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n        self.false_negatives = self.add_weight(name='falseneg', initializer='zeros')\n        \n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(y_true, tf.bool)\n        y_pred = tf.cast(K.round(y_pred), tf.bool)\n\n        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n        values = tf.cast(values, tf.float32)\n        self.true_positives.assign_add(tf.reduce_sum(values))\n        \n        values = tf.logical_and(tf.equal(y_true, False), tf.equal(y_pred, True))\n        values = tf.cast(values, tf.float32)\n        self.false_positives.assign_add(tf.reduce_sum(values))\n        \n        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, False))\n        values = tf.cast(values, tf.float32)\n        self.false_negatives.assign_add(tf.reduce_sum(values))\n        \n    def reset_states(self):\n        K.batch_set_value([(v, 0) for v in self.variables])\n    \n    def result(self):\n        precision = self.true_positives \/ (self.true_positives + self.false_positives + K.epsilon())\n        recall = self.true_positives \/ (self.true_positives + self.false_negatives + K.epsilon())\n        f1_score = 2 * recall * precision \/ (recall + precision + K.epsilon())\n        return f1_score\n    ","d6efb164":"model = Sequential()\nmodel.add(Embedding(EMBEDDING_MATRIX.shape[0], EMBEDDING_MATRIX.shape[1],\n                    embeddings_initializer=Constant(EMBEDDING_MATRIX), \n                    input_length=maxlen, trainable=False))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\noptimizer = Adam(learning_rate=3e-4)\n\nmodel.compile(optimizer=optimizer,\n             loss='binary_crossentropy', \n             metrics=['accuracy', F1Score()])\n# model.save_weights('model.h5')\n# model.load_weights('model.h5')\n","faf1f5f1":"train_X, val_X, train_y, val_y = train_test_split(train_padded_sequences, train.target.values, test_size=0.2, random_state=123, stratify=train.target.values)","83ff75ff":"history = model.fit(train_X, train_y, batch_size=64, epochs=10, validation_data=(val_X, val_y), verbose=2)#, callbacks=[MyCustomCallback()])","b370d94e":"f1_score(train_y, np.round(model.predict(train_X)).ravel().astype(int), average='binary'), f1_score(val_y, np.round(model.predict(val_X)).ravel().astype(int), average='binary')","e07c30ad":"pd.Series(np.round(model.predict(test_padded_sequences)).ravel()).value_counts(normalize=True)","796855df":"make_submission(np.round(model.predict(test_padded_sequences)).ravel().astype(int))","a90f2668":"import tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, Dense, SpatialDropout1D, LSTM, GRU, Dropout, Conv1D, Concatenate, GlobalAvgPool1D, GlobalMaxPool1D, Input, concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras import backend as K","c5929518":"train = pd.read_csv('.\/train_%s_%s_%s.csv'%(True, True, True))\ntrain.clean_text = train.clean_text.fillna('')\ntest.clean_text = test.clean_text.fillna('')","97af63a1":"EMBEDDINGS_FILE = '..\/input\/glovetwitter100d\/glove.twitter.27B.100d.txt'\nWORD2IDX = {}\nWORD2VEC = {}\nEMBEDDING_MATRIX = []\nwith open(EMBEDDINGS_FILE, 'r') as f:\n    for idx, row in enumerate(f.readlines()):\n        row = row.strip().split(' ')\n        values = np.asarray(row[1:], dtype='float32')\n        if values.shape[0] == 99:\n            print(row[0], row)\n            continue\n#             break\n        WORD2IDX[row[0]] = idx\n        EMBEDDING_MATRIX.append(values)\n        WORD2VEC[row[0]] = EMBEDDING_MATRIX[-1]\n        \nEMBEDDING_MATRIX = np.array(EMBEDDING_MATRIX)","d93727b9":"def text2idx(texts):\n    idx = []\n    corpus = []\n    for text in texts:\n        idx.append([])\n        corpus.append([])\n        for word in text.split(' '):\n            if word in WORD2IDX:\n                idx[-1].append(WORD2IDX[word])\n                corpus[-1].append(word)\n    return idx, corpus\n\ntrain_text2idx, train_corpus = text2idx(train.clean_text.values)\ntest_text2idx, test_corpus = text2idx(test.clean_text.values)\n\ndef detirmineMaxLen(corpus):\n    maxlen = 0\n    for text in corpus:\n        text_len = len(text)\n        if text_len > maxlen:\n            maxlen = text_len\n    return maxlen\n\ntrain_maxlen = detirmineMaxLen(train_corpus)\ntest_maxlen = detirmineMaxLen(test_corpus)\nmaxlen = max(train_maxlen, test_maxlen)\nprint(train_maxlen, test_maxlen)\n\n","ebd72d11":"train_padded_sequences = pad_sequences(train_text2idx, maxlen=maxlen, dtype='int32', padding='pre', truncating='pre', value=0.0)\ntest_padded_sequences = pad_sequences(test_text2idx, maxlen=maxlen, dtype='int32', padding='pre', truncating='pre', value=0.0)","ac95179d":"train_padded_sequences = pad_sequences(train_text2idx, maxlen=maxlen, dtype='int32', padding='post', truncating='post', value=0.0)\ntest_padded_sequences = pad_sequences(test_text2idx, maxlen=maxlen, dtype='int32', padding='post', truncating='post', value=0.0)","26ce6816":"class F1Score(keras.metrics.Metric):\n    \n    def __init__(self, name='f1_score', **kwargs):\n        super(F1Score, self).__init__(**kwargs)\n        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n        self.false_negatives = self.add_weight(name='falseneg', initializer='zeros')\n        \n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(y_true, tf.bool)\n        y_pred = tf.cast(K.round(y_pred), tf.bool)\n\n        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n        values = tf.cast(values, tf.float32)\n        self.true_positives.assign_add(tf.reduce_sum(values))\n        \n        values = tf.logical_and(tf.equal(y_true, False), tf.equal(y_pred, True))\n        values = tf.cast(values, tf.float32)\n        self.false_positives.assign_add(tf.reduce_sum(values))\n        \n        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, False))\n        values = tf.cast(values, tf.float32)\n        self.false_negatives.assign_add(tf.reduce_sum(values))\n        \n    def reset_states(self):\n        K.batch_set_value([(v, 0) for v in self.variables])\n    \n    def result(self):\n        precision = self.true_positives \/ (self.true_positives + self.false_positives + K.epsilon())\n        recall = self.true_positives \/ (self.true_positives + self.false_negatives + K.epsilon())\n        f1_score = 2 * recall * precision \/ (recall + precision + K.epsilon())\n        return f1_score\n    ","ca840805":"embed_layer = Embedding(EMBEDDING_MATRIX.shape[0], EMBEDDING_MATRIX.shape[1],\n                        embeddings_initializer=Constant(EMBEDDING_MATRIX), \n                        input_length=maxlen, trainable=False)\nhead1 = Sequential()\nhead1.add(embed_layer)\nhead1.add(Conv1D(filters=100, kernel_size=3, activation='relu'))\nhead1.add(GlobalMaxPool1D())\n\nhead2 = Sequential()\nhead2.add(embed_layer)\nhead2.add(Conv1D(filters=100, kernel_size=4, activation='relu'))\nhead2.add(GlobalMaxPool1D())\n\nhead3 = Sequential()\nhead3.add(embed_layer)\nhead3.add(Conv1D(filters=100, kernel_size=5, activation='relu'))\nhead3.add(GlobalMaxPool1D())\n\nconcat_layer = Concatenate([head1, head2, head3])\n","8c467d4d":"embed_layer = Embedding(EMBEDDING_MATRIX.shape[0], EMBEDDING_MATRIX.shape[1],\n                        embeddings_initializer=Constant(EMBEDDING_MATRIX), \n                        input_length=maxlen, trainable=False)\n\nsequence_input = Input(shape=(maxlen,), dtype='float32')\nembedded_sequences = embed_layer(sequence_input)\n\nhead1 = Conv1D(filters=32, kernel_size=3, activation='relu')(embedded_sequences)\nhead1 = GlobalMaxPool1D()(head1)\n\nhead2 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedded_sequences)\nhead2 = GlobalMaxPool1D()(head2)\n\nhead3 = Conv1D(filters=32, kernel_size=5, activation='relu')(embedded_sequences)\nhead3 = GlobalMaxPool1D()(head3)\n\nconcat_layer = concatenate([head1, head2, head3])\n\n# fc1 = Dense(32, activation='relu')(concat_layer)\nresult = Dense(1, activation='sigmoid')(concat_layer)\n\nmodel = Model(sequence_input, result)\noptimizer = Adam(learning_rate=3e-4)\n\nmodel.compile(optimizer=optimizer,\n             loss='binary_crossentropy', \n             metrics=['accuracy', F1Score()])","25e8d9c4":"model = Sequential()\nmodel.add(embed_layer)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Conv1D(filters=32, kernel_size=(5), activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)))\nmodel.add(GlobalMaxPool1D())\n# model.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\noptimizer = Adam(learning_rate=3e-4)\n\nmodel.compile(optimizer=optimizer,\n             loss='binary_crossentropy', \n             metrics=['accuracy', F1Score()])\n# model.save_weights('model.h5')\n# model.load_weights('model.h5')\n","f273fce5":"train_X, val_X, train_y, val_y = train_test_split(train_padded_sequences, train.target.values, test_size=0.2, random_state=123, stratify=train.target.values)","981986f1":"history = model.fit(train_X, train_y, batch_size=32, epochs=15, validation_data=(val_X, val_y), verbose=2)#, callbacks=[MyCustomCallback()])","0425035e":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","8a50150a":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, Dense, SpatialDropout1D, LSTM, GRU, Dropout, Conv1D, Concatenate, GlobalAvgPool1D, GlobalMaxPool1D, Input, concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras import backend as K\nimport tensorflow_hub as hub\n\nimport tokenization","aeda35d1":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","3bd6ca39":"len(tf.config.experimental.list_physical_devices('GPU'))","8adca6a5":"def bert_encode(texts, tokenizer, max_len=128):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","8b494efb":"class F1Score(keras.metrics.Metric):\n    \n    def __init__(self, name='f1_score', **kwargs):\n        super(F1Score, self).__init__(**kwargs)\n        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n        self.false_negatives = self.add_weight(name='falseneg', initializer='zeros')\n        \n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(y_true, tf.bool)\n        y_pred = tf.cast(K.round(y_pred), tf.bool)\n\n        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n        values = tf.cast(values, tf.float32)\n        self.true_positives.assign_add(tf.reduce_sum(values))\n        \n        values = tf.logical_and(tf.equal(y_true, False), tf.equal(y_pred, True))\n        values = tf.cast(values, tf.float32)\n        self.false_positives.assign_add(tf.reduce_sum(values))\n        \n        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, False))\n        values = tf.cast(values, tf.float32)\n        self.false_negatives.assign_add(tf.reduce_sum(values))\n        \n    def reset_states(self):\n        K.batch_set_value([(v, 0) for v in self.variables])\n    \n    def result(self):\n        precision = self.true_positives \/ (self.true_positives + self.false_positives + K.epsilon())\n        recall = self.true_positives \/ (self.true_positives + self.false_negatives + K.epsilon())\n        f1_score = 2 * recall * precision \/ (recall + precision + K.epsilon())\n        return f1_score\n    ","0957883f":"def build_model(bert_layer, max_len=128):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy', F1Score()])\n    \n    return model","3de07aed":"max_seq_length = 128  # Your choice here.\nalbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/albert_en_base\/1\",\n                              trainable=True)\n# pooled_output, sequence_output = albert_layer([input_word_ids, input_mask, segment_ids])","b5ba8d80":"sp_model_file = albert_layer.resolved_object.sp_model_file.asset_path.numpy()\ntokenizer = tokenization.FullSentencePieceTokenizer(sp_model_file)","20e9f557":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","b6d6b221":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","f5d83274":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","dcfc279d":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","46c2db3f":"model = build_model(bert_layer, max_len=160)\n# model.summary()","8ff08eac":"model = build_model(albert_layer, max_len=160)\n# model.summary()","7e43c969":"history = model.fit(train_input, train_labels, batch_size=16, epochs=3, validation_split=0.2, verbose=1)","f1b49794":"len(tf.config.experimental.list_physical_devices('GPU'))","3737e234":"## Model: build, train","b6161e0d":"### RNN with Glove and Tensorflow.keras","1f7c2366":"### RNN with Glove and Pytorch","f47c5da9":"### Mean Glove embeddings + logreg","1a577a48":"### BERT","12f910fc":"### CNN + embeddings + tf.keras","bf03c2e9":"## Load and Preprocess"}}