{"cell_type":{"b6ebbbe8":"code","ae937c63":"code","0915b371":"code","2a3b5c3f":"code","26f3906b":"code","19ff90e7":"code","f57c0486":"code","28d620f0":"code","72ef935b":"code","37ab8e05":"code","ed21a29d":"code","aca61a1f":"code","70c26735":"code","9c369c54":"code","79dc61c5":"code","0a189c82":"code","e7216bef":"code","41055757":"markdown","a8fef98b":"markdown","3c87d736":"markdown","3b0d7b98":"markdown","87faec2b":"markdown","935a4526":"markdown","a82d94ca":"markdown","36f7e8ea":"markdown"},"source":{"b6ebbbe8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn","ae937c63":"# Create DataFrames\ndf_train = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\", encoding=\"latin1\")\ndf_test = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\", encoding=\"latin1\")\n\n# Shuffle DataFrames\ndf_train = df_train.sample(frac=1)\ndf_test = df_test.sample(frac=1)","0915b371":"df_train.head()","2a3b5c3f":"from transformers import RobertaTokenizer\n\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\ndef token_counter(text, tokenizer):\n    return len(tokenizer.encode(text))\n\ntok_len = df_train[\"OriginalTweet\"].apply(lambda x : token_counter(x, tokenizer))\n\nmax(list(tok_len))","26f3906b":"tokenizer.model_max_length = 200","19ff90e7":"print(\"The training dataframe contains {} Tweets\".format(len(df_train)))\nprint(\"The test dataframe contains {} Tweets\".format(len(df_test)))","f57c0486":"df_train.Sentiment.value_counts().loc[[\"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Extremely Positive\"]].plot(kind=\"bar\")","28d620f0":"import re\ndef remove_links(text):\n    to_remove = ['\\r','\\n',',',';',':','.']\n    \n    out = re.sub(r'http\\S+', '', text)\n    \n    for token in to_remove:\n        out = out.replace(token, '')\n    \n    return re.sub(' +', ' ', out.lower()) #Remove duplicate spaces\n\ndef tokenize(text, tokenizer):\n    return tokenizer.encode(text, padding='max_length')\n\nname_to_idx = {\n    \"Extremely Negative\" : 0,\n    \"Negative\" : 1,\n    \"Neutral\" : 2,\n    \"Positive\" : 3,\n    \"Extremely Positive\" : 4\n}\n\ndef process_tgt(value):\n    return name_to_idx[value]","72ef935b":"train_text = list(df_train[\"OriginalTweet\"].apply(remove_links).apply(lambda x : tokenize(x, tokenizer)))\ntrain_labels = list(df_train[\"Sentiment\"].apply(process_tgt))\n\ntest_text = list(df_test[\"OriginalTweet\"].apply(remove_links).apply(lambda x : tokenize(x, tokenizer)))\ntest_labels = list(df_test[\"Sentiment\"].apply(process_tgt))","37ab8e05":"from torch.utils.data import Dataset\n\nclass CreateDataset(Dataset):\n    \n    def __init__(self, data, labels):\n        super().__init__()\n        self.data = data\n        self.labels = labels\n        \n        \n    def __getitem__(self, idx):\n        return torch.tensor(self.data[idx]), torch.tensor(self.labels[idx])\n    \n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = CreateDataset(train_text, train_labels)\ntest_dataset = CreateDataset(test_text, test_labels)","ed21a29d":"len(train_dataset),len(test_dataset)","aca61a1f":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size = 32)\ntest_loader = DataLoader(test_dataset, batch_size = 32)","70c26735":"class LSTMModel(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(input_size = embedding_dim, \n                            hidden_size = hidden_size, \n                            num_layers = num_layers,\n                            dropout = dropout,\n                            batch_first = True,\n                            bidirectional = True)\n        self.linear = nn.Linear(512*200, 5)\n        \n    def forward(self, inputs):\n        emb = self.embedding(inputs)\n        lstm_out, _ = self.lstm(emb)\n        \n        output = self.linear(lstm_out.reshape(lstm_out.size()[0], -1))\n        \n        return output\n    \nmodel = LSTMModel(tokenizer.vocab_size, 256, 256, 4, 0.2)","9c369c54":"from tqdm import tqdm\n\nclass Trainer():\n    \n    def __init__(self, model, train_loader, valid_loader):\n        \n        self.model = model\n        self.train_loader = train_loader\n        self.valid_loader = valid_loader\n\n    def train_epoch(self, f_loss, optimizer, device):\n\n        # We enter train mode. This is useless for the linear model\n        # but is important for layers such as dropout, batchnorm, ...\n        self.model.train()\n\n        correct = 0\n        tot_loss = 0\n        N = 41157 # Dataset length\n\n        # iterator = tqdm(enumerate(self.train_loader))\n        iterator = enumerate(self.train_loader)\n\n        for i, (inputs, targets) in iterator:\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Compute the forward pass through the network up to the loss\n            outputs = self.model(inputs)\n\n            loss = f_loss(outputs, targets)\n\n            loss_value = loss.item()\n\n            # Backward and optimize\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            tot_loss += inputs.shape[0] * loss_value\n\n            predicted_targets = outputs.argmax(dim=1)\n            correct += (predicted_targets == targets).sum().item()\n\n            #iterator.set_description(\"loss : {: .3f} | accuracy : {: .3f}\".format(tot_loss\/(inputs.shape[0]*(i+1)), correct\/(inputs.shape[0]*(i+1))))\n\n        return tot_loss\/N, correct\/N\n\n    def valid_epoch(self, f_loss, device):\n        # We enter train mode. This is useless for the linear model\n        # but is important for layers such as dropout, batchnorm, ...\n        self.model.eval()\n\n        correct = 0\n        tot_loss = 0\n        N = 3798 # Dataset length\n\n        # iterator = tqdm(enumerate(self.valid_loader))\n        iterator = enumerate(self.valid_loader)\n\n        with torch.no_grad():\n            for i, (inputs, targets) in iterator:\n                inputs, targets = inputs.to(device), targets.to(device)\n\n                # Compute the forward pass through the network up to the loss\n                outputs = self.model(inputs)\n\n                loss = f_loss(outputs, targets)\n\n                tot_loss += inputs.shape[0] * loss.item()\n\n                predicted_targets = outputs.argmax(dim=1)\n                correct += (predicted_targets == targets).sum().item()\n\n                # iterator.set_description(\"loss : {: .3f} | accuracy : {: .3f}\".format(tot_loss\/(inputs.shape[0]*(i+1)), correct\/(inputs.shape[0]*(i+1))))\n\n        return tot_loss\/N, correct\/N\n\n    def training(self, f_loss, optimizer, device, epochs = 10):\n\n        train_loss = []\n        train_acc = []\n        valid_loss = []\n        valid_acc = []\n\n        for i in range(epochs):\n            print(\"EPOCH {}\/{}\".format(i + 1, epochs))\n            train_results = self.train_epoch(f_loss, optimizer, device)\n            print(\"Training loss : {: .3f} | Training accuracy : {: .3f}\".format(*train_results))\n            valid_results = self.valid_epoch(f_loss, device)\n            print(\"Validation loss : {: .3f} | Validation accuracy : {: .3f}\\n\".format(*valid_results))\n\n            train_loss.append(train_results[0])\n            train_acc.append(train_results[1])\n            valid_loss.append(valid_results[0])\n            valid_acc.append(valid_results[1])\n\n        return train_loss, train_acc, valid_loss, valid_acc","79dc61c5":"device = torch.device('cuda')\nmodel = model.cuda()\n\nf_loss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())","0a189c82":"trainer = Trainer(model, train_loader, test_loader)\n\ntrain_loss, train_acc, valid_loss, valid_acc = trainer.training(f_loss, optimizer, device, epochs = 8)","e7216bef":"plt.plot(train_loss, label = \"train set\")\nplt.plot(valid_loss, label = \"test set\")\nplt.legend()\nplt.title(\"Loss of the model during training\")\nplt.show()\n\nplt.plot(train_acc, label = \"train set\")\nplt.plot(valid_acc, label = \"test set\")\nplt.legend()\nplt.title(\"Accuracy of the model during training\")\nplt.show()","41055757":"## Create the model","a8fef98b":"The Dataset is well balanced between categories.","3c87d736":"## Train the model","3b0d7b98":"## Load data","87faec2b":"## Processing the text","935a4526":"The longest tweet contains 184 tokens, we don't have to use padding up to the 512th token, we will stop at 200 to reduce the size of the tensors handled.","a82d94ca":"Let's look at the length of the tweets.","36f7e8ea":"## Data analysis"}}