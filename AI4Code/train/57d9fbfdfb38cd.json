{"cell_type":{"6ce696c5":"code","e2fb5709":"code","48cee2a0":"code","bb6eb13b":"code","a450b0e1":"code","74550273":"code","b9f7412f":"code","7e731eb9":"code","14d51228":"code","dc537bc0":"code","aa6c8458":"code","013e9ec9":"code","dc55b4b4":"code","8470842b":"code","bee50130":"code","8b9c237a":"code","ed8d50db":"code","2d54397f":"code","7bb55cd0":"code","b2e42618":"code","64307f7f":"code","7b688785":"code","628bde5e":"code","f0d24923":"code","86b2a83e":"code","2d09493a":"code","bf28ba8f":"code","f88f39aa":"code","ac4eb7b6":"code","c9167d85":"code","45a7ba25":"code","32fc778c":"code","f9ff48ab":"markdown","77c5b92c":"markdown","62433494":"markdown","b4db05ac":"markdown","627249bc":"markdown","6b972f47":"markdown","82962391":"markdown"},"source":{"6ce696c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Any results you write to the current directory are saved as output.","e2fb5709":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","48cee2a0":"pd.set_option('display.max_columns', 100)\ntrain.head()","bb6eb13b":"f = open('..\/input\/house-prices-advanced-regression-techniques\/data_description.txt','r')\nmessage = f.read()\nprint(message)","a450b0e1":"NA_col = pd.DataFrame(train.isna().sum(), columns = ['NA_Count'])\nNA_col['% of NA'] = (NA_col.NA_Count\/len(train))*100\nNA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')\n\n# Dropping 'PoolQC', 'MiscFeature', 'Alley', 'Fence'","74550273":"pd.set_option('display.max_rows', 500)\nNA_row = pd.DataFrame(train.isnull().sum(axis = 1), columns = ['NA_Row_Count'])\nNA_row['% of NA'] = (NA_row.NA_Row_Count\/len(train.columns))*100\nNA_row.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')\n\n# We are good row-wise","b9f7412f":"NA_col = pd.DataFrame(test.isna().sum(), columns = ['NA_Count'])\nNA_col['% of NA'] = (NA_col.NA_Count\/len(test))*100\nNA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')\n\n# Dropping 'PoolQC', 'MiscFeature', 'Alley', 'Fence'","7e731eb9":"pd.set_option('display.max_rows', 500)\nNA_row = pd.DataFrame(test.isnull().sum(axis = 1), columns = ['NA_Row_Count'])\nNA_row['% of NA'] = (NA_row.NA_Row_Count\/len(test.columns))*100\nNA_row.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')\n\n# We are good row-wise","14d51228":"train_data = train.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1) \ntest_data = test.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1) ","dc537bc0":"train_wo_target = train_data.drop(['Id', 'SalePrice'], axis=1)","aa6c8458":"cols = train_wo_target.columns\nnum_cols = train_wo_target._get_numeric_data().columns\ncat_cols = list(set(cols) - set(num_cols))","013e9ec9":"num_cols","dc55b4b4":"cat_cols","8470842b":"X_train = train_data.copy().drop('SalePrice', axis = 1)\ny_train = train_data[['Id','SalePrice']]","bee50130":"from sklearn.preprocessing import Imputer\nfrom sklearn.impute import SimpleImputer","8b9c237a":"# Imputing Numerical Columns\n\nmean_imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\", axis=0)\nmean_imputer.fit(X_train[num_cols])\nX_train[num_cols] = pd.DataFrame(mean_imputer.fit_transform(X_train[num_cols]))\ntest_data[num_cols] = pd.DataFrame(mean_imputer.fit_transform(test_data[num_cols]))","ed8d50db":"train_objs_num = len(X_train)\ndataset = pd.concat(objs=[X_train, test_data], axis=0)\ndataset_preprocessed = pd.get_dummies(dataset)\nX_train = dataset_preprocessed[:train_objs_num]\ntest_data = dataset_preprocessed[train_objs_num:]","2d54397f":"X_train.isna().sum()","7bb55cd0":"test_data.isna().sum()","b2e42618":"X_train.set_index('Id', inplace = True)\ny_train.set_index('Id', inplace = True)\ntest_data.set_index('Id', inplace = True)","64307f7f":"print(X_train.shape)\nprint(test_data.shape)","7b688785":"## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","628bde5e":"%%time\n## Reducing memory\nX_train = reduce_mem_usage(X_train)\ny_train = reduce_mem_usage(y_train)\ntest_data = reduce_mem_usage(test_data)","f0d24923":"from sklearn import metrics, tree\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\nfrom xgboost import XGBRegressor","86b2a83e":"XGB = XGBRegressor(n_jobs=-1)\n \n# Use a grid over parameters of interest\nparam_grid = {\n     'colsample_bytree': np.linspace(0.3, 0.8, 1.0),\n     'n_estimators':[40 ,80, 150, 200,240, 300],\n     'max_depth': [1,2, 3,5, 8]\n}\n\n \nCV_XGB = GridSearchCV(estimator=XGB, param_grid=param_grid, cv= 10)","2d09493a":"# Train XGBoost Regressor\n%time CV_XGB.fit(X_train, y_train)","bf28ba8f":"best_xgb_model = CV_XGB.best_estimator_\nprint (CV_XGB.best_score_, CV_XGB.best_params_)","f88f39aa":"pred_train_xgb = best_xgb_model.predict(X_train)\npred_test_xgb = best_xgb_model.predict(test_data)","ac4eb7b6":"print(metrics.mean_squared_log_error(y_train, pred_train_xgb).round(5))","c9167d85":"y_pred_test_xgb = pd.DataFrame(pred_test_xgb, columns = ['SalePrice'])\ny_pred_test_xgb['Id'] = test['Id']","45a7ba25":"columnsTitles = ['Id', 'SalePrice']\n\nsubmission = y_pred_test_xgb.reindex(columns=columnsTitles)\nsubmission .head()","32fc778c":"filename = 'House_Pricing_XGB.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","f9ff48ab":"### Dropping columns having >= 80% NAs","77c5b92c":"### Reading Files","62433494":"## Simple analysis using XG Boost","b4db05ac":"### Imputation","627249bc":"### Model Building - Gradient Boost","6b972f47":"### Dropping the 'Id' column and the target variable to segregate Numerical and Categorical columns","82962391":"### Checking NAs"}}