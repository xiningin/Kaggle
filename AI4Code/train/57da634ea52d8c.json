{"cell_type":{"64398aca":"code","c85a83db":"code","68b9c4fc":"code","7f06fdbe":"code","d17f2683":"code","943b9253":"code","48c5630d":"code","1245e3b8":"code","fb574a9d":"code","ae690076":"code","16aa1218":"code","2d1c4da5":"code","d4514ced":"code","b68494e6":"code","26d88f45":"code","1497f12d":"code","71ccf386":"code","188305f3":"code","93d7e921":"code","204a8b8e":"code","b12a42f1":"code","cbc2a500":"code","e4378bbd":"code","c81fe362":"code","a869b193":"markdown","67fa117a":"markdown","773cfc3e":"markdown","0cbcb054":"markdown","ea944a49":"markdown","45554855":"markdown","dacdaafc":"markdown","9cc9974b":"markdown","dd889612":"markdown","9fb9de42":"markdown","87d74aa3":"markdown","75e79e9f":"markdown","84f6ede1":"markdown","86292242":"markdown","58b4fec5":"markdown","ee3df97f":"markdown","7bfa7c2d":"markdown"},"source":{"64398aca":"# Dependencies\nimport torch\nfrom torch import optim\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n","c85a83db":"# test cuda if available to speed training with gpu power\nuse_cuda  = torch.cuda.is_available()","68b9c4fc":"# transform image to pytorch tensor\n\"\"\"it should be a great idea to do data augmentation in order to detect\n    numbers in different positions and orientations, but we can just skip that now for the demo\"\"\"\n\ntransform = transforms.ToTensor()\n\n# load the training and test datasets\ntrain_data = datasets.MNIST(root='data', train=True,\n                                   download=True, transform=transform)\ntest_data = datasets.MNIST(root='data', train=False,\n                                  download=True, transform=transform)\n\n# Create training and test dataloaders\n\n# number of subprocesses to use for data loading\nnum_workers = 0\n# how many samples per batch to load\nbatch_size = 16\n\n# prepare data loaders\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)","7f06fdbe":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 5)\n        self.conv2 = nn.Conv2d(32, 32, 3)\n        self.conv3 = nn.Conv2d(32, 800, 5)\n        self.conv4 = nn.Conv2d(800, 800, 1)\n        self.conv5 = nn.Conv2d(800, 10, 1)\n        self.pool = nn.MaxPool2d((2,2))\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = self.conv5(x)\n        \n        return x\n        \n        ","d17f2683":"model = Model()\nif use_cuda:\n    model = model.cuda()","943b9253":"def train_model(model, train_loader, test_loader, n_epochs):\n    \"\"\"helper function to train the model\n        inputs: model --> our model\n                train_loader --> the train images loader\n                test_loader --> the test images loader\n                n_epchs --> number of iterations                \n    \"\"\"\n    # specify loss function (categorical cross-entropy)\n    criterion = nn.CrossEntropyLoss()\n    # specify optimizer\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    min_test_accuracy = 0\n    for epoch in range(1, n_epochs+1):\n\n        # keep track of training and validation loss\n        train_loss = 0.0\n        train_accuracy = 0\n        test_accuracy = 0\n        ###################\n        # train the model #\n        ###################\n        model.train()\n        for images, labels in train_loader:\n            # move tensors to GPU if CUDA is available\n            if use_cuda:\n                images = images.cuda()\n                labels = labels.cuda()\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            logits = model(images).squeeze()\n            _, outs_k = logits.topk(1, dim=1)\n            equals = (outs_k == labels.view(*outs_k.shape))\n            train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n            \n            # calculate the batch loss\n            loss = criterion(logits, labels)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # update training loss\n            train_loss += loss.item()\n\n\n        train_loss = train_loss\/len(train_loader)\n        train_accuracy \/= len(train_loader)\n        ######################\n        # validate the model #\n        ######################\n        model.eval()\n        for images, labels in test_loader:\n            if use_cuda:\n                images = images.cuda()\n                labels = labels.cuda()\n            logits = model(images).squeeze()\n            _, outs_k = logits.topk(1, dim=1)\n            equals = (outs_k == labels.view(*outs_k.shape))\n            test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n        \n        test_accuracy \/= len(test_loader)\n        print(\"\"\"Epoch: {} \\tTraining Loss: {:.6f} \n        \\tTrain accuracy: {:.6f} \\Test accuracy: {:.6f}\"\"\".format(\n            epoch, train_loss,  train_accuracy, test_accuracy))\n    \n        # save model if validation accuracy has increased\n        if test_accuracy >= min_test_accuracy:    \n            torch.save(model.state_dict(), 'model.pt')\n            min_test_accuracy = test_accuracy","48c5630d":"train_model(model, train_loader, test_loader, 5)","1245e3b8":"model.eval()","fb574a9d":"# load first batch\ntest_iterator = iter(test_loader)\nimages, labels = test_iterator.next()\n# send it to gpu if available\nif use_cuda:\n    images = images.cuda()\n    labels = labels.cuda()\n# get predictions\npredictions = F.softmax(model(images), dim=1).squeeze()\nprobs , outs = predictions.topk(1, dim=1)\n\n# get random images from bacth\nrandom_idx = np.random.randint(batch_size)\nif use_cuda:\n    image1 = images[random_idx].cpu().detach().numpy().squeeze()\n    image2 = images[(random_idx+1)%batch_size].cpu().detach().numpy().squeeze()\nelse : \n    image1 = images[random_idx].numpy().squeeze()\n    image2 = images[(random_idx+1)%batch_size].numpy().squeeze()    \n\nprob1 = probs[random_idx, 0]. item()\nout1 = outs[random_idx, 0]. item()\nprob2 = probs[(random_idx+1)%batch_size, 0]. item()\nout2 = outs[(random_idx+1)%batch_size, 0]. item()\n\n# visualise results\nfig, axs = plt.subplots(1, 2)\naxs[0].imshow(image1)\naxs[0].set_title('{}({:.2f})'.format(out1,prob1))\naxs[1].imshow(image2)\naxs[1].set_title('{}({:.2f})'.format(out2,prob2))\n\n","ae690076":"first_tensor = images[random_idx]\nsecond_tensor = images[(random_idx+1)%batch_size]\nthird_tensor = images[(random_idx+2)%batch_size]\n\nblack_tensor = torch.zeros_like(first_tensor)\n\ntmp_0 = torch.cat((black_tensor, black_tensor, black_tensor, black_tensor,\n                   black_tensor, black_tensor, black_tensor, third_tensor), 2)\ntmp_1 = torch.cat((black_tensor, black_tensor, black_tensor, black_tensor,\n                   black_tensor, black_tensor, black_tensor, black_tensor), 2)\ntmp_2 = torch.cat((black_tensor, black_tensor, black_tensor, black_tensor,\n                   black_tensor, black_tensor, black_tensor, black_tensor), 2)\n\ntmp_3 = torch.cat((black_tensor, black_tensor,first_tensor, second_tensor,\n                   black_tensor, black_tensor, black_tensor, black_tensor), 2)\ntmp_4 = torch.cat((black_tensor, black_tensor, black_tensor, black_tensor,\n                   black_tensor, black_tensor, black_tensor, black_tensor), 2)\n\ntest_image = torch.cat((tmp_0, tmp_1, tmp_2, tmp_3, tmp_4), 1)\n\nif use_cuda:\n    test_image_numpy = test_image.cpu().numpy().squeeze()\nelse : \n    test_image_numpy = test_image.numpy().squeeze()\n    \nplt.imshow(test_image_numpy)","16aa1218":"# get predicted labels and score\nprediction = F.softmax(model(test_image.unsqueeze(dim=0)))\nvalues, indices = prediction.squeeze().max(0)\n# get the shape\nvalues.shape, indices.shape","2d1c4da5":"if use_cuda:\n    # score\n    values = values.cpu().detach().numpy()\n    # labels\n    indices = indices.cpu().detach().numpy()\nelse :\n    values = values.numpy()\n    indices = indices.numpy()\n    ","d4514ced":"# let's define our confidence level\nCONFIDENCE_LEVEL = 0.99 ","b68494e6":"plt.imshow(values)\nplt.colorbar()\nheight, width = indices.shape\nfor y in range(height):\n        for x in range(width):\n            val = indices[y, x]\n            plt.annotate(str(val), xy=(x,y),\n                    horizontalalignment='center',\n                    verticalalignment='center', size=8,\n                    color='white' if values[y][x]<CONFIDENCE_LEVEL else 'black')","26d88f45":"def extract_boxes(image, step, sub_size_x=28, sub_size_y=28):\n    height, width = image.shape[0], image.shape[1]\n    nb_slides_x = (width-sub_size_x)\/\/step + 1\n    nb_slides_y = (height-sub_size_y)\/\/step + 1\n    location_boxes = []\n    for y in range(nb_slides_y):\n        for x in range(nb_slides_x):\n            location_boxes.append([y*step, x*step, sub_size_y, sub_size_x])\n    return np.array(location_boxes)","1497f12d":"location_boxes = extract_boxes(test_image_numpy, step=4)","71ccf386":"len(location_boxes)","188305f3":"len(values.flatten())","93d7e921":"# keep cells which activates more than the confidence level\nindexes_to_keep = np.where(values.flatten() > CONFIDENCE_LEVEL)[0]\n# These are the confidences and bounding boxes we want to keep for the detection\nconfident_scores = values.flatten()[indexes_to_keep]\nconfident_boxes = location_boxes[indexes_to_keep]\nconfident_labels = indices.flatten()[indexes_to_keep]","204a8b8e":"def draw_predictions(image, confident_boxes):\n    for y, x, box_size_y, box_size_x in confident_boxes:\n        image = cv2.rectangle(image,(x,y),(x+box_size_x,y+box_size_y),(255,0,0),1)\n    return image","b12a42f1":"test_image_with_boxes = cv2.cvtColor(test_image_numpy.copy(), cv2.COLOR_GRAY2BGR)\ntest_image_with_boxes = draw_predictions(test_image_with_boxes, confident_boxes)\nplt.imshow(test_image_with_boxes);\n","cbc2a500":"def expansion(bounding_boxes, slide_x = 1, slide_y=1):\n    nb_boxes = len(bounding_boxes)   \n    idx = 0\n    while idx < nb_boxes-1:\n        y1, x1, box_size_y1, box_size_x1 = bounding_boxes[idx]\n        y2, x2, box_size_y2, box_size_x2 = bounding_boxes[idx+1]      \n        if (((x1 + box_size_x1 + slide_x >= x2) or (x2 + box_size_x2 + slide_x >= x1))\n            and (y1 + box_size_y1 + slide_y >= y2)):\n            x = min(x1, x2)\n            y = min(y1, y2)\n            box_size_y = max(y2+box_size_y2, y1+box_size_y1) - min(y1,y2)\n            box_size_x = max(x2+box_size_x2, x1+box_size_x1) - min(x1,x2)\n            bounding_boxes[idx]= np.array([y, x, box_size_y, box_size_x])\n            bounding_boxes = np.delete(bounding_boxes, idx+1, axis = 0 )\n            nb_boxes = len(bounding_boxes)   \n        else:\n            idx+=1\n\n    return bounding_boxes\n        ","e4378bbd":"bounding_boxes = expansion(confident_boxes)","c81fe362":"test_image_with_boxes = cv2.cvtColor(test_image_numpy.copy(), cv2.COLOR_GRAY2BGR)\ntest_image_with_boxes = draw_predictions(test_image_with_boxes, bounding_boxes)\nplt.imshow(test_image_with_boxes);","a869b193":"# Photo OCR\nPhoto OCR as defined by wikipedia: \"Optical character recognition or optical character reader, often abbreviated as OCR, is the mechanical or electronic conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example from a television broadcast)\" <a href=\"https:\/\/en.wikipedia.org\/wiki\/Optical_character_recognition\">[source]<\/a> <br>\nIn andrew's ng <a href=\"https:\/\/www.coursera.org\/learn\/machine-learning\">course<\/a>, i found a great pipeline definition of the problem, which we will be using as a guideline through subsequent kernels. <img src=\"https:\/\/raw.githubusercontent.com\/ritchieng\/machine-learning-stanford\/master\/w11_application_example_ocr\/photoocr.png\" alt=\"Photo OCR Pipeline\" \/>","67fa117a":"So we've succesfully detected the numbers, but as we saw, we only used 28x28 frames to detect numbers, what if it they were bigger or very small. One solution is instead of changing the size of the frames, we can resize the input image, to make the frames bigger or smaller. We are not going to tackle this problem in this kernel (it's already complicated as it is), but you can shoot me a message and i'll be glad to help.<br>\nNext step is the text  segmentation, i will be releasing a kernel shortly after this one about it with the link to it in this kernel.<br>\nThis is my first public kernel, so please let me know if you have some remarks or found some mistakes.<br>\nThank you for sticking around.","773cfc3e":"As you can see the number of bounding boxes corresponds to the number of cells in the output.","0cbcb054":"It's time to write the expansion function, which joins boxes close to each other with a certain slide defined","ea944a49":"# Train the Model\nLet's create first a classifier using convolution layers, the output will be of shape (1, 1, 10) for a single image in order to classify from the 10 different classes.<br>\nLarger images would have larger output shape, but this will be used to find parts of it that were responsible for the largest activation, which also means the bounding boxes will be of (28,28) shape, the same as the numbers images.<br>\nNote :  We will talk about a technique to use in order to use other bounding box sizes.","45554855":"# Conclusion","dacdaafc":"# Introduction\nIn this kernel we will be getting our hands on a very famous computer vision problem, called photo OCR. More specifically the characters detection part of the problem, although i will be releasing in future kernels the rest of the other parts of the project (just to not make this kernel very crowded). <br>\nWe will be using pytorch, and relying on andrew's ng <a href=\"https:\/\/www.coursera.org\/learn\/machine-learning\">course<\/a> method to frame the problem.","9cc9974b":"# Text detection\nThis part of the problem consits of creating bouding boxes around the detected text of the image. There are many techniques to do it, like sliding windows. But in this kernel we are going to use a more efficient way, allowing us to bound the text in boxes in a single pass, thus reducing the computation. The technique is called Convolutional fast sliding windows.<br>\nFollowing this, we should expand the boxes close to each other in orther to detect words and phrases, so the end result should look something like this. <img src=\"https:\/\/raw.githubusercontent.com\/ritchieng\/machine-learning-stanford\/master\/w11_application_example_ocr\/photoocr4.png\" alt=\"end result\" \/>","dd889612":"# Load the data and dependencies\nWe will be using mnsit, and try to detect numbers in different parts of an image.","9fb9de42":"Let's see the results giving by the model","87d74aa3":"As you can see the output is no longer 1d array, but this is where the trick comes handy each cell in the output corresponds to a 28x28 frame in the original image. So we can see which parts of the original image activate the most (with a certain confidence level).","75e79e9f":"Let's draw the most confident boxes","84f6ede1":"let's  see the parts which activated the most in to output.<br>\nWe will be matching these outputs to a 28x28 frame in the input image shortly.","86292242":"Now let's write a function that maps each output cell to a 28x28 frame in the input, let's define each frame with (y, x, y_witdth, x_width)<br>\nThe first cell in the (29, 50) output which is (0, 0) corresponds to (0, 0, 28, 28) box in the input. The second cell (0, 1) corresponds to (0, 4, 28, 28) box, in other words the step is 4.\n","58b4fec5":"# <a>Content<\/a>\n\n1. <a href=\"#1\">Introduction<\/a>\n2. <a href=\"#2\">Photo OCR<\/a>\n3. <a href=\"#3\">Text detection<\/a>\n4. <a href=\"#4\">Load the data and dependencies<\/a>\n5. <a href=\"#5\">Train the model<\/a>\n6. <a href=\"#6\">Detect numbers in larger images<\/a>\n7. <a href=\"#7\">Conclusion<\/a>","ee3df97f":"# Detect numbers in larger images","7bfa7c2d":"Let's create a bigger image with random numbers"}}