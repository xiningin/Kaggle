{"cell_type":{"1e878558":"code","5444c563":"code","38eba3e4":"code","31f88cd5":"code","9958adf6":"code","0067ac1d":"code","9ef37d6b":"code","386e203c":"code","70b4c556":"code","fde5e8ca":"code","499fed3c":"code","f1b3dc3f":"code","bd542776":"code","fbb7fd96":"code","7999212a":"code","d1a037a7":"code","9f01371d":"code","5b5d0ae6":"code","b7f23ce0":"code","ff596dc9":"code","51d48610":"code","ff694b83":"code","6fa06a67":"code","16cf0ee9":"code","f608120a":"code","65c1e495":"code","400338e7":"code","2b12c37d":"code","2e5b6ff1":"code","72417be6":"markdown","829c749d":"markdown","3f8fe21d":"markdown","750dd0e4":"markdown","efb49a07":"markdown","55bffe1f":"markdown"},"source":{"1e878558":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5444c563":"df_sub = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\nprint(df_sub)\n","38eba3e4":"\nimport nltk\nimport string # library used to deal with some text data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score\nimport os\nimport re\nfrom sklearn.svm import SVC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n\nimport warnings\nwarnings.filterwarnings('ignore')","31f88cd5":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ndf_train","9958adf6":"X_test=(df_test['Name'])\nX_train=(df_train['Name'])\nX=[]\nX.extend(X_test)\nX.extend(X_train)\ny_train=(df_train['Survived'])\n#y_train=(df_train['Survived'])\n","0067ac1d":"df_train['Title'] = df_train[\"Name\"].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n#df_all = concat_df(df_train, df_test)\n#df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n#https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial","9ef37d6b":"df_train['Title'].value_counts().plot()","386e203c":"import re,string\n\ndef strip_links(text):\n    link_regex    = re.compile('((https?):((\/\/)|(\\\\\\\\))+([\\w\\d:#@%\/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n    links         = re.findall(link_regex, text)\n    for link in links:\n        text = text.replace(link[0], ', ')    \n    return text\n\ndef strip_all_entities(text):\n    #removing money hashtags and users entities\n    entity_prefixes = ['@','#','$','\u00a3','www']\n    for separator in  string.punctuation:\n        if separator not in entity_prefixes :\n            text = text.replace(separator,' ')\n    words = []\n    for word in text.split():\n        word = word.strip()\n        if word:\n            if word[0] not in entity_prefixes:\n                words.append(word)\n    return ' '.join(words)\n\nfor t in range(len(X)):\n    X[t]=strip_all_entities(strip_links(X[t]))\nfor t in range(len(X_test)):\n    X_test[t]=strip_all_entities(strip_links(X_test[t]))\nfor t in range(len(X_train)):\n    X_train[t]=strip_all_entities(strip_links(X_train[t]))\n#source> https:\/\/stackoverflow.com\/questions\/8376691\/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression","70b4c556":"import re\n# this will remove all numbers (\\d+) and puntuation simbols.\n\nREPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\nREPLACE_WITH_SPACE = re.compile(\"(<br\\s*\/><br\\s*\/>)|(\\-)|(\\\/)\")\nNO_SPACE = \"\"\nSPACE = \" \"\n\ndef preprocess_reviews(reviews):\n    \n    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n    \n    return reviews\n\nX= preprocess_reviews(X)\nX_test= preprocess_reviews(X_test)\nX_train= preprocess_reviews(X_train)","fde5e8ca":"#nltk.download()\n","499fed3c":"import nltk\ndef get_stemmed_text(corpus):\n    from nltk.stem.porter import PorterStemmer\n    stemmer = PorterStemmer()\n    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\nget_stemmed_text(X)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(ngram_range=(1, 1),stop_words='english')\nprint(vectorizer)\nX = vectorizer.fit_transform(X)\nX_test=vectorizer.transform(X_test)\nX_train=vectorizer.transform(X_train)\nprint(np.shape(X_train),np.shape(X_test),np.shape(X))","f1b3dc3f":"type(X)","bd542776":"from sklearn.feature_extraction.text import CountVectorizer\nfrom yellowbrick.text import FreqDistVisualizer\nfrom yellowbrick.datasets import load_hobbies\n\nfeatures   = vectorizer.get_feature_names()\nvisualizer = FreqDistVisualizer(features=features, orient='v')\nvisualizer.fit(X_test)\nvisualizer.show()\n#source https:\/\/www.scikit-yb.org\/en\/latest\/api\/text\/freqdist.html","fbb7fd96":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\naccu=[]\nc_values=[0.01, 0.05, 0.25, 0.5, 1,10,100]\n\n# vamos a probar LR con varios valores de regularizacion.\n\nfor c in range(len(c_values)):   \n    lr = LogisticRegression(C=c_values[c], solver='lbfgs')\n    lr.fit(X_train, y_train)\n    accu.append(accuracy_score(y_train, lr.predict(X_train)))\n    print (\"Accuracy for C=%s: %s\" \n           % (c_values[c], accu[c]))","7999212a":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,6))\nplt.plot(c_values,accu,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Accuracy vs. C Value')\nplt.xlabel('C')\nplt.ylabel('Accuracy')","d1a037a7":"final_model = LogisticRegression(C=c_values[accu.index(max(accu))], solver='lbfgs')\nclf = final_model.fit(X_train, y_train)\nfrom sklearn.model_selection import cross_val_score\nprint(\"Cross-Validation Score:\",cross_val_score(clf, X_train, y_train, cv=5).mean())","9f01371d":"\nfrom sklearn.model_selection import cross_val_score\nclass_names = (final_model.classes_)\n\nprint(\"Cross-Validation Score:\",cross_val_score(final_model, X_train, y_train, cv=5).mean())\nprint(\"For C=\",c_values[accu.index(max(accu))])\nfrom sklearn.metrics import classification_report\n\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(final_model, X_train, y_train,\n                             display_labels=class_names,\n                             cmap=plt.cm.Blues,\n                             normalize=None)\n#y_prima=(final_model.predict(X_train))\n#print(classification_report(np.array(X_train),np.array(y_prima), target_names=class_names))\n","5b5d0ae6":"accu=[]\ng_values=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1,'auto']\n# vamos a probar LR con varios valores de regularizacion.\n\nfor c in range(len(g_values)):   \n    model = svm.SVC(kernel='rbf', gamma=g_values[c]) # intentar con gamma = 2\n    clf = model.fit(X_train, y_train)\n    accu.append(accuracy_score(y_train, clf.predict(X_train)))\n    print (\"Accuracy for gamma=%s: %s\" \n           % (g_values[c], accu[c]))","b7f23ce0":"final_model = svm.SVC(kernel='linear')\nclf = final_model.fit(X_train, y_train)\n\nprint(\"Cross-Validation Score:\",cross_val_score(clf, X_train, y_train, cv=5).mean())","ff596dc9":"from sklearn.model_selection import cross_val_score\nclass_names = (final_model.classes_)\n\nprint(\"Cross-Validation Score:\",cross_val_score(final_model, X_train, y_train, cv=5).mean())\nprint(\"For C=\",c_values[accu.index(max(accu))])\nfrom sklearn.metrics import classification_report\n\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(final_model, X_train, y_train,\n                             display_labels=class_names,\n                             cmap=plt.cm.Blues,\n                             normalize=None)\n#y_prima=(final_model.predict(X_train))\n#print(classification_report(np.array(X_train),np.array(y_prima), target_names=class_names))\n","51d48610":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\naccu=[]\nc_values=[0.01, 0.05, 0.25, 0.5, 1,10,100]\n# vamos a probar LR con varios valores de regularizacion.\n\nfor c in range(len(c_values)):   \n    lr = LogisticRegression(C=c_values[c], solver='lbfgs')\n    lr.fit(X_train, y_train)\n    accu.append(accuracy_score(y_train, lr.predict(X_train)))\n    print (\"Accuracy for C=%s: %s\" \n           % (c_values[c], accu[c]))","ff694b83":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,6))\nplt.plot(c_values,accu,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Accuracy vs. C Value')\nplt.xlabel('C')\nplt.ylabel('Accuracy')","6fa06a67":"final_model = LogisticRegression(C=c_values[c], solver='lbfgs')\nclf = final_model.fit(X_train, y_train)\nfrom sklearn.model_selection import cross_val_score\nprint(\"Cross-Validation Score:\",cross_val_score(clf, X_train, y_train, cv=5).mean())","16cf0ee9":"from sklearn.model_selection import cross_val_score\nclass_names = (final_model.classes_)\n\nprint(\"Cross-Validation Score:\",cross_val_score(final_model, X_train, y_train, cv=5).mean())\nprint(\"For C=\",c_values[accu.index(max(accu))])\nfrom sklearn.metrics import classification_report\n\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(final_model, X_train, y_train,\n                             display_labels=class_names,\n                             cmap=plt.cm.Blues,\n                             normalize=None)\n#y_prima=(final_model.predict(X_train))\n#print(classification_report(np.array(X_train),np.array(y_prima), target_names=class_names))\n","f608120a":"y_prima=(final_model.predict(X_test))\nmy_submission = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Survived': y_prima})\nmy_submission.to_csv('my_submission.csv', index=False)","65c1e495":"import pandas as pd\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\nimport matplotlib.pyplot as plt","400338e7":"# entrenar el random forest\nrf = RandomForestClassifier().fit(X_train, y_train)\n","2b12c37d":"from sklearn.model_selection import cross_val_score\n\nprint('cross_val bosque = ', cross_val_score(RandomForestClassifier(), X_train, y_train, cv=10).mean())","2e5b6ff1":"y_prima=rf.predict(X_test)\nmy_submission = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Survived': y_prima})  \nmy_submission.to_csv('my_submission.csv', index=False)","72417be6":"# Kernel Lineal","829c749d":"# Entrenamiento con Logisitc Regression","3f8fe21d":"# LogisticRegression","750dd0e4":"# RandomForestClassifier","efb49a07":"# Kernel Radial","55bffe1f":"este fue el mejor resultado."}}