{"cell_type":{"e50b5c28":"code","c1a8f69e":"code","2db3de65":"code","7f635a69":"code","cfda5435":"code","8fea4264":"code","317733ff":"code","32bc8d29":"code","9f0d2612":"code","6ecbfbb7":"code","6035518a":"code","39fd24e5":"code","5f5adcff":"code","dc9fe184":"code","2e05b598":"code","9c18bdfb":"code","3a9ef7ea":"code","bab924bc":"code","b34efc5a":"code","59aa9dc9":"code","6b4b979f":"code","b5a2538a":"code","193d09be":"code","5bcf354a":"code","d87d0917":"code","dea484b8":"code","7e894e36":"code","aeff83f5":"code","a82761fc":"code","077e3b0d":"code","62cb7e77":"code","fe0ee95f":"code","a8c29091":"code","87b479ce":"code","463b627c":"code","df9ac979":"code","fc359bf8":"code","e48c903e":"code","c17d737f":"code","15f9cc16":"code","40690c3a":"code","24e4c1b8":"code","e3e821d6":"code","82d30515":"code","8a05c609":"code","2043edd2":"code","135da1c6":"code","1343a438":"code","3364438f":"code","fe127fda":"code","ccc4933f":"code","0d093dea":"code","3609953c":"code","9e862e5e":"code","41f3a8dd":"code","32a3ef01":"code","c5aeb994":"code","a1f22fbd":"code","cbdf4131":"code","8a7a76e2":"code","329bef2d":"code","12b65ce4":"markdown","fbacc1e2":"markdown","ceea567c":"markdown","72163286":"markdown","6a160af9":"markdown","851aa900":"markdown","aea3e46f":"markdown","08d70f3b":"markdown","b8328430":"markdown","5334929e":"markdown"},"source":{"e50b5c28":"import warnings\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier\nimport seaborn as sns","c1a8f69e":"PATH_TO_DATA = Path('..\/input\/flight-delays-fall-2018\/')","2db3de65":"train_df = pd.read_csv(PATH_TO_DATA \/ 'flight_delays_train.csv')","7f635a69":"train_df.head()","cfda5435":"# train_df['target'] = train_df['dep_delayed_15min'].map({'Y':1, 'N':0})","8fea4264":"test_df = pd.read_csv(PATH_TO_DATA \/ 'flight_delays_test.csv')","317733ff":"test_df.head()","32bc8d29":"train_df['flight'] = train_df['Origin'] + '-->' + train_df['Dest']\ntest_df['flight'] = test_df['Origin'] + '-->' + test_df['Dest']","9f0d2612":"train_df.head()","6ecbfbb7":"train_df.shape, test_df.shape","6035518a":"# from public kernel\n\n# Extract the labels\ntrain_y = train_df.pop('dep_delayed_15min')\ntrain_y = train_y.map({'N': 0, 'Y': 1})\n\n# Concatenate for preprocessing\ntrain_split = train_df.shape[0]\nfull_df = pd.concat((train_df, test_df))\n\n# Hour and minute\nfull_df['hour'] = full_df['DepTime'] \/\/ 100\nfull_df.loc[full_df['hour'] == 24, 'hour'] = 0\nfull_df.loc[full_df['hour'] == 25, 'hour'] = 1\nfull_df['minute'] = full_df['DepTime'] % 100\n\n# Season\nfull_df['summer'] = (full_df['Month'].isin(['c-6', 'c-7', 'c-8'])).astype(np.int32)\nfull_df['autumn'] = (full_df['Month'].isin(['c-9', 'c-10', 'c-11'])).astype(np.int32)\nfull_df['winter'] = (full_df['Month'].isin(['c-12', 'c-1', 'c-2'])).astype(np.int32)\nfull_df['spring'] = (full_df['Month'].isin(['c-3', 'c-4', 'c-5'])).astype(np.int32)\n\n# Daytime\nfull_df['daytime'] = pd.cut(full_df['hour'], bins=[0, 6, 12, 18, 23], include_lowest=True)\n","39fd24e5":"full_df.head()","5f5adcff":"full_df.drop('DepTime', axis=1, inplace=True)","dc9fe184":"full_df.head()","2e05b598":"# train_df['is_weekend'] = (train_df['DayOfWeek'] == \"c-6\") | (train_df['DayOfWeek'] == \"c-7\")\n# pd.crosstab(train_df['is_weekend'], train_df['target'])","9c18bdfb":"sns.countplot(data=train_df, x=\"is_weekend\", hue=\"target\");","3a9ef7ea":"sns.countplot(data=train_df, x=\"DayOfWeek\", hue=\"target\");","bab924bc":"sns.countplot(data=train_df, x=\"Month\", hue=\"target\");","b34efc5a":"(train_df.groupby('flight')['target'].sum() \/ train_df.groupby('flight')['target'].count()).sort_values(ascending=False)","59aa9dc9":"train_df[train_df['flight']=='DCA-->ROC']","6b4b979f":"train_df['DepTime'].describe()","b5a2538a":"import seaborn as sns\n\ntrain_df.groupby('target')['Distance'].median()","193d09be":"train_df['hour'] = (train_df['DepTime'] \/\/ 100).astype(str)\n(train_df.groupby('hour')['target'].sum() \/ train_df.groupby('hour')['target'].count()).sort_values(ascending=False)","5bcf354a":"(train_df.groupby('Origin')['target'].sum() \/ train_df.groupby('Origin')['target'].count()).sort_values(ascending=False)","d87d0917":"train_df[ train_df['Origin']=='GST' ]","dea484b8":"test_df[ test_df['Origin']=='GST' ]","7e894e36":"sns.distplot(train_df['Distance'])","aeff83f5":"sns.distplot(test_df['Distance'])","a82761fc":"train_df.head()","077e3b0d":"train_df_1 = train_df.drop(['DepTime', 'Origin', 'Dest', 'dep_delayed_15min'], axis=1)\ntrain_df_1.head()","62cb7e77":"train_df_1.info()","fe0ee95f":"X = train_df_1.drop('target', axis=1)\ny = train_df_1['target']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=17)","a8c29091":"X.head()","87b479ce":"cf = [f for f in X.columns if f != 'Distance']\ncf","463b627c":"from sklearn.preprocessing import OneHotEncoder\n\none_hot_enc = OneHotEncoder(categorical_features=['hour'], sparse=False)\n\nprint('Original number of features: \\n', X_train.shape[1], \"\\n\")\ndata_ohe_train = (one_hot_enc.fit_transform(X_train['hour']))\ndata_ohe_val = (one_hot_enc.transform(X_val['hour']))\nprint('Features after OHE: \\n', data_ohe_train.shape[1])","df9ac979":"from category_encoders import HashingEncoder\n\n","fc359bf8":"train_df = full_df[:train_split]\ntest_df = full_df[train_split:]","e48c903e":"cols = np.array(train_df.columns)\nprint(cols)\ncateg_feat_idx = np.where(cols != 'Distance')[0]\nprint(categ_feat_idx)","c17d737f":"train_df['daytime'].dtype","15f9cc16":"daytime_dict = {'(-0.001, 6.0]':'night', '(6.0, 12.0]':'morning', '(12.0, 18.0]':'afternoon', '(18.0, 23.0]':'evening'}\ntrain_df['daytime_f'] = train_df['daytime'].astype(str).map(daytime_dict)\ntest_df['daytime_f'] = test_df['daytime'].astype(str).map(daytime_dict)","40690c3a":"train_df.head()","24e4c1b8":"train_df.info()","e3e821d6":"train_df['is_weekend'] = (train_df['DayOfWeek'] == 6) | (train_df['DayOfWeek'] == 7)\ntest_df['is_weekend'] = (test_df['DayOfWeek'] == 6) | (test_df['DayOfWeek'] == 7)","82d30515":"train_df.head()","8a05c609":"train_df['Dep_hour_flag'] = ((train_df['hour'] >= 6) & (train_df['hour'] < 23)).astype('int')\ntest_df['Dep_hour_flag'] = ((test_df['hour'] >= 6) & (test_df['hour'] < 23)).astype('int')\ntrain_df.head()","2043edd2":"tmp_df = pd.read_csv('..\/input\/flight-delays-fall-2018\/flight_delays_train.csv')\ntmp_df['target'] = tmp_df['dep_delayed_15min'].map({'N':0, 'Y':1})\ntmp = tmp_df.groupby('Dest')['target'].sum() \/ tmp_df.groupby('Dest')['target'].count()\ntmp","135da1c6":"train_df['busy_dest'] = train_df['Dest'].map(tmp.to_dict())\ntest_df['busy_dest'] = test_df['Dest'].map(tmp.to_dict())","1343a438":"train_df['busy_day'] = ((train_df['DayOfWeek'] == 1) | (train_df['DayOfWeek'] == 4) | (train_df['DayOfWeek'] == 5)).astype('int')\ntest_df['busy_day'] = ((test_df['DayOfWeek'] == 1) | (test_df['DayOfWeek'] == 4) | (test_df['DayOfWeek'] == 5)).astype('int')","3364438f":"z = test_df['busy_dest']\ndel test_df['busy_dest']\ntest_df['busy_dest'] = z\ntest_df.head()","fe127fda":"X_test = test_df.drop('daytime', axis=1).values","ccc4933f":"X_train = train_df.drop('daytime', axis=1).values\ny_train = train_y.values\nX_test = test_df.drop('daytime', axis=1).values","0d093dea":"X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_train, y_train, \n                                                                test_size=0.3, \n                                                                random_state=17)","3609953c":"cols = np.array(train_df.drop('daytime', axis=1).columns)\nprint(cols)\ncateg_feat_idx = np.where((cols != 'Distance') & (cols != 'busy_dest'))[0]\nprint(categ_feat_idx)","9e862e5e":"params = {'iterations':2000,\n          'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'cat_features': categ_feat_idx,\n          #'ignored_features':[7, 17],\n          'task_type': 'GPU',\n          'border_count': 32,\n          'verbose': 200,\n          'random_seed': 17\n         }\nctb = CatBoostClassifier(**params)","41f3a8dd":"%%time\nctb.fit(X_train_part, y_train_part);","32a3ef01":"ctb_valid_pred = ctb.predict_proba(X_valid)[:, 1]\nctb_valid_pred","c5aeb994":"roc_auc_score(y_valid, ctb_valid_pred)","a1f22fbd":"%%time\nctb.fit(X_train, y_train,\n        cat_features=categ_feat_idx);","cbdf4131":"ctb_test_pred = ctb.predict_proba(X_test)[:, 1]","8a7a76e2":"with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    sample_sub = pd.read_csv(PATH_TO_DATA \/ 'sample_submission.csv', \n                             index_col='id')\n    sample_sub['dep_delayed_15min'] = ctb_test_pred\n    sample_sub.to_csv('ctb_pred.csv')","329bef2d":"!head ctb_pred.csv","12b65ce4":"# \u041f\u043e\u043f\u044b\u0442\u043a\u0438 \u0447\u0442\u043e-\u0442\u043e \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c","fbacc1e2":"**Read the data**","ceea567c":"**We got some 0.756 ROC AUC on the hold-out set.**","72163286":"**Create only one feature - \u201cflight\u201d (this you need to improve - add more features)**","6a160af9":"Now's your turn! Go and improve the model to beat **\"A2 baseline (10 credits)\"** - **0.75914** LB score. It's crucial to come up with some good features. \n\nFor discussions, stick to the **#a2_kaggle_fall2019** thread in the **mlcourse_ai_news** [ODS Slack](http:\/\/opendatascience.slack.com) channel. Serhii Romanenko (@serhii_romanenko) will be there to help. \n\nWelcome to Kaggle!\n\n<img src='https:\/\/habrastorage.org\/webt\/fs\/42\/ms\/fs42ms0r7qsoj-da4x7yfntwrbq.jpeg' width=50%>\n*from the [\"Nerd Laughing Loud\"](https:\/\/www.kaggle.com\/general\/76963) thread.*","851aa900":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\" \/>\n<\/center> \n     \n## <center>  [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \n\n#### <center> Author: [Yury Kashnitsky](https:\/\/yorko.github.io) (@yorko) \n\n# <center>Assignment #2. Fall 2019\n## <center> Part 2. Gradient boosting","aea3e46f":"**Train on the whole train set, make prediction on the test set. We got ~0.734 in the competition - \"Catboost starter\" baseline**","08d70f3b":"**Train Catboost with default arguments, passing only the indexes of categorical features.**","b8328430":"**Remember indexes of categorical features (to be passed to CatBoost)**","5334929e":"**In this assignment, you're asked to beat a baseline in the [\"Flight delays\" competition](https:\/\/www.kaggle.com\/c\/flight-delays-fall-2018).**\n\nThis time we decided to share a pretty decent CatBoost baseline, you'll have to improve the provided solution.\n\nPrior to working on the assignment, you'd better check out the corresponding course material:\n 1. [Classification, Decision Trees and k Nearest Neighbors](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse_open\/blob\/master\/jupyter_english\/topic03_decision_trees_kNN\/topic3_decision_trees_kNN.ipynb?flush_cache=true), the same as an interactive web-based [Kaggle Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-3-decision-trees-and-knn) \n 2. Ensembles:\n  - [Bagging](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse_open\/blob\/master\/jupyter_english\/topic05_ensembles_random_forests\/topic5_part1_bagging.ipynb?flush_cache=true), the same as a [Kaggle Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-5-ensembles-part-1-bagging)\n  - [Random Forest](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse_open\/blob\/master\/jupyter_english\/topic05_ensembles_random_forests\/topic5_part2_random_forest.ipynb?flush_cache=true), the same as a [Kaggle Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-5-ensembles-part-2-random-forest)\n  - [Feature Importance](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse_open\/blob\/master\/jupyter_english\/topic05_ensembles_random_forests\/topic5_part3_feature_importance.ipynb?flush_cache=true), the same as a [Kaggle Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-5-ensembles-part-3-feature-importance)\n 3. - [Gradient boosting](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse_open\/blob\/master\/jupyter_english\/topic10_boosting\/topic10_gradient_boosting.ipynb?flush_cache=true), the same as a [Kaggle Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-10-gradient-boosting) \n   - Logistic regression, Random Forest, and LightGBM in the \"Kaggle Forest Cover Type Prediction\" competition: [Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-10-practice-with-logit-rf-and-lightgbm) \n 4. You can also practice with demo assignments, which are simpler and already shared with solutions:\n  - \"Decision trees with a toy task and the UCI Adult dataset\": [assignment](https:\/\/www.kaggle.com\/kashnitsky\/a3-demo-decision-trees) + [solution](https:\/\/www.kaggle.com\/kashnitsky\/a3-demo-decision-trees-solution)\n  - \"Logistic Regression and Random Forest in the credit scoring problem\": [assignment](https:\/\/www.kaggle.com\/kashnitsky\/assignment-5-logit-and-rf-for-credit-scoring) + [solution](https:\/\/www.kaggle.com\/kashnitsky\/a5-demo-logit-and-rf-for-credit-scoring-sol)\n 5. There are also 7 video lectures on trees, forests, boosting and their applications: [mlcourse.ai\/video](https:\/\/mlcourse.ai\/video) \n 6. mlcourse.ai tutorials on [categorical feature encoding](https:\/\/www.kaggle.com\/waydeherman\/tutorial-categorical-encoding) (by Wayde Herman) and [CatBoost](https:\/\/www.kaggle.com\/mitribunskiy\/tutorial-catboost-overview) (by Mikhail Tribunskiy)\n 7. Last but not the least: [Public Kernels](https:\/\/www.kaggle.com\/c\/flight-delays-fall-2018\/notebooks) in this competition\n\n### Your task is to:\n 1. beat **\"A2 baseline (10 credits)\"** on Public LB (**0.75914** LB score)\n 2. rename your [team](https:\/\/www.kaggle.com\/c\/flight-delays-fall-2018\/team) in full accordance with A1 and the [course rating](https:\/\/docs.google.com\/spreadsheets\/d\/15e1K0tg5ponA5R6YQkZfihrShTDLAKf5qeKaoVCiuhQ\/) (to appear on 16.09.2019)\n \nThis task is intended to be relatively easy. Here you are not required to upload your reproducible solution.\n \n### <center> Deadline for A2: 2019 October 6, 20:59 CET (London time)"}}