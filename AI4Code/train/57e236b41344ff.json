{"cell_type":{"6f3a63cb":"code","1bd7f105":"code","31737979":"code","3a058724":"code","ffc62e7a":"code","738fd713":"code","0ad33f55":"code","f701df7f":"code","b6aec1c0":"code","42413b88":"code","df043fc1":"code","1b6ec79e":"code","d75e0e92":"code","2198632e":"code","b601f112":"code","ed5e8ef6":"code","aad6754b":"code","103c87af":"code","5269da37":"code","68e71e45":"code","1d52e32c":"code","5948c2e9":"code","127e2050":"code","fda6e28c":"code","4d78177a":"code","caf05828":"code","17171a13":"code","83377641":"code","021de061":"code","8811b2ec":"code","06b664a9":"code","5633966c":"code","73259edd":"code","f71cf1ed":"code","4f36dd04":"code","3ace4f8b":"code","d11c12ba":"code","dfc5e0a6":"code","04f46a26":"code","4caa6fdc":"code","2da85b0f":"code","5dc7420a":"code","6c87e610":"code","4e1018b7":"markdown","26fe14b0":"markdown","2552770b":"markdown","1cf70beb":"markdown","e77b07c2":"markdown"},"source":{"6f3a63cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom scipy.stats import skew,boxcox\nfrom sklearn.preprocessing import PowerTransformer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport os\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1bd7f105":"def eda_data(path):\n    df=pd.read_csv(path).drop(['id'],axis=1)\n    print(f\"\\n{'='*40}Showing the first 5 rows of data{'='*40}\\n\")\n    display(df.head(5))\n    print(f\"\\n{'='*40}Showing the last 5 rows of data{'='*40}\\n\")\n    display(df.tail(5))\n    print(f\"\\n{'='*40}Printing Non null value and Data type of the features{'='*40}\\n\")\n    print(df.info())\n    print(f\"\\n{'='*40}Data types in data{'='*40}\\n\")\n    \n    df_dt=df.dtypes.to_frame().value_counts().rename_axis('0').reset_index(name='counts')\n    df_dt.columns=['feature','count']\n    print(df_dt)\n    print(f\"\\n{'='*40}Feature descriptions{'='*40}\\n\")\n     \n    for i in df.columns:\n         print('='*40,i,'='*40)\n         stats(i,df)\n    print(f\"\\n{'='*40}Missing values{'='*40}\\n\")    \n    for i in df.columns:\n        \n        print(f\"Missing values in {i} : {df[i].isnull().sum()} ({(df[i].isnull().sum() \/ df[i].isnull().shape[0] *100).round(2)}%)\")\n    return df\n\n# General statistics\ndef stats(x,df):\n    \n    print(f\"Variable: {x}\")\n    print(f\"Type of variable: {df[x].dtype}\")\n    print(f\"Total observations: {df[x].shape[0]}\")\n    detect_null_val = df[x].isnull().values.any()\n    if detect_null_val:\n        print(f\"Missing values: {df[x].isnull().sum()} ({(df[x].isnull().sum() \/ df[x].isnull().shape[0] *100).round(2)}%)\")\n    else:\n        print(f\"Missing values? {df[x].isnull().values.any()}\")\n    print(f\"Unique values: {df[x].nunique()}\")\n    if df[x].dtype != \"O\":\n        print(f\"Min: {int(df[x].min())}\")\n        print(f\"25%: {int(df[x].quantile(q=[.25]).iloc[-1])}\")\n        print(f\"Median: {int(df[x].median())}\")\n        print(f\"75%: {int(df[x].quantile(q=[.75]).iloc[-1])}\")\n        print(f\"Max: {int(df[x].max())}\")\n        print(f\"Mean: {df[x].mean()}\")\n        print(f\"Std dev: {df[x].std()}\")\n        print(f\"Variance: {df[x].var()}\")\n        print(f\"Skewness: {scipy.stats.skew(df[x])}\")\n        print(f\"Kurtosis: {scipy.stats.kurtosis(df[x])}\")\n        print(\"\")\n        \n        # Percentiles 1%, 5%, 95% and 99%\n        print(\"Percentiles 1%, 5%, 95%, 99%\")\n        display(df[x].quantile(q=[.01, .05, .95, .99]))\n        print(\"\")\n    else:\n        print(f\"List of unique values: {df[x].unique()}\")\ndf=eda_data('..\/input\/song-popularity-prediction\/train.csv')\n\n","31737979":"#visualizing missing values with heatmap\n\nplt.figure(figsize=(20,10))\nsns.heatmap(df.isna().transpose(),\n            cmap=\"YlGnBu\",\n            cbar_kws={'label': 'Missing Data'})\nplt.title('Missing title as heatmap',weight='bold',size=20,color='blue')\nplt.xticks(color='blue',size=8)\nplt.yticks(color='blue',size=8)\nplt.show()","3a058724":"#lets analyze each and every feature in the datset\n#songpopularity value counts\ntotal=df['song_popularity'].value_counts().to_frame()\n#plotting pieplot\ncp=sns.color_palette('pastel')[0:2]\npercentage=total.apply(lambda x : (x\/x.sum())*100).round(2)\npercentage.plot(labels=['False','True'],kind='pie',colors=cp, subplots=True, autopct='%1.2f%%', explode= (0.05, 0.05),fontsize=12, figsize=(14,6))\n#here we can analyze it imbalanced data but,not much","ffc62e7a":"#lets check the distribution of each continous features \nplt.figure(figsize=(20,20))\nfor i in range(len(df.columns)):\n    if df[df.columns[i]].dtype =='float64':\n        plt.subplot(4,4,i+1)\n        sns.kdeplot(data=df, x=df.columns[i],fill=\"crest\")\n        plt.title(\"distribution of \" + df.columns[i])\n    else:\n        plt.subplot(4,4,i+1)\n        sns.countplot(x=df.columns[i], data=df, palette=\"Set3\")\n        plt.title(\"distribution of \" + df.columns[i])\n\n        \nplt.show()    ","738fd713":"con_val=[df.columns[i] for i in range(len(df.columns)) if df[df.columns[i]].dtype =='float64']\n\nplt.figure(figsize=(20, 20))\nfor i, col in enumerate(con_val):\n    plt.subplot(2,6 , i+1)\n    sns.violinplot(data=df, x='song_popularity',y=col)\n    plt.title(col)\n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\nplt.show()","0ad33f55":"#plt.subplots(figsize=(12,8))\nplt.figure(figsize=(20,20))\nfor i in range(len(df.columns)):\n    if df[df.columns[i]].dtype =='float64':\n        plt.subplot(4,4,i+1)\n        sns.kdeplot(data=df, x=df.columns[i], hue='song_popularity')\n        plt.title(\"distribution of \" + df.columns[i])\n   \nplt.show()","f701df7f":"plt.figure(figsize = (12,12))\ncorr=df.corr()\nplt.figure(figsize=(16, 6))\n# define the mask to set the values in the upper triangle to True\nmask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\nheatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='magma')\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16);","b6aec1c0":"sns.pairplot(\n    df.drop(['song_popularity','time_signature','audio_mode'],axis=1),\n    plot_kws=dict(marker=\"+\", linewidth=1),\n    diag_kws=dict(fill=False)\n)","42413b88":"test=pd.read_csv('..\/input\/song-popularity-prediction\/test.csv').drop('id',axis=1)#test data\ndf#traindata\n#finding missing valuues in both df and test\nncount_df=df.isna().mean() #pERCENTAGE OF MISSING VALUES\nncount_test=test.isna().mean()#pERCENTAGE OF MISSING VALUES test dataset\nncounts=pd.DataFrame([df.isna().mean(),test.isna().mean()]).T #total number of \nncounts=ncounts.rename(columns={0:'Train_data_missing',1:'Test_data_missing'})\nncounts.query(\"Train_data_missing > 0\").plot(kind='barh',figsize=(20,20),title='% of value missing')#plotting missing using barpplot\nnull_cols=ncounts.query(\"Train_data_missing > 0\").index","df043fc1":"\n#lets analyze the missing values row wise\ndf['n_missing']=df.isna().sum(axis=1)\ntest['n_missing']=df.isna().sum(axis=1)","1b6ec79e":"#lets plot it with bar\ndf['n_missing'].value_counts().plot(kind='bar',figsize=(10,10),title=\"% misssing value per sample\")\nplt.show()\ntest['n_missing'].value_counts().plot(kind='bar',figsize=(10,10),title=\"% misssing value per sample\")\nplt.show()","d75e0e92":"df['audio_mode'].value_counts()#lets find how many of these values are null.It will helps us to find any imbalance of null values \n#if null values in 0 is huge and 1 is nothing . Then it tells something.\npd.DataFrame(df.groupby('audio_mode')['n_missing'].mean()).T.plot(kind='bar',title=\"% percentage of null values missing IN audio_mode\")#grouping works by first it  take audio mode and divide the whole datset by 0 and 1 and divide the n_missing value too\n#we can find the imbalance in null values for categorical features also for continous features if we convert them to categorical(<(lessthan),>(greaterthan))\npd.DataFrame(df.groupby('time_signature')['n_missing'].mean()).T.plot(kind='bar',title=\"% percentage of null values missing IN audio_mode\")\npd.DataFrame(df.groupby('song_popularity')['n_missing'].mean()).T.plot(kind='bar',title=\"% percentage of null values missing IN audio_mode\")\n","2198632e":"train_missing_df=df[list(null_cols)].isna()","b601f112":"#lets add columns that tells the model whether a column has a missing value before imputation\ntrain_missing_df=df[null_cols].isna()#selecting coluns that has atleast one missing value .This approach is bets if u have less features\ntrain_missing_df.columns=[f\"{c}_missing\" for c in train_missing_df.columns ]\n\n#lets concate new columns with our training data\ndf=pd.concat([df,train_missing_df],axis=1)","ed5e8ef6":"#lets add columns that tells the model whether a column has a missing value before imputation\ntest_missing_df=test[null_cols].isna()#selecting coluns that has atleast one missing value .This approach is bets if u have less features\ntest_missing_df.columns=[f\"{c}_missing\" for c in test_missing_df.columns ]\n\n#lets concate new columns with our training data\ntest=pd.concat([test,test_missing_df],axis=1)\ndf['train']=1\ntest['train']=0","aad6754b":"#appending \ndf_tt=pd.concat([df.drop('song_popularity',axis=1),test],axis=0)\ncol_names=df_tt.columns","103c87af":"#lets dude impending on missing values in columns\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimp_mean = IterativeImputer(random_state=0)\nimp_mean.fit(df_tt)\ndf_tt=pd.DataFrame(imp_mean.transform(df_tt))\ndf_tt.columns=col_names","5269da37":"df_tt","68e71e45":"#lets do outlier detection on the above columns\ndf_tt.iloc[:,:13]","1d52e32c":"con_val\nplt.figure\n\nimport numpy as np \nimport pylab as p \n#lets plot the ouliers using box plot\ndef determine_outlier_thresholds_std(dataframe, col_name):\n    upper_boundary = dataframe[col_name].mean() + 3 * dataframe[col_name].std()\n    lower_boundary = dataframe[col_name].mean() - 3 * dataframe[col_name].std()\n    return lower_boundary, upper_boundary\npower = PowerTransformer(method='yeo-johnson', standardize=False)\ndf_tt[con_val]=power.fit_transform(df_tt[con_val])\n# we have 9 numerical values.finding skewness rate and also some statistical values\nfor r in con_val:\n            \n            print(f'Skewness for data {r} : {skew(df_tt[r])}')\n           \n            sns.histplot(data=df_tt, x= df_tt[r])\n            plt.show()\n            \n            \n            plt.boxplot(df_tt[r])\n           \n           \n            plt.show()\n ","5948c2e9":"#lets do outlier detection and removal based on distribution of the features before that lets cluster oru dataset using a clustering algorithm\n","127e2050":"df_tt[df_tt['train']==1.0]\n","fda6e28c":"#from sklearn.cluster import KMeans\n#kmeans = KMeans(n_clusters=2, random_state=2).fit_predict(X_train)\n# get centroids\n#kmeans\n","4d78177a":"#df['song_popularity']\n#from sklearn.metrics import accuracy_score\n#accuracy_score(kmeans,df['song_popularity'])","caf05828":"#from box plots we know the data with less skewness or havong normal istrubtions\ncol_nd=['song_duration_ms','instrumentalness','liveness','tempo','key']\n# lets do clipping in normal distributed datas above\n\nfor r in col_nd:\n    ll,ul=determine_outlier_thresholds_std(df_tt,r)\n    df_tt[r]=np.clip( df_tt[r], a_min =ll, a_max = ul)\n    print(ll,ul)\n    sns.histplot(data=df_tt, x= df_tt[r])\n    plt.show()\n","17171a13":"#from box plots we know the data with skewness\n\ncol_nd=['song_duration_ms','instrumentalness','liveness','tempo','key']\n# iqr method is used for skeweded data in the df_tt\nfor r in df_tt[con_val].drop(col_nd,axis=1):\n     \n    percentile25 = df_tt[r].quantile(0.25)\n    percentile75 = df_tt[r].quantile(0.75)\n    iqr=percentile75-percentile25\n    upper_limit = percentile75 + 1.5 * iqr\n    lower_limit = percentile25 - 1.5 * iqr\n    df_tt[r]=np.clip(df_tt[r], a_min =lower_limit, a_max = upper_limit)\n    sns.histplot(data=df_tt, x= df_tt[r])\n    plt.show()\n    \n","83377641":"col=df_tt.drop('train',axis=1).columns\n#lets do standard scaling\n#there will be data leakage for, competetions its ok\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nscaler =  StandardScaler()\ntrain_col=df_tt['train']\ntrain_col_ns=df_tt[df_tt['train']==1].drop('train',axis=1)\ndf_tt=scaler.fit_transform(df_tt.drop('train',axis=1))\ndf_tt=pd.DataFrame(df_tt,columns = col)\ndf_tt=pd.concat([df_tt,train_col],axis=1)\n","021de061":"df_tt","8811b2ec":"#lets do feature selection, creations\n#from here onwards we only use training datasets\ntrain_df=pd.concat([df['song_popularity'],df_tt[df_tt['train']==1].drop('train',axis=1)],axis=1)","06b664a9":"test_df=df_tt[df_tt['train']==0].drop('train',axis=1)","5633966c":"from sklearn.feature_selection import mutual_info_classif\nimportances=mutual_info_classif(train_df.drop('song_popularity',axis=1),train_df['song_popularity'])\nplt_1 = plt.figure(figsize=(100, 100))\nfeat_imp=pd.Series(importances,train_df.drop('song_popularity',axis=1).columns[0:len(train_df.drop('song_popularity',axis=1).columns)])\nfeat_imp.plot(kind='barh')\nplt.show()\nprint(importances)","73259edd":"sel_f=['song_duration_ms','acousticness','energy','key','instrumentalness','audio_mode','tempo','speechiness','time_signature','audio_valence','n_missing','danceability_missing','liveness_missing','loudness_missing']\n#!pip install mlxtend\n'''\nfrom mlxtend.feature_selection import SequentialFeatureSelector\nffs= SequentialFeatureSelector(model,k_features='best', \n          forward=True, \n          floating=False, \n          scoring='accuracy',\n          cv=4,\n          n_jobs=-1)\nffs.fit(train_df[sel_f],train_df['song_popularity'])\nfeat=list(ffs.k_feature_names_)\n'''\n\n","f71cf1ed":"n_estimators: 50\n\t\tlearning_rate: 0.3010945\n\t\tnum_leaves: 20\n\t\tmax_depth: 3\n\t\tmin_data_in_leaf: 50\n\t\tlambda_l1: 0\n\t\tlambda_l2: 8","4f36dd04":"\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom numpy import std\n#!pip install lightgbm\nmodel = LGBMClassifier(objective=\"binary\",n_estimators=  50,\n\t\tlearning_rate=  0.3010945,\n\t\tnum_leaves=20,\n\t\tmax_depth= 3,\n\t\tmin_data_in_leaf= 50,\n\t\tlambda_l1= 0,\n\t\tlambda_l2= 8)\n# evaluate the model\n#cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=120399)\n#n_scores = cross_val_score(model, train_df[sel_f], train_df['song_popularity'], scoring='roc_auc', cv=cv, n_jobs=-1)\n# report performance\nmodel.fit(train_df.drop('song_popularity',axis=1),train_df['song_popularity'])\nmodel.predict([train_df.drop('song_popularity',axis=1).loc[2]])","3ace4f8b":"submission=pd.read_csv('..\/input\/song-popularity-prediction\/sample_submission.csv')","d11c12ba":"ar=model.predict(test_df)\nsub=pd.DataFrame(ar)\nsubmission['song_popularity']=ar","dfc5e0a6":"submission.to_csv('submission.csv')","04f46a26":"\n\nbest_trials=[]\nimport optuna\nfrom lightgbm import LGBMClassifier\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nfrom sklearn.model_selection import cross_val_score\nfrom optuna.integration import LightGBMPruningCallback\nfrom sklearn.metrics import log_loss,roc_auc_score,accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\n\n\ndef objective(trial, X, y):\n    param_grid = {\n        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        \"n_estimators\": trial.suggest_int('n_estimators',10,80,step=1),\n        \"learning_rate\":  trial.suggest_float('learning_rate', 0.1,2.5,step = 0.0000005),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 100, step=2),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10,800, step=10),\n        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=1),\n        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=1),\n        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\n            \"bagging_fraction\", 0.1, 0.95, step=0.05\n        ),\n        \"feature_fraction\": trial.suggest_float(\n            \"feature_fraction\", 0.1, 0.95, step=0.05\n        ),\n    }\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1009)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        model = LGBMClassifier(objective=\"binary\", **param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            eval_metric=\"auc\",\n            early_stopping_rounds=100,\n        #    callbacks=[           LightGBMPruningCallback(trial, \"binary_error\")],  # Add a pruning\n        )\n        preds = model.predict_proba(X_test)[:,1]\n        cv_scores[idx] = roc_auc_score(y_test, preds)\n\n    return np.mean(cv_scores)\n\n\nstudy = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\nfunc = lambda trial: objective(trial,train_col_ns.iloc[:,:14], train_df['song_popularity'])\nstudy.optimize(func, n_trials=20)\n\nprint(f\"\\tBest value (auc): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\n\nfor key, value in study.best_params.items():\n        print(f\"\\t\\t{key}: {value}\")","4caa6fdc":"train_df.iloc[:,:14]","2da85b0f":"\n#dataset train_df.iloc[:,:14]\nbest_trials=[]\nimport optuna\nfrom lightgbm import LGBMClassifier\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nfrom sklearn.model_selection import cross_val_score\nfrom optuna.integration import LightGBMPruningCallback\nfrom sklearn.metrics import log_loss,roc_auc_score,accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\n\n\ndef objective(trial, X, y):\n    param_grid = {\n        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        \"n_estimators\": trial.suggest_int('n_estimators',20,80,step=2),\n        \"learning_rate\":  trial.suggest_float('learning_rate', 0.1,2.5,step = 0.0000005),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 100, step=2),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10,1000, step=5),\n        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 75, step=1),\n        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 75, step=1),\n    \n    }\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=112)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        model = LGBMClassifier(objective=\"binary\", **param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            eval_metric=\"auc\",\n            early_stopping_rounds=100,\n        #    callbacks=[           LightGBMPruningCallback(trial, \"binary_error\")],  # Add a pruning\n        )\n        preds = model.predict_proba(X_test)[:,1]\n        cv_scores[idx] = roc_auc_score(y_test, preds)\n\n    return np.mean(cv_scores)\n\n\nstudy = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\nfunc = lambda trial: objective(trial, train_df.iloc[:,:14].drop('song_popularity',axis=1), train_df['song_popularity'])\nstudy.optimize(func, n_trials=20)\n\nprint(f\"\\tBest value (auc): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\n\nfor key, value in study.best_params.items():\n        print(f\"\\t\\t{key}: {value}\")","5dc7420a":"\n#dataset train_df.drop('song_popularity',axis=1)\nbest_trials=[]\nimport optuna\nfrom lightgbm import LGBMClassifier\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nfrom sklearn.model_selection import cross_val_score\nfrom optuna.integration import LightGBMPruningCallback\nfrom sklearn.metrics import log_loss,roc_auc_score,accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\n\n\ndef objective(trial, X, y):\n    param_grid = {\n        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        \"n_estimators\": trial.suggest_int('n_estimators',13,50,step=1),\n        \"learning_rate\":  trial.suggest_float('learning_rate', 0.1,2.5,step = 0.0000005),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 30, step=2),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 50,150, step=5),\n        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 20, step=1),\n        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 20, step=1),\n    \n    }\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=112)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        model = LGBMClassifier(objective=\"binary\", **param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            eval_metric=\"auc\",\n            early_stopping_rounds=100,\n        #    callbacks=[           LightGBMPruningCallback(trial, \"binary_error\")],  # Add a pruning\n        )\n        preds = model.predict_proba(X_test)[:,1]\n        cv_scores[idx] = roc_auc_score(y_test, preds)\n\n    return np.mean(cv_scores)\n\n\nstudy = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\nfunc = lambda trial: objective(trial, train_df.drop('song_popularity',axis=1), train_df['song_popularity'])\nstudy.optimize(func, n_trials=20)\n\nprint(f\"\\tBest value (auc): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\n\nfor key, value in study.best_params.items():\n        print(f\"\\t\\t{key}: {value}\")","6c87e610":"import warnings\nwarnings.filterwarnings(\"ignore\")","4e1018b7":"### Outlier Detection","26fe14b0":"## Lets take missing values and find insights\n","2552770b":"## BASIC INFORMATION\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#8585e0;\n           font-size:150%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 2px;color:white;\">  <ol>\n    <li> Printing first and last five rows of data<\/li>\n    <li> Finding count,mean ,medan,SD, skewness,variance and Data types of features<\/li>\n    <li>Printing Null values and it's count<\/li> <\/ol>   \n              <\/p>\n<\/div>","1cf70beb":"## ANALYZING FEATURE DISTRIBUTION\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#8585e0;\n           font-size:150%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 2px;color:white;\">  <ol>\n    <li>Checking whether the data is imbalnced or not<\/li>\n    <li>Finding distribution of continous features<\/li>\n    <\/ol>\n              <\/p>\n<\/div>","e77b07c2":"## FINDING COORELATION\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#8585e0;\n           font-size:150%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 2px;color:white;\">  <ol>\n    <li> Checking correlation and ploting heatmap <\/li>\n    <li> Plotting pairplot <\/li>\n   <\/ol>   \n              <\/p>\n<\/div>"}}