{"cell_type":{"35f19ac7":"code","00c7205c":"code","feaf2380":"code","2b87c123":"code","c973c112":"code","fe95e56f":"code","3a63a688":"code","ef4f97d3":"code","8b46a660":"code","9ebfc3d0":"code","ce5b5913":"code","34390360":"code","13b56395":"code","8e6e29be":"code","33df7f02":"code","275e696b":"code","5aac6583":"code","fb615e66":"code","914ef872":"code","30c03136":"code","842cddb0":"code","dbc54d64":"code","3383d1c2":"code","e766a572":"code","ec609cec":"code","7cab3d4f":"code","bada39e3":"code","372067ee":"code","c6792a1b":"code","e3425081":"code","760d5100":"code","f95cd921":"code","4200d06c":"code","0a3eae54":"markdown","dba2e6f2":"markdown","40d3dac2":"markdown","51886831":"markdown","fb276e2a":"markdown","ee5345d6":"markdown","b4568e68":"markdown","91d4e3c6":"markdown","333eef83":"markdown","7f4ac034":"markdown","1bf0dcdd":"markdown","d1cde8c0":"markdown","1c90761c":"markdown","2231773c":"markdown","e6800891":"markdown"},"source":{"35f19ac7":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","00c7205c":"# Maths and data imports\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\n# Plots imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# ML modeling imports\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom xgboost import XGBRegressor","feaf2380":"%matplotlib inline\nsns.set()","2b87c123":"import warnings\nwarnings.filterwarnings('ignore')","c973c112":"train_path = '\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv'\ntest_path = '\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv'\n\ntrain = pd.read_csv(train_path, index_col='id')\ntest = pd.read_csv(test_path, index_col='id')","fe95e56f":"train_df = train.copy()\ntest_df = test.copy()","3a63a688":"train_df.head()","ef4f97d3":"# check the shape, to find the number of examples and features in training data\ntrain_df.shape","8b46a660":"# check for null values\ntrain_df.isnull().sum()","9ebfc3d0":"# let check for duplicate examples\ntrain_df.duplicated().sum()","ce5b5913":"# let's check the dtype (examine no of categorical and numerical features)\ntrain_df.info()","34390360":"# lets see some stats\ntrain_df.describe().T","13b56395":"fig, axs = plt.subplots(7, 2, figsize=(15, 30))\n\nfor i, ax in zip(train_df.drop(['target'], axis=1), axs.flatten()):\n    sns.distplot(train_df[i], ax=ax, label='Train')\n    sns.distplot(test_df[i], ax=ax, color='red', label='Test')\n    ax.set_xlabel(i)\n    ax.legend(loc='best')\nplt.show()","8e6e29be":"# lets check for (multi)collinearity\nsns.pairplot(train_df)\nplt.show()","33df7f02":"fig = plt.figure(figsize=(20, 20))\nsns.heatmap(train_df.corr(), annot=True)\nplt.show()","275e696b":"corr = train_df.corr()\n\nfor col in corr.columns:\n    for rel_col in corr[col][corr[col] > 0.7].index:\n        if rel_col != col:\n            print((col, rel_col))","5aac6583":"# lets check for outliers and skewness\nfig = plt.figure(figsize=(20, 10))\nsns.boxplot(data=train_df.drop(['target'], axis=1))\nplt.xlabel('Exploratory Variables')\nplt.ylabel('Values')\nplt.show()","fb615e66":"# lets check for skewness again\nfig = plt.figure(figsize=(20, 10))\nsns.violinplot(data=train_df.drop(['target'], axis=1))\nplt.xlabel('Exploratory Variable')\nplt.ylabel('Values')\nplt.show()","914ef872":"# let draw its distribution, if it's not normal let's convert it to normal\nfig = plt.figure(figsize=(10, 5))\nsns.distplot(train_df['target'])\nplt.show()","30c03136":"# import statsmodels.api as sm\n\n# fig = plt.figure(figsize=(10, 5))\n# sm.qqplot(train_df['target'], line='s')\n# plt.show()","842cddb0":"z = (train_df.target - train_df.target.mean()) \/ train_df.target.std()\n\nfig = plt.figure(figsize=(5, 5))\nstats.probplot(z, dist='norm', plot=plt)\nplt.xlabel('Theoretical Quantiles')\nplt.ylabel('Experimental Quantiles')\nplt.show()","dbc54d64":"X = train_df.drop(['target'], axis=1)\ny = train_df['target']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)","3383d1c2":"X_train.head()","e766a572":"X_valid.head()","ec609cec":"y_train.head()","7cab3d4f":"y_valid.head()","bada39e3":"models = {\n    'RFR': RandomForestRegressor,\n    'ABR': AdaBoostRegressor,\n    'XGBR': XGBRegressor\n}","372067ee":"def fit_model(name, model, train_ds, valid_ds):\n    X, y = train_ds\n    X_val, y_val = valid_ds\n    \n    model.fit(X, y)\n    y_hat = model.predict(X)\n    y_hat_val = model.predict(X_val)\n    \n    mse = mean_squared_error(y, y_hat)\n    mse_val = mean_squared_error(y_val, y_hat_val)\n    \n    print(f'Model: {name}, Train MSE: {mse}, Val MSE: {mse_val}')","c6792a1b":"n_est = [10, 25, 50, 100, 200]\nfor i in range(len(n_est)):\n    print(f'n_estimators: {n_est[i]}')\n    for name, model in models.items():\n        model = model(n_estimators=n_est[i])\n        fit_model(name, model, (X_train, y_train), (X_valid, y_valid))\n    print('-'*20)","e3425081":"abr = AdaBoostRegressor(n_estimators=100)\nxgbr = XGBRegressor(n_estimators=50)\n\nabr.fit(X_train, y_train)\nxgbr.fit(X_train, y_train)\n\ny_hat1, y_hat2 = abr.predict(X_valid), xgbr.predict(X_valid)\ny_hat = (y_hat1+y_hat2)\/2\nmse = mean_squared_error(y_valid, y_hat)\nprint(f'Ensembel MSE: {mse}')","760d5100":"pred = (abr.predict(test_df) + xgbr.predict(test_df))\/2\nsubmission = pd.DataFrame(pred, columns=['target'])\nsubmission = pd.concat([pd.DataFrame(test_df.index), submission], axis=1)\n\nsubmission.head()","f95cd921":"fig = plt.figure(figsize=(20, 10))\nsns.distplot(train['target'], label='Train')\nsns.distplot(submission['target'], color='red', label='Test')\nplt.show()","4200d06c":"submission.to_csv('result.csv', index=False, header=True)","0a3eae54":"## Data preprocessing","dba2e6f2":"## Train Dev Split","40d3dac2":"The notebook was created after studying [this notebook](https:\/\/www.kaggle.com\/ankitverma2010\/tubular-playground-regression). ","51886831":"## Analysing the response\/target variable","fb276e2a":"**Conclusion**\n\n* The exploratory variables seem almost in the same range, so, we'll skip standardization for now.\n* Few variables such as, count2, count3, count5, count8 etc seem to be skewed. Lets confirm it.","ee5345d6":"**Conclusion**\n\nThis seems like a bimodeal distribution. It could be the case that it is created by mixing two normal distribuitons. Let's confirm.","b4568e68":"**Conclusion**\n\nIt seems `AdaBoostRegressor` and `XGBRegressor` tend to perform good with `n_estimators=50` and `n_estimators=100` respectively. At `n_estimators=100`, `XGBRegressor` seem to slightly overfit.\n\nLets try an ensembel of both.","91d4e3c6":"## Import Libraries","333eef83":"## Models","7f4ac034":"**Conclusion**\n\nSeems like a normal distribution.","1bf0dcdd":"**Conclusion**\n\n* count5, count13 seem to be right skewed\n* Most of the variables seem to have multiple peaks \n\nMaybe they have muliple clusters","d1cde8c0":"**Conclusion**\n\nThe result seems to have a normal distribution, but its a huge peak, so, we can expect an okaish performance on the test set. We can reiterate and try out a couple of things to make the model better.","1c90761c":"**Conclusion**\n\n* The explanatory variables don't seem to be multicollinear\n* No explanatory variable seems to be correlated to the targets\n* Further inspection required","2231773c":"## EDA","e6800891":"**Conclusion**\n\nThere seem to be quite a few correlated (positively) variables. Let's try leaving them for now. (may be we'll look at them in the next iteration)"}}