{"cell_type":{"4ff1a1eb":"code","438a4ae3":"code","76cd3cf9":"code","cc15d480":"code","916689cd":"code","af01d13f":"code","cf5bfc69":"code","d873cd5d":"code","0f57a3c0":"code","7b4a83b8":"code","a568dc1f":"code","bb86fb21":"code","b40f7e32":"code","35117849":"code","7b7aa6c1":"code","907e808d":"code","710bb855":"code","ca8ccf91":"code","b6ae89c6":"code","392e2a71":"code","a968223b":"code","a33debf0":"code","cfc88b35":"code","ee952355":"code","3602c2a8":"code","d69fe26c":"code","a45b2e70":"code","c95d434b":"code","5a18abcc":"code","48250481":"code","de9d836c":"code","616ddbbb":"code","6cfba860":"code","5787d48c":"code","9092538b":"code","883377a7":"code","4bee694d":"code","70d18b61":"code","c5940ca2":"code","5b93e8a5":"code","bad57906":"code","c8e03e72":"code","7c9cfacc":"code","c3475df7":"markdown","fefe8349":"markdown","e908c439":"markdown","f1d6751b":"markdown","0d85523a":"markdown","c91bc1af":"markdown","ce22bdf3":"markdown","c61aaaa6":"markdown","7827279e":"markdown","bda3f549":"markdown"},"source":{"4ff1a1eb":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Libraries to help with reading and manipulating data\nimport numpy as np\nimport pandas as pd\n\n# Libraries to help with data visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score,roc_auc_score, roc_curve, confusion_matrix\nfrom sklearn.model_selection import train_test_split ,GridSearchCV, RandomizedSearchCV,StratifiedKFold, cross_val_score\n\n#libraries for UP\/Down sampling, Imputation and Pipelines\nfrom sklearn.pipeline import Pipeline, make_pipeline\n#libraries to help with model building\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (\n    AdaBoostClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier,BaggingClassifier)\nfrom xgboost import XGBClassifier","438a4ae3":"path = '\/kaggle\/input\/credit-card-customers\/BankChurners.csv'\ndata = pd.read_csv(path) #load the data","76cd3cf9":"df= data.copy()\nprint(f'There is {df.shape[0]} rows and {df.shape[1]} columns in the dataset')\nnp.random.seed(4)\ndf.sample(10)","cc15d480":"df.info()\n","916689cd":"df.nunique()","af01d13f":"df.drop(['CLIENTNUM','Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n        'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'],axis=1,inplace = True)\ndf['Dependent_count'] = df['Dependent_count'].astype('category')\ncols = df.select_dtypes(['object']) #selecting all object datatype\nfor i in cols.columns:\n    df[i] = df[i].astype('category')","cf5bfc69":"df.describe().T","d873cd5d":"cat_cols = df.select_dtypes(['category'])\nfor i in cat_cols.columns:\n    print(cat_cols[i].value_counts())\n    print('-'*50)","0f57a3c0":"#Stacked plot of categorical variables with Personal Loans\ndef stacked_plot(x):\n    sns.set(palette='Set1')\n    tab1 = pd.crosstab(x,df['Attrition_Flag'],margins=True)\n    print(tab1)\n    print('-'*120)\n    tab = pd.crosstab(x,df['Attrition_Flag'],normalize='index')\n    tab.plot(kind='bar',stacked=True,figsize=(10,5))\n    plt.legend(loc='lower left', frameon=True)\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\n    plt.ylabel('Percentage')\n    plt.show()","7b4a83b8":"stacked_plot(df.Gender)","a568dc1f":"stacked_plot(df.Dependent_count)","bb86fb21":"stacked_plot(df.Education_Level)","b40f7e32":"stacked_plot(df.Marital_Status)","35117849":"stacked_plot(df.Income_Category)","7b7aa6c1":"stacked_plot(df.Card_Category)","907e808d":"corr= df.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(corr,annot= True,vmin=-0.5,vmax=1, cmap='coolwarm',linewidths=0.75)\nplt.show()","710bb855":"#Let's group and plot certain Numerical variables together for a comparison study with the target variable\ncols1 = df[['Total_Relationship_Count',\n           'Months_Inactive_12_mon',\n            'Contacts_Count_12_mon']].columns.tolist()\ncols2 = df[['Credit_Limit',\n           'Total_Revolving_Bal',\n           'Avg_Open_To_Buy',\n            'Avg_Utilization_Ratio'\n            ]].columns.tolist()\ncols3 = df[['Total_Trans_Amt',\n           'Total_Trans_Ct',\n           'Total_Ct_Chng_Q4_Q1',\n            'Total_Amt_Chng_Q4_Q1']].columns.tolist()\ncols4 = df[['Customer_Age','Months_on_book']]","ca8ccf91":"def bi_plot(x):\n    plt.figure(figsize=(9,7))\n    for i,count in enumerate(x):\n        plt.subplot(2,2,i+1)\n        #plt.subplots_adjust(hspace=3, wspace=7)\n        sns.boxplot(df['Attrition_Flag'],df[count],palette=\"YlOrBr_r\",showmeans=True)\n        plt.title('Attrition_Flag Vs '+count,fontsize=12,fontweight = 'bold')\n        plt.tight_layout()","b6ae89c6":"bi_plot(cols1)","392e2a71":"bi_plot(cols2)","a968223b":"bi_plot(cols3)","a33debf0":"bi_plot(cols4)","cfc88b35":"#  Total_Trans_Ct Vs Total_Trans_Amt\nplt.figure(figsize=(15,7))\nsns.lineplot(df.Total_Trans_Ct,df.Total_Trans_Amt,hue=df.Attrition_Flag)\n","ee952355":"plt.figure(figsize=(15,7))\nsns.scatterplot(x='Total_Ct_Chng_Q4_Q1', y='Total_Amt_Chng_Q4_Q1',hue='Attrition_Flag',\n             data=df)","3602c2a8":"plt.figure(figsize=(15,7))\nsns.jointplot(df.Avg_Utilization_Ratio,df.Total_Revolving_Bal, hue = df.Attrition_Flag)","d69fe26c":"X= df.drop(['Attrition_Flag'],axis=1)\nY = df['Attrition_Flag'].apply(lambda x: x=='Attrited Customer').astype('int')\n# Splitting data into training and test set:\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=23,stratify=Y)\nprint(X_train.shape, X_test.shape)","a45b2e70":"X_train=pd.get_dummies(X_train,drop_first=True)\nX_test=pd.get_dummies(X_test,drop_first=True)\nprint(X_train.shape, X_test.shape)","c95d434b":"def make_confusion_matrix(model,y_actual):\n    '''\n    y_predict: prediction of class\n    y_actual : ground truth  \n    '''\n    sns.set(font_scale=2.0) # to set font size for the matrix\n    y_predict = model.predict(X_test)\n    cm=confusion_matrix(y_actual,y_predict)\n    group_names = ['True -ve','False +ve','False -ve','True +ve']\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cm.flatten()\/np.sum(cm)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2,v3 in\n              zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    plt.figure(figsize = (10,7))\n    sns.heatmap(cm, annot=labels,fmt='',cmap='Blues')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","5a18abcc":"def scores(model,train_x,train_y,flag=True):\n    \"\"\" model : classifier to predict X values \"\"\"\n    score_list=[] # creating an empty list to store the accuracy and f1(metric of interst)\n    \n    y_pred_train = model.predict(train_x)\n    y_pred_test = model.predict(X_test)\n    \n    train_acc = metrics.accuracy_score(train_y,y_pred_train)\n    test_acc = metrics.accuracy_score(y_test,y_pred_test)\n    \n    train_recall = metrics.recall_score(train_y,y_pred_train)\n    test_recall = metrics.recall_score(y_test,y_pred_test)\n    \n    train_precision = metrics.precision_score(train_y,y_pred_train)\n    test_precision = metrics.precision_score(y_test,y_pred_test)\n    \n    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))\n    \n    if flag== True:\n        print(\"Accuracy on training set : \",metrics.accuracy_score(train_y,y_pred_train))\n        print(\"Accuracy on test set : \",metrics.accuracy_score(y_test,y_pred_test))\n\n        print(\"\\nRecall on training set : \",metrics.recall_score(train_y,y_pred_train))\n        print(\"Recall on test set : \",metrics.recall_score(y_test,y_pred_test))\n    \n        print(\"\\nPrecision on training set : \",metrics.precision_score(train_y,y_pred_train))\n        print(\"Precision on test set : \",metrics.precision_score(y_test,y_pred_test))\n    \n    elif flag == False:\n        return score_list #return this when flag is False","48250481":"#creating an empty list to store models\nall_models = []\n\n#Appending pipelines to the empty list\nall_models.append(('ADA',make_pipeline\n                   (AdaBoostClassifier(random_state=23)\n                   )))\nall_models.append(('GRB',make_pipeline\n                   (GradientBoostingClassifier(random_state=23)\n                   )))\nall_models.append(('XGB',make_pipeline\n                   (XGBClassifier(random_state=23,eval_metric='logloss')\n                   )))","de9d836c":"for i,j in all_models:\n    j.fit(X_train,y_train)\n    scoring='recall'\n    kfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=23)#Setting number of splits equal to 5\n    #performing Cross-validation on undersampled train set\n    cv_result=cross_val_score(estimator=j, X=X_train, y=y_train, scoring=scoring, cv=kfold) \n    print(f'{i}: {cv_result.mean()}')","616ddbbb":"for i,j in all_models:\n    print(i)\n    scores(j,X_train,y_train)\n    print('-'*35)","6cfba860":"%%time\n#Creating pipeline\npipe_ADA2 = make_pipeline(AdaBoostClassifier(random_state=23))\n\n# Parameter Grid\nparameters = {\n    \"adaboostclassifier__base_estimator\":[DecisionTreeClassifier(max_depth=1,random_state=23),\n                                         DecisionTreeClassifier(max_depth=2,random_state=23),\n                                         DecisionTreeClassifier(max_depth=3,random_state=23)],\n    \"adaboostclassifier__n_estimators\": np.arange(10,60,5),\n    \"adaboostclassifier__learning_rate\": [0.05,0.15,0.45,0.75]} \n                                                   \n\n#scoring metric\nscoring = metrics.make_scorer(metrics.recall_score)\n\n# GridSearch CV\nRS_cv = RandomizedSearchCV(estimator=pipe_ADA2,scoring=scoring,param_distributions=parameters,random_state=23,n_iter=50,n_jobs=-1,cv=5)\nRS_cv.fit(X_train,y_train)\n\nprint(RS_cv.best_params_)\nprint(RS_cv.best_score_)","5787d48c":"#creating new pipeline with the best parameters \nada_tuned2 = make_pipeline(AdaBoostClassifier\n                         (base_estimator=DecisionTreeClassifier(max_depth=3, random_state=23),\n                         learning_rate=0.75,\n                         n_estimators= 35,\n                         random_state=23))\n#fitting model on train data\nada_tuned2.fit(X_train,y_train)","9092538b":"#calculate the metric scores\nscores(ada_tuned2,X_train,y_train)\nmake_confusion_matrix(ada_tuned2,y_test)","883377a7":"%%time\n#Creating pipeline\npipe_GRB2 = make_pipeline(GradientBoostingClassifier(random_state=23))\n# Parameter Grid\nparameters = {\n    \"gradientboostingclassifier__n_estimators\": np.arange(20,100,20),\n    \"gradientboostingclassifier__max_features\":[0.6,0.7,0.8,0.9],\n    'gradientboostingclassifier__learning_rate': [0.01,0.05,0.35,0.5],\n    'gradientboostingclassifier__subsample':[0.6,0.7,0.8,0.9]\n    }\n\n#scoring metric\nscoring = metrics.make_scorer(metrics.recall_score)\n\nRS_cv = RandomizedSearchCV(estimator=pipe_GRB2,\n                           scoring=scoring,\n                           param_distributions=parameters,\n                           random_state=23,n_iter=50,n_jobs=-1,cv=5)\nRS_cv.fit(X_train,y_train)\n\nprint(RS_cv.best_params_)\nprint(RS_cv.best_score_)","4bee694d":"grb_tuned2 = make_pipeline(GradientBoostingClassifier\n                          (learning_rate=0.35,\n                           max_features=0.6,\n                           n_estimators=80,random_state=23,\n                          subsample = 0.7))\n                                                                                      \n#fitting model on train data\ngrb_tuned2.fit(X_train,y_train)","70d18b61":"#calculate the metric scores\nscores(grb_tuned2,X_train,y_train)\nmake_confusion_matrix(grb_tuned2,y_test)","c5940ca2":"%%time\n#Creating pipeline\npipe_XBG2 = make_pipeline(\n                   (XGBClassifier\n                    (random_state=23,eval_metric='logloss')\n                   ))\n\n# Parameter Grid\nparameters = {\n    \"xgbclassifier__n_estimators\": np.arange(30,100,20),\n     \"xgbclassifier__subsample\":[0.6,0.7,0.8],\n    \"xgbclassifier__learning_rate\":[0.05,0.15,0.2,0.3],\n    \"xgbclassifier__gamma\":[0,1,2,3],\n    }\n#scoring metric\nscoring = metrics.make_scorer(metrics.recall_score)\n\n#RandomizedSearch CV\nRS_cv = RandomizedSearchCV(estimator=pipe_XBG2,\n                           scoring=scoring,\n                           param_distributions=parameters,\n                           random_state=23,n_iter=50,n_jobs=-1,cv=5)\nRS_cv.fit(X_train,y_train)\n\nprint(RS_cv.best_params_)\nprint(RS_cv.best_score_)","5b93e8a5":"#creating new pipeline with the best parameters \nxgb_tuned2 = make_pipeline(\n                   (XGBClassifier\n                    (random_state=23,\n                     eval_metric='logloss',\n                    learning_rate = 0.2,\n                    n_estimators =90,\n                    subsample = 0.7,\n                    gamma = 3\n                    )))\n#fitting model on train data\nxgb_tuned2.fit(X_train,y_train)","bad57906":"#calculate the metric scores\nscores(xgb_tuned2,X_train,y_train)\nmake_confusion_matrix(xgb_tuned2,y_test)","c8e03e72":"models = [ada_tuned2,\n          grb_tuned2,\n          xgb_tuned2]\n# defining empty lists to add train and test results\nacc_train = []\nacc_test = []\nrecall_train = [] \nrecall_test = []\nprecision_train = [] \nprecision_test = []\n\n# looping through all the models to get the metrics score - Accuracy and F1 Score\nfor model in models:\n    j = scores(model,X_train,y_train,False)\n    acc_train.append(j[0])\n    acc_test.append(j[1])\n    recall_train.append(j[2])\n    recall_test.append(j[3])\n    precision_train.append(j[4])\n    precision_test.append(j[5])","7c9cfacc":"comparison_frame = pd.DataFrame({'Model':['ADA_Boost-RandomizedSearchCV',\n                                          'Gradient_Boost-RandomizedSearchCV',\n                                          'XG_Boost-RandomizedSearchCV'],\n                                 'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train, 'Test_Recall':recall_test,\n                                           'Train_Precision':precision_train, 'Test_Precision':precision_test}) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_Recall',ascending=False).reset_index()\n","c3475df7":"**Building Hypertuned Boosting models using RandomSearchCV**","fefe8349":"### Model Building: Decision Tree, Random Forest, Bagging Classifier,Ada, Gradient and XG Boost\n\nWe will use the **make_pipeline** function to create pipelines for all the models.\nThis function does not need naming the estimators and will provide lowecase names of the types automatically.","e908c439":"### Comparing all models","f1d6751b":"### Thus ADA boost has the best recall and is the best model for this dataset","0d85523a":"## Split Data into Train and Test set","c91bc1af":"Insights from the Analysis\n\nInterestingly, we note that the among categorical variables the percentage of Attrited Customers seems to be fairly equal across all categories of all the Variables.\nDespite having a large imbalance in the proportions across the categories; the attrition however is quite similar.\nThere seems to be no significant categorical variable that shows a strong indicator for Attrition.\n\n## Correlation Matrix","ce22bdf3":"## EDA - Relationship of variables with the target ","c61aaaa6":"We can drop the CLIENTNUM column as its unique to each customer and not useful for the model.\nWe will also convert all object datatype to category for further processing.\nThe Dependent_count variable only has 6 unique values; from 0 to 5. As the dependents are people, we will convert this variable to category.","7827279e":"**Insights on above Analysis**","bda3f549":"Model Evaluation Criterion:\nModel can make two kinds of wrong predictions:\n\n1.Predicting that a customer will cancel their Credit Card services but doesnt : False Positive\n2. Predicting that a customer wont cancel their Credit Card servicebut does : False Negative\n\nThe Bank's objective is to identify all potential Customer's who wish to close their Credit Card Services.\nPredicting that a Customer wont cancel their Card Serivces but they do end up attriting, will lead to loss.\nHence the False Negative values must be reduced\nMetric for Optimization\n\nThe Recall must be maximized to ensure lesser chances of False Negatives."}}