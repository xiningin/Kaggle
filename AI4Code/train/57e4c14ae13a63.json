{"cell_type":{"997d0789":"code","fdde95a9":"code","b5ab9d3a":"code","1ab1e4a1":"code","e5ef3760":"code","d08ab2aa":"code","039542ac":"code","93d3361d":"code","cb56b37a":"code","9f9f08a2":"code","602d9a49":"code","63161268":"code","100e9f30":"code","34478d4a":"code","62fc6934":"code","72efcb47":"code","bffe12d4":"code","1b25eb8d":"code","ba117338":"code","ac56cd79":"code","0450f06b":"code","43369de9":"code","e6f2a0c6":"code","5e87169e":"markdown","2a6fc46e":"markdown","c1d81f56":"markdown","ceeb9293":"markdown","fce8379a":"markdown","96b1c67d":"markdown","98dc2c88":"markdown","ff9b3851":"markdown","8655af0c":"markdown","1d3543ac":"markdown","872da0b6":"markdown","b9ee41ae":"markdown"},"source":{"997d0789":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fdde95a9":"datafilepath=\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\"\ndata=pd.read_csv(datafilepath, index_col='id', parse_dates=True)\nimport seaborn as sns\nX=data.copy()\ntarget_var=X.pop('stroke')","b5ab9d3a":"for r in data.columns:\n    tmp=data[r].isnull().values.any()\n    tmp2=data[r].isnull().sum()\n    print(r + \"=\" + str(tmp*1.0) + \"\/\/ % NaN = \" + str(tmp2*1.0 \/ (1.0*len(data[r]))))","1ab1e4a1":"data['stroke'].eq(0).sum()","e5ef3760":"data.info()\ndata.describe()","d08ab2aa":"from matplotlib import *\nnumcat = data.select_dtypes(include=['int64', 'float64'])\nprint(numcat.columns)\nfor p in (numcat.columns):\n    pyplot.figure()\n    sns.histplot(data=data, x=p)","039542ac":"stringscat = data.select_dtypes(include=['object'])\nprint(stringscat.columns)\nfor p in (stringscat.columns):\n    pyplot.figure()\n    sns.histplot(data=data, x=p)","93d3361d":"data_smoke_unkwn = data[data['smoking_status']=='Unknown']\ndata_smoke_unkwn['stroke'].eq(0).sum()*100\/(len(data_smoke_unkwn['stroke']))\ndata_smoke_unkwn['stroke'].eq(1).sum()*100\/(len(data_smoke_unkwn['stroke']))","cb56b37a":"selection={'stroke': [1]}\ndata_withstroke = data[data['stroke']==1]\ndata_withstroke.info()\ndata_withOutstroke = data[data['stroke']==0]\ndata_withOutstroke.info()","9f9f08a2":"for p in (stringscat.columns):\n    pyplot.figure()\n    sns.histplot(data=data, x=p, hue='stroke')","602d9a49":"for p in (numcat.columns):\n    pyplot.figure()\n    sns.histplot(data=data, x=p, hue='stroke')","63161268":"#only numeric\ndatacorr=data.corr()\nsns.heatmap(datacorr, annot=True)\n#only numeric after standardization\ndataSc= (data - data.mean(axis=0))\/data.std(axis=0)\ndataSccorr=dataSc.corr()\npyplot.figure()\nsns.heatmap(dataSccorr, annot=True)","100e9f30":"Xcat=X.select_dtypes('object')\nXcat.describe()\nfrom sklearn.preprocessing import LabelEncoder\nle_Xcat = X.copy()\nle = LabelEncoder()\nfor p in X.select_dtypes('object'):\n    le_Xcat[p] = le.fit_transform(X[p])\nle_Xcat.describe()\n","34478d4a":"from sklearn.impute import SimpleImputer\nmyimputer = SimpleImputer()\nimp_le_Xcat = pd.DataFrame(myimputer.fit_transform(le_Xcat))\nprint(le_Xcat.info())\nfeaturesName = le_Xcat.columns\nprint(featuresName)","62fc6934":"print('% no stroke = ' + str(100*data['stroke'].eq(0).sum() \/ len(data['stroke'])))\nprint('% stroke = ' + str(100*data['stroke'].eq(1).sum() \/ len(data['stroke'])))","72efcb47":"from sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE\n\nmySampler = SMOTE(random_state=0)\nxres, yres = mySampler.fit_resample(imp_le_Xcat,target_var)\nxres.columns = featuresName","bffe12d4":"xres.info()","1b25eb8d":"xres.head()","ba117338":"#standardize bmi avg glucose level and age\nstdfeat = ['age', 'avg_glucose_level', 'bmi']\nxstd = xres[stdfeat]\nxstdn = (xstd - xstd.mean(axis=0)) \/ (xstd.std(axis=0))\n#nonstd_age = xres.drop('age')\n#nonstd_avggluc = xres.drop('avg_glucose_level')\n#nonstd_bmi = xres.drop('bmi')\n#xresn = pd.concat([xres, xstdn], axis=0)\nxrescopy=xres.copy()\nxrescopy[stdfeat] = xstdn[stdfeat]\nxresn = xrescopy","ac56cd79":"#from sklearn.preprocessing import StandardScaler\n#myScaler = StandardScaler()\n#xres_sc = (xres - xres.mean(axis=0)) \/ xres.std(axis=0)\n#xres_sc","0450f06b":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.utils import shuffle\nxtrain, xval, ytrain, yval = train_test_split(xresn, yres, train_size=0.8, test_size=0.2, random_state=0)\nfrom xgboost import XGBClassifier\nxtrain = shuffle(xtrain, random_state=1)\nytrain = shuffle(ytrain, random_state=1)\nmodel= XGBClassifier(\n    n_estimators=256,\n    learning_rate=0.1,\n    subsample=0.5,\n    use_label_encoder=False,\n    random_state=1)\nmodel.fit(xtrain,ytrain)\nypred=model.predict(xval)","43369de9":"print(classification_report(yval,ypred))\nff=classification_report(yval,ypred)\nff1=f1_score(yval,ypred)\nprint(ff1)","e6f2a0c6":"xval2, yval2 = shuffle(xval, yval, random_state=21)\nypred2=model.predict(xval2)\nprint(classification_report(yval2,ypred2))","5e87169e":"# this is my first autonomous and indipnendet work after following some Kaggle courses on ML, DL and data visualization.","2a6fc46e":"***NaN check***","c1d81f56":"# i am beginner in the world of ML and deep leanring. I am eager to interact and learn from the experts.  here below my proposed model to analyze the stroke dataset.\n**I reached approx. 97% F1 score on validation set **using SMOTE for oversampling, standardization of 3 features i.e. BMI, AGE and AVG-GLUCOSE-LEVEL and as model I used XGBClassifier.\nI thank anyone willing to comment and give feedback on what could be improved in terms of EDA, preprocessing, models, etc.**","ceeb9293":"****bmi is only var with NaN. NaN percentage is 4%, low. I replace NaN with mean****","fce8379a":"****1D data visualization**** ","96b1c67d":"****XGB classifier as first investigated approach****\n","98dc2c88":"****easy approach \/ undersample the 'no stroke case' so to have 50\/50****","ff9b3851":"# very much eager to interact with outher users and read feedbacks\/comments to speed up my leanring curve and build up more and more experience.\n#  ","8655af0c":"****imputing NaN****","1d3543ac":"****data encoding****","872da0b6":"****search for correlation \/ ongoing****","b9ee41ae":"# handling the imbalanced dataset"}}