{"cell_type":{"572fb9ff":"code","7ab50af6":"code","a6ace5fd":"code","3b505946":"code","6de54ac7":"code","ee5f3379":"code","d18a3450":"code","00316687":"code","41364d79":"code","e2ed792d":"code","1c0b643e":"code","5747f0d9":"code","cdbe49f5":"code","07166a8f":"code","5d1d9554":"code","e748a05f":"code","adf1e82d":"code","95e94c1e":"code","a18206c2":"code","d0604deb":"code","85cf5594":"code","29ae95f8":"code","33583381":"code","2d087ab3":"code","fa9bfcf5":"code","23bfa791":"code","3ed14d3f":"code","d3666950":"code","ace8aac7":"code","659375d1":"code","70dcb45a":"markdown","c6ea98b5":"markdown","1a7c0890":"markdown","0d6ad354":"markdown","f9643a00":"markdown","9eac67b6":"markdown","70d291d9":"markdown","7056e2f4":"markdown","dae00758":"markdown","299a1c50":"markdown"},"source":{"572fb9ff":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import Statsmodels\nfrom statsmodels.tsa.api import VAR\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tools.eval_measures import rmse, aic","7ab50af6":"filepath = '..\/input\/ntt-data-global-ai-challenge-06-2020\/COVID-19_and_Price_dataset.csv'\ndf = pd.read_csv(filepath)\n\nprint(df.shape)  # (123, 8)\ndf.tail()","a6ace5fd":"df = df.dropna(axis=1)","3b505946":"df.tail()","6de54ac7":"df.shape\n","ee5f3379":"cor = df.corr()\n#Correlation with output variable\ncor_target = abs(cor[\"Price\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.98]\nrelevant_features","d18a3450":"df = df[[ 'BritishVirginIslands_total_cases' , 'Cyprus_total_deaths',\n        'Grenada_total_cases', 'Guyana_total_deaths'   , 'Niger_total_cases' ,\n         'Singapore_total_deaths', 'SriLanka_total_deaths','Vatican_total_cases','Price'] ]","00316687":"df.shape","41364d79":"\nfrom statsmodels.tsa.stattools import grangercausalitytests\nmaxlag=12\ntest = 'ssr_chi2test'\ndef grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n\n    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n    for c in df.columns:\n        for r in df.index:\n            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n            min_p_value = np.min(p_values)\n            df.loc[r, c] = min_p_value\n    df.columns = [var + '_x' for var in variables]\n    df.index = [var + '_y' for var in variables]\n    return df\n\ngrangers_causation_matrix(df, variables = df.columns)           ","e2ed792d":"nobs = 30\ndf_train, df_test = df[0:-nobs], df[-nobs:]\n\n# Check size\nprint(df_train.shape)  # (119, 8)\nprint(df_test.shape)  # (4, 8)","1c0b643e":"test =df_test[['Price']]","5747f0d9":"def adfuller_test(series, signif=0.05, name='', verbose=False):\n    r = adfuller(series, autolag='AIC')\n    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n    p_value = output['pvalue'] \n    def adjust(val, length= 6): return str(val).ljust(length)\n\n    # Print Summary\n    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n    print(f' Significance Level    = {signif}')\n    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n\n    for key,val in r[4].items():\n        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n\n    if p_value <= signif:\n        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n        print(f\" => Series is Stationary.\")\n    else:\n        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n        print(f\" => Series is Non-Stationary.\")    ","cdbe49f5":"# ADF Test on each column\nfor name, column in df_train.iteritems():\n    adfuller_test(column, name=column.name)\n    print('\\n')","07166a8f":"# 1st difference\ndf_differenced = df_train.diff().dropna()","5d1d9554":"# ADF Test on each column of 1st Differences Dataframe\nfor name, column in df_differenced.iteritems():\n    adfuller_test(column, name=column.name)\n    print('\\n')","e748a05f":"# Second Differencing\ndf_differenced = df_differenced.diff().dropna()","adf1e82d":"# ADF Test on each column of 2nd Differences Dataframe\nfor name, column in df_differenced.iteritems():\n    adfuller_test(column, name=column.name)\n    print('\\n')","95e94c1e":"# third Differencing\ndf_differenced = df_differenced.diff().dropna()","a18206c2":"# ADF Test on each column of 3rd Differences Dataframe\nfor name, column in df_differenced.iteritems():\n    adfuller_test(column, name=column.name)\n    print('\\n')","d0604deb":"model = VAR(df_differenced)\nfor i in [1,2,3,4,5]:\n    result = model.fit(i)\n    print('Lag Order =', i)\n    print('AIC : ', result.aic)\n    print('BIC : ', result.bic)\n    print('FPE : ', result.fpe)\n    print('HQIC: ', result.hqic, '\\n')","85cf5594":"x = model.select_order(maxlags=5)\nx.summary()","29ae95f8":"model_fitted = model.fit()\nmodel_fitted.summary()","33583381":"# Get the lag order\nlag_order = model_fitted.k_ar\nprint(lag_order)  #> 4\n\n# Input data for forecasting\nforecast_input = df_differenced.values[-lag_order:]\nforecast_input","2d087ab3":"# Forecast\nfc = model_fitted.forecast(y=forecast_input, steps=nobs)\ndf_forecast = pd.DataFrame(fc, index=df.index[-nobs:], columns=df.columns + '_2d')\ndf_forecast","fa9bfcf5":"def invert_transformation(df_train, df_forecast, second_diff=False):\n    df_fc = df_forecast.copy()\n    columns = df_train.columns\n    for col in columns:        \n        # Roll back 2nd Diff\n        if second_diff:\n            df_fc[str(col)+'_1d'] = (df_train[col].iloc[-1]-df_train[col].iloc[-2]) + df_fc[str(col)+'_2d'].cumsum()\n        # Roll back 1st Diff\n        df_fc[str(col)+'_forecast'] = df_train[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()\n    return df_fc","23bfa791":"df_results = invert_transformation(df_train, df_forecast, second_diff=True)        \n","3ed14d3f":"predict = df_results['Price_forecast']","d3666950":"len(predict)","ace8aac7":"import math\nfrom sklearn.metrics import mean_squared_error\nmath.sqrt( mean_squared_error(test,predict))","659375d1":"import matplotlib.pyplot as plt\n# zoom plot\nplt.figure(figsize=(20,10))\nplt.plot(test)\nplt.plot(predict, color='green')\nplt.title('Actual Vs Predicted')\nplt.show()\n","70dcb45a":"<h1 align='center'> $\\color{Red}{\\text{Guidance on Vector Auto Regression for Beginner's}}$ <\/h1>\n<h2>The increasing spread of the coronavirus across countries has prompted many governments to introduce unprecedented measures to contain the epidemic. These are priority measures that are imposed by a sanitary situation, which leave little room for other options as health should remain the primary concern. These measures have led to many businesses being shut down temporarily,widespread restrictions on travel and mobility, financial market turmoil, an erosion of confidence and heighted uncertainty.<\/h2>\n\n\n<img src=\"https:\/\/www.balcanicaucaso.org\/var\/obc\/storage\/images\/aree\/croazia\/croazia-conseguenze-economiche-da-covid-19-201468\/1963446-1-ita-IT\/Croazia-conseguenze-economiche-da-Covid-19.jpg\" width=\"1200px\">\n","c6ea98b5":"references:\nhttps:\/\/www.machinelearningplus.com\/time-series\/vector-autoregression-examples-python\/\nhttps:\/\/towardsdatascience.com\/prediction-task-with-multivariate-timeseries-and-var-model-47003f629f9\nhttps:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.stattools.adfuller.html\nhttps:\/\/en.wikipedia.org\/wiki\/Vector_autoregression#:~:text=Vector%20autoregression%20(VAR)%20is%20a,more%20than%20one%20evolving%20variable.\n","1a7c0890":"# <h1 align='left'> $\\color{green}{\\text{ADF Test}}$ <\/h1>\nIn order the predict the time series is Stationary or not. We use <B>Augumented Dickey-Fuller Test(ADF Test)<\/B>.\n\n\nWhat is **Stationary**?\n Series is stationary when the **mean and variance are constant over a time**.\n \n \nWhat is **non-stationary**?\n Series is non-stationary when the **mean and variance are dependent on a time**.\n \n \n ![equation_2.png](attachment:equation_2.png)","0d6ad354":"## <h1 align='left'> $\\color{green}{\\text{Statsmodels}}$ <\/h1>\n<ul>\n<li>The library built on other packages like Numpy, Scipy. <\/li>\n<li>Library contains the function used for testing the statistical model and to build a model. <\/li>\n<li>Statistical testing tool like adfuller(AUGUMENTED DICKEY-FULLER), rmse and aic.<\/li>\n<\/ul>","f9643a00":"# <h1 align='left'> $\\color{green}{\\text{Vector Autoregression}}$ <\/h1>\nVector autoregression (VAR) is a stochastic process model used to capture the linear interdependencies among multiple time series. \nVAR models generalize the univariate autoregressive model (AR model) by allowing for more than one evolving variable. \n![VAR.svg](attachment:VAR.svg)","9eac67b6":"# <h1 align='left'> $\\color{Green}{\\text{Feature Extraction}}$ <\/h1>","70d291d9":"## <h1 align='left'> $\\color{green}{\\text{Comment}}$ <\/h1>\nDropped the column containing 'NAN' values","7056e2f4":"# <h1 align='left'> $\\color{green}{\\text{Grangercausality test}}$ <\/h1>\n\nThe grangercausality test used to find the determinant between two variable in the series. \nIt uses prior datasets to find the correlation. i.e., X is cause of Y or Y is cause of X.\nIt uses Bottom up\/top down approach to see if the variables are generated independently or not from each other.\nThe test gives value as null hypothesis i.e, variation in y does not interrupted by x.\nGrangercausality test used to find the dependencies between the variable in particular instantaneous time. \n\n# <h1 align='left'> $\\color{green}{\\text{Null Hypothesis}}$ <\/h1>\nUsed to test unit root between the time series.\n","dae00758":"# <h1 align='left'> $\\color{green}{\\text{Comments}}$ <\/h1>\n*  First order difference was used to convert Non-stationary Series to Stationary series.\n*  And ADFnuller function is used to evaluate the Stationary.\n","299a1c50":"# <h1 align='left'> $\\color{green}{\\text{Comment}}$ <\/h1>\n* Number of column reduced is 128. \n* In order to reduced the column further, we are using correlation to get relevant features. Correlation target is set to 0.98. "}}