{"cell_type":{"8540f2c6":"code","934998af":"code","56c49a9b":"code","d28f985e":"code","bdd3c446":"code","a0092acb":"code","83fef181":"code","7508f114":"code","8973dca9":"code","7c231430":"code","a8da5768":"code","3a08cc96":"code","cf144886":"code","6185563f":"code","426aa140":"code","ac0b6c89":"code","b7aad531":"code","30e3b3b3":"code","59eb9a09":"code","fbbd0054":"code","212b9167":"code","c7592935":"code","a0e22821":"code","41d11d3d":"code","0a82b750":"code","c18b7f50":"code","0b3af0ba":"code","dab673f0":"code","d661906d":"code","7ab75eb3":"code","f83b0ed3":"code","4bc25997":"code","78c5bf61":"code","01d13bbe":"code","ced1188f":"code","ef06a103":"code","c773171e":"code","f05b074f":"code","96be28a2":"code","aed3de09":"code","86a1da9c":"code","29cb1a01":"code","8b763db0":"code","8f3f8b3e":"code","08d73e9d":"code","244d954b":"code","bc96ff52":"code","8e9060fe":"code","4a0438e8":"code","89c0027d":"code","4b2a70b8":"code","42ab176b":"code","d7ab5672":"code","c451345c":"code","2b1bbd22":"code","a0256bbe":"code","09000361":"code","cf6383ee":"code","d73b5947":"code","23498eac":"code","39940b7a":"code","a51badac":"code","e18572bd":"code","9c95e79f":"code","e6273628":"code","dfe3953c":"code","53db73a1":"code","45d30500":"code","ab9a9bcd":"code","cc44d30a":"code","c69c3881":"code","8c371dad":"code","04890c3e":"code","4ec6bbe2":"code","e5393229":"code","ae0d568c":"code","4b51e3a7":"code","5d768ad1":"code","c61bcab5":"code","4f6772fe":"code","db38fb73":"code","78103630":"code","cba31dfd":"code","f59fd49b":"code","fb42d049":"code","f079691d":"code","e4a8ddf2":"code","f1ce64e5":"code","1cd42f7d":"code","ac1e7124":"code","e98b9ef5":"code","5c1a689d":"code","c6919649":"code","6d33524c":"code","0eb16a36":"code","ca6cd7c0":"code","9e5ec683":"code","39588b0e":"code","65068d89":"code","846230a4":"code","dc7364bd":"code","b7c49995":"code","de5046c7":"code","9e23b9f7":"code","4cf41c8d":"code","fa2992f0":"code","a2ccba6b":"code","4e628635":"code","20119fb6":"markdown","ff143082":"markdown","68ff1a85":"markdown","a0a8f28a":"markdown","38e7ce96":"markdown","7fc4b096":"markdown","745ade0f":"markdown","834a246c":"markdown","9cd3f498":"markdown","ad6886e9":"markdown","555351ed":"markdown","3b03af86":"markdown","6911a710":"markdown","d45cb8be":"markdown","17fcd292":"markdown","0bdd3321":"markdown","6e493d2e":"markdown","a950f524":"markdown","2de1cf7a":"markdown"},"source":{"8540f2c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","934998af":"color = sns.color_palette()\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls","56c49a9b":"train_df=pd.read_csv('..\/input\/train.csv')","d28f985e":"train_df.shape","bdd3c446":"dtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","a0092acb":"sns.countplot(x=\"Target\",  data=train_df)","83fef181":"train_df_2=train_df.drop(['Id','v2a1'],axis=1)\nlist_to_compare=['no','yes','No','Yes','NO','YES']\nfor col in train_df_2:\n    unique=train_df_2[col].unique()\n    if (set(list_to_compare)&set(unique)):\n        print(\"Column \",col,\"has the values \",set(list_to_compare)&set(unique))","7508f114":"# Let us replace all yes with 1 and no with 0 as defined in the competition page, data description.\n\ntrain_df=train_df.replace('no','0')\ntrain_df=train_df.replace('yes','1')","8973dca9":"missing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\nmissing_df","7c231430":"\ntrain_df.groupby(['idhogar','v18q']).aggregate('count')['v18q1'].reset_index()","a8da5768":"train_df['rez_esc'].fillna(0, inplace=True)","3a08cc96":"train_df=train_df[~train_df['meaneduc'].isnull()]","cf144886":"train_df['v18q1'].fillna(0, inplace=True)","6185563f":"train_df['rez_esc'].fillna(0, inplace=True)","426aa140":"\ntrain_df_reg=train_df","ac0b6c89":"train_df_reg=train_df","b7aad531":"train_df_reg_train=train_df_reg[~train_df_reg['v2a1'].isnull()]","30e3b3b3":"train_df_reg_train.shape","59eb9a09":"train_df_reg_test=train_df_reg[train_df_reg['v2a1'].isnull()]","fbbd0054":"train_df_reg_test.shape","212b9167":"\ntrain_df_reg_train_Y=train_df_reg_train['v2a1']","c7592935":"\ntrain_df_reg_test_Y=train_df_reg_test['v2a1']","a0e22821":"train_df_reg_train_X=train_df_reg_train.loc[:, train_df_reg_train.columns != 'v2a1']","41d11d3d":"train_df_reg_test_X=train_df_reg_test.loc[:, train_df_reg_test.columns != 'v2a1']","0a82b750":"train_df_reg_train_X_id=train_df_reg_train_X","c18b7f50":"\ntrain_df_reg_train_X=train_df_reg_train_X.drop(['Id','idhogar'],axis=1)","0b3af0ba":"train_df_reg_test_X_id=train_df_reg_test_X","dab673f0":"train_df_reg_test_X=train_df_reg_test_X.drop(['Id','idhogar'],axis=1)","d661906d":"\nregr = RandomForestRegressor(max_depth=10, random_state=0, n_estimators=100)","7ab75eb3":"regr.fit(train_df_reg_train_X,train_df_reg_train_Y)","f83b0ed3":"train_df_reg_test_Y=regr.predict(train_df_reg_test_X)\n","4bc25997":"train_df_reg_test_Y=np.round(train_df_reg_test_Y)","78c5bf61":"train_df_reg_test_Y = pd.DataFrame(train_df_reg_test_Y, dtype='float')","01d13bbe":"\ntrain_df_reg_test_Y.shape","ced1188f":"train_df_reg_test_Y","ef06a103":"train_df_reg_test_X_id.loc[:,'v2a1']=train_df_reg_test_Y.values","c773171e":"train_df_reg_test_X_id.shape","f05b074f":"train_df_reg_train_X_id.loc[:,'v2a1']=train_df_reg_train_Y.values","96be28a2":"train_df_reg_train_X_id['v2a1']","aed3de09":"output_train_df = pd.concat([train_df_reg_train_X_id, train_df_reg_test_X_id], axis=0, ignore_index=True,sort='False')","86a1da9c":"output_train_df.shape","29cb1a01":"merge_df=output_train_df[['Id','v2a1']]","8b763db0":"train_df_reg_train_X_id['v2a1']","8f3f8b3e":"imputed_df=train_df.merge(merge_df,on='Id',how='outer')","08d73e9d":"imputed_df.shape","244d954b":"imputed_df=imputed_df.drop(['v2a1_x'],axis=1)","bc96ff52":"imputed_df.shape","8e9060fe":"imputed_df=imputed_df.rename(index=str, columns={\"v2a1_y\": \"v2a1\"})","4a0438e8":"\nunique_df = imputed_df.nunique().reset_index()\nunique_df.columns = [\"col_name\", \"unique_count\"]\nconstant_df = unique_df[unique_df[\"unique_count\"]==1]\nconstant_df","89c0027d":"imputed_df= imputed_df.drop(['elimbasu5'],axis=1)","4b2a70b8":"from scipy.stats import spearmanr\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nlabels = []\nvalues = []\nfor col in train_df.columns:\n    if col not in [\"Id\", \"Target\"]:\n        labels.append(col)\n        values.append(spearmanr(train_df[col].values, train_df[\"Target\"].values)[0])\ncorr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n \ncorr_df = corr_df[(corr_df['corr_values']>0.1) | (corr_df['corr_values']<-0.1)]\nind = np.arange(corr_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,30))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='b')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","42ab176b":"\ncols_to_use = corr_df[(corr_df['corr_values']>0.21) | (corr_df['corr_values']<-0.21)].col_labels.tolist()\n\ntemp_df = train_df[cols_to_use]\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=1., square=True, cmap=\"YlGnBu\", annot=True)\nplt.title(\"Important variables correlation map\", fontsize=15)\nplt.show()","d7ab5672":"train_X = imputed_df.drop([\"Id\", \"Target\"], axis=1)\ntrain_y = imputed_df[\"Target\"].values","c451345c":"import sklearn\nfrom sklearn import ensemble\nmodel = ensemble.ExtraTreesClassifier(n_estimators=200, max_depth=20, max_features=0.5, n_jobs=-1, random_state=0)\ntrain_X=train_X.drop(['idhogar'],axis=1)\nmodel.fit(train_X, train_y)\n\n## plot the importances ##\nfeat_names = train_X.columns.values\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","2b1bbd22":"from sklearn.linear_model import LogisticRegression\n\nX=train_X\ny=train_y\nclf = LogisticRegression(random_state=0, solver='lbfgs',  multi_class='multinomial').fit(X, y)","a0256bbe":"test_df=pd.read_csv('..\/input\/test.csv')","09000361":"test_df.shape","cf6383ee":"test_df_orig=test_df","d73b5947":"test_df=test_df.replace('no','0')\ntest_df=test_df.replace('yes','1')","23498eac":"\ntest_df.shape","39940b7a":"test_df['rez_esc'].fillna(0, inplace=True)\ntest_df['meaneduc'].fillna(0, inplace=True)\ntest_df['SQBmeaned'].fillna(0, inplace=True)\ntest_df['v18q1'].fillna(0, inplace=True)\ntest_df['rez_esc'].fillna(0, inplace=True)","a51badac":"test_df.shape","e18572bd":"\ntest_df_reg=test_df","9c95e79f":"test_df_reg_train=test_df_reg[~test_df_reg['v2a1'].isnull()]","e6273628":"test_df_reg_train.shape","dfe3953c":"test_df_reg_test=test_df_reg[test_df_reg['v2a1'].isnull()]","53db73a1":"test_df_reg_test.shape","45d30500":"\ntest_df_reg_train_Y=test_df_reg_train['v2a1']\ntest_df_reg_test_Y=test_df_reg_test['v2a1']\ntest_df_reg_train_X=test_df_reg_train.loc[:, test_df_reg_train.columns != 'v2a1']\ntest_df_reg_test_X=test_df_reg_test.loc[:, test_df_reg_test.columns != 'v2a1']\ntest_df_reg_train_X_id=test_df_reg_train_X\ntest_df_reg_train_X=test_df_reg_train_X.drop(['Id','idhogar'],axis=1)\ntest_df_reg_test_X_id=test_df_reg_test_X\ntest_df_reg_test_X=test_df_reg_test_X.drop(['Id','idhogar'],axis=1)","ab9a9bcd":"regr = RandomForestRegressor(max_depth=10, random_state=0, n_estimators=100)","cc44d30a":"regr.fit(test_df_reg_train_X,test_df_reg_train_Y)","c69c3881":"test_df_reg_test_Y=regr.predict(test_df_reg_test_X)","8c371dad":"\ntest_df_reg_test_Y=np.round(test_df_reg_test_Y)","04890c3e":"\ntest_df_reg_test_Y = pd.DataFrame(test_df_reg_test_Y, dtype='float')","4ec6bbe2":"test_df_reg_test_Y.shape","e5393229":"test_df_reg_test_X_id.loc[:,'v2a1']=test_df_reg_test_Y.values","ae0d568c":"\ntest_df_reg_test_X_id.shape","4b51e3a7":"test_df_reg_train_X_id.loc[:,'v2a1']=test_df_reg_train_Y.values","5d768ad1":"test_df_reg_train_X_id['v2a1']","c61bcab5":"output_test_df = pd.concat([test_df_reg_train_X_id, test_df_reg_test_X_id], axis=0, ignore_index=True,sort='False')\n","4f6772fe":"output_test_df.shape","db38fb73":"merge_test_df=output_test_df[['Id','v2a1']]","78103630":"\nimputed_test_df=test_df.merge(merge_test_df,on='Id',how='outer')","cba31dfd":"imputed_test_df.shape","f59fd49b":"imputed_test_df=imputed_test_df.drop(['v2a1_x'],axis=1)\nimputed_test_df=imputed_test_df.rename(index=str, columns={\"v2a1_y\": \"v2a1\"})\nimputed_test_df=imputed_test_df.drop(['Id','idhogar'],axis=1)","fb42d049":"imputed_test_df.shape","f079691d":"imputed_test_df= imputed_test_df.drop(['elimbasu5'],axis=1)","e4a8ddf2":"target_y=clf.predict(imputed_test_df)","f1ce64e5":"target_y","1cd42f7d":"type(test_df_orig['Id'])","ac1e7124":"len(test_df_orig['Id'])","e98b9ef5":"submission_df=pd.concat([test_df_orig['Id'], pd.Series(target_y)], axis=1)","5c1a689d":"submission_df.shape","c6919649":"submission_df=submission_df.rename(index=int, columns={0: \"Target\"})","6d33524c":"submission_df.to_csv('submission.csv', index=False)","0eb16a36":"from sklearn.ensemble import RandomForestClassifier","ca6cd7c0":"rf_clf = RandomForestClassifier(n_estimators=1000, random_state=0,n_jobs=-1)","9e5ec683":"rf_clf.fit(X, y)","39588b0e":"\ntarget_y=rf_clf.predict(imputed_test_df)","65068d89":"\nsubmission_df=pd.concat([test_df_orig['Id'], pd.Series(target_y)], axis=1)\nsubmission_df=submission_df.rename(index=int, columns={0: \"Target\"})\nsubmission_df.to_csv('submission_rf.csv', index=False)","846230a4":"from xgboost import XGBClassifier","dc7364bd":"model = XGBClassifier()\nX=X.replace('no',0)\nX=X.replace('yes',1)\nX['dependency']=X['dependency'].astype(str).astype(float,copy=True)\nX['edjefe']=X['edjefe'].astype(str).astype(int,copy=True)\nX['edjefa']=X['edjefa'].astype(str).astype(int,copy=True)","b7c49995":"type(X['dependency'])","de5046c7":"X.to_csv('debug.csv')","9e23b9f7":"dtype_df = X.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","4cf41c8d":"model.fit(X, y)","fa2992f0":"imputed_test_df['dependency']=imputed_test_df['dependency'].astype(str).astype(float,copy=True)\nimputed_test_df['edjefe']=imputed_test_df['edjefe'].astype(str).astype(float,copy=True)\nimputed_test_df['edjefa']=imputed_test_df['edjefa'].astype(str).astype(float,copy=True)","a2ccba6b":"target_y=model.predict(imputed_test_df)","4e628635":"submission_df=pd.concat([test_df_orig['Id'], pd.Series(target_y)], axis=1)\nsubmission_df=submission_df.rename(index=int, columns={0: \"Target\"})\nsubmission_df.to_csv('submission_xgb.csv', index=False)","20119fb6":"Having done the imputation let us proceed and look at corelation that exists between the features and in between the features and the target column.Before that lets first check if there are any columns which hold just one unique value. Such features can be removed from the dataset as they do not add any value to the model.","ff143082":"## Feature Importance","68ff1a85":"## Imputation Process\n\n\nLet us first consider the column meaneduc. For now, i will just remove the 4 rows which are having null values in this column and will revisit later and check if it increases the accuracy of the model. Removing meaneduc rows with null meaneduc will also take care of null values in SQBmeaned.","a0a8f28a":"I do not see any features that have any high degree of correlation with the target feature. As the next step we can check for correlation between the independendent features","38e7ce96":"\nNow the last column that needs imputation is v2a1. This is a continuous numerical column which provides the rent paid for each of the house. I think the best way to impute this is by applying a regressor here. v2a1 will be our target variable and rest of the datasets will be our independent features. Next bit of code will prepare the dataset so that we can apply a regressor. Let us apply a RF Regressor\n","7fc4b096":"##  Check the datatypes of the dataset","745ade0f":"## Basic RF Classifier","834a246c":"As expected feature importance values that we see are low. Let us now fit a model on the preprocessed dataset to get a prediction and accuracy which we can use as benchmark and then apply the other algorithms and compare the accuracies","9cd3f498":"## Check if there any columns with constant values****","ad6886e9":"## Non Numeric value cleansing","555351ed":"## Preprocess\/Impute test data","3b03af86":"## First Benchmark Model- Logistic regression","6911a710":"We can now verify that there are no non numeric values in the dataset.\n\n## Check number of missing values in the dataset","d45cb8be":"##  XGBoost","17fcd292":"All the blank values in v18q1 can be imputed with 0 safely since the null values exist where individual tablet count is 0. When the individual count is 0, the overall household count will also be 0\n\nImputing rez_esc also with 0 for now as I am unable to find any correlation or any other suitable stratergy to impute this column with.","0bdd3321":"\nNow that we have the output of the regressor, let us merge the predictions we obtained on v2a1 column back to the original dataset. With this we would have imputed the null values in v2a1 feature","6e493d2e":"## Correlation Heat Map and Corelation to target variables****","a950f524":"\n## Target Value Distribution in Training dataset","2de1cf7a":"We saw that none of the features is strongly correlated with the target feature and hence I dont expect any particular feature will have high feature importance. Nevertheless lets check by fitting a classifier on the dataset and applying the feature imp function provided by that classifier"}}