{"cell_type":{"3a1246ec":"code","d022c374":"code","f173347f":"code","d036b349":"code","65ea701f":"code","c1fe1bcb":"code","8620cf14":"code","3dfc4504":"code","1a128c97":"code","2698f458":"code","a3dc24ed":"code","9e2cbe76":"code","29979a2e":"code","a2e401f3":"code","8e3c7491":"code","5e5450e0":"code","98cad3b0":"code","1e1f7df1":"code","16de0a9d":"code","514c40e8":"code","d5836b33":"code","4b456e42":"code","254c2a1f":"code","5c9f21f5":"code","151af8b3":"code","918f6665":"code","7701e959":"markdown","9fbd4a0f":"markdown","805f81bd":"markdown","beea6c25":"markdown","e77f2f61":"markdown","1e349540":"markdown","2051920d":"markdown","993c3868":"markdown","60a55103":"markdown","4e0a0c23":"markdown","bd110081":"markdown","d3a64bcd":"markdown","1604a9d1":"markdown","ff461c07":"markdown","ebb45981":"markdown","b1b8ce44":"markdown"},"source":{"3a1246ec":"# modeling\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\n\n# result\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')          # graph settings\nplt.rcParams['figure.figsize'] = (12,5)    # graph settings\n\n# data wrangling\nimport numpy as np \nimport pandas as pd\n\n# corpus\nfrom nltk.corpus import stopwords\n\n# string manipulation\nimport re\nimport spacy\nimport collections","d022c374":"import nltk \nnltk.download(\"stopwords\") ","f173347f":"train = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='latin1')\ntest = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding='latin1')\n#train = pd.read_csv('Corona_NLP_train.csv', encoding='latin1')\n#test = pd.read_csv('Corona_NLP_test.csv', encoding='latin1')\ntrain.head()","d036b349":"print('Examples in train data: {}'.format(len(train)))\nprint('Examples in test data: {}'.format(len(test)))","65ea701f":"train.isna().sum()","c1fe1bcb":"# delete because, we dont use it\ntrain = train.drop(columns=[\"Location\"])\ntrain","8620cf14":"dist_train = train['Sentiment'].value_counts()\ndist_test = test['Sentiment'].value_counts()\n\ndef ditribution_plot(x, y, name):\n    sns.barplot(x=x, y=y)\n    plt.title(name)\n    plt.show()","3dfc4504":"ditribution_plot(x=dist_train.index, y=dist_train.values, name='Class Distribution train')","1a128c97":"ditribution_plot(x=dist_test.index, y=dist_test.values, name='Class Distribution test')","2698f458":"import wordcloud\nfrom wordcloud import WordCloud\nallWords = ' '.join([twts for twts in train['OriginalTweet']])\nwordCloud = WordCloud(width=500, height=300, random_state=21, max_font_size=110).generate(allWords)\n\nplt.figure(figsize = (10, 8))\nplt.imshow(wordCloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","a3dc24ed":"dtf = train\ndtf['word_count'] = dtf[\"OriginalTweet\"].apply(lambda x: len(str(x).split(\" \")))\ndtf['char_count'] = dtf[\"OriginalTweet\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\ndtf['sentence_count'] = dtf[\"OriginalTweet\"].apply(lambda x: len(str(x).split(\".\")))\ndtf['avg_word_length'] = dtf['char_count'] \/ dtf['word_count']\ndtf['avg_sentence_lenght'] = dtf['word_count'] \/ dtf['sentence_count']\ndtf.head()","9e2cbe76":"X = train['OriginalTweet'].copy()\ny = train['Sentiment'].copy()","29979a2e":"def data_cleaner(tweet):\n    tweet = re.sub(r'http\\S+', ' ', tweet)   # remove urls\n    tweet = re.sub(r'<.*?>',' ', tweet)      # remove html tags\n    tweet = re.sub(r'\\d+',' ', tweet)        # remove digits\n    tweet = re.sub(r'#\\w+',' ', tweet)       # remove hashtags\n    tweet = re.sub(r'@\\w+',' ', tweet)       # remove mentions\n    tweet = \" \".join([word for word in tweet.split() if not word in stop_words])   # remove stop words\n    return tweet\n\nstop_words = stopwords.words('english')\nX_cleaned = X.apply(data_cleaner)\nX_cleaned.head()","a2e401f3":"allWords1 = ' '.join([twts for twts in X_cleaned])\nwordCloud1 = WordCloud(width=500, height=300, random_state=21, max_font_size=110).generate(allWords1)\n\nplt.figure(figsize = (10, 8))\nplt.imshow(wordCloud1, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","8e3c7491":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_cleaned)\nX = tokenizer.texts_to_sequences(X_cleaned)\nvocab_size = len(tokenizer.word_index) + 1\n\nprint(\"Vocabulary size: {}\".format(vocab_size))\nprint(\"\\nExample:\\n\")\nprint(\"Sentence:\\n{}\".format(X_cleaned[6]))\nprint(\"\\nAfter tokenizing :\\n{}\".format(X[6]))\n\nX = pad_sequences(X, padding='post')\nprint(\"\\nAfter padding :\\n{}\".format(X[6]))","5e5450e0":"encoding = {\n    'Extremely Negative': 0,\n    'Negative': 0,\n    'Neutral': 1,\n    'Positive': 2,\n    'Extremely Positive': 2\n}\n\nlabels = ['Negative', 'Neutral', 'Positive']\ny.replace(encoding, inplace=True)","98cad3b0":"y","1e1f7df1":"tf.keras.backend.clear_session()\n\n# hyperparameters\nEPOCHS = 2\nBATCH_SIZE = 32\nembedding_dim = 54\nunits = 256\n\nmodel = tf.keras.Sequential([\n    L.Embedding(vocab_size, embedding_dim, input_length=X.shape[1]),\n    L.Bidirectional(L.GRU(units, return_sequences=True)),\n    L.GlobalMaxPool1D(),\n    L.Dropout(0.4),\n    L.Dense(64, activation=\"relu\"),\n    L.Dropout(0.4),\n    L.Dense(3)\n])\n\nmodel.compile(\n    loss=SparseCategoricalCrossentropy(from_logits=True),\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\nmodel.summary()","16de0a9d":"history = model.fit(X, y, epochs=2, validation_split=0.12, batch_size=BATCH_SIZE)","514c40e8":"def history_plot(history):\n    plt.plot(history.history['loss'], label='train loss')\n    plt.plot(history.history['val_loss'], label='validation loss')\n    plt.title('Model Loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend()\n    plt.show()\n\n    plt.plot(history.history['accuracy'], label='train accuracy')\n    plt.plot(history.history['val_accuracy'], label='validation accuracy')\n    plt.title('Model Accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend()\n    plt.show()","d5836b33":"history_plot(history)","4b456e42":"X_test = test['OriginalTweet'].copy()\ny_test = test['Sentiment'].copy()\n\nX_test = X_test.apply(data_cleaner)\nX_test = tokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test, padding='post', maxlen=X.shape[1])\ny_test.replace(encoding, inplace=True)","254c2a1f":"loss, acc = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss: {}'.format(loss))\nprint('Test Accuracy: {}'.format(acc))","5c9f21f5":"pred = np.argmax(model.predict(X_test), axis=-1)","151af8b3":"conf = confusion_matrix(y_test, pred)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in labels],\n    columns = [i for i in labels]\n)\n\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","918f6665":"print(classification_report(y_test, pred, target_names=labels))","7701e959":"## Classification report","9fbd4a0f":"After 2 epochs, we get overfitting","805f81bd":"## Tokenizing","beea6c25":"## Accuracy and loss","e77f2f61":"# Class distribution","1e349540":"# Evaluation","2051920d":"# Model building and training","993c3868":"## Confusion matrix","60a55103":"## Dataset size","4e0a0c23":"## Preprocessing test data","bd110081":"# Dataset","d3a64bcd":"## Cleaning","1604a9d1":"## Feature encoding","ff461c07":"## Missing values","ebb45981":"# Data preprocessing","b1b8ce44":"# Import libraries"}}