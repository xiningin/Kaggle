{"cell_type":{"3f6e641f":"code","8358c577":"code","889bc257":"code","70dcfcb1":"code","c922109c":"code","65d598ba":"code","6b585149":"code","e79b8a36":"code","009298e4":"code","4ceee238":"code","a51798f1":"code","8207aa76":"code","0f1512bf":"code","99e758ab":"code","43cb57c2":"code","00e89153":"code","ba56a714":"code","9d0795e0":"code","abd72c9c":"code","0225e683":"code","193fa928":"code","516c59e5":"code","af0709af":"code","2866033f":"code","55a3629e":"code","a3217e01":"code","33eb3356":"code","9261935c":"code","06db064c":"code","724ee695":"code","5b55dd3b":"code","61a6f6a4":"code","d78e43ab":"code","44401725":"code","3388e084":"code","4af1ab2e":"code","4345467a":"code","643bede4":"code","f9f67331":"code","70601b6f":"code","672d9e9c":"code","aabe10d6":"code","d1fa5a27":"code","bfdcc4fa":"code","ae5edb13":"code","85fb5355":"code","f96e56ff":"code","138b9071":"code","8365e954":"code","6392cc75":"code","b3ff0649":"code","124e35eb":"code","d1b548b6":"code","4f9c9ee4":"code","bedacad4":"code","24bbb39c":"code","53f9d6c9":"code","f1394e7d":"code","3b7ec094":"code","a04ae0e5":"code","103e506c":"code","499323d3":"code","239ed9c4":"code","11b1da66":"code","1c2e7adf":"code","9afcaa78":"code","586e4a4d":"code","dfb2af9c":"markdown","65e2d9b7":"markdown","0c1c28df":"markdown","6875c836":"markdown","7560c86f":"markdown","bbdb8074":"markdown","c6bbfbce":"markdown","3f427c78":"markdown","add94294":"markdown","c1b24df9":"markdown","092c45a6":"markdown","2556373c":"markdown"},"source":{"3f6e641f":"# https:\/\/www.kaggle.com\/c\/bengaliai-cv19\/discussion\/123198\n# \uc704\uc640 \uac19\uc774 best single model\uc5d0 \ub4e4\uc5b4\uac00\uc11c \uc0ac\ub78c\ub4e4\uc774 \uc5b4\ub5a4 \uc2dd\uc73c\ub85c \ubaa8\ub378\ub9c1\uc744 \ud558\uace0 \uc810\uc218\ub97c \uc5b4\ub290\uc815\ub3c4 \uc5bb\uc5c8\ub294\uc9c0 \ud30c\uc545\ud558\uace0 \n# \ub300\ud68c\ub97c \uc9c4\ud589\ud558\uba74 \ube44\uad50\uc801 \uc218\uc6d4\ud558\uac8c \uc9c4\ud589\uac00\ub2a5","8358c577":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os","889bc257":"data_dir = '..\/input\/bengaliai-cv19'","70dcfcb1":"files_train = [f'train_image_data_{fid}.parquet' for fid in range(4)]\n# for\ubb38\uacfc f string\uc744 \uc774\uc6a9\ud574\uc11c \ubd88\ub7ec\uc628\ub2e4.\n\n\n### f string example ###\n# name = 'Song' sex = 'male' \n# f'Hi, I am {name}. I am {sex}.'\n# >>> 'Hi, I am song. I am male.'\n","c922109c":"files_train","65d598ba":"train0 = pd.read_parquet('..\/input\/bengaliai-cv19\/train_image_data_0.parquet')","6b585149":"train0.head()","e79b8a36":"print(train0.shape)\n\n# column\uc774 \ub9ce\uc740 \uc774\uc720 : gray scale\uc758 \uc0ac\uc9c4\uc778\ub370 (137 heights, 236 widths)\ud06c\uae30\uc758 \uc774\ubbf8\uc9c0\uc774\ubbc0\ub85c \n\nprint(137*236+1) \n\n# \uc5ec\uae30\uc11c +1\uc740 'image_id'\uc774\ub2e4.","009298e4":"idx=0\nimg=train0.iloc[idx,1:].values.astype(np.uint8) # '1:'\uc778 \uc774\uc720\ub294 image_id \ub97c \ube7c\uc57c\ud558\uae30 \ub54c\ubb38 \n\n# 'astype(np.uint8)' : \uad73\uc774 \ud070 \uc6a9\ub7c9\uc774 \ud544\uc694\ud55c datatype\uc744 \uc0ac\uc6a9\ud560 \ud544\uc694\uac00 \uc5c6\uace0 \ud6a8\uc728\uc801\uc73c\ub85c \uc9c4\ud589\ud558\uae30\uc704\ud574","4ceee238":"img.shape","a51798f1":"img.reshape(137,236).shape","8207aa76":"plt.imshow(img.reshape(137,236))","0f1512bf":"plt.imshow(img.reshape(137,236), cmap = 'gray') # \ud751\ubc31\uc774\ubbf8\uc9c0 \uc2a4\ucf00\uc77c\ub85c \ubcf4\uae30","99e758ab":"len(train0) \n\n# \ud589\uc758 \uc218","43cb57c2":"idx = np.random.randint(len(train0))\n\n# \uc774\ubbf8\uc9c0 \ub300\ud68c\ub97c \ud560\ub54c \uc911\uc694\ud55c \uac83\uc740 \uc0ac\uc9c4\uc744 \uacc4\uc18d \ubcf4\uba74\uc11c \uc5b4\ub5bb\uac8c \uc0dd\uacbc\ub294\uc9c0 \ub300\ucda9 \ud30c\uc545\uc744 \ud574\uc57c \ub41c\ub2e4.\n# \ub610\ud55c, \uc5b4\ub5a4 augmentation\uc744 \ud560\uc9c0\ub3c4 \uc0dd\uac01.\n# \uc9c0\uae08 \uc774\ubbf8\uc9c0\ub97c \ubd84\uc11d\ud574\ubcf4\uba74 \uc5b4\ub514 \uc3e0\ub9ac\uc9c0 \uc54a\uace0 \ub098\ub984 \uc911\uc2ec\uc744 \ub450\uace0 \uc801\ud600\uc788\ub2e4. -> \uc0ac\uc9c4\uc758 quality\uac00 \uad1c\ucc2e\ub2e4\n\nimg=train0.iloc[idx,1:].values.astype(np.uint8)\n\nplt.imshow(255 - img.reshape(137,236), cmap = 'gray') # \ud751\ubc31\uc774\ubbf8\uc9c0 \ubc18\uc804 \uc2a4\ucf00\uc77c\ub85c \ubcf4\uae30 like MNIST","00e89153":"df_train = pd.read_csv('..\/input\/bengaliai-cv19\/train.csv')","ba56a714":"df_train.head()","9d0795e0":"df_train.shape","abd72c9c":"plt.figure(figsize=(10,50))\ndf_train['grapheme_root'].value_counts().sort_index().plot.barh()\n\n# grapheme_root\ub77c\ub294 label\uc758 class\uac00 \ucd1d 168\uac1c(0~167)\uac00 \uc788\ub294\ub370\n# \uac01 \ud074\ub798\uc2a4\uac00 \uc5b4\ub5a4 count\ub97c \uac00\uc9c0\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504","0225e683":"plt.figure(figsize=(10,10))\ndf_train['vowel_diacritic'].value_counts().sort_index().plot.barh()","193fa928":"plt.figure(figsize=(10,7))\ndf_train['consonant_diacritic'].value_counts().sort_index().plot.barh()","516c59e5":"!pip install iterative-stratification\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","af0709af":"df_train['id'] = df_train['image_id'].apply(lambda x: int(x.split('_')[1]))","2866033f":"X = df_train[['id', 'grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].values[:, 0] # id\ny = df_train[['id', 'grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].values[:, 1:] # target","55a3629e":"mskf = MultilabelStratifiedKFold(n_splits=6, random_state=1944, shuffle=True)","a3217e01":"df_train['fold'] = -1","33eb3356":"for i, (trn_idx, vid_idx) in enumerate(mskf.split(X,y)):\n    df_train.loc[vid_idx, 'fold'] = i  ","9261935c":"df_train['fold'].value_counts()","06db064c":"df_train.to_csv('df_folds.csv', index=False)","724ee695":"import joblib\n\nfrom tqdm import tqdm\n# tqdm : \ubc18\ubcf5\ubb38\uc774 \uc5b4\ub514\uae4c\uc9c0 \uc9c4\ud589\ub418\uc5c8\ub294\uc9c0 \uc54c\uace0\uc2f6\uc744\ub54c \uc0ac\uc6a9","5b55dd3b":"train0.head()","61a6f6a4":"img_ids = train0['image_id'].values\nimg_array = train0.iloc[:, 1:].values","d78e43ab":"for idx in tqdm(range(len(train0))):\n    break","44401725":"img_id = img_ids[idx]\nimg = img_array[idx]","3388e084":"os.mkdir('\/kaggle\/working\/train_images\/')","4af1ab2e":"joblib.dump(img, f'.\/train_images\/{img_id}.pkl')\n\n# pkl\ub85c \uc800\uc7a5\ud558\uba74 \uc544\uc8fc \ube68\ub9ac \uc77d\uc744 \uc218 \uc788\ub2e4","4345467a":"%%time\nimg0 = joblib.load(f'.\/train_images\/{img_id}.pkl')","643bede4":"%%time\ntrain0.iloc[0, 1:].values","f9f67331":"25.2\/1.8\n# 14\ubc30 \uc815\ub3c4 \uc2dc\uac04\ucc28\uc774\uac00 \ub09c\ub2e4.!!\n# RAM\uc774 \uc791\ub2e4\uba74 pkl\ub85c \uc800\uc7a5\ud574\uc11c load\ud558\ub294\uac8c \ud6e8\uc52c \ud6a8\uc728\uc801\uc774\ub2e4","70601b6f":"## Full code\n\n# for fname in files_train: \n#     F = os.path.join(data_dir, fname)\n#     train_image = pd.read_parquet(F)\n#     img_ids = train_image['image_id'].values\n#     img_array = train_image.iloc[:, 1:].values\n#     for idx in tqdm(range(len(train_image))):\n#         img_id = img_ids[idx]\n#         img = img_array[idx]\n#         joblib.dump(img, f'.\/train_images\/{img_id}.pkl')\n","672d9e9c":"import torch \n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nfrom torch.utils.data import Dataset\nimport matplotlib.pyplot as plt\nimport joblib","aabe10d6":"df_train.head()","d1fa5a27":"index=0\nHEIGHT=137\nWIDTH=236","bfdcc4fa":"img_ids = df_train['image_id'].values","ae5edb13":"img_id = img_ids[index]","85fb5355":"img_id","f96e56ff":"img = joblib.load(f'.\/train_images\/{img_id}.pkl').astype(np.uint8)","138b9071":"img.shape","8365e954":"img = img.reshape(HEIGHT, WIDTH) ","6392cc75":"plt.imshow(img, cmap='gray')","b3ff0649":"img = 255-img","124e35eb":"plt.imshow(img, cmap='gray')","d1b548b6":"# pytorch model\uc5d0 \ub123\uc5b4\uc8fc\ub824\uba74 channel\uc774 \uc788\uc5b4\uc57c \ud568\n\nprint(img[:, :, np.newaxis].shape)\nimg = img[:, :, np.newaxis]","4f9c9ee4":"label_1 = df_train.iloc[index].grapheme_root\nlabel_2 = df_train.iloc[index].vowel_diacritic\nlabel_3 = df_train.iloc[index].consonant_diacritic","bedacad4":"class BengaliDataset(Dataset):\n    def __init__(self, csv, img_height, img_width):\n        self.csv = csv.reset_index()\n        self.img_ids = csv['image_id'].values\n        self.img_height = img_height\n        self.img_width = img_width\n    \n    def __len__(self):\n        return len(self.csv) # Dataset \uae38\uc774 ; \uc9c0\uae08\uc740 csv\ud30c\uc77c\uc774 dataset\uc774\ubbc0\ub85c csv\ud30c\uc77c \uae38\uc774\ub97c return\ud558\uba74 \ub428\n    \n    def __getitem__(self, index):\n        img_id = self.img_ids[index]\n        img = joblib.load(f'.\/train_images\/{img_id}.pkl')\n        img = img.reshape(self.img_height, self.img_width).astype(np.uint8)\n        img = 255-img\n        img = img[:,:,np.newaxis]\n        \n        label_1 = self.csv.iloc[index].grapheme_root\n        label_2 = self.csv.iloc[index].vowel_diacritic\n        label_3 = self.csv.iloc[index].consonant_diacritic\n        \n        return (torch.tensor(img, dtype=torch.float).permute(2,0,1), torch.tensor(label_1, dtype=torch.long), \n                torch.tensor(label_2, dtype=torch.long), torch.tensor(label_3, dtype=torch.long))\n","24bbb39c":"print(img.shape)\nprint(torch.tensor(img).shape)\nprint(torch.tensor(img).permute(2,0,1).shape) # permute : \ucc28\uc6d0\uc744 \ubc14\uafd4\uc8fc\uae30 \uc704\ud574 \uc0ac\uc6a9","53f9d6c9":"df_train['fold'] = pd.read_csv('.\/df_folds.csv')['fold']","f1394e7d":"df_train['fold']","3b7ec094":"trn_fold = [i for i in range(6) if i not in [5]]","a04ae0e5":"trn_fold","103e506c":"vid_fold = [5]","499323d3":"trn_idx = df_train.loc[df_train['fold'].isin(trn_fold)].index\nvid_idx = df_train.loc[df_train['fold'].isin(vid_fold)].index","239ed9c4":"trn_dataset = BengaliDataset(csv=df_train.loc[trn_idx], img_height = HEIGHT, img_width = WIDTH)","11b1da66":"trn_dataset[0][0].shape","1c2e7adf":"plt.imshow(trn_dataset[0][0].permute(1,2,0).numpy()[:,:,0], cmap='gray')","9afcaa78":"print(trn_dataset[0][0].permute(1,2,0).numpy().shape) # channel\uc744 \uc5c6\uc560\uc8fc\uc5b4\uc57c \uc0ac\uc9c4\uc744 \ubcfc \uc218 \uc788\ub2e4.\nprint(trn_dataset[0][0].permute(1,2,0).numpy()[:,:,0].shape) # '[:,:,0]'\uc744 \ud1b5\ud574 channel \uc0ad\uc81c","586e4a4d":"# idx = 0\n# idx += 1\n# plt.imshow(trn_dataset[idx][0].permute(1,2,0).numpy()[:,:,0], cmap='gray')","dfb2af9c":"#### Now we have to use Stratified fold, but The stratified fold provided by sklearn is applied to only one label.\n#### Now that we are dealing with three labels, we have to use 'iterative-stratification', a library that helps us fold while keeping the distribution of all three labels the same.","65e2d9b7":"- Same situation as grapheme_root","0c1c28df":"# Efficient learning process\n- dataframe\uc744 row by row\ub85c \uc798\ub77c\uc11c \ud559\uc2b5\ud560 \ub54c \uc870\uae08 \ub354 \ube68\ub9ac \ud560 \uc218 \uc788\uac8c \ud558\ub294 \ubc29\ubc95\n- \ubcf8 \ub300\ud68c\ub294 parquet\ud30c\uc77c 4\uac1c\ub97c \ud559\uc2b5\uc2dc\ucf1c\uc11c \ud569\uccd0\uc57c \ud558\uae30 \ub54c\ubb38\uc5d0 \uba54\ubaa8\ub9ac\uac00 \ud06c\uc9c0 \uc54a\uc73c\uba74 \uc790\uce6b \ud559\uc2b5\ud558\ub2e4\uac00 \ub04a\uae38\uc218\ub3c4 \uc788\ub2e4..\n- pandas\uac00 \uc77d\uace0 \uc4f0\ub294 \uc18d\ub3c4\uac00 \uc0dd\uac01\ubcf4\ub2e4 \ub290\ub9b0 \ud3b8.\n- \ub9cc\uc57d\uc5d0 \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294 \ucf54\ub4dc\ub97c \ub9cc\ub4e4\uc5c8\ub294\ub370 pandas\uc5d0\uc11c \ubd88\ub7ec\uc624\ub294 \uc2dd\uc73c\ub85c \ub9cc\ub4e4\uba74 \uc804\uccb4\uc801\uc778 \ud559\uc2b5 \uc2dc\uac04\uc774 \uae38\uc5b4\uc9c4\ub2e4.\n- So How to fix it??","6875c836":"- Each class in grapheme_root is very unbalanced!\n- In this condition, if we randomly sample, the distribution of classes for each fold may not be properly entered.\n- If that happens, the model may not be able to train properly.\n#### So we have to do a stratified fold","7560c86f":"- Same situation as grapheme_root","bbdb8074":"#  Image visualization and folding","c6bbfbce":"### Checking the Distribution of label","3f427c78":"##### \uacb0\uad6d \ud55c row\uac00 \ud558\ub098\uc758 \uc774\ubbf8\uc9c0\ub97c \ub73b\ud55c\ub2e4. \uc989 (137 multiply 236)\ub97c \uc77c\ub82c\ub85c \ub098\uc5f4\ud55c \uac83\uc774\ubbc0\ub85c \uc774\ubbf8\uc9c0\ub97c \ubcf4\uace0\uc790 \ud558\uba74 \ub2e4\uc2dc (137 multiply 236)\ud615\ud0dc\ub85c \ub9cc\ub4e4\uc5b4\uc8fc\uba74 \ub418\ub294 \uac83.","add94294":"# Multi-label stratification folding","c1b24df9":"##### For Pytorch model\n\n- (B, W, H, C) -> (B, C, W, H)\n- B : Batch, W : Width, H : Height, C : Channel\n- (16, 137, 236, 1) -> (16, 1, 137, 236)","092c45a6":"# Pytorch dataset","2556373c":"- \ud2b9\uc815 index\uc5d0 \ub9de\ub294 image pkl\uc744 \ubd88\ub7ec\uc628\ub2e4\n- \uadf8 pkl\uc744 reshape \ud55c\ub2e4\n- channel \ucd94\uac00(for pytorch)\n- \uadf8 image\uac00 \uac00\uc9c0\uace0 \uc788\ub294 \uac01 label\ubcc4 \ud074\ub798\uc2a4\ub97c \ub2e4 \uac00\uc838\uc628\ub2e4"}}