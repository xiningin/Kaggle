{"cell_type":{"aa170b76":"code","83252a96":"code","35126e26":"code","b4e1d91e":"code","30eab3dd":"code","ef53d923":"code","e669e5de":"code","46f78deb":"code","ce9d8f72":"code","06233916":"code","bd6fbed0":"code","115556f6":"code","cae4af49":"code","34176c15":"code","1d295e93":"code","aedce190":"code","62543b0a":"code","56a268eb":"code","ef23a20e":"code","1cdb9970":"code","d5cf0338":"code","8228deff":"code","60581fcc":"code","173def25":"code","485573fb":"code","9efdd5ec":"code","b5808249":"code","66d5d3be":"code","86ce8810":"code","c97c3374":"code","9ef46aa0":"markdown","0aa01956":"markdown","0b53c1e6":"markdown","36d425db":"markdown","d796d4b3":"markdown","a65b5033":"markdown","ec5500a2":"markdown","8f07c268":"markdown","710ef926":"markdown","74881b4c":"markdown","2c5a2727":"markdown","66cc4df4":"markdown","6f99690d":"markdown","5d161874":"markdown","25a7d7ff":"markdown","26534330":"markdown","42c7124b":"markdown","fbbaa876":"markdown"},"source":{"aa170b76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# plotly library\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","83252a96":"data = pd.read_csv('..\/input\/data.csv')","35126e26":"data.info()","b4e1d91e":"data.head()","30eab3dd":"data.describe()","ef53d923":"# I drop the unnecessery columns for my prediction\ndata = data.drop(['Unnamed: 32', 'id'], axis=1)\ndata.head()","e669e5de":"data['diagnosis'].value_counts()","46f78deb":"color_list = ['red' if i == 'M' else 'blue' for i in data.loc[:,'diagnosis']]\npd.plotting.scatter_matrix(data.iloc[:, 7:13],\n                                       c=color_list,\n                                       figsize= [10,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '.',\n                                       edgecolor= \"black\")\nplt.show()","ce9d8f72":"data['diagnosis'] = [1 if x=='M' else 0 for x in  data['diagnosis']]","06233916":"data.head()","bd6fbed0":"#Choosing x and y values\n\n#x is our features except diagnosis (classification columns)\n#y is diagnosis\nx_data = data.iloc[:,1:]\ny = data['diagnosis']","115556f6":"# Normalization\nx = (x_data - np.min(x_data) \/ (np.max(x_data) - np.min(x_data)))","cae4af49":"x.head()","34176c15":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1) # 0.3 means 30% of data is splitted for testing. Remaining 70% is used to train our data\nprint('x_train shape : ', x_train.shape)\nprint('y_train shape : ', y_train.shape)\nprint('x_test shape : ', x_test.shape)\nprint('y_test shape : ', y_test.shape)","1d295e93":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train) # to train our data\npredicted_values = knn.predict(x_test)\ncorrect_values = np.array(y_test) # just to make them array\nprint('KNN (with k=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy","aedce190":"# find best n value for knn\nbest_neig= range(1,25) \ntrain_accuracy_list =[]\ntest_accuracy_list =[]\n\nfor each in best_neig:\n    knn = KNeighborsClassifier(n_neighbors =each)\n    knn.fit(x_train,  y_train)\n    train_accuracy_list.append( knn.score(x_train, y_train))    \n    test_accuracy_list.append( knn.score(x_test, y_test))    \n    \n        \nprint( 'best k for Knn : {} , best accuracy : {}'.format(test_accuracy_list.index(np.max(test_accuracy_list))+1, np.max(test_accuracy_list)))\nplt.figure(figsize=[13,8])\nplt.plot(best_neig, train_accuracy_list,label = 'Train Accuracy')\nplt.plot(best_neig, test_accuracy_list,label = 'Test Accuracy')\nplt.title('Neighbors vs accuracy ')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid()\nplt.xticks(best_neig)\nplt.show()","62543b0a":"#SVM\nfrom sklearn.svm import SVC\nsvm = SVC(random_state = 1,gamma='auto')\nsvm.fit(x_train,y_train)\nprint(\"accuracy of svm: \",svm.score(x_test,y_test))","56a268eb":"#Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"accuracy of naive bayes: \",nb.score(x_test,y_test))","ef23a20e":"#Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"accuracy of Decision Tree Classification: \", dt.score(x_test,y_test))","1cdb9970":"#Random Forest Classcification\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100 ,random_state=1)\nrf.fit(x_train,y_train)\nprint(\"accuracy of Random Forest Classicifation: \",rf.score(x_test,y_test))","d5cf0338":"score_list1=[]\nfor i in range(100,501,50):\n    rf2=RandomForestClassifier(n_estimators=i,random_state=1)\n    rf2.fit(x_train,y_train)\n    score_list1.append(rf2.score(x_test,y_test))\nplt.figure(figsize=(10,10))\nplt.plot(range(100,501,50),score_list1)\nplt.xlabel(\"number of estimators\")\nplt.ylabel(\"accuracy\")\nplt.grid()\nplt.show()\n\nprint(\"Maximum value of accuracy is {} \\nwhen n_estimators= {}.\".format(max(score_list1),(1+score_list1.index(max(score_list1)))*100))","8228deff":"#Confusion matrix of RFC\ny_pred=rf.predict(x_test)\ny_true=y_test\n\n#cm\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\n#cm visualization\nf,ax=plt.subplots(figsize=(8,8))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"predicted value\")\nplt.ylabel(\"real value\")\nplt.show()","60581fcc":"##Confusion matrix of KNN\ny_pred1=knn.predict(x_test)\ny_true=y_test\n#cm\ncm1=confusion_matrix(y_true,y_pred1)\n\n#cm visualization\nf,ax=plt.subplots(figsize=(8,8))\nsns.heatmap(cm1,annot=True,linewidths=0.5,linecolor=\"blue\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"predicted value\")\nplt.ylabel(\"real value\")\nplt.show()","173def25":"#Confusion matrix of Decision Tree Classf.\ny_pred2=dt.predict(x_test)\ny_true=y_test\n#cm\ncm2=confusion_matrix(y_true,y_pred2)\n\n#cm visualization\nf,ax=plt.subplots(figsize=(8,8))\nsns.heatmap(cm2,annot=True,linewidths=0.5,linecolor=\"green\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"predicted value\")\nplt.ylabel(\"real value\")\nplt.show()","485573fb":"y_pred3=svm.predict(x_test)\ny_true=y_test\n#cm\ncm3=confusion_matrix(y_true,y_pred3)\n\n#cm visualization\nf,ax=plt.subplots(figsize=(8,8))\nsns.heatmap(cm3,annot=True,linewidths=0.5,linecolor=\"green\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"predicted value\")\nplt.ylabel(\"real value\")\nplt.show()","9efdd5ec":"y_pred4=nb.predict(x_test)\ny_true=y_test\n#cm\ncm4=confusion_matrix(y_true,y_pred4)\n\n#cm visualization\nf,ax=plt.subplots(figsize=(8,8))\nsns.heatmap(cm4,annot=True,linewidths=0.5,linecolor=\"green\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"predicted value\")\nplt.ylabel(\"real value\")\nplt.show()","b5808249":"dictionary={\"model\":[\"KNN\",\"SVM\",\"NB\",\"DT\",\"RF\"],\"score\":[knn.score(x_test,y_test),svm.score(x_test,y_test),nb.score(x_test,y_test),dt.score(x_test,y_test),rf.score(x_test,y_test)]}\ndf1=pd.DataFrame(dictionary)","66d5d3be":"#sort the values of data \nnew_index5=df1.score.sort_values(ascending=False).index.values\nsorted_data5=df1.reindex(new_index5)\n\n# create trace1 \ntrace1 = go.Bar(\n                x = sorted_data5.model,\n                y = sorted_data5.score,\n                name = \"score\",\n                marker = dict(color = 'rgba(200, 125, 200, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                text = sorted_data5.model)\ndat = [trace1]\nlayout = go.Layout(barmode = \"group\",title= 'Scores of Classifications')\nfig = go.Figure(data = dat, layout = layout)\niplot(fig)","86ce8810":"from sklearn.metrics import classification_report\nprint('Classification report: \\n',classification_report(y_test,y_pred))","c97c3374":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred = logreg.predict(x_test)\nprint('logistic regression score: ', logreg.score(x_test, y_test))","9ef46aa0":"<a id=\"8\"><\/a>\n### Confusion Matrix, Raports & Visualizations\nConfusion matrix gives the number of true and false predicitons in our classificaiton. It is more reliable than accuracy. \nHere,\n\n* y_pred: results that we predict.\n* y_test: our real values.\n* TP: I predicted True and the label True\n* TN: I predicted False and the label False\n* FP: I predicted True but the labes False \n* FN: I predicten False but the labels True ","0aa01956":"<a id=\"13\"><\/a>\n## Conclusion\n* It seems that Decision Tree Classification is more effective which can also be seen from accuracy scores","0b53c1e6":"<a id=\"11\"><\/a>\n### Logistic Regression","36d425db":"<a id=\"3\"><\/a>\n### KNN (K-Nearest Neighbour) Classification\n<a href=\"https:\/\/ibb.co\/z8cN8yM\"><img src=\"https:\/\/i.ibb.co\/KNZmNMP\/10.jpg\" alt=\"10\" border=\"0\"><\/a>\n<a href=\"https:\/\/ibb.co\/bPxnJwM\"><img src=\"https:\/\/i.ibb.co\/xDyQH9t\/11.jpg\" alt=\"11\" border=\"0\"><\/a><br \/><a target='_blank' href='http:\/\/www.statewideinventory.org\/ford-0-60-times\/'>2012 ford taurus 0 60<\/a><br \/>","d796d4b3":"<a id=\"1\"><\/a> \n## EDA(Exploratory Data Analysis)","a65b5033":"We know that our accuracy is 0.9649, which is the best result. But here we predicted\n\n* TP: 108\n* TN: 0\n* FP: 6\n* FN:57","ec5500a2":"<a id=\"2\"><\/a>\n## MACHINE LEARNING","8f07c268":"[](http:\/\/) #** Introduction** \nIn this kernel we will learn what is the supervised algorithm and how to supervised algorithm use in the kernel. \n\n1. [EDA (Exploratory Data Analysis)](#1)\n1. [MACHINE LEARNING](#2)\n    1. [KNN (K-Nearest Neighbour) Classification](#3)\n    1. [Support Vector Machine( SVM) Classification](#4)\n    1. [Naive Bayes Classification](#5)\n    1. [Decision Tree Classification](#6)\n    1. [Random Forest Classification](#7)\n    1. [Confusion Matrix, Raports & Visualizations](#8)\n    1. [Classification Raports](#10)\n    1. [Logistic Regression Classification](#11)\n   \n    \n1. [Conclusion](#13)  ","710ef926":"We know that our accuracy is 0.9356, which is the best result, . But here we predicted\n\n* TP: 104\n* TN: 4\n* FP: 11\n* FN:52","74881b4c":"<a id=\"5\"><\/a>\n### Naive Bayes\n\n\n<a href=\"https:\/\/ibb.co\/HG9T2gT\"><img src=\"https:\/\/i.ibb.co\/440RfNR\/13.jpg\" alt=\"13\" border=\"0\"><\/a>","2c5a2727":"<a id=\"10\"><\/a>\n### Classification Report","66cc4df4":"We know that our accuracy is 0.9473, which is the best result. But here we predicted\n\n* TP: 104\n* TN: 4\n* FP: 5\n* FN:58","6f99690d":"We know that our accuracy is 0.9649, which is the best result. But here we predicted\n\n* TP: 108\n* TN: 0\n* FP: 63\n* FN:0","5d161874":"Reference :\n* https:\/\/www.kaggle.com\/sibelkcansu\/machine-learning-classification,\n* https:\/\/medium.com\/@hashinclude\/knn-kth-nearest-neighbour-algorit-4b83c6fa31bf,\n* https:\/\/www.slideshare.net\/tilanigunawardena\/k-nearest-neighbors,\n* https:\/\/www.slideshare.net\/gladysCJ\/lesson-71-naive-bayes-classifier,\n* https:\/\/www.datacamp.com\/community\/tutorials\/decision-tree-classification-python,\n* https:\/\/medium.com\/machine-learning-bites\/machine-learning-decision-tree-classifier-9eb67cad263e","25a7d7ff":"<a id=\"6\"><\/a>\n### Decision Tree Classification\n* A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome.\n* It partitions the tree in recursively manner call recursive partitioning.\n* This flowchart-like structure helps you in decision making.\n\n<a href=\"https:\/\/ibb.co\/T0kxbsq\"><img src=\"https:\/\/i.ibb.co\/qgd3WvF\/14.jpg\" alt=\"14\" border=\"0\"><\/a><br \/><a target='_blank' href='https:\/\/poetandpoem.com\/meaning-bonolota-sen-jibanananda-das'>jibanananda das kobita in bengali<\/a><br \/>","26534330":"<a id=\"3\"><\/a>\n## Support Vector Machine( SVM) Classification\n* Use for both classification or regression challenges\n* It is mostly useful in non-linear separation problem.\n\n<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/i.ibb.co\/jbVJpgs\/12.jpg\" alt=\"12\" border=\"0\"><\/a>","42c7124b":"We know that our accuracy is 0.9590, which is the best result, when number of estimators is 200 . But here we predicted\n\n* TP: 57\n* TN: 106\n* FP: 2\n* FN:6\n","fbbaa876":"<a id=\"7\"><\/a>\n### Random Forest Classification\n* It consists of a combination of more than one decision tree cllassifications."}}