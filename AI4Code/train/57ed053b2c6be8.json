{"cell_type":{"9bd07140":"code","e814af97":"code","b126312a":"code","3bdb68d3":"code","88a5c571":"code","7d440edc":"code","1f4777c2":"code","2b73e41b":"code","c8023236":"code","7bf954b2":"code","fc5f3f81":"code","1197495b":"code","2eaa841e":"code","05cbe9e0":"code","b9076ae3":"code","15da1d71":"code","2cdfcd75":"code","e7a503b5":"code","a3188b73":"code","d7b0d22b":"code","1365d4f1":"code","cabc3a21":"code","6ae8c730":"code","e4e7f285":"code","a3b150cf":"code","1eba924b":"code","1253a16a":"code","efe04a7c":"code","8a3b2fe0":"code","f6185d35":"code","15779348":"code","c00b7a72":"code","34f20811":"code","61c0069b":"code","22aa8b96":"code","8acb33d4":"code","5026b34c":"code","bf43dbec":"code","31fb8cf8":"code","ce2ba910":"code","71226db7":"code","e9fc13c1":"markdown","1d69fc5c":"markdown","e67bfdd2":"markdown","30eef7f7":"markdown","5a38f7bd":"markdown","b4dc96ba":"markdown","d00ee818":"markdown","a11e130a":"markdown","ac9c1e3a":"markdown","3f3c4f2a":"markdown","72639086":"markdown","35e08b2b":"markdown","1762d539":"markdown","9cb52280":"markdown","95a389c2":"markdown","88f3fb0f":"markdown","5a741124":"markdown","e943f45b":"markdown","951d6888":"markdown","d8011ad1":"markdown","dafa142b":"markdown","5e6ac1b7":"markdown","a4b3043d":"markdown","4d6510f6":"markdown","b22a8b8d":"markdown","0c49d70d":"markdown","e2c4d82d":"markdown","19bc98e2":"markdown","789b73c7":"markdown","3ce334cf":"markdown","d5e62132":"markdown"},"source":{"9bd07140":"#!pip install yellowbrick\n\nimport pandas as pd\nimport scipy as si\nimport numpy as pi\nimport seaborn as sn\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","e814af97":"# Setting the display layout for the coding environment\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:90% !important; }<\/style>\"))\n\nsn.set(font_scale=1.0)","b126312a":"import os\n\nprint('Listing files in the folder')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3bdb68d3":"# Reading the data from the data source\n\ndata_parkinson = pd.read_csv(\"..\/input\/parkinsons-data-set\/parkinsons.data\")\n\nprint(\"\\nDimensions of the data\")\n\nprint(\"Shape of the data :{0}\".format(data_parkinson.shape))\nprint(\"Size of the data :{0}\".format(data_parkinson.size))\nprint(\"nDim of the data :{0}\".format(data_parkinson.ndim))\nprint(\"Shape x * y :{0}\".format(data_parkinson.shape[0]*data_parkinson.shape[1]))\n\nprint(\"\\nData types of the Data\")\nprint(data_parkinson.info())\n\nprint(\"\\nData elements from file\")\ndata_parkinson.head(10).T","88a5c571":"print(\"\\nCheck for the NULL values\")\ndata_parkinson.isna().sum()","7d440edc":"\n# define functions to identify the number of outliers\n\ndef Identify_Outliers(data_column):\n    \n    dataFrame = pd.DataFrame(data_column)\n    \n    Quar1 = dataFrame.quantile(0.25)  \n    Quar3 = dataFrame.quantile(0.75)  \n    \n    IQR = Quar3 - Quar1\n    \n    return ((dataFrame < (Quar1-1.5*IQR)) | (dataFrame> (Quar3+1.5*IQR))).sum()\n    \n\nprint(\"\\n Number of outliers in each attribute \\n\")\nfor (columnName, columnData) in data_parkinson.iteritems(): \n    if(columnName == \"name\"):\n        continue\n    print(Identify_Outliers(data_parkinson[columnName]))\n\ndata_columns = data_parkinson.columns","1f4777c2":"# Visualising the outliers which are >=10\n\nfig, axis = plt.subplots(2, 5, figsize=(25, 15), sharex=False)\n\naxis[0,0].set_title(\"MDVP:Fhi(Hz)\")\nsn.boxplot(data_parkinson[\"MDVP:Fhi(Hz)\"],color='green',orient='v',ax=axis[0,0]);\n\naxis[0,1].set_title(\"MDVP:Jitter(%)\")\nsn.boxplot(data_parkinson[\"MDVP:Jitter(%)\"],color='green',orient='v',ax=axis[0,1])\n\naxis[0,2].set_title(\"MDVP:RAP\")\nsn.boxplot(data_parkinson[\"MDVP:RAP\"],color='green',orient='v',ax=axis[0,2])\n\naxis[0,3].set_title(\"MDVP:PPQ\")\nsn.boxplot(data_parkinson[\"MDVP:PPQ\"],color='green',orient='v',ax=axis[0,3])\n\naxis[0,4].set_title(\"Jitter:DDP\")\nsn.boxplot(data_parkinson[\"Jitter:DDP\"],color='green',orient='v',ax=axis[0,4])\n\n\n\naxis[1,0].set_title(\"MDVP:Shimmer(dB)\")\nsn.boxplot(data_parkinson[\"MDVP:Shimmer(dB)\"],color='orange',orient='v',ax=axis[1,0])\n\naxis[1,1].set_title(\"Shimmer:APQ5\")\nsn.boxplot(data_parkinson[\"Shimmer:APQ5\"],color='orange',orient='v',ax=axis[1,1])\n\naxis[1,2].set_title(\"MDVP:APQ\")\nsn.boxplot(data_parkinson[\"MDVP:APQ\"],color='orange',orient='v',ax=axis[1,2])\n\naxis[1,3].set_title(\"NHRP\")\nsn.boxplot(data_parkinson[\"NHR\"],color='orange',orient='v',ax=axis[1,3])\n\n\nplt.show()","2b73e41b":"# Linear info for the data\nprint('\\nFive point summary for the attributes')\ndata_parkinson.describe().T\n","c8023236":"print('Shape of Data: ', data_parkinson.shape)\n\nprint('\\nMedian for the data')\nprint(data_parkinson.median())\n\nprint('\\nMode for the data')\nprint(data_parkinson.mode())","7bf954b2":"print('\\nSkewing of the data\\n')\n\nfor col_name in data_columns:\n    \n    if(col_name == 'name' or col_name=='status'):\n        continue\n    print('{0} Parameter is Right Skewed:   {1}'.format(col_name,data_parkinson[col_name].mean() > data_parkinson[col_name].median()))\n    \nprint('\\n{0} Parameter is Left Skewed: {1}'.format('HNR',data_parkinson['HNR'].mean() < data_parkinson['HNR'].median()))\nprint('{0} Parameter is Left Skewed: {1}'.format('DFA',data_parkinson['DFA'].mean() < data_parkinson['DFA'].median()))\n\nprint('\\nMeasures of the Skewed values\\n')\nprint(\"DFA: \",data_parkinson['DFA'].skew(axis=0))\nprint(\"HNR: \",data_parkinson['HNR'].skew(axis=0))\nprint(\"spread1: \",data_parkinson['spread1'].skew(axis=0))\nprint(\"spread2: \",data_parkinson['spread2'].skew(axis=0))\nprint(\"D2: \",data_parkinson['D2'].skew(axis=0))\nprint(\"PPE: \",data_parkinson['PPE'].skew(axis=0))\nprint(\"NHR: \",data_parkinson['NHR'].skew(axis=0))\nprint(\"Jitter:DDP: \",data_parkinson['Jitter:DDP'].skew(axis=0))","fc5f3f81":"def Display_BoxPlot(col,axis_rad,color):\n    axis_rad.set_title(col)\n    sn.distplot(data_parkinson[col],color=color,ax=axis_rad);\n\n\nfig, axis = plt.subplots(3, 4, figsize=(30, 15), sharex=False)\n\nDisplay_BoxPlot(data_columns[1],axis[0,0],'green')\nDisplay_BoxPlot(data_columns[2],axis[0,1],'green')\nDisplay_BoxPlot(data_columns[3],axis[0,2],'green')\nDisplay_BoxPlot(data_columns[4],axis[0,3],'green')\n\nDisplay_BoxPlot(data_columns[5],axis[1,0],'orange')\nDisplay_BoxPlot(data_columns[6],axis[1,1],'orange')\nDisplay_BoxPlot(data_columns[7],axis[1,2],'orange')\nDisplay_BoxPlot(data_columns[8],axis[1,3],'orange')\n\nDisplay_BoxPlot(data_columns[9],axis[2,0],'red')\nDisplay_BoxPlot(data_columns[10],axis[2,1],'red')\nDisplay_BoxPlot(data_columns[11],axis[2,2],'red')\nDisplay_BoxPlot(data_columns[12],axis[2,3],'red')\n\nplt.show()","1197495b":"fig, axis = plt.subplots(3, 4, figsize=(30, 15), sharex=False)\n\nDisplay_BoxPlot(data_columns[13],axis[0,0],'green')\nDisplay_BoxPlot(data_columns[14],axis[0,1],'green')\nDisplay_BoxPlot(data_columns[15],axis[0,2],'green')\nDisplay_BoxPlot(data_columns[16],axis[0,3],'green')\n\n#Display_BoxPlot(data_columns[17],axis[1,0],'orange') Skipping the status value\nDisplay_BoxPlot(data_columns[18],axis[1,1],'orange')\nDisplay_BoxPlot(data_columns[19],axis[1,2],'orange')\nDisplay_BoxPlot(data_columns[20],axis[1,3],'orange')\n\nDisplay_BoxPlot(data_columns[21],axis[2,0],'red')\nDisplay_BoxPlot(data_columns[22],axis[2,1],'red')\nDisplay_BoxPlot(data_columns[23],axis[2,2],'red')\n\nplt.show()","2eaa841e":"print(\"\\nNumber of Unique values in each attribute\")\nprint(data_parkinson.nunique())\n\nprint(\"\\nTotal values in categorical variables\" )\nprint(data_parkinson['status'].value_counts())\n#print(data_parkinson['MDVP:Jitter(Abs)'].value_counts())","05cbe9e0":"fig = plt.subplots(figsize=(8, 5), sharex=False)\n\nchart=sn.countplot(y='status',data=data_parkinson);\nplt.show()\n\nfig = plt.subplots(figsize=(20, 8), sharex=False)\n\nchart=sn.countplot(y='MDVP:Jitter(Abs)',data=data_parkinson);\n#chart.set_xticklabels(chart.get_xticklabels(), rotation=80)\n\nplt.show()","b9076ae3":"# Identify the Correlation between the variables\n\nfig, axis = plt.subplots(2, 1, figsize=(30, 30), sharex=False)\n\nsn.set(font_scale=1.2)\n\nsn.heatmap(data_parkinson.corr(), mask=pi.triu(data_parkinson.corr()),\n           \n           annot_kws={\"size\": 14}, annot=True,fmt='.3f',ax=axis[0],cmap='coolwarm');\n\ncorr_thresold=0.6\ncorel_data_parkinson=data_parkinson.corr()>corr_thresold\n\nsn.heatmap(corel_data_parkinson,mask=pi.triu(data_parkinson.corr()), annot_kws={\"size\": 16},annot=True,fmt='d',ax=axis[1]);\n\nplt.show()\n","15da1d71":"sn.set(font_scale=1.0)\n\nplot_vars=['MDVP:Fo(Hz)','MDVP:Fhi(Hz)','MDVP:Flo(Hz)']\n\nsn.pairplot(data_parkinson,kind='scatter',x_vars=plot_vars,y_vars=plot_vars,hue='status');\n\n\nplt.show()","2cdfcd75":"\nplot_vars=['MDVP:Jitter(%)','MDVP:Jitter(Abs)','MDVP:RAP','MDVP:PPQ','Jitter:DDP']\n\nsn.pairplot(data_parkinson,kind='scatter',x_vars=plot_vars,y_vars=plot_vars,hue='status');\n\n\nplt.show()","e7a503b5":"plot_vars=['MDVP:Shimmer','MDVP:Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5','MDVP:APQ','Shimmer:DDA']\n\nsn.pairplot(data_parkinson,kind='scatter',x_vars=plot_vars,y_vars=plot_vars,hue='status');\n\n\nplt.show()","a3188b73":"plot_vars=['NHR','HNR','RPDE','D2']\n\nsn.pairplot(data_parkinson,kind='scatter',x_vars=plot_vars,y_vars=plot_vars,hue='status');\n\n\nplt.show()","d7b0d22b":"plot_vars=['DFA','spread1','spread2','PPE']\n\nsn.pairplot(data_parkinson,kind='scatter',x_vars=plot_vars,y_vars=plot_vars,hue='status');\n\n\nplt.show()","1365d4f1":"fig, axis = plt.subplots(2, 3, figsize=(20, 12), sharex=False)\n\n\nsn.swarmplot(data=data_parkinson,y='DFA',x='status',hue='status',ax=axis[0,0]);\nsn.swarmplot(data=data_parkinson,y='PPE',x='status',hue='status',ax=axis[0,1]);\nsn.swarmplot(data=data_parkinson,y='MDVP:Flo(Hz)',x='status',hue='status',ax=axis[0,2]);\n\n\nsn.violinplot(data=data_parkinson,y='MDVP:RAP',x='status',hue='status',ax=axis[1,0]);\nsn.swarmplot(data=data_parkinson,y='MDVP:Shimmer',x='status',hue='status',ax=axis[1,1]);\nsn.violinplot(data=data_parkinson,y='NHR',x='status',hue='status',ax=axis[1,2]);\n\nplt.show()\n","cabc3a21":"data_extract_parkinson = data_parkinson.drop('name',axis=1).copy()\n\ncolumn_names = data_extract_parkinson.columns\n\nscaler_model = preprocessing.MinMaxScaler() # It looks better hence going with this scaler\n#scaler_model = preprocessing.StandardScaler()\nscaled_data= scaler_model.fit_transform(data_extract_parkinson)\n\ndata_scaled_parkinson = pd.DataFrame(scaled_data,columns=column_names)\ndata_scaled_parkinson.head()\n\n#print(\"\\nNumber of Unique values in each attribute\")\n#print(data_scaled_parkinson.nunique())","6ae8c730":"#Spliting the data for the model training\n\naxis_x = data_scaled_parkinson.drop(['status'],axis=1)\naxis_y = data_scaled_parkinson['status']\n\n#spliting the data into 70\/30\nx_train,x_test,y_train,y_test = train_test_split(axis_x,axis_y,test_size=0.3,random_state=120)\n\naxis_y.head()","e4e7f285":"# Identify the % of the data value\n\nprint('\\nSummary of the training and test data \\n')\npd_true = len(data_scaled_parkinson.loc[data_scaled_parkinson['status'] == 1])\npd_false = len(data_scaled_parkinson.loc[data_scaled_parkinson['status'] == 0])\n\nprint (f\"{len(x_train)\/len(data_scaled_parkinson)*100} % data in the Training\")\nprint (f\"{len(x_test)\/len(data_scaled_parkinson)*100} % data in the Testing\")\n\nprint(\"\\nPercent of the Parkinson's diseased\")\nprint (f\"Parkinsons: {pd_true} in total {len(data_scaled_parkinson)} {pd_true\/len(data_scaled_parkinson)*100}%\")\nprint (f\"Healthy: {pd_false} in total {len(data_scaled_parkinson)} {pd_false\/len(data_scaled_parkinson)*100}%\")\n\nprint(\"\\nPercent of the parkinson's diseased in Training data\")\nprint (f\"Parkinsons: {len(y_train.loc[y_train[:]==1])} in total {len(y_train)} {len(y_train.loc[y_train[:]==1])\/len(y_train)*100}%\")\nprint (f\"Healthy: {len(y_train.loc[y_train[:]==0])} in total {len(y_train)} {len(y_train.loc[y_train[:]==0])\/len(y_train)*100}%\")\n\nprint(\"\\nPercent of the parkinson's diseased in Test data\")\nprint (f\"Parkinsons: {len(y_test.loc[y_test[:]==1])} in total {len(y_test)} {len(y_test.loc[y_test[:]==1])\/len(y_test)*100}%\")\nprint (f\"Healthy: {len(y_test.loc[y_test[:]==0])} in total {len(y_test)} {len(y_test.loc[y_test[:]==0])\/len(y_test)*100}%\")","a3b150cf":"model_report_parkinson = pd.DataFrame(columns=['model_name','precision','recall','f1-score','support','train_accuracy','test_accuracy','auc_score'])\nmodel_report_healthy = pd.DataFrame(columns=['model_name','precision','recall','f1-score','support','train_accuracy','test_accuracy','auc_score'])\n\n\n#define a function to prepare the list with addtional parameters\n\ndef Get_Series(model_name,accuracy_train,accuracy_test,AUC_score):\n    \n    data_series = pd.Series([model_name,accuracy_train,accuracy_test,AUC_score], index=['model_name','train_accuracy','test_accuracy','auc_score'])\n    return data_series","1eba924b":"#define a function to generate a classification report list\n\ndef Generate_Classification_Report(model_name, actual_train,pred_train,actual_test,pred_test):\n    \n    accuracy_train=metrics.accuracy_score(actual_train,pred_train)\n    accuracy_test=metrics.accuracy_score(actual_test,pred_test)\n    AUC_score=metrics.roc_auc_score(actual_test, pred_test)\n\n    print(\"Model: {0}, Training Accuracy Score: {1}\".format(model_name,accuracy_train))\n    print(\"Model: {0}, Test Accuracy Score: {1}\".format(model_name,accuracy_test))\n    print(\"Model: {0}, AUC Score: {1}\".format(model_name,AUC_score))\n\n    report=classification_report(actual_test,pred_test,labels=[1,0],output_dict=True)\n    \n    series_pass = pd.Series(report['1'])\n    series_pass = pd.concat([series_pass,Get_Series(model_name,accuracy_train,accuracy_test,AUC_score)])\n    \n    series_fail = pd.Series(report['0'])\n    series_fail =pd.concat([series_fail,Get_Series(model_name,accuracy_train,accuracy_test,AUC_score)])\n        \n    return series_pass, series_fail\n","1253a16a":"# Define a function so that will be useful for displaying confusion matrix\n\ndef Display_Confusion_Matrix(actual_data,predicted_data):\n    confusin_matix = metrics.confusion_matrix(y_test,predicted_data,labels=[1,0])\n    confusin_matix = pd.DataFrame(confusin_matix, index=['Parkinsons','Healthy'],\n                              columns=['Pred_Parkinsons', 'Pred_Healthy'])\n\n    plt.figure(figsize=(10,5))\n    sn.heatmap(confusin_matix,annot=True,fmt='g');\n    plt.show()\n","efe04a7c":"from sklearn.linear_model import LogisticRegression\n\nmodel_report_parkinson=model_report_parkinson\n\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(x_train,y_train)\n\nthreshold=0.6\nthres_predict_train=model.predict_proba(x_train)[:,1] > threshold # Threshold changes the confusion matrix\nthres_predict_test=model.predict_proba(x_test)[:,1] > threshold \n\nreport_parkinson,report_healthy =  Generate_Classification_Report('Logistic',y_train,thres_predict_train,y_test,thres_predict_test)\n\nmodel_report_parkinson.loc[0]=report_parkinson\nmodel_report_healthy.loc[0]=report_healthy\n\nDisplay_Confusion_Matrix (y_test,thres_predict_test)\n\nmodel_report_parkinson.head(10)","8a3b2fe0":"from sklearn.naive_bayes import GaussianNB \n\ndata_model = GaussianNB()\n\ndata_model.fit(x_train,y_train.ravel()) #ravel convert to an one dimensional array\n\nnaive_predict_train = data_model.predict(x_train)\nnaive_predict_test = data_model.predict(x_test)\n                                            \nreport_parkinson,report_healthy =  Generate_Classification_Report('NaiveBayes',y_train,naive_predict_train,y_test,naive_predict_test)\n\nmodel_report_parkinson.loc[1]=report_parkinson\nmodel_report_healthy.loc[1]=report_healthy\n\nDisplay_Confusion_Matrix (y_test,naive_predict_test)\nmodel_report_parkinson.head()","f6185d35":"from sklearn.neighbors import KNeighborsClassifier\n\n\nKNN_model = KNeighborsClassifier(n_neighbors=20,weights='distance',metric='euclidean') # \n\nKNN_model.fit(x_train,y_train)\n\nKNN_predict_train = KNN_model.predict(x_train)\nKNN_predict_test = KNN_model.predict(x_test)\n\nreport_parkinson,report_healthy =  Generate_Classification_Report('KNN',y_train,KNN_predict_train,y_test,KNN_predict_test)\n\nmodel_report_parkinson.loc[2]=report_parkinson\nmodel_report_healthy.loc[2]=report_healthy\n\nDisplay_Confusion_Matrix (y_test,KNN_predict_test)\nmodel_report_parkinson.head(10)","15779348":"from sklearn import svm\n\nsvm_model = svm.SVC(gamma=10,C=4)\nsvm_model.fit(x_train,y_train)\n\nsvm_predict_train = svm_model.predict(x_train)\nsvm_predict_test = svm_model.predict(x_test)\n\nreport_parkinson,report_healthy =  Generate_Classification_Report('SVM',y_train,svm_predict_train,y_test,svm_predict_test)\n\nmodel_report_parkinson.loc[3]=report_parkinson\nmodel_report_healthy.loc[3]=report_healthy\n\nDisplay_Confusion_Matrix(y_test,svm_predict_test)\n\nmodel_report_parkinson.head(10)","c00b7a72":"from sklearn.ensemble import RandomForestClassifier\n\n\nforest_model = RandomForestClassifier(n_estimators=50, min_samples_leaf=int(len(x_train)*0.1),min_samples_split=int(len(x_train)*.3))\nforest_model.fit(x_train, y_train)\n\nforest_predict_train=forest_model.predict(x_train)\nforest_predict_test=forest_model.predict(x_test)\n\nreport_parkinson,report_healthy =  Generate_Classification_Report('RandomForest',y_train,forest_predict_train,y_test,forest_predict_test)\n\nmodel_report_parkinson.loc[4]=report_parkinson\nmodel_report_healthy.loc[4]=report_healthy\n\nDisplay_Confusion_Matrix(y_test,forest_predict_test)\nmodel_report_parkinson.head(10)","34f20811":"from sklearn.ensemble import BaggingClassifier\n\n#0.7\n\nbagging_model = BaggingClassifier(n_estimators=100, max_samples=0.7, bootstrap=True, oob_score=True,random_state=213);\nbagging_model.fit(x_train, y_train)\n\nbagging_predict_train=bagging_model.predict(x_train)\nbagging_predict_test=bagging_model.predict(x_test)\n\nreport_parkinson,report_healthy =  Generate_Classification_Report('Bagging',y_train,bagging_predict_train,y_test,bagging_predict_test)\n\nmodel_report_parkinson.loc[5]=report_parkinson\nmodel_report_healthy.loc[5]=report_healthy\n\nDisplay_Confusion_Matrix(y_test,bagging_predict_test)\nmodel_report_parkinson.head(10)","61c0069b":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradBoost_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.2, random_state=230);\ngradBoost_model.fit(x_train, y_train)\n\ngradBoost_predict_train=bagging_model.predict(x_train)\ngradBoost_predict_test=bagging_model.predict(x_test)\n\nreport_parkinson,report_healthy =  Generate_Classification_Report('GradientBoost',y_train,gradBoost_predict_train,y_test,gradBoost_predict_test)\n\nmodel_report_parkinson.loc[6]=report_parkinson\nmodel_report_healthy.loc[6]=report_healthy\n\nDisplay_Confusion_Matrix(y_test,gradBoost_predict_test)\nmodel_report_parkinson.head(10)","22aa8b96":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC\n\nestimators = [('rf',RandomForestClassifier(n_estimators=100,random_state=120)),\n              ('svc',make_pipeline(\n                                    preprocessing.StandardScaler(),\n                                    SVC(random_state=120)\n                                  ))]\n\nstacking_model=StackingClassifier(estimators=estimators,\n                                  final_estimator=DecisionTreeClassifier())\nstacking_model.fit(x_train, y_train)\n\n\nstacking_predict_train=stacking_model.predict(x_train)\nstacking_predict_test=stacking_model.predict(x_test)\n\nreport_parkinson,report_healthy =  Generate_Classification_Report('Stacking',y_train,stacking_predict_train,y_test,stacking_predict_test)\n\nmodel_report_parkinson.loc[7]=report_parkinson\nmodel_report_healthy.loc[7]=report_healthy\n\nDisplay_Confusion_Matrix(y_test,stacking_predict_test)\nmodel_report_parkinson.head(10)\n","8acb33d4":"# displaying the recorded score in the dataframe\n\nprint(f\"\\nThe Different measurements for the Parkinsons Detected\")\nmodel_report_parkinson.head(10)","5026b34c":"# displaying the recorded score in the dataframe\n\nprint(f\"\\nThe Different measurements for healthy\")\nmodel_report_healthy.head(10)","bf43dbec":"def Display_Barplot (title,x_axis, y_axis, data,palette, axis):\n    \n    chart=sn.barplot(y=y_axis,x=x_axis,data=data,palette=palette,ax=axis)\n    for patch in chart.patches:\n        chart.annotate(format(patch.get_height(), '.4f'), \n                       (patch.get_x() + patch.get_width() \/ 2., patch.get_height()), \n                       ha = 'center', va = 'center', \n                       xytext = (0, 9), textcoords = 'offset points')\n    chart.set_title(title)","31fb8cf8":"#sn.factorplot(x='train_accuracy', y='test_accuracy', hue='model_name', data=model_report_parkinson, kind='bar')\n\n","ce2ba910":"print(\"\\nVisualisation of various score values for Parkinsons Detected\")\n\nfig, axis = plt.subplots(3, 2, figsize=(25, 23), sharex=False)\n\nsn.set(font_scale=1.2)\n\nDisplay_Barplot('Train Accuracy','model_name','train_accuracy',model_report_parkinson,\"rocket\",axis[0,0])\nDisplay_Barplot('Test Accuracy','model_name','test_accuracy',model_report_parkinson,\"rocket\",axis[0,1])\n\nDisplay_Barplot('Precision','model_name','precision',model_report_parkinson,\"Spectral\",axis[1,0])\nDisplay_Barplot('Recall','model_name','recall',model_report_parkinson,\"Spectral\",axis[1,1])\n\nDisplay_Barplot('F1-Score','model_name','f1-score',model_report_parkinson,\"icefire\",axis[2,0])\nDisplay_Barplot('AUC Score','model_name','auc_score',model_report_parkinson,\"icefire\",axis[2,1])\n\nplt.show()\n\n\nsn.set(font_scale=1.0)","71226db7":"print(\"\\nVisualisation of various score values for Healthy\")\n\nfig, axis = plt.subplots(2, 2, figsize=(25, 15), sharex=False)\n\nsn.set(font_scale=1.2)\n\n#Display_Barplot('Train Accuracy','model_name','train_accuracy',model_report_healthy,\"Spectral\",axis[0,0])\n#Display_Barplot('Test Accuracy','model_name','test_accuracy',model_report_healthy,\"Spectral\",axis[0,1])\n\nDisplay_Barplot('Precision','model_name','precision',model_report_healthy,'cubehelix',axis[0,0])\nDisplay_Barplot('Recall','model_name','recall',model_report_healthy,'cubehelix',axis[0,1])\n\nDisplay_Barplot('F1-Score','model_name','f1-score',model_report_healthy,'viridis',axis[1,0])\nDisplay_Barplot('AUC Score','model_name','auc_score',model_report_healthy,'viridis',axis[1,1])\n\nplt.show()\n\n\nsn.set(font_scale=1.0)","e9fc13c1":"### Summary of the categorical variable\n\n- 'MDVP:Jitter(Abs)' has only 19 unique values could have a behaviour of the cataegorical variable, leaving this variable unchanged\n- Unique values gives more clarity on the categorical values\n- Target value looks unbalanced on the count ","1d69fc5c":"### Summary of pair plot for the first set\n\n- The Average vocal fundamental freq showing some positive corelation with the low frequency\n- The high and low does not show any correlation\n- Higher the value healthier the people, some people with low value has posibility. Could be other parameter involved in that case\n","e67bfdd2":"## Preparing the data for the analysis\n\n- Making the values to the common units\n- Remove the 'name' attribute as it may not needed for the model training","30eef7f7":"### Summary of Outliers Analysis\n\n- The number of outliers on these attributes looks to be ok. Does not impact the data\n- The outliers will not affect the algorithm have choosen, will be leaving it as it is\n- Visual representation gives details on the way outliers are scattered\n- The 'NHRP' clearly shows the values of outliers in different segments, this info will help to decide on treating the outliers","5a38f7bd":"### Categorical Variable Analaysis\n\n- Identify categorical by listing the unique values of the column\n- Visualising the categorical values for the value distribution","b4dc96ba":"### Summary of the selected variable analysis\n\n- Swarm plot helps to magnify the target attribute spread on the some of selected variables analysed in the pairplot\n- Violin plot helped to show the spread and distribution of the values, ex: MDVP:RAP & NHR. Healther people are more in lowest range\n- DFA values shows the healthy vs parkinsons detected are evenly across in range of values\n- MDVP:Flo(Hz) show the values in lower and higher in the central area not much healthier people, looks to be strange\n","d00ee818":"## Naive Bayes - Classification Model","a11e130a":"## Dataset Analysis\n\n- Loading the data set along with the columns\n- Analysing the various elements of the data","ac9c1e3a":"### Summary of pair plot for the third set\n\n- The measures of the variation in amplitude showing postivie corelation in various attributes\n- The healthy people in this category is very less, need a magnification of specific point might help. Swarm or violin plot is needed\n","3f3c4f2a":"## Logistic Regression - Classification Model ","72639086":"### Summary of Corelation variable analysis\n\n- The corelation between the variables shown through the heat map, set threshold so that can view the specific column in detailed\n- The attribute seems to be related only on the specific sets like fundamental frequency and fundamental amplitude\n- Used a higher value for thresold since there are many co-related variables\n\n\nSpliting the pair plots so that could be done on the specific sets as the number of columns are high","35e08b2b":"## KNN Classifier - Classification Model","1762d539":"## Splitting the data to training and testing\n\n- Normal approach to split it into 70:30, the same will be following","9cb52280":"### Summary for the attributes\n\n- 'spread2' 'D2' 'PPE' 'spread1' 'HNR' 'RPDE' looks better normalised\n- 'DFA' & 'HNR' are skewed to left a little \n- Attributes 'MDVP:APQ' 'Shimmer:DDA' 'NHR' are very well right skewed","95a389c2":"### Summary of pair plot for the fourth set\n\n- Corelation is showing negative for the HNR attribute\n- The lower value of the NHR and D2 have more healthier people, on the other hand HNR value needs to be high for healthier people\n- But the RPDE value is in the cnetral range for the healthier people\n\n","88f3fb0f":"### Summary for the attributes\n\n- 'MDVP:Fo(Hz)' looks better normalised and little skew on the right\n- All the other attributed are very well right skewed","5a741124":"### Summary of pair plot for the last set\n- The nonlinear mesasure spread 1 showing positive correlation with the PPE and spread2\n- More healthier people are in the lower central range\n- Lower the spread, probably healthier the people are \n","e943f45b":"### Summary of the Five point summary\n\n- Most of the data is skewed to the right, the slight change in the mean median will be shown as skwed. ex: spread1, spread2, D2, PPE show very less difference for the mean vs median\n- The NHR & Jitter:DDP are extermely skewed to the right, visualing it will give more clarity\n- Mode is not an interesting factor in this data set as there is not much categorical data \n- The max and min value difference is very less for the attributes RPDE, PFA","951d6888":"## Support vector machine - classification model","d8011ad1":"## Result score from various models\n\n- Visualising the scores for different models through barplot","dafa142b":"### Summary of pair plot for the second set\n\n- The measures of the variation in fundamental frequency showing postivie corelation in various attributes\n- MDVP:Jitter(Abs) is very strange with other attributes, it is more like a categorical\n- The healthy people in this category is very less, need a magnification of specific point might help. Swarm or violin plot is needed\n","5e6ac1b7":"## Univariate Analysis\n\n- Five point summary analysis\n- Data skew and the data distribution analysis\n- Central tendency, spread and tail analysis","a4b3043d":"## Bivariate and Multivariate analysis","4d6510f6":"## Stacking Classifier - Meta Classifer","b22a8b8d":"   ## Bagging Classifier ","0c49d70d":"### Summary of the Dataset Analysis\n\n- Large number of attributes in the dataset, total 24 attributes\n- Almost all the attribute looks to be  linear, status attribute is alone categorical. Needs more exploration identify if any more\n- Data types for each attributes looks fine does not need any conversion\n- Name column is not relevant to our analysis would need to be dropped before splitting the data for training\n- No null values in the dataset\n- **'Status'** is the Target variable, will do detailed analysis on that attribute later\n\n#### Complexties in the data set\n\n- Values of the attributes are in different units, hence needs to be converted to the common type by preprocessors\n- 'spread1' is in negative value, it will anyway not going to impact the algorithm choosen\n- The quantity of the data in the datasets is low (195)\n- The data is not very well balanced, it would impact the model performance","e2c4d82d":"## Summary of the Analysis\n\n- Measuring the test and train accuracy - KNN, SVM & Random Forest, All these algorithm is showing very minimal differece bettween the test and training data\n- Measuring the precision of the parkinsons prediction - Naive Bayes, SVM, Stacking\n- Measuring the recall of the parkinsons prediction - KNN & SVM\n- Measuring the F1-Score of the parkinsons prediction - SVM\n- Measuring the AUC Score of the parkinsons prediction - SVM\n\n\nDeciding the model by going through the various parameters, weightage would be more applied on the F1-Score. **Support Vector Machine** ","19bc98e2":"# Parkinsons Disease data Analysis","789b73c7":"## Ensemble Model\n\n### Random Forest Classifier","3ce334cf":"## Gadient Boost Classifier","d5e62132":"### Summary on Target Variable\n\n- The healthy and the unhealthy data is not fully balanced\n- The model has to consider the sensitivity, recall, precession score along with accuracy for the best fit, there is possibility of prediction to tend towards the "}}