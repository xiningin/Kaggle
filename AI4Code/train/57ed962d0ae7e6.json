{"cell_type":{"ae22d5f0":"code","65e0f6b9":"code","f88c2d6b":"code","b63448b7":"code","51cf330c":"code","70fed7d5":"code","99e10e9d":"code","2314205d":"code","b5191525":"code","5cfcf5c4":"code","edc825a3":"code","7ee5fba4":"code","65d248ba":"code","b129f3ff":"code","07a6413d":"code","9731e8c3":"code","0d9d7fbb":"code","0c63fc4f":"code","0093578f":"code","6e6ca7d6":"code","05db9516":"code","b68ca222":"code","8c6bba8b":"markdown","1746dfbd":"markdown","efb24723":"markdown","843772b7":"markdown","d1414f83":"markdown","76178b94":"markdown","216ab96d":"markdown","8b7d15cf":"markdown","61e429d0":"markdown","665972ad":"markdown","64631c50":"markdown","9197b3b1":"markdown","9d8253e1":"markdown","14cac7c3":"markdown","74d58786":"markdown","d479e01d":"markdown","09cb43c3":"markdown","44edd83c":"markdown","f38addd2":"markdown","b2821700":"markdown"},"source":{"ae22d5f0":"import numpy as np # linear algebra \n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.preprocessing import MinMaxScaler #importing the data scaling tool MinMaxScaler\n\nfrom scipy import linalg # linear algebra calculations and matrix inversion included\n\n%matplotlib inline \n\nimport matplotlib.pyplot as plt #for detailed visualization\n\nimport seaborn as sns #for fast and flexable visualization","65e0f6b9":"#read the csv file\ndf = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n\n# select columns to use\ndf = df[['1stFlrSF',\"SalePrice\"]] \n\n#to show the first 5 rows\ndf.head()","f88c2d6b":"#information about the data (null values , datatype)\ndf.info()","b63448b7":"sns.set_style(\"darkgrid\") #set style to make it look better\n\nplt.plot(df[\"1stFlrSF\"], df[\"SalePrice\"],'ro') \n#graph of size column on x axis and price column on y axis\n\nplt.xlabel('First Floor square feet') #x axis label\nplt.ylabel('Sale Price')  #y axis label\n\nplt.title(\"First Floor square feet and price scatter plot relation\"); #title of graph","51cf330c":"sns.kdeplot(df[\"1stFlrSF\"])\nplt.title(\"distribution of the First Floor square feet\");","70fed7d5":"#select the independent columns\nX = df[\"1stFlrSF\"].values # cahnge it from pd.series to numpy array\nX = X.reshape((X.shape[0],1)) # change the shape from (1460,None) to (1460,1) to prevent shape errors\n\n# select the dependant column\nY = df[\"SalePrice\"].values # change it from pd.series to numpy array\nY = Y.reshape((Y.shape[0],1)) #  # change the shape from (1460,None) to (1460,1) to prevent shape errors\n\n#number of training examples\nm = X.shape[0] \n\nprint(\"independant data shape : \",X.shape)\nprint(\"dependant data shape : \",Y.shape)\nprint(\"number of training examples : \",m)","99e10e9d":"#initializing MinMaxScaler to an object\nscale = MinMaxScaler()\n\nX_scaled = scale.fit_transform(X)","2314205d":"fig, axs = plt.subplots(2,figsize=(10,6)) \nsns.kdeplot(x=X_scaled[:,0],ax=axs[0])\naxs[0].set_title(\"Scaled Data distribution\")\n\nsns.kdeplot(x=X[:,0],ax=axs[1])\naxs[1].set_title(\"Original Data distribution\");","b5191525":"def hypothesis(X, Theta): \n    #hypothesis function\n    X = np.append(np.ones((X.shape[0],1)), X.reshape((X.shape[0],1)), axis = 1) # (97, 2) append ones to identify x[0]\n    Theta = Theta.reshape(2,1)  # (2, 1) two thetas for x[0] and for x[1]\n    return X.dot(Theta)  # (1460, 2) dot (2, 1) = (1460, 1)","5cfcf5c4":"def cost_function(X, Theta, Y):\n    return (1\/(2*m)) * np.sum( np.square(Y - hypothesis(X, Theta)) )","edc825a3":"def gradient_descent(X, Y, Theta, alpha, iterations):\n    history=[]\n    for i in range(iterations):\n\n        Theta = Theta - ( (alpha\/m) * (X.T).dot(hypothesis(X, Theta) - Y) )\n        Cost = cost_function(X, Theta, Y)\n        history.append(Cost)\n    return Theta,history","7ee5fba4":"Theta,history = gradient_descent(X_scaled, Y, Theta = np.array([77756,542573]).reshape((2,1)), alpha = 0.01,iterations= 1000)\n\nP = hypothesis(X_scaled, Theta)\n\nCost_linear = cost_function(X_scaled, Theta, Y)","65d248ba":"print(\"the trained weights : \\n\",Theta)","b129f3ff":"plt.figure(figsize=(10,6))\nplt.plot(X_scaled, Y, 'ko') #scatterplot of the data\nplt.plot(X_scaled, P, '-') #our model prediction\n\nplt.title(\"our prediction visualization\")\nplt.xlabel('First Floor square feet')\nplt.ylabel('Sale Price');","07a6413d":"plt.figure(figsize=(10,6))\nsns.lineplot(x=range(0,1000),y=history)\n\nplt.title(\"cost function after each iteration\")\nplt.xlabel('number of iterations')\nplt.ylabel('cost function value');","9731e8c3":"print(\"the value of cost function after each iteration : \\n\",history[:10])\nprint(\"\\nfinal cost function of the linear regression module : \\n\",Cost_linear)","0d9d7fbb":"def hypothesis(X, Theta): \n    #hypothesis function\n    X = np.append(np.ones((X.shape[0],1)), X.reshape((X.shape[0],1)), axis = 1) # (97, 2) append ones to identify x[0]\n    Theta = Theta.reshape(2,1)  # (2, 1) two thetas for x[0] and for x[1]\n    return X.dot(Theta)  # (97, 2) dot (2, 1) = (97, 1)","0c63fc4f":"def normal_equation(X,Y):\n    X = np.append(np.ones((X.shape[0],1)), X.reshape((X.shape[0],1)), axis = 1)\n    return ( linalg.inv((X.T).dot(X)).dot((X.T).dot(Y)) )","0093578f":"def cost_function(X, Theta, Y):\n    return (1\/(2*m)) * np.sum( np.square(Y - hypothesis(X, Theta)) )","6e6ca7d6":"Theta = normal_equation(X_scaled,Y).reshape(2,1)\nP = hypothesis(X_scaled, Theta)\nCost_normal = cost_function(X_scaled, Theta, Y)\n\nprint(\"the trained weights : \\n\",Theta,\"\\n\")\nprint(\"the final cost value of the normal equation : \\n\",Cost_normal)","05db9516":"plt.figure(figsize=(10,6))\nplt.plot(X, Y, 'ko') #scatterplot of the data\nplt.plot(X, P, '-') #our model prediction\n\nplt.title(\"our prediction visualization\")\nplt.xlabel('First Floor square feet')\nplt.ylabel('Sale Price');","b68ca222":"print(\"normal function cost : \",Cost_normal)\nprint(\"\\nlinear regression cost : \",Cost_linear)","8c6bba8b":"### hypothesis function :\n#### function used to predict\nin linear model it is used to predict by making line pass through the points\n\n### $$\\hat{Y} = \\hat{\\theta}_{0} + \\sum \\limits _{i=1} ^ {m} X_{i}\\hat{\\theta}_{i}$$","1746dfbd":"## Importing the tools needed ","efb24723":"## find relations","843772b7":"### visualization ","d1414f83":"#### Normal function equation\n\n$$ (X^T . X)^{-1} . (X^T.Y)$$","76178b94":"### Cost function (mean squared):\n#### why we need cost function?\nas the learning process is to predict by random weights, then calculate your error and then fix the weights.\n<br\/>\nso cost function calculates the difference in value between the predicted and the true target value.\n\n### $$J(\\theta_0,\\theta_1) = \\frac{1}{2m}\\sum\\limits _{i=1}^{m}(\\hat{Y}-Y^{i})^2 $$\n\n![1_ool361dWI61RMDyUAmalmA](https:\/\/user-images.githubusercontent.com\/59618586\/114189551-99afd880-994a-11eb-802b-f131a9b0e4fe.png)","216ab96d":"### Lets see how our model did visually on data","8b7d15cf":"## Scaling (Min Max Scaler):\n\nMinMaxScaler take the same data distribution and scale it between 0 and 1\n\n### why we need scaling :\n* to prevent very large numbers to appear at the gradient descent process and miss direct the learning weight\n\n### MinMaxScaler formula :\n### $$ X_{scaled} = \\frac{X_{i} - X_{min}}{X_{max} - X_{min}}$$\nwhich $x_i$ is a training example in the dataset","61e429d0":"#### Cost function (mean squared):\n\n$$J(\\theta_0,\\theta_1) = \\frac{1}{2m} \\sum \\limits _{i=1}^{m}(\\hat{Y}-Y^{i})^2 $$","665972ad":"### Gradient descent optimizer\n#### first of all what is optimization function ? :\noptimization function minimize the cost function by changing the weights (theta) after each iteration and gradient descent is the easiest one of them , dont worry it may at first for some people seem overwealming but you will gain intiuation after you see this gif and it will be clear for you.\n\n### $$ \\theta = \\theta - \\frac{\\alpha}{m} \\sum \\limits _{i=1}^{m}(\\hat{Y}-Y^{i}).X^i$$\n\n##### -> theta is the learning weight which will change after each iteration\n##### -> alpha is the learning step which the point takes to move (prefered not to be too large or too small range(0.0001 to 0.1))\n##### -> m is the number of training examples\n##### -> after summation part is the drivative of the cost funtion which will calulate the cost of every point , questions most of people ask is why we didnt use cost function itself and use the drivative.the answer is to calculate the point tangent to know weather it is a negative tangent to increase theta or positive tangent to decrease theta also the vealue of the tangent affects the alpha which by getting close to the minimum the step of the point decreases.\n##### -> summition is to sum up all the points cost functions to make one number for full estimation\n<br\/>\n\n![GIF1](https:\/\/user-images.githubusercontent.com\/59618586\/116012670-37362800-a62c-11eb-87b1-c2b612061253.gif)\n","64631c50":"### hypothesis function :\n\n$$\\hat{Y} = \\hat{\\theta}_{0} + \\sum \\limits _{i=1} ^ {m} X_{i}\\hat{\\theta}_{i}$$","9197b3b1":"the distribution of the data is skewed to the right which most of the points are in range 500 to 2000Cost_normal","9d8253e1":"we see slightly a decrease in the cost function due to the effect of the tangent in the (cost\/theta) convex function on alpha the Learning rate which large at the beginning and then decreased by decreasing the slope","14cac7c3":"### In this notebook we will discuss :\n* scaling:\n    * why we need scaling?\n    * MinMaxScaler formula\n    * visalization\n    \n* linear regregression:\n    * hypothesis function formula\n    \n* cost function:\n    * cost function formula\n    * why we need cost function?\n \n* gradient descent:\n    * what is the optimization function?\n    * gradiant descent formula\n    * visualization\n    \n* Bonus:\n    * Normal function\n    * comparing normal function to the gradient descent\n    * pros and cons for the normal function\n    * visualization","74d58786":"we see it is more accurate than the linear regression module and less cost function also.","d479e01d":"#### ------------------------------------------------ Please don't forget to upvote if you found this notebook helpful ----------------------------------------------------","09cb43c3":"## show the Data","44edd83c":"we see strong posititve correlation that will make it easier for out model to fit good to it","f38addd2":"### normal equation\nsame functionality as gradient descent to get the right learning weights.\n\npros :\n\n* no hyperparameters\n* no need for loops and iterations (less time)\n* more accurate\n\ncons:\n\n* can't work with more than 10k training examples","b2821700":"## Bonus"}}