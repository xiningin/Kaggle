{"cell_type":{"1ba3feae":"code","74c46944":"code","5f29fa69":"code","8cfb1c96":"code","75899e2e":"code","2f3b0b10":"code","6a0d419a":"code","b694ad09":"code","03ba0ba8":"code","ac7c73cf":"code","73eff4be":"code","ba5166cf":"code","37533d0a":"code","e5b2f5e9":"code","0293cabc":"code","fab1ad5e":"code","05b59e21":"code","46ef115c":"code","a6cd5191":"code","57966ca1":"code","602ee390":"code","4caeec06":"code","085f6444":"code","60af0edc":"code","f9d4ec5d":"markdown","6466a71c":"markdown","cf50f91c":"markdown","79db28cb":"markdown","8484dd05":"markdown","b1adbad7":"markdown","0c1fd624":"markdown","8e15c8ac":"markdown","69f5a7dc":"markdown","059ba4f1":"markdown","bca515f4":"markdown","98e59039":"markdown"},"source":{"1ba3feae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","74c46944":"import tensorflow as tf\nimport matplotlib.pyplot as plt","5f29fa69":"train_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\nprint(train_data.shape, test_data.shape)\n","8cfb1c96":"X = np.array(train_data.drop(\"label\", axis=1)).astype('float32')\ny = np.array(train_data['label']).astype('float32')","75899e2e":"plt.figure()\nplt.imshow(X[0].reshape(28, 28))\nplt.colorbar()\nplt.grid(False)","2f3b0b10":"plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X[i].reshape(28, 28), cmap=plt.cm.binary)\n    plt.xlabel(y[i])\nplt.show()","6a0d419a":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\n\nX = X \/ 255.0\nX = X.reshape(-1, 28, 28, 1)\ny = to_categorical(y)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2) #To create validation data = 20% of total training data\n\nX_train.shape, X_val.shape","b694ad09":"X_test = np.array(test_data).astype('float32')\nX_test = X_test \/ 255.0\nX_test = X_test.reshape(-1, 28, 28, 1)\n\nX_test.shape","03ba0ba8":"from tensorflow.keras import callbacks\n\nlr_schedule = callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10 ** (epoch\/10))","ac7c73cf":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.MaxPool2D(2, 2),\n    \n    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.MaxPool2D(2, 2),\n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256, activation=\"relu\"),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(512, activation=\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])","73eff4be":"model.summary()","ba5166cf":"from tensorflow.keras.optimizers import Adam\n\nmodel.compile(optimizer=Adam(learning_rate=1e-8), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=525, validation_data=(X_val, y_val), callbacks=[lr_schedule])","37533d0a":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.axis([10, 60, 0.98, 1])\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","e5b2f5e9":"lrs = 1e-8 * (10 ** (np.arange(50)\/10))\nplt.semilogx(lrs, loss)\nplt.axis([1e-8, 1e-3, 0, 0.01])","0293cabc":"model.compile(optimizer=Adam(learning_rate=1e-4), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=525, validation_data=(X_val, y_val))","fab1ad5e":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.axis([10, 60, 0.98, 1])\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","05b59e21":"model.evaluate(X_val, y_val)","46ef115c":"predictions = model.predict_classes(X_val)","a6cd5191":"y_true = np.argmax(y_val, axis=1)\n\ncorrect = np.nonzero(predictions==y_true)[0]\nincorrect = np.nonzero(predictions!=y_true)[0]\n\nprint(incorrect.shape)","57966ca1":"plt.figure(figsize=(10, 10))\nfor i, incorrect in enumerate(incorrect[0:25]):\n    plt.subplot(5,5,i+1)\n    plt.imshow(X_val[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\"Predicted {}, Class {}\".format(predictions[incorrect], y_true[incorrect]))\n    plt.tight_layout()","602ee390":"model.compile(optimizer=Adam(learning_rate=1e-4), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X, y, epochs=50, batch_size=525)","4caeec06":"final_predictions = model.predict_classes(X_test)\n\nsubmit = pd.DataFrame(final_predictions,columns=[\"Label\"])\nsubmit[\"ImageId\"] = pd.Series(range(1,(len(final_predictions)+1)))","085f6444":"submission = submit[[\"ImageId\",\"Label\"]]\nsubmission.shape","60af0edc":"submission.to_csv(\"submission.csv\",index=False)","f9d4ec5d":"<h2>1. Importing data<\/h2>\nwe use pandas library to read csv file","6466a71c":"<h2> 6. Submission<\/h2>","cf50f91c":"We will increase the learning rate on every epoch, using callback","79db28cb":"After looking at this graph, we will set learning rate = 1e-4","8484dd05":"Throughout the project, we will be working with numpy arrays thus we need to convert our data to numpy array","b1adbad7":"<h2>4. Evaluating the model<\/h2>","0c1fd624":"The values are between 0 to 255, we will normalise the data by dividing each value by 255. We do this because neural network prefer working with small range of value(0-1) tensors.","8e15c8ac":"From a sample of 42000 images, we got 55 wrong.","69f5a7dc":"<h2>2. Data preprocessing<\/h2>","059ba4f1":"These pictures could fool a human also.","bca515f4":"<h2>5. Classification Report<\/h2>","98e59039":"<h2>3. Model creation<\/h2>"}}