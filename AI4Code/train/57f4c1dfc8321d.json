{"cell_type":{"0eca02c2":"code","e2c9200a":"code","c5ede2e1":"code","3e7d2f19":"code","f20266cb":"code","4128cabe":"code","49d58bc3":"code","9de4ad83":"code","44abf04d":"code","7589f98c":"code","d79baf26":"code","207b6527":"code","3b131a95":"code","bc47321b":"code","171af1b8":"code","da109982":"code","ade8b15e":"code","9b1e2a41":"markdown","2da1599c":"markdown","b994ee0a":"markdown","fc9c6b88":"markdown","ca3e4949":"markdown","6428c2b2":"markdown","2eac62e2":"markdown","85523392":"markdown","1c0b6f64":"markdown","c0701610":"markdown","9830f360":"markdown","2f91aa5a":"markdown","db1d5cb7":"markdown","8ea6ef2f":"markdown","bc9a0e65":"markdown","0aa427bb":"markdown","b9e8fb3d":"markdown","066255fc":"markdown","8f9262b8":"markdown","e1fc4026":"markdown","ee4b6a87":"markdown","a2278ed9":"markdown","41f60fdd":"markdown","bf9fb4de":"markdown","310d375e":"markdown","9b0a2ed5":"markdown","83076fe7":"markdown","850c4797":"markdown"},"source":{"0eca02c2":"# Importing the required Libraries...\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\n\nplt.style.use('fivethirtyeight')","e2c9200a":"# Creating a random dataset using sklearn - shape:(200,1)\nX, y = datasets.make_regression(n_samples=200, n_features=1, noise=20)\n\ndata = pd.DataFrame(X,y)\ndata.reset_index(inplace=True)\ndata.rename(columns={'index':'x', 0:'y'}, inplace=True)\ndata.head()","c5ede2e1":"# Visualising the dataset by scatter plot \nplt.figure(figsize=(9,6))\nsns.scatterplot(x=data.x, y=data.y, s=120)\nplt.xlabel('independent (x)')\nplt.ylabel('dependent (y)')\nplt.show()","3e7d2f19":"# Getting slope and intercept using the above formula:\n# mean of x column\nx_mean = data.x.mean()\n\n# mean of y column\ny_mean = data.y.mean()\n\n# Slope \nnumerator = sum((data.x - x_mean) * (data.y - y_mean))\ndenominator = sum((data.x - x_mean) ** 2)\nslope = numerator \/ denominator\nprint('Slope (m): {}'.format(slope))\n\n# Intercept\nintercept = y_mean - (slope * x_mean)\nprint('Intercept (b): {}'.format(intercept))\n\n# Getting the best line which passes through the points... using y = mx + b\npredicted_y = (slope * data.x) + intercept ","f20266cb":"# Ploting the best fit line\nplt.figure(figsize=(9,6))\nsns.scatterplot(x=data.x, y=data.y, s=120)\nplt.plot(data.x, predicted_y, 'r-')\nplt.xlabel('independent')\nplt.ylabel('dependent')\nplt.show()","4128cabe":"# Creating a random dataset using numpy - shape:(100,1)\nx = 6 * np.random.rand(100,1) - 3\ny = 0.8 * x**2 + 0.9 * x + 2 + np.random.randn(100,1)\n\ndata = np.concatenate((x,y), axis=1)\ndata = pd.DataFrame(data)\ndata.rename(columns={0:'x', 1:'y'}, inplace=True)\ndata.head()","49d58bc3":"# Visualising the dataset by scatter plot \nplt.figure(figsize=(9,6))\nsns.scatterplot(x=data.x, y=data.y, s=120)\nplt.xlabel('independent (x)')\nplt.ylabel('dependent (y)')\nplt.show()","9de4ad83":"# Generating polynomial features.\ndef poly_transform(data, degree):\n    \n    poly_x = np.insert(data, 0, 1, axis=1)# Inserting bias column\n    \n    for i in range(2, degree+1):\n        x = data**i\n        poly_x = np.concatenate((poly_x, x), axis=1)\n        \n    return poly_x","44abf04d":"degree = 2\npoly_x = poly_transform(x, degree)\n\n# printing first five rows from polyomial applied dataset\npoly_x[:5,:]","7589f98c":"# Getting slope and intercept using the above formula\nintercept_nd_coef = np.linalg.inv(np.dot(poly_x.T, poly_x)).dot(poly_x.T).dot(y)\n\n# intercept\nprint(\"Intercept : \", intercept_nd_coef[0])\n\n# slope\nprint(\"Slope : \",intercept_nd_coef[1:].T)\n","d79baf26":"# Creating a random test data and predicting\nx_test = np.linspace(-3, 3, 100).reshape(100, 1)\nx_test_poly = poly_transform(x_test,degree)\npredicted_y = np.dot(x_test_poly,intercept_nd_coef)","207b6527":"# Ploting the best fit line\nplt.figure(figsize=(9,6))\nsns.scatterplot(x=data.x, y=data.y, s=120)\nplt.plot(x_test, predicted_y, 'r-')\nplt.xlabel('independent (x)')\nplt.ylabel('dependent (y)')\nplt.show()","3b131a95":"# Polynomial with degree 1\ndeg1 = poly_transform(x, 1)\nparameters_deg1 = np.linalg.inv(np.dot(deg1.T, deg1)).dot(deg1.T).dot(y)\nx_test = np.linspace(-3, 3, 100).reshape(100, 1)\nx_test_deg1 = poly_transform(x_test, 1)\ndeg1_y = np.dot(x_test_deg1,parameters_deg1)\n\n# Polynomial with degree 2\ndeg2 = poly_transform(x, 2)\nparameters_deg2 = np.linalg.inv(np.dot(deg2.T, deg2)).dot(deg2.T).dot(y)\nx_test = np.linspace(-3, 3, 100).reshape(100, 1)\nx_test_deg2 = poly_transform(x_test, 2)\ndeg2_y = np.dot(x_test_deg2,parameters_deg2)\n\n# Polynomial with degree 20\ndeg20 = poly_transform(x, 20)\nparameters_deg20 = np.linalg.inv(np.dot(deg20.T, deg20)).dot(deg20.T).dot(y)\nx_test = np.linspace(-3, 3, 100).reshape(100, 1)\nx_test_deg20 = poly_transform(x_test, 20)\ndeg20_y = np.dot(x_test_deg20,parameters_deg20)\n\n# Ploting those three lines to compare\nplt.figure(figsize=(20,10))\nsns.scatterplot(x=data.x, y=data.y, s=220)\nplt.xlabel('independent (x)')\nplt.ylabel('dependent (y)')\nplt.plot(x_test, deg1_y, 'g-', linewidth=2, label='degree = 1')\nplt.plot(x_test, deg2_y, 'r-', linewidth=2, label='degree = 2')\nplt.plot(x_test, deg20_y, 'k-', linewidth=2, label='degree = 20')\nplt.legend(shadow=True)\nplt.xlim(-3.2,3.2)\nplt.ylim(0,14)\nplt.show()","bc47321b":"# Creating a random dataset using numpy - shape:(10,1)\nx = 6 * np.random.rand(10,1) - 3\ny = 0.8 * x**2 + 0.9 * x + 2 + np.random.randn(10,1)\n\ndata = np.concatenate((x,y), axis=1)\ndata = pd.DataFrame(data)\ndata.rename(columns={0:'x', 1:'y'}, inplace=True)\ndata.head()","171af1b8":"# Polynomial Regression of degree 6\ndegree = 6\npoly_x = poly_transform(x, degree)\nparameters = np.linalg.inv(np.dot(poly_x.T, poly_x)).dot(poly_x.T).dot(y)\n\nx_test = np.linspace(-3, 3, 100).reshape(100, 1)\nx_test_poly = poly_transform(x_test, degree)\npoly_y = np.dot(x_test_poly,parameters)\n\n# Ploting overfitted line\nplt.figure(figsize=(9,6))\nsns.scatterplot(x=data.x, y=data.y, s=120)\nplt.xlabel('independent (x)')\nplt.ylabel('dependent (y)')\nplt.plot(x_test, poly_y, 'k-', linewidth=2, label='degree = 6')\nplt.legend(shadow=True)\nplt.ylim(0, 12)\nplt.show()","da109982":"# Getting slope and intercept using the above formula\nalpha = 0.9  # Hyperparameter\nI = np.identity(degree+1)\nintercept_nd_coef = np.linalg.inv(np.dot(poly_x.T, poly_x) + alpha*I).dot(poly_x.T).dot(y)","ade8b15e":"x_test = np.linspace(-3, 3, 100).reshape(100, 1)\nx_test_reg = poly_transform(x_test, degree)\nreg_y = np.dot(x_test_reg,intercept_nd_coef)\n\n# Ploting overfitted line and regularized line\nplt.figure(figsize=(9,6))\nsns.scatterplot(x=data.x, y=data.y, s=120)\nplt.xlabel('independent (x)')\nplt.ylabel('dependent (y)')\nplt.plot(x_test, poly_y, 'k-', linewidth=2, label='degree = 6')\nplt.plot(x_test, reg_y, 'r-', linewidth=2, label='alpha = 0.9')\nplt.legend(shadow=True)\nplt.ylim(0,12)\nplt.xlim(-3,3)\nplt.show()","9b1e2a41":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:lightblue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px; color:white; text-align:center\"><b>Simple Linear Regression Code<\/b><\/p>\n<\/div>","2da1599c":"<div class=\"alert alert-block alert-success\" style=\"margin-left: 80px; margin-right: 80px\"> <h3 style='text-align:center;font-family:cursive'> If you found this interesting please don't forget to upvote \ud83d\udc4d<\/h3>\n      \n<\/div>","b994ee0a":"# <span style=\"font-family:cursive;\">Ridge Regularization (L2)<\/span>\n\n* **Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values to be far away from the actual values.**\n\n* **As we increase the value of alpha, the magnitude of the coefficient decreases, where the values reaches to zero but not absolute zero.**\n\n> **The bias increases as \u03bb increases.<br>\n> The variance decreases as \u03bb increases.**","fc9c6b88":"#### **Creating a random polynomial dataset for regularization**","ca3e4949":"#### **Multiple Linear Regression:**\n> **Multiple linear regression is essentially similar to the simple linear model, with the exception that multiple independent variable are used in the model.**\n\n> **The mathematical representation of multiple linear regression is :** <h4 style=\"color:Black\"><b>$y = \\beta_0+\\beta_1x_1+\\beta_2x_2+ \u2026 +\\beta_nx_n$<\/b> <br><br>$y = X\\beta$ (matrix form)<\/h4>\n> **where,**\n> >  **$y$ - Dependent variable**<br>\n> >  **$x_1,x_2,x_3,\u2026,x_n$ - Independent variable**<br>\n> >  **$\\beta_1,\\beta_2,\\beta_3,\u2026,\\beta_n$ - Slope**<br>\n> >  **$\\beta_0$ - Intercept**\n","6428c2b2":"# <span style=\"font-family:cursive;\">Linear Regression<\/span>","2eac62e2":"# <span style=\"font-family:cursive;\">Regularization<\/span>\n\n**This is a form of regression that constrains, regularizes or shrinks the coefficient estimate towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting. The commonly used regularization techniques are :** \n> *  **L2 Regularization** (Ridge) \n\n> *  **L1 Regularization** (LASSO)\n\n> *  **Elastic Net** (Combination of L1 and L2)","85523392":"### ***What is Regression Analysis?***","1c0b6f64":"#### **Creating a random polynomial dataset for regression**","c0701610":"#### **Building a complex\/overfitting model to apply regularization**","9830f360":"![](https:\/\/cdn.lynda.com\/course\/578082\/578082-1554221406179-16x9.jpg)","2f91aa5a":"#### **Simple Linear Regression:**\n> **Simple linear regression is a model that assesses the relationship between a dependent variable and an independent variable.**\n\n> **The simple linear model is expressed using the following equation:** <h4 style=\"color:Black; padding:\"><b>$y = mx + b$<\/b><\/h4>\n> **where,**\n> >  **y - Dependent variable**<br>\n> >  **x - Independent variable**<br>\n> >  **m - Slope**<br>\n> >  **b - Intercept**\n\n","db1d5cb7":"<div class=\"alert alert-block alert-warning\" style=\"margin-left: 80px; margin-right: 80px\"> <h4 style='margin-left:20px'>\ud83d\udccc Note: <\/h4>\n    <div>\n         <p style=\"margin-left:80px\">\ud83d\udfe2 Underfitted model, this means that we do not have enough parameters to capture the trends in the underlying system. We have data that is parabolic in nature, but we fit this with linear function (degree = 1), with one parameter. So we end up with a high bias and poor model.<br><br>\ud83d\udd34 Optimum model, this means that the model has low bias and low variance. This model is able to capture the relationship in the data and also doesn't have complexity <br><br>\u26ab Overfitted model, this means that we have too many parameters (degree = 15) therefore it becomes a overly complex model and trys to catch every points. We have a parabolic data, but we used higher order to fit. The result is a model that has high variance. This means that the model will have good accuracy in training data but not in testing data.<\/p>\n    <\/div>    \n<\/div>","8ea6ef2f":"#### **Formula for finding Slope and Intercept :**<br>\n>### $ m = \\frac{\\sum {(x - \\bar{x}) (y - \\bar{y})}}{\\sum{(x - \\bar{x})^2}}$\n> **where,**\n> > * $\\bar{x}$ = mean of x\n> > * $\\bar{y}$ = mean of y\n\n> ### $ b = \\bar{y} - m*\\bar{x} $\n> **where,**\n> > * $\\bar{x}$ = mean of x\n> > * $\\bar{y}$ = mean of y","bc9a0e65":"#### **Formula for finding Slope and Intercept :**\n\n> <h3>$ \\beta =  (X^TX)^{-1} X^TY$<\/h3>","0aa427bb":"<div class=\"alert alert-block alert-warning\" style=\"margin-left: 80px; margin-right: 80px\"> <h4 style='margin-left:20px'>\ud83d\udccc Note: <\/h4>\n    <div>\n        <p style=\"margin-left:80px\"> The features turned from 1D to 3D because we added some polynomial degree <br> The first column consists of 1 which is bias, second x and third $x^2$. As we add more degrees the dimensions also increases <br> We can not use simple linear regression form on this 3-Dimensional data so we'll apply multiple linear regression using matrix multiplication<\/p>\n    <\/div>    \n<\/div>","b9e8fb3d":"# <center><span style=\"font-family:cursive;\">Regression Analysis \ud83d\udcc8<\/span><\/center>","066255fc":"* **<span style=\"font-family:cursive;\">Regression analysis is a set of statistical methods used for the estimation of relationships between a dependent variable and one or more independent variable. It can be utilized to assess the strength of the relationship between variables and for modeling\nthe future relationship between them.<\/span>**<br><br>\n* **<span style=\"font-family:cursive;\">Regression analysis includes several variations, such as linear, multiple linear and nonlinear. The most common models are simple linear regression and multiple linear regression. Nonlinear regression analysis is commonly used for more complicated datasets in which the dependent and independent variables show a nonlinear relationship. (I'll exclude mutliple linear regression)**<br><br>\n* **<span style=\"font-family:cursive;\">Regression analysis offers numerous applications in various disciplines, including finance.<\/span>**","8f9262b8":"![](https:\/\/cdn.corporatefinanceinstitute.com\/assets\/regression-analysis5.png)","e1fc4026":"#### **Ridge regularization formula for finding Slope and Intercept :**\n\n> <h3>$ \\beta =  (X^TX+\\lambda I)^{-1} X^TY$<\/h3>\n> where,\n\n> > $ \\lambda $ is a hyperparameter","ee4b6a87":"#### **Polynomial Regression**\n> **In simple linear regression algorithm only works when the relationship between the data is linear But suppose if we have non-linear data then Linear regression will not capable to draw a best-fit line and It fails in such conditions. Hence, we introduce polynomial regression to overcome this problem, which helps identify the curvilinear relationship between independent and dependent variables.**","a2278ed9":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:lightblue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px; color:white; text-align:center\"><b>Ridge Regression Code<\/b><\/p>\n<\/div>","41f60fdd":"# <span style=\"font-family:cursive;\">Polynomial Regression<\/span>","bf9fb4de":"# <span style=\"font-family:cursive;\">Comparing Lower and Higher degree polynomials<\/span>","310d375e":"> **Polynomial regression is a form of Linear regression where only due to the Non-linear relationship between dependent and independent variables we add some polynomial terms to linear regression to convert it into Polynomial regression.**\n\n> **$$y =  \\beta_0 + \\beta_1x_1 + \\beta_2x_1^2 + \u2026 + \\beta_nx_1^n$$**","9b0a2ed5":"<div class=\"alert alert-block alert-warning\" style=\"margin-left: 80px; margin-right: 80px\"> <h5 style='text-align:center'>We reduced the complexity of the model using regularization technique (ridge regression)<\/h5>\n      \n<\/div>","83076fe7":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:lightblue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px; color:white; text-align:center\"><b>Polynomial Regression Code<\/b><\/p>\n<\/div>","850c4797":"#### **Creating a random linear dataset for regression**"}}