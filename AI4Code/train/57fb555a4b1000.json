{"cell_type":{"437ac41c":"code","5a1d2916":"code","9382fa65":"code","c72a74b6":"code","12e926b8":"code","823d5463":"code","c3a95f25":"code","49faf2d0":"code","5c4d3ad8":"code","11ea9066":"code","37bf0591":"code","188c377e":"code","4ec17e64":"code","793d0c5a":"code","f6b54c47":"code","cc2db3a8":"code","446967f2":"code","33cb6053":"code","e7c7322c":"code","77b5ea37":"code","f5a7fc4c":"code","0f331706":"code","6bb90282":"code","801e89fc":"code","0b8a04e8":"code","ebb5691d":"code","a569cab0":"code","a415882b":"code","9952dd67":"code","c06cdcd6":"code","4e0acbc9":"code","daea9dfd":"code","5e58f43a":"code","a812e666":"code","9111953d":"markdown","3b966dc6":"markdown","e60aa668":"markdown","3473c673":"markdown","ccbd3283":"markdown","daf307e7":"markdown","c3d2fb55":"markdown","8fc51907":"markdown","e999bb35":"markdown","f5a8105e":"markdown"},"source":{"437ac41c":"%matplotlib inline\n\n# importando bibliotecas necess\u00e1rias\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport matplotlib.pyplot as plt\nimport sklearn as sk\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nplt.style.use('seaborn')\nimport warnings\n\n","5a1d2916":"#Importing data and substituting non number characters \ndata = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\",sep=r'\\s*,\\s*',engine='python',na_values=\"?\")\ndata_test = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\",sep=r'\\s*,\\s*',engine='python',na_values=\"?\")\ndata \n \n","9382fa65":"data.head(5)","c72a74b6":"#Here the code gets how many values are null on the collumns \nnulo = data.isna().sum().sum()\nprint(\" Null characters equals to: \",nulo)\n","12e926b8":"#Code calculates the percentage of null values and check on their columns  \nx = (data.isna().sum().sum())*100 \/ (data.shape[0]*data.shape[1])\n\nprint(\"Null data equals to {0:.3}%\".format(x))\ndata.isna().sum()","823d5463":"Imp = SimpleImputer(strategy='most_frequent')\n\ndata[['workclass', 'occupation']]= pd.DataFrame(Imp.fit_transform(data[['workclass', 'occupation']]))","c3a95f25":"#Lines below verifies if there are either some null values or any special characters \n\nprint(\"There are {} null values on the data\".format((data.isna().sum().sum())))\nprint(\"There are {} special characters\".format(data.isin(['?']).sum().sum()))\n ","49faf2d0":"data.describe()","5c4d3ad8":"data.head()","11ea9066":"data2 = data.copy(deep=True)\n\ndata2=data.drop(columns=[\"income\"])\n\ndata2[\"Target\"] = [0 if s ==\"<=50K\" else 1 for s in data.income ]\n\ndata2.head()","37bf0591":"#Showing income for groups in wich 50k is the limit for categoricals variables\n\nls = ['workclass', 'education','marital.status', 'occupation', \n         'relationship', 'race', 'sex','native.country'\n        ]\nplt.figure(figsize = (40,75))\ni = 0\nfor v in ls:\n    i+= 1\n    plt.subplot(4,2,i)\n    sns.barplot(x = data2[v],y = data2[\"Target\"])\n    plt.xticks(rotation= 90 )\n\nplt.show()","188c377e":"pd.crosstab(data2.race, data2.Target,margins=True)","4ec17e64":"#Verifying null values in data \n\ndata_test[['workclass','occupation']] = pd.DataFrame(Imp.fit_transform(data_test[['workclass', 'occupation']]))\nx = data_test.isna().sum().sum()\nprint(\"Null values in data_test: {} values\".format(x))\n","793d0c5a":"data_test.shape","f6b54c47":"data_test[['workclass','occupation']] = pd.DataFrame(Imp.fit_transform(data_test[['workclass', 'occupation']]))","cc2db3a8":"print(\"The data test has {} empty values, after handling it\".format(data_test.isna().sum().sum()))","446967f2":"data_test.head()","33cb6053":"#Here the data shownm in the graph are distributed according its occurency on the data frame\n\nvariables = ['education.num','capital.gain', 'capital.loss', 'hours.per.week', 'age']\n\nplt.figure(figsize=(25,15))\n\ni = 0\nfor column in variables:\n    i=i+1\n    plt.subplot(3,2,i)\n    sns.distplot(x = data[column])\n    plt.xlabel(column)\nplt.show()","e7c7322c":"# On the histogram is possible to visualize the discret and continuous variables\n\n\nplt.figure(figsize=(15,10))\ndata.hist(bins=100, figsize=(20, 20))","77b5ea37":"tX1 = pd.get_dummies(data.drop(columns=[\"income\", 'native.country', 'education.num', 'Id', 'fnlwgt']))\ntX1.shape\ntY = data.income","f5a7fc4c":"N=25\nmod = KNeighborsClassifier(n_neighbors=N, metric='euclidean')\nmod.fit(tX1,tY)","0f331706":"media = cross_val_score(mod, tX1, tY, cv=10)","6bb90282":"tX1.shape","801e89fc":"print(\"KnN score is equal to {}\".format(media.mean()))","0b8a04e8":"minmx = MinMaxScaler()\ndata_2 = data.copy(deep=True)","ebb5691d":"variables2= ['capital.gain', 'capital.loss', 'hours.per.week', 'age']\n\ntX2 = minmx.fit_transform(X = [data_2[\"capital.gain\"], data_2[\"capital.loss\"], data_2[\"hours.per.week\"], data_2[\"age\"]] )\n\ndata_explore_normalized = pd.DataFrame(tX2.transpose(), columns=variables2)\ndata_explore_normalized.columns \n","a569cab0":"X2 =pd.get_dummies(data_2.drop(columns=[\"income\", \"native.country\", \"education.num\",  \"capital.gain\", \"capital.loss\", \"hours.per.week\", \"age\"]))\nX2.reset_index(inplace=True)\nX2.drop(columns=['index'], inplace=True)\nX2[['capital.gain', 'capital.loss', 'hours.per.week', 'age']] = data_explore_normalized[['capital.gain', 'capital.loss', 'hours.per.week', 'age']] \nY2 = data_2.income\n","a415882b":"N=30\nmod2 = KNeighborsClassifier(n_neighbors=N, metric='euclidean')\nmod2.fit(X2,Y2)","9952dd67":"media2 = cross_val_score(mod2, X2, Y2, cv=10)\nprint(\"KnN score is equal to {}\".format( media2.mean()))","c06cdcd6":"xfinal = pd.get_dummies(data_test.drop(columns=['native.country', 'education.num', 'Id','fnlwgt',]))\n\nShow = pd.DataFrame(mod.predict(xfinal))\n                                  \nwarnings.filterwarnings('ignore')","4e0acbc9":"xfinal.shape","daea9dfd":"Show = pd.DataFrame(mod.predict(xfinal), data_test['Id'], columns=['income'])","5e58f43a":"Show.head()","a812e666":"Show.to_csv('Submission.csv', index=True, index_label='Id')","9111953d":"## Training and handling data ","3b966dc6":"## Testing","e60aa668":"## Functional variables","3473c673":"# Libraries","ccbd3283":"## a) Utilizing KnN with N = 25 without normalization","daf307e7":"# Descriptive Analysis","c3d2fb55":"## Training models","8fc51907":"## b) Utilizing KnN with N = 30 with normalization","e999bb35":"# Explorer Analysis","f5a8105e":"## Results"}}