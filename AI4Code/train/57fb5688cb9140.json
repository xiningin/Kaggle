{"cell_type":{"dee1cfaf":"code","8bb988a3":"code","b2252d32":"code","6ac0b5d9":"code","84f6a413":"code","2585b835":"code","394439fc":"code","c0c44965":"code","9d25e142":"code","5fbf6248":"code","4ad2eb89":"code","e2624390":"code","9b64af48":"code","61dbe612":"code","cd77655c":"code","9a00833b":"code","fdde41ae":"markdown","965ed8f0":"markdown","c75019d9":"markdown","fb228aa0":"markdown","d6bb6e04":"markdown","3e856f5e":"markdown","1d2a762d":"markdown","bb85f544":"markdown","bc384cc0":"markdown","b60a1f08":"markdown","e9113aa4":"markdown","06d92506":"markdown","dd032fe9":"markdown","15be85c9":"markdown","1e07f948":"markdown","66dfa196":"markdown","b705efdc":"markdown","4833cf74":"markdown","b2814360":"markdown","f054f1f2":"markdown","e676876e":"markdown","73a5a890":"markdown","f6fbbca8":"markdown","b96c2943":"markdown","1214f397":"markdown"},"source":{"dee1cfaf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8bb988a3":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.impute import SimpleImputer\n\n%matplotlib inline","b2252d32":"# data loading\ndata = pd.read_csv('\/kaggle\/input\/pump-sensor-data\/sensor.csv')\nprint(data.shape)\ndata.head()","6ac0b5d9":"# time series plots for sensor data\ndata.plot(subplots =True, sharex = True, figsize = (20,50))","84f6a413":"# missing value percentage\nround((data.isnull().sum() * 100\/ len(data)),2).sort_values(ascending=False)","2585b835":"# sensor_50 data with machine status\nBroken = data[data['machine_status']=='BROKEN']\nRecovery = data[data['machine_status']=='RECOVERING']\n\nsns.set_context('talk')\n_ = plt.figure(figsize=(20,5))\n_ = plt.plot(Recovery['sensor_50'], linestyle='none', marker='o', color='blue', markersize=5)\n_ = plt.plot(Broken['sensor_50'], linestyle='none', marker='X', color='red', markersize=14)\n_ = plt.plot(data['sensor_50'], color='grey')\n_ = plt.title('sensor_50')\nplt.show()","394439fc":"# sensor_51 data with machine status\nsns.set_context('talk')\n_ = plt.figure(figsize=(20,5))\n_ = plt.plot(Recovery['sensor_51'], linestyle='none', marker='o', color='blue', markersize=5)\n_ = plt.plot(Broken['sensor_51'], linestyle='none', marker='X', color='red', markersize=14)\n_ = plt.plot(data['sensor_51'], color='grey')\n_ = plt.title('sensor_51')\nplt.show()","c0c44965":"# Remove relavant variables\ndf = data.drop(['sensor_15','sensor_50','sensor_51','Unnamed: 0'], axis = 1)\ndf.head()","9d25e142":"# sensor data with machine status\nSensors = df.iloc[:,1:50]\nsensorNames=Sensors.columns\nfor sensor in sensorNames:\n    sns.set_context('talk')\n    _ = plt.figure(figsize=(18,3))\n    _ = plt.plot(Recovery[sensor], linestyle='none', marker='o', color='blue', markersize=5)\n    _ = plt.plot(Broken[sensor], linestyle='none', marker='X', color='red', markersize=14)\n    _ = plt.plot(df[sensor], color='grey')\n    _ = plt.title(sensor)\n    plt.show()","5fbf6248":"# Replace missing values and normalize the dataset\nnames = df.iloc[:,1:50].columns\nx = df.iloc[:,1:50].fillna(method = 'ffill')\nscaler = preprocessing.Normalizer(norm='max')\nx = scaler.fit_transform(x.transpose())\nx = pd.DataFrame(x.transpose())\nx.columns = names\n\ny = df['machine_status']\n\nx.head()","4ad2eb89":"y.unique()","e2624390":"one_hot = pd.get_dummies(y)\none_hot.head()","9b64af48":"# feature importance scores\nselector = SelectKBest(score_func=chi2, k=10)\nfit = selector.fit(x, one_hot['NORMAL'])\nfit.scores_","61dbe612":"# select 10 important features\nselected = selector.fit_transform(x, one_hot['NORMAL'])\nmask = selector.get_support()\nfeature_names = list(x.columns.values)\n\nnew_features = []\nfor bool, feature in zip(mask, feature_names):\n    if bool:\n        new_features.append(feature)\n\nselected = x.loc[:,new_features]\nselected.head()","cd77655c":"selected.plot(subplots =True, sharex = True, figsize = (30,50))","9a00833b":"Sensors = selected\nsensorNames=Sensors.columns\nfor sensor in sensorNames:\n    sns.set_context('talk')\n    _ = plt.figure(figsize=(18,3))\n    _ = plt.plot(Recovery[sensor], linestyle='none', marker='o', color='blue', markersize=5)\n    _ = plt.plot(Broken[sensor], linestyle='none', marker='X', color='red', markersize=14)\n    _ = plt.plot(df[sensor], color='grey')\n    _ = plt.title(sensor)\n    plt.show()","fdde41ae":"This notebook discusses about the **_SelectedKBest_** approach which is from _Scikit-learn_","965ed8f0":"# EDA for sensor data","c75019d9":"Let's do a forward fill !","fb228aa0":"# SelectKBest","d6bb6e04":"There are 220320 recordings which recorded at a 1min time-samples with the data from 50 sensors","3e856f5e":"Here, the variable of interest is a categorical variable with 3 classes. _oneHotEncoder_ creates a binary column for each category.","1d2a762d":"## Imputation\nAll the other missing values should be imputed in a proper manner. There are some basic approaches that we can use to impute missing values are,  \n1. zero imputation  \n2. mean imputation  \n3. forward or backward fill imputation  \n\nAppropriate imputation method should be selected through a insight analysis between missing values and machine status.","bb85f544":"Since there is no information about a breakdown at the period of missing data, those missing status won't add any significant value to the machine status. Therefore, removing sensor_51 wouldn't make any huge affect. **(Remove sensor_51)**","bc384cc0":"Look more into sensor_50 and sensor_51 with machine status !","b60a1f08":"Allmost all the times, sensor data has gone missing when there is a breakdown or a recovery period. Therfore, mean imputation would not be a best method to impute the missing values.  \nZero imputation would be better method, since sensor data may lost when there is a breakdown. This is a good approach for the first set of sensors, as data is lost when a breakdown occurs. But that is not the case for the final sensor kit. Those data shows a higher values when there is a breakdown.  \nTherefore, forward fill or backward fill would be a best match for missing value imputaion in this kind of scenarios. ","e9113aa4":"NOW WE ARE READY TO DO THE FEATURE ENGINEERING !!!","06d92506":"Let's do a data-preprocessing to look more !","dd032fe9":"Let's plot the senseor data with machine status !","15be85c9":"This uses the hypothesis testing as follows,  \nH_0: The feature has no role to play with the response variable (The feature is not important)  \nH_1: The feature has a role to play with the response variable (The feature is important)","1e07f948":"## Handling Missing Values  \nMissing values and outliers are more sensetive when considering a predictive maintenance analysis. Those points could actualy be a breakdown or an annomaly. Therefore, insight analysis is requared when dealing with missing values as well as outliers.","66dfa196":"The Scikit-learning API provides the **_SelectKBest_** class to extract the best features from a given data set. _score_func_ parameter allow to select the features according to the highest scores. _SelectKBest_ can use for both regression and classification data.  \nSince the response variable **machine_status** is a categorical variable, we can use _chi2_ as the scoring function for _SelectKBest_.","b705efdc":"# Data Pre-Processing","4833cf74":"Let's look at the time plots of the selected features !","b2814360":"Time series plots of sensor data shows that the sensor_15 not contain any data.  \nConsidering the data pattern between some sensors, they are very similar to each other.   \nThere are signals that are very noisy and seem to follow no trend in particular. (Eg: 38 -- 44).","f054f1f2":"Sensor_50 doesn't contain any data after ~ 140000 onwards. If we are removing data points we have to loss nearly 35% of observations. Therefore, removing data is not a good solution. So, we have to remove the variable to maintain the quality of the dataset. **(Remove sensor_50)**","e676876e":"# Data Cleaning and Feature Engineering","73a5a890":"Redundant sources of information may be more important for many kinds of scenarios as well as may be a nothing but just a noise for many other kinds of scenarios. Large set of simulation and independent measurements of the same system would produce a redundant data. Feature engineering techniques have a special ability to deal with this redundant data, to avoid computational and problem tractability reasons.","f6fbbca8":"There are missing values in every column of interest.  \nSensor_15 contain 100% of missing values, so we can remove the sensor_15 without any hesitation. **(Remove sensor_15)**  \nBoth sensor_50 and sensor_51 have more than 5% of missing values.  \nOther features comprise of very low percentage of missing values.","b96c2943":"Now we have the important set of features that we can use for predictive maintenance.","1214f397":"### Selected features with machine status"}}