{"cell_type":{"81d7db9e":"code","cd6f6f9f":"code","eaa280f9":"code","ec8c5aab":"code","2e9098db":"code","3e4ab301":"code","6390685e":"code","e46c44cb":"code","770844f8":"markdown","6c42d17e":"markdown","554f38f0":"markdown","8a1f0875":"markdown","aaa86fdd":"markdown","4a48179f":"markdown","9a7fe7ad":"markdown"},"source":{"81d7db9e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd6f6f9f":"path = \"\/kaggle\/input\/lhcb-jet-data-separation\/\"\nsignalData = pd.read_csv(path+\"bjet_train.csv\") # signal has mc_flavour = 5\nbackgroundData = pd.concat([pd.read_csv(path+\"cjet_train.csv\"), \n                            pd.read_csv(path+\"ljet_train.csv\")]) # background has mc_flavour != 5\nprint(\"First of {} signal rows\".format(signalData.shape[0]))\ndisplay(signalData.iloc[0])\nprint(\"First of {} background rows\".format(backgroundData.shape[0]))\ndisplay(backgroundData.iloc[0])","eaa280f9":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplotCols = list(signalData.columns)\n\nfor i in range(len(plotCols)):\n    print(\"Plotting {}\".format(plotCols[i]))\n    plt.hist(signalData[plotCols[i]],label = \"Sig\")\n    plt.hist(backgroundData[plotCols[i]],label = \"Bkg\")\n    plt.legend()\n    plt.xlabel(plotCols[i])\n    plt.show()\n\n# Note if the plots do not display minimise then maximise the output area below (double arrow button to the top right)","ec8c5aab":"# Try fdChi2 as log10, others as linear\nlogCol = ['fdChi2']\nlinCol = ['PT', 'ETA', 'drSvrJet', 'fdrMin', \n          'm', 'mCor', 'mCorErr', 'pt', 'ptSvrJet',\n          'tau', 'ipChi2Sum', 'nTrk', 'nTrkJet'] # Note skip Id as that is not helpful\n# redefine columns as log10(col), so ranges are more similar between variables\nfor l in logCol:\n    signalData[l] = np.log10(signalData[l])\n    backgroundData[l] = np.log10(backgroundData[l])\n\nnTrainSig = signalData.shape[0]\/\/2 # half the rows for training, half for evaluation\nnTrainBkg = backgroundData.shape[0]\/\/2\n\n# first half as training\nx_data = np.concatenate([signalData[logCol+linCol][:nTrainSig].values,\n                         backgroundData[logCol+linCol][:nTrainBkg].values])\ny_data = np.concatenate([(signalData[\"mc_flavour\"][:nTrainSig]==5).values.astype(np.int),\n                         (backgroundData[\"mc_flavour\"][:nTrainBkg]==5).values.astype(np.int)])\n\n#second half as evaulation\nx_eval = np.concatenate([signalData[logCol+linCol][nTrainSig:].values,\n                         backgroundData[logCol+linCol][nTrainBkg:].values])\ny_eval = np.concatenate([(signalData[\"mc_flavour\"][nTrainSig:]==5).values.astype(np.int),\n                         (backgroundData[\"mc_flavour\"][nTrainBkg:]==5).values.astype(np.int)])","2e9098db":"# Simple 2 layer Keras network:\n# import Keras overall\nimport keras\n# a single NN layer of type \"Dense\" i.e. all inputs connected to all outputs\nfrom keras.layers import Dense\n# The input layer, takes x and starts NN processing\nfrom keras.layers import Input\n# Keras functional methods for defining a NN model\nfrom keras.models import Model\n\n# optimiser to use\nAdam = keras.optimizers.Adam() # defaults for optimiser\n\n# define a Functional keras NN model\n# define input layer\nnVal = x_data.shape[1]\ninputs = Input(shape=(nVal,)) \n# input->internal layer with nVal nodes\niLayer = Dense(nVal, activation='relu')(inputs) \n# prev layer -> output (1 node)\noutput = Dense(1, activation='sigmoid')(iLayer) \n# a model is created from connecting all the layers from input to output\nmodel = Model(inputs=inputs, outputs=output)\n# Compiling the model sets up how to optimise it\nmodel.compile(optimizer=Adam, \n              loss='binary_crossentropy', # define what is to be optimised\n              metrics=['accuracy']) # what to store at each step\n\n# run an evaluation before optimisation to see what the random initialisation\n# gave as an output\nscore = model.evaluate(x_eval, y_eval, verbose=1)\nprint('Initial loss:', score[0])\nprint('Inital accuracy:', score[1]) # note random get you to ~60% accuracy if the data are 60% true\n","3e4ab301":"# Choose a batch size \nbatchSize = 4096\n# Rather than run a fixed number of rounds, stop when the output stops improving\nfrom keras.callbacks import EarlyStopping\n# stop training early if after 10 iterations the result has not improved\nearly_stopping = EarlyStopping(monitor=\"loss\", patience=10)\n# Now run the optimisation, taking events from the generator\n# Each epoch is one pass through the data, if not stopped do 20 epochs\nhistory=model.fit(x=x_data,y=y_data,batch_size=batchSize,\n                  verbose=1,\n                  epochs=25,\n                  shuffle=True, # needed as training is all true then all false\n                  callbacks=[early_stopping])\nprint(\"Stopped after \",history.epoch[-1],\" epochs\")","6390685e":"# Now evaluate the model after training on the evaluation sample, should be better (but could be much improved still)\nscore = model.evaluate(x_eval, y_eval, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","e46c44cb":"testData = pd.read_csv(path+\"competitionData.csv\")\n# apply log10 to columns that need it\nfor l in logCol:\n    testData[l] = np.log10(testData[l])\nx_test = testData[logCol+linCol].values\n\npredMCFloat = model.predict(x_test)\n# predMCFloat is a float: need to convert to an int\npredMC = (predMCFloat>0.5).astype(np.int)\ntestData[\"Prediction1\"] = predMC\n\n# solution to submit\ndisplay(testData[[\"Id\",\"Prediction1\"]]) # display 5 rows\n# write to a csv file for submission\ntestData.to_csv(\"submit.csv.gz\",index=False,columns=[\"Id\",\"Prediction1\"],compression=\"gzip\") # Output a compressed csv file for submission: see \/kaggle\/working to the right","770844f8":"### Now fit the model to half of the training data","6c42d17e":"## Do some input processing and put data into a numpy array for use in Keras","554f38f0":"## Plot the data in each column","8a1f0875":"# Now load data for the competition","aaa86fdd":"## Load the data","4a48179f":"# Kaggle default setup for notebooks","9a7fe7ad":"## Using Keras make a simple one hidden layer model"}}