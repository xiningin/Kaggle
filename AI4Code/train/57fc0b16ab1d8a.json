{"cell_type":{"e5116312":"code","08620119":"code","204e8e87":"code","15e09b8c":"code","e47068ab":"code","4f0fb594":"code","5639a76f":"code","aa4168ef":"code","28597610":"code","e1e13adc":"code","db6e6b41":"code","e7077d6f":"code","01c284b7":"code","489a3f36":"code","74a3400f":"code","367b071e":"code","078fb084":"code","549b4e6e":"code","8efcdc1c":"code","459b2a2b":"code","c092eab6":"code","11cece1f":"markdown","6babc427":"markdown","50d92e17":"markdown","2b8a54d7":"markdown"},"source":{"e5116312":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","08620119":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col=0)\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col=0)\nsample = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","204e8e87":"# Missing features\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()","15e09b8c":"# List of categorial features\ncat_features = [col for col in train.columns if train[col].dtypes == 'object']\nprint(len(cat_features), cat_features)\n\n# List of numerical features\nnum_features = [col for col in train.columns if train[col].dtypes != 'object']\n# Remove id and target\nnum_features = [i for i in num_features if i not in ['Id', 'SalePrice']]\nprint(len(num_features), num_features)","e47068ab":"# Most Frequest fillna for Categorical features\ncat = train[cat_features].fillna(train.mode().iloc[0])\n# Mean fillna for Continuous features\nnum = train[num_features].fillna(train.mean().iloc[0])\n\nX_train = cat.join(num)","4f0fb594":"# Most Frequest fillna for Categorical features\ncat = test[cat_features].fillna(test.mode().iloc[0])\n# Mean fillna for Continuous features\nnum = test[num_features].fillna(test.mean().iloc[0])\n\nX_test = cat.join(num)","5639a76f":"# Heatmap\nplt.figure(dpi=100, figsize=(30, 12))\nsns.heatmap(train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2)","aa4168ef":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nX_train[cat_features] = ordinal_encoder.fit_transform(X_train[cat_features])\nX_test[cat_features] = ordinal_encoder.transform(X_test[cat_features])\n\n# Log transform numerical columns\nX_train[num_features] = np.log1p(X_train[num_features])\ntrain.SalePrice = np.log1p(train.SalePrice)\nX_test[num_features] = np.log1p(X_test[num_features])","28597610":"X_train","e1e13adc":"import matplotlib.pyplot as plt\nfrom sklearn.feature_selection import mutual_info_regression\n\ny = train['SalePrice']\nX = X_train\n\n# Calculate MI Scores\nmi_scores = mutual_info_regression(X, y)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\nscores = mi_scores.sort_values(ascending=False)\n\n# Plot MI scores\nplt.figure(dpi=100, figsize=(30, 12))\nscores = scores.sort_values(ascending=True)\nwidth = np.arange(len(scores))\nticks = list(scores.index)\nplt.barh(width, scores)\nplt.yticks(width, ticks)\nplt.title(\"Mutual Information Scores\")","db6e6b41":"scores.loc[scores > 0.1]","e7077d6f":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_log_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport warnings\nwarnings.filterwarnings('ignore')","01c284b7":"def model_predict(model):\n    splits = 5\n    kf = KFold(n_splits=splits, shuffle=True, random_state=8)\n\n    oof_rmsle = []\n    test_predictions = []\n    \n    for fold, (train_indices, valid_indices) in enumerate(kf.split(X)):\n        # Divide train and validation data using folds\n        X_train, X_valid = X.iloc[train_indices], X.iloc[valid_indices]\n        y_train, y_valid = y.iloc[train_indices], y.iloc[valid_indices]\n        \n        # Fit the model\n        model.fit(X_train, y_train, verbose=False)\n\n        valid_preds = model.predict(X_valid)\n        msle = mean_squared_log_error(y_valid, valid_preds)\n        oof_rmsle.append(np.sqrt(msle))\n        print(fold, np.sqrt(msle))\n        \n        test_preds = np.expm1(model.predict(X_test))\n        test_predictions.append(test_preds)\n\n    print('Mean RMSLE:', np.mean(oof_rmsle), 'STD:', np.std(oof_rmsle))\n    return test_predictions","489a3f36":"%%time\n# Simple model without GPU\nmodel = XGBRegressor(random_state=64)\n\n# Calculate Test predictions\ntest_predictions = model_predict(model)\n\n# Submit predictions\nsample.SalePrice = np.mean(np.column_stack(test_predictions), axis=1)\nsample.to_csv('xgb_naive_log.csv', index=False)","74a3400f":"%%time\n# Simple model without GPU\nmodel = LGBMRegressor(random_state=64)\n\n# Calculate Test predictions\ntest_predictions = model_predict(model)\n\n# Submit predictions\nsample.SalePrice = np.mean(np.column_stack(test_predictions), axis=1)\nsample.to_csv('lgbm_naive_log.csv', index=False)","367b071e":"%%time\n# Simple model without GPU\nmodel = CatBoostRegressor(random_state=64)\n\n# Calculate Test predictions\ntest_predictions = model_predict(model)\n\n# Submit predictions\nsample.SalePrice = np.mean(np.column_stack(test_predictions), axis=1)\nsample.to_csv('cat_naive_log.csv', index=False)","078fb084":"from sklearn.preprocessing import StandardScaler\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split","549b4e6e":"# Standard Scaling\nscaler = StandardScaler()\nX = pd.DataFrame(scaler.fit_transform(X))\nX_test = pd.DataFrame(scaler.transform(X_test))","8efcdc1c":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)","459b2a2b":"model = keras.Sequential([\n    layers.Dense(1024, activation='relu', input_shape=[79]),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1),\n])\n\nearly_stopping = EarlyStopping(\n    min_delta=0.0001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\nmodel.compile(\n    optimizer='adam',\n    loss='msle',\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=128,\n    epochs=100,\n    callbacks=[early_stopping], # put your callbacks in a list\n    verbose=0,  # turn off training log\n)\n\nhistory_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5\nhistory_df.loc[5:, ['loss', 'val_loss']].plot()\n\nprint(\"Best Validation Loss: {:0.4f}\".format(np.sqrt(history_df['val_loss'].min())))","c092eab6":"# Submit predictions\nsample.SalePrice = model.predict(X_test)\nsample.to_csv('dl_naive_log.csv', index=False)","11cece1f":"## Ordinal Encoding + Log transform","6babc427":"## Mutual Information","50d92e17":"## Deep Learning","2b8a54d7":"## Boosting models"}}