{"cell_type":{"544ba4c7":"code","7040a04e":"code","2d1c22d3":"code","53490229":"code","f1a049fd":"code","cd385080":"code","c4dc3b74":"code","926630ce":"code","64e28398":"code","4cdc93a2":"code","f88784a9":"code","71488c8c":"code","882bc4c3":"code","c281badc":"code","7b57851f":"code","a2c4f0b3":"code","c07f6528":"code","4767f41b":"code","5d9d4c74":"code","f029c79d":"code","bd886f70":"code","ce56f9b7":"code","aca98e3e":"code","f2a3828b":"code","4a736c41":"code","0c34c071":"code","ef071897":"code","eb4048f6":"code","b616f540":"code","da373543":"code","b2be5137":"code","402e562a":"code","efd62f14":"code","94153711":"code","6c658a33":"code","93261bb0":"code","81e0f1d0":"code","c91047f1":"code","c2d4a368":"code","b3183a97":"code","a63ec74d":"code","0e9d4dd7":"code","1ce6a7ce":"code","610d8678":"code","ce7c087d":"code","d20391df":"code","ef9cc019":"code","6409bdd5":"code","d2da9330":"code","76c1b116":"markdown","06330ee7":"markdown","5c40a587":"markdown","f8484404":"markdown","e2b5807a":"markdown","31c15c8b":"markdown","3c94d846":"markdown","137a70dc":"markdown","6b7a4737":"markdown","75f1dc84":"markdown","3c6c3a53":"markdown","643a8086":"markdown","5d80ea2e":"markdown","41bce7a7":"markdown","3ae9bee9":"markdown","9dd41968":"markdown","35bf038b":"markdown","404d22d9":"markdown","02c58e9a":"markdown","4617bc15":"markdown","da095489":"markdown","ce235263":"markdown","34f52783":"markdown","75147081":"markdown","9e042227":"markdown","01eb014e":"markdown","9ff897c2":"markdown","0f3a0417":"markdown","293201ab":"markdown","a27168df":"markdown","70570917":"markdown","ee7758a6":"markdown","bb27623e":"markdown","4b358b0b":"markdown","ab7138ce":"markdown","2edc84c1":"markdown","cf4d770a":"markdown","2d06b374":"markdown"},"source":{"544ba4c7":"import datetime as dt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport pickle\nimport re\nimport gc\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.preprocessing as prep \nimport sklearn.ensemble as ens\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import *","7040a04e":"MyPath='..\/input\/future-sales-prediction\/'\nCompPath='..\/input\/competitive-data-science-predict-future-sales\/'\nTRPath='..\/input\/translated\/'\nRawCategories=pd.read_csv(CompPath+'item_categories.csv')\nRawItems=pd.read_csv(CompPath+'items.csv')\nRawSalesTrain=pd.read_csv(CompPath+'sales_train.csv')\nRawSalesTrain['Date']=pd.to_datetime(RawSalesTrain['date'],format='%d.%m.%Y')\nRawShops=pd.read_csv(TRPath+'shopsTR.csv')\nSalesTest1=pd.read_csv(CompPath+'test.csv')\nSalesTest=SalesTest1.set_index(['shop_id','item_id'])\nprint(\"Expected Number of Predictions:\"+str(SalesTest.size))\nShopIds=np.sort(SalesTest1.shop_id.unique())\nprint(\"Expected Shops for prediction(\"+str(len(ShopIds))+\" shops):\"+  str(ShopIds))","2d1c22d3":"def LowerTypes(df,coltypes):\n    for col in coltypes.keys():\n        if col in df.columns:\n            df[col]=df[col].astype(coltypes[col])\n    return df\n\ndef delifexists(var):\n    if var in globals():\n        del var\n        \ndef delifexistslist(varlist):\n    for var in varlist:\n        if var in globals():\n            del var\n    gc.collect()","53490229":"We are expected to predict for the below month","f1a049fd":"M=RawSalesTrain.Date.max().month+1\nY=RawSalesTrain.Date.max().year\nTargetPeriod=str(Y)+str(M)\nTargetMonthId=RawSalesTrain.date_block_num.max()+1\nprint(\"Month \"+ str(M)+\" of \"+ str(Y)+\",Period:\"+TargetPeriod+\",date_block_num:\"+str(TargetMonthId))","cd385080":"fig, ax = plt.subplots(1,2)\n#plt.figure(figsize=(6,2))\nsns.boxplot(x=RawSalesTrain.item_cnt_day,ax=ax[0])\nsns.boxplot(x=RawSalesTrain.item_price,ax=ax[1])","c4dc3b74":"SalesTrain=RawSalesTrain[(RawSalesTrain.item_cnt_day>0) & (RawSalesTrain.item_cnt_day<1000)& (RawSalesTrain.item_price>0) & (RawSalesTrain.item_price<100000)]\nSalesTrain=SalesTrain.drop(columns=['date'])","926630ce":"#Remove ! character before we slice\nRawShops.shop_name_en=RawShops.shop_name_en.str.replace('!','').str.strip()\n#Get City out\nRawShops[['City','Type','NL']]=RawShops.shop_name_en.str.lower().str.split(' ',2,expand=True)\nRawShops.loc[RawShops.shop_id==9,'City']=''\nRawShops.loc[RawShops.shop_id==12,'City']=''\nRawShops.loc[RawShops.shop_id==55,'City']=''\nShopIdx=[0,6,10,11,22,57]\nRawShops.loc[RawShops.index.isin(ShopIdx),'Type']='shop'\nMallIdx=[2,4,13,29,30,32,39,40,46]\nRawShops.loc[RawShops.index.isin(MallIdx),'Type']='mall'\nOnlineIdx=[12,55]\nRawShops.loc[RawShops.index.isin(OnlineIdx),'Type']='online'\nRawShops.loc[RawShops.index.isin(OnlineIdx),'City']='online'\nRawShops.loc[RawShops.City=='rostovnadonu','City']='rostov-on-don'\nRawShops.loc[RawShops.City=='chekhov','City']='moscow'\nRawShops1=RawShops.loc[ RawShops.shop_id.isin(ShopIds),['shop_id','City','Type']]","64e28398":"#Populations taken from Google searches\nPopulationsK={\n            'adygea':440,\n            'balashikha':229,\n            'volzhsky':330,\n            'vologda':304,\n            'voronezh':998,\n            'zhukovsky':107,\n            'online':0,\n            'kazan':1170,\n            'kaluga':329,\n            'kolomna':145,\n            'krasnoyarsk':1007,\n            'kursk':426,\n            'moscow':1192,\n            'n.novgorod':1257,\n            'novosibirsk':1511,\n            'omsk':1159,\n            'rostov-on-don':1100,\n            'spb':4991,\n            'samara':1170,\n            'sergiev':109,\n            'surgut':321,\n            'tomsk':544,\n            'tyumen':622,\n            'ufa':1075,\n            'yakutsk':282,\n            'yaroslavl':597\n            }\n#Convert to Dataframe with index\nPop=pd.DataFrame({'City':list(PopulationsK.keys()),'Population':list(PopulationsK.values())}).set_index('City')\nPop.Population=(Pop.Population.astype('float32')\/1000).astype(np.float32)\nShops1=RawShops1.set_index('City').join(Pop).reset_index().set_index('shop_id')\n#le1 = prep.LabelEncoder()\n#le2 = prep.LabelEncoder()\n#le1.fit(Shops1.City)\nShops1['CityEn']=prep.LabelEncoder().fit_transform(Shops1.City).astype(np.int8)\n#le2.fit(Shops1.Type)\nShops1['TypeEn']=prep.LabelEncoder().fit_transform(Shops1.Type).astype(np.int8)\nShops=Shops1.drop(columns=['City','Type'])\nShops.info()","4cdc93a2":"#Lets calculate number of items and shops\nNumber_Of_Shops=Shops.index.size\nNumber_Of_Items=RawItems.item_id.size\nTotal_Shop_Item_Combinations=Number_Of_Items*Number_Of_Shops\nNumber_Of_TestIDs=SalesTest.ID.size\nprint('Test IDs to submit='+str(Number_Of_TestIDs)+\",Combinations=\"+str(Total_Shop_Item_Combinations))\n# Not all shop\/item combinations are requested!!!","f88784a9":"def name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x\n\nRawItems[\"name1\"], RawItems[\"name2\"] = RawItems.item_name.str.split(\"[\", 1).str\nRawItems[\"name1\"],RawItems[\"name3\"] = RawItems.item_name.str.split(\"(\", 1).str\n\nRawItems[\"name2\"] = RawItems.name2.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\nRawItems[\"name3\"] = RawItems.name3.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\nRawItems = RawItems.fillna(\"0\")\n\nRawItems[\"item_name\"] = RawItems[\"item_name\"].apply(lambda x: name_correction(x))\nRawItems.name2 = RawItems.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")\nRawItems[\"type\"] = RawItems.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\nRawItems.loc[(RawItems.type == \"x360\") | (RawItems.type == \"xbox360\") | (RawItems.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nRawItems.loc[ RawItems.type == \"\", \"type\"] = \"mac\"\nRawItems.type = RawItems.type.apply( lambda x: x.replace(\" \", \"\") )\nRawItems.loc[ (RawItems.type == 'pc' )| (RawItems.type == 'p\u0441') | (RawItems.type == \"pc\"), \"type\" ] = \"pc\"\nRawItems.loc[ RawItems.type == '\u0440s3' , \"type\"] = \"ps3\"\nGroupedItems = RawItems.groupby([\"type\"]).agg({\"item_id\": \"count\"}).reset_index()\nto_drop = []\nfor cat in GroupedItems.type.unique():\n    if GroupedItems.loc[(GroupedItems.type == cat), \"item_id\"].values[0] <40:\n        to_drop.append(cat)\nRawItems.name2 = RawItems.name2.apply( lambda x: \"etc\" if (x in to_drop) else x )\nRawItems = RawItems.drop([\"type\"], axis = 1)\nRawItems.name2 = prep.LabelEncoder().fit_transform(RawItems.name2)\nRawItems.name3 = prep.LabelEncoder().fit_transform(RawItems.name3)\nItems=RawItems.drop(['item_name','name1'],axis=1)\ncoltypes={'item_id':np.int16,'item_category_id':np.int8,'name2':np.int8,'name3':np.int8}\nItems=LowerTypes(Items,coltypes)\nItems.info()","71488c8c":"RawCategories['item_category_name_2']=RawCategories[\"item_category_name\"].apply(lambda x: x.split()[0])\nRawCategories[\"item_category_name_2\"]=prep.LabelEncoder().fit_transform(RawCategories[\"item_category_name_2\"]).astype(np.int8)\nCategories=RawCategories.drop(columns=['item_category_name'])\nCategories.info()","882bc4c3":"#Group by month, shop_id, item_id\nSalesPerPeriodPerItem1=SalesTrain.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day':'sum','item_price':'mean'}).reset_index().rename(columns={'item_cnt_day':'item_cnt_month','item_price':'avg_shopitem_price'})\nShopItemCombinationsFor33=SalesPerPeriodPerItem1[SalesPerPeriodPerItem1.date_block_num==33].shape[0]\nprint('Train data for month 33 is '+str(ShopItemCombinationsFor33)+\", where for month 34 we predict for \"+str(Number_Of_TestIDs))","c281badc":"%%time\nSalesTemp=SalesTest.drop(columns=['ID']).reset_index()\nSalesTable=pd.DataFrame({'date_block_num':[0],'shop_id':[0],'item_id':[0]})\nfor i in range(34):  \n    #print(i)\n    SalesTemp['date_block_num']=i\n    SalesTemp2=SalesTemp.set_index(['date_block_num','shop_id','item_id'])\n    SalesPPX=SalesPerPeriodPerItem1[SalesPerPeriodPerItem1.date_block_num==i].set_index(['date_block_num','shop_id','item_id'])\n    SalesTemp2=SalesTemp2.join(SalesPPX).reset_index().fillna(0)\n    SalesTable=SalesTable.append(SalesTemp2)\nSalesTable=SalesTable.iloc[1:]\nSalesTable.info()","7b57851f":"#Group by month, item_id\nSalesPerPeriodPerItem2=SalesTrain.groupby(['date_block_num','item_id']).agg({'item_cnt_day':'sum','item_price':'mean'}).reset_index().rename(columns={'item_cnt_day':'item_cnt_month','item_price':'avg_item_price'})\nSalesPerPeriodPerItem3=SalesTable.set_index(['date_block_num','item_id']).join(SalesPerPeriodPerItem2.set_index(['date_block_num','item_id']).drop(columns=['item_cnt_month'])).fillna(0)","a2c4f0b3":"#Calculate price difference for each item in each shop to mean item price\nSalesPerPeriodPerItem3['price_diff']=(SalesPerPeriodPerItem3['avg_shopitem_price']-SalesPerPeriodPerItem3['avg_item_price'])\/SalesPerPeriodPerItem3['avg_item_price']\nSalesPerPeriodPerItem4=SalesPerPeriodPerItem3.drop(columns=['avg_shopitem_price','avg_item_price']).reset_index()\n\nSalesPerPeriodPerItem4.info()","c07f6528":"X_target1=SalesTest.reset_index().drop(columns=['ID'])\nX_target1['date_block_num']=34\nX_target1['item_cnt_month']=0\nX_target1['price_diff']=0\nSalesPerPeriodPerItem=SalesPerPeriodPerItem4.append(X_target1).fillna(0)\ncoltypes={'date_block_num':np.int8,'shop_id':np.int8,'item_id':np.int16,'price_diff':np.float32,'item_cnt_month':np.float32}\nSalesPerPeriodPerItem=LowerTypes(SalesPerPeriodPerItem,coltypes)\n#if (SalesPerPeriodPerItem[SalesPerPeriodPerItem.date_block_num==34].shape[0])>214200:\n#    print('!!!ERROR, row number larger than 214200')\ndelifexistslist(['SalesPerPeriodPerItem1','SalesPerPeriodPerItem2','SalesPerPeriodPerItem3','SalesPerPeriodPerItem4'])","4767f41b":"MostBusyShopIds=np.argsort(SalesPerPeriodPerItem.groupby('shop_id').item_cnt_month.mean(),axis=None)[-3:]\nBusyShopActivity=SalesPerPeriodPerItem[SalesPerPeriodPerItem.shop_id.isin(MostBusyShopIds)]\nplt.figure(figsize=(14,4))\nsns.lineplot(data=BusyShopActivity,x='date_block_num',y='item_cnt_month',hue='shop_id')\n#There is a decreasing trend in sales of shops\n#There is a seasonality to the data with peaks happening on december (due to christmas).","5d9d4c74":"#https:\/\/stackoverflow.com\/questions\/45184055\/how-to-plot-multiple-seasonal-decompose-plots-in-one-figure\ndef plotseasonal(res, axes, title):\n    res.observed.plot(ax=axes[0], legend=False)\n    axes[0].set_ylabel('Observed')\n    res.trend.plot(ax=axes[1], legend=False)\n    axes[1].set_ylabel('Trend')\n    res.seasonal.plot(ax=axes[2], legend=False)\n    axes[2].set_ylabel('Seasonal')\n    res.resid.plot(ax=axes[3], legend=False)\n    axes[3].set_ylabel('Residual')\n    axes[0].set_title(title)\n    \nfig, axes1 = plt.subplots(ncols=2, nrows=4, sharex=True, figsize=(12,5))    \nShopActivity19=BusyShopActivity.loc[BusyShopActivity.shop_id==19,['date_block_num','item_cnt_month']].set_index('date_block_num')\nShopActivity18=BusyShopActivity.loc[BusyShopActivity.shop_id==18,['date_block_num','item_cnt_month']].set_index('date_block_num')\nresult19 = seasonal_decompose(ShopActivity19, model='additive', period=12)\nresult18 = seasonal_decompose(ShopActivity18, model='additive', period=12)\nplotseasonal(result19,axes1[:,0], title = 'Sales decomposition for Store 19')\nplotseasonal(result18,axes1[:,1], title = 'Sales decomposition for Store 8')\nplt.tight_layout()","f029c79d":"SalesPerPeriodPerItem1=SalesPerPeriodPerItem.set_index('item_id').join(Items.set_index('item_id')).reset_index()\nSalesPerPeriodPerItem2=SalesPerPeriodPerItem1.set_index('item_category_id').join(Categories.set_index('item_category_id')).reset_index()\nEnrichedSales1=SalesPerPeriodPerItem2.set_index('shop_id').join(Shops).reset_index()\nEnrichedSales1.info()","bd886f70":"# add month day, week day and weekend count\n\ndef last_day_of_month(date_block_num):\n    addyear=date_block_num\/\/12\n    addmonth=date_block_num%12+1\n    date=dt.datetime(year=2013+addyear,month=addmonth,day=28)\n    next_month = date + dt.timedelta(days=4)\n    return (next_month - dt.timedelta(days=next_month.day)).day\n\ndef count_holidays(date_block_num):\n    m = 1 + date_block_num % 12\n    if m == 1:\n        return 1\n    elif m == 2:\n        return 1\n    elif m == 3:\n        return 1\n    elif m == 5:\n        return 2\n    elif m == 6:\n        return 1\n    elif m == 11:\n        return 1\n    elif m == 12:\n        return 2\n    else:\n        return 0\n    \ndef count_weekdays(date_block_num):\n    try:\n        y = 2013 + date_block_num \/\/ 12\n        m = 1 + date_block_num % 12\n        if m < 9:\n            return np.busday_count(f'{y}-0{m}', f'{y}-0{m+1}')\n        elif m ==9:\n            return np.busday_count(f'{y}-0{m}', f'{y}-10')\n        elif m != 12:\n             return np.busday_count(f'{y}-{m}', f'{y}-{m+1}')\n        else:\n            return np.busday_count(f'{y}-{m}', f'{y+1}-01')\n    except ValueError:\n        print(m,y)","ce56f9b7":"%%time\nEnrichedSales1['days_in_month']=EnrichedSales1.date_block_num.apply(lambda x:last_day_of_month(x)).astype(np.int8)\nEnrichedSales1['holidays']=EnrichedSales1.date_block_num.apply(lambda x:count_holidays(x)).astype(np.int8)\nEnrichedSales1['weekdays']=EnrichedSales1.date_block_num.apply(lambda x:count_weekdays(x)).astype(np.int8)\nEnrichedSales1.info()","aca98e3e":"ShopActivity19['zdata']=ShopActivity19.item_cnt_month-ShopActivity19.item_cnt_month.rolling(window=12).mean()\/ShopActivity19.item_cnt_month.rolling(window=12).std()\nfig, ax = plt.subplots(2, figsize=(12,6))\nax[0].plot(ShopActivity19.index,ShopActivity19.item_cnt_month,label='Raw Data')\nax[0].plot(ShopActivity19.item_cnt_month.rolling(window=12).mean(),label='Rolling Mean')\nax[0].plot(ShopActivity19.item_cnt_month.rolling(window=12).std(),label='Rolling Std')\nax[0].legend()\nax[1].plot(ShopActivity19.index,ShopActivity19.zdata,label='Untrended Data')\nax[1].plot(ShopActivity19.zdata.rolling(window=12).mean(),label='Rolling Mean')\nax[1].plot(ShopActivity19.zdata.rolling(window=12).std(),label='Rolling Std')\nax[1].legend()\nplt.tight_layout()","f2a3828b":"fig, ax = plt.subplots(2, figsize=(12,6))\nax[0] = plot_acf(ShopActivity19.zdata.dropna(), ax=ax[0], lags=10)\nax[1] = plot_pacf(ShopActivity19.zdata.dropna(), ax=ax[1], lags=10)","4a736c41":"%%time\nFirstIntroShop=EnrichedSales1.groupby(['item_id','shop_id']).date_block_num.min().reset_index().rename(columns={'date_block_num':'item_first_intro_shop'})\nEnrichedSales1=EnrichedSales1.reset_index().set_index(['item_id','shop_id'])\nEnrichedSales2=EnrichedSales1.join(FirstIntroShop.set_index(['item_id','shop_id'])).reset_index()\nEnrichedSales2['item_first_intro_shop']=EnrichedSales2['date_block_num']-EnrichedSales2['item_first_intro_shop']\n\nFirstIntroCity=EnrichedSales2.groupby(['item_id','CityEn']).date_block_num.min().reset_index().rename(columns={'date_block_num':'item_first_intro_city'})\nEnrichedSales2=EnrichedSales2.reset_index().set_index(['item_id','CityEn'])\nEnrichedSales3=EnrichedSales2.join(FirstIntroCity.set_index(['item_id','CityEn'])).reset_index()\nEnrichedSales3['item_first_intro_city']=EnrichedSales3['date_block_num']-EnrichedSales3['item_first_intro_city']\n\nFirstIntro=EnrichedSales3.groupby(['item_id']).date_block_num.min().reset_index().rename(columns={'date_block_num':'item_first_intro'})\nEnrichedSales3=EnrichedSales3.set_index(['item_id'])\nEnrichedSales4=EnrichedSales3.join(FirstIntro.set_index(['item_id'])).reset_index()\nEnrichedSales4['item_first_intro']=EnrichedSales4['date_block_num']-EnrichedSales4['item_first_intro']\n\nEnrichedSales4['item_first_intro_shopCity']=EnrichedSales4['item_first_intro_shop']-EnrichedSales4['item_first_intro_city']\nEnrichedSales4['item_first_intro_shopGen']=EnrichedSales4['item_first_intro_shop']-EnrichedSales4['item_first_intro']\n\nEnrichedSales4.info()","0c34c071":"#Clean memory\nEnrichedSales1=EnrichedSales4.copy()\ndelifexistslist(['Enriched2','Enriched3','Enriched4','FirstIntroShop','FirstIntroCity','FirstIntro'])","ef071897":"def laggedFeature(df, lags, col,ntype):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        NewcolumnName=col+'_lag_' + str(i)\n        shifted=shifted.rename(columns={col:NewcolumnName})\n        shifted['date_block_num'] += i\n        shifted=shifted.set_index(['date_block_num','shop_id','item_id'])\n        df=df.set_index(['date_block_num','shop_id','item_id']).join(shifted).reset_index()\n        df[NewcolumnName]=df[NewcolumnName].astype(ntype)\n    return df.fillna(0)","eb4048f6":"%%time\nEnrichedSales2=laggedFeature(EnrichedSales1,[1,2,3],'item_cnt_month',np.float32)","b616f540":"ESL1=EnrichedSales2[EnrichedSales2.date_block_num==34].set_index(['shop_id','item_id']).item_cnt_month_lag_1\nESL2=EnrichedSales2[EnrichedSales2.date_block_num==33].set_index(['shop_id','item_id']).item_cnt_month\ntestdf=pd.DataFrame(ESL1)\ntestdf=testdf.join(ESL2).fillna(0)\ntestdf['diff']=testdf.item_cnt_month_lag_1-testdf.item_cnt_month\nif testdf['diff'].unique().size==1:\n    print('Good')\nelse:\n    print('There must be an error')","da373543":"%%time\nEnrichedSales3=laggedFeature(EnrichedSales2,[1,2,3],'price_diff',np.float32).fillna(0).drop(columns=['price_diff'])\nEnrichedSales3.info()","b2be5137":"%%time\n#Item-item_cnt_month Encoding\nItemTargetEncoding=EnrichedSales3.groupby(['date_block_num','item_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_enc\"})\nEnrichedSales4=EnrichedSales3.set_index(['date_block_num','item_id']).join(ItemTargetEncoding.set_index(['date_block_num','item_id'])).reset_index()\n#Item-item+Shop_id_cnt_month Encoding\nShopItemTargetEncoding=EnrichedSales4.groupby(['date_block_num','shop_id','item_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"shopitem_target_enc\"})\nEnrichedSales5=EnrichedSales4.set_index(['date_block_num','shop_id','item_id']).join(ShopItemTargetEncoding.set_index(['date_block_num','shop_id','item_id'])).reset_index()\n#Item+item_category_id-item_cnt_month Encoding\nItemCategoryTargetEncoding=EnrichedSales5.groupby(['date_block_num','item_id','item_category_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_cat_enc\"})\nEnrichedSales6=EnrichedSales5.set_index(['date_block_num','item_id','item_category_id']).join(ItemCategoryTargetEncoding.set_index(['date_block_num','item_id','item_category_id'])).reset_index()\n\n#Item+item_category_name_2-item_cnt_month Encoding\nItemCategoryTargetEncoding=EnrichedSales6.groupby(['date_block_num','item_id','item_category_name_2'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_cat_enc2\"})\nEnrichedSales7=EnrichedSales6.set_index(['date_block_num','item_id','item_category_name_2']).join(ItemCategoryTargetEncoding.set_index(['date_block_num','item_id','item_category_name_2'])).reset_index()\n\n#Item+CityEn-item_cnt_month Encoding\nItemCategoryCityTargetEncoding=EnrichedSales7.groupby(['date_block_num','item_id','CityEn'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_city_enc\"})\nEnrichedSales8=EnrichedSales7.set_index(['date_block_num','item_id','CityEn']).join(ItemCategoryCityTargetEncoding.set_index(['date_block_num','item_id','CityEn'])).reset_index()","402e562a":"#Clean memory\ndelifexistslist(['ItemTargetEncoding','ShopItemTargetEncoding','ItemCategoryTargetEncoding','ItemCategoryTargetEncoding','ItemCategoryCityTargetEncoding','EnrichedSales3','EnrichedSales4','EnrichedSales5','EnrichedSales6','EnrichedSales7'])","efd62f14":"%%time\nEnrichedSales9=laggedFeature(EnrichedSales8,[1,2,3],'item_target_enc',np.float32)\nEnrichedSales10=laggedFeature(EnrichedSales9,[1,2,3],'shopitem_target_enc',np.float32)\nEnrichedSales11=laggedFeature(EnrichedSales10,[1,2,3],'item_target_cat_enc',np.float32)\nEnrichedSales12=laggedFeature(EnrichedSales11,[1,2,3],'item_target_cat_enc2',np.float32)\nEnrichedSales13=laggedFeature(EnrichedSales12,[1,2,3],'item_target_city_enc',np.float32)\n#*************************\n# It is extremely important to delete below columns after introducing lagged values \n# as they are a source of target leakage\n#*************************\nEnrichedSales13=EnrichedSales13.drop(columns=['item_target_enc','shopitem_target_enc','item_target_cat_enc','item_target_cat_enc2','item_target_city_enc'])\nEnrichedSales13=EnrichedSales13[EnrichedSales11['date_block_num']>2]\nEnrichedSales13.info()","94153711":"coltypes={'date_block_num':np.int8,'shop_id':np.int8,'item_id':np.int16,'CityEn':np.int8,'item_category_id':np.int8}\nEnrichedSales13=LowerTypes(EnrichedSales13,coltypes)","6c658a33":"EnrichedSales=EnrichedSales13\npickle.dump(EnrichedSales, open(\"EnrichedSales.pkl\", \"wb\"))\ndelifexistslist(['RawShops','RawItems','Items','Categories']) \ndelifexistslist(['SalesTrain','SalesTable','SalesPerPeriodPerItem1','SalesPerPeriodPerItem2','SalesPerPeriodPerItem3','SalesPerPeriodPerItem'])\ndelifexistslist(['EnrichedSales1','EnrichedSales2','EnrichedSales3','EnrichedSales4','EnrichedSales5'])\ndelifexistslist(['EnrichedSales6','EnrichedSales7','EnrichedSales8','EnrichedSales9','EnrichedSales10'])\ndelifexistslist(['EnrichedSales11','EnrichedSales12'])","93261bb0":"corrMatrix1=EnrichedSales.corr()\nsns.heatmap(corrMatrix1, annot=False)","81e0f1d0":"MyPath='..\/input\/future-sales-prediction\/'\nCompPath='..\/input\/competitive-data-science-predict-future-sales\/'\nTRPath='..\/input\/translated\/'\nModelData = pickle.load(open(MyPath+\"EnrichedSales.pkl\", \"rb\"))\nSalesTest1=pd.read_csv(CompPath+'test.csv')\nSalesTest=SalesTest1.set_index(['shop_id','item_id'])\nModelData.item_cnt_month[ModelData.item_cnt_month>20]=20\nModelData201509=ModelData[ModelData.date_block_num==32]\nModelData201509y=SalesTest.join(ModelData201509.set_index(['shop_id','item_id']).item_cnt_month).fillna(0).item_cnt_month.values\nModelData201510=ModelData[ModelData.date_block_num==33]\nModelData201510y=SalesTest.join(ModelData201510.set_index(['shop_id','item_id']).item_cnt_month).fillna(0).item_cnt_month.values\n#Lets check RSME value if 201509 is used as prediction for 201510\nrms = mean_squared_error(ModelData201510y, ModelData201509y, squared=False)\nprint('RMSE for 201509->201510 Regession:'+str(rms))","c91047f1":"#Create Train, Validation Set\nX_train=ModelData[ModelData.date_block_num<33].drop(columns=['item_cnt_month'])\ny_train=ModelData[(ModelData.date_block_num<33)]['item_cnt_month']\nX_valid=ModelData[ModelData.date_block_num==33].drop(columns=['item_cnt_month'])\ny_valid=ModelData[ModelData.date_block_num==33]['item_cnt_month'].values\nX_target=ModelData[ModelData.date_block_num==34].drop(columns=['item_cnt_month'])","c2d4a368":"model1=LinearRegression()\nmodel1.fit(X_train,y_train)\ny_pred1=model1.predict(X_valid)\ny_pred=np.rint(y_pred1)\ny_pred[y_pred>20]=20\nrms = mean_squared_error(y_valid, y_pred, squared=False)\nprint('RMSE for Linear Regession:'+str(rms))\nplt.style.use('seaborn-whitegrid')\nplt.plot(y_valid,'.',color='red')\nplt.plot(y_pred,'.',color='blue')\nplt.show()","b3183a97":"%%time\ncategorical=['date_block_num','shop_id','item_id','CityEn','item_category_id','name2','name3','TypeEn']\nmodel2 = CatBoostRegressor(\n            iterations=50,\n            learning_rate=1,\n            depth=2)\n\nmodel2.fit(\n    X_train, y_train,\n    cat_features=categorical,\n    eval_set=(X_valid, y_valid),\n    logging_level='Silent'\n)\nprint('Model is fitted: ' + str(model2.is_fitted()))\nmodel2.get_best_score()\ny_pred1=model2.predict(X_valid)\ny_pred=np.rint(y_pred1)\ny_pred[y_pred>20]=20\nrms = mean_squared_error(y_valid, y_pred, squared=False)\nprint('RMSE for Catboost:'+str(rms))\nplt.plot(y_valid,'.',color='red')\nplt.plot(y_pred,'.',color='blue')\nplt.show()\npickle.dump(model2, open(\"model2.pkl\", \"wb\"))","a63ec74d":"%%time\n#params_rf = {\n#    'n_estimators': [20,30,40,50], \n#    'max_depth':[2,4,6], \n#    'max_features': ['auto', 'sqrt'], \n#}\n#n_estimators larger than 20 does not improve score considerably\n#max_depth is 6\n# Below paramaters results better score\n#RFR=ens.RandomForestRegressor()\n#model3 = GridSearchCV(RFR, params_rf, cv=None, refit=True,n_jobs=-1)\nmodel3 = ens.RandomForestRegressor(n_estimators=20,max_depth=6, random_state=0,n_jobs=-1)\nmodel3.fit(X_train, y_train)\n#print(model3.best_params_)\ny_pred1=model3.predict(X_valid)\ny_pred=np.rint(y_pred1)\ny_pred[y_pred>20]=20\nrms = mean_squared_error(y_valid, y_pred, squared=False)\nprint('RMSE for RandomForest Regession:'+str(rms))\nplt.plot(y_valid,'.',color='red')\nplt.plot(y_pred,'.',color='blue')\nplt.show()\npickle.dump(model3, open(\"model3.pkl\", \"wb\"))","0e9d4dd7":"%%time\nmodel4 = XGBRegressor(\n    max_depth=10,\n    booster='gbtree',\n    n_estimators=100,\n    min_child_weight=0.5, \n    subsample=0.8,\n    sampling_method=\"uniform\",\n    colsample_bynode=1,\n    colsample_bytree=0.8, \n    eta=0.1,\n    seed=0)\n\nmodel4.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_valid, y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)","1ce6a7ce":"%%time\ny_pred1=model4.predict(X_valid)\ny_pred=np.rint(y_pred1)\ny_pred[y_pred>20]=20\nrms = mean_squared_error(y_valid, y_pred, squared=False)\nprint('RMSE for RandomForest Regession:'+str(rms))\nplt.plot(y_valid,'.',color='red')\nplt.plot(y_pred,'.',color='blue')\nplt.show()\npickle.dump(model4, open(\"model4.pkl\", \"wb\"))","610d8678":"fig, ax = plt.subplots(1,1,figsize=(15,20))\nplot_importance(booster=model4, ax=ax)","ce7c087d":"X_all=ModelData.drop(columns=['item_cnt_month'])\ny_all=ModelData['item_cnt_month'].values\nmodelFinal = ens.RandomForestRegressor(n_estimators=20,max_depth=6, random_state=0)\n\nmodelFinal=pickle.load(open(\".\/model4.pkl\", \"rb\"))","d20391df":"#Predict Final Model Output\nModelOutput1=modelFinal.predict(X_target)\nPredictions=np.rint(ModelOutput1)\n#Clip values between [0:20]\nPredictions[Predictions>20]=20\nPredictions[Predictions<0]=0\ndf=pd.DataFrame(Predictions)","ef9cc019":"SalesTest1=pd.read_csv(CompPath+'test.csv')\nSalesTest=SalesTest1.set_index(['shop_id','item_id'])","6409bdd5":"PredsDF=X_target[['shop_id','item_id']]\nPredsDF['item_cnt_month']=Predictions\nSubmission=SalesTest.join(PredsDF.set_index(['shop_id','item_id'])).reset_index().drop(columns=['shop_id','item_id'])\nplt.plot(Submission.item_cnt_month,'.')","d2da9330":"Submission.to_csv('submission10.csv',index=False)","76c1b116":"We will train randomforest model and test it on validation set, please note that we used GridSearch for HYPERPARAMETER tuning","06330ee7":"### Items","5c40a587":"Lets expand our train data to accomodate our test data","f8484404":"# 4.Make Predictions and submit","e2b5807a":"Now we can seperate features from Shop Name field","31c15c8b":"First lets group the sales data last available month (2015-09) as a baseline","3c94d846":"Lets look at the correlation matrix","137a70dc":"Lets look at outliers and duplicates","6b7a4737":"For submission, encapsulate data to create csv file","75f1dc84":"### Shops","3c6c3a53":"Convert data and lower mem usage","643a8086":"### Item Category","5d80ea2e":"Lets clean Item data (thanks Mykyta Minenko since I have no info on Russian and couldnt trasilate)","41bce7a7":"save and clear data","3ae9bee9":"Lets check if it was populated for test part","9dd41968":"We will train a linear regression model and test it on validation set","35bf038b":"Lets look at moving averages to seperate trends and seasonalities","404d22d9":"## Future Sales Predictions\nlets first import necessary libraries","02c58e9a":"We need to expand train data to cover all shop-item pairs and pad empty values for zeros","4617bc15":"We will train catboostregressor model and test it on validation set","da095489":"Add previous Target Encodings to EnrichedSales","ce235263":"Lets add the change of prices as a feature","34f52783":"It is important to note that although shop Id's 0,1,10 seem duplicates, they are stripped from predictions and can be safely ignored.","75147081":"### Agregate to months","9e042227":"## 2.Feature Extraction","01eb014e":"We add City and Type features from shops","9ff897c2":"Then we import the data","0f3a0417":"## 3.Create and Tune Model","293201ab":"Lets do a seasonal decompose","a27168df":"Organize output data for submission","70570917":"Lets also try xgboost","ee7758a6":"Lets group sales based on shop_id and item_id","bb27623e":"Lets find how most busy shops are doing","4b358b0b":"Remove outliers, returns (item_cnt_day<0) and date column","ab7138ce":"Add item introduction","2edc84c1":"Add lagged features","cf4d770a":"Lets analyze test data","2d06b374":"Use the most promising method, train it on whole data set"}}