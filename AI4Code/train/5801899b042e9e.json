{"cell_type":{"bf70a805":"code","680131bb":"code","cf4ec980":"code","13e04e0d":"code","35334f6b":"code","c6d2f8f6":"code","3a794491":"code","0fc789d3":"markdown","06c7754f":"markdown","e7fdac0b":"markdown","f5b0bd30":"markdown","6ae75234":"markdown","c264e820":"markdown","173b75b2":"markdown","97bcf062":"markdown"},"source":{"bf70a805":"from tqdm.notebook import tqdm\nimport transformers\nimport pandas as pd\nimport numpy as np\nimport typing\nimport os","680131bb":"BASE_MODEL = \"roberta-base\"\nSEQ_LEN = 1024\nTEXT_PATH = \"..\/input\/feedback-prize-2021\/train\"\nCSV_PATH = \"..\/input\/feedback-prize-2021\/train.csv\"\nLABEL_MAP = {\n    \"Lead\": 0,\n    \"Position\": 1,\n    \"Evidence\": 2,\n    \"Claim\": 3,\n    \"Concluding Statement\": 4,\n    \"Counterclaim\": 5,\n    \"Rebuttal\": 6,\n}","cf4ec980":"class Tokenizer:\n    InputIds = typing.TypeVar(\"InputIds\", bound=typing.List[int])\n    AttentionMask = typing.TypeVar(\"AttentionMask\", bound=typing.List[int])\n    Labels = typing.TypeVar(\"Labels\", bound=typing.List[int])\n    OffsetMapping = typing.TypeVar(\n        \"OffsetMapping\",\n        bound=typing.List[typing.Tuple[int, int]],\n    )\n    TokenizerOutput = typing.NamedTuple(\n        \"TokenizerOutput\",\n        [\n            (\"input_ids\", InputIds),\n            (\"attention_mask\", AttentionMask),\n            (\"offset_mapping\", OffsetMapping),\n        ],\n    )\n    Output = typing.NamedTuple(\n        \"Output\",\n        [\n            (\"input_ids\", InputIds),\n            (\"attention_mask\", AttentionMask),\n            (\"labels\", Labels),\n        ],\n    )\n\n    def __init__(\n        self, df: pd.DataFrame, base_model: transformers.AutoTokenizer\n    ) -> None:\n        \"\"\"\n        Initialize tokenizer instance\n\n        :param df: DataFrame with labels for all texts\n        :param base_model: pre-loaded tokenizer\n        \"\"\"\n        self._df = df\n        self._base_model = base_model\n\n    def _init_output(self, n: int) -> Output:\n        \"\"\"\n        Return output-like arrays of zeros\n\n        :param n: number of unique text ids\n        \"\"\"\n        return self.Output(\n            input_ids=np.zeros((n, SEQ_LEN), dtype=\"int32\"),\n            attention_mask=np.zeros((n, SEQ_LEN), dtype=\"int32\"),\n            labels=np.zeros((n, SEQ_LEN, 2 * len(LABEL_MAP) + 1), dtype=\"int32\"),\n        )\n\n    def _get_tokenizer_output(self, text: str) -> TokenizerOutput:\n        \"\"\"\n        Return input token ids, attention mask and offset mapping of given text\n\n        :param text: essay text to tokenize\n        \"\"\"\n        encoding = self._base_model.encode_plus(\n            text,\n            max_length=SEQ_LEN,\n            padding=\"max_length\",\n            truncation=True,\n            return_offsets_mapping=True,\n        )\n        return self.TokenizerOutput(\n            input_ids=encoding[\"input_ids\"],\n            attention_mask=encoding[\"attention_mask\"],\n            offset_mapping=encoding[\"offset_mapping\"],\n        )\n\n    @staticmethod\n    def _get_labels(df: pd.DataFrame, offset_mapping: OffsetMapping) -> Labels:\n        \"\"\"\n        Return labels translated from initial text words (given) to tokens (machine understandable)\n\n        :param df: slice of DataFrame containing single text id\n        :param offset_mapping: offset mapping returned by tokenizer\n        \"\"\"\n        labels = np.zeros((SEQ_LEN, 2 * len(LABEL_MAP) + 1), dtype=\"int32\")\n        offset_index = 0\n\n        for _, (discourse_start, discourse_end, discourse_type) in df.iterrows():\n            if offset_index > len(offset_mapping) - 1:\n                break\n\n            k = LABEL_MAP[discourse_type]\n\n            token_start = offset_mapping[offset_index][0]\n            token_end = offset_mapping[offset_index][1]\n\n            first = True\n\n            while discourse_end > token_start:\n                if (token_start >= discourse_start) and (token_end <= discourse_end):\n                    if first:\n                        labels[offset_index, 2 * k] = 1\n                        first = False\n                    else:\n                        labels[offset_index, 2 * k + 1] = 1\n\n                offset_index += 1\n\n                if offset_index > len(offset_mapping) - 1:\n                    break\n\n                token_start = offset_mapping[offset_index][0]\n                token_end = offset_mapping[offset_index][1]\n\n        labels[:, -1] = 1 - np.max(labels, axis=-1)\n        return labels\n\n    def tokenize(self, verbose: int = 0) -> Output:\n        n = self._df.index.nunique()\n        ids = enumerate(self._df.index.unique())\n\n        if verbose > 0:\n            ids = tqdm(ids, total=n, desc=\"Tokenizing\")\n\n        output = self._init_output(n=n)\n\n        for i, id_ in ids:\n            with open(os.path.join(TEXT_PATH, id_ + \".txt\")) as file:\n                text = file.read().strip().lower()\n                tokenizer_output = self._get_tokenizer_output(text)\n\n            output.input_ids[i] = tokenizer_output.input_ids\n            output.attention_mask[i] = tokenizer_output.attention_mask\n            output.labels[i] = self._get_labels(\n                self._df[self._df.index == id_],\n                tokenizer_output.offset_mapping,\n            )\n\n        return output","13e04e0d":"df = pd.read_csv(\n    CSV_PATH,\n    usecols=[\"id\", \"discourse_start\", \"discourse_end\", \"discourse_type\"],\n    dtype={\n        \"id\": \"object\",\n        \"discource_start\": \"int32\",\n        \"discourse_end\": \"int32\",\n        \"discourse_type\": \"category\",\n    },\n    index_col=\"id\",\n)\nbase_model = transformers.AutoTokenizer.from_pretrained(BASE_MODEL)","35334f6b":"quote = \"It's understood that Hollywood sells Californication...\"\nencoding = base_model.encode_plus(quote, return_offsets_mapping=True)\n\npd.DataFrame(\n    {\"token\": [quote[x[0] : x[1]] for x in encoding[\"offset_mapping\"]]},\n    index=pd.Index(encoding[\"input_ids\"], name=\"token_id\"),\n)","c6d2f8f6":"tokenizer = Tokenizer(df=df, base_model=base_model)\noutput = tokenizer.tokenize(verbose=1)","3a794491":"np.save(\"input_ids.npy\", output.input_ids)\nnp.save(\"attention_mask.npy\", output.attention_mask)\nnp.save(\"labels.npy\", output.labels)","0fc789d3":"Tokenizer code\n--------------\n\nTreat it simply as a black box unless you want to discover it in-depth.","06c7754f":"Save results\n------------","e7fdac0b":"Configuration\n-------------","f5b0bd30":"Load data and RoBerta tokenizer\n-------------------------------","6ae75234":"Introduction\n------------\n\nThis notebook is a small preparation stage for\u00a0[Training RoBERTa in 10 minutes][1]. \nBy splitting the work into two parts I aim to save the TPU quota, separate concerns, and keep each part easy to read and follow.\n\nMany thanks to\u00a0[Chris Deotte][2]\u00a0for his\u00a0[amazing work][3]. \nThis notebook is its Copy-Edit version with just a handful of changes.\n\nAll the outputs of this notebook are also published to\u00a0[this dataset][4].\n\nImports\n-------\n\n[1]: https:\/\/www.kaggle.com\/nickuzmenkov\/feedback-prize-training-roberta-in-10-minutes\/\n[2]: https:\/\/www.kaggle.com\/cdeotte\n[3]: https:\/\/www.kaggle.com\/cdeotte\/tensorflow-longformer-ner-cv-0-633\n[4]: https:\/\/www.kaggle.com\/nickuzmenkov\/feedback-prize-roberta-tokens-1024","c264e820":"Conclusion\n----------\n\nThanks for reading. If you like this work, please visit the next part: [Training RoBERTa in 10 minutes][1].\nAll the outputs of this notebook are also published to [this dataset][2].\n\nI am in no way good at NLP, so feel free to correct me if you feel so.\n\n[1]: https:\/\/www.kaggle.com\/nickuzmenkov\/feedback-prize-training-roberta-in-10-minutes\/\n[2]: https:\/\/www.kaggle.com\/nickuzmenkov\/feedback-prize-roberta-tokens-1024","173b75b2":"The tricky part\n---------------\n\nIn the training data, each\u00a0**word**\u00a0is marked by one of 8 classes (lead, position, evidence, claim, counterclaim, rebuttal, concluding statement, or none of those). But what tokenizer produces is\u00a0**tokens**\u00a0that are loosely connected to words. Tokens\u00a0**can be**\u00a0words as well as word parts, special signs (e.g. text start\u00a0`0`\u00a0and text end\u00a0`2`), or punctuation marks:\n","97bcf062":"But what we have is word-wise labels, not token-wise labels. So we have to map the former to the latter.\n\nRun tokenizing\n--------------"}}