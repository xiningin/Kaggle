{"cell_type":{"09b81f68":"code","60404e0e":"code","65d13991":"code","bea6668d":"code","acc732f1":"code","186efccc":"code","1417ba94":"code","4fe893b3":"code","df3a190c":"code","bdae91f4":"code","93ce3bbc":"markdown","eb1c5e43":"markdown","48a4984d":"markdown"},"source":{"09b81f68":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","60404e0e":"def sigmoid(theta, X):\n    return 1 \/ (1 + np.exp((-np.matmul(X,theta.transpose()))))","65d13991":"def calculate_cost(theta, X, y, lbda): # theta is dimensions n x 1, X is dimensions m x n, y is dimensions m x 1, lbda is regularization constant\n    m = X.shape[0]\n    h = sigmoid(theta, X)\n    cost = (1\/m)*(-y*np.log(h)-(1-y)*np.log(1-h)).sum() + (lbda \/ (2*m))*np.square(theta).sum()\n    cost -= (lbda \/ (2*m)) * theta[0]**2 # remove contribution of theta_zero as it should not be included in cost\n    return cost","bea6668d":"def calculate_grad(theta, X, y, lbda):\n    m = X.shape[0]\n    h = sigmoid(theta, X)\n    grad = np.matmul(X.transpose(),h - y) # vectorized implementation of gradient\n    grad += (lbda\/m) * theta\n    grad[0] -= (lbda\/m) * theta[0] # remove contribution of theta_zero as it should not be included in grad calculation\n    return grad","acc732f1":"def logistic_regression(X, y, alpha, iterations, test_X, test_y):\n    theta = np.random.rand(X.shape[1]) # randomly initiates weights\n    m = X.shape[0]\n    costs_train = []\n    costs_test = []\n    for i in range(iterations):\n        costs_train.append(calculate_cost(theta, X, y, 1))\n        theta -= alpha * (1\/m)*calculate_grad(theta, X ,y, 1)\n        costs_test.append(calculate_cost(theta, test_X, test_y, 1))\n    x_graph = np.arange(0,iterations,1);    \n    plt.plot(x_graph,costs_train, label='train') \n    plt.plot(x_graph,costs_test, label='test')\n    plt.legend()\n    return theta  ","186efccc":"def predict(theta, X, threshold):\n    pred = sigmoid(theta, X)\n    pred_result = (pred>=threshold).astype(int) # those above threshold = 1, 0 otherwise\n    return pred_result","1417ba94":"def normalize(X, mean, std):\n    return (X-mean) \/ std","4fe893b3":"df = pd.read_csv(\"\/kaggle\/input\/diabetes-dataset\/diabetes2.csv\")\ndf.isnull().sum()\nprint(len(df[df['Outcome']==1]))\nprint(len(df[df['Outcome']==0])) # making sure data is balanced, a bit of imbalance but should be okay\ntrain=df.sample(frac=0.75,random_state=150) #random state is a seed value\ntest=df.drop(train.index)\n\ntrain_x = train.loc[:,train.columns != \"Outcome\"] # splitting dependent and independent variables\ntest_x = test.loc[:,test.columns != \"Outcome\"]\ntrain_y = train['Outcome'].values\ntest_y = test['Outcome'].values\n\ntrain_mean = train_x.mean(axis=0) # mean normalization\ntrain_std = train_x.std(axis=0)\ntrain_x = normalize(train_x,train_mean ,train_std)\ntest_x = normalize(test_x,train_mean ,train_std )\n\ntrain_x.insert(0, 'One', 1) # adding column of ones for theta that is independent of features\ntest_x.insert(0, 'One', 1)","df3a190c":"theta = logistic_regression(train_x.values, train_y, 0.05, 500, test_x.values, test_y)","bdae91f4":"pred_y = predict(theta, test_x.values, 0.6)\nresult = pred_y == test_y\npred_train_y = predict(theta, train_x.values, 0.6)\nresult_train = pred_train_y == train_y\nprint(sum(result) \/ len(result))","93ce3bbc":"<h2> Calculates cost with regularization <\/h2>\n<h2> Note that $ \\theta_{0} $ is not included in the calculation of cost <\/h2>\n<h1> $$ \\frac{1}{m} \\sum_{i=1}^m (-y^{(i)}*ln(h_{\\theta}(x^{(i)}))-(1-y^{(i)})ln(1-h_{\\theta}(x^{(i)}))) + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$ <\/h1>\n<h4> where m = number of training examples, n = number of weights, $y^{(i)}$ and $x^{(i)}$ are training example i <\/h4>","eb1c5e43":"<h2> Vectorized implementation gradient calculation together with regularization <\/h2>\n<h2> Note that for $\\theta_{0}$, regularization is not needed <\/h2>\n<h1> $$ \\frac{\\partial J}{\\partial \\theta_{j}} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} + \\frac{\\lambda}{m} \\theta_{j}$$ <\/h1>\n<h3> for j = 1,2,...,n where n is the number of weights theta, and m is the number of training examples <\/h3>","48a4984d":"<h2> Hypothesis which uses the following sigmoid function <\/h2>\n<h1> $$ \\frac{1}{(1 + e^{-\\theta^\\intercal X})} $$ <\/h1>"}}