{"cell_type":{"ff36a39b":"code","4ba4e5ef":"code","1ad8378c":"code","2c0bd26a":"code","8f3106c8":"code","39d2ac5a":"code","ff575171":"code","273b58d1":"code","7dc642fb":"code","5351ce8e":"code","2c769b4c":"code","777092ff":"code","4cc6bb78":"code","b267b5f0":"code","b32db43f":"code","536a3046":"code","588e28d9":"code","79066178":"code","460fa149":"code","cab0d6e7":"code","acfc8b85":"code","2b6858a0":"code","525e3e02":"code","834d5285":"code","190cbfd6":"code","43e4f20b":"code","3eed05d2":"code","0061e37d":"code","f59109f1":"code","eb26bd59":"code","a3565de6":"code","0bd42775":"code","1870a678":"code","b71f3f3a":"code","1d95c68d":"code","f633abe6":"code","f3fbb64d":"code","ce574e67":"code","37544549":"code","76c50c14":"code","724bb5c7":"code","c3bce76a":"code","274c876f":"code","3165e6dd":"code","3410805d":"code","bd8c64f0":"code","ed0d55fa":"code","cca5a6b8":"code","4301f091":"markdown","ca29dfd1":"markdown","5361b2e6":"markdown","3f6241e2":"markdown","fdf74fd0":"markdown","ad00ca32":"markdown","b0e9fd02":"markdown","aaeccc21":"markdown","91ec51e5":"markdown","ba849ef5":"markdown","4be40372":"markdown","fbdaa2e5":"markdown","85cfee68":"markdown","4c833b27":"markdown","e989b0a3":"markdown","2ff01fb2":"markdown","abfbd525":"markdown","d6b42282":"markdown","27aed641":"markdown","085af789":"markdown","0e934835":"markdown","064e5b6a":"markdown","38707427":"markdown","230503ad":"markdown","506c85da":"markdown","fdf267b4":"markdown"},"source":{"ff36a39b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4ba4e5ef":"from sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, learning_curve, RandomizedSearchCV, KFold, cross_validate\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV, Lasso, LassoCV, LassoLarsIC, Ridge, RidgeCV, TweedieRegressor, BayesianRidge, LinearRegression\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew, boxcox_normmax\nfrom scipy.special import boxcox1p\n\n\n# Setting plot styling.\nplt.style.use('seaborn')","1ad8378c":"train_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nfull_data = pd.concat([train_df, test_df]).reset_index(drop=True)\n\nprint('train_df\\t{}'.format(train_df.shape))\nprint('test_df \\t{}'.format(test_df.shape))\nprint('full_data \\t{}'.format(full_data.shape))\n\ntrain_df.head()","2c0bd26a":"# Check different columns between train and test columns\nset(train_df.columns) - set(test_df.columns)","8f3106c8":"# Drop columns 'Id'\ntrain_df.drop(columns = 'Id', inplace=True)\ntest_df.drop(columns = 'Id', inplace=True)\nfull_data.drop(columns = 'Id', inplace=True)\nprint('Drop column Id completed')","39d2ac5a":"plt.subplots(figsize=(12,9))\n\ncorrmat = train_df.corr()\nsns.heatmap(corrmat, mask = corrmat < 0.75, linewidth = 0.5, cmap = 'Blues');","ff575171":"top_corr = abs(corrmat.SalePrice).sort_values(ascending=False).head(10)\ntop_corr_col = list(top_corr.index)\ntop_corr_col.remove('SalePrice')\ntop_corr","273b58d1":"plt.subplots(figsize=(12,9))\n\ncorrmat = train_df[top_corr_col].corr()\nsns.heatmap(corrmat, linewidth = 0.5, cmap = 'Blues');","7dc642fb":"# Plot variable vs target variable(SalePrice)\ndef plot_(df, col):\n    if df[col].dtype != 'object':\n        if len(df[col].unique()) <= 12:\n            fig, ax = plt.subplots(1,2,figsize=(12,6))\n            sns.stripplot(x=col, y='SalePrice', alpha = 0.5, data=df, ax=ax[0])\n            sns.countplot(df[col], ax=ax[1])\n            fig.suptitle(str(col) + ' analysis')\n        else:\n            fig, ax = plt.subplots(1,2,figsize=(12,6))\n            sns.scatterplot(x=col, y='SalePrice', alpha = 0.5, data=df, ax=ax[0])\n            sns.distplot(df[col], kde=False, ax=ax[1])\n            fig.suptitle(str(col) + ' analysis')\n    else:\n        fig, ax = plt.subplots(1,2,figsize=(12,6), sharey=True)\n        sns.stripplot(x=col, y='SalePrice', alpha = 0.5, data=df, ax=ax[0])\n        sns.boxplot(x=col, y='SalePrice', data=df, ax=ax[1])\n        fig.suptitle(str(col) + ' analysis')\n        \n# Explore missing data\ndef plot_missing(df):\n    miss_col = df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending = False)\n    percent_miss = round((miss_col \/ len(df) *100),2)\n    missing = pd.DataFrame([miss_col, percent_miss]).T.rename(columns = {0:'Feature', 1:'% missing'})\n    return missing\n\n# Sort category vs target variable\ndef sort_cate(df, col, target=\"SalePrice\"):\n    \n    \n    print(\"{}  | type: {}\\n\".format(col, df[col].dtype))\n    d = pd.DataFrame({\"n\": df[col].value_counts(),\n                                \"Ratio\": 100 * df[col].value_counts() \/ len(df),\n                                \"TARGET_MEDIAN\": df.groupby(col)[target].median(),\n                                \"Target_MEAN\": df.groupby(col)[target].mean()}).sort_values(by='TARGET_MEDIAN', ascending=False)\n    print(d, end=\"\\n\\n\")\n    \n    fig, ax = plt.subplots(1,2,figsize=(20,6), sharey=True)\n    sns.stripplot(x=col, y='SalePrice', data=df, alpha = 0.5, order = d.index,  ax=ax[0])\n\n    sns.boxplot(x=col, y='SalePrice', data=df, order = d.index, ax=ax[1])\n    fig.suptitle(str(col) + ' analysis')\n    \n    fig.autofmt_xdate(rotation=45)\n#     plt.xticks(rotation=45)\n#     set_xticklabels(chart.get_xticklabels(ax=ax[1]), rotation=45, horizontalalignment='right')\n    plt.show();\n\nprint('Plot functions are ready to use')","5351ce8e":"sort_cate(train_df, 'Neighborhood')","2c769b4c":"# for col in top_corr_col:\n#     plot_(train_df, col)","777092ff":"train_df = train_df.drop(train_df[(train_df['OverallQual'] < 5) & (train_df['SalePrice'] > 200000)].index)\ntrain_df = train_df.drop(train_df[(train_df['OverallQual'] > 9) & (train_df['SalePrice'] < 200000)].index)\n\n\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea'] > 4000) & (train_df['SalePrice'] < 200000)].index)\n\ntrain_df = train_df.drop(train_df[(train_df['GarageArea'] > 1200) & (train_df['SalePrice'] < 200000)].index)\n\ntrain_df = train_df.drop(train_df[(train_df['TotalBsmtSF'] > 3000) & (train_df['SalePrice'] < 200000)].index)\n\ntrain_df = train_df.drop(train_df[(train_df['1stFlrSF'] > 3000) & (train_df['SalePrice'] < 200000)].index)","4cc6bb78":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\n\nax1.set_title('Distplot')\nsns.distplot(train_df['SalePrice'], fit=norm,  ax = ax1)\n\nax2.set_title('Boxplot')\nsns.boxplot(train_df['SalePrice'], ax = ax2)\n\n\nprint ('Skewness: ', np.round(train_df['SalePrice'].skew(), 2))\nprint ('Kurtosis: ', np.round(train_df['SalePrice'].kurt(), 2))","b267b5f0":"# Apply log(1+x) in order to transform target variable\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n\nfig, ax = plt.subplots(1, 2, figsize = (12, 4))\nax1.set_title('Distplot Log-transformation')\nsns.distplot(train_df['SalePrice'],fit=norm,  ax = ax[0])\n\nax2.set_title('Boxplot Log-transformation')\nsns.boxplot(train_df['SalePrice'], ax = ax[1])\n\nprint ('Skewness: ', np.round(train_df['SalePrice'].skew(), 2))\nprint ('Kurtosis: ', np.round(train_df['SalePrice'].kurt(), 2))\n","b32db43f":"plot_missing(test_df)","536a3046":"none_cols = ['Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageType',\n             'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n             'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType']\n\n# Impute missing value\nfor col in none_cols:\n    train_df[col].replace(np.nan, 'None', inplace=True)\n    test_df[col].replace(np.nan, 'None', inplace=True)","588e28d9":"bsm=['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath','BsmtHalfBath','BsmtQual']\ntrain_df[bsm].groupby('BsmtQual').sum()","79066178":"gar = ['GarageYrBlt', 'GarageArea', 'GarageCars','GarageQual']\ntrain_df[gar].groupby('GarageQual').sum()","460fa149":"# Replace these features by 0 \nzero_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath','BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea']\n\nfor col in zero_cols:\n    train_df[col].replace(np.nan, 0, inplace=True)\n    test_df[col].replace(np.nan, 0, inplace=True)","cab0d6e7":"# Replace these features by most frequency value or mode\nmost_freq_cols = ['Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual','SaleType', 'Utilities']\n\nfor col in most_freq_cols:\n    train_df[col].replace(np.nan, train_df[col].mode()[0], inplace=True)\n    test_df[col].replace(np.nan, test_df[col].mode()[0], inplace=True)","acfc8b85":"fig, ax = plt.subplots(1,1,figsize = (10, 8))\nsns.countplot(x=\"MSSubClass\", hue=\"MSZoning\", data=train_df);","2b6858a0":"train_df['MSZoning'] = train_df.groupby('MSSubClass')['MSZoning'].apply(\n    lambda x: x.fillna(x.mode()[0]))\ntest_df['MSZoning'] = test_df.groupby('MSSubClass')['MSZoning'].apply(\n    lambda x: x.fillna(x.mode()[0]))\n\n\ntrain_df[\"LotFrontage\"] = train_df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\ntest_df[\"LotFrontage\"] = test_df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n\n# Change type to str\nchg_type = ['MSSubClass', 'YrSold', 'MoSold']\ntrain_df[chg_type] = train_df[chg_type].astype(str)\ntest_df[chg_type] = test_df[chg_type].astype(str)","525e3e02":"sort_cate(train_df, 'Neighborhood')","834d5285":"# Transform categorical to ordinal features\ndef cate_to_ordinal(df):\n    \n    n_map = {'MeadowV': 1,'IDOTRR': 1,'BrDale': 1,'OldTown': 2,'Edwards': 2,'BrkSide': 2,\n                 'Sawyer': 3,'Blueste': 3,'SWISU': 3,'NPkVill': 3,'NAmes': 3,'Mitchel': 4,\n                 'SawyerW': 5,'NWAmes': 5,'Gilbert': 5,'Blmngtn': 5,'CollgCr': 5,\n                 'ClearCr': 6,'Crawfor': 6,'Veenker': 7,'Somerst': 7,'Timber': 7,\n                 'StoneBr': 8,'NoRidge': 9,'NridgHt': 10}\n    df['Neighborhood'] = df['Neighborhood'].map(n_map).astype('int')\n\n    ext_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n    df['ExterQual'] = df['ExterQual'].map(ext_map).astype('int')\n\n    ext_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n    df['ExterCond'] = df['ExterCond'].map(ext_map).astype('int')\n\n    bsm_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n    df['BsmtQual'] = df['BsmtQual'].map(bsm_map).astype('int')\n    df['BsmtCond'] = df['BsmtCond'].map(bsm_map).astype('int')\n\n    bsmf_map = {'None': 0,'Unf': 1,'LwQ': 2,'Rec': 3,'BLQ': 4,'ALQ': 5,'GLQ': 6}\n    df['BsmtFinType1'] = df['BsmtFinType1'].map(bsmf_map).astype('int')\n    df['BsmtFinType2'] = df['BsmtFinType2'].map(bsmf_map).astype('int')\n\n    heat_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n    df['HeatingQC'] = df['HeatingQC'].map(heat_map).astype('int')\n    df['KitchenQual'] = df['KitchenQual'].map(heat_map).astype('int')\n    df['FireplaceQu'] = df['FireplaceQu'].map(bsm_map).astype('int')\n    df['GarageCond'] = df['GarageCond'].map(bsm_map).astype('int')\n    df['GarageQual'] = df['GarageQual'].map(bsm_map).astype('int')\n    \ncate_to_ordinal(train_df)\ncate_to_ordinal(test_df)","190cbfd6":"train_df['TotalSF'] = train_df['BsmtFinSF1'] + train_df['BsmtFinSF2'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']\n\ntest_df['TotalSF'] = test_df['BsmtFinSF1'] + test_df['BsmtFinSF2'] + test_df['1stFlrSF'] + test_df['2ndFlrSF']","43e4f20b":"num_col = list(train_df.select_dtypes(exclude='object').columns)\nnum_col.remove('SalePrice')\n\n# Get skewness of numerical feature\nskew_feature = abs(train_df[num_col].apply(lambda x: skew(x))).sort_values(ascending = False)\n\n# Filter feature that has skewness > 0.75 \nhigh_skew = skew_feature[skew_feature > 0.75]\n\n# Transform by using Boxcox-Transformation\n\nfor f in high_skew.index:\n    train_df[f] = boxcox1p(train_df[f], boxcox_normmax(train_df[f] + 1))\n    test_df[f] = boxcox1p(test_df[f], boxcox_normmax(test_df[f] + 1))","3eed05d2":"train_df['Utilities'].value_counts(normalize = True).iloc[0]","0061e37d":"thd = 0.95\ndrop_col = []\n\nfor column in train_df.drop('SalePrice', axis = 1):\n    \n    most_freq_value = train_df[column].value_counts(normalize = True).iloc[0]\n    if most_freq_value > thd:\n        drop_col.append(column)\n        print ('{}: \\t {}% same value'.format(column, np.round(most_freq_value, 3)))","f59109f1":"drop_col += ['GarageYrBlt','TotRmsAbvGrd', '1stFlrSF', 'GarageCars']\n# Drop column\ntrain_df.drop(columns = drop_col, inplace=True)\ntest_df.drop(columns = drop_col, inplace=True)\n\nprint ('Drop Columns Completed')","eb26bd59":"y_train = train_df['SalePrice']\ntrain_X = train_df.drop(columns = 'SalePrice')\n\nprint ('Splitting \"SalePrice\" Completed')","a3565de6":"from sklearn.preprocessing import OneHotEncoder\n\n# Get categorical columns\ncat_col = train_X.select_dtypes(include='object').columns\nnum_col = train_X.select_dtypes(exclude='object').columns\n\n# creating instance of one-hot-encoder\nenc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n\n# Apply one-hot encoder to each column with categorical data\nOH_cols_train = pd.DataFrame(enc.fit_transform(train_X[cat_col]))\nOH_cols_test = pd.DataFrame(enc.transform(test_df[cat_col]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train_X.index\nOH_cols_test.index = test_df.index\n\n# Put columns name into One-hot encoding columns\nOH_cols_train.columns = enc.get_feature_names(cat_col)\nOH_cols_test.columns = enc.get_feature_names(cat_col)\n\n# Concat Categorical with Numerical columns\nX_train = pd.concat([train_X[num_col], OH_cols_train], axis=1)\nX_test = pd.concat([test_df[num_col], OH_cols_test], axis=1)","0bd42775":"# Basic LinearRegression\n\nprint('='*30 +'BASE MODEL' + '='*30)\nslr = LinearRegression()\nslr.fit(X_train, y_train)\nprint('slope \uff1a {:0.2f}'.format(slr.coef_[0]))\nprint('intercept : {:0.2f}'.format(slr.intercept_))\nprint('\\n')\ny_pre = slr.predict(X_train)\ny_test = slr.predict(X_test)\nRMSE = -1*cross_val_score(slr, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error').mean()\nprint(f'RMSE: {RMSE}')","1870a678":"models = [('LinearRegression', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('RandomForestRegressor', RandomForestRegressor()),\n          ('GradientBoostingRegressor', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor()),\n          (\"LightGBM\", LGBMRegressor()),\n          (\"CatBoost\", CatBoostRegressor(verbose=False))]\n\nprint('='*30 +'RMSE BASE MODEL' + '='*30)\n\nmean_rmse = []\nsd_rmse = []\nmodel_name = []\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    RMSE = -1*cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n    print(f'{name:<25} {RMSE.mean():.5f} \u00b1{RMSE.std():.3f}')\n    \n    mean_rmse.append(RMSE.mean())\n    sd_rmse.append(RMSE.std())\n    model_name.append(name)\n\n    \nbase = pd.DataFrame({'model':model_name, 'mean_rmse':mean_rmse, 'sd_rmse':sd_rmse})\nsns.barplot(x=\"mean_rmse\", y=\"model\", data=base, orient = 'h',**{'xerr': sd_rmse})\nplt.title('Baseline: Cross Validation Scores')\nplt.axvline(x = np.mean(mean_rmse), color = 'firebrick', linestyle = '--');","b71f3f3a":"# Ridge\npipe = make_pipeline(RobustScaler(), Ridge())\n\nparam_grid = {'ridge__alpha': [0.01, 0.1, 1, 10, 15 ,20, 25, 30]}\n\ngrid_rid = GridSearchCV(pipe, param_grid = param_grid, cv = 5, scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n\nbest_grid_rid = grid_rid.fit(X_train, y_train)\n\nrid_param = best_grid_rid.best_params_\n\nprint(f'best_params_: {rid_param}')\nprint(f'score: {-1*best_grid_rid.best_score_:.5f}')","1d95c68d":"# Lasso\npipe = make_pipeline(Lasso())\n\nparam_grid = {'lasso__alpha': [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008],\n              'lasso__max_iter': [1e3, 1e5, 1e7]}\n\ngrid_las = GridSearchCV(pipe, param_grid = param_grid, cv = 5, scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n\nbest_grid_las = grid_las.fit(X_train, y_train)\n\nlas_param = best_grid_las.best_params_\n\nprint(f'best_params_: {las_param}')\nprint(f'score: {-1*best_grid_las.best_score_:.5f}')","f633abe6":"# ElasticNet\npipe = make_pipeline(ElasticNet())\n\nparam_grid = {'elasticnet__alpha': [0.0001, 0.001, 0.01, 0.1, 1], # constant that multiplies the penalty terms, default = 1.0\n              'elasticnet__l1_ratio': [0.25, 0.5, 0.75]} # the mixing parameter, default = 0.5\n\ngrid_en = GridSearchCV(pipe, param_grid = param_grid, cv = 5,scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n\nbest_grid_en = grid_en.fit(X_train, y_train)\n\nen_param = best_grid_en.best_params_\n\nprint(f'best_params_: {en_param}')\nprint(f'score: {-1*best_grid_en.best_score_:.5f}')","f3fbb64d":"# RandomForestRegressor\nrf = RandomForestRegressor()\n\nparam_grid = {'bootstrap': [True, False],\n             'max_depth': [10, 25, 50],\n             'max_features': ['auto', 'sqrt'],\n             'min_samples_leaf': [1, 2, 3],\n             'min_samples_split': [2, 3],\n             'n_estimators': [300,500]\n             }\n\ngrid_rf = RandomizedSearchCV(rf, param_distributions = param_grid, cv = 5,scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n\nbest_grid_rf = grid_rf.fit(X_train, y_train)\n\nrf_param = best_grid_rf.best_params_\n\nprint(f'best_params_: {rf_param}')\nprint(f'score: {-1*best_grid_rf.best_score_:.5f}')","ce574e67":"# GradientBoostingRegressor\ngbr = GradientBoostingRegressor()\n\nparam_grid = {'loss': ['ls', 'huber'],  # loss function to be optimized (default = \u2019ls\u2019)\n              'n_estimators': [200, 300],    # The number of boosting stages to perform (default = 100)\n              'learning_rate': [0.01, 0.1, 1],  # (default = 0.1)\n              'min_samples_split': [3, 5, 10],  # The minimum number of samples required to split an internal node (default = 2)\n              'max_depth': [3, 5, 10]}  # maximum depth of the individual regression estimators (default = 3)\n\ngrid_gbr = RandomizedSearchCV(gbr, param_distributions = param_grid, cv = 5,scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n\nbest_grid_gbr = grid_gbr.fit(X_train, y_train)\n\ngbr_param = best_grid_gbr.best_params_\n\nprint(f'best_params_: {gbr_param}')\nprint(f'score: {-1*best_grid_gbr.best_score_:.5f}')","37544549":"# XGBRegressor\nxgb = XGBRegressor()\n\nparam_grid = {'learning_rate' : [0.01, 0.1, 0.5], # Boosting learning rate\n              'n_estimators' : [200],       # Number of gradient boosted trees\n              'max_depth' : [3, 6, 10],     # Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n              'min_child_weight' : [1, 5, 10],  # Increasing this value will make model more conservative.\n              'reg_alpha' : [0.01, 0.1, 1],  # Increasing this value will make model more conservative.\n              'reg_lambda' : [0.01, 0.1, 1]}  # Increasing this value will make model more conservative.\n\ngrid_xgb = RandomizedSearchCV(xgb, param_distributions = param_grid, cv = 5,scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n\nbest_grid_xgb = grid_xgb.fit(X_train, y_train)\n\nxgb_param = best_grid_xgb.best_params_\n\nprint(f'best_params_: {xgb_param}')\nprint(f'score: {-1*best_grid_xgb.best_score_:.5f}')","76c50c14":"# LGBMRegressor\nlgbm = LGBMRegressor()\n\nparam_grid = {'max_depth' : [5, 10, 15],    # Maximum tree depth, <=0 means no limit (default = -1)\n              'learning_rate' : [0.01, 0.1, 0.2],    # Boosting learning rate (default = 0.1)\n              'n_estimators' : [250, 500, 1000],   # Number of boosted trees to fit (default = 100)\n              'feature_fraction' : [0.4, 0.6, 0.8], # \n              'min_child_samples' : [5, 15, 25]} # Minimum number of data needed in a leaf (default = 20)\n\ngrid_lgbm = RandomizedSearchCV(lgbm, param_distributions = param_grid, cv = 5,scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n\nbest_grid_lgbm = grid_lgbm.fit(X_train, y_train)\n\nlgbm_param = best_grid_lgbm.best_params_\n\nprint(f'best_params_: {lgbm_param}')\nprint(f'score: {-1*best_grid_lgbm.best_score_:.5f}')","724bb5c7":"# CatBoostRegressor\ncat = CatBoostRegressor()\n\nparam_grid = {'max_depth' : [5, 10, 15],    # Maximum tree depth, <=0 means no limit (default = -1)\n              'learning_rate' : [0.0001, 0.001, 0.01, 0.1],    # Boosting learning rate (default = 0.1)\n              'n_estimators' : [100, 300, 500, 1000, 1300, 1600],   # Number of boosted trees to fit (default = 100)\n              'l2_leaf_reg' : [0.001, 0.01, 0.1],\n              'feature_fraction' : [0.4, 0.6, 0.8], # \n              'min_child_samples' : [2, 5, 10, 15, 20]} # Minimum number of data needed in a leaf (default = 20)\n\ngrid_cat = RandomizedSearchCV(lgbm, param_distributions = param_grid, cv = 5,scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n\nbest_grid_cat = grid_cat.fit(X_train, y_train)\n\ncat_param = best_grid_cat.best_params_\n\nprint(f'best_params_: {cat_param}')\nprint(f'score: {-1*best_grid_cat.best_score_:.5f}')","c3bce76a":"svr_scores = cross_val_score(best_grid_en.best_estimator_, X_train, y_train,\n                             cv=5, n_jobs=-1, error_score=\"neg_root_mean_squared_error\")\n\nsvr_scores.mean()","274c876f":"stack = StackingCVRegressor(regressors = (best_grid_en.best_estimator_,\n                                          best_grid_rid.best_estimator_,\n                                          best_grid_las.best_estimator_,\n#                                           best_grid_rf.best_estimator_,\n                                          best_grid_gbr.best_estimator_,\n#                                           best_grid_xgb.best_estimator_,\n                                          best_grid_lgbm.best_estimator_,\n                                         best_grid_cat.best_estimator_),\n                            meta_regressor = best_grid_xgb.best_estimator_, \n                            use_features_in_secondary = True)\n\n\nstack.fit(X_train, y_train);\n\nstack_score = -cross_val_score(stack, X_train, y_train, scoring = 'neg_root_mean_squared_error', cv = 5)\n\nprint(f'score: {stack_score.mean():.5f} \u00b1{stack_score.std():.4f}')","3165e6dd":"models = [(\"Ridge\", best_grid_rid.best_estimator_),\n          (\"Lasso\", best_grid_las.best_estimator_),\n          (\"ElasticNet\", best_grid_en.best_estimator_),\n          ('RandomForest', best_grid_rf.best_estimator_),\n          ('GradientBoosting', best_grid_gbr.best_estimator_),\n          (\"XGBoost\", best_grid_xgb.best_estimator_),\n          (\"LightGBM\", best_grid_lgbm.best_estimator_),\n          (\"CatBoost\", best_grid_cat.best_estimator_)]\n\nprint('='*30 +'RMSE TUNE MODEL' + '='*30)\n\nmean_rmse = []\nsd_rmse = []\nmodel_name = []\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    RMSE = -1*cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n    print(f'{name:<25} {RMSE.mean():.5f} \u00b1{RMSE.std():.3f}')\n    \n    mean_rmse.append(RMSE.mean())\n    sd_rmse.append(RMSE.std())\n    model_name.append(name)\n\nmean_rmse.append(stack_score.mean())\nsd_rmse.append(stack_score.std())\nmodel_name.append('Stacking')\n\ntune = pd.DataFrame({'model':model_name, 'mean_rmse':mean_rmse, 'sd_rmse':sd_rmse})\n\n\nsns.barplot(x=\"mean_rmse\", y=\"model\", data=tune, orient = 'h',**{'xerr': sd_rmse})\nplt.title('TUNE: Cross Validation Scores')\nplt.axvline(x = np.mean(mean_rmse), color = 'firebrick', linestyle = '--');","3410805d":"model_names = ['Ridge', 'Lasso', 'Elastic_Net', 'RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM', 'CatBoost', 'StackingRegressor']\nmodels = [best_grid_rid, best_grid_las, best_grid_en, best_grid_rf, best_grid_gbr, best_grid_xgb, best_grid_lgbm, best_grid_cat, stack]\n\nsubmission_ = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nfor i in range(len(models)):\n    \n    y_pred = models[i].predict(X_test)\n    \n    y_pred_final = np.expm1(y_pred)\n    \n    submission_['SalePrice'] = y_pred_final\n    \n    submission_ = submission_[['Id', 'SalePrice']]\n    \n    submission_.to_csv('{}-{}.csv'.format(i+1, model_names[i]), index = False)\n\nprint ('Submission files created!')","bd8c64f0":"# Blending models by assigning weights:\n\ndef blend_models_predict(X):\n    \n    return ((0.1 * best_grid_rid.predict(X)) +\n            (0.1 * best_grid_las.predict(X)) +\n            (0.1 * best_grid_en.predict(X)) +\n            (0.1 * best_grid_rf.predict(X)) +\n            (0.1 * best_grid_gbr.predict(X)) +\n            (0.1 * best_grid_xgb.predict(X)) +\n            (0.1 * best_grid_lgbm.predict(X)) +\n            (0.1 * best_grid_cat.predict(X)) +\n            (0.2 * stack.predict(X)))\n","ed0d55fa":"submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\n# Inversing and flooring log scaled sale price predictions\nsubmission['SalePrice'] = np.floor(np.expm1(blend_models_predict(X_test)))\n\nsubmission = submission[['Id', 'SalePrice']]\nsubmission.head()","cca5a6b8":"submission.to_csv('mysubmission7.csv', index=False)\nprint('Save submission')","4301f091":"`MSZoning` should be considered to replace NaN by gruop of `MSSubClass` \n\n`LotFrontage` relate to `Neighborhood` so we will replace NaN by gruop of `Neighborhood`\n\nFor `MSSubClass`, `YrSold`, `MoSold` should be change to str","ca29dfd1":"## Ensemble Algorithms <a id=\"3.2\"><\/a>","5361b2e6":"# Prediction <a id=\"4\"><\/a>","3f6241e2":"We would create useful new features from existing feature in order to add more information to target variable.","fdf74fd0":"Like if **No Garage** `GarageYrBlt`, `GarageArea`, `GarageCars` should be replaced with 0 also.","ad00ca32":"## Missing Value <a id=\"2.2\"><\/a>","b0e9fd02":"There are some data points that are far from other data points. We would remove outliers which are problematic for many statistical analyses because they can distort real results.","aaeccc21":"We will see the correlated variables which are linear relationship highly that might be multicollinearity.\n\nMulticollinearity is a correlated with another independent variable in some case two or more other independent variables so that can reduce the performance of some algorithm. Estimates for regression coefficients can be unreliable and tests of significance for regression coefficients can be misleading.\n\nAs figure above there are some variable are correlate:\n* `GarageYrBlt` and `YearBuilt`\n* `TotRmsAbvGrd` and `GrLivArea`\n* `1stFlrSF` and `TotalBsmtSF`\n* `GarageArea` and `GarageCars`\n\n**We would drop some of these variables later.**","91ec51e5":"# Data Preprocessing <a id=\"2\"><\/a>","ba849ef5":"### Bivariate Analysis \nFirst we have to understand overall data by take a quick look and visualize them easy to gain information.\n\n**Check corralation**","4be40372":"We would focus on the variables that are high correlation to **target variable** and then consider to eliminate outliers because they are values that are notably different from other data points, and they can cause problems like miss significant findings or distort real results.","fbdaa2e5":"## Load Data <a id=\"1.1\"><\/a>","85cfee68":"<img src=\"https:\/\/thailand-property-news.knightfrank.co.th\/wp-content\/uploads\/2017\/12\/pexels-photo-728428.jpeg\" width = 1200>\n","4c833b27":"We use Stacking is an ensemble learning technique to combine multiple regression models via a meta-regressor. In this case we use XGBRegressor as meta-regressor.","e989b0a3":"## Hyperparameters Tuning <a id=\"3.1\"><\/a>","2ff01fb2":"For other features such as `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF`, `TotalBsmtSF`, `BsmtFullBath`, `BsmtHalfBath` all of these are numerical features if **No basement** that should be replaced with 0.","abfbd525":"# Table of Contents\n1. [Data Preparation](#1)\n    * [1.1 Load Data](#1.1)  \n    * [1.2 Explore the data](#1.2)  \n2. [Data Preprocessing](#2)  \n    * [2.1 Dealing with Outlierss](#2.1)\n    * [2.2 Missing Value](#2.2)\n    * [2.3 Feature Engineering](#2.3)\n3. [Modeling](#3)\n    * [3.1 Hyperparameters Tuning](#3.1)\n    * [3.2 Ensemble Algorithms](#3.2)\n4. [Prediction](#4)","d6b42282":"# Modeling <a id=\"3\"><\/a>","27aed641":"**Target Variable (SalePrice)**","085af789":"## Dealing with Outlierss <a id=\"2.1\"><\/a>","0e934835":"## Explore the data <a id=\"1.2\"><\/a>","064e5b6a":"# Data Preparation <a id=\"1\"><\/a>","38707427":"`PoolQC`: Pool quality\n\t\t\n       Ex\tExcellent\n       Gd\tGood\n       TA\tAverage\/Typical\n       Fa\tFair\n       NA\tNo Pool\n\n`MiscFeature`: Miscellaneous feature not covered in other categories\n\t\t\n       Elev\tElevator\n       Gar2\t2nd Garage (if not described in garage section)\n       Othr\tOther\n       Shed\tShed (over 100 SF)\n       TenC\tTennis Court\n       NA\tNone\n\n`Alley`: Type of alley access to property\n\n       Grvl\tGravel\n       Pave\tPaved\n       NA \tNo alley access\n       \n`BsmtQual`: Evaluates the height of the basement\n\n       Ex\tExcellent (100+ inches)\t\n       Gd\tGood (90-99 inches)\n       TA\tTypical (80-89 inches)\n       Fa\tFair (70-79 inches)\n       Po\tPoor (<70 inches\n       NA\tNo Basement\n       \nFor some feature such as `PoolQC`, `MiscFeature` and `Alley` there are moer than 90% of missing value but in this case NaN mean **No Pool**, **No Miscellaneous** and **No alley access**.\n\nWe will replace NaN with '**None**' and for the other features we would take into account and impute NaN to the following column.","230503ad":"Target variable is a positive skewness, so we need to transform data by Log-transformation of the target variable.","506c85da":"## Feature Engineering <a id=\"2.3\"><\/a>","fdf267b4":"If the values of a certain independent variable (feature) are skewed, depending on the model, skewness may violate model assumptions (e.g. logistic regression) or may impair the interpretation of feature importance.\nIn this case we will use Boxcox-Transformation to transform high skewness features"}}