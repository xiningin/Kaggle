{"cell_type":{"5c9a79e5":"code","a80dcd89":"code","6bbdb765":"code","bd9df6c8":"code","094ebed5":"code","3821f43d":"code","04c1bfdd":"code","5b885808":"code","fb60af55":"code","4c20f881":"code","aef8ea89":"code","1b29fe75":"code","dd6a95a1":"code","9beaeaf7":"code","73cea01c":"code","5f089c4b":"code","d1a4a98d":"code","abf5ccfd":"markdown","5c891d48":"markdown","427e59de":"markdown","081e3fff":"markdown"},"source":{"5c9a79e5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a80dcd89":"my_data = pd.read_csv('..\/input\/drug200\/drug200.csv')\nmy_data.head()","6bbdb765":"import numpy as np \nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier","bd9df6c8":"my_data.shape\n","094ebed5":"X = my_data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values\nX[0:5]\n","3821f43d":"from sklearn import preprocessing\nle_sex = preprocessing.LabelEncoder()\nle_sex.fit(['F','M'])\nX[:,1] = le_sex.transform(X[:,1]) \n\n\nle_BP = preprocessing.LabelEncoder()\nle_BP.fit([ 'LOW', 'NORMAL', 'HIGH'])\nX[:,2] = le_BP.transform(X[:,2])\n\n\nle_Chol = preprocessing.LabelEncoder()\nle_Chol.fit([ 'NORMAL', 'HIGH'])\nX[:,3] = le_Chol.transform(X[:,3]) \n\nX[0:5]\n","04c1bfdd":"y = my_data[\"Drug\"]\ny[0:5]","5b885808":"from sklearn.model_selection import train_test_split","fb60af55":"X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=3)","4c20f881":"print('shape of X_trainset ==> ', X_trainset.shape)\nprint('shape of X_testset ==> ', X_testset.shape)\nprint('shape of y_trainset ==> ', y_trainset.shape)\nprint('shape of y_testset ==> ', y_testset.shape)","aef8ea89":"drugTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\ndrugTree # it shows the default parameters","1b29fe75":"drugTree.fit(X_trainset,y_trainset)\npredTree = drugTree.predict(X_testset)\nprint (predTree [0:5])\nprint (y_testset [0:5])","dd6a95a1":"from sklearn import metrics\nimport matplotlib.pyplot as plt\nprint(\"DecisionTrees's Accuracy: \", round(metrics.accuracy_score(y_testset, predTree),3))","9beaeaf7":"# Notice: You might need to uncomment and install the pydotplus and graphviz libraries if you have not installed these before\n!conda install -c conda-forge pydotplus -y\n!conda install -c conda-forge python-graphviz -y","73cea01c":"!python -m pip install --upgrade pip\n    ","5f089c4b":"!pip install pydotplus","d1a4a98d":"from  io import StringIO\nimport pydotplus\nimport matplotlib.image as mpimg\nfrom sklearn import tree\n%matplotlib inline ","abf5ccfd":"**As you may figure out, some features in this dataset are categorical, such as Sex or BP. Unfortunately, Sklearn Decision Trees does not handle categorical variables. We can still convert these features to numerical values using pandas.get_dummies() to convert the categorical variable into dummy\/indicator variables.**","5c891d48":"# **Modeling\nWe will first create an instance of the DecisionTreeClassifier called drugTree.\nInside of the classifier, specify criterion=\"entropy\" so we can see the information gain of each node.**","427e59de":"Now train_test_split will return 4 different parameters. We will name them:\nX_trainset, X_testset, y_trainset, y_testset\n\nThe train_test_split will need the parameters:\nX, y, test_size=0.3, and random_state=3.\n\nThe X and y are the arrays required before the split, the test_size represents the ratio of the testing dataset, and the random_state ensures that we obtain the same splits.","081e3fff":"# **About the dataset\nImagine that you are a medical researcher compiling data for a study. You have collected data about a set of patients, all of whom suffered from the same illness. During their course of treatment, each patient responded to one of 5 medications, Drug A, Drug B, Drug c, Drug x and y.\n\nPart of your job is to build a model to find out which drug might be appropriate for a future patient with the same illness. The features of this dataset are Age, Sex, Blood Pressure, and the Cholesterol of the patients, and the target is the drug that each patient responded to.\n\nIt is a sample of multiclass classifier, and you can use the training part of the dataset to build a decision tree, and then use it to predict the class of an unknown patient, or to prescribe a drug to a new patient.**"}}