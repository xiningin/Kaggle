{"cell_type":{"0170a58d":"code","043c4c5e":"code","2fb5c972":"code","b4caf16e":"code","463fa23f":"code","1a5a6ede":"code","e96533a5":"code","b5faa3ed":"code","01ec16a4":"code","dab0cfae":"code","b8499e4b":"code","1af66fd0":"code","24e21b93":"code","8b1f62ac":"code","f8a36ba4":"code","aae99cd5":"code","b78f156c":"code","b471a1b1":"code","cf137f8e":"code","e7823f0b":"code","1fa18f74":"code","86494004":"code","2cf888c5":"code","003d2a9a":"code","27468697":"code","ecf9187e":"code","5f195b1e":"code","b21f8bf1":"code","5c886dd4":"code","3f2359a4":"code","3607f35d":"code","deaee3e5":"code","90dceae9":"code","35a098d3":"code","2dd4544a":"code","99cb2fce":"code","e51349f4":"code","5b164091":"code","f7c329b6":"code","e6e270d9":"code","0a216a73":"code","5d949b2a":"code","1dbbf3f6":"code","d8fbb517":"code","103eb900":"code","4ee5bece":"code","fcd22848":"code","c97e06ad":"code","50705b32":"code","3e15c7a3":"code","b36d87e7":"code","42730963":"code","aa48613e":"code","e796e82e":"code","1267bb60":"code","e4629b18":"code","17bbb0a6":"code","4b905f60":"code","b6c57205":"code","f2bd6680":"code","c860152c":"code","7f19b6c2":"code","fbd309d4":"code","34d15578":"code","5ec67d14":"code","cd52bbf5":"code","b2a789d8":"code","2c0fe15c":"code","d8de9c76":"code","a68e9276":"code","b3b03dbb":"code","68b900c9":"code","b8544e78":"code","7ba2266b":"markdown","e11bb266":"markdown","fc186cc1":"markdown","10022b16":"markdown","2380efa1":"markdown","a258ebf5":"markdown","9162dc29":"markdown","30556c56":"markdown","8e97abfc":"markdown","f2a49313":"markdown","a8734ec5":"markdown","e048c707":"markdown","cd63d136":"markdown","6eedfc89":"markdown","86ed8ef5":"markdown","2767f032":"markdown","31706dae":"markdown","51008cca":"markdown","437e510e":"markdown"},"source":{"0170a58d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","043c4c5e":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder","2fb5c972":"train = pd.read_csv(\"..\/input\/restaurant-revenue-prediction\/train.csv.zip\")\ntest = pd.read_csv(\"..\/input\/restaurant-revenue-prediction\/test.csv.zip\")","b4caf16e":"train.head(5)","463fa23f":"test.head(5)","1a5a6ede":"display(train.columns)\n","e96533a5":"train.info()","b5faa3ed":"test.columns\n","01ec16a4":"test.info()","dab0cfae":"train[\"revenue\"].describe()","b8499e4b":"sns.distplot(train['revenue'])\n","1af66fd0":"train[\"revenue\"].head()","24e21b93":"train[\"revenue\"].isna().sum()\n","8b1f62ac":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(15, 12))\nsns.heatmap(corrmat, vmax=1, square=True);","f8a36ba4":"train[\"City\"].head()","aae99cd5":"drop_values = ['Id','Open Date']","b78f156c":"train.drop(drop_values,axis=1,inplace=True)\ntest.drop(drop_values,axis=1,inplace=True)","b471a1b1":"pip install dataprep","cf137f8e":"from dataprep.datasets import get_dataset_names\nfrom dataprep.datasets import load_dataset\nfrom dataprep.eda import create_report,plot,plot_missing\nimport scipy.stats as stats","e7823f0b":"# function for overall statistical report\ndef overall_stat(df):\n    # display the overall stat report\n    display(plot(df, display=['Stats', 'Insights']))\n    # display(df.info())\n\n    # store and display the numerical and nonn-numerical cols in df\n    num_cols=list(df.select_dtypes(include=['number']).columns)\n    non_num_cols=list((set(df.columns)-set(num_cols)))\n\n    print(f'Num cols = {num_cols}')\n    print(f'Non-num cols = {non_num_cols}')","1fa18f74":"# display the overall stats\noverall_stat(train)","86494004":"# define the interest feature you want to explore\ninter_features='revenue'\n\n# define the function for univariate analysis\ndef num_uni_analysis(df,inter_features):\n    display(plot(df,inter_features,display=['Stats','KDE Plot','Normal Q-Q Plot','Box Plot']))\n    skewness=df[inter_features].skew()\n    kurtosis=df[inter_features].kurtosis()\n    print(f'-The Skewness = {skewness}')\n    if abs(skewness)<1:\n        print(f'The [{inter_features}] distribution is nearly normal')\n    elif skewness>1:\n        print(f'The [{inter_features}] distribution is right skewed ')\n    else:\n        print(f'The [{inter_features}] distribution is left skewed ')\n    print(f'-The Kurtosis = {kurtosis}')","2cf888c5":"# display the univariate analysis result for feature [revenue]\nnum_uni_analysis(train,inter_features)","003d2a9a":"# define the function for univariate analysis\ndef cat_uni_analysis(df,inter_features):\n    print(f'The Non-Numerical Column You Choose is: [{inter_features}]\\n')\n    display(plot(df,inter_features,display=['Stats','Pie Chart','Value Table']))","27468697":"cat_uni_analysis(train,inter_features='Type')\ncat_uni_analysis(train,inter_features='City')\ncat_uni_analysis(train,inter_features='City Group')","ecf9187e":"import matplotlib.pyplot as plt\nimport seaborn as sns","5f195b1e":"# overall num-num relationship: correlation heatmap\ndef heatmap(df,figsize):\n    fig, axs=plt.subplots(figsize=figsize)\n    sns.heatmap(df.corr(),annot=True, linewidths=.7,cmap='coolwarm',fmt='.1f',ax=axs)\n    ","b21f8bf1":"heatmap(df=train,figsize=(25,25))","5c886dd4":"pip install scikit-posthocs","3f2359a4":"#kruskal test used for cat-num relationship\nimport scikit_posthocs as sp\n","3607f35d":"# categoircal-numerical relationship (cat_feature - target)\n\ndef cat_num_relationship(df,cat_col,num_col):\n    # visualization\n    print(f'[{cat_col}] --- [{num_col}] relationship')\n    display(plot(df,num_col,cat_col))\n    \n    # hypothesis testing for catgorical-numerical relationship (Kruskal test)\n    pc = sp.posthoc_conover(df, val_col=num_col, group_col=cat_col,p_adjust = 'holm')\n    # visualization of the heatmap\n    heatmap_args = {'linewidths': 0.25, 'linecolor': '0.5', 'square': True, 'cbar_ax_bbox': [0.80, 0.35, 0.04, 0.3]}\n\n    # plot\n    fig, ax = plt.subplots(ncols=1)\n    fig.suptitle('Significance Plot')\n    sp.sign_plot(pc,**heatmap_args,ax=ax) \n    fig.show()","deaee3e5":"cat_num_relationship(df=train,\n                    cat_col='City',\n                    num_col='revenue')\n\ncat_num_relationship(df=train,\n                    cat_col='Type',\n                    num_col='revenue')\n\ncat_num_relationship(df=train,\n                    cat_col='City Group',\n                    num_col='revenue')","90dceae9":"from scipy.stats import chi2_contingency","35a098d3":"# categoircal-categorical relationship\n\ndef cat_cat_relationship(df,cat_col_1,cat_col_2):\n    # visualization\n    plot(df,cat_col_1,\n         cat_col_2,\n         display=['Stacked Bar Chart','Heat Map'])\n    \n    # Chi-square test\n    \n    # 1st step convert the data into a contingency table with frequencies\n    chi_contigency=pd.crosstab(df[cat_col_1],df[cat_col_2])\n    print(f'Selected cols [{cat_col_1}] and [{cat_col_2}]')\n    print('chi2-contingency table')\n    display(chi_contigency)\n    \n    # 2nd step: Chi-square test of independence.\n    c, p, dof, expected = chi2_contingency(chi_contigency)\n    if p<0.05:\n      print('Reject Null Hypothesis')\n      print(f'The:\\n [{cat_col_1}],[{cat_col_2}] are not independent\\n')\n    else:\n      print('Fail to Reject Null Hypothesis')\n      print(f'The:\\n [{cat_col_1}],[{cat_col_2}] are independent\\n') \n    print(f'The P-value = {p}')","2dd4544a":"# display the result\ncat_cat_relationship(df=train,\n                    cat_col_1='City Group',\n                    cat_col_2='Type')","99cb2fce":"nulls_train = train.isnull().sum()\nnulls_train[nulls_train > 0]","e51349f4":"nulls_test = test.isnull().sum()\nnulls_test[nulls_test > 0]","5b164091":"from sklearn import preprocessing\n","f7c329b6":"train","e6e270d9":"# define the function for label or one-hot encoding\ndef label_encode_transform(df,cols):\n    cols=cols\n    le = preprocessing.LabelEncoder()\n    df[cols]=df[cols].apply(le.fit_transform)\n    return df\n    \ndef onehot_encode_transform(df,cols):\n    cols=cols\n    df=pd.get_dummies(df,columns=cols)\n    return df","0a216a73":"train_df_encoded = label_encode_transform(df=train,cols = ['City'])\n\ntrain_df_encoded = onehot_encode_transform(df=train,cols = ['City Group','Type'])\n\ntest_df_encoded = label_encode_transform(df=test,cols = ['City'])\n\ntest_df_encoded = onehot_encode_transform(df=test,cols = ['City Group','Type'])","5d949b2a":"train_df_encoded.info()","1dbbf3f6":"# seperate the source and the target variables\nfeature_cols = [x for x in train_df_encoded.columns if x != 'revenue']\nX_train = train_df_encoded[feature_cols]\ny_train = train_df_encoded['revenue']\n\nX_test  = test_df_encoded[feature_cols]","d8fbb517":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n#for this post we will use MinMaxScaler\nscaler=MinMaxScaler()","103eb900":"X_train_scaled=pd.DataFrame(scaler.fit_transform(X_train.T).T,columns=X_train.columns)\nX_test_scaled=pd.DataFrame(scaler.fit_transform(X_test.T).T,columns=X_test.columns)","4ee5bece":"# decision tree for feature importance on a regression problem\nfrom sklearn.datasets import make_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom matplotlib import pyplot\n\n# define the model\nmodel = DecisionTreeRegressor()\n# fit the model\nmodel.fit(X_train_scaled, y_train)\n# get importance\nimportance = model.feature_importances_\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))","fcd22848":"n_top_features = 25\ntop_features = importance.argsort()[-n_top_features:]\nprint(top_features)  # [ 0  4  7 12  5]","c97e06ad":"X_train_final = X_train_scaled.iloc[:, top_features]\nX_test_final = X_test_scaled.iloc[:, top_features]","50705b32":"X_train_final.info()","3e15c7a3":"X_test_final.info()","b36d87e7":"##Linear Regression\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nreg = LinearRegression().fit(X_train_final, y_train)\nreg.score(X_train_final, y_train)","42730963":"reg.coef_\n","aa48613e":"reg.intercept_\n","e796e82e":"y_train_pred = reg.predict(X_train_final)\n","1267bb60":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_pred)))","e4629b18":"##Ridge Regression\n\nfrom sklearn.linear_model import Ridge\nimport numpy as np\nfrom pandas import read_csv\n# evaluate an ridge regression model on the dataset\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import absolute\nfrom pandas import read_csv\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import Ridge\n\n\nridge = Ridge(alpha=0.1, normalize=True)\nridge.fit(X_train_final, y_train)\ny_train_ridge_pred = ridge.predict(X_train_final)","17bbb0a6":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_ridge_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_ridge_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_ridge_pred)))","4b905f60":"import pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n##Lasso Regression\nmodel_lasso = Lasso(alpha=0.01)\nmodel_lasso.fit(X_train_final, y_train) \ny_train_lasso_pred = model_lasso.predict(X_train_final)","b6c57205":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_lasso_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_lasso_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_lasso_pred)))","f2bd6680":"##KNN\nmodel_knn = KNeighborsRegressor(n_neighbors=8)\nprint(model)\nKNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n          metric_params=None, n_jobs=1, n_neighbors=8, p=2,\n          weights='uniform') \n\nmodel_knn.fit(X_train_final, y_train)\n\ny_train_knn_pred = model_knn.predict(X_train_final)","c860152c":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_knn_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_knn_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_knn_pred)))","7f19b6c2":"#Random Regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nX, y = make_regression(n_features=4, n_informative=2,\n                       random_state=0, shuffle=False)\nregrr = RandomForestRegressor(max_depth=2, random_state=0)\nregrr.fit(X_train_final, y_train)\ny_train_rr_pred = regrr.predict(X_train_final)","fbd309d4":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_rr_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_rr_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_rr_pred)))","34d15578":"##SVM\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nn_samples, n_features = 10, 5\nregrsvm = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\nregrsvm.fit(X_train_final, y_train)\n\ny_train_svm_pred = regrsvm.predict(X_train_final)","5ec67d14":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_svm_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_svm_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_svm_pred)))","cd52bbf5":"pip install xgboost\n","b2a789d8":"pip install lightgbm\n","2c0fe15c":"from sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import ElasticNetCV\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom sklearn.metrics import mean_squared_error","d8de9c76":"cv=25\n\n# fit the Models\nlassoCV=LassoCV(cv=cv).fit(X_train_final,y_train)\nridgeCV=RidgeCV(cv=cv).fit(X_train_final,y_train)\nelasticnetCV=ElasticNetCV(cv=cv).fit(X_train_final,y_train)\nlightgbm=lgb.LGBMRegressor().fit(X_train_final,y_train)\nxgboost=xgb.XGBRegressor().fit(X_train_final,y_train)\n\n# generate the prediction for train dataset\nlasso_train_pred=lassoCV.predict(X_train_final)\nridge_train_pred=ridgeCV.predict(X_train_final)\nelasticnet_train_pred=elasticnetCV.predict(X_train_final)\nlgbm_train_pred=lightgbm.predict(X_train_final)\nxgb_train_pred=xgboost.predict(X_train_final)\n\n# generate RMSE for each models\nlasso_RMSE= np.sqrt(mean_squared_error(y_train, lasso_train_pred))\nridge_RMSE= np.sqrt(mean_squared_error(y_train, ridge_train_pred))\nelasticnet_RMSE= np.sqrt(mean_squared_error(y_train, elasticnet_train_pred))\nlgbm_RMSE= np.sqrt(mean_squared_error(y_train, lgbm_train_pred))\nxgb_RMSE= np.sqrt(mean_squared_error(y_train, xgb_train_pred))","a68e9276":"model_list=['Lasso','Ridge','ElasticNet','LGBM','XGBoost']\nrmse_list=[lasso_RMSE,ridge_RMSE,elasticnet_RMSE,lgbm_RMSE,xgb_RMSE]\n\n# plot the RMSE for each model\nax=sns.barplot(y=model_list,x=rmse_list)\nax.set_title('Model RMSE Result')\n\n# print the result RMSE number\nprint(f' lasso={lasso_RMSE} \\n ridge = {ridge_RMSE}\\n Elastic_Net = {elasticnet_RMSE}\\n LGBM = {lgbm_RMSE}\\n XGBoost= {xgb_RMSE}\\n')","b3b03dbb":"# generate prediction for test dataset\nKNN_test_pred=model_knn.predict(X_test_final)","68b900c9":"# store the result\nsubmission_df=pd.DataFrame(\n{'Id':test.index,\n'Prediction':KNN_test_pred}\n)","b8544e78":"submission_df.to_csv('submission_dcx.csv',index=False)","7ba2266b":"**Target Varaible = \"Revenue\"**\n","e11bb266":"**Bivariate Analysis**","fc186cc1":"Id : Restaurant id.\n\nOpen Date : opening date for a restaurant\n\nCity : City that the restaurant is in. Note that there are unicode in the names. \n\nCity Group: Type of the city. Big cities, or Other. \n\nType: Type of the restaurant. FC: Food Court, IL: Inline, DT: Drive Thru, MB: Mobile\n\nP1, P2 - P37: There are three categories of these obfuscated data. Demographic data are gathered from third party providers with GIS systems. These include population in any given area, age and gender distribution, development scales. Real estate data mainly relate to the m2 of the location, front facade of the location, car park availability. Commercial data mainly include the existence of points of interest including schools, banks, other QSR operators.\n\nRevenue: The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. Please note that the values are transformed so they don't mean real dollar values. ","10022b16":"**Label Encoding**","2380efa1":"**I choose KNN as the final model for prediction**","a258ebf5":"Categorical Univariate Analysis","9162dc29":"Data Preprocessing:\n\nEDA\/Data Cleaning: Exploring Data & Identifying and correcting mistakes or errors in the data.\n\nFeature Selection: Identifying those input variables that are most relevant to the task.\n\nData Transforms: Changing the scale or distribution of variables.\n\nFeature Engineering: Deriving new variables from available data.\n\nDimensionality Reduction: Creating compact projections of the data.","30556c56":"**Defining X & Y**","8e97abfc":"**Features Explanation**","f2a49313":"Numerical Univariate Analysis","a8734ec5":"**Models**","e048c707":"![image.png](attachment:cea4d4c9-e1bb-454c-954c-76c4d44b527c.png)","cd63d136":"**Scaling**","6eedfc89":"**Data Viz**","86ed8ef5":"**Univariate Analysis**","2767f032":"**Categorical-Categorical Relationship**","31706dae":"Model Building\n\nLinear Regression\n\nRidge Regression\n\nLasso Regression\n\nKNN\n\nDecision Tree Algorithm\n\nRandom Regression\n\nSVM","51008cca":"Cat-Num Relationship","437e510e":"Dropping Some Unncessary Features"}}