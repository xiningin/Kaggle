{"cell_type":{"e4201a15":"code","5b5ebe01":"code","d2aee2a6":"code","2a6860ca":"code","e110776e":"code","59340e05":"code","5b1febb5":"code","812d44d1":"code","3c7bae34":"code","7f4298f1":"code","ec11abbe":"code","f4d251df":"code","dfdedd48":"code","2ff46d81":"code","e939c3b5":"code","e9fb3a3f":"code","7bcd87be":"code","954f0a0d":"code","3583784d":"code","734e8d68":"code","b51ff9d3":"code","38a82518":"code","ef9babf3":"code","4e36aed6":"code","41871573":"code","e05d6307":"code","d97aca45":"markdown","6f718977":"markdown","724eae20":"markdown","549c48b4":"markdown"},"source":{"e4201a15":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5b5ebe01":"df = pd.read_csv('..\/input\/voicegender\/voice.csv')\ndf.columns","d2aee2a6":"df.shape","2a6860ca":"df.dtypes","e110776e":"df.head(3)","59340e05":"df.isnull().values.any()","5b1febb5":"# Distribution of target varibles\ncolors = ['pink','Lightblue']\ndata_y = df[df.columns[-1]]\nplt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])\nplt.axis('equal')\nprint (df['label'].value_counts())","812d44d1":"# Box plot see comparision in labels by other features\ndf.boxplot(column = 'meanfreq',by='label',grid=False)","3c7bae34":"# plot correlation matrix (here i used the headmape using seaborn)\ncorrelation =df.corr()\nsns.heatmap(correlation)\nplt.show()","7f4298f1":"# train_test_split is responsible to split the data into (Train and Test)\nfrom sklearn.model_selection import train_test_split\nX = df[df.columns[:-1]].values\n#y = df['lable']\ny = df[df.columns[-1]].values\n# We will divide the data into 70-30% into train and test data\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)","ec11abbe":"import warnings\nwarnings.filterwarnings(\"ignore\")\n# Random forest\n# Importing Random forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrand_forest = RandomForestClassifier()\nrand_forest.fit(Xtrain, ytrain)\ny_pred = rand_forest.predict(Xtest)\n","f4d251df":"#import matrix to calcualte accuracy\nfrom sklearn import metrics, neighbors\nfrom sklearn.metrics import accuracy_score\nprint(metrics.accuracy_score(ytest, y_pred))","dfdedd48":"# Import confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, y_pred))","2ff46d81":"# 10-fold cross validation\n# Importing Bausian Classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\nCVFirst = GaussianNB()\nCVFirst = CVFirst.fit(Xtrain, ytrain)\ntest_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')\nprint('Accuracy obtained from 10-fold cross validation is:',test_result.mean())","e939c3b5":"male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &\n                               (df['label'] == 'male')].index\nfemale_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &\n                                 (df['label'] == 'female')].index","e9fb3a3f":"index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)\nlen(index_to_remove)","7bcd87be":"data_x = df[df.columns[0:20]].copy()\ndata2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()\ndata2.head(3)\ndata2 = data2.drop(index_to_remove,axis=0)\n\n#y = df['lable']\ndata_y = pd.Series(y).drop(index_to_remove,axis=0)\nXtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )\nclf1 = RandomForestClassifier()\nclf1.fit(Xtrain, ytrain)\ny_pred = clf1.predict(Xtest)\nprint(metrics.accuracy_score(ytest, y_pred))","954f0a0d":"# Importing Decision Trees Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nclf2 = DecisionTreeClassifier()\nclf2.fit(Xtrain, ytrain)\ny_predict = clf2.predict(Xtest)\nprint(metrics.accuracy_score(ytest, y_predict))","3583784d":"clf3 = GaussianNB()\nclf3 = clf3.fit(Xtrain, ytrain)\ny_predd = clf3.predict(Xtest)\nprint(metrics.accuracy_score(ytest,y_predd))","734e8d68":"# Importing linear Regression classifier\nfrom sklearn.linear_model import LogisticRegression\nclf4 = LogisticRegression()\nclf4.fit(Xtrain,ytrain)\ny_predict4 = clf4.predict(Xtest)\nprint(metrics.accuracy_score(ytest,y_predict4))","b51ff9d3":"# cross validation with same classifire as first time \ntest_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')\nprint('Accuracy obtained from 10-flod cross validation is:',test_result.mean())","38a82518":"# cros validation on the best result\ntest_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')\nprint('Accuracy obtained from 10-fold validation is:',test_result.mean())","ef9babf3":"import pylab as pl\nlabels = ['female', 'male']\ncm = confusion_matrix(ytest,y_pred,labels)  #ypred for RandomForest\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax =ax.matshow(cm)\npl.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\npl.xlabel('Predicted')\npl.ylabel('True')\npl.show()","4e36aed6":"from sklearn.metrics import classification_report\nprint(classification_report(ytest, y_pred))","41871573":"# Distribution of male and female\nsns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,\"meanfun\").add_legend()\nplt.show()","e05d6307":"#Since we're doing flat-clustering , our task is a bit easier since we can tell the machine that we want it category\n\nfrom sklearn.cluster import KMeans\nfrom matplotlib import style\nstyle.use(\"ggplot\")\n\ndata_x = np.array(df[['meanfreq','meanfun']])\nkmeans = KMeans(n_clusters= 2)\nkmeans.fit(data_x)\n\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\n\n#print(centroids)\n#print(labels) # 0-male, 1-Female( the machine has assigned on its own.)\n\ncolors = [\"g.\",\"b.\"]  #green = male\n\nfor i in range(len(data_x)):\n    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)\n\n    \nplt.scatter(centroids[:,0],centroids[:, 1], marker = \"x\", s=150, linewidths = 5, zorder = 10)\nplt.ylabel('meanfun')\nplt.xlabel('meanfun')\n\nplt.show()","d97aca45":"# Data cleaning","6f718977":"# Machine learning part:\n","724eae20":" Exceptable range of voice freq for a human as per will is betwwen 0.085\nand 0.255KHz and hence we will identify the variable which has the frequency\ninformation and remove them assuming it to be a outlier based on domain knowledge.\nAs per the station given in wiki we can say that typical adult male will\nhave a fundamental frequency from 85 to 189 Hz and typical adult female from 165 to 255 Hz.","549c48b4":"# Confusion matrix"}}