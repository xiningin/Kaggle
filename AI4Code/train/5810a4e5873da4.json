{"cell_type":{"9fc9239f":"code","f41eb306":"code","738c5b24":"code","cbba4402":"code","eb5d649a":"code","63e0abd5":"code","0e0fdb2b":"code","09f1830c":"code","ef4413ce":"code","d146e1cf":"code","a0c8b9cb":"code","b63a6389":"code","49eae7b0":"code","a1c9e608":"code","16b257e1":"code","6b1370f2":"code","65eed9c7":"code","31fd1ee6":"code","d10ba15d":"code","00bd0897":"code","c6bb6781":"code","c67be267":"code","a2449aee":"code","8e901b74":"code","89808253":"code","1b6df575":"code","8f309148":"code","b180e9b8":"code","4a47906d":"code","a11b5ab9":"code","500e3ef4":"code","1be15b3f":"code","3d15d560":"code","8d6d2488":"code","1558f41c":"code","8b7e2719":"code","7081ab47":"code","85bc098a":"code","40a11c92":"code","1ceabd72":"code","b1662f12":"code","cbb74f74":"code","c621f827":"code","b8f49cde":"code","c265022e":"code","2ad10c83":"code","542f7ec2":"code","3265dc74":"code","5210a26e":"code","60ae71a7":"code","05d27c45":"code","12989584":"code","196edcbc":"code","67c526ea":"code","72ec75e0":"code","cb6afbfd":"code","6c9225b3":"code","5ae80325":"code","24f39fc0":"code","d32bf927":"code","cda475d4":"code","614af515":"code","9f775981":"code","3ded3b2b":"code","1c77b97c":"code","d7169091":"code","9c959b95":"code","2afc1f6c":"code","c7be22bb":"markdown","017171fa":"markdown","0a5e77e4":"markdown","0b876740":"markdown","2619df38":"markdown","737e74a5":"markdown","5c3351d6":"markdown","65fde4af":"markdown","db5a9ca9":"markdown","01e9a686":"markdown","9e2a9ec1":"markdown","c8ad9e8f":"markdown","10deb7af":"markdown"},"source":{"9fc9239f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f41eb306":"# read csv file\ndataset= pd.read_csv('\/kaggle\/input\/car-price-prediction\/CarPrice_Assignment.csv')","738c5b24":"# print top-5 records\ndataset.head()","cbba4402":"# Check shape\ndataset.shape","eb5d649a":"# Check statistic \ndataset.describe()","63e0abd5":"# check datatypes and number of records\ndataset.info()","0e0fdb2b":"# Check Missing value\ndataset.isnull().sum()","09f1830c":"# here car_ID column not important so drop car_ID column\ndataset.drop('car_ID',axis=1,inplace=True)","ef4413ce":"# First check how many numerics,categoricals,temporal (Date, time eg..) features\n\n# Create list of numeric features\nnumeric_features= list(dataset.select_dtypes(include=['int64','float64']).keys())","d146e1cf":"# print top-5 numeric records\ndataset[numeric_features].head()","a0c8b9cb":"# Check how many descrete variables in numerics features\ndescrete_features=[feature for feature in numeric_features if len(dataset[feature].unique())<25]\ndataset[descrete_features].head()","b63a6389":"# now check the relation between decrete variables and dependent feature price\nfor feature in descrete_features:\n    df=dataset.copy()\n    df.groupby(feature)['price'].median().plot.bar()\n    plt.title(feature)\n    plt.xlabel(feature)\n    plt.ylabel('Price')\n    plt.show()","49eae7b0":"# now create list for continous varibles\ncontinous_features=[feature for feature in numeric_features if feature not in descrete_features]\ndataset[continous_features].head()","a1c9e608":"# crete histogram for continous variables\nfor feature in continous_features:\n    df=dataset.copy()\n    df[feature].hist(bins=25)\n    plt.title(feature)\n    plt.xlabel(feature)\n    plt.ylabel('count')\n    plt.show()","16b257e1":"# Now Check outliers \nfor feature in continous_features:\n    df=dataset.copy()\n    if 0 in df[feature].unique():\n        pass\n    else:\n        df[feature]=np.log(df[feature])\n        df.boxplot(column=feature)\n        plt.title(feature)\n        plt.ylabel(feature)\n        plt.show()","6b1370f2":"#create list for categorical variables\ncategorical_features= list(dataset.select_dtypes(include=['object']).keys())\n\n#print top-5 records\ndataset[categorical_features].head()","65eed9c7":"# check how many categories present in each feature\nfor feature in categorical_features:\n    print(f\"The feature is {feature} and number of categories {len(dataset[feature].unique())}\")\n    print(\"----------------------------------------------\")","31fd1ee6":"# Now check relation between categorical features and dependent features\ncategorical_features.remove('CarName')\nfor feature in categorical_features:\n    df=dataset.copy()\n    df.groupby(feature)['price'].median().plot.bar()\n    plt.title(feature)\n    plt.xlabel(feature)\n    plt.ylabel('Price')\n    plt.show()","d10ba15d":"# Take only company name from the care name\ndata=dataset.copy()\nnew=data['CarName'].str.split(\" \",n = 1, expand = True)\ndata['c_name']=new[0]\ndata.head()","00bd0897":"# now check company name and price relaion\nplt.figure(figsize=(15,10))\ndata.groupby('c_name')['price'].median().plot.bar()","c6bb6781":"d=data['c_name'].value_counts()\nplt.figure(figsize=(20,15))\nplt.pie(d.values,labels=d.keys(), shadow = True,autopct='%1.2f%%')\nplt.legend(loc='upper right')","c67be267":"# Transfrom Skewed data into log normal transformation\n# create list for skewed features\nnum_features= ['wheelbase','carlength','carwidth','carheight','curbweight','enginesize','boreratio','stroke','compressionratio','horsepower','citympg','highwaympg','price']\nfor feature in num_features:\n    dataset[feature]=np.log(dataset[feature])","a2449aee":"dataset.head()","8e901b74":"#now crete new feature with help of carname\ndataset['company_name']=data['CarName'].str.split(\" \",n = 1, expand = True)[0]\ndataset.head()","89808253":"#drop the carname column\ndataset.drop('CarName',axis=1,inplace=True)","1b6df575":"dataset.head()","8f309148":"# check categories in company_name column\ndataset['company_name'].value_counts()","b180e9b8":"# replace missplled company_name using loc\n# for volkswagen\ndataset.loc[(dataset.company_name=='vw'),'company_name']='volkswagen'\ndataset.loc[(dataset.company_name=='vokswagen'),'company_name']='volkswagen'\n\n# for porsche\ndataset.loc[(dataset.company_name=='porcshce'),'company_name']='porsche'\n\n#for toyota\ndataset.loc[(dataset.company_name=='toyouta'),'company_name']='toyota'\n\n#for nissan\ndataset.loc[(dataset.company_name=='Nissan'),'company_name']='nissan'\n\n#for mazda\ndataset.loc[(dataset.company_name=='maxda'),'company_name']='mazda'","4a47906d":"dataset.head()","a11b5ab9":"dataset['company_name'].value_counts()","500e3ef4":"# now Handle rare categorical feature\n# remove the feature that are present less then 1% oberservation\ncat_features=list(dataset.select_dtypes(include=['object']).keys())","1be15b3f":"cat_features","3d15d560":"for feature in cat_features:\n    temp=dataset.groupby(feature)['price'].count()\/len(dataset)\n    temp_df=temp[temp>0.01].index\n    print(temp_df)","8d6d2488":"from sklearn.preprocessing import LabelEncoder","1558f41c":"for feature in cat_features:\n    le=LabelEncoder()\n    dataset[feature]=le.fit_transform(dataset[feature])","8b7e2719":"# import minmaxscaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","7081ab47":"X=dataset.drop('price',axis=1)\ny=dataset[['price']]","85bc098a":"X.head()","40a11c92":"y.head()","1ceabd72":"col=X.columns\nx=pd.DataFrame(scaler.fit_transform(X),columns=col)\nx.head()","b1662f12":"# Importing statsmodels module as sm\nimport statsmodels.api as sm\n# Adding a constant column to our X_train dataframe\nX_train = sm.add_constant(X_train)\n# create a first fitted model\nmodel=sm.OLS(y_train,X_train)\nlm_1 = model.fit()","cbb74f74":"print(lm_1.summary())","c621f827":"plt.figure(figsize=(16,10))\nsns.heatmap(dataset.corr(),annot=True)\nplt.show()","b8f49cde":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n# Define vif_scores function as stated above\ndef vif_score(X):\n  vif_data=pd.DataFrame()\n  vif_data['Variables']=X.columns\n  vif_data['VIF']=[variance_inflation_factor(X.values,i) for i in range(len(X.columns))]\n  return vif_data\n\n\n\n\n# print vif scores for all current input features\nprint(vif_score(x))","c265022e":"x.drop(columns=['wheelbase','cylindernumber','boreratio','stroke','compressionratio'],axis=1,inplace=True)\nprint(vif_score(x))","2ad10c83":"x.drop(columns=['drivewheel','peakrpm'],axis=1,inplace=True)\nprint(vif_score(x))","542f7ec2":"# split the data into train_test\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=41)","3265dc74":"print(\"shape of X_train: \",X_train.shape)\nprint(\"shape of X_test: \",X_test.shape)\nprint(\"Shape of y_train: \",y_train.shape)\nprint(\"Shape of y_test: \",y_test.shape)","5210a26e":"#import libraries\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel","60ae71a7":"#Initialize lasso\nlasso=Lasso()\n\n#crete list of alpha \nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\n#inintialize gridsearchcv\ngrid_lasso=GridSearchCV(estimator=lasso,param_grid=params,scoring='neg_mean_absolute_error',cv=5,return_train_score=True,verbose=1)\n\n#fit model\ngrid_lasso.fit(X_train,y_train)","05d27c45":"#create dataframe for cv results\ncv_results=pd.DataFrame(grid_lasso.cv_results_)\n\n#print cv_results\ncv_results.head()","12989584":"#ploting mean test and train score with alpha\n# change param_alpha datatype to float\ncv_results['param_alpha'] = cv_results['param_alpha'].astype(int)\n\n# plotting\nplt.figure(figsize=(10,7))\nplt.xlabel('alpha',fontsize=15)\nplt.ylabel('Negative mean absolute error',fontsize=15)\nplt.title('Negative mean absolute error and alpha',fontsize=20)\nplt.plot(cv_results['param_alpha'],cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'],cv_results['mean_test_score'])\nplt.legend(['Train Score','TestScore'])","196edcbc":"grid_lasso.best_params_","67c526ea":"#let's take alpha value 0.01 and select best features\n\nfeature_sel_model=SelectFromModel(Lasso(alpha=0.001,random_state=0))\nfeature_sel_model.fit(X_train,y_train)","72ec75e0":"feature_sel_model.get_support()","cb6afbfd":"selected_feat=X_train.columns[(feature_sel_model.get_support())]\nselected_feat","6c9225b3":"print(f\"Total features: {X_train.shape[1]}\")\nprint(f\"selected features: {len(selected_feat)}\")\nprint(f\"features with coefficient shrank to 0 : {np.sum(feature_sel_model.estimator_.coef_==0)}:\")","5ae80325":"# crete model with selected features\nfrom sklearn.linear_model import LinearRegression","24f39fc0":"model=LinearRegression()","d32bf927":"#fit the data\nmodel.fit(X_train[selected_feat],y_train)","cda475d4":"y_pred=model.predict(X_test[selected_feat])","614af515":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n#calculate and print RMSE\nmse=mean_squared_error(y_test,y_pred)\nprint(\"Root Mean Squared error(RMSE) is: \",np.sqrt(mse))","9f775981":"#calculate and print MSE\nprint(\"Mean squared Error(MSE) is: \",mse)","3ded3b2b":"#calculate and print MAE\nprint(\"Mean Absolute error(MAE) is: \",mean_absolute_error(y_test,y_pred))","1c77b97c":"residual=y_test-y_pred\nsns.distplot(residual)","d7169091":"np.mean(residual)","9c959b95":"fig,ax=plt.subplots(figsize=(6,2.5))\n_=ax.scatter(y_pred,residual)","2afc1f6c":"import statsmodels.tsa.api as smt\nacf=smt.graphics.plot_acf(residual,lags=40,alpha=0.05)\nacf.show()","c7be22bb":"# Feature Selection","017171fa":"Droping the stroke,boreration and compression ratio","0a5e77e4":"## Normality of residuals","0b876740":"## Homoscedasticity","2619df38":"* > *Here you can see most of the features are not follow normal distribution so convert into normal distribution*","737e74a5":"# Feature Engineering","5c3351d6":"### categorical features","65fde4af":"# Feature Scaling\n","db5a9ca9":"# Verify assumptions","01e9a686":"# Exploratory Data Analysis","9e2a9ec1":"### Numeric Features","c8ad9e8f":"# Data Prepration","10deb7af":"## No auto correlation of residual"}}