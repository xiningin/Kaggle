{"cell_type":{"4f1bbb52":"code","4ce7a745":"code","548a11a7":"code","18a3ae54":"code","b783e844":"code","bbbd7530":"code","296d9be3":"code","dd1ff0ff":"code","a26e95d4":"code","6ed1f002":"code","954d8193":"code","977cb9a8":"code","ee46d2bf":"code","e05c5e6c":"code","1b320b75":"code","dd7e9de5":"code","043dd709":"code","cc42c979":"code","dbc28322":"code","21686346":"code","cb1b7328":"code","5bcfa665":"code","46938474":"code","9d0e650b":"code","52e638fa":"code","a545cb29":"code","dd64d68a":"code","e3efa5d8":"code","e67ccc5a":"code","5750fb78":"code","161f45f9":"code","d55f406a":"code","fb1433a7":"markdown","9c258d3f":"markdown","76dbc594":"markdown","57175542":"markdown","9d9462d4":"markdown","60f32683":"markdown","93e1a53d":"markdown","743ce5f0":"markdown","2dfb9cc5":"markdown","4f814fa6":"markdown","401883b5":"markdown","3579cc53":"markdown","4da0bdd8":"markdown","9c13a2d6":"markdown","28597835":"markdown","f00e018d":"markdown","e39a2abf":"markdown","bfb7e7ae":"markdown","bda7e80d":"markdown","7a8d7d9e":"markdown","7e715ebc":"markdown","51d2aacd":"markdown","cbf68e14":"markdown","4dd4eaf0":"markdown","a17ed6be":"markdown","a1ad0913":"markdown","316a51ff":"markdown","8a777553":"markdown","95900faf":"markdown","d563bf30":"markdown","650ee381":"markdown","85544996":"markdown","0d18185b":"markdown","286aa395":"markdown","ad2bcd3d":"markdown","dfbc1f59":"markdown","786a5b93":"markdown","54702e7a":"markdown"},"source":{"4f1bbb52":"!pip install seaborn --upgrade #Update Seaborn for Plotting","4ce7a745":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno #Visualize null\n\n#Plotting Functions\nimport matplotlib.pyplot as plt\n\n#Aesthetics\nimport seaborn as sns\nsns.set_style('ticks') #No grid with ticks\nprint(sns.__version__)","548a11a7":"#Data Import\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","18a3ae54":"income=pd.read_csv('\/kaggle\/input\/income\/train.csv')\nincome.head()","b783e844":"income.info()\nmsno.bar(income)","bbbd7530":"#Generating column labels in data frame for simple copy paste\nincome.columns","296d9be3":"#Using a for loop we can print each unique value per column.\ncol_list=['workclass','education','marital-status', 'occupation','relationship', 'race', 'gender','native-country']\nfor label in col_list:\n    print('***',label, income[label].unique()) #*** are used for indentation","dd1ff0ff":"new_train=income.copy()#Setting a check point for errors. This is good practice for when manipulating values on data frames through multiple code blocks\n\n#Renaming income for simple input\nnew_train.rename(columns={'income_>50K':'income'}, inplace=True)\n\n#workclass\nnew_train['workclass'].fillna('nan', inplace=True)#First replace the null value with a 'nan' as part of that column contains them already. The .fillna function replaces nulls with whatever string label you want\nnew_train['workclass']=new_train['workclass'].replace({'Self-emp-not-inc':'Self-emp','Self-emp-inc':'Self-emp',\n        'Never-worked':'Un-emp\/Unk','Without-pay':'Un-emp\/Unk', 'State-gov':'Government', \n        'Federal-gov':'Government','Local-gov':'Government','nan':'Un-emp\/Unk'}) #This first line is a harsh way of doing this and is very tedious.\n\n\"\"\"Looking below we can abbreviate this through several for loops using the replace function. Setting a list of \nvariables that you want to converge to a single variable as seen below. Once a list is made you can implement \nit into the for loop and place the pulled values to be replaced by a specific value (pull label:new label)\"\"\"\n\n#Education\nedu_lHS=['12th','7th-8th','9th','10th', '11th','5th-6th','1st-4th','Preschool'] # <HS\nedu_assoc=['Assoc-voc','Assoc-acdm','Some-college'] #Associate\nedu_mpro=['Masters','Prof-school']#Mas-Pro\nfor edu in edu_lHS:\n    new_train['education']=new_train['education'].replace({edu:'<HS'})\nfor edu in edu_assoc:\n    new_train['education']=new_train['education'].replace({edu:'Associate'})\nfor edu in edu_mpro:\n    new_train['education']=new_train['education'].replace({edu:'Mas-Pro'})\n\n#Marital Status\nsep_div=['Divorced','Separated']#Sep-Div\nmarried=['Married-civ-spouse','Married-spouse-absent','Married-AF-spouse']#Married\nfor mar in sep_div:\n    new_train['marital-status']=new_train['marital-status'].replace({mar:'Sep-Div'})\nfor mar in married:\n    new_train['marital-status']=new_train['marital-status'].replace({mar:'Married'})\n\n#Occupation\nnew_train['occupation'].fillna('nan', inplace=True)\n\n#Relationship: \nmarr=['Husband', 'Wife']#Married\nfor mar in marr:\n    new_train['relationship']=new_train['relationship'].replace({mar:'Married'})\n\n#Native Country\neurope=['England', 'Italty', 'Germany', 'France','Yugoslavia', 'Poland', 'Greece', 'Ireland', 'Scotland',\n       'Hungary','Holand-Netherlands','Portugal']\nasia=['China', 'Philippines','Vietnam','Thailand','Taiwan','Laos','Cambodia','Japan', 'Hong','India','Iran']\ncaribbean=['Jamaica','Dominican-Republic','Cuba','Haiti','Trinadad&Tobago', 'Puerto-Rico']\nn_america=['United-States','Canada']\nc_america=['Mexico','Honduras','El-Salvador','Guatemala','Nicaragua']\ns_america=['Columbia','Ecuador','Peru']\nfor coun in europe:\n    new_train['native-country']=new_train['native-country'].replace({coun:'Europe'})\nfor coun in asia:\n    new_train['native-country']=new_train['native-country'].replace({coun:'Asia'})\nfor coun in caribbean:\n    new_train['native-country']=new_train['native-country'].replace({coun:'Carribean'})\nfor coun in n_america:\n    new_train['native-country']=new_train['native-country'].replace({coun:'N.America'})\nfor coun in c_america:\n    new_train['native-country']=new_train['native-country'].replace({coun:'C.America'})\nfor coun in s_america:\n    new_train['native-country']=new_train['native-country'].replace({coun:'S.America'})\nnew_train['native-country'].fillna('nan', inplace=True)\nnew_train['native-country']=new_train['native-country'].replace({'South':'nan'})#As we do not know what south is we will just convert it to nan\n\n#Once we are done we can look that we replaced all the null values for either 'nan' or something else\nnew_income=new_train.copy()# Train was a fragment of a previous notebook I was working on and did not want to replace it for every line.\nnew_income.info()\nfor label in col_list:\n    print(label, new_income[label].unique())\nnew_income.head()","a26e95d4":"def cleaner(data,column, group_by=False, category=None, score=None,  \n            old_label1=None, new_label1=None, old_label2=None , new_label2=None, \n            old_label3=None, new_label3=None, old_label4=None, new_label4=None, \n            old_label5=None, new_label5=None, old_label6=None,new_label6=None):\n    \n    \"\"\" Quick helper function to make data cleaning faster and require less lines \n    of code per column for replacing values. Following the code you will see that \n    old_label#=old_label#. Example (1:1 as 2:2.). This function will automatically\n    fill null values as 'nan'. If you do not want 'nan' it must be specified in\n    list.\n    \n    Inputs:\n    data: pandas DataFrame\n    column: 'column name' as string\n    old_label: variable of a string or string list of values. default=None\n    new_label: variable of a string to replace the old_label(s) with. default=None\n    group_by: Groups Data by category, default=False, bool\n    category: Category in DataFrame to group data by, default=None, string\n    score: Transform null by string, default=None, 'mean', 'median', 'mode'\n    ***Note group_by, category and score all must have a value to be used together***\n    \n    Output: Updated DataFrame in specified column and a print of the number of sets of strings replaced\"\"\"\n    \n    print('***',column,'***')\n    #Null Filling\n    if data[column].isnull().sum()==0:\n        print('No null in', column)\n    elif group_by==True:\n        null=data[column].isnull().sum()\n        data[column].fillna(data.groupby(category)[column].transform(score), inplace=True)\n        print(column,'null filled by', score, 'of',category)\n        print(null, 'null values filled in', column)\n    else:\n        null=data[column].isnull().sum()\n        data[column].fillna('nan', inplace=True)\n        print(null, 'null values filled in', column)\n        \n    #First set of labels\n    if old_label1==None:\n        return print('No label replacements made!')\n    else:\n        for old in old_label1:\n            new_data=data[column].replace({old:new_label1}, inplace=True)\n            x=1\n    \n    #Second set of labels\n    if old_label2==None:\n        return print(x, 'set of labels were replaced in',column)\n    else:\n        for old in old_label2:\n            new_data=data[column].replace({old:new_label2}, inplace=True)\n            x2=x+1\n    \n    #Third set of labels        \n    if old_label3==None:\n        return print(x2, 'set of labels were replaced in', column)\n    else:\n        for old in old_label3:\n            new_data=data[column].replace({old:new_label3}, inplace=True)\n            x3=x2+1\n    \n    #Fourth set of labels\n    if old_label4==None:\n        return  print(x3, 'set of labels were replaced in', column)\n    else:\n        for old in old_label4:\n            new_data=data[column].replace({old:new_label4}, inplace=True)\n            x4=x3+1\n            \n    #Fifth set of labels\n    if old_label5==None:\n        return print(x4, 'set of labels were replaced in', column)\n    else:\n        for old in old_label5:\n            new_data=data[column].replace({old:new_label5}, inplace=True)\n            x5=x4+1\n    \n    #Sixth set of labels\n    if old_label6==None:\n        return print(x5, 'set of labels were replaced in', column)\n    else:\n        for old in old_label6:\n            new_data=data[column].replace({old:new_label6}, inplace=True)\n            x6=x5+1\n            return print(x6, 'set of labels were replaced in', column)","6ed1f002":"new_income=income.copy()#Re-using new_income\n\n#Renaming income for simple input\nnew_income.rename(columns={'income_>50K':'income'}, inplace=True)\n\n#age\ncleaner(new_income, 'age')\n\n#workclass\nwork_self=['Self-emp-not-inc','Self-emp-inc']#Self-emp\nwork_unuk=['Never-worked','Without-pay','nan']#Un-emp\/Unk\nwork_gov=['State-gov','Federal-gov','Local-gov']#Government\ncleaner(new_income, 'workclass', work_self, 'Self-emp', work_unuk, 'Un-emp\/Unk', work_gov, 'Government')\n\n#Education\nedu_lHS=['12th','7th-8th','9th','10th', '11th','5th-6th','1st-4th','Preschool'] # <HS\nedu_assoc=['Assoc-voc','Assoc-acdm','Some-college'] #Associate\nedu_mpro=['Masters','Prof-school']#Mas-Pro\ncleaner(new_income, 'education', edu_lHS, '<HS', edu_assoc, 'Associate', edu_mpro, 'Mas-Pro')\n\n#Occupation\nnew_income['occupation'].fillna('nan', inplace=True)#We only need to fill nan here\n\n#Marital Status\nsep_div=['Divorced','Separated']#Sep-Div\nmarried=['Married-civ-spouse','Married-spouse-absent','Married-AF-spouse']#Married\ncleaner(new_income, 'marital-status', sep_div, 'Sep-Div', married, 'Married')\n\n#Relationship\nmarr=['Husband', 'Wife']#Married\ncleaner(new_income, 'relationship', marr, 'Married')\n\n#Native Country\neurope=['England', 'Italty', 'Germany', 'France','Yugoslavia', 'Poland', 'Greece', 'Ireland', 'Scotland',\n       'Hungary','Holand-Netherlands','Portugal']\nasia=['China', 'Philippines','Vietnam','Thailand','Taiwan','Laos','Cambodia','Japan', 'Hong','India','Iran']\ncaribbean=['Jamaica','Dominican-Republic','Cuba','Haiti','Trinadad&Tobago', 'Puerto-Rico']\nn_america=['United-States','Canada']\nc_america=['Mexico','Honduras','El-Salvador','Guatemala','Nicaragua']\ns_america=['Columbia','Ecuador','Peru']\ncleaner(new_income, 'native-country', europe, 'Europe', asia, 'Asia', caribbean, 'Caribbean', n_america,'N.America', c_america,'C.America', s_america,'S.America')\nnew_income['native-country']=new_income['native-country'].replace({'South':'nan'})#As we do not know what south is we will just convert it to nan\n\n#Checking the data\nnew_income.info()\nfor label in col_list:\n    print(label, new_income[label].unique())\nnew_income.head()","954d8193":"iris=pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\niris.info()\niris.head() ","977cb9a8":"cleaner(iris,'Species')\ncleaner(iris,'SepalLengthCm')","ee46d2bf":"fig, ax=plt.subplots()# Required for ax parameterization\n\nsns.scatterplot(data=iris, x='SepalLengthCm' ,y='SepalWidthCm', hue='Species', palette=['b','r','g'], s=30)\n\n\n#Ticks\nax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\nplt.xticks(fontsize=12, fontweight='bold', rotation=0)\nplt.yticks(fontsize=12, fontweight='bold')\n\n\n#Labels\nplt.xlabel('Sepal Length (cm)', fontsize=12, fontweight=None, color='k')\nplt.ylabel('Sepal Width (cm)', fontsize=12, fontweight=None, color='k')\n\n#Removing Spines and setting up remianing\nax.spines['top'].set_color(None)\nax.spines['right'].set_color(None)\nax.spines['bottom'].set_color('k')\nax.spines['bottom'].set_linewidth(3)\nax.spines['left'].set_color('k')\nax.spines['left'].set_linewidth(3)\n\nplt.savefig('Sepal.png')","e05c5e6c":"def Plotter(plot, x_label, y_label, x_rot=None, y_rot=None,  fontsize=12, fontweight=None, legend=None, save=False,save_name=None):\n    \"\"\"\n    Helper function to make a quick consistent plot with few easy changes for aesthetics.\n    Input:\n    plot: sns or matplot plotting function\n    x_label: x_label as string\n    y_label: y_label as string\n    x_rot: x-tick rotation, default=None, can be int 0-360\n    y_rot: y-tick rotation, default=None, can be int 0-360\n    fontsize: size of plot font on axis, defaul=12, can be int\/float\n    fontweight: Adding character to font, default=None, can be 'bold'\n    legend: Choice of including legend, default=None, bool, True:False\n    save: Saves image output, default=False, bool\n    save_name: Name of output image file as .png. Requires Save to be True.\n               default=None, string: 'Insert Name.png'\n    Output: A customized plot based on given parameters and an output file\n    \n    \"\"\"\n    #Ticks\n    ax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\n    plt.xticks(fontsize=fontsize, fontweight=fontweight, rotation=x_rot)\n    plt.yticks(fontsize=fontsize, fontweight=fontweight, rotation=y_rot)\n\n    #Legend\n    if legend==None:\n        pass\n    elif legend==True:\n        plt.legend()\n        ax.legend()\n        pass\n    else:\n        ax.legend().remove()\n        \n    #Labels\n    plt.xlabel(x_label, fontsize=fontsize, fontweight=fontweight, color='k')\n    plt.ylabel(y_label, fontsize=fontsize, fontweight=fontweight, color='k')\n\n    #Removing Spines and setting up remianing, preset prior to use.\n    ax.spines['top'].set_color(None)\n    ax.spines['right'].set_color(None)\n    ax.spines['bottom'].set_color('k')\n    ax.spines['bottom'].set_linewidth(3)\n    ax.spines['left'].set_color('k')\n    ax.spines['left'].set_linewidth(3)\n    \n    if save==True:\n        plt.savefig(save_name)\n    ","1b320b75":"fig, ax=plt.subplots()#Required outside of function. This needs to be activated first when plotting in every code block\nplot=sns.scatterplot(data=iris, x=\t'SepalLengthCm' ,y='SepalWidthCm', hue='Species', palette=['b','r','g'], s=30)#Scatter plot\nPlotter(plot, 'SepalLength (cm)', 'SepalWidth (cm)', legend=True, save=True, save_name='Scatter.png')#Plotter function for aesthetics\nplot","dd7e9de5":"fig, ax=plt.subplots()\nplot=sns.boxplot(data=iris, x=\t'Species' ,y='SepalWidthCm', palette=sns.color_palette(\"mako\"))#Generates box plot\nplot=sns.swarmplot(data=iris, x=\t'Species' ,y='SepalWidthCm', palette=['red'], marker='^')#Generates points as triangles\nPlotter(plot, 'Species', 'SepalWidth (cm)',20,None,10,legend=None, save=True, save_name='Box-Swarm.png')\nplot","043dd709":"fig, ax=plt.subplots()\nplot=sns.histplot(iris, x='SepalLengthCm', hue='Species', element=\"step\",stat=\"density\",kde=True, common_norm=False,)\nPlotter(plot, 'Sepal Length (cm)', 'Density',legend=None, save=True, save_name='histo.png')\nplot","cc42c979":"fig, ax=plt.subplots()#Required outside of function. This needs to be activated first when plotting in every code block\nplot=sns.heatmap(iris.corr(),annot=True, cmap='Blues', linewidths=1)\nPlotter(plot, None, None, 50,legend=False, save=True, save_name='Corr.png')\n#plot=plt.hist(iris.groupby('Species')['SepalWidthCm'], label=iris['Species'])\n#Plotter(plot, 'Species', 'SepalWidth (cm)',legend=True, save=True, save_name='Box-Swarm.png')\n#plot","dbc28322":"lter=pd.read_csv('\/kaggle\/input\/palmer-archipelago-antarctica-penguin-data\/penguins_lter.csv')\nlter.info()","21686346":"print('***Species',lter['Species'].unique())\nprint('***Sex',lter['Sex'].unique())","cb1b7328":"new_lter=lter.copy()\n#Species name\ncleaner(new_lter,'Species',old_label1=['Adelie Penguin (Pygoscelis adeliae)'], new_label1='P.adeliae',old_label2=['Chinstrap penguin (Pygoscelis antarctica)'],\n        new_label2='P.antartica',old_label3=['Gentoo penguin (Pygoscelis papua)'],new_label3='P.papua')\nprint(new_lter['Species'].unique())\n\n#Culmen Length\ncleaner(new_lter,'Culmen Length (mm)', group_by=True, category='Species', score='mean')#Fill in numerical data by species mean\n\n#Culmen Depth\ncleaner(new_lter,'Culmen Depth (mm)', group_by=True, category='Species', score='mean')\n\n#Flipper Length\ncleaner(new_lter,'Flipper Length (mm)', group_by=True, category='Species', score='mean')\n\n#Body Mass\ncleaner(new_lter,'Body Mass (g)', group_by=True, category='Species', score='mean')\nnew_lter['Body Mass (kg)']=new_lter['Body Mass (g)']\/1000 #Conversion to kilograms\n\n#Sex \nfiller=['nan','.']\ncleaner(new_lter,'Sex',old_label1=filler, new_label1=new_lter['Sex'].mode()[0],\n        old_label2=['MALE'],new_label2='male', old_label3=['FEMALE'], new_label3='female')\nprint(new_lter['Sex'].unique())\n\n#Delta 15N\ncleaner(new_lter,'Delta 15 N (o\/oo)', group_by=True, category='Species', score='mean')\n\n#Delta 13C\ncleaner(new_lter,'Delta 13 C (o\/oo)', group_by=True, category='Species', score='mean')\n\n#Dropping Features\nnew_lter=new_lter.drop(['Body Mass (g)','studyName','Sample Number','Comments','Individual ID',\n                       'Region', 'Stage', 'Date Egg'], axis=1)\n\nprint(new_lter.info())#Lets check to make sure all NaN is gone\nnew_lter.head()#Visualize the table to confirm the changes we wanted","5bcfa665":"fig, ax=plt.subplots()\nplot=sns.countplot(data=new_lter, x='Species', hue='Sex', palette=['darkblue','darkred'])\nPlotter(plot, x_label='Species', y_label='Count',legend=False, save=True, save_name='Penguin Species Count.png')\nplot","46938474":"fig, ax=plt.subplots()\nplot=sns.boxplot(data=new_lter, x=\"Species\", y='Body Mass (kg)', palette=['darkblue','darkred','darkgreen'])\nPlotter(plot,'Species', 'Body Mass (kg)',x_rot=20, legend=False, save=True, save_name='Species-Mass.png')\nplot","9d0e650b":"#Label Encoding\nfrom sklearn import preprocessing \nLE=preprocessing.LabelEncoder()\n\nlter_encode=new_lter.copy()\nlter_encode['Island']=LE.fit_transform(lter_encode['Island'])\nlter_encode['Clutch Completion']=LE.fit_transform(lter_encode['Clutch Completion'])\nlter_encode['Sex']=LE.fit_transform(lter_encode['Sex'])\nlter_encode['Species_Code']=LE.fit_transform(lter_encode['Species']) #This will be used for a correlation matrix\nlter_encode.head()","52e638fa":"#Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nX=lter_encode.drop(['Species', 'Species_Code'], axis=1)\nY=lter_encode['Species']\nbestfeatures = SelectKBest(score_func=f_classif, k='all')\nfit = bestfeatures.fit(X,Y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(12,'Score'))  #print 10 best features\n\n#Plotting Feature Importance\nfig, ax = plt.subplots(figsize=(7,7))\nplot=sns.barplot(data=featureScores, x='Score', y='Feature', palette='viridis',linewidth=0.5, saturation=2, orient='h')\nPlotter(plot, 'Importance Score', 'Feature', legend=False, save=True, save_name='Feature Importance.png')\nplot","a545cb29":"fig, ax = plt.subplots(figsize=(7,7))\nplot=sns.heatmap(lter_encode.corr(),annot=True, linewidths=0.1)\nPlotter(plot,None,None,90, fontweight='bold',legend=None,save=True, save_name='Corr Matrix.png')\nplot","dd64d68a":"OHE=preprocessing.OneHotEncoder()\nlter_encode_drop=lter_encode.copy()\nlter_encode_drop=lter_encode_drop.drop(['Sex','Clutch Completion','Species_Code'], axis=1)\nlter_ohe=lter_encode_drop.copy()\nlter_code=OHE.fit_transform(lter_ohe[['Island']]).toarray()\nlter_list=list(sorted(new_lter['Island'].unique()))  #Will name the column values to the respective Island\nlter_code=pd.DataFrame(lter_code, columns=lter_list)#Setting OHE dataframe for merge\nlter_ohe=pd.concat([lter_code,lter_ohe], axis=1)#Merging Data Frames\nlter_ohe=lter_ohe.drop(['Island'], axis=1)","e3efa5d8":"#Splitting\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(lter_ohe.drop(['Species'], axis=1), lter_ohe['Species'],test_size=0.25, random_state=0)\n\n#Scaling\nscaler=preprocessing.StandardScaler()\n\nX_train_scaled=scaler.fit_transform(X_train) #Scaling and fitting the training set to a model\nX_test_scaled=scaler.transform(X_test) #Transformation of testing set based off of trained scaler model\n\n#PCA Dimension Reduction\nfrom sklearn.decomposition import PCA\nn=5 #Number of components\npca=PCA(n_components=n)\nX_train_pca=pca.fit_transform(X_train_scaled)# Fitting and transforming the training data\nX_test_pca=pca.transform(X_test_scaled)# Transforming the test data by the fitted trained PCA()","e67ccc5a":"from sklearn.svm import SVC #Classifier\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV #Paramterizers\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix #Accuracy metrics\nimport itertools #Used for iterations","5750fb78":"def Searcher(estimator, param_grid, search, train_x, train_y, test_x, test_y,label=None):\n    \"\"\"\n    This is a helper function for tuning hyperparameters using the two search methods.\n    Methods must be GridSearchCV or RandomizedSearchCV.\n    Inputs:\n        estimator: Any Classifier\n        param_grid: Range of parameters to search\n        search: Grid search or Randomized search\n        train_x: input variable of your X_train variables \n        train_y: input variable of your y_train variables\n        test_x: input variable of your X_test variables\n        test_y: input variable of your y_test variables\n        label: str to print estimator, default=None\n    Output:\n        Returns the estimator instance, clf\n        \n    Modified from: https:\/\/www.kaggle.com\/crawford\/hyperparameter-search-comparison-grid-vs-random#To-standardize-or-not-to-standardize\n    \n    \"\"\"   \n    from sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV #Paramterizers\n    from sklearn.metrics import accuracy_score, classification_report #Accuracy metrics\n    import itertools #Used for iterations\n    \n    try:\n        if search == \"grid\":\n            clf = GridSearchCV(\n                estimator=estimator, \n                param_grid=param_grid, \n                scoring=None,\n                n_jobs=-1, \n                cv=10, #Cross-validation at 10 replicates\n                verbose=0,\n                return_train_score=True\n            )\n        elif search == \"random\":           \n            clf = RandomizedSearchCV(\n                estimator=estimator,\n                param_distributions=param_grid,\n                n_iter=10,\n                n_jobs=-1,\n                cv=10,\n                verbose=0,\n                random_state=1,\n                return_train_score=True\n            )\n    except:\n        print('Search argument has to be \"grid\" or \"random\"')\n        sys.exit(0) #Exits program if not grid or random\n        \n    # Fit the model\n    clf.fit(X=train_x, y=train_y)\n    \n    #Testing the model\n    try:\n        if search=='grid':\n            cfmatrix=confusion_matrix(\n            y_true=test_y, y_pred=clf.predict(test_x))\n        \n            #Defining prints for accuracy metrics of grid\n            print(\"**Grid search results of\", label,\"**\")\n            print(\"The best parameters are:\",clf.best_params_)\n            print(\"Best training accuracy:\\t\", clf.best_score_)\n            print('Classification Report:')\n            print(classification_report(y_true=test_y, y_pred=clf.predict(test_x))\n             )\n        elif search == 'random':\n            cfmatrix=confusion_matrix(\n            y_true=test_y, y_pred=clf.predict(test_x))\n\n            #Defining prints for accuracy metrics of grid\n          \n            print(\"**Random search results of\", label,\"**\")\n            print(\"The best parameters are:\",clf.best_params_)\n            print(\"Best training accuracy:\\t\", clf.best_score_)\n            print('Classification Report:')\n            print(classification_report(y_true=test_y, y_pred=clf.predict(test_x))\n                 )\n    except:\n        print('Search argument has to be \"grid\" or \"random\"')\n        sys.exit(0) #Exits program if not grid or random\n        \n    return clf, cfmatrix; #Returns a trained classifier with best parameters","161f45f9":"def plot_confusion_matrix(cm, label,color=None,title=None):\n    \"\"\"\n    Plot for Confusion Matrix:\n    Inputs:\n        cm: sklearn confusion_matrix function for y_true and y_pred as seen in https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html\n        title: title of confusion matrix as a 'string', default=None\n        label: the unique label that represents classes for prediction can be done as sorted(dataframe['labels'].unique()).\n        color: confusion matrix color, default=None, set as a plt.cm.color, based on matplot lib color gradients\n    \"\"\"\n    classes=sorted(label)\n    plt.imshow(cm, interpolation='nearest', cmap=color)\n    plt.title(title)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    thresh = cm.mean()\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j]), \n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] < thresh else \"black\") ","d55f406a":"#Grid Search SVM Parameters\nsvm_param = {\n    \"C\": [.01, .1, 1, 5, 10, 100], #Specific parameters to be tested at all combinations\n    \"gamma\": [0, .01, .1, 1, 5, 10, 100],\n    \"kernel\": [\"rbf\",\"linear\",\"poly\"],\n    \"random_state\": [1]}\nsvm_grid, cfmatrix_grid= Searcher(SVC(), svm_param, \"grid\", X_train_pca, y_train, X_test_pca, y_test,label='SVC')\n\nprint('_____'*20)\n#Randomized Search SVM Parameters\nsvm_dist = {\n    \"C\": np.arange(0.01,2, 0.01),   #By using np.arange it will select from randomized values\n    \"gamma\": np.arange(0,1, 0.01),\n    \"kernel\": [\"rbf\",\"linear\",\"poly\"],\n    \"random_state\": [1]}\nsvm_rand, cfmatrix_rand= Searcher(SVC(), svm_dist, \"random\", X_train_pca, y_train, X_test_pca, y_test)\n\n#Plotting the confusion matrices\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title='Random Search Confusion Matrix',label=lter_ohe['Species'].unique(), color=plt.cm.Greens) #grid matrix function\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title='Grid Search Confusion Matrix', label=lter_ohe['Species'].unique(), color=plt.cm.Blues) #randomized matrix function\n\nplt.savefig('confusion.png')","fb1433a7":"As we can see, each feature contains multiple values. Though these look nice for if you are considering of implementing an algorithm, these may be too much. So we will clean the data through two ways: \n* The first way is by looking at each feature manually and filling in the data with several lines of code (this will look ugly)\n* The second way will be to use a predefined function to do the same job.","9c258d3f":"# Exploratory Analysis\nFor this part I will use a different data set for EDA as we can observe relationships with the data while having fun with functions. The data set that will be used here will be the iris dataset. https:\/\/www.kaggle.com\/uciml\/iris","76dbc594":"As we can see, this function provides us with print output telling us per column in the for loop instances such as the number of null values and the number of lists that were replaced for a specified values. If we look at age the code reads that it had no null values as well as no replacements were made. This was because there was no inserted replacements and the function stoped without any label variables. This can give us a secondary confirmation that the changes did occur. By checking the info again and looking at the first few rows we see that we successfully cleaned the data. Lastly, we can print the unique values to see that we have less data. Even though that code block looks bulcky still, it is only because we set many strings to one variable. However, by using a function we can remove parameterizing several for-loops repitiously and let the function do the for loops and null insertion for us. Similarly, when implementing this to other notebooks it is a simple copy and paste of the function at the top of the notebook and it is good to go (for as long as you follow the input alignment).","57175542":"As you can see, this plot looks like most standard plots. The only thing different compared to what others do is that I removed the spines on the top and the right. This is useful for those who use python for graphing aesthetics as sometimes in science, people don't like boxed in plots. We were able to make this plot in several lines of code and could make many more just by tweaking a few features. However, to constantly copy and paste this many lines of code and change it is tedious. Therefore, I defined a function below that uses this line of code and gives us the options of customization still.","9d9462d4":"# Machine Learning with functions\nI am going to import the penguin dataset to demosntrate functions with grid and randomized searches on an SVM. I will skip over most EDA. However, I will work through a typical machine learning flow of data preprocessing and such, incorporating the functions above into it so you can see the full utility on a different data set. This will not be heavy on machine learning procedures but more of the functions. I offer heavy discussion of machine learning procedure in this notebook here if you would like more information to that. Kaggle: https:\/\/www.kaggle.com\/christopherwsmith\/classify-that-penguin-100-accuracy#Cleaning-The-Data.","60f32683":"Using the cleaner function, all features were fixed. Notice part of the cleaner function that was used, group_by, allows us to replace specific values by a specific category and then by either the mean, median or mode. Now with a cleaned data set lets quickly do an EDA to show the plotter function again.","93e1a53d":"Again, we can observe that both Sex and Clutch Completion have almost no correlation to species. Therfore, they will be dropped along with species code as that will no longer be needed either.","743ce5f0":"# Search Function\nI will demonstrate using the search functions for SVC parameterization. This general function can be applied to any estimator of your choosing. It entail cross-validation and uses conditional statements.  ","2dfb9cc5":"## One Hot Encoding\nNext we will expand our categorical feature, Island, in a One-Hot encoding format to expand the data by cateogrical values. Given Island is the only one that is categorical for prediction we will use OHE on that.","4f814fa6":"The two least important features as shown on the bar plot above are Clutch Completion and Sex. We can further verify and compare this by looking at a correlation matrix which considers a pearson (linear) correlation coefficient.","401883b5":"# Conclusion\n* Functions have a vast array of utility for reptitious tasks.\n* The function cleaner allows for a few simple inputs for cleaning data null values and replacements.\n* The function Plotter allows for aesthetic mapping onto a plot for publication ready figures.\n* Functions for machine learning application can allow for a multitude of estimators to be tested with out a ton of repitious code.","3579cc53":"## Importing Penguin Data","4da0bdd8":"# Splitting the Data, Scaling and PCA Dimension Reduction\nFirst, the data will be split so we can train a scaler model to apply to an unknwon (test) data set.","9c13a2d6":"# Next Steps\n* Build a Class or package to use these customizable functions as an import, this will reduce the amount of code used in a notebook.\n* Figure out how to plot ~~distplot,~~ pairplot and more with Plotter function ~~(Note: If someone knows how to plot multiclass KDE plots that would be great to know as I am struggling to do so.~~\n* ~~Add onto the searcher function to print out accuracy scores and confusion matrices.~~  \n\n\nSuggestions?<\/br>\n\n<b>If you stuck around to the end please leave a comment for feedback or upvote!<\/b>  \n\nIf you use any of these functions please add this notebook url reference it in the red brackets within the function. Sometimes others want to see a source.\n\nLike what I have done? Check out my other notebooks here: https:\/\/www.kaggle.com\/christopherwsmith\n\nEDA of penguin data and ML explanation can be found here: https:\/\/www.kaggle.com\/christopherwsmith\/classify-that-penguin-100-accuracy#Conclusion\n\nQuick ML approach for lung cancer prediction: https:\/\/www.kaggle.com\/christopherwsmith\/how-to-predict-lung-cancer-levels-100-accuracy","28597835":"## Modeling","f00e018d":"## Machine Learning With Functions\n### SVM and Grid\/Randomization Search\nFor modeling we will use the support vector machines classifiers (SVC). The SVC's can handle higher dimensional data and genearte hyperplanes for separation and score on a yes (1) no (1) basis. The rulings are decided for where a data point lands within a decision boundary. We can evalute multiple parameters at one using Grid or Randomization Search functions. Grid Search evalutes several input parameters at all combinations input while randomized search looks for the best. Cross-validation is the models self assemessment when trying to find the best parameters on the training data and can be done in \"n\" amount of replicates.","e39a2abf":"The general aesthetics were maintained and can be changed easily by inputing the parameters. By setting x_rot to 20, the angle of the x-tick labels were rotated. Setting 10 to font size reduced the font size by the original. With the new seaborn update to 0.11.0 we can also use this function with the seaborn histogram to plot a nice looking plot.","bfb7e7ae":"## Plotting","bda7e80d":"Both the info and bar functions provide information for missing null values for each feature in the data frame. There is a total of 43,957 rows and the features workclass and occupation are missing so data values. These missing values are marked as null and can harm a machine learning algorithm later. To fix this we can clean the data. However, before we clean the data, lets take a look at all of the unique values of our categorical data.","7a8d7d9e":"# Welcome to the kernel\nI decided to make this kernel as I found making functions for reptiotious tasks to be a little difficult when begining.\n# Introduction\nFor beginners, working on cleaning and exploring data can get very annoying having to repitiously utilize several lines of code. Ways to come around those is by using functions. Functions allow us to call many parameters\/lines of code in a single command. This kernel will look at a function used to clean data and then used the clean data for exploratory data analysis. Furthermore, I will then apply this to an ML method using both functions for cleaning and brief EDA. I will then apply functions for machine learning search parameterization and confusion matrices. The overall idea of using custom functions is to reduce the amount of code for repitious tasks. As well as for implementation in multiple notebooks.\n\nI hope any one at any skill of python can find something to take away from this. If not please leave a comment. If you use any of these functions please add this notebook url reference it in the red brackets within the function. Sometimes others want to see a source.\n\nDisclaimer: These functions provided here were done by myself. These may not be the most efficient approaches to problems but they are an approach.","7e715ebc":"As we can see, iris has no null values and is overall numerical in features so we will not need to worry about cleaning the data at all. However, to again demonstrate the function above in how it will not make unnecessary changes we will run it through the function.","51d2aacd":"## Quick Feature Selection and Encoding\n### Label Encoding\nUsing label encoding will allow the conversion of categorical values to be changed to ordinal per column which can be then used for feature selection by correlation matrix. However, note that none of these variables are not truly ordinal and if selected will be converted to a One Hot Encoding (OHE) scheme as that is more appropriate for categorical data.\n### Feature Selection\nBefore building models, it is good practice to remove features that may be unneccessary. We can do this using the SelectKBest function in the sklearn package. This will give us a score of importance by ANOVA F-ratios. The higher the ratio the more important the feature will be.","cbf68e14":"## Quick EDA","4dd4eaf0":"We can extend this function as well to a correlation matrix.","a17ed6be":"There are some missing values within feature #'s 9-15. We will use the cleaner function to fix these values. Lets first print our categoricals.","a1ad0913":"## Plot Function","316a51ff":"# Data Cleaning Function\nThe data I chose to demonstrate this functon is from the income dataset for binary classification (https:\/\/www.kaggle.com\/mastmustu\/income). This data set offers a vast array of features that have null values and several values per feature that we can consolidate with the function.","8a777553":"The same exact plot was generated using the function above. We can further show the power of using the plot function once as seen below with the box plots.","95900faf":"# Confusion Matrix Funtion:\nSetting aesthetics for confusion matrices is also easy through using functions as well. As can be seen below. When using this function, all inputs are required except title.","d563bf30":"# All Data Import","650ee381":"## Cleaning The Data\nWe will now fill all of the missing values and abbreviate the long species names.","85544996":"As we can see we successfull filled every null value and condensed the number of values per column. However, that line of code was pretty ugly and can be summarized in a function. Lets look at how we can turn this into a function.","0d18185b":"# <u> <b> Tutorial: Quick, Custom, and Helpful Functions <\/b> <\/u>\n## Author: Christopher W. Smith\n\nUnfinished, Updated:10\/13\/2020\n\n\nlinkdin: www.linkedin.com\/in\/christopher-w-smith022\n\n![image.png](attachment:image.png)","286aa395":"When performing machine learning tasks it is good practice to do an EDA first to get a feel for the data. Many plot tools exist and I will be using matplotlib and seaborn for aesthetics. A common thing that I do see on Kaggle are users making the same generic plots that are boxed in and use  several lines of code. I will demonstrate the same result using several lines of code and just one function to obtain the same result. The function allows for easy consistent plotting aesthitics that can look publication ready. \n  \nFirst lets look at the relationship between sepal length and width for all three species on a scatterplot.","ad2bcd3d":"First, importing the data, we see that there are 9 categroical values, 5 integers and the target column of income_>50K. To find out if we have missing values below we can us the missingo bar() function.","dfbc1f59":"# Importing Releavant Libraries","786a5b93":"Though the function above is a large chunk of code, it can be universally applied to every feature in this data set. We can also monitor how many lists this function works with as it will give us a print output. The way this function works is by inserting the inputs into cleaner() as described in <span style=\"color:red\">red text in the code chunk<\/span>. The function then uses the foreloops (similar to above) to replace an inserted list of values with a specific value. This function can be expanded for more but after completing each for loop it will move on to the next conditional statement of if's. If the function recognized to the default of None input then it will stop the function there (<b><span style=\"color:green\">return<\/span><\/b>) and print the number of lists it worked with.","54702e7a":"Though this function is helpful for plotting, a limitation of it is that it is hard to work with certain graphing aspects. The current limitations I have found using it so far is a struggle is using lmplot for multiple classes and pairplot. An error occurs where two plots show up and one is blank for the first two, and pairplot does not plot the changes. That will be a problem to resolve in the future and is out of the scope of this tutorial. However, I will lastly demonstrate utility of functions using them with machine learning tasks."}}