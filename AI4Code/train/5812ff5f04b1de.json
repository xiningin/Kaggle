{"cell_type":{"1f4e35f7":"code","f2484ef0":"code","7c4b9cc8":"code","76c6e067":"code","91c809d5":"code","dc378d99":"code","3f4bb91f":"code","621164fb":"code","1a305d82":"code","11c7c9d3":"code","7f4b6c70":"code","ac5e49f4":"code","03eb406b":"code","699d5d45":"code","d69a3486":"code","0d139f56":"code","52a52287":"code","bd78d94b":"code","45b13c69":"code","51a5ec17":"code","e264a5ec":"code","33d5bb73":"code","d76b4e12":"markdown","0158f38c":"markdown","93a4c061":"markdown","201518da":"markdown","61eae4c8":"markdown","04c1e197":"markdown","89167b06":"markdown","4a8da606":"markdown","4d5960c6":"markdown","9fb69bd8":"markdown","46b2d554":"markdown","a468c3f2":"markdown","2171ad9c":"markdown","163a3d84":"markdown","1dc75d02":"markdown","146d4a8d":"markdown","4c76bcca":"markdown","55b231c0":"markdown","af1abddc":"markdown"},"source":{"1f4e35f7":"import numpy as np \nimport pandas as pd\nimport torch\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline","f2484ef0":"# \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0440\u0430\u0431\u043e\u0447\u0438\u0435 \u0434\u0438\u0440\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438 \u0434\u043b\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a Muskmelon \u0438 Carambola\n# \u043f\u0435\u0440\u0435\u043d\u0435\u0441\u0451\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0441 Muskmelon \u0438 Carambola \u0432 \u0440\u0430\u0431\u043e\u0447\u0438\u0435 \u0434\u0438\u0440\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438\n\n# \u0444\u0440\u0443\u043a\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c \u0434\u0440\u0443\u0433 \u0441 \u0434\u0440\u0443\u0433\u043e\u043c\nfruits = ['Carambola', 'Muskmelon']\n\n# \u043e\u0431\u0449\u0438\u0435 \u043f\u0443\u0442\u0438 \u0434\u043b\u044f \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0439\nworking_dir = '..\/output\/kaggle\/working\/'\nbase_dir = '..\/input\/fruit-recognition\/'\n        \ndef create_working_dirs_and_move_images():\n    if(not os.path.isdir(base_dir)):\n        print('\u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0438\u0441\u0445\u043e\u0434\u043d\u0430\u044f \u0431\u0430\u0437\u0430 \u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438: ', base_dir)\n    \n    # \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0444\u0440\u0443\u043a\u0442\u0430 \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u043d\u043e\u0432\u0443\u044e \u0440\u0430\u0431\u043e\u0447\u0443\u044e \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044e \u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u0449\u0430\u0435\u043c \u0442\u0443\u0434\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n    for fruit in fruits:\n        # \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u043f\u0443\u0442\u044f\u0445 \u0438\u043c\u0435\u043d\u0430 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0439 \u0438\u043d\u043e\u0433\u0434\u0430 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u0441 \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u043e\u0439 \u0431\u0443\u043a\u0432\u044b\n        if(fruit == 'Muskmelon'): \n            fruit_source = 'muskmelon'\n        else:\n            fruit_source = fruit\n        \n        # \u0438\u0441\u0445\u043e\u0434\u043d\u0430\u044f \u0438 \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044f \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0444\u0440\u0443\u043a\u0442\u0430\n        fruit_source_path = os.path.join(base_dir, fruit_source)\n        fruit_destination_path = os.path.join(working_dir, fruit)\n        \n        if(not os.path.isdir(fruit_source_path)):\n            print('\u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0438\u0441\u0445\u043e\u0434\u043d\u0430\u044f \u0431\u0430\u0437\u0430 \u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438: ', fruit_source_path)\n            break\n        \n        if(not os.path.isdir(fruit_destination_path)):\n            # \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u0440\u0430\u0431\u043e\u0447\u0443\u044e \u0434\u0438\u0440\u0435\u0442\u043a\u043e\u0440\u0438\u044e\n            os.mkdir(os.path.join('\/kaggle\/working', fruit))\n            # \u043f\u0435\u0440\u0435\u0435\u0449\u0430\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n            shutil.copytree(fruit_source_path, fruit_destination_path)\n            print('\\n\u0421\u043e\u0437\u0434\u0430\u043d\u0430 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044f ', fruit_destination_path)\n        else:\n            print('\\n\u0420\u0430\u0431\u043e\u0447\u0430\u044f \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044f ', fruit_destination_path, '\u0443\u0436\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 (\u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043d\u0435 \u0431\u0443\u0434\u0443\u0442 \u043f\u043e\u0432\u0442\u043e\u0440\u043d\u043e \u043f\u0435\u0440\u0435\u043c\u0435\u0449\u0430\u0442\u044c\u0441\u044f)')\n        \n        # \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0447\u0438\u0441\u043b\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0444\u0440\u0443\u043a\u0442\u0430 \u0432 \u0440\u0430\u0431\u043e\u0447\u0435\u0439 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438\n        path, dirs, files = next(os.walk(fruit_destination_path))\n        img_count = len(files)\n        print(fruit, ': ', fruit_destination_path, ' - ', img_count, ' \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a')\n        \n    # \u043e\u0442\u043e\u0431\u0440\u0430\u0437\u0438\u043c \u0432\u0441\u0435 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438 \u0432 \u0440\u0430\u0431\u043e\u0447\u0435\u0439 \u043f\u0430\u043f\u043a\u0435\n    print('\\n\u0412\u0441\u0435 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438 \u0432 \u0440\u0430\u0431\u043e\u0447\u0435\u043c \u043f\u0443\u0442\u0438:', os.listdir(working_dir))\n\ncreate_working_dirs_and_move_images()","7c4b9cc8":"# \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u0432\u0441\u0435\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor, Compose, Grayscale\n\n# \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0432 \u043e\u0434\u043d\u043e\u043a\u0430\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0444\u043e\u0440\u043c\u0430\u0442 (\u0447\u0451\u0440\u043d\u043e-\u0431\u0435\u043b\u044b\u0439 \u0432 \u0438\u0442\u043e\u0433\u0435) \u0438 \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\ncomposed_transformers = Compose([Grayscale(num_output_channels=1), ToTensor()])\n# composed_transformers = Compose([ToTensor()])\ndata = ImageFolder('..\/output\/kaggle\/working', composed_transformers)\n\n# \u043e\u0431\u0449\u0438\u0439 \u043e\u0431\u044a\u0451\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\nprint('\u0412\u0441\u0435\u0433\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a: ', len(data))\n\n# \u043f\u0435\u0440\u0435\u0431\u043e\u0440 \u0432\u0441\u0435\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0441 \u0444\u0440\u0443\u043a\u0442\u0430\u043c\u0438 \u0434\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0451\u0442\u0430 \u0447\u0438\u0441\u043b\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0441 \u0440\u0430\u0437\u043d\u044b\u043c\u0438 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044f\u043c\u0438\nfor i in range(len(data)):\n    img, label = data[i]\n    if(i == 0):\n        n = 1\n        previous_img_shape = img.shape\n        previous_label = label\n        continue\n    \n    if( i == len(data)-1 ):\n        n += 1\n        print('\u0420\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0430:', img.shape, \" | \u041a\u043b\u0430\u0441\u0441:\", label, ' | \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a:', n)\n   \n    if(previous_img_shape == img.shape and previous_label == label): \n        n += 1\n        continue\n    else:\n        print('\u0420\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0430:', previous_img_shape, ' | \u041a\u043b\u0430\u0441\u0441:', previous_label, ' | \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a:', n)\n        n = 1\n    \n    previous_img_shape = img.shape\n    previous_label = label","76c6e067":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043e\u0434\u043d\u043e\u0433\u043e \u0444\u0440\u0443\u043a\u0442\u0430\ndef show_img(img, label):\n    print('\u0424\u0440\u0443\u043a\u0442:', data.classes[label], ' | \u041a\u043b\u0430\u0441\u0441:', label)\n    print(img.shape)\n    # \u043e\u0442\u043e\u0431\u0440\u0430\u0437\u0438\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0443\u0431\u0440\u0430\u0432 \u043b\u0438\u0448\u043d\u044e\u044e \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c (\u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u043a\u0430\u043d\u0430\u043b\u0430, \u0442.\u043a. \u043e\u043d \u043e\u0434\u0438\u043d)\n    image_squeezed = torch.squeeze(img)\n    plt.imshow(image_squeezed)\n    #plt.imshow(img.permute(1, 2, 0), cmap='Greys')\n\n# \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u0443\u044e \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 (* - \u0440\u0430\u0441\u043a\u0440\u044b\u0432\u0430\u0435\u0442 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432 tuple)\nshow_img(*data[3])","91c809d5":"# \u0441\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c train \u0438 test \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nfrom torch.utils.data import random_split\n\ntorch.manual_seed(42)\n\ntrain_size = int(round(0.75*len(data)))\nval_size = 0\ntest_size = len(data) - train_size\n#val_size = int(round(0.2*len(data)))\n#test_size = len(data) - train_size - val_size\n\ntrain, val, test = random_split(data, [train_size, val_size, test_size])","dc378d99":"# \u043e\u0442\u043e\u0431\u0440\u0430\u0437\u0438\u043c \u043e\u0434\u0438\u043d \u044d\u043b\u0435\u043c\u0435\u043d\u0442 \u0438\u0437 train\ntrain[3]","3f4bb91f":"# \u0437\u0430\u0433\u0440\u0443\u0437\u0447\u0438\u043a\u0438 \u0432\u044b\u0431\u043e\u0440\u043e\u043a\n\n# \u0440\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430\nbatch_size = 8\n\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=4)\n#val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=4)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=4)","621164fb":"# \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u0431\u0430\u0442\u0447 \u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438 (32 \u0448\u0442.) \u0434\u043b\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438 train\nfrom torchvision.utils import make_grid\n\nfor images, _ in train_loader:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=8).permute((1, 2, 0)))\n    break","1a305d82":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass neural(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(1*258*320, 128)\n        self.fc2 = nn.Linear(128, 32)\n        self.fc3 = nn.Linear(32, 2)\n\n    def forward(self, x):\n        # Flatten images into vectors\n        x = x.view(x.size(0), -1)\n        # Apply layers & activation functions\n        x = F.sigmoid(self.fc1(x))\n        x = F.sigmoid(self.fc2(x))\n        x = self.fc3(x)\n        return F.softmax(x, dim=1)\n\nnet = neural()\nprint(net)","11c7c9d3":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)","7f4b6c70":"for epoch in range(2):  # \u043f\u0440\u043e\u0445\u043e\u0434\u0438\u043c \u0447\u0435\u0440\u0435\u0437 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437\n\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 [inputs, labels]\n        inputs, labels = data\n\n        # \u043e\u0431\u043d\u0443\u043b\u044f\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430\n        running_loss += loss.item()\n        \n        # \u043f\u0435\u0447\u0430\u0442\u044c \u043a\u0430\u0436\u0434\u044b\u0435 100 \u043c\u0438\u043d\u0438-\u0431\u0430\u0442\u0447\u0435\u0439\n        if i % 100 == 99:\n            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss \/ 100))\n            running_loss = 0.0\n\nprint('\u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430 \u043e\u043a\u043e\u043d\u0447\u0435\u043d\u0430')","ac5e49f4":"# \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043d\u0430\u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0433\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f\nPATH = '.\/fruit_net.pth'\ntorch.save(net.state_dict(), PATH)","03eb406b":"# \u043e\u0442\u043e\u0431\u0440\u0430\u0437\u0438\u043c 8 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a (\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c \u0431\u0430\u0442\u0447\u0430) \u0438\u0437 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n\n# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\ndef imshow(img):\n    plt.figure(figsize=(16,8))\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0445\u043e\u0434\u0438\u043c \u043f\u043e \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435\ndataiter = iter(test_loader)\nimages, labels = dataiter.next()\n\n# \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\nimshow(make_grid(images))\n# \u043f\u0435\u0447\u0430\u0442\u0430\u0435\u043c \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u044c \u043a \u043a\u043b\u0430\u0441\u0441\u0443\nclasses = ('Carambola', 'Muskmelon')\nprint(' '.join('%5s' % classes[labels[j]] for j in range(8)))","699d5d45":"# \u043e\u0442\u043e\u0431\u0440\u0430\u0437\u0438\u043c \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u0441\u0435\u0442\u0438 \u0434\u043b\u044f \u0442\u0435\u0445 \u0436\u0435 \u0441\u0430\u043c\u044b\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a\n\n# \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0441\u0435\u0442\u044c\nnet.load_state_dict(torch.load(PATH))\n\n# \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0447\u0435\u0440\u0435\u0437 \u0441\u0435\u0442\u044c \u0434\u043b\u044f \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0430\noutputs = net(images)\n\n# \u0412\u044b\u0445\u043e\u0434\u043e\u043c \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0438\u043d\u043f\u0443\u043b\u044c\u0441 \u0434\u043b\u044f \u0434\u0432\u0443\u0445 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \n# \u0427\u0435\u043c \u0441\u0438\u043b\u044c\u043d\u0435\u0435 \u0438\u043c\u043f\u0443\u043b\u044c\u0441, \u0442\u0435\u043c \u0431\u043e\u043b\u0435\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e \u0441\u0435\u0442\u044c \u043e\u0442\u043d\u043e\u0441\u0438\u0442 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u043a \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0451\u043d\u043d\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443\n# \u0417\u0434\u0435\u0441\u044c \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0438\u043d\u0434\u0435\u043a\u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0438\u043c\u043f\u0443\u043b\u044c\u0441\u0430\n_, predicted = torch.max(outputs, 1)\nprint('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(8)))","d69a3486":"correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0441\u0435\u0442\u0438 \u043d\u0430 \u0432\u0441\u0435\u0439 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435: %d %%' % (100 * correct \/ total))","0d139f56":"# \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0444\u0440\u0443\u043a\u0442\u043e\u0432\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(4):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(2):\n    print('\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0434\u043b\u044f %5s : %2d %%' % (classes[i], 100 * class_correct[i] \/ class_total[i]))","52a52287":"# \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e\u0441\u0442\u044c GPU\n# \u0435\u0441\u043b\u0438 \u043c\u044b \u043d\u0430 GPU, \u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0432\u0435\u0434\u0435\u043d\u043e \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0435 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u0435, \u0438\u043d\u0430\u0447\u0435 - \u043e\u0442\u043e\u0431\u0440\u0430\u0437\u0438\u0442\u0441\u044f CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nif(not torch.cuda.is_available()):\n    print('\u0422\u0435\u043a\u0443\u0449\u0438\u0439 device - CPU. \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u044c GPU (\u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0430 \u043d\u0435 \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0430)')\nelse:\n    print(device)","bd78d94b":"# \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c \u0441\u0435\u0442\u044c \u043d\u0430 GPU\nnet.to(device)","45b13c69":"# \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c loss function \u043d\u0430 GPU\ncriterion = nn.CrossEntropyLoss().cuda()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)","51a5ec17":"# \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 (inputs, labels = data) \u043d\u0430 GPU \u0438 \u0442\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u0441\u0435\u0442\u044c \u0437\u0430\u043d\u043e\u0432\u043e\n\nfor epoch in range(2):  # \u043f\u0440\u043e\u0445\u043e\u0434\u0438\u043c \u0447\u0435\u0440\u0435\u0437 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437\n\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 [inputs, labels]\n        # inputs, labels = data\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # \u043e\u0431\u043d\u0443\u043b\u044f\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430\n        running_loss += loss.item()\n        \n        # \u043f\u0435\u0447\u0430\u0442\u044c \u043a\u0430\u0436\u0434\u044b\u0435 100 \u043c\u0438\u043d\u0438-\u0431\u0430\u0442\u0447\u0435\u0439\n        if i % 100 == 99:\n            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss \/ 100))\n            running_loss = 0.0\n\nprint('\u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430 \u043e\u043a\u043e\u043d\u0447\u0435\u043d\u0430')","e264a5ec":"correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in test_loader:\n        #images, labels = data\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0441\u0435\u0442\u0438 \u043d\u0430 \u0432\u0441\u0435\u0439 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435: %d %%' % (100 * correct \/ total))","33d5bb73":"# \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0444\u0440\u0443\u043a\u0442\u043e\u0432\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(4):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(2):\n    print('\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0434\u043b\u044f %5s : %2d %%' % (classes[i], 100 * class_correct[i] \/ class_total[i]))","d76b4e12":"# \u041f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a","0158f38c":"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u043b\u044f Muskmelon \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u043b\u0443\u0447\u0448\u0435","93a4c061":"### \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u043a\u0430\u043a\u0438\u0435 \u0444\u0440\u0443\u043a\u0442\u044b \u0441\u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b \u043b\u0443\u0447\u0448\u0435, \u0430 \u043a\u0430\u043a\u0438\u0435 \u0445\u0443\u0436\u0435","201518da":"### \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c Loss function","61eae4c8":"# Pytorch \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 CPU","04c1e197":"# \u0424\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0432\u044b\u0431\u043e\u0440\u043e\u043a","89167b06":"### \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c","4a8da606":"# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0439 \u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a","4d5960c6":"# Pytorch \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 GPU","9fb69bd8":"### \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c","46b2d554":"\u0411\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0451\u043d \u043b\u0438 \u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435 Muskmelon \u0438\u043b\u0438 Carambola (\u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0441 \u044d\u0442\u0438\u043c\u0438 \u0434\u0432\u0443\u043c\u044f \u0444\u0440\u0443\u043a\u0442\u0430\u043c\u0438).\n","a468c3f2":"\u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0440\u043e\u0448\u043e \u043e\u0442\u0440\u0430\u0436\u0430\u0435\u0442 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f","2171ad9c":"\u0422\u0430\u043a\u0430\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c \u043f\u0440\u043e\u0441\u0442\u043e\u0435 \u0443\u0433\u0430\u0434\u044b\u0432\u0430\u043d\u0438\u0435 (\u0441 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c\u044e 50%, \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u044f, \u0447\u0442\u043e \u0443 \u043d\u0430\u0441 \u0432\u0441\u0435\u0433\u043e 2 \u043a\u043b\u0430\u0441\u0441\u0430)","163a3d84":"\u0412\u0438\u0434\u043d\u043e, \u0447\u0442\u043e \u0432\u0441\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0438\u043c\u0435\u044e\u0442 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c 1 x 258 x 320 \u0438 \u043d\u0435 \u0442\u0440\u0435\u0431\u0443\u044e\u0442 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438","1dc75d02":"\u041d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435, \u043d\u0435 \u0441\u0442\u043e\u0438\u0442 \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 GPU \u043f\u043e\u0441\u043b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 CPU, \u0442.\u043a. \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043f\u043b\u044b\u0432\u0451\u0442 (\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u044d\u0442\u043e \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u0441 \u0442\u0435\u043c, \u0447\u0442\u043e \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0438\u0437 \u043f\u0440\u043e\u0448\u043b\u043e\u0439 \u043c\u043e\u0435\u043b\u0438 - Loss Function, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440)","146d4a8d":"### \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u0441\u0435\u0442\u0438 \u043d\u0430 \u0432\u0441\u0435\u0439 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435","4c76bcca":"### \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u043a\u0430\u043a\u0438\u0435 \u0444\u0440\u0443\u043a\u0442\u044b \u0441\u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b \u043b\u0443\u0447\u0448\u0435, \u0430 \u043a\u0430\u043a\u0438\u0435 \u0445\u0443\u0436\u0435","55b231c0":"### \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u0441\u0435\u0442\u0438 \u043d\u0430 \u0432\u0441\u0435\u0439 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435","af1abddc":"### \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u044b \u0441\u0435\u0442\u0438 \u043d\u0430 \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u043c\u0435\u0440\u0430\u0445 \u0438\u0437 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438"}}