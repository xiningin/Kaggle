{"cell_type":{"de179051":"code","757fc8ef":"code","46330891":"code","de9fe969":"code","f0173d6f":"code","5cc55dbe":"code","ea096d56":"code","562462d4":"code","6cf32066":"code","2e6bad2e":"code","166689e7":"code","f5daa4f5":"code","95380eac":"code","fb9c86a6":"code","5afe83b9":"code","d3176b9d":"code","19058501":"code","d9355493":"code","d2733726":"code","4e555d14":"code","a3484846":"code","65f76034":"code","b1bad312":"code","27f6af65":"markdown","473148b4":"markdown","1b6da393":"markdown","3bfccca4":"markdown","72eafec7":"markdown","6add46ae":"markdown","bb0128f0":"markdown","769e017b":"markdown","70dde12a":"markdown","1841a26d":"markdown","996cfe80":"markdown","0bd27f86":"markdown","8db6282e":"markdown","fec02806":"markdown","e67eaea2":"markdown","3e126e28":"markdown","2c915991":"markdown","03ff4e7d":"markdown","724f1075":"markdown"},"source":{"de179051":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nsns.set()\nplt.rcParams['figure.figsize'] = (20, 10)","757fc8ef":"df = pd.read_csv('\/kaggle\/input\/board-game-data\/bgg_db_1806.csv', encoding='latin1')\ndf['weight'].replace(0.0, 1.0, inplace=True) # There are just 9 games with 0.0 weight, missing values?\ndf.head()","46330891":"df.describe()","de9fe969":"fig, ax = plt.subplots(2, 3)\n# Outliers for the below variables cause visualizations to be useless, remove them\nsns.countplot(data=df, x='min_players', ax=ax[0][0])\nsns.countplot(x=df['max_players'].map(lambda x: min(x, 16)), ax=ax[0][1]) # 16 means 16 or more\ndf['weight'].plot.hist(ax=ax[0][2])\nax[0][2].set_xlabel('weight')\ndf['min_time'].map(lambda x: min(x, 300)).plot.hist(ax=ax[1][0])\nax[1][0].set_xlabel('min_time')\ndf['avg_time'].map(lambda x: min(x, 300)).plot.hist(ax=ax[1][1])\nax[1][1].set_xlabel('avg_time')\ndf['max_time'].map(lambda x: min(x, 300)).plot.hist(ax=ax[1][2])\nax[1][2].set_xlabel('max_time');","f0173d6f":"fig, ax = plt.subplots(2, 3)\ndf['avg_rating'].plot.hist(ax=ax[0][0], title='avg_rating')\ndf['geek_rating'].plot.hist(ax=ax[0][1], title='geek_rating')\ndf[df['num_votes'] < df['num_votes'].quantile(0.99)]['num_votes'].plot.hist(ax=ax[0][2], title='num_votes')\ndf['age'].plot.hist(ax=ax[1][0], title='age')\ndf[df['owned'] < df['owned'].quantile(0.99)]['owned'].plot.hist(ax=ax[1][1], title='owned');","5cc55dbe":"(df['max_time'] - df['avg_time']).describe()","ea096d56":"dfsort = df.sort_values(by='min_time').reset_index()\nplt.figure()\nplt.fill_between(dfsort.index, dfsort['min_time'], dfsort['avg_time'], color='purple')\nplt.ylim(-10, 610)\nplt.xlabel('Game number')\nplt.ylabel('Duration')\nplt.title('Range of durations for each game');","562462d4":"sns.scatterplot(x=df['avg_time'].map(lambda x: min(x, 600)), y=df['weight']);","6cf32066":"_, ax = plt.subplots(1, 2)\nplt.sca(ax[0])\nplt.scatter(df['num_votes'], df['geek_rating'], alpha=0.5, c='black', edgecolors='none')\nplt.xlabel('num_votes')\nplt.ylabel('geek_rating')\nplt.title('num_votes vs. geek_rating')\nplt.sca(ax[1])\nplt.scatter(df['num_votes'], df['geek_rating'], alpha=0.5, c='black', edgecolors='none')\nplt.xlabel('num_votes')\nplt.ylabel('geek_rating')\nplt.title('num_votes vs. geek_rating (zoomed)')\nplt.xlim((-100, 10000));","2e6bad2e":"sns.scatterplot(x=df['avg_rating'], y=df['geek_rating'], hue=df['num_votes'].map(np.log), palette=plt.cm.cool, legend=False);","166689e7":"plt.figure()\ndfsort = df.reset_index().sort_values(by='rank')\nx = dfsort['rank']\nplt.plot(x, dfsort['geek_rating'], label='geek_rating')\nplt.plot(x, dfsort['avg_rating'], label='avg_rating')\nplt.legend()\nplt.xlabel('rank')\nplt.ylabel('rating');","f5daa4f5":"authors = set()\npd.Series(df.designer.unique()).map(lambda x: x.split(', ')).apply(lambda x: authors.update(x))\nauthors = pd.Series(index=sorted(authors), name='num_games', data=0)\ndef incrcount(x):\n    for auth in x:\n        authors[auth] += 1\ndf.designer.map(lambda x: x.split(', ')).apply(incrcount)\nauthors.drop('(Uncredited)', inplace=True)\nauthors.sort_values(ascending=False, inplace=True)\nsns.countplot(x=authors.map(lambda x: min(x, 12)))","95380eac":"def popular_auth(auths):\n    return float(any([auth for auth in auths.split(', ') if auth != '(Uncredited)' and authors[auth] > 2]))\ndf['popular_designer'] = df.designer.map(popular_auth)\nsns.countplot(data=df, x='popular_designer');","fb9c86a6":"df['top'] = (df['rank'] < 1000).astype('float64')","5afe83b9":"features = [\n    'min_players',\n    'max_players',\n    'min_time',\n    'avg_time',\n    'age',\n    'owned',\n    'weight',\n    'popular_designer'\n]\ncorr = df[features + ['geek_rating']].corr()\nidx = corr['geek_rating'].abs().sort_values(ascending=False).index\nsns.heatmap(corr.loc[idx, idx], cmap=plt.cm.BrBG, annot=True)","d3176b9d":"sns.pairplot(df[['geek_rating', 'owned', 'weight', 'popular_designer', 'age']]);","19058501":"x_min, x_max = -1000, 45000\ny_min, y_max = 0.8, 5.\nplt.figure(figsize=(16, 8))\nsns.scatterplot(data=df, x='owned', y='weight', hue='top', palette={0.0: 'red', 1.0: 'green'})\nplt.xlim((x_min, x_max))\nplt.ylim((y_min, y_max));","d9355493":"X, y = df[['owned', 'weight']], df['top']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train);","d2733726":"model.score(X_test, y_test)","4e555d14":"theta0 = model.intercept_[0]\ntheta1 = model.coef_[0][0]\ntheta2 = model.coef_[0][1]\nprint(theta0)\nprint(theta1)\nprint(theta2)","a3484846":"xx, yy = np.meshgrid(np.arange(x_min, x_max, 10), np.arange(y_min, y_max+0.05, 0.05))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\nxboundary = np.array([0, 10000])\nyboundary = (-theta0 - theta1 * xboundary) \/ theta2\n\nplt.figure(figsize=(16, 8))\nplt.pcolormesh(xx, yy, Z, cmap=ListedColormap([(1.0, 0.7, 0.7), (0.7, 1.0, 0.7)]))\nsns.scatterplot(data=df, x='owned', y='weight', hue='top', palette={0.0: 'red', 1.0: 'green'})\nplt.plot(xboundary, yboundary, color='black', linewidth=4, linestyle='--')\nplt.xlim((x_min, x_max))\nplt.ylim((y_min, y_max));","65f76034":"probas = model.predict_proba(df[['owned', 'weight']])[:, 1]\nplt.figure(figsize=(16, 8))\nsns.scatterplot(data=df, x='owned', y='weight', hue=probas, palette=plt.cm.RdYlGn, legend=False)\nplt.colorbar(mpl.cm.ScalarMappable(mpl.colors.Normalize(), plt.cm.RdYlGn), label='Predicted probability of y=1')\nplt.plot(xboundary, yboundary, color='black', linewidth=4, linestyle='--')\nplt.xlim((x_min, x_max))\nplt.ylim((y_min, y_max));","b1bad312":"probas = model.predict_proba(df[['owned', 'weight']])[:, 1]\ncosts = -y * np.log(probas) - (1-y) * np.log(1-probas)\nsizes = costs * 10\nplt.figure(figsize=(16, 8))\nplt.scatter(x=df['owned'], y=df['weight'], c=y, s=sizes, cmap=ListedColormap(['red', 'green']))\nplt.plot(xboundary, yboundary, color='black', linewidth=4, linestyle='--')\nplt.xlim((x_min, x_max))\nplt.ylim((y_min, y_max))\nplt.xlabel('owned')\nplt.ylabel('weight')\nplt.title('Cost of each example');","27f6af65":"Finally, the costs per example:","473148b4":"Now, the predicted probabilities:","1b6da393":"# The model\n\nLet's now define and train our model:","3bfccca4":"Now let's see the relationship between complexity and estimated duration:","72eafec7":"So `rank` is calculated using `geek_rating`, as we supposed.\n\nNext, let's try to do something useful with `designer`. Let's figure out how many different designers we have and how many games has each one authored:","6add46ae":"# The Dataset\n\nThis dataset has 4999 rows, each one describing a board game. As described in the dataset page, the dataset comes from the [Board Game Geek page](https:\/\/boardgamegeek.com\/), where the community discusses and rates games. Some of the features it contains:\n\n- Number of players: described in two features, `min_players` (the minimum allowable) and `max_players` (the max). You can play the game as long as you have a number of players in between.\n- Duration: there are three related features, `min_time`, `max_time` and `avg_time`. Describe the estimated minimum, maximum and average time required to play the game.\n- `age`. Minimum age recommended to play the game.\n- Type of game: described through `mechanic` and `category`. The first one describes how players interact with the game (do they build a deck? do they have hidden roles?). The second one describes the tematic (war? medical? economical?) and the physical type of game (cards? wargame?). Both of them consist of a comma-separated list of string values (such that a game may belong to more than one type). For the sake of simplicity, I won't delve into them in this notebook.\n- `designer`. The names of the game authors, separated by commas.\n- `weight`. Indicates how complex the game is, in a scale from 1.0 to 5.0. Lower weight games are shorter, easier to understand and have simpler rules. Games with more weight are more complex, longer ones.\n- `owned` indicates how many people have bought the game.\n- Ranking. Board Game Geek ranks games according to the community's votes. When you vote, you assign the game a number of starts, from 1 to 10. The feature `avg_rating` is an average of the ratings given by the community. `num_votes` indicates how many people voted. To prevent games with very little votes from climbing to the top ranking, BGG computes a Bayesian average by adding \"dummy\" votes scoring 5.5 to the average rating, until the vote count reaches a certain quantity. `geek_rating` represents this average and is the feature that is used to order the games and thus create the `rank` variable.\n\nLet's do some exploratory data analysis next:","bb0128f0":"How good is it? For simplicity, let's use accuracy, although ROC AUC or F1 score may be worth trying, as the dataset is slightly imbalanced.","769e017b":"Most of the authors have designed a single game, some of them have authored 2 or three, and there are a bunch that have done 12 or more (I guess professional ones).\n\nI will create a new feature that tells us if the author has been implied in many other games or not. It intends to measure whether the degree of professionality of the game's designer.","70dde12a":"As expected, there is a strong positive correlation between duration and weight: longer games are generally more heavyweight.\n\nNext, let's confirm the 5.5 vote theory on `geek_rating` (the figure in the right is obtained by zooming into the left corner of the other image):","1841a26d":"# The decision boundary and cost per example\n\nThese visualizations may help us understand better what logistic regression is doing. See [the blog post](https:\/\/anarthal.github.io\/kernel\/posts\/logistic-regression\/) for details.\n\nFirst, the learnt parameters:","996cfe80":"That doesn't make much sense. It means that `max_time` and `avg_time` are almost identical, except for some games that have `max_time` to be less than `avg_time`. I will just ignore `max_time` and use `avg_time`. Let's now see the range of duration each game has, as defined by \\[`min_time`, `avg_time`\\]. Each bar represents an individual game.","0bd27f86":"# Logistic regression using board game data\n\nHi all! In this kernel we will go through a logistic regression example using board game data. This kernel contains the code for [this blog post](https:\/\/anarthal.github.io\/kernel\/posts\/logistic-regression\/). This kernel focuses on getting some intuition about the maths behind logistic regression, including how the cost function works and how the decision boundary is placed.\n\nWe will perform a binary classification of board games. We will try to predict whether a board game is within the best 1000 or not. We will just use two features, which makes visualizing stuff easier. We will perform some exploratory data analysis to decide which features to choose.\n\nIf you found this kernel useful, please upvote! Comments, suggestions and feedback are always welcome.","8db6282e":"# Feature importance\n\nLet's do a quick scan of which feature have the most importance by doing a correlation plot. I won't use any of the ratings or the number of votes, as I consider these part of the target more than the features. I will use `owned`, which I expect to be highly correlated to the rank, though. Features are ordered by importance in the plot.","fec02806":"Decision boundary. The dashed line is plotted solving analytically the decision boundary inequation, as logistic regression is a linear model. To make sure we are doing things right, the colored regions are generated numerically by sampling the classifier in a mesh (folling [this approach](https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_voting_decision_regions.html)).","e67eaea2":"Some quick thoughts:\n\n- Most games require 1-4 players as minimum to play. There isn't a lot of dispersion here. Quite logical, as if you require a lot of people to play a game, it becomes more difficult to arrange a group to play it.\n- There is an awful lot of dispersion in the maximum number of players. I've actually maxed it out to 16, with 16 meaning \"16 or more players\". There isn't a cap here like in min players: the more maximum number of players, the best. Imagine your group ends up having 9+ people.\n- Weight follows a normal-ish distribution, with most games being of medium complexity. There are actually more lightweight games (complexity 1) than super complex games (complexity 5). Simple games (these are called fillers) are targeted to a wider public, people who don't play regularly or are starting.\n- The three time variables have similar distributions. Most games are in the 45min to 1h30min duration. There are a bunch that take longer, too. `avg_time` and `max_time` are more skewed than `min_time`, as it is logical. Note that there are a bunch of games that have really long maximum durations. I've actually grouped them together at 300min, with this quantity meaning 300 or more. These are usually heavyweight games, or games that may become very long on certain circumstances. Note that, for some games, the duration may have a lot of dispersion. Depending on the number of players and the decisions they make into the game, I've seen games take from one hour to more than six (all of this being the same game!).\n- `avg_rating` follows an almost-normal distribution centered on 7.0, approx. This is the community-based rating. We can see that people generally vote when they particularly like a game (there aren't as many haters, apparently!)\n- `geek_rating` has an enormous peak at 5.5. That also makes sense. As we said, for games with only a few votes, a lot of 5.5 \"dummy\" votes are added, so little known games don't artificially climb to the top. Games with 5.5 rating are probably games that are not well known.\n- Most of games are `owned` by relatively few people, while a few of them have a big share.\n\nLet's first examine the time variables:","3e126e28":"# Conclusion\n\nThis ends our study of logistic regression using board games. If you have found the kernel interesting, please upvote! Any feedback is welcome.\n\nI would also like to thank [this other kernel](https:\/\/www.kaggle.com\/devisangeetha\/insights-geek-board-game) for its wonderful work with this same dataset.","2c915991":"We can clearly see the 'capping' effect the Bayesian average has. The games with more votes are in the 45\u00ba line (the `avg_rating` is reflected in `geek_rating`), while games with less votes are below the line. Apparently, the score added by BGG is a little bit more than 5.5.\n\nLet's now make sure about how the ranking is calculated:","03ff4e7d":"Most of the games are designed by professionals.\n\n# Target variable\n\nAs I mentioned earlier, this notebook intends to support [this post](https:\/\/anarthal.github.io\/kernel\/posts\/logistic-regression\/) about binary classification using logistic regression. Our goal variable will be `top`, which we will define as whether a game is between the top 1000 or not (I have chosen this number to not produce an excessively imbalanced dataset).","724f1075":"As expected, `owned` is positively correlated with the rating. `weight` is also important (geeks like complex games). Games with a `popular_designer` have also better ranking. I will just use two features for the sake of simplicity: `owned` and `weight`:"}}