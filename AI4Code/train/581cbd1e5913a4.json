{"cell_type":{"0af0e1a8":"code","0111fed9":"code","73af834a":"code","2cf9ff18":"code","fcddd883":"code","9e32b93c":"code","501fd32a":"code","c7befe85":"code","2c73d513":"code","8aa03157":"code","95d5ff30":"code","86887ee3":"code","15ba19da":"code","23bc19d0":"code","9425beb2":"code","a5771cd0":"code","4583aac5":"code","5dda126e":"code","f35fd51c":"code","399b19ef":"code","d585b00f":"code","29cc26d2":"code","3933ca59":"code","6f1501f7":"code","832002d7":"code","1451bd91":"code","7f50fff6":"code","329a6989":"code","1228c843":"code","2ce4763a":"code","d709dad6":"code","a0146158":"code","a5ece31f":"code","92eb0d67":"code","31ebca6c":"code","adf42669":"code","4d640399":"code","0de3eb27":"code","b64658b5":"code","047ad025":"code","5fb1a9ae":"code","ca6e5b6e":"code","45ba8a06":"code","4507c8ba":"code","5a892ac2":"code","61b9a7c3":"code","596b6c23":"code","e76a081a":"code","5a94e80c":"code","66283e2e":"code","f024237e":"code","cde58961":"code","b1c664ab":"code","e5c581d0":"code","cef37253":"code","8b607a8e":"code","518f7659":"code","5a03703f":"code","0660460f":"code","4629e752":"code","ea999f9b":"code","5de87b4a":"code","328b482e":"code","063d33b2":"code","327c450d":"code","dfa532d1":"code","44b1d141":"code","b05a5b49":"code","a759214d":"code","5a63e6ec":"code","9436744e":"code","6fa1edc1":"code","a627b177":"code","06b4e9c7":"code","7eca6913":"markdown","a7aa8e49":"markdown","beb9d2b1":"markdown","6842040f":"markdown","b037a51e":"markdown","686910c8":"markdown","6ed2f5f7":"markdown","1395f7b5":"markdown","54617fdf":"markdown","ba78d548":"markdown","4c0d48d1":"markdown","4cb6ae45":"markdown","d73c6b02":"markdown","2ec6b448":"markdown","7387d801":"markdown","aef323e6":"markdown","d75fce83":"markdown","d04a59c3":"markdown","aa178b99":"markdown","0a95a008":"markdown","cd5c2773":"markdown"},"source":{"0af0e1a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0111fed9":"import matplotlib.pyplot as plt","73af834a":"data_train=pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv',encoding=\"latin1\")\ndata_test=pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv',encoding='latin1')","2cf9ff18":"data_train.head()","fcddd883":"data_test.head()","9e32b93c":"print(data_train['Sentiment'].unique())","501fd32a":"for i in range(0,len(data_train)):\n    if(data_train['Sentiment'][i]=='Extremely Negative'):\n        data_train['Sentiment'][i]='Negative'\n    elif(data_train['Sentiment'][i]=='Extremely Positive'):\n        data_train['Sentiment'][i]='Positive'","c7befe85":"print(data_train['Sentiment'].unique())","2c73d513":"data_train['Sentiment'].isna().sum()","8aa03157":"from sklearn.preprocessing import LabelEncoder","95d5ff30":"le=LabelEncoder()","86887ee3":"data_train['Sentiment']=le.fit_transform(data_train['Sentiment'])","15ba19da":"##Converting into one hot encoding form is important when doing MultiClass Classification in Tensorflow\ny=pd.get_dummies(data_train['Sentiment'])","23bc19d0":"data_train.head()","9425beb2":"print(le.inverse_transform([0,1,2]))","a5771cd0":"n=0\nne=0\np=0","4583aac5":"for i in range(0,len(data_train)):\n    if(data_train['Sentiment'][i]==0):\n        n=n+1\n    elif(data_train['Sentiment'][i]==1):\n        ne=ne+1\n    elif(data_train['Sentiment'][i]==2):\n        p=p+1\n   ","5dda126e":"print('Total Labels belonging to Negative Category are ',n)\nprint('Total Labels belonging to Neutral Category are ',ne)\nprint('Total Labels belonging to Positive Category are ',p)\n","f35fd51c":"print('Total entries in the train dataset is ',len(data_train))","399b19ef":"##Segregating the tweet class from their original dataset\nX=data_train.iloc[:,4]","d585b00f":"from sklearn.model_selection import train_test_split\ntrain_text,validation_text,train_labels,validation_labels=train_test_split(X.to_numpy(),y.to_numpy(),test_size=0.2,random_state=42)","29cc26d2":"##printing validation labels to check if they are in one hot encoded form or not\nvalidation_labels","3933ca59":"type(train_text)","6f1501f7":"import re\nimport string","832002d7":"for i in range(0,len(train_text)):\n    temp=train_text[i]\n    temp=re.sub(\"@\\S+\", \" \", temp)\n    temp=re.sub(\"https*\\S+\", \" \", temp)\n    temp=re.sub(\"#\\S+\", \" \", temp)\n    temp=re.sub(\"\\'\\w+\", '', temp)\n    temp=re.sub('[%s]' % re.escape(string.punctuation), ' ', temp)\n    temp=re.sub(r'\\w*\\d+\\w*', '', temp)\n    temp=re.sub('\\s{2,}', \" \", temp)\n    train_text[i]=temp\n    ##print(temp)","1451bd91":"for i in range(0,len(validation_text)):\n    temp=validation_text[i]\n    temp=re.sub(\"@\\S+\", \" \", temp)\n    temp=re.sub(\"https*\\S+\", \" \", temp)\n    temp=re.sub(\"#\\S+\", \" \", temp)\n    temp=re.sub(\"\\'\\w+\", '', temp)\n    temp=re.sub('[%s]' % re.escape(string.punctuation), ' ', temp)\n    temp=re.sub(r'\\w*\\d+\\w*', '', temp)\n    temp=re.sub('\\s{2,}', \" \", temp)\n    validation_text[i]=temp\n    ##print(temp)","7f50fff6":"import random","329a6989":"for i in range(0,10):\n    print('New Tweet')\n    print('***********')\n    random_number=random.randint(0,len(train_text)-1)\n    print(train_text[random_number])","1228c843":"for i in range(0,10):\n    print('New Tweet')\n    print('***********')\n    random_number=random.randint(0,len(validation_text)-1)\n    print(validation_text[random_number])","2ce4763a":"type(train_text)","d709dad6":"import tensorflow as tf","a0146158":"from tensorflow.keras.preprocessing.text import Tokenizer","a5ece31f":"sum_length_of_tweet=0","92eb0d67":"##finding the average length of each tweet\nfor i in X:\n    temp=i\n    sum_length_of_tweet=sum_length_of_tweet+len(temp.split())\n    ","31ebca6c":"max_length=round(sum_length_of_tweet\/len(X))","adf42669":"print(max_length)","4d640399":"Tokenizer?","0de3eb27":"tokenizer=Tokenizer(num_words=10000,oov_token='<\/OOV>')","b64658b5":"len(tokenizer.word_index)","047ad025":"from tensorflow.keras.preprocessing.sequence import pad_sequences","5fb1a9ae":"tokenizer.fit_on_texts(train_text)","ca6e5b6e":"train_text_sequences=tokenizer.texts_to_sequences(train_text)","45ba8a06":"train_text_padded=pad_sequences(train_text_sequences,maxlen=max_length,padding='post')","4507c8ba":"validation_text_sequences=tokenizer.texts_to_sequences(validation_text)","5a892ac2":"validation_text_padded=pad_sequences(validation_text_sequences,maxlen=max_length,padding='post')","61b9a7c3":"for i in range(0,10):\n    index=random.randint(0,len(train_text)-1)\n    print('Original Sentence')\n    print('*******')\n    print(train_text[index])\n    print('Padded Sentence')\n    print('*********')\n    print(train_text_padded[index])","596b6c23":"for i in range(0,10):\n    index=random.randint(0,len(validation_text)-1)\n    print('Original Sentence')\n    print('*******')\n    print(validation_text[index])\n    print('Padded Sentence')\n    print('*********')\n    print(validation_text_padded[index])","e76a081a":"train_text_padded.shape[1]","5a94e80c":"model_1=tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(10000,128,input_length=train_text_padded.shape[1]),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(8,activation='relu'),\n    tf.keras.layers.Dense(3,activation='softmax')\n])","66283e2e":"model_1.summary()","f024237e":"model_1.compile(loss=tf.keras.losses.CategoricalCrossentropy(),optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])","cde58961":"history_1=model_1.fit(train_text_padded,train_labels,epochs=4,validation_data=(validation_text_padded,validation_labels))","b1c664ab":"model_1.evaluate(validation_text_padded,validation_labels)","e5c581d0":"model_2=tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(10000,128,input_length=train_text_padded.shape[1]),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(16,activation='relu'),\n    tf.keras.layers.Dense(8,activation='relu'),\n    tf.keras.layers.Dense(3,activation='softmax')\n])\nmodel_2.compile(loss=tf.keras.losses.CategoricalCrossentropy(),optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])\nhistory_2=model_2.fit(train_text_padded,train_labels,epochs=4,validation_data=(validation_text_padded,validation_labels))","cef37253":"model_3=tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(10000,128,input_length=max_length),\n    tf.keras.layers.LSTM(8),\n    tf.keras.layers.Dense(8,activation='relu'),\n    tf.keras.layers.Dense(3,activation='softmax')\n])\nmodel_3.compile(loss=tf.keras.losses.CategoricalCrossentropy(),optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])\nhistory_3=model_3.fit(train_text_padded,train_labels,epochs=3,validation_data=(validation_text_padded,validation_labels))","8b607a8e":"model_4=tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(10000,128,input_length=train_text_padded.shape[1]),\n    tf.keras.layers.LSTM(512),\n    tf.keras.layers.Dense(256,activation='relu'),\n    tf.keras.layers.Dense(128,activation='relu'),\n    tf.keras.layers.Dense(32,activation='relu'),\n    tf.keras.layers.Dense(3,activation='softmax')\n])\nmodel_4.compile(loss=tf.keras.losses.CategoricalCrossentropy(),optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])\nhistory_4=model_4.fit(train_text_padded,train_labels,epochs=4,validation_data=(validation_text_padded,validation_labels))","518f7659":"len(train_text_padded[0])","5a03703f":"model_4.summary()","0660460f":"import tensorflow_hub as hub\n","4629e752":"embed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\")","ea999f9b":"train_embed=embed(train_text)\ntest_embed=embed(validation_text)","5de87b4a":"len(train_embed[0])","328b482e":"model_6=tf.keras.models.Sequential([\n    tf.keras.layers.Dense(3,activation='softmax')\n])\nmodel_6.compile(loss=tf.keras.losses.CategoricalCrossentropy(),optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])\nmodel_6.fit(train_embed,train_labels,epochs=30,validation_data=(test_embed,validation_labels))","063d33b2":"model_new=tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(10000,128,input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(16,activation='relu'),\n    tf.keras.layers.Dense(3,activation='softmax')\n])\nmodel_new.compile(loss=tf.keras.losses.CategoricalCrossentropy(),optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])\nmodel_new.fit(train_embed,train_labels,epochs=5,validation_data=(test_embed,validation_labels))","327c450d":"data_test","dfa532d1":"data_test.iloc[:,4]","44b1d141":"for i in range(0,len(data_test)):\n    if(data_test['Sentiment'][i]=='Extremely Negative'):\n        data_test['Sentiment'][i]='Negative'\n    elif(data_test['Sentiment'][i]=='Extremely Positive'):\n        data_test['Sentiment'][i]='Positive'","b05a5b49":"X_test=data_test.iloc[:,4]","a759214d":"##preprocessing the test dataset\nfor i in range(0,len(X_test)):\n    temp=X_test[i]\n    temp=re.sub(\"@\\S+\", \" \", temp)\n    temp=re.sub(\"https*\\S+\", \" \", temp)\n    temp=re.sub(\"#\\S+\", \" \", temp)\n    temp=re.sub(\"\\'\\w+\", '', temp)\n    temp=re.sub('[%s]' % re.escape(string.punctuation), ' ', temp)\n    temp=re.sub(r'\\w*\\d+\\w*', '', temp)\n    temp=re.sub('\\s{2,}', \" \", temp)\n    X_test[i]=temp\n    ##print(temp)","5a63e6ec":"final_result=[]","9436744e":"for i in range(0,len(X_test)):\n    text=[X_test[i]]\n    text=tokenizer.texts_to_sequences(text)\n    text=pad_sequences(text,maxlen=max_length,padding='post')\n    result=model_4.predict_classes(text)\n    if(result==0):\n        final_result.append('Negative')\n    elif(result==1):\n        final_result.append('Neutral')\n    elif(result==2):\n        final_result.append('Positive')","6fa1edc1":"actual_result=data_test.iloc[:,5]","a627b177":"type(actual_result)","06b4e9c7":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(actual_result,final_result)","7eca6913":"# e) Trying Universal Sentence Encoder from TensorHub.","a7aa8e49":"# Using Tensorflow 2.x to create models \ud83d\ude0d","beb9d2b1":"# <span style=color:red>*Accuracy improves to ~85% on the validation dataset while using LSTM Based Architecture*<\/span>","6842040f":"# Plotting the confusion matrix to check for accuracy.","b037a51e":"# **Basic EDA \ud83e\udd14**","686910c8":"# *We have got more than 83% accuracy on the test dataset which is fabulous. Kudos for sticking around \ud83e\udd1d*","6ed2f5f7":"# <span style=color:red>*Baseline Model looks good. It gives us an accuracy of 81% on the validation dataset*<\/span>","1395f7b5":"# *Merging the extreme cases with their parent class*","54617fdf":"# b) Improving the baseline Model","ba78d548":"# *Tokenization and Padding the textual data*","4c0d48d1":"# *Let's print some examples to check if it functions the way we expect it to work or not*","4cb6ae45":"# a) Making Baseline Model","d73c6b02":"#  *Let's convert the Sentiment Labels into numericals for easier processing*","2ec6b448":"# *Print some random strings to check if the data has been preprocessed and cleaned well*","7387d801":"# *Cleaning the textual data- Removing all Hashtags, Mentions, Punctuations and Links*","aef323e6":"# *Printing the total data for each individual classes*","d75fce83":"# d) Improving the LSTM based model even further","d04a59c3":"# *We got our best result from Model 4 ie Improved LSTM architecture. We will be using the same model on our test dataset*","aa178b99":"# f) Final attempt: Bidirectional LSTM based Architecture","0a95a008":"# c) LSTM based Model","cd5c2773":"# <span style=color:red>*Again close to 85% on the validation dataset. Looks good*<\/span>"}}