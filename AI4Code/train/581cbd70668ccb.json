{"cell_type":{"76d49ada":"code","b183a2d1":"code","0cfa173d":"code","4c786acc":"code","69d61dee":"code","a08aca43":"code","1eb43eb5":"code","50e10b52":"code","b756012f":"code","6e68c9ac":"code","7c7f5b23":"code","50e129b3":"code","3cccdaff":"code","a1173746":"code","cf7a3388":"code","45438419":"code","8925ea32":"code","3c5cdb4d":"code","c77c0598":"code","44a5312f":"code","e69b2c90":"code","6b87a3d2":"code","52b05f2f":"code","2b63337e":"code","f87f6b38":"code","f90e7817":"code","00fda969":"code","790f7eaf":"code","09f0d54d":"code","49f6c5a5":"code","7f747835":"code","dcff2dd2":"code","51d2af54":"code","1c7da54a":"markdown","9ef813df":"markdown","f9699a78":"markdown","9a928a59":"markdown","000936f4":"markdown","993a9687":"markdown","b1de7663":"markdown","586c0b27":"markdown","77dd1600":"markdown","c9177c08":"markdown","5128dbfc":"markdown","a2c66409":"markdown","85f23324":"markdown","8baf1ac2":"markdown","7058b0ce":"markdown","ade084a6":"markdown","1b5c6939":"markdown","7f11dd98":"markdown","fed3d5f2":"markdown","76ebee1c":"markdown","c71a7c00":"markdown","8359523d":"markdown","32d8d06d":"markdown","eac35355":"markdown","579e3ca1":"markdown","446147a0":"markdown","c7c42c6c":"markdown","6762a7d9":"markdown","7024a19c":"markdown","864c8052":"markdown","1ccb2a07":"markdown","7c137412":"markdown","ae7d6240":"markdown","579f5da5":"markdown","322f7189":"markdown","d8750dd0":"markdown","947981cc":"markdown","aa115158":"markdown","3a7711fb":"markdown","6b6cf687":"markdown","44ef89f7":"markdown","ff2045fd":"markdown","bc02ac24":"markdown","a5a1e227":"markdown","09b19c9f":"markdown","25660fba":"markdown","99d223da":"markdown","43283b74":"markdown","e13283b8":"markdown","4b6e54e6":"markdown","36469a35":"markdown","6af3d362":"markdown","52dee032":"markdown"},"source":{"76d49ada":"\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# read data set\ncompany_data = pd.read_csv('..\/input\/company-reviews\/company_reviews.csv')\nprint(f\"The original dataset size was {len(company_data)}\")\ncompany_data.head()\n","b183a2d1":"\ncompany_data = company_data.drop(['name', 'description','locations','interview_difficulty', 'interview_duration',\n       'interview_count', 'interview_experience','website', 'roles', 'salary','headquarters'  ], axis=1)\n\n# find empty data\ncompany_data = company_data.fillna(np.nan)\ncompany_data = company_data.replace(to_replace = '{}', value=np.nan)\ncompany_data.isnull().sum()","0cfa173d":"\ncompany_data = company_data.dropna(subset = ['rating', 'reviews'])\nlen(company_data)\n","4c786acc":"\n# convert the column 'reviews' to number instead of text, including conversion of 'K' symbol\ncompany_data['reviews'] = company_data['reviews'].str.replace(' reviews', '')\ncompany_data['reviews'] = company_data['reviews'].str.replace(' review', '')\ncompany_data['reviews'] = company_data['reviews'].replace({'K': '*1e3'}, regex=True).map(pd.eval).astype(int)\n\n# drop companies with less then 20 reviewers\ncompany_data = company_data.drop(company_data[company_data.reviews < 20].index)\nlen(company_data)","69d61dee":"\n# convert the number of voting about CEO approval to integer, replace nan with 0\ncompany_data['ceo_count'] = company_data['ceo_count'].str.replace('CEO Approval is based on ', '')\ncompany_data['ceo_count'] = company_data['ceo_count'].str.replace(' ratings', '')\ncompany_data['ceo_count'] = company_data['ceo_count'].replace(np.nan, '0')\ncompany_data['ceo_count'] = company_data['ceo_count'].str.replace(',', '')\ncompany_data['ceo_count'] = pd.to_numeric(company_data['ceo_count'])\n\n# convert the CEO approval percentage to int, replace nan with mean\ncompany_data['ceo_approval'] = company_data['ceo_approval'].str.replace('%', '')\ncompany_data['ceo_approval'] = pd.to_numeric(company_data['ceo_approval'])\n\n# replace ceo approval with nan with the weighted mean\nmean = (company_data['ceo_count'] * company_data['ceo_approval']).sum() \/ company_data['ceo_count'].sum()\ncompany_data['ceo_approval'] = company_data['ceo_approval'].replace(np.nan, int(mean))\n","a08aca43":"\n# read the ratings data dictionary, and convert it to seperate columns for \n# each statement. drop the original column\nratings = company_data.ratings.tolist()\nrating_dict = []\nfor i in range(len(ratings)):\n    rating_dict.append(eval(ratings[i]))\n    \nkeys = ['Compensation\/Benefits', 'Job Security\/Advancement', 'Management', 'Culture', 'Work\/Life Balance']\n\nfor key in keys:\n    company_data[key] = [x.get(key, np.nan) for x in rating_dict]\n    company_data[key] = pd.to_numeric(company_data[key], errors='coerce')\n \ncompany_data = company_data.dropna(subset = ['Management', 'Compensation\/Benefits','Job Security\/Advancement','Culture','Work\/Life Balance'])\ncompany_data = company_data.drop(['ratings'], axis=1)\n","1eb43eb5":"company_data = company_data.dropna(subset = ['employees', 'revenue'])\nlen(company_data)","50e10b52":"pd.unique(company_data['employees'])\nscale_mapper = {\"1\":1, \"2 to 10\":2, \"11 to 50\":3, \"51 to 200\":4,\"201 to 500\":5,\"501 to 1,000\":6,\"1,001 to 5,000\":7, \"5,001 to 10,000\":8, \"10,000+\":9}\ncompany_data['employees']=company_data['employees'].replace(scale_mapper)\n# replace na in industry with \"Unknown\"\ncompany_data['industry'] = company_data['industry'].fillna(value='Unknown')\n\n","b756012f":"scale_mapper = {\"less than $1M (USD)\":1, \"$1M to $5M (USD)\":2, \"$5M to $25M (USD)\":3, \"$25M to $100M (USD)\":4,\"$100M to $500M (USD)\":5,\"$500M to $1B (USD)\":6,\"$1B to $5B (USD)\":7, \"$5B to $10B (USD)\":8, \"more than $10B (USD)\":9}\n\ncompany_data['revenue']=company_data['revenue'].replace(scale_mapper)","6e68c9ac":"plt.figure()\nsns.heatmap(company_data[['rating', 'ceo_approval', 'employees', 'revenue', 'Management', 'Compensation\/Benefits','Job Security\/Advancement','Culture','Work\/Life Balance' ]].corr(), xticklabels=True, yticklabels=True, vmin=-1.0, vmax=1.0)\n","7c7f5b23":"plt.figure()\n\ng = sns.catplot(x=\"employees\",y=\"rating\",data=company_data,kind=\"bar\", \npalette = \"muted\").set(title='work satisfaction vs company size')\ng.despine(left=True)\ng = g.set_ylabels(\"company reviews rate\")\nplt.show()\n","50e129b3":"plt.figure()\n\ng = sns.catplot(x=\"revenue\",y=\"rating\",data=company_data,kind=\"bar\", \npalette = \"muted\").set(title='work satisfaction vs company revenue')\ng.despine(left=True)\ng = g.set_ylabels(\"company reviews rate\")","3cccdaff":"plt.figure()\nsns.heatmap(company_data[['rating', 'Management', 'Compensation\/Benefits','Job Security\/Advancement','Culture','Work\/Life Balance' ]].corr(), xticklabels=True, yticklabels=True, vmin=0.5, vmax=1.0)\n","a1173746":"\nrating_by_industry = company_data.groupby('industry').mean().rating.sort_values(ascending=False)\ncount_by_industry = company_data.groupby('industry').count().rating.sort_values(ascending=False)\n# filter only industries that have more then 50 rows in the dataset\nindustries = count_by_industry[count_by_industry<50].index.tolist()\nrating_by_industry = rating_by_industry.drop(labels=industries)\n\nplt.figure()\n\ng = plt.bar(rating_by_industry.index[0:5], rating_by_industry.tolist()[0:5])\nlow = 3.5\nhigh = 4.2\nplt.ylim(low, high)\nplt.xticks(rotation=90)\nplt.title(\"Best industries for work satisfaction\")\nplt.show()\n","cf7a3388":"plt.figure()\n\ng = plt.bar(rating_by_industry.index[-5:], rating_by_industry.tolist()[-5:])\nplt.xticks(rotation=90)\nlow = 3.0\nhigh = 3.6\nplt.ylim(low, high)\nplt.title(\"Worse industries for work satisfaction\")\nplt.show()\n","45438419":"\nplt.figure()\nplt.scatter(company_data.ceo_approval, company_data.rating)\nplt.title(\"company rating vs. CEO approval\")\nplt.show()\n","8925ea32":"plt.figure()\nplt.scatter(company_data['Compensation\/Benefits'], company_data.rating)\nplt.title(\"company rating vs. compensation\")\nplt.show()\n","3c5cdb4d":"plt.figure()\nsns.kdeplot(company_data['rating'], legend=True)\nsns.kdeplot(company_data['Compensation\/Benefits'], legend=True)\nsns.kdeplot(company_data['Job Security\/Advancement'])\nsns.kdeplot(company_data.Management)\nsns.kdeplot(company_data.Culture)\nsns.kdeplot(company_data['Work\/Life Balance'])\nplt.legend(labels = ['Rating', 'Management', 'Compensation\/Benefits','Job Security\/Advancement','Culture','Work\/Life Balance' ])\nplt.show()","c77c0598":"company_data_with_happiness = company_data.copy()\n\ncompany_data_with_happiness['happiness'] = company_data_with_happiness['happiness'].fillna(value = \"{}\")\nhappiness = company_data_with_happiness.happiness.tolist()\nhappiness_dict = []\nfull_dict = {}\nfor i in range(len(happiness)):\n    new_entry = eval(happiness[i])\n    key_list = list(new_entry.keys())\n    new_key_list = [\"happiness \" + s for s in key_list]\n    for count, key in enumerate(key_list):\n        new_entry[new_key_list[count]] = new_entry.pop(key)\n    \n    full_dict.update(new_entry)\n    happiness_dict.append(new_entry)\n    \nfor key in full_dict.keys():\n    company_data_with_happiness[key] = [x.get(key, np.nan) for x in happiness_dict]\n    company_data_with_happiness[key] = pd.to_numeric(company_data_with_happiness[key], errors='coerce')\n    \ncompany_data_with_happiness = company_data_with_happiness.drop(['happiness'], axis=1)\nfor key in full_dict.keys():\n    company_data_with_happiness = company_data_with_happiness.dropna(subset=[key])\ncompany_data_with_happiness = company_data_with_happiness.drop(['reviews', 'ceo_count', 'industry'], axis=1)\n\nplt.figure()\nkey_list = [key for key in full_dict.keys()]\nkey_list.insert(0, 'rating')\nsns.heatmap(company_data_with_happiness[key_list].corr(), xticklabels=True, yticklabels=True, vmin=0.5, vmax=1.0)\n","44a5312f":"# drop unnecessary data\ncompany_data = company_data.drop(['happiness'], axis=1)\ncompany_data = company_data.drop(['reviews', 'ceo_count'], axis=1)\n\n# convert the industry data to one hot encoding\ncompany_data = pd.get_dummies(data=company_data, columns=['industry'], drop_first=True)\n\nreduced_company_data = company_data[['rating', 'ceo_approval', 'Compensation\/Benefits','Job Security\/Advancement', 'Management', 'Culture', 'Work\/Life Balance', \n                                     'industry_Auto', 'industry_Construction', 'industry_Financial Services']]\nlimited_company_data_1 = company_data[['rating',  'Culture', 'Work\/Life Balance']]\nlimited_company_data_2 = company_data[['rating',  'Compensation\/Benefits', 'Job Security\/Advancement']]\n\n","e69b2c90":"X = company_data.values[:,1:]\ny = company_data.values[:,0]\nX = MinMaxScaler().fit(X).transform(X)\n\nX_reduced = reduced_company_data.values[:,1:]\ny_reduced = reduced_company_data.values[:,0]   \nX_reduced = MinMaxScaler().fit(X_reduced).transform(X_reduced)\n\nX_limited_1 = limited_company_data_1.values[:,1:]\ny_limited_1 = limited_company_data_1.values[:,0]    \nX_limited_1 = MinMaxScaler().fit(X_limited_1).transform(X_limited_1)\n\nX_limited_2 = limited_company_data_2.values[:,1:]\ny_limited_2 = limited_company_data_2.values[:,0]    \nX_limited_2 = MinMaxScaler().fit(X_limited_2).transform(X_limited_2)\n\nX_happiness = company_data_with_happiness.values[:,1:]\ny_happiness = company_data_with_happiness.values[:,0]\nX_happiness = MinMaxScaler().fit(X_happiness).transform(X_happiness) ","6b87a3d2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_reduced_train, X_reduced_test, y_reduced_train, y_reduced_test = train_test_split(X_reduced, y_reduced, test_size=0.3, random_state=42)\nX_limited_1_train, X_limited_1_test, y_limited_1_train, y_limited_1_test = train_test_split(X_limited_1, y_limited_1, test_size=0.3, random_state=42)\nX_limited_2_train, X_limited_2_test, y_limited_2_train, y_limited_2_test = train_test_split(X_limited_2, y_limited_2, test_size=0.3, random_state=42)\nX_happy_train, X_happy_test, y_happy_train, y_happy_test = train_test_split(X_happiness, y_happiness, test_size=0.3, random_state=42)\n\n\n\n\n","52b05f2f":"def compute_model(model, X_train, X_test, y_train, y_test, model_text):\n    y_pred_train = model.fit(X_train, y_train).predict(X_train)\n    mse_train = mean_squared_error(y_train, y_pred_train)\n    \n    \n    y_pred_test = model.fit(X_train, y_train).predict(X_test)\n    mse_test = mean_squared_error(y_test, y_pred_test)\n    \n    return mse_test","2b63337e":"def plot_linear_regression(reg, data_type):\n    if data_type == \"Full data set\":\n        plt.bar(company_data.columns[1:23], reg.coef_[:22])\n        plt.xticks(rotation=90)\n        ymin, ymax = -0.5, 1.2\n        plt.ylim(ymin, ymax)\n        plt.title(\"Linear regression - effect of different parameters\")\n        plt.show()\n        \n        plt.bar(company_data.columns[23:], reg.coef_[22:])\n        plt.xticks(rotation=90)\n        ymin, ymax = -0.5, 1.2\n        plt.ylim(ymin, ymax)\n        plt.title(\"Linear regression - effect of different parameters\")\n        plt.show()\n    elif data_type == \"Reduced data set\":\n        plt.bar(reduced_company_data.columns[1:], reg.coef_[:])\n        plt.xticks(rotation=90)\n        ymin, ymax = -0.5, 1.2\n        plt.ylim(ymin, ymax)\n        plt.title(\"Linear regression - effect of different parameters\")\n        plt.show()\n    elif data_type == \"Full happiness data set\":\n        plt.bar(company_data_with_happiness.columns[1:], reg.coef_[:])\n        plt.xticks(rotation=90)\n        plt.title(\"Linear regression - effect of different parameters (happiness dataset)\")\n        plt.show()\n        \n    else:\n        pass","f87f6b38":"def compute_regression_models(X_train, X_test, y_train, y_test, data_type):\n    \n    mse_list = []\n    \n    # Linear regression\n    reg = LinearRegression().fit(X_train, y_train)\n    y_pred_train = reg.predict(X_train)\n    \n    mse_train = mean_squared_error(y_train, y_pred_train)\n    \n    y_pred_test = reg.predict(X_test)\n    mse_test = mean_squared_error(y_test, y_pred_test)\n    mse_list.append({\"linear regression\": mse_test})\n    \n    plot_linear_regression(reg, data_type)\n    \n    # KNN \n\n    test_results = []\n    \n    MAX_K = 12\n    for i in range(2, MAX_K):\n    \n        neigh = KNeighborsRegressor(n_neighbors=i)\n        y_pred_train = neigh.fit(X_train, y_train).predict(X_train)\n        \n        mse_train = mean_squared_error(y_train, y_pred_train)\n        \n        \n        y_pred_test = neigh.fit(X_train, y_train).predict(X_test)\n        \n        mse_test = mean_squared_error(y_test, y_pred_test)\n        test_results.append(mse_test)\n        \n    plt.figure()\n    plt.scatter(range(2,MAX_K),test_results)\n    plt.ylabel('MSE')\n    plt.xlabel('K')\n    plt.title(\"MSE for KNN with various K\")\n    plt.show()\n    \n    mse_list.append({\"KNN\": test_results[4]})\n    \n    # SVR\n    \n    reg = SVR()\n    mse = compute_model(reg, X_train, X_test, y_train, y_test,\"SVM regression with gaussian kernel\")\n    mse_list.append({\"SVR Gaussian kernel\": mse})\n\n     \n    reg = SVR(kernel='linear')\n    mse = compute_model(reg, X_train, X_test, y_train, y_test,\"SVM regression with linear kernel\")\n    mse_list.append({\"SVR linear kernel\": mse})\n\n    # decision tree regressor\n    \n    reg = DecisionTreeRegressor(max_depth=2)\n    mse = compute_model(reg, X_train, X_test, y_train, y_test,\"Decision tree regressor with max depth 2\")\n    mse_list.append({\"Decision tree depth 2\": mse})\n\n    \n    reg = DecisionTreeRegressor(max_depth=None)\n    mse = compute_model(reg, X_train, X_test, y_train, y_test,\"Decision tree regressor with unlimited depth\")\n    mse_list.append({\"Decision tree unlimited\": mse})\n\n    \n    # random forest\n    reg = RandomForestRegressor(max_depth=2)\n    mse = compute_model(reg, X_train, X_test, y_train, y_test,\"Random Forest with depth=2\")\n    mse_list.append({\"Random Forest depth 2\": mse})\n\n\n    # gradient boosting\n    reg = GradientBoostingRegressor()\n    mse = compute_model(reg, X_train, X_test, y_train, y_test,\"Gradient Boosting\")\n    mse_list.append({\"gradient Boosting\": mse})\n    \n    return mse_list","f90e7817":"# plot graph of MSE of different regression models\ndef show_mse(mse_list, title):\n    values = [list(x.values()) for x in mse_list]\n    values = [item for sublist in values for item in sublist]\n    keys = [list(x.keys()) for x in mse_list]\n    keys = [item for sublist in keys for item in sublist]\n    \n    \n    plt.bar(keys, values)\n    plt.ylabel('MSE')\n    plt.xticks(rotation=90)\n    plt.title(title)\n    plt.show()","00fda969":"full_mse_list = compute_regression_models(X_train, X_test, y_train, y_test, \"Full data set\")\n","790f7eaf":"show_mse(full_mse_list, \"regression result - full data set (57 features)\")\n","09f0d54d":"reduced_mse_list = compute_regression_models(X_reduced_train, X_reduced_test, y_reduced_train, y_reduced_test, \"Reduced data set\")\nshow_mse(reduced_mse_list, \"regression result - reduced data set (9 features)\")\n","49f6c5a5":"limited_1_mse_list = compute_regression_models(X_limited_1_train, X_limited_1_test, y_limited_1_train, y_limited_1_test, \"Limited data set\")\nshow_mse(limited_1_mse_list, \"regression result - limited data set (Culture and Work\/Life balance)\")\n","7f747835":"limited_2_mse_list = compute_regression_models(X_limited_2_train, X_limited_2_test, y_limited_2_train, y_limited_2_test, \"Limited data set\")\nshow_mse(limited_2_mse_list, \"regression result - limited data set (Compensation \/ Benefits and Advancement \/ Job security)\")","dcff2dd2":"happy_mse_list = compute_regression_models(X_happy_train, X_happy_test, y_happy_train, y_happy_test, \"Happiness data set\")\nshow_mse(happy_mse_list, \"regression result - Happiness data set\")","51d2af54":"# compare the various datasets\ny = [full_mse_list[2].get('SVR Gaussian kernel'), reduced_mse_list[2].get('SVR Gaussian kernel'), limited_1_mse_list[2].get('SVR Gaussian kernel'), limited_2_mse_list[2].get('SVR Gaussian kernel'), happy_mse_list[2].get('SVR Gaussian kernel')] \nx = [\"Full Dataset (57 features)\", \"Reduced dataset (8 features)\",  \"minimal dataset (2 features, culture and work-life balance)\",\"minimal dataset (2 features, compensation and advancement)\", \"Happiness dataset (21 features)\" ]\n\nplt.figure()\nplt.bar(x, y)\nplt.ylabel('MSE')\nplt.xticks(rotation=90)\nplt.title(\"datasets comparison\")\nplt.show()\n","1c7da54a":"Again, we have correlation with large variance.\nLets check the distribution of rating versus other factors","9ef813df":"<a id=\"Feature_Processing\"><\/a>\n## Feature preprocessing and missing values\nWe can see that the data need a lot of preprocessing. Let's start.\nFirst, we will get rid of irrelevant data for this analysis:\n* Company name\n* Description \n* Locations\n* interviews related data\n* website\n* roles\n* salary\n* headquarters","f9699a78":"Let's check the performance of all the regressors on the full data set and compare","9a928a59":"To our surprise, we see that the compensation \/ benefits factor is the least correlated to the overall company rating.\n\n**Is it really a surprise? No. The fact that compensation is not a major factor for employee satisfaction or even attrition is well known from a lot of studies.**","000936f4":"Let's see the correlation between features","993a9687":"The company reviews database arise the question if we can predict the rating of a company by other factors.\nIs compensation the major factor for a high rating company review? \nMaybe other factors are more important than compensation and benefits?\nThis notebook will try to answer the question","b1de7663":"The data about the company size (the number of employees) are strings giving the range of number of employees. we will convert it\nto ordinal data in accordance with the following:\n* 1 - 1 employee\n* 2 - 2 to 10 employees\n* 3 - 11 to 50 employees\n* 4 - 50 to 200 employees\n* 5 - 201 to 500 employees\n* 6 - 501 to 1000 employees\n* 7 - 1001 to 5000 employees\n* 8 - 5001 to 10000 employees\n* 9 - 10000+ employees\n\n\"na\" industry we will replace with \"unknown\"","586c0b27":"Let's check with the \"happiness\" dataset. This time we have additional 13 new features to the reduced dataset","77dd1600":"We can see that the best regressors are the same, and we didn't affect the performance of the models by reducing the feature set from 57 to 9","c9177c08":"The next fields for pre-processing are the 'ceo_approval' and 'ceo_count' (the number of votes about the ceo of a company). \nwe will convert the 'ceo_count' to integer, and replace empty fields of 'ceo_approval' with the weighted mean of the 'ceo_approval column","5128dbfc":"<a id=\"limited_1\"><\/a>\n### Limited data set - Culture and Work\/Life Balance","a2c66409":"\n<a id=\"Pre-Processing\"><\/a>\n# Pre-Processing\n<a id=\"Data_Loading\"><\/a>\n## Data Loading","85f23324":"<a id=\"section-one\"><\/a>\n# Introduction","8baf1ac2":"Since our dataset is large, we can ignore more lines with missing data.\nlet's drop rows without data about the company size (number of employees) and the company revenue\nOut dataset includes 7823 companies after the elimination","7058b0ce":"Let's check now what is the influence of the industry section on the company rating. We will show the 5 industry section with the highest company rating and the 5 industries with the lowest company rating","ade084a6":"<a id=\"Split\"><\/a>\n## Split to train and test data","1b5c6939":"<a id=\"Modeling\"><\/a>\n# Modeling\nWe want to predict the review rating of companies using several features which the dataset contains, thus we should use regression models.\nThe following models will be evaluated:\n* Linear Regression\n* KNN regressor (we will find the optimal K)\n* SVM regressor (with linear kernel and with Gaussian kernel)\n* Decision Trees (with max_depth = 2 and with unlimited max_depth)\n* Random Forest Regressor\n* Gradient Boosting\n","7f11dd98":"It seems that there is a very small correlation between the company rating and company size or company revenue, while there is positive corelation to the other features.\nLet's see this in more details\n","fed3d5f2":"We can see that company rating is slightly larger for small and large companies\nLet's look at the rating vs company revenue. We might think that the larger the revenue the better the rating..","76ebee1c":"In addition to the overall rating of the company, we have detailed rating about 5 aspects:\n* Compensation \/ Benefits\n* Culture\n* Job security \/ Advancement\n* Management\n* Work\/life balance\n\nAll this data is embedded within one column, as sort of dictionary. \nWe want to have separate column for each one of these ratings","c71a7c00":"We can see that the best regressors are the same. The performance is worse than the bigger data sets, but still the regression prediction is very good (0.015 MSE)","8359523d":"In the first 2 graphs, we can see the weights of the features for the linear regression model. we can see that the values in the second graph are neglectable, and in the first graph, there are few significant features - exactly the features that we decided to adopt for the reduced data set.\nIn the third graph, we can see the KNN results (Mean Square Error) for various K's (from 2 to 12). we can see that the best result is for K=5, and this value will be taken for further comparisons. ","32d8d06d":"<a id=\"limited_2\"><\/a>\n### Limited data set - Compensation and Job security","eac35355":"<a id=\"Regression\"><\/a>\n## Regression models functions","579e3ca1":"The revenue of the companies are strings giving the range of the revenue. We will convert it to ordinal data in accordance with the following:\n* 1 - 'less than $1M (USD)'\n\n* 2 - $1M to $5M (USD)\n* 3 - $5M to $25M (USD)\n* 4 - $25M to $100M (USD)\n* 5 - $100M to $500M (USD)\n* 6 - $500M to $1B (USD)\n* 7 - $1B to $5B (USD)\n* 8 - $5B to $10B (USD)\n* 9 - more than $10B (USD)\n","446147a0":"The full database contains 17,050 records as we saw earlier, thus we can delete rows that don't contain rating (this is the parameter that we want to predict) and reviews (i.e. the number of reviews for each company)\nwe can see that the remaining data contains 15,494 records ","c7c42c6c":"Quite interesting - almost all the happiness parameters have a high correlation, except the compensation.\nWell, it's time for model prediction!","6762a7d9":"Let's try with the limited data set - only 2 features: Culture and Work\/Life balance","7024a19c":"Let's compare the results of all the datasets","864c8052":"We can see that there is a slight increase in company rating as the revenue grows","1ccb2a07":"<a id=\"happy\"><\/a>\n### \"Happiness\" data set","7c137412":"<a id=\"compare\"><\/a>\n## Comparing the Datasets","ae7d6240":"As expected, beside some outliers we see corelation between the two, even if the variance is quite large.\nAnd what about compensation and benefits?","579f5da5":"<a id=\"Exploratory_Data\"><\/a>\n# Exploratory data analysis","322f7189":"Let's focus on the rating factors and see the correlation to the overall company rating","d8750dd0":"![](https:\/\/live.staticflickr.com\/3491\/3251369440_566f1cfac1_b.jpg)","947981cc":"We can see that the best regressors are the same, but the prediction results are much better","aa115158":"<a id=\"Reduced\"><\/a>\n### Reduced data set (21 features)","3a7711fb":"Let's try to run the other databases:","6b6cf687":"First we have to do some data exploration:","44ef89f7":"<a id=\"Conclusions\"><\/a>\n# Conclusions\nWe can predict with excellent results companies rating in accordance with various factors using different methods for linear regression.\nWe can reduce the number of features from 57 to 8 without reducing the performance.\nWe have shown that compensation and job security have less effect than company culture, which is in line with many HR articles and researches.\nThe happiness data needs further investigation and separation of the various features. ","ff2045fd":"Let's see what is the relation between the company rating and CEO approval ","bc02ac24":"<a id=\"Full\"><\/a>\n### Full Dataset (57 features)","a5a1e227":"<a id=\"Data_Scaling\"><\/a>\n## Data Scaling\nIn order to have all the data on the same scale, we will do data scaling using min-max scale","09b19c9f":"<a id=\"Run\"><\/a>\n## Run regression models","25660fba":"Let's look at the \"reviews\" column, convert it to int, and ignore companies with less than 20 reviews.\nwe still have more than 10,000 companies in the database","99d223da":"# What are the major factors for high rating company review?","43283b74":"We can see that the best predictors are:\n* Linear regression\n* SVM Regressor (with linear or gaussian kernel)\n* Gradient boosting","e13283b8":"Let's try with another limited dataset with 2 features - but this time we will use the compensation \/ benefits and advancement \/ job security features","4b6e54e6":"<a id=\"Datasets\"><\/a>\n## Define the datasets to explore\nWe will use 5 variations of datasets for the company review modeling\nThe first 4 contain about 7000 instances, the last one contains about 3200 instances\n1. \"Full\" dataset - contains the following features (57 features):\n* CEO Approval (% approval of the CEO)\n* employees (size of the company)\n* Revenue\n* Compensation\/Benefits rating\n* Job Security\/Advancement rating\n* Management rating\n* Culture rating\n* Work\/Life Balance rating\n* Industry (one hot encoding)\n2. Reduced dataset - with the features that we analyzed which are the most important with very limited industry data (9 features)\n3. Small dataset 1 - with only 2 features which seem to be the most relevant:\n* culture rating\n* Work-life balance rating\n4. Small dataset 2 - with 2 other features that sometimes are regarded as the most important\n* Compensation\/Benefits rating\n* Job Security\/Advancement rating\n5. \"Happiness\" Dataset - contains the following:\n* CEO Approval (% approval of the CEO)\n* employees (size of the company)\n* Revenue\n* Compensation\/Benefits rating\n* Job Security\/Advancement rating\n* Management rating\n* Culture rating\n* Work\/Life Balance rating\n* happiness Work Happiness Score\n* happiness Appreciation\n* happiness Purpose\n* happiness learning\n* happiness Support\n* happiness Achievement\n* happiness Flexibility\n* happiness Trust\n* happiness Energy\n* happiness Inclusion\n* happiness Belonging\n* happiness Management\n* happiness Compensation\n\n\n","36469a35":"# Table of Contents\n* [Introduction](#section-one)\n* [Pre-Processing](#Pre-Processing)\n    * [Data Loading](#Data_Loading)\n    *[Feature preprocessing and missing values](#Feature_Processing)\n*[Exploratory data analysis](#Exploratory_Data)\n*[Modeling](#Modeling)\n    * [Define the datasets to explore](#Datasets)\n    * [ Data Scaling](#Data_Scaling)\n    * [Split to train and test data](#Split)\n    * [ Regression models functions](#Regression)\n    * [Run regression models](#Run)\n        * [Full Dataset (57 features)](#Full)\n        * [Reduced data set (21 features)](#Reduced)\n        * [Limited data set - Culture and Work\/Life Balance](#limited_1)\n        * [Limited data set - Compensation and Job security](#limited_2)\n        * [\"Happiness\" data set](#happy)\n    * [Comparing the Datasets](#compare)\n* [Conclusions](#Conclusions)","6af3d362":"There is additional column that we want to explore: the happiness column. We should pay attention that only about 30% of the companies have the happiness rating, but still we have about 3200 with this data and it is a significant dataset.\nFirst, we should convert the data from dictionary like to separate columns. The happiness has 13 different statements about happiness like belonging, energy, learning, purpose etc.\nLet's plot the correlation map","52dee032":"This time the SVM regressor with gaussian kernel is the best, but the performance is worse."}}