{"cell_type":{"4e5fd628":"code","8851e2fc":"code","1b143b4f":"code","2044c77e":"code","1d53aed4":"code","b4f6e6c2":"code","1e73ca6c":"code","28fff99f":"code","7d629e9b":"code","860a50a3":"code","990c849c":"code","be2c3596":"code","29639011":"code","8e9b5b98":"code","3fd4aab9":"code","978bce8a":"code","2d99a006":"code","5b395505":"code","14fd7652":"markdown","58dc665d":"markdown","3639f007":"markdown","80f67ab9":"markdown","03a35fed":"markdown","b9b9ec19":"markdown","e66b3225":"markdown","d04b63f9":"markdown","a1a9a2e6":"markdown","8e0f931d":"markdown","42b59063":"markdown","3289b2cf":"markdown","58e1c5e7":"markdown","faf4d555":"markdown","d93bbda2":"markdown","8d779b13":"markdown","af881738":"markdown","e170d8d1":"markdown","349cbe5e":"markdown","8dedaf96":"markdown","54b4eade":"markdown","52383ded":"markdown","1a78a24a":"markdown","9d7e5a75":"markdown","138e0d69":"markdown","dd5fe4e9":"markdown","c54cd8b2":"markdown","8eff9e8d":"markdown","702ffd7d":"markdown"},"source":{"4e5fd628":"import tensorflow as tf","8851e2fc":"graph1 = tf.Graph()","1b143b4f":"with graph1.as_default():\n    a = tf.constant([2], name = 'constant_a')\n    b = tf.constant([3], name = 'constant_b')","2044c77e":"print(a)","1d53aed4":"# Printing the value of a\n# sess = tf.Session(graph = graph1) # Decomission\nsess = tf.compat.v1.Session(graph = graph1)\nresult = sess.run(a)\nprint(result)\nsess.close()","b4f6e6c2":"with graph1.as_default():\n    c = tf.add(a, b)\n    #c = a + b is also a way to define the sum of the terms","1e73ca6c":"sess = tf.compat.v1.Session(graph = graph1)","28fff99f":"result = sess.run(c)\nprint(result)","7d629e9b":"sess.close()","860a50a3":"with tf.compat.v1.Session(graph = graph1) as sess:\n    result = sess.run(c)\n    print(result)","990c849c":"graph2 = tf.Graph()\n\nwith graph2.as_default():\n    Scalar = tf.constant(2)\n    Vector = tf.constant([5,6,2])\n    Matrix = tf.constant([[1,2,3],[2,3,4],[3,4,5]])\n    Tensor = tf.constant( [ [[1,2,3],[2,3,4],[3,4,5]] , [[4,5,6],[5,6,7],[6,7,8]] , [[7,8,9],[8,9,10],[9,10,11]] ] )\n    \nwith tf.compat.v1.Session(graph = graph2) as sess:\n    result = sess.run(Scalar)\n    print (\"Scalar (1 entry):\\n %s \\n\" % result)\n    result = sess.run(Vector)\n    print (\"Vector (3 entries) :\\n %s \\n\" % result)\n    result = sess.run(Matrix)\n    print (\"Matrix (3x3 entries):\\n %s \\n\" % result)\n    result = sess.run(Tensor)\n    print (\"Tensor (3x3x3 entries) :\\n %s \\n\" % result)","be2c3596":"Scalar.shape","29639011":"Tensor.shape","8e9b5b98":"v = tf.Variable(0)","3fd4aab9":"# update = tf.assign(v, v+1) # Depricated in TF 2.0\nupdate = tf.compat.v1.assign(v, v+1)","978bce8a":"# init_op = tf.global_variables_initializer() # Depricated in TF 2.0\ninit_op = tf.compat.v1.global_variables_initializer()","2d99a006":"# with tf.compat.v1.Session(graph = graph1) as session:\n#     session.run(init_op)\n#     print(session.run(v))\n#     for _ in range(3):\n#         session.run(update)\n#         print(session.run(v))","5b395505":"# a = tf.placeholder(tf.float32)","14fd7652":"# What is a Tensor?\nWell as we have already noted, the data that's passed between the operations are ensors. In effect, a tensor is a multi-dimensional array. \n1. It can be zero dimensional, such as scalar values, \n2. 1-D as a line or vector,  \n3. 2-D such as a Matric and so on.\n\n","58dc665d":"# What is Tensorflow?\nTensorflow is an open source library developed by the Google.\n\nIt's an extermely versitile library, originally created for tasks that require heavy numerical computations.\n\nTensorflow application uses a structure knwon as a data flow graph, which is very useful to first build and then execute it.\n\nIt supports CPU's; GPU's and even distributed processing in a cluster. It is a very important features as you can train a Neural Network using CPU and multiple GPU's which makes the models very efficient on large-scale systems.\n\nTensorflow's structure is based on the execution of a data flow graph. A data flow graph has 2 basic units.\n1. Nodes --> This represents a mathematical operation,\n2. Edges --> which represents the multi-dimensional arrays known as Tensors.\n\nThis high-level abstraction reveals how the data flows between operations.\n\nAlso using the data flow graph, we can easily visualize different parts of the graph, which is not an option while using other python libraries such as Numpy; scikit.\n\nThe standard usage is to build a graph first and then execute it in a session.","3639f007":"# What are Dimensional arrays?\n\nThe `zero dimension` can be seen as a point, a single object or a single item.\n\nThe `first dimension` can be seen as a line, a one-dimensional array can be seen as numbers along this line, or as points along the line. One dimension can contain infinite zero dimension\/points elements.\n\nThe `second dimension` can be seen as a surface, a two-dimensional array can be seen as an infinite series of lines along an infinite line.\n\nThe `third dimension` can be seen as volume, a three-dimensional array can be seen as an infinite series of surfaces along an infinite line.\n\nThe `Fourth dimension` can be seen as the hyperspace or spacetime, a volume varying through time, or an infinite series of volumes along an infinite line. And so forth on...\n\n![image.png](attachment:image.png)\n","80f67ab9":"Now we call the TensorFlow functions that construct new tf.Operation and tf.Tensor objects and add them to the graph1. As mentioned, each tf.Operation is a node and each tf.Tensor is an edge in the graph.\n\nLets add 2 constants to our graph. For example, calling tf.constant([2], name = 'constant_a') adds a single tf.Operation to the default graph. This operation produces the value 2, and returns a tf.Tensor that represents the value of the constant.\nNotice: tf.constant([2], name=\"constant_a\") creates a new tf.Operation named \"constant_a\" and returns a tf.Tensor named \"constant_a:0\".","03a35fed":"## Building a Graph\nAs we said before, TensorFlow works as a graph computational model. Let's create our first graph which we named as graph1.","b9b9ec19":"## Importing TensorFlow\nTo use TensorFlow, we need to import the library. We imported it and optionally gave it the name \"tf\", so the modules can be accessed by tf.module-name:","e66b3225":"# Variables\nNow that we are more familiar with the structure of data, we will take a look at how TensorFlow handles variables. First of all, having tensors, \n\n## why do we need variables?\nTensorFlow variables are used to share and persistent some stats that are manipulated by our program. That is, when you define a variable, TensorFlow adds a tf.Operation to your graph. Then, this operation will store a writable tensor value that persists between tf.Session.run calls. So, you can update the value of a variable through each run, while you cannot update tensor (e.g a tensor created by tf.constant()) through multiple runs in a session.\n\n## How to define a variable?\nTo define variables we use the command tf.Variable(). To be able to use variables in a computation graph it is necessary to initialize them before running the graph in a session. This is done by running tf.global_variables_initializer().\n\nTo update the value of a variable, we simply run an assign operation that assigns a value to the variable:","d04b63f9":"As you can see, it just show the name, shape and type of the tensor in the graph. We will see it's value when we run it in a TensorFlow session.","a1a9a2e6":"In TensorFlow all data is passed between operations in a computation graph, and these are passed in the form of Tensors, hence the name of TensorFlow.","8e0f931d":"Then TensorFlow needs to initialize a session to run our code. Sessions are, in a way, a context for creating a graph inside TensorFlow. Let's define our session:","42b59063":"# How does TensorFlow work?\nTensorFlow defines computations as Graphs, and these are made with operations (also know as \u201cops\u201d). So, when we work with TensorFlow, it is the same as defining a series of operations in a Graph.\nTo execute these operations as computations, we must launch the Graph into a Session. The session translates and passes the operations represented into the graphs to the device you want to execute them on, be it a GPU or CPU. In fact, TensorFlow's capability to execute the code on different devices such as CPUs and GPUs is a consequence of it's specific structure.\n\nFor example, the image below represents a graph in TensorFlow. W, x and b are tensors over the edges of this graph. MatMul is an operation over the tensors W and x, after that Add is called and add the result of the previous operator with b. The resultant tensors of each operation cross the next one until the end where it's possible to get the wanted result.\n\n![image.png](attachment:image.png)","3289b2cf":"# Placeholder","58e1c5e7":"To avoid having to close sessions every time, we can define them in a with block, so after running the with block the session will close automatically:","faf4d555":"Lets look at the tensor a.","d93bbda2":"# What is Deep Learning?\nDL is a series of supervised, semi-supervised and unsupervised methods that try to solve some ML problems using Deep Neural Networks.\nA Deep Neural Network which often has more than 2 layers and uses specific mathematical modeling in each layer to process data.\n\nTo better understand DL, lets first take a look at different Deep Neural Networks and their applications namely\n1. Convolutional Network Network (CNN)\n2. Recurrent Neural Network (RNN)\n3. Restricted Boltzmann Machine (RBM)\n4. Deep Belief Networks (DBN)\n5. Autoencoders\n\n`CNN` is a DL approach that learns directly from samples in a way that is much more effective than traditional Neural Networks.\nCNN achieve this type of automatic feature selection and classification through multiple specific layers of sophicated mathematical operations.\nCNN used especially in machine vision projects, including image recoginition or classification.\nCNN are also used in object detection, or coloring black-white images and creating art images.\n\n`RNN` is a tyoe if DL approach that tries to solve the problem of modeling sequential data. Whenever the points in a datasets are dependent on the previous points, the data is said to be sequential. \nFor example, a Stock Market price is a sequential type of data because the price of any given stock in tomorrow's market, to a great extent, depends on its price today. As such, predcting the stock price tomorrow is something that RNNs can be used for.\nWe simply need to feed the network with the sequential data and thus, learns the patterns within data.\nRNN can also be used for sentiment analysis.\nRNN can also be used to predict the next word in a sentence. I'm sure we've all seen how our mobile phone suggests words.\nLanguage Translation service,is another example of how RNN can be used. We enter a sequence of words in English and it outputs a sequence of the words in selected language be it Spanish; French etc. This task is not based on a word-by-word transalation and appliying grammer rules. Instead, it is a probability model that has been trained on lots of data where the exact same text is translated into another language.\nSpeech-to-text is yet another useful and increasingly common application of RNN's.\nIn this case, the recognized voice is not only based on the word sound; RNNs also use the context around that sound to accurately recognize of the words being spoken into the device's microphone.\n","8d779b13":"# Activation functions\nActivation functions are a cornerstone of Machine Learning. In general, Activation Functions define how a processing unit will treat its input -- usually passing this input through it and generating an output through its result. To begin the process of having a more intuitive understanding, let's go through some of the most commonly used functions.\n\n1. The Step Functions\nThe Step function was the first one designed for Machine Learning algorithms. It consists of a simple threshold function that varies the Y value from 0 to 1. This function has been historically utilized for classification problems, like Logistic Regression with two classes.\n\nThe Step Function simply functions as a limiter. Every input that goes through this function will be applied to gets either assigned a value of 0 or 1. As such, it is easy to see how it can be handy in classification problems.\n\nThere are other variations of the Step Function such as the Rectangle Step and others, but those are seldom used.\n\n2. The Sigmoid Functions\nThe next in line for Machine Learning problems is the family of the ever-present Sigmoid functions. Sigmoid functions are called that due to their shape in the Cartesian plane, which resembles an \"S\" shape.\n\nSigmoid functions are very useful in the sense that they \"squash\" their given inputs into a bounded interval. This is exceptionally handy when combining these functions with others such as the Step function.\n\nMost of the Sigmoid functions you should find in applications will be the Logistic, Arctangent, and Hyperbolic Tangent functions.\n\nLogistic Regression (sigmoid)\nThe Logistic function, as its name implies, is widely used in Logistic Regression. It is defined as ![image.png](attachment:image.png). Effectively, this makes it so you have a Sigmoid over the  `(0,1)`  interval.\n\n3. etc.","af881738":"Variables must be initialized by running an initialization operation after having launched the graph. We first have to add the initialization operation to the graph:","e170d8d1":"After that, let's make an operation over these tensors. The function tf.add() adds two tensors (you could also use c = a + b).","349cbe5e":"Even this silly example of adding 2 constants to reach a simple result defines the basis of TensorFlow. Define your operations (In this case our constants and tf.add), and start a session to build a graph.","8dedaf96":"Let's first create a simple counter, a variable that increases one unit at a time:\n\nTo do this we use the tf.assign(reference_variable, value_to_update) command. tf.assign takes in two arguments, the reference_variable to update, and assign it to the value_to_update it by.","54b4eade":"# Defining multidimensional arrays using TensorFlow","52383ded":"Close the session to release resources:","1a78a24a":"Now lets look at Deep Belief Networks (DBN) and see how they are build on top of RBMs.\nA DBN is a network that was invented to solve an old problem in traditional artificial neural networks, which is back-propagation problem, that can often cause \"local minima\" or \"vanishing gradients\" issues in the learning process.\nA DBN is built to solve this by the stacking of multiple RBMs.\nDBNs are generally used for classification such as Image recognition.\nThe important part to remember, here is that a DBN is a very accurate discriminative classifier. As such we don't need a big set of labeled data to train a DBN; in fact a small set works fine because feature extraction is unsupervised by a stack of RBMs.","9d7e5a75":"tf.shape returns the shape of our data structure.","138e0d69":"Let's run the session to get the result from the previous defined 'c' operation:","dd5fe4e9":"We then start a session to run the graph, first initialize the variables, then print the initial value of the state variable, and then run the operation of updating the state variable and printing the result after each update:","c54cd8b2":"Moving onto Autoencoders. Much like RBMs, Autoencoders were invented to address the issue of extracting desirable features, and again, much like RBMs autoencoders try to recreate a given input, but do so with a slighlty different network architecture and learning method.\nAutoencoders take a set of unlabeled inputs, encodes them into `short codes`, and then uses those to reconstruct the original image, while extracting the most valuable information from the data.\nApplications of Autoencoders: \n* Dimensionality reduction\n* Feature Extraction\n* Image Recognition\n*","8eff9e8d":"# Why Tensors?\nThe Tensor structure helps us by giving the freedom to shape the dataset in the way we want.\n\nAnd it is particularly helpful when dealing with images, due to the nature of how information in images are encoded,\n\nThinking about images, its easy to understand that it has a height and width, so it would make sense to represent the information contained in it with a two dimensional structure (a matrix)... until you remember that images have colors, and to add information about the colors, we need another dimension, and thats when Tensors become particularly helpful.\n\nImages are encoded into color channels, the image data is represented into each color intensity in a color channel at a given point, the most common one being RGB, which means Red, Blue and Green. The information contained into an image is the intensity of each channel color into the width and height of the image, just like below image.\n\nSo the intensity of the red channel at each point with width and height can be represented into a matrix, the same goes for the blue and green channels, so we end up having three matrices, and when these are combined they form a tensor.\n\n![image.png](attachment:image.png)\n\nImage source : MSDN","702ffd7d":"Noe lets look at another type of Neural Network called as Restricted Boltzman Machine (RBMs), RBM are used to find patterns in data in an unsupervised manner. They are shallow neural nets that learn to reconstruct data by themselves. They are very important models, because they can automatically extract meaningful features from a given input, without the need to label them.\nRBMs might not be outstanding if you look at them as independent networks, but they are significant as building blocks of other networks, such as Deep Belief Networks.\nRBMs are useful for unsupervised tasks such as:\n* Feature Extraction\n* Dimensionality Reduction\n* Pattern recognition\n* Recommender SYstems\n* Handling missing values\n* topic modeling.\n"}}