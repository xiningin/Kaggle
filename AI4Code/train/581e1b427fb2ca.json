{"cell_type":{"b489a873":"code","85ad4a76":"code","42444e5a":"code","c4b3f827":"code","fe580e43":"code","5863d95e":"code","10d10bb0":"code","99c923ac":"code","89b01a87":"code","e487ae8c":"code","8dd302d5":"code","dcfc1cc0":"code","da9bff66":"code","adeab263":"markdown"},"source":{"b489a873":"import pandas as pd\nimport numpy as np\n\n# Natural Language Processing\nimport nltk\nimport re\nfrom keras.preprocessing import text, sequence\n\n# Nueral Networks\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\n# Misc Stuff\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nimport time\nimport random\nimport gc\nimport os\nimport multiprocessing as mp\nimport matplotlib.pyplot as plt","85ad4a76":"# disable progress bars when submitting\ndef is_interactive():\n   return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop","42444e5a":"# Code was taken from this kernel: https:\/\/www.kaggle.com\/bminixhofer\/simple-lstm-pytorch-version\n\ndef clean_special_chars(text):\n    '''\n    # Credit goes to: https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-some-text-preprocessing\n\n    Takes in text as input and returns the text with spaces around each item listed \n    in the variable punct. Special characters are additionally replaced in the outputted text.'''\n\n    punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\n    for p in punct:\n        text = text.replace(p, f' {p} ')\n\n    specials = {'\\u200b': ' ', '\u2026': ' ... ', '\\ufeff': '', '\u0915\u0930\u0928\u093e': '', '\u0939\u0948': ''}\n    for s in specials:\n        text = text.replace(s, specials[s])\n\n    return text\n\ndef clean_text(data):\n    punct_pattern = '|'.join([\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"])\n    possess_pattern = '|'.join([\"'s\", \"s'\"])\n\n    # lowercasing the text\n    data['comment_text'] = data['comment_text'].apply((lambda x: str.lower(x)))\n    # replace incorrect\/alternative forms of apostrophes to its correct form\n    data['comment_text'] = data['comment_text'].str.replace(punct_pattern, \"'\")\n    # expand contractions \n    data['comment_text'] = data['comment_text'].apply((lambda x: expand_contractions(x)))\n    # remove endings for possessives\n    data['comment_text'] = data['comment_text'].str.replace(possess_pattern, \"\") # embeddings for 9.072% training vocab after this step\n    # separate text from symbols and punctuations\n    data['comment_text'] = data['comment_text'].apply((lambda x: clean_special_chars(x))) # embeddings for 39.745% vocab\n    # remove anything that is not characters or whitespace\n    data['comment_text'] = data['comment_text'].apply((lambda x: re.sub('[^a-zA-z\\s]', '', x)))\n    # remove excess whitespace\n    data['comment_text'] = data['comment_text'].astype(str).apply(lambda x: re.sub(' +', ' ',x))\n    \n    return data\n\n# Credit goes to: https:\/\/github.com\/kootenpv\/contractions\/blob\/master\/contractions\/__init__.py\ncontractions_dict = {\n    \"ain't\": \"are not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"c'mon\": \"come on\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how're\": \"how are\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so is\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you shall have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\",\n    \"doin'\": \"doing\",\n    \"goin'\": \"going\",\n    \"nothin'\": \"nothing\",\n    \"somethin'\": \"something\",\n}\n\ncontractions_re = re.compile('|'.join(contractions_dict.keys()))\ndef expand_contractions(s, contractions_dict = contractions_dict):\n    def replace(match):\n        v = match.group()\n        if v in contractions_dict:\n            return contractions_dict[v]\n    return contractions_re.sub(replace, s)","c4b3f827":"def load_data(size=4):\n    \n    start = time.time()\n    \n    train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n    \n    train = clean_text(train)\n    test = clean_text(test)\n     \n    x_train = train['comment_text']\n    y_train = train['target']\n    \n    x_test = test['comment_text']\n    \n    y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n    \n    print('Data was successfully read in and preprocessed. Time taken: ',\n      round((time.time() - start)\/60, 2), ' minutes') \n    \n    return  x_train, x_test, y_train, y_aux_train, test","fe580e43":"def tokenize_text(x_train, x_test, pad_len):\n\n    MAX_WORDS = 100_000\n    \n    start = time.time()\n    \n    tokenizer = text.Tokenizer(num_words = MAX_WORDS)\n    tokenizer.fit_on_texts(list(x_train) + list(x_test))\n\n    x_train = tokenizer.texts_to_sequences(x_train)\n    x_train = sequence.pad_sequences(x_train, maxlen=pad_len)\n\n    x_test = tokenizer.texts_to_sequences(x_test)\n    x_test = sequence.pad_sequences(x_test, maxlen=pad_len)\n        \n    print('Text has been tokenized in', round((time.time() - start)\/60, 2), ' minutes')\n    \n    return x_train, x_test, tokenizer\n","5863d95e":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path, encoding=\"utf8\") as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))   \n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words    \n\ndef make_embedder(tokenizer):\n    \n    GLOVE_PATH = '..\/input\/crawl300d2m\/crawl-300d-2M.vec'\n    CRAWL_PATH = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n\n    start = time.time()\n    glove_matrix, unknown_words = build_matrix(tokenizer.word_index, GLOVE_PATH)\n    crawl_matrix, unknown_words = build_matrix(tokenizer.word_index, CRAWL_PATH)\n\n    embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis = -1)\n\n    gc.collect()\n    \n    print('Embedding matrix was successfully made. Time taken:',\n          round((time.time() - start)\/60,2), ' minutes') \n    \n    return embedding_matrix","10d10bb0":"def make_train_test(x_train, x_test, y_train, y_aux_train):\n    \n    # make test\/train data for pytorch\n    \n    x_train_torch = torch.tensor(x_train, dtype=torch.long).cuda()\n    x_test_torch =  torch.tensor(x_test, dtype=torch.long).cuda()\n    y_train_torch = torch.tensor(\n        np.hstack([y_train[:, np.newaxis], y_aux_train]),\n        dtype=torch.float32).cuda()\n\n    train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n    test_dataset = data.TensorDataset(x_test_torch)\n    \n    print('Pytorch data has been made.')\n    \n    return train_dataset, test_dataset, y_train_torch","99c923ac":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n    \nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets, LSTM_UNITS, max_features):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        \n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","89b01a87":"# Code was taken from this kernel: https:\/\/www.kaggle.com\/bminixhofer\/simple-lstm-pytorch-version\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\ndef train_model(model, train, test, loss_fn, output_dim,\n                lr=0.001, batch_size=512, n_epochs=4, \n                enable_checkpoint_ensemble=True):\n    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n    all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n    \n#    model_stats = pd.DataFrame(columns = ['Epoch', 'Loss', 'Accuracy'])\n    for epoch in range(n_epochs):\n        start_time = time.time()\n        \n        scheduler.step()\n        \n        model.train()\n        avg_loss = 0.\n        \n        for data in tqdm(train_loader, disable=False):\n            x_batch = data[:-1]\n            y_batch = data[-1]\n\n            y_pred = model(*x_batch)            \n            loss = loss_fn(y_pred, y_batch)\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            avg_loss += loss.item() \/ len(train_loader)\n            \n        model.eval()\n        test_preds = np.zeros((len(test), output_dim))\n    \n        for i, x_batch in enumerate(test_loader):\n            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy())\n\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    \n        all_test_preds.append(test_preds)\n        elapsed_time = time.time() - start_time\n        \n        print('Epoch {}\/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n              epoch + 1, n_epochs, avg_loss, elapsed_time))\n        \n\n            \n    if enable_checkpoint_ensemble:\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n    else:\n        test_preds = all_test_preds[-1]\n        \n    return test_preds","e487ae8c":"def seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\nseed_everything()","8dd302d5":"# read in and preprocess data\nx_train, x_test, y_train, y_aux_train, test = load_data(size=1)\n\n# tokenize text\nx_train, x_test, tokenizer = tokenize_text(x_train, x_test, pad_len = 205)\n\n# embedding matrix\nembedding_matrix = make_embedder(tokenizer)","dcfc1cc0":"# set variable units\nlstm_units = 128\nnum_epochs = 5\npad_length = 250\n\n# read in and preprocess data\nx_train, x_test, y_train, y_aux_train, test = load_data(size=1)\n\n# tokenize text\nx_train, x_test, tokenizer = tokenize_text(x_train, x_test, pad_len = pad_length)\n\n# embedding matrix\nembedding_matrix = make_embedder(tokenizer)\n\n# make train\/test data for pytorch\ntrain_dataset, test_dataset, y_train_torch = make_train_test(x_train, x_test, y_train, y_aux_train)\n\nall_test_preds = []\n\nstart = time.time()\n\nNUM_MODELS = 2\nfor model_idx in range(NUM_MODELS):\n    print('Model ', model_idx)\n    seed_everything(0 + model_idx)\n    \n    model = NeuralNet(\n        embedding_matrix, y_aux_train.shape[-1],\n        lstm_units, len(tokenizer.word_index) + 1)\n    model.cuda()\n    test_preds = train_model(\n        model, train_dataset, test_dataset, \n        output_dim=y_train_torch.shape[-1], n_epochs = num_epochs,\n        loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n    \n    all_test_preds.append(test_preds)\n    print('Total time taken: ', round((time.time() - start)\/60,2), 'minutes')\n    print()","da9bff66":"submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n})\n\nsubmission.to_csv('submission.csv', index=False)","adeab263":"The starting framework for this code was taken from: \n https:\/\/www.kaggle.com\/bminixhofer\/simple-lstm-pytorch-version\n\nThere were a few adaptions made such as:\n* adding a contraction map\n* removed possesives\n* removed excess white space\n* \n* moving a lot of the processes into functions to make it easier to rerun and test things\n\nWorks Used:\n\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib]\n\nT. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, A. Joulin. Advances in Pre-Training Distributed Word Representations"}}