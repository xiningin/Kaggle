{"cell_type":{"c2d2cf3f":"code","cf08383c":"code","79969e77":"code","eceee944":"code","de14225e":"code","ac794e98":"code","399bd6e1":"code","5a4983cd":"code","37074c8d":"code","93b5f5cb":"code","93639463":"code","184a2783":"code","5cebdea7":"code","f23d87ea":"code","2344a379":"code","b13593b4":"code","2c2d5393":"code","cbbddc0c":"code","33049c5e":"code","27b08e6b":"code","a0a718f1":"code","e2527fa2":"code","8588546c":"code","18eac452":"code","305bc0ca":"code","efb63b5d":"code","5649b175":"markdown","2805513c":"markdown","2755f229":"markdown","40d16172":"markdown"},"source":{"c2d2cf3f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nimport xgboost as xgb\n\nimport warnings\nwarnings.simplefilter('ignore')","cf08383c":"train_df = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntrain_df.head()","79969e77":"cols = [c for c in train_df.columns if c not in('id')]\nprint(cols)","eceee944":"plt.figure(figsize=(40, 80))\nfor i, col in enumerate(cols):\n    plt.subplot(7,2,i+1)\n    train_df[col].plot.kde()\n    plt.title(col)\n    \nplt.show()","de14225e":"np.unique(train_df['song_popularity'], return_counts=True) # not balanced --> Stratified Kfold","ac794e98":"# Normalize the data\ntcols = [c for c in cols if c not in ('song_popularity')]\n\nnormalizers = {} # saving them for standardizing testing data\nfor col in tcols:\n    std_scaler = StandardScaler()\n    train_df[col] = std_scaler.fit_transform(train_df[col].values.reshape(-1, 1))\n    normalizers[col] = std_scaler","399bd6e1":"train_df = train_df[cols]","5a4983cd":"train_df.head()","37074c8d":"def create_folds(df, target=\"target\"):\n\n    # We create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n\n    # randomizing the rows of the data\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    # Fetch targets || Required in Stratified KFold\n    y = df[target].values\n\n    # init the kfold class from model selection module\n    kf = model_selection.StratifiedKFold(n_splits=5) # k = 5\n\n    for fold, (trn_, val_) in enumerate(kf.split(X=df, y=y)):\n\n        df.loc[val_, 'kfold'] = fold\n\n    # save the new csv with kfol column\n    df.to_csv(\"train_folds.csv\", index=False)","93b5f5cb":"create_folds(train_df, 'song_popularity')","93639463":"# checking\ndf = pd.read_csv('.\/train_folds.csv')\ndf.head()","184a2783":"def run(fold, target=\"target\", xgb_params=None):\n\n    # load the full training data with folds\n    df = pd.read_csv('.\/train_folds.csv')\n\n    # all columns are features excpet target and kfold columns\n    features = [\n        f for f in df.columns if f not in (target, \"kfold\")\n    ]\n\n    # get the training data using folds\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n\n    # get the validation data using folds\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n\n    # transform training data\n    x_train = df_train[features]\n\n    # transform validation data\n    x_valid = df_valid[features]\n\n    # init the XGBoost model\n    model = xgb.XGBClassifier(**xgb_params)\n\n    # fit model on training data\n    model.fit(x_train, df_train[target].values)\n\n    # predict on validation data\n    # we need the probability values as we are calculating AUC\n    # we will use the probability of 1s\n    valid_preds = model.predict_proba(x_valid)[:, 1]\n\n    # get the roc auc score\n    auc = metrics.roc_auc_score(df_valid[target].values, valid_preds)\n\n    # print auc\n    print(f\"Fold = {fold}, AUC = {auc}\")\n    \n    return (auc, model)","5cebdea7":"best_model = None\nbest_score = -1\nspw = len(train_df['song_popularity'] == 0) \/ len(train_df['song_popularity'] == 1)","f23d87ea":"xgb_params= {\n        \"n_estimators\": 3000,\n        \"max_depth\": 8,\n        \"objective\":\"binary:logistic\",\n        \"n_jobs\": 4,\n        \"seed\": 42,\n        'tree_method': \"gpu_hist\",\n        \"gpu_id\": 0,\n        \"eval_metric\": \"auc\", \n        \"subsample\": 0.7,\n        \"colsample_bytree\": 0.4,\n        \"learning_rate\": 0.03,\n        \"scale_pos_weight\" : spw,\n        \"subsample\" : 0.7,\n        \"reg_alpha\" : 0.5,\n        \"gamma\":10\n    }\n\n\nfor fold in range(5):\n    score, model = run(fold, 'song_popularity', xgb_params)\n    if score > best_score:\n        best_model = model\n        best_score = score","2344a379":"best_score","b13593b4":"test = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")","2c2d5393":"test_df = test[tcols]","cbbddc0c":"test_df.info()","33049c5e":"# Standardizing testing data\n\nfor col in tcols:\n    std_scaler = normalizers[col]\n    test_df[col] = std_scaler.transform(test_df[col].values.reshape(-1, 1))","27b08e6b":"test_df.head()","a0a718f1":"# using the best model to predict\n\npreds = best_model.predict_proba(test_df)","e2527fa2":"sub = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\nsub.head()","8588546c":"sub['song_popularity'] = preds","18eac452":"sub","305bc0ca":"sub['song_popularity'] = 1 - sub['song_popularity']\n\nsub.head()","efb63b5d":"sub.to_csv('submission.csv', index=False)","5649b175":"## Measuring the performance of Logistic Regression Model","2805513c":"## Data Exploration (very simple EDA, will try out more)","2755f229":"## Submission","40d16172":"## Spliting the dataset"}}