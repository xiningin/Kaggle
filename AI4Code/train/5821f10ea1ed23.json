{"cell_type":{"85fdc674":"code","29f64b29":"code","b7be0c47":"code","efff056f":"code","e7a6eb2f":"code","4de1db4b":"code","4b14ead2":"code","9a781bb3":"code","b289e179":"code","1c9d4a51":"code","42787509":"code","217fc77e":"code","c9ca8153":"code","b198e3bc":"code","d377e689":"code","ab5f078b":"code","739426d9":"code","6e87bb37":"code","2e05831f":"code","6f143041":"markdown","0a01c770":"markdown","a15f56a8":"markdown","a539c2fd":"markdown","c35e957a":"markdown","96791b68":"markdown","198d4c8e":"markdown","fd0a8637":"markdown","899ac6d9":"markdown","485243a9":"markdown","96a83a4b":"markdown","8ee6512f":"markdown","21a7eb75":"markdown","72a7aaa3":"markdown","4ddbb4b7":"markdown","63061a55":"markdown"},"source":{"85fdc674":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n%matplotlib inline\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","29f64b29":"df=pd.read_json('\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json',lines=True)\ndf.head()","b7be0c47":"df.tail()","efff056f":"df.shape","e7a6eb2f":"df.info()","4de1db4b":"df['is_sarcastic'].value_counts()","4b14ead2":"sns.set_style('whitegrid')\nsns.countplot(x='is_sarcastic',data=df);","9a781bb3":"X=df['headline'].values\ny=df['is_sarcastic'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","b289e179":"vocab_size = 10000\noov_tok = '<00V>'\nmax_length = 120\npadding_type = 'post'\ntrunc_type = 'post'\nembedding_dim = 16\nnum_epochs = 10","1c9d4a51":"tokenizer= Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\nword_index=tokenizer.word_index\ntraining_sequences=tokenizer.texts_to_sequences(X_train)\ntraining_padded=pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\ntesting_sequences=tokenizer.texts_to_sequences(X_test)\ntesting_padded=pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","42787509":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\nmodel.summary()","217fc77e":"history = model.fit(training_padded, \n                    y_train, \n                    validation_data=(testing_padded, y_test), \n                    epochs=30, \n                    verbose=2)","c9ca8153":"def plot_acc(model,epochsnum):\n    df_m=pd.DataFrame(model.history.history)\n    df_m['Epoch']=range(1,epochsnum+1)\n    df_m.index=df_m['Epoch']\n    score = model.evaluate(testing_padded, y_test, batch_size=64, verbose=1)\n    print('Test accuracy:', score[1])\n    plt.plot(df_m['loss'])\n    plt.plot(df_m['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'val_loss'])\n    plt.show()","b198e3bc":"plot_acc(model=model,epochsnum=30)","d377e689":"model2 = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel2.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\nmodel2.summary()\n# train model\nhistory= model2.fit(training_padded, \n                    y_train, \n                    validation_data=(testing_padded, y_test), \n                    epochs=10, \n                    verbose=2)","ab5f078b":"plot_acc(model=model2,epochsnum=30)","739426d9":"model_lstm = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel_lstm.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\nmodel_lstm.summary()","6e87bb37":"history = model_lstm.fit(training_padded, \n                    y_train, \n                    validation_data=(testing_padded, y_test), \n                    epochs=10, \n                    verbose=2)","2e05831f":"plot_acc(model=model_lstm,epochsnum=10)","6f143041":"### data has no null values","0a01c770":"### data classes are not equal","a15f56a8":"## LSTM ","a539c2fd":"## Flatten model","c35e957a":"## Visualize model accuracy and loss","96791b68":"### it's clear we have overfitting :(","198d4c8e":"## Split the data into training\/testing sets, 80% for training and 20% for validation or testing","fd0a8637":"## Building a sequential model","899ac6d9":"### the dataset has 3 columns\n* is_sarcastic: 1 if the record is sarcastic otherwise 0\n* headline: the headline of the news article\n* article_link: link to the original news article. Useful in collecting supplementary data","485243a9":"## The dataset:\n### you can find it [here](https:\/\/www.kaggle.com\/rmisra\/news-headlines-dataset-for-sarcasm-detection) along with all the info","96a83a4b":"### GlobalAveragePooling1D","8ee6512f":"## In this notebook I tried to plainly appply what I learnt without adding any optimization, data cleaning or feature engineering, I'll add these later and observe changes in performance.","21a7eb75":"# Day 5 Action Item, Sarcasm detection\n### In this notebook I'll apply what I learnt about deep learning in the previous days to detect weather a tweet is sarcastic or not.","72a7aaa3":"## Tokens and padding\n### in NLP problems Tokenization is splitting a phrase, sentence, paragraph, or an entire text document into smaller units,individual words or terms.each one of these units is called a token.\n### Padding helps in solving the problem of unequal token lenghts, it adds 0 to fixed lenght zeros to the end of the sequence. ","4ddbb4b7":"### Reading dataset","63061a55":"## Importing libraries"}}