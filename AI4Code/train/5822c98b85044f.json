{"cell_type":{"c41c3fd1":"code","b5ae8603":"code","c156008f":"code","80143724":"code","d0579b93":"code","d960aa04":"code","fb300b74":"code","7075b960":"code","043d8411":"markdown","20be97b8":"markdown"},"source":{"c41c3fd1":"import matplotlib.pyplot as plt","b5ae8603":"import numpy as np\n  \n# define Unit Step Function\ndef unitStep(v):\n    if v >= 0:\n        return 1\n    else:\n        return 0\n  \n# design Perceptron Model\ndef perceptronModel(x, w, b):\n    v = np.dot(w, x) + b\n    y = unitStep(v)\n    return y\n  \n# OR Logic Function\n# w1 = 1, w2 = 1, b = -0.5\ndef OR_logicFunction(x):\n    w = np.array([1, 1])\n    b = -0.5\n    return perceptronModel(x, w, b)\n  \n# testing the Perceptron Model\ntest1 = np.array([0, 1])\ntest2 = np.array([1, 1])\ntest3 = np.array([0, 0])\ntest4 = np.array([1, 0])\n  \nprint(\"OR({}, {}) = {}\".format(0, 1, OR_logicFunction(test1)))\nprint(\"OR({}, {}) = {}\".format(1, 1, OR_logicFunction(test2)))\nprint(\"OR({}, {}) = {}\".format(0, 0, OR_logicFunction(test3)))\nprint(\"OR({}, {}) = {}\".format(1, 0, OR_logicFunction(test4)))","c156008f":"area = 200\nfig = plt.figure(figsize=(6, 6))\nplt.title('The AND Gate', fontsize=20)\nax = fig.add_subplot(111)\n# color red: is class 0 and color blue is class 1.\nax.scatter(0, 0, s=area, c='r', label=\"Class 0\")\nax.scatter(0, 5, s=area, c='b', label=\"Class 0\")\nax.scatter(5, 0, s=area, c='b', label=\"Class 0\")\nax.scatter(5, 5, s=area, c='b', label=\"Class 1\")\nplt.grid()\nplt.show()","80143724":"# Numpy is used for all mathematical operations\nimport numpy as np\n# For plotting purposes\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d # 3 Dimensional plotting\nfrom matplotlib import cm # For some fancy plotting ;-)","d0579b93":"def Cross_Entropy(y_hat, y):\n    # There are 2 possibilities for the ground-truth: either 0 or 1\n    # Note that np.log() is actually the natural logarithm with e, for its base\n    if y == 1:\n      return -np.log(y_hat)\n    else:\n      return -np.log(1 - y_hat)\n# This is just the classic sigmoid function, given input z\ndef sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))","d960aa04":"def derivative_Cross_Entropy(y_hat, y):\n    # Again we account for 2 possibilities of y=0\/1\n    if y == 1:\n      return -1\/y_hat\n    else:\n      return 1 \/ (1 - y_hat)\n# The derivative of sigmoid is quite straight-forward\ndef derivative_sigmoid(x):\n    return x*(1-x)","fb300b74":"# Our data\nX = np.array([[0, 0], [0, 5], [5, 0], [5, 5]])\n# The ground truth (i.e., what AND returns and our perceptron should learn to produce)\nY = np.array([0, 0, 0, 1])","7075b960":"\narea = 200\nfig = plt.figure(figsize=(6, 6))\nplt.title('The AND Gate', fontsize=20)\nax = fig.add_subplot(111)\n# color red: is class 0 and color blue is class 1.\nax.scatter(0, 0, s=area, c='r', label=\"Class 0\")\nax.scatter(0, 5, s=area, c='r', label=\"Class 0\")\nax.scatter(5, 0, s=area, c='r', label=\"Class 0\")\nax.scatter(5, 5, s=area, c='b', label=\"Class 1\")\nplt.grid()\nplt.show()","043d8411":"# OR GATE","20be97b8":"# AND GATE"}}