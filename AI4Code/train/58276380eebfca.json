{"cell_type":{"d05a4107":"code","c017ba4e":"code","71ba02f8":"code","c21ec140":"code","b9c0dc44":"code","0189e65d":"code","799f266c":"code","5138e702":"code","21375a1b":"code","e183e1f4":"code","30676972":"code","ddda4a34":"code","ebbe79c8":"code","e0d36bd4":"code","01bc9bb9":"code","1419df83":"code","3bfdd62a":"code","bb703524":"code","1b3a3920":"markdown","45069257":"markdown","7c3de7c5":"markdown","d1fdff3d":"markdown"},"source":{"d05a4107":"#installing contractions library\n!pip install contractions","c017ba4e":"#Generic Data Processing & Visualization Libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re,string,unicodedata\nimport contractions #import contractions_dict\n%matplotlib inline","71ba02f8":"#Importing text processing libraries\nimport spacy\nimport spacy.cli\n#from spacy.matcher import Matcher\n#from spacy.tokens import Span\n\nimport nltk\nimport collections\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\n#downloading wordnet\/punkt dictionary\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')","c21ec140":"#Loading Data\ndata = pd.read_csv('..\/input\/slogan-dataset\/sloganlist.csv', header='infer')","b9c0dc44":"data.shape","0189e65d":"#checking for null \/ missing values\ndata.isna().sum()","799f266c":"#function to count words\ndef word_count(x):\n  count = len(str(x).split(\" \"))\n  return count\n\n#data ['word_count'] = data['Slogan'].apply(lambda x:len(str(x).split(\" \")))\ndata ['word_count'] = data['Slogan'].apply(word_count)","5138e702":"#lowering cases\ndata['Slogan'] = data['Slogan'].str.lower()\n\n#stripping leading spaces (if any)\ndata['Slogan'] = data['Slogan'].str.strip()","21375a1b":"#removing punctuations\nfrom string import punctuation\n\ndef remove_punct(text):\n  for punctuations in punctuation:\n    text = text.replace(punctuations, '')\n  return text\n\n#apply to the dataset\ndata['Slogan'] = data['Slogan'].apply(remove_punct)","e183e1f4":"#function to remove special characters\ndef remove_special_chars(text, remove_digits=True):\n  pattern = r'[^a-zA-z0-9\\s]'\n  text = re.sub(pattern, '', text)\n  return text\n\n#applying the function on the clean dataset\ndata['Slogan'] = data['Slogan'].apply(remove_special_chars)","30676972":"#function to remove macrons & accented characters\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\n#applying the function on the clean dataset\ndata['Slogan'] = data['Slogan'].apply(remove_accented_chars)  ","ddda4a34":"#Function to expand contractions\ndef expand_contractions(con_text):\n  con_text = contractions.fix(con_text)\n  return con_text\n\n#applying the function on the clean dataset\ndata['Slogan'] = data['Slogan'].apply(expand_contractions) ","ebbe79c8":"#Dataset Backup\ndata_backup = data.copy()","e0d36bd4":"stopword_list = set(stopwords.words('english'))\ntokenizer = ToktokTokenizer()","01bc9bb9":"#function to remove stopwords\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n\n#applying the function\ndata['Slogan_Norm'] = data['Slogan'].apply(remove_stopwords)  ","1419df83":"data ['word_count_norm'] = data['Slogan_Norm'].apply(word_count)","3bfdd62a":"data.head()","bb703524":"plt.style.use('seaborn-deep')\nplt.figure(figsize=(10,10))\nplt.grid(True)\nx = data['word_count']\ny = data['word_count_norm']\nplt.hist([x,y], label=['Word Count Distribution','Normalized Word Count Distribution'])\nplt.legend(loc='upper right')\nplt.title('Word Count Distribution')\nplt.show()","1b3a3920":"**Text Processing\/Normalization**","45069257":"From the above visual, it can be concluded that most of the slogans have 3 words.","7c3de7c5":"**Slogans - Data Analysis**","d1fdff3d":"**Visualisation**"}}