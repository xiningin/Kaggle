{"cell_type":{"bec7d8c8":"code","f62083e0":"code","baf53630":"code","73c683c8":"code","713b4315":"code","633560c6":"code","850d24b2":"code","7777befb":"code","c944c99c":"code","d2006d33":"code","32b680e5":"code","2638d422":"code","7f6d58d7":"code","f51d4bb7":"code","64565a2f":"code","c7fef504":"code","cbdedb22":"code","5d3ed317":"code","97fbd098":"code","c3d4e340":"code","f0a9f4eb":"code","893ae7bf":"code","954247f2":"code","09f6b494":"code","0e18285a":"code","6c57e445":"code","ea8e3a0f":"code","18ddf313":"code","c744a909":"code","31d63fd2":"code","057f364a":"code","0d820d9e":"code","fd01d77e":"code","3348ce53":"code","ad3f3e2d":"code","612bc5c2":"code","13496b09":"code","b679cf6c":"code","dc14c2ca":"code","11962933":"code","c1b96e49":"markdown","dd50186c":"markdown","2d23cca3":"markdown","9ba0ddfd":"markdown","90fa4eb3":"markdown","3cc2497e":"markdown","43370a01":"markdown","b604dc76":"markdown","2b2cd710":"markdown","62262d03":"markdown","c0d8d1c4":"markdown","622fa76a":"markdown","85299ce2":"markdown","2cd83bd0":"markdown","0f8177fa":"markdown","b730e01b":"markdown","6727fd99":"markdown","ca9b8fdb":"markdown","07955efb":"markdown","cd8efd8f":"markdown","68448647":"markdown","870b9792":"markdown","3f20b0c7":"markdown","7c8a831f":"markdown","97cbaa6d":"markdown","a7786777":"markdown","eb53a27f":"markdown","1379b357":"markdown","2e2b0eb8":"markdown","b66368df":"markdown","631f9e25":"markdown"},"source":{"bec7d8c8":"# Ignorando alguns conjunto de Erros\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# Pacotes de visualiza\u00e7\u00e3o e Manipula\u00e7\u00e3o de Dados\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n \n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#Sec\u00e7\u00e3o relacionada a cria\u00e7\u00e3o do modelo\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\n\n#pr\u00e9-processamento.\nfrom keras.preprocessing.image import ImageDataGenerator\n\n#dl libraraies\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense , merge\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.callbacks import ReduceLROnPlateau\n\nfrom keras.layers.merge import dot\nfrom keras.models import Model\n\n# Biblioteca espec\u00edfica para deeplearning.\nfrom keras.layers import Dropout, Flatten,Activation,Input,Embedding\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nimport tensorflow as tf\nimport random as rn\nfrom IPython.display import SVG\n\n#Tratamento de Imagem\nimport cv2                  \nimport numpy as np  \nfrom tqdm import tqdm\nimport os                   \nfrom random import shuffle  \nfrom zipfile import ZipFile\nfrom PIL import Image\n\n#modelo pr\u00e9-treinado VGG-16\nfrom keras.applications.vgg16 import VGG16","f62083e0":"train=pd.read_csv(r'..\/input\/movielens100k\/ratings.csv')","baf53630":"df=train.copy()","73c683c8":"df.head()","713b4315":"df['userId'].unique()","633560c6":"len(df['userId'].unique())","850d24b2":"df['movieId'].unique()","7777befb":"len(df['movieId'].unique())","c944c99c":"df['userId'].isnull().sum()","d2006d33":"df['rating'].isnull().sum()","32b680e5":"df['movieId'].isnull().sum()","2638d422":"df['rating'].min() # minimum rating","7f6d58d7":"df['rating'].max() # maximum rating","f51d4bb7":"df.userId = df.userId.astype('category').cat.codes.values\ndf.movieId = df.movieId.astype('category').cat.codes.values","64565a2f":"df['userId'].value_counts(ascending=True)","c7fef504":"df['movieId'].unique()","cbdedb22":"# criando matriz de utilidade.\nindex=list(df['userId'].unique())\ncolumns=list(df['movieId'].unique())\nindex=sorted(index)\ncolumns=sorted(columns)\n \nutil_df=pd.pivot_table(data=df,values='rating',index='userId',columns='movieId')\n# Nan implica que o usu\u00e1rio n\u00e3o classificou o filme correspondente.","5d3ed317":"util_df","97fbd098":"util_df.fillna(0)","c3d4e340":"# x_train,x_test,y_train,y_test=train_test_split(df[['userId','movieId']],df[['rating']],test_size=0.20,random_state=42)\nusers = df.userId.unique()\nmovies = df.movieId.unique()\n\nuserid2idx = {o:i for i,o in enumerate(users)}\nmovieid2idx = {o:i for i,o in enumerate(movies)}","f0a9f4eb":"df['userId'] = df['userId'].apply(lambda x: userid2idx[x])\ndf['movieId'] = df['movieId'].apply(lambda x: movieid2idx[x])\nsplit = np.random.rand(len(df)) < 0.8\ntrain = df[split]\nvalid = df[~split]\nprint(train.shape , valid.shape)","893ae7bf":"n_movies=len(df['movieId'].unique())\nn_users=len(df['userId'].unique())\nn_latent_factors=64  # hyperparamter to deal with. ","954247f2":"user_input=Input(shape=(1,),name='user_input',dtype='int64')","09f6b494":"user_embedding=Embedding(n_users,n_latent_factors,name='user_embedding')(user_input)\n#user_embedding.shape","0e18285a":"user_vec =Flatten(name='FlattenUsers')(user_embedding)\n#user_vec.shape","6c57e445":"movie_input=Input(shape=(1,),name='movie_input',dtype='int64')\nmovie_embedding=Embedding(n_movies,n_latent_factors,name='movie_embedding')(movie_input)\nmovie_vec=Flatten(name='FlattenMovies')(movie_embedding)\n#movie_vec","ea8e3a0f":"sim=dot([user_vec,movie_vec],name='Simalarity-Dot-Product',axes=1)\nmodel =keras.models.Model([user_input, movie_input],sim)\n# #model.summary()\n# # A summary of the model is shown below-->","18ddf313":"model.compile(optimizer=Adam(lr=1e-4),loss='mse')","c744a909":"train.shape\nbatch_size=128\nepochs=50","31d63fd2":"History = model.fit([train.userId,train.movieId],train.rating, batch_size=batch_size,\n                              epochs =epochs, validation_data = ([valid.userId,valid.movieId],valid.rating),\n                              verbose = 1)","057f364a":"from pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\nimport matplotlib.pyplot as plt\nplt.plot(History.history['loss'] , 'g')\nplt.plot(History.history['val_loss'] , 'b')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.grid(True)\nplt.show()","0d820d9e":"n_latent_factors=50\nn_movies=len(df['movieId'].unique())\nn_users=len(df['userId'].unique())","fd01d77e":"user_input=Input(shape=(1,),name='user_input',dtype='int64')\nuser_embedding=Embedding(n_users,n_latent_factors,name='user_embedding')(user_input)\nuser_vec=Flatten(name='FlattenUsers')(user_embedding)\nuser_vec=Dropout(0.40)(user_vec)","3348ce53":"movie_input=Input(shape=(1,),name='movie_input',dtype='int64')\nmovie_embedding=Embedding(n_movies,n_latent_factors,name='movie_embedding')(movie_input)\nmovie_vec=Flatten(name='FlattenMovies')(movie_embedding)\nmovie_vec=Dropout(0.40)(movie_vec)","ad3f3e2d":"sim=dot([user_vec,movie_vec],name='Simalarity-Dot-Product',axes=1)","612bc5c2":"nn_inp=Dense(96,activation='relu')(sim)\nnn_inp=Dropout(0.4)(nn_inp)\n# nn_inp=BatchNormalization()(nn_inp)\nnn_inp=Dense(1,activation='relu')(nn_inp)\nnn_model =keras.models.Model([user_input, movie_input],nn_inp)\nnn_model.summary()\n","13496b09":"nn_model.compile(optimizer=Adam(lr=1e-3),loss='mse')","b679cf6c":"batch_size=128\nepochs=20","dc14c2ca":"History = nn_model.fit([train.userId,train.movieId],train.rating, batch_size=batch_size,\n                              epochs =epochs, validation_data = ([valid.userId,valid.movieId],valid.rating),\n                              verbose = 1)","11962933":"from pylab import rcParams\nrcParams['figure.figsize'] = 9, 6\nimport matplotlib.pyplot as plt\nplt.plot(History.history['loss'] , 'g')\nplt.plot(History.history['val_loss'] , 'b')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.grid(True)\nplt.show()","c1b96e49":"* * ## 2.1 ) As colunas passam a ser o tipo \"categoria\" Pandas","dd50186c":"## 2.3 ) Cria\u00e7\u00e3o de conjuntos de treinamento e valida\u00e7\u00e3o.","2d23cca3":"## Sistemas de recomenda\u00e7\u00e3o baseados em Filtragem colaborativa de fatora\u00e7\u00e3o de matriz e Rede Neural em Keras.","9ba0ddfd":"## 5. 4) Ajuste no conjunto de treinamento e valida\u00e7\u00e3o no conjunto de valida\u00e7\u00e3o.","90fa4eb3":"Isso confirma que nenhuma das colunas possui qualquer valor NULL ou Nan.","3cc2497e":"1. <a id=\"content2\"><\/a>\n1. ## 2 ) Preparando os Dados","43370a01":"#### Agora vamos nos concentrar na outra coisa principal !!! Usando um NN para a fatora\u00e7\u00e3o da matriz.\n\n1) Observe que esta forma n\u00e3o \u00e9 muito diferente da abordagem anterior.\n\n2) A principal diferen\u00e7a \u00e9 que usamos camadas Fully Connected, bem como as camadas Dropout e BatchNormalization.\n\n3) O n\u00famero de unidades e o n\u00famero de camadas etc. s\u00e3o os hiperpar\u00e2metros aqui como em uma rede neural tradicional.","b604dc76":"1. #### Observe que a perda de valida\u00e7\u00e3o est\u00e1 perto de 0,85, o que \u00e9 bastante decente. Observe tamb\u00e9m que diminuiu de 1,26 no caso da Fatora\u00e7\u00e3o de Matriz normal para este valor aqui.","2b2cd710":"#### Observe o resumo do modelo e tamb\u00e9m a arquitetura do modelo que voc\u00ea pode ajustar, \u00e9 claro.","62262d03":" #### Observe que usei 50 fatores latentes, pois isso parece fornecer um desempenho razo\u00e1vel. O ajuste mais avan\u00e7ado e a otimiza\u00e7\u00e3o cuidadosa podem fornecer resultados ainda melhores.","c0d8d1c4":"#### Da mesma forma, jogando sem nenhum fator latente, outros par\u00e2metros na arquitetura do modelo podem dar resultados ainda melhores !!!!!","622fa76a":"Observe que a m\u00e9trica usada \u00e9 'Erro quadr\u00e1tico m\u00e9dio'. Nosso objetivo \u00e9 minimizar o mse no conjunto de treinamento, ou seja, sobre os valores que o usu\u00e1rio classificou (100004 avalia\u00e7\u00f5es).","85299ce2":"## 5.1 ) Criando Embeddings","2cd83bd0":"Da mesma forma, temos 9066 filmes exclusivos. Observe tamb\u00e9m que, conforme informado, cada usu\u00e1rio votou em pelo menos em 20 filmes. Veremos que a matriz \u00e9 gigante.","0f8177fa":"## 5.3 ) Compilando o modelo","b730e01b":"#### QUEBRANDO -\n\n1) Primeiro, precisamos criar embeddings para o usu\u00e1rio e tamb\u00e9m para o item ou filme. Para isso, usei a camada Embedding de keras.\n\n2) Especifique a entrada que se espera que seja incorporada (tanto na incorpora\u00e7\u00e3o do usu\u00e1rio quanto do item). Use uma camada de Embedding que espera o n\u00e3o de fatores latentes no embedding resultante e tamb\u00e9m o n\u00e3o de usu\u00e1rios ou itens.\n\n3) Em seguida, pegamos o 'Produto Ponto' de ambos os embeddings usando a camada 'mesclar'. Observe que 'produto escalar' \u00e9 apenas uma medida de similitude e podemos usar qualquer outro modo como 'multiplicar' ou 'simularidade de cosseno' ou 'concatenar' etc ...\n\n4) Por \u00faltimo, fazemos um modelo Keras a partir dos detalhes especificados.","6727fd99":"#### QUEBRANDO -\n\n1) Esta \u00e9 a matriz de utilidade; para cada um dos 671 usu\u00e1rios dispostos em linha; cada coluna mostra a avalia\u00e7\u00e3o do filme dada por um determinado usu\u00e1rio.\n\n2) Observe que a maioria da matriz \u00e9 preenchida com 'Nan', o que mostra que a maioria dos filmes n\u00e3o \u00e9 classificada por muitos usu\u00e1rios.\n\n3) Para cada par filme-usu\u00e1rio, se a entrada N\u00c3O for 'Nan', o valor indica a classifica\u00e7\u00e3o dada pelo usu\u00e1rio ao filme correspondente.\n\n4) Por enquanto, irei preencher o valor 'Nan' com o valor '0'. Mas observe que isso \u00e9 apenas indicativo, um 0 significa SEM AVALIA\u00c7\u00c3O e n\u00e3o significa que o usu\u00e1rio classificou 0 para aquele filme. N\u00e3o representa nenhuma classifica\u00e7\u00e3o.","ca9b8fdb":"## 5.2 ) Especificando a arquitetura do modelo","07955efb":"## 3.1 ) Criando os Embeddings, Mesclando e Fazendo o Modelo a partir de Embeddings","cd8efd8f":"<a id=\"content3\"><\/a>\n## 3 )  Fatora\u00e7\u00e3o de Matriz","68448647":"## 3.2 )Compilando o modelo","870b9792":"<a id=\"content4\"><\/a>\n## 4 ) Avaliando o desempenho do modelo","3f20b0c7":"## 3.3 ) Ajuste no conjunto de treinamento e valida\u00e7\u00e3o no conjunto de valida\u00e7\u00e3o.","7c8a831f":"<a id=\"content5\"><\/a>\n## 5 ) Usando Uma Neural Network","97cbaa6d":">> #### A\u00ed vem a parte principal !!!\n\n1) Agora passamos para o ponto crucial do notebook, ou seja, a fatora\u00e7\u00e3o de matrizes. Na facoriza\u00e7\u00e3o da matriz, basicamente dividimos uma matriz em geralmente 2 matrizes menores, cada uma com dimens\u00f5es menores. essas matrizes s\u00e3o freq\u00fcentemente chamadas de 'Embeddings'. Podemos ter variantes de Matrix Factorizartion-> 'Low Rank MF', 'Non-Negaive MF' (NMF) e assim por diante.\n\n2) Aqui, usei a chamada 'Fatora\u00e7\u00e3o de Matriz de Baixo Rank'. Criei embeddings para o usu\u00e1rio e tamb\u00e9m para o item; filme no nosso caso. O n\u00famero de dimens\u00f5es ou os chamados 'Fatores latentes' nos embeddings \u00e9 um hiperpar\u00e2metro com o qual lidar nesta implementa\u00e7\u00e3o de Filtragem Colaborativa.","a7786777":"## 1.2 ) Lendo o arquivo CSV ","eb53a27f":"* Observe que, no total, temos 671 usu\u00e1rios cujo ID de usu\u00e1rio varia de 1 a> 671.","1379b357":"# # #### Observe que, para 671 usu\u00e1rios e 9066 filmes, podemos ter no m\u00e1ximo 671 * 9066 = 6083286 avalia\u00e7\u00f5es. Mas observe que temos apenas 100004 avalia\u00e7\u00f5es conosco. Portanto, a matriz de utilidade tem apenas cerca de 1**,6% dos valores totais. Assim, pode-se concluir que \u00e9 bastante esparso. Isso limita o uso de alguns algoritmos. Portanto, criaremos embeddings para eles mais tarde.","2e2b0eb8":"## 2.2 ) Criando uma Matriz de Utilidade","b66368df":"## 1.3 ) Explorando o Banco de Dados em CSV","631f9e25":"Observe que este arquivo cont\u00e9m as classifica\u00e7\u00f5es dadas para um conjunto de usu\u00e1rios em diferentes filmes. Ao todo, ele cont\u00e9m classifica\u00e7\u00f5es totais de 100K; para ser exato 1000004."}}