{"cell_type":{"c67c8ce3":"code","0855cff6":"code","4049404e":"code","94257902":"code","d990a9ee":"code","16d4bbeb":"code","d98b1e9a":"code","152f5143":"code","c5a3a986":"code","ef3851fb":"code","ee56885c":"code","98481900":"code","16e475d0":"code","db3849b7":"code","5a488b52":"code","e62e1bfb":"code","2fc5265a":"code","4fdf535e":"code","fe54168c":"code","da97cd63":"code","e4dce02a":"code","070b89ba":"code","aa3a5043":"code","7af9f7a4":"code","dcff111b":"code","679c72d5":"code","3e192954":"code","a017d03c":"code","f7867aaf":"code","27a4539b":"code","2e959458":"code","1ec680b4":"code","1cea0763":"code","0234d22c":"code","381e4c52":"code","32e12fb2":"code","362698d6":"code","fe0aebe8":"code","fc5288c9":"code","5bcffa43":"code","8b018949":"code","c041b2d4":"code","adb612ea":"code","0e6f776b":"code","ed2aa592":"code","099cb300":"code","529f5772":"code","681f397b":"code","4fa50118":"code","c01265c6":"code","8ddccd91":"code","96a72e70":"code","10ddab1e":"code","af7e0ec8":"code","1286a659":"code","a7b4ad32":"code","cb551389":"code","096d1627":"code","5f47fc97":"code","122b638e":"code","c7b3ebb6":"code","2df35c6c":"code","dd1c1d4f":"code","cbf39644":"code","0711572b":"code","1a0ffd8a":"code","ae847959":"code","267eba81":"code","33f166d9":"code","9993c058":"code","3fefb399":"code","681a145b":"code","7ce0128f":"code","afd30f63":"code","6b5ad0ff":"code","fb548e40":"code","6e6cb4e9":"code","915c0d4f":"code","462236b8":"code","c4a56ac5":"code","1f5759d4":"code","65ddb269":"code","b40971d9":"code","160a5464":"code","1552932f":"markdown","28ab38d1":"markdown","bc7aa2f7":"markdown","01c651c9":"markdown","59675da6":"markdown","4386c6ad":"markdown","018b1164":"markdown","8e712fbf":"markdown","f9af3587":"markdown","fd6fb03f":"markdown","f058abeb":"markdown","e4ecc897":"markdown","55c3457a":"markdown","e30db19c":"markdown","f1c64438":"markdown","c1db052d":"markdown","704a9e28":"markdown","910ce0e8":"markdown","3183f3db":"markdown","e1737e69":"markdown","12f02a09":"markdown","a4abe1d2":"markdown","40133a14":"markdown","0ecec7e1":"markdown","99fe877e":"markdown","1a19b172":"markdown","a7072fe7":"markdown","e56077a8":"markdown","cf3f51be":"markdown","4c82e53b":"markdown","640b3e9e":"markdown","e5bc1cf8":"markdown","95cee084":"markdown","159a055e":"markdown","790a73ab":"markdown","a1515d28":"markdown","6c0095cf":"markdown","7a20c3d6":"markdown","b5111e40":"markdown","9bec889a":"markdown","cb22e386":"markdown","836b14c3":"markdown","9baf7254":"markdown","27f513fe":"markdown","29e02d09":"markdown","bbe9c967":"markdown","65df72f7":"markdown"},"source":{"c67c8ce3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing, model_selection, ensemble, metrics, tree, linear_model, kernel_ridge\nimport xgboost as xgb\nimport lightgbm as lgb\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nimport sys, time\nfrom scipy import special, stats\nfrom mlxtend import regressor","0855cff6":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","4049404e":"train.head(5)","94257902":"train.describe()","d990a9ee":"train.info()","16d4bbeb":"finalTrain = train.copy()\nfinalTest = test.copy()","d98b1e9a":"corr = finalTrain.corr()['SalePrice'].sort_values(ascending=False)[:11]\ncorr","152f5143":"fig = plt.figure(figsize=[30,30])\nfor col,i in zip(corr.index,range(1,12)):\n    axes = fig.add_subplot(6,2,i)\n    sns.regplot(finalTrain[col],finalTrain.SalePrice,ax=axes)\nplt.show()","c5a3a986":"fig = plt.figure(figsize=[12,6])\nsns.regplot(finalTrain.GrLivArea, finalTrain.SalePrice).set_title('Ground Living Area vs Sale Price')","ef3851fb":"finalTrain = finalTrain[finalTrain['GrLivArea']<4600]","ee56885c":"fig = plt.figure(figsize=[12,6])\nsns.regplot(finalTrain.GrLivArea, finalTrain.SalePrice).set_title('Ground Living Area vs Sale Price')","98481900":"fig = plt.figure(figsize=[12,6])\nsns.distplot(finalTrain.SalePrice, fit = stats.norm)","16e475d0":"finalTrain['SalePrice'] = np.log(1+finalTrain['SalePrice'].values)\n\nfig = plt.figure(figsize=[12,6])\nsns.distplot(finalTrain.SalePrice, fit = stats.norm)","db3849b7":"\nIDTrain = finalTrain['Id']\nIDTest = finalTest['Id']\n\nfinalTrain.drop('Id', axis = 1, inplace = True)\nfinalTest.drop('Id', axis = 1, inplace = True)","5a488b52":"fig = plt.figure(figsize = [30,20])\nmask = np.zeros_like(finalTrain.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(finalTrain.corr(), cmap=sns.diverging_palette(150, 275, s=80, l=55, n=9), mask = mask, annot=True, center = 0)\nplt.title(\"Correlation Matrix (HeatMap)\", fontsize = 30)","e62e1bfb":"unstackedCorrelations = pd.DataFrame({'Correlations' : finalTrain.corr().abs().unstack()})\nunstackedCorrelations = unstackedCorrelations.reset_index().sort_values(by='Correlations')\nunstackedCorrelations[(unstackedCorrelations.Correlations > 0.8) & (unstackedCorrelations.Correlations < 1)]","2fc5265a":"finalTrain.drop('GarageArea', axis = 1, inplace = True)\nfinalTrain.drop('TotRmsAbvGrd', axis = 1, inplace = True)\nfinalTrain.drop('GarageYrBlt', axis = 1, inplace = True)\n\nfinalTest.drop('GarageArea', axis = 1, inplace = True)\nfinalTest.drop('TotRmsAbvGrd', axis = 1, inplace = True)\nfinalTest.drop('GarageYrBlt', axis = 1, inplace = True)","4fdf535e":"SalePriceTrain = finalTrain['SalePrice']\n\nfinalTrain.drop('SalePrice', axis = 1, inplace = True)","fe54168c":"print(finalTrain.shape)\nprint(finalTest.shape)\n\nfinalData = pd.concat([finalTrain,finalTest])\n\nprint(finalData.shape)","da97cd63":"DataMissing = finalData.isnull().sum()*100\/len(finalData)\nDataMissingByColumn = pd.DataFrame({'Percentage Nulls':DataMissing})\nDataMissingByColumn.sort_values(by='Percentage Nulls',ascending=False,inplace=True)\nDataMissingByColumn[DataMissingByColumn['Percentage Nulls']>0]","e4dce02a":"finalData.drop('PoolQC', axis = 1, inplace=True)\nfinalData.drop('MiscFeature', axis = 1, inplace=True)\nfinalData.drop('Alley', axis = 1, inplace=True)\nfinalData.drop('Fence', axis = 1, inplace=True)","070b89ba":"objData = finalData.select_dtypes(\"object\")\n\nfor i in objData:\n    finalData[i].fillna(finalData[i].mode()[0], inplace = True)\n\nintData = finalData.select_dtypes([\"int64\",\"float64\"])\n\nfor i in intData:\n    finalData[i].fillna(finalData[i].median(), inplace = True)","aa3a5043":"DataMissing = finalData.isnull().sum()*100\/len(finalData)\nDataMissingByColumn = pd.DataFrame({'Percentage Nulls':DataMissing})\nDataMissingByColumn.sort_values(by='Percentage Nulls',ascending=False,inplace=True)\nDataMissingByColumn[DataMissingByColumn['Percentage Nulls']>0]","7af9f7a4":"finalData['TotalBaths'] = finalData['BsmtFullBath'] + 0.5 * finalData['BsmtHalfBath'] + finalData['FullBath'] + 0.5 * finalData['HalfBath']","dcff111b":"finalData['HouseAge'] = finalData['YrSold'] - finalData['YearRemodAdd']","679c72d5":"finalData['TotalPorches'] = finalData['OpenPorchSF'] + finalData['EnclosedPorch'] + finalData['3SsnPorch'] + finalData['ScreenPorch']","3e192954":"finalData['Season'] = np.where(finalData['MoSold'].isin([10,11,12,1,2,3]),'Winter','Summer')","a017d03c":"finalData['OverallRate'] = 0.5 * (finalData['OverallQual'] + finalData['OverallCond'])","f7867aaf":"finalData['TotalSF'] = finalData['TotalBsmtSF'] + finalData['1stFlrSF'] + finalData['2ndFlrSF']","27a4539b":"print(finalData.shape)","2e959458":"nonObjectColList = finalData.dtypes[finalData.dtypes != 'object'].index\n\nskewMeasure = finalData[nonObjectColList].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending = False)\n\nskewMeasure=pd.DataFrame({'skew':skewMeasure})\n\nskewMeasure = skewMeasure[abs(skewMeasure)>0.5].dropna()\n\nskewMeasure\n","1ec680b4":"for i in skewMeasure.index:\n    finalData[i] = special.boxcox1p(finalData[i],0.15) #lambda = 0.15","1cea0763":"finalData = pd.get_dummies(finalData)\n\nprint(finalData.shape)","0234d22c":"stdDev = finalData.std().sort_values()\nremoveList = stdDev[stdDev < 0.025]\n\nfor i in removeList.index:\n    finalData.drop(i, axis = 1, inplace = True)\nprint(finalData.shape)","381e4c52":"trainDF = finalData[:len(finalTrain)]\ntestDF = finalData[len(finalTest)-1:]\nprint(trainDF.shape)\nprint(testDF.shape)","32e12fb2":"xTrain, xTest, yTrain, yTest = model_selection.train_test_split(trainDF.to_numpy(),SalePriceTrain.to_numpy(),test_size=0.2,random_state=1010)","362698d6":"lgbAttributes = lgb.LGBMRegressor(objective='regression', n_jobs=-1, random_state=1010)\n                               \n\nlgbParameters = {\n    'max_depth' : [2,3,5],\n    'learning_rate': [0.01,0.05, 0.1],\n    'colsample_bytree' : [1,1.1,1.2,1.3,1.5],\n    'n_estimators' : [480,600,720],\n    'num_leaves' : [4,5,6,7],\n    'max_bin' : [50,100,150,200],\n    'verbose' : [-1],\n    'bagging_seed' : [7],\n    'bagging_freq' : [3,5,7,9],\n    'bagging_fraction' : [0.7,0.8,0.9],\n    'feature_fraction' : [0.2319,0.25,0.27,0.3,0.33],\n    'feature_fraction_seed' : [7],\n    'min_data_in_leaf' : [2,3,4,5,6],\n    'min_sum_hessian_in_leaf' : [15,16,18,20]\n}\n","fe0aebe8":"lgbModel = model_selection.RandomizedSearchCV(lgbAttributes, param_distributions = lgbParameters, cv = 5, random_state=1010)\n\nstart = time.time()\nlgbModel.fit(xTrain,yTrain.flatten())\nend = time.time()\n\nprint('Training took {:.2f} mins'.format((end-start)\/60))\n\nlgbPred = lgbModel.predict(xTest)","fc5288c9":"lgbModel.best_estimator_","5bcffa43":"LGBMMetrics = pd.DataFrame({'Model': 'LightGBM', \n                            'MSE': metrics.mean_squared_error(yTest, lgbPred),\n                            'RMSE' : np.sqrt(metrics.mean_squared_error(yTest, lgbPred)),\n                            'MAE' : metrics.mean_absolute_error(yTest, lgbPred),\n                            'MSLE' : metrics.mean_squared_log_error(yTest, lgbPred), \n                            'RMSLE' : np.sqrt(metrics.mean_squared_log_error(yTest, lgbPred)),\n                            'R-Square' : metrics.r2_score(yTest, lgbPred)},index=[1])\n\nLGBMMetrics","8b018949":"fig = plt.figure(figsize=[12,6])\nsns.regplot(lgbPred,yTest,truncate=False)\nplt.title('Actual vs Predicted')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')","c041b2d4":"LGBMAvP = pd.DataFrame({'Actual': np.exp(yTest), 'Predicted': np.exp(lgbPred)})\nLGBMAvP","adb612ea":"lasAttributes = linear_model.Lasso(alpha=0.0005, max_iter = 2000, random_state=1010)\nlasAttributesCV = linear_model.LassoCV(max_iter = 2000, cv=5, verbose=-1, random_state=1010, n_jobs=-1)","0e6f776b":"start = time.time()\nlasAttributes.fit(xTrain,yTrain.flatten())\nend = time.time()\n\nprint('Training for Lasso took {:.2f} mins'.format((end-start)\/60))\n\nlasPred = lasAttributes.predict(xTest)","ed2aa592":"LASMetrics = pd.DataFrame({'Model': 'Lasso', \n                            'MSE': metrics.mean_squared_error(yTest, lasPred),\n                            'RMSE' : np.sqrt(metrics.mean_squared_error(yTest, lasPred)),\n                            'MAE' : metrics.mean_absolute_error(yTest, lasPred),\n                            'MSLE' : metrics.mean_squared_log_error(yTest, lasPred), \n                            'RMSLE' : np.sqrt(metrics.mean_squared_log_error(yTest, lasPred)),\n                            'R-Square' : metrics.r2_score(yTest, lasPred)},index=[2])\n\nLASMetrics","099cb300":"plt.figure(figsize=[12,6])\nsns.regplot(lasPred,yTest,truncate=False)\nplt.title('Actual vs Predicted')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')","529f5772":"LASAvP = pd.DataFrame({'Actual': np.exp(yTest), 'Predicted': np.exp(lasPred)})\nLASAvP","681f397b":"start = time.time()\nlasAttributesCV.fit(xTrain,yTrain.flatten())\nend = time.time()\n\nprint('Training for LassoCV took {:.2f} mins'.format((end-start)\/60))\n\nlasCVPred = lasAttributesCV.predict(xTest)","4fa50118":"LASCVMetrics = pd.DataFrame({'Model': 'LassoCV', \n                            'MSE': metrics.mean_squared_error(yTest, lasCVPred),\n                            'RMSE' : np.sqrt(metrics.mean_squared_error(yTest, lasCVPred)),\n                            'MAE' : metrics.mean_absolute_error(yTest, lasCVPred),\n                            'MSLE' : metrics.mean_squared_log_error(yTest, lasCVPred), \n                            'RMSLE' : np.sqrt(metrics.mean_squared_log_error(yTest, lasCVPred)),\n                            'R-Square' : metrics.r2_score(yTest, lasCVPred)},index=[3])\n\nLASCVMetrics","c01265c6":"plt.figure(figsize=[12,6])\nsns.regplot(lasCVPred,yTest,truncate=False)\nplt.title('Actual vs Predicted')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')","8ddccd91":"LASCVAvP = pd.DataFrame({'Actual': np.exp(yTest), 'Predicted': np.exp(lasCVPred)})\nLASCVAvP","96a72e70":"ridAttributes = linear_model.Ridge(alpha=5, max_iter=2000, random_state=1010)\nridCVAttributes = linear_model.RidgeCV(alphas=(1,5,10), cv=5)","10ddab1e":"start = time.time()\nridAttributes.fit(xTrain,yTrain.flatten())\nend = time.time()\n\nprint('Training for Ridge took {:.2f} mins'.format((end-start)\/60))\n\nridPred = ridAttributes.predict(xTest)","af7e0ec8":"RIDMetrics = pd.DataFrame({'Model': 'Ridge', \n                            'MSE': metrics.mean_squared_error(yTest, ridPred),\n                            'RMSE' : np.sqrt(metrics.mean_squared_error(yTest, ridPred)),\n                            'MAE' : metrics.mean_absolute_error(yTest, ridPred),\n                            'MSLE' : metrics.mean_squared_log_error(yTest, ridPred), \n                            'RMSLE' : np.sqrt(metrics.mean_squared_log_error(yTest, ridPred)),\n                            'R-Square' : metrics.r2_score(yTest, ridPred)},index=[4])\n\nRIDMetrics","1286a659":"plt.figure(figsize=[12,6])\nsns.regplot(ridPred,yTest,truncate=False)\nplt.title('Actual vs Predicted')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')","a7b4ad32":"RIDAvP = pd.DataFrame({'Actual': np.exp(yTest), 'Predicted': np.exp(ridPred)})\nRIDAvP","cb551389":"start = time.time()\nridCVAttributes.fit(xTrain,yTrain.flatten())\nend = time.time()\n\nprint('Training for RidgeCV took {:.2f} mins'.format((end-start)\/60))\n\nridCVPred = ridCVAttributes.predict(xTest)","096d1627":"RIDCVMetrics = pd.DataFrame({'Model': 'RidgeCV', \n                            'MSE': metrics.mean_squared_error(yTest, ridCVPred),\n                            'RMSE' : np.sqrt(metrics.mean_squared_error(yTest, ridCVPred)),\n                            'MAE' : metrics.mean_absolute_error(yTest, ridCVPred),\n                            'MSLE' : metrics.mean_squared_log_error(yTest, ridCVPred), \n                            'RMSLE' : np.sqrt(metrics.mean_squared_log_error(yTest, ridCVPred)),\n                            'R-Square' : metrics.r2_score(yTest, ridCVPred)},index=[5])\n\nRIDCVMetrics","5f47fc97":"plt.figure(figsize=[12,6])\nsns.regplot(ridCVPred,yTest,truncate=False)\nplt.title('Actual vs Predicted')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')","122b638e":"RIDCVAvP = pd.DataFrame({'Actual': np.exp(yTest), 'Predicted': np.exp(ridCVPred)})\nRIDCVAvP","c7b3ebb6":"xgbAttributes = xgb.XGBRegressor(n_jobs=-1, random_state=1010)\n\nxgbParameters = {\n      \n    'max_depth' : [2,3],\n    'learning_rate': [0.01,0.05, 0.1],\n    'colsample_bytree' : [0.4,0.6,0.8],\n    'n_estimators' : [1000,2000],\n    'gamma' : [0.15,0.3,0.5],\n    'subsample': [0.6,0.7,0.8], #,0.9,1\n    'min_child_weight': [3,4,5],#6,10\n    'scale_pos_weight': [10,20],\n    'reg_alpha' : [0.5,0.75],\n    'reg_lambda' : [0.5,0.75],\n    #'num_leaves' : [3,4],\n    'max_bin' : [200],\n}\n\nxgbModel = model_selection.RandomizedSearchCV(xgbAttributes, param_distributions = xgbParameters, cv = 5, random_state=1010)\n\nstart = time.time()\nxgbModel.fit(xTrain,yTrain.flatten())\nend = time.time()\n\nprint('Training for XGBoost took {:.2f} mins'.format((end-start)\/60))\n\nxgbPred = xgbModel.predict(xTest)","2df35c6c":"XGBMetrics = pd.DataFrame({'Model': 'XGBoost', \n                            'MSE': metrics.mean_squared_error(yTest, xgbPred),\n                            'RMSE' : np.sqrt(metrics.mean_squared_error(yTest, xgbPred)),\n                            'MAE' : metrics.mean_absolute_error(yTest, xgbPred),\n                            'MSLE' : metrics.mean_squared_log_error(yTest, xgbPred), \n                            'RMSLE' : np.sqrt(metrics.mean_squared_log_error(yTest, xgbPred)),\n                            'R-Square' : metrics.r2_score(yTest, xgbPred)},index=[6])\n\nXGBMetrics","dd1c1d4f":"plt.figure(figsize=[12,6])\nsns.regplot(xgbPred,yTest,truncate=False)\nplt.title('Actual vs Predicted')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')","cbf39644":"XGBAvP = pd.DataFrame({'Actual': np.exp(yTest), 'Predicted': np.exp(xgbPred)})\nXGBAvP","0711572b":"adaAttributes = ensemble.AdaBoostRegressor(base_estimator = tree.DecisionTreeRegressor(max_depth=5), random_state = 1010)\n\nadaParameters = {\n    'learning_rate':[0.05,0.1],\n    'n_estimators' : [800,1600]\n}\n\nadaModel = model_selection.RandomizedSearchCV(adaAttributes, param_distributions = adaParameters, cv = 5, random_state=1010)\n\nstart = time.time()\nadaModel.fit(xTrain,yTrain.flatten())\nend = time.time()\n\nprint('Training for AdaBoost took {:.2f} mins'.format((end-start)\/60))\n\nadaPred = adaModel.predict(xTest)","1a0ffd8a":"ADAMetrics = pd.DataFrame({'Model': 'AdaBoost', \n                            'MSE': metrics.mean_squared_error(yTest, adaPred),\n                            'RMSE' : np.sqrt(metrics.mean_squared_error(yTest, adaPred)),\n                            'MAE' : metrics.mean_absolute_error(yTest, adaPred),\n                            'MSLE' : metrics.mean_squared_log_error(yTest, adaPred), \n                            'RMSLE' : np.sqrt(metrics.mean_squared_log_error(yTest, adaPred)),\n                            'R-Square' : metrics.r2_score(yTest, adaPred)},index=[7])\n\nADAMetrics","ae847959":"plt.figure(figsize=[12,6])\nsns.regplot(adaPred,yTest,truncate=False)\nplt.title('Actual vs Predicted')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')","267eba81":"AdaAvP = pd.DataFrame({'Actual': np.exp(yTest), 'Predicted': np.exp(adaPred)})\nAdaAvP","33f166d9":"grbAttributes = ensemble.GradientBoostingRegressor(random_state=1010)\n\ngrbParameters = {\n    'n_estimators': [5000,6000],\n    'max_depth' : [3,4,5],\n    'learning_rate' : [0.01,0.05,0.1],\n    'max_features' : ['sqrt'],\n    'loss' : ['huber'],\n    'min_samples_leaf' : [10,15],\n    'min_samples_split' : [10,15]\n}\n\ngrbModel = model_selection.RandomizedSearchCV(grbAttributes, param_distributions = grbParameters, cv=5, random_state=1010)\n\nstart = time.time()\ngrbModel.fit(xTrain,yTrain.flatten())\nend = time.time()\n\nprint('Training for Gradient Boosting took {:.2f} mins'.format((end-start)\/60))\n\ngrbPred = grbModel.predict(xTest)","9993c058":"GRBMetrics = pd.DataFrame({'Model': 'Gradient Boosting', \n                            'MSE': metrics.mean_squared_error(yTest, grbPred),\n                            'RMSE' : np.sqrt(metrics.mean_squared_error(yTest, grbPred)),\n                            'MAE' : metrics.mean_absolute_error(yTest, grbPred),\n                            'MSLE' : metrics.mean_squared_log_error(yTest, grbPred), \n                            'RMSLE' : np.sqrt(metrics.mean_squared_log_error(yTest, grbPred)),\n                            'R-Square' : metrics.r2_score(yTest, grbPred)},index=[8])\n\nGRBMetrics","3fefb399":"plt.figure(figsize=[12,6])\nsns.regplot(grbPred,yTest,truncate=False)\nplt.title('Actual vs Predicted')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')","681a145b":"GrbAvP = pd.DataFrame({'Actual': np.exp(yTest), 'Predicted': np.exp(grbPred)})\nGrbAvP","7ce0128f":"rfAttributes = ensemble.RandomForestRegressor(n_jobs = -1, random_state=1010)\n\nrfParameters = {\n    'n_estimators' : [1000,1500],\n    'max_depth' : [5,10,15],\n    'min_samples_leaf' : [4,5],\n    'min_samples_split' : [5,10],\n    'oob_score' : [True]\n}\n\nrfModel = model_selection.RandomizedSearchCV(rfAttributes, param_distributions = rfParameters, cv=5, random_state = 1010)\n\nstart = time.time()\nrfModel.fit(xTrain, yTrain.flatten())\nend = time.time()\n\nprint('Training for Random Forest took {:.2f} mins'.format((end-start)\/60))\n\nrfPred = rfModel.predict(xTest)","afd30f63":"RFMetrics = pd.DataFrame({'Model': 'Random Forest', \n                            'MSE': metrics.mean_squared_error(yTest, rfPred),\n                            'RMSE' : np.sqrt(metrics.mean_squared_error(yTest, rfPred)),\n                            'MAE' : metrics.mean_absolute_error(yTest, rfPred),\n                            'MSLE' : metrics.mean_squared_log_error(yTest, rfPred), \n                            'RMSLE' : np.sqrt(metrics.mean_squared_log_error(yTest, rfPred)),\n                            'R-Square' : metrics.r2_score(yTest, rfPred)},index=[9])\n\nRFMetrics","6b5ad0ff":"plt.figure(figsize=[12,6])\nsns.regplot(rfPred,yTest,truncate=False)\nplt.title('Actual vs Predicted')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')","fb548e40":"rfAvP = pd.DataFrame({'Actual': np.exp(yTest), 'Predicted': np.exp(rfPred)})\nrfAvP","6e6cb4e9":"stckAttributes = regressor.StackingCVRegressor(regressors = (lasAttributes, lgbModel, lasAttributesCV, ridAttributes, ridCVAttributes),\n                                               random_state = 1010,\n                                               meta_regressor = lgbModel, use_features_in_secondary = True)\n\nstart = time.time()\nstckAttributes.fit(xTrain,yTrain.flatten())\nend = time.time()\n\nprint('Training for Stacking took {:.2f} mins'.format((end-start)\/60))\n\nstckPred = stckAttributes.predict(xTest)","915c0d4f":"STCKMetrics = pd.DataFrame({'Model': 'Stacking', \n                            'MSE': metrics.mean_squared_error(yTest, stckPred),\n                            'RMSE' : np.sqrt(metrics.mean_squared_error(yTest, stckPred)),\n                            'MAE' : metrics.mean_absolute_error(yTest, stckPred),\n                            'MSLE' : metrics.mean_squared_log_error(yTest, stckPred), \n                            'RMSLE' : np.sqrt(metrics.mean_squared_log_error(yTest, stckPred)),\n                            'R-Square' : metrics.r2_score(yTest, stckPred)},index=[10])\n\nSTCKMetrics","462236b8":"plt.figure(figsize=[12,6])\nsns.regplot(stckPred,yTest,truncate=False)\nplt.title('Actual vs Predicted')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')","c4a56ac5":"stckAvP = pd.DataFrame({'Actual': np.exp(yTest), 'Predicted': np.exp(stckPred)})\nstckAvP","1f5759d4":"frames = [LGBMMetrics,LASMetrics,LASCVMetrics,RIDMetrics,RIDCVMetrics,XGBMetrics,ADAMetrics,GRBMetrics,RFMetrics,STCKMetrics]\nTrainingResult = pd.concat(frames)\nTrainingResult","65ddb269":"def blend(X):\n    return((0.3*stckAttributes.predict(X)) + (0.5*lasAttributes.predict(X)) + (0.1*lgbModel.predict(X)) + (0.1*ridCVAttributes.predict(X)))\n\nblendedPred = blend(xTest)","b40971d9":"BlendMetrics = pd.DataFrame({'Model': 'Blend', \n                            'MSE': metrics.mean_squared_error(yTest, blendedPred),\n                            'RMSE' : np.sqrt(metrics.mean_squared_error(yTest, blendedPred)),\n                            'MAE' : metrics.mean_absolute_error(yTest, blendedPred),\n                            'MSLE' : metrics.mean_squared_log_error(yTest, blendedPred), \n                            'RMSLE' : np.sqrt(metrics.mean_squared_log_error(yTest, blendedPred)),\n                            'R-Square' : metrics.r2_score(yTest, blendedPred)},index=[11])\n\nBlendMetrics","160a5464":"SalePrice = stckAttributes.predict(testDF.to_numpy())\nSalePrice = np.exp(SalePrice)\n\nblendedPrice = blend(testDF.to_numpy())\nblendedPrice = np.exp(blendedPrice)\n\ntoSubmit = pd.DataFrame({'Id' : IDTest, 'SalePrice' : blendedPrice})\n\ntoSubmit.to_csv('submission.csv', index=False) ","1552932f":"## Stacking","28ab38d1":"Now, we plan to remove the columns with a very low Standard Deviation in its values. As a lower SD will only increase the complexity to our predictive modelling. Removing columns with SD below 0.025.","bc7aa2f7":"After diving into our dataset more, another feature we can create is the Total Number of Porches by combining the following columns:\n\n- OpenPorchSF\t\n- EnclosedPorch\t\n- 3SsnPorch\t\n- ScreenPorch","01c651c9":"From the above table, we can draw the following conclusions:\n\n- **GarageCars** and **GarageArea** are highly correlated to each other, and from the heatmap, both are highly correlated to the **SalePrice**. Hence, removing **GarageArea** from our analysis since it adds redundancy.\n- **TotRmsAbvGrd** and **GrLivArea** are highly correlated to each other, and from our previous analysis to remove outliers from **GrLivArea**, we observed **GrLivArea** and **SalePrice** are highly correlated to each other. Hence, we remove **TotRmsAbvGrd** from our analysis since it adds redundancy.\n- Similar analysis and assumptions are applicable to **GarageYrBlt** as well. Therefore, removing it from our data to avoid redundancy.\n- **1stFlrSF** and **2ndFlrSF** are being kept intact as they will be added to feature engineering (in the later sections of this notebook)\n- **SalePrice** and **OverallQual** are highly related to each other and should not be disturbed.","59675da6":"## LightGBM\n\n**Hyperparameter Tuning**","4386c6ad":"# Feature Engineering\n\nLet's deep dive into our dataset again. We can see 4 columns related to Bath:\n\n- BsmtFullBath\n- BsmtHalfBath\n- FullBath\n- HalfBath\n\nWe can create a new feature from these 4 columns to 2 by using the process where Half Bath = 1\/2 * Full Bath and adding it to the Full Bath columns.","018b1164":"We can observe that there are 81 columns in our dataset which calls for some room for improvement. \n\nWe will be particularly looking to merge some features and create some new out of the process. \n\nFirst let's create final training and test sets which will contain our new features and the merged old features, it will be a copy of the original training set","8e712fbf":"## Gradient Boosting\n\n**Hyperparameter Tuning**","f9af3587":"## Lasso & LassoCV","fd6fb03f":"To dig deep into our dataset,let's first understand the structure of the dataset","f058abeb":"In order to handle NAs, we will replace the nulls in the columns of datatype 'object' with the mode of the respective column, whereas for the columns for datatypes 'integer', or 'float', we will replace the nulls with the median of the respective column.","e4ecc897":"- **LASSO**","55c3457a":"## Blending the Models","e30db19c":"# Model Training\n\n## Train-Test Split\n\nSplitting our Training and Test in 70-30 proportions.","f1c64438":"## XGBoost\n\n**Hyperparameter Tuning**","c1db052d":"The 4th feature which we will be adding is the Season feature. As you can see the MoSold column, which suggests the Month the house was sold. But since the column is an integer from 1 to 12, it can be interpreted as levels during modelling. Hence, we group those values as:\n\n- 10,11,12,1,2,3 - Winter\n- 4,5,6,7,8,9 - Summer","704a9e28":"### Removing Irrelevant or High Correlated Columns\n\nAlso, to add some simplicity to our analysis, we will perform the following steps:\n\n- Remove ID columns from Training and Test Sets (Irrelevant for Predictive Modelling)\n- Find correlation between different values and remove columns with high correlation with each other, as well as the SalePrice column.\n- Remove SalePrice column from the training data and storing in a different variable.\n- Merge Training and Test sets to perform the rest of the EDA in order to maintain consistency. We will spli them later before applying our regression models.\n\n**Step 1**: Removing ID Columns","910ce0e8":"**Step 4**: Merging finalTrain and finalTest","3183f3db":"**Step 2**: Performing Correlation Analysis as mentioned above.","e1737e69":"Converting all columns into numerical values makes our life a bit easier when performing predictive modelling. Now we are ready to implement a few models on this training set.","12f02a09":"Since, BoxCox Transformation has reduced the skewness in our data. We move on the next step of our notebook.\n\n### One Hot Encoding\n\nFinally, we convert all the remaining categorical columns into dummies i.e. One Hot Encoding to make our analysis simpler and better for the machine to understand in order to perform predictive modelling.","a4abe1d2":"# Housing Prices: Advanced Regression\n\nAfter a deep dive into the data dictionary, rigorous Exploratory Data Analysis was done on the dataset. Following are the list of important parts in this notebook:\n\n1. EDA\n2. Box Cox Transformation\n3. Feature Engineering\n4. Light GBM\n5. Lasso\n6. LassoCV\n7. Ridge\n8. RidgeCV\n9. XGBoost\n10. AdaBoost\n11. Gradient Boosting\n12. Random Forest\n13. Stacking\n14. Blending Best Models\n\n\nAll comments are welcome, and please upvote if you find it helpful. Thank you","40133a14":"Rechecking for Nulls to make sure our piece above worked.","0ecec7e1":"Much Better, as now, we can see a complete increasing relationship between the two features.\n\n### Skewness Check in the Column to be Predicted\n\nWe need to be sure that in order to calculate accurate predictions, our column which is to be predicted should not be skewed. Hence, checking the distribution of the column SalePrice.","99fe877e":"### Skewness Check in all the columns\n\n\nNow checking the skewness for all the columns integer columns. We are doing this practice in order to reduce the skewness in our dataset. We need the values to be more normal in order to make our analyses simpler. Box Cox Transformation is one such method of doing this which reduces the skewness of our data & helps us conduct a variety of tests. More of this can be found [here](https:\/\/www.statisticshowto.com\/box-cox-transformation\/#:~:text=A%20Box%20Cox%20transformation%20is,a%20broader%20number%20of%20tests.)","1a19b172":"- **RidgeCV**","a7072fe7":"Creating two more features of the total number of square foots in a house which will be the sum of basement, 1st floor and the 2nd floor.","e56077a8":"Now Let's look at our regplot for GrLivArea vs SalePrice again","cf3f51be":"# Exploratory Data Analysis","4c82e53b":"**Step 3**: Removing SalePrice column","640b3e9e":"Here we can see there are skewness in many columns. Hence, we will apply Box Cox transformation to all these columns in order to reduce their skewness. We would consider lambda as 0, to apply log transformation.","e5bc1cf8":"You can see an increasing relationship between GrLivArea and SalePrice but there are a few outliers where GrLivArea is very high for a lower price. Safe to say we can remove these two rows for which we are seeing such anomaly.","95cee084":"- **RIDGE**","159a055e":"Therefore, there are no nulls remaining. Hence, we are in a good position to move to the next step in our analysis.","790a73ab":"The 5th Feature that we will make is the overall average rating of the house to determine its price. We will do that by taking the arithmetic average of the **OverallQual** and **OverallCond**","a1515d28":"We can see the columns 'PoolQC', 'MiscFeature', 'Alley' and 'Fence' are almost nulls, hence we will remove those columns as our basic assumption is that they do not add to the predictions much.","6c0095cf":"- **LASSOCV**","7a20c3d6":"## AdaBoost\n\n**Hyperparameter Tuning**","b5111e40":"We see that our data is skewed, hence in order to bring it to normal values, we can apply log transformation over it in order to reduce the skewness.","9bec889a":"## Ridge & RidgeCV","cb22e386":"Since the Sale Price is the column to predict and there are 81 columns, there are many ways to continue our EDA. First we will perform Outlier Analysis.\n\n### Outlier Analysis\n\nFor this project, let's consider the columns with the highest correlation to the feature SalePrice. Let's take the features greater than 0.5 with respect to SalePrice.","836b14c3":"## Random Forest\n\n**Hyperparameter Tuning**","9baf7254":"### Missing Data Handling\n\nFor the next part, we will consider how to handle NaNs for this project.","27f513fe":"From the above table, we see that Lasso has the least RMSLE. But when submitted, the stacking classifier showed better performance score.","29e02d09":"#### Actual vs Predicted","bbe9c967":"Have an individual look at all the graphs here and try to determine and increasing or decreasing relationship between the features.\n\nFrom these 10 plots we see that only **'GrLivArea'** has some outliers. Let's have a look at it individually.","65df72f7":"We can create another feature where we can monitor the age of house from its selling date to the last time it was remodelled."}}