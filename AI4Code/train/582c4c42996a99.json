{"cell_type":{"e73889f0":"code","8ea22c15":"code","f3a5b2c8":"code","57e7380b":"code","b1d1b690":"code","ee3f6f0b":"code","980c7da9":"code","9e23fcf2":"code","686d64bc":"code","8d4641c5":"code","094ef075":"code","f8d70bb4":"code","9140d65d":"code","5867b93d":"code","b42b3b26":"code","22d01ef0":"code","da15d5ac":"code","2f4bb2a6":"code","686dc5ca":"code","797b65a2":"code","cbc20cf6":"code","85d8a2b6":"code","ab7fbf1c":"code","5ddd9155":"code","cd77a6fc":"code","50467dbd":"code","6ef3c76b":"code","db646cb0":"code","02b6932a":"code","3b0d85ae":"markdown","b8ae4bde":"markdown","5a48ca67":"markdown","d8d374ea":"markdown","bc8605bc":"markdown","7603ab3d":"markdown","38d54b3a":"markdown","74dab81b":"markdown","ee516f81":"markdown","4520e39d":"markdown","62f40c0e":"markdown","1b4b399c":"markdown","a5ac1669":"markdown","206d746c":"markdown","326a5cb7":"markdown","0d037a20":"markdown","2d3763b4":"markdown","ef25b3f2":"markdown","3ed870b0":"markdown","bbe7a899":"markdown","bdd3a663":"markdown","edb4e1a5":"markdown","084f9550":"markdown","61425036":"markdown","1fc351ee":"markdown","179b0821":"markdown","525e8076":"markdown","fed0c991":"markdown","05034be3":"markdown","4540c8be":"markdown","3cecf137":"markdown","e3923b3d":"markdown","01efb748":"markdown","456bc91a":"markdown","f2671478":"markdown","9bfc34ff":"markdown"},"source":{"e73889f0":"# Utilities\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\n\n# Plot libraries\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","8ea22c15":"# Importing the dataset\nDATASET_COLUMNS  = [\"num\",\"\"\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\ndataset = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',\n                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\ndataset.head()","f3a5b2c8":"# Removing the unnecessary columns.\ndataset = dataset[['sentiment','text']]\n\n# Replacing the values.\ndataset['sentiment'] = dataset['sentiment'].replace(4,1)","57e7380b":"ax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n                                               legend=False)\nax = ax.set_xticklabels(['Negative','Positive'], rotation=0)","b1d1b690":"# Reading contractions.csv and storing it as a dict.\ncontractions = pd.read_csv('..\/input\/contractions\/contractions.csv', index_col='Contraction')\ncontractions.index = contractions.index.str.lower()\ncontractions.Meaning = contractions.Meaning.str.lower()\ncontractions_dict = contractions.to_dict()['Meaning']\n\n# Defining regex patterns.\nurlPattern        = r\"((http:\/\/)[^ ]*|(https:\/\/)[^ ]*|(www\\.)[^ ]*)\"\nuserPattern       = '@[^\\s]+'\nhashtagPattern    = '#[^\\s]+'\nalphaPattern      = \"[^a-z0-9<>]\"\nsequencePattern   = r\"(.)\\1\\1+\"\nseqReplacePattern = r\"\\1\\1\"\n\n# Defining regex for emojis\nsmileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\nsademoji          = r\"[8:=;]['`\\-]?\\(+\"\nneutralemoji      = r\"[8:=;]['`\\-]?[\\\/|l*]\"\nlolemoji          = r\"[8:=;]['`\\-]?p+\"\n\ndef preprocess_apply(tweet):\n\n    tweet = tweet.lower()\n\n    # Replace all URls with '<url>'\n    tweet = re.sub(urlPattern,'<url>',tweet)\n    # Replace @USERNAME to '<user>'.\n    tweet = re.sub(userPattern,'<user>', tweet)\n    \n    # Replace 3 or more consecutive letters by 2 letter.\n    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n\n    # Replace all emojis.\n    tweet = re.sub(r'<3', '<heart>', tweet)\n    tweet = re.sub(smileemoji, '<smile>', tweet)\n    tweet = re.sub(sademoji, '<sadface>', tweet)\n    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n    tweet = re.sub(lolemoji, '<lolface>', tweet)\n\n    for contraction, replacement in contractions_dict.items():\n        tweet = tweet.replace(contraction, replacement)\n\n    # Remove non-alphanumeric and symbols\n    tweet = re.sub(alphaPattern, ' ', tweet)\n\n    # Adding space on either side of '\/' to seperate words (After replacing URLS).\n    tweet = re.sub(r'\/', ' \/ ', tweet)\n    return tweet","ee3f6f0b":"%%time\ndataset['processed_text'] = dataset.text.apply(preprocess_apply)","980c7da9":"count=0\nfor row in dataset.itertuples():\n    print(\"Text:\", row[2])\n    print(\"Processed:\", row[3])\n    count+=1\n    if count>10:\n        break","9e23fcf2":"processedtext = list(dataset['processed_text'])\ndata_pos = processedtext[800000:]\ndata_neg = processedtext[:800000]","686d64bc":"wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n              collocations=False).generate(\" \".join(data_pos))\nplt.figure(figsize = (20,20))\nplt.imshow(wc)","8d4641c5":"wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n               collocations=False).generate(\" \".join(data_neg))\nplt.figure(figsize = (20,20))\nplt.imshow(wc)","094ef075":"from sklearn.model_selection import train_test_split","f8d70bb4":"X_data, y_data = np.array(dataset['processed_text']), np.array(dataset['sentiment'])\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n                                                    test_size = 0.05, random_state = 0)\nprint('Data Split done.')","9140d65d":"from gensim.models import Word2Vec\n\nEmbedding_dimensions = 100\n\n# Creating Word2Vec training dataset.\nWord2vec_train_data = list(map(lambda x: x.split(), X_train))","5867b93d":"%%time\n\n# Defining the model and training it.\nword2vec_model = Word2Vec(Word2vec_train_data,\n                 vector_size=Embedding_dimensions,\n                 workers=8,\n                 min_count=5)\n\nprint(\"Vocabulary Length:\", len(word2vec_model.wv.key_to_index))","b42b3b26":"# Defining the model input length.\ninput_length = 60\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","22d01ef0":"vocab_length = 60000\n\ntokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\ntokenizer.fit_on_texts(X_data)\ntokenizer.num_words = vocab_length\nprint(\"Tokenizer vocab length:\", vocab_length)","da15d5ac":"X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\nX_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)\n\nprint(\"X_train.shape:\", X_train.shape)\nprint(\"X_test.shape :\", X_test.shape)","2f4bb2a6":"embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n\nfor word, token in tokenizer.word_index.items():\n    if word2vec_model.wv.__contains__(word):\n        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n\nprint(\"Embedding Matrix Shape:\", embedding_matrix.shape)","686dc5ca":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding","797b65a2":"def getModel():\n    embedding_layer = Embedding(input_dim = vocab_length,\n                                output_dim = Embedding_dimensions,\n                                weights=[embedding_matrix],\n                                input_length=input_length,\n                                trainable=False)\n\n    model = Sequential([\n        embedding_layer,\n        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n        Conv1D(100, 5, activation='relu'),\n        GlobalMaxPool1D(),\n        Dense(16, activation='relu'),\n        Dense(1, activation='sigmoid'),\n    ],\n    name=\"Sentiment_Model\")\n    return model","cbc20cf6":"training_model = getModel()\ntraining_model.summary()","85d8a2b6":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\ncallbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]","ab7fbf1c":"training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","5ddd9155":"history = training_model.fit(\n    X_train, y_train,\n    batch_size=1024,\n    epochs=2,\n    validation_split=0.1,\n    callbacks=callbacks,\n    verbose=1,\n)","cd77a6fc":"acc,  val_acc  = history.history['accuracy'], history.history['val_accuracy']\nloss, val_loss = history.history['loss'], history.history['val_loss']\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","50467dbd":"from sklearn.metrics import confusion_matrix, classification_report\n\ndef ConfusionMatrix(y_pred, y_test):\n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    categories  = ['Negative','Positive']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() \/ np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","6ef3c76b":"# Predicting on the Test dataset.\ny_pred = training_model.predict(X_test)\n\n# Converting prediction to reflect the sentiment predicted.\ny_pred = np.where(y_pred>=0.5, 1, 0)\n\n# Printing out the Evaluation metrics. \nConfusionMatrix(y_pred, y_test)","db646cb0":"# Print the evaluation metrics for the dataset.\nprint(classification_report(y_test, y_pred))","02b6932a":"# Saving Word2Vec-Model\nword2vec_model.wv.save('Word2Vec-twitter-100')\nword2vec_model.wv.save_word2vec_format('Word2Vec-twitter-100-trainable')\n\n# Saving the tokenizer\nwith open('Tokenizer.pickle', 'wb') as file:\n    pickle.dump(tokenizer, file)\n\n# Saving the TF-Model.\ntraining_model.save('Sentiment-BiLSTM')\ntraining_model.save_weights(\"Model Weights\/weights\")","3b0d85ae":"We'll now train our model using the **fit** method and store the output learning parameters in **history**, which can be used to plot out the learning curve.\n\n**Arguements:**\n* **batch_size:** Number of samples per gradient update. Increasing the batch_size speeds up the training.\n* **epochs:** Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided.\n* **validation_split:** Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch\n* **callbacks:** List of callbacks to apply during training process.\n* **verbose:** 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.","b8ae4bde":"### Printing out the Learning curve\n\nLearning curves show the relationship between training set size and your chosen evaluation metric (e.g. RMSE, accuracy, etc.) on your training and validation sets. They can be an extremely useful tool when diagnosing your model performance, as they can tell you whether your model is suffering from bias or variance.","5a48ca67":"## Introduction to Natural Language Processing\n\n**Natural Language Processing (NLP):** The discipline of computer science, artificial intelligence and linguistics that is concerned with the creation of computational models that process and understand natural language. These include: making the computer understand the semantic grouping of words (e.g. cat and dog are semantically more similar than cat and spoon), text to speech, language translation and many more\n\n**Sentiment Analysis:** It is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis allows organizations to identify public sentiment towards certain words or topics.\n\nIn this notebook, we'll develop a **Sequence model** that can perform **Sentiment Analysis** to categorize a tweet as **Positive or Negative.**\n\n\n## Table of Contents\n1. [Importing dependencies](#1)\n2. [Importing dataset](#2)\n3. [Preprocessing the Text](#3)\n4. [Analysing the data](#4)\n5. [Splitting the data](#5)\n6. [Creating Word Embeddings](#6)\n7. [Tokenizing and Padding datasets](#7)\n8. [Creating Embedding matrix](#8)\n9. [Creating the Model](#9)\n10. [Training the Model](#10)\n11. [Evaluating Model](#11)\n12. [Saving the Model](#12)\n\n\n## <a id=\"1\">Importing Dependencies<\/a>","d8d374ea":"## <a id=\"9\">Creating the Model<\/a>\n\nThere are different approches which we can use to build our Sentiment analysis model. We're going to build a deeplearning **Sequence model.**\n\n**Sequence model** are very good at getting the context of a sentence, since it can understand the meaning rather than employ techniques like counting positive or negative words like in a **Bag-of-Words model**.","bc8605bc":"We require only the **sentiment** and **text** fields, so we discard the rest.\n\nFurthermore, we're remaping the **sentiment** field so that it has new values to reflect the sentiment. **(0 = Negative, 1 = Positive)**","7603ab3d":"history = training_model.fit(\n    X_train, y_train,\n    batch_size=256,\n    epochs=3,\n    validation_split=0.1,\n    callbacks=callbacks,\n    verbose=1,\n)","38d54b3a":"## <a id=\"11\">Evaluating Model<\/a>\n\nSince our dataset is not **skewed**, i.e. it has equal number of **Positive and Negative Predictions**. We're choosing **Accuracy** as our evaluation metric. Furthermore, we're plotting the **Confusion Matrix** to get an understanding of how our model is performing on both classification types.","74dab81b":"## <a id=\"12\">Saving the Model<\/a>\n\nWe're saving the **tokenizer, Word2Vec and Tensorflow model** for use later.\n\n**Word2Vec model** has been saved as 2 different formats:\n1. **KeyedVectors:** KeyedVectors do not support further training. On the other hand, by shedding the internal data structures necessary for training, KeyedVectors offer a smaller RAM footprint and a simpler interface.\n2. **Full Word2Vec Model:** The Full Word2Vec model contains the model state to enable further training. Saving in this format enables us to update the Vectors later.","ee516f81":"From the training curve we can conclude that our model doesn't have bias nor is it overfitting. The accuracy curve has flattened but is still rising, which means training for more epochs can yield better results.\n\nThe Validation loss is lower than the training loss because the dropouts in LSTM aren't active while evaluating the model.","4520e39d":"history = training_model.fit(\n    X_train, y_train,\n    batch_size=1024,\n    epochs=12,\n    validation_split=0.1,\n    callbacks=callbacks,\n    verbose=1,\n)","62f40c0e":"### Classification Report","1b4b399c":"Tokenizing the **X_train and X_test** dataset and padding them to the length **'input_length'**.\n\nThe tokenized list is **pre-padded**, i.e padding tokens are added to the start. After padding, the length of the data would be equal to **'input_length'**.","a5ac1669":"## <a id=\"4\">Analysing the Data<\/a>\nNow we're going to analyse the preprocessed data to get an understanding of it. We'll plot **Word Clouds** for **Positive and Negative** tweets from our dataset and see which words occur the most.","206d746c":"# Importing the dataset\nDATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\ndataset = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',\n                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\ndataset.head()","326a5cb7":"**Word2Vec()** function creates and trains the word embeddings using the data passed.\n\n**Training Parameters**\n* **size:** The number of dimensions (N) that the Word2Vec maps the words onto. Bigger size values require more training data, but can lead to better (more accurate) models.\n* **workers:** Specifies the number of worker threads for training parallelization, to speed up training.\n* **min_count:** min_count is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there\u2019s not enough data to make any meaningful training on those words, so it\u2019s best to ignore them.","0d037a20":"## <a id=\"7\">Tokenizing and Padding datasets<\/a>\n\n**Tokenization** is a common task in **Natural Language Processing (NLP)**. It\u2019s a fundamental step in both traditional NLP methods like **Count Vectorizer** and Advanced Deep Learning-based architectures like **Transformers**.\n\n**Tokenization** is a way of separating a piece of text into smaller units called **tokens**. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types \u2013 word, character, and subword (n-gram characters) tokenization.\n\nAll the neural networks require to have inputs that have the same shape and size. However, when we pre-process and use the texts as inputs for our model e.g. LSTM, not all the sentences have the same length. We need to have the inputs with the same size, this is where the **padding** is necessary.\n\n**Padding** is the process by which we can add padding tokens at the start or end of a sentence to increase it's length upto the required size. If required, we can also drop some words to reduce to the specified length.\n\n\n* **Tokenizer:** Tokenizes the dataset into a list of tokens.\n* **pad_sequences:** Pads the tokenized data to a certain length.\n\nThe **input_length** has been set to 60. This will be the length after the data is tokenized and padded.","2d3763b4":"### Model Compile\n\nThe Model must be compiled to define the **loss, metrics and optimizer**. Defining the proper loss and metric is essential while training the model.\n\n**Loss:** We're using **Binary Crossentropy**. It is used when we have binary output categories. Check out this article on [losses](https:\/\/machinelearningmastery.com\/how-to-choose-loss-functions-when-training-deep-learning-neural-networks\/).\n\n**Metric:** We've selected **Accuracy** as it is one of the common evaluation metrics in classification problems when the category data is equal. Learn more about metrics [here](https:\/\/machinelearningmastery.com\/custom-metrics-deep-learning-keras-python\/).\n\n**Optimizer:** We're using **Adam**, optimization algorithm for Gradient Descent. You can learn more about Adam [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers\/Adam).","ef25b3f2":"Defining the **Tokenizer** and fitting it on the training data. Here, we are tokenzing the data by spliting it up with the **delimiter space ' '**.\n\n**Args in Tokenizer():**\n1. **filters:** Characters to filter out from the sentences to tokenize.\n2. **lower:** True\/False. Whether to lowerCase the sentence or not.\n3. **oov_token:** Out of Vocabulary token to put in for words which aren't in the tokenizer vocab.\n\nFilters and lower has been turned off because we've already done those steps during the preprocessing step.","3ed870b0":"We're **plotting the distribution** for the dataset to see whether we have equal number of positive and negatives tweets or not.","bbe7a899":"From the confusion matrix, it can be concluded that the model makes more False Negative predictions than positive. This means that the model is **somewhat biased** towards predicting negative sentiment.","bdd3a663":"### Model Architecture\n\n1) **Embedding Layer:** Layer responsible for converting the tokens into their vector representation that is generated by Word2Vec model. We're using the predefined layer from Tensorflow in out model.\n\n**Arguments -**\n* **input_dim**: Size of the vocabulary.\n* **output_dim**: Dimension of the dense embedding.\n* **weights**: Initiazises the embedding matrix.\n* **trainable**: Specifies whether the layer is trainable or not.\n\n2) **Bidirectional:** Bidirectional wrapper for RNNs. It means the context are carried from both left to right and right to left in the wrapped RNN layer.\n\n3) **LSTM:** **Long Short Term Memory**, its a variant of **RNN** which has memory state cell to learn the context of words which are at further along the text to carry contextual meaning rather than just neighbouring words as in case of RNN. \n\n**Arguments -**\n* **units:** Positive integer, dimensionality of the output space.\n* **dropout:**  Fraction of the units to drop for the linear transformation of the inputs.\n* **return_sequence:** Whether to return the last output in the output sequence, or the full sequence.\n\n4) **Conv1D:** This layer creates a convolution kernel that is convolved with the layer input over a single dimension to produce a tensor of outputs.\n\n**Arguments -**\n* **filters:** The dimensionality of the output space (i.e. the number of output filters in the convolution).\n* **kernel_size:** Specifies the length of the 1D convolution window.\n* **activation:** Specifies the activation function to use. \n\n5) **GlobalMaxPool1D:** Downsamples the input representation by taking the maximum value over the different dimensions.\n\n\n6) **Dense:** Dense layer adds a fully connected layer in the model. The argument passed specifies the number of nodes in that layer.\n\nThe last dense layer has the activation **\"Sigmoid\"**, which is used to transform the input to a number between 0 and 1. Sigmoid activations are generally used when we have 2 categories to output in.","edb4e1a5":"## <a id=\"6\">Creating Word Embeddings using Word2Vec model<\/a>\n\n**Word embedding** is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. Loosely speaking, word embeddings are **vector representations** of a particular word.\n\n**Word2Vec** was developed by Google and is one of the most popular technique to learn word embeddings using shallow neural network.\nWord2Vec can create word embeddings using two methods (both involving Neural Networks): **Skip Gram** and **Common Bag Of Words (CBOW)**.\n\nTo learn more about Word2Vec and Word embeddings check out this [article](https:\/\/towardsdatascience.com\/introduction-to-word-embedding-and-word2vec-652d0c2060fa).","084f9550":"## <a id=\"2\">Importing Dataset<\/a>\nThe dataset being used is the **sentiment140 dataset**. It contains 1,600,000 tweets extracted using the **Twitter API**. The tweets have been annotated **(0 = Negative, 4 = Positive)** and they can be used to detect sentiment.\n \n**[The training data isn't perfectly categorised as it has been created by tagging the text according to the emoji present. So, any model built using this dataset may have lower than expected accuracy, since the dataset isn't perfectly categorised.]**\n\n**It contains the following 6 fields:**\n1. **sentiment**: the polarity of the tweet *(0 = negative, 4 = positive)*\n2. **ids**: The id of the tweet *(2087)*\n3. **date**: the date of the tweet *(Sat May 16 23:58:44 UTC 2009)*\n4. **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n5. **user**: the user that tweeted *(robotickilldozr)*\n6. **text**: the text of the tweet *(Lyx is cool)*","61425036":"## <a id=\"5\">Splitting the Data<\/a>\n\nMachine Learning models are trained and tested on different sets of data. This is done so to reduce the chance of the model overfitting to the training data, i.e it fits well on the training dataset but a has poor fit with new ones.\n\n**sklearn.model_selection.train_test_split** shuffles the dataset and splits it into train and test dataset.\n\nThe Pre-processed Data is divided into 2 sets of data: \n1. **Training Data:** The dataset upon which the model would be trained on. Contains 95% data.\n2. **Test Data:** The dataset upon which the model would be tested against. Contains 5% data.","1fc351ee":"Cleaning up the tweet text and storing it in **\"processed_text\"** field in the dataframe.","179b0821":"### Word-Cloud for Negative tweets.","525e8076":"### Model Summary","fed0c991":"### Model Callbacks\n\n**Callbacks** are objects that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc).\n\nWe can use callbacks to write **TensorBoard logs** after every batch of training, periodically save our model, stop training early or even to get a view on internal states and statistics during training.\n\n**ReduceLROnPlateau:** Reduces Learning Rate whenever the gain in performance metric specified stops improving.\n\n* monitor: quantity to be monitored.\n* patience: number of epochs with no improvement after which learning rate will be reduced.\n* cooldown: number of epochs to wait before resuming normal operation after lr has been reduced.\n\n**EarlyStopping:** Stop training when a monitored metric has stopped improving.\n\n* monitor: Quantity to be monitored.\n* min_delta: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n* patience: Number of epochs with no improvement after which training will be stopped.\n\nFor more details on callbacks, check out the [documentation](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/Callback) on tensorflow.","05034be3":"### Word-Cloud for Positive tweets.","4540c8be":"As we can see from the graphs, we have equal number of Positive\/Negative tweets. Both equaling to 800,000 tweets. This means our dataset is **not skewed** which makes working on the dataset easier for us.","3cecf137":"### Confusion Matrix","e3923b3d":"## <a id=\"8\">Creating Embedding Matrix<\/a>\n\n**Embedding Matrix** is a maxtrix of all words and their corresponding embeddings. We use embedding matrix in an **Embedding layer** in our model to embedded a token into it's vector representation, that contains information regarding that token or word.\n\nWe get the embedding vocabulary from the tokenizer and the corresponding vectors from the Embedding Model, which in this case is the Word2Vec model.\n\n**Shape of Embedding matrix** is usually the **Vocab Length * Embedding Dimension.**\n","01efb748":"## <a id=\"3\">Preprocessing the Text<\/a>\n**Text Preprocessing** is traditionally an important step for **Natural Language Processing (NLP)** tasks. It transforms text into a more digestible form so that deep learning algorithms can perform better.\n\nTweets usually contains a lot of information apart from the text, like mentions, hashtags, urls, emojis or symbols. Since normally, NLP models cannot parse those data, we need to clean up the tweet and replace tokens that actually contains meaningful information for the model.\n\n**The Preprocessing steps taken are:**\n1. **Lower Casing:** Each text is converted to lowercase.\n2. **Replacing URLs:** Links starting with **'http' or 'https' or 'www'** are replaced by **'<url\\>'**.\n3. **Replacing Usernames:** Replace @Usernames with word **'<user\\>'**. [eg: '@Kaggle' to '<user\\>'].\n4. **Replacing Consecutive letters:** 3 or more consecutive letters are replaced by 2 letters. [eg: 'Heyyyy' to 'Heyy']\n5. **Replacing Emojis:** Replace emojis by using a regex expression. [eg: ':)' to '<smile\\>']\n6. **Replacing Contractions:** Replacing contractions with their meanings. [eg: \"can't\" to 'can not']\n7. **Removing Non-Alphabets:** Replacing characters except Digits, Alphabets and pre-defined Symbols with a space.\n\nAs much as the preprocessing steps are important, the actual sequence is also important while cleaning up the text. For example, removing the punctuations before replacing the urls means the regex expression cannot find the urls. Same with mentions or hashtags. So make sure, the actual sequence of cleaning makes sense.","456bc91a":"Using the classification report, we can see that that the model achieves nearly **85% Accuracy** after training for just **12 epochs.** This is really good and better than most other models achieve.","f2671478":"## <a id=\"10\">Training the Model<\/a>","9bfc34ff":"Let's have a look and see what the processed text looks like."}}