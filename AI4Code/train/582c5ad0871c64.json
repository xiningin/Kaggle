{"cell_type":{"5c1f02cc":"code","6e8e050c":"code","73bcd36a":"code","4501d61d":"code","e79cc9f2":"code","96dec823":"code","7226547c":"code","135a3002":"code","5de686e7":"code","8f1fa034":"code","e04782a2":"code","16b090ea":"code","50dfffdc":"code","49e106d7":"code","54907c73":"code","1948383d":"code","70ccdf83":"code","ef5d82c7":"code","4591528e":"code","06151ea5":"code","d985f709":"code","f67f8e25":"code","1e742b0b":"code","59e4a460":"code","bfff27cd":"code","c40bf26b":"code","060f00cd":"code","29fd0bb6":"code","8acad6f6":"code","5e5617fd":"code","f29f7392":"code","1ef7ad17":"code","2a67d196":"code","8f897d8d":"code","f3c52ee6":"code","0b12df01":"code","c706a3c3":"code","460dada5":"code","5c4437f0":"code","351e93de":"code","25012102":"code","05b1ed24":"code","c4e32582":"code","2a7572a6":"code","64357aec":"code","6b61d656":"code","912f493c":"code","dfad4bf1":"code","507a8d70":"code","b85ea2eb":"markdown","7ccdc3d4":"markdown","bb099e45":"markdown","2f4049e6":"markdown","7009cc87":"markdown","a12aa95b":"markdown","d6739a3b":"markdown","15d645fd":"markdown","87df1eb1":"markdown","adea4732":"markdown","836f5613":"markdown","7510f759":"markdown","a7242e40":"markdown","ae6036ea":"markdown","a99a64a3":"markdown","23eed63b":"markdown","18b4c26f":"markdown","5a508533":"markdown","7d9dfeb9":"markdown","14ff9790":"markdown","80163655":"markdown","d4c29ff3":"markdown","e3d4b609":"markdown","f243f53e":"markdown","9997128a":"markdown","b308c5ca":"markdown","cdb9b8c4":"markdown","e4404079":"markdown","e25c83eb":"markdown","5e2eeb9b":"markdown","e75b225a":"markdown","8afcec41":"markdown","e1427007":"markdown","10703fb5":"markdown"},"source":{"5c1f02cc":"from surprise import Reader, Dataset, SVD, evaluate\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nr_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\nratings = pd.read_csv('..\/input\/ml-100k\/u.data', sep='\\t', names=r_cols, index_col='movie_id', encoding='latin-1')\n\nm_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url','unknown', 'Action', 'Adventure',\\\n          'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy','Film-Noir', 'Horror',\\\n          'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies = pd.read_csv('..\/input\/ml-100k\/u.item', sep='|', names=m_cols, index_col=0, encoding='latin-1')\n\nu_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\nusers = pd.read_csv('..\/input\/ml-100k\/u.user', sep='|', names=u_cols, encoding='latin-1', parse_dates=True)","6e8e050c":"from datetime import datetime\n\nratings['unix_timestamp'] = ratings['unix_timestamp'].apply(datetime.fromtimestamp)\nratings.columns = ['user_id', 'rating', 'time']\nratings.head(10)","73bcd36a":"ratings['rating'].hist(bins=9)","4501d61d":"movies['release_date'] = pd.to_datetime(movies['release_date'])\nmovies.head(10)","e79cc9f2":"for i in users['occupation'].unique():\n    users[i] = users['occupation'] == i\nusers.drop('occupation', axis=1, inplace=True)\nusers.head(10)","96dec823":"ratings_movie_summary = ratings.groupby('movie_id')['rating'].agg(['count', 'mean', 'std'])\nratings_movie_summary.head(10)","7226547c":"ratings_user_summary = ratings.groupby('user_id')['rating'].agg(['count', 'mean', 'std'])\nratings_user_summary.head(10)","135a3002":"ratings_movie_summary.sort_values(by='count')['count'].hist(bins=20)","5de686e7":"ratings_movie_summary.sort_values(by='mean')['mean'].hist(bins=20)","8f1fa034":"ratings_user_summary.sort_values(by='count')['count'].hist(bins=20)","e04782a2":"ratings_user_summary.sort_values(by='mean')['mean'].hist(bins=20)","16b090ea":"ratings_p = pd.pivot_table(ratings, values='rating', index='user_id', columns='movie_id')\nratings_p.iloc[:10, :10]","50dfffdc":"mean = ratings_p.stack().mean()\nstd = ratings_p.stack().std()","49e106d7":"movie_mean = np.ones(ratings_p.shape)\nmovie_mean = pd.DataFrame(movie_mean * np.array(ratings_movie_summary['mean']).reshape(1,1682))\nuser_mean = np.ones(ratings_p.T.shape)\nuser_mean = pd.DataFrame(user_mean * np.array(ratings_user_summary['mean'])).T\npred = movie_mean + user_mean - mean\nscore = abs(np.array(ratings_p) - pred)\nscore_2 = score ** 2\nprint('RMSE: {:.4f}'.format(np.sqrt(score_2.stack().mean())))\nprint('MAE: {:.4f}'.format(score.stack().mean()))","54907c73":"from sklearn.model_selection import KFold\n\nkfolds = KFold(n_splits = 5, random_state = 13)\nrmse = []\nmae = []\ni = 0\nprint('Evaluating RMSE, MAE of the Baseline Model. \\n')\nprint('-'*12)\nfor train_index, test_index in kfolds.split(ratings):\n    train = ratings.copy()\n    test = ratings.copy()\n    train['rating'].iloc[test_index] = np.NaN\n    test['rating'].iloc[train_index] = np.NaN\n    train_movie_summary = train.groupby('movie_id')['rating'].agg(['count', 'mean', 'std'])\n    train_user_summary = train.groupby('user_id')['rating'].agg(['count', 'mean', 'std'])\n    test_p = pd.pivot_table(test, values='rating', index='user_id', columns='movie_id', dropna=False)\n    movie_mean = np.ones(ratings_p.shape)\n    movie_mean = pd.DataFrame(movie_mean * np.array(train_movie_summary['mean']).reshape(1,1682))\n    user_mean = np.ones(ratings_p.T.shape)\n    user_mean = pd.DataFrame(user_mean * np.array(train_user_summary['mean'])).T\n    train_p = movie_mean + user_mean - mean\n    score = abs(np.array(test_p) - train_p)\n    score_2 = score ** 2\n    rmse += [np.sqrt(score_2.stack().mean())]\n    mae += [score.stack().mean()]\n    i += 1\n    print('Fold', i)\n    print('RMSE: {:.4f}'.format(np.sqrt(score_2.stack().mean())))\n    print('MAE: {:.4f}'.format(score.stack().mean()))\n    print('-'*12)\nprint('-'*12)\nprint('Mean RMSE: {:.4f}'.format(np.mean(rmse)))\nprint('Mean MAE: {:.4f}'.format(np.mean(mae)))\nprint('-'*12)\nprint('-'*12)","1948383d":"movie_mean = np.ones(ratings_p.shape)\nmovie_mean = pd.DataFrame(movie_mean * np.array(ratings_movie_summary['mean']).reshape(1,1682))\nuser_mean = np.ones(ratings_p.T.shape)\nuser_mean = pd.DataFrame(user_mean * np.array(ratings_user_summary['mean'])).T\nuser_std = np.ones(ratings_p.T.shape)\nuser_std = pd.DataFrame(user_std * np.array(ratings_user_summary['std'])).T\npred_plus = user_mean + (movie_mean - mean)\/std * user_std\nscore_plus = abs(np.array(ratings_p) - pred_plus)\nscore_2_plus = score_plus ** 2\nprint('RMSE: {:.4f}'.format(np.sqrt(score_2_plus.stack().mean())))\nprint('MAE: {:.4f}'.format(score_plus.stack().mean()))","70ccdf83":"user_196 = movies[['title', 'release_date']]\nuser_196['Estimate_Score'] = np.array(pred_plus.loc[195])\nuser_196 = user_196.sort_values('Estimate_Score', ascending=False)\nprint(user_196.head(10))","ef5d82c7":"rmse_plus = []\nmae_plus = []\ni = 0\nprint('Evaluating RMSE, MAE of the Baseline_Plus Model. \\n')\nprint('-'*12)\nfor train_index, test_index in kfolds.split(ratings):\n    train = ratings.copy()\n    test = ratings.copy()\n    train['rating'].iloc[test_index] = np.NaN\n    test['rating'].iloc[train_index] = np.NaN\n    train_movie_summary = train.groupby('movie_id')['rating'].agg(['count', 'mean', 'std'])\n    train_user_summary = train.groupby('user_id')['rating'].agg(['count', 'mean', 'std'])\n    test_p = pd.pivot_table(test, values='rating', index='user_id', columns='movie_id', dropna=False)\n    movie_mean = np.ones(ratings_p.shape)\n    movie_mean = pd.DataFrame(movie_mean * np.array(train_movie_summary['mean']).reshape(1,1682))\n    user_mean = np.ones(ratings_p.T.shape)\n    user_mean = pd.DataFrame(user_mean * np.array(train_user_summary['mean'])).T\n    user_std = np.ones(ratings_p.T.shape)\n    user_std = pd.DataFrame(user_std * np.array(train_user_summary['std'])).T\n    train_p = user_mean + (movie_mean - mean)\/std * user_std\n    score = abs(np.array(test_p) - train_p)\n    score_2 = score ** 2\n    rmse_plus += [np.sqrt(score_2.stack().mean())]\n    mae_plus += [score.stack().mean()]\n    i += 1\n    print('Fold', i)\n    print('RMSE: {:.4f}'.format(np.sqrt(score_2.stack().mean())))\n    print('MAE: {:.4f}'.format(score.stack().mean()))\n    print('-'*12)\nprint('-'*12)\nprint('Mean RMSE: {:.4f}'.format(np.mean(rmse_plus)))\nprint('Mean MAE: {:.4f}'.format(np.mean(mae_plus)))\nprint('-'*12)\nprint('-'*12)","4591528e":"from sklearn.svm import SVR\n\nmovie_mean = np.ones(ratings_p.shape)\nmovie_mean = pd.DataFrame(movie_mean * np.array(ratings_movie_summary['mean']).reshape(1,1682))\nX = np.array(ratings_p*0) + movie_mean\nsvm = SVR(gamma=1, C=1)\npred_svm = ratings_p.copy()\nfor i in range(ratings_p.shape[0]):\n    svm.fit(np.array(X.iloc[i].dropna()).reshape(-1,1), ratings_p.iloc[i].dropna())\n    pred_svm.iloc[i] = svm.predict(np.array(movie_mean.iloc[0]).reshape(-1,1))\nscore_svm = abs(np.array(ratings_p) - pred_svm)\nscore_2_svm = score_svm ** 2\nprint('RMSE: {:.4f}'.format(np.sqrt(score_2_svm.stack().mean())))\nprint('MAE: {:.4f}'.format(score_svm.stack().mean()))","06151ea5":"user_196_svm = movies[['title', 'release_date']]\nuser_196_svm['Estimate_Score'] = np.array(pred_svm.loc[195])\nuser_196_svm = user_196_svm.sort_values('Estimate_Score', ascending=False)\nprint(user_196_svm.head(10))","d985f709":"rmse_svm = []\nmae_svm = []\nfold = 0\nmovie_mean = pd.DataFrame(np.ones(ratings_p.shape) * np.array(ratings_movie_summary['mean']).reshape(1,1682))\nprint('Evaluating RMSE, MAE of the Baseline_SVM Model. \\n')\nprint('-'*12)\nfor train_index, test_index in kfolds.split(ratings):\n    train = ratings.copy()\n    test = ratings.copy()\n    train['rating'].iloc[test_index] = np.NaN\n    test['rating'].iloc[train_index] = np.NaN\n    train_movie_summary = train.groupby('movie_id')['rating'].agg(['count', 'mean', 'std'])\n    train_user_summary = train.groupby('user_id')['rating'].agg(['count', 'mean', 'std'])\n    train_p = pd.pivot_table(train, values='rating', index='user_id', columns='movie_id', dropna=False)\n    test_p = pd.pivot_table(test, values='rating', index='user_id', columns='movie_id', dropna=False)\n    train_mean = pd.DataFrame(np.ones(ratings_p.shape) * np.array(train_movie_summary['mean']).reshape(1,1682))\n    X = np.array(train_p*0) + train_mean\n    pred = ratings_p.copy()\n    for i in range(ratings_p.shape[0]):\n        svm.fit(np.array(X.iloc[i].dropna()).reshape(-1,1), train_p.iloc[i].dropna())\n        pred.iloc[i] = svm.predict(np.array(movie_mean.iloc[0]).reshape(-1,1))\n    score = abs(np.array(test_p) - pred)\n    score_2 = score ** 2\n    rmse_svm += [np.sqrt(score_2.stack().mean())]\n    mae_svm += [score.stack().mean()]\n    fold += 1\n    print('Fold', fold)\n    print('RMSE: {:.4f}'.format(np.sqrt(score_2.stack().mean())))\n    print('MAE: {:.4f}'.format(score.stack().mean()))\n    print('-'*12)\nprint('-'*12)\nprint('Mean RMSE: {:.4f}'.format(np.mean(rmse_svm)))\nprint('Mean MAE: {:.4f}'.format(np.mean(mae_svm)))\nprint('-'*12)\nprint('-'*12)","f67f8e25":"from xgboost import XGBRegressor\n\nmovie_mean = np.ones(ratings_p.shape)\nmovie_mean = pd.DataFrame(movie_mean * np.array(ratings_movie_summary['mean']).reshape(1,1682))\nX = np.array(ratings_p*0) + movie_mean\nxgb = XGBRegressor(learning_rate=0.1, max_depth=2, min_child_weight=10, gamma=1)\npred_xgb = ratings_p.copy()\nfor i in range(ratings_p.shape[0]):\n    xgb.fit(np.array(X.iloc[i].dropna()).reshape(-1,1), ratings_p.iloc[i].dropna())\n    pred_xgb.iloc[i] = xgb.predict(np.array(movie_mean.iloc[0]).reshape(-1,1))\nscore_xgb = abs(np.array(ratings_p) - pred_xgb)\nscore_2_xgb = score_xgb ** 2\nprint('RMSE: {:.4f}'.format(np.sqrt(score_2_xgb.stack().mean())))\nprint('MAE: {:.4f}'.format(score_xgb.stack().mean()))","1e742b0b":"rmse_xgb = []\nmae_xgb = []\nfold = 0\nmovie_mean = pd.DataFrame(np.ones(ratings_p.shape) * np.array(ratings_movie_summary['mean']).reshape(1,1682))\nprint('Evaluating RMSE, MAE of the Baseline_XGB Model. \\n')\nprint('-'*12)\nfor train_index, test_index in kfolds.split(ratings):\n    train = ratings.copy()\n    test = ratings.copy()\n    train['rating'].iloc[test_index] = np.NaN\n    test['rating'].iloc[train_index] = np.NaN\n    train_movie_summary = train.groupby('movie_id')['rating'].agg(['count', 'mean', 'std'])\n    train_user_summary = train.groupby('user_id')['rating'].agg(['count', 'mean', 'std'])\n    train_p = pd.pivot_table(train, values='rating', index='user_id', columns='movie_id', dropna=False)\n    test_p = pd.pivot_table(test, values='rating', index='user_id', columns='movie_id', dropna=False)\n    train_mean = pd.DataFrame(np.ones(ratings_p.shape) * np.array(train_movie_summary['mean']).reshape(1,1682))\n    X = np.array(train_p*0) + train_mean\n    pred = ratings_p.copy()\n    for i in range(ratings_p.shape[0]):\n        xgb.fit(np.array(X.iloc[i].dropna()).reshape(-1,1), train_p.iloc[i].dropna())\n        pred.iloc[i] = xgb.predict(np.array(movie_mean.iloc[0]).reshape(-1,1))\n    score = abs(np.array(test_p) - pred)\n    score_2 = score ** 2\n    rmse_xgb += [np.sqrt(score_2.stack().mean())]\n    mae_xgb += [score.stack().mean()]\n    fold += 1\n    print('Fold', fold)\n    print('RMSE: {:.4f}'.format(np.sqrt(score_2.stack().mean())))\n    print('MAE: {:.4f}'.format(score.stack().mean()))\n    print('-'*12)\nprint('-'*12)\nprint('Mean RMSE: {:.4f}'.format(np.mean(rmse_xgb)))\nprint('Mean MAE: {:.4f}'.format(np.mean(mae_xgb)))\nprint('-'*12)\nprint('-'*12)","59e4a460":"def recommend(movie_title, min_count):\n    print(\"For movie ({})\".format(movie_title))\n    print(\"- Top 10 movies recommended based on Pearsons'R correlation - \")\n    i = movies[movies['title'] == movie_title].index[0]\n    target = ratings_p[i]\n    similar_to_target = ratings_p.corrwith(target)\n    corr_target = pd.DataFrame(similar_to_target, columns = ['PearsonR'])\n    corr_target.dropna(inplace = True)\n    corr_target = corr_target.sort_values('PearsonR', ascending = False)\n    corr_target.index = corr_target.index.map(int)\n    corr_target = corr_target.join(movies).join(ratings_movie_summary)\\\n                  [['PearsonR', 'title', 'count', 'mean']]\n    print (corr_target[corr_target['count']>min_count][:10].to_string(index=False))","bfff27cd":"recommend('Shawshank Redemption, The (1994)', 10)","c40bf26b":"sim = ratings_p.corr().abs()\nsim.iloc[:10, :10]","060f00cd":"knn_pred = ratings_p.copy()\nfor i in ratings_p.index:\n    if i % 10 == 0:\n        print(i)\n    N = sim.loc[ratings[ratings['user_id'] == i].index]\n    for j in ratings_p.columns:\n        try:\n            N_k = N[j].sort_values(ascending=False, kind='heapsort').drop(j)[:30]\n        except:\n            N_k = N[j].sort_values(ascending=False, kind='heapsort')[:30]\n        weighted_rating = N_k*ratings_p.loc[i, N_k.index]\n        knn_pred.loc[i, j] = weighted_rating.sum()\/N_k.sum()\n\nknn_pred.iloc[:10, :10]","29fd0bb6":"score_knn = abs(np.array(ratings_p) - knn_pred)\nscore_2_knn = score_knn ** 2\nprint('RMSE: {:.4f}'.format(np.sqrt(score_2_knn.stack().mean())))\nprint('MAE: {:.4f}'.format(score_knn.stack().mean()))","8acad6f6":"rmse_knn = []\nmae_knn = []\nfold = 0\nprint('Evaluating RMSE, MAE of the Baseline_XGB Model. \\n')\nprint('-'*12)\nfor train_index, test_index in kfolds.split(ratings):\n    train = ratings.copy()\n    test = ratings.copy()\n    train['rating'].iloc[test_index] = np.NaN\n    test['rating'].iloc[train_index] = np.NaN\n    train_movie_summary = train.groupby('movie_id')['rating'].agg(['count', 'mean', 'std'])\n    train_user_summary = train.groupby('user_id')['rating'].agg(['count', 'mean', 'std'])\n    train_p = pd.pivot_table(train, values='rating', index='user_id', columns='movie_id', dropna=False)\n    test_p = pd.pivot_table(test, values='rating', index='user_id', columns='movie_id', dropna=False)\n    knn_pred = ratings_p.copy()\n    for i in ratings_p.index:\n        N = sim.loc[train[train['user_id'] == i].index]\n        for j in ratings_p.columns:\n            try:\n                N_k = N[j].sort_values(ascending=False, kind='heapsort').drop(j)[:30]\n            except:\n                N_k = N[j].sort_values(ascending=False, kind='heapsort')[:30]\n            weighted_rating = N_k*train_p.loc[i, N_k.index]\n            knn_pred.loc[i, j] = weighted_rating.sum()\/N_k.sum()\n    score = abs(np.array(test_p) - knn_pred)\n    score_2 = score ** 2\n    rmse_knn += [np.sqrt(score_2.stack().mean())]\n    mae_knn += [score.stack().mean()]\n    fold += 1\n    print('Fold', fold)\n    print('RMSE: {:.4f}'.format(np.sqrt(score_2.stack().mean())))\n    print('MAE: {:.4f}'.format(score.stack().mean()))\n    print('-'*12)\nprint('-'*12)\nprint('Mean RMSE: {:.4f}'.format(np.mean(rmse_knn)))\nprint('Mean MAE: {:.4f}'.format(np.mean(mae_knn)))\nprint('-'*12)\nprint('-'*12)","5e5617fd":"knn_plus_pred = ratings_p.copy()\nfor i in ratings_p.index:\n    N = sim.loc[ratings[ratings['user_id'] == i].index]\n    for j in ratings_p.columns:\n        try:\n            N_k = N[j].sort_values(ascending=False, kind='heapsort').drop(j)[:30]\n        except:\n            N_k = N[j].sort_values(ascending=False, kind='heapsort')[:30]\n        weighted_rating = N_k*(ratings_p.loc[i, N_k.index] - ratings_movie_summary.loc[N_k.index, 'mean'])\/ ratings_movie_summary.loc[N_k.index, 'std']\n        knn_plus_pred.loc[i, j] = weighted_rating.sum()\/N_k.sum() * ratings_movie_summary.loc[j, 'std'] + ratings_movie_summary.loc[j, 'mean']\n\nknn_plus_pred.iloc[:10, :10]","f29f7392":"score_knn_plus = abs(np.array(ratings_p) - knn_plus_pred)\nscore_2_knn_plus = score_knn ** 2\nprint('RMSE: {:.4f}'.format(np.sqrt(score_2_knn_plus.stack().mean())))\nprint('MAE: {:.4f}'.format(score_knn_plus.stack().mean()))","1ef7ad17":"rmse_knn_plus = []\nmae_knn_plus = []\nfold = 0\nprint('Evaluating RMSE, MAE of the Baseline_XGB Model. \\n')\nprint('-'*12)\nfor train_index, test_index in kfolds.split(ratings):\n    train = ratings.copy()\n    test = ratings.copy()\n    train['rating'].iloc[test_index] = np.NaN\n    test['rating'].iloc[train_index] = np.NaN\n    train_movie_summary = train.groupby('movie_id')['rating'].agg(['count', 'mean', 'std'])\n    train_user_summary = train.groupby('user_id')['rating'].agg(['count', 'mean', 'std'])\n    train_p = pd.pivot_table(train, values='rating', index='user_id', columns='movie_id', dropna=False)\n    test_p = pd.pivot_table(test, values='rating', index='user_id', columns='movie_id', dropna=False)\n    knn_plus_pred = ratings_p.copy()\n    for i in ratings_p.index:\n        if i % 100 == 0:\n            print(i)\n        N = sim.loc[train[train['user_id'] == i].index]\n        for j in ratings_p.columns:\n            try:\n                N_k = N[j].sort_values(ascending=False).drop(j)[:30]\n            except:\n                N_k = N[j].sort_values(ascending=False)[:30]\n            weighted_rating = N_k*(train_p.loc[i, N_k.index] - train_movie_summary.loc[N_k.index, 'mean'])\/ train_movie_summary.loc[N_k.index, 'std']\n            knn_plus_pred.loc[i, j] = weighted_rating.sum()\/N_k.sum() * train_movie_summary.loc[j, 'std'] + train_movie_summary.loc[j, 'mean']\n    score = abs(np.array(test_p) - knn_plus_pred)\n    score_2 = score ** 2\n    rmse_knn_plus += [np.sqrt(score_2.stack().mean())]\n    mae_knn_plus += [score.stack().mean()]\n    fold += 1\n    print('Fold', fold)\n    print('RMSE: {:.4f}'.format(np.sqrt(score_2.stack().mean())))\n    print('MAE: {:.4f}'.format(score.stack().mean()))\n    print('-'*12)\nprint('-'*12)\nprint('Mean RMSE: {:.4f}'.format(np.mean(rmse_knn_plus)))\nprint('Mean MAE: {:.4f}'.format(np.mean(mae_knn_plus)))\nprint('-'*12)\nprint('-'*12)","2a67d196":"from datetime import datetime\n\nr_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\nratings = pd.read_csv('..\/input\/ml-100k\/u.data', sep='\\t', names=r_cols, encoding='latin-1')\nratings['unix_timestamp'] = ratings['unix_timestamp'].apply(datetime.fromtimestamp)\nratings.columns = ['user_id', 'movie_id', 'rating', 'time']\nratings.head(10)","8f897d8d":"lr = 0.005 # Learning Rate(Gamma)\nreg = 0.1 # regularization(Lambda)\nn = 100 # Number of iterations\nk = 70 # Number of factors","f3c52ee6":"u = ratings_p.shape[0] # Number of users\ni = ratings_p.shape[1] # Number of movies\n\nbu = np.zeros(u) # A series of bias constant, one for each user\nbi = np.zeros(i) # A series of bias constant, one for each movie\npu = np.random.mtrand._rand.normal(0, 0.1, (u, k)) # A series of vectors(rows) with length k, one row for each user\nqi = np.random.mtrand._rand.normal(0, 0.1, (i, k)) # A series of vectors(rows) with length k, one row for each movie","0b12df01":"ratings_array = np.array(ratings.drop('time', axis=1))\nfor i in range(n): # For each iteration\n    for row in ratings_array: # For each instance\n        user_id = row[0]-1\n        movie_id = row[1]-1\n        rating = row[2]\n        mult = 0\n        for j in range(k): # For each factor\n            mult += qi[movie_id, j] * pu[user_id, j] # Compute the dot product(interaction q_i^Tp_u)\n        error = rating - (mean + bu[user_id] + bi[movie_id] + mult) # error = rating - (prediction)\n        bu[user_id] += lr * (error - reg * bu[user_id]) # Update Bias on user\n        bi[movie_id] += lr * (error - reg * bi[movie_id]) # Update Bias on movie\n        for k in range(k): # Again for each factor\n            pu_k = pu[user_id, k]\n            qi_k = qi[movie_id, k]\n            pu[user_id, k] += 0.005 * (error * qi_k - 0.1 * pu_k) # Update this factor using the final error\n            qi[movie_id, k] += 0.005 * (error * pu_k - 0.1 * qi_k) # Update this factor","c706a3c3":"U = ratings['user_id'].unique()\nI = ratings['movie_id'].unique()\ndef svd_pred(user_id, movie_id):\n    pred = mean\n    if user_id in U:\n        pred += bu[user_id]\n    if movie_id in I:\n        pred += bi[movie_id]\n    if (user_id in U) and (movie_id in I):\n        pred += np.dot(qi[movie_id], pu[user_id])\n    return pred\n\nsvd_pred_ratings = np.array(ratings)\nfor row in svd_pred_ratings:\n    row[3] = svd_pred(row[0]-1, row[1]-1)\nsvd_pred_ratings = pd.DataFrame(svd_pred_ratings, columns=['user_id', 'movie_id', 'rating', 'pred'])\n\nprint('RMSE: {:.4f}'.format(np.sqrt(((svd_pred_ratings['pred'] - svd_pred_ratings['rating']) ** 2).mean())))\nprint('MAE: {:.4f}'.format(abs(svd_pred_ratings['pred'] - svd_pred_ratings['rating']).mean()))","460dada5":"rmse_svd = []\nmae_svd = []\nfold = 0\nprint('Evaluating RMSE, MAE of the SVD Model. \\n')\nprint('-'*12)\nfor train_index, test_index in kfolds.split(ratings):\n    \n    u = ratings_p.shape[0] # Number of users\n    i = ratings_p.shape[1] # Number of movies\n    bu = np.zeros(u) # A series of bias constant, one for each user\n    bi = np.zeros(i) # A series of bias constant, one for each movie\n    pu = np.random.mtrand._rand.normal(0, 0.1, (u, k)) # A series of vectors(rows) with length k, one row for each user\n    qi = np.random.mtrand._rand.normal(0, 0.1, (i, k)) # A series of vectors(rows) with length k, one row for each movie\n    \n    ratings_array = ratings.iloc[train_index]\n    ratings_array = np.array(ratings_array.drop('time', axis=1))\n    for i in range(n): # For each iteration\n        for row in ratings_array: # For each instance\n            user_id = row[0]-1\n            movie_id = row[1]-1\n            rating = row[2]\n            mult = 0\n            for j in range(k): # For each factor\n                mult += qi[movie_id, j] * pu[user_id, j] # Compute the dot product(interaction q_i^Tp_u)\n            error = rating - (mean + bu[user_id] + bi[movie_id] + mult) # error = rating - (prediction)\n            bu[user_id] += lr * (error - reg * bu[user_id]) # Update Bias on user\n            bi[movie_id] += lr * (error - reg * bi[movie_id]) # Update Bias on movie\n            for k in range(k): # Again for each factor\n                pu_k = pu[user_id, k]\n                qi_k = qi[movie_id, k]\n                pu[user_id, k] += 0.005 * (error * qi_k - 0.1 * pu_k) # Update this factor using the final error\n                qi[movie_id, k] += 0.005 * (error * pu_k - 0.1 * qi_k) # Update this factor\n    \n    svd_pred_ratings = np.array(ratings.iloc[test_index])\n    for row in svd_pred_ratings:\n        row[3] = svd_pred(row[0]-1, row[1]-1)\n    svd_pred_ratings = pd.DataFrame(svd_pred_ratings, columns=['user_id', 'movie_id', 'rating', 'pred'])\n    \n    score = abs(svd_pred_ratings['pred'] - svd_pred_ratings['rating'])\n    score_2 = score ** 2\n    rmse_svd += [np.sqrt(score_2.mean())]\n    mae_svd += [score.mean()]\n    fold += 1\n    print('Fold', fold)\n    print('RMSE: {:.4f}'.format(np.sqrt(score_2.mean())))\n    print('MAE: {:.4f}'.format(score.mean()))\n    print('-'*12)\nprint('-'*12)\nprint('Mean RMSE: {:.4f}'.format(np.mean(rmse_svd)))\nprint('Mean MAE: {:.4f}'.format(np.mean(mae_svd)))\nprint('-'*12)\nprint('-'*12)","5c4437f0":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\n#occupation = {'none': 0, 'administrator': 1, 'artist': 2, 'doctor': 3, 'educator': 4, 'engineer': 5, 'entertainment': 6,\\\n#              'executive': 7, 'healthcare': 8, 'homemaker': 9, 'lawyer': 10, 'librarian': 11, 'marketing': 12,\\\n#              'programmer': 13, 'salesman': 14, 'scientist': 15, 'student': 16, 'technician': 17, 'writer': 18,\\\n#              'retired': 19, 'other': 20}\ndf = ratings_p.stack(dropna=False).reset_index()\ndf.columns = ['user_id', 'movie_id', 'rating']\ndf = df.merge(users, on='user_id')\ndf = df.merge(movies, on='movie_id')\ndf['sex'] = df['sex'].replace(['F', 'M'], [1, 0])\n#df['occupation'] = df['occupation'].replace(occupation)\ndf.drop(['release_date', 'video_release_date', 'imdb_url', 'title', 'zip_code'], axis=1, inplace=True)\ndf_train = df.dropna()\ndf.head(10)","351e93de":"rmse_reg = []\nmae_reg = []\ni = 0\nprint('Evaluating RMSE, MAE of the XGB_Reg Model. \\n')\nprint('-'*12)\nfor train_index, test_index in kfolds.split(ratings):\n    X_train = df_train.drop('rating', axis=1).iloc[train_index]\n    y_train = df_train['rating'].iloc[train_index]\n    X_test = df_train.drop('rating', axis=1).iloc[test_index]\n    y_test = df_train['rating'].iloc[test_index]\n    xgb = XGBRegressor(learning_rate=0.1, max_depth=10, min_child_weight=10, gamma=0.03).fit(X_train, y_train)\n    y_pred = xgb.predict(X_test)\n    score = abs(y_test - y_pred)\n    score_2 = score**2\n    rmse_reg += [np.sqrt(np.mean(score_2))]\n    mae_reg += [np.mean(score)]\n    i += 1\n    print('Fold', i)\n    print('RMSE: {:.4f}'.format(np.sqrt(np.mean(score_2))))\n    print('MAE: {:.4f}'.format(np.mean(score)))\n    print('-'*12)\nprint('-'*12)\nprint('Mean RMSE: {:.4f}'.format(np.mean(rmse_reg)))\nprint('Mean MAE: {:.4f}'.format(np.mean(mae_reg)))\nprint('-'*12)\nprint('-'*12)","25012102":"baseline_results = {'Baseline': [np.mean(rmse), np.mean(mae)], 'Baseline_Plus': [np.mean(rmse_plus), np.mean(mae_plus)],\\\n                    'SVM': [np.mean(rmse_svm), np.mean(mae_svm)], 'XGradientBoosting': [np.mean(rmse_xgb), np.mean(mae_xgb)],\\\n                    'kNN': [np.mean(rmse_knn), np.mean(mae_knn)], 'kNN_Plus': [np.mean(rmse_knn_plus), np.mean(mae_knn_plus)],\\ \n                    'SVD': [np.mean(rmse_svd), np.mean(mae_svd)], 'Supervised_Learning': [np.mean(rmse_reg), np.mean(mae_reg)]}\nbaseline_results = pd.DataFrame(baseline_results, index=['RMSE', 'MAE']).T\nbaseline_results","05b1ed24":"import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(10, 6))\nax = plt.subplot(111)\nax.set_axisbelow(True)\nplt.bar(np.arange(1, 3*baseline_results.shape[0], 3), baseline_results['RMSE']-0.7, width=1, label='RMSE')\nplt.bar(np.arange(2, 3*baseline_results.shape[0], 3), baseline_results['MAE']-0.7, width=1, label='MAE')\nplt.xticks(np.arange(1.5, 3*baseline_results.shape[0], 3), baseline_results.index)\nplt.yticks(np.arange(0, 0.5, 0.1), [0.7, 0.8, 0.9, 1.0, 1.1])\nplt.grid()\nplt.legend()\nplt.show()","c4e32582":"### BUG. Need Fix. ###\n\n#df = df.merge(ratings_p, left_on='user_id', right_index=True)\n#df = df.merge(ratings_p.T, left_on='movie_id', right_index=True)\n#df.head(10)","2a7572a6":"#df_train = df_train.merge(ratings_p, left_on='user_id', right_index=True)\n#df_train = df_train.merge(ratings_p.T, left_on='movie_id', right_index=True)\n#df_train.head(10)","64357aec":"#for i in range(df_train.shape[0]):\n#    if i % 1000 == 0:\n#        print(i)\n#    row = df_train.iloc[i]\n#    df_train.iloc[i][str(row[1])+'_x'] = np.NaN\n#    df_train.iloc[i][str(row[0])+'_y'] = np.NaN","6b61d656":"#X_train, X_test, y_train, y_test = train_test_split(df_train.drop(['rating'], axis=1), df_train['rating'], random_state = 0)\n#xgb = XGBRegressor(learning_rate=0.1, max_depth=10, min_child_weight=10, gamma=0.03).fit(X_train, y_train)\n#y_pred = xgb.predict(X_test)\n#score = abs(y_test - y_pred)\n#score_2 = score**2\n#print('RMSE: {:.4f}'.format(np.sqrt(np.mean(score_2))))\n#print('MAE: {:.4f}'.format(np.mean(score)))\n\n# RMSE: 0.7401\n# MAE: 0.5674","912f493c":"#pred_196 = df[df['user_id']==196]\n#pred_196 = pred_196.merge(ratings_p, left_on='user_id', right_index=True)\n#pred_196 = pred_196.merge(ratings_p.T, left_on='movie_id', right_index=True)\n#pred_196.head(10)","dfad4bf1":"#xgb = XGBRegressor(learning_rate=0.1, max_depth=10, min_child_weight=10, gamma=0.03)\\\n#      .fit(df_train.drop(['rating'], axis=1), df_train['rating'])\n#pred_196['rating'] = xgb.predict(pred_196.drop('rating', axis=1))\n#user_196_reg = movies[['movie_id', 'title', 'release_date']]\n#user_196_reg['Estimate_Score'] = np.array(pred_196['rating'])\n#user_196_reg.drop('movie_id', axis=1, inplace=True)\n#user_196_reg = user_196_reg.sort_values('Estimate_Score', ascending=False)\n#print(user_196_reg.head(10))","507a8d70":"#rmse_reg_plus = []\n#mae_reg_plus = []\n#i = 0\n#print('Evaluating RMSE, MAE of the XGB_Reg_Plus Model. \\n')\n#print('-'*12)\n#for train_index, test_index in kfolds.split(ratings):\n#    X_train = df_train.drop('rating', axis=1).iloc[train_index]\n#    y_train = df_train['rating'].iloc[train_index]\n#    X_test = df_train.drop('rating', axis=1).iloc[test_index]\n#    y_test = df_train['rating'].iloc[test_index]\n#    xgb = XGBRegressor(learning_rate=0.1, max_depth=10, min_child_weight=10, gamma=0.03).fit(X_train, y_train)\n#    y_pred = xgb.predict(X_test)\n#    score = abs(y_test - y_pred)\n#    score_2 = score**2\n#    rmse_reg_plus += [np.sqrt(np.mean(score_2))]\n#    mae_reg_plus += [np.mean(score)]\n#    i += 1\n#    print('Fold', i)\n#    print('RMSE: {:.4f}'.format(np.sqrt(np.mean(score_2))))\n#    print('MAE: {:.4f}'.format(np.mean(score)))\n#    print('-'*12)\n#print('-'*12)\n#print('Mean RMSE: {:.4f}'.format(np.mean(rmse_reg_plus)))\n#print('Mean MAE: {:.4f}'.format(np.mean(mae_reg_plus)))\n#print('-'*12)\n#print('-'*12)","b85ea2eb":"Use our svm model to recommend movies for user 196.","7ccdc3d4":"The next model takes more than 6 hours to run. So I don't commit it this time. I'll move it into a seperate notebook.","bb099e45":"Train-test-split score.","2f4049e6":"- **Improvement on Gradient Boosting Model (?)**\n\nIn the previous model, we use rating as the target variable, therefore the model knows nothing about what ratings that the movie has received, and what ratings the user has given. Now we might want to include all related ratings, both from the same user and for the same movie, as features in our model.\n\nThis model takes a much longer time than others.\n\nNotice that two of the slots on each row in these new features will actually contain the target value! However, I right now have no clue that this will result in data leakage since the model has no idea where these hidden correct ratings locate.\n\nIt makes sense that this model will beat all other models, since it contains more information like the age and gender of the user, genre of the movie, etc. However, it is still not safe to say that we are free of data leakage here. So I leave a question mark here.","7009cc87":"- **Baseline Model with SVM\/Gradient Boosting**\n\nWe can improve this model even more, by applying SVM regressor or Gradient Boosting on each approximation, instead of just using z-score.","a12aa95b":"- **Supervised Learning Model**\n\nGradient Boosting as supervised learning.\n\nI take each user-movie combination as one instance, and hence take full use of the features from the user and movie.","d6739a3b":"We create a pivot table for ratings and store the total mean and standard deviation values.","15d645fd":"- **K-Nearest Neighbor (kNN) Model**\n\nWe can treat the **Pearsons' R Correlation** between movies as the distance, and using these distances to build a **K-Nearest Neighbor model**.\n\nNotations:\n\n$r_{ui}$ : User u's rating on movie i.\n\n$\\hat{r}_{ui}$ : Prediction about user u's rating on movie i.\n\n$N^k_u(i)$ : k nearest neighbors of movie i, that are rated by user u.\n\nThen we can have our kNN model as:\n\n$$\\hat{r}_{ui} = \\frac{\\sum_{j \\in N^k_u(i)} corr(i, j) * r_{uj}}{\\sum_{j \\in N^k_u(i)} corr(i, j)}$$","87df1eb1":"- **SVD**\n\nAccording to Matrix Factorization Techniques for Recommender Systems, written by Yehoda Koren, Robert Bell, and Chris Volinsky, Our prediction looks like this:\n\n$$\\hat{r}_{ui} = \\mu + b_u + b_i + q_i^Tp_u$$\n\nWhere $q_i$ and $p_u$ are the corresponding factor vectors for movie $i$ and user $u$. So $q_i ^ T p_u$ term is the interaction factor between movie $i$ and user $u$.\n\nAnd $\\mu + b_u + b_i$ is the bias term. $b_u$ and $b_i$ represents the factor that depends solely on user $u$ or on movie $i$.\n\nHence, we want to minimize the following:\n\n$$\\min_{p^*, q^*, b^*} \\sum_{(u, i) \\in R} (r_{ui} - \\hat{r}_{ui})^2 + \\lambda (\\| p_u \\|^2 + \\| q_i \\|^2 + b_u ^2 + b_i ^2)$$\n\nAnd we achive this by doing stochastic gradient descent on $p_u, q_i, b_u, $ and $b_i$. Namely:\n\n$$b_u  \\leftarrow b_u + \\gamma (e_{ui} - \\lambda b_u)$$\n$$b_i  \\leftarrow b_i + \\gamma (e_{ui} - \\lambda b_i)$$\n$$p_u  \\leftarrow p_u + \\gamma (e_{ui} \\cdot q_i - \\lambda p_u)$$\n$$q_i  \\leftarrow q_i + \\gamma (e_{ui} \\cdot p_u - \\lambda q_i)$$\n\nWhere $\\gamma$ is the learning rate, and $\\lambda$ is the regularization, and we want to run this gradient descent by $n$ times. (So far we set $\\gamma = 0.005$, $\\lambda = 0.1$, and $n=100$. And we want number of factors as $k=70$.)","adea4732":"Cross-Validation for SVD","836f5613":"Here is the recommendation to user 196.","7510f759":"- SVM","a7242e40":"**Notations:**\n\n$\\mu_i$ : The mean of all ratings received by movie i.\n\n$\\mu_u$ : The mean of all ratings from user u.\n\n$\\mu$ : The mean of all ratings.\n\n$\\sigma_i$ : The standard deviation of all ratings received by movie i.\n\n$\\sigma_u$ : The standard deviation of all ratings from user u.\n\n$r_{ui}$ : User u's rating on movie i.\n\n$\\hat{r}_{ui}$ : Prediction about user u's rating on movie i.\n\n$N^k_u(i)$ : k nearest neighbors of movie i, that are rated by user u.","ae6036ea":"For each user, we count how many ratings he or she gives, and the mean and standard deviation as well.","a99a64a3":"Cross-Validation","23eed63b":"- **Pearsons'R Correlation Model**\n\nWe might also want to recommend movies just for a specific movie. Like the recommendation list showed on the webpage of a specific movie.\n\nHere we recommend new movies based on the Pearsons'R correlation between movies.","18b4c26f":"So far we will only use the movie title from this DataFrame. We may need the types of the movie later in our model.","5a508533":"Here are the Cross Validation RMSE and MAE scores for all models in this notebook","7d9dfeb9":" Import packages and Data","14ff9790":"Cross-Validation","80163655":"- **Baseline Model**\n\nIn the first baseline model, we predict the rating from a specific user on a specific movie, just by the average rating that a movie receives, with adjustment by how this user's average rating compared with the total average.\n\n$$\\hat{r}_{ui} = \\mu_u + \\mu_i - \\mu$$","d4c29ff3":"Since we don't have train-test-split in our prediction, we are actually using the mean of the data to predict every single data. Therefore, the score might be biased because of data leakage. So we do cross-validation on the model.","e3d4b609":"Gradient Boosting","f243f53e":"Use our model to recommond movie for user 196.","9997128a":"- **kNN_Plus Model**\n\nNow we can improve our KNN model just by the same trick we used on our baseline model: adjust by the z-score.\n\nNotation:\n\n$\\mu_i$ : The mean of all ratings received by movie i.\n\n$\\sigma_i$ : The standard deviation of all ratings received by movie i.\n\n$$\\hat{r}_{ui} = \\mu_i + \\sigma_i * \\frac{\\sum_{j \\in N^k_u(i)} corr(i, j) *( r_{uj} - \\mu_j) \/ \\sigma_j}{\\sum_{j \\in N^k_u(i)} corr(i, j)}$$","b308c5ca":"Cross-Validation on kNN_Plus","cdb9b8c4":"Cross-Validation","e4404079":"Here is all movies rated by user 196.","e25c83eb":"Here we can see how ratings distributed.","5e2eeb9b":"Use our baseline model to recommend movies for user 196.","e75b225a":"Cross-Validation on kNN","8afcec41":"Here is the cross-validation score for our second model","e1427007":"For each movie we count how many ratings it got, and what's the mean and standard deviation.","10703fb5":"- **Baseline_Plus Model**\n\nIn the second model, we want to do it slightly better using z-score.\n\n$$\\hat{r}_{ui} = \\mu_u + \\sigma_u * \\frac{(\\mu_i - \\mu)}{\\sigma}$$"}}