{"cell_type":{"91475659":"code","b7fe8bb9":"code","7de9a918":"code","2ce66341":"code","94d25d83":"code","57a8fc2a":"code","0937d33b":"code","842c398e":"code","f0af198f":"code","4f7845ee":"code","72510693":"code","4f815953":"code","ed4b3ece":"code","72cedd16":"code","47831661":"code","1fcfd385":"code","f7542294":"code","2ec87fd0":"code","efe266df":"code","8490861d":"markdown","a0f9fcb5":"markdown","a2b02dfb":"markdown","ab07ae51":"markdown","ef09130e":"markdown"},"source":{"91475659":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b7fe8bb9":"import tensorflow as tf\nimport keras\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy import stats","7de9a918":"print(\"TF version: \", tf.__version__)\nprint(\"Keras Version: \", keras.__version__)","2ce66341":"dataset = pd.read_csv('\/kaggle\/input\/fake-news\/train.csv')\ndataset_submission = pd.read_csv('\/kaggle\/input\/fake-news\/test.csv')","94d25d83":"dataset","57a8fc2a":"dataset_submission","0937d33b":"print(dataset.isnull().sum())\nprint()\nprint('-----------------')\nprint()\nprint(dataset_submission.isnull().sum())\n","842c398e":"dataset['title'].ffill(inplace=True)\ndataset_submission['title'].ffill(inplace=True)\ntitle_list_dataset = list(dataset['title'])\ntitle_list_dataset_submission = list(dataset_submission['title'])\nprint(len(title_list_dataset), dataset.shape)\nprint(len(title_list_dataset_submission), dataset_submission.shape)","f0af198f":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport string\nimport re\n\ndef clean_data(data_list):\n    corpus = []\n    for sentence in data_list:\n        # Lower\n        title = sentence.lower()\n        # Remove punct\n        title = re.sub('[^a-zA-Z]', ' ', title)\n        # stem\n        title = title.split()\n        ps = PorterStemmer()\n        all_stopwords = stopwords.words('english')\n        title = [ps.stem(word) for word in title if not word in set(all_stopwords)]\n        title = ' '.join(title)\n        \n        corpus.append(title)\n        \n    return corpus\n\n## Apply on them\ncorpus_train = clean_data(title_list_dataset)\ncorpus_submit = clean_data(title_list_dataset_submission)\n\nprint(\"Corpus train length: \", len(corpus_train))\nprint(\"Corpus submission length: \", len(corpus_submit))","4f7845ee":"print(stats.mode(dataset['title'].str.len()))\nprint(stats.mode(dataset_submission['title'].str.len()))","72510693":"## Making the tokenizer\ntokenizer = keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(corpus_train + corpus_submit)\n\n## SPlit the data into training and testing set\nfrom sklearn.model_selection import train_test_split\ntrain_data_x,validate_x, train_data_y, validate_y = train_test_split(corpus_train, list(dataset['label']), test_size=0.2)\n\nprint(\"train_data len \", len(train_data_x))\nprint(\"train_data y len \", len(train_data_y))\nprint(\"Validate data len \", len(validate_x))\nprint(\"validate_data y len \", len(validate_y))","4f815953":"maxlen = 50\n\n## Apply tokenization and padding\nX_train = keras.preprocessing.sequence.pad_sequences(\n            tokenizer.texts_to_sequences(train_data_x),\n            maxlen=maxlen,\n            padding='post'\n        )\ny_train = np.array(train_data_y)\n\nX_validate = keras.preprocessing.sequence.pad_sequences(\n              tokenizer.texts_to_sequences(validate_x),\n              maxlen=maxlen,\n              padding='post'\n        )\ny_validate = np.array(validate_y)\n\nX_submit = keras.preprocessing.sequence.pad_sequences(\n            tokenizer.texts_to_sequences(corpus_submit),\n            maxlen=maxlen,\n            padding='post'\n        )","ed4b3ece":"print(\"X_train shape: \", X_train.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"X_test shape: \", X_validate.shape)\nprint(\"y_test shape: \", y_validate.shape)\nprint(\"X_train[0]: \", X_train[0])\nprint(\"y_train[0]: \", y_train[0])\nprint(\"X_test[0]: \", X_validate[0])\nprint(\"y_test[0]: \", y_validate[0])","72cedd16":"rnn = keras.models.Sequential()\nrnn.add(keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32, input_length=maxlen))\nrnn.add(keras.layers.LSTM(units=128, return_sequences=True))\nrnn.add(keras.layers.Dropout(rate=0.5))\nrnn.add(keras.layers.LSTM(units=64))\nrnn.add(keras.layers.Dropout(rate=0.5))\nrnn.add(keras.layers.Dense(units=1, activation='sigmoid'))\n\nrnn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])","47831661":"rnn.summary()","1fcfd385":"trained_obj = rnn.fit(X_train, y_train, epochs=15, batch_size=64, validation_data=(X_validate, y_validate))","f7542294":"plt.plot(trained_obj.history['loss'])\nplt.plot(trained_obj.history['val_loss'])\nplt.plot(trained_obj.history['accuracy'])\nplt.plot(trained_obj.history['val_accuracy'])\nplt.legend(['loss', 'val_loss', 'accuracy', 'val_accuracy'])","2ec87fd0":"pred_submission = np.array(rnn.predict(X_submit) >= 0.5, dtype='int32')\nsubmit_frame = pd.DataFrame({'id':dataset_submission['id'] , 'label': pred_submission.reshape(1, -1)[0]})\nsubmit_frame.set_index('id', inplace=True)\nsubmit_frame.to_csv('Submission.csv')","efe266df":"submit_frame","8490861d":"## Make the Model","a0f9fcb5":"### 1)  Remove punctuation of the title \n### 2) Fill the null values\n### 3) convert to lowercase\n\n## We will use only titles and ID","a2b02dfb":"## Import Dataset","ab07ae51":"## Predicting Result","ef09130e":"### Splitting the Data into training and validation\n### tokenization\n### Padding"}}