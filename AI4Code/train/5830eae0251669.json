{"cell_type":{"bf0bd3c4":"code","a643364d":"code","42659b33":"code","aa688b44":"code","c79c1a41":"code","fe4b2099":"code","b0aa323c":"code","48fb1ce3":"code","e36335f2":"code","b3ea49ef":"code","867e60c3":"code","32124085":"code","ae786145":"code","a12340f6":"code","e4b74536":"markdown","f11600e4":"markdown","be3e01f4":"markdown","76af2457":"markdown"},"source":{"bf0bd3c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a643364d":"import tensorflow as tf","42659b33":"\n#libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set() # setting seaborn default for plots\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils import np_utils\nfrom keras.datasets import mnist\n\n# for Convolutional Neural Network (CNN) model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.optimizers import Adam\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\nfrom keras.layers.advanced_activations import LeakyReLU \nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import LearningRateScheduler\n\nfrom keras import backend as K\nK.set_image_dim_ordering('th')","aa688b44":"train = pd.read_csv('..\/input\/train.csv')\nprint (train.shape)\ntrain.head()","c79c1a41":"test = pd.read_csv('..\/input\/test.csv')\nprint(test.shape)\ntest.head()","fe4b2099":"# Separating the labels from training dataset and making it as x_label\ny_train = train['label']\nx_train = train.drop(labels=['label'],axis=1)\nx_test = test\n","b0aa323c":"# Set values of the Data\nx_train = x_train.values.astype('float32') # pixel values of all images in train set\ny_train = y_train.values.astype('int32') # labels of all images\nx_test = test.values.astype('float32') # pixel values of all images in test set","48fb1ce3":"# fix random seed for reproducibility\nrandom_seed = 7\nnp.random.seed(random_seed)","e36335f2":"# one hot encode outputs'\ny_train = np_utils.to_categorical(y_train)\nnum_classes = y_train.shape[1]\nnum_classes","b3ea49ef":"# Reshaping the Image for CNN 2-dimesional input in [samples][pixels][width][height]\nx_train = x_train.reshape(x_train.shape[0], 1, 28, 28).astype('float32')\nx_test = x_test.reshape(x_test.shape[0], 1, 28, 28).astype('float32')\nnum_pixels = x_train.shape[1]\nprint (num_pixels, x_train.shape, x_test.shape)","867e60c3":"nn = 10\nmodel = [0]*nn\n\nfor j in range(nn):\n    model[j] = Sequential()\n    model[j].add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(1, 28, 28), activation='relu',data_format='channels_first'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Dropout(0.4))\n    \n    model[j].add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Dropout(0.4))\n    \n    model[j].add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Flatten())\n    model[j].add(Dropout(0.4))\n    model[j].add(Dense(10, activation='softmax'))\n    \n    # Compile model\n    model[j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","32124085":"# With data augmentation to prevent overfitting\ndatagen = ImageDataGenerator(\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)","ae786145":"# DECREASE LEARNING RATE EACH EPOCH\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\nhistory = [0] * nn\nepochs = 50\n\n# Fit the model\nfor j in range(nn):\n    x_train2, x_val, y_train2, y_val = train_test_split(x_train, y_train, test_size = 0.10, random_state=random_seed)\n    history[j] = model[j].fit_generator(datagen.flow(x_train2,y_train2, batch_size=64),\n                              epochs = epochs, validation_data = (x_val,y_val),\n                              verbose = 0, steps_per_epoch=(len(x_train)\/\/64),validation_steps=(len(x_val)\/\/64),callbacks=[annealer])\n    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))","a12340f6":"results = np.zeros( (x_test.shape[0],10) )\nfor j in range(nn):\n    results = results + model[j].predict(x_test)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"ENSEMBLE.csv\",index=False)","e4b74536":"## Train 10 CNNS\nEverytime before the training, it divides the dataset into training and validation","f11600e4":"# Ensemble\nUsing ensemble of cnn for training and prediction. Using 10 CNNs.\nModel idea and code from [here](https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist#)","be3e01f4":"**\nConverting Output into one hot code**\n\nA one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1 and as this is a multi classification problem so we can convert the output class values into one-hot format which is simply a binary matrix, i.e.\n\nvalue 0 will be converted to one-hot format as [1, 0, 0, 0, 0, 0, 0, 0, 0]\n\nvalue 1 will be converted to one-hot format as [0, 1, 0, 0, 0, 0, 0, 0, 0] etc","76af2457":"## Ensemble Predictions and submitting the result\nTill now I got 0.997 accuracy with just 5 models."}}