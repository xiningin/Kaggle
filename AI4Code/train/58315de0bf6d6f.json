{"cell_type":{"b420d64a":"code","6730f2fe":"code","a82aa5d2":"code","afc7a92b":"code","09b6a73b":"code","b01de31a":"code","1ad2bcba":"code","f2177d15":"code","60081bc5":"code","dfab8305":"code","8482e950":"code","86e50aa3":"code","adebc5a6":"code","b33940b2":"code","e6043fe9":"code","4aaa5581":"code","08b52009":"code","17f82a7f":"code","aef5864e":"code","328add8a":"code","749b8274":"code","309c6b81":"code","57cd31c1":"code","5439f12b":"code","7498c339":"code","8b7d1376":"code","7369c1fe":"code","c0807d6c":"code","303a5645":"code","1d7471e7":"code","96dcff5f":"code","311bb3d1":"code","b0a7fb76":"code","54358927":"code","b7a02049":"code","d8511528":"code","515fec80":"code","c9fc360e":"code","9797d805":"code","64eb764a":"code","1cff1a35":"code","0d544b85":"code","2c4aad87":"code","8a418b64":"code","fc0ba67d":"code","2c0e5b23":"code","4f12be4f":"code","d46f7a20":"code","346cd848":"code","fbbeb91d":"code","94b56ad1":"code","9970d011":"code","6dee4f47":"code","478f3931":"code","26573cf0":"code","43878ed6":"code","111b36ed":"code","d34c9e3e":"code","3512cec8":"markdown","b74fe54f":"markdown","4c33bf36":"markdown","4f983777":"markdown","2b209423":"markdown","e7e45218":"markdown","46911f34":"markdown","48de255b":"markdown","af521cc3":"markdown","0e9df1ce":"markdown","c8759014":"markdown","89a6f6a8":"markdown","8f516c2c":"markdown","45ec2698":"markdown","f7375b80":"markdown","9b36d0b7":"markdown","9106ba66":"markdown","096a86a5":"markdown","f0292f7e":"markdown","e3284d1c":"markdown","ddf90e66":"markdown","06b96e2b":"markdown","5ac94fe5":"markdown","1e0b64e6":"markdown","b66fea6c":"markdown","5b8c0fea":"markdown","4523fbaa":"markdown","a3c0ed2a":"markdown","51c384b6":"markdown","178373a6":"markdown","b321c0ed":"markdown","70b30fa5":"markdown","6656fe25":"markdown","556814cf":"markdown","700f4114":"markdown","80c97ce2":"markdown","aa4c0083":"markdown","ec8c1c5a":"markdown","d0bdb32f":"markdown","1193d22d":"markdown","7240fa48":"markdown","ae6d1c0e":"markdown","75130d68":"markdown","868c249a":"markdown","766d9175":"markdown","fddc9b0b":"markdown","af8f33d3":"markdown","0cdb89e9":"markdown","a15e1f97":"markdown","85f3aa50":"markdown","b78a7a7f":"markdown","6146e101":"markdown","d7cd5512":"markdown","869148ef":"markdown","3dd69d1a":"markdown","37e51cd0":"markdown","fa7757a7":"markdown","348c6b13":"markdown","32df7eaa":"markdown","598434a1":"markdown","13657d84":"markdown","d10644ae":"markdown","381d9cdb":"markdown","bad4a60c":"markdown","0f21f9c8":"markdown","9ea2c864":"markdown","532a54be":"markdown","3090c1ef":"markdown","0fd3f0b3":"markdown","2f11263d":"markdown"},"source":{"b420d64a":"import os\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom pandas.plotting import scatter_matrix\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom imblearn.over_sampling import SMOTE","6730f2fe":"PATH = !pwd\nprint(os.listdir('..\/input'))","a82aa5d2":"os_path = '..\/input\/predicting-churn-for-bank-customers'\nos_path","afc7a92b":"#Reading the data as a csv file and saving it as a pandas dataframe\ndef load_churn_data(data_path=PATH):\n    csv_path = os.path.join(data_path, \"Churn_Modelling.csv\")\n    return pd.read_csv(csv_path)","09b6a73b":"churn_df = load_churn_data(data_path=os_path)\n\n#printing the number of columns \nprint(\"Number of columns: \", len(churn_df.columns))\n\n# First 5 rows of the dataset\nchurn_df.head()","b01de31a":"# Listing the features and their data type\nchurn_df.info()","1ad2bcba":"# Viewing how many datapoints in each category of Geography attribute\nchurn_df[\"Geography\"].value_counts()\n","f2177d15":"# Viewing how many datapoints in each category of Gender attribute\nchurn_df[\"Gender\"].value_counts()\n","60081bc5":"# Descriptive statistics\nchurn_df.describe()","dfab8305":"%matplotlib inline\n#Creating histogram for numerical attributes\nchurn_df.drop(columns=['CustomerId', 'RowNumber']).hist(bins=50, figsize=(20,15))\nplt.show()","8482e950":"# Viewing how many datapoints represents customers who have churned or not in the target variable\nnumbers = churn_df[\"Exited\"].value_counts()\n\nfig, ax = plt.subplots(figsize=(10, 8))\n#creating a pie chart to visualize the customers who churned and who did not churn\nax.pie(numbers, labels=['Churned', 'Stayed with the Bank'], colors = ['green', 'red'], autopct='%2.2f%%', startangle=90)\nplt.title(\"Comparison of customers churned and stayed with bank\", size =18)\nplt.show()","86e50aa3":"%matplotlib inline\n#Creating histogram for numerical attribute (Tenure)\nchurn_df[\"Tenure\"].hist(bins=30, figsize=(10,7))\nplt.show()","adebc5a6":"# Provides train\/test indices to split data in train\/test sets.\nsplit_cond = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n#splitting the data to get train and test set according to tenure category\nfor i, j in split_cond.split(churn_df, churn_df[\"Tenure\"]):\n    strat_train_data = churn_df.loc[i]\n    strat_test_data = churn_df.loc[j]\n    \nY_strat_train_data = strat_train_data[\"Exited\"]\nY_strat_test_data = strat_test_data[\"Exited\"]\n","b33940b2":"churn = strat_train_data.copy()#creating a copy of data in order to play with it\nchurn.head()","e6043fe9":"sns.set(style=\"ticks\")\n#Creating a pairplot using seaborn library\nsns.pairplot(churn[['CreditScore','Age','EstimatedSalary','Balance','Exited']], hue=\"Exited\",height = 4.0)\nplt.show()","4aaa5581":"corr_matrix = churn.corr()#correlation among all variables\ncorr_matrix[\"Exited\"].sort_values(ascending=False)#correlation between target variable and other attributes","08b52009":"attributes = [\"Exited\", \"Age\", \"Balance\"]\n#correlation visualization among most correlated features to target variable.\nscatter_matrix(churn[attributes], figsize=(12, 8))","17f82a7f":"sns.set(style=\"darkgrid\")#setting the dark background\n\n#creating subplots\nfigure, axes = plt.subplots(1, 4, figsize=(20, 10))\ni = 0\n#A list of all categorical attributes\ny_ = [\"HasCrCard\", \"Gender\", 'Geography', 'IsActiveMember']\ntype(y_)\nfor axi in y_:\n    ax = sns.countplot(x=axi, hue=\"Exited\", data=churn, ax=axes[i])#countplot of 3 categorical\n    i=i+1\nplt.show()","aef5864e":"#Using boxplot to detect outliers\nboxplot = churn_df.boxplot(column=['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n       'IsActiveMember', 'EstimatedSalary'], figsize=(12,8))","328add8a":"#performing feature selection by removing irrelevant features\ny_train_data = churn['Exited']\ny_test_data = strat_test_data['Exited']\n\nX = churn.drop(columns = ['RowNumber', 'CustomerId', 'Surname'])#Train data\n\nX_test_data = strat_test_data.drop(columns=['RowNumber', 'CustomerId', 'Surname'])#test data\n","749b8274":"#Training set\n#Geography\nencoder_geo = LabelEncoder()#creating label encoder object\nX[\"Geography\"] = encoder_geo.fit_transform(X[\"Geography\"]).copy()#converting to numberical for geography attribute\n\n#Gender\nencoder_gender = LabelEncoder()#creating a label encoder object\nX[\"Gender\"] = encoder_gender.fit_transform(X[\"Gender\"]).copy()#converting to numerical for gender\nX.shape","309c6b81":"#For test set\n#Geography\nencoder_geo = LabelEncoder()#creating label encoder object\nX_test_data[\"Geography\"] = encoder_geo.fit_transform(strat_test_data[\"Geography\"]).copy()#converting to numberical for geography\n\n#Gender\nencoder_gender = LabelEncoder()#Creating label encoder object\n##converting to numberical for gender attribute\nX_test_data[\"Gender\"] = encoder_gender.fit_transform(strat_test_data[\"Gender\"]).copy()\n\nX_test_data.shape","57cd31c1":"#combining balance and estimated salary\nX['Balance_extimatedsalary_ratio'] = X.Balance\/X.EstimatedSalary\nX_test_data['Balance_extimatedsalary_ratio'] = X_test_data.Balance\/X_test_data.EstimatedSalary\n\n#combining balance and age. Balance and age ratio\nX['Balance_age_ratio'] = X.Balance\/(X.Age)\nX_test_data['Age_ratio'] = X_test_data.Balance\/(X_test_data.Age)\n","5439f12b":"#correlation among independent variables and dependent variable\ncorr_matrix_ = X.corr()\ncorr_matrix_[\"Exited\"].sort_values(ascending=False)\n","7498c339":"#dropping target variable\nX = X.drop(columns=[\"Exited\"])\nX_test_data = X_test_data.drop(columns=[\"Exited\"])","8b7d1376":"#Performing feature extraction by combining other features\npoly_features = PolynomialFeatures(2, interaction_only=True, include_bias=False)\n\n#creating new attributes for training set\nnew_attrib = poly_features.fit_transform(X)\n\n#creating a dataframe for new attributes\nnew_attrib = pd.DataFrame(new_attrib,columns = poly_features.get_feature_names(X.columns))\nnew_attrib.shape","7369c1fe":"#test test \n#creating new attributes for testing set\nnew_attrib_test = poly_features.fit_transform(X_test_data)\n\n#creating a dataframe for new attributes\nnew_attrib_test = pd.DataFrame(new_attrib_test,columns = poly_features.get_feature_names(X_test_data.columns))\nnew_attrib_test.shape","c0807d6c":"# Train set\n# Creating correlation matrix\ncorr_matrix2 = new_attrib.corr()\n\n# Select high values of correlation matrix\nhigher_values = corr_matrix2.where(np.triu(np.ones(corr_matrix2.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.7\nattr_to_drop = [column for column in higher_values.columns if any(abs(higher_values[column]) > 0.7)]\n\n#dropping several attributes with higher correlation to avoid collinearity issue\nX_train_new = new_attrib.drop(columns = attr_to_drop)#dropping attributes with higher correlation\nX_train_new.shape","303a5645":"# Test set\n# Creating correlation matrix\ncorr_matrix3 = new_attrib_test.corr()\n\n# Select high values of correlation matrix\nhigher_v_test = corr_matrix3.where(np.triu(np.ones(corr_matrix3.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.7\nattr_to_drop_test = [column for column in higher_v_test.columns if any(abs(higher_v_test[column]) > 0.7)]\n\n#dropping several attributes with higher correlation to avoid collinearity issue\nX_test_new = new_attrib_test.drop(columns = attr_to_drop_test)\nX_test_new.shape","1d7471e7":"#creating a pipeline for several transformations for training set\npipe = Pipeline([('std_scaler', StandardScaler())])\n\n#Using pipeline object to call fit_transform\nX_transformed = pipe.fit_transform(X)","96dcff5f":"##creating a pipeline for several transformations for testing set\npipe_test = Pipeline([('std_scaler', StandardScaler())])\n\n#Using pipeline object to call fit_transform\ntest_transformed = pipe_test.fit_transform(X_test_data)","311bb3d1":"#data to be used for model training testing after all data preprocessing and EDA\nX_train = X_transformed.copy()#data for training the model (features)\nX_test = test_transformed.copy()#data for test the model performance (independent variables)\ny_train = y_train_data.copy()#data for training (target)\ny_test = y_test_data.copy()#data for test the model performance (target)\n\nprint(\"Train data: \", X_train.shape)\nprint(\"Test data: \", X_test.shape)\nprint(\"Train for y: \", y_train.shape)\nprint(\"Test for y \", y_test.shape)\n","b0a7fb76":"# #Splitting the data into test and train sets\n# X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.3, random_state=42)\n\n'''Creating a Gaussian Classifier, balanced so that I can avoid imbalanced class \n   issues, random state to 1 to avoid change of results (predictions)\n'''\n#rf_classifier=RandomForestClassifier(n_estimators=100)\nrf_classifier = RandomForestClassifier(class_weight=\"balanced\", random_state=1)\n\nrf_classifier.fit(X_train, y_train)\n#cross validation\npredictions_results = cross_val_predict(rf_classifier, X_train, y_train, cv=10)\n\npredicted_rf = pd.Series(predictions_results)\n\n#predicted","54358927":"#cross validation accuracy\naccuracies = cross_val_score(estimator = rf_classifier, X = X_train, y = y_train, cv = 10, n_jobs = -1)\nsum(accuracies)\/10","b7a02049":"# Error metrics\n# False positives.\nfp_filter = (predicted_rf == 1) & (y_train == 0)\nfp = len(predicted_rf[fp_filter])\n\n# True positives.`\ntp_filter = (predicted_rf == 1) & (y_train == 1)\ntp = len(predicted_rf[tp_filter])\n\n# False negatives.\nfn_filter = (predicted_rf == 0) & (y_train == 1)\nfn = len(predicted_rf[fn_filter])\n\n# True negatives\ntn_filter = (predicted_rf == 0) & (y_train == 0)\ntn = len(predicted_rf[tn_filter])\n\n# Rates\ntpr = tp \/ (tp + fn)#recall\nfpr = tp \/ (tp + fp)#precision\n\nprint(\"False positive: \",fp)\nprint(\"True positive: \", tp)\nprint(\"False negative: \", fn)\nprint(\"True negative: \", tn)","d8511528":"#printing precision, recall and f-score on train set\nprint(\"Precison score: \", precision_score(y_train, predicted_rf))\nprint(\"Recall score: \", recall_score(y_train, predicted_rf))\nprint(\"f1_score score: \", f1_score(y_train, predicted_rf))","515fec80":"confusion_matrix(y_train, predicted_rf)\n#y_train","c9fc360e":"#printing precision, recall and f1-score for both classes using train set\nprint(classification_report(y_train, predicted_rf))","9797d805":"#printing roc score using train sample\nroc_auc_score(y_train, predicted_rf)","64eb764a":"#predicting which clients are likely to open a term deposit account\ny_pred_rf = rf_classifier.predict(X_test)\n\nprint(\"precison score: \", precision_score(y_test, y_pred_rf))\nprint(\"recall score: \", recall_score(y_test, y_pred_rf))\nprint(\"f1_score score: \", f1_score(y_test, y_pred_rf))\n","1cff1a35":"#printing precision, recall and f1-score for both classes using test set\nprint(classification_report(y_test, y_pred_rf))\n#printing roc score using test samples\nroc_auc_score(y_test, y_pred_rf)","0d544b85":"svm_classifier = SVC(kernel='poly', degree=8, probability=True)#Building SVM classifier\nsvm_classifier.fit(X_train, y_train)#fitting the model\n\ny_pred_svm = svm_classifier.predict(X_train)#predicting\n","2c4aad87":"print(classification_report(y_train, y_pred_svm))","8a418b64":"#building a model using XGBoost algorithm\nXGB_model = xgb.XGBClassifier(objective=\"binary:logistic\",eta= 0.002,subsample=0.5,max_depth=6, n_estimators=1000)\nXGB_model.fit(X_train, y_train)#fitting the model using training data","fc0ba67d":"#Predicting on train data\ny_pred_XGB = XGB_model.predict(X_train)\ny_pred_XGB","2c0e5b23":"#cross validation accuracy\naccuracies = cross_val_score(estimator = XGB_model, X = X_train, y = y_train, cv = 10, n_jobs = -1)\nsum(accuracies)\/10","4f12be4f":"#printing precision, recall and f-score\nprint(\"Precison score: \", precision_score(y_train, y_pred_XGB))\nprint(\"Recall score: \", recall_score(y_train, y_pred_XGB))\nprint(\"f1_score score: \", f1_score(y_train, y_pred_XGB))","d46f7a20":"#making predictions using XGB classifier\ny_pred_XGB_t = XGB_model.predict(X_test)","346cd848":"#printing precision, recall and f1-score for both classes using train data\nprint(classification_report(y_train, y_pred_XGB))\n\n#printing roc score using test samples\nroc_auc_score(y_test, y_pred_XGB_t)","fbbeb91d":"#tuning hyparameters for XGBoost model\nXGB_tuned_model = xgb.XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=5, min_child_weight=1, objective= 'binary:logistic', \n    scale_pos_weight=1)\n\n#building a dictionary with several parameters\nparam_tuning = {'max_depth': [3,5,7,9],'min_child_weight':[1,3,5]} ","94b56ad1":"#building a gridsearch \ngsearch_instance = GridSearchCV(estimator = XGB_tuned_model, param_grid = param_tuning, scoring='accuracy',n_jobs=-1,iid=False, cv=10)\n\n#fit with all sets of parameters \ngsearch_instance.fit(X_train, y_train)\n\n#best parameters after gridsearch hyperparameter tuning\ngsearch_instance.best_params_, gsearch_instance.best_score_","9970d011":"#Building a model based on tuned parameters \nXGB_tuned_model_result = xgb.XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=3, min_child_weight=1, objective= 'binary:logistic', \n    scale_pos_weight=1)\n\n##fitting the model using train data\nXGB_tuned_model_result.fit(X_train, y_train)\n\n#making predictions\ny_pred_XGB_tuned = XGB_tuned_model_result.predict(X_train)","6dee4f47":"#printing precision, recall and f1-score for both classes using train data\nprint(classification_report(y_train, y_pred_XGB_tuned))","478f3931":"'''\n   This function plot the roc curve using false positive and true positive \n   \n   Input: f: false positive rate\n   \n          t: True positive rate\n          \n          l: Label\n'''\n\ndef roc_curve_p(f, t, l=None):\n    plt.plot(f, t, linewidth=2, label=l)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)')\n    plt.ylabel('True Positive Rate (TPR)')","26573cf0":"#getting scores for the random forest classifier\ny_probas_forest = cross_val_predict(rf_classifier, X_train, y_train, cv=10, method=\"predict_proba\")\ny_scores_forest = y_probas_forest[:, 1]\n\n#getting scores for XGboost classifier\ny_probas_xgb = cross_val_predict(XGB_tuned_model_result, X_train, y_train, cv=10, method=\"predict_proba\")\ny_scores_xgb = y_probas_xgb[:, 1]\n\n#getting scores for SVM classifier\ny_probas_svm = cross_val_predict(svm_classifier, X_train, y_train, cv=10, method=\"predict_proba\")\ny_scores_svm = y_probas_svm[:, 1]\n\n#using roc_curve function to get false positives and true positives for random forest classifier\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_train, y_scores_forest)\n##using roc_curve function to get false positives and true positives for SVM classifier\nfpr_svm, tpr_svm, thresholds_svm = roc_curve(y_train, y_scores_svm)\n#using roc_curve function to get false positives and true positives for XGboost classifier\nfpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_train, y_scores_xgb)\n\n#plotting roc curve for random forest model\nplt.plot(fpr_rf, tpr_rf, \"b:\", label=\"Random Forest\")\n#plotting roc curve for SVM classifier\nplt.plot(fpr_svm, tpr_svm, \"r:\", label=\"SVM\")\n#plotting roc curve for XGboost classifier\nroc_curve_p(fpr_xgb, tpr_xgb, \"XGboost\")\nplt.legend(loc=\"bottom right\")\nplt.show()","43878ed6":"#making predictions using XGB classifier\ny_pred_XGB_test = XGB_tuned_model_result.predict(X_test)","111b36ed":"#printing precision, recall and f1-score for both classes\nprint(classification_report(y_test, y_pred_XGB_test))","d34c9e3e":"#creating an object of SMOTE to perform over-sampling of minority class\nsm = SMOTE(random_state = 1) \n\n#resampling the dataset\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n\n#fitting the model using resampled data\nXGB_tuned_model_result.fit(X_train_res, y_train_res)\n\n#making predictions using the model \npredictions_res= XGB_tuned_model_result.predict(X_test)\n\n#printing the classification report\nprint(classification_report(y_test, predictions_res))","3512cec8":"Before selecting a baseline model, I have decided to choose error metric which help in identifying if a model is performing well or poorly. <\/br> \n\nGoing back to the purpose of this exercise, which is to predict customer churn (whether a customer will churn in the future or will stay as Bank's customer); the **error metric** will help me to determine if the model will accurately classify if a customer will churn or not. <\/br>\n\nSince, this is a classification problem, **false positives and false negatives** are both misclassifications which help in quantifying the model perfomance. <\/br>\n\nAlso, the class imbalance problem that we might face tells me that using **precision and recall** error metric is also a good choice, *instead of using accuracy as error metric because the classifier my predict all customers to churn but still have high accuracy*. \n\nThe table below explains several outcomes and their error types: ","b74fe54f":"We can also use histogram to graphically understand the data.","4c33bf36":"### Transformation pipeline","4f983777":"### Target variable","2b209423":"#### Creating a Test Set","e7e45218":"In my opinion, because the bank is more interested in knowing which customer will churn, I am more interested in model predicting **1's\" correctly**. <\/br>\n","46911f34":"## Model selection","48de255b":"Some values like `count, mean, max, and min ` are easy to understand. std is the standard deviation means how far are the values from the mean in an attribute. <\/br>\n25%, 50%, and 75% are the percentiles. For instance, 25% of the customers are not active member, do not have credit card, do not have a balance, have lowest tenure, and are below 32 years of age.","af521cc3":"### V. 3. XGBoost","0e9df1ce":"# II. Data preprocessing","c8759014":"##### checking the relevance of new features","89a6f6a8":"**The newly created features have an improved correlation with a target variable which is what I wanted.**","8f516c2c":"*  Customers from France are more frequent which explains why they are more who did not churn.\n*  I noticed that customers with credit cards who did not churn are greater than those without credit cards but      supringly `those with credit cards churned more than those without credit cards`\n\n*  The dataset has a large number of male than of female. Despite that, female churned more.\n*  Active members as we would except do not churn more","45ec2698":"### Tuning max_depth, min_child_weight, and learning rate\n","f7375b80":"#### cross validation score","9b36d0b7":"### Confusion matrix","9106ba66":"Before digging into feature extraction, two variables which appear to be important are still categorical.<\/br>\n\n#### Handling Text and Categorical Attributes","096a86a5":"Let's now try possible *feature extraction* and *feature selection*. <\/br>\n\nWe can start by removing variables like (`'RowNumber', 'CustomerId', 'Surname'`)\n\nThen use sklearn's polynomial features to combine several features. We will then remove correlated features so that we can feed best features to our machine learning model.","f0292f7e":"#### Removing correlated attributes","e3284d1c":"### V. 1. Random forest","ddf90e66":"Gender attribute is self explanatory but Male is more repetitive than female. <\/br> \n*Taking a look at other attributes using describe() method* which shows the summary of numerical attributes.","06b96e2b":"## Import Data","5ac94fe5":"The goal of this section is to use data analysis techniques and data visualizations to explore the relationship between the features and the target variable, `Exited`.","1e0b64e6":"#### Feature scaling","b66fea6c":"### Testing the choosen model on unseen data to provide conclusion","5b8c0fea":"As a result, customers come from either of the three different countries (`France`, `Germany`, and `Spain`)","4523fbaa":"### Balancing the dataset using SMOTE","a3c0ed2a":"#### Let's use ROC (Receiver operating characteristics) curve","51c384b6":"*   The target, will the customer churn or not (Exited) is binary. \n*   Exited = 1 if the client has churned\n*   Exited = 0 if the client has not churned\n\nFrom the histogram, we can notice that we have **class imbalance**, because most customers did not churn. \nThis is confirmed by using value_counts()","178373a6":"When the correlation (which is linear correlation) is close to 1, there is a positive and strong correlation between the two variables, negative otherwise. <\/br>\n\nIt looks like None of the variables is strongly correlated but some variables like (`Age`, `Balance` looks promising). <\/br>\n\nThere is also another method for correlation, using pandas' scatter_matrix. ","b321c0ed":"Finally, let's see if there are some outliers so that we can remove them.","70b30fa5":"As our dataset is not very large, we can avoid using random (if the data was too big, we could stick with random methods) method in splitting the data into train and test set. Using a method called stratified sampling let us choose a test test and make sure that it is representative of the overall data which is good to fully test the performance of the model on unseen data. <\/br>\n\nInstead of using a method like: <\/br>\n\n`X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.3, random_state=42)`\n\nWe can use stratified sampling.\n\nLet use Tenure for stratified sampling. This will help us to make sure that test dataset represents the whole data. Then, we use it to split the data in test set and train set.\n\n1. We look at histogram.\n2. Convert it to categorical since it is numerical (creating Tenure category attribute).\n","6656fe25":"XGboost seems to perform better than Random forest and SVM, with the precision of 86% on the test set and 46%\n\nAlso the tuned model has 83% precision and 48% recall which is more of a balance between precision and recall. ","556814cf":"The reason for creating a test set at this time is to avoid what is called **Data snooping**. Looking at the test set while exploiting the data and choosing an algorithm to use is not a good practice as it might trick us to choose the a particular model depending on the pattern in the test test. <\/br>\n\nSeveral methods are used to split the data into test and train set and It is often easy. We could just pick 20% of the data for test and other 80% for training randomly but running the program will generate new test set which is what we need to avoid. <\/br>\n\n","700f4114":"As I stated during EDA, there is a non-linear relationship among data, which indicates that instead of spending time on linear models like logistic or linear classifiers, I can just try non-linear models such as Random-Forest. <\/br>\n\nFor regularization, I will use K-folds cross-validation which randomly splits train set into k subsets (called folds), then also train and evaluates the model K times. \n\nIt also picks different subset (fold) on every evaluation and train the model on the remaining 9 subset. \n","80c97ce2":"The task is supervised learning because we have labeled training examples (`Exited`). Morever, It is also a classification problem since it is about classifying which customers may churn. More speficically, It is a **binary classification problem** since the model will predict whether a client may churn or not. Finally, no continuous data flow is present, there is no particular need to pay attention on rapid data change and also data are not that big and they can fit in memory, thus, **batch learning** is enough for this case. ","aa4c0083":"Using standardization method, it is performed by substracting the mean value and divides by the variance. ","ec8c1c5a":"#### Using GridSearch to fine tune hyperparameters","d0bdb32f":"There are no considerable outliers.","1193d22d":"As a result, the info() method informs that there are 10000 instances within the dataset, and all attributes have 10000 non-null values, meaning that all customers do not miss any feature. <\/br>\n\nEleven of the attributes are numerical, `Surname`, `Geography`, and `Gender` are of object type and since the data is from a CSV file, it means they must be text attributes. <\/br>\n\nLooking at the first 5 columns:\n*  Surname represents the first name of each customer.\n*  Geography and Gender must be categorical attributes. \n\nLet's use value_counts() method to find out how many customers belongs to each category in these two attributes (Geography and Gender)\n","7240fa48":"#### Looking for correlation","ae6d1c0e":"Every month BankCo loses thousands of customers to it\u2019s competitors. Customers who leaves the bank are known as \"Churned customers\" which is undesirable situation for the BankCo. In this exercise, I will help the BankCo to predict which customers are likely to churn in the future. As a result, BankCo will take measures based on the predictions. ","75130d68":"1. One possibility is to combine Balance and estimated salary. How is the customers' balance compared to the his\/her salary.\n\n2. Does balance have something to with Age. May be older people tend to save money.","868c249a":"## VI. Conclusion","766d9175":"In my opinion, I am more interested in model predicting \"1's\" correctly because the bank is more interested in knowing which customer will churn,  <\/br>\n\nBy default, precision and recall function from sklearn print the class 1 results which is what I want. \n\nTo view all statistics, I will use `classification_report` function. ","fddc9b0b":"### Parameter Tuning for XGBoost model","af8f33d3":"### Precision and recall","0cdb89e9":"Most customers have Tenure period between 0-10","a15e1f97":"The purpose of the exercise was to predict which customers are likely to churn in the future. Depending on several factors such as roc curve, precion-recall trade-off and f1-score I have choosen to use XGBoost model. With the performance on the test set: \n\n* A precision of 0.79 means that in all customers that the model predict will churn, 77 % of them actually     churned. \n\n* A recall of 0.44 means that in all customers in the dataset that churned, 42 % of them actually churned. \n\nTo improve on the accuracy several factors might be considered such as data augmentation on the target variable using several methods of over-sampling (**as it can be seen below**). However, It does perform well on test set. \n\nAlso, collecting new data to get better features. ","85f3aa50":"There is an overlap among several attribute which indicates alot of `non-linear relationship` between variables. This also give insights about the algorithm to consider while choosing the model. Probabably **`a non-linear model`** would be better choice. ","b78a7a7f":"## V. 2. SVM","6146e101":"3. generating polynomial and interaction features\n\nAccording to python documentation, I will generate polynomial and interaction features.\n\nGenerate a new feature matrix consisting of all polynomial combinations\nof the features with degree less than or equal to the specified degree.\nFor example, if an input sample is two dimensional and of the form\n[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]","d7cd5512":"Each row represents one customer. As can be seen using head() function and columns , there are 14 attributes. \n*  RowNumber\n*  CustomerId\n*  Surname   \n*  **CreditScore**: numerical number to quantify how trusty is an individual to pay off the debt.\n*  Geography  \n*  Gender     \n*  Age        \n*  **Tenure**: How long has a customer stayed with the bank.     \n*  Balance    \n*  NumOfProducts\n*  HasCrCard    \n*  IsActiveMember\n*  EstimatedSalary\n*  Exited         ","869148ef":"The goal of this section is to develop a model capable of predicting which clients are likely to churn.  ","3dd69d1a":"Balance and Age seems to have a correlation even though it is not high. There is of extracting a feature between the two.","37e51cd0":"We can start by looking at how each attributes correlates to the Exited dependent variable.","fa7757a7":"In my opinion, the bank would want to minimize both False positive and False Negative. And also false positive is more riskier because we would be predicting that a customer **may not** churn but the customer will churn. ","348c6b13":"Let's start with pairplot which will let us compare the attributes with the target variable. Specifically, it will help to visualize the distributions of several independent variables against the target variable. ","32df7eaa":"# IV. Feature Engineering","598434a1":"# Predicting Customer Churn at a Bank","13657d84":"# V. Classification Model Development\n","d10644ae":"A class imbalance in target variable is confirmed by this pie chart above, 79.63% represents customers who churned and 20.37% represents customers who did not churn. ","381d9cdb":"*The info() method is particularly useful in getting the description of the dataset, for instance the each attribute type, the number of rows of each attribute and also non-null values.*","bad4a60c":"## III. Exploratory Data Analysis and Data Visualization","0f21f9c8":"# I. Framing the problem","9ea2c864":"## Creating a baseline model","532a54be":"A precision of 0.78 means that in all customers that the model predict will churn, 77 % of them actually churned. \n\nA recall of 0.42 means that in all customers in the dataset that churned, 42 % of them actually churned. \n\nA precision of 78% is not that bad considering the class imbalance we have. ","3090c1ef":"### Testing the Random forest model for unseen data","0fd3f0b3":"| Exited         | Prediction   | error type     |\n| :------------- | :----------: | -----------:   |\n|  0             | 1            | False positive |\n|  1             | 1            | True positive  |\n|  0             | 0            | True Negative  |\n|  1             | 0            | False Negative |","2f11263d":"#### Feature extraction"}}