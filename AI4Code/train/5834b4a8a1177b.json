{"cell_type":{"b9fb7865":"code","b2d83cd3":"code","ed9278a7":"code","568b758f":"code","bdcf91e2":"code","dea54c68":"code","7aad7287":"code","1825518b":"code","e42944dc":"code","a1290852":"code","eb9cb256":"code","23f8dcfd":"code","c48bb978":"code","700f8dd0":"code","c6e04f9b":"code","a825be68":"code","64ea3f1b":"code","f50e5e0e":"code","7683fe21":"code","ca1589d5":"markdown","93c14bb0":"markdown","ab026be9":"markdown","a93327c4":"markdown","ccf0325d":"markdown","5746e324":"markdown","76fe6bef":"markdown","0001c5f8":"markdown","bc3e2e3b":"markdown","99baddf7":"markdown","3b3fbdf8":"markdown","ac45b592":"markdown","c10aeac6":"markdown"},"source":{"b9fb7865":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom numba import jit\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b2d83cd3":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nfeatures = [c for c in train.columns if c not in ['ID_code', 'target']]\ntarget = train['target']\nprint (\"Data is ready!\")","ed9278a7":"print (\"Test \",test.shape)\nprint (\"Train \",train.shape)","568b758f":"train.head(15)","bdcf91e2":"train.describe()","dea54c68":"train[train['target']==0].describe()","7aad7287":"train[train['target']==1].describe()","1825518b":"print (\"Missing data at training\")\ntrain.isnull().values.any()","e42944dc":"print (\"Missing data at test\")\ntest.isnull().values.any()","a1290852":"train = train.drop([\"ID_code\", \"target\"], axis=1)","eb9cb256":"sns.set_style('whitegrid')\nsns.countplot(target)\nsns.set_style('whitegrid')","23f8dcfd":"@jit\ndef augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","c48bb978":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.335,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.041,\n    'learning_rate': 0.0083,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': -1\n}","700f8dd0":"#kfold = 15\n#folds = StratifiedKFold(n_splits=kfold, shuffle=False, random_state=44000)\nnum_folds = 11\nfeatures = [c for c in train.columns if c not in ['ID_code', 'target']]\n\nfolds = KFold(n_splits=num_folds, random_state=2319)\noof = np.zeros(len(train))\ngetVal = np.zeros(len(train))\npredictions = np.zeros(len(target))\nfeature_importance_df = pd.DataFrame()","c6e04f9b":"for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    \n    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]\n    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]\n    \n    X_tr, y_tr = augment(X_train.values, y_train.values)\n    X_tr = pd.DataFrame(X_tr)\n    \n    print(\"Fold idx:{}\".format(fold_ + 1))\n    trn_data = lgb.Dataset(X_tr, label=y_tr)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n    \n    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    getVal[val_idx]+= clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) \/ folds.n_splits\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits","a825be68":"print(\"\\n >> CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","64ea3f1b":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","f50e5e0e":"submission = pd.DataFrame({\"ID_code\": test.ID_code.values})\nsubmission[\"target\"] = predictions\nsubmission.to_csv(\"submission.csv\", index=False)","7683fe21":"submission.head()","ca1589d5":"<h1><center><font size=\"6\">Santander Customer Transaction Prediction<\/font><\/center><\/h1>\n<h1><center><font size=\"5\">Can you identify who will make a transaction?<\/font><\/center><\/h1>\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/4\/4a\/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\" width=\"500\"><\/img>\n\n<br>\n<b>\n    \nOur data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n\nIn this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.\n\nThe data is anonimyzed, each row containing 200 numerical values identified just with a number.<\/b>\n     \n","93c14bb0":"<a id=1><pre><b>Run LGBM model<\/b><\/pre><\/a>","ab026be9":"# Submission","a93327c4":"<a id=1><pre><b>Load Packages<\/b><\/pre><\/a>","ccf0325d":"Let's see basic stats on the 2 different groups.","5746e324":"**Target = 0 or Target = 1, binary classification**","76fe6bef":"# Build the Light GBM Model","0001c5f8":"<a id=1><pre><b>Import the Data<\/b><\/pre><\/a>","bc3e2e3b":"**Missing data**","99baddf7":"<a id=1><pre><b>Classification augment<\/b><\/pre><\/a>","3b3fbdf8":"**There is no missing data**","ac45b592":"### Check for Class Imbalance","c10aeac6":"<a id=1><pre><b>Parameters<\/b><\/pre><\/a>"}}