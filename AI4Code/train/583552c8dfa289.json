{"cell_type":{"f5a9ca74":"code","8121bffb":"code","3a1c3720":"code","e64e9a76":"code","354b1ffd":"code","fc1ef2f2":"code","211ab25c":"code","ca459a81":"code","45adac39":"code","7a86c443":"code","fa58f5ad":"code","18ccfadb":"code","4126ece5":"code","5ef81ee0":"code","e6413b5f":"code","074bd2e2":"code","f3561500":"code","74af4e3c":"code","ab3f75d3":"code","17139e61":"markdown","2ef9bcd9":"markdown","7c284572":"markdown","8c43fefe":"markdown","353f6689":"markdown","88832865":"markdown","0617d65f":"markdown","400d5c03":"markdown","8de4ee8c":"markdown","bf05ef86":"markdown"},"source":{"f5a9ca74":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport progressbar\nimport nltk\nimport matplotlib.pyplot as plt\nimport re\n\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8121bffb":"TRAIN_DATA_PATH = \"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\"\nTEST_DATA_PATH = \"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\"\nVALID_DATA_PATH = \"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\"\nSAMPLE_SUBMISSION = \"\/kaggle\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\"","3a1c3720":"df_train = pd.read_csv(TRAIN_DATA_PATH)\ndf_test = pd.read_csv(TEST_DATA_PATH)\ndf_validation_data = pd.read_csv(VALID_DATA_PATH)\ndf_sample_submission = pd.read_csv(SAMPLE_SUBMISSION)","e64e9a76":"# for col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n#     print(f'****** {col} *******')\n#     display(df_train.loc[df_train[col]==1,['comment_text',col]].sample(10))","354b1ffd":"# Create a score that messure how much toxic is a comment\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\n# cat_mtpl = {'obscene': 3, 'toxic': 4, 'threat': 4, \n#             'insult': 2, 'severe_toxic': 4, 'identity_hate': 2}\n\nfor category in cat_mtpl:\n    df_train[category] = df_train[category] * cat_mtpl[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)\n\ndf_train['y'] = df_train['score']\n\nmin_len = (df_train['y'] > 0).sum()  # len of toxic comments\ndf_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\ndf_train_new = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\ndf_train_new","fc1ef2f2":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+')  # Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml')  # Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) # Remove special Charecters\n    text = re.sub(' +', ' ', text) # Remove Extra Spaces\n    text = text.strip().lower() # remove spaces at the beginning and at the end of string and make string lower\n    \n    # lemmatization\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])\n    # del stopwords\n    text = ' '.join([word for word in text.split(' ') if word not in stop])\n\n    return text","211ab25c":"def clean(data, col):\n    \n    data[col] = data[col].str.replace('https?:\/\/\\S+|www\\.\\S+', ' social medium ')      \n        \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\") \n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\") \n    data[col] = data[col].str.replace(\"1\", \"i\") \n    data[col] = data[col].str.replace(\"!\", \"i\") \n    data[col] = data[col].str.replace(\"|\", \"i\") \n    data[col] = data[col].str.replace(\"0\", \"o\") \n    data[col] = data[col].str.replace(\"l3\", \"b\") \n    data[col] = data[col].str.replace(\"7\", \"t\") \n    data[col] = data[col].str.replace(\"7\", \"+\") \n    data[col] = data[col].str.replace(\"8\", \"ate\") \n    data[col] = data[col].str.replace(\"3\", \"e\") \n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\")\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")    \n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \")\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \")\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you')\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \")\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace('\\s+', ' ')  # will remove more than one whitespace character\n#     text = re.sub(r'\\b([^\\W\\d_]+)(\\s+\\1)+\\b', r'\\1', re.sub(r'\\W+', ' ', text).strip(), flags=re.I)  # remove repeating words coming immediately one after another\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1') # 2 or more characters are replaced by 2 characters\n#     text = re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags = re.I)\n    data[col] = data[col].str.replace(\"[:|\u2663|'|\u00a7|\u2660|*|\/|?|=|%|&|-|#|\u2022|~|^|>|<|\u25ba|_]\", '')\n    \n    \n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" \")\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    \n    return data","ca459a81":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n\nlemmatizer = WordNetLemmatizer()\nstop = stopwords.words('english')\n\n# tqdm.pandas()\n# df_train_new['clean_text'] = df_train_new['comment_text'].progress_apply(text_cleaning)\ndf_train_new = clean(df_train_new, 'comment_text')","45adac39":"tqdm.pandas()\ndf_train_new['clean_text'] = df_train_new['comment_text'].progress_apply(text_cleaning)","7a86c443":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport lightgbm as lgbm\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.linear_model import Ridge, LogisticRegression, RidgeCV, ElasticNet, SGDRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.ensemble import StackingRegressor, VotingRegressor","fa58f5ad":"# df_train_new = df_train_new.reset_index(drop=True)\nlabels = df_train_new['y']\ncomments = df_train_new['clean_text']\n\nvectorizer = TfidfVectorizer(min_df=3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5))\ncomments_tr = vectorizer.fit_transform(comments)\ncomments_tr","18ccfadb":"# vectorizer = TfidfVectorizer()\n# counts = vectorizer.fit_transform(X_train)\n\n# regressor = Ridge(alpha=0.2)\n# regressor.fit(comments_tr, labels)\n\n# models = [('ridge05', Ridge(random_state=42, alpha=0.8)),\n# #           ('linSVR', LinearSVR(random_state=42)),\n#           ('sgd', SGDRegressor(random_state=42))]\n\n# final_estimator = lgbm.LGBMRegressor(random_state=42)\n\n# regressor = VotingRegressor(estimators=models)\nregressor = Ridge(random_state=42, alpha=0.8)\nregressor.fit(comments_tr, labels)","4126ece5":"# preprocess val data\n\ntqdm.pandas()\ndf_validation_data = clean(df_validation_data, 'less_toxic')\ndf_validation_data = clean(df_validation_data, 'more_toxic')\ndf_validation_data['less_toxic'] = df_validation_data['less_toxic'].progress_apply(text_cleaning)\ndf_validation_data['more_toxic'] = df_validation_data['more_toxic'].progress_apply(text_cleaning)\n\nless_toxic = vectorizer.transform(df_validation_data['less_toxic'])\nmore_toxic = vectorizer.transform(df_validation_data['more_toxic'])\n\n# make predictions\ny_pred_less = regressor.predict(less_toxic)\ny_pred_more = regressor.predict(more_toxic)\n\n(y_pred_less < y_pred_more).mean()","5ef81ee0":"# regressor = Ridge(alpha=0.8)\n# regressor.fit(comments_tr, labels)\n\n# y_pred_less = regressor.predict(less_toxic)\n# y_pred_more = regressor.predict(more_toxic)\n\n# print(0.8, (y_pred_less < y_pred_more).mean())","e6413b5f":"# for alpha in np.linspace(0.1, 2.5, 25):\n#     regressor = Ridge(alpha=alpha)\n#     regressor.fit(comments_tr, labels)\n\n#     y_pred_less = regressor.predict(less_toxic)\n#     y_pred_more = regressor.predict(more_toxic)\n\n#     print(alpha, (y_pred_less < y_pred_more).mean())","074bd2e2":"# df_test['text'] = df_test['text'].apply(lambda x: ' '.join([word for word in words_pattern.findall(x) if word not in stopwords]))\n# df_test['text'] = df_test['text'].apply(lambda elem: elem.strip().lower())\ndf_test = clean(df_test, 'text')\ndf_test['text'] = df_test['text'].progress_apply(text_cleaning)\ndf_test","f3561500":"df_test['prediction'] = regressor.predict(vectorizer.transform(df_test['text']))\ndf_test = df_test[['comment_id','prediction']]","74af4e3c":"df_test['score'] = df_test['prediction']\ndf_test = df_test[['comment_id','score']]","ab3f75d3":"df_test.to_csv('.\/submission.csv', index=False)","17139e61":"<h2>Upload Datasets<\/h2>","2ef9bcd9":"<h2>Imports<\/h2>","7c284572":"# Scoring training data","8c43fefe":"<h2>Clean text<\/h2>\n\nBoth data sets (train al test) need to be cleaned for better predictions.","353f6689":"# Compare few models","88832865":"0.6765643682742128 - Ridge, TFIDF, words lower  \n0.6770293609671848 - Ridge, TFIDF, words lower, severe_toxic=1.5  \n0.6786900491563704 - LinearSVR, TFIDF, words lower, severe_toxic=1.5. 0.821 \u043d\u0430 \u0442\u0435\u0441\u0442\u0435  \n0.6777932775342101 - Ridge, TFIDF, words lower, severe_toxic=1.5, alpha=1.342857142857143","0617d65f":"# Please upvote if you like my work :)","400d5c03":"# Predictions and load submission.csv","8de4ee8c":"### Please upvote if you like my work :)","bf05ef86":"## Lets test our model"}}