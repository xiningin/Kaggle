{"cell_type":{"b166e88d":"code","d2dc7e55":"code","cf213dbc":"code","6dbfe45b":"code","a83e7ca9":"code","480ba4e7":"code","28f91542":"code","76436cff":"code","f5a341ab":"code","797ca910":"code","bc652ab8":"code","3a8bc138":"code","8ad1be0a":"code","423bcc12":"code","1c717d27":"code","687ab0c4":"markdown","7e2eba18":"markdown","fb113901":"markdown","c2a7807b":"markdown","d173002d":"markdown","bfd2368f":"markdown","60560238":"markdown","9217d86a":"markdown","e3034478":"markdown","9a2b41d5":"markdown"},"source":{"b166e88d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 100)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d2dc7e55":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error","cf213dbc":"train_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_data","6dbfe45b":"print(train_data.apply(lambda col: col.dtypes))","a83e7ca9":"categorical_list = list(test_data.select_dtypes(include='object'))\n\ntrain_data['train'] =1\ntest_data['train'] =0\ncombined = pd.concat([train_data, test_data])\n\nfor col in categorical_list:\n    combined[col] = combined[col].astype('category').cat.codes\n\ntrain_data = combined[combined['train']==1]\ntest_data = combined[combined['train']==0]\ntrain_data = train_data.drop(['train','Id'] ,axis=1)\nids = test_data['Id']\ntest_data = test_data.drop(['Id','train','SalePrice'], axis=1)","480ba4e7":"numeric_transformer = Pipeline(steps=[\n    ('imputer', IterativeImputer(random_state=42)), \n    ('robust', RobustScaler(quantile_range=(10.0,90.0))),\n    ('minmax', MinMaxScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', IterativeImputer(random_state=42))\n])","28f91542":"preprocessor = ColumnTransformer(transformers=[\n    ('num', numeric_transformer, selector(dtype_exclude='object')),\n    ('cat', categorical_transformer, selector(dtype_include='object'))\n])","76436cff":"estimators = [\n     ('ridge', KernelRidge(alpha=14.1)),\n     ('lasso', Lasso(max_iter=2500, alpha=0.0008, random_state=42)),\n     ('svr', SVR(C=3.0, max_iter=2500, epsilon= 0.008, gamma=0.0003)),\n     ('xgb', XGBRegressor(learning_rate=0.01, max_depth=3, gamma=0, sub_sample=0.7,seed=42))\n]","f5a341ab":"clf = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('rfecv', RFECV(RandomForestRegressor(random_state=42), step=1, cv=2)),\n    ('model', StackingRegressor(estimators=estimators,final_estimator=RandomForestRegressor(n_estimators=100,random_state=42)))\n])","797ca910":"y_train = train_data['SalePrice']\nx_train = train_data.drop('SalePrice', axis=1)","bc652ab8":"x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)","3a8bc138":"clf.fit(x_train, y_train)","8ad1be0a":"y_pred = clf.predict(x_valid)\nprint(\"MSE \", np.sqrt(mean_squared_error(y_valid, y_pred)))","423bcc12":"predictions = clf.predict(test_data)","1c717d27":"output = pd.DataFrame({ 'Id' : ids, 'SalePrice': predictions })\noutput.to_csv('submission.csv', index=False)","687ab0c4":"As we can see, the categorical data types are marked as 'object'. We need to convert it to the data type of 'category' to easily LabelEncode them","7e2eba18":"**OUTLIER REMOVAL**\n\n**ROBUST SCALER**\n\nA normalization technique is required to scale the continuous features to a range. For this the robust scaler has been used. By using RobustScaler(), we can remove the outliers and then use either StandardScaler or MinMaxScaler for preprocessing the dataset. Here, we remove the data that is outside of the interquartile range between 10th Quantile and 90th Quantile\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html","fb113901":"**DATA IMPUTATION**\n\n**MICE IMPUTER**\n\nAn imputer is used to fill the missing values in a dataset. I favoured the MICE technique as it takes into account the other features in the dataset in order to fill the missing values of a particular feature. It helps in reducing the anomlaies that may occur due to the univariate imputation techiques. Learn more about MICE here https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html","c2a7807b":"**STACKING REGRESSOR**\n\nStacked generalization consists in stacking the output of individual estimator and use a regressor to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.","d173002d":"**FEATURE ENGINEERING**\n\nFeature Engineering is used to select features based on statistic or wrapper methods to find the subset of features that can improve the model performance","bfd2368f":"# Please upvote and share this kernel if you like it. Check out my profile, hit the follow button and upvote my other kernels as well. I will try to make a community post everyday and a kernel notebook every week\n\n# Cheers ! \ud83d\ude03","60560238":"Till now in this notebook we have used pipelines to build the model. Now we can see what it is about\n\n**PIPELINES**\n\nPipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n\n*Convenience and encapsulation*\n\nYou only have to call fit and predict once on your data to fit a whole sequence of estimators.\n\n*Joint parameter selection*\n\nYou can grid search over parameters of all estimators in the pipeline at once.\n\n*Safety*\n\nPipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n\nTaken from - https:\/\/scikit-learn.org\/stable\/modules\/compose.html - I could not have explained it any better !!\n","9217d86a":"1. The list of object type columns is stored in a list\n2. The ensure consistency of encoding between train and test data, the datasets are combined and encoded\n3. Once Encoded, the data is put back to its original split using the train marker column that we created\n\nNote that the Encoding is done outside the pipeline because the SKLearn LabelEncoder does not have an option to skip missing values, which is essential as the categorical columns have missing data in them","e3034478":"# **HOUSE PRICE PREDICTION**\n\nThis Kernel uses the Ames house price prediction dataset on Kaggle. The goal is to predict the sale price of the houses using pipelines. **This notebook kernel can be a very good starting point for anyone looking to build pipelines in SKLearn**\n\nThis notebook focusses on:\n* SKLearn Pipeline\n* Label Encoding\n* Feature Scaling\n* MICE Data Imputation\n* Wrapper based Feature Selection\n* Stacking Regressor\n\nThis notebook does not focus on:\n* EDA\n* Hyperparamter tuning","9a2b41d5":"**LABEL ENCODING**"}}