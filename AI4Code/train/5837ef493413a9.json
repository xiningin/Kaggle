{"cell_type":{"5c749900":"code","0d7d2faf":"code","be8f4fc8":"code","cdbdb652":"code","381e7b12":"code","50d54619":"code","c0c7708b":"code","2d98c3c2":"code","9d299a2e":"code","30578266":"code","9363bea9":"code","7d2f7d66":"code","c0159770":"code","e6b15f65":"code","3197f7bd":"code","92099bfc":"code","c2100ad1":"code","1f9aa5a8":"code","acdc314d":"code","4c502ef8":"code","b7891f41":"code","06af25a9":"code","b112eb94":"code","ef043e73":"markdown","c99756b4":"markdown","75a216f2":"markdown","0c13c8f7":"markdown","d4dd73e2":"markdown","1f53b18c":"markdown","171f1268":"markdown","5a432990":"markdown"},"source":{"5c749900":"# Import necessary everyday os libs\nimport gc\nimport sys\nfrom time import time\n\n# Import the usual suspects\nimport numpy as np\nimport pandas as pd","0d7d2faf":"# Universal pandas dataframe memory footprint reducer for those dealing with big data but not that big that require spark\ndef df_footprint_reduce(df, skip_obj=False, skip_int=False, skip_float=False, print_comparison=True):\n    '''\n    :param df              : Pandas Dataframe to shrink in memory footprint size\n    :param skip_obj        : If not desired string columns can be skipped during shrink operation\n    :param skip_int        : If not desired integer columns can be skipped during shrink operation\n    :param skip_float      : If not desired float columns can be skipped during shrink operation\n    :param print_comparison: Beware! Printing comparison needs calculation of each columns datasize\n                             so if you need speed turn this off. It's just here to show you info                            \n    :return                : Pandas Dataframe of exactly the same data and dtypes but in less memory footprint    \n    '''\n    if print_comparison:\n        print(f\"Dataframe size before shrinking column types into smallest possible: {round((sys.getsizeof(df)\/1024\/1024),4)} MB\")\n    for column in df.columns:\n        if (skip_obj is False) and (str(df[column].dtype)[:6] == 'object'):\n            num_unique_values = len(df[column].unique())\n            num_total_values = len(df[column])\n            if num_unique_values \/ num_total_values < 0.5:\n                df.loc[:,column] = df[column].astype('category')\n            else:\n                df.loc[:,column] = df[column]\n        elif (skip_int is False) and (str(df[column].dtype)[:3] == 'int'):\n            if df[column].min() > np.iinfo(np.int8).min and df[column].max() < np.iinfo(np.int8).max:\n                df[column] = df[column].astype(np.int8)\n            elif df[column].min() > np.iinfo(np.int16).min and df[column].max() < np.iinfo(np.int16).max:\n                df[column] = df[column].astype(np.int16)\n            elif df[column].min() > np.iinfo(np.int32).min and df[column].max() < np.iinfo(np.int32).max:\n                df[column] = df[column].astype(np.int32)\n        elif (skip_float is False) and (str(df[column].dtype)[:5] == 'float'):\n            if df[column].min() > np.finfo(np.float16).min and df[column].max() < np.finfo(np.float16).max:\n                df[column] = df[column].astype(np.float16)\n            elif df[column].min() > np.finfo(np.float32).min and df[column].max() < np.finfo(np.float32).max:\n                df[column] = df[column].astype(np.float32)\n    if print_comparison:\n        print(f\"Dataframe size after shrinking column types into smallest possible: {round((sys.getsizeof(df)\/1024\/1024),4)} MB\")\n    return df","be8f4fc8":"# Universal pandas dataframe null\/nan cleaner\ndef df_null_cleaner(df, fill_with=None, drop_na=False, axis=0):\n    '''\n    Very good information on dealing with missing values of dataframes can be found at \n    http:\/\/pandas.pydata.org\/pandas-docs\/stable\/missing_data.html\n    \n    :param df        : Pandas Dataframe to clean from missing values \n    :param fill_with : Fill missing values with a value of users choice\n    :param drop_na   : Drop either axis=0 for rows containing missing fields\n                       or axis=1 to drop columns having missing fields default rows                   \n    :return          : Pandas Dataframe cleaned from missing values \n    '''\n    df[(df == np.NINF)] = np.NaN\n    df[(df == np.Inf)] = np.NaN\n    if drop_na:\n        df.dropna(axis=axis,inplace=True)\n    if ~fill_with:\n        df.fillna(fill_with, inplace=True)\n    return df","cdbdb652":"def feature_engineering(df,is_train=True):\n    if is_train:          \n        df = df[df['maxPlace'] > 1].copy()\n\n    target = 'winPlacePerc'\n    print('Grouping similar match types together')\n    df.loc[(df['matchType'] == 'solo'), 'matchType'] = 1\n    df.loc[(df['matchType'] == 'normal-solo'), 'matchType'] = 1\n    df.loc[(df['matchType'] == 'solo-fpp'), 'matchType'] = 1\n    df.loc[(df['matchType'] == 'normal-solo-fpp'), 'matchType'] = 1\n\n    df.loc[(df['matchType'] == 'duo'), 'matchType'] = 2\n    df.loc[(df['matchType'] == 'normal-duo'), 'matchType'] = 2\n    df.loc[(df['matchType'] == 'duo-fpp'), 'matchType'] = 2    \n    df.loc[(df['matchType'] == 'normal-duo-fpp'), 'matchType'] = 2\n\n    df.loc[(df['matchType'] == 'squad'), 'matchType'] = 3\n    df.loc[(df['matchType'] == 'normal-squad'), 'matchType'] = 3    \n    df.loc[(df['matchType'] == 'squad-fpp'), 'matchType'] = 3\n    df.loc[(df['matchType'] == 'normal-squad-fpp'), 'matchType'] = 3\n    \n    df.loc[(df['matchType'] == 'flaretpp'), 'matchType'] = 0\n    df.loc[(df['matchType'] == 'flarefpp'), 'matchType'] = 0\n    df.loc[(df['matchType'] == 'crashtpp'), 'matchType'] = 0\n    df.loc[(df['matchType'] == 'crashfpp'), 'matchType'] = 0\n    df.loc[(df['rankPoints'] < 0), 'rankPoints'] = 0\n    \n    print('Adding new features using existing ones')\n    df['headshotrate'] = df['kills']\/df['headshotKills']\n    df['killStreakrate'] = df['killStreaks']\/df['kills']\n    df['healthitems'] = df['heals'] + df['boosts']\n    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n    df['killPlace_over_maxPlace'] = df['killPlace'] \/ df['maxPlace']\n    df['headshotKills_over_kills'] = df['headshotKills'] \/ df['kills']\n    df['distance_over_weapons'] = df['totalDistance'] \/ df['weaponsAcquired']\n    df['walkDistance_over_heals'] = df['walkDistance'] \/ df['heals']\n    df['walkDistance_over_kills'] = df['walkDistance'] \/ df['kills']\n    df['killsPerWalkDistance'] = df['kills'] \/ df['walkDistance']\n    df['skill'] = df['headshotKills'] + df['roadKills']\n    \n    # Clean null values from dataframe\n    df = df_null_cleaner(df,fill_with=0)\n\n    features = list(df.columns)\n    features.remove(\"Id\")\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n    features.remove(\"matchType\")\n    features.remove(\"maxPlace\")\n    \n    y = pd.DataFrame()\n    if is_train: \n        print('Preparing target variable')\n        y = df.groupby(['matchId','groupId'])[target].agg('mean')\n        gc.collect()\n        features.remove(target)\n        \n    print('Aggregating means')   \n    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n    gc.collect()\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    gc.collect()\n    \n    if is_train: \n        X = agg.reset_index()[['matchId','groupId']]\n    else: \n        X = df[['matchId','groupId']]\n\n    X = X.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    X = X.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n    del agg, agg_rank\n    gc.collect()\n    \n    print('Aggregating maxes')\n    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n    gc.collect()\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    gc.collect()\n    X = X.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    X = X.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n    del agg, agg_rank\n    gc.collect()\n    \n    print('Aggregating mins')  \n    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n    gc.collect()\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    gc.collect()\n    X = X.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    X = X.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n    del agg, agg_rank\n    gc.collect()\n    \n    print('Aggregating group sizes')\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n    gc.collect()\n    X = X.merge(agg, how='left', on=['matchId', 'groupId'])\n    print('Aggregating match means')\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    gc.collect()\n    X = X.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n    print('Aggregating match sizes')\n    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n    gc.collect()\n    X = X.merge(agg, how='left', on=['matchId'])\n    del df, agg\n    gc.collect()\n\n    X.drop(columns = ['matchId', \n                      'groupId'\n                     ], axis=1, inplace=True)  \n    gc.collect()\n    if is_train:\n        return X, y\n    \n    return X","381e7b12":"X_train = pd.read_csv('..\/input\/train_V2.csv', engine='c')","50d54619":"X_train = df_footprint_reduce(X_train, skip_obj=True)  # Reduce memory footprint inorder to fit in memory of Kaggle Docker image\ngc.collect()","c0c7708b":"X_train, y_train = feature_engineering(X_train, True)\ngc.collect()","2d98c3c2":"X_train = df_footprint_reduce(X_train, skip_obj=True) # Reduce memory footprint again after feature generation\ngc.collect()","9d299a2e":"from sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=False).fit(X_train)","30578266":"X_train = scaler.transform(X_train)","9363bea9":"# Import the real deal\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import backend","7d2f7d66":"swish = lambda x: x*backend.sigmoid(x)","c0159770":"# create model\nmodel = Sequential()\nmodel.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation = swish))\nmodel.add(Dense(round((X_train.shape[1]\/3)*2), kernel_initializer='normal', activation = swish))\n\n# output Layer\nmodel.add(Dense(1, kernel_initializer='normal'))\n\n# Compile the network :\nmodel.compile(loss='mae', optimizer='adam', metrics=['mae'])\nmodel.summary()","e6b15f65":"seed = 13\nnp.random.seed(seed)\nfrom tensorflow import set_random_seed\nset_random_seed(seed)","3197f7bd":"import time\ntimeout = time.time() + 120\nwhile True:\n    model.fit(x=X_train, y=y_train, batch_size=96,\n                epochs=1, verbose=0,\n                validation_split=0.2, shuffle=True)\n    if time.time() > timeout:\n        break","92099bfc":"# Clean memory and load test set\ndel X_train, y_train\ngc.collect()","c2100ad1":"test_x = pd.read_csv('..\/input\/test_V2.csv', engine='c')","1f9aa5a8":"test_x = df_footprint_reduce(test_x, skip_obj=True)\ngc.collect()","acdc314d":"test_x = feature_engineering(test_x, False)\ngc.collect()","4c502ef8":"test_x = scaler.transform(test_x)","b7891f41":"%%time\npred_test = model.predict(test_x)\ndel test_x\ngc.collect()","06af25a9":"test_set = pd.read_csv('..\/input\/test_V2.csv', engine='c')","b112eb94":"submission = pd.read_csv(\"..\/input\/sample_submission_V2.csv\")\nsubmission['winPlacePerc'] = pred_test\nsubmission.loc[submission.winPlacePerc < 0, \"winPlacePerc\"] = 0\nsubmission.loc[submission.winPlacePerc > 1, \"winPlacePerc\"] = 1\nsubmission = submission.merge(test_set[[\"Id\", \"matchId\", \"groupId\", \"maxPlace\", \"numGroups\"]], on=\"Id\", how=\"left\")\nsubmission_group = submission.groupby([\"matchId\", \"groupId\"]).first().reset_index()\nsubmission_group[\"rank\"] = submission_group.groupby([\"matchId\"])[\"winPlacePerc\"].rank()\nsubmission_group = submission_group.merge(\n    submission_group.groupby(\"matchId\")[\"rank\"].max().to_frame(\"max_rank\").reset_index(), \n    on=\"matchId\", how=\"left\")\nsubmission_group[\"adjusted_perc\"] = (submission_group[\"rank\"] - 1) \/ (submission_group[\"numGroups\"] - 1)\nsubmission = submission.merge(submission_group[[\"adjusted_perc\", \"matchId\", \"groupId\"]], on=[\"matchId\", \"groupId\"], how=\"left\")\nsubmission[\"winPlacePerc\"] = submission[\"adjusted_perc\"]\nsubmission.loc[submission.maxPlace == 0, \"winPlacePerc\"] = 0\nsubmission.loc[submission.maxPlace == 1, \"winPlacePerc\"] = 1\nsubset = submission.loc[submission.maxPlace > 1]\ngap = 1.0 \/ (subset.maxPlace.values - 1)\nnew_perc = np.around(subset.winPlacePerc.values \/ gap) * gap\nsubmission.loc[submission.maxPlace > 1, \"winPlacePerc\"] = new_perc\nsubmission.loc[(submission.maxPlace > 1) & (submission.numGroups == 1), \"winPlacePerc\"] = 0\nassert submission[\"winPlacePerc\"].isnull().sum() == 0\nsubmission[[\"Id\", \"winPlacePerc\"]].to_csv(\"submission.csv\", index=False)","ef043e73":"### Model Prediction ","c99756b4":"### Feature Engineering","75a216f2":"### Model Training","0c13c8f7":"### Prepare for submission ","d4dd73e2":"### Useful functions for community","1f53b18c":"# Keras Tensorflow DNN Approach","171f1268":"### Load dataset files","5a432990":"##### Credits for work at post processing section goes to:\n###### https:\/\/www.kaggle.com\/anycode\/simple-nn-baseline-4\n###### https:\/\/www.kaggle.com\/ceshine\/a-simple-post-processing-trick-lb-0237-0204"}}