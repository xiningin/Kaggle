{"cell_type":{"037e2e97":"code","e9604780":"code","7d976663":"code","380986c3":"code","75919088":"code","f9d3101f":"code","fb8a7863":"code","15815c9e":"code","d6103e1a":"code","7ce9cd18":"code","5bed142d":"code","f10bb82a":"code","d8aa5845":"code","90d3d185":"code","6cdb199f":"code","22b17283":"code","513745a3":"code","d967d446":"code","d018665a":"code","592db26c":"code","37339914":"code","b8e7d30b":"code","6aba9788":"code","2e04356f":"code","10a3a015":"code","6c7ef6e6":"code","bae7a209":"markdown","9db528fc":"markdown","69d6122e":"markdown","f87a735d":"markdown","5eb1a48b":"markdown","e6ab3c58":"markdown","915e7d03":"markdown","0628b5f5":"markdown","23a0f5d7":"markdown","f01b181c":"markdown","faf8d056":"markdown","04957c05":"markdown","06e9a5d6":"markdown","5e1df44d":"markdown","97706f3d":"markdown"},"source":{"037e2e97":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e9604780":"df = pd.read_csv('..\/input\/customer-personality-analysis\/marketing_campaign.csv', sep=\"\\t\", parse_dates=['Dt_Customer'])\ndf.head()","7d976663":"_,ax = plt.subplots(1,3,figsize=(15,5))\n\ndf.Year_Birth.plot.hist(ax=ax[0])\ndf.Dt_Customer.dt.year.plot.hist(ax=ax[1], bins=3)\ndf.Income.plot.hist(ax=ax[2])","380986c3":"# Checking `Year_Birth` lower than 1940\n\ndf.query(\n    \"Year_Birth <= 1940\"\n).assign(\n    age_when_last_purchase = lambda df: df.Dt_Customer.dt.year.sub(df.Year_Birth)\n).filter(\n    ['ID','Year_Birth','Dt_Customer','age_when_last_purchase','Income']\n)","75919088":"# Checking incomes distributions\ndf.Income.describe(percentiles=[0.05,0.1,0.25,0.5,0.75,0.9,0.95])","f9d3101f":"_,ax = plt.subplots(1,2,figsize=(10,5))\ndf.query(\"Income >= 79_844 and Income < 666_666\").Year_Birth.plot.hist(ax=ax[0])\ndf.query(\"Income >= 79_844 and Income < 666_666\").Income.plot.hist(ax=ax[1])","fb8a7863":"_,ax = plt.subplots(1,2,figsize=(10,5))\ndf.Marital_Status.value_counts().plot.pie(ax=ax[0])\nsns.boxplot(data=df.query(\"Income < 666_666\"), y='Marital_Status', x='Income', ax=ax[1])","15815c9e":"def group_marital_status(df):\n    return np.where(\n        df.Marital_Status.isin(['Widow','Alone','Absurd','YOLO']),\n        \"Non Common\",\n        df.Marital_Status\n    )\n\ndf = df.assign(\n    Marital_Status_grouped = lambda df: group_marital_status(df)\n)","d6103e1a":"_,ax = plt.subplots(1,2,figsize=(10,5))\ndf.Marital_Status_grouped.value_counts().plot.pie(ax=ax[0])\nsns.boxplot(data=df.query(\"Income < 666_666\"), y='Marital_Status_grouped', x='Income', ax=ax[1])","7ce9cd18":"df.Recency.plot.hist()","5bed142d":"def clear_wrong_year_birth(df):\n    return np.where(\n        df.Year_Birth.lt(1940),\n        np.nan,\n        df.Year_Birth\n    )\n\ndef clear_wrong_income(df):\n    return np.where(\n        df.Income.ge(666_666),\n        np.nan,\n        df.Income\n    )\n\ndf = df.assign(\n    Year_Birth_Interpolated = lambda df: clear_wrong_year_birth(df),\n    Income_Interpolated = lambda df: clear_wrong_income(df)\n)","f10bb82a":"df.query(\n    \"Year_Birth_Interpolated != Year_Birth_Interpolated or Income_Interpolated != Income_Interpolated\"\n)","d8aa5845":"_,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(\n    df.query(\"Year_Birth_Interpolated == Year_Birth_Interpolated or Income_Interpolated == Income_Interpolated\").corr(),\n    ax=ax\n)","90d3d185":"_,ax = plt.subplots(1,2,figsize=(10,5))\ndf.query(\n    \"Year_Birth_Interpolated == Year_Birth_Interpolated and Income_Interpolated == Income_Interpolated\"\n).plot.scatter(\n    x='Income',\n    y='NumCatalogPurchases',\n    ax=ax[0]\n)\n\ndf.query(\n    \"Year_Birth_Interpolated == Year_Birth_Interpolated and Income_Interpolated == Income_Interpolated\"\n).plot.scatter(\n    x='Income',\n    y='MntMeatProducts',\n    ax=ax[1]\n)","6cdb199f":"q25 = df.query(\n    \"Year_Birth_Interpolated == Year_Birth_Interpolated and Income_Interpolated == Income_Interpolated\"\n).MntMeatProducts.quantile(q=0.25)\nq75 = df.query(\n    \"Year_Birth_Interpolated == Year_Birth_Interpolated and Income_Interpolated == Income_Interpolated\"\n).MntMeatProducts.quantile(q=0.75)\niqr = q75 - q25\nmeat_products_right_tail = q75+3*iqr","22b17283":"q25 = df.query(\n    \"Year_Birth_Interpolated == Year_Birth_Interpolated and Income_Interpolated == Income_Interpolated\"\n).Income.quantile(q=0.25)\nq75 = df.query(\n    \"Year_Birth_Interpolated == Year_Birth_Interpolated and Income_Interpolated == Income_Interpolated\"\n).Income.quantile(q=0.75)\niqr = q75 - q25\nincome_right_tail = q75+3*iqr","513745a3":"# The plot shows less outliers of high value for MntMeatProducts\ndf.query(\n    \"Year_Birth_Interpolated == Year_Birth_Interpolated and \\\n     Income_Interpolated == Income_Interpolated and \\\n     MntMeatProducts < @meat_products_right_tail and \\\n     Income < @income_right_tail\"\n).plot.scatter(\n    y='Income',\n    x='MntMeatProducts',\n)","d967d446":"from statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\n\ndata_income_interpolation = data=df.query(\n    \"Year_Birth_Interpolated == Year_Birth_Interpolated and \\\n     Income_Interpolated == Income_Interpolated and \\\n     MntMeatProducts < @meat_products_right_tail and \\\n     Income < @income_right_tail\"\n)\n\nincome_lm = ols(\n    \"Income ~ np.log(MntMeatProducts+1)\",\n    data=data_income_interpolation\n).fit()\nprint(income_lm.summary())","d018665a":"# Plot the fitted data against the regression line, under my perception it's not bad\nfrom statsmodels.graphics.api import interaction_plot, abline_plot\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.scatter(\n    np.log(\n        data_income_interpolation.MntMeatProducts.values+1\n    ),\n    data_income_interpolation.Income.values\n)\n\nfig = abline_plot(\n    intercept=income_lm.params[\"Intercept\"],\n    slope=income_lm.params[\"np.log(MntMeatProducts + 1)\"],\n    ax=ax,\n    color='red'\n    \n)","592db26c":"# An important part is evaluation the model. I tried the `mean_squared_error` but it returns a very high value.\n# Then I though I need to compare apples with apples, so the mean square log error shows better results.\nfrom sklearn.metrics import mean_squared_log_error\n\nmean_squared_log_error(\n    data_income_interpolation.assign(\n        Income_Pred = income_lm.predict(df.MntMeatProducts).round(2)\n    ).Income,\n    data_income_interpolation.assign(\n        Income_Pred = income_lm.predict(df.MntMeatProducts).round(2)\n    ).Income_Pred\n)","37339914":"# Finally, we have Income interpolated with consistent data.\ndf = df.assign(\n    Income_Pred = lambda df: income_lm.predict(df.MntMeatProducts).round(2),\n    Income_Interpolated = lambda df: df.Income_Interpolated.fillna(df.Income_Pred)\n).drop(\n    columns=\"Income_Pred\"\n)","b8e7d30b":"df.query(\n    \"Year_Birth_Interpolated == Year_Birth_Interpolated or Income_Interpolated == Income_Interpolated\"\n).corr().loc['Year_Birth_Interpolated'].sort_values(ascending=False)","6aba9788":"df.query(\n    \"Year_Birth_Interpolated == Year_Birth_Interpolated or Income_Interpolated == Income_Interpolated\"\n).assign(\n    Members_Marital_Status = lambda df: df.Marital_Status_grouped.map({'Single':1,'Together':2,'Married':2,'Divorced':1,'Non Common':1}),\n    Teenhome_sub_Kidhome = lambda df: np.cbrt(df.Teenhome.sub(df.Kidhome)),\n    Teenhome_sub_Kidhome_sub_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.sub(df.Members_Marital_Status),\n    Teenhome_sub_Kidhome_add_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.add(df.Members_Marital_Status),\n    Income_mul_Members_Marital_Status = lambda df: np.log(df.Income_Interpolated.mul(df.Members_Marital_Status)),\n    Income_div_Members_Marital_Status = lambda df: np.log(df.Income_Interpolated.div(df.Members_Marital_Status)),\n    Teenhome_sub_Kidhome_mul_Income_mul_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.mul(df.Income_mul_Members_Marital_Status),\n    Teenhome_sub_Kidhome_div_Income_mul_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.div(df.Income_mul_Members_Marital_Status),\n    Teenhome_sub_Kidhome_mul_Income_div_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.mul(df.Income_div_Members_Marital_Status),\n    Teenhome_sub_Kidhome_div_Income_div_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.div(df.Income_div_Members_Marital_Status),\n    Teenhome_sub_Kidhome_sub_Members_Marital_Status_mul_Income_Interpolated = lambda df: df.Teenhome_sub_Kidhome_sub_Members_Marital_Status.mul(df.Income_Interpolated),\n    Teenhome_sub_Kidhome_div_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.div(df.Members_Marital_Status),\n    Year_Birth_Interpolated = lambda df: pd.to_numeric(df.Year_Birth_Interpolated)\n).filter(\n    ['Members_Marital_Status','Teenhome_sub_Kidhome','Income_mul_Members_Marital_Status','Income_div_Members_Marital_Status','Year_Birth_Interpolated',\n     'Teenhome_sub_Kidhome_mul_Income_mul_Members_Marital_Status','Teenhome_sub_Kidhome_div_Income_mul_Members_Marital_Status',\n     'Teenhome_sub_Kidhome_mul_Income_div_Members_Marital_Status','Teenhome_sub_Kidhome_div_Income_div_Members_Marital_Status',\n     'Teenhome_sub_Kidhome_sub_Members_Marital_Status','Teenhome_sub_Kidhome_add_Members_Marital_Status','Teenhome_sub_Kidhome_div_Members_Marital_Status']\n).corr().Year_Birth_Interpolated","2e04356f":"# Looking at its distributions, seems like there is a great relationship between Year_Birth and oir variable\n# However, there are cases where appear no clear distributions. Maybe another variable could help\ndf = df.assign(\n    Members_Marital_Status = lambda df: df.Marital_Status_grouped.map({'Single':1,'Together':2,'Married':2,'Divorced':1,'Non Common':1}),\n    Teenhome_sub_Kidhome = lambda df: np.cbrt(df.Teenhome.sub(df.Kidhome)),\n    Teenhome_sub_Kidhome_sub_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.sub(df.Members_Marital_Status),\n    Teenhome_sub_Kidhome_add_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.add(df.Members_Marital_Status),\n    Income_mul_Members_Marital_Status = lambda df: np.log(df.Income_Interpolated.mul(df.Members_Marital_Status)),\n    Income_div_Members_Marital_Status = lambda df: np.log(df.Income_Interpolated.div(df.Members_Marital_Status)),\n    Teenhome_sub_Kidhome_mul_Income_mul_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.mul(df.Income_mul_Members_Marital_Status),\n    Teenhome_sub_Kidhome_div_Income_mul_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.div(df.Income_mul_Members_Marital_Status),\n    Teenhome_sub_Kidhome_mul_Income_div_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.mul(df.Income_div_Members_Marital_Status),\n    Teenhome_sub_Kidhome_div_Income_div_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.div(df.Income_div_Members_Marital_Status),\n    Teenhome_sub_Kidhome_sub_Members_Marital_Status_mul_Income_Interpolated = lambda df: df.Teenhome_sub_Kidhome_sub_Members_Marital_Status.mul(df.Income_Interpolated),\n    Teenhome_sub_Kidhome_div_Members_Marital_Status = lambda df: df.Teenhome_sub_Kidhome.div(df.Members_Marital_Status),\n    Year_Birth_Interpolated = lambda df: pd.to_numeric(df.Year_Birth_Interpolated)\n)\n\ndf.query(\n    \"Year_Birth_Interpolated == Year_Birth_Interpolated or Income_Interpolated == Income_Interpolated\"\n).plot.scatter(\n    y='Year_Birth_Interpolated',\n    x='Teenhome_sub_Kidhome_div_Income_div_Members_Marital_Status'\n)","10a3a015":"sns.heatmap(\n    df.assign(\n        teen_add_kid = lambda df: df.Teenhome.add(df.Kidhome)\n    ).filter(\n        ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts','MntSweetProducts', 'MntGoldProds','Incomes_Interpolated',\n         'Year_Birth_Interpolated','Teenhome','Kidhome','Marital_Status_grouped','teen_add_kid']\n    ).query(\n        'Year_Birth_Interpolated == Year_Birth_Interpolated'\n    ).corr()\n)","6c7ef6e6":"df.filter(\n    ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts','MntSweetProducts', 'MntGoldProds','Incomes_Interpolated',\n     'Year_Birth_Interpolated','Teenhome','Kidhome','Marital_Status_grouped']\n).assign(\n    teenhome_add_kidhome = lambda df: df.Teenhome.add(df.Kidhome),\n    MntWines = lambda df: np.cbrt(df.MntWines),\n    MntMeatProducts = lambda df: np.cbrt(df.MntMeatProducts)\n).query(\n    'Year_Birth_Interpolated == Year_Birth_Interpolated'\n).plot.scatter(\n    x='MntWines',\n    y='MntMeatProducts'\n)","bae7a209":"Looking at histograms, we have two things to take into consideration:\n\n1. `Year_Birth` has values previous 1940. That is weird because that means customers older than 80 years and life expectancy, at least in north america, is roughly 80 years.\n2. `Income` has very extream values, that is easy to see because incomes shouldn't have such long right tail (left tails would be even weird, we don't use to have negative income)\n\nThe next step would be look into these extream values, and decide wether keep or remove them. To do this analysis, my best option is do a 'close up' and see what we found there.","9db528fc":"### Interpolating `Income`\n\nAs we can see at the scatter plots, `MntMeatProducts` and `NumCatalogPurchases` seem to have a non linear relationship with `Income`. And a really detailed one for `MntMeatProducts`. Starting from here, we are going to develop a model that reproduce this behaviour to do an more precise interpolation on `Income`.","69d6122e":"# Data EDA & preparation\n\nLet's start by exploring values related to customers, according with the variables descriptions the attributed related to customers are \n- ID: Customer's unique identifier\n- Year_Birth: Customer's birth year\n- Education: Customer's education level\n- Marital_Status: Customer's marital status\n- Income: Customer's yearly household income\n- Kidhome: Number of children in customer's household\n- Teenhome: Number of teenagers in customer's household\n- Dt_Customer: Date of customer's enrollment with the company\n- Recency: Number of days since customer's last purchase\n- Complain: 1 if customer complained in the last 2 years, 0 otherwise\n\nWhat I'd like to find insights that lead us to a comprehension of the data. That is, find customers characteristics.\n\nI use to start my explorations on dates and amounts because, under my experience, there is always easy-to-find extream values.","f87a735d":"To start modeling, I'll remove outlier from both `Income` and `MntMeatProducts` to let the model focuse on the really important things. An easy technique to remove outliers is using IQR.\n\n> Quick refresh: IQR is defined as $Q_{75}-Q_{25}$ Then removing outliers is as simple as filtering values lower than $Q_{75}+3*IQR$ and, in case you need it, values greater than $Q_{25}-3*IQR$","5eb1a48b":"#### Conclusion so far about `Year_Birth`\n\nAt this point, `Year_Birth` is the most complex variable to predict and the effort would not be worthy. However, from previous analysis it's possible to conclude a couple of things:\n\n1. The variable has a strong correlations to `Keedhome` and `Teenhome` only when $$\\frac{\\sqrt[3]{Teenhome - Kidhome}}{\\ln(\\frac{Income}{Members Marital Status})} \\neq 0$$\n2. There's not sign of relationship between those that correlate and those that don't\n3. $Teenhome - Kidhome$ doesn't produce the same results as the formula described:\n    - I was thinking that the formula could be reduced to just $Teenhome - Kidhome$, but the problem is that we lost a degree of fredom, becoming the scatterplot into something more discreate. This is not a good idea if the goal is to predict `Year_Birth`\n\n>How do we get the formula in the first point?\n\n>It stated by trying to figured out a set of variables interation that lead to a strong correlation with `Year_Birth`. Having this in mind, I started creating two differente set interactions using just adds and subs between `Kidhome` and `Teenhome` because they have the most correlations to the dependant variable. Then, I look for a more complex correlation once I saw that $Teenhome - Kidhome$ improves correlation.\n\n>This new set of variables interaction includes divisions and products between the second variable with a correlation greater than -0.1 (`Income_Interpolated`) and one that I though should work (`Marital_Status`). Although, `Marital_Status` is a categorical variable, I assume a number of members based on experience and create the following rule:\n\n>If `Marital_Status` is `Single` or `Divorced` or `Non common` Then return 1 Else If `Marital_Status` is `Together` or `Married` Then return 2\n\n>Now, I could divide and multiply between `Marital_Status` and `Income_Interpolated`.\n\n>Finally, both variables are in different scales, so I convert them using cubic root for  $Teenhome - Kidhome$ and natural log $\\frac{Income Interpolated}{Members Marital Status}$ and perform division and multiplication between those two and select the best one.\n\n>Why cubic root for $Teenhome - Kidhome$? Because logarithm returns $-inf$ if a zero is given,\n\n>Why natural log for $\\frac{Income Interpolated}{Members Marital Status}$? becuase it returns a better correlation than using cubit root even this last one results in a better normal distribution\n\n# Customer patterns\n\nOnce all this was performed, then it's time to research about how customers behaviour and try to determine patterns.\n\nLet's start by seeing at customer's items bought and the characteristics the each customer has. Maybe we can identify a pattern of purchase\n\nAt the correlation map, we can see a strong correlation between `MntWines` and `MntMeatProducts`. Also, `MntMeatProducts` relate with all `Mnt` variable excepting `MntGoldProds`.","e6ab3c58":"### Interpolate `Year_Birth`\n\nThis will be hard, because `Year_Birth` doesn't give any insight to somewhere to look at for a correlation. But we can look at some interactions and try to find a linear relation that performs well in predictions `Year_Birth`.\n\nInstead of doing combination by combination, we can take the ones with the best correlation, either positive or negative. In that case, `Kidhome` and `Teenhome` are the two best correlated.","915e7d03":"## Customers greater than 80 years old\n\nWhat we're doing next, is understanding those values beyond 1940. As we can see, there are 3 customers greater than 100 years when they made their last purchase.\n\nI really don't know if someone greater than 100 years would go to a store and buy they groceries, but they are clearly outliers and don't require much processing to be removed (they just represent 4 records).\n\nMaybe these values are errors during capturing, so I'll leave them and see in a further analysis doing a interpolation and change those values or remove them.","0628b5f5":"For `Income` there are two positive candidates, `MntMeatProducts` and `NumCatalogPurchases`. In fact the correlations can be considered strong, the scatter plot shows a non linear relationship. We can try to fit a linear regressor and analysis or look for non linear method to solve this task.\n\nHowever, `Year_Birth` hasn't the same luck. But not everything is lost, we can try to plot all its relationships looking for one non linear, at least.","23a0f5d7":"Now that we define variables that could help, let's start by doing simple math operation between `Teenhome` and `Kidhome`, also I include interactions between the result of the two with `Income_Interpolated` and `Marital_Status_grouped`.\n\nFor this last one, I assume a members on the marital status, that is:\n\n- Single: 1\n- Together: 2\n- Married: 2\n- Divorced: 1\n- Non Common (includes Widow,Alone, Absurd, YOLO): 1\n\nThe results shows that subtracting `Kidhome` from `Teenhome` results in a greater improvement of correlation with `Year_Birth_Interpolated`","f01b181c":"## Marital Status Exploration\n\nIn fact, married is the most popular status, it's has lower variance than together or even single.\n\nTogether, Married and Divorced has nearly the same median, Single is a bit lower than those three. Widow, Alone, Absurd and YOLO are goind to be grouped, this way we can reduce a bit the variance inside the data.","faf8d056":"Once outliers are no more in the way, we can prepare an easy regression model using `statsmodels`. The way we are doing this is by creating non linear relationship apply a `log` operator over `MntMeatProducts`. How I know this? Well, take your data and try transformations that roughtly look like your desire plot. Surely their will be better ways to do that, and I'll love to read about it if you'd like to share it :)","04957c05":"# Next steps\n\n## Interpolation\n\nI'm not really worry about year birth outliers, because they are just 4 records. But income outliers could be a problem, from our 2,240 records they represent 10% of the data, that's an issue in such a small dataset. Hopefully we can do something about those 2 problems.\n\nFirst of all, I'll remove that 666,666 income (it's a record) and the empty ones (I didn't notice earlier). Leaving the Year_Birth outliers to perform a simple interpolation.\n\nBefore anything else, let's understand customers attributes in general and particular. Then we can propose a value and confirm it using a ML model.","06e9a5d6":"## Customer with high incomes\n\nNext, exploring incomes. Surely, we'll find extream values with low representation inside the database. We're looking for high representative populations to do a correct segmentations, and extream values are problematic to reach this goal.\n\nThe first thing to do is a close up to values beyond a threshold, the threshold can be set looking at percentiles 5, 10, 90 and 95. There we have as lower yearly income as 1,730 and as higher as 666,666 (we should see why is this value, maybe is a way to represent emptiness).\n\nThe close up I choose is from percentile 90 but lower than 666,666, results shows outliers just in `Year_Birth`.","5e1df44d":"## Recency Exploration\n\nContrary to the other variables, this seems to be more equally distributed. What we are looking here is break points that give us some insights to split the data.","97706f3d":"## Infering missings\n\nI'm plotting the missing from both `Income` and `Year_Birth`. The result shows when `Year_Birth` is empty, `Income` is not and vice versa.\n\nNow, the second task is to identify a correlation that lead us to a model to interpolate those missing values. The perfect tool for this task is a heatmap."}}