{"cell_type":{"f2d55c57":"code","45d472bf":"code","bb9a1b90":"code","96bbcbb1":"code","aa5d45c0":"code","33d58998":"code","6c10763d":"code","30887f23":"code","7306b6f3":"code","9dd1010b":"code","e4703cea":"code","3405c3b3":"code","0a938645":"code","907522ee":"code","367da0ea":"code","a41a0854":"code","7566f0b3":"code","5aa5c6c9":"code","bf28494a":"code","d5a80179":"code","87553b82":"code","7ac4124f":"code","6ea94fea":"code","bc2436f4":"code","29894b47":"code","4f953df4":"code","3b8e1647":"code","ee9adb34":"code","ffecd0b4":"code","1c7f819e":"code","0675faee":"code","a40b68c0":"code","db902773":"code","2f988f8a":"code","649e558e":"code","6e22feba":"code","7d846989":"code","15f73259":"code","3777c81c":"code","555f6b1d":"code","179a62e7":"code","982a16c7":"code","42037c80":"code","1e84195b":"code","2ba35fc4":"code","0fd58ae2":"code","c802b4fa":"code","b81a3104":"code","58e79a42":"code","f772bec9":"code","b70fbaa7":"code","a0963cd2":"code","d70871c0":"code","b43df177":"code","c238483f":"code","c3d85a7b":"markdown","1ee2c6fe":"markdown","7c9a5c6d":"markdown","88cc88be":"markdown","ac0caa46":"markdown","7113a62a":"markdown","f6d19099":"markdown","f13a2aa3":"markdown","b64be0da":"markdown","4a7bec28":"markdown","1c337613":"markdown","c7b69642":"markdown","1b401a73":"markdown","1ae4a1c7":"markdown","bd33f2ba":"markdown","28151427":"markdown","f63ffa7e":"markdown","224bd5e8":"markdown","007057c2":"markdown","f59b17d5":"markdown","72571b73":"markdown","0afac98f":"markdown","8a298117":"markdown","dbdf53ea":"markdown","8be9785e":"markdown","96d431dd":"markdown","071a5923":"markdown","7665bf4f":"markdown"},"source":{"f2d55c57":"import os\nimport fnmatch\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.applications.resnet50 import preprocess_input\nfrom keras.preprocessing import image\nnp.random.seed(21)\n\n\n\nimport tensorflow as tf\nfrom keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,BatchNormalization,Activation,Dropout,GlobalAveragePooling2D\nfrom keras.models import Sequential,Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\n\n\na = '\/kaggle\/input\/plant-seedlings-classification'\n\nprint(os.listdir(os.path.join(a,'train')))","45d472bf":"    from sklearn.metrics import accuracy_score\n","bb9a1b90":"def get_train_data(root):\n    \n    \"\"\"Performs required pre processing on the input images and fetches data\n\n    Args:\n      root: Directory in which we are working\n\n    Returns: \n      train_img: A numpy array consisting of train images\n      train_y : A OHE numpy array of train labels\n    \"\"\"\n    train_dir = (os.path.join(root,'train'))\n    train_label = []\n    train_img = []\n    label2num = {'Loose Silky-bent':0, 'Charlock':1, 'Sugar beet':2, 'Small-flowered Cranesbill':3,\n                 'Common Chickweed':4, 'Common wheat':5, 'Maize':6, 'Cleavers':7, 'Scentless Mayweed':8,\n                 'Fat Hen':9, 'Black-grass':10, 'Shepherds Purse':11}\n    for i in os.listdir(train_dir):\n        label_number = label2num[i]\n        new_path = os.path.join(train_dir,i)\n        for j in fnmatch.filter(os.listdir(new_path), '*.png'):\n            temp_img = image.load_img(os.path.join(new_path,j), target_size=(128,128))\n            train_label.append(label_number)\n            temp_img = image.img_to_array(temp_img)\n            train_img.append(temp_img)\n        print(i)\n    train_img = np.array(train_img)\n\n    train_y=pd.get_dummies(train_label)\n    train_y = np.array(train_y)\n    train_img=preprocess_input(train_img)\n    \n    return train_img,train_y\n\n","96bbcbb1":"train_img,train_y = get_train_data(a)\nprint('Training data shape: ', train_img.shape)\nprint('Training labels shape: ', train_y.shape)","aa5d45c0":"from sklearn.model_selection import train_test_split\nX_train, X_valid, Y_train, Y_valid=train_test_split(train_img,train_y,test_size=0.1, random_state=42)","33d58998":"from keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n        rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=True)  # randomly flip images\n\ndatagen.fit(X_train)\n","6c10763d":"def vgg16_model(num_classes=None):\n    \n    \"\"\" Adding custom model to the VGG-16\n\n    Args:\n      num_classes: Number of layers in the final layer(Number of classes)\n\n    Returns:\n      model: Returns the custom model added to VGG\n    \"\"\"\n\n    model = VGG16(weights='imagenet', include_top=False,input_shape=(128,128,3))\n    model.layers.pop()\n    model.layers.pop()\n    model.layers.pop()\n\n    model.outputs = [model.layers[-1].output]\n\n    #model.layers[-2].outbound_node= []\n    x=Conv2D(256, kernel_size=(2,2),strides=2)(model.output)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)    \n    x=Conv2D(128, kernel_size=(2,2),strides=1)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x=Flatten()(x)\n    x=Dense(num_classes, activation='softmax')(x)\n\n    model=Model(model.input,x)\n\n    for layer in model.layers[:15]:\n\n        layer.trainable = False\n\n\n    return model","30887f23":"from keras.applications.vgg16 import VGG16\nfrom keras import backend as K\nnum_classes=12\nmodel = vgg16_model(num_classes)\nmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","7306b6f3":"from keras.callbacks import ModelCheckpoint\nepochs = 10\nbatch_size = 32\nmodel_checkpoint = ModelCheckpoint('vgg_weights.h5', monitor='val_accuracy', save_best_only=True,mode='max',verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=7, min_lr=0.000001)\n#early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=7, verbose=0, mode='min', restore_best_weights=True)\n\nmodel.fit(datagen.flow(X_train,Y_train),\n          batch_size=128,\n          epochs=20,\n          verbose=1, shuffle=True, validation_data=(X_valid,Y_valid), callbacks=[model_checkpoint,reduce_lr])","9dd1010b":"import pathlib\nimport time","e4703cea":"tflite_models_dir = pathlib.Path(os.path.join(os.getcwd(),'tflite_models'))\ntflite_models_dir.mkdir(exist_ok=True, parents=True)","3405c3b3":"converter_vgg = tf.lite.TFLiteConverter.from_keras_model(model)\n","0a938645":"# Convert to TF Lite without quantization\nvgg16_tflite_file = tflite_models_dir\/\"vgg16.tflite\"\nvgg16_tflite_file.write_bytes(converter_vgg.convert())","907522ee":"interpreter = tf.lite.Interpreter(model_path='.\/tflite_models\/vgg16.tflite')\ninput_type = interpreter.get_input_details()[0]['dtype']\nprint('input: ', input_type)\noutput_type = interpreter.get_output_details()[0]['dtype']\nprint('output: ', output_type)","367da0ea":"def run_tflite_model(tflite_file, test_image_indices):\n    global X_valid\n\n  # Initialize the interpreter\n    interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()[0]\n\n    predictions = np.zeros((len(test_image_indices),), dtype=int)\n    for i, test_image_index in enumerate(test_image_indices):\n        test_image = X_valid[test_image_index]\n        test_label = Y_valid[test_image_index]\n\n        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n        interpreter.set_tensor(input_details[\"index\"], test_image)\n        interpreter.invoke()\n        output = interpreter.get_tensor(output_details[\"index\"])[0]\n\n        predictions[i] = output.argmax()\n\n    return predictions","a41a0854":"def evaluate_model(tflite_file, model_type):\n    global X_valid\n    global Y_valid\n\n    test_image_indices = range(X_valid.shape[0])\n    predictions = run_tflite_model(tflite_file, test_image_indices)\n    \n    predictions=pd.get_dummies(predictions)\n    predictions = np.array(predictions)\n    #print(predictions)\n    \n    accuracy = accuracy_score(y_true=Y_valid, y_pred=predictions) \n\n    #accuracy = (np.sum(Y_valid== predictions) * 100) \/ len(X_valid)\n\n    return accuracy","7566f0b3":"op = evaluate_model(vgg16_tflite_file, model_type=\"Float\")\nprint(\"Accuracy of TFLite - VGG : {}\".format(op))","5aa5c6c9":"from tensorflow.python.compiler.tensorrt import trt_convert as trt\n","bf28494a":"model.save('vgg_saved_model')","d5a80179":"print('Converting to TF-TRT FP32...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32,\n                                                               max_workspace_size_bytes=8000000000)\n\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir='vgg_saved_model',\n                                    conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='vgg_saved_model_TFTRT_FP32')\nprint('Done Converting to TF-TRT FP32')","87553b82":"batch_size = 8\nbatched_input = np.zeros((batch_size, 128, 128, 3), dtype=np.float32)\n\nfor i in range(batch_size):\n    img_path = '..\/input\/plant-seedlings-classification\/train\/Common Chickweed'\n    for j in os.listdir(img_path):\n        img = image.load_img(os.path.join(img_path,j), target_size=(128, 128))\n        x = image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        x = preprocess_input(x)\n        batched_input[i, :] = x\nbatched_input = tf.constant(batched_input)\nprint('batched_input shape: ', batched_input.shape)","7ac4124f":"from tensorflow.python.saved_model import tag_constants\n\ndef benchmark_tftrt(input_saved_model):\n    saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n    infer = saved_model_loaded.signatures['serving_default']\n\n    N_warmup_run = 50\n    N_run = 500\n    elapsed_time = []\n\n    for i in range(N_warmup_run):\n        labeling = infer(batched_input)\n\n    for i in range(N_run):\n        start_time = time.time()\n        labeling = infer(batched_input)\n        #prob = labeling['probs'].numpy()\n        end_time = time.time()\n        elapsed_time = np.append(elapsed_time, end_time - start_time)\n        if i % 50 == 0:\n            print('Step {}: {:4.1f}ms'.format(i, (elapsed_time[-50:].mean()) * 1000))\n\n    print('Throughput: {:.0f} images\/s'.format(N_run * batch_size \/ elapsed_time.sum()))","6ea94fea":"benchmark_tftrt('vgg_saved_model_TFTRT_FP32')","bc2436f4":"print('Converting to TF-TRT FP16...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.FP16,\n    max_workspace_size_bytes=8000000000)\nconverter = trt.TrtGraphConverterV2(\n   input_saved_model_dir='vgg_saved_model', conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='vgg_saved_model_TFTRT_FP16')\nprint('Done Converting to TF-TRT FP16')","29894b47":"benchmark_tftrt('vgg_saved_model_TFTRT_FP16')","4f953df4":"print('Converting to TF-TRT INT8...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.INT8, \n    max_workspace_size_bytes=8000000000, \n    use_calibration=True)\nconverter = trt.TrtGraphConverterV2(\n    input_saved_model_dir='vgg_saved_model', \n    conversion_params=conversion_params)\n\ndef calibration_input_fn():\n    yield (batched_input, )\nconverter.convert(calibration_input_fn=calibration_input_fn)\n\nconverter.save(output_saved_model_dir='vgg_saved_model_TFTRT_INT8')\nprint('Done Converting to TF-TRT INT8')","3b8e1647":"benchmark_tftrt('vgg_saved_model_TFTRT_INT8')","ee9adb34":"def resnet_model(num_classes=None):\n    \n    \"\"\" Adding custom model to the ResNet50\n\n    Args:\n      num_classes: Number of layers in the final layer(Number of classes)\n\n    Returns:\n      model: Returns the custom model added to ResNet\n    \"\"\"\n\n    model = ResNet50(weights='imagenet', include_top=False,input_shape=(128,128,3))\n    model.layers.pop()\n    #model.layers.pop()\n    #model.layers.pop()\n\n    model.outputs = [model.layers[-1].output]\n\n    #model.layers[-2].outbound_node= []\n    x=Conv2D(256, kernel_size=(2,2),strides=2)(model.output)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)    \n    x=Conv2D(128, kernel_size=(2,2),strides=1)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x=Flatten()(x)\n    x=Dense(num_classes, activation='softmax')(x)\n\n    model=Model(model.input,x)\n\n    for layer in model.layers[:15]:\n\n        layer.trainable = False\n\n\n    return model","ffecd0b4":"from keras.applications.resnet50 import ResNet50\nfrom keras import backend as K\nnum_classes=12\nmodel_res = resnet_model(num_classes)\nmodel_res.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_res.summary()","1c7f819e":"epochs = 10\nbatch_size = 32\nmodel_checkpoint = ModelCheckpoint('resnet_weights.h5', monitor='val_accuracy', save_best_only=True,mode='max',verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=7, min_lr=0.000001)\n#early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=7, verbose=0, mode='min', restore_best_weights=True)\n\nmodel_res.fit(datagen.flow(X_train,Y_train),\n          batch_size=128,\n          epochs=20,\n          verbose=1, shuffle=True, validation_data=(X_valid,Y_valid), callbacks=[model_checkpoint,reduce_lr])","0675faee":"converter_resnet = tf.lite.TFLiteConverter.from_keras_model(model_res)\n","a40b68c0":"# Convert to TF Lite without quantization\nresnet_tflite_file = tflite_models_dir\/\"resnet.tflite\"\nresnet_tflite_file.write_bytes(converter_resnet.convert())","db902773":"interpreter = tf.lite.Interpreter(model_path='.\/tflite_models\/resnet.tflite')\ninput_type = interpreter.get_input_details()[0]['dtype']\nprint('input: ', input_type)\noutput_type = interpreter.get_output_details()[0]['dtype']\nprint('output: ', output_type)","2f988f8a":"op = evaluate_model(resnet_tflite_file, model_type=\"Float\")\nprint(\"Accuracy of TFLite - Resnet50 : {}\".format(op))","649e558e":"model_res.save('res_saved_model')","6e22feba":"print('Converting to TF-TRT FP32...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32,\n                                                               max_workspace_size_bytes=8000000000)\n\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir='res_saved_model',\n                                    conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='resnet_saved_model_TFTRT_FP32')\nprint('Done Converting to TF-TRT FP32')","7d846989":"benchmark_tftrt('resnet_saved_model_TFTRT_FP32')","15f73259":"print('Converting to TF-TRT FP16...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.FP16,\n    max_workspace_size_bytes=8000000000)\nconverter = trt.TrtGraphConverterV2(\n   input_saved_model_dir='res_saved_model', conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='res_saved_model_TFTRT_FP16')\nprint('Done Converting to TF-TRT FP16')","3777c81c":"benchmark_tftrt('res_saved_model_TFTRT_FP16')","555f6b1d":"print('Converting to TF-TRT INT8...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.INT8, \n    max_workspace_size_bytes=8000000000, \n    use_calibration=True)\nconverter = trt.TrtGraphConverterV2(\n    input_saved_model_dir='res_saved_model', \n    conversion_params=conversion_params)\n\ndef calibration_input_fn():\n    yield (batched_input, )\nconverter.convert(calibration_input_fn=calibration_input_fn)\n\nconverter.save(output_saved_model_dir='resnet_saved_model_TFTRT_INT8')\nprint('Done Converting to TF-TRT INT8')","179a62e7":"benchmark_tftrt('resnet_saved_model_TFTRT_INT8')","982a16c7":"def incep_model(num_classes=None):\n    \n    \"\"\" Adding custom model to the InceptionV3\n\n    Args:\n      num_classes: Number of layers in the final layer(Number of classes)\n\n    Returns:\n      model: Returns the custom model added to Inception\n    \"\"\"\n\n    model = InceptionV3(weights='imagenet', include_top=False,input_shape=(128,128,3))\n    \n    x = model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(0.3)(x)\n    x = Dense(512)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(0.3)(x)\n\n    predictions = Dense(12, activation='softmax')(x)\n\n    model = Model(model.input, predictions)\n\n\n\n\n    return model","42037c80":"from keras.applications.inception_v3 import InceptionV3\nfrom keras import backend as K\nnum_classes=12\nmodel_incep = incep_model(num_classes)\nmodel_incep.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_incep.summary()","1e84195b":"epochs = 10\nbatch_size = 32\nmodel_checkpoint = ModelCheckpoint('incep_weights.h5', monitor='val_accuracy', save_best_only=True,mode='max',verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=7, min_lr=0.000001)\n#early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=7, verbose=0, mode='min', restore_best_weights=True)\n\nmodel_incep.fit(datagen.flow(X_train,Y_train),\n          batch_size=128,\n          epochs=20,\n          verbose=1, shuffle=True, validation_data=(X_valid,Y_valid), callbacks=[model_checkpoint])","2ba35fc4":"converter_incep = tf.lite.TFLiteConverter.from_keras_model(model_incep)","0fd58ae2":"# Convert to TF Lite without quantization\nincep_tflite_file = tflite_models_dir\/\"incep.tflite\"\nincep_tflite_file.write_bytes(converter_incep.convert())","c802b4fa":"interpreter = tf.lite.Interpreter(model_path='.\/tflite_models\/incep.tflite')\ninput_type = interpreter.get_input_details()[0]['dtype']\nprint('input: ', input_type)\noutput_type = interpreter.get_output_details()[0]['dtype']\nprint('output: ', output_type)","b81a3104":"op = evaluate_model(incep_tflite_file, model_type=\"Float\")\nprint(\"Accuracy of TFLite - InceptionV3 : {}\".format(op))","58e79a42":"model_incep.save('incep_saved_model')","f772bec9":"print('Converting to TF-TRT FP32...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32,\n                                                               max_workspace_size_bytes=8000000000)\n\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir='incep_saved_model',\n                                    conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='incep_saved_model_TFTRT_FP32')\nprint('Done Converting to TF-TRT FP32')","b70fbaa7":"benchmark_tftrt('incep_saved_model_TFTRT_FP32')","a0963cd2":"print('Converting to TF-TRT FP16...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.FP16,\n    max_workspace_size_bytes=8000000000)\nconverter = trt.TrtGraphConverterV2(\n   input_saved_model_dir='incep_saved_model', conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='incep_saved_model_TFTRT_FP16')\nprint('Done Converting to TF-TRT FP16')","d70871c0":"benchmark_tftrt('incep_saved_model_TFTRT_FP16')","b43df177":"print('Converting to TF-TRT INT8...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.INT8, \n    max_workspace_size_bytes=8000000000, \n    use_calibration=True)\nconverter = trt.TrtGraphConverterV2(\n    input_saved_model_dir='incep_saved_model', \n    conversion_params=conversion_params)\n\ndef calibration_input_fn():\n    yield (batched_input, )\nconverter.convert(calibration_input_fn=calibration_input_fn)\n\nconverter.save(output_saved_model_dir='incep_saved_model_TFTRT_INT8')\nprint('Done Converting to TF-TRT INT8')","c238483f":"benchmark_tftrt('incep_saved_model_TFTRT_INT8')","c3d85a7b":"## **Part 3- Transfer Learing**","1ee2c6fe":"#### Model Optimization Using TF-Lite(Post Training Dynamic range quantization) ","7c9a5c6d":"### Optimization of Resnet Model with TensorRT","88cc88be":"#### **Customizing ResNet50 for our problem statement**","ac0caa46":"## **Part 2: Data Augmentation**\n","7113a62a":"#### **Customizing InceptionV3 for our problem statement**","f6d19099":"#### Model Optimization Using TF-Lite(Post Training Dynamic range quantization) ","f13a2aa3":"## **Part 1: Loading,Transforming and One Hot Encoding**\n","b64be0da":"#### **Training InceptionV3**","4a7bec28":"### **1) VGG-16**","1c337613":"### **2) ResNet50**","c7b69642":"#### TF-TRT FP32 model","1b401a73":"#### TF-TRT INT8 model (with calibration)","1ae4a1c7":"#### TF-TRT FP32 model","bd33f2ba":"#### TF-TRT INT8 model (with calibration)","28151427":"#### **Training ResNet50**","f63ffa7e":"#### **Customizing VGG-16 for our problem statement**","224bd5e8":"#### TF-TRT INT8 model (with calibration)","007057c2":"#### TF-TRT FP32 model","f59b17d5":"#### TF-TRT FP16 model","72571b73":"#### Optimization of InceptionV3 Model with TensorRT","0afac98f":"#### TF-TRT FP16 model","8a298117":"#### TF-TRT FP16 model","dbdf53ea":"### **3) InceptionV3**","8be9785e":"#### Model Optimization Using TF-Lite(Post Training Dynamic range quantization) ","96d431dd":"#### **Training VGG-16**","071a5923":"# **Optimize Model Performance Using TF-TRT and TensorFlow Model Optimization Toolkit**\n\n\n\n\n\n","7665bf4f":"### Optimization of VGG Model with TensorRT"}}