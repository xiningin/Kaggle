{"cell_type":{"2c9c1b79":"code","2ce2e66a":"code","4a19318a":"code","697514b6":"code","dcc226a4":"code","21d3e9a8":"code","154192a6":"markdown"},"source":{"2c9c1b79":"import tensorflow.keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport csv","2ce2e66a":"def load_dataset(flatten=False):\n    (X_train, y_train), (X_test, y_test) = tensorflow.keras.datasets.mnist.load_data()\n    \n    # normalize x\n    X_train = X_train.astype(float) \/ 255.0\n    X_test = X_test.astype(float) \/ 255.0\n\n    # we reserve the last 10,000 training examples for validation\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n\n    if flatten:\n        X_train = X_train.reshape([X_train.shape[0], -1])\n        X_val = X_val.reshape([X_val.shape[0], -1])\n        X_test = X_test.reshape([X_test.shape[0], -1])\n\n    return (X_train, y_train), (X_val, y_val), (X_test, y_test)","4a19318a":"def save_y_pred(filename, y):\n    with open(filename + '.csv', mode='w') as csv_file:\n        csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        csv_writer.writerow(['Id', 'Category'])\n        for index in range(y_test.shape[0]):\n            csv_writer.writerow([str(index), str(y[index])])","697514b6":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom tensorflow.python.framework.ops import Tensor\n\n# *****Tips*****\n# 1. To achieve high accuracy use stddev=1\/sqrt(float(num_layer_inputs)), for variable tensors\n# 2. Note that the W and b are tuple of tensors\ndef mlp(num_inputs: int, num_outputs: int, num_layer_1: int, num_layer_2: int) -> (Tensor, Tensor, Tensor, tuple, tuple):\n    X = ___\n    y = ___\n    y_pred = ___\n    W = ___\n    b = ___\n\n    return X, y, y_pred, W, b","dcc226a4":"(X_train, y_train), (X_val, y_val), (X_test, y_test) = load_dataset()\nprint(X_train.shape, y_train.shape)\nplt.imshow(X_train[0], cmap=\"Greys\");\n\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\nX_val = X_val.reshape((X_val.shape[0], X_val.shape[1]*X_val.shape[2]))\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\nprint(f'Training dim: {X_train.shape}, Val dim: {X_val.shape}, Test dim: {X_test.shape}')\n\nlb = LabelBinarizer()\ny_train = lb.fit_transform(y_train)\ny_val = lb.transform(y_val)\ny_test = lb.transform(y_test)\nprint(f'Train lbl dim: {y_train.shape}, Val lbl dim: {y_val.shape}, Test lbl dim: {y_test.shape}')\n\nnum_classes = y_train.shape[1]\nnum_features = X_train.shape[1]\nnum_output = y_train.shape[1]\nnum_layer_0 = 512\nnum_layer_1 = 256\nstarter_learning_rate = 1e-3\nregularizer_rate = 1e-1  ","21d3e9a8":"X, y, y_pred, w, b = mlp(num_features, num_output, num_layer_0, num_layer_1)\nbias_sum = tf.reduce_sum(tf.square(b[0]))\nfor i in range(1, len(b)-1):\n    bias_sum = bias_sum + tf.reduce_sum(tf.square(b[i]))\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred, labels=y)) \\\n      + regularizer_rate * bias_sum\n\nvar_list = w + b\nlearning_rate = tf.train.exponential_decay(starter_learning_rate, 0, 5, 0.85, staircase=True)\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=var_list)\n\ncorrect_prediction = tf.equal(tf.argmax(y_train, 1), tf.argmax(y_pred, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nbatch_size = 128\n# *****Tips*****\n# 1. For debug, use epochs=1. After model is done, use epochs=12\nepochs = 12\n\ntraining_accuracy = []\ntraining_loss = []\ntesting_accuracy = []\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for e in range(epochs):\n        arr = np.arange(X_train.shape[0])\n        np.random.shuffle(arr)\n        for i in range(0, X_train.shape[0], batch_size):\n            sess.run(optimizer, {X: X_train[arr[i:i+batch_size]], y: y_train[arr[i:i+batch_size]]})\n        training_accuracy.append(sess.run(accuracy, feed_dict={X: X_train, y: y_train}))\n        training_loss.append(sess.run(loss, {X: X_train, y: y_train}))\n\n        y_pred_val = sess.run(y_pred, {X: X_test})\n\n        save_y_pred('y_pred', y_pred_val.argmax(1))\n\n        testing_accuracy.append(accuracy_score(y_test.argmax(1), y_pred_val.argmax(1)))\n        print(f'Ephoch: {e}, Train loss: {training_loss[e]:.2f}, Train acc: {training_accuracy[e]:.3f}, Test acc: {testing_accuracy[e]:.3f}')","154192a6":"## Fun\u00e7\u00e3o a ser implementada `mlp`"}}