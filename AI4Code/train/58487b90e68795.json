{"cell_type":{"51487b40":"code","9d96e9d8":"code","2091893c":"code","36e3c1b9":"code","4eda78c5":"code","2dfb7862":"code","20d4e1a5":"code","f0b91c3c":"code","7f38c95c":"code","33d35e3f":"code","5af577ee":"code","0ebb4ee2":"code","302b5b62":"code","d3f91a06":"code","ab21b7aa":"code","862475c3":"code","204abfb4":"code","6077883c":"code","52be6846":"code","7450554d":"code","a567e6d5":"code","800fb647":"code","4752a07b":"markdown","8bda144b":"markdown","6e4a9c57":"markdown","68b1d6c4":"markdown","51613cbe":"markdown","a4139155":"markdown"},"source":{"51487b40":"import graphviz\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import tree\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay, plot_confusion_matrix, f1_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV, RepeatedStratifiedKFold\n\n\ndf1 = pd.read_csv('..\/input\/pittsburgh-bridges-data-set\/bridges1.csv')\ndf2 = pd.read_csv('..\/input\/pittsburgh-bridges-data-set\/bridges2.csv')","9d96e9d8":"df2","2091893c":"X = df2.iloc[:, 1:8].drop('location', axis=1)\nY = df2.drop(X.columns.to_list() + ['location'], axis=1).columns.to_list()\nY.remove('Id')\n\nY = [df2[feat] for feat in Y]","36e3c1b9":"for y in Y: \n    print(y.value_counts())","4eda78c5":"df2[df2['type'] == 'NIL']","2dfb7862":"Y = [y.drop(63) for y in Y]\nX = X.drop(63)\n\nsns.heatmap(X.isnull());","20d4e1a5":"Ord = OrdinalEncoder()\nmask = X.drop('lanes', axis = 1).isnull().to_numpy()\nXc = X.drop('lanes', axis = 1).fillna('~')\n\nOrd.fit(Xc)\n\ncat_ord = Ord.transform(Xc)\n\nfor i in range(0, 5):\n    \n    if len(cat_ord[mask[:, i], i]) > 0:\n        cat_ord[mask[:, i], i] = np.nan\n        \nXc = pd.DataFrame(data = cat_ord, index = Xc.index, columns = Xc.columns)\n\nX.update(Xc)\n\nX","f0b91c3c":"def view_ord():\n    for ord_cat, x in zip(Ord.categories_, Xc):\n        print(f'\\n{x}')\n        for i, o in enumerate(ord_cat):\n            if o != '~':\n                print(f'{o} : {i}')\nview_ord()               ","7f38c95c":"sns.heatmap(X.isnull());","33d35e3f":"X.info()","5af577ee":"for x in X:\n    X[x] = pd.to_numeric(X[x])\n    \nX.info()","0ebb4ee2":"for x in X:\n    \n    if X[x].isnull().sum(axis=0) > 0:\n        \n        s = X.isnull()\n        mask1 = s.sum(axis=0)[s.sum(axis=0) == 0].index.to_list()\n        mask2 = s[x].to_numpy()\n        \n        X_train = X.loc[~mask2, mask1]\n        y_train = X.loc[~mask2, x]\n        \n        X_test = X.loc[mask2, mask1]\n        y_test = X.loc[mask2, x]\n        \n        model = DecisionTreeClassifier()\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        \n        X.loc[y_test.index, x] = y_pred\n        \nX","302b5b62":"clf = DecisionTreeClassifier()\n\nind = Y[0][Y[0].isnull()].index\n\nX_ = X.drop(ind)\ny1 = Y[0].drop(ind)\n\nclf.fit(X_, y1)","d3f91a06":"y_pred = clf.predict(X_)\nprint(classification_report(y1, y_pred))","ab21b7aa":"parameters = {'max_depth': [2, 3, 4, 5, 6, 7],\n              'min_samples_split': [3, 5, 10, 15, 20],\n              'min_samples_leaf': [2, 4, 6, 8]}\n\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5)\n\nresults = {}\n\nfor y in Y:\n    ind = y[y.isnull()].index\n    X_ = X.drop(ind)\n    y_ = y.drop(ind)\n    X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.33, random_state=42)\n    gscv = GridSearchCV(estimator = DecisionTreeClassifier(), param_grid = parameters, cv=cv, scoring ='f1_macro')\n    gscv.fit(X_train, y_train)\n    results[y.name] = (gscv, X_train, X_test, y_train, y_test)\n    print(y.name, gscv.best_params_, gscv.best_score_)\n    \nresults;","862475c3":"fig, axes = plt.subplots(5, 2, figsize=(15, 35))\n\nfor i, y in enumerate(Y):\n    n = y.name\n    axes[i, 0].set_title(f'Train {n}')\n    plot_confusion_matrix(results[n][0].best_estimator_, results[n][1], results[n][3], ax=axes[i, 0]);\n    axes[i, 1].set_title(f'Test {n}')\n    plot_confusion_matrix(results[n][0].best_estimator_, results[n][2], results[n][4], ax=axes[i, 1]);","204abfb4":"view_ord()","6077883c":"def tree_plotting(i):\n    \n    dot_data = tree.export_graphviz(results[Y[i].name][0].best_estimator_, out_file=None, \n                                feature_names=X.columns.to_list(),  \n                                class_names=np.sort(Y[i].dropna().unique()),\n                                filled=True)\n    \n    graph = graphviz.Source(dot_data, format=\"png\") \n    \n    return graph\n    \ntree_plotting(0)  ","52be6846":"tree_plotting(1) ","7450554d":"tree_plotting(2) ","a567e6d5":"tree_plotting(3) ","800fb647":"tree_plotting(4) ","4752a07b":"Despite being robust to missing data, the implementation of CARTs in sci-kit learn does not accept missing data. For that, we are going to use the self Decision Tree to predict missing data and impute it (there some meta-modeling thing going on here).","8bda144b":"In this notebook, the five design properties will be predicted by a Classification Tree Algorithm.","6e4a9c57":"Due to having only one instance of type NIL, where are going to drop it.","68b1d6c4":"It's works well if you use the whole dataset.","51613cbe":"Using OrdinalEncoder only because is not influenced by [ordinality.](http:\/\/https:\/\/datascience.stackexchange.com\/questions\/58745\/ordinal-features-to-decision-tree-in-python)","a4139155":"`f1_macro` was used to avoid missclassifications on other classes due to imbalance"}}