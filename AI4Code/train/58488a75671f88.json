{"cell_type":{"fab6b5ff":"code","255cd484":"code","88ceffd5":"code","c91c72ec":"code","32514f3c":"code","d9e907aa":"code","e755cfe6":"code","109b302c":"code","a773e31e":"code","c86e60e3":"code","057d1cfb":"code","251bc9be":"code","2abbccb5":"code","1e57b20b":"code","ed79c191":"code","06dea77e":"code","0a12d8b6":"code","d482a83c":"code","7fe902f1":"code","b8739a33":"markdown","ace706e9":"markdown","6ff1f363":"markdown","7dd958e8":"markdown","d6408dda":"markdown"},"source":{"fab6b5ff":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import cross_val_score\n\nfrom lightgbm import LGBMClassifier\n\nimport optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nfrom optuna.integration import LightGBMPruningCallback\n\nfrom tqdm import tqdm","255cd484":"train=pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nsub=pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","88ceffd5":"conditions = [\n    (train.target == \"Class_1\"),\n    (train.target == \"Class_2\"),\n    (train.target == \"Class_3\"),\n    (train.target == \"Class_4\")\n]\nchoices = [1, 2, 3, 4]\ntrain[\"target\"] = np.select(conditions, choices)","c91c72ec":"X_test = test.drop(['id'], axis=1)\nX = train.drop(['id', 'target'], axis=1)\ny = train.target","32514f3c":"def kfold_prediction(X, y, X_test, k, hyperparams, fixed_paramns, early_stopping_rounds):\n\n    yp_test = np.zeros([len(X_test), 4])\n    loss_val = 0\n    \n    kf = StratifiedKFold(n_splits=k,random_state=42,shuffle=True)\n    model = LGBMClassifier(**fixed_paramns, **hyperparams)\n    \n    pbar = tqdm(total=K)\n    for i, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n        #print(f\"FOLD {i} ...\", end =\" \")\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n\n        model.fit(X_train, y_train,\n                  eval_set=(X_val, y_val),\n                  early_stopping_rounds=early_stopping_rounds,\n                  verbose=0,\n                  eval_metric=\"multi_logloss\" \n                 )\n        \n        for i in [0,1,2,3]:\n            yp_test[:,i] += model.predict_proba(X_test)[:,i] \/ k\n        \n        yp_val = model.predict_proba(X_val)\n        loss_val += log_loss(y_val, yp_val) \/ k\n        \n        pbar.update(n=1)\n            \n    return loss_val, yp_test","d9e907aa":"fixed_paramns = {\n    'random_state': 314,\n    'n_estimators': 100000, \n    'learning_rate': 0.02,\n    #'boosting_type':'goss',\n    'metric':'multi_logloss'\n}\n\nK = 8\n\nloss, y_pred = kfold_prediction(X, y, X_test, 8, {}, fixed_paramns, early_stopping_rounds = 70)\n\nprint('\\nvalidation loss:', loss)","e755cfe6":"def model_instance(hyperparams, fixedparams):\n\n    clf = LGBMClassifier(**hyperparams['clf'], **fixedparams) \n    \n    return clf","109b302c":"def objective(trial):\n    \n    global X, y, K, fixed_paramns\n    \n    # Default value of tree_depth, used for upper bound of num_leaves.\n    max_depth = trial.suggest_int('max_depth', 2, 12)\n    max_num_leaves = (2 ** max_depth) - 1\n    \n    hyperparams = {\n        #'esr': trial.suggest_int('esr', 30, 100, 10), \n        'clf':{\n            # 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5) \n            'max_depth': max_depth,\n            'num_leaves': trial.suggest_int('num_leaves', 2, max_num_leaves),\n            #'max_bin': trial.suggest_int('max_bin', 32, 255),\n            ##'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 256),\n            ##'min_data_in_bin': trial.suggest_int('min_data_in_bin', 1, 256),\n            'min_split_gain' : trial.suggest_float('min_split_gain', 1e-8, 5, log=True),\n            'reg_alpha': trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n            'reg_lambda': trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.04, 1.0),\n            'subsample': trial.suggest_float('subsample', 0.04, 1.0),\n            'subsample_freq': trial.suggest_int(\"subsample_freq\", 1, 7),\n            'min_child_samples': trial.suggest_int(\"min_child_samples\", 5, 100)\n        }\n\n    }\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=42)\n    \n    model = model_instance(hyperparams, fixed_paramns)\n    cv = StratifiedKFold(n_splits=K,random_state=42,shuffle=True)\n    \n    fit_params = {\n        'eval_set':(X_val, y_val),\n        'early_stopping_rounds': 70, #hyperparams['esr'],\n        'verbose':0,\n        'eval_metric':\"multi_logloss\",\n        'callbacks': [LightGBMPruningCallback(trial, 'multi_logloss')]\n    }\n\n    cv_score = cross_val_score(model,\n                               X_train, y_train, cv=cv,\n                               fit_params=fit_params,\n                               n_jobs=-1, verbose=0,\n                               error_score='raise',\n                               scoring='neg_log_loss')\n\n    return -np.mean(cv_score)","a773e31e":"study = optuna.create_study(direction='minimize',\n                            pruner=optuna.pruners.HyperbandPruner())","c86e60e3":"%%time\n\nstudy.optimize(objective, timeout=60*15, \n               n_trials=None, gc_after_trial=False)","057d1cfb":"study.best_value","251bc9be":"plot_optimization_history(study)","2abbccb5":"optuna.visualization.plot_parallel_coordinate(study)","1e57b20b":"plot_param_importances(study)","ed79c191":"study.best_params","06dea77e":"final_params = dict()\n#final_params['clf']=dict(study.best_params)\n\n# after long train...\nfinal_params['clf'] = {\n    'max_depth': 3,\n    'num_leaves': 6,\n    'min_split_gain': 0.17865452483871047,\n    'reg_alpha': 9.540720621520459,\n    'reg_lambda': 4.5781292529661375,\n    'colsample_bytree': 0.0644950794287173,\n    'subsample': 0.9314592865852914,\n    'subsample_freq': 7,\n    'min_child_samples': 57}","0a12d8b6":"loss, y_pred = kfold_prediction(X, y, X_test, 8,final_params['clf'] ,fixed_paramns, early_stopping_rounds = 70)","d482a83c":"print('\\nvalidation loss:', loss)","7fe902f1":"sub=pd.concat([\n    test.id,\n    pd.DataFrame(y_pred, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4'])\n], axis=1)\n\nsub.to_csv('lgbm_optuna_tpe.csv', index=False)","b8739a33":"# Submission","ace706e9":"# Prepare data","6ff1f363":"# Dependencies","7dd958e8":" # Baseline","d6408dda":"# Problem definition\n\nFrom description:\n\n\"The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\"\n\n\nSee notebooks using R:\n\n1. [Finding the best pre-processing configuration and predictive models based on the original data](https:\/\/www.kaggle.com\/gomes555\/tps-may2021-r-eda-tidymodels-workflowsets\/)\n2. [Create DAE dataset and fit models in DAE data](https:\/\/www.kaggle.com\/gomes555\/tps-may2021-r-dae-keras) \n4. [Stacking all](https:\/\/www.kaggle.com\/gomes555\/tps-may2021-r-tidymodels-stacks\/)\n\nNotebooks using Python:\n\n1. [LightGbm sequencial tuning with Optuna Step-wise by LightGBM Tuner](https:\/\/www.kaggle.com\/gomes555\/tps-may2021-optuna-lightgbm-tuner)\n2. **LightGbm tuning with Optuna TPE (Tree-structured Parzen Estimator)**\n3. [LightGbm tuning one vs rest with Optuna Step-wise by LightGBM Tuner](https:\/\/www.kaggle.com\/gomes555\/tps-may2021-optuna-tuner-one-x-rest)\n4. [LightGbm tuning pseudo label with Optuna Tuner](https:\/\/www.kaggle.com\/gomes555\/tps-may2021-lightgbm-pseudolabel\/)\n5. [Stacking All](https:\/\/www.kaggle.com\/gomes555\/tps-may2021-stacking)\n\nAll notebooks will be public and suggestions and criticism are very welcome!\n\n\n<br>\n\n<p align=\"right\"><span style=\"color:firebrick\">Dont forget the upvote if you liked the notebook! <i class=\"fas fa-hand-peace\"><\/i><\/span> <\/p>"}}