{"cell_type":{"bb766a75":"code","ea2623d5":"code","18c08b0e":"code","20260257":"code","eaa4e9a8":"code","45483711":"code","71d58fe2":"code","16e2b1e3":"code","6b08b38c":"code","264c9bf3":"code","578019e7":"code","f4646547":"code","d5eee779":"code","926ddf65":"markdown","2e6a01a7":"markdown","daef8168":"markdown","c1acd8e7":"markdown","ad494d9a":"markdown","37cc1efc":"markdown"},"source":{"bb766a75":"###Lectura de librerias\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler","ea2623d5":"###Definici\u00f3n de par\u00e1metros para el modelo\nNUM_MODELS = 2\nMAX_FEATURES = 100000 \nBATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4\nMAX_LEN = 220","18c08b0e":"###Las siguientes funciones son empleadas para extraer la informaci\u00f3n de los word vectors en una matriz\n\n###Funci\u00f3n que retorna una matriz con las palabras y el arreglo de dimensiones de cada palabra\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n###Funci\u00f3n que abre el path de word vectors y env\u00eda la salida de get_coefs a un diccionario\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n\n###Funci\u00f3n que retorna una matriz con los vectores (word vectors) de todas las palabras \u00fanicas contenidas en los datasets \n###del caso (train y test)\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","20260257":"###Funci\u00f3n de construcci\u00f3n del modelo\ndef build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    \n    ###Capas de LSTM bidireccional\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","eaa4e9a8":"### Lectura de datasets\ntrain = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')","45483711":"train.describe()","71d58fe2":"###Seleccion de datasets\nx_train = train['comment_text'].fillna('').values\nx_test = test['comment_text'].fillna('').values\n\n###Definici\u00f3n de y_train\ny_train = np.where(train['target'] >= 0.5, 1, 0)\n\n###Extracci\u00f3n de columnas que implican toxicidad para la y auxiliar\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n\n###Extraccion de columnas de identidad para definir los pesos de cada texto en el modelo\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n###Caracteres a borrar\n# CHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'\nCHARS_TO_REMOVE =\"\"","16e2b1e3":"sample_weights = np.ones(len(x_train), dtype=np.float32)\nsample_weights += train[identity_columns].sum(axis=1)\nsample_weights += train['target'] * (train[identity_columns]).sum(axis=1)\nsample_weights += (train['target']) * train[identity_columns].sum(axis=1) * 5\nsample_weights \/= sample_weights.mean()","6b08b38c":"###Crear tokenizaci\u00f3n de todas las palabras en los DF\ntokenizer = text.Tokenizer(num_words=MAX_FEATURES,filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\n###Aplicar tokenizado a los dataframe de variables predictoras\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","264c9bf3":"#crawl-300d-2M es un word vector que contiene un modelo pre entrenado con 2 millones de palabras y 300 dimensiones\n#glove.840B.300d es un word vector que contiene un modelo pre entrenado con 840 billones de palabras repetidas y 300 dimensiones\nEMBEDDING_FILES = ['..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec','..\/input\/glove840b300dtxt\/glove.840B.300d.txt']","578019e7":"embedding_matrix = np.concatenate([build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n\ncheckpoint_predictions = []\nweights = []","f4646547":"###Correr modelo LSTM\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            x_train,\n            [y_train, y_aux_train],\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=2,\n            sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n            callbacks=[\n                LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch))\n            ]\n        )\n        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n        weights.append(2 ** global_epoch)","d5eee779":"###Hacer predicciones y enviar modelo a CSV\npredictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n\nsubmission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)","926ddf65":"# Definici\u00f3n de funciones y par\u00e1metros del modelo","2e6a01a7":"Para el desarrollo del modelo se hacen uso de 2 modelos preentrenados denominados word vectors, los cuales traen un gran numero de palabras decodificadas en vectores. Estos vectores dan un peso a estas palabras respecto a la relevancia de cada palabra en las categor\u00edas en las que fueron entrenados en los datasets.","daef8168":"### DataFrames de los Word Vectors","c1acd8e7":"# Preprocesamiento de datos","ad494d9a":"# Entrenamiento","37cc1efc":"### DataFrames de Train y Test"}}