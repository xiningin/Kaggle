{"cell_type":{"d7d192f9":"code","97e63504":"code","87e6a55b":"code","19a2fa80":"code","f79f9e01":"code","d913a85d":"code","ded6e083":"code","6874f468":"code","b3672bc3":"code","4af3ba77":"code","b5fdda23":"code","53ace48e":"code","c4992245":"code","83d50eb8":"code","c7dd7e74":"code","e6759be8":"code","66979163":"code","cffcc6d5":"code","f5fd76a2":"code","d1c8229c":"code","119ca4ff":"code","2af6ad7a":"code","b0075a41":"code","96000113":"code","4d3fee60":"code","85f79966":"code","2ae9a22c":"code","1726b7ea":"code","96bde462":"code","088a1069":"markdown","fc5efa7c":"markdown","396cc53f":"markdown","2fb3cc3d":"markdown","9755022f":"markdown","840368da":"markdown","152d32f5":"markdown","7f779e0b":"markdown","a6f6b015":"markdown","ebdaf207":"markdown","980a6308":"markdown","cc873625":"markdown","56d35bd6":"markdown","bb357ab4":"markdown"},"source":{"d7d192f9":"!pip install ipython-autotime\n%load_ext autotime","97e63504":"import os,re\nimport inspect\nimport functools\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input,Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential,Model\nimport tensorflow_probability as tfp\n\nimport tensorflow.compat.v1 as tf1\ntf1.disable_v2_behavior()","87e6a55b":"data_path = '..\/input\/cc-live-proj\/'\n\ndata = pd.read_csv(data_path + 'AI-DataTrain.csv')\ndata = data.drop(columns=['Unnamed: 0'])\n\ntest_data = pd.read_csv(data_path + 'AI-DataTest.csv')\n\nprint('Data shape: ',data.shape)\nprint('Test_data shape:', test_data.shape)","19a2fa80":"# data.loc[1000,:]=[None,1,0,0,None,0,1,1,1,1,None,1,0,0,None,0,1,1,1,1,None,1,0,0,None,0,1,1,1,1,None,1,0,0,None,0,1,1,1,1,None,1,0,0,None,0,1,1,1,1]\ndata.tail()","f79f9e01":"counts_data = data.apply(pd.Series.value_counts)\ncounts_data = counts_data.T\ncounts_data[0] = counts_data[0]\/data.shape[0]\ncounts_data[1] = counts_data[1]\/data.shape[0]\ncounts_data['Final Weights']=counts_data[0]\/(counts_data[0]+counts_data[1])\ncounts_data.head()","d913a85d":"train_data = data.iloc[0:900,:]\ncounts_train = train_data.apply(pd.Series.value_counts)\ncounts_train = counts_train.T\ncounts_train[0] = counts_train[0]\/train_data.shape[0]\ncounts_train[1] = counts_train[1]\/train_data.shape[0]\ncounts_train['Train Weights']=counts_train[0]\/(counts_train[0]+counts_train[1])\ncounts_train.head()","ded6e083":"x_train = counts_train[[0,1]].values\n\ny_train = counts_train['Train Weights'].values\ny_train = np.resize(y_train,(len(y_train),1))\n\nx_valid = counts_data[[0,1]].values\n\ny_valid = counts_train['Train Weights'].values\ny_valid = np.resize(y_valid,(len(y_valid),1))\n\nprint('Train data shape: ',x_train.shape)\nprint('Train labels shape: ',y_train.shape)\nprint('Valid data shape: ',x_valid.shape)\nprint('Valid labels shape: ',y_valid.shape)","6874f468":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(128,input_shape=(2,),activation=tf.nn.swish),\n    tf.keras.layers.Dense(64,activation=tf.nn.swish),\n    tf.keras.layers.Dense(1)\n])\nmodel.compile(\n    optimizer='adam',\n    loss = 'mse',\n    metrics=[tf.keras.metrics.RootMeanSquaredError()]\n)\nmodel.summary()","b3672bc3":"EPOCHS = 50\nhistory = model.fit(x_train,y_train,epochs=EPOCHS,validation_data=(x_valid, y_valid))","4af3ba77":"epochs_range = range(EPOCHS)\n\nplt.figure(figsize=(20, 6))\n\nplt.subplot(121)\nplt.plot(epochs_range,history.history['loss'], label='loss')\nplt.plot(epochs_range,history.history['val_loss'], label='val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.ylim(0,1)\n# plt.xlim(150,300)\nplt.legend(loc='upper right')\n\nplt.subplot(122)\nplt.plot(epochs_range,history.history['root_mean_squared_error'], label='root_mean_squared_error')\nplt.plot(epochs_range,history.history['val_root_mean_squared_error'], label='val_root_mean_squared_error')\nplt.xlabel('Epoch')\nplt.ylabel('Root mean squared error')\n# plt.ylim(0,1)\n# plt.xlim(150,300)\nplt.legend(loc='upper right')\n\nplt.suptitle('Train and validation loss and root mean square error',fontsize=24)","b5fdda23":"x = model.predict(x_train)\nx.T","53ace48e":"print('Easiest:',np.argmin(x.T)+1)\nprint('Toughest:',np.argmax(x.T)+1)","c4992245":"model.save('qws.h5')","83d50eb8":"m = tf.keras.models.load_model('qws.h5')\nm.summary()","c7dd7e74":"t = m.predict(x_valid).T\nt","e6759be8":"print('Easiest:',np.argmin(t)+1)\nprint('Toughest:',np.argmax(t)+1)","66979163":"train_data = data.iloc[0:900,:].values.astype('float')\nval_data = data.iloc[900:,:].values.astype('float')\n\nprint('Train data shape: ', train_data.shape[0])\nprint('Val data shape: ', val_data.shape[0])","cffcc6d5":"train_data","f5fd76a2":"data_shape = train_data.shape\nlearning_rate = 1e-3","d1c8229c":"def sigmoid(x):\n    return tf1.sigmoid(x)\n\n\ndef log(x):\n    return tf1.log(x)\n    \n    \ndef compute_cost(X,alpha,delta):\n    offset = alpha-delta\n    log_likelihood = tf1.reduce_sum(X * log(sigmoid(offset)) + (1-X) * log(1-sigmoid(offset)))\n    return -log_likelihood","119ca4ff":"tf1.reset_default_graph()\n\nX = tf1.placeholder(dtype='float' ,shape=data_shape, name=\"X\")\nalpha = tf1.Variable(initial_value=np.zeros((data_shape[0],1)), name=\"alpha\", dtype='float')\ndelta = tf1.Variable(initial_value=np.zeros((1,data_shape[1])), name=\"delta\", dtype='float')\n\nout = tf1.layers.Dense(64,activation='relu')(X)\nout = tf1.layers.Dense(1,activation='sigmoid')(out)\n\n# log_likelihood = tf1.reduce_sum(X * tf1.log(tf1.sigmoid(alpha-delta)) + (1-X) * tf1.log(1-tf1.sigmoid(alpha-delta)))\ncost = compute_cost(X,alpha,delta)\n\noptimizer = tf1.train.AdamOptimizer(learning_rate)\ntraining_op = optimizer.minimize(cost)","2af6ad7a":"init = tf1.global_variables_initializer()\nn_epochs = 100000","b0075a41":"with tf1.Session() as sess:\n    sess.run(init)\n\n    for epoch in range(n_epochs):\n        if epoch % 20000 == 0:\n            print(\"Epoch: \", epoch, \"\\tCost =\", cost.eval(feed_dict={X: train_data}))\n        sess.run(training_op, feed_dict={X: train_data})\n    \n    best_alpha = alpha.eval()\n    best_delta = delta.eval()","96000113":"best_alpha.T","4d3fee60":"best_delta","85f79966":"print('output shape:',best_delta.shape)\nprint('MIN:',np.argmin(best_delta)+1,'\\tMAX:',np.argmax(best_delta)+1)","2ae9a22c":"# def sigmoid(x):\n#     return tf1.sigmoid(x)\n\n# def log(x):\n#     return tf1.log(x)\n\n# def compute_cost_2PL(X,discrimination,ability,difficulty):\n#     eff = discrimination*(-ability+difficulty)\n#     log_likelihood = tf1.reduce_sum(X * log(sigmoid(eff)) + (1-X) * log(1-sigmoid(eff)))\n#     cost = -log_likelihood\n#     return cost","1726b7ea":"# learning_rate = 1e-5\n# data_shape = train_data.shape\n\n# tf1.reset_default_graph()\n\n# X = tf1.placeholder(dtype='float' ,shape=data_shape, name=\"X\")\n# ability = tf1.Variable(initial_value=np.zeros((data_shape[0],1)), name=\"ability\", dtype='float')\n# difficulty = tf1.Variable(initial_value=np.zeros((1,data_shape[1])), name=\"difficulty\", dtype='float')\n# discrimination = tf1.Variable(initial_value=np.zeros((1,data_shape[1])), name=\"discrimination\", dtype='float')\n\n# out = tf1.layers.Dense(64,activation='relu')(X)\n# out = tf1.layers.Dense(1,activation='sigmoid')(out)\n\n# # out1 = tf1.layers.Dense(64,activation='relu')(X)\n# # out1 = tf1.layers.Dense(1,activation='sigmoid')(out1)\n\n# # out2 = tf1.layers.Dense(64,activation='relu')(X)\n# # out2 = tf1.layers.Dense(1,activation='sigmoid')(out2)\n\n# cost = compute_cost_2PL(X,discrimination,ability,difficulty)\n\n# optimizer = tf1.train.AdamOptimizer(learning_rate).minimize(cost)","96bde462":"# init = tf1.global_variables_initializer()\n# n_epochs = 100000\n\n# with tf1.Session() as sess:\n#     sess.run(init)\n\n#     for epoch in range(n_epochs):\n#         if epoch % 20000 == 0:\n#             print(\"Epoch: \", epoch, \"\\tCost =\", cost.eval(feed_dict={X: train_data}))\n#         sess.run(optimizer, feed_dict={X: train_data})\n    \n#     best_ability = ability.eval()\n#     best_difficulty = difficulty.eval()\n#     best_dicrimination = discrimination.eval()","088a1069":"# 2PL model","fc5efa7c":"### Data Prep for Rasch","396cc53f":"# Other resource links and References\n1.[Grade response theory model](https:\/\/colab.research.google.com\/drive\/1ZAmkuQF2XoV0Jy9_EJtGTJrD3Tn4UlAJ#scrollTo=bg9eMEZO5onP)<br>\n2.[Rasch model](https:\/\/www.kaggle.com\/mlarionov\/the-rasch-model)<br>\n3.[Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Item_response_theory)<br>\n4.[catsim](https:\/\/douglasrizzo.com.br\/catsim\/introduction.html)<br>\n5.[edstan](https:\/\/pystan.readthedocs.io\/en\/latest\/installation_beginner.html)<br>\n6.[Item Response Theory: What It Is and How You Can Use the IRT Procedure to Apply It](https:\/\/support.sas.com\/resources\/papers\/proceedings14\/SAS364-2014.pdf)<br>\n7.[Comparison- of 1-, 2-, and 3-Parameter IKT Models](https:\/\/pdfs.semanticscholar.org\/1476\/89b601462b869b25c20697188a6576e42efe.pdf)<br>\n8.http:\/\/www.lcc.uma.es:8080\/repository\/fileDownloader?rfname=LCC559.pdf<br>\n9.http:\/\/www.personality-project.org\/r\/book\/Chapter8.pdf","2fb3cc3d":"# THE END","9755022f":"# Importing dependencies and libraries","840368da":"# Simple Regression model","152d32f5":"#### Saving the model","7f779e0b":"# Custom model","a6f6b015":"Final and Train calculated weights","ebdaf207":"# Feature Engineering","980a6308":"# Reading the data","cc873625":"### Build Rasch Model -1PL","56d35bd6":"# Rasch Model-1PL","bb357ab4":"# Data Preparation"}}