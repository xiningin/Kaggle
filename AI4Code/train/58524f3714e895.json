{"cell_type":{"2b64fd4d":"code","5899cb87":"code","2ec8e03d":"code","8cebbfaa":"code","2242e43d":"code","8ddfed5b":"code","1cc1f1ba":"code","18aecaad":"code","c0becc17":"code","d957ea12":"markdown","6f970690":"markdown","f377c1aa":"markdown","458d4e99":"markdown"},"source":{"2b64fd4d":"import math\nimport warnings\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nimport transformers\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom torch import nn as nn\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\nfrom shopee_datasets import ShopeeDataModule\nfrom shopee_models import ArcMarginProduct, find_threshold, ShopeeNet\nfrom shopee_utils import KerasProgressBar, seed_everything","5899cb87":"warnings.filterwarnings(\"ignore\")","2ec8e03d":"CSV_TRAIN = \"..\/input\/shopee-product-matching\/train.csv\"\nIMAGES_TRAIN = \"..\/input\/shopee-product-matching\/train_images\"\nN_SPLITS = 5\n\nNUM_WORKERS = 4\nTRAIN_BATCH_SIZE = 256\nEPOCHS = 30\nSEED = 23\nLR = 5e-5\nTRANSFORMER_MODEL = \"sentence-transformers\/paraphrase-xlm-r-multilingual-v1\"\nTOKENIZER_MAX_LEN = 50\n\n# FC layer parameters\nUSE_FC = True\nFC_DIM = 512\nDROPOUT = 0.2\n\n# Metric Loss and its params\nLOSS_MODULE = \"arcface\"\nS = 30.0\nM = 0.3\nLS_EPS = 0.01\nEASY_MARGIN = False\nTHETA_ZERO = math.pi \/ 4\n\nSEARCH_SPACE = np.arange(55, 85, 3)\n\nglobal schedule\nschedule = get_cosine_schedule_with_warmup","8cebbfaa":"seed_everything(SEED)","2242e43d":"checkpoint_callback = ModelCheckpoint(\n    monitor=\"val_score\",\n    dirpath=\"checkpoints\",\n    filename=\"ckpt-epoch{epoch:02d}-val_score{val_score:.2f}\",\n    save_top_k=1,\n    mode=\"max\",\n)\n\nearly_stop_callback = EarlyStopping(\n    monitor=\"val_score\", min_delta=0.00, patience=8, verbose=False, mode=\"max\"\n)","8ddfed5b":"dm = ShopeeDataModule(\n    path_to_csv=CSV_TRAIN,\n    path_to_images=IMAGES_TRAIN,\n    n_splits=N_SPLITS,\n    random_state=SEED,\n    batch_size=TRAIN_BATCH_SIZE,\n    tokenizer_max_len=TOKENIZER_MAX_LEN,\n    num_workers=NUM_WORKERS,\n    tokenizer_path=\"tokenizer\"\n)\ndm.setup(\"train\")","1cc1f1ba":"model_params = {\n    \"n_classes\": 11014,\n    \"valid_df\": dm.data.query(\"fold==0\"),\n    \"model_name\": TRANSFORMER_MODEL,\n    \"use_fc\": USE_FC,\n    \"fc_dim\": FC_DIM,\n    \"dropout\": DROPOUT,\n    \"loss_module\": LOSS_MODULE,\n    \"s\": S,\n    \"margin\": M,\n    \"ls_eps\": LS_EPS,\n    \"easy_margin\": EASY_MARGIN,\n    \"theta_zero\": THETA_ZERO,\n    \"num_warmup_steps\": dm.num_batches * 2,\n    \"num_training_steps\": dm.num_batches * EPOCHS,\n    \"search_space\": SEARCH_SPACE\n}\n\n\nmodel = ShopeeNet(**model_params)\nbar = KerasProgressBar()","18aecaad":"trainer = pl.Trainer(\n    gpus=1 if torch.cuda.is_available() else None, \n    max_epochs=EPOCHS, \n    callbacks=[bar, checkpoint_callback, early_stop_callback],\n    gradient_clip_val=0.5,\n)\ntrainer.fit(model, dm)","c0becc17":"%rm -rf lightning_logs","d957ea12":"# Summary\nThis notebook demonstrates the use of **Pytorch Lightning** with most utility functions moved to the utility scripts to keep the notebook clean.\n\nMost of the code is based on and inspired by the following notebooks:\n* https:\/\/www.kaggle.com\/tanulsingh077\/metric-learning-pipeline-only-text-sbert\n* https:\/\/www.kaggle.com\/underwearfitting\/pytorch-densenet-arcface-validation-training\/notebook\n\n**Inference notebook**:<br>\nhttps:\/\/www.kaggle.com\/kcostya\/ride-the-lightning-inference\/","6f970690":"### Model","f377c1aa":"### Config","458d4e99":"### Run!"}}