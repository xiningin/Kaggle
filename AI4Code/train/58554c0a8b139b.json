{"cell_type":{"b522e90e":"code","d11dfa27":"code","d1145b2d":"code","a68cb344":"code","a5887c3e":"code","a57eb548":"code","8992d1b4":"code","98a04aa9":"code","fce69cb9":"code","692b2a32":"code","9979be93":"code","113ca265":"code","65b7a976":"code","d5709372":"code","762c9609":"code","a1562e5f":"code","91833add":"code","073f8cee":"code","636231a6":"code","b6101c84":"code","008b11b4":"code","77202b1f":"code","38b2b5d4":"code","72f91a68":"code","a47de305":"code","b0014005":"code","fad34327":"code","ec3884df":"code","cda7b188":"code","3869bb45":"markdown","d89e9bab":"markdown","39e0f35e":"markdown","dab484d1":"markdown","264373b7":"markdown","5861c6c9":"markdown"},"source":{"b522e90e":"import numpy as np \nimport pandas as pd \nfrom typing import List, Iterator, Callable\nfrom time import time\nimport glob\nimport os\nfrom functools import lru_cache\nfrom sklearn.metrics import r2_score\nfrom tqdm import tqdm\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.base import BaseEstimator\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom utils import DataLoader\n\nimport statsmodels.api as sm","d11dfa27":"MAX_TIMEIDS = None\nMAX_STOCKS = None\n# MAX_TIMEIDS = 10\n# MAX_STOCKS = 30\ndata_loader = DataLoader(MAX_TIMEIDS, False)","d1145b2d":"train_df = data_loader.read_labels(test=False)\nif MAX_STOCKS and MAX_STOCKS < len(train_df[\"stock_id\"].unique()):\n    print(\"Sampling {} stocks for training.\".format(MAX_STOCKS))\n    sampled_stocks = np.random.choice(train_df[\"stock_id\"].unique(), size=MAX_STOCKS, replace=False)\n    train_df = train_df[train_df[\"stock_id\"].isin(sampled_stocks)]","a68cb344":"# helpers to extract features from book\ndef calc_realized_volatility(log_returns: pd.Series) -> float:\n    return np.sqrt(np.sum(log_returns ** 2))\n\n\ndef aggregate_book_for_stock_and_time_id(book_time_slice: pd.DataFrame) -> pd.Series:\n    volatilities = dict()\n    for i in range(1, 4):\n        log_returns = np.log(book_time_slice[f\"WAP{i}\"]).diff()\n        volatilities[f\"volatility_{i}\"] = calc_realized_volatility(log_returns)\n        \n        log_returns = np.log(book_time_slice.tail(100)[f\"WAP{i}\"]).diff()\n        volatilities[f\"volatility_tail_{i}\"] = calc_realized_volatility(log_returns)\n\n    return pd.Series(volatilities)\n  \n    \n\ndef aggregate_book_for_stock(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"WAP1\"] = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1']+ df['ask_size1'])\n    df[\"WAP2\"] = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2']+ df['ask_size2'])\n    df[\"WAP3\"] = (\n        (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) +\n        (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])\n    ) \/ (\n        (df['bid_size1']+ df['ask_size1']) + (df['bid_size2']+ df['ask_size2'])\n    )\n    \n    start = time()\n    \n    g = df.groupby([\"time_id\"], as_index=False)\n    result = g.apply(aggregate_book_for_stock_and_time_id)\n#     print(time() - start, \"seconds to aggregate book per time_id\")\n    return result\n\ndef get_book_features_for_all_stocks(stock_ids, test):\n    features_all_books = list()\n    print(\"Getting book features for {} stocks.\".format(len(stock_ids)))\n    for stock_id in tqdm(stock_ids):\n        book_df = data_loader.read_book(stock_id, test=test)\n        features_this_book = aggregate_book_for_stock(book_df)\n        features_this_book.insert(0, \"stock_id\", stock_id)\n        features_all_books.append(features_this_book)\n\n    features_all_books = pd.concat(features_all_books)\n    return features_all_books\n    \n","a5887c3e":"# get book features per stock\nbook_features_train = get_book_features_for_all_stocks(train_df[\"stock_id\"].unique(), False)","a57eb548":"train_enriched = train_df.merge(book_features_train, how=\"inner\")\ntrain_enriched.head()","8992d1b4":"def make_X(feature_df):\n    stock_dummies = pd.get_dummies(feature_df[[\"stock_id\"]])\n    return pd.concat([\n        feature_df[[\"volatility_1\"]],\n        stock_dummies\n    ], \n        axis=1\n    )","98a04aa9":"def rmspe(y_true: pd.Series, y_pred: pd.Series) -> float:\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))  \n\ndef score_model_on_rmspe(model, X: pd.DataFrame, y_true: pd.Series) -> float:\n    y_pred = model.predict(X)\n    return rmspe(y_true, y_pred)","fce69cb9":"X_train = make_X(train_enriched)\ny = train_enriched[\"target\"]","692b2a32":"model = LinearRegression(fit_intercept=False)\n\nn_splits = 10\ncv_results = cross_val_score(model, X_train, y, scoring=score_model_on_rmspe, cv=n_splits)\nprint(\"RMSPE from cross-validation:\")\nprint(round(cv_results.mean(), 3),  \" +\/-\", round(1.96 * cv_results.std(), 2))","9979be93":"model.fit(X_train, y)","113ca265":"# Prepare features for test\ntest_df = data_loader.read_labels(test=True)\nbook_features_test = get_book_features_for_all_stocks(test_df[\"stock_id\"].unique(), test=True)\ntest_enriched = test_df.merge(book_features_test, how=\"left\") # to fail early I don't use inner join","65b7a976":"print(\"Missing value prevalence in X_test (will be mean-imputed):\")\nna_pct = test_enriched.isnull().mean()\nprint(na_pct[na_pct>0])\ntest_enriched = test_enriched.fillna(test_enriched.mean(numeric_only=True))","d5709372":"X_test = make_X(test_enriched)\ny_predicted = model.predict(X_test)","762c9609":"def prepare_submissions(test_df, y_predicted):\n    # TODO: ensure that y_predicted is aligned with test_df\n    test_df[\"target\"] = y_predicted\n    \n    submission_df = test_df[[\"row_id\", \"target\"]].fillna(test_df[\"target\"].mean())\n    submission_df.to_csv('submission.csv',index = False)\n    print(\"submissions prepared - done\")\n    \nprepare_submissions(test_df, y_predicted)","a1562e5f":"\n#     stock_ids = test_df[\"stock_id\"].unique()","91833add":"# # simple utils\n# def flatten_hierachical_column_index(df: pd.DataFrame) -> None:\n#     df.columns = ['_'.join(col).strip() for col in df.columns.values]\n    \n# def columns_are_primary_key(df: pd.DataFrame, colnames: List[str]) -> bool:\n#     if not df[colnames].duplicated().any():\n#         print(f\"({', '.join(colnames)}) is a primary key\")\n#         return True","073f8cee":"\n \n    \n# def aggregate_trades_for_stock_and_time_id(trade_time_slice: pd.DataFrame) -> pd.Series:\n#     trade_time_slice = trade_time_slice.assign(\n#         trade_price_1 = lambda df: df.price\n#     )\n#     volatilities = dict()\n#     for i in range(1, 2):\n#         log_returns = np.log(trade_time_slice[f\"trade_price_{i}\"]).diff()\n#         volatilities[f\"trade_volatility_{i}\"] = calc_realized_volatility(log_returns)\n\n#     return pd.Series(volatilities)\n    \n# def aggregate_trades_for_stock(df: pd.DataFrame) -> pd.DataFrame:\n#     g = df.groupby([\"time_id\"], as_index=False)\n#     result = g.apply(aggregate_trades_for_stock_and_time_id)\n#     result[\"stock_id\"] = df[\"stock_id\"].iloc[0]\n#     return result\n    \n    \n# # @lru_cache(maxsize=MAX_CACHE)\n# def get_stock_x_time_df_by_stock_id(stock_id: int, test: bool, verbose: bool =True) -> pd.DataFrame:\n#     book_df = df_from_parquet_for_stock(stock_id, test=test, verbose=verbose)\n#     book_agg = aggregate_book_for_stock(book_df)\n    \n#     trade_df = df_from_parquet_for_stock(stock_id, test=test, book_or_trade=\"trade\", verbose=verbose)\n#     trade_agg = aggregate_trades_for_stock(trade_df)\n    \n#     joined = book_agg.merge(trade_agg, on=[\"time_id\", \"stock_id\"], how=\"left\")\n\n#     return joined\n\n\n# def iter_stock_x_time_dfs(stock_ids: int, test: bool, verbose: bool =False) -> Iterator[pd.DataFrame]:\n#     for stock_id in tqdm(stock_ids):\n#         yield get_stock_x_time_df_by_stock_id(stock_id, test=test, verbose=verbose)\n        \n        \n    \n# get_stock_x_time_df_by_stock_id(37, test=False, verbose=False)    \n","636231a6":"# train_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n# # \n# columns_are_primary_key(train_df, [\"stock_id\", \"time_id\"])","b6101c84":"# # compile training df\n# possible_stocks = train_df[\"stock_id\"].unique()\n# stock_ids = possible_stocks[:MAX_STOCKS_FOR_TRAINING]\n\n# prepared_training_dfs = list()\n# for i, stock_x_time_df in enumerate(iter_stock_x_time_dfs(stock_ids, test=False)):\n#     if i >= MAX_STOCKS_FOR_TRAINING:\n#         break     \n#     prepared_training_dfs.append(stock_x_time_df)\n\n    \n# prepared_training_df = pd.concat(prepared_training_dfs)\n# prepared_training_df = prepared_training_df.merge(train_df, on=[\"time_id\", \"stock_id\"], how=\"inner\")\n# prepared_training_df.describe()","008b11b4":"# # check if there are any null values\n# null_count = prepared_training_df.isnull().sum()\n# null_count[null_count > 0].to_frame(\"null count\")","77202b1f":"# prepared_training_df = prepared_training_df.fillna(prepared_training_df.mean())","38b2b5d4":"# feature_names = [\"volatility_1\", \"volatility_tail_1\"]\n# y = prepared_training_df[\"target\"]\n# X = prepared_training_df[feature_names]","72f91a68":"# lm = sm.OLS(endog=y, exog=X)\n# lm = lm.fit()\n# lm.summary()","a47de305":"# simple_model = LinearRegression(fit_intercept=False)\n\n# n_splits = 10\n# cv_results = cross_val_score(simple_model, X, y, scoring=score_model_on_rmspe, cv=n_splits)\n# print(\"OLS on target - RMSPE from cross-validation:\")\n# print(round(cv_results.mean(), 3),  \" +\/-\", round(1.96 * cv_results.std(), 2))","b0014005":"# pf = PolynomialFeatures(interaction_only=True, include_bias=False)\n# X_plus = pf.fit_transform(X)\n# cv_results = cross_val_score(simple_model, X_plus, y, scoring=score_model_on_rmspe, cv=n_splits)\n# print(\"OLS on target - RMSPE from cross-validation:\")\n# print(round(cv_results.mean(), 3),  \" +\/-\", round(1.96 * cv_results.std(), 2))","fad34327":"# def predict_and_prepare_submission(fitted_model) -> pd.DataFrame:\n    \n#     test_df = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n#     stock_ids = test_df[\"stock_id\"].unique()\n# #     print(test_df)\n#     predictions = list()\n#     for stock_x_time_df in iter_stock_x_time_dfs(stock_ids, test=True):\n# #         print(stock_x_time_df)\n#         X = stock_x_time_df[feature_names].copy()\n#         X.fillna(0, inplace=True)\n#         y_pred = fitted_model.predict(X)\n#         stock_x_time_df[\"target\"] = y_pred\n        \n#         predictions.append(stock_x_time_df[[\"stock_id\", \"time_id\", \"target\"]])\n        \n#     predictions = pd.concat(predictions)\n#     test_df = test_df.merge(predictions, on=[\"time_id\", \"stock_id\"], how=\"left\")\n#     return test_df[[\"row_id\", \"target\"]].fillna(test_df[\"target\"].mean())","ec3884df":"# simple_model.fit(X, y)","cda7b188":"# submission_df = predict_and_prepare_submission(simple_model)\n# submission_df.to_csv('submission.csv',index = False)","3869bb45":"### Look at simple OLS regression stats for selected features","d89e9bab":"## Final Fit and Predict","39e0f35e":"### Compile Training Data","dab484d1":"### Cross validate model using target metric","264373b7":"## Predict on test set and submit","5861c6c9":"## Cross Validate Model"}}