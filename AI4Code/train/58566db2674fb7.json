{"cell_type":{"105ef061":"code","c84dac30":"code","462ff15e":"code","e9b9b14e":"code","a7082694":"code","d8b52e6f":"code","2b07a7cc":"code","72246838":"code","ccfeff46":"code","2e673b65":"code","208da15e":"code","7f123c41":"code","a01bdab0":"code","6b22e155":"code","9f4b3d3e":"code","aff35d1e":"code","898e4504":"code","84e47f50":"code","50d7843a":"code","8688a98d":"code","ab2741e0":"code","1f1f54d1":"code","1840385e":"code","17691c4d":"code","ad5cd8a6":"code","7f4db07a":"code","110aaefa":"markdown","83eda429":"markdown","0c2dec25":"markdown","51c65808":"markdown","b32d20e5":"markdown","9df41072":"markdown","30a6f3fe":"markdown","b8b534bb":"markdown","86cdff4b":"markdown","68159f6b":"markdown","0c335c6d":"markdown","4a82a027":"markdown","3af37227":"markdown","47a2c75c":"markdown","9cc26355":"markdown","be4ac54e":"markdown","2e1ce198":"markdown","d5824952":"markdown","33ecaa95":"markdown","c243e8f8":"markdown","0f34f19f":"markdown","2389a758":"markdown","4011e5bc":"markdown","5eff4399":"markdown"},"source":{"105ef061":"import nltk","c84dac30":"nltk.download('twitter_samples')","462ff15e":"from nltk.corpus import twitter_samples","e9b9b14e":"positive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\ntext = twitter_samples.strings('tweets.20150430-223406.json')","a7082694":"nltk.download('punkt')","d8b52e6f":"tweet_tokens = twitter_samples.tokenized('positive_tweets.json')","2b07a7cc":"tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\nprint(tweet_tokens[0])","72246838":"nltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')","ccfeff46":"from nltk.tag import pos_tag\ntweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nprint(pos_tag(tweet_tokens[0]))","2e673b65":"from nltk.stem.wordnet import WordNetLemmatizer","208da15e":"def lemmatize_sentence(tokens):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_sentence = []\n    for word, tag in pos_tag(tokens):\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n    return lemmatized_sentence\nprint(lemmatize_sentence(tweet_tokens[0]))","7f123c41":"import re, string","a01bdab0":"def remove_noise(tweet_tokens, stop_words = ()):\n    cleaned_tokens = []\n    for token, tag in pos_tag(tweet_tokens):\n        token = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens","6b22e155":"nltk.download('stopwords')","9f4b3d3e":"from nltk.corpus import stopwords","aff35d1e":"stop_words = stopwords.words('english')\nprint(remove_noise(tweet_tokens[0], stop_words))","898e4504":"positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\nnegative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n\npositive_cleaned_tokens_list = []\nnegative_cleaned_tokens_list = []\n\nfor tokens in positive_tweet_tokens:\n    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\nfor tokens in negative_tweet_tokens:\n    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n\nprint(positive_tweet_tokens[500])\nprint(positive_cleaned_tokens_list[500])","84e47f50":"def get_all_words(cleaned_tokens_list):\n    for tokens in cleaned_tokens_list:\n        for token in tokens:\n            yield token\nall_pos_words = get_all_words(positive_cleaned_tokens_list)","50d7843a":"from nltk import FreqDist","8688a98d":"freq_dist_pos = FreqDist(all_pos_words)\nprint(freq_dist_pos.most_common(10))","ab2741e0":"def get_tweets_for_model(cleaned_tokens_list):\n    for tweet_tokens in cleaned_tokens_list:\n        yield dict([token, True] for token in tweet_tokens)\n        \npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)","1f1f54d1":"import random\n\npositive_dataset = [(tweet_dict, 'Positive')\n                   for tweet_dict in positive_tokens_for_model]\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\ndataset = positive_dataset + negative_dataset\n\nrandom.shuffle(dataset)\n\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]","1840385e":"from nltk import classify\nfrom nltk import NaiveBayesClassifier\nclassifier = NaiveBayesClassifier.train(train_data)\n\nprint('Accuracy is: ', classify.accuracy(classifier, test_data))\nprint(classifier.show_most_informative_features(10))","17691c4d":"from nltk.tokenize import word_tokenize\n\ncustom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n\ncustom_tokens = remove_noise(word_tokenize(custom_tweet))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens)))","ad5cd8a6":"from nltk.tokenize import word_tokenize\n\ncustom_tweet = \"Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies\"\n\ncustom_tokens = remove_noise(word_tokenize(custom_tweet))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens)))","7f4db07a":"from nltk.tokenize import word_tokenize\n\ncustom_tweet = \"Thank you for sending my baggage to CityX and flying me to CityY at the same time. Brilliant service. #thanksGenericAirline\"\n\ncustom_tokens = remove_noise(word_tokenize(custom_tweet))\n\nprint(classifier.classify(dict([token, True] for token in custom_tokens)))","110aaefa":"Noise is any part of the text that does not add meaning or information to data.\n\nFor instance, the most common words in a language are called stop words. Some examples of stop words are \u201cis\u201d, \u201cthe\u201d, and \u201ca\u201d. They are generally irrelevant when processing language, unless a specific use case warrants their inclusion.","83eda429":"you will use regular expressions in Python to search for and remove these items:\n\n   Hyperlinks - All hyperlinks in Twitter are converted to the URL shortener t.co. Therefore, keeping them in the text      processing would not add any value to the analysis.\n   Twitter handles in replies - These Twitter usernames are preceded by a @ symbol, which does not convey any meaning.\n   Punctuation and special characters - While these often provide context to textual data, this context is often difficult to process. For simplicity, you will remove all punctuation and special characters from tweets.","0c2dec25":"The function lemmatize_sentence first gets the position tag of each token of a tweet. Within the if statement, if the tag starts with NN, the token is assigned as a noun. Similarly, if the tag starts with VB, the token is assigned as a verb.","51c65808":"You will notice that the verb being changes to its root form, be, and the noun members changes to member.\n\nNow that you have successfully created a function to normalize words, you can remove noise. ","b32d20e5":"Your script is now configured to tokenize data. In the next step you will update the script to normalize the data.","9df41072":"Now that you have compiled all words in the sample of tweets, you can find out which are the most common words using the FreqDist class of NLTK.","30a6f3fe":"From this data, you can see that emoticon entities form some of the most common parts of positive tweets.\n\nTo summarize, you extracted the tweets from nltk, tokenized, normalized, and cleaned up the tweets for using in the model. Finally, you also looked at the frequencies of tokens in the data and checked the frequencies of the top ten tokens.\n\nIn the next step you will prepare data for sentiment analysis.","b8b534bb":"If you\u2019d like to test the script to see the .tokenized method in action, add the highlighted content to your script. This will tokenize a single tweet from the positive_tweets.json dataset:","86cdff4b":"This code attaches a Positive or Negative label to each tweet. It then creates a dataset by joining the positive and negative tweets.\n\nBy default, the data contains all positive tweets followed by all negative tweets in sequence. When training the model, you should provide a sample of your data that does not contain any bias. To avoid bias, you\u2019ve added code to randomly arrange the data using the .shuffle() method of random.","68159f6b":"Next, you can check how the model performs on random tweets from Twitter.","0c335c6d":"Sentiment analysis is a process of identifying an attitude of the author on a topic that is being written about. You will create a training data set to train a model. It is a supervised learning machine learning process, which requires you to associate each dataset with a \u201csentiment\u201d for training. In this tutorial, your model will use the \u201cpositive\u201d and \u201cnegative\u201d sentiments.\n\nIn the data preparation step, you will prepare the data for sentiment analysis by converting tokens to the dictionary form and then split the data for training and testing purposes.","4a82a027":"From the list of tags, here is the list of the most common items and their meaning:\n\n   NNP: Noun, proper, singular\n   NN: Noun, common, singular or mass\n   IN: Preposition or conjunction, subordinating\n   VBG: Verb, gerund or present participle\n   VBN: Verb, past participle","3af37227":"# Sentiment Analysis\n\nSentiment analysis is the practice of using algorithms to classify various samples of related text into overall positive and negative categories. With NLTK, you can employ these algorithms through powerful built-in machine learning operations to obtain insights from linguistic data.","47a2c75c":"Stemming is a process of removing affixes from a word. Stemming, working with only simple verb forms, is a heuristic process that removes the ends of words.\n\nLemmatization normalizes a word with the context of vocabulary and morphological analysis of words in text. This algorithm analyzes the structure of the word and its context to convert it to a normalized form.\n\nA comparison of stemming and lemmatization ultimately comes down to a trade off between speed and accuracy.","9cc26355":"Notice that the function removes all @ mentions, stop words, and converts the words to lowercase.\n\nBefore proceeding to the modeling exercise in the next step, use the remove_noise() function to clean the positive and negative tweets.","be4ac54e":"A large amount of data that is generated today is unstructured, which requires processing to generate insights. Some examples of unstructured data are news articles, posts on social media, and search history. The process of analyzing natural language and making sense out of it falls under the field of Natural Language Processing (NLP).","2e1ce198":"Now that you\u2019ve tested both positive and negative sentiments, update the variable to test a more complex sentiment like sarcasm.","d5824952":"The most basic form of analysis on textual data is to take out the word frequency. A single tweet is too small of an entity to find out the distribution of words, hence, the analysis of the frequency of words would be done on all positive tweets.","33ecaa95":"Before using a tokenizer in NLTK, you need to download an additional resource, punkt. The punkt module is a pre-trained model that helps you tokenize words and sentences.","c243e8f8":"This will import three datasets from NLTK that contain various tweets to train and test the model:\n\n   negative_tweets.json: 5000 tweets with negative sentiments\n   positive_tweets.json: 5000 tweets with positive sentiments\n   tweets.20150430-223406.json: 20000 tweets with no sentiments","0f34f19f":"Normalization in NLP is the process of converting a word to its canonical form.\n\nIt helps group together words with the same meaning but different forms. Without normalization, \u201cran\u201d, \u201cruns\u201d, and \u201crunning\u201d would be treated as different words, even though you may want them to be treated as the same word.","2389a758":"You will use the negative and positive tweets to train your model on sentiment analysis later in the tutorial. The tweets with no sentiments will be used to test your model.\n\nIf you would like to use your own dataset, you can gather tweets from a specific time period, user, or hashtag by using the Twitter API.","4011e5bc":"Language in its original form cannot be accurately processed by a machine, so you need to process the language to make it easier for the machine to understand. The first part of making sense of the data is through a process called tokenization, or splitting strings into smaller parts called tokens.\n\nA token is a sequence of characters in text that serves as a unit. Based on how you create the tokens, they may consist of words, emoticons, hashtags, links, or even individual characters. A basic way of breaking language into tokens is by splitting the text based on whitespace and punctuation.","5eff4399":"To incorporate this into a function that normalizes a sentence, you should first generate the tags for each token in the text, and then lemmatize each word using the tag."}}