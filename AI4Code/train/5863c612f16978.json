{"cell_type":{"7f79bc54":"code","10b3c8cd":"code","3a6bfb01":"code","7a13b060":"code","1f89ac56":"code","ef31de46":"code","b9bdfb3e":"code","f9f10749":"code","cd8427c2":"code","124e210a":"code","ff10728d":"code","cb79b836":"code","8e463259":"code","e87dc7e8":"code","67797524":"code","5a2d02e6":"code","3cd9bb03":"code","60ab36e3":"code","0e345c43":"code","463a996f":"code","da67d4e3":"code","ab569be3":"code","07a98c4a":"code","f08a661a":"markdown","fa2b2030":"markdown","74a7c37f":"markdown","79851379":"markdown","f8fb2dbb":"markdown","95a70098":"markdown","424d1fcb":"markdown","8baff155":"markdown","e05ae5b2":"markdown","7f8e7a1e":"markdown","f6de7622":"markdown","de38b203":"markdown","34b235d0":"markdown","5d094459":"markdown","3992008b":"markdown","6b2137d8":"markdown"},"source":{"7f79bc54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","10b3c8cd":"import matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.utils.np_utils import to_categorical","3a6bfb01":"(X_train, y_train), (X_test, y_test) = mnist.load_data()","7a13b060":"plt.imshow(X_train[7])    # show first number in the dataset\nplt.show()\nprint('Label: ', y_train[7])","1f89ac56":"for i in range(0,10):\n    plt.imshow(X_train[i])    # show first number in the dataset\n    plt.show()\n    print('Label: ', y_train[i])","ef31de46":"plt.imshow(X_test[0])    # show first number in the dataset\nplt.show()\nprint('Label: ', y_test[0])","b9bdfb3e":"# reshaping X data: (n, 28, 28) => (n, 784)\nX_train = X_train.reshape((X_train.shape[0], -1))\nX_test = X_test.reshape((X_test.shape[0], -1))","f9f10749":"# use only 33% of training data to expedite the training process\nX_train, _ , y_train, _ = train_test_split(X_train, y_train, test_size = 0.67, random_state = 7)","cd8427c2":"# converting y data into categorical (one-hot encoding)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","124e210a":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","ff10728d":"from keras.models import Sequential\nfrom keras.layers import Activation, Dense\nfrom keras import optimizers","cb79b836":"model = Sequential()","8e463259":"model.add(Dense(50, input_shape = (784, )))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(50))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(50))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(50))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))","e87dc7e8":"sgd = optimizers.SGD(lr = 0.001)\nmodel.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])","67797524":"history = model.fit(X_train, y_train, batch_size = 256, validation_split = 0.3, epochs = 100, verbose = 0)","5a2d02e6":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.legend(['training', 'validation'], loc = 'upper left')\nplt.show()","3cd9bb03":"results = model.evaluate(X_test, y_test)","60ab36e3":"print('Test accuracy: ', results[1])","0e345c43":"# from now on, create a function to generate (return) models\ndef mlp_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (784, ), kernel_initializer='he_normal'))     # use he_normal initializer\n    model.add(Activation('sigmoid'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n    model.add(Activation('sigmoid'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n    model.add(Activation('sigmoid'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n    model.add(Activation('sigmoid'))    \n    model.add(Dense(10, kernel_initializer='he_normal'))                            # use he_normal initializer\n    model.add(Activation('softmax'))\n    \n    sgd = optimizers.SGD(lr = 0.001)\n    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","463a996f":"model = mlp_model()\nhistory = model.fit(X_train, y_train, validation_split = 0.3, epochs = 100, verbose = 0)","da67d4e3":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.legend(['training', 'validation'], loc = 'upper left')\nplt.show()","ab569be3":"results = model.evaluate(X_test, y_test)","07a98c4a":"print('Test accuracy: ', results[1])","f08a661a":"# He normal or Xavier normal initialization schemes are SOTA at the moment","fa2b2030":"selu' (scaled exponential linear unit) is one of the most recent ones","74a7c37f":"# 1. Weight Initialization","79851379":"# Load Dataset****","f8fb2dbb":"# Using TensorFlow backend.","95a70098":"Doc: https:\/\/keras.io\/activations\/","424d1fcb":"MNIST dataset\nsource: http:\/\/yann.lecun.com\/exdb\/mnist\/","8baff155":"# There are many choices apart from sigmoid and tanh; try many of them!","e05ae5b2":"Advanced MLP\n\n# Advanced techniques for training neural networks\n\n# Weight Initialization\n\n# Nonlinearity (Activation function)\n# \n# Optimizers\n\n# Batch Normalization\n\n# Dropout (Regularization)\n\n# Model Ensemble","7f8e7a1e":"relu' (rectified linear unit) is one of the most popular ones","f6de7622":"# 2. Nonlinearity (Activation function)","de38b203":"Doc: https:\/\/keras.io\/initializers\/\n","34b235d0":"# Naive MLP model without any alterations","5d094459":"# Basic MLP model","3992008b":"# Sigmoid functions suffer from gradient vanishing problem, making training slow****","6b2137d8":"# Changing weight initialization scheme can significantly improve training of the model by preventing vanishing gradient problem up to some degree"}}