{"cell_type":{"258b60d2":"code","d05b79cb":"code","9ae4fe14":"code","0aed3d0e":"code","a3ea6f8a":"code","bc966693":"code","91f9ff6b":"code","2b5609a2":"code","1bb35a70":"code","4028d82e":"code","9a590c73":"code","4c5cc280":"code","4a24ffe8":"code","563f653a":"code","3857207b":"code","598bad3b":"code","786ebc38":"code","edcd6cb9":"code","4e5eec90":"code","f92a4a82":"code","42425ee6":"code","ce0abcd3":"code","fd4ce301":"code","71bd88eb":"code","bf834bd4":"code","5fe8dad7":"code","6f7757f6":"code","67dd7b16":"code","3952e44c":"code","27552011":"code","ca67003b":"code","a74bcca8":"code","351c6869":"code","d996b5a8":"code","3da53d11":"code","468b0775":"code","ef4741e0":"code","33096b31":"code","b3c2896c":"code","0f24f16e":"code","138c6c86":"code","d66fbfca":"code","cec70e0d":"code","1416b537":"code","c5e20451":"code","18ebb34c":"code","82755337":"code","977513d6":"code","f288f936":"code","b45f877c":"code","88d9fe68":"code","a471aa25":"code","227e32cd":"code","c73d6501":"code","cf5f1a25":"code","eeeba94f":"code","949a1444":"code","403ad7ef":"code","6a7bf6d1":"code","b768b0ca":"code","9cd576af":"code","270d285a":"code","b5686ef1":"code","27bb8c92":"code","c6d12adb":"code","d8765ee1":"code","75443542":"code","65c579c5":"code","8673c1ee":"code","cf2ea574":"code","49182959":"code","b5fcf883":"code","056e546f":"code","e2ec82eb":"code","56abe1f7":"code","7b9fc5a2":"markdown","e4db1539":"markdown","5a85f28e":"markdown","8cd32310":"markdown","85b7b2be":"markdown","3865077d":"markdown","7179d36d":"markdown","d00f0e7a":"markdown","701b29a7":"markdown","57feffb6":"markdown","f01b6bf7":"markdown","c21c767f":"markdown","dd0ef07c":"markdown","2eb473b9":"markdown","3ebf560f":"markdown","536dd73e":"markdown","bd0b7ff1":"markdown","21177dc4":"markdown","0d5d47a5":"markdown","effe9378":"markdown","e51225d4":"markdown","09ddcc66":"markdown","23640130":"markdown","2a8dd450":"markdown","eb6525de":"markdown","14042bb6":"markdown","615e4a7f":"markdown","8771fa99":"markdown","74d526e0":"markdown","b15a1b8b":"markdown","6f2f9b38":"markdown","9511caad":"markdown","068f9cdc":"markdown","2cdba000":"markdown","c9998948":"markdown","dd8d55c6":"markdown","edbd139a":"markdown","3545fd8d":"markdown","bacdd355":"markdown","d2d44ba0":"markdown","b70af2ce":"markdown","0a34256a":"markdown","5718131a":"markdown","8969935b":"markdown","3e5b96b1":"markdown","02e918a0":"markdown","b8b82175":"markdown","60199636":"markdown","a69956d7":"markdown","1461ab82":"markdown","8b58896c":"markdown","00f18c41":"markdown","25ed21ee":"markdown","cca818cc":"markdown","485da186":"markdown","3ecdb24e":"markdown","5b38acab":"markdown","5554f3b8":"markdown","65849819":"markdown","5409a83d":"markdown","89c00057":"markdown","150cc99c":"markdown","6d81899b":"markdown","a3dd4d97":"markdown","b2553dba":"markdown","7e8d615e":"markdown","2b1adfa6":"markdown","a1b6dcaa":"markdown","8d468122":"markdown","57d2272a":"markdown","eb5bf940":"markdown","74d9626d":"markdown","acddd175":"markdown","007906fd":"markdown","d7ee3c16":"markdown","a4df6062":"markdown","b81f842f":"markdown","5bf1ce4d":"markdown","1b9a3fec":"markdown"},"source":{"258b60d2":"# for data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# pandas options\npd.set_option('display.max_columns', 50)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('mode.use_inf_as_na', True)\npd.options.mode.chained_assignment = None\n\n# for date manipulation\nfrom datetime import datetime\n\n# for visualization: matplotlib\nfrom matplotlib import pyplot as plt\nfrom IPython.core.pylabtools import figsize\n%matplotlib inline\n# to display visuals in the notebook\n\n# for visualization: seaborn\nimport seaborn as sns\nsns.set_context(font_scale=2)\n\n# for feature engineering: itertools\nfrom itertools import combinations\n\n# for data preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n\n# for building the model and calculate RMSE\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# for hyperparamter optimization and the cross-validation search\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\nfrom sklearn.model_selection import KFold\n# from sklearn.model_selection import cross_val_score\n# from sklearn.model_selection import TimeSeriesSplit\n\n# for model explainability\nimport shap\n\n# to cleanup memory usage\nimport gc\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d05b79cb":"# load train  data\nbuilding = pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/building_metadata.csv\")\nweather_train = pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/weather_train.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/train.csv\")\n\n# drop floor_count\nbuilding.drop(columns=[\"floor_count\"], inplace=True)\n\n# convert timestamp column of weather_train and train\ntrain[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"],\n                                   format='%Y-%m-%d %H:%M:%S')\nweather_train[\"timestamp\"] = pd.to_datetime(weather_train[\"timestamp\"],\n                                            format='%Y-%m-%d %H:%M:%S')","9ae4fe14":"## Function to reduce the DF size\n# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\ndef reduce_memory_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))","0aed3d0e":"reduce_memory_usage(building)\nreduce_memory_usage(weather_train)\nreduce_memory_usage(train)","a3ea6f8a":"# add building age column\ncurrent_year = datetime.now().year\nbuilding['building_age'] = current_year - building['year_built']\nbuilding.drop(columns=['year_built'], inplace=True)\n\n# since NA values only present in building age fillna can be used\nbuilding.fillna(round(building.building_age.mean(),0),\n                inplace=True)","bc966693":"# check if any NA values left\nbuilding.isna().sum()","91f9ff6b":"# create label encoder object and transform the column\nle = LabelEncoder()\nle_primary_use = le.fit_transform(building.primary_use)\n\n# add label encoded column to dataframe\nbuilding['primary_use'] = le_primary_use\n\ndel le, le_primary_use\ngc.collect()","2b5609a2":"# add month, day of week, day of month and hour\nweather_train['month'] = weather_train['timestamp'].dt.month.astype(np.int8)\nweather_train['day_of_week'] = weather_train['timestamp'].dt.dayofweek.astype(np.int8)\nweather_train['day_of_month']= weather_train['timestamp'].dt.day.astype(np.int8)\nweather_train['hour'] = weather_train['timestamp'].dt.hour\n\n# add is weekend column\nweather_train['is_weekend'] = weather_train.day_of_week.apply(lambda x: 1 if x>=5 else 0)","1bb35a70":"def convert_season(month):\n    if (month <= 2) | (month == 12):\n        return 0\n    # as winter\n    elif month <= 5:\n        return 1\n    # as spring\n    elif month <= 8:\n        return 2\n    # as summer\n    elif month <= 11:\n        return 3\n    # as fall","4028d82e":"weather_train['season'] = weather_train.month.apply(convert_season)","9a590c73":"weather_train = weather_train.set_index(\n    ['site_id','day_of_month','month'])","4c5cc280":"# create dataframe of daily means per site id\nair_temperature_filler = pd.DataFrame(weather_train\n                                      .groupby(['site_id','day_of_month','month'])\n                                      ['air_temperature'].mean(),\n                                      columns=[\"air_temperature\"])\nair_temperature_filler.isna().sum()","4a24ffe8":"# create dataframe of air_temperatures to fill\ntemporary_df = pd.DataFrame({'air_temperature' : weather_train.air_temperature})\n\n# update NA air_temperature values\ntemporary_df.update(air_temperature_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"air_temperature\"] = temporary_df[\"air_temperature\"]\n\ndel temporary_df, air_temperature_filler\ngc.collect()","563f653a":"# create dataframe of daily means per site id\ncloud_coverage_filler = pd.DataFrame(weather_train\n                                     .groupby(['site_id','day_of_month','month'])\n                                     ['cloud_coverage'].mean(),\n                                     columns = ['cloud_coverage'])\ncloud_coverage_filler.isna().sum()","3857207b":"round(cloud_coverage_filler.cloud_coverage.mean(),0)","598bad3b":"cloud_coverage_filler.fillna(round(cloud_coverage_filler.cloud_coverage.mean(),0), \n                             inplace=True)\n\n# create dataframe of cloud_coverages to fill\ntemporary_df = pd.DataFrame({'cloud_coverage' : weather_train.cloud_coverage})\n\n# update NA cloud_coverage values\ntemporary_df.update(cloud_coverage_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"cloud_coverage\"] = temporary_df[\"cloud_coverage\"]\n\ndel temporary_df, cloud_coverage_filler\ngc.collect()","786ebc38":"# create dataframe of daily means per site id\ndew_temperature_filler = pd.DataFrame(weather_train\n                                      .groupby(['site_id','day_of_month','month'])\n                                      ['dew_temperature'].mean(),\n                                      columns=[\"dew_temperature\"])\ndew_temperature_filler.isna().sum()","edcd6cb9":"# create dataframe of dew_temperatures to fill\ntemporary_df = pd.DataFrame({'dew_temperature' : weather_train.dew_temperature})\n\n# update NA dew_temperature values\ntemporary_df.update(dew_temperature_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"dew_temperature\"] = temporary_df[\"dew_temperature\"]\n\ndel temporary_df, dew_temperature_filler\ngc.collect()","4e5eec90":"# create dataframe of daily means per site id\nprecip_depth_filler = pd.DataFrame(weather_train\n                                   .groupby(['site_id','day_of_month','month'])\n                                   ['precip_depth_1_hr'].mean(),\n                                   columns=['precip_depth_1_hr'])\nprecip_depth_filler.isna().sum()","f92a4a82":"round(precip_depth_filler['precip_depth_1_hr'].mean(),0)","42425ee6":"precip_depth_filler.fillna(round(precip_depth_filler['precip_depth_1_hr'].mean(),0)\n                           , inplace=True)\n\n# create dataframe of precip_depth_1_hr to fill\ntemporary_df = pd.DataFrame({'precip_depth_1_hr' : weather_train.precip_depth_1_hr})\n\n# update NA precip_depth_1_hr values\ntemporary_df.update(precip_depth_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"precip_depth_1_hr\"] = temporary_df[\"precip_depth_1_hr\"]\n\ndel precip_depth_filler, temporary_df\ngc.collect()","ce0abcd3":"# create dataframe of daily means per site id\nsea_level_filler = pd.DataFrame(weather_train\n                                .groupby(['site_id','day_of_month','month'])\n                                ['sea_level_pressure'].mean(),\n                                columns=['sea_level_pressure'])\nsea_level_filler.isna().sum()","fd4ce301":"mean_sea_level_pressure = round(\n    sea_level_filler\n    ['sea_level_pressure']\n    .astype(float)\n    .mean(),2)","71bd88eb":"sea_level_filler.fillna(mean_sea_level_pressure, inplace=True)\n\n# create dataframe of sea_level_pressure to fill\ntemporary_df = pd.DataFrame({'sea_level_pressure' : weather_train.sea_level_pressure})\n\n# update NA sea_level_pressure values\ntemporary_df.update(sea_level_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"sea_level_pressure\"] = temporary_df[\"sea_level_pressure\"]\n\ndel sea_level_filler, temporary_df\ngc.collect()","bf834bd4":"# create dataframe of daily means per site id\nwind_direction_filler = pd.DataFrame(weather_train\n                                     .groupby(['site_id','day_of_month','month'])\n                                     ['wind_direction'].mean(),\n                                     columns=['wind_direction'])\nwind_direction_filler.isna().sum()","5fe8dad7":"# create dataframe of wind_direction to fill\ntemporary_df = pd.DataFrame({'wind_direction' : weather_train.wind_direction})\n\n# update NA wind_direction values\ntemporary_df.update(wind_direction_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"wind_direction\"] = temporary_df[\"wind_direction\"]\n\ndel temporary_df, wind_direction_filler\ngc.collect()","6f7757f6":"# create dataframe of daily means per site id\nwind_speed_filler = pd.DataFrame(weather_train\n                                 .groupby(['site_id','day_of_month','month'])\n                                 ['wind_speed'].mean(),\n                                 columns=['wind_speed'])\nwind_speed_filler.isna().sum()","67dd7b16":"# create dataframe of wind_speed to fill\ntemporary_df = pd.DataFrame({'wind_speed' : weather_train.wind_speed})\n\n# update NA wind_speed values\ntemporary_df.update(wind_speed_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"wind_speed\"] = temporary_df[\"wind_speed\"]\n\ndel temporary_df, wind_speed_filler\ngc.collect()","3952e44c":"# check if NA values left\nweather_train.isna().sum()","27552011":"weather_train = weather_train.reset_index()","ca67003b":"weather_train.head()","a74bcca8":"def convert_direction(degrees):\n    if degrees <= 90:\n        return 0\n    # as norteast direction\n    elif degrees <= 180:\n        return 1\n    # as southeast direction\n    elif degrees <= 270:\n        return 2\n    # as southwest direction\n    elif degrees <= 360:\n        return 3\n    # as northwest direction","351c6869":"weather_train['wind_compass_direction'] = weather_train.wind_direction.apply(convert_direction)\nweather_train.drop(columns=['wind_direction'], inplace=True)","d996b5a8":"# create list of weather variables\nweather_variables = [\"air_temperature\", \"cloud_coverage\", \"dew_temperature\",\n                    \"precip_depth_1_hr\", \"sea_level_pressure\", \"wind_speed\"]","3da53d11":"for i, j in combinations(weather_variables, 2):\n    train[\"mean\" + i + \"_\" + j] = (weather_train[i] + weather_train[j]) \/ 2","468b0775":"# merge dataframes on train dataframe\ntrain = train.merge(building, on = \"building_id\", how = \"left\")\ntrain = train.merge(weather_train, on = [\"site_id\", \"timestamp\"], how='left')","ef4741e0":"del building\ndel weather_train\ndel weather_variables\ngc.collect()","33096b31":"print(\"Number of unique columns in the train dataset:\", train.shape[1])","b3c2896c":"train.isna().sum()","0f24f16e":"correlations_transformed = pd.DataFrame(train.corr())\ncorrelations_transformed = pd.DataFrame(correlations_transformed[\"meter_reading\"]).reset_index()\n\n# format, and display sorted correlations_transformed\ncorrelations_transformed.columns = [\"Feature\", \"Correlation with meter_reading\"]\ncorr_df = (correlations_transformed[correlations_transformed[\"Feature\"] != \"meter_reading\"]\n                .sort_values(by=\"Correlation with meter_reading\", ascending=True))\ncorr_df","138c6c86":"del corr_df, correlations_transformed\ngc.collect()","d66fbfca":"# add log_meter_reading column to the dataframe\ntrain['log_meter_reading'] = np.log1p(train.meter_reading)","cec70e0d":"correlations_transformed = pd.DataFrame(train.corr())\ncorrelations_transformed = pd.DataFrame(correlations_transformed[\"log_meter_reading\"]).reset_index()\n\n# format, and display sorted correlations_transformed\ncorrelations_transformed.columns = [\"Feature\", \"Correlation with log_meter_reading\"]\ncorr_df = (correlations_transformed[correlations_transformed[\"Feature\"] != \"log_meter_reading\"]\n                .sort_values(by=\"Correlation with log_meter_reading\", ascending=True))\ncorr_df","1416b537":"initial_feature_list = (corr_df[\n    (corr_df[\"Correlation with log_meter_reading\"] >= 0.004) | \n    (corr_df[\"Correlation with log_meter_reading\"] <= -0.004)][\"Feature\"].\n                        to_list())","c5e20451":"#original_feature_set = ['building_age', 'le_primary_use', 'cloud_coverage', \n#                        'is_weekend','wind_speed', 'day_of_week',\n#                        'wind_compass_direction', 'sea_level_pressure', 'air_temperature',\n#                        'day_of_month', 'dew_temperature', 'hour', \n#                        'month', 'meter', 'building_id', \n#                        'site_id', 'square_feet']","18ebb34c":"# we included meter_reading in the initial feature set\n# which is not a feature\n# replace meter_reading with precip_depth_1_hr\nfor n, i in enumerate(initial_feature_list):\n    if i == \"meter_reading\":\n        initial_feature_list[n] = \"precip_depth_1_hr\"\ninitial_feature_list","82755337":"del corr_df, correlations_transformed\ngc.collect()","977513d6":"X = train[initial_feature_list]\ny = train['log_meter_reading']","f288f936":"print(\"Pearson coefficient based feature selection leaves us with {} features.\".\n      format(len(X.columns)))","b45f877c":"del train\ngc.collect()","88d9fe68":"X = X.fillna(method='ffill', axis=1)","a471aa25":"X.isna().sum()","227e32cd":"# split train and validation set into 75 and 25 percent sequentially\nX_train = X[:int(3 * X.shape[0] \/ 4)]\nX_valid = X[int(3 * X.shape[0] \/ 4):]\n\ny_train = y[:int(3 * y.shape[0] \/ 4)]\ny_valid = y[int(3 * y.shape[0] \/ 4):]","c73d6501":"# make sure train and validation sets shape align\nprint(\"Shape of the training set is: \", X_train.shape)\nprint(\"Shape of the validation set is: \", X_valid.shape)\nprint(\"Shape of the training labels are: \", y_train.shape)\nprint(\"Shape of the validation labels are: \", y_valid.shape)","cf5f1a25":"def rmse(y_true, y_pred):\n    return np.sqrt(\n        np.mean(\n            np.square(y_true - y_pred)\n        )\n    )","eeeba94f":"baseline_guess = np.median(y_train)\nprint('The baseline guess is a score of %0.2f' % baseline_guess)\nprint(\"Baseline Performance on the valid set: RMSE = %0.4f\" % rmse(y_valid, baseline_guess))","949a1444":"print(\"Min value of meter_reading is:\", y.min())\nprint(\"Median value of meter_reading is:\", y.median())\nprint(\"Max value of meter_reading is:\", y.max())","403ad7ef":"def fit_evaluate_model(model, X_train, y_train, X_valid, y_valid):\n    model.fit(X_train, y_train)\n    y_predicted = model.predict(X_valid)\n    return sqrt(mean_squared_error(y_valid, y_predicted))","6a7bf6d1":"%%time\n# create model apply fit_evaluate_model\nlinear_regression = LinearRegression()\nlr_rmse = fit_evaluate_model(linear_regression, X_train, y_train, X_valid, y_valid)\nprint(\"RMSE of the linear regression model is:\", lr_rmse)","b768b0ca":"del linear_regression\ndel lr_rmse\ngc.collect()","9cd576af":"# %%time\n# create scaler\n# scaler = MinMaxScaler()\n\n# apply min_max_scaler to training set and transform training set\n# X_train_scaled = scaler.fit_transform(X_train, y_train)\n\n# transform validation set\n# X_valid_scaled = scaler.transform(X_valid)\n\n# knn_regressor = KNeighborsRegressor()\n# knn_rmse = fit_evaluate_model(knn_regressor, X_train_scaled, y_train, X_valid_scaled, y_valid)\n# print(\"RMSE of the k nearest neighbors regressor is:\", knn_rmse)","270d285a":"%%time\n# create model apply fit_evaluate_model\nlgbm_regressor = lgb.LGBMRegressor(random_state=42)\nlgbm_rmse = fit_evaluate_model(lgbm_regressor, X_train, y_train, X_valid, y_valid)\nprint(\"RMSE of the light gbm regressor is:\", lgbm_rmse)","b5686ef1":"del lgbm_regressor\ndel lgbm_rmse\ngc.collect()","27bb8c92":"# create categorical features \ncategorical_features = ['building_id', 'site_id', 'meter',\n                        'primary_use', 'wind_compass_direction',\n                        'day_of_week', 'hour','is_weekend', 'season']","c6d12adb":"# tranform training and validation set into lgbm datasets\ntrain_dataset = lgb.Dataset(X_train, label=y_train, \n                            categorical_feature=categorical_features, \n                            free_raw_data=False)\nvalid_dataset = lgb.Dataset(X_valid, label=y_valid, \n                            categorical_feature=categorical_features, \n                            free_raw_data=False)\n\n# to record eval results for plotting\nevals_result = {} \n\n# initial parameters of light gbm algorithm\ninitial_params = {\"objective\": \"regression\",\n                  \"boosting\": \"gbdt\",\n                  \"num_leaves\": 60,\n                  \"learning_rate\": 0.05,\n                  \"feature_fraction\": 0.85,\n                  \"reg_lambda\": 2,\n                  \"metric\": {'rmse'}\n}","d8765ee1":"print(\"Building model with first 3 quarter pieces and evaluating the model on the last quarter:\")\nlgb_model = lgb.train(initial_params, \n                      train_set = train_dataset, \n                      num_boost_round = 1000, \n                      valid_sets=[train_dataset, valid_dataset],\n                      verbose_eval = 100,\n                      early_stopping_rounds = 500,\n                      evals_result=evals_result)","75443542":"print('Training and Validation Error of the Model')\nax = lgb.plot_metric(evals_result, metric='rmse')\nplt.show()","65c579c5":"del lgb_model\ndel train_dataset\ndel valid_dataset\ndel X_train\ndel X_valid\ndel y_train\ndel y_valid\ngc.collect()","8673c1ee":"# add back to train and validation sets\n# X = pd.concat([X_train, \n#              X_valid])\n\n#y = pd.concat([y_train,\n#               y_valid])","cf2ea574":"# cretae kfold object and empty model and evaluation lists\nkf = KFold(n_splits=4, shuffle=False, random_state=42)\nmodels = []\nevaluations = []\n\n# dynamically split X and y with the k-fold split indexes\nfor train_index,valid_index in kf.split(X):\n    X_train_kf = X.loc[train_index]\n    y_train_kf = y.loc[train_index]\n    \n    X_valid_kf = X.loc[valid_index]\n    y_valid_kf = y.loc[valid_index]\n    \n    d_train = lgb.Dataset(X_train_kf, \n                          label=y_train_kf,\n                          categorical_feature=categorical_features, \n                          free_raw_data=False)\n    \n    d_valid = lgb.Dataset(X_valid_kf, \n                          label=y_valid_kf,\n                          categorical_feature=categorical_features, \n                          free_raw_data=False)\n    evaluation_result = {}\n    \n    model = lgb.train(initial_params, \n                      train_set=d_train, \n                      num_boost_round=1000, \n                      valid_sets=[d_train, d_valid],\n                      verbose_eval=100, \n                      early_stopping_rounds=500,\n                      evals_result=evaluation_result)\n    \n    models.append(model)\n    evaluations.append(evaluation_result)\n    \n    del X_train_kf, y_train_kf, X_valid_kf, y_valid_kf, d_train, d_valid\n    gc.collect()","49182959":"print('Training and Validation Error of the First Fold')\nax = lgb.plot_metric(evaluations[0], metric='rmse')\nplt.show()","b5fcf883":"print('Training and Validation Error of the Second Fold')\nax = lgb.plot_metric(evaluations[1], metric='rmse')\nplt.show()","056e546f":"print('Training and Validation Error of the Third Fold')\nax = lgb.plot_metric(evaluations[2], metric='rmse')\nplt.show()","e2ec82eb":"'''# objective function to optimize\ndef objective(params):\n    # parameters to perform search\n    params = {\n        'num_leaves': int(params['num_leaves']),\n        'colsample_bytree': '{:.2f}'.format(params['colsample_bytree']),\n        'learning_rate': '{:.3f}'.format(params['learning_rate'])}\n    \n    # model and parameters to kept constant\n    lgb_model = lgb.LGBMRegressor(\n        reg_lambda= 2,\n        **params,\n        random_state=42)\n    \n    score = fit_evaluate_model(lgb_model, X_train, y_train, X_valid, y_valid)\n    print(\"RMSE is {:.4f} with parameters {}\".format(score, params))\n    return score\n\n# define search space\nspace = {\n    'num_leaves': hp.choice('num_leaves', range(60, 130, 2)),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.8, 1.0),\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.1)\n}\n\nSEED = 42\n# best model with the hyperopt\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5,\n            rstate= np.random.RandomState(SEED))'''","56abe1f7":"X.to_csv('X.csv', index=False)\ny.to_csv('y.csv', index=False)","7b9fc5a2":"Distance based models requires scaling prior to the applying the model. So specific to k-nearest regressor I am going to apply min_max_scaler from the scikit-learn library.","e4db1539":"I am going to add the mean of the numeric weather variables.","5a85f28e":"# <a id='4'> 4. Compare Several Machine Learning Models <\/a>\n<a href='#top'> Back to Top <\/a>","8cd32310":"## <a id='4-4'>  4.4. Setting the initial parameters of light gbm <\/a>\n<a href='#top'> Back to Top <\/a>","85b7b2be":"After performing the cross validation the lowest RMSE is 1.06, overall resuting in avearge RMSE of 1.12. So in the last notebook I will perform cross_validation on the model then generate the predictions.","3865077d":"## <a id='3-1'>  3.1. Feature Generation & Imputation <\/a>\n<a href='#top'> Back to Top <\/a>","7179d36d":"### <a id='3-1-4'> 3.1.4. Merge dataframes <\/a>\n<a href='#top'> Back to Top <\/a>","d00f0e7a":"**Reset indexes to transfrom weather dataframe to original form**","701b29a7":"#### <a id='3-1-2-3'>  3.1.2.3. Convert wind direction  <\/a>\n<a href='#top'> Back to Top <\/a>","57feffb6":"## <a id='3-3'> 3.3. Split train set into training and validation set <\/a>\n<a href='#top'> Back to Top <\/a>","f01b6bf7":"**Some Notes on Collinearity:**\n<br> Usually a data scientist don't want highly collinear features going into the model. \n\nI dropped the original feature if I have generated a new one. There are some collinear features left though from the original dataset: site_id & building_id and air_temperature & dew_temperature.\n\nAs both feature sets play a major role determining log_meter_reading values I am going to keep both sets, and add precip_depth_1_hr additionally.","c21c767f":"# <a id='6'> Conclusions <\/a>\n<a href='#top'> Back to Top <\/a>","dd0ef07c":"#### <a id='3-1-2-1'> 3.1.2.1. Add time related data from timestamp <\/a>\n<a href='#top'> Back to Top <\/a>","2eb473b9":"In the initial round we started with the 1.31 RMSE of the validation error, and in the final iteration we received 1.19 RMSE for the validation set, 0.12 point decrease, which is a 9 percent improvement overall.\n\nTraining error decreased more rapidly through the iterations from 0.95 to 0.68, 0.27 point decrease. This shows that the model learns faster from the training set and this is a consequence of model is trained with the same set of training set in every iteration.\n\nWe learned one thing from the initial parameter set round and the iterations: \n\n* Both training and validation errors decreased, so if I set the num_boost rounds to a higher number, I would probably achieve less RMSE in the cost of higher runtimes. \n* Gap between training and validation error increased, so this is a sign of we are overfitting, to eliminate the possibility of overfitting I will introduce cross-validation into the model rather than hyperparameter tuning.","3ebf560f":"Strongest correlation we get is -0.1 and 0.2 which is not as high as I expected. I am going to try looking at the correlations with the natural logarithm of the meter reading+1 using np.log1p. This might help, as it helped for the visualization case.","536dd73e":"## <a id='3-2'> 3.2. Feature Selection & Further Imputation <\/a>\n<a href='#top'> Back to Top <\/a>","bd0b7ff1":"Here is the definition of the initially set parameters from the [documentation](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html):\n\n**LGBM train Parameters:**\n* `feature_fraction`: randomly selected part of features on each iteration (tree) (default=1) \n* `num_boost_round`: Number of boosting iterations (default=100)\n* `boosting`: traditional Gradient Boosting Decision Tree\n* `num_leaves`: max number of leaves in one tree (default=31), to control tree complexity\n* `learning_rate`: the rate at which the model learning from the train set and achieving the best results (default=0.1), together with num_leaves used to control overfitting and underfitting\n* `reg_lambda`: parameter to apply L2 regularization (default=0)","21177dc4":"This shows average estimate is 2 points away from the training median in total. Log_meter_reading values are spread between 0 and 17 so RMSE of 2 shows that the average error from a naive method is about 12%. Although this is a pretty high baseline already let's see if any machine learning model can beat that score. \ud83e\uddd0","0d5d47a5":"# <a id='3'> 3. Feature Engineering & Selection <\/a>\n<a href='#top'> Back to Top <\/a>","effe9378":"In the previous notebook we had a look at the  distirbution of the target(meter_reading) from various aspects and its relationship of each variable to the meter reading values. Moreover, we observed distribution, missing values and time-series plot of each individual feature.\n\n**Here is the distribution of the natural logarithm of the meter reading+1 among different meter categories:**\n\n![image.png](attachment:image.png)\n\nThe training data from 2016 has 20 million rows, and using that data we are going to predict 40 million rows of 2017 and 2018 data in different meter categories. \n\nThere are 15 columns to use as a potential feature set. Some features highly impacting the energy consumption, like square_feet and air_temperature. However, some are not that much significant. \n\nIf a not-signifciant column has also high missing values like floor_count, it will be dropped. Others will be transformed into categorical features (like wind_direction). Other weather data will be used for feature generation.","e51225d4":"This notebook aims to predict a building's energy consumption over 2017 and 2018 using the data from 2016 in 4 different consumpiton categories (electricity, chilled water, steam, hot water) using ASHRAE data, which is our problem statement as well.\n\nThis is a supervised machine learning model, meaning based on the columns available in the datasets and data from 2016, we are going to train the model to predict an energy consumption of a building in each category. Since, consumption values are labeled as meter_reading and they are continuous, we are going to apply regression techniques to generate predictions on meter_reading.\n\nIt is a highly debated and popular competition in Kaggle currently, however my main motivation is to contribute to make energy-efficient buildings by estimating its energy consumption. It seemed like a good start to save our energy for future!\n\nThere will be 3 notebooks covering the complete machine learning building pipeline.\n\n[Notebook 1](https:\/\/www.kaggle.com\/cereniyim\/save-the-energy-for-the-future-1-detailed-eda) covered understanding the data and detailed EDA.\n\nThis notebook will focus on parts 3, 4 and 5; feature engineering, building the machine learning model and tuning its parameters.\n\n\n1) Understand, Cleand and Format Data\n\n2) Exploratory Data Analysis\n\n**3) Feature Engineering & Selection**\n\n**4) Compare Several Machine Learning Models**\n\n**5) Perform Hyperparameter Tuning and Cross Validation**\n\n6) Evaluate Model with Test Data\n\n7) Interpret Model Results\n\n8) Submissions & Summary & Conclusions\n\n[Notebook 3](niyim\/save-the-energy-for-the-future-3-predictions) will generate the predictions with the selected model in this notebook and interpret them with an overall summary.\n\nMachine Learning application and building is not a linear and one time process. Steps above enable me to follow a structured way for an end-to-end machine project flow and preparation for the each step ahead. All in all, steps might be modified or revisited according to findings. You can use the table of contents to navigate to each section \ud83d\udc47\n\nEnjoy reading !","09ddcc66":"#### <a id='3-1-2-2'> 3.1.2.2. Impute missing values for weather variables <\/a>\n<a href='#top'> Back to Top <\/a>","23640130":"## <a id='4-1'> 4.1. Linear Regression <\/a>\n<a href='#top'> Back to Top <\/a>","2a8dd450":"**Wind Speed**","eb6525de":"This is the end of the second notebook, if you are still with me, thank you for your patience \ud83d\ude07\n\nIn this notebook I covered:\n\n3. Feature Engineering & Selection\n4. Compare Several Machine Learning Models\n5. Perform Hyperparameter Tuning on the Best Model\n\nusing the findings of the [first notebook](https:\/\/www.kaggle.com\/cereniyim\/save-the-energy-for-the-future-1-detailed-eda\/notebook#--Conclusions-).\n\nWith the feature engineering we generated useful features with intuition and combination of numeric features.\n\nLight GBM proved its robustness when working with a large dataset, outperformed linear regression and selected as best model. Thanks to the public notebooks available I was able to start from a good place and lower the RMSE from 1.45 to 1.2 with the initial parameters set. Thank you all Kagglers, for the collaboration!\n\nI also used this as a chance to explore light GBM library. After tuning the parameters of light GBM, I used hyperopt library to perform hyperparameter tuning. It can be an alternative to Grid Search as it performs more comprehensive search. When the model is introduced with cross validation, average validation error is decreased to 1.1, so in the next notebook model will be cross-validated then the predictions will be generated.\n\nBefore moving to the third notebook, I faced the challenges of working with 20 million rows all the time. I received runtime errors and ran out of RAM. Morever, some functions did not work (E.g. Simple Imputer) or a distance based model simply ran forever, I had to skip revisiting feature selection.\n\nKeeping there are 40 million rows in the test dataset, we are ready to generate predictions for our supervised regression model. Based on the 30 features and available, training dataset (1 year time-series data) and our light gbm model let's go and find the energy consumption of the buildings for 2017 and 2018.\n\nI will save the features and taget and move on to the next steps, see you on the [third notebook](https:\/\/www.kaggle.com\/cereniyim\/save-the-energy-for-the-future-3-predictions) \ud83d\udc4b","14042bb6":"Cross validation allows the model to iteratively run on the different portions of the training set. \n\nRecall that, I initially manually split train and test split into 4 pieces, allocated first 3 quarters as training set and last quarter as validation set.\n\nNow, I am going to iteratively run the model on the 75% of the training data and see if RMSE will improve.\n\nI am going to use [KFold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html), and split X to 4 pieces and run light GBM iteratively on the each fold. \n\nHere is a quick visual from scikit-learn's website of how Kfold performs splits:\n![image.png](attachment:image.png)","615e4a7f":"When we are loading the datasets, I dropped floor_count due to the 75% of the data is missing. For the building dataset, I am going to convert year_built to age column as I did for the visualization case and impute missing values of the building_age with the mean.\n\nAnd I will tranform the primary_use column to a numeric column by using one of the encoding strategies.","8771fa99":"Recall that, although I performed imputation on the train datasets individually we still have some NA values in the training dataset. None of the machine learning models can work with NA values, I am going to replace NA values. To replace I am going to use [fillna function](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.fillna.html) for simplicity and to avoid data leakage. With this method  propagate last valid observation forward to next valid backfill.\n\nThis isa benefical point because, it prevents future data leaking into the training dataset. Preventing data leakage is important in a way [defined](https:\/\/machinelearningmastery.com\/data-leakage-machine-learning\/) by the machine learning mastery as follows:\n> Data leakage is when information from outside the training dataset is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the model being constructed.","74d526e0":"In this part, using the findings from the EDA and with some human interpretation, I will generate more features. \n\nAdding and extracting more features based on the existing ones and deciding on which ones to use whether with an algorithm or by manual selection using the human or domain knowledge is called feature enegineering. (definition from wikipedia is [here](https:\/\/en.wikipedia.org\/wiki\/Feature_engineering)).\n\nSome common options for feature engineering:\n* automatic tools like [featuretools](https:\/\/www.featuretools.com\/): this does all the process defined for us, and I gave it a shot to use in this project. I guess due to the amount of data we have, I faced some runtime errors.\n* transformations of numerical columns: by taking the logarithm and square root\n* transformation of categorical columns: with several encoding techniques\n* combination of numeric features: by creating linear combinations of the numeric features\n* manual approach: looking at the definitions of the features and by understanding the relationship between them\n<br> I will follow the manual and combination approach \ud83d\udc46<\/br>\n\nWe have 15 unique columns in all three train datasets. I think 15 is a relatively low number for a feature set if we think that we have 40 million rows to predict in the test set. Using very few features might end up in underfitting.\n\nMoreover, when I search my own ML notes, here is the one of the valuable learnings I got from Andrew Ng's Machine Learning course is (quoting from my personal notes):\n\n> To get a low test error, use algorith with many features. This will ensure to create a low-bias algortihm. On the other hand, if you feed your algorith with a very large training set you will lower the variance. This way you can balance out the bias and variance, increasing the probablity of getting a low test error model.\n\nSo, for this specific project, I am going to impute each dataset first then add and transform features:\n\n* tranformation numerical features into categorical\n  * wind_direction\n  * primary_use\n* extract features from existing columns\n  * from year_built\n    * age of the building\n  * from timestamp\n    * month\n    * hour\n    * day of the month\n    * day of the week\n    * is_weekend\n    * season\n* combine numeric features\n  \nMoreover, I will try a different approach and impute the missing values at the same time with the feature generation before merging the 3 train data frames.\n\nMoreover, I will change imputing strategy to mean, meaning:\n- for the building dataset I will impute the columns with the mean\n- for the weather_train dataset I will imputer each weather variable with the daily mean for that month of the measurements in each site\n\nAfter the generation and imputing, I will look at the pearson coefficients of the features to the target and look at the [collinearity](https:\/\/en.wikipedia.org\/wiki\/Multicollinearity) (to be discussed later on!) between the features to decide on the final set.","b15a1b8b":"There are several methods and algorithms available for feature selection:\n* [removing highly collinear features](https:\/\/stackoverflow.com\/questions\/29294983\/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on\/43104383#43104383)\n* [principal component analysis](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis)\n* [functions in the feature selection class from from scikit-learn](https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html)\nand so on...\n\nAs a quick and dirty solution and to start with I am going to look at pearson coefficients correlations. This is not the best solution for feature selection as it only considers the correlation between one feature to the target, one at a time, rather than considering the consolidated effect of the features to the target.","6f2f9b38":"**Sea Level Pressure**","9511caad":"Now we have 15 numerical features and 9 categorical features to implement.","068f9cdc":"For the feautre selection, if pearson coeffiecient is greater than the absolute value of 0.004, I will keep that feature, and this method includes meter_reading in the feature set so I replace it and include the original feature precip_depth_1_hr.","2cdba000":"## <a id='3-4'> 3.4. Create a Baseline Metric <\/a>\n<a href='#top'> Back to Top <\/a>","c9998948":"I am going list how closely related each feature to the target. To do that, I am going to create pearson coefficients dataframe.\n\nA Pearson correlation is a number between -1 and 1 that indicates the extent to which two variables are linearly related ([source](https:\/\/www.spss-tutorials.com\/pearson-correlation-coefficient\/)). So as it gets closer to 1 or -1, the correlation gets stronger.","dd8d55c6":"**Cloud Coverage**","edbd139a":"# <a id='5-2'> 5. Updated on Version 10: Perform Cross Validation on the Best Model <\/a>\n<a href='#top'> Back to Top <\/a>","3545fd8d":"After the merge we still have some null values, due to the fact that train dataset has more observations than the tweather_train dataset. I am going to keep it as is for now, and impute missing values before creating the ML models.","bacdd355":"Searching and setting the best and optimal set of parameters for a machine learning model can be defined as hyperparameter tuning. In my previous projects I used [random search](https:\/\/en.wikipedia.org\/wiki\/Random_search) and [grid search](http:\/\/) algorithms but in this project I am going to try [hyperopt library](https:\/\/github.com\/hyperopt\/hyperopt), a library built for performing the hyperparameter tuning. Here is a good [notebook](https:\/\/www.kaggle.com\/eikedehling\/tune-and-compare-xgb-lightgbm-rf-with-hyperopt) on understanding how hyperopt works.\n\nI will define a search space for the parameters to be fine tuned, the objective -the evaluation metric that I am trying to decrease so far- and run the optimization function and let hyperopt to suggest me the best parameters.\n\nThe features that I am going to tune is:\n\n* Num_leaves: since this is discrete integer values, `hp.choice` will be used and sequential space of 60, 62, .., 128, 130 will be searched.\n* col_sample_by_tree is the equivalent parameter of the feature_fraction in the previously trained model.\n* Learning rate & (col_sample_by_tree): since they are continuous float values, `hp.uniform` will be used:\n  * Real numbers between 0.01 to 1 will be seacrhed\n  * Real numbers between 0.8 to 1.0 will be searched respectively.\n* Reg_lambda kept constant for this search.\n* Default value for boosting_type is 'gdbt', so I am not going to specify it explicitly.\n\n\nAdjudting num_leaves and learning rate together helps to control the trade-off between underfit and overfit. [Notebook](https:\/\/www.kaggle.com\/eikedehling\/tune-and-compare-xgb-lightgbm-rf-with-hyperopt) contains a handy function already for a light gbm model, I am going to implement and modify it according to my objectives.","d2d44ba0":"Now we are going to split the training set into train and validation set.\n\n**Training set** will be used to for the model to learn and capture the patterns and relate them to the target.\n<br> **Validation set** will be used for to understand how well the model generalizes learnings and captures to the newly introduced datasets. \n\nThe score of the evaluation metric obtained from the validation set can be an indicator of the test error.\n\nI removed the timestamp column but it is still a time series data, meaning it has its own trends and seasonality factors. If this dataset was time independent, scikit-learn's train_test_split can be applied straightforward, but I want to keep my training and validation set sequential during the split.\n\nSo, I am going to use custom approach and split train and validation set as below. This way, I am going to keep first 75% of the columns as training set and last quarter will be validation set.","b70af2ce":"**Reset Index for Update**","0a34256a":"We had a quite deal of missing values in the training dataset, except site_id and time_stamp:\n![image.png](attachment:image.png)\n\nI will impute evey missing data from the daily mean of each site per month. This is the most logical and [proven method](https:\/\/www.kaggle.com\/aitude\/ashrae-kfold-lightgbm-without-leak-1-08) (so far). For that, I am going to use update function of pandas.\n\nI modified the function from [this notebook](https:\/\/www.kaggle.com\/aitude\/ashrae-kfold-lightgbm-without-leak-1-08) for imputation. If daily means per month per site_id is not available then I will fill it with the mean values of the filler object created by goruping site_id, day and month.","5718131a":"We achieved an improvement compared to linear regression. Linear regression yielded 1.98 of RMSE whereas light GBM's RMSE was 1.44. So, I will continue further tuning and adjusting the light gbm to achieve less RMSE.\n\nLinear regression resulted in worse RMSE in less runtimes. Distance based algorithm had the highest run time. Light gbm has the highest run time, but generated the best result so far. This results are a concrete example that supports Rachel Tatman's (@rctatman) summary table of the machine learning algorithm's performances from [PyCon 2019](https:\/\/youtu.be\/qw5dBdTXLEs?t=1372).\n\nSo let's start some tuning!","8969935b":"#### <a id='3-1-1-1'> 3.1.1.1. Age from year built <\/a>\n<a href='#top'> Back to Top <\/a>","3e5b96b1":"### <a id='3-2-1'> 3.2.1. Pearson coefficients of the features <\/a>\n<a href='#top'> Back to Top <\/a>","02e918a0":"**Function to evaluate model's performance on train and validation set with RMSLE**","b8b82175":"### <a id='3-1-2'> 3.1.2. Weather features transformation <\/a>\n<a href='#top'> Back to Top <\/a>","60199636":"We have 36 columns after the feature generation one of them being the target value: `meter_reading`.","a69956d7":"So the best iteration we get is with the parameters:\n\n* num_leaves: 106\n* colsample_bytree: 0.99\n* learning_rate: 0.075\nLet's update our initial parameters, and see if we can lower training and validation RMSE.\n\nSurprisingly, the hyperparameter search's parameters did not result in lower RMSE both for training and validation set. So maybe we already found the optimal model with the selected features and initial parameters.\n\nSo I am going to use the inital parameters specified in the 4.5. Setting the initial parameters of light gbm while generating the predictions.","1461ab82":"Fetching month, day of the week, day of month and hour from the timestamp column will help the model to catch the seasonality impact without using the timestamp column when training the model.\n\nAdditionally, I will add is_weekend and season (winter, spring, summer and fall) to emphasize seasonality in the model.","8b58896c":"# <a id='1'> Quick Recap from the Previous Notebook <\/a>\n<a href='#top'> Back to Top <\/a>","00f18c41":"### <a id='3-1-3'> 3.1.3. Combine numeric features <\/a>\n<a href='#top'> Back to Top <\/a>","25ed21ee":"# <a id='5-1'> 5. Perform Hyperparameter Tuning on the Best Model <\/a>\n<a href='#top'> Back to Top <\/a>","cca818cc":"Linear regression model guessed almost the same as the baseline model. Let's see how light gbm will perform.","485da186":"It took more than 10 mins to run k-nearest neighbors regressor and it did not generated any results, so I am going to skip that algorithm. ","3ecdb24e":"We did a heavy lifting job until here and faced the challenges of dealing with the big data, failed to use some libraries in feature selection due to the runtime errors. So this showed me that I need some robust algorithms for this project. \n\nSo I will try out three models: a regression model, a distance based model and a tree-based algorithm that can handle big datasets.\n\nI am going to run simple linear regression, k nearest neighbors regressor and a light gradient boosting machine. Light gradient boosting machine already proved its robustness for big data in several notebooks in this competition. Here are some exmaple notebooks here: [Notebook1](https:\/\/www.kaggle.com\/rohanrao\/ashrae-half-and-half\/output), [Notebook2](https:\/\/www.kaggle.com\/isaienkov\/lightgbm-fe-1-19).","5b38acab":"### <a id='3-1-1'> 3.1.1. Building features transformation <\/a>\n<a href='#top'> Back to Top <\/a>","5554f3b8":"Recall that, wind_direction values span between 0 and 365 representing compass direction. I will aggreagte wind_direction values under 0, 1, 2 and 3 each for Northeast, Southeast, Southwest and Northwest directions.\n\nAfter converting to categorical data, I am going to drop wind_direction column.","65849819":"There are still NA values, so I will first fill NA values of the filler with the rounded mean value (because precip_depth_1_hour takes discrete values), as I did for cloud_coverage.","5409a83d":"#### <a id='3-1-1-2'>  3.1.1.2. Primary use  <\/a>\n<a href='#top'> Back to Top <\/a>","89c00057":"As light GBM outperformed linear regression, let's set some initial parameters for the algorithm. One note is that, when categorical features are specifically specified, light GBM works better. I am going to treat some of the numeric features as categorical features. Here is how more categorical features work better in light gbm implementation:\n\n> LightGBM offers good accuracy with integer-encoded categorical features. LightGBM applies Fisher (1958) to find the optimal split over categories as described here. This often performs better than one-hot encoding.\n\nA good summary and how Light GBM works can be found in this [blog post](https:\/\/towardsdatascience.com\/lightgbm-vs-xgboost-which-algorithm-win-the-race-1ff7dd4917d), which I also benefitted a lot for this project. Let's move to the implementation.\n\nTo decide on the inital parameters, I used the ones in this [notebook](https:\/\/www.kaggle.com\/rohanrao\/ashrae-half-and-half\/output).","150cc99c":"## <a id='4-3'> 4.3. Light GBM <\/a>\n<a href='#top'> Back to Top <\/a>","6d81899b":"## <a id='4-2'> 4.2. KNeighbors Regressor<\/a>\n<a href='#top'> Back to Top <\/a>","a3dd4d97":"# <a id='top'> Table of Contents <\/a>\n- <a href='#1'>Quick Recap from the Previous Notebook<\/a>\n\n\n- <a href='#3'> 3. Feature Engineering & Selection <\/a>\n  - <a href='#3-1'> 3.1. Feature Generation & Imputation <\/a>\n    - <a href='#3-1-1'> 3.1.1. Building features transformation <\/a>\n      - <a href='#3-1-1-1'> 3.1.1.1. Age from year built <\/a>\n      - <a href='#3-1-1-2'>  3.1.1.2. Primary use  <\/a>\n    - <a href='#3-1-2'> 3.1.2. Weather features transformation <\/a>\n      - <a href='#3-1-2-1'> 3.1.2.1. Add time related data from timestamp <\/a>\n      - <a href='#3-1-2-2'> 3.1.2.2. Impute missing values for weather variables <\/a>\n      - <a href='#3-1-2-3'>  3.1.2.3. Convert wind direction  <\/a>\n    - <a href='#3-1-3'> 3.1.3. Combine numeric features <\/a>\n    - <a href='#3-1-4'> 3.1.4. Merge dataframes <\/a>\n  - <a href='#3-2'> 3.2. Feature Selection & Further Imputation <\/a>\n    - <a href='#3-2-1'> 3.2.1. Pearson coefficients of the features <\/a>\n    - <a href='#3-2-2'> 3.2.2. Imputation of Train Dataset <\/a>\n  - <a href='#3-3'> 3.3. Split train set into training and validation set <\/a>\n  - <a href='#3-4'> 3.4. Create a Baseline Metric <\/a>\n\n\n- <a href='#4'> 4. Compare Several Machine Learning Models <\/a>\n  - <a href='#4-1'> 4.1. Linear Regression <\/a>\n  - <a href='#4-2'> 4.2. KNeighbors Regressor<\/a>\n  - <a href='#4-3'> 4.3. Light GBM <\/a>\n  - <a href='#4-4'> 4.4. Setting the initial parameters of light gbm <\/a>\n\n\n- <a href='#5-1'> 5. Perform Hyperparameter Tuning on the Best Model <\/a>\n- <a href='#5-2'> 5. Updated on Version10: Perform cross validation on the Light GBM <\/a>\n- <a href='#6'> Conclusions <\/a>","b2553dba":"There are still NA values, so I will first fill NA values of the filler with the mean value, as I did for cloud_coverage.","7e8d615e":"Recall that we found some seasonality factors in the time-series data especially for the chilled and hot water consumption.\n![image.png](attachment:image.png)","2b1adfa6":"We observed that when we transform year_built values it will scale it to 0 to 120 which bring better intepretability to our model. \n\nAfter adding age I am going to drop year_built because they are providing the same information to the model. As a final step, I will fill missing values with the mean.","a1b6dcaa":"**Imports:**\nI will mainly use scikit-learn library for the machine learning model building and give the light gbm library a try. For the hyperparameter tuning I will use hyperopt.","8d468122":"**Dew Temperature**","57d2272a":"There are several options for encoding a categorical variable: label encoding, one-hot-encodig, count-encoding and so on. \n\nI want to have only one column for primary_use, so I am going to keep it simple and go with the label encoding.","eb5bf940":"### <a id='3-2-2'> 3.2.2. Imputation of Train Dataset <\/a>\n<a href='#top'> Back to Top <\/a>","74d9626d":"**Lets check again for the null values one more time**","acddd175":"Before diving deep into the ML algorithms, I am going to calculate a common sense baseline. A common sense baseline is defined in this [article](https:\/\/towardsdatascience.com\/first-create-a-common-sense-baseline-e66dbf8a8a47) in simple terms, how a person has a knowledge in that field would solve the problem without using any data science tricks. Alternatively, as explained in this [post](https:\/\/machinelearningmastery.com\/implement-baseline-machine-learning-algorithms-scratch-python\/), it can be a dummy or simple algorithm, consisting of few lines of code, to use as a baseline metric.\n\nBaseline metrics can be [different](https:\/\/machinelearningmastery.com\/how-to-get-baseline-results-and-why-they-matter\/) in regression and classification problems. For a regression problem it can be a central tendency measure as the result for all predictions, such as the mean or the median.\n\nSince this a regression problem and competition's results will be [evaluated](https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/overview\/evaluation) for root mean squared logarithmic error, \n\n![image.png](attachment:image.png)\n\nBaseline metrics are important in a way that, if a ML model cannot beat the simple and intuitive prediction of a person's or an algorithm's guess, the original problem needs reconsideration or training data needs reframing.\n\nI have already applied log1p transformation to the target so throughout the project I am going to use RMSE (root mean squared error) as the single evaluation metric, which is:\n\n![image.png](https:\/\/miro.medium.com\/max\/966\/1*lqDsPkfXPGen32Uem1PTNg.png)\n\nI am aware that those metrics are not the same, however I already decreased the variance in the target by applying log1p, and I expect a machine learning model to generate predictions within the same ranges as log1p of target. By measuring it with RMSE and predicting log1p of the target, I can get closer results to measuring with RMSLE and target.","007906fd":"**Wind Direction**","d7ee3c16":"![image](https:\/\/www.technotification.com\/wp-content\/uploads\/2018\/09\/Renewable-Energy-Ideas-1200x600.jpg)\nImage source: [technotification](https:\/\/www.technotification.com\/2018\/09\/amazing-renewable-energy-ideas.html)","a4df6062":"**Precip Depth 1 Hour**","b81f842f":"There are still NA values, so I will first fill NA values of the filler with the rounded mean value, because cloud_coverage takes discrete values.","5bf1ce4d":"In the above folds the gap between the training and validation RMSE is less than the first model built without cross validation. So as we intorduced different training sets, model learned better to interpret the newly intorduced datasets.","1b9a3fec":"**Air temperature**"}}