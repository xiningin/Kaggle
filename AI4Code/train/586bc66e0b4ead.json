{"cell_type":{"16d90365":"code","6d5edf5d":"code","853537d8":"code","64894415":"code","4d241fc6":"code","aa651d71":"code","58fb6fe0":"code","4b7bf3b8":"code","842a61ed":"code","b57e99b4":"code","597faeca":"code","c450d46d":"code","9b5c4e74":"code","ad2263c1":"code","62adb64f":"code","0da8ed57":"code","b1789a65":"code","65449376":"code","be4764ec":"code","09c82430":"code","934e6dcc":"code","7433c086":"code","2f71112e":"code","987034d1":"code","172280e1":"code","2125f6ac":"code","b037c267":"code","11e8c07d":"code","8f7bee21":"code","68f3ed37":"code","1e52b5ae":"code","999331de":"code","7da13a13":"markdown","ca41f763":"markdown","2aa21a73":"markdown","50665fe9":"markdown","d312798b":"markdown","b90c3c86":"markdown","994a22a4":"markdown","4032689d":"markdown","c8333fe6":"markdown","42e69e8f":"markdown","89fc07c3":"markdown","aab2d8dd":"markdown","2bc6d7bb":"markdown","eda77188":"markdown","46e418e9":"markdown","7f792073":"markdown","a5cba0c2":"markdown","5b74fe51":"markdown","02f3bb91":"markdown","390f5970":"markdown"},"source":{"16d90365":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\nimport missingno as msno\nfrom matplotlib import pyplot as plt\nfrom datetime import date\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\nfrom pandas.core.common import SettingWithCopyWarning\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate\nfrom sklearn.model_selection import RandomizedSearchCV","6d5edf5d":"warnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)","853537d8":"df = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")","64894415":"df.head(10)","4d241fc6":"df.shape","aa651d71":"df.dtypes","58fb6fe0":"# Number of NAs in our columns\ndf.isnull().sum()\n# As you can see in the results, we have 56 NA values in our target variable","4b7bf3b8":"df.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T","842a61ed":"# We checked our dependent variable \"Salary\" with graphs to see the distribution of data.\n\ndf[\"Salary\"].describe()\nsns.distplot(df.Salary)\nplt.show()","b57e99b4":"sns.boxplot(df[\"Salary\"])\nplt.show()","597faeca":"# We have 3 categorical and 17 numeric columns\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car\n\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df, cat_th=10, car_th=20)\n","c450d46d":"# We examine the numerical distribution of categorical variable classes and their ratios relative to each other.\n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\nrare_analyser(df, \"Salary\", cat_cols)","9b5c4e74":"\ndef num_summary(dataframe, numerical_col, plot=False):\n\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n\n    print(dataframe[numerical_col].describe(quantiles).T)\n\n    if plot:\n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()\n\nfor col in num_cols:\n    num_summary(df, col, plot=True)","ad2263c1":"# We should remove our target column from numerical columns list.\nnum_cols.remove(\"Salary\")","62adb64f":"def outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name, q1=0.05, q3=0.95):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name, q1, q3)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\nfor col in num_cols:\n    print(col, check_outlier(df, col, q1=0.1, q3=0.9))","0da8ed57":"df.describe()","b1789a65":"def replace_with_thresholds(dataframe, variable, q1=0.05, q3=0.95):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable, q1, q3)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\nfor col in num_cols:\n    if check_outlier(df, col, q1=0.1, q3=0.9):\n        replace_with_thresholds(df, col, q1=0.1, q3=0.9)\n\n\n","65449376":"df.describe()","be4764ec":"def missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns","09c82430":"df.shape","934e6dcc":"missing_values_table(df, True)","7433c086":"df.dropna(inplace=True)\ndf.shape ","2f71112e":"def correlated_map(dataframe, plot=False):\n    corr = dataframe.corr()\n    if plot:\n        sns.set(rc={'figure.figsize': (15, 15)})\n        sns.heatmap(corr, cmap=\"YlGnBu\", annot=True, linewidths=.7)\n        plt.xticks(rotation=60, size=15)\n        plt.yticks(size=15)\n        plt.title('Correlation Map', size=20)\n        plt.show()\n\ncorrelated_map(df, plot=True)","987034d1":"# We can see in the matrix,the correlation between CAtBat and CHits is 1.\n# Therefore, we may remove one of them or we can analysis\n# df.drop(\"CAtBat\", axis=1)\n# df.drop(\"CHits\", axis=1)","172280e1":"## Player success hit rate\ndf['NEW_Success_CHit'] = df['CHits'] \/ df['CAtBat']  # total career success hits \/ total career hits\n## Success hit rate in the 1986-1987 season\ndf['NEW_Success_86_87_Hits'] = df['Hits'] \/ df['AtBat'] # total success hits in 1986-1987 \/ total hits in 1986-1987\n## The point player earned per hit in 1986-1987\ndf['NEW_Success_86_87_Hits'] = df['Runs'] \/ df['AtBat'] # The points player earned for his team in the 1986-1987 season \/ number of hits in the 1986-1987 season\n# Number of misses in the 1986-1987 season\ndf[\"Hits_Success\"] = (df[\"AtBat\"] - df[\"Hits\"])\n## Average number of points,the player earned per year\ndf[\"NEW_CRUNS_RATE\"] = df[\"CRuns\"] \/ df[\"Years\"]\n## Average number of success hits per year\ndf[\"NEW_CHITS_RATE\"] = df[\"CHits\"] \/ df[\"Years\"]\n## The ratio of the number of players when a batsman hit, to player's career time\ndf['NEW_Avg_RBI'] = df['CRBI'] \/ df['Years'] # Number of players run when a batsman hit \/ career year\n\n## I divided it into 3 categorical classes according to helping your teammate during the game.\nPutouts_label = [\"little_helper\", \"medium_helper\", \"very_helper\"]\ndf[\"NEW_PUTOUTS_CAT\"] = pd.qcut(df[\"PutOuts\"], 3, labels=Putouts_label)\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n","2125f6ac":"def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\ndf = one_hot_encoder(df, cat_cols, drop_first=True)\n\ndf.head(10)","b037c267":"y = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)\n\nmodels = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor()),\n          # (\"CatBoost\", CatBoostRegressor(verbose=False))\n          ]\n\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n","11e8c07d":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(2, 40, num = 20)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 3, 5, 6, 8, 10, 15, 20]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)\n","8f7bee21":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation,\n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 200, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X, y)\nbest_param = rf_random.best_params_\nprint(best_param)","68f3ed37":"# Lets check RMSE of train with our best parameters for Random Forest\n\n\ny = df[\"Salary\"]\nX = df.drop(\"Salary\", axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.20, random_state=16)\n\n\nrf = RandomForestRegressor(n_estimators = 800, min_samples_split= 2, min_samples_leaf= 1, max_features= \"sqrt\", max_depth= 30)\n\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_train)\nnp.sqrt(mean_squared_error(y_train, y_pred))","1e52b5ae":"# Lets check RMSE of test data with our best parameters for Random Forest\n\ny_pred = rf.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","999331de":"cart_params = {'max_depth': range(1, 20),\n               \"min_samples_split\": range(2,30)}\n\nrf_params = {\"max_depth\": [3, 8, 10, 20, 30, 32, None],\n             \"max_features\": [3, 5, 7, \"auto\", \"sqrt\"],\n             \"min_samples_split\": [2, 3, 5, 6, 10],\n             \"n_estimators\": [50, 100, 200, 300, 500,800, 1000]}\n\nxgboost_params = {\"learning_rate\": [0.1, 0.01, 0.01],\n                  \"max_depth\": [3, 5, 7,  9, 12],\n                  \"n_estimators\": [50, 100, 150, 200, 300],\n                  \"colsample_bytree\": [0.3, 0.5, 0.8, 1]}\n\nlightgbm_params = {\"learning_rate\": [0.1, 0.01, 0.001],\n                   \"n_estimators\": [300, 500, 700, 900, 1000, 1500],\n                   \"colsample_bytree\": [0.2, 0.3, 0.5, 0.7, 1]}\n\nregressors = [(\"CART\", DecisionTreeRegressor(), cart_params),\n              (\"RF\", RandomForestRegressor(), rf_params),\n              ('XGBoost', XGBRegressor(objective='reg:squarederror'), xgboost_params),\n              ('LightGBM', LGBMRegressor(), lightgbm_params)]\n\nbest_models = {}\n\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \") \n\n    gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model\n    ","7da13a13":"### CATEGORICAL VARIABLE ANALYSIS","ca41f763":" Lets take a general look to data","2aa21a73":"### CORRELATION MATRIX","50665fe9":"### Randomized Search CV","d312798b":"### OUTLIERS ANALYSIS","b90c3c86":"<h1 style=\"background-color:#a83299;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 20px 20px;font-family:cursive\">Salary Prediction with Machine Learning<\/h1>","994a22a4":"### NA OBSERVATION ANALYSIS","4032689d":"## MODELLING","c8333fe6":" ### Business Problem\n Can a machine learning project be implemented to estimate the salaries of baseball players whose salary information and career statistics for 1986 are shared?\n\n ### Dataset Story\n This dataset was originally taken from the StatLib library at Carnegie Mellon University.\n \n The dataset is part of the data used in the 1988 ASA Graphics Section Poster Session.\n \n Salary data originally from Sports Illustrated, April 20, 1987.\n \n 1986 and career statistics are from the 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n\n","42e69e8f":"### NUMERICAL VARIABLE ANALYSIS","89fc07c3":"### Automated Hyperparameter Optimization","aab2d8dd":"### ONE-HOT ENCODING","2bc6d7bb":"\n### Variables\n\n AtBat: Number of hits with a baseball bat during the 1986-1987 season\n \n Hits: Number of hits in the 1986-1987 season\n \n HmRun: Most valuable hits in the 1986-1987 season\n \n Runs: The points he earned for his team in the 1986-1987 season\n \n RBI: Number of players jogged,  when a batsman hit\n \n Walks: Number of mistakes made by the opposing player\n \n Years: Player's playing time in major league (years)\n \n CAtBat: Number of hits during a player's career\n \n CHits: The number of hits the player has taken throughout his career\n \n CHmRun: The player's most valuable hit during his career\n \n CRuns: Points earned by the player during his career\n \n CRBI: The number of players the player has made during his career\n \n CWalks: Number of mistakes made by the opposing player during the player's career\n \n League: A factor with A and N levels showing the league in which the player played until the end of the season\n \n Division: A factor with levels E and W indicating the position played by the player at the end of 1986\n \n PutOuts: Helping your teammate in-game\n \n Assists: Number of assists made by the player in the 1986-1987 season\n \n Errors: Player's number of errors in the 1986-1987 season\n \n Salary: The salary of the player in the 1986-1987 season (over thousand)\n \n NewLeague: a factor with A and N levels indicating the player's league at the start of the 1987 season\n\n","eda77188":"### DETERMINING CATEGORICAL AND NUMERICAL VARIABLES","46e418e9":"## Exploratory Data Analysis (EDA)","7f792073":" Here, by examining the distribution of the numeric variable in the data, its maximum and minimum values,\n we get information before performing outlier analysis.","a5cba0c2":" We have 59 NA values in our target column, now we should remove them","5b74fe51":"### DEPENDENT VARIABLE ANALYSIS","02f3bb91":"### FEATURE EXTRACTION","390f5970":" We have some outliers in our \"CHits\",  \"CHmRun\", \"CWalks\" columns. And we want to change outliers to thresholds.\n \n We can not directly remove them, due to we don't want to decrease our data"}}