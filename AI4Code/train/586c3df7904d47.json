{"cell_type":{"5912bf0c":"code","ec9b1bc8":"code","34f55cc2":"code","507e4b3b":"code","daa14766":"code","d27b0940":"code","3ea9104c":"code","9ecdb48e":"code","8224d55c":"code","a9d2312b":"code","ecf01555":"code","1f431aa7":"code","d11c2c21":"code","aabb848f":"code","9e96473c":"code","059dd760":"code","2dde48bc":"code","2604f376":"code","de9e493a":"code","a2ce889b":"code","f1913aeb":"code","ab369ba4":"code","43e10c94":"code","ce734f95":"code","a2be101f":"code","634268b8":"code","bd6c7a36":"code","b17cdae0":"code","3e128a1e":"code","acdf5aa8":"code","be70c154":"code","e4bf17f8":"code","406a2688":"code","77757a9f":"code","1b6cafa6":"code","b3b6fc8f":"code","75126e53":"code","b578c386":"code","3c57f919":"code","df715241":"code","74bf8d16":"code","50465bba":"code","0973c9b6":"code","db0b7b2a":"code","d9188889":"code","2ed69df8":"code","2a085c5a":"code","99302e19":"code","6c8e2114":"markdown","a587409c":"markdown","0dd3e869":"markdown","74a55788":"markdown","3d7f89e7":"markdown","64a390d6":"markdown","f69c2103":"markdown","659d04e7":"markdown"},"source":{"5912bf0c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if 'csv' in filename: \n            print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec9b1bc8":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","34f55cc2":"crbase = pd.read_csv('\/kaggle\/input\/credit-card-data\/CardBase.csv')\ncsbase = pd.read_csv('\/kaggle\/input\/credit-card-data\/CustomerBase.csv')\nfrbase = pd.read_csv('\/kaggle\/input\/credit-card-data\/FraudBase.csv')\ntrbase = pd.read_csv('\/kaggle\/input\/credit-card-data\/TransactionBase.csv')\n","507e4b3b":"crbase.head(), crbase.info()","daa14766":"# Pri key Card_number\ncrbase.nunique()","d27b0940":"## card type distribution\npx.histogram(data_frame=crbase, x='Card_Family')","3ea9104c":"## Plotting credit limit distribution to check majority\/minority bins.\n## Conclusion: Credit limits up to 200k construct majority, less than 50k is highest. Further check which bin is targeted for frauds.\nimport plotly.graph_objects as go\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=crbase['Credit_Limit'],\n    marker_color='green',\n    opacity=0.75\n))\nfig.update_layout(\n    title_text='Credit Distribution', # title of plot\n    xaxis_title_text='Credit_Limit', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1 # gap between bars of the same location coordinates\n)\nfig.show()","9ecdb48e":"csbase.head(), csbase.info()","8224d55c":"csbase.nunique()","a9d2312b":"## Plotting age distribution to check if particular age group is majority\/minority.\n## Conclusion: No particular age group in majority\/minority.\nimport plotly.graph_objects as go\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=csbase['Age'],\n    marker_color='green',\n    opacity=0.75\n))\nfig.update_layout(\n    title_text='Age Distribution', # title of plot\n    xaxis_title_text='Age', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1 # gap between bars of the same location coordinates\n)\nfig.show()","ecf01555":"trbase.head(), trbase.info()","1f431aa7":"trbase.nunique() ## Primary key Transaction_ID","d11c2c21":"# Converting strings to pd.DateTime format.\ntrbase['Transaction_Date'] = pd.to_datetime(trbase['Transaction_Date'], format='%d-%b-%y')","aabb848f":"trbase['Transaction_Date'].nunique() ## 366 unique dates, Data for complete 2016.\n\n## plotting transaction counts on\n## daily basis\ndf = trbase.sort_values(by='Transaction_Date')#.value_counts()\ndf.index = df['Transaction_Date']\ndf['Transaction_Count'] = pd.DataFrame(np.ones(10000,dtype=int), index=df.index)\ndf = df.groupby(by=df.index).sum()\npx.line(data_frame=df,x=df.index, y='Transaction_Count', title='Daily Transactions')\n## no conclusive pattern","9e96473c":"## monthly basis\ndf_m = df.resample('M').sum()\npx.line(data_frame=df_m,x=df_m.index, y='Transaction_Count', title='Monthly Transactions')\n## no conclusive pattern other than transaction decline at end of financial year","059dd760":"frbase.head(), frbase.info()","2dde48bc":"frbase.nunique()","2604f376":"## left merge || merging transaction base and fraud base \n## conclusion: Most frauds happening in sep months, but overalll evenly distributed across year\ntrfr_l = pd.merge(trbase, frbase, how='left', on='Transaction_ID')\ntrfr_l['Fraud_Flag'] = trfr_l['Fraud_Flag'].fillna(0)\ndf = trfr_l.sort_values(by='Transaction_Date')\ndf_g = df.groupby(by='Transaction_Date').sum()\ndf_m = df_g.resample('M').sum()\npx.line(data_frame=df_m, x=df_m.index, y='Fraud_Flag')\n","de9e493a":"df = trfr_l.sort_values(by='Transaction_Date')\npx.histogram(data_frame=df, x='Transaction_Segment',color='Fraud_Flag', title='Frauds per Segment')\n#sns.histplot(df, x='Transaction_Segment', hue='Fraud_Flag')","a2ce889b":"# right merge\ntrfr_r = pd.merge(trbase, frbase, how='right', on='Transaction_ID')\ntrfr_r\ndf = trfr_r.sort_values(by='Transaction_Date')\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=trfr_r['Transaction_Segment'],\n    marker_color='green',\n    opacity=0.75\n))\nfig.update_layout(\n    title_text='Transaction_Segment', # title of plot\n    xaxis_title_text='Segment_type', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1 # gap between bars of the same location coordinates\n)\nfig.show()","f1913aeb":"fig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=trfr_r['Transaction_Value'],\n    marker_color='red',\n    opacity=0.75\n))\nfig.update_layout(\n    title_text='Fraud Transaction Value', # title of plot\n    xaxis_title_text='Value', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1 # gap between bars of the same location coordinates\n)\nfig.show()\n","ab369ba4":"# checking if customers of credit case and sustomer base are same or not\n# conclusion: crbase customers are subset of csbase\ndf = pd.concat([crbase['Cust_ID'], csbase['Cust_ID']])\nprint(len(crbase['Cust_ID']), len(csbase['Cust_ID']), len(df))\ndf.drop_duplicates(keep='first')","43e10c94":"print(crbase.columns) #500 records, 482 unique customer\nprint(csbase.columns) #5674 records, all unique\nprint(trfr_l.columns)","ce734f95":"crcs = pd.merge(crbase, csbase, how='inner', on='Cust_ID')\n# Renaming Primary key column(Card_Number)\ncrcs.columns = ['Credit_Card_ID', 'Card_Family', 'Credit_Limit', 'Cust_ID', 'Age', 'Customer_Segment', 'Customer_Vintage_Group']\ncrcs","a2be101f":"# final merge to get all transactional info\ndf_final = pd.merge(trfr_l, crcs, how='left', on='Credit_Card_ID')\ndf_final = df_final.sort_values(by='Transaction_Date')\ndf_final","634268b8":"## plotting age groups vulnerable to frauds\n## conclusion: age group of 20-40 is more vulnerable to frauds\ndf = df_final.loc[df_final['Fraud_Flag'] == 1.0].groupby(by='Cust_ID').mean()\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=df['Age'],\n    marker_color='yellow',\n    opacity=0.75,\n    nbinsx=2\n))\nfig.update_layout(\n    title_text='Age Distribution of customers who were victim', # title of plot\n    xaxis_title_text='Age bin', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1 # gap between bars of the same location coordinates\n)\nfig.show()","bd6c7a36":"df_final.loc[df_final['Fraud_Flag'] == 1.0].drop_duplicates(subset='Cust_ID')","b17cdae0":"## customer segments affected by frauds\n## conclusion: Diamond segment customers are more prone to frauds compared to gold and platinum\ndf = df_final.loc[df_final['Fraud_Flag'] == 1.0].drop_duplicates(subset='Cust_ID')\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=df['Customer_Segment'],\n    marker_color='yellow',\n    opacity=0.75\n))\nfig.update_layout(\n    title_text='customers segments who were victim', # title of plot\n    xaxis_title_text='Segments', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1 # gap between bars of the same location coordinates\n)\nfig.show()","3e128a1e":"## customer vintage group affected by frauds\n## conclusion: Customer vintage grp and customer segments are highly correlated, thus should be dropped.\ndf = df_final.loc[df_final['Fraud_Flag'] == 1.0].drop_duplicates(subset='Cust_ID')\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=df['Customer_Vintage_Group'],\n    marker_color='yellow',\n    opacity=0.75\n))\nfig.update_layout(\n    title_text='customer vintage group who were victim', # title of plot\n    xaxis_title_text='Vintage Group', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1 # gap between bars of the same location coordinates\n)\nfig.show()","acdf5aa8":"## customer segments affected by frauds\n## conclusion: \ndf = df_final.loc[df_final['Fraud_Flag'] == 1.0].drop_duplicates(subset='Cust_ID')\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=df['Card_Family'],\n    marker_color='yellow',\n    opacity=0.75\n))\nfig.update_layout(\n    title_text='Card Family of victims', # title of plot\n    xaxis_title_text='Card Family', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1 # gap between bars of the same location coordinates\n)\nfig.show()","be70c154":"from fastai import *\n# from fastai.tabular import *\n# from fastai.vision import *\n# from fastai.data.all import *\n# from fastai.tabular.core import *\nfrom fastai.tabular.all import *\nfrom sklearn.model_selection import train_test_split\n","e4bf17f8":"print(df_final.select_dtypes(include=[int, float]).columns)\nprint(df_final.select_dtypes(exclude=[int, float]).columns)\nlen(df_final.columns)","406a2688":"# attempt 1: \ndf_final['Transaction_Date'] = df_final['Transaction_Date'].apply(lambda x: str(x.month))\ndf_final.drop(columns=['Customer_Vintage_Group','Transaction_ID'], inplace=True)\n# X = df_final.drop(columns=['Fraud_Flag'])\n# y = df_final['Fraud_Flag']\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n# train = pd.concat([X_train, y_train], axis=1)\n# test = pd.concat([X_test, y_test], axis=1)","77757a9f":"dep_var = 'Fraud_Flag'\ncont_cols = ['Transaction_Value', 'Credit_Limit', 'Age']\ncat_cols = ['Transaction_Date', 'Credit_Card_ID','Transaction_Segment', 'Card_Family',\n            'Cust_ID', 'Customer_Segment']\n\n# Applying Transforms\nsplits = RandomSplitter(valid_pct=0.3, seed=42)(range_of(df_final))\nto_master = TabularPandas(df_final, procs=[Categorify, Normalize],\n                   cat_names=cat_cols, cont_names=cont_cols,\n                   y_names=dep_var,\n                   splits=splits)","1b6cafa6":"X_train, y_train = to_master.train.xs, to_master.train.ys.values.ravel()\nX_test, y_test = to_master.valid.xs, to_master.valid.ys.values.ravel()\nCounter(y_train), Counter(y_test), len(df_final)","b3b6fc8f":"## 'to_master' can decode transformed values back to their original, thus when databunch will be created after smote,\n# we can use decoder to  transform prediction back to original form\nto_master.items.iloc[2400]\nto_master.decode_row(X_test.iloc[2400])","75126e53":"## SMOTE\nfrom imblearn.over_sampling import SMOTE\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using SMOTE\nsmt = SMOTE()\n#X_train, y_train = smt.fit_resample(X_train, y_train)\nX_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_sm)\nprint('After',counter)\n\ntrain_sm = pd.concat([X_train_sm,pd.DataFrame({'Fraud_Flag': y_train_sm}, index=X_train_sm.index)], axis=1)\ndls_sm = TabularDataLoaders.from_df(train_sm, path='.',y_names=dep_var, y_block=CategoryBlock)","b578c386":"learn = tabular_learner(dls_sm, metrics=accuracy)\nprint(f'Accuracy metrics on train set')\nlearn.fit_one_cycle(1)","3c57f919":"# Prediction on Test set\nfrom sklearn.metrics import accuracy_score\ndl = learn.dls.test_dl(X_test)\npreds = learn.get_preds(dl=dl, with_decoded=True)\nprint(f'Accuracy with SMOTE on testset: {round(accuracy_score(y_test, preds[2]),3)}')","df715241":"## SMOTE ADASYN\nfrom imblearn.over_sampling import ADASYN\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using ADASYN\nada = ADASYN(random_state=130)\nX_train_ada, y_train_ada = ada.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_ada)\nprint('After',counter)\n\ntrain_ada = pd.concat([X_train_ada,pd.DataFrame({'Fraud_Flag': y_train_ada}, index=X_train_ada.index)], axis=1)\ndls_ada = TabularDataLoaders.from_df(train_ada, path='.',y_names=dep_var, y_block=CategoryBlock)","74bf8d16":"learn = tabular_learner(dls_ada, metrics=accuracy)\nprint(f'Accuracy metrics on train set')\nlearn.fit_one_cycle(1)","50465bba":"# Prediction on Test set\nfrom sklearn.metrics import accuracy_score\ndl = learn.dls.test_dl(X_test)\npreds = learn.get_preds(dl=dl, with_decoded=True)\nprint(f'Accuracy with ADASYN SMOTE on testset: {round(accuracy_score(y_test, preds[2]),3)}')","0973c9b6":"## SMOTE + TomekLinks\nfrom imblearn.combine import SMOTETomek\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using SMOTE + Tomek\nsmtom = SMOTETomek(random_state=139)\nX_train_smtom, y_train_smtom = smtom.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_smtom)\nprint('After',counter)\n\n\ntrain_smtom = pd.concat([X_train_smtom,pd.DataFrame({'Fraud_Flag': y_train_smtom}, index=X_train_smtom.index)], axis=1)\ndls_smtom = TabularDataLoaders.from_df(train_smtom, path='.',y_names=dep_var, y_block=CategoryBlock)","db0b7b2a":"learn = tabular_learner(dls_smtom, metrics=accuracy)\nprint(f'Accuracy metrics on train set')\nlearn.fit_one_cycle(1)","d9188889":"# Prediction on Test set\nfrom sklearn.metrics import accuracy_score\ndl = learn.dls.test_dl(X_test)\npreds = learn.get_preds(dl=dl, with_decoded=True)\nprint(f'Accuracy with SMOTE + TomekLinks on testset: {round(accuracy_score(y_test, preds[2]),3)}')","2ed69df8":"## SMOTE + ENN\nfrom imblearn.combine import SMOTEENN\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using SMOTE + ENN\nsmenn = SMOTEENN()\nX_train_smenn, y_train_smenn = smenn.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_smenn)\nprint('After',counter)\n\ntrain_smenn = pd.concat([X_train_smenn,pd.DataFrame({'Fraud_Flag': y_train_smenn}, index=X_train_smenn.index)], axis=1)\ndls_smenn = TabularDataLoaders.from_df(train_smenn, path='.',y_names=dep_var, y_block=CategoryBlock)","2a085c5a":"learn = tabular_learner(dls_smenn, metrics=accuracy)\nprint(f'Accuracy metrics on train set')\nlearn.fit_one_cycle(1)","99302e19":"# Prediction on Test set\nfrom sklearn.metrics import accuracy_score\ndl = learn.dls.test_dl(X_test)\npreds = learn.get_preds(dl=dl, with_decoded=True)\nprint(f'Accuracy with SMOTE + ENN on testset: {round(accuracy_score(y_test, preds[2]),3)}')","6c8e2114":"### * Due to time constraints other classification techniques such RF and XGBoost were not implemented.","a587409c":"# Summary","0dd3e869":"# Table Merge","74a55788":"# Modelling","3d7f89e7":"![image.png](attachment:4e3ac469-a7f0-4ae8-b9c9-181dda361123.png)","64a390d6":"# EDA","f69c2103":"* This NB contains detailed analysis on credit card frauds. EDA section contains various insights on different data sheets. Code cells have experiments\/analysis performed and insight gathered out of that particular experiment.\n* 'Table Merge' section has few insights gathered after merge. Final merge is done at end of this section to get master table.\n* Modelling section foucuses on preparing suitable data for model ingestion.\n    * FastAI library has been used to generate classification model, which can be used to detect fraudulent transactions.\n    * Different sampling techniques present in imblearn library are used to acheive optimal results.\n    * Best model acheives train accuracy of 85% and test accuracy of 84%.\n\n\n","659d04e7":"## Going all guns blazing SMOTE"}}