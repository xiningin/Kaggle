{"cell_type":{"4202676c":"code","cd083d57":"code","ed73cc2a":"code","aa83c298":"code","943bd736":"code","028298ba":"code","f233f94f":"code","886003ac":"code","18d521fb":"code","d33c1b74":"code","8dfe4a4b":"code","ca844489":"code","f80adc3b":"code","74727111":"code","9e6ca470":"code","93c91c88":"markdown","cc4792ae":"markdown","4e01fedd":"markdown","b2e9cf93":"markdown","b1f3cb8b":"markdown","db1277a0":"markdown","daf6973a":"markdown","6382ec94":"markdown","864f72e3":"markdown","cd718713":"markdown","fbf96180":"markdown","9e4a33c2":"markdown","cfbe24b0":"markdown"},"source":{"4202676c":"import numpy as np\nimport pandas as pd\nimport re","cd083d57":"train_1 = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\ntrain_1","ed73cc2a":"train_1_cleaned = pd.concat(\n    [pd.DataFrame({\"text\":train_1.less_toxic.unique(), \"score\":np.zeros(train_1.less_toxic.nunique())}),\n     pd.DataFrame({\"text\":train_1.more_toxic.unique(), \"score\":np.ones(train_1.more_toxic.nunique())})],\n    axis=0).reset_index(drop=True)\ntrain_1_cleaned","aa83c298":"train_2 = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\ntrain_2","943bd736":"train_2_cleaned = pd.DataFrame({'text':train_2.comment_text, 'score':train_2.target})\ntrain_2_cleaned['score'] = train_2_cleaned['score'].apply(lambda x: 0 if x <= 0.05 else x)\ntrain_2_cleaned","028298ba":"train_3 = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\")\ntrain_3","f233f94f":"score = np.mean(train_3[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']], axis=1)\ntrain_3_cleaned = pd.DataFrame({'text':train_3.comment_text, 'score':score})\ntrain_3_cleaned['score'] = train_3_cleaned['score'].apply(lambda x: 0 if x <= 0.05 else x)\ntrain_3_cleaned","886003ac":"train_df = pd.concat(\n    [train_1_cleaned, train_2_cleaned, train_3_cleaned],\n    axis=0\n).reset_index(drop=True)\ntrain_df","18d521fb":"printed = []\nfor i in sorted(train_df.score.unique()):\n    n = np.round(i, 2) \n    if n in printed:\n        continue\n    printed.append(n)\n    print(f\"{len(printed):<3}: {i:.5f}\\t{repr(np.random.choice(train_df[train_df.score==i]['text']))[:100]}\")","d33c1b74":"from tqdm.notebook import tqdm\ntqdm.pandas()\nimport re\n\nadd_space_before_punc = lambda x: re.sub(r'(\\W|_)', r' \\1 ', x)\nremove_whitespaces = lambda x: re.sub(r'\\s+', ' ', x)\nremove_multiples = lambda x: re.sub(r'(.)\\1{2,}', r'\\1\\1', x) #Remove repeated char multiple times\n\ntrain_df['clean_text'] = train_df.text.progress_apply(\n    lambda x: remove_whitespaces(remove_multiples(add_space_before_punc(x)))\n)","8dfe4a4b":"import json\nimport gc\n\nneutral_words = {}\ntoxic_words = {}\n\ndef get_words(all_words, toxic=False):\n    keys = set(neutral_words.keys())\n    print(\"Looping through all words...\")\n    for word in tqdm(all_words):\n        if toxic and not (word in keys):\n            toxic_words[word] = toxic_words.get(word, 0) + 1\n        elif not toxic:\n            neutral_words[word] = neutral_words.get(word, 0) + 1\n            \n    with open(f\"{'neutral' if not toxic else 'toxic'}_word_counts.json\", 'w') as f:\n        if toxic:\n            json.dump(toxic_words, f)\n        else:\n            json.dump(neutral_words, f)\n\n#Separate neutral from toxic\nprint(\"Separating words...\")\nneutral_df = (' '.join(train_df[train_df.score == 0]['clean_text'].values)).split()\ntoxic_df = (' '.join(train_df[train_df.score != 0]['clean_text'].values)).split()\nprint(\"Neutral words:\", len(neutral_df))\nprint(\"Toxic words:\", len(toxic_df))\n\nprint(\"Creating word count mapping...\")\nget_words(neutral_df)\nget_words(toxic_df, True)\nprint(len(neutral_words), len(toxic_words))\ndel neutral_df, toxic_df\ngc.collect()","ca844489":"neutral_words = {word: 0.1\/count for word,count in tqdm(neutral_words.items())}\ntoxic_words = {word: 256 * 0.995 ** count for word,count in tqdm(toxic_words.items())}\nprint(f\"Neutral min\/max: {min(neutral_words.values()):.5f}, {max(neutral_words.values()):.5f}\")\nprint(f\"Toxic min\/max: {min(toxic_words.values()):.5f}, {max(toxic_words.values()):.5f}\")","f80adc3b":"with open(\"neutral_word_scores.json\", 'w') as f:\n    json.dump(neutral_words, f)\nwith open(\"toxic_word_scores.json\", 'w') as f:\n    json.dump(toxic_words, f)","74727111":"def score_sentence(text):\n    all_words = text.split()\n    return sum(neutral_words[word] if neutral_words.get(word, 0) else\n              toxic_words[word] for word in all_words)\n\ntrain_df['text_score'] = train_df.clean_text.progress_apply(lambda x: score_sentence(x))\ntrain_df['text_score'].describe()","9e6ca470":"from scipy.stats import rankdata\n\ntest = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ntest['clean_text'] = test.text.progress_apply(\n    lambda x: remove_whitespaces(remove_multiples(add_space_before_punc(x)))\n)\n\npreds = test.clean_text.progress_apply(lambda x: score_sentence(x))\ndisplay(preds, preds.min(), preds.max())\nsub = pd.DataFrame({'comment_id':test.comment_id.values, 'score':rankdata(preds, method='ordinal')})\nsub.to_csv('submission.csv', index=False)\nsub","93c91c88":"We will be using multiple dataset from previous Jigsaw competitions as complementary train data.","cc4792ae":"Let's check one random comment for each unique score (based from a rounded cutoff or 2 decimal places) printing the first 100 raw chars of the corresponding text.","4e01fedd":"# Word Scoring\n\nWe will be scoring each word used based on frequency. Words used on non-toxic comments will be treated with low scores and words exclusively used on offensive\/toxic comments will be scored high.","b2e9cf93":"For the data from `jigsaw-toxic-comment-classification-challenge`, we will be scoring them based on the average of the five columns present as targets. Similar to `train_2` we will be using base 50% once more.","b1f3cb8b":"# Data Preprocessing","db1277a0":"# Testing","daf6973a":"For the data from `jigsaw-unintended-bias-in-toxicity-classification` we will only be using the `comment_text` and the `target` columns.","6382ec94":"Now that we have the map of word and word counts, we will be scoring them. For neutral words, the more frequent words will have lower scores and rarer words will have higher scores. We will use the equation `word_score = 0.1 \/ word_count` to actualize this metric.\n\nFor toxic words, the less the frequency of the word, the higher the score. More frequent words will have lower score. We will use the equation of `word_score = 256 * 0.995 ** (word_count)` where `256` is the arbitrary limit of the toxicity score.","864f72e3":"For the main train data, we will simple associate the column `less_toxic` with the score of `0` and the column `more_toxic` with the score of `1`. There are some text that are duplicates so we will filter out the uniques for each level of toxicity.","cd718713":"# Submission\n\nBest Score on Public LB is: `0.716`","fbf96180":"# Begin","9e4a33c2":"The prediction function is at [this notebook](https:\/\/www.kaggle.com\/seraphwedd18\/jigsaw-toxic-severity-word-scoring-prediction) with a max LB score of `0.749` as of creation of this notebook.","cfbe24b0":"Now, we will merge all train data as one."}}