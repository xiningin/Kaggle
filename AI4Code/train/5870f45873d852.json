{"cell_type":{"6afcb4a1":"code","5110e438":"code","3b98e4e6":"code","0cada9fc":"code","91524c7c":"code","dde30b3e":"code","780239e7":"code","16d1af56":"code","3ad18c75":"code","e7eda82d":"code","26c23a7f":"code","24c7ea6f":"code","ace7a442":"code","f7e1a884":"code","05a5964d":"code","f3df091e":"code","d0766f0d":"code","aec98496":"code","bc8c199a":"code","dabc2a6d":"code","52af97fa":"code","78cbab49":"code","4848fbd1":"code","9b6ecf83":"code","ff94c03f":"code","83f32bcb":"code","c2398e25":"code","4be234dd":"markdown","d4765131":"markdown","aedd194d":"markdown","1bae1601":"markdown","8e96e120":"markdown","7931ba09":"markdown","5c4609e0":"markdown","a0fcdde7":"markdown","41b6c677":"markdown","c4228588":"markdown","98f45a40":"markdown","33901e9c":"markdown","66e1e39d":"markdown","9230dc89":"markdown","d3d559a3":"markdown"},"source":{"6afcb4a1":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport shutil\nimport os","5110e438":"metadata=pd.read_csv(\"..\/input\/coronahack-chest-xraydataset\/Chest_xray_Corona_Metadata.csv\")\nmetadata.head()","3b98e4e6":"fig, ax = plt.subplots(1, 2,figsize=(20, 5))\nax[0].hist(metadata['Label']);\nax[1].hist(metadata['Label_1_Virus_category'].astype(str));","0cada9fc":"#get training data and testing data separatly from the metadata csv file\ntrain_df = metadata[metadata['Dataset_type'] == 'TRAIN']\ntest_df = metadata[metadata['Dataset_type'] == 'TEST']","91524c7c":"#Divide each virus with corresponding images to different variables\ntrain_virus = train_df[train_df.Label_1_Virus_category == 'Virus']['X_ray_image_name']\ntrain_bacterial=train_df[train_df.Label_1_Virus_category == 'bacteria']['X_ray_image_name']\ntrain_normal=train_df[train_df.Label == 'Normal']['X_ray_image_name']\n\nlen(train_virus),len(train_bacterial),len(train_normal)","dde30b3e":"def split_to_training_validation(data,split=0.2):\n    \"\"\"\n    This function takes a data series and split 20% of it to validation set and 80% to training set\n    \n    Args:\n    data -> data series images\n    split -> parameter to split\n    \n    returns a validation and training set\n    \"\"\"\n    \n    valid_data=data[:round(split*len(data))]\n    train_data=data[round(split*len(data)):]\n    \n    return valid_data, train_data","780239e7":"valid_virus,train_virus=split_to_training_validation(train_virus)\nvalid_bacterial,train_bacterial=split_to_training_validation(train_bacterial)\nvalid_normal,train_normal=split_to_training_validation(train_normal)\n\nlen(train_virus),len(valid_virus), len(valid_normal),len(train_normal)","16d1af56":"lables=['Healthy','Viral-pneumonia','Bacterial-pneumonia']\ntraining_data_classes=[train_normal,train_virus,train_bacterial]\nsource='..\/input\/coronahack-chest-xraydataset\/Coronahack-Chest-XRay-Dataset\/Coronahack-Chest-XRay-Dataset\/train'\n\nfor i in range(0,len(lables)):\n    target='\/dataset\/train\/'+lables[i] #choose where the data from kaggle should be placed\n    \n    os.makedirs('\/dataset\/train\/'+lables[i]) #create new folder with lables\n    move=training_data_classes[i]\n    for j in move:\n        #move everything from source path to new target path as iterating through the labels\n        path=os.path.join(source,j)\n        shutil.copy(path,target)","3ad18c75":"validation_data_classes=[valid_normal,valid_virus,valid_bacterial]\nfor i in range(0,len(lables)):\n    target='\/dataset\/valid\/'+lables[i] #choose where the data from kaggle should be placed\n    \n    os.makedirs('\/dataset\/valid\/'+lables[i]) #create new folder with lables\n    move=validation_data_classes[i]\n    for j in move:\n        #move everything from source path to new target path as iterating through the labels\n        path=os.path.join(source,j)\n        shutil.copy(path,target)","e7eda82d":"test_virus = test_df[test_df.Label_1_Virus_category == 'Virus']['X_ray_image_name']\ntest_bacterial=test_df[test_df.Label_1_Virus_category == 'bacteria']['X_ray_image_name']\ntest_normal=test_df[test_df.Label == 'Normal']['X_ray_image_name']\n\nlen(test_virus),len(test_bacterial),len(test_normal)","26c23a7f":"classes=[test_normal,test_virus,test_bacterial]\nsource='..\/input\/coronahack-chest-xraydataset\/Coronahack-Chest-XRay-Dataset\/Coronahack-Chest-XRay-Dataset\/test'\n\nfor i in range(0,len(lables)):\n    \n    target='\/dataset\/test\/'+lables[i] #choose where the data from kaggle should be placed \n    \n    os.makedirs('\/dataset\/test\/'+lables[i]) #create new folder with lables\n    move=classes[i]\n    for j in move:\n        #move everything from source path to new target path as iterating through the labels\n        path=os.path.join(source,j)\n        shutil.copy(path,target)","24c7ea6f":"import pathlib\n\n#Print out classes from the created directory\ndata_dir = pathlib.Path(\"\/dataset\/valid\")\nclass_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")])) # Created a list of class_names from the subdirectories\nprint(class_names)","ace7a442":"import matplotlib.image as mpimg\nfrom matplotlib.pyplot import figure\nimport random\n\ndef view_random_image(target_dir, target_class):\n    # Setup the target directory \n    target_folder = target_dir+target_class\n\n    # Get a random image path\n    random_image = random.sample(os.listdir(target_folder), 1)\n    print(random_image)\n    \n    # Read in the image and plot it using matplotlib\n    plt.figure(figsize=(7, 5))\n    plt.subplot(1, 1,1)\n    \n    img = mpimg.imread(target_folder + \"\/\" + random_image[0])\n    plt.imshow(img,cmap='gray')\n    plt.title(target_class)\n    plt.axis(\"off\");\n    print(f\"Image shape: {img.shape}\") # show the shape of the image\n\n    return img","f7e1a884":"#dir and class can be changed\nimage_1= view_random_image(target_dir=\"\/dataset\/valid\/\",\n                        target_class=\"Bacterial-pneumonia\")","05a5964d":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 64\n\ntf.random.set_seed(42)\n\n#Define training and testing directories\ntrain_dir = \"\/dataset\/train\"\nvalid_dir=\"\/dataset\/valid\"\ntest_dir = \"\/dataset\/test\"\n\n#Normalize images\ntrain_aug = ImageDataGenerator(rescale=1\/255.,\n                               shear_range=0.1,\n                               rotation_range=20,\n                               zoom_range=0.1)\n\nvalid_gen=ImageDataGenerator(rescale=1\/255.)\ntest_gen = ImageDataGenerator(rescale=1\/255.)\n\n\ntrain_data = train_aug.flow_from_directory(train_dir,\n                                          target_size=IMG_SIZE,\n                                          color_mode='grayscale',\n                                          batch_size=BATCH_SIZE,\n                                          class_mode=\"categorical\")\n\nvalid_data=valid_gen.flow_from_directory(valid_dir,\n                                        target_size=IMG_SIZE,\n                                        color_mode='grayscale',\n                                        batch_size=BATCH_SIZE,\n                                        class_mode=\"categorical\")\n\ntest_data = test_gen.flow_from_directory(test_dir,\n                                        target_size=IMG_SIZE,\n                                        color_mode='grayscale',\n                                        batch_size=BATCH_SIZE,\n                                        class_mode=\"categorical\")","f3df091e":"#Plot three images of augmented training data\nfor _ in range(3):\n    img, label = train_data.next()\n    plt.figure(figsize=(7, 7))\n    plt.imshow(img[0],cmap=\"gray\")\n    plt.show()","d0766f0d":"import datetime\ndef create_tensorboard_callback(dir_name, experiment_name):\n\n    #store log files with filepath to tensorboard\n    log_dir = dir_name + \"\/\" + experiment_name + \"\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n      log_dir=log_dir\n    )\n    print(f\"Saving TensorBoard log files to: {log_dir}\")\n    return tensorboard_callback","aec98496":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Activation,BatchNormalization, Dropout\n\n#Create model, increase filter and decrease kernel as going deeper since pixels are bigger than 128x128. This is the AlexNet neural netowrk\n#architecture\n\nmodel=Sequential([\n    Conv2D(96, 11, 4, activation='relu', input_shape=(224,224,1)),\n    BatchNormalization(),\n    MaxPool2D(3, 2),\n    Conv2D(256, 5, 1, activation='relu', padding=\"same\"),\n    BatchNormalization(),\n    MaxPool2D(3, 2),\n    Conv2D(384, 3, 1, activation='relu', padding=\"same\"),\n    BatchNormalization(),\n    Conv2D(384,3, 1, activation='relu', padding=\"same\"),\n    BatchNormalization(),\n    Conv2D(256, 3, 1, activation='relu', padding=\"same\"),\n    BatchNormalization(),\n    MaxPool2D(3, 2),\n    Flatten(),\n    Dense(4096, activation='relu'),\n    \n    #set dropout to regularize\n    Dropout(0.5),\n    Dense(4096, activation='relu'),\n    Dropout(0.5),\n    Dense(3, activation='softmax')\n])\n\n# Compile\nmodel.compile(loss=\"categorical_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,decay=1e-5),\n                metrics=[\"AUC\"])","bc8c199a":"# Set checkpoint path\ncheckpoint_path = \"weights\/checkpoint.ckpt\"\n\n# Create a ModelCheckpoint callback that saves the model's weights only\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                         save_best_only=True,\n                                                         save_freq=\"epoch\", # save every epoch\n                                                         verbose=1)","dabc2a6d":"# Fit the model saving checkpoints every epoch\nepochs = 50\n\n#Train the model with 150 epochs\nhistory = model.fit(train_data,\n                          epochs=epochs,\n                          steps_per_epoch=train_data.samples\/\/BATCH_SIZE,\n                          validation_data=valid_data,\n                          validation_steps=len(valid_data),\n                          callbacks=[create_tensorboard_callback(dir_name=\"history_callback\",\n                                                                                 experiment_name=\"Chest_Xray\"),\n                                                     checkpoint_callback])\n\n","52af97fa":"model.evaluate(test_data)","78cbab49":"#Load best weights from the saved model on checkpoint\nmodel_best_weights= tf.keras.models.load_model('weights\/checkpoint.ckpt')","4848fbd1":"model_best_weights.evaluate(test_data)","9b6ecf83":"y_pred = model_best_weights.predict(test_data)\ny_pred.shape","ff94c03f":"y_pred","83f32bcb":"# Create a function to load and prepare images for prediction\ndef load_and_prep_image(filename, img_shape=224, scale=True):\n\n  # Read in the image\n    img = tf.io.read_file(filename)\n\n    # Decode image into tensor\n    img = tf.io.decode_image(img, channels=1)\n\n    # Resize the image\n    img = tf.image.resize(img, [img_shape, img_shape])\n\n    # Scale? Yes\/no\n    if scale:\n    # rescale the image (get all values between 0 and 1)\n        return img\/255.\n    else:\n        return img ","c2398e25":"# Make preds on a series of random images\nimport os\nimport random\n\nplt.figure(figsize=(17, 10))\n\n#get for random images from testdataset and use model to predict infection\nfor i in range(4):\n    \n  # Choose random image(s) from random class(es)\n    class_name = random.choice(class_names)\n    filename = random.choice(os.listdir(test_dir + \"\/\" + class_name))\n    filepath = test_dir + \"\/\"+ class_name + \"\/\" + filename\n\n    # Load the image and make predictions\n    img = load_and_prep_image(filepath)\n    \n    img_expanded = tf.expand_dims(img, axis=0)\n    print(img_expanded.shape)\n    pred_prob = model_best_weights.predict(img_expanded) # get prediction probabilities array\n    pred_class = class_names[pred_prob.argmax()] # get highest prediction probability index and match it class_names list\n    #slice out last dimension\n    img = img[:,:,0]\n    print(pred_prob)\n    plt.subplot(2, 2,i+1)\n    # Plot the images\n    print(filename)\n    plt.imshow(img,cmap='gray')\n    if (class_name == pred_class): # if predicted class matches truth class, make text green\n        title_color = \"g\"\n    else:\n        title_color = \"r\"\n    plt.title(f\"Actual class: {class_name}, Pred class: {pred_class}, Pred prob: {pred_prob.max():.2f}%\", c=title_color)\n    plt.axis(False);","4be234dd":"### The model with best weights achieved an accuracy of roughly 82%","d4765131":"**Evaluating the model as it currently is versus the best fitted model during training**","aedd194d":"**Create sequential deep learning model**\n\n* Get prediction probabilites later on using softmax activation function in dense layer\n* Use categorical crossentropy as loss metric","1bae1601":"# Prediction and visualization\n\n* Predictions are made with the model fitted with best weights. Perhaps better result could be achieved by training longer","8e96e120":"**Plot one random image of bacterial-pneumonia lung (it can be changed to others by changing the target_class arg)**","7931ba09":"# General information about this notebook\n\nThis notebook devides this dataset to three following categories: Healthy, Bacterial-pneumonia and Viral-pneumonia. If a chest xray is classified as the two latters, the person is thus infected with covid-19. The reason i didnt combine bacterial and viral pneumonia in one category, namely covid, is that there are different features in images which shows if someone has a bacterial or viral infection. Combining this two in one category would thus mess up the training data and accuracy. \n\nAnother important note is that this dataset has different amount of data in respective category which affects the accuracy result. For example we have 2000+ xray images of bacterial-pneumonia and only roughly 1400 images of healthy lungs. ","5c4609e0":"**Split the data to test,valid and training**","a0fcdde7":"# Examining the dataset","41b6c677":"**Predict randomly and plot**\n\nThis section takes 4 images out of testing data and classifies them using the model that was trained\n\nThe actual and predicted categories plus their prediction probability is also plotted","c4228588":"# Preprocessing data for the neural network","98f45a40":"* **4228** images for training\n* **1056** for validation\n* **624** for testing purpose","33901e9c":"* Validation is used when fitting the model\n* This ensures that hyperparameter tuning isnt chosen based on the unseen test data","66e1e39d":"**Get class names to confirm the division is done right**","9230dc89":"# Divide the data to three categories as mentioned in beginning","d3d559a3":"# Discussion\n\n* The above cell can be used as a further method of checking how the model performs\n\n* If the dataset contained more image data of healthy lungs then the reslut wouldve been better\n\n* The missclassified image has a probability of 58% which shows that even the model is not confident enough of its own prediction, which is self explanatory that it needs more image data of healthy lungs\n\n* The reason i didnt use transfer learning on this dataset is that it will only learn patterns in pneumonia tensors since they are alot more examples of this type in this dataset than healthy lungs. The RUC gives an accuracy of 92% using transfer learning, but when you visualize your result it will only be able to predict with a fixed probability every single time, thus i only used the network architechtur of AlexNet to train the network and it now has learned the dataset with AUC of 82% and is also able to predict with less \"false positives\" than transfer learning\n\n* **Future work:** Apply VGG16 architecture on this dataset and compare with AlexNet without using transfer learning"}}