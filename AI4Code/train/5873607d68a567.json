{"cell_type":{"49e66e35":"code","0bb75276":"code","3354d555":"code","d390a190":"code","8675a065":"code","9b9ebfd8":"code","5dfbf9f5":"code","7314856c":"code","fd24323b":"code","574b3f97":"code","fb95ab83":"code","a6e74201":"code","cf7bb4a8":"code","47e8318c":"code","97c1aac4":"code","1be8df1c":"code","6add056e":"code","2691d785":"code","fea54021":"code","3b52092a":"code","1ffa3ff0":"code","6a7b674a":"code","9c5b2034":"code","fa96bccb":"code","b90421ba":"code","f3241916":"code","9202a4a4":"code","0a203f17":"markdown","f9916d7e":"markdown","70bde090":"markdown","2c4f34f2":"markdown","e6ae92bd":"markdown","8333ca55":"markdown","ecdd84c6":"markdown"},"source":{"49e66e35":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0bb75276":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn import feature_selection\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","3354d555":"files = ['\/kaggle\/input\/titanic\/train.csv', '\/kaggle\/input\/titanic\/test.csv']\ntrain_df, test_df = [pd.read_csv(i) for i in files]\ndf_list = [train_df, test_df]\ntrain_df.head()","d390a190":"train_df.info()\nprint('-' * 40)\ntest_df.info()","8675a065":"train_df[['Survived', 'Age', 'Fare']].describe()","9b9ebfd8":"train_df.describe(include = ['O'])","5dfbf9f5":"def p_survive(check_list, dataframe):\n    '''quickly group the variable to check the survived rate for each categorical variable in the check_list'''\n    \n    for name in check_list:\n        print('{} Survived Possibility: '.format(name))\n        print(dataframe[[name, 'Survived']].groupby(\n            name, as_index = True).mean().sort_values(\n            by = 'Survived', ascending = False).reset_index())\n        print('-' * 10)\n\ncheck_list = ['Pclass', 'Sex', 'Embarked', 'SibSp', 'Parch']\np_survive(check_list, train_df)","7314856c":"# Sex & Age\ng = sns.FacetGrid(train_df, hue = 'Survived', col = 'Sex', height = 3, aspect = 2)\ng.map(plt.hist, 'Age', alpha = .5, bins = 20)\ng.add_legend()\nplt.show()","fd24323b":"# Sex & Fare\ng = sns.FacetGrid(train_df, hue = 'Survived', col = 'Sex', height = 4, aspect = 1.5)\ng.map(plt.hist, 'Fare', alpha = .5)\ng.add_legend()\nplt.show()","574b3f97":"# Sex & Pclass & Embarked\nFacetGrid = sns.FacetGrid(train_df, row='Embarked', height = 3 , aspect = 2)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette = 'Blues')\nFacetGrid.add_legend()\nplt.show()","fb95ab83":"# Age & Pclass \ng = sns.FacetGrid(train_df, hue = 'Survived', row = 'Pclass', height = 3, aspect = 2.5)\ng.map(plt.hist, 'Age', alpha = .5, bins = 20)\ng.add_legend()\nplt.show()","a6e74201":"df_list = [df.drop(['Ticket', 'Cabin'], axis = 1) for df in df_list]\n\ndef manipulate_df(df):\n    '''part 1: filling all missing values\n       part 2: creating features\n       part 3: setting segments for continues variables'''\n    \n    df.Age.fillna(df.Age.median(), inplace = True)\n    df.Fare.fillna(df.Fare.median(), inplace = True)\n    df.Embarked.fillna(df.Embarked.mode()[0], inplace = True)\n    \n    df['FamilySize'] = df.SibSp + df.Parch + 1\n    df['Alone'] = [1 if i == 1 else 0 for i in df.FamilySize]\n    df['Title'] = df.Name.str.extract('([A-Za-z]+)\\.', expand = False)\n    df['FarePP'] = (df.Fare\/df.FamilySize).astype(int)\n    \n    df['FareRange'] = pd.qcut(df.Fare, 4, labels = [1, 2, 3, 4])\n    df['FarePPRange'] = pd.qcut(df.FarePP, 4, labels = [1, 2, 3, 4])\n    df['AgeRange'] = pd.cut(df.Age.astype(int), 5, labels = [1, 2, 3, 4, 5])\n    \n    return df","cf7bb4a8":"df_list[0] = manipulate_df(df_list[0])\n\n# checking dataset & null values\ndisplay(df_list[0].head(3))\nprint('Titles in the dataset\\n', df_list[0].Title.unique())\nprint('-' * 30)\nprint(df_list[0].isnull().sum())","47e8318c":"# creating title groups \nother_titles = ['Jonkheer', 'Col', 'Capt', 'Countess', 'Major', 'Dr', 'Master', 'Rev']\nfemale_titles1 = ['Miss', 'Mlle', 'Ms']\nfemale_titles2 = ['Mme', 'Lady', 'Mrs', 'Dona']\nmale_titles = ['Mr', 'Don', 'Sir']\n\ndef title_replace(df):\n    '''grouping titles for Machine Learning preprocessing'''\n    \n    df['Title'] = df.Title.replace(other_titles, 'Others')\n    df['Title'] = df.Title.replace(female_titles1, 'Miss')\n    df['Title'] = df.Title.replace(female_titles2, 'Mrs')\n    df['Title'] = df.Title.replace(male_titles, 'Mr')\n    \n    return df","97c1aac4":"df_list[0] = title_replace(df_list[0])\n\n# checking survived rate by grouping new features\ncheck_list2 = ['Title', 'FamilySize', 'Alone', 'AgeRange', 'FareRange', 'FarePPRange']\np_survive(check_list2, df_list[0])","1be8df1c":"# visualizing suivived rate by grouping FamilySize\nsns.factorplot('FamilySize','Survived', data = df_list[0], height = 3.5, aspect = 3.5)\nplt.show()","6add056e":"# preprocessing dataset\nle = LabelEncoder()\ndef label_encoder(cat_list, coded_list, dataframe):\n    '''step 1: encoding all labels in the cat_list in the dataframe \n       step 2: appending new labels into a new list named coded_list\n       step 3: return new dataframe and coded_list'''\n    \n    for col in cat_list:\n        dataframe['{}_le'.format(col)] = le.fit_transform(dataframe['{}'.format(col)])\n        coded_list.append('{}_le'.format(col))\n    return dataframe, coded_list","2691d785":"# combining all the features (original & created)\ncat_list = check_list + check_list2\n\ncoded_list = []\nlabel_encoder(cat_list, coded_list, df_list[0])\ndf_list[0][cat_list].head(3)","fea54021":"# checking the labels for all the features \nX = df_list[0][coded_list]\ny = df_list[0].Survived\nprint('Categorical Columns:\\n', cat_list)\nprint('-' * 40)\nprint('Encoded Categorical Columns:\\n', X.columns.values)","3b52092a":"# spliting data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n\n# creating a model list to get a better model with highest score\nmodels = []\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('KNearest Neighbors', KNeighborsClassifier(n_neighbors = 4)))\nmodels.append(('SGD Classfier', SGDClassifier()))\nmodels.append(('Gaussian NB', GaussianNB()))\nmodels.append(('Linear SVC', LinearSVC()))\nmodels.append(('Random Forest', RandomForestClassifier(max_depth = 5, n_estimators = 200, oob_score = True)))\nmodels.append(('AdaBoost Classifier', AdaBoostClassifier(DecisionTreeClassifier(max_depth = 5), \n                                              n_estimators = 200, algorithm = \"SAMME.R\")))\nmodels.append(('Bagging Classifier', BaggingClassifier(DecisionTreeClassifier(max_depth = 5), \n                                            n_estimators = 200, bootstrap = True)))\n\nlabels = []\nresults = []\nstd = []\n\nfor label, model in models:\n    kfold = KFold(n_splits = 10, shuffle = True)\n    cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = 'accuracy')\n    labels.append(label)\n    results.append(round(cv_results.mean() * 100, 2))\n    std.append(round(cv_results.std(), 4))\n\n# creating a sorted dataframe to show the highest avg.score model\ndf_result = pd.DataFrame({'Model': labels, 'Avg_score': results, 'Avg_std': std})\ndf_result.sort_values(by = 'Avg_score', ascending = False)","1ffa3ff0":"# tuning the model by excluding the unimportance features from the highest avg.score model \nrf = RandomForestClassifier(max_depth = 5, n_estimators = 200)\nrf.fit(X_train, y_train)\n\ndf_vips = pd.DataFrame({'Feature': cat_list, \n                       'Importance': rf.feature_importances_})\n\nplt.figure(figsize = (10, 5))\nsns.barplot(x = 'Importance', y = 'Feature', \n            data = df_vips.sort_values(by = 'Importance', ascending = False), palette = 'Blues_d')\nplt.title('Importance Score', fontsize = 14)\nplt.show()","6a7b674a":"# dropping two features with lowest importance score \nX_train_better, X_test_better = [X.drop(['Alone_le', 'Parch_le'], axis = 1) for X in [X_train, X_test]]\n\n# refitting the model \nrf = RandomForestClassifier(max_depth = 5, n_estimators = 200, oob_score = True)\nrf.fit(X_train_better, y_train)\ny_pred = rf.predict(X_test_better)\n\nprint('Better Random Forest Training Set Score: {:.2f}%'.format(rf.score(X_train_better, y_train) * 100))\nprint('Better Random Forest Predicting Set Score: {:.2f}%'.format(rf.score(X_test_better, y_pred) * 100))","9c5b2034":"# hyperparameters tuning to get the best model params\nhp_params = {'max_depth': range(3, 8, 1),\n             'n_estimators': range(150, 400, 50),\n             'criterion': ['gini', 'entropy']}\n\nrf_sg = GridSearchCV(estimator = RandomForestClassifier(oob_score = True), param_grid = hp_params, cv = 5)\nrf_sg.fit(X_train_better, y_train)\ny_pred_best = rf_sg.best_estimator_.predict(X_test_better)\n\nprint('Best Params: {}\\nTraining Set Score: {:.2f}%\\nPredicting Score on: {:.2f}%'.format(\n    rf_sg.best_params_, rf_sg.best_score_*100, rf_sg.best_estimator_.score(X_test_better, y_pred_best)*100))","fa96bccb":"df_list[1].head()","b90421ba":"# manipulating test dataset\ndf_list[1] = manipulate_df(df_list[1])\ndf_list[1] = title_replace(df_list[1])\n\n# checking the details of the new dataset\ndisplay(df_list[1].head())\nprint('Titles in the dataset\\n', df_list[1].Title.unique())\nprint('-' * 30)\nprint(df_list[1].isnull().sum())","f3241916":"# preprocessing the new dataset\ncoded_list2 = []\nlabel_encoder(cat_list, coded_list2, df_list[1])\n\nX2 = df_list[1][coded_list2].drop(['Alone_le', 'Parch_le'], axis = 1)\nprint('Encoded Categorical Columns:\\n', X2.columns.values)","9202a4a4":"# final result\ny2_pred = rf_sg.best_estimator_.predict(X2)\ntest_df['Survived'] = y2_pred\nsubmission = test_df[['PassengerId', 'Survived']]\n\nsubmission.to_csv(\"Titanic_submit.csv\", index = False)","0a203f17":"### Data Manipulation","f9916d7e":"### Categorical Variables Overview","70bde090":"### Description Overview","2c4f34f2":"### Machine Learning","e6ae92bd":"### Summary\n- Sex is highly correlated with survived rate while Embarked is lowerly correlated with survived rate\n- Other variables are correlated with survived rate (next will visualize the data for further information)","8333ca55":"### Visualization Overview","ecdd84c6":"#### Summary\n- There are a few missing values in column Age, Cabin and Fare\n- Survivided rate is only 38.38% in train_df\n- No Name duplicate, Ticket and Cabin have low freq "}}