{"cell_type":{"f0e6da7b":"code","c2589e05":"code","df8427f5":"code","d7f41cbd":"code","6d687fd1":"code","7e5ef3d0":"code","714094ac":"code","c3d4c150":"code","4ac0ee3e":"code","7faa1715":"code","aa44ddcf":"code","985cf576":"code","c0b11617":"code","97ed0ef9":"code","74c76ca2":"code","c816dc64":"code","21b6188c":"code","d40628a7":"code","381df844":"code","04ef41ba":"code","2be4e738":"code","d5851e2a":"code","ebc914ff":"code","41426144":"code","4d5f2314":"code","02284d23":"code","1b9b2844":"code","4e8691f2":"code","28f6d5aa":"code","ad126c5c":"code","2e2de02f":"code","3941a6f1":"code","62f692f6":"markdown","f430aafc":"markdown","e18bcff0":"markdown","44497155":"markdown","71e68a86":"markdown","f56ab2b6":"markdown","8ced2510":"markdown","bb9ce673":"markdown","c0b7bbac":"markdown","f38b01e8":"markdown","34715f59":"markdown","99de2a06":"markdown","a525962b":"markdown","d8ea029d":"markdown"},"source":{"f0e6da7b":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nimport seaborn as sns\nimport collections\n\nfrom tqdm import tqdm, tqdm_notebook\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score)\nfrom sklearn.metrics import accuracy_score, precision_score\n                            \nimport torch.nn as nn\nfrom torch.autograd import Variable as V\nimport torch.nn.functional as F\nimport torch\nfrom torch.utils.data import DataLoader\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom bokeh.plotting import figure, output_notebook, show, ColumnDataSource\nfrom bokeh.models import HoverTool, NumeralTickFormatter\nfrom bokeh.palettes import Set3_12\nfrom bokeh.transform import jitter\n\nimport gc\ngc.enable()","c2589e05":"# Training epochs\nepochs=10","df8427f5":"train_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)","d7f41cbd":"train.head()","6d687fd1":"# Drop columns\ndef dropper(column_name, train, test):\n    train = train.drop(column_name, axis=1)\n    test = test.drop(column_name, axis=1)\n    return train, test\n\ndel_columns = ['TransactionDT']\nfor col in del_columns:\n    train, test = dropper(col, train, test)\n\ndef scaler(scl, column_name, data):\n    data[column_name] = scl.fit_transform(data[column_name].values.reshape(-1,1))\n    return data\n\nscl_columns = ['TransactionAmt', 'card1', 'card3', 'card5', 'addr1', 'addr2']\nfor col in scl_columns:\n    train = scaler(StandardScaler(), col, train)\n    test = scaler(StandardScaler(), col, test)","7e5ef3d0":"train.head()","714094ac":"\"\"\"\n#TODO: Learning should be done by using non fraud data\ntrain = train[train['isFraud'] == 0]\ntrain_fraud = train[train['isFraud'] == 1].copy()\n\"\"\"","c3d4c150":"y_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n# Drop target\nX_train = train.drop('isFraud', axis=1)\n#X_train_fraud = train_fraud.drop('isFraud', axis=1)\nX_test = test.copy()\n\ndel train, test\n    \n# TODO: change methods\n# Fill in NaNs\nX_train = X_train.fillna(-999)\n#X_train_fraud = X_train_fraud.fillna(-999)\nX_test = X_test.fillna(-999)\n\n# TODO: change to Label Count Endocing\n# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values)) #+ list(X_train_fraud[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        #X_train_fraud[f] = lbl.transform(list(X_train_fraud[f].values)) \n        X_test[f] = lbl.transform(list(X_test[f].values)) \n        \ngc.collect()","4ac0ee3e":"print(X_train.head())\n#print(X_train_fraud.head())\nprint(X_test.head())","7faa1715":"\"\"\"\n    params:\n        data : data desired to be split\n        ratio : validation ratio for split\n        \n    output:\n        train_data, validation_data\n\"\"\"\n\ndef splitter(data, ratio=0.2):\n    num = int(ratio*len(data))\n    return data[num:], data[:num]\n\nX_train, X_val = splitter(X_train)\ny_train, y_val = splitter(y_train)\n\n# Check number of data\nprint(len(X_train), len(X_val), len(y_train), len(y_val))","aa44ddcf":"xtr = torch.FloatTensor(X_train.values)\nxts = torch.FloatTensor(X_test.values)\n# X_val: validation data for isFraud == 0\nxvl = torch.FloatTensor(X_val.values) \n# X_train_fraud: validation data for isFraud == 1\n#xvt = torch.FloatTensor(X_train_fraud.values)\n\nxdl = DataLoader(xtr,batch_size=1000)\ntdl = DataLoader(xts,batch_size=1000)\nvdl = DataLoader(xvl,batch_size=1000)\n#fdl = DataLoader(xvt,batch_size=1000)\n\nprint(len(X_train.values), len(X_test.values), len(X_val.values)) #, len(X_train_fraud))\ngc.collect()","985cf576":"class AutoEncoder(nn.Module):\n    def __init__(self, length):\n        super().__init__()\n        self.lin1 = nn.Linear(length,20)\n        self.lin2 = nn.Linear(20,10)\n        self.lin7 = nn.Linear(10,20)\n        self.lin8 = nn.Linear(20,length)\n        \n        self.drop2 = nn.Dropout(0.05)\n        \n        self.lin1.weight.data.uniform_(-2,2)\n        self.lin2.weight.data.uniform_(-2,2)\n        self.lin7.weight.data.uniform_(-2,2)\n        self.lin8.weight.data.uniform_(-2,2)\n\n    def forward(self, data):\n        x = F.tanh(self.lin1(data))\n        x = self.drop2(F.tanh(self.lin2(x)))\n        x = F.tanh(self.lin7(x))\n        x = self.lin8(x)\n        return x\n    \ndef score(x):\n    y_pred = model(V(x))\n    x1 = V(x)\n    return loss(y_pred,x1).item()","c0b11617":"model = AutoEncoder(len(X_train.columns))\nloss=nn.MSELoss()\nlearning_rate = 1e-2\noptimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\nprint(model)\n\n# Utilize a named tuple to keep track of scores at each epoch\nmodel_hist = collections.namedtuple('Model','epoch loss val_loss')\nmodel_loss = model_hist(epoch = [], loss = [], val_loss = [])","97ed0ef9":"def train(epochs, model, model_loss):\n    try: c = model_loss.epoch[-1]\n    except: c = 0\n    for epoch in tqdm_notebook(range(epochs),position=0, total = epochs):\n        losses=[]\n        dl = iter(xdl)\n        for t in range(len(dl)):\n            # Forward pass: compute predicted y and loss by passing x to the model.\n            xt = next(dl)\n            y_pred = model(V(xt))\n            \n            l = loss(y_pred,V(xt))\n            losses.append(l)\n            optimizer.zero_grad()\n\n            # Backward pass: compute gradient of the loss with respect to model parameters\n            l.backward()\n\n            # Calling the step function on an Optimizer makes an update to its parameters\n            optimizer.step()\n            \n        val_dl = iter(tdl)\n        val_scores = [score(next(val_dl)) for i in range(len(val_dl))]\n        \n        model_loss.epoch.append(c+epoch)\n        model_loss.loss.append(l.item())\n        model_loss.val_loss.append(np.mean(val_scores))\n        print(f'Epoch: {epoch}   Loss: {l.item():.4f}    Val_Loss: {np.mean(val_scores):.4f}')\n\ntrain(model=model, epochs=epochs, model_loss=model_loss)","74c76ca2":"x = np.linspace(0, epochs-1, epochs)\nprint(model_loss.loss)\nprint(model_loss.val_loss)\nprint(x)\nplt.plot(x, model_loss.loss, label=\"loss\")\nplt.legend()\nplt.show()\n\nplt.plot(x, model_loss.val_loss, label=\"val_loss\")\nplt.legend()\nplt.show()\n\ngc.collect()","c816dc64":"# Iterate through the dataloader and get predictions for each batch of the test set.\np = iter(vdl)\npreds = np.vstack([model(V(next(p))).cpu().data.numpy() for i in range(len(p))])\n\n# Create a pandas DF that shows the Autoencoder MSE vs True Labels\nerror_nonfraud = np.mean(np.power((X_val-preds),2), axis=1)\n\"\"\"\np = iter(fdl)\npreds = np.vstack([model(V(next(p))).cpu().data.numpy() for i in range(len(p))])\nerror_fraud = np.mean(np.power((X_train_fraud-preds),2), axis=1)\n\npd.DataFrame(error_fraud)\n\"\"\"\nerror_df = pd.DataFrame(data = {'error':error_nonfraud,'true':y_val})\n\nerror_df.groupby('true')['error'].describe().reset_index()","21b6188c":"fpr, tpr, thresholds = roc_curve(error_df.true, error_df.error)\nroc_auc = auc(fpr, tpr)\n\nplt.plot(fpr, tpr, label='ROC curve (area = {})'.format(roc_auc))\nplt.legend()\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)","d40628a7":"temp_df = error_df[error_df['true'] == 0]\nthreshold = temp_df['error'].mean() + temp_df['error'].std()\nprint(f'Threshold: {threshold:.3f}')","381df844":"y_pred = [1 if e > threshold else 0 for e in error_df.error.values]\nprint(classification_report(error_df.true.values,y_pred))","04ef41ba":"conf_matrix = confusion_matrix(error_df.true, y_pred)\n\nsns.set(font_scale = 1.2)\nplt.figure(figsize=(10, 10))\nsns.heatmap(conf_matrix, xticklabels=['Not Fraud','Fraud'], yticklabels=['Not Fraud','Fraud'], annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","2be4e738":"plt.figure(figsize=(12, 12))\nm = []\nthreshold_min = threshold * 0.99\nthreshold_max = threshold * 1.01\n\nfor thresh in np.linspace(threshold_min, threshold_max):\n    y_pred = [1 if e > thresh else 0 for e in error_df.error.values]\n    conf_matrix = confusion_matrix(error_df.true, y_pred)\n    m.append((conf_matrix,thresh))\n    \ncount = 0\nfor i in range(3):\n    for j in range(3):\n        plt.subplot2grid((3, 3), (i, j))\n        sns.heatmap(m[count][0], xticklabels=['Not Fraud','Fraud'], yticklabels=['Not Fraud','Fraud'], annot=True, fmt=\"d\");\n        plt.title(f\"Threshold - {m[count][1]:.3f}\")\n        plt.ylabel('True class')\n        plt.xlabel('Predicted class')\n        plt.tight_layout()\n        count += 1\nplt.show()","d5851e2a":"# Iterate through the dataloader and get predictions for each batch of the test set.\np = iter(tdl)\npreds = np.vstack([model(V(next(p))).cpu().data.numpy() for i in range(len(p))])\n\n# Create a pandas DF that shows the Autoencoder MSE vs True Labels\nerror = np.mean(np.power((X_test-preds),2), axis=1)","ebc914ff":"def min_max_normalization(x):\n    x_min = x.min()\n    x_max = x.max()\n    x_norm = (x-x_min) \/ (x_max-x_min)\n    return x_norm\n\n# min max normalization\n#error_df = pd.DataFrame(data={'isFraud':min_max_normalization(error)})\nerror_df = pd.DataFrame(data={'isFraud':error})\n\nprint(\"Num data: \" + str(len(error_df)))\nprint(\"Beyond threshold num data: \" + str(len(error_df[error_df['isFraud'] > threshold])))\n#error_df[error_df['isFraud'] > threshold]\n\nx_min = 3600000\nx_max = 4200000\nplt.hlines(threshold, x_min, x_max, \"black\")\nplt.plot(error_df, alpha=0.3)\nplt.show()","41426144":"error_df = pd.DataFrame(data={'isFraud':min_max_normalization(error)})\nerror_df.head()","4d5f2314":"sample_submission['isFraud'] = error_df\nsample_submission.to_csv('simple_autoencoder.csv')","02284d23":"# Making day\/hour features. See the kernel below:\n# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100400#latest-579480\n\"\"\"\ndef make_day_feature(df, offset=0, tname='TransactionDT'):\n    # found a good offset is 0.58\n    days = df[tname] \/ (3600*24)        \n    encoded_days = np.floor(days-1+offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    hours = df[tname] \/ (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours\n\n# check train\nprint(make_day_feature(train, offset=0.58).value_counts().head(10))\nprint(make_hour_feature(train).value_counts().head(10))\n\ntrain['TransactionDay'] = make_day_feature(train, offset=0.58)\ntrain['TransactionHour'] = make_hour_feature(train)\ntest['TransactionDay'] = make_day_feature(test, offset=0.58)\ntest['TransactionHour'] = make_hour_feature(test)\n\n\"\"\"","1b9b2844":"\"\"\"\n# Delete columns\ndrop_true = False\nif(drop_true):\n    drop_col = ['V300', 'V309', 'V111', 'C3', 'V124', 'V106', 'V125', 'V315', 'V134', 'V102', 'V123', 'V316', 'V113', 'V136', 'V305', 'V110', 'V299', 'V289', 'V286', 'V318', 'V103', 'V304', 'V116', 'V298', 'V284', 'V293', 'V137', 'V295', 'V301', 'V104', 'V311', 'V115', 'V109', 'V119', 'V321', 'V114', 'V133', 'V122', 'V319', 'V105', 'V112', 'V118', 'V117', 'V121', 'V108', 'V135', 'V320', 'V303', 'V297', 'V120']\n    X_train.drop(drop_col,axis=1, inplace=True)\n    X_test.drop(drop_col, axis=1, inplace=True)\n    X_train.head()\n    \n\"\"\"","4e8691f2":"\"\"\"\ncorrelation_matrix = X_train.corr()\nfig = plt.figure(figsize=(12,9))\nsns.heatmap(correlation_matrix,vmax=0.8,square = True)\n\nplt.show()\n\n\"\"\"","28f6d5aa":"\"\"\"\nfrom sklearn.cluster import KMeans\nKMeans(n_clusters=2).fit(X_train)\n\nclf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    missing=-999,\n    tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n)\n\"\"\"","ad126c5c":"\"\"\"\ndef get_avg_auc_kfold(X_train, y_train, X_test, clf, NFOLDS=2, shuffle=True):\n    kf = KFold(n_splits=NFOLDS, shuffle=shuffle)\n    y_preds = np.zeros(X_test.shape[0])\n    y_oof = np.zeros(X_train.shape[0])\n    score = 0\n  \n    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train, y_train)):    \n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        clf.fit(X_tr, y_tr)\n        y_pred_train = clf.predict_proba(X_vl)[:,1]\n        y_oof[val_idx] = y_pred_train\n        print(\"FOLD: \",fold,' AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n        score += roc_auc_score(y_vl, y_pred_train) \/ NFOLDS\n        y_preds+= clf.predict_proba(X_test)[:,1] \/ NFOLDS\n    \n        del X_tr, X_vl, y_tr, y_vl\n        gc.collect()\n    print (\">> Avg AUC: \", score)\n    return score\n\nexecuteKFold = False\nif(executeKFold):\n    print(get_avg_auc_kfold(X_train, y_train, X_test, clf))\n    \n\"\"\"","2e2de02f":"\"\"\"\n# Training\n%time clf.fit(X_train, y_train)\n\n# Get xgBoost importances\nimportance_dict = {}\nfor import_type in ['weight', 'gain', 'cover']:\n    importance_dict['xgBoost-'+import_type] = clf.get_booster().get_score(importance_type=import_type)\n    \n# MinMax scale all importances\nimportance_df = pd.DataFrame(importance_dict).fillna(0)\nimportance_df = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(importance_df),\n    columns=importance_df.columns,\n    index=importance_df.index\n)\n\n# Create mean column\nimportance_df['mean'] = importance_df.mean(axis=1)\n\n# Plot the feature importances\nimportance_df.sort_values('mean').head(50).plot(kind='bar', figsize=(20, 7))\n\n\"\"\"","3941a6f1":"\"\"\"\nsample_submission['isFraud'] = clf.predict_proba(X_test)[:,1]\nsample_submission.to_csv('simple_xgboost.csv')\n\"\"\"","62f692f6":"## KFold and validation","f430aafc":"### ROC_AUC","e18bcff0":"# To be continued\n\n# Feature Engineering\n## Day\/Hour features from TransactionDT","44497155":"## Validation","71e68a86":"# Training\n\nDAYS OF RESEARCH BROUGHT ME TO THE CONCLUSION THAT I SHOULD SIMPLY SPECIFY `tree_method='gpu_hist'` IN ORDER TO ACTIVATE GPU (okay jk, took me an hour to figure out, but I wish XGBoost documentation was more clear about that).","f56ab2b6":"# Preprocessing","8ced2510":"## Drop columns and Standard Scaling","bb9ce673":"# Train\n## training and visualizing all importances of futures","c0b7bbac":"# AutoEncoder\n## Preprocessing","f38b01e8":"### Precision Recall F1-Score","34715f59":"### Plot Precision Recall","99de2a06":"# Anomaly Detection with AutoEncoder (pytorch)\nHi! I'm new to kaggle, and this is my first competition in my life. \nIf you notice the better way about my implementation please tell me! :)\n\nIn past fraud detection competition, some people used auto encoder approach to detect anomalous for fraud data.\nThere are categorical variables in the case, however, I thought the same approach can be applied to the task.\nSo I just tried it.\n\nTODO:\n- Current implementation is incorrect. training should be done by using only non fraud data.\n\nhttps:\/\/www.kaggle.com\/jdoz22\/detecting-fraud-using-an-autoencoder-and-pytorch?scriptVersionId=2802227\nhttps:\/\/www.kaggle.com\/artgor\/eda-and-models\/notebook","a525962b":"## Check Loss\/Validation Loss","d8ea029d":"### Plot Precision Recall for each thresholds\nChanging the threshold from threshold_min to threshold_max."}}