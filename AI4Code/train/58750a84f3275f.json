{"cell_type":{"a5aa3964":"code","f4f2e34d":"code","3eddd947":"code","25b39c0f":"code","0ab4b65c":"code","6c801225":"code","d7912d40":"code","6e6fee87":"code","1ad8710e":"code","5d82c241":"code","9159197e":"code","f5016019":"code","b6b0e23a":"code","822f835b":"code","c914b800":"markdown","82eb788d":"markdown","d1307278":"markdown","6081c0e8":"markdown","40740eaf":"markdown","9a4ff048":"markdown","6b8d3b96":"markdown","5689a74d":"markdown","2fa3012a":"markdown","b5c1d966":"markdown","851c3245":"markdown","d3b8d79d":"markdown"},"source":{"a5aa3964":"!pip install -q mtranslate\n!pip install -q transformers","f4f2e34d":"import torch\nfrom mtranslate import translate\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm.notebook import tqdm","3eddd947":"class GoogleBackTranslator():\n    def __init__(self, n_diff:int=1):\n        # minimum number of differences\n        self.n_diff = n_diff\n\n    def __call__(self, sentence:str, languages:list):\n        # any languages from fa .... \n        for i, lang in enumerate(languages[:-1]):\n            sentence = translate(sentence, from_language=lang, to_language=languages[i+1])\n        # the last back to the first element of the languages list\n        back_translated = translate(sentence, from_language=languages[i+1], to_language=languages[0])\n        tokens = set(back_translated.split(' '))\n        if len(tokens.intersection(sentence.split(' '))) >= len(tokens)-self.n_diff:\n            return ''\n        return back_translated","25b39c0f":"bk = GoogleBackTranslator(n_diff=2)\nbk('\u0627\u0645\u0631\u0648\u0632 \u0686\u0646\u062f \u0634\u0646\u0628\u0633\u061f', ['fa', 'en'])","0ab4b65c":"bk = GoogleBackTranslator(n_diff=2)\nbk('\u0686\u062c\u0648\u0631\u06cc \u0645\u06cc\u0634\u0647 \u0627\u0632 \u0633\u0627\u06cc\u062a \u0634\u0645\u0627 \u062e\u0631\u06cc\u062f \u06a9\u0631\u062f\u061f', ['fa', 'ru'])","6c801225":"class SentenceSimilarity():\n    def __init__(self, max_len=16, model_name='m3hrdadfi\/bert-fa-base-uncased-wikitriplet-mean-tokens'):\n        self.model_name = model_name\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name).eval()\n        self.max_len = max_len\n\n    def __call__(self, text:list):\n        # tokenization step\n        tokens = self.tokenizer(text, truncation=True, padding='max_length', \n                                max_length=self.max_len, return_tensors='pt')\n\n        # model.forward step\n        with torch.no_grad():\n            embeddings = self.model(**tokens).last_hidden_state\n        # Create masked embeddings (just expend size)\n        mask = tokens['attention_mask'].unsqueeze(-1).expand(embeddings.shape).float()\n        # create sentence embedding (sum embs \/ sum mask)\n        sentence_embeddings = torch.sum(embeddings * mask, dim=1) \/ torch.clamp(mask.sum(1), min=1e-9) \n        # expand dim for each embedding (helpful for cosine similarity)\n        return sentence_embeddings.unsqueeze(1)\n\n    def cosine_similarity(self, a:str, b:str):\n        a, b = self([a, b])\n        return torch.cosine_similarity(a, b).item()","d7912d40":"ss = SentenceSimilarity(max_len=32)","6e6fee87":"ss.cosine_similarity(a='\u0628\u0631\u0627\u06cc \u062a\u0631\u06a9 \u06a9\u0627\u0645\u0644 \u0633\u06cc\u06af\u0627\u0631 \u0686\u0647 \u0628\u0627\u06cc\u062f \u06a9\u0631\u062f\u061f', b='\u0628\u0631\u0627\u06cc \u062a\u0631\u06a9 \u06a9\u0627\u0645\u0644 \u0633\u06cc\u06af\u0627\u0631 \u0686\u0647 \u06a9\u0627\u0631\u06cc \u0628\u0627\u06cc\u062f \u0627\u0646\u062c\u0627\u0645 \u062f\u0647\u06cc\u062f\u061f')","1ad8710e":"ss.cosine_similarity(a='\u0628\u0631\u0627\u06cc \u062a\u0631\u06a9 \u06a9\u0627\u0645\u0644 \u0648\u0631\u0632\u0634 \u0686\u0647 \u0628\u0627\u06cc\u062f \u06a9\u0631\u062f\u061f', b='\u0628\u0631\u0627\u06cc \u062a\u0631\u06a9 \u06a9\u0627\u0645\u0644 \u0633\u06cc\u06af\u0627\u0631 \u0686\u0647 \u06a9\u0627\u0631\u06cc \u0628\u0627\u06cc\u062f \u0627\u0646\u062c\u0627\u0645 \u062f\u0647\u06cc\u062f\u061f')","5d82c241":"class FilteredBackTranslation():\n      # TODO: Parrallel BackTranslator\n    def __init__(self, min_score:float=.8, n_diff:int=1, similar_model_name:str='m3hrdadfi\/bert-fa-base-uncased-wikitriplet-mean-tokens'):\n        self.min_score = min_score\n        self.back_translator = GoogleBackTranslator(n_diff=n_diff)\n        self.sentence_similar = SentenceSimilarity(model_name=similar_model_name)\n        # I found that the following languages work well for Persian back translation\n        self.languages = [['fa', 'en'], ['fa', 'ru'], ['fa', 'ar'], ['fa', 'fr']]\n\n    def __call__(self, sentences:list, top_chain:int=2):\n        # top chain: use all those 4 languages for creating backtranslation\n        augmented = []\n        for i, s in tqdm(enumerate(sentences), total=len(sentences)):\n            paraphrazes = []\n            scores = []\n            for langs in self.languages[:top_chain]:\n                aug = self.back_translator(s, languages=langs) \n                if aug not in paraphrazes:\n                    score = self.sentence_similar.cosine_similarity(s, aug)\n                    if score >= self.min_score:\n                        scores.append(score)\n                        paraphrazes.append(aug)\n\n        if len(scores)>0:\n            augmented.append({'id': i, 'org': s, 'aug': paraphrazes, 'score': scores})\n\n        return augmented","9159197e":"augmenter = FilteredBackTranslation(min_score=.9)","f5016019":"sentences = ['\u0628\u0631\u0627\u06cc \u062a\u0631\u06a9 \u06a9\u0627\u0645\u0644 \u0633\u06cc\u06af\u0627\u0631 \u0628\u0627\u06cc\u062f \u0686\u06cc \u06a9\u0627\u0631 \u06a9\u0631\u062f\u061f']\naugmenter(sentences, top_chain=4) ","b6b0e23a":"sentences = ['\u0686\u0647 \u062c\u0648\u0631\u06cc \u0645\u06cc\u062a\u0648\u0646\u0645 \u0648\u0632\u0646\u0645 \u0631\u0648 \u06a9\u0645 \u06a9\u0646\u0645\u061f']\naugmenter(sentences, top_chain=4)","822f835b":"sentences = ['\u0631\u0627\u0647 \u0647\u0627\u06cc \u062f\u0631\u0645\u0627\u0646 \u062e\u0648\u062f\u0634\u06cc\u0641\u062a\u06af\u06cc \u0631\u0627 \u0628\u06cc\u0627\u0646 \u06a9\u0646\u06cc\u062f\u061f']\naugmenter(sentences, top_chain=4)","c914b800":"- negative example","82eb788d":"- an example of good back translation","d1307278":"## All in one:\n\nWe can now use both components (back translation, similarity check) together\n- A reasonable back translation must have a similarity score of at least 80%","6081c0e8":"You can see similarities between some of the examples in this part\n\n- positive example","40740eaf":"in this example, the model couldn't find any good back translation","9a4ff048":"Using Similarity Check, we can reduce the number of these noises by comparing the embedding spaces of our Original sentence and our Augmented sentence\n- We now ignore Augmented Sentences with a low Similarity threshold ","6b8d3b96":"# Filtered Back Translation\nAuthor: Sajjad Ayoubi","5689a74d":"## Google Back Translator\n\nFor Persian, the best translation is Google Translate (according to my research. \n\nFor this notebook, we're going to use it for the translator.","2fa3012a":"There are times when naive back translation works, and times when it doesn't and just adds noise to your training data","b5c1d966":"- and the bad back translation","851c3245":"## Similarity Check\n\nin this part you can see similarity between some examples\n\nnote: Although this is not the most efficient method (you can do this on GPUs or using batch processing).","d3b8d79d":"#### **Give this notebook an upvote if you find it useful :)**"}}