{"cell_type":{"2d03df16":"code","5669b8a0":"code","5b070f3e":"code","0aacfef2":"code","4e65e4d0":"code","88e354d3":"code","d978f226":"code","1f419238":"code","53f91e5f":"code","00ce8a53":"code","8d61654b":"code","8564584c":"code","d598d050":"code","76a55a7d":"code","6dfd36bd":"code","03e64caa":"code","3fa55426":"code","594885e6":"code","82612e8a":"code","2156b271":"code","a9f4d155":"code","d1d81e99":"code","3998136e":"code","75bfc824":"code","c235e6a5":"code","77b9240f":"code","c51432f3":"code","d996c24e":"code","7e3bbc11":"code","ab980f43":"code","d873e27c":"code","00faf0dd":"code","48454285":"code","b4a6f0bd":"code","a16ed7be":"code","dc877ba3":"code","07937aa0":"code","9f65616b":"code","e71841b8":"code","562cf5d2":"code","ecad997e":"code","7bbd6d2e":"code","4f727789":"markdown","b5688c39":"markdown","d84ed105":"markdown","e6907881":"markdown","a81fd7f8":"markdown","7dc9e44b":"markdown","a20c0ed4":"markdown"},"source":{"2d03df16":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5669b8a0":"df=pd.read_csv(\"\/kaggle\/input\/stock-dataset\/Stock.csv\")\ndf.head()","5b070f3e":"df.describe()","0aacfef2":"df.info()","4e65e4d0":"df.isnull().sum()","88e354d3":"df.shape","d978f226":"df1=df.reset_index()[\"close\"]","1f419238":"df1","53f91e5f":"df1.shape","00ce8a53":"import matplotlib.pyplot as plt\nplt.plot(df1)","8d61654b":"#LSTM are sensitive to the scaled data, so we apply minmax scaler\nfrom sklearn.preprocessing import MinMaxScaler\n#to set the range of values between 0 and 1\nscaler=MinMaxScaler(feature_range=(0,1))\ndf1=scaler.fit_transform(np.array(df1).reshape(-1,1))","8564584c":"df1.shape","d598d050":"#splitting the dataset into training and test consecutively as it is an time series problem\ntraining_size=int(len(df1)*0.65)\ntest_size=len(df1)-training_size\ntrain_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]","76a55a7d":"training_size,test_size","6dfd36bd":"train_data","03e64caa":"test_data","3fa55426":"def create_dataset(dataset,time_step=1):\n    #Convert an array of values into a dataset matrix\n    dataX,dataY=[],[]\n    for i in range(len(dataset)-time_step-1):\n        a=dataset[i:(i+time_step),0]   #i=0 then values will be from \"i\" to \"n-1\" will be in \"x\" and \"n\" will be in \"y\".\n        dataX.append(a)\n        dataY.append(dataset[i+time_step,0])\n    return np.array(dataX) , np.array(dataY)","594885e6":"#reshape into X=t,t+1,t+2,t+3 and y=t+4\ntime_step=100\nX_train,y_train = create_dataset(train_data,time_step)\nX_test,y_test = create_dataset(test_data,time_step)","82612e8a":"X_train","2156b271":"X_train.shape, y_train.shape","a9f4d155":"X_test.shape , y_test.shape","d1d81e99":"#reshape the input to be [samples , time_step , features] which is required for LSTM\nX_train = X_train.reshape(X_train.shape[0] , X_train.shape[1] , 1)\nX_test = X_test.reshape(X_test.shape[0] , X_test.shape[1] , 1)","3998136e":"from tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import LSTM , Dense","75bfc824":"#It is a stack LSTM model which means one LSTM after another\nmodel=Sequential()\nmodel.add(LSTM(50,return_sequences=True,input_shape=(100,1)))  #input shape must be (X_train.shape[1],1)\nmodel.add(LSTM(50,return_sequences=True))\nmodel.add(LSTM(50))\nmodel.add(Dense(1))\nmodel.compile(loss=\"mean_squared_error\" , optimizer=\"adam\")","c235e6a5":"model.summary()","77b9240f":"model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=64,verbose=1)","c51432f3":"#Doing prediction and checking performance matrix\ntrain_predict=model.predict(X_train)\ntest_predict=model.predict(X_test)","d996c24e":"#Inverse transforming the values back to original\ntrain_predict=scaler.inverse_transform(train_predict)\ntest_predict=scaler.inverse_transform(test_predict)","7e3bbc11":"#Calculating RMS performance matrix\nimport math\nfrom sklearn.metrics import mean_squared_error\nmath.sqrt(mean_squared_error(y_train,train_predict))","ab980f43":"#RMS for test data\nmath.sqrt(mean_squared_error(y_test,test_predict))","d873e27c":"### Plotting \n# shift train predictions for plotting\nlook_back=100\ntrainPredictPlot = np.empty_like(df1)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(df1)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(df1))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","00faf0dd":"len(test_data)","48454285":"#because if we predict the output for next 30 days we will use the previous 100 days values for prediction so from 341 to 441 we have the data of previous 100 days\nx_input=test_data[341:].reshape(1,-1)\nx_input.shape","b4a6f0bd":"temp_input=list(x_input)\ntemp_input=temp_input[0].tolist()","a16ed7be":"temp_input","dc877ba3":"# demonstrate prediction for next 10 days\nfrom numpy import array\n\nlst_output=[]\nn_steps=100\ni=0\nwhile(i<30):\n    \n    if(len(temp_input)>100):\n        #print(temp_input)\n        #shifting one postion ahead\n        x_input=np.array(temp_input[1:])\n        print(\"{} day input {}\".format(i,x_input))\n        x_input=x_input.reshape(1,-1)\n        x_input = x_input.reshape((1, n_steps, 1))\n        #print(x_input)\n        yhat = model.predict(x_input, verbose=0)\n        print(\"{} day output {}\".format(i,yhat))\n        temp_input.extend(yhat[0].tolist())\n        temp_input=temp_input[1:]\n        #print(temp_input)\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    else:\n        #first cycle will start from here \n        x_input = x_input.reshape((1, n_steps,1))\n        yhat = model.predict(x_input, verbose=0)\n        print(yhat[0])\n        temp_input.extend(yhat[0].tolist())\n        print(len(temp_input))\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    \n\nprint(lst_output)","07937aa0":"#taking the previous 100 outputs\nday_new=np.arange(1,101)\n#taking the next 30 values as per predictions\nday_pred=np.arange(101,131)","9f65616b":"len(df1)","e71841b8":"#Plotting for the previous values in the dataset\nplt.plot(day_new,scaler.inverse_transform(df1[1158:]) , color=\"red\")\n#Plotting for the predicted values\nplt.plot(day_pred,scaler.inverse_transform(lst_output) , color=\"green\")","562cf5d2":"df3=df1.tolist()\ndf3.extend(lst_output)\nplt.plot(df3[1200:] )","ecad997e":"df3=scaler.inverse_transform(df3).tolist()","7bbd6d2e":"plt.plot(df3)","4f727789":"# Predicting the values for next 30 days","b5688c39":"# splitting the dataset into training and testing set","d84ed105":"# Importing dataset","e6907881":"# Model Creation","a81fd7f8":"# Data Preprocessing","7dc9e44b":"Here \"Blue\" line represents the whole dataset , \"Yellow\" line represents the training data and \"Green\" line represents the predicted values","a20c0ed4":"# Predicting the values"}}