{"cell_type":{"e0fa83fb":"code","2095bdd0":"code","c70592c7":"code","d33d99fc":"code","1cd02521":"code","ccb3da2a":"code","5c166185":"code","5496a895":"code","be9a728a":"code","0150ea8a":"code","1d486e6e":"code","ef7ddd6a":"code","a39d298c":"code","9c19fde7":"code","f00ed61a":"code","66c07759":"code","f2466754":"code","e1fb1a32":"code","c8daf8eb":"code","59529fc0":"code","3d06fc2d":"code","6fcfbcb9":"code","a477c4e5":"markdown","c0b04da1":"markdown","c01d3cd3":"markdown","9d718948":"markdown","7f9982b9":"markdown","ba05d075":"markdown","343f0a20":"markdown","6ce48f05":"markdown","e9cbafd5":"markdown","78d3401a":"markdown"},"source":{"e0fa83fb":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import skew\nfrom scipy.stats import uniform\n\nfrom xgboost import XGBRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import make_scorer \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ignore Deprecation Warning\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n# load the data\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf = df_train.append(df_test , ignore_index = True)\n\n# basic inspection\ndf_train.shape, df_test.shape, df_train.columns.values","2095bdd0":"df.SalePrice = np.log(df.SalePrice)","c70592c7":"# divide the data into numerical (\"quan\") and categorical (\"qual\") features\nquan = list( df_test.loc[:,df_test.dtypes != 'object'].drop('Id',axis=1).columns.values )\nqual = list( df_test.loc[:,df_test.dtypes == 'object'].columns.values )","d33d99fc":"# Find out how many missing values there are for the quantitative and categorical features\nhasNAN = df[quan].isnull().sum()\nhasNAN = hasNAN[hasNAN > 0]\nhasNAN = hasNAN.sort_values(ascending=False)\nprint(hasNAN)\nprint('**'*40)\nhasNAN = df[qual].isnull().sum()\nhasNAN = hasNAN[hasNAN > 0]\nhasNAN = hasNAN.sort_values(ascending=False)\nprint(hasNAN)","1cd02521":"# Filling missing values for numerical features. Most of the NAN should mean that \n# the corresponding facillity\/structure doesn't exist, so I use zero for most cases\n\ndf.LotFrontage.fillna(df.LotFrontage.median(), inplace=True)\n\n# NAN should mean no garage. I temporarily use yr = 0 here. Will come back to this later. \ndf.GarageYrBlt.fillna(0, inplace=True)\n\n# Use zero\ndf.MasVnrArea.fillna(0, inplace=True)    \ndf.BsmtHalfBath.fillna(0, inplace=True)\ndf.BsmtFullBath.fillna(0, inplace=True)\ndf.GarageArea.fillna(0, inplace=True)\ndf.GarageCars.fillna(0, inplace=True)    \ndf.TotalBsmtSF.fillna(0, inplace=True)   \ndf.BsmtUnfSF.fillna(0, inplace=True)     \ndf.BsmtFinSF2.fillna(0, inplace=True)    \ndf.BsmtFinSF1.fillna(0, inplace=True)    ","ccb3da2a":"# Filling missing values for categorical features\ndf.PoolQC.fillna('NA', inplace=True)\ndf.MiscFeature.fillna('NA', inplace=True)    \ndf.Alley.fillna('NA', inplace=True)          \ndf.Fence.fillna('NA', inplace=True)         \ndf.FireplaceQu.fillna('NA', inplace=True)    \ndf.GarageCond.fillna('NA', inplace=True)    \ndf.GarageQual.fillna('NA', inplace=True)     \ndf.GarageFinish.fillna('NA', inplace=True)   \ndf.GarageType.fillna('NA', inplace=True)     \ndf.BsmtExposure.fillna('NA', inplace=True)     \ndf.BsmtCond.fillna('NA', inplace=True)        \ndf.BsmtQual.fillna('NA', inplace=True)        \ndf.BsmtFinType2.fillna('NA', inplace=True)     \ndf.BsmtFinType1.fillna('NA', inplace=True)     \ndf.MasVnrType.fillna('None', inplace=True)   \ndf.Exterior2nd.fillna('None', inplace=True) \n\n# These are general properties that all houses should have, so NANs probably \n# just mean the values were not recorded. I therefore use \"mode\", the most \n# common value to fill in\ndf.Functional.fillna(df.Functional.mode()[0], inplace=True)       \ndf.Utilities.fillna(df.Utilities.mode()[0], inplace=True)          \ndf.Exterior1st.fillna(df.Exterior1st.mode()[0], inplace=True)        \ndf.SaleType.fillna(df.SaleType.mode()[0], inplace=True)                \ndf.KitchenQual.fillna(df.KitchenQual.mode()[0], inplace=True)        \ndf.Electrical.fillna(df.Electrical.mode()[0], inplace=True)    \n\n# MSZoning should highly correlate with the location, so I use the mode values of individual \n# Neighborhoods\nfor i in df.Neighborhood.unique():\n    if df.MSZoning[df.Neighborhood == i].isnull().sum() > 0:\n        df.loc[df.Neighborhood == i,'MSZoning'] = \\\n        df.loc[df.Neighborhood == i,'MSZoning'].fillna(df.loc[df.Neighborhood == i,'MSZoning'].mode()[0]) \n","5c166185":"# These categorical features are \"rank\", so they can be transformed to \n# numerical features\ndf.Alley = df.Alley.map({'NA':0, 'Grvl':1, 'Pave':2})\ndf.BsmtCond =  df.BsmtCond.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.BsmtExposure = df.BsmtExposure.map({'NA':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4})\ndf['BsmtFinType1'] = df['BsmtFinType1'].map({'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})\ndf['BsmtFinType2'] = df['BsmtFinType2'].map({'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})\ndf.BsmtQual = df.BsmtQual.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.ExterCond = df.ExterCond.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.ExterQual = df.ExterQual.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.FireplaceQu = df.FireplaceQu.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.Functional = df.Functional.map({'Sal':1, 'Sev':2, 'Maj2':3, 'Maj1':4, 'Mod':5, 'Min2':6, 'Min1':7, 'Typ':8})\ndf.GarageCond = df.GarageCond.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.GarageQual = df.GarageQual.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.HeatingQC = df.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.KitchenQual = df.KitchenQual.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.LandSlope = df.LandSlope.map({'Sev':1, 'Mod':2, 'Gtl':3}) \ndf.PavedDrive = df.PavedDrive.map({'N':1, 'P':2, 'Y':3})\ndf.PoolQC = df.PoolQC.map({'NA':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\ndf.Street = df.Street.map({'Grvl':1, 'Pave':2})\ndf.Utilities = df.Utilities.map({'ELO':1, 'NoSeWa':2, 'NoSewr':3, 'AllPub':4})\n\n# Update my lists of numerical and categorical features\nnewquan = ['Alley','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtQual',\n           'ExterCond','ExterQual','FireplaceQu','Functional','GarageCond',\n           'GarageQual','HeatingQC','KitchenQual','LandSlope','PavedDrive','PoolQC',\n           'Street','Utilities']\nquan = quan + newquan \nfor i in newquan: qual.remove(i)\n\n\n# This is actually a categorical feature\ndf.MSSubClass = df.MSSubClass.map({20:'class1', 30:'class2', 40:'class3', 45:'class4',\n                                   50:'class5', 60:'class6', 70:'class7', 75:'class8',\n                                   80:'class9', 85:'class10', 90:'class11', 120:'class12',\n                                   150:'class13', 160:'class14', 180:'class15', 190:'class16'})\n\n# Keeping \"YrSold\" is enough\ndf=df.drop('MoSold',axis=1)\n\n# Update my lists of numerical and categorical features\nquan.remove('MoSold')\nquan.remove('MSSubClass')\nqual.append('MSSubClass')","5496a895":"df['Age'] = df.YrSold - df.YearBuilt\ndf['AgeRemod'] = df.YrSold - df.YearRemodAdd\ndf['AgeGarage'] = df.YrSold - df.GarageYrBlt\n\n# For the houses without a Garage, I filled the NANs with zeros, which makes AgeGarage ~ 2000\n# Here I replace their AgeGarage with the maximum value among the houses with Garages\nmax_AgeGarage = np.max(df.AgeGarage[df.AgeGarage < 1000])\ndf['AgeGarage'] = df['AgeGarage'].map(lambda x: max_AgeGarage if x > 1000 else x)\n\n# Some of the values are negative because the work was done after the house \n# was sold. In these cases, I change them to zero to avoid negative ages.\ndf.Age = df.Age.map(lambda x: 0 if x < 0 else x)\ndf.AgeRemod = df.AgeRemod.map(lambda x: 0 if x < 0 else x)\ndf.AgeGarage = df.AgeGarage.map(lambda x: 0 if x < 0 else x)\n\n# drop the original time variables \ndf=df.drop(['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt'],axis=1)\n\n# update my list of numerical feature\nfor i in ['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt']: quan.remove(i)\nquan = quan + ['Age','AgeRemod','AgeGarage']","be9a728a":"# visualize the distribution of each numerical feature\n\ntemp = pd.melt(df.drop('SalePrice',axis=1), value_vars=quan);\ngrid = sns.FacetGrid(temp, col=\"variable\",  col_wrap=5 , size=3.0, \n                     aspect=1.0,sharex=False, sharey=False);\ngrid.map(sns.distplot, \"value\");\nplt.show();","0150ea8a":"# scatter plots\ntemp = pd.melt(df, id_vars=['SalePrice'],value_vars=quan)\ngrid = sns.FacetGrid(temp, col=\"variable\",  col_wrap=4 , size=3.0, \n                     aspect=1.2,sharex=False, sharey=False)\ngrid.map(plt.scatter, \"value\",'SalePrice', s=3)\nplt.show()","1d486e6e":"# These are somewhat arbitrary \nindex_drop = df.LotFrontage[df.LotFrontage > 300].index\nindex_drop = np.append(index_drop, df.LotArea[df.LotArea > 100000].index)\nindex_drop = np.append(index_drop, df.BsmtFinSF1[df.BsmtFinSF1 > 4000].index)\nindex_drop = np.append(index_drop, df.TotalBsmtSF[df.TotalBsmtSF > 6000].index)\nindex_drop = np.append(index_drop, df['1stFlrSF'][df['1stFlrSF'] > 4000].index)\nindex_drop = np.append(index_drop, df.GrLivArea[(df.GrLivArea > 4000) & (df.SalePrice < 13)].index)\nindex_drop = np.unique(index_drop)\n\n# make sure we only remove data from the training set\nindex_drop = index_drop[index_drop < 1460] \n\ndf = df.drop(index_drop).reset_index(drop=True)\nprint(\"{} examples in the training set are dropped.\".format(len(index_drop)))","ef7ddd6a":"# print the skewness of each numerical feature\nfor i in quan:\n    print(i+': {}'.format(round(skew(df[i]),2)))","a39d298c":"# transform those with skewness > 0.5\nskewed_features = np.array(quan)[np.abs(skew(df[quan])) > 0.5]\ndf[skewed_features] = np.log1p(df[skewed_features])","9c19fde7":"## visualize the distribution again\ntemp = pd.melt(df, value_vars=quan)\ngrid = sns.FacetGrid(temp, col=\"variable\",  col_wrap=5 , size=3.0, \n                     aspect=1.0,sharex=False, sharey=False)\ngrid.map(sns.distplot, \"value\")\nplt.show()","f00ed61a":"# scatter plots\ntemp = pd.melt(df, id_vars=['SalePrice'],value_vars=quan)\ngrid = sns.FacetGrid(temp, col=\"variable\",  col_wrap=4 , size=3.0, \n                     aspect=1.2,sharex=False, sharey=False)\ngrid.map(plt.scatter, \"value\",'SalePrice', s=3)\nplt.show()","66c07759":"# create of list of dummy variables that I will drop, which will be the last\n# column generated from each categorical feature\ndummy_drop = []\nfor i in qual:\n    dummy_drop += [ i+'_'+str(df[i].unique()[-1]) ]\n\n# create dummy variables\ndf = pd.get_dummies(df,columns=qual) \n# drop the last column generated from each categorical feature\ndf = df.drop(dummy_drop,axis=1)","f2466754":"X_train  = df[:-1459].drop(['SalePrice','Id'], axis=1)\ny_train  = df[:-1459]['SalePrice']\nX_test  = df[-1459:].drop(['SalePrice','Id'], axis=1)\n\n# fit the training set only, then transform both the training and test sets\nscaler = RobustScaler()\nX_train[quan]= scaler.fit_transform(X_train[quan])\nX_test[quan]= scaler.transform(X_test[quan])\n\nX_train.shape, X_test.shape # now we have 220 features!","e1fb1a32":"xgb = XGBRegressor()\nxgb.fit(X_train, y_train)\nimp = pd.DataFrame(xgb.feature_importances_ ,columns = ['Importance'],index = X_train.columns)\nimp = imp.sort_values(['Importance'], ascending = False)\n\nprint(imp)","c8daf8eb":"# Define a function to calculate RMSE\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(np.mean((y_true-y_pred)**2))\n\n# Define a function to calculate negative RMSE (as a score)\ndef nrmse(y_true, y_pred):\n    return -1.0*rmse(y_true, y_pred)\n\nneg_rmse = make_scorer(nrmse)\n\nestimator = XGBRegressor()\nselector = RFECV(estimator, cv = 3, n_jobs = -1, scoring = neg_rmse)\nselector = selector.fit(X_train, y_train)\n\nprint(\"The number of selected features is: {}\".format(selector.n_features_))\n\nfeatures_kept = X_train.columns.values[selector.support_] \nX_train = X_train[features_kept]\nX_test = X_test[features_kept]","59529fc0":"# These are the selected features \nfeatures_kept","3d06fc2d":"ridge = KernelRidge()\n\nparameters = {'alpha': uniform(0.05, 1.0), 'kernel': ['polynomial'], \n              'degree': [2], 'coef0':uniform(0.5, 3.5)}\n\nrandom_search = RandomizedSearchCV(estimator = ridge,\n                                   param_distributions = parameters,\n                                   n_iter = 1000,\n                                   cv = 3,\n                                   scoring = neg_rmse,\n                                   n_jobs = -1,\n                                   random_state=0)\n\nrandom_search = random_search.fit(X_train, y_train)\n\nprint(\"Parameters of the best_estimator:\")\nprint(random_search.best_params_)\nprint(\"Mean cross-validated RMSE of the best_estimator: {}\".format(-random_search.best_score_))\nmodel = random_search.best_estimator_\nprint(\"RMSE of the whole training set: {}\".format(rmse(y_train, model.predict(X_train))))","6fcfbcb9":"# Make predictions on the test set\ny_pred = np.exp(model.predict(X_test))\noutput = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': y_pred})\noutput.to_csv('prediction.csv', index=False)","a477c4e5":"For all the time variables, what matters should be the time duration. So I create three features: the age of the house (``Age``), the time duration since the remodel date (``AgeRemod``), and the age of the Garage (``AgeGarage``)","c0b04da1":"## Feature Selection ##","c01d3cd3":"Now there are no NANs anymore, so we can further transform some of the features.","9d718948":"For the categorical features, I will transform them to dummy variables, but I'll drop one column from each of them to avoid [dummy variable trap](http:\/\/www.algosome.com\/articles\/dummy-variable-trap-regression.html). ","7f9982b9":"We see correlations in the scatter plots for some features, such as ``LotFrontage``, ``LotArea``, ``TotalBsmtSF``...etc. There are some obvious outliers in some of these plots. By removing some outliers in the training set, the model will generalize better on unseen data.","ba05d075":"## Data Preprocessing ##","343f0a20":"Now I will first take care of NANs.","6ce48f05":"## Modeling and Prediction ##","e9cbafd5":"Since the final result is evaluated using the Root-Mean-Squared-Error (RMSE) of the logarithm of the predicted house prices, it would be a good idea to log-transform ``SalePrice``. At the end of the code, I will transform my final predictions back to the real values for the output csv file. ","78d3401a":"# Kaggle House Price Prediction"}}