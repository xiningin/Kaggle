{"cell_type":{"d73c344a":"code","982c5f73":"code","5f5345da":"code","0d98b207":"code","8317489e":"code","36882b23":"code","0bd0ad0f":"code","76d89e8b":"code","e5338e0e":"code","f7f23782":"code","2cec3f7a":"code","1fb4a029":"code","dd4d25d7":"code","8e642021":"code","d46c86e8":"code","7111fd18":"code","bb5953cf":"code","69c2631c":"code","6b950b30":"code","6d600cec":"code","bd35214c":"markdown","7e7370bc":"markdown","12163ef1":"markdown","697c58c2":"markdown","e9939081":"markdown","038f47e3":"markdown","3355dfd6":"markdown","535dfaee":"markdown"},"source":{"d73c344a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","982c5f73":"import random\nimport math\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('fivethirtyeight')","5f5345da":"df = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/merc.csv')","0d98b207":"df.head()","8317489e":"df.fuelType.value_counts()","36882b23":"petrol = df[df.fuelType == 'Petrol']","0bd0ad0f":"def LeastSquares(xs, ys):\n    meanx, varx = np.mean(xs), np.var(xs)\n    meany = np.mean(ys)\n    \n    slope = Cov(xs, ys, meanx, meany) \/ varx\n    inter = meany - slope * meanx\n    \n    return inter, slope\n\n## Cov denotes covariance\ndef Cov(xs, ys, meanx=None, meany=None):\n    xs = np.asarray(xs)\n    ys = np.asarray(ys)\n    \n    if meanx is None:\n        meanx = np.mean(xs)\n    if meany is None:\n        meany = np.mean(ys)\n    \n    cov = np.dot(xs - meanx, ys - meany) \/ len(xs)\n    return cov","76d89e8b":"def FitLine(xs, inter, slope):\n    fit_xs = np.sort(xs)\n    fit_ys = inter + slope * fit_xs\n    return fit_xs, fit_ys","e5338e0e":"df.info()","f7f23782":"price = petrol.price\nmileage = petrol.mileage","2cec3f7a":"inter, slope = LeastSquares(mileage, price)\nfit_xs, fit_ys = FitLine(mileage, inter, slope)","1fb4a029":"print('inter is {} and slope is {}'.format(inter, slope))","dd4d25d7":"plt.figure(figsize = (15, 8))\n\nplt.xlabel('Mileage')\nplt.ylabel('Price')\nplt.plot(fit_xs, fit_ys, color = 'black', linewidth = 2)\nplt.scatter(mileage, price, color = 'green', s = 10)","8e642021":"def Residuals(xs, ys, inter, slope):\n    xs = np.asarray(xs)\n    ys = np.asarray(ys)\n    res = ys - (inter + slope * xs)\n    return res","d46c86e8":"res = Residuals(mileage, price, inter, slope)\npetrol['residual'] = res","7111fd18":"sum(res ** 2)","bb5953cf":"petrol.residual.describe()","69c2631c":"petrol.price.describe()","6b950b30":"print('RMSE if we use mileage to predict the price: {}'.format(np.std(petrol.residual)))\nprint('RMSE if we do not use mileage to predict the price: {}'.format(np.std(petrol.price)))\nprint('\\n')\nprint('difference in both RMSEs: {}'.format(np.std(petrol.price)-np.std(petrol.residual)))","6d600cec":"r_squared = 1 - (np.var(petrol.residual) \/ np.var(petrol.price))\nprint(r_squared)","bd35214c":"1. Now that we have plotted a line by fitting the values of mileage and price, we need to find out how good our line is in predicting the price using the mileage.\n2. To demonstrate the goodness of our fit, we will use the \"coefficient of determination\" or 'rho square'.\n3. The formula for 'rho square' is demonstrated in the code cell below.","7e7370bc":"# What are residuals?","12163ef1":"***What does the output tell us?***\n\nThe RMSE is lesser when we use 'mileage' to predict the price of a vehicle than when we do not. The difference is much bigger. Therefore, we will use mileage to predict the prices.","697c58c2":"# What are Linear Least Squares?","e9939081":"1. Linear least square is a method that is used to estimate the slope of the straight line that indicates the relationship between 2 variables.\n2. It is similar to finding correlation. However, in correlation, we measure the strength and the sign of the relationship not the slope that we do with the help of this method.\n3. Using least squares; we first estimate the intercept and the slope of the correlation line using the mean of both the variables. We use the formula; \"***mean(y) = mean(x) * slope + inter***\".\n4. We then fit the sorted values of the variable 'x' in the formula to find the corresponding values of 'y'. The 'FitLine' function does the job.","038f47e3":"***What does the output suggest?***\n\nThe coefficeint of determination is almost 0.17 which suggests that mileage predicts almost 17% of the variance in prices.","3355dfd6":"1. After looking at the graph above, it would be right to say that most of the points either lie above the line or below it. And it was supposed to be so. Why? Because we didn't use the values of the vaiable 'ys' but estimated some 'fit_ys' to fit the line with the original values of 'xs'.\n\n***Why do we do that?***\n\n1. Because with the original values, a straight line could not be constructed. The purpose of this line is to estimate the correlation between the 2 variables. \n\nThat said, if we were to connect all those points, the structure would be anything but a straight line. Those points that are not exactly on the line (the green points in the above figure), are called residuals.\n\n***Why are residuals important?***\n\n1. Residuals give an overall picture of how good is our line.\n2. The Root mean square error (RMSE) if we use mileage to predict the price is known to us by the virtue of the standard deviation of the residuals.\n3. If the standard deviation of residuals turns out to be less than the standard deviation of the 'ys' variable, we say that using 'xs' as a feature to predict the values of 'ys' does not makes a difference. And that, we shouldn't use the feature.\n4. Lastly, one of the primary concern of the regression analysis is to ***reduce the value of \"sum(res ** 2)\"***.","535dfaee":"# Goodness of a fit:"}}