{"cell_type":{"2a590673":"code","ca1a320c":"code","5ba81cf1":"code","bee81975":"code","1c11b4b7":"code","3695c482":"code","ece16519":"code","82a06dad":"code","cc189723":"code","40c8a723":"code","15d44e47":"code","a5bfc08c":"code","ce8f5d1a":"code","ed9775a0":"code","8f9c9e84":"code","bb5b6d23":"code","489f90a4":"code","cd4dae59":"code","f7740487":"code","fc039d3d":"code","783f1749":"code","b4ebd58c":"code","8d710d07":"code","7a997f67":"code","014c5583":"code","d32528b7":"code","5cfdbe1b":"markdown","06c68369":"markdown","92b19365":"markdown","0d917e8c":"markdown","4b7d0887":"markdown","4eec1464":"markdown","1455eb46":"markdown"},"source":{"2a590673":"! pip install --upgrade wandb","ca1a320c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport wandb\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Embedding, GRU, Dense\n\nprint(f'TensorFlow version: {tf.__version__}')","5ba81cf1":"# Building the dataframe for better structuring of the data\ndata = pd.read_csv('..\/input\/engfre-sentences\/eng_fra.txt',sep=\"\\t\",header=None)\ndata.columns=['English', \"French\"]\n\ndata['English'] = \"<start> \"+data['English']+\" <end>\"\ndata['French'] = \"<start> \"+data['French']+\" <end>\"","bee81975":"data.head()","1c11b4b7":"# Log the dataframe to wandb\nrun = wandb.init(project=\"seq2seq\", entity=\"authors\")\ntable = wandb.Table(dataframe=data.iloc[:100,:])\nwith run:\n    run.log({'translations':table})","3695c482":"# Train, Validation and Test set segregation\ndata = data.sample(frac=1).reset_index(drop=True)\n\ntrain = data.iloc[:int(0.7*len(data)),:]\nval = data.iloc[int(0.7*len(data)):int(0.8*len(data)),:]\ntest = data.iloc[int(0.8*len(data)):,:]","ece16519":"# Choose the top 10000 words from the vocabulary\ntop_k = 10000\n\neng_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                      oov_token=\"<unk>\",\n                                                      filters='!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~')\nfre_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                      oov_token=\"<unk>\",\n                                                      filters='!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~')","82a06dad":"# build the vocabulary\neng_tokenizer.fit_on_texts(train['English'])\n\nfre_tokenizer.fit_on_texts(train['French'])","cc189723":"# This is a sanity check function\ndef check_eng_vocab(word):\n    i = eng_tokenizer.word_index[word]\n    print(f\"The index of the word: {i}\")\n    print(f\"Index {i} is word {eng_tokenizer.index_word[i]}\")\n\n# This is a sanity check function\ndef check_fre_vocab(word):\n    i = fre_tokenizer.word_index[word]\n    print(f\"The index of the word: {i}\")\n    print(f\"Index {i} is word {fre_tokenizer.index_word[i]}\")\n\n    \ncheck_eng_vocab(\"go\")\nprint()\ncheck_fre_vocab(\"empreinte\")\n","40c8a723":"# We need to use the <pad> string to\n# help pad the sentences to the same length\neng_tokenizer.word_index['<pad>'] = 0\neng_tokenizer.index_word[0] = '<pad>'\n\nfre_tokenizer.word_index['<pad>'] = 0\nfre_tokenizer.index_word[0] = '<pad>'","15d44e47":"# Turn the sentences to indices\ntrain_eng = eng_tokenizer.texts_to_sequences(train[\"English\"])\nval_eng = eng_tokenizer.texts_to_sequences(val[\"English\"])\ntest_eng = eng_tokenizer.texts_to_sequences(test[\"English\"])\n\ntrain_fre = fre_tokenizer.texts_to_sequences(train[\"French\"])\nval_fre = fre_tokenizer.texts_to_sequences(val[\"French\"])\ntest_fre = fre_tokenizer.texts_to_sequences(test[\"French\"])","a5bfc08c":"train_eng_vector = tf.keras.preprocessing.sequence.pad_sequences(train_eng, maxlen=50, padding='post')\nval_eng_vector = tf.keras.preprocessing.sequence.pad_sequences(val_eng, maxlen=50, padding='post')\ntest_eng_vector = tf.keras.preprocessing.sequence.pad_sequences(test_eng, maxlen=50, padding='post')\n\ntrain_fre_vector = tf.keras.preprocessing.sequence.pad_sequences(train_fre, maxlen=50, padding='post')\nval_fre_vector = tf.keras.preprocessing.sequence.pad_sequences(val_fre, maxlen=50, padding='post')\ntest_fre_vector = tf.keras.preprocessing.sequence.pad_sequences(test_fre, maxlen=50, padding='post')\n\n\n# build the dataset\ntrain_eng = tf.data.Dataset.from_tensor_slices(train_eng_vector)\nval_eng = tf.data.Dataset.from_tensor_slices(val_eng_vector)\ntest_eng = tf.data.Dataset.from_tensor_slices(test_eng_vector)\n\ntrain_fre = tf.data.Dataset.from_tensor_slices(train_fre_vector)\nval_fre = tf.data.Dataset.from_tensor_slices(val_fre_vector)\ntest_fre = tf.data.Dataset.from_tensor_slices(test_fre_vector)","ce8f5d1a":"with open(\"eng_token.txt\", 'w') as f:\n    eng_json = eng_tokenizer.to_json()\n    f.write(eng_json)\nwith open(\"fre_token.txt\", 'w') as f:\n    fre_json = fre_tokenizer.to_json()\n    f.write(fre_json)","ed9775a0":"# prefecth and batch the dataset\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 512\n\ntrain_ds = tf.data.Dataset.zip((train_eng, train_fre)).shuffle(42).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\nval_ds = tf.data.Dataset.zip((val_eng, val_fre)).shuffle(42).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\ntest_ds = tf.data.Dataset.zip((test_eng, test_fre)).shuffle(42).batch(BATCH_SIZE, drop_remainder=True).prefetch(buffer_size=AUTOTUNE)","8f9c9e84":"for eng, fra in train_ds.take(1):\n    eng_one = eng[1]\n    fra_one = fra[1]\n    print(\"English:\")\n    for e in eng_one:\n        if eng_tokenizer.index_word[e.numpy()] == '<pad>':\n            break\n        else:\n            print(eng_tokenizer.index_word[e.numpy()], end=\" \")\n    print(\"\\nFrench:\")\n    for f in fra_one:\n        if fre_tokenizer.index_word[f.numpy()] == '<pad>':\n            break\n        else:\n            print(fre_tokenizer.index_word[f.numpy()], end=\" \")","bb5b6d23":"# Some global variables\nEMBEDDIN_DIM = 512\nVOCAB_SIZE = 10000\nUNITS_RNN = 256","489f90a4":"encoder_input = Input(shape=(None,))\nencoder_embedded = Embedding(input_dim=VOCAB_SIZE, output_dim=64)(encoder_input)\n\n# Return states in addition to output\noutput = GRU(UNITS_RNN, return_sequences=True)(encoder_embedded)\noutput = GRU(UNITS_RNN, return_sequences=True)(output)\n_, state_h = GRU(UNITS_RNN, return_state=True)(output)\n\nencoder_state = [state_h]\n\ndecoder_input = Input(shape=(None,))\ndecoder_embedded = Embedding(input_dim=VOCAB_SIZE, output_dim=64)(decoder_input)\n\n# Pass the state to a new GRU layer, as initial state\ndecoder_output = GRU(UNITS_RNN, return_sequences=True)(decoder_embedded, initial_state=encoder_state)\ndecoder_output = GRU(UNITS_RNN, return_sequences=True)(decoder_output, initial_state=encoder_state)\ndecoder_output = GRU(UNITS_RNN, return_sequences=True)(decoder_output, initial_state=encoder_state)\n\noutput = Dense(VOCAB_SIZE)(decoder_output)\n\nmodel = tf.keras.Model([encoder_input, decoder_input], output)\nmodel.summary()","cd4dae59":"# checking the shape\nfor eng, fre in train_ds.take(1):\n    p = model(inputs=[eng,fre])\n    print(fre.shape)\n    print(p.shape)","f7740487":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    \n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    \n    return tf.reduce_mean(loss_)","fc039d3d":"@tf.function\ndef train_step(eng, fre):\n    loss = 0\n    with tf.GradientTape() as tape:\n        predictions = model(inputs=[eng,fre])\n\n        loss = loss_function(fre[:,1:], predictions[:,:-1,:])\n\n    trainable_variables = model.trainable_variables\n\n    gradients = tape.gradient(loss, trainable_variables)\n\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n    return loss","783f1749":"@tf.function\ndef val_step(eng, fre):\n    loss = 0\n    predictions = model(inputs=[eng,fre])\n    loss = loss_function(fre[:,1:], predictions[:,:-1,:])\n    return loss","b4ebd58c":"EPOCHS = 15\nepoch_wise_loss = []\nepoch_wise_val_loss = []\n\nrun = wandb.init(project=\"seq2seq\", entity=\"authors\", name=\"training\")\n\nfor epoch in range(EPOCHS):\n    batch_wise_loss = []\n    for (batch, (eng, fre)) in enumerate(train_ds):\n        loss = train_step(eng, fre)\n        batch_wise_loss.append(loss.numpy())\n        if batch%100 == 0:\n            print(f'Epoch: {epoch} Batch: {batch} Loss: {batch_wise_loss[-1]:.3f}')\n    epoch_wise_loss.append(np.mean(batch_wise_loss))\n    \n    batch_wise_val_loss = []\n    for (batch, (eng, fre)) in enumerate(val_ds):\n        loss = val_step(eng, fre)\n        batch_wise_val_loss.append(loss.numpy())\n    epoch_wise_val_loss.append(np.mean(batch_wise_val_loss))\n    print(f'Epoch: {epoch} Total Loss: {epoch_wise_loss[-1]:.3f} Val Loss:{epoch_wise_val_loss[-1]:.3f}')\n    run.log({'loss':epoch_wise_loss[-1], 'val_loss':epoch_wise_val_loss[-1]})\n    print('-'*40)\nrun.finish()","8d710d07":"plt.plot(epoch_wise_loss, label='Train Loss')\nplt.plot(epoch_wise_val_loss, label='Validation Loss')\n\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\nplt.show()","7a997f67":"run = wandb.init(project=\"seq2seq\", entity=\"authors\", name=\"model_artifact\")\nartifact = wandb.Artifact('my-model', type='model_weights')\nmodel.save_weights(\".\/model_epoch30.h5\")\nartifact.add_file(\".\/model_epoch30.h5\")\nrun.log_artifact(artifact)\nrun.finish()","014c5583":"batch_loss = []\nfor (batch, (eng, fre)) in enumerate(test_ds):\n    loss = val_step(eng, fre)\n    batch_loss.append(loss.numpy())\nprint(f'Test Loss: {np.mean(batch_wise_val_loss):.3f}')","d32528b7":"print(\"English\")\neng = [\"<start> i am good <end>\"]\nprint(eng[0])\neng = np.array(eng_tokenizer.texts_to_sequences(eng))\n\nfre_words = [\"<start>\"]\nfre = np.array(fre_tokenizer.texts_to_sequences(fre_words))\nword = \"\"\nwhile word != \"<end>\":\n    prediction = model(inputs=[eng, fre])\n    word = fre_tokenizer.index_word[tf.math.argmax(prediction[0,-1]).numpy()]\n    fre_words[0] = fre_words[0]+\" \"+word\n    fre = np.array(fre_tokenizer.texts_to_sequences(fre_words))\nprint(\"French:\")\nprint(fre_words[0])","5cfdbe1b":"# Produce the text","06c68369":"# Data\n\nDataset Kaggle Kernel: [English to French](https:\/\/www.kaggle.com\/aritrag\/english-french-data)","92b19365":"# Imports\nThe packages that we will use in this kernel are:\n- numpy\n- pandas\n- matplotlib\n- tensorflow\n- wandb","0d917e8c":"# Inference","4b7d0887":"# Introduction\nThis kernel is a shot at reproducing the academic paper [Sequence to Sequence Learning with Neural Networks](https:\/\/arxiv.org\/abs\/1409.3215v3) by Ilya Sutskever et.al. Here the authors provide an end to end method for machine translation with the help of deep learning models. This kernel will be written out in TensorFlow. Please feel free to fork it and experiment with the models and the methods.\n\nIf you like this notebook, please upvote. This way I would be motivated to keep providing good kernels where in I reproduce academic paper with robust details.","4eec1464":"# Save the tokenizer","1455eb46":"# Processing the data\nHere we use the `tf.keras.preprocessing.text.Tokenizer` to help us with the pre-processing steps. This is a great API that takes away a lot of boilerplate to clean the text data.\n\n- Defined the size of the vocab which is 5000.\n- Initialized the Tokenizer class.\n    - Standardized (all to lower case)\n    - Filters the punctuations\n    - Splits the text\n    - Creates the vocabulary (<start>, <end> and <unk> is defined)"}}