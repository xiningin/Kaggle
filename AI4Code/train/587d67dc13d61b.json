{"cell_type":{"b929a156":"code","da21cae5":"code","ef0751db":"code","64cd2870":"code","9206fbac":"code","718d7325":"code","957a89ad":"code","31255d83":"code","9c0cf6c2":"code","c57155e1":"code","d79ef88a":"code","0cd0f05c":"code","f4434430":"code","6c5f32e3":"code","5039cfaa":"code","c14c9204":"code","16a643dc":"code","24a202bb":"code","2fc8c12f":"code","b6df3737":"code","c3b2ef53":"code","cea7278c":"code","66d2086a":"code","1fbb5a21":"code","9cca8c0a":"code","806952c0":"code","1cb4097c":"code","ac0a0b94":"code","6b142136":"code","0630ef7d":"code","3746c95b":"code","4f464250":"code","d40b773c":"code","28a914e6":"markdown","eda32520":"markdown","8da57886":"markdown","6f7c9db2":"markdown","d6427518":"markdown"},"source":{"b929a156":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:a\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da21cae5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import classification_report","ef0751db":"train_set = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","64cd2870":"display(train_set.head())\ndisplay(train_set.describe())\nprint(train_set.info())","9206fbac":"for col in train_set.columns:\n    print(col)\n    print(train_set[col].value_counts(normalize = True))","718d7325":"sns.pairplot(train_set,\n             kind = 'reg',\n             diag_kind = 'hist',)\nplt.show()","957a89ad":"sns.pairplot(train_set,\n             hue = 'Survived',\n             kind = 'reg',\n             diag_kind = 'hist',\n             diag_kws = {'alpha':0.5})\nplt.show()","31255d83":"corr_matrix = train_set.corr()\nsns.heatmap(corr_matrix)","9c0cf6c2":"prep_df = train_set.drop(columns = ['PassengerId', 'Name', 'Cabin', 'Ticket'])\nprep_df = pd.get_dummies(prep_df, drop_first=False)\ndisplay(prep_df.head())\ncorr_matrix = prep_df.corr()\nprint(corr_matrix)\nsns.heatmap(corr_matrix)\nplt.show()","c57155e1":"X = prep_df.drop(columns = 'Survived').values\ny = prep_df['Survived'].values\n\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nscaler = StandardScaler()\n\nimp.fit(X)\nX_imp = imp.transform(X)\n\nX_scaled = scaler.fit_transform(X_imp)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled,\n                                                    y,\n                                                    stratify = y,\n                                                    test_size = 0.2,\n                                                    random_state = 42)","d79ef88a":"standard_dt = DecisionTreeClassifier(random_state = 42)\nstandard_lr = LogisticRegression()\nstandard_knn = KNeighborsClassifier()\nstandard_svc = SVC(random_state = 42)\nstandard_rf = RandomForestClassifier(random_state = 42)\nstandard_mlp = MLPClassifier(random_state = 42)\n\nstandard_classifiers = [ ('Standard Decision Tree', standard_dt),\n                        ('Standard Logistic Regression', standard_lr),\n                        ('Standard KNN', standard_knn),\n                        ('Standard SVC', standard_svc),\n                        ('Standard Random Forest', standard_rf)]#,('Standard MLP', standard_mlp)]\n\nfor clf_name, clf in standard_classifiers:\n    try:\n        clf.fit(X_train, y_train)\n        loop_pred = clf.predict(X_test)\n        loop_pred_prob = clf.predict_proba(X_test)[:, 1]\n        print(clf_name)\n        print(roc_auc_score(y_test, loop_pred_prob))\n        print(classification_report(y_test, loop_pred))\n        print(confusion_matrix(y_test, loop_pred))\n        print(accuracy_score(y_test, loop_pred))\n    except:\n        clf.fit(X_train, y_train)\n        loop_pred = clf.predict(X_test)\n        print(clf_name)\n        print(classification_report(y_test, loop_pred))\n        print(confusion_matrix(y_test, loop_pred))\n        print(accuracy_score(y_test, loop_pred))\n\n        \nvc = VotingClassifier(estimators = standard_classifiers)     \n\nvc.fit(X_train, y_train)   \n\nvc_pred = vc.predict(X_test)\nprint('Voting Classifier')\nprint(classification_report(y_test, vc_pred))\nprint(confusion_matrix(y_test, vc_pred))\nprint(accuracy_score(y_test, vc_pred))","0cd0f05c":"params_dt = {'max_depth':[3,4,5,6,7,8,14, None],\n             'min_samples_leaf':[0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.3, None],\n             'max_features':[0.2, 0.4, 0.8, 1, 2, 3],\n             'criterion':['gini', 'entropy']}\n\ngrid_dt = GridSearchCV(estimator=standard_dt,\n                       param_grid=params_dt,\n                       scoring = 'roc_auc',\n                       cv = 10,\n                       n_jobs = -1)\n\ngrid_dt.fit(X_train, y_train)\n\nbest_hyperparams_dt = grid_dt.best_params_\n\nbest_score_dt = grid_dt.best_score_\n\nbest_model_dt = grid_dt.best_estimator_\n\nprint(best_hyperparams_dt)\nprint(best_score_dt)\n\ny_pred_dt = best_model_dt.predict(X_test)\ny_pred_prob_dt = best_model_dt.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_dt)\nplt.plot(fpr, tpr)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot([0, 1], [0, 1], linestyle = \"--\")\nplt.show()\nprint(roc_auc_score(y_test, y_pred_prob_dt))\nprint(classification_report(y_test, y_pred_dt))\nconfusion_matrix(y_test, y_pred_dt)","f4434430":"params_lr = {'penalty':['l1', 'l2', 'elasticnet', 'none'],\n             'fit_intercept':[True, False],\n             'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}#, 'tol':[0.0004, 0.004, 0.04, 0.4, 4]}\n\ngrid_lr = GridSearchCV(estimator=standard_lr,\n                       param_grid=params_lr,\n                       scoring = 'roc_auc',\n                       cv = 10,\n                       n_jobs = -1)\n\ngrid_lr.fit(X_train, y_train)\n\nbest_hyperparams_lr = grid_lr.best_params_\n\nbest_score_lr = grid_lr.best_score_\n\nbest_model_lr = grid_lr.best_estimator_\n\nprint(best_hyperparams_lr)\nprint(best_score_lr)\n\ny_pred_lr = best_model_lr.predict(X_test)\ny_pred_prob_lr = best_model_lr.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_lr)\nplt.plot(fpr, tpr)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot([0, 1], [0, 1], linestyle = \"--\")\nplt.show()\nprint(roc_auc_score(y_test, y_pred_prob_lr))\nprint(classification_report(y_test, y_pred_lr))\nconfusion_matrix(y_test, y_pred_lr)","6c5f32e3":"params_knn = {'n_neighbors':list(range(1,30)),\n             'leaf_size':list(range(1,50)),\n             'p':[1, 2]}\n\ngrid_knn = GridSearchCV(estimator=standard_knn,\n                       param_grid=params_knn,\n                       scoring = 'roc_auc',\n                       cv = 10,\n                       n_jobs = -1)\n\ngrid_knn.fit(X_train, y_train)\n\nbest_hyperparams_knn = grid_knn.best_params_\n\nbest_score_knn = grid_knn.best_score_\n\nbest_model_knn = grid_knn.best_estimator_\n\nprint(best_hyperparams_knn)\nprint(best_score_knn)\n\ny_pred_knn = best_model_knn.predict(X_test)\ny_pred_prob_knn = best_model_knn.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_knn)\nplt.plot(fpr, tpr)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot([0, 1], [0, 1], linestyle = \"--\")\nplt.show()\nprint(roc_auc_score(y_test, y_pred_prob_knn))\nprint(classification_report(y_test, y_pred_knn))\nconfusion_matrix(y_test, y_pred_knn)","5039cfaa":"params_svc = {'degree':list(range(2,5)),\n             'C':[0.1, 1, 10, 100],\n              'kernel':['linear', 'rbf', 'poly']}\n\ngrid_svc = GridSearchCV(estimator=standard_svc,\n                       param_grid=params_svc,\n                       scoring = 'accuracy',\n                       cv = 10,\n                       n_jobs = 1,\n                        verbose = 10)\n\ngrid_svc.fit(X_train, y_train)\n\nbest_hyperparams_svc = grid_svc.best_params_\n\nbest_score_svc = grid_svc.best_score_\n\nbest_model_svc = grid_svc.best_estimator_\n\nprint(best_hyperparams_svc)\nprint(best_score_svc)\n\ny_pred_svc = best_model_svc.predict(X_test)\n\n\nprint(classification_report(y_test, y_pred_svc))\nconfusion_matrix(y_test, y_pred_svc)","c14c9204":"params_rf = {'max_features':['auto', 'sqrt', 'log2'],\n             'max_depth':[5,6,7,8,14, None],\n             'min_samples_leaf':[0.02, 0.04, 0.06, 0.08, 0.1],\n             'max_features':[0.4, 0.8, 1],\n             'bootstrap':[True, False],\n             'criterion':['gini', 'entropy']}\n\ngrid_rf = GridSearchCV(estimator = standard_rf,\n                       param_grid = params_rf,\n                       scoring = 'roc_auc',\n                       cv = 10,\n                       n_jobs = -1, verbose = 10)\n\ngrid_rf.fit(X_train, y_train)\n\nbest_hyperparams_rf = grid_rf.best_params_\n\nbest_score_rf = grid_rf.best_score_\n\nbest_model_rf = grid_rf.best_estimator_\n\nprint(best_hyperparams_rf)\nprint(best_score_rf)\n\ny_pred_rf = best_model_rf.predict(X_test)\ny_pred_prob_rf = best_model_rf.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_rf)\nplt.plot(fpr, tpr)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot([0, 1], [0, 1], linestyle = \"--\")\nplt.show()\nprint(roc_auc_score(y_test, y_pred_prob_rf))\nprint(classification_report(y_test, y_pred_rf))\nconfusion_matrix(y_test, y_pred_rf)","16a643dc":"best_classifiers = [ ('Best Decision Tree', best_model_dt),\n                        ('Best Logistic Regression', best_model_lr),\n                        ('Best KNN', best_model_knn),\n                        ('Best SVC', best_model_svc),('Best Random Forest', best_model_rf)]\n\nfor clf_name, clf in best_classifiers:\n    try:\n        clf.fit(X_train, y_train)\n        loop_pred = clf.predict(X_test)\n        loop_pred_prob = clf.predict_proba(X_test)[:, 1]\n        print(clf_name)\n        print(roc_auc_score(y_test, loop_pred_prob))\n        print(classification_report(y_test, loop_pred))\n        print(confusion_matrix(y_test, loop_pred))\n        print(accuracy_score(y_test, loop_pred))\n    except:\n        clf.fit(X_train, y_train)\n        loop_pred = clf.predict(X_test)\n        print(clf_name)\n        print(classification_report(y_test, loop_pred))\n        print(confusion_matrix(y_test, loop_pred))\n        print(accuracy_score(y_test, loop_pred))\n\n        \nbest_vc = VotingClassifier(estimators = best_classifiers)     \n\nbest_vc.fit(X_train, y_train)   \n\nbest_vc_pred = best_vc.predict(X_test)\nprint('Voting Classifier')\nprint(classification_report(y_test, best_vc_pred))\nprint(confusion_matrix(y_test, best_vc_pred))\nprint(accuracy_score(y_test, best_vc_pred))","24a202bb":"params_dt_2 = {'max_depth':[3,4,5,6,7,8,14, None],\n             'min_samples_leaf':[0.01, 0.02, 0.04, 0.06, 0.08, 0.1],\n             'max_features':[0.2, 0.4, 0.8, 1],\n             'criterion':['gini', 'entropy']}\n\ngrid_dt_2 = GridSearchCV(estimator=standard_dt,\n                       param_grid=params_dt_2,\n                       scoring = 'accuracy',\n                       cv = 10,\n                       n_jobs = -1)\n\ngrid_dt_2.fit(X_scaled, y)\n\nbest_hyperparams_dt_2 = grid_dt_2.best_params_\n\nbest_score_dt_2 = grid_dt_2.best_score_\n\nbest_model_dt_2 = grid_dt_2.best_estimator_\n\nprint(best_hyperparams_dt_2)\nprint(best_score_dt_2)","2fc8c12f":"params_lr_2 = {'penalty':['l1', 'l2', 'elasticnet', 'none'],\n             'fit_intercept':[True, False],\n             'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\ngrid_lr_2 = GridSearchCV(estimator=standard_lr,\n                       param_grid=params_lr_2,\n                       scoring = 'accuracy',\n                       cv = 10,\n                       n_jobs = -1)\n\ngrid_lr_2.fit(X_scaled, y)\n\nbest_hyperparams_lr_2 = grid_lr_2.best_params_\n\nbest_score_lr_2 = grid_lr_2.best_score_\n\nbest_model_lr_2 = grid_lr_2.best_estimator_\n\nprint(best_hyperparams_lr_2)\nprint(best_score_lr_2)","b6df3737":"params_knn_2 = {'n_neighbors':list(range(3,15)),\n             'leaf_size':list(range(1,10)),\n             'p':[1, 2]}\n\ngrid_knn_2 = GridSearchCV(estimator=standard_knn,\n                       param_grid=params_knn_2,\n                       scoring = 'accuracy',\n                       cv = 10,\n                       n_jobs = -1)\n\ngrid_knn_2.fit(X_scaled, y)\n\nbest_hyperparams_knn_2 = grid_knn_2.best_params_\n\nbest_score_knn_2 = grid_knn_2.best_score_\n\nbest_model_knn_2 = grid_knn_2.best_estimator_\n\nprint(best_hyperparams_knn_2)\nprint(best_score_knn_2)","c3b2ef53":"params_svc_2 = {'degree':list(range(2,5)),\n             'C':[0.1, 1, 10],\n              'kernel':['linear', 'rbf', 'poly']}\n\ngrid_svc_2 = GridSearchCV(estimator = standard_svc,\n                       param_grid = params_svc_2,\n                       scoring = 'accuracy',\n                       cv = 10,\n                       n_jobs = -1)\n\ngrid_svc_2.fit(X_scaled, y)\n\nbest_hyperparams_svc_2 = grid_svc_2.best_params_\n\nbest_score_svc_2 = grid_svc_2.best_score_\n\nbest_model_svc_2 = grid_svc_2.best_estimator_\n\nprint(best_hyperparams_svc_2)\nprint(best_score_svc_2)","cea7278c":"params_rf_2 = {'max_features':['auto', 'sqrt', 'log2'],\n             'max_depth':[6,7,8,12, 14, 16],\n             'min_samples_leaf':[0.02, 0.04, 0.06, 0.08, 0.1],\n             'max_features':[0.4, 0.8, 1],\n             'bootstrap':[True, False],\n             'criterion':['gini', 'entropy']}\n\ngrid_rf_2 = GridSearchCV(estimator = standard_rf,\n                       param_grid = params_rf_2,\n                       scoring = 'accuracy',\n                       cv = 10,\n                       n_jobs = -1)\n\ngrid_rf_2.fit(X_scaled, y)\n\nbest_hyperparams_rf_2 = grid_rf_2.best_params_\n\nbest_score_rf_2 = grid_rf_2.best_score_\n\nbest_model_rf_2 = grid_rf_2.best_estimator_\n\nprint(best_hyperparams_rf_2)\nprint(best_score_rf_2)","66d2086a":"best_classifiers_2 = [ ('Best Decision Tree 2', best_model_dt_2),\n                        ('Best Logistic Regression 2', best_model_lr_2),\n                        ('Best KNN 2', best_model_knn_2),\n                        ('Best SVC 2', best_model_svc_2),\n                      ('Best Random Forest 2', best_model_rf_2)]\n\nfor clf_name, clf in best_classifiers_2:\n    try:\n        clf.fit(X_train, y_train)\n        loop_pred = clf.predict(X_test)\n        loop_pred_prob = clf.predict_proba(X_test)[:, 1]\n        print(clf_name)\n        print(roc_auc_score(y_test, loop_pred_prob))\n        print(classification_report(y_test, loop_pred))\n        print(confusion_matrix(y_test, loop_pred))\n        print(accuracy_score(y_test, loop_pred))\n    except:\n        clf.fit(X_train, y_train)\n        loop_pred = clf.predict(X_test)\n        print(clf_name)\n        print(classification_report(y_test, loop_pred))\n        print(confusion_matrix(y_test, loop_pred))\n        print(accuracy_score(y_test, loop_pred))\n\n        \nbest_vc_2 = VotingClassifier(estimators = best_classifiers_2)     \n\nbest_vc_2.fit(X_train, y_train)   \n\nbest_vc_pred_2 = best_vc_2.predict(X_test)\nprint('Voting Classifier 2')\nprint(classification_report(y_test, best_vc_pred_2))\nprint(confusion_matrix(y_test, best_vc_pred_2))\nprint(accuracy_score(y_test, best_vc_pred_2))","1fbb5a21":"best_vc_3 = VotingClassifier(estimators = best_classifiers_2)\nbest_vc_3.fit(X_scaled, y)","9cca8c0a":"test_set = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndisplay(test_set.head())","806952c0":"prep_test_df = test_set.drop(columns = ['PassengerId', 'Name', 'Cabin', 'Ticket'])\nprep_test_df = pd.get_dummies(prep_test_df, drop_first=False)\ndisplay(prep_test_df.head())","1cb4097c":"X_test_set = prep_test_df.values\n\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nscaler = StandardScaler()\n\nimp.fit(X_test_set)\nX_imp_test_set = imp.transform(X_test_set)\n\nX_scaled_test_set = scaler.fit_transform(X_imp_test_set)","ac0a0b94":"print(result)","6b142136":"print(ootb_result)","0630ef7d":"plt.hist(result)","3746c95b":"result = best_vc_2.predict(X_scaled_test_set)\ndf = pd.DataFrame()\ndf['PassengerId'] = test_set.PassengerId.values\ndf['Survived'] = result\ndf.to_csv('result.csv', index = False)","4f464250":"otb_result = vc.predict(X_scaled_test_set)df2 = pd.DataFrame()\ndf2['PassengerId'] = test_set.PassengerId.values\ndf2['Survived'] = ootb_result\ndf2.to_csv('result2.csv', index = False)","d40b773c":"df3= pd.DataFrame()\ndf3['PassengerId'] = test_set.PassengerId.values\ndf3['Survived'] = best_vc_3.predict(X_scaled_test_set)\ndf3.to_csv('result_4.csv', index = False)","28a914e6":"# EDA","eda32520":"# Preprocessing","8da57886":"# Modelling","6f7c9db2":"# Modelling","d6427518":"## Standard Models"}}