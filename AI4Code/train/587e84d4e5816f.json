{"cell_type":{"5794e6c3":"code","e8ab779b":"code","ee44f130":"code","cb741f49":"code","274849ed":"code","40599d76":"code","429048d3":"code","fe6825bc":"code","e8850694":"code","58eee888":"code","0ada0bfa":"code","50488f40":"code","4577e719":"code","c3c330ed":"code","3fb37dc3":"markdown","467f288e":"markdown","53f9d9f0":"markdown","b3765547":"markdown","3d39ccac":"markdown","2f1b095a":"markdown","2a308d81":"markdown","cbf6869b":"markdown","ef01c02d":"markdown","afea4823":"markdown","27954929":"markdown"},"source":{"5794e6c3":"from keras.datasets import imdb\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Flatten\nfrom keras.callbacks import EarlyStopping\nimport string\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","e8ab779b":"top_words = 10000\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)","ee44f130":"word_dict = imdb.get_word_index()\nword_dict = { key:(value + 3) for key, value in word_dict.items() }\nword_dict[''] = 0                                                    # Padding\nword_dict['>'] = 1                                                   # Start\nword_dict['?'] = 2                                                   # Unknown word\nreverse_word_dict = { value:key for key, value in word_dict.items() }\nprint(' '.join(reverse_word_dict[id] for id in x_train[0]))","cb741f49":"max_review_length = 500\nx_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\nx_test = sequence.pad_sequences(x_test, maxlen=max_review_length)","274849ed":"embedding_vector_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\nmodel.add(Flatten())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","40599d76":"es = EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=100)\nhist = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=1000, batch_size=128, callbacks=[es], verbose=0)\n\n_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n_, test_acc = model.evaluate(x_test, y_test, verbose=1)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","429048d3":"%matplotlib inline\n\nsns.set()\nacc = hist.history['accuracy']\nval = hist.history['val_accuracy']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, '-', label='Training accuracy')\nplt.plot(epochs, val, ':', label='Validation accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.plot()","fe6825bc":"%matplotlib inline\n\nsns.set()\nloss = hist.history['loss']\nval = hist.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, '-', label='Training loss')\nplt.plot(epochs, val, ':', label='Validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='upper left')\nplt.plot()","e8850694":"scores = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1] * 100))","58eee888":"def analyze(text):\n    # Prepare the input by removing punctuation characters, converting\n    # characters to lower case, and removing words containing numbers\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n    text = text.lower().split(' ')\n    text = [word for word in text if word.isalpha()]\n\n    # Generate an input tensor\n    input = [1]\n    for word in text:\n        if word in word_dict and word_dict[word] < top_words:\n            input.append(word_dict[word])\n        else:\n            input.append(2)\n    padded_input = sequence.pad_sequences([input], maxlen=max_review_length)\n\n    # Invoke the model and return the result\n    result = model.predict(np.array([padded_input][0]))[0][0]\n    return result","0ada0bfa":"analyze('Easily the most stellar experience I have ever had.')","50488f40":"analyze('This is the shittiest thing I have ever heard!') # Perhaps \"shittiest\" would be an outlier. (Need to take care of in future models!) ","4577e719":"analyze('I had a really bad experience with the customer service.')","c3c330ed":"analyze('This film is a once-in-a-lifetime opportunity')","3fb37dc3":"### Train a sequential model.","467f288e":"### Handle the reviews with unknown words.","53f9d9f0":"# This is a notebook to analyze the sentiment of a sentence using the IMDb movie review dataset. Although this model is trained on reviews on IMDb but it could be generalised to the general English sentences as well. Feel free to play around with this model!","b3765547":"### A function taking an English sentence as input and invoking the trained model to analyze the sentiment of the input sentence.","3d39ccac":"### Import necessary libraries","2f1b095a":"### Accuracy Plot","2a308d81":"### Fit the model with EarlyStopping enabled.","cbf6869b":"### Play with random sentences to check the sentiment score between 0 and 1, closer to 0 being a negative sentiment and closer to 1 being a positive sentiment.","ef01c02d":"Restrict the max_review_length to 500","afea4823":"### Loss Plot","27954929":"### Instantiate top 10,000 words from the dataset."}}