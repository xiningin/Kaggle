{"cell_type":{"6b639be7":"code","feb4a71b":"code","debe7509":"code","e6189028":"code","f2914909":"code","51f32620":"code","0b89e08f":"code","d767fcea":"code","7b11404e":"code","958e46e2":"code","f176a3e5":"code","def86e85":"code","e677d991":"code","8585daa8":"code","641ea7cb":"code","374f609f":"code","2003888b":"code","50ca56b3":"code","e3977a7a":"code","39ac55c8":"code","be6c0d23":"code","a4e2b470":"code","70f9ac40":"code","b6413b9c":"markdown","b9b926de":"markdown","88f21972":"markdown","ffbcc0cb":"markdown","8efe291f":"markdown","6cb2e028":"markdown","e526adca":"markdown","35e18e25":"markdown","9bfd95af":"markdown","24a628f0":"markdown","148027a0":"markdown","84b2a175":"markdown","f521235a":"markdown"},"source":{"6b639be7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport struct as st\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom keras import layers\nfrom keras.layers import Dropout , Input,GlobalAveragePooling2D, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\nfrom keras.models import Model, load_model\nfrom keras.initializers import glorot_uniform\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn.preprocessing import OneHotEncoder\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","feb4a71b":"data_types = {\n        0x08: ('ubyte', 'B', 1),\n        0x09: ('byte', 'b', 1),\n        0x0B: ('>i2', 'h', 2),\n        0x0C: ('>i4', 'i', 4),\n        0x0D: ('>f4', 'f', 4),\n        0x0E: ('>f8', 'd', 8)}","debe7509":"files = glob.glob(\"..\/input\/*\")\nfiles","e6189028":"f = open(files[0],\"rb\")\nf1 = open(files[1],\"rb\")\nf2 = open(files[4] , \"rb\")\nf3 = open(files[2] , \"rb\")","f2914909":"f.seek(0)\nmagic = st.unpack('>4B',f.read(4))\nn = st.unpack('>I',f.read(4))[0] #num of images\nnR = st.unpack('>I',f.read(4))[0] #num of rows\nnC = st.unpack('>I',f.read(4))[0] #num of column\nimages = np.zeros((n,nR,nC))\nnBytesTotal = n*nR*nC*1 #since each pixel data is 1 byte\nimages = 255 - np.asarray(st.unpack('>'+'B'*nBytesTotal,f.read(nBytesTotal))).reshape((n,nR,nC))","51f32620":"plt.imshow(images[12])","0b89e08f":"kernel = np.array([[0,1,0],[1,-4,1],[0,1,0]])\nX_train = np.zeros((60000,56,56))\nfor i in range(len(images)):\n    img = images[i].astype(\"float32\")\n    img = cv2.resize(img,(56,56))\n    img = cv2.filter2D(img , -1 , kernel)\n    img = cv2.blur(img , (3,3))\n    X_train[i] = img\n   ","d767fcea":"plt.imshow(X_train[12])","7b11404e":"f1.seek(8)\ndataFormat = data_types[magic[2]][1]\ndataSize = data_types[magic[2]][2]\ntrain_label = np.asarray(st.unpack('>'+\"B\"*n,f1.read(n*1))).reshape((n,1))\n","958e46e2":"enc = OneHotEncoder(sparse = False)\ny_train = enc.fit_transform(train_label)","f176a3e5":"f2.seek(0)\nmagic = st.unpack('>4B',f2.read(4))\nn = st.unpack('>I',f2.read(4))[0] #num of images\nnR = st.unpack('>I',f2.read(4))[0] #num of rows\nnC = st.unpack('>I',f2.read(4))[0] #num of column\nimages = np.zeros((n,nR,nC))\nnBytesTotal = n*nR*nC*1 #since each pixel data is 1 byte\nimages_test = 255 - np.asarray(st.unpack('>'+'B'*nBytesTotal,f2.read(nBytesTotal))).reshape((n,nR,nC))","def86e85":"kernel = np.array([[0,1,0],[1,-4,1],[0,1,0]])\nX_test = np.zeros((10000,56,56))\nfor i in range(len(images_test)):\n    img = images_test[i].astype(\"float32\")\n    img = cv2.resize(img,(56,56))\n    img = cv2.filter2D(img , -1 , kernel)\n    img = cv2.blur(img , (3,3))\n    X_test[i] = img","e677d991":"plt.imshow(images_test[4])","8585daa8":"plt.imshow(X_test[4])","641ea7cb":"X_test = np.reshape(X_test , (-1,56,56,1))","374f609f":"f3.seek(8)\ndataFormat = data_types[magic[2]][1]\ndataSize = data_types[magic[2]][2]\ntest_label = np.asarray(st.unpack('>'+\"B\"*n,f3.read(n*1))).reshape((n,1))\n","2003888b":"X_train = np.reshape(X_train,(60000,56,56,-1))","50ca56b3":"y_test = enc.transform(test_label)","e3977a7a":"inp = Input(shape = (56,56,1))\nx = Conv2D(filters = 16 , kernel_size = (3,3),strides = (1,1),padding = \"valid\",kernel_initializer=glorot_uniform())(inp)\nx = Activation(\"relu\")(x)#54\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Conv2D(filters = 32, kernel_size=(4,4) , strides = (2,2), padding=\"valid\",kernel_initializer=glorot_uniform())(x)\nx = Activation(\"relu\")(x)#26\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding=\"valid\")(x)#13\nx = Conv2D(filters = 64, kernel_size = (4,4), strides = (1,1), padding=\"valid\", kernel_initializer=glorot_uniform())(x)\nx = Activation(\"relu\")(x)#10\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Conv2D(filters = 128 , kernel_size = (4,4) , strides = (2,2), padding=\"valid\",kernel_initializer=glorot_uniform())(x)\nx = Activation(\"relu\")(x)#4\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = GlobalAveragePooling2D()(x)\n#x = Flatten()(x)\nx = Dense(64)(x)\nx = Activation(\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(32)(x)\nx = Activation(\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(10)(x)\nout = Activation(\"softmax\")(x)\nmod = Model(inputs=inp,outputs=out)\n","39ac55c8":"mod.compile(loss = \"categorical_crossentropy\" , optimizer = \"adam\" , metrics=[\"accuracy\"])\nhistory = mod.fit(x=X_train,y=y_train,epochs=16,validation_split=0.15,shuffle=True)","be6c0d23":"mod.evaluate(X_test,y_test)","a4e2b470":"plt.plot(history.history['acc'])\nplt.plot(history.history[\"val_acc\"])\nplt.title(\"Model Accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","70f9ac40":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Loss\")\nplt.ylabel(\"Epochs\")\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","b6413b9c":"## Image Preprocessing","b9b926de":"## Image after Preprocessing","88f21972":"## Loading Training Datatset","ffbcc0cb":"### Provides a better outline of the object after applying the filter and resizing the image.","8efe291f":"## Plots","6cb2e028":"## Loading Testing Labels","e526adca":"## Loading Test Dataset","35e18e25":"## Image Preprocessing for test images","9bfd95af":"## Accuracy","24a628f0":"## Loading Training Labels","148027a0":"## Deep Learning Model","84b2a175":"## Creating One Hot Encoder for Training labels","f521235a":"## Test image after Preprocessing"}}