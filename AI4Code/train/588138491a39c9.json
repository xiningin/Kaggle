{"cell_type":{"16db1b5b":"code","0ee8ce99":"code","b9a8bcc8":"code","5ec3d40f":"code","e0c129f9":"code","e2735de5":"code","e35dc768":"code","52a4fe18":"code","cad06e60":"code","ca37ec70":"code","e4051d55":"code","6389e3c6":"code","6b28fe00":"code","ebf08dbb":"code","f97dd588":"code","3e5761bf":"code","324fe4ed":"code","faa49a7c":"code","14ea60b7":"code","7f6234ec":"code","55ff3500":"code","4f2b9846":"code","13b678c7":"code","4548479a":"code","2f632deb":"code","1bd55ac7":"code","d5da10b4":"code","c7545275":"code","1c021ac3":"code","d0a28c5b":"code","70eca759":"code","848ed143":"code","925c3523":"code","2d45b16a":"code","6b362692":"code","5ba8ba2d":"code","f8b4f746":"code","4e9fb7ac":"code","0640c177":"code","6fae1ac8":"code","a80dd854":"code","5e60b563":"code","5aadfc28":"code","edcf0358":"code","4d29bd5e":"code","6fb98199":"markdown","4c2a6085":"markdown","0681c64c":"markdown","cc8808f3":"markdown","db60560a":"markdown","6756d483":"markdown","1721629d":"markdown","ee45b250":"markdown","1eb2b7e1":"markdown","15308e84":"markdown","6b334133":"markdown","d7af940c":"markdown","890f0e9a":"markdown","97a791b6":"markdown","23cbf4b5":"markdown","d95f45cb":"markdown","d2461fd2":"markdown","8de0f28b":"markdown","06771f1f":"markdown","7669cd9c":"markdown","65b5a36e":"markdown","03fa56af":"markdown","7ae505ac":"markdown","d3e5cc52":"markdown","3186aa7f":"markdown","0559e08f":"markdown","29ed1f29":"markdown","1fe52ec9":"markdown","b2ffd5f9":"markdown","8da36b4e":"markdown","b2748f89":"markdown","37a9ea9e":"markdown","ffa39432":"markdown","fea5a425":"markdown","812af8b4":"markdown","07174c90":"markdown","a09c6f01":"markdown","b199900b":"markdown","6d94e081":"markdown","b3ef1e01":"markdown","31f98fc9":"markdown","9d61d61b":"markdown","196be1ab":"markdown","214d7e83":"markdown","d47c290f":"markdown","2df562ab":"markdown","a14d2815":"markdown","03aff3c6":"markdown","c1d07dd3":"markdown","6c471ae2":"markdown","1b720d48":"markdown"},"source":{"16db1b5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np  # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data Splitting Process\n\nfrom sklearn.model_selection import train_test_split\n\n# Training Process\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Performance Measures \n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0ee8ce99":"mnist_train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\nmnist_test  = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","b9a8bcc8":"\n#Take copies of the master dataframes\n\ntrain = mnist_train.copy()\ntest = mnist_test.copy()","5ec3d40f":"train.shape","e0c129f9":"test.shape","e2735de5":"train.head()","e35dc768":"train.tail()","52a4fe18":"test.head()","cad06e60":"test.tail()","ca37ec70":"train.describe()","e4051d55":"print(train.keys())","6389e3c6":"print(test.keys())","6b28fe00":"train.isnull().any().any()","ebf08dbb":"X, y = train.drop(labels = [\"label\"],axis = 1).to_numpy(), train[\"label\"]\nX.shape","f97dd588":"X.shape","3e5761bf":"y.shape","324fe4ed":"some_digit = X[20]\nsome_digit_show = plt.imshow(X[20].reshape(28,28), cmap=mpl.cm.binary)\ny[20]","faa49a7c":"y = y.astype(np.uint8)","14ea60b7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","7f6234ec":"y_train_8 = (y_train == 8)\ny_test_8 = (y_test == 8)","55ff3500":"sgd_clf = SGDClassifier(max_iter=1000,random_state = 42)\nsgd_clf.fit(X_train, y_train_8)","4f2b9846":"sgd_clf.predict([some_digit])","13b678c7":"rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_clf.fit(X_train, y_train_8)","4548479a":"rf_clf.predict([some_digit])","2f632deb":"cv_score_sgd = cross_val_score(sgd_clf, X_train, y_train_8, cv = 3, scoring = \"accuracy\")","1bd55ac7":"cv_score_sgd = np.mean(cv_score_sgd)\ncv_score_sgd","d5da10b4":"cv_score_rf = cross_val_score(rf_clf, X_train, y_train_8, cv= 3, scoring = \"accuracy\")","c7545275":"cv_score_rf = np.mean(cv_score_rf)\ncv_score_rf","1c021ac3":"class Never8Classifier(BaseEstimator):\n    def fit(sef, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n\nnever_8_clf = Never8Classifier()\n\ncross_val_score(never_8_clf, X_train, y_train_8, cv=3, scoring=\"accuracy\")","d0a28c5b":"y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_8, cv= 3)\n\n\nconfusion_matrix(y_train_8, y_train_pred)","70eca759":"precision_score(y_train_8, y_train_pred)","848ed143":"recall_score(y_train_8, y_train_pred)","925c3523":"Score = f1_score(y_train_8, y_train_pred)\nprint(Score)","2d45b16a":"y_scores= cross_val_predict(sgd_clf, X_train, y_train_8, cv=3, method=\"decision_function\")\nprint(y_scores)","6b362692":"precisions, recalls, thresholds = precision_recall_curve(y_train_8,y_scores)\n\n# here we use matplotlib to plot recall and precision as functions of the thresholds\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"center left\")\n    plt.ylim([0, 1])\n    plt.title('Precision and recall versus the decision threshold')\n\n    \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","5ba8ba2d":"y_scores = sgd_clf.decision_function([X_train[0]])\nprint(\"Score for 1st digit: {0}\".format(y_scores[0]))\nprint(\"Was this digit a real 8? {0}\".format(y_train_8[0]))\n\ndigit_image = X_train[0].reshape(28,28)\nplt.imshow(digit_image, cmap= matplotlib.cm.binary, interpolation=\"nearest\")\nplt.axis(\"off\")\nplt.title(\"Digit image\")\nplt.show()","f8b4f746":"threshold = -200000\ny_some_digit_pred = (y_scores > threshold)\ny_some_digit_pred","4e9fb7ac":"def print_recalls_precision(recalls, precisions, title):\n    plt.figure(figsize=(8,6))\n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.title(\"Precision vs Recall plot - {0}\".format(title), fontsize=16)\n    plt.axis([0,1,0,1])\n    plt.show()\nprint_recalls_precision(recalls, precisions, \"stochastic gradient descend\")","0640c177":"rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\ny_probas_forest = cross_val_predict(rf_clf, X_train, y_train_8, cv= 3, method= \"predict_proba\")\ny_scores_forest = y_probas_forest[:,1]\n\n# y_probas_forest contains 2 columns, one per class. Each row's sum of probabilities is equal to 1\n\nprecisions_forest, recalls_forest, thresholds = precision_recall_curve(y_train_8,y_scores_forest)\nprint_recalls_precision(recalls_forest, precisions_forest, \"Random Forest Classifier\")","6fae1ac8":"never_8_predictions = cross_val_predict(never_8_clf, X_train, y_train_8, cv=3)\n\nprecisions_dumb, recalls_dumb, thresholds = precision_recall_curve(y_train_8, never_8_predictions)\n\nprint_recalls_precision(recalls_dumb, precisions_dumb, \"dumb classifier\")","a80dd854":"plt.figure(figsize=(8,6))\nplt.plot(precisions_forest, recalls_forest, \"-r\", label=\"Random Forest\")\nplt.plot(precisions,recalls, \"-g\",label=\"stochastic gradient descend\")\nplt.plot(precisions_dumb, recalls_dumb, \"-b\", label=\"dumb classifier\")\nplt.plot([0, 1], [1,0], \"k--\", label=\"Random guess\")\n\nplt.xlabel(\"Recall\", fontsize=16)\nplt.ylabel(\"precision\", fontsize=16)\n\n\nplt.title(\"Precision vs Recall - model comparison\", fontsize=16)\nplt.axis([0,1,0,1])\nplt.legend(loc=\"center left\")\nplt.ylim([0, 1])","5e60b563":"print(\"F1 score for dumb classifier: {0}\".format(f1_score(y_train_8, never_8_predictions)))\nprint(\"F1 score for SGD classifier: {0}\".format(f1_score(y_train_8, y_train_pred)))\nprint(\"F1 score for Random Forest: {0}\".format(f1_score(y_train_8, y_scores_forest > 0.5)))","5aadfc28":"predictions_sgd = sgd_clf.predict(X_test).astype(int)","edcf0358":"Label = pd.Series(predictions_sgd,name = 'Label')\nImageId = pd.Series(range(1,28001),name = 'ImageId')\nsubmission = pd.concat([ImageId,Label],axis = 1)\nsubmission.to_csv('submission.csv',index = False)","4d29bd5e":"# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# clf.fit(X_train, y_train_8)\n\n\npredictions_forest = clf.predict(X_test).astype(int)\n\nLabel = pd.Series(predictions_forest,name = 'Label')\nImageId = pd.Series(range(1,28001),name = 'ImageId')\nsubmission = pd.concat([ImageId,Label],axis = 1)\nsubmission.to_csv('submission_forest.csv',index = False)","6fb98199":"## 9.3 Precision","4c2a6085":"The confusion matrix gives a good results but sometimes we might use another metric more concise like the accuracy of the positive predictions, this called PRECISION of the classifier,\n\nPrecision is the ratio of correctly predicted positive observations, to the total predicted positive observations.","0681c64c":"   \nThe goal of this notebook is to analyse a classification model with the MNIST data, so in this notebook, we will detect one number from MNIST data set using binary classifiers (A  classifier is an algorithm of machine learning that will determine the class to which the input data belongs to, based on a set of features). And then we will evaluate the measures of performance, and choose the model that have a great accuracy. ","cc8808f3":"1. Summary\n2. Import the relevant libraries\n3. Loading the MNIST data\n4. Exploratory data analysis\n5. Preprocess the data set\n\n   5.1 Cleaning the data set\n   \n   5.2 Separate Features and Labels\n   \n6. Plotting the data set\n7. Data Splitting Process\n8. Training \n\n   8.1 Training a binary classifier\n\n         # STOCHASTIQUE GRADIENT DESCENT\n         # RANDOM FOREST ALGORITHM\n         # Comparing with a dump classifier\n         \n9. Performance Measures\n\n   9.1 Cross Validation\n   \n   9.2  Confusion Matrix\n   \n   9.3  Precision \n   \n   9.4 Recall \n   \n   9.5  F1 \n   \n   9.6 Precision\/Recall Trade-off\n   \n10. The Test set","db60560a":"#### Stochastique Gradien Descent","6756d483":"#### Random Forest","1721629d":"#### Stochastique Gradien Descent","ee45b250":"#### dumb classifier","1eb2b7e1":"## 8.1 Training a binary classifier","15308e84":"Recall is the ratio of correctly predicted positive observations to the all observations in actual class, Recall is also called sensitivity or true positive rate (TPR).","6b334133":"# 4. Exploratory data analysis","d7af940c":"## 9.2 Confusion Matrix","890f0e9a":"# 9.1 Performance Measures","97a791b6":"Another way to select the best value of the threshold is to plot precision directly against recall ","23cbf4b5":"we can conclude that the random forest classifier performs better than the other classifiers","d95f45cb":"#### RANDOM FOREST ALGORITHM","d2461fd2":"In general 92% accuracy seems good but we need to create a dumb \"Never8Classifier\", by extending Scikit-Learn's BaseEstimator","8de0f28b":"# 3. Loading the MNIST data","06771f1f":"#### Random Forest","7669cd9c":"### 9.1 Cross Validation","65b5a36e":"We were just training our model to predict 8.","03fa56af":"#### Stochastique Gradien Descent","7ae505ac":"### 5.2 Separate Features and Labels","d3e5cc52":"# 2. Import the relevant libraries","3186aa7f":" feature X[20] contains '8' (image_pixel data) pixels 784 = 28*28\n y[20] contain 8 value","0559e08f":"so we can conclude that : some_digit X[20] == 8 is  True","29ed1f29":"A good way to measure the performance of a classifier is to look at the confusion matrix. The confusion matrix is the number of correct predictions and incorrect predictions are summarized with a count values and broke down by each class.\n\nTo calculate the confusion matrix we need a set of predictions, so that they can be compared to the actual targets.\n\nInstead, we can use the function of sklearn cross_val_predict().","1fe52ec9":"# 10. the test set","b2ffd5f9":"We notice that only 10% of the images are 8s, so if we guess that an image is not a 8 , we will be right about 90% of the time.","8da36b4e":"# 6. Plotting the data set","b2748f89":"## 9.4 Recall","37a9ea9e":"## 9.5 F1 Score","ffa39432":"To evaluate the performance of a classifier model we can use the cross validation, but the accuracy is generally not the preferred performance measure for classifiers especially when some classes are more frequent than others.","fea5a425":"## 7. 1 Spliting Train and Test sets","812af8b4":"# 7. Data Splitting Process","07174c90":"#### STOCHASTIQUE GRADIENT DESCENT","a09c6f01":"Now the 8-detector does not look as the results of the accuracy, so when it claims an image represents a 8, it is correct only for 62.5%. More over, it only detects 66.5% of the 8s","b199900b":"Each row in the confusion matrix is an actual class, and each column represents a predicted class.\n\nThe first row of this matrix : 32,684 were correctly considred as non-8s (True Negatives)\nThe second row : 1,226, were wrongly classified as non-8s(there are called False Negative)\nThe first column : 1,456 we wrongly classifies as 8s (False Positive)\nThe second column : 2,434 we correcltly classifioed as 8s(True Positive)","6d94e081":"the results means that the data is already clean, so we don't have any missing values","b3ef1e01":"After we build our machine learning algorithm, we need to evaluate the performance for both models(SGD and Random Forest), there are many performance measures, in this notebook we will use the cross validation, Confusion Matrix, Precision\/Recall\/F1 score and ROC curve, and then we will analyze which model performs better.","31f98fc9":"### 5.1 Cleaning the data set","9d61d61b":"# 5. Preprocess the data set","196be1ab":"So here we set thresold to a very low value -250000, ","214d7e83":"## 9.6 Precision\/Recall Trade-off","d47c290f":"# 8.Training Process","2df562ab":"we can plot the precision and recall ratio by using the decision score, because sklearn does not give us the access to set the threshold. So using decision_function() we can get score values and decide whether it should be classified as 8 or not 8.","a14d2815":"F1 score is precision and recall combined into single metric. It's the harmonic mean of precision and recall","03aff3c6":"#### Comparing with a dump classifier","c1d07dd3":"The graph results that the Random Forest Classifier performs clearly better than the SGD classifier.\n\nOtherwise we will plot the same graph for the dumb classifier, so that we can compare all the 3 classifiers; dumb classifier, Random Forest Classifier and SGD classifier.","6c471ae2":"# 1. Summary","1b720d48":"Let's use RandomForestClassifier and compare it with SGDClassifier"}}