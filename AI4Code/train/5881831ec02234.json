{"cell_type":{"c9d9272e":"code","9fa0b708":"code","17d34c99":"code","4fc6dc4f":"code","3644db9a":"code","30aabafc":"code","ad213a32":"code","80cd8238":"code","dc4a7019":"code","26618c47":"code","2428a289":"code","1f7dfc98":"code","a4ea837c":"code","23a82764":"code","240ae4ba":"code","e905c51d":"code","eb5bce72":"code","b696a893":"code","1b07cc54":"code","7b44a7b4":"code","e955788b":"code","2c4bb0ff":"code","84969c1c":"code","33dee379":"code","58187e3d":"code","09d74a9c":"code","05e23c5c":"code","50ce9130":"code","1054a333":"code","a1709ce1":"code","078ab530":"code","0f862725":"code","4edf60e3":"code","b7af74e8":"code","5547ab48":"code","f946d241":"code","a63833e2":"code","dc0ba717":"code","c31fe76d":"code","eb31291a":"code","3bf67a27":"markdown","01709618":"markdown","24437cc0":"markdown","ecd1a235":"markdown","23827de8":"markdown","a92c1aa0":"markdown","2e48a5ce":"markdown","00a03b69":"markdown"},"source":{"c9d9272e":"# Import packages\n# Basic packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pickle\nfrom math import floor\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# Evaluation and bayesian optimization\nfrom sklearn.metrics import make_scorer, mean_absolute_error\nfrom sklearn.metrics import mean_squared_error as MSE\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", None)","9fa0b708":"# Load dataset\ntrainSet = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv').drop(columns=['Id'])\ntestSet = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv').drop(columns=['Id'])\nsubmitSet = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\ntrainSet.head()","17d34c99":"# Drop columns with too much NA\ntrain = trainSet.drop(columns=['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=0)\n\n# Fill missing value with median or most occurance value\ntrain['LotFrontage'] = train['LotFrontage'].fillna(train['LotFrontage'].median())\ntrain['MasVnrType'] = train['MasVnrType'].fillna(train['MasVnrType'].value_counts().idxmax())\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(train['MasVnrArea'].median())\ntrain['MasVnrType'] = train['MasVnrType'].fillna(train['MasVnrType'].value_counts().idxmax())\ntrain['BsmtQual'] = train['BsmtQual'].fillna(train['BsmtQual'].value_counts().idxmax())\ntrain['BsmtCond'] = train['BsmtCond'].fillna(train['BsmtCond'].value_counts().idxmax())\ntrain['BsmtExposure'] = train['BsmtExposure'].fillna(train['BsmtExposure'].value_counts().idxmax())\ntrain['BsmtFinType1'] = train['BsmtFinType1'].fillna(train['BsmtFinType1'].value_counts().idxmax())\ntrain['BsmtFinType2'] = train['BsmtFinType2'].fillna(train['BsmtFinType2'].value_counts().idxmax())\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].value_counts().idxmax())\ntrain['GarageType'] = train['GarageType'].fillna(train['GarageType'].value_counts().idxmax())\ntrain['GarageYrBlt'] = train['GarageYrBlt'].fillna(train['GarageYrBlt'].median())\ntrain['GarageFinish'] = train['GarageFinish'].fillna(train['GarageFinish'].value_counts().idxmax())\ntrain['GarageQual'] = train['GarageQual'].fillna(train['GarageQual'].value_counts().idxmax())\ntrain['GarageCond'] = train['GarageCond'].fillna(train['GarageCond'].value_counts().idxmax())\n\ntrain.info()","4fc6dc4f":"# Encode categorical val\ntrain = pd.get_dummies(train)\ntrain.head()","3644db9a":"selected = ['BsmtQual_Gd', 'SaleType_New', 'Fireplaces', 'BsmtExposure_Gd', '2ndFlrSF',\n            'BsmtFinType1_GLQ', 'BsmtFinSF1', 'LowQualFinSF', 'GarageType_Detchd',\n            'KitchenAbvGr', 'MSZoning_RM', 'TotalBsmtSF', 'CentralAir_N', 'GrLivArea',\n            'KitchenQual_Ex', 'Exterior2nd_CmentBd', 'GarageCars', 'LandSlope_Gtl',\n            'BsmtQual_Ex', 'OverallQual', 'SalePrice']","30aabafc":"train_sel = train.loc[:,selected]\n\n# train validation split\nX_train, X_val, y_train, y_val = train_test_split(train_sel.drop(columns=['SalePrice']),\n                                                  train_sel['SalePrice'],\n                                                  test_size=0.2, random_state=123)","ad213a32":"import h2o\nfrom h2o.automl import H2OAutoML","80cd8238":"h2o.init()","dc4a7019":"# Prepare the data\nXy_train = X_train.reset_index(drop=True)\nXy_train['SalePrice'] = y_train.reset_index(drop=True)\n\nXy_val = X_val.reset_index(drop=True)\nXy_val['SalePrice'] = y_val.reset_index(drop=True)","26618c47":"# Convert H2O Frame\nXy_train_h2o = h2o.H2OFrame(Xy_train)\nX_val_h2o = h2o.H2OFrame(X_val)","2428a289":"# Create the model\nh2o_model = H2OAutoML(max_runtime_secs=120, seed=123)\n\n# Fit the model\nh2o_model.train(x=Xy_train_h2o.columns, y='SalePrice', training_frame=Xy_train_h2o)","1f7dfc98":"# Predict the training data\nh2o_pred = h2o_model.predict(X_val_h2o)","a4ea837c":"# Convert back H2ODataFrame to Pandas DataFrame\nh2o_pred_ = h2o.as_list(h2o_pred['predict'])\nh2o_pred_ = h2o_pred_['predict']\n\n# Scatter plot true and predicted values\nplt.scatter(h2o_pred_, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(MSE(y_val, h2o_pred_)**0.5,3)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(y_val, h2o_pred_),3)))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(y_val, h2o_pred_)[0,1],3)))\nplt.show()","23a82764":"# Show the model results\nleaderboard_h2o = h2o.automl.get_leaderboard(h2o_model, extra_columns = 'ALL')\nleaderboard_h2o","240ae4ba":"!pip install pycaret","e905c51d":"from pycaret.regression import *","eb5bce72":"# Generate\nval_index = np.random.choice(range(trainSet.shape[0]), round(trainSet.shape[0]*0.2), replace=False)\n\n# Split trainSet\ntrainSet1 = trainSet.drop(val_index)\ntrainSet2 = trainSet.iloc[val_index,:]","b696a893":"# Create the model\ncaret = setup(data = trainSet1, target='SalePrice', session_id=111,\n              numeric_imputation='mean',  categorical_imputation='constant',\n              normalize = True, combine_rare_levels = True, rare_level_threshold = 0.05,\n              remove_multicollinearity = True, multicollinearity_threshold = 0.95)","1b07cc54":"# Show the models\ncaret_models = compare_models(fold=5)","7b44a7b4":"# Create the top 5 models\nbr = create_model('br', fold=5)\nhuber = create_model('huber', fold=5)\nomp = create_model('omp', fold=5)\nridge = create_model('ridge', fold=5)\npar = create_model('par', fold=5)\n# If each algorithm is created in 1 cell, each output will show each cross-validation result.\n# Below is the cross_validation report of PAR as the it is the last line","e955788b":"# Tune the models\nbr_tune = tune_model(br, fold=5)","2c4bb0ff":"# Tune the models\nhuber_tune = tune_model(huber, fold=5)\nomp_tune = tune_model(omp, fold=5)\nridge_tune = tune_model(ridge, fold=5)\npar_tune = tune_model(par, fold=5)\n# If each algorithm is created in 1 cell, each output will show each cross-validation result.","84969c1c":"# Show the tuned hyperparameters, for example for BR\nplot_model(br_tune, plot='parameter')","33dee379":"# Bagging BR\nbr_bagging = ensemble_model(br_tune, fold=5)","58187e3d":"# Boosting BR\nbr_boost = ensemble_model(br_tune, method='Boosting', fold=5)","09d74a9c":"# Return top 5 models\ncaret_models_5 = compare_models(n_select=5)","05e23c5c":"# Stacking with GBR as the meta-model\nstack = stack_models(caret_models_5, meta_model=huber, fold=5)","50ce9130":"# Blending top models\ncaret_blend = blend_models(estimator_list=[br_tune,huber_tune,omp_tune,ridge_tune,par_tune])","1054a333":"caret_pred = predict_model(caret_blend, data = trainSet2.drop(columns=['SalePrice']))\ncaret_pred.head()","a1709ce1":"# Predict the validation data\ncaret_pred = predict_model(caret_blend, data = trainSet2.drop(columns=['SalePrice']))\ncaret_pred = caret_pred['Label']\n\n# Scatter plot true and predicted values\nplt.scatter(caret_pred, trainSet2['SalePrice'], alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(MSE(trainSet2['SalePrice'], caret_pred)**0.5)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(trainSet2['SalePrice'], caret_pred))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(trainSet2['SalePrice'], caret_pred)[0,1],4)))\nplt.show()","078ab530":"!pip install autoviml\n!pip install shap","0f862725":"from autoviml.Auto_ViML import Auto_ViML","4edf60e3":"# Create the model\nviml, features, train_v, test_v = Auto_ViML(trainSet1, 'SalePrice', trainSet2.drop(columns=['SalePrice']),\n                                            scoring_parameter='', hyper_param='RS',\n                                            feature_reduction=True, Boosting_Flag=True,\n                                            Binning_Flag=False,Add_Poly=0, Stacking_Flag=False, \n                                            Imbalanced_Flag=True, verbose=1)","b7af74e8":"viml\n# The model picks XGBRegressor","5547ab48":"# Scatter plot true and predicted values\nplt.scatter(test_v['SalePrice_predictions'], trainSet2['SalePrice'], alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(MSE(trainSet2['SalePrice'], test_v['SalePrice_predictions'])**0.5)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(trainSet2['SalePrice'], test_v['SalePrice_predictions']))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(trainSet2['SalePrice'], test_v['SalePrice_predictions'])[0,1],4)))\nplt.show()","f946d241":"!pip install -U https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/raw\/fix\/logging\/LightAutoML-0.2.16.2-py3-none-any.whl\n!pip install openpyxl","a63833e2":"from lightautoml.automl.presets.tabular_presets import TabularAutoML\nfrom lightautoml.tasks import Task","dc0ba717":"# Create the model\nlight = TabularAutoML(task=Task('reg',), timeout=60*3, cpu_limit=4)\n\ntrain_data = pd.concat([X_train, y_train], axis=1)\n# Fit the training data\ntrain_light = light.fit_predict(train_data, roles = {'target': 'SalePrice', 'drop':[]})\n\n# Predict the validation data\npred_light = light.predict(X_val)","c31fe76d":"# Convert the prediction result into dataframe\npred_light2 = pred_light.data\npred_light2 = pd.DataFrame(pred_light2, columns=['Pred'])\npred_light2 = pred_light2.Pred\npred_light2","eb31291a":"# Scatter plot true and predicted values\nplt.scatter(pred_light2, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(MSE(y_val, pred_light2)**0.5)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(y_val, pred_light2))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(y_val, pred_light2)[0,1], 4)))\nplt.show()","3bf67a27":"# \ud83d\udcda Automated Machine Learning-Regression\n\nThis notebook provides Automated Machine Learning (AutoML) algorithms for a regression task. Data preparation is just simply performed as the pre-processing will be automatically done, followed by building Machine Learning algorithms and tuning the hyperparameters. The objective of this notebook is to serve as a cheat sheet.","01709618":"Now, we can perform ensemble methods to combine more than one model. In this notebook, bagging, boosting, and stacking will be performed, but blending will be used for the final prediction model.","24437cc0":"# 7. H2O","ecd1a235":"Please find the part 1 here https:\/\/www.kaggle.com\/rendyk\/automl-regression-part1","23827de8":"# 8. PyCaret","a92c1aa0":"To find the process of feature selection, please visit this notebook https:\/\/www.kaggle.com\/rendyk\/regression-rmse-house-prices\n\nThat notebook demonstrates regression using conventional Machine Learning algorithms for learning the same dataset.","2e48a5ce":"# 9. AutoViML","00a03b69":"# 10. LightAutoML"}}