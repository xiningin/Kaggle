{"cell_type":{"d77ff060":"code","c9d18114":"code","7d90f971":"code","3a9624ce":"code","22cc601e":"code","77f195f2":"code","51a114a2":"code","5af30b85":"code","7fe149c2":"code","7b389fb1":"code","c61d8e47":"code","dcec87f5":"code","206fb115":"code","ebf21791":"code","793715ca":"code","ff16b725":"code","329d37da":"code","2a9e9f27":"code","488343de":"code","aea54046":"code","7a498c27":"code","584c2ecd":"code","b745344b":"code","391b25ca":"code","b788aff9":"code","83a217ba":"code","c437f618":"code","d6677b27":"code","f9d0fdcf":"code","6273af76":"code","f05546a4":"code","7517b03a":"code","1f27b295":"code","f351d8b3":"markdown","b99aeda6":"markdown","ade833a9":"markdown","e2f398a3":"markdown","0a5efee0":"markdown","60e5cd2a":"markdown","86dc2e94":"markdown"},"source":{"d77ff060":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9d18114":"df = pd.read_csv(\"\/kaggle\/input\/sales-analysis\/SalesKaggle3.csv\")\nprint (df.shape)\nprint (df.columns)\ndf.head()","7d90f971":"df.info()","3a9624ce":"df.describe().T","22cc601e":"## check null percentages\n100*df.isna().sum()\/df.shape[0]","77f195f2":"## set the dtypes properly\nstr_fields = ['Order', 'SKU_number', 'ReleaseYear']\nfor i in str_fields :\n    df[i] = df[i].astype(str)\n\n## divide the fields into categorical and numerical\ncateg_cols = ['ReleaseYear', 'File_Type', 'MarketingType', 'ReleaseNumber', 'New_Release_Flag','SoldFlag']\nnum_cols = ['SoldCount', 'ReleaseNumber', 'StrengthFactor', 'PriceReg', 'ItemCount', 'LowUserPrice', 'LowNetPrice']\n\n# ReleaseNumber seems ambiguous to be considered as a numerical field as it seems to be a ordinal categorical field.","51a114a2":"df['Order'].unique().shape, df['SKU_number'].unique().shape","5af30b85":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n## plot bar charts for the categorical fields\nfor i in categ_cols :\n    df[i].value_counts(normalize=True).plot(kind='bar',figsize=(13,4))\n    plt.title(i)\n    plt.show()","7fe149c2":"## plot distribution and box plots on numerical fields on the entire data\nfor i in num_cols : \n    _, ax = plt.subplots(1,2,figsize=(13,4))\n    df[i].plot(kind='hist',ax=ax[0])\n    df[i].plot(kind='box',ax=ax[1])\n    ax[0].set_title(i + \"- histogram\")\n    ax[1].set_title(i + \"- boxplot\")\n    plt.show()","7b389fb1":"import seaborn as sns\n\n## plot the distributions category wise based on the target.\ntarget = 'SoldFlag'\nfor i in categ_cols : \n    if i != target :\n        temp = df.value_counts([i,target],normalize=True).reset_index()\n        plt.figure(figsize=(13,4))\n        sns.barplot(data=temp, x=i,y=0, hue=target)\n        plt.xticks(rotation=45)\n        plt.show()","c61d8e47":"print (df[df['File_Type']=='Historical']['SoldFlag'].unique())\nprint (df[df['File_Type']=='Active']['SoldFlag'].unique())\nprint (df[df['File_Type']=='Historical']['SoldCount'].unique())\nprint (df[df['File_Type']=='Active']['SoldCount'].unique())\nprint (df['File_Type'].value_counts())\ncommon_prods = set(df[df['File_Type']=='Active']['SKU_number'].unique()).intersection(set(df[df['File_Type']=='Historical']['SKU_number'].unique()))\nprint (len(common_prods))","dcec87f5":"## consider only Historical file_type and plot the charts again\ntrain = df[df['File_Type']=='Historical'].copy()\nprint (train.shape)\nfor i in categ_cols : \n    if i != target :\n        temp = train.value_counts([i,target],normalize=True).reset_index()\n        plt.figure(figsize=(13,4))\n        sns.barplot(data=temp, x=i,y=0, hue=target)\n        plt.xticks(rotation=45)\n        plt.show()","206fb115":"## plot distribution and box plots on numerical fields on the train data\nfor i in num_cols : \n    _, ax = plt.subplots(1,2,figsize=(13,4))\n    train[i].plot(kind='hist',ax=ax[0])\n    train[i].plot(kind='box',ax=ax[1])\n    ax[0].set_title(i + \"- histogram\")\n    ax[1].set_title(i + \"- boxplot\")\n    plt.show()","ebf21791":"## set constraints on the categorical data\nrelease_years = list(map(str, range(1999,2015)))\ntrain['ReleaseYear'] = train['ReleaseYear'].apply(lambda x: x if x in release_years else 'Other') ## bucket the low-dist into Other\ntrain['ReleaseNumber'] = train['ReleaseNumber'].apply(lambda x:x if x >=0 and x<=14 else 15) ## Cap the field after 15","793715ca":"## set constraints on the numerical data (iteratively choose the ranges)\ntrain = train[train['ItemCount']<800]\ntrain = train[train['LowUserPrice']<6000]\ntrain = train[train['LowNetPrice']<6000]\ntrain = train[train['PriceReg']<2500]\ntrain = train[train['SoldCount']<40]\ntrain.shape","ff16b725":"## Strenght-factor seems to take very large values and so divide by 1e6\ntrain['StrengthFactor'] = train['StrengthFactor']\/(10**6)\ntrain['StrengthFactor'].plot(kind='hist')","329d37da":"## most of the data seems to be within SF of 12 and so remove the outliers\ntrain = train[train['StrengthFactor']<=12]\ntrain.shape","2a9e9f27":"print (train.columns)\ntrain.describe()","488343de":"## plot numerical distributions w.r.t target col\nfor i in num_cols :\n    plt.figure(figsize=(8,4))\n    sns.boxplot(data=train, x=target, y=i)\n    plt.xticks(rotation=45)\n    plt.show()","aea54046":"train = train[train['LowNetPrice']<2000]\ntrain = train[train['LowUserPrice']<2000]\nprint(train.shape)\ntrain.columns","7a498c27":"train['MarketingType'].value_counts()","584c2ecd":"train['MarketingType'] = train['MarketingType'].apply(lambda x:0 if x=='S' else 1)","b745344b":"# Creating a dummy variable for the variable 'ReleaseYear' and dropping the first one.\ntemp = pd.get_dummies(train['ReleaseYear'],prefix='Year',drop_first=True)\n#Adding the results to the master dataframe\ntrain = pd.concat([train,temp],axis=1)\ntrain.shape","391b25ca":"train.columns","b788aff9":"## define cols used for training the model. \n## Note: In this case, SoldFlag and SoldCount are dependant and so based on the underlying problem, either of them can be dropped and \n## the other can be considered as a target. (SoldFlag --> Classification problem, SoldCount --> Regression problem)\ntarget = 'SoldFlag'\ncols = ['LowNetPrice', 'LowUserPrice', 'ItemCount', 'PriceReg','StrengthFactor','New_Release_Flag', 'MarketingType']\nyear_cols = [\"Year_\"+str(i) for i in range(2000,2015)]+['Year_Other']\ncols = cols + year_cols","83a217ba":"X = train[cols].copy()\ny = train[target].copy()\nX.shape, y.shape","c437f618":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8,random_state=36)\nX_train.shape, X_test.shape","d6677b27":"import xgboost as xgb\n\nmodel = xgb.XGBClassifier(objective='binary:logistic')\nmodel.fit(X_train,y_train,verbose=True)","f9d0fdcf":"y_train_pred = model.predict_proba(X_train)[:,1]\ny_test_pred = model.predict_proba(X_test)[:,1]\n\n_, ax = plt.subplots(1,4,figsize=(13,4))\nax[0].hist(y_train)\nax[1].hist(y_train_pred)\nax[2].hist(y_test)\nax[3].hist(y_test_pred)\nax[0].set_title(\"y_train\")\nax[1].set_title(\"y_train_pred\")\nax[2].set_title(\"y_test\")\nax[3].set_title(\"y_test_pred\")\nplt.show()","6273af76":"from sklearn.metrics import plot_roc_curve, confusion_matrix\n\n\nplot_roc_curve(model, X_train, y_train)\nplt.title(\"Train ROC\")\nplt.show()\nplot_roc_curve(model, X_test, y_test)\nplt.title(\"Test ROC\")\nplt.show()","f05546a4":"# Let's create columns with 10 different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)] ## define a list of thresholds\ny_train_pred_df = pd.DataFrame({'true':y_train, 'pred':y_train_pred})\n\n## classify based on each set threshold\nfor i in numbers:\n    y_train_pred_df[i]= y_train_pred_df.pred.map(lambda x: 1 if x > i else 0)\n    \n# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensitivity','specificity'])\n# compute the parameters for each threshold considered\nfor i in numbers:\n    cm1 = confusion_matrix( y_train_pred_df.true, y_train_pred_df[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] = [ i ,accuracy,sensi,speci]\n    \n# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensitivity','specificity'])\nplt.title('Comparison of performance across various thresholds')\nplt.show()","7517b03a":"threhsold = 0.2\ny_train_pred_class = [1 if i >= threhsold else 0 for i in y_train_pred]\ny_test_pred_class = [1 if i >= threhsold else 0 for i in y_test_pred]","1f27b295":"print (\"Confusion matrix - Train predictions : \")\nprint (confusion_matrix(y_train, y_train_pred_class))\nprint (\"Confusion matrix - Test predictions : \")\nprint (confusion_matrix(y_test, y_test_pred_class))","f351d8b3":"All nan values in the SoldFlag and SoldCount seem to be coming from Active inventories. It makes sense as the active inventories still have products and perhaps, it can be treated as a test set. However, the size of Active one is almost twice as that of Historical one.\n","b99aeda6":"Objective - Determine which products should continue to be sold and which products need to be removed from the inventory.\n\nData - Set of products(each row for one product) + consists of historical and active inventory + a score\/value for each product as a determinant evaluating the inventory + only 10% of the products sell per year\n\n- 'Order' -> Just a sequential counter. Can be ignored.\n- 'File_Type' -> historical(sales for past 6 months)\/active identifier\n- 'SKU_number' -> unique identifier for each product\n- 'SoldFlag' -> binary target 1=sale, 0=no sale in past 6 months. + likely target\n- 'SoldCount',\n- 'MarketingType' -> Two categories of how the product is marketed. This should probably be ignored, or better yet, each type should be considered independently.\n\n- 'ReleaseNumber' ,\n- 'New_Release_Flag' -> Any product that has had a future release (i.e., Release Number > 1)\n- 'StrengthFactor',\n- 'PriceReg',\n- 'ReleaseYear', \n- 'ItemCount', \n- 'LowUserPrice',\n- 'LowNetPrice'","ade833a9":"Based on the above plot, threhsold of 0.2 can be considered to be the optimal one where the sensitivity and the specificity of the model are balanced.","e2f398a3":"Observations :\n- Recent years(21st century) seem to have more products released.\n- ~40% of the product sales are from active inventory.\n- Both marekting types are almost equally spanned across the dataset.\n- Most of the products have their release numbers < 20.\n- ~35% of the products had new releases.\n- Only ~20% of the products got sold in the past 6 months.\n","0a5efee0":"## Determine Optimal Threshold for Binary Classification\n","60e5cd2a":"- Positive - Product being sold in 6 months.\n- Negative - Product being not sold in months.\n\n- More False-positives : Model predicting large numeber of non-selling products as selling products.\n- More False-negatives : Model predicting large number of selling products as non-selling products.\n\nThe False positives of the baseline model are high but on the other hand, the False negatives are comparatively low. The precision of this model may not be high(somewhere close to 50%) but the recall is good. Also, from the business perspective, the tradeoff seems valid as having more false-negatives might suggest a loss in the income.","86dc2e94":"Most of the data seems to be right\/+ve skewed and also most of them have outliers."}}