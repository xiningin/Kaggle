{"cell_type":{"0a9aa32b":"code","973ebcd1":"code","da2d8915":"code","79472e5c":"code","2369474b":"code","56580c95":"code","255ea199":"code","fcf231e6":"code","dfeeb5fb":"code","401b99ce":"code","61c81171":"code","925c847e":"code","f7794723":"code","68b649ee":"code","beecd369":"code","0b13b640":"code","fe508439":"code","c84065a7":"code","4e57e659":"code","eb78739c":"code","e278c9de":"markdown","5c07dbc1":"markdown","d6505ce0":"markdown","4d0b95ff":"markdown","85ec6a2b":"markdown","59ab520e":"markdown","60218171":"markdown","effe880f":"markdown","28ae94c4":"markdown","7c27ea30":"markdown","a4c3535a":"markdown","6ba7c0a7":"markdown","9fc2db0f":"markdown","6bbe0c17":"markdown","063d08a9":"markdown"},"source":{"0a9aa32b":"# For better confusion matrices\n!pip install daze","973ebcd1":"import os, sklearn, torch, optuna, daze\nimport numpy as np, pandas as pd, wandb as wb, matplotlib.pyplot as plt, seaborn as sns\n\nfrom torch import nn\nfrom optuna.integration.wandb import WeightsAndBiasesCallback\nfrom early_stopping import EarlyStopping","da2d8915":"# Set seed for reproducible randomness\nseed = 0\nrandom_state = np.random.RandomState(seed)\n\n# Use GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","79472e5c":"# Hide warnings from experimental 'Lazy' PyTorch modules\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Disable W&B logging\nos.environ['WANDB_SILENT'] = 'true'","2369474b":"# All of the possible genres\nclasses = ('blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock')\nn_classes = len(classes)\n\n# Fit the encoder for the genre labels\nlabel_encoder = sklearn.preprocessing.LabelEncoder()\nlabel_encoder.fit(classes)","56580c95":"class GTZAN(torch.utils.data.Dataset):\n    \"\"\"Fetches data from the preprocessed GTZAN dataset.\"\"\"\n    def __init__(self, split, encoder):\n        data_path = self.path(split, 'data')\n        self.data_files = [os.path.join(data_path, item) for item in os.listdir(data_path)]\n        self.data_files.sort(key=self.get_id)\n        self.labels = pd.read_csv(self.path(split, 'labels.csv'), index_col=0, squeeze=True)\n        self.encoder = encoder\n        \n    def get_id(self, file):\n        return int(os.path.splitext(os.path.basename(file))[0])\n        \n    def path(self, *sub):\n        return os.path.join('\/kaggle\/input\/gtzan-genre-classification-preprocessing-1-2', *sub)\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, index):\n        file = self.data_files[index]\n        \n        # Fetch the preprocessed input\n        x = torch.load(file)\n        \n        # Fetch the audio label and encode it\n        label = self.labels.iloc[index]\n        y = self.encoder.transform([label]).item()\n        \n        return x, y","255ea199":"# No. Features\nn_features = 12\n\n# Create Dataset objects for each split\ntrain_set = GTZAN('train', label_encoder)\nval_set = GTZAN('val', label_encoder)\ntest_set = GTZAN('test', label_encoder)","fcf231e6":"class ConvolutionalBlock(nn.Module):\n    def __init__(self, n_channels, channel_widths):\n        super().__init__()\n        \n        self.model = nn.ModuleDict({\n            'conv1': nn.Sequential(\n                nn.LazyConv2d(n_channels[0], kernel_size=(8, channel_widths[0]), stride=(3, 4), padding=(11, 4)),\n                nn.BatchNorm2d(n_channels[0]),\n                nn.ReLU(),\n            ),\n            'conv2': nn.Sequential(\n                nn.LazyConv2d(n_channels[1], kernel_size=(6, channel_widths[1]), stride=(2, 3), padding=(4, 3)),\n                nn.BatchNorm2d(n_channels[1]),\n                nn.ReLU(),\n            ),\n            'conv3': nn.Sequential(\n                nn.LazyConv2d(n_channels[2], kernel_size=(4, channel_widths[2]), stride=(1, 2), padding=(0, 2)),\n                nn.BatchNorm2d(n_channels[2]),\n                nn.ReLU(),\n            ),\n            'conv4': nn.Sequential(\n                nn.LazyConv2d(n_channels[3], kernel_size=(3, channel_widths[3]), stride=(1, 1), padding=(0, 1)),\n                nn.BatchNorm2d(n_channels[3]),\n                nn.ReLU(),\n            ),\n            'flatten': nn.Flatten()\n        })\n        \n        self.show_shapes = False\n        \n    def forward(self, x):\n        # Convolutional layers\n        for i in range(1, 5):\n            x = self.model[f'conv{i}'](x)\n            if self.show_shapes:\n                print(f'conv{i}: {x.shape}')\n            \n        # Flattened output\n        return self.model['flatten'](x)","dfeeb5fb":"class ClassificationBlock(nn.Module):\n    def __init__(self, n_classes, n_linear, dropout):\n        super().__init__()\n        \n        self.model = nn.ModuleDict({\n            'fc1': nn.Sequential(\n                nn.LazyLinear(n_linear[0]),\n                nn.ReLU(),\n                nn.Dropout(dropout[0])\n            ),\n            'fc2': nn.Sequential(\n                nn.LazyLinear(n_linear[1]),\n                nn.ReLU(),\n                nn.Dropout(dropout[1])\n            ),\n            'fc3': nn.Sequential(\n                nn.LazyLinear(n_linear[2]),\n                nn.ReLU(),\n                nn.Dropout(dropout[2])\n            ),\n            'softmax': nn.Sequential(\n                nn.LazyLinear(n_classes),\n                nn.LogSoftmax(dim=1)\n            )\n        })\n        \n    def forward(self, x):\n        # Fully-connected layers\n        for i in range(1, 4):\n            x = self.model[f'fc{i}'](x)\n            \n        # Log softmax class outputs\n        return self.model['softmax'](x)","401b99ce":"class GTZANCNN(nn.Module):\n    def __init__(self, n_classes, n_channels, channel_widths, n_linear, dropout):\n        super().__init__()\n        \n        self.model = nn.ModuleDict({\n            'conv_block': ConvolutionalBlock(n_channels, channel_widths),\n            'clf_block': ClassificationBlock(n_classes, n_linear, dropout)\n        })\n        \n    def forward(self, x):\n        x = self.model['conv_block'](x)\n        return self.model['clf_block'](x)","61c81171":"def train(model, optimizer, criterion, train_gen, val_gen):\n    n_epochs = 100\n    \n    # Average training\/validation losses over batches per epoch\n    avg_train_losses, avg_val_losses = [], []\n    \n    # Training\/validation accuracy over epochs\n    train_accuracies, val_accuracies = [], []\n    \n    # Initialize early stopping\n    early_stopping = EarlyStopping(patience=10, verbose=False)\n    \n    for epoch in range(n_epochs):\n        # Training loop\n        model.train()\n        train_correct, train_losses = [], []\n        for i, (batch, labels) in enumerate(train_gen):\n            # Send data to GPU\n            batch, labels = batch.to(device), labels.to(device)\n            # Reset the optimizer\n            optimizer.zero_grad()\n            # Calculate predictions for batch\n            log_prob = model(batch)\n            y_pred = torch.argmax(log_prob, dim=1)\n            # Calculate and back-propagate loss\n            train_loss = criterion(log_prob, labels)\n            train_losses.append(train_loss.item())\n            train_loss.backward()\n            # Store label comparisons for accuracy\n            train_correct.append(labels == y_pred)\n            # Update the optimizer\n            optimizer.step()\n            \n        # Store average training loss and accuracy for epoch\n        avg_train_losses.append(torch.Tensor(train_losses).mean().item())\n        train_accuracies.append(torch.cat(train_correct).float().mean().item())\n\n        # Validation loop\n        model.eval()\n        val_correct, val_losses = [], []\n        with torch.no_grad():\n            for i, (batch, labels) in enumerate(val_gen):\n                # Send data to GPU\n                batch, labels = batch.to(device), labels.to(device)\n                # Calculate predictions for batch\n                log_prob = model(batch)\n                y_pred = torch.argmax(log_prob, dim=1)\n                # Calculate loss\n                val_loss = criterion(log_prob, labels)\n                val_losses.append(val_loss.item())\n                # Store label comparisons for accuracy\n                val_correct.append(labels == y_pred)\n                \n        # Store average validation loss and accuracy for epoch\n        avg_val_losses.append(torch.Tensor(val_losses).mean().item())\n        val_accuracies.append(torch.cat(val_correct).float().mean().item())\n        \n        # Update the early stopping object with average validation loss\n        early_stopping(avg_val_losses[epoch], model)\n        if early_stopping.early_stop:\n            break\n        \n    return avg_train_losses, avg_val_losses, train_accuracies, val_accuracies, epoch + 1","925c847e":"def objective(trial):\n    # Suggest output channel depths\n    n_channels = (\n        trial.suggest_categorical('conv1_depth', (32, 64)),\n        trial.suggest_categorical('conv2_depth', (64, 128)),\n        trial.suggest_categorical('conv3_depth', (128, 256)),\n        trial.suggest_categorical('conv4_depth', (256, 512)),\n    )\n    \n    # Suggest channel widths\n    channel_widths = (\n        trial.suggest_categorical('conv1_width', (16, 32)),\n        trial.suggest_categorical('conv2_width', (8, 16)),\n        trial.suggest_categorical('conv3_width', (4, 8)),\n        trial.suggest_categorical('conv4_width', (2, 4)),\n    )\n    \n    # Suggest fully-connected units\n    n_linear = (\n        trial.suggest_categorical('fc1', (256, 512)),\n        trial.suggest_categorical('fc2', (128, 256)),\n        trial.suggest_categorical('fc3', (64, 128))\n    )\n    \n    # Suggest dropout probability\n    dropout = (\n        trial.suggest_uniform('p1', 0.1, 0.8),\n        trial.suggest_uniform('p2', 0.1, 0.8),\n        trial.suggest_uniform('p3', 0.1, 0.8),\n    )\n    \n    # Suggest a learning rate and weight decay for the optimizer\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-7, 1e-3)\n    weight_decay = trial.suggest_loguniform('weight_decay', 1e-7, 1e-3)\n    \n    # Initialize the model and send it to GPU\n    model = GTZANCNN(n_classes, n_channels, channel_widths, n_linear, dropout).to(device)\n    trial.set_user_attr(key='model', value=model)\n    \n    # Set Adam optimizer and negative log-likelihood loss function\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    criterion = nn.NLLLoss()\n    \n    # Create training, validation and test set batch iterators\n    train_gen = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n    val_gen = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True, num_workers=2)\n\n    # Train model and evaluate validation NLL\n    avg_train_losses, avg_val_losses, train_accuracies, val_accuracies, epochs = train(model, optimizer, criterion, train_gen, val_gen)\n    \n    # Record training\/validation losses and accuracies for this trial\n    trial.set_user_attr(key='train_losses', value=avg_train_losses)\n    trial.set_user_attr(key='val_losses', value=avg_val_losses)\n    trial.set_user_attr(key='train_accuracies', value=train_accuracies)\n    trial.set_user_attr(key='val_accuracies', value=val_accuracies)\n    trial.set_user_attr(key='epochs', value=epochs)\n\n    # Return the average validation loss over the batches of the last epoch\n    return avg_val_losses[-1]","f7794723":"# Create a new Optuna study for hyper-parameter optimization\nsampler = optuna.samplers.TPESampler(seed=seed)\nstudy = optuna.create_study(direction='minimize', sampler=sampler)\n\n# W&B integration - Initializes a new job for keeping track of hyper-parameter optimization\nwb_callback = WeightsAndBiasesCallback(metric_name='val_loss', wandb_kwargs={'project': 'gtzan-cnn', 'name': 'final-search'})\n\n# Callback to save the model that had the best Optuna trial\ndef model_callback(study, trial):\n    if study.best_trial.number == trial.number:\n        study.set_user_attr(key='best_model', value=trial.user_attrs['model'])\n\n# Run the hyper-parameter search\nstudy.optimize(objective, n_trials=25, show_progress_bar=True, callbacks=[wb_callback, model_callback])\nwb.finish()","68b649ee":"# Plot the trials on a single plot to see interaction with loss\nfig = optuna.visualization.plot_parallel_coordinate(study)\nfig.show()","beecd369":"# Fetch the best trial and model\ntrial = study.best_trial\nmodel = study.user_attrs['best_model']\n\n# Display model configuration\nprint(f'NLL (Val): {trial.value}')\nprint('Configuration: ')\nfor key, value in trial.params.items():\n    print(f'\\t{key}: {value}')","0b13b640":"# Show model summary\nmodel","fe508439":"fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n\n# Show training\/validation loss curves\nsns.lineplot(data=pd.DataFrame({\n    'Training': trial.user_attrs['train_losses'], \n    'Validation': trial.user_attrs['val_losses']\n}), ax=axs[0])\naxs[0].set(title='Loss (Negative Log-Likelihood)', xlabel='Epoch', ylabel='Loss')\naxs[0].xaxis.get_major_locator().set_params(integer=True)\n\n# Show training\/validation accuracy curves\naxs[1] = sns.lineplot(data=pd.DataFrame({\n    'Training': trial.user_attrs['train_accuracies'], \n    'Validation': trial.user_attrs['val_accuracies']\n}))\naxs[1].set(title='Accuracy', xlabel='Epoch', ylabel='Accuracy')\naxs[1].xaxis.get_major_locator().set_params(integer=True)\n\nplt.show()","c84065a7":"# Create the summary run for writing loss and accuracy curves (for the best model) to W&B\nwith wb.init(project='gtzan-cnn', name='final-summary', job_type='logging') as summary:\n    for i in range(trial.user_attrs['epochs']):\n        summary.log({\n            'final_train_loss': trial.user_attrs['train_losses'][i],\n            'final_val_loss': trial.user_attrs['val_losses'][i],\n            'final_train_accuracy': trial.user_attrs['train_accuracies'][i],\n            'final_val_accuracy': trial.user_attrs['val_accuracies'][i]\n        }, step=i)","4e57e659":"# Toggle evaluation mode\nmodel.eval()\n\n# Create test set batch iterator\ntest_gen = torch.utils.data.DataLoader(test_set, batch_size=len(test_set), num_workers=2)\n\n# Retrieve test set as a single batch and send to GPU\nbatch, labels = next(iter(test_gen))\nbatch, labels = batch.to(device), labels.to(device)\n\n# Calculate predictions for test set\ny = model(batch)\ny_pred = torch.argmax(y, dim=1)\n\n# Calculate accuracy\ntorch.mean((labels == y_pred).float())","eb78739c":"# Plot the test confusion matrix with per-class precision and recall values\nfig, ax = plt.subplots(figsize=(11, 11))\ncm = sklearn.metrics.confusion_matrix(labels.cpu().numpy(), y_pred.cpu().numpy())\ndaze.plot_confusion_matrix(cm, measures=('a', 'c', 'p', 'r'), display_labels=classes, measures_format='.2f', ax=ax)\nplt.show()","e278c9de":"## Fetching the preprocessed audio data\n\nIn [the first notebook](https:\/\/www.kaggle.com\/eonuonga\/gtzan-genre-classification-preprocessing-1-2), we preprocessed the audio recordings into a collection of MFCCs, chromagrams and their respective \u2206 and \u2206\u2206 features. \n\nThe following class simply makes it possible to fetch these preprocessed features along with their labels, for the training, validation and test set splits.","5c07dbc1":"# GTZAN Genre Classification \u2013 CNN (2\/2)\n\n**This notebook is the second of a two-part set of notebooks that investigates how convolutional neural networks (CNNs) can be applied to the task of music genre classification, using the GTZAN dataset which is [available on Kaggle](https:\/\/www.kaggle.com\/andradaolteanu\/gtzan-dataset-music-genre-classification).**\n\nThe [Weights & Biases reports page](https:\/\/wandb.ai\/eonu\/gtzan-cnn\/reportlist) for this project is a useful way to visualize results. You can also see into more detail on the [main W&B project page](https:\/\/wandb.ai\/eonu\/gtzan-cnn), such as information about the use of computational resources. Runs prefixed with `prep` refer to preprocessing experiments, whereas `final` is the final model that was trained. `search` runs are for hyper-parameter optimization, and `summary` runs report the results of the best hyper-parameter configurations.\n\n**See part 1 here (audio preprocessing) [>>>](https:\/\/www.kaggle.com\/eonuonga\/gtzan-genre-classification-preprocessing-1-2)**\n\n---\n\n## Package imports and configuration","d6505ce0":"## Creating a label encoder\n\nWe use the [`sklearn.preprocessing.LabelEncoder`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html) to encode the 10 genre names into integers from 0\u20139.","4d0b95ff":"## Training and hyper-parameter optimization \n\nNext, we define a typical training and validation loop within a function. To compare results, we keep track of the average training and validation loss over the batches of each epoch, as well as accuracies.\n\nIt is important to note that we are using an `EarlyStopping` callback (defined as a utility script), which stops the training loop if the validation loss hasn't improved for 10 epochs.","85ec6a2b":"## Conclusion\n\nOur final model achieved a test accuracy of **69%**.\n\nThis is a decent result, but there are also many other notebooks achieving much higher scores, and I highly recommend you to [check them out](https:\/\/www.kaggle.com\/andradaolteanu\/gtzan-dataset-music-genre-classification\/code) and see what alternative approaches have been used on this task.\n\nFrom the confusion matrix, we can see that some genres tend to be easier to classify than others \u2013 for example classical music with 100% precision and recall. However, others such as blues, metal and hiphop are more challenging to classify with our model, as evident by the low precision for these genres.\n\n### Improvements\n\n- Perhaps using MFCCs, chromagrams and their \u2206s and \u2206\u2206s is too many features. This may be one of the main contributors to overfitting, so it would be interesting to see if using fewer features would resut in better performance.\n- Ideally, $K$-fold cross validation should be used for hyper-parameter optimization rather than using a single held-out validation set \u2013 even moreso because we only have 1,000 recordings, which is relatively small in machine learning dataset terms.\n- As mentioned in the first notebook, there are many more audio representations and features that can be explored. Focusing more on music-specific transformations such as zero-crossing rate and spectral centroids is very likely to improve performance on this dataset.\n- While CNNs are becoming increasingly popular for learning from spectrogram-like audio representations, it could be interesting to look at RNNs and see if their ability to model sequences is more beneficial than the spacial information learned by a CNN. Or even more excitingly, one could look at feeding the output of a CNN into an RNN.","59ab520e":"### Classification block\n\nTypically, the output of a CNN is a flattened vector that is then passed through a series of fully-connected layers. The purpose of this is to continue to extract information from the output of the CNN, but more importantly, reduce the very large CNN output (gradually) to a softmax layer with just 10 units.\n\nAnother key use of the fully-connected layers is to apply dropout at this stage of the network. Dropout is usually avoided between convolutional layers, since filters usually have quite a small number of weights anyway, but also due to correlation issues resulting from overlapping filter receptive fields when lower strides are used.\n\nWe use three fully-connected layers of decreasing size, each with a ReLU activation function and an associated dropout probability which is treated as a hyper-parameter to be optimized.\n\nThe output of the final fully-connected layer passes through a softmax layer which generates the model's predictions \u2013 a $\\log$-softmax score for each genre.","60218171":"### Convolutional block\n\nWe will use four convolutional layers, with three aims:\n\n#### Reducing the output height as we move through the layers\n\nThis allows us to extract the knowledge from the 12 features of each channel, and summarize them in a smaller representation.\n\n**We fix the filter heights of these layers to be 8, 6, 4 and 3, and use appropriate vertical padding and decreasing stride to reduce a 12 feature input into a single feature.**\n\n#### Reducing the output width as we move through the layers\n\nNoting that we have quite a large input width of $T=1290$ frames, we would also like to try summarize this information in a more compact way.\n\nWe will start with wider filters and slowly decrease their width, so that we don't require too many operations to cover the entire input at the start.\n\n**We treat the filter widths as hyper-parameters to be optimized, but also use fixed values for stride and horizontal padding.**\n\n#### Increasing the number of channels as we move through the layers\n\nSimply reducing the number of features from 12 to 1 (as described above) would result in too much of a loss of information. In order to extract information from those features as we reduce their dimensions, we will increase the depth of each filter at the same time as reducing their height as we move through the network.\n\n**We treat the filter depths as hyper-parameters to be optimized.**\n\n---\n\nAfter each convolution, we perform batch normalization and pass the normalized output through the ReLU activation function.\n\nThe output of the final convolutional layer will be of size $B \\times C_L \\times W_L$, where $C_L$ is the number of channels in the last filter, and $W_L$ is the width of the output from the last layer. \n\nBefore we pass this to the classification block, we flatten this into a batch of vectors with size $B \\times (W_L \\times C_L)$.","effe880f":"Finally, let's evaluate the performance of the best model on the test set, to obtain test accuracy and a confusion matrix.","28ae94c4":"### Defining an objective function for hyper-parameter optimization\n\nWe will be using [Optuna](https:\/\/optuna.org\/), an increasingly popular tool for general hyper-parameter optimization. This is typically more effective in terms of time and resource usage than a na\u00efve grid search, as it uses a more sophisticated approach to deciding which hyper-parameters to attempt.\n\nAs with usual hyper-parameter optimization, we want to find the configuration that makes the validation loss as small as possible in order to improve generalization to new data.\n\nThe `objective` function represents a single choice of hyper-parameters which are used to train the model (via `train`). The value of the batch-averaged validation loss at the last epoch is used as the objective.\n\nAt the start of this function, we also suggest either a set of values to explore for each hyper-parameter, or a range of values (along with a sampling distribution, e.g. uniform or $\\log$-uniform). Note that in addition to the hyper-parameters that we discussed for the model earlier, we also explore different learning rates and L2 penalties for the Adam learning rule.","7c27ea30":"### Initializing Optuna and running a study\n\nIn Optuna's terminology, a _study_ refers to a specified number of trials which are made in order to optimize the objective function. We create the study by providing a _sampler_ which decides how hyper-parameter combinations are chosen, and specifying whether to maximize or minimize the objective that we specified.\n\nTo help with monitoring, we also use [Weights & Biases](https:\/\/wandb.ai\/) as it is very easy to incorporate, and is a really great tool. You can find the W&B reports for this project [here](https:\/\/wandb.ai\/eonu\/gtzan-cnn\/reportlist).\n\nFinally, we start the study and specify 25 trials to explore hyper-parameter configurations with.","a4c3535a":"## Creating a CNN model\n\n<span style=\"color:red;\"><b><u>Disclaimer<\/u><\/b><\/span>: I am not an expert at CNNs, and this project was mainly a learning process for me to gain exposure to designing CNN architectures. _Some of my choices may be questionable!_\n\nWe will now define the CNN architecture that we will use for classifying the preprocessed audio data.\n\nSince all of our recordings are the same length, our input batches have size $B \\times C \\times D \\times T$, where:\n\n- $B$ is the batch size (which we will fix to 16),\n- $C$ is the number of input channels (our 6 types of features),\n- $D$ is the number of dimensions for each time step (12 MFCCs, chroma features and their \u2206s & \u2206\u2206s),\n- $T$ is the number of time steps or frames (1290 after windowing).\n\nAs seen in the image below, we want our CNN filters to span all of the channels, while convolving along the feature and time dimensions.\n\n<div style=\"width:100%;text-align:center;\">\n    <img src=\"https:\/\/i.ibb.co\/RTtPHFQ\/features-stacked-filter.png\" alt=\"Stacked Input Features\"\/>\n<\/div>","6ba7c0a7":"We can now view the configuration of the best trial, and also display a `torch` summary of the model corresponding to this trial.","9fc2db0f":"### Putting everything together\n\nWe can now combine these two blocks into a single module, giving us the following architecture.\n\n<div style=\"width:100%;text-align:center;\">\n    <img src=\"https:\/\/i.ibb.co\/Zcvcn3d\/cnn.png\" alt=\"NN Architecture\"\/>\n<\/div>","6bbe0c17":"## Evaluation\n\nBelow are the loss and accuracy curves for the training and validation set on the best model. We can see that the model is able to learn how to predict genres from the training set with high accuracy, though the lower validation accuracy suggests some level of overfitting.\n\nYou can view an interactive version of this plot on [Weights & Biases](https:\/\/wandb.ai\/eonu\/gtzan-cnn\/reports\/GTZAN-Final-Model-Report--VmlldzoxNDk5NTA0).\n\nDespite using dropout between the layers of the classification block, as well as applying an L2 penalty to weights, it seems that exploring other regularization methods such as data augmentation would be beneficial in improving generalization.","063d08a9":"### Parallel coordinate plot for hyper-parameter and objective interactions\n\nThe below plot shows how the different hyper-parameters interact and how much influence they have on the objective function \u2013 _it's hard to read, I know!_"}}