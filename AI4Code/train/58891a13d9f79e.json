{"cell_type":{"4258a7d2":"code","41b2ccc6":"code","2b795e64":"code","d36169ca":"code","db146f6f":"code","1e66d02c":"code","e50f3bfd":"code","6363ef9b":"code","bcea085b":"code","c89f3c5c":"code","94aafbe6":"code","0b7bb4b6":"code","9a22aa81":"code","0922b74b":"code","a72f0dbb":"code","f809549d":"code","cfc9f752":"code","3b1294e6":"code","d471a808":"code","63d244ea":"code","f5151d9e":"code","574d97f4":"code","93249d37":"code","35c3f804":"code","5ab0c9e6":"code","7b082699":"code","f98a031a":"code","0a79486e":"code","f69df9e5":"code","31e5048c":"code","4f3dc4da":"code","248a1c1b":"code","abefd771":"markdown","b85485ee":"markdown","e02000ec":"markdown","2735e044":"markdown"},"source":{"4258a7d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","41b2ccc6":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n#display all the columns of the dataframe\npd.pandas.set_option('display.max_columns', None)","2b795e64":"#loading the data set\ndataset = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n#printing the number of rows and columns in the data\nprint(dataset.shape)\ndataset.head()","d36169ca":"#let us calculate all the nan values\n#first lets concentrate on categorical features\nfeatures_nan = [feature for feature in dataset.columns if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes == 'O']\n\nfor feature in features_nan:\n    print(\"{}: {}% mising values\".format(feature, np.round(dataset[feature].isnull().mean(),4)))\n    \ndataset.head()","db146f6f":"#Replace missing value with a new label\ndef replace_cat_feature(dataset,features_nan):\n    data = dataset.copy()\n    data[features_nan] =data[features_nan].fillna(\"Missing\")\n    return data\n\ndataset = replace_cat_feature(dataset, features_nan)\ndataset[features_nan].isnull().sum()\ndataset.head()","1e66d02c":"#Now lets find out missing values present in the numerical variable\n\nnumerical_with_nan = [feature for feature in dataset.columns if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes != 'O' ]\n\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.round(dataset[feature].isnull().mean(),4)))\ndataset.head()","e50f3bfd":"##Since their were a lot of outlinersin the numerical values\n#So we will replace the missing values with median or mode\n\nfor feature in numerical_with_nan:\n    median_val=dataset[feature].median()\n    \n    dataset[feature + 'nan'] = np.where(dataset[feature].isnull() ,1,0)\n    dataset[feature].fillna(median_val, inplace = True)\n    \n\ndataset[numerical_with_nan].isnull().sum()","6363ef9b":"dataset.head(50)","bcea085b":"#Temporaral variables(date\/time variables)\nfor feature in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n    \n    dataset[feature] = dataset['YrSold'] - dataset[feature]\n","c89f3c5c":"dataset.head()","94aafbe6":"dataset[['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']].head()","0b7bb4b6":"dataset.head()","9a22aa81":"# Convert skewed distribution into a log normal distribution\nnum_features = ['LotFrontage','LotArea','1stFlrSF','SalePrice']\nfor feature in num_features:\n    dataset[feature] = np.log (dataset[feature])\n","0922b74b":"dataset.head()","a72f0dbb":"categorical_features = [feature for feature in dataset.columns if dataset[feature].dtype == 'O']\ncategorical_features","f809549d":"for feature in categorical_features:\n    temp = dataset.groupby(feature)['SalePrice'].count()\/len(dataset)\n    temp_df = temp[temp>0.01].index\n    dataset[feature] = np.where(dataset[feature].isin(temp_df),dataset[feature],'Rare_var')","cfc9f752":"dataset.head(50)","3b1294e6":"for feature in categorical_features:\n    labels_ordered=dataset.groupby([feature])['SalePrice'].mean().sort_values().index\n    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n    dataset[feature]=dataset[feature].map(labels_ordered)","d471a808":"dataset.head(50)","63d244ea":"feature_scales = [feature for feature in dataset.columns if feature not in ['Id', 'SalePrice']]\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(dataset[feature_scales])\n","f5151d9e":"data =pd.concat([dataset[['Id', 'SalePrice']].reset_index(drop = True),\n                   pd.DataFrame(scaler.transform(dataset[feature_scales]),columns = feature_scales)],\n                   axis = 1)","574d97f4":"data.head()","93249d37":"data.to_csv('X_train.csv',index=False)","35c3f804":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\n# pd.pandas.set_option('display.max_colums',None)","5ab0c9e6":"data.head()","7b082699":"#Capture the dependent feature\ny_train = data[['SalePrice']]","f98a031a":"# drop dataset feature from datatset\nX_train = data.drop(['Id','SalePrice'],axis = 1)\n","0a79486e":"## Apply feature selection\n#First, I specify the lasso Regression model, and I\n#Select a suitable alpha the less features that will be selected\n#The bigger the aplha the less features that will be selected\n\n#Then I use the selectfromModel from sklearn, whcih\n#will sect the features which coefficants are non zero\n\nfeature_sel_model = SelectFromModel(Lasso(alpha = 0.005, random_state = 0))\nfeature_sel_model.fit(X_train,y_train)\n","f69df9e5":"feature_sel_model.get_support()","31e5048c":"# Lets print the number of total selected featurs\n\n#this is how we make a list of selected  features\nselected_feat = X_train.columns[(feature_sel_model.get_support())]\n\n#Lets print some stats\nprint('Total features: {}'.format(X_train.shape[1]))\nprint('Selected Featrues: {}'.format(len(selected_feat)))\n# print('features that were not selected and sharnk to 0: {}'.fomrat(np.sum(sel_.estimator_.coef_ == 0)))\n","4f3dc4da":"selected_feat","248a1c1b":"X_train = X_train[selected_feat]","abefd771":"# Feature Scaling","b85485ee":"# Handling Rare Categorical Feature\n## We will remove categorical variables that are present in less than 1% of the obseravtions","e02000ec":"## FEATURE SELECTION\n","2735e044":"## Numerical Variables(Part 2)\n"}}