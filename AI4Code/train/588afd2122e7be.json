{"cell_type":{"18d1e9f1":"code","cd8e8c2c":"code","f198f013":"code","da8b2ffe":"code","a32bca0b":"code","ec84ed53":"code","43f2b8af":"code","b0988a96":"code","dd42cc91":"code","5dee37f5":"code","f3339e3a":"code","f5852cb5":"code","e8f66e5b":"code","cb97595d":"code","ca8d158b":"code","ec5cc61b":"code","cb1d4467":"code","bba624d9":"code","c4fae739":"code","ef07c543":"code","6ddd6944":"code","875f5b47":"code","16c97d11":"code","922c3bde":"code","bb03f26f":"code","8866e9dd":"code","32614b3a":"code","514619cd":"code","42df0e82":"code","6bb6e8c9":"code","9f1b1268":"code","bbd8c89c":"code","a298b2fc":"markdown","6b731f43":"markdown","a03fd3c7":"markdown","e0a03cb2":"markdown","17bae777":"markdown","37a1007b":"markdown","9b85d947":"markdown","c9962c98":"markdown","818dd75e":"markdown","e74da4b0":"markdown","7ec8f213":"markdown","2e4aa624":"markdown","9daa6285":"markdown","d0515f79":"markdown","c4bb178a":"markdown","e177c2ba":"markdown","c931ed48":"markdown","4f315853":"markdown","5ac85a70":"markdown","d311ffc9":"markdown","b9e3837f":"markdown","1bbfa036":"markdown","b18669a4":"markdown","3f2ef38d":"markdown"},"source":{"18d1e9f1":"from transformers import pipeline","cd8e8c2c":"nlp = pipeline(\"sentiment-analysis\")","f198f013":"result = nlp(\"Everyone hates you\")\nresult","da8b2ffe":"result = nlp(\"Your family loves you\")\nresult","a32bca0b":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch","ec84ed53":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")","43f2b8af":"classes = [\"Not_Paraphrase\", \"Paraphrase\"]","b0988a96":"sentence1 = \"A reply from you is what I'm expecting\"\nsentence2 = \"You are very cheerful today\"\nsentence3 = \"I am awaiting a response from you\"","dd42cc91":"paraphrase = tokenizer(sentence1, sentence3, return_tensors=\"pt\")\nnot_paraphrase = tokenizer(sentence1, sentence2, return_tensors=\"pt\")","5dee37f5":"paraphrase","f3339e3a":"paraphrase_model = model(**paraphrase)\nnonparaphrase_model = model(**not_paraphrase)","f5852cb5":"paraphrase_model, nonparaphrase_model","e8f66e5b":"paraphrase_result = torch.softmax(paraphrase_model[0], dim=1).tolist()[0]\nnonparaphrase_result = torch.softmax(nonparaphrase_model[0], dim=1).tolist()[0]","cb97595d":"paraphrase_result","ca8d158b":"# Paraphrase output\nfor i in range(len(classes)):\n    print(f\"{classes[i]}: {paraphrase_result[i] * 100:.2f}%\")","ec5cc61b":"# Non Paraphrase output\nfor i in range(len(classes)):\n    print(f\"{classes[i]}: {paraphrase_result[i] * 100:.2f}%\")","cb1d4467":"nlp = pipeline(\"question-answering\")\n\ncontext = r'''Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 \nto 1966. Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions. \nApollo used Saturn family rockets as launch vehicles. Apollo\/Saturn vehicles were also used for an Apollo Applications Program, \nwhich consisted of Skylab, a space station that supported three manned missions in 1973\u201374, and the Apollo\u2013Soyuz Test Project, \na joint Earth orbit mission with the Soviet Union in 1975'''","bba624d9":"result = nlp(question=\"What space station supported three manned missions in 1973\u20131974?\", context=context)\nresult","c4fae739":"result = nlp(question=\"What is Apollo\u2013Soyuz Test Project?\", context=context)\nresult","ef07c543":"from transformers import AutoModelForQuestionAnswering\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")","6ddd6944":"questions = [\"What space station supported three manned missions in 1973\u20131974?\", \"What is Apollo\u2013Soyuz Test Project?\",\n             \"What are Gemini missions?\"]","875f5b47":"for question in questions:\n    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].tolist()[0]\n    \n    answer_start_scores, answer_end_scores = model(**inputs)\n    \n    # Get the likely beginning of answer\n    answer_start = torch.argmax(answer_start_scores) \n    # Get the likely end of answer\n    answer_end = torch.argmax(answer_end_scores) + 1\n    \n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\")","16c97d11":"from transformers import pipeline\nnlp = pipeline(\"fill-mask\")","922c3bde":"from pprint import pprint\npprint(nlp(f\"Learning another {nlp.tokenizer.mask_token} is like becoming another person\"))","bb03f26f":"pprint(nlp(f\"I love Kaggle because it gives me {nlp.tokenizer.mask_token}\"))","8866e9dd":"from transformers import AutoModelWithLMHead, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\nmodel = AutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")\n\nsequence = f\"Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to fill {tokenizer.mask_token} with an appropriate token\"\n\ninput = tokenizer.encode(sequence, return_tensors=\"pt\")","32614b3a":"mask_token_id = torch.where(input == tokenizer.mask_token_id)[1]\nmodel_output = model(input)[0]","514619cd":"mask_token_logits = model_output[0, mask_token_id, :]\nprint(mask_token_logits)","42df0e82":"top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\nfor token in top_5_tokens:\n    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))","6bb6e8c9":"from transformers import AutoModelWithLMHead, AutoTokenizer, top_k_top_p_filtering\n\nfrom torch.nn import functional as F\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n \nsequence = f\"Psychology is the study of human \"\n\ninput_ids = tokenizer.encode(sequence, return_tensors=\"pt\")","9f1b1268":"model(input_ids)[0][:, -1, :]","bbd8c89c":"# get logits of last hidden state\nnext_token_logits = model(input_ids)[0][:, -1, :]\n\nfiltered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n\nprobs = F.softmax(filtered_next_token_logits, dim=-1)\nnext_token = torch.multinomial(probs, num_samples=1)\n\ngenerated = torch.cat([input_ids, next_token], dim=-1)\n\nresulting_string = tokenizer.decode(generated.tolist()[0])\n\nprint(resulting_string)","a298b2fc":"Instantiate a tokenizer and a model from the checkpoint name which is MRPC in this case. Weights for the model are loaded from checkpoint","6b731f43":"Retrieve the predictions at the index of the mask token. This tensor has the same size as the vocabulary, and the values are the scores attributed to each token. The model gives higher score to tokens it deems probable in that context.","a03fd3c7":"That's all folks for this notebook. Notebook was getting lengthy. I will make a second part to cover remaining tasks. \n\nPlease upvote if you like!","e0a03cb2":"Define a sequence with a masked token, placing the tokenizer.mask_token instead of a word. Encode that sequence into a list of IDs and find the position of the masked token in that list","17bae777":"# List of Task Types -\n\n1) Sequence Classification\n\n2) Sequence Paraphrasing\n\n3) Extractive Question Answering\n\n4) Masked Language modelling\n\n5) Casual Language modelling\n\n6) Text Generation\n\n7) Named Entity Recognition\n\n8) Summarization\n\n9) Translation","37a1007b":"A list is returned which has a dict of label and score","9b85d947":"Build a sequence from the two sentences. We will get Input IDs, Token type IDs and Attention mask as the output. A thing to note is that tensors are returned as either one of the following - 'pt' for pytorch, 'tf' for tensorflow and 'np' for numpy. Since we are using Pytorch so 'pt' is set as return_tensors","c9962c98":"Let's do the same with a model and tokenizer now. We will use DistilBERT as the model and load the weights stored in the checkpoint","818dd75e":"Retrieve the top 5 tokens using the PyTorch topk methods. Replace the mask token by the tokens and print the results","e74da4b0":"Transformers now SOTA in NLP landscape can handle a variety of tasks exceptionally well. In this notebook I will attempt to list and explain them","7ec8f213":"# Causal Language Modeling\nCausal language modeling is the task of predicting the token following a sequence of tokens. In this situation, the model only attends to the left context (tokens on the left of the mask). Such a training is particularly interesting for generation tasks.\n\nUsually, the next token is predicted by sampling from the logits of the last hidden state the model produces from the input sequence","2e4aa624":"# Sequence Paraphrasing Task -\nParaphrasing is the task of expressing the meaning of another phrase, sentence, etc. using different words to achieve clarity\nWe will leverage Auto models for this task. Automodels are classes that will instantiate a model according to a given checkpoint, automatically selecting the correct model architecture.\n\nIn order for a model to perform well on a task, it must be loaded from a checkpoint corresponding to that task. These checkpoints are usually pre-trained on a large corpus of data and fine-tuned on a specific task.\n\nModels are fine tuned on specific dataset which may not overlap with our use case or domain. In that case we need to create our own script\n\nThe Microsoft Research Paraphrase Corpus or MRPC in short is a paraphrase identification dataset, where systems aim to identify if two sentences are paraphrases of each other","9daa6285":"# Transformers - \nTransformers (from HuggingFace, formerly known as pytorch-transformers and pytorch-pretrained-bert) provide general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch\n\nSome of the features are -\n\n1) Researchers can share trained models instead of always retraining\n\n2) Practitioners can reduce compute time and production costs\n\n3) 8 architectures with over 30 pretrained models, some in more than 100 languages\n\n\nWe will be using different GLUE datasets in our code. Let's briefly take a look at different GLUE datasets","d0515f79":"BAM!! Model predicted corrected word for mask in its top score. Let's try one more albeit this one on a lighter note","c4bb178a":"1) COLA - Default config\n\nThe Corpus of Linguistic Acceptability consists of English acceptability judgments drawn from books and journal articles on linguistic theory. Each example is a sequence of words annotated with whether it is a grammatical English sentence.\n\n2) SST2\n\nThe Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. Two-way (positive\/negative) class split only sentence-level labels is used\n\n3) MRPC\n\nThe Microsoft Research Paraphrase Corpus is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent\n\n4) QQP\n\nThe Quora Question Pairs dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent\n\n5) STSB\n\nThe Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data. Each pair is human-annotated with a similarity score from 1 to 5\n\n6) MNLI\n\nThe Multi-Genre Natural Language Inference Corpus is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are gathered from ten different sources, including transcribed speech, fiction, and government reports. It has 2 variants MNLI-Matched and MNLI-Mismatched\n\n7) QNLI\n\nThe Stanford Question Answering Dataset is a question-answering dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). We convert the task into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. \n\n8) RTE\n\nThe Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges. Data is combined from RTE1, RTE2 , RTE3, and RTE5.4 Examples are constructed based on news and Wikipedia text. All datasets is converted to a two-class split, where for three-class datasets neutral and contradiction are collapsed into not entailment, for consistency.\n\n9) WNLI\n\nThe Winograd Schema Challenge is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from a list of choices. The examples are manually constructed to foil simple statistical methods: Each one is contingent on contextual information provided by a single word or phrase in the sentence. To convert the problem into sentence pair classification, we construct sentence pairs by replacing the ambiguous pronoun with each possible referent. The task is to predict if the sentence with the pronoun substituted is entailed by the original sentence.\n\n10) AX\n\nA manually-curated evaluation dataset for fine-grained analysis of system performance on a broad range of linguistic phenomena. This dataset evaluates sentence understanding through Natural Language Inference (NLI) problems. Use a model trained on MulitNLI to produce predictions for this dataset.","e177c2ba":"Let us ask one more question","c931ed48":"# GLUE Datasets","4f315853":"# **Masked Language Modeling -**\n\nMasked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to fill that mask with an appropriate token. This allows the model to attend to both the right context (tokens on the right of the mask) and the left context (tokens on the left of the mask). Such a training creates a strong basis for downstream tasks, requiring bi-directional context such as SQuAD\n\nHere is an example of using pipelines to replace a mask from a sequence:","5ac85a70":"# Extractive Question Answering\nExtractive Question Answering is the task of extracting an answer from a text given a question. SQuAD dataset is an example of a question answering dataset. It is entirely based on that task. \n\nWe will use pipelines which leverages fine tuned model on SQuAD to extract an answer from a text given a question","d311ffc9":"# Language Modeling\nLanguage modeling is the task of fitting a model to a corpus, which can be domain specific. All popular transformer-based models are trained using a variant of language modeling, e.g. BERT with masked language modeling, GPT-2 with causal language modeling.","b9e3837f":"Compute the softmax of the result to get probabilities over the classes","1bbfa036":"Pass this sequence through the model so that it is classified in one of the two available classes: 0 (not a paraphrase) and 1 (is a paraphrase).","b18669a4":"# Sequence Classification Task -\n\nIt is the simplest task of classifying sequences. We will use transformer pipeline which is easy-to-use abstraction.\nWe will download a GLUE dataset and it will leverage a fine-tuned model on SST2, which is a GLUE task as seen earlier","3f2ef38d":"Now lets build a model using tokenizers and without pipeline. Firstly instantiate a tokenizer and a model from the checkpoint name"}}