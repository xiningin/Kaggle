{"cell_type":{"00e0107c":"code","407e9907":"code","f350577e":"code","865a54bb":"code","472581e1":"code","b8b5acba":"code","280141d6":"code","4f3dcf29":"code","de83db47":"code","3816088a":"code","ef4c237c":"code","320f4bd1":"code","16816820":"code","86f07863":"markdown","f2995b25":"markdown","fa5b5872":"markdown","d016572a":"markdown","52f288c9":"markdown","3b2dc12c":"markdown","42d4d97a":"markdown","5381ec20":"markdown","3896aa2a":"markdown","3a0b8642":"markdown","d4dde9ce":"markdown"},"source":{"00e0107c":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt","407e9907":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f350577e":"train_path = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntrain_path = train_path.sample(frac=1)\ntrain_path.head()\n","865a54bb":"training_sentences, training_labels = [], []\ndef add_data( l, d):\n    for i in train_path[d]:\n        l.append(i)\nadd_data( training_sentences, 'text')\nadd_data( training_labels,  'target')","472581e1":"tokenizer = Tokenizer(oov_token = \"<404>\")\ntokenizer.fit_on_texts(training_sentences)\n\nword_index = tokenizer.word_index\n\ntraining = tokenizer.texts_to_sequences(training_sentences)\ntrain_padded = pad_sequences(training, padding = 'pre')\ntrain_sentences_ = np.array(train_padded)\n\ntraining_labels_ = np.array(training_labels)","b8b5acba":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(len(word_index)+1, 16, input_length = 33),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(128, activation = 'relu'),\n#     tf.keras.layers.Dense(128, activation = 'relu'),\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\n])\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'])\nmodel.summary()","280141d6":"history = model.fit(train_sentences_, training_labels_, epochs = 15, validation_split = 0.1)","4f3dcf29":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nloss = history.history['loss']\nval_acc = history.history['val_acc']\nval_loss = history.history['val_loss']\nfig, ax = plt.subplots(2,1)\nax[0].plot(acc, 'b', label = \"Training_accuracy\")\nax[0].plot(val_acc, 'r', label = \"dev_accuracy\")\nax[0].legend(loc = 'best', shadow = True)\n    \nax[1].plot(loss, 'b', label = \"trianing_loss\")\nax[1].plot(val_loss, 'r', label = \"dev_loss\")\nax[1].legend(loc = 'best', shadow = True)\n# plot_graphs(acc, val_acc, loss, val_loss)","de83db47":"test_path = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest_set = [x for x in test_path['text']]\ntest_sequences = tokenizer.texts_to_sequences(test_set)\ntest_padded = pad_sequences(test_sequences,maxlen = 33, truncating = 'post', padding = 'pre')","3816088a":"pred = model.predict(test_padded)","ef4c237c":"test_predictions = []\nfor i in pred.round().astype(int):\n    test_predictions.append(i[0])","320f4bd1":"ids = [str(x) for x in test_path['id']]\nsol = pd.DataFrame({'id':ids, 'target': test_predictions})\nsol.to_csv('prediction3.csv', index = False)","16816820":"#########################################################################\n#########################################################################\nsingle_test=[]\nsingle_test.append(input())\nsingle_test_seqs = tokenizer.texts_to_sequences(single_test)\nsingle_test_padded = pad_sequences(single_test_seqs, maxlen = 33, truncating = 'post', padding = 'pre')\nprint(\"Disaster\" if model.predict(single_test_padded).round() else \"fake\")","86f07863":"Import necessary Libraries","f2995b25":"Training the model for 25 epochs and validating on 10% of training set","fa5b5872":"Rounding of predictions to 2 classes","d016572a":"checking on custom sentences.....","52f288c9":"Converting the predictions to csv file","3b2dc12c":"Preprocessing test set and converting them to padded sequences fit for classifying","42d4d97a":"that's not a disaster? :)","5381ec20":"(This is maybe a very dumb model, but  very basic)","3896aa2a":"Visulaising the train and deva accuracy and loss","3a0b8642":"Shuffle the training sentences","d4dde9ce":"Create a tokenizer and pass it over training sentences and labels\n\nAnd convert them to numpy array"}}