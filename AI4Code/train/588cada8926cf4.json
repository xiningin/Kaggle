{"cell_type":{"86ad668d":"code","7ce1bfb5":"code","7baf8ec9":"code","45969731":"code","57ef7d95":"code","091c5926":"code","426ee065":"code","e7b65f82":"code","81f31619":"code","fee762d9":"code","5d27bf5f":"code","e7f7eee3":"code","af072e73":"code","e838d611":"code","1a8ac26c":"code","a66c0a5e":"code","48ea4a7f":"code","bdb82c80":"code","96164d97":"code","62e46668":"code","6e5c51a4":"code","c8fe4edb":"code","601a1ff3":"code","8a76a79d":"code","87a9f0d1":"code","107b3af8":"code","ea27252c":"code","2c09df54":"code","8fe8abed":"code","b380d5c3":"code","e29ac40c":"code","53f28fed":"code","ae8805f0":"code","bcaba51b":"code","09606b3c":"code","4b856065":"code","9135de5d":"code","59a8e92d":"code","6b973848":"code","e834c634":"code","fa81a8d7":"code","176df96e":"code","b799d5fa":"code","9ca838ea":"code","9dbbfe41":"code","c77212e4":"code","efa43b84":"code","16076b02":"code","ec0e3e4d":"code","5e1c6c1f":"code","68cbd501":"code","a11104a0":"code","c4f9f35e":"code","06e8a353":"code","b5209a03":"code","ad0cdfbe":"code","88410833":"code","acd0ddfd":"code","8a177e18":"code","26a8a8c8":"code","21e33902":"code","3a9630d4":"code","9dd70efd":"code","ac2c64a9":"code","d4abf75d":"code","becc44ca":"code","96f9b370":"code","5dfdaac6":"code","a7cceaca":"code","a6b9532d":"code","85eee219":"code","6ac09589":"code","db6e246c":"code","c4d8c877":"code","2fa78466":"code","42e06521":"code","fcf15171":"code","0620de0f":"markdown","48336e8d":"markdown","ba25b7eb":"markdown","81c1db02":"markdown","5b5f2ab2":"markdown","daa89499":"markdown","9476a9bd":"markdown","04aefcf4":"markdown","69b1f96a":"markdown","92f92da8":"markdown","49f55c5b":"markdown","93eeed7f":"markdown","100857d7":"markdown","06f68b48":"markdown","ef763470":"markdown","a4bbadc0":"markdown","49cc8087":"markdown","7b149654":"markdown","147f6685":"markdown","6c53825f":"markdown","afe72d17":"markdown","56edd77d":"markdown","2df413ee":"markdown","d5adfd5d":"markdown","6ba279ea":"markdown","91b759a8":"markdown","b1ec1522":"markdown","1a313416":"markdown","b869f770":"markdown","54f0e2be":"markdown","4969c0f1":"markdown","2d37f003":"markdown","09d423ed":"markdown","5fe5c5ba":"markdown","469700ec":"markdown","bccd7b37":"markdown","695de4bf":"markdown","eca49190":"markdown","95992112":"markdown","931147f4":"markdown","146dea20":"markdown","c60c196d":"markdown","765db6cd":"markdown","03566e8e":"markdown","ff4d1477":"markdown","81aab1c5":"markdown","c51721d0":"markdown","cba5347e":"markdown","f694c1d0":"markdown","73ad2c4b":"markdown","fd49d616":"markdown","692ca636":"markdown","268a1001":"markdown","8630dc8c":"markdown","efe674cf":"markdown","272f8fbe":"markdown","d412fd35":"markdown","b4e3d471":"markdown"},"source":{"86ad668d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nfrom datetime import datetime\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Setting random seed for reproducibility\nmyseed = 1337\nnp.random.seed(myseed)","7ce1bfb5":"### Missing valus Table\ndef missing_zero_values_table(df):\n        zero_val = (df == 0.00).astype(int).sum(axis=0)\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)\n        mz_table = mz_table.rename(\n        columns = {0 : 'Zero Values', 1 : 'Missing Values', 2 : '% of Total Values'})\n        mz_table['Total Zero Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']\n        mz_table['% Total Zero Missing Values'] = 100 * mz_table['Total Zero Missing Values'] \/ len(df)\n        mz_table['Data Type'] = df.dtypes\n        mz_table = mz_table[\n            mz_table.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n            \"There are \" + str(mz_table.shape[0]) +\n              \" columns that have missing values.\")\n#         mz_table.to_excel('D:\/sampledata\/missing_and_zero_values.xlsx', freeze_panes=(1,0), index = False)\n        return mz_table","7baf8ec9":"# Show Mean, Standatd Deviation\ndef display_scores(scores):\n#     print(\"Scores:\", scores)\n    print(\"Mean :\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","45969731":"## get categorical and numerical column names\n\ndef get_num_cat_cols(dataframe):\n    cols = list(dataframe.columns)\n    num_cols = list(dataframe.select_dtypes([np.number]).columns)\n    cat_cols = list(set(cols) - set(num_cols))\n    \n    print(\"Total Attributes       : {:d}\".format(len(cols)))\n    print(\"Numerical Attributes   : {:d}\".format(len(num_cols)))\n    print(\"Categorical Attributes : {:d}\\n\".format(len(cat_cols)))\n    \n    print(f\"Numerical columns in the dataset \\n {num_cols}\\n\")\n    print(f\"Categorical columns in the dataset \\n {cat_cols}\")\n    \n    return num_cols, cat_cols","57ef7d95":"train = pd.read_csv('..\/input\/mldub-comp1\/train_data.csv')\ntest = pd.read_csv('..\/input\/mldub-comp1\/test_data.csv')\nsample_sub = pd.read_csv('\/kaggle\/input\/mldub-comp1\/sample_sub.csv')\n\nprint (\"Data is loaded!\")\nprint (\"Train: \",train.shape[0],\"observations, and \",train.shape[1],\"features\")\nprint (\"Test: \",test.shape[0],\"observations, and \",test.shape[1],\"features\")","091c5926":"train.head()","426ee065":"num_cols, cat_cols = get_num_cat_cols(train)","e7b65f82":"train.info()","81f31619":"train.describe()","fee762d9":"test.head()","5d27bf5f":"num_cols, cat_cols = get_num_cat_cols(test)","e7f7eee3":"test.info()","af072e73":"test.describe()","e838d611":"sns.set_style(\"whitegrid\")\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()\nplt.show()","1a8ac26c":"y = train['target_variable']\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=stats.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=stats.lognorm)\nplt.show()","a66c0a5e":"missing_zero_values_table(train)","48ea4a7f":"missing_zero_values_table(test)","bdb82c80":"# train_noNA = train.drop([\"type\", \"origin\", \"other_text\", \"about\"], axis=1)\n# test_noNA = test.drop([\"type\", \"origin\", \"other_text\", \"about\"], axis=1)\ntrain_noNA = train.copy()\ntest_noNA = test.copy()","96164d97":"missing_zero_values_table(train_noNA)","62e46668":"train.hist(bins=50, figsize=(20,15))\nplt.show()","6e5c51a4":"quantitative = [f for f in train.columns if train.dtypes[f] != 'object']\nquantitative.remove('target_variable')\nquantitative.remove('id')\nqualitative = [f for f in train.columns if train.dtypes[f] == 'object']\nprint('Quantitative Variables: ', quantitative)\nprint('Qualitative Variables: ', qualitative)","c8fe4edb":"test_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01\nnormal = pd.DataFrame(train[quantitative])\nnormal = normal.apply(test_normality)\nprint(not normal.any())","601a1ff3":"train[\"target_variable\"].hist(bins=50, figsize=(20,15))\nplt.show()","8a76a79d":"np.log(train[\"target_variable\"]).hist(bins=50, figsize=(20,15))\nplt.show()","87a9f0d1":"train_clean = train_noNA.drop('id', axis=1)\ntest_clean = test_noNA.drop('id', axis=1)","107b3af8":"corr = train_clean.corr()\ncorr","ea27252c":"import numpy as np; np.random.seed(0)\nimport seaborn as sns; sns.set()\n# uniform_data = np.random.rand(10, 12)\nax = sns.heatmap(corr)","2c09df54":"# There are some weird data points in `status` category, let's just replace them with 'Unknown' for both train and test.\n# Check results\nprint(train_clean['status'].unique())\nprint(test_clean['status'].unique())\n\nfor df in [train_clean, test_clean]:\n    df.loc[~df['status'].isin(['Deadpool', 'Submission', 'Confirmed']), 'status'] = 'Unknown'\n\n# Check results\nprint('Cleared: ', train_clean['status'].unique())","8fe8abed":"train_clean.nunique()","b380d5c3":"len(train_clean)","e29ac40c":"train_clean[\"origin_place\"].value_counts()","53f28fed":"sum(train_clean[\"origin_place\"].value_counts() > 1)","ae8805f0":"train_clean[\"status\"].unique()","bcaba51b":"test_clean[\"status\"].unique()","09606b3c":"from sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\nstatus_1hot_train = encoder.fit_transform(train_clean[\"status\"])\nstatus_1hot_test = encoder.fit_transform(test_clean[\"status\"])\nstatus_1hot_train","4b856065":"status_df_train = pd.DataFrame(status_1hot_train, columns=encoder.classes_)\nstatus_df_test = pd.DataFrame(status_1hot_test, columns=encoder.classes_)","9135de5d":"status_df_train","59a8e92d":"#train_clean = pd.concat([train_clean, status_df_train], axis=1).drop(\"status\", axis=1)\n#test_clean = pd.concat([test_clean, status_df_test], axis=1).drop(\"status\", axis=1)","6b973848":"train_clean.head()","e834c634":"train_clean[\"type\"].value_counts()","fa81a8d7":"train_clean[\"type\"]","176df96e":"train_clean[\"tags\"].value_counts()","b799d5fa":"train_clean[\"details\"] = train_clean[\"type\"].fillna(\" \")\ntrain_clean.drop(\"type\", axis=1, inplace=True)\n\ntest_clean[\"details\"] = test_clean[\"type\"].fillna(\" \") \ntest_clean.drop(\"type\", axis=1, inplace=True)","9ca838ea":"train_clean[\"origin_year\"]","9dbbfe41":"train_clean[\"origin_year\"].value_counts()","c77212e4":"def fix_origin_year(dataframe):\n    #dataframe['origin_year'] = dataframe['origin_year'].replace('Unknown', '2060')\n    year_old =  ['1999','1998','1997','1996','1995','1994',\n                 '1993','1992','1991','1990','1989','1988',\n                 '1987','1986','1985','1984','1983','1982',\n                 '1981','1980','1979','1978','1977','1976',\n                 '1975','1974','1973','1972','1971','1970',\n                 '1969','1968','1967','1966','1965','1964',\n                 '1963','1962','1961','1960','1959','1958',\n                 '1957','1956','1955','1954','1953','1952',\n                 '1951','1950','1949','1948','1947','1946',\n                 '1945','1944','1943','1942','1941','1940',\n                 '1939','1938','1937','1935','1933','1932',\n                 '1931','1930','1929','1928','1927','1926',\n                 '1924','1923','1922','1921','1920','1919',\n                 '1918','1916','1915','1914','1913','1912',\n                 '1910','1909','1908','1907','1906','1905',\n                 '1904','1903','1902','1900','1897','1894',\n                 '1893','1891','1890','1889','1880','1878',\n                 '1874','1871','1869','1867','1865','1863',\n                 '1861','1857','1847','1846','1844','1839',\n                 '1837','1829','1819','1818','1814','1812',\n                 '1809','1800','1797','1795','1792','1790',\n                 '1785','1780','1776','1775','1774','1772',\n                 '1753','1717','1700','1694','1663','1657',\n                 '1621','1605','1600','1596','1590','1589',\n                 '1580','1564','1561','1516','1503','1495',\n                 '1490','1445','1415','1392','1341','1336',\n                 '1232','1212','1111','1100','1070','1017',\n                 '1000','Unknown']\n    year_new = ['2890','2500','2104','2074','2069','2021']\n    dataframe['origin_year'] = dataframe['origin_year'].replace(year_old, '1900')\n    dataframe['origin_year'] = dataframe['origin_year'].replace(year_new, '2020')\n    dataframe['origin_year'] = pd.to_numeric(dataframe['origin_year']) \n    dataframe['origin_year'] = pd.to_datetime(dataframe['origin_year'], format='%Y', infer_datetime_format=True)\n    \n    date_differnce = (pd.to_datetime(dataframe[\"date_added\"]) - dataframe[\"origin_year\"])\n    year_difference = date_differnce\/np.timedelta64(1, 'Y')\n    dataframe['time_gap'] = year_diff_int = year_difference.astype(int)\n    \n    ### Here we fix some features after train-test join\n    dataframe['date_added'] = pd.to_datetime(dataframe['date_added'], infer_datetime_format=True)\n    dataframe['date_added_year']= dataframe['date_added'].dt.year\n    dataframe['date_added_month']= dataframe['date_added'].dt.month\n    dataframe['date_added_day']= dataframe['date_added'].dt.day\n    dataframe['date_added_dayofweek']= dataframe['date_added'].dt.dayofweek\n    \n    #dataframe.drop(\"origin_year\", axis=1, inplace=True)\n    #dataframe.drop(\"date_added\", axis=1, inplace=True)\n    \n    #dataframe['origin_year']=dataframe['origin_year'].map(datetime.toordinal)\n    #dataframe['origin_year']=dataframe['origin_year'].map(datetime.toordinal)\n    #dataframe[\"origin_year\"] = pd.to_datetime(dataframe[\"origin_year\"]).dt.strftime(\"%Y%m%d\")\n    \n    return dataframe\n\n\"\"\"def add_year_gap(dataframe):\n    dataframe.loc[dataframe['time_gap'] < 1, 'time_gap'] = \"time_gap_new\"\n    dataframe.loc[dataframe['time_gap'] == 1, 'time_gap'] = \"time_gap_new\"\n    dataframe.loc[dataframe['time_gap'] == 2, 'time_gap'] = \"time_gap_recent\"\n    dataframe.loc[dataframe['time_gap'] > 2, 'time_gap'] = \"time_gap_old\"\n   # dataframe.drop(\"origin_year\", axis=1, inplace=True)\n    dataframe.drop(\"date_added\", axis=1, inplace=True)\n    return dataframe\n\"\"\"\n\ndef prepare_dataset(dataframe):\n    \n    dataframe = dataframe.drop('id', axis=1)\n    \n    dataframe.loc[~dataframe['status'].isin(['Deadpool', 'Submission', 'Confirmed']), 'status'] = 'Unknown'\n    \n    #Encoder\n    encoder = LabelBinarizer()\n    status_1hot = encoder.fit_transform(dataframe[\"status\"])\n    status_df = pd.DataFrame(status_1hot, columns=encoder.classes_)\n    dataframe = pd.concat([dataframe, status_df], axis=1).drop(\"status\", axis=1)\n    \n    dataframe[\"details\"] = dataframe[\"type\"].astype(str).fillna(\" \")\n    dataframe.drop(\"type\", axis=1, inplace=True)\n    \n    fix_origin_year(dataframe)\n    #train_clean1 = dataframe.copy()\n    #fix_origin_year(train_clean1)\n    #add_year_gap(dataframe, train_clean1)\n    \n    dataframe.loc[dataframe[\"origin_place\"] == \"Unknown\", 'origin_place'] = \" \"\n    dataframe[\"details\"] = dataframe[\"origin_place\"].astype(str)  + dataframe[\"details\"].astype(str) \n    dataframe[\"details\"] = dataframe[\"other_text\"].astype(str)  + \" \" + dataframe[\"about\"].astype(str)  + \" \" + dataframe[\"details\"].astype(str) \n    dataframe[\"details\"] = dataframe[\"origin\"].astype(str)  + \" \" + dataframe[\"details\"]\n    \n    # Simple statistics on text\n    dataframe['name_len'] = dataframe['name'].fillna('').apply(len)\n    dataframe['about_len'] = dataframe['about'].fillna('').apply(len)\n    dataframe['origin_len'] = dataframe['origin'].fillna('').apply(len)\n    dataframe['other_text_len'] = dataframe['other_text'].fillna('').apply(len)\n    dataframe['tags_len'] = dataframe['tags'].fillna('').apply(len)\n\n    # Create log1p versions of numeric features\n    dataframe['log1p_comments'] = np.log1p(dataframe['comments'])\n    dataframe['log1p_photos'] = np.log1p(dataframe['photos'])\n#    dataframe['log1p_date_added_year'] = np.log1p(dataframe['date_added_year'])\n\n    dataframe['log1p_name_len'] = np.log1p(dataframe['name_len'])\n    dataframe['log1p_about_len'] = np.log1p(dataframe['about_len'])\n    dataframe['log1p_origin_len'] = np.log1p(dataframe['origin_len'])\n    dataframe['log1p_other_text_len'] = np.log1p(dataframe['other_text_len'])\n    dataframe['log1p_tags_len'] = np.log1p(dataframe['tags_len'])\n        \n    #encoder = LabelBinarizer()\n    #time_gap_1hot_train = encoder.fit_transform(dataframe[\"time_gap\"])\n    #time_gap_1hot_train\n    #time_gap_df_train = pd.DataFrame(time_gap_1hot_train, columns=encoder.classes_)\n    #dataframe = pd.concat([dataframe, time_gap_df_train], axis=1) # .drop(\"time_gap\", axis=1)\n    \n    #dataframe.drop('origin_place', axis=1, inplace=True)\n    #dataframe.drop(\"other_text\", axis=1, inplace=True)\n    #dataframe.drop(\"about\", axis=1, inplace=True)\n    #dataframe.drop(\"origin\", axis=1, inplace=True)\n\n    return dataframe","efa43b84":"\n#add_year_gap(test_clean)\n#add_year_gap(train_clean)","16076b02":"train_clean = prepare_dataset(train)\ntest_clean = prepare_dataset(test)","ec0e3e4d":"#1\ntrain_clean.loc[train_clean[\"origin_place\"] == \"Unknown\", 'origin_place'] = \" \"\ntrain_clean[\"details\"] = train_clean[\"origin_place\"] + train_clean[\"details\"]\ntrain_clean.drop('origin_place', axis=1, inplace=True)\n\ntest_clean.loc[test_clean[\"origin_place\"] == \"Unknown\", 'origin_place'] = \" \"\ntest_clean[\"details\"] = test_clean[\"origin_place\"] + test_clean[\"details\"]\ntest_clean.drop('origin_place', axis=1, inplace=True)\n\n\n#2 \ntrain_clean[\"details\"] = train_clean[\"other_text\"] + \" \" + train_clean[\"about\"] + \" \" + train_clean[\"details\"]\ntrain_clean.drop(\"other_text\", axis=1, inplace=True)\ntrain_clean.drop(\"about\", axis=1, inplace=True)\n\ntest_clean[\"details\"] = test_clean[\"other_text\"] + \" \" + test_clean[\"about\"] + \" \" + test_clean[\"details\"]\ntest_clean.drop(\"other_text\", axis=1, inplace=True)\ntest_clean.drop(\"about\", axis=1, inplace=True)\n\n#3\ntrain_clean[\"details\"] = train_clean[\"origin\"] + \" \" + train_clean[\"details\"]\ntrain_clean.drop(\"origin\", axis=1, inplace=True)\n\ntest_clean[\"details\"] = test_clean[\"origin\"] + \" \" + test_clean[\"details\"]\ntest_clean.drop(\"origin\", axis=1, inplace=True)\n\n#4\n#encoder = LabelBinarizer()\n#time_gap_1hot_train = encoder.fit_transform(train_clean[\"time_gap\"])\n#time_gap_1hot_train\n\n#time_gap_df_train = pd.DataFrame(time_gap_1hot_train, columns=encoder.classes_)\n# status_df_test = pd.DataFrame(status_1hot_test, columns=encoder.classes_)\n#time_gap_df_train\n\n#train_clean = pd.concat([train_clean, time_gap_df_train], axis=1).drop(\"time_gap\", axis=1)\n# test_clean = pd.concat([test_clean, status_df_test], axis=1).drop(\"status\", axis=1)","5e1c6c1f":"train_clean.columns","68cbd501":"train_clean.head()","a11104a0":"corr = train_clean.corr()\nax = sns.heatmap(corr)","c4f9f35e":"train_features = train_clean.drop(\"target_variable\", axis=1)\ntest_features = test_clean.copy()\ntrain_labels = train_clean[\"target_variable\"]","06e8a353":"######### SET THE Y to target_variable\n\n# train = train[train.GrLivArea < 4500]\n# train.reset_index(drop=True, inplace=True)\ntrain['target_variable'].head\ntrain[\"target_variable_log1p\"] = np.log1p(train[\"target_variable\"])\ny = train['target_variable_log1p'].reset_index(drop=True)\n#y = train['target_variable'].reset_index(drop=True)\ntrain['target_variable'].head\ny.head","b5209a03":"# SET the X and X_test\nX_train = train_clean.drop(['target_variable'], axis=1)\nX_test = test_clean","ad0cdfbe":"train_len = len(X_train)\ntest_len = len(X_test)","88410833":"X_all = pd.concat([X_train, X_test])\nX_all","acd0ddfd":"X_all.columns","8a177e18":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef get_vect_arr(df):\n    corpus = df['tags'].astype('U').values\n    vectorizer = CountVectorizer(min_df = 0.005) #includes ytmnd\n    tags = vectorizer.fit_transform(corpus)\n    tags = pd.DataFrame(tags.toarray(),index=df.index)\n    tags.columns = vectorizer.get_feature_names()\n    tags = tags.add_prefix('tags_')\n  #  return np.concatenate((df.values, tags.toarray()), axis=1)\n    return pd.concat([df, tags], ignore_index=False, axis=1, sort=False).reset_index(drop=True)\n\nX_all = get_vect_arr(X_all)\nX_all","26a8a8c8":"# Leave only numeric variables \nX_all_num = X_all.drop(['date_added', 'name', 'origin_year', 'tags', 'details'], axis=1)\nX_all_num","21e33902":"# Don't forget to create word embeddings\nt2v_tags = pd.read_csv('..\/input\/mldub1text2vectags\/text_vectors_tags.csv') # Note that this part is done separately.\n#t2v_tags = t2v_tags.drop(['Unnamed: 0'])\ncols = [0]\nt2v_tags.drop(t2v_tags.columns[cols],axis=1,inplace=True)\nt2v_tags.columns = ['tags_' + str(col) for col in t2v_tags.columns]\n#X_num_all \nprint (\"Text2Vec for Tags var is loaded!\")\nt2v_tags","3a9630d4":"t2v_about_other = pd.read_csv('..\/input\/t2v-about-other\/t2v_about_other.csv') # Note that this part is done separately.\n#t2v_tags = t2v_tags.drop(['Unnamed: 0'])\ncols = [0]\nt2v_about_other.drop(t2v_about_other.columns[cols],axis=1,inplace=True)\nt2v_about_other.columns = ['aboutother_' + str(col) for col in t2v_about_other.columns]\n#X_num_all \nprint (\"Text2Vec for Tags var is loaded!\")\nt2v_about_other","9dd70efd":"#X_num_all_idx = X_num_all.reset_index()\nX_all_num_t2v = pd.concat([X_all_num, t2v_tags], ignore_index=False, axis=1, sort=False).reset_index(drop=True)\nX_all_num_t2v = pd.concat([X_all_num_t2v, t2v_about_other], ignore_index=False, axis=1, sort=False).reset_index(drop=True)\nX_all_num_t2v","ac2c64a9":"X1_train = X_all_num_t2v[:train_len]\nX1_test = X_all_num_t2v[train_len:]","d4abf75d":"# Checking all the dataframes I created\n%whos DataFrame","becc44ca":"def create_submition(pred,  save_name=\"submission.csv\", save=True):\n    \n    id_series = pd.Series(np.arange(0, test_len), name=\"id\")\n    pred_series = pd.Series(np.expm1(pred), name=\"target_variable\").astype('int64')\n    \n    sub_df = pd.concat([id_series, pred_series], axis=1)\n    \n    if save:\n        sub_df.to_csv(save_name, index=False)\n        print(\"\\n file saved at: {:s}\".format(\"submissions\/\"+save_name))\n    \n    return sub_df","96f9b370":"from sklearn.model_selection import cross_val_score\ndef display_scores(scores):\n    print(\"RMSE Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","5dfdaac6":"def create_model_pred(reg, X_train, X_test, save_name, y=y):\n    \n    print(\"Fitting... \")\n    reg.fit(X_train, y)\n    scores = cross_val_score(reg, X_train, y, scoring=\"neg_mean_squared_error\", cv=10)\n    rmse_scores = np.sqrt(-scores)\n    display_scores(rmse_scores)\n    print(\"Predicting... \")\n    predictions =  reg.predict(X_test)\n    \n    return create_submition(predictions, save_name)","a7cceaca":"# Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\ncreate_model_pred(lin_reg, X1_train, X1_test, save_name=\"lin_reg_submission.csv\")","a6b9532d":"from sklearn.svm import SVR\n\nsvm_reg = SVR(gamma='auto', kernel='rbf')\ncreate_model_pred(svm_reg, X1_train, X1_test, save_name=\"svm_reg_submission.csv\")","85eee219":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\n#param_grid = [{'n_estimators': [500, 1000, 5000], 'max_features': ['auto']},\n#{'bootstrap': [False], 'n_estimators': [500, 1000], 'max_features': ['auto']},]\n#forest_reg = RandomForestRegressor()\n#grid_search = GridSearchCV(forest_reg, param_grid, cv=4,scoring='neg_mean_squared_error')\n#grid_search.fit(X1_train, y.values.ravel())","6ac09589":"#grid_search.best_estimator_","db6e246c":"rf_reg = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features=2, max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=5000, n_jobs=None, oob_score=False,\n                      random_state=None, verbose=0, warm_start=False)\ncreate_model_pred(rf_reg, X1_train, X1_test, save_name=\"rf_reg_submission_Jul14.csv\")","c4d8c877":"from sklearn.metrics import mean_squared_error\nfrom sklearn import feature_selection\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\ncategorical = []\n#categorical = ['about', 'name,''origin', 'origin_place', 'origin_year', 'status', 'type', 'text']\n#categorical = ['Confirmed' , 'old']\n#categorical = X.select_dtypes(include=['object']).columns\n#categorical = ['about', 'name', 'origin', 'origin_place', 'other_text', 'tags', 'details', 'time_gap']\n\nX_catboost = X_all_num_t2v.drop(['about', 'name', 'origin', 'origin_place', 'origin_year', 'other_text', 'tags', 'details', 'time_gap'], axis=1)\n\n# Prepare Categorical Variables\ndef column_index(df, query_cols):\n    cols = df.columns.values\n    sidx = np.argsort(cols)\n    return sidx[np.searchsorted(cols,query_cols,sorter=sidx)]\n\ncategorical_features_pos = column_index(X_catboost, categorical)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_catboost, y, test_size=0.10, random_state=myseed)\n\n","2fa78466":"# Grid Search\ncb_model = CatBoostRegressor()\n\ngrid = {'learning_rate': [0.02, 0.03, 0.4],\n        'depth': [6, 7, 8, 9],\n       'l2_leaf_reg': [1, 2, 3]}         # \n\ngrid_search_result = cb_model.grid_search(grid, \n                                       X=X_train, \n                                       y=y_train, \n                                       plot=True)\n\ngrid_search_result","42e06521":"# Train Model\nprint(\"Train CatBoost Decision Tree\")\n# depth': 9, 'l2_leaf_reg': 2, 'learning_rate': 0.03\ncb_model = CatBoostRegressor(iterations=999,\n                             learning_rate=0.03,\n                             depth=9,\n                             eval_metric='RMSE',\n                             random_seed = myseed,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 75,\n                             od_wait=100)\ncb_model.fit(X_train, y_train,\n             eval_set=(X_valid,y_valid),\n             cat_features=categorical_features_pos,\n             use_best_model=True,\n             verbose=True)","fcf15171":"XGB_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n\ncreate_model_pred(XGB_model, X1_train, X1_test, save_name=\"xgb_submission.csv\")","0620de0f":"## 4.2. Some manual fixes","48336e8d":"The train dataset consists of character, numerical, and integer variables. In total, there are 15 columns, of which the last one is the `target_variable`. Below, I am displaying only a glimpse of the data. All of them are discussed in detail throughout the document.","ba25b7eb":"# 5. Results and Summary\n\nMy final sumbission consists of an average of preditions of the following models:\n\n<ol>\n<li> Linear Regression on minimal amount of features <\/li>\t\n<li> SVR on minimal amount of features <\/li>\n<li> Random Forest on minimal amount of features <\/li>\t\n<li> CatBoost on all created features <\/li>\n<li> CatBoost on all created features sans vector representation of texts <\/li>\n<li> XGboost on all created features sans vector representation of texts <\/li>\n<\/ol>\n\n\nIt has scored an RMSE of `246115.08330` on public leaderboard (place #6 by time of close) and an RMSE of `320439.72336` on private scoring leaderbaord (also place #6). I betted on a robust approach to avoid shake-up after close, and I have achieved it. However, some features may have not been cleaned all the way, resulting in slightly worse RMSE.\n\nThanks for reading throught the kernel.\n\n![](https:\/\/chappellroberts.com\/thoughts\/wp-content\/uploads\/2016\/03\/Toy-Story.jpg)\n","81c1db02":"## 4.6.4. CatBoost gradient boosting","5b5f2ab2":"### Missing Value Analysis","daa89499":"## 3.2. Setting up several custom functions right from the beginning","9476a9bd":"## `type`","04aefcf4":"##### Here we are using 2 datasets\n* 1. train\n* 2. test","69b1f96a":"## 3.4. Exploratiry Data Analysis","92f92da8":"### 4.4.2. Converting \"tags\" to a vectors based on pre-trained embeddings","49f55c5b":"Given results above, we will convert the `target_variable` into a log-form. We will use `log1p` instead of `log`:\n\n* Logarithmic transformation makes a large number of views look similar to a small number of views. An x% variation will cost the same, whatever the baseline is.\n* Logarithm of zero is undefined. `log` transformation cannot be applied directly given there can be 0 unit sales. Log1p is one of habitual ways to mitigate this, although there are many others.","93eeed7f":"## Dataset\n#### The dataset is scrape of KnowYourMeme.com on a date in June 2020. 13,188 observations in training and 8,793 observations in test.\n\n### The data fields\n<ol>\n<li> Id - unique page identifier <\/li>\n<li> name - meme name  <\/li>\n<li> Status - meme status <\/li>\n<li> Type - meme type  <\/li>\n<li> Origin_year - meme origin year  <\/li>\n<li> Origin_place - place from which the meme came  <\/li>\n<li> Date_added - the data when joke was first published  <\/li>\n<li> Views - target variable. Calculated in May\u201920 <\/li>\n<li> Videos, photos, comments - numeric meme stats  <\/li>\n<li> Tags - tags assigned to the meme  <\/li>\n<li> About - meme description  <\/li>\n<li> Origin - meme history  <\/li>\n<li> Other_text - all other text from meme page  <\/li>\n<\/ol>\n\n","100857d7":"## 3.4.2. Data \"test\"","06f68b48":"\n`origin_place`\nIf the origin place is popular that might help to viral so, it will be added to created `details` column.\n\n`other_text` and `about`\nlets add other_text and about to details column\n\n`origin` \nLet's add origin to the detials column because those has some similarities \n\n`time_gap` \nAlso a categorical varialble so it have to be encoded.","ef763470":"## 3.4.5. Histogram","a4bbadc0":"## 3.4.1. Data \"train\" ","49cc8087":"## 4.6. Fitting and Predicting","7b149654":"#### Summary of each numerical attribute","147f6685":"I am reading in the CSV\u2019s into Python as pandas dataframes.","6c53825f":"#### Grid search for random forest","afe72d17":"#### Missing values","56edd77d":"## 3.4.6. Correlation Analysis","2df413ee":"## 4.5. Helper functions for making predictions and submissions","d5adfd5d":"#### Data Structure","6ba279ea":"#### Checking for missing values in original data shows that some textual descriptions are missing, e.g. `about`, as well a big chunk of `origin` and `type` data.","91b759a8":"### 4.4.3. Concatenating \"about\" and \"other_text\" and representing as vectors","b1ec1522":"## 4.6.5. XGboost gradient boosting\n","1a313416":"## `origin_year` and `Date_added`","b869f770":"### `test`","54f0e2be":"### Numerical atgributes have less corelation","4969c0f1":"All vector representations are base on GloVe word embeddings based on Twitter data (glove.twitter.27B.25d.txt; 245.76 MB). I think Twitter embeddings could help more that those trained on Wikipedia given the context of memes.","2d37f003":"### These percentages are much higer than 5% and cannot be filled. So, potentially we need to remove these attributes from the dataset before training, or alleviate this by combining some variables.","09d423ed":"### `train`","5fe5c5ba":"## 3.3. Reading the data into this Python notebook","469700ec":"### We can clearly see missing values percentages with\n\n|                | `train`   | `test` |\n|----------------|-----------|--------|\n| 1. `type`      | 45.6%     | 44.8%  |\n| 2. `origin`    | 33.3%     | 32.3%  |\n| 3. `other_text`| 11.2%     | 11.6   |\n| 4. `about`     | 8.7%      | 8.8%   |","bccd7b37":"Setting up.","695de4bf":"Making a submistion funcion.","eca49190":"##### type has the most number of mising values, so `nan` will be replaced by `\" \"` and let's create a new column called `details`","95992112":"# 4. Feature Engineering, Modeling, and Submission\n\n## 4.1. Feature Engineering","931147f4":"## 3.4.3. Data Preprocessing and Cleaning","146dea20":"First I suggest breaking down tags strings into separate works using a trick with Vectorizer.","c60c196d":"### We can already see that we will need to fix some data.\nAt the very least, `origin_year` seems like it should be a numeric variable, not a string. Perhaps, there is a few text entries in it.   \n\nWe can also discuss here in this part what is needed to do separately on train and test before train-test join. E.g. maybe percentage of total things.","765db6cd":"Our hunch is formally validated by Shapiro test for normality returning `False`","03566e8e":"## 4.6.1. Linear Regression","ff4d1477":"## 4.6.3. Random Forest Regression","81aab1c5":"## 3.4.4. Handling & Cleaning Missing values","c51721d0":"> ## 4.6.2. Support Vector Regression","cba5347e":"### ML@DUB: Competition #1\n\n# Predicting Meme Popularity on KnowYourMeme.com\n\n![KnowYourMeme](https:\/\/i.imgur.com\/nUq7IkC.png)\n\n# 1. Executive Summary\n\nI started this competition by trying to achieve a good understanding of the dataset to improve my results and building working predictors. Initially, I stared in R, my main language of data analysis to date, but one week in the experience of Machinge Learning in R became increasinly frustrating. At that point I have pledged to myself to finally adopt Python as my main language. So I have rewritten the whole notebook from scratch. This is the first notebook I've written in Python in a year. I am very happy with this start, thanks to organizing for allowing me to find motivation to use Python for everything.\n\n\n# 2. Introduction\n\nThe ML@Dublin organizers describe it as follows: \"In this competition, you will on a knowyourmeme.com meme dataset, and predict a meme\u2019s popularity via the number of views it gets!\"\n\nWe need to predict the number of views a page gets! It was scraped from the box on the right.\n\n# 3. Setup and EDA\n\n![KnowYourMeme](https:\/\/i.imgur.com\/o7b4zoh.png)\n\n## 3.1. Loading in required libraries \n\nLoading packages used besides base Python.","f694c1d0":"#### `status` has only 5 unique values so that can be handled with one hot encoder","73ad2c4b":"## 4.4. Continued Feature Engineering on Train-Test join\n\n### 4.4.1. Preparing Features and Labels for Modeling","fd49d616":"#### We also notice that the target variable follows `log-normal` distribution much closer than `normal` distribution.","692ca636":"Let's do all those things and more as one function:","268a1001":"##### Numerical and Categorical attributes","8630dc8c":"## 4.3. Correlations","efe674cf":"Making functions for easy regression automation.","272f8fbe":"The Attribute ID obviously can be removed because it does not give any information about the target.","d412fd35":"## Unique values","b4e3d471":"## It is more convinetint the origin date and added date as taken as how old it is \n\n### Less than 1 year, `new`\n### 1-2 years `recent`\n### More than 2 years `old`"}}