{"cell_type":{"c139e05d":"code","9e096133":"code","bac20a0f":"code","25555332":"code","a6053306":"code","42ba1f8c":"code","0bc04069":"code","9d1b08b1":"code","b9482ffa":"code","663da7ff":"code","7c3a794e":"code","8ecd6b55":"code","28fde6f1":"code","95204383":"code","5b0d579b":"code","c429c755":"markdown","4e278af5":"markdown","e0d1f2a8":"markdown","a4232686":"markdown","1928dcdd":"markdown","25e9efd7":"markdown","ffbd274f":"markdown","69c56014":"markdown","7f5baef9":"markdown","d8ccf26b":"markdown","c8d262ed":"markdown","c10c29d2":"markdown","173cdfc9":"markdown"},"source":{"c139e05d":"IMAGE_PARAMS = {\n    'WIDTH': 512,\n    'HEIGHT': 512,\n    'CHANNELS': 3\n}\n\nIMAGE_DIMS = (1, IMAGE_PARAMS['HEIGHT'], IMAGE_PARAMS['WIDTH'], IMAGE_PARAMS['CHANNELS'])\n\n# The mean value of the training data of the ImageNet in BGR format\nVGG_MEAN = [123.68, 116.779, 103.939]\n\nCONTENT_IMAGE_PATH = \"..\/input\/images\/venice_italy.jpg\"\nSTYLE_IMAGE_PATH = \"..\/input\/images\/rain_princess.jpg\"\n\nCONTENT_WEIGHT = 0.0175\nSTYLE_WEIGHT = 7\nTOTAL_VARIATION_WEIGHT = 1.1\n\n# Gatys used layer \"block2_conv2\"\n# Do experiments with other layers\n# He only used one layer implying that it produces better results\nCONTENT_LAYER = \"block2_conv2\"\n\nSTYLE_LAYERS = [\n    'block1_conv2',\n    'block2_conv2',\n    'block3_conv3',\n    'block4_conv3',\n    'block5_conv3'\n]\n\nITERATIONS = 50","9e096133":"from PIL import Image\nfrom IPython.display import display\n\ndef resize_image(image_file_path):\n    img = Image.open(image_file_path)\n    img = img.resize((IMAGE_PARAMS['WIDTH'], IMAGE_PARAMS['HEIGHT']))\n    return img\n\ncontent_image = resize_image(CONTENT_IMAGE_PATH)\nstyle_image = resize_image(STYLE_IMAGE_PATH)","bac20a0f":"display(content_image)\ndisplay(style_image)","25555332":"import numpy as np\n\n# Normalize the image by the mean values of VGG and transformed to BGR (from RGB)\n# This is required so the feature maps of VGG are applied correctly\ndef normalize_by(image, bgr_mean):\n    image = np.asarray(image, dtype=\"float32\")\n    image = np.expand_dims(image, axis=0)\n\n    image[:, :, :, 0] -= bgr_mean[2]\n    image[:, :, :, 1] -= bgr_mean[1]\n    image[:, :, :, 2] -= bgr_mean[0]\n\n    # Change the order to BGR (because of VGG model uses it)\n    return image[:, :, :, ::-1]","a6053306":"from keras import backend\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16\n\ndef build_vgg_input_tensor(content_image, style_image, generated_image):\n    # Build the VGG16 model\n    content_image = backend.variable(content_image)\n    style_image = backend.variable(style_image)\n\n    input_tensor = backend.concatenate([\n        content_image,\n        style_image,\n        generated_image\n    ], axis=0)\n    \n    return input_tensor","42ba1f8c":"# Freeze the base model weights and biases, only the generated image pixel values are \"trained\"\ndef freeze_model(model):\n    for layer in model.layers:\n        layer.trainable = False\n    return model","0bc04069":"def content_loss(content_features, generated_features):\n    return backend.sum(backend.square(generated_features - content_features))","9d1b08b1":"# Also known as style matrix\ndef gram_matrix(x):\n    features = backend.batch_flatten(\n        backend.permute_dimensions(x, (2, 0, 1))\n    )\n    return backend.dot(features, backend.transpose(features))\n\ndef style_loss(style, generated, image_params):\n    height = style.shape[0]\n    width = style.shape[1]\n\n    gram_style = gram_matrix(style)\n    gram_generated = gram_matrix(generated)\n    \n    channels = image_params['CHANNELS']\n    height = image_params['HEIGHT']\n    width = image_params['WIDTH']\n    # Total style loss of the layer at hand\n    square_loss = backend.square(gram_generated - gram_style)\n    \n    # The hyper parameter from Gatys paper, does not really matter since beta will absorb this one\n    multiplier = 1. \/ (4. * (channels ** 2) * ((height * width) ** 2))\n    return backend.sum(multiplier * square_loss)\n\ndef total_style_loss(style_layers, layers, image_params):\n    loss = backend.variable(0.)\n    for layer_name in style_layers:\n        layer_features = layers[layer_name]\n        style_features = layer_features[1, :, :, :]\n    \n        combination_features = layer_features[2, :, :, :]\n    \n        style_loss_amount = style_loss(style_features, combination_features, image_params)\n        \n        # We could assign invidual weights for each layer but keep them equally important for now\n        loss = loss + (1.0 \/ len(style_layers)) * style_loss_amount\n    return loss","b9482ffa":"def total_variation_loss(image):\n    # Calculate how much horizontal and vertical neighbor pixels differ from each other\n    # Add the difference as extra cost, this way the noise of the image can be regularized\n    vertical_difference = backend.square(image[:, :-1, :-1, :] - image[:, 1:, :-1, :])\n    horizontal_difference = backend.square(image[:, :-1, :-1, :] - image[:, :-1, 1:, :])\n\n    return backend.sum(vertical_difference + horizontal_difference)","663da7ff":"def total_loss(content_layer, style_layers, vgg_model, image_params):\n    loss = backend.variable(0.)\n    layers = dict([(layer.name, layer.output) for layer in vgg_model.layers])\n\n    # Content loss\n    content_layer_features = layers[content_layer]\n    content_features = content_layer_features[0, :, :, :]\n    generated_features = content_layer_features[2, :, :, :]\n    loss = loss + CONTENT_WEIGHT * content_loss(content_features, generated_features)\n  \n    # Style loss\n    loss = loss + STYLE_WEIGHT * total_style_loss(style_layers, layers, image_params)\n    \n    # Total variation, the noisiness of the images\n    loss = loss + TOTAL_VARIATION_WEIGHT * total_variation_loss(result_image)\n    return loss","7c3a794e":"def evaluate_loss_and_gradients(generated_image, loss_and_gradients, img):\n    img = img.reshape(IMAGE_DIMS)\n    \n    # Generated image is the current input to the model\n    # We want to capture the new loss and gradients every round\n    out = backend.function([generated_image], loss_and_gradients)([img])\n    loss = out[0]\n    gradients = out[1].flatten().astype(\"float64\")\n    return loss, gradients","8ecd6b55":"# VGG normalized pixel values back to 0-255 range\ndef restore_image(img, vgg_mean):\n    img = img.reshape((\n        IMAGE_PARAMS['HEIGHT'],\n        IMAGE_PARAMS['WIDTH'],\n        IMAGE_PARAMS['CHANNELS']\n    ))\n\n    img = img[:, :, ::-1]\n    \n    img[:, :, 0] += vgg_mean[2]\n    img[:, :, 1] += vgg_mean[1]\n    img[:, :, 2] += vgg_mean[0]\n    img = np.clip(img, 0, 255).astype(\"uint8\")\n    \n    return img\n\ndef save_image(img_array, filename):\n    image = Image.fromarray(img_array)\n    image.save(filename)\n    \n    return image","28fde6f1":"from scipy.optimize import fmin_l_bfgs_b\nfrom functools import partial\n\ndef random_initial_image(image_params):\n    return np.random.uniform(\n        -128.0,\n        128.0,\n        IMAGE_DIMS\n    )\n\ndef optimize(img, result_image, loss_and_gradients, iterations, vgg_mean, save_iterations = False):\n    evaluate = partial(evaluate_loss_and_gradients, result_image, loss_and_gradients)\n\n    for i in range(iterations):\n        img, loss, info = fmin_l_bfgs_b(\n            evaluate,           # Function to minimise\n            img.flatten(),      # Initial guess\n            maxfun=20           # How many times loss and gradients are evaluated per iteration\n        )\n        \n        print('Iteration %d, current loss %d' % (i, loss))\n        \n        if save_iterations:\n            r = restore_image(img.copy(), vgg_mean)\n            filename = '.\/process\/' + str(i) + '.png'\n            save_image(r, filename)\n    return img","95204383":"# Read and normalize input images\ncontent_image = normalize_by(content_image, VGG_MEAN)\nstyle_image = normalize_by(style_image, VGG_MEAN)\n\nresult_image = backend.placeholder(IMAGE_DIMS)\n\n# Build the VGG model with the input images + placeholder for the result image'\ninput_tensor = build_vgg_input_tensor(content_image, style_image, result_image)\nvgg_model = VGG16(input_tensor=input_tensor, include_top=False)\n\n# Freeze the model weights and biases, we are only \"training\" the pixel values of the result image\nvgg_model = freeze_model(vgg_model)\n\ninitial_loss = total_loss(\n    CONTENT_LAYER,\n    STYLE_LAYERS,\n    vgg_model,\n    IMAGE_PARAMS\n)\n\n# Gradients return a list of tensors\nloss_and_gradients = [initial_loss, *backend.gradients(initial_loss, result_image)]\n\ninitial_image = random_initial_image(IMAGE_PARAMS)\nimg = optimize(\n    initial_image,\n    result_image,\n    loss_and_gradients,\n    ITERATIONS,\n    VGG_MEAN\n)","5b0d579b":"# Save the final image\nrestored = restore_image(img, VGG_MEAN)\nsave_image(restored, \".\/final.png\")","c429c755":"# Evaluation","4e278af5":"# VGG model","e0d1f2a8":"## Total variation loss\n- Without any regularization, the result image is very grainy\n- Smooth out the image with variation regularization among the neighbour pixels","a4232686":"# Image style transfer with CNN\n\nThe goal is to generate new image from two input images the way that the output image contains the content of the first input image (spatial features) and the style (texture) of the another input image. The actual generator is implemented using convolutional neural network (CNN).\n\nIn other words, define a style transfer process which modifies the content image style while preserving its content close to the original. You can also think it as a process of merging two images together resulting in an output image containing aspects from both input images.\n\nThis notebook is an implementation of the method used to extract the artistic style of an image described in the following white paper (https:\/\/arxiv.org\/pdf\/1508.06576.pdf)","1928dcdd":"# Further experiments\n- Change VGG pooling operation from max pooling to avg pooling, might produce better results","25e9efd7":"## Total loss","ffbd274f":"## Overview\n\n#### Initialization\n- Two images (content and style) from which the feature maps are extracted\n    - Content image, the one containing all the spatial features\n    - Style image, contains the overall style \/ texture\n- The feature maps are used to adjust the features of random generated initial image to be as close as possible to the extracted content and style features\n- A random generated image is passed as an input to the model\n- After the style transfer process, the output image is overpainted to the input image by adjusting the pixel values during the process\n\n#### Base model\n- In order to extract the correct feature maps from the input images (spatial features from the content image, style features from the style image), a general machine learning model is needed\n- There are several models which can be used as a base model to extract the feature maps\n    - VGG16 is arguably the most popular one, at least for this purpose\n    - Other models like VGG19, InceptionV3, and ResNet50\n- The model is only used to get the right features, it will not be trained during the process (in other words, the model weights won't be update)\n\n#### The problem definition\nThe style transfer process can be turned into a machine learning optimization problem. The optimization problem here is to minimize the loss function which is defined as follows:\n\nTotal loss = content loss + style loss\n\nContent loss = The difference in content between initial input image and the content image\nStyle loss = The difference in style between the input and the style image\n\nThe smaller the total loss is, the closer the features of the output image are to the input images => matching content and style of the original images\n\n#### The process description\n- Random initial image\n- Match the initial image's feature maps to the extracted feature map activations at chosen feature convolutional layer => backpropagates the input image pixels instead of the model weights\n\n![NST Architecture](https:\/\/github.com\/thushv89\/exercises_thushv_dot_com\/raw\/dd79478562dd4c0f53472af5b79252404e030838\/neural_style_transfer_light_on_math_ml\/nst_architecture.jpg)","69c56014":"# Demo\n##### Inputs:\n- Content image: Venice river view\n- Style image: image of the painting \"Rain pricess\" by Leonid Afremov\n- Last part is the combination of the two\n\n![Images used in the process](https:\/\/raw.githubusercontent.com\/laaksonenl\/machine-learning\/master\/cnn-style-transfer\/demo\/demo.png)","7f5baef9":"### VGG CNN architecture\n- Visual Geometry Group (VGG) achieved the object classification error rate of 7% with this model\n- Effective at object recognition\n\n#### Network details\n- Consists of 16 convolutional layers\n- ReLU as the activation function\n- 5 pooling layers\n- 3 fully connected layers\n\n![VGG vertical architecture](https:\/\/shafeentejani.github.io\/assets\/images\/style_transfer\/vgg_architecture_vertical.png)\n![VGG architecture](http:\/\/www.cs.toronto.edu\/~frossard\/post\/vgg16\/vgg16.png)","d8ccf26b":"## Content loss\n\n- The lower levels of CNNs are more focused on individual pixel values\n- The higher levels are composed from the lower levels and thereby forming actual features of the image from the more basic values\n- Calculate the root mean squared error between the activations of the generated image and the content image\n- Different feature maps of the higher layers are activated when there's different objects in the images (https:\/\/arxiv.org\/pdf\/1311.2901.pdf)\n","c8d262ed":"## Style loss\n- Calculated with a set of layers (content used only one)\n- From the quantifying point of view, style is the amount of correlation between features maps in a layer\n- Style loss = the difference of correlation between the feature maps of generated image and the style image","c10c29d2":"# Implementation","173cdfc9":"# Links\nGreat, detailed explanations and examples:\n- https:\/\/towardsdatascience.com\/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee\n- https:\/\/towardsdatascience.com\/neural-style-transfer-and-visualization-of-convolutional-networks-7362f6cf4b9b\n- https:\/\/github.com\/gsurma\/style_transfer\n\n### Images\nOriginal images used in this notebook\n- https:\/\/shafeentejani.github.io\/assets\/images\/style_transfer\/vgg_architecture_vertical.png\n- http:\/\/www.cs.toronto.edu\/~frossard\/post\/vgg16\/vgg16.png\n- https:\/\/github.com\/thushv89\/exercises_thushv_dot_com\/raw\/dd79478562dd4c0f53472af5b79252404e030838\/neural_style_transfer_light_on_math_ml\/nst_architecture.jpg"}}