{"cell_type":{"1bd97b35":"code","3fb9e77a":"code","30107acc":"code","195ac3cc":"code","5d79c9b2":"code","a93dc203":"code","827fcb9b":"code","e65bec70":"code","4abd091d":"code","59628fe6":"code","800d652e":"code","2d4037bb":"code","6c9a2bdc":"code","c0e1ccbd":"code","c4b09b50":"code","9c456d15":"code","6259a041":"code","b66bba65":"code","3d7e1ad1":"code","fc8e4fa7":"code","7f2bf397":"code","38475093":"code","f788da6b":"code","973fa71d":"code","7a317094":"code","25c371ba":"code","aa3b9412":"markdown","255b8905":"markdown","cf2c4d94":"markdown","cb7ee05f":"markdown","73aee6bb":"markdown"},"source":{"1bd97b35":"import os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer, AutoConfig\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\nfrom fastai.text import *\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Flatten\n","3fb9e77a":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","30107acc":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\n# GCS_DS_PATH = KaggleDatasets().get_gcs_path('')\n\n# Configuration\nEPOCHS = 2\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","195ac3cc":"from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler","5d79c9b2":"XLM_PROCESSED_PATH = Path(\"\/kaggle\/input\/xlmrobertabase\/xlm_roberta_processed\/\"); XLM_PROCESSED_PATH.ls()","a93dc203":"# def one_hot_encode(x): return np.array([[1,0], [0,1]])[x]","827fcb9b":"class JigsawArrayDataset(torch.utils.data.Dataset):\n    def __init__(self, input_ids:np.array, attention_mask:np.array, toxic:np.array=None):\n        self.input_ids = input_ids\n        self.attention_mask = attention_mask\n        self.toxic = toxic\n    \n    def __getitem__(self, idx):\n        xb = (tensor(self.input_ids[idx]), tensor(self.attention_mask[idx]))\n        yb = tensor(0.) if self.toxic is None else tensor(self.toxic[idx])\n        return xb,yb    \n        \n    def __len__(self):\n        return len(self.input_ids)","e65bec70":"# train_ds\ntrain_input_ids = np.load(XLM_PROCESSED_PATH\/'translated_train_inputs\/input_ids.npy')\ntrain_attetion_mask = np.load(XLM_PROCESSED_PATH\/'translated_train_inputs\/attention_mask.npy').astype(np.int32)\ntrain_toxic = np.load(XLM_PROCESSED_PATH\/'translated_train_inputs\/toxic.npy').astype(np.float32)\ntrain_lang = np.load(XLM_PROCESSED_PATH\/'translated_train_inputs\/lang.npy', allow_pickle=True)","4abd091d":"# labels for stratified batch sampler\ntrain_stratify_labels = array(s1+s2 for (s1,s2) in zip(train_lang, train_toxic.astype(str)))\nlabels2int = {v:k for k,v in enumerate(np.unique(train_stratify_labels))}\nlabels = [labels2int[o] for o in train_stratify_labels]\nbalanced_sampler = BalanceClassSampler(labels)","59628fe6":"idxs1 = list(iter(balanced_sampler))\nidxs2 = list(iter(balanced_sampler))\nidxs3 = list(iter(balanced_sampler))\nidxs4 = list(iter(balanced_sampler))\nidxs5 = list(iter(balanced_sampler))","800d652e":"# reshape for loss and metric\ntrain_toxic = train_toxic.reshape(-1,1)","2d4037bb":"train_input_ids_sampled = np.vstack([train_input_ids[idxs] for idxs in [idxs1,idxs2,idxs3,idxs4,idxs5]])\ntrain_attetion_mask_sampled = np.vstack([train_attetion_mask[idxs] for idxs in [idxs1,idxs2,idxs3,idxs4,idxs5]])\ntrain_toxic_sampled = np.vstack([train_toxic[idxs] for idxs in [idxs1,idxs2,idxs3,idxs4,idxs5]])","6c9a2bdc":"train_input_ids_sampled.shape, train_attetion_mask_sampled.shape, train_toxic_sampled.shape","c0e1ccbd":"del train_input_ids, train_attetion_mask, train_toxic, train_lang\ngc.collect()","c4b09b50":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(((train_input_ids_sampled, train_attetion_mask_sampled), train_toxic_sampled))\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)","9c456d15":"### NotFoundError: No registered 'PyFunc' OpKernel for CPU devices compatible with node {{node PyFunc}} \n# def generate(self):\n#     for idx in idxs1:\n#         yield ((train_input_ids[idx], train_attetion_mask[idx]), train_toxic[idx])\n\n# train_dataset = (\n#     tf.data.Dataset\n#     .from_generator(generate, \n#                     ((tf.int32, tf.int32), tf.float32),\n#                     ((tf.TensorShape([256]), tf.TensorShape([256])), tf.TensorShape([1]))\n#                    )\n#     .repeat()\n#     .shuffle(2048)\n#     .batch(BATCH_SIZE)\n#     .prefetch(AUTO)\n# )","6259a041":"valid_input_ids = np.load(XLM_PROCESSED_PATH\/'valid_inputs\/input_ids.npy')\nvalid_attention_mask = np.load(XLM_PROCESSED_PATH\/'valid_inputs\/attention_mask.npy').astype(np.int32)\nvalid_toxic = np.load(XLM_PROCESSED_PATH\/'valid_inputs\/toxic.npy').astype(np.float32).reshape(-1,1)\n# valid_toxic = one_hot_encode(valid_toxic).astype(np.float32)","b66bba65":"valid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(((valid_input_ids, valid_attention_mask), valid_toxic))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)","3d7e1ad1":"MODEL = 'jplu\/tf-xlm-roberta-large'","fc8e4fa7":"def get_xlm_roberta(modelname=MODEL):        \n    conf = AutoConfig.from_pretrained(modelname)\n    conf.output_hidden_states = True\n    model = TFAutoModel.from_pretrained(modelname, config=conf)\n    return model","7f2bf397":"def build_model(xlm_roberta, max_len=256, p=0.5):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n    \n    _, _, hidden_states = xlm_roberta([input_ids, attention_mask])\n    x = tf.concat(hidden_states[-2:], -1)\n    x = tf.concat((tf.reduce_mean(x, 1), tf.reduce_max(x, 1)), -1)    \n    x = Dropout(rate=0.5)(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n    \n    model = Model(inputs=(input_ids, attention_mask), outputs=out)\n    \n    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.1, name='binary_crossentropy')\n    \n    model.compile(Adam(lr=1e-5), loss=loss_fn, metrics=[tf.metrics.AUC()])\n    \n    return model","38475093":"with strategy.scope():\n    xlm_roberta = get_xlm_roberta()\n    model = build_model(xlm_roberta)\nmodel.summary()","f788da6b":"BATCH_SIZE, EPOCHS = 64*strategy.num_replicas_in_sync, 2\nBATCH_SIZE, EPOCHS","973fa71d":"checkpoint_filepath = 'bestmodel'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_auc',\n    mode='max',\n    save_best_only=True)","7a317094":"# model.evaluate((valid_input_ids[:128], valid_attention_mask[:128]), valid_toxic[:128])","25c371ba":"n_steps = len(balanced_sampler) \/\/ BATCH_SIZE\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS,\n    callbacks=[model_checkpoint_callback]\n)","aa3b9412":"https:\/\/www.kaggle.com\/keremt\/xlm-roberta-tpu-training-fastai-style","255b8905":"### Training","cf2c4d94":"### fin","cb7ee05f":"### Load Model","73aee6bb":"### Create Datasets"}}