{"cell_type":{"b9dbbe6b":"code","27b7fc14":"code","2e78f6c9":"code","d0276577":"code","bf4e2d0a":"code","81f5115f":"code","e003d1c5":"code","9d345fd7":"code","5a37fa83":"code","508f48cd":"code","c33313e2":"code","86940a29":"code","0efbce94":"code","0eaa390b":"code","0d23b38a":"code","fd17c0a7":"code","617d26d1":"code","39c7f024":"code","7a992e0c":"code","1436f0ae":"code","41e879c4":"code","2daac4de":"code","f3b480ce":"code","4530edb0":"code","aaf5076c":"code","d4dc38f0":"code","49abffd3":"code","822bbf9d":"code","8f2e7ae7":"code","b86e4d8a":"code","5eeb82f3":"code","8a8bba2a":"code","fa40d4ca":"markdown","363dfdd0":"markdown","1acd4c0e":"markdown","023c5e3d":"markdown","2983b6d3":"markdown","efe29d51":"markdown"},"source":{"b9dbbe6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","27b7fc14":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","2e78f6c9":"t_train = train['label']\nx_train = train.drop(labels = ['label'], axis = 1)","d0276577":"train.shape, t_train.shape, x_train.shape, test.shape","bf4e2d0a":"x_train2 = x_train.to_numpy()\nx_test2 = test.to_numpy()\nx_train2.shape, x_test2.shape","81f5115f":"type(x_train2), type(x_train)","e003d1c5":"x_train3 = x_train2.reshape(42000, 1, 28, 28)\nt_train3 = t_train\nx_test3 = x_test2.reshape(-1, 1, 28, 28)\nx_train3.shape, x_test3.shape, t_train3.shape","9d345fd7":"# reshape data for convolution net (warning : raw data -> overuse memory )\nsam_x_train3 = x_train3[0:2000, :, :, :]\nsam_t_train3 = t_train3[0:2000]\nsam_x_test3 = x_test3[0:2000, : , : , :]\nsam_x_train3.shape, sam_x_test3.shape, sam_t_train3.shape","5a37fa83":"def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n    \"\"\"\ub2e4\uc218\uc758 \uc774\ubbf8\uc9c0\ub97c \uc785\ub825\ubc1b\uc544 2\ucc28\uc6d0 \ubc30\uc5f4\ub85c \ubcc0\ud658\ud55c\ub2e4(\ud3c9\ud0c4\ud654).\n    \n    Parameters\n    ----------\n    input_data : 4\ucc28\uc6d0 \ubc30\uc5f4 \ud615\ud0dc\uc758 \uc785\ub825 \ub370\uc774\ud130(\uc774\ubbf8\uc9c0 \uc218, \ucc44\ub110 \uc218, \ub192\uc774, \ub108\ube44)\n    filter_h : \ud544\ud130\uc758 \ub192\uc774\n    filter_w : \ud544\ud130\uc758 \ub108\ube44\n    stride : \uc2a4\ud2b8\ub77c\uc774\ub4dc\n    pad : \ud328\ub529\n    \n    Returns\n    -------\n    col : 2\ucc28\uc6d0 \ubc30\uc5f4\n    \"\"\"\n    N, C, H, W = input_data.shape\n    out_h = (H + 2*pad - filter_h)\/\/stride + 1\n    out_w = (W + 2*pad - filter_w)\/\/stride + 1\n\n    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n    return col\n\n\ndef col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n    \"\"\"(im2col\uacfc \ubc18\ub300) 2\ucc28\uc6d0 \ubc30\uc5f4\uc744 \uc785\ub825\ubc1b\uc544 \ub2e4\uc218\uc758 \uc774\ubbf8\uc9c0 \ubb36\uc74c\uc73c\ub85c \ubcc0\ud658\ud55c\ub2e4.\n    \n    Parameters\n    ----------\n    col : 2\ucc28\uc6d0 \ubc30\uc5f4(\uc785\ub825 \ub370\uc774\ud130)\n    input_shape : \uc6d0\ub798 \uc774\ubbf8\uc9c0 \ub370\uc774\ud130\uc758 \ud615\uc0c1\uff08\uc608\uff1a(10, 1, 28, 28)\uff09\n    filter_h : \ud544\ud130\uc758 \ub192\uc774\n    filter_w : \ud544\ud130\uc758 \ub108\ube44\n    stride : \uc2a4\ud2b8\ub77c\uc774\ub4dc\n    pad : \ud328\ub529\n    \n    Returns\n    -------\n    img : \ubcc0\ud658\ub41c \uc774\ubbf8\uc9c0\ub4e4\n    \"\"\"\n    N, C, H, W = input_shape\n    out_h = (H + 2*pad - filter_h)\/\/stride + 1\n    out_w = (W + 2*pad - filter_w)\/\/stride + 1\n    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n\n    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n\n    return img[:, :, pad:H + pad, pad:W + pad]","508f48cd":"x1 = np.random.rand(1, 3, 7, 7) # (N, C, H, W)\ncol1 = im2col(x1, 5, 5, stride = 1, pad = 0)\ncol1.shape","c33313e2":"x1","86940a29":"from collections import OrderedDict","0efbce94":"def softmax(x):\n    if x.ndim == 2:\n        x = x.T\n        x = x - np.max(x, axis=0)\n        y = np.exp(x) \/ np.sum(np.exp(x), axis=0)\n        return y.T \n\n    x = x - np.max(x) # \uc624\ubc84\ud50c\ub85c \ub300\ucc45\n    return np.exp(x) \/ np.sum(np.exp(x))\n\n\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n        \n    # \ud6c8\ub828 \ub370\uc774\ud130\uac00 \uc6d0-\ud56b \ubca1\ud130\ub77c\uba74 \uc815\ub2f5 \ub808\uc774\ube14\uc758 \uc778\ub371\uc2a4\ub85c \ubc18\ud658\n    if t.size == y.size:\n        t = t.argmax(axis=1)\n             \n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) \/ batch_size","0eaa390b":"class Relu:\n    def __init__(self):\n        self.mask = None\n\n    def forward(self, x):\n        self.mask = (x <= 0)\n        out = x.copy()\n        out[self.mask] = 0\n\n        return out\n\n    def backward(self, dout):\n        dout[self.mask] = 0\n        dx = dout\n\n        return dx\n\n\nclass Sigmoid:\n    def __init__(self):\n        self.out = None\n\n    def forward(self, x):\n        out = sigmoid(x)\n        self.out = out\n        return out\n\n    def backward(self, dout):\n        dx = dout * (1.0 - self.out) * self.out\n\n        return dx\n\n\nclass Affine:\n    def __init__(self, W, b):\n        self.W = W\n        self.b = b\n        \n        self.x = None\n        self.original_x_shape = None\n        # \uac00\uc911\uce58\uc640 \ud3b8\ud5a5 \ub9e4\uac1c\ubcc0\uc218\uc758 \ubbf8\ubd84\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        # \ud150\uc11c \ub300\uc751\n        self.original_x_shape = x.shape\n        x = x.reshape(x.shape[0], -1)\n        self.x = x\n\n        out = np.dot(self.x, self.W) + self.b\n\n        return out\n\n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout, axis=0)\n        \n        dx = dx.reshape(*self.original_x_shape)  # \uc785\ub825 \ub370\uc774\ud130 \ubaa8\uc591 \ubcc0\uacbd(\ud150\uc11c \ub300\uc751)\n        return dx\n\n\nclass SoftmaxWithLoss:\n    def __init__(self):\n        self.loss = None # \uc190\uc2e4\ud568\uc218\n        self.y = None    # softmax\uc758 \ucd9c\ub825\n        self.t = None    # \uc815\ub2f5 \ub808\uc774\ube14(\uc6d0-\ud56b \uc778\ucf54\ub529 \ud615\ud0dc)\n        \n    def forward(self, x, t):\n        self.t = t\n        self.y = softmax(x)\n        self.loss = cross_entropy_error(self.y, self.t)\n        \n        return self.loss\n\n    def backward(self, dout=1):\n        batch_size = self.t.shape[0]\n        if self.t.size == self.y.size: # \uc815\ub2f5 \ub808\uc774\ube14\uc774 \uc6d0-\ud56b \uc778\ucf54\ub529 \ud615\ud0dc\uc77c \ub54c\n            dx = (self.y - self.t) \/ batch_size\n        else:\n            dx = self.y.copy()\n            dx[np.arange(batch_size), self.t] -= 1\n            dx = dx \/ batch_size\n        \n        return dx\n\n    ################# NEW ###################\n\nclass Convolution:\n    def __init__(self, W, b, stride=1, pad=0):\n        self.W = W\n        self.b = b\n        self.stride = stride\n        self.pad = pad\n        \n        # \uc911\uac04 \ub370\uc774\ud130\uff08backward \uc2dc \uc0ac\uc6a9\uff09\n        self.x = None   \n        self.col = None\n        self.col_W = None\n        \n        # \uac00\uc911\uce58\uc640 \ud3b8\ud5a5 \ub9e4\uac1c\ubcc0\uc218\uc758 \uae30\uc6b8\uae30\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        FN, C, FH, FW = self.W.shape\n        N, C, H, W = x.shape\n        out_h = 1 + int((H + 2*self.pad - FH) \/ self.stride)\n        out_w = 1 + int((W + 2*self.pad - FW) \/ self.stride)\n\n        col = im2col(x, FH, FW, self.stride, self.pad)\n        col_W = self.W.reshape(FN, -1).T\n\n        out = np.dot(col, col_W) + self.b\n        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n\n        self.x = x\n        self.col = col\n        self.col_W = col_W\n\n        return out\n\n    def backward(self, dout):\n        FN, C, FH, FW = self.W.shape\n        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n\n        self.db = np.sum(dout, axis=0)\n        self.dW = np.dot(self.col.T, dout)\n        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n\n        dcol = np.dot(dout, self.col_W.T)\n        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n\n        return dx\n\n\nclass Pooling:\n    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n        self.pool_h = pool_h\n        self.pool_w = pool_w\n        self.stride = stride\n        self.pad = pad\n        \n        self.x = None\n        self.arg_max = None\n\n    def forward(self, x):\n        N, C, H, W = x.shape\n        out_h = int(1 + (H - self.pool_h) \/ self.stride)\n        out_w = int(1 + (W - self.pool_w) \/ self.stride)\n\n        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n        col = col.reshape(-1, self.pool_h*self.pool_w)\n\n        arg_max = np.argmax(col, axis=1)\n        out = np.max(col, axis=1)\n        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n\n        self.x = x\n        self.arg_max = arg_max\n\n        return out\n\n    def backward(self, dout):\n        dout = dout.transpose(0, 2, 3, 1)\n        \n        pool_size = self.pool_h * self.pool_w\n        dmax = np.zeros((dout.size, pool_size))\n        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n        dmax = dmax.reshape(dout.shape + (pool_size,)) \n        \n        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n        \n        return dx","0d23b38a":"class SimpleConvNet:\n    \"\"\"\ub2e8\uc21c\ud55c \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd\n    \n    conv - relu - pool - affine - relu - affine - softmax\n    \n    Parameters\n    ----------\n    input_size : \uc785\ub825 \ud06c\uae30\uff08MNIST\uc758 \uacbd\uc6b0\uc5d4 784\uff09\n    hidden_size_list : \uac01 \uc740\ub2c9\uce35\uc758 \ub274\ub7f0 \uc218\ub97c \ub2f4\uc740 \ub9ac\uc2a4\ud2b8\uff08e.g. [100, 100, 100]\uff09\n    output_size : \ucd9c\ub825 \ud06c\uae30\uff08MNIST\uc758 \uacbd\uc6b0\uc5d4 10\uff09\n    activation : \ud65c\uc131\ud654 \ud568\uc218 - 'relu' \ud639\uc740 'sigmoid'\n    weight_init_std : \uac00\uc911\uce58\uc758 \ud45c\uc900\ud3b8\ucc28 \uc9c0\uc815\uff08e.g. 0.01\uff09\n        'relu'\ub098 'he'\ub85c \uc9c0\uc815\ud558\uba74 'He \ucd08\uae43\uac12'\uc73c\ub85c \uc124\uc815\n        'sigmoid'\ub098 'xavier'\ub85c \uc9c0\uc815\ud558\uba74 'Xavier \ucd08\uae43\uac12'\uc73c\ub85c \uc124\uc815\n    \"\"\"\n    def __init__(self, input_dim=(1, 28, 28), \n                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n                 hidden_size=100, output_size=10, weight_init_std=0.01):\n        filter_num = conv_param['filter_num']\n        filter_size = conv_param['filter_size']\n        filter_pad = conv_param['pad']\n        filter_stride = conv_param['stride']\n        input_size = input_dim[1]\n        conv_output_size = (input_size - filter_size + 2*filter_pad) \/ filter_stride + 1\n        pool_output_size = int(filter_num * (conv_output_size\/2) * (conv_output_size\/2))\n\n        # \uac00\uc911\uce58 \ucd08\uae30\ud654\n        self.params = {}\n        self.params['W1'] = weight_init_std * \\\n                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n        self.params['b1'] = np.zeros(filter_num)\n        self.params['W2'] = weight_init_std * \\\n                            np.random.randn(pool_output_size, hidden_size)\n        self.params['b2'] = np.zeros(hidden_size)\n        self.params['W3'] = weight_init_std * \\\n                            np.random.randn(hidden_size, output_size)\n        self.params['b3'] = np.zeros(output_size)\n\n        # \uacc4\uce35 \uc0dd\uc131\n        self.layers = OrderedDict()\n        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n                                           conv_param['stride'], conv_param['pad'])\n        self.layers['Relu1'] = Relu()\n        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n        self.layers['Relu2'] = Relu()\n        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n\n        self.last_layer = SoftmaxWithLoss()\n\n    def predict(self, x):\n        for layer in self.layers.values():\n            x = layer.forward(x)\n\n        return x\n\n    def loss(self, x, t):\n        \"\"\"\uc190\uc2e4 \ud568\uc218\ub97c \uad6c\ud55c\ub2e4.\n        Parameters\n        ----------\n        x : \uc785\ub825 \ub370\uc774\ud130\n        t : \uc815\ub2f5 \ub808\uc774\ube14\n        \"\"\"\n        y = self.predict(x)\n        return self.last_layer.forward(y, t)\n\n    def accuracy(self, x, t, batch_size=100):\n        if t.ndim != 1 : t = np.argmax(t, axis=1)\n        \n        acc = 0.0\n        \n        for i in range(int(x.shape[0] \/ batch_size)):\n            tx = x[i*batch_size:(i+1)*batch_size]\n            tt = t[i*batch_size:(i+1)*batch_size]\n            y = self.predict(tx)\n            y = np.argmax(y, axis=1)\n            acc += np.sum(y == tt) \n        \n        return acc \/ x.shape[0]\n\n    def gradient(self, x, t):\n        \"\"\"\uae30\uc6b8\uae30\ub97c \uad6c\ud55c\ub2e4(\uc624\ucc28\uc5ed\uc804\ud30c\ubc95).\n        Parameters\n        ----------\n        x : \uc785\ub825 \ub370\uc774\ud130\n        t : \uc815\ub2f5 \ub808\uc774\ube14\n        Returns\n        -------\n        \uac01 \uce35\uc758 \uae30\uc6b8\uae30\ub97c \ub2f4\uc740 \uc0ac\uc804(dictionary) \ubcc0\uc218\n            grads['W1']\u3001grads['W2']\u3001... \uac01 \uce35\uc758 \uac00\uc911\uce58\n            grads['b1']\u3001grads['b2']\u3001... \uac01 \uce35\uc758 \ud3b8\ud5a5\n        \"\"\"\n        # forward\n        self.loss(x, t)\n\n        # backward\n        dout = 1\n        dout = self.last_layer.backward(dout)\n\n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n\n        # \uacb0\uacfc \uc800\uc7a5\n        grads = {}\n        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n\n        return grads","fd17c0a7":"net = SimpleConvNet()\nnet","617d26d1":"# conv - relu - pool - affine - relu - affine - softmax sequantially execute forward()\npred1 = net.predict(sam_x_train3)","39c7f024":"loss1 = net.loss(sam_x_train3, sam_t_train3)\nloss1","7a992e0c":"accu1 = net.accuracy(sam_x_train3, sam_t_train3)\naccu1","1436f0ae":"grad1 = net.gradient(sam_x_train3, sam_t_train3)\ngrad1.keys()","41e879c4":"class AdaGrad:\n\n    \"\"\"AdaGrad\"\"\"\n\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        self.h = None\n        \n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n            \n        for key in params.keys():\n            self.h[key] += grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] \/ (np.sqrt(self.h[key]) + 1e-7)","2daac4de":"net2 = SimpleConvNet()\n\niter_max = 1000\ntrain_size = x_train3.shape[0]\nbatch_size = 100\nlr = 0.01\n\ntrain_loss_list = []\ntrain_acc_list = []\niter_per_epoch = max(train_size \/ batch_size, 1)","f3b480ce":"# SGD\nfor i in range(iter_max):\n    batch_mask = np.random.choice(train_size,batch_size)\n    x_batch = x_train3[batch_mask]\n    t_batch = t_train3[batch_mask]\n    \n    grad = net2.gradient(x_batch, t_batch)\n    \n    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n        net2.params[key] -= lr * grad[key]\n    \n    loss = net2.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n    \n    if i % iter_per_epoch == 0:\n        train_acc = net2.accuracy(x_train3, t_train3)\n        train_acc_list.append(train_acc)\n\ndel x_train3, t_train3, grad\n    ","4530edb0":"train_acc_list, train_acc","aaf5076c":"x_test3.shape","d4dc38f0":"# pred_range = list(range(0, 28000+1, 4000))\n# len(pred_range), pred_range","49abffd3":"# pred_iter = x_test3.shape[0]\n# predbatch_size = 4000\n\n# rst = np.array([])\n\n# pred_interval = list(range(0, pred_iter+1, predbatch_size))\n# for i in range(7) :\n#     t_test = net2.predict(x_test3[pred_interval[i]:pred_interval[i+1],:,:,:])\n#     t_pred = np.argmax(t_test, axis = 1)\n#     rst = np.append(rst, t_pred, axis = 0)\n# rst.shape","822bbf9d":"# sub = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\n# sub['Label'] = rst\n# sub.to_csv('.\/sub1.csv', index = False)","8f2e7ae7":"t_test1 = net2.predict(x_test3[0:5000,:,:,:])\nt_pred1 = np.argmax(t_test1, axis = 1)\nt_test2 = net2.predict(x_test3[5000:10000,:,:,:])\nt_pred2 = np.argmax(t_test2, axis = 1)\nt_test3 = net2.predict(x_test3[10000:15000,:,:,:])\nt_pred3 = np.argmax(t_test3, axis = 1)","b86e4d8a":"t_test4 = net2.predict(x_test3[15000:20000,:,:,:])\nt_pred4 = np.argmax(t_test4, axis = 1)\nt_test5 = net2.predict(x_test3[20000:25000,:,:,:])\nt_pred5 = np.argmax(t_test5, axis = 1)\nt_test6 = net2.predict(x_test3[25000:28001,:,:,:])\nt_pred6 = np.argmax(t_test6, axis = 1)","5eeb82f3":"rst1 = np.append(t_pred1, t_pred2, axis = 0)\nrst2 = np.append(rst1, t_pred3, axis = 0)\nrst3 = np.append(rst2, t_pred4, axis = 0)\nrst4 = np.append(rst3, t_pred5, axis = 0)\nrst5 = np.append(rst4, t_pred6, axis = 0)\nrst5.shape","8a8bba2a":"sub = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\nsub['Label'] = rst5\nsub.to_csv('.\/sub2.csv', index = False)","fa40d4ca":"## 1-3 one iter - pred, loss, accuracy and gradient : # of samples 2000","363dfdd0":"## 1-2 class & functions definition","1acd4c0e":"# **2. 10000 iteration**","023c5e3d":"## 1-1. im2col \nch 7.4.3","2983b6d3":"# FOR NOVICE 3 (including me) - Deep Learning from Scratch 1 ch1 ~ ch5 (my exercise)\n\n! After \"MNIST for novice 2 (Deep Learning from Scratch 1)\" notebook. (I'm a beginner too.. haha)\n\n\n\n!Please understand that my native language is not English.\n\n**!if you fine something wrong in my code, please feel free to let me know!!**\n\n----------------------------------------------------------------------------------------\n**\uc774 \ub178\ud2b8\ubd81\uc740 \uc81c\uac00 \uacf5\ubd80\ud55c \ucf54\ub4dc\ub97c \uacf5\uc720\ud55c \uac83\uc785\ub2c8\ub2e4**\n\n! \uc55e\uc5d0 \uc791\uc131\ud55c \"MNIST for novice 2 (Deep Learning from Scratch 1)\" notebook\uc5d0 \uc774\uc5b4\uc9c4 \ub0b4\uc6a9\uc785\ub2c8\ub2e4. (\uc800\ub3c4 \ucd08\uc2ec\uc790\uc785\ub2c8\ub2e4..\u314e\u314e)\n\n\"MNIST for novice 2 (Deep Learning from Scratch 1)\"\uc5d0\uc11c\ub294 \ucc45\uc5d0\uc11c \uc815\uc758\ud55c TwoLayerNet \ud568\uc218\ub97c \ud65c\uc6a9\ud574\uc11c \uc2e4\uc81c MNIST \ub370\uc774\ud130\ub97c \ud559\uc2b5\ud558\uace0, \uc608\uce21\ud574\uc11c \uc81c\ucd9c\ud574 \ubcf4\uc558\uc2b5\ub2c8\ub2e4.\n\n**\uc774 notebook\uc5d0\uc11c\ub294 ch 7.4.3\uc5d0 im2col \ud568\uc218\ub97c \uc774\uc6a9\ud574 convolution \ud074\ub798\uc2a4\ub97c \uad6c\ud604\ud55c \ud6c4, forward \ud568\uc218\uc758 \uc791\ub3d9 \uacfc\uc815\uc744 \ud558\ub098\uc529 \ub72f\uc5b4\ubd05\ub2c8\ub2e4. \uadf8\ub7f0 \ub4a4, CNN\uc744 \uad6c\ud604\ud558\uc5ec MNIST \ub370\uc774\ud130\ub97c \ud559\uc2b5\ud55c \ub4a4, test\uc5d0 \uc801\uc6a9\ud574 \uc81c\ucd9c\ud569\ub2c8\ub2e4. \uc774\ub54c 6\uc7a5\uc5d0 \uc788\ub294 \uac00\uc911\uce58 \uac31\uc2e0 \ubc29\ubc95 \uc911 Adagrad\ub97c \ucd94\uac00\uc801\uc73c\ub85c \uc0ac\uc6a9\ud574 \ubcfc \uac81\ub2c8\ub2e4.**\n\n\uc774 \ub178\ud2b8\ubd81\uc744 \ubcf4\uae30 \uc55e\uc11c, 6\uc7a5, 7\uc7a5\uc758 \ub0b4\uc6a9\uc744 \ud55c\ubc88\uc740 \uc815\ub3c5\ud558\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4.\n\n> 1. convolution class\uc758 forward \ud568\uc218\uc758 \uc791\ub3d9 \uacfc\uc815\uc744 \ubd05\ub2c8\ub2e4. - \uc608\uc815\n> 2. ch 7.5\uc758 figure\uc5d0 \ub098\uc628 \ud615\uc2dd\ub300\ub85c CNN\uc744 \uc815\uc758\ud558\uace0, MNIST\ub97c \ud6c8\ub828\uc2dc\ud0a8\ub4a4, test set\uc5d0 \uc801\uc6a9 \ud6c4 submit\ud569\ub2c8\ub2e4. \uc774 \ub54c \uac00\uc911\uce58 \uac31\uc2e0 \ubc29\ubc95 \uc911 Adagrad\ub97c \uc801\uc6a9\ud574 \ubd05\ub2c8\ub2e4. - \uc608\uc815\n\n\uc870\uae08\uc529 \uacf5\ubd80\ud574\uac00\uba74\uc11c \ucd94\uac00\ud560 \uc608\uc815\uc785\ub2c8\ub2e4.\n\n**\ud2c0\ub9b0 \ubd80\ubd84 \uc788\uc73c\uba74 \ud3b8\ud558\uac8c \ub9d0\uc500\ud574\uc8fc\uc138\uc694!**","efe29d51":"# **1. convolution class define**"}}