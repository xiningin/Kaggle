{"cell_type":{"d3f54139":"code","3b611135":"code","a20d94b5":"code","feba0b5c":"code","3e3831cb":"code","c4e0794f":"code","a55f9e9c":"code","3087f1f1":"code","16f2d77b":"code","54f69e49":"code","28f67698":"code","2a2d8dc4":"code","e0c59a52":"code","ba7eb9a3":"code","3c8ce661":"markdown","9dcc3344":"markdown","f0d19504":"markdown","a13a0d6d":"markdown"},"source":{"d3f54139":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tqdm\nimport shutil\nimport matplotlib.pyplot as plt","3b611135":"fn_num = 0\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if fn_num < 5:\n            print(os.path.join(dirname, filename))\n        fn_num += 1\nprint('Num of images:', fn_num)","a20d94b5":"CONTENT_DIR = '\/kaggle\/input\/flowers-recognition\/flowers'\nlabels = os.listdir(CONTENT_DIR)\nprint('Labels:', labels)","feba0b5c":"TRAIN_DIR = '\/kaggle\/content\/train'\nVALID_DIR = '\/kaggle\/content\/valid'\nfor label in labels:\n    source = os.path.join(CONTENT_DIR, label)\n    images = os.listdir(source)\n    print('images in {}: {}'.format(label, len(images)))\n    \n    split = int(len(images) * 0.8)\n    train, valid = images[:split], images[split:]\n    \n    for image in tqdm.tqdm(train):\n        destination = os.path.join(TRAIN_DIR, label)\n        os.makedirs(destination, exist_ok=True)\n        shutil.copy(os.path.join(source, image), destination)\n        \n    for image in tqdm.tqdm(valid):\n        destination = os.path.join(VALID_DIR, label)\n        os.makedirs(destination, exist_ok=True)\n        shutil.copy(os.path.join(source, image), destination)","3e3831cb":"# !rm -rf \/kaggle\/content\n# !ls -la \/kaggle\/content\/train\/daisy\/ | wc -l","c4e0794f":"BATCH_SIZE = 128\nIMAGE_SHAPE = 128","a55f9e9c":"image_generator = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    rescale=1.\/255\n)\n\ntrain_data = image_generator.flow_from_directory(\n    directory=TRAIN_DIR,\n    target_size=(IMAGE_SHAPE, IMAGE_SHAPE),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    class_mode='binary'\n)","3087f1f1":"augmented_images = [train_data[0][0][i] for i in range(5)]\nfig, axes = plt.subplots(1, 5, figsize=(20, 20))\nfor img, ax in zip(augmented_images, axes.flatten()):\n    ax.imshow(img)\nplt.tight_layout()\nplt.show() ","16f2d77b":"valid_generator = ImageDataGenerator(rescale=1.\/255)\nvalid_data = valid_generator.flow_from_directory(\n    directory=VALID_DIR,\n    target_size=(IMAGE_SHAPE, IMAGE_SHAPE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False\n)","54f69e49":"model = tf.keras.models.Sequential([    \n    # full parameter names for example\n    tf.keras.layers.Conv2D(\n        filters=16,\n        kernel_size=(3, 3),\n        activation='relu',\n        padding='same',\n        input_shape=(IMAGE_SHAPE, IMAGE_SHAPE, 3)\n    ),\n    tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    \n    # dropout layer\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=512, activation='relu'),\n    tf.keras.layers.Dense(units=5, activation='softmax')\n])","28f67698":"model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","2a2d8dc4":"model.summary()","e0c59a52":"EPOCHS = 60\nhistory = model.fit_generator(\n    generator=train_data,\n    steps_per_epoch=(train_data.n + BATCH_SIZE - 1) \/\/ BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=valid_data,\n    validation_steps=(valid_data.n + BATCH_SIZE - 1) \/\/ BATCH_SIZE,\n    verbose=1\n)","ba7eb9a3":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(EPOCHS), history.history['accuracy'], label='train')\nplt.plot(range(EPOCHS), history.history['val_accuracy'], label='valid')\nplt.legend(loc='lower right')\nplt.title('Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(EPOCHS), history.history['loss'], label='train')\nplt.plot(range(EPOCHS), history.history['val_loss'], label='valid')\nplt.legend(loc='upper left')\nplt.title('Loss (sparse_categorical_crossentropy)')\n\nplt.show()","3c8ce661":"# Model","9dcc3344":"<pre>\nCreate structure:\n|__ train\n    |______ daisy:     [1.jpg, 2.jpg, 3.jpg ...]\n    |______ dandelion: [1.jpg, 2.jpg, 3.jpg ...]\n    |______ rose:      [1.jpg, 2.jpg, 3.jpg ...]\n    |______ sunflower: [1.jpg, 2.jpg, 3.jpg ...]\n    |______ tulip:     [1.jpg, 2.jpg, 3.jpg ...]\n |__ valid\n    |______ daisy:     [800.jpg, 801.jpg, 802.jpg ...]\n    |______ dandelion: [800.jpg, 801.jpg, 802.jpg ...]\n    |______ rose:      [800.jpg, 801.jpg, 802.jpg ...]\n    |______ sunflower: [800.jpg, 801.jpg, 802.jpg ...]\n    |______ tulip:     [800.jpg, 801.jpg, 802.jpg ...]\n<\/pre>","f0d19504":"# Augmentation","a13a0d6d":"# Prepare dataset"}}