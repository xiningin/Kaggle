{"cell_type":{"5ab11414":"code","4194364f":"code","efdf2b0a":"code","326ae83e":"code","865ad7fa":"code","218c4099":"code","0ec48fb6":"code","c27ee552":"code","02e91773":"code","034a2401":"code","0819f478":"code","4e863960":"code","461c4d9a":"code","38782296":"code","bca88343":"code","afc95c06":"code","92de0368":"code","541b7e3b":"code","3736a49d":"code","7dce75a7":"code","079249fd":"code","03ea09ef":"code","b1dafef9":"code","2ed6a069":"code","2b8eca07":"code","3ac97406":"code","86e82eff":"code","a14d61e4":"code","9277b710":"code","2a81a8b4":"code","28c8fcc3":"code","925cc94a":"code","3aa487a7":"code","dd4b04fd":"code","e96cb1fe":"code","4c12acfa":"code","05f0ea38":"code","791f280b":"code","b5ed23ef":"code","830ec8c7":"code","04582353":"code","f9ecd1a7":"code","6d35b1ce":"code","37daf31e":"code","e7042bbf":"code","87b68fd3":"code","47fa0727":"code","b4278424":"code","a5d59ba7":"code","2617b103":"code","6484fde8":"code","4a87a41f":"code","c6247171":"code","78993f3b":"code","898391bc":"code","017aa93f":"code","686ca550":"code","582e925e":"code","8cd75407":"code","1f05d995":"code","951cb09b":"code","4370ee58":"code","e528f0a7":"code","993257bb":"code","01788a44":"code","fe6ca5ed":"code","f74ec3dc":"code","e631153f":"code","1fefc0d2":"code","463f2751":"code","ad390f13":"code","c70dc7d7":"code","567d0da4":"code","2104f0e2":"code","61b583d2":"code","fdce65c1":"code","842b8e39":"code","430f8082":"code","6b292625":"code","51666d58":"code","8fc5da49":"code","8bfd1628":"code","03ea85c7":"code","f5a36c37":"markdown","ddd300ea":"markdown","7a9e0cbd":"markdown","fcdfe1fd":"markdown","de58f818":"markdown","89b21f20":"markdown","ac23e7e2":"markdown","34788042":"markdown","bd404590":"markdown","c8533f73":"markdown","25e0a878":"markdown","a1f0774f":"markdown","127f8018":"markdown","5cc35850":"markdown","3b16ec18":"markdown","b48fd2d2":"markdown","7c4ea7cd":"markdown","90b39cd7":"markdown","af57fe39":"markdown","da391334":"markdown","d847e1e5":"markdown","9c3225ec":"markdown","ef7a6978":"markdown","5d35ca15":"markdown","7a773e28":"markdown","86c625b3":"markdown","649b1c44":"markdown","6927f3df":"markdown","8f15998c":"markdown"},"source":{"5ab11414":"#Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4194364f":"# Data loading\ndf_train=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test=pd.read_csv(\"..\/input\/titanic\/test.csv\")","efdf2b0a":"df_train.head()","326ae83e":"df_test.head()","865ad7fa":"df_test[\"Survived\"]=np.nan\nsurvived=df_test.pop(\"Survived\")\ndf_test.insert(1,\"Survived\",survived)","218c4099":"df_test.head()","0ec48fb6":"df=pd.concat([df_train,df_test])","c27ee552":"df","02e91773":"### DataSanity check\ndf.shape","034a2401":"df.head()","0819f478":"# We shall make PassengerId as index for our dataset.\ndf.set_index(\"PassengerId\",inplace=True)","4e863960":"df.head()","461c4d9a":"# We shall drop the Name & Ticket variable as it won't influence the target variable.\ndf.drop([\"Name\",\"Ticket\"],inplace=True,axis=1)","38782296":"df.head()","bca88343":"df.info()","afc95c06":"### There are null values in \"Age\",\"Cabin\" & \"Embarked\" and there are more missing values in Cabin variable.\n### Let's check the percentage of the missing values in Cabin column.\nsum(df.Cabin.isnull())\/len(df.Cabin)*100\n","92de0368":"### Percantage of the missing values in Cabin are high and as it is more than 50%. We shall drop this Cabin variable.\ndf.drop([\"Cabin\"],axis=1,inplace=True)","541b7e3b":"df.head()","3736a49d":"# Let's check the imputation method for \"Age\" variable.\nsns.distplot(df.Age)","7dce75a7":"df.Age.mean()","079249fd":"df.Age.median()","03ea09ef":"### Here, Age variable is clearly normally distributed. So, we shall impute the missing values with mean value\ndf.Age=df.Age.fillna(df.Age.mean())","b1dafef9":"sum(df.Age.isnull())","2ed6a069":"# let's leave embarked column as it is! As the missing values are very less 3 and this won't reflect any changes in the model.","2b8eca07":"df.head()","3ac97406":"df.info()","86e82eff":"df.head()","a14d61e4":"df.info()","9277b710":"# By Analysing we can say that categorical and continous variables are:\ncat=[\"Survived\",\"Pclass\",\"Sex\",\"Embarked\"]\ncont=[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]","2a81a8b4":"def num_plot(i):\n    plt.figure(figsize=(15,3))\n    ax1=plt.subplot(121)\n    sns.barplot(y=df[i],x=df[\"Survived\"],ax=ax1)\n    plt.title(\"Barplot distribution of people Survive status vs {}\".format(i))\n    ax2=plt.subplot(122)\n    sns.boxplot(y=df[i],x=df[\"Survived\"],ax=ax2)\n    plt.title(\"Boxplot distribution of people Survive status vs {}\".format(i))\n    plt.show()\n    ","28c8fcc3":"for j in cont:\n    num_plot(j)","925cc94a":"def cat_plot(i):\n    plt.figure(figsize=(15,3))\n    ax1=plt.subplot(121)\n    df[i].value_counts().plot.pie(ax=ax1)\n    plt.title(\"Pie chart distribution of '{}' variable\".format(i))\n    ax2=plt.subplot(122)\n    sns.countplot(df[i],ax=ax2,hue=df[\"Survived\"])\n    plt.title(\"Countplot distribution of '{}' variable for each survive status of passenger\".format(i))\n    plt.show()","3aa487a7":"cat1=[\"Pclass\",\"Sex\",\"Embarked\"]\nfor i in cat1:\n    cat_plot(i)\n    print(\"*\"*80)","dd4b04fd":"cat","e96cb1fe":"### Creating dummy variables for \"Pclass\" and \"Embarked\".\nPclass=pd.get_dummies(df['Pclass'],prefix=\"Pclass_\",drop_first=True)\nEmbarked=pd.get_dummies(df['Embarked'],prefix=\"Embarked_\",drop_first=True)","4c12acfa":"#Concatinating dummy variables with the \"df\" dataframe\ndf=pd.concat([df,Pclass,Embarked],axis=1)","05f0ea38":"df.head()","791f280b":"# Dropping the original categorical variables\ndf.drop([\"Pclass\",\"Embarked\"],axis=1,inplace=True)","b5ed23ef":"df.head()","830ec8c7":"# Mapping the binary values of \"Sex\" variable into 0 as female and 1 as male.\ndf.Sex=df.Sex.map({\"male\":0,\"female\":1})","04582353":"df.Sex","f9ecd1a7":"df_train=df.loc[:891]","6d35b1ce":"df_test=df.loc[892:]","37daf31e":"df_test.head()","e7042bbf":"df.describe()","87b68fd3":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()","47fa0727":"df_train[cont]=scaler.fit_transform(df_train[cont])","b4278424":"df_train[cont].describe()","a5d59ba7":"y_train=df_train.pop(\"Survived\")","2617b103":"X_train=df_train","6484fde8":"X_train.head()","4a87a41f":"import statsmodels.api as sm","c6247171":"log=sm.GLM(y_train,(sm.add_constant(X_train)),family=sm.families.Binomial())\nlog.fit().summary()","78993f3b":"X_train1=X_train.drop([\"Embarked__Q\"],axis=1)","898391bc":"X_train1.head()","017aa93f":"log=sm.GLM(y_train,(sm.add_constant(X_train1)),family=sm.families.Binomial())\nlog.fit().summary()","686ca550":"X_train2=X_train1.drop([\"Parch\"],axis=1)","582e925e":"X_train2.head()","8cd75407":"log=sm.GLM(y_train,(sm.add_constant(X_train2)),family=sm.families.Binomial())\nlog.fit().summary()","1f05d995":"X_train3=X_train2.drop([\"Fare\"],axis=1)","951cb09b":"X_train3.head()","4370ee58":"log=sm.GLM(y_train,(sm.add_constant(X_train3)),family=sm.families.Binomial())\nlog=log.fit()\nlog.summary()","e528f0a7":"from statsmodels.stats.outliers_influence import variance_inflation_factor","993257bb":"# Create \"V\" dataframe.\nV=pd.DataFrame()\nV[\"features\"]=X_train3.columns\nV[\"Vif's\"]=[variance_inflation_factor(X_train3.values,i) for i in  range(len(X_train3.columns))]\nV.sort_values(by=\"Vif's\",ascending=False)","01788a44":"y_train_predicted=log.predict(sm.add_constant(X_train3))","fe6ca5ed":"prob_train_prediction=pd.DataFrame()\nprob_train_prediction[\"Actual Servived status\"]=y_train.astype(\"int\")\nprob_train_prediction[\"Predicted Servivability Probability\"]=y_train_predicted\n\n# Let's assume that threshold value as 0.5 randomly and find the predcitive status.\nprob_train_prediction[\"Predicted Servived status\"]=y_train_predicted.map(lambda x:1 if x>0.5 else 0)\n\nprob_train_prediction.head()","f74ec3dc":"### Let's evaluate the metrics to check whether the model predcits correctly or not.\nfrom sklearn import metrics","e631153f":"metrics.confusion_matrix(prob_train_prediction[\"Actual Servived status\"],prob_train_prediction[\"Predicted Servived status\"])","1fefc0d2":"fpr,tpr,threshold=metrics.roc_curve(prob_train_prediction[\"Actual Servived status\"],prob_train_prediction[\"Predicted Servivability Probability\"],\n                  drop_intermediate=False)\nauc=metrics.roc_auc_score(prob_train_prediction[\"Actual Servived status\"],prob_train_prediction[\"Predicted Servivability Probability\"])\nsns.lineplot(fpr,tpr,label=\"ROC curve area=%0.1f\"%auc)\nsns.lineplot([0,1],[0,1])\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.show()","463f2751":"precision, recall, threshold=metrics.precision_recall_curve(prob_train_prediction[\"Actual Servived status\"],prob_train_prediction[\"Predicted Servivability Probability\"])","ad390f13":"sns.lineplot(threshold,precision[:-1])\nsns.lineplot(threshold,recall[:-1]);","c70dc7d7":"df_test.head()","567d0da4":"# Creating X and y variables:\ny_test=df_test.pop(\"Survived\")","2104f0e2":"X_test=df_test","61b583d2":"#Scaling the numerical columns of the test dataset.\ndf_test[cont]=scaler.transform(df_test[cont])","fdce65c1":"df_test[cont].describe()","842b8e39":"y_test_predicted=log.predict(sm.add_constant(df_test[X_train3.columns]))","430f8082":"y_test_predicted.head()","6b292625":"prob_test_prediction=pd.DataFrame()\nprob_test_prediction[\"Predicted Servivability Probability\"]=y_test_predicted\n\n# Making predictions using optimum threshold value i.e around 0.5.\nprob_test_prediction[\"Predicted Servived status\"]=y_test_predicted.map(lambda x:1 if x>0.5 else 0)\n\nprob_test_prediction.head()","51666d58":"### Creating the final predcited dataset\nfinal_predicted=prob_test_prediction.drop([\"Predicted Servivability Probability\"],axis=1)","8fc5da49":"final_predicted.rename({\"Predicted Servived status\":\"Survived\"},axis=1,inplace=True)","8bfd1628":"final_predicted.head()","03ea85c7":"final_predicted.to_csv(\"Submission_Titanic.csv\")","f5a36c37":"### Plotting Roc Curve:","ddd300ea":"### Now finally, our variables are significant and free from mutlicollinearity. So, now we shall use this model for futher process.","7a9e0cbd":"### Now split the test and train dataset with the same original rows.","fcdfe1fd":"### From above confusion matrix, we shall evulate parameters as given below for a threshold of 0.5\n\n* True positive(TP)  = 240\n* True Negative(TN)  = 475\n* False Positive(FP) = 74\n* False Negative(FN) = 102\n* Accuracy           = (TP+TN)\/(TP+TN+FN+FP)=>(240+475)\/(240+475+102+74)=> 0.802\n        \n* True postive Rate\/Sensitivity\/Recall = TP\/Actual postives in dataset =>240\/(102+240) => 0.70\n* False postive Rate\/1-Specificity     = FP\/Actual Negatives in dataset =>74\/(74+475)=> 0.13\n* Recall=FP\/(FP+TP)=74\/(74+240)=>0.23\n                                   ","de58f818":"From the above there is __clear trade off in between 0.4 to 0.6 threshold value.__ So, what we have assumed __0.5 threshold value is good.__","89b21f20":"### Scaling the numerical variables","ac23e7e2":"### Exploratory Data Analysis:\n* Before doing that let's create categorical list as cat and continous list  as cont.","34788042":"### Model Building\n\n* We shall use statsmodels package to find the optimal classification model","bd404590":"As the build model is __inclined towards y-axis and having area greater than 50%.__ We can say that the build model is absouletly working great","c8533f73":"### Dummy variable creation for categorical variables:","25e0a878":"### Creating a new dataset \"prob_test_prediction\"","a1f0774f":"We can observe that test dataset starts from index 892 and not having the \"Survived\" variable. For better yeild of the results let's create a survived column and merge the two datasets intially as \"df\".","127f8018":"### Takeaways from above graphs: \n* Age Variable:\n    * Barplot - People having high age were died more than with lesser age.\n    * Boxplot - There are travellers with age between 20 and 40. Simultaneously, they were equally dead and suvived in these ages.\n* SibSp Variable:\n    * Barplot - Travellers having more SibSp were died more. \n    * Boxplot - We can intuitively tell that maxmimum tarvellers are having 0 to 2 SibSp.\n* Parch Variable:\n    * Barplot - Travellers having high Parch values were survived more.\n    * Boxplot - Maximum travellers who were died are not having any Parch.\n* Fare Variable:\n    * Barplot - Travellers who spent more on tickets were survived more.\n    * Boxplot - There are some travellers who spent equally as those spent by died travllers.","5cc35850":"### Creating X(target variable) and y(predictor dataset)","3b16ec18":"Now, P value for Parch is 0.419 which means this variable is not much significant. So we shall drop this \"Parch\" variable.","b48fd2d2":"### Categorical column Analysis","7c4ea7cd":"### Creating a new dataset \"prob_train_prediction\"","90b39cd7":"P value for Embarked_Q is 0.927 which means this variable is not much significant. So we shall drop this \"Embarked__Q\" variable.","af57fe39":"* For a threshold value of __0.5__ , our model accuracy is __80.2%__, Sensitivity is __70%__, False Positivity rate is __13%__ which looks good but __we should find the optimal threshold value to get best results.__ This can be done by using the __Receiving Operating Characteristic curve(ROC).__","da391334":"* Let's find the predicted probabilities in train dataset using our trained model.","d847e1e5":"### Numerical Analysis vs target variable.","9c3225ec":"### Understanding the target variable\n* Here target variable is \"Survived\" and it is has only two unique values i.e either 0-Not survived , 1- for survived.\n* So, as our target variable is of categorical type we can use classification supervised machine learning model.","ef7a6978":"### Let's find the optimum threshold value:","5d35ca15":"Here, we shall use MinMax also called for numerical columns conversion as if we see \"Fare\" column.There were sudden increase of value from 75th percentile to maximum value. It wil help us to squeeze the data along with outliers to 0 to 1.","7a773e28":"We can observe that test dataset starts from index 892. For better yeild of the results let's merge the two datasets intially as \"df\".","86c625b3":"### Takeaways from above graph:\n* Pclass variable:\n    * Pie chart: More passengers travelled in 3rd class.\n    * Count plot: Ratio of passengers that are survived high are from 1st class.\n* Sex Variable:\n    * Pie chart : More male passengers travelled.\n    * Count plot : Similiarly, more percentage of the male travellers were dead compared to female travellers.\n* Embarked Variable:\n    * Pie chart: Most of the passengers are from Southamptom.\n    * Countplot: Passengers from Southampton were mostly died and ratio of passengers from Cherbourg were survived high.","649b1c44":"Now the p values of the variables are significant so let's see the whether any multicollinearity is there between variables.","6927f3df":"Now, P value for Fare is 0.435 which means this variable is not much significant. So we shall drop this \"Fare\" variable.","8f15998c":"### Making predictions on test dataset."}}