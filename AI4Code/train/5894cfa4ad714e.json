{"cell_type":{"d181b8b8":"code","bd97b343":"code","4587ec83":"code","63218bb2":"code","f75073c4":"code","f59349a3":"code","b25b5b63":"code","e86c9f37":"code","c1162bbd":"code","567dbaa2":"code","bcdf5535":"code","e5d000fe":"code","e2302493":"code","c3ac471f":"code","7db98b87":"code","5ecf4883":"code","1d47c0d3":"code","1d35cd91":"code","233f2f44":"code","c65d8983":"code","425b1af9":"code","f675aff3":"code","0192252f":"code","2b542cfc":"code","6322d5f9":"code","5225c6d9":"code","26049feb":"code","d2fe2654":"markdown","d0ab1b22":"markdown","da703e08":"markdown","8e3f3020":"markdown","d2355985":"markdown","d0ff9c84":"markdown","bfe1881e":"markdown","91138b1e":"markdown","ea0b8e5a":"markdown","294b4ef9":"markdown","f5f6e23b":"markdown","fdb371ec":"markdown","08a03087":"markdown","1ad01339":"markdown","f31be259":"markdown","5e5405a2":"markdown","26783a6e":"markdown","da128ad0":"markdown","12506680":"markdown","c6c4e4e6":"markdown","1c8d013b":"markdown","502e441a":"markdown","a2ea26fe":"markdown","e1d48913":"markdown","b669b42a":"markdown","02a491bb":"markdown","6304b267":"markdown","efdc7733":"markdown"},"source":{"d181b8b8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # plotting\nimport matplotlib.pyplot as plt # plots handling\nimport os # directory\/files handling\n\n# Kaggle file system setup\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bd97b343":"#\u00a0Load the csv files using the read_csv function\ndata  = pd.read_csv('..\/input\/quarterly-census-of-employment-and-wages-may-2020\/data.csv')\nnaics = pd.read_csv('..\/input\/quarterly-census-of-employment-and-wages-may-2020\/naics_4_digit.csv')\n\n#\u00a0We direclty define the target variable for the model part\nTARGET = 'may2020_empl'","4587ec83":"data.head()","63218bb2":"naics.head()","f75073c4":"print(f\"data  : There are {data.shape[0]} rows and {data.shape[1]} columns.\")\nprint(f\"naics : There are {naics.shape[0]} rows and {naics.shape[1]} columns.\")","f59349a3":"# Descriptive stats for data.csv\ndata.describe()","b25b5b63":"print(f\"There are {data.isna().sum().sum()} missing values.\")","e86c9f37":"#\u00a0Values for the figure size\nWIDTH  = 20\nHEIGHT = 6","c1162bbd":"# Set up subplots with size\nfig, ax = plt.subplots(1, 2, figsize=(WIDTH, HEIGHT))\n\n# Generate a distribution plot\nsns.histplot(x=data[TARGET], ax=ax[0])\nax[0].title.set_text('Total employment distribution')\n\n# Generate a distribution plot with log scale\nsns.histplot(x=data[TARGET].apply(np.log), ax=ax[1])\nax[1].title.set_text('Total employment distribution (Log scale)')\n\n# Display a plot\nplt.show()","567dbaa2":"correlation = data.corr()[TARGET]\n\nsorted_correlation = correlation.apply(np.abs).sort_values(ascending=False)\n\n# No. of variables to display\nN = 30\n\nplt.figure(figsize=(WIDTH, HEIGHT))\ncorrelation[sorted_correlation.index].iloc[1:N].plot(kind='bar')\nplt.show()","bcdf5535":"def print_industry(row):\n    print(\"_\"*60)\n    print(f\"Code  : {row['industry_code']}\")\n    print(f\"Title : {row['industry_title']}\")","e5d000fe":"# Extract the variables names\nnames = sorted_correlation[1:3].index.tolist()\n\n# Extract the industry_code from the names\nindustry_codes = [int(name.split('_')[1]) for name in names]\n\n# Extract the industry title from industry code\n_ = naics.query(f'industry_code in {industry_codes}').apply(lambda x: print_industry(x), axis=1)","e2302493":"def handle_string(string):\n    return ''.join(string.split(',')[1:]).strip()\n\ndata['area_state'] = data['area_title'].apply(lambda string: handle_string(string)).astype('category')","c3ac471f":"print(f\"There are {len(data['area_state'].unique())} U.S. stats\/territories.\")","7db98b87":"from wordcloud import WordCloud # plot strings\n\nwordcloud = WordCloud(background_color=\"black\", colormap=\"tab20b\").generate_from_frequencies(data['area_state'].value_counts())\n\nplt.figure(figsize=(WIDTH, HEIGHT))\nplt.title('U.S. States\/Territories | Size by frequency')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.show()","5ecf4883":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nscaler = StandardScaler(with_mean=False)","1d47c0d3":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\n\nSEED  = 42\n\ndef modelling(X, y, model, f_importance=False, fit=False):\n    # Type of modelling : Train & Test basic splitting\n    importance, tt_train_score, tt_test_score  = train_test_model(X, y, model, f_importance=f_importance)\n    \n    # Type of modelling : KFold Train & Test splitting\n    kf_train_score, kf_test_score = kfold_model(X, y, model)\n    \n    if fit:\n        model.fit(X, y)\n        return model, tt_test_score, kf_test_score\n    \n    return (importance, tt_train_score, tt_test_score, kf_train_score, kf_test_score) if f_importance else (tt_train_score, tt_test_score, kf_train_score, kf_test_score)\n\ndef train_test_model(X, y, model, f_importance=True):\n    \n    importance = None\n    \n    # Train & test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=SEED)\n    \n    # Fitting\n    model.fit(X_train, y_train)\n    \n    # Scores\n    train_pred = model.predict(X_train)\n    test_pred  = model.predict(X_test)\n    \n    train_score = mean_squared_error(y_train, model.predict(X_train), squared=False)\n    test_score = mean_squared_error(y_test, model.predict(X_test), squared=False)\n    \n    # Feature importances\n    if f_importance:\n        try:\n            try:\n                importance = model.feature_importances_\n            except:\n                try:\n                    importance = model.coef_\n                except:\n                    pass\n            \n            features   = X.columns.tolist()\n            importance = pd.Series(index=features, data=importance)\n            return importance, train_score, test_score\n        except:\n            pass\n        \n    # Model, RMSE on train, RMSE on test\n    return importance, train_score, test_score\n\ndef kfold_model(X, y, model):\n    # Parameters & variables\n    K            = 5\n    kf           = KFold(K)\n    train_scores = list() \n    test_scores  = list() \n    \n    # Looping over the folds\n    for train_index, test_index in kf.split(X):\n        \n        # Define datasets\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        # Fitting\n        model.fit(X_train, y_train)\n        \n        # Scores\n        train_pred = model.predict(X_train)\n        test_pred  = model.predict(X_test)\n        \n        train_score = mean_squared_error(y_train, model.predict(X_train), squared=False)\n        test_score = mean_squared_error(y_test, model.predict(X_test), squared=False)\n        \n        # Increments\n        train_scores.append(train_score)\n        test_scores.append(test_score)\n    \n    kf_train_score = np.mean(train_scores)\n    kf_test_score  = np.mean(test_scores)\n    \n    return kf_train_score, kf_test_score","1d35cd91":"from xgboost import XGBRegressor\n\npipeline = make_pipeline(scaler, XGBRegressor(n_estimators=2500, random_state=42))","233f2f44":"features = data.columns.tolist()\n\nfeatures.remove(TARGET)  # Remove the target variable\nfeatures.remove('area_title')  # Not processed here\nfeatures.remove('area_fips')  # Not processed here\nfeatures.remove('may2020_empl_yy_pc')  # Not processed here\nfeatures.remove('area_state')  # Not processed here","c65d8983":"train = data.copy()\n\nX = pd.concat([train[features], pd.get_dummies(train['area_state'])], axis=1)\ny = train[TARGET].apply(np.log)\n\ntt_train_score, tt_test_score, kf_train_score, kf_test_score = modelling(X, y, pipeline)","425b1af9":"SPACE = 16\n\ndef print_score(score=None, label=None):\n    print(f\"{label.rjust(SPACE)} : {str(score).rjust(SPACE)}\")\n    \nprint_score(score=tt_train_score, label=\"Train RMSE\")\nprint_score(score=tt_test_score, label=\"Test RMSE\")\nprint_score(score=kf_train_score, label=\"Train CV RMSE\")\nprint_score(score=kf_test_score, label=\"Test CV RMSE\")","f675aff3":"_ = pipeline.fit(X, y)","0192252f":"# Compute the feature importance early on\nimportance = pd.Series(pipeline[1].feature_importances_, index=X.columns)","2b542cfc":"plt.figure(figsize=(WIDTH, HEIGHT))\nsns.histplot(data[TARGET].apply(np.log), label=\"True\", alpha=0.2, color=\"navy\")\nsns.histplot(pipeline.predict(X), label=\"Predictions\", alpha=0.15, color=\"crimson\")\nplt.title(\"Real vs Predictions distribution\")\nplt.show()","6322d5f9":"N = 5\n\nfor col in importance.sort_values(ascending=False)[:N].index.tolist():\n    plt.figure(figsize=(WIDTH, HEIGHT))\n    sns.scatterplot(x=data[col], y=data[TARGET], label=\"True\", alpha=.8, color=\"navy\")\n    sns.scatterplot(x=data[col], y=np.exp(pipeline.predict(X)), label=\"Predictions\", alpha=.4, color=\"crimson\")\n    plt.title(f\"Real vs Predictions | {col} vs {TARGET}\")\n    plt.show()","5225c6d9":"N = 30\n\nplt.figure(figsize=(WIDTH, HEIGHT))\nimportance.sort_values(ascending=False)[:N].plot(kind='bar')\nplt.title(\"Feature importance\")\nplt.xticks(rotation=60)\nplt.show()","26049feb":"N = 5\n\n# Extract the variables names\nnames = importance.sort_values(ascending=False)[:N].index.tolist()\n\n# Extract the industry_code from the names\nindustry_codes = [int(name.split('_')[1]) for name in names]\n\n# Extract the industry title from industry code and print\nprint(\"Feature importance | Top 5 Features\")\n_ = naics.query(f'industry_code in {industry_codes}').apply(lambda x: print_industry(x), axis=1)","d2fe2654":"> We can see that we have a **highly skewed distribution** on the left plot and a good habit in order to check if despite the skewed distribution, we have a Gaussian distribution. The advice here to check that we look at the data but in **log scale**! As we can see on the right, the plot has a somehow Gaussisan distribution!","d0ab1b22":"### Plotting","da703e08":"## Load the data\n\nLet's load the following data using the `pandas` package:\n- *data.csv* (2.52 MB)\n- *naics_4_digit.csv* (15.66 KB)","8e3f3020":"___\n# Quick EDA\n\nLet's take a look at our data, we won't review eveything because there are a lot of columns (307)... So in this case we will take a look at the target variable and `'area_fips'`.\n","d2355985":"## Target","d0ff9c84":"### Correlation\n\nLet's check if there are any correlation between the target and the other columns.","bfe1881e":"> We can see that there are 2 variables that are `> 0.5`, let's check what do they represent.","91138b1e":"##\u00a0Missing values\nIn most of the cases, real data contains missing values so we will check if our data contains some.","ea0b8e5a":"##\u00a0Descriptive statistics","294b4ef9":"> Happily the data is complete! **No missing values** here!","f5f6e23b":"## Quick look\nLet's have a look at how our data looks like with the `.head(**kwargs)` method of `pandas.DataFrame` object.","fdb371ec":"# Quarterly Census of Employment and Wages, May 2020 \n\n<div style=\"align:center\">\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1130943\/1897863\/8b001332a1abf0ba5fcb7a2b642e870b\/dataset-cover.jpg?t=2021-02-01-00-43-00\">\n<\/div>","08a03087":"___\n# Post-modelling analysis\n\n## Real vs Prediction","1ad01339":"___\n# Basic insights","f31be259":"### Manipulation","5e5405a2":"## Feature importance","26783a6e":"# Table of contents\n\n- Setting up\n- Basic insights\n- Quick EDA\n- Pre-processing\n- Modelling\n- Post-modelling analysis","da128ad0":"### Distribution","12506680":"## Introduction\n\n> \"In May 2020, the United States suffered one of the largest single-month job losses in its history as state and local government imposed public policy measures to slow the spread of the COVID-19 virus which, in many cases, forced businesses to close or significantly curtail business activity. But not all counties experienced job losses compared to the prior year. Instead, some supported job gains. Can location quotients, which measure the importance of jobs in specific industries, be efficient predictors of job losses? Are there certain businesses, or groups of businesses, that had an effect on job gains\/losses?\" - ***@davincermak (Davin Cermak)***\n\nThe file data.csv contains Quarterly Census of Employment and Wage data published by the U.S. Bureau of Labor Statistics ([https:\/\/www.bls.gov\/cew\/](https:\/\/www.bls.gov\/cew\/)). The data is combined data from 2019 and May 2020, for each county, or county-equivalent, in the U.S.\n\nareafips: FIPS codes for U.S. county and county-equivalent entities\nareatitle: Name of county\nmay2020emplyypc: Year-over-year percent change in county total employment in May 2020\nmay2020empl: Count of total employment in May 2020\nnaics1111 to naics9999: Employment concentration\/location quotient for each 4-digit NAICS sectors. A location quotient less than 1.0 indicates that the count's share of sector employment to total employment is lower than the same ratio in the U.S overall, while a location quotient greater than 1.0 means that the county's share of sector employment to total employment is higher than the U.S. ratio. A description of the NAICS 4-digit numeric codes can be found at [link](https:\/\/www.bls.gov\/cew\/classifications\/industry\/industry-titles.htm).\n\n\n## Issue : How closely can you predict job losses?\n\nIn May 2020, the United States experienced some of the worst single-month job losses in its history as state and local governments implemented public policies aimed at reducing the spread of the COVID-19 virus.\n\n- Using private sector employment concentrations (locations quotients) averages from 2019, how well can this information predict the year-over-year percentage change in total employment in May 2020?\n- **What sectors, or group of sectors, were the most important predictors of employment change?**\n- Post your data analysis and modeling descriptions using Notebooks\n- Concentrate not only on producing the most accurate model but also **explain what the model means in practical terms**.\n- **Use the Root Mean Square Error (RMSE) metric** to evaluate the data.","c6c4e4e6":"___\n# Modelling","1c8d013b":"##\u00a0Dimensions\nLet's check the number of rows and columns of our data.","502e441a":"## Interpretation","a2ea26fe":"## Area\nThis variable is a string so we will have to do some manipulation on it...","e1d48913":"___\n# Setting up\n\n## Imports\n\nLet's first import the main packages that we will use. Though other packages may be needed but they will be introduced later on...","b669b42a":"___\n# Pre-processing\n\nIn order to pre-process the data we will create pipelines with the following steps :\n- Numerical variables |\u00a0Standardization\n- Categorical variables |\u00a0One Hot Encoding","02a491bb":"\u26a0\ufe0f In construction...","6304b267":"> Well it seems that a lot of rows are data located in **Georgia** and **Texas**!","efdc7733":"___\n# Conclusion\n\nThis dataset was cool to use though there was a lot of theory to learn in order to make great interpretation... I will try to think more on this topic when I have time.\n\nThanks you for looking at my work and thank you **[@davincermak](https:\/\/www.kaggle.com\/davincermak)** for the data!"}}