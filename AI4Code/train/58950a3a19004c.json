{"cell_type":{"fe0b558c":"code","e2c30eed":"code","c79c8b86":"code","889880a3":"code","a13c1c26":"code","73f897c8":"code","70635238":"code","c54347aa":"code","4dc95511":"code","09508286":"code","f68b7355":"code","674f3a8f":"code","5a8fb205":"code","164ac069":"code","8730818c":"code","46237355":"code","ef7e9355":"code","beb13df2":"code","6e7e5a61":"code","81a717bb":"code","33887c1d":"code","21f1a0e6":"code","47beff2f":"code","6a5210cf":"code","4487e214":"code","bb78a096":"code","fece1dd2":"code","0ae5a25a":"code","597e390e":"code","2017894e":"code","c3a18aae":"code","32a5676c":"code","ce59bf4d":"code","7cc5c731":"code","54778f95":"code","cf13fe3e":"code","4456624a":"code","b4185aba":"code","555ab7fa":"code","5582eee5":"code","bfcf2c27":"code","281d119b":"code","65702296":"code","0ebaba23":"code","b1d0ac85":"code","9134421f":"code","d40a554b":"code","349dba87":"code","d867295e":"markdown","0f199f0f":"markdown","683acb75":"markdown","07e17ab6":"markdown"},"source":{"fe0b558c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, f1_score, accuracy_score\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, ElasticNet","e2c30eed":"df = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")","c79c8b86":"df.head()","889880a3":"# understand the data\ndef check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(3))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(3))\n    \ncheck_df(df)","a13c1c26":"print(\"Num of Object Variables:\", df.select_dtypes(object).shape[1])\nprint(\"Num of Integer Variables:\", df.select_dtypes(\"integer\").shape[1])\nprint(\"Num of Float Variables:\", df.select_dtypes(\"float\").shape[1])","73f897c8":"# Null values\ndf.isnull().sum()","70635238":"# check the missing values\ndf[df.Salary.isnull()==True].head()","c54347aa":"# drop null values because they are what we need to predict(salary). Check again.\ndf.dropna(inplace=True)\ndf.isnull().sum()","4dc95511":"# Descriptive Analysis\ndf.describe([0.05,0.25,0.50,0.75,0.95,0.99]).T","09508286":"# Outliers\nsns.boxplot(x = df[\"Salary\"])\nplt.show()","f68b7355":"# determine the threshold values for outliers\n\ndef outlier_thresholds(dataframe, col_name):\n    quartile1 = dataframe[col_name].quantile(0.05)\n    quartile3 = dataframe[col_name].quantile(0.95)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","674f3a8f":"# limits for salary\n\nlow_limit,up_limit = outlier_thresholds(df, \"Salary\")\nprint(\"Low Limit : {0}  Up Limit : {1}\".format(low_limit,up_limit))","5a8fb205":"# replace outliers with thresholds\n\ndef replace_with_thresholds(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if low_limit > 0:\n        dataframe.loc[(dataframe[col_name] < low_limit), col_name] = low_limit\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit\n    else:\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit","164ac069":"# numerical variables\nnum_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\nprint('\\n',num_cols)","8730818c":"#Outlier analysis for numerical variables\nfor col in num_cols:\n    replace_with_thresholds(df, col)\n    \n# Outliers in the dependent variable have been eliminated\nsns.boxplot(x = df[\"Salary\"])\nplt.show()","46237355":"# check_outliers for salary\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\ncheck_outlier(df, \"Salary\")","ef7e9355":"# correlation analysis\ndf.corr(method=\"pearson\")","beb13df2":"# correlation analysis before feature engineering\n\nplt.figure(figsize=(20,10))\nsns.heatmap(df.corr(),annot=True );","6e7e5a61":"def grab_col_names(dataframe, cat_th=10, car_th=20):    \n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car","81a717bb":"cat_cols, num_cols, cat_but_car = grab_col_names(df)\ncat_cols","33887c1d":"num_cols","21f1a0e6":"cat_but_car","47beff2f":"# encoding\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nbinary_cols = [col for col in df.columns if df[col].dtype not in [int, float] and df[col].nunique() == 2]","6a5210cf":"for i in binary_cols:\n    df[i] = le.fit_transform(df[i])","4487e214":"#one hot encoding\nohe_cols = [col for col in df.columns if 10 >= df[col].nunique() > 2]\ndf=pd.get_dummies(df, columns=ohe_cols, drop_first=True)","bb78a096":"#standardization\nfrom sklearn.preprocessing import MinMaxScaler\n\nfor col in num_cols:\n        transformer = MinMaxScaler().fit(df[[col]])\n        df[col] = transformer.transform(df[[col]])","fece1dd2":"# set model before feature engineering\nfrom sklearn.linear_model import LinearRegression\ny = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=42)","0ae5a25a":"# Faeture Importance\ndef plot_importance(model, features, num=len(X)):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    \n#plot_importance(LR, X_test)","597e390e":"def train_model(model,name):\n    \n    model.fit(X_train, y_train)\n    y_pred_1 = model.predict(X_train)\n    y_pred = model.predict(X_test)\n    \n    print(model.intercept_)\n    print(model.coef_)\n    \n    return model, y_pred_1, y_pred","2017894e":"LinReg, y_pred_1, y_pred = train_model(LinearRegression(),\"LinearRegressor\")","c3a18aae":"print(\"Train RMSE:\", \"{:,.2f}\".format(np.sqrt(mean_squared_error(y_train, y_pred_1))))\nprint(\"Test RMSE:\", \"{:,.2f}\".format(np.sqrt(mean_squared_error(y_test, y_pred))), \"\\n\")\n          \nprint(\"Train MAE:\", \"{:,.2f}\".format(mean_absolute_error(y_train, y_pred_1)))\nprint(\"Test MAE:\", \"{:,.2f}\".format(mean_absolute_error(y_test, y_pred)), \"\\n\")\n      \nprint(\"Train R^2:\", \"{:,.2f}\".format(r2_score(y_train, y_pred_1)))\nprint(\"Test R^2:\", \"{:,.2f}\".format(r2_score(y_test, y_pred)))","32a5676c":"LinReg.score(X_test, y_test)","ce59bf4d":"# As we can see, our accuracy is so low, therefore we need to create different features\n# Below will be updated constantly as I work on it.","7cc5c731":"df = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf.dropna(inplace=True)\ndf.isnull().sum()\nlow_limit,up_limit = outlier_thresholds(df, \"Salary\")\nnum_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\n\nfor col in num_cols:\n    replace_with_thresholds(df, col)\n    \ncheck_outlier(df, \"Salary\")","54778f95":"df[\"CAtBat_y\"]=df[\"CAtBat\"]\/df[\"Years\"]\ndf[\"CHits_y\"]=df[\"CHits\"]\/df[\"Years\"]\ndf[\"CHmRun_y\"]=df[\"CHmRun\"]\/df[\"Years\"]\ndf[\"CRuns_y\"]=df['CRuns']\/df[\"Years\"]\ndf[\"CRBI_y\"]=df[\"CRBI\"]\/df[\"Years\"]\ndf[\"CWalks_y\"]=df[\"CWalks\"]\/df[\"Years\"]","cf13fe3e":"df[\"n_Walks\"]=df[\"Walks\"]\/df[\"CWalks\"]\ndf[\"n_Atbat\"]=df[\"AtBat\"]\/df[\"CAtBat\"]\ndf[\"n_Hits\"]=df[\"Hits\"]\/df[\"CHits\"]\ndf[\"n_Runs\"]=df[\"Runs\"]\/df['CRuns']\ndf[\"n_RBI\"]=df[\"RBI\"]\/df[\"CRBI\"]","4456624a":"df[\"RBI_Walks\"]=df[\"RBI\"]*df[\"Walks\"]","b4185aba":"df['Runs_CRuns_y'] = df['Runs'] \/ (df['CRuns'] * df['Years'])","555ab7fa":"df.loc[(df[\"Runs\"] > df[\"CRuns_y\"]),\"new_runs\"] = 1\ndf.loc[(df[\"Runs\"] <= df[\"CRuns_y\"]),\"new_runs\"] = 0","5582eee5":"def gen(df,c,a,b,t,y,y1,y2):\n    df.loc[(df[c] < a), t] = y\n    df.loc[(df[c] >= a) & (df[c] < b), t] = y1\n    df.loc[(df[c] >= b), t] = y2\n    return df[t]\ngen(df,\"Years\",4,11,\"experience\",\"beginner\",\"intermediate\",\"senior\")\ngen(df,\"Errors\",3,11,\"condition\",\"verygood\",\"good\",\"bad\")\ndf.head()","bfcf2c27":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","281d119b":"binary_cols = [col for col in df.columns if df[col].dtype not in [int, float]and df[col].nunique() == 2]                                     \n","65702296":"for i in binary_cols:\n    df[i] = le.fit_transform(df[i])","0ebaba23":"ohe_cols = [col for col in df.columns if 10 >= df[col].nunique() > 2]\n                                       \ndf=pd.get_dummies(df, columns=ohe_cols, drop_first=True)","b1d0ac85":"for col in num_cols:\n        transformer = MinMaxScaler().fit(df[[col]])\n        df[col] = transformer.transform(df[[col]])","9134421f":"X = df.drop([\"Salary\"], axis=1)\ny = df[\"Salary\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=42)\nLinReg2, y_pred_n_2, y_pred_2 = train_model(LinearRegression(),\"LinearRegressor\")","d40a554b":"print(\"Train RMSE:\", \"{:,.2f}\".format(np.sqrt(mean_squared_error(y_train, y_pred_n_2))))\nprint(\"Test RMSE:\", \"{:,.2f}\".format(np.sqrt(mean_squared_error(y_test, y_pred))), \"\\n\")\n          \nprint(\"Train MAE:\", \"{:,.2f}\".format(mean_absolute_error(y_train, y_pred_n_2)))\nprint(\"Test MAE:\", \"{:,.2f}\".format(mean_absolute_error(y_test, y_pred)), \"\\n\")\n      \nprint(\"Train R^2:\", \"{:,.2f}\".format(r2_score(y_train, y_pred_n_2)))\nprint(\"Test R^2:\", \"{:,.2f}\".format(r2_score(y_test, y_pred)))","349dba87":"LinReg2.score(X_test, y_test)","d867295e":"![](https:\/\/metshotcorner.com\/wp-content\/uploads\/2016\/05\/2016.05.25.jpg)","0f199f0f":"# Linear Regression on Hitters Baseball Dataset ","683acb75":"Just with a simple feature engineering, we boost our score from 0.29 to 0.65","07e17ab6":"Variables were 20, now we have 36"}}