{"cell_type":{"6fb3b7d1":"code","be2bb959":"code","ab1bf477":"code","a5dfe4c8":"code","d8a79704":"code","5d8908bb":"code","2be941a6":"code","b80e65fc":"code","a49cb101":"code","d658c6a4":"code","9b0a15b9":"code","00327df6":"code","4011dc40":"code","dcb28ddb":"code","51025dd0":"code","3f333454":"code","45f2da53":"code","aac38c0f":"code","a3f9c45d":"code","169e1b7b":"code","d43e810a":"code","95fd601f":"code","a0b779f9":"code","94ec70c2":"code","8cb6c2ef":"code","9ebf2954":"code","e486fca1":"code","3d5f9ab2":"code","5a6ae5d2":"code","39373d26":"code","cb37625d":"code","777fbb00":"code","564ff929":"code","e41b7926":"code","60ac446b":"code","bad13fdb":"code","70e508c2":"code","bd726a94":"code","3d8041c2":"code","ef79ccf1":"code","73a8eeb5":"code","0cd8f52a":"code","4dbffa84":"code","1d646c18":"code","84ca6ece":"code","8b01f739":"code","da461164":"code","8910e0d0":"code","ccef2685":"code","0e75f456":"code","32ae95d1":"code","618c97f1":"code","be1028e7":"code","13de5d71":"code","32835451":"code","8a52bdc1":"code","21eb99b3":"code","5afa913f":"code","9b67e9e7":"code","3922dab9":"code","94fbdce0":"code","f7f4425a":"code","65815832":"code","0aafd3cd":"code","b7a902ac":"code","f462e2b6":"code","5eb1c1ef":"code","a201a6bc":"code","60d8843a":"code","f3a0c5ce":"code","4765c8a4":"code","eba812b6":"code","ac836d25":"code","3ed59682":"code","40d24ec3":"code","9a721490":"code","2739984d":"code","d23b8f06":"code","2e3318da":"code","1c2d524d":"code","464e243d":"markdown","aed56eb2":"markdown","013691e0":"markdown","a01ee012":"markdown","948ad932":"markdown","bd164d50":"markdown","beb44420":"markdown","a2d2842a":"markdown","e987ac83":"markdown","9c61abca":"markdown","4346941a":"markdown","7b70e233":"markdown","e9522382":"markdown","7732796c":"markdown","01caee3a":"markdown","e6f04164":"markdown","fe55b2f9":"markdown","1009cef2":"markdown","2f60c24c":"markdown","ea5e8369":"markdown","febe195b":"markdown","4da5d037":"markdown","7ff1336e":"markdown","8536e80c":"markdown","30a6aadc":"markdown","15f83de5":"markdown","f3a76421":"markdown","b48d3c01":"markdown","6fd4bb3a":"markdown","daebf76d":"markdown","dc2045d1":"markdown","35372fba":"markdown","07d47190":"markdown","c5e91956":"markdown","d4f79a9b":"markdown","fded9995":"markdown","639e521e":"markdown","c2f982f3":"markdown","a0af1a5c":"markdown","d1ef104a":"markdown","ba60ba3c":"markdown","9872d26c":"markdown","29f09f01":"markdown","c3bfd666":"markdown","da708241":"markdown","459c705c":"markdown","d372ed56":"markdown","f8b467ad":"markdown","8b94194a":"markdown","643264c9":"markdown","30271fc8":"markdown","8e7633fc":"markdown","2d392719":"markdown","b9ea1d09":"markdown","3063874d":"markdown","367f8d24":"markdown","f8f30ee3":"markdown","a9585f5b":"markdown","e50d35bc":"markdown","89ba2b58":"markdown","291bd61d":"markdown","212e1084":"markdown","1ca5da95":"markdown","4a36730d":"markdown","e559c1b8":"markdown","4779778a":"markdown","37f1d444":"markdown","c8f6624c":"markdown","af102237":"markdown","8cf85881":"markdown","26cc31d6":"markdown","f59890b1":"markdown","25880a21":"markdown","b05ebfa3":"markdown","1f6075e2":"markdown","6cd49523":"markdown"},"source":{"6fb3b7d1":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","be2bb959":"# Import the numpy and pandas package\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 500)","ab1bf477":"# Read the given CSV file, and view some sample records\n\nbike = pd.read_csv('..\/input\/us-boombike\/day.csv')\nbike.head()","a5dfe4c8":"#Determining the number of rows and columns\nbike.shape","d8a79704":"#summary of all the numeric columns in the dataset\nbike.describe()","5d8908bb":"#Datatypes of each column\nbike.info()","2be941a6":"#Checking missing values\nbike.isnull().sum()","b80e65fc":"#Rename the columns for better understanding\nbike.rename(columns = {'yr':'Year','mnth':'month','hum':'humidity','cnt':'count'}, inplace = True) \nbike.head()","a49cb101":"#Mapping variables season, month, weathersit, weekday\n\nbike['season']=bike.season.map({1: 'spring', 2: 'summer',3:'fall', 4:'winter' })\nbike['month']=bike.month.map({1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'June',7:'July',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'})\nbike['weathersit']=bike.weathersit.map({1: 'Clear',2:'Mist + Cloudy',3:'Light Snow',4:'Snow + Fog'})\nbike['weekday']=bike.weekday.map({0:'Sun',1:'Mon',2:'Tue',3:'Wed',4:'Thu',5:'Fri',6:'Sat'})\n\nbike.head()","d658c6a4":"import matplotlib.pyplot as plt\nimport seaborn as sns","9b0a15b9":"# I can check the number of unique values is a column\n# If the number of unique values <=40: Categorical column\n# If the number of unique values in a columns> 50: Continuous\n\nbike.nunique().sort_values()","00327df6":"#Pairplot for numeric variables\nsns.pairplot(bike, vars=[\"temp\", \"humidity\",'casual','windspeed','registered','atemp','count','instant'])\nplt.show()","4011dc40":"##Relationship between categorical and continuous variable\nplt.figure(figsize=(20, 12))\nplt.subplot(2,4,1)\nsns.boxplot(x = 'Year', y = 'count', data = bike)\nplt.subplot(2,4,2)\nsns.boxplot(x = 'holiday', y = 'count', data = bike)\nplt.subplot(2,4,3)\nsns.boxplot(x = 'workingday', y = 'count', data = bike)\nplt.subplot(2,4,4)\nsns.boxplot(x = 'month', y = 'count', data = bike)\nplt.subplot(2,4,5)\nsns.boxplot(x = 'weathersit', y = 'count', data = bike)\nplt.subplot(2,4,6)\nsns.boxplot(x = 'season', y = 'count', data = bike)\nplt.subplot(2,4,7)\nsns.boxplot(x = 'weekday', y = 'count', data = bike)\nplt.show()","dcb28ddb":"#Barplot to see relation between season and count of bike rentals\nsns.barplot('season','count',data=bike,palette=\"rocket\",)\nplt.show()\n","51025dd0":"#Relation between weather and count of bike rentals\nsns.barplot('weathersit','count',palette=\"muted\",data=bike)\nplt.show()\n           ","3f333454":"#Relation between Year and count of bike rentals\nsns.barplot('Year','count',data=bike)\nplt.show()","45f2da53":"#Relation between month and \nplt.figure(figsize=(10,5))\nsns.barplot('month','count',hue='Year',data=bike,palette='Paired')\nplt.show()","aac38c0f":"#scatter plot for temperature to count\nsns.scatterplot(x='temp',y='count' ,data=bike)\nplt.show()","a3f9c45d":"sns.scatterplot(x='humidity', y='count',data=bike)\nplt.show()","169e1b7b":"#Heatmap to see correlation between variables\nplt.figure(figsize=(25, 12))\nsns.heatmap(bike.corr(), cmap='RdYlGn', annot = True)\nplt.title(\"Correlation between Variables\")\nplt.show()","d43e810a":"#drop unnecessary columns\nbike=bike.drop(['instant','dteday','casual', 'registered','atemp'], axis=1)\nbike.head()","95fd601f":"#Checking datatypes of all the columns\nbike.dtypes","a0b779f9":"# # Get the dummy variables for month, season, weathersit, weekday and Let's drop the first column from  using 'drop_first = True'pd.get_dummies(bike.season,drop_first=True)\nmonths=pd.get_dummies(bike.month,drop_first=True)\nweekdays=pd.get_dummies(bike.weekday,drop_first=True)\nweather_sit=pd.get_dummies(bike.weathersit,drop_first=True)\nseasons=pd.get_dummies(bike.season,drop_first=True)\n\n#bike=pd.concat([seasons,bike], axis=1)\n","94ec70c2":"# Add the results to the original bike dataframe\nbike=pd.concat([months,weekdays,weather_sit,seasons,bike],axis=1)\nbike.head()","8cb6c2ef":"# Drop 'season','month','weekday','weathersit' as we have created the dummies for it\nbike.drop(['season','month','weekday','weathersit'], axis = 1, inplace = True)\nbike.head()","9ebf2954":"#Number of rows and columns\nbike.shape","e486fca1":"#Now lets check the correlation between variables again\n#Heatmap to see correlation between variables\nplt.figure(figsize=(25, 20))\nsns.heatmap(bike.corr(), cmap='YlGnBu', annot = True)\nplt.show()","3d5f9ab2":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\n#np.random.seed(0)\nbike_train, bike_test = train_test_split(bike, train_size = 0.7, random_state = 100)","5a6ae5d2":"#Rows and columns after split\nprint(bike_train.shape)\nprint(bike_test.shape)","39373d26":"#Normalisation = (x-xmin)\/(x max-x min)\n#Standardisation= (x-mu)\/ sigma\n#import the library\nfrom sklearn.preprocessing import MinMaxScaler","cb37625d":"#Instantiate an object\nscaler = MinMaxScaler()\n\n#Create a list of numeric variables\nnum_vars=['temp','humidity','windspeed','count']\n\n#Fit on data\nbike_train[num_vars] = scaler.fit_transform(bike_train[num_vars])\nbike_train.head()","777fbb00":"#Checking numeric variables(min and max) after scaling\nbike_train.describe()","564ff929":"# Let's check the correlation coefficients to see which variables are highly correlated after scaling\n#Little to no multicollinearity among predictors\n\nplt.figure(figsize=(25, 20))\nsns.heatmap(bike_train.corr(),cmap='YlOrRd',annot = True)\nplt.show()","e41b7926":"#Divide the data into X and y\ny_train = bike_train.pop('count')\nX_train = bike_train","60ac446b":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n","bad13fdb":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             # running RFE\nrfe = rfe.fit(X_train, y_train)","70e508c2":"#List of variables selected\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","bd726a94":"#Columns where RFE support is True\ncol = X_train.columns[rfe.support_]\ncol","3d8041c2":"#Columns where RFE support is False\nX_train.columns[~rfe.support_]","ef79ccf1":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]\n","73a8eeb5":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_rfe = sm.add_constant(X_train_rfe)\n","0cd8f52a":"# Running the linear model \nlm = sm.OLS(y_train,X_train_rfe).fit()","4dbffa84":"print(lm.summary())","1d646c18":"#Drop the constant term B0\nX_train_rfe = X_train_rfe.drop(['const'], axis=1)","84ca6ece":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","8b01f739":"#Drop January\nX_train_new1 = X_train_rfe.drop([\"Jan\"], axis = 1)\n","da461164":"#Build a model\nX_train_lm1 = sm.add_constant(X_train_new1)\nlm1 = sm.OLS(y_train,X_train_lm1).fit()\nprint(lm1.summary())","8910e0d0":"#Drop the constant term B0\nX_train_lm1 = X_train_lm1.drop(['const'], axis=1)","ccef2685":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new1\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","0e75f456":"#Drop humidity\nX_train_new2 = X_train_lm1.drop([\"humidity\"], axis = 1)\n","32ae95d1":"#Build a model\nX_train_lm2 = sm.add_constant(X_train_new2)\nlm2 = sm.OLS(y_train,X_train_lm2).fit()\nprint(lm2.summary())","618c97f1":"#Drop the constant\nX_train_lm2=X_train_lm2.drop(['const'],axis=1)","be1028e7":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","13de5d71":"#Drop the holiday column\nX_train_new3=X_train_lm2.drop(['holiday'],axis=1)","32835451":"# Adding a constant variable \nX_train_lm3 = sm.add_constant(X_train_new3)\nlm3 = sm.OLS(y_train,X_train_lm3).fit()\nprint(lm3.summary())","8a52bdc1":"#Drop constant \nX_train_lm3=X_train_lm3.drop(['const'],axis=1)","21eb99b3":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new3\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","5afa913f":"#Drop July\nX_train_new4= X_train_lm3.drop(['windspeed'], axis=1)","9b67e9e7":"#Build a model\nX_train_lm4=sm.add_constant(X_train_new4)\nlm4=sm.OLS(y_train,X_train_lm4).fit()\nprint(lm4.summary())","3922dab9":"#Drop constant\nX_train_lm4= X_train_lm4.drop(['const'], axis=1)","94fbdce0":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX =X_train_new4\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f7f4425a":"#Drop July\nX_train_new5=X_train_lm4.drop(['July'], axis=1)","65815832":"#Building a model\nX_train_lm5= sm.add_constant(X_train_new5)\nlm5=sm.OLS(y_train,X_train_lm5).fit()\nprint(lm5.summary())","0aafd3cd":"#Drop the constant\nX_train_lm7=X_train_lm5.drop(['const'],axis=1)","b7a902ac":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new5\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f462e2b6":"#Drop temp for the lm4 model \nX_train_new6=X_train_lm4.drop(['temp'], axis=1)\n","5eb1c1ef":"#Building a model\nX_train_lm6= sm.add_constant(X_train_new6)\nlm6=sm.OLS(y_train,X_train_lm6).fit()\nprint(lm6.summary())","a201a6bc":"#X_train_lm5=sm.add_constant(X_train_lm5)\n#X_train_lm5.columns\nX_train_lm5","60d8843a":"#y train predicted\ny_train_pred = lm5.predict(X_train_lm5)","f3a0c5ce":"# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","4765c8a4":"# Plot the histogram of the error terms\n\nfig = plt.figure()\nplt.figure(figsize=(14,7))\nsns.distplot((y_train - y_train_pred), bins = 20)\nplt.title('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)  # X-label\nplt.show()","eba812b6":"#Create a list of numeric variables\nnum_vars=['temp','humidity','windspeed','count']\n\n#Fit on data\nbike_test[num_vars] = scaler.transform(bike_test[num_vars])\nbike_test.head()","ac836d25":"#Dividing into X_test and y_test\ny_test = bike_test.pop('count')\nX_test = bike_test\nX_test.describe()","3ed59682":"#Columns\nX_train_new5.columns","40d24ec3":"# Now let's use our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new5.columns]\n\n# Adding a constant variable \nX_test_new1 = sm.add_constant(X_test_new)\nX_test_new1.head()","9a721490":"# Making predictions\ny_pred = lm5.predict(X_test_new1)\n","2739984d":"#Evaluate R-square for test\nfrom sklearn.metrics import r2_score\nr2_score(y_test,y_pred)","d23b8f06":"#Adjusted R^2\n#adj r2=1-(1-R2)*(n-1)\/(n-p-1)\n\n#n =sample size , p = number of independent variables\n\nAdj_r2=1-(1-0.8115083)*(11-1)\/(11-1-1)\nprint(Adj_r2)","2e3318da":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.figure(figsize=(15,8))\nplt.scatter(y_test,y_pred,color='blue')\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","1c2d524d":"#Regression plot\nplt.figure(figsize=(15,8))\nsns.regplot(x=y_test, y=y_pred, ci=68, fit_reg=True,scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})\n\nplt.title('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label\nplt.show()","464e243d":"**We can see Error terms are normally distributed**","aed56eb2":"**Windspeed seems to be insignificant,by looking at  high VIF and negative correlation with count.  Lets drop it**","013691e0":"## Step 5: Building a linear model","a01ee012":"#### Applying the scaling on the test sets","948ad932":"## Step 1: Reading and Understanding the Data\n\nLet's start with the following steps:\n\n1. Importing data using the pandas library\n2. Understanding the structure of the data","bd164d50":"### Building model using statsmodel, for the detailed statistics","beb44420":"### RFE","a2d2842a":"### Finding R-squared and Adjusted R-Squared for Test set","e987ac83":"From the heatmap we can see temp, year are correlated to the 'count' variable. Even in August, September months we can see the counts are little high.\n","9c61abca":"## Step 2: Data Visualisation","4346941a":"#### Dividing into X_test and y_test\n","7b70e233":"# Problem Statement\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\nWhich variables are significant in predicting the demand for shared bikes.\nHow well those variables describe the bike demands\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. ","e9522382":"#### Dividing into X and Y sets for the model building","7732796c":"No missing values in the dataset","01caee3a":"Let's inspect the various aspects of our dataframe","e6f04164":"# Final Result Comparison between Train model and Test: ","fe55b2f9":"### Visualising Numeric Variables\n\nLet's make a pairplot of all the numeric variables,  to visualise which variables are most correlated to the target variable 'count'.","1009cef2":"##### Min-Max scaling","2f60c24c":"The plots above shows the relationship between categorical variables and a Target variable.  \n- Bike Rentals are more during the Fall season and then in summer\n- Bike Rentals are more in the year 2019 compared to 2018\n- Bike Rentals are more in partly cloudy weather\n- Bike Rentals are more on Saturday,wednesday and thursday\n","ea5e8369":"##### Bike Rentals are more in partly cloudy weather","febe195b":"\nWe will be using the **LinearRegression function from SciKit Learn** for its compatibility with RFE (which is a utility from sklearn)","4da5d037":"**All the numeric variables are now mapped between 0 and 1**","7ff1336e":"**Here we can see there is a huge drop on R-square and adjusted R-squared. So this wont be a good model.**\n### So our model is lm5 which is obtained by removing January, windspeed, holiday, July and humidity variables from the RFE support columns**","8536e80c":"### Identify Continuous and Categorical Features","30a6aadc":"## Step 3: Data Preparation","15f83de5":"### Check the datatypes","f3a76421":"##### Rebuilding the model without 'Jan'","b48d3c01":"### Drop the unnecessary variables from the dataset","6fd4bb3a":"**July column can be dropped due to its p value and low VIF**","daebf76d":"##### **Still lets check one more model by dropping temp keeping windspeed","dc2045d1":"#### 5: Temperature","35372fba":"# Business Goals:","07d47190":"Before model building, you first need to perform the test-train split and scale the features.","c5e91956":"##### Visualising the fit on the test set","d4f79a9b":"We can see the dataset has some variables that are not required. \n**We can drop instant, dteday, casual, registered**","fded9995":"It is important to have all the variables on the same scale for the model to be easily interpretable. \nWe can use standardization or normalization so that the units of the coefficients obtained are all on the same scale. \n- **There are two common ways of rescaling:**\n\n- Min-Max scaling (Normalisation):Between 0 and 1\n- Standardisation :mean-0, sigma-1\n","639e521e":"#### 3: Year","c2f982f3":"##### Bike rentals more at high humidity","a0af1a5c":"By analysing all the plots above, we can see that there are some independent variables look positively correlated to the 'count' variable. \n-  Bike rentals are more correlated to temperature\n\n","d1ef104a":"#### 2: Weathersit","ba60ba3c":"### - Train R^2 :  0.826\n\n### - Train Adjusted R^2 : 0.82\n\n### - Test R^2: 0.8115\n\n### - Test Adjusted R^2: 0.790564\n\n### - Difference in R^2 between train and test: 1.5%\n\n### - Difference in adjusted R^2 between Train and test: 3.15% which is less than 5%\n\n## Yes! Its a best model\n\n","9872d26c":"##### year (0: 2018, 1:2019)","29f09f01":"##### Yes! Now we can see we have our model.\n\n##### The p values represent the significance of the variables and VIF which represent how variables are correlated to each other. Based on these two parameters we decided which variable to drop.\n\n##### The VIFs and p-values both are within an acceptable range. So we go ahead and make our predictions using this model only.\n\n","c3bfd666":"### - We arrived at a very decent model for the  the demand for shared bikes with the significant variables\n\n### -  We can see that temperature variable is having the highest coefficient 0.4914, which means if the temperature increases by one unit the number of bike rentals increases by 0.4914 units.\n\n### Similary we can see coefficients of other variables in the equation for best fitted line.\n\n### We also see there are some variables with negative coefficients, A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease. We have spring, mist cloudy , light snow variables with negative coefficient.  The coefficient value signifies how much the mean of the dependent variable changes given a one-unit shift in the independent variable while holding other variables in the model constant.","da708241":"### Rescaling the Features","459c705c":"##### Bike rentals are more in the year 2019 compared to 2018","d372ed56":"### -  A US bike-sharing provider BoomBikes can focus more on Temperature\n\n### -  We can see demand for bikes was more in 2019 than 2018, so just focus as there is increase in 2019 and might be facing dips in their revenues due to the ongoing Corona pandemic and by the time it reduces the things will be better\n\n### - Can focus more  on Summer & Winter season, August, September month, Weekends, Working days as they have good influence on bike rentals.\n\n### - We can see spring season has negative coefficients and negatively correlated to bike rentals. So we can give some offers there to increase the demand\n\n### -  Now seeing to weathersit variable, we have got negative coefficients for Mist +cloudy and Lightsnow weather... And yes we can give offers","f8b467ad":"**Recursive Feature Elimination**","8b94194a":"**January is insignificant in presence of other variables due to high p-value and low VIF; can be dropped**","643264c9":"#### 1: Season","30271fc8":"### Visualising Categorical Variables\n\nAs you might have noticed, there are a few categorical variables as well. Let's make a boxplot for some of these variables.","8e7633fc":"#### 4: Month","2d392719":"**- The p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (< 0.05) indicates that you can reject the null hypothesis.**\n\n**-  A rule of thumb commonly used in practice is if a VIF is > 10, you have high multicollinearity. In our case, with values less than 5, we are in good shape, and can proceed with our regression**\n\n**- R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 \u2013 100% scale. And we have the R-square value of 0.826 or 82.6%**\n\n**- The adjusted R-squared adjusts for the number of terms in the model. And we got it around 0.82 or 82%**","b9ea1d09":"\nWe can see that the equation of our best fitted line is:\n\n$ count=         0.4914 \\times temp+   0.0916   \\times September + 0.0645 \\times Saturday +0.0527 \\times summer + 0.0970 \\times winter + 0.2334 \\times Year + 0.0566 \\times working day   - 0.03041 \\times light snow - 0.0786 \\times mist cloudy -0.065 \\times spring $","3063874d":"##### Rebuilding the model without 'humidity'","367f8d24":"##### Rebuilding the model without July","f8f30ee3":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","a9585f5b":"**'humidity' variable can be dropped as its insignificant by looking at very high  VIF**","e50d35bc":"## Step 4: Splitting the Data into Training and Testing Sets","89ba2b58":"##### Bike Rentals are observed at higher temperatures","291bd61d":"**Lets understand the variables better**","212e1084":"## Step 6: Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","1ca5da95":"## Step 8: Model Evaluation","4a36730d":"##### Rebuilding the model without windspeed","e559c1b8":"Multicollinearity: Assumes that predictors are not correlated with each other. If there is\n                       correlation among the predictors","4779778a":"**Holiday variable seems to be insignificant, by looking at p value and low  VIF. We can drop it**","37f1d444":"##### Bike Rentals are more in the year 2019 compared to 2018","c8f6624c":"# Interpretation:","af102237":"##### Rebuliding the model without holiday","8cf85881":"#### 6: Humidity","26cc31d6":"##### Bike Rentals are more during the Fall season and then in summer","f59890b1":"### Dummy variables","25880a21":"## Step 7: Making Predictions","b05ebfa3":"As is visible from the pairplot and the heatmap, we can see temp, atemp, casual,registered,instant variables are correlated to 'count' variable\n- We can also see some other variables are also most correlated.\n- **Both the plots above helps to interpret the data well and identify the variables that can turn out to be useful in building the model**\n- **So yes we can consider a Linear Regression Model.**","1f6075e2":"### Heatmap ","6cd49523":"We can see that temperature,Summer season,June to october months are in good correlation with the 'count' variable. And seem to have good influence on the number of bike rentals."}}