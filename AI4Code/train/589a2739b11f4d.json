{"cell_type":{"89f18299":"code","e00783b1":"code","51a62eaa":"code","f0a9c9b9":"code","20269629":"code","f5f381ce":"code","016e7779":"code","c44336f7":"code","be814933":"code","e37b419e":"code","77f91c1f":"code","4dbee42d":"code","e0948af3":"code","2c9deba6":"code","09b35a0f":"code","c1009196":"code","b54740cf":"code","e4610828":"code","f2dafb80":"code","5bdd6aaf":"code","1b304565":"code","3708d99e":"code","cdc7eb00":"code","a968397e":"code","b08ad4c2":"code","95b11027":"code","05a4a394":"code","d9f20b6a":"code","a972e968":"code","14fccfc4":"code","5b37c701":"code","3c286f65":"code","2613be35":"code","a93e64a5":"code","611b7c3d":"code","3b45273a":"code","05328899":"code","a15ef79b":"code","fd4db4c5":"markdown","104eac65":"markdown","22df6b33":"markdown","9e186faa":"markdown","0dbb3849":"markdown","a24867a1":"markdown","62586743":"markdown","2ee608fa":"markdown","a43a0bbd":"markdown","eb58aa68":"markdown","2596b789":"markdown","b6c200cc":"markdown","32980292":"markdown","2655f6d9":"markdown","63e24f03":"markdown","b263f200":"markdown","b05db879":"markdown","4a16c4c1":"markdown","8acab82c":"markdown","bad318a6":"markdown","34f2a4f8":"markdown","e4ddbb99":"markdown"},"source":{"89f18299":"!pip install tensorflow-gpu\n!pip install catboost\n# \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\n!pip install albumentations - q\n!pip install pymorphy2\n!pip install pymorphy2-dicts\n!pip install DAWG-Python","e00783b1":"import random\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport PIL\nimport cv2\nimport re\nimport pymorphy2\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# # keras\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import regularizers\nimport albumentations\n\n# plt\nimport matplotlib.pyplot as plt\n# \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043c \u0434\u0435\u0444\u043e\u043b\u0442\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n# \u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0432 svg \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u0442\u043a\u0438\u043c\u0438\n%config InlineBackend.figure_format = 'svg'\n%matplotlib inline\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51a62eaa":"DATA_DIR = '..\/input\/sf-dst-car-price-prediction-part2\/'\ntrain = pd.read_csv(DATA_DIR + 'train.csv')\ntest = pd.read_csv(DATA_DIR + 'test.csv')\nsample_submission = pd.read_csv(DATA_DIR + 'sample_submission.csv')","f0a9c9b9":"print('Python       :', sys.version.split('\\n')[0])\nprint('Numpy        :', np.__version__)\nprint('Tensorflow   :', tf.__version__)","20269629":"def mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred-y_true)\/y_true))","f5f381ce":"# \u0432\u0441\u0435\u0433\u0434\u0430 \u0444\u0438\u043a\u0441\u0438\u0440\u0443\u0439\u0442\u0435 RANDOM_SEED, \u0447\u0442\u043e\u0431\u044b \u0432\u0430\u0448\u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u0431\u044b\u043b\u0438 \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u044b!\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)","016e7779":"test.info()","c44336f7":"train.info()","be814933":"train.nunique()","e37b419e":"train.head(20)","77f91c1f":"# split \u0434\u0430\u043d\u043d\u044b\u0445\ndata_train, data_test = train_test_split(\n    train, test_size=0.15, shuffle=True, random_state=RANDOM_SEED)","4dbee42d":"# \u041d\u0430\u0438\u0432\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c\npredicts = []\nfor index, row in pd.DataFrame(data_test[['model_info', 'productionDate']]).iterrows():\n    query = f\"model_info == '{row[0]}' and productionDate == '{row[1]}'\"\n    predicts.append(data_train.query(query)['price'].median())\n\n# \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u043c \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u043d\u044b\u0435 \u0441\u043e\u0432\u043f\u0430\u0434\u0435\u043d\u0438\u044f\npredicts = pd.DataFrame(predicts)\npredicts = predicts.fillna(predicts.median())\n\n# \u043e\u043a\u0440\u0443\u0433\u043b\u0438\u043c\npredicts = (predicts \/\/ 1000) * 1000\n\n# \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u043c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c\nprint(\n    f\"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u0438\u0432\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 MAPE: {(mape(data_test['price'], predicts.values[:, 0]))*100:0.2f}%\")","e0948af3":"# \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u043a\u0430\u043a \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\ndef visualize_distributions(titles_values_dict):\n    columns = min(3, len(titles_values_dict))\n    rows = (len(titles_values_dict) - 1) \/\/ columns + 1\n    fig = plt.figure(figsize=(columns * 6, rows * 4))\n    for i, (title, values) in enumerate(titles_values_dict.items()):\n        hist, bins = np.histogram(values, bins=20)\n        ax = fig.add_subplot(rows, columns, i + 1)\n        ax.bar(bins[:-1], hist, width=(bins[1] - bins[0]) * 0.7)\n        ax.set_title(title)\n    plt.show()\n\n\nvisualize_distributions({\n    'mileage': train['mileage'].dropna(),\n    'modelDate': train['modelDate'].dropna(),\n    'productionDate': train['productionDate'].dropna()\n})","2c9deba6":"# \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u0441\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043a\u0430\u043a \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0431\u0435\u0437 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438\ncategorical_features = ['bodyType', 'brand', 'color', 'engineDisplacement', 'enginePower', 'fuelType', 'model_info', 'name',\n                        'numberOfDoors', 'vehicleTransmission', '\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b', '\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435', '\u041f\u0422\u0421', '\u041f\u0440\u0438\u0432\u043e\u0434', '\u0420\u0443\u043b\u044c']\n\n# \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u0441\u0435 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\nnumerical_features = ['mileage', 'modelDate', 'productionDate']\n\n# \u0421\u0442\u043e\u043b\u0431\u0446\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\ncat_in_numerical = ['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b', '\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435',\n                    'enginePower', 'engineDisplacement']","09b35a0f":"# \u0412\u0410\u0416\u041d\u041e! \u0434\u0440\u044f \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0442\u0440\u0435\u0439\u043d \u0438 \u0442\u0435\u0441\u0442 \u0432 \u043e\u0434\u0438\u043d \u0434\u0430\u0442\u0430\u0441\u0435\u0442\ntrain['sample'] = 1  # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0440\u0435\u0439\u043d\ntest['sample'] = 0  # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0435\u0441\u0442\n# \u0432 \u0442\u0435\u0441\u0442\u0435 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f price, \u043c\u044b \u0435\u0433\u043e \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u043e\u043a\u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043d\u0443\u043b\u044f\u043c\u0438\ntest['price'] = 0\n\ndata = test.append(train, sort=False).reset_index(drop=True)  # \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c\nprint(train.shape, test.shape, data.shape)","c1009196":"# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043d\u0443\u043b\u0435\u0432\u044b\u0445 \u0437\u043d\u0447\u0435\u043d\u0438\u0439\ndef NaN_Sum(col): return col.isnull().sum()\n\n\ncolumns = list(data.columns)\nfor col in columns:\n    print(\"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0443\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\", col, NaN_Sum(data[col]), sep=' ')","b54740cf":"# cat_in_numerical=['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b','\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435','enginePower','engineDisplacement']\n\n# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0449\u043d\u043e\u0441\u0442\u0438 \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a (\u043b\u043e\u0448\u0430\u0434\u0438\u043d\u044b\u0435 \u0441\u0438\u043b\u044b)\ndf = data.copy()\n\n\ndef enginePower_to_numeric(data):\n    result = int(data['enginePower'][:3])\n    return result\n\n\n# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0449\u043d\u043e\u0441\u0442\u0438 \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a (\u043e\u0431\u044a\u0435\u043c \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044f)\ndef engineDisplacement_to_numeric(Data):\n    try:\n        pattern = re.compile('\\d.\\d')\n        result = float(pattern.search(Data['engineDisplacement'])[0])\n        return result\n    except:\n        return random.uniform(1.8, 3.5)\n\n# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0432\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u0435\u0432 \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\n\n\ndef owners_to_int(Data):\n    try:\n        pattern = re.compile('\\d')\n        result = int(pattern.match(Data['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'])[0])\n        return result\n    except:\n        return 1\n\n# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0432\u043b\u0430\u0434\u0435\u043d\u0438\u044f \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0435\u043c \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\n\n\ndef ownership_to_float(Data):\n    try:\n        pattern = re.compile('\\d')\n        result = float('.'.join(pattern.findall(Data['\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435'])))\n        return result\n    except:\n        return (2020-Data['productionDate'])\/Data['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b']\n\n\ndef create_xDrive(Data):\n    try:\n        pattern = re.compile('x[A-Z][a-z]*')\n        result = pattern.search(Data['name'])[0]\n        if result == 'xDrive':\n            return 'yes'\n        else:\n            return 'no'\n    except:\n        return 'no'\n\n\ndf['xdrive'] = df.apply(lambda df: create_xDrive(df), axis=1)\ndf['enginePower'] = df.apply(lambda df: enginePower_to_numeric(df), axis=1)\ndf['engineDisplacement'] = df.apply(\n    lambda df: engineDisplacement_to_numeric(df), axis=1)\ndf['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'] = df.apply(lambda df: owners_to_int(df), axis=1)\ndf['\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435'] = df.apply(lambda df: ownership_to_float(df), axis=1)","e4610828":"df.head(5)","f2dafb80":"# \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u0441\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043a\u0430\u043a \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0431\u0435\u0437 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438\ncategorical_features = ['bodyType', 'brand', 'color',  'fuelType', 'model_info',\n                        'numberOfDoors', 'vehicleTransmission', '\u041f\u0422\u0421', '\u041f\u0440\u0438\u0432\u043e\u0434', '\u0420\u0443\u043b\u044c', 'xdrive']\n\n# \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u0441\u0435 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\nnumerical_features = ['mileage', 'modelDate', 'productionDate', '\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b', '\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435',\n                      'enginePower', 'engineDisplacement']","5bdd6aaf":"def preproc_data(df_input):\n    '''includes several functions to pre-process the predictor data.'''\n\n    df_output = df_input.copy()\n\n    # ################### 1. \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 ##############################################################\n    # \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b\u0435 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    df_output.drop(['description', 'sell_id', 'name'], axis=1, inplace=True)\n\n    # ################### Numerical Features ##############################################################\n    # \u0414\u0430\u043b\u0435\u0435 \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438\n    # for column in numerical_features:\n    #    df_output[column].fillna(df_output[column].median(), inplace=True)\n    # \u0442\u0443\u0442 \u0432\u0430\u0448 \u043a\u043e\u0434 \u043f\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 NAN\n    # \u041d\u0430 \u0434\u0430\u043d\u043d\u043e\u043c \u0448\u0430\u0433\u0435 \u0432 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0435 df \u0443\u0436\u0435 \u043d\u0435\u0442 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u0438 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043d\u0435 \u0432\u043d\u043e\u0441\u0438\u0442\u044c \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f\n\n    # \u041f\u0440\u043e\u043b\u043e\u0433\u0438\u0440\u0438\u0444\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b\n    #numerical_features_log = ['mileage', 'modelDate', 'productionDate']\n    # for l in numerical_features_log:\n    #df_output[l] = np.log(df_output[l])\n\n    # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445\n    scaler = MinMaxScaler()\n    for column in numerical_features:\n        df_output[column] = scaler.fit_transform(df_output[[column]])[:, 0]\n\n    # ################### Categorical Features ##############################################################\n    # Label Encoding\n    for column in categorical_features:\n        df_output[column] = df_output[column].astype('category').cat.codes\n\n    # One-Hot Encoding: \u0432 pandas \u0435\u0441\u0442\u044c \u0433\u043e\u0442\u043e\u0432\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f - get_dummies.\n    df_output = pd.get_dummies(\n        df_output, columns=categorical_features, dummy_na=False)\n    # \u0442\u0443\u0442 \u0432\u0430\u0448 \u043a\u043e\u0434 \u043d\u0430 Encoding \u0444\u0438\u0442\u0447\u0435\u0439\n    # ....\n\n    # ################### Feature Engineering ####################################################\n    # \u0442\u0443\u0442 \u0432\u0430\u0448 \u043a\u043e\u0434 \u043d\u0430 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044e \u043d\u043e\u0432\u044b\u0445 \u0444\u0438\u0442\u0447\u0435\u0439\n    # \u043d\u043e\u0432\u044b\u0435 \u0444\u0438\u0447\u0438 \u0431\u044b\u043b\u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u044b \u043d\u0430 \u043f\u0440\u043e\u0448\u043b\u043e\u043c \u0448\u0430\u0433\u0435, \u0434\u0443\u0431\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u044e\u0434\u0430 \u043d\u0435 \u0432\u0438\u0436\u0443 \u0441\u043c\u044b\u0441\u043b\u0430\n\n    # ################### Clean ####################################################\n    # \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0435\u0449\u0435 \u043d\u0435 \u0443\u0441\u043f\u0435\u043b\u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c,\n    df_output.drop(['vehicleConfiguration'], axis=1, inplace=True)\n\n    return df_output","1b304565":"df_preproc = preproc_data(df)\ndf_preproc.info()","3708d99e":"# \u0422\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0434\u0435\u043b\u0438\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.price.values     # \u043d\u0430\u0448 \u0442\u0430\u0440\u0433\u0435\u0442\nX = train_data.drop(['price'], axis=1)\nX_sub = test_data.drop(['price'], axis=1)","cdc7eb00":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.15, shuffle=True, random_state=RANDOM_SEED)\n\nmodel = CatBoostRegressor(iterations=5000,\n                          # depth=10,\n                          learning_rate=0.11,\n                          random_seed=RANDOM_SEED,\n                          eval_metric='MAPE',\n                          custom_metric=['RMSE', 'MAE'],\n                          od_wait=500,\n                          # task_type='GPU',\n                          )\nmodel.fit(X_train, y_train,\n          eval_set=(X_test, y_test),\n          verbose_eval=100,\n          use_best_model=True,\n          plot=True\n          )\n\ntest_predict_catboost = model.predict(X_test)\nprint(f\"TEST mape: {(mape(y_test, test_predict_catboost))*100:0.2f}%\")","a968397e":"sub_predict_catboost = model.predict(X_sub)\nsample_submission['price'] = sub_predict_catboost\nsample_submission.to_csv('catboost_submission.csv', index=False)","b08ad4c2":"model = Sequential()\nmodel.add(L.Dense(512, input_dim=X_train.shape[1], kernel_regularizer=regularizers.l1_l2(\n    l1=0.000000001, l2=0.000000001), activation=\"relu\"))\nmodel.add(L.Dropout(0.5))\nmodel.add(L.Dense(256, kernel_regularizer=regularizers.l1_l2(\n    l1=0.000000001, l2=0.000000001), activation=\"relu\"))\nmodel.add(L.Dropout(0.25))\nmodel.add(L.Dense(1, activation=\"linear\"))\n\n# Compile model\noptimizer = tf.keras.optimizers.Adam(0.01)\nmodel.compile(loss='MAPE', optimizer=optimizer, metrics=['MAPE'])\n\n\ncheckpoint = ModelCheckpoint(\n    '..\/working\/best_model_car.hdf5', monitor=['val_MAPE'], verbose=0, mode='min')\nearlystop = EarlyStopping(\n    monitor='val_MAPE', patience=50, restore_best_weights=True,)\ncallbacks_list = [checkpoint, earlystop]\n\nhistory = model.fit(X_train, y_train,\n                    batch_size=512,\n                    epochs=500,  # \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043c\u044b \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043f\u043e\u043a\u0430 EarlyStopping \u043d\u0435 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n                    validation_data=(X_test, y_test),\n                    callbacks=callbacks_list,\n                    verbose=2,\n                    )\n\nplt.title('Loss')\nplt.plot(history.history['MAPE'], label='train')\nplt.plot(history.history['val_MAPE'], label='test')\nplt.show()\n\nmodel.load_weights('..\/working\/best_model_car.hdf5')\nmodel.save('..\/working\/nn_1_car.hdf5')\n\ntest_predict_nn1 = model.predict(X_test)\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn1[:,0]))*100:0.2f}%\")\n\nsub_predict_nn1 = model.predict(X_sub)\nsample_submission['price'] = sub_predict_nn1[:, 0]\nsample_submission.to_csv('nn1_submission.csv', index=False)","95b11027":"morph = pymorphy2.MorphAnalyzer()\ndf_NLP = data.copy()","05a4a394":"# \u0412 \u0434\u0430\u043d\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043c\u044b \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u043c \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044e \u0438 \u043e\u0447\u0438\u0441\u0442\u043a\u0443 \u0442\u0435\u0441\u0442\u0430 \u043e\u0442 \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432, \u0447\u0442\u043e \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0442 \u043d\u0430\u043c \u0432\u0440\u0435\u043c\u044f\n# \u2219\u2013\u00ab\u00bb\u2033\u201c\u201d\u27a5\u20bd\u2116\npatterns = \"[A-Za-z0-9!#$%&'()*+,.\/:;<=>?@[\\]^_`{|}~\u2014\\\"\\-]+\"\n\n\ndef lemmatize(doc):\n    doc = re.sub(patterns, ' ', doc)\n    tokens = []\n    for token in doc.split():\n        token = token.strip()\n        token = morph.normal_forms(token)[0]\n        tokens.append(token)\n    return ' '.join(tokens)","d9f20b6a":"df_NLP['description'] = df_NLP.apply(\n    lambda df_NLP: lemmatize(df_NLP.description), axis=1)","a972e968":"# TOKENIZER\n# The maximum number of words to be used. (most frequent)\nMAX_WORDS = 100000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 256\n\n# split \u0434\u0430\u043d\u043d\u044b\u0445\ntext_train = df_NLP.description.iloc[X_train.index]\ntext_test = df_NLP.description.iloc[X_test.index]\ntext_sub = df_NLP.description.iloc[X_sub.index]\n\ntokenize = Tokenizer(num_words=MAX_WORDS)\ntokenize.fit_on_texts(df_NLP['description'])\n\ntext_train_sequences = sequence.pad_sequences(\n    tokenize.texts_to_sequences(text_train), maxlen=MAX_SEQUENCE_LENGTH)\ntext_test_sequences = sequence.pad_sequences(\n    tokenize.texts_to_sequences(text_test), maxlen=MAX_SEQUENCE_LENGTH)\ntext_sub_sequences = sequence.pad_sequences(\n    tokenize.texts_to_sequences(text_sub), maxlen=MAX_SEQUENCE_LENGTH)\n\nprint(text_train_sequences.shape,\n      text_test_sequences.shape, text_sub_sequences.shape, )\n\n\n# \u0432\u043e\u0442 \u0442\u0430\u043a \u0442\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u043d\u0430\u0448 \u0442\u0435\u043a\u0441\u0442\nprint(text_train.iloc[6])\nprint('-----------------------------------------------')\nprint(text_train_sequences[6])\n","14fccfc4":"#  RNN NLP \u041d\u0435\u0439\u0440\u043e\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c \u0434\u043b\u044f \u0442\u0435\u043a\u0441\u0442\u0430 (Description)\nmodel_nlp = Sequential()\nmodel_nlp.add(L.Input(shape=MAX_SEQUENCE_LENGTH, name=\"seq_description\"))\nmodel_nlp.add(L.Embedding(len(tokenize.word_index)+1, MAX_SEQUENCE_LENGTH,))\nmodel_nlp.add(L.LSTM(256, return_sequences=True))\nmodel_nlp.add(L.Dropout(0.5))\nmodel_nlp.add(L.LSTM(128,))\nmodel_nlp.add(L.Dropout(0.5))\nmodel_nlp.add(L.Dense(64, kernel_regularizer=regularizers.l1_l2(\n    l1=0.000000001, l2=0.000000001), activation=\"relu\"))\nmodel_nlp.add(L.Dropout(0.25))\n\n\n# MLP \u041d\u0435\u0439\u0440\u043e\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c \u0434\u043b\u044f \u0442\u0430\u0431\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\nmodel_mlp = Sequential()\nmodel_mlp.add(L.Dense(512, input_dim=X_train.shape[1], kernel_regularizer=regularizers.l1_l2(\n    l1=0.000000001, l2=0.000000001), activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))\nmodel_mlp.add(L.Dense(256, kernel_regularizer=regularizers.l1_l2(\n    l1=0.000000001, l2=0.000000001), activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.25))\n\n# Multiple Inputs NN\ncombinedInput = L.concatenate([model_nlp.output, model_mlp.output])\n# being our regression head\nhead = L.Dense(64, activation=\"relu\")(combinedInput)\nhead = L.Dense(1, activation=\"linear\")(head)\n\nmodel = Model(inputs=[model_nlp.input, model_mlp.input], outputs=head)\noptimizer = tf.keras.optimizers.Adam(0.01)\nmodel.compile(loss='MAPE', optimizer=optimizer, metrics=['MAPE'])\ncheckpoint = ModelCheckpoint(\n    '..\/working\/best_model_car.hdf5', monitor=['val_MAPE'], verbose=0, mode='min')\nearlystop = EarlyStopping(\n    monitor='val_MAPE', patience=10, restore_best_weights=True,)\ncallbacks_list = [checkpoint, earlystop]\n\nhistory = model.fit([text_train_sequences, X_train], y_train,\n                    batch_size=512,\n                    epochs=500,  # \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043c\u044b \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043f\u043e\u043a\u0430 EarlyStopping \u043d\u0435 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n                    validation_data=([text_test_sequences, X_test], y_test),\n                    callbacks=callbacks_list\n                    )\n\nplt.title('Loss')\nplt.plot(history.history['MAPE'], label='train')\nplt.plot(history.history['val_MAPE'], label='test')\nplt.show()\n\nmodel.load_weights('..\/working\/best_model_car.hdf5')\nmodel.save('..\/working\/nn_mlp_nlp_car.hdf5')\n\ntest_predict_nn2 = model.predict([text_test_sequences, X_test])\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn2[:,0]))*100:0.2f}%\")\n\nsub_predict_nn2 = model.predict([text_sub_sequences, X_sub])\nsample_submission['price'] = sub_predict_nn2[:, 0]\nsample_submission.to_csv('nn2_submission_car.csv', index=False)","5b37c701":"# \u0443\u0431\u0435\u0434\u0438\u043c\u0441\u044f, \u0447\u0442\u043e \u0446\u0435\u043d\u044b \u0438 \u0444\u043e\u0442\u043e \u043f\u043e\u0434\u0433\u0440\u0443\u0437\u0438\u043b\u0438\u0441\u044c \u0432\u0435\u0440\u043d\u043e\nplt.figure(figsize=(12, 8))\n\nrandom_image = train.sample(n=9)\nrandom_image_paths = random_image['sell_id'].values\nrandom_image_cat = random_image['price'].values\n\nfor index, path in enumerate(random_image_paths):\n    im = PIL.Image.open(DATA_DIR+'img\/img\/' + str(path) + '.jpg')\n    plt.subplot(3, 3, index + 1)\n    plt.imshow(im)\n    plt.title('price: ' + str(random_image_cat[index]))\n    plt.axis('off')\nplt.show()","3c286f65":"size = (320, 240)\n\n\ndef get_image_array(index):\n    images_train = []\n    for index, sell_id in enumerate(data['sell_id'].iloc[index].values):\n        image = cv2.imread(DATA_DIR + 'img\/img\/' + str(sell_id) + '.jpg')\n        assert(image is not None)\n        image = cv2.resize(image, size)\n        images_train.append(image)\n    images_train = np.array(images_train)\n    print('images shape', images_train.shape, 'dtype', images_train.dtype)\n    return(images_train)\n\nimages_train = get_image_array(X_train.index)\nimages_test = get_image_array(X_test.index)\nimages_sub = get_image_array(X_sub.index)\n\n# \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u043c\u0430\u0442\u0440\u0438\u0446\nnp.save(DATA_DIR+'images_train', images_train)\nnp.save(DATA_DIR+'images_test', images_test)\nnp.save(DATA_DIR+'images_sub', images_sub)\n\n# \u041f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u0432 \u0440\u0435\u0448\u0435\u043d\u0438\u0435\nimages_train = np.load(DATA_DIR+'images_train.npy')\nimages_test = np.load(DATA_DIR+'images_test.npy')\nimages_sub = np.load(DATA_DIR+'images_sub.npy')","2613be35":"from albumentations import (\n    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90, ChannelShuffle,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n    IAASharpen, IAAEmboss, RandomBrightness, OneOf, Compose, Rotate, RandomContrast,\n    HorizontalFlip, RGBShift, RandomBrightnessContrast\n)\n\n\n# \u043f\u0440\u0438\u043c\u0435\u0440 \u0432\u0437\u044f\u0442 \u0438\u0437 \u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438: https:\/\/albumentations.readthedocs.io\/en\/latest\/examples.html\n\"\"\"augmentation = Compose([\n            RandomBrightness(limit=0.4),\n            HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n            RandomContrast(limit=0.2, p=0.6),\n            HorizontalFlip(),\n            CLAHE(p=0.2),\n            Blur(p=0.1),\n            HueSaturationValue(p=0.6),\n            IAAPerspective(p=0.2),\n            OpticalDistortion(p=0.2),\n            #ChannelShuffle(p=1),\n            RGBShift(p=1),\n], p=1)\"\"\"\n\naugmentation = Compose([\n    HorizontalFlip(),\n    OneOf([\n        IAAAdditiveGaussianNoise(),\n        GaussNoise(),\n    ], p=0.2),\n    OneOf([\n        MotionBlur(p=0.2),\n        MedianBlur(blur_limit=3, p=0.1),\n        Blur(blur_limit=3, p=0.1),\n    ], p=0.2),\n    ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2,\n                     rotate_limit=15, p=1),\n    OneOf([\n        CLAHE(clip_limit=2),\n        IAASharpen(),\n        IAAEmboss(),\n        RandomBrightnessContrast(),\n    ], p=0.3),\n    HueSaturationValue(p=0.3),\n], p=1)\n\n# \u043f\u0440\u0438\u043c\u0435\u0440\nplt.figure(figsize=(12, 8))\nfor i in range(9):\n    img = augmentation(image=images_train[0])['image']\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(img)\n    plt.axis('off')\nplt.show()","a93e64a5":"def make_augmentations(images):\n    print('\u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439', end='')\n    augmented_images = np.empty(images.shape)\n    for i in range(images.shape[0]):\n        if i % 200 == 0:\n            print('.', end='')\n        augment_dict = augmentation(image=images[i])\n        augmented_image = augment_dict['image']\n        augmented_images[i] = augmented_image\n    print('')\n    return augmented_images","611b7c3d":"# NLP part\ntokenize = Tokenizer(num_words=MAX_WORDS)\ntokenize.fit_on_texts(df_NLP.description)","3b45273a":"def process_image(image):\n    return augmentation(image=image.numpy())['image']\n\n\ndef tokenize_(descriptions):\n    return sequence.pad_sequences(tokenize.texts_to_sequences(descriptions), maxlen=MAX_SEQUENCE_LENGTH)\n\n\ndef tokenize_text(text):\n    return tokenize_([text.numpy().decode('utf-8')])[0]\n\n\ndef tf_process_train_dataset_element(image, table_data, text, price):\n    im_shape = image.shape\n    [image, ] = tf.py_function(process_image, [image], [tf.uint8])\n    image.set_shape(im_shape)\n    [text, ] = tf.py_function(tokenize_text, [text], [tf.int32])\n    return (image, table_data, text), price\n\n\ndef tf_process_val_dataset_element(image, table_data, text, price):\n    [text, ] = tf.py_function(tokenize_text, [text], [tf.int32])\n    return (image, table_data, text), price\n\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    images_train, X_train, df_NLP.description.iloc[X_train.index], y_train\n)).map(tf_process_train_dataset_element)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    images_test, X_test, df_NLP.description.iloc[X_test.index], y_test\n)).map(tf_process_val_dataset_element)\n\ny_sub = np.zeros(len(X_sub))\nsub_dataset = tf.data.Dataset.from_tensor_slices((\n    images_sub, X_sub, df_NLP.description.iloc[X_sub.index], y_sub\n)).map(tf_process_val_dataset_element)\n\n# \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c, \u0447\u0442\u043e \u043d\u0435\u0442 \u043e\u0448\u0438\u0431\u043e\u043a (\u043d\u0435 \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0431\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435):\ntrain_dataset.__iter__().__next__()\ntest_dataset.__iter__().__next__()\nsub_dataset.__iter__().__next__()","05328899":"# \u0421\u0442\u0440\u043e\u0438\u043c \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u0443\u044e \u0441\u0435\u0442\u044c \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0431\u0435\u0437 \"\u0433\u043e\u043b\u043e\u0432\u044b\"\n# \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u0430 \u0432 \u0441\u043e\u0441\u0442\u0430\u0432 \u043c\u043e\u0434\u0435\u043b\u0438 EfficientNetB3, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0430 \u0432\u0445\u043e\u0434 \u043e\u043d\u0430 \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u0442\u0438\u043f\u0430 uint8\nefficientnet_model = tf.keras.applications.efficientnet.EfficientNetB3(\n    weights='imagenet', include_top=False, input_shape=(size[1], size[0], 3))\nefficientnet_output = L.GlobalAveragePooling2D()(efficientnet_model.output)\n\n# \u0441\u0442\u0440\u043e\u0438\u043c \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0442\u0430\u0431\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\ntabular_model = Sequential([\n    L.Input(shape=X.shape[1]),\n    L.Dense(512, kernel_regularizer=regularizers.l1_l2(\n        l1=0.000000001, l2=0.000000001), activation='relu'),\n    L.Dropout(0.5),\n    L.Dense(256, kernel_regularizer=regularizers.l1_l2(\n        l1=0.000000001, l2=0.000000001), activation=\"relu\"),\n    L.Dropout(0.25),\n])\n\n# NLP\nnlp_model = Sequential([\n    L.Input(shape=MAX_SEQUENCE_LENGTH, name=\"seq_description\"),\n    L.Embedding(len(tokenize.word_index)+1, MAX_SEQUENCE_LENGTH,),\n    L.LSTM(256, return_sequences=True),\n    L.Dropout(0.5),\n    L.LSTM(128),\n    L.Dropout(0.5),\n    L.Dense(64, kernel_regularizer=regularizers.l1_l2(\n        l1=0.000000001, l2=0.000000001), activation=\"relu\"),\n    L.Dropout(0.25)\n])\n\n# \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0432\u044b\u0445\u043e\u0434\u044b \u0442\u0440\u0435\u0445 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0439\ncombinedInput = L.concatenate(\n    [efficientnet_output, tabular_model.output, nlp_model.output])\n\n# being our regression head\nhead = L.Dense(256, kernel_regularizer=regularizers.l1_l2(\n    l1=0.000000001, l2=0.000000001), activation=\"relu\")(combinedInput)\nhead = L.Dense(1,)(head)\n\nmodel = Model(inputs=[efficientnet_model.input,\n                      tabular_model.input, nlp_model.input], outputs=head)\n\n\noptimizer = tf.keras.optimizers.Adam(0.005)\nmodel.compile(loss='MAPE', optimizer=optimizer, metrics=['MAPE'])\n\ncheckpoint = ModelCheckpoint(\n    '..\/working\/best_model.hdf5', monitor=['val_MAPE'], verbose=0, mode='min')\nearlystop = EarlyStopping(\n    monitor='val_MAPE', patience=10, restore_best_weights=True,)\ncallbacks_list = [checkpoint, earlystop]\n\nhistory = model.fit(train_dataset.batch(10),\n                    epochs=40,\n                    validation_data=test_dataset.batch(10),\n                    callbacks=callbacks_list\n                    )\n\nplt.title('Loss')\nplt.plot(history.history['MAPE'], label='train')\nplt.plot(history.history['val_MAPE'], label='test')\nplt.show()\n\nmodel.load_weights('..\/working\/best_model.hdf5')\nmodel.save('..\/working\/nn_final.hdf5')\n\ntest_predict_nn3 = model.predict(test_dataset.batch(30))\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn3[:,0]))*100:0.2f}%\")\n\nsub_predict_nn3 = model.predict(sub_dataset.batch(30))\nsample_submission['price'] = sub_predict_nn3[:, 0]\nsample_submission.to_csv('nn3_submission.csv', index=False)\n\n","a15ef79b":"blend_predict = (test_predict_catboost + test_predict_nn3[:, 0]) \/ 2\nprint(f\"TEST mape: {(mape(y_test, blend_predict))*100:0.2f}%\")\n\nblend_sub_predict = (sub_predict_catboost + sub_predict_nn3[:, 0]) \/ 2\nsample_submission['price'] = blend_sub_predict\nsample_submission.to_csv(DATA_DIR+'blend_submission.csv', index=False)\n","fd4db4c5":"# Model 3: Tabular NN\n\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u043e\u0431\u044b\u0447\u043d\u0443\u044e \u0441\u0435\u0442\u044c:","104eac65":"# PreProc Tabular Data","22df6b33":"## \u0412\u043d\u0435\u0441\u0435\u043c \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0432 \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 ","9e186faa":"## \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b: \n\n1. CatBoost MAPE = 11.77%\n2. Tabular NN MAPE = 11.86%\n3. NLP + NN MAPE = 12.46%\n4. NLP + NN MAPE + Picture MAPE = 12.97%\n5. Blend result MAPE = 11.29% \n\nKAGGLE result = 12.52156%\n\n# \u0412\u044b\u0432\u043e\u0434\u044b:\n#### CatBoost\n  \u0412 \u0446\u0435\u043b\u043e\u043c \u0442\u0443\u0442 \u0432\u0441\u0435 \u0445\u043e\u0440\u043e\u0448\u043e, \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043c\u043e\u0436\u0435\u0442 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043d\u043e\u0432\u044b\u0445 \u0444\u0438\u0447, \u043d\u043e \u043f\u043e\u043a\u0430 \u043d\u0435\u0442 \u0438\u0434\u0435\u0439 \u0447\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0435\u0449\u0435 \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \n#### Tabular NN\n  \u041f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0438\u0437\u043c\u0435\u043d\u044f\u0442\u044c \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438, \u043d\u043e \u044d\u0442\u043e \u043d\u0435 \u0434\u0430\u043b\u043e \u043a\u0430\u043a\u0438\u0445 \u0442\u043e \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0440\u0435\u0437\u044c\u0442\u0430\u0442\u043e\u0432, \u043d\u043e \u0437\u0430\u0442\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438 l1_l2 \u0434\u043e\u043b\u043e \u0431\u043e\u043b\u0435\u0435 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \n####  NLP + NN MAPE\n  \u041c\u043e\u0436\u043d\u043e \u0435\u0449\u0435 \u043f\u043e\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430\u0434 \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u0438\u0435\u043c \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u0430 \u0441\u0445\u043e\u0436\u0438\u0445 \u0431\u043b\u043e\u043a\u043e\u0432 \u0438 \u0438\u0445 \u0437\u0430\u043c\u0435\u043d\u044b \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0433\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f, \u043d\u043e \u043d\u0435 \u0443\u0441\u043f\u0435\u043b \u044d\u0442\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \n####  NLP + NN MAPE + Picture MAPE\n  \u041f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0440\u0430\u0437\u043c\u043e\u0440\u043e\u0437\u0438\u0442\u044c \u043f\u043e\u043b\u043e\u0432\u0438\u043d\u0443 \u0432\u0435\u0441\u043e\u0432 \u0432 EfficientNetB3, \u043d\u043e \u0441\u0435\u0442\u044c \u043e\u0431\u0443\u0447\u0430\u043b\u043e\u0441\u044c \u043e\u0447\u0435\u043d\u044c \u0434\u043e\u043b\u0433\u043e \u0438 \u0432 \u0438\u0442\u043e\u0433\u0435 \u044d\u0442\u043e \u043d\u0435 \u0434\u0430\u043b\u043e \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 + \u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0441\u0435\u0442\u044c  EfficientNetB6, \u043d\u043e \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043b\u043e\u0441\u044c ~ \u0432 3 \u0440\u0430\u0437\u0430, \u043d\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043d\u0435 \u0443\u043b\u0443\u0447\u0448\u0438\u043b\u0441\u044f, \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0433\u043e\u043b\u043e\u0432\u044b \u043d\u0430\u0434 \u0431\u0430\u0437\u043e\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u044e \u0442\u043e\u0436\u0435 \u043d\u0435 \u0434\u0430\u043b\u043e \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 \n\n## \u0415\u0441\u0442\u044c \u043a\u0443\u0434\u0430 \u0441\u0442\u0440\u0435\u043c\u0438\u0442\u044c\u0441\u044f \u0438 \u0447\u0442\u043e \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043b\u044c\u0448\u0435   ","0dbb3849":"## \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0443\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \n\n\u0418\u043c\u0435\u0435\u043c \u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0432 \u0434\u0432\u0443\u0445 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u0445 \n- \u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b 1 (\u043c\u043e\u0436\u043d\u043e \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c \u043d\u0430 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u043b\u0430\u0435\u043b\u044c\u0446\u0435\u0432 \u0441\u0438\u043b\u044c\u043d\u043e \u043d\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u044d\u0442\u043e \u043d\u0435 \u043f\u043e\u0432\u043b\u0438\u044f\u0435\u0442)\n- \u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435 5418 (\u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043c \u0437\u0430\u043c\u0435\u043d\u0443 \u043d\u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0440\u0430\u0432\u043d\u043e\u0435 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0443 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044e \u043a \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0432\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u0435\u0432 \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u0447\u0442\u043e \u0432\u0441\u0435 \u0432\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b \u0432\u043b\u0430\u0434\u0435\u043b\u0438 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0435\u043c \u0440\u0430\u0432\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438)","a24867a1":"####  \u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: TEST mape: 13.39%\n#### \u041c\u043e\u0439 \u043f\u0435\u0440\u0432\u044b\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043d\u043e\u0439 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0435\u0439, \u043f\u043e \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u043d\u044b\u043c \u043f\u0440\u043e\u0448\u043b\u044b\u043c \u0440\u0435\u0437\u0443\u044c\u0442\u0430\u0442\u043e\u043c: TEST mape: 13.66% (25 \u043f\u043e\u043a\u043e\u043b\u0435\u043d\u0438\u0439)\n#### \u041c\u043e\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 c \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0435\u0439, \u043f\u043e \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u043d\u044b\u043c \u043f\u0440\u043e\u0448\u043b\u044b\u043c \u0440\u0435\u0437\u0443\u044c\u0442\u0430\u0442\u043e\u043c: TEST mape: 13.20% (25 \u043f\u043e\u043a\u043e\u043b\u0435\u043d\u0438\u0439) \u0432\u0440\u0435\u043c\u044f ~ 3h\n#### \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u0435\u0439 \u0432 \u0440\u0435\u0448\u0435\u043d\u0438\u0435, Adam=0.005: TEST mape: 12.97%% (28 \u043f\u043e\u043a\u043e\u043b\u0435\u043d\u0438\u0439), \u0432\u0440\u0435\u043c\u044f ~3h","62586743":"# Model 2: CatBoostRegressor","2ee608fa":"## \u0411\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u044f \u043f\u0440\u0435\u0434\u044a\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u0441\u043e\u043a\u0440\u0430\u0442\u0438\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432 \u043f\u043e\u0441\u043b\u0435 One Hot Encoding \u043d\u0430 383","a43a0bbd":"#### \u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435: TEST mape: 13.84%\n#### \u041c\u043e\u0438 \u043f\u0435\u0440\u0432\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0441\u043e \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u043e\u0439: TEST mape: 13.33%\n#### \u041c\u043e\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f: TEST mape: 11.82%\n#### \u041c\u043e\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f: TEST mape: 11.78%\n\n## \u0418\u0442\u043e\u0433\n- \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f Adam -> SGD \u043d\u0435 \u0434\u0430\u043b\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \n- \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f learning_rate \u043d\u0435 \u0434\u0430\u043b\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\n- \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0451\u0432 \u043d\u0435 \u0434\u0430\u043b\u043e \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\n# Model 4: NLP + Multiple Inputs\n\n### \u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f\n","eb58aa68":"## \u0421\u0442\u043e\u043b\u0431\u0446\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438","2596b789":"#### \u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: TEST mape: 14.61%\n#### \u041c\u043e\u0439 \u043f\u0435\u0440\u0432\u044b\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0441\u043e \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u044e: TEST mape: 12.31%\n#### \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438: TEST mape: 11.96%\n#### \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043d\u043e\u0432\u043e\u0439 \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438: TEST mape: 13.26%\n#### \u0414\u043b\u044f \u0442\u0430\u0431\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u0437\u044f\u0442\u0430 \u043d\u0435\u0439\u0440\u043e\u043d\u043a\u0430 \u0441 \u043f\u0440\u043e\u0448\u043b\u043e\u0433\u043e \u0448\u0430\u0433\u0430: TEST mape: 12.43%\n#### \u0423\u0431\u0440\u0430\u043b \u0438\u0437 \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u2219\u2013\u00ab\u00bb\u2033\u201c\u201d\u27a5\u20bd\u2116 : TEST mape: 12.26%\n#### \u0414\u043e\u0431\u0430\u0432\u0438\u043b \u0432 RNN NLP \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044e regularizers.l1_l2(l1=0.000000001,l2=0.000000001): TEST mape: 12.35%\n#### \u0418\u0437\u043c\u0435\u043d\u0438\u043b DropOut2 \u0432 NLP \u043d\u0430 0.5 : TEST mape: 12.25%\n","b6c200cc":"albumentations","32980292":"#### \u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439 TEST mape: 13.23%\n#### \u0421 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0432\u0430\u043d\u0438\u0435 \u0421atBoostRegression \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u0434\u043e\u0441\u0442\u0438\u0433\u043d\u0443\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430: TEST mape: 12.46%\n#### \u0421 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0432\u0430\u043d\u0438\u0435 \u0421atBoostRegression: learning_rate=0.1 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u0434\u043e\u0441\u0442\u0438\u0433\u043d\u0443\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430: TEST mape: 12.08%\n#### \u0421 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0432\u0430\u043d\u0438\u0435 \u0421atBoostRegression: learning_rate=0.11 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u0434\u043e\u0441\u0442\u0438\u0433\u043d\u0443\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430: TEST mape: 11.90%\n#### \u0421 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0432\u0430\u043d\u0438\u0435 XGBoostRegressor \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u0434\u043e\u0441\u0442\u0438\u0433\u043d\u0443\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430: TEST mape: 18.68%","2655f6d9":"# Blend","63e24f03":"## Split data","b263f200":"# Model 1: \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \"\u043d\u0430\u0438\u0432\u043d\u0443\u044e\" \u043c\u043e\u0434\u0435\u043b\u044c \n\u042d\u0442\u0430 \u043c\u043e\u0434\u0435\u043b\u044c \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u044e\u044e \u0446\u0435\u043d\u0443 \u043f\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0433\u043e\u0434\u0443 \u0432\u044b\u043f\u0443\u0441\u043a\u0430. \nC \u043d\u0435\u0439 \u0431\u0443\u0434\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c \u0434\u0440\u0443\u0433\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438.","b05db879":"# EDA \n\n\u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0431\u044b\u0441\u0442\u0440\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c, \u0441\u043c\u043e\u0436\u0435\u0442 \u043b\u0438 \u0441 \u044d\u0442\u0438\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430\u0448 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c.\n\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u043a\u0430\u043a \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:","4a16c4c1":"# Model 5: \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n\n","8acab82c":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0442\u0438\u043f\u044b \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\n\n* bodyType - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* brand - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* color - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* description - \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0439\n* engineDisplacement - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u043a\u0430\u043a \u0442\u0435\u043a\u0441\u0442\n* enginePower - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u043a\u0430\u043a \u0442\u0435\u043a\u0441\u0442\n* fuelType - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* mileage - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439\n* modelDate - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439\n* model_info - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* name - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439, \u0436\u0435\u043b\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0441\u043e\u043a\u0440\u0430\u0442\u0438\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c\n* numberOfDoors - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* price - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439, \u0446\u0435\u043b\u0435\u0432\u043e\u0439\n* productionDate - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439\n* sell_id - \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 (\u0444\u0430\u0439\u043b \u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d \u043f\u043e \u0430\u0434\u0440\u0435\u0441\u0443, \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u043e\u043c\u0443 \u043d\u0430 sell_id)\n* vehicleConfiguration - \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f (\u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044f \u0434\u0440\u0443\u0433\u0438\u0445 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432)\n* vehicleTransmission - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* \u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* \u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435 - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u043a\u0430\u043a \u0442\u0435\u043a\u0441\u0442\n* \u041f\u0422\u0421 - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* \u041f\u0440\u0438\u0432\u043e\u0434 - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* \u0420\u0443\u043b\u044c - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439","bad318a6":"### RNN NLP","34f2a4f8":"### Tokenizer","e4ddbb99":"\u0418\u0442\u043e\u0433\u043e:\n* CatBoost \u0441\u043c\u043e\u0436\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438 \u0438 \u0432 \u0442\u0430\u043a\u043e\u043c \u0432\u0438\u0434\u0435, \u043d\u043e \u0434\u043b\u044f \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438 \u043d\u0443\u0436\u043d\u044b \u043d\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435."}}