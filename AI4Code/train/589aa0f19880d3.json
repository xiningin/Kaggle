{"cell_type":{"51974d3b":"code","300e3913":"code","69c7937c":"code","1b37f8cd":"code","9baf8d1a":"code","509ddaa5":"code","30c4a93c":"code","d99c247b":"code","bd4c1b7c":"code","aa344276":"code","141e6f3b":"code","80fa3ca6":"code","e18c56c1":"code","04447188":"code","6348f52a":"markdown","7fe80628":"markdown","4cd30953":"markdown","45dbf3d6":"markdown","68d0f4a8":"markdown","b75c0def":"markdown","a21c03d2":"markdown","e2b2b573":"markdown","57702832":"markdown","62e82e13":"markdown"},"source":{"51974d3b":"import csv\nimport numpy as np\nimport pandas as pd\n\nfrom datetime import datetime as dt\n\nfrom IPython.display import display\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom xgboost import XGBClassifier\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.pipeline import FeatureUnion, make_pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom eli5 import show_weights, show_prediction","300e3913":"with open('..\/input\/train.csv', 'rt') as f:\n    data_train = list(csv.DictReader(f))\ndata_train[:1]","69c7937c":"with open('..\/input\/test.csv', 'rt') as f:\n    data_test = list(csv.DictReader(f))\ndata_test[:1]","1b37f8cd":"_all_xs = [{k: v for k, v in row.items() if k != 'Survived'} for row in data_train]\n_all_ys = np.array([int(row['Survived']) for row in data_train])\n\nall_xs, all_ys = shuffle(_all_xs, _all_ys, random_state=0)\ntrain_xs, valid_xs, train_ys, valid_ys = train_test_split(\n    all_xs, all_ys, test_size=0.25, random_state=0)\n\nprint('{} items total, {:.1%} true'.format(len(all_xs), np.mean(all_ys)))","9baf8d1a":"for x in all_xs:\n    if x['Age']:\n        x['Age'] = float(x['Age'])\n    else:\n        x.pop('Age')\n    x['Fare'] = float(x['Fare'])\n    x['SibSp'] = int(x['SibSp'])\n    x['Parch'] = int(x['Parch'])","509ddaa5":"### Load the test set, do the basic pre-processing steps same as above, and make predictions on it.\n\nall_xs_test = [{k: v for k, v in row.items()} for row in data_test]\n\nfor x in all_xs_test:\n    if x['Age']:\n        x['Age'] = float(x['Age'])\n    else:\n        x.pop('Age')\n    if x['Fare']:\n        x['Fare'] = float(x['Fare'])\n    else:\n        x.pop('Fare')\n    x['SibSp'] = int(x['SibSp'])\n    x['Parch'] = int(x['Parch'])","30c4a93c":"class CSCTransformer:\n    def transform(self, xs):\n        # work around https:\/\/github.com\/dmlc\/xgboost\/issues\/1238#issuecomment-243872543\n        return xs.tocsc()\n    def fit(self, *args):\n        return self\n\nclf = XGBClassifier()\nvec = DictVectorizer()\npipeline = make_pipeline(vec, CSCTransformer(), clf)\n\ndef evaluate(_clf):\n    scores = cross_val_score(_clf, all_xs, all_ys, scoring='accuracy', cv=10)\n    print('Accuracy: {:.3f} \u00b1 {:.3f}'.format(np.mean(scores), 2 * np.std(scores)))\n    _clf.fit(train_xs, train_ys)  # so that parts of the original pipeline are fitted\n    return np.mean(scores), _clf.predict(all_xs_test)\n\nscore_1, preds_1 = evaluate(pipeline)","d99c247b":"show_weights(clf, vec=vec)","bd4c1b7c":"show_prediction(clf, valid_xs[1], vec=vec, show_feature_values=True)","aa344276":"no_missing = lambda feature_name, feature_value: not np.isnan(feature_value)\nshow_prediction(clf, valid_xs[1], vec=vec, show_feature_values=True, feature_filter=no_missing)","141e6f3b":"vec2 = FeatureUnion([\n    ('Name', CountVectorizer(\n        analyzer='char_wb',\n        ngram_range=(3, 4),\n        preprocessor=lambda x: x['Name'],\n        max_features=100,\n    )),\n    ('All', DictVectorizer()),\n])\nclf2 = XGBClassifier()\n\npipeline2 = make_pipeline(vec2, CSCTransformer(), clf2)\n\nscore_2, preds_2 = evaluate(pipeline2)","80fa3ca6":"show_weights(clf2, vec=vec2)","e18c56c1":"# We hide missing features \nfor idx in [4, 5, 7, 37, 81]:\n    display(show_prediction(clf2, valid_xs[idx], vec=vec2,\n                            show_feature_values=True, feature_filter=no_missing))","04447188":"df_sub = pd.read_csv('..\/input\/gender_submission.csv')\n\nfilename = 'subm_{:.6f}_{}.csv'.format(score_2, \n                     dt.now().strftime('%Y-%m-%d-%H-%M'))\nprint('save to {}'.format(filename))\n\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = df_sub['PassengerId']\nsubmission['Survived'] = preds_2\n\nsubmission.to_csv(filename, index=False)","6348f52a":"### Feature importances","7fe80628":"### Idea for submission filename drawn from Chia-ta Tsai's kernel https:\/\/www.kaggle.com\/cttsai\/forked-lgbm-w-ideas-from-kernels-and-discuss. Obviously the LB score will improve a lot if we do a little more parameter tuning + feature engineering, but I just wanted to understand how ELI5 works for now. ","4cd30953":"### We don't want to process name as a categorical variable","45dbf3d6":"### Just using some basic feature engineering, same as the ELI5 tutorial. For a more detailed exploration, cf. my old Titanic kernels (https:\/\/www.kaggle.com\/delayedkarma\/basic-eda-feature-engineering-and-modeling & https:\/\/www.kaggle.com\/delayedkarma\/grid-search-and-rf-lb-0-80861-top-10)","68d0f4a8":"### The CSCTransformer is used to convert the data to a CSC format (http:\/\/www.scipy-lectures.org\/advanced\/scipy_sparse\/csc_matrix.html) to ensure the XGBoost classifier deals well with sparse data. We don't want to just convert everything to a dense representation since then we will lose the ability to analyze features with zero value, and differentiate missing features. ","b75c0def":" I learnt about ELI5 (https:\/\/eli5.readthedocs.io\/en\/latest\/index.html) from Konstantin Lopuhin's kernel (https:\/\/www.kaggle.com\/lopuhin\/eli5-for-mercari), as well as SRK's kernel on the Quora Insincere Questions Classification Competition (https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc). \n \n I'm learning how to use this, and the ELI5 tutorials have a simple example for the Titanic competition (https:\/\/eli5.readthedocs.io\/en\/latest\/tutorials\/xgboost-titanic.html), so i figured I'd make that into a kernel for ease of reference, because I foresee myself using this library a lot in the future. ","a21c03d2":"### Some name-based features have some weightage","e2b2b573":"### The *show_predictions* method allows us to examine individual predictions","57702832":"### So, if you're classified as a Female and you have paid a higher fare, you are more likely to survive than a passenger in 3rd class. We can also just examine the individual predictions but just for features without missing values. ","62e82e13":"### Now let's check the feature importances for these basic features"}}