{"cell_type":{"821418c7":"code","21e53bbf":"code","4cb77152":"code","0373a563":"code","d25435ef":"code","d259e256":"code","0e894443":"code","5dfe54fc":"code","459639a7":"code","06f8d932":"code","da715573":"code","5a212ed5":"code","9a6ee02f":"code","4b0bc0e5":"code","93411ebd":"code","ba11737d":"markdown","f0f7b064":"markdown","09c8e503":"markdown"},"source":{"821418c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib as mpl \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import cluster, preprocessing, linear_model, tree, model_selection, feature_selection\nfrom sklearn import base, ensemble, decomposition, metrics, pipeline, datasets, impute\nfrom skopt import gp_minimize, space, gbrt_minimize, dummy_minimize, forest_minimize\nfrom functools import partial\nimport os\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nfrom sklearn import ensemble, preprocessing, tree, model_selection, feature_selection, pipeline, metrics, svm\nfrom imblearn import under_sampling, over_sampling, combine\nfrom imblearn import pipeline as imb_pipeline\nfrom imblearn import ensemble as imb_ensemble\nfrom sklearn.model_selection import StratifiedKFold\n\n!pip install rfpimp\n","21e53bbf":"train = pd.read_csv('\/kaggle\/input\/janatahack-customer-segmentation\/Train.csv')\ntest = pd.read_csv('\/kaggle\/input\/janatahack-customer-segmentation\/Test.csv')","4cb77152":"# View target balance\/Imbalance\ntrain['Segmentation'].value_counts()","0373a563":"mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\nrev_mapping = {0: 'A', 1: 'B', 2: 'C', 3: 'D'}","d25435ef":"train.head(2)","d259e256":"## Label Encode and preprocess\ntrain_copy  = train.copy()\ntest_copy = test.copy()\ntrain_copy['tr'] = 1\ntest_copy['tr'] = 0\n\nappended = pd.concat([train_copy, test_copy], axis = 0)\n\ncat_cols = ['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score', 'Var_1']\nlabel_enc = {}\nfor col in cat_cols:\n    appended[col] = appended[col].astype(str)\n    enc = preprocessing.LabelEncoder().fit(appended[col])\n    appended[col] = enc.transform(appended[col])\n    label_enc[col] = enc\n\ncats = ['Gender', 'Ever_Married','Graduated','Profession','Spending_Score',\n'Var_1']\nappended = pd.get_dummies(appended, columns = cats)\n##################### create features from ID column ##############\ndef id_features(data):\n    df = data.copy()\n    df['week'] = df['ID']%7\n    df['month'] = df['ID']%30\n    df['year'] = df['ID']%365\n    df['num_weeks'] = df['ID']\/\/7\n    df['num_year'] = df['ID']\/\/365\n    df['num_quarter'] = df['ID']\/\/90\n    df['quarter'] = df['ID']%90\n    df['num_days'] = df['ID'].values - 458982\n    df['num_weeks_2'] = (df['ID'].values - 458982)\/\/7\n    df['num_months_2'] = (df['ID'].values - 458982)\/\/30\n\n    return df\ndef id_features(data):\n    df = data.copy()\n    df['week'] = df['ID']%7\n    df['month'] = df['ID']%30\n    df['year'] = df['ID']%365\n    df['quarter'] = df['ID']%90\n\n\n    return df\nappended = id_features(appended)\n#appended = pd.get_dummies(appended, columns = cat_cols)\ntrain_copy = appended.loc[appended['tr'] == 1]\ntest_copy = appended.loc[appended['tr'] == 0]\nXcols = appended.drop(['Segmentation', 'tr'], axis = 1).columns\n'''Xcols = ['Gender', 'Ever_Married', 'Age', 'Graduated', 'Profession',\n       'Work_Experience', 'Spending_Score', 'Family_Size', 'Var_1']'''\nycol = 'Segmentation'\n\nX = train_copy[Xcols]\ny = train_copy[ycol]\n\nXtest = test_copy[Xcols]","0e894443":"############ Tune Random Forest\ndef optimize_sk(params, param_names, X, y, scoring, estimator, cv = model_selection.StratifiedKFold(n_splits = 5)):\n    '''params: list of param values\n    param_names: param names\n    x: training exogs\n    y: training endogs\n    return: negative metric after k fold validation'''\n\n    params = dict(zip(param_names, params))\n\n    # Initialize the model\n    model = estimator(**params)\n\n    kf = cv\n\n    scores = []\n    for train_index, test_index in kf.split(X, y):\n        # Split Data\n        X_train, y_train = np.array(X)[train_index, :], y[train_index]\n        X_test, y_test = np.array(X)[test_index, :], y[test_index]\n\n        # Fit model\n        im = impute.KNNImputer().fit(X_train)\n        X_train = im.transform(X_train)\n        model.fit(X_train, y_train)\n\n        # Evaluate model\n        preds = model.predict(im.transform(X_test))\n        scores.append(scoring(y_test, preds))\n\n    return -np.mean(scores)\n\n# Scoring\ndef f1_score(y_true, y_pred):\n    return metrics.f1_score(y_true, y_pred, average = 'macro')\n\ndef accuracy(y_true, y_pred):\n    return metrics.accuracy_score(y_true, y_pred)\n\n# Parameter Space\nparam_space = [\n    space.Integer(100, 1000, name = 'n_estimators'),\n    space.Integer(2, 25, name = 'max_depth'),\n    space.Real(0, 1, name = 'max_features'),\n    space.Integer(2, 25, name = 'min_samples_leaf'),\n    space.Categorical(['gini', 'entropy'], name = 'criterion'),\n    space.Categorical([None, 'balanced', 'balanced_subsample'], name = 'class_weight'),\n    space.Categorical([True, False], name = 'bootstrap')\n]\n\n# Param names\nnames = ['n_estimators', 'max_depth', 'max_features', 'min_samples_leaf', 'criterion', 'class_weight', 'bootstrap']\n\ncat_cols =  ['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score', 'Var_1', 'ID']\ncat_cols =  ['Var_1']\n\n# Define objective - reformat it in terms of what is required for skopt\nobjective_optimization = partial(optimize_sk, param_names = names, X = X, y = y, \n                                scoring = accuracy, estimator = partial(ensemble.RandomForestClassifier, n_jobs = -1, random_state = 0))\n\n# Perform Optimization\n#gbrt_minimize, dummy_minimize, forest_minimize\n'''skopt_optimization = gp_minimize(func = objective_optimization, \n                                dimensions = param_space, n_calls = 10, n_random_starts = 10, \n                                x0 = None, y0 = None, random_state = 10, \n                                verbose = 10)'''\nskopt_optimization = dummy_minimize(func = objective_optimization, \n                                dimensions = param_space, n_calls = 10,\n                                x0 = None, y0 = None, random_state = 10, \n                                verbose = 10)","5dfe54fc":"model = pipeline.make_pipeline(impute.KNNImputer(), \n                               ensemble.RandomForestClassifier(**dict(zip(names, skopt_optimization.x)), \n                                                               n_jobs = -1, random_state = 0)).fit(X, y)","459639a7":"dict(zip(names, skopt_optimization.x))","06f8d932":"from sklearn import impute, pipeline","da715573":"model_cb = cb.CatBoostClassifier( verbose = False)\n\nmodel_lgb = lgb.LGBMClassifier(n_estimators = 1000, min_samples_in_leaf = 10, learning_rate = .02, \n                          feature_fraction = .8, max_depth = 8)\n\n# Soft Voting Classifier\nmodel_voting = ensemble.VotingClassifier([('catboost', model_cb), ('lightgbm', model_lgb)], \n                                         voting = 'soft').fit(X, y)","5a212ed5":"# .94\nmodel4 = pipeline.make_pipeline(impute.KNNImputer(n_neighbors = 10), ensemble.RandomForestClassifier(class_weight = 'balanced_subsample',\n                    n_estimators = 200, max_depth = 20, criterion = 'entropy', max_features = .8, oob_score = True, random_state = 0)).fit(X, y)","9a6ee02f":"# .94\nmodel2 = lgb.LGBMClassifier(n_estimators=300, max_features = .85, max_depth = 15, learning_rate = 1.1).fit(X, y)","4b0bc0e5":"###### View Feature importance  -  Using Permutation Importance\nimport rfpimp\n\nimp = rfpimp.importances(model2, X, y)\nimp","93411ebd":"pred = pd.DataFrame()\npred['ID'] = test['ID'].values\npred['Segmentation'] = pd.Series(model2.predict(Xtest))\npred.to_csv('Seg.csv', index = None)","ba11737d":"# Ensemble and perform hand Tuning. Tuning models by hand gave me better results.","f0f7b064":"# Tune Hyperparameters - skopt","09c8e503":"# The best pipelines so far:\n### * kNN Imputation + RandomForest\n### * Lightgbm"}}