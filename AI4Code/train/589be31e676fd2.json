{"cell_type":{"de5aa6e4":"code","6daf17f1":"code","86652465":"code","9bee08d4":"code","2239d923":"code","d073090c":"code","77d7639b":"code","dcd0030d":"code","de396691":"code","05e2fce5":"code","18ee19a5":"code","84222095":"code","bd25c2c0":"code","72c79e26":"code","dfd19a94":"code","31f7465a":"code","9e99c1d8":"code","464d2589":"code","a9740309":"code","beb7cd1d":"code","6e63b703":"code","129e0836":"code","7b360097":"markdown","82943341":"markdown","a13cd16b":"markdown","0f9c7c75":"markdown","3efa4814":"markdown","d03cb7f2":"markdown","0d73b7fb":"markdown","fee27b42":"markdown","cc7a0907":"markdown","bc137498":"markdown"},"source":{"de5aa6e4":"!pip install timm","6daf17f1":"import os\nimport time\nfrom glob import glob\nimport random\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nfrom torch import nn\n\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.model_selection import StratifiedKFold\n\nimport cv2\n\nfrom tqdm import tqdm\n\nimport timm","86652465":"CFG = {\n    'rootdir': '.\/classify-leaves\/',\n    'fold_num': 5,\n    'seed': 123,\n    'model_arch': 'tf_efficientnet_l2_ns_475',\n    'img_size': 224,\n    'epochs': 8,\n    'train_bs': 12,\n    'valid_bs': 12,\n    'T_0': 4, # Number of iterations for the first restart\n    'lr': 2e-4,\n    'min_lr': 1e-6,\n    'weight_decay':1e-6,\n    'num_workers': 8,\n    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1, # 0 (silent),1 (progress bar), 2 (one line per epoch)\n    'device': 'cuda:1'\n}","9bee08d4":"train = pd.read_csv('..\/input\/classify-leaves\/train.csv')\ntrain_clean = pd.read_csv('..\/input\/d\/baishaothu\/classify-leaves\/train_clean.csv')","2239d923":"t_labelencoder = preprocessing.LabelEncoder()\nt_labelencoder.fit(train['label'])\ntrain_clean['label'] = t_labelencoder.transform(train_clean['label'])","d073090c":"train_clean","77d7639b":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","dcd0030d":"# Prepare Data\nclass MyDataset(Dataset):\n    def __init__(self, df, data_root, transforms=None, output_label=True):\n        \n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.data_root = data_root\n        \n        self.output_label = output_label\n        \n        if output_label == True:\n            self.labels = self.df['label'].values\n            \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        # get labels\n        if self.output_label:\n            target = self.labels[index]\n          \n        img  = get_img(\"{}\/{}\".format(self.data_root, self.df.loc[index]['image']))\n\n        if self.transforms:\n            img = self.transforms(image=img)['image']\n        \n        if self.output_label == True:\n            return img, target\n        else:\n            return img","de396691":"# Augmentation\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, Rotate, ShiftScaleRotate, RandomBrightnessContrast, Perspective, CLAHE, \n    Transpose, Blur, OpticalDistortion, GridDistortion, HueSaturationValue, ColorJitter, GaussNoise, MotionBlur, MedianBlur,\n    Emboss, Sharpen, Flip, OneOf, SomeOf, Compose, Normalize, CoarseDropout, CenterCrop, GridDropout, Resize\n)\nfrom albumentations.pytorch import ToTensorV2","05e2fce5":"def get_train_transforms():\n    return Compose([\n            OneOf([\n            CoarseDropout(p=0.5),\n            GaussNoise(),\n            ], p=0.5),\n            SomeOf([\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            ], n=3, p=0.6),\n            Resize(CFG['img_size'], CFG['img_size']),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)","18ee19a5":"def get_valid_transforms():\n    return Compose([\n            Resize(CFG['img_size'], CFG['img_size']),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)","84222095":"def prepare_dataloader(df, trn_idx, val_idx, data_root='..\/input\/classify-leaves\/'):\n\n    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n        \n    train_ds = MyDataset(train_, data_root, transforms=get_train_transforms(), output_label=True)\n    valid_ds = MyDataset(valid_, data_root, transforms=get_valid_transforms(), output_label=True)\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_ds,\n        batch_size=CFG['train_bs'],\n        pin_memory=True,\n        drop_last=False,\n        shuffle=True,        \n        num_workers=CFG['num_workers'],\n    )\n    val_loader = torch.utils.data.DataLoader(\n        valid_ds, \n        batch_size=CFG['valid_bs'],\n        num_workers=CFG['num_workers'],\n        shuffle=False,\n        pin_memory=True,\n    )\n    return train_loader, val_loader","bd25c2c0":"# Model\nclass EfficientModel(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_arch, pretrained=pretrained)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(n_features, n_class)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","72c79e26":"# Training\ndef train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None):\n    model.train()\n\n    running_loss = None\n\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for step, (imgs, image_labels) in pbar:\n        imgs = imgs.to(device).float()\n        image_labels = image_labels.to(device).long()\n\n        with autocast():\n            image_preds = model(imgs)\n\n            loss = loss_fn(image_preds, image_labels)\n            scaler.scale(loss).backward()\n\n            if running_loss is None:\n                running_loss = loss.item()\n            else:\n                running_loss = running_loss * .99 + loss.item() * .01\n\n            if ((step + 1) %  CFG['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad() \n\n            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n                description = f'epoch {epoch} loss: {running_loss:.4f}'\n                \n                pbar.set_description(description)\n                \n    if scheduler is not None:\n        scheduler.step()\n        \n    return running_loss","dfd19a94":"def valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None):\n    model.eval()\n\n    loss_sum = 0\n    sample_num = 0\n    image_preds_all = []\n    image_targets_all = []\n    val_loss = None\n    \n    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n    for step, (imgs, image_labels) in pbar:\n        imgs = imgs.to(device).float()\n        image_labels = image_labels.to(device).long()\n        \n        image_preds = model(imgs)\n        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n        image_targets_all += [image_labels.detach().cpu().numpy()]\n        \n        loss = loss_fn(image_preds, image_labels)\n        \n        loss_sum += loss.item()*image_labels.shape[0]\n        sample_num += image_labels.shape[0]\n\n        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n            val_loss = loss_sum\/sample_num\n            description = f'epoch {epoch} loss: {val_loss:.4f}'\n            pbar.set_description(description)\n    \n    image_preds_all = np.concatenate(image_preds_all)\n    image_targets_all = np.concatenate(image_targets_all)\n    \n    val_acc = (image_preds_all==image_targets_all).mean()\n    print('validation multi-class accuracy = {:.4f}'.format(val_acc))\n    \n    if scheduler is not None:\n        scheduler.step()\n        \n    return val_acc, val_loss","31f7465a":"if __name__ == '__main__':\n\n    seed_everything(CFG['seed'])\n    \n    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train_clean.shape[0]), train_clean.label.values)\n    train_loss_all = []\n    val_loss_all = []\n    val_acc_all = []\n    for fold, (trn_idx, val_idx) in enumerate(folds):\n        start_time = time.time()\n        # \u6d4b\u8bd5\u4e24\u4e2afold\n        # if fold > 1:\n        #     break \n            \n        print('Training with {} started'.format(fold))\n\n        print(len(trn_idx), len(val_idx))\n        train_loader, val_loader = prepare_dataloader(train_clean, trn_idx, val_idx, data_root=CFG['rootdir'])\n\n        device = torch.device(CFG['device'])\n        \n        model = EfficientModel(CFG['model_arch'], train_clean.label.nunique(), pretrained=True).to(device)\n        scaler = GradScaler()\n        optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=CFG['epochs']-1)\n        \n        loss_tr = nn.CrossEntropyLoss().to(device)\n        loss_fn = nn.CrossEntropyLoss().to(device)\n        train_loss_temp = []\n        val_loss_temp = [] \n        val_acc_temp = []\n        \n        for epoch in range(CFG['epochs']):\n            train_loss_temp = train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, device, scheduler=scheduler)\n            train_loss_all.append([fold, epoch, train_loss_temp])\n            with torch.no_grad():\n                val_acc_temp, val_loss_temp = valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None)\n                \n                val_loss_all.append([fold, epoch, val_loss_temp])\n                val_acc_all.append([fold, epoch, val_acc_temp])\n            if epoch > CFG['epochs'] - 4:  #save last three models\n                torch.save(model.state_dict(),'{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch))\n                \n        del model, optimizer, train_loader, val_loader, scaler, scheduler\n        torch.cuda.empty_cache()","9e99c1d8":"# Inference Loop\nCFG['weights'] = [0.8,1,0.9] # weight for out model\nCFG['tta'] = 3 # set TTA times\nCFG['used_epochs'] = [5,6,7] # choose the model\ndef get_inference_transforms():\n    return Compose([\n            Resize(CFG['img_size'], CFG['img_size']),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            #HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            #RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)","464d2589":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        \n        image_preds = model(imgs)\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all","a9740309":"if __name__ == '__main__':\n    \n    seed_everything(CFG['seed'])\n    \n    folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train_clean.shape[0]), train_clean.label.values)\n    tst_preds_all = []\n    for fold, (trn_idx, val_idx) in enumerate(folds):\n        print(fold)\n        # if fold > 1:\n        #     break\n\n        print('Inference fold {} started'.format(fold))\n        \n        valid_ = train_clean.loc[val_idx,:].reset_index(drop=True)\n        valid_ds = MyDataset(valid_, CFG['rootdir'], transforms=get_inference_transforms(), output_label=False)\n\n        test = pd.read_csv(CFG['rootdir'] + 'test.csv')\n        test_ds = MyDataset(test, CFG['rootdir'], transforms=get_inference_transforms(), output_label=False)\n\n        val_loader = torch.utils.data.DataLoader(\n            valid_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=True,\n        )\n\n        tst_loader = torch.utils.data.DataLoader(\n            test_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=True,\n        )\n        \n        device = torch.device(CFG['device'])\n        model = EfficientModel(CFG['model_arch'], train.label.nunique()).to(device)\n        \n        val_preds = []\n        tst_preds = []\n        \n        for i, epoch in enumerate(CFG['used_epochs']):    \n            model.load_state_dict(torch.load('{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch)))\n            print('{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch))\n\n            with torch.no_grad():\n                for _ in range(CFG['tta']):\n                    # val_preds += [CFG['weights'][i]\/sum(CFG['weights'])\/CFG['tta']*inference_one_epoch(model, val_loader, device)]\n                    tst_preds += [CFG['weights'][i]\/sum(CFG['weights'])\/CFG['tta']*inference_one_epoch(model, tst_loader, device)]\n\n                    \n        # val_preds = np.mean(val_preds, axis=0) \n        tst_preds = np.mean(tst_preds, axis=0) \n        tst_preds_all.append(tst_preds)\n        \n        # print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n        # print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n        del model\n        torch.cuda.empty_cache()","beb7cd1d":"# calculate the average result from all folds\navg_tst = np.mean(tst_preds_all, axis=0)\n\ntest['label'] = np.argmax(avg_tst, axis=1)\ntest.head()\n","6e63b703":"# inverse our label\ntest['label'] = t_labelencoder.inverse_transform(test['label'])\n\ntest.to_csv('submission.csv', index=False)","129e0836":"Private Score\uff1a0.98636   18th\nPublic  Score\uff1a0.98295   14th","7b360097":"## 5.\u6a21\u578b","82943341":"## 8.\u9884\u6d4b","a13cd16b":"## 3.\u8bfb\u53d6\u6570\u636e","0f9c7c75":"### \u611f\u8c22Xichao Wang\u63d0\u4f9b\u7684baseline\uff0c\u672c\u5b9e\u9a8c\u57fa\u4e8e\u6b64baseline\u7a0d\u4f5c\u8c03\u6574\uff0c\u5728\u4e0b\u9762\u505a\u7b80\u5355\u9648\u5217\uff1a\n- \u6a21\u578b\uff1a\u4f7f\u7528tf_efficientnet_l2_ns_475\n- \u6570\u636e\uff1a\u53c2\u8003\u4e86Xichao Wang\u7684\u65b9\u6cd5\uff0c\u5bf9train\u505a\u4e86\u6570\u636e\u6e05\u6d17\uff0c\u5220\u9664\u4e86\u90e8\u5206\u6570\u636e\n- \u53c2\u6570\uff1a\u56e0\u4e3a\u4f7f\u7528\u4e86\u5927\u6a21\u578b\uff0c\u53c2\u6570\u90fd\u5f80\u5c0f\u4e86\u8c03\n- \u6570\u636e\u589e\u5f3a\uff1a\u9009\u53d6\u4e86Transpose,HorizontalFlip,VerticalFlip,GaussNoise\u51e0\u79cd\u57fa\u672c\u7684\u6570\u636e\u589e\u5f3a\u65b9\u5f0f\uff0c\u6ca1\u6709\u4f7f\u7528\u8c03\u8272\u76f8\u5173\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\n\n\u7531\u4e8e\u65f6\u95f4\u5173\u7cfb\uff0c\u6709\u4e9b\u60f3\u6cd5\u5c1a\u672a\u5b9e\u73b0\uff0c\u4f9b\u5927\u5bb6\u4ea4\u6d41\uff1a\n- Normalize\uff1a\u672c\u6bd4\u8d5b\u4f7f\u7528Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\uff0c\u5982\u679c\u9488\u5bf9\u672c\u6570\u636e\u96c6\u8ba1\u7b97\u51fa\u5bf9\u5e94\u7684\u5404\u901a\u9053\u5747\u503c\u548c\u6807\u51c6\u5dee\uff0c\u662f\u5426\u4f1a\u6709\u63d0\u5347\u6548\u679c\u3002\n- \u6570\u636e\u6e05\u6d17\uff1a\u5bf9Xichao Wang\u7684\u6570\u636e\u6e05\u6d17\u65b9\u5f0f\u8fdb\u884c\u6539\u8fdb\uff0c\u9488\u5bf9\u6709\u4e89\u8bae\u7684\u6570\u636e\uff08\u56fe\u7247\u76f8\u540c\uff0c\u6807\u7b7e\u4e0d\u540c\uff09\u4e88\u4ee5\u5168\u90e8\u6e05\u9664\n- \u591a\u6a21\u578b\u96c6\u6210\n\n","3efa4814":"## 7.main","d03cb7f2":"## 6.\u8bad\u7ec3","0d73b7fb":"## 2.\u5b9a\u4e49\u53c2\u6570","fee27b42":"## 4.\u6570\u636e\u589e\u5f3a","cc7a0907":"## 1.\u5bfc\u5165\u5e93","bc137498":"## 9.\u6210\u7ee9"}}