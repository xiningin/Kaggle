{"cell_type":{"6116f432":"code","66ef4220":"code","dd139e26":"code","b8d81b17":"code","c869ba89":"code","dc43d7ef":"code","8a48fbff":"code","a7b93567":"code","083e7973":"code","b404ab43":"code","ad30e95d":"code","ff114eef":"code","375f3b75":"code","346ef31b":"code","261c6913":"code","176b844b":"code","ed71b65d":"code","5cfe4d89":"code","b00f75a4":"code","cd0a84fe":"code","df6bde99":"code","2c7ba5d8":"code","d4d1929e":"code","a2b95b9e":"code","816e1b2d":"code","79c455bb":"code","2646833b":"code","a259cf2a":"code","c66c4046":"code","09061b9c":"code","4f7855dc":"code","9aeb6955":"code","9bd62648":"code","b51d50fb":"code","a44cb62c":"code","694a9dc3":"code","3248b753":"code","cfa0c51c":"code","e3aa5748":"code","63b7cf14":"code","5a7866ed":"code","c86da36a":"code","55e15797":"code","0bea6c43":"code","e8c2d266":"code","029fbbfb":"code","d1f89305":"code","d57dc54a":"code","2332345d":"code","ca30a7b0":"code","0201c1b6":"code","0b869c80":"code","a4bb7235":"code","531ad3e2":"code","91202ff1":"code","87df865c":"code","ae29b1cc":"code","09b1db7a":"code","bb94de8c":"markdown","6c1e1976":"markdown","27e3f0ba":"markdown","c4004830":"markdown","f4bdd918":"markdown","aecf9e60":"markdown","ca04879c":"markdown","d39e907d":"markdown","e83eef8e":"markdown","d5215033":"markdown","635d32a9":"markdown","6d2b29df":"markdown","53a25344":"markdown","2743e202":"markdown","5d743e5b":"markdown","7f7535af":"markdown"},"source":{"6116f432":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66ef4220":"df=pd.read_csv('\/kaggle\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')","dd139e26":"df.head()\n## change column names\n##df.columns.str.lower().str.replace(' ','_')","b8d81b17":"df.columns","c869ba89":"## type of each column\ndf.dtypes","dc43d7ef":"strings=list(df.dtypes[df.dtypes=='object'].index)\nstrings","8a48fbff":"for col in strings:\n    df[col]=df[col].str.lower().str.replace(' ','_')","a7b93567":"for col in df.columns:\n    print(col)\n    print(df[col].unique()[:5]) #first five unique values\n    print(df[col].nunique()) #how many unique values","083e7973":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","b404ab43":"sns.histplot(df.price,bins=50) #long tail distribution->zoom in\n#This will confuse data, so we want to use log","ad30e95d":"sns.histplot(df.price[df.price<1000],bins=50)","ff114eef":"np.log1p([0,1])","375f3b75":"price_log=np.log1p(df.price)","346ef31b":"sns.histplot(price_log,bins=50)\n#bell shaped, normal distribution, ideal model","261c6913":"cols=['latitude',\n'longitude',\n'price',\n'minimum_nights',\n'number_of_reviews',\n'reviews_per_month',\n'calculated_host_listings_count',\n'availability_365']","176b844b":"df1=df[cols]","ed71b65d":"df1.isnull().sum()","5cfe4d89":"df1['minimum_nights'].median()","b00f75a4":"n=len(df1)\nn_val=int(n*0.2)\nn_test=int(n*0.2)\nn_train=n-n_val-n_test","cd0a84fe":"n,n_val,n_test,n_train","df6bde99":"df1.iloc[:10] ","2c7ba5d8":"idx=np.arange(n)","d4d1929e":"np.random.seed(42)\nnp.random.shuffle(idx)","a2b95b9e":"df_train=df1.iloc[idx[:n_train]]\ndf_val=df1.iloc[idx[n_train:n_train+n_val]]\ndf_test=df1.iloc[idx[n_train+n_val:]]","816e1b2d":"df_train.head()","79c455bb":"len(df_train),len(df_test),len(df_val)","2646833b":"df_train=df_train.reset_index(drop=True)\ndf_val=df_val.reset_index(drop=True)\ndf_test=df_test.reset_index(drop=True)","a259cf2a":"y_train=np.log1p(df_train.price.values)\ny_val=np.log1p(df_val.price.values)\ny_test=np.log1p(df_test.price.values)","c66c4046":"del df_train['price']\ndel df_val['price']\ndel df_test['price']","09061b9c":"X_train=df_train.fillna(0).values","4f7855dc":"def train_linear_regression(X, y):\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n    XTX = X.T.dot(X)\n    XTX_inv = np.linalg.inv(XTX)\n    w = XTX_inv.dot(X.T).dot(y)\n    return w[0], w[1:]","9aeb6955":"w0,w=train_linear_regression(X_train,y_train)","9bd62648":"y_pred=w0+X_train.dot(w)","b51d50fb":"sns.histplot(y_pred,color='red',alpha=0.5,bins=30)\nsns.histplot(y_train,color='green',alpha=0.5,bins=30)","a44cb62c":"def rmse(y,y_pred):\n    se=(y- y_pred)**2\n    mse=se.mean()\n    return np.sqrt(mse)","694a9dc3":"rmse(y_train,y_pred)","3248b753":"def prepare_X(df):\n    df=df.copy()\n    df_num=df\n    df_num=df_num.fillna(0)\n    X=df_num.values\n    return X","cfa0c51c":"X_val=prepare_X(df_val)\ny_vpred=w0+X_val.dot(w)\nrmse(y_val,y_vpred)","e3aa5748":"train_mean=df_train.mean()","63b7cf14":"X_newtrain=df_train.fillna(train_mean).values","5a7866ed":"w0,w=train_linear_regression(X_newtrain,y_train)","c86da36a":"y_newpred=w0+X_newtrain.dot(w)","55e15797":"sns.histplot(y_newpred,color='red',alpha=0.5,bins=30)\nsns.histplot(y_train,color='green',alpha=0.5,bins=30)","0bea6c43":"X_val=prepare_X(df_val)\ny_newvpred=w0+X_val.dot(w)\nrmse(y_val,y_newvpred)","e8c2d266":"df.head()","029fbbfb":"df.neighbourhood_group.value_counts()","d1f89305":"# def catprepare_X(df):\n#     df=df.copy()\n#     features=cols.copy()\n#     for v in ['manhattan','brooklyn','queens','bronx','staten_island']:\n#         df['neighbourhood_group_%s' %v]=(df.neighbourhood_group==v).astype('int')\n#         features.append('neighbourhood_group')\n#     df_num=df[features]\n#     df_num=df_num.fillna(0)\n#     X=df_num.values\n#     return X","d57dc54a":"# df_trainc=df.iloc[idx[:n_train]]\n# df_valc=df.iloc[idx[n_train:n_train+n_val]]\n# df_testc=df.iloc[idx[n_train+n_val:]]","2332345d":"# X_trainc=catprepare_X(df_trainc)\n# w0,w=train_linear_regression(X_trainc,y_train)\n# X_valc=catprepare_X(df_valc)\n# y_predc=w0+X_valc.dot(w)\n\n# rmse(y_val,y_predc)","ca30a7b0":"X_train=prepare_X(df_train)\nw0,w=train_linear_regression_reg(X_train,y_train,r=0.01)\n\nX_val=prepare_X(df_val)\ny_pred=w0+X_val.dot(w)\nrmse(y_val,y_pred)","0201c1b6":"X_train=df_train.fillna(0).values","0b869c80":"def train_linear_regression_reg(X, y,r=0.001):\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n    \n    XTX = X.T.dot(X)\n    XTX = XTX + r * np.eye(XTX.shape[0])\n    \n    XTX_inv = np.linalg.inv(XTX)\n    w = XTX_inv.dot(X.T).dot(y)\n    \n    return w[0], w[1:]","a4bb7235":"X_train=prepare_X(df_train)\nX_val=prepare_X(df_val)\nscores=[]\nfor r in  [0, 0.000001, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10]:\n    w0,w=train_linear_regression_reg(X_train,y_train,r)\n    y_pred=w0+X_val.dot(w)\n    scores.append(round(rmse(y_val,y_pred),2))\nprint(scores)","531ad3e2":"def train_linear_regression(X, y):\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n    XTX = X.T.dot(X)\n    XTX_inv = np.linalg.inv(XTX)\n    w = XTX_inv.dot(X.T).dot(y)\n    return w[0], w[1:]","91202ff1":"def train_linear_regression_reg(X, y,r=0.001):\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n    \n    XTX = X.T.dot(X)\n    XTX = XTX + r * np.eye(XTX.shape[0])\n    \n    XTX_inv = np.linalg.inv(XTX)\n    w = XTX_inv.dot(X.T).dot(y)\n    \n    return w[0], w[1:]","87df865c":"def rmse(y,y_pred):\n    se=(y- y_pred)**2\n    mse=se.mean()\n    return np.sqrt(mse)","ae29b1cc":"idx=np.arange(n)\nscores=[]\nfor seed in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n    np.random.seed(seed)\n    np.random.shuffle(idx)\n    df_train=df1.iloc[idx[:n_train]]\n    df_val=df1.iloc[idx[n_train:n_train+n_val]]\n    df_test=df1.iloc[idx[n_train+n_val:]]\n    X_train=df_train.fillna(0).values\n    X_train=prepare_X(df_train)\n    X_val=prepare_X(df_val)\n    w0,w=train_linear_regression(X_train,y_train)\n    y_pred=w0+X_val.dot(w)\n    scores.append((rmse(y_val,y_pred)))\nprint(scores)\nstd=np.std(scores)\nprint(round(std,3))","09b1db7a":"idx=np.arange(n)\nnp.random.seed(9)\nnp.random.shuffle(idx)\ndf_train=df1.iloc[idx[:n_train]]\ndf_val=df1.iloc[idx[n_train:n_train+n_val]]\ndf_test=df1.iloc[idx[n_train+n_val:]]\nX_train=df_train.fillna(0).values\nX_train=prepare_X(df_train)\nX_val=prepare_X(df_val)\nX_test=prepare_X(df_test)\nw0,w=train_linear_regression_reg(X_train,y_train,r=0.001)\ny_pred=w0+X_test.dot(w)\nscore=rmse(y_test,y_pred)\nprint(score)","bb94de8c":"* Split the data","6c1e1976":"## Question 5\n* We used seed 42 for splitting the data. Let's find out how selecting the seed influences our score.\n* Try different seed values: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9].\n* For each seed, do the train\/validation\/test split with 60%\/20%\/20% distribution.\n* Fill the missing values with 0 and train a model without regularization.\n* For each seed, evaluate the model on the validation dataset and collect the RMSE scores.\n* What's the standard deviation of all the scores? To compute the standard deviation, use np.std.\n* Round the result to 3 decimal digits (round(std, 3))\n\n>Note: Standard deviation shows how different the values are. If it's low, then all values are approximately the same. If it's high, the values are different. If standard deviation of scores is low, then our model is stable.\n\n","27e3f0ba":" fill missing values with 0","c4004830":"## Question 2\nWhat's the median (50% percentile) for variable 'minimum_nights'?","f4bdd918":"Fill NAN with mean of train ","aecf9e60":"## Data Preparation","ca04879c":"* Make sure that the target value ('price') is not in your dataframe.\n* Apply the log transformation to the price variable using the np.log1p() function.","d39e907d":"It is reviews_per_month with 10052 missing values","e83eef8e":"## Question 1\nFind a feature with missing values. How many missing values does it have?","d5215033":"* Split your data in train\/val\/test sets, with 60%\/20%\/20% distribution.","635d32a9":"## Categorical variables","6d2b29df":"## Question 3\n* We need to deal with missing values for the column from Q1.\n* We have two options: fill it with 0 or with the mean of this variable.\n* Try both options. For each, train a linear regression model without regularization using the code from the lessons.\n* For computing the mean, use the training only!\n* Use the validation dataset to evaluate the models and compare the RMSE of each option.\n* Round the RMSE scores to 2 decimal digits using round(score, 2)\n* Which option gives better RMSE?","53a25344":"## Question 4\n* Now let's train a regularized linear regression.\n* For this question, fill the NAs with 0.\n* Try different values of r from this list: [0, 0.000001, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10].\n* Use RMSE to evaluate the model on the validation dataset.\n* Round the RMSE scores to 2 decimal digits.\n* Which r gives the best RMSE?\n\nIf there are multiple options, select the smallest r.","2743e202":"* Shuffle the initial dataset, use seed 42.\n","5d743e5b":"The best r is 0","7f7535af":"## Question 6\n* Split the dataset like previously, use seed 9.\n* Combine train and validation datasets.\n* Fill the missing values with 0 and train a model with r=0.001.\n* What's the RMSE on the test dataset?"}}