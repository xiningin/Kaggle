{"cell_type":{"a02d4c41":"code","8edba91e":"code","39a6f7a7":"code","07323d18":"code","4f0a4fe7":"code","84c8c4bf":"code","c861065a":"code","d80a9d93":"code","d1c2d761":"code","2ca19f58":"code","9b654dfb":"code","ab521c9b":"code","1d02b38b":"code","eb74a852":"code","844f0ff8":"code","a27e4fb7":"code","9fa01f94":"code","2025535a":"code","7826f5d1":"code","53570624":"code","8cff57ee":"code","dae9d592":"code","4d4e9bbf":"code","ba7169aa":"code","4854ed3a":"code","e44a944d":"code","e30744a7":"code","1da50905":"code","381d9f42":"code","ac3a68d6":"code","66c05e1d":"code","93947624":"code","b4aad82c":"code","c74dc678":"code","210eba74":"code","f990534a":"code","f48fdb20":"code","89bb31e0":"code","060d2ccf":"code","b1237019":"code","51dee844":"code","a3a1f08e":"code","4aa30765":"code","1ec6b7c2":"code","519af353":"code","0aa4e76c":"code","4d060bde":"code","c1509265":"code","61ecb47a":"code","96b309ac":"code","03a17d91":"markdown","1b78296c":"markdown","5a08f83c":"markdown","e0aefe94":"markdown","e793939a":"markdown","bec4d59f":"markdown","1ce63fb6":"markdown","a5c6e33e":"markdown","346e7c0f":"markdown","6e42916b":"markdown","5c5e372c":"markdown","a038dd30":"markdown","991b9637":"markdown","3c245abe":"markdown","c1709718":"markdown","4686316f":"markdown","74436a82":"markdown","3677d4a9":"markdown","9e4d3d39":"markdown","dcba43d0":"markdown","7cf7087a":"markdown","e0a731d4":"markdown","b4e1bf1d":"markdown","42cebbc3":"markdown","d48c3efb":"markdown","c6ab918e":"markdown","e5072911":"markdown","093c455b":"markdown","4064c4f4":"markdown","4c533c2f":"markdown","5a8361bf":"markdown","6d8891f9":"markdown","e002d7c8":"markdown","93126179":"markdown","552b349a":"markdown","1dc2a218":"markdown","35eacea5":"markdown","37c1828e":"markdown","51659603":"markdown","c617534d":"markdown","36e479f6":"markdown","70dc722b":"markdown","7b951e98":"markdown","ecf1b0ce":"markdown","7a4a4320":"markdown","a942700e":"markdown","dc63df14":"markdown"},"source":{"a02d4c41":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8edba91e":"#libraries to transform and plot data\nimport matplotlib.pyplot as plt\nimport math\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer,KNNImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\npd.set_option('display.max_columns',500)\nfrom datetime import datetime as dt\nimport warnings\nwarnings.filterwarnings('ignore')\n#libraries to preprocess data, create the model and evaluate the performance of the model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import RandomizedSearchCV\nimport itertools\n#library to do some statistics on data\nfrom scipy import stats\npd.set_option('display.max_colwidth', 400)\npd.set_option('display.max_rows', 500)","39a6f7a7":"Acq_Auser = pd.read_csv(r'..\/input\/acea-water-prediction\/Aquifer_Auser.csv')\nAcq_Doganella = pd.read_csv(r'..\/input\/acea-water-prediction\/Aquifer_Doganella.csv')\nAcq_Luco =  pd.read_csv(r'..\/input\/acea-water-prediction\/Aquifer_Luco.csv')\nAcq_Petrignano = pd.read_csv(r'..\/input\/acea-water-prediction\/Aquifer_Petrignano.csv')\nRiver_Arno = pd.read_csv(r'..\/input\/acea-water-prediction\/River_Arno.csv')\nLake_Bilancino = pd.read_csv(r'..\/input\/acea-water-prediction\/Lake_Bilancino.csv')\nWs_Amiata = pd.read_csv(r'..\/input\/acea-water-prediction\/Water_Spring_Amiata.csv')\nWs_Lupa = pd.read_csv(r'..\/input\/acea-water-prediction\/Water_Spring_Lupa.csv')\nWs_Madonna_di_Canneto = pd.read_csv(r'..\/input\/acea-water-prediction\/Water_Spring_Madonna_di_Canneto.csv')\nprint(\"Dati importati\")","07323d18":"class DataPreparation(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class is used to prepare data before Features engineering. It return a dataframe \n    without null values and with Date field as index. KNNImputer is used to impute\n    null values\n    \"\"\"\n    def __init__(self, target):\n        self.target = target\n        \n    def fit(self, X, y = None):\n        pass\n    \n    def transform(self, X):\n        Xt = X.copy()\n        target = [col for col in Xt.columns if col.startswith(self.target)]\n        features = [col for col in Xt.columns if col not in self.target]\n        max_na_target_variable =[]\n        for col in target:\n            max_na_target_variable.append(Xt[col].notnull().idxmax())\n            df_temp = Xt.iloc[max(max_na_target_variable)+1:,:]\n        df_temp['Date'] = pd.to_datetime(df_temp['Date'],dayfirst = True, format = '%d\/%m\/%Y')\n        df_temp.set_index('Date', inplace = True)\n        imputer = KNNImputer(n_neighbors=15, weights=\"distance\")\n        imputed = imputer.fit_transform(df_temp)\n        df_imputed = pd.DataFrame(imputed,index = df_temp.index, columns = df_temp.columns)\n        return df_imputed\n\n    \n    \nclass FeatureEngineering(BaseEstimator, TransformerMixin):\n    def __init__(self,target, shift_period = 1, resampled_freq = 'W'):\n        self.target = target\n        self.shift_period = shift_period\n        self.resampled_freq = resampled_freq\n        \n    def fit(self, X, y = None):\n        pass\n    \n    def transform(self,X):\n        Xt = X.copy()\n        rainfall_columns = [col for col in Xt.columns if col.startswith('Rain')]\n        for col in rainfall_columns:\n            Xt[f'Day_rainy_{col}'] = Xt[col].apply(lambda x : 1 if x > 0 else 0)\n            Xt[f'Day_not_rainy_{col}'] = Xt[col].apply(lambda x : 1 if x == 0 else 0 )\n        aggregation_dict  = {}\n        for col in Xt.columns:\n            if col.startswith(('Depth', 'Temp','Volu','Hydrom', 'Flow','Lake')):\n                aggregation_dict[col] = 'mean'\n            elif col.startswith(('Rain','Day')):\n                aggregation_dict[col] = 'sum'\n            else:\n                aggregation_dict[col] = 'mean'\n        for col in Xt.columns:\n            if col.startswith('Flow'):\n                Xt[col] = Xt[col].abs()\n        Xt = Xt.resample(self.resampled_freq).agg(aggregation_dict)\n        Xt['Month'] = Xt.index.month\n        Xt['Year'] = Xt.index.year\n        Xt['Quarter'] = Xt.index.quarter\n        Xt['Season'] = (Xt.index.month) % 12 \/\/ 3 +1\n        Xt['Total_days_rainy'] = 0\n        for col in Xt.columns:\n            if col.startswith('Rain'):\n                Xt['Total_days_rainy'] = Xt['Total_days_rainy'] + Xt[col]\n        targets = [col for col in Xt.columns if col.startswith(self.target)]\n        features = [col for col in Xt.columns if col not in targets]\n        for col in features:\n            for i in range(1,self.shift_period):\n                Xt[f'Differenced_{col}_by_{i}'] = Xt[col].diff(periods = i)\n            Xt[col] = Xt[col].shift(self.shift_period)\n        Xt.dropna(axis = 0, inplace = True)\n        return Xt\n            ","4f0a4fe7":"class EDA():\n    def __init__(self, df, target,df_name, width=20, hight=10):\n        self.df = df\n        self.target = target\n        self.width = width\n        self.hight = hight\n        self.df_name = df_name\n    def plot_target(self):\n        target_list = [col for col in self.df.columns if col.startswith(self.target)]\n        fig, axes = plt.subplots(nrows = 1, ncols = 2,squeeze = False, figsize = (self.width, self.hight))\n        for i in range(len(target_list)):\n            sns.lineplot(x =self.df.index, y =self.df[target_list[i]] ,ax = axes[0][0], label=target_list[i])\n            axes[0][0].set_title(f\"Target variables for {self.df_name} \")\n        sns.heatmap(self.df.corr(), ax = axes[0][1])\n        axes[0][1].set_title(f\"Correltation plot for {self.df_name}  \")\n        plt.tight_layout()","84c8c4bf":"class TrainTestSplit:\n    \"\"\"\n    This class is used to split data in train and test. Input data is split ratio and as output are \n    returned four dataframes: X_train, y_train, X_test, y_test\n    \"\"\"\n    def __init__(self,df,target,split_ratio = 0.8):\n        self.df = df\n        self.split_ratio = split_ratio\n        self.target = target\n    def split(self):\n        temp_df = (self.df).copy()\n        target_list = [col for col in temp_df.columns if col.startswith(self.target)]\n        features = [col for col in temp_df.columns if col not in target_list]\n        X = temp_df[features]\n        y = temp_df[target_list]\n        train_size = math.ceil((temp_df.shape[0])*0.8)\n        X_train = X.iloc[:train_size , :]\n        y_train = y.iloc[:train_size, :] \n        X_test = X.iloc[train_size:,:]\n        y_test = y.iloc[train_size:,:]\n        return X_train, y_train, X_test, y_test","c861065a":"class Model():\n    \"\"\"\n    This is the class that model the data. A pipe is created to model data. Pipe is encapsulated in a\n    RandomizedSearchCV to have the flexibility to search for the best parameters both for scaler and RandoForestRegressor.\n    As output data predicted, dataframe with residuals, dataframe with importances and best parameters for the model.\n    \"\"\"\n    def __init__(self,X_train, y_train,X_test, y_test):\n        self.X_train =X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n    def model(self):\n        pipe = Pipeline([('scaler', RobustScaler()),('rfr',RandomForestRegressor(random_state = 0))])\n        param_grid = {'rfr__criterion': ['mse','mae'], 'rfr__max_features':['auto','log2'],\n                     'rfr__n_estimators':[10,20,30,40,50], 'rfr__min_samples_split':[2,4,8]}\n        gs = RandomizedSearchCV(estimator = pipe,param_distributions =  param_grid,n_iter=20,n_jobs=2,verbose = 0, random_state=0 )\n        search = gs.fit(X_train, y_train)\n        best_params = search.best_estimator_.named_steps['rfr'].get_params()\n        importances = search.best_estimator_.named_steps['rfr'].feature_importances_\n        importances_df = pd.DataFrame(data = importances, index = X_train.columns, columns = ['Importance'])\n        importances_df.sort_values('Importance',ascending = False, inplace = True)\n        importances_df['Importance_cumulated'] = importances_df['Importance'].cumsum()  \n        predicted = pd.DataFrame(gs.predict(X_test), columns = (y_test.columns+'_predicted'), index = X_test.index)\n        residuals = y_test-predicted.values\n        for col in residuals:\n            residuals.rename(columns ={col: col+'_res'}, inplace = True)\n        return predicted, residuals, importances_df, best_params","d80a9d93":"class ModelEvaluation():\n    \"\"\"\n    This class is used to evaluate the trained model. As input test data, data predicted, shift_period of variables,\n    frequency used to resample data and best params of trained model. As output 'Mean absolute error' and 'Mean squared error' are calculated\n    \"\"\"\n    def __init__(self, X_test, y_test, df_name,frequency, shift_period,df_predicted, best_params):\n        self.X_test = X_test\n        self.y_test = y_test\n        self.df_name = df_name\n        self.frequency = frequency\n        self.shift_period =shift_period\n        self.df_predicted = df_predicted\n        self.best_params = best_params\n    def score_model(self):\n        mse = mean_squared_error(self.y_test, self.df_predicted)\n        mae = mean_absolute_error(self.y_test, self.df_predicted)\n        temp_dict = {'Data': [self.df_name], 'Frequency': [self.frequency], 'Shift period': [self.shift_period],  'Mean squared error': [mse], \n                     'Mean absolute error': [mae],'Best parameters':[self.best_params]}\n        model_eval  = pd.DataFrame(data = temp_dict)\n        return model_eval","d1c2d761":"class PlotResults():\n    \"\"\"\n    This class is used to plot results of the model and plot importances\n    \"\"\"\n    def __init__(self, df_predicted, df_test, df_residual, df_importances):\n        self.df_predicted = df_predicted\n        self.df_test = df_test\n        self.df_residual = df_residual\n        self.df_importances = df_importances\n    def plot_result(self,figure_width=18,figure_height =30):\n        pred_test = list(zip(self.df_predicted.columns, self.df_test.columns, self.df_residual.columns))\n        df_temp = self.df_predicted.merge(self.df_test, left_index = True, right_index = True)\n        fig, axes = plt.subplots(nrows = len(pred_test), ncols = 3, squeeze = False ,figsize = (figure_width,figure_height))\n        for i in range(len(pred_test)):\n            sns.lineplot(x =df_temp.index, y =df_temp[pred_test[i][0]] ,ax = axes[i][0], color = 'red' ,label=pred_test[i][0])\n            axes[i][0].set_title(f\"Predicted vs actual for variable {pred_test[i][0]}\")\n            sns.lineplot(x =df_temp.index, y = df_temp[pred_test[i][1]],ax = axes[i][0], color = 'blue' ,label=pred_test[i][1])\n            sns.regplot(x= pred_test[i][0], y=pred_test[i][1], data=df_temp,ax = axes[i][1] ) \n            axes[i][1].set_title(f\"Regression plot for variable {pred_test[i][1]}\")\n            sns.scatterplot(x=df_temp[pred_test[i][0]] , y=self.df_residual[pred_test[i][2]],ax = axes[i][2])\n            axes[i][2].set_title(f\"Residual plot for variable {pred_test[i][2]}\")\n            plt.tight_layout()\n    def plot_importances(self, n_var_to_display,figure_width=12,figure_height =9):\n        fig,ax = plt.subplots( ncols = 2, figsize =(figure_width,figure_height),squeeze= True)\n        sns.barplot(x = self.df_importances[:n_var_to_display]['Importance'],y=self.df_importances[:n_var_to_display].index,orient = 'h', ax = ax[0])\n        ax[0].set_title(f\"Features importance (first {n_var_to_display} variables)\")\n        sns.barplot(x=self.df_importances.index[:n_var_to_display], y = self.df_importances['Importance_cumulated'][:n_var_to_display],ax = ax[1])\n        ax[1].set_xticklabels(ax[1].get_xticklabels(),rotation=90)\n        ax[1].set_title(f\"Cumulated importance (first {n_var_to_display} variables)\")\n        plt.tight_layout()","2ca19f58":"class Statistics():\n    \"\"\"\n    This class is used to return some statistics about data modeled. Statistics method return a dataframe with\n    some statistics wheras plot_stat plot residual plot and probability plot\n    \"\"\"\n    def __init__(self, df):\n        self.df = df\n    def statistics(self):\n        df_stat = pd.DataFrame()\n        temp_dict = {'Target' : [],'Shapiro':[],'Mean':[],'Variance':[],'Skewness':[],'Kurtosis':[],'Standard_deviation':[],'Standard_error_mean':[]}\n        for col in self.df.columns:\n            temp_dict['Target'].append(col)\n            temp_dict['Shapiro'].append(stats.shapiro(self.df[f'{col}'])[1])\n            temp_dict['Mean'].append(stats.tmean(self.df[f'{col}']))\n            temp_dict['Variance'].append(stats.tvar(self.df[f'{col}']))\n            temp_dict['Skewness'].append(stats.skew(self.df[f'{col}']))\n            temp_dict['Kurtosis'].append(stats.kurtosis(self.df[f'{col}']))\n            temp_dict['Standard_deviation'].append(stats.tstd(self.df[f'{col}']))\n            temp_dict['Standard_error_mean'].append(stats.tsem(self.df[f'{col}']))\n            statistics_df = pd.DataFrame.from_dict(temp_dict)\n        return statistics_df \n    def plot_stat(self,figure_width=20,figure_height =18):\n        fig , ax = plt.subplots(nrows = self.df.shape[1], ncols =3, figsize = (figure_width,figure_height),squeeze = False)\n        df_columns = list(self.df.columns)\n        for i in range(len(df_columns)):\n            stats.probplot(self.df[f'{df_columns[i]}'], dist=\"norm\", plot=ax[i,0])\n            ax[i,0].set_title(f\"Probability plot for {df_columns[i]}\")\n            ax[i,1].hist(self.df[f'{df_columns[i]}'], bins = 50)\n            ax[i,1].set_title(f\"Histogram plot for {df_columns[i]}\")\n            sns.boxplot(x=self.df[f'{df_columns[i]}'], ax = ax[i,2])\n            ax[i,2].set_title(f\"Boxplot for {df_columns[i]}\")\n            plt.tight_layout()","9b654dfb":"resampled_freq = ['W','M','SM']\nshift_period = [1,2,3]\ncombinations = list((itertools.product(shift_period,resampled_freq)))\ndata_input = {'Auser':[Acq_Auser,'Depth'], 'Doganella':[Acq_Doganella,'Depth'],'Luco':[Acq_Luco,'Depth'], 'Petrignano':[Acq_Petrignano,'Depth']}\noutput_model = {}\nmodel_results = pd.DataFrame(columns = ['Data', 'Frequency', 'Shift period',  'Mean squared error', 'Mean absolute error','Best parameters'] )","ab521c9b":"for d in data_input:\n    df = DataPreparation(data_input[d][1]).transform(data_input[d][0])\n    EDA(df,'Depth',df_name = d).plot_target()\n    ","1d02b38b":"for d in data_input:\n    df = DataPreparation(data_input[d][1]).transform(data_input[d][0])\n    for comb in combinations:\n        df_model =  FeatureEngineering(target= data_input[d][1],shift_period = comb[0], resampled_freq = comb[1]).transform(df)\n        X_train, y_train, X_test, y_test = TrainTestSplit(df_model,target =data_input[d][1]).split()\n        predicted_df, residuals_df, importances_df , best_params= Model(X_train, y_train,X_test, y_test).model()\n        output_model[f\"{d}_{comb[1]}_{comb[0]}\"] = [predicted_df, y_test,residuals_df,importances_df, best_params] \n        model_results = model_results.append(ModelEvaluation(X_test, y_test,d,comb[1],comb[0], output_model[f\"{d}_{comb[1]}_{comb[0]}\"][0],best_params).score_model())\n        print(f\"Finished model {d} with shift period {comb[0]} and resampled frequency {comb[1]}\")","eb74a852":"model_results.sort_values(by = ['Data','Mean squared error', 'Mean absolute error'])","844f0ff8":"PlotResults(output_model['Auser_SM_1'][0], output_model['Auser_SM_1'][1],output_model['Auser_SM_1'][2],output_model['Auser_SM_1'][3]).plot_result()","a27e4fb7":"PlotResults(output_model['Auser_SM_1'][0], output_model['Auser_SM_1'][1],output_model['Auser_SM_1'][2],output_model['Auser_SM_1'][3]).plot_importances(9,figure_width=12,figure_height =5)","9fa01f94":"Statistics(output_model['Auser_SM_1'][2]).statistics()","2025535a":"Statistics(output_model['Auser_SM_1'][2]).plot_stat()","7826f5d1":"resampled_freq = ['W','M','SM']\nshift_period = [1,2,3]\ncombinations = list((itertools.product(shift_period,resampled_freq)))\ndata_input = {'Amiata':[Ws_Amiata,'Flow'], 'Lupa':[Ws_Lupa,'Flow'],'Madonna_di_canneto':[Ws_Madonna_di_Canneto,'Flow']}\noutput_model = {}\nmodel_results = pd.DataFrame(columns = ['Data', 'Frequency', 'Shift period',  'Mean squared error', 'Mean absolute error','Best parameters'] )","53570624":"for d in data_input:\n    df = DataPreparation(data_input[d][1]).transform(data_input[d][0])\n    EDA(df,'Flow',df_name = d).plot_target()\n    ","8cff57ee":"for d in data_input:\n    df = DataPreparation(data_input[d][1]).transform(data_input[d][0])\n    for comb in combinations:\n        df_model =  FeatureEngineering(target= data_input[d][1],shift_period = comb[0], resampled_freq = comb[1]).transform(df)\n        X_train, y_train, X_test, y_test = TrainTestSplit(df_model,target =data_input[d][1]).split()\n        predicted_df, residuals_df, importances_df , best_params= Model(X_train, y_train,X_test, y_test).model()\n        output_model[f\"{d}_{comb[1]}_{comb[0]}\"] = [predicted_df, y_test,residuals_df,importances_df, best_params] \n        model_results = model_results.append(ModelEvaluation(X_test, y_test,d,comb[1],comb[0], output_model[f\"{d}_{comb[1]}_{comb[0]}\"][0],best_params).score_model())\n        print(f\"Finished model {d} with shift period {comb[0]} and resampled frequency {comb[1]}\")","dae9d592":"model_results.sort_values(by = ['Data','Mean squared error', 'Mean absolute error'])","4d4e9bbf":"PlotResults(output_model['Amiata_W_1'][0], output_model['Amiata_W_1'][1],output_model['Amiata_W_1'][2],output_model['Amiata_W_1'][3]).plot_result()","ba7169aa":"PlotResults(output_model['Amiata_W_1'][0], output_model['Amiata_W_1'][1],output_model['Amiata_W_1'][2],output_model['Amiata_W_1'][3]).plot_importances(9,figure_width=12,figure_height =7)","4854ed3a":"Statistics(output_model['Amiata_W_1'][2]).statistics()","e44a944d":"Statistics(output_model['Amiata_W_1'][2]).plot_stat()","e30744a7":"resampled_freq = ['W','M','SM']\nshift_period = [1,2,3]\ncombinations = list((itertools.product(shift_period,resampled_freq)))\ndata_input = {'Arno' : [River_Arno, 'Hydro'] }\noutput_model = {}\nmodel_results = pd.DataFrame(columns = ['Data', 'Frequency', 'Shift period',  'Mean squared error', 'Mean absolute error','Best parameters'] )","1da50905":"for d in data_input:\n    df = DataPreparation(data_input[d][1]).transform(data_input[d][0])\n    EDA(df,'Hydro',df_name = d).plot_target()","381d9f42":"for d in data_input:\n    df = DataPreparation(data_input[d][1]).transform(data_input[d][0])\n    for comb in combinations:\n        df_model =  FeatureEngineering(target= data_input[d][1],shift_period = comb[0], resampled_freq = comb[1]).transform(df)\n        X_train, y_train, X_test, y_test = TrainTestSplit(df_model,target =data_input[d][1]).split()\n        predicted_df, residuals_df, importances_df , best_params= Model(X_train, y_train,X_test, y_test).model()\n        output_model[f\"{d}_{comb[1]}_{comb[0]}\"] = [predicted_df, y_test,residuals_df,importances_df, best_params] \n        model_results = model_results.append(ModelEvaluation(X_test, y_test,d,comb[1],comb[0], output_model[f\"{d}_{comb[1]}_{comb[0]}\"][0],best_params).score_model())\n        print(f\"Finished model {d} with shift period {comb[0]} and resampled frequency {comb[1]}\")","ac3a68d6":"model_results.sort_values(by = ['Data','Mean squared error', 'Mean absolute error'])","66c05e1d":"PlotResults(output_model['Arno_M_1'][0], output_model['Arno_M_1'][1],output_model['Arno_M_1'][2],output_model['Arno_M_1'][3]).plot_result(figure_width=12,figure_height =7)","93947624":"PlotResults(output_model['Arno_M_1'][0], output_model['Arno_M_1'][1],output_model['Arno_M_1'][2],output_model['Arno_M_1'][3]).plot_importances(9,figure_width=12,figure_height =6)","b4aad82c":"Statistics(output_model['Arno_M_1'][2]).statistics()","c74dc678":"Statistics(output_model['Arno_M_1'][2]).plot_stat(figure_width=12,figure_height =5)","210eba74":"resampled_freq = ['W','M','SM']\nshift_period = [1,2,3]\ncombinations = list((itertools.product(shift_period,resampled_freq)))\ndata_input = {'Lake_bilancino':[Lake_Bilancino,'Flow']}\noutput_model = {}\nmodel_results = pd.DataFrame(columns = ['Data', 'Frequency', 'Shift period',  'Mean squared error', 'Mean absolute error','Best parameters'] )","f990534a":"for d in data_input:\n    df = DataPreparation(data_input[d][1]).transform(data_input[d][0])\n    EDA(df,'Flow',df_name = d).plot_target()","f48fdb20":"for d in data_input:\n    df = DataPreparation(data_input[d][1]).transform(data_input[d][0])\n    for comb in combinations:\n        df_model =  FeatureEngineering(target= data_input[d][1],shift_period = comb[0], resampled_freq = comb[1]).transform(df)\n        X_train, y_train, X_test, y_test = TrainTestSplit(df_model,target =data_input[d][1]).split()\n        predicted_df, residuals_df, importances_df , best_params= Model(X_train, y_train,X_test, y_test).model()\n        output_model[f\"{d}_{comb[1]}_{comb[0]}\"] = [predicted_df, y_test,residuals_df,importances_df, best_params] \n        model_results = model_results.append(ModelEvaluation(X_test, y_test,d,comb[1],comb[0], output_model[f\"{d}_{comb[1]}_{comb[0]}\"][0],best_params).score_model())\n        print(f\"Finished model {d} with shift period {comb[0]} and resampled frequency {comb[1]}\")","89bb31e0":"model_results.sort_values(by = ['Data','Mean squared error', 'Mean absolute error'])","060d2ccf":"PlotResults(output_model['Lake_bilancino_M_1'][0], output_model['Lake_bilancino_M_1'][1],output_model['Lake_bilancino_M_1'][2],output_model['Lake_bilancino_M_1'][3]).plot_result(figure_width=12,figure_height =5)","b1237019":"PlotResults(output_model['Lake_bilancino_M_1'][0], output_model['Lake_bilancino_M_1'][1],output_model['Lake_bilancino_M_1'][2],output_model['Lake_bilancino_M_1'][3]).plot_importances(9,figure_width=12,figure_height =5)","51dee844":"Statistics(output_model['Lake_bilancino_M_1'][2]).statistics()","a3a1f08e":"Statistics(output_model['Lake_bilancino_M_1'][2]).plot_stat(figure_width=12,figure_height =5)","4aa30765":"resampled_freq = ['W','M','SM']\nshift_period = [1,2,3]\ncombinations = list((itertools.product(shift_period,resampled_freq)))\ndata_input = {'Lake_bilancino':[Lake_Bilancino,'Lake']}\noutput_model = {}\nmodel_results = pd.DataFrame(columns = ['Data', 'Frequency', 'Shift period',  'Mean squared error', 'Mean absolute error','Best parameters'] )","1ec6b7c2":"for d in data_input:\n    df = DataPreparation(data_input[d][1]).transform(data_input[d][0])\n    EDA(df,'Lake',df_name = d).plot_target()","519af353":"for d in data_input:\n    df = DataPreparation(data_input[d][1]).transform(data_input[d][0])\n    for comb in combinations:\n        df_model =  FeatureEngineering(target= data_input[d][1],shift_period = comb[0], resampled_freq = comb[1]).transform(df)\n        X_train, y_train, X_test, y_test = TrainTestSplit(df_model,target =data_input[d][1]).split()\n        predicted_df, residuals_df, importances_df , best_params= Model(X_train, y_train,X_test, y_test).model()\n        output_model[f\"{d}_{comb[1]}_{comb[0]}\"] = [predicted_df, y_test,residuals_df,importances_df, best_params] \n        model_results = model_results.append(ModelEvaluation(X_test, y_test,d,comb[1],comb[0], output_model[f\"{d}_{comb[1]}_{comb[0]}\"][0],best_params).score_model())\n        print(f\"Finished model {d} with shift period {comb[0]} and resampled frequency {comb[1]}\")","0aa4e76c":"model_results.sort_values(by = ['Data','Mean squared error', 'Mean absolute error'])","4d060bde":"PlotResults(output_model['Lake_bilancino_W_1'][0], output_model['Lake_bilancino_W_1'][1],output_model['Lake_bilancino_W_1'][2],output_model['Lake_bilancino_W_1'][3]).plot_result(figure_width=12,figure_height =5)","c1509265":"PlotResults(output_model['Lake_bilancino_W_1'][0], output_model['Lake_bilancino_W_1'][1],output_model['Lake_bilancino_W_1'][2],output_model['Lake_bilancino_W_1'][3]).plot_importances(9,figure_width=12,figure_height =5)","61ecb47a":"Statistics(output_model['Lake_bilancino_W_1'][2]).statistics()","96b309ac":"Statistics(output_model['Lake_bilancino_W_1'][2]).plot_stat(figure_width=12,figure_height =5)","03a17d91":"### Plot results and statistics","1b78296c":"Model results (models are ranked by Mean squared error and Mean absolute error)","5a08f83c":"Rainfall is the most important variables for lake Bilancino","e0aefe94":"For river Arno rainfall is important but there is also a correlation with temperature (in this dataset there is the temperature of Firenze).","e793939a":"Model results (models are ranked by Mean squared error and Mean absolute error)","bec4d59f":"Model results (models are ranked by Mean squared error and Mean absolute error)","1ce63fb6":"****We can plot results for every waterbody and combination of resampled frequency and shift period****. \nLet's do it for Amiata waterbody. Best model has a frequency of W and a shift period of 1. Every output is in ouyput dictionary. So we need to search for key 'Amiata_W_1' and plot statistics about that model","a5c6e33e":"## Lake bilancino level predictions ","346e7c0f":"### EDA Arno","6e42916b":"### Plot results and statistics","5c5e372c":"## Acquifers predictions","a038dd30":"Model results (models are ranked by Mean squared error and Mean absolute error)","991b9637":"### Plot results and statistics","3c245abe":"## **Data**","c1709718":"## **Waterbodies forecasting as a supervised machine learning model: a regression model**","4686316f":"The aim of this notebook is to develop a model to forececast the water level in a waterbody (water spring, lake, river, or aquifer) to handle consumption. The aim is to help Acea, one of the leading Italian multiutility operators, to preserve the health of each managed waterbody in order to manage in the most efficient way water availability. \n","74436a82":"## Arno river prediction","3677d4a9":"## Water Spring predictions","9e4d3d39":"Auser --> best model has a frequency SM and a shift period of 1 with a mse and a mae respectively of 0.205 and 0.324 m\n\nDoganella --> best model has a frequency M and a shift period of 1 with a mse and a mae respectively of 15.60 and 2.77 m\n\nLuco --> best model has a frequency W and a shift period of 1 with a mse and a mae respectively of 1.11 and 0.89 m\n\n\nPetrignano --> best model has a frequency M and a shift period of 3 with a mse and a mae respectively of 5.21 and 2.05","dcba43d0":"Here's a graph representation of the model","7cf7087a":"Lake Bilancino (lake level prediction) --> best model has a frequency W and a shift period of 1 with a mse and a mae respectively of 0.83 and 0.74","e0a731d4":"Amiata --> best model has a frequency W and a shift period of 1 with a mse and a mae respectively of 0.69 and 0.65\n\nLupa --> best model has a frequency W and a shift period of 3 with a mse and a mae respectively of 75.5 and 6.2\n\nMadonna di Canneto --> best model has a frequency W and a shift period of 2 with a mse and a mae respectively of 677 and 18","b4e1bf1d":"Lake Bilancino (Flow rate prediction) --> best model has a frequency M and a shift period of 1 with a mse and a mae respectively of 4.62 and 1.49","42cebbc3":"Arno --> best model has a frequency M and a shift period of 1 with a mse and a mae respectively of 0.10 and 0.23","d48c3efb":"The model is based upon the following 8 classes:\n\n* **DataPreparation**: this class is used to prepare data before features engineering. This class takes the raw data and the target of each waterbody as input and return a new dataframe without null values and with \"Date\" field as index. Raw data has many null values especially in the first occurrences of each dataframe so for each target variable it's calculated what's the index of the last null occurrence. This index it's used to slice the dataframe. As imputer for null values it's used KNNImputer. The reason it's that I want to impute missing values according to the values near the missing ones. As parameter of the imputer it's used \"distance\" as weights since points near the missing ones are more important than point at a greater distance\n* **FeatureEngineering**: this class is used to generate new features, for resampling and for shifting the target variables. This class takes target variable, shift period (with 1 as default) and  resampling frequency (with 'W' week as default) as input and return a dataframe with new features and the target variable shifted. The forecast period and level of aggregation for which make predictions are not pre-defined but are input parameters of the model. As new features are calculated \"Day_rainy\" and \"Day_not_rainy\" respectively as the numbers of rainy and not rainy days over the resampled period. New variables are also created by differencing features variables in the range of shifting period.\n* **EDA**: this class is used for Exploratory Data Analysis of target variables. There is only one method (plot_target) used to plot target variables and correlations between variables after features engineering.\n* **TrainTestSplit**: this class is used to split data in training and testing data. Split ratio between train and test is a model input (default value = 0.8)\n* **Model**: this class implements the predictive model. Taking train and test data as input three different dataframes and a list with best parameters for the model are produced as output. The three dataframes are predictions, residuals and importances_df with the most important variables for the model. As model a RandomForestRegressor is used. Before sending the data to the predictive model a RobustScaler is used as preprocessing. It's used a RobustScaler to deal in a better way the outliers. Everything is encapsulated inside a RandomizedSearchCV which takes the pipe as input and enable the model hyperparameters tuning. For each steps of the modelling best estimator it's taken and for this estimator predictions, residuals and features importances are calculated \n* **ModelEvaluation**: this class is used to scoring the model by calculation mae and mse for each combinations of frequency and shift_period. \n* **PlotResults**: this class is used to plot some evidences about the data predicted. In particular there is a lineplot with data predicted and actual data, a regression plot and a residual plot with the aim to see if residuals have some trends. There is also a method (plot_importances) to plot the most important variables for the model at the level of single variables and cumulated variables.\n* **Statistics**:this class is used to return some statistics about data modeled. As input takes the residual dataframe and some indicators such as Shapiro,Mean,Variance,Skewness,Kurtosis,Standard_deviation and Standard_error_mean are calculated.The goodness of the model is evaluated by looking at the residuals graph. In the method plot_stat() a probabily plot an histogram plot and a boxplot are calculated.Shapiro - Wilk test tests the null hypothesis that the data was drawn from a normal distribution. Alpha is 0.05. If p-value is greater than alpha we fail to reject null hypothesys and residuals are normally distribuited wheras if p value is less than alpha we reject null hypothesis and data are not normally distribuited.Skew indicates the symmetry or asymmetry of a distribuition in particular how much a distribution is push left or right.Kurtosys is an indicator of how much of the distribution is on the tail.The goodness of the model is evaluated by looking at the residuals graph. In the method plot_stat() a probabily plot an histogram plot and a boxplot are calculated.\n\n","c6ab918e":"### EDA Water Spring","e5072911":"![ACEA_model.jpg](attachment:ACEA_model.jpg)","093c455b":"# Acea Smart water analytics: predicting water availability to handle consumption ","4064c4f4":"Features importance","4c533c2f":"There are four different waterbodies for which to predict water availability:\n* Acquifers (Auser, Doganella, Petrignano and Luco): target variable \"Depth to groundwater\"\n* Water spring (Aminata, Madonna di Canneto and Lupa): target variable \"Flow rate\"\n* Lake (Bilancino): target variable \"Hydrometry (Lake level), Flow Rate \n* River (Arno): target variable \"Hydrometry (River level)\"\n\nEach waterbody is represented from a different dataset and differs from the others. Each waterbody is also unique: has different features and different variables can influence water availability over time so we need to predict not only what are the most important variables but also which is the most correct time horizon with which to make predictions.\n\nLet's dive into the model!","5a8361bf":"### Model Acquifers","6d8891f9":"### How model works","e002d7c8":"### EDA Acquifers","93126179":"Model results (models are ranked by Mean squared error and Mean absolute error)","552b349a":"## Conclusions","1dc2a218":"One of the main things about this model is flexibility. There is flexibility about resample frequency and forecast period. For every waterbody a different model is trained for different combinations of resampled frequency and forecast period. As resampled frequency (resampled_freq) it's used 'W' (weekly frequency), 'M'(month end frequency) and 'SM'(semi-month end frequency (15th and end of month)). As shift period values 1,2 and 3 are used. This means that if a model with parameters 'W' and 2 is developed we are trying to forecast at a weekly level with predicted forecast of 2 weeks. If parameters were 'M' and 2 the forecast is montly with a predicted forecast of two months and so on. \n\n\nThe different datasets are passed to the model through a dictionary (data_input). As key it's used the name of the waterbody that we need to predict whereas values is a list containing the dataframe of each waterbody and and the target variable. \n\nWe iterate through dictionary and for each waterboby and DataPreparation class is applied in order to eliminate null values. \n\nThen for each combinations of resampled_frequency and shift_period classes FeatureEngineering, TrainTestSplit, Model are applied.\n\nThe result of the application of our model for each waterbody and combination of resampled frequency and shift period are saved in a new dictionary (output model). The keys of this dictionary are defined as follow: key of input dictionary + resampled frequency  + shift period. Output dictionary enable us to save every model trained for every different combination of frequency and shift period.\n\nA new dataframe (model_results) containing the results of each model is then created. This dataframe is filled rows by rows by appending result of class ModelEvaluation to each model trained. \n\nAfter that we can select the best combination of resampled frequency and forecast period for each waterbody and in model results are saved the best parameters for the model as resulting from hyperparameters tuning by RandomizedSearchCV.\n\nThe best model is plotted with PlotResults class and then the goodness of the model is evaluated with some statistics using Statistics class.\n","35eacea5":"### Plot results and statistics","37c1828e":"### EDA Lake Bilancino level","51659603":"## Lake bilancino flow rate predictions","c617534d":"From the EDA analysis we can see that there is a correlation between Depth to groundwater and rainfall. Rainfall is an important variable in predicting depth to groundwater and we can build new variables based upon rainfall to increase the ability of our model to predict the target variable. Volume variables are others important variables for acquifers.","36e479f6":"****We can plot results for every waterbody and combination of resampled frequency and shift period****. \nLet's do it for Auser waterbody. Best model has a frequency of SM and a shift period of 1. Every output is in ouyput dictionary. So we need to search for key 'Auser_SM_1' and plot statistics about that model","70dc722b":"### Plot results and statistics","7b951e98":"Hydrometry Piaggione and Monte San Quirico are the most important variables. The first 9 variables account for more than 50% of importance.","ecf1b0ce":"### EDA Lake Bilancino Flow Rate","7a4a4320":"****We can plot results for every waterbody and combination of resampled frequency and shift period****. \nLet's do it for Lake Bilancino waterbody. Best model has a frequency of M and a shift period of 1. Every output is in ouyput dictionary. So we need to search for key 'Lake_bilancino_M_1' and plot statistics about that model","a942700e":"****We can plot results for every waterbody and combination of resampled frequency and shift period****. \nLet's do it for Arno waterbody. Best model has a frequency of M and a shift period of 1. Every output is in ouyput dictionary. So we need to search for key 'Arno_M_1' and plot statistics about that model","dc63df14":"The model developed can give useful insights on how to predict water availabylity for a waterbody. It's developed to guarantee maximum flexibility of application to every waterbody.  Given the waterbody you want to predict you can adapt the model to the waterbody by just modify one or more classes. For example you can create new features only by modifying FeatureEngineering class. You can choose a different model rather than RandomForestRegressor only by modifying the model inside the pipe and the parameters inside RandomizedSearchCV. For model evaluation has been developed ModelEvaluation class in which mae and mse are calculated. For model evaluation it's also important to analyze residual plot using Statistics class. Data visualization is used for plotting importance variables (cumulated and not), for EDA in which is represented correlation between target variable and others features and for model evaluation and statistics. You can also see the results of every trained model using output_model dictionary. Model are ranked in model results dataframe which rank model by waterbody, mae and mse. An important point to note is that the resampled frequency and forecast period are input of the model. The best combination can be defined using a parameter like mae or mse. Every waterbody has an optimal combination of resampled frequency and forecast period and that are determined by the model. In conclusion the model developed  is easily extendable to other waterbodies just by modify one or more classes and can give useful insights on what are the features that most effect water availability. \n\n"}}