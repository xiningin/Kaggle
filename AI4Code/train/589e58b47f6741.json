{"cell_type":{"ebbf2fa6":"code","5ff74b36":"code","9997c8c1":"code","c35a4cbb":"code","fd8b8c44":"code","4c68f979":"code","43b0a1f2":"code","0002b2f4":"code","aae748e6":"code","f2f6bebb":"code","8fa96f70":"code","087c7a87":"code","04736d83":"code","5c3b67c4":"code","9c2f3880":"code","69cb42de":"code","1d4e476b":"code","036b025c":"code","78fdb0fc":"code","91955239":"code","205bb5e2":"code","6d27e014":"code","99d073e6":"code","2449af7f":"code","6b81ad61":"code","7328e4de":"code","d9de6273":"code","83304a00":"code","80470099":"code","85263464":"code","0b34456a":"code","dd23ab6c":"code","83577901":"code","07464e5d":"code","ca761f00":"code","07448e76":"code","bb954b49":"code","37f54d99":"code","ed5b2734":"code","6d9706ac":"code","4ecdcd04":"code","3c490a1b":"code","a88ca6b4":"code","9a3f4ca8":"code","48a9d4de":"code","21e1a49d":"code","873ac33d":"code","3fb55819":"code","a93bffbd":"code","6b231b7a":"code","21b6c56d":"code","5620a35d":"code","b4311ee4":"code","72ac3d13":"code","fa671024":"code","d1ff0fdf":"code","e7240995":"code","c4d0a7b8":"code","ad444ca0":"code","7a521352":"code","7263f8c4":"code","175f68a4":"code","ad4b020a":"code","5d8955b0":"code","b056d3ca":"code","b33d0a34":"code","070368e8":"code","ca1db6ea":"code","31ebc16a":"markdown","9bc0cb4f":"markdown","e6723820":"markdown","82e992bc":"markdown","94b6cd98":"markdown","af7933bf":"markdown","53f35ce2":"markdown","5c640c80":"markdown","d85c8fde":"markdown","2ca782d9":"markdown","1ac819a8":"markdown","df707458":"markdown","ece4a130":"markdown","25d5908a":"markdown","19ee6717":"markdown","d20089b6":"markdown","cec12ec9":"markdown","d323ea29":"markdown","c1a683e8":"markdown","a901d93b":"markdown","12cdb60a":"markdown","41002b2e":"markdown","cc1c88f4":"markdown","4f59945b":"markdown","c322f8e7":"markdown","a28208fc":"markdown","e0d1d63f":"markdown","f04c070b":"markdown","a8627d7c":"markdown","ab83a7dd":"markdown","89d11194":"markdown","fe9b431b":"markdown","6de3d161":"markdown","7b32f072":"markdown","fc7c94d9":"markdown","7cef4ce9":"markdown","599f9baf":"markdown","91bcf321":"markdown","64fdf4dc":"markdown","e8579783":"markdown","ade6f883":"markdown","5f5593b7":"markdown","0759eb19":"markdown","59236e3d":"markdown","e485ffb6":"markdown","ee091c9b":"markdown","73271bd2":"markdown","6610a5b3":"markdown","c88dda4f":"markdown","1929f90d":"markdown","3b519302":"markdown","cd9b7433":"markdown"},"source":{"ebbf2fa6":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport re\nimport string\nfrom wordcloud import WordCloud, STOPWORDS \nimport spacy\nfrom tqdm import tqdm\nimport os\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5ff74b36":"train_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\ntrain_df.head()","9997c8c1":"print(train_df.shape)\nprint(test_df.shape)","c35a4cbb":"train_df.info()","fd8b8c44":"print(train_df[train_df['text'].isnull()])\nprint(train_df[train_df['selected_text'].isnull()])","4c68f979":"# drop the row with missing data\ntrain_df.dropna(inplace = True)\n# reset index and drop the index column made by reset_index()\ntrain_df = train_df.reset_index(drop = True)","43b0a1f2":"train_df.info()","0002b2f4":"train_df.describe()","aae748e6":"# Calculating no of words in each tweet in 'text' column\ntrain_df['N_text_words'] = train_df['text'].apply(lambda tweet : len(tweet.split()))\n\n# Calculating no of words in each tweet in 'selected_text' column\ntrain_df['N_selected_text_words'] = train_df['selected_text'].apply(lambda tweet : len(tweet.split()))\n\n# Calculating difference in no. of words in text and selected_text\ntrain_df['N_words_difference'] = train_df['N_text_words'] - train_df['N_selected_text_words']","f2f6bebb":"train_df.head()","8fa96f70":"x1 = train_df['N_text_words']\nx2 = train_df['N_selected_text_words']\n\ngroup_labels = ['N_text_words', 'N_selected_text_words']\n\ncolors = ['blue', 'orange']\n\n# Create distplot with curve_type set to 'normal'\nfig = ff.create_distplot([x1, x2], group_labels, bin_size=1,colors=colors,show_curve = True)\n\n# Add title\nfig.update_layout(title_text='Distplot of N_text_words and N_selected_text_words')\nfig.show()","087c7a87":"x1 = train_df['N_words_difference']\n\ngroup_labels = ['N_words_difference']\n\n# Create distplot with curve_type set to 'normal'\nfig = ff.create_distplot([x1], group_labels, bin_size=1,show_curve = True)\n\n# Add title\nfig.update_layout(title_text='Distplot of N_words_difference')\nfig.show()","04736d83":"# No of unique sentiments and there values\nprint(\"There are {0} unique sentiments having values {1}\".format(train_df['sentiment'].nunique(), train_df['sentiment'].unique()))","5c3b67c4":"# Counting number of neutral sentiments \nn_neutral = train_df['sentiment'].loc[train_df['sentiment'] == 'neutral'].count()\n# Counting number of positive sentiments \nn_positive = train_df['sentiment'].loc[train_df['sentiment'] == 'positive'].count()\n# Counting number of negative sentiments \nn_negative = train_df['sentiment'].loc[train_df['sentiment'] == 'negative'].count()","9c2f3880":"print(f\"Neutral tweets : {n_neutral}\")\nprint(f\"Positive tweets : {n_positive}\")\nprint(f\"Negative tweets : {n_negative}\")","69cb42de":"plt.figure(figsize=(12,6))\nplt.xlabel('Sentiments',fontsize = 20)\nplt.ylabel('Count',fontsize = 20)\nsns.countplot(x='sentiment',data=train_df)","1d4e476b":"sentiments = ['Neutral', 'Positive', 'Negative']\nfig = go.Figure(data = [go.Pie(labels = sentiments, values=[n_neutral, n_positive, n_negative])])\nfig.show()","036b025c":"# breaking the dataframe sentimentwise\n\n# dataframe with all neutral sentiments\ndf_neutral = train_df.loc[train_df['sentiment'] == 'neutral']\n# dataframe with all positive sentiments\ndf_positive = train_df.loc[train_df['sentiment'] == 'positive']\n# dataframe with all negative sentiments\ndf_negative = train_df.loc[train_df['sentiment'] == 'negative']","78fdb0fc":"fig = make_subplots(rows=1, cols=3, subplot_titles = (\"Neutral\", \"Positive\", \"Negative\"))\n\nfig.add_trace(\n    go.Histogram(x=df_neutral[\"N_text_words\"]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df_positive[\"N_text_words\"]),\n    row = 1, col = 2\n)\n\nfig.add_trace(\n    go.Histogram(x=df_negative[\"N_text_words\"]),\n    row = 1, col = 3\n)\n\nfig.update_layout(title_text=\"Distribution of No. of words in 'text'\", showlegend = False)\n\nfig.show()","91955239":"fig = make_subplots(rows=1, cols=3, subplot_titles = (\"Neutral\", \"Positive\", \"Negative\"))\n\nfig.add_trace(\n    go.Histogram(x=df_neutral[\"N_selected_text_words\"]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df_positive[\"N_selected_text_words\"]),\n    row = 1, col = 2\n)\n\nfig.add_trace(\n    go.Histogram(x=df_negative[\"N_selected_text_words\"]),\n    row = 1, col = 3\n)\n\nfig.update_layout(title_text=\"Distribution of No. of words in 'selected_text'\", showlegend = False)\n\nfig.show()","205bb5e2":"fig = make_subplots(rows=1, cols=3, subplot_titles = (\"Neutral\", \"Positive\", \"Negative\"))\n\nfig.add_trace(\n    go.Histogram(x=df_neutral[\"N_words_difference\"]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df_positive[\"N_words_difference\"]),\n    row = 1, col = 2\n)\n\nfig.add_trace(\n    go.Histogram(x=df_negative[\"N_words_difference\"]),\n    row = 1, col = 3\n)\n\nfig.update_layout(title_text=\"Distribution of difference of words 'N_words_difference'\", showlegend = False)\n\nfig.show()","6d27e014":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","99d073e6":"jaccard_score = []\nfor i in range(train_df.shape[0]):\n    str1 = train_df['text'][i].strip()\n    str2 = train_df['selected_text'][i].strip()\n    jaccard_score.append(jaccard(str1,str2))","2449af7f":"# Adding Jaccard score as a column to our training dataframe\ntrain_df['Jaccard_score'] = jaccard_score","6b81ad61":"train_df.head()","7328e4de":"plt.figure(figsize = (16,6))\nsns.kdeplot(train_df['Jaccard_score'], shade=True, color=\"r\")\nplt.title('Distribution of Jaccard score across the data set')","d9de6273":"plt.figure(figsize=(12,6))\nplt1 = sns.kdeplot(train_df[train_df['sentiment'] == 'positive']['Jaccard_score'], shade=True, color=\"b\").set_title('KDE of Jaccard Scores across different Sentiments')\nplt2 = sns.kdeplot(train_df[train_df['sentiment'] == 'negative']['Jaccard_score'], shade=True, color=\"r\")\nplt.legend(labels=['positive','negative'])","83304a00":"df_neutral = train_df[train_df['sentiment'] == 'neutral']\nfig = px.histogram(df_neutral, x=\"Jaccard_score\", nbins=30)\nfig.show()","80470099":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","85263464":"train_df['text_cleaned'] = train_df['text'].apply(lambda x:clean_text(x))\ntrain_df['selected_text_cleaned'] = train_df['selected_text'].apply(lambda x:clean_text(x))","0b34456a":"STOPWORDS = stopwords.words('english')\ndef remove_stopwords(text):\n    return [word for word in text.split() if word not in STOPWORDS]","dd23ab6c":"train_df['text_cleaned'] = train_df['text_cleaned'].apply(lambda x : remove_stopwords(x))\ntrain_df['selected_text_cleaned'] = train_df['selected_text_cleaned'].apply(lambda x : remove_stopwords(x))","83577901":"train_df.head()","07464e5d":"def get_all_words(df_col):\n    all_words_text = []\n    for row in df_col:\n        for word in row:\n            all_words_text.append(word)\n    return all_words_text\n\nall_words_text = get_all_words(train_df['text_cleaned'])\nall_words_selected_text = get_all_words(train_df['selected_text_cleaned'])","ca761f00":"def most_common_words(all_words):\n    return nltk.FreqDist(all_words)","07448e76":"# for 'text' column\nwords = []\nfrequency = []\nfreqDist = most_common_words(all_words_text)\nfor word, freq in freqDist.most_common(20):\n        words.append(word)\n        frequency.append(freq)\n\nfor i in range(20):\n    print(f'{words[i]} : {frequency[i]}')\n\nfig = px.bar(x=frequency, y=words, title='Top 20 Most Commmon Words in Text', orientation='h', \n             width=700, height=700,color=words)\nfig.show()","bb954b49":"# for 'selected_text' column\nwords = []\nfrequency = []\nfreqDist = most_common_words(all_words_selected_text)\nfor word, freq in freqDist.most_common(20):\n        words.append(word)\n        frequency.append(freq)\n\nfor i in range(20):\n    print(f'{words[i]} : {frequency[i]}')\n\nfig = px.bar(x=frequency, y=words, title='Top 20 Most Commmon Words in Selected Text', orientation='h', \n             width=700, height=700,color=words)\nfig.show()","37f54d99":"all_words_neutral = get_all_words(train_df[train_df['sentiment'] == 'neutral']['text_cleaned'])\nall_words_positive = get_all_words(train_df[train_df['sentiment'] == 'positive']['text_cleaned'])\nall_words_negative = get_all_words(train_df[train_df['sentiment'] == 'negative']['text_cleaned'])","ed5b2734":"# for 'neutral' sentiment\nwords = []\nfrequency = []\nfreqDist = most_common_words(all_words_neutral)\nfor word, freq in freqDist.most_common(20):\n        words.append(word)\n        frequency.append(freq)\n\nfor i in range(20):\n    print(f'{words[i]} : {frequency[i]}')\n\nfig = px.bar(x=frequency, y=words, title='Top 20 Most Commmon Words with Neutral Sentiment', orientation='h', \n             width=700, height=700,color=words)\nfig.show()","6d9706ac":"# for 'positive' sentiment\nwords = []\nfrequency = []\nfreqDist = most_common_words(all_words_positive)\nfor word, freq in freqDist.most_common(20):\n        words.append(word)\n        frequency.append(freq)\n\nfor i in range(20):\n    print(f'{words[i]} : {frequency[i]}')\n\nfig = px.bar(x=frequency, y=words, title='Top 20 Most Commmon Words with positive Sentiment', orientation='h', \n             width=700, height=700,color=words)\nfig.show()","4ecdcd04":"# for 'negative' sentiment\nwords = []\nfrequency = []\nfreqDist = most_common_words(all_words_negative)\nfor word, freq in freqDist.most_common(20):\n        words.append(word)\n        frequency.append(freq)\n\nfor i in range(20):\n    print(f'{words[i]} : {frequency[i]}')\n\nfig = px.bar(x=frequency, y=words, title='Top 20 Most Commmon Words with Negative Sentiment', orientation='h', \n             width=700, height=700,color=words)\nfig.show()","3c490a1b":"def plot_wordcloud(all_words):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'u', \"im\"}\n    stopwords = stopwords.union(more_stopwords)\n    all_words = \" \".join(all_words)\n    wordcloud = WordCloud(width = 400, height = 200, \n                background_color ='white',\n                max_words = 200,\n                stopwords = stopwords,\n                min_font_size = 10)\n    wordcloud = wordcloud.generate(all_words)\n    \n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.show() ","a88ca6b4":"plot_wordcloud(all_words_positive)","9a3f4ca8":"plot_wordcloud(all_words_negative)","48a9d4de":"plot_wordcloud(all_words_neutral)","21e1a49d":"# df_train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\n# df_test = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\n# df_submission = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","873ac33d":"# df_train['Num_words_text'] = df_train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main Text in train set","3fb55819":"# df_train = df_train[df_train['Num_words_text']>=3]","a93bffbd":"# def save_model(output_dir, nlp, new_model_name):\n#     ''' This Function Saves model to \n#     given output directory'''\n    \n#     output_dir = f'..\/working\/{output_dir}'\n#     if output_dir is not None:        \n#         if not os.path.exists(output_dir):\n#             os.makedirs(output_dir)\n#         nlp.meta[\"name\"] = new_model_name\n#         nlp.to_disk(output_dir)\n#         print(\"Saved model to\", output_dir)","6b231b7a":"# # pass model = nlp if you want to train on top of existing model \n\n# def train(train_data, output_dir, n_iter=20, model=None):\n#     \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n#     \"\"\n#     if model is not None:\n#         nlp = spacy.load(output_dir)  # load existing spaCy model\n#         print(\"Loaded model '%s'\" % model)\n#     else:\n#         nlp = spacy.blank(\"en\")  # create blank Language class\n#         print(\"Created blank 'en' model\")\n    \n#     # create the built-in pipeline components and add them to the pipeline\n#     # nlp.create_pipe works for built-ins that are registered with spaCy\n#     if \"ner\" not in nlp.pipe_names:\n#         ner = nlp.create_pipe(\"ner\")\n#         nlp.add_pipe(ner, last=True)\n#     # otherwise, get it so we can add labels\n#     else:\n#         ner = nlp.get_pipe(\"ner\")\n    \n#     # add labels\n#     for _, annotations in train_data:\n#         for ent in annotations.get(\"entities\"):\n#             ner.add_label(ent[2])\n\n#     # get names of other pipes to disable them during training\n#     other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n#     with nlp.disable_pipes(*other_pipes):  # only train NER\n#         # sizes = compounding(1.0, 4.0, 1.001)\n#         # batch up the examples using spaCy's minibatch\n#         if model is None:\n#             nlp.begin_training()\n#         else:\n#             nlp.resume_training()\n\n\n#         for itn in tqdm(range(n_iter)):\n#             random.shuffle(train_data)\n#             batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n#             losses = {}\n#             for batch in batches:\n#                 texts, annotations = zip(*batch)\n#                 nlp.update(texts,  # batch of texts\n#                             annotations,  # batch of annotations\n#                             drop=0.5,   # dropout - make it harder to memorise data\n#                             losses=losses, \n#                             )\n#             print(\"Losses\", losses)\n#     save_model(output_dir, nlp, 'st_ner')","21b6c56d":"# def get_model_out_path(sentiment):\n#     '''\n#     Returns Model output path\n#     '''\n#     model_out_path = None\n#     if sentiment == 'positive':\n#         model_out_path = 'models\/model_pos'\n#     elif sentiment == 'negative':\n#         model_out_path = 'models\/model_neg'\n#     return model_out_path","5620a35d":"# def get_training_data(sentiment):\n#     '''\n#     Returns Trainong data in the format needed to train spacy NER\n#     '''\n#     train_data = []\n#     for index, row in df_train.iterrows():\n#         if row.sentiment == sentiment:\n#             selected_text = row.selected_text\n#             text = row.text\n#             start = text.find(selected_text)\n#             end = start + len(selected_text)\n#             train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n#     return train_data","b4311ee4":"# sentiment = 'positive'\n\n# train_data = get_training_data(sentiment)\n# model_path = get_model_out_path(sentiment)\n# train(train_data, model_path, n_iter=7, model=None)","72ac3d13":"# sentiment = 'negative'\n\n# train_data = get_training_data(sentiment)\n# model_path = get_model_out_path(sentiment)\n\n# train(train_data, model_path, n_iter=7, model=None)","fa671024":"# sentiment = 'neutral'\n\n# train_data = get_training_data(sentiment)\n# model_path = get_model_out_path(sentiment)\n\n# train(train_data, model_path, n_iter=7, model=None)","d1ff0fdf":"# def predict_entities(text, model):\n#     doc = model(text)\n#     ent_array = []\n#     for ent in doc.ents:\n#         start = text.find(ent.text)\n#         end = start + len(ent.text)\n#         new_int = [start, end, ent.label_]\n#         if new_int not in ent_array:\n#             ent_array.append([start, end, ent.label_])\n#     selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n#     return selected_text","e7240995":"# selected_texts = []\n# MODELS_BASE_PATH = '..\/input\/tse-spacy-model\/models\/'\n\n# if MODELS_BASE_PATH is not None:\n#     print(\"Loading Models  from \", MODELS_BASE_PATH)\n#     model_pos = spacy.load(MODELS_BASE_PATH + 'model_pos')\n#     model_neg = spacy.load(MODELS_BASE_PATH + 'model_neg')\n        \n#     for index, row in df_test.iterrows():\n#         text = row.text\n#         output_str = \"\"\n#         if row.sentiment == 'neutral' or len(text.split()) <= 2:\n#             selected_texts.append(text)\n#         elif row.sentiment == 'positive':\n#             selected_texts.append(predict_entities(text, model_pos))\n#         else:\n#             selected_texts.append(predict_entities(text, model_neg))\n        \n# df_test['selected_text'] = selected_texts","c4d0a7b8":"# df_submission['selected_text'] = df_test['selected_text']\n# df_submission.to_csv(\"submission.csv\", index=False)\n# display(df_submission.head(10))","ad444ca0":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","7a521352":"MAX_LEN = 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\ntrain.head()","7263f8c4":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","175f68a4":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","ad4b020a":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model","5d8955b0":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","b056d3ca":"'''\njac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n        \n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n        \n#     model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n#         epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n#         validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n#         [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n    print('Loading model...')\n    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/skf.n_splits\n    preds_end += preds[1]\/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()\n'''","b33d0a34":"preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nDISPLAY=1\nn_splits = 5\nfor i in range(5):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    model.load_weights('..\/input\/tf-roberta-weights\/v0-roberta-%i.h5'%i)\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/n_splits\n    preds_end += preds[1]\/n_splits","070368e8":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","ca1db6ea":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","31ebc16a":"### Distribution of No. of words in text 'N_text_words' - sentimentwise","9bc0cb4f":"# Acknowledgements\n1. https:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-eda-and-model\n2. https:\/\/www.kaggle.com\/shahules\/complete-eda-baseline-model-0-708-lb\n3. https:\/\/www.kaggle.com\/rohitsingh9990\/ner-training-using-spacy-ensemble\n4. https:\/\/www.kaggle.com\/parulpandey\/eda-and-preprocessing-for-bert","e6723820":"Both missing values are from the same row. So, we will just simply drop it.","82e992bc":"## Test data","94b6cd98":"Since we have got our list of all words from both the columns, we can now find the most common words in both these columns and see some beautiful visualizations. ","af7933bf":"**I was not able to plot KDE plot for Neutral sentiments because as soon as I included it , I got an error - 'RuntimeError: Selected KDE bandwidth is 0. Cannot estimate density.' - This is because of the fact that the difference in number of words for 'text' and 'selected_text' i.e. 'N_words_difference' is mostly 0 for neutral sentiments.\n**\n\n**Do not worry, we will draw a distplot to visualize the distribution of Jaccard score for Neutral sentiments**","53f35ce2":"# RoBERTa model - TensorFlow","5c640c80":"## Training data","d85c8fde":"### Distribution of Jaccard Score for positive and negative sentiments","2ca782d9":"Jaccard similarity or intersection over union is defined as size of intersection divided by size of union of two sets.The Jaccard Index, also known as the Jaccard similarity coefficient, is a statistic used in understanding the similarities between sample sets. The measurement emphasizes similarity between finite sample sets, and is formally defined as the size of the intersection divided by the size of the union of the sample sets. The mathematical representation of the index is written as:\n\n![image.png](attachment:image.png)\n\nSimilar to the Jaccard Index, which is a measurement of similarity, the Jaccard distance measures dissimilarity between sample sets. The Jaccard distance is calculated by finding the Jaccard index and subtracting it from 1, or alternatively dividing the differences ny the intersection of the two sets. The formula for the Jaccard distance is represented as:\n\n![image.png](attachment:image.png)\n\nHow does the Jaccard Index work?\nBreaking down the formula, the Jaccard Index is essentially the number in both sets, divided by the number in either set, multiplied by 100. This will produce a percentage measurement of similarity between the two sample sets. Accordingly, to find the Jaccard distance, simply subtract the percentage value from 1. For example, if the similarity measurement is 35%, then the Jaccard distance (1 - .35) is .65 or 65%.","1ac819a8":"# Preprocessing\n\nBefore we start with any NLP project we need to pre-process the data .We need to clean our data thouroughly, remove all stopwords and convert our data to lowercase. Let's create a helper function which will perform the following preprocessing tasks for us :\n* Remove punctuations\n* Remove hyperlinks\n* Remove words in square brackets\n* Convert text to lowercase\n* Removing stopwords","df707458":"`Suprisingly, there are 17 'selected_text' values that are non-unique !\n\nAlso, we have 'good' as the most common 'selected_text' (199 times) and most of the tweet sentiments are 'neutral' (11117 tweets)","ece4a130":"### Creating the file for submission","25d5908a":"## Train the model","19ee6717":"## Build the model","d20089b6":"### Training models for Positive, Negative and Neutral tweets\u00b6\n","cec12ec9":"###  'text' and 'selected_text' both have 1 missing value. Let's have a look...\n","d323ea29":"### 1. Most common words for 'text' and 'selected_text'","c1a683e8":"## Prediction","a901d93b":"# End Note\n\nHi everyone. This is my first ever kernel on EDA. There are a lot of things which I learnt while writing this kernel and there are still a lot of concepts left untouched. Kaggle provides us with the opportunity to learn and grow through such long term competitions and a wide variety of datasets. I promise to keep putting kernels for everyone to learn and to learn myself and to comeback each time with more and more knowledge, concepts and implementations. A major part of help was taken from Tanul Singh's kernel while writing this kernel.\n\nIf you have any doubts, feedbacks or concerns, feel free to comment down below and I will try to get back as soon as possible. Really sorry if you find any mistakes in this kernel, it would be great if you point them out... I will try to resolve them ASAP.\n\n#### <font color = 'red'>If you find this kernel useful, please consider **upvoting it** \ud83d\ude0a. It will motivate me for doing more hard work and to produce more quality content for you guys.<\/font>\n\n\nMANY THANKS TO ALL :)","12cdb60a":"# Text Based Analysis\n\nSince now we have a cleaner version of our data, it will be interesting to see some text based analysis and their visualizations like:\n\n1. Most common words for text and selected_text.\n2. Most common words - sentimentwise.\n3. Word Clouds.\n\nLet's start off with the first one.","41002b2e":"### Let's explore the 'text' and 'selected_text' columns and try to find a relationship (if any)","cc1c88f4":"## Metric","4f59945b":"## **Bringing 'sentiment' into the picture...[](http:\/\/)**","c322f8e7":"To get the most common words, we first need to prepare a list of all words present in the 'text' and 'selected_text' column using the below helper function.","a28208fc":"# Evaluation Metric","e0d1d63f":"# Importing data into our kernel","f04c070b":"### 3. Word Clouds\n\nIt's time now to look at some Word Clouds. We will visualize them in the following order : \n1. WordCloud for positive sentiments.\n2. WordCloud for negative sentiments.\n3. WordCloud for neutral sentiments.\n\nWordCloud will give us more clarity ont the most common words because the most common words will be shown with biggest fonts. Higher the frequency of a word, bigger will be it's font in the WordCloud. Also, WordClouds are visually more appealing.","a8627d7c":"### Now it will be more interesting to see the distribution of sentiment with -\n1. No. of words in text\n2. No. of words in selected_text\n3. Difference in No. of words in text and selected_text","ab83a7dd":"#### WORDCLOUD FOR NEGATIVE TWEETS","89d11194":"####  WORDCLOUD FOR NEUTRAL TWEETS","fe9b431b":"**NOTE : From here onwards, we will be bringing each column into consideration one by one for better understanding and Analysis of the data starting from 'text' and 'selected_text'**","6de3d161":"### Distribution of Jaccard Score for Neutral sentiments","7b32f072":"## Submission","fc7c94d9":"### Lets see how Jaccard Score is distributed across the dataset","7cef4ce9":"Let's have a graphical visualization...","599f9baf":"# About the Competition\n\n![image.png](attachment:image.png)\n\n\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.\n\nHelp build your skills in this important area with this broad dataset of tweets. Work on your technique to grab a top spot in this competition. What words in tweets support a positive, negative, or neutral sentiment? How can you help make that determination using machine learning tools?\n\n### <font color = 'red'>If you find this kernel useful please consider **upvoting it** \ud83d\ude0a which keeps me motivated for doing hard work and to produce more quality content.<\/font>","91bcf321":"# Importing the relevant libraries and getting to know the files","64fdf4dc":"### What do the above 3 plots tell us?\n\n**In the first distribution plot :**\nAll three graphs are right skewed and have most of data lying in the following range:\n* Neutral sentiment tweets  - (3,12)\n* Positive sentiment tweets - (4,16)\n* Negative sebtiment tweets - (4,16)\n   \n**In the second distribution plot:**\nAll three graphs are right skewed but,\n* Neutral graph has a high kurtosis (Leptokurtic)\n* Positve and Negative graph have a peak when No. of words in selected_text = 1 and then there is a drastic decline.\n\n**In the thrid distribution plot:**\nAll three graphs are right skewed and,\n* Almost all **i.e 10,278\/11117** selected_text are same as the text when the sentiment is Neutral.\n* Positve and Negative graph have a peak when difference in No.of words = 0 following which we can see a right skewed distribution.","e8579783":"#### WORDCLOUD FOR POSITIVE TWEETS","ade6f883":"# Modelling - Using NER\n\n**What is NER?**\n\nNamed-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n\nHere for modelling purpose, I have used NER model using SpaCy. The solution for this type of model is provided by Rohit Singh. So here, I am going to use the same model but with little tweaks.\n\nThanks to Rohit Singh for the amazing kernel and the solution he has provided through his kernel. Do check out his kernel using the below link and upvote it if you find it useful.\n\nRohit Singh's kernel on NER model training - https:\/\/www.kaggle.com\/rohitsingh9990\/ner-training-using-spacy-ensemble\n\nFor understanding NER here is very good article : https:\/\/towardsdatascience.com\/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da","5f5593b7":"The distributions here are very interesting and insightful :\n\nIn the first distribution:\n* Firstly, ~ 25% of selected_text comprise only of one word which means in 25% of the tweets, only one word is responsible for determing the sentiment of the tweet\n* Sencondly, the distribution is right skewed meaning number of words in most of tweets are in range 0-25\n\nIn the second distribution: \n* ~45% of the differences are 0 implying that nearly 45% of selected_text are same length as that of text and that these 45% text\/selected_text are responsible for the sentiment wholely.\n* The distribution is rigth skewed","0759eb19":"### The whole purpose of this competition is to find word(s) responsible for the sentiment of tweets using the word-level Jaccard Coefficient\/Score as the evaluation metric\n\n**Kaggle and competiton oraganizers were kind enough to provide us with code for computing Jaccard score. So, lets bring Jaccard score into play... **\n","59236e3d":"Lets look at the distribution of 'N_text_words' and 'N_selected_text_words' and then look the distribution of 'N_words_difference' afterwards.","e485ffb6":"## Load data and tokenizers","ee091c9b":"### Descriptive Statistics","73271bd2":"### Distribution of No. of words in selected_text 'N_selected_text_words' - sentimentwise","6610a5b3":"### Predicting with the trained model","c88dda4f":"### Distribution of difference of words 'N_words_difference' - sentimentwise","1929f90d":"### Distribution of Jaccard Score - Sentimentwise","3b519302":"# EDA\n\nBefore we jump into the EDA part, let's check if there is any missing data because it may cause a problem to us later on. \n\n### Check for missing values in columns","cd9b7433":"### 2. Most common words - sentimentwise"}}