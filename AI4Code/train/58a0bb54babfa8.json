{"cell_type":{"3bbdb331":"code","4c0a7d97":"code","94ab9956":"code","60e79b40":"code","a557ea0d":"code","b2353a4d":"code","1d0a4b8e":"code","09a9240f":"code","8105d97a":"code","80da9941":"code","d9035e94":"code","2a4fc384":"code","a073ff4f":"code","963a61e1":"code","01161f1d":"code","4c6d7c02":"code","690fa484":"code","7b4c1195":"code","c75ec354":"code","9c30c9a4":"code","05610dca":"code","58476f96":"code","f7fd9998":"code","2ea175be":"code","b5da0fce":"code","da9f6ba2":"code","a7f85ac8":"code","398effa5":"markdown","f0d9a350":"markdown","a18bc3b3":"markdown","627d55cc":"markdown","96763e9f":"markdown","74bdf58e":"markdown","a36a54eb":"markdown","a614ecca":"markdown"},"source":{"3bbdb331":"# import the required libraries\nimport glob\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport cv2\n\nkey_pts_frame = pd.read_csv('..\/input\/facialkeypoint\/training_frames_keypoints.csv')\n\nn = 0\nimage_name = key_pts_frame.iloc[n, 0]\nkey_pts = key_pts_frame.iloc[n, 1:].to_numpy()\nkey_pts = key_pts.astype('float').reshape(-1, 2)\n\nprint('Image name: ', image_name)\nprint('Landmarks shape: ', key_pts.shape)\nprint('First 4 key pts: {}'.format(key_pts[:4]))\n\n\n# print out some stats about the data\nprint('Number of images: ', key_pts_frame.shape[0])","4c0a7d97":"def show_keypoints(image, key_pts):\n    \"\"\"Show image with keypoints\"\"\"\n    plt.imshow(image)\n    plt.scatter(key_pts[:, 0], key_pts[:, 1], s=20, marker='.', c='m')\n    \n# Display a few different types of images by changing the index n\n\n# select an image by index in our data frame\nn = 0\nimage_name = key_pts_frame.iloc[n, 0]\nkey_pts = key_pts_frame.iloc[n, 1:].to_numpy()\nkey_pts = key_pts.astype('float').reshape(-1, 2)\n\nplt.figure(figsize=(5, 5))\nimage = mpimg.imread(os.path.join('..\/input\/facialkeypoint\/training\/', image_name))\nshow_keypoints(image, key_pts)\nplt.show()","94ab9956":"from torch.utils.data import Dataset, DataLoader\n\nclass FacialKeypointsDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.key_pts_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.key_pts_frame)\n\n    def __getitem__(self, idx):\n        image_name = os.path.join(self.root_dir,\n                                self.key_pts_frame.iloc[idx, 0])\n        \n        image = mpimg.imread(image_name)\n        \n        # if image has an alpha color channel, get rid of it\n        if(image.shape[2] == 4):\n            image = image[:,:,0:3]\n        \n        key_pts = self.key_pts_frame.iloc[idx, 1:].to_numpy()\n        key_pts = key_pts.astype('float').reshape(-1, 2)\n        sample = {'image': image, 'keypoints': key_pts}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample","60e79b40":"\n# Construct the dataset\nface_dataset = FacialKeypointsDataset(csv_file='..\/input\/facialkeypoint\/training_frames_keypoints.csv',\n                                      root_dir='..\/input\/facialkeypoint\/training\/')\n\n# print some stats about the dataset\nprint('Length of dataset: ', len(face_dataset))\n\n# Display a few of the images from the dataset\nnum_to_display = 3\n\nfor i in range(num_to_display):\n    \n    # define the size of images\n    fig = plt.figure(figsize=(20,10))\n    \n    # randomly select a sample\n    rand_i = np.random.randint(0, len(face_dataset))\n    sample = face_dataset[rand_i]\n\n    # print the shape of the image and keypoints\n    print(i, sample['image'].shape, sample['keypoints'].shape)\n\n    ax = plt.subplot(1, num_to_display, i + 1)\n    ax.set_title('Sample #{}'.format(i))\n    \n    # Using the same display function, defined earlier\n    show_keypoints(sample['image'], sample['keypoints'])","a557ea0d":"import torch\nfrom torchvision import transforms, utils\n# tranforms\nmean = [0.485, 0.456, 0.406] \nstd = [0.229, 0.224, 0.225]\n\nclass Normalize(object):\n    \"\"\"Convert a color image to grayscale and normalize the color range to [0,1].\"\"\"        \n\n    def __call__(self, sample):\n        image, key_pts = sample['image'], sample['keypoints']\n        \n        image_copy = np.copy(image)\n        key_pts_copy = np.copy(key_pts)\n\n        # convert image to grayscale\n        #image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        \n        # scale color range from [0, 255] to [0, 1]\n        image_copy = np.copy(image)\n        image_copy = image_copy \/ 255.0\n        image_copy= ( image_copy-mean ) \/std\n        \n        # scale keypoints to be centered around 0 with a range of [-1, 1]\n        # mean = 100, sqrt = 50, so, pts should be (pts - 100)\/50\n        key_pts_copy = (key_pts_copy - 100)\/50.0\n\n\n        return {'image': image_copy, 'keypoints': key_pts_copy}\n\n\nclass Rescale(object):\n    \"\"\"Rescale the image in a sample to a given size.\n\n    Args:\n        output_size (tuple or int): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, key_pts = sample['image'], sample['keypoints']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h \/ w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w \/ h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        img = cv2.resize(image, (new_w, new_h))\n        \n        # scale the pts, too\n        key_pts = key_pts * [new_w \/ w, new_h \/ h]\n\n        return {'image': img, 'keypoints': key_pts}\n\n\nclass RandomCrop(object):\n    \"\"\"Crop randomly the image in a sample.\n\n    Args:\n        output_size (tuple or int): Desired output size. If int, square crop\n            is made.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, key_pts = sample['image'], sample['keypoints']\n\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        image = image[top: top + new_h,\n                      left: left + new_w]\n\n        key_pts = key_pts - [left, top]\n\n        return {'image': image, 'keypoints': key_pts}\n\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        image, key_pts = sample['image'], sample['keypoints']\n         \n        # if image has no grayscale color channel, add one\n        if(len(image.shape) == 2):\n            # add that third color dim\n            image = image.reshape(image.shape[0], image.shape[1], 1)\n            \n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n        \n        return {'image': torch.from_numpy(image),\n                'keypoints': torch.from_numpy(key_pts)}\n    \n","b2353a4d":"# test out some of these transforms\nrescale = Rescale(100)\ncrop = RandomCrop(50)\ncomposed = transforms.Compose([Rescale(250),\n                               RandomCrop(224)])\n\n# apply the transforms to a sample image\ntest_num = 500\nsample = face_dataset[test_num]\n\nfig = plt.figure()\nfor i, tx in enumerate([rescale, crop, composed]):\n    transformed_sample = tx(sample)\n\n    ax = plt.subplot(1, 3, i + 1)\n    plt.tight_layout()\n    ax.set_title(type(tx).__name__)\n    show_keypoints(transformed_sample['image'], transformed_sample['keypoints'])\n\nplt.show()","1d0a4b8e":"# define the data tranform\n# order matters! i.e. rescaling should come before a smaller crop\ndata_transform = transforms.Compose([Rescale(250),\n                                     RandomCrop(224),\n                                     Normalize(),\n                                     ToTensor()])\n\n# create the transformed dataset\ntransformed_dataset = FacialKeypointsDataset(csv_file='..\/input\/facialkeypoint\/training_frames_keypoints.csv',\n                                             root_dir='..\/input\/facialkeypoint\/training\/',\n                                             transform=data_transform)\n\n# print some stats about the transformed data\nprint('Number of images: ', len(transformed_dataset))\n\n# make sure the sample tensors are the expected size\nfor i in range(5):\n    sample = transformed_dataset[i]\n    print(i, sample['image'].size(), sample['keypoints'].size())","09a9240f":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FacialKeyNet(nn.Module):\n    def __init__(self , in_channel , out_channel , seed):\n        super(FacialKeyNet , self).__init__()\n        self.in_channel = in_channel\n        self.out_channel = out_channel\n        self.seed = torch.manual_seed(seed)\n        \n        self.conv_1 = FacialKeyNet.__conv_block( in_size = 3 , out_size = 32 )   # 1- 224 - 224 --> 32 - 224 - 224\n        self.conv_2 = FacialKeyNet.__conv_block( in_size = 32 , out_size = 64  , maxpool=True ) # 32 - 224 - 224 --> 64 - 112 - 112\n        #self.conv_3 = FacialKeyNet.__conv_block( in_size = 64 , out_size = 64  , maxpool=True ) \n        self.conv_4 = FacialKeyNet.__conv_block( in_size = 64 , out_size = 128 , maxpool=True ) # 64 - 112 - 112 --> 128 - 56 - 56 \n        #self.conv_5 = FacialKeyNet.__conv_block( in_size = 128 , out_size = 128 , maxpool=True )\n        self.conv_6 = FacialKeyNet.__conv_block( in_size = 128 , out_size = 256 , maxpool=True) # 128 - 56 - 56 --> 256 - 28 - 28\n        #self.conv_7 = FacialKeyNet.__conv_block( in_size = 256 , out_size = 256 , maxpool=True ) # 256 - 28 - 28 --> 512 - 14 - 14\n        #self.conv_8 = FacialKeyNet.__conv_block( in_size = 256 , out_size = 512 , maxpool=False )\n        #self.conv_9 = FacialKeyNet.__conv_block( in_size = 512 , out_size = 512 , maxpool=True )\n        #self.conv_10 = FacialKeyNet.__conv_block(in_size = 512 , out_size = 1024 , maxpool=True ) # 512 - 14 - 14  --> 1024 - 7 - 7\n        self.flatten_size = 256 * 28 * 28\n        self.fc_1 = nn.Linear(self.flatten_size , self.out_channel)\n        #self.fc_bn1 = nn.BatchNorm1d(1024)\n        #self.fc_2 = nn.Linear(1024 , self.out_channel )\n        self.tanh = nn.Tanh()\n        \n    def forward(self , x):\n        x = self.conv_1(x)\n        x = self.conv_2(x)\n        #x = self.conv_3(x)\n        x = self.conv_4(x)\n        #x = self.conv_5(x)\n        x = self.conv_6(x)\n        #x = self.conv_7(x)\n        #x = self.conv_8(x)\n        #x = self.conv_9(x)\n        #x = self.conv_10(x)\n        x = x.view(-1 , self.flatten_size)\n        #x = F.dropout(F.relu(self.fc_bn1(self.fc_1(x))) , 0.5)\n        x = self.fc_1(x)\n        \n        return x\n        \n        \n    @staticmethod\n    def __conv_block( in_size , out_size , kernel_size = 3 ,\n                     strides = 1 , batch_norm=True , maxpool=False , dropout = 0.3 , drop_in=False):\n        layers =[]\n        conv_2d = nn.Conv2d(in_channels=in_size , out_channels=out_size , kernel_size=kernel_size ,\n                            stride=strides , padding=1 , bias=True)\n        layers.append(conv_2d)\n        if(batch_norm):\n            bn = nn.BatchNorm2d(out_size)\n            layers.append(bn)\n            \n        activ = nn.LeakyReLU(negative_slope=0.2)\n        layers.append(activ)\n        \n        if(maxpool):\n            m_pool = nn.MaxPool2d(kernel_size=2 , stride=2)\n            layers.append(m_pool)\n        if (drop_in):  \n            drop_layer = nn.Dropout2d(p=dropout)\n            layers.append(drop_layer)\n\n        return nn.Sequential(*layers)      \n    ","8105d97a":"from torchvision import models\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass VggFacialNet(nn.Module):\n    def __init__(self , in_channel , out_channel , seed):\n        super(VggFacialNet , self).__init__()\n        self.in_channel = in_channel\n        self.out_channel = out_channel\n        self.seed = seed\n\n        self.vgg_19 = models.vgg19_bn(pretrained=True, progress=True)\n        \n        self.vgg_19.classifier = nn.Sequential(\n            nn.Linear(in_features=25088 , out_features=self.out_channel , bias=True) \n        )\n        \n    def forward(self , x ):\n        \n        x = self.vgg_19(x)\n        \n        return x","80da9941":"facial_net = VggFacialNet(in_channel=3 , out_channel=68*2 , seed=1444)","d9035e94":"facial_net.load_state_dict(torch.load(\"..\/input\/facialnetmodels\/facial_vggnet.pth\"))","2a4fc384":"from torchvision import models\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ResNetFacialNet(nn.Module):\n    def __init__(self , in_channel , out_channel , seed):\n        super(ResNetFacialNet , self).__init__()\n        self.in_channel = in_channel\n        self.out_channel = out_channel\n        self.seed = seed\n\n        self.resnet_101 = models.resnet101(pretrained=True, progress=True)\n        \n        self.resnet_101.fc = nn.Sequential(\n            nn.Linear(in_features=2048 , out_features=self.out_channel , bias=True) \n        )\n        \n    def forward(self , x ):\n        \n        x = self.resnet_101(x)\n        \n        return x","a073ff4f":"from torchvision import models\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MobileNetFacialNet(nn.Module):\n    def __init__(self , in_channel , out_channel , seed):\n        super(MobileNetFacialNet , self).__init__()\n        self.in_channel = in_channel\n        self.out_channel = out_channel\n        self.seed = seed\n\n        self.mobilenet_v2 = models.mobilenet_v2(pretrained=True, progress=True)\n        \n        self.mobilenet_v2.classifier = nn.Sequential(\n            nn.Dropout(p=0.2 , inplace=False) , \n            nn.Linear(in_features=1280 , out_features=self.out_channel , bias=True) \n        )\n        \n    def forward(self , x ):\n        \n        x = self.mobilenet_v2(x)\n        \n        return x","963a61e1":"facial_net = MobileNetFacialNet(in_channel=3 , out_channel=68*2 , seed=1444)","01161f1d":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nfacial_net.to(device)","4c6d7c02":"import torch\nfacial_net.load_state_dict(torch.load('..\/input\/mobilenet-facial\/facial_net.pth', map_location='cpu'))","690fa484":"from torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\nbatch_size = 16\n\ntrain_transform = transforms.Compose([\n    Rescale(224) ,\n    Normalize() , \n    ToTensor()\n])\n\ntest_transform = transforms.Compose([\n    Rescale(224) , \n    Normalize() , \n    ToTensor()\n])\n\n# create the transformed dataset\ntrain_dataset = FacialKeypointsDataset(csv_file='..\/input\/facialkeypoint\/training_frames_keypoints.csv',\n                                             root_dir='..\/input\/facialkeypoint\/training\/',\n                                             transform=data_transform)\n\n# create the test dataset\ntest_dataset = FacialKeypointsDataset(csv_file='..\/input\/facialkeypoint\/test_frames_keypoints.csv',\n                                             root_dir='..\/input\/facialkeypoint\/test\/',\n                                             transform=data_transform)\n\n\ntrain_loader = DataLoader(train_dataset , batch_size= batch_size , shuffle=True , num_workers=0)\ntest_loader = DataLoader(test_dataset , batch_size= batch_size , shuffle=True , num_workers=0)\n","7b4c1195":"# test the model on a batch of test images\n\ndef net_sample_output():\n    \n    # iterate through the test dataset\n    for i, sample in enumerate(test_loader):\n        \n        # get sample data: images and ground truth keypoints\n        images = sample['image'].to(device)\n        key_pts = sample['keypoints'].to(device)\n\n        # convert images to FloatTensors\n        images = images.type(torch.FloatTensor).to(device)\n\n        # forward pass to get net output\n        facial_net.eval()\n        output_pts = facial_net(images)\n        \n        # reshape to batch_size x 68 x 2 pts\n        output_pts = output_pts.view(output_pts.size()[0], 68, -1)\n        \n        # break after first image is tested\n        if i == 0:\n            return images, output_pts, key_pts\n# call the above function\n# returns: test images, test predicted keypoints, test ground truth keypoints\ntest_images, test_outputs, gt_pts = net_sample_output()\n\n# print out the dimensions of the data to see if they make sense\nprint(test_images.data.size())\nprint(test_outputs.data.size())\nprint(gt_pts.size())\n\ndef show_all_keypoints(image, predicted_key_pts, gt_pts=None):\n    \"\"\"Show image with predicted keypoints\"\"\"\n    # image is grayscale\n    image = (image * std )+mean\n    plt.imshow(image, cmap='gray')\n    plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='m')\n    # plot ground truth points as green pts\n    if gt_pts is not None:\n        plt.scatter(gt_pts[:, 0], gt_pts[:, 1], s=20, marker='.', c='g')\n        \n# visualize the output\n# by default this shows a batch of 10 images\ndef visualize_output(test_images, test_outputs, gt_pts=None, batch_size=10):\n\n    for i in range(batch_size):\n        plt.figure(figsize=(50,20))\n        ax = plt.subplot(1, batch_size, i+1)\n\n        # un-transform the image data\n        image = test_images[i].data   # get the image from it's wrapper\n        image = image.to('cpu').detach().numpy()   # convert to numpy array from a Tensor\n        image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\n\n        # un-transform the predicted key_pts data\n        print(test_outputs.shape)\n        predicted_key_pts = test_outputs[i].data\n        predicted_key_pts = predicted_key_pts.to('cpu').detach().numpy()\n        # undo normalization of keypoints  \n        predicted_key_pts = predicted_key_pts*50.0+100\n        \n        # plot ground truth points for comparison, if they exist\n        ground_truth_pts = None\n        if gt_pts is not None:\n            ground_truth_pts = gt_pts[i]         \n            ground_truth_pts = ground_truth_pts*50.0+100\n        \n        # call show_all_keypoints\n        show_all_keypoints(np.squeeze(image), predicted_key_pts, ground_truth_pts.to('cpu').detach())\n            \n        plt.axis('off')\n\n    plt.show()\n    \n# call it\nvisualize_output(test_images, test_outputs, gt_pts)","c75ec354":"class RMSLoss(nn.Module):\n    def __init__(self):\n        super(RMSLoss , self).__init__()\n        self.criterion = nn.MSELoss()\n        \n    def forward(self , y_pred , y_true):\n        \n        loss = torch.sqrt(self.criterion(y_pred , y_true))\n        \n        return loss","9c30c9a4":"## TODO: Define the loss and optimization\nimport torch.optim as optim\n\ncriterion =  nn.SmoothL1Loss(reduction='sum')\n\noptimizer = optim.Adam(facial_net.parameters() , lr= 0.001 , weight_decay=0.1)","05610dca":"def train_net(n_epochs):\n\n    # prepare the net for training\n    facial_net.train()\n\n    for epoch in range(n_epochs):  # loop over the dataset multiple times\n        \n        running_loss = 0.0\n\n        # train on batches of data, assumes you already have train_loader\n        for batch_i, data in enumerate(train_loader):\n            # get the input images and their corresponding labels\n            images = data['image']\n            key_pts = data['keypoints']\n\n            # flatten pts\n            key_pts = key_pts.view(key_pts.size(0), -1)\n\n            # convert variables to floats for regression loss\n            key_pts = key_pts.type(torch.FloatTensor).to(device)\n            images = images.type(torch.FloatTensor).to(device)\n\n            # forward pass to get outputs\n            output_pts = facial_net(images)\n\n            # calculate the loss between predicted and target keypoints\n            loss = criterion(output_pts, key_pts)\n\n            # zero the parameter (weight) gradients\n            optimizer.zero_grad()\n            \n            # backward pass to calculate the weight gradients\n            loss.backward()\n\n            # update the weights\n            optimizer.step()\n\n            # print loss statistics\n            # to convert loss into a scalar and add it to the running_loss, use .item()\n            running_loss += loss.item()\n            if batch_i % 10 == 9:    # print every 10 batches\n                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss\/1000))\n                running_loss = 0.0\n\n    print('Finished Training')","58476f96":"\n# train your network\nn_epochs = 50 # start small, and increase when you've decided on your model structure and hyperparams\n\ntrain_net(n_epochs)","f7fd9998":"torch.save(facial_net.state_dict(),\"facial_net.pth\")","2ea175be":"#keep the index ranges for the better visualization\nkey_pairs={\n\"face_boarder\" : [(1,2),(2,3) ,(3,4) ,(4,5) ,(5,6) ,(6,7) ,(7,8) ,(8,9) ,(9,10) ,(10,11) ,(11,12) ,(12,13) ,(13,14) ,(14,15) ,(15,16) , [16,17]],\n\"upper_lips\" : [(49,50) , (50,51),(51,52),(52,53),(53,54),(54,55),(49,61),(61,62),(62,63),(63,64),(64,65),(65,55)] ,\n\"lower_lips\" : [ (49,60) , (60,59) , (59,58) , (58,57) , (57,56) , (56,55) , (49,61) , (61,68) , (68,67) ,(67,66) , (66,65) , (65,55) ],\n\"left_eye_upper\":[(18,19),(19,20),(20,21),(21,22)],\n\"right_eye_upper\" :[(23,24),(24,25),(25,26),(26,27)],\n\"left_eye\" :[(37,38),(38,39),(39,40),(40,41),(41,42),(42,37)],\n\"right_eye\" :[(43,44),(44,45),(45,46),(46,47),(47,48),(48,43) ],\n\"nose_upper\" :[(28,29),(29,30),(30,31)],\n\"nose_down\" :[(32,33),(33,34),(34,35),(35,36)]\n}","b5da0fce":"import cv2\n# select an image by index in our data frame\nn = 0\nimage_name = key_pts_frame.iloc[n, 0]\nkey_pts = key_pts_frame.iloc[n, 1:].to_numpy()\nkey_pts = key_pts.astype('float').reshape(-1, 2)\n\nplt.figure(figsize=(5, 5))\nimage = cv2.imread(os.path.join('..\/input\/facialkeypoint\/training\/', image_name))\nimage = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\nshow_keypoints(image, key_pts)\nplt.show()","da9f6ba2":"import numpy as np\nfig = plt.figure(figsize=(20,10))\nfor index, item in enumerate(key_pairs.items()): \n    for data in item[1] :\n        p1 = key_pts[data[0]-1].astype(int)\n        p2 = key_pts[data[1]-1].astype(int)\n       \n        cv2.line(image, tuple(p1), tuple(p2), [0, 255, 0], 1) \nplt.imshow(image)","a7f85ac8":"fig = plt.figure(figsize=(50,20))\ndef visualize_output(test_images, test_outputs, gt_pts=None, batch_size=10):\n\n    for i in range(batch_size):\n        plt.figure(figsize=(50,20))\n        ax = plt.subplot(1, batch_size, i+1)\n\n        # un-transform the image data\n        image = test_images[i].data   # get the image from it's wrapper\n        image = image.to('cpu').detach().numpy()   # convert to numpy array from a Tensor\n        image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\n\n        # un-transform the predicted key_pts data\n        predicted_key_pts = test_outputs[i].data\n        predicted_key_pts = predicted_key_pts.to('cpu').detach().numpy()\n        # undo normalization of keypoints  \n        predicted_key_pts = predicted_key_pts*50.0+100\n        \n        # plot ground truth points for comparison, if they exist\n        ground_truth_pts = None\n        if gt_pts is not None:\n            ground_truth_pts = gt_pts[i]         \n            ground_truth_pts = ground_truth_pts*50.0+100\n        \n        # call show_all_keypoints\n        show_all_keypoints(np.squeeze(image), predicted_key_pts, ground_truth_pts.to('cpu').detach())\n            \n        plt.axis('off')\n    plt.show()\n    \ndef show_all_keypoints(image, predicted_key_pts, gt_pts=None):\n    \"\"\"Show image with predicted keypoints\"\"\"\n    # image is grayscale\n    image = (image * std )+mean\n    plt.imshow(image, cmap='gray')\n    plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='m')\n    image = np.uint8(image*255.0)\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    # plot ground truth points as green pts\n    if gt_pts is not None:\n        plt.scatter(gt_pts[:, 0], gt_pts[:, 1], s=20, marker='.', c='g')\n    for index, item in enumerate(key_pairs.items()): \n        for data in item[1] :\n            p1 = predicted_key_pts[data[0]-1].astype(int)\n            p2 = predicted_key_pts[data[1]-1].astype(int)\n            cv2.line(image, tuple(p1), tuple(p2), [0, 255, 0], 1) \n    plt.imshow(image)\n        \ntest_images, test_outputs, gt_pts = net_sample_output()\n\nvisualize_output(test_images, test_outputs, gt_pts)","398effa5":"# Visualize the mesh\n![](https:\/\/miro.medium.com\/max\/1536\/1*mArsPXT2PB19dF4sPR-VSA.jpeg)","f0d9a350":"## Define the image transforms","a18bc3b3":"# 1.Custom Model","627d55cc":"## Facial keypoint vizualization","96763e9f":"## Model Architectuers","74bdf58e":"# 2. Vgg19 Model","a36a54eb":"# 4.MobileNet V2","a614ecca":"# 3. ResNet Model"}}