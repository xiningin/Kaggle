{"cell_type":{"f5f6461d":"code","ce8dec0c":"code","5b399137":"code","0bf4aa59":"code","042b75ff":"code","7c842742":"code","ad3443d5":"code","491bdfb4":"code","4bd7a1ce":"code","54b895dc":"code","ede8f85f":"code","1f6d0a5f":"code","71903779":"markdown","5a20a1d7":"markdown","63424d1f":"markdown","4d133c78":"markdown","99bc354b":"markdown","ab881682":"markdown","328f7d01":"markdown","46c8a5fa":"markdown","2b21f906":"markdown","3597b103":"markdown"},"source":{"f5f6461d":"import sys\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')","ce8dec0c":"from tqdm import tqdm\nimport gc\nimport math\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Visuals and CV2\nimport matplotlib.pyplot as plt\nimport cudf, cuml, cupy\nimport cv2\n\n# albumentations for augs\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n#torch\nimport torch\nimport timm\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nimport warnings \nwarnings.filterwarnings('ignore')","5b399137":"DIM = (512,512)\n\nNUM_WORKERS = 4\nBATCH_SIZE = 16\nEPOCHS = 6\nSEED = 2020\nLR = 3e-4\n\nTRAIN_IMG = '..\/input\/shopee-product-matching\/train_images'\n\nDEVICE = \"cuda\"\n\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]\n\n\n################################################# MODEL ####################################################################\n\nMODEL_NAME = 'efficientnet_b0' #efficientnet_b3 #efficientnetb5 #efficientnetb7\n\nSCHEDULER = 'CosineAnnealingWarmRestarts' #'CosineAnnealingLR'\nT_0=3 # CosineAnnealingWarmRestarts\nmin_lr=1e-6","0bf4aa59":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","042b75ff":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","7c842742":"def get_train_transforms():\n    return albumentations.Compose(\n        [\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=120, p=0.8),\n            albumentations.RandomBrightness(limit=(0.09, 0.6), p=0.5),\n            #albumentations.Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, always_apply=False, p=0.5),\n            #albumentations.ShiftScaleRotate(\n             #   shift_limit=0.25, scale_limit=0.1, rotate_limit=0\n            #),\n            albumentations.Normalize(\n                MEAN, STD, max_pixel_value=255.0, always_apply=True\n            ),\n        \n            ToTensorV2(p=1.0),\n        ]\n    )\n\ndef get_valid_transforms():\n\n    return albumentations.Compose(\n        [albumentations.Normalize(MEAN, STD, max_pixel_value=255.0, always_apply=True),\n        ToTensorV2(p=1.0)\n        ]\n    )","ad3443d5":"class SiameseNetworkDataset(Dataset):\n    \n    def __init__(self,image_1,image_2,labels,dim=(512,512),augmentation=None):\n        self.image_1 = image_1\n        self.image_2 = image_2\n        self.labels = labels\n        self.dim = dim\n        self.augmentation = augmentation\n        \n    def __len__(self):\n        return len(self.image_1)\n        \n    def __getitem__(self,index):\n        img_0 = self.image_1[index]\n        img_1 = self.image_2[index]\n        \n        img_0 = cv2.imread(f'{TRAIN_IMG}\/{img_0}')\n        img_1 = cv2.imread(f'{TRAIN_IMG}\/{img_1}')\n        \n        img_0 = cv2.cvtColor(img_0, cv2.COLOR_BGR2RGB)\n        img_1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2RGB)\n        \n        if self.dim:\n            img_0 = cv2.resize(img_0,self.dim)\n            img_1 = cv2.resize(img_1,self.dim)\n            \n        if self.augmentation:\n            augmented_0 = self.augmentation(image=img_0)\n            augmented_1 = self.augmentation(image=img_1)\n            img_0 = augmented_0['image']\n            img_1 = augmented_1['image']\n            \n    \n        return img_0,img_1 ,torch.tensor(self.labels[index],dtype=torch.float32)","491bdfb4":"class SiameseModel(nn.Module):\n    def __init__(self, model_name='efficientnet_b0',out_features=2,pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        n_features = self.model.classifier.in_features\n        \n        self.model.global_pool = nn.Identity()\n        self.model.classifier = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Sequential(nn.Linear(n_features, n_features\/\/2),\n                                nn.ReLU(),\n                                nn.Linear(n_features\/\/2, out_features)\n                                )\n        \n    def forward_once(self, x):\n        bs = x.size(0)\n        output = self.model(x)\n        output = self.pooling(output).view(bs, -1)\n        \n        output = self.classifier(output)\n        \n        return output\n\n    def forward(self, image_1,image_2):\n        output1 = self.forward_once(image_1)\n        output2 = self.forward_once(image_2)\n        return output1, output2\n    \nd = SiameseModel(model_name='efficientnet_b0',pretrained=False)\nt1 = torch.ones((1,3,512,512))\nt2 = torch.ones((1,3,512,512))\nx1,x2 = d(t1,t2)\n\nprint(x1.size())\nprint(x2.size())\n\ndel x1,x2,t1,t2,d","4bd7a1ce":"class ContrastiveLoss(torch.nn.Module):\n    \"\"\"\n    Contrastive loss function.\n    Based on: http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/hadsell-chopra-lecun-06.pdf\n    \"\"\"\n\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2)\n        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n\n\n        return loss_contrastive","54b895dc":"def train_fn(dataloader,model,criterion,optimizer,device,scheduler,epoch):\n    model.train()\n    loss_score = AverageMeter()\n    \n    tk0 = tqdm(dataloader, total=len(dataloader))\n    for img_0,img_1,label in tk0:\n        \n        img_0 = img_0.to(device)\n        img_1 = img_1.to(device)\n\n        label = label.to(device)\n        \n        batch_size = img_0.shape[0]\n        \n        optimizer.zero_grad()\n        \n        output_1,output_2 = model(img_0,img_1)\n        \n        loss = criterion(output_1,output_2,label)\n        loss.backward()\n        optimizer.step()\n        \n        loss_score.update(loss.detach().item(), batch_size)\n        \n        \n        tk0.set_postfix(Train_Loss=loss_score.avg,Epoch=epoch,LR=optimizer.param_groups[0]['lr'])\n    \n    if scheduler is not None:\n            scheduler.step()\n        \n    return loss_score","ede8f85f":"def run():\n    \n    df = pd.read_csv('..\/input\/shopee-siamese-training\/siamese_data.csv')\n\n    \n    # Defining DataSet\n    train_dataset = SiameseNetworkDataset(\n        image_1=df['image_1'].values.tolist(),\n        image_2 = df['image_2'].values.tolist(),\n        labels=df['label'].values.tolist(),\n        dim = DIM,\n        augmentation=get_train_transforms(),\n    )\n        \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=NUM_WORKERS\n    )\n    \n    # Defining Device\n    device = torch.device(\"cuda\")\n    \n    # Defining Model for specific fold\n    model = SiameseModel(model_name= MODEL_NAME,out_features=64,pretrained=True)\n    model.to(device)\n    \n    #DEfining criterion\n    criterion = ContrastiveLoss()\n    criterion.to(device)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n    #Defining LR SCheduler\n    scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=T_0)\n    \n    # THE ENGINE LOOP\n    best_loss = 10000\n    for epoch in range(EPOCHS):\n        train_loss = train_fn(train_loader, model,criterion, optimizer, device,scheduler=scheduler,epoch=epoch)\n        \n        if train_loss.avg < best_loss:\n            best_loss = train_loss.avg\n            torch.save(model.state_dict(),f'model_best_loss.bin')","1f6d0a5f":"run()","71903779":"# About this Notebook\n\nHi everyone , welcome to this competition . This is really a very interesting competition given the problem . We have to match each element in the test set to similar elements in the same test set . We have already seen some very good starter notebooks like :\n\n* https:\/\/www.kaggle.com\/ragnar123\/shopee-inference-cosine-distance-approach\/notebook by @ragnar\n* https:\/\/www.kaggle.com\/cdeotte\/part-2-rapids-tfidfvectorizer-cv-0-700 by @chrisdeotte\n\nIn his kernel chris uses raw efficientnet to generate image embeddings while ragnar first fine tunes the efficientnet on the train data then uses it to generate embeddings on test data. We all know a fine tuned model will work better as it will generate more relevant embeddings and hence will be succesful in finding similar image pairs . Now there can a few techniques to fine tune the model on train set , one of the technique is :\n\n* Using label groups as targets trying to predict the label group as done by @ragnar in his kernel \n\nOther very useful technique when it comes to generating embeddings for semantic matching is <b>Siamese Style Training<\/b> , this notebook dicusses this approach \n\n<b>In this notebook I fine-tune <u>Efficientnet-B0 in SIAMESE STYLE<\/u> so that it can be used for embedding generation in the later stage <\/b>\n\n# About Training Method Used\n\nI have created a dataset much like Avito Adds Detection competition in which there are the following columns :\n\n* image_1 : id of first image\n* image_2 : id of second image\n* title_1: title of first image\n* title_2: title of second image\n* label : 1 or 0 for similar and dissimilar images respectively\n\nNow instead of trainig for predicting label groups , I predict whether the images are same or not using a contrastive loss . This style of training is sometimes called 'SIAMESE STYLE OF TRAINING' where two same CNN architectures sharing the same set of params take two different image and try to predict whether they look the same or not. More resources on SIAMESE network are available [here](https:\/\/towardsdatascience.com\/a-friendly-introduction-to-siamese-networks-85ab17522942)\n\nThe dataset used for Siamese Type Training can be found [here](https:\/\/www.kaggle.com\/tanulsingh077\/shopee-siamese-training)","5a20a1d7":"# Loss","63424d1f":"# Engine","4d133c78":"# Conclusion\n\nThere can be a lot of use cases of the data generated :\n\n* we can take the titles present for two different images and fine tune an S-BERT model which is already pretrained using Siamese Approach\n* We can train a classifier like XGB to predict 0 or 1 and then use the classifier to predict on test set\n* Since the data prepared is in coherence with the Avito Adds Detection challenge data we can utilize a lot of approaches used there\n\nAs the competition has just started there are still a lot of ways which can be tried\nThanks for reading the kernel , I hope it helped you with something","99bc354b":"# Dataset","ab881682":"# Augs","328f7d01":"# Utils","46c8a5fa":"# Configuration","2b21f906":"# Model","3597b103":"# Train-Loop"}}