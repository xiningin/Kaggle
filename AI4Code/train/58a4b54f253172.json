{"cell_type":{"2247dc89":"code","d58c58ac":"code","aaddc143":"code","18ab49e7":"code","92468944":"code","4ca56bce":"code","dd667469":"code","3a71a1b7":"code","0c25c5a6":"code","bd76f9c0":"code","bc59a92e":"code","42c5f497":"code","c978b134":"code","364b6840":"code","85158205":"code","2055b3af":"code","6e880852":"code","f5b946c0":"code","e0514985":"code","023c1f06":"code","6bff1912":"code","bf43eec9":"code","796d25a8":"code","b94573bf":"code","bfe575cc":"code","74830c51":"code","f03d7e66":"code","50f20b0d":"code","5d406bca":"code","900b5b13":"code","b6e1da22":"code","c9f22b37":"code","6b305954":"code","dca66c1d":"code","53e04516":"code","b1f88ecf":"code","2316d855":"code","98f8e24c":"code","da7da602":"markdown","9eeed5e6":"markdown","ae536075":"markdown","1a5d0b91":"markdown","3a4fc808":"markdown","aea21b28":"markdown","f2c786ac":"markdown"},"source":{"2247dc89":"import pandas as pd\nimport numpy as np\nimport re\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt\nfrom keras.applications.resnet50 import ResNet50,preprocess_input\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","d58c58ac":"import os","aaddc143":"df=pd.read_csv('..\/input\/flickr-image-dataset\/flickr30k_images\/results.csv',delimiter='|')\n\ndf.head()\n\ndf.iloc[0]\n\nid=df['image_name'].values\n\nid.shape\n\ncomment=df[' comment'].values\n\ncomment.shape\n\ncomment[5]\n\ndef sentence_cleaning(sentence):\n    try:\n        sentence=sentence.lower()\n        sentence=re.sub('[^a-z]+',' ',sentence)\n        sentence=sentence.split()\n        sentence=[s for s in sentence if len(s)>1]\n        sentence=' '.join(sentence)\n        return(sentence)\n    except:\n        return(sentence_cleaning('A dog runs across the grass .'))\n\nvocab_dic={}\nfor i in range(comment.shape[0]):\n    if id[i] not in vocab_dic:\n        vocab_dic[id[i]]=[]\n    sen=sentence_cleaning(comment[i])\n    vocab_dic[id[i]].append(sen)\n\nlen(vocab_dic)\n\nprint(vocab_dic['1000092795.jpg'])\n\nword_dic={}\n\nfor i in vocab_dic:\n    for j in vocab_dic[i]:\n        l=j.split()\n        for k in l:\n            if k not in word_dic:\n                word_dic[k]=1\n            else:\n                word_dic[k]+=1\n\nprint(len(word_dic))\n\nfinal_words=[x for x in word_dic if word_dic[x]>10]\n\nprint(len(final_words))\n\nfor i in vocab_dic:\n    for j in range(len(vocab_dic[i])):\n        vocab_dic[i][j]='startseq '+vocab_dic[i][j]+' endseq'\n\ns=1\nword_to_idx={}\nidx_to_word={}\nfor i in final_words:\n    word_to_idx[i]=s\n    idx_to_word[s]=i\n    s+=1\n\nprint(len(word_to_idx))\n\n### Two Special words\nword_to_idx['startseq']=5119\nword_to_idx['endseq']=5120\nidx_to_word[5119]='startseq'\nidx_to_word[5120]='endseq'\n\nlen(word_to_idx)\n\nvocab_size=len(word_to_idx)+1 # adding one for 0 because that will also in our vector\n\nmax_len=20","18ab49e7":"model=ResNet50(weights='imagenet',input_shape=(224,224,3))","92468944":"model.summary()","4ca56bce":"new_model=Model(model.input,model.layers[-2].output)","dd667469":"def preprocess_img(path):\n    img=image.load_img(path,target_size=(224,224,3))\n    img=image.img_to_array(img)\n    img=img.reshape(1,224,224,3)\n    img=preprocess_input(img)#mormalizing the img\n    return img","3a71a1b7":"img=preprocess_img('..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/10002456.jpg')","0c25c5a6":"plt.imshow(img[0])","bd76f9c0":"def encode_img(path):\n    img=preprocess_img(path)\n    feature_vector=new_model.predict(img)\n    feature_vector=feature_vector.reshape((-1,))\n    return feature_vector\n    ","bc59a92e":"# train_description['1244140539_da4804d828']","42c5f497":"encoded_img_dic={}\ns=0\nfor i in vocab_dic:\n    path='..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/'+i\n    encoded_img_dic[i]=encode_img(path)\n    s+=1\n    if s%100==0:\n        print(s)","c978b134":"np.save('encoded_img_dic.npy',encoded_img_dic)","364b6840":"!ls -l --b=M  .\/encoded_img_dic.npy | cut -d \" \" -f5","85158205":"def data_generator(train_description,vocab_size,word_to_idx,encoded_img_dic,max_len,batch_size):\n    X1,X2,y=[],[],[]\n    n=0\n    while True:\n        for key,desc_list in train_description.items():\n            n+=1\n            encoding_of_photo=encoded_img_dic[key]\n            for desc in desc_list:\n                seq=[word_to_idx[i] for i in desc.split() if i in word_to_idx]\n                for i in range(1,len(seq)):\n                    xi=seq[0:i]\n                    yi=seq[i]\n                    \n                    xi=pad_sequences([xi],maxlen=max_len,value=0,padding='post')[0]\n                    yi=to_categorical([yi],num_classes=vocab_size)[0]\n                    \n                    \n                    X1.append(encoding_of_photo)\n                    X2.append(xi)\n                    y.append(yi)\n                if n==batch_size:\n                    yield [np.array(X1),np.array(X2)],np.array(y)\n                    X1,X2,y=[],[],[]\n                    n=0","2055b3af":"# with open('glove.6B.50d.txt',encoding='utf8') as f:\n#     glove_data=f.read()\n\n# len(glove_data)\n\n# type(glove_data)\n\n# glove_data=glove_data.split('\\n')\n\n# type(glove_data)\n\n# len(glove_data)\n\n# glove_data=glove_data[:-1]\n\n# len(glove_data)\n\n# glove_data[0].split()[0]\n\n# embedding_index={}\n# for line in glove_data:\n#     line=line.split()\n#     word=line[0]\n#     embeding=np.array(line[1:],dtype='float')\n#     embedding_index[word]=embeding\n    \n\n# len(embedding_index)\n\n# embedding_index['the'].shape\n\n# def get_embedding_matrix():\n#     dim=50\n#     matrix=np.zeros((vocab_size,dim))\n#     for word,number in word_to_idx.items():\n#         embedding_vector=embedding_index.get(word)\n#         if embedding_vector is not None:\n#             matrix[word_to_idx[word]]=embedding_vector\n#     return matrix\n    \n\n# embedding_matrix=get_embedding_matrix()\n\n# len(embedding_matrix)\n\n# embedding_matrix[word_to_idx['the']]\n\n","6e880852":"embedding_matrix=np.load('..\/input\/embedding-matrix\/embedding_matrix.npy')","f5b946c0":"embedding_matrix[3]","e0514985":"encoded_img_dic['10002456.jpg'].shape","023c1f06":"## For images\ninput_img_features=Input(shape=(2048,))\ninp_img1=Dropout(0.3)(input_img_features)\ninp_img2=Dense(256,activation='relu')(inp_img1)\n","6bff1912":"vocab_size","bf43eec9":"input_captions=Input(shape=(max_len,))\ninp_cap1=Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_captions)\ninp_cap2=Dropout(0.3)(inp_cap1)\ninp_cap3=LSTM(256)(inp_cap2)","796d25a8":"decoder1=add([inp_img2,inp_cap3])\ndecoder2=Dense(256,activation='relu')(decoder1)\noutputs=Dense(vocab_size,activation='softmax')(decoder2)","b94573bf":"actual_model=Model(inputs=[input_img_features,input_captions],outputs=outputs)\n","bfe575cc":"actual_model.summary()","74830c51":"actual_model.layers[2].output","f03d7e66":"actual_model.layers[2].set_weights([embedding_matrix])\nactual_model.layers[2].trainable=False","50f20b0d":"actual_model.compile(loss='categorical_crossentropy',optimizer='adam')","5d406bca":"epochs=10\nbatch_size=3\nsteps=len(vocab_dic)\/\/batch_size","900b5b13":"def train():\n    for i in range(epochs):\n        generator=data_generator(vocab_dic,vocab_size,word_to_idx,encoded_img_dic,max_len,batch_size)\n        actual_model.fit_generator(generator,epochs=1,steps_per_epoch=steps,verbose=1)\n        actual_model.save('.\/Models\/model'+str(i)+'.h5')\n        ","b6e1da22":"# os.mkdir('Models')","c9f22b37":"train()","6b305954":"max_len","dca66c1d":"def predict_caption(img):\n    img=img.reshape(1,224,224,3)\n    img=preprocess_input(img)\n    feature_vector=new_model.predict(img)\n    feature_vector=feature_vector.reshape((1,2048,1))\n    in_text='startseq'\n    for i in range(max_len):\n        seq=[word_to_idx[w] for w in in_text.split() if w in word_to_idx]\n        seq=pad_sequences([seq],maxlen=max_len,padding='post')\n        y_pred=actual_model.predict([feature_vector,seq])\n        y_pred=y_pred.argmax()\n        word=idx_to_word[y_pred]\n        in_text+=' '+word\n        \n        if word=='endseq':\n            break\n    final_caption=in_text.split()[1:-1]\n    final_caption=' '.join(final_caption)\n    return final_caption","53e04516":"img=image.load_img('..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/10010052.jpg',target_size=(224,224,3))\nimg=image.img_to_array(img)\nplt.imshow(img\/255)","b1f88ecf":"predict_caption(img)","2316d855":"plt.imshow(img)","98f8e24c":"ls -l --b=M  .\/Models\/model5.h5 | cut -d \" \" -f5","da7da602":"## loading embedding matrix using numpy","9eeed5e6":"## Training the Model","ae536075":"## Predicting the caption using the trained model","1a5d0b91":"## Custom Data Generator","3a4fc808":"## Creating Vocab dictionary and caption preprocessing","aea21b28":"## Model Architecture","f2c786ac":"## Image data preprocessing"}}