{"cell_type":{"b8096610":"code","d47c36e2":"code","b1b9d393":"code","91aa6bd4":"code","36fe4ccc":"code","f99e4f02":"code","cf224c5f":"code","51a75580":"code","6845110d":"code","6bc81021":"code","71168f4b":"code","8c1ea2fc":"code","00dd4ab7":"code","bfa561c2":"code","47212a51":"code","cb9563e2":"code","5240eb09":"code","75d41c70":"code","2a8c1a1e":"code","114cd387":"code","65250a07":"code","40f235bd":"code","e46e03a8":"code","0fabc78f":"code","37c9badb":"code","3a5dace1":"code","86732273":"code","ed8628b4":"code","700d8182":"code","a7a145db":"code","ae33f903":"code","67bac6f4":"code","567b05c9":"code","bda4c294":"code","452c2bf0":"code","456887fd":"code","f35e62ff":"code","fa47882b":"code","a2f41fcc":"code","99b2f3e9":"code","ccfdf986":"code","1a73f501":"code","9d298b85":"code","0ad532f6":"code","b8b2a574":"code","0456937c":"code","9ca49438":"code","ff5b75a7":"code","f7c99347":"code","34910f6a":"code","bb8967cd":"code","edf2dff2":"markdown","14693726":"markdown","95e5c5b8":"markdown","9c7352a5":"markdown","0d867f92":"markdown","9cd00c26":"markdown","3f0caacd":"markdown","a01b55c2":"markdown","5c03d538":"markdown","d702a01e":"markdown","4ecaeabf":"markdown","447bed75":"markdown","bff12aaf":"markdown","fd7faca4":"markdown","5312e3ce":"markdown","b2c9696a":"markdown","95745ba2":"markdown","d5afb694":"markdown"},"source":{"b8096610":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom scipy.stats import boxcox\nfrom sklearn.preprocessing import StandardScaler,Normalizer\nfrom sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV,StratifiedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectFromModel, RFE,SelectPercentile\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport umap\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d47c36e2":"dfdata = pd.read_csv(\"..\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv\")\ndfdata.columns = dfdata.columns.str.lower()\ndfdata.head()","b1b9d393":"dfdata.describe().T","91aa6bd4":"print(\"Data : \",dfdata.shape)\nprint(\"Duplicate rows : \",dfdata[dfdata.duplicated()].shape)\ndfdata.drop_duplicates(inplace = True)\nprint(\"After drop duplicates : \",dfdata.shape)\n","36fe4ccc":"dfdata.info()","f99e4f02":"dfdata.isnull().sum()","cf224c5f":"msno.matrix(dfdata,figsize=(20,6))\nplt.show()","51a75580":"msno.heatmap(dfdata,figsize=(8,8))\nplt.show()","6845110d":"msno.dendrogram(dfdata,figsize=(12,12))\nplt.show()","6bc81021":"dfdata.isnull().sum()","71168f4b":"dfdata[dfdata['distance'].isnull() == True]","8c1ea2fc":"# from dendrogram 9 columns are null, drop it\ndfdata.drop(dfdata[dfdata['distance'].isnull() == True].index,axis=0,inplace=True)\ndfdata.shape","00dd4ab7":"dfdata[dfdata['bedroom2'].isnull() == True][['rooms','price','landsize','car']].describe().T","bfa561c2":"pd.crosstab(dfdata['bedroom2'],dfdata['rooms'])\n","47212a51":"sns.countplot(dfdata[dfdata['bedroom2'].isnull() == True]['rooms'])\nplt.show()","cb9563e2":"dfdata['bedroom2'].fillna(dfdata['rooms'],inplace=True)\ndfdata.isnull().sum()","5240eb09":"pd.crosstab(dfdata['bathroom'],dfdata['rooms'])","75d41c70":"dftemp = dfdata.groupby(['rooms'],as_index=False)['bathroom'].median()\nindices = dfdata[dfdata['bathroom'].isnull() == True].index\ndfdata.loc[indices,'bathroom'] = dfdata.loc[indices,'rooms'].apply(lambda x : dftemp[dftemp['rooms']==x]['bathroom'].values[0])\ndfdata.isnull().sum()","2a8c1a1e":"dfdata[dfdata['bathroom'] == 'Nan']","114cd387":"def fill_via_suburb(colname):\n    indices = dfdata[dfdata[colname].isnull() == True].index\n    dfdata.loc[indices,colname] = dfdata.loc[indices,'suburb'].map(lambda x: dfdata[dfdata['suburb'] == x][colname].mode()[0])\n\nfill_via_suburb('councilarea')\nfill_via_suburb('regionname')\nfill_via_suburb('propertycount')\ndfdata.isnull().sum()","65250a07":"dfdata.drop(['car','landsize','buildingarea','yearbuilt','lattitude','longtitude'],axis=1,inplace=True)\ndfdata.isnull().sum()","40f235bd":"dfdata.dropna(subset = {'price'},inplace = True)\ndfdata.describe()","e46e03a8":"fig,ax = plt.subplots(2,3,figsize = (24,8))\nsns.distplot(dfdata['price'], ax = ax[0][0])\nsns.boxplot(dfdata['price'],ax = ax[0][1])\nstats.probplot(dfdata['price'],plot = ax[0][2])\nsns.distplot(dfdata['distance'], ax = ax[1][0])\nsns.boxplot(dfdata['distance'],ax = ax[1][1])\nstats.probplot(dfdata['distance'],plot = ax[1][2])\nplt.show()","0fabc78f":"fig,ax = plt.subplots(1,3,figsize = (24,5))\nsns.distplot(dfdata['rooms'], ax = ax[0])\nsns.distplot(dfdata['bedroom2'],ax = ax[1])\nsns.distplot(dfdata['distance'],ax = ax[2])\nplt.show()","37c9badb":"dfskew = dfdata[['distance','price']]\ndfskew_normal = Normalizer().fit_transform(dfskew)\ndfskew_standard = StandardScaler().fit_transform(dfskew)\nfig,ax = plt.subplots(2,2,figsize = (18,4))\nsns.distplot(dfskew_normal[:,0], ax = ax[0][0])\nsns.distplot(dfskew_normal[:,1], ax = ax[0][1])\nsns.distplot(dfskew_standard[:,0], ax = ax[1][0])\nsns.distplot(dfskew_standard[:,1], ax = ax[1][1])\nplt.show()","3a5dace1":"dfskew_log = np.log1p(dfskew)\nfig,ax = plt.subplots(1,2,figsize = (18,4))\nsns.distplot(dfskew_log['distance'], ax = ax[0])\nsns.distplot(dfskew_log['price'], ax = ax[1])\nplt.show()","86732273":"def sigmoid(x):\n    e = np.exp(1)\n    y = 1 \/(1 + e** (-x))\n    return y","ed8628b4":"dfskew_sigmoid = sigmoid(dfskew)\nfig,ax = plt.subplots(1,2,figsize = (18,4))\nsns.distplot(dfskew_sigmoid['distance'], ax = ax[0])\nsns.distplot(dfskew_sigmoid['distance'], ax = ax[1])\nplt.show()","700d8182":"dfskew_tanh = np.tanh(dfskew)\nfig,ax = plt.subplots(1,2,figsize = (18,4))\nsns.distplot(dfskew_tanh['distance'], ax = ax[0])\nsns.distplot(dfskew_tanh['price'], ax = ax[1])\nplt.show()","a7a145db":"dfskew_root3 = dfskew**(1\/3)\nfig,ax = plt.subplots(1,2,figsize = (18,4))\nsns.distplot(dfskew_root3['distance'], ax = ax[0])\nsns.distplot(dfskew_root3['price'], ax = ax[1])\nplt.show()","ae33f903":"dfskew_cube = dfskew**(3)\nfig,ax = plt.subplots(1,2,figsize = (18,4))\nsns.distplot(dfskew_cube['distance'], ax = ax[0])\nsns.distplot(dfskew_cube['price'], ax = ax[1])\nplt.show()","67bac6f4":"dfskew_boxcox = pd.DataFrame()\ndfskew_boxcox['price'],power_selected = boxcox(dfskew['price'])\nprint(\"lambda selected\",round(power_selected,2))\nfig,ax = plt.subplots(1,2,figsize = (18,4))\nsns.distplot(dfskew['price'], ax = ax[0])\nsns.distplot(dfskew_boxcox['price'], ax = ax[1])\nplt.show()\n","567b05c9":"dfskew_linear = dfskew.rank(method='min').apply(lambda x:(x-1)\/(dfskew.shape[0] - 1))\nfig,ax = plt.subplots(1,2,figsize = (18,4))\nsns.distplot(dfskew_linear['distance'], ax = ax[0])\nsns.distplot(dfskew_linear['price'], ax = ax[1])\nplt.show()","bda4c294":"def get_lr_result(X,y):\n    lr = LinearRegression().fit(X,y)\n    return round(100 * lr.score(X, y),1),lr","452c2bf0":"dfdatanew = dfdata[['rooms','price']]\ndfdatanew['rooms'] = dfdatanew['rooms'].astype('str')\nX = dfdatanew[['rooms']]\ny = dfdatanew['price']\nprint(\"Rooms -> Price                  \",get_lr_result(X,y)[0])\nX_encoded = pd.get_dummies(X,drop_first=True)\nprint(\"one hot encoded rooms -> Price  \",get_lr_result(X_encoded,y)[0])\ncoef1 = np.round(get_lr_result(X_encoded,y)[1].coef_)\ntmp = X_encoded.sum(axis=1)\nindices = tmp[tmp == 0].index\nX_encoded.loc[indices,:] = -1.0\nprint(\"effect encoded rooms -> Price  \",get_lr_result(X_encoded,y)[0])\ncoef2 = np.round(get_lr_result(X_encoded,y)[1].coef_)\nplt.figure(figsize=(18,4))\nplt.plot(coef1,label='one-hot-encoding')\nplt.plot(coef2,label='effect encoding')\nplt.xticks(range(len(coef1)),X_encoded.columns,rotation=45)\nplt.legend()\nplt.show()","456887fd":"dfdatanew['rooms'] = dfdatanew['rooms'].astype('int')\ndfdatanew['roomsnew'] = np.where(dfdatanew['rooms'] > 5,6,dfdatanew['rooms'])\nfig,ax = plt.subplots(1,2,figsize=(18,5),sharey=True)\nsns.countplot(x='rooms',data=dfdatanew,ax=ax[0])\nsns.countplot(x='roomsnew',data=dfdatanew,ax=ax[1])\nplt.show()\nX_encoded = pd.get_dummies(dfdatanew['roomsnew'],drop_first=True)\nprint(\"with backoff bins -> Price  \",get_lr_result(X_encoded,y)[0])","f35e62ff":"X = dfdata['rooms']\nX.value_counts()","fa47882b":"X_encoded = pd.get_dummies(X, drop_first=True)\nX_encoded.columns","a2f41fcc":"from sklearn.feature_selection import VarianceThreshold\nvt_filter = VarianceThreshold(threshold=0.01)\nvt_filter.fit(X_encoded)\ndrop_list = [column for column in X_encoded.columns if column not in X_encoded.columns[vt_filter.get_support()]]\nprint(drop_list)","99b2f3e9":"numeric_columns = dfdata.columns[dfdata.dtypes != 'object']\nnumeric_columns","ccfdf986":"corr_matrix = np.corrcoef(dfdata[numeric_columns],rowvar=False)\ncorr_matrix = dfdata[numeric_columns].corr()\nsns.heatmap(corr_matrix,annot=True,fmt='.1g')\nplt.show()","1a73f501":"X = dfdata[['rooms','type','method','distance','bathroom','regionname']]\ny = dfdata['price']","9d298b85":"X_encoded = pd.get_dummies(X,drop_first=True)\nX_encoded.columns","0ad532f6":"X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, random_state=0)\nselect = SelectPercentile(percentile=50)\nselect.fit(X_train, y_train)\nX_train_selected = select.transform(X_train)\n\ncolumn_list = [column for column in X_encoded.columns if column not in X_encoded.columns[select.get_support()]]\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nprint(\"Selected Features\",column_list)","b8b2a574":"model = RandomForestRegressor(max_depth=10,random_state=0)\nselect = SelectFromModel(model,threshold=\"median\")\nselect.fit(X_train, y_train)\nX_train_selected = select.transform(X_train)\n\ncolumn_list = [column for column in X_encoded.columns if column not in X_encoded.columns[select.get_support()]]\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nprint(\"Selected Features\",column_list)","0456937c":"model = RandomForestRegressor(max_depth=10,random_state=0)\nselect = RFE(model,n_features_to_select=8)\nselect.fit(X_train, y_train)\nX_train_selected = select.transform(X_train)\n\ncolumn_list = [column for column in X_encoded.columns if column not in X_encoded.columns[select.get_support()]]\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nprint(\"Selected Features\",column_list)","9ca49438":"X = dfdata.select_dtypes(include=np.number).drop(['price'],axis=1)\nX_scaled = pd.DataFrame(StandardScaler().fit_transform(X),columns = X.columns)\nX_scaled.head()","ff5b75a7":"pca = PCA(n_components = X_scaled.shape[1]).fit(X_scaled)\ncumratio = np.round(100 * pca.explained_variance_ratio_.cumsum(),1)\nsns.pointplot(x=X_scaled.columns.values,y=cumratio)\nplt.xticks(rotation=45)\nplt.show()","f7c99347":"pca = PCA(n_components = 4)\npca.fit(X_scaled)\nX_pca = pca.transform(X_scaled)\nsns.heatmap(pca.components_,cmap='viridis',annot=True,fmt='.1g')\nplt.xticks(range(len(X.columns)),X.columns,rotation=45)\nplt.show()\nprint(\"All Numeric -> Price          \",get_lr_result(X_scaled,y)[0])\nprint(\"PCA -> Price                  \",get_lr_result(X_pca,y)[0])","34910f6a":"tsne_2d = TSNE(n_components=2, perplexity=20)\ntsne_data = tsne_2d.fit_transform(X_scaled[['rooms']])\nsns.scatterplot(tsne_data[:,0],tsne_data[:,1])\nplt.show()","bb8967cd":"umap_data = umap.UMAP(n_neighbors=20).fit_transform(X_scaled[['rooms']])\nsns.scatterplot(umap_data[:,0],umap_data[:,1])\nplt.show()","edf2dff2":"# Feature Selection","14693726":"# Missing Values<br>\nmissingno package can be used to find reason and correlation of missing values with graphical representations.<br>\nMissing (Not\/Completely) at Random. (MAR,MNAR,MCAR)<br>\nSimpleImputer and fancyimpute packages can be used for imputation of missing values.","95e5c5b8":"## Normalization vs Standardization<br>\n- Linear Models, Neural Network and Gradient Descent Algorithms : Only normal distributed data can be forecasted with high scores. High valued data and coefficients is not preferred.\n- Clustering : Normal distributed and scaled data can be clustered correctly.\n- Tree Based Algorithms : Normal distribution has a low impact on final score.\n- Train on train set, fit on test set\n\n- **Normalization** : \n    - Data is converted between 0 and 1\n    - Used when data is not normal distributed.\n    - Can be used with algorithms which doesnt require normality\n- **Standardization** :\n    - Data mean will be shifted around 0.\n    - Used when data is normal distributed.\n    - StandardScaler, MinMaxScaler or RobustScaler can be used.","9c7352a5":"## Hyperbolic Tangent (tanh)<br>\nFunction converges to -1 and +1.","0d867f92":"## Percentile Linearization<br>\nIf data rank is important, we can use this.","9cd00c26":"## Log function<br>\nFor only positive numbers.If negative exists, add same big number to all data. Example : log1p adds 1 to zero.","3f0caacd":"## Cube<br>\nIn case log cant be used because of negative numbers to decrease left skewness.","a01b55c2":"# Categorical Variables<br>\n- **One hot encoding** vs **get dummies** vs **Effect Coding**\n    - Feasible for only linear models\n    - Bad for tree based models if data sparsity ratio is high \n    - Not used for growing categories\n    - Computationally inefficient\n    - Suitable for online learning\n    - Effect coding performs better in linear models since coefficients are lower\n- Bin Counting\n    - Binning via conditional probabilities\n    - Used in case of high number of categories\n    - Effective in tree based algorithms\n    - Not suitable for online learning\n- Rare Categories\n    - Create back-off bins","5c03d538":"## Correlation Matrix<br>\n- Instead of correlation value, reasonability is important.<br>\n- Some correlations : [Tyler Vigen](http:\/\/www.tylervigen.com\/spurious-correlations)\n- np.corrcoef() is much more faster than df.corr() for high volume of data\n- High correlation features are bad for all machine learning models","d702a01e":"**Suburb:** Suburb\n\n**Address:** Address\n\n**Rooms:** Number of rooms\n\n**Price:** Price in Australian dollars\n\n**Method:**\n- S - property sold;\n- SP - property sold prior;\n- PI - property passed in;\n- PN - sold prior not disclosed;\n- SN - sold not disclosed;\n- NB - no bid;\n- VB - vendor bid;\n- W - withdrawn prior to auction;\n- SA - sold after auction;\n- SS - sold after auction price not disclosed.\n- N\/A - price or highest bid not available.\n\n**Type:**\n- br - bedroom(s);\n- h - house,cottage,villa, semi,terrace;\n- u - unit, duplex;\n- t - townhouse;\n- dev site - development site;\n- o res - other residential.\n\n**SellerG:** Real Estate Agent\n\n**Date:** Date sold\n\n**Distance:** Distance from CBD in Kilometres\n\n**Regionname:** General Region (West, North West, North, North east \u2026etc)\n\n**Propertycount:** Number of properties that exist in the suburb.\n\n**Bedroom2 :** Scraped # of Bedrooms (from different source)\n\n**Bathroom:** Number of Bathrooms\n\n**Car:** Number of carspots\n\n**Landsize:** Land Size in Metres\n\n**BuildingArea:** Building Size in Metres\n\n**YearBuilt:** Year the house was built\n\n**CouncilArea:** Governing council for the area","4ecaeabf":"# Skewed Data<br>\n- Actions for non normal distribution\n    - If data set consists of various different normal distributions, split data\n    - If data set consists of uniform data sets, try binning, discretization and convertion to categorical data\n    - If data set consists of one skewed distribution, use the methods below.\n    ","447bed75":"## TSNE and UMAP<br>\n- Graph based nonlinear feature elimination tool.\n- Used for visualization of data in 2d or 3d dimensions.\n- Calculate distance based on distance to neighbors.\n- UMAP is used in Google Streets\n- TSNE Parameters : Perplexity (btw 10 - 50) and learning rate (btw 10 - 200) are main hyper parameters\n- UMAP Parameters : number of neighbors (low means local, high means global importance)","bff12aaf":"## Cube Root<br>\nIn case log cant be used because of negative numbers to decrease right skewness.","fd7faca4":"## Variance Threshold","5312e3ce":"## Sigmoid Function<br>\nFunction converges between 0 and 1.","b2c9696a":"## PCA<br>\n- Linear Dimensionality Reduction Tool\n- Performs better with normalized and scaled data\n- Good for low-dimensional linear relational data\n- Stucks at non linear relational data\n- Can be used for extremely fast and easy image recognition\n- Number of components is unknown. Generally %80 - %90 explained variance ratio is good","95745ba2":"## Feature Selection via Models<br>\n- Select From Model : Select via feature_importance_ of models.\n- Select Percentile : Univariate feature analysis and selection of percentile of features.\n- RFE : Iterative selection of features via models.","d5afb694":"## Box Cox Transformation<br>\n- Generalized solution of normalization via power of data.\n- Selects power which log normalize data best."}}