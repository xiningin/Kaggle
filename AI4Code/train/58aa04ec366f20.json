{"cell_type":{"97ecc8a9":"code","3e14730a":"code","629f485e":"code","053ed284":"code","eb76d121":"code","738f8bc6":"code","6294d1b8":"code","5e6c95c1":"code","7190f3ae":"code","6adda82a":"code","e17f9512":"code","68958c33":"code","216c7bf2":"code","a6393196":"code","e08f952d":"code","197df086":"markdown","d4a988da":"markdown","d8370e91":"markdown","1e070331":"markdown","4c6302ff":"markdown"},"source":{"97ecc8a9":"debug = False\nif debug: \n    STEPS = 200\n    val_steps = 10\nelse:\n    STEPS = 800\n    val_steps = 100\n    \nSTROKE_COUNT = 100\nEPOCHS = 45\nbatchsize = 1000\n","3e14730a":"%matplotlib inline\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nfrom keras.metrics import top_k_categorical_accuracy\ndef top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom glob import glob\nimport gc\ngc.enable()\ndef get_available_gpus():\n    from tensorflow.python.client import device_lib\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n","629f485e":"def preds2catids(predictions):\n    return pd.DataFrame(np.argsort(-predictions, axis=1)[:, :3], columns=['a', 'b', 'c'])\n\ndef top_3_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)","053ed284":"from ast import literal_eval\n\ndef _stack_it(raw_strokes):\n    \"\"\"preprocess the string and make \n    a standard Nx3 stroke vector\"\"\"\n    stroke_vec = literal_eval(raw_strokes) # string->list\n    # unwrap the list\n    in_strokes = [(xi,yi,i)  \n     for i,(x,y) in enumerate(stroke_vec) \n     for xi,yi in zip(x,y)]\n    c_strokes = np.stack(in_strokes)\n    # replace stroke id with 1 for continue, 2 for new\n    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n    c_strokes[:,2] += 1 # since 0 is no stroke\n    # pad the strokes with zeros\n    return pad_sequences(c_strokes.swapaxes(0, 1), \n                         maxlen=STROKE_COUNT, \n                         padding='post').swapaxes(0, 1)\n","eb76d121":"DP_DIR = '..\/input\/shuffle-csv-50k'\nINPUT_DIR = '..\/input\/quickdraw-doodle-recognition'\nBASE_SIZE = 256\nNCSVS = 100\nNCATS = 340\nnp.random.seed(seed=1987)\ntf.set_random_seed(seed=1987)\n\ndef f2cat(filename: str) -> str:\n    return filename.split('.')[0]\n\ndef list_all_categories():\n    files = os.listdir(os.path.join(INPUT_DIR, 'train_simplified'))\n    return sorted([f2cat(f) for f in files], key=str.lower)\n\n\n","738f8bc6":"def image_generator_xd( batchsize, ks):\n    while True:\n        for k in np.random.permutation(ks):\n            filename = os.path.join(DP_DIR, 'train_k{}.csv.gz.gz'.format(k))\n            for df in pd.read_csv(filename, chunksize=batchsize):\n                \n                df['drawing'] = df['drawing'].map(_stack_it)\n                x2 = np.stack(df['drawing'], 0)\n                y = keras.utils.to_categorical(df.y, num_classes=NCATS)\n                yield x2, y\n\ndef df_to_image_array_xd(df):\n    df['drawing'] = df['drawing'].map(_stack_it)\n    x2 = np.stack(df['drawing'], 0)\n    return x2","6294d1b8":"train_datagen = image_generator_xd(batchsize=batchsize, ks=range(NCSVS - 2))\nval_datagen = image_generator_xd(batchsize=batchsize, ks=range(NCSVS - 2, NCSVS))","5e6c95c1":"from keras.models import Sequential\nfrom keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout, Bidirectional\n#if len(get_available_gpus())>0:\n    # https:\/\/twitter.com\/fchollet\/status\/918170264608817152?lang=en\n#    from keras.layers import CuDNNLSTM as LSTM # this one is about 3x faster on GPU instances\nstroke_read_model = Sequential()\nstroke_read_model.add(BatchNormalization(input_shape = (None,)+(3,)))\n# filter count and length are taken from the script https:\/\/github.com\/tensorflow\/models\/blob\/master\/tutorials\/rnn\/quickdraw\/train_model.py\nstroke_read_model.add(Conv1D(256, (5,), activation = 'relu'))\nstroke_read_model.add(Dropout(0.2))\nstroke_read_model.add(Conv1D(256, (5,), activation = 'relu'))\nstroke_read_model.add(Dropout(0.2))\nstroke_read_model.add(Conv1D(256, (3,), activation = 'relu'))\nstroke_read_model.add(Dropout(0.2))\nstroke_read_model.add(Bidirectional(LSTM(128, dropout = 0.3, recurrent_dropout= 0.3,  return_sequences = True)))\nstroke_read_model.add(Bidirectional(LSTM(128,dropout = 0.3, recurrent_dropout= 0.3, return_sequences = True)))\nstroke_read_model.add(Bidirectional(LSTM(128,dropout = 0.3, recurrent_dropout= 0.3, return_sequences = False)))\nstroke_read_model.add(Dense(512, activation = 'relu'))\nstroke_read_model.add(Dropout(0.2))\nstroke_read_model.add(Dense(NCATS, activation = 'softmax'))\nstroke_read_model.compile(optimizer = 'adam', \n                          loss = 'categorical_crossentropy', \n                          metrics = ['categorical_accuracy', top_3_accuracy])\nstroke_read_model.summary()","7190f3ae":"weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_bidirectional_relu')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=4, \n                                   verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=3) \ncallbacks_list = [checkpoint, early, reduceLROnPlat]","6adda82a":"# Change the number of epochs to 20\n\n\nfrom IPython.display import clear_output\nhist = stroke_read_model.fit_generator(train_datagen, steps_per_epoch=STEPS, epochs=EPOCHS, verbose=1,\n                        validation_data=val_datagen, validation_steps = val_steps,\n                      callbacks = callbacks_list)\nclear_output()","e17f9512":"hist_df = pd.DataFrame(hist.history) \nhist_df.to_csv('hist_training.csv')\nhist_df.index = np.arange(1, len(hist_df)+1)\nfig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\naxs[0].plot(hist_df.val_top_3_accuracy, lw=5, label='Validation Accuracy')\naxs[0].plot(hist_df.top_3_accuracy, lw=5, label='Training Accuracy')\naxs[0].set_ylabel('Accuracy')\naxs[0].set_xlabel('Epoch')\naxs[0].grid()\naxs[0].legend(loc=0)\naxs[1].plot(hist_df.val_loss, lw=5, label='Validation MLogLoss')\naxs[1].plot(hist_df.loss, lw=5, label='Training MLogLoss')\naxs[1].set_ylabel('MLogLoss')\naxs[1].set_xlabel('Epoch')\naxs[1].grid()\naxs[1].legend(loc=0)\nfig.savefig('hist.png', dpi=300)\nplt.show();","68958c33":"valid_df = pd.read_csv(os.path.join(DP_DIR, 'train_k{}.csv.gz.gz'.format(NCSVS - 1)), nrows=34000)\nx_valid = df_to_image_array_xd(valid_df)\ny_valid = keras.utils.to_categorical(valid_df.y, num_classes=NCATS)\nlstm_results = stroke_read_model.evaluate(x_valid, y_valid, batch_size = 4096)\nprint('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))\n","216c7bf2":"sub_df = pd.read_csv(os.path.join(INPUT_DIR, 'test_simplified.csv'))\nsub_df['drawing'] = sub_df['drawing'].map(_stack_it)\nsub_vec = np.stack(sub_df['drawing'].values, 0)\nsub_pred = stroke_read_model.predict(sub_vec, verbose=True, batch_size=4096)\n","a6393196":"top3 = preds2catids(sub_pred)\ncats = list_all_categories()\nid2cat = {k: cat.replace(' ', '_') for k, cat in enumerate(cats)}\ntop3cats = top3.replace(id2cat)\nsub_df['word'] = top3cats['a'] + ' ' + top3cats['b'] + ' ' + top3cats['c']\nsub_df.head()","e08f952d":"sub_df[['key_id', 'word']].to_csv('lstm_relu_datagen.csv', index=False)","197df086":"# Bidirectional LSTM","d4a988da":"\nThis notebook is a combination of the data generator from Beluga's notebook: https:\/\/www.kaggle.com\/gaborfodor\/greyscale-mobilenet-lb-0-892 and largely based on Kevin Mader's LSTM code, with modifications in the network architecture https:\/\/www.kaggle.com\/kmader\/quickdraw-baseline-lstm-reading-and-submission. \n\nI am grateful for their contributions and can take little credit for this notebook. Running this notebook should achieve 0.823 on the LB. ","d8370e91":"# Submission\n","1e070331":"### LSTM to Parse Strokes\nThe model suggeted from the tutorial is\n\n![Suggested Model](https:\/\/www.tensorflow.org\/versions\/master\/images\/quickdraw_model.png)","4c6302ff":"### Stroke-based Classification\nHere we use the stroke information to train a model and see if the strokes give us a better idea of what the shape could be. "}}