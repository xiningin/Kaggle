{"cell_type":{"ec1c6efe":"code","df181247":"code","c473813c":"code","52b34791":"code","f568cee9":"code","48f79f5c":"code","8ba8c18b":"code","3d088ae9":"code","ad6cb5d3":"code","ece68e37":"code","2af9965e":"code","06e4bfea":"code","bc56bc75":"code","903d1a72":"code","26e83cd6":"code","00ce9eba":"code","f602f83a":"code","0892961d":"code","0984f414":"code","2a383b3d":"code","a46eba0f":"markdown","f1e7e1da":"markdown","08fe8047":"markdown","1b49544c":"markdown","ca69cdcd":"markdown","8101926e":"markdown","e33971d0":"markdown","36be4ff7":"markdown","a7b281b2":"markdown","f4fde7ae":"markdown","25b6a640":"markdown"},"source":{"ec1c6efe":"\nimport numpy as np \nimport pandas as pd \n# Change Label To 10 NumpyArry\nfrom keras.utils import np_utils\n# For Neural Netword \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import SGD\nfrom keras.losses import categorical_crossentropy\n","df181247":"train = pd.read_csv('..\/input\/mnist-in-csv\/mnist_train.csv')","c473813c":"test = pd.read_csv('..\/input\/mnist-in-csv\/mnist_test.csv')","52b34791":"y_train = train.iloc[:, 0]","f568cee9":"y_test = test.iloc[:, 0]","48f79f5c":"train.drop('label', axis='columns', inplace=True)","8ba8c18b":"test.drop('label', axis='columns', inplace=True)","3d088ae9":"x_test = test.iloc[:, :]","ad6cb5d3":"x_train = train.iloc[:, :]","ece68e37":"x_test = x_test.astype('float32')","2af9965e":"x_train = x_train.astype('float32')","06e4bfea":"x_train \/= 255\nx_test \/= 255","bc56bc75":"y_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)","903d1a72":"myModel = Sequential()\n# just input layer need input_shape\nmyModel.add(Dense(400, activation='relu', input_shape=(784,)))\nmyModel.add(Dropout(0.2))\nmyModel.add(Dense(300, activation='relu'))\n#myModel.add(Dropout(0.2))\nmyModel.add(Dense(300, activation='relu'))\n#myModel.add(Dropout(20))\nmyModel.add(Dense(10, activation='softmax'))\n\nmyModel.summary()\nmyModel.compile(optimizer=SGD(lr=0.001), loss=categorical_crossentropy, metrics=['accuracy'])\n# for Reduce The Overfiting Use Dropout","26e83cd6":"result_nerual = myModel.fit(x_train, y_train, batch_size=128, epochs=25, validation_split=0.2)","00ce9eba":"predict = myModel.predict(x_test)\n","f602f83a":"predict = np.argmax(predict, axis=1)","0892961d":"import matplotlib.pyplot as plt","0984f414":"history = result_nerual.history","2a383b3d":"loss = history['loss']\nval_loss = history['val_loss']\nplt.plot(loss)\nplt.plot(val_loss)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['loss', 'val_loss'])\nplt.figure()\n\naccuracy = history['accuracy']\nval_accuracy = history['val_accuracy']\nplt.plot(accuracy)\nplt.plot(val_accuracy)\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend(['accuracy', 'val_acuracy'])\nplt.legend()","a46eba0f":"# Visualize ","f1e7e1da":"# Change To 10 Part Of Number Not Just 0, 8, 9 ","08fe8047":"# Dou You Know How Calculate param 235500? \n## Dont Worry I Explain It As Clear As I Can \n## Our Input Are 768 So Multiply On Next Neural Which Is 300 , And Each Neural Has Bias Finally ==> 768*300+300 = 235500\n## Next Hidden Layer 300*300+300 = 90300\n## I Hope You Undrestand How This Param Work","1b49544c":"# -----------------------------Done-----------------------------------\n## Fit Our Model And See loss Must Be Near To 0 And Accuracy Must Near To 1","ca69cdcd":"# Task1: Load Our Data Into Train And Test","8101926e":"# If See The Result Of Predict It's Not Undrestandable So We Use argmax To Return It As It Before Like 0, 8, 7, ","e33971d0":"# If You Look At The Data See The Both Of Theme Has label Columns We Seprate It Into y_train, y_test","36be4ff7":"# Normalize:","a7b281b2":"# For Normalize The Data Our Type Must Be Float Not Int","f4fde7ae":"### Befor We Go Down Some Option Exists For Loss\n### If You Use ** sparse_categorical_crossentropy ** We must Have Sparse Label (For Each Instance, There Is Just A Target Class Index From 0 To 9\n### So If You Want Use this loss You Dont Need To Use np_utils.to_categorical\n### If We Were Doing Binary Classification loss Must Be ** binary_crossentropy **\n### For One Probability Per class For Each Instance Use ** categorical_crossentropy **","25b6a640":"# TASK2: Creat Model:"}}