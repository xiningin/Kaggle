{"cell_type":{"eb22058d":"code","8649ad55":"code","b4cbf5e6":"code","760993ba":"code","7243674f":"code","87752e80":"code","d72077ec":"code","1c3019dd":"code","0094b85e":"code","e11a5f46":"code","907e2af1":"code","a4084c8c":"code","60d74e28":"code","9ba41154":"code","e1404532":"code","4f97c17d":"code","4a944950":"code","bd18129b":"code","4f20af53":"code","c42fc4de":"code","9cdd0160":"code","8b0086d1":"code","52f0e562":"code","6bbbd477":"code","708ee3ae":"code","2fb9e752":"code","95ec2b39":"code","97608836":"code","a1ea0700":"code","d0cf4571":"code","4262e174":"code","cdbf57ef":"code","33f16ed5":"markdown","0f0d6274":"markdown","8c03e6a4":"markdown","107ce173":"markdown","9bd80477":"markdown","d79b50d0":"markdown","755e5389":"markdown","67fa6edd":"markdown","585c4601":"markdown","a6ad839b":"markdown"},"source":{"eb22058d":"!pip install torch-lr-finder","8649ad55":"import os\nimport torch\nimport torchaudio\nimport torchaudio.transforms as T\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.dataset import Dataset\n%matplotlib inline\n\nimport re\nfrom datetime import datetime\nfrom itertools import zip_longest\nfrom tqdm import tqdm\n\nimport torch.optim as optim\nfrom torch_lr_finder import LRFinder\nfrom torch.optim import lr_scheduler","b4cbf5e6":"torch.manual_seed(0)","760993ba":"audio_file = '\/kaggle\/input\/TH_S01E01.wav'\nos.listdir('\/kaggle\/input\/')","7243674f":"waveform, sample_rate = torchaudio.load(audio_file)\nduration_min = waveform.size()[1] \/ sample_rate \/ 1000 \/ 1000 \/ 60\n\nprint(\"duration_min\", duration_min)\nprint(\"Shape of waveform: {}\".format(waveform.size()))\nprint(\"Sample rate of waveform: {}\".format(sample_rate))","87752e80":"fft_length = 10 # how many ms goes into a single line of pixels\nspectrogram = T.Spectrogram(win_length=16*fft_length)(waveform)\n\nplt.figure(figsize=(20, 20))\nplt.imshow(spectrogram.log2()[0,:,:2000].numpy())\nplt.figure(figsize=(20, 20))\nplt.imshow(spectrogram.log2()[0,:,1750:1751].t())\nspectrogram.log2().size()","d72077ec":"fft_list = spectrogram[0].t()\nfft_list.size()","1c3019dd":"def grouper(iterable, n, fillvalue=None):\n    \"Collect data into fixed-length chunks or blocks\"\n    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n    args = [iter(iterable)] * n\n    return zip_longest(*args, fillvalue=fillvalue)\n\ndef pessimistic_subs(times):\n    crop_by = 200\n    return [[start + crop_by, end - crop_by] for start, end in times if end - start > crop_by*2+100]","0094b85e":"# Pessimistic times\nsub_file_path = '\/kaggle\/input\/TerraceHouse_OpeningNewDoors_S01E01_ja.vtt'\nsub_file = open(sub_file_path, \"r\")\nlines = sub_file.read().split(\"\\n\")\ntimes_arr = [ int(datetime.strptime(time, '%H:%M:%S.%f').replace(year=1970).timestamp() * 1000) for line in lines if \"-->\" in line for time in re.findall('\\d\\d:\\d\\d:\\d\\d\\.\\d{3}', line) ]\nsub_times_speech = pessimistic_subs(grouper(times_arr, 2))\nsub_times_silence = pessimistic_subs(grouper([0] + times_arr[:-1], 2))","e11a5f46":"# Separate speech from silence\n\ntrain_speech_a = []\ntrain_silence_a = []\ntrain_leftover_a = []\n\nfor fft_i in tqdm(range(len(fft_list)), desc=\"Matching fft to sub times\"):\n    fft_ms = fft_i * fft_length\n    added = 0\n    found_start = -1\n    fft_item = fft_list[fft_i].numpy()\n    \n    # FIX: This doesn't train on silence..\n    if fft_ms < times_arr[0]:\n        train_silence_a.append(fft_item)\n        continue\n        \n    for sub_start, sub_end in sub_times_speech:\n        if fft_ms > sub_start and fft_ms < sub_end:\n            added += 1\n            found_start = sub_start\n            train_speech_a.append(fft_item)\n            break\n            \n    for sub_start, sub_end in sub_times_silence:\n        if fft_ms > sub_start and fft_ms < sub_end:\n            added += 1\n            found_start = sub_start\n            train_silence_a.append(fft_item)\n            break\n            \n    if added == 0:\n        train_leftover_a.append(fft_item)\n            \n    assert added <= 1, \"Should only add once. added: \" + str(added) + \", index: \" + str(fft_i)\n\nprint(\"Speech FFT count:\", len(train_speech_a))\nprint(\"Silence FFT count:\", len(train_silence_a))\nprint(\"Leftover FFT count:\", len(train_leftover_a))","907e2af1":"class MyDataset(Dataset):\n    def __init__(self, speech, silence):\n        self.data = list(map(lambda x: (x, torch.tensor(0)), silence)) + list(map(lambda x: (x, torch.tensor(1)), speech))\n        \n    def __getitem__(self, index):\n        return self.data[index]\n    \n    def __len__(self):\n        return len(self.data)","a4084c8c":"batch_size = 1024\n\nwhole_ds = MyDataset(train_speech_a, train_silence_a)\n\nvalid_size_perc = 0.2\ntrain_size = int(len(whole_ds) * (1 - valid_size_perc))\nvalid_size = int(len(whole_ds) - train_size)\n\ntrain_ds, valid_ds = torch.utils.data.random_split(whole_ds, [train_size, valid_size])\ntest_ds = torch.tensor(train_leftover_a)\n\ntrain_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size, pin_memory=True)\ntrain_dl_normal = DataLoader(train_ds, shuffle=True, batch_size=batch_size, pin_memory=True)\nvalid_dl = DataLoader(valid_ds, shuffle=False, batch_size=batch_size, pin_memory=True, drop_last=True)\ntest_dl = DataLoader(test_ds, shuffle=False, batch_size=batch_size, pin_memory=True, drop_last=True)\n\nprint(\"Train dataset size:\", len(train_ds))\nprint(\"Valid dataset size:\", len(valid_ds))\nprint(\"Test dataset size:\", len(test_ds))","60d74e28":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)\n\ndevice = get_default_device()\ndevice","9ba41154":"train_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)\ntest_dl = DeviceDataLoader(test_dl, device)","e1404532":"class Net(nn.Module):\n    \n    def __init__(self):\n        super(Net, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(201, 400),\n            nn.Linear(400, 200),\n            nn.Linear(200, 50),\n            nn.Linear(50, 2)\n        )\n\n    def forward(self, x):\n        return self.model(x)","4f97c17d":"def accuracy(out, targets):\n    targets_indexes = targets.cpu().data.numpy()\n    out_indexes = torch.max(out, -1, True)[1].cpu().data.numpy()\n    assert len(targets_indexes) == len(out_indexes)\n    \n    hits = []\n    for i in range(len(targets_indexes)):\n        out_item = out_indexes[i].item()\n#         print(\"out\", out_item, \"actual\", targets_indexes[i])\n        hits.append(targets_indexes[i] == out_item)\n            \n    return (hits, out_indexes)","4a944950":"net = Net()\ncriterion_lr = nn.CrossEntropyLoss()\noptimizer_lr = optim.Adam(net.parameters(), lr=1e-7)\nlr_finder = LRFinder(net, optimizer_lr, criterion_lr, device=device)\nlr_finder.range_test(train_dl_normal, end_lr=100, num_iter=100)\nlr_finder.plot() # to inspect the loss-learning rate graph\nlr_finder.reset() # to reset the model and optimizer to their initial state","bd18129b":"epochs = 100","4f20af53":"net = Net()\nnet = to_device(net, device)","c42fc4de":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=1e-5)\n# cycle_lr = lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, total_steps=epochs*len(train_dl), cycle_momentum=False)\n# cycle_lr = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.99 ** epoch)\n# cycle_lr = lr_scheduler.ExponentialLR(optimizer, gamma=0.98)","9cdd0160":"loss_history = []\nvalid_history = []\nvalid_acc_history = []\nlr_history = []\nsilence_count_history = []\nspeech_count_history = []","8b0086d1":"for epoch in tqdm(range(epochs)):\n    train_loss_epoch = []\n    valid_loss_epoch = []\n    valid_acc_epoch = []\n    lr_epoch = []\n    silence_count_epoch = []\n    speech_count_epoch = []\n    net.train(True)\n    for (inputs, targets) in train_dl:\n        lr_epoch.append(optimizer.param_groups[0]['lr'])\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n#         cycle_lr.step()\n        train_loss_epoch.append(loss.item())\n    net.train(False)\n    for (inputs, targets) in valid_dl:\n        out = net(inputs)\n        loss = criterion(out, targets)\n        valid_loss_epoch.append(loss.item())\n        hits, out_indexes = accuracy(out, targets)\n        silence_count_epoch.append((np.array(out_indexes) == False).sum())\n        speech_count_epoch.append((np.array(out_indexes) == True).sum())\n        valid_acc_epoch.append(hits)\n    loss_history.append(np.average(train_loss_epoch))\n    valid_history.append(np.average(valid_loss_epoch))\n    valid_acc_flat = np.array(valid_acc_epoch).flatten().flatten()\n    valid_acc_history.append(np.average(valid_acc_flat))\n    lr_history.append(np.average(lr_epoch))\n    silence_count_history.append(np.sum(silence_count_epoch))\n    speech_count_history.append(np.sum(speech_count_epoch))","52f0e562":"plt.figure(figsize=(9,6))\nplt.plot(lr_history)","6bbbd477":"plt.figure(figsize=(9,6))\nplt.plot(silence_count_history)\n# plt.plot(speech_count_history)","708ee3ae":"plt.figure(figsize=(9,6))\n# plt.ylim([0,1])\nplt.plot(loss_history, label=\"train loss\")\nplt.plot(valid_history, label=\"validation loss\")\nplt.plot(valid_acc_history, label=\"validation accuracy\")\nplt.legend()\nplt.show()","2fb9e752":"result = net(test_ds.to(device))\nprint_count = 10\nskip = int(len(result)\/print_count)\nfor i in range(print_count):\n    print(result[i*skip])","95ec2b39":"preds = net(fft_list.to(device))\npreds[:10]","97608836":"def gen_timecodes(preds):\n    detections = torch.max(preds, -1, True)[1].cpu().data.numpy()\n    timecodes = []\n    started = None\n    for i in range(len(detections)):\n        detection = detections[i].item()\n        ms = fft_length * i\n        if started == None and detection == 1:\n            started = ms\n        elif started != None and detection == 0:\n            timecodes.append([started, ms])\n            started = None\n    return timecodes\n\ntimecodes = gen_timecodes(preds)\ntimecodes[:10]","a1ea0700":"def gen_vtt(times, text):\n    vtt_str = \"\"\"WEBVTT\n\nNOTE Netflix\nNOTE Profile: webvtt-lssdh-ios8\nNOTE Date: 2018\/01\/29 18:46:46\n\nNOTE SegmentIndex\nNOTE Segment=596.554 16670@498 125\nNOTE Segment=597.597 20459@17168 159\nNOTE Segment=596.471 23265@37627 178\nNOTE Segment=596.137 24518@60892 185\nNOTE Segment=122.539 4791@85410 37\nNOTE \/SegmentIndex\n\n\n\n\"\"\"\n\n    for i, [start_ms, end_ms] in enumerate(times, 1):\n        start_time = datetime.fromtimestamp(start_ms\/1000.0).strftime('%H:%M:%S.%f')[:-3]\n        end_time = datetime.fromtimestamp(end_ms\/1000.0).strftime('%H:%M:%S.%f')[:-3]\n        vtt_str += f\"{i}\\n{start_time} --> {end_time}\\n{text}\\n\\n\"\n        \n    return vtt_str","d0cf4571":"print(gen_vtt(timecodes, \"AImazing\"))","4262e174":"print(gen_vtt(sub_times_speech, \"speech\"))","cdbf57ef":"print(gen_vtt(sub_times_silence, \"silence\"))","33f16ed5":"# Export a .vtt file with results","0f0d6274":"# Turn wav into an array of FFT's","8c03e6a4":"# What learning rate to use?","107ce173":"# Divide FFT's to training data","9bd80477":"# Training functions","d79b50d0":"# Generate timings for whole episode","755e5389":"### Goal: Improve subtitle timing.\n\n### Training data:\n\n- Common Voices\n- An episode of TH\n- Subtitles of the episode\n\n### Validation data:\n\n- Episode chooped up into spectrogram strips\n\n### Approaches:\n\n1. Supervised: take Common Voices and a lot of random noises to make two classes.\n2. Unsupervised: Try different unsupervised algorithms to cluster speech\/non-speech\n3. Supervised: Use existing subtitle timings of an episode to make two classes. Sub times need to be adjusted towards pessimistic.\n\n### Papers with code for unsupervised learning:\n\n- [basic clustering](https:\/\/github.com\/rusty1s\/pytorch_cluster) no self-learning\n- [basic kmeans](https:\/\/www.kernel-operations.io\/keops\/_auto_tutorials\/kmeans\/plot_kmeans_torch.html) no self-learning\n- [Deep Clustering with Convolutional Autoencoders (2018)](https:\/\/github.com\/michaal94\/torch_DCEC)\n- [Unsupervised Deep Embedding for Clustering Analysis (2018)](https:\/\/github.com\/Deepayan137\/DeepClustering)\n- [deep-clustering-toolbox (2019)](https:\/\/github.com\/jizongFox\/deep-clustering-toolbox) no usage guide\n- [Deep Clustering for Unsupervised Learning of Visual Features (2018)](https:\/\/github.com\/facebookresearch\/deepcluster) very old versoins\n- [Deep Clustering Discriminative Embeddings for Segmentation and Separation (2018)](https:\/\/github.com\/funcwj\/deep-clustering)\n- [L2C: Learning to Cluster (2018)](https:\/\/github.com\/GT-RIPL\/L2C)\n\nAnd a bunch more papers here: https:\/\/github.com\/zhoushengisnoob\/DeepClustering","67fa6edd":"# Actual training","585c4601":"# Move to GPU","a6ad839b":"# Prepare modules"}}