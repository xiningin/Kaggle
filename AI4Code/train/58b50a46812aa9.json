{"cell_type":{"5f44bf73":"code","232d88f5":"code","631f796e":"code","16700934":"code","08c6996e":"code","486d8f80":"code","3d777798":"code","d1f97acf":"code","84b8a7a6":"code","e3604e26":"code","20f2783b":"code","f6d54739":"code","a03ac96c":"code","d3a80c5b":"code","941d9501":"code","84456022":"code","d6e2d92f":"code","2d38efb0":"code","2b09541f":"code","bbb1f0f5":"code","a4fe6249":"code","3c35cb5d":"code","840b8abd":"code","83ba22f9":"code","94be72e0":"code","5e2bd53c":"code","00a1e724":"code","b9eb7780":"code","4d1d73a0":"code","3d78be1a":"code","a85348cf":"code","48d2ce3e":"code","27e66399":"code","0081e9db":"code","33d38fb2":"code","4e57724b":"code","e9660264":"code","3423c814":"code","a3ffd79d":"code","dd56238a":"code","d02bde97":"code","370810fa":"code","c653765b":"code","6f8339ef":"code","d9214346":"code","b8dc43f9":"code","a27964cc":"code","b859708d":"code","3e69031c":"code","d6cd59df":"code","682beba1":"code","f918caaf":"code","2ea894b1":"code","cfb5338f":"code","2dee7d81":"code","5097b3f4":"code","0011930c":"code","74ef0e56":"code","ee335183":"code","1121f18a":"code","05ac0324":"code","ae8237e5":"code","c2bdc0c7":"code","f4f5bf8c":"code","c1a1faa5":"code","b264543b":"code","d4168bc8":"code","19c3409e":"code","1a0edf49":"code","a44afce9":"code","9e839f11":"code","47af26e4":"code","c9718ecd":"code","20ffcd81":"code","01dc7d44":"code","93eb2900":"code","c0af1b36":"code","95b91ee2":"code","7ec4af5d":"code","04927e50":"code","26e39f21":"code","07033ff1":"code","0e6bac63":"code","9f022baf":"code","6a1f0492":"code","5a230110":"code","7ddac760":"code","0365a28f":"code","d40fba98":"code","cd89ab02":"code","8b8b5629":"code","3b9c7200":"code","c84f6597":"markdown","42f6777e":"markdown","ac55c44c":"markdown","5ed5caa9":"markdown","9dc63104":"markdown","b7b23d95":"markdown","b19c229e":"markdown","66b7d39e":"markdown","d5c26314":"markdown","3e44f115":"markdown","dabcb866":"markdown","91493a26":"markdown","af92d514":"markdown","dcc0adeb":"markdown","e18ed8ad":"markdown","141855c1":"markdown","40361750":"markdown","b9a5f140":"markdown","5db6dadf":"markdown","9e23596f":"markdown","785a8fd2":"markdown","6ee8d900":"markdown","b0d4ca5d":"markdown","344b109d":"markdown","3d4d06b5":"markdown","d14c416a":"markdown","f4277860":"markdown","0a21c67b":"markdown","266f47af":"markdown","cb24445f":"markdown","851e5f67":"markdown","dffc6911":"markdown","c537ff96":"markdown","004c35ad":"markdown","cf533bb8":"markdown","1142eaad":"markdown","8fa84350":"markdown","64d4b4a2":"markdown","18e9b174":"markdown","b01d7b18":"markdown","60fec2fb":"markdown","636ecfa4":"markdown","27f0bc05":"markdown","aa53e849":"markdown","828c6288":"markdown","79479211":"markdown","8e256ad2":"markdown","cf8499b1":"markdown","e9b8b610":"markdown","882d6de3":"markdown","1a0571b2":"markdown","7c130438":"markdown","9db86f75":"markdown"},"source":{"5f44bf73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","232d88f5":"# Import the required libraries\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import recall_score, accuracy_score, confusion_matrix, f1_score\nfrom sklearn.metrics import precision_score, auc, roc_auc_score, roc_curve, precision_recall_curve\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# To display all the columns\npd.options.display.max_columns = None\n\n# To display all the rows\npd.options.display.max_rows = None\n\n# To map Empty Strings or numpy.inf as Na Values\npd.options.mode.use_inf_as_na = True\n\npd.options.display.expand_frame_repr =  False\n\n%matplotlib inline\n\n# Set Style\nsns.set(style = \"whitegrid\")","631f796e":"# train data\ntitanic_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', low_memory = False, skipinitialspace = True, float_precision = 2)\n\n# data glimpse\ntitanic_data.head()","16700934":"# test data\ntitanic_test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', low_memory = False, skipinitialspace = True, float_precision = 2)\n\n# data glimpse\ntitanic_test_data.head()","08c6996e":"print(\"Train Data Shape:\", titanic_data.shape)\nprint(\"Test Data Shape:\", titanic_test_data.shape)","486d8f80":"# train set columns\nprint(\"Columns in the train set :\")\nprint(titanic_data.columns)\n\nprint('\\n')\n\n# test set columns\nprint(\"Columns in the test set :\")\nprint(titanic_test_data.columns)","3d777798":"# the columns 'PassengerId' and 'Ticket' willl be of no value in the analysis.\n# Lets drop those columns from both the train ans test set.\n\n# train set\ntitanic_data.drop(columns = ['PassengerId', 'Ticket'], axis = 1, inplace = True)\n\n# test set\ntitanic_test_passengerId = titanic_test_data[['PassengerId']]\ntitanic_test_data.drop(columns = ['PassengerId', 'Ticket'], axis = 1, inplace = True)\n\n# data glimpse\ntitanic_data.head()","d1f97acf":"# Check the total missing values in each column.\nprint(\"Total NULL Values in each columns\")\nprint(\"*********************************\")\nprint(titanic_data.isnull().sum())","84b8a7a6":"# Lets check the percentage of missing values column-wise in the train data\n\n(titanic_data.isnull().sum()\/ len(titanic_data)) * 100","e3604e26":"# Lets check the percentage of missing values column-wise in the test data\n\n(titanic_test_data.isnull().sum()\/ len(titanic_test_data)) * 100","20f2783b":"# With more than 75% of values missing, we can drop the column - 'Cabin' but before doing that let us have a look \n# at it if we can get something out of almost nothing - \n\ntitanic_data.Cabin.value_counts()","f6d54739":"titanic_test_data.Cabin.value_counts()","a03ac96c":"# Observations from the above cell - \n# 1. We see above that there are about 147 unique values and some rows have multiple cabins allocated.\n\n# 2. Each cabin has 2 parts. Digging into the cabin numbers in the titanic, wikipedia shows the first letter\n#    in the cabin is the Deck and the number is the room number.\n\n# We will extract the deck information from the cabin\n\ntitanic_data[\"deck\"] = titanic_data[\"Cabin\"].str.slice(0,1)\n\ntitanic_test_data[\"deck\"] = titanic_test_data[\"Cabin\"].str.slice(0,1)\n\n# data glimpse\ntitanic_data.head()","d3a80c5b":"# Lets now check the value counts for the newly created column - 'deck'\n\nprint('Deck Value Counts for train set - ')\nprint(titanic_data.deck.value_counts())\n\nprint('\\n')\n\nprint('Deck Value Counts for test set - ')\nprint(titanic_test_data.deck.value_counts())","941d9501":"# Lets assign a default deck - say 'Z' for both the train and the test sets -\n\n# train set\ntitanic_data[\"deck\"] = titanic_data[\"deck\"].fillna(\"Z\")\n\n# also replace the single T-deck with G\ntitanic_data['deck'].replace(['T'], ['G'], inplace=True)\n\n# test set\ntitanic_test_data[\"deck\"] = titanic_test_data[\"deck\"].fillna(\"Z\")\n\n# data glimpse\ntitanic_data.head()","84456022":"# Now we have the deck information, We can drop the cabin column\n\n# train set\ntitanic_data.drop(columns = ['Cabin'], axis = 1, inplace = True)\n\n# test set\ntitanic_test_data.drop(columns = ['Cabin'], axis = 1, inplace = True)\n\n# data glimpse\ntitanic_data.head()","d6e2d92f":"# Let's have a look at the age values wrt -\n# 1. gender\n# 2. salutation (in the name column every person has a salutaion prefix. This can be a factor in\n# imputing the missing age values)\n\n# lets check the gender wise age distribution in the train set\n\nprint('Train Set')\nprint('Age of the oldest Passenger was:', titanic_data['Age'].max(),'Years')\nprint('Age of the youngest Passenger was:', titanic_data['Age'].min(),'Years')\nprint('Average Age on the ship:', titanic_data['Age'].mean(),'Years\\n')\n\nprint(titanic_data.groupby(by = ['Sex']).Age.describe())\n\nprint('\\n-----------------------------------------------------------------------------\\n')\nprint('Test Set')\n# lets check the gender wise age distribution in the test set\n\nprint('Age of the oldest Passenger was:', titanic_test_data['Age'].max(),'Years')\nprint('Age of the youngest Passenger was:', titanic_test_data['Age'].min(),'Years')\nprint('Average Age on the ship:', titanic_test_data['Age'].mean(),'Years\\n')\n\nprint(titanic_test_data.groupby(by = ['Sex']).Age.describe())\n","2d38efb0":"# 2. salutation (in the name column every person has a salutaion prefix. This can be a factor in\n# imputing the missing age values)\n\n# We will extract the salution from every person's name in the Name column.\n\nsalutation = titanic_data.Name.str.split(', ').str[1].str.split('.').str[0]\ntitanic_data['salutation'] = salutation\ntitanic_data.salutation.value_counts()","2b09541f":"# applying the above in the test set to extract the salutation there -\n\nsalutation = titanic_test_data.Name.str.split(', ').str[1].str.split('.').str[0]\ntitanic_test_data['salutation'] = salutation\ntitanic_test_data.salutation.value_counts()","bbb1f0f5":"# Salutations inputations in train data\n\ntitanic_data['salutation'].replace(['Mme','Lady','the Countess','Jonkheer'], ['Mrs','Mrs', 'Mrs','Mrs'], inplace=True)\n\ntitanic_data['salutation'].replace(['Mlle', 'Jonkheer', 'Ms' ], ['Miss','Miss', 'Miss'], inplace=True)\n\ntitanic_data['salutation'].replace(['Sir'], ['Mr'], inplace=True)\n\ntitanic_data['salutation'].replace(['Major', 'Col', 'Capt'], ['army_rank', 'army_rank', 'army_rank'], inplace=True)","a4fe6249":"# age distribution with respect to the name salutations\n\nprint(titanic_data.groupby(by = ['salutation']).Age.describe())","3c35cb5d":"# Salutations inputations in test data\n\ntitanic_test_data['salutation'].replace(['Mme','Lady','the Countess','Jonkheer'], ['Mrs','Mrs', 'Mrs','Mrs'], inplace=True)\n\ntitanic_test_data['salutation'].replace(['Mlle', 'Jonkheer', 'Ms' ], ['Miss','Miss', 'Miss'], inplace=True)\n\ntitanic_test_data['salutation'].replace(['Sir'], ['Mr'], inplace=True)\n\ntitanic_test_data['salutation'].replace(['Major', 'Col', 'Capt'], ['army_rank', 'army_rank', 'army_rank'], inplace=True)\n\ntitanic_test_data['salutation'].replace(['Dona'], ['Don'], inplace=True)","840b8abd":"# age distribution with respect to the name salutations\n\nprint(titanic_test_data.groupby(by = ['salutation']).Age.describe())","83ba22f9":"# Missing age value for each category - \n\nprint('Number of presons with salutation type Mr having missing age value - ', \n      titanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Mr')].shape[0])\n\nprint('Number of presons with salutation type Capt having missing age value - ', \n      titanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Capt')].shape[0])\n\nprint('Number of presons with salutation type Col having missing age value - ', \n      titanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Col')].shape[0])\n\nprint('Number of presons with salutation type Don having missing age value - ', \n      titanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Don')].shape[0])\n\nprint('Number of presons with salutation type Dr having missing age value - ', \n      titanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Dr')].shape[0])\n\nprint('Number of presons with salutation type Major having missing age value - ', \n      titanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Major')].shape[0])\n\nprint('Number of presons with salutation type Master having missing age value - ', \n      titanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Master')].shape[0])\n\nprint('Number of presons with salutation type Miss having missing age value - ', \n      titanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Miss')].shape[0])\n\nprint('Number of presons with salutation type Mrs having missing age value - ', \n      titanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Mrs')].shape[0])\n\nprint('Number of presons with salutation type Rev having missing age value - ', \n      titanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Rev')].shape[0])","94be72e0":"# the values (mean & median) are taken from the - 'age distribution with respect to the name salutations' cell above.\n\ntitanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Mr'),'Age'] = 30\ntitanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Dr'),'Age'] = 42\ntitanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Master'),'Age'] = 3.5\ntitanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Miss'),'Age'] = 21\ntitanic_data.loc[(titanic_data['Age'].isnull()) & (titanic_data['salutation'] == 'Mrs'),'Age'] = 35","5e2bd53c":"# Repeating the above for the test set as well -\n\n# Missing age value for each category - \n\nprint('Number of presons with salutation type Mr having missing age value - ', \n      titanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Mr')].shape[0])\n\nprint('Number of presons with salutation type Capt having missing age value - ', \n      titanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Capt')].shape[0])\n\nprint('Number of presons with salutation type Col having missing age value - ', \n      titanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Col')].shape[0])\n\nprint('Number of presons with salutation type Don having missing age value - ', \n      titanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Don')].shape[0])\n\nprint('Number of presons with salutation type Dr having missing age value - ', \n      titanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Dr')].shape[0])\n\nprint('Number of presons with salutation type Major having missing age value - ', \n      titanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Major')].shape[0])\n\nprint('Number of presons with salutation type Master having missing age value - ', \n      titanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Master')].shape[0])\n\nprint('Number of presons with salutation type Miss having missing age value - ', \n      titanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Miss')].shape[0])\n\nprint('Number of presons with salutation type Mrs having missing age value - ', \n      titanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Mrs')].shape[0])\n\nprint('Number of presons with salutation type Rev having missing age value - ', \n      titanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Rev')].shape[0])","00a1e724":"# the values (mean & median) are taken from the - 'age distribution with respect to the name salutations' cell above.\n\ntitanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Mr'),'Age'] = 28.5\ntitanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Master'),'Age'] = 7\ntitanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Miss'),'Age'] = 21.77\ntitanic_test_data.loc[(titanic_test_data['Age'].isnull()) & (titanic_test_data['salutation'] == 'Mrs'),'Age'] = 36.5","b9eb7780":"# Lets have a look at the percentage of null values in the data set now - \n\n(titanic_data.isnull().sum()\/ len(titanic_data)) * 100","4d1d73a0":"# Lets have a look at the percentage of null values in the data set now - \n\n(titanic_test_data.isnull().sum()\/ len(titanic_test_data)) * 100","3d78be1a":"# Now lets check on the missing values for the 'Embarked' column -\n\ntitanic_data.loc[(titanic_data['Embarked'].isnull())]","a85348cf":"titanic_data.loc[(titanic_data['Fare'] >= 79) & (titanic_data['Fare'] <= 82)]","48d2ce3e":"titanic_data.loc[(titanic_data['Embarked'].isnull()) & (titanic_data['Age'] == 38),'Embarked'] = 'S'\ntitanic_data.loc[(titanic_data['Embarked'].isnull()) & (titanic_data['Age'] == 62),'Embarked'] = 'C'","27e66399":"# Lets have a look at the percentage of null values in the train set now - \n\n(titanic_data.isnull().sum()\/ len(titanic_data)) * 100","0081e9db":"# the test set has some missing values in the fare column. lets examine that\n\ntitanic_test_data.Fare.describe()","33d38fb2":"# as mean > median value for Fare in test set, lets impute the missing value with median - \n\n# test set\ntitanic_test_data[\"Fare\"] = titanic_test_data[\"Fare\"].fillna(14.45)","4e57724b":"# Lets have a look at the percentage of null values in the test set now - \n\n(titanic_test_data.isnull().sum()\/ len(titanic_test_data)) * 100","e9660264":"# dropping the name column -\n\ntitanic_data.drop(['Name'], axis=1, inplace = True)\ntitanic_test_data.drop(['Name'], axis=1, inplace = True)","3423c814":"# data glimpse\n\ntitanic_data.head()","a3ffd79d":"# data glimpse\n\ntitanic_test_data.head()","dd56238a":"# Custom Function for Default Plotting variables\n\n# Function Parameters  - \n\n# figure_title         -    The title to use for the plot.\n# xlabel               -    The x-axis label for the plot.\n# ylabel               -    The y-axis label for the plot.\n# xlabel_rotation      -    The degree of rotation for the x-axis ticks (values).\n# legend_flag          -    Boolean flag to check if a legend is required to be their in the plot.\n# legend               -    Place legend on axis subplots.\n\ndef set_plotting_variable(figure_title, xlabel, ylabel, xlabel_rotation, legend_flag, legend):\n    \n    plt.title(figure_title)\n    plt.xticks(rotation = xlabel_rotation)\n    plt.xlabel(xlabel, labelpad = 15)\n    plt.ylabel(ylabel, labelpad = 10)\n    \n    if legend_flag == True:\n        plt.legend(loc = legend)","d02bde97":"# Function Parameters   -\n\n# category              -      The category of the variable in consideration - Categorical or Continuous.\n# plot_type             -      The type of the plot - Unordered Categorical (-lineplot) or Ordered Categorical (-countplot).\n# series                -      The series\/column from the data frame for which the univariate analysis is being considered for.\n# figsize_x             -      The width of the plot figure in inches.\n# figsize_y             -      The height of the plot figure in inches.\n# subplot_x             -      The rows for the subplot.\n# subplot_y             -      The columns for the subplot.\n# xlabel                -      The x-axis label for the plot.\n# ylabel                -      The y-axis label for the plot.\n# x_axis                -      The series\/variable to be plotted along the x-axis.\n# hue                   -      The variable (categorical) in the data for which the plot is being considered for.\n# data                  -      The data frame.\n# legend                -      Place legend on axis subplots.\n\n# hspace                -      The amount of height reserved for space between subplots,\n#                              expressed as a fraction of the average axis height\n\n# wspace                -      The amount of width reserved for space between subplots,\n#                              expressed as a fraction of the average axis width\n\n# xlabel_rotation       -      The degree of rotation for the x-axis ticks (values).\n\ndef plot_univariate(category, plot_type, series, figsize_x, figsize_y, subplot_x, subplot_y,\n                    xlabel, ylabel, x_axis, hue, data, legend, hspace, wspace, xlabel_rotation):\n    \n    plt.figure(figsize = (figsize_x, figsize_y))\n    \n    if category == 'Categorical':\n        \n        title_1 = \"Frequency Plot of \" + xlabel\n        title_2 = title_1 + \" across Survival Status\"\n        \n        # Subplot - 1\n        plt.subplot(subplot_x, subplot_y, 1)\n        \n        if plot_type == 'Unordered Categorical':\n            sns.lineplot(data = series)\n        \n        elif plot_type == 'Ordered Categorical':\n            sns.countplot(x = x_axis, order = series.sort_index().index, data = data)\n        \n        # Call Custom Function\n        set_plotting_variable(title_1, xlabel, ylabel, xlabel_rotation, False, legend)\n        \n        # Subplot - 2\n        plt.subplot(subplot_x, subplot_y, 2)\n        \n        sns.countplot(x = x_axis, hue = hue, order = series.sort_index().index, data = data)\n        # Call Custom Function\n        set_plotting_variable(title_2, xlabel, ylabel, xlabel_rotation, True, legend)\n    \n    elif category == 'Continuous':\n        \n        title_1 = \"Distribution Plot of \" + xlabel\n        title_2 = \"Box Plot of \" + xlabel\n        title_3 = title_2 + \" across Survival Status\"\n        \n        # Subplot - 1\n        plt.subplot(subplot_x, subplot_y, 1)\n        \n        sns.distplot(data[x_axis], hist = True, kde = True, color = 'g')\n        # Call Custom Function\n        set_plotting_variable(title_1, xlabel, ylabel, xlabel_rotation, False, legend)\n        \n        # Subplot - 2\n        plt.subplot(subplot_x, subplot_y, 2)\n        \n        sns.boxplot(x = x_axis, data = data, color = 'm')\n        # Call Custom Function\n        set_plotting_variable(title_2, xlabel, ylabel, xlabel_rotation, False, legend)\n           \n    plt.subplots_adjust(hspace = hspace)\n    plt.subplots_adjust(wspace = wspace)\n    plt.show()","370810fa":"series = titanic_data.Survived.value_counts(dropna = False)\n\nprint(series.sort_index())\nprint('\\n')\n\nplt.figure(figsize = (8, 6))\n\nplt.title('Frequency Plot of Persons Survived')\nsns.countplot(x = 'Survived',  \n              order = series.sort_index().index, \n              data = titanic_data)\nplt.xlabel('Survived Status', labelpad = 15)\nplt.ylabel('Frequency', labelpad = 10)\n\nplt.subplots_adjust(wspace = 0.6)\nplt.show()\n\n# Survived - 1 means the person survived and 0 means the person did not survive the Titanic mishap.","c653765b":"# Rank-Frequency Plot of Unordered Categorical Variable: Embarked\n\nprint('Embarked - ' + 'Port of Embarkation' + '\\n' \n      +'-------------------------------' + '\\n' + 'C = Cherbourg \\nQ = Queenstown \\nS = Southampton'  + '\\n'\n      +'-------------------------------')\n\nseries = titanic_data.Embarked.value_counts(dropna = False)\n\nprint(series.sort_index())\nprint('\\n')\nprint(titanic_data.groupby(by = 'Survived').Embarked.value_counts(dropna = False).sort_index())\nprint('\\n')\n\n# Call Custom Function\nplot_univariate(category = 'Categorical',\n                plot_type = 'Unordered Categorical',\n                series = series,\n                figsize_x = 15,\n                figsize_y = 6,\n                subplot_x = 1,\n                subplot_y = 2,\n                xlabel = \"Port of Embarkation\",\n                ylabel = \"Frequency\",\n                x_axis = 'Embarked',\n                hue = 'Survived',\n                data = titanic_data,\n                legend = 'upper center',\n                hspace = 0,\n                wspace = 0.3,\n                xlabel_rotation = 0)","6f8339ef":"series = titanic_data.salutation.value_counts(dropna = False)\n\nprint(series.sort_index())\nprint('\\n')\nprint(titanic_data.groupby(by = 'Survived').salutation.value_counts(dropna = False).sort_index())\nprint('\\n')\n\n# Call Custom Function\nplot_univariate(category = 'Categorical',\n                plot_type = 'Unordered Categorical',\n                series = series,\n                figsize_x = 15,\n                figsize_y = 6,\n                subplot_x = 1,\n                subplot_y = 2,\n                xlabel = \"Salutations\",\n                ylabel = \"Frequency\",\n                x_axis = 'salutation',\n                hue = 'Survived',\n                data = titanic_data,\n                legend = 'upper center',\n                hspace = 0,\n                wspace = 0.3,\n                xlabel_rotation = 0)","d9214346":"print('Pclass - ' + 'Passenger Class' + '\\n' \n      +'-------------------------------' + '\\n' + 'Ticket class \\n1 = 1st \\n2 = 2nd \\n3 = 3rd' + '\\n'\n      +'-------------------------------')\n\nseries = titanic_data.Pclass.value_counts(dropna = False)\n\nprint(series.sort_index())\nprint('\\n')\nprint(titanic_data.groupby(by = 'Survived').Pclass.value_counts(dropna = False).sort_index())\nprint('\\n')\n\n# Call Custom Function\nplot_univariate(category = 'Categorical',\n                plot_type = 'Unordered Categorical',\n                series = series,\n                figsize_x = 15,\n                figsize_y = 6,\n                subplot_x = 1,\n                subplot_y = 2,\n                xlabel = \"Passenger Class\",\n                ylabel = \"Frequency\",\n                x_axis = 'Pclass',\n                hue = 'Survived',\n                data = titanic_data,\n                legend = 'upper center',\n                hspace = 0,\n                wspace = 0.3,\n                xlabel_rotation = 0)","b8dc43f9":"series = titanic_data.Sex.value_counts(dropna = False)\n\nprint(series.sort_index())\nprint('\\n')\nprint(titanic_data.groupby(by = 'Survived').Sex.value_counts(dropna = False).sort_index())\nprint('\\n')\n\n# Call Custom Function\nplot_univariate(category = 'Categorical',\n                plot_type = 'Unordered Categorical',\n                series = series,\n                figsize_x = 15,\n                figsize_y = 6,\n                subplot_x = 1,\n                subplot_y = 2,\n                xlabel = \"Gender\",\n                ylabel = \"Frequency\",\n                x_axis = 'Sex',\n                hue = 'Survived',\n                data = titanic_data,\n                legend = 'upper center',\n                hspace = 0,\n                wspace = 0.3,\n                xlabel_rotation = 0)","a27964cc":"abc = titanic_data.groupby(['Sex', 'Survived']).Pclass.value_counts(dropna = False).sort_index()\nprint(abc)\n\nabc.unstack().plot()","b859708d":"print(titanic_data.Fare.describe())\nprint('\\n')\nprint(titanic_data.groupby(by = 'Survived').Fare.describe().sort_index())\nprint('\\n')\n\n# Call Custom Function\nplot_univariate(category = 'Continuous',\n                plot_type = 'Quantitative',\n                series = [1, 0],\n                figsize_x = 15,\n                figsize_y = 12,\n                subplot_x = 2,\n                subplot_y = 2,\n                xlabel = \"Fare\",\n                ylabel = \"Distribution\",\n                x_axis = 'Fare',\n                hue = 'Survived',\n                data = titanic_data,\n                legend = 'best',\n                hspace = 0.4,\n                wspace = 0.3,\n                xlabel_rotation = 0)","3e69031c":"f,sub_plt = plt.subplots(1,2,figsize=(20,10))\nx_bins = list(range(0,85,5))\n\n# Survived Status 0\ntitanic_data[titanic_data['Survived'] == 0].Age.plot.hist(ax=sub_plt[0], bins=20)\nsub_plt[0].set_title('Survived = 0')\nsub_plt[0].set_xticks(x_bins)\n\n# Survived Status 1\ntitanic_data[titanic_data['Survived'] == 1].Age.plot.hist(ax=sub_plt[1], bins=20)\nsub_plt[1].set_title('Survived = 1')\nsub_plt[1].set_xticks(x_bins)\n\nplt.show()","d6cd59df":"print(titanic_data.groupby(by = 'Survived').SibSp.value_counts(dropna = False).sort_index())\n\nseries = titanic_data.SibSp.value_counts(dropna = False)\n\nplot_univariate(category = 'Categorical',\n                plot_type = 'Unordered Categorical',\n                series = series,\n                figsize_x = 15,\n                figsize_y = 6,\n                subplot_x = 1,\n                subplot_y = 2,\n                xlabel = \"SibSp\",\n                ylabel = \"Frequency\",\n                x_axis = 'SibSp',\n                hue = 'Survived',\n                data = titanic_data,\n                legend = 'upper center',\n                hspace = 0,\n                wspace = 0.3,\n                xlabel_rotation = 0)","682beba1":"print(titanic_data.groupby(by = 'Survived').Parch.value_counts(dropna = False).sort_index())\n\nseries = titanic_data.Parch.value_counts(dropna = False)\n\nplot_univariate(category = 'Categorical',\n                plot_type = 'Unordered Categorical',\n                series = series,\n                figsize_x = 15,\n                figsize_y = 6,\n                subplot_x = 1,\n                subplot_y = 2,\n                xlabel = \"Parch\",\n                ylabel = \"Frequency\",\n                x_axis = 'Parch',\n                hue = 'Survived',\n                data = titanic_data,\n                legend = 'upper center',\n                hspace = 0,\n                wspace = 0.3,\n                xlabel_rotation = 0)","f918caaf":"# creation of the new feature - familysize\ntitanic_data['familysize'] = titanic_data['SibSp'] + titanic_data['Parch']\n\ntitanic_test_data['familysize'] = titanic_test_data['SibSp'] + titanic_test_data['Parch']\n\n# lets now drop thae columns 'SibSp' and 'Parch' as they would be correlated and predict the 'familysize'\n\n# train set\ntitanic_data.drop(columns = ['SibSp', 'Parch'], axis = 1, inplace = True)\n\n# test set\ntitanic_test_data.drop(columns = ['SibSp', 'Parch'], axis = 1, inplace = True)\n\n# data glimpse\ntitanic_data.head()","2ea894b1":"titanic_data['familysize'].describe()","cfb5338f":"print(titanic_data.groupby(by = 'Survived').familysize.value_counts(dropna = False).sort_index())\n\nseries = titanic_data.familysize.value_counts(dropna = False)\n\nplot_univariate(category = 'Categorical',\n                plot_type = 'Unordered Categorical',\n                series = series,\n                figsize_x = 15,\n                figsize_y = 6,\n                subplot_x = 1,\n                subplot_y = 2,\n                xlabel = \"familysize\",\n                ylabel = \"Frequency\",\n                x_axis = 'familysize',\n                hue = 'Survived',\n                data = titanic_data,\n                legend = 'upper center',\n                hspace = 0,\n                wspace = 0.3,\n                xlabel_rotation = 0)","2dee7d81":"# train set\n\ntitanic_data['age_category'] = 0\ntitanic_data.loc[titanic_data['Age'] <= 15,'age_category'] = 0\ntitanic_data.loc[(titanic_data['Age'] > 15) & (titanic_data['Age'] <= 30),'age_category'] = 1\ntitanic_data.loc[(titanic_data['Age'] > 30) & (titanic_data['Age'] <= 45),'age_category'] = 2\ntitanic_data.loc[(titanic_data['Age'] > 45) & (titanic_data['Age'] <= 60),'age_category'] = 3\ntitanic_data.loc[(titanic_data['Age'] > 60),'age_category'] = 4\n\n# dropping the 'Age' feature\ntitanic_data.drop(['Age'], axis = 1, inplace = True)\n\n# data glimpse\ntitanic_data.head()","5097b3f4":"# test set\n\ntitanic_test_data['age_category'] = 0\ntitanic_test_data.loc[titanic_test_data['Age'] <= 15,'age_category'] = 0\ntitanic_test_data.loc[(titanic_test_data['Age'] > 15) & (titanic_test_data['Age'] <= 30),'age_category'] = 1\ntitanic_test_data.loc[(titanic_test_data['Age'] > 30) & (titanic_test_data['Age'] <= 45),'age_category'] = 2\ntitanic_test_data.loc[(titanic_test_data['Age'] > 45) & (titanic_test_data['Age'] <= 60),'age_category'] = 3\ntitanic_test_data.loc[(titanic_test_data['Age'] > 60),'age_category'] = 4\n\n# dropping the 'Age' feature\ntitanic_test_data.drop(['Age'], axis = 1, inplace = True)\n\n# data glimpse\ntitanic_test_data.head()","0011930c":"titanic_data.Fare.describe(percentiles = [.20, .40, .60, .80])","74ef0e56":"# train set\n\ntitanic_data['fare_category'] = 0\ntitanic_data.loc[titanic_data['Fare'] <= 7.85,'fare_category'] = 0\ntitanic_data.loc[(titanic_data['Fare'] > 7.85) & (titanic_data['Fare'] <= 10.50),'fare_category'] = 1\ntitanic_data.loc[(titanic_data['Fare'] > 10.50) & (titanic_data['Fare'] <= 21.67),'fare_category'] = 2\ntitanic_data.loc[(titanic_data['Fare'] > 21.67) & (titanic_data['Fare'] <= 39.68),'fare_category'] = 3\ntitanic_data.loc[(titanic_data['Fare'] > 39.68),'fare_category'] = 4\n\n# dropping the 'Age' feature\ntitanic_data.drop(['Fare'], axis = 1, inplace = True)\n\n# data glimpse\ntitanic_data.head()","ee335183":"# test set\n\ntitanic_test_data['fare_category'] = 0\ntitanic_test_data.loc[titanic_test_data['Fare'] <= 7.85,'fare_category'] = 0\ntitanic_test_data.loc[(titanic_test_data['Fare'] > 7.85) & (titanic_test_data['Fare'] <= 10.50),'fare_category'] = 1\ntitanic_test_data.loc[(titanic_test_data['Fare'] > 10.50) & (titanic_test_data['Fare'] <= 21.67),'fare_category'] = 2\ntitanic_test_data.loc[(titanic_test_data['Fare'] > 21.67) & (titanic_test_data['Fare'] <= 39.68),'fare_category'] = 3\ntitanic_test_data.loc[(titanic_test_data['Fare'] > 39.68),'fare_category'] = 4\n\n# dropping the 'Age' feature\ntitanic_test_data.drop(['Fare'], axis = 1, inplace = True)\n\n# data glimpse\ntitanic_test_data.head()","1121f18a":"titanic_data['Sex'] = titanic_data.Sex.map({'female':0, 'male':1})\n\n# data glimpse\ntitanic_data.head()","05ac0324":"titanic_test_data['Sex'] = titanic_test_data.Sex.map({'female':0, 'male':1})\n\n# data glimpse\ntitanic_test_data.head()","ae8237e5":"# train data\n\n# Use get_dummies\ndummies_for_decks = pd.get_dummies(titanic_data['deck'], prefix = 'deck', drop_first = False)\n\n# data glimpse\ndummies_for_decks.head()","c2bdc0c7":"# lets drop the 'deck_T'column\ndummies_for_decks.drop(['deck_G'], axis = 1, inplace = True)\n\n# Merging to the master data frame \ntitanic_data = titanic_data.join(dummies_for_decks)\n\n# dropping the 'deck' column from the master frame\ntitanic_data.drop(['deck'], axis = 1, inplace = True)\n\n# data glimpse\ntitanic_data.head()","f4f5bf8c":"# test data\n\n# Use get_dummies\ndummies_for_decks = pd.get_dummies(titanic_test_data['deck'], prefix = 'deck', drop_first = False)\n\n# data glimpse\ndummies_for_decks.head()","c1a1faa5":"# lets drop the 'deck_T'column\ndummies_for_decks.drop(['deck_G'], axis = 1, inplace = True)\n\n# Merging to the master data frame \ntitanic_test_data = titanic_test_data.join(dummies_for_decks)\n\n# dropping the 'deck' column from the master frame\ntitanic_test_data.drop(['deck'], axis = 1, inplace = True)\n\n# data glimpse\ntitanic_test_data.head()","b264543b":"# train data\n\ntitanic_data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n\n#data glimpse\ntitanic_data.head()","d4168bc8":"# test data\n\ntitanic_test_data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n\n#data glimpse\ntitanic_test_data.head()","19c3409e":"# train data\n\n# we have a total of 8 different salutation present in the data as seen in the above cell\n# let's encode these to a number range 0 to 7 - \ntitanic_data['salutation'].replace(['Mr', 'Miss', 'Mrs', 'Master', 'Dr', 'Rev', 'army_rank', 'Don' ],\n                                   [0, 1, 2, 3, 4, 5, 6, 7], inplace=True)\n\n#data glimpse\ntitanic_data.head()","1a0edf49":"# test data\n\n# we have a total of 8 different salutation present in the data as seen in the above cell\n# let's encode these to a number range 0 to 7 - \ntitanic_test_data['salutation'].replace(['Mr', 'Miss', 'Mrs', 'Master', 'Dr', 'Rev', 'army_rank', 'Don' ],\n                                   [0, 1, 2, 3, 4, 5, 6, 7], inplace=True)\n\n#data glimpse\ntitanic_test_data.head()","a44afce9":"# Custom Function to get Scores and plots\ndef get_scores(scores, reg, X_test):\n    \n    # Plot ROC and PR curves using all models and test data\n    fig, axes = plt.subplots(1, 2, figsize = (14, 6))\n\n    pred_test = reg.predict(X_test.values)\n\n    pred_test_probs = reg.predict_proba(X_test.values)[:, 1:]\n\n    fpr, tpr, thresholds = roc_curve(y_test.values.ravel(), pred_test)\n    p, r, t = precision_recall_curve(y_test.values.ravel(), pred_test_probs)\n\n    model_f1_score = f1_score(y_test.values.ravel(), pred_test)\n    model_precision_score = precision_score(y_test.values.ravel(), pred_test)\n    model_recall_score = recall_score(y_test.values.ravel(), pred_test)\n    model_accuracy_score = accuracy_score(y_test.values.ravel(), pred_test)\n    model_auc_roc = auc(fpr, tpr)\n    model_auc_pr = auc(p, r, reorder = True)\n\n    scores.append((model_f1_score,\n                   model_precision_score,\n                   model_recall_score,\n                   model_accuracy_score,\n                   model_auc_roc,\n                   model_auc_pr,\n                   confusion_matrix(y_test.values.ravel(), pred_test)))\n\n    axes[0].plot(fpr, tpr, label = f\"auc_roc = {model_auc_roc:.3f}\")\n    axes[1].plot(r, p, label = f\"auc_pr = {model_auc_pr:.3f}\")\n\n    axes[0].plot([0, 1], [0, 1], 'k--')\n    axes[0].legend(loc = \"lower right\")\n    axes[0].set_xlabel(\"False Positive Rate\")\n    axes[0].set_ylabel(\"True Positive Rate\")\n    axes[0].set_title(\"AUC ROC curve\")\n\n    axes[1].legend(loc = \"lower right\")\n    axes[1].set_xlabel(\"recall\")\n    axes[1].set_ylabel(\"precision\")\n    axes[1].set_title(\"PR curve\")\n\n    plt.tight_layout()\n    plt.show()\n    \n    return scores","9e839f11":"# Custom Function for hyper parameter tuning\n\ndef tune_hyper_parameter(X, y, param_grid, model_type, ml = 'None'):\n   \n    gc = GridSearchCV(estimator = ml, param_grid = param_grid, scoring = 'roc_auc',\n                          n_jobs = 15, cv = 5, verbose = 2, return_train_score=True)\n    \n    gc = gc.fit(X.values, y.values.ravel())\n\n    return gc","47af26e4":"# Custom Function to plot GridSearch Result to get the best value\n\ndef hypertuning_plot(scores, parameter):\n    \n    col = \"param_\" + parameter\n    \n    plt.figure()\n    \n    plt.plot(scores[col], scores[\"mean_train_score\"], label = \"training accuracy\")\n    plt.plot(scores[col], scores[\"mean_test_score\"], label = \"test accuracy\")\n    \n    plt.xlabel(parameter)\n    plt.ylabel(\"Accuracy\")\n    \n    plt.legend()\n    plt.show()","c9718ecd":"X = titanic_data.drop('Survived', axis = 1)\ny = titanic_data[['Survived']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.85, test_size = 0.15, random_state = 100)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","20ffcd81":"# XGBoost with Default Parameters\n\nxgb = XGBClassifier(n_jobs = -1, random_state = 100)\n\nxgb = xgb.fit(X_train.values, y_train.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, xgb, X_test)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                   'auc_roc', 'auc_pr', 'confusion_matrix'])\nsampling_results","01dc7d44":"# GridSearchCV to find optimal max_depth\n\nxgb = XGBClassifier(n_jobs = -1, random_state = 100)\n\nparameter = 'max_depth'\n\nparam_grid = {parameter: range(4, 40)}\n\ngcv = tune_hyper_parameter(X_train, y_train, param_grid, 'Individual', xgb)\n    \n# scores of GridSearch CV\nscores = gcv.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngcv.best_params_","93eb2900":"# GridSearchCV to find optimal learning_rate\n\nxgb = XGBClassifier(max_depth = 4, n_jobs = -1, random_state = 100)\n\nparameter = 'learning_rate'\n\nparam_grid = {parameter: [0.001, 0.01, 0.1, 0.2, 0.3, 0.6, 0.9, 0.95, 0.99]}\n\ngc = tune_hyper_parameter(X_train, y_train, param_grid, 'Individual', xgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","c0af1b36":"# GridSearchCV to find optimal n_estimators\n\nxgb = XGBClassifier(max_depth = 4, learning_rate = 0.1, n_jobs = -1, random_state = 100)\n\nparameter = 'n_estimators'\n\nparam_grid = {parameter: range(100, 1100, 100)}\n\ngc = tune_hyper_parameter(X_train, y_train, param_grid, 'Individual', xgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","95b91ee2":"# GridSearchCV to find optimal min_child_weight\n\nxgb = XGBClassifier(max_depth = 4, learning_rate = 0.1, n_estimators = 100, n_jobs = -1, random_state = 100)\n\nparameter = 'min_child_weight'\n\nparam_grid = {parameter: range(1, 11)}\n\ngc = tune_hyper_parameter(X_train, y_train, param_grid, 'Individual', xgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","7ec4af5d":"# GridSearchCV to find optimal subsample: \n\nxgb = XGBClassifier(max_depth = 4, learning_rate = 0.1, n_estimators = 100, min_child_weight = 8,\n                    n_jobs = -1, random_state = 100)\n\nparameter = 'subsample'\n\nparam_grid = {parameter: np.arange(0.1, 1.1, 0.1)}\n\ngc = tune_hyper_parameter(X_train, y_train, param_grid, 'Individual', xgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","04927e50":"# GridSearchCV to find optimal colsample_bytree: \n\nxgb = XGBClassifier(max_depth = 4, learning_rate = 0.1, n_estimators = 100, min_child_weight = 8,\n                    subsample = 1.0, n_jobs = -1, random_state = 100)\n\nparameter = 'colsample_bytree'\n\nparam_grid = {parameter: np.arange(0.1, 1.1, 0.1)}\n\ngc = tune_hyper_parameter(X_train, y_train, param_grid, 'Individual', xgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","26e39f21":"# Random Forest with best parameters obtained from grid search\n\nxgb = XGBClassifier(max_depth = 4, learning_rate = 0.1, n_estimators = 100, min_child_weight = 8,\n                    subsample = 1.0, colsample_bytree = 1.0, n_jobs = -1, random_state = 100)\n\nxgb = xgb.fit(X_train.values, y_train.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, xgb, X_test)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                   'auc_roc', 'auc_pr', 'confusion_matrix'])\nsampling_results","07033ff1":"# LightGBM with Default Parameters\n\nlgb = LGBMClassifier(objective = 'binary', n_jobs = -1, random_state = 100)\n\nlgb = lgb.fit(X_train.values, y_train.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, lgb, X_test)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                   'auc_roc', 'auc_pr', 'confusion_matrix'])\nsampling_results","0e6bac63":"# GridSearchCV to find optimal num_leaves\n\nlgb = LGBMClassifier(objective = 'binary', n_jobs = -1, random_state = 100)\n\nparameter = 'num_leaves'\n\nparam_grid = {parameter: range(20, 72)}\n\ngc = tune_hyper_parameter(X_train, y_train, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","9f022baf":"# GridSearchCV to find optimal max_depth\n\nlgb = LGBMClassifier(num_leaves = 20, objective = 'binary', n_jobs = -1, random_state = 100)\n\nparameter = 'max_depth'\n\nparam_grid = {parameter: range(8, 72)}\n\ngc = tune_hyper_parameter(X_train, y_train, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","6a1f0492":"# GridSearchCV to find optimal learning_rate\n\nlgb = LGBMClassifier(num_leaves = 20, max_depth = 8, objective = 'binary', n_jobs = -1, random_state = 100)\n\nparameter = 'learning_rate'\n\nparam_grid = {parameter: np.arange(0.1, 1.1, 0.1)}\n\ngc = tune_hyper_parameter(X_train, y_train, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","5a230110":"# GridSearchCV to find optimal n_estimators\n\nlgb = LGBMClassifier(num_leaves = 20, max_depth = 8, learning_rate = 0.1, objective = 'binary',\n                     n_jobs = -1, random_state = 100)\n\nparameter = 'n_estimators'\n\nparam_grid = {parameter: range(100, 1100, 100)}\n\ngc = tune_hyper_parameter(X_train, y_train, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","7ddac760":"# GridSearchCV to find optimal min_child_samples\n\nlgb = LGBMClassifier(num_leaves = 20, max_depth = 8, learning_rate = 0.1, n_estimators = 100,\n                     objective = 'binary', n_jobs = -1, random_state = 100)\n\nparameter = 'min_child_samples'\n\nparam_grid = {parameter: range(1, 26)}\n\ngc = tune_hyper_parameter(X_train, y_train, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","0365a28f":"# GridSearchCV to find optimal subsample: \n\nlgb = LGBMClassifier(num_leaves = 20, max_depth = 8, learning_rate = 0.1, n_estimators = 100, min_child_samples = 4,\n                     objective = 'binary', n_jobs = -1, random_state = 100)\n\nparameter = 'subsample'\n\nparam_grid = {parameter: np.arange(0.1, 1.1, 0.1)}\n\ngc = tune_hyper_parameter( X_train, y_train, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","d40fba98":"# GridSearchCV to find optimal colsample_bytree: \n\nlgb = LGBMClassifier(num_leaves = 20, max_depth = 8, learning_rate = 0.1, n_estimators = 100, min_child_samples = 4,\n                     subsample = 0.1, objective = 'binary', n_jobs = -1, random_state = 100)\n\nparameter = 'colsample_bytree'\n\nparam_grid = {parameter: np.arange(0.1, 1.1, 0.1)}\n\ngc = tune_hyper_parameter(X_train, y_train, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","cd89ab02":"# LightGBM with best parameters obtained from grid search\n\nlgb = LGBMClassifier(num_leaves = 20, max_depth = 8, learning_rate = 0.1, n_estimators = 100, min_child_samples = 4,\n                     subsample = 0.1, colsample_bytree = 0.2, objective = 'binary', n_jobs = -1, random_state = 100)\n\nlgb = lgb.fit(X_train.values, y_train.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, lgb, X_test)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                   'auc_roc', 'auc_pr', 'confusion_matrix'])\nsampling_results","8b8b5629":"xgb_pred = xgb.predict(titanic_test_data.values)\n\nxg_pred_file = pd.DataFrame({'PassengerId' : titanic_test_passengerId['PassengerId'],\n                       'Survived': xgb_pred.T})\nxg_pred_file.to_csv(\"submit_xg.csv\", index=False)","3b9c7200":"lgb_pred = lgb.predict(titanic_test_data)\n\nlg_pred_file = pd.DataFrame({'PassengerId' : titanic_test_passengerId['PassengerId'],\n                       'Survived': lgb_pred.T})\nlg_pred_file.to_csv(\"submit_lgbm.csv\", index=False)","c84f6597":"### 1. familysize\n\n**The two features - 'Prach' and 'SibSp' can be combined to get total family size. We will create a new variable 'familysize' adding these two features.**","42f6777e":"## Feature Engineering","ac55c44c":"#### LightGBM - HyperTuning max_depth","5ed5caa9":"The above data shows that the 'Emabrked' values are C and S with almost equal distribution. Let us maintain this distribuion and impute C & S to each of the 2 missing embarked values.","9dc63104":"#### LightGBM - HyperTuning min_child_samples","b7b23d95":"## Test Train Split","b19c229e":"#### XGBoost - HyperTuning learning_rate","66b7d39e":"#### 1. Frequency plot for persons survived vs not survived\n\n        -- 1 - Survived\n        -- 0 - Not Survived","d5c26314":"#### We see in the train set - \n1. 'Age' column has 19.86% missing values \n2. 'Cabin' column has 77.10% missing values \n3. 'Embarked' column has 0.22% missing values\n\n#### Let's check the same for the test set","3e44f115":"## 1. XG Boost Model\n\n### Default Parameters","dabcb866":"#### 9. SibSp vs Survived Status","91493a26":"#### XGBoost - HyperTuning n_estimators","af92d514":"#### Running the LightGBM with best parameters obtained from grid search - ","dcc0adeb":"So there are just 2 rows with misssing 'Embarked' - Port of Embarkation value. We will see some values near the fare amount for these 2 rows i.e., 80","e18ed8ad":"We see that only some category have missing age values. We will impute them with the following measures\n\n    -- Salution type 'Mr' - Age can be imputed with the Median (as Mean > Median)\n    -- Salution type 'Dr' - Age can be imputed with the Mean value\n    -- Salution type 'Master' - Age can be imputed with the Median (as Mean > Median)\n    -- Salution type 'Miss' - Age can be imputed with the Median (as Mean > Median)\n    -- Salution type 'Mrs' - Age can be imputed with the Median (as Mean > Median)","141855c1":"#### 8. Age vs Survived Status","40361750":"### 2. age_category\n\n**Converting Age into bins (as it is continuos with no proper distribution)** \n**We will define a new variable  - age_category and have the following bins to categorize the person based on the age -**\n\n    -- Age <= 15             -- age_category - 0\n    -- Age > 15 & <= 30      -- age_category - 1\n    -- Age > 30 & <= 45      -- age_category - 2\n    -- Age > 45 & <= 60      -- age_category - 3\n    -- Age > 60              -- age_category - 4\n\n**As we have a new feature for age, we will drop the original Age feature as it would be redundant now**","b9a5f140":"#### 4. Passenger Class vs Survived Status","5db6dadf":"#### 5. Gender vs Survival Status","9e23596f":"# Exploratory Data Analysis\n\n#### Defining some custom functions for EDA graph plottting","785a8fd2":"**Lets now see how this new feature 'familysize' relates to the survived status**\n\n#### Plotting familysize vs survival status","6ee8d900":"#### 7. Ticket Fare Distribution","b0d4ca5d":"#### LightGBM - HyperTuning subsample\u00b6","344b109d":"# Titanic: Machine Learning from Disaster\n\n## Problem Description\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n","3d4d06b5":"## 2. LightGBM\n\n### Running the LightGBM Classifier with default parameters.","d14c416a":"We have the age distributions for each salutation category. We will use this information to impute each of the missing age values with an appropriate value is the respective salutation category.\n\nLets check the number fo missing values in each of these categories - ","f4277860":"#### We see in the train set - \n1. 'Age' column has 20.57% missing values \n2. 'Cabin' column has 78.22% missing values ","0a21c67b":"## Hyperparameter Tuning for XGBoost\n\n#### XGBoost - HyperTuning max_depth","266f47af":"# Data Exploraton and Preparation","cb24445f":"### 6. Encoding the 'salutation' column values with numbers\n\n#### Value_Counts for salutation column\n    Mr           518\n    Miss         185\n    Mrs          129\n    Master        40\n    Dr             7\n    Rev            6\n    army_rank      5\n    Don            1","851e5f67":"## VariableDefinitionKey \n    -- survival - Survival 0 = No, 1 = Yes \n    -- pclass - Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd \n    -- sex - Sex \n    -- Age - Age in years \n    -- sibsp - # of siblings \/ spouses aboard the Titanic parch # of parents \/ children aboard the Titanic \n    -- ticket - Ticket number \n    -- fare - Passenger fare \n    -- cabin - Cabin number \n    -- embarked - Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton \n\n## Variable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","dffc6911":"# Predictions","c537ff96":"### 5. Encode the 'Embarked ' column with numbers 0, 1 & 2","004c35ad":"#### XGBoost - HyperTuning subsample","cf533bb8":"#### We have a perfectly clean data set now. There are still some columns like Name, Sex and salutation which need to be handled.\n\n    -- Name column can be dropped.\n    -- Sex column can be mapped to 1 - Female & 0 - Male (we will handle this column after the EDA section)\n    -- salutation column can be one hot encoded. (we will handle this column after the EDA section)","1142eaad":"#### Running the XGBoost with best parameters obtained from grid search - ","8fa84350":"#### 3. Person Salutation v\/s Survived Status ","64d4b4a2":"### 3. fare_category\n\n**As Fare is also a continuous variable without any pattern, let's convert the fare into a new variable - 'fare_category'**\n\n        -- Fare : 0 to 7.85          - fare_category - 0\n        -- Fare : 7.85 to 10.50      - fare_category - 1\n        -- Fare : 10.50 to 21.67     - fare_category - 2\n        -- Fare : 21.67 to 39.68     - fare_category - 3\n        -- Fare : > 39.68            - fare_category - 4\n\n* fare ranges obtained from the below table columns description\n       \n**Once we get the fare_category column, let's delete the original Fare column as that would now be redundant.","18e9b174":"#### LightGBM - HyperTuning colsample_bytree","b01d7b18":"### 3. Converting Sex to numerical values - 0 and 1\n\n    -- 1 - Male\n    -- 0 - Female","60fec2fb":"## LightGBM - HyperTuning\n\n#### LightGBM - HyperTuning num_leaves","636ecfa4":"#### Let's seee how we can handle the missing values here","27f0bc05":"#### LightGBM - HyperTuning n_estimators","aa53e849":"#### 2. Port of Embarkation vs Survived Status","828c6288":"#### 10. Parch vs Survived Status","79479211":"### 4. Converting Deck column to Label Encoded columns","8e256ad2":"#### XGBoost - HyperTuning min_child_weight","cf8499b1":"# Data Import","e9b8b610":"### Salutations Data Dictionary\n\n    -- Mlle         - unmarried woman - Mademoiselle\n    -- Mme          - older married woman - Madame\n    -- countess     - the wife or widow of a count or earl\n    -- Jonkheer     - jonkvrouw is literally translated as \"young lord\" or \"young lady\"\n    -- Rev          - The Most Reverend is a style applied to certain religious figures, primarily within the \n                      historic   denominations of Christianity, but occasionally in some more modern traditions also.\n\n**The above data is taken from respective wikipedia or from results returned by google.**\n\nGoing by the above meanings of the salutations, we can combine some of the salutations to one.\n\n    -- Mlle, Jonkheer, Ms and Miss can be replaced with 'Miss'\n    -- Mme, countess, Lady and Mrs can be replaced with 'Mrs'\n    -- Sir can be replaced with 'Mr'\n    -- Rest of the salutations will be treated as is\n    -- Major, Col, Capt can be replaced with a common term - 'army_rank'","882d6de3":"#### 6. Gender, Survived vs Pclass","1a0571b2":"#### XGBoost - HyperTuning colsample_bytree","7c130438":"# Model Building","9db86f75":"#### LightGBM - HyperTuning learning_rate"}}