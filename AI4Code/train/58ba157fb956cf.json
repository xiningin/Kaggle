{"cell_type":{"fe6159d8":"code","7b6b35c8":"code","4f1376e9":"code","f4b4e20e":"code","e576f248":"code","b53b00a7":"code","14306677":"code","820a74a7":"code","7c60fe6e":"code","c5d5e73e":"code","3e27b1c4":"code","24751746":"code","416cc4bc":"code","a1cbbdfc":"code","07819864":"code","b5bc650a":"code","7c7f7cd4":"code","a6354516":"code","65a2c5fb":"code","12f03a89":"code","8c488e16":"code","12eb3079":"code","95f348fa":"code","87fc146a":"code","6587d14e":"markdown","e7fd6ded":"markdown","4e8c5495":"markdown","9da73c5b":"markdown","9fa3710f":"markdown","fdb03885":"markdown","338778d4":"markdown","57957177":"markdown","47213954":"markdown","94db4acd":"markdown","36eac506":"markdown","01d82864":"markdown","236bca87":"markdown","0a0ec80f":"markdown","9d6b8ff9":"markdown","99312755":"markdown","7dc3df91":"markdown","21b88a10":"markdown","e3256ecc":"markdown","4fbb5eaf":"markdown","f1bcb6ef":"markdown","2131a33a":"markdown","1a4da391":"markdown","2b8b911e":"markdown","50bb79bd":"markdown","88366639":"markdown","0b04e89c":"markdown","131ca6a2":"markdown","f0da1b51":"markdown","c0cea5b8":"markdown","757a8ed9":"markdown"},"source":{"fe6159d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# Any results you write to the current directory are saved as output.","7b6b35c8":"train = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/test.csv')","4f1376e9":"#https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,5)); res = 1000\nplt.plot(range(0,train.shape[0],res),train.signal[0::res])\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Signal - 10 batches',size=20)\nplt.show()\n","f4b4e20e":"\n#https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930\nplt.figure(figsize=(20,5)); res = 1000\nplt.plot(range(0,train.shape[0],res),train.open_channels[0::res])\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Channels Open',size=16); \nplt.title('Training Data Open Channels - 10 batches',size=20)\nplt.show()","e576f248":"train.head()","b53b00a7":"test.head()","14306677":"print(len(train))\nprint(len(test))","820a74a7":"train.describe()","7c60fe6e":"from IPython.core.display import HTML\n\n\nHTML('''<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/CaCcOwJPytQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe> ''')","c5d5e73e":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nimport random\n#from tqdm import tqdm\nfrom tqdm.notebook import tqdm\nimport gc\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 500)\n\nimport os\n\n\n# Any results you write to the current directory are saved as output.","3e27b1c4":"# configurations and main hyperparammeters\nEPOCHS = 150\nNNBATCHSIZE = 32\nGROUP_BATCH_SIZE = 4000\nSEED = 123\nLR = 0.001\nSPLITS = 5\n\noutdir = 'wavenet_models'\nflip = False\nnoise = False\n\n\nif not os.path.exists(outdir):\n    os.makedirs(outdir)\n\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","24751746":"# read data\ndef read_data():\n    train = pd.read_csv('\/kaggle\/input\/clean-kalman\/train_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n    test  = pd.read_csv('\/kaggle\/input\/clean-kalman\/test_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32})\n    #from https:\/\/www.kaggle.com\/sggpls\/wavenet-with-shifted-rfc-proba and\n    # https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/144645\n    Y_train_proba = np.load(\"\/kaggle\/input\/ion-shifted-rfc-proba\/Y_train_proba.npy\")\n    Y_test_proba = np.load(\"\/kaggle\/input\/ion-shifted-rfc-proba\/Y_test_proba.npy\")\n    \n    for i in range(11):\n        train[f\"proba_{i}\"] = Y_train_proba[:, i]\n        test[f\"proba_{i}\"] = Y_test_proba[:, i]\n        \n    sub  = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv', dtype={'time': np.float32})\n    return train, test, sub\n\n# create batches of 4000 observations\ndef batching(df, batch_size):\n    #print(df)\n    df['group'] = df.groupby(df.index\/\/batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    return df\n\n# normalize the data (standard scaler). We can also try other scalers for a better score!\ndef normalize(train, test):\n    train_input_mean = train.signal.mean()\n    train_input_sigma = train.signal.std()\n    train['signal'] = (train.signal - train_input_mean) \/ train_input_sigma\n    test['signal'] = (test.signal - train_input_mean) \/ train_input_sigma\n    return train, test\n\n# get lead and lags features\ndef lag_with_pct_change(df, windows):\n    for window in windows:    \n        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n    return df\n\n# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\ndef run_feat_engineering(df, batch_size):\n    # create batches\n    df = batching(df, batch_size = batch_size)\n    # create leads and lags (1, 2, 3 making them 6 features)\n    df = lag_with_pct_change(df, [1, 2, 3])\n    # create signal ** 2 (this is the new feature)\n    df['signal_2'] = df['signal'] ** 2\n    return df\n\n# fillna with the mean and select features for training\ndef feature_selection(train, test):\n    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n    train = train.replace([np.inf, -np.inf], np.nan)\n    test = test.replace([np.inf, -np.inf], np.nan)\n    for feature in features:\n        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n        train[feature] = train[feature].fillna(feature_mean)\n        test[feature] = test[feature].fillna(feature_mean)\n    return train, test, features\n\n\ndef split(GROUP_BATCH_SIZE=4000, SPLITS=5):\n    print('Reading Data Started...')\n    train, test, sample_submission = read_data()\n    train, test = normalize(train, test)\n    print('Reading and Normalizing Data Completed')\n    print('Creating Features')\n    print('Feature Engineering Started...')\n    train = run_feat_engineering(train, batch_size=GROUP_BATCH_SIZE)\n    test = run_feat_engineering(test, batch_size=GROUP_BATCH_SIZE)\n    train, test, features = feature_selection(train, test)\n    print(train.head())\n    print('Feature Engineering Completed...')\n\n    target = ['open_channels']\n    group = train['group']\n    kf = GroupKFold(n_splits=SPLITS)\n    splits = [x for x in kf.split(train, train[target], group)]\n    new_splits = []\n    for sp in splits:\n        new_split = []\n        new_split.append(np.unique(group[sp[0]]))\n        new_split.append(np.unique(group[sp[1]]))\n        new_split.append(sp[1])\n        new_splits.append(new_split)\n    target_cols = ['open_channels']\n    print(train.head(), train.shape)\n    train_tr = np.array(list(train.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n    train = np.array(list(train.groupby('group').apply(lambda x: x[features].values)))\n    test = np.array(list(test.groupby('group').apply(lambda x: x[features].values)))\n    print(train.shape, test.shape, train_tr.shape)\n    return train, test, train_tr, new_splits","416cc4bc":"#from https:\/\/www.kaggle.com\/hanjoonchoe\/wavenet-lstm-pytorch-ignite-ver\n\nclass Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        \n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n        \n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        \n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n        \n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim), \n            self.weight\n        ).view(-1, step_dim)\n        \n        if self.bias:\n            eij = eij + self.b\n            \n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n        \n        if mask is not None:\n            a = a * mask\n\n        a = a \/ torch.sum(a, 1, keepdim=True) + 1e-10\n\n        weighted_input = x * torch.unsqueeze(a, -1)\n        return torch.sum(weighted_input, 1)","a1cbbdfc":"import torch.nn.functional as F\n","07819864":"# from https:\/\/www.kaggle.com\/hanjoonchoe\/wavenet-lstm-pytorch-ignite-ver        \nclass Wave_Block(nn.Module):\n    \n    def __init__(self,in_channels,out_channels,dilation_rates):\n        super(Wave_Block,self).__init__()\n        self.num_rates = dilation_rates\n        self.convs = nn.ModuleList()\n        self.filter_convs = nn.ModuleList()\n        self.gate_convs = nn.ModuleList()\n        \n        self.convs.append(nn.Conv1d(in_channels,out_channels,kernel_size=1))\n        dilation_rates = [2**i for i in range(dilation_rates)]\n        for dilation_rate in dilation_rates:\n            self.filter_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate,dilation=dilation_rate))\n            self.gate_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate,dilation=dilation_rate))\n            self.convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=1))\n            \n    def forward(self,x):\n        x = self.convs[0](x)\n        res = x\n        for i in range(self.num_rates):\n            x = F.tanh(self.filter_convs[i](x))*F.sigmoid(self.gate_convs[i](x))\n            x = self.convs[i+1](x)\n            #x += res\n            res = torch.add(res, x)\n        return res\n    \n    \n\n    \nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        input_size = 128\n        self.LSTM1 = nn.GRU(input_size=19,hidden_size=64,num_layers=2,batch_first=True,bidirectional=True)\n\n        self.LSTM = nn.GRU(input_size=input_size,hidden_size=64,num_layers=2,batch_first=True,bidirectional=True)\n        #self.attention = Attention(input_size,4000)\n        #self.rnn = nn.RNN(input_size, 64, 2, batch_first=True, nonlinearity='relu')\n       \n        \n        self.wave_block1 = Wave_Block(128,16,12)\n        self.wave_block2 = Wave_Block(16,32,8)\n        self.wave_block3 = Wave_Block(32,64,4)\n        self.wave_block4 = Wave_Block(64, 128, 1)\n        self.fc = nn.Linear(128, 11)\n            \n    def forward(self,x):\n        x,_ = self.LSTM1(x)\n        x = x.permute(0, 2, 1)\n      \n        x = self.wave_block1(x)\n        x = self.wave_block2(x)\n        x = self.wave_block3(x)\n        \n        #x,_ = self.LSTM(x)\n        x = self.wave_block4(x)\n        x = x.permute(0, 2, 1)\n        x,_ = self.LSTM(x)\n        #x = self.conv1(x)\n        #print(x.shape)\n        #x = self.rnn(x)\n        #x = self.attention(x)\n        x = self.fc(x)\n        return x\n\n   \n    \nclass EarlyStopping:\n    def __init__(self, patience=7, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n        self.counter, self.best_score = 0, None\n        self.is_maximize = is_maximize\n\n\n    def load_best_weights(self, model):\n        model.load_state_dict(torch.load(self.checkpoint_path))\n\n    def __call__(self, score, model):\n        if self.best_score is None or \\\n                (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n            torch.save(model.state_dict(), self.checkpoint_path)\n            self.best_score, self.counter = score, 0\n            return 1\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                return 2\n        return 0","b5bc650a":"from torch.utils.data import Dataset, DataLoader\nclass IronDataset(Dataset):\n    def __init__(self, data, labels, training=True, transform=None, seq_len=5000, flip=0.5, noise_level=0, class_split=0.0):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n        self.training = training\n        self.flip = flip\n        self.noise_level = noise_level\n        self.class_split = class_split\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        data = self.data[idx]\n        labels = self.labels[idx]\n\n        return [data.astype(np.float32), labels.astype(int)]","7c7f7cd4":"train, test, train_tr, new_splits = split()","a6354516":"pip install torchcontrib","65a2c5fb":"\nfrom torchcontrib.optim import SWA\nimport torchcontrib","12f03a89":"model = Classifier()\nmodel","8c488e16":"%%time\ntest_y = np.zeros([int(2000000\/GROUP_BATCH_SIZE), GROUP_BATCH_SIZE, 1])\ntest_dataset = IronDataset(test, test_y, flip=False)\ntest_dataloader = DataLoader(test_dataset, NNBATCHSIZE, shuffle=False)\ntest_preds_all = np.zeros((2000000, 11))\n\n\noof_score = []\nfor index, (train_index, val_index, _) in enumerate(new_splits[0:], start=0):\n    print(\"Fold : {}\".format(index))\n    train_dataset = IronDataset(train[train_index], train_tr[train_index], seq_len=GROUP_BATCH_SIZE, flip=flip, noise_level=noise)\n    train_dataloader = DataLoader(train_dataset, NNBATCHSIZE, shuffle=True,num_workers = 16)\n\n    valid_dataset = IronDataset(train[val_index], train_tr[val_index], seq_len=GROUP_BATCH_SIZE, flip=False)\n    valid_dataloader = DataLoader(valid_dataset, NNBATCHSIZE, shuffle=False)\n\n    it = 0\n    model = Classifier()\n    model = model.cuda()\n\n    early_stopping = EarlyStopping(patience=40, is_maximize=True,\n                                   checkpoint_path=os.path.join(outdir, \"gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index,\n                                                                                                             it)))\n\n    weight = None#cal_weights()\n    criterion = nn.CrossEntropyLoss(weight=weight)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n    optimizer = torchcontrib.optim.SWA(optimizer, swa_start=10, swa_freq=2, swa_lr=0.0011)\n    \n    \n\n\n    #schedular = torch.optim.lr_scheduler.CyclicLR(optimizer,base_lr=LR, max_lr=0.003, step_size_up=len(train_dataset)\/2, cycle_momentum=False)\n    \n    schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.2)\n    \n    avg_train_losses, avg_valid_losses = [], []\n\n    \n\n    for epoch in range(EPOCHS):\n        \n        train_losses, valid_losses = [], []\n        tr_loss_cls_item, val_loss_cls_item = [], []\n\n        model.train()  # prep model for training\n        train_preds, train_true = torch.Tensor([]).cuda(), torch.LongTensor([]).cuda()#.to(device)\n        \n        print('**********************************')\n        print(\"Folder : {} Epoch : {}\".format(index, epoch))\n        print(\"Curr learning_rate: {:0.9f}\".format(optimizer.param_groups[0]['lr']))\n        \n            #loss_fn(model(input), target).backward()\n        for x, y in tqdm(train_dataloader):\n            x = x.cuda()\n            y = y.cuda()\n            #print(x.shape)\n            \n         \n            \n            optimizer.zero_grad()\n            predictions = model(x)\n\n            predictions_ = predictions.view(-1, predictions.shape[-1])\n            y_ = y.view(-1)\n\n            loss = criterion(predictions_, y_)\n\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            \n            schedular.step(loss)\n            # record training lossa\n            train_losses.append(loss.item())\n            train_true = torch.cat([train_true, y_], 0)\n            train_preds = torch.cat([train_preds, predictions_], 0)\n\n        #model.eval()  # prep model for evaluation\n        \n        optimizer.update_swa()\n        optimizer.swap_swa_sgd()\n        val_preds, val_true = torch.Tensor([]).cuda(), torch.LongTensor([]).cuda()\n        print('EVALUATION')\n        with torch.no_grad():\n            for x, y in tqdm(valid_dataloader):\n                x = x.cuda()#.to(device)\n                y = y.cuda()#..to(device)\n\n                predictions = model(x)\n                predictions_ = predictions.view(-1, predictions.shape[-1])\n                y_ = y.view(-1)\n\n                loss = criterion(predictions_, y_)\n\n                valid_losses.append(loss.item())\n\n\n                val_true = torch.cat([val_true, y_], 0)\n                val_preds = torch.cat([val_preds, predictions_], 0)\n \n        \n        # calculate average loss over an epoch\n        train_loss = np.average(train_losses)\n        valid_loss = np.average(valid_losses)\n        avg_train_losses.append(train_loss)\n        avg_valid_losses.append(valid_loss)\n        print(\"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n\n        train_score = f1_score(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy().argmax(1),\n                               labels=list(range(11)), average='macro')\n\n        val_score = f1_score(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy().argmax(1),\n                             labels=list(range(11)), average='macro')\n\n        schedular.step(val_score)\n        print(\"train_f1: {:0.6f}, valid_f1: {:0.6f}\".format(train_score, val_score))\n        res = early_stopping(val_score, model)\n        #print('fres:', res)\n        if  res == 2:\n            print(\"Early Stopping\")\n            print('folder %d global best val max f1 model score %f' % (index, early_stopping.best_score))\n            break\n        elif res == 1:\n            print('save folder %d global val max f1 model score %f' % (index, val_score))\n    print('Folder {} finally best global max f1 score is {}'.format(index, early_stopping.best_score))\n    oof_score.append(round(early_stopping.best_score, 6))\n    \n    model.eval()\n    pred_list = []\n    with torch.no_grad():\n        for x, y in tqdm(test_dataloader):\n            \n            x = x.cuda()\n            y = y.cuda()\n\n            predictions = model(x)\n            predictions_ = predictions.view(-1, predictions.shape[-1]) # shape [128, 4000, 11]\n            #print(predictions.shape, F.softmax(predictions_, dim=1).cpu().numpy().shape)\n            pred_list.append(F.softmax(predictions_, dim=1).cpu().numpy()) # shape (512000, 11)\n            #a = input()\n        test_preds = np.vstack(pred_list) # shape [2000000, 11]\n        test_preds_all += test_preds\n   ","12eb3079":"print('all folder score is:%s'%str(oof_score))\nprint('OOF mean score is: %f'% (sum(oof_score)\/len(oof_score)))\nprint('Generate submission.............')\nsubmission_csv_path = '\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv'\nss = pd.read_csv(submission_csv_path, dtype={'time': str})\ntest_preds_all = test_preds_all \/ np.sum(test_preds_all, axis=1)[:, None]\ntest_pred_frame = pd.DataFrame({'time': ss['time'].astype(str),\n                                'open_channels': np.argmax(test_preds_all, axis=1)})\ntest_pred_frame.to_csv(\".\/gru_preds.csv\", index=False)\nprint('over')","95f348fa":"'''x = torch.randn((16,4000, 128))\nprint(x.shape)\n#x = x.permute(0, 2, 1)\nprint(x.shape)\n#x = x.permute(0, 2, 1)\nattention = Attention(128,4000)\nattention(x)'''\n","87fc146a":"\n'''x = torch.randn((2,64,300))\nprint(x.shape)\n#x = x.permute(0, 2, 1)\nprint(x.shape)\n#x = x.permute(0, 2, 1)\nattention = Attention(300,64)\nattention(x)\n#attention'''\n","6587d14e":"# what does Membrane Protein do?","e7fd6ded":"[ STOCHASTIC WEIGHT AVERAGING](https:\/\/pytorch.org\/blog\/stochastic-weight-averaging-in-pytorch\/)\n\nStochastic Weight Averaging (SWA) is a simple procedure that improves generalization in deep learning over Stochastic Gradient Descent (SGD) at no additional cost, and can be used as a drop-in replacement for any other optimizer in PyTorc\n\n![](https:\/\/scontent.fdac6-1.fna.fbcdn.net\/v\/t1.0-9\/59705847_2248977985403173_8149245770332110848_o.png?_nc_cat=107&_nc_sid=8024bb&_nc_ohc=PMOb2aDgLVsAX9dRes8&_nc_ht=scontent.fdac6-1.fna&oh=13176a691e130400ac1229830ffc27cc&oe=5EBDC505)","4e8c5495":"![](https:\/\/miro.medium.com\/max\/1400\/1*H7ZkZ5Ftd0gutXZylM0Ctg.png)\n\n**The overall model involves some stacks of dilated conv layers, nonlinear filter and gates, residual and skip connections, and last 1x1 convolutions.**","9da73c5b":"# Before Understanding Wavenet we need to understand what's The Kalman Filtering?","9fa3710f":"> An electrophysiology study (EPS) is a test that records your heart's electrical activity. It lets the electrophysiologist know if it's beating as it should or if you'll need treatment to get it back in rhythm.\n\n![](https:\/\/www.chestercountyhospital.org\/-\/media\/images\/chestercounty\/news%20images\/2019\/arrhythmia.ashx?h=419&w=800&la=en)","fdb03885":"**Lot more to come,i am new in this field, any suggestions in the comment box for improving this model is highly appreciated,thanks**","338778d4":"[**Ion Channels and Action Potential Generation**](https:\/\/www.sciencedirect.com\/topics\/neuroscience\/voltage-clamp)","57957177":"# Part 1 : Understanding Ion-switching","47213954":"in this news we can see [Ion channel VRAC enhances immune response against viruses](https:\/\/neurosciencenews.com\/vrac-ion-channel-virus-16144\/)","94db4acd":"# Part 2 : Modeling\n","36eac506":"The voltage-clamp technique is an experimental method that allows an experimenter to control (or \u201ccommand\u201d) the desired membrane voltage of the cell. The experimenter uses a set of electronic equipment (referred to here as a voltage-clamp device) to hold the membrane voltage at a desired level (the command voltage) while measuring the current that flows across the cell membrane at that voltage. The voltage-clamp device uses a negative feedback circuit to control the membrane voltage. To do this, the equipment measures the membrane voltage and compares it with the command voltage set by the experimenter. If the measured voltage is different from the command voltage, an error signal is generated and this tells the voltage-clamp device to pass current through an electrode in the neuron in order to correct the error and set the voltage to the command level. This can be accomplished using two microelectrodes inserted into the cell, one to measure voltage and another to pass current (see Figure Below), or using one large-diameter electrode that performs both functions.\n\n![](https:\/\/ars.els-cdn.com\/content\/image\/3-s2.0-B9780128153208000041-f04-10-9780128153208.jpg?_)\n\n> Figure Description : The two-electrode voltage-clamp technique.\n> This diagram depicts the circuit that is used to clamp the voltage of a neuron and measure the current that flows at that membrane voltage.","01d82864":"The WaveNet proposes an autoregressive learning with the help of convolutional networks with some tricks. Basically, we have a convolution window sliding on the  data, and at each step try to predict the next sample value that it did not see yet. In other words, it builds a network that learns the causal relationships between consequtive timesteps. (see below)\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*ABrc8jyFQ6xRubDtKE7ZBg.png)\n\nHere, the receptive size of the network is 5: We try to predict the next sample by using last 5 steps.\n\n![](https:\/\/miro.medium.com\/max\/884\/1*tlBZS9pSpk90H5jm3O3JhA.png)\n\nSample x_t is dependent on the previous n samples [not exactly t=1 to t=T].\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*ZYts5RoIwHobDqtpC66LuQ.png)\n\nIn order to increase the size of the receptive field, we can apply dilations. At each layer, number of dilation is increased by the factor of 2, hence the receptive size is 16 instead of 5.\n\n![](https:\/\/miro.medium.com\/max\/1140\/1*www46FWqJCc3OZQKP_QRoQ.gif)","236bca87":"as from this kernel [One Feature Model](https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930) we get to know that The training data is recordings in time. At each 10,000th of a second, the strength of the signal was recorded and the number of ion channels open was recorded. It is our task to build a model that predicts the number of open channels from signal at each time step. Furthermore we are told that the data was recorded in batches of 50 seconds. Therefore each 500,000 rows is one batch. The training data contains 10 batches and the test data contains 4 batches. Let's display the number of open channels and signal strength together for each training batch.","0a0ec80f":"> from the image above we can see that Upon infection of cells with a DNA virus (left), viral DNA binds to the enzyme cGAS which then synthesizes the messenger molecule cGAMP. The present work shows that cGAMP can leave the cell through the anion channel VRAC and diffuses to non-infected cells in the vicinity. After entering the cell \u2013 again through VRAC \u2013 it binds to a receptor called STING and stimulates indirectly the synthesis of interferon, which leaves the cell and suppresses, after binding to a receptor, the propagation of the virus (left cell). This provides a powerful amplification of the innate immune response against DNA viruses. The image is credited to Rosa Planells-Cases.\n\nfor more detail check this : [Ion channel VRAC enhances immune response against viruses](https:\/\/neurosciencenews.com\/vrac-ion-channel-virus-16144\/)\n\nFrom human diseases to how climate change affects plants, faster detection of ion channels could greatly accelerate solutions to major world problems.","9d6b8ff9":"# About Data","99312755":"# wavenet ","7dc3df91":"![](https:\/\/image.shutterstock.com\/z\/stock-photo-types-of-ion-channel-classification-by-gating-mechanism-of-action-voltage-gated-ligand-gated-514716034.jpg)","21b88a10":"for experimenting with wavenet pytorch model i will use this kernel [Wavenet pytorch ](https:\/\/www.kaggle.com\/cswwp347724\/wavenet-pytorch) and in each version i will try my level best to update models performance by trying different tactics,check ChangeLog section below....","e3256ecc":"**For more check this [WaveNet Implementation and Experiments](https:\/\/medium.com\/@evinpinar\/wavenet-implementation-and-experiments-2d2ee57105d5) from where i took all the informations attached above while learning WaveNet **","4fbb5eaf":"i found the attached video below best for understanding kalman filtering","f1bcb6ef":"![](https:\/\/image.slidesharecdn.com\/m7s5jm7zqxq4ntnu80nv-signature-5967dd0451739e2af475d635299752bfdaf370c5b5350b71f63bc5acce8dc46d-poli-150110161535-conversion-gate01\/95\/ionic-equilibria-and-membrane-potential-13-638.jpg?cb=1420906606)\n\n![](https:\/\/image.slidesharecdn.com\/m7s5jm7zqxq4ntnu80nv-signature-5967dd0451739e2af475d635299752bfdaf370c5b5350b71f63bc5acce8dc46d-poli-150110161535-conversion-gate01\/95\/ionic-equilibria-and-membrane-potential-15-638.jpg?cb=1420906606)","2131a33a":"**Membrane proteins are the gatekeepers to the cell and are essential to the function of all cells, controlling the flow of molecules and information across the cell membrane.**\nsource : [Membrane Protein](https:\/\/www.sciencedirect.com\/topics\/medicine-and-dentistry\/membrane-protein)","1a4da391":"# ChangeLog\n\nthe goal is to improve the models performance,so in \n\n* version 1 : i will try adding Stochastic weight averaging(swa) and adamW (lb 0.939)\n* version 2 : Adam with swa_lr=0.002 (lb 0.94)\n* version 3 : Adding LSTM layer before conv2\n* version 4 : our model was never using LSTM in version 3,i am trying to add LSTM again after wave_block4 (if i am making mistakes again,please help me in the comment box) [failed : waited more than 8 hours]\n\n* version 5 : 1 epoch for 5 fold takes  4min 23s so i will try 80 epochs instead of 150 (got lb 0.942) \n* version 6 : trying [Wavenet with SHIFTED-RFC Proba](https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/144645) as [this kernel ](https:\/\/www.kaggle.com\/sggpls\/wavenet-with-shifted-rfc-proba) for 90 epochs and batch size = 32\n* version 7 : solving SWA issue,trying cyclicLR and  solving model bug\n* version 8 : doing res = torch.add(res, x) instead of res+x and switching back to reducelronplateau scheduler and epoch = 150,swa_lr = 0.0011, added 1 more lstm before first wave block","2b8b911e":"For more about Ion channels understanding please check this research paper : [Voltage Clamp](https:\/\/www.sciencedirect.com\/topics\/neuroscience\/voltage-clamp) , this nature publication [Deep-Channel uses deep neural networks to detect single-molecule events from patch-clamp data](https:\/\/www.nature.com\/articles\/s42003-019-0729-3) and also this beautiful EDA kernel : [Ion Switching Competition : Signal EDA](https:\/\/www.kaggle.com\/tarunpaparaju\/ion-switching-competition-signal-eda)","50bb79bd":"# Electrophysiology: The Art of the Heart's Rhythm\n\n**[What Happens During an Electrophysiology Study?](https:\/\/www.chestercountyhospital.org\/news\/health-eliving-blog\/2019\/february\/electrophysiology-the-art-of-the-hearts-rhythm)**","88366639":"![](https:\/\/i2.wp.com\/neurosciencenews.com\/files\/2020\/04\/ion-channel-viruses-neuroscinews.jpg?w=800&ssl=1)","0b04e89c":"from the picture above we can see, in state 1 when Ionic Current (nA) is in 0th level means channel is closed then in state 2 we can see the graph moving toward -2 means the voltage gate is open and electric current are flowing through so the channel is open then in state 3 it is inactive as it reaches -2 level then in stage 4 it is inactivated again because it hasn't yet reach 0th level then in state 5 it gets closed as we reach at 0th level position\n\n**I'm not expert in this field,just trying to understand. so i can be wrong,correct me in the comment box if i am wrong**\n\ninfo source : [Ionic Equilibria and Membrane Potential ](https:\/\/www.slideshare.net\/CsillaEgri\/membrane-potential-43389721)","131ca6a2":"**The Kalman filtering is an amazing tool for estimating predicted values. It is an iterative mathematical process that uses set of equations and consecutive data inputs to quickly estimate the true value,position,velocity etc of the object being measured. The reason for using this is : \"Let's say you 50 or 100 data points that come in one at a time,yes we can do something like find the distribution of these data points and find the average value and say we have the average value that might be close to true value but in order to do that we need whole bunch of inputs already and the good thing is the kalman filter doesn't wait for a whole bunch of inputs and it very quickly starts to narrow in to the true value by taking a few of those inputs and by understanding the variations or uncertainty of those inputs  **","f0da1b51":"# imports","c0cea5b8":"**The training data is recordings in time. At each 10,000th of a second, the strength of the signal was recorded and the number of ion channels open was recorded. It is our task to build a model that predicts the number of open channels from signal at each time step. Furthermore we are told that the data was recorded in batches of 50 seconds. Therefore each 500,000 rows is one batch. The training data contains 10 batches and the test data contains 4 batches. Let's display the number of open channels and signal strength together for each training batch.**","757a8ed9":"![](https:\/\/image.slidesharecdn.com\/m7s5jm7zqxq4ntnu80nv-signature-5967dd0451739e2af475d635299752bfdaf370c5b5350b71f63bc5acce8dc46d-poli-150110161535-conversion-gate01\/95\/ionic-equilibria-and-membrane-potential-19-638.jpg?cb=1420906606)"}}