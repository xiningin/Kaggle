{"cell_type":{"e1bad2d0":"code","c1dea21f":"code","7a8f347a":"code","87e3aa5b":"code","915ca294":"code","d11aa539":"code","1cab950b":"code","7a918fb2":"code","404417e6":"code","e73e159c":"code","94c05c37":"code","69c9bd66":"code","f634ff72":"code","1df21f78":"code","40d9f7e5":"code","39c7fd68":"code","b2948535":"code","2d754d82":"code","c30e95a4":"code","6ef53f05":"code","ee669418":"code","fdd9f0e6":"code","fe5a6f0b":"code","d0f9c0e3":"code","12d4191c":"code","5310c578":"code","632dda6a":"code","a0a5ae24":"code","3f9bc029":"code","ca2c0931":"code","b9f12081":"code","01706cd0":"code","ad4acea7":"code","62d6be9e":"code","97319f38":"code","8c6d413e":"code","ceee5cbb":"code","1d9ede21":"code","8bbc387d":"code","985ff74c":"markdown","fdf79db3":"markdown","70f7230b":"markdown","89f08199":"markdown","8d633fbf":"markdown","11544d20":"markdown","35b5c212":"markdown","84835b35":"markdown","90b47b30":"markdown","99b67d52":"markdown","07493686":"markdown","f9211b2f":"markdown","9ac87209":"markdown","76a01350":"markdown","5de789da":"markdown","dbcb70f8":"markdown","771b4699":"markdown","72e40225":"markdown","8d9a691f":"markdown","b3a34c21":"markdown","5d5945ba":"markdown","ec84877c":"markdown","810bc3c7":"markdown","298d8548":"markdown","734a52b4":"markdown","a932191a":"markdown","7a96bbd9":"markdown","f1b064f2":"markdown","ed7c3e61":"markdown","cacb752a":"markdown","011aefa1":"markdown","2a2a69d0":"markdown","9325a295":"markdown","4f1c1ed8":"markdown","1612cb34":"markdown","a04dea2d":"markdown","dfc923cf":"markdown","e4ebaa91":"markdown"},"source":{"e1bad2d0":"# Basic libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.simplefilter(\"ignore\")\n# Directry check\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c1dea21f":"# Statistics library\nfrom scipy.stats import norm\nfrom scipy import stats\nimport scipy\n\n# Data preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n# Visualization\nfrom matplotlib import pyplot as plt\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\n\n# Dimension reduction\nfrom sklearn.manifold import TSNE\n\n# Machine learning\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\n\n# Validataion\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","7a8f347a":"# data loading\nsample = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ndf_train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","87e3aa5b":"# Create sale price log columns\ndf_train[\"log_SalePrice\"] = np.log10(df_train[\"SalePrice\"])","915ca294":"# dtype object\ndtype = pd.DataFrame({\"columns\":df_train.dtypes.index,\n                     \"dtype\":df_train.dtypes})\ndtype[\"dtype\"] = [str(i) for i in dtype[\"dtype\"]]\n\n# columns\nnum_columns = dtype.query('dtype==\"int64\" | dtype==\"float64\"')[\"columns\"].values[1:-2]\nobj_columns = dtype.query('dtype==\"object\"')[\"columns\"].values\n\nprint(\"numerical_values_count:{}\".format(len(num_columns)))\nprint(\"object_values_count:{}\".format(len(obj_columns)))","d11aa539":"### Separate data frame\ny = df_train[\"log_SalePrice\"]\n\ndf_num_train = df_train[num_columns]\ndf_num_test = df_test[num_columns]\n\ndf_cate_train = df_train[obj_columns]\ndf_cate_test = df_test[obj_columns]","1cab950b":"# fill by median\ncolumns = df_num_train.columns\nfor i in range(len(columns)):\n    median = df_train[columns[i]].median()\n    df_train[columns[i]].fillna(median, inplace=True)\n    df_test[columns[i]].fillna(median, inplace=True)\n    \ndf_num_train = df_train[num_columns]\ndf_num_test = df_test[num_columns]","7a918fb2":"# Training data\ndf_num_train[\"LotArea\"] = np.log10(df_num_train[\"LotArea\"]+1)\ndf_num_train[\"1stFlrSF\"] = np.log10(df_num_train[\"1stFlrSF\"]+1)\ndf_num_train[\"GrLivArea\"] = np.log10(df_num_train[\"GrLivArea\"]+1)\n\n# test data\ndf_num_test[\"LotArea\"] = np.log10(df_num_test[\"LotArea\"]+1)\ndf_num_test[\"1stFlrSF\"] = np.log10(df_num_test[\"1stFlrSF\"]+1)\ndf_num_test[\"GrLivArea\"] = np.log10(df_num_test[\"GrLivArea\"]+1)","404417e6":"# prepairing dataframe\ndf_cha = pd.concat([df_cate_train,y], axis=1)\n\n# define function\n# Change to mean value function\ndef change_cate_mean(df1, df2):\n    col_list_m = []\n    for i in range(0,len(df1.columns)-1):\n        # mean values\n        index = df1.iloc[:,i].value_counts().index\n        for j in range(len(index)):\n            mean = df1[df1.iloc[:,i]==index[j]][\"log_SalePrice\"].mean()\n            df1.iloc[:,i][df1.iloc[:,i]==index[j]] = mean\n            df2.iloc[:,i][df2.iloc[:,i]==index[j]] = mean\n        # update column name\n        col = df1.columns[i]+'_m'\n        col_list_m.append(col) \n    # create out put df\n    df_c1 = df1.copy().drop(\"log_SalePrice\", axis=1)\n    df_c2 = df2.copy()\n    df_c = pd.concat([df_c1, df_c2]).reset_index().drop(\"index\", axis=1)\n    df_c.columns = col_list_m\n    return df_c\n\n# Change to std value function\ndef change_cate_std(df1, df2):\n    col_list_m = []\n    for i in range(0,len(df1.columns)-1):\n        # mean values\n        index = df1.iloc[:,i].value_counts().index\n        for j in range(len(index)):\n            std = df1[df1.iloc[:,i]==index[j]][\"log_SalePrice\"].std()\n            df1.iloc[:,i][df1.iloc[:,i]==index[j]] = std\n            df2.iloc[:,i][df2.iloc[:,i]==index[j]] = std\n        # update column name\n        col = df1.columns[i]+'_s'\n        col_list_m.append(col) \n    # create out put df\n    df_c1 = df1.copy().drop(\"log_SalePrice\", axis=1)\n    df_c2 = df2.copy()\n    df_c = pd.concat([df_c1, df_c2]).reset_index().drop(\"index\", axis=1)\n    df_c.columns = col_list_m\n    return df_c\n\n# Change to float value function\ndef change_float(df):\n    for i in range(0,len(df.columns)-1):\n        lists = []\n        for j in range(len(df.iloc[:,i])):\n            flo = float(df.iloc[:,i][j])\n            lists.append(flo)\n        df.iloc[:,i] = lists\n    return df\n\n# Fill by median function\ndef fill_median(df):\n    for i in range(0,len(df.columns)-1):\n        median = df.iloc[:,i].median()\n        df.iloc[:,i].fillna(median, inplace=True)\n    return df\n\n# create df fix\ndf_cate_fix = pd.concat([\n    fill_median(change_float(change_cate_mean(df_cha, df_cate_test))), fill_median(change_float(change_cate_std(df_cha, df_cate_test))) ], axis=1)\n\ndf_cate_fix.head()","e73e159c":"# Re-Separate \ndf_cate_train = df_cate_fix.head(len(df_cate_train))\ndf_cate_test = df_cate_fix.tail(len(df_cate_test)).reset_index().drop(\"index\", axis=1)","94c05c37":"# corrlation coefficient\ncoef = pd.concat([df_cate_train,y], axis=1).corr().round(3)\n\n# Select variables with border of correlation with SalePrice values\n# Check the accuracy(R2 score)\nR2_train_score_list =[]\nR2_test_score_list = []\nvariable_count_list = []\nR2_valid_df = pd.DataFrame({})\n# Roop\nfor i in range(0,8):\n    col_select = coef[(coef[\"log_SalePrice\"] > 0.1*i) | (coef[\"log_SalePrice\"] < -0.1*i)].index\n    # calc R2 score of Ridge regression\n    # Dataset\n    X = df_cate_train[col_select[:-1]]\n    y = y\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n    # Scaling\n    msc = MinMaxScaler()\n    msc.fit(X)\n    X_train_msc = msc.fit_transform(X_train)\n    X_test_msc = msc.fit_transform(X_test)\n    # Training and score\n    ridge = Ridge().fit(X_train_msc, y_train)\n    R2_train_score = ridge.score(X_train_msc, y_train)\n    R2_test_score = ridge.score(X_test_msc, y_test)\n    variable_count = len(col_select)-1\n    R2_train_score_list.append(R2_train_score)\n    R2_test_score_list.append(R2_test_score)\n    variable_count_list.append(variable_count)\n    \n\n# create df\nR2_valid_df[\"valid\"] = [\"All\",\">0.1|<-0.1\",\">0.2|<-0.2\",\">0.3|<-0.3\",\">0.4|<-0.4\",\">0.5|<-0.5\",\">0.6|<-0.6\",\">0.7|<-0.7\"]\nR2_valid_df[\"R2_train_score\"] = R2_train_score_list\nR2_valid_df[\"R2_test_score\"] = R2_test_score_list\nR2_valid_df[\"R2_variable_count\"] = variable_count_list\n\n# Check\nR2_valid_df","69c9bd66":"col_select = coef[(coef[\"log_SalePrice\"] > 0.2) | (coef[\"log_SalePrice\"] < -0.2)].index[:-1]","f634ff72":"df_cate_train = df_cate_train[col_select]\ndf_cate_test = df_cate_test[col_select]","1df21f78":"# Data set\nX_data = pd.concat([df_num_train, df_cate_train], axis=1)\ny = df_train[\"log_SalePrice\"]\nX_test = pd.concat([df_num_test, df_cate_test], axis=1)","40d9f7e5":"# Correlation\nmatrix = X_data\n\n# Visualization\nplt.figure(figsize=(20,20))\nhm = sns.heatmap(matrix.corr(), vmax=1, vmin=-1, cmap=\"bwr\", square=True)","39c7fd68":"# quantile 75% sales price\nquat_price_80 = y.quantile(0.80)\ny_class = []\nfor i in y:\n    if i >= quat_price_80:\n        res = 1\n        y_class.append(res)\n    else:\n        res = 0\n        y_class.append(res)","b2948535":"# Train test data split\nX_train, X_val, y_train, y_val = train_test_split(X_data, y_class, test_size=0.2, random_state=10)","2d754d82":"# Random forest classifier\n# Create instance\nforest = RandomForestClassifier(n_estimators=10, random_state=10)\n\n# Gridsearch\nparam_range = [10, 15, 20]\nleaf = [70, 80, 90]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"n_estimators\":param_range, \"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\ngs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=1)\n\n# Fitting\ngs = gs.fit(X_train, y_train)\n\nprint(gs.best_score_)\nprint(gs.best_params_)","c30e95a4":"# Fitting for forest instance\nforest = RandomForestClassifier(n_estimators=15, random_state=10,\n                               criterion='gini', max_depth=15,\n                               max_leaf_nodes=60)\nforest.fit(X_train, y_train)\n\n# Importance \nimportance = forest.feature_importances_\n\n# index\nindices = np.argsort(importance)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" %(f+1, 30, X_data.columns[indices[f]], importance[indices[f]]))","6ef53f05":"# Visualization\nplt.figure(figsize=(15,6), facecolor=\"lavender\")\n\nplt.title('Feature Importances')\nplt.bar(range(X_train.shape[1]), importance[indices], color='blue', align='center')\nplt.xticks(range(X_train.shape[1]), X_data.columns[indices], rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.tight_layout()","ee669418":"# Select variables\nnumber = 34\nimportant_columns = X_data.columns[indices[0:number]]","fdd9f0e6":"# data set\n# target price change to log form price distribution analysis\nX = pd.concat([df_num_train, df_cate_train], axis=1)[important_columns]\ny = df_train[\"log_SalePrice\"]\nX_test = pd.concat([df_num_test, df_cate_test], axis=1)[important_columns]","fe5a6f0b":"# Train test data split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)","d0f9c0e3":"# Scaling\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_sc = sc.fit_transform(X_train)\nX_val_sc = sc.fit_transform(X_val)\nX_test_sc = sc.fit_transform(X_test)","12d4191c":"# Create instance\ntsne = TSNE(n_components=2, random_state=10)\n\n# Figging\ntsne.fit(X_train_sc)\n\n# Fit transform\ntsne_X_train = tsne.fit_transform(X_train_sc)\ntsne_X_val = tsne.fit_transform(X_val_sc)","5310c578":"# Visualization by plot\nx1 = tsne_X_train[:,0]\ny1 = tsne_X_train[:,1]\n\nx2 = tsne_X_val[:,0]\ny2 = tsne_X_val[:,1]\n\nplt.figure(figsize=(10,8))\nplt.scatter(x1, y1, c=\"blue\", label=\"train_data\")\nplt.scatter(x2, y2, c=\"red\", label=\"val_data\")\nplt.xlabel(\"iso_axis1\")\nplt.ylabel(\"iso_axis2\")\nplt.legend()","632dda6a":"# create instance\nforest = RandomForestRegressor(n_estimators=100, random_state=10)\n\nparams = {\"max_depth\":[20,21,22,23, 24], \"n_estimators\":[31,32,33,34,35]}\n\n# Fitting\ncv_f = GridSearchCV(forest, params, cv = 10, n_jobs =10)\n\ncv_f.fit(X_train, y_train)\n\nprint(\"Best params:{}\".format(cv_f.best_params_))\n\nbest_f = cv_f.best_estimator_\n\n# prediction\ny_train_pred_f = best_f.predict(X_train)\ny_val_pred_f = best_f.predict(X_val)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_f)))\nprint(\"MSE test;{}\".format(mean_squared_error(y_val, y_val_pred_f)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_f)))\nprint(\"R2 score test:{}\".format(r2_score(y_val, y_val_pred_f)))","a0a5ae24":"# Create instance\nxgbr = xgb.XGBRegressor()\n\nparams = {'learning_rate': [0.1, 0.15, 0.2, 0.25], 'max_depth': [3, 5,7,9], \n          'subsample': [0.8, 0.85, 0.9, 0.95], 'colsample_bytree': [0.3, 0.5, 0.8]}\n\n# Fitting\ncv_x = GridSearchCV(xgbr, params, cv = 10, n_jobs =1)\ncv_x.fit(X_train, y_train)\n\nprint(\"Best params:{}\".format(cv_x.best_params_))\n\nbest_x = cv_x.best_estimator_\n\n# prediction\ny_train_pred_x = best_x.predict(X_train)\ny_val_pred_x = best_x.predict(X_val)\n\n# prediction\ny_train_pred_x = cv_x.predict(X_train)\ny_val_pred_x = cv_x.predict(X_val)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_x)))\nprint(\"MSE val;{}\".format(mean_squared_error(y_val, y_val_pred_x)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_x)))\nprint(\"R2 score val:{}\".format(r2_score(y_val, y_val_pred_x)))","3f9bc029":"# Create instance\nlgbm = lgb.LGBMRegressor()\n\nparams = {'learning_rate': [0.01,0.05, 0.08], 'max_depth': [10, 15, 20, 25]}\n\n# Fitting\ncv_lg = GridSearchCV(lgbm, params, cv = 10, n_jobs =1)\ncv_lg.fit(X_train, y_train)\n\nprint(\"Best params:{}\".format(cv_lg.best_params_))\n\nbest_lg = cv_lg.best_estimator_\n\n# prediction\ny_train_pred_lg = best_lg.predict(X_train)\ny_val_pred_lg = best_lg.predict(X_val)\n\n# prediction\ny_train_pred_lg = cv_lg.predict(X_train)\ny_val_pred_lg = cv_lg.predict(X_val)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_lg)))\nprint(\"MSE val;{}\".format(mean_squared_error(y_val, y_val_pred_lg)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_lg)))\nprint(\"R2 score val:{}\".format(r2_score(y_val, y_val_pred_lg)))","ca2c0931":"# Create instance\ngbm = GradientBoostingRegressor(random_state=10)\n\nparams = {'max_depth': [2, 3, 5], 'learning_rate': [0.05, 0.1, 0.15, 0.2]}\n\n# Fitting\ncv_g = GridSearchCV(gbm, params, cv = 10, n_jobs =1)\ncv_g.fit(X_train, y_train)\n\nprint(\"Best params:{}\".format(cv_g.best_params_))\n\nbest_g = cv_g.best_estimator_\n\n# prediction\ny_train_pred_g = cv_g.predict(X_train)\ny_val_pred_g = cv_g.predict(X_val)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_g)))\nprint(\"MSE val;{}\".format(mean_squared_error(y_val, y_val_pred_g)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_g)))\nprint(\"R2 score val:{}\".format(r2_score(y_val, y_val_pred_g)))","b9f12081":"# Training and score\nridge = Ridge()\nparams = {'alpha': [1000, 100, 10, 1, 0.1, 0.01, 0.001]}\n\n# Fitting\ncv_r = GridSearchCV(ridge, params, cv = 10, n_jobs =1)\ncv_r.fit(X_train_sc, y_train)\n\nprint(\"Best params:{}\".format(cv_r.best_params_))\n\nbest_r = cv_r.best_estimator_\n\n# prediction\ny_train_pred_r = best_r.predict(X_train_sc)\ny_val_pred_r = best_r.predict(X_val_sc)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_r)))\nprint(\"MSE val;{}\".format(mean_squared_error(y_val, y_val_pred_r)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_r)))\nprint(\"R2 score val:{}\".format(r2_score(y_val, y_val_pred_r)))","01706cd0":"# Training and score\nlasso = Lasso()\nparams = {'alpha': [1000, 100, 10, 1, 0.1, 0.01, 0.001]}\n\n# Fitting\ncv_l = GridSearchCV(lasso, params, cv = 10, n_jobs =1)\ncv_l.fit(X_train_sc, y_train)\n\n\nprint(\"Best params:{}\".format(cv_l.best_params_))\n\nbest_l = cv_l.best_estimator_\n\n# prediction\ny_train_pred_l = best_l.predict(X_train_sc)\ny_val_pred_l = best_l.predict(X_val_sc)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_l)))\nprint(\"MSE val;{}\".format(mean_squared_error(y_val, y_val_pred_l)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_l)))\nprint(\"R2 score val:{}\".format(r2_score(y_val, y_val_pred_l)))","ad4acea7":"# Training and score\nelas = ElasticNet()\nparams = {'alpha': [1000, 100, 10, 1, 0.1, 0.01, 0.001]}\n\n# Fitting\ncv_e = GridSearchCV(elas, params, cv = 10, n_jobs =1)\ncv_e.fit(X_train_sc, y_train)\n\nprint(\"Best params:{}\".format(cv_e.best_params_))\n\nbest_e = cv_e.best_estimator_\n\n# prediction\ny_train_pred_e = best_e.predict(X_train_sc)\ny_val_pred_e = best_e.predict(X_val_sc)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_e)))\nprint(\"MSE val;{}\".format(mean_squared_error(y_val, y_val_pred_e)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_e)))\nprint(\"R2 score val:{}\".format(r2_score(y_val, y_val_pred_e)))","62d6be9e":"plt.figure(figsize=(10,6))\nplt.scatter(y_val_pred_f, y_val_pred_f - y_val, c=\"blue\", marker='o', alpha=0.5, label=\"RandomForest\")\nplt.scatter(y_val_pred_x, y_val_pred_x - y_val, c=\"red\", marker='o', alpha=0.5, label=\"XGB\")\nplt.scatter(y_val_pred_g, y_val_pred_g - y_val, c=\"purple\", marker='o', alpha=0.5, label=\"GBR\")\nplt.scatter(y_val_pred_lg, y_val_pred_lg - y_val, c=\"pink\", marker='o', alpha=0.5, label=\"LGBM\")\nplt.scatter(y_val_pred_r, y_val_pred_r - y_val, c=\"green\", marker='o', alpha=0.5, label=\"Rigde\")\nplt.scatter(y_val_pred_l, y_val_pred_l - y_val, c=\"orange\", marker='o', alpha=0.5, label=\"Lasso\")\nplt.scatter(y_val_pred_e, y_val_pred_e - y_val, c=\"gray\", marker='o', alpha=0.5, label=\"Elastic Net\")\n\nplt.xlabel('Predicted values')\nplt.ylabel('Residuals')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin = 4.6, xmax = 5.7, lw = 2, color = 'black')\n#plt.xlim([-10, 50])","97319f38":"# Create instance\ntsne = TSNE(n_components=2, random_state=10)\n\n# Figging\ntsne.fit(X_train_sc)\n\n# Fit transform\ntsne_X_train = tsne.fit_transform(X_train_sc)\ntsne_X_test = tsne.fit_transform(X_test_sc)\n\n# Visualization by plot\nx1 = tsne_X_train[:,0]\ny1 = tsne_X_train[:,1]\n\nx2 = tsne_X_test[:,0]\ny2 = tsne_X_test[:,1]\n\nplt.figure(figsize=(10,8))\nplt.scatter(x1, y1, c=\"blue\", label=\"train_data\")\nplt.scatter(x2, y2, c=\"green\", label=\"test_data\")\nplt.xlabel(\"iso_axis1\")\nplt.ylabel(\"iso_axis2\")\nplt.legend()","8c6d413e":"## Test prediction\ny_test_pred_f = best_f.predict(X_test)\ny_test_pred_x = best_x.predict(X_test)\ny_test_pred_lg = best_lg.predict(X_test)\ny_test_pred_g = best_g.predict(X_test)\ny_test_pred_r = best_r.predict(X_test_sc)\ny_test_pred_l = best_l.predict(X_test_sc)\ny_test_pred_e = best_e.predict(X_test_sc)\n\n# submit prediction\ny_submit = 10**((y_test_pred_f*0.05 + y_test_pred_x*0.05 + y_test_pred_lg*0.05 + y_test_pred_g*0.05 + y_test_pred_r*0.25 + y_test_pred_l*0.25 + y_test_pred_e*0.30))","ceee5cbb":"# submit data\nprint(\"average:{}\".format(round(y_submit.mean(),0)))\nprint(\"std:{}\".format(round(y_submit.std(),0)))\nprint(\"max:{}\".format(round(y_submit.max(),0)))\nprint(\"min:{}\".format(round(y_submit.min(),0)))","1d9ede21":"# submit dataframe\nsubmit = pd.DataFrame({\"Id\":df_test[\"Id\"], \"SalePrice\":y_submit})","8bbc387d":"submit.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","985ff74c":"## LightGBM","fdf79db3":"### Check correlation","70f7230b":"## Train data and Test data comparison with tSNE","89f08199":"## Test data prediction","8d633fbf":"Before constructing the prediction model, is the created feature value overlapped with the training data and the val data? Confirm by dimension reduction.","11544d20":"Logarithmize the feature quantity with uneven distribution.","35b5c212":"Looking at the results, the results at> 0.3 and <-0.3 have the best scores. Moreover, the degree of over fitting is suppressed, and it can be said that it is most suitable. In this result, the overall accuracy is low, because it does not include variables other than categorical variables, and we will include them in the next step after this analysis.","84835b35":"## GBR","90b47b30":"### EDA\nNumerical values EDA<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/house-prices-eda-and-dimension-reduction<br>\nCategorical values EDA<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/house-prices-eda-for-categorical-data","99b67d52":"### t-SNE pre check\nBefore the prediction of Test data, feature space comparison is performed with tSNE. The results are Test data, some of which are outside the Train data. It is speculated that this would be difficult to obtain as high a prediction as the val data. In order to improve the prediction accuracy, it is necessary to create features that overlap feature spaces or to improve the generalization performance of the model.<br>\nHow much can the model made this time estimate for outer prediction? Will be tried.\n\n","07493686":"Separate columns to numerical and categorical","f9211b2f":"* ## Random forest Regression","9ac87209":"The criteria was set to 0.005, and it was decided to give more features to learning.The variables can be cut in half.","76a01350":"## Validation","5de789da":"## Elastic Net","dbcb70f8":"### Libraries","771b4699":"Target value change to log value<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/house-prices-eda-and-rf-xgb-ensemble-prediction","72e40225":"### Create submission data","8d9a691f":"## Combine each completed data frame","b3a34c21":"## XGB regressor","5d5945ba":"Null values have the information that the spec is none or no value. It may be the method predict value if there is not, this time i take priority on data distribution, filled by median.","ec84877c":"## Lasso regression","810bc3c7":"## Data preprocessing","298d8548":"As confirmed by the visualization by tSNE, the model trained on the training data can predict relatively good val data.<br>\nElastic Net has the best results. However, other methods have achieved high results of 0.87 to 0.89 <br>\nFor the prediction of test data, I decided each params weight for ensemble, RandomForest = 0.1, others = 0.15.","734a52b4":"### Data loading","a932191a":"This time I tried to predict Sale Price by emsemble.<br>\nAnd I validated features with t-SNE dimension reduction before predicting.<br>\nMachine learning method used is folllowong<br>\n- Random Forest\n- XGBoost\n- LightGBM\n- GBR\n- Ridge regression\n- Lasso regression\n- Elastic Net","7a96bbd9":"### Data chacking<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/house-prices-eda-and-dimension-reduction","f1b064f2":"## Feature importance\nIn order to determine the variables, we classify the prices in a random forest and determine the important features.","ed7c3e61":"The results of dimension reduction by tSNE are plotted. Looking at the figure, the val data is included inside the train data, and there is an interpolating relationship in the feature space, so high prediction accuracy is expected. However, because the training data is located in a sparse location, how does this affect it? Execute and confirm.","cacb752a":"Result of categorical values","011aefa1":"Each variable shows a non-correlated or positive correlation with the price. We assumed that the missing value did not have that specification, and same as nymerical value, all categorical data filled by median.","2a2a69d0":"The distribution of residuals is uniform for all models.","9325a295":"### Categorical values preprocessing<br>\n\u2193Pre study<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/house-prices-eda-for-categorical-data","4f1c1ed8":"## Data preprocessing","1612cb34":"# Ensemble prediction, Sale Price","a04dea2d":"### EDA and Data preprocessing past posts\nNumerical values preprocessing<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/house-prices-eda-and-dimension-reduction<br>\nCategorical values preprocessing<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/house-prices-eda-for-categorical-data","dfc923cf":"### Numerical values preprocessing\n\u2193Pre study<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/house-prices-eda-and-dimension-reduction","e4ebaa91":"## Ridge regression"}}