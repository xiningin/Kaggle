{"cell_type":{"327cb6a5":"code","2ce0d926":"code","4e8782c3":"code","ae22b89c":"code","f345a5ef":"code","288468df":"code","e8dac0f8":"code","41cbdadd":"code","432aa687":"code","d1cdba33":"code","8c719503":"code","d8afc098":"code","6e82f0f8":"code","b30535be":"code","a66acb07":"code","7ad2db32":"code","0b5f9291":"code","d08140ac":"code","67d2573a":"code","5b3e4339":"code","a6e38115":"code","e7132c78":"code","1810c1fc":"code","a98e7b27":"code","05f99c81":"code","adb59b1e":"code","2f5fab7e":"code","275751b9":"code","8ec89139":"code","55ffd302":"code","3ebac3f8":"code","2188255f":"code","841c9c4c":"code","33299bee":"code","5544fea8":"code","a0df9956":"code","b20a4847":"code","ad0d5d93":"code","35ad994a":"code","e6b62974":"code","da07d73b":"code","0a072f95":"code","63a67497":"code","6c94b674":"code","c7fafc74":"code","18907f17":"code","0f51341c":"code","b19b8cdf":"code","25349dd1":"code","ed6ea980":"code","151995b5":"code","b2827228":"code","4f6e13be":"code","9c8e4028":"code","705412ac":"code","19e1e65b":"code","dca34829":"code","838518c3":"code","68c2aa1b":"code","6dcf2bfe":"code","571a500f":"code","1b9b85a7":"code","4e8bfb62":"code","b21223dc":"code","b4d85489":"code","d261afa8":"code","8c4177fb":"code","75f810e3":"code","d2211282":"code","30932ca5":"code","f9577219":"code","c6cb5fb7":"code","d1c28c7d":"code","6eb4c269":"code","dd08bd23":"code","d2c125f9":"code","54b889c2":"code","4e89c81c":"code","743d9f94":"code","d464068f":"code","77dd8b7d":"code","42f51d46":"code","f9e972b8":"code","3d838a5a":"code","8f38fb3e":"code","0d072e78":"code","e35ed215":"markdown","47c1dce1":"markdown","c5e4355f":"markdown","0957ddc9":"markdown","e0705f0f":"markdown","dd3a53b2":"markdown","39cd9bca":"markdown","6ab05fe4":"markdown","22ba79d3":"markdown","3b4c1065":"markdown","fb67037f":"markdown","b10c2bb4":"markdown","b71830b6":"markdown","696fc09b":"markdown","ea24d6ef":"markdown","965ebc08":"markdown","6c7eb980":"markdown","ea379222":"markdown","a6d98b19":"markdown","81193eb4":"markdown","44967ec7":"markdown","a3aa056e":"markdown","1773d0fc":"markdown","9dbe5a72":"markdown","483ea6bf":"markdown","805bcce3":"markdown","d3167a2e":"markdown","af628175":"markdown","6a197ebe":"markdown","4f44b548":"markdown","2f57e505":"markdown","cd0b5ec6":"markdown","ced21b8f":"markdown","10faaeb7":"markdown","79836981":"markdown","48c9b0ef":"markdown"},"source":{"327cb6a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2ce0d926":"# To check all the columns present\npd.set_option('display.max_columns', 500)","4e8782c3":"! unzip \/kaggle\/input\/sberbank-russian-housing-market\/train.csv.zip","ae22b89c":"! unzip \/kaggle\/input\/sberbank-russian-housing-market\/test.csv.zip","f345a5ef":"! ls \/kaggle\/working","288468df":"train_df = pd.read_csv(f\"\/kaggle\/working\/train.csv\")\ntest_df = pd.read_csv(f\"\/kaggle\/working\/test.csv\")\nprint(f\"train data shape:- {train_df.shape}\")\nprint(f\"test data shape:- {test_df.shape}\")","e8dac0f8":"train_df['timestamp'] =pd.to_datetime(train_df.timestamp)\ntest_df['timestamp'] =pd.to_datetime(test_df.timestamp)\ntrain_df.head()","41cbdadd":"train_df = train_df.sort_values(by=['timestamp'])\ntrain_df.head()","432aa687":"test_df.head()","d1cdba33":"train_df['year'] = pd.DatetimeIndex(train_df['timestamp']).year\ntrain_df['month'] = pd.DatetimeIndex(train_df['timestamp']).month\ntest_df['year'] = pd.DatetimeIndex(test_df['timestamp']).year\ntest_df['month'] = pd.DatetimeIndex(test_df['timestamp']).month\ntrain_df['day'] = pd.DatetimeIndex(train_df['timestamp']).day\ntrain_df['week'] = pd.DatetimeIndex(train_df['timestamp']).week\ntest_df['day'] = pd.DatetimeIndex(test_df['timestamp']).day\ntest_df['week'] = pd.DatetimeIndex(test_df['timestamp']).week","8c719503":"# Groupby year in price mean\nmean_year_df = train_df.groupby(\"year\")[\"price_doc\"].agg(\"mean\").reset_index()\nplt.figure(figsize=(8,6))\nplt.scatter(range(mean_year_df.shape[0]), mean_year_df.price_doc.values)\nplt.xlabel('index', fontsize=12)\nplt.ylabel('price', fontsize=12)\nplt.show()","d8afc098":"# Groupby month on price\nmean_month_df = train_df.groupby(\"month\")[\"price_doc\"].agg(\"mean\").reset_index()\nplt.figure(figsize=(8,6))\nplt.scatter(range(mean_month_df.shape[0]), mean_month_df.price_doc.values)\nplt.xlabel('index', fontsize=12)\nplt.ylabel('price', fontsize=12)\nplt.show()","6e82f0f8":"# groupby year and month on price mean\nmean_year_month_df = train_df.groupby([\"year\", \"month\"])[\"price_doc\"].agg(\"mean\").reset_index()\nplt.figure(figsize=(8,6))\nplt.scatter(range(mean_year_month_df.shape[0]), mean_year_month_df.price_doc.values)\nplt.xlabel('index', fontsize=12)\nplt.ylabel('price', fontsize=12)\nplt.show()","b30535be":"mean_month_year_df = train_df.groupby([\"month\", \"year\"])[\"price_doc\"].agg(\"mean\").reset_index()\nplt.figure(figsize=(8,6))\nplt.scatter(range(mean_month_year_df.shape[0]), mean_month_year_df.price_doc.values)\nplt.xlabel('index', fontsize=12)\nplt.ylabel('price', fontsize=12)\nplt.show()","a66acb07":"\ntrain_df.plot.scatter(x='product_type', y='price_doc')\n","7ad2db32":"\ntrain_df.plot.scatter(x='num_room', y='price_doc')","0b5f9291":"train_df.shape","d08140ac":"train_df = train_df[(train_df[\"num_room\"]<10) | (train_df[\"num_room\"].isnull())]\ntrain_df.plot.scatter(x='num_room', y='price_doc')","67d2573a":"train_df.shape","5b3e4339":"train_df = train_df.drop([\"id\", \"timestamp\"], 1)\ntest_df = test_df.drop([\"id\", \"timestamp\"], 1)\n","a6e38115":"# Separating the target price as series\ntarget = train_df[\"price_doc\"]\n#train_df = train_df.drop([\"price_doc\"], 1)\ntrain_df.shape","e7132c78":"target ","1810c1fc":"plt.figure(figsize=(8,6))\nplt.scatter(range(target.shape[0]), np.sort(target.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('price', fontsize=12)\nplt.show()","a98e7b27":"import matplotlib.pyplot as plt\nplt.figure(figsize=(8,6))\nplt.scatter(range(target.shape[0]), target.values)\nplt.xlabel('index', fontsize=12)\nplt.ylabel('price', fontsize=12)\nplt.show()","05f99c81":"#merged_df = pd.concat([train_df,test_df])","adb59b1e":"train_df.info()","2f5fab7e":"# All cols present\nall_cols = train_df.columns\n# numerical columns\nnum_and_float_cols = train_df._get_numeric_data().columns\n# categories columns\nobject_cols = list(set(all_cols) - set(num_and_float_cols))\nprint(f\"total numeric cols :- {len(num_and_float_cols)}, categorical cols:- {len(object_cols)}\")","275751b9":"# Filtering out all the columns which contains some nan values\nnull_cols_val = {all_cols[col_idx]:val for col_idx, val in enumerate(train_df.isnull().sum()) if val>0}\nnull_cols = [i[0] for i in null_cols_val.items() ]\n# List of all columns having null value\nnull_cols","8ec89139":"train_df.head()","55ffd302":"corelated_df = train_df[num_and_float_cols].corr()#.reset_index()","3ebac3f8":"corelated_df.head()","2188255f":"price_important_feat = corelated_df.loc[\"price_doc\"].to_dict()","841c9c4c":"# Plot of all corelated variables\nplt.figure(figsize=(50,50))\nplt.bar(range(len(price_important_feat)), list(price_important_feat.values()), align='center')\nplt.xticks(range(len(price_important_feat)), list(price_important_feat.keys()))\nplt.show()","33299bee":"#correlated_features = set()\n# Corelated sets\ncorrelated_features = {}\n # Featues that is related to price\nfeatures_related_to_price = []\ncorr_cols = corelated_df.columns\nalready_done = []\nfor i in range(len(corr_cols)):\n    correlated_features[corr_cols[i]] = []\n    for j in range(len(corr_cols)):\n        if  i!=j and ([i,j] not in already_done or [j,i] not in already_done)  and abs(corelated_df.iloc[i, j]) > 0.8:\n            already_done.append([i,j])\n            if corr_cols[i]==\"price_doc\":\n                features_related_to_price.append([corr_cols[j],corelated_df.iloc[i, j]])\n            elif corr_cols[j]==\"price_doc\":\n                features_related_to_price.append([corr_cols[i],corelated_df.iloc[i, j]])    \n            else:    \n                #correlated_features.add(corr_cols[i])\n                correlated_features[corr_cols[i]].append(corr_cols[j])\n    if not correlated_features[corr_cols[i]]:\n        del correlated_features[corr_cols[i]]\n           ","5544fea8":"value_correlated_features = []\nfor kv in correlated_features.items():\n    value_correlated_features.extend(kv[1])\nfinal_corelated_sets = []    \nrestricted_sets = set(value_correlated_features)\nfor kv in correlated_features.items():\n    if kv[0] not in restricted_sets:\n\n        restricted_sets.add(kv[0])","a0df9956":"# Uncomment to check features\n#corelated_sets_to_remove","b20a4847":"len(restricted_sets)\ncorelated_sets_to_remove = list(restricted_sets)\ntrain_df = train_df.drop(corelated_sets_to_remove,axis=1)\ntest_df = test_df.drop(corelated_sets_to_remove,axis=1)","ad0d5d93":"new_cols_list = train_df.columns\nprint(train_df.shape)\nnum_and_float_cols = [col for col in num_and_float_cols if col in new_cols_list]\nobject_cols = [col for col in object_cols if col in new_cols_list]","35ad994a":"limited_price_important_feat = {i[0]:i[1] for i in price_important_feat.items() if i[0] not in corelated_sets_to_remove+[\"price_doc\"]}\nplt.figure(figsize=(20,20))\nplt.barh(*zip(*limited_price_important_feat.items()))\n# plt.bar(range(len(limited_price_important_feat)), list(limited_price_important_feat.values()), align='center')\n# plt.xticks(range(len(limited_price_important_feat)), list(limited_price_important_feat.keys()))\nplt.show()","e6b62974":"train_df[object_cols[-5]]","da07d73b":"len(object_cols), len(num_and_float_cols)","0a072f95":"for col in num_and_float_cols:\n    if col in null_cols:\n        print(col)\n        plt.figure(figsize=(10,6))\n        sns.distplot(train_df[col].values, bins=50, kde=True)\n        plt.xlabel(col, fontsize=12)\n        plt.show()","63a67497":"train_df.head()","6c94b674":"# Converting continuous variables into limited bins bases on quantiles\nfrom sklearn.preprocessing import KBinsDiscretizer\n\nfeatures_to_bin = [\"industrial_km\", \"big_market_km\", \"market_shop_km\", \"church_synagogue_km\", \"incineration_km\", \"big_road1_km\", \"bus_terminal_avto_km\", \"mosque_km\"]\nbinned_features = []\ndef binning():\n    global binned_features\n    for feature in features_to_bin:\n        binf = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n        binf = binf.fit(train_df[feature].values.reshape(-1,1))#.astype(int)\n        train_df[f\"{feature}_bin\"] = binf.transform(train_df[feature].values.reshape(-1,1)).astype(int)\n        test_df[f\"{feature}_bin\"] = binf.transform(test_df[feature].values.reshape(-1,1)).astype(int)\n        binned_features.append(f\"{feature}_bin\")\n\n         \nbinning()","c7fafc74":"train_df.head()","18907f17":"def feature_engneering(data):\n    numeric_data_added = []\n    categoric_data_added = []\n    \n    \n    # When someone buys a home for living he makes sure, school is nearby, hospital is nearby\n    # metro is nearbuy, market is nearby, water is nearby\n    data[\"sub_area_hospital_centres\"] = data[\"sub_area\"] + data[\"healthcare_centers_raion\"].astype(\"str\")\n    categoric_data_added.append(\"sub_area_hospital_centres\")\n    data[\"sub_area_school\"] = data[\"sub_area\"] + data[\"school_education_centers_top_20_raion\"].astype(\"str\")\n    categoric_data_added.append(\"sub_area_school\")\n    data[\"sub_area_market\"] = data[\"sub_area\"] + data[\"big_market_raion\"].astype(\"str\")\n    categoric_data_added.append(\"sub_area_market\")\n    data[\"sub_area_metro\"] = data[\"sub_area\"] + data[\"ID_metro\"].astype(\"str\")\n    categoric_data_added.append(\"sub_area_metro\")\n    for feature in binned_features:\n        data[f\"sub_area_{feature}\"] = data[\"sub_area\"] + data[feature].astype(\"str\")\n        categoric_data_added.append(f\"sub_area_{feature}\")\n        \n    return data, categoric_data_added, numeric_data_added\n    \n    \ntrain_df, categoric_data_added, numeric_data_added = feature_engneering(train_df) \ntest_df, categoric_data_added, numeric_data_added = feature_engneering(test_df) \n","0f51341c":"#err","b19b8cdf":"for col in num_and_float_cols:\n    if col in null_cols:\n        #print(null_cols_val[col])\n        #print(abs(price_important_feat[col]))\n        train_df[col].fillna(-1, inplace=True)\n        test_df[col].fillna(-1, inplace=True)","25349dd1":"# Import label encoder\n#from sklearn import preprocessing\n#label_encoder = preprocessing.LabelEncoder()\nfrom category_encoders import TargetEncoder\n \nfor col in object_cols+categoric_data_added:\n    if col in null_cols:\n        train_df[col].fillna(f\"{col}nan\", inplace=True)\n        test_df[col].fillna(f\"{col}nan\", inplace=True)\n    encoder = TargetEncoder()\n    encoder = encoder.fit(train_df[col], train_df['price_doc'])\n    train_df[col] = encoder.transform(train_df[col])\n    test_df[col] = encoder.transform(test_df[col])\n#     label_encoder= label_encoder.fit(train_df[col])\n#     train_df[col] = label_encoder.transform(train_df[col])\n#     test_df[col] = label_encoder.transform(test_df[col])","ed6ea980":"train_df.head()","151995b5":"# for cols in num_and_float_cols:\n#     fig = plt.figure(figsize =(5, 3))\n\n#     # Creating plot\n#     plt.boxplot(train_df[cols].values)\n\n#     # show plot\n#     plt.show()","b2827228":"# Get all the X variables\nX = train_df.drop([\"price_doc\"], 1)\n","4f6e13be":"target = target#.iloc[:10000]","9c8e4028":"#X.info()","705412ac":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\nX_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2, random_state=0)\n","19e1e65b":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","dca34829":"logreg = LinearRegression(n_jobs=-1)\nlogreg.fit(X_train, y_train)","838518c3":"y_pred_log = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","68c2aa1b":"y_pred_test = logreg.predict(test_df)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","6dcf2bfe":"from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n","571a500f":"mean_squared_error(y_pred_log, y_test)","1b9b85a7":"mean_absolute_error(y_pred_log, y_test)","4e8bfb62":"# Train XGBoost model and validate results\n\nimport xgboost as xgb\nfrom sklearn import metrics\nclf = xgb.XGBRegressor(n_estimators=150, max_depth=7, learning_rate=0.01, min_child_weight=20)\nclf.fit(X_train, y_train)\n\nprint(metrics.mean_squared_error(y_test, clf.predict(X_test))**0.5)","b21223dc":"# Plot importances of XGBoost model\n# Some of created features can be noticed in top 50 important features!\nfig, ax = plt.subplots(1, 1, figsize=(8, 16))\nxgb.plot_importance(clf, max_num_features=50, height=0.5, ax=ax);","b4d85489":"# Plot true values vs preicted ones\n\nplt.scatter(y_train, clf.predict(X_train), alpha=0.3, c='red')\nplt.scatter(y_test, clf.predict(X_test), alpha=0.3, c='blue');\nplt.xlabel('true values')\nplt.ylabel('predicted values')\nplt.axis([13,19,13,19])\nplt.plot([13,19],[13,19]);\n","d261afa8":"import xgboost as xgb\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)","8c4177fb":"from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n","75f810e3":"import numpy as np\n# \"Learn\" the mean from the training data\nmean_train = np.mean(y_train)\n# Get predictions on the test set\nbaseline_predictions = np.ones(y_test.shape) * mean_train\n# Compute MAE\nmae_baseline = mean_squared_error(y_test, baseline_predictions)\nprint(\"Baseline MAE is {:.2f}\".format(mae_baseline))","d2211282":"# I chose this parameters for initial testing.\n# We can also use gridsearchcv or randomsearch to select different best features \nparams = {'eta': 0.05, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, 'silent':1,\n          'min_child_weight': 1, 'gamma': 0, 'objective': 'reg:linear', 'eval_metric': 'rmse'} # default params","30932ca5":"#params['eval_metric'] = \"rmse\"\n","f9577219":"num_boost_round = 999\n","c6cb5fb7":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=20\n)","d1c28c7d":"#https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\nxgb_cv = model.predict(dtest)\nmean_absolute_error(xgb_cv, y_test)\n","6eb4c269":"r2_score(xgb_cv, y_test)\n","dd08bd23":"mean_squared_error(model.predict(dtest), y_test)","d2c125f9":"# !pip install --upgrade tensorflow\n# !pip install --upgrade tensorflow-gpu","54b889c2":"#! pip install keras","4e89c81c":"import numpy\nimport matplotlib.pyplot as plt\nimport pandas\nimport math\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","743d9f94":"# normalize the dataset\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n# normalize the dataset\n#scaler = MinMaxScaler(feature_range=(0, 1))\n#dataset = scaler.fit_transform(dataset)\nX_train_new = np.reshape(X_train, X_train.shape + (1,))\nX_test_new = np.reshape(X_test, X_test.shape + (1,))","d464068f":"# Reason I used Dense + LSTM because it's capturing better result.\n# Optimiser is adam\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(X_train.shape[1], 1)))\nmodel.add(Dense(50))\nmodel.add(Dense(25))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(X_train_new, y_train, epochs=1000, batch_size=64, verbose=2)","77dd8b7d":"cv_predict = model.predict(X_test_new)\n","42f51d46":"fig, axs = plt.subplots(figsize=(20,20), sharey=True)\nplt.title('output plot')\naxs.scatter(list(range(len(y_test))),cv_predict, color=\"red\")\naxs.scatter(list(range(len(y_test))), y_test, color=\"blue\")","f9e972b8":"mean_absolute_error(cv_predict, y_test)\n","3d838a5a":"mean_squared_error(cv_predict, y_test)","8f38fb3e":"! pip install prettytable","0d072e78":"from prettytable import PrettyTable\nx=PrettyTable()\nx.field_names = [\"Model\", \"mean_squared_error\", \"mean_absolute_error\"]\n\nx.add_row([\"Lstm\", f\"{mean_squared_error(cv_predict, y_test)}\", f\"{mean_absolute_error(cv_predict, y_test)}\"])\nx.add_row([\"Xgboost\", f\"{mean_squared_error(xgb_cv, y_test)}\", f\"{mean_absolute_error(xgb_cv, y_test)}\"])\nx.add_row([\"linear regression\", f\"{mean_squared_error(y_pred_log, y_test)}\", f\"{mean_absolute_error(y_pred_log, y_test)}\"])\n\nprint(x)","e35ed215":"# Description\n\nHousing costs demand a significant investment from both consumers and developers. And when it comes to planning a budget\u2014whether personal or corporate\u2014the last thing anyone needs is uncertainty about one of their biggets expenses. Sberbank, Russia\u2019s oldest and largest bank, helps their customers by making predictions about realty prices so renters, developers, and lenders are more confident when they sign a lease or purchase a building.\n\nAlthough the housing market is relatively stable in Russia, the country\u2019s volatile economy makes forecasting prices as a function of apartment characteristics a unique challenge. Complex interactions between housing features such as number of bedrooms and location are enough to make pricing predictions complicated. Adding an unstable economy to the mix means Sberbank and their customers need more than simple regression models in their arsenal.\n\nIn this competition, Sberbank is challenging Kagglers to develop algorithms which use a broad spectrum of features to predict realty prices. Competitors will rely on a rich dataset that includes housing data and macroeconomic patterns. An accurate forecasting model will allow Sberbank to provide more certainty to their customers in an uncertain economy.","47c1dce1":"> From above we can see that price is maximum b\/w 2nd to 4th month","c5e4355f":"**Based on experience onehot encoding should have worked better but that will increase the number of features as well. So I decided to target encoding(Just for testing)**","0957ddc9":"**Numeric univariate plote and its distribution in space**","e0705f0f":"# Multicolinear check starts here","dd3a53b2":"> From above price plot we can see that mean price has increased every year\n\n> We also check median price and how much there is a difference in price every year","39cd9bca":"# Unzip Files","6ab05fe4":"# Evaluation\n\n> I chose mean_squared_error as metric because it is generally used when large errors are particularly undesirable. Like in this case we are tend to get large errors.\n\n> 2nd metric i tried on in mean absolute error.\n\n> Lstm + dense model tend to have worked better","22ba79d3":"> Semms like there are some outliers in price as well. But let's model it without filtering it first","3b4c1065":"# Bin some of the features","fb67037f":"**Train test split data into train 80% and cv*(X_test) 20%**","b10c2bb4":"**Uncomment below codes to get univariate box plot to remove outliers if any. Sicnce Currenty I am going with outliers also. So commented it**","b71830b6":"> Seems like people tend to pay more when they are buying for investment purpose","696fc09b":"# Feature Engineering","ea24d6ef":"# Things left in time series.\n\n> Prepare the data accordingly considering all previous results and predicting future values Like below link\n\n> https:\/\/towardsdatascience.com\/simple-multivariate-time-series-forecasting-7fa0e05579b2","965ebc08":"**Sorting the training data based on time stamp because house prices does increases over time**","6c7eb980":"> This is how price has increased over the years, not filtering any of the price for now after sorting","ea379222":"> corelated_sets_to_remove these are the inter corelated sets which is intercorelated and more than 80%","a6d98b19":"# Xgboost","81193eb4":"# Merging both train and test for preprocessing","44967ec7":"# Simple LSTM time series(Browser started to get freezed at this point)","a3aa056e":"> With increse in num rooms price increases\n\n> Here are some num rooms greater than 10 but prices are very less, these seems like an outlier we can define remove them","1773d0fc":"# Note: Have done some basic Feature Engineering. Could have been extended to few more featues but I am stopping with these features only","9dbe5a72":"**Reading train and test data**","483ea6bf":"**without sorting**","805bcce3":"**corelation free depandend variable and plots**","d3167a2e":"# Comparision","af628175":"> Seems like num_rooms is contributing more than full_sq followed by others","6a197ebe":"**Lets dig deep into o\/p variable price_doc**","4f44b548":"**Note: Same way we can do multivariate analysis for other categorial variable also**","2f57e505":"*datetime column into timestamp*","cd0b5ec6":"**filling null values with -1, for continus we can also try, mean, median values. I used -1 just to separate this feature and it will also contribute less while training**","ced21b8f":"# Modelling","10faaeb7":"> Above graph represents how the price has increased over the year and month","79836981":"**Training with few epochs and layers because of hanging issue**","48c9b0ef":"**https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f**"}}