{"cell_type":{"319b93bf":"code","5464de96":"code","d05a5cc1":"code","01a8d458":"code","d801b83a":"code","6cc6b801":"code","5d34d992":"code","e044b2a6":"code","a9405594":"code","967fe328":"code","7afe513f":"code","e7e02896":"code","70c8c70e":"code","9f40bd71":"code","906423b5":"code","e3b17395":"code","0b9b3d0b":"code","0daf1e04":"code","090a2541":"code","f836d5cf":"code","ecb32d3e":"code","44b669ef":"code","1af972ba":"code","1872ba3b":"code","1a4810f9":"code","ff04938d":"code","a3f8f838":"code","8b4901e9":"code","c6a6e1d1":"code","37601477":"code","52335be4":"code","9b7a8a89":"code","8f0c8d2a":"code","edbb2645":"code","e41d420c":"code","da905521":"code","8d0fe8ad":"code","c0d2330b":"code","29824219":"code","0c6526da":"code","ca47f231":"code","7b6a9ec2":"code","03de1a7c":"code","9562bf39":"code","d9bc08cc":"code","5c77bbef":"code","7a52b46e":"markdown","11d200c2":"markdown","b3d4b6ac":"markdown","46806762":"markdown","6d20fff8":"markdown","5fe23743":"markdown","44342cd6":"markdown","9b42503a":"markdown","97592ee7":"markdown","8c5aac04":"markdown","eab05056":"markdown","ffdebd6d":"markdown","b0881079":"markdown","f1980b09":"markdown","a2575013":"markdown","472687a7":"markdown","592f72b9":"markdown","1ac07a42":"markdown","bb1c1372":"markdown","c1bf2077":"markdown","72fa33fd":"markdown","11418501":"markdown","140fcf28":"markdown","6427ea53":"markdown","d4fccc55":"markdown","dcb2bfca":"markdown","42b5010a":"markdown","9c9221d0":"markdown","30dad253":"markdown","9864653e":"markdown","171c0609":"markdown","11906dfa":"markdown","3cbe6877":"markdown","a1e4d256":"markdown","0a811e54":"markdown","50358320":"markdown","d34514bd":"markdown","90b7b840":"markdown","0d071671":"markdown","a53d85a2":"markdown","c224c20c":"markdown","a81c1d29":"markdown","97955203":"markdown","3b99b8a8":"markdown","0da31060":"markdown","042cfe8d":"markdown","f227d55f":"markdown","ec68668b":"markdown","2623d3b5":"markdown","96435d22":"markdown","3240d0f6":"markdown","f63b56d1":"markdown","71f73857":"markdown","ae7e1d91":"markdown","83780436":"markdown","2e8be7ca":"markdown","b5440c90":"markdown","89005061":"markdown","338719fe":"markdown","2b7e5ade":"markdown","f6e76cf3":"markdown","45e556c6":"markdown","aa5df2d1":"markdown","7e4c3eb4":"markdown","7ed7e811":"markdown","eefa0aaa":"markdown","1f309aaf":"markdown","1538d3b0":"markdown"},"source":{"319b93bf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5464de96":"df_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","d05a5cc1":"print('Shape of train dataset: {}'.format(df_train.shape))\nprint('Shape of test dataset: {}'.format(df_test.shape))","01a8d458":"df_train.head(15)","d801b83a":"df_train.describe()","6cc6b801":"df_test.describe()","5d34d992":"print(\"Train dataset consist missing values: \" + str(df_train.isnull().values.any()))\nprint(\"Number of missing values: \" + str(df_train.isnull().sum().sum()))\n\nprint(\"\\n{}\".format(df_train.isnull().sum()))","e044b2a6":"print(\"Train dataset consist missing values: \" + str(df_test.isnull().values.any()))\nprint(\"Number of missing values: \" + str(df_test.isnull().sum().sum()))\n\nprint(\"\\n{}\".format(df_test.isnull().sum()))","a9405594":"df_train.columns","967fe328":"df_num = df_train[['Age','SibSp','Parch','Fare']]\ndf_cat = df_train[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]","7afe513f":"df_num.hist(figsize=(25,5), layout=(1,4))","e7e02896":"corr = df_num.corr()\n\n# Generate a mask for the upper triangle \nmask = np.triu(np.ones_like(corr, dtype=bool), 1)\n\nsns.heatmap(corr, mask=mask, cmap='Blues', annot=True, fmt=\".2f\")\nplt.show()","70c8c70e":"pd.pivot_table(df_train, index='Survived', values=df_num.columns, aggfunc='mean')","9f40bd71":"plt.figure(figsize=(15,7.5))\nplt.ylim(-10, 530)\nplt.xlim(-1, 82)\nsns.scatterplot(x='Age', y='Fare', hue='Survived', data=df_train)","906423b5":"count_survivorship_plot = sns.countplot(x='Survived', data=df_cat)\ncount_survivorship_plot.set_xticklabels(['No','Yes'])","e3b17395":"count_class_plot = sns.countplot(x='Pclass', hue='Survived', data=df_cat)\ncount_class_plot.legend([\"Didn't survived\", 'Survived'])","0b9b3d0b":"count_gender_plot = sns.countplot(x='Sex', hue='Survived', data=df_cat)\ncount_gender_plot.legend([\"Didn't survived\", 'Survived'])","0daf1e04":"count_embarking_plot = sns.countplot(x='Embarked', hue='Survived', data=df_cat)\ncount_embarking_plot.set_xticklabels(['Southampton','Cherbourg','Queenstown'])\ncount_embarking_plot.legend([\"Didn't survived\", 'Survived'])","090a2541":"data = [df_train, df_test]","f836d5cf":"for dataset in data:\n    dataset.drop(['Ticket', 'Cabin'], inplace=True, axis=1)","ecb32d3e":"df_train.drop(['PassengerId'], inplace=True, axis=1)","44b669ef":"for dataset in data:\n    dataset['Title'] = dataset['Name'].apply(lambda x: x.split(', ')[1].split('.')[0])\n    dataset.drop(['Name'], inplace=True, axis=1)","1af972ba":"df_train['Title'].value_counts()","1872ba3b":"popular_titles = df_train['Title'].value_counts()[:4].index.tolist()\n\nfor dataset in data:\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Rare' if x not in popular_titles else x)","1a4810f9":"df_train['Title'].value_counts()","ff04938d":"for dataset in data:\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)","a3f8f838":"dataset.groupby(['Sex', 'Pclass'])['Age'].median()","8b4901e9":"for dataset in data:\n    dataset['Age'] = dataset.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","c6a6e1d1":"for dataset in data:\n    dataset['Fare'] = dataset.groupby(['Sex', 'Pclass'])['Fare'].apply(lambda x: x.fillna(x.median()))","37601477":"print(\"Train dataset consist missing values: \" + str(df_train.isnull().values.any()))\nprint(\"Train dataset consist missing values: \" + str(df_test.isnull().values.any()))","52335be4":"df_test.head()","9b7a8a89":"for dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 14, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 14) & (dataset['Age'] <= 20), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 24), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 24) & (dataset['Age'] <= 30), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 38), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 38) & (dataset['Age'] <= 50), 'Age'] = 5\n    dataset.loc[ dataset['Age'] > 50, 'Age'] = 6","8f0c8d2a":"df_train['Age'].value_counts()","edbb2645":"df_train['Fare'].describe()[3:]","e41d420c":"# boundaries of the bins\nbins = [0, 7.9104, 14.4542, 31.0, 512.3292]\n# labels of the bins\nlabels = [0, 1, 2, 3]\n\nfor dataset in data:\n    dataset['Fare'] = pd.cut(dataset['Fare'], bins, labels=labels)","da905521":"df_train.head()","8d0fe8ad":"# One hot encoding for df_train\ndummies = pd.get_dummies(df_train[['Sex', 'Embarked', 'Title']], drop_first=True)\ndf_train.drop(['Sex', 'Embarked', 'Title'], axis=1, inplace=True)\ndf_train = pd.concat([df_train, dummies], axis=1)\n\n# One hot encoding for df_test\ndummies = pd.get_dummies(df_test[['Sex', 'Embarked', 'Title']], drop_first=True)\ndf_test.drop(['Sex', 'Embarked', 'Title'], axis=1, inplace=True)\ndf_test = pd.concat([df_test, dummies], axis=1)","c0d2330b":"df_train.head()","29824219":"from sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import SGDClassifier","0c6526da":"X_train = df_train.drop(columns=['Survived'])\ny_train = df_train['Survived']","ca47f231":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn, X_train, y_train, cv=5)\n\nprint('Accuracy: ', round(cv.mean() * 100,3),'%')","7b6a9ec2":"lr = LogisticRegression(solver='liblinear')\ncv = cross_val_score(lr, X_train, y_train, cv=5)\n\nprint('Accuracy: ', round(cv.mean() * 100,3),'%')","03de1a7c":"rf = RandomForestClassifier()\ncv = cross_val_score(rf, X_train, y_train, cv=5)\n\nprint('Accuracy: ', round(cv.mean() * 100,3),'%')","9562bf39":"ab = AdaBoostClassifier()\ncv = cross_val_score(ab, X_train, y_train, cv=5)\n\nprint('Accuracy: ', round(cv.mean() * 100,3),'%')","d9bc08cc":"sgcd = SGDClassifier()\ncv = cross_val_score(sgcd, X_train, y_train, cv=5)\n\nprint('Accuracy: ', round(cv.mean() * 100,3),'%')","5c77bbef":"ab.fit(X_train, y_train)\nX_test = df_test.drop(columns=['PassengerId'])\npredictions = ab.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)","7a52b46e":"## 3.4. Encoding","11d200c2":"## 4.2. Submitting results","b3d4b6ac":"### Count survivorship","46806762":"Now, I'll use classification models to make predictions and evaluate them using prepared train dataset. To evaluate each model performance, I'll use cross-validation method.","6d20fff8":"Things covered in EDA chapter:\n1. Inspect first rows of datasets\n1. Meaning of the variables - Data Dictionary\n1. Inspect descriptive statistics of datasets\n1. Checking if datasets consist any missing values\n1. Numerical features histograms\n1. Correlation matrix heatmap for numerical values\n1. Average feature values depending on survival\n1. Survivorship depending on age and fare\n1. Count plots for categorical features","5fe23743":"**More advanced feature engineering ideas:**\n\nIf we would want to be more accurate, we could try to deep analyse every title and make more specifisc groups. \n\nWe could also change some 'rare' titles to already existing ones (e.g. French \"Mlle\" is equivalent to the English \"Miss\")","44342cd6":"From this plot we can see tendencies that young people (under 10 years old) and people who paid more for their tickets, tend to survive more often.","9b42503a":"### Inspect numerical and categorical features separately","97592ee7":"Most people didn't survive Titanic disaster","8c5aac04":"#### Test dataset","eab05056":"## 2.2. Meaning of the variables - Data Dictionary","ffdebd6d":"### 3.3.3. Fill Fare missing value","b0881079":"I also drop PassengerId from df_train dataset because there is no need for this information in the training data (in the testing data we will use it to prepare final submission file).","f1980b09":"People with higher class tickets has greater chance of surviving. Especially big difference is when changing class from the cheapest to medium.","a2575013":"## 3.3. Inputting missing values","472687a7":"**More advanced feature engineering ideas:**\n\n* We could search for some pattern in Ticket ID's (e.g. separate tickets with letters). \n* In Cabine column there are a lot of missing values, but we could try to group cabines by its first letter (maybe we should group all NaN's together?)","592f72b9":"I decided to input NaN values in Age column with median of corresponding \"Sex\",\"Pclass\" groups.\n\nAt first, let's look at the median value of each group.","1ac07a42":"The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","bb1c1372":"## 2.6. Correlation matrix heatmap for numerical values","c1bf2077":"## 4.1. Predictions summary ","72fa33fd":"### 3.4.1. Continous variables into categorical","11418501":"## 3.2. Extracting title from Name column","140fcf28":"## 2.4. Checking if datasets consist any missing values","6427ea53":"As we can see, Adaptive Boosting Classifier with default parameters, make the best predictions by far, when it's validated using cross-validation (accuracy above 82%).\n\nAdaBoostClassifier is ensamble classifier, which uses DecisionTreeClassifier as base estimator by default.\n\nAt this moment I do not have enough time to go deeper in that notebook, so I'll just write down what next steps would I do (and hopefully complete them in the future ;))\n\nWe can see now that performance of two of used models is very poor - Logistic Regression and Stochastic gradient descent Classifier. There is a need to inspect that. As I saw in other people Titanic notebooks, these models usually performs much better than they're here. I would start by checking predictions using other metrics: Precision, Recall and F1 Score.\n\nI should definitely try to tune hyperparameters, which would probably lead to even better results.\n\nI also should think about what other models I could use to make predictions and use them. ","d4fccc55":"## 2.9. Count plots for categorical features","dcb2bfca":"## 2.7. Average feature values depending on survival","42b5010a":"## 3.1. Getting rid of some features","9c9221d0":"At this moment, dataset consists mainly categorical features. \n\nAt first, I'am going to change remaining continous features (Age and Fare) into categorical ones with order. \n\nValues in Age column have been divided into five groups as following:","30dad253":"### 3.3.4. Check missing values","9864653e":"### Count survivorship by gender","171c0609":"## 2.5. Numerical features histograms","11906dfa":"Welcome to my first kaggle notebook, where I participate in \"Titanic - Machine Learning from Disaster\" competition. You can see my analysis and predictions below. Any comments are welcome!\n# 1. The challenge ","3cbe6877":"## 1.1. Import libraries","a1e4d256":"As we could see in EDA (2.4), datasets consist some missing values in four columns - Age, Cabin, Fare, Embarked.\n\nAt the start of the feature engeering phase, we got rid of Cabin column (3.1).\n\nNow, I'll input missing values for Age, Fare and Embarked column.","0a811e54":"Now I use pandas method 'cut' to bin values from Fare column into discrete intervals as following:","50358320":"Now, let's one hot encode Sex, Embarked and Title columns.\n\nThe reason why I'am doing one-hot encoding insted od using LabelEncoder() is that values in these columns do not have any logical order.\n\nI've used drop_first argument to remove rendundant data.","d34514bd":"It looks like there is no really strong correlation between numerical features. There is no reproduction of any information.\n\n**Strongest one is correlation of SibSp** (number of siblings\/spouses aboard the Titanic) **with ParCh** (numbert of parents\/children aboard the Titanic), which is understandable. Families tend to travel together.\n\n**Second strongest correlation is negative correlation between SibSp and Age**. It means it's more likely to travel with sibling or spouse if you're young.","90b7b840":"![data_dictionary.JPG](attachment:a201014c-2769-40ed-bd7f-cd6c19da1458.JPG)","0d071671":"People embarking from Cherbourg had higher chances to survive, but I'm not sure if it is important feature in case of predicting survivorship or just coincidence.","a53d85a2":"### Count survivorship by ticket class","c224c20c":"We can see that train and test datasets are well distributed (like splitting using stratification). Corresponding values in upper description tables are similar (e.g. mean of Age is around 30).","a81c1d29":"I replace NaN values in Embarked column with most frequent value (S).","97955203":"### Count survivorship by embarking location","3b99b8a8":"# 4. Predictions","0da31060":"We can see there are four most popular Titles (Mr, Miss, Mrs, Master).\n\nBecause there is not much people with other titles, I will group them together (as \"Rare\"), to make model simpler. ","042cfe8d":"### 3.4.2. One hot encoding","f227d55f":"After extracting person's title from Name column, I'm dropping Name column because I don't see any more predictive potential here.","ec68668b":"We see that the median age of people traveling first class, is significantly higher. We can also see some little tendency, that median of women's age is smaller.\n\nNow I fill NaN values with median of values in corresponding group.","2623d3b5":"## 1.2. Import train and test data","96435d22":"I decided to drop two features:\n\n* Ticket\n* Cabine\n\nBoth features seems to have some little predictive potential after thoughtful engineering, but at this moment I'm dropping them.","3240d0f6":"### Inspect shapes of datasets","f63b56d1":"# 2. Exploratory data analysis","71f73857":"Let's look at the distribution of Age bins.","ae7e1d91":"### 3.3.2. Fill Age missing values","83780436":"Now, there are no more missing values in the datasets.","2e8be7ca":"## 2.1. Inspect first rows of dataset","b5440c90":"## 2.3. Inspect descriptive statistics of datasets","89005061":"**Age**: We can see that people who survived tends to be younger than ones who didn't. \n\n**Fare**: They also statistically paid twice as much for their tickets.\n\n**ParCh**: Survivors were more likely to travel with parents or childrens.\n\n**SibSp**: Interesting statistic is that people travelling with siblings or spouse had lower percent of surviving.","338719fe":"Now, every feature in datasets is categorical.","2b7e5ade":"## 3.5. Transformations summary ","f6e76cf3":"# 3. Feature engineering","45e556c6":"### 3.3.1. Fill Embarked missing values","aa5df2d1":"Using similar technique, I fill the missing value in the Fare column.","7e4c3eb4":"My current approach assumes that datasets used for training and classification will consist **only categorical variables**.\n\nFor categorical data with **no logical order** (like Title, Embarked) i used one-hot encoding method.\n\nCategorical data with **logical order** (like Pclass, Age) remains label-encoded.","7ed7e811":"#### Train dataset","eefa0aaa":"## 2.8. Survivorship depending on age and fare","1f309aaf":"I will do transformations as on train as on test datast, so we can be sure that data have the same columns.","1538d3b0":"As we can expect, women have much greater survivorship rate than men."}}