{"cell_type":{"d0eac18b":"code","ea0f549d":"code","1c912bb7":"code","9fc142ea":"code","f0da8df2":"code","bf264a10":"code","19749748":"code","b2694ba8":"code","1f947745":"code","6c93c263":"code","3ded29b6":"code","a45f56a2":"code","d2e3ce04":"code","79bb5c1d":"code","bb962d13":"code","d8f34549":"code","625bf1bb":"code","f1bb370d":"code","36c4161c":"code","83b52dd8":"code","8e82d6dd":"code","df28c1fc":"code","a460d029":"code","419b6f65":"code","958cffa6":"code","0a44191d":"code","f8350157":"code","83538bb9":"code","41db6134":"code","c6d42f33":"markdown","17c49f2f":"markdown","38a5fa27":"markdown","940475fa":"markdown","dbaf24fb":"markdown","39e12559":"markdown","b76c8a1e":"markdown","7790a78f":"markdown"},"source":{"d0eac18b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import text,sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,Activation\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Conv1D,GlobalMaxPooling1D,MaxPooling1D\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","ea0f549d":"print(tf.__version__)","1c912bb7":"train_df=pd.read_csv('..\/input\/toxic-text-classification\/train.csv').fillna(' ')\ntest_df=pd.read_csv('..\/input\/toxic-text-classification\/test.csv').fillna(' ')","9fc142ea":"x=train_df['comment_text'].values\nx","f0da8df2":"# using wordcloud\n\nfrom wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt\n\ncomments =train_df['comment_text'].loc[train_df['toxic']==1].values\nwordcloud=WordCloud(\n    width=640,\n    height=640,\n    stopwords=STOPWORDS).generate(str(comments))\n\nfig=plt.figure(\n    figsize=(12,8),\n    facecolor='k',\n    edgecolor='k')\n\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","bf264a10":"y=train_df['toxic'].values","19749748":"#plot the freq of comments\ntrain_df['toxic'].plot(kind='hist',title='Freq of toxic comments')\ntrain_df['toxic'].value_counts()\n## therefore we use stratified sampling due to this imbalance distribution","b2694ba8":"max_features=20000\nmax_text_length=400","1f947745":"x_tokenizer=text.Tokenizer(max_features)\nx_tokenizer.fit_on_texts(list(x))\nx_tokenized=x_tokenizer.texts_to_sequences(x)\nx_train_val=sequence.pad_sequences(x_tokenized,maxlen=max_text_length)","6c93c263":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip -q glove.6B.zip","3ded29b6":"embedding_dim=100\nembeddings_index=dict()\nf=open('glove.6B.100d.txt')\nfor line in f:\n    values=line.split()\n    word=values[0]\n    coefs=np.asarray(values[1:],dtype='float32')\n    embeddings_index[word]=coefs\nf.close()\nprint(f'Found {len(embeddings_index)} word vectors.')","a45f56a2":"embedding_matrix=np.zeros((max_features,embedding_dim))\nnp.shape(embedding_matrix)","d2e3ce04":"x_tokenizer.word_index","79bb5c1d":"# create the embedding matrix\nfor word,index in x_tokenizer.word_index.items():\n    if index > max_features-1:\n        break\n    else:\n        embedding_vector=embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index]=embedding_vector","bb962d13":"embedding_matrix","d8f34549":"print('embedding_dim = ',embedding_dim)\nprint('max_features = ',max_features)","625bf1bb":"Embedding(max_features,embedding_dim)","f1bb370d":"# 1st layer of model\nmodel=Sequential()\nmodel.add(Embedding(max_features,\n                   embedding_dim,\n                   embeddings_initializer=tf.keras.initializers.Constant(\n                   embedding_matrix),\n                   trainable=False))\nmodel.add(Dropout(0.2))","36c4161c":"# some hyperparameters\nfilters=250\nkernel_size=3\nhidden_dims=250","83b52dd8":"model.add(Conv1D(filters,\n                kernel_size,\n                padding='valid'))\n# add some maxpooling\nmodel.add(MaxPooling1D())\nmodel.add(Conv1D(filters,\n                5,\n                padding='valid',\n                activation='relu'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(hidden_dims,activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1,activation='sigmoid')) # give value between 0 to 1\n\nmodel.summary()\n","8e82d6dd":"model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","df28c1fc":"x_train,x_val,y_train,y_val=train_test_split(x_train_val,y,\n                                            test_size=0.15,random_state=1)","a460d029":"batch_size=32\nepochs=3\n\nmodel.fit(x_train,y_train,\n         batch_size=batch_size,\n         epochs=epochs,\n         validation_data=(x_val,y_val))","419b6f65":"x_test=test_df['comment_text'].values","958cffa6":"x_test_tokenized=x_tokenizer.texts_to_sequences(x_test)\nx_testing=sequence.pad_sequences(x_test_tokenized,maxlen=max_text_length)","0a44191d":"# predict\ny_testing=model.predict(x_testing,verbose=1,batch_size=32)","f8350157":"y_testing.shape","83538bb9":"y_testing[0]","41db6134":"# convert it to labels\ntest_df['Toxic']=['not toxic' if x<0.5 else 'toxic' for x in y_testing]\ntest_df.sample(10,random_state=42)","c6d42f33":"# Binary toxic classifications using NN","17c49f2f":"## 5. Train Model","38a5fa27":"## 1.  Tokenize and pad text (GloVe)","940475fa":"## 6. Evaluation  ","dbaf24fb":"## 4. Build the Model","39e12559":"## 3. Create the Embedding Layer","b76c8a1e":"## 2. Prepare embedding matrix with pre-trained GloVe Embeddings","7790a78f":"## 0. Load Data and EDA"}}