{"cell_type":{"5a224662":"code","e37fdee6":"code","9051bfe8":"code","b9ae26c8":"code","be39b078":"code","b0e39e5e":"code","c5f6bca4":"code","1bd6c9d9":"code","debf525b":"code","4e9eafdb":"code","3e41b115":"code","a0a91d8c":"code","a6d58250":"code","78f04fd7":"code","06b71876":"code","ed1452bf":"code","137e2888":"code","ed4e3b9a":"code","3603e356":"code","79a59c67":"code","07dcc858":"code","4d1f0bf8":"code","3e7c0f21":"code","a8867982":"code","79cd83e8":"code","16b1f39a":"code","7d1c1ff6":"code","a3aad417":"code","a21e2fcc":"code","7f3de6bf":"code","bbd2ea4c":"code","f5c75b20":"code","763df74b":"code","ba79ada8":"code","30f4a3bb":"code","bc7baa03":"code","a879dd77":"code","da054319":"code","58fd1332":"code","68a3d0ca":"code","d1f0f78a":"code","42c52732":"code","3b643ba7":"code","80f175e2":"code","5d4052fc":"code","6fb779ee":"code","c8abfe4e":"code","6b0b03a1":"code","64a4fafe":"code","83da2511":"code","8d4ebac6":"code","b8955373":"code","e9596b3e":"code","5e3e8b4f":"code","c666ca2e":"code","d46c4310":"code","d5903fc9":"code","09d0820e":"code","2558c03c":"code","30a7ec3a":"code","1d40f905":"code","c7b04afe":"code","61e14c3c":"code","8d956861":"code","9b004b00":"code","47fd5715":"code","e2d72aa2":"code","802be8b1":"code","46cd221d":"code","87b76b45":"code","d07e1535":"code","eac60bcd":"code","c5babf23":"code","81dbdffe":"code","7afc1393":"code","cf12771a":"code","fc6f9056":"code","6f6a1be7":"code","f55d61ca":"code","f8971c4b":"code","4303d113":"code","0cce4315":"code","17aafe16":"code","4e32dd19":"code","a9936b25":"code","62f688e3":"code","ad9c69d1":"code","7c7e94cf":"code","fe2bfa67":"code","062b9f42":"markdown","873f35a3":"markdown","3cb94ed5":"markdown","040f29a8":"markdown","0b9ac1e6":"markdown","e267a9b1":"markdown","9bb7f77f":"markdown","5b82171a":"markdown","65e89a1d":"markdown","17d92f40":"markdown","8ea5e8be":"markdown","231a6782":"markdown","784041fc":"markdown","185cd184":"markdown","9ab4068e":"markdown","b1fdaa58":"markdown","91227451":"markdown","2b3c0329":"markdown","089d671e":"markdown","84f18c3a":"markdown","2222ce95":"markdown","5f0f9822":"markdown","7005d5b3":"markdown","5f44dbe1":"markdown","44d4b1ae":"markdown","404d8159":"markdown","7de9d772":"markdown","eb28f7a1":"markdown","bc91a11b":"markdown","55be5b90":"markdown","ec7b4396":"markdown","0228d825":"markdown","053c4975":"markdown","facc54b8":"markdown","746e989e":"markdown","c62968bc":"markdown"},"source":{"5a224662":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\nwarnings.simplefilter(action='ignore', category=RuntimeWarning)","e37fdee6":"# Libaries import\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom copy import copy\nfrom sklearn.manifold import TSNE\n","9051bfe8":"data = pd.read_csv(\"\/kaggle\/input\/uci-online-news-popularity-data-set\/OnlineNewsPopularity.csv\")\ndata.head(n=4)\n","b9ae26c8":"# Here we drop the two non-preditive (url and timedelta) attributes. They won't contribute anything\ndata.drop(labels=['url', ' timedelta'], axis = 1, inplace=True)\ndata.head(n=4)\n# remove noise from n_tokens_content. those equals to 0\ndata  = data[data[' n_tokens_content'] != 0]\n# Comment - Visualizing the n_non_stop_words data field shows that the present of a record with 1042 value, \n# futher observation of that data shows that it belongs to entertainment which is not actually. It belongs to world news or others.\n# this particluar also contains 0 on a lot of attributes. This record is classifed as a noise and will be remove.\ndata = data[data[' n_non_stop_words'] != 1042]\n# Here, we will go ahead and drop the field of ' n_non_stop_words. It doesn't contain relaible information.\ndata.drop(labels=[' n_non_stop_words'], axis = 1, inplace=True)\n\noriginal_data = copy(data)\n","be39b078":"# describing the data\ndata.describe()\n# from the data, there will be need to normailze the data if their will be need for condersing any machine learning model.","b0e39e5e":"share_data = np.sort(data[' shares'].values)\nprint(share_data.shape)\nleng = share_data.shape[0]\n\nmiddle = share_data[int(leng*0.7)-1]\nmiddle","c5f6bca4":"# ver good shares\n'''\nshare_data = np.sort(data[' shares'].values)\n\nleng = share_data.shape[0]\n\ntop_70 = share_data[int(leng*0.80)-1]\nprint (top_70)\ntop_50 = share_data[int(leng*0.40)-1]\nprint (top_50)\n#temp_data = data[(data[' shares'] >= top_70)]\ntemp_data = data[(data[' shares'] >= top_50) & (data[' shares'] < top_70)]\n\nprint(temp_data.shape)\n\n'''","1bd6c9d9":"# create label grades for the classes\n'''\nVery good = 7746 # top 80%\nGood = 7785 # top 60 - top 80\nAverage = 8585 # 40% - 60%\nPoor = 14346 # less than 40%\n'''\nshare_label = list()\nfor share in data[' shares']:\n    if share <= 1400:\n        share_label.append('Unpopular')\n    else:\n        share_label.append('Popular')\n\n# Update this class label into the dataframe\ndata = pd.concat([data.reset_index(drop=True), pd.DataFrame(share_label, columns=['popularity'])], axis=1)\ndata.head(4)","debf525b":"# Evaluating features (sensors) contribution towards the label\nfig = plt.figure(figsize=(15,5))\nax = sns.countplot(x='popularity',data=data,alpha=0.5)\n\ndata_channel_data = data.groupby('popularity').size().reset_index()\ndata_channel_data.columns = ['popularity','No of articles']\ndata_channel_data","4e9eafdb":"print(\"Skewness: %f\" % data[' shares'].skew())\nprint(\"Kurtosis: %f\" % data[' shares'].kurt())\n\nfrom scipy.stats import norm, probplot\n\n#histogram and normal probability plot\ntemp_data = data[data[' shares'] <= 100000]\nfig,ax = plt.subplots(figsize=(10,10))\nsns.distplot(data[' shares'], fit=norm);\nfig = plt.figure()\nres = probplot(data[' shares'], plot=plt)\n'''\n'Shares' doesn't have a normal distribution. It shows 'peakedness', positive skewness and does not follow the diagonal line.\nThus some statistic analysis might not be suitable for it\n'''","3e41b115":"#applying log transformation\nnew_shares_data = copy(data)\n\nnew_shares_data.loc[new_shares_data[' shares'] > 0, ' shares'] = np.log(data.loc[data[' shares'] > 0, ' shares'])\nnew_shares_log = new_shares_data[' shares']\n#transformed histogram and normal probability plot\nfig,ax = plt.subplots(figsize=(10,10))\nsns.distplot(new_shares_log, fit=norm);\nfig = plt.figure()\nres = probplot(new_shares_log, plot=plt)","a0a91d8c":"# use log transformation to transform each features to a normal distribution\n\n# note log transformation can only be performed on data without zero value\nfor col in data.iloc[:,:-1].columns:\n    #applying log transformation\n    temp = data[data[col] == 0]\n    # only apply to non-zero features\n    if temp.shape[0] == 0:\n        data[col] = np.log(data[col])\n        print (col)\n","a6d58250":"# Evaluating the impact of log transformation","78f04fd7":"# before log transformation\nsns.distplot(original_data[' n_tokens_content'], fit=norm);","06b71876":"# after log transformation\nsns.distplot(data[' n_tokens_content'], fit=norm);","ed1452bf":"# scale the data\n# StandardScaler cannot guarantee balanced feature scales in the presence of outliers.\n# StandardScaler removes the mean and scales the data to unit variance.\n# However, the outliers have an influence when\n# computing the empirical mean and standard deviation which shrink the range of the feature values \n\n# source: https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\n\n# Scale features using statistics that are robust to outliers.\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\n\n# scalled all the feature selections aside shares and populairty\nscalled_data = scaler.fit_transform(data.iloc[:, :-2])\n\n# update the dataframe back with the scalled data\ndata.iloc[:, :-2] = scalled_data","137e2888":"# the data after log transformation and robust scaler\ndata.describe()","ed4e3b9a":"data.iloc[:,:-2]","3603e356":"from sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans","79a59c67":"# Kmeans perform poorly on high feature space\npca = PCA(n_components=2)\nreduced_data = pca.fit_transform(data.iloc[:,:-2])\nreduced_data.shape","07dcc858":"# plotting the clusters PCA\nplt.figure(figsize=(10,10))\nplt.plot(reduced_data[:,0], reduced_data[:,1], 'r.')\nplt.title('PCA Transformation')\n\nplt.show()","4d1f0bf8":"tsne = TSNE(n_components=2, n_iter=300)\nreduced_tsne = tsne.fit_transform(data.iloc[:,:-2])\n\n# plotting the clusters TSNE\nplt.figure(figsize=(10,10))\nplt.plot(reduced_tsne[:,0], reduced_tsne[:,1], 'r.')\nplt.title('TSNE Transformation')\nplt.show()","3e7c0f21":"k=list(range(1,9))\nssd=[]\nfor i in k:\n    kmeans=KMeans(n_clusters=i).fit(reduced_tsne)\n    ssd.append(kmeans.inertia_)\n    \nplt.plot(k,ssd,'o-')\nplt.xlabel('k')\nplt.ylabel('Sum of squared error')\nplt.show()","a8867982":"# Predicts the clusters\nkmeans=KMeans(init='k-means++',n_clusters=5)\nkmeans.fit(reduced_tsne)\nkmeans_preds=kmeans.predict(reduced_tsne)\n","79cd83e8":"centroids = kmeans.cluster_centers_\nclusters = np.unique(kmeans_preds)\n\n# ploting the result of of the clusters\nax, fig = plt.subplots(figsize=(15,7))\nplt.scatter(centroids[:, 0], centroids[:, 1],\n            marker='x', s=169, linewidths=3,\n            color='r', zorder=10)\n\n# ploting the cluster numbers\nfor i in range(clusters.shape[0]):\n    plt.text(centroids[i, 0], centroids[i, 1], clusters[i], fontsize=20, color='white', \n             bbox=dict(facecolor='black', alpha=0.5))\n    \nplt.scatter(reduced_tsne[:,0],reduced_tsne[:,1],c=kmeans_preds,marker='.')\nplt.show()","16b1f39a":"# fussing the cluster data into the dataframe\ndata1=pd.concat([data.reset_index(drop=True), pd.DataFrame(kmeans_preds, columns=['clusters'])],axis=1)","7d1c1ff6":"data1.shape","a3aad417":"# extrating individual cluster from the data\ncluster1_data = data1[data1['clusters'] == 0]\ncluster2_data = data1[data1['clusters'] == 1]\ncluster3_data = data1[data1['clusters'] == 2]\ncluster4_data = data1[data1['clusters'] == 3]\ncluster5_data = data1[data1['clusters'] == 4]\nprint ('Cluster1 size: ',cluster1_data.shape)\nprint ('Cluster2 size: ',cluster2_data.shape)\nprint ('Cluster3 size: ',cluster3_data.shape)\nprint ('Cluster4 size: ',cluster4_data.shape)\nprint ('Cluster5 size: ',cluster5_data.shape)","a21e2fcc":"# Mutual Information computation\n# our label is the popularity and will be disregarding the shares data\nfrom sklearn.feature_selection import mutual_info_classif\n\n# Mutual information for cluster 1\nX1 = cluster1_data.iloc[:, :-3]\ny1 = cluster1_data.iloc[:, -2]\nmi_data_clus1 = mutual_info_classif(X1, y1, 'auto')","7f3de6bf":"# mututal information for cluster 2\nX2 = cluster2_data.iloc[:, :-3]\ny2 = cluster2_data.iloc[:, -2]\nmi_data_clus2 = mutual_info_classif(X2, y2, 'auto')","bbd2ea4c":"# mututal information for cluster 3\nX3 = cluster3_data.iloc[:, :-3]\ny3 = cluster3_data.iloc[:, -2]\nmi_data_clus3 = mutual_info_classif(X3, y3, 'auto')\n# mututal information for cluster 3\nX4 = cluster4_data.iloc[:, :-3]\ny4 = cluster4_data.iloc[:, -2]\nmi_data_clus4 = mutual_info_classif(X4, y4, 'auto')\n# mututal information for cluster 3\nX5 = cluster5_data.iloc[:, :-3]\ny5 = cluster5_data.iloc[:, -2]\nmi_data_clus5 = mutual_info_classif(X5, y5, 'auto')","f5c75b20":"# ploting the result of mutual information\nplt.figure(figsize=(15, 5))\ng = sns.barplot(x=X1.columns,y=mi_data_clus1)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.title(\"Mutual Information for all Features - Cluster 1\")","763df74b":"plt.figure(figsize=(15, 5))\ng = sns.barplot(x=X2.columns,y=mi_data_clus2)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.title(\"Mutual Information for all Features - Cluster 2\")","ba79ada8":"plt.figure(figsize=(15, 5))\ng = sns.barplot(x=X3.columns,y=mi_data_clus3)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.title(\"Mutual Information for all Features - Cluster 3\")","30f4a3bb":"plt.figure(figsize=(15, 5))\ng = sns.barplot(x=X4.columns,y=mi_data_clus4)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.title(\"Mutual Information for all Features - Cluster 4\")","bc7baa03":"plt.figure(figsize=(15, 5))\ng = sns.barplot(x=X5.columns,y=mi_data_clus5)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.title(\"Mutual Information for all Features - Cluster 5\")","a879dd77":"### an helper function for extracting the best features possible\ndef extract_best_features(feature_scores, feature_col, n=5, sort_metric=False):\n    # this function extracts out the best features.\n    # inputs \n    temp = np.hstack((feature_scores.reshape(-1,1), feature_col.reshape(-1,1)))\n    features = pd.DataFrame(temp, columns=['score', 'name'])\n    # sort the features\n    features = features.sort_values(by=['score'], ascending=sort_metric).reset_index(drop=True)\n    # extract the best features\n    best_features = features.iloc[:n, :].to_numpy()\n    return best_features\n","da054319":"best_features = extract_best_features(mi_data_clus4, X4.columns.values, n=10)\nbest_features\n","58fd1332":"from sklearn.feature_selection import f_classif","68a3d0ca":"# F-Score for cluster 1\n\nf_test_data = f_classif(X1, y1)\nf_score_1=f_test_data[0]\n\nplt.figure(figsize=(15, 5))\ng = sns.barplot(x=X1.columns,y=f_score_1)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.title(\"F score for all Features - Cluster 1\")","d1f0f78a":"# F-Score for cluster 2\nf_test_data = f_classif(X2, y2)\nf_score_2=f_test_data[0]\n\nplt.figure(figsize=(15, 5))\ng = sns.barplot(x=X2.columns,y=f_score_2)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.title(\"F score for all Features - Cluster 2\")","42c52732":"# F-Score for cluster 3\nf_test_data = f_classif(X3, y3)\nf_score_3=f_test_data[0]\n\nplt.figure(figsize=(15, 5))\ng = sns.barplot(x=X3.columns,y=f_score_3)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.title(\"F score for all Features - Cluster 3\")","3b643ba7":"# F-Score for cluster 4\nf_test_data = f_classif(X4, y4)\nf_score_4=f_test_data[0]\n\n# F-Score for cluster 5\nf_test_data = f_classif(X5, y5)\nf_score_5=f_test_data[0]\n","80f175e2":"plt.figure(figsize=(15, 5))\ng = sns.barplot(x=X4.columns,y=f_score_4)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.title(\"F score for all Features - Cluster 4\")","5d4052fc":"plt.figure(figsize=(15, 5))\ng = sns.barplot(x=X4.columns,y=f_score_5)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.title(\"F score for all Features - Cluster 5\")","6fb779ee":"best_features = extract_best_features(f_score_1, X1.columns.values, n=10)\nbest_features","c8abfe4e":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier","6b0b03a1":"# Random forest is used as the model for RFE\n# RFE for Cluster 1\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=10)\n# for 5 features\nrfe = RFE(model, 5) \nrfe = rfe.fit(X1, y1)\nrfe_5_features_clus1 = X1.columns.values[rfe.get_support()]\n# for 10 features\nrfe = RFE(model, 10) \nrfe = rfe.fit(X1, y1)\nrfe_10_features_clus1 = X1.columns.values[rfe.get_support()]\n# for 20 features\nrfe = RFE(model, 20) \nrfe = rfe.fit(X1, y1)\nrfe_20_features_clus1  = X1.columns.values[rfe.get_support()]\n# for 30 features\nrfe = RFE(model, 30) \nrfe = rfe.fit(X1, y1)\nrfe_30_features_clus1 = X1.columns.values[rfe.get_support()]","64a4fafe":"# Random forest is used as the model for RFE\n# RFE for Cluster 2\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=10)\n# for 5 features\nrfe = RFE(model, 5) \nrfe = rfe.fit(X2, y2)\nrfe_5_features_clus2 = X2.columns.values[rfe.get_support()]\n# for 10 features\nrfe = RFE(model, 10) \nrfe = rfe.fit(X2, y2)\nrfe_10_features_clus2 = X2.columns.values[rfe.get_support()]\n# for 20 features\nrfe = RFE(model, 20) \nrfe = rfe.fit(X2, y2)\nrfe_20_features_clus2  = X2.columns.values[rfe.get_support()]\n# for 30 features\nrfe = RFE(model, 30) \nrfe = rfe.fit(X2, y2)\nrfe_30_features_clus2 = X2.columns.values[rfe.get_support()]","83da2511":"# Random forest is used as the model for RFE\n# RFE for Cluster 3\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=10)\n# for 5 features\nrfe = RFE(model, 5) \nrfe = rfe.fit(X3, y3)\nrfe_5_features_clus3 = X3.columns.values[rfe.get_support()]\n# for 10 features\nrfe = RFE(model, 10) \nrfe = rfe.fit(X3, y3)\nrfe_10_features_clus3 = X3.columns.values[rfe.get_support()]\n# for 20 features\nrfe = RFE(model, 20) \nrfe = rfe.fit(X3, y3)\nrfe_20_features_clus3  = X3.columns.values[rfe.get_support()]\n# for 30 features\nrfe = RFE(model, 30) \nrfe = rfe.fit(X3, y3)\nrfe_30_features_clus3 = X3.columns.values[rfe.get_support()]","8d4ebac6":"# Random forest is used as the model for RFE\n# RFE for Cluster 4\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=10)\n# for 5 features\nrfe = RFE(model, 5) \nrfe = rfe.fit(X4, y4)\nrfe_5_features_clus4 = X4.columns.values[rfe.get_support()]\n# for 10 features\nrfe = RFE(model, 10) \nrfe = rfe.fit(X4, y4)\nrfe_10_features_clus4 = X4.columns.values[rfe.get_support()]\n# for 20 features\nrfe = RFE(model, 20) \nrfe = rfe.fit(X4, y4)\nrfe_20_features_clus4  = X4.columns.values[rfe.get_support()]\n# for 30 features\nrfe = RFE(model, 30) \nrfe = rfe.fit(X4, y4)\nrfe_30_features_clus4 = X4.columns.values[rfe.get_support()]","b8955373":"# Random forest is used as the model for RFE\n# RFE for Cluster 5\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=10)\n# for 5 features\nrfe = RFE(model, 5) \nrfe = rfe.fit(X5, y5)\nrfe_5_features_clus5 = X5.columns.values[rfe.get_support()]\n# for 10 features\nrfe = RFE(model, 10) \nrfe = rfe.fit(X5, y5)\nrfe_10_features_clus5 = X5.columns.values[rfe.get_support()]\n# for 20 features\nrfe = RFE(model, 20) \nrfe = rfe.fit(X5, y5)\nrfe_20_features_clus5  = X5.columns.values[rfe.get_support()]\n# for 30 features\nrfe = RFE(model, 30) \nrfe = rfe.fit(X5, y5)\nrfe_30_features_clus5 = X5.columns.values[rfe.get_support()]","e9596b3e":"from sklearn.decomposition import PCA","5e3e8b4f":"#########################################################\n# PCA for cluster 1\n# for 5 features\ntransformer = PCA(n_components=5)\npca_clus1_5 = transformer.fit_transform(X1)\n# for 10 features\ntransformer = PCA(n_components=10)\npca_clus1_10 = transformer.fit_transform(X1)\n# for 20 features\ntransformer = PCA(n_components=20)\npca_clus1_20 = transformer.fit_transform(X1)\n# for 30 features\ntransformer = PCA(n_components=30)\npca_clus1_30 = transformer.fit_transform(X1)","c666ca2e":"# PCA for cluster 2\n# for 5 features\ntransformer = PCA(n_components=5)\npca_clus2_5 = transformer.fit_transform(X2)\n# for 10 features\ntransformer = PCA(n_components=10)\npca_clus2_10 = transformer.fit_transform(X2)\n# for 20 features\ntransformer = PCA(n_components=20)\npca_clus2_20 = transformer.fit_transform(X2)\n# for 30 features\ntransformer = PCA(n_components=30)\npca_clus2_30 = transformer.fit_transform(X2)","d46c4310":"# PCA for cluster 3\n# for 5 features\ntransformer = PCA(n_components=5)\npca_clus3_5 = transformer.fit_transform(X3)\n# for 10 features\ntransformer = PCA(n_components=10)\npca_clus3_10 = transformer.fit_transform(X3)\n# for 20 features\ntransformer = PCA(n_components=20)\npca_clus3_20 = transformer.fit_transform(X3)\n# for 30 features\ntransformer = PCA(n_components=30)\npca_clus3_30 = transformer.fit_transform(X3)","d5903fc9":"# PCA for cluster 4\n# for 5 features\ntransformer = PCA(n_components=5)\npca_clus4_5 = transformer.fit_transform(X4)\n# for 10 features\ntransformer = PCA(n_components=10)\npca_clus4_10 = transformer.fit_transform(X4)\n# for 20 features\ntransformer = PCA(n_components=20)\npca_clus4_20 = transformer.fit_transform(X4)\n# for 30 features\ntransformer = PCA(n_components=30)\npca_clus4_30 = transformer.fit_transform(X4)","09d0820e":"# PCA for cluster 5\n# for 5 features\ntransformer = PCA(n_components=5)\npca_clus5_5 = transformer.fit_transform(X5)\n# for 10 features\ntransformer = PCA(n_components=10)\npca_clus5_10 = transformer.fit_transform(X5)\n# for 20 features\ntransformer = PCA(n_components=20)\npca_clus5_20 = transformer.fit_transform(X5)\n# for 30 features\ntransformer = PCA(n_components=30)\npca_clus5_30 = transformer.fit_transform(X5)","2558c03c":"# encoding the label set with a label encoder\nfrom sklearn.preprocessing import LabelEncoder\n\nlabelEn = LabelEncoder()\nencoded_labels = labelEn.fit_transform(y1.values)\nclass_names = labelEn.classes_\nclass_names","30a7ec3a":"# Splitting the data for Training and Testing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.metrics import recall_score","1d40f905":"#### PCA Features","c7b04afe":"# defining the model\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# For Cluster 1\nencoded_labels = labelEn.fit_transform(y1.values)\n\npca_data = [pca_clus1_5, pca_clus1_10, pca_clus1_20, pca_clus1_30]\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(pca_data)):\n    # For PCA Feature Extraction\n    X_train, X_test, y_train, y_test = train_test_split(pca_data[i], encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 1::PCA - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n    \n# For Cluster 2\nencoded_labels = labelEn.fit_transform(y2.values)\n\npca_data = [pca_clus2_5, pca_clus2_10, pca_clus2_20, pca_clus2_30]\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(pca_data)):\n    # For PCA Feature Extraction\n    X_train, X_test, y_train, y_test = train_test_split(pca_data[i], encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 2::PCA - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 3\nencoded_labels = labelEn.fit_transform(y3.values)\n\npca_data = [pca_clus3_5, pca_clus3_10, pca_clus3_20, pca_clus3_30]\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(pca_data)):\n    # For PCA Feature Extraction\n    X_train, X_test, y_train, y_test = train_test_split(pca_data[i], encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 3::PCA - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n    \n# For Cluster 4\nencoded_labels = labelEn.fit_transform(y4.values)\n\npca_data = [pca_clus4_5, pca_clus4_10, pca_clus4_20, pca_clus4_30]\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(pca_data)):\n    # For PCA Feature Extraction\n    X_train, X_test, y_train, y_test = train_test_split(pca_data[i], encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 4::PCA - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 5\nencoded_labels = labelEn.fit_transform(y5.values)\n\npca_data = [pca_clus5_5, pca_clus5_10, pca_clus5_20, pca_clus5_30]\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(pca_data)):\n    # For PCA Feature Extraction\n    X_train, X_test, y_train, y_test = train_test_split(pca_data[i], encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 5::PCA - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      ","61e14c3c":"#### KNN Cross-Validation","8d956861":"import matplotlib.pyplot as plt\n\n##############################################################\n# Cros Validation for any of the features\nk_range = np.arange(1,120)\naccuracy = []\n\nfor n in k_range:    \n    neigh = KNeighborsClassifier(n_neighbors=n, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    #print (\"Random Forest Classifer Result\")\n    #print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test_2)) + \"%\")\n    accuracy.append(100*accuracy_score(y_pred, y_test))\n\n\n\nplt.figure(figsize=(20,13))\nplt.plot(k_range, accuracy, 'r-', label='KNN Accuracy Vs KNN Neighbors size')\nplt.plot(k_range, accuracy, 'bx')\nplt.xlabel('KNN Neighbors size')\nplt.ylabel('KNN Accuracy')\nplt.legend()\nplt.grid()\nplt.title('KNN Accuracy Vs Neighbors size')\nplt.show()","9b004b00":"# For Cluster 1\nencoded_labels = labelEn.fit_transform(y1.values)\n\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nn_features = [5, 10, 20, 30]\n\nfor i in range(len(features_list)):\n    best_features = extract_best_features(mi_data_clus1, X1.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X1.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X1.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 1::MI - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 2\nencoded_labels = labelEn.fit_transform(y2.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\n\nfor i in range(len(features_list)):\n    best_features = extract_best_features(mi_data_clus2, X2.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X2.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X2.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 2::MI - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 3\nencoded_labels = labelEn.fit_transform(y3.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(mi_data_clus3, X3.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X3.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X3.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 3::MI - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n\n# For Cluster 4\nencoded_labels = labelEn.fit_transform(y4.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(mi_data_clus4, X4.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X4.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X4.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 4::MI - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 5\nencoded_labels = labelEn.fit_transform(y5.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(mi_data_clus5, X5.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X5.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X5.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 5::MI - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      ","47fd5715":"# For Cluster 1\nencoded_labels = labelEn.fit_transform(y1.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nn_features = [5, 10, 20, 30]\n\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_1, X1.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X1.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X1.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 1::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 2\nencoded_labels = labelEn.fit_transform(y2.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\n\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_2, X2.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X2.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X2.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 2::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 3\nencoded_labels = labelEn.fit_transform(y3.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_3, X3.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X3.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X3.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 3::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  \n# For Cluster 4\nencoded_labels = labelEn.fit_transform(y4.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_4, X4.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X4.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X4.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 4::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  \n# For Cluster 5\nencoded_labels = labelEn.fit_transform(y5.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_5, X5.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X5.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X5.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 5::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  ","e2d72aa2":"# For Cluster 1\nencoded_labels = labelEn.fit_transform(y1.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nn_features = [5, 10, 20, 30]\n\nrfe_features_clus1 = [rfe_5_features_clus1, rfe_10_features_clus1, rfe_20_features_clus1, rfe_30_features_clus1]\n\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X1.columns.values) - set(rfe_features_clus1[i]))\n    data_clus_rfe = X1.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 1::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 2\nencoded_labels = labelEn.fit_transform(y2.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nrfe_features_clus2 = [rfe_5_features_clus2, rfe_10_features_clus2, rfe_20_features_clus2, rfe_30_features_clus2]\n\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X2.columns.values) - set(rfe_features_clus2[i]))\n    data_clus_rfe = X2.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 2::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 3\nencoded_labels = labelEn.fit_transform(y3.values)\nrfe_features_clus3 = [rfe_5_features_clus3, rfe_10_features_clus3, rfe_20_features_clus3, rfe_30_features_clus3]\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X3.columns.values) - set(rfe_features_clus3[i]))\n    data_clus_rfe = X3.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 3::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 4\nencoded_labels = labelEn.fit_transform(y4.values)\nrfe_features_clus4 = [rfe_5_features_clus4, rfe_10_features_clus4, rfe_20_features_clus4, rfe_30_features_clus4]\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X4.columns.values) - set(rfe_features_clus4[i]))\n    data_clus_rfe = X4.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 4::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 5\nencoded_labels = labelEn.fit_transform(y5.values)\nrfe_features_clus5 = [rfe_5_features_clus5, rfe_10_features_clus5, rfe_20_features_clus5, rfe_30_features_clus5]\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X5.columns.values) - set(rfe_features_clus5[i]))\n    data_clus_rfe = X5.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 3::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      ","802be8b1":"#### PCA","46cd221d":"# defining the model\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# For Cluster 1\nencoded_labels = labelEn.fit_transform(y1.values)\n\npca_data = [pca_clus1_5, pca_clus1_10, pca_clus1_20, pca_clus1_30]\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\n\n\nfor i in range(len(pca_data)):\n    # For PCA Feature Extraction\n    X_train, X_test, y_train, y_test = train_test_split(pca_data[i], encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=100,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 1::PCA - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 2\nencoded_labels = labelEn.fit_transform(y2.values)\n\npca_data = [pca_clus2_5, pca_clus2_10, pca_clus2_20, pca_clus2_30]\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(pca_data)):\n    # For PCA Feature Extraction\n    X_train, X_test, y_train, y_test = train_test_split(pca_data[i], encoded_labels, test_size=0.3, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=100,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 2::PCA - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 3\nencoded_labels = labelEn.fit_transform(y3.values)\n\npca_data = [pca_clus3_5, pca_clus3_10, pca_clus3_20, pca_clus3_30]\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(pca_data)):\n    # For PCA Feature Extraction\n    X_train, X_test, y_train, y_test = train_test_split(pca_data[i], encoded_labels, test_size=0.3, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=100,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 3::PCA - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  \n# For Cluster 4\nencoded_labels = labelEn.fit_transform(y4.values)\n\npca_data = [pca_clus4_5, pca_clus4_10, pca_clus4_20, pca_clus4_30]\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(pca_data)):\n    # For PCA Feature Extraction\n    X_train, X_test, y_train, y_test = train_test_split(pca_data[i], encoded_labels, test_size=0.3, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=100,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 4::PCA - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n   \n# For Cluster 5\nencoded_labels = labelEn.fit_transform(y5.values)\n\npca_data = [pca_clus5_5, pca_clus5_10, pca_clus5_20, pca_clus5_30]\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(pca_data)):\n    # For PCA Feature Extraction\n    X_train, X_test, y_train, y_test = train_test_split(pca_data[i], encoded_labels, test_size=0.3, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=100,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 5::PCA - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n   ","87b76b45":"from sklearn.ensemble import RandomForestClassifier\n\n# For Cluster 1\nencoded_labels = labelEn.fit_transform(y1.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nn_features = [5, 10, 20, 30]\n\nfor i in range(len(features_list)):\n    best_features = extract_best_features(mi_data_clus1, X1.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X1.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X1.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=1000, n_jobs=5, max_depth=100,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 1::MI - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 2\nencoded_labels = labelEn.fit_transform(y2.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\n\nfor i in range(len(features_list)):\n    best_features = extract_best_features(mi_data_clus2, X2.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X2.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X2.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=1000, n_jobs=5, max_depth=100,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 2::MI - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 3\nencoded_labels = labelEn.fit_transform(y3.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(mi_data_clus3, X3.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X3.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X3.drop(drop_these, axis=1, inplace=False)\n    clf = RandomForestClassifier(n_estimators=1000, n_jobs=5, max_depth=100,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 3::MI - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  \n# For Cluster 4\nencoded_labels = labelEn.fit_transform(y4.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(mi_data_clus4, X4.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X4.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X4.drop(drop_these, axis=1, inplace=False)\n    clf = RandomForestClassifier(n_estimators=1000, n_jobs=5, max_depth=100,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 4::MI - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  \n# For Cluster 5\nencoded_labels = labelEn.fit_transform(y5.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(mi_data_clus5, X5.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X5.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X5.drop(drop_these, axis=1, inplace=False)\n    clf = RandomForestClassifier(n_estimators=1000, n_jobs=5, max_depth=100,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 5::MI - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n     ","d07e1535":"#### Cross validation For Random Forest\n\nnns = [1, 5, 10, 50, 100, 200, 500, 1000, 2000, 3000]\naccuracy = []\n\nfor n in nns:    \n    clf = RandomForestClassifier(n_estimators=n, n_jobs=5, max_depth=500,\n                                 random_state=0)\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    #print (\"Random Forest Classifer Result\")\n    #print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test_2)) + \"%\")\n    accuracy.append(100*accuracy_score(y_pred, y_test))\n\n    \nplt.figure(figsize=(10,7))\nplt.plot(nns, accuracy, 'r-', label='Random Forest Accuracy Vs Number of Tress')\nplt.plot(nns, accuracy, 'bx')\nplt.xlabel('Random Forest Tree Sizes')\nplt.ylabel('Random Forest Accuracy')\nplt.legend()\nplt.grid()\nplt.title('Random Forest Accuracy Vs Number of Tress')\nplt.show()","eac60bcd":"# For Cluster 1\nencoded_labels = labelEn.fit_transform(y1.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nn_features = [5, 10, 20, 30]\n\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_1, X1.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X1.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X1.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 1::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 2\nencoded_labels = labelEn.fit_transform(y2.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\n\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_2, X2.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X2.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X2.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 2::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 3\nencoded_labels = labelEn.fit_transform(y3.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_3, X3.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X3.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X3.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 3::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  \n    \n# For Cluster 4\nencoded_labels = labelEn.fit_transform(y4.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_4, X4.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X4.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X4.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 4::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  \n    \n# For Cluster 5\nencoded_labels = labelEn.fit_transform(y5.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_5, X5.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X5.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X5.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 5::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  ","c5babf23":"# For Cluster 1\nencoded_labels = labelEn.fit_transform(y1.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nn_features = [5, 10, 20, 30]\n\nrfe_features_clus1 = [rfe_5_features_clus1, rfe_10_features_clus1, rfe_20_features_clus1, rfe_30_features_clus1]\n\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X1.columns.values) - set(rfe_features_clus1[i]))\n    data_clus_rfe = X1.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 1::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  \n# For Cluster 2\nencoded_labels = labelEn.fit_transform(y2.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nrfe_features_clus2 = [rfe_5_features_clus2, rfe_10_features_clus2, rfe_20_features_clus2, rfe_30_features_clus2]\n\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X2.columns.values) - set(rfe_features_clus2[i]))\n    data_clus_rfe = X2.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 2::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  \n# For Cluster 3\nencoded_labels = labelEn.fit_transform(y3.values)\nrfe_features_clus3 = [rfe_5_features_clus3, rfe_10_features_clus3, rfe_20_features_clus3, rfe_30_features_clus3]\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X3.columns.values) - set(rfe_features_clus3[i]))\n    data_clus_rfe = X3.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 3::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n     \n# For Cluster 4\nencoded_labels = labelEn.fit_transform(y4.values)\nrfe_features_clus4 = [rfe_5_features_clus4, rfe_10_features_clus4, rfe_20_features_clus4, rfe_30_features_clus4]\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X4.columns.values) - set(rfe_features_clus4[i]))\n    data_clus_rfe = X4.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 4::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n     \n# For Cluster 5\nencoded_labels = labelEn.fit_transform(y5.values)\nrfe_features_clus5 = [rfe_5_features_clus5, rfe_10_features_clus5, rfe_20_features_clus5, rfe_30_features_clus5]\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X5.columns.values) - set(rfe_features_clus5[i]))\n    data_clus_rfe = X5.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 5::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n     ","81dbdffe":"#### RFE\nfrom sklearn.svm import SVC\n","7afc1393":"# For Cluster 1\nencoded_labels = labelEn.fit_transform(y1.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nn_features = [5, 10, 20, 30]\n\nrfe_features_clus1 = [rfe_5_features_clus1, rfe_10_features_clus1, rfe_20_features_clus1, rfe_30_features_clus1]\n\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X1.columns.values) - set(rfe_features_clus1[i]))\n    data_clus_rfe = X1.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = SVC(gamma='auto')\n\n    # commence training - NOTE: It takes hours to be complete\n    clf.fit(X_train, y_train) \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 1::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n  \n# For Cluster 2\nencoded_labels = labelEn.fit_transform(y2.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nrfe_features_clus2 = [rfe_5_features_clus2, rfe_10_features_clus2, rfe_20_features_clus2, rfe_30_features_clus2]\n\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X2.columns.values) - set(rfe_features_clus2[i]))\n    data_clus_rfe = X2.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = SVC(gamma='auto')\n\n    # commence training - NOTE: It takes hours to be complete\n    clf.fit(X_train, y_train) \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 2::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n  \n# For Cluster 3\nencoded_labels = labelEn.fit_transform(y3.values)\nrfe_features_clus3 = [rfe_5_features_clus3, rfe_10_features_clus3, rfe_20_features_clus3, rfe_30_features_clus3]\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X3.columns.values) - set(rfe_features_clus3[i]))\n    data_clus_rfe = X3.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = SVC(gamma='auto')\n\n    # commence training - NOTE: It takes hours to be complete\n    clf.fit(X_train, y_train) \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 3::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n     \n# For Cluster 4\nencoded_labels = labelEn.fit_transform(y4.values)\nrfe_features_clus4 = [rfe_5_features_clus4, rfe_10_features_clus4, rfe_20_features_clus4, rfe_30_features_clus4]\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X4.columns.values) - set(rfe_features_clus4[i]))\n    data_clus_rfe = X4.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = SVC(gamma='auto')\n\n    # commence training - NOTE: It takes hours to be complete\n    clf.fit(X_train, y_train) \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 4::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n     \n# For Cluster 5\nencoded_labels = labelEn.fit_transform(y5.values)\nrfe_features_clus5 = [rfe_5_features_clus5, rfe_10_features_clus5, rfe_20_features_clus5, rfe_30_features_clus5]\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X5.columns.values) - set(rfe_features_clus5[i]))\n    data_clus_rfe = X5.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_rfe, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = SVC(gamma='auto')\n\n    # commence training - NOTE: It takes hours to be complete\n    clf.fit(X_train, y_train) \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 5::RFE - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n","cf12771a":"# Classification of the Data Channel an article was presented ","fc6f9056":"data_weekdays = copy(data.iloc[:, :-1])\ndata_weekdays","6f6a1be7":"# combine the data_channels as one\n\n# Merging the data channels as one single column\nDataChannelMerge=data[[' data_channel_is_lifestyle',' data_channel_is_entertainment' ,' data_channel_is_bus',\n                        ' data_channel_is_socmed' ,' data_channel_is_tech',' data_channel_is_world' ]]\n#logic to merge data channel\nDataChannel_arr=[]\nfor r in list(range(DataChannelMerge.shape[0])):\n    if (((DataChannelMerge.iloc[r,0])==0) and ((DataChannelMerge.iloc[r,1])==0) and ((DataChannelMerge.iloc[r,2])==0) and ((DataChannelMerge.iloc[r,3])==0) and ((DataChannelMerge.iloc[r,4])==0) and ((DataChannelMerge.iloc[r,5])==0)):\n        DataChannel_arr.append('Others')\n    for c in list(range(DataChannelMerge.shape[1])):\n        if ((c==0) and (DataChannelMerge.iloc[r,c])==1):\n            DataChannel_arr.append('Lifestyle')\n        elif ((c==1) and (DataChannelMerge.iloc[r,c])==1):\n            DataChannel_arr.append('Entertainment')\n        elif ((c==2) and (DataChannelMerge.iloc[r,c])==1):\n            DataChannel_arr.append('Business')\n        elif ((c==3) and (DataChannelMerge.iloc[r,c])==1):\n            DataChannel_arr.append('Social Media')\n        elif ((c==4) and (DataChannelMerge.iloc[r,c])==1):\n            DataChannel_arr.append('Tech')\n        elif ((c==5) and (DataChannelMerge.iloc[r,c])==1):\n            DataChannel_arr.append('World')\n","f55d61ca":"# merge the the new data into the dataframe\ndata_weekdays.insert(loc=58, column='channels', value=DataChannel_arr)\n\n# Now I drop the old data\ndata_weekdays.drop(labels=[' data_channel_is_lifestyle',' data_channel_is_entertainment' ,' data_channel_is_bus',\n                        ' data_channel_is_socmed' ,' data_channel_is_tech',' data_channel_is_world'], axis = 1, inplace=True)\nprint(data_weekdays.shape)\ndata_weekdays.head(n=4)","f8971c4b":"# Evaluating features (sensors) contribution towards the label\nfig = plt.figure(figsize=(15,5))\nax = sns.countplot(x='channels',data=data_weekdays,alpha=0.5)\n","4303d113":"# Fetch the counts for each class\nclass_counts = data_weekdays.groupby('channels').size().reset_index()\nclass_counts.columns = ['channels','No of articles']\nclass_counts\n","0cce4315":"# transform the data\n\nencoded_labels = labelEn.fit_transform(data_weekdays.loc[:, 'channels'].values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\n\n# the feautures are stored in the seconds column\nX_train, X_test, y_train, y_test = train_test_split(data_weekdays.iloc[:, :-1], encoded_labels, test_size=0.2, shuffle=False)\n\nclf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                             random_state=0)\n\nclf.fit(X_train, y_train)  \n\n# predict the result\ny_pred = clf.predict(X_test)\nprint (\"Random Forest - ::Full - \")\nprint (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\nprint (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))","17aafe16":"tsne = TSNE(n_components=2, n_iter=300)\nreduced_tsne = tsne.fit_transform(data_weekdays.iloc[:,:-1])\n\n# plotting the clusters TSNE\nplt.figure(figsize=(10,10))\nplt.plot(reduced_tsne[:,0], reduced_tsne[:,1], 'r.')\nplt.title('TSNE Transformation')\nplt.show()","4e32dd19":"# Predicts the clusters\nkmeans=KMeans(init='k-means++',n_clusters=5)\nkmeans.fit(reduced_tsne)\nkmeans_preds=kmeans.predict(reduced_tsne)\n","a9936b25":"# fussing the cluster data into the dataframe\ndata_weekdays2=pd.concat([data_weekdays.reset_index(drop=True), pd.DataFrame(kmeans_preds, columns=['clusters'])],axis=1)","62f688e3":"# extrating individual cluster from the data\ncluster1_data_weekdays = data_weekdays2[data_weekdays2['clusters'] == 0]\ncluster2_data_weekdays = data_weekdays2[data_weekdays2['clusters'] == 1]\ncluster3_data_weekdays = data_weekdays2[data_weekdays2['clusters'] == 2]\ncluster4_data_weekdays = data_weekdays2[data_weekdays2['clusters'] == 3]\ncluster5_data_weekdays = data_weekdays2[data_weekdays2['clusters'] == 4]\nprint ('Cluster1 size: ',cluster1_data_weekdays.shape)\nprint ('Cluster2 size: ',cluster2_data_weekdays.shape)\nprint ('Cluster3 size: ',cluster3_data_weekdays.shape)\nprint ('Cluster4 size: ',cluster4_data_weekdays.shape)\nprint ('Cluster5 size: ',cluster5_data_weekdays.shape)","ad9c69d1":"### F- score\n\n# mututal information for cluster 3\nX1_week = cluster1_data_weekdays.iloc[:, :-2]\ny1_week = cluster1_data_weekdays.iloc[:, -2]\n\n# mututal information for cluster 3\nX2_week = cluster2_data_weekdays.iloc[:, :-2]\ny2_week = cluster2_data_weekdays.iloc[:, -2]\n\n# mututal information for cluster 3\nX3_week = cluster3_data_weekdays.iloc[:, :-2]\ny3_week = cluster3_data_weekdays.iloc[:, -2]\n\n# mututal information for cluster 3\nX4_week = cluster4_data_weekdays.iloc[:, :-2]\ny4_week = cluster4_data_weekdays.iloc[:, -2]\n\n# mututal information for cluster 3\nX5_week = cluster5_data_weekdays.iloc[:, :-2]\ny5_week = cluster5_data_weekdays.iloc[:, -2]\n\n\n\n# F-Score for cluster 1\nf_test_data = f_classif(X1_week, y1_week)\nf_score_1_wk=f_test_data[0]\n\n# F-Score for cluster 5\nf_test_data = f_classif(X2_week, y2_week)\nf_score_2_wk=f_test_data[0]\n\n# F-Score for cluster 5\nf_test_data = f_classif(X3_week, y3_week)\nf_score_3_wk=f_test_data[0]\n\n# F-Score for cluster 4\nf_test_data = f_classif(X4_week, y4_week)\nf_score_4_wk=f_test_data[0]\n\n# F-Score for cluster 5\nf_test_data = f_classif(X5_week, y5_week)\nf_score_5_wk=f_test_data[0]\n","7c7e94cf":"# For Cluster 1\nencoded_labels = labelEn.fit_transform(y1_week.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nn_features = [5, 10, 20, 30]\n\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_1_wk, X1_week.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X1_week.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X1_week.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 1::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 2\nencoded_labels = labelEn.fit_transform(y2_week.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\n\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_2_wk, X2_week.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X2_week.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X2_week.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 2::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n      \n# For Cluster 3\nencoded_labels = labelEn.fit_transform(y3_week.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_3_wk, X3_week.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X3_week.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X3_week.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 3::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  \n# For Cluster 4\nencoded_labels = labelEn.fit_transform(y4_week.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_4_wk, X4_week.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X4_week.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X4_week.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 4::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  \n# For Cluster 5\nencoded_labels = labelEn.fit_transform(y5_week.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_5_wk, X5_week.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X5_week.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X5_week.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    neigh = KNeighborsClassifier(n_neighbors=63, n_jobs=-1)\n\n    neigh.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test)\n    print (\"KNN - Cluster 5::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='micro')))\n  ","fe2bfa67":"### Random forest\n\n# For Cluster 1\nencoded_labels = labelEn.fit_transform(y1_week.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nn_features = [5, 10, 20, 30]\n\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_1_wk, X1_week.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X1_week.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X1_week.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 1::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='weighted')))\n      \n# For Cluster 2\nencoded_labels = labelEn.fit_transform(y2_week.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\n\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_2_wk, X2_week.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X2_week.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X2_week.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 2::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='weighted')))\n      \n# For Cluster 3\nencoded_labels = labelEn.fit_transform(y3_week.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_3_wk, X3_week.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X3_week.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X3_week.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 3::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='weighted')))\n  \n    \n# For Cluster 4\nencoded_labels = labelEn.fit_transform(y4_week.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_4_wk, X4_week.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X4_week.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X4_week.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 4::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='weighted')))\n  \n    \n# For Cluster 5\nencoded_labels = labelEn.fit_transform(y5_week.values)\n\nfeatures_list = ['5 Features', '10 Features', '20 Features', '30 Features']\nfor i in range(len(features_list)):\n    best_features = extract_best_features(f_score_5_wk, X5_week.columns.values, n=n_features[i])\n    # the feautures are stored in the seconds column\n    drop_these = list(set(X5_week.columns.values) - set(best_features[:,1]))\n    data_clus_mi = X5_week.drop(drop_these, axis=1, inplace=False)\n    X_train, X_test, y_train, y_test = train_test_split(data_clus_mi, encoded_labels, test_size=0.2, shuffle=False)\n\n    clf = RandomForestClassifier(n_estimators=500, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\n    clf.fit(X_train, y_train)  \n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    print (\"Random Forest - Cluster 5::F-score - \" + str(features_list[i]))\n    print (\"Accuracy - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n    print (\"Recall - \" + str(recall_score(y_test, y_pred, average='weighted')))\n  ","062b9f42":"### Normal distribution analysis for Shares\nEvaluating the effects of normal distribution on the shares","873f35a3":"#### Mutual Information","3cb94ed5":"#### RFE","040f29a8":"# Results\nIn the classification task, we applied 3 machine learning models on the clusters obtained from the cluster extraction with four feature space namely top 5, top 10, top 20 and top 30 features from all feature selection methods: mutual information, f-score, RFE and PCA. Accuracy and recall were used as the main evaluation criteria for this task.\n\nFor Popularity results, the overal cluster results is presented below:\n\n![image.png](attachment:image.png)\n\nRandom Forest generally performed the best and their wasn't much signficant difference between all the feature selection considered.\n\nFor data channel prediciton, the overal cluster results is shown below:\n![image.png](attachment:image.png)","0b9ac1e6":"### Recursive Feature Selection","e267a9b1":"# Exploratory Analysis For Online News Popularity v2 - A Classification Problem\n\n- by [Ayo Ayibiowu](https:\/\/www.linkedin.com\/in\/ayoayibiowu\/)\n\nThis is a follow up of the [intiial exploratory data analysis](https:\/\/www.kaggle.com\/thehapyone\/exploratory-analysis-for-online-news-popularity) done for the online news populairty. In this notebook, we will using the knowledge gained so far to address a classification problem. In the [intial notebook](https:\/\/www.kaggle.com\/thehapyone\/exploratory-analysis-for-online-news-popularity), I attempted a popularity classification of the shares, but the model accuracy wasn't that awesome. In this notebook, I will attempt to improve that previous model accuracy, and also attempt to do some other classfication challenges. \n\nThere will emphasize on feature selection algorthims that can be used in modelling the system for better results. Also, aside using Accuracy as the evulation metric, I will also consider some other metric evaluation.\n\nThe process followed is highlighted below:\n - Data Cleaning - Noise removal\n - Data Transformation - Transform using log-transformation\n - Data Clustering - Grouping Similar Articles together.\n - Feature Selection and Evaluation\n - Machine Learning Classification\n - Summary and Conclusion. \n \n","9bb7f77f":"## Classification of Data Channel.\nFor this challenge, F-score will only be used as the form of feature selection methods.","5b82171a":"# Classification Problem\nNow, we will use the results of our feature selections and extraction for classification of the shares and data channel\n3 Machine learning models will be considered:\n - KNN\n - SVC\n - Random Forest","65e89a1d":"### F-Score","17d92f40":"# Data Processing and Noise Removal\n","8ea5e8be":"### KNN\n","231a6782":"### Mutual Information\nMutual information can find relations between two random variables. It can observe both the linear and unlinear relationship. If the mutual information between 2 variables is 0, then according to mutual information, those 2 variables doesn't have any relationship.","784041fc":"Plotting the cluster results","185cd184":"### Finding the normal distrubution of the dataset\nTransforming the whole position data to a normal distribution","9ab4068e":"# Data Clustering - Grouping Similar Articles together.\nHere, we are going to find any special pattern from the data. Using unsupervised learning to find cluster from the data and then create a new set of data out from the clusters formed. The idea is that it is easy to build a model for similar clustered articles than to build a model for all the articles.","b1fdaa58":"In this project, we analyzed the given online news data set and was able to observe some interesting patterns that good articles do have in common. We initially carried out a subjective analysis which was based on our intuition and because we understand it is easily possible for human intuition to be biased or crowded from experience, and use a quantitative analysis to confirm our initial hypothesis by doing univariate and bivariate analysis using scatter plot, boxplot, and bar plot of each feature with the shares feature.\n\nTwo main popularity class was considered for the popularity prediction. Un-supervised learning approach was implemented for transforming the data to a 2-dimensional data that is feed to a K-means clustering.\n\nClustering of the article idea was the major novelty in this project, it allowed us to be able to group similar articles together and their by deploy machine learning models on those clusters as compared to using the models on the whole dataset an effort that was worth it. Generally, without the clustering the best result from other similar works was around 68% using random forest, but by leveraging article clustering we were able to achieve a 75% accuracy and this result can even be better if we consider increasing the number of clusters used and also the feature space.\n\nThe same clustering ideology was implemented in the data channel prediction task, which gave us a maximum accuracy of 89% possible as compared to only 82% without clustering. From the insight analysis carried out on the dataset, the following are some of the things we recommend improving the popularity of an article:\n- The number of words in the article should be less than 1500 words. The lesser the better.\n- Article title shouldn\u2019t be too long or too short. 6 \u2013 17 words is the ideal number of words to have for titles.\n- Articles should have good amount of images. Between 1 \u2013 40 images is great.\n- Also having a couple of videos is also nice for article popularity, but not too much. The higher the lower the odds.\n- Easy to read words helps to improve article popularity.\n- The number of keywords in the metadata really influences the shares to a margin. The higher the value the better the shares chances. A value upward of 5 is recommend.\n- Articles referencing popular articles have a higher chance of improving their own popularity.\n- Increase the number of popular unique words in the article to increase the chances of having better popularity.\n- Avoid the use of longer words in the articles.\n- Best popular articles are usually posted on Mondays and Wednesday (and a bit of Tuesdays). Sundays and Saturdays (Weekends generally) are the worsts days to publish an article. \n- Articles that talks about current trending tends to have higher popularity.\n- Increase the amount of subjectivity in the title and content. \n- The \"Business\" and \"Entertainment\" channel are great for the best popularity. Coming in third position will be the \"World\" and\/or \"Tech\" channels.","91227451":"### PCA","2b3c0329":"#### F-Score","089d671e":"## Reading the data","84f18c3a":"#### RFE","2222ce95":"### Scalling The Data\nA scaler that is immune to outliers needs to be used because there is a lot of outliers in the given data.","5f0f9822":"### SVM","7005d5b3":"# Data Transformation - Log Transform\nThe given data doesn't have a normal distribution. A log transformation will be carried out to transform the full data to have a normal distribution as close as possible","5f44dbe1":"#### Mutual Information","44d4b1ae":"#### F-Score","404d8159":"### Create the Shares label\nTwo labels will be considered: Popular and Unpopular. Popular are shares above the median while unpopular are shares from median downwards","7de9d772":"From the observation gotten so far, the higher the number of clusters the more likelihood similar articles can be grouped together.","eb28f7a1":"## Classification of Article Popularity","bc91a11b":"### KNN","55be5b90":"Although shares doesn't have a normal distrubition, we can do a log transformation to give us a normal distrubition data","ec7b4396":"t-SNE transformation seems to make more sense that PCA transformation. We will go ahead with using t-SNE transformation","0228d825":"### Here we check the class balance\n","053c4975":"# Feature Selection and Feature Extraction\nWe will performe a couple of feature selections and also feature extraction on the data clusters. The reults of the feature selections will be used to as input to a machine learning classfication model to predict the popularity of the article. The below feature selections and extraction were considered:\n* Mutual Information\n* F-Score\n* Recursive Feature Elimination\n* PCA (Principal Component Analysis)\n* KPCA (Kernel PCA) - It was later removed, wasn't improving the model, time and memory consuming \n* MDS (Manifold Sculpting) - It was later removed, wasn't improving the model, time and memory consuming \n* T-SNE - It was later removed, wasn't improving the model, time and memory consuming \n","facc54b8":"### Dimensionality reduction: PCA or t-SNE?\nWhich one should we used: PCA or t-SNE to transform the to a 2-dimension because k-means generally perform better on low dimension space.","746e989e":"# Summary and Conclusion","c62968bc":"### Random Forest"}}