{"cell_type":{"d831d9f2":"code","5e928ea5":"code","987d21b9":"code","e6ab09d6":"code","dadb58f9":"code","11f60a83":"code","7cf47daf":"code","2c9acd28":"code","50c1d82e":"code","f297a28c":"code","43809742":"code","a70dc2fe":"code","63302424":"code","0316d476":"code","376d57ed":"code","aa3723c6":"code","0e28596b":"code","6349a8a3":"code","ffb35c1b":"code","e625da70":"code","eef49a0d":"code","605a1312":"code","10a27767":"code","be6d60c9":"code","12bfa459":"code","28e2d922":"code","312dca0b":"code","8510d25c":"code","f38fc6c7":"code","94c5cb04":"code","71052b9b":"code","881e704b":"code","0bc27b15":"code","4f65b2df":"code","a508bdd0":"code","6fe9a342":"markdown","be2a48b0":"markdown","7e48f185":"markdown","0e6860d1":"markdown","7d5d0dbd":"markdown","5833d10c":"markdown","4c950652":"markdown","53deb4fd":"markdown","779db630":"markdown"},"source":{"d831d9f2":"import pandas as pd","5e928ea5":"train = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\ntest = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')\nsubmission = pd.read_csv('..\/input\/contradictory-my-dear-watson\/sample_submission.csv')","987d21b9":"submission","e6ab09d6":"train['language'].value_counts(normalize=True)*100","dadb58f9":"test['language'].value_counts(normalize=True)*100","11f60a83":"train.language.unique()","7cf47daf":"train['label_str'] = train['label'].map({0 : \"entailment\", 1 : \"neutral\", 2 : \"contradiction\"})","2c9acd28":"import matplotlib.pyplot as plt\nimport seaborn as sns \n\nplt.figure(figsize=(8,5))\nsns.countplot(y ='label_str', data = train, alpha=.5, palette=\"muted\")","50c1d82e":"plt.figure(figsize=(8,5))\nsns.countplot(y ='language', hue = \"label_str\", data = train, alpha=.5, palette=\"muted\")","f297a28c":"# !pip install -q transformers==3.0.2\n!pip install -q nlp","43809742":"from transformers import BertTokenizer, AutoTokenizer, TFBertModel, TFXLMRobertaModel, TFAutoModel\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Embedding, GlobalAveragePooling1D\nfrom keras.optimizers import Adam\n\nfrom nlp import load_dataset\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","a70dc2fe":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU: {tpu.master()}')\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\nprint('Number of replicas:', strategy.num_replicas_in_sync)","63302424":"# https:\/\/huggingface.co\/jplu\/tf-xlm-roberta-large\n# encoder_handle = 'jplu\/tf-xlm-roberta-large'\nencoder_handle = 'joeddav\/xlm-roberta-large-xnli'","0316d476":"# https:\/\/huggingface.co\/jplu\/tf-xlm-roberta-large\/raw\/main\/config.json \uac00\ub2a5\n!curl https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/jplu\/tf-xlm-roberta-large\/config.json","376d57ed":"tokenizer = AutoTokenizer.from_pretrained(encoder_handle)","aa3723c6":"# \ub108\ubb34 \uc791\uc740 \uac74 \uc544\ub2cc\uac00?\nmax_len = 120 # max sequence length\n# random_seed = 2021\nrandom_seed = 11887\nlearning_rate = 1e-5 # Controls how large a step is taken when updating model weights during training.\nepochs = 3\nbatch_size = 16 * strategy.num_replicas_in_sync # The number of examples that will be processed in parallel during training. Tailored for TPUs.\nloss = 'sparse_categorical_crossentropy'\nmetrics = ['accuracy']\n# steps_per_epoch = 1000\n\nauto = tf.data.experimental.AUTOTUNE","0e28596b":"def encode_sentence(s, tokenizer):\n    \"\"\"\n    Turn a sequence of words into and array of numbers using a selected tokenizer.\n    Args:\n        s (list of str) - Input string.\n        tokenizer - XLM-R tokenizer.\n    Returns:\n        (list of int) - Tokenized string.\n\n    \"\"\"\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append(tokenizer.sep_token)\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef tokenize(data, tokenizer, max_len):\n    \"\"\"\n    Encode hypotheses and premises into arrays of numbers using a selected tokenizer. \n    Args:\n        data - An array consisting of [hypothesis (str), premise (str)] pairs.\n        tokenizer - Tokenizer handle.\n        max_len - Max sequence length.\n    Returns: (dictionary of tensors)\n        input_word_ids - Indices of input sequence tokens in the vocabulary, truncated to max_len.\n        input_mask - Real input indices mapped to ones. Padding indices mapped to zeroes.\n        input_type_ids - Segment token indices to indicate first and second portions of the inputs.\n    \"\"\"\n\n    PAD_ID = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n \n    # Append a separator to each sentence, tokenize, and concatenate.\n    tokens1 = tf.ragged.constant([encode_sentence(s[0], tokenizer) for s in data], dtype=tf.int32) # ENCODED_SEQUENCE_A [SEP]\n    tokens2 = tf.ragged.constant([encode_sentence(s[1], tokenizer) for s in data], dtype=tf.int32) # ENCODED_SEQUENCE_B [SEP]\n    cls_label = [tokenizer.convert_tokens_to_ids([tokenizer.cls_token])]*tokens1.shape[0] # [CLS] ENCODED_SEQUENCE_A [SEP]\n    tokens = tf.concat([cls_label, tokens1, tokens2], axis=-1) # [CLS] ENCODED_SEQUENCE_A [SEP] ENCODED_SEQUENCE_B [SEP]\n\n    # Truncate to max_len.\n    tokens = tokens[:, :max_len]\n\n    # Pad with zeroes if len < max_len.\n    tokens = tokens.to_tensor(default_value=PAD_ID)\n    pad = max_len - tf.shape(tokens)[1]\n    tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID)\n    input_word_ids = tf.reshape(tokens, [-1, max_len])\n\n    # The input mask allows the model to cleanly differentiate between the content and the padding. \n    input_mask = tf.cast(input_word_ids != PAD_ID, tf.int32)\n    input_mask = tf.reshape(input_mask, [-1, max_len])\n\n    # Map tokens1 indices to zeroes and tokens2 indices to ones.\n    input_type_ids = tf.concat([tf.zeros_like(cls_label), tf.zeros_like(tokens1), tf.ones_like(tokens2)], axis=-1).to_tensor()\n\n\n    inputs = {\n      'input_word_ids': input_word_ids,\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n    return inputs","6349a8a3":"def build_dataset(x, y, mode, batch_size):\n    \"\"\"\n    Build a batched TF training, validation, or test dataset.\n    \n    (This function is borrowed from some of the other notebooks in this competition -\n    not sure who to credit exactly so thanks all!)\n    \"\"\"\n    if mode == \"train\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((x, y))\n            .repeat()\n            .shuffle(5678)\n            .batch(batch_size)\n            .prefetch(auto)\n        )\n    elif mode == \"valid\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((x, y))\n            .batch(batch_size)\n            .cache()\n            .prefetch(auto)\n        )\n    elif mode == \"test\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices(x)\n            .batch(batch_size)\n            )\n    else:\n        raise NotImplementedError\n    return dataset","ffb35c1b":"!pip3 install datasets","e625da70":"from datasets import load_dataset\n\ndef load_mnli(use_validation=True):\n    result = []\n    dataset = load_dataset('multi_nli')\n    print(dataset['train'])\n    keys = ['train', 'validation_matched','validation_mismatched'] if use_validation else ['train']\n    for k in keys:\n        for record in dataset[k]:\n            c1, c2, c3 = record['premise'], record['hypothesis'], record['label']\n            if c1 and c2 and c3 in {0,1,2}:\n                result.append((c1,c2,c3,'en'))\n    result = pd.DataFrame(result, columns=['premise','hypothesis','label','lang_abv'])\n    return result","eef49a0d":"mnli = load_mnli()","605a1312":"total_train = train[['premise', 'hypothesis', 'label']]\ntotal_train = pd.concat([total_train, mnli], axis=0)","10a27767":"total_train","be6d60c9":"x_train, x_valid, y_train, y_valid = train_test_split(total_train[['premise', 'hypothesis']].values.tolist(), total_train['label'], test_size=0.25, random_state=12345)","12bfa459":"x_train_ = tokenize(x_train, tokenizer, max_len)\nx_valid_ = tokenize(x_valid, tokenizer, max_len)","28e2d922":"train_dataset = build_dataset(x_train_, y_train, \"train\", batch_size)\nvalid_dataset = build_dataset(x_valid_, y_valid, \"valid\", batch_size)","312dca0b":"def build_model(encoder_handle, random_seed, learning_rate, loss, metrics, max_len):\n    \n    tf.keras.backend.clear_session()\n    tf.random.set_seed(random_seed)\n    \n    with strategy.scope():\n        \n        input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n#          input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n        # RoBERTa doesn\u2019t use token_type_ids.\n        \n        #  Create an instance of a model defined in encoder_handle\n#         roberta = TFXLMRobertaModel.from_pretrained(encoder_handle)\n#         roberta = roberta([input_word_ids, input_mask])[0]\n        roberta = TFAutoModel.from_pretrained(encoder_handle)\n        roberta = roberta([input_word_ids])[0]\n        out = GlobalAveragePooling1D()(roberta)\n        out = Dense(3, activation='softmax')(out)\n        \n        model = Model(inputs=[input_word_ids], outputs = out)\n        model.compile(optimizer=Adam(lr=learning_rate), loss=loss, metrics=metrics)\n    \n    model.summary()\n    \n    return model","8510d25c":"model = build_model(encoder_handle, random_seed, learning_rate, loss, metrics, max_len)","f38fc6c7":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                                  verbose=1,\n                                                  patience=2,\n                                                  mode='min',\n                                                  restore_best_weights=True)","94c5cb04":"steps_per_epoch = len(x_train) \/\/ batch_size\nhistory = model.fit(train_dataset,\n                    validation_data=valid_dataset,\n                    steps_per_epoch=steps_per_epoch,\n                    epochs=epochs,\n                    callbacks=[early_stopping])","71052b9b":"print(history.history.keys())","881e704b":"import numpy as np\n\n# summarize history for loss\nep_nbr = np.arange(1, len(history.history['accuracy']) + 1)\nplt.plot(ep_nbr, history.history['loss'])\nplt.plot(ep_nbr, history.history['val_loss'])\nplt.title('Unadjusted Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# Training loss is continually reported over the course of an entire epoch.\n# Validation metrics are computed over the validation set only once the current training epoch is completed.\n# This implies, that on average, training losses are measured half an epoch earlier.\n\n# plot the *shifted* training and validation loss\nplt.plot(ep_nbr - 0.5, history.history['loss'], label=\"train_loss\")\nplt.plot(ep_nbr, history.history['val_loss'], label=\"val_loss\")\nplt.title(\"Shifted Loss\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.show()\n\n# summarize history for accuracy\nplt.plot(ep_nbr, history.history['accuracy'])\nplt.plot(ep_nbr, history.history['val_accuracy'])\nplt.title('Unadjusted Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# plot the *shifted* training and validation accuracy\nplt.plot(ep_nbr - 0.5, history.history['accuracy'], label=\"train_accuracy\")\nplt.plot(ep_nbr, history.history['val_accuracy'], label=\"val_accuracy\")\nplt.title(\"Shifted Accuracy\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.show()","0bc27b15":"x_test = tokenize(test[['premise', 'hypothesis']].values.tolist(), tokenizer, max_len)\ntest_dataset  = build_dataset(x_test, None, \"test\", batch_size)","4f65b2df":"import numpy as np\n\npredictions_prob = model.predict(test_dataset)\nfinal = predictions_prob.argmax(axis=-1)   \n\nsubmission = pd.DataFrame()    \nsubmission['id'] = test['id']\nsubmission['prediction'] = final.astype(np.int32)","a508bdd0":"submission.to_csv(\"\/kaggle\/working\/submission.csv\", index = False)","6fe9a342":"# TPU \uc0ac\uc6a9 \uc900\ube44","be2a48b0":"# \ubaa8\ub378 \uc900\ube44","7e48f185":"# Visualization","0e6860d1":"# encoding \ud568\uc218\n\nragged: \ub204\ub354\uae30\uac00 \ub41c\n\ntf.ragged.constant: \ub370\uc774\ud130\ub294 \ub2e4\uc591\ud55c \ud615\ud0dc\ub85c \uc81c\uacf5\ub429\ub2c8\ub2e4; \ud150\uc11c\ub3c4 \ub9c8\ucc2c\uac00\uc9c0\uc785\ub2c8\ub2e4. \ube44\uc815\ud615 \ud150\uc11c\ub294 \uc911\ucca9 \uac00\ubcc0 \uae38\uc774 \ubaa9\ub85d\uc5d0 \ud574\ub2f9\ud558\ub294 \ud150\uc11c\ud50c\ub85c\uc785\ub2c8\ub2e4. \ub2e4\uc74c\uc744 \ud3ec\ud568\ud558\uc5ec \uade0\uc77c\ud558\uc9c0 \uc54a\uc740 \ubaa8\uc591\uc73c\ub85c \ub370\uc774\ud130\ub97c \uc27d\uac8c \uc800\uc7a5\ud558\uace0 \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n- \uc77c\ub828\uc758 \uc601\ud654\uc758 \ubc30\uc6b0\ub4e4\uacfc \uac19\uc740 \uac00\ubcc0 \uae38\uc774 \uae30\ub2a5\n- \ubb38\uc7a5\uc774\ub098 \ube44\ub514\uc624 \ud074\ub9bd\uacfc \uac19\uc740 \uac00\ubcc0 \uae38\uc774 \uc21c\ucc28\uc801 \uc785\ub825\uc758 \ubc30\uce58\n- \uc808, \ub2e8\ub77d, \ubb38\uc7a5 \ubc0f \ub2e8\uc5b4\ub85c \uc138\ubd84\ud654\ub41c \ud14d\uc2a4\ud2b8 \ubb38\uc11c\uc640 \uac19\uc740 \uacc4\uce35\uc801 \uc785\ub825\n- \ud504\ub85c\ud1a0\ucf5c \ubc84\ud37c\uc640 \uac19\uc740 \uad6c\uc870\ud654\ub41c \uc785\ub825\uc758 \uac1c\ubcc4 \ud544\ub4dc\n\nragged tensor\ub97c to_tensor \ub97c \ud1b5\ud574\uc11c \uc77c\ubc18 tensor\ub85c \ubc14\uafb8\uac8c \ub418\uba74, \ub0b4\ubd80\uc758 \ubaa8\ub4e0 \ub370\uc774\ud130\uac00 \uac19\uc740 \uae38\uc774\ub97c \uac16\uac8c \ub41c\ub2e4. \uc774 \ub54c \uc9e7\uc740 \uac12\ub4e4\uc774 \uae38\uc5b4\uc9c0\uac8c \ub418\uba74\uc11c \uc0c8\ub85c\uc6b4 \uac12\ub4e4\uc774 \ucc44\uc6cc\uc9c0\uac8c \ub418\ub294\ub370 \uadf8 \ub54c \uc0ac\uc6a9\ub418\ub294 \uac12\uc774 default_value \uac12\uc774\ub2e4.","7d5d0dbd":"# Train, validation dataset \uc900\ube44\n\n- mnli \ub370\uc774\ud130\uc14b\ub3c4 \ud568\uaed8 \uc0ac\uc6a9\ud558\uba74 \uc131\ub2a5\uc774 \uc62c\ub77c\uac00\ub294 \uac83\uc73c\ub85c \ubcf4\uc778\ub2e4. (\ucc38\uace0: https:\/\/www.kaggle.com\/rahulbana\/contradictory-my-dear-watson-using-xlni-robert2)\n\ud558\uc9c0\ub9cc \ub2e4\uc74c\uc758 \ucf54\ub4dc\ub294 \uc660\uc9c0\ubaa8\ub974\uac8c \uc791\ub3d9\uc744 \uc798 \uc548\ud568\n\n```\n\n```\n\n\uadf8\ub798\uc11c huggingface \uc5d0 [dataset](https:\/\/github.com\/huggingface\/datasets) \uc744 \uc774\uc6a9\ud558\uae30\ub85c \ud568.","5833d10c":"# \ubaa8\ub378 \uace0\ub974\uae30\n\nMulti-lingual model \uc744 \uc0ac\uc6a9\ud558\uae30 \uc704\ud574\uc11c [huggingface](https:\/\/huggingface.co\/transformers\/multilingual.html) \uc0ac\uc774\ud2b8\ub97c \ucc38\uace0\ud558\uc600\uc2b5\ub2c8\ub2e4. multilingual model \uc774 \uc81c\uacf5\ud574\uc8fc\ub294 \uc5b8\uc5b4\uac00 XNLI \uc778 \uacbd\uc6b0\uc5d0\ub294 [facebook \uc758 XNLI github](https:\/\/github.com\/facebookresearch\/XNLI) \uc744 \ubcf4\uba74 \ub418\ub294\ub370, \ub2e4\uc74c\uacfc \uac19\uc740 14\uac1c\uc758 \uc5b8\uc5b4\ub97c \ud0c0\uac9f\uc73c\ub85c \ud558\uace0 \uc788\ub2e4\uace0 \ud558\uace0 \ud574\ub2f9 \uc5b8\uc5b4\ub4e4\uc740 \uc774 competition \uc5d0\uc11c \uc81c\uacf5\ud558\ub294 train set \uacfc \uc77c\uce58\ud569\ub2c8\ub2e4.\n- French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu\n\n\uc774 \uc911\uc5d0\uc11c \uc800\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 XLM-RoBERTa \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\ubcf4\uace0\uc790 \ud569\ub2c8\ub2e4.\n- 100\uac1c\uc758 \uc5b8\uc5b4\ub85c \ub41c \uc0c8\ub86d\uac8c \uc0dd\uc131\ub41c \uae68\ub057\ud55c CommonCrawl data 2.5TB \ub97c \uae30\ubc18\uc73c\ub85c \ud559\uc2b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n- mBEERT, XLM \uacfc \uac19\uc740 \uc5b8\uc5b4\ubaa8\ub378\ubcf4\ub2e4 downstream tasks \uc5d0 \uac15\uc810\uc744 \uac00\uc9c4\ub2e4\uace0 \ud569\ub2c8\ub2e4. (\ubd84\ub958, sequence labeling, question answering)\n- 2\uac1c\uc758 \ubaa8\ub378\uc774 \uc874\uc7ac\ud569\ub2c8\ub2e4: xlm-roberta-base, xlm-roberta-large\n\n\uc800\ub294 large \ub97c \uc0ac\uc6a9\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n[\uc774 \uc0ac\uc774\ud2b8](https:\/\/huggingface.co\/transformers\/model_doc\/xlmroberta.html) \ub97c \uc0b4\ud3b4\ubcf4\uba74 \ub300\ub7b5\uc801\uc778 \uc0ac\uc6a9\ubc29\ubc95\uc5d0 \ub300\ud574\uc11c \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub108\ubb34 \uc5b4\ub824\uc6cc\uc11c [\uc774 \ub178\ud2b8\ubd81](https:\/\/www.kaggle.com\/jbagdon\/predict-with-tf-xlm-roberta-large) \uc744 \ub530\ub77c\ud558\uae30\ub85c \ud558\uc600\uc2b5\ub2c8\ub2e4.","4c950652":"# Train model\n\n[transformers.XLMRobertaModel](https:\/\/huggingface.co\/transformers\/model_doc\/xlmroberta.html#tfxlmrobertamodel) \uc740 tf.keras.Model \uc758 subclass \ub85c \ub9cc\ub4e4\uc5b4 \uc84c\uc73c\uba70, TF2.0 Keras model \ub85c \ub9cc\ub4e4\uc5b4\uc84c\ub2e4. Return \uc740 return_dict=True \uc778 \uacbd\uc6b0\uc5d0\ub294 TFBaseModelOutputWithPooling \uc774 \ub098\uc624\uba70, \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 tf.Tensor \uac00 \ubc18\ud658\ub41c\ub2e4. roberta \uac00 \ubc49\uc740 output \uc911\n- last_hidden_state \ub294 `(batch_size, sequence_length, hidden_size)` \uc758 shape \uc744 \uac16\ub294\ub2e4. \n- pooler_outpout \uc740 `(batch_size, hidden_size)` \uc758 \ud06c\uae30\ub97c \uac16\uc73c\uba70 classification token \uc778 sequence \uc758 \uccab\ubc88\uc9f8 \ud1a0\ud070\uc758 hidden-state \uc774\ub2e4.   \n  - pooler_output (tf.Tensor of shape (batch_size, hidden_size)) \u2013 Last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n  - This output is usually not a good summary of the semantic content of the input, **you\u2019re often better with averaging or pooling the sequence of hidden-states for the whole input sequence.**\n\nroberta \uc758 output \uc744 3\uac1c\uc758 unit \uc5d0 \ub300\ud55c softmax \ub85c \ub9cc\ub4dc\ub294 \uac83\uc774 \ubaa9\ud45c\uc778\ub370, \uc774 \uacbd\uc6b0 \ubcf4\ud1b5\uc740 Flatten \uc744 \ud558\uace0 Dense \ub97c \uc313\ub294 \ubc29\uc2dd\uc73c\ub85c \uac00\ub294\ub370 \uc5ec\uae30\uc11c\ub294 \ud2b9\uc774\ud558\uac8c\ub3c4 GlobalAveragePooling1D \ub97c \uc0ac\uc6a9\ud558\uc600\ub2e4. \uc544\ub9c8 \uc704\uc5d0\uc11c hidden-states \ub97c averaging \ud558\uac70\ub098 pooling \ud558\ub294\uac8c \uc88b\ub2e4\uace0 \ud574\uc11c \uadf8\ub7f0\uac83 \uac19\ub2e4.","53deb4fd":"# \ubaa8\ub378\uc744 \ubc14\uafd4\uc11c \uc131\ub2a5\uc744 \ub192\uc5ec\ubcf4\uc790.\n- [Contradictory, My Dear Watson using XLNI Robert2](https:\/\/www.kaggle.com\/rahulbana\/contradictory-my-dear-watson-using-xlni-robert2)\n  - https:\/\/huggingface.co\/joeddav\/xlm-roberta-large-xnli \ubaa8\ub378 \uc0ac\uc6a9","779db630":"# Submission"}}