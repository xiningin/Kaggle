{"cell_type":{"5bb26607":"code","5fe35e63":"code","3ae58fe2":"code","243b49cb":"code","463a56ec":"code","5e648fca":"code","867b063f":"code","dbe46314":"code","c356505e":"code","6d9de2f9":"code","b2b58b62":"code","19a54e4e":"code","25720625":"code","a5536e18":"code","239cce06":"code","49736e26":"code","8963840a":"code","c7a23a3e":"code","ddc63599":"code","3ba7f326":"code","6ca6b82f":"code","a56d1be3":"code","cebc1436":"code","bdde7df9":"code","efbfefa3":"code","28affe1e":"code","948d5e90":"code","abaf7a6b":"code","f424397e":"code","dd03a34b":"code","65533908":"code","2bde9100":"code","de6ead88":"code","448366d0":"code","86657c18":"code","fc5c99d5":"code","e00baac4":"code","d04d638c":"code","dbd00d53":"code","3e064509":"code","ee8326d8":"code","1671d8f5":"code","92e8b641":"code","2e039051":"code","f00049f7":"code","c7671497":"code","539e3175":"code","cd66ebf6":"code","16d92680":"code","50f2b800":"markdown","b2cb1121":"markdown","ed812296":"markdown","3daf19e4":"markdown","8b1b0bb2":"markdown","25869ed9":"markdown","29ce522b":"markdown","c141ac86":"markdown","a70950ea":"markdown","f7b4e5d0":"markdown","8d80ef95":"markdown","4ad4311b":"markdown","09469947":"markdown","b5030373":"markdown","1d094321":"markdown","1e32185c":"markdown","9155ba47":"markdown","22d218a1":"markdown","9c48af28":"markdown","fe8b62ce":"markdown","2f60940c":"markdown","b8d655c6":"markdown","431cb147":"markdown","f81ae070":"markdown","0d634b03":"markdown","630ef916":"markdown","45856565":"markdown","539d2642":"markdown","d1cf7345":"markdown","9b57be24":"markdown","01fed79c":"markdown","60f1dc2e":"markdown","42d66fda":"markdown","02d1359e":"markdown","e8f44ccd":"markdown","35fab1aa":"markdown","5f7b0767":"markdown","992e47b1":"markdown","c5d674b8":"markdown","a81196f3":"markdown","4d8d9591":"markdown","96c7fb59":"markdown","52e569ab":"markdown","ed8a639f":"markdown","e26be58e":"markdown"},"source":{"5bb26607":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom IPython.display import display\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.gridspec as gridspec\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport tensorflow as tf\nimport optuna\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nimport missingno as msno\nimport scipy.stats as stats \nfrom scipy.special import boxcox1p\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5fe35e63":"train_data = pd.read_csv(\"\/kaggle\/input\/ventilator-pressure-prediction\/train.csv\")","3ae58fe2":"train_data.head()","243b49cb":"train_data.tail()","463a56ec":"train_data.columns","5e648fca":"print('lenght of data is', len(train_data))","867b063f":"train_data.shape","dbe46314":"train_data.info()","c356505e":"train_data.dtypes","6d9de2f9":"train_data[train_data.isnull().any(axis=1)].head()","b2b58b62":"np.sum(train_data.isnull().any(axis=1))","19a54e4e":"train_data.isnull().values.any()","25720625":"train_data.isnull().sum()","a5536e18":"test_data = pd.read_csv(\"\/kaggle\/input\/ventilator-pressure-prediction\/test.csv\")\nids_test_data = test_data['id'].values","239cce06":"test_data.head()","49736e26":"test_data.tail()","8963840a":"test_data.columns","c7a23a3e":"print('lenght of data is', len(test_data))","ddc63599":"test_data.shape","3ba7f326":"test_data.info()","6ca6b82f":"test_data.dtypes","a56d1be3":"test_data[test_data.isnull().any(axis=1)].head()","cebc1436":"np.sum(test_data.isnull().any(axis=1))","bdde7df9":"test_data.isnull().values.any()","efbfefa3":"test_data.isnull().sum()","28affe1e":"NANColumns=[]\ni=-1\nfor a in train_data.isnull().sum():\n    i+=1\n    if a!=0:\n        print(train_data.columns[i],a)\n        NANColumns.append(train_data.columns[i])","948d5e90":"NANColumns=[]\ni=-1\nfor a in test_data.isnull().sum():\n    i+=1\n    if a!=0:\n        print(test_data.columns[i],a)\n        NANColumns.append(test_data.columns[i])","abaf7a6b":"train_data.hist(figsize=(50,50),bins = 20, color=\"#107009AA\")\nplt.title(\"Features\/Columns Distribution with values counts\")\nplt.show()","f424397e":"corr_feat = train_data.corr().nlargest(10,\"pressure\")[\"pressure\"].index\ncmap = np.corrcoef(train_data[corr_feat].values.T)\nmask = np.zeros_like(cmap,dtype=bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,10))\nsns.heatmap(cmap,\n            annot=True,\n            fmt=\".3f\",\n            annot_kws = {\"size\":10},\n            cmap=sns.cubehelix_palette(),\n            xticklabels = corr_feat.values,\n            yticklabels = corr_feat.values,\n            mask=mask)","dd03a34b":"y = train_data[\"pressure\"]","65533908":"all_data = pd.concat([train_data,test_data],axis=0).reset_index(drop=True)","2bde9100":"all_data = all_data.drop([\"pressure\",\"id\"],axis=1)","de6ead88":"def missing_value(df):\n    number = df.isnull().sum().sort_values(ascending=False)\n    number = number[number > 0]\n    percentage = df.isnull().sum() *100 \/ df.shape[0]\n    percentage = percentage[percentage > 0].sort_values(ascending=False)\n    return  pd.concat([number,percentage],keys=[\"Total\",\"Percentage\"],axis=1)\nmissing_value(all_data)","448366d0":"## Bias feature reducer\nbias_feat = []\nfor feat in all_data.columns:\n    counts = all_data[feat].value_counts().iloc[0] ## mode value counts\n    if counts \/ len(all_data) * 100 > 99.94:\n        bias_feat.append(feat)\n\nbias_feat","86657c18":"## Remove the bias feature from the dataset\nall_data = all_data.drop(bias_feat,axis=1)","fc5c99d5":"n = len(y)\ntrain_data = all_data[:n]\ntest_data = all_data[n:]","e00baac4":"X_train, X_test, y_train, y_test =  train_test_split(train_data,y,test_size=0.33,random_state=42)\nprint(\"Shapes of data: \", X_train.shape, X_test.shape, y_train.shape, y_test.shape)","d04d638c":"## Create an empty list\npipeline_models = []\n\n# Assign all models into the list\nseed = 42\nmodels = [Ridge(tol=10,random_state=seed),\n          Lasso(tol=1,random_state=seed),\n          RandomForestRegressor(random_state=seed),\n          ExtraTreesRegressor(random_state=seed),\n          GradientBoostingRegressor(),\n          DecisionTreeRegressor(),\n          KNeighborsRegressor()]\n\nmodel_names = [\"Ridge\",\"Lasso\",\"RFR\",\"ETR\",\"GBoost_Reg\",\"DT_Reg\",\"KNN_Reg\"]\n\n## Assign each model to a pipeline\nfor name, model in zip(model_names,models):\n    pipeline = (\"Scaled_\"+ name,\n                Pipeline([(\"Scaler\",StandardScaler()),\n                          (name,model)\n                         ]))\n    pipeline_models.append(pipeline)","dbd00d53":"## Create a dataframe to store all the models' cross validation score\nevaluate = pd.DataFrame(columns=[\"model\",\"cv\",\"std\"])\n\n\n## Encoded dataset\nfor name,model in pipeline_models:\n    kfold = KFold(n_splits=7,shuffle=True,random_state=42)\n    cv = cross_val_score(model, X_train, y_train, cv=kfold, n_jobs=-1, scoring=\"r2\")\n    \n    row = evaluate.shape[0]\n    evaluate.loc[row,\"model\"] = name\n    evaluate.loc[row,\"cv\"] = round(cv.mean(),3)\n    evaluate.loc[row,\"std\"] = \"+\/- {}\".format(round(cv.std(),4))\n    \n    evaluate = evaluate.sort_values(\"cv\",ascending=False)","3e064509":"## Visualization\nfig, ax = plt.subplots(1,1,sharey=False,figsize=(16,9))\n\n## Encoded dataset\nbar = sns.barplot(evaluate[\"model\"], evaluate[\"cv\"],ax=ax,palette = sns.cubehelix_palette(evaluate.shape[0]))\nfor rec in bar.patches:\n    height = rec.get_height()\n    ax.text(rec.get_x() + rec.get_width()\/2, height*1.02,height,ha=\"center\")\nax.set_title(\"Cross Validate Score\")\nax.set_xticklabels(evaluate[\"model\"].to_list(),rotation =50)","ee8326d8":"final_model = GradientBoostingRegressor()\nfinal_model = final_model.fit(X_train,y_train)","1671d8f5":"submission_results = pd.read_csv(\"\/kaggle\/input\/ventilator-pressure-prediction\/sample_submission.csv\")\nsubmission_results.iloc[:,1] = np.floor(np.expm1(final_model.predict(test_data)))\nsubmission_results.to_csv('submission_results', index=False)","92e8b641":"DEBUG = False\n\ntrain = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nsubmission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\n\nif DEBUG:\n    train = train[:80*10000]","2e039051":"def add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag'] = df['u_in'].shift(2).fillna(0)\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df = pd.get_dummies(df)\n    return df\n\ntrain = add_features(train)\ntest = add_features(test)","f00049f7":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)","c7671497":"RS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)","539e3175":"train = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","cd66ebf6":"EPOCH = 200\nBATCH_SIZE = 1024\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nwith tpu_strategy.scope():\n    kf = KFold(n_splits=5, shuffle=True, random_state=2021)\n    test_preds = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        model = keras.models.Sequential([\n            keras.layers.Input(shape=train.shape[-2:]),\n            keras.layers.Bidirectional(keras.layers.LSTM(300, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(250, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(150, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(100, return_sequences=True)),\n            keras.layers.Dense(50, activation='selu'),\n            keras.layers.Dense(1),\n        ])\n        model.compile(optimizer=\"adam\", loss=\"mae\")\n\n        scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)\/BATCH_SIZE), 1e-5)\n        lr = LearningRateScheduler(scheduler, verbose=1)\n\n        #es = EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1, mode=\"min\", restore_best_weights=True)\n\n        model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr])\n        #model.save(f'Fold{fold+1} RNN Weights')\n        test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())\n","16d92680":"submission[\"pressure\"] = sum(test_preds)\/5\nsubmission.to_csv('submission.csv', index=False)","50f2b800":"# Count of missing values","b2cb1121":"# Length of data","ed812296":"<div class=\"alert alert-block alert-info\">  \n    <h1><strong>\ud83d\udc68\u200d\ud83d\udcbb Getting Started with Google Brain - Ventilator Pressure Prediction<\/strong><\/h1>\n    <i><\/i>\n<\/div>","3daf19e4":"# Checking missing Values","8b1b0bb2":"<div class=\"alert alert-block alert-success\">  \n<h1><center><strong> Submitting the predicted pressure on test data<\/strong><\/center><\/h1>\n        \n<\/div>","25869ed9":"# Data information","29ce522b":"# Shape of data","c141ac86":"# Exploratory data analysis of train data","a70950ea":"# Counts of missing values in each column","f7b4e5d0":"# Checking missing Values","8d80ef95":"# Now splitting the data for training and testing with same index ID's","4ad4311b":"# <img src=\"https:\/\/thumbs.dreamstime.com\/t\/bright-colorful-thank-you-banner-vector-overlapping-letters-118244535.jpg\">","09469947":"<div class=\"alert alert-block alert-danger\">  \n    <h1><strong>Loading training data<\/strong><\/h1>\n    <i><\/i>\n<\/div>","b5030373":"## A function for checking the missing values","1d094321":"# Five last records of data","1e32185c":"# Data types of all coloumns","9155ba47":"# Splitting the Train data into 70% for training and 30% for testing ","22d218a1":"# Hitogram of all columns where we are going to check that how the values of each column distributed with their counts","9c48af28":"# Coloumns\/features in data","fe8b62ce":"# <img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/29594\/logos\/header.png?t=2021-07-29-12-44-09\">","2f60940c":"## Drop the pressure & Id columns","b8d655c6":"# Length of data","431cb147":"<div class=\"alert alert-block alert-info\">  \n<h2><center><strong> Building the models for training and testing<\/strong><\/center><\/h2>\n        \n<\/div>","f81ae070":"<div class=\"alert alert-block alert-danger\">  \n    <h1><strong>Loading testing data<\/strong><\/h1>\n    <i><\/i>\n<\/div>","0d634b03":"# Is there any missing values?","630ef916":"# Importing Python Libraries \ud83d\udcd5 \ud83d\udcd7 \ud83d\udcd8 \ud83d\udcd9","45856565":"# Shape of data","539d2642":"## Extract the pressure out","d1cf7345":"# Five top records of data","9b57be24":"# Coloumns\/features in data","01fed79c":"<div class=\"alert alert-block alert-info\">  \n<h2><center><strong>Data Processing<\/strong><\/center><\/h2>\n        \n<\/div>","60f1dc2e":"# Data types of all coloumns","42d66fda":"## Correlation of Pressure on training data","02d1359e":"# Is there any missing values?","e8f44ccd":"<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong> Best Model is Gradient Boosting Regressor<\/strong><\/center><\/h2>\n        \n<\/div>","35fab1aa":"# Counts of missing values in each column","5f7b0767":"# Looking at the test data missing values.","992e47b1":"## Combining the train and test dataset","c5d674b8":"# Data information","a81196f3":"# Exploratory data analysis of test data","4d8d9591":"# Looking at the train data missing values.","96c7fb59":"# Count of missing values","52e569ab":"# Five top records of data","ed8a639f":"<div class=\"alert alert-block alert-info\">  \n<h2><center><strong> Training the models<\/strong><\/center><\/h2>\n        \n<\/div>","e26be58e":"# Five last records of data"}}