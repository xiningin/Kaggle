{"cell_type":{"7156c63d":"code","a45ab1e3":"code","58ceb263":"code","1b0103f4":"code","2b3379f2":"code","929948c6":"code","cc4682de":"code","af9fa516":"code","1b720535":"code","779b1ffb":"code","a23259e6":"code","9a106d70":"code","167c0fe3":"code","cf35c648":"code","04044094":"code","b472c609":"code","3f238c25":"code","b91c7db9":"code","ec95c875":"code","c65dc70d":"code","f65acb1b":"code","a85ac2f8":"code","8dba3568":"code","b20909ff":"code","a53fc9a8":"code","5db4b04a":"code","cf319cbc":"code","d35e2497":"code","e3b347b7":"code","b61d56e6":"code","64b41f08":"code","ba24cc31":"code","c7d2c472":"code","bf717772":"code","20790a36":"code","fd654af3":"code","ed3dfadb":"code","350c862f":"code","dc7db8b1":"code","61be8c14":"code","f693d248":"code","4ab5417a":"code","32ad5170":"code","9a932a13":"code","92cdae72":"code","a03fc14d":"code","71eb94cb":"markdown","cdc3916b":"markdown","ae687946":"markdown","2a5a99fa":"markdown","0d7f739c":"markdown","f25fb910":"markdown","eec8baa5":"markdown","e1dbde80":"markdown","631cef09":"markdown","45a25a06":"markdown","e9c57dc3":"markdown","7a198c1e":"markdown","8456dbc1":"markdown"},"source":{"7156c63d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","a45ab1e3":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')","58ceb263":"train.head()","1b0103f4":"train.head()","2b3379f2":"train.shape","929948c6":"train.info()","cc4682de":"train.isna().sum()","af9fa516":"plt.hist(data = train , x = 'Age')","1b720535":"train['Age'].fillna(round(train['Age'].mean()) , inplace = True)","779b1ffb":"train['Embarked'].unique()","a23259e6":"train['Embarked'].fillna(train['Embarked'].mode()[0] , inplace = True)","9a106d70":"train.Cabin.unique()","167c0fe3":"train['Cabin'].value_counts(ascending = False)","cf35c648":"new_train = train.drop(['Cabin','Ticket' , 'PassengerId' , 'Name'] , axis = 1)","04044094":"new_train.isna().sum()","b472c609":"test.columns","3f238c25":"pass_id = test['PassengerId'] \ntest.drop(['PassengerId' , 'Name' , 'Cabin' , 'Ticket'] , axis = 1 , inplace = True)","b91c7db9":"test.isna().sum()","ec95c875":"plt.hist(data = test , x = 'Fare')","c65dc70d":"test.Age.fillna(round(test.Age.mean()) , inplace = True)\ntest.Fare.fillna(round(test.Fare.mean()) , inplace = True)","f65acb1b":"new_train = pd.get_dummies(new_train , drop_first=True)\ntest =  pd.get_dummies(test , drop_first=True)","a85ac2f8":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnew_train[['Age','Fare']] = scaler.fit_transform(new_train[['Age','Fare']])\ntest[['Age','Fare']] = scaler.fit_transform(test[['Age','Fare']])","8dba3568":"new_train.head()","b20909ff":"test.head()","a53fc9a8":"def sigmoid(z):\n    return  1 \/ (1 + np.exp(-z))\n\ndef cost(theta , x ,y):\n    theta = np.matrix(theta)\n    x = np.matrix(x)\n    y = np.matrix(y)\n    first = np.multiply( -y , np.log(sigmoid(x * theta.T)))\n    second = np.multiply((1-y) , np.log(1- sigmoid(x * theta.T)))\n#     return first,second\n    return np.sum(first - second) \/ (len(x))\n\ndef gradient(theta , x , y):\n    theta = np.matrix(theta)\n    x = np.matrix(x)\n    y = np.matrix(y)\n\n    parameters = int(theta.ravel().shape[1])\n    grad = np.zeros(parameters)\n\n    error = sigmoid(x * theta.T) - y\n    for i in range(parameters):\n        term = np.multiply(error , x[:,i])\n        grad[i] = np.sum(term) \/ len(x)\n\n    return grad\n\ndef costReg(theta , x ,y , lr):\n    theta = np.matrix(theta)\n    x = np.matrix(x)\n    y = np.matrix(y)\n    first = np.multiply( -y , np.log(sigmoid(x * theta.T)))\n    second = np.multiply((1-y) , np.log(1- sigmoid(x * theta.T)))\n    reg = (lr \/ 2*len(x)) * np.sum(np.power(theta[:,1:],2))\n    return np.sum(first - second) \/ (len(x)) + reg\n\n\ndef gradientReg(theta , x , y , lr):\n    theta = np.matrix(theta)\n    x = np.matrix(x)\n    y = np.matrix(y)\n\n    parameters = int(theta.ravel().shape[1])\n    grad = np.zeros(parameters)\n\n    error = sigmoid(x * theta.T) - y\n    for i in range(parameters):\n        term = np.multiply(error , x[:,i])\n        if i==0:\n            grad[i] = np.sum(term) \/ len(x)\n        else:\n            grad[i] = (np.sum(term) \/ len(x)) + ((lr\/len(x))*theta[:,i:])\n    return grad\n                                                 \n                                                 ","5db4b04a":"x_train = new_train.iloc[:,1:]\ny_train = new_train.iloc[:,0]\nx_test = test","cf319cbc":"x_train.insert(0,'ones',1)\ntheta = np.zeros(x_train.shape[1])","d35e2497":"x_train.shape , theta.shape , y_train.shape","e3b347b7":"learningRate = 1","b61d56e6":"import scipy.optimize as opt\nresult1 = opt.fmin_tnc(func = cost , x0 = theta , fprime = gradient ,\n                       args=(x_train,y_train))\n\n# result2 = opt.fmin_tnc(func = costReg , x0 = theta , fprime = gradientReg ,\n#                        args=(x_train,y_train,learningRate))","64b41f08":"def predict(theta , x_test):\n    prob = sigmoid(np.dot(x_test , theta.T))\n#     return prob\n    return [1 if x >= 0.5 else 0 for x in prob]","ba24cc31":"result1[0]","c7d2c472":"cost(np.ones(x_train.shape[1]) , x_train , y_train) , cost(result1[0] , x_train , y_train) \n\npred = predict(result1[0],x_train)","bf717772":"correct = [1 if ((a == 1 and b == 1) or\n                (a == 0 and b == 0)) else 0\n          for (a,b) in zip(pred ,y_train )]","20790a36":"sum(correct) \/ len(correct)","fd654af3":"sum(round(x_train.iloc[0,:]*result1[0],2)) ,sum(round(x_train.iloc[5,:]*result1[0],2))","ed3dfadb":"from sklearn.linear_model import LogisticRegression","350c862f":"x_train.drop(['ones'] , axis = 1 , inplace = True)\nlog = LogisticRegression()\nclf = log.fit(x_train , y_train)\n","dc7db8b1":"clf.score(x_train , y_train)","61be8c14":"from sklearn import svm","f693d248":"clf = svm.SVC(C = 1 ,kernel='poly' , gamma = 'scale')","4ab5417a":"clf.fit(x_train , y_train)","32ad5170":"clf.score(x_train , y_train)","9a932a13":"pred = clf.predict(x_test)","92cdae72":"new_data = pd.DataFrame({'PassengerId':pass_id , 'Survived':pred})","a03fc14d":"new_data.to_csv('submission.csv', index=False)","71eb94cb":"# Gradient Descent and cost function","cdc3916b":"> **`C` when it increase the regularization will be increase but it will make over fitting also when it's decrease it will make under fitting so we should't increase it in high number or decrease it in low number it should be a good number and it's default = 1**","ae687946":"# Explore data","2a5a99fa":"# Logistic Regression","0d7f739c":"# Support Vector Machine","f25fb910":"#### Now let's explore the Cabin (categorical data)","eec8baa5":"> **it seems like right skewed**\n\n>**so we can replace the nan value by the mean**","e1dbde80":"#### we have many passengers that don't have cabin data we have only 204 from 891 so we will remove it now","631cef09":"# implement classification algorithm","45a25a06":"### Before we make the new processes let's remove the columns from test data","e9c57dc3":"## univariate exploration","7a198c1e":"> **I will try to implement classification algorithm first with gradient descent and cost function**","8456dbc1":"> **we will fill na here by the mode value**"}}