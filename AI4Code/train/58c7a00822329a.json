{"cell_type":{"160ed14e":"code","5b003103":"code","50ce8b5c":"code","211c1f52":"code","65ffb22a":"code","4c0f9e05":"code","3c246171":"code","c96b608a":"code","4f8fb416":"code","9a41f149":"code","677b4676":"code","7e786bef":"code","ee44265c":"code","656d9eba":"code","8ca887cb":"code","72e041ba":"code","d32d5c0d":"code","b79cdfec":"code","7e51dfef":"code","6fff121c":"code","0c4a87b6":"code","6aa202ef":"code","9a988ce8":"code","c0bb205d":"code","45532d9a":"code","2ab02758":"code","206b0446":"code","a8b18c8b":"code","9013331e":"code","5b607281":"code","30257959":"code","58601cd6":"code","8e8e1e99":"code","025a312e":"code","01fba5bb":"markdown"},"source":{"160ed14e":"import pandas as pd\nfull_data = pd.read_csv('..\/input\/weather-in-aus\/weatherAUS.csv')\nfull_data.head()","5b003103":"full_data['Date'] = pd.to_datetime(full_data['Date'])\nfull_data['year'] = full_data['Date'].dt.year\nfull_data['month'] = full_data['Date'].dt.month\nfull_data['day'] = full_data['Date'].dt.day\nfull_data.drop(['Date'], axis = 1,inplace=True) \nfull_data.head()","50ce8b5c":"full_data.shape","211c1f52":"full_data.info()","65ffb22a":"full_data['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\nfull_data['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)\nfull_data.head()","4c0f9e05":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (20,5))\nax=full_data.RainTomorrow.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\nplt.title('RainTomorrow Indicator No(0) and Yes(1) in the Imbalanced Dataset')\nfor p in ax.patches:\n    ax.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.show()","3c246171":"from sklearn.utils import resample\n\nno = full_data[full_data.RainTomorrow == 0]\nyes = full_data[full_data.RainTomorrow == 1]\nyes_oversampled = resample(yes, replace=True, n_samples=len(no), random_state=42)\noversampled = pd.concat([no, yes_oversampled])\n\nfig = plt.figure(figsize = (20,5))\nax=oversampled.RainTomorrow.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\nplt.title('RainTomorrow Indicator No(0) and Yes(1) after Oversampling (Balanced Dataset)')\nfor p in ax.patches:\n    ax.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.show()","c96b608a":"# Missing Data Pattern in Training Data\nimport seaborn as sns\nplt.figure(figsize = (20,5))\nsns.heatmap(oversampled.isnull(), cbar=False, cmap='PuBu')\nplt.show()","4f8fb416":"total = oversampled.isnull().sum().sort_values(ascending=False)\npercent = (oversampled.isnull().sum()\/oversampled.isnull().count()).sort_values(ascending=False)\nmissing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing.head()","9a41f149":"oversampled.select_dtypes(include=['object']).columns","677b4676":"# Impute categorical var with Mode\noversampled['Location'] = oversampled['Location'].fillna(oversampled['Location'].mode()[0])\noversampled['WindGustDir'] = oversampled['WindGustDir'].fillna(oversampled['WindGustDir'].mode()[0])\noversampled['WindDir9am'] = oversampled['WindDir9am'].fillna(oversampled['WindDir9am'].mode()[0])\noversampled['WindDir3pm'] = oversampled['WindDir3pm'].fillna(oversampled['WindDir3pm'].mode()[0])","7e786bef":"# Convert categorical features to continuous features with Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nlencoders = {}\nfor col in oversampled.select_dtypes(include=['object']).columns:\n    lencoders[col] = LabelEncoder()\n    oversampled[col] = lencoders[col].fit_transform(oversampled[col])","ee44265c":"import warnings\nwarnings.filterwarnings(\"ignore\")\n# Multiple Imputation by Chained Equations\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nMiceImputed = oversampled.copy(deep=True) \nmice_imputer = IterativeImputer()\nMiceImputed.iloc[:, :] = mice_imputer.fit_transform(oversampled)","656d9eba":"# Detecting outliers with IQR\nQ1 = MiceImputed.quantile(0.25)\nQ3 = MiceImputed.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","8ca887cb":"# Removing outliers from the dataset\nMiceImputed = MiceImputed[~((MiceImputed < (Q1 - 1.5 * IQR)) |(MiceImputed > (Q3 + 1.5 * IQR))).any(axis=1)]\nMiceImputed.shape","72e041ba":"# Correlation Heatmap\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorr = MiceImputed.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20,20))\ncmap = sns.diverging_palette(250, 25, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=True, linewidths=.5, cbar_kws={\"shrink\": .9})\nplt.show()","d32d5c0d":"sns.pairplot( data=MiceImputed, vars=('MaxTemp','MinTemp','Pressure9am','Pressure3pm'), hue='RainTomorrow' )\nplt.show()","b79cdfec":"sns.pairplot( data=MiceImputed, vars=('Temp9am', 'Temp3pm', 'Evaporation'), hue='RainTomorrow' )\nplt.show()","7e51dfef":"# Standardizing data\nfrom sklearn import preprocessing\nr_scaler = preprocessing.MinMaxScaler()\nr_scaler.fit(MiceImputed)\nmodified_data = pd.DataFrame(r_scaler.transform(MiceImputed), index=MiceImputed.index, columns=MiceImputed.columns)","6fff121c":"# Feature Importance using Filter Method (Chi-Square)\nfrom sklearn.feature_selection import SelectKBest, chi2\nX = modified_data.loc[:,modified_data.columns!='RainTomorrow']\ny = modified_data[['RainTomorrow']]\nselector = SelectKBest(chi2, k=10)\nselector.fit(X, y)\nX_new = selector.transform(X)\nprint(X.columns[selector.get_support(indices=True)])","0c4a87b6":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier as rf\n\nX = MiceImputed.drop('RainTomorrow', axis=1)\ny = MiceImputed['RainTomorrow']\nselector = SelectFromModel(rf(n_estimators=100, random_state=0))\nselector.fit(X, y)\nsupport = selector.get_support()\nfeatures = X.loc[:,support].columns.tolist()\nprint(features)\nprint(rf(n_estimators=100, random_state=0).fit(X,y).feature_importances_)","6aa202ef":"features = MiceImputed[['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', \n                       'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', \n                       'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', \n                       'RainToday']]\ntarget = MiceImputed['RainTomorrow']\n\n# Split into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42,\n                                                    shuffle=True, stratify=target)\n\n# Normalize Features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","9a988ce8":"def plot_roc_cur(fper, tper):  \n    plt.plot(fper, tper, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","c0bb205d":"import time\nfrom sklearn.metrics import accuracy_score, roc_auc_score, cohen_kappa_score, plot_confusion_matrix, roc_curve, classification_report\ndef run_model(model, X_train, y_train, X_test, y_test, verbose=True):\n    t0=time.time()\n    if verbose == False:\n        model.fit(X_train,y_train, verbose=0)\n    else:\n        model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred) \n    coh_kap = cohen_kappa_score(y_test, y_pred)\n    time_taken = time.time()-t0\n    print(\"Accuracy = {}\".format(accuracy))\n    print(\"ROC Area under Curve = {}\".format(roc_auc))\n    print(\"Cohen's Kappa = {}\".format(coh_kap))\n    print(\"Time taken = {}\".format(time_taken))\n    print(classification_report(y_test,y_pred,digits=5))\n    \n    probs = model.predict_proba(X_test)  \n    probs = probs[:, 1]  \n    fper, tper, thresholds = roc_curve(y_test, probs) \n    plot_roc_cur(fper, tper)\n    \n    plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues, normalize = 'all')\n    \n    return model, accuracy, roc_auc, coh_kap, time_taken\n","45532d9a":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nparams_lr = {'penalty': 'l1', 'solver':'liblinear'}\n\nmodel_lr = LogisticRegression(**params_lr)\nmodel_lr, accuracy_lr, roc_auc_lr, coh_kap_lr, tt_lr = run_model(model_lr, X_train, y_train, X_test, y_test)","2ab02758":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nparams_dt = {'max_depth': 16,\n             'max_features': \"sqrt\"}\n\nmodel_dt = DecisionTreeClassifier(**params_dt)\nmodel_dt, accuracy_dt, roc_auc_dt, coh_kap_dt, tt_dt = run_model(model_dt, X_train, y_train, X_test, y_test)\n\n","206b0446":"# Neural Network\nfrom sklearn.neural_network import MLPClassifier\n\nparams_nn = {'hidden_layer_sizes': (30,30,30),\n             'activation': 'logistic',\n             'solver': 'lbfgs',\n             'max_iter': 500}\n\nmodel_nn = MLPClassifier(**params_nn)\nmodel_nn, accuracy_nn, roc_auc_nn, coh_kap_nn, tt_nn = run_model(model_nn, X_train, y_train, X_test, y_test)","a8b18c8b":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nparams_rf = {'max_depth': 16,\n             'min_samples_leaf': 1,\n             'min_samples_split': 2,\n             'n_estimators': 100,\n             'random_state': 42}\n\nmodel_rf = RandomForestClassifier(**params_rf)\nmodel_rf, accuracy_rf, roc_auc_rf, coh_kap_rf, tt_rf = run_model(model_rf, X_train, y_train, X_test, y_test)","9013331e":"# Light GBM\nimport lightgbm as lgb\nparams_lgb ={'colsample_bytree': 0.95, \n         'max_depth': 16, \n         'min_split_gain': 0.1, \n         'n_estimators': 200, \n         'num_leaves': 50, \n         'reg_alpha': 1.2, \n         'reg_lambda': 1.2, \n         'subsample': 0.95, \n         'subsample_freq': 20}\n\nmodel_lgb = lgb.LGBMClassifier(**params_lgb)\nmodel_lgb, accuracy_lgb, roc_auc_lgb, coh_kap_lgb, tt_lgb = run_model(model_lgb, X_train, y_train, X_test, y_test)\n","5b607281":"# Catboost\n!pip install catboost\nimport catboost as cb\nparams_cb ={'iterations': 50,\n            'max_depth': 16}\n\nmodel_cb = cb.CatBoostClassifier(**params_cb)\nmodel_cb, accuracy_cb, roc_auc_cb, coh_kap_cb, tt_cb = run_model(model_cb, X_train, y_train, X_test, y_test, verbose=False)\n","30257959":"# XGBoost\nimport xgboost as xgb\nparams_xgb ={'n_estimators': 500,\n            'max_depth': 16}\n\nmodel_xgb = xgb.XGBClassifier(**params_xgb)\nmodel_xgb, accuracy_xgb, roc_auc_xgb, coh_kap_xgb, tt_xgb = run_model(model_xgb, X_train, y_train, X_test, y_test)\n","58601cd6":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.plotting import plot_decision_regions\n\nvalue = 1.80\nwidth = 0.90\n\nclf1 = LogisticRegression(random_state=42)\nclf2 = DecisionTreeClassifier(random_state=42) \nclf3 = MLPClassifier(random_state=42, verbose = 0)\nclf4 = RandomForestClassifier(random_state=42)\nclf5 = lgb.LGBMClassifier(random_state=42, verbose = 0)\nclf6 = cb.CatBoostClassifier(random_state=42, verbose = 0)\nclf7 = xgb.XGBClassifier(random_state=42)\neclf = EnsembleVoteClassifier(clfs=[clf4, clf5, clf6, clf7], weights=[1, 1, 1, 1], voting='soft')\n\nX_list = MiceImputed[[\"Sunshine\", \"Humidity9am\", \"Cloud3pm\"]] #took only really important features\nX = np.asarray(X_list, dtype=np.float32)\ny_list = MiceImputed[\"RainTomorrow\"]\ny = np.asarray(y_list, dtype=np.int32)\n\n# Plotting Decision Regions\ngs = gridspec.GridSpec(3,3)\nfig = plt.figure(figsize=(18, 14))\n\nlabels = ['Logistic Regression',\n          'Decision Tree',\n          'Neural Network',\n          'Random Forest',\n          'LightGBM',\n          'CatBoost',\n          'XGBoost',\n          'Ensemble']\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf7, eclf],\n                         labels,\n                         itertools.product([0, 1, 2],\n                         repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, \n                                filler_feature_values={2: value}, \n                                filler_feature_ranges={2: width}, \n                                legend=2)\n    plt.title(lab)\n\nplt.show()","8e8e1e99":"accuracy_scores = [accuracy_lr, accuracy_dt, accuracy_nn, accuracy_rf, accuracy_lgb, accuracy_cb, accuracy_xgb]\nroc_auc_scores = [roc_auc_lr, roc_auc_dt, roc_auc_nn, roc_auc_rf, roc_auc_lgb, roc_auc_cb, roc_auc_xgb]\ncoh_kap_scores = [coh_kap_lr, coh_kap_dt, coh_kap_nn, coh_kap_rf, coh_kap_lgb, coh_kap_cb, coh_kap_xgb]\ntt = [tt_lr, tt_dt, tt_nn, tt_rf, tt_lgb, tt_cb, tt_xgb]\n\nmodel_data = {'Model': ['Logistic Regression','Decision Tree','Neural Network','Random Forest','LightGBM','Catboost','XGBoost'],\n              'Accuracy': accuracy_scores,\n              'ROC_AUC': roc_auc_scores,\n              'Cohen_Kappa': coh_kap_scores,\n              'Time taken': tt}\ndata = pd.DataFrame(model_data)\n\nfig, ax1 = plt.subplots(figsize=(12,10))\nax1.set_title('Model Comparison: Accuracy and Time taken for execution', fontsize=13)\ncolor = 'tab:green'\nax1.grid()\nax1.set_xlabel('Model', fontsize=13)\nax1.set_ylabel('Time taken', fontsize=13, color=color)\nax2 = sns.barplot(x='Model', y='Time taken', data = data, palette='summer')\nax1.tick_params(axis='y')\nax2 = ax1.twinx()\ncolor = 'tab:red'\nax2.set_ylabel('Accuracy', fontsize=13, color=color)\nax2 = sns.lineplot(x='Model', y='Accuracy', data = data, sort=False, color=color)\nax2.tick_params(axis='y', color=color)","025a312e":"fig, ax3 = plt.subplots(figsize=(12,10))\nax3.set_title('Model Comparison: Area under ROC and Cohens Kappa', fontsize=13)\ncolor = 'tab:blue'\nax3.grid()\nax3.set_xlabel('Model', fontsize=13)\nax3.set_ylabel('ROC_AUC', fontsize=13, color=color)\nax4 = sns.barplot(x='Model', y='ROC_AUC', data = data, palette='winter')\nax3.tick_params(axis='y')\nax4 = ax3.twinx()\ncolor = 'tab:red'\nax4.set_ylabel('Cohen_Kappa', fontsize=13, color=color)\nax4 = sns.lineplot(x='Model', y='Cohen_Kappa', data = data, sort=False, color=color)\nax4.tick_params(axis='y', color=color)\nplt.show()","01fba5bb":"Rainfall Prediction is one of the difficult and uncertain tasks that have a significant impact on human society. Timely and accurate forecasting can proactively help reduce human and financial loss. This study presents a set of experiments that involve the use of common machine learning techniques to create models that can predict whether it will rain tomorrow or not based on the weather data for that day in major cities in Australia."}}