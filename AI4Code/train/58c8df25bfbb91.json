{"cell_type":{"3a7f448a":"code","6559fde8":"code","7f9d5b85":"code","5a0f7581":"code","6276bc17":"code","7da38bd0":"code","2094f4da":"code","9edeba02":"code","b2de370e":"code","b977b8f3":"code","2c788fa1":"code","e52b53db":"code","fcdd8242":"code","e564c7ff":"code","3b1b8571":"code","5d8452f5":"code","3832894b":"code","58850456":"code","5131fd60":"code","eb339cb1":"code","b8e01626":"code","c201911a":"code","5e495354":"code","e6748de9":"code","cb893ebd":"code","b273da69":"code","0fd8246e":"code","5adf4a89":"code","bc145962":"code","864f1038":"code","7bd7eef0":"code","600fcf36":"code","5ac27b94":"markdown","38c921c3":"markdown","a3931dba":"markdown"},"source":{"3a7f448a":"from mlxtend.plotting import plot_decision_regions\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","6559fde8":"# Loading dataset \ndf = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\n\n# Look at the first 5 rows\ndf.head()","7f9d5b85":"# Basic EDA (Exploratory Data Analysis)\ndf.info()","5a0f7581":"df.describe()","6276bc17":"# .T (transpose the table) --> maybe easier to inspect?\ndf.describe().T\n# we can see that there are variables that have an invalid zero value\n# Glucose, BloodPressure, SkinThickness, Insulin, BMI (at min)","7da38bd0":"# Copy the original dataframe to new variable\ndf_new = df.copy()\n\n# we will replace 0 with NaN\ndf_new[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = df_new[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0,np.NaN)\n\n# check the counts of NaN\nprint(df_new.isnull().sum())","2094f4da":"# But need to understand data distribution first before replacing NaN with methods\np = df.hist(figsize=(10,10))","9edeba02":"# center at the middle -> mean\n# skew -> median\n# object with missing value filled if inplace = True\n# Glucose, BloodPressure, SkinThickness, Insulin, BMI\ndf_new['Glucose'].fillna(df_new['Glucose'].mean(), inplace=True)\ndf_new['BloodPressure'].fillna(df_new['BloodPressure'].mean(), inplace=True)\ndf_new['SkinThickness'].fillna(df_new['SkinThickness'].median(), inplace=True)\ndf_new['Insulin'].fillna(df_new['Insulin'].median(), inplace=True)\ndf_new['BMI'].fillna(df_new['BMI'].mean(), inplace=True)","b2de370e":"# Plotting after NaN removal\np = df_new.hist(figsize=(10,10))","b977b8f3":"df.shape","2c788fa1":"# data type analysis\n# using seaborn\nsns.countplot(y=df_new.dtypes.map(str), data=df_new)\nplt.xlabel(\"Count of each data type\")\nplt.ylabel(\"Data types\")\nplt.show()","e52b53db":"# null count analysis\nimport missingno as msno\npmis = msno.bar(df_new)","fcdd8242":"# checking bias in number of diabetic patients\np = df.Outcome.value_counts().plot(kind='bar')\n# 0 = non-diabetic patients\n# 1 = diabetic patients\n# 0 is almost 2 times higher than 1","e564c7ff":"from pandas.plotting import scatter_matrix\np = scatter_matrix(df, figsize=(20,20))","3b1b8571":"p = sns.pairplot(df_new, hue='Outcome')\n# can be related to Pearson's correlation coefficient","5d8452f5":"# Heatmap for unclean data\np = sns.heatmap(df.corr(), annot=True, cmap='RdYlGn')\nplt.figure(figsize=(18,16))","3832894b":"# Heatmap for clean data\np = sns.heatmap(df_new.corr(), annot=True, cmap='RdYlGn')\nplt.figure(figsize=(18,16))","58850456":"# Scale the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# scale and drop the target (so we get feature --> x)\nX = pd.DataFrame(scaler.fit_transform(df_new.drop([\"Outcome\"],axis=1),), columns=['Pregnancies','Glucose','BloodPressure','SkinThickness',\n                                                                                 'Insulin','BMI','DiabetesPedigreeFunction','Age'])","5131fd60":"# All features are present\nX.head()","eb339cb1":"# Target --> 0 and 1\ny = df_new.Outcome\ny.head()","b8e01626":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)","c201911a":"from sklearn.neighbors import KNeighborsClassifier\n\n# we will try knn range from 1-15 and check its score\ntest_score = []\ntrain_score = []\n\nfor i in range(1,15):\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train, y_train)\n    train_score.append(knn.score(X_train, y_train))\n    test_score.append(knn.score(X_test, y_test))","5e495354":"max_train_score = max(train_score)\nidx_train_max = max(range(len(train_score)), key=train_score.__getitem__) + 1\nprint('Max train score {} % and k = {}'.format(max_train_score*100, idx_train_max))","e6748de9":"max_test_score = max(test_score)\nidx_test_max = max(range(len(test_score)), key=test_score.__getitem__) + 1\nprint('Max test score {} % and k = {}'.format(max_test_score*100, idx_test_max))","cb893ebd":"# Result visualization\nplt.figure(figsize=(15,8))\np = sns.lineplot(range(1,15), train_score, marker='*', label='Train score')\np = sns.lineplot(range(1,15), test_score, marker='o', label='Test score')","b273da69":"# So, the best result (for test score) is k=11, so we will use it for final model\nknn = KNeighborsClassifier(11)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)","0fd8246e":"from sklearn.metrics import confusion_matrix\ny_pred = knn.predict(X_test)\nconfusion_matrix(y_test, y_pred)","5adf4a89":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","bc145962":"from sklearn.metrics import roc_curve\ny_pred_proba = knn.predict_proba(X_test)[:,1]\nfpr, tpr, threshold = roc_curve(y_test, y_pred_proba)","864f1038":"plt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='Knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('Knn(n_neighbors=11) ROC curve')\nplt.show()","7bd7eef0":"#Area under ROC curve (AUC)\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred_proba)","600fcf36":"from sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)} # try n_neighbors from 1 to 50\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5) # cross validate = 5\nknn_cv.fit(X,y)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","5ac27b94":"## Hyper Parameter optimization\nGrid search is an approach to hyperparameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.","38c921c3":"### Model performance analysis\n* Confusion matrix\n* Classification report\n* ROC-AUC","a3931dba":"## OSEMN Pipeline\n* ### O - Obtaining data\n* ### S - Scrubbing data (cleaning)\n* ### E - Exploring data (visualizing to find patterns)\n* ### M - Modeling data\n* ### N - iNterpreting data"}}