{"cell_type":{"d6444fc5":"code","3a992c78":"code","1a0a56e7":"code","11cbfee5":"code","c0a4a0ae":"code","4b7687bf":"code","8d52342d":"code","b98f74e1":"code","a31f765d":"code","9ed26711":"code","7ed57b93":"code","cc4afb7d":"code","887ba389":"code","58324b40":"code","857543c8":"code","a5bfa3a6":"markdown","4efe1e23":"markdown","1cdb13dd":"markdown","e11ccaac":"markdown","4ac00f7f":"markdown","427d9504":"markdown","079c410f":"markdown","e0cb257b":"markdown"},"source":{"d6444fc5":"import matplotlib.pyplot as plt\nimport pylab\n\nimport numpy as np\nimport os \nimport warnings\nimport pandas as pd \nimport pickle\nfrom tqdm import tqdm \n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\ntf.random.set_seed(12345)\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, TimeDistributed,Flatten, Conv3D, MaxPooling3D\nfrom tensorflow.keras import Sequential\n\nimport random\nrandom.seed(0)","3a992c78":"def load_from_csv(DIR):\n    '''\n    helper function to load all data from one directory\n    :param DIR: directory to load data from\n    :return x_values: values read from the files\n    '''\n    filenames = [name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]\n    samples_per_row = len(pd.read_csv(os.path.join(DIR, filenames[0]), sep=\"\\t\", nrows=1).columns)\n    x_values = np.zeros([len(filenames), 20480, samples_per_row])\n    filenames = sorted(filenames)\n    for i,file in  tqdm (enumerate(filenames),  desc=\"Reading Data\",ascii=False, ncols=100, total=len(filenames)):\n        x_values[i:i+1,:,:] =np.fromfile(os.path.join(DIR, file), dtype=np.float, sep=\" \").reshape(20480,samples_per_row)\n    return x_values","1a0a56e7":"def load_raw_data(force=False):\n    '''\n    loads the data from all three datasets if the data is not already stored in a pickle, the loaded data is stored in seperate pickles. \n    Because loading all three into memory is quite memory intensive.\n    :param force: defines, whether the program is forced to reload the data from the csv files and ignore any existing pickles.\n    :return : Nothing is returned since the data is stored in a pickle instead\n    '''\n    DIRS = ['..\/input\/bearing-dataset\/1st_test\/1st_test\/',\n            '..\/input\/bearing-dataset\/2nd_test\/2nd_test\/',\n            '..\/input\/bearing-dataset\/3rd_test\/4th_test\/txt\/',\n           ]\n    for i in range(3):\n        if \"test\"+str(i)+\".pkz\" in os.listdir(\".\") and not force:\n            print(\"test\",i, \"already loaded.\")\n            continue\n        x = load_from_csv(DIRS[i])\n        with open(\"test\"+str(i)+\".pkz\", \"wb\") as file:\n            pickle.dump(x, file)\nload_raw_data(force=False)\n","11cbfee5":"def binning(bins, raw_data):\n    '''\n    takes raw_data values and calculates the fft analysis of them. Then divides the fft data into bins and takes the mean of each bin.\n    :param bins: bins to devide the data into \n    :param raw_data: data to analyse and put into bin afterwards\n    :retrun values: the values for each bin with shape:(length of test, number of bearings, number of bins)\n    '''\n    values = np.zeros((raw_data.shape[0],raw_data.shape[2],len(bins)-1))\n    for j in tqdm(range(raw_data.shape[2]),desc=\"Binning Frequencies\",  ascii=True, ncols=100):\n        f = np.fft.fft(raw_data[:,:,j])\n        freq = np.fft.fftfreq(20480)*20000\n        for i in range(len(bins)-1):\n            values[:,j,i]+=np.absolute(f[:,(freq>bins[i])&(freq<=bins[i+1])]).mean(axis=1)\n    return values\n\ndef feature_engeneering(raw_data):\n    '''\n    engineers features of raw data: for each bearing following features are engineered: maximums, standard deviation and frequency bins\n    beacause test 1 measures two values per bearing every other value is dropped so the tests are compareable.\n    :param raw_data: data to engineer features from\n    :return values: engineered values with shape (length of test, number of bearings*number of engineered features)\n    '''\n    if raw_data.shape[2] == 8:\n        raw_data = raw_data[:,:,[0,2,4,6]]\n    bins = np.array([0,250,1000,2500,5000,10000])\n    values = binning(bins,raw_data)\n    maxs = np.expand_dims(abs(raw_data).max(axis=1),2)\n    stds = np.expand_dims(raw_data.std(axis=1),2)\n    values = np.concatenate((maxs, stds, values),axis = 2)\n  \n    values = np.swapaxes(values, 1,2)\n    values = values.reshape((values.shape[0], values.shape[1]*values.shape[2]))\n    return values, bins\n\ndef load_data(force = False):\n    '''\n    loads raw_data from pickle files and then engineers feature from that data. \n    if data.pkz already exists it just loads this pickle \n    :param force: force function to engineer features again even though data.pkz exists\n    :return data: data with engineered features for each test has shape:\n            ((length of test 1, number of bearings*number of engineered features ),\n             (length of test 2, number of bearings*number of engineered features ),\n             (length of test 3, number of bearings*number of engineered features ))\n    '''\n    if \"data.pkz\" in os.listdir(\".\") and not force:\n        print(\"Data already engineered. Loading from pickle\")\n        with open(\"data.pkz\", \"rb\") as file:\n            data = pickle.load(file)\n    else:\n        data = []\n        for i in range(3):\n            with open(\"test\"+str(i)+\".pkz\", \"rb\") as file:\n                raw_data = pickle.load(file)\n            values, bins =  feature_engeneering(raw_data)\n            data.append(values)\n        data = np.array(data)\n        with open(\"data.pkz\", \"wb\") as file:\n            pickle.dump(data, file)\n    return data ","c0a4a0ae":"data = load_data(force = False) # load data ","4b7687bf":"def scale(data,test_size=0.5, minmax=True):\n    '''\n    scales data with the Standard or MinMaxScaler from Scikit-Learn\n    :param data: array to be scaled\n    :param test_size: percentage of the dataset to be treated as test set\n    :param minmax: use Minmax Scaler instead of standard scaler\n    :return values: scaled values\n    '''\n    l = int(data.shape[0]*(1-test_size))\n    if minmax:\n        scaler = MinMaxScaler()\n    else:\n        scaler = StandardScaler()\n    scaler.fit(data[:l])\n    values = scaler.transform(data)\n    return values\n\ndef generate_sequences_no_padding(data, seq_len):\n    '''\n    generates sequences from data without padding\n    :param data: data from which the sequence should be generated\n    :param seq_len: length of each sequence (must be int)\n    :return X: sequences stored in an array with shape: \n            (length of test - sequence length, sequence length, number of bearings*number of features engineered)\n    :return y: values to be predicted. Next value after each sequence has shape:\n            (length of test - sequence length, number of bearings*number of features engineered)\n    '''\n    X = np.zeros([data.shape[0]-seq_len, seq_len, data.shape[1]])\n    for i in tqdm(range (0,seq_len),  desc=\"Generating sequences\",  ascii=True, ncols=100):\n        X[:,i,:] = data[i:-seq_len+i,:]\n    y = data[seq_len:,:]\n    return X,y\n\ndef generate_sequences_pad_front(data, seq_len):\n    '''\n    generates sequences from data with padding zeros in front\n    :param data: data from which the sequence should be generated\n    :param seq_len: length of each sequence (must be int)\n    :return X: sequences stored in an array with shape: \n            (length of test, sequence length, number of bearings*number of features engineered)\n    :return y: values to be predicted. Next value after each sequence has shape:\n            (length of test, number of bearings*number of features engineered)\n    '''\n    X = np.zeros([data.shape[0], seq_len, data.shape[1]])\n    d =  np.pad(data, ((seq_len,0),(0,0)), 'constant')\n    for i in tqdm(range (0,seq_len),  desc=\"Generating sequences\",  ascii=True, ncols=100):\n        X[:,i,:] = d[i:-seq_len+i,:]\n    y = data[:,:]\n    return X,y\n\ndef split_data_set(X,y, test_size = 0.5):\n    '''\n    splits data set into train and test set\n    :param X: data to spilt for X_train and X_test\n    :param y: data to spilt for y_train and y_test\n    :param test_size: percentage of data that should be in the test sets\n    :return X_train, X_test, y_train, y_test: X and y values for train and test\n    '''\n    length = X.shape[0]\n    X_train = X[:int(-length*test_size)]\n    y_train = y[:int(-length*test_size)]\n    X_test = X[int(-length*test_size):]\n    y_test = y[int(-length*test_size):]\n    return X_train, X_test, y_train, y_test\n\ndef prepare_data_series(data, seq_len, test_size=0.5):\n    '''\n    Generates X_train, X_test, y_train, y_test\n    Each of the four arrays contains a dataset for each of the test runs. So if you want to \n    train on the first test your data set would be called by X_train[0].\n    Addiotanally X_train and y_train have the possibility to train on all test at the same time.\n    The values for that are stored in X_train[3] and y_train[3]\n    :param data: data to be used for generation of train and test sets\n    :param seq_len:  length of each sequence (must be int)\n    :param test_size: percentage of data that should be in the test sets\n    :return X_train_series, X_test_series, y_train, y_test: Data sets for test and train, the X_values for each are in sequential form.\n    '''\n    prepared_data = []\n    for d in data:\n        d = scale(d, test_size=test_size, minmax=True)\n        X_series,y_series = generate_sequences_no_padding(d, seq_len)\n        prepared_data.append(split_data_set(X_series,y_series,test_size))\n    prepared_data = np.array(prepared_data)\n    X_train_series = np.array([prepared_data[i][0]for i in range(prepared_data.shape[0])])\n    X_test_series = np.array([prepared_data[i][1]for i in range(prepared_data.shape[0])])\n    y_train = np.array([prepared_data[i][2]for i in range(prepared_data.shape[0])])\n    y_test = np.array([prepared_data[i][3]for i in range(prepared_data.shape[0])])\n\n    # Append combination of all three Training Sets to X_train_series and to y_train\n    _X_train_series = [X_train_series[i] for i in range(3)]\n    _X_train_series.append(np.concatenate(X_train_series))\n    X_train_series = np.array(_X_train_series)\n\n    _y_train = [y_train[i] for i in range(3)]\n    _y_train.append(np.concatenate(y_train))\n    y_train = np.array(_y_train)\n    \n    return X_train_series, X_test_series, y_train, y_test","8d52342d":"test_size = 0.6                 # define size of test set\nfor i in range(3):\n    data[i] = scale(data[i], test_size=test_size, minmax = True) #scale data\nbins = np.array([0,250,1000,2500,5000,10000])           # define bins to sort frequencies into\ntest_names = [\"1st\", \"2nd\", \"3rd\"]                      # test names\ndata_about_tests = [{\"name\": \"1st\", \"length\": 2156, \"broken\": [2,3]},\n                    {\"name\": \"2nd\", \"length\": 984, \"broken\": [0]},  \n                    {\"name\": \"3rd\", \"length\": 6324, \"broken\": [2]}] # data about test displayed in plots","b98f74e1":"seq_len=30 # sequence length\nX_train_series, X_test_series, y_train, y_test = prepare_data_series(data,seq_len, test_size=test_size) # generate train and test sets\n\nsubsequences = 5    # number of subsequences look at in 3D Convolutional layers\ntimesteps = seq_len\/\/subsequences   #timesteps left in each subsequence\nX_train_series_sub = np.array([X_train_series[i].reshape((X_train_series[i].shape[0], \n        subsequences, timesteps,4,X_train_series[i].shape[-1]\/\/4,1)) for i in range(4)]) # generate X_train with sub sequences\nX_test_series_sub = np.array([X_test_series[i].reshape((X_test_series[i].shape[0], \n        subsequences, timesteps,4,X_train_series[i].shape[-1]\/\/4,1))for i in range(3)])  # generate X_test with sub sequences\n\nprint('Train set shape', [X_train_series_sub[i].shape for i in range(4)])\nprint('Test set shape', [X_test_series_sub[i].shape for i in range(3)])","a31f765d":"test=3\ncnn_lstm = Sequential()\ncnn_lstm.add(TimeDistributed(Conv3D(filters=70, kernel_size=(1,2,3), activation='relu'), \n                input_shape=(X_train_series_sub[test].shape[1:])))\ncnn_lstm.add(TimeDistributed(MaxPooling3D(pool_size=(X_train_series_sub[test].shape[2], 2,3))))\ncnn_lstm.add(TimeDistributed(Flatten()))\ncnn_lstm.add(Dropout(0.3))\ncnn_lstm.add(LSTM(50))\n\ncnn_lstm.add(Dense(y_train[test].shape[-1]))\ncnn_lstm.compile(loss='mse', optimizer=\"adam\")\n\ncnn_lstm.summary()","9ed26711":"test=3\ncnn_lstm.fit(X_train_series_sub[test] ,y_train[test] , epochs=150, batch_size=16, validation_split=0.2, verbose=1, shuffle = True,\ncallbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=40, verbose=0, mode='min',\n                                   restore_best_weights=True)])","7ed57b93":"def plot(y_true, bins, data_about_tests, y_pred=np.array([]), anomalies1 = np.array([]),anomalies2 = np.array([]), cols = 4, seperate_legend=False):\n    '''\n    Plots the data in seperate plots if no other constraints are set this will be a grid of 4 columns and 7 rows.\n    Each column represents a bearing from one a particular test run. Each row represents an engineered feature.  \n    :param y_true: true data that was measured in the test run will be displayed in blue\n    :param bins: features that were engineered. In this case it will be [max, std, 0Hz-250Hz, ... , 5000Hz-10000Hz]\n    :param data_about_tests: Dictionary with data about test-run containing the key \"broken\"\n    :param y_pred: predicted data will be displyed in orange\n    :param anomalies1: boolean array containing which True if an anomaly1 alarm was fired at that position\n    :param anomalies2: array for each bearing containing the anomaly2 scores\n    :param cols: number of columns you want to plot should be equivalent with number of bearings\n    :param seperate_legend: if you want a seperate legend outside of the plot set this to True\n    :return fig: figure containing all the subplots if seperate_legend figure containing only the legend is also returned \n    '''\n    print(\"Plotting\")\n    rows = y_true.shape[1] \/\/ cols \n\n    fig = pylab.figure(figsize=(cols*4,rows*3))\n    if seperate_legend:\n        figlegend = pylab.figure(figsize=(3,2))\n\n    axs = np.empty((rows,cols), dtype=object)\n    axs2 = np.empty((rows,cols), dtype=object)\n\n\n    y_position_of_title=0.85\n    labels=[]\n    lines=[]\n    ano1 = True\n    for k in range(y_true.shape[-1]):\n        i = k%cols\n        j = k\/\/cols\n        axs[j,i] = fig.add_subplot(rows, cols, k+1 , sharey = axs[j,0], )\n        axs[j,i] .tick_params(axis='y', labelcolor=\"tab:blue\")\n\n        lines.append(axs[j,i].plot(y_true[:,k])[0])\n        labels.append(\"True_values\" if j == 0 and i ==0 else \"_True_values\")\n\n        if y_pred.size!=0:\n            lines.append(axs[j,i].plot(y_pred[:,k])[0])\n            labels.append(\"Predicted_values\" if j == 0 and i ==0 else \"_Predicted_values\")\n\n        if anomalies1.size!=0:\n            w = 1.5\n            for xc in np.arange(anomalies1.shape[0])[anomalies1[:,k]]:\n                lines.append(axs[j,i].axvspan(xc-w, xc+w,  facecolor=\"red\", ymax=1, alpha = 0.4))\n                labels.append(\"Anomaly level 1 alarm\" if ano1 else \"_Anomaly1\")\n                ano1=False\n        if anomalies2.size!=0:\n            axs2[j,i] = axs[j,i] .twinx()  # instantiate a second axes that shares the same x-axis\n            axs2[j,i].get_shared_y_axes().join(axs2[j,i], axs2[j,0])\n            color = 'black'\n            lines.append(axs2[j,i].plot((anomalies2[:,k%cols]), color = color)[0])\n            axs2[j,i].tick_params(axis='y', labelcolor=color)\n            labels.append(\"Anomaly level 2 score\" if j==0 and i ==0 else \"_Anomaly2\")\n        if j == 0:\n            if i in data_about_tests[\"broken\"]:\n                axs[j,i].set_title(\"Bearing \"+ str(i)+\"\\nBreaks in the end\\n\\n Maximum Values\", y = y_position_of_title)\n            else:\n                axs[j,i].set_title(\"Bearing \"+ str(i)+\"\\n\\n\\n Maximum Values\", y = y_position_of_title)\n        elif j == 1:\n            axs[j,i].set_title(\"Standard Deviation\", y = y_position_of_title)\n        else:\n            axs[j,i].set_title(str(bins[j-2])+\"Hz-\"+str(bins[j-2+1])+\"Hz\", y = y_position_of_title)\n    if seperate_legend:\n        figlegend.legend(lines,labels,   \"center\" )\n        return fig, figlegend\n    else:\n        fig.legend(lines, labels,  bbox_to_anchor = (0.8, 0.96))\n        return fig\n","cc4afb7d":"def evaluate(model, X, y_true, test_size, test_number,slice_to_plot=np.s_[:], anomaly_1_factor = 5, window_size=30, \n            show_y_pred=True, show_anomaly1 = True, show_anomaly2=True, cols=4):\n    '''\n    calculates the error between predicted and true values. Then calculates a boundary how much \n    the error may differ from the true value. If the error exceeds that boundary a level one anomaly alarm is stored. \n    Then calculates a level two anomly score with the percentage of level one alarms in last 30 timesteps.\n    :param model: machine learning model used for prediction\n    :param X: X_values that get fed into the model for prediction\n    :param y_true: true labels for the data in X\n    :param test_size: the size of the test set, important because the the boundary is only calculated on the train_set\n    :param test_number: which test-run the data is from. Can only be 0,1,2\n    :param slice_to_plot: if you only want to plot a certain slice of the plots. e.g. if you want to plot only the last\n                        1000 values set this to [-1000:] or if you only want to plot bearing 0 set this to [:,[0,4,8,12,16,20,24]]\n                        and also set cols to 1\n    :param anomaly_1_factor: by how the standard deviation is multiplied to calculate the boundary\n    :param window_size: size of the window over which the level two anomaly score is calculated\n    :param show_y_pred: wether to show y_pred in the plots\n    :param show_anomaly1: wether to show level one anomalies in the plots\n    :param show_anomaly2: wether to show the level anomaly score in the plots\n    :param cols: how many columns you want to plot in, should be number of bearings you want to plot\n    :return fig: figure containing the subplots \n    '''\n    global data_about_tests\n    train_size = int(X.shape[0]*(1-test_size))\n    y_pred = model.predict(X, batch_size=10000)\n    error = y_true-y_pred\n    boundary = error[:train_size].mean(axis=0) + anomaly_1_factor*error[:train_size].std(axis=0)\n    anomalies = error**2 > boundary\n    anomalies2,_ = generate_sequences_pad_front(anomalies[slice_to_plot],window_size) #Always look at anomalies in window\n    anomalies2 = anomalies2.reshape((anomalies2.shape[0],window_size, anomalies2.shape[-1]\/\/cols,cols))\n\n    anomalies2 = anomalies2.mean(axis=1)\n    anomalies2 = anomalies2.mean(axis=1)\n    print(\"2nd level alarm over 0.5:\")\n    [print(np.where(anomalies2[:,i]>0.5)[0][:10]) for i in range(cols)]\n    fig = plot(y_true[slice_to_plot], bins, data_about_tests[test_number], y_pred[slice_to_plot] if show_y_pred else np.array([]), \n        anomalies1 =  anomalies[slice_to_plot] if show_anomaly1 else np.array([]) , anomalies2 = anomalies2[:] if show_anomaly2 else np.array([]), cols=cols)\n    fig.suptitle(data_about_tests[test_number][\"name\"]+\"_test\\nstd_factor: \"+str(anomaly_1_factor)+\"\\nwindow_size:\"+str(window_size), fontsize=20)\n    return fig","887ba389":"\ntest=0\nX = np.concatenate((X_train_series_sub[test],X_test_series_sub[test]))\ny_true = np.concatenate((y_train[test],y_test[test]))\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fig = evaluate(cnn_lstm, X, y_true, test_size ,test, slice_to_plot=np.s_[:],anomaly_1_factor=3, window_size=30,\n                show_y_pred=True, anomaly1 = True, anomaly2 = True)","58324b40":"test=1\nX = np.concatenate((X_train_series_sub[test],X_test_series_sub[test]))\ny_true = np.concatenate((y_train[test],y_test[test]))\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fig = evaluate(cnn_lstm, X, y_true, test_size ,test, slice_to_plot=np.s_[:],anomaly_1_factor=3, window_size=30,\n                    show_y_pred=True, anomaly1 = True, anomaly2 = True)","857543c8":"test=2\nX = np.concatenate((X_train_series_sub[test],X_test_series_sub[test]))\ny_true = np.concatenate((y_train[test],y_test[test]))\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fig = evaluate(cnn_lstm, X, y_true, test_size ,test, slice_to_plot=np.s_[:],anomaly_1_factor=5, window_size=30,\n                    show_y_pred=True, anomaly1 = True, anomaly2 = True)\n","a5bfa3a6":"# Prepare Data","4efe1e23":"# Loading","1cdb13dd":"# Conv LSTM","e11ccaac":"## loading from csv\nThe raw data is loaded and stored in a seperate pickle to be loaded quicker in future runs.","4ac00f7f":"# Imports","427d9504":"# Feature Engineering","079c410f":"## Idea of the Algorithm:\nThe algorithm used for anomaly detection is rather simple. \n1.\tTake raw data and engineer some features (Max Value, Standard Deviation, Frequency Bands, etc.)\n2.\tTrain a machine learning model to predict these features for the next time step.\n3.\tCalculate the error between the true and the predicted value\n4.\tIf the squared error is bigger than a certain factor times the standard deviation of the error on the training set fire a first level alarm\n5.\tMeasure the Percentage of first level alarms fired in a certain time window. If that percentage is above a certain threshold fire a second level alarm\n6.\tIf a second level alarm is fired the machine is broken or likely to break soon so maintenance is required\n","e0cb257b":"# Evaluation"}}