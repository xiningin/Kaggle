{"cell_type":{"f1885512":"code","6e287cb0":"code","6c9a0888":"code","c27ba52d":"code","5a02cf59":"code","7a3554aa":"code","251f8db1":"code","30f39c97":"code","71624f9d":"code","bf60b910":"code","43da10de":"code","8ce6c97b":"code","0890e2e3":"code","e2d892d6":"code","db4e577d":"code","b8cd74df":"code","29887aa4":"code","fc48ea3c":"code","937ef9c1":"code","bdd59677":"code","f7d04b49":"code","ea657d21":"code","6be71aa3":"code","a1fd5840":"code","a68a41d7":"code","0f931dba":"code","b43e5520":"code","5dab4c4a":"code","283cd92d":"code","939b7b3f":"code","24988465":"code","77af442f":"code","6989cb40":"code","efa54c63":"code","c8ba08dd":"code","be7d41e7":"code","b42c2c23":"markdown","47bd8161":"markdown","1429cafe":"markdown","bfc56a3c":"markdown","b6105dbf":"markdown","23a0d98a":"markdown","84761b95":"markdown","3b4bae42":"markdown","813d0ea1":"markdown","584fdfc3":"markdown","10814fd9":"markdown","da3303ad":"markdown","f011f256":"markdown","8691c9b4":"markdown","18588df1":"markdown","b2b1bdb0":"markdown","55890620":"markdown","e713883f":"markdown","e21b82b0":"markdown","6eedafdf":"markdown","54f0f641":"markdown","38bb6e32":"markdown","76c17202":"markdown","02c720e2":"markdown","d0204946":"markdown","a09c7e46":"markdown","9fdce2f5":"markdown","d69d0e51":"markdown","38f7ab56":"markdown","72d91e61":"markdown","e51b9a3d":"markdown","b045f789":"markdown","93fda446":"markdown","0b3ed03a":"markdown","276ee4bb":"markdown","7959880f":"markdown","c296a3ca":"markdown","fc3b36c5":"markdown","c5e8922b":"markdown","bcb2a4c5":"markdown"},"source":{"f1885512":"# Import libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n#from sklearn_pandas import DataFrameMapper # Notice that this is actually Sklearn-Pandas library\n%matplotlib inline","6e287cb0":"# Load dataset\ndata = pd.read_csv('..\/input\/gender-classifier-DFE-791531.csv', encoding='latin-1')\n\n# Drop unnecessary columns\/features\ndata.drop (columns = ['_unit_id',\n                      '_last_judgment_at',\n                      'user_timezone',\n                      'tweet_coord',\n                      'tweet_count',\n                      'tweet_created', \n                      'tweet_id',\n                      'tweet_location',\n                      'profileimage',\n                      'created'], inplace = True)\n\ndata.info()","6c9a0888":"data.head(3)","c27ba52d":"data['gender'].value_counts()\n# We can see that there are 1117 unknown genders, so get rid of them","5a02cf59":"drop_items_idx = data[data['gender'] == 'unknown'].index\n\ndata.drop (index = drop_items_idx, inplace = True)\n\ndata['gender'].value_counts()","7a3554aa":"print ('profile_yn information:\\n',data['profile_yn'].value_counts())\n\ndata[data['profile_yn'] == 'no']['gender']","251f8db1":"drop_items_idx = data[data['profile_yn'] == 'no'].index\n\ndata.drop (index = drop_items_idx, inplace = True)\n\nprint (data['profile_yn'].value_counts())\n\ndata.drop (columns = ['profile_yn','profile_yn:confidence','profile_yn_gold'], inplace = True)","30f39c97":"# Double check the data \nprint (data['gender'].value_counts())\n\nprint ('---------------------------')\ndata.info()","71624f9d":"print ('Full data items: ', data.shape)\nprint ('Data with label-confidence < 100%: ', data[data['gender:confidence'] < 1].shape)","bf60b910":"drop_items_idx = data[data['gender:confidence'] < 1].index\n\ndata.drop (index = drop_items_idx, inplace = True)\n\nprint (data['gender:confidence'].value_counts())\n\ndata.drop (columns = ['gender:confidence'], inplace = True)","43da10de":"data.drop (columns = ['_golden','_unit_state','_trusted_judgments','gender_gold'], inplace = True)\n\n# Double check the data \nprint (data['gender'].value_counts())\n\nprint ('---------------------------')\ndata.info()","8ce6c97b":"from collections import Counter\n\ntwit_vocab = Counter()\nfor twit in data['text']:\n    for word in twit.split(' '):\n        twit_vocab[word] += 1\n        \n# desc_vocab = Counter()\n# for twit in data['description']:\n#     for word in twit.split(' '):\n#         desc_vocab[word] += 1\n        \ntwit_vocab.most_common(20)\n# desc_vocab.most_common(20)","0890e2e3":"import nltk\n\nnltk.download('stopwords')","e2d892d6":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ntwit_vocab_reduced = Counter()\nfor w, c in twit_vocab.items():\n    if not w in stop:\n        twit_vocab_reduced[w]=c\n\ntwit_vocab_reduced.most_common(20)","db4e577d":"import re\n\ndef preprocessor(text):\n    \"\"\" Return a cleaned version of text\n    \"\"\"\n    # Remove HTML markup\n    text = re.sub('<[^>]*>', '', text)\n    # Save emoticons for later appending\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    # Remove any non-word character and append the emoticons,\n    # removing the nose character for standarization. Convert to lower case\n    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n    \n    return text\n\nprint(preprocessor('This!!@ twit :) is <b>nice<\/b>'))","b8cd74df":"from nltk.stem import PorterStemmer\n\nporter = PorterStemmer()\n\ndef tokenizer(text):\n    return text.split()\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]\n\nprint(tokenizer('Hi there, I am loving this, like with a lot of love'))\nprint(tokenizer_porter('Hi there, I am loving this, like with a lot of love'))","29887aa4":"sns.countplot(data['gender'],label=\"Gender\")","fc48ea3c":"sns.barplot (x = 'gender', y = 'fav_number',data = data)","937ef9c1":"sns.barplot (x = 'gender', y = 'retweet_count',data = data)","bdd59677":"male_top_sidebar_color = data[data['gender'] == 'male']['sidebar_color'].value_counts().head(7)\nmale_top_sidebar_color_idx = male_top_sidebar_color.index\nmale_top_color = male_top_sidebar_color_idx.values\n\nmale_top_color[2] = '000000'\nprint (male_top_color)\nl = lambda x: '#'+x\n\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \"#F5ABB5\"})\nsns.barplot (x = male_top_sidebar_color, y = male_top_color, palette=list(map(l, male_top_color)))","f7d04b49":"female_top_sidebar_color = data[data['gender'] == 'female']['sidebar_color'].value_counts().head(7)\nfemale_top_sidebar_color_idx = female_top_sidebar_color.index\nfemale_top_color = female_top_sidebar_color_idx.values\n\nfemale_top_color[2] = '000000'\nprint (female_top_color)\n\nl = lambda x: '#'+x\n\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \"#F5ABB5\"})\nsns.barplot (x = female_top_sidebar_color, y = female_top_color, palette=list(map(l, female_top_color)))","ea657d21":"male_top_link_color = data[data['gender'] == 'male']['link_color'].value_counts().head(7)\nmale_top_link_color_idx = male_top_link_color.index\nmale_top_color = male_top_link_color_idx.values\nmale_top_color[1] = '009999'\nmale_top_color[5] = '000000'\nprint(male_top_color)\n\nl = lambda x: '#'+x\n\nsns.set_style(\"whitegrid\", {\"axes.facecolor\": \"white\"})\nsns.barplot (x = male_top_link_color, y = male_top_link_color_idx, palette=list(map(l, male_top_color)))","6be71aa3":"female_top_link_color = data[data['gender'] == 'female']['link_color'].value_counts().head(7)\nfemale_top_link_color_idx = female_top_link_color.index\nfemale_top_color = female_top_link_color_idx.values\n\nl = lambda x: '#'+x\n\nsns.set_style(\"whitegrid\", {\"axes.facecolor\": \"white\"})\nsns.barplot (x = female_top_link_color, y = female_top_link_color_idx, palette=list(map(l, female_top_color)))","a1fd5840":"# Firstly, convert categorical labels into numerical ones\n# Function for encoding categories\nfrom sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ny = encoder.fit_transform(data['gender'])\n\n\n# split the dataset in train and test\nX = data['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n#In the code line above, stratify will create a train set with the same class balance than the original set\n\nX_train.head()","a68a41d7":"from sklearn.linear_model import LogisticRegression\n\ntfidf = TfidfVectorizer(lowercase=False,\n                        tokenizer=tokenizer_porter,\n                        preprocessor=preprocessor)\nclf = Pipeline([('vect', tfidf),\n                ('clf', LogisticRegression(multi_class='ovr', random_state=0))])\n\nclf.fit(X_train, y_train)\n\npredictions = clf.predict(X_test)\nprint('Accuracy:',accuracy_score(y_test,predictions))\nprint('Confusion matrix:\\n',confusion_matrix(y_test,predictions))\nprint('Classification report:\\n',classification_report(y_test,predictions))","0f931dba":"from sklearn.ensemble import RandomForestClassifier\n# Plot the correlation between n_estimators and accuracy\n\n# X_train_sample = X_train.head(5000) # this is series\n# y_train_sample = y_train[:5000] # this is array\n\n# print (X_train_sample.shape)\n# print (y_train_sample.shape)\n\nn = range (1,100,10) #step 10\n\nresults = []\nfor i in n:\n    clf = Pipeline([('vect', tfidf),\n                ('clf', RandomForestClassifier(n_estimators = i, random_state=0))])\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n    results.append(accuracy_score(y_test, predictions))\nplt.grid()\nplt.scatter(n, results)","b43e5520":"tfidf = TfidfVectorizer(lowercase=False,\n                        tokenizer=tokenizer_porter,\n                        preprocessor=preprocessor)\nclf = Pipeline([('vect', tfidf),\n                ('clf', RandomForestClassifier(n_estimators = 40, random_state=0))])\n\nclf.fit(X_train, y_train)\n\npredictions = clf.predict(X_test)\nprint('Accuracy:',accuracy_score(y_test,predictions))\nprint('Confusion matrix:\\n',confusion_matrix(y_test,predictions))\nprint('Classification report:\\n',classification_report(y_test,predictions))","5dab4c4a":"# the SVM model\nfrom sklearn.svm import SVC\n\ntfidf = TfidfVectorizer(lowercase=False,\n                        tokenizer=tokenizer_porter,\n                        preprocessor=preprocessor)\nclf = Pipeline([('vect', tfidf),\n                ('clf', SVC(kernel = 'linear'))])\nclf.fit(X_train, y_train)\n\npredictions = clf.predict(X_test)\nprint('Accuracy:',accuracy_score(y_test,predictions))\nprint('Confusion matrix:\\n',confusion_matrix(y_test,predictions))\nprint('Classification report:\\n',classification_report(y_test,predictions))","283cd92d":"data.head(3)","939b7b3f":"#Fill NaN with empty string\ndata.fillna(\"\", inplace = True)\n\n# Concatenate text with description, add white space between. \n# By using Series helper functions Series.str()\ndata['text_description'] = data['text'].str.cat(data['description'], sep=' ')\n\ndata['text_description'].isnull().value_counts() # Check if any null values, True if there is at least one.","24988465":"# split the dataset in train and test\nX = data['text_description']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n#In the code line above, stratify will create a train set with the same class balance than the original set\n\nX_train.head()\nX_train.isnull().values.any() # Check if any null values, True if there is at least one.","77af442f":"from sklearn.linear_model import LogisticRegression\n\ntfidf = TfidfVectorizer(lowercase=False,\n                        tokenizer=tokenizer_porter,\n                        preprocessor=preprocessor)\nclf = Pipeline([('vect', tfidf),\n                ('clf', LogisticRegression(multi_class='ovr', random_state=0))])\n\nclf.fit(X_train, y_train)\n\npredictions = clf.predict(X_test)\nprint('Accuracy:',accuracy_score(y_test,predictions))\nprint('Confusion matrix:\\n',confusion_matrix(y_test,predictions))\nprint('Classification report:\\n',classification_report(y_test,predictions))","6989cb40":"# Plot the correlation between n_estimators and accuracy\n\n# X_train_sample = X_train.head(5000) # this is series\n# y_train_sample = y_train[:5000] # this is array\n\n# print (X_train_sample.shape)\n# print (y_train_sample.shape)\n\nn = range (1,120,10) #step 10\n\nresults = []\nfor i in n:\n    clf = Pipeline([('vect', tfidf),\n                ('clf', RandomForestClassifier(n_estimators = i, random_state=0))])\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n    results.append(accuracy_score(y_test, predictions))\nplt.grid()    \nplt.scatter(n, results)","efa54c63":"from sklearn.ensemble import RandomForestClassifier\n\ntfidf = TfidfVectorizer(lowercase=False,\n                        tokenizer=tokenizer_porter,\n                        preprocessor=preprocessor)\nclf = Pipeline([('vect', tfidf),\n                ('clf', RandomForestClassifier(n_estimators = 80, random_state=0))])\n\nclf.fit(X_train, y_train)\n\npredictions = clf.predict(X_test)\nprint('Accuracy:',accuracy_score(y_test,predictions))\nprint('Confusion matrix:\\n',confusion_matrix(y_test,predictions))\nprint('Classification report:\\n',classification_report(y_test,predictions))","c8ba08dd":"# the SVM model\nfrom sklearn.svm import SVC\n\ntfidf = TfidfVectorizer(lowercase=False,\n                        tokenizer=tokenizer_porter,\n                        preprocessor=preprocessor)\nclf = Pipeline([('vect', tfidf),\n                ('clf', SVC(kernel = 'linear'))])\nclf.fit(X_train, y_train)\n\npredictions = clf.predict(X_test)\nprint('Accuracy:',accuracy_score(y_test,predictions))\nprint('Confusion matrix:\\n',confusion_matrix(y_test,predictions))\nprint('Classification report:\\n',classification_report(y_test,predictions))","be7d41e7":"from sklearn.ensemble import VotingClassifier\nclf1 = LogisticRegression(multi_class='ovr', random_state=0)\nclf2 = RandomForestClassifier(n_estimators = 80, random_state=0)\nclf3 = SVC(kernel = 'linear',probability = True, random_state=0)\n\nensemble_clf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('svm', clf3)], voting='soft')\n\nclf = Pipeline([('vect', tfidf),\n                ('clf', ensemble_clf)])\n\nclf.fit(X_train, y_train)\n\n# ensemble_clf.fit(X_train, y_train)\n\npredictions = clf.predict(X_test)\nprint('Accuracy:',accuracy_score(y_test,predictions))\nprint('Confusion matrix:\\n',confusion_matrix(y_test,predictions))\nprint('Classification report:\\n',classification_report(y_test,predictions))","b42c2c23":"## Training classification models with Tweet-text only","47bd8161":"### Try with Random Forest","1429cafe":"### Try with Random Forest","bfc56a3c":"### Try with Logistic Regression Model","b6105dbf":"### Low-confidence gender (gender:confidence)\n\nI decide to keep only 100% confidence of labeling Gender and get rid of those < 100% confidence.","23a0d98a":"## Cleaning Dataset","84761b95":"## Future works","3b4bae42":"# Predict user gender based on Twitter profile information with text-data\n\n### Motivation:\n- This problem is interesting, oddly interesting...\n- Problem context is easily understandable\n- There's already a popular classification problem based on Twitter text: Sentiment classification","813d0ea1":"It is shown that with approximately **40** trees, Random Forest classifier starts reaching the highest performance.","584fdfc3":"### Removing stop-words in Twits\n\nFirst we need to take a glance at the most common words","10814fd9":"### How relevant are words? Term frequency-inverse document frequency\n\nWe could use these raw term frequencies to score the words in our algorithm. There is a problem though: If a word is very frequent in _all_ documents, then it probably doesn't carry a lot of information. In order to tacke this problem we can use **term frequency-inverse document frequency**, which will reduce the score the more frequent the word is accross all twits. It is calculated like this:\n\n\\begin{equation*}\ntf-idf(t,d) = tf(t,d) ~ idf(t,d)\n\\end{equation*}\n\n_tf(t,d)_ is the raw term frequency descrived above. _idf(t,d)_ is the inverse document frequency, than can be calculated as follows:\n\n\\begin{equation*}\n\\log \\frac{n_d}{1+df\\left(d,t\\right)}\n\\end{equation*}\n\nwhere `n` is the total number of documents (number of _twits_ in this problem) and _df(t,d)_ is the number of documents where the term `t` appears. \n\nThe `1` addition in the denominator is just to avoid zero term for terms that appear in all documents. Ans the `log` ensures that low frequency term don't get too much weight.\n\nThe IDF (inverse document frequency) of a word is the measure of how significant that term is in the whole corpus (the whole collection of _twits_ in this problem).\n\nThe higher the TF-IDF weight value, the rarer the term. The smaller the weight, the more common the term.\n\nFortunately for us `scikit-learn` does all those calculations for us:","da3303ad":"As you can see, the most common words are meaningless in terms of sentiment: _I, to, the, and..._ . They're basically noise that can most probably be eliminated. These kind of words are called **stop words**, and it is a common practice to remove them when doing text analysis.","f011f256":"## About Dataset\n\nThis dataset is obtained from [Kaggle](https:\/\/www.kaggle.com\/crowdflower\/twitter-user-gender-classification\/home).\n\nThe dataset contains 20,000 rows, each with a user name, a random tweet, account profile and image, location, and even link and sidebar color.\n\nAttributes that do not provide useful information for _Gender classification_:\n - **_unit_id**: a unique id for user\n - **_last_judgment_at**: date and time of last contributor judgment; blank for gold standard observations\n - **user_timezone**: the timezone of the user\n - **tweet_coord**: if the user has location turned on, the coordinates as a string with the format \"[latitude, longitude]\"\n - **tweet_count**: number of tweets that the user has posted\n - **tweet_created**: when the random tweet (in the text column) was created\n - **tweet_id**: the tweet id of the random tweet\n - **tweet_location**: location of the tweet; seems to not be particularly normalized \n - **profileimage**: a link to the profile image\n - **created**: date and time when the profile was created\n \n \nAttributes that potentially provide useful information for _Gender classification_:\n - **_golden**: whether the user was included in the gold standard for the model; TRUE or FALSE\n - **_unit_state**: state of the observation; one of finalized (for contributor-judged) or golden (for gold standard observations)\n - **_trusted_judgments**: number of trusted judgments (int); always 3 for non-golden, and what may be a unique id for gold standard observations\n - **gender**: one of male, female, or brand (for non-human profiles)\n - **gender:confidence**: a float representing confidence in the provided gender\n - **gender_gold**: if the profile is golden, what is the gender?\n - **profile_yn**: \"no\" here seems to mean that the profile was meant to be part of the dataset but was not available when contributors went to judge it\n - **profile_yn:confidence**: confidence in the existence\/non-existence of the profile\n - **profile_yn_gold**: whether the profile y\/n value is golden\n - **description**: the user's profile description\n - **fav_number**: number of tweets the user has favorited\n - **link_color**: the link color on the profile, as a hex value\n - **name**: the user's name\n - **retweet_count**: number of times the user has retweeted (or possibly, been retweeted)\n - **sidebar_color**: color of the profile sidebar, as a hex value\n - **text**: text of a random one of the user's tweets\n","8691c9b4":"### Removing special characters and \"trash\"\n\nWe still se a very uneaven distribution. If you look closer, you'll see that we're also taking into consideration punctuation signs ('-', ',', etc) and other html tags like `&amp`. We can definitely remove them for the sentiment analysis, but we will try to keep the emoticons, since those _do_ have a sentiment load:","18588df1":"## Adding content of Description into Text ","b2b1bdb0":"# Conclusions\n\nI implemented a system of Gender classification based on the dataset provided on Kaggle.\nThis is actually an interesting problem among with the Sentiment classification problem, which is more popular.\n\nAs I intended to implement classifiers based on Text data, i also wanted to explore whether other features can help the model classify Gender. Therefore, i plotted different graphs to visualize them. The results show that **link_color** may give additional useful information for classification task. \n\nThe results show that Only the **Tweet text** can yield a moderate accuracy, although it's not sustantially high.\nBut with the content from the **Description**, the classifiers actually improve its performance significantly.\n\nI also tried implementing Ensemble learning as one of my idea during the implementation, however, it only slightly increases the accuracy. \n","55890620":"### Create a bar plot to visualize the amount of *favorites* and *retweets*","e713883f":"### Get rid of remaining useless features","e21b82b0":"We are almost ready! There is another trick we can use to reduce our vocabulary and consolidate words. If you think about it, words like: love, loving, etc. _Could_ express the same positivity. If that was the case, we would be  having two words in our vocabulary when we could have only one: lov. This process of reducing a word to its root is called **stemming**. An alternative way is called **Lemmatization**.\n\nA popular stemming algorithm for English is **Porter** algorithm.\n\nWe also need a _tokenizer_ to break down our twits in individual words. We will implement two tokenizers, a regular one and one that does steaming:","6eedafdf":"Here, i can observe that approximately **26.7%** (5032\/18836) of labeled instances were lower 100% of confidence\n\nThen, i get rid of those instances and the feature **gender:confidence** as it is now useful anymore.","54f0f641":"### Experimental Results\n\n**The increase of accuracy is not significant at all - only 0.3% (68.68% - 68.97%)**. But it is worth running experiment since ensemble learning usually yields better results.\n\n--> Maybe i need to research more to select precisely the classifiers for ensemble system in the futures.","38bb6e32":"### Experimental Results\n\nAccuracy:\n - **Logistic Regression**: 68.17%  \n - **Random Forest**: 64.38%  \n - **SVM**: 68.68%  \n \nWinner: **SVM** model","76c17202":"### 'Gender' Attribute (gender)","02c720e2":"For **sidebar color**, the top 3 colors of both male and female are the same (this seems to be these colors are default theme color of Twitter). It is shown that the number of 2nd and 3rd color of female is larger but this can be explained by the fact that the number of female users are more than male.\n\nSo, at this point, sidebar_color may not give me any useful information for classifying gender.","d0204946":"### Concatenating 'description' to 'text'","a09c7e46":"### 'Profile' Attribute (profile_yn, profile_yn:confidence, profile_yn_gold)\n\n**'No'**: Profile was meant to be part of the dataset but was not available when contributors went to judge it.","9fdce2f5":"### Experimental Results\n\nAccuracy:\n - **Logistic Regression**: 59.95%  \n - **Random Forest**: 57.07%  \n - **SVM**: 59.80%  \n \nWinner: **Logistic Regression** model","d69d0e51":"It is shown that with approximately **80** trees, Random Forest classifier starts reaching the highest performance.","38f7ab56":"### Try with SVM","72d91e61":"### Re-create training dataset","e51b9a3d":"### Lemmatization","b045f789":"### Try with SVM","93fda446":"## Try Ensemble technique - Take advantage of both 3 models","0b3ed03a":"### Try with Logistic Regression","276ee4bb":"### Visualize Colors attribute","7959880f":"## Manipulate Text data","c296a3ca":"### Create a countplot to visualize the amount of each label","fc3b36c5":"It is shown that all of 97 instances with **profile_yn** == **no** are all **NaN** in **gender**. \n\nTherefore, i get rid of these 97 instances. Also, i get rid of **profile_yn**, **profile_yn:confidence** and **profile_yn_gold** as they are not useful anymore.","c5e8922b":" - Re-implement ensemble learning system with further research\n - Extract link_color features to add to the models\n - Try applying Deep Learning (optional)","bcb2a4c5":"## Visualize Data\n\nAmong text data, i want to find out if other features can give me useful information or show some special characteristics."}}