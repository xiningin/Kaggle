{"cell_type":{"b32648d1":"code","a77e31ee":"code","5e587b53":"code","bacd2211":"code","74de6367":"code","8859354c":"code","25ee475d":"code","7e601f01":"code","e7f62a7f":"code","50c60d58":"code","efa29614":"code","70f0bfde":"code","cdbc0d28":"code","294f70ae":"code","3ce517a9":"code","43631df8":"code","29049228":"code","5defa96c":"code","2e40360c":"code","375c3e6a":"code","11430b46":"code","ce0e231e":"code","b6cce51c":"code","671b9dcb":"code","d3fec478":"code","5f11439e":"code","cda8441a":"code","818da3d7":"code","4406a9df":"code","8e5646c1":"markdown"},"source":{"b32648d1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport pandas_profiling as pp#data visualization\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import boxcox,skew,norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","a77e31ee":"# will do 76.9% reduction im memory usage\ndef reduce_mem_usage(df, verbose=True):#from santander ml comp\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","5e587b53":"%%time\ntrain= reduce_mem_usage(pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv'))\ntest= reduce_mem_usage(pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv'))\nprint(\"Shape of train set: \",train.shape)\nprint(\"Shape of test set: \",test.shape)\nsample = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')","bacd2211":"train.drop(['id'],axis=1,inplace=True)","74de6367":"train.isna().sum().sum()","8859354c":"test.isna().sum().sum()","25ee475d":"gc.collect()","7e601f01":"#x,y \nx = train.drop(['target'],axis=1)\ny =train.target","e7f62a7f":"#scaling by max \nx = x \/ x.max()\n#scaling test\ntest =test\/test.max()","50c60d58":"del train\ngc.collect()","efa29614":"#Spliting data in to train and test for model training\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=1)\ngc.collect()","70f0bfde":"\n##Import libraries for classification model \nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier\nimport sklearn.metrics as metrics\nfrom sklearn import model_selection","cdbc0d28":"clf = lgb.LGBMClassifier()\nrf_model =RandomForestClassifier(criterion='entropy',random_state=1,max_depth=10,n_estimators=21)\nlr = LogisticRegression(solver='liblinear')\n","294f70ae":"rf_model.fit(x_train,y_train)\ngc.collect()\nclf.fit(x_train,y_train)\ngc.collect()\nlr.fit(x_train,y_train)","3ce517a9":"gc.collect()","43631df8":"##validation\nval_pred_rf = rf_model.predict_proba(x_test)\nval_pred_lgb = clf.predict_proba(x_test)\nval_pred_lr = lr.predict_proba(x_test)\n","29049228":"val_pred_rf = [x[1] for x in val_pred_rf]\nval_pred_lgb = [x[1] for x in val_pred_lgb]\nval_pred_lr = [x[1] for x in val_pred_lr]\ngc.collect()","5defa96c":"#droping id from test\ntest =test.drop(['id'],axis=1)","2e40360c":"##getting predict probs\npredictions_rf = rf_model.predict_proba(test)\npredictions_lgb = clf.predict_proba(test)\npredictions_lr= lr.predict_proba(test)","375c3e6a":"predictions_rf = [x[1] for x in predictions_rf]\npredictions_lgb = [x[1] for x in predictions_lgb]\npredictions_lr = [x[1] for x in predictions_lr]","11430b46":"val_pred_rf = pd.DataFrame(val_pred_rf,columns=['rf'])\nval_pred_lgb = pd.DataFrame(val_pred_lgb,columns=['lgb'])\nval_pred_lr = pd.DataFrame(val_pred_lr,columns=['lr'])\n\npredictions_rf = pd.DataFrame(predictions_rf,columns=['rf'])\npredictions_lgb = pd.DataFrame(predictions_lgb,columns=['lgb'])\npredictions_lr = pd.DataFrame(predictions_lr,columns=['lr'])","ce0e231e":"# Adding predicted values as new feature\nx_test['rf'] = val_pred_rf['rf'].values\nx_test['lgb'] = val_pred_lgb['lgb'].values\nx_test['lr'] = val_pred_lr['lr'].values\n","b6cce51c":"x_test.shape","671b9dcb":"# Adding predicted values as new feature in test data also\ntest['rf'] = predictions_rf['rf'].values\ntest['lgb'] = predictions_lgb['lgb'].values\ntest['lr'] = predictions_lr['lr'].values","d3fec478":"#new  estimation\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(x_test,y_test)\nmodel.score(x_test,y_test)","5f11439e":"hold_out_pred = model.predict_proba(test)\nhold_out_pred = [x[1] for x in hold_out_pred]\nsample['target'] = hold_out_pred","cda8441a":"#submission and see the result for the base M\nsample.to_csv(\"submission.csv\", index=False)","818da3d7":"# Will do the Auto EDA\n# pp.ProfileReport(train)","4406a9df":"#continue","8e5646c1":"### Predict validation data and test data and use the predictions as new feature"}}