{"cell_type":{"8c20e0e0":"code","b4b6279e":"code","3bb1f634":"code","6f69797a":"code","2e557e1e":"code","1b6cf3e5":"code","a2c2c6d9":"code","9448cd56":"code","e1a04e27":"code","16de2e27":"code","0aca5d8a":"code","907f000e":"code","c8524e67":"code","b2f5d2c6":"code","8118ae7a":"code","e6ad520e":"code","d98cdd95":"code","1f6fa97d":"code","f6f6712d":"code","308d1604":"code","38f817ff":"code","c4a233d2":"code","d3f83bf3":"code","75539e2d":"code","d3897726":"markdown","15fb7b30":"markdown","bc7cb9d2":"markdown","4d8e955f":"markdown","64d105da":"markdown","aa404fbd":"markdown","322adde3":"markdown","0ff489b5":"markdown","a86cf359":"markdown","a4f45143":"markdown","b142435a":"markdown","bf489245":"markdown","f30e9e0c":"markdown","6ee2b30d":"markdown","6b2d0fb8":"markdown","c214235a":"markdown","85f91181":"markdown","a5f4fe4f":"markdown","d4642e19":"markdown","6655ab1c":"markdown","acc5057b":"markdown"},"source":{"8c20e0e0":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport math\n\n# Visualisations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Stats and Metrics\nfrom sklearn import metrics\nfrom scipy import stats\n\n# Models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# File\ndf = pd.read_csv('..\/input\/Melbourne_housing_FULL.csv')\ndf.info()","b4b6279e":"# Let's assess variables\ndf.info()","3bb1f634":"# Covert objects to categorical variables\nchange_objects = ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'CouncilArea','Regionname']\nfor colname in change_objects:\n    df[colname] = df[colname].astype('category')  \n    \n# Convert numerical variable to categorical\nchange_numeric = ['Postcode']\nfor colname in change_numeric:\n    df[colname] = df[colname].astype('category')\n    \n# Convert date to object  \ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Check it worked\ndf.info()","6f69797a":"# Compare Rooms and Bedroom2 variables\ndf ['Rooms v Bedroom2'] = df['Rooms'] - df['Bedroom2']\ndf.head(100)","2e557e1e":"# Drop Bedroom2 and Rooms v Bedroom2\ndf = df.drop(['Bedroom2', 'Rooms v Bedroom2'], 1)","1b6cf3e5":"# Check min, max and mean of values to ensure it makes sense\ndf.describe().transpose()","a2c2c6d9":"# Remove false BuildingArea\ndf = df[df['BuildingArea']!=0]\n\n# Remove false YearBuilt (Melbourne Founded 1835)\ndf = df[df['YearBuilt']> 1835]","9448cd56":"# Display total number of null values\ndf.isnull().sum()","e1a04e27":"# Showed that dropping rows is better\ndf.dropna(inplace = True)\n\n# Uncomment below to initiate either mean or median imputation\n# not_null = ['Price', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Lattitude', 'Longtitude']\n\n# Uncomment for Null Values to Mean (ensure dropping rows above and median below are both commented)\n#for colname in not_null:\n    #df[colname].fillna(df[colname].mean(), inplace = True)\n\n# Uncomment for Null Values to Median (ensure dropping rows and mean above are both commented)\n#for colname in not_null:\n    #df[colname].fillna(df[colname].median(), inplace = True)","16de2e27":"# Build Histogram to visualise price distribution\nnum_bins = 50\nn, bins, patches = plt.hist(df.Price, num_bins, color='b', alpha=0.5, histtype = 'bar', ec = 'black')\nplt.ylabel ('Frequency')\nplt.xlabel ('Price ($)')\nplt.xlim([0, 6000000])\nplt.title ('Histogram House Prices')\nplt.show()","0aca5d8a":"# Determine Numerical Values\ndf.select_dtypes(['float64', 'int64']).columns\n\n# Pairplot variables to visualise inter-variable relationships\npair_plot = sns.pairplot(df[['Rooms', 'Price', 'Distance', 'Bathroom', 'Car', 'Landsize','BuildingArea', 'Propertycount', 'YearBuilt', 'Type']], hue = 'Type')","907f000e":"# Build Heatmap to visualise correlations\nfig, ax = plt.subplots(figsize=(15,15)) \nheat_map = sns.heatmap(df[df[\"Type\"] == \"h\"].corr(), cmap = 'jet', annot=True)\n","c8524e67":"# Check in on dataframe\ndf.info()","b2f5d2c6":"# Create features (x) and target (y)\nX = df[['Rooms', 'Distance', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']]\ny = df['Price']\n\n# Apply MinMaxScaler\nscaler = MinMaxScaler()\nx = scaler.fit_transform(X)","8118ae7a":"# Uncomment to use PCA\n#pca = PCA()\n#pca.fit(x)\n#x = pca.transform(x)","e6ad520e":"# Split the training data and test data\nx_train, x_test, y_train, y_test = train_test_split(x , y , test_size = 0.2 , random_state = 0)","d98cdd95":"# Initiate max R^2 score\nmax_r2 = 0\n\n# Create Gradient Boosting Regression model that iterates through learning rates \nfor i in np.linspace(0.1, 1, 50):\n    \n    # Initiate model for learning rate i\n    gbr = GradientBoostingRegressor(learning_rate = i)\n    gbr.fit(x_train, y_train)\n    \n    # Make prediction\n    y_pred = gbr.predict(x_test)\n    \n    # Return values for corresponding learning rate\n    print ('For learning rate i: %0.2f' %i)\n    print('Gradient Boosting Regression MAE: %0.5f'%metrics.mean_absolute_error(y_test,y_pred))\n    print('Gradient Boosting MSE:%0.5f'%metrics.mean_squared_error(y_test,y_pred))\n    print('Gradient Boosting RMSE:%0.5f'%np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n    print('Gradient Boosting R^2: %0.5f' %metrics.explained_variance_score(y_test,y_pred))\n    print ('---------------------------------')\n\n    # If R^2 new maximum score, save score and the learning rate\n    if metrics.explained_variance_score(y_test,y_pred) > max_r2:\n        max_r2 = metrics.explained_variance_score(y_test,y_pred)\n        max_i = i\n        y_pred_gbr = y_pred\n        \n        # Store Standard Error\n        se_gbr = stats.sem(y_pred_gbr)\n\n# Print maximum R^2 score and corresponding learning rate\nprint ('Max R^2 is: %0.5f' %max_r2, 'with learning rate: %0.2f' %max_i)\n","1f6fa97d":"# Plot residual Plot of the GBR\nplt.scatter(y_test, y_pred_gbr, c = 'blue')\nplt.ylim([200000, 1000000])\nplt.xlim([200000, 1000000])\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices:\")\nplt.title(\"GBR Residual Plot\")","f6f6712d":"# Initialise Linear Regression model\nlr = LinearRegression()\nlr.fit(x_train, y_train)\n\n# Make Prediction\ny_pred_lr = lr.predict(x_test)\n\n# Return Results\nprint('Linear Regression MAE: %0.5f'%metrics.mean_absolute_error(y_test,y_pred))\nprint('Linear Regression MSE:%0.5f'%metrics.mean_squared_error(y_test,y_pred))\nprint('Linear Regression RMSE:%0.5f'%np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\nprint('Linear Regression R^2: %0.5f' %metrics.explained_variance_score(y_test,y_pred))\n\n# Store Standard Error\nse_lr = stats.sem(y_pred_lr)","308d1604":"# Plot residual Plot of the LR\nplt.scatter(y_test, y_pred_lr, c = 'black')\nplt.ylim([200000, 1000000])\nplt.xlim([200000, 1000000])\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices:\")\nplt.title(\"LR Residual Plot\")","38f817ff":"# Initialise Lasso Regression model\nlcv = LassoCV()\nlcv.fit(x_train, y_train)\n\n# Make Prediction\ny_pred_lcv = lcv.predict(x_test)\n\n# Return Results\nprint('Lasso Regression MAE: %0.5f'%metrics.mean_absolute_error(y_test,y_pred))\nprint('Lasso Regression MSE:%0.5f'%metrics.mean_squared_error(y_test,y_pred))\nprint('Lasso Regression RMSE:%0.5f'%np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\nprint('Lasso Regression R^2: %0.5f' %metrics.explained_variance_score(y_test,y_pred))\n\nse_lcv = stats.sem(y_pred_lcv)","c4a233d2":"# Plot residual Plot of the LCV\nplt.scatter(y_test, y_pred_lcv, c = 'yellow')\nplt.ylim([200000, 1000000])\nplt.xlim([200000, 1000000])\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices:\")\nplt.title(\"LCV Residual Plot\")","d3f83bf3":"# Initialise Max R^2 variable \nmax_r2 = 0\n\n# Create Random Forest Model that iterates between 64 --> 128 trees\nfor n_trees in range (64, 129):\n    \n    # Initiate model for value n_tree\n    rfr = RandomForestRegressor(n_estimators=n_trees, n_jobs=-1) \n    rfr.fit(x_train, y_train)\n    \n    # Make prediction for n_tree sized model\n    y_pred = rfr.predict(x_test)\n    \n    # Store Standard Error\n    rfr_sem = stats.sem (y_pred)\n    \n    # Print Results\n    print('For a Random Forest with', n_trees, 'trees in total:')\n    print('MAE: %0.5f'%metrics.mean_absolute_error(y_test,y_pred))\n    print('MSE:%0.5f'%metrics.mean_squared_error(y_test,y_pred))\n    print('RMSE:%0.5f'%np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n    print('R^2: %0.5f' %metrics.explained_variance_score(y_test,y_pred))\n    print('--------------------------------------')\n    \n    # If new R^2 the max, store it for reference\n    if metrics.explained_variance_score(y_test,y_pred) > max_r2:\n        max_r2 = metrics.explained_variance_score(y_test,y_pred)\n        max_n_trees = n_trees\n        max_rfr_sem = rfr_sem\n        y_pred_rfr= y_pred\n        \n        # Store Standard Error\n        se_rfr = stats.sem(y_pred_rfr)\n\n# Return max R^2 and corresponding amount of trees in forest\nprint ('Max R^2 is: %0.5f' %max_r2, 'at', max_n_trees, 'trees')","75539e2d":"# Plot residual Plot of the LR\nplt.scatter(y_test, y_pred_rfr, c = 'green')\nplt.ylim([200000, 1000000])\nplt.xlim([200000, 1000000])\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices:\")\nplt.title(\"RFR Residual Plot\")","d3897726":"### 1.3. Removing Duplicates\nBedroom2 and Rooms were duplicates. This has been illustrated below with Bedroom2 removed from the dataframe","15fb7b30":"### 2.2. Seaborn Pairplot","bc7cb9d2":"### 3.1. MinMax Scaler\n\nMin-max scaling, which is often referred to as normalisation, rescales the range of features to range in [0, 1] As each variable contained different units, from meters-squared (BuildingArea) to the target Price (often in the millions of dollars), it was imperative to feature scale the dataset to ensure variables weren\u2019t weighted unfairly. The smaller range also enables any models utilising gradient descent to converge faster.","4d8e955f":"## 5. Benchmarks","64d105da":"#### 1.5.2. Handling the NaN values ","aa404fbd":"### 3.3. Train Test Split\n\nIn this regression model, the housing dataset was split into 80% train and 20% test, a common starting point for most regression models.\n","322adde3":"### 1.4. Removing Outliers","0ff489b5":"## 3. Additional pre-processing for computation","a86cf359":"### 1.2. Initial Data Exploration","a4f45143":"## 4. Gradient Boosting Regressor\nThe model selected for this property price predictor is a gradient boosting regressor (GBR), a type of \u201cinductively generated tree ensemble model\u201d. Considered to be incredibly effective in practice, a gradient boosting regressor is to a linear regressor what a UH-60 Blackhawk helicopter is to a Toyota Camry in the data science community. With GBRs like XGBoost often used to win various competitions on Kaggle, it was believed that this model would be the best suited for this problem.\n\nDespite their success, there is an inherent risk with GBR. While the gradient boosted decision trees learn quick, they can overfit the data. Also referred to as \u2018shrinkage\u2019, the correct learning rate is impera- tive to ensure the model doesn\u2019t take too long or overshoot the best fit. As seen in the attached Jupyter Note- book, the model was built to iteratively run through 50 different learning rates between 0.1 and 1.\n\nIn order to ensure this was the best regression model for the Melbourne housing dataset, it was benchmarked against a Linear regressor, Lasso regressor, and Random Forest Regressor. An overview of these models can be seen below.","b142435a":"## 6. Quick Analysis\n*Please note that the R^2, RMSE etc values may be slightly different depending on when the model was run vs. when this was typed. Overall, it should not change the conclusion*\n\nThe goal of this project was to make a regressor that accurately predicted the price of a house in the Melbourne property market. With a gradient boosting regressor (GBR) believed to be the best model for the dataset, an additional 3 benchmark models were constructed: linear regressor, lasso regressor and random forest regressor.\n\nAs seen, the GBR achieved an R-squared value of 0.84803, a root mean squared error (RMSE) of \\$264,430 and a mean absolute error (MAE) of \\$163554 with a learning rate of \u03b1lpha = 0.3. This means the GBR model explains almost 85% of the variance in the model and the predicted results are incor- rect by an average margin of \\$264,430. \n\nHow does it compare to the benchmarks?\n\nIn comparison to the linear regressor (R2 = 0.61984, RMSE = \\$418,285, MAE = \\$274258) and the lasso regressor (R2 = 0.61912, RMSE = \\$418,679, MAE = \\$274553), the GBR model was clearly the strongest model. However, in comparison to the random forest (RF) model, it isn\u2019t so obvious. The RF performed as predicted: well. When constructed with 102 trees, the model had an R-squared of 0.83309 and RMSE of \\$277,586; the GBR outperformed on these two measures. \n\nHowever, the RF had an MAE of \\$161,197, per- forming better than the GBR on this measure. This MAE score is also evident in the Residual Plot Comparison. While GBR and RF clearly stand out from the other two models built by the author, the former performs better with the R-squared and RMSE measures while the latter outperforms with MAE (and with assessment of the constructed residual plot). \n\nHow do we decide on the best model of the two?\n\nBoth MAE and RMSE are two of the most common metrics used to measure accuracy in continuous variables and regression models. While they both present the prediction in the unit of interest, in this case the price of the house, RMSE differs as it takes the square root of the average squared errors the model while MAE takes the average of the absolute value of the errors. \n\nBecause the errors are squared, RMSE gives a higher weight to larger errors; it is a more useful score where large errors are undesirable. In the property market, a multitude of factors: amount of funds available, additional financing (mortgage) and return on in- vestment to name a few, are detrimentally impacted by larger errors. You may often hear someone increasing their budget from \\$1M to \\$1.05M, however, increasing it to \\$1.5M is far less likely; larger errors need to be heavily penalised in this model. As a result, RMSE is favoured in assessing the regression models in comparison to MAE on this dataset. \n\nThe GBR is the model of choice for this dataset.","bf489245":"## 1. Data Pre-processing\n\n### 1.1.Imports","f30e9e0c":"### 2.3. Heatmap to show correlation","6ee2b30d":"# Melbourne Housing Market \n\n### By Ben Lindsay\n\nThe goal of this project is to construct a regression that, in comparison to benchmarks, most accurately predicts the price of a house in the Melbourne property market to assist home-buyers in making their pur- chasing decision\n\nIn order to accurately predict property values, a well-fitted regression model is required. R2 in conjunction with Mean Absolute Error (MAE), Mean Square Error (MSE), and Root Mean Square Error (RMSE) were used to assess the models accuracy in comparison to various benchmarks.","6b2d0fb8":"### 5.3. Random Forest\n\nRandom Forest is a \u201cflexible, easy to use machine learning algorithm\u201d that performs well, even with- out hyper-parameter tuning. The forest is comprised of decision trees and can be used for both classifica- tion and regression models, however, overfitting is a potential pitfall for the model. In order to determine the total amount of trees in the forest, the random forest regressor built in the notebook iterated between 64 and 128 trees, which according to Oshinro 2019 \u201cis the range of the optimal number of trees for your model and returns the best models number of trees and accuracy\/error scores\u201d.","c214235a":"### 3.2. Principal Component Analysis\nPrincipal component analysis (PCA), is a dimension reduction tool that projects data onto lower di- mensions, commonly referred to as principal components, in order to reduce the total number of variables to smaller data set with negligible information loss. In other words, if a feature is determined to be highly correlated to another, the feature is removed in order to help prevent overfitting of the model. \n\nA heat map was constructed above confirming no notable correlation between the variables. In addition to this, a seaborn pair plot was constructed to help visualise relationships between variables. \n\nWhile the lack of correlation was shown, PCA was tested in this model out of FOMO but it was confirmed to be detrimental to its overall performance. ","85f91181":"### 5.1. Linear Regression\nAs simple and often robust regression model, the (multivariable) Linear Regression model maps out a linear line of best fit by measuring the relationship between the purchase price of the house and the features.","a5f4fe4f":"## 2. Data Visualisation\n\n### 2.1. Histogram of House Prices","d4642e19":"#### 1.5.1. How many null values?","6655ab1c":"### 1.5. Handling NaN Values\nIn order too potentially not skew the results, mean and median imputation were initially utilised to replace the null values; however, when comparing each method across all regression models, dropping all null values resulted in a model that best fit the data. These imputation methods can be utilised in the Jupyter notebook by uncommented the respective lines.","acc5057b":"### 5.2. Lasso Regression\nThe least absolute shrinkage and selection operator (or \u2018Lasso\u2019) model is capable of reducing the vari- ability of the model and improve the overall accuracy. The aim of lasso regression is \u201cto obtain the subset of predictors that minimises prediction error for a quantitative response variable by imposing a constraint on the model parameters that causes regression coefficients for some variables to shrink toward zero\u201d. After the shrinkage, variables with coefficient equal to zero are removed from the model."}}