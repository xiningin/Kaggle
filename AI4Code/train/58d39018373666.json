{"cell_type":{"bb242bcf":"code","b11bdd5e":"code","7bc24333":"code","97340f7d":"code","1661eee9":"code","35fdcefe":"code","ed56eca4":"code","ce2bc11b":"code","eb832b03":"code","2461989a":"code","f94b779f":"code","ac39647c":"code","45cae2ee":"code","0a6fdbe4":"code","8b715b39":"code","3f8ad49d":"code","8c72fec8":"code","fc9ce907":"code","c9d62dca":"code","42ec934b":"code","5b1c98e0":"code","8c56d247":"code","6c059891":"code","65b085aa":"code","a81dd481":"code","57e8201c":"code","fa36b73d":"code","565e12fd":"code","cf3706c0":"code","b6bed2b4":"code","99bfd4f7":"code","444655dd":"code","4b1a4ede":"code","855ffb6f":"markdown","2ebedbff":"markdown","43a86255":"markdown","f08b18a0":"markdown","0db00ea9":"markdown","1df393eb":"markdown","c68c61f4":"markdown","5f318b29":"markdown","9f504332":"markdown","dfea912a":"markdown","28388bba":"markdown","5bd3487a":"markdown","f744568a":"markdown","891c4fe1":"markdown","937d5839":"markdown"},"source":{"bb242bcf":"# installation block\n!pip -q  install transformers > \/dev\/null\n\n#import block\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport string\nimport torch\nimport time\nimport re\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nimport transformers as ppb # pytorch transformers\nfrom transformers import AdamW\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, auc\nimport xgboost as xgb\n\nfrom IPython.display import clear_output\nfrom collections import Counter\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords', quiet=True)\nstop_words = stopwords.words('english')","b11bdd5e":"def clean_text(text):\n    \"\"\"\n    Text preprocessing function.\n    \"\"\"\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","7bc24333":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\n\ndf_train['clean_text'] = df_train['text'].apply(lambda x:clean_text(x))\ndf_train['Length'] = df_train.text.str.len()\ndf_train['Word_count'] = df_train['text'].str.split().map(len)","97340f7d":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport pandas as pd\nimport re\n\nfig = make_subplots(\n    rows=3, cols=1, \n    shared_xaxes=True,\n    vertical_spacing=0.03,\n    specs=[[{\"type\": \"table\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=df_train[\"id\"],\n        y=df_train[\"Length\"],\n        mode=\"lines\",\n        name=\"Number of letters in a tweet\"\n    ),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=df_train[\"id\"],\n        y=df_train[\"Word_count\"],\n        mode=\"lines\",\n        name=\"Number of words in a tweet\"\n    ),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n        \n            values=[\n                'ID' , 'Keyword', 'Location', 'Text', 'Target', 'Clean_text', 'Length', 'Word_count'\n            ],\n            font=dict(size=10),\n            align=\"left\"\n        ),\n        cells=dict(\n            values=[df_train[k].tolist() for k in df_train.columns],\n            align = \"left\")\n    ),\n    row=1, col=1\n)\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"TWEETS DATASET\",\n)\n\nfig.show()","1661eee9":"len_word_dis = len(df_train[df_train.target==1])\nlen_word_nondis = len(df_train[df_train.target==0])\n\ntweets = ['Disaster' , 'Non-Disaster']\ncolors = [ '#330C73', '#EF553B']\n          \nfig = go.Figure([go.Bar(x=tweets, y=[len_word_dis, len_word_nondis], marker_color=colors  )])\nfig.update_layout(title_text='Compared count Disaster and Non-Disaster tweets')\nfig.show()","35fdcefe":"fig = make_subplots(\n    rows=2, cols=2,\n    specs=[\n           [{}, {\"rowspan\": 2}],\n           [{}, None]\n    ],\n    subplot_titles=(\"Real\",\"Bouth\", \"Fake\"))\n\ntweet_real = df_train.query('target == \"1\"')['Length']\ntweet_fake = df_train.query('target == \"0\"')['Length']\n\nfig.add_trace(go.Histogram(x = tweet_real, marker_color='#330C73'), row=1, col=1)\nfig.add_trace(go.Histogram(x = tweet_real, marker_color='#330C73'), row=1, col=2)\nfig.add_trace(go.Histogram(x = tweet_fake, marker_color='#EF553B'), row=1, col=2)\nfig.add_trace(go.Histogram(x = tweet_fake, marker_color='#EF553B'), row=2, col=1)\n\nfig.update_layout(showlegend=False, title_text=\"Histograms of the number of characters in the tweets\")\nfig.show()","ed56eca4":"fig = make_subplots(\n    rows=2, cols=2,\n    specs=[\n           [{}, {\"rowspan\": 2}],\n           [{}, None]\n    ],\n    subplot_titles=(\"Real\",\"Bouth\", \"Fake\"))\n\ntweet_real = df_train.query('target == \"1\"')['Word_count']\ntweet_fake = df_train.query('target == \"0\"')['Word_count']\n\nfig.add_trace(go.Histogram(x = tweet_real, marker_color='#330C73'), row=1, col=1)\nfig.add_trace(go.Histogram(x = tweet_real, marker_color='#330C73'), row=1, col=2)\nfig.add_trace(go.Histogram(x = tweet_fake, marker_color='#EF553B'), row=1, col=2)\nfig.add_trace(go.Histogram(x = tweet_fake, marker_color='#EF553B'), row=2, col=1)\n\nfig.update_layout(showlegend=False, title_text=\"Histograms of the number of words in the tweets\")\nfig.show()","ce2bc11b":"dis = \" \".join(df_train.loc[df_train['target'] == 1]['clean_text']).lower().split()\nnon_dis = \" \".join(df_train.loc[df_train['target'] == 0]['clean_text']).lower().split()\n\ntop_dis = Counter([i for i in dis if i not in stop_words])\ntop_non_dis = Counter([i for i in non_dis if i not in stop_words])\n\ntemp_dis = pd.DataFrame(top_dis.most_common(40))\ntemp_non_dis = pd.DataFrame(top_non_dis.most_common(40))\n\ntemp_dis.columns = ['Common_words','count']\ntemp_non_dis.columns = ['Common_words','count']","eb832b03":"fig = px.treemap(temp_non_dis, path=['Common_words'], values='count',title='Popular Non-Disasters Words')\nfig.update_layout(treemapcolorway = [\"#330C73\", \"slateblue\"])\nfig.show()","2461989a":"fig = px.treemap(temp_dis, path=['Common_words'], values='count',title='Popular Disasters Words')\nfig.update_layout(treemapcolorway = [\"#EF553B\", \"mistyrose\"])\nfig.show()","f94b779f":"#1\nmodel_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n\"\"\" \nYou can try 'bert-large-uncased' model\nBut the kernel will take longer\n\"\"\" \ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)\n\n#2\ntokenized = df_train['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n\n#3\nmax_len = 0\nfor i in tokenized.values:\n    if len(i) > max_len:\n        max_len = len(i)\n\n#4\npadded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\nattention_mask = np.where(padded != 0, 1, 0)","ac39647c":"class Dataset(torch.utils.data.Dataset):\n    \n  def __init__(self, input_ids, attention_masks):\n        self.input_ids = input_ids\n        self.attention_mask = attention_masks\n\n  def __len__(self):\n        return len(self.input_ids)\n\n  def __getitem__(self, index):\n        input_ids = self.input_ids[index]\n        attention_mask = self.attention_mask[index]\n        return input_ids, attention_mask","45cae2ee":"input_ids = torch.tensor(padded)  \nattention_mask = torch.tensor(attention_mask)\n\ndataset = Dataset(input_ids=input_ids, attention_masks=attention_mask)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=8, drop_last=False)","0a6fdbe4":"features = []\n\nfor batch_input_ids, batch_attention_mask in tqdm(dataloader):\n  with torch.no_grad():\n    # get embeddings\n    tmp = model(batch_input_ids, attention_mask=batch_attention_mask)\n    # get [CLS] vectors\n    features.extend(tmp[0][:,0,:].numpy())\n    clear_output(True)","8b715b39":"labels = df_train['target'][:len(features)]\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels)","3f8ad49d":"# clf = xgb.XGBClassifier()\n# parameters = {\n#      \"eta\"    : [0.02] ,\n#      \"max_depth\"        : [ 7], # 5, 7, 9],\n#      \"min_child_weight\" : [ 5], #, 3 , 5],\n#      \"gamma\"            : [ 0.0] ,\n#      'n_estimators'     : [1000]\n#      }\n# grid = GridSearchCV(clf,parameters, n_jobs=4, scoring=\"accuracy\",cv=3)\n# grid.fit(pd.DataFrame(train_features),train_labels)\n# best_est = grid.best_estimator_\n# best_est\n\nclf = GradientBoostingClassifier(random_state=0)\nclf.fit(train_features, train_labels)\nclf.score(test_features, test_labels)","8c72fec8":"pred_val = clf.predict_proba(pd.DataFrame(test_features))[:,1]\nfpr, tpr, _ = roc_curve(test_labels, pred_val)\nroc_auc = auc(fpr, tpr)","fc9ce907":"lw = 2\n\ntrace1 = go.Scatter(x=fpr, y=tpr, \n                    mode='lines', \n                    line=dict(color='darkorange', width=lw),\n                    name='ROC curve (area = %0.2f)' % roc_auc\n                   )\n\ntrace2 = go.Scatter(x=[0, 1], y=[0, 1], \n                    mode='lines', \n                    line=dict(color='navy', width=lw, dash='dash'),\n                    showlegend=False)\n\nlayout = go.Layout(title='Receiver operating characteristic ',\n                   xaxis=dict(title='False Positive Rate'),\n                   yaxis=dict(title='True Positive Rate'))\n\nfig = go.Figure(data=[trace1, trace2], layout=layout)\n\nfig.show()","c9d62dca":"test_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest_tokenized = test_df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n\ntest_max_len = 0\nfor i in test_tokenized.values:\n    if len(i) > test_max_len:\n        test_max_len = len(i)\n\ntest_padded = np.array([i + [0]*(test_max_len-len(i)) for i in test_tokenized.values])\ntest_attention_mask = np.where(test_padded != 0, 1, 0)\n\ntest_input_ids = torch.tensor(test_padded)  \ntest_attention_mask = torch.tensor(test_attention_mask)\n\ntest_dataset = Dataset(input_ids=test_input_ids, attention_masks=test_attention_mask)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=8, drop_last=False)","42ec934b":"test_features = []\n\nfor batch_input_ids, batch_attention_mask in tqdm(test_dataloader):\n  with torch.no_grad():\n    tmp = model(batch_input_ids, attention_mask=batch_attention_mask)\n    test_features.extend(tmp[0][:,0,:].numpy())\n    clear_output(True)","5b1c98e0":"predict = clf.predict(pd.DataFrame(test_features))\nsub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsub['target'] = predict\nsub.to_csv('sample_submission.csv', index=False)","8c56d247":"model_class, tokenizer_class, pretrained_weights = (ppb.BertForSequenceClassification, ppb.BertTokenizer, 'bert-base-cased')# ppb.BertTokenizer, 'distilbert-base-cased') #'bert-large-uncased')\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)\n\ntokenized = df_train['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n\nmax_len = 0\nfor i in tokenized.values:\n    if len(i) > max_len:\n        max_len = len(i)\n\npadded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\nattention_mask = np.where(padded != 0, 1, 0)","6c059891":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","65b085aa":"class Dataset(torch.utils.data.Dataset):\n  def __init__(self, input_ids, attention_masks, labels):\n        self.input_ids = input_ids\n        self.attention_mask = attention_masks\n        self.labels = labels\n\n  def __len__(self):\n        return len(self.input_ids)\n\n  def __getitem__(self, index):\n        input_ids = self.input_ids[index]\n        attention_mask = self.attention_mask[index]\n        labels = self.labels[index]\n        return input_ids, attention_mask, labels","a81dd481":"train_input_ids = torch.tensor(padded[:7000])  \ntrain_attention_mask = torch.tensor(attention_mask[:7000])\ntrain_labels = list(df_train['target'][:7000])\n\nvalidation_input_ids = torch.tensor(padded[7000:7613])  \nvalidation_attention_mask = torch.tensor(attention_mask[7000:7613])\nvalidation_labels = list(df_train['target'][7000:7613])\n\ndataset = Dataset(input_ids=train_input_ids, attention_masks=train_attention_mask, labels=train_labels)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=8, drop_last=False)\n\nvalidation_dataset = Dataset(input_ids=validation_input_ids, attention_masks=validation_attention_mask, labels=validation_labels)\nvalidation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=8, drop_last=False)","57e8201c":"optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8 )\nfrom transformers import get_linear_schedule_with_warmup\n\nepochs = 2\ntotal_steps = len(dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","fa36b73d":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","565e12fd":"def format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","cf3706c0":"import random\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\nloss_values = []\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    t0 = time.time()\n    total_loss = 0\n    model.train()\n    for step, batch in enumerate(dataloader):\n        if step % 40 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(dataloader), elapsed))\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        model.zero_grad()        \n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        loss = outputs[0]\n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n    avg_train_loss = total_loss \/ len(dataloader)            \n    loss_values.append(avg_train_loss)\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    print(\"\")\n    print(\"Running Validation...\")\n    t0 = time.time()\n    model.eval()\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    \n    for batch in validation_dataloader: \n        \n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        with torch.no_grad():        \n            outputs = model(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n\n        logits = outputs[0]\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        eval_accuracy += tmp_eval_accuracy\n\n        nb_eval_steps += 1\n\n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy\/nb_eval_steps))\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\nprint(\"\")\nprint(\"Training complete!\")","b6bed2b4":"class Dataset(torch.utils.data.Dataset):\n  def __init__(self, input_ids, attention_masks):\n        self.input_ids = input_ids\n        self.attention_mask = attention_masks\n\n  def __len__(self):\n        return len(self.input_ids)\n\n  def __getitem__(self, index):\n        input_ids = self.input_ids[index]\n        attention_mask = self.attention_mask[index]\n        return input_ids, attention_mask","99bfd4f7":"test_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest_tokenized = test_df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n\ntest_max_len = 0\nfor i in test_tokenized.values:\n    if len(i) > test_max_len:\n        test_max_len = len(i)\n\ntest_padded = np.array([i + [0]*(test_max_len-len(i)) for i in test_tokenized.values])\ntest_attention_mask = np.where(test_padded != 0, 1, 0)\n\ntest_input_ids = torch.tensor(test_padded)  \ntest_attention_mask = torch.tensor(test_attention_mask)\n\ntest_dataset = Dataset(input_ids=test_input_ids, attention_masks=test_attention_mask)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=8, drop_last=False)","444655dd":"from IPython.display import clear_output\n\nmodel.eval()\ntest_features = []\npreds = []\n\nfor batch_input_ids, batch_attention_mask in tqdm(test_dataloader):\n  with torch.no_grad():\n\n    logits = model(batch_input_ids, attention_mask=batch_attention_mask)\n\n    a = logits[0]\n    a = (np.argmax(a.numpy(),axis=1))\n\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n\n\n    pred_flat = np.argmax(logits, axis=1).flatten()\n    preds.extend(a)\n    clear_output(True)","4b1a4ede":"sub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('sample_submission2.csv', index=False)","855ffb6f":"## Pretrained-BERT Model ","2ebedbff":"This is where I get the proposal embeddings and only select the first elements (vectors) from them. Bert specifically creates them to classification!","43a86255":"## Fine-Tuning a BERT \n\n<img src=\"https:\/\/github.com\/tesemnikov-av\/kaggle-rep\/blob\/master\/fine_tuning.png?raw=true\" width=\"1200\">","f08b18a0":"We already did this for training data. Now we do it for predictions.","0db00ea9":"<h1 style=\"text-align:center;font-size:200%;;\">BERT PyTorch : Pretrained + Fine-Tuning<\/h1>\n\n![Forgive My Laughter. I Have A Condition](https:\/\/github.com\/tesemnikov-av\/kaggle-rep\/blob\/master\/Joker-killing-the-three-Wayne-Enterprises-employees-on-a-train.jpg?raw=true)\n\n<h3  style=\"text-align:left;\">Tags : <span class=\"label label-success\">NLP<\/span> <span class=\"label label-success\">BEGINER<\/span>\n\n<div class=\"alert alert-warning\" role=\"alert\"> - Yet Another Kernel About Bert. - Forgive My Laughter. I Have A Condition.<\/div>","1df393eb":"Here we ask our model to predict the values for new tweets. This model (Fine-tuning BERT) works longer, but its results are better.","c68c61f4":"<img src=\"https:\/\/github.com\/tesemnikov-av\/kaggle-rep\/blob\/master\/BERT_DATASET2.png?raw=true\" width=\"1200\">","5f318b29":"We divide our data into training and validation samples.","9f504332":"Here I load a modified BERT model specifically for classification. We can train it on our data!","dfea912a":"## Exploratory Data Analysis\n\n<img src=\"https:\/\/github.com\/tesemnikov-av\/kaggle-rep\/blob\/master\/MASK.png?raw=true\" width=\"1200\">","28388bba":"1. Load pretrained model and tokenizer\n2. Get tokens for tweets\n3. Get max len in token\n4. Created padded and attention mask","5bd3487a":"In this kernel I will use two models BERT. In one of them, I will use a pre-trained BERT model  +  gradient boosting classifier. In the second model, I will use the fine-tuning of a Bert model designed specifically for classification.\n   \nI apologize for writing few comments. If you would like to learn more about how these models work, visit the following pages:\n   \n**Pre-trained BERT model:**\nhttp:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n\n**Fine-tuning of a Bert model**\nhttps:\/\/mccormickml.com\/2019\/07\/22\/BERT-fine-tuning\/\n\n   Let's GO!","f744568a":"<img src=\"https:\/\/github.com\/tesemnikov-av\/kaggle-rep\/blob\/master\/WSS2.png?raw=true\" width=\"1200\">","891c4fe1":"There is a lot of code here, but in fact this is the most common training procedure and model validation on two epochs (we will go through all the data twice and so adjust the weights for our dataset). The number of epochs is a hyperparameter and can be changed.","937d5839":"![Bert + XGBOOST](https:\/\/github.com\/tesemnikov-av\/kaggle-rep\/blob\/master\/bert_xgboost.png?raw=true)"}}