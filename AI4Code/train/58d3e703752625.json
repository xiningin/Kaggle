{"cell_type":{"4fbb6786":"code","1661135a":"code","6cabb5da":"code","53007594":"code","5dc5a424":"code","4a5274d9":"code","4cd15838":"code","e9541c12":"code","7863c686":"code","0d6e40b2":"code","845b8e37":"code","7f7f953d":"code","b882599e":"code","5a3b5e16":"code","940d04d9":"code","f2540fc2":"code","3c842b85":"code","c4315b30":"code","b9028945":"code","7285850a":"markdown","d4a9d9d6":"markdown","2fb4809c":"markdown","16eea0e1":"markdown","be01b994":"markdown","29b1cf09":"markdown","1b0c6063":"markdown","f33c6b50":"markdown","dde61dbd":"markdown","2eca6c09":"markdown","5272433c":"markdown","de576c13":"markdown","016c6fa4":"markdown","e849b811":"markdown","19bf4968":"markdown"},"source":{"4fbb6786":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n\nplt.style.use(\"seaborn-whitegrid\")\n\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.preprocessing import (OrdinalEncoder, StandardScaler, \n                                   MinMaxScaler, PolynomialFeatures,\n                                   PowerTransformer)\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.linear_model import Lasso, RidgeCV\nfrom sklearn.ensemble import (RandomForestRegressor, AdaBoostRegressor,\n                             GradientBoostingRegressor, ExtraTreesRegressor)\nfrom sklearn.ensemble import VotingRegressor\n\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom sklearn.svm import SVC \nfrom xgboost import XGBRegressor\n%matplotlib inline\n\nfrom bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')","1661135a":"X_full = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col='id')\nX_test_full = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col='id')\nSample_result = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")","6cabb5da":"(mu, sigma) = norm.fit(X_full['target'])\nplt.figure(figsize = (12,6))\nsns.distplot(X_full['target'], kde=True, hist=True, fit=norm)\nplt.title('Target Distribution', fontsize=12)\nplt.legend([f\"mu:{mu}, sigma:{sigma}\"],loc=\"best\")\nplt.show()","53007594":"print(f\"Skewness: {X_full['target'].skew()}\")\nprint(f\"Kurtosis: {X_full['target'].kurt()}\")","5dc5a424":"X_full.describe()","4a5274d9":"num_types = X_full.select_dtypes(include=['int64', 'float64']).columns[:-1]\nfig, axes = plt.subplots(nrows=7, ncols=2, figsize=(10,25))\nfor indx,feature in enumerate(num_types):\n    row = indx \/\/ 2\n    col = indx % 2\n    X_full[num_types[indx]].hist(ax=axes[row,col]);axes[row,col].set_title(f\"{num_types[indx]}\")   ","4cd15838":"mean = X_full['target'].mean()\nstd = X_full['target'].std()\ncut_off = std * 3\nlower, upper = mean - cut_off, mean + cut_off\noutliers = X_full[(X_full['target'] < lower) | (X_full['target'] > upper)]\nprint(f\"Orginal Dataset size: {X_full.shape}\")\n#X_full.drop(outliers.index.to_list(), inplace=True)\nprint(f\"Number of outliers: {len(outliers)}\")\nprint(f\"New size: {X_full.shape}\")","e9541c12":"features = X_full.drop(['target'], axis=1, inplace=False)\ntargets = X_full['target'].copy()\n\nfor cols in features.select_dtypes(\"object\"):\n    features[cols], _ = features[cols].factorize()\n    \ndiscrete_features = features.dtypes == int\n\nmi_value = mutual_info_regression(features, targets, discrete_features=discrete_features)\nmi_value = pd.Series(mi_value, name=\"MI\", index=features.columns)\nmi_value = mi_value.sort_values(ascending=True)\nwidth = np.arange(len(mi_value))\nticks = list(mi_value.index)\n\nplt.figure(dpi=100, figsize=(8,5))\nplt.barh(width, mi_value)\nplt.yticks(width, ticks)\nplt.title(\"Mutual Information\")","7863c686":"X_full.columns","0d6e40b2":"X_full.dropna(axis=0, subset=['target'], inplace=True)\ny = X_full['target']\nX_full.drop(['target'], axis=1, inplace=True)","845b8e37":"X = X_full[list(mi_value.index)[-24:]]\nX_test = X_test_full[list(mi_value.index)[-24:]]","7f7f953d":"print(f\"Shape of training data: {X.shape}\")\nmissing_values = X.isnull().sum()\nprint(missing_values[missing_values > 0])","b882599e":"num_cols = X.select_dtypes(include=['int64', 'float64']).columns\ncat_cols = X.select_dtypes(include=['object', 'bool']).columns","5a3b5e16":"skewed_features = X[num_cols].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\nskewed_features = skewed_features[abs(skewed_features) > 0.75]\nprint(skewed_features)\n\nfor f in skewed_features.index:\n    X[f] = np.log1p(X[f])\n    X_test[f] = np.log1p(X_test[f])","940d04d9":"print(\"Number of unique category for each categorical Feature\")\nfor cols in cat_cols:\n    print(f\"{cols}: {X[cols].nunique()}\")","f2540fc2":"cat_transformer = OrdinalEncoder()\nnum_transformer = StandardScaler()\npreprocessor = ColumnTransformer(transformers=[\n    ('cat', cat_transformer, cat_cols),\n    ('num', num_transformer, num_cols)])","3c842b85":"def train(model):\n    \n    clf = Pipeline(steps=[('preprocessor', preprocessor),\n                       ('model', model)\n                     ])\n\n    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n    ypred = 0\n    total_loss = 0\n    for train_indx, test_indx in cv.split(X):\n        X_train, X_val = X.iloc[train_indx], X.iloc[test_indx]\n        y_train, y_val = y.iloc[train_indx], y.iloc[test_indx]\n        clf.fit(X_train, y_train)\n        \n        yhat = clf.predict(X_val)\n        score = mean_squared_error(yhat, y_val, squared=False)\n        print(f\"Loss:{score}\")\n        ypred += clf.predict(X_test) \/ 10\n        total_loss += score \/ 10\n        \n    print(f\"Avg. Loss: {total_loss}\")     \n    return ypred\n    ","c4315b30":"model = XGBRegressor(n_estimators= 10000, booster='gbtree', tree_method='gpu_hist', \n                    learning_rate= 0.03628302216953097, subsample= 0.7875490025178415, max_depth= 3,\n                    colsample_bytree = 0.11807135201147481, reg_alpha = 23.13181079976304, random_state = 1,\n                    reg_lambda = 0.0008746338866473539, n_jobs=-1)\n\nfinal_prediction = train(model)","b9028945":"Sample_result['target'] = final_prediction \nSample_result.to_csv(\"submission.csv\", index=False)","7285850a":"### Outlier Detection and Removal \nThough it is recommended to remove outliers for proper model training, there was a dip in performance after removing outliers. This may be because either w\/o outliers the model may be overfitting and the outliers are providing a regularization effect in the form of noise or the outliers may be having some important features which may be helping in the final prediction.\nIn the given cell in order to remove outliers uncomment the single commented line.","d4a9d9d6":"### Load the Dataset","2fb4809c":"### Import the libraries","16eea0e1":"### Feature Engineering\nFeature engineering is one of the most important step for any data science problem.   \nIt involves transforming the original raw data in a form which helps the model in learning the given function.  \nIt can include either skipping redundant features or transforming the original feature or adding features.  \nTo get an idea about the features and its relation to the given target two popular tools used are:\n* Correlation\n* Mutual Information  \n\nWhile **correlation** is limited to only linear relationships, **mutual information** can be used for any kind of relationships. *Mutual Information* describes how presence of a given features reduces the uncertainty of the target variable. MI is lower bounded by 0 and there is no upper bound. The variables are independent if MI is 0.","be01b994":"In general skewness should be in range of [-0.5,0.5] and kurtosis in [-2,2].","29b1cf09":"### Preprocess the Input Data ","1b0c6063":"#### EDA : Exploratory Data Analysis, this is used for getting a better insight of the data","f33c6b50":"### Feature Visualisation","dde61dbd":"### Extract Features and Target","2eca6c09":"### Analyse the Dataset","5272433c":"#### Taking the top N features\nCurrently all 24 features are being used.","de576c13":"### **Using Pipelines to preprocess and create a model**","016c6fa4":"#### Transform the skewed features to normal distribution","e849b811":"### Training","19bf4968":"### Final Submission"}}