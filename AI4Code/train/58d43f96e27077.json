{"cell_type":{"28927e4d":"code","f37d8e7c":"code","05a76bf3":"code","9b0544c9":"code","f701ef44":"code","f3cb9907":"code","6fa36d14":"code","ff238c04":"code","d9629763":"code","9fd2e4c8":"code","19f50c60":"code","19b817b7":"code","4a7e9694":"code","9bedaef2":"code","51b97add":"code","e6d4eab2":"code","a2c69bfc":"code","c5db732e":"code","9e3230b9":"code","206a3cbf":"code","31171bea":"code","68fefa1e":"code","945b73da":"code","ff6dd8cd":"code","99c04d03":"code","08f08214":"code","9a29b4e7":"code","41f818f8":"code","475e9ccf":"code","a218c7f8":"code","874b1b42":"code","10f974c9":"code","8db2105d":"code","1a8c1c63":"code","b9a4df8d":"code","f6b38568":"code","d5e5a403":"code","773fc08e":"code","413ddfbf":"code","cb2da20b":"code","98921e8b":"code","acf28fdb":"code","80dc6619":"code","4cd68e90":"code","bf52e82f":"code","0a73972f":"code","92687287":"code","ee854d4d":"code","ed19e16a":"code","9fb85e38":"code","fbd95093":"code","6d8c8b36":"code","2d9aa5d9":"code","ea739d57":"code","052b809c":"code","2a4beecf":"code","9a883b66":"markdown","f6bbccae":"markdown","3c95c09e":"markdown","09ff495d":"markdown","0300a601":"markdown","4e983fe1":"markdown","8f3426b6":"markdown","d270fc8e":"markdown","1cf26948":"markdown","be279ffc":"markdown","5a1e58c7":"markdown","8377a030":"markdown","cf38d225":"markdown","65c5f545":"markdown","8c41909f":"markdown","a22490b9":"markdown","c16398ed":"markdown","fab30972":"markdown","3ec29b1d":"markdown","83226815":"markdown","6b3ec2ed":"markdown","831c7c45":"markdown","fe114a0c":"markdown","c8aaa34b":"markdown","fc27aca4":"markdown"},"source":{"28927e4d":"!pip install --upgrade scikit-learn","f37d8e7c":"import sklearn","05a76bf3":"print(sklearn.__version__)","9b0544c9":"import multiprocessing\nimport pandas as pd \nimport numpy as np\n\n#preprocessing \nfrom sklearn.model_selection import train_test_split, cross_val_score,StratifiedKFold\n\n#scaling \nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\n\n# metrics \nfrom sklearn.metrics import roc_auc_score, classification_report\n\n#models \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n## hyperparameter tuning\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping \nfrom tensorflow.keras.metrics import AUC\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier, VotingClassifier\n\n# viz \nimport seaborn as sns\nimport matplotlib.pyplot as plt ","f701ef44":"# remove the Id as its not needed\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\",index_col=0)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\",index_col=0)","f3cb9907":"train.shape","6fa36d14":"train.isnull().sum().sort_values(ascending = False)","ff238c04":"train.describe(exclude=\"float64\")","d9629763":"train[\"target\"].value_counts().plot(kind = \"bar\")","9fd2e4c8":"#check for duplicates\ntrain[train.duplicated()]","19f50c60":"train['sum'] = train.sum(axis = 1)\ntrain['mean'] = train.mean(axis = 1)\ntrain['std'] = train.std(axis = 1)\ntrain['min'] = train.min(axis = 1)\ntrain['max'] = train.max(axis = 1)\n\ntest['sum'] = test.sum(axis = 1)\ntest['mean'] = test.mean(axis = 1)\ntest['std'] = test.std(axis = 1)\ntest['min'] = test.min(axis = 1)\ntest['max'] = test.max(axis = 1)","19b817b7":"# split into target and values \nX= train.drop(\"target\", axis =1 )\ny = train[\"target\"]","4a7e9694":"X.head()","9bedaef2":"y.value_counts()","51b97add":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","e6d4eab2":"std_scaler = StandardScaler()\nminmax_scaler = MinMaxScaler()","a2c69bfc":"\"\"\"minmax_scaler.fit(X_train)\nX_train_m = minmax_scaler.transform(X_train)\nX_test_m = minmax_scaler.transform(X_test)\"\"\"\n\nstd_scaler.fit(X_train)\nX_train= std_scaler.transform(X_train)\nX_test = std_scaler.transform(X_test)","c5db732e":"param = {'metric': \"auc\"}","9e3230b9":"# Linear Models \nlr = LogisticRegression()\nsvc = LinearSVC(max_iter=4000)  # used linearSVC as this is faster than SVC()\nridge = RidgeClassifier()\nknn = KNeighborsClassifier()\n\n# Trees & boosting \n# dtree = DecisionTreeClassifier()  -wont use as we have enough trees\nrf = RandomForestClassifier()\n\nlgb_i = lgb.LGBMClassifier()\nada_i = AdaBoostClassifier()    # takes long \nxgb_i = xgb.XGBClassifier(n_estimators =1000)","206a3cbf":"## Cross validaition \ndef model_scoring_CV(model):\n    #cv = StratifiedKFold(n_splits=3)\n    cross_val = cross_val_score(model,X_train,y_train,cv =5,scoring=\"roc_auc\")\n    print(\"mean CV roc_auc\",cross_val.mean())\n    \n# Basic model\ndef model_scoring(model):\n    \n    model.fit(X_train,y_train)\n    \n    y_pred = model.predict(X_test)\n\n    print(\"Training score:\",model.score(X_train,y_train))  \n    print(\"Test auc score\", roc_auc_score(y_test, y_pred))\n    print(\"\\n\")\n    print(classification_report(y_test, model.predict(X_test)))\n    \n    return model","31171bea":"# Logistic Regression \n#model_scoring_CV(lr)\n\nlr_model = model_scoring(lr)","68fefa1e":"# Support Vector Classifier \n#model_scoring_CV(svc)  # has convergence warnings\n\nSVC_model = model_scoring(svc) ","945b73da":"#model_scoring_CV(ridge)\n\nridge_model = model_scoring(ridge)","ff6dd8cd":"xgb_i.fit(X_train,\n          y_train,\n          eval_metric= [\"auc\"],\n          early_stopping_rounds=10,\n          eval_set= [(X_test, y_test)]\n         )\ny_pred = xgb_i.predict(X_test)","99c04d03":"print(\"Training score:\",xgb_i.score(X_train,y_train))  \nprint(\"Test auc score\", roc_auc_score(y_test, y_pred))\nprint(\"\\n\")\nprint(classification_report(y_test, xgb_i.predict(X_test)))","08f08214":"\"\"\"param_xgb = {\"eta\": [0, 0.001,0.01,0.1,1],\n             'max_depth': [2, 4, 6],\n             'eval_metric' : ['auc'],\n             'n_estimators': [xgb_model.best_iteration]         \n            }\"\"\"\n\nparam = {\"eval_metric\" : [\"auc\"]}","9a29b4e7":"def Halving_CV(model, param):\n    #cv = StratifiedKFold(n_splits=3)\n    clf = HalvingGridSearchCV(model, param, cv=5 ,scoring= \"roc_auc\")\n\n    clf.fit(X_train, y_train)\n    print(\"Best score:\",clf.score(X_train,y_train))  \n    print(\"train auc score\", roc_auc_score(y_train, clf.predict(X_train)))\n    print(\"Test auc score\", roc_auc_score(y_test, clf.predict(X_test)))\n    print(classification_report(y_test, clf.predict(X_test)))\n    \n    return clf ","41f818f8":"rf_model = Halving_CV(rf,{\n    \"max_depth\":[8],\n    \"max_features\":[80]\n})","475e9ccf":"#ada_model = Halving_CV(ada_i,{})","a218c7f8":"param_lgb = {'metric': \"auc\",\n        \"learning_rate\" : 0.01,\n        #\"boosting\": \"dart\",\n            \"max_depth\" : 10,\n            \"num_leaves\": 30}","874b1b42":"\"\"\"lgb_train = lgb.Dataset(X_train, label= y_train)\nlgb_test = lgb.Dataset(X_test, label=y_test)\n\nlgb_model = lgb.train(params= param_lgb, train_set=lgb_train, valid_sets=[lgb_test], early_stopping_rounds=10, num_boost_round = 10000)\"\"\"","10f974c9":"\"\"\"lgb_model.best_score\"\"\"","8db2105d":"## Round probabilities\n#### lgb can only predict on the raw data (not the lgb transformed data)\n\"\"\"lgb_pred_train = (lgb_model.predict(X_train)>0.5).astype(\"int32\")\nlgb_pred = (lgb_model.predict(X_test)>0.5).astype(\"int32\")\"\"\"","1a8c1c63":"\"\"\"print(\"Train auc score\", roc_auc_score(y_train, lgb_pred_train))\nprint(\"Test auc score\", roc_auc_score(y_test, lgb_pred))\nprint(classification_report(y_test, lgb_pred))\"\"\"","b9a4df8d":"lgb_i = lgb.LGBMClassifier(learning_rate=0.01, max_depth=10,num_leaves = 30, n_estimators= 10000)\nlgb_i.fit(X_train,y_train, eval_metric = \"auc\",eval_set = [(X_test,y_test)] , early_stopping_rounds=10)","f6b38568":"lgb_pred_train = lgb_i.predict(X_train)\nlgb_pred = lgb_i.predict(X_test)","d5e5a403":"print(\"Train auc score\", roc_auc_score(y_train, lgb_pred_train))\nprint(\"Test auc score\", roc_auc_score(y_test, lgb_pred))\nprint(classification_report(y_test, lgb_pred))","773fc08e":"#knn_model = Halving_CV(knn,{})","413ddfbf":"def create_model():   \n    deep_model = Sequential()\n    deep_model.add(Dense(100, activation = \"relu\"))\n    deep_model.add(Dropout(0.5))\n\n    deep_model.add(Dense(50, activation = \"relu\"))\n    deep_model.add(Dropout(0.5))\n\n    deep_model.add(Dense(20, activation = \"relu\"))\n    deep_model.add(Dropout(0.5))\n\n    deep_model.add(Dense(1, activation = \"sigmoid\"))\n\n    deep_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics= [AUC()])\n    return deep_model","cb2da20b":"deep_model = create_model()\ndeep_model.fit(X_train, y_train, epochs = 2000, validation_data =(X_test, y_test) , batch_size = 128,callbacks=EarlyStopping(patience=20),use_multiprocessing=True)","98921e8b":"history = deep_model.history.history\nhistory = pd.DataFrame(history)\n\ny_deep =(deep_model.predict(X_test) > 0.5).astype(\"int32\")\n\nprint(\"Best score:\",deep_model.evaluate(X_train,y_train))  \nprint(\"train auc score\", roc_auc_score(y_train, (deep_model.predict(X_train) > 0.5).astype(\"int32\")))\nprint(\"Test auc score\", roc_auc_score(y_test,y_deep))\nprint(classification_report(y_test, y_deep))","acf28fdb":"history.plot()","80dc6619":"ann_model  = KerasClassifier(build_fn=create_model, epochs = 2000, validation_data =(X_test, y_test) , batch_size = 128,callbacks=EarlyStopping(patience=10))\nann_model._estimator_type = \"classifier\"","4cd68e90":"vc_linear = VotingClassifier(\n    estimators=[\n        ('lr', lr_model),\n        (\"svc\",SVC_model),\n        (\"ridge\",ridge_model),\n        #('ann', ann_model)\n    ], \n    voting='hard')","bf52e82f":"vc_linear.fit(X_train,y_train)","0a73972f":"y_vc = vc_linear.predict(X_test)\n\nprint(\"Best score:\",vc_linear.score(X_train,y_train))\nprint(\"train auc score\", roc_auc_score(y_train, vc_linear.predict(X_train)))\nprint(\"Test auc score\", roc_auc_score(y_test,y_vc))\nprint(classification_report(y_test, y_vc))","92687287":"vc_tree = VotingClassifier(\n    estimators=[\n        (\"lgb\",lgb_model),\n        (\"xgb\",xgb_i),\n        (\"rf\",rf_model),\n       # (\"ada\",ada_model)\n    ], \n    voting='hard')","ee854d4d":"vc_tree.fit(X_train,y_train)","ed19e16a":"y_vc = vc.predict(X_test)\n\nprint(\"Best score:\",vc.score(X_train,y_train))\nprint(\"train auc score\", roc_auc_score(y_train, vc_tree.predict(X_train)))\nprint(\"Test auc score\", roc_auc_score(y_test,y_vc))\nprint(classification_report(y_test, y_vc))","9fb85e38":"# standard scaling\ns_test = std_scaler.transform(test)","fbd95093":"lin_pred = vc_linear.predict(s_test)\n\nsub = pd.DataFrame(lin_pred, columns=[\"target\"])\nsub.set_index(test.index,inplace=True)\n\nsub.to_csv(\"submission_linear.csv\")","6d8c8b36":"sub.sample(10)","2d9aa5d9":"tree_pred = vc_tree.predict(s_test)\n\nsub = pd.DataFrame(tree_pred, columns=[\"target\"])\nsub.set_index(test.index,inplace=True)\n\nsub.to_csv(\"submission_tree.csv\")","ea739d57":"sub.sample(10)","052b809c":"ann_pred = vc_linear.predict(s_test)\n\nsub = pd.DataFrame(ann_pred, columns=[\"target\"])\nsub.set_index(test.index,inplace=True)\n\nsub.to_csv(\"submission_ann.csv\")","2a4beecf":"sub.sample(10)","9a883b66":"# K-Nearest Neightbors","f6bbccae":"### Linear","3c95c09e":"# Feature Engineering \n\nIdea from this kernel \nhttps:\/\/www.kaggle.com\/christoforum\/tps-nov-2021-lightgbm-optuna\/comments","09ff495d":"# Scaling:\nDependent on model used\n\n* Standardisation will be used for general models\n* MinMaxscaler will be used if needed","0300a601":"# Linear Models","4e983fe1":"**Unbelievable score**, this looks to be the best model however after submission using only SVC we got +-55% AUC \\\nThis count indicate that :\n1. we have leakage - however we are using Cross validation so this doesnt seem to be the case  or \n2. Our training data is biased to linear modelling and\/or doesnt represent the full dataset.","8f3426b6":"## Halving GridSearch CV \nDue to the number of observations & features, certain models take to long to converge \/ complete. \n\nWe will use an experimental package, **HalvingGridSearchCV**, which applies successive halving. \\\nAs per the [documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.HalvingGridSearchCV.html?highlight=halvinggrid#sklearn.model_selection.HalvingGridSearchCV):\n\n*The search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources*","d270fc8e":"# Quick EDA ","1cf26948":"### ANN","be279ffc":"## Using the .Fit call (not .train)\nWe use .fit as this is comparible with Voting Classifier ","5a1e58c7":"# Voting Classifier \nWe can now create a ensemble model with a few of our trained models \\\nWe will do something interesting here and try ensembel our models into TREES ,  LINEAR and ANN\nnote: we could have merged ANN and Linear however ANN only uses predict probabilties and LinearSVC only has exclusive classication outputs\n\n**Type of voting used:**\n*If \u2018hard\u2019, uses predicted class labels for ***majority*** rule voting. Else if \u2018soft\u2019, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers*\n\nWe will use \"hard\" as our models arent optimized","8377a030":"### Tree ","cf38d225":"### Instantiate models","65c5f545":"# Deep Learning (ANN)","8c41909f":"# Trees and Boosting \n### Due to the time taken to run tree models we can use either:\n* optimized methods built into the algorithm (i.e. XGboost with Dmatrices) \n* HalvingSearchCSV  - experimental sklearn package that applies successive halving of the data (essentially reduces run time) ","a22490b9":"## XGBoost","c16398ed":"### Keras doesnt interact with Voting Classifier \nWe therefore need to use the keras sklearn wrapper to make the model compatible","fab30972":"# Light GBM\nlets apply light GBM again using its optimized dataset and with evaluation data + early stopping ","3ec29b1d":"# Train test split","83226815":"## Tree Voting - hard voting","6b3ec2ed":"# Base model selection with Cross Validation \nWe will try to identify the best model to use by apply the default model paramaters to the training and validation data and scoring their AUC results \\\nCross validation score will also be used to check we arent overfitting \n\n**Hyperparameter tuning:** Tuning take a long time to optimize a with Kaggles limitation of 9hrs runtime we will therefore use the default hyperparameters unless we find obvious optimizations","831c7c45":"# Submissions","fe114a0c":"# Libraries","c8aaa34b":"## Linear - hard voting\nHard voting as this will give us the best output and LinearSVC doesnt have a \"predict_proba\" method, so cant be used for Soft","fc27aca4":"Time of KNN is too long - removed as auc was tested in isolation and was very low"}}