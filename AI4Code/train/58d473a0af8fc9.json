{"cell_type":{"f06e08e3":"code","f053e379":"code","57e7556d":"code","7aa86e3b":"code","a09e794c":"code","3b1c15e0":"code","377f0b5a":"code","53fff099":"code","63c71cf2":"code","2b29ee6b":"code","cbc0b7b9":"code","10ac5221":"code","b640c711":"code","9c191a23":"code","d81cc846":"code","343c459e":"markdown","ec711faf":"markdown","47c7bb67":"markdown","5e1e5348":"markdown","9913ccba":"markdown","7b0816da":"markdown","a7341dcf":"markdown","a101fd73":"markdown","d7d94e18":"markdown"},"source":{"f06e08e3":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom tqdm.autonotebook import tqdm","f053e379":"TRAINING_FILE = \"..\/input\/pascfolds\/cleaned_8fold\/train0.csv\"\n\nTRAIN_BATCH_SIZE = 128\nVALID_BATCH_SIZE = 64\nHIDDEN_SIZE = 64\nLEARNING_RATE = 1e-3\nEPOCHS = 100","57e7556d":"#Standard utils from abhishek\/utils\nclass AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","7aa86e3b":"def accuracy_score(y_pred, y_true): \n    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n    correct_results_sum = (y_pred_tag == y_true).sum().float()\n    acc = correct_results_sum\/y_true.shape[0]\n    acc = torch.round(acc * 100)\n    return acc","a09e794c":"class DoctorModel(nn.Module):\n    def __init__(self, in_size, hidden_size):\n        super(DoctorModel, self).__init__()\n        \n        self.layer_1 = nn.Linear(in_size, hidden_size) \n        self.layer_2 = nn.Linear(hidden_size, hidden_size)\n        self.layer_out = nn.Linear(hidden_size, 1) \n        \n        torch.nn.init.normal_(self.layer_out.weight, std=0.02)\n        torch.nn.init.kaiming_normal_(self.layer_1.weight, mode='fan_in', nonlinearity='relu')\n        torch.nn.init.kaiming_normal_(self.layer_2.weight, mode='fan_in', nonlinearity='relu')\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.3)\n        self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n        self.batchnorm2 = nn.BatchNorm1d(hidden_size)\n        \n    def forward(self, inputs):\n        x = self.relu(self.layer_1(inputs))\n        x = self.batchnorm1(x)\n    \n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm2(x)\n    \n        x = self.dropout(x)\n        x = self.layer_out(x)\n        \n        return x","3b1c15e0":"class DoctorDataset:\n    def __init__(self, dfx):\n        self.dfx = dfx\n        self.X = self.dfx.iloc[:, 0:-1]\n        self.Y = self.dfx.iloc[:, -1]\n        \n    def __len__(self):\n        return len(self.dfx)\n    \n    def __getitem__(self, idx):\n        return {\n            'ids': torch.tensor(self.X.iloc[[idx]].values, dtype=torch.float).squeeze(0),\n            'truth': torch.tensor(self.Y.iloc[[idx]].values, dtype=torch.float)\n        }","377f0b5a":"def train_fn(data_loader, model, optimizer, criterion, device):\n    model.train()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    \n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        truth = d[\"truth\"]\n        \n        ids = ids.to(device, dtype=torch.float)\n        truth = truth.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        pred = model(ids)\n        loss = criterion(pred, truth)\n        score = accuracy_score(pred, truth)\n        loss.backward()\n        optimizer.step()\n        \n        losses.update(loss.item(), ids.size(0))\n        scores.update(score, ids.size(0))\n        tk0.set_postfix(loss=losses.avg, score=scores.avg)","53fff099":"def eval_fn(data_loader, model, criterion, device):\n    model.eval()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    \n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            truth = d[\"truth\"]\n\n            ids = ids.to(device, dtype=torch.float)\n            truth = truth.to(device, dtype=torch.float)\n\n            pred = model(ids)\n            loss = criterion(pred, truth)\n            score = accuracy_score(pred, truth)\n\n            losses.update(loss.item(), ids.size(0))\n            scores.update(score, ids.size(0))\n            tk0.set_postfix(loss=losses.avg, score=scores.avg)\n        \n    print(f\"Accuracy: {scores.avg}\")\n    return scores.avg    ","63c71cf2":"def run(fold):\n    dfx = pd.read_csv(TRAINING_FILE)\n    df_train = dfx[dfx.kfold != fold].drop([\"kfold\"], axis=1).reset_index(drop=True)\n    df_valid = dfx[dfx.kfold == fold].drop([\"kfold\"], axis=1).reset_index(drop=True)\n    \n    train_dataset = DoctorDataset(df_train)\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size = TRAIN_BATCH_SIZE,\n        shuffle = True,\n        num_workers = 4)\n    \n    valid_dataset = DoctorDataset(df_valid)\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size = VALID_BATCH_SIZE,\n        shuffle = True,\n        num_workers = 2)\n    \n    device = torch.device(\"cpu\")\n    \n    model = DoctorModel(in_size=len(dfx.columns)-2, hidden_size=HIDDEN_SIZE)\n    model = model.to(device)\n    \n    num_train_steps = int(len(df_train) \/ TRAIN_BATCH_SIZE * EPOCHS)\n    \n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    criterion = nn.BCEWithLogitsLoss()\n    criterion = criterion.to(device)\n    \n    es = EarlyStopping(patience=5, mode=\"max\")\n    print(f\"Training is starting...\")\n    \n    for epoch in range(EPOCHS):\n        train_fn(train_data_loader, model, optimizer, criterion, device)\n        score = eval_fn(valid_data_loader, model, criterion, device)\n        es(score, model, model_path=f\"model.bin\")\n        if es.early_stop:\n            print(\"Early Stoppage\")\n            break","2b29ee6b":"run(fold=0)","cbc0b7b9":"df_test = pd.read_csv(\"..\/input\/pascfolds\/test_all.csv\")\ndf_test.head()","10ac5221":"device = torch.device(\"cpu\")\n\nmodel = DoctorModel(in_size=len(df_test.columns)-1, hidden_size=HIDDEN_SIZE)\nmodel = model.to(device)\nmodel.load_state_dict(torch.load(\"model.bin\", map_location=torch.device('cpu')))\nmodel.eval()","b640c711":"test_dataset = DoctorDataset(df_test)\ntest_data_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size = VALID_BATCH_SIZE,\n        shuffle = False,\n        num_workers = 1)\n\nfinal_outputs = []\nwith torch.no_grad():\n    tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        ids = ids.to(device, dtype=torch.float)\n\n        pred = model(ids)\n        pred = torch.sigmoid(pred)\n  \n        pred = pred.cpu().detach().numpy()\n        pred = np.rint(pred).tolist()\n        for p in pred:\n            output = \"no\" if p[0] == 0.0 else \"yes\"\n            final_outputs.append(output)","9c191a23":"sample = pd.read_csv(\"..\/input\/pasc-data-quest-20-20\/sample_submission.csv\")\nsample.loc[:, \"Y\"] = final_outputs\nsample.to_csv(\"submission.csv\", index=False)","d81cc846":"sample.head()","343c459e":"**Note about the Data used**<br>\nProcessed Data splits are available at [pasc-folds](http:\/\/www.kaggle.com\/rhn1903\/pascfolds)<br>\nData processing is done in a separate notebook that I won't be able to properly comment. But here's a short summary -\n* Remove rows with more than 2 outliers with IQR Rule\n* Undersample negative samples to achieve 1:1 output class distribution\n* Split dataset while maintaining 1:1 distrib[](http:\/\/)ution resulting in 8 balanced splits\n* Scale the numerical features, encode the categorical features (OHE)\n* Further use a KFold CV split to get around 40 different possible train-dev sets","ec711faf":"# Creating the Model","47c7bb67":"# Loading the data","5e1e5348":"# Training the model","9913ccba":"# All the imports","7b0816da":"Train","a7341dcf":"# **PASC DL BASELINE**\nThis is the baseline model for PASC DataQuest. Due to lack of time, I have used only 1 data split for the competition out of the 40 available to achieve an accuracy around **85%-86% (dev), 88.9% (Public LB), 85.9% (Private LB)**. Using all the splits to create a CV ensemble will definitely give much better results.","a101fd73":"# Setting the hyperparameters","d7d94e18":"# Evaluate\nThe test data loaded here follows the same steps as described at the beginning of this notebook."}}