{"cell_type":{"e8c30da8":"code","b95c9cba":"code","42be4612":"code","0dc2017d":"code","8dfdc7ed":"code","6352b26b":"code","d9a1b0db":"code","e58464dc":"code","6288dbd3":"code","5e9bf3d5":"code","6dbcbc84":"code","cc8148fb":"code","758eebff":"code","7be361a4":"code","261caefc":"code","a9b0f13c":"code","34545155":"code","b977117e":"code","6db473f2":"code","a659091c":"code","2f9e50b9":"code","843be92a":"code","35ab344f":"code","8d95dde0":"code","e0e9db92":"code","022eadab":"code","6f3b5313":"code","98ae376b":"code","a895a3b2":"code","d8f28f3c":"code","258fa15b":"code","a36136ff":"code","50c42b4b":"code","6d161cc6":"code","bbd796cc":"code","0672b8be":"code","b8c06882":"code","1dcfb0e1":"code","a155b3d1":"code","81c733e0":"code","b9804cb2":"code","e5d2f087":"code","6a508313":"code","6d94555c":"code","3391d4ca":"code","b74c0aa2":"code","3f459d1d":"code","edb4c63d":"code","80aa6a92":"code","27a06308":"code","798525b8":"code","207c81b4":"code","7903b1d2":"code","f8b43394":"code","6623d838":"code","28ad0b29":"code","e73cc5c2":"code","23695194":"code","019382d5":"code","9b2f6aa4":"code","ab57bd79":"code","7ff41872":"markdown","f7f07d10":"markdown","bc105c9e":"markdown","8c9ac6e3":"markdown","f5e7204e":"markdown","5eb2f621":"markdown","1ee1cc7e":"markdown","c720b29b":"markdown","709c0e64":"markdown","e19fdd6b":"markdown","57a658af":"markdown","990473d0":"markdown","43ce9772":"markdown","c271dfa3":"markdown","8e651361":"markdown","b1378176":"markdown"},"source":{"e8c30da8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b95c9cba":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","42be4612":"df_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","0dc2017d":"df_train.head(10)   # See first 10 rows","8dfdc7ed":"df_test.tail(10)   # See last 10 rows","6352b26b":"df_train.info()   # Give informatin about features","d9a1b0db":"df_train.isnull().sum()  # Check null values","e58464dc":"df_train.describe()  #statistical informations","6288dbd3":"df_train.corr()  # Check colleration with features","5e9bf3d5":"categorical_features = [\"Survived\",\"Sex\",\"Pclass\",\"Embarked\",\"SibSp\", \"Parch\"]\n\nfor each in categorical_features:\n    plt.figure(figsize = (6,4))\n    feature = df_train[each]\n    count_variable = feature.value_counts()\n    plt.bar(count_variable.index, count_variable)\n    plt.ylabel(\"Total Count Variable\")\n    plt.title(each)\n    plt.show()","6dbcbc84":"n\u00fcmerical_features = [\"Fare\", \"Age\"]\n\nfor each in n\u00fcmerical_features:\n    plt.figure(figsize = (6,4))\n    feature = df_train[each]\n    count_variable = feature.value_counts()\n    plt.bar(count_variable.index, count_variable)\n    plt.ylabel(\"Total Count Variable\")\n    plt.title(each)\n    plt.show()","cc8148fb":"sns.distplot(df_train['Age']);","758eebff":"sns.distplot(df_train['Fare']);","7be361a4":"sns.pairplot(df_train)    # See all plots amongst all features","261caefc":"fig, ax = plt.subplots(figsize=(12,7))\nsns.heatmap(df_train.corr(), annot=True, linewidths = .5, ax = ax);","a9b0f13c":"features = [\"Age\",\"SibSp\",\"Parch\",\"Fare\"]","34545155":"outlier_indices = []\nfor each in features:\n    Q1 = np.percentile(df_train[each],25)    # 1st quartile\n    Q3 = np.percentile(df_train[each],75)   # 3rd quartile\n    IQR = Q3 - Q1                           # IQR\n    outlier_step = IQR * 1.5                # outlier steps formula\n    outlier_list_col = df_train[(df_train[each] < Q1 - outlier_step) | (df_train[each] > Q3 + outlier_step)].index  # detect outlier and their indeces\n    outlier_indices.extend(outlier_list_col)","b977117e":"from collections import Counter\n\noutlier_indices = Counter(outlier_indices)\nmultiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)","6db473f2":"multiple_outliers    # These are outliers' indices","a659091c":"def detect_outliers(df,features):        # need dataframe and features\n    outlier_indices = []\n    \n    for each in features:\n        Q1 = np.percentile(df[each],25)\n        Q3 = np.percentile(df[each],75)\n        IQR = Q3 - Q1\n        outlier_step = IQR * 1.5\n        outlier_list_col = df[(df[each] < Q1 - outlier_step) | (df[each] > Q3 + outlier_step)].index\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","2f9e50b9":"df_train.loc[detect_outliers(df_train,features)]","843be92a":"df_train = df_train.drop(detect_outliers(df_train,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]),axis = 0).reset_index(drop = True)\ndf_train.head(10)","35ab344f":"df_train.isnull().sum()    # Again we should check null values","8d95dde0":"df_train[df_train[\"Embarked\"].isnull()]","e0e9db92":"df_train[\"Embarked\"] = df_train[\"Embarked\"].fillna(\"C\")   # Fill embarked feature","022eadab":"df_train[df_train[\"Embarked\"].isnull()]   # Again check this feature","6f3b5313":"df_train[df_train[\"Age\"].isnull()].head(10)    # Check another feature","98ae376b":"sns.factorplot(x = \"Parch\", y = \"Age\", data = df_train, kind = \"box\")\nsns.factorplot(x = \"SibSp\", y = \"Age\", data = df_train, kind = \"box\")\nplt.show()","a895a3b2":"index_nan_age = list(df_train[\"Age\"][df_train[\"Age\"].isnull()].index)\nfor i in index_nan_age:\n    age_pred = df_train[\"Age\"][((df_train[\"SibSp\"] == df_train.iloc[i][\"SibSp\"]) &(df_train[\"Parch\"] == df_train.iloc[i][\"Parch\"])& (df_train[\"Pclass\"] == df_train.iloc[i][\"Pclass\"]))].median()\n    age_med = df_train[\"Age\"].median()\n    if not np.isnan(age_pred):\n        df_train[\"Age\"].iloc[i] = age_pred\n    else:\n        df_train[\"Age\"].iloc[i] = age_med","d8f28f3c":"df_train[df_train[\"Age\"].isnull()]   # Check this feature","258fa15b":"sex_variable = []\nfor each in df_train[\"Sex\"]:\n    if each == 'male':\n        i = 1\n        sex_variable.append(i)\n    else:\n        i = 0\n        sex_variable.append(i)\n        \ndf_train[\"Sex\"] = sex_variable    ","a36136ff":"g = sns.factorplot(x = \"SibSp\", y = \"Survived\", data = df_train, kind = \"bar\", size = 5)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","50c42b4b":"g = sns.factorplot(x = \"Parch\", y = \"Survived\", kind = \"bar\", data = df_train, size = 5)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","6d161cc6":"g = sns.FacetGrid(df_train, col = \"Survived\")\ng.map(sns.distplot, \"Age\", bins = 15)\nplt.show()","bbd796cc":"g = sns.FacetGrid(df_train, col = \"Survived\", row = \"Pclass\", size = 4)\ng.map(plt.hist, \"Age\", bins = 15)\ng.add_legend()\nplt.show()","0672b8be":"df_train.head()","b8c06882":"name = df_train[\"Name\"]\n\npdlist = []\n\nfor i in name:\n    name_change = i.split(\".\")[0].split(\",\")[-1].strip()\n    pdlist.append(name_change)\n    \ndf_train[\"Title\"] = pdlist","1dcfb0e1":"df_train[\"Title\"] = df_train[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"],\"other\")\n\ntitle_variables = []\n\nfor i in df_train[\"Title\"]:\n    if i == \"Master\":\n        i = 0\n    elif i == \"Miss\" or i == \"Ms\" or i == \"Mlle\" or i == \"Mrs\":\n        i == 1\n    \n    elif i == \"Mr\":\n        i == 2 \n    \n    else:\n        i == 3\n        \ndf_train[\"Title\"] = title_variables","a155b3d1":"df_train[\"Fsize\"] = df_train[\"SibSp\"] + df_train[\"Parch\"] + 1\n\nfor i in df_train[\"Fsize\"]:\n    if i < 5:\n        i = 1\n    else:\n        i = 0","81c733e0":"sns.countplot(x = \"family_size\", data = df_train)\nplt.show()","b9804cb2":"df_train.head(10)   # Check dataset","e5d2f087":"df_train = pd.get_dummies(df_train, columns= [\"family_size\"])\ndf_train.head()","6a508313":"sns.countplot(x = \"Embarked\", data = df_train)\nplt.show()","6d94555c":"df_train = pd.get_dummies(df_train, columns=[\"Embarked\"])\ndf_train.head(10)  # Check dataset","3391d4ca":"tickets = []\nfor i in list(df_train.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"x\")\n        \ndf_train[\"Ticket\"] = tickets","b74c0aa2":"df_train = pd.get_dummies(df_train, columns= [\"Ticket\"], prefix = \"T\")\ndf_train.head()","3f459d1d":"sns.countplot(x = \"Pclass\", data = df_train)\nplt.show()","edb4c63d":"df_train[\"Pclass\"] = df_train[\"Pclass\"].astype(\"category\")\ndf_train = pd.get_dummies(df_train, columns= [\"Pclass\"])\n\ndf_train[\"Sex\"] = df_train[\"Sex\"].astype(\"category\")\ndf_train = pd.get_dummies(df_train, columns=[\"Sex\"])","80aa6a92":"df_train.drop(labels = [\"PassengerId\", \"Cabin\"], axis = 1, inplace = True)","27a06308":"df_train.head(10)","798525b8":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","207c81b4":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df_train.drop('Survived',axis=1), \n                                                    df_train['Survived'], test_size=0.30, \n                                                    random_state=42)","7903b1d2":"print(\"X_train\",len(X_train))\nprint(\"X_test\",len(X_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))","f8b43394":"from sklearn.linear_model import LogisticRegression\nLR  = LogisticRegression()\nLR.fit(X_train, y_train)\n\nprint(\"Train Datasets Accuracy: {}\".format(LR.score(X_train, y_train)))\nprint(\"Test Datasets Accuracy: {}\".format(LR.score(X_test, y_test)))","6623d838":"from sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier(n_estimators = 300,max_depth = 5,min_samples_split = 3)\nRFC.fit(X_train,y_train)\n\nprint(\"Train Datasets Accuracy: {}\".format(RFC.score(X_train,y_train)))\nprint(\"Test Datasets Accuracy: {}\".format(RFC.score(X_test,y_test)))","28ad0b29":"RFC = RandomForestClassifier(n_estimators = 300 ,max_depth = 5, min_samples_split = 10)\nRFC.fit(X_train,y_train)\n\nprint(\"Train Datasets Accuracy: {}\".format(RFC.score(X_train,y_train)))\nprint(\"Test Datasets Accuracy: {}\".format(RFC.score(X_test,y_test)))","e73cc5c2":"RFC = RandomForestClassifier(n_estimators = 500 ,max_depth = 10 , min_samples_split = 6)\nRFC.fit(X_train,y_train)\n\nprint(\"Train Datasets Accuracy: {}\".format(RFC.score(X_train,y_train)))\nprint(\"Test Datasets Accuracy: {}\".format(RFC.score(X_test,y_test)))","23695194":"from sklearn.neighbors import KNeighborsClassifier\nn_neighbors = 30  # as an example\n\nknn = KNeighborsClassifier(n_neighbors = n_neighbors) # n_neighbors = k\n\nknn.fit(X_train,y_train)\n\nprediction = knn.predict(X_test)\nprint(\" {} nn score: {} \".format(n_neighbors, knn.score(X_test,y_test)))","019382d5":"score_list = []\n\nfor each in range(10,40):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(X_train,y_train)\n    score_list.append(knn2.score(X_test,y_test))\n    \nplt.plot(range(10,40),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","9b2f6aa4":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\n\nprint(\"accuracy of decision tree: \", dt.score(X_test,y_test))","ab57bd79":"from lightgbm import LGBMClassifier\n\nLGB = LGBMClassifier(learning_rate = 0.01,max_depth = 5, num_leaves = 3)\nLGB.fit(X_train,y_train)\n\nprint(\"Train Datasets Accuracy: {}\".format(LGB.score(X_train,y_train)))\nprint(\"Test Datasets Accuracy: {}\".format(LGB.score(X_test,y_test)))","7ff41872":"### Train - Test Split","f7f07d10":"### KNN","bc105c9e":"## EDA","8c9ac6e3":"### We prepare another categorical feature to n\u00fcmerical feature","f5e7204e":" ### Feature Analysis and Corralation between two feature to Understand dataset","5eb2f621":"### Fill Age Feature","1ee1cc7e":"### Removing outliers","c720b29b":"### Outliers values are contrary to the distribution in the dataset. Therefore, its detection and removal from the dataset will increase the success rate of the model.","709c0e64":"### LGBM","e19fdd6b":"### Apply and check dataset within multiple_outliers","57a658af":"### Logistic Regression with SKLearn","990473d0":"### Finding missing value","43ce9772":"### Decision Tree","c271dfa3":"### In order to apply this process to its entire database, we need to convert it to a function.","8e651361":"### Feature Engineering \/ Get Dummies\n\n#####  Categorical features --> N\u00fcmerical Features to prepare models","b1378176":"### Random Forest"}}