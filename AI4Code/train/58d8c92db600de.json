{"cell_type":{"fd2758d9":"code","2c19117b":"code","417bf7f7":"code","780e1f43":"code","ebee471f":"code","52aa2321":"code","dc0416a5":"code","e474eb46":"code","448fdb6f":"code","925702dd":"markdown","3234aae1":"markdown","fd0c8084":"markdown","46c3afa3":"markdown","22f2ac77":"markdown","facf6484":"markdown","d8e31a34":"markdown","7a44698d":"markdown","4dee6b5b":"markdown","c60d24c1":"markdown","6032352b":"markdown"},"source":{"fd2758d9":"import cv2\nimport gc\nimport io\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nfrom tqdm import tqdm_notebook as tqdm\nimport zipfile\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2c19117b":"# To support Multiprocessing\nimport multiprocessing","417bf7f7":"HEIGHT = 137\nWIDTH = 236\nSIZE = 128\n\nTRAIN = ['\/kaggle\/input\/bengaliai-cv19\/train_image_data_0.parquet',\n         '\/kaggle\/input\/bengaliai-cv19\/train_image_data_1.parquet',\n         '\/kaggle\/input\/bengaliai-cv19\/train_image_data_2.parquet',\n         '\/kaggle\/input\/bengaliai-cv19\/train_image_data_3.parquet']","780e1f43":"def bbox(img):\n    rows = np.any(img, axis=1)\n    cols = np.any(img, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n    \n    return rmin, rmax, cmin, cmax\n\ndef crop_resize(img0, size = SIZE, pad = 16):\n    # Crop a box around pixels large than the threshold \n    # Some images contain line at the sides\n    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 80)\n    \n    # Cropping may cut too much, so we need to add it back\n    xmin = xmin - 13 if (xmin > 13) else 0\n    ymin = ymin - 10 if (ymin > 10) else 0\n    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n    img = img0[ymin:ymax,xmin:xmax]\n    \n    # Remove low intensity pixels as noise\n    img[img < 28] = 0\n    lx, ly = xmax-xmin,ymax-ymin\n    l = max(lx,ly) + pad\n    \n    # Make sure that the aspect ratio is kept in rescaling\n    img = np.pad(img, [((l-ly)\/\/2,), ((l-lx)\/\/2,)], mode = 'constant')\n    \n    return cv2.resize(img, (size, size))","ebee471f":"# Start timer\nstart_time = time.time()\n\nwith zipfile.ZipFile('train_single.zip', 'w') as img_out:\n    for fname in TRAIN:\n        # Read parquet file into pandas.\n        df = pd.read_parquet(fname)\n        \n        # The input inverted\n        data = 255 - df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH).astype(np.uint8)\n        for idx in tqdm(range(len(df))):\n            name = df.iloc[idx,0]\n            \n            # Normalize each image by its max val\n            img = (data[idx]*(255.0\/data[idx].max())).astype(np.uint8)\n            img = crop_resize(img)\n        \n            img = cv2.imencode('.png',img)[1]\n            img_out.writestr(name + '.png', img)\n            \n# Total time\nprint('Processing time standard: {0} [sec]'.format(time.time() - start_time))","52aa2321":"def process_image_multi_v1(data, name):\n    # Reshape data\n    data = data.reshape(HEIGHT, WIDTH)\n    \n    # Normalize each image by its max val\n    data = 255 - data.astype(np.uint8)\n    img = (data*(255.0\/data.max())).astype(np.uint8)\n    \n    # Crop, Resize and encode as PNG file\n    img = crop_resize(img)\n    img = cv2.imencode('.png',img)[1]\n    \n    return name, img","dc0416a5":"# CPU core count\ncpu_count = multiprocessing.cpu_count()\nprint(cpu_count)","e474eb46":"# Start Multi Processing\nstart_multi_time_v1 = time.time()\n\ntry:\n    # Setup multiprocessing pool\n    pool = multiprocessing.Pool(processes = cpu_count)\n\n    with zipfile.ZipFile('train_multi.zip', 'w') as img_out:\n        for fname in TRAIN:\n            # Read Parquet file\n            df = pd.read_parquet(fname)\n            \n            # Prep the input for pool.starmap.\n            data = df.iloc[:, 1:].values\n            names = df.image_id.tolist()\n\n            for name, img in pool.starmap(process_image_multi_v1, zip(data, names)):\n                img_out.writestr(name + '.png', img)\n\nfinally:\n    pool.close()\n    pool.join()\n\n# Total time\nprint('Processing time multi v1: {0} [sec]'.format(time.time() - start_multi_time_v1))","448fdb6f":"def process_image_multi_v2(data, name):\n    # Reshape data\n    data = data.reshape(HEIGHT, WIDTH)\n    \n    # Normalize each image by its max val\n    data = 255 - data.astype(np.uint8)\n    img = (data*(255.0\/data.max())).astype(np.uint8)\n    \n    # Crop, Resize and encode as PNG file\n    img = crop_resize(img)\n    img = cv2.imencode('.png',img)[1]\n    \n    # Save image\n    cv2.imwrite('.\/subdir\/' + name + '.png', img)\n    \n# Start Multi Processing\nstart_multi_time_v2 = time.time()\n\ntry:\n    pool = multiprocessing.Pool(processes = cpu_count)\n\n    # Process Images Standard.\n    for fname in TRAIN:\n        # Read Parquet file\n        df = pd.read_parquet(fname)\n\n        # Invert the input\n        data = df.iloc[:, 1:].values\n        names = df.image_id.tolist()\n        \n        # Multi Process Images\n        pool.starmap(process_image_multi_v2, zip(data, names))\n\nfinally:\n    pool.close()\n    pool.join()\n\n# Total time\nprint('Processing time multi v2: {0} [sec]'.format(time.time() - start_multi_time_v2))","925702dd":"# Single process\nFirst lets run the preprocessing in the standard single process way and see what the processing time is.","3234aae1":"As you can see we achieved a nice performance gain by cutting down the total processing times. May'be you would expect more in performance gain. However we have to take into account the time required for the loading of the train input files. This takes some amount of time.\n\nI hope I showed a little of the benefits of using multiprocessing. With some tweaking you can just replace the existing preprocessing code and put in your own code. \n\nIf you have any questions or remarks I'am always happy to help.","fd0c8084":"Next the image preprocessing code as mentioned from the referenced kernel.","46c3afa3":"We can see that looping over the training input files and processing each file separately takes roughlybetween 500-600 seconds. Note that this can vary slightly with each run.","22f2ac77":"We can see that there is a significant increase in performance by cutting down the required processing time because all images are processed in parallel.","facf6484":"# Multiprocess Version 1\nLets explore a first version of multiprocessing. Here we will loop over the train input files and process all the images in parallel processes. As an end result all the processed images will be added to the final zip file.\n\nFor setting up an effective multiprocessing code it is also import to take into account how much CPU cores are available, whats the available memory and also the disksubsystem. As a general guideline you can take the number of CPU cores as the number of processes that can run in parallel. If the CPU supports HyperTreading then you might get a small benefit by doubling the number of processes. However it does not always work. Some experimentation could be required ;-)\n\nTo use multiprocessing I define a function 'process_image_multi_v1'. It performs the complete processing for 1 image in combination with the earlier defined preprocessing functions.","d8e31a34":"Verify and get the CPU count. We will use the cpu_count to specify the number of parallel processes.","7a44698d":"Next we setup the multiprocessing pool. With pool.starmap we can send multiple iterable variables to the function 'process_image_multi_v1'. For each process that returns a processed image the name and image are used to write them to the output zip file.","4dee6b5b":"# Description\n\nIn a lot of kernels I see that when doing preprocessing or postprocessing we use regular single proces way of coding. A lot of times it takes only a few extra lines of code to implement multiprocessing and get a nice performance boost.\n\nWhen we do deeplearning we want it to run as fast as possible with parallel processes on a GPU. So why not also use parallel processes for the pre- or postprocessing parts that we implement.\n\nTo show you that it is quitte easy I will use the preprocessing code as was written in this very nice [kernel](https:\/\/www.kaggle.com\/iafoss\/image-preprocessing-128x128). We will take a look at the performance difference when running it with multiprocessing.\n\nNote that you should be able to just modify this kernel to run your own pre- or postprocessing code.\n\nAnd if you like the kernel then I would appreciate an upvote.","c60d24c1":"# Multiprocess Version 2\nLets also explore a second option. This time we won't write to the final output .zip file but we will write each image directly to an output directory. This might save a little overhead for each process.","6032352b":"To implement support for multi-processing we need to import the multiprocessing module."}}