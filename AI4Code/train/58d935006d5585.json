{"cell_type":{"c5d6bd3f":"code","eefed330":"code","67514f9c":"code","1351bb29":"code","f936c0b8":"code","859033f0":"code","5c367403":"code","bade49cf":"code","977cbd2f":"code","019a6c81":"code","0c607080":"code","39798942":"code","4a97614c":"code","4334f175":"code","3dd49ddd":"code","e1dce2b4":"code","af4f86e6":"code","98cad06b":"code","0436384d":"code","9f159e5a":"markdown","7cc52539":"markdown","1d15054e":"markdown","f80cada7":"markdown","32165c89":"markdown","fc9d4f28":"markdown","ab03a306":"markdown"},"source":{"c5d6bd3f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport pandas as pd \nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport matplotlib.pyplot as plt\nimport pathlib\nimport os\nimport statsmodels.api as sm\nimport plotly.graph_objects as go\nfrom statsmodels.tsa.stattools import adfuller\n\nimport numpy as np\nfrom numpy import array\nfrom numpy import split\n\nfrom math import floor\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport requests\n\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping","eefed330":"# Get Data via API Request\ndemand_request = requests.get('http:\/\/api.eia.gov\/series\/?api_key=77ef7b650dfa286336c87c0a22dc9d70&series_id=EBA.US48-ALL.D.H')\nassert(demand_request.status_code == 200)","67514f9c":"# Format JSON data \ndemand_df = pd.DataFrame(demand_request.json()['series'][0]['data'])\ndemand_df[0] = pd.to_datetime(demand_df[0])\ndemand_df = demand_df.rename(columns={0: 'datetime', 1: 'value'})\ndemand_df['units'] = 'MWh'\n\n\n# Inspect the Data\nprint(\"US (Lower 48) Electricity Demand DataFrame\")\nprint(demand_df.head())\nprint('=' * 60 + '\\n')\nprint('DataFrame Info')\nprint(demand_df.info())\nprint('=' * 60 + '\\n')\nprint(\"Descrintive Statistics\")\nprint(demand_df.describe())\nprint('=' * 60 + '\\n')\nprint(\"Number of Missing Datapoints\")\nprint(demand_df.isnull().sum().sum())\n","1351bb29":"# Seasonally Decompose the data \nX = demand_df.set_index('datetime')\nX = X.sort_values(by='datetime')\nX = X.drop('units', axis=1)\nresults = seasonal_decompose(X.value, model='additive', period=24*365)\nresults.plot()\nplt.show()","f936c0b8":"# Plot Multiple Moving Averages\nfig = go.Figure()\n\n# set index to datetime to use resample\n\nX_daily_avg = X.resample('D').mean()\nX_weekly_avg = X.resample('W').mean()\nX_montly_avg = X.resample('M').mean()\nX_yearly_avg = X.resample('Y').mean()[:-1] \n    # exclude 2020 because it will produce skewed yearly average\n\n    \n# Add traces\nfig.add_trace(go.Scatter(x=X.index, y=X.value,\n                    mode='lines',\n                    name='hourly energy demand'))\nfig.add_trace(go.Scatter(x=X_daily_avg.index, y=X_daily_avg.value,\n                    mode='lines',\n                    name='daily average'))\nfig.add_trace(go.Scatter(x=X_weekly_avg.index, y=X_weekly_avg.value,\n                    mode='lines',\n                    name='weekly average'))\nfig.add_trace(go.Scatter(x=X_montly_avg.index, y=X_montly_avg.value,\n                    mode='lines',\n                    name='monthly average'))\nfig.add_trace(go.Scatter(x=X_yearly_avg.index, y=X_yearly_avg.value,\n                    mode='lines',\n                    name='yearly average'))\n\n\nfig.update_layout(title='United States Electricity Demand MWh (Lower 48)',\n                xaxis_title=\"Time\",\n                yaxis_title=\"Energy Demand (MWh)\",)\nfig.show()","859033f0":"# \nX_aug = X\nX_aug['weekday'] = X_aug.index.weekday\nX_aug['day_name'] = X_aug.index.day_name\nX_aug['month'] = X_aug.index.month\nX_aug['day'] = X_aug.index.day\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=X_aug.groupby('weekday')['value'].mean().index, y=X_aug.groupby('weekday')['value'].mean(),\n                    mode='lines',\n                    name='yearly average'))\n\n\nfig.update_layout(title='United States Electricity Demand MWh (Lower 48)',\n                xaxis_title=\"Week Day, (Monday=0, Sunday=6)\",\n                yaxis_title=\"Average Energy Demand (MWh)\",)\nfig.show()\n\nfig = go.Figure()\n\nday_dict = {\n    0: 'Monday',\n    1: 'Tuesday',\n    2: 'Wednessday',\n    3: 'Thursday',\n    4: 'Friday',\n    5: 'Saturday',\n    6: 'Sunday'\n}\n\nfig.add_trace(go.Scatter(x=X_aug.groupby('month')['value'].mean().index, y=X_aug.groupby('month')['value'].mean(),\n                    mode='lines',\n                    name='yearly average'))\n\n\nfig.update_layout(title='United States Electricity Demand MWh (Lower 48)',\n                xaxis_title='Month',\n                yaxis_title=\"Average Energy Demand (MWh)\",\n                )\nfig.show()\n\n\n","5c367403":"# Basic Test and Train Split\n\ndata_train = X['2016':'2018']['value']\ndata_test = X['2019']['value']\n\nprint(data_train.shape)\nprint(data_test.shape)\n","bade49cf":"\n# Since LSTMs store long term memory state, we create data data stucture with (A) \n# Time steps and (B) outputs\n\nX_train = []\ny_train = []\n\ninput_time = 24 * 30\npred_time = 24\n\n\n# use previous 24 hours to predict the next 24 hours\nfor i in range(input_time, len(data_train) - input_time):\n    X_train.append(data_train[i-input_time:i])\n    y_train.append(data_train[i:i+pred_time])\n\nX_train, y_train = np.array(X_train), np.array(y_train)\n\nX_train.shape, y_train.shape\n\n# pd.DataFrame(y_train).head()","977cbd2f":"#Normalising the dataset between 0 and 1\n\nx_scaler = MinMaxScaler()\nX_train = x_scaler.fit_transform(X_train)","019a6c81":"#Normalising the dataset\n\ny_scaler = MinMaxScaler()\ny_train = y_scaler.fit_transform(y_train)","0c607080":"pd.DataFrame(X_train).head()","39798942":"X_train = X_train.reshape(24864, 720, 1)\nX_train.shape\n\nnum_x_signals = X_train.shape[1]\nnum_x_signals\n\n","4a97614c":"# prepare test data the same way\ndata_test = np.array(data_test)\nX_test, y_test = [], []\n\n\n# use previous 24 hours to predict the next 24 hours\nfor i in range(input_time, len(data_test) - input_time):\n    X_test.append(data_test[i-input_time:i])\n    y_test.append(data_test[i:i+pred_time])\n    \nX_test, y_test = np.array(X_test), np.array(y_test)\n\nX_test = x_scaler.transform(X_test)\ny_test = y_scaler.transform(y_test)\n\nX_test = X_test.reshape(7320, 720, 1)\nX_test.shape","4334f175":"# The LSTM architecture\nregressor = Sequential()\n# First LSTM layer with Dropout regularisation\nregressor.add(LSTM(units=200, return_sequences=True, input_shape=(X_train.shape[1],1)))\nregressor.add(Dropout(0.2))\n# Second LSTM layer\nregressor.add(LSTM(units=50, return_sequences=True))\nregressor.add(Dropout(0.2))\n# Third LSTM layer\nregressor.add(LSTM(units=50, return_sequences=True))\nregressor.add(Dropout(0.2))\n# Fourth LSTM layer\nregressor.add(LSTM(units=50))\nregressor.add(Dropout(0.2))\n# The output layer\nregressor.add(Dense(units=24))\n\n# implement early stopping callback in order to prevent \n # overfitting\nearly_stopping = EarlyStopping(\n    monitor='loss',\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=10, # how many epochs to wait before stopping\n    restore_best_weights=True,\n    verbose=True,\n)\n\n# Compiling the RNN\nregressor.compile(\n                  optimizer='rmsprop',\n                  loss='mean_squared_error',\n                 )\n# Fitting to the training set\nhistory = regressor.fit(X_train,y_train,epochs=50,\n                       callbacks=[early_stopping],)","3dd49ddd":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, 'loss'].plot();\n\n# # Export and Save the Model for Productionalization\n# regressor.save('.\/model')\n","e1dce2b4":"y_predict = regressor.predict(X_test)\n\ny_true = y_scaler.inverse_transform(y_test)\ny_pred = y_scaler.inverse_transform(y_predict)\n\nprint('Predicted Demand')\nprint(y_true[-1])\nprint('=' * 60 +'\\n')\nprint('Actual Demand')\nprint(y_pred[-1])\n\n\n","af4f86e6":"assert(len(y_test) == len(y_pred) )\n\n\n\n# get_accuracy_metrics(predicted_demand, actual_demand, name='LSTM')\n\n\nactual_df = pd.DataFrame(y_true)\npredicted_df = pd.DataFrame(y_pred)\n\n\nactual_df.head()\n\n# Export 2019 files for productionalization\n# actual_df.to_csv('.\/real_2019_daily_demand.csv')\n# predicted_df.to_csv('.\/pred_2019_daily_demand.csv')","98cad06b":"def evaluate_model(y_true, y_predicted):\n    scores = []\n    \n    #calculate scores for each day\n    for i in range(y_true.shape[1]):\n        mse = mean_squared_error(y_true[:, i], y_predicted[:, i])\n        rmse = np.sqrt(mse)\n        scores.append(rmse)\n    \n    #calculate score for whole prediction\n    total_score = 0\n    for row in range(y_true.shape[0]):\n        for col in range(y_predicted.shape[1]):\n            total_score = total_score + (y_true[row, col] - y_predicted[row, col])**2\n    total_score = np.sqrt(total_score\/(y_true.shape[0]*y_predicted.shape[1]))\n    \n    return total_score, scores\n\nscore, scores = evaluate_model(y_true, y_pred)\nassert (score < np.std(y_true[0]))\n\nprint(\"Total Mean Square Error:\")\nprint(score)","0436384d":"\n\n# Plot the prediction\nfig = go.Figure()\n\nfor step in range(1, 335):\n    fig.add_trace(go.Scatter(x=np.array(range(0, 23)), y=actual_df.iloc[step] ,\n                        visible=False,\n                        mode='lines',\n                        name='Actual'))\n\n    fig.add_trace(go.Scatter(x=np.array(range(0, 23)),y=predicted_df.iloc[step] ,\n                        visible=False,\n                        mode='lines',\n                        name='LSTM Prediction'))\n\nfig.data[315].visible = True\nfig.data[316].visible = True\n\n# Create and add slider\nsteps = []\nfor i in range(365):\n    step = dict(\n        method=\"update\",\n        args=[{\"visible\": [False] * len(fig.data)},\n              {\"title\": \"Day: \" + str(i)}],  # layout attribute\n    )\n    if i % 2 == 0:\n        step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n        step[\"args\"][0][\"visible\"][i-1] = True  # Toggle i'th trace to \"visible\"\n        steps.append(step)\n    else:\n        step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n        step[\"args\"][0][\"visible\"][i+1] = True  # Toggle i'th trace to \"visible\"\n        steps.append(step)\n\nsliders = [dict(\n    active=69,\n    currentvalue={\"prefix\": \"Day: \"},\n    pad={\"t\": 50},\n    steps=steps\n)]\n\nfig.update_layout(\n    sliders=sliders,\n    title='Daily United States Electricity Demand Actual vs Prediction (Lower 48)',\n                xaxis_title=\"Hour \",\n                yaxis_title=\"Energy Demand (MWh)\",\n)\n\nfig.show()\n\n","9f159e5a":"# Interactive Representation of the Results","7cc52539":"# Results \nAfter analyzing the testing data we see that the total root mean squared error is 14057. As a sanity check, we see that our model's root mean squared error is less than our standard deviation. Finally, we interactively plot all of our test data which encapsulates the entire year of 2019. Since our forecast looks ahead 24 hours at a time we will be displaying it as such. However, we don't want to exclusively look at only one day at a time and therefore we will be adding a slider so that we can slide around the year and view how our prediction does throughout the year.\n\n","1d15054e":"# The Dataset\nThe data which will be used is provided by United States Energy Information Administration (EIA). The data was pulled down via an API request. The dataset contains data going back to July 2016 and is continuosly updated every hour with the current electricity demand. Additionally, we see that this is an extremely apealing dataset to work with as it has no missing values and requires very little data cleaning.  ","f80cada7":"# Predicting Hourly Electricity Demand in the US (L48)\n\n# Introduction: \nKnowing how much power is being demanded at any one point in time is essential to a utility maintaining grid reliability in a cost effective way. The grid is the greatest real time system. Electricity supply always has to equal electricity demand. If supply is less than demand, a the grid will loose stability and blackouts will occur. If supply is greater than demand, to maintain grid stability, the excess power is shorted to ground and then your company goes bankrupt because you are wasting all your product.\n\nThe comprehensive goal of this project is to create a forecast which looks 24 hours into the future and predicts the hourly electricity demand. As the use of electricty arises from complex interactions from multiple factors from meterorological to socialeconomic factors, I will be utilizing a Deep Learning Model to accomplish this. While exoginous variables such as weather, weekends\/holidays, natural disasters, ext. would be features to include if a fully fledged productionized model, I will take the myoptic view and use univariate data (previous demand) to predict future demand as this is the first time I have used time series data as well as the first time I have used a deep learning model. However, it should be important to note that the model which I am implementing can very easily be modified to incorperated these exogenous factors.","32165c89":"# LSTM Model\n   In order to forecast US Energy Demand we use a Long Short Term Memory Recurrant Neural Network (LSTM). For simplicity we will be taking the myoptic stance and look at the data as a univariate sequence. In order to do this we make the the previous 24 hours the input to our model and the next 24 hours will be our target. Before feeding our data into the model we need to prepare it and scale it. We utilize a sklearn object in order to do this. \n   \n### Methodology \n\n    (first 30 days) -> (day 31) \n    (30 days) -> (day 32) \n    (30 days) -> (day 33) \n     ext.. ","fc9d4f28":"# Preliminary Analysis\nIn this section, the primary goal will to identify multiple trends on multiple timeframes in order. This will be done utilizing multiple methods. This is a typical starting point for time series data. As we can see there is an underlying annual trend as well a heavily seasonal influence","ab03a306":"The following diagram is another visual way to accomplish a seasonal decomponsition. The plotly library allows for the construction of interactive graphs. As such the user can click on the lines in the legend to choose which averages to include. Additionally, the user can zoom in and out in order dive more deeply into the data. "}}