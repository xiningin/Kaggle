{"cell_type":{"994da609":"code","dec20df1":"code","54740436":"code","1add5839":"code","954df7d9":"code","44452cd0":"code","2f1d130e":"code","98f8e773":"code","fa9cfeae":"code","b0ac87e6":"code","5cc61ae9":"code","e2cfe01b":"code","2ac046ea":"code","dde705a2":"code","b047b0d6":"code","3ac4808d":"code","d0febd9b":"code","0d6f3db4":"code","67239b04":"code","6a4875e4":"code","eb93a06b":"code","fca4b599":"code","50a4b72f":"code","395c3f5c":"code","b11c0fbf":"code","15bc5939":"code","5e4566b5":"code","0c65c4d9":"code","6c507ec6":"code","a6789bbd":"code","3640de72":"code","025c26e3":"code","28417ecc":"markdown","fd809258":"markdown","5771dc14":"markdown","7edc2c04":"markdown","5ec09262":"markdown","f3cb3170":"markdown","60a12603":"markdown","280d65eb":"markdown","700f1731":"markdown","e329ecd4":"markdown","8545006b":"markdown","22bb7464":"markdown","100b21e5":"markdown","9d260af3":"markdown","098bce68":"markdown","26466e50":"markdown","f0f5e23d":"markdown","7b56d8b1":"markdown","2d4ad7e3":"markdown","686e567f":"markdown","13417ee3":"markdown","99398d8b":"markdown","82407a3a":"markdown","21cd7c53":"markdown","2f11e03b":"markdown","72a9eeea":"markdown","ccfab97a":"markdown","03f94f9b":"markdown","4e23a4f6":"markdown","d59540db":"markdown","b559dbc4":"markdown"},"source":{"994da609":"# Importing Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\n\n# K-means clustering libraries\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# hierarchical Clustering libraries\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree\n%matplotlib inline\n\npd.set_option(\"display.max_rows\", None,\"display.max_columns\", None)\n\nwarnings.simplefilter(action='ignore')\nplt.style.use('seaborn')","dec20df1":"# Importing data.csv\ndf = pd.read_csv('..\/input\/ecommerce-data\/data.csv', sep=\",\", encoding=\"ISO-8859-1\", header=0)\ndf.head()","54740436":"df.shape","1add5839":"# basics of the df\ndf.info()","954df7d9":"round(100*(df.isnull().sum())\/len(df), 2)","44452cd0":"df.dropna(inplace=True)","2f1d130e":"round(100*(df.isnull().sum())\/len(df), 2)","98f8e773":"df.shape","fa9cfeae":"df['CustomerID'] = df['CustomerID'].astype('int')\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'],infer_datetime_format=True)","b0ac87e6":"df['Amount'] = df['Quantity'] * df['UnitPrice']\ndf.head()","5cc61ae9":"df.head()","e2cfe01b":"# Calculating Monetary attribute\ncus_data = df.groupby('CustomerID')[['Amount']].sum() # Total amount spent\ncus_data.rename(columns={'Amount':'Monetary'},inplace=True)\ncus_data.head()","2ac046ea":"# Calculating frequency attribute\ncus_data['Frequency'] = df.groupby('CustomerID')['InvoiceNo'].count()\ncus_data.head()","dde705a2":"max_date = max(df['InvoiceDate'])\nmax_date","b047b0d6":"df['diff'] = max_date - df['InvoiceDate']\ndf.head()","3ac4808d":"import datetime as dt\n\ncus_data['Recency'] = df.groupby('CustomerID')['diff'].min().dt.days\ncus_data = cus_data.reset_index()\ncus_data.head()","d0febd9b":"num_features = cus_data.columns[1:]\nr = c = 0\nfig,ax = plt.subplots(2,2,figsize=(12,6))\n\nfor n,i in enumerate(num_features):\n    sns.boxplot(x=i, data=cus_data,ax=ax[r,c])\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","0d6f3db4":"h_cap = 0.95\nh_cap_val = cus_data['Monetary'].quantile(h_cap)\ncus_data['Monetary'][cus_data['Monetary'] > h_cap_val] = h_cap_val\nl_cap = 0.05\nl_cap_val = cus_data['Monetary'].quantile(l_cap)\ncus_data['Monetary'][cus_data['Monetary'] < l_cap_val] = l_cap_val","67239b04":"cap = 0.95\ncap_val = cus_data['Frequency'].quantile(cap)\ncus_data['Frequency'][cus_data['Frequency'] > cap_val] = cap_val","6a4875e4":"num_features = cus_data.columns[1:]\nr = c = 0\nfig,ax = plt.subplots(2,2,figsize=(12,6))\n\nfor n,i in enumerate(num_features):\n    sns.boxplot(x=i, data=cus_data,ax=ax[r,c])\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","eb93a06b":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\n\npreprocessor = Pipeline(\n        [\n            (\"scaler\", MinMaxScaler()),\n            (\"pca\", PCA(n_components=2, random_state=42)),\n        ]\n    )","fca4b599":"X = cus_data.drop('CustomerID',axis=1)\nX_scaled = pd.DataFrame(preprocessor.fit_transform(X),columns=['PC_1','PC_2'])","50a4b72f":"#Calculating the Hopkins statistic\nfrom sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","395c3f5c":"for i in range(5):\n  print('Hopkins statistic value is:',round(hopkins(X_scaled),3))","b11c0fbf":"# elbow-curve\/SSD\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(X_scaled)\n    ssd.append(kmeans.inertia_)\n\nplt.plot(range_n_clusters,ssd)\nplt.xlabel('Number of clusters(k)')\nplt.ylabel('SSD')\nplt.title('Elbow Curve')\nplt.show()","15bc5939":"# silhouette analysis\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(X_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, round(silhouette_avg,2)))","5e4566b5":"# final model with k=3\nkmeans = KMeans(n_clusters=3, max_iter=50,random_state=1)\nkmeans.fit(X_scaled)","0c65c4d9":"# Adding cluster labels to master dataframe\nX_scaled['cluster_id'] = kmeans.labels_\nX['cluster_id'] = kmeans.labels_","6c507ec6":"# Number of customers per cluster\n\nfont_title = {\n        'color':  '#104C6C',\n        'weight': 'normal',\n        'size': 16,\n        }\n\n\nfont_label = {\n        'color':  '#104C6C',\n        'weight': 'normal',\n        'size': 13,\n        }\nplt.figure(figsize=(8,6))\nax = X_scaled['cluster_id'].value_counts().plot(kind='bar')\nax.set_xticklabels(['Promising','Slipping','Loyal'])\nax.set_ylabel('Number of Customers',font_label)\nax.set_xlabel('Clusters',font_label)\nax.set_title(\"Customer Distribution\",font_title)\nplt.show()","a6789bbd":"# Visualizing Numerical columns using Boxplots\ncols = X.columns[0:-1].tolist()\nr = c = 0\nfig,ax = plt.subplots(2,2,figsize=(15,25))\n\nfor n,i in enumerate(cols):\n    sns.boxplot(x='cluster_id',y=cols[n], data=X,ax=ax[r,c])\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","3640de72":"fig = plt.figure(figsize=[20,8])\n\nplt.subplot(1,3,1)\nsns.scatterplot(data=X,x=\"Recency\",y=\"Frequency\",hue=\"cluster_id\",size=\"cluster_id\",palette=\"Set1\")\nplt.subplot(1,3,2)\n'''sns.scatterplot(data=X,x=\"Frequency\",y=\"Monetary\",hue=\"cluster_id\",size=\"cluster_id\",palette=\"Set1\")\nplt.subplot(1,3,3)'''\nsns.scatterplot(data=X,x=\"Monetary\",y=\"Recency\",hue=\"cluster_id\",size=\"cluster_id\",palette=\"Set1\")\nplt.show()","025c26e3":"# Vusializing clusters using Principle Components\nfig = plt.figure(figsize=[20,8])\n\nsns.scatterplot(data=X_scaled,x=\"PC_1\",y=\"PC_2\",hue=\"cluster_id\",size=\"cluster_id\",palette=\"Set1\")\nplt.show()","28417ecc":"# **2. Data preprocessing**","fd809258":"## **Hopkins Test**","5771dc14":"# **3. Data preparation for modeling**","7edc2c04":"Since the Hopkins test value hovers around 0.95, therefore given data have high clustering tendency.","5ec09262":"Using silhouette analysis k=3 seems to be the optimal number of clusters.","f3cb3170":"**Frequency -** Total number of transactions made by customer or average time between transactions.","60a12603":"As customer segmentation would not be possible without Customer ID, so records with missing customer ID have been dropped.","280d65eb":"## **Formatting 'CustomerID' and 'InvoiveDate'**","700f1731":"The Hopkins statistic, is a statistic which gives a value which indicates the cluster tendency, in other words: how well the data can be clustered.\n\n- If the value is between {0.01, ...,0.3}, the data is regularly spaced.\n\n- If the value is around 0.5, it is random.\n\n- If the value is between {0.7, ..., 0.99}, it has a high tendency to cluster.","e329ecd4":"### **Elbow method**","8545006b":"## **Generating 'Amount' column using 'Quantity' & 'UnitPrice'**","22bb7464":"**Recency -** Time since last order or engaged with product.","100b21e5":"Before we start with data modelling, we have to group transaction at customer level and calculate their R-F-M metrics.","9d260af3":"## **Calculating RFM metrics**","098bce68":"## **Creating pipeline for feature scaling & dimensionality reduction**","26466e50":"After segmenting customers into loyal, slipping, and promising it empowers businesses to run personalized, high-performing campaigns and preserves profit margin. Below are a few recommendations or targeted strategies for each customer segment:\n\n- **Loyal** - Loyalty programs are effective for these repeat visitors. Advocacy programs and reviews are also common X1X strategies. Lastly, consider rewarding these customers with Free Shipping or other like benefits.\n\n- **Promising** - Focus on increasing monetization through product recommendations based on past purchases and incentives tied to spending thresholds.\n\n- **Slipping** - Customers leave for a variety of reasons. Depending on your situation price deals, new product launches, or other retention strategies.","f0f5e23d":"## **b) Run K-means using optimal K**","7b56d8b1":"# **4. Cluster assignment using K-means clustering**","2d4ad7e3":"Based on the above box plots, it's visible that Monetary and Frequency attributes have outliers. Since clustering algorithms like K-means are susceptible to outliers we will be treating them by capping them at 95% quantile.","686e567f":"# **1. Overview and understanding of data**","13417ee3":"**Inference:**\n1. ***Cluster 0*** contains the customers who generate the least revenue and are not frequent, most likely because these were one-time customers. Hence they can be labeled as ***Slipping***. \n2. ***Cluster 1*** seems to have the most ***loyal*** customers, as they bring the most revenue and are often the most frequent customers. \n3. ***Cluster 2*** customers seem ***promising*** as it consists of frequent buyers, however revenue generation is not as high as Loyal customers. \n\n<H2>Cluster Labels:<\/h2>\n<H3>\nCluster 0 - Slipping<br>\nCluster 1 - Loyal<br>\nCluster 2 - Promising<br>\n<\/H3>","99398d8b":"**Monetary -** Total or average amount spent by the customer.\n","82407a3a":"# **5. Recommendations**","21cd7c53":"## **a) Finding Optimal value of K (Clusters)**\n\n- Elbow Method\n- Silhouette Analysis","2f11e03b":"## **Treating Outliers**","72a9eeea":"### What is clustering ?\nClustering is the process of grouping observations of similar kinds into smaller groups within the larger population. It has widespread application in business analytics. One of the questions facing businesses is how to organize the huge amounts of available data into meaningful structures.Or break a large heterogeneous population into smaller homogeneous groups. Cluster analysis is an exploratory data analysis tool which aims at sorting different objects into groups in a way that the degree of association between two objects is maximal if they belong to the same group and minimal otherwise.\n\n### When to use clustering?\n\n- *Clustering is primarily used to perform segmentation, be it customer, product or store. We have already talked about customer segmentation using cluster analysis in the example above. Similarly products can be clustered together into hierarchical groups based on their attributes like use, size, brand, flavor etc; stores with similar characteristics \u2013 similar sales, size, customer base etc, can be clustered together.*\n\n- Clustering can also be used for anomaly detection, for example, identifying fraud transactions. Cluster detection methods can be used on a sample containing only good transactions to determine the shape and size of the \u201cnormal\u201d cluster. When a transaction comes along that falls outside the cluster for any reason, it is suspect. This approach has been used in medicine to detect the presence of abnormal cells in tissue samples and in telecommunications to detect calling patterns indicative of fraud.\n\n- Clustering is often used to break large set of data into smaller groups that are more amenable to other techniques. For example, logistic regression results can be improved by performing it separately on smaller clusters that behave differently and may follow slightly different distributions.\n\nFor our business problem, we have to segment customers based on their behavoiural attrubutes. One of the most popular, easy-to-use, and effective segmentation methods to enable marketers to analyze customer behavior is RFM analysis.\n\n### What is RFM Analysis?\nThe  idea is to segment customers based on when their last purchase was, how often they\u2019ve purchased in the past, and how much they\u2019ve spent overall. All three of these measures have proven to be effective predictors of a customer's willingness to engage in marketing messages and offers.\n\n<img src=\"https:\/\/d35fo82fjcw0y8.cloudfront.net\/2018\/03\/01013508\/Incontent_image.png\">","ccfab97a":"# **Business Problem**[<a href=\"https:\/\/www.kaggle.com\/carrie1\/ecommerce-data\">src<\/a>]\n\n## **Description**\nAs a manager of the online store, you would want to group the customers into different clusters, so that you can make a customised marketing campaign for each of the group. You would have to decide what the important business criteria are on which you would want to segregate the customers.\nThis is a transnational data set which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\n## **Task**\n\n- Decide the important business criteria to segregate he customers\n- Group customers into clusters to run targeted campaigns\n\n## **Steps Involved :**\n\n1. Overview and understanding of data\n2. Data preprocessing\n3. Data preparation for modeling\n4. Cluster assignment using K-means clustering\n5. Recommendations\n\n**Refrences**\n- [5 Clustering algorithms data scientists need to know](https:\/\/towardsdatascience.com\/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n- [RFM Analysis](https:\/\/clevertap.com\/blog\/rfm-analysis\/)","03f94f9b":"### **Silhouette Analysis**","4e23a4f6":"## **Dropping records with missing Customer ID**","d59540db":"Based on above \"elbow\" curve, k=3 seems to be the optimal number of clusters.","b559dbc4":"## **c) Clustering profiling using R-F-M**"}}