{"cell_type":{"adf0a667":"code","805ea0c8":"code","97b1d1a4":"code","f18a08b5":"code","8a5764b3":"code","b78c2524":"code","909a7ff9":"code","0209f4a0":"code","cad9efb0":"code","6b29a6f3":"code","5f6075d1":"code","bcb7d2af":"code","bbdd80de":"code","6387a245":"code","6b7607fd":"code","22cce001":"code","71a4e6dc":"code","cd7fd093":"code","ce688eb8":"code","1ff50c1a":"code","36cf925b":"code","8a40809b":"code","170bc6c1":"code","b23d3c0d":"code","90961caf":"code","f0159eee":"code","2c58bcd6":"code","fea3fb57":"markdown","d9606728":"markdown","1b96f7d6":"markdown","a997b685":"markdown","20bc24ee":"markdown","9df490fe":"markdown","7ce5e6b5":"markdown","5d86ea9c":"markdown"},"source":{"adf0a667":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path","805ea0c8":"# Lets go ahead  and have a look at data\nDATA_PATH = \"..\/input\/santander-customer-transaction-prediction\/\"  \n\ntrain = pd.read_csv(str(Path(DATA_PATH) \/ \"train.csv\"))\ntest = pd.read_csv(str(Path(DATA_PATH) \/ \"test.csv\"))\n\nprint(\"Train and test shapes\", train.shape, test.shape)","97b1d1a4":"train.columns, test.columns","f18a08b5":"train.target.value_counts()","8a5764b3":"# https:\/\/www.listendata.com\/2015\/03\/weight-of-evidence-woe-and-information.html\ndef woe(X, y):\n    tmp = pd.DataFrame()\n    tmp[\"variable\"] = X\n    tmp[\"target\"] = y\n    var_counts = tmp.groupby(\"variable\")[\"target\"].count()\n    var_events = tmp.groupby(\"variable\")[\"target\"].sum()\n    var_nonevents = var_counts - var_events\n    tmp[\"var_counts\"] = tmp.variable.map(var_counts)\n    tmp[\"var_events\"] = tmp.variable.map(var_events)\n    tmp[\"var_nonevents\"] = tmp.variable.map(var_nonevents)\n    events = sum(tmp[\"target\"] == 1)\n    nonevents = sum(tmp[\"target\"] == 0)\n    tmp[\"woe\"] = np.log(((tmp[\"var_nonevents\"])\/nonevents)\/((tmp[\"var_events\"])\/events))\n    tmp[\"woe\"] = tmp[\"woe\"].replace(np.inf, 0).replace(-np.inf, 0)\n    tmp[\"iv\"] = (tmp[\"var_nonevents\"]\/nonevents - tmp[\"var_events\"]\/events) * tmp[\"woe\"]\n    iv = tmp.groupby(\"variable\")[\"iv\"].last().sum()\n    return tmp[\"woe\"], tmp[\"iv\"], iv","b78c2524":"iv_values = []\nfeats = [\"var_{}\".format(i) for i in range(200)]\ny = train[\"target\"]\nfor f in feats:\n    X = pd.qcut(train[f], 10, duplicates='drop')\n    _, _, iv = woe(X, y)\n    iv_values.append(iv)\n    \niv_inds = np.argsort(iv_values)[::-1][:50]\niv_values = np.array(iv_values)[iv_inds]\nfeats = np.array(feats)[iv_inds]\n","909a7ff9":"plt.figure(figsize=(10, 16))\nsns.barplot(y=feats, x=iv_values, orient='h')\nplt.show()","0209f4a0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler","cad9efb0":"feats = [\"var_{}\".format(i) for i in range(200)]\nX = train[feats]\nX_test = test[feats]\ny = train[\"target\"]\n\ncvlist = list(StratifiedKFold(5, random_state=12345786).split(X, y))\nscaler = StandardScaler()\n\nX_sc = scaler.fit_transform(X)\nX_test_sc = scaler.fit_transform(X_test)\n\nlr = LogisticRegression()\ny_preds_lr = cross_val_predict(lr, X_sc, y, cv=cvlist, method=\"predict_proba\")[:, 1]\n\nlr.fit(X_sc, y)\ny_test_preds_lr = lr.predict_proba(X_test_sc)[:, 1] \nroc_auc_score(y, y_preds_lr)","6b29a6f3":"sns.distplot(y_preds_lr)\nsns.distplot(y_test_preds_lr)\nplt.show()","5f6075d1":"import lightgbm as lgb\n#model = lgb.LGBMClassifier(n_estimators=2000, learning_rate=0.1, num_leaves=2, subsample=0.4, colsample_bytree=0.4)\n\n#y_preds_lgb = np.zeros((len(y)))\n#for i, (tr_idx, val_idx) in enumerate(cvlist):\n#    X_dev, y_dev = X.iloc[tr_idx], y.iloc[tr_idx]\n#    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n#    model.fit(X_dev, y_dev, eval_set=[(X_val, y_val)], eval_metric=\"auc\", verbose=50, early_stopping_rounds=200)\n#    val_preds = model.predict_proba(X_val)[:, 1]\n#    y_preds_lgb[val_idx] = val_preds\n#    print(\"Score for fold {} is {}\".format(i, roc_auc_score(y_val, val_preds)))\n    \n#print(\"Overall Score for oof predictions \", roc_auc_score(y, y_preds_lgb))","bcb7d2af":"#model = lgb.LGBMClassifier(n_estimators=1500, learning_rate=0.1, num_leaves=8, subsample=0.6, colsample_bytree=0.6)\n#model.fit(X, y)\n#y_test_preds_lgb = model.predict_proba(X_test)[:, 1]\n","bbdd80de":"#sns.distplot(y_preds)\n#sns.distplot(y_test_preds_lgb)","6387a245":"from scipy.stats import gmean","6b7607fd":"np.mean([0.9, 0.9, 0.9, 0.98, 0.9])","22cce001":"gmean([0.9, 0.9, 0.9, 0.98, 0.9])","71a4e6dc":"!pip install -U lightgbm","cd7fd093":"import lightgbm as lgb\nmodel = lgb.LGBMClassifier(boosting_type='gbdt', n_estimators=200000, learning_rate=0.02, num_leaves=2, subsample=0.4, colsample_bytree=0.4, seed=1)\n\ny_preds_lgb = np.zeros((len(y)))\ntest_preds_allfolds = []\nfor i, (tr_idx, val_idx) in enumerate(cvlist):\n    X_dev, y_dev = X.iloc[tr_idx], y.iloc[tr_idx]\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n    model.fit(X_dev, y_dev, eval_set=[(X_val, y_val)], eval_metric=\"auc\", verbose=5000, early_stopping_rounds=1000)\n    val_preds = model.predict_proba(X_val)[:, 1]\n    test_preds = model.predict_proba(X_test)[:, 1]\n    test_preds_allfolds.append(test_preds)\n    y_preds_lgb[val_idx] = val_preds\n    print(\"Score for fold {} is {}\".format(i, roc_auc_score(y_val, val_preds)))\n    # break\nprint(\"Overall Score for oof predictions \", roc_auc_score(y, y_preds_lgb))","ce688eb8":"y_test_preds_lgb = gmean(test_preds_allfolds, 0)\nsns.distplot(y_preds_lgb)\nsns.distplot(y_test_preds_lgb)","1ff50c1a":"sub = test[[\"ID_code\"]]\nsub[\"target\"] = y_test_preds_lgb\nsub.to_csv(\"submission_lgbm2_v1.csv\", index=False)","36cf925b":"weighted_preds = y_preds_lr* 0.05 + y_preds_lgb * 0.95\nweighted_test_preds = y_test_preds_lr* 0.05 + y_test_preds_lgb * 0.95\nroc_auc_score(y, weighted_preds)","8a40809b":"public_sub = pd.read_csv(\"..\/input\/santander-lgb-new-features-rank-mean-10-folds\/submission_LGBM.csv\")\npublic_sub.head()","170bc6c1":"sub[\"target\"] = weighted_test_preds","b23d3c0d":"sub[\"target\"] = 0.2*sub[\"target\"].rank() + 0.8*public_sub[\"target\"]\nsub.to_csv(\"submission_blend.csv\", index=False)","90961caf":"import keras\nfrom keras.layers import (Flatten, Conv1D, Conv2D, Input, Dense, Dropout, BatchNormalization,\n                          concatenate, GaussianNoise, Reshape, TimeDistributed, LeakyReLU, PReLU, Embedding)\nfrom keras.models import Model, load_model, save_model\nfrom keras.optimizers import SGD, Adam\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom pathlib import Path\nfrom keras.callbacks import Callback\n\nclass ROC_AUC(Callback):\n    def __init__(self, validation_data):\n        self.X_val, self.y_val = validation_data\n    \n    def on_epoch_end(self, epoch, logs={}):\n        print(\"ROC AUC for this fold, is \", roc_auc_score(self.y_val, self.model.predict(X_val)))\n        \nclass NNv1(BaseEstimator, ClassifierMixin):\n    def __init__(self,\n                 inp_shape=200,\n                 gaussian_noise=0.01,\n                 dense1_dim=32,\n                 dense2_dim=32,\n                 dense1_kwargs=None,\n                 dense2_kwargs=None,\n                 classifier_kwargs=None,\n                 optimizer=SGD,\n                 opt_kwargs=None,\n                 ):\n        self.inp_shape = inp_shape\n        self.gaussian_noise = gaussian_noise\n        self.dense1_dim = dense1_dim\n        self.dense2_dim = dense2_dim\n        self.dense1_kwargs = dense1_kwargs\n        self.dense2_kwargs = dense2_kwargs\n        self.classifier_kwargs = classifier_kwargs\n        self.optimizer = optimizer\n        self.opt_kwargs = opt_kwargs\n        self._default_initiaization()\n\n    def _default_initiaization(self):\n        if self.dense1_kwargs is None:\n            self.dense1_kwargs = {\"kernel_initializer\": \"glorot_uniform\"}\n        if self.dense2_kwargs is None:\n            self.dense2_kwargs = {\"kernel_initializer\": \"he_uniform\"}\n        if self.classifier_kwargs is None:\n            self.classifier_kwargs = {\"kernel_initializer\": \"he_uniform\"}\n        if self.opt_kwargs is None:\n            self.opt_kwargs = {}\n\n    def _build_model(self):\n        inp = Input(shape=(self.inp_shape,))\n        # x = GaussianNoise(self.gaussian_noise)(inp)\n        x = Reshape((self.inp_shape, 1))(inp)\n        d1 = Dense(self.dense1_dim, activation='tanh',)(x)\n        #d1 = TimeDistributed(Dropout(0.2))(d1)\n        d2 = Dense(self.dense1_dim, activation='relu',)(x)\n        #d2 = PReLU()(d2)\n        #d2 = TimeDistributed(Dropout(0.2))(d2)\n        x = concatenate([d1, d2])\n        x = Flatten()(x)\n        out = Dense(1, activation='sigmoid', **self.classifier_kwargs)(x)\n\n        model = Model(inputs=inp, outputs=out)\n        opt = self.optimizer(**self.opt_kwargs)\n        model.compile(loss='binary_crossentropy', optimizer=opt)\n        return model\n\n    def fit(self, X, y, *args, **kwargs):\n        self.model = self._build_model()\n        print(self.model.summary())\n        self.model.fit(X, y, *args, **kwargs)\n        return self\n\n    def predict(self, X, y=None, weight_path=None, **kwargs):\n        if self.model:\n            if weight_path is not None:\n                self.model.load_weights(weight_path)\n            y_hat = self.model.predict(X, **kwargs)\n        else:\n            raise ValueError(\"Model not fit yet\")\n        return y_hat\n","f0159eee":"model = NNv1(opt_kwargs = {\"lr\": 0.01, \"momentum\": 0.9, \"nesterov\": True, \"clipnorm\": 1})\ny_preds_nn = np.zeros((len(y)))\nfor tr_idx, val_idx in cvlist:\n    X_dev, y_dev = X_sc[tr_idx], y.iloc[tr_idx]\n    X_val, y_val = X_sc[val_idx], y.iloc[val_idx]\n    roc_auc = ROC_AUC((X_val, y_val))\n    \n    model.fit(X_dev, y_dev, validation_data=(X_val, y_val), epochs=40, batch_size=256, verbose=0, callbacks=[roc_auc])\n    val_preds = model.predict(X_val, batch_size=5000)\n    y_preds_nn[val_idx] = val_preds.flatten()\n","2c58bcd6":"roc_auc_score(y, y_preds_nn)","fea3fb57":"## EDA\n\n### Pointers\n* Check out existing kernels\nhttps:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction\nhttps:\/\/www.kaggle.com\/artgor\/santander-eda-fe-fs-and-models\nhttps:\/\/www.kaggle.com\/mjbahmani\/santander-ml-explainability\n\n* Check distributions\n* Compare train and test distributions\n* Identify important features (Most of the times feature engineering is going to be around features with high predictive power)\n* Attach a logic to why featurea are important ( Note: data is anonymised  here so hard to do this)\n* Check previous solutions to similar problems\n\n\n### Observations\n* Data normalization and imputation\n* Weak corelations between features and target\n* IV values ??\n* Most variables have distribution close to normal\n* Almost no corelation between differnt variable - What does it mean ??\n* No NA values (already imputed??)\n* Some features seem to have been clipped at one end\n* Spikes in distributions (imputed values??)\n* less unique ","d9606728":"### Method 2 - use validation fold models to predict on test set\n","1b96f7d6":"### Method -1 : train on full and predict on test\n - rule  - scale boosting rounds by train data ratio to data during validation - 1500 ","a997b685":"* This notebook was part of UpGrad Kagglethon, initiative to help their cohort getting started with Kaggle competitions. To be compliant with rules, I am sharing everything that was discussed during those sessions. ","20bc24ee":"# Next steps:\n\n* Feature engineering - interactions, bucketing etc\n* try other algorithms -- catboost, xgboost, RGF (regularized greedy forest), different NN architecture\n* weighted average\n* add more public solutions to blend\n* submit and keep making progress\n* maintain a list of ideas to be executed, you should never run out of things to do\n\n### ** Happy Kaggling and thank you :) **\n\nMeanwhile something to inspire you from one of the greats:  https:\/\/www.youtube.com\/watch?v=7XEMPU17-Wo","9df490fe":"** Problem statement **\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\n\n","7ce5e6b5":"### Modelling\n\nPointers:\n*  Validation strategy -- Random KFold, holdout or temporal split ??\n* What to trust validation score or LB socre?? trust score from more data; if test data is more we should treat LB as additional fold\n* Hyperparamter tuning -- Combination of manual tuning and bayesian optimization libraries like `hyperopt` and `scikit-optimize`. Initial tuninng on single fold and then move to 5 folds.\n* Always check validation and test set prediction distributions\n* ** Read forums and participate in discussions **\n\nDisussions:\n* Sometimes using geometric mean of probabilities is better than using simple mean\n* When metric is ROC_AUC, even rank average can be used\n* Blending -- blend of your solution and public solution can be used to improve LB score. But, better approach is to understand what is working for other people and integrate in your models.\n","5d86ea9c":"** Why do Kaggle**\n\n* Learning new things\n* strenghtnen intuition for ml algorithms and techniques\n* like competing with fellow kagglers"}}