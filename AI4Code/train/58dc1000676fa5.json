{"cell_type":{"f0d093cf":"code","7d18a544":"code","18ed1f6c":"code","0e351aaa":"code","6570eb9f":"code","b382801a":"code","fa724242":"code","353a63fb":"code","ca72f91d":"code","f545930e":"code","6917e154":"code","eb592471":"code","9809421e":"code","e78bdd86":"code","26ae5497":"code","43dfb7f8":"code","8d4b6e59":"code","aa3f2f7d":"code","e61c92e7":"code","8effe8bd":"code","0b022cb8":"code","bac20d5d":"code","e0158d3b":"code","2a1bac5d":"code","c972e768":"code","c2ad7e5e":"code","9622ed8a":"code","956ffec8":"code","243801aa":"markdown","40f6b88b":"markdown","1cfde36e":"markdown","efede803":"markdown","6da3e8ec":"markdown","5c8f610f":"markdown","8fe1d39b":"markdown","fc75a757":"markdown","a631bc18":"markdown","c48bbd0f":"markdown","c111cf9e":"markdown","e8e08753":"markdown","1c315fcf":"markdown","ef1f9ba8":"markdown","7596add5":"markdown","18a965ff":"markdown","42eb4045":"markdown","385345d5":"markdown","bf7d02fd":"markdown","452614b6":"markdown"},"source":{"f0d093cf":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy import stats","7d18a544":"df = pd.read_csv(\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/breast-cancer\/breast-cancer.data\",\n                 header=None)\n# label columns\ndf.columns = [\"class\", \"age\", \"menopause\", \"tumor_size\",\n                    \"inv_nodes\", \"node_caps\", \"deg_malig\",\n                    \"breast\", \"breast_quad\", \"irradiat\"]","18ed1f6c":"df.isnull().sum()","0e351aaa":"df.info()","6570eb9f":"df[\"class\"].value_counts()","b382801a":"df[\"age\"].value_counts()","fa724242":"df[\"tumor_size\"].value_counts()","353a63fb":"df[\"inv_nodes\"].value_counts()","ca72f91d":"df[\"menopause\"].value_counts()","f545930e":"df[\"node_caps\"].value_counts()","6917e154":"df[\"breast\"].value_counts()","eb592471":"df[\"irradiat\"].value_counts()","9809421e":"df[\"breast_quad\"].value_counts()","e78bdd86":"df.replace(\"?\", np.nan, inplace=True)\ndf.isnull().sum()\n# now we have nan values. drop these observations\ndf.dropna(inplace=True)","26ae5497":"fig, ax = plt.subplots(2, 3)\n\nlong = df[[\"class\", \"age\"]].groupby([\"age\", \"class\"]).size().reset_index(name=\"count\").pivot(index=\"age\", columns=\"class\", values=\"count\")\nlong[\"total\"] = long.sum(axis=1)\nlong[\"no-recurrence-events\"] = long[\"no-recurrence-events\"] \/ long[\"total\"]\nlong[\"recurrence-events\"] = long[\"recurrence-events\"] \/ long[\"total\"]\nlong.drop(\"total\", axis=1, inplace=True)\nlong.plot(kind=\"bar\", stacked=True, ax=ax[0,0])\nax[0,0].set_xlabel(\"age\")\nax[0,0].get_legend().remove()\n\nlong = df[[\"class\", \"menopause\"]].groupby([\"menopause\", \"class\"]).size().reset_index(name=\"count\").pivot(index=\"menopause\", columns=\"class\", values=\"count\")\nlong[\"total\"] = long.sum(axis=1)\nlong[\"no-recurrence-events\"] = long[\"no-recurrence-events\"] \/ long[\"total\"]\nlong[\"recurrence-events\"] = long[\"recurrence-events\"] \/ long[\"total\"]\nlong.drop(\"total\", axis=1, inplace=True)\nlong.plot(kind=\"bar\", stacked=True, rot=0, ax=ax[0,1])\nax[0,1].set_xlabel(\"menopause\")\nax[0,1].get_legend().remove()\n\nlong = df[[\"class\", \"tumor_size\"]].groupby([\"tumor_size\", \"class\"]).size().reset_index(name=\"count\").pivot(index=\"tumor_size\", columns=\"class\", values=\"count\")\nlong[\"total\"] = long.sum(axis=1)\nlong[\"no-recurrence-events\"] = long[\"no-recurrence-events\"] \/ long[\"total\"]\nlong[\"recurrence-events\"] = long[\"recurrence-events\"] \/ long[\"total\"]\nlong.drop(\"total\", axis=1, inplace=True)\norder = [\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\"]\nlong.loc[order].plot(kind=\"bar\", stacked=True, ax=ax[0,2])\nax[0,2].set_xlabel(\"tumor size\")\nax[0,2].get_legend().remove()\n\nlong = df[[\"class\", \"breast\"]].groupby([\"breast\", \"class\"]).size().reset_index(name=\"count\").pivot(index=\"breast\", columns=\"class\", values=\"count\")\nlong[\"total\"] = long.sum(axis=1)\nlong[\"no-recurrence-events\"] = long[\"no-recurrence-events\"] \/ long[\"total\"]\nlong[\"recurrence-events\"] = long[\"recurrence-events\"] \/ long[\"total\"]\nlong.drop(\"total\", axis=1, inplace=True)\nlong.plot(kind=\"bar\", stacked=True, rot=0, ax=ax[1,0])\nax[1,0].set_xlabel(\"breast\")\nax[1,0].get_legend().remove()\n\nlong = df[[\"class\", \"breast_quad\"]].groupby([\"breast_quad\", \"class\"]).size().reset_index(name=\"count\").pivot(index=\"breast_quad\", columns=\"class\", values=\"count\")\nlong[\"total\"] = long.sum(axis=1)\nlong[\"no-recurrence-events\"] = long[\"no-recurrence-events\"] \/ long[\"total\"]\nlong[\"recurrence-events\"] = long[\"recurrence-events\"] \/ long[\"total\"]\nlong.drop(\"total\", axis=1, inplace=True)\nlong.plot(kind=\"bar\", stacked=True, ax=ax[1,1])\nax[1,1].set_xlabel(\"breast quad\")\nax[1,1].get_legend().remove()\n\nlong = df[[\"class\", \"deg_malig\"]].groupby([\"deg_malig\", \"class\"]).size().reset_index(name=\"count\").pivot(index=\"deg_malig\", columns=\"class\", values=\"count\")\nlong[\"total\"] = long.sum(axis=1)\nlong[\"no-recurrence-events\"] = long[\"no-recurrence-events\"] \/ long[\"total\"]\nlong[\"recurrence-events\"] = long[\"recurrence-events\"] \/ long[\"total\"]\nlong.drop(\"total\", axis=1, inplace=True)\nlong.plot(kind=\"bar\", stacked=True, rot=0, ax=ax[1,2])\nax[1,2].set_xlabel(\"deg_malig\")\nax[1,2].get_legend().remove()\nhandles, labels = ax[1,2].get_legend_handles_labels()\nfig.legend(handles, labels, loc=\"upper center\", ncol=2)","43dfb7f8":"df[df.select_dtypes(\"object\").columns] = df.select_dtypes(\"object\").astype(\"category\")","8d4b6e59":"# perform chi2 tests\n# get categorical columns\ncategorical = df[df.select_dtypes(\"category\").columns]\n# remove the class column\ncategorical = categorical.loc[:, categorical.columns != \"class\"]\n# now loop and perform chi2 test and save results\nchi2_results = pd.DataFrame({\"variable\": object(), \"p-value\": float(), \"significance\": object()}, index=[])\nfor column in categorical.columns:\n    crosstab = pd.crosstab(df[\"class\"], df[column])\n    chi2, p, dof, expected = stats.chi2_contingency(crosstab)\n    if p < 0.001:\n        stars = \"***\"\n    elif p < 0.01:\n        stars = \"**\"\n    elif p < 0.05:\n        stars = \"*\"\n    elif p>= 0.05:\n        stars = \"\"\n    chi2_results = chi2_results.append({\"variable\": column, \"p-value\": p, \"significance\": stars}, ignore_index=True)\nchi2_results.sort_values(\"p-value\")","aa3f2f7d":"col1 = df[df[\"class\"] == \"no-recurrence-events\"]\ncol2 = df[df[\"class\"] == \"recurrence-events\"]\nt, p = stats.ttest_ind(col1[\"deg_malig\"], col2[\"deg_malig\"])\nif p < 0.001:\n    print(p, \"***\")\nelif p < 0.01:\n    print(p, \"**\")\nelif p < 0.05:\n    print(p, \"*\")\nelif p >= 0.05:\n    print(p)","e61c92e7":"lb_age = LabelEncoder()\nlb_age.fit([\"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\"])\ndf[\"age\"] = lb_age.transform(df[\"age\"])\n\nlb_tumor = LabelEncoder()\nlb_tumor.fit([\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\"])\ndf[\"tumor_size\"] = lb_tumor.transform(df[\"tumor_size\"])\n\nlb_nodes = LabelEncoder()\nlb_nodes.fit([\"0-2\", \"3-5\", \"6-8\", \"9-11\", \"12-14\", \"15-17\", \"24-26\"])\ndf[\"inv_nodes\"] = lb_nodes.transform(df[\"inv_nodes\"])","8effe8bd":"df[\"class\"].value_counts()\n# let us convert this to factors where 1 means recurrence and 0 means no-recurrence\nlb_class = LabelEncoder()\nlb_class.fit([\"no-recurrence-events\", \"recurrence-events\"])\ndf[\"class\"] = lb_class.transform(df[\"class\"])\n\ndf[\"menopause\"].value_counts()\n# Since there are three classes and order is not important, we will use hot coding\nhot = pd.get_dummies(df[\"menopause\"], prefix=\"menopause\")\n# merge into data set\ndf = df.join(hot)\n\ndf[\"node_caps\"].value_counts()\n# now we just convert these columns to factors without thinking of the order since there is no order.\n# use binary since there are only two possible values\ndf[\"node_caps\"] = df[\"node_caps\"].cat.codes\ndf[\"breast\"].value_counts()\ndf[\"breast\"] = df[\"breast\"].cat.codes\ndf[\"irradiat\"].value_counts()\ndf[\"irradiat\"] = df[\"irradiat\"].cat.codes\ndf[\"breast_quad\"].value_counts()\n# use hot coding since there are more than two classes\nhot = pd.get_dummies(df[\"breast_quad\"], prefix=\"breast_quad\")\ndf = df.join(hot)","0b022cb8":"# split data set\nx = df[[\"age\", \"tumor_size\", \"inv_nodes\", \"node_caps\", \"deg_malig\", \"menopause_ge40\", \"menopause_lt40\",\n        \"menopause_premeno\", \"breast_quad_central\", \"breast_quad_left_low\", \"breast_quad_left_up\",\n        \"breast_quad_right_low\", \"breast_quad_right_up\", \"irradiat\"]]\ny = df[\"class\"]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=8)","bac20d5d":"results = pd.DataFrame({\"model\": object(), \"accuracy\": float(), \"f1\": float()}, index=[])","e0158d3b":"#define the parameters\nleaf_size = list(range(1, 10))\nn_neighbors = list(range(1, 10))\np = [1,2]\nparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\nneigh = KNeighborsClassifier()\n#define the grid search\nneigh_grid = GridSearchCV(neigh, parameters, cv=10, scoring=\"f1\")\n# find the best fit model\nneigh_best = neigh_grid.fit(x_train, y_train)\n# get the parameters of the best fit model\nparameters_best = neigh_best.best_estimator_.get_params()\n# fit the best fit model\nneigh = KNeighborsClassifier(n_neighbors=parameters_best[\"n_neighbors\"],\n                             leaf_size=parameters_best[\"leaf_size\"],\n                             p=parameters_best[\"p\"])\nneigh.fit(x_train, y_train)\n# get the predicted values\nyhat_neigh = neigh.predict(x_test)\nresults = results.append({\"model\": \"knn\", \"accuracy\": metrics.accuracy_score(y_test, yhat_neigh), \"f1\": metrics.f1_score(y_test, yhat_neigh)}, ignore_index=True)\nresults","2a1bac5d":"# define the parameters\nmax_depth = list(range(1, 10))\nmin_samples_split = list(range(2, 10))\nmin_samples_leaf = list(range(1, 5))\nparameters = dict(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\ntree = DecisionTreeClassifier()\n# define the grid search\ntree_grid = GridSearchCV(tree, parameters, cv=10, scoring=\"f1\")\n# find the best fit model\ntree_best = tree_grid.fit(x_train, y_train)\n# get the parameters of the best fit model\nparameters_best = tree_best.best_estimator_.get_params()\n# fit the best model\ntree = DecisionTreeClassifier(max_depth=parameters_best[\"max_depth\"],\n                              min_samples_split=parameters_best[\"min_samples_split\"],\n                              min_samples_leaf=parameters_best[\"min_samples_leaf\"])\ntree.fit(x_train, y_train)\nyhat_tree = tree.predict(x_test)\nresults = results.append({\"model\": \"tree\", \"accuracy\": metrics.accuracy_score(y_test, yhat_tree), \"f1\": metrics.f1_score(y_test, yhat_tree)}, ignore_index=True)\nresults","c972e768":"# define parameters\nc = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\nparameters = dict(C=c)\nlr = LogisticRegression(max_iter=500)\n# define the grid search\nlog_grid = GridSearchCV(lr, parameters, cv=10, scoring=\"f1\")\n# get the best fit model\nlog_best = log_grid.fit(x_train, y_train)\n# get the parameters of the best fit model\nparameters_best = log_best.best_estimator_.get_params()\n# fit the best model\nlr = LogisticRegression(C=parameters_best[\"C\"], max_iter=500)\nlr.fit(x_train, y_train)\nyhat_log = lr.predict(x_test)\nresults = results.append({\"model\": \"logistic\", \"accuracy\": metrics.accuracy_score(y_test, yhat_log), \"f1\": metrics.f1_score(y_test, yhat_log)}, ignore_index=True)\nresults","c2ad7e5e":"# define the parameters\nkernel = ['poly', 'rbf', 'sigmoid', 'linear']\nc = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\nparameters = dict(kernel=kernel, C=c)\nsv =svm.SVC()\n# define the grid search\nsv_grid = GridSearchCV(sv, parameters, cv=10, scoring=\"f1\")\n# get the best fit model\nsv_best = sv_grid.fit(x_train, y_train)\n# get the parameters of the best fit model\nparameters_best = sv_best.best_estimator_.get_params()\n# fit the best model\nsv = svm.SVC(C=parameters_best[\"C\"], kernel=parameters_best[\"kernel\"])\nsv.fit(x_train, y_train)\nyhat_svm = sv.predict(x_test)\nresults = results.append({\"model\": \"SVM\", \"accuracy\": metrics.accuracy_score(y_test, yhat_svm), \"f1\": metrics.f1_score(y_test, yhat_svm)}, ignore_index=True)\nresults","9622ed8a":"# define the parameters\nmax_depth = [int(x) for x in np.linspace(1, 50, 10)]\nmax_features = [int(x) for x in np.linspace(1, 10, 2)]\nn_estimators = [int(x) for x in np.linspace(1, 100, 10)]\nparameters = dict(max_depth=max_depth, max_features=max_features, n_estimators=n_estimators)\nforest = RandomForestClassifier()\n# define the grid search\nforest_grid = GridSearchCV(forest, parameters, cv=10, scoring=\"f1\")\n# get the best fit model\nforest_best = forest_grid.fit(x_train, y_train)\n# get the parameters of the best model\nparameters_best = forest_best.best_estimator_.get_params()\n# fit the model with the best parameters\nforest = RandomForestClassifier()\nforest.fit(x_train, y_train)\nyhat_forest = forest.predict(x_test)\nresults = results.append({\"model\": \"Random forest\", \"accuracy\": metrics.accuracy_score(y_test, yhat_forest), \"f1\": metrics.f1_score(y_test, yhat_forest)}, ignore_index=True)\nresults","956ffec8":"plt.plot(results[\"model\"], results[\"accuracy\"], '-o', label=\"accuracy\")\nplt.plot(results[\"model\"], results[\"f1\"], '-o', label=\"f1 score\")\nplt.legend()","243801aa":"## Splitting the data set into train and test sets\n\nWe now split the data in preparation of running the ML algorithms:","40f6b88b":"## Random forest","1cfde36e":"Let us look at the values of the object variables:","efede803":"We note that in somce cases there are question marks. We should treat them like missing values:","6da3e8ec":"Summary of results:","5c8f610f":"## Decision tree","8fe1d39b":"Create data frame to save results:","fc75a757":"## SVC","a631bc18":"# Bivariate analysis\n\nNow perform the chi2 test on the categorical columns:","c48bbd0f":"# Cleaning the data\n\nWe check for missing values:","c111cf9e":"## Logistic regression","e8e08753":"# Machine learning\n\n## Encoding the categorical variables\n\nMany of the categories have a natural order. We reorder so that this natural order is maintained the three columns are age, tumor_size, and inv_nodes:","1c315fcf":"Next we read the data. The data comes from the UCI repository. The data set contains information about breast cancer patients. The variable that we are trying to predict is whether there will be recurrence or not. These are the variables in the data set:\n\n* Class: no-recurrence-events, recurrence-events\n* age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n* menopause: lt40, ge40, premeno.\n* tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n* inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n* node-caps: yes, no.\n* deg-malig: 1, 2, 3.\n* breast: left, right.\n* breast-quad: left-up, left-low, right-up, right-low, central.\n* irradiat: yes, no.\n\nWe download the data set and name the columns:\n","ef1f9ba8":"Now we look at the other columns to make sure that everything looks alright:","7596add5":"Looking at the figure above, it would be prudent to perform chi2 tests on each of the variables to measure the significance level of the results. First we convert the variables from type object to type categorical:","18a965ff":"# Initialising the environment\n\nFirst we set up the environment:","42eb4045":"## KNN","385345d5":"# Visualizing the data\n\nLet us plot a figure that will help visualise the distribution of both classes in each group of the categorical variables:","bf7d02fd":"Look at the types of the columns:","452614b6":"Now perform ttest on the single numeric variable that we have:"}}