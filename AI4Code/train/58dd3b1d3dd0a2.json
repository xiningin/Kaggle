{"cell_type":{"0c27e4aa":"code","7afde4bb":"code","17e77290":"code","225c502c":"code","a2ecbb02":"code","afd14c18":"code","fb489787":"code","70ad1902":"code","80c3b483":"code","0cfe5a90":"code","3684f68b":"code","6d36ea4f":"code","c6b7bd5e":"code","a22b8c52":"code","09c5daa5":"code","966857ad":"code","f1d15c83":"code","e6d2d4f2":"code","19115b5d":"code","14ecdcab":"code","ad6916d4":"code","18b50aaa":"code","d4f24619":"code","af9986b5":"code","321c1727":"code","e416188a":"code","6c6e3543":"code","43bd2506":"code","5c940f9c":"code","df6bdc0f":"code","e1555461":"code","052bb7cc":"code","1f5e3399":"code","f9991758":"code","112b255d":"code","b294a606":"code","bb1364f6":"code","6465a7a6":"code","216c2a97":"code","f73924d8":"code","bc4a2379":"code","13495dcf":"code","46141591":"code","b5f094ba":"code","6ef444a2":"code","1ca9b797":"code","7a61ec17":"code","9eebbe83":"code","81dde310":"code","c4c17f87":"code","11207fa2":"code","aa1b5564":"code","778d1f68":"code","9ba10f97":"code","33df4b16":"code","e927af09":"code","a2a9ef98":"code","6981e8af":"code","1618771d":"code","6461f0d6":"code","39b4d34a":"code","da6e1c09":"code","63a75816":"code","eb5b7c92":"code","96094843":"code","6195b51d":"code","29b25b2e":"code","78ba637e":"markdown","0de4160f":"markdown","a1349011":"markdown","760c17da":"markdown","75e6357f":"markdown","c5230199":"markdown","03687458":"markdown","12ee678e":"markdown","ba7464fd":"markdown","93f59cd9":"markdown","3b7f13b9":"markdown","296d7cd3":"markdown","2fe92873":"markdown","7501227b":"markdown","e214c190":"markdown","21fd7d36":"markdown","fe8555ca":"markdown","bdd5ed7c":"markdown","f5d1fd33":"markdown","f4f7509e":"markdown","d9379ad9":"markdown","884185d6":"markdown","581e90a9":"markdown","6f227dcb":"markdown","d2aaaaa8":"markdown","89a09bcb":"markdown","0c4da73e":"markdown","fa40fd0a":"markdown","8f988e61":"markdown","5a07da5d":"markdown","019ed2cc":"markdown","21834d10":"markdown","c8f2afa3":"markdown","a8d302af":"markdown","1168c89b":"markdown","3e920b76":"markdown","c2cff552":"markdown","ec70559f":"markdown","46dbd5ee":"markdown","2a1c8324":"markdown","bfce06a2":"markdown","4eec6f95":"markdown","f5342985":"markdown","0baa9299":"markdown","5cd4240f":"markdown"},"source":{"0c27e4aa":"import numpy as np \nimport pandas as pd\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","7afde4bb":"data = pd.read_csv(\"..\/input\/BreadBasket_DMS.csv\")\ndata.shape","17e77290":"data.head(10)","225c502c":"data['Date'] = pd.to_datetime(data['Date'],format='%Y-%m-%d')","a2ecbb02":"data['Time'] = pd.to_datetime(data['Time'])\ndata['Times'] = data['Time'].dt.time","afd14c18":"#tot_tr = data.groupby('Date', as_index=True)['Transaction'].sum().reset_index()\ntot_tr1 = data.groupby(['Date', 'Transaction']).size().reset_index()\ntot_tr1.columns = ['Date', 'Transaction', 'count']","fb489787":"tot_tr = tot_tr1.groupby('Date', as_index=True)['count'].sum().reset_index()\ntot_tr.columns = ['Date', 'Transaction']","70ad1902":"tot1 = tot_tr.iloc[:56]\ntot2 = tot_tr.iloc[56:]","80c3b483":"b = pd.to_datetime('2017-01-02 00:00:00',format='%Y-%m-%d')\nc = pd.to_datetime('2016-12-25 00:00:00',format='%Y-%m-%d')\nd = pd.to_datetime('2016-12-26 00:00:00',format='%Y-%m-%d')","0cfe5a90":"tot2.loc[-2] = [c, 0]\ntot2.loc[-1] = [d, 0] \ntot2.index = tot2.index + 2\ntot2 = tot2.sort_index()\ntot2 = tot2.rename(index={0: 56, 1:57})","3684f68b":"#tot1.append(tot2)\ntot_tr2 = tot1.append(tot2)","6d36ea4f":"tot1 = tot_tr2.iloc[:64]\ntot2 = tot_tr2.iloc[64:]","c6b7bd5e":"tot2.loc[-1] = [b, 0]\ntot2.index = tot2.index + 1\ntot2 = tot2.sort_index()\ntot2 = tot2.rename(index={0:64})","a22b8c52":"#tot1.append(tot2)\ntot_tr3 = tot1.append(tot2)","09c5daa5":"tot_tr3 = tot_tr3.replace({'Transaction': {0: 1}})","966857ad":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nxtr = tot_tr.loc[tot_tr['Transaction'].idxmax()][0]\nytr = tot_tr['Transaction'].max()\n\ndata1 = [go.Scatter(\n          x=tot_tr.Date,\n          y=tot_tr.Transaction)]\n\nlayout = go.Layout(\n   showlegend=False,\n    annotations=[\n        dict(\n            x=xtr,\n            y=ytr,\n            xref='x',\n            yref='y',\n            text='Highest',\n            showarrow=True,\n            arrowhead=7,\n            ax=0,\n            ay=-40\n        )\n    ]\n)\n\nfig = go.Figure(data=data1, layout=layout)\npy.iplot(fig, filename='multiple-annotation')","f1d15c83":"data['hour'] = data['Time'].dt.hour\n#data.head(10)","e6d2d4f2":"s = data['hour'].value_counts().reset_index()","19115b5d":"plt.rcParams['figure.figsize']=(20,20)\ng = sns.jointplot(x=s['index'], y=s['hour'], data=s, kind=\"kde\", color = \"m\", size=12, aspect=3);\ng.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")\ng.ax_joint.collections[0].set_alpha(0)\ng.set_axis_labels(\"Hour\", \"Count of transactions\");","14ecdcab":"item_cnt = data['Item'].value_counts().reset_index()\nitem_cnt.columns = ['Item', 'Count']\nitem_cnt = item_cnt[item_cnt.Item != 'NONE']\nitem_cnt = item_cnt.head(15)\n#item_cnt","ad6916d4":"import plotly.graph_objs as go\n\nlabels = item_cnt['Item'].values.tolist()\nvalues = item_cnt['Count'].values.tolist()\n\ntrace = go.Pie(labels=labels, values=values)\n\npy.iplot([trace], filename='item_chart')","18b50aaa":"item_cnt = data.groupby(['Item', 'hour']).size().reset_index()\nitem_cnt.columns = ['Item', 'Hour', 'Count']\nitem_cnt = item_cnt[item_cnt.Item != 'NONE']\nitem_cnt = item_cnt.sort_values(by='Count', ascending=False)\n#item_cnt#.head(100)","d4f24619":"item_cnt_cf = data.groupby(['Item', 'hour']).size().reset_index()\nitem_cnt_cf.columns = ['Item', 'Hour', 'Count']\nitem_cnt_cf = item_cnt[item_cnt['Item'].isin(['Coffee', 'Bread', 'Tea'])]\n#item_cnt_cf","af9986b5":"g = sns.PairGrid(item_cnt_cf, hue=\"Item\", height=10, aspect=1)\ng.map_diag(plt.hist)\ng.map_offdiag(plt.scatter)\ng.add_legend();","321c1727":"item_cnt_cake = data.groupby(['Item', 'hour']).size().reset_index()\nitem_cnt_cake.columns = ['Item', 'Hour', 'Count']\nitem_cnt_cake = item_cnt[item_cnt['Item'].isin(['Cake', 'Pastry', 'Sandwich', 'Medialuna'])]\n\ng = sns.PairGrid(item_cnt_cake, hue=\"Item\", height=10, aspect=1)\ng.map_diag(plt.hist)\ng.map_offdiag(plt.scatter)\ng.add_legend();","e416188a":"train= tot_tr3[0:100] \ntest= tot_tr3[100:]","6c6e3543":"train_series = pd.Series(train.Transaction.values, index=pd.date_range(train.Date.min(),train.Date.max(),freq='D'))\ntest_series = pd.Series(test.Transaction.values, index=pd.date_range(test.Date.min(),test.Date.max(),freq='D'))","43bd2506":"from matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 18, 12\nplt.plot(train_series, label='train')\nplt.plot(test_series, label='test')\nplt.title('Train and test Graph')\nplt.legend()\nplt.show()","5c940f9c":"from statsmodels.tsa.seasonal import seasonal_decompose\nts_trs = pd.Series(tot_tr3.Transaction.values, index=pd.date_range(tot_tr3.Date.min(),tot_tr3.Date.max(),freq='D'))\ndeompose = seasonal_decompose(ts_trs, freq=24)\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 15, 10\ndeompose.plot()","df6bdc0f":"tot_tr3['moving_average'] = tot_tr3['Transaction'].rolling(window=3, center=False).mean()\nplt.figure(figsize=(20,10))\nplt.plot(tot_tr3.Date, tot_tr3.Transaction,'-',color='black',alpha=0.3)\nplt.plot(tot_tr3.Date, tot_tr3.moving_average,color='b')\nplt.title('Transaction and Moving Average Smoothening')\nplt.legend()\nplt.show()","e1555461":"y_hat_avg = test.copy()\ny_hat_avg['moving_avg_forecast'] = train['Transaction'].rolling(60).mean().iloc[-1]\nplt.figure(figsize=(18,12))\nplt.plot(train['Date'], train['Transaction'], label='Train')\nplt.plot(test['Date'], test['Transaction'], label='Test')\nplt.plot(y_hat_avg['Date'],y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast')\nplt.legend(loc='best')\nplt.show()","052bb7cc":"from math import sqrt\nfrom sklearn.metrics import mean_squared_error\nrms = sqrt(mean_squared_error(test.Transaction, y_hat_avg.moving_avg_forecast))\nprint(rms)","1f5e3399":"tot_tr3['ewma'] = tot_tr3['Transaction'].ewm(halflife=3, ignore_na=False,min_periods=0, adjust=True).mean()\nplt.figure(figsize=(20,10))\nplt.plot(tot_tr3.Transaction,'-',color='black',alpha=0.3)\nplt.plot(tot_tr3.ewma,color='g')\nplt.title('Transaction and Exponential Smoothening')\nplt.legend()\nplt.show()","f9991758":"#from statsmodels.tsa.api import SimpleExpSmoothing, Holt\n#from statsmodels.tsa.api import ExponentialSmoothing\n#import statsmodels.tsa.holtwinters.ExponentialSmoothing\n#y_hat_avg = test.copy()\n#fit2 = SimpleExpSmoothing(np.asarray(train['Transaction'])).fit(smoothing_level=0.6,optimized=False)\n#y_hat_avg['SES'] = fit2.forecast(len(test))\n#plt.figure(figsize=(16,8))\n#plt.plot(train['Transaction'], label='Train')\n#plt.plot(test['Transaction'], label='Test')\n#plt.plot(y_hat_avg['SES'], label='SES')\n#plt.legend(loc='best')\n#plt.show()","112b255d":"# Dickey Fuller test for Stationarity\n    \nfrom statsmodels.tsa.stattools import adfuller\ndef ad_fuller_test(ts):\n    dftest = adfuller(ts, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value','#Lags Used', 'Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n        print(dfoutput)","b294a606":"# Plot rolling stats\n    \ndef plot_rolling_stats(ts):\n    a = tot_tr3['Transaction']\n    ts_log = (a) \n    rolling_mean = ts_log.rolling(12).mean()\n    rolling_std = ts_log.rolling(12).std()\n    orig = plt.plot(ts, color='blue',label='Original')\n    mean = plt.plot(rolling_mean, color='red', label='Rolling Mean')\n    std = plt.plot(rolling_std, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)","bb1364f6":"log_series =  (tot_tr3.Transaction.values)\nlog_series1 = np.log(tot_tr3.Transaction.values)\ntot_tr3['log_series1'] = np.log(tot_tr3.Transaction.values)\nad_fuller_test(log_series)","6465a7a6":"ad_fuller_test(log_series1)","216c2a97":"plt.figure(figsize=(18,12))\nplt.plot(log_series1, 'blue', label='normal')\nplt.plot(log_series, 'red', label='log')\n\nplt.legend(loc='best')\nplt.show()","f73924d8":"from matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 20, 12\na = tot_tr3['Transaction']\nplot_rolling_stats(a)","bc4a2379":"log_series_shift = log_series1[1:] - log_series1[:-1]\nlog_series_shift = log_series_shift[~np.isnan(log_series_shift)]","13495dcf":"ad_fuller_test(log_series_shift)","46141591":"plt.plot(log_series_shift)","b5f094ba":"plot_rolling_stats(log_series_shift)","6ef444a2":"import statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA, ARMA\n\nfrom pandas.plotting import autocorrelation_plot\nfig = plt.figure(figsize=(12,12))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(log_series_shift , lags=20, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(log_series_shift, lags=20, ax=ax2)","1ca9b797":"def auto_arima(param_max=1,series=pd.Series(),verbose=True):\n    # Define the p, d and q parameters to take any value \n    # between 0 and param_max\n    p = d = q = range(0, param_max+1)\n\n    # Generate all different combinations of seasonal p, d and q triplets\n    pdq = [(x[0], x[1], x[2]) for x in list(itertools.product(p, d, q))]\n    \n    model_resuls = []\n    best_model = {}\n    min_aic = 10000000\n    for param in pdq:\n        try:\n            mod = sm.tsa.ARIMA(series, order=param)\n\n            results = mod.fit()\n            \n            if verbose:\n                print('ARIMA{}- AIC:{}'.format(param, results.aic))\n            model_resuls.append({'aic':results.aic,\n                                 'params':param,\n                                 'model_obj':results})\n            if min_aic>results.aic:\n                best_model={'aic':results.aic,\n                            'params':param,\n                            'model_obj':results}\n                min_aic = results.aic\n        except Exception as ex:\n            print(ex)\n    if verbose:\n        print(\"Best Model params:{} AIC:{}\".format(best_model['params'],\n              best_model['aic']))  \n        \n    return best_model, model_resuls","7a61ec17":"import itertools\nimport matplotlib.dates as mdates\nfrom sklearn.model_selection import TimeSeriesSplit\n\ndef arima_gridsearch_cv(series, cv_splits=2,verbose=True,show_plots=True):\n    # prepare train-test split object\n    tscv = TimeSeriesSplit(n_splits=cv_splits)\n    \n    # initialize variables\n    splits = []\n    best_models = []\n    all_models = []\n    i = 1\n    \n    # loop through each CV split\n    for train_index, test_index in tscv.split(series):\n        print(\"*\"*20)\n        print(\"Iteration {} of {}\".format(i,cv_splits))\n        i = i + 1\n        \n        # print train and test indices\n        if verbose:\n            print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        splits.append({'train':train_index,'test':test_index})\n        \n        # split train and test sets\n        train_series = series.ix[train_index]\n        test_series = series.ix[test_index]\n        \n        print(\"Train shape:{}, Test shape:{}\".format(train_series.shape,\n              test_series.shape))\n        \n        # perform auto arima\n        _best_model, _all_models = auto_arima(series=train_series)\n        best_models.append(_best_model)\n        all_models.append(_all_models)\n        \n        # display summary for best fitting model\n        if verbose:\n            print(_best_model['model_obj'].summary())\n        results = _best_model['model_obj']\n        \n        if show_plots:\n            # show residual plots\n            residuals = pd.DataFrame(results.resid)\n            residuals.plot()\n            plt.title('Residual Plot')\n            plt.show()\n            residuals.plot(kind='kde')\n            plt.title('KDE Plot')\n            plt.show()\n            print(residuals.describe())\n        \n            # show forecast plot\n            fig, ax = plt.subplots(figsize=(18, 4))\n            fig.autofmt_xdate()\n            ax = train_series.plot(ax=ax)\n            test_series.plot(ax=ax)\n            fig = results.plot_predict(test_series.index.min(), \n                                       test_series.index.max(), \n                                       dynamic=True,ax=ax,\n                                       plot_insample=False)\n            plt.title('Forecast Plot ')\n            plt.legend()\n            plt.show()\n\n            # show error plot\n            insample_fit = list(results.predict(train_series.index.min()+1, \n                                                train_series.index.max(),\n                                                typ='levels')) \n            plt.plot((np.exp(train_series.ix[1:].tolist())-\\\n                             np.exp(insample_fit)))\n            plt.title('Error Plot')\n            plt.show()\n    return {'cv_split_index':splits,\n            'all_models':all_models,\n            'best_models':best_models}","9eebbe83":"tot_tr3c = tot_tr3.copy()\ntot_tr3c = tot_tr3c.set_index('Date')\npd.to_datetime(tot_tr3c.index)\nresults_dict = arima_gridsearch_cv(tot_tr3c.log_series1,cv_splits=5)","81dde310":"model = ARIMA(log_series_shift, order=(1,0,0))  \nresults_AR = model.fit()\nplt.plot(log_series1)\nplt.plot(results_AR.fittedvalues, color='red')","c4c17f87":"transactions = pd.DataFrame(tot_tr)\ntransactions.columns = ['ds', 'y']","11207fa2":"from fbprophet import Prophet\n\nm = Prophet()\nm.fit(transactions)\nfuture = m.make_future_dataframe(periods=365)\nforecast = m.predict(future)\nforecast.head(10)","aa1b5564":"py.iplot([\n    go.Scatter(x=transactions['ds'], y=transactions['y'], name='y'),\n    go.Scatter(x=forecast['ds'], y=forecast['yhat'], name='yhat'),\n    go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill='tonexty', mode='none', name='upper'),\n    go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='none', name='lower'),\n    go.Scatter(x=forecast['ds'], y=forecast['trend'], name='Trend')\n])","778d1f68":"m = Prophet(changepoint_prior_scale=2.5)\nm.fit(transactions)\nfuture = m.make_future_dataframe(periods=365)\nforecast = m.predict(future)","9ba10f97":"# Calculate root mean squared error.\n\nprint('RMSE: %f' % np.sqrt(np.mean((forecast.loc[:1682, 'yhat']-transactions['y'])**2)) )\npy.iplot([\n    go.Scatter(x=transactions['ds'], y=transactions['y'], name='y'),\n    go.Scatter(x=forecast['ds'], y=forecast['yhat'], name='yhat'),\n    go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill='tonexty', mode='none', name='upper'),\n    go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='none', name='lower'),\n    go.Scatter(x=forecast['ds'], y=forecast['trend'], name='Trend')\n])","33df4b16":"m = Prophet(changepoint_prior_scale=2.5)\nm.add_seasonality(name='monthly', period=30.5, fourier_order=5)\nm.fit(transactions)\nfuture = m.make_future_dataframe(periods=365)\nforecast = m.predict(future)","e927af09":"# Calculate root mean squared error.\n\nprint('RMSE: %f' % np.sqrt(np.mean((forecast.loc[:1682, 'yhat']-transactions['y'])**2)) )\npy.iplot([\n    go.Scatter(x=transactions['ds'], y=transactions['y'], name='y'),\n    go.Scatter(x=forecast['ds'], y=forecast['yhat'], name='yhat'),\n    go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill='tonexty', mode='none', name='upper'),\n    go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='none', name='lower'),\n    go.Scatter(x=forecast['ds'], y=forecast['trend'], name='Trend')\n])","a2a9ef98":"item_cnt = data['Item'].value_counts().reset_index()\nitem_cnt.columns = ['Item', 'Count']\nitem_cnt = item_cnt[item_cnt.Item != 'NONE']","6981e8af":"objects = (list(item_cnt['Item'].head(n=20)))\ny_pos = np.arange(len(objects))\nperformance = list(item_cnt['Count'].head(n=20))\nplt.bar(y_pos, performance, align='center', alpha=0.5)\nplt.xticks(y_pos, objects, rotation='vertical')\nplt.ylabel('Item count')\nplt.title('Item Sales distribution')","1618771d":"total_item_count = item_cnt['Item'].count()\nitem_cnt['item_perc'] = item_cnt['Count']\/total_item_count\nitem_cnt['total_perc'] = item_cnt.item_perc.cumsum()\nict = item_cnt.head(10)","6461f0d6":"import plotly.graph_objs as go\n\ntrace = go.Table(\n    header=dict(values=['Item', 'Count', 'Item Perc', 'Total Perc'],\n                line = dict(color='#7D7F80'),\n                fill = dict(color='#a1c3d1'),\n                align = ['left'] * 5),\n    cells=dict(values=[ict['Item'].values.tolist(),\n                       ict['Count'].values.tolist(),\n                       ict['item_perc'].values.tolist(),\n                       ict['total_perc'].values.tolist()],\n               line = dict(color='#7D7F80'),\n               fill = dict(color='#EDFAFF'),\n               align = ['left'] * 5))\n\nlayout = dict(width=600, height=600)\ndata1 = [trace]\nfig = dict(data=data1, layout=layout)\npy.iplot(fig, filename = 'styled_table')","39b4d34a":"from mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules","da6e1c09":"items = []\nfor i in data['Transaction'].unique():\n    itemlist = list(set(data[data[\"Transaction\"]==i]['Item']))\n    if len(itemlist) > 0:\n        items.append(itemlist)","63a75816":"### We need to one-hot encode the items as the library we are working with accepts only 1,0, True or False\n\nfrom mlxtend.preprocessing import TransactionEncoder\n\noht = TransactionEncoder()\noht_item = oht.fit(items).transform(items)\ndf1 = pd.DataFrame(oht_item, columns=oht.columns_)\nfrequent_itemset = apriori(df1, use_colnames=True, min_support=0.02)\nrules = association_rules(frequent_itemset, metric=\"lift\", min_threshold=0.5)","eb5b7c92":"rules.head(10)","96094843":"plt.figure(figsize=(12,12))\nplt.scatter(rules['support'],rules['confidence'],marker='*',edgecolors='grey',s=100,c=rules['lift'])\nplt.colorbar(label='Lift')\nplt.xlabel('support')\nplt.ylabel('confidence')","6195b51d":"def draw_graph(rules, rules_to_show):\n    import networkx as nx  \n    plt.figure(figsize=(10,8))\n    G1 = nx.DiGraph()\n    \n    color_map=[]\n    N = 400\n    colors = np.random.rand(N)    \n    strs=[]\n    for i in range(rules_to_show):\n        strs.append('R'+str(i))\n    \n    for i in range (rules_to_show):      \n        G1.add_nodes_from([\"R\"+str(i)])\n         \n        for a in rules.iloc[i]['antecedents']:\n                \n            G1.add_nodes_from([a])        \n            G1.add_edge(a, \"R\"+str(i), color=colors[i] , weight = 1.5)\n        \n        for c in rules.iloc[i]['consequents']:         \n            G1.add_nodes_from([c])\n            G1.add_edge(\"R\"+str(i), c, color=colors[i],  weight=1.5)\n    \n    for node in G1:\n        if node in strs:\n            color_map.append('black')\n        else:\n            color_map.append('red')\n            \n    edges = G1.edges()\n    colors = [G1[u][v]['color'] for u,v in edges]\n    weights = [G1[u][v]['weight'] for u,v in edges]\n\n    pos = nx.spring_layout(G1, k=16, scale=1)\n    nx.draw(G1, pos, edges=edges, node_color = color_map, edge_color=colors, width=weights, font_size=14, with_labels=False)            \n    for p in pos:  # raise text positions\n        pos[p][1] += 0.08\n    nx.draw_networkx_labels(G1, pos)","29b25b2e":"draw_graph(rules,len(rules))","78ba637e":"### ARIMA \n\nKey Concepts\n\n* Stationarity: One the key assumptions behind the ARIMA models. Stationarity refers to the property where for a time series its mean,\nvariance, and autocorrelation are time invariant. In other words, mean, variance,\nand autocorrelation do not change with time. For instance, a time series having\nan upward (or downward) trend is a clear indicator of a non-stationarity because\nits mean would change with time. \n\n* Differencing: One of the methods of stationarizing series. Though there can be other\ntransformations, differencing is widely used to stabilize the mean of a time series. We\ncompute difference between consecutive observations to obtain a differenced\nseries. We can then apply different tests to confirm if the resulting series is stationary\nor not. We can also perform second order differencing, seasonal differencing, and so\non, depending on the time series at hand.\n\n* Unit Root Tests: Statistical tests that help us understand if a given series is stationary\nor not. The Augmented Dickey Fuller test begins with a null hypothesis of series being\nnon-stationary, while Kwiatkowski-Phillips-Schmidt-Shin test or KPSS has a null\nhypothesis that the series is stationary. We then perform a regression fit to reject or\nfail to reject the null hypothesis\n\nARIMA stands for Auto Regressive Integrated Moving Average model.  Let\u2019s look at the basics and constituents of this model.\n\n* Auto Regressive or AR Modeling: A simple linear regression model where current\nobservation is regressed upon one or more prior observations.\n\n* Moving Average or MA Modeling: Is again essentially a linear regression model that\nmodels the impact of noise\/error from prior observations to current one.\n\nThe ARIMA model is a logical progression and combination of the two models. Yet if we combine AR and MA with a differenced series, what we get is called as ARIMA(p,d,q) model.\nwhere,\n* p is the order of Autoregression\n* q is the order of Moving average\n* d is the order of differencing\n\nThus, for a stationary time series ARIMA models combine autoregressive and moving average concepts to model the behavior of a long running time series and helps in forecasting. Let\u2019s now apply these concepts to transactions forecasting.","0de4160f":"### Visualizing using Prophet ","a1349011":"### We need the list of items from the same transaction as a list so we can form associations","760c17da":"#### Traditional Approaches\n\nThere are matured and extensive set of modeling techniques available for time series . Out of the many, the following are a few most commonly used\nand explored techniques:\n* Simple moving average and exponential smoothing based forecasting\n* Holt\u2019s, Holt-Winter\u2019s Exponential Smoothing based forecasting\n* Box-Jenkins methodology (AR, MA, ARIMA, S-ARIMA, etc.)","75e6357f":"### Now, to implement few techniques in this kernel, lets split the dataset we have into train and test","c5230199":"### Plot of the forecast  & the trends","03687458":"Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.","12ee678e":"### Data pre-processing","ba7464fd":"### Again, same result as above. the differencing didn't result in a stationary series either","93f59cd9":"**Association Analysis 101**\n\nAlthough, there are many complex ways to analyze data (clustering, regression, Neural Networks, Random Forests, SVM, etc.) the challenge with many of these approaches is that they can be difficult to tune, challenging to interpret and require quite a bit of data prep and feature engineering to get good results. In other words, they can be very powerful but require a lot of knowledge to implement properly.\n\nAssociation analysis is relatively light on the math concepts and easy to explain to non-technical people. In addition, it is an unsupervised learning tool that looks for hidden patterns so there is limited need for data prep and feature engineering. It is a good start for certain cases of data exploration and can point the way for a deeper dive into the data using other approaches.\n\nAssociation rules are normally written like this: {Diapers} -> {Beer} which means that there is a strong relationship between customers that purchased diapers and also purchased beer in the same transaction.\n\nIn the above example, the {Diaper} is the antecedent and the {Beer} is the consequent. Both antecedents and consequents can have multiple items. In other words, {Diaper, Gum} -> {Beer, Chips} is a valid rule.\n\nHere are the  key metrics to consider when evaluating association rules:\n***\n**Support** is the relative frequency that the rules show up. In many instances, you may want to look for high support in order to make sure it is a useful relationship. However, there may be instances where a low support is useful if you are trying to find \u201chidden\u201d relationships.\n\nsupport(A\u2192C)=support(A\u222aC),range: [0,1]\n\n**Confidence** is a measure of the reliability of the rule. A confidence of .5 in the above example would mean that in 50% of the cases where Diaper and Gum were purchased, the purchase also included Beer and Chips. For product recommendation, a 50% confidence may be perfectly acceptable but in a medical situation, this level may not be high enough.\n\nConfidence(A\u2192C)=support(A\u2192C)support(A),range: [0,1]\n\n**Lift** is the ratio of the observed support to that expected if the two rules were independent (see wikipedia). The basic rule of thumb is that a lift value close to 1 means the rules were completely independent. Lift values > 1 are generally more \u201cinteresting\u201d and could be indicative of a useful rule pattern.\n\nlift(A\u2192C)=confidence(A\u2192C)support(C),range: [0,\u221e]\n\n**leverage:**\nlevarage(A\u2192C)=support(A\u2192C)\u2212support(A)\u00d7support(C),range: [\u22121,1]\n\nLeverage computes the difference between the observed frequency of A and C appearing together and the frequency that would be expected if A and C were independent. An leverage value of 0 indicates independence.\n\n**conviction':**\nconviction(A\u2192C)=1\u2212support(C)1\u2212confidence(A\u2192C),range: [0,\u221e]\n\nA high conviction value means that the consequent is highly depending on the antecedent. For instance, in the case of a perfect confidence score, the denominator becomes 0 (due to 1 - 1) for which the conviction score is defined as 'inf'. Similar to lift, if items are independent, the conviction is 1.\n***","3b7f13b9":"### How do you interpret the above network graph?\n\nThe R1, R2 etc..  are the row numbers in the rules dataframe and the arrows pointing to and from the corresponding nodes are the antecedents\nand consequents of that row. ","296d7cd3":"#### Exponential Smoothing (also called exponentially weighted moving average or EWMA for short)\n\nUnlike Moving average, exponential smoothening techniques apply exponentially decreasing weights to older observations. In simple words, exponential smoothening methods give more weight to recent past observations as compared to older observations. Depending on the level of smoothening required, there may be one or more smoothening parameters to set in case of exponential smoothening.","2fe92873":"### Smoothing techniques","7501227b":"***\nThe time series at hand is data related to bakery transactions.\n\nLet\u2019s now try to deconstruct various components that make up the time series at hand. A time\nseries is said to be comprised of the following three major components:\n\n* Seasonality: These are the periodic fluctuations in the observed data. \n* Trend: This is the increasing or decreasing behavior of the series with time.\n*  Residual: This is the remaining signal after removing the seasonality and trend\nsignals. It can be further decomposed to remove the noise component as well.\n***","e214c190":"# Association Rule-Mining using the  Apriori algorithm","21fd7d36":"The ACF and PACF plot also help us understand if a series is stationary or not. If a series has gradually decreasing values for ACF and PACF, it points toward non-stationarity property in the series & here by looking at the plot above we can say p & q to be 1 & 0, & d to be 0.\n\nAnother method to derive the p, d, q parameters is to perform a grid search of the parameter space. This is more in tune with the Machine Learning way of hyperparameter tuning. \n\n We need to split our dataset into train and test sets. We utilize scikit-learn\u2019s TimeSeriesSplit utility to help us get proper training and testing sets.","fe8555ca":"### We can see in the above graph that, Feb 4th had the most number of transactions","bdd5ed7c":"#### Since stationarity is one of the primary assumptions of ARIMA models, we will utilize Augmented Dickey Fuller test to check our series for stationarity. If the test statistic of AD Fuller test is less than the critical value(s), we reject the null hypothesis of nonstationarity.","f5d1fd33":"### Lets visualize the network graph of the associated items","f4f7509e":"### The moving average methods gives us 44.6 RMSE. Lets check the other methods","d9379ad9":"The time series has both upward and downward trend. It shows a decreasing trend and then an increasing trend as we saw in the initial plot as well.\n\nThe series certainly has a monthly periodicity or seasonality to it. The remaining signal is what is marked as residual.","884185d6":"####  Moving Average\n\n\nInstead of taking an average of complete time series to summarize, moving average makes use of a rolling windowed approach. In this case, we compute the mean\nof each successive smaller windows of past data to smoothen out the impact of random variation. ","581e90a9":"### Lets write a custom function to compute the DF test","6f227dcb":"### Among the top 3 items bought, lets see at what time of the day they are normally purchased","d2aaaaa8":"Why Smoothing? \n\nTime series data have inherent dependency on historical observations and there are multiple factors impacting each observation.\nIt is an inherent property of time series data to have random variation to it apart from its other constituents. \n\nTo better understand, model, and utilize time series for prediction related tasks, lets perform smoothing.\n\nSmoothing helps reduce the effect of random variation and helps clearly reveal the seasonality, trend, and residual components of the series. There are various methods\nto smooth out a time series. ","89a09bcb":"### Train - test split and prediction ","0c4da73e":"### RMSE after adding seasonality","fa40fd0a":"### Plotting rolling stats","8f988e61":"### A peek at the day-to-day transactions","5a07da5d":"### The lowest AIC p, d, q value wins and in this case it is 1, 0, 0 and looking at the forecast graph,  although it captures the trend,  it does miss out at times.","019ed2cc":"### As seen in the plot above, its between 11AM & 3 PM where most transactions happened in any given day.","21834d10":"### Smoothening for the entire set","c8f2afa3":"### Lets visualize support and confidence","a8d302af":"### What about the items bought?","1168c89b":"### Looking at the above table, we can say that cake, bread & coffee are bought together often","3e920b76":"#### The Test Statistic is less than the 5% critical value and therefore can be considered a stationary series. While for the log transformed data series, that isn't the case and therefore it is non-stationary.","c2cff552":"### Trend of transactions - Which day had the most number of transactions?","ec70559f":"### Among the top 15 items bought, coffee has been bought nearly 33% of the time","46dbd5ee":"### The aim of this notebook is to perform Time Series analysis on a set of transaction data & also perform association-rule based analysis.\n\n### Do read and upvote.! ","2a1c8324":"### Which hour had the most transactions?","bfce06a2":"#### 3 month moving average","4eec6f95":"We still need to figure out the order of autoregression and moving average components, i.e., p and q.\n\nOne of the commonly used methods is the plotting of **ACF and PACF plots** to determine p and q values. **ACF or Auto Correlation Function plot and PACF or the Partial Auto Correlation Function** plot helps us\nnarrow down the search space of determining the p and q values with a few caveats. \n\nThe ACF plot helps us understand the correlation of an observation with its lag (or previous value. The ACF plot is used to determine the MA order, i.e. q. The value at which ACF drops is the order of the MA model.\nOn the same lines, PACF points toward correlation between an observation and a specific lagged value, excluding effect of other lags. The value at which PACF drops points toward the order of AR model or the p in ARIMA(p,d,q)","f5342985":"The rolling mean and std seems to be stationary and we are good to go. But, \n\nLets also look at a first order differenced of the log series and perform the same test of stationary","0baa9299":"### Time Series Components","5cd4240f":"### As seen in the plot above, Coffee & tea is mostly during the entire day, a little scattered, while bread is bought during the morning hours mostly."}}