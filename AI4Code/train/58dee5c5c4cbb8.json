{"cell_type":{"a9006eac":"code","74c0886c":"code","ea38e7bd":"code","88ca044c":"code","cd1cacaf":"code","cb36b4ec":"code","25eedb91":"code","66ee6a19":"code","dc9c309f":"code","5d3cc5aa":"code","2cd8c9c8":"code","e639093b":"code","f5421f84":"code","8fd1e0b0":"code","29cf90dd":"code","260ec01f":"code","6d1c1b6e":"code","3aa46537":"code","6ad63c8f":"code","680da839":"code","bfa2f5ac":"code","df0aeab3":"code","d11e104a":"code","aacfde2b":"code","5aa59a56":"code","2e4b51f5":"code","b781ea52":"code","dced533d":"code","8ea69f5f":"code","9fc112f0":"code","775877da":"code","d0b9607c":"code","25b12298":"code","5e3ddd4e":"code","cca6df0b":"code","efd758da":"code","fda6928c":"code","50a3aa30":"code","4c41dbb4":"code","61b3f952":"code","459b4d20":"code","8999a365":"code","c89b54e5":"code","589743be":"code","1588403f":"code","20df0043":"code","487b9f6a":"code","93742809":"markdown","37d1322b":"markdown","b46f2487":"markdown","b8321b47":"markdown","aa4237fe":"markdown","ef15c1f2":"markdown","5a2c688d":"markdown","ea25b11b":"markdown","7bd89fe6":"markdown","762c76d5":"markdown","488e07f9":"markdown","c5023579":"markdown","8e862d12":"markdown","7f5bb517":"markdown"},"source":{"a9006eac":"from copy import deepcopy\nimport json\nimport random\nimport time\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport torch\nimport tqdm\nfrom torch.utils import data\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torch.nn import functional as fnn\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nnp.random.seed(2205)\ntorch.manual_seed(2205)","74c0886c":"class CarPlatesDatasetWithRectangularBoxes(data.Dataset):\n    def __init__(self, root, transforms, split='train', train_size=0.9):\n        super(CarPlatesDatasetWithRectangularBoxes, self).__init__()\n        self.root = Path(root)\n        self.train_size = train_size\n        \n        self.image_names = []\n        self.image_ids = []\n        self.image_boxes = []\n        self.image_texts = []\n        self.box_areas = []\n        \n        self.transforms = transforms\n        \n        if split in ['train', 'val']:\n            plates_filename = self.root \/ 'train.json'\n            with open(plates_filename) as f:\n                json_data = json.load(f)\n            train_valid_border = int(len(json_data) * train_size) + 1 # \u0433\u0440\u0430\u043d\u0438\u0446\u0430 \u043c\u0435\u0436\u0434\u0443 train \u0438 valid\n            data_range = (0, train_valid_border) if split == 'train' \\\n                else (train_valid_border, len(json_data))\n            self.load_data(json_data[data_range[0]:data_range[1]]) # \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u0444\u0430\u0439\u043b\u043e\u0432 \u0438 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443\n            return\n\n        if split == 'test':\n            plates_filename = self.root \/ 'submission.csv'\n            self.load_test_data(plates_filename, split, train_size)\n            return\n\n        raise NotImplemented(f'Unknown split: {split}')\n        \n    def load_data(self, json_data):\n        for i, sample in enumerate(json_data):\n            if sample['file'] == 'train\/25632.bmp':\n                continue\n            self.image_names.append(self.root \/ sample['file'])\n            self.image_ids.append(torch.Tensor([i]))\n            boxes = []\n            texts = []\n            areas = []\n            for box in sample['nums']:\n                points = np.array(box['box'])\n                x_0 = np.min([points[0][0], points[3][0]])\n                y_0 = np.min([points[0][1], points[1][1]])\n                x_1 = np.max([points[1][0], points[2][0]])\n                y_1 = np.max([points[2][1], points[3][1]])\n                boxes.append([x_0, y_0, x_1, y_1])\n                texts.append(box['text'])\n                areas.append(np.abs(x_0 - x_1) * np.abs(y_0 - y_1))\n            boxes = torch.FloatTensor(boxes)\n            areas = torch.FloatTensor(areas)\n            self.image_boxes.append(boxes)\n            self.image_texts.append(texts)\n            self.box_areas.append(areas)\n        \n    \n    def load_test_data(self, plates_filename, split, train_size):\n        df = pd.read_csv(plates_filename, usecols=['file_name'])\n        for row in df.iterrows():\n            self.image_names.append(self.root \/ row[1][0])\n        self.image_boxes = None\n        self.image_texts = None\n        self.box_areas = None\n         \n    \n    def __getitem__(self, idx):\n        target = {}\n        if self.image_boxes is not None:\n            boxes = self.image_boxes[idx].clone()\n            areas = self.box_areas[idx].clone()\n            num_boxes = boxes.shape[0]\n            target['boxes'] = boxes\n            target['area'] = areas\n            target['labels'] = torch.LongTensor([1] * num_boxes)\n            target['image_id'] = self.image_ids[idx].clone()\n            target['iscrowd'] = torch.Tensor([False] * num_boxes)\n#             target['texts'] = self.image_texts[idx]\n\n        image = cv2.imread(str(self.image_names[idx]))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transforms is not None:\n            image = self.transforms(image)\n        return image, target\n\n    def __len__(self):\n        return len(self.image_names)","ea38e7bd":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\ndef create_model(device):\n    # load a model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    # replace the classifier with a new one, that has\n    # num_classes which is user-defined\n    num_classes = 2  # 1 class (person) + background\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model.to(device)\n\n# \u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f dataloader'\u0430\ndef collate_fn(batch):\n    return tuple(zip(*batch))","88ca044c":"transformations= transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n                    ])","cd1cacaf":"# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nmodel = create_model(device)\n\n# use our dataset and defined transformations\ntrain_dataset = CarPlatesDatasetWithRectangularBoxes('data', transformations, 'train')\nval_dataset = CarPlatesDatasetWithRectangularBoxes('data', transformations, 'val')\ntest_dataset = CarPlatesDatasetWithRectangularBoxes('data', transformations, 'test')\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=collate_fn)\n\nval_loader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=2, shuffle=False, num_workers=4,\n    collate_fn=collate_fn)","cb36b4ec":"# \u0427\u0430\u0441\u0442\u044c \u043a\u043e\u0434\u0430 \u0432\u0437\u044f\u0442\u0430 \u0438\u0437  pytorch utils\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\n\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    model.train()\n\n    for images, targets in tqdm.tqdm(train_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n    \n    batch_losses = []\n    for images, targets in tqdm.tqdm(val_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        batch_losses.append(losses.item())\n        optimizer.zero_grad()\n    \n    batch_losses = np.array(batch_losses)\n    batch_losses = batch_losses[np.isfinite(batch_losses)]\n    print(f'Valid_loss: {np.mean(batch_losses)}')\n    lr_scheduler.step()\n\nprint(\"That's it!\")","25eedb91":"print(f'\u0421\u0440\u0435\u0434\u043d\u0438\u0439 \u043b\u043e\u0441\u0441 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438: {np.mean(batch_losses)}')","66ee6a19":"# \u0421\u043e\u0445\u0440\u0430\u043d\u0438\u043b\u0438\n# with open('fasterrcnn_resnet50_fpn_1_epoch', 'wb') as fp:\n#     torch.save(model.state_dict(), fp)","dc9c309f":"# \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043b\u0438\nwith open('fasterrcnn_resnet50_fpn_1_epoch', 'rb') as fp:\n    state_dict = torch.load(fp, map_location=\"cpu\")\nmodel.load_state_dict(state_dict)\nmodel.to(device)","5d3cc5aa":"unnormalize_1 = transforms.Normalize(mean=[-0.485, -0.456, -0.406],\n                                         std=[1, 1, 1])\nunnormalize_2 = transforms.Normalize(mean=[0, 0, 0],\n                                         std=[1\/0.229, 1\/0.224, 1\/0.225])\nunnormalize = transforms.Compose([unnormalize_2, unnormalize_1])\n\nstart = 2\n\nimages = []\nfor i in range(start, start + 2):\n    images.append(val_dataset[i][0].to(device))","2cd8c9c8":"def detach_dict(pred):\n    return{k:v.detach().cpu() for (k,v) in pred.items()}\n\nmodel.eval()\npreds = model(images)\npreds = [detach_dict(pred) for pred in preds]","e639093b":"preds","f5421f84":"fig,ax = plt.subplots(1, 2, figsize = (20, 8))\n\nfor i in range(2):\n    image = unnormalize(images[i].clone().cpu())\n    ax[i].imshow(image.numpy().transpose([1,2,0]))\n    for box in preds[i]['boxes']:\n        box = box.detach().cpu().numpy()\n        rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=1,edgecolor='r',facecolor='none')\n        ax[i].add_patch(rect)\n\nplt.show()","8fd1e0b0":"class CarPlatesFragmentsDataset(data.Dataset):\n    def __init__(self, root, transforms, split='train', train_size=0.9):\n        super(CarPlatesFragmentsDataset, self).__init__()\n        self.root = Path(root)\n        self.train_size = train_size\n        \n        self.image_names = []\n        self.image_ids = []\n        self.image_boxes = []\n        self.image_texts = []\n        self.box_areas = []\n        \n        self.transforms = transforms\n        \n        if split in ['train', 'val']:\n            plates_filename = self.root \/ 'train.json'\n            with open(plates_filename) as f:\n                json_data = json.load(f)\n            train_valid_border = int(len(json_data) * train_size) + 1 # \u0433\u0440\u0430\u043d\u0438\u0446\u0430 \u043c\u0435\u0436\u0434\u0443 train \u0438 valid\n            data_range = (0, train_valid_border) if split == 'train' \\\n                else (train_valid_border, len(json_data))\n            self.load_data(json_data[data_range[0]:data_range[1]]) # \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u0444\u0430\u0439\u043b\u043e\u0432 \u0438 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443\n            return\n\n        if split == 'test':\n            plates_filename = self.root \/ 'test_boxes.json'\n            with open(plates_filename) as f:\n                json_data = json.load(f)\n            self.load_test_data(json_data)\n            return\n            \n        raise NotImplemented(f'Unknown split: {split}')\n        \n    def load_data(self, json_data):\n        for i, sample in enumerate(json_data):\n            if sample['file'] == 'train\/25632.bmp':\n                continue\n            for box in sample['nums']:\n                points = np.array(box['box'])\n                x_0 = np.min([points[0][0], points[3][0]])\n                y_0 = np.min([points[0][1], points[1][1]])\n                x_1 = np.max([points[1][0], points[2][0]])\n                y_1 = np.max([points[2][1], points[3][1]])\n                if x_0 > x_1 or y_0 > y_1:\n                    # \u0415\u0441\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432, \u043a\u043e\u0433\u0434\u0430 \u0442\u043e\u0447\u043a\u0438 \u043f\u0440\u043e\u043d\u0443\u043c\u0435\u0440\u043e\u0432\u0430\u043d\u044b \u0432 \u0434\u0440\u0443\u0433\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435 - \u043f\u043e\u043a\u0430 \u043d\u0435 \u0432\u044b\u044f\u0441\u043d\u044f\u0435\u043c\n                    continue\n                self.image_boxes.append(np.clip([x_0, y_0, x_1, y_1], a_min=0, a_max=None))\n                self.image_texts.append(box['text'])\n                self.image_names.append(self.root \/ sample['file'])\n                \n    def load_test_data(self, json_data):\n        for i, sample in enumerate(json_data):\n            for box in sample['boxes']:\n                if box[0] >= box[2] or box[1] >= box[3]:\n                    continue\n                points = np.array(box)\n                self.image_boxes.append(np.clip(points, a_min=0, a_max=None))\n                self.image_names.append(sample['file'])\n        self.image_texts = None\n    \n    def __getitem__(self, idx):\n        target = {}\n        image = cv2.imread(str(self.image_names[idx]))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        target = {}\n        if self.image_boxes is not None:\n            box = self.image_boxes[idx]\n            image = image.copy()[box[1]:box[3], box[0]:box[2]]\n            \n        if self.image_texts is not None:\n            target['text'] = self.image_texts[idx]\n\n        if self.transforms is not None:\n            image = self.transforms(image)\n        return image, target\n\n    def __len__(self):\n        return len(self.image_names)","29cf90dd":"transformations= transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n                    ])\n\ntrain_dataset = CarPlatesFragmentsDataset('data', transformations, 'train')","260ec01f":"# \u0424\u0443\u043d\u0446\u0438\u0438 \u0434\u043b\u044f \u043f\u043e\u0438\u0441\u043a\u0430 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0445 \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432 \u0432 \u043d\u043e\u043c\u0435\u0440\u0435\n\ndef auto_canny(image, sigma=0.33):\n    # compute the median of the single channel pixel intensities\n    v = np.median(image)\n \n    # apply automatic Canny edge detection using the computed median\n    lower = int(max(0, (1.0 - sigma) * v))\n    upper = int(min(255, (1.0 + sigma) * v))\n    edged = cv2.Canny(image, lower, upper)\n \n    # return the edged image\n    return edged\n\ndef find_number_boxes(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    thresh_inv = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY_INV,39,1)\n    edges = auto_canny(thresh_inv)\n    ctrs, hier = cv2.findContours(edges.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n    sorted_ctrs = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])\n    img_area = image.shape[0]*image.shape[1]\n    boxes = []\n    for i, ctr in enumerate(sorted_ctrs):\n        x, y, w, h = cv2.boundingRect(ctr)\n        roi_area = w*h\n        roi_ratio = roi_area\/img_area\n        if((roi_ratio >= 0.015) and (roi_ratio < 0.09)):\n            if ((h>1.2*w) and (3*w>=h)):\n                boxes.append([x, y, w, h])\n    # \u041e\u0441\u0442\u0430\u0432\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435\n    unique_boxes = []\n    for box in boxes:\n        if box not in unique_boxes:\n            unique_boxes.append(box)\n    # \u0423\u0431\u0435\u0440\u0435\u043c \u0432\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u0435 \u0438 \u0441\u0438\u043b\u044c\u043d\u043e\u043f\u0435\u0440\u0435\u0441\u0435\u043a\u0430\u044e\u0449\u0438\u0435\u0441\u044f\n    box_num = len(unique_boxes)\n    valid_boxes = [True] * box_num\n    i = 0\n    while i < box_num:\n        if not valid_boxes[i]:\n            i += 1\n            continue\n        area_i = unique_boxes[i][2] * unique_boxes[i][3]\n        j = i + 1\n        while j < box_num:\n            if not valid_boxes[j]:\n                j += 1\n                continue\n            # \u041d\u0430\u0445\u043e\u0434\u0438\u043c \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u044f\n            left = max(unique_boxes[i][0], unique_boxes[j][0])\n            right = min(unique_boxes[i][0] + unique_boxes[i][2], unique_boxes[j][0] + unique_boxes[j][2])\n            top = max(unique_boxes[i][1], unique_boxes[j][1])\n            bottom = min(unique_boxes[i][1] + unique_boxes[i][3], unique_boxes[j][1] + unique_boxes[j][3])\n            if left >= right or top >= bottom:\n                j += 1\n                continue\n            intersection_area = (right - left) * (bottom - top)\n            area_j = unique_boxes[j][2] * unique_boxes[j][3]\n            share_i = intersection_area \/ area_i\n            share_j = intersection_area \/ area_j\n            if share_i >= share_j:\n                if share_i > 0.75:\n                    valid_boxes[i] = False\n            else:\n                if share_j > 0.75:\n                    valid_boxes[j] = False\n            j += 1\n        i += 1\n    boxes = []\n    for i, box in enumerate(unique_boxes):\n        if valid_boxes[i]:\n            boxes.append(box)\n        \n    return boxes","6d1c1b6e":"train_dataset = CarPlatesFragmentsDataset('data', transformations, 'train', train_size=1)\nidx = 26793\nimg_for_search = train_dataset[idx][0].clone().cpu()\nimg_for_search = unnormalize(img_for_search).numpy().transpose([1,2,0])\nimg_for_search = (img_for_search * 255).astype(np.uint8)\nboxes = find_number_boxes(img_for_search)\nprint(train_dataset[idx][1]['text'])\nimage = unnormalize(train_dataset[idx][0].clone().cpu())\nfig, ax = plt.subplots(1)\nax.imshow(image.numpy().transpose([1,2,0]))\nfor box in boxes:\n    rect = patches.Rectangle((box[0],box[1]),box[2],box[3],linewidth=1,edgecolor='r',facecolor='none')\n    ax.add_patch(rect)","3aa46537":"train_dataset = CarPlatesFragmentsDataset('data', transformations, 'train', train_size=1)\nvalid_number_of_letters = 0\nfor sample in tqdm.tqdm(train_dataset):\n    text = sample[1]['text']\n    img_for_search = unnormalize(sample[0].clone().cpu()).numpy().transpose([1,2,0])\n    img_for_search = (img_for_search * 255).astype(np.uint8)\n    boxes = find_number_boxes(img_for_search)\n    if len(text) == len(boxes):\n        valid_number_of_letters += 1","6ad63c8f":"print(f'\u0412\u0441\u0435\u0433\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439, \u0433\u0434\u0435 \u043d\u0430\u0448\u043b\u0438 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432 {valid_number_of_letters} \u0438\u0437 {len(train_dataset)}')","680da839":"path = Path('data')\nsymbols_path = path \/ 'symbols'\nsymbols_path.mkdir(exist_ok=True)","bfa2f5ac":"num = 0\nlabels = {}\nfor sample in tqdm.tqdm(train_dataset):\n    text = sample[1]['text']\n    img_for_search = unnormalize(sample[0].clone().cpu()).numpy().transpose([1,2,0])\n    img_for_search = (img_for_search * 255).astype(np.uint8)\n    boxes = find_number_boxes(img_for_search)\n    if len(text) == len(boxes):\n        for i in range(len(boxes)):\n            file_name = f'{num}.jpg'\n            labels[file_name] = text[i]\n            box = boxes[i]\n            symbol = img_for_search.copy()[box[1]:box[1]+box[3], box[0]:box[0]+box[2]]\n            cv2.imwrite(str(symbols_path \/ file_name), symbol )\n            num += 1","df0aeab3":"labels_df = pd.DataFrame(list(labels.items()), columns=['file_name', 'label'])\n\nlabels_df.loc[labels_df.label == '\u0410', 'label'] = 'A'\nlabels_df.loc[labels_df.label == '\u041e', 'label'] = 'O'\nlabels_df.loc[labels_df.label == '\u041d', 'label'] = 'H'\nlabels_df.loc[labels_df.label == '\u041a', 'label'] = 'K'\nlabels_df.loc[labels_df.label == '\u0421', 'label'] = 'C'\nlabels_df.loc[labels_df.label == '\u0420', 'label'] = 'P'\nlabels_df.loc[labels_df.label == '\u0412', 'label'] = 'B'\nlabels_df.loc[labels_df.label == '\u0425', 'label'] = 'X'\nlabels_df.loc[labels_df.label == '\u0415', 'label'] = 'E'\nlabels_df.loc[labels_df.label == '\u0422', 'label'] = 'T'\nlabels_df.loc[labels_df.label == '\u041c', 'label'] = 'M'\nlabels_df.loc[labels_df.label == '\u0435', 'label'] = 'E'\nlabels_df.loc[labels_df.label == 'o', 'label'] = 'O'\nlabels_df.loc[labels_df.label == '\u043c', 'label'] = 'M'\nlabels_df.loc[labels_df.label == 'e', 'label'] = 'E'\nlabels_df.loc[labels_df.label == '\u043a', 'label'] = 'K'\nlabels_df.loc[labels_df.label == '\u0423', 'label'] = 'Y'\nlabels_df.loc[labels_df.label == '\u043e', 'label'] = 'O'\nlabels_df.loc[labels_df.label == '\u0432', 'label'] = 'B'\nlabels_df.loc[labels_df.label == 'y', 'label'] = 'Y'\n\nlabels_df.to_csv('data\/symbols.csv', index=False)","d11e104a":"print(f'\u0412\u0441\u0435\u0433\u043e \u0440\u0430\u0437\u043d\u044b\u0445 \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432: {labels_df.label.nunique()}')","aacfde2b":"class SymbolsDataset(data.Dataset):\n    def __init__(self, root, transforms, split='train', train_size=0.9):\n        super(SymbolsDataset, self).__init__()\n        self.root = Path(root)\n        self.image_path = self.root \/ 'symbols'\n        self.train_size = train_size\n        self.label_to_class = {}\n        self.class_to_label = {}\n        self.data = None\n        \n        self.transforms = transforms\n        \n        symbols_filename = self.root \/ 'symbols.csv'\n        labels_df = pd.read_csv(symbols_filename)\n        for i, ch in enumerate(labels_df.label.unique()):\n            self.label_to_class[ch] = i\n            self.class_to_label[i] = ch\n        \n        if split in ['train', 'val']:\n            train_valid_border = int(len(labels_df) * train_size) + 1 # \u0433\u0440\u0430\u043d\u0438\u0446\u0430 \u043c\u0435\u0436\u0434\u0443 train \u0438 valid\n            if split == 'train':\n                self.data = labels_df.iloc[:train_valid_border]\n            else:\n                self.data = labels_df.iloc[train_valid_border:]\n            return\n\n        raise NotImplemented(f'Unknown split: {split}')\n    \n    def __getitem__(self, idx):\n        target = {}\n        file_name = self.image_path \/ self.data.iloc[idx].file_name\n        label = self.data.iloc[idx].label\n        image = cv2.imread(str(file_name))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #COLOR_BGR2GRAY\n        image = cv2.resize(image, (75, 100))\n        \n        if self.transforms is not None:\n            image = self.transforms(image)\n        return image, torch.LongTensor([self.label_to_class[label]])\n\n    def __len__(self):\n        return len(self.data)","5aa59a56":"symbol_transforms= transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n                    ])\nsymbol_train_dataset = SymbolsDataset('data', symbol_transforms, 'train')\nsymbol_train_dataloader = torch.utils.data.DataLoader(\n        symbol_train_dataset, batch_size=64, shuffle=True, num_workers=4)\n\nsymbol_val_dataset = SymbolsDataset('data', symbol_transforms, 'val')\nsymbol_val_dataloader = torch.utils.data.DataLoader(\n        symbol_val_dataset, batch_size=64, shuffle=False, num_workers=4)","2e4b51f5":"batch = next(iter(symbol_train_dataloader))\nfig, axes = plt.subplots(4, 4, figsize=(20, 20))\nfor i, sample in enumerate(list(zip(*batch))[:16]):\n    axes[i \/\/ 4, i % 4].imshow(sample[0].numpy().transpose([1,2,0]))\n    axes[i \/\/ 4, i % 4].set_title(symbol_train_dataset.class_to_label[sample[1].item()])","b781ea52":"def create_symbol_classifier(num_symbols, devide):\n    model = models.alexnet(pretrained=False)\n    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_symbols, bias=True)\n    return model.to(device)","dced533d":"num_symbols = 22\nclassifier = create_symbol_classifier(num_symbols, device)","8ea69f5f":"optimizer = optim.Adam(classifier.parameters(), lr=1e-4, amsgrad=True)\nloss_fn = fnn.cross_entropy","9fc112f0":"num_epochs = 5\nfor epoch in range(num_epochs):\n    print(f'Epoch #{epoch + 1}')\n    time.sleep(0.5)\n    train_loss = []\n    model.train()\n    for batch in tqdm.tqdm(symbol_train_dataloader, total=len(symbol_train_dataloader), desc=\"training...\"):\n        images = batch[0].to(device)\n        classes = batch[1].squeeze()\n\n        pred_classes = classifier(images).cpu()\n        loss = loss_fn(pred_classes, classes, reduction=\"mean\")\n        train_loss.append(loss.item())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'Train loss: {np.mean(train_loss):0.4f}')\n    model.eval()\n    valid_loss = []\n    correct_predictions = 0\n    for batch in tqdm.tqdm(symbol_val_dataloader, total=len(symbol_val_dataloader), desc=\"validation...\"):\n        images = batch[0].to(device)\n        classes = batch[1].squeeze()\n        pred_classes = classifier(images).cpu()\n        loss = loss_fn(pred_classes, classes, reduction=\"mean\")\n        valid_loss.append(loss.item())\n        \n        pred_class = torch.argmax(pred_classes, axis=1)\n        correct_predictions += torch.sum(classes == pred_class).item()\n    print(f'Valid loss: {np.mean(valid_loss):0.4f}')\n    print(f'Validation accuracy :{correct_predictions \/ len(symbol_val_dataloader):0.2f}, correct: {correct_predictions}')","775877da":"batch = next(iter(symbol_train_dataloader))\noutput = classifier(batch[0].to(device))\npredictions = torch.argmax(output, axis=1).cpu().numpy()\nground_true = batch[1].squeeze().numpy()\npredictions = [symbol_val_dataset.class_to_label[c] for c in predictions]\nground_true = [symbol_val_dataset.class_to_label[c] for c in ground_true]\nfig, axes = plt.subplots(4, 4, figsize=(20, 20))\nfor i, sample in enumerate(list(zip(*batch))[:16]):\n    axes[i \/\/ 4, i % 4].imshow(sample[0].numpy().transpose([1,2,0]))\n    axes[i \/\/ 4, i % 4].set_title(f'Pred: {predictions[i]} True: {ground_true[i]}')","d0b9607c":"# \u0421\u043e\u0445\u0440\u0430\u043d\u0438\u043b\u0438\n# with open('alexnet_symbol_classifier', 'wb') as fp:\n#     torch.save(classifier.state_dict(), fp)","25b12298":"# \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043b\u0438\nwith open('alexnet_symbol_classifier', 'rb') as fp:\n    state_dict = torch.load(fp, map_location=\"cpu\")\nclassifier.load_state_dict(state_dict)\nclassifier.to(device)","5e3ddd4e":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# \u041c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u043d\u043e\u043c\u0435\u0440\u043d\u044b\u0445 \u0437\u043d\u0430\u043a\u043e\u0432 \u043d\u0430 \u0444\u043e\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u044f\u0445\nmodel = create_model(device)\n\n# \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043b\u0438\nwith open('fasterrcnn_resnet50_fpn_1_epoch', 'rb') as fp:\n    state_dict = torch.load(fp, map_location=\"cpu\")\nmodel.load_state_dict(state_dict)\nmodel.to(device)\n\n# Test dataset\ntest_dataset = CarPlatesDatasetWithRectangularBoxes('data', transformations, 'test')\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=2, shuffle=False, num_workers=4,\n    collate_fn=collate_fn)","cca6df0b":"predicted_boxes = []\nfor batch in tqdm.tqdm(test_loader):\n    images = list(image.to(device) for image in batch[0])\n    model.eval()\n    preds = model(images)\n    preds = [{k: v.detach().cpu().numpy() for k, v in prediction.items()} for prediction in preds]\n    predicted_boxes.extend(preds)","efd758da":"boxes = [box.astype(int).tolist() for box in (boxes_in_image['boxes'] for boxes_in_image in predicted_boxes)]\nassert len(boxes) == len(test_dataset)","fda6928c":"json_data = []\nfor file_name, box in zip(test_dataset.image_names, boxes):\n    json_data.append({'boxes': box, 'file': str(file_name)})\n\nwith open('data\/test_boxes.json', 'w') as fp:\n    json.dump(json_data, fp)","50a3aa30":"transformations = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n                    ])\n\ntest_plates_dataset = CarPlatesFragmentsDataset('data', transformations, 'test')","4c41dbb4":"idx = 0\nimg_for_search = test_plates_dataset[idx][0].clone().cpu()\nimg_for_search = unnormalize(img_for_search).numpy().transpose([1,2,0])\nimg_for_search = (img_for_search * 255).astype(np.uint8)\nboxes = find_number_boxes(img_for_search)\nimage = unnormalize(test_plates_dataset[idx][0].clone().cpu())\nfig, ax = plt.subplots(1)\nax.imshow(image.numpy().transpose([1,2,0]))\nfor box in boxes:\n    rect = patches.Rectangle((box[0],box[1]),box[2],box[3],linewidth=1,edgecolor='r',facecolor='none')\n    ax.add_patch(rect)","61b3f952":"num_symbols = 22\nclassifier = create_symbol_classifier(num_symbols, device)\n\nwith open('alexnet_symbol_classifier', 'rb') as fp:\n    state_dict = torch.load(fp, map_location=\"cpu\")\nclassifier.load_state_dict(state_dict)\nclassifier.to(device)","459b4d20":"transformations = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n                    ])","8999a365":"symbol_val_dataset = SymbolsDataset('data', None, 'val')\nclass_to_label = symbol_val_dataset.class_to_label","c89b54e5":"submit = {}\nfor idx in tqdm.tqdm(range(len(test_plates_dataset))):\n    img_for_search = test_plates_dataset[idx][0].clone().cpu()\n    file_name = test_plates_dataset.image_names[idx][5:]\n    img_for_search = unnormalize(img_for_search).numpy().transpose([1,2,0])\n    img_for_search = (img_for_search * 255).astype(np.uint8)\n    symbols = find_number_boxes(img_for_search)\n    symbols_array = []\n    if file_name not in submit:\n            submit[file_name] = []\n    if symbols:\n        for symbol in symbols:\n            symbol_img = img_for_search.copy()[symbol[1]:symbol[1]+symbol[3], symbol[0]:symbol[0]+symbol[2]]\n            symbol_img = cv2.resize(symbol_img, (75, 100))\n            symbols_array.append(transformations(symbol_img))\n        batch = torch.stack(symbols_array).to(device)\n        output = classifier(batch)\n        predictions = torch.argmax(output, axis=1).cpu().numpy()\n        plate = ''.join([class_to_label[p] for p in predictions])\n        submit[file_name].append(plate)","589743be":"submit = [(k, ' '.join(v)) for k,v in submit.items()]\nsubmission = pd.DataFrame(submit, columns=['file_name', 'plates_string'])","1588403f":"random_submission = pd.read_csv('submission.csv')","20df0043":"submission = pd.merge(random_submission, submission, how='left', on='file_name')\nsubmission.drop('plates_string_x', axis=1, inplace=True)\nsubmission.columns = ['file_name', 'plates_string']\nsubmission","487b9f6a":"submission.to_csv('baseline.csv', index=False)","93742809":"#### \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432","37d1322b":"## 3. \u041e\u0442\u0434\u0435\u043b\u044c\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e \u0444\u0440\u0430\u0433\u043c\u0435\u043d\u0442\u0443 \u0444\u043e\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u0438 \u0441 \u043d\u043e\u043c\u0435\u0440\u043e\u043c, \u0431\u0443\u0434\u0435\u0442 \u0435\u0433\u043e \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u043e\u0432\u0430\u0442\u044c","b46f2487":"### \u0420\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0435\u043c \u043d\u043e\u043c\u0435\u0440\u043d\u044b\u0435 \u0437\u043d\u0430\u043a\u0438 \u043f\u043e\u0441\u0438\u043c\u0432\u043e\u043b\u044c\u043d\u043e","b8321b47":"## \u0421\u043e\u0431\u0435\u0440\u0435\u043c \u0432\u0441\u0435 \u0432\u043c\u0435\u0441\u0442\u0435 \u0438 \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u043b\u044f test_data","aa4237fe":"## 2. \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c 1 \u044d\u043f\u043e\u0445\u0443","ef15c1f2":"### \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442:","5a2c688d":"###  \u041c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432","ea25b11b":"### \u041c\u0438\u043d\u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430","7bd89fe6":"### Dataset \u0434\u043b\u044f \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432","762c76d5":"## 1. Dataset, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u0435\u0442\u0438 \u043f\u043e\u0438\u0441\u043a\u0430 bbox'\u043e\u0432 \u043d\u043e\u043c\u0435\u0440\u043e\u0432 - \u044d\u0442\u043e \u043f\u0435\u0440\u0432\u044b\u0439 \u044d\u0442\u0430\u043f","488e07f9":"### \u0421\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432 \u0444\u0430\u0439\u043b","c5023579":"## \u0421\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0432\u0441\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0435\u0441\u0442\u044c \u043d\u0430 \u043d\u043e\u043c\u0435\u0440\u0430\u0445, \u0433\u0434\u0435 \u043c\u044b \u043d\u0430\u0448\u043b\u0438 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432 \u0432 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u0443\u044e \u043f\u0430\u043f\u043a\u0443 \u0438 \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0444\u0430\u0439\u043b \u0441 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u043e\u0439","8e862d12":"### \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0447\u0442\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c:","7f5bb517":"## \u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043b\u044f \u043d\u0430\u0440\u0435\u0437\u0430\u043d\u0438\u044f \u043d\u0430 \u0431\u0443\u043a\u0432\u044b"}}