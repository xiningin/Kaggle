{"cell_type":{"84a9c164":"code","7518ee7e":"code","624d008e":"code","cc634fdd":"code","e1cb1c57":"code","ece6d045":"code","6ac8e7d4":"code","42f86d2f":"code","21afc32c":"code","231afed8":"code","31d96675":"code","e1f8933d":"code","fa4c6db4":"code","0bd77cc2":"code","fa6531ea":"code","e06d0a03":"code","3329e556":"code","6f6152d8":"code","ae7bd4a7":"code","59b6cdb1":"code","8a444775":"code","39efda18":"code","76ca3829":"code","2adfbc08":"code","5993a351":"code","c1f2fa34":"code","8c04bda3":"code","49fb47c5":"code","003b947f":"code","a02857c9":"code","064de7ee":"code","9fc2a1de":"code","bce63ad0":"code","477cd77d":"code","cd864d17":"code","94f1ded1":"code","bc8d0a72":"code","bf1fcc29":"code","614d2cdf":"code","2f37512e":"code","3ade718c":"code","6dc97a36":"code","a4b3fb26":"code","07deed26":"code","aff305b9":"code","88a9516a":"code","6fc40eae":"code","77d30e23":"code","748ff0e2":"code","9896f0bf":"code","3f55471c":"code","79c8929b":"code","0a01a916":"code","612e3238":"code","b825a1d6":"code","b84bf1fd":"code","80e52943":"code","a14f02c2":"code","5e34b8cb":"markdown","e0fc8c92":"markdown","5690d6e9":"markdown","eee08631":"markdown","3fe784c0":"markdown","4f76831f":"markdown","e231d456":"markdown","922db436":"markdown","2e73726d":"markdown","8c30e897":"markdown","e35242dc":"markdown","4f3fc778":"markdown","5ea4dca7":"markdown","73c9190c":"markdown","9755044d":"markdown","f2545e31":"markdown","2f7559f8":"markdown","e7ca2fb2":"markdown","49f14cda":"markdown","eaef94b7":"markdown","0dd8cacd":"markdown","81fb42dd":"markdown","793e67cd":"markdown","e2d2a59f":"markdown","c8496c0f":"markdown"},"source":{"84a9c164":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport missingno as msno \nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom scipy.stats import randint \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import RandomizedSearchCV \nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier","7518ee7e":"def display_confusion_matrix(y_test,y_pred):\n  cm = confusion_matrix(y_test, y_pred_lr)\n  group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n  group_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\n  group_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm.flatten()\/np.sum(cm)]\n  labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\n  labels = np.asarray(labels).reshape(2,2)\n  sns.heatmap(cm, annot=labels, fmt=\"\", cmap=\"Blues\")\n  print(classification_report(y_test, y_pred))\n","624d008e":"# Reading the dataset\nckd_df = pd.read_csv('..\/input\/ckdisease\/kidney_disease.csv')","cc634fdd":"#check the columns\nckd_df.columns","e1cb1c57":"##Rename the columns to have meaningful names\ncol_dict={\"bp\":\"blood_pressure\",\n          \"sg\":\"specific_gravity\",\n          \"al\":\"albumin\",\n          \"su\":\"sugar\",\n          \"rbc\":\"red_blood_cells\",\n          \"pc\":\"pus_cell\",\n          \"pcc\":\"pus_cell_clumps\",\n          \"ba\":\"bacteria\",\n          \"bgr\":\"blood_glucose_random\",\n          \"bu\":\"blood_urea\",\n          \"sc\":\"serum_creatinine\",\n          \"sod\":\"sodium\",\n          \"pot\":\"potassium\",\n          \"hemo\":\"hemoglobin\",\n          \"pcv\":\"packed_cell_volume\",\n          \"wc\":\"white_blood_cell_count\",\n          \"rc\":\"red_blood_cell_count\",\n          \"htn\":\"hypertension\",\n          \"dm\":\"diabetes_mellitus\",\n          \"cad\":\"coronary_artery_disease\",\n          \"appet\":\"appetite\",\n          \"pe\":\"pedal_edema\",\n          \"ane\":\"anemia\"}\n\nckd_df.rename(columns=col_dict, inplace=True)","ece6d045":"#Check the column names again\nckd_df.columns","6ac8e7d4":"#Check the shape\nprint(ckd_df.shape)","42f86d2f":"# Check the data first\nckd_df.head(5)","21afc32c":"# Observing the summarized information of data\nckd_df.info()","231afed8":"#Check the number of NULL values in the train Dataset\nprint('Null values in Train Data: \\n', ckd_df.isnull().sum())","31d96675":"#Check the target value counts\nprint('Total Count of the Prediction Output Column Classification values: \\n', ckd_df['classification'].value_counts())","e1f8933d":"for c in ckd_df.columns:\n  print(c)\n  print(c,ckd_df[c].unique())","fa4c6db4":"#Replace incorrect values\nckd_df['diabetes_mellitus'] =ckd_df['diabetes_mellitus'].replace(to_replace={'\\tno':'no','\\tyes':'yes',' yes':'yes'})\nckd_df['coronary_artery_disease'] = ckd_df['coronary_artery_disease'].replace(to_replace='\\tno',value='no')\nckd_df['white_blood_cell_count'] = ckd_df['white_blood_cell_count'].replace(to_replace='\\t8400',value='8400')","0bd77cc2":"ckd_df[\"classification\"].value_counts()","fa6531ea":"ckd_df[\"classification\"]=ckd_df[\"classification\"].replace(\"ckd\\t\", \"ckd\")","e06d0a03":"# Looking at the statistial distribution of the data including categorical variables\nckd_df.describe(include='all').T","3329e556":"sns.countplot(x='classification',data=ckd_df)\nplt.xlabel(\"classification\")\nplt.ylabel(\"Count\")\nplt.title(\"target classification Distribution\")\nplt.show()","6f6152d8":"##Percent of target data\nprint(\"Percent of distribution below:\")\nckd_df[\"classification\"].value_counts()\/len(ckd_df)*100","ae7bd4a7":"#histograms for all continous variables\nckd_df.hist(['age','blood_pressure', 'blood_glucose_random', 'blood_glucose_random', \n         'blood_glucose_random', 'sodium', 'potassium',  \n         'packed_cell_volume', 'packed_cell_volume', 'red_blood_cell_count'], figsize=(30,15))","59b6cdb1":"#Check distribution between age and ckd presence\n# KDE is used to study PDF of a continuous RV\nckd_df[\"classification\"] = [1 if i == \"ckd\" else 0 for i in ckd_df[\"classification\"]]\nsns.jointplot(ckd_df.age, ckd_df.classification, kind=\"kde\", size=7)","8a444775":"# Check distribution of age for each target class\ng = sns.FacetGrid(ckd_df,col=\"classification\")\ng.map(sns.distplot,\"age\", bins=10)\nplt.show()","39efda18":"corr_df = ckd_df.corr()","76ca3829":"f,ax=plt.subplots(figsize=(15,15))\nsns.heatmap(corr_df,annot=True,fmt=\".2f\",ax=ax,linewidths=0.5,linecolor=\"orange\")\nplt.xticks(rotation=45)\nplt.yticks(rotation=45)\nplt.title('Correlations between different predictors')\nplt.show()","2adfbc08":"##Find missing values and impute them\n# Visualize missing values as a matrix \nmsno.matrix(ckd_df) ","5993a351":"## Visualize the missing values in a histogram\nmsno.bar(ckd_df)","c1f2fa34":"##Check if the values are good\nfor c in ckd_df.columns:\n  print(c,ckd_df[c].unique())","8c04bda3":"ckd_df[\"white_blood_cell_count\"]=ckd_df[\"white_blood_cell_count\"].replace(\"\\t?\", np.nan)\nckd_df[\"red_blood_cell_count\"]=ckd_df[\"red_blood_cell_count\"].replace(\"\\t?\", np.nan)\nckd_df['diabetes_mellitus'] = ckd_df['diabetes_mellitus'].replace(to_replace={'\\tno':'no','\\tyes':'yes',' yes':'yes'})\nckd_df['coronary_artery_disease'] = ckd_df['coronary_artery_disease'].replace(to_replace='\\tno',value='no')\nckd_df['white_blood_cell_count'] = ckd_df['white_blood_cell_count'].replace(to_replace='\\t8400',value='8400')\nckd_df[\"packed_cell_volume\"]= ckd_df[\"packed_cell_volume\"].replace(\"\\t?\", np.nan)","49fb47c5":"##Re-check if the values are good\nfor c in ckd_df.columns:\n  print(c,ckd_df[c].unique())","003b947f":"for string_column in [\"red_blood_cells\",\"pus_cell\",\"pus_cell_clumps\",\"bacteria\",\"hypertension\",\"diabetes_mellitus\",\"coronary_artery_disease\",\"pedal_edema\",\"anemia\",\"appetite\"]:\n  ckd_df[string_column]=ckd_df[string_column].astype(str)\n","a02857c9":"## Do some encoding to use KNN Imputer\nckd_df['red_blood_cells']=ckd_df['red_blood_cells'].replace({'normal':1,'abnormal':0})\nckd_df['pus_cell']=ckd_df['pus_cell'].replace({'normal':1,'abnormal':0})\nckd_df['pus_cell_clumps']=ckd_df['pus_cell_clumps'].replace({'notpresent':0,'present':1})\nckd_df['bacteria']=ckd_df['bacteria'].replace({'notpresent':0,'present':1})\nckd_df['hypertension']=ckd_df['hypertension'].replace({'no':0,'yes':1})\nckd_df['diabetes_mellitus']=ckd_df['diabetes_mellitus'].replace({'no':0,'yes':1})\nckd_df['coronary_artery_disease']=ckd_df['coronary_artery_disease'].replace({'no':0,'yes':1})\nckd_df['pedal_edema']=ckd_df['pedal_edema'].replace({'no':0,'yes':1})\nckd_df['anemia']=ckd_df['anemia'].replace({'no':0,'yes':1})\nckd_df['appetite']=ckd_df['appetite'].replace({'poor':0,'good':1})","064de7ee":"##List all columns with % NaNs\nprint (round((ckd_df.isnull().sum() * 100\/ len(ckd_df)),2).sort_values(ascending=False))","9fc2a1de":"# define imputer\nimputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')","bce63ad0":"impute_columns=list(set(ckd_df.columns)-set([\"classification\"]))\nprint(impute_columns)","477cd77d":"imputer.fit(ckd_df[impute_columns])","cd864d17":"X_trans=pd.DataFrame(imputer.transform(ckd_df[impute_columns]), columns=impute_columns)","94f1ded1":"X_trans.head(3)","bc8d0a72":"##List all columns with % NaNs\nprint (round((X_trans.isnull().sum() * 100\/ len(X_trans)),2).sort_values(ascending=False))","bf1fcc29":"X=X_trans[X_trans[\"id\"].isin(ckd_df[\"id\"])].drop([\"id\"],axis=1)","614d2cdf":"y=ckd_df[\"classification\"]","2f37512e":"X_prod=X_trans[X_trans[\"id\"].isin(ckd_df[\"id\"])].drop([\"id\"],axis=1)","3ade718c":"print(X.shape)\nprint(y.shape)\nprint(X_prod.shape)","6dc97a36":"X.info()","a4b3fb26":"cat_columns=[\"red_blood_cells\",\n  \"pus_cell\",\n  \"pus_cell_clumps\",\n  \"bacteria\",\n  \"hypertension\",\n  \"diabetes_mellitus\",\n  \"coronary_artery_disease\",\n  \"appetite\",\n  \"pedal_edema\",\n  \"anemia\"]","07deed26":"##Split train and test\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 4658)","aff305b9":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","88a9516a":"# Logistic Regression Hyper parameter tuning\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import GridSearchCV \n\nc_space = np.logspace(-5, 8, 15) \nparam_grid = {'C': c_space} \n\nlr = LogisticRegression() \n\nlr = GridSearchCV(lr, param_grid, cv = 5) \n\nlr.fit(X_train, y_train) \n# Print the tuned parameters and score \nprint(\"Tuned Logistic Regression Parameters: {}\".format(lr.best_params_)) \nprint(\"Best score is {}\".format(lr.best_score_))","6fc40eae":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","77d30e23":"y_pred_lr = lr.predict(X_test)\ndisplay_confusion_matrix(y_test, y_pred_lr)\naccuracy_lr=accuracy_score(y_test, y_pred_lr)\nprint(\"Accuracy of Logistic Regression is :\", accuracy_lr)","748ff0e2":"from sklearn.model_selection import RandomizedSearchCV\n\nhyperparam_combs = {\n    'max_depth': [4, 6, 8, 10, 12],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [2, 10, 20, 30, 40],\n    'max_features': [0.2, 0.4, 0.6, 0.8, 1],\n    'max_leaf_nodes': [8, 16, 32, 64, 128],\n    'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 2}, {0: 1, 1: 3}, {0: 1, 1: 4}, {0: 1, 1: 5}]\n}\n\nclf = RandomizedSearchCV(DecisionTreeClassifier(),\n                         hyperparam_combs,\n                         scoring='f1',\n                         random_state=1,\n                         n_iter=20)\n\ndt_model = clf.fit(X_train, y_train)\n\n# Print the tuned parameters and score \nprint(\"Tuned Decision Tree Parameters: {}\".format(dt_model.best_params_)) \nprint(\"Best score is {}\".format(dt_model.best_score_)) ","9896f0bf":"y_pred_dt = dt_model.predict(X_test)\ndisplay_confusion_matrix(y_test, y_pred_dt)\naccuracy_dt=accuracy_score(y_test, y_pred_dt)\nprint(\"Accuracy of Decision Tree is :\", accuracy_dt)","3f55471c":"# Parameters for Random Foresthypertuning\nparam_grid = {\"n_estimators\": np.arange(2, 300, 2),\n              \"max_depth\": np.arange(1, 28, 1),\n              \"min_samples_split\": np.arange(1,150,1),\n              \"min_samples_leaf\": np.arange(1,60,1),\n              \"max_leaf_nodes\": np.arange(2,60,1),\n              \"min_weight_fraction_leaf\": np.arange(0.1,0.4, 0.1)}\n\nrf = RandomizedSearchCV(RandomForestClassifier(),\n                         param_grid,\n                         scoring='f1',\n                         random_state=4658,\n                         n_iter=20)\n\nrf_model = rf.fit(X_train, y_train)\n\n# Print the tuned parameters and score \nprint(\"Tuned Random Tree Parameters: {}\".format(rf_model.best_params_)) \nprint(\"Best score is {}\".format(rf_model.best_score_)) ","79c8929b":"y_pred_rf = rf_model.predict(X_test)\ndisplay_confusion_matrix(y_test, y_pred_rf)\naccuracy_rf=accuracy_score(y_test, y_pred_rf)\nprint(\"Accuracy of Random Forests model is :\", accuracy_rf)","0a01a916":"params = {'depth':[2, 3, 4],\n          'loss_function': ['Logloss', 'CrossEntropy'],\n          'l2_leaf_reg':np.logspace(-20, -19, 3)\n}\n\ncb = RandomizedSearchCV(CatBoostClassifier(),\n                         params,\n                         scoring='f1',\n                         random_state=4658,\n                         n_iter=20)\n\ncb_model = cb.fit(X_train, y_train)\n\n# Print the tuned parameters and score \nprint(\"Tuned Catboost Parameters: {}\".format(cb_model.best_params_)) \nprint(\"Best score is {}\".format(cb_model.best_score_)) ","612e3238":"y_pred_cb = cb_model.predict(X_test)\ndisplay_confusion_matrix(y_test, y_pred_cb)\naccuracy_cb=accuracy_score(y_test, y_pred_cb)\nprint(\"Accuracy of CatBoost model is :\", accuracy_cb)","b825a1d6":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nparam_test ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}","b84bf1fd":"import lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n\nlgbm = RandomizedSearchCV(\n    estimator=lgbm_model, \n    param_distributions=param_test, \n    n_iter=20,\n    scoring='roc_auc',\n    cv=3,\n    refit=True,\n    random_state=4658,\n    verbose=True)\n \n \nlgbm_model = lgbm.fit(X_train, y_train)\n\n# Print the tuned parameters and score \nprint(\"Tuned LGBM Parameters: {}\".format(lgbm_model.best_params_)) \nprint(\"Best score is {}\".format(lgbm_model.best_score_)) ","80e52943":"y_pred_lgbm = lgbm_model.predict(X_test)\ndisplay_confusion_matrix(y_test, y_pred_lgbm)\naccuracy_lgbm=accuracy_score(y_test, y_pred_lgbm)\nprint(\"Accuracy of LGBM model is :\", accuracy_lgbm)","a14f02c2":"models= ['LogisticRegression', 'DecisionTrees', 'RandomForests', 'CatBoost', 'LGBM']\naccuracies = [accuracy_lr,accuracy_dt,accuracy_rf,accuracy_cb,accuracy_lgbm]\n\n# Figure Size \nfig, ax = plt.subplots(figsize =(16, 9)) \n\n# Horizontal Bar Plot \nax.barh(models, accuracies) \n\n# Remove axes splines \nfor s in ['top', 'bottom', 'left', 'right']: \n\tax.spines[s].set_visible(False) \n\n# Remove x, y Ticks \nax.xaxis.set_ticks_position('none') \nax.yaxis.set_ticks_position('none') \n\n# Add padding between axes and labels \nax.xaxis.set_tick_params(pad = 5) \nax.yaxis.set_tick_params(pad = 10) \n\n# Add x, y gridlines \nax.grid(b = True, color ='grey', \n\t\tlinestyle ='-.', linewidth = 0.5, \n\t\talpha = 0.2) \n\n# Show top values \nax.invert_yaxis() \n\n# Add annotation to bars \nfor i in ax.patches: \n\tplt.text(i.get_width()+0.2, i.get_y()+0.5, \n\t\t\tstr(round((i.get_width()), 2)), \n\t\t\tfontsize = 10, fontweight ='bold', \n\t\t\tcolor ='grey') \n\n# Add Plot Title \nax.set_title('Accuracies of different models', \n\t\t\tloc ='left') \n\n# Show Plot \nplt.show() ","5e34b8cb":"Inferences\n1. Any correlation coefficients close to +1 and -1 imply the features are highly correlated.\n2. hemo and pcv have correlation coefficient = 0.9. Highly correlated.\n3. Any correlationclose to 0 with target can be removed. Here **potassium** can be removed from the features.","e0fc8c92":"# STEP 6 : Predictive Models with hyperparameter tuning Section","5690d6e9":"# STEP 5 : Check correlation between features","eee08631":"Inference - Distribution is balanced and good. Target values need no correction.","3fe784c0":"Inference - all NaNs have been imputed","4f76831f":"# STEP 6 : Data preprocessing","e231d456":"# STEP 2 : Importing training dataset","922db436":"Inference - There are \\t? values, which need to be replaced","2e73726d":"Inference - there are 26 features and 400 rows. Small dataset for training.","8c30e897":"Inference - no imbalance in the target feature","e35242dc":"# STEP 3 : Exploratory Data Analysis\n","4f3fc778":"Inference - There are missing values for rbc, wbc. Given the small size of train dataset, there is imputation required.","5ea4dca7":"Inference - There are incorrect values for diabetes_mellitus, coronary_artery_disease and white_blood_cell_count which need to be corrected.\n\n","73c9190c":"# Predictor feature details:\nage - age<br>\nbp - blood pressure<br>\nsg - specific gravity<br>\nal - albumin<br>\nsu - sugar<br>\nrbc - red blood cells<br>\npc - pus cell<br>\npcc - pus cell clumps<br>\nba - bacteria<br>\nbgr - blood glucose random<br>\nbu - blood urea<br>\nsc - serum creatinine<br>\nsod - sodium<br>\npot - potassium<br>\nhemo - hemoglobin<br>\npcv - packed cell volume<br>\nwc - white blood cell count<br>\nrc - red blood cell count<br>\nhtn - hypertension<br>\ndm - diabetes mellitus<br>\ncad - coronary artery disease<br>\nappet - appetite<br>\npe - pedal edema<br>\nane - anemia<br>\nclass - class<br>","9755044d":"Inference - most of the features are float. The object features need verified categorical values","f2545e31":"Inferences:\n\n1. All features, including categorical are listed.\n2. The count value varies from 187 to 280. This means there are few features with missing values.\n3. The feature **id** is a running number and seems to be an unique identifier for each row. So, it is not an influencing feature.\n4. **age** is between 2 and 90. It is normally distributed.\n5. **blood_pressure** , **sodium** and **hemoglobin** are normally distributed.\n6. **blood_glucose_random** and **potassium** are right skewed\n7. **blood_urea** and **serum_creatinine** are left skewed.\n8. Categorical target value is well balanced.\n\n","2f7559f8":"If target variable's distribution is too skewed then the predictive modeling will not be possible. Bell curve is desirable but slightly positive skew or negative skew is also fine. To make sure there is a balance in the the distribution of each class otherwise it impacts the Machine Learning algorithms ability to learn all the classes","e7ca2fb2":"Inferences \n1. nockd - concentrated more around age=40\n2. ckd - concentrated more around age = 65","49f14cda":"Inference - Values look good now except for missing values \"\\t?\" and NaN. These will be treated in imputation","eaef94b7":"# STEP 4 : Distribution of all features\n","0dd8cacd":"INFERENCE - It can be seen that DT, RF and catboost give the best accuracies and their best parameters have been determined using RandomSearchCV.","81fb42dd":"# STEP 1 : Importing requisite libraries\n","793e67cd":"Inference - Many columns have missing values. Needs to be fixed","e2d2a59f":"# Problem Statement: \nCreate a Predictive model to predict if a patient is suffering from a chronic kidney disease or not?\n\n# Target Variable: \n'classification' ('ckd' or 'notckd', ckd - chronic kidney disease)\n\n# Predictors: \n'id', 'age', 'bp', 'sg', 'al', 'su', 'rbc', 'pc', 'pcc', 'ba', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wc', 'rc', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane'\n\n","c8496c0f":"Inferences\n1. For class nockd, the data is normally distributed\n2. For class ckd, the data is right skewed. There are larger number of values distributed on the right side of the plot. "}}