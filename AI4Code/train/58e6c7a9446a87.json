{"cell_type":{"e0a3a9a6":"code","3ac8347b":"code","7e87651c":"code","8071f4ca":"code","333e2394":"code","0749bc3e":"code","2fb3305f":"code","2e944274":"code","ef669480":"code","a0d0ae25":"code","4c499ec9":"code","7c211c4d":"code","b237e470":"code","bbb57f0f":"code","30daeb67":"code","d4a8145e":"code","7dbcfeb7":"code","d2ed0c9c":"code","8694db0b":"code","c37b805c":"code","f8da5631":"code","fb976b24":"code","02147356":"code","1e1408d6":"code","469ff0f5":"code","be95462a":"code","7b447317":"code","cc8ee30a":"code","192c5164":"code","8f77ead2":"code","8403a56c":"code","ee402c74":"code","14de6a9d":"code","a8dcefd2":"code","bef49df0":"code","96e8b6ff":"code","c123c337":"code","62de94d3":"code","2935d3aa":"code","2ecf1c26":"code","10150c3f":"markdown","798d642e":"markdown","095ef88a":"markdown","70377ca1":"markdown","7aca87f7":"markdown","8ce68c35":"markdown","b648420a":"markdown","0cd1ab99":"markdown","83e4b1af":"markdown","02f1eba7":"markdown","c0b90caf":"markdown","86a57cca":"markdown","5121740f":"markdown","d5c0002e":"markdown","f6b55389":"markdown","732c700a":"markdown","b509000b":"markdown","6f69c3ff":"markdown","05ba35bd":"markdown","b2987d62":"markdown","999cc420":"markdown","3cdd6aa9":"markdown","1ce5fa53":"markdown","0cfd59d1":"markdown","95b93377":"markdown","1c498334":"markdown","a0c1d558":"markdown","97b70d91":"markdown","cfdfebda":"markdown","4c6d5580":"markdown","3499878e":"markdown","31693c69":"markdown","8b2c7248":"markdown","bc1c4b99":"markdown","f4273a65":"markdown","6176cd09":"markdown","0a9fa0f0":"markdown","cb756dca":"markdown","ac4c118f":"markdown","73d385d0":"markdown","1c3a36cf":"markdown"},"source":{"e0a3a9a6":"! pip install yfinance","3ac8347b":"import pandas as pd \nimport plotly.express as px\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport missingno as msno\nimport yfinance as yf\n","7e87651c":"tsla = yf.Ticker(\"TSLA\")","8071f4ca":"tsla=tsla.history(period='max')\n","333e2394":"tsla.head()","0749bc3e":"tsla.info()","2fb3305f":"tsla.describe()","2e944274":"tsla.drop(['Dividends','Stock Splits'],axis=1, inplace=True)\ntsla.head()","ef669480":"tsla.dtypes","a0d0ae25":"tsla=tsla.asfreq('b')\ntsla.head()","4c499ec9":"tsla.isnull().sum()","7c211c4d":"msno.matrix(tsla)\n","b237e470":"for col in tsla.columns:\n    tsla[col]=tsla[col].fillna(method='ffill')","bbb57f0f":"tsla.isnull().sum()","30daeb67":"fig=px.line(tsla,x=tsla.index,y=tsla.Close,width=900,height=400)\nfig.update_layout(title='The Closing prices for TESLA',xaxis_title='Years',yaxis_title='Closing Price')\nfig.update_layout({'plot_bgcolor':'white'})\nfig.show()\n","d4a8145e":"#filtering the data\ndf=tsla['2020':]\nprint(df.shape)\n#creat a training data\ntrain_data = df[:int(len(df)*0.8)]\nprint(train_data.shape)","7dbcfeb7":"#creat a testing data\ntest_data=df[int(len(df)*0.8):]\nprint(test_data.shape)","d2ed0c9c":"#frist we will import from statsmodels mudel adfuller\nfrom statsmodels.tsa.stattools import adfuller\nresuts=adfuller(train_data['Close'])\nresuts","8694db0b":"p_value=resuts[1]\np_value","c37b805c":"# creating a column of closing return\ntrain_data['Close_return']=train_data.Close.diff()\ntrain_data.head()","f8da5631":"# plotting Close_return column\nfig=px.line(train_data,x=train_data.index[1:],y=train_data.Close_return[1:],width=1000,height=400)\nfig.update_layout(title='The Closing prices for TESLA',xaxis_title='Years',yaxis_title='Closing Price')\nfig.update_layout({'plot_bgcolor':'white'})\nfig.show()","fb976b24":"result1=adfuller(train_data['Close_return'][1:])\nresult1","02147356":"p_value=result1[1]\np_value","1e1408d6":"#importing plot_acf\nfrom statsmodels.graphics.tsaplots import plot_acf\nplot_acf(train_data['Close'],lags=40,zero=False,alpha=0.05)\nplt.show()","469ff0f5":"#importing plot_pacf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(train_data['Close'],zero=False,lags=40,alpha=0.05)\nplt.show()","be95462a":"from statsmodels.tsa.arima_model import ARIMA\nP=[]\nQ=[]\naic_arima=[]\nllf_arima=[]\nfor q in [0,1,2,3,4,5]:\n    for p in [0,1,2,3,4,5]:\n        try:\n            model=ARIMA(train_data['Close'],order=(p,1,q))\n            result=model.fit()\n            P.append(p)\n            Q.append(q)\n            aic_arima.append(result.aic)\n            llf_arima.append(result.llf)\n        except:\n            P.append(p)\n            Q.append(q)\n            aic_arima.append(None)\n            llf_arima.append(None)","7b447317":"arima_df=pd.DataFrame({'p':P,'q':Q,'aic':aic_arima,'log_likelihood':llf_arima})\narima_df.sort_values('log_likelihood',ascending=False)","cc8ee30a":"arima_df.sort_values('aic',ascending=True)","192c5164":"! pip install pmdarima","8f77ead2":"from pmdarima.arima import auto_arima\nmodel_auto = auto_arima(train_data.Close, start_p=0, start_q=0,\n                      test='adf',       # use adftest to find optimal 'd'\n                      max_p=5, max_q=5, # maximum p and q\n                      m=1,              # frequency of series\n                      d=None,           # let model determine 'd'\n                      seasonal=False,   # No Seasonality\n                      start_P=0, \n                      D=0, \n                      trace=True,\n                      error_action='ignore',  \n                      suppress_warnings=True, \n                      stepwise=True)\nmodel_auto","8403a56c":"model1=ARIMA(train_data.Close,order=(5,1,5))\nresult1=model1.fit(disp=-1)\nresult1.summary()","ee402c74":"resid1=result1.resid","14de6a9d":"model2=ARIMA(train_data.Close,order=(4,1,5))\nresult2=model2.fit(disp=-1)\nresult2.summary()","a8dcefd2":"resid2=result2.resid\n","bef49df0":"model_auto.summary()","96e8b6ff":"resid3=model_auto.resid()","c123c337":"print('the mean square error for ARIMA1 is',np.mean(np.square(resid1)))\nprint('the mean square error for ARIMA2 is',np.mean(np.square(resid2)))\nprint('the mean square error for ARIMA3 is',np.mean(np.square(resid3)))\n","62de94d3":"# Generate predictions\nsteps=len(test_data)\nfc,sc,conf = result1.forecast(steps=steps)\n","2935d3aa":"fc=pd.Series(fc,index=test_data[:457].index)\nlower=pd.Series(conf[:,0],index=test_data[:steps].index)\nupper=pd.Series(conf[:,1],index=test_data[:steps].index)","2ecf1c26":"# plot the tsla data\nplt.figure(figsize=(16,10))\nplt.plot(tsla.index,tsla.Close)\n# plot our predictions\nplt.plot( fc, color='r', label='forecast')\n\n# shade the area between your confidence limits\nplt.fill_between(lower.index, lower, \n               upper, color='pink')\n\n# set labels, legends and show plot\nplt.xlabel('Date')\nplt.ylabel('TESLA Stock Price - Close USD')\nplt.legend()\nplt.show()","10150c3f":"### stage 3\n<p>from stage two we've got there models ARIMA(4,1,5), ARIMA(5,1,5) and ARIMA(0,1,1)  . so now we will diagnose them and choose the best model of them according to mean squared error<\/p> ","798d642e":"##### Data types","095ef88a":"<p>here we will start the fun, we will apply model to our data to estimate<\/p>\n<p> we will apply<\/p>\n<ul>\n<li>ARIMA model <\/li>\n<\/ul>\n<p>And then will choose the best model according to AIC score and the lag likelihood<\/p>\n<b> so let's begain<\/b>","70377ca1":"##### set business days frequency","7aca87f7":"> According to <code>AIC Score<\/code> the best model is ARIMA(4,1,5)","8ce68c35":"###### ARIMA(4,1,5)","b648420a":"##### ARIMA model\n<p>we will fit the data to the model manually and with auto_arima<\/p>\nso let's start with manual one","0cd1ab99":"# Imprting the Packages","83e4b1af":"#### stage 4\n<p> so now it's time to forecast<\/p>","02f1eba7":"# Introduction\n<p><img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/bd\/Tesla_Motors.svg\"><\/p>\n<p> TESLA is an American electric vehicle and clean energy company.Tesla designs and manufactures electric cars, battery energy storage from home to grid-scale, solar panels and solar roof tiles, and related products and services.it's one of the most fastest growing company. this note book is consist of two parts.<\/p>\n<p>First part, we will extract TESLA's Stock prices using web scraping techniques<\/p>\n<p> secod part,  we will be trying to forecast Closing Prices for TESLA using ARIMA Model<\/p>\n<p> let's start with the First Part<\/p>\n<p> we will extract the data using <code>yfinance<\/code>library<\/p>\n<p>yfinance is a popular open source library developed by Ran Aroussi as a means to access the financial data available on Yahoo Finance.<\/p>\n<p>Yahoo Finance offers an excellent range of market data on stocks, bonds, currencies and cryptocurrencies. It also offers market news, reports and analysis and additionally options and fundamentals data- setting it apart from some of it\u2019s competitors.and <a herf=\"https:\/\/algotrading101.com\/learn\/yfinance-guide\/\">here<\/a> is a good report to be familiar withe the library<\/p>\n<p>so let's first import the packges we will be using in the two parts<\/p>\n    \n    \n","c0b90caf":"> so we can expect that the time series is a  stationarity but to make sure we will apply the dicky-fuller test once again ","86a57cca":"#####  Summary statistic","5121740f":"### <font color='blue'> Extracting The Data<\/font>\nUsing the `Ticker` module we can create an object that will allow us to access functions to extract data. To do this we need to provide the ticker symbol for the stock, here the company is Tesla and the ticker symbol is `TSLA`.","d5c0002e":">now we could say that our data is consist of <code>7<\/code> columns and <code>2862<\/code> rows so let's see some quick summary statistics of the data and it's types","f6b55389":">As you could see all the lage till 35 lag are significant","732c700a":"> So as you could see now the p-value is equal to (2.4402316495481924e-06) which means that this time series is stationarity ","b509000b":"#####  Checking dataset columns","6f69c3ff":"### <font color='blue'>Preprocessing<\/font>\n<p>now we are about to apply some data wrangling techniques to prepare our data for modling<\/p>\n<p>but as usual let's first list our todolist here<\/p>\n<ul>\n    <li>Checking dataset columns<\/li>\n    <li>Ckecking data types<\/li>\n    <li>set business days frequency<\/li>\n    <li>Checking the missing values<\/li>\n    <li>hendeling the missing values<\/li>\n    <li>Plotting the closing prices<\/li>\n<\/ul>\n<B>so let's get started<\/B>","05ba35bd":"> so we have <code>Dividends<\/code> and <code>Stock Splits<\/code> columns with almost no entries so we can drop them to clean the data a little bit","b2987d62":"to change the time series to be stationarity we will be using the returns of the closing prices instead of the closing prices itself. So let's make that change","999cc420":"### <font color='blue'>Extracting Share Prices<\/font>\nA share is the single smallest part of a company's stock  that you can buy, the prices of these shares fluctuate over time. Using the <code>history()<\/code> method we can get the share price of the stock over a certain period of time. Using the `period` parameter we can set how far back from the present to get data. The options for `period` are 1 day (1d), 5d, 1 month (1mo) , 3mo, 6mo, 1 year (1y), 2y, 5y, 10y, ytd, and max.","3cdd6aa9":"#### Stage one\n<p>in stage one, we will check if this time series is stationary or not.<\/p>\n<p>But befor we start let me first give you a quick brief of what is staionarity time series<\/p>\n<p><B>staionarity<\/B> means that the statistical properties of a time series do not change over time. and it's important because many useful analytical tools and statistical tests and models rely on it<\/p>\n<p> we're goning to use dicky-fuller test to diagnose if the time series is stationarity or not<\/p>\n","1ce5fa53":"> you could see the p-value is  equal to 0.73 which means that the time series is  not stationarity so there some models we can't use such as <code>AR<\/code> and <code>MR<\/code> for instance. to fix this problem we 'll be working with close prices returns.","0cfd59d1":"> so the best model is ARIMA(5,1,5). but i've noticed from the summary table that most of the coefficients aren't significant the p-values from most of them are higher than 0.05. ","95b93377":"### Checking the missing values","1c498334":">After setting the business days frequency on the data, missing values are generated <code>103<\/code> in each column. So now let's plot the distribution of the missing values using <code>missingno<\/code> library","a0c1d558":"##### plotting the close prices of the data\nfirst we will start by plotting <code>close<\/code> column of the data using <code>plotly<\/code> library ","97b70d91":"> from the plot you could see that our model is donig a decent job actually. ","cfdfebda":"# First Part","4c6d5580":"> so you can notice from the figure that at 2020 starters there was a big jump in the closing prices for TESLA compared to the other years","3499878e":"#### Stage two","31693c69":"###### ARIMA(0,1,1)","8b2c7248":"##### Handeling The Missing Values\n<p>there are a several methods and techinques to handel the missing valus,the method we will use here is called <code>Front Filling<\/code>.it assigns the value of the previos period <\/p>","bc1c4b99":">this method suggest ARIMA(0,1,1)","f4273a65":"Before we move to the next stage we have to plot the <code>ACF<\/code> and <code>PACF<\/code> plots to get some insights of the lags and the correlation between them","6176cd09":"# Second Part\n<p>Here we're going to continue our work on the TESLA project. so before we get started let me first  give you some intuition of what we're going to do <\/p>\n<ul>\n    <li>first we'll try to understand our data a little bit, we'll be looking at Columns and Data Types<\/li>\n    <li>Then we'll check the missing values in our data and if we found them we will figure out some techniques to handle them<\/li>\n    <li>Finally we will use box jenkins method to forecast<\/li>\n<\/ul>\n<p>So now let's get to the business and start with importing our Packages and reading our dataset<\/p>\n<p>So now let's get back to business<\/p>","0a9fa0f0":"###### ARIMA(5,1,5)","cb756dca":"now let's fit the model with auto_arima","ac4c118f":"### <font color='blue'>box jenkins method<\/font>\n<p>so now we successfully cleaned the data, Checked data types, set the business frequency to the data and plotting the closeing prices<\/p>\n<p>Now we can move to Modeling Phase we will apply a technique called <code>box jenkins method<\/code> <\/p>\n<p>here is a diagram shows our stages <img src='https:\/\/www.researchgate.net\/profile\/Montserrat-San-Martin-2\/publication\/258382499\/figure\/fig8\/AS:331598466699280@1456070643710\/3-stage-Box-Jenkins-methodology.png'><\/p>\n<ul>\n    <li>in stage one we will diagnose the time sreies and determine if it's stationary or not, if it's not stationary we will aplly some methods to transform it to a stationary time series<\/li>\n    <li>in stage two we will apply models to estimate the data ,evaluate them and choose the best estimator<\/li>\n    <li>in stage three we will diagnose the best estimator and decide if this model is doing well or not<\/li>\n    <li>Lastly we will forecaste the closing prices for the next three years\n<\/ul>\n<p>before modeling we frist have to frist splite the data to a training set and testing set. But there are two problems here 1-we can't shuffle time series data 2-because <code>ARIMA<\/code> and <code>ARMA<\/code> models are so sample so they won't catch the changes of a long time period very well.<\/p>\n<p>so for that we will simplfy the data a little bit and we will be wirking with the data from 2020 untill now<\/p>\n<p>so stop wasting time and let'e split the data<\/p>\n\n","73d385d0":">so you can see from the figure that only the first lag is significant but the others aren't","1c3a36cf":"> According to <code>Log Likelihood<\/code> the best model is ARIMA(5,1,5)"}}