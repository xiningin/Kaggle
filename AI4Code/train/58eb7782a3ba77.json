{"cell_type":{"56ad9500":"code","aeef52de":"code","9add0c07":"code","2b2cb1f9":"code","e346f4a9":"code","d0c0b99d":"code","c8eca39b":"code","68a42ba8":"code","396e696a":"code","a41575b9":"code","fb5708e2":"code","9c934eb9":"code","d8a80507":"code","de5dc7ae":"code","83df3f73":"code","b16db893":"code","bbfd3ca5":"code","dad21f34":"code","32dee572":"code","3f6ad8f0":"code","5ff5a182":"code","986cc7c3":"code","258e057a":"code","3667798e":"code","94880d9f":"code","f1b91af4":"code","80b1f234":"code","330139ef":"code","a6fb58fe":"code","07c337f7":"code","1fdfc0d6":"code","c35b6e9d":"code","ef8a0878":"code","dc70ce0f":"code","81e293c4":"code","8e79884b":"code","e98a9ee4":"code","467d4097":"code","5dd7eeed":"markdown","9542ddb8":"markdown","5c7b7420":"markdown","e22460e5":"markdown","e7fb0cf1":"markdown","e394953f":"markdown","387780de":"markdown","39b81afb":"markdown","a29a8c02":"markdown","3401209c":"markdown","888c2a12":"markdown","ee285f79":"markdown","fb8a22f1":"markdown","a90c16e1":"markdown","3d27552e":"markdown","804fcc49":"markdown","7efbf4bb":"markdown","f4e35d17":"markdown","fc4267e9":"markdown","a4e1614e":"markdown","655b3282":"markdown","60e0b328":"markdown","a459e780":"markdown","c5861b58":"markdown","e6f617fb":"markdown","4de4ce58":"markdown"},"source":{"56ad9500":"import sys\nimport os\n\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport timm\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nimport pandas as pd\nimport numpy as np\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nimport joblib\nimport gc\nfrom glob import glob\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\nimport timm\nfrom PIL import Image\nimport PIL\n\nfrom tqdm import tqdm\nimport joblib\nimport time\nfrom tqdm.notebook import tqdm\nimport joblib\nfrom sklearn.model_selection import StratifiedKFold\n\nimport cuml\n\nprint(np.__version__)\nprint(pd.__version__)\nprint(torch.__version__)\nprint(timm.__version__)","aeef52de":"!pip install ..\/input\/openaiclipweights\/python-ftfy-master\/python-ftfy-master\n!pip install ..\/input\/openaiclipweights\/clip\/CLIP\n!cp ..\/input\/openaiclipweights\/CLIP-main\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt \/opt\/conda\/lib\/python3.7\/site-packages\/clip\/.\n!gzip -k \/opt\/conda\/lib\/python3.7\/site-packages\/clip\/bpe_simple_vocab_16e6.txt\n\nimport clip","9add0c07":"train = pd.read_csv('..\/input\/petfinderdata\/train-folds-1.csv')\ntest = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\nsub = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')\n\ntrain['path'] = train['Id'].map(lambda x: '..\/input\/petfinder-pawpularity-score\/train\/'+x+'.jpg')\ntest['path'] = test['Id'].map(lambda x: '..\/input\/petfinder-pawpularity-score\/test\/'+x+'.jpg')\n\n# If its Public LB run, then augment Testset to chack batch size memory consumption.\nif test.shape[0]<10:\n    test = pd.concat([\n        test, test, test, test, test, \n    ])\n    test = test.reset_index(drop=True)\n\nprint(train.shape, test.shape, sub.shape)","2b2cb1f9":"train['bins'] = (train['Pawpularity']\/\/5).round()\n\ntrain['fold0'] = -1\nskf = StratifiedKFold(n_splits = 20, shuffle=True, random_state = 1)\nfor i, (_, test_index) in enumerate(skf.split(train.index, train['bins'])):\n    train.iloc[test_index, -1] = i\n\ntrain['fold0'] = train['fold0'].astype('int')\ngc.collect()\n\ntrain.groupby(['fold0'])['Pawpularity'].agg(['mean','std','count'])","e346f4a9":"train.head()","d0c0b99d":"test.head()","c8eca39b":"avail_pretrained_models = timm.list_models(pretrained=True)\nlen(avail_pretrained_models), avail_pretrained_models","68a42ba8":"names = [\n    'deit_base_distilled_patch16_384',\n    'fbnetc_100',\n    'ig_resnext101_32x8d',\n    'ig_resnext101_32x48d',\n    'repvgg_b0',\n    'resnetv2_152x4_bitm',\n    'rexnet_200',\n    'resnest269e',\n    'swsl_resnext101_32x8d',\n    'tf_efficientnet_b6_ns',\n    'tf_efficientnet_b7_ns',\n    'tf_efficientnet_b8_ap',\n    'tf_efficientnet_l2_ns_475',\n    'vit_base_patch16_384',\n    'vit_large_patch16_384',\n    'vit_large_r50_s32_384',\n]\n\nnames_hflip_crop = [\n    'tf_efficientnet_l2_ns_hflip_384',\n    'deit_base_distilled_patch16_384_hflip_384',\n    'ig_resnext101_32x48d_hflip_384',\n    'tf_efficientnet_l2_ns_512',\n]\n\nnames_orig = [\n    'ig_resnext101_32x48d',\n    'vit_large_r50_s32_384',\n    'clip_RN50x4',\n    'clip_ViT-B-16',\n    'clip_RN50x16',\n    'clip_ViT-B-32',\n]\n","396e696a":"modelpath = { m.split('\/')[-1].split('.')[0] :m for m in glob('..\/input\/pytorch-pretrained-0\/*.pt')+glob('..\/input\/pytorch-pretrained-1\/*.pt')+glob('..\/input\/pytorch-pretrained-2\/*.pt')+glob('..\/input\/pytorch-pretrained-3\/*.pt')}\nmodelpath","a41575b9":"class PawpularDataset:\n    def __init__(self, images, base_path='..\/input\/petfinder-pawpularity-score\/train\/', modelcfg=None, aug=0 ):\n        \n        self.images = images.copy()\n        self.base_path = base_path\n        self.transform = create_transform(**modelcfg)\n        self.aug=aug\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, item):\n        img = Image.open(self.base_path + self.images[item] + '.jpg').convert('RGB')\n        img = self.transform(img)\n        return img\n\n\nEMB_TEST = {}\nfor arch in names:\n    starttime = time.time()\n\n    model = timm.create_model(arch, pretrained=False).to('cuda')\n    model.load_state_dict(torch.load(modelpath[arch]))\n    model.eval()\n\n    train_dataset = PawpularDataset(\n        images = test.Id.values,\n        base_path='..\/input\/petfinder-pawpularity-score\/test\/',\n        modelcfg = resolve_data_config({}, model=model),\n        aug = 0,\n    )\n    BS = 10 if arch in ['tf_efficientnet_l2_ns'] else 16\n    train_dataloader = DataLoader(train_dataset, batch_size=BS, num_workers= 2, shuffle=False)\n    \n    with torch.no_grad():\n        res = [model(img.to('cuda')).cpu().numpy() for img in train_dataloader]\n    res = np.concatenate(res, 0)\n    EMB_TEST[arch] = res\n    \n    print( arch, ', Done in:', int(time.time() - starttime), 's' )\n    \n    del model, res\n    torch.cuda.empty_cache() # PyTorch thing to clean RAM\n    gc.collect()\n\nprint(time.time() )    \nlen(EMB_TEST), EMB_TEST.keys()","fb5708e2":"class PawpularDataset_HFLIP:\n    def __init__(self, images, base_path='..\/input\/petfinder-pawpularity-score\/train\/', modelcfg=None, doflip=False ):\n        \n        self.images = images.copy()\n        self.base_path = base_path\n        self.transform = modelcfg\n        self.doflip=doflip\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, item):\n        img = Image.open(self.base_path + self.images[item] + '.jpg').convert('RGB')\n        \n        if self.doflip==True:\n            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n            width, height = img.size\n            img = img.crop((0.0*width, 0.02*height, 0.98*width, 0.98 * height))  \n        \n        img = self.transform(img)\n        return img\n\n\nfor arch in names_hflip_crop:\n    starttime = time.time()\n\n    archname = arch.split('_hflip_')[0]\n    if arch == 'tf_efficientnet_l2_ns_512':\n        archname = 'tf_efficientnet_l2_ns'\n    model = timm.create_model(archname, pretrained=False).to('cuda')\n    model.load_state_dict(torch.load(modelpath[archname]))\n    model.eval()\n\n    # Get model default transforms\n    transf = resolve_data_config({}, model=model)\n    sz = int(arch.split('_')[-1])\n    transf['input_size'] = (3, sz, sz)\n    transf['crop_pct'] = 1.0        \n    transf = create_transform(**transf)\n\n    doflip = True if arch.split('_')[-2] == 'hflip' else False\n    train_dataset = PawpularDataset_HFLIP(\n        images = test.Id.values,\n        base_path='..\/input\/petfinder-pawpularity-score\/test\/',\n        modelcfg = transf,\n        doflip = doflip,\n    )\n\n    BS = 10 if archname in ['tf_efficientnet_l2_ns'] else 16\n    train_dataloader = DataLoader(train_dataset, batch_size=BS, num_workers= 2, shuffle=False)\n\n    with torch.no_grad():\n        res = [model(img.to('cuda')).cpu().numpy() for img in train_dataloader]\n    res = np.concatenate(res, 0)\n    EMB_TEST[arch] = res\n\n    print( arch, 'imge size:', sz, 'Hflip:', doflip, ',Done in:', int(time.time() - starttime), 's' )\n\n    del model, res\n    torch.cuda.empty_cache() # PyTorch thing to clean RAM\n    gc.collect()\n\nprint(time.time() )    \nlen(EMB_TEST), EMB_TEST.keys()","9c934eb9":"class CustomDataset:\n    def __init__(self, data, base_path='..\/input\/petfinder-pawpularity-score\/test\/', preprocess=None):\n        \n        self.data = data.copy()\n        self.base_path = base_path\n        if 'Pawpularity' not in self.data.columns:\n            self.data['Pawpularity'] = 0\n        self.preprocess=preprocess\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        img = Image.open(self.base_path + self.data.Id[item] + '.jpg').convert(\"RGB\")\n        img = self.preprocess(img)\n        return img\n\n\nfor m in ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B-16', 'ViT-B-32']:\n    starttime = time.time()\n    model, preprocess = clip.load(\"..\/input\/openaiclipweights\/clip\/CLIP\/models\/\"+m+\".pt\")\n    model.cuda().eval()\n    \n    EMB = []\n    with torch.no_grad():\n        test_dataset = CustomDataset(data = test, base_path='..\/input\/petfinder-pawpularity-score\/test\/', preprocess=preprocess)\n        test_data_loader = DataLoader(test_dataset, batch_size=64,num_workers=2,shuffle=False,pin_memory=True,)\n        for batch in test_data_loader:\n            image_features = model.encode_image(batch.to('cuda'))\n            #image_features \/= image_features.norm(dim=-1, keepdim=True)\n            logits = image_features.cpu().numpy()\n            EMB.append(logits)\n    EMB = np.concatenate(EMB, 0)\n    EMB = EMB.astype('float32')\n    gc.collect()\n    \n    EMB_TEST['clip_'+m] = EMB\n    print( m, ', Done in:', int(time.time() - starttime), 's' )\n    \n    del model\n    torch.cuda.empty_cache() # PyTorch thing to clean RAM\n    gc.collect()\n    \ngc.collect()\nprint(EMB_TEST.keys())","d8a80507":"EMB_TRAIN = joblib.load('..\/input\/petfinderdata\/train-embeddings-direct-1.joblib')\ngc.collect()\n\nresclip = joblib.load('..\/input\/openai-clip\/train-embeddings-openai-clip-1.joblib')\nfor m in resclip.keys():\n    EMB_TRAIN[m] = resclip[m]\ndel resclip\ngc.collect()\n\nhflipmodels = joblib.load('..\/input\/petfinder-extracted-pretrained-1\/extracted-pretrained-1.joblib')\nfor col in names_hflip_crop:\n    EMB_TRAIN[col] = hflipmodels[col]\ndel hflipmodels\ngc.collect()\n\nprint( len(EMB_TRAIN) )\nprint(EMB_TRAIN.keys())","de5dc7ae":"for m in EMB_TEST.keys():\n    print(EMB_TRAIN[m].shape, EMB_TEST[m].shape, m)","83df3f73":"names0 = [\n    'clip_RN50x16',\n    'clip_ViT-B-32',\n    'clip_ViT-B-16',\n    'clip_RN50x4',\n    'deit_base_distilled_patch16_384',\n    'ig_resnext101_32x48d',\n    'repvgg_b0',\n    'resnetv2_152x4_bitm',\n    'swsl_resnext101_32x8d',\n    'tf_efficientnet_l2_ns_475',\n    'vit_base_patch16_384',\n    'vit_large_r50_s32_384',\n]\n\nnames1 = [\n    'clip_RN50x16',\n    'clip_RN101', \n    'clip_RN50',\n    'fbnetc_100',\n    'ig_resnext101_32x8d',\n    'rexnet_200',\n    'resnest269e',\n    'tf_efficientnet_b6_ns',\n    'tf_efficientnet_b8_ap',\n    'tf_efficientnet_b7_ns',\n    'vit_large_patch16_384',\n]\n\nnames2 = [\n    'tf_efficientnet_l2_ns_hflip_384',\n    'deit_base_distilled_patch16_384_hflip_384',\n    'ig_resnext101_32x48d_hflip_384',\n    'tf_efficientnet_l2_ns_512',\n    'ig_resnext101_32x48d',\n    'vit_large_r50_s32_384',\n    'clip_RN50x4',\n    'clip_ViT-B-16',\n    'clip_RN50x16',\n    'clip_ViT-B-32',\n]\n\nnames = np.unique(names0 + names1 + names2)\nlen(names), names","b16db893":"feats = list(EMB_TRAIN.keys())\nfor n in feats:\n    if n not in names:\n        del EMB_TRAIN[n]\n        gc.collect()","bbfd3ca5":"from cuml.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\n\ndef fit_gpu_svr(TRAIN, TEST, kfoldcol='fold0'):\n    \n    ypredtrain_ = np.zeros(train.shape[0])\n    ypredtest_ = np.zeros(test.shape[0])\n\n    for fold in range(train[kfoldcol].max()+1):\n        ind_train = train[kfoldcol] != fold\n        ind_valid = train[kfoldcol] == fold\n\n        model = SVR(C=16.0, kernel='rbf', degree=3, max_iter=4000, output_type='numpy')\n        model.fit(TRAIN[ind_train], train.Pawpularity[ind_train].clip(1, 85)  )\n\n        ypredtrain_[ind_valid] = np.clip(model.predict(TRAIN[ind_valid]), 1 , 100)\n        ypredtest_ += np.clip(model.predict(TEST), 1, 100)\n\n        del model\n        gc.collect()\n\n    ypredtest_ \/= (train[kfoldcol].max()+1)\n\n    return ypredtrain_, ypredtest_\n\ndef rmse(ytrue, ypred):\n    return np.sqrt(np.mean((ytrue-ypred)**2))","dad21f34":"for col in names:\n    \n    TRAIN = EMB_TRAIN[col].copy()\n    TEST = EMB_TEST[col].copy()\n\n    scaler = StandardScaler()\n    scaler.fit( np.vstack((TRAIN, TEST)) )\n    TRAIN = scaler.transform(TRAIN)\n    TEST = scaler.transform(TEST)\n    \n    ypredtrain, ypredtest = fit_gpu_svr(TRAIN, TEST, 'fold0')\n    print(rmse(train.Pawpularity,ypredtrain), col)    ","32dee572":"print('Concatenating:', names0)\n\nTRAIN = np.concatenate([EMB_TRAIN[k] for k in names0], 1)\nTEST = np.concatenate([EMB_TEST[k] for k in names0], 1)\nscaler = StandardScaler()\nscaler.fit( np.vstack((TRAIN, TEST)) )\ngc.collect()\n\nTRAIN = scaler.transform(TRAIN)\nTEST = scaler.transform(TEST)\ngc.collect()\n\n# Check the output shape\nprint(TRAIN.shape, TEST.shape)\n\nypredtrainA, ypredtestA = fit_gpu_svr(TRAIN, TEST, 'fold0')\nprint(rmse(train.Pawpularity, ypredtrainA))","3f6ad8f0":"del TRAIN, TEST\ngc.collect()\ntorch.cuda.empty_cache() # PyTorch thing","5ff5a182":"print('RMSE:', rmse(train.Pawpularity, 1.032*ypredtrainA))","986cc7c3":"print('Concatenating:', names1)\n\nTRAIN = np.concatenate([EMB_TRAIN[k] for k in names1], 1)\nTEST = np.concatenate([EMB_TEST[k] for k in names1], 1)\nscaler = StandardScaler()\nscaler.fit( np.vstack((TRAIN, TEST)) )\ngc.collect()\n\nTRAIN = scaler.transform(TRAIN)\nTEST = scaler.transform(TEST)\ngc.collect()\n\nprint( TRAIN.shape, TEST.shape )\n\nypredtrainB, ypredtestB = fit_gpu_svr(TRAIN, TEST, 'fold0')\nprint('RMSE:', rmse(train.Pawpularity, ypredtrainB))","258e057a":"del TRAIN, TEST\ngc.collect()\ntorch.cuda.empty_cache() # PyTorch thing","3667798e":"print('Concatenating:', names2)\n\nTRAIN = np.concatenate([EMB_TRAIN[k] for k in names2], 1)\nTEST = np.concatenate([EMB_TEST[k] for k in names2], 1)\nscaler = StandardScaler()\nscaler.fit( np.vstack((TRAIN, TEST)) )\ngc.collect()\n\nTRAIN = scaler.transform(TRAIN)\nTEST = scaler.transform(TEST)\ngc.collect()\n\nprint( TRAIN.shape, TEST.shape )\n\nypredtrainC, ypredtestC = fit_gpu_svr(TRAIN, TEST, 'fold0')\nprint('RMSE:', rmse(train.Pawpularity, ypredtrainC))","94880d9f":"del TRAIN, TEST\ndel EMB_TRAIN, EMB_TEST\ngc.collect()\n\ntorch.cuda.empty_cache() # PyTorch thing to free GPU memory\ngc.collect()","f1b91af4":"from torch.utils.data import Dataset, DataLoader\nimport albumentations as A\n\ndevice = torch.device('cuda')\nclass Config:\n    model_name = \"swin_large_patch4_window7_224\"\n    base_dir = \"..\/input\/petfinder-pawpularity-score\"\n    data_dir = base_dir\n    model_dir = \"exp\"\n    output_dir = model_dir\n    img_test_dir = os.path.join(data_dir, \"test\")\n    model_path = \"swin_large_patch4_window7_224\"\n    im_size =  384\n    batch_size = 16\n\n\nclass PetDataset(Dataset):\n    def __init__(self, image_filepaths, targets, transform=None):\n        self.image_filepaths = image_filepaths\n        self.targets = targets\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.image_filepaths[idx]\n        with open(image_filepath, 'rb') as f:\n            image = Image.open(f)\n            image_rgb = image.convert('RGB')\n        image = np.array(image_rgb)\n\n        if self.transform is not None:\n            image = self.transform(image = image)[\"image\"]\n        \n        image = image \/ 255\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        target = self.targets[idx]\n\n        image = torch.tensor(image, dtype = torch.float)\n        target = torch.tensor(target, dtype = torch.float)\n        return image, target    \n\n\ndef get_inference_fixed_transforms(mode=0, dim = 224):\n    if mode == 0: # do not original aspects, colors and angles\n        return A.Compose([\n                A.SmallestMaxSize(max_size=dim, p=1.0),\n                A.CenterCrop(height=dim, width=dim, p=1.0),\n            ], p=1.0)\n    elif mode == 1:\n        return A.Compose([\n                A.SmallestMaxSize(max_size=dim+16, p=1.0),\n                A.CenterCrop(height=dim, width=dim, p=1.0),\n                A.HorizontalFlip(p = 1.0)\n            ], p=1.0)\n\n\nclass PetNet(nn.Module):\n    def __init__(\n        self,\n        model_name = Config.model_path,\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False,\n    ):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=False, in_chans=3, num_classes = 1)\n    \n    def forward(self, image):\n        output = self.model(image)\n        return output    \n\n\ndef tta_fn(filepaths, model, ttas=[0, 1]):\n    print('Image Size:', Config.im_size)\n    model.eval()\n    tta_preds = []\n    for tta_mode in ttas:#range(Config.tta_times):\n        print(f'tta mode:{tta_mode}')\n        test_dataset = PetDataset(\n          image_filepaths = filepaths,\n          targets = np.zeros(len(filepaths)),\n          transform = get_inference_fixed_transforms(tta_mode, dim = Config.im_size )\n        )\n        test_loader = DataLoader(\n          test_dataset,\n          batch_size = Config.batch_size,\n          shuffle = False,\n          num_workers = 2,\n          pin_memory = True\n        )\n        #stream = tqdm(test_loader)\n        tta_pred = []\n        for images, target in test_loader:#enumerate(stream, start = 1):\n            images = images.to(device, non_blocking = True).float()\n            target = target.to(device, non_blocking = True).float().view(-1, 1)\n            with torch.no_grad():\n                output = model(images)\n\n            pred = (torch.sigmoid(output).detach().cpu().numpy() * 100).ravel().tolist()\n            tta_pred.extend(pred)\n        tta_preds.append(np.array(tta_pred))\n    \n    fold_preds = tta_preds[0]\n    for n in range(1, len(tta_preds)):\n        fold_preds += tta_preds[n]\n    fold_preds \/= len(tta_preds)\n        \n    del test_loader, test_dataset\n    gc.collect()\n    torch.cuda.empty_cache()\n    return fold_preds    ","80b1f234":"# List all test files\nfilepaths = test['path'].values.copy()\nlen(filepaths)","330139ef":"%%time\n\nclass Config:\n    model_dir = \"exp53\"\n    output_dir = \"exp53\"\n    model_name = \"swin_large_patch4_window7_224\"\n    im_size =  224\n    model_path = model_name\n    base_dir = \"..\/input\/petfinder-pawpularity-score\"\n    data_dir = base_dir\n    img_test_dir = os.path.join(data_dir, \"test\")\n    batch_size = 16\n\n\ntest_preds = []\ntest_preds_model = []\nmodelfiles = glob('..\/input\/petfinder-'+Config.model_dir+'\/*.pth')\nfor mi, model_path in enumerate(modelfiles):\n    print(f'inference: {model_path}')\n    test_preds_fold = []\n    model = PetNet(\n        model_name = Config.model_path,\n        out_features = 1,\n        inp_channels = 3,\n        pretrained=False\n    )\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(device)\n    model = model.float()\n    model.eval()\n    test_preds_fold = tta_fn(filepaths, model, [1] )        \n    test_preds_model.append(test_preds_fold)\n    \noof53 = pd.read_csv('..\/input\/petfinder-'+Config.model_dir+'\/oof_tta.csv')\nfinal_predictions53 = np.mean(np.array(test_preds_model), axis=0)\nfinal_predictions53","a6fb58fe":"%%time\nclass Config:\n    model_dir = \"exp55\"\n    output_dir = \"exp55\"\n    model_name = \"beit_large_patch16_224\"\n    im_size =  224\n    model_path = model_name\n    base_dir = \"..\/input\/petfinder-pawpularity-score\"\n    data_dir = base_dir\n    img_test_dir = os.path.join(data_dir, \"test\")\n    batch_size = 16\n\n\ntest_preds = []\ntest_preds_model = []\nmodelfiles = glob('..\/input\/petfinder-'+Config.model_dir+'\/*.pth')\nfor mi, model_path in enumerate(modelfiles):\n    print(f'inference: {model_path}')\n    test_preds_fold = []\n    model = PetNet(\n        model_name = Config.model_path,\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False\n    )\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(device)\n    model = model.float()\n    model.eval()\n    test_preds_fold = tta_fn(filepaths, model, [0] )        \n    test_preds_model.append(test_preds_fold)\n\n\noof55 = pd.read_csv('..\/input\/petfinder-'+Config.model_dir+'\/oof_tta.csv')\nfinal_predictions55 = np.mean(np.array(test_preds_model), axis=0)\nfinal_predictions55","07c337f7":"%%time\nclass Config:\n    model_dir = \"exp66\"\n    output_dir = \"exp66\"\n    model_name = \"swin_large_patch4_window12_384_in22k\"\n    im_size =  384\n    model_path = model_name\n    base_dir = \"..\/input\/petfinder-pawpularity-score\"\n    data_dir = base_dir\n    img_test_dir = os.path.join(data_dir, \"test\")\n    batch_size = 16\n\n    \ntest_preds = []\ntest_preds_model = []\nmodelfiles = glob('..\/input\/petfinder-'+Config.model_dir+'\/*.pth')\nfor mi, model_path in enumerate(modelfiles):\n    print(f'inference: {model_path}')\n    test_preds_fold = []\n    model = PetNet(\n        model_name = Config.model_path,\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False\n    )\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(device)\n    model = model.float()\n    model.eval()\n    test_preds_fold = tta_fn(filepaths, model, [0] )        \n    test_preds_model.append(test_preds_fold)\n    \noof66 = pd.read_csv('..\/input\/petfinder-'+Config.model_dir+'\/oof_tta.csv')\nfinal_predictions66 = np.mean(np.array(test_preds_model), axis=0)\nfinal_predictions66","1fdfc0d6":"%%time\nclass Config:\n    model_dir = \"exp77\"\n    output_dir = \"exp77\"\n    model_name = \"beit_large_patch16_224\"\n    im_size =  224\n    model_path = model_name\n    base_dir = \"..\/input\/petfinder-pawpularity-score\"\n    data_dir = base_dir\n    img_test_dir = os.path.join(data_dir, \"test\")\n    batch_size = 16\n\n    \nclass PetNet(nn.Module):\n    def __init__(\n        self,\n        model_name = Config.model_path,\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False\n    ):\n        super().__init__()\n        NC = 1000\n        self.model = timm.create_model(model_name, pretrained=False)\n        self.dropout = nn.Dropout(0.05)\n        self.head = nn.Linear(NC, 1)\n    \n    def forward(self, image):\n        output = self.model(image)\n        output = self.dropout(output)\n        output = self.head(output)\n        return output    \n    \ntest_preds = []\ntest_preds_model = []\nmodelfiles = glob('..\/input\/petfinder-'+Config.model_dir+'\/*.pth')\nfor mi, model_path in enumerate(modelfiles):\n    print(f'inference: {model_path}')\n    test_preds_fold = []\n    model = PetNet(\n        model_name = Config.model_path,\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False\n    )\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(device)\n    model = model.float()\n    model.eval()\n    test_preds_fold = tta_fn(filepaths, model, [0] )        \n    test_preds_model.append(test_preds_fold)\n    \noof77 = pd.read_csv('..\/input\/petfinder-'+Config.model_dir+'\/oof_tta.csv')\nfinal_predictions77 = np.mean(np.array(test_preds_model), axis=0)\nfinal_predictions77","c35b6e9d":"%%time\nclass Config:\n    model_dir = \"exp82\"\n    output_dir = \"exp82\"\n    model_name = \"tf_efficientnet_b6_ns\"\n    im_size =  528\n    model_path = model_name\n    base_dir = \"..\/input\/petfinder-pawpularity-score\"\n    data_dir = base_dir\n    img_test_dir = os.path.join(data_dir, \"test\")\n    batch_size = 16\n\n\nclass PetNet(nn.Module):\n    def __init__(\n        self,\n        model_name = Config.model_path,\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False\n    ):\n        super().__init__()\n        NC = 1000\n        self.model = timm.create_model(model_name, pretrained=False)\n        self.dropout = nn.Dropout(0.15)\n        self.head = nn.Linear(NC, out_features)\n    \n    def forward(self, image):\n        output = self.model(image)\n        output = self.dropout(output)\n        output = self.head(output)\n        return output\n    \ntest_preds = []\ntest_preds_model = []\nmodelfiles = glob('..\/input\/petfinder-'+Config.model_dir+'\/*.pth')\nfor mi, model_path in enumerate(modelfiles):\n    print(f'inference: {model_path}')\n    test_preds_fold = []\n    model = PetNet(\n        model_name = Config.model_path,\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False\n    )\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(device)\n    model = model.float()\n    model.eval()\n    test_preds_fold = tta_fn(filepaths, model, [0] )        \n    test_preds_model.append(test_preds_fold)\n    \noof82 = pd.read_csv('..\/input\/petfinder-'+Config.model_dir+'\/oof_tta.csv')\nfinal_predictions82 = np.mean(np.array(test_preds_model), axis=0)\nfinal_predictions82","ef8a0878":"oof = oof53.copy()\noof['pred'] = (\n    3*oof53['pred'] +\n    4*oof55['pred'] +\n    3*oof66['pred'] +\n    4*oof77['pred'] +\n    2*oof82['pred']  \n) \/ (3+4+3+4+2) \n\nfinal_train_predictions = train.merge(oof, on='Id', how='left')['pred'].values.copy()\n\nrmse(train.Pawpularity.values, final_train_predictions)","dc70ce0f":"final_test_predictions = (\n    3*final_predictions53 +\n    4*final_predictions55 +\n    3*final_predictions66 +\n    4*final_predictions77 +\n    2*final_predictions82 \n) \/ (3+4+3+4+2)","81e293c4":"from scipy.optimize import minimize\n\ndef min_func(K):\n    ypredtrain = K[0]*ypredtrainA + K[1]*ypredtrainB + K[2]*ypredtrainC + K[3]*final_train_predictions\n    return rmse(train.Pawpularity, ypredtrain)\n   \nres = minimize(min_func, [1\/4]*4, method='Nelder-Mead', tol=1e-6)\nK = res.x\nres","8e79884b":"ypredtrain = K[0]*ypredtrainA + K[1]*ypredtrainB + K[2]*ypredtrainC + K[3]*final_train_predictions\n\ntest['Pawpularity'] = K[0]*ypredtestA + K[1]*ypredtestB + K[2]*ypredtestC + K[3]*final_test_predictions\n\nprint('Ensemble weights:', K )\nprint('Final RMSE:' rmse(train.Pawpularity, ypredtrain) )","e98a9ee4":"test.head(8)","467d4097":"test[['Id','Pawpularity']].to_csv('submission.csv', index=False)","5dd7eeed":"# First, lets fit one SVR for each architecture independently","9542ddb8":"# The pretrained models found by the forward model selection algorithm used in this solution are listed above.","5c7b7420":"# Weighted average image models","e22460e5":"# Now lets start to extract imagenet pretrained models features","e7fb0cf1":"# SVR C","e394953f":"# Also I noticed that using a multiplier of 1.032 boosts both CV and LB. It may be by the fact that SRV optimizes mean squared error and not RMSE.","387780de":"# As you can see above, features extracted from individual architectures have SVR RMSE ranging from 17.56 to 18.52.\n# But what happens if we stack some architecture features side by side before fitting the SVR?","39b81afb":"# Concatenate some features and standardize","a29a8c02":"# Now extract TESTSET features from CLIP architecture","3401209c":"# Fit the SVR A using all K Folds.\n# I noticed cliping the target in 85 slightly boosts RMSE.","888c2a12":"# Install OpenAI CLIP","ee285f79":"# Now run inference using Deep Learning finetuned image models.","fb8a22f1":"# Extract features using Horizontal Flip and small crop","a90c16e1":"# Lest check all models available in timm library","3d27552e":"# Just create K Folds and check target consistency accross folds","804fcc49":"# Now interactively extract the TESTSET features from each imagenet pretrained model and append to a dictionary","7efbf4bb":"# Check the shape of the features","f4e35d17":"# Just clean memory of offline trainset features not going to be used here","fc4267e9":"# As we can see there are 575 pretrained model architectures available in timm library.\n# The first part of the solution is basically extract the features from the last layer of that models and run a SVR on that extracted features.\n# Most of the models in timm are trained using 1000 classes in imagenet, so output shape is 1000 for each model.\n# Extracting features from all 575 models is something crazy and unthinkable, specially taking into account submission time of 9h. So the idea is to find a subset of models (from that 575) that performs well in terms of RMSE.\n# To do that it was used a forward models selection algorithm, following by RMSE hill climbing logic. Starting with one model, then keep adding models until it stop increasing RMSE performance.\n","a4e1614e":"# Create a dictionary with the path of all pretrained weights available in Kaggle datasets","655b3282":"# Now fit a SVR B using a second subset of features. The idea of fitting more subsets is to add diversity in posterior model ensemble and avoid the curse of dimensionality increasing too much the number of features.","60e0b328":"# Free RAM and GPU memory","a459e780":"# Load TRAINSET extracted features (made offline)","c5861b58":"# Now its time to fit a GPU accelerated SVR using cuml ","e6f617fb":"# Optimize the overall RMSE using OOF prediction of cuML SVR A, B, C, and Image models ensemble.","4de4ce58":"# Load Train and Test"}}