{"cell_type":{"00a0003d":"code","a4c7b894":"code","ceff0f3b":"code","0c0b1f9a":"code","8c790b19":"code","29388755":"code","825ab748":"code","64d32fa0":"code","4a0c3224":"code","2852c565":"code","c7a7bd33":"code","a4824592":"code","4f4c8cc7":"code","e664c2c6":"code","d4f4b574":"code","d4fa2129":"code","7d5b7121":"code","1f249662":"code","a6aa7e10":"code","227fada1":"code","ce7a426a":"code","18fcfc55":"code","7f3e6b97":"markdown","12cbc0e6":"markdown","a90df70b":"markdown","7d064fe2":"markdown","2030034e":"markdown","7c1c47d2":"markdown","4a1b5a5b":"markdown","cc19a929":"markdown","6c7ea069":"markdown","15f64df2":"markdown","fe26fb8f":"markdown","5bc80db6":"markdown","e2be9000":"markdown","38c7f29d":"markdown","a2eb409f":"markdown","62553916":"markdown","3892acbf":"markdown","4fc282d1":"markdown","bec50109":"markdown","aeac1722":"markdown","8924602c":"markdown","6145815f":"markdown","9954c63e":"markdown","6a9bc7ce":"markdown","e3b7d91b":"markdown","58c4fe86":"markdown"},"source":{"00a0003d":"!pip install hyperas","a4c7b894":"# Basic compuational libaries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom sklearn.model_selection import KFold\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.layers import Dense, Dropout, Conv2D, GlobalAveragePooling2D, Flatten, GlobalMaxPooling2D\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop, Adam, SGD, Nadam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom keras import regularizers\n\n# Import hyperopt for tunning hyper params\nfrom hyperopt import hp, tpe, fmin\nfrom hyperopt import space_eval\n\nsns.set(style='white', context='notebook', palette='deep')\n# Set the random seed\nrandom_seed = 2","ceff0f3b":"def data():\n    # Load the data\n    train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n    test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n    Y_train = train[\"label\"]\n    \n    # Drop 'label' column\n    X_train = train.drop(labels = [\"label\"],axis = 1) \n    \n    # Normalize the data\n    X_train = X_train \/ 255.0\n    test = test \/ 255.0\n    \n    # Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n    X_train = X_train.values.reshape(-1,28,28,1)\n    test = test.values.reshape(-1,28,28,1)\n    \n    # Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n    Y_train = to_categorical(Y_train, num_classes = 10)\n    \n    return X_train, Y_train, test\n\nX, Y, X_test = data()\n","0c0b1f9a":"g = sns.countplot(np.argmax(Y, axis=1))","8c790b19":"for i in range(0, 9):\n    plt.subplot(330 + (i+1))\n    plt.imshow(X[i][:,:,0], cmap=plt.get_cmap('gray'))\n    plt.title(np.argmax(Y[i]));\n    plt.axis('off')\nplt.tight_layout()    ","29388755":"epochs = 30 # Turn epochs to 30 to get 0.9967 accuracy\nbatch_size = 64","825ab748":"# With data augmentation to prevent overfitting (accuracy 0.99286)\ntrain_aug = ImageDataGenerator(        \n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        )\n\ntest_aug = ImageDataGenerator()\n","64d32fa0":"# Set the CNN model \ndef train_model(train_generator, valid_generator, params):    \n    model = Sequential()\n\n    model.add(Conv2D(filters = params['conv1'], kernel_size = params['kernel_size_1'], padding = 'Same', \n                     activation ='relu', input_shape = (28,28,1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = params['conv2'], kernel_size = params['kernel_size_2'], padding = 'Same', \n                     activation ='relu'))\n    model.add(MaxPool2D(pool_size = params['pooling_size_1']))\n    model.add(Dropout(params['dropout1']))\n\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = params['conv3'], kernel_size = params['kernel_size_3'], padding = 'Same', \n                     activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = params['conv4'], kernel_size = params['kernel_size_4'], padding = 'Same', \n                     activation ='relu'))\n    model.add(MaxPool2D(pool_size = params['pooling_size_1'], strides=(2,2)))\n    model.add(Dropout(params['dropout2']))\n\n    model.add(Flatten())\n    model.add(BatchNormalization())\n    model.add(Dense(params['dense1'], activation = \"relu\"))\n    model.add(Dropout(params['dropout3']))\n    model.add(Dense(10, activation = \"softmax\"))\n    \n    if params['opt'] == 'rmsprop':\n        opt = RMSprop()\n    elif params['opt'] == 'sgd':\n        opt = SGD()\n    elif params['opt'] == 'nadam':\n        opt = Nadam()\n    else:\n        opt = Adam()\n    \n    model.compile(loss=params['loss'], optimizer=opt, metrics=['acc'])\n        \n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, mode='auto', cooldown=2, min_lr=1e-7)\n    early = EarlyStopping(monitor='val_loss', patience=3)\n    \n    callbacks_list = [reduce_lr, early]    \n    \n    history = model.fit_generator(train_generator,\n                                  validation_data=valid_generator,\n                                  steps_per_epoch=len(train_generator),\n                                  validation_steps=len(valid_generator),\n                                  callbacks=callbacks_list, epochs = epochs,\n                                  verbose=2)\n    \n    score, acc = model.evaluate_generator(valid_generator, steps=len(valid_generator), verbose=0)\n    \n    return acc, model, history","4a0c3224":"#This is the space of hyperparameters that we will search\nspace = {\n    'opt':hp.choice('opt', ['adam', 'sgd', 'rmsprop']),\n    \n    'conv1':hp.choice('conv1', [16, 32, 64, 128]),\n    'conv2':hp.choice('conv2', [16, 32, 64, 128]),\n    'kernel_size_1': hp.choice('kernel_size_1', [3, 5]),\n    'kernel_size_2': hp.choice('kernel_size_2', [3, 5]),\n    'dropout1': hp.choice('dropout1', [0, 0.25, 0.5]),\n    'pooling_size_1': hp.choice('pooling_size_1', [2, 3]),\n    \n    'conv3':hp.choice('conv3', [32, 64, 128, 256, 512]),\n    'conv4':hp.choice('conv4', [32, 64, 128, 256, 512]),\n    'kernel_size_3': hp.choice('kernel_size_3', [3, 5]),\n    'kernel_size_4': hp.choice('kernel_size_4', [3, 5]),\n    'dropout2':hp.choice('dropout2', [0, 0.25, 0.5]),\n    'pooling_size_2': hp.choice('pooling_size_2', [2, 3]),\n    \n    'dense1':hp.choice('dense1', [128, 256, 512, 1024]),\n    'dropout3':hp.choice('dropout3', [0, 0.25, 0.5]),\n    \n    'loss': hp.choice('loss', ['categorical_crossentropy', 'kullback_leibler_divergence']),\n}","2852c565":"X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, random_state=random_seed)\n# only apply data augmentation with train data\ntrain_gen = train_aug.flow(X_train, Y_train, batch_size=batch_size)\nvalid_gen = test_aug.flow(X_val, Y_val, batch_size=batch_size)\n\ndef optimize(params):\n    acc, model, history = train_model(train_gen, valid_gen, params)\n    \n    return -acc","c7a7bd33":"best = fmin(fn = optimize, space = space, \n            algo = tpe.suggest, max_evals = 2) # change to 50 to search more","a4824592":"best_params = space_eval(space, best)\nprint('best hyper params: \\n', best_params)","4f4c8cc7":"acc, model, history = train_model(train_gen, valid_gen, best_params)\nprint(\"validation accuracy: {}\".format(acc))","e664c2c6":"optimizers = ['rmsprop', 'sgd', 'adam']\nhists = []\nparams = best_params\nfor optimizer in optimizers:\n    params['opt'] = optimizer\n    print(\"Train with optimizer: {}\".format(optimizer))\n    _, _, history = train_model(train_gen, valid_gen, params)\n    hists.append((optimizer, history))","d4f4b574":"for name, history in hists:\n    plt.plot(history.history['val_acc'], label=name)\n\nplt.legend(loc='best', shadow=True)\nplt.tight_layout()","d4fa2129":"loss_functions = ['categorical_crossentropy', 'kullback_leibler_divergence']\nhists = []\nparams = best_params\nfor loss_funct in loss_functions:\n    params['loss'] = loss_funct\n    print(\"Train with loss function : {}\".format(loss_funct))\n    _, _, history = train_model(train_gen, valid_gen, params)\n    hists.append((loss_funct, history))","7d5b7121":"for name, history in hists:\n    plt.plot(history.history['val_acc'], label=name)\n\nplt.legend(loc='best', shadow=True)\nplt.tight_layout()\n","1f249662":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","a6aa7e10":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n    fig.tight_layout()\n    \n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","227fada1":"kf = KFold(n_splits=5)\npreds = []\nfor train_index, valid_index in kf.split(X):\n    X_train, Y_train, X_val, Y_val = X[train_index], Y[train_index], X[valid_index], Y[valid_index]\n    train_gen = train_aug.flow(X_train, Y_train, batch_size=batch_size)\n    valid_gen = test_aug.flow(X_val, Y_val, batch_size=batch_size)\n    acc, model, history = train_model(train_gen, valid_gen, best_params)\n    \n    pred = model.predict(X_test)\n    preds.append(pred)","ce7a426a":"# predict results\nresults = np.mean(preds, axis=0)\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","18fcfc55":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","7f3e6b97":"## Hi\u1ec3n th\u1ecb m\u1ed9t s\u1ed1 tr\u01b0\u1eddng h\u1ee3p b\u1ecb sai\n\u0110\u1ec3 c\u00f3 c\u00e1i nh\u00ecn r\u00f5 h\u01a1n v\u1ec1 m\u1ed9t s\u1ed1 m\u1eabu b\u1ecb sai, ch\u00fang ta quan s\u00e1t top c\u00e1c m\u1eabu c\u00f3 gi\u00e1 tr\u1ecb d\u1ef1 \u0111o\u00e1n kh\u00e1c nh\u1ea5t so v\u1edbi nh\u00e3n th\u1eadt ","12cbc0e6":"# 4. X\u00e2y d\u01b0ng m\u00f4 h\u00ecnh CNN\nCNN bao g\u1ed3m t\u1eadp h\u1ee3p c\u00e1c l\u1edbp c\u01a1 b\u1ea3n bao g\u1ed3m: convolution layer + nonlinear layer, pooling layer, fully connected layer. C\u00e1c l\u1edbp n\u00e0y li\u00ean k\u1ebft v\u1edbi nhau theo m\u1ed9t th\u1ee9 t\u1ef1 nh\u1ea5t \u0111\u1ecbnh. Th\u00f4ng th\u01b0\u1eddng, m\u1ed9t \u1ea3nh s\u1ebd \u0111\u01b0\u1ee3c lan truy\u1ec1n qua t\u1ea7ng convolution layer + nonlinear layer \u0111\u1ea7u ti\u00ean, sau \u0111\u00f3 c\u00e1c gi\u00e1 tr\u1ecb t\u00ednh to\u00e1n \u0111\u01b0\u1ee3c s\u1ebd lan truy\u1ec1n qua pooling layer, b\u1ed9 ba convolution layer + nonlinear layer + pooling layer c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c l\u1eb7p l\u1ea1i nhi\u1ec1u l\u1ea7n trong network. V\u00e0 sau \u0111\u00f3 \u0111\u01b0\u1ee3c lan truy\u1ec1n qua t\u1ea7ng fully connected layer v\u00e0 softmax \u0111\u1ec3 t\u00ednh s\u00e1c xu\u1ea5t \u1ea3nh \u0111\u00f3 ch\u1ee9a v\u1eadt th\u1ebf g\u00ec.\n\n![](https:\/\/pbcquoc.github.io\/images\/cnn_model.png)\n### \u0110\u1ecbnh ngh\u0129a m\u00f4 h\u00ecnh\nCh\u00fang ta s\u1eed d\u1ee5ng Keras Sequential API \u0111\u1ec3 \u0111\u1ecbnh ngh\u0129a m\u00f4 h\u00ecnh. C\u00e1c layer \u0111\u01b0\u1ee3c th\u00eam v\u00e0o r\u1ea5t d\u1ec5 d\u00e0ng v\u00e0 t\u01b0\u01a1ng \u0111\u1ed1i linh \u0111\u1ed9ng. \n\u0110\u1ea7u ti\u00ean ch\u00fang ta s\u1eed d\u1ee5ng layer Conv2D tr\u00ean \u1ea3nh \u0111\u1ea7u v\u00e0o. Conv2D bao g\u1ed3m m\u1ed9t t\u1eadp c\u00e1c filters c\u1ea7n ph\u1ea3i h\u1ecdc. M\u1ed7i filters s\u1ebd tr\u01b0\u1ee3c qua to\u00e0n b\u1ed9 b\u1ee9c \u1ea3nh \u0111\u1ec3 detect c\u00e1c \u0111\u1eb7t tr\u01b0ng tr\u00ean b\u1ee9c \u1ea3nh \u0111\u00f3. \n\nPooling layer l\u00e0 t\u1ea7ng quan tr\u1ecdng v\u00e0 th\u01b0\u1eddng \u0111\u1ee9ng sau t\u1ea7ng Conv. T\u1ea7ng n\u00e0y c\u00f3 ch\u1ee9c n\u0103ng gi\u1ea3m chi\u1ec1u c\u1ee7a feature maps tr\u01b0\u1edbc \u0111\u00f3. \u0110\u1ed1i v\u1edbi max-pooling, t\u1ea7ng n\u00e0y ch\u1ec9 \u0111\u01a1n gi\u1ea3n ch\u1ecdn gi\u00e1 tr\u1ecb l\u1edbn nh\u1ea5t trong v\u00f9ng c\u00f3 k\u00edch th\u01b0\u1edbc pooling_size x pooling_size (th\u01b0\u1eddng l\u00e0 2x2). T\u1ea7ng pooling n\u00e0y \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 gi\u1ea3m chi ph\u00ed t\u00ednh to\u00e1n v\u00e0 gi\u1ea3m \u0111\u01b0\u1ee3c overfit c\u1ee7a m\u00f4 h\u00ecnh. \n\n\u0110\u1ed3ng th\u1eddi, Dropout c\u0169ng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 h\u1ea1n ch\u1ebf overfit. Dropout s\u1ebd b\u1ecf \u0111i ng\u1eabu nhi\u00ean c\u00e1c neuron b\u1eb1ng c\u00e1ch nh\u00e2n v\u1edbi mask zeros, do \u0111\u00f3, gi\u00fap m\u00f4 h\u00ecnh h\u1ecdc \u0111\u01b0\u1ee3c nh\u1eefng \u0111\u1eb7c tr\u01b0ng h\u1eefu \u00edch. Dropout trong h\u1ea7u h\u1ebft c\u00e1c tr\u01b0\u1eddng h\u1ee3p \u0111\u1ec1u gi\u00fap t\u0103ng \u0111\u1ed9 ch\u00ednh x\u00e1c v\u00e0 h\u1ea1n ch\u1ebft overfit c\u1ee7a m\u00f4 h\u00ecnh. \n\n\u1ede t\u1ea7ng cu\u1ed1i c\u00f9ng, ch\u00fang ta flatten feature matrix th\u00e0nh m\u1ed9t vector, sau \u0111\u00f3 s\u1eed d\u1ee5ng c\u00e1c t\u1ea7ng fully connected layers \u0111\u1ec3 ph\u00e2n lo\u1ea1i \u1ea3nh th\u00e0nh c\u00e1c l\u1edbp cho tr\u01b0\u1edbc.\n\n\u0110\u1ec3 gi\u00fap m\u00f4 h\u00ecnh h\u1ed9i t\u1ee5 g\u1ea7n v\u1edbi gobal minima ch\u00fang ta s\u1eed d\u1ee5ng annealing learning rate. Learning s\u1ebd \u0111\u01b0\u1ee3c \u0111i\u1ec1u ch\u1ec9nh nh\u1ecf d\u1ea7n sau m\u1ed7i l\u1ea7n c\u1eadp nh\u1eadt n\u1ebfu nh\u01b0 sau m\u1ed9t s\u1ed1 b\u01b0\u1edbc nh\u1ea5t \u0111\u1ecbnh m\u00e0 loss c\u1ee7a m\u00f4 h\u00ecnh kh\u00f4ng gi\u1ea3m n\u1eefa. \u0110\u1ec3 gi\u1ea3m th\u1eddi gian t\u00ednh to\u00e1n, ch\u00fang ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng learning ban \u0111\u1ea7u l\u1edbn, sau \u0111\u00f3 gi\u1ea3m d\u1ea7n \u0111\u1ec3 m\u00f4 h\u00ecnh h\u1ed9i t\u1ee5 nhanh h\u01a1n.\n\nNgo\u00e0i ra, ch\u00fang ta s\u1eed d\u1ee5ng early stopping \u0111\u1ec3 h\u1ea1n ch\u1ebf hi\u1ec7n t\u01b0\u1ee3ng overfit c\u1ee7a m\u00f4 h\u00ecnh. early stopping s\u1ebd d\u1eebng qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n n\u1ebfu nh\u01b0 loss tr\u00ean t\u1eadp validation t\u0103ng d\u1ea7n trong khi tr\u00ean t\u1eadp l\u1ea1i gi\u1ea3m. \n\n### S\u1eed d\u1ee5ng hyperas \u0111\u1ec3 tunning si\u00eau tham s\u1ed1\nTrong qu\u00e1 tr\u00ecnh \u0111\u1ecbnh ngh\u0129a m\u00f4 h\u00ecnh, ch\u00fang ta s\u1ebd l\u1ed3ng v\u00e0o \u0111\u00f3 c\u00e1c \u0111o\u1ea1n m\u00e3 \u0111\u1ec3 h\u1ed7 tr\u1ee3 qu\u00e1 tr\u00ecnh search si\u00eau tham s\u1ed1 \u0111\u00e3 \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a \u1edf tr\u00ean. Ch\u00fang ta s\u1ebd c\u1ea7n search c\u00e1c tham s\u1ed1 nh\u01b0 filter_size, pooling_size, dropout rate, dense size. \u0110\u1ed3ng th\u1eddi ch\u00fang ta c\u0169ng th\u1eed vi\u1ec7c \u0111i\u1ec1u ch\u1ec9nh c\u1ea3 optimizer c\u1ee7a m\u00f4 h\u00ecnh.","a90df70b":"# 1. Gi\u1edbi thi\u1ec7u\nTrong notebook n\u00e0y, m\u00ecnh s\u1ebd tr\u00ecnh b\u00e0y c\u00e1ch gi\u1ea3i quy\u1ebft \u0111\u1ec1 t\u00e0i tuy\u1ec3n d\u1ee5ng c\u1ee7a VinID. M\u00f4 h\u00ecnh CNN \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 ph\u00e2n lo\u1ea1i 10 s\u1ed1 vi\u1ebft tay trong b\u1ed9 MNIST. Trong notebook n\u00e0y,bao g\u1ed3m c\u00e1c ph\u1ea7n sau:\n\n* **1. Gi\u1edbi thi\u1ec7u**[](http:\/\/)\n* **2. Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u**\n    * 2.1 Load d\u1eef li\u1ec7u\n    * 2.2 Ki\u1ec3m tra missing value\n    * 2.3 Chu\u1ea9n h\u00f3a    \n    * 2.5 Label encoding\n    * 2.6 X\u00e2y d\u1ef1ng t\u1eadp train\/test\n* **3. Data augmentation**    \n* **4. X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh**\n    * 4.1 X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh\n* **5. Tunning parameter**    \n    * 5.1 Khai b\u00e1o kh\u00f4ng gian t\u00ecm ki\u1ebfm si\u00eau tham s\u1ed1\n    * 5.2 Grid search\n* **6. So s\u00e1nh c\u00e1c optimizer v\u00e0 loss function**    \n    * 6.1 So s\u00e1nh optimizer\n    * 6.2 So s\u00e1nh loss function\n* **7. \u0110\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh**\n    * 7.1 Confusion matrix\n* **8. D\u1ef1 \u0111o\u00e1n**\n    * 8.1 Predict and Submit results\n","7d064fe2":"# 6. So s\u00e1nh optimizers v\u00e0 loss\n## 6.1 So s\u00e1nh c\u00e1c optimzers\n\nM\u1ee5c ti\u00eau c\u1ee7a qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh ML l\u00e0 gi\u1ea3m \u0111\u1ed9 l\u1ed7i c\u1ee7a h\u00e0m loss function \u0111\u01b0\u1ee3c t\u00ednh b\u1eb1ng s\u1ef1 kh\u00e1c bi\u1ec7t c\u1ee7a gi\u00e1 tr\u1ecb m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n v\u00e0 gi\u00e1 tr\u1ecb th\u1ef1c t\u1ebf. \u0110\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c m\u1ee5c \u0111\u00edch n\u00e0y ch\u00fang ta th\u01b0\u1eddng s\u1eed d\u1ee5ng gradient descent. Gradient descent s\u1ebd c\u1eadp nh\u1eadt tr\u1ecdng s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh ng\u01b0\u1ee3c v\u1edbi chi\u1ec1u gradient \u0111\u1ec3 gi\u1ea3m \u0111\u1ed9 l\u1ed7i c\u1ee7a loss function. \n![image.png](https:\/\/miro.medium.com\/max\/414\/1*6a9Gx2UlB1ksh92TabyGPQ.png)\n\nCh\u00fang ta s\u1eed th\u01b0\u1eddng s\u1eed d\u1ee5ng 3 optimzer ph\u1ed5 bi\u1ebfn sau l\u00e0 adam, sgd, rmsprop \u0111\u1ec3 c\u1eadp nh\u1eadt tr\u1ecdng s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh. \nStochastic Gradient Descent l\u00e0 m\u1ed9t bi\u1ebfn th\u1ec3 c\u1ee7a Gradient Descent, y\u00eau c\u1ea7u ch\u00fang ta ph\u1ea3i shuffle d\u1ef1 li\u1ec7u tr\u01b0\u1edbc khi hu\u1ea5n luy\u1ec7n. Trong khi \u0111\u00f3 RMSProp v\u00e0 Adam l\u00e0 2 optimizer h\u01b0\u1edbng \u0111\u1ebfn vi\u1ec7c \u0111i\u1ec1u ch\u1ec9nh learning rate t\u1ef1 \u0111\u1ed9ng theo qu\u00e1 tr\u00ecnh h\u1ecdc.\n\nRMSprop (Root mean square propagation) \u0111\u01b0\u1ee3c gi\u1edbi thi\u1ec7u b\u1edfi Geoffrey Hinton. RMSProp gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 gi\u1ea3m d\u1ea7n learning rate c\u1ee7a Adagrad b\u1eb1ng c\u00e1ch chu\u1ea9n h\u00f3a learning v\u1edbi gradient g\u1ea7n v\u1edbi th\u1eddi \u0111i\u1ec3m c\u1eadp nh\u1eadt m\u00e0 th\u00f4i. \u0110\u1ec3 l\u00e0m \u0111\u01b0\u1ee3c \u0111i\u1ec1u n\u00e0y t\u00e1c gi\u1ea3 chia learning rate cho t\u1ed5ng b\u00ecnh ph\u01b0\u01a1ng gradient gi\u1ea3m d\u1ea7n. \n![](https:\/\/miro.medium.com\/max\/786\/1*adEDAdjulZUJisfzurVuWw.png)\n\nAdam l\u00e0 optimizer ph\u1ed5 bi\u1ebfn nh\u1ea5t t\u1ea1i th\u1eddi \u0111i\u1ec3m hi\u1ec7n t\u1ea1i. Adam c\u0169ng t\u00ednh learning ri\u00eang bi\u1ec7t cho t\u1eebng tham s\u1ed1, t\u01b0\u01a1ng t\u1ef1 nh\u01b0 RMSProp v\u00e0 Adagrad. Adam chu\u1ea9n h\u00f3a learning c\u1ee7a m\u1ed7i tham s\u1ed1 b\u1eb1ng first v\u00e0 second order moment c\u1ee7a gradient. ","2030034e":"Ch\u00fang ta th\u1ea5y r\u1eb1ng trong c\u00e1c lo\u1ea1i optimzer tr\u00ean. SGD h\u1ed9i t\u1ee5 l\u00e2u nh\u1ea5t v\u00e0 \u0111\u1ed9 ch\u00ednh x\u00e1c c\u0169ng th\u1ea5p nh\u1ea5t so v\u1edbi c\u00e1c optimizers c\u00f2n l\u1ea1i. Trong khi \u0111\u00f3, Adam h\u1ed9i t\u1ee5 nhanh nh\u1ea5t, v\u00e0 c\u00f3 \u0111\u1ed9 ch\u00ednh x\u00e1c cao nh\u1ea5t.","7c1c47d2":"## 5.2 Optimze \u0111\u1ec3 t\u00ecm b\u1ed9 tham s\u1ed1 t\u1ed1t nh\u1ea5t\nHyperas s\u1ebd ph\u00e1t sinh c\u00e1c b\u1ed9 tham s\u1ed1 gi\u1eef tr\u00ean kh\u00f4ng gian t\u00ecm ki\u1ebfm \u0111\u1ecbnh ngh\u0129a tr\u01b0\u1edbc c\u1ee7a ch\u00fang ta. Sau \u0111\u00f3 th\u01b0 vi\u1ec7n s\u1ebd h\u1ed7 tr\u1ee3 qu\u00e1 tr\u00ecnh t\u00ecm ki\u1ebfm c\u00e1c tham s\u1ed1 n\u00e0y \u0111\u01a1n gi\u1ea3n b\u1eb1ng m\u1ed9t s\u1ed1 API c\u00f3 s\u1eb5n. ","4a1b5a5b":"Ch\u1ea1y qu\u00e1 tr\u00ecnh search tham s\u1ed1. B\u1ed9 si\u00eau tham s\u1ed1 t\u1ed1t nh\u1ea5t s\u1ebd \u0111\u01b0\u1ee3c ghi nh\u1eadn l\u1ea1i \u0111\u1ec3 ch\u00fang ta s\u1eed d\u1ee5ng trong m\u00f4 h\u00ecnh cu\u1ed1i c\u00f9ng. ","cc19a929":"\u0110\u1ecbnh ngh\u0129a s\u1ed1 epochs c\u1ea7n hu\u1ea5n luy\u1ec7n v\u00e0 bachsize","6c7ea069":"C\u00e0i \u0111\u1eb7t th\u01b0 vi\u1ec7n hyperas \u0111\u1ec3 h\u1ed7 tr\u1ee1 qu\u00e1 tr\u00ecnh tunning si\u00eau tham s\u1ed1. Hyperas cung c\u1ea5p c\u00e1c api r\u1ea5t ti\u1ec7n l\u1ee3i cho qu\u00e1 tr\u00ecnh theo hu\u1ea5n luy\u1ec7n v\u00e0 theo d\u00f5i \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a model t\u1ea1i m\u1ed7i b\u1ed9 tham s\u1ed1. \n","15f64df2":"## 6.2 So s\u00e1nh c\u00e1c loss function \nTrong b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i nhi\u1ec1u l\u1edbp. Ch\u00fang ta th\u01b0\u1eddng s\u1eed d\u1ee5ng 2 lo\u1ea1i loss function sau:\n* Cross entropy\n* Kullback Leibler Divergence Loss\n\nCross entropy \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng ph\u1ed5 bi\u1ebfn nh\u1ea5t trong b\u00e0i to\u00e1n c\u1ee7a ch\u00fang ta. Cross entropy loss c\u00f3 n\u1ec1n t\u1ea3ng to\u00e1n h\u1ecdc c\u1ee7a maximun likelihood \u0111\u01b0\u1ee3c t\u00ednh b\u1eb1ng t\u1ed5ng c\u1ee7a s\u1ef1 kh\u00e1c bi\u1ec7t gi\u1eef gi\u00e1 tr\u1ecb d\u1ef1 \u0111o\u00e1n v\u00e0 gi\u00e1 tr\u1ecb th\u1ef1c t\u1ebf c\u1ee7a d\u1eef li\u1ec7u. Cross entropy error t\u1ed1t nh\u1ea5t khi c\u00f3 gi\u00e1 tr\u1ecb b\u1eb1ng 0.\n\nKL loss (Kullback Leibler Divergence Loss) th\u1ec3 hi\u1ec7n s\u1ef1 kh\u00e1c bi\u1ec7t gi\u1eef 2 ph\u00e2n b\u1ed1 x\u00e1c su\u1ea5t. KL loss b\u1eb1ng 0, ch\u1ee9ng t\u1ecf 2 ph\u00e2n b\u1ed1 n\u00e0y ho\u00e0n to\u00e0n gi\u1ed1ng nhau. \n\nCross entropy cho b\u1eb1ng to\u00e1n ph\u00e2n lo\u1ea1i nhi\u1ec1u l\u1edbn t\u01b0\u01a1ng \u0111\u1ed1i gi\u1ed1ng v\u1edbi KL Loss v\u1ec1 m\u1eb7t to\u00e1n h\u1ecdc, n\u00ean c\u00f3 th\u1ec3 xem 2 \u0111\u1ed9 l\u1ed7i n\u00e0y l\u00e0 m\u1ed9t trong b\u00e0i to\u00e1n c\u1ee7a ch\u00fang ta. ","fe26fb8f":"# 5. Hyper-params tunning\nCh\u00fang ta s\u1eed d\u1ee5ng Hyperas \u0111\u1ec3 tunning c\u00e1c tham s\u1ed1. Hyperas s\u1ebd ph\u00e1t sinh b\u1ed9 tham s\u1ed1 d\u1ef1a tr\u00ean khai b\u00e1o \u1edf tr\u00ean. Sau \u0111\u00f3 hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh v\u00e0 \u0111\u00e1nh gi\u00e1 tr\u00ean t\u1eadp validation. B\u1ed9 tham s\u1ed1 c\u00f3 \u0111\u1ed9 ch\u00ednh x\u00e1c cao nh\u1ea5t tr\u00ean t\u1eadp validation s\u1ebd \u0111\u01b0\u1ee3c ghi nh\u1eadn l\u1ea1i. ","5bc80db6":"L\u00fac ch\u1ea1y th\u1ef1c t\u1ebf, c\u1ea7n thay max_evals l\u00fac search tham s\u1ed1 th\u00e0nh 50 \u0111\u1ec3 c\u00f3 \u0111\u01b0\u1ee3c accuracy tr\u00ean t\u1eadp test > 0.997. K\u1ebft qu\u1ea3 d\u01b0\u1edbi \u0111\u00e2y \u0111\u01b0\u1ee3c submit tr\u00ean kaggle m\u00e0 kh\u00f4ng s\u1eed d\u1ee5ng kfold\n![](https:\/\/github.com\/pbcquoc\/pbcquoc.github.io\/raw\/master\/images\/kaggle.png)","e2be9000":"## 8. Kfold, Predict v\u00e0 submit k\u1ebft qu\u1ea3\nCh\u00fang ta hu\u1ea5n luy\u1ec7n l\u1ea1i m\u00f4 h\u00ecnh s\u1eed d\u1ee5ng kfold, k\u1ebft qu\u1ea3 cu\u1ed1i d\u1ef1 \u0111o\u00e1n cu\u1ed1i c\u00f9ng l\u00e0 trung b\u00ecnh c\u1ed9ng s\u1ef1 \u0111o\u00e1n c\u1ee7a c\u00e1c m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n tr\u00ean m\u1ed7i fold.\nCh\u00fang ta ch\u1ecdn nh\u00e3n l\u00e0 l\u1edbp \u0111\u01b0\u1ee3c d\u1ef1 \u0111o\u00e1n c\u00f3 x\u00e1c su\u1ea5t cao nh\u1ea5t m\u00e0 m\u00f4 h\u00ecnh nh\u1eadn d\u1ea1ng \u0111\u01b0\u1ee3c","38c7f29d":"## Ki\u1ec3m tra ph\u00e2n b\u1ed1 c\u1ee7a nh\u00e3n\nCh\u00fang ta th\u1ea5y r\u1eb1ng s\u1ed1 l\u01b0\u1ee3ng m\u1eabu d\u1eef li\u1ec7u cho m\u1ed7i nh\u00e3n t\u01b0\u01a1ng \u0111\u01b0\u01a1ng nhau. ","a2eb409f":"Th\u1eed nh\u00ecn qua m\u1ed9t s\u1ed1 m\u1eabu trong t\u1eadp hu\u1ea5n luy\u1ec7n. Ch\u00fang ta th\u1ea5y r\u1eb1ng h\u1ea7u h\u1ebft c\u00e1c \u1ea3nh \u0111\u1ec1u r\u00f5 n\u00e9t v\u00e0 t\u01b0\u01a1ng \u0111\u1ed1i d\u1ec5 d\u00e0ng \u0111\u1ec3 nh\u1eadn d\u1ea1ng. ","62553916":"Plot qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh v\u1edbi 2 lo\u1ea1i loss function kh\u00e1c nhau.","3892acbf":"## 3. Data Augmentation\nK\u0129 thu\u1eadt data augmentation \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 ph\u00e1t sinh th\u00eam nh\u1eefng m\u1eabu d\u1eef li\u1ec7u m\u1edbi b\u1eb1ng c\u00e1ch \u00e1p d\u1ee5ng c\u00e1c k\u0129 thu\u1eadt x\u1eed l\u00fd \u1ea3nh tr\u00ean b\u1ee9c \u1ea3nh. C\u00e1c ph\u00e9p bi\u1ebfn \u0111\u1ed5i nh\u1ecf n\u00e0y ph\u1ea3i \u0111\u1ea3m b\u1ea3o kh\u00f4ng l\u00e0m thay \u0111\u1ed5i nh\u00e3n c\u1ee7a b\u1ee9c \u1ea3nh. \n\nM\u1ed9t s\u1ed1 k\u0129 thu\u1eadt ph\u1ed5 bi\u1ebfn c\u1ee7a data augmentation nh\u01b0 l\u00e0:\n* Rotation: Xoay m\u1ed9t g\u00f3c nh\u1ecf\n* Translation: T\u00ednh ti\u1ebfn\n* Brightness, Staturation: Thay \u0111\u1ed5i \u0111\u1ed9 s\u00e1ng, t\u01b0\u01a1ng ph\u1ea3n \n* Zoom: zoom to\/nh\u1ecf b\u1ee9c \u1ea3nh\n* Elastic Distortion: bi\u1ebfn d\u1ea1ng b\u1ee9c \u1ea3nh\n* Flip: l\u1eadt tr\u00e1i\/ph\u1ea3i\/tr\u00ean\/d\u01b0\u1edbi.\n\n\u1ede d\u01b0\u1edbi \u0111\u00e2y, ch\u00fang ta s\u1ebd ch\u1ecdn xoay 1 g\u00f3c trong 0-10 \u0111\u1ed9. Zoom \u1ea3nh 0.1 l\u1ea7n, t\u1ecbnh ti\u1ebfn 0.1 l\u1ea7n m\u1ed7i chi\u1ec1u.","4fc282d1":"V\u1edbi c\u00e1c m\u1eabu \u1ea3nh sai, ch\u00fang ta c\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng nh\u1eefng m\u1eabu n\u00e0y r\u1ea5t kh\u00f3 nh\u1eadn d\u1ea1ng nh\u1ea7m l\u1eabn s\u00e1ng c\u00e1c l\u1edbp kh\u00e1c. v\u00ed d\u1ee5 s\u1ed1 9 v\u00e0 4 hay l\u00e0 3 v\u00e0 8","bec50109":"K\u1ebft qu\u1ea3 tr\u00ean t\u1eadp validation kh\u00e1 cao v\u1edbi acc > 99%","aeac1722":"# 7. \u0110\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh. \nCh\u00fang ta s\u1ebd xem x\u00e9t m\u1ed9t s\u1ed1 l\u1ed7i c\u1ee7a m\u00f4 h\u00ecnh d\u1ef1 hu\u1ea5n luy\u1ec7n \u0111\u01b0\u1ee3c. M\u1ed9t s\u1ed1 l\u1ed7i d\u1ec5 d\u00e0ng \u0111\u01b0\u1ee3c ph\u00e1t hi\u1ec7n b\u1eb1ng confusion matrix th\u1ec3 hi\u1ec7n x\u00e1c xu\u1ea5t\/s\u1ed1 \u1ea3nh b\u1ecb ph\u00e2n lo\u1ea1i nh\u1ea7m th\u00e0nh l\u1edbp kh\u00e1c.","8924602c":"Plot qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh v\u1edbi 3 l\u1ecdai optimizers kh\u00e1c nhau.","6145815f":"## 5.1 Khai b\u00e1o kh\u00f4ng gian t\u00ecm ki\u1ebfm si\u00eau tham s\u1ed1\nC\u00f3 r\u1ea5t nhi\u1ec1u si\u00eau tham s\u1ed1 c\u1ea7n \u0111\u01b0\u1ee3c tunning nh\u01b0: ki\u1ebfn tr\u00fac m\u1ea1ng, s\u1ed1 filter, k\u00edch th\u01b0\u1edbc m\u1ed7i filters, k\u00edch th\u01b0\u1edbc pooling, c\u00e1c c\u00e1ch kh\u1edfi t\u1ea1o, h\u00e0m k\u00edch ho\u1ea1t, t\u1ec9 l\u1ec7 dropout,... Trong ph\u1ea7n n\u00e0y, ch\u00fang ta s\u1ebd t\u1eadp trung v\u00e0o c\u00e1c tham s\u1ed1  nh\u01b0 k\u00edch th\u01b0\u1edbc filter, s\u1ed1 filters, pooling size.\n\n\u0110\u1ea7u ti\u00ean, ch\u00fang ta c\u1ea7n khai b\u00e1o c\u00e1c si\u00eau tham \u0111\u1ec3 hyperas c\u00f3 th\u1ec3 t\u00ecm ki\u1ebfm trong t\u1eadp \u0111\u1ea5y. \u1ede m\u1ed7i t\u1ea7ng conv, ch\u00fang ta s\u1ebd tunning k\u00edch th\u01b0\u1edbc filter, filter size. \u1ede t\u1ea7ng pooling, k\u00edch th\u01b0\u1edbc pooling size s\u1ebd \u0111\u01b0\u1ee3c tunning. \u0110\u1ed3ng th\u1eddi, t\u1ec9 l\u1ec7 dropout \u1edf t\u1ea7ng Dropout c\u0169ng \u0111\u01b0\u1ee3c tunning. S\u1ed1 filters \u1edf t\u1ea7ng conv th\u01b0\u1eddng t\u1eeb 16 -> 1024, k\u00edch th\u01b0\u1edbc filter hay th\u01b0\u1eddng d\u00f9ng nh\u1ea5t trong l\u00e0 3 v\u1edbi 5. C\u00f2n t\u1ec9 l\u1ec7 dropout n\u1eb1m trong \u0111o\u1ea1n 0-1","9954c63e":"Hu\u1ea5n luy\u1ec7n l\u1ea1i m\u00f4 h\u00ecnh v\u1edbi b\u1ed9 tham s\u1ed1 t\u1ed1t nh\u1ea5t \u1edf tr\u00ean.","6a9bc7ce":"Ch\u00fang ta th\u1ea5y r\u1eb1ng kh\u00f4ng c\u00f3 s\u1ef1 kh\u00e1c bi\u1ec7t r\u00f5 r\u1eb1ng v\u1ec1 t\u1ed1c \u0111\u1ed9 h\u1ed9i t\u1ee5 gi\u1eef 2 h\u00e0m loss function l\u00e0 cross-entropy v\u00e0 KL loss trong b\u00e0i to\u00e1n c\u1ee7a ch\u00fang ta.","e3b7d91b":"# 2. Ti\u1ec1n x\u1eed l\u00fd\n\u0110\u1ecdc b\u1ed9 d\u1eef li\u1ec7u MNIST, ch\u00fang ta chia b\u1ed9 d\u1eef li\u1ec7u th\u00e0nh t\u1eadp train\/valid\/test. \u0110\u1ed3ng th\u1eddi chu\u1ea9n h\u00f3a d\u1eef li\u1ec7u v\u1ec1 kho\u1ea3ng [0-1] \u0111\u1ec3 gi\u00fap t\u0103ng t\u1ed1c qu\u00e1 tr\u00ecnh h\u1ed9i t\u1ee5. T\u1eadp valid c\u1ee7a ch\u00fang ta s\u1ebd g\u1ed3m 20% t\u1eadp train. ","58c4fe86":"C\u00e1c gi\u00e1 tr\u1ecb tr\u00ean \u0111\u01b0\u1eddng ch\u00e9o r\u1ea5t cao, ch\u00fang ta m\u00f4 h\u00ecnh ch\u00fang ta c\u00f3 \u0111\u1ed9 ch\u00ednh x\u00e1c r\u1ea5t t\u1ed1t.\nNh\u00ecn v\u00e0o confusion matrix \u1edf tr\u00ean, ch\u00fang ta c\u00f3 m\u1ed9t s\u1ed1 nh\u1eadn x\u00e9t nh\u01b0 sau:\n* S\u1ed1 4 hay nh\u1ea7m l\u1eabn v\u1edbi s\u1ed1 9, b\u1edfi v\u00ec khi vi\u1ebft tay \u0111\u01b0\u1eddng n\u00e9t c\u1ee7a 2 s\u1ed1 n\u00e0y t\u01b0\u01a1ng t\u1ef1 nhau kh\u00e1 nhi\u1ec1u\n* s\u1ed1 3 v\u00e0 s\u1ed1 8, c\u0169ng hay b\u1ecb t\u00ecnh tr\u1ea1ng t\u01b0\u01a1ng t\u1ef1. "}}