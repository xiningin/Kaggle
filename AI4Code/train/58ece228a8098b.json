{"cell_type":{"6b23ae36":"code","915e0e4e":"code","710d21d2":"code","cf3ba77f":"code","0ce64990":"code","af7816ac":"code","2bd8b53f":"code","307d7894":"code","d0a338f5":"code","c678d795":"code","c2381e82":"code","100d3489":"code","84fd4903":"code","970e9e48":"markdown","70d1ca21":"markdown","a9c26f98":"markdown","a0b1022c":"markdown","3cc043f3":"markdown","5f74f9a2":"markdown","1a8a712a":"markdown","0af7da61":"markdown"},"source":{"6b23ae36":"import numpy as np\nimport pandas as pd\n\nimport lightgbm as lgb\nfrom sklearn import model_selection","915e0e4e":"train_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')","710d21d2":"train_features['TARGET'] = 1\ntest_features['TARGET'] = 0","cf3ba77f":"data = pd.concat([train_features, test_features])","0ce64990":"data['cp_type'].value_counts(normalize=True)","af7816ac":"data = data[data['cp_type'] == 'trt_cp'].copy()\ndata.head()","2bd8b53f":"data = data.drop(['sig_id', 'cp_type'], axis=1)","307d7894":"data['cp_dose'] = data['cp_dose'].astype('category')","d0a338f5":"X_data = data.drop(['TARGET'], axis=1)\ny_data = data['TARGET']","c678d795":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X_data, y_data, train_size=0.33, shuffle=True)","c2381e82":"train = lgb.Dataset(X_train, label=y_train)\ntest = lgb.Dataset(X_test, label=y_test)","100d3489":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 5,\n         'learning_rate': 0.2,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 44,\n         \"metric\": 'auc',\n         \"verbosity\": -1}","84fd4903":"num_round = 50\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)","970e9e48":"Let's also remove the sig_id and cp_type features. The TARGET will be removed further up the notebook.","70d1ca21":"## Train and test split\n\nNow let's separate this new dataset into train and test. Don't forget to shuffle!","a9c26f98":"## Processing the new dataset\n\nBefore we continue, take a look at this:\n\n> cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs\n\nSince control perturbations (8% of our dataset) have no MoAs - and we will not be predicting for them - let's remove them.","a0b1022c":"Let's change the data type of the cp_dose to 'category' so our model knows how to interpret it.","3cc043f3":"# Adversarial Validation on MoA Dataset\n\nIn this notebook, we'll take a look at how similar the train and test dataset are. The way we're going to do this is by using a technique called [Adversarial Validation](http:\/\/fastml.com\/adversarial-validation-part-one\/).\n\nWe're going to assign new labels to the data, TRAIN (1) or TEST (0), then we'll train a classifier that will try to predict if an example comes from the train or test dataset. We hope the classifier perform no better than random - this would correspond to ROC AUC of 0.5, as said by Zygmunt in the post linked above.","5f74f9a2":"Great, we're fine! The validation's AUC was close to 0.50. Our model was not able to distinguish train from test, so we can expect a good validation in this competition. :)","1a8a712a":"## Creating the model\n\nLet's create our classifier and see how it performs","0af7da61":"## Reading the datasets\n\nLet's start by reading the data, and assigning the new label and concatenating it so it becomes one dataset."}}