{"cell_type":{"92ff6eda":"code","d88391d3":"code","f20eb92e":"code","805ba8bb":"code","bec6a070":"code","2e9752c9":"code","684b0dd3":"code","ed0e5e2c":"code","f83b86b3":"code","094e6a1c":"code","aec918cb":"code","93b4957d":"code","ac1b91ff":"code","695093aa":"code","9b313f65":"code","40746bde":"code","6154788f":"code","f6658412":"code","4a434e2c":"code","ba7455a3":"code","912cf324":"code","00f157c1":"markdown","b7d781f7":"markdown","70ac45de":"markdown","cd427c09":"markdown","204f0f86":"markdown","ab47100b":"markdown","af0ccdf7":"markdown","914c6114":"markdown","2ba13f45":"markdown","0a84a461":"markdown","d1188072":"markdown","986857fa":"markdown","42b1c36a":"markdown","03317eb7":"markdown","2a5317cf":"markdown"},"source":{"92ff6eda":"import pandas as pd\nimport numpy as np\n\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom category_encoders.woe import WOEEncoder\nfrom category_encoders.target_encoder import TargetEncoder\nfrom category_encoders.sum_coding import SumEncoder\nfrom category_encoders.m_estimate import MEstimateEncoder\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\nfrom category_encoders.helmert import HelmertEncoder\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom category_encoders.james_stein import JamesSteinEncoder\nfrom category_encoders.one_hot import OneHotEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","d88391d3":"%%time\ndef concat_cols(df, cols):\n    it = iter(cols)\n    concated = df[next(it)].copy()\n    for i in it:\n        concated += df[i]\n    return concated\n\ndef ticket_features(df):\n    s = df['Ticket'].str.split(expand=True)[0]\n    check = ~(s.str.isdigit().fillna(True))\n    df['letter_ticket'] = s[check].str.lower().str.replace(r\"[\\.|\\s|\\\/]+\", \"\", regex=True)\n    df['number_ticket'] = df['Ticket'].str.extract(r'(\\d+)')[0].astype(float)\n\n    return df\n\ndef cabin_features(df):\n    df['letter_cabin'] = df['Cabin'].str.extract(r'(\\D+)')[0].str.lower()\n    df['number_cabin'] = df['Cabin'].str.extract(r'(\\d+)')[0].astype(float)\n    \n    return df\n\ndef fill_null(df):\n    cols = df.dtypes.to_dict()\n    \n    for i, _type  in cols.items():\n        if _type == np.int_:\n            df[i] = df[i].fillna(df[i].mean())\n        if _type == np.float_:\n            df[i] = df[i].fillna(df[i].median())\n        if _type == np.object_:\n            df[i] = df[i].fillna('x')\n    \n    return df\n\ndf = (\n    pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv', index_col='PassengerId')\n    .pipe(ticket_features)\n    .pipe(cabin_features)\n    .pipe(fill_null)\n    .drop(['Name', 'Ticket', 'Cabin'], axis=1)\n)\n\ncat_features = ['Sex', 'Embarked', 'letter_ticket', 'letter_cabin']\ntarget_col = 'Survived'\ntarget = df[target_col]\n\ndf","f20eb92e":"%%time\nlabel_encoder = OrdinalEncoder(cat_features).fit(df)","805ba8bb":"%%time\nOHE_encoder = OneHotEncoder(cat_features).fit(df)","bec6a070":"%%time\nSE_encoder = SumEncoder(cat_features).fit(df, target)","2e9752c9":"%%time\nHE_encoder = HelmertEncoder(cat_features).fit(df, target)","684b0dd3":"%%time\nTE_encoder = TargetEncoder(cat_features).fit(df, target)","ed0e5e2c":"%%time\nMEE_encoder = MEstimateEncoder(cat_features).fit(df, target)","f83b86b3":"%%time\nWOE_encoder = WOEEncoder(cat_features).fit(df, target)","094e6a1c":"%%time\nJSE_encoder = JamesSteinEncoder(cat_features).fit(df, target)","aec918cb":"%%time\nLOOE_encoder = LeaveOneOutEncoder(cat_features).fit(df, target)","93b4957d":"%%time\nCBE_encoder = CatBoostEncoder(cat_features).fit(df, target)","ac1b91ff":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb","695093aa":"cat_features = ['Sex', 'Embarked', 'letter_ticket', 'letter_cabin']\n\nencoders = {\n    'Label Encoder': OrdinalEncoder(cat_features),\n    'One-Hot Encoder': OneHotEncoder(cat_features),\n    'Sum Encoder': SumEncoder(cat_features),\n    'Helmert Encoder': HelmertEncoder(cat_features),\n    'Target Encoder': TargetEncoder(cat_features),\n    'M-Estimate Encoder': MEstimateEncoder(cat_features),\n    'Weight Of Evidence Encoder': WOEEncoder(cat_features),\n    'James-Stein Encoder': JamesSteinEncoder(cat_features),\n    'Leave-one-out Encoder': LeaveOneOutEncoder(cat_features),\n    'Catboost Encoder': CatBoostEncoder(cat_features),\n}","9b313f65":"def run_model(df_raw, model, encoder, target='Survived'):\n    features_raw = df_raw.columns.drop(target)\n\n    X = df_raw[features_raw]\n    y = df_raw[target]\n\n    cv = KFold()\n    auc_scores = []\n    for train, test in cv.split(X, y):\n        y_train = y.iloc[train]\n        X_train = encoder.fit_transform(X.iloc[train], y_train)\n        y_test = y.iloc[test]\n        X_test = encoder.transform(X.iloc[test])\n        \n        model.fit(X_train, y_train)\n        auc_scores.append(roc_auc_score(y_test, model.predict(X_test)))\n    return np.mean(auc_scores)","40746bde":"%%time\nresults = {}\nfor name, i in encoders.items():\n    results[name] = run_model(df, lgb.LGBMRegressor(), i)","6154788f":"pd.DataFrame.from_dict(results, orient='index', columns=['auc']).style.highlight_max()","f6658412":"def concat_cols(df, cols):\n    it = iter(cols)\n    aux = df[next(it)]\n    for i in it:\n        aux += df[i]\n    return aux\n\ndef ticket_reducing_cat(df):\n    contain_letter = lambda letter, regex: df['letter_ticket'].str.contains(letter, regex=regex).astype('int').astype('str')\n    letter_dict = {\n        'letter_ticket__a': contain_letter('a', regex=False),\n        'letter_ticket__c': contain_letter('c', regex=False),\n        'letter_ticket__p': contain_letter('p', regex=False),\n        'letter_ticket__s': contain_letter(r'(?!stono|sotono)s', regex=True),\n        'letter_ticket__stono': contain_letter(r'(stono|sotono)', regex=True),\n        'letter_ticket__paris': contain_letter(r'(paris)', regex=True),\n    }\n    \n    new_df = df.assign(**letter_dict)\n    new_df['letter_ticket__concat'] = concat_cols(new_df, letter_dict.keys())\n    new_df = new_df.drop(list(letter_dict.keys())+['letter_ticket'], axis=1)\n    return new_df\n","4a434e2c":"cat_features = ['Sex', 'Embarked', 'letter_ticket', 'letter_cabin', 'letter_ticket__concat']\n\nencoders = {\n    'Label Encoder': OrdinalEncoder(cat_features),\n    'One-Hot Encoder': OneHotEncoder(cat_features),\n    'Sum Encoder': SumEncoder(cat_features),\n    'Helmert Encoder': HelmertEncoder(cat_features),\n    'Target Encoder': TargetEncoder(cat_features),\n    'M-Estimate Encoder': MEstimateEncoder(cat_features),\n    'Weight Of Evidence Encoder': WOEEncoder(cat_features),\n    'James-Stein Encoder': JamesSteinEncoder(cat_features),\n    'Leave-one-out Encoder': LeaveOneOutEncoder(cat_features),\n    'Catboost Encoder': CatBoostEncoder(cat_features),\n}\n\ndf_filter = ticket_reducing_cat(df)","ba7455a3":"%%time\nresults = {}\nfor name, i in encoders.items():\n    results[name] = run_model(df_filter, lgb.LGBMRegressor(), i)","912cf324":"pd.DataFrame.from_dict(results, orient='index', columns=['auc']).style.highlight_max()","00f157c1":"[back to top](#table-of-contents)\n<a id=\"10\"><\/a>\n\n# 10. Catboost Encoder\n---\n**Catboost** is a recently created target-based categorical encoder. \n\nIt is intended to overcome target leakage problems inherent in LOO. \n\nIf you use `Category-Encoders` it will look like this code below.","b7d781f7":"<a id='table-of-contents'><\/a>\n# Table of Contents\n---\n* [0. Introduction](#0)\n* [1. Label Encoder](#1)\n* [2. One-Hot Encoder](#2)\n* [3. Sum Encoder](#3)\n* [4. Helmert Encoder](#4)\n* [5. Target Encoder](#5)\n* [6. M-Estimate Encoder](#6)\n* [7. Weight Of Evidence Encoder](#7)\n* [8. James-Stein Encoder](#8)\n* [9. Leave-one-out Encoder](#9)\n* [10. Catboost Encoder](#10)\n* [11. Validation](#11)\n* [12. Reducing Cardinality](#12)","70ac45de":"[back to top](#table-of-contents)\n<a id=\"12\"><\/a>\n\n# 12. Reducing Cardinality","cd427c09":"[back to top](#table-of-contents)\n<a id=\"3\"><\/a>\n# 3. Sum Encoder (Deviation Encoder, Effect Encoder)\n---\nThis encoding technique is also known as Deviation Encoding or Effect Encoding. Sum encoding is almost similar to dummy encoding, with a little difference. In dummy coding, we use 0 and 1 to represent the data but in effect encoding, we use three values i.e. 1,0, and -1, comparing the mean of the dependent variable (target) for a given level of a categorical column to the overall mean of the target. \n\nIf you use `Category-Encoders` it will look like this code below.","204f0f86":"[back to top](#table-of-contents)\n<a id=\"9\"><\/a>\n\n# 9. Leave-one-out Encoder (LOO or LOOE)\n---\n\n**Leave-one-out Encoding** is another example of target-based encoders.\n\nThis encoder calculate mean target of category k for observation j if observation j is removed from the dataset:\n\n$$\\hat{x}^k_i = \\frac{\\sum_{j \\neq i}(y_j * (x_j == k) ) - y_i }{\\sum_{j \\neq i} x_j == k}$$\n\nWhile encoding the test dataset, a category is replaced with the mean target of the category k in the train dataset:\n\n$$\\hat{x}^k = \\frac{\\sum y_j * (x_j == k)  }{\\sum x_j == k}$$\n\nIf you use `Category-Encoders` it will look like this code below.","ab47100b":"[back to top](#table-of-contents)\n<a id=\"2\"><\/a>\n# 2. One-Hot Encoder (OHE, dummy encoder)\n---\nSo what can you do to give values \u200b\u200bby category instead of ordering them?\n\nIf you have data with specific category values, you can create a column. If the base Label Encoder label type is N, then OHE is the way to create N columns.\n\nSince only the row containing the content is given as 1, it is called one-hot encoding. Also called dummy encoding in the sense of creating a dummy.\n\n\nIn this competition:\n\n``` python\ntraintest = pd.concat([train, test])\ndummies = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\ntrain_ohe = dummies.iloc[:train.shape[0], :]\ntest_ohe = dummies.iloc[train.shape[0]:, :]\ntrain_ohe = train_ohe.sparse.to_coo().tocsr()\ntest_ohe = test_ohe.sparse.to_coo().tocsr()\n```\n\nIf you use `Category-Encoders` it will look like this code below.","af0ccdf7":"[back to top](#table-of-contents)\n<a id=\"8\"><\/a>\n\n# 8. James-Stein Encoder\n---\n**James-Stein Encoder** is a target-based encoder.\n\nThe idea behind James-Stein Encoder is simple. Estimation of the mean target for category k could be calculated according to the following formula:\n\n$$\\hat{x}^k = (1-B) * \\frac{n^+}{n} + B * \\frac{y^+}{y} $$\n\nOne way to select B is to tune it like a hyperparameter via cross-validation, but Charles Stein came up with another solution to the problem:\n\n$$B = \\frac{Var[y^k]}{Var[y^k] + Var[y]}$$\n\nSeems quite fair, but James-Stein Estimator has a big disadvantage \u2014 it is defined only for normal distribution (which is not the case for any classification task). \n\nTo avoid that, we can either convert binary targets with a log-odds ratio as it was done in WoE Encoder (which is used by default because it is simple) or use beta distribution.\n\nIf you use `Category-Encoders` it will look like this code below.","914c6114":"[back to top](#table-of-contents)\n<a id=\"5\"><\/a>\n# 5. Target Encoder\n---\n\nThis is a work in progress for many kernels.\n\nThe encoded category values are calculated according to the following formulas:\n\n$$s = \\frac{1}{1+exp(-\\frac{n-mdl}{a})}$$\n\n$$\\hat{x}^k = prior * (1-s) + s * \\frac{n^{+}}{n}$$\n\n- mdl means **'min data in leaf'**\n- a means **'smooth parameter, power of regularization'**\n\nTarget Encoder is a powerful, but it has a huuuuuge disadvantage \n\n> **target leakage**: it uses information about the target. \n\nTo reduce the effect of target leakage, \n\n- Increase regularization\n- Add random noise to the representation of the category in train dataset (some sort of augmentation)\n- Use Double Validation (using other validation)\n\nLet's use while being careful about overfitting.\n\nIf you use `Category-Encoders` it will look like this code below.","2ba13f45":"[back to top](#table-of-contents)\n<a id=\"11\"><\/a>\n\n# 11. Validation","0a84a461":"<a id=\"0\"><\/a>\n# 0. Introduction\n---\n1. **References**\n- [11 Categorical Encoders and Benchmark](https:\/\/www.kaggle.com\/subinium\/11-categorical-encoders-and-benchmark) - The encoders' description is from this notebook \n- [CategoricalEncodingBenchmark](https:\/\/github.com\/DenisVorotyntsev\/CategoricalEncodingBenchmark)\n\n2. **Methodology**\n- Some Feature Engineeging (Just in `Ticket` and `Cabin`)\n- Fill null with `mean`, `median` and `x`\n- Remove `Name`\n- `KFold(5)` for Cross Validation\n- LightGBM for Modeling","d1188072":"[back to top](#table-of-contents)\n<a id=\"7\"><\/a>\n# 7. Weight of Evidence Encoder \n---\n**Weight Of Evidence** is a commonly used target-based encoder in credit scoring. \n\nIt is a measure of the \u201cstrength\u201d of a grouping for separating good and bad risk (default). \n\nIt is calculated from the basic odds ratio:\n\n``` python\na = Distribution of Good Credit Outcomes\nb = Distribution of Bad Credit Outcomes\nWoE = ln(a \/ b)\n```\n\nHowever, if we use formulas as is, it might lead to **target leakage**(and overfit).\n\nTo avoid that, regularization parameter a is induced and WoE is calculated in the following way:\n\n$$nomiinator = \\frac{n^+ + a}{y^+ + 2*a}$$\n\n$$denominator = ln(\\frac{nominator}{denominator})$$\n\nIf you use `Category-Encoders` it will look like this code below.","986857fa":"[back to top](#table-of-contents)\n<a id=\"1\"><\/a>\n# 1. Label Encode\n---\nAn encoding method that converts categorical data into numbers.\nThe code is very simple, and when you encode a specific column you can proceed as follows:\n\n``` python\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\n\ntrain[column_name] = label.fit_transform(train[column_name])\n```\n\nThe simple idea is to convert the same class to a number with the same value.\nSo the range of numbers maps from 0 to n-1 as labels.\n\nThe disadvantage is that the labels are ordered randomly (in the existing order of the data), which can add noise while assigning an unexpected order between labels. In other words, the data becomes ordinary (ordinal, ordered) data, which can lead to unintended consequences.\n\nIf you use `Category-Encoders` it will look like this code below.","42b1c36a":"[back to top](#table-of-contents)","03317eb7":"[back to top](#table-of-contents)\n<a id=\"4\"><\/a>\n# 4. Helmert Encoder\n---\n\n**Helmert Encoding** is a third commonly used type of categorical encoding for regression along with OHE and Sum Encoding. \n\nIt compares each level of a categorical variable to the mean of the subsequent levels. \n\nThis type of encoding can be useful in certain situations where levels of the categorical variable are ordered.\n\nIf you use `Category-Encoders` it will look like this code below.","2a5317cf":"[back to top](#table-of-contents)\n<a id=\"6\"><\/a>\n# 6. M-Estimate Encoder\n---\n\n**M-Estimate Encoder** is a **simplified version of Target Encoder**. It has only one hyperparameter (Wrong Fomular but did good work?!)\n\n$$\\hat{x}^k = \\frac{n^+ + prior * m}{y^+ + m}$$\n\nThe higher value of m results into stronger shrinking. Recommended values for m is in the range of 1 to 100.\n\nIf you use `Category-Encoders` it will look like this code below."}}