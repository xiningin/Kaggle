{"cell_type":{"84dedd5e":"code","e885ff69":"code","1ecb33b7":"code","ad69d8a5":"code","70d5e6d1":"code","f93d3ed2":"code","e270f1b8":"code","d47d082f":"code","2750d9eb":"code","b0da197b":"code","9eacfebf":"code","df6e5dc7":"code","de532214":"code","2d4b4f34":"code","d5b9a29f":"code","61506016":"code","82a6c9bd":"code","96d12627":"code","8621dd25":"code","4f0e2900":"code","0126bb50":"code","68a54814":"code","952fb354":"code","c2981890":"code","1f3cfa53":"code","e0dbd552":"code","abce27ec":"code","f20c9f42":"code","9cd5de87":"code","66c801cc":"code","85b6a62d":"code","f840f022":"code","3be1521b":"code","c3cc19ee":"code","e474062f":"code","924327aa":"code","b1db7744":"code","5dd93afb":"code","e764a54c":"code","762b9e4d":"code","5f02d547":"code","fa723c8d":"code","7e19ff75":"code","4e464af5":"code","279157b5":"code","d30b8143":"code","79bda2c1":"code","2ca771ce":"code","8ec87f5d":"code","cff44584":"code","4ff4ec1e":"code","eb9e8301":"code","871ed261":"code","5955c1bc":"code","5742585d":"code","999fc8f6":"code","0b3a2150":"code","1b3b084d":"code","7ac45323":"code","786dd374":"code","442c0d43":"code","a23a5854":"code","b936ef76":"code","5c394391":"code","c1ba6c73":"code","cd1190d7":"code","dda72033":"code","02887c68":"code","3084c36d":"code","5c2ba5cb":"code","aa41ca08":"code","7919bef7":"code","71ed300b":"code","022d8977":"code","e86fb9fa":"code","15bc2a0c":"code","a218ab8d":"code","9b150de9":"code","189df159":"code","d6d23051":"code","2ab1cebc":"code","b47257f3":"code","054bc3a6":"code","d872b4d5":"code","53812561":"code","66aec8ab":"code","7b31c0dd":"code","cc1fa8f0":"code","f1598f16":"code","054b9a0c":"code","a960b635":"code","2fa0c7be":"code","d5cd6bfb":"code","4094fcbb":"code","96d1bb50":"code","ed4ff3c8":"code","ae1c87b1":"code","b581d43f":"code","23fa24ec":"code","db24a0d1":"code","2eec5428":"code","d2f64c17":"code","41d6cb9c":"code","6a92d87b":"code","d32f75a3":"code","0d9dc781":"code","590342de":"code","0c40a9c7":"code","38767266":"code","bd1cfa27":"code","5ebb3fb4":"code","b52472f6":"code","996b1f61":"code","249b382a":"code","9a407f4d":"code","8401ae84":"code","2b41d98d":"code","e8acd461":"code","8540825c":"code","e9ca5ae2":"code","b8d5f89c":"code","c85730c8":"code","75457d8c":"code","1dda6880":"code","1c43c5d8":"code","22ffbca4":"code","ee6ca6d6":"code","4829c088":"code","219f78b5":"code","12edffdf":"code","7ff3553b":"code","d9350972":"code","c5a77bc1":"code","3d09e5a0":"code","9f8a6db1":"code","ab58e368":"code","6dc35781":"code","20de942c":"code","f1b0c677":"code","fa9bfce8":"code","667e7e58":"code","14b65ef9":"code","db0ec426":"code","efbee012":"code","b81dbc19":"code","6def37b8":"code","8c154c9b":"code","37fdc8e3":"code","5f4c4c6c":"code","7af47462":"code","c6f32f3e":"code","951334eb":"code","c689a996":"code","288dc3f6":"code","4bf4d96c":"code","2220c349":"code","38380490":"markdown","ad51938b":"markdown","daaad108":"markdown","c09c1df5":"markdown","1484abb6":"markdown","95e0d6b2":"markdown","380133f7":"markdown","a9520fe4":"markdown","00abc008":"markdown","747269a3":"markdown","46d0cca0":"markdown","2250e461":"markdown","06a4759e":"markdown","19cd1f49":"markdown","769bd76b":"markdown","2f25a74c":"markdown","d9d8ab17":"markdown","1ac32878":"markdown","62d31485":"markdown","aca9480d":"markdown","30a65343":"markdown","292d09cf":"markdown","b43f7f6d":"markdown","5e39a479":"markdown","6d34291a":"markdown","444777ce":"markdown","ff942179":"markdown","d96963ce":"markdown","d7139ba8":"markdown","9de6236b":"markdown","5488b9d2":"markdown","e13c7ba4":"markdown","95b59c44":"markdown","7e8017cd":"markdown","2b05e94f":"markdown","7a9a7bce":"markdown","ed340fbe":"markdown","774827af":"markdown","82fdcda2":"markdown","938e480d":"markdown","21b26b7f":"markdown","94dfd3da":"markdown","aaa2cf57":"markdown","433902ce":"markdown","99e808d9":"markdown","fc58143a":"markdown","91fa7c54":"markdown","336f2c88":"markdown","004919e5":"markdown","d54b5218":"markdown","5aaabd5f":"markdown","2675b052":"markdown","aee30a60":"markdown","d86de8d6":"markdown","0226f7ac":"markdown","c9369547":"markdown","58a0bded":"markdown","aa4512a6":"markdown","127c8860":"markdown","10494883":"markdown","0d3f7645":"markdown","4cabe5cd":"markdown","524d8006":"markdown","f98361a4":"markdown","47d8bdfb":"markdown","014c0a69":"markdown","395ba6eb":"markdown","2b0fa43e":"markdown","24c63af0":"markdown","1f3d042a":"markdown","c28859f6":"markdown","5affd583":"markdown","76da8105":"markdown","9728ab7f":"markdown","d619a4d8":"markdown","6b1862b5":"markdown","89813534":"markdown","3fcce412":"markdown","a444e893":"markdown","3cf31c58":"markdown","46e1b535":"markdown","b5e14a23":"markdown","dd3131fd":"markdown","8bef5f37":"markdown","58cf1e75":"markdown","cdfa7ead":"markdown","bacd2e38":"markdown","530d329a":"markdown","bc6ff059":"markdown","54391cdf":"markdown","b096fd59":"markdown","c9f6ff5d":"markdown"},"source":{"84dedd5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport os\nimport warnings\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score, make_scorer\n\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) \n# will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) \n# that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, \n# but they won't be saved outside of the current session\n\n# Ignore pandas warnings\nwarnings.filterwarnings('ignore')\n\n#!pip install flake8 pycodestyle_magic\n#%load_ext pycodestyle_magic\n#%pycodestyle_on","e885ff69":"# load data\ntrain_data = pd.read_csv(r\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(r\"\/kaggle\/input\/titanic\/test.csv\")","1ecb33b7":"train_set = train_data.copy()\ntest_set = test_data.copy()","ad69d8a5":"train = train_set.sample(frac=0.75, random_state=0)\nval = train_set.drop(train.index)","70d5e6d1":"# Creating a copy of our datasets\ntrainData = train.copy()\nvalData = val.copy()\ntestData = test_set.copy()","f93d3ed2":"# Let put the PassengerId variable apart for the testing.\ntest_PassengerID = testData['PassengerId']","e270f1b8":"TotalPassengers = len(trainData) + len(valData) + len(testData)\nTotalPassengers","d47d082f":"train.head()","2750d9eb":"train.columns.tolist()","b0da197b":"train.info()","9eacfebf":"total = train.isnull().sum().sort_values(ascending=False)\npercent_1 = train.isnull().sum()\/train.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data","df6e5dc7":"sns.heatmap(train.isnull(), cbar=False)","de532214":"# grouping data by Survival type\ndata = train.groupby(\"Survived\")[\"PassengerId\"].count()\ndata","2d4b4f34":"pie, ax = plt.subplots(figsize=[10, 6])\nlabels = data.keys()\nplt.pie(x=data,\n        autopct=\"%.1f%%\",\n        explode=[0.05]*2,\n        labels=labels,\n        pctdistance=0.5)\nplt.title(\"Survived Passengers\", fontsize=14)","d5b9a29f":"# grouping data by Survival type\ndata = train.groupby(\"Pclass\")[\"PassengerId\"].count()\ndata","61506016":"pie, ax = plt.subplots(figsize=[10, 6])\nlabels = data.keys()\nplt.pie(x=data,\n        autopct=\"%.1f%%\",\n        explode=[0.05]*3,\n        labels=labels,\n        pctdistance=0.5)\nplt.title(\"Pclass analysis\", fontsize=14)","82a6c9bd":"# grouping data by Survival type\ndata = train.groupby(\"Sex\")[\"PassengerId\"].count()\ndata","96d12627":"pie, ax = plt.subplots(figsize=[10, 6])\nlabels = data.keys()\nplt.pie(x=data,\n        autopct=\"%.1f%%\",\n        explode=[0.05]*2,\n        labels=labels,\n        pctdistance=0.5)\nplt.title(\"Gender analysis\", fontsize=14)","8621dd25":"# grouping data by Survival type\ndata = train.groupby(\"SibSp\")[\"PassengerId\"].count()\ndata","4f0e2900":"pie, ax = plt.subplots(figsize=[10, 6])\nlabels = data.keys()\nplt.pie(x=data,\n        autopct=\"%.1f%%\",\n        explode=[0.05]*7,\n        labels=labels,\n        pctdistance=0.5)\nplt.title(\"SibSp analysis\", fontsize=14)","0126bb50":"# grouping data by Survival type\ndata = train.groupby(\"Parch\")[\"PassengerId\"].count()\ndata","68a54814":"pie, ax = plt.subplots(figsize=[10, 6])\nlabels = data.keys()\nplt.pie(x=data,\n        autopct=\"%.1f%%\",\n        explode=[0.05]*7,\n        labels=labels,\n        pctdistance=0.5)\nplt.title(\"Parch analysis\", fontsize=14)","952fb354":"# grouping data by Survival type\ndata = train.groupby(\"Embarked\")[\"PassengerId\"].count()\ndata","c2981890":"pie, ax = plt.subplots(figsize=[10, 6])\nlabels = data.keys()\nplt.pie(x=data,\n        autopct=\"%.1f%%\",\n        explode=[0.05]*3,\n        labels=labels,\n        pctdistance=0.5)\nplt.title(\"Embarked analysis\", fontsize=14)","1f3cfa53":"train.Age.describe()","e0dbd552":"fig, axs = plt.subplots(figsize=[10, 6], ncols=3)\nsns.scatterplot(data=train[\"Age\"], ax=axs[0])\nsns.boxplot(y=train[\"Age\"], ax=axs[1])\nsns.distplot(train[\"Age\"], ax=axs[2])","abce27ec":"qt = QuantileTransformer(n_quantiles=100,\n                         output_distribution='normal',\n                         random_state=0)\nqt_transf = qt.fit_transform(train[\"Age\"].values.reshape(-1,1))\nsns.distplot(qt_transf)","f20c9f42":"train.Fare.describe()","9cd5de87":"train.Fare.quantile([.80, .85, .90, .95])","66c801cc":"fig, axs = plt.subplots(figsize=[16, 6], ncols=3)\nsns.scatterplot(data=train[\"Fare\"], ax=axs[0])\nsns.boxplot(y=train[\"Fare\"], ax=axs[1])\nsns.distplot(train[\"Fare\"], ax=axs[2])","85b6a62d":"qt = QuantileTransformer(n_quantiles=100,\n                         output_distribution='normal',\n                         random_state=0)\nqt_transf = qt.fit_transform(train[\"Fare\"].values.reshape(-1, 1))\nsns.distplot(qt_transf)","f840f022":"train.describe(include=['O'])","3be1521b":"fig, axs = plt.subplots(figsize=[15, 6], ncols=2)\nsns.barplot(x='Pclass', y='Survived', data=train, ax=axs[0])\nprop_df = (train['Survived']\n           .groupby(train['Pclass'])\n           .value_counts(normalize=True)\n           .rename('%Survived')\n           .reset_index())\nsns.barplot(x='Survived',\n            y='%Survived',\n            hue='Pclass',\n            data=prop_df,\n            ax=axs[1])","c3cc19ee":"fig, axs = plt.subplots(figsize=[15, 6], ncols=2)\nsns.barplot(x='Sex', y='Survived', data=train, ax=axs[0])\nprop_df = (train['Survived']\n           .groupby(train['Sex'])\n           .value_counts(normalize=True)\n           .rename('%Survived')\n           .reset_index())\nsns.barplot(x='Survived',\n            y='%Survived',\n            hue='Sex',\n            data=prop_df,\n            ax=axs[1])","e474062f":"fig, axs = plt.subplots(figsize=[15, 6], ncols=2)\nsns.barplot(x='Embarked', y='Survived', data=train, ax=axs[0])\nprop_df = (train['Survived']\n           .groupby(train['Embarked'])\n           .value_counts(normalize=True)\n           .rename('%Survived')\n           .reset_index())\nsns.barplot(x='Survived',\n            y='%Survived',\n            hue='Embarked',\n            data=prop_df,\n            ax=axs[1])","924327aa":"fig, axs = plt.subplots(figsize=[10, 6], ncols=2)\nsns.boxplot(x='Survived', y='Age', data=train, ax=axs[0])\nsns.boxplot(x='Survived', y='Age', hue='Sex', data=train, ax=axs[1])","b1db7744":"fig, axs = plt.subplots(figsize=[10, 6], ncols=2)\nsns.boxplot(x='Survived', y='Fare', data=train, ax=axs[0])\nsns.boxplot(x='Survived', y= 'Fare', hue='Sex', data= train, ax=axs[1])","5dd93afb":"fig, axs = plt.subplots(figsize=[10, 6], ncols=2)\nsns.boxplot(x='Pclass', y='Fare', data=train, ax=axs[0])\nsns.boxplot(x='Pclass', y='Fare', hue='Sex', data=train, ax=axs[1])","e764a54c":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(figsize=[10, 6], ncols=2)\nwomen = train[train['Sex'] == 'female']\nmen = train[train['Sex'] == 'male']\nax = sns.distplot(women[women['Survived'] == 1].Age.dropna(),\n                  bins=18, label=survived, ax=axes[0], kde=False)\nax = sns.distplot(women[women['Survived'] == 0].Age.dropna(),\n                  bins=40, label=not_survived, ax=axes[0], kde=False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived'] == 1].Age.dropna(),\n                  bins=18, label=survived, ax=axes[1], kde=False)\nax = sns.distplot(men[men['Survived'] == 0].Age.dropna(),\n                  bins=40, label=not_survived, ax=axes[1], kde=False)\nax.legend()\n_ = ax.set_title('Male')","762b9e4d":"FacetGrid = sns.FacetGrid(train, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex',\n              palette=None,  order=None, hue_order=None)\nFacetGrid.add_legend()","5f02d547":"grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","fa723c8d":"sns.factorplot('SibSp', 'Survived', data=train, aspect=2.5)","7e19ff75":"sns.factorplot('Parch', 'Survived', data=train, aspect=2.5)","4e464af5":"trainPrepr = train.copy()","279157b5":"trainPrepr['InfoCabin'] = np.where(trainPrepr['Cabin'].isnull(), 0, 1)","d30b8143":"trainPrepr['InfoCabin'].value_counts()","79bda2c1":"# grouping data by Survival type\ndata = trainPrepr.groupby(\"InfoCabin\")[\"PassengerId\"].count()\n\npie, ax = plt.subplots(figsize=[10, 6])\nlabels = data.keys()\nplt.pie(x=data, autopct=\"%.1f%%\", explode=[0.05]*2,\n        labels=labels, pctdistance=0.5)\nplt.title(\"InfoCabin\", fontsize=14)","2ca771ce":"trainPrepr['Cabin'] = trainPrepr['Cabin'].astype(str)\ntrainPrepr['NbCabin'] = trainPrepr['Cabin'].str.split().apply(len)\ntrainPrepr.loc[trainPrepr.Cabin == 'nan', ['NbCabin']] = 0","8ec87f5d":"trainPrepr['NbCabin'].value_counts()","cff44584":"deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ntrainPrepr['Deck'] = trainPrepr['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\ntrainPrepr['Deck'] = trainPrepr['Deck'].map(deck)\ntrainPrepr['Deck'] = trainPrepr['Deck'].fillna(0)\ntrainPrepr['Deck'] = trainPrepr['Deck'].astype(int)\n","4ff4ec1e":"trainPrepr.head()","eb9e8301":"mean = trainPrepr[\"Age\"].mean()\nstd = trainPrepr[\"Age\"].std()\nis_null = trainPrepr[\"Age\"].isnull().sum()\n# compute random numbers between the mean, std and is_null\nrand_age = np.random.randint(mean - std, mean + std, size=is_null)\n# fill NaN values in Age column with random values generated\nage_slice = trainPrepr[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\ntrainPrepr[\"Age\"] = age_slice\ntrainPrepr[\"Age\"] = trainPrepr[\"Age\"].astype(int)","871ed261":"trainPrepr['Embarked'] = trainPrepr['Embarked'].fillna(trainPrepr.Embarked.mode().iloc[0])","5955c1bc":"sns.heatmap(trainPrepr.isnull(), cbar=False)","5742585d":"titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n# extract titles\ntrainPrepr['Title'] = trainPrepr.Name.str.extract(' ([A-Za-z]+)\\.',\n                                                  expand=False)\n# replace titles with a more common title or as Rare\ntrainPrepr['Title'] = trainPrepr['Title'].replace(['Lady', 'Countess',\n                                                   'Capt', 'Col',\n                                                   'Don', 'Dr',\n                                                   'Major', 'Rev',\n                                                   'Sir', 'Jonkheer',\n                                                   'Dona'],\n                                                  'Rare')\ntrainPrepr['Title'] = trainPrepr['Title'].replace('Mlle', 'Miss')\ntrainPrepr['Title'] = trainPrepr['Title'].replace('Ms', 'Miss')\ntrainPrepr['Title'] = trainPrepr['Title'].replace('Mme', 'Mrs')\n# convert titles into numbers\ntrainPrepr['Title'] = trainPrepr['Title'].map(titles)\n# filling NaN with 0, to get safe\ntrainPrepr['Title'] = trainPrepr['Title'].fillna(0)","999fc8f6":"trainPrepr.Title.value_counts()","0b3a2150":"genders = {\"male\": 0, \"female\": 1}\ntrainPrepr['Sex'] = trainPrepr['Sex'].map(genders)","1b3b084d":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ntrainPrepr['Embarked'] = trainPrepr['Embarked'].map(ports)","7ac45323":"qt = QuantileTransformer(n_quantiles=100,\n                         output_distribution='normal',\n                         random_state=0)\nqt_transf = qt.fit_transform(trainPrepr[\"Age\"].values.reshape(-1, 1))\nsns.distplot(qt_transf)","786dd374":"pd.Series(qt_transf.ravel()).describe()","442c0d43":"c1 = min(qt_transf.ravel()) - 1\nc2 = np.mean(qt_transf.ravel()) - np.std(qt_transf.ravel())\nc3 = np.mean(qt_transf.ravel())\nc4 = np.mean(qt_transf.ravel()) + np.std(qt_transf.ravel())\nc5 = max(qt_transf.ravel()) + 1\n\nbins = [c1, c2, c3, c4, c5]\ntrainPrepr['AgeCat'] = pd.cut(qt_transf.ravel(),\n                              bins=bins,\n                              labels=[0, 1, 2, 3])","a23a5854":"sns.displot(qt_transf.ravel(), bins=100)\nplt.axvline(c2, linewidth=4, color='r')\nplt.axvline(c3, linewidth=4, color='r')\nplt.axvline(c4, linewidth=4, color='r')","b936ef76":"# grouping data by Survival type\ndata = trainPrepr.groupby(\"AgeCat\")[\"PassengerId\"].count()\n\npie, ax = plt.subplots(figsize=[10, 6])\nlabels = data.keys()\nplt.pie(x=data, autopct=\"%.1f%%\", explode=[0.05]*4,\n        labels=labels, pctdistance=0.5)\nplt.title(\"AgeCat\", fontsize=14)","5c394391":"trainPrepr[['AgeCat', 'Survived']].groupby(['AgeCat'],\n                                           as_index=False).mean().sort_values(by='AgeCat',\n                                                                              ascending=True)","c1ba6c73":"qt = QuantileTransformer(n_quantiles=100,\n                         output_distribution='normal',\n                         random_state=0)\nqt_transf = qt.fit_transform(trainPrepr[\"Fare\"].values.reshape(-1, 1))\nsns.distplot(qt_transf)","cd1190d7":"pd.Series(qt_transf.ravel()).describe()","dda72033":"c1 = min(qt_transf.ravel()) - 1\nc2 = np.mean(qt_transf.ravel()) - np.std(qt_transf.ravel())\nc3 = np.mean(qt_transf.ravel())\nc4 = np.mean(qt_transf.ravel()) + np.std(qt_transf.ravel())\nc5 = max(qt_transf.ravel()) + 1\n\nbins = [c1, c2, c3, c4, c5]\ntrainPrepr['FareCat'] = pd.cut(qt_transf.ravel(),\n                              bins=bins,\n                              labels=[0, 1 ,2, 3])","02887c68":"sns.displot(qt_transf.ravel(), bins=100)\nplt.axvline(c2, linewidth=4, color='r')\nplt.axvline(c3, linewidth=4, color='r')\nplt.axvline(c4, linewidth=4, color='r')","3084c36d":"# grouping data by Survival type\ndata = trainPrepr.groupby(\"FareCat\")[\"PassengerId\"].count()\n\npie, ax = plt.subplots(figsize=[10, 6])\nlabels = data.keys()\nplt.pie(x=data, autopct=\"%.1f%%\", explode=[0.05]*4,\n        labels=labels, pctdistance=0.5)\nplt.title(\"FareCat\", fontsize=14)","5c2ba5cb":"trainPrepr[['FareCat', 'Survived']].groupby(['FareCat'], as_index=False).mean().sort_values(by='FareCat', ascending=True)","aa41ca08":"trainPrepr['FamilySize'] = trainPrepr['SibSp'] + trainPrepr['Parch'] + 1\ntrainPrepr[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","7919bef7":"sns.countplot(data=trainPrepr, x='FamilySize', order=trainPrepr.FamilySize.value_counts().index)","71ed300b":"trainPrepr['FamilySizeCat'] = trainPrepr['FamilySize']\ntrainPrepr.loc[trainPrepr['FamilySizeCat'] > 3, ['FamilySizeCat']] = 4\nsns.countplot(data=trainPrepr, x='FamilySizeCat',\n              order=trainPrepr.FamilySizeCat.value_counts().index)","022d8977":"trainPrepr['IsAlone'] = 0\ntrainPrepr.loc[trainPrepr['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrainPrepr[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","e86fb9fa":"# grouping data by Survival type\ndata = trainPrepr.groupby(\"IsAlone\")[\"PassengerId\"].count()\n\npie, ax = plt.subplots(figsize=[10, 6])\nlabels = data.keys()\nplt.pie(x=data, autopct=\"%.1f%%\", explode=[0.05]*2, labels=labels, pctdistance=0.5)\nplt.title(\"IsAlone\", fontsize=14)","15bc2a0c":"trainPrepr['Age_Class']= trainPrepr['Age'] * trainPrepr['Pclass'] ","a218ab8d":"fig, axs = plt.subplots(figsize=[16, 6], ncols=3)\nsns.scatterplot(data=trainPrepr[\"Age_Class\"], ax=axs[0])\nsns.boxplot(y=trainPrepr[\"Age_Class\"], ax=axs[1])\nsns.distplot(trainPrepr[\"Age_Class\"], ax=axs[2])","9b150de9":"trainPrepr['Fare_Per_Person'] = trainPrepr['Fare']\/(trainPrepr['FamilySize'])\ntrainPrepr['Fare_Per_Person'] = trainPrepr['Fare_Per_Person'].astype(int)","189df159":"fig, axs = plt.subplots(figsize=[16, 6], ncols=3)\nsns.scatterplot(data=trainPrepr[\"Fare_Per_Person\"], ax=axs[0])\nsns.boxplot(y=trainPrepr[\"Fare_Per_Person\"], ax=axs[1])\nsns.distplot(trainPrepr[\"Fare_Per_Person\"], ax=axs[2])","d6d23051":"# Let's take a last look at the training set, \n# before we start training the models.\ntrainPrepr.head(10)","2ab1cebc":"trainPrepr = trainPrepr.drop(['PassengerId', 'Name',\n                              'Ticket', 'Cabin'], axis=1)","b47257f3":"trainPrepr.info()","054bc3a6":"trainPrepr['Pclass'] = trainPrepr['Pclass'].astype(str)\ntrainPrepr['Embarked'] = trainPrepr['Embarked'].astype(str)\ntrainPrepr['Title'] = trainPrepr['Title'].astype(str)\nTrainFinal = pd.get_dummies(trainPrepr,\n                            columns=['Pclass', 'Embarked', 'Title'],\n                            prefix=['Pclass', 'Embarked', 'Title'],\n                            drop_first=True)","d872b4d5":"print(TrainFinal.shape)\nTrainFinal.head()","53812561":"TrainFinal.info()","66aec8ab":"from sklearn.pipeline import Pipeline\n\n\nclass DataframeFunctionTransformer():\n    def __init__(self, func):\n        self.func = func\n\n    def transform(self, input_df, **transform_params):\n        return self.func(input_df)\n\n    def fit(self, X, y=None, **fit_params):\n        return self","7b31c0dd":"# this function takes a dataframe as input and\n# returns a modified version thereof\ndef process_dataframe(input_df):\n    \n    input_df['InfoCabin'] = np.where(input_df['Cabin'].isnull(), 0, 1)\n    input_df['Cabin'] = input_df['Cabin'].astype(str)\n    input_df['NbCabin'] = input_df['Cabin'].str.split().apply(len)\n    input_df.loc[input_df.Cabin == 'nan', ['NbCabin']] = 0\n    \n    deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n    input_df['Deck'] = input_df['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    input_df['Deck'] = input_df['Deck'].map(deck)\n    input_df['Deck'] = input_df['Deck'].fillna(0)\n    input_df['Deck'] = input_df['Deck'].astype(int)\n    \n    mean = input_df[\"Age\"].mean()\n    std = input_df[\"Age\"].std()\n    is_null = input_df[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size=is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = input_df[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    input_df[\"Age\"] = age_slice\n    input_df[\"Age\"] = input_df[\"Age\"].astype(int)\n    \n    meanFare = input_df[\"Fare\"].mean()\n    stdFare = input_df[\"Fare\"].std()\n    is_nullFare = input_df[\"Fare\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_Fare = np.random.randint(meanFare - stdFare,\n                                  meanFare + stdFare,\n                                  size=is_nullFare)\n    # fill NaN values in Age column with random values generated\n    Fare_slice = input_df[\"Fare\"].copy()\n    Fare_slice[np.isnan(Fare_slice)] = rand_Fare\n    input_df[\"Fare\"] = Fare_slice\n    input_df[\"Fare\"] = input_df[\"Fare\"].astype(int)\n    \n    input_df['Embarked'] = input_df['Embarked'].fillna(input_df.Embarked.mode().iloc[0])\n    input_df['Pclass'] = input_df['Pclass'].fillna(input_df.Pclass.mode().iloc[0])\n    input_df['Sex'] = input_df['Sex'].fillna(input_df.Sex.mode().iloc[0])\n    input_df['SibSp'] = input_df['SibSp'].fillna(input_df.SibSp.mode().iloc[0])\n    input_df['Parch'] = input_df['Parch'].fillna(input_df.Parch.mode().iloc[0])\n    \n    titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    # extract titles\n    input_df['Title'] = input_df.Name.str.extract(' ([A-Za-z]+)\\.',\n                                                  expand=False)\n    # replace titles with a more common title or as Rare\n    input_df['Title'] = input_df['Title'].replace(['Lady', 'Countess',\n                                                   'Capt', 'Col', 'Don',\n                                                   'Dr', 'Major',\n                                                   'Rev', 'Sir',\n                                                   'Jonkheer', 'Dona'],\n                                                  'Rare')\n    input_df['Title'] = input_df['Title'].replace('Mlle', 'Miss')\n    input_df['Title'] = input_df['Title'].replace('Ms', 'Miss')\n    input_df['Title'] = input_df['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    input_df['Title'] = input_df['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    input_df['Title'] = input_df['Title'].fillna(0)\n    \n    genders = {\"male\": 0, \"female\": 1}\n    input_df['Sex'] = input_df['Sex'].map(genders)\n    \n    ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n    input_df['Embarked'] = input_df['Embarked'].map(ports)\n    \n    qt = QuantileTransformer(n_quantiles=100,\n                             output_distribution='normal',\n                             random_state=0)\n    qt_transf_Age = qt.fit_transform(input_df[\"Age\"].values.reshape(-1, 1))\n    c1 = min(qt_transf_Age.ravel()) - 1\n    c2 = np.mean(qt_transf_Age.ravel()) - np.std(qt_transf_Age.ravel())\n    c3 = np.mean(qt_transf_Age.ravel())\n    c4 = np.mean(qt_transf_Age.ravel()) + np.std(qt_transf_Age.ravel())\n    c5 = max(qt_transf_Age.ravel()) + 1\n\n    bins = [c1, c2, c3, c4, c5]\n    input_df['AgeCat'] = pd.cut(qt_transf_Age.ravel(),\n                                bins=bins,\n                                labels=[0, 1, 2, 3])\n    \n    input_df['AgeCat'] = input_df['AgeCat'].astype(int)\n    \n    qt_transf_Fare = qt.fit_transform(input_df[\"Fare\"].values.reshape(-1, 1))\n    c1 = min(qt_transf_Fare.ravel()) - 1\n    c2 = np.mean(qt_transf_Fare.ravel()) - np.std(qt_transf_Fare.ravel())\n    c3 = np.mean(qt_transf_Fare.ravel())\n    c4 = np.mean(qt_transf_Fare.ravel()) + np.std(qt_transf_Fare.ravel())\n    c5 = max(qt_transf_Fare.ravel()) + 1\n\n    bins = [c1, c2, c3, c4, c5]\n    input_df['FareCat'] = pd.cut(qt_transf_Fare.ravel(),\n                                 bins=bins,\n                                 labels=[0, 1, 2, 3])\n    \n    input_df['FareCat'] = input_df['FareCat'].astype(int)\n    \n    input_df['FamilySize'] = input_df['SibSp'] + input_df['Parch'] + 1\n    input_df['FamilySizeCat'] = input_df['FamilySize']\n    input_df.loc[input_df['FamilySizeCat'] > 3, ['FamilySizeCat']] = 4\n    \n    input_df['IsAlone'] = 0\n    input_df.loc[input_df['FamilySize'] == 1, 'IsAlone'] = 1\n    \n    input_df['Age_Class'] = input_df['Age'] * input_df['Pclass']\n    \n    input_df['Fare_Per_Person'] = input_df['Fare'] \/ (input_df['FamilySize'])\n    input_df['Fare_Per_Person'] = input_df['Fare_Per_Person'].astype(int)\n    \n    input_df = input_df.drop(['PassengerId', 'Name',\n                              'Ticket', 'Cabin'], axis=1)\n    \n    input_df['Pclass'] = input_df['Pclass'].astype(str)\n    input_df['Embarked'] = input_df['Embarked'].astype(str)\n    input_df['Title'] = input_df['Title'].astype(str)\n    input_df = pd.get_dummies(input_df,\n                              columns=['Pclass', 'Embarked', 'Title'],\n                              prefix=['Pclass', 'Embarked', 'Title'],\n                              drop_first=True)\n    col = input_df.columns.tolist()\n    colNum = []\n    for elem in col:\n        if max(input_df[elem]) > 1:\n            colNum.append(elem)\n    for elem in colNum:\n        scaler = MinMaxScaler()\n        input_df[elem] = scaler.fit_transform(input_df[elem].values.reshape(-1, 1))\n    return input_df","cc1fa8f0":"# this pipeline has a single step\npipeline = Pipeline([\n    (\"transform\", DataframeFunctionTransformer(process_dataframe))\n])","f1598f16":"# apply the pipeline to the input dataframe\ntrainData = pipeline.fit_transform(trainData)","054b9a0c":"trainData.head(10)","a960b635":"trainData.describe()","2fa0c7be":"plt.figure(figsize=(16, 7))\n\ncor = trainData.corr()\nmask = np.zeros_like(cor, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(cor, mask=mask, annot=True)","d5cd6bfb":"valData = pipeline.transform(valData)","4094fcbb":"valData.head()","96d1bb50":"X_train = trainData.drop(\"Survived\", axis=1)\nY_train = trainData[\"Survived\"]\nX_val = valData.drop(\"Survived\", axis=1)\nY_val = valData[\"Survived\"]\n","ed4ff3c8":"sgd = SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\n\nscore = round(sgd.score(X_train, Y_train) * 100, 2)\nprint(\"Training score: \", score)","ae1c87b1":"# Predicting and accuracy check\nY_pred = sgd.predict(X_val)\n\ncm = confusion_matrix(Y_val, Y_pred)\nprint(cm)","b581d43f":"sns.heatmap(cm, annot=True)","23fa24ec":"cr = classification_report(Y_val, Y_pred)\nacc_sgd = f1_score(Y_pred, Y_val, average='macro')\nprint(cr)","db24a0d1":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nscore = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(\"Training score: \", score)","2eec5428":"# Predicting and accuracy check\nY_pred = random_forest.predict(X_val)\n\ncm = confusion_matrix(Y_val, Y_pred)\nprint(cm)","d2f64c17":"sns.heatmap(cm, annot=True)","41d6cb9c":"cr = classification_report(Y_val, Y_pred)\nacc_random_forest = f1_score(Y_pred, Y_val, average='macro')\nprint(cr)","6a92d87b":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nscore = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(\"Training score: \", score)","d32f75a3":"# Predicting and accuracy check\nY_pred = logreg.predict(X_val)\n\ncm = confusion_matrix(Y_val, Y_pred)\nprint(cm)","0d9dc781":"sns.heatmap(cm, annot=True)","590342de":"cr = classification_report(Y_val, Y_pred)\nacc_log = f1_score(Y_pred, Y_val, average='macro')\nprint(cr)","0c40a9c7":"# KNN\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, Y_train)\n\nscore = round(knn.score(X_train, Y_train) * 100, 2)\nprint(\"Training score: \", score)","38767266":"# Predicting and accuracy check\nY_pred = knn.predict(X_val)\n\ncm = confusion_matrix(Y_val, Y_pred)\nprint(cm)","bd1cfa27":"sns.heatmap(cm, annot=True)","5ebb3fb4":"cr = classification_report(Y_val, Y_pred)\nacc_knn = f1_score(Y_pred, Y_val, average='macro')\nprint(cr)","b52472f6":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\n\nscore = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(\"Training score: \", score)","996b1f61":"# Predicting and accuracy check\nY_pred = gaussian.predict(X_val)\n\ncm = confusion_matrix(Y_val, Y_pred)\nprint(cm)","249b382a":"sns.heatmap(cm, annot=True)","9a407f4d":"cr = classification_report(Y_val, Y_pred)\nacc_gaussian = f1_score(Y_pred, Y_val, average='macro')\nprint(cr)","8401ae84":"perceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\n\nscore = round(perceptron.score(X_train, Y_train) * 100, 2)\nprint(\"Training score: \", score)","2b41d98d":"# Predicting and accuracy check\nY_pred = perceptron.predict(X_val)\n\ncm = confusion_matrix(Y_val, Y_pred)\nprint(cm)","e8acd461":"sns.heatmap(cm, annot=True)","8540825c":"cr = classification_report(Y_val, Y_pred)\nacc_perceptron = f1_score(Y_pred, Y_val, average='macro')\nprint(cr)","e9ca5ae2":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nscore = round(linear_svc.score(X_train, Y_train) * 100, 2)\nprint(\"Training score: \", score)","b8d5f89c":"# Predicting and accuracy check\nY_pred = linear_svc.predict(X_val)\n\ncm = confusion_matrix(Y_val, Y_pred)\nprint(cm)","c85730c8":"sns.heatmap(cm, annot=True)","75457d8c":"cr = classification_report(Y_val, Y_pred)\nacc_linear_svc = f1_score(Y_pred, Y_val, average='macro')\nprint(cr)","1dda6880":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train)  \n\nscore = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(\"Training score: \", score)","1c43c5d8":"# Predicting and accuracy check\nY_pred = decision_tree.predict(X_val)\n\ncm = confusion_matrix(Y_val, Y_pred)\nprint(cm)","22ffbca4":"sns.heatmap(cm, annot=True)","ee6ca6d6":"cr = classification_report(Y_val, Y_pred)\nacc_decision_tree = f1_score(Y_pred, Y_val, average='macro')\nprint(cr)","4829c088":"voting_clf = VotingClassifier(estimators=[('SGD', sgd),\n                                          ('RF', random_forest),\n                                          ('LR', logreg),\n                                          ('KNN', knn),\n                                          ('NaiveBayes', gaussian),\n                                          ('Perceptron', perceptron),\n                                          ('SVC', linear_svc),\n                                          ('DT', decision_tree)],\n                              voting='hard')\nvoting_clf.fit(X_train, Y_train)\n\n\nscore = round(voting_clf.score(X_train, Y_train) * 100, 2)\nprint(\"Training score: \", score)","219f78b5":"# Predicting and accuracy check\nY_pred = voting_clf.predict(X_val)\n\ncm = confusion_matrix(Y_val, Y_pred)\nprint(cm)","12edffdf":"sns.heatmap(cm, annot=True)","7ff3553b":"cr = classification_report(Y_val, Y_pred)\nacc_voting_clf = f1_score(Y_pred, Y_val, average='macro')\nprint(cr)","d9350972":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression',\n              'Random Forest', 'Naive Bayes', 'Perceptron',\n              'Stochastic Gradient Decent',\n              'Decision Tree', 'Voting'],\n    'Score': [acc_linear_svc, acc_knn, acc_log,\n              acc_random_forest, acc_gaussian, acc_perceptron,\n              acc_sgd, acc_decision_tree, acc_voting_clf]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df","c5a77bc1":"#%pycodestyle_off","3d09e5a0":"# the metric we are using\nf1 = make_scorer(f1_score, average='macro')","9f8a6db1":"# Apply the pipeline to the input dataframe\n# We are using train_set dataset that corresponds\n# to the original train set without being split.\n\ntrain_set_tr = pipeline.fit_transform(train_set)","ab58e368":"train_set_tr.head()","6dc35781":"X_train = train_set_tr.drop(\"Survived\", axis=1)\nY_train = train_set_tr[\"Survived\"]","20de942c":"param_grid = {'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3],  # learning rate\n              'tol': [1e-4, 1e-3, 1e-2, 1e-1],\n              'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],  # logistic regression\n              'penalty': ['l2', 'l1', 'elasticnet'],\n              'random_state' : [1]}\n\nsgd = SGDClassifier()\nsgd_clf = GridSearchCV(estimator=sgd, param_grid=param_grid, cv = 10, scoring = f1, n_jobs=-1)\nsgd_clf.fit(X_train, Y_train)\nsgd_clf.best_params_","f1b0c677":"acc_sgd2 = sgd_clf.best_score_\nprint(\"Training score: \", acc_sgd2)","fa9bfce8":"param_grid = {\"criterion\" : [\"gini\", \"entropy\"],\n              \"n_estimators\": [i for i in range(50, 250, 50)],\n              \"max_depth\": [i for i in range(5, 25, 2)],\n              \"min_samples_split\" : [12, 16, 18, 25, 35],\n              'oob_score' : [True],\n              'random_state' : [1],\n              'n_jobs' : [-1]}\n\nrf = RandomForestClassifier()\nrf_clf = GridSearchCV(estimator=rf, param_grid=param_grid, cv = 10, scoring = f1, n_jobs=-1)\nrf_clf.fit(X_train, Y_train)\nrf_clf.best_params_","667e7e58":"acc_random_forest2 = rf_clf.best_score_\nprint(\"Training score: \", acc_random_forest2)","14b65ef9":"param_grid = {'tol': [1e-4, 1e-3, 1e-2, 1e-1],\n              \"C\":[i for i in range(7, 20, 2)],\n              \"penalty\": [\"l1\",\"l2\"],  # l1 lasso l2 ridge\n              'solver' : ['lbfgs', 'liblinear'],\n              'max_iter' : [i for i in range(100, 1000, 100)],\n              'random_state' : [1]\n             }\n\nlogreg = LogisticRegression()\nlogreg_clf = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=10, scoring = f1, n_jobs=-1)\nlogreg_clf.fit(X_train, Y_train)\nlogreg_clf.best_params_","db0ec426":"acc_log2 = logreg_clf.best_score_\nprint(\"Training score: \", acc_log2)","efbee012":"param_grid = {\"weights\": [\"uniform\", \"distance\"],\n              'n_neighbors': [3, 5, 7, 9, 11, 13, 15]  # usually odd numbers\n             }\n\nknn = KNeighborsClassifier()\nknn_clf = GridSearchCV(estimator=knn, param_grid=param_grid,\n                       cv=10, scoring=f1, n_jobs=-1)\nknn_clf.fit(X_train, Y_train)\nknn_clf.best_params_","b81dbc19":"acc_knn2 = knn_clf.best_score_\nprint(\"Training score: \", acc_knn2)","6def37b8":"param_grid = {'eta0' : [1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10],\n              'max_iter' : [i for i in range(500, 2000, 500)],\n              'random_state' : [1]\n             }\n\nperceptron = Perceptron()\nperceptron_clf = GridSearchCV(estimator=perceptron, param_grid=param_grid,\n                              cv=10, scoring = f1, n_jobs=-1)\nperceptron_clf.fit(X_train, Y_train)\nperceptron_clf.best_params_","8c154c9b":"acc_perceptron2 = perceptron_clf.best_score_\nprint(\"Training score: \", acc_perceptron2)","37fdc8e3":"param_grid = {'C' : np.arange(0.01, 100, 10),\n              'loss' : ['hinge', 'squared_hinge'],\n              'tol' : [1e-4, 1e-3, 1e-2, 1e-1],\n              'random_state' : [1]\n             }\n\nlinear_svc = LinearSVC()\nlinear_svc_clf = GridSearchCV(estimator=linear_svc, param_grid=param_grid,\n                              cv=10, scoring=f1, n_jobs=-1)\nlinear_svc_clf.fit(X_train, Y_train)\nlinear_svc_clf.best_params_","5f4c4c6c":"acc_linear_svc2 = linear_svc_clf.best_score_\nprint(\"Training score: \", acc_linear_svc2)","7af47462":"param_grid = {'criterion' : ['gini', 'entropy'],\n              'max_depth' : [i for i in range(7, 20, 2)],\n              'max_leaf_nodes': list(range(2, 100)),\n              'min_samples_split': [2, 3, 4]}\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree_clf = GridSearchCV(estimator=decision_tree, param_grid=param_grid,\n                                 cv=10, scoring=f1, n_jobs=-1)\ndecision_tree_clf.fit(X_train, Y_train)\ndecision_tree_clf.best_params_","c6f32f3e":"acc_decision_tree2 = decision_tree_clf.best_score_\nprint(\"Training score: \", acc_decision_tree2)","951334eb":"sgd = SGDClassifier(alpha=0.001, loss='log',\n                    penalty='l2', tol=0.0001,\n                    random_state=1)\nrf = RandomForestClassifier(n_estimators=50, criterion='entropy',\n                            min_samples_split=16, max_depth=15,\n                            oob_score=True, random_state=1,\n                            n_jobs=-1)\nlogreg = LogisticRegression(C=15.0, penalty='l2',\n                            max_iter=100, solver='liblinear',\n                            tol=0.01, random_state=1)\nknn = KNeighborsClassifier(n_neighbors=7, weights='uniform')\nperceptron = Perceptron(eta0=1.0,\n                        max_iter=500,\n                        random_state=1)\nlinear_svc = LinearSVC(C=50.01,\n                       loss='squared_hinge',\n                       random_state=1,\n                       tol=0.0001)\ndecision_tree = DecisionTreeClassifier(criterion='entropy',\n                                       max_depth=11,\n                                       max_leaf_nodes=10,\n                                       min_samples_split=2)\n\n\nvoting_clf = VotingClassifier(estimators=[('SGD', sgd),\n                                          ('RF', random_forest),\n                                          ('LR', logreg),\n                                          ('KNN', knn),\n                                          ('NaiveBayes', gaussian),\n                                          ('Perceptron', perceptron),\n                                          ('SVC', linear_svc),\n                                          ('DT', decision_tree)],\n                              voting='hard')\n\nscores = cross_val_score(voting_clf, X_train, Y_train,\n                         cv=10, scoring='f1_macro')\n\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\nacc_voting_clf2 = scores.mean()","c689a996":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression',\n              'Random Forest', 'Perceptron',\n              'Stochastic Gradient Decent',\n              'Decision Tree', 'Voting'],\n    'Score': [acc_linear_svc2, acc_knn2, acc_log2,\n              acc_random_forest2, acc_perceptron2,\n              acc_sgd2, acc_decision_tree2, acc_voting_clf2]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df","288dc3f6":"pipeline.fit(train_set)\nX_test = pipeline.transform(test_set)\nX_test.head()","4bf4d96c":"voting_clf.fit(X_train, Y_train)\npredictions = voting_clf.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_PassengerID,\n                       'Survived': predictions})\noutput.head()","2220c349":"output.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","38380490":"We can see that SVM perform better than the voting model (83.47% VS 79.85%). And the different scores are somehow high. We can be happy and stop here. But we can also criticize our method of splitting the training dataset into train and validation sets: \n<ul>\n    <li>We do not have enough data in the trainind dataset to do that.<\/li>\n    <li>What if the spliting haven't captured the structure of data. We could end-up with an easy validation set to predict.<\/li>\n<\/ul>\n\nTo try to resolve these issues, we are going to implement the K-fold cross validation and also we are going to tuning some parameters in the models to make them perform better.","ad51938b":"**Which features are numerical?**\n\nThese values change from sample to sample. Within numerical features are the values `discrete`, `continuous`, or `timeseries based`? Among other things this helps us select the appropriate plots for visualization.\n\n<ul>\n    <li>Continous<\/li>\n        <ul>\n            <li>Age<\/li>\n            <li>Fare<\/li>\n        <\/ul>\n    <li>Discrete<\/li>\n        <ul>\n            <li>SibSp<\/li>\n            <li>Parch<\/li>\n        <\/ul>\n<\/ul>","daaad108":"As a reminder, we have to deal with Cabin, Embarked and Age.","c09c1df5":"# Titanic - Machine Learning from Disaster","1484abb6":"### Survived & Gender","95e0d6b2":"### Embarked","380133f7":"### Age & Gender","a9520fe4":"### Cabin","00abc008":"# Building Machine Learning Models","747269a3":"Let's make a copy of the two datasets","46d0cca0":"### Encoding Variables","2250e461":"**Which features may contain errors or typos?**\n\nThis is harder to review for a large dataset, however reviewing a few samples from a smaller dataset may just tell us outright, which features may require correcting.\n\n<li>Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.<\/li>","06a4759e":"### Age variable","19cd1f49":"## Logistic Regression","769bd76b":"## Drop Variables","2f25a74c":"**What is the distribution of numerical feature values across the training dataset?**\n\nThis helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n\n<li>Total samples in the training set are 668 or 51% of the actual number of passengers on board the Titanic (1,309).<\/li>\n<li>Survived is a categorical feature with 0 or 1 values.<\/li>\n<li>Around 59% of the passengers survived<\/li>\n<li>Most passengers (> 75%) did not travel with parents or children.<\/li>\n<li>Nearly 30% of the passengers had siblings and\/or spouse aboard.<\/li>\n<li>Fares varied significantly with few passengers (< 1%) paying as high as \\$512<\/li>\n<li>Few elderly passengers (< 1%) within age range 65-80. <\/li>","d9d8ab17":"### Fare variable","1ac32878":"We have improved some model by tuning there parameters. And now we see that the voting performs better than the others. We will apply the voting to predict the test set and submit for evaluation into Kaggle.","62d31485":"## Linear Support Vector Machine","aca9480d":"### Survived & Relatives ","30a65343":"## Univariate Analysis","292d09cf":"## Missing data","b43f7f6d":"# Best Model","5e39a479":"### Random Forest","6d34291a":"## Survived & Fare","444777ce":"### Pclass variable","ff942179":"# K-Fold Cross Validation & Hyper-parameter tuning","d96963ce":"The notebook walks us through the workflow for using machine learning to create a best model that predicts which passengers survived the Titanic shipwreck. This corresponds to my 8th and last project in my Engineer Machine Learning path with OpenClassroom.","d7139ba8":"# Descriptive analysis training dataset","9de6236b":"### Survived & Pclass","5488b9d2":"We are splitting the training dataset into two parts for the validation set. 75% of the data would be used for training and 25% for the validation.","e13c7ba4":"**How many Passengers do we have in total**","95b59c44":"### Family size","7e8017cd":"## Logistic Regression","2b05e94f":"## Voting ","7a9a7bce":"**What is the distribution of categorical features?**\n\n<li>Names are unique across the dataset (count=unique=668)<\/li>\n<li>Sex variable as two possible values with 64% male (top=male, freq=430\/count=668).<\/li>\n<li>Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.<\/li>\n<li>Embarked takes three possible values. S port used by most passengers (top=S)<\/li>\n<li>Ticket feature has high ratio (18.56%) of duplicate values (unique=544).<\/li>","ed340fbe":"### Parch variable ","774827af":"As we have done for Sex feature, let's do the same for Embarked.","82fdcda2":"Age and fare features are float. We will now create categories from these features.","938e480d":"### Embarked variable","21b26b7f":"## Creating Categories","94dfd3da":"## Stochastic Gradient Descent (SGD)","aaa2cf57":"## K Nearest Neighbor","433902ce":"## Voting","99e808d9":"# Importing the Libraries","fc58143a":"### SibSp variable","91fa7c54":"**Which features contain blank, null or empty values?**\n\nThese will require correcting.\n\n<li>Cabin<\/li>\n<li>Age<\/li>\n<li>Embarked<\/li>","336f2c88":"Let's use the Name feature to extract the Titles for each passenger.","004919e5":"## Survived & Age ","d54b5218":"# Data Preprocessing","5aaabd5f":"### Survived & Embarked","2675b052":"### Fare per Person","aee30a60":"### Is alone","d86de8d6":"## Bivariate Analysis","0226f7ac":"## Decision Tree","c9369547":"The training dataset has 668 entries with 12 variables. Within these variables, 7 are numerics(integer or floats) and 5 are strings(object). ","58a0bded":"## Perceptron","aa4512a6":"![](http:\/\/)Though we have already created some new features in our dataset, we are going to add four (04) other variables to the dataset by combining some existing features: FamilySize; IsAlone; Age_Class and Fare_Per_Person.","127c8860":"## Creating new Features","10494883":"### Sex variable","0d3f7645":"### Age Categorise","4cabe5cd":"### Age","524d8006":"## Decision Tree","f98361a4":"**which features are available in the dataset?**","47d8bdfb":"**Which features are alphanumeric data types?**\n\nThese are candidates for correcting goal.\n\n<li>Ticket is a mix of numeric and alphanumeric data types.<\/li> \n<li>Cabin is alphanumeric.<\/li> ","014c0a69":"## Gaussian Naive Bayes","395ba6eb":"### Name","2b0fa43e":"### Sex","24c63af0":"## Best Model","1f3d042a":"### Fare Categorise","c28859f6":"## Converting Features","5affd583":"### Embarked","76da8105":"### PClass & Fare","9728ab7f":"### Survived variable","d619a4d8":"* **Which features are categorical?**\n\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n\n<ul>\n    <li>Categorical<\/li>\n        <ul>\n            <li>Survived<\/li>\n            <li>Sex<\/li>\n            <li>Embarked<\/li>\n        <\/ul>\n    <li>ordinal<\/li>\n        <ul>\n            <li>Pclass<\/li>\n        <\/ul>\n<\/ul>","6b1862b5":"## Random Forest","89813534":"Let's drop useless features for our training models.","3fcce412":"We are encoding Sex feature to convert it into numeric.","a444e893":"Pclass, Embarked and Title features have to be encoded and not be treated as numeric. ","3cf31c58":"## Stochastic Gradient Descent (SGD)","46e1b535":"Instead of deleting the `Cabin` variable we will extract the letter as it refers to the deck. Therefore we\u2019re going to create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. The missing values will be converted to zero.","b5e14a23":"# Getting the Data","dd3131fd":"Now let's order the models we have chosen to implement base on their f1score.","8bef5f37":"## Linear Support Vector Machine","58cf1e75":"# Custom Transformer & Pipeline of transformation","cdfa7ead":"## Perceptron","bacd2e38":"Noting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here!](https:\/\/www.kaggle.com\/c\/titanic\/data).","530d329a":"###  Age times Class","bc6ff059":"Let's deal with the issue with the age features missing values. We will create an array that contains random numbers, which are computed based on the mean age value in regards to the standard deviation and is_null.","54391cdf":"## K Nearest Neighbor","b096fd59":"For the Embarked feature, as it is a categorical variable, we are going to fill the missing values with the most common one.","c9f6ff5d":"### Embarked, Pclass and Sex"}}