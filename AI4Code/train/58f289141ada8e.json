{"cell_type":{"89362fe5":"code","5d3f1fbe":"code","d3f5df5d":"code","5bffbb83":"code","f53b0d8a":"code","77c0541d":"code","a47d61cc":"code","7572803e":"code","c0cf03d0":"code","7094f6a5":"code","e3507903":"code","efeaaa32":"code","b9ae5353":"code","2abd8764":"code","dbc41821":"code","99c875c8":"code","097eb34e":"code","86cf9d17":"code","cd827d67":"code","057fd7b3":"code","031b31ef":"code","66aab36c":"code","0f8b7c73":"code","2ff3b747":"code","3c8d89e7":"code","f99909c9":"code","aa54ac47":"code","d706e78b":"code","697d0391":"code","6ada85de":"code","0a8ca7e9":"code","27ce9ba0":"code","7c704abe":"code","e3669984":"code","8b5a57cb":"code","e72c5abd":"code","e1c7b2cb":"code","5c2a99b7":"code","c3ecc9f0":"code","526fb578":"code","2dbaf609":"code","48366481":"code","66d17cc8":"code","e6f287df":"markdown","7134a739":"markdown","21d10d1e":"markdown","e74b4f24":"markdown","c3d9088e":"markdown","9dea26ee":"markdown","79e2ca56":"markdown","ba128a44":"markdown","bced077e":"markdown","8562a103":"markdown","74167893":"markdown","8fb13264":"markdown","191584c5":"markdown","7acffe27":"markdown","8239bcaf":"markdown"},"source":{"89362fe5":"#Importing necessary pre-processing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5d3f1fbe":"df= pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding=\"latin-1\")","d3f5df5d":"print(df.columns)","5bffbb83":"print(\"** First five rows of the dataset **\")\nprint()\n\n#Dropping unncessary columns from the dataset\ndf=df.drop([\"Unnamed: 2\" , \"Unnamed: 3\" , \"Unnamed: 4\"] , axis=1)\ndf=df.rename(columns={\"v1\": \"Target\" , \"v2\": \"Text\"})\ndf.head()","f53b0d8a":"print(\"** Value Counts of Target **\")\nprint()\nprint(df['Target'].value_counts())\nprint()\nprint(\"** Basic description of dataset **\")\nprint()\ndf.describe()\n","77c0541d":"print(\" ** Basic Information **\")\nprint()\ndf.info()","a47d61cc":"def without_hue(data,feature,ax):\n    \n    total=float(len(data))\n    bars_plot=ax.patches\n    \n    for bars in bars_plot:\n        percentage = '{:.1f}%'.format(100 * bars.get_height()\/total)\n        x = bars.get_x() + bars.get_width()\/2.0\n        y = bars.get_height()\n        ax.text(x, y,(percentage,bars.get_height()),ha='center',fontweight='bold',fontsize=14)","7572803e":"#setting theme\nsns.set_theme(context='notebook',style='white',font_scale=3)\n\n#setting the background and foreground color\nfig=plt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_facecolor(\"#F2EDD7FF\")\nfig.patch.set_color(\"#F2EDD7FF\")\n\n#Dealing with spines\nfor i in ['left','top','right']:\n    ax.spines[i].set_visible(False)\n    \nax.grid(linestyle=\"--\",axis='y',color='gray')\n\n#countplot\na=sns.countplot(data=df,x='Target',saturation=3,palette='cool')\n\nwithout_hue(df,'target',a)\n\nplt.title(\"Label Distribution\",weight='bold',fontsize=15)","c0cf03d0":"#Adding new feature 'message_length'\ndf['message_length']=df['Text'].apply(lambda x: len(x.split(\" \")))","7094f6a5":"df","e3507903":"df_ham= df[\"message_length\"][df[\"Target\"]==\"ham\"].value_counts()\ndf_spam=df[\"message_length\"][df[\"Target\"]==\"spam\"].value_counts()\ndf_ham=pd.DataFrame(df_ham)\ndf_spam=pd.DataFrame(df_spam)","efeaaa32":"df_ham","b9ae5353":"fig=plt.figure(figsize=(20,10))\nfig.patch.set_color(\"#F2EDD7FF\")\n\nax=plt.axes()\nax.set_facecolor(\"#F2EDD7FF\")\nfig.patch.set_color(\"#F2EDD7FF\")\n\n#Dealing with spines\nfor i in ['left','top','right']:\n    ax.spines[i].set_visible(False)\n    \nax.grid(linestyle=\"--\",axis='y',color='gray')\n\n\n\nsns.scatterplot(data=df_ham,x=df_ham.index,y=df_ham['message_length'],label=\"ham\")\nsns.scatterplot(data=df_spam,x=df_spam.index,y=df_spam['message_length'],label='spam')\nplt.xlabel(\"Message Length\",fontsize=15,fontweight='bold')\nplt.ylabel(\"Message Length Frequencies\",fontsize=15,fontweight='bold')","2abd8764":"import re\nimport string","dbc41821":"#Using regex functions to clean the text\n\ndef text_cleaning(text):\n    \n    #Converting text into lowercase\n    text = str(text).lower()\n    \n    #Removing square brackets from the text\n    text = re.sub('\\[.*?\\]','',text)\n    \n    \n    #Removing links starting with (https or www)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    \n    #Removing <\"text\"> type of text \n    text = re.sub('<.*?>+','',text)\n    \n    #Removing punctuations\n    text = re.sub(\"[%s]\" % re.escape(string.punctuation),'',text)\n    \n    #Removing new lines\n    text = re.sub(\"\\n\",'',text)\n    \n    #Removing alphanumeric numbers \n    text = re.sub('\\w*\\d\\w*','',text)\n    \n    return(text)\n        ","99c875c8":"#Applying 'text_cleaning' function on the dataset\ndf['cleaned_text']=df['Text'].apply(text_cleaning)\n\n\ndf.head()","097eb34e":"print(\"***** First five sentences of the cleaned and uncleaned text *****\")\nprint()\nfor i in range(0,5):\n    print(\"Uncleaned sentence ==>\",i+1 , \".\", df[\"Text\"][i])\n    print(\"Cleaned sentence ==>\",i+1,\".\", df['cleaned_text'][i])\n    print()","86cf9d17":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nwordnet = WordNetLemmatizer()\ndef remove_stopwords(text):\n    text = text.split()\n    text = [wordnet.lemmatize(word) for word in text if word not in set(stopwords.words('english'))]\n    text = \" \".join(text)\n    return(text)\n\ndf['cleaned_text']=df['cleaned_text'].apply(remove_stopwords)","cd827d67":"print(\"***** Dataset after lemmatizing and removing stop words *****\")\nprint()\ndf.head()","057fd7b3":"df=df.drop(['Text'],axis=1)","031b31ef":"df_ham_cleaned= df[df[\"Target\"]==\"ham\"]\ndf_spam_cleaned=df[df[\"Target\"]==\"spam\"]\ndf_ham_cleaned=pd.DataFrame(df_ham_cleaned)\ndf_spam_cleaned=pd.DataFrame(df_spam_cleaned)","66aab36c":"df_ham_cleaned","0f8b7c73":"para_ham = \" \".join([word for word in df_ham_cleaned['cleaned_text']])\npara_spam = \" \".join([word for word in df_spam_cleaned['cleaned_text']])","2ff3b747":"from wordcloud import WordCloud\n\nwordcloud=WordCloud(width=2000,height=1000,background_color='#F2EDD7FF').generate(para_ham)\n\nplt.figure(figsize=(20,30))\nplt.imshow(wordcloud)\nplt.title(\"Non_Spam Messages\")\nplt.show()","3c8d89e7":"from wordcloud import WordCloud\n\nwordcloud=WordCloud(width=2000,height=1000,background_color='#F2EDD7FF').generate(para_spam)\n\nplt.figure(figsize=(20,30))\nplt.imshow(wordcloud)\nplt.title(\"Spam Messages\")\nplt.show()","f99909c9":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","aa54ac47":"le=LabelEncoder()\ndf['Target']=le.fit_transform(df[\"Target\"])\ndf.head()","d706e78b":"x=df['cleaned_text']\ny=df['Target']\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=42)","697d0391":"print(\"*** Size of x_train ***\",x_train.shape)\nprint(\"*** Size of y_train ***\",y_train.shape)\nprint(\"*** Size of x_test *** \",x_test.shape)\nprint(\"*** Size of y_test *** \",y_test.shape)","6ada85de":"cv=CountVectorizer()\nvect=cv.fit(x_train)\nx_train_vector=vect.transform(x_train)\nx_test_vector=vect.transform(x_test)","0a8ca7e9":"print(\"**** Shape of training dataset after vectorization ****\" , x_train_vector.shape)\nprint(\"**** Shape of test dataset after vectorization ****\" , x_test_vector.shape)","27ce9ba0":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nimport xgboost as xgb\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score , classification_report","7c704abe":"lis=[]\ndef modelling(dic):\n    for models in dic:\n        print(\"**** Training with\", models , \"algorithm ****\")\n        dic[models].fit(x_train_vector,y_train)\n        print(\"**** Predicting with\",models , \"algorithm ****\")\n        print(\"......\")\n        pred=dic[models].predict(x_test_vector)\n        print()\n        print(\"**** Getting Accuracy of\" , models , \"algorithm ****\")\n        print(\"......\")\n        print(accuracy_score(y_test,pred))\n        lis.append(accuracy_score(y_test,pred))\n        print(\"......\")\n        print(\"**** Getting Classification report of\", models , \"algorithm ****\")\n        print()\n        print(classification_report(y_test,pred))\n        print(\"----------------------------------------------------------------\")\n        print()\n        \n        ","e3669984":"dic={\"Naive Bayes\": MultinomialNB(),\"Decision Tree\": DecisionTreeClassifier(random_state=42),\"SVM\":svm.SVC(),\n     \"Random Forest\":RandomForestClassifier(n_estimators=200,random_state=42),\"XGB\":xgb.XGBClassifier(n_estimators=80),\n      }","8b5a57cb":"modelling(dic)","e72c5abd":"models_dataframe=pd.DataFrame({\n    \"Models\":[\"Naive Bayes\" , \"Decision Tree\" , \"SVM\" , \"Random Forest\" , \"XGBoost\"] ,\n    \"Accuracy_score\":[i for i in lis]\n})","e1c7b2cb":"models_dataframe","5c2a99b7":"plt.figure(figsize=(20,10))\nsns.barplot(y=models_dataframe['Models'],x=models_dataframe['Accuracy_score'],palette='rocket')\nplt.title(\"Accuracy of models with BOW method\")\nplt.show()","c3ecc9f0":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#Usinf TF-IDF method\ntf=TfidfVectorizer()\ntf_vect=tf.fit(x_train)\nx_train_vector_tf= tf_vect.transform(x_train)\nx_test_vector_tf=tf_vect.transform(x_test)\n\nprint(\"**** Shape of training dataset after vectorization ****\" , x_train_vector.shape)\nprint(\"**** Shape of test dataset after vectorization ****\" , x_test_vector.shape)","526fb578":"lis_tf=[]\ndef modelling_tf(dic):\n    for models in dic:\n        print(\"**** Training with\", models , \"algorithm ****\")\n        dic[models].fit(x_train_vector_tf,y_train)\n        print(\"**** Predicting with\",models , \"algorithm ****\")\n        print(\"......\")\n        pred=dic[models].predict(x_test_vector_tf)\n        print()\n        print(\"**** Getting Accuracy of\" , models , \"algorithm ****\")\n        print(\"......\")\n        print(accuracy_score(y_test,pred))\n        lis_tf.append(accuracy_score(y_test,pred))\n        print(\"......\")\n        print(\"**** Getting Classification report of\", models , \"algorithm ****\")\n        print()\n        print(classification_report(y_test,pred))\n        print(\"----------------------------------------------------------------\")\n        print()\n        \ndic={\"Naive Bayes\": MultinomialNB(),\"Decision Tree\": DecisionTreeClassifier(random_state=42),\"SVM\":svm.SVC(),\n     \"Random Forest\":RandomForestClassifier(n_estimators=200,random_state=42),\"XGB\":xgb.XGBClassifier(n_estimators=80),\n      }\n\n","2dbaf609":"modelling_tf(dic)","48366481":"models_dataframe=pd.DataFrame({\n    \"Models\":[\"Naive Bayes\" , \"Decision Tree\" , \"SVM\" , \"Random Forest\" , \"XGBoost\"] ,\n    \"Accuracy_score\":[i for i in lis_tf]\n})\n\nmodels_dataframe","66d17cc8":"plt.figure(figsize=(20,10))\nsns.barplot(y=models_dataframe['Models'],x=models_dataframe['Accuracy_score'],palette='rocket_r')\nplt.title(\"Accuracy of models with TF-IDF method\")\nplt.show()","e6f287df":"<a id=3.3><\/a>\n### **Word Cloud of non-spam messages \u2601**","7134a739":"<a id=\"top\"><\/a>\n## **Table of Contents \u23e9**\n\n* [Spam Classifier](#1)\n\n* [Basic Overview of Dataset \ud83d\udcfa](#2)\n\n* [Preprocessing and EDA \ud83d\udcca\ud83d\udcb9](#3)\n  * [Data Cleaning using Regex \ud83e\uddf9](#3.1)\n  * [Removing stop words](#3.2)\n  * [Word Cloud of non-spam messages \u2601](#3.3)\n  * [Word Cloud of spam messages \u2601](#3.4)\n \n \n* [Modelling with Bag of words Method \ud83d\udcb0 ](#4)\n\n* [Modelling with TF-IDF method \u23e9](#5)\n  \n\n","21d10d1e":"<a id=3.4><\/a>\n### **Word Cloud of spam messages \u2601**","e74b4f24":"[Slide to top](#top)\n\n<a id=3.1><\/a>\n### **Data Cleaning \ud83e\uddf9**","c3d9088e":"* From figure we can conclude that spam messages are more lengthy than ham messages","9dea26ee":"[Slide to top](#top)\n\n<a id=3.2><\/a>\n### **Removing Stopwords**","79e2ca56":"[Slide to top](#top)\n<a id=\"2\"><\/a>\n## **Basic Overview of dataset \ud83d\udcfa**","ba128a44":"**THANK YOU BEING PATIENT AND SCROLL THIS DOWN INTO THIS NOTEBOOK**\n\n**If you like my work please give it a upvote and any feedback is appreciated**\n\n**Very new to NLP and doing my hands dirty with basics will come up another notebook which will contain word embedding and deep learning implementation of \"Spam Classifier\" , STAY TUNED \ud83d\ude09**\n\n**Made with LOVE\u2764**","bced077e":"[Slide to Top](#top)\n\n<a id=3><\/a>\n## **Preprocessing and EDA \ud83d\udcca\ud83d\udcb9**","8562a103":"[Slide to top](#top)\n<a id=\"1\"><\/a>\n## **Spam Classifier \ud83c\udfdb**","74167893":"* Data is imbalanced\n\n* 86.6 % are \"ham\" messages and remaining 13.4 % are \"spam\" messages","8fb13264":"[Slide to Top](#top)\n<a id=5><\/a>\n## **Modelling with TF-IDF Method \u23e9**","191584c5":"![spam or ham](https:\/\/analyticsindiamag.com\/wp-content\/uploads\/2020\/10\/spamimage.jpg)","7acffe27":"[Silde to Top](#top)\n<a id=4><\/a>\n## **Modelling with Bag of Words Method \ud83d\udcb0**","8239bcaf":"**Using machine learning algorithms**\n* Naive Bayes\n* Decision Tree Classifier\n* SVM\n* RandomForest CLassifier\n* XGBoost"}}