{"cell_type":{"b96dc454":"code","b59584c7":"code","0254ad4a":"code","4518542f":"code","36c6b79f":"code","694c84f8":"code","a8f80da3":"code","eca753ce":"code","f4af0f9e":"code","ec853ab9":"code","4bacddd6":"code","9b322711":"code","414c8304":"code","72e2391d":"code","d162b291":"code","6d07f5c1":"code","6805c78a":"markdown","4ab05ba8":"markdown"},"source":{"b96dc454":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b59584c7":"##### This is for implementing U-Net model\n\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, ZeroPadding2D\nfrom keras.layers import InputLayer, concatenate, BatchNormalization, Dropout, Activation, Dense, Flatten\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","0254ad4a":"##### model (obtained from https:\/\/github.com\/zhixuhao\/unet)\n\ndef model(image_size, kernel_size = 8, optimizer = 'adam'):\n\n    n_classes=2 # This is the number of output channel. Since it's AB channel for us, the number equals to 2\n    n_channels=1 # This si the numper of input channel, which is just 1 (L channel)\n    n_filters_start=kernel_size # This is the number of filters for convolutional layer to start\n    growth_factor=2 # This is the factor number indicating how much deeper convolutional layers are going to be.\n    upconv=False # Whether to use Conv2dTranspose or UpSampling2D layer for up-convolution process. \n\n    droprate=0.25\n    n_filters = n_filters_start\n    inputs = Input((image_size[0], image_size[1], n_channels))\n    input2 = inputs\n    # input2 = BatchNormalization()(input2)\n    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(input2)\n    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    # pool1 = Dropout(droprate)(pool1)\n\n    n_filters *= growth_factor\n    # pool1 = BatchNormalization()(pool1)\n    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    # pool2 = Dropout(droprate)(pool2)\n\n    n_filters *= growth_factor\n    # pool2 = BatchNormalization()(pool2)\n    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    # pool3 = Dropout(droprate)(pool3)\n\n    n_filters *= growth_factor\n    # pool3 = BatchNormalization()(pool3)\n    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool3)\n    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_0)\n    pool4_1 = MaxPooling2D(pool_size=(2, 2))(conv4_0)\n    # pool4_1 = Dropout(droprate)(pool4_1)\n\n    n_filters *= growth_factor\n    # pool4_1 = BatchNormalization()(pool4_1)\n    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_1)\n    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_1)\n    pool4_2 = MaxPooling2D(pool_size=(2, 2))(conv4_1)\n    # pool4_2 = Dropout(droprate)(pool4_2)\n\n    n_filters *= growth_factor\n    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_2)\n    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv5)\n\n    n_filters \/\/= growth_factor\n    if upconv: # conv4_1 layer is covolutioned layer from pool4_1 layer\n        up6_1 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv5), conv4_1])\n    else:\n        up6_1 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4_1])\n    # up6_1 = BatchNormalization()(up6_1)\n    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_1)\n    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_1)\n    # conv6_1 = Dropout(droprate)(conv6_1)\n\n    n_filters \/\/= growth_factor\n    if upconv:\n        up6_2 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_1), conv4_0])\n    else:\n        up6_2 = concatenate([UpSampling2D(size=(2, 2))(conv6_1), conv4_0])\n    # up6_2 = BatchNormalization()(up6_2)\n    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_2)\n    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_2)\n    # conv6_2 = Dropout(droprate)(conv6_2)\n\n    n_filters \/\/= growth_factor\n    if upconv:\n        up7 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_2), conv3])\n    else:\n        up7 = concatenate([UpSampling2D(size=(2, 2))(conv6_2), conv3])\n    # up7 = BatchNormalization()(up7)\n    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up7)\n    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv7)\n    # conv7 = Dropout(droprate)(conv7)\n\n    n_filters \/\/= growth_factor\n    if upconv:\n        up8 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv7), conv2])\n    else:\n        up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2])\n    # up8 = BatchNormalization()(up8)\n    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up8)\n    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv8)\n    # conv8 = Dropout(droprate)(conv8)\n\n    n_filters \/\/= growth_factor\n    if upconv:\n        up9 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv8), conv1])\n    else:\n        up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1])\n    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up9)\n    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv9)\n\n    conv10 = Conv2D(n_classes, (1, 1), activation='tanh')(conv9)\n\n    model = Model(inputs=inputs, outputs=conv10)\n\n    model.summary()\n\n    model.compile(optimizer=optimizer, loss='mse')\n\n    return model","4518542f":"##### This is for image preprocessing. For more information about RGB to LAB image, you can visit my Github readme (https:\/\/github.com\/dabsdamoon\/Anime-Colorization)\n\nimport os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom skimage.color import rgb2lab, lab2rgb, rgb2gray, xyz2lab\n","36c6b79f":"##### function transforming from RGB to LAB image\n\ndef rgb_lab_transformation(image_resized, image_size):\n    \n    # create 3 dimensional gray image with rgb2gray * 3\n    image_gray = rgb2gray(image_resized).reshape(image_size[0], image_size[1], 1) # this will be the validation dataset\n    image_gray3 = np.concatenate([image_gray]*3, axis = 2)\n    image_lab3 = rgb2lab(image_gray3)\n    image_feature = image_lab3[:,:,0]\/128 # scaling from -1 to 1 based on LAB space value range\n\n    # create label first with rgb2lab image\n    image_lab = rgb2lab(image_resized)\n    image_label = image_lab[:,:,1:]\/128 # scaling from -1 to 1 based on LAB space value range\n    \n    return image_feature, image_label # return feature would be 1st column of lab image generated by gray3 image\n\n##### Function creating individual feature and label\n\ndef feature_label_generation(character_dir, filename_list, image_size):\n    \n    feature = []\n    label = []\n    \n    for filename_num in range(len(filename_list)):\n    \n        image_chosen = cv2.resize(plt.imread(character_dir + filename_list[filename_num]), image_size)\n\n        if image_chosen.shape[2] == 3:\n\n            feature_indiv, label_indiv = rgb_lab_transformation(image_chosen, image_size)\n\n            feature.append(feature_indiv)\n            label.append(label_indiv)\n\n    return feature, label\n","694c84f8":"##### creating dictionaries\n\ndir_name = '..\/input\/moeimouto-faces\/moeimouto-faces\/'\n\nsize_row, size_col = 128, 128\n\nfolder_name = os.listdir(dir_name)\n\ncharacter_name = [i[4:] for i in folder_name]\ncharacter_name.sort()\n\nfolder_dict = {}\n\nfor i in range(len(folder_name)):\n    folder_dict[folder_name[i][4:]] = folder_name[i]\n\nsingle_character= 'melon-chan'\n\nfile_list = os.listdir(dir_name + folder_dict[single_character])\nimage_list = [i for i in file_list if i[-3:] == 'png']\n# image_dict[folder_name[i][4:]] = image_list","a8f80da3":"##### pull out 1 image for observing the colorization process\nimage_pop = image_list.pop(76)","eca753ce":"##### Create feature and label\n\nfeature = []\nlabel = []\nimage_size = (size_row, size_col)\n\nfor filename in image_list:\n\n    image_chosen = cv2.resize(plt.imread(dir_name + '\/' + folder_dict[single_character] + '\/' + filename), image_size)\n\n    if image_chosen.shape[2] == 3:\n\n        feature_indiv, label_indiv = rgb_lab_transformation(image_chosen, image_size)\n\n        feature.append(feature_indiv)\n        label.append(label_indiv)\n        \nx_feature = np.array(feature).reshape(len(feature), size_row, size_col, 1)\nx_label = np.array(label).reshape(len(label), size_row, size_col, 2)","f4af0f9e":"##### Define model\n\nK.clear_session()\n\ninput_shape = (size_row,size_col,1)\nkernel_size = 4\n\nmodel = model(image_size = input_shape[:2], kernel_size = kernel_size, optimizer = Adam(0.0002, 0.5))","ec853ab9":"##### Make the image binary (grayscale)\n\nimage_valid = cv2.resize(plt.imread(dir_name + '\/' + folder_dict[single_character] + '\/' + image_pop), image_size)\n\nplt.imshow(image_valid)\nplt.axis('off')\nplt.show()\n\nimg_gray = rgb2gray(image_valid).reshape(128, 128, 1)\nimg_gray3 = np.concatenate([img_gray]*3, axis = 2) # concatenating three gray images so that it can have 3 channels\nimg_lab3 = rgb2lab(img_gray3) # convert concatenated image to lab\n\nplt.imshow(img_gray.reshape(128, 128), cmap = 'gray')\nplt.axis('off')\nplt.show()","4bacddd6":"##### Fit the model\n\nmodel_loss = []\nimg_colorized = []\n\nepochs = 100\nsave_interval = int(epochs\/20) # later, going to visualize training process with 20 images  \n\nfor i in range(epochs):\n\n    model_hist = model.fit(x=x_feature, \n                           y=x_label, \n                           batch_size= 1, \n                           epochs=1,\n                           verbose = 0)\n    \n    model_loss.append(model_hist.history['loss'])\n    \n    if i % save_interval == 0:\n        \n        print(i, model_hist.history['loss'])\n        \n        valid_input = img_lab3[:,:,0].reshape(1, size_row, size_col, 1)\/128\n        \n        pred = model.predict(valid_input)    \n        pred = pred.reshape(size_row, size_col, 2)\n        \n        cur_pred = np.zeros((size_row, size_col, 3))             \n        # Output colorizations\n        cur_pred[:,:,0] = valid_input.reshape(size_row, size_col) * 128 # lab class\n        cur_pred[:,:,1:] = pred*128 # lab predicted\n        \n        img_colorized.append(lab2rgb(cur_pred))        ","9b322711":"##### Plot loss function\n\nplt.plot(model_loss)\nplt.ylabel('loss')\nplt.xlabel('epoch')\n\nplt.show()","414c8304":"##### Visualization of input value\n\ninput_val = img_gray.copy()\nplt.imshow(input_val.reshape(size_row, size_col), cmap = 'gray')\nplt.axis('off')\nplt.show()","72e2391d":"##### Visualizing the learning process\n\ngrid_row = 4\ngrid_col = 5\n\ng_row = []\ni = 0\n\nfor r in range(grid_row):\n    \n    g_column = []\n    \n    for c in range(grid_col):\n        g_column.append(img_colorized[i])\n        i += 1\n        \n    g_column = np.concatenate(g_column, axis = 1)\n    g_row.append(g_column)\n\ng_row = np.concatenate(g_row)\n\nplt.figure(figsize = (12,12))\nplt.axis('off')\nplt.imshow(g_row)","d162b291":"##### Fun exercise: use another character for validation input!\n\nimg_another = cv2.resize(plt.imread(dir_name + folder_dict['horo'] + '\/' +  os.listdir(dir_name + folder_dict['horo'])[5]), image_size)\n\nimg_gray = rgb2gray(img_another).reshape(128, 128, 1)\nimg_gray3 = np.concatenate([img_gray]*3, axis = 2) # concatenating three gray images so that it can have 3 channels\nimg_lab3 = rgb2lab(img_gray3) # convert concatenated image to lab\n\nvalid_input = img_lab3[:,:,0].reshape(1, size_row, size_col, 1)\/128\n\npred = model.predict(valid_input)    \npred = pred.reshape(size_row, size_col, 2)\n\nimage_pred = np.zeros((size_row, size_col, 3))             \n# Output colorizations\nimage_pred[:,:,0] = valid_input.reshape(size_row, size_col) * 128 # lab class\nimage_pred[:,:,1:] = pred*128 # lab predicted\n","6d07f5c1":"plt.imshow(img_another)\nplt.axis('off')\nplt.show()\n\nplt.imshow(img_gray.reshape(128, 128), cmap = 'gray')\nplt.axis('off')\nplt.show()\n\nplt.imshow(lab2rgb(image_pred))\nplt.axis('off')\nplt.show()","6805c78a":"#### Below codes are my own practice of colorizing black-white binary image. Please note that I've looked up many codes, so the codes may look similar to other professionals' codes. If this causes problem, please leave comments and I'll delete the kernel. Thank you!","4ab05ba8":"### For more information, visit following links:\n\n<p> General colorization ideas and codes: https:\/\/blog.floydhub.com\/colorizing-b-w-photos-with-neural-networks\/ <\/p>\n<p> U-Net paper: https:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net\/ <\/p>\n<p> U-Net Keras implementation:  https:\/\/github.com\/zhixuhao\/unet <\/p>\n<p> More detailed version: https:\/\/github.com\/dabsdamoon\/Anime-Colorization  (This is my own Github) <\/p>\n\nThank you for reading and any comments\/suggests are welcome."}}