{"cell_type":{"2c691455":"code","f2436b3c":"code","0a7346e6":"code","d5350e8a":"code","c979014d":"code","d5cb4e5f":"code","8727875c":"code","eff7d3f3":"code","15980db6":"code","c72c520d":"code","f9993426":"code","0bc6a1f3":"code","943c7a4c":"code","7fd83e67":"code","4e13fed4":"code","e9a0c3cd":"code","32c8ce61":"code","9d4143e0":"markdown","85770045":"markdown","4f5ddfb8":"markdown","a45bd09e":"markdown","1408ae53":"markdown"},"source":{"2c691455":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2436b3c":"import cv2 \nfrom keras.models import Sequential #model\nfrom keras.layers import MaxPool2D, Dense, Flatten, Conv2D, Dropout #layer\nfrom keras.optimizers import Adam, SGD  #optimizer\nfrom keras.utils import to_categorical #utilities\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau #callback - it is a set of functions applied at the given stage of the training procedure. \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","0a7346e6":"df = pd.read_csv('\/kaggle\/input\/az-handwritten-alphabets-in-csv-format\/A_Z Handwritten Data.csv').astype('float32')\ndf.head() #veiwing top 5 rows","d5350e8a":"x = df.drop('0', axis =1) #features\ny = df['0'] #labels","c979014d":"#train test split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n\n#reshaping data\nx_train = np.reshape(x_train.values , (x_train.shape[0], 28, 28))\nx_test = np.reshape(x_test.values, (x_test.shape[0], 28, 28)) \n\n#the dataset we have it contains 784 columns pixel data which we converted into 28 X 28 pixel data ","d5cb4e5f":"print('train data shape : ', x_train.shape)\nprint('test data shape : ', x_test.shape)","8727875c":"#creating a word dict\nword_dict = {0 : 'A', 1 : 'B', 2 : 'C', 3 : 'D',4: 'E',5:'F',6: 'G',7: 'H',8: 'I',9: 'J',\n             10 : 'K',11: 'L',12: 'M',13: 'N',14: 'O',15: 'P',16: 'Q',\n             17: 'R',18 : 'S',19: 'T',20: 'U',21: 'V',22: 'W',23: 'X',24:'Y' ,25:'Z'}\nfor i in word_dict:\n    print(i, word_dict[i]) #values ","eff7d3f3":"#converting labels into integer\ny = np.int0(y)\ncount = np.zeros(26, dtype = 'int')\n\n\n#appending count to y\nfor i in y:\n    count[i] = count[i] + 1\n    \n    \ncount #this count list is the number of images present in the dataset to each alphabet ","15980db6":"alphabets = []\nfor i in word_dict.values():\n    alphabets.append(i)\nalphabets #it contains all the values of the dictionary","c72c520d":"#plotting alphabets\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize= (11, 11))\nsns.barplot(y = alphabets, x = count, palette = 'cool_r')\nplt.xlabel('Count')\nplt.ylabel('Alphabets')\nplt.show()","f9993426":"#shuffling\nshuffle_ = shuffle(x_train[:100]) #this shuffling is for random images\n\nfig, ax = plt.subplots(3, 3, figsize = (11,11)) #creating 9 plots in 3 X 3 shape and displaying threshold images of 9 alphabets\naxes = ax.flatten()\n\n\nfor i in range(9): #looping\n    _, shu = cv2.threshold(shuffle_[i], 30, 200, cv2.THRESH_BINARY) #setting threshold\n    axes[i].imshow(np.reshape((shuffle_[i]),(28, 28)), cmap = 'Greys') #image display\nplt.show()","0bc6a1f3":"#reshaping data for train and test for model input\nx_train  = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n#x train reshapping\nx_test  = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n\n#reshapping labels\ny_train = to_categorical(y_train, num_classes = 26, dtype = 'int') #converted float into categorical\ny_test = to_categorical(y_test, num_classes = 26, dtype = 'int') #converted float into categorical\n\n\nprint('new shape of x train: ',x_train.shape)\nprint(' new shape of x test shape: ', x_test.shape)\nprint('new shape for y train : ', y_train.shape)\nprint('new shape for y test: ', y_test.shape)","943c7a4c":"#MODEL CREATION\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', input_shape = (28, 28, 1))) #convolutional\nmodel.add(MaxPool2D(pool_size = (2, 2), strides = 2)) #pooling\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(MaxPool2D(pool_size = (2, 2), strides = 2))\n\nmodel.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(MaxPool2D(pool_size = (2, 2), strides = 2))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(64, activation = 'relu')) #dense layer\nmodel.add(Dense(128, activation = 'relu'))\n\nmodel.add(Dense(26, activation = 'softmax'))","7fd83e67":"model.compile(optimizer = Adam(learning_rate = 0.01), loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmodel_fit = model.fit(x_train, y_train, epochs = 1, validation_data = (x_test, y_test))\nmodel.summary()","4e13fed4":"#saving the model\nmodel.save(r'model_hand.h5')","e9a0c3cd":"model_fit.history #this displays train and validation accuracies and losses.","32c8ce61":"#prediction on test\nfig, axes = plt.subplots(3, 3, figsize = (10, 10))\naxes = axes.flatten()\n\nfor i, ax in enumerate(axes):\n    image = np.reshape(x_test[i], (28, 28))\n    ax.imshow(image, cmap = 'Greys')\n    \n    pred = word_dict[np.argmax(y_test[i])]\n    ax.set_title('Prediciton : ' +pred)\n    \n    ","9d4143e0":"- Here we can witness that the predictions are pretty accurate. \n- successfully created text reognition using Python, Tensorflow and machine learning libraries.","85770045":"THIS POTRAYS THE GREYSCALE IMAGES WE OBTAINED FROM THIS DATASET.","4f5ddfb8":"**SPLITTING  DATA**","a45bd09e":"**IMPORTING DATASET**","1408ae53":"IMPORT LIBRARIES"}}