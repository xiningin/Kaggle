{"cell_type":{"85928301":"code","6fe256a1":"code","5be54fbc":"code","86716c09":"code","a57e8840":"code","03b0b62a":"code","0fc18980":"code","22d9dc28":"code","77a8bb24":"code","f72a6e9c":"code","3df97741":"code","308c6974":"code","b52e7e76":"code","5f9df645":"code","36e833b2":"code","d839f1cf":"code","f7e6b472":"code","42d5970b":"code","e8e7cd70":"code","e2552e05":"code","570decbc":"code","afefc7aa":"code","94922670":"code","9be38f57":"code","7a0fb0ab":"code","ff033c3d":"code","3e8662e2":"markdown","0931a459":"markdown","9d97fed5":"markdown","814d9ba2":"markdown","da2df079":"markdown","cd7ab27f":"markdown","82ea4934":"markdown","de97f562":"markdown","851d8174":"markdown","2b8e738f":"markdown","228cc3ab":"markdown","f42b2e14":"markdown","c1ca2810":"markdown","14e122ae":"markdown","d1845182":"markdown","deec713d":"markdown","0758a0cf":"markdown","f85e0407":"markdown","b7d2db64":"markdown","c3cc4c13":"markdown","c53ebbf0":"markdown","b2679190":"markdown"},"source":{"85928301":"import tensorflow as tf \nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt \ntf.__version__","6fe256a1":"Name0=os.listdir('..\/input\/manga-facial-expressions')\nName=sorted(Name0)\nn=len(Name)\nN=list(range(n))\nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) ","5be54fbc":"img_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n                            #rotation_range=90,\n                            brightness_range=(0.5,1), \n                            #shear_range=0.2, \n                            #zoom_range=0.2,\n                            channel_shift_range=0.2,\n                            horizontal_flip=False,\n                            vertical_flip=False,\n                            rescale=1.\/255,\n                            validation_split=0.3)","86716c09":"root_dir = '..\/input\/manga-facial-expressions'\n\nimg_generator_flow_train = img_generator.flow_from_directory(\n    directory=root_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    shuffle=True,\n    subset=\"training\")\n\nimg_generator_flow_valid = img_generator.flow_from_directory(\n    directory=root_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    shuffle=True,\n    subset=\"validation\")","a57e8840":"imgs, labels = next(iter(img_generator_flow_train))\nfor img, label in zip(imgs, labels):\n    value=np.argmax(label)\n    plt.imshow(img)\n    plt.title(reverse_mapping[value])\n    plt.show()","03b0b62a":"base_model = tf.keras.applications.InceptionV3(input_shape=(224,224,3),\n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )","0fc18980":"base_model.trainable = False","22d9dc28":"model = tf.keras.Sequential([\n    base_model,\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(7, activation=\"softmax\")\n])","77a8bb24":"model.summary()","f72a6e9c":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001),\n              loss = tf.keras.losses.CategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.CategoricalAccuracy()])","3df97741":"model.fit(img_generator_flow_train, \n          validation_data=img_generator_flow_valid, \n          steps_per_epoch=10, epochs=100) ","308c6974":"# Visualise train \/ Valid Accuracy\nplt.plot(model.history.history[\"categorical_accuracy\"], c=\"r\", label=\"train_accuracy\")\nplt.plot(model.history.history[\"val_categorical_accuracy\"], c=\"b\", label=\"test_accuracy\")\nplt.legend(loc=\"upper left\")\nplt.show()","b52e7e76":"# Visualise train \/ Valid Loss\nplt.plot(model.history.history[\"loss\"], c=\"r\", label=\"train_loss\")\nplt.plot(model.history.history[\"val_loss\"], c=\"b\", label=\"test_loss\")\nplt.legend(loc=\"upper left\")\nplt.show()","5f9df645":"imgs, labels = next(iter(img_generator_flow_valid))","36e833b2":"for layer in model.layers:\n    print(layer.name)","d839f1cf":"base_model = model.layers[0]","f7e6b472":"last_conv_layer_name = \"mixed10\"\nclassifier_layer_names = [layer.name for layer in model.layers][1:]","42d5970b":"# We start by setting up the dependencies we will use\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Display\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm","e8e7cd70":"# The Grad-CAM algorithm\ndef get_img_array(img_path, size):\n    # `img` is a PIL image of size 299x299\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 Numpy array of shape (299, 299, 3)\n    array = keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 299, 299, 3)\n    array = np.expand_dims(array, axis=0)\n    return array\n\n\ndef make_gradcam_heatmap(\n    img_array, base_model, model, last_conv_layer_name, classifier_layer_names):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer\n    last_conv_layer = base_model.get_layer(last_conv_layer_name)\n    last_conv_layer_model = keras.Model(base_model.inputs, last_conv_layer.output)\n\n    # Second, we create a model that maps the activations of the last conv\n    # layer to the final class predictions\n    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for layer_name in classifier_layer_names:\n        x = model.get_layer(layer_name)(x)\n    classifier_model = keras.Model(classifier_input, x)\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(img_array)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) \/ np.max(heatmap)\n    return heatmap","e2552e05":"# Print what the top predicted class is\npreds = model.predict(imgs)\npred_labels = tf.argmax(preds, axis = -1)\n\nprint(\"Prediction output:\", preds)\nprint(\"Predicted label:\", pred_labels)","570decbc":"# Generate class activation heatmap\nheatmaps = []\n\nfor img in imgs:\n    heatmap = make_gradcam_heatmap(\n    tf.expand_dims(img,axis=0),\n        base_model, model, \n        last_conv_layer_name, \n        classifier_layer_names\n  )\n    heatmaps.append(heatmap)\n\n\n# Display heatmap\nplt.matshow(heatmaps[0])\nplt.show()\n","afefc7aa":"from pathlib import Path\n\nfor img, pred_label, true_label, heatmap in zip(imgs, pred_labels, labels, heatmaps): \n    # We rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # We use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # We use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # We create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * 0.003 + img\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    save_path = \"saved_img.jpg\"\n    superimposed_img.save(save_path)\n\n    # Display Grad CAM\n    pred_file_path = np.argmax(img_generator_flow_valid.labels == pred_label)\n    pred_label_name = Path(img_generator_flow_valid.filepaths[pred_file_path]).parent.name\n\n    true_file_path = np.argmax(img_generator_flow_valid.labels == tf.argmax(true_label))\n    true_label_name = Path(img_generator_flow_valid.filepaths[true_file_path]).parent.name\n\n    print(\"Predicted label:\",pred_label_name)\n    print(\"True label:\", true_label_name)\n\n    display(Image(save_path))","94922670":"LABEL=[]\nfor item in labels:\n    LABEL+=[np.argmax(item)]\nprint(LABEL)","9be38f57":"PRED=pred_labels.numpy().tolist()\nprint(PRED)","7a0fb0ab":"from sklearn.metrics import classification_report\nprint(classification_report(LABEL, PRED))","ff033c3d":"print(reverse_mapping)","3e8662e2":"## Predicted label and heatmap","0931a459":"## What is Transfer Learning ?\nhttps:\/\/www.aismartz.com\/blog\/an-introduction-to-transfer-learning\/<br\/>\nTransfer learning is a method wherein a model developed for a particular task is used as a starting point for another task. By model here, we mean a neural network that is trained with data and knowledge gained while solving one problem. For example, the knowledge gained in learning to recognize crocodiles can be used to recognize alligators because they have a lot of features in common.","9d97fed5":"## Visualize a batch of images","814d9ba2":"# Preprocessing with ImageDataGenerator","da2df079":"for layer in base_model.layers:\n    print(layer.name)","cd7ab27f":"tf.keras.utils.plot_model(base_model, show_shapes=True, show_layer_names=True)","82ea4934":"## make_gradcam_heatmap","de97f562":"# Transfer Learning for Manga Facial Expressions\nThis notebook referred to the following notebook.<br\/>\nhttps:\/\/www.kaggle.com\/pierrelouisdanieau\/transfer-learning-on-images-with-tensorflow","851d8174":"## Import a pretrained model\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/InceptionV3","2b8e738f":"## Train the model","228cc3ab":"## Prepare img_generator_flow","f42b2e14":"## Visualize accuracy and loss","c1ca2810":"## Prepare ImageDataGenerator\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator","14e122ae":"## classification_report","d1845182":"## Create imgs and labels","deec713d":"## Compile model","0758a0cf":"## Predict","f85e0407":"## Create heatmap","b7d2db64":"## Create model","c3cc4c13":"# Interpretation with Grad Cam\n","c53ebbf0":"# Transfer Learning ","b2679190":"## Set the weights of the imported model"}}