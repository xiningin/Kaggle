{"cell_type":{"6b204a69":"code","eb53407f":"code","83caf977":"code","c5e3a527":"code","f2138668":"code","f9eabf7a":"code","49e2dc77":"code","21734cd1":"code","141e0b98":"code","8981b294":"code","6faa7301":"code","4d3bf041":"code","4d570219":"code","19689cf4":"code","a723f986":"code","e382257d":"code","d1f80052":"code","fe812f7e":"code","c1ad613c":"code","1b7dc34d":"code","11367424":"code","bce5158d":"code","98e5895c":"code","cbad1c99":"code","5d18155b":"code","a7dd2a5e":"code","683d1c3a":"code","08b4d30d":"code","bd27d599":"code","cffe6a57":"markdown","2b834dc0":"markdown","dbc5561d":"markdown","8b332e63":"markdown","623b3156":"markdown","a76189a7":"markdown","2dea99ad":"markdown","ed2329d9":"markdown","3994559c":"markdown","0fa40b52":"markdown"},"source":{"6b204a69":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","eb53407f":"train_path = \"\/kaggle\/input\/titanic\/train.csv\"\ntrain_df = pd.read_csv(train_path)\n\ntest_path = \"\/kaggle\/input\/titanic\/test.csv\"\ntest_df = pd.read_csv(test_path)\ntest_passenger_id = test_df.PassengerId.values\n\ntrain_df.head()","83caf977":"train_df.describe()","c5e3a527":"print(train_df.dtypes)","f2138668":"print(train_df.shape)","f9eabf7a":"for col in train_df.columns:\n    print(col, \" No. of categories: \", len(train_df[col].value_counts()))","49e2dc77":"train_df.isna().sum()","21734cd1":"FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()\nplt.show()","141e0b98":"sns.barplot(x='Pclass', y='Survived', data=train_df)\nplt.ylim(0,1)\nplt.show()","8981b294":"# analyze influence of gender\nsurvived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\nax.set_title('Male')\nplt.show()","6faa7301":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nscaler = StandardScaler()","4d3bf041":"datasets = [train_df, test_df]\nlabels = train_df[['Survived']]\ntrain_df.drop('Survived', axis=1, inplace=True)","4d570219":"for i in range(len(datasets)):\n    data = datasets[i]\n\n    # select columns \n    data = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n    \n    # encode categorical variables \n    Sex = pd.get_dummies(data.Sex)\n    Pclass = pd.get_dummies(data.Pclass, prefix=\"class\")\n    Embarked = pd.get_dummies(data.Embarked)\n\n    data = data.merge(Sex, how=\"left\", left_index=True, right_index=True).merge(Pclass, how=\"left\", left_index=True, right_index=True).merge(Embarked, how=\"left\", left_index=True, right_index=True)\n    data.drop(['Sex', 'Pclass', 'Embarked'], axis=1, inplace=True)\n        \n    # replace NAN values \n    data['Age'] = data['Age'].fillna(data['Age'].mean())\n    data['Fare'] = data['Fare'].fillna(data['Fare'].mean())\n    \n    # datatypes \n    data = data.astype({'Fare': 'int32'}) \n    data = data.astype({'Age': 'int32'}) \n    \n    feature_cols = [\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"female\",\"male\",\"class_1\",\"class_2\",\"class_3\",\"C\",\"Q\",\"S\"]\n    \n    # assign preprocessed dataset to train\/test set\n    if i == 0:\n        train_df = data\n        scaler.fit(train_df[feature_cols])\n        x_train = scaler.transform(train_df[feature_cols])\n        y_train = labels.values\n    else: \n        test_df = data\n        x_test = scaler.transform(test_df[feature_cols])\n    \nprint(\"train and test set successfully pre-processed\")","19689cf4":"results=[]\n\nfrom sklearn.model_selection import GridSearchCV","a723f986":"from sklearn.svm import SVC\n\nparam_grid = {\n            \"kernel\":[\"linear\", \"rbf\", \"poly\"],\n            \"C\":[4,5,6],\n            \"gamma\": [0.09, 0.1, 0.11]\n            }\n\ncv = GridSearchCV(estimator=SVC(), param_grid=param_grid, n_jobs=-1, cv =5)\ngrid_result = cv.fit(x_train, y_train)\n\n# summary\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with %r\" % (mean, stdev, param))\n\nresults.append(grid_result.best_score_)","e382257d":"from sklearn.neighbors import KNeighborsClassifier\n\nparam_grid = {\"n_neighbors\": [2,3,4,5,6,7,8,9,10]}\n\ncv = GridSearchCV(estimator = KNeighborsClassifier(), cv=5, param_grid=param_grid, n_jobs=-1)\ngrid_result = cv.fit(x_train, y_train)\n\n# summary\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with %r\" % (mean, stdev, param))\n\nresults.append(grid_result.best_score_)","d1f80052":"from sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {\n            \"n_estimators\":[80, 90, 100],\n            \"max_depth\":[10,12],\n            \"min_samples_split\":[2,3],\n            \"max_features\":[0.6, 0.7]\n            }\n\ncv = GridSearchCV(estimator = RandomForestClassifier(), cv=5, param_grid=param_grid, n_jobs=-1)\ngrid_result = cv.fit(x_train, y_train)\n\n# summary\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with %r\" % (mean, stdev, param))\n    \nresults.append(grid_result.best_score_)","fe812f7e":"from sklearn.ensemble import GradientBoostingClassifier\n\nparam_grid = {\n            \"n_estimators\":[75,80,85],\n            \"learning_rate\":[0.005, 0.01, 0.015,],\n            \"max_depth\":[6,7],\n            \"min_samples_split\":[4,5],\n            \"max_features\":[0.4, 0.5]\n            }\n\ncv = GridSearchCV(estimator = GradientBoostingClassifier(), cv=5, param_grid=param_grid, n_jobs=-1)\ngrid_result = cv.fit(x_train, y_train)\n\n# summary\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with %r\" % (mean, stdev, param))\n\nresults.append(grid_result.best_score_)","c1ad613c":"import tensorflow as tf\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.optimizers import *\nimport keras\nimport keras.backend as K","1b7dc34d":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","11367424":"# define input dim\ninput_dim = x_train.shape[1]\noutput_dim = 1\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n\n# Function to create model, required for KerasRegressor\ndef create_model(activation, neurons, dropout, learning_rate):\n    \n    adam = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n\n    model = Sequential()\n    model.add(Dense(neurons, input_dim=input_dim))\n    model.add(Activation(activation))\n    model.add(Dropout(dropout))\n    model.add(Dense(neurons))\n    model.add(Activation(activation))\n    model.add(Dropout(dropout))\n    model.add(Dense(output_dim))\n    model.add(Activation(\"sigmoid\"))\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])    \n    return model\n\n# create model\ngrid_model = KerasClassifier(build_fn=create_model, epochs=20, verbose=1)\n\n# define the grid search parameters\nneurons = [8, 16]\ndropout= [0.1, 0.2, 0.3]\nactivation = ['tanh', 'relu']\nlearning_rate = [0.001, 0.0001]\n\n# define grid with parameters to be tuned\nparam_grid = dict(neurons=neurons,activation=activation, dropout=dropout, learning_rate=learning_rate)\n\n# instanciate GridSearchCV with defined scoring and cv\ngrid = GridSearchCV(estimator=grid_model, \n                    param_grid=param_grid, \n                    scoring='accuracy', \n                    cv=3)\n#fit grid model\ngrid_result = grid.fit(x_train, y_train)\n    \nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with %r\" % (mean, stdev, param))","bce5158d":"# define input dim\ninput_dim = x_train.shape[1]\noutput_dim = 1\n\nadam = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=input_dim))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(16))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(output_dim))\nmodel.add(Activation(\"sigmoid\"))\n\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', precision_m, recall_m, f1_m])\nprint(model.summary())","98e5895c":"# fit model\nmodel_history = model.fit(x_train, y_train, \n                            epochs=200, \n                            validation_split=0.2,\n                            shuffle=True,\n                            callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min')])\n\nprint(model_history.history.keys())","cbad1c99":"fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n\naxs[0].plot(model_history.history['accuracy'], color=\"b\")\naxs[0].plot(model_history.history['val_accuracy'], color=\"black\")\naxs[0].set_xlabel('\\nEpoche', fontsize=14)\naxs[0].set_title('ACC\\n', fontsize=14)\naxs[0].legend(['Training', 'Validation'], loc='best')\n\naxs[1].plot(model_history.history['precision_m'], color=\"b\")\naxs[1].plot(model_history.history['val_precision_m'], color=\"black\")\naxs[1].set_xlabel('\\nEpoche', fontsize=14)\naxs[1].set_title('Precision\\n', fontsize=14)\naxs[1].legend(['Training', 'Validation'], loc='best')\n\naxs[2].plot(model_history.history['recall_m'], color=\"b\")\naxs[2].plot(model_history.history['val_recall_m'], color=\"black\")\naxs[2].set_xlabel('\\nEpoche', fontsize=14)\naxs[2].set_title('Recall\\n', fontsize=14)\naxs[2].legend(['Training', 'Validation'], loc='best')\naxs[2].yaxis.set_major_formatter(plt.FormatStrFormatter('%.2f'))\n\naxs[3].plot(model_history.history['f1_m'], color=\"b\")\naxs[3].plot(model_history.history['val_f1_m'], color=\"black\")\naxs[3].set_xlabel('\\nEpoche', fontsize=14)\naxs[3].set_title('\\nF1 Score\\n', fontsize=14)\naxs[3].legend(['Training', 'Validation'], loc='best')\naxs[3].yaxis.set_major_formatter(plt.FormatStrFormatter('%.2f'))\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.3)\nplt.show()","5d18155b":"nb_epochs = len(model_history.history['val_accuracy'])\nval_acc = model_history.history['val_accuracy']\nfinal_acc = val_acc[nb_epochs-1]\n\nresults.append(final_acc)","a7dd2a5e":"result_df = pd.DataFrame(data=results, columns=[\"Accuracy\"])\nresult_df.index=[\"SVC\", \"KNN\", \"Random Forest\", \"Gradient Boosting\", \"ANN\"]\nresult_df","683d1c3a":"plt.figure(figsize=(8,5))\nplt.bar(result_df.index, result_df.Accuracy, color=\"black\")\nplt.ylabel(\"ACC\\n\")\nplt.xticks(rotation=30)\nplt.ylim(0.7,1)\nplt.show()","08b4d30d":"from sklearn.metrics import confusion_matrix\n\n# predictions\ny_pred = model.predict_classes(x_train)\n\ncm = confusion_matrix(y_train, y_pred)\nprint(\"Confusion Matrix Training Data: \\n\\n\", cm)","bd27d599":"# make predictions on test data\ntest_predictions = model.predict_classes(x_test)\n\ndf1 = pd.DataFrame(test_passenger_id, index=None, columns=[\"PassengerId\"])\ndf2 = pd.DataFrame(data=test_predictions, columns=[\"Survived\"])\nfinal_data = df1.merge(df2, how=\"left\", left_index=True, right_index=True)\nfinal_data.to_csv('test.csv', header=True, index=False)\nfinal_data","cffe6a57":"## Import and Study Data","2b834dc0":"## Pre-Processing","dbc5561d":"## Performance Analysis ","8b332e63":"### Artificial Neural Network\nFristly, a GridSearch will help to find ideal Parameter.\nSecondly, the Network is trainer and analyzed.","623b3156":"### Gradient Boosting Classifier","a76189a7":"## Selection of best Model\n- Artificial Neural Network","2dea99ad":"### K Nearest Neighbor Classifier","ed2329d9":"### Support Vector Machine","3994559c":"### Random Forest Classifier","0fa40b52":"## Model\nTesting different models and comparing performances on the given dataset."}}