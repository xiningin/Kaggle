{"cell_type":{"a011e955":"code","4a4252a0":"code","e2cf0ef9":"code","9f0b2561":"code","7829c17e":"code","8dae263d":"code","8c15d6d5":"code","b420c390":"code","5892e859":"code","2c0a6e02":"code","c6f8aece":"code","838e0735":"code","79f8d5ab":"code","d60924ef":"code","d8bb164e":"code","5dc3e7ee":"code","5076c866":"code","30e2ef25":"code","7feef1d1":"code","91ec078d":"code","16de4c81":"code","2071305e":"code","15dfd4df":"code","63a6b1c5":"code","9a85b55e":"code","92336fcc":"code","d06b1cfc":"code","60b66be2":"code","61665f33":"code","226d15b6":"code","2e49904e":"code","a70a44b5":"code","888ca835":"code","699e9bff":"markdown","2742e962":"markdown"},"source":{"a011e955":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom nltk.tokenize import word_tokenize\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport string\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\n\nfrom collections import Counter\nimport re\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4a4252a0":"df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","e2cf0ef9":"tokenizer = Tokenizer(num_words=100, oov_token='OOV')\ntokenizer.fit_on_texts(df.head()['text'])","9f0b2561":"word_index = tokenizer.word_index\nprint(word_index)","7829c17e":"sequences = tokenizer.texts_to_sequences(df.head()['text'])\nprint(sequences)","8dae263d":"padded = pad_sequences(sequences, padding='post')\nprint(padded)","8c15d6d5":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F' \n        u'\\U0001F300-\\U0001F5FF' \n        u'\\U0001F680-\\U0001F6FF' \n        u'\\U0001F1E0-\\U0001F1FF' \n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\ndef remove_num(text):\n    text = ''.join([i for i in text if not i.isdigit()])\n    return text\n\n\ndf['text_clean'] = df['text'].apply(lambda x: remove_URL(x))\ndf['text_clean'] = df['text_clean'].apply(lambda x: remove_emoji(x))\ndf['text_clean'] = df['text_clean'].apply(lambda x: remove_html(x))\ndf['text_clean'] = df['text_clean'].apply(lambda x: remove_punct(x))\ndf['text_clean'] = df['text_clean'].apply(lambda x: remove_num(x))\n\ntest['text_clean'] = test['text'].apply(lambda x: remove_URL(x))\ntest['text_clean'] = test['text_clean'].apply(lambda x: remove_emoji(x))\ntest['text_clean'] = test['text_clean'].apply(lambda x: remove_html(x))\ntest['text_clean'] = test['text_clean'].apply(lambda x: remove_punct(x))\ntest['text_clean'] = test['text_clean'].apply(lambda x: remove_num(x))","b420c390":"df['tokenized'] = df['text_clean'].apply(word_tokenize)\ntest['tokenized'] = test['text_clean'].apply(word_tokenize)","5892e859":"df['lower'] = df['tokenized'].apply(lambda x: [word.lower() for word in x])\ntest['lower'] = test['tokenized'].apply(lambda x: [word.lower() for word in x])","2c0a6e02":"stop = set(stopwords.words('english'))","c6f8aece":"df['stopwords_removed'] = df['lower'].apply(lambda x: [word for word in x if word not in stop])\ntest['stopwords_removed'] = test['lower'].apply(lambda x: [word for word in x if word not in stop])","838e0735":"df['pos_tags'] = df['stopwords_removed'].apply(nltk.tag.pos_tag)\ntest['pos_tags'] = test['stopwords_removed'].apply(nltk.tag.pos_tag)","79f8d5ab":"def get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n\ndf['wordnet_pos'] = df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\ntest['wordnet_pos'] = test['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])","d60924ef":"wnl = WordNetLemmatizer()\ndf['lemmatized'] = df['wordnet_pos'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\ndf['lemmatized'] = df['lemmatized'].apply(\n    lambda x: [word for word in x if word not in stop])\ndf['lemma_str'] = [' '.join(map(str, l)) for l in df['lemmatized']]\n\ntest['lemmatized'] = test['wordnet_pos'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\ntest['lemmatized'] = test['lemmatized'].apply(\n    lambda x: [word for word in x if word not in stop])\ntest['lemma_str'] = [' '.join(map(str, l)) for l in test['lemmatized']]","d8bb164e":"trace = go.Pie(labels = ['Disaster : no', 'Disaster : yes'], values = df['target'].value_counts(), \n               textfont=dict(size=15), opacity = 0.8,\n               marker=dict(colors=['lightblue','gold'], \n                           line=dict(color='#000000', width=1.5)))\n\n\nlayout = dict(title =  'Real or not?',\n                        autosize = False,\n                        height  = 500,\n                        width   = 800)\n           \nfig = dict(data = [trace], layout=layout)\niplot(fig)","5dc3e7ee":"df['Character_Count'] = df['text_clean'].apply(lambda x: len(str(x).split()))","5076c866":"def kdeplot(feature):\n    plt.figure(figsize=(9, 4))\n    plt.title(\"KDE for {}\".format(feature))\n    ax0 = sns.kdeplot(df[df['target'] == 1][feature].dropna(), color= 'navy', label= 'Dissaster: Yes')\n    ax1 = sns.kdeplot(df[df['target'] == 0][feature].dropna(), color= 'orange', label= 'Disaster: No')\nkdeplot('Character_Count')","30e2ef25":"results = Counter()\ndf['lemma_str'].str.lower().str.split().apply(results.update)","7feef1d1":"text = df['lemma_str']\nplt.subplots(figsize=(16,12))\nwordcloud = WordCloud(\n                          background_color='white',\n                          width=800,\n                          height=600\n                         ).generate(\" \".join(text))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","91ec078d":"disaster = df[df['target'] == 1]\nnon_disaster = df[df['target'] == 0]","16de4c81":"text = disaster['lemma_str']\nplt.subplots(figsize=(16,12))\nwordcloud = WordCloud(\n                          background_color='white',\n                          width=800,\n                          height=600\n                         ).generate(\" \".join(text))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","2071305e":"count = Counter(df['lemma_str'])\nmost_common = count.most_common(10)","15dfd4df":"lis = [\n    df[df['target'] == 0]['lemma_str'],\n    df[df['target'] == 1]['lemma_str']\n]","63a6b1c5":"fig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis, axes):\n\n    new = i.str.split()\n    new = new.tolist()\n    corpus = [word for i in new for word in i]\n\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n    for word, count in most[:30]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)\n\n    sns.barplot(x=y, y=x, palette='plasma', ax=j)\naxes[0].set_title('Non Disaster Tweets')\n\naxes[1].set_title('Disaster Tweets')\naxes[0].set_xlabel('Count')\naxes[0].set_ylabel('Word')\naxes[1].set_xlabel('Count')\naxes[1].set_ylabel('Word')\n\nfig.suptitle('Most Common Unigrams', fontsize=24, va='baseline')\nplt.tight_layout()","9a85b55e":"def ngrams(n, title):\n    \"\"\"A Function to plot most common ngrams\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n    axes = axes.flatten()\n    for i, j in zip(lis, axes):\n\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word for i in new for word in i]\n\n        def _get_top_ngram(corpus, n=None):\n            #getting top ngrams\n            vec = CountVectorizer(ngram_range=(n, n),\n                                  max_df=0.9,\n                                  stop_words='english').fit(corpus)\n            bag_of_words = vec.transform(corpus)\n            sum_words = bag_of_words.sum(axis=0)\n            words_freq = [(word, sum_words[0, idx])\n                          for word, idx in vec.vocabulary_.items()]\n            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n            return words_freq[:15]\n\n        top_n_bigrams = _get_top_ngram(i, n)[:15]\n        x, y = map(list, zip(*top_n_bigrams))\n        sns.barplot(x=y, y=x, palette='plasma', ax=j)\n        \n        axes[0].set_title('Non Disaster Tweets')\n        axes[1].set_title('Disaster Tweets')\n        axes[0].set_xlabel('Count')\n        axes[0].set_ylabel('Words')\n        axes[1].set_xlabel('Count')\n        axes[1].set_ylabel('Words')\n        fig.suptitle(title, fontsize=24, va='baseline')\n        plt.tight_layout()","92336fcc":"ngrams(3, 'The trigram')","d06b1cfc":"from sklearn import feature_extraction, linear_model, model_selection\ncount_vectorizer = feature_extraction.text.CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(df[\"lemma_str\"])\nclf = linear_model.RidgeClassifier()","60b66be2":"scores = model_selection.cross_val_score(clf, train_vectors, df[\"target\"], cv=3, scoring=\"f1\")\n\nscores","61665f33":"clf.fit(train_vectors, df[\"target\"])","226d15b6":"test_vectors = count_vectorizer.transform(test[\"lemma_str\"])","2e49904e":"submission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","a70a44b5":"submission['target'] = clf.predict(test_vectors)","888ca835":"submission.to_csv(\"submission.csv\", index=False)","699e9bff":"## Modeling","2742e962":"I referred to other Kaggler's wisdom. thank you."}}