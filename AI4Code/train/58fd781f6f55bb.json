{"cell_type":{"30676750":"code","12a3b71c":"code","3019ea87":"code","64080950":"code","a12c7f19":"code","8d462bca":"code","65cca770":"code","88cb67a6":"code","1640e7a8":"code","3dfff536":"code","4aafc744":"code","c0e8e237":"markdown","3a179d7b":"markdown"},"source":{"30676750":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom subprocess import check_output","12a3b71c":"import os\nos.listdir('..\/input\/m5-uncertainity')","3019ea87":"#Ashish Model\nfile_1 = pd.read_csv('..\/input\/m5-uncertainity\/submission-0.11719.csv')\n\n#Ashish Model\nfile_2 = pd.read_csv('..\/input\/m5-uncertainity\/submission-0.11440.csv')\n\n#Ashish Model\nfile_3 = pd.read_csv('..\/input\/m5-uncertainity\/submission-0.11407.csv')\n\n#Mukharbek Model\nfile_4 = pd.read_csv('..\/input\/m5-uncertainity\/submission-0.11238.csv')\n\n#Ashish Model\nfile_5 = pd.read_csv('..\/input\/m5-uncertainity\/submission-0.11248.csv')\n\n#Mukharbek Model\nfile_6 = pd.read_csv('..\/input\/m5-uncertainity\/submission-.11029.csv')\n\nfile = file_1","64080950":"file_1.head(5)","a12c7f19":"file_2.head(5)","8d462bca":"file_3.head(5)","65cca770":"file_4.head(5)","88cb67a6":"file_5.head(5)","1640e7a8":"file_6.head(5)","3dfff536":"file['F1'] = (file_3['F1']+file_2['F1']+file_1['F1']+file_4['F1']+file_5['F1']+file_6['F1'])\/6\nfile['F2'] = (file_3['F2']+file_2['F2']+file_1['F2']+file_4['F2']+file_5['F2']+file_6['F2'])\/6\nfile['F3'] = (file_3['F3']+file_2['F3']+file_1['F3']+file_4['F3']+file_5['F3']+file_6['F3'])\/6\nfile['F4'] = (file_3['F4']+file_2['F4']+file_1['F4']+file_4['F4']+file_5['F4']+file_6['F4'])\/6\nfile['F5'] = (file_3['F5']+file_2['F5']+file_1['F5']+file_4['F5']+file_5['F5']+file_6['F5'])\/6\nfile['F6'] = (file_3['F6']+file_2['F6']+file_1['F6']+file_4['F6']+file_5['F6']+file_6['F6'])\/6\nfile['F7'] = (file_3['F7']+file_2['F7']+file_1['F7']+file_4['F7']+file_5['F7']+file_6['F7'])\/6\nfile['F8'] = (file_3['F8']+file_2['F8']+file_1['F8']+file_4['F8']+file_5['F8']+file_6['F8'])\/6\nfile['F9'] = (file_3['F9']+file_2['F9']+file_1['F9']+file_4['F9']+file_5['F9']+file_6['F9'])\/6\nfile['F10'] = (file_3['F10']+file_2['F10']+file_1['F10']+file_4['F10']+file_5['F10']+file_6['F10'])\/6\nfile['F11'] = (file_3['F11']+file_2['F11']+file_1['F11']+file_4['F11']+file_5['F11']+file_6['F11'])\/6\nfile['F12'] = (file_3['F12']+file_2['F12']+file_1['F12']+file_4['F12']+file_5['F12']+file_6['F12'])\/6\nfile['F13'] = (file_3['F13']+file_2['F13']+file_1['F13']+file_4['F13']+file_5['F13']+file_6['F13'])\/6\nfile['F14'] = (file_3['F14']+file_2['F14']+file_1['F14']+file_4['F14']+file_5['F14']+file_6['F14'])\/6\nfile['F15'] = (file_3['F15']+file_2['F15']+file_1['F15']+file_4['F15']+file_5['F15']+file_6['F15'])\/6\nfile['F16'] = (file_3['F16']+file_2['F16']+file_1['F16']+file_4['F16']+file_5['F16']+file_6['F16'])\/6\nfile['F17'] = (file_3['F17']+file_2['F17']+file_1['F17']+file_4['F17']+file_5['F17']+file_6['F17'])\/6\nfile['F18'] = (file_3['F18']+file_2['F18']+file_1['F18']+file_4['F18']+file_5['F18']+file_6['F18'])\/6\nfile['F19'] = (file_3['F19']+file_2['F19']+file_1['F19']+file_4['F19']+file_5['F19']+file_6['F19'])\/6\nfile['F20'] = (file_3['F20']+file_2['F20']+file_1['F20']+file_4['F20']+file_5['F20']+file_6['F20'])\/6\nfile['F21'] = (file_3['F21']+file_2['F21']+file_1['F21']+file_4['F21']+file_5['F21']+file_6['F21'])\/6\nfile['F22'] = (file_3['F22']+file_2['F22']+file_1['F22']+file_4['F22']+file_5['F22']+file_6['F22'])\/6\nfile['F23'] = (file_3['F23']+file_2['F23']+file_1['F23']+file_4['F23']+file_5['F23']+file_6['F23'])\/6\nfile['F24'] = (file_3['F24']+file_2['F24']+file_1['F24']+file_4['F24']+file_5['F24']+file_6['F24'])\/6\nfile['F25'] = (file_3['F25']+file_2['F25']+file_1['F25']+file_4['F25']+file_5['F25']+file_6['F25'])\/6\nfile['F26'] = (file_3['F26']+file_2['F26']+file_1['F26']+file_4['F26']+file_5['F26']+file_6['F26'])\/6\nfile['F27'] = (file_3['F27']+file_2['F27']+file_1['F27']+file_4['F27']+file_5['F27']+file_6['F27'])\/6\nfile['F28'] = (file_3['F28']+file_2['F28']+file_1['F28']+file_4['F28']+file_5['F28']+file_6['F28'])\/6","4aafc744":"file.to_csv(\"submission.csv\", index=False)","c0e8e237":"![Picture](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/18600\/logos\/thumb76_76.png?t=2020-02-07-00-58-27)\nIn this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world\u2019s largest company by revenue, to forecast daily sales for the next 28 days and to make uncertainty estimates for these forecasts. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.\n\nContributors on the Project\n* Ashish Gupta (https:\/\/www.kaggle.com\/roydatascience)\n* Mukharbek Organokov (https:\/\/www.kaggle.com\/muhakabartay) ","3a179d7b":"Kudos to Ulrich GOUE's kernels, that we used to improve our models upon. Further we averaged our solutions.\n\n* Ulrich GOUE's Kernel : https:\/\/www.kaggle.com\/ulrich07\/do-not-write-off-rnn-cnn\n* Ulrich GOUE's Kernel : https:\/\/www.kaggle.com\/ulrich07\/quantile-regression-with-keras"}}