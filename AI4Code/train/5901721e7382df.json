{"cell_type":{"cc050a3a":"code","53f23c0f":"code","7991f68f":"code","bb94157b":"code","c0dc47bc":"code","e16c5fa0":"code","ccd79dbc":"code","b7202496":"code","d4169481":"code","9613151f":"code","ab8330fa":"code","fc85a5cb":"code","8bfb1b3a":"code","e2beb0b4":"code","08f04a70":"code","9b740683":"code","167e97db":"code","32d1eb79":"code","0c9ac42e":"code","6f4041a7":"code","e3330f6e":"code","5e994539":"code","021c6491":"code","adaa9758":"code","5ee27fa7":"code","fb2185c1":"code","f33c38d3":"code","ae78b376":"code","ef87aaf3":"code","b761137c":"code","b8d0d06c":"code","c423655d":"code","7d7aa149":"code","46558bef":"code","faa70ace":"code","d3975f02":"code","b1db2994":"code","5e5dc887":"code","cd75dfca":"code","6f2de083":"code","0d47b999":"code","d324252b":"code","8fa9eb94":"code","b0cd6e9f":"code","b0ba0589":"code","4e354c6a":"code","f93d2fa7":"code","9b6fd0d6":"code","801fa690":"code","9c96117b":"code","173d7ba7":"code","bd69a574":"code","1e5356c8":"code","30da0fca":"code","82306937":"code","b7f2ce71":"code","2331d448":"code","d72fe12c":"code","c9a7202a":"code","d837583f":"code","4afcbfa0":"code","c17b003a":"code","6e6182c9":"code","542efdc0":"code","f2d49150":"code","8a6531b8":"code","84b59311":"code","2f3a4afa":"code","26e65960":"code","c2b097b2":"code","1ad3e439":"code","7acbb803":"code","f5ddaa39":"code","8159e06c":"code","2dad6c0e":"code","ba496dc0":"code","5c909884":"code","4d513a55":"code","1a79da54":"code","91716145":"code","247ed5d3":"code","c2d89504":"code","f8fa55a2":"code","41fc0a16":"code","461e2c39":"code","c97ce5b9":"code","51f7a69c":"code","05392df7":"code","0a6e1273":"code","99a565c8":"code","af9fa4f8":"code","5e51eef0":"code","e6371c18":"code","24fb54f1":"code","5173fd91":"code","a9267a32":"code","fb872212":"code","a5247447":"code","90a24dc0":"code","4bbe1d9e":"code","becd061b":"markdown","273b71fa":"markdown","5665886f":"markdown","59677d24":"markdown","7c882713":"markdown","00375ba7":"markdown","6b2a96a9":"markdown","271cf6d8":"markdown","7195d421":"markdown","17caa40a":"markdown","b0652b1c":"markdown","3cc0ecc2":"markdown","d613b52c":"markdown","148881a6":"markdown","2b65868f":"markdown","e517aa66":"markdown","274dd1a2":"markdown","6a43ee13":"markdown","b6ee5e45":"markdown","6c1b97e8":"markdown","f5cf9cc5":"markdown","22341989":"markdown","8f7c45a3":"markdown","7a66d670":"markdown","ea15dac7":"markdown","9f08bb5f":"markdown","d3f73266":"markdown","438966c1":"markdown","5076cc35":"markdown","18946d68":"markdown","31e1c84a":"markdown","90d8d1bb":"markdown","c51f4119":"markdown","788ecd2b":"markdown","fa3ac782":"markdown","d2a9b7fc":"markdown","1c576330":"markdown","7ecd0020":"markdown","4b2772bc":"markdown","863e4d39":"markdown","f24e2da2":"markdown","46596973":"markdown","e43223c5":"markdown","fb8fddfa":"markdown","b38884d4":"markdown","df0e9255":"markdown","743a1249":"markdown","bd2dc014":"markdown","caeb24a6":"markdown","73865227":"markdown","b66febd3":"markdown","dfcc2bcd":"markdown","585cc0ec":"markdown","ec6d87ec":"markdown","2e2c512f":"markdown","e6e372b4":"markdown","6da43edc":"markdown","4b193325":"markdown","1122aadb":"markdown","29b3487c":"markdown","9810a310":"markdown","d7ce76e4":"markdown","66a1b85a":"markdown","a610d7b9":"markdown","52abd2bb":"markdown","eec5b016":"markdown","51ac68c3":"markdown","ed7fce64":"markdown","23552fbc":"markdown","256214c5":"markdown","4156f0de":"markdown","1f72c3bf":"markdown","96ad291e":"markdown","1269ed78":"markdown","6bfdd3a3":"markdown"},"source":{"cc050a3a":"!pip install feature-engine\n!pip install arcticdata=='1.4'","53f23c0f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport arcticdata.ExploratoryDataAnalysis as eda\nimport arcticdata.FeatureEngineering as fe\nimport arcticdata.FeatureSelection as fs\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7991f68f":"train_raw = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_raw = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain = train_raw.copy()\ntest = test_raw.copy()\ntrain.head(3)","bb94157b":"eda.corr_plot(train)","c0dc47bc":"ids=test.PassengerId\n\nfor set_ in (train,test):\n    set_.drop(columns='PassengerId',inplace=True)\n\neda.missing_data(train,percentage=True).head()","e16c5fa0":"eda.missing_data(test,percentage=True).head()","ccd79dbc":"eda.feature_analysis(train,\"Survived\",test,\"Survived\")","b7202496":"eda.value_counts(train.Survived, percentage=True)","d4169481":"eda.feature_analysis(train,\"Pclass\",test,\"Survived\")","9613151f":"eda.value_counts(train.Pclass,percentage=True)","ab8330fa":"def get_survivors(x):\n    print(train[[x,\"Survived\"]].groupby([x],as_index=False).mean())\n    fig, ax = plt.subplots(figsize=(12,6))\n    sns.barplot(x=x,y=\"Survived\",data=train,ax=ax,palette='colorblind')\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n\nget_survivors(\"Pclass\")","fc85a5cb":"eda.feature_analysis(train,\"Name\",test,\"Survived\")","8bfb1b3a":"print('Name cardinality: {}'.format(len(train.Name.unique())))\n\nname_status=[]\nfor x in train.Name.to_list():\n    name_status.append(x.split(\", \")[1].split('.')[0])\ntrain['Name']=name_status\n\nname_status=[]\nfor x in test.Name.to_list():\n    name_status.append(x.split(\", \")[1].split('.')[0])\ntest['Name']=name_status\nprint('Name cardinality: {}'.format(len(train.Name.unique())))\ntrain.head()","e2beb0b4":"train['Name'] = train['Name'].replace('Mlle', 'Miss')\ntrain['Name'] = train['Name'].replace('Ms', 'Miss')\ntrain['Name'] = train['Name'].replace('Mme', 'Mrs')\n\ntest['Name'] = test['Name'].replace('Mlle', 'Miss')\ntest['Name'] = test['Name'].replace('Ms', 'Miss')\ntest['Name'] = test['Name'].replace('Mme', 'Mrs')","08f04a70":"train.head()","9b740683":"eda.value_counts(train.Name,percentage=True).head()","167e97db":"train['Name']=fe.group_rare_labels(train.Name,train.Name)\ntest['Name']=fe.group_rare_labels(train.Name,test.Name)\n\neda.value_counts(train.Name,percentage=True)","32d1eb79":"get_survivors(\"Name\")","0c9ac42e":"eda.feature_analysis(train,\"Sex\",test,\"Survived\")","6f4041a7":"eda.value_counts(train.Sex,percentage=True)","e3330f6e":"get_survivors(\"Sex\")","5e994539":"eda.feature_analysis(train,\"Age\",test,\"Survived\")","021c6491":"eda.distribution_diag(train,'Age')","adaa9758":"test = fe.impute_missing(train.drop('Survived',axis=True),test,method='indicator',variables=[\"Age\"])\ntrain = fe.impute_missing(train,train,method='indicator',variables=[\"Age\"])","5ee27fa7":"print(eda.outliers(train,'Age',table=True, method=\"IQR3\").shape)\nprint(eda.outliers(train,'Age',table=True, method=\"IQR\").shape)","fb2185c1":"eda.outliers(train,'Age',table=True,limits=False)","f33c38d3":"train = fe.impute_missing(train,train,variables=[\"Age\"],method='median')\ntest = fe.impute_missing(train.drop('Survived',axis=1),test,variables=[\"Age\"],method='median')\neda.missing_data(train).head(3)","ae78b376":"eda.missing_data(test).head(3)","ef87aaf3":"eda.distribution_diag(train,'Age')","b761137c":"test[\"AgeFreq\"] = fe.discretization(train.drop('Survived',axis=1),test,variables=['Age'])['Age']\ntrain[\"AgeFreq\"] = fe.discretization(train,train,variables=['Age'])['Age']\ntest[\"AgeRange\"] = fe.discretization(train.drop('Survived',axis=1),test,variables=['Age'],method=\"equalrange\")['Age']\ntrain[\"AgeRange\"] = fe.discretization(train,train,variables=['Age'],method=\"equalrange\")['Age']\ntrain.head(3)","b8d0d06c":"train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n\nget_survivors(\"FamilySize\")","c423655d":"train['IsAlone'] = 0\ntrain.loc[train['FamilySize'] == 1, 'IsAlone'] = 1\ntest['IsAlone'] = 0\ntest.loc[test['FamilySize'] == 1, 'IsAlone'] = 1","7d7aa149":"train.head()","46558bef":"eda.feature_analysis(train,\"SibSp\",test,\"Survived\")","faa70ace":"eda.value_counts(train.SibSp,percentage=True)","d3975f02":"get_survivors(\"SibSp\")","b1db2994":"eda.feature_analysis(train,\"Parch\",test,\"Survived\")","5e5dc887":"eda.value_counts(train.Parch,percentage=True)","cd75dfca":"get_survivors(\"Parch\")","6f2de083":"eda.feature_analysis(train,\"Ticket\",test,\"Survived\")","0d47b999":"train.Ticket.head(8)","d324252b":"print(\"Ticket cardinality: {}\".format(len(train.Ticket.unique())))","8fa9eb94":"train.drop(columns='Ticket',inplace=True)\ntest.drop(columns='Ticket',inplace=True)","b0cd6e9f":"eda.feature_analysis(train,\"Fare\",test,\"Survived\")","b0ba0589":"eda.distribution_diag(train,'Fare')","4e354c6a":"eda.outliers(train,'Fare',table=True,limits=False,method=\"IQR3\").shape","f93d2fa7":"train=fe.impute_missing(train,train,variables=['Fare'],method='median')\ntest=fe.impute_missing(train.drop('Survived',axis=1),test,variables=['Fare'],method='median')","9b6fd0d6":"train=fe.transform(train,variables='Fare',method='yeo')\ntest=fe.transform(test,variables='Fare',method='yeo')\neda.distribution_diag(train,'Fare')","801fa690":"eda.outliers(train,'Fare',table=True,limits=False,method=\"IQR3\").shape","9c96117b":"eda.missing_data(train).head(3)","173d7ba7":"eda.missing_data(test).head(3)","bd69a574":"test[\"FareFreq\"] = fe.discretization(train.drop('Survived',axis=1),test,variables=['Fare'])['Fare']\ntrain[\"FareFreq\"] = fe.discretization(train,train,variables=['Fare'])['Fare']\ntest[\"FareRange\"] = fe.discretization(train.drop('Survived',axis=1),test,variables=['Fare'],method=\"equalrange\")['Fare']\ntrain[\"FareRange\"] = fe.discretization(train,train,variables=['Fare'],method=\"equalrange\")['Fare']","1e5356c8":"eda.feature_analysis(train,\"Cabin\",test,\"Survived\")","30da0fca":"print(\"Cabin cardinality: {}\".format(len(train.Cabin.unique())))","82306937":"train['Cabin']=train.Cabin.str[0]\ntest['Cabin']=test.Cabin.str[0]\nprint(\"Cabin cardinality: {}\".format(len(train.Cabin.unique())))\neda.value_counts(train.Cabin,percentage=True)","b7f2ce71":"test=fe.impute_missing(train.drop('Survived',axis=1),test,variables=['Cabin'],method='indicator')\ntrain=fe.impute_missing(train,train,variables=['Cabin'],method='indicator')\n\ntrain=fe.impute_missing(train,train,variables=['Cabin'],method='category')\ntest=fe.impute_missing(train.drop('Survived',axis=1),test,variables=['Cabin'],method='category')\neda.missing_data(train).head(3)","2331d448":"eda.missing_data(test).head(3)","d72fe12c":"train['Cabin']=fe.group_rare_labels(train.Cabin,train.Cabin)\ntest['Cabin']=fe.group_rare_labels(train.drop('Survived',axis=1).Cabin,test.Cabin)\n\neda.value_counts(train.Cabin,percentage=True)","c9a7202a":"get_survivors(\"Cabin\")","d837583f":"eda.feature_analysis(train,\"Embarked\",test,\"Survived\")","4afcbfa0":"eda.value_counts(train.Embarked,percentage=True)","c17b003a":"train.loc[(train.Embarked!='S')&(train.Embarked!='C')&(train.Embarked!='Q'),'Embarked'] = 'S'\neda.missing_data(train).head(3)","6e6182c9":"eda.missing_data(test).head(3)","542efdc0":"train.head()","f2d49150":"train.shape,test.shape","8a6531b8":"test = fe.encoding(train.drop('Survived',axis=1),test,variables=['Name','Cabin','Embarked','Sex'],drop_last=False)\ntrain = fe.encoding(train,train,variables=['Name','Cabin','Embarked','Sex'],drop_last=False)","84b59311":"train.shape,test.shape","2f3a4afa":"testcols = test.columns.values.tolist()\ntraincols = train.columns.values.tolist()\nprint(traincols)\nprint(testcols)","26e65960":"train.dtypes","c2b097b2":"train.head()","1ad3e439":"X_train_g = train.drop(\"Survived\", axis=1)\ny_train_g = train[\"Survived\"]\nX_test_g  = test\nX_train_g.shape, y_train_g.shape, X_test_g.shape","7acbb803":"X_train_g = fe.scale(X_train_g)\nX_test_g = fe.scale(X_test_g)","f5ddaa39":"X_train_g.head()","8159e06c":"import pickle\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_g,y_train_g, test_size=0.2, random_state=0)","2dad6c0e":"np.savez_compressed(\"np_savez_comp\", X=X_train, y=y_train)\ndata = np.load(\"np_savez_comp.npz\")\n\nX = data[\"X\"]\ny = data[\"y\"]","ba496dc0":"def confusion_plot():\n    print(classification_report(y_valid,predicted))\n    matrix = confusion_matrix(y_valid, predicted)\n    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\n    plt.xlabel(\"predicted\")\n    plt.ylabel(\"actual\")\n    plt.show()","5c909884":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\nwith open('Naive Bayes.pickle', mode='wb') as fp:\n    pickle.dump(gnb, fp)\n    \nprint(f'NB score: {cross_val_score(gnb,X_train,y_train,cv=10).mean()}\\n\\n')\n\npredicted=gnb.predict(X_valid)\n\n\nconfusion_plot()","4d513a55":"from sklearn.linear_model import LogisticRegression\nlogistic_regression = LogisticRegression(random_state=0)\nlogistic_regression.fit(X_train, y_train)\n\nwith open('Logistic Regression.pickle', mode='wb') as fp:\n    pickle.dump(logistic_regression, fp)\n\nprint(f'NB score: {cross_val_score(logistic_regression,X_train,y_train,cv=10).mean()}\\n\\n')\n\npredicted=logistic_regression.predict(X_valid)\n\n\n\nconfusion_plot()","1a79da54":"from sklearn.svm import SVC\nsvm = SVC(kernel='linear', C=1.0, random_state=0)\nsvm.fit(X_train, y_train)\n\nwith open('Support Vector Machine.pickle', mode='wb') as fp:\n    pickle.dump(svm, fp)\n    \nprint(f'NB score: {cross_val_score(svm,X_train,y_train,cv=10).mean()}\\n\\n')\n\n\nsvm2 = SVC(kernel='rbf', C=1.0, random_state=0)\nsvm2.fit(X_train, y_train)\n\nprint(f'NB score: {cross_val_score(svm2,X_train,y_train,cv=10).mean()}\\n\\n')\n\npredicted=svm.predict(X_valid)\n\n\n\nconfusion_plot()","91716145":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\nknn.fit(X_train, y_train)\n\nwith open('Nearest Neighbors.pickle', mode='wb') as fp:\n    pickle.dump(knn, fp)\n    \nprint(f'NB score: {cross_val_score(knn,X_train,y_train,cv=10).mean()}\\n\\n')\n\npredicted=knn.predict(X_valid)\n\n\nconfusion_plot()","247ed5d3":"from sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(criterion='entropy',random_state=0)\ndecision_tree.fit(X_train, y_train)\n\nwith open('Decision Tree.pickle', mode='wb') as fp:\n    pickle.dump(decision_tree, fp)\n    \nprint(f'NB score: {cross_val_score(decision_tree,X_train,y_train,cv=10).mean()}\\n\\n')\n\npredicted=decision_tree.predict(X_valid)\n\n\nconfusion_plot()","c2d89504":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(random_state=0)\nrandom_forest.fit(X_train, y_train)\n\nwith open('Random Forest.pickle', mode='wb') as fp:\n    pickle.dump(random_forest, fp)\n    \nprint(f'NB score: {cross_val_score(random_forest,X_train,y_train,cv=10).mean()}\\n\\n')\n\npredicted=random_forest.predict(X_valid)\n\n\nconfusion_plot()","f8fa55a2":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)\n\nwith open('Gradient Boosting.pickle', mode='wb') as fp:\n    pickle.dump(gb, fp)\n    \nprint(f'NB score: {cross_val_score(gb,X_train,y_train,cv=10).mean()}\\n\\n')\n\npredicted=gb.predict(X_valid)\n\n\nconfusion_plot()","41fc0a16":"names = [\"Support Vector Machine\", \"Logistic Regression\", \"Nearest Neighbors\",\n         \"Decision Tree\",\"Random Forest\", \"Naive Bayes\",\"Gradient Boosting\"]\nresult = []\n\nprint(\"For guessing the Survived feature we used the following models:\")\nprint()\n\nfor name in names:\n    with open(name + '.pickle', 'rb') as fp:\n        clf = pickle.load(fp)    \n    clf.fit(X_train, y_train)\n    score1 = cross_val_score(clf,X_train,y_train,cv=10).mean()\n    score2 = cross_val_score(clf,X_valid,y_valid,cv=10).mean()\n    result.append([score1, score2])\n    \n    print(name)\n\ndf_result = pd.DataFrame(result, columns=['Training', 'Validation'], index = names)\ndf_result.sort_values(\"Validation\",ascending=False)","461e2c39":"model=GradientBoostingClassifier()\nfs.step_forward(X_train,y_train,model=model,kfeatures=26,info=False)","c97ce5b9":"model=GradientBoostingClassifier()\nfs.step_forward(X_train,y_train,model=model,kfeatures=6,info=True)","51f7a69c":"model=RandomForestClassifier()\nfs.step_forward(X_train,y_train,model=model,kfeatures=26,info=False)","05392df7":"model=RandomForestClassifier()\nfs.step_forward(X_train,y_train,model=model,kfeatures=6,info=True)","0a6e1273":"features = ['Fare', 'FamilySize', 'IsAlone', 'Name_Mr', 'Name_Rare', 'Embarked_Q', 'Pclass', 'Age_na']\nX_train_g = X_train_g[['Fare', 'FamilySize', 'IsAlone', 'Name_Mr', 'Name_Rare', 'Embarked_Q', 'Pclass', 'Age_na']]\nX_test_g = X_test_g[features]\nX_train = X_train[features]\nX_valid = X_valid[features]\nX_train_g.head()","99a565c8":"COVERAGE95=0.95\nCOVERAGE99=0.99\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(X_train_g)\n\ncontributions = pca.explained_variance_ratio_\ncoverages = pca.explained_variance_ratio_.cumsum()\nk95 = np.argmax(coverages >= COVERAGE95)\nk99 = np.argmax(coverages >= COVERAGE99)\n\nprint(\"k-th primary component for 95% coverage is {}\".format(k95 + 1))\nprint(\"k-th primary component for 99% coverage is {}\".format(k99 + 1))","af9fa4f8":"pca = PCA(n_components=7)\npca.fit(X_train_g)\n#print(pca.explained_variance_ratio_)\nx=pca.transform(X_train_g)\nX_train_pca = pd.DataFrame(x)","5e51eef0":"params = {'n_estimators'  : [500],\n              'max_features'  : ['auto'],\n              'max_depth'     : [None, 1, 3, 5, 10, 20],\n              'subsample'     : [0.5, 1],\n              'learning_rate' : [0.01]\n             }\n\nclf = GridSearchCV(gb, params, cv=10, return_train_score=True)\nbest_clf = clf.fit(X_train,y_train)\nbest_clf\n\nmeans=clf.cv_results_[\"mean_test_score\"]\nstds=clf.cv_results_[\"std_test_score\"]\nparams=clf.cv_results_[\"params\"]\n\nfor m,s,p in zip(means,stds,params):\n    print(\"%0.3f (+\/-%0.3f) for %r\"%(m,2*s,p))\n    \nprint(\"=\"*100)\nprint(\"=\"*100)\n\nprint('best score: {:0.3f}'.format(clf.score(X_train, y_train)))\nprint('best params: {}'.format(clf.best_params_))\nprint('best val score:  {:0.3f}'.format(clf.best_score_))","e6371c18":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nparams =  {'n_estimators': [100], \n                                  'bootstrap': [False],\n                                  'max_depth': [50,75,100,None],\n                                  'max_features': ['sqrt'],\n                                  'min_samples_leaf': [2,4,5],\n                                  'min_samples_split': [2,5]}\n\nclf = GridSearchCV(random_forest, params, cv=10, return_train_score=True)\nbest_clf = clf.fit(X_train,y_train)\nbest_clf\n\nmeans=clf.cv_results_[\"mean_test_score\"]\nstds=clf.cv_results_[\"std_test_score\"]\nparams=clf.cv_results_[\"params\"]\n\nfor m,s,p in zip(means,stds,params):\n    print(\"%0.3f (+\/-%0.3f) for %r\"%(m,2*s,p))\n    \nprint(\"=\"*100)\nprint(\"=\"*100)\n\nprint('best score: {:0.3f}'.format(clf.score(X_train, y_train)))\nprint('best params: {}'.format(clf.best_params_))\nprint('best val score:  {:0.3f}'.format(clf.best_score_))","24fb54f1":"model = RandomForestClassifier(bootstrap=False, max_depth=50, max_features='sqrt',min_samples_leaf=2, random_state=0)\n\nrf_predtrain = model.fit(X_train_g,y_train_g).predict(X_train_g)\nrf_predtest = model.fit(X_train_g,y_train_g).predict(X_test_g)","5173fd91":"pca = PCA(n_components=7)\npca.fit(X_test_g)\n#print(pca.explained_variance_ratio_)\nx=pca.transform(X_test_g)\nX_test_pca = pd.DataFrame(x)\n\nrf_predtrainpca = model.fit(X_train_pca,y_train_g).predict(X_train_pca)\nrf_predtestpca = model.fit(X_train_pca,y_train_g).predict(X_test_pca)","a9267a32":"X_train_g.shape,y_train_g.shape,X_test_g.shape","fb872212":"rf_predtrain.shape,rf_predtest.shape,rf_predtrainpca.shape,rf_predtestpca.shape","a5247447":"best_gb = GradientBoostingClassifier(learning_rate=0.01, max_features='auto',n_estimators=500, subsample=0.5)\nXtrain = np.column_stack((X_train_g,rf_predtrain))\nXtest = np.column_stack((X_test_g,rf_predtest))\n\nXtrainpca = np.column_stack((X_train_pca,rf_predtrainpca))\nXtestpca = np.column_stack((X_test_pca,rf_predtestpca))","90a24dc0":"preds = best_gb.fit(Xtrain,y_train_g).predict(Xtest)\noutput = pd.DataFrame({ \"PassengerId\" : ids, \"Survived\": preds })\noutput.to_csv(\"submission.csv\", index=False)","4bbe1d9e":"preds = best_gb.fit(Xtrainpca,y_train_g).predict(Xtestpca)\noutput = pd.DataFrame({ \"PassengerId\" : ids, \"Survived\": preds })\noutput.to_csv(\"submissionpca.csv\", index=False)","becd061b":"**38.38%** of the passengers survived while the other **61.62%** did not","273b71fa":"![](https:\/\/i.imgur.com\/N16UsDR.png)\nNumeric features:\n   *     PassengerID: Identifier for each passenger. This feature won\u00b4t have value for our model  (Continuous)\n   *     Survived: Survival status of the passenger. 1=Survived 0=Deceased. This is the feature that we need to predict in the test set (Discrete)\n   *     Pclass: Ticket class. 1=First Class  2=Second Class  3=Third Class  (Discrete)\n   *     Age: Age of the passenger  (Continuous)\n   *     SibSp: Number of siblings and spouses aboard  (Discrete)\n   *     Parch: Number of parents and children aboard  (Discrete)\n   *     Fare: Ticket Fare  (Continuous)\n        \nCategorical features:\n   * Cabin: Cabin number. (Alphanumeric)\n   * Embarked: Embarkation port. C=Cherbourg  Q=Queenstown  S=Southampton\n   * Name: Title and name of the passenger\n   * Sex: Male or Female\n   * Ticket: Ticket number (Alphanumeric)","5665886f":"<br>\n<br>","59677d24":"# Sex","7c882713":"Since the variable 'PassengerId' does not provide any value to our models, we will remove it.\n\nThen, we will look at the missing data in training and test set","00375ba7":"![](https:\/\/i.imgur.com\/TdKX2gW.png)","6b2a96a9":"# Pclass","271cf6d8":"Most of the passengers boarded at Southampton. ","7195d421":"Missing values may not be missing randomly and their absence may be due to another variable. It is possible that the passenger's cabin or age could not be identified if they didn't survive so we will create a new variable to identify whether or not there was missing data.","17caa40a":"# We take the best model for predicting","b0652b1c":"As we can see, there are 116 outliers so we will have to transform the variable to improve its distribution and reduce the outliers.","3cc0ecc2":"Most of the passengers were of a lower socio-economic class and therefore traveled in third class. ","d613b52c":"We have performed the transformation with Yeo-Johnson's method and when checking the number of outliers we see that the number of outliers has gone from 116 to 18. We will treat outliers as missing values to be imputed by performing a simple random sampling.","148881a6":"# Survived","2b65868f":"# **Titanic prediction**\nIn this notebook we will try to predict if someone survived to the Titanic. This is my second notebook so any suggestions are welcome. If anyone has any recommendations or tips let me know them in the comments. Enjoy it!","e517aa66":"# Embarked","274dd1a2":"Let's create another variable to indicate whether a passenger was traveling alone or not.","6a43ee13":"As we can see, women had much more probability for surviving the disaster","b6ee5e45":"# Name\nThe first thing we notice about this variable is its high cardinality, which does not add any value to the model, so we must first reduce this cardinality to keep the important information.","6c1b97e8":"# Naive Bayes","f5cf9cc5":"# Logistic Regression","22341989":"# Parch","8f7c45a3":"Let's create a variable that divides age into intervals of equal frequency to decide which variable best explains the data and other variable that divides age into intervals of equal range","7a66d670":"# Ticket","ea15dac7":"<br>\n<br>","9f08bb5f":"We can see how the distribution of the variable 'Age' is almost Gaussian, it presents some outliers and as we saw before, it has 177 NAN in the training set and 86 NAN in the test set.\n\nWe create an indicator in the training set and in the test set to see when there were missing values in the original variable in case those values were missing non-randomly.","d3f73266":"# Best Model","438966c1":"<br>\n<br>","5076cc35":"We can see how the probability of surviving increases to a family of 4 and then decreases.","18946d68":"The variable 'Ticket' also has a high cardinality and it seems that we cannot extract any useful information from the variable so we will remove it.","31e1c84a":"The variable 'Cabin' also has a very high cardinality so we will have to reduce it. For this purpose, we will keep only the letter of the cabin","90d8d1bb":"Some categories are below 5% of representativeness so we are going to group these categories creating a new one called 'rare'. This will help when putting it into production","c51f4119":"# IsAlone","788ecd2b":"<br>\n<br>","fa3ac782":"# K Nearest Neighbor","d2a9b7fc":"# Fare","1c576330":"<br>\n<br>","7ecd0020":"<br>\n<br>","4b2772bc":"As before, we replace the missing values with a random sample so as not to modify the original distribution of the variable.","863e4d39":"<br>\n<br>","f24e2da2":"We are going to create a new variable to indicate the size of the family through the variables SibSp and Parch.","46596973":"<br>\n<br>","e43223c5":"<br>\n<br>","fb8fddfa":"![](https:\/\/i.imgur.com\/z7cwz70.png)","b38884d4":"We can see how about **60%** of first class passengers survived while only about **25%** of third class passengers survived.","df0e9255":"The 'fare' variable is asymmetric, has missing data and also has several outliers (some closer and some further).","743a1249":"# Creating the Submission File","bd2dc014":"# Extreme Gradient Boosting","caeb24a6":"# Random Forest","73865227":"The survival rate for female passengers was **74%** while the survival rate for male passengers was **19%**.","b66febd3":"**64.76%** of the passengers were male while **35.24%** of the passengers were female.","dfcc2bcd":"## Correlations","585cc0ec":"<br>\n<br>","ec6d87ec":"# Missing values","2e2c512f":"# FamilySize","e6e372b4":"<br>\n<br>","6da43edc":"# Support Vector Classifier","4b193325":"# Cabin","1122aadb":"# Age","29b3487c":"<br>\n<br>","9810a310":"<br>\n<br>\n<br>\n<br>\n<br>","d7ce76e4":"We will divide the fare variable into ranges with the same frequency as for age","66a1b85a":"![](https:\/\/i.imgur.com\/HEmI1A5.png)","a610d7b9":"![](https:\/\/i.imgur.com\/7XqqaQE.png)","52abd2bb":"<br>\n<br>","eec5b016":"<br>\n<br>","51ac68c3":"As we can see in the graph we still have a relatively high cardinality and some categories are below 5% of representativeness so we are going to group these categories creating a new one called 'rare'. ","ed7fce64":"# Hope you liked it! Upvote if you liked and comment if you have any tip!","23552fbc":"<br>\n<br>","256214c5":"<br>\n<br>\n<br>\n<br>\n<br>","4156f0de":"![](https:\/\/i.imgur.com\/z3KMxLA.png)","1f72c3bf":"<br>\n<br>","96ad291e":"<br>\n<br>","1269ed78":"# SibSp","6bfdd3a3":"# Decision Tree"}}