{"cell_type":{"013f4cf5":"code","03d1b2b0":"code","7e534efa":"code","f593367b":"code","abb4bc25":"code","60fbcb02":"code","214ae066":"code","a8e98966":"code","bb497fb2":"code","b7c4b257":"code","7a95888f":"code","f15c488a":"code","3efef71d":"code","26367a7a":"code","f7dff535":"code","df518b81":"code","504f7467":"code","1603ab46":"code","a04d5242":"code","b20aa02b":"code","8f3d0270":"code","600f7e98":"code","e4140cbe":"code","bdf2a791":"code","85e9f5bb":"code","8149c279":"code","5740c113":"code","431ab8e2":"code","4c8a6388":"code","832dc02f":"code","8abf721e":"code","bb41ea34":"code","465f4db5":"code","82abbe86":"code","b2440bbb":"code","5a5646c3":"code","68f53e8e":"code","f7467120":"code","cabec25d":"code","ba95c485":"code","6c1157ae":"code","07790f02":"code","29de377c":"code","4ef70e5a":"code","b6aeb68a":"code","2ac59253":"code","2729c2f0":"code","b2daeb3d":"code","3ad6e455":"code","3af0bd06":"code","d1e74b1b":"code","b6a65ad4":"code","050032fe":"code","25cc65f5":"code","c3d94214":"code","84da6a46":"code","e0ef9fda":"code","7b459bdc":"code","c4bf061a":"code","2356a0f9":"code","008a5bd0":"code","1fb4553a":"code","7998b316":"code","5a497fec":"code","92eff6d5":"code","79a0722b":"code","f88afb2e":"code","81a6ba14":"code","6db13e3b":"code","6f060b53":"code","f2bc57be":"markdown","1813c5d6":"markdown","53f4baae":"markdown","20dbfe61":"markdown","07b68431":"markdown","2d176dc8":"markdown","88b46326":"markdown","14de2345":"markdown","1bb0078d":"markdown","8d3ab724":"markdown","655a0a4e":"markdown","c0042fa2":"markdown","ac84b7bf":"markdown","ecdca82d":"markdown","68e6ed4f":"markdown","dd7e26bd":"markdown","ba510585":"markdown","2dc1c67c":"markdown","3ccdf88c":"markdown","a5f8c203":"markdown","355b61c2":"markdown","dc2f9dd4":"markdown","a91d5403":"markdown","1cdcf5ea":"markdown","4ec81f58":"markdown","37a57d0e":"markdown","b6ad8451":"markdown","8680c286":"markdown","1d6724a6":"markdown","ff6acb97":"markdown","f81f3749":"markdown","653a8918":"markdown","d30987df":"markdown","3423fed3":"markdown","75c71aa9":"markdown","58ab3088":"markdown","2d1aaa17":"markdown","16555d02":"markdown","5a366496":"markdown","8ff5e35a":"markdown","43f37689":"markdown","06334ce3":"markdown","374f30a7":"markdown"},"source":{"013f4cf5":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve,\\\n                            classification_report, confusion_matrix, plot_confusion_matrix\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'  # \u2018all\u2019|\u2019last\u2019|\u2019last_expr\u2019|\u2019none\u2019","03d1b2b0":"df_train = pd.read_csv('..\/input\/jobathon-train\/train_job_a_thon.csv')\ndf_test = pd.read_csv('..\/input\/jobathon-train\/test_job_a_thon.csv')\ntest_submission = df_test[['ID']]\ny = df_train['Is_Lead']\n","7e534efa":"df_train.head()\ndf_test.head()\ntest_submission.head()","f593367b":"dd = [['ID',\"Unique Identifier for a row\"],\n             ['Gender','Gender of the Customer'],\n             ['Age','Age of the Customer (in Years)'],\n             ['Region_Code','Code of the Region for the customers'],\n             ['Occupation','Occupation Type for the customer'],\n             ['Channel_Code','Acquisition Channel Code for the Customer  (Encoded)'],\n             ['Vintage',\"Vintage for the Customer (In Months).\"],\n             ['Credit_Product',\"If the Customer has any active credit product (Home loan, Personal loan, Credit Card etc.)\"],\n             ['Avg_Account_Balance','Average Account Balance for the Customer in last 12 Months'],\n             ['Is_Active','If the Customer is Active in last 3 Months'],\n             ['Is_Lead(Target)','If the Customer is interested for the Credit Card:- \\n\\t\\t\\t  0 : Customer is not interested; 1 : Customer is interested']\n            ]\ndd = pd.DataFrame(dd,columns=['Attributes','Description'])\n\nprint(\"\\033[1m\\033[4m\\033[43m     Variable Name:      \\033[0m\\\n \\033[1m\\033[4m\\033[43m            Description:                   \\033[0m\")\nfor i in range(dd.shape[0]):\n    print(\"\\033[7m{:2}- {:20s}:\\033[0m {}\".format(\n        str(i+1), dd.iloc[i, 0], dd.iloc[i, 1]))","abb4bc25":"print ('\\033[1mThe Train dataset has {}-rows and {}-columns.\\033[0m'.format(df_train.shape[0], df_train.shape[1]))\nprint ('\\033[1mThe Test dataset has {}-rows and {}-columns.\\033[0m'.format(df_test.shape[0], df_test.shape[1]))","60fbcb02":"print ('\\n\\033[7mTrain dataset Info.\\033[0m\\n')\ndf_train.info()\n\nprint ('\\n\\n\\033[7mTest dataset Info.\\033[0m\\n')\ndf_test.info()","214ae066":"print ('\\n\\033[7mCount of Null in Train dataset.\\033[0m')\ndf_null = df_train.isna().sum().reset_index().rename(columns = {'index': 'Columns', 0: 'Count of Null'})\ndf_null['%age of Null'] = round(df_null['Count of Null'] *100 \/ df_train.shape[0], 2)\ndf_null\n\nprint ('\\n\\033[7mCount of Null in Test dataset.\\033[0m')\ndf_null = df_test.isna().sum().reset_index().rename(columns = {'index': 'Columns', 0: 'Count of Null'})\ndf_null['%age of Null'] = round(df_null['Count of Null'] *100 \/ df_test.shape[0], 2)\ndf_null","a8e98966":"dups = df_train.duplicated(keep='first')\nprint('\\033[7mTotal count of duplicate records in Train dataset = {}.\\033[0m'.format(dups.sum()))\ndf_train[dups]\n\ndups = df_test.duplicated(keep='first')\nprint('\\n\\033[7mTotal count of duplicate records in Test dataset = {}.\\033[0m'.format(dups.sum()))\ndf_test[dups]","bb497fb2":"dups = df_train.drop(['ID','Is_Lead'],axis=1).duplicated(keep='first')\nprint('\\033[7mTotal count of duplicate records (without ID columns) in Train dataset = {}.\\033[0m'.format(dups.sum()))\ndf_train[dups]\n\ndups = df_test.drop(['ID'],axis=1).duplicated(keep='first')\nprint('\\n\\033[7mTotal count of duplicate records (without ID columns) in Test dataset = {}.\\033[0m'.format(dups.sum()))\ndf_test[dups]","b7c4b257":"df_train.drop(['ID'],axis=1, inplace=True)\ndf_test.drop(['ID'],axis=1, inplace=True)","7a95888f":"print ('\\n\\033[7mDescription of Train dataset.\\033[0m')\nfor col in df_train.select_dtypes(include = 'object').columns:\n    df_train[col] = df_train[col].astype('category')\n\ndf_train['Is_Lead'] = df_train['Is_Lead'].astype('category')\ndf_train.describe(exclude='number') \ndf_train.describe()\n\nprint ('\\n\\033[7mDescription of Test dataset.\\033[0m')\nfor col in df_test.select_dtypes(include = 'object').columns:\n    df_test[col] = df_test[col].astype('category')\n\ndf_test.describe(exclude='number') \ndf_test.describe()\n","f15c488a":"InteractiveShell.ast_node_interactivity = 'last_expr'","3efef71d":"for col in df_train.select_dtypes(include = 'category'):\n    print(f\"\\n\\033[7mVariable\/attribute: {col.upper()} \\033[0m\\n\")\n    print(f\"{col.title()} attribute has following unique values\/levels:\\n{df_train[col].unique()}\\n\")\n    print(f\"Count of data for each level of '{col}' attribute: \\n{df_train[col].value_counts(1)}\")\n    \n    plt.figure(figsize=(12,4))\n    plt.subplot(1,2,1)\n    df_train[col].value_counts().plot(kind='pie')\n    plt.subplot(1,2,2)\n    df_train[col].value_counts().plot(kind='bar')\n    plt.title(f'Countplot of {col} variable')\n    plt.tight_layout()\n    plt.show();","26367a7a":"df_test.select_dtypes(include = 'category').head()\n","f7dff535":"InteractiveShell.ast_node_interactivity = 'all'","df518b81":"df_train[df_train['Credit_Product'] == 'Yes']['Is_Lead'].value_counts(1)\ndf_train[df_train['Credit_Product'] == 'No']['Is_Lead'].value_counts(1)\ndf_train[df_train['Credit_Product'].isnull()]['Is_Lead'].value_counts(1)","504f7467":"plt.figure(figsize = (16,8))\nsns.heatmap(df_train.isnull(), cbar = False, cmap = 'coolwarm', yticklabels = False)\nplt.show()","1603ab46":"df_train['Credit_Product'].value_counts()","a04d5242":"#val = df_train['Credit_Product'].mode()[0]\n#df_train['Credit_Product'] = df_train['Credit_Product'].fillna(value='Yes')\n#df_test['Credit_Product'] = df_test['Credit_Product'].fillna(value='Yes')\n","b20aa02b":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ncat_cols = ['Gender','Region_Code','Occupation','Channel_Code','Credit_Product','Is_Active']\nfor col in cat_cols:\n    df_train[col] = le.fit_transform(df_train[col])\n    df_test[col] = le.fit_transform(df_test[col])","8f3d0270":"df_train['Credit_Product'].value_counts()","600f7e98":"InteractiveShell.ast_node_interactivity = 'all'","e4140cbe":"df_train.head()\ndf_test.head()","bdf2a791":"# The below function plots a histogram and corresponding boxplot for n variables at a time.\ndef plot_graph(l, u, df):\n    i = 0\n    cols = df.select_dtypes(include=[np.number]).columns\n\n    if len(cols[l:u]) == 0:\n        return -1\n    else:\n        j = len(cols[l:u])\n\n    plt.figure(figsize=(5*j, 5))\n    f, axs = plt.subplots(2, j, sharex='col',  figsize=(5*j, 5),\n                          gridspec_kw={\"height_ratios\": (0.2, 1), 'wspace': (0.2), 'hspace': (0.00)})\n    #(ax_box1,ax_box2,ax_box3), (ax_hist1,ax_hist2,ax_hist3) = axs\n\n    for col in cols[l:u]:\n        mean = df[col].mean()\n        median = df[col].median()\n        mode = df[col].mode()[0]\n\n        if j == 1:\n            ax_box = axs[0]\n            ax_hist = axs[1]\n        else:\n            ax_box = axs[0, i]\n            ax_hist = axs[1, i]\n\n        sns.boxplot(df[col], ax=ax_box)\n        ax_box.axvline(mean, color='r', linestyle='--')\n        ax_box.axvline(median, color='g', linestyle='-')\n        ax_box.axvline(mode, color='b', linestyle='-')\n        ax_box.set_title(col, fontdict={'fontsize': 15, 'color': 'blue'})\n\n        sns.distplot(df[col], ax=ax_hist)\n        ax_hist.axvline(mean, color='r', linestyle='--')\n        ax_hist.axvline(median, color='g', linestyle='-')\n        ax_hist.axvline(mode, color='b', linestyle='-')\n        #ax_hist.set_title(col, fontdict={'fontsize': 15, 'color': 'blue'})\n        ax_hist.legend({'Mean': mean, 'Median': median, 'Mode': mode})\n        i += 1\n        ax_box.set(xlabel='')\n\n#    plt.tight_layout()\n    plt.show()\n\n\n# Call the plot_graph function for n variables at a time.\ndef plot_hist_box(df, n):\n    for i in range(0, 100, n):\n        x = plot_graph(i, i+n, df)\n        if x == -1:\n            break\n","85e9f5bb":"plot_hist_box(df_train, 3)","8149c279":"plt.figure(figsize=(10,6))\nsns.heatmap(df_train.corr().round(2),annot=True);","5740c113":"sns.pairplot(df_train,hue='Is_Lead')","431ab8e2":"# take backup of dataset\ndf_train_backup = df_train.copy()\ndf_test_backup = df_test.copy()\ndf_test_backup.head()","4c8a6388":"y = df_train['Is_Lead']\ndf_train = df_train.drop('Is_Lead', axis=1)\ndf_train['from_dataset'] = 'Train'\ndf_test['from_dataset'] = 'Test'","832dc02f":"df = pd.concat([df_train, df_test])\ndf","8abf721e":"InteractiveShell.ast_node_interactivity = 'last_expr'","bb41ea34":"plt.subplot(1, 2, 1)\n(df['Avg_Account_Balance']).plot.hist(bins=50, figsize=(12, 6), edgecolor = 'white')\nplt.xlabel('Avg_Account_Balance', fontsize=12)\nplt.title('Avg_Account_Balance Distribution', fontsize=12)\nplt.subplot(1, 2, 2)\nnp.log(df['Avg_Account_Balance']).plot.hist(bins=50, figsize=(12,6), edgecolor='white')\nplt.xlabel('log(Avg_Account_Balance+1)', fontsize=12)\nplt.title('Avg_Account_Balance Distribution', fontsize=12)\n","465f4db5":"df['Avg_Account_Balance'] = np.log(df['Avg_Account_Balance'])","82abbe86":"from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nimport scipy.cluster.hierarchy as shc\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nInteractiveShell.ast_node_interactivity = 'last_expr'","b2440bbb":"WSS = {}\nfor k in range(1,21):\n    Kmeans = KMeans(n_clusters=k,random_state=1)\n    Kmeans.fit(df.drop('from_dataset', axis=1))\n    WSS[k] = Kmeans.inertia_\nprint(\"\\033[1m\\033[43mListed below are the Within Sum of Squares(WSS) value for 1 to 10 clusters:\\033[0m\")\nWSS\n\n#plt.plot(list(WSS.keys()), list(WSS.values()))\nsns.pointplot(x = list(WSS.keys()), y = list(WSS.values()))\nplt.xlabel('K Numbers')\nplt.ylabel('WSS value (inertia)')\nplt.title('Within Sum of Squares (WSS) plot')\nplt.grid()\nplt.show()","5a5646c3":"k_means = KMeans(n_clusters=40, random_state=1)\nk_means.fit(df.drop('from_dataset', axis=1))\nk_labels = k_means.labels_\n\ndf['Kcluster'] = k_labels","68f53e8e":"df['Kcluster'].value_counts()\ndf['Kcluster'].value_counts(1)    ","f7467120":"plt.figure(figsize=(12,5))\nsns.boxplot(x='Kcluster', y='Avg_Account_Balance',data=df);\nplt.figure(figsize=(12,5))\nsns.countplot(x='Kcluster',data=df);","cabec25d":"df.columns, df.shape","ba95c485":"# df_train[\"CC_Mean_Acc_Bal\"] = df_train.groupby(['Channel_Code'])['Avg_Account_Balance'].transform('mean')\n# Mean_Acc_Bal_dict = df_train.groupby(['Channel_Code'])['Avg_Account_Balance'].mean().to_dict()\n# df_test['CC_Mean_Acc_Bal'] = df_test['Channel_Code'].apply(lambda x:Mean_Acc_Bal_dict.get(x,0))\n\n\ndf[\"Kcluster_Mean_Acc_Bal\"] = df.groupby(['Kcluster'])['Avg_Account_Balance'].transform('mean')\ndf[\"Kcluster_Max_Acc_Bal\"] = df.groupby(['Kcluster'])['Avg_Account_Balance'].transform('max')\n\ndf[\"Age_Mean_Acc_Bal\"] = df.groupby(['Age'])['Avg_Account_Balance'].transform('mean')\ndf[\"Age_Max_Acc_Bal\"] = df.groupby(['Age'])['Avg_Account_Balance'].transform('max')\n\ndf[\"Vin_Mean_Acc_Bal\"] = df.groupby(['Vintage'])['Avg_Account_Balance'].transform('mean')\ndf[\"Vin_Max_Acc_Bal\"] = df.groupby(['Vintage'])['Avg_Account_Balance'].transform('max')\n\ndf[\"KClus_RC_Mean_Acc_Bal\"] = df.groupby(['Kcluster','Region_Code'])['Avg_Account_Balance'].transform('mean')\ndf[\"KClus_RC_Max_Acc_Bal\"] = df.groupby(['Kcluster','Region_Code'])['Avg_Account_Balance'].transform('max')\n\ndf[\"KClus_Occ_Mean_Acc_Bal\"] = df.groupby(['Kcluster','Occupation'])['Avg_Account_Balance'].transform('mean')\ndf[\"KClus_Occ_Max_Acc_Bal\"] = df.groupby(['Kcluster','Occupation'])['Avg_Account_Balance'].transform('max')\n\ndf[\"KClus_CC_Mean_Acc_Bal\"] = df.groupby(['Kcluster','Channel_Code'])['Avg_Account_Balance'].transform('mean')\ndf[\"KClus_CC_Max_Acc_Bal\"] = df.groupby(['Kcluster','Channel_Code'])['Avg_Account_Balance'].transform('max')\n\n\ndf[\"All_Vin_Mean_Acc_Bal\"] = df.groupby(['Kcluster','Gender', 'Region_Code', 'Occupation', 'Channel_Code', 'Vintage', 'Credit_Product', 'Is_Active']\n                                   )['Avg_Account_Balance'].transform('mean')\ndf[\"All_Vin_Max_Acc_Bal\"] = df.groupby(['Kcluster','Gender', 'Region_Code', 'Occupation', 'Channel_Code', 'Vintage', 'Credit_Product', 'Is_Active']\n                                  )['Avg_Account_Balance'].transform('max')\n\ndf[\"All_Age_Mean_Acc_Bal\"] = df.groupby(['Kcluster','Gender', 'Region_Code', 'Occupation', 'Channel_Code', 'Age', 'Credit_Product', 'Is_Active']\n                                   )['Avg_Account_Balance'].transform('mean')\ndf[\"All_Age_Max_Acc_Bal\"] = df.groupby(['Kcluster','Gender', 'Region_Code', 'Occupation', 'Channel_Code', 'Age', 'Credit_Product', 'Is_Active']\n                                  )['Avg_Account_Balance'].transform('max')\n\ndf[\"All-Reg_Vin_Mean_Acc_Bal\"] = df.groupby(['Kcluster','Gender', 'Occupation', 'Channel_Code', 'Vintage', 'Credit_Product', 'Is_Active']\n                                   )['Avg_Account_Balance'].transform('mean')\ndf[\"All-Reg_Vin_Max_Acc_Bal\"] = df.groupby(['Kcluster','Gender', 'Occupation', 'Channel_Code', 'Vintage', 'Credit_Product', 'Is_Active']\n                                  )['Avg_Account_Balance'].transform('max')\n\ndf[\"All-Reg_Age_Mean_Acc_Bal\"] = df.groupby(['Kcluster','Gender', 'Occupation', 'Channel_Code', 'Age', 'Credit_Product', 'Is_Active']\n                                   )['Avg_Account_Balance'].transform('mean')\ndf[\"All-Reg_Age_Max_Acc_Bal\"] = df.groupby(['Kcluster', 'Gender', 'Occupation', 'Channel_Code', 'Age', 'Credit_Product', 'Is_Active']\n                                  )['Avg_Account_Balance'].transform('max')\n\ndf[\"Gen-Occ-Ch-CP-Act_Mean_Acc_Bal\"] = df.groupby(['Kcluster','Gender', 'Occupation', 'Channel_Code', 'Credit_Product', 'Is_Active']\n                                   )['Avg_Account_Balance'].transform('mean')\ndf[\"Gen-Occ-Ch-CP-Act_Max_Acc_Bal\"] = df.groupby(['Kcluster','Gender',  'Occupation', 'Channel_Code', 'Credit_Product', 'Is_Active']\n                                  )['Avg_Account_Balance'].transform('max')\n\ndf.head()","6c1157ae":"plot_hist_box(df, 4)","07790f02":"df_train = df[df['from_dataset'] == 'Train']\ndf_test = df[df['from_dataset'] == 'Test']\ndf_train['Is_Lead'] = y","29de377c":"df_train.drop('from_dataset',axis=1,inplace=True)\ndf_test.drop('from_dataset',axis=1,inplace=True)","4ef70e5a":"df_train.shape, df_test.shape\ndf_train.head()\ndf_test.head()","b6aeb68a":"plt.figure(figsize=(12,5))\nsns.boxplot(x='Kcluster', y='Avg_Account_Balance', hue='Is_Lead', data=df_train);\nplt.figure(figsize=(12,5))\nsns.countplot(x='Kcluster',hue='Is_Lead', data=df_train);","2ac59253":"#df_train = pd.read_csv('train_clean_features.csv')\n#df_test = pd.read_csv('test_clean_features.csv')\n#test_submission = pd.read_csv('submission_file_backup.csv')","2729c2f0":"InteractiveShell.ast_node_interactivity = 'all'","b2daeb3d":"X = df_train.drop(['Is_Lead'],axis=1)\ny = df_train[['Is_Lead']]\n\nX.head()\ny.head()","3ad6e455":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=51,stratify=df_train['Is_Lead'])\n\n\nprint(f'\\033[1mSpliting the Company dataset in Ratio 75:25. \\nShape of ---> \\033[0m\\n\\033[34mX_Train:{X_train.shape}, \\ny_train:{y_train.shape},\\\n      \\nX_Test:{X_test.shape}, \\ny_test:{y_test.shape}\\033[0m')\n","3af0bd06":"print(f\"\\n\\033[7mProportion of Target class in original dataset:\\033[0m\\n{df_train.Is_Lead.value_counts(1)}\")\nprint(f\"\\n\\033[7mProportion of Target class post split in y_test:\\033[0m\\n{y_test.Is_Lead.value_counts(1)}\")","d1e74b1b":"#Function to create a list of log from the existing model log excel file in working directory.\ndef create_perflog_list(perf_list, df):\n    cols = df.columns\n    for row in df.itertuples(index=False): \n        my_dict = {}\n        for i, col in enumerate(cols):\n            my_dict[col] = row[i]\n        perf_list.append(my_dict)\n\n    return perf_list\n\n# Load the Model parameters and performance metrics log file from current working directory, if not present a new one will be created.\nimport os\nfrom datetime import datetime\nwd = os.getcwd()\nfilename = 'model_perf_log.csv'\nfpath = wd + '\\\\' + filename #model log file name (which stores the model parameters and performance metrics)\nperf_list = []   #create an empty list.\nif os.path.isfile(fpath):\n    print(\"Opening Model Log File at:\\n\\033[7m{}\\033[0m\".format(fpath))\n    model_perf_log = pd.read_csv(filename)    #read existing log file into dataframe.\n    perf_list = create_perflog_list(perf_list, model_perf_log)\nelse:\n    print('The log file \\033[7m{}\\033[0m does not exist in current working directory. A new log file will be created.'.format(filename))\n    print('Current working directory is:\\n\\033[7m{}\\033[0m'.format(wd))","b6a65ad4":"def capture_parameters(Model,Predict_On,model_score,AUC,Recall,Precision,F1_Score,best_grid,params,perf_list):\n    grid_dict = {}\n    grid_dict['Model'] = Model\n    grid_dict['Predict_On'] = Predict_On\n    grid_dict['Accuracy_Score'] = round(model_score,4)\n    grid_dict['Precision'] = round(Precision,4)\n    grid_dict['Recall'] = round(Recall,4)\n    grid_dict['F1_Score'] = round(F1_Score,4)\n    grid_dict['AUC'] = round(AUC,4)\n    grid_dict['DateTimeStamp'] = str(datetime.now())\n    grid_dict['Best_Parameters'] = str(params)\n    grid_dict['Best_Grid'] = str(best_grid)\n    \n    perf_list.append(grid_dict)\n    df = pd.DataFrame(perf_list)   \n    return df, perf_list\n\ndef generate_model_performance(model_name,thres,predict_type,model,X,y,params,perf_list):\n    # Generate the y predicted for Train and Test.\n    ypred = model.predict(X)\n    if 'SM_Logit' in model_name:\n        ypred = sm_predict_y(model, thres, X)\n    else:\n        ypred = model.predict(X)\n\n    print(f\"\\033[7m\\nAccuracy score of {predict_type} Dataset:\\033[0m\")  # Accuracy Score\n    train_acc = accuracy_score(y, ypred)\n    print(f\"Accuracy Score of {predict_type} Dataset is: {round(train_acc,4)*100} %\")\n\n    print(f\"\\033[7m\\n\\nClassification Report of {predict_type} Dataset:\\033[0m\")  # Classification Reports:\n    train_precision, train_recall, train_f1 = classification(y, ypred)\n    \n    print(f\"\\033[7m\\n\\nConfusion Matrix of {predict_type} Dataset:\\033[0m\")  # Confusion Matrix\n    confusion(y, ypred)\n    \n    print(f\"\\033[7m\\n\\nAUC ROC Curve of {predict_type} Dataset:\\033[0m\") # AUC Score\n    train_auc = auc_roc(model_name,model,X,y)\n    print(f\"\\033[7mArea Under the Curve for {predict_type} Dataset is: {round(train_auc,4)*100} %\\033[0m\\n\")\n      \n    \n    model_perf_log, perf_list = capture_parameters(model_name,predict_type,train_acc,train_auc,train_recall,\n                                         train_precision,train_f1,model,params,perf_list)\n    model_perf_log.to_csv('model_perf_log.csv', index=False)\n    return model_perf_log, perf_list\n\ndef compare_model_performance(model_name,thres,model,X_train,y_train,X_test,y_test,params,perf_list):\n    # Generate the y predicted for Train and Test.\n\n    if 'SM_Logit' in model_name:\n        ypred_train = sm_predict_y(model, thres, X_train)\n        ypred_test = sm_predict_y(model, thres, X_test)\n    else:\n        ypred_train = model.predict(X_train)\n        ypred_test = model.predict(X_test)\n\n    print(f\"\\033[7m\\nAccuracy scores:\\033[0m\")  # Accuracy Score\n    train_acc = accuracy_score(y_train, ypred_train)\n    test_acc = accuracy_score(y_test, ypred_test)\n    print('Accuracy Score of Training Dataset:',round(train_acc,4)*100,'%')\n    print('Accuracy Score of Testing Dataset:',round(test_acc,4)*100,'%')\n\n#    print(f\"\\033[7m\\n\\nClassification Reports:\\033[0m\")  # Classification Reports:\n    print(f\"\\033[7m\\n\\nClassification Report of Training Dataset:\\033[0m\")\n    train_precision, train_recall, train_f1 = classification(y_train, ypred_train)\n    print(f\"\\033[7m\\nClassification Report of Testing Dataset:\\033[0m\")\n    test_precision, test_recall, test_f1 = classification(y_test, ypred_test)\n    \n#    print(f\"\\033[7m\\nConfusion Matrix Reports:\\033[0m\")  # Confusion Matrix\n    print(f\"\\033[7m\\n\\nConfusion Matrix of Training Dataset:\\033[0m\")\n    confusion(y_train, ypred_train)\n    print(f\"\\033[7m\\nConfusion Matrix of Testing Dataset:\\033[0m\")\n    confusion(y_test, ypred_test)\n    \n    print(f\"\\033[7m\\n\\nAUC ROC Curve of Training Dataset:\\033[0m\") # AUC Score\n    train_auc = auc_roc(model_name,model,X_train,y_train)\n    print('\\033[7mArea Under the Curve for Training Dataset',round(train_auc,4)*100,'%\\033[0m\\n')\n    print(f\"\\033[7m\\nAUC ROC Curve of Testing Dataset:\\033[0m\") # AUC Score\n    test_auc = auc_roc(model_name,model,X_test,y_test)\n    print('\\033[7mArea Under the Curve for Testing Dataset',round(test_auc,4)*100,'%\\033[0m\\n')\n      \n   \n    model_perf_log, perf_list = capture_parameters(model_name,'Training',train_acc,train_auc,train_recall,\n                                         train_precision,train_f1,model,params,perf_list)\n    model_perf_log, perf_list = capture_parameters(model_name,'Testing',test_acc,test_auc,test_recall,\n                                         test_precision,test_f1,model,params,perf_list)\n    model_perf_log.to_csv('model_perf_log.csv', index=False)\n    return model_perf_log, perf_list\n","050032fe":"def classification(y,ypred):\n    print(classification_report(y, ypred, target_names=['Non-Default', 'Default']))\n    metrics=classification_report(y, ypred, output_dict=True)\n    df=pd.DataFrame(metrics).transpose()\n    precision=df.loc[\"1\"][0]\n    recall=df.loc[\"1\"][1]\n    f1=df.loc[\"1\"][2]\n    \n    return precision, recall, f1\n\ndef confusion(y,ypred):\n    cm = confusion_matrix(y,ypred)\n    #print(cm)\n    classNames = ['Non-Default', 'Default']\n    cmlabel = np.array([['TN', 'FP'], ['FN','TP']])\n    labels = (np.asarray([\"{0} = {1:3d}\".format(cmlabel, cmvalue)\n                          for cmlabel, cmvalue in zip(cmlabel.flatten(), cm.ravel())])\n             ).reshape(2, 2)\n    #  'flatten' flattens the array and returns a copy, ravel returns a view of original array.\n        #print(labels)\n    sns.heatmap(cm, annot=labels, fmt='s', cbar=False,cmap='YlGnBu',\n              xticklabels=classNames, yticklabels=classNames)\n    plt.xlabel('Predicted Label')\n    plt.ylabel('Actual Label')\n    plt.title('Confusion Matrix')\n    plt.yticks(rotation=0)\n    plt.show()\n    tn, fp, fn, tp = cm.ravel()\n#    print('\\nTN={}, FP={}, FN={}, TP={}'.format(tn, fp, fn, tp))\n    print('\\nActual Positives={}, Predicted Positives={}, Actual Negative={}, Predicted Negative={}'.format(\n        (tp+fn),(tp+fp),(tn+fp),(tn+fn)))\n    print('\\nTotal number of observations = {}'.format(tn+fp+fn+tp))\n    \ndef auc_roc(model_name,model,X,y):\n    if 'SM_Logit' in model_name:\n        ypred_prob = model.predict(X)    \n    else:\n        ypred_prob = model.predict_proba(X)[:,1]\n        \n    auc = roc_auc_score(y,ypred_prob)\n    \n    fpr, tpr,_=roc_curve(y,ypred_prob)\n    plt.plot(fpr, tpr, marker='x', label='')\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC')\n    plt.show()\n    \n    return auc\n\n# Function to Predict y for Stats Model Logistic Regression prediction\ndef sm_predict_y(SM_model, thres, X):\n    y_predict_prob = pd.DataFrame(SM_model.predict(X))\n    y_class_pred= y_predict_prob.applymap(lambda x: 1 if x>thres else 0)\n    return y_class_pred","25cc65f5":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport statsmodels.formula.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve,\\\n                            classification_report, confusion_matrix, plot_confusion_matrix","c3d94214":"from sklearn.pipeline import Pipeline\npipelines = []\npipelines.append(('Logistic Regression', Pipeline([('LR',LogisticRegression())])))\npipelines.append(('LDA', Pipeline([('LDA',LinearDiscriminantAnalysis())])))\n#pipelines.append(('KNN Classifier', Pipeline([('KNN',KNeighborsClassifier())])))\n#pipelines.append(('Naive Bayes', Pipeline([('NB',GaussianNB())])))\n#pipelines.append(('Random Forest', Pipeline([('RF',RandomForestClassifier())])))\n#pipelines.append(('GB Classifier', Pipeline([('GB',GradientBoostingClassifier())])))\n#pipelines.append(('XGBoost Classifier', Pipeline([('XGB',XGBClassifier())])))\npipelines.append(('LightGBM Classifier', Pipeline([('LGBM',lgb.LGBMClassifier())])))\n#pipelines.append(('ADABoost Classifier', Pipeline([('ADB',AdaBoostClassifier())])))\n#pipelines.append(('Support Vector', Pipeline([('SVC',svm.SVC())])))\n\n#pipelines.append(('ScaledGBM', Pipeline([('Scaler', MinMaxScaler()),('GBM', GradientBoostingRegressor())])))\n\nresults = []\nnames = []\nfor name, model in pipelines:\n    cv_results = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n#    cv_results = cross_val_score(model, X1_sm, y1_sm, cv=3, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"\\033[7m%20s:\\033[0m \\033[44mMean Score=%f\\033[0m, \\033[43mstd=%.4f\\033[0m, \\033[46mmax=%.4f\\033[0m, \\033[47mmin=%.4f\\033[0m, \\033[45mvar=%.4f\\033[0m\"\\\n    % (name, cv_results.mean(), cv_results.std(), cv_results.max(),cv_results.min(), (cv_results.max()-cv_results.min()))\n    print(msg)","84da6a46":"results = []\nnames = []\nfor name, model in pipelines:\n#    kfold = KFold(n_splits=10, random_state=101)\n    cv_results = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"\\033[7m%20s:\\033[0m \\033[44mMean Score=%f\\033[0m, \\033[43mstd=%.4f\\033[0m, \\033[46mmax=%.4f\\033[0m, \\033[47mmin=%.4f\\033[0m, \\033[45mvar=%.4f\\033[0m\"\\\n    % (name, cv_results.mean(), cv_results.std(), cv_results.max(),cv_results.min(), (cv_results.max()-cv_results.min()))\n    print(msg)","e0ef9fda":"results = []\nnames = []\nfor name, model in pipelines:\n#    kfold = KFold(n_splits=10, random_state=101)\n    cv_results = cross_val_score(model, X, y, cv=5, scoring='roc_auc')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"\\033[7m%20s:\\033[0m \\033[44mMean Score=%f\\033[0m, \\033[43mstd=%.4f\\033[0m, \\033[46mmax=%.4f\\033[0m, \\033[47mmin=%.4f\\033[0m, \\033[45mvar=%.4f\\033[0m\"\\\n    % (name, cv_results.mean(), cv_results.std(), cv_results.max(),cv_results.min(), (cv_results.max()-cv_results.min()))\n    print(msg)","7b459bdc":"from sklearn.model_selection import GridSearchCV\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score","c4bf061a":"import lightgbm as lgb\nmodel_LGBMcl = lgb.LGBMClassifier(boosting_type='gbdt',learning_rate=0.025,\n                           n_estimators=10000,n_jobs=-1,max_depth=30, max_bin=100,\n                            num_leaves=10, reg_alpha=8.457, reg_lambda=6.853, subsample=0.75, #min_child_samples=50,\n                           random_state=1,silent=False,force_col_wise=True)\n\nmodel_LGBMcl.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=500,verbose=500)","2356a0f9":"LGBM_param = model_LGBMcl.get_params()\nprint(f\"\\033[7mBest Parameters of Light-GBM Model:\\033[0m\\n{LGBM_param}\")","008a5bd0":"print(f\"\\033[7m\\nAccuracy scores:\\033[0m\") \nprint('Base score of Light-GBM Model on Training Dataset:', model_LGBMcl.score(X_train, y_train).round(3))\nprint('Base score of Light-GBM Model on Testing Dataset:', model_LGBMcl.score(X_test, y_test).round(3))","1fb4553a":"x=pd.DataFrame(model_LGBMcl.feature_importances_,index=X_test.columns).sort_values(by=0,ascending=False)\nplt.figure(figsize=(12,8))\nsns.barplot(x[0],x.index,palette='rainbow')\nplt.ylabel('Feature Name')\nplt.xlabel('Feature Importance value')\nplt.title('Feature Importance Plot')\nplt.show();","7998b316":"model_perf_log, perf_list = compare_model_performance('LightGBM',0.5,model_LGBMcl,X_train, y_train,X_test,y_test,LGBM_param,perf_list)","5a497fec":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\ndef cross_val(X, y, model, params, folds=9):\n\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=21)\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        alg = model(**params)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=100,\n                verbose=400)\n\n        pred = alg.predict_proba(x_test)[:, 1]\n        roc_score = roc_auc_score(y_test, pred)\n        print(f\"roc_auc_score: {roc_score}\")\n        print(\"-\"*50)\n    \n    return alg","92eff6d5":"lgb_params= {'learning_rate': 0.025, \n             'n_estimators': 10000, \n             'max_bin': 100,\n             'num_leaves': 10, \n             'max_depth': 30, \n             'reg_alpha': 8.457, \n             'reg_lambda': 6.853, \n             'subsample': 0.75,\n            }","79a0722b":"from lightgbm import LGBMClassifier\nlgb_model = cross_val(X, y, LGBMClassifier, lgb_params)","f88afb2e":"x=pd.DataFrame(lgb_model.feature_importances_,index=X.columns).sort_values(by=0,ascending=False)\nplt.figure(figsize=(12,10))\nsns.barplot(x[0],x.index,palette='rainbow')\nplt.ylabel('Feature Name')\nplt.xlabel('Feature Importance value')\nplt.title('Feature Importance Plot')\nplt.show();","81a6ba14":"model = lgb_model  # lgb_model, model_LGBMcl","6db13e3b":"#model.fit(X, y)\nypred_lgb = model.predict_proba(df_test)[:,1]\ntest_submission['Is_Lead'] = ypred_lgb\ntest_submission.head()","6f060b53":"test_submission.to_csv('submission_file.csv',index=False)","f2bc57be":"## Light-GBM Model","1813c5d6":"## Create a Model Performance Log file to capture the Model performance output.","53f4baae":"### Split into Train-Test and write the datasets with new features to files","20dbfe61":"## Analysis of Null Values [Credit_Product] distribution of Target variable.","07b68431":"### Check the shape and datatype info of the data.","2d176dc8":"**Insights from above Correlation heatmap.**\n\n- Age variable has very high correlation with Channel code and Vintage. Which would mean higher age group people have more longer relation with Bank. Also Channel code X4 seems to have more people of higher age group compared to X1.\n\n- Vintage and Channel code also has high correlation.\n\n- The target variable (Is_Lead) has got high correlation with Age, Channel code and Vintage. Which means older people having more relationship are more likely to be interested for Credit card.\n\n- Also people already having credit product is having slightly more chance of opting for credit card.","88b46326":"### Model performance through pipeline Cross-validation using Original data.","14de2345":"## Train-Test Split\n\nSplit the data in Train-Test ration of 70:30, for both Linear model data and original data.","1bb0078d":"**There are no null values in data now.**","8d3ab724":"- **We observe there are no duplicate records in the dataset, each record belong to unique person (ID).**","655a0a4e":"### Pairplot.","c0042fa2":"## Save the datasets with all numeric features into file for future use.","ac84b7bf":"### Generate 4 Clusters using K-Means clustering and add to dataset.","ecdca82d":"## Combine Train-Test and do Feature Engineering.","68e6ed4f":"## Bivariate Analysis\n\n### Check the distribution of data and check for outliers.","dd7e26bd":"## Generate submission file.","ba510585":"## Create a pipeline of base models and use Cross validation to check performance.","2dc1c67c":"## LGBM Model with Pipeline Cross validation","3ccdf88c":"## Univariate Analysis\n\n### Inspect the Categorical data.","a5f8c203":"**We observe that in both Train and Test there are around 12% null values in Credit_Product variable.**","355b61c2":"### Check for duplicate records in the dataset.","dc2f9dd4":"## Load the given survey dataset and perform basic Exploratory data analysis.","a91d5403":"# Problem Statement - Credit Card Lead Prediction\n\nHappy Customer Bank is a mid-sized private bank that deals in all kinds of banking products, like Savings accounts, Current accounts, investment products, credit products, among other offerings.\n\n\n\nThe bank also cross-sells products to its existing customers and to do so they use different kinds of communication like tele-calling, e-mails, recommendations on net banking, mobile banking, etc. \n\n\n\nIn this case, the Happy Customer Bank wants to cross sell its credit cards to its existing customers. The bank has identified a set of customers that are eligible for taking these credit cards.\n\n\n\nNow, the bank is looking for our help in identifying customers that could show higher intent towards a recommended credit card, given:\n\n- Customer details (gender, age, region etc.)\n- Details of his\/her relationship with the bank (Channel_Code,Vintage, 'Avg_Asset_Value etc.)","1cdcf5ea":"### Correlation Heatmap.","4ec81f58":"### Data Dictionary.\n\nCheck the data definition of each features in given dataset.","37a57d0e":"**Insights from above histograms.**\n- All the features are categorical in nature except the Avg_Account_Balance. \n- Avg Account Balance distribution is right skewed and has lot of outliers. \n- Also the scale of the data is in very high, which will cause problem for distance based algorithms like KNN, SVM etc. We can try using log transformation for this feature.","b6ad8451":"## Clustering","8680c286":"### Drop the ID column from the datasets and check Data description.","1d6724a6":"## Log Transform the Avg Account Balance variable.","ff6acb97":"## Label Encode all Categorical columns to numerical columns.","f81f3749":"**We can see that column Credit_Product has got null values**\n- We first imputed null with Mode and ROC_AUC score did not improve from 0.78. hence now we will impute Credit Product missing value as 1 and check results. \n- After imputing missing value of Credit Product as 1 Roc_Auc improved to 85%.\n- Since the distribution of Target column 'Is_Lead' is different(15%) compared to 31.5% for Yes and 7% for No, we will impute the null value as separate category and check performance which comes around 87%.","653a8918":"<p style=\"page-break-after:always;\"><\/p>\n<div style=\"page-break-after: always;\"><\/div>","d30987df":"### Fit model on full Train data and generate submission file.","3423fed3":"### Create Mean, Max, Min of Avg_Account_Balance column based on Channel Code, Occupation, and\/or Age, Region.","75c71aa9":"**Above features are non-numeric, we will need to convert them to numeric datatype.**\n\n**First we will convert these using lable encoding or ordinal encoding and check the model performance. Then we will gradually convert some of these as One-Hot encoding and see if there is improvement in performance.**\n\n- Gender will be converted as a binary data\n- For Occupation we will use label encoding.\n- For Channel Code we will use label encoding.\n- For Credit Product we will do a binary encoding\n- For Is Active we will do a binary encoding.\n- For Region Codes we will do a label encoding. Since this has too many levels doing one hot encoding will create too many features.\n\n**Later we will try doing One-hot coding for Occupation, Channel code.**","58ab3088":"<p style=\"page-break-after:always;\"><\/p>\n<div style=\"page-break-after: always;\"><\/div>","2d1aaa17":"- **Without the ID column we can see there are 23 Train records which are duplicate. Which is possible as most of the feature except ID are generic and can be same for multiple people. Hence we will not remove any duplicate records in this case.**","16555d02":"- **Credit_Product column has null values in both Train and Test dataset.**\n- **Age, Vintage, Avg_Account_Balance and Is_Lead feature are integer type variable, which is as expected. All other variables are of object type.**","5a366496":"**Insights from the above pairplot.**\n\n- We see similar insights as observed above in correlation heatmat.\n- Higher age group people and Vintage customer are more interested for credit card.\n- People in channel code X2 and X3 are more interested in credit card.\n- People having higher average account balance is more likely to be interested in credit card.\n- Female are less likely to be interested in credit card.","8ff5e35a":"### Load the given Train and Test dataset and check the data header.","43f37689":"### Check for any null value in the dataset.","06334ce3":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement---Credit-Card-Lead-Prediction\" data-toc-modified-id=\"Problem-Statement---Credit-Card-Lead-Prediction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Problem Statement - Credit Card Lead Prediction<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Load-the-given-survey-dataset-and-perform-basic-Exploratory-data-analysis.\" data-toc-modified-id=\"Load-the-given-survey-dataset-and-perform-basic-Exploratory-data-analysis.-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Load the given survey dataset and perform basic Exploratory data analysis.<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Load-the-given-Train-and-Test-dataset-and-check-the-data-header.\" data-toc-modified-id=\"Load-the-given-Train-and-Test-dataset-and-check-the-data-header.-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;<\/span>Load the given Train and Test dataset and check the data header.<\/a><\/span><\/li><li><span><a href=\"#Data-Dictionary.\" data-toc-modified-id=\"Data-Dictionary.-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;<\/span>Data Dictionary.<\/a><\/span><\/li><li><span><a href=\"#Check-the-shape-and-datatype-info-of-the-data.\" data-toc-modified-id=\"Check-the-shape-and-datatype-info-of-the-data.-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;<\/span>Check the shape and datatype info of the data.<\/a><\/span><\/li><li><span><a href=\"#Check-for-any-null-value-in-the-dataset.\" data-toc-modified-id=\"Check-for-any-null-value-in-the-dataset.-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;<\/span>Check for any null value in the dataset.<\/a><\/span><\/li><li><span><a href=\"#Check-for-duplicate-records-in-the-dataset.\" data-toc-modified-id=\"Check-for-duplicate-records-in-the-dataset.-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;<\/span>Check for duplicate records in the dataset.<\/a><\/span><\/li><li><span><a href=\"#Drop-the-ID-column-from-the-datasets-and-check-Data-description.\" data-toc-modified-id=\"Drop-the-ID-column-from-the-datasets-and-check-Data-description.-1.1.6\"><span class=\"toc-item-num\">1.1.6&nbsp;&nbsp;<\/span>Drop the ID column from the datasets and check Data description.<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Univariate-Analysis\" data-toc-modified-id=\"Univariate-Analysis-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Univariate Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Inspect-the-Categorical-data.\" data-toc-modified-id=\"Inspect-the-Categorical-data.-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;<\/span>Inspect the Categorical data.<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Analysis-of-Null-Values-[Credit_Product]-distribution-of-Target-variable.\" data-toc-modified-id=\"Analysis-of-Null-Values-[Credit_Product]-distribution-of-Target-variable.-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;<\/span>Analysis of Null Values [Credit_Product] distribution of Target variable.<\/a><\/span><\/li><li><span><a href=\"#Label-Encode-all-Categorical-columns-to-numerical-columns.\" data-toc-modified-id=\"Label-Encode-all-Categorical-columns-to-numerical-columns.-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;<\/span>Label Encode all Categorical columns to numerical columns.<\/a><\/span><\/li><li><span><a href=\"#Bivariate-Analysis\" data-toc-modified-id=\"Bivariate-Analysis-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;<\/span>Bivariate Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Check-the-distribution-of-data-and-check-for-outliers.\" data-toc-modified-id=\"Check-the-distribution-of-data-and-check-for-outliers.-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;<\/span>Check the distribution of data and check for outliers.<\/a><\/span><\/li><li><span><a href=\"#Correlation-Heatmap.\" data-toc-modified-id=\"Correlation-Heatmap.-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;<\/span>Correlation Heatmap.<\/a><\/span><\/li><li><span><a href=\"#Pairplot.\" data-toc-modified-id=\"Pairplot.-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;<\/span>Pairplot.<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Save-the-datasets-with-all-numeric-features-into-file-for-future-use.\" data-toc-modified-id=\"Save-the-datasets-with-all-numeric-features-into-file-for-future-use.-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;<\/span>Save the datasets with all numeric features into file for future use.<\/a><\/span><\/li><li><span><a href=\"#Combine-Train-Test-and-do-Feature-Engineering.\" data-toc-modified-id=\"Combine-Train-Test-and-do-Feature-Engineering.-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;<\/span>Combine Train-Test and do Feature Engineering.<\/a><\/span><\/li><li><span><a href=\"#Log-Transform-the-Avg-Account-Balance-variable.\" data-toc-modified-id=\"Log-Transform-the-Avg-Account-Balance-variable.-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;<\/span>Log Transform the Avg Account Balance variable.<\/a><\/span><\/li><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;<\/span>Clustering<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Generate-4-Clusters-using-K-Means-clustering-and-add-to-dataset.\" data-toc-modified-id=\"Generate-4-Clusters-using-K-Means-clustering-and-add-to-dataset.-1.9.1\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;<\/span>Generate 4 Clusters using K-Means clustering and add to dataset.<\/a><\/span><\/li><li><span><a href=\"#Create-Mean,-Max,-Min-of-Avg_Account_Balance-column-based-on-Channel-Code,-Occupation,-and\/or-Age,-Region.\" data-toc-modified-id=\"Create-Mean,-Max,-Min-of-Avg_Account_Balance-column-based-on-Channel-Code,-Occupation,-and\/or-Age,-Region.-1.9.2\"><span class=\"toc-item-num\">1.9.2&nbsp;&nbsp;<\/span>Create Mean, Max, Min of Avg_Account_Balance column based on Channel Code, Occupation, and\/or Age, Region.<\/a><\/span><\/li><li><span><a href=\"#Split-into-Train-Test-and-write-the-datasets-with-new-features-to-files\" data-toc-modified-id=\"Split-into-Train-Test-and-write-the-datasets-with-new-features-to-files-1.9.3\"><span class=\"toc-item-num\">1.9.3&nbsp;&nbsp;<\/span>Split into Train-Test and write the datasets with new features to files<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Train-Test-Split\" data-toc-modified-id=\"Train-Test-Split-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;<\/span>Train-Test Split<\/a><\/span><\/li><li><span><a href=\"#Create-SMOTE-oversampled-dataset---Skipping-as-it-didn't-improve-Auc-score.\" data-toc-modified-id=\"Create-SMOTE-oversampled-dataset---Skipping-as-it-didn't-improve-Auc-score.-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;<\/span>Create SMOTE oversampled dataset - Skipping as it didn't improve Auc score.<\/a><\/span><\/li><li><span><a href=\"#Create-a-Model-Performance-Log-file-to-capture-the-Model-performance-output.\" data-toc-modified-id=\"Create-a-Model-Performance-Log-file-to-capture-the-Model-performance-output.-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;<\/span>Create a Model Performance Log file to capture the Model performance output.<\/a><\/span><\/li><li><span><a href=\"#Create-a-pipeline-of-base-models-and-use-Cross-validation-to-check-performance.\" data-toc-modified-id=\"Create-a-pipeline-of-base-models-and-use-Cross-validation-to-check-performance.-1.13\"><span class=\"toc-item-num\">1.13&nbsp;&nbsp;<\/span>Create a pipeline of base models and use Cross validation to check performance.<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-performance-through-pipeline-Cross-validation-using-Original-data.\" data-toc-modified-id=\"Model-performance-through-pipeline-Cross-validation-using-Original-data.-1.13.1\"><span class=\"toc-item-num\">1.13.1&nbsp;&nbsp;<\/span>Model performance through pipeline Cross-validation using Original data.<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Light-GBM-Model\" data-toc-modified-id=\"Light-GBM-Model-1.14\"><span class=\"toc-item-num\">1.14&nbsp;&nbsp;<\/span>Light-GBM Model<\/a><\/span><\/li><li><span><a href=\"#Drop-some-Not-useful-features\" data-toc-modified-id=\"Drop-some-Not-useful-features-1.15\"><span class=\"toc-item-num\">1.15&nbsp;&nbsp;<\/span>Drop some Not useful features<\/a><\/span><\/li><li><span><a href=\"#LGBM-Model-with-Pipeline-Cross-validation\" data-toc-modified-id=\"LGBM-Model-with-Pipeline-Cross-validation-1.16\"><span class=\"toc-item-num\">1.16&nbsp;&nbsp;<\/span>LGBM Model with Pipeline Cross validation<\/a><\/span><\/li><li><span><a href=\"#Generate-submission-file.\" data-toc-modified-id=\"Generate-submission-file.-1.17\"><span class=\"toc-item-num\">1.17&nbsp;&nbsp;<\/span>Generate submission file.<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Fit-model-on-full-Train-data-and-generate-submission-file.\" data-toc-modified-id=\"Fit-model-on-full-Train-data-and-generate-submission-file.-1.17.1\"><span class=\"toc-item-num\">1.17.1&nbsp;&nbsp;<\/span>Fit model on full Train data and generate submission file.<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/div>","374f30a7":"\n**<center><span style=\"font-family: Arial; color:blue; font-size:2.0em; background-color:;\">JOB-A-THON - May 2021<\/span><\/center>**\n\n<center><span style=\"font-family: verdana; color:blue; font-size:150%; background-color:yellow\">Submitted By: Abhinava Kumar Saha<\/span><\/center>\n\n\n<center><span style=\"color:black; font-size:100%\">Date: May-30, 2021<\/span><\/center>"}}