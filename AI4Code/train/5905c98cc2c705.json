{"cell_type":{"c2a4e2ae":"code","9996bbde":"code","14be0adb":"code","4b797e5d":"code","5c605523":"code","8ef9a2c3":"code","8b0a4f33":"code","0b3284f1":"code","a6de2e39":"code","95c480ee":"code","0f496d83":"code","205ce384":"code","8b1243f9":"code","65e1bc10":"code","8a6df737":"code","ff29bd9f":"code","7a6b66d9":"markdown","274c6e2f":"markdown","d6cb681d":"markdown","6151c175":"markdown","0fbfacbd":"markdown","791aeea5":"markdown"},"source":{"c2a4e2ae":"# Set up code checking\nimport os\nif not os.path.exists(\"..\/input\/train.csv\"):\n    os.symlink(\"..\/input\/home-data-for-ml-course\/train.csv\", \"..\/input\/train.csv\")  \n    os.symlink(\"..\/input\/home-data-for-ml-course\/test.csv\", \"..\/input\/test.csv\") \nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ml_intermediate.ex6 import *\nprint(\"Setup Complete\")","9996bbde":"import pandas as pd\nfrom sklearn.model_selection import train_test_split \nimport numpy as np\n\nX = pd.read_csv('..\/input\/train.csv', index_col='Id') \nX_test_full = pd.read_csv('..\/input\/test.csv', index_col='Id')\n\nX.dropna(axis=0, subset=['SalePrice'], inplace=True) \ny = X.SalePrice \nX.drop(['SalePrice'], axis=1, inplace=True) \n\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0) \nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\nmy_cols = low_cardinality_cols + numeric_cols \nX_train = X_train_full[my_cols].copy() \nX_valid = X_valid_full[my_cols].copy() \nX_test = X_test_full[my_cols].copy() \n\nX_train = pd.get_dummies(X_train) \nX_valid = pd.get_dummies(X_valid) \nX_test = pd.get_dummies(X_test) \nX_train, X_valid = X_train.align(X_valid, join='left', axis=1) \nX_train, X_test = X_train.align(X_test, join='left', axis=1)\n","14be0adb":"print(X_train.shape)","4b797e5d":"from xgboost import XGBRegressor\n\n# Define the model\nmy_model_1 = XGBRegressor(random_state = 0)\n\n# Fit the model\nmy_model_1.fit(X_train, y_train)","5c605523":"from sklearn.metrics import mean_absolute_error\n\n# Get predictions\npredictions_1 = my_model_1.predict(X_valid)","8ef9a2c3":"# Calculate MAE\nmae_1 = mean_absolute_error(predictions_1, y_valid)\n\n\nprint(\"Mean Absolute Error:\" , mae_1)","8b0a4f33":"X_train.isnull().sum()","0b3284f1":"X_valid.isnull().sum()","a6de2e39":"X_test.isnull().sum()","95c480ee":"X_train.head()","0f496d83":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputed_X_train = imputer.fit_transform(X_train)\nimputed_X_valid = imputer.transform(X_valid)\nimputed_X_test = imputer.transform(X_test)","205ce384":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nimputed_X_train = sc.fit_transform(imputed_X_train)\nimputed_X_valid = sc.transform(imputed_X_valid)\nimputed_X_test = sc.transform(imputed_X_test)","8b1243f9":"from tensorflow import keras \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel = keras.Sequential([ \nlayers.Dense(1024, activation='relu', input_shape=[227]), \nlayers.Dropout(rate = 0.4),\nlayers.Dense(1024, activation='relu'),\nlayers.Dropout(rate = 0.4),\nlayers.Dense(1024, activation='relu'), \nlayers.Dropout(rate = 0.4),\nlayers.Dense(1), \n])\n\nearly_stopping = EarlyStopping(min_delta = 0.001,\n                               patience = 30,\n                               restore_best_weights = True)","65e1bc10":"model.compile( \n    optimizer='adam', \n    loss='mae', )","8a6df737":"prediction = model.fit( \n    imputed_X_train, y_train, \n    validation_data=(imputed_X_valid, y_valid), \n    batch_size=256, \n    epochs=1000,\n    callbacks = [early_stopping],\n    verbose = 2,\n)","ff29bd9f":"output = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': prediction})\noutput.to_csv('submission.csv', index=False)","7a6b66d9":"I maintained the exercise preprocessing.","274c6e2f":"Now we tune the models. We tried LightGBM but XGB works better. It took a lot of iterating to get here.","d6cb681d":"Impute value in order to get ourselves rid of missing values. We will use the mean of the values we do have in place of the ones we don\u2019t.  ","6151c175":"It has been a huge learning lesson for me I have been several days at it. Hope you like it and get something out of it.","0fbfacbd":"Check for missing values","791aeea5":"Exploratory first model. Cool to see the difference between default and after hyperparameters turning."}}