{"cell_type":{"12924fcf":"code","e4d4cb65":"code","5d4a6f95":"code","5f508d20":"code","8477181e":"code","32873019":"code","4d803ad7":"code","72c6327e":"code","6dc36a03":"code","9622d7a2":"code","13f2d4b2":"code","266da1e4":"code","0792a74e":"code","b34e57d2":"code","f3e09eab":"code","defb7e3a":"code","fb3fea4a":"code","e331cc69":"code","f2f1d8bd":"code","523d3cc0":"code","15712331":"code","3dccaeb9":"code","458ecb32":"code","b68a056c":"code","1b86a523":"code","0e3e8eca":"code","a7f68eb3":"code","a3be513a":"code","953ced09":"code","7ae63a3e":"code","46de6f03":"code","8d405ffd":"code","c57a8c8a":"code","1b923ef2":"code","d6296220":"code","63346b59":"code","c926d96c":"code","fe3f10be":"code","1e76c1d2":"code","48c5c1fa":"code","87e8633a":"code","004cc4ad":"code","d1bd2e6e":"code","a3c95bf8":"code","e3ccf3fc":"code","f2acdf88":"code","ad530487":"code","fb01f2f1":"code","27e89d2a":"code","afa51496":"code","93714f39":"code","b0254a4c":"code","179b97c0":"code","ad689311":"code","82d10108":"code","13670daf":"code","aea62a7d":"code","eda5b269":"code","d9c43f0f":"code","d8079ada":"code","95633799":"code","4106a6be":"code","bd178b53":"code","5484ceaa":"code","bd0f5070":"code","6f12e23b":"code","30d4d59b":"code","0628504a":"markdown","966056e4":"markdown","cba53f12":"markdown","8dda9234":"markdown","c5555963":"markdown","b0a49666":"markdown","edd5571d":"markdown","69c6c148":"markdown","924c4bd6":"markdown","1229c021":"markdown","13846fb4":"markdown","afe155a1":"markdown","80a062e8":"markdown","0f8fd6ae":"markdown","0077636c":"markdown","a66a41d3":"markdown","831d34bf":"markdown","f338f867":"markdown","1adf3e61":"markdown","f537cbf9":"markdown","b568945d":"markdown","55259078":"markdown","0091d819":"markdown","910c0302":"markdown","b4dca3b8":"markdown","d358838c":"markdown","c5575c67":"markdown","09818a65":"markdown","cf021942":"markdown","304c3ada":"markdown","c71ff05f":"markdown","4f6ff654":"markdown","2635b775":"markdown","7e649542":"markdown","65db91d4":"markdown","780421d0":"markdown","842b29f5":"markdown","c43868c1":"markdown","89d31bfa":"markdown","c6dc3058":"markdown"},"source":{"12924fcf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e4d4cb65":"# Vers\u00e3o da Linguagem Python\nfrom platform import python_version\nprint('Vers\u00e3o da Linguagem Python Usada Neste Jupyter Notebook:', python_version())","5d4a6f95":"# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n# pip install -U nome_pacote\n\n# Para instalar a vers\u00e3o exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n# pip install nome_pacote==vers\u00e3o_desejada\n\n# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.","5f508d20":"# Instala o pacote watermark. \n# Esse pacote \u00e9 usado para gravar as vers\u00f5es de outros pacotes usados neste jupyter notebook.\n!pip install -q -U watermark","8477181e":"# Por enquanto precisaremos somente do NumPy\nimport numpy as np","32873019":"# Vers\u00f5es dos pacotes usados neste jupyter notebook\n%reload_ext watermark\n%watermark --iversions","4d803ad7":"# Fun\u00e7\u00e3o para inicializa\u00e7\u00e3o rand\u00f4mica dos par\u00e2metros do modelo\ndef inicializa_parametros(dims_camada_entrada):\n    \n    # Dicion\u00e1rio para os par\u00e2metros\n    parameters = {}\n    \n    # Comprimento das dimens\u00f5es das camadas\n    comp = len(dims_camada_entrada)\n    \n    # Loop pelo comprimento\n    for i in range(1, comp):\n        \n        # Inicializa\u00e7\u00e3o da matriz de pesos, considerando um limite inferior e superior para encontrar um valor randomico.\n        # Utilizando valores de uma distribui\u00e7\u00e3o normal (randn)\n        parameters[\"W\" + str(i)] = np.random.randn(dims_camada_entrada[i], dims_camada_entrada[i - 1]) * 0.01\n        \n        # Inicializa\u00e7\u00e3o do bias\n        parameters[\"b\" + str(i)] = np.zeros((dims_camada_entrada[i], 1))\n    \n    return parameters","72c6327e":"# Fun\u00e7\u00e3o sigm\u00f3ide\n# Entrega valores entre 0 e 1.\ndef sigmoid(Z):\n    A = 1 \/ (1 + np.exp(-Z))\n    return A, Z","6dc36a03":"# Fun\u00e7\u00e3o de ativa\u00e7\u00e3o ReLu (Rectified Linear Unit)\n# Parametros de entrada s\u00e3o multiplacados pelos Pesos (pr\u00e9-definidos) que gera uma sa\u00edda multiplicada pelo Bias que \u00e9 o \n# parametro de entrada da fun\u00e7\u00e3o ReLu.\ndef relu(Z):\n    A = abs(Z * (Z > 0))\n    return A, Z","9622d7a2":"# Opera\u00e7\u00e3o de ativa\u00e7\u00e3o\n# A \u00e9 a matriz com os dados de entrada\n# W \u00e9 a matriz de pesos\n# b \u00e9 o bias\ndef linear_activation(A, W, b):\n    Z = np.dot(W, A) + b\n    cache = (A, W, b)\n    return Z, cache\n\n# O resultado da fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u00e9 a entrada da fun\u00e7\u00e3o Relu ou Sigmoid","13f2d4b2":"# Movimento para frente (forward)\ndef forward(A_prev, W, b, activation):\n    \n    # Se a fun\u00e7\u00e3o de ativa\u00e7\u00e3o for Sigmoid, entramos neste bloco\n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_activation(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n        \n    # Se n\u00e3o, se for ReLu, entramos neste bloco    \n    elif activation == \"relu\":\n        Z, linear_cache = linear_activation(A_prev, W, b)\n        A, activation_cache = relu(Z)\n        \n    cache = (linear_cache, activation_cache)\n    \n    return A, cache","266da1e4":"# Propaga\u00e7\u00e3o para frente\ndef forward_propagation(X, parameters):\n    \n    # Lista de valores anteriores (cache)\n    caches = []\n    \n    # Dados de entrada\n    A = X\n    \n    # Comprimento dos par\u00e2metros\n    L = len(parameters) \/\/ 2\n   \n    # Loop\n    for i in range(1, L):\n      \n        # Guarda o valor pr\u00e9vio de A\n        A_prev = A\n        \n        # Executa o forward\n        A, cache = forward(A_prev, parameters[\"W\" + str(i)], parameters[\"b\" + str(i)], activation = \"relu\")\n        \n        # Grava o cache\n        caches.append(cache)\n    \n    # Sa\u00edda na \u00faltima camada\n    A_last, cache = forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = \"sigmoid\")\n    \n    # Grava o cache\n    caches.append(cache)\n    \n    return(A_last, caches)","0792a74e":"# Fun\u00e7\u00e3o de custo (ou fun\u00e7\u00e3o de erro)\ndef calcula_custo(A_last, Y):\n    \n    # Ajusta o shape de Y para obter seu comprimento (total de elementos)\n    m = Y.shape[1]\n    \n    # Calcula o custo comparando valor real e previso\n    custo = (-1 \/ m) * np.sum((Y * np.log(A_last)) + ((1 - Y) * np.log(1 - A_last)))\n    \n    # Ajusta o shape do custo\n    custo = np.squeeze(custo)\n    \n    return(custo)","b34e57d2":"# Fun\u00e7\u00e3o sigmoid para o backpropagation \n# Fazemos o c\u00e1lculo da derivada pois n\u00e3o queremos o valor completo da fun\u00e7\u00e3o, mas sim sua varia\u00e7\u00e3o\ndef sigmoid_backward(da, Z):\n    \n    # Calculamos a derivada de Z\n    dg = (1 \/ (1 + np.exp(-Z))) * (1 - (1 \/ (1 + np.exp(-Z))))\n    \n    # Encontramos a mudan\u00e7a na derivada de z\n    dz = da * dg\n    return dz\n\n# Compare com a fun\u00e7\u00e3o sigmoid do forward propagation\n# A = 1 \/ (1 + np.exp(-Z))","f3e09eab":"# Fun\u00e7\u00e3o relu para o backpropagation \n# Fazemos o c\u00e1lculo da derivada pois n\u00e3o queremos o valor completo da fun\u00e7\u00e3o, mas sim sua varia\u00e7\u00e3o\ndef relu_backward(da, Z):\n    \n    dg = 1 * ( Z >= 0)\n    dz = da * dg\n    return dz\n\n# Compare com a fun\u00e7\u00e3o relu do forward propagation:\n# A = abs(Z * (Z > 0))","defb7e3a":"# Ativa\u00e7\u00e3o linear para o backpropagation\ndef linear_backward_function(dz, cache):\n    \n    # Recebe os valores do cache (mem\u00f3ria)\n    A_prev, W, b = cache\n    \n    # Shape de m\n    m = A_prev.shape[1]\n    \n    # Calcula a derivada de W (resultado da opera\u00e7\u00e3o com dz)\n    dW = (1 \/ m) * np.dot(dz, A_prev.T)\n    \n    # Calcula a derivada de b (resultado da opera\u00e7\u00e3o com dz)\n    db = (1 \/ m) * np.sum(dz, axis = 1, keepdims = True)\n    \n    # Calcula a derivada da opera\u00e7\u00e3o\n    dA_prev = np.dot(W.T, dz)\n    \n    return dA_prev, dW, db","fb3fea4a":"# Fun\u00e7\u00e3o que define o tipo de ativa\u00e7\u00e3o (relu ou sigmoid)\ndef linear_activation_backward(dA, cache, activation):\n    \n    # Extrai o cache\n    linear_cache, activation_cache = cache\n    \n    # Verifica se a ativa\u00e7\u00e3o \u00e9 relu\n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward_function(dZ, linear_cache)\n        \n    # Verifica se a ativa\u00e7\u00e3o \u00e9 sigmoid\n    if activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward_function(dZ, linear_cache)\n        \n    return dA_prev, dW, db","e331cc69":"# Algoritmo Backpropagation (calcula os gradientes para atualiza\u00e7\u00e3o dos pesos)\n# AL = Valor previsto no Forward\n# Y = Valor real\ndef backward_propagation(AL, Y, caches):\n    \n    # Dicion\u00e1rio para os gradientes\n    grads = {}\n    \n    # Comprimento dos dados (que est\u00e3o no cache)\n    L = len(caches)\n    \n    # Extrai o comprimento para o valor de m\n    m = AL.shape[1]\n    \n    # Ajusta o shape de Y\n    Y = Y.reshape(AL.shape)\n    \n    # Calcula a derivada da previs\u00e3o final da rede (feita ao final do Forward Propagation)\n    dAL = -((Y \/ AL) - ((1 - Y) \/ (1 - AL)))\n    \n    # Captura o valor corrente do cache\n    current_cache = caches[L - 1]\n    \n    # Gera a lista de gradiente para os dados, os pesos e o bias\n    # Fazemos isso uma vez, pois estamos na parte final da rede, iniciando o caminho de volta\n    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n    \n    # Loop para calcular a derivada durante as ativa\u00e7\u00f5es lineares com a relu\n    for l in reversed(range(L - 1)):\n        \n        # Cache atual\n        current_cache = caches[l]\n        \n        # Calcula as derivadas\n        dA_prev, dW, db = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n        \n        # Alimenta os gradientes na lista, usando o \u00edndice respectivo\n        grads[\"dA\" + str(l)] = dA_prev\n        grads[\"dW\" + str(l + 1)] = dW\n        grads[\"db\" + str(l + 1)] = db\n        \n    return grads","f2f1d8bd":"# Fun\u00e7\u00e3o de atualiza\u00e7\u00e3o de pesos\ndef atualiza_pesos(parameters, grads, learning_rate):\n    \n    # Comprimento da estrutura de dados com os par\u00e2metros (pesos e bias)\n    L = len(parameters)\/\/2\n    \n    # Loop para atualiza\u00e7\u00e3o dos pesos\n    for l in range(L):\n        \n        # Atualiza\u00e7\u00e3o dos pesos\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - (learning_rate * grads[\"dW\" + str(l + 1)])\n        \n        # Atualiza\u00e7\u00e3o do bias\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - (learning_rate * grads[\"db\" + str(l + 1)])\n    \n    return parameters","523d3cc0":"# Modelo completo da rede neural\ndef modeloNN(X, Y, dims_camada_entrada, learning_rate = 0.0075, num_iterations = 100):\n    \n    # Lista para receber o custo a cada \u00e9poca de treinamento\n    custos = []\n    \n    # Inicializa os par\u00e2metros\n    parametros = inicializa_parametros(dims_camada_entrada)\n    \n    # Loop pelo n\u00famero de itera\u00e7\u00f5es (\u00e9pocas)\n    for i in range(num_iterations):\n        \n        # Forward Propagation\n        AL, caches = forward_propagation(X, parametros)\n        \n        # Calcula o custo\n        custo = calcula_custo(AL, Y)\n        \n        # Backward Propagation\n        # Nota: ao inv\u00e9s de AL e Y, poder\u00edamos passar somente o valor do custo\n        # Estamos passando o valor de AL e Y para fique claro didaticamente o que est\u00e1 sendo feito\n        gradientes = backward_propagation(AL, Y, caches)\n        \n        # Atualiza os pesos\n        parametros = atualiza_pesos(parametros, gradientes, learning_rate)\n        \n        # Print do valor intermedi\u00e1rio do custo\n        # A redu\u00e7\u00e3o do custo indica o aprendizado do modelo\n        if i % 10 == 0:\n            print(\"Custo Ap\u00f3s \" + str(i) + \" itera\u00e7\u00f5es \u00e9 \" + str(custo))\n            custos.append(custo)\n            \n    return parametros, custos ","15712331":"# Fun\u00e7\u00e3o para fazer as previs\u00f5es\n# N\u00e3o precisamos do Backpropagation pois ao fazer previs\u00f5es como o modelo treinado, \n# teremos os melhores valores de pesos (parametros)\ndef predict(X, parametros):\n    AL, caches = forward_propagation(X, parametros)\n    return AL","3dccaeb9":"# Imports\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","458ecb32":"# Vers\u00f5es dos pacotes usados neste jupyter notebook\n%reload_ext watermark\n%watermark -a \"Data Science Academy\" --iversions","b68a056c":"# Carregamos o objeto completo\ntemp = load_breast_cancer()","1b86a523":"# Tipo do objeto\ntype(temp)","0e3e8eca":"# Visualiza o objeto\ntemp","a7f68eb3":"# Carregamos o dataset\ndados = pd.DataFrame(columns = load_breast_cancer()[\"feature_names\"], data = load_breast_cancer()[\"data\"])","a3be513a":"# Shape\ndados.shape","953ced09":"# Visualiza os dados\ndados.head()","7ae63a3e":"# Verifica se temos valores ausentes\ndados.isnull().any()","46de6f03":"# Separa a vari\u00e1vel target\ntarget = load_breast_cancer()[\"target\"]","8d405ffd":"type(target)","c57a8c8a":"# Visualiza a vari\u00e1vel\ntarget","1b923ef2":"# Total de registros por classe - C\u00e2ncer Benigno\nnp.count_nonzero(target == 1)","d6296220":"# Total de registros por classe - C\u00e2ncer Maligno\nnp.count_nonzero(target == 0)","63346b59":"# Vamos extrair os labels\n\n# Dicion\u00e1rio para os labels\nlabels = {}\n\n# Nomes das classes da vari\u00e1vel target\ntarget_names = load_breast_cancer()[\"target_names\"]\n\n# Mapeamento\nfor i in range(len(target_names)):\n    labels.update({i:target_names[i]})","c926d96c":"# Visualiza os labels\nlabels","fe3f10be":"# Agora preparamos as vari\u00e1veis preditoras em X\nX = np.array(dados)","1e76c1d2":"# Visualiza os dados de entrada\nX","48c5c1fa":"# Dividimos os dados de entrada e sa\u00edda em treino e teste\nX_treino, X_teste, y_treino, y_teste = train_test_split(X, target, test_size = 0.15, shuffle = True)","87e8633a":"# Shape dos dados de treino\nprint(X_treino.shape)\nprint(y_treino.shape)","004cc4ad":"# Shape dos dados de teste\nprint(X_teste.shape)\nprint(y_teste.shape)","d1bd2e6e":"# Ajusta o shape dos dados de entrada\nX_treino = X_treino.T\nX_teste = X_teste.T","a3c95bf8":"print(X_treino.shape)\nprint(X_teste.shape)","e3ccf3fc":"# Precisamos ajustar tamb\u00e9m os dados de sa\u00edda\ny_treino = y_treino.reshape(1, len(y_treino))\ny_teste = y_teste.reshape(1, len(y_teste))","f2acdf88":"print(y_treino.shape)\nprint(y_teste.shape)","ad530487":"# Vari\u00e1vel com as dimens\u00f5es de entrada para oo n\u00famero de neur\u00f4nios \ndims_camada_entrada = [X_treino.shape[0], 50, 20, 5, 1]","fb01f2f1":"dims_camada_entrada","27e89d2a":"# Treinamento do modelo\n\nprint(\"\\nIniciando o Treinamento.\\n\")\n\nparametros, custo = modeloNN(X = X_treino, \n                             Y = y_treino, \n                             dims_camada_entrada = dims_camada_entrada, \n                             num_iterations = 3000, \n                             learning_rate = 0.0075)\n\nprint(\"\\nTreinamento Conclu\u00eddo.\\n\")","afa51496":"# Plot do erro durante o treinamento\nplt.plot(custo)","93714f39":"# Previs\u00f5es com os dados de treino\ny_pred_treino = predict(X_treino, parametros)","b0254a4c":"# Visualiza as previs\u00f5es\ny_pred_treino","179b97c0":"# Ajustamos o shape em treino\ny_pred_treino = y_pred_treino.reshape(-1)\ny_treino = y_treino.reshape(-1)","ad689311":"y_pred_treino > 0.5","82d10108":"# Convertemos as previs\u00f5es para o valor bin\u00e1rio de classe \n# (0 ou 1, usando como threshold o valor de 0.5 da probabilidade)\ny_pred_treino = 1 * (y_pred_treino > 0.5)","13670daf":"y_pred_treino","aea62a7d":"# Calculamos a acur\u00e1cia comparando valor real com valor previsto\nacc_treino = sum(1 * (y_pred_treino == y_treino)) \/ len(y_pred_treino) * 100","eda5b269":"print(\"Acur\u00e1cia nos dados de treino: \" + str(acc_treino))","d9c43f0f":"print(classification_report(y_treino, y_pred_treino, target_names = ['Maligno', 'Benigno']))","d8079ada":"# Previs\u00f5es com o modelo usando dados de teste\ny_pred_teste = predict(X_teste, parametros)","95633799":"# Visualiza os dados\ny_pred_teste","4106a6be":"# Ajustamos os shapes\ny_pred_teste = y_pred_teste.reshape(-1)\ny_teste = y_teste.reshape(-1)","bd178b53":"# Convertemos as previs\u00f5es para o valor bin\u00e1rio de classe\ny_pred_teste = 1 * (y_pred_teste > 0.5)","5484ceaa":"# Visualizamos as previs\u00f5es\ny_pred_teste","bd0f5070":"# Calculamos a acur\u00e1cia\nacuracia = sum(1 * (y_pred_teste == y_teste)) \/ len(y_pred_teste) * 100","6f12e23b":"print(\"Acur\u00e1cia nos dados de teste: \" + str(acuracia))","30d4d59b":"print(classification_report(y_teste, y_pred_teste, target_names = ['Maligno', 'Benigno']))","0628504a":"Fa\u00e7a a leitura do manual em pdf no pr\u00f3ximo item de aprendizagem: Por Que Inicializamos os Pesos de Um Modelo de Rede Neural?","966056e4":"### Desenvolvendo a Fun\u00e7\u00e3o ReLU\n\nPara usar a descida de gradiente estoc\u00e1stico com retropropaga\u00e7\u00e3o de erros para treinar redes neurais profundas, \u00e9 necess\u00e1ria uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o que se assemelhe e atue como uma fun\u00e7\u00e3o linear, mas \u00e9, de fato, uma fun\u00e7\u00e3o n\u00e3o linear que permite que relacionamentos complexos nos dados sejam aprendidos.\n\nA solu\u00e7\u00e3o \u00e9 usar a fun\u00e7\u00e3o de ativa\u00e7\u00e3o linear retificada ou ReL para abreviar. Um n\u00f3 ou unidade que implementa essa fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u00e9 chamado de unidade de ativa\u00e7\u00e3o linear retificada ou ReLU, para abreviar. Frequentemente, as redes que usam a fun\u00e7\u00e3o retificadora para as camadas ocultas s\u00e3o chamadas de redes retificadas.\n\nA fun\u00e7\u00e3o ReLU \u00e9 definida como \ud835\udc53(\ud835\udc65) = max (0, \ud835\udc65). Normalmente, ela \u00e9 aplicada elemento a elemento \u00e0 sa\u00edda de alguma outra fun\u00e7\u00e3o, como um produto de vetor e matriz. \n\nA ado\u00e7\u00e3o da ReLU pode ser facilmente considerada um dos marcos na revolu\u00e7\u00e3o do aprendizado profundo, por ex. as t\u00e9cnicas que agora permitem o desenvolvimento rotineiro de redes neurais muito profundas.\n\nA derivada da fun\u00e7\u00e3o linear retificada tamb\u00e9m \u00e9 f\u00e1cil de calcular. **A derivada da fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u00e9 necess\u00e1ria ao atualizar os pesos de um n\u00f3 como parte da retropropaga\u00e7\u00e3o de erro.**\n\nA derivada da fun\u00e7\u00e3o \u00e9 a inclina\u00e7\u00e3o. A inclina\u00e7\u00e3o para valores negativos \u00e9 0,0 e a inclina\u00e7\u00e3o para valores positivos \u00e9 1,0.\n\nTradicionalmente, o campo das redes neurais evitou qualquer fun\u00e7\u00e3o de ativa\u00e7\u00e3o que n\u00e3o fosse completamente diferenci\u00e1vel, talvez adiando a ado\u00e7\u00e3o da fun\u00e7\u00e3o linear retificada e de outras fun\u00e7\u00f5es lineares. Tecnicamente, n\u00e3o podemos calcular a derivada quando a entrada \u00e9 0,0; portanto, podemos assumir que \u00e9 zero. Este n\u00e3o \u00e9 um problema na pr\u00e1tica.\n\nOs gradientes das ativa\u00e7\u00f5es tangentes e hiperb\u00f3licas s\u00e3o menores que a por\u00e7\u00e3o positiva da ReLU. Isso significa que a parte positiva \u00e9 atualizada mais rapidamente \u00e0 medida que o treinamento avan\u00e7a. No entanto, isso tem um custo. O gradiente 0 no lado esquerdo tem seu pr\u00f3prio problema, chamado \"neur\u00f4nios mortos\", no qual uma atualiza\u00e7\u00e3o de gradiente define os valores recebidos para uma ReLU, de modo que a sa\u00edda \u00e9 sempre zero; unidades ReLU modificadas, como ELU (ou Leaky ReLU, ou PReLU, etc.) podem melhorar isso.","cba53f12":"## A Arquitetura de Redes Neurais Artificiais","8dda9234":"### Desenvolvendo o Backward Propagation - Ativa\u00e7\u00e3o Linear Backward","c5555963":"### Desenvolvendo a Ativa\u00e7\u00e3o Linear","b0a49666":"### Desenvolvendo o Backward Propagation - Fun\u00e7\u00e3o Sigm\u00f3ide Backward","edd5571d":"![title](imagens\/net-relu.png)","69c6c148":"Em cada ponto, a derivada de f(x) \u00e9 a tangente do \u00e2ngulo que a reta tangente \u00e0 curva faz em rela\u00e7\u00e3o ao eixo das abscissas. A reta \u00e9 sempre tangente \u00e0 curva azul; a tangente do \u00e2ngulo que ela faz com o eixo das abscissas \u00e9 a derivada. Note-se que a derivada \u00e9 positiva quando verde, negativa quando vermelha, e zero quando preta.\n\nA derivada de uma fun\u00e7\u00e3o y = f(x) num ponto x = x0, \u00e9 igual ao valor da tangente trigonom\u00e9trica do \u00e2ngulo formado pela tangente geom\u00e9trica \u00e0 curva representativa de y=f(x), no ponto x = x0, ou seja, a derivada \u00e9 o coeficiente angular da reta tangente ao gr\u00e1fico da fun\u00e7\u00e3o no ponto x0.\n\nA fun\u00e7\u00e3o derivada \u00e9 representada por f'(x).","924c4bd6":"### Parte 1A - Forward Propagation\n\nhttps:\/\/arxiv.org\/pdf\/1905.07490.pdf","1229c021":"![title](imagens\/backpropagation.png)","13846fb4":"No C\u00e1lculo, a derivada em um ponto de uma fun\u00e7\u00e3o y = f(x) representa a taxa de varia\u00e7\u00e3o instant\u00e2nea de y em rela\u00e7\u00e3o a x neste ponto. \n\nUm exemplo t\u00edpico \u00e9 a fun\u00e7\u00e3o velocidade que representa a taxa de varia\u00e7\u00e3o (derivada) da fun\u00e7\u00e3o espa\u00e7o. Do mesmo modo, a fun\u00e7\u00e3o acelera\u00e7\u00e3o \u00e9 a derivada da fun\u00e7\u00e3o velocidade. Geometricamente, a derivada no ponto x = a de y = f(x) representa a inclina\u00e7\u00e3o da reta tangente ao gr\u00e1fico desta fun\u00e7\u00e3o no ponto (a, f(a)).\n\nA fun\u00e7\u00e3o que a cada ponto x associa a derivada neste ponto de f(x) \u00e9 chamada de fun\u00e7\u00e3o derivada de f(x).","afe155a1":"### Desenvolvendo a Fun\u00e7\u00e3o Sigm\u00f3ide\n\nA principal raz\u00e3o pela qual usamos a fun\u00e7\u00e3o sigm\u00f3ide \u00e9 porque ela permite converter n\u00fameros para valores entre 0 e 1. \n\nPortanto, \u00e9 especialmente usada para modelos em que temos que prever a probabilidade como uma sa\u00edda. Como a probabilidade de qualquer coisa existir apenas entre o intervalo de 0 e 1, sigmoide \u00e9 a escolha certa. Algumas caracter\u00edsiticas da fun\u00e7\u00e3o sigm\u00f3ide:\n\n- A fun\u00e7\u00e3o \u00e9 diferenci\u00e1vel. Isso significa que podemos encontrar a inclina\u00e7\u00e3o da curva sigm\u00f3ide em dois pontos.\n- A fun\u00e7\u00e3o sigm\u00f3ide log\u00edstica pode fazer com que uma rede neural fique presa no momento do treinamento.\n- A fun\u00e7\u00e3o softmax \u00e9 uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o log\u00edstica mais generalizada, utilizada para a classifica\u00e7\u00e3o em v\u00e1rias classes.","80a062e8":"### Desenvolvendo a Fun\u00e7\u00e3o Para Inicializa\u00e7\u00e3o de Pesos","0f8fd6ae":"### Parte 1 - Implementando Uma Rede Neural Artificial Somente com F\u00f3rmulas Matem\u00e1ticas (Sem Frameworks)","0077636c":"![title](imagens\/custo.png)","a66a41d3":"![title](imagens\/relu.png)","831d34bf":"Uma rede neural t\u00edpica \u00e9 constitu\u00edda por um conjunto de neur\u00f4nios interligados, infuenciando uns aos outros formando um sistema maior, capaz de armazenar conhecimento adquirido por meio de exemplos apresentados e, assim, podendo realizar infer\u00eancias sobre novos conjuntos de dados. Vejamos a arquitetura de redes neurais artificiais.\n\nAs redes neurais s\u00e3o comumente apresentadas como um grafo orientado, onde os v\u00e9rtices s\u00e3o os neur\u00f4nios e as arestas as sinapses. A dire\u00e7\u00e3o das arestas informa o tipo de alimenta\u00e7\u00e3o, ou seja, como os neur\u00f4nios s\u00e3o alimentados (recebem sinais de entrada). As redes neurais derivam seu poder devido a sua estrutura massiva e paralela e a habilidade de aprender por experi\u00eancia. Essa experi\u00eancia \u00e9 transmitida por meio de exemplos obtidos do mundo real, definidos como um conjunto de caracter\u00edsticas formados por dados de entrada e de sa\u00edda. Se apresentamos esses dados de entrada e sa\u00edda \u00e0 rede, estamos diante de aprendizagem supervsionada e caso apresentemos apenas os dados de entrada, estamos diante de aprendizagem n\u00e3o supervisionada!\n\nO conhecimento obtido pela rede atrav\u00e9s dos exemplos \u00e9 armazenado na forma de pesos das conex\u00f5es, os quais ser\u00e3o ajustados a fim de tomar decis\u00f5es corretas a partir de novas entradas, ou seja, novas situa\u00e7\u00f5es do mundo real n\u00e3o conhecidas pela rede. O processo de ajuste dos pesos sinapticos \u00e9 realizado pelo algoritmo de aprendizagem, respons\u00e1vel em armazenar na rede o conhecimento do mundo real obtido atraves de exemplos. Existem v\u00e1rios algoritmos de aprendizagem, dentre eles o backpropagation que \u00e9 o algoritmo mais utilizado.\n\n![title](\/kaggle\/input\/nnet.png)","f338f867":"### Construindo o Processo de Forward Propagation","1adf3e61":"### Desenvolvendo o Backward Propagation - Fun\u00e7\u00e3o ReLu Backward","f537cbf9":"### Gradientes e Atualiza\u00e7\u00e3o dos Pesos","b568945d":"### Carregando os Dados\n\nhttps:\/\/scikit-learn.org\/stable\/datasets\/index.html#breast-cancer-dataset","55259078":"### Importando os Pacotes","0091d819":"### Combinando Ativa\u00e7\u00e3o e Propaga\u00e7\u00e3o","910c0302":"![title](imagens\/derivada.gif)","b4dca3b8":"## A Matem\u00e1tica das Redes Neurais Artificiais\n\n### Construindo a Rede Neural com Programa\u00e7\u00e3o e Matem\u00e1tica","d358838c":"Fa\u00e7a a leitura do manual em pdf no pr\u00f3ximo item de aprendizagem: Par\u00e2metros x Hiperpar\u00e2metros.","c5575c67":"### Parte 1B - Backward Propagation","09818a65":"### Implementando a Rede Completa","cf021942":"### Desenvolvendo o Backward Propagation - Ativa\u00e7\u00e3o Linear Backward","304c3ada":"### Combinando Ativa\u00e7\u00e3o e Retropropaga\u00e7\u00e3o - Algoritmo Backpropagation","c71ff05f":"Teremos 2 Partes:\n\n- Parte 1 - Vamos construir uma rede neural artificial somente com opera\u00e7\u00f5es matem\u00e1ticas\n- Parte 2 - Vamos treinar a rede para Prever a Ocorr\u00eancia de C\u00e2ncer","4f6ff654":"### Desenvolvendo a Fun\u00e7\u00e3o de Custo","2635b775":"# <font color='blue'>Machine Learning<\/font>","7e649542":"### Mini-Projeto 4 - Usando a Rede Neural Para Prever a Ocorr\u00eancia de C\u00e2ncer","65db91d4":"![title](imagens\/nn.png)","780421d0":"A sa\u00edda da fun\u00e7\u00e3o relu durante o Forward propagation \u00e9 sempre positivo. Valores negativos ser\u00e3o transformados em 0.","842b29f5":"![title](imagens\/sigmoid.png)","c43868c1":"**Afinal, O Que \u00e9 Derivada?**\n\n![title](imagens\/derivada.png)","89d31bfa":"Se a fun\u00e7\u00e3o parecer muito abstrata ou estranha para voc\u00ea, n\u00e3o se preocupe muito com detalhes como o n\u00famero de Euler e ou como algu\u00e9m criou essa fun\u00e7\u00e3o. Para aqueles que n\u00e3o s\u00e3o conhecedores de matem\u00e1tica, a \u00fanica coisa importante sobre a fun\u00e7\u00e3o sigm\u00f3ide \u00e9 primeiro, sua curva e, segundo, sua derivada. Aqui est\u00e3o mais alguns detalhes:\n\n- **A fun\u00e7\u00e3o sigm\u00f3ide produz resultados semelhantes aos da fun\u00e7\u00e3o de passo (Step Function) em que a sa\u00edda est\u00e1 entre 0 e 1. A curva cruza 0,5 a z = 0, e podemos definir regras para a fun\u00e7\u00e3o de ativa\u00e7\u00e3o, como: Se a sa\u00edda do neur\u00f4nio sigm\u00f3ide for maior que ou igual a 0,5, gera 1; se a sa\u00edda for menor que 0,5, gera 0.**\n\n\n- A fun\u00e7\u00e3o sigm\u00f3ide \u00e9 suave e possui uma derivada simples de \u03c3(z) * (1 - \u03c3 (z)), que \u00e9 diferenci\u00e1vel em qualquer lugar da curva. \n\n\n- Se z for muito negativo, a sa\u00edda ser\u00e1 aproximadamente 0; se z for muito positivo, a sa\u00edda \u00e9 aproximadamente 1; mas em torno de z = 0, onde z n\u00e3o \u00e9 muito grande nem muito pequeno, temos um desvio relativamente maior \u00e0 medida que z muda.","c6dc3058":"### Parte 2 - Vamos treinar a rede para Prever a Ocorr\u00eancia de C\u00e2ncer"}}