{"cell_type":{"c8066894":"code","0a594474":"code","5d4f67f6":"code","322f5da8":"code","eee2cf21":"code","391c6e38":"code","3a6035eb":"code","8456fe96":"code","d7089538":"code","a2c84d01":"code","77cdc3fd":"code","e30d7f2e":"code","b4db09db":"code","905b77c0":"code","c96bc07e":"code","bb7e8108":"code","13f26432":"markdown","da4cc515":"markdown","7af9af27":"markdown","294f22fc":"markdown","bf128267":"markdown","4edd2991":"markdown","f8e823d2":"markdown","9811cc7b":"markdown","82663275":"markdown","0d514941":"markdown","3e5502a7":"markdown","ccae29c6":"markdown","d7f50075":"markdown","0d284d56":"markdown","b8e05146":"markdown","9be91516":"markdown","f4adb53c":"markdown"},"source":{"c8066894":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n","0a594474":"dfTrain = pd.read_csv('..\/input\/Train.csv')   #Training Dataset\ndfTest = pd.read_csv('..\/input\/Test.csv')   #Test Dataset\ndfValid = pd.read_csv('..\/input\/Valid.csv') #Validation Dataset\ndfTrain.head()","5d4f67f6":"dfTrain.plot(x='X',y='Y',kind='scatter')","322f5da8":"def extractFeatures(df):\n    df_Features=df.iloc[:,0:1]\n    df_Label=df.iloc[:,1:2]\n    X=df_Features.values\n    Y=df_Label.values\n    return X,Y","eee2cf21":"X,Y=extractFeatures(dfTrain)","391c6e38":"def addBiasFeature(X):\n    inputX=np.concatenate((np.ones((X.shape[0],1)),X),axis=1)\n    return inputX","3a6035eb":"inputX=addBiasFeature(X)","8456fe96":"batchSize=len(Y)         #no of Examples in batch\niterations = 5000\nalpha = 0.001\nlossList=np.zeros((iterations,1),dtype=float)  #for plotting loss curve\nfeatureCount=inputX.shape[1]   #no of features + 1 (after added bias term)\nweights=np.zeros((featureCount, 1)) #initialize Weight Paramters\n","d7089538":"\nfor k in range(iterations):\n    #Hypothesis\n    hypothesis=np.matmul( inputX,weights)           \n    \n    #Loss\n    loss=hypothesis-Y  \n    \n    \n    # derivative\n    dW=np.matmul(inputX.T,loss)  #Derivative\n   \n    \n    #gradient Update\n    weights=weights - (alpha\/batchSize)*dW              \n    \n    #Compute Loss for Plotting\n    newLoss=np.matmul( inputX,weights)-Y\n    newLossSqr=np.multiply(newLoss,newLoss)\n    lossList[k]=(1.0\/(2.0*batchSize))* np.sum(newLossSqr)\n\n","a2c84d01":"plt.plot(lossList,color='r')","77cdc3fd":"def predict(X,weights):\n    inputX=addBiasFeature(X)\n    pY=np.matmul(inputX, weights)\n    return pY","e30d7f2e":"def getRMSE(aY,pY):\n    Error=aY- pY\n    ErrorSqr=Error**2\n    MSE=ErrorSqr.mean()\n    RMSE=np.sqrt(MSE)\n    return RMSE","b4db09db":"X,Y=extractFeatures(dfTrain)\npY=predict(X, weights)  # Predict with bias feature added\nprint(getRMSE(Y, pY))","905b77c0":"vX,vY=extractFeatures(dfValid)\npY=predict(vX, weights)  # Predict with bias feature added\nprint(getRMSE(vY, pY))","c96bc07e":"tX,tY=extractFeatures(dfTest)\npY=predict(tX, weights)  # Predict with bias feature added\nprint(getRMSE(tY, pY))","bb7e8108":"x_min, x_max = X[:, 0].min() - 20, X[:, 0].max() + 20\nlineX = np.linspace(x_min, x_max, 100)\nlineX.shape=(len(lineX),1) \nlineY=predict(lineX, weights)  # Predict with bias feature added\nplt.scatter(X,Y)\nplt.plot(lineX, lineY,color='r')","13f26432":"<h5> Gradient Descent Updates","da4cc515":"<h5> RMSE on Validation Data","7af9af27":"<h1> Prediction\/RMSE Evaluation","294f22fc":"<h5> RMSE on Test Data","bf128267":"<h3> Gradient Descent Algorithm <\/h3>\n<p>\nWe start with assumpution equation (Called hypothesis) which can fit above data points.   \n<p>\n$h(x) = w_0 + w_1 x$\n<\/p> \nThe two coefficients with initial guess (i.e. $w_0$ and $w_1$) of $h(x)$ will be fed into the algorithm.\nThen Program will start from initial guess and then iterate steps to find the best fit.\n\n<p>\n Our objective is to minimize Loss.\n    <p>\n $ L(W)=   \\hat{Y}-Y$  Where  $\\hat{Y}=h(X)$\n <\/p>\nSince Loss can negative or postive, we need to minimize the absolute values ( OR Mean squared) Loss so we define Loss\/Cost function as follows","4edd2991":"<h5> Add Bias to Input Features X","f8e823d2":"<h1>Training","9811cc7b":"<h5>RMSE on Training Data","82663275":"<h1>Linear Regression <\/h1>\n<p>\nTrying to fit 2D straight Line to data using Gradient Descent Algorithm","0d514941":"<h5> Visualize Data","3e5502a7":"We minimize Loss by taking the derivative (the tangential line to a function) of our cost\/loss function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost\/loss function in the direction with the steepest descent. The size of each step is determined by the parameter \u03b1($alpha$), which is called the learning rate. The direction in which the step is taken is determined by the partial derivative of $L(w_0,w_1)$. \n\nThe gradient descent algorithm is:\n\nrepeat until convergence:<p>\n{<p>\n&nbsp;&nbsp;    $w_0 := w_0 - \\alpha \\frac{\\partial}{\\partial w_0} L(w_0, w_1) $<p>\n&nbsp;&nbsp;    $w_1 := w_1 - \\alpha \\frac{\\partial}{\\partial w_1} L(w_0, w_1) $<p>\n}\n\nOR<p>\n$\\begin{align*} \\text{repeat until convergence: } \\lbrace & \\newline w_0 := & w_0 - \\alpha \\frac{1}{n} \\sum\\limits_{i=1}^{n}(h_w(x_{i}) - y_{i}) \\newline w_1 := & w_1 - \\alpha \\frac{1}{n} \\sum\\limits_{i=1}^{n}\\left((h_w(x_{i}) - y_{i}) x_{i}\\right) \\newline \\rbrace& \\end{align*}$","ccae29c6":"<H1>Read Data from CSV","d7f50075":"<h3>Cost\/Loss Function<\/h3>\nWe can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.\n\n$L(W) = \\dfrac {1}{2n} \\displaystyle \\sum _{i=1}^n \\left ( \\hat{Y}_{i}- Y_{i} \\right)^2$\n<p>\n$L(w_0, w_1)  = \\dfrac {1}{2n} \\displaystyle \\sum _{i=1}^n \\left (h(x_{i}) - y_{i} \\right)^2$\n\nThis Loss\/cost function is also called the \"Squared error function\", or \"Mean squared error\". The mean is halved $\\left(\\frac{1}{2}\\right)$as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\frac{1}{2}$  term.","0d284d56":"<h2>Extract Input Feature to <b>X <\/b>and Label to <b>y<\/b>","b8e05146":"<h5> Initialization","9be91516":"<h1>Plotting Hypothesis","f4adb53c":"<h1>Plot Loss"}}