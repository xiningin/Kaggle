{"cell_type":{"11633e29":"code","9c33e7c4":"code","06a0d195":"code","7077e197":"code","f90e7f33":"code","ed62f5a3":"code","276c875e":"code","ed3eaf7d":"code","e4ab1b35":"code","080bd5bc":"code","4d40d250":"code","1b0a8b8a":"code","f3d732e7":"code","e633afd5":"code","505b05af":"code","b3479cce":"code","06820d3f":"code","45e731a7":"code","41c7994e":"code","3d5f9283":"code","18357f76":"code","eeab14b4":"code","505d67b2":"code","6746888d":"code","27b6ebce":"code","73e61384":"code","42e3ef91":"code","29692dd8":"code","8f78c795":"code","f1ce7ff4":"code","ac92c7e2":"code","9c32efd3":"code","1e7910ff":"code","4a76ba4a":"code","7b730f4f":"code","69695639":"code","5ba52f4e":"code","e8115414":"markdown","374c961b":"markdown","b101499c":"markdown","db413ac9":"markdown","1283cd7e":"markdown","baff076a":"markdown","34648601":"markdown","b75a01ab":"markdown","79bae4f4":"markdown","a02ef6cc":"markdown","51addd6a":"markdown","b57c7eb7":"markdown","cdc75c55":"markdown","aecef242":"markdown","54f252f8":"markdown","d802f136":"markdown","830338ac":"markdown","981c956a":"markdown","7a06cf9a":"markdown","7066fc12":"markdown","c60a3a79":"markdown","994fd143":"markdown","1b21ced7":"markdown","3614232e":"markdown","367663ae":"markdown","475b709d":"markdown","784d9e9e":"markdown"},"source":{"11633e29":"!pip install datefinder","9c33e7c4":"!pip install geotext","06a0d195":"import pandas as pd\nimport numpy as np\nfrom pylab import *\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom sklearn import svm\nfrom sklearn import model_selection\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport math\nimport geopandas as gpd\nfrom geotext import GeoText\nimport datefinder\n\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ny=train.target\nX_train,X_val,y_train,y_val = train_test_split(train,y,test_size=0.2)    ","7077e197":"def print_full(x):\n    pd.set_option('display.max_rows', len(x))\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.width', 2000)\n    pd.set_option('display.float_format', '{:20,.2f}'.format)\n    pd.set_option('display.max_colwidth', -1)\n    print(x)\n    pd.reset_option('display.max_rows')\n    pd.reset_option('display.max_columns')\n    pd.reset_option('display.width')\n    pd.reset_option('display.float_format')\n    pd.reset_option('display.max_colwidth')","f90e7f33":" print_full(X_train[X_train.target.eq(0)].head(15))","ed62f5a3":" print_full(X_train[X_train.target.eq(1)].head(15))","276c875e":"print_full(X_train.describe(include='object'))","ed3eaf7d":"print(X_train.describe(include='number'))","e4ab1b35":"train_part=pd.concat(g for _, g in X_train.groupby(\"text\") if len(g) > 1)\nprint(train_part.head(15))","080bd5bc":"print_full(X_train[X_train.keyword.isna()].head(15))","4d40d250":"#Ajouter les mots suivants aux listes anthrax, pathogen, pathogens\n#print_full(keyword_list)\n\nadd={'key':['anthrax','pathogen']}\naddendum=pd.DataFrame(add)\n\nkeyword_list=pd.DataFrame(train.keyword.unique())\nkeyword_list.columns=['key']\nkeyword_list=keyword_list.append(addendum)\nkeyword_list.dropna(inplace=True)\n\n\nkeyword_list2=pd.DataFrame(test.keyword.unique())\nkeyword_list2.columns=['key']\nkeyword_list2=keyword_list2.append(addendum)\nkeyword_list2 = keyword_list2.dropna(inplace=True)","1b0a8b8a":"def keyword(word1, word2, key_list) : \n    for i in range(len(key_list)) : \n        if key_list.iloc[i,0].count(\"%20\")>0 : \n            temp=key_list.iloc[i,0].split(\"%20\")\n            if str.lower(word1).find(temp[0])!=-1 and str.lower(word2).find(temp[1])!=-1 :\n                return key_list.iloc[i,0]\n        else : \n            if word1.find(key_list.iloc[i,0])!=-1 : \n                print(\"trouv\u00e9 : \"+ word1+\" \" + key_list.iloc[i,0])\n                return key_list.iloc[i,0]\n    return \"\"","f3d732e7":"X_train['k_list']=\"\"\nX_val['k_list']=\"\"\ntest['k_list']=\"\"\nl2=\"\"\nfor label, row in X_train.iterrows():    \n    temp=row['text'].split()\n    l=\"\"\n    for i in range(len(temp)):\n        j=i+1\n        if i<len(temp)-1:\n            l2=keyword(temp[i], temp[j],keyword_list)\n            if l2!=\"\" : \n                l= l +\" \"+ l2\n        else : \n            l2=keyword(temp[i], \"\",keyword_list)\n            if l2!=\"\" : \n                l= l +\" \" +l2\n    X_train.loc[label,'k_list']=l\n\n    \nl2=\"\"\nfor label, row in X_val.iterrows():    \n    temp=row['text'].split()\n    l=\"\"\n    for i in range(len(temp)):\n        j=i+1\n        if i<len(temp)-1:\n            l2=keyword(temp[i], temp[j],keyword_list)\n            if l2!=\"\" : \n                l= l +\" \"+ l2\n        else : \n            l2=keyword(temp[i], \"\",keyword_list)\n            if l2!=\"\" : \n                l= l +\" \" +l2\n    X_val.loc[label,'k_list']=l\n\nl2=\"\"\nfor label, row in test.iterrows():    \n    temp=row['text'].split()\n    l=\"\"\n    for i in range(len(temp)):\n        j=i+1\n        if i<len(temp)-1:\n            l2=keyword(temp[i], temp[j],keyword_list)\n            if l2!=\"\" : \n                l= l +\" \"+ l2\n        else : \n            l2=keyword(temp[i], \"\",keyword_list)\n            if l2!=\"\" : \n                l= l +\" \" +l2\n    test.loc[label,'k_list']=l","e633afd5":"print_full(X_train[['k_list', 'text']].head(40))","505b05af":"print(Dtrain.k_list.str.count(row['key']).sum())\nprint(X_train.k_list.str.count(row['key']).sum())","b3479cce":"##### cr\u00e9er liste des scores de chacun des mots\n\nDtrain=X_train[X_train.target.eq(1)]\n\nfor label, row in keyword_list.iterrows() : \n    keyword_list.loc[label,'rate']=((Dtrain.k_list.str.count(row['key']).sum())\/(X_train.k_list.str.count(row['key']).sum()))-0.5\n\nkeyword_list['rate'].fillna(0, inplace=True)\n\n\nDtest=test[test.target.eq(1)]\n\nfor label, row in keyword_list2.iterrows() : \n    keyword_list2.loc[label,'rate']=((test.k_list.str.count(row['key']).sum())\/(test.k_list.str.count(row['key']).sum()))-0.5\n\nkeyword_list2['rate'].fillna(0, inplace=True)","06820d3f":"print_full(keyword_list.head(100))","45e731a7":"for label, row in X_train.iterrows():\n    res=0\n    temp=row['k_list'].split()\n    l=\"\"\n    for i in range(len(temp)):\n        #print(type(keyword_list.loc[keyword_list['key']==temp[i],['rate']].iloc[0,0]))\n        #print(keyword_list.loc[keyword_list['key']==temp[i],['rate']])\n        res+=keyword_list.loc[keyword_list['key']==temp[i],['rate']].iloc[0,0]\n        X_train.loc[label,'score']=res\n\nX_train['score'].fillna(0, inplace=True)\n\nfor label, row in X_val.iterrows():\n    res=0\n    temp=row['k_list'].split()\n    l=\"\"\n    for i in range(len(temp)):\n        res+=keyword_list.loc[keyword_list['key']==temp[i],['rate']].iloc[0,0]\n        X_val.loc[label,'score']=res\n\nX_val['score'].fillna(0, inplace=True)\n\nfor label, row in test.iterrows():\n    res=0\n    temp=row['k_list'].split()\n    l=\"\"\n    for i in range(len(temp)):\n        res+=keyword_list2.loc[keyword_list2['key']==temp[i],['rate']].iloc[0,0]\n        test.loc[label,'score']=res\n\ntest['score'].fillna(0, inplace=True)\n","41c7994e":"print(X_train.head(15))","3d5f9283":"def has_keyword(sentence, df_k):\n    \"\"\"\"This function returns the keyword listed in df_k that appears in the sentence or \"\" \"\"\"\n    for i in range(len(df_k)) :\n        #print(\" : \"+df_k.iloc[i,0])\n        if sentence.find(df_k.iloc[i,0])!=-1:\n            #print(\"Trouv\u00e9\")\n            return df_k.iloc[i,0]\n    return \"\"\n\ndef nb_keyword(sentence, df_k):\n    \"\"\"\"This function returns the number of keyword listed in df_k that appears in the sentence or \"\" \"\"\"\n    result=0\n    for i in range(len(df_k)) :\n        #print(\" : \"+df_k.iloc[i,0])\n        if sentence.find(df_k.iloc[i,0])!=-1:\n            #print(\"Trouv\u00e9\")\n            result=result+1\n    return result\ndef haslink(sentence):\n    return sentence.count(\"http:\")\ndef hassaut(sentence):\n    return sentence.count(\"\\n\")\n\ndef hasap(sentence):\n    return sentence.count(\"'\")\ndef hasupperletter(sentence):\n    return sum(1 for c in sentence if c.isupper())\ndef hasat(sentence):\n    return sentence.count(\"@\")\ndef hastags(sentence):\n    return sentence.count(\"#\")\ndef hasmoney(sentence):\n    result=str.lower(sentence).count(\"money\")\n    result=result+str.lower(sentence).count(\"dollar\")\n    result=result+sentence.count(\"$\")\n    return result\ndef hasfun(sentence):\n    result=str.lower(sentence).count(\"fun\")\n    result=result+str.lower(sentence).count(\"game\")\n    result=result+str.lower(sentence).count(\"lol\")\n    return result\n    \n\ndef hasdoublekey(sentence):\n    return sentence.count(\"%20\")\n\ndef hasnumber(sentence):\n    l = []\n    for t in sentence.split():\n        try:\n            l.append(float(t))\n        except ValueError:\n            pass\n    return len(l)\ndef hasreligions(sentence):\n    result=str.lower(sentence).count(\"god\")\n    result=result+str.lower(sentence).count(\"temple\")\n    result=result+str.lower(sentence).count(\"sikh\")\n    result=result+str.lower(sentence).count(\"pray\")\n    result=result+str.lower(sentence).count(\"pastor\")\n    result=result+str.lower(sentence).count(\"prophet\")\n    result=result+str.lower(sentence).count(\"islam\")\n    result=result+str.lower(sentence).count(\"muslim\")\n    result=result+str.lower(sentence).count(\"mosque\")\n    return result\n\ndef hassex(sentence):\n    result=str.lower(sentence).count(\"sex\")\n    result=result+str.lower(sentence).count(\"ass\")\n    result=result+str.lower(sentence).count(\"cock\")\n    return result\n\ndef haspunct(sentence):\n    result=sentence.count(\".\")\n    result=result+sentence.count(\"?\")\n    result=result+sentence.count(\"!\")\n    result=result+sentence.count(\":\")\n    result=result+sentence.count(\";\")\n    return result\n\ndef haslocations(sentence) : \n    places=GeoText(sentence)\n    countries=places.countries\n    cities=places.cities\n    return len(countries)+len(cities)\ndef hasphoto(sentence) : \n    result=str.lower(sentence).count(\"photo\")\n    result=result+str.lower(sentence).count(\"video\")\n    return result\ndef hasdates(sentence) : \n    #print(sentence)\n    result=0\n    try : \n        matches = list(datefinder.find_dates(sentence))\n        result= len(matches)\n    finally : \n        return result\n    ","18357f76":"print(hasreligions(\"I love Paris and Manhattan and london and Hong Kong in February 2010 photo Photo photos god\"))","eeab14b4":"#train['nbcar']=train.apply(lambda row: len(row.text), axis=1)\n#train['wordcount']=train.apply(lambda row: len(row.text.split()), axis=1)\n#train['av_wordlength']=train.apply(lambda row: row.nbcar\/row.wordcount, axis=1)\n#train['key'] = train.apply(lambda row: 1 if str(row.keyword).strip() and str(row.keyword) else 0,axis=1)\n#train['nb_keyword']=train.apply(lambda row: nb_keyword(str.lower(row.text),keyword_list), axis=1)\n#train['loc'] = train.apply(lambda row: 1 if not pd.isnull(row.location) else 0,axis=1)\n#train['link']=train.apply(lambda row: haslink(row.text),axis=1)\n#train['upletter']=train.apply(lambda row: hasupperletter(row.text),axis=1)\n#train['ap']=train.apply(lambda row: hasap(row.text),axis=1)\n#train['tag']=train.apply(lambda row: hastags(row.text),axis=1)\n#train['at']=train.apply(lambda row: hasat(row.text),axis=1)\n#train['money']=train.apply(lambda row: hasmoney(row.text),axis=1)\n#train['found_loc']=train.apply(lambda row: haslocations(row.text),axis=1)\n#train['date']=train.apply(lambda row: hasdates(row.text),axis=1)\n#train['dbk']=train.apply(lambda row: 0 if pd.isnull(row.keyword) else hasdoublekey(row.keyword),axis=1)\n#train['number']=train.apply(lambda row: hasnumber(row.text),axis=1)\n#train['photo']=train.apply(lambda row: hasphoto(row.text),axis=1)\n\n\n\nX_train['nbcar']=X_train.apply(lambda row: len(row.text), axis=1)\nX_train['wordcount']=X_train.apply(lambda row: len(row.text.split()), axis=1)\nX_train['av_wordlength']=X_train.apply(lambda row: row.nbcar\/row.wordcount, axis=1)\nX_train['key'] = X_train.apply(lambda row: 1 if str(row.keyword).strip() and str(row.keyword) else 0,axis=1)\nX_train['nb_keyword']=X_train.apply(lambda row: nb_keyword(str.lower(row.text),keyword_list), axis=1)\nX_train['loc'] = X_train.apply(lambda row: 1 if not pd.isnull(row.location) else 0,axis=1)\nX_train['link']=X_train.apply(lambda row:  haslink(row.text) ,axis=1)\nX_train['upletter']=X_train.apply(lambda row: hasupperletter(row.text),axis=1)\nX_train['ap']=X_train.apply(lambda row: hasap(row.text),axis=1)\nX_train['tag']=X_train.apply(lambda row: hastags(row.text),axis=1)\nX_train['at']=X_train.apply(lambda row: hasat(row.text),axis=1)\nX_train['money']=X_train.apply(lambda row: hasmoney(row.text),axis=1)\nX_train['found_loc']=X_train.apply(lambda row: haslocations(row.text),axis=1)\nX_train['date']=X_train.apply(lambda row: hasdates(row.text),axis=1)\nX_train['dbk']=X_train.apply(lambda row: 0 if pd.isnull(row.keyword) else hasdoublekey(row.keyword),axis=1)\nX_train['number']=X_train.apply(lambda row: hasnumber(row.text),axis=1)\nX_train['photo']=X_train.apply(lambda row: hasphoto(row.text),axis=1)\nX_train['religion']=X_train.apply(lambda row: hasreligions(row.text),axis=1)\nX_train['fun']=X_train.apply(lambda row: hasfun(row.text),axis=1)\nX_train['sex']=X_train.apply(lambda row: hassex(row.text),axis=1)\nX_train['saut']=X_train.apply(lambda row: hassaut(row.text),axis=1)\nX_train['punct']=X_train.apply(lambda row: haspunct(row.text),axis=1)\n\n\nX_val['nbcar']=X_val.apply(lambda row: len(row.text), axis=1)\nX_val['wordcount']=X_val.apply(lambda row: len(row.text.split()), axis=1)\nX_val['av_wordlength']=X_val.apply(lambda row: row.nbcar\/row.wordcount, axis=1)\nX_val['key'] = X_val.apply(lambda row: 1 if str(row.keyword).strip() and str(row.keyword) else 0,axis=1)\nX_val['nb_keyword']=X_val.apply(lambda row: nb_keyword(str.lower(row.text),keyword_list), axis=1)\nX_val['loc'] = X_val.apply(lambda row: 1 if not pd.isnull(row.location) else 0,axis=1)\nX_val['link']=X_val.apply(lambda row: haslink(row.text) ,axis=1)\nX_val['upletter']=X_val.apply(lambda row: hasupperletter(row.text),axis=1)\nX_val['ap']=X_val.apply(lambda row: hasap(row.text),axis=1)\nX_val['tag']=X_val.apply(lambda row: hastags(row.text),axis=1)\nX_val['at']=X_val.apply(lambda row: hasat(row.text),axis=1)\nX_val['money']=X_val.apply(lambda row: hasmoney(row.text),axis=1)\nX_val['found_loc']=X_val.apply(lambda row: haslocations(row.text),axis=1)\nX_val['date']=X_val.apply(lambda row: hasdates(row.text),axis=1)\nX_val['dbk']=X_val.apply(lambda row: 0 if pd.isnull(row.keyword) else hasdoublekey(row.keyword),axis=1)\nX_val['number']=X_val.apply(lambda row: hasnumber(row.text),axis=1)\nX_val['photo']=X_val.apply(lambda row: hasphoto(row.text),axis=1)\nX_val['religion']=X_val.apply(lambda row: hasreligions(row.text),axis=1)\nX_val['fun']=X_val.apply(lambda row: hasfun(row.text),axis=1)\nX_val['sex']=X_val.apply(lambda row: hassex(row.text),axis=1)\nX_val['saut']=X_val.apply(lambda row: hassaut(row.text),axis=1)\nX_val['punct']=X_val.apply(lambda row: haspunct(row.text),axis=1)\n\n\ntest['nbcar']=test.apply(lambda row: len(row.text), axis=1)\ntest['wordcount']=test.apply(lambda row: len(row.text.split()), axis=1)\ntest['av_wordlength']=test.apply(lambda row: row.nbcar\/row.wordcount, axis=1)\ntest['key']=test.apply(lambda row: 1 if str(row.keyword).strip() and str(row.keyword) else 0,axis=1)\ntest['nb_keyword']=test.apply(lambda row: nb_keyword(str.lower(row.text),keyword_list2), axis=1)\ntest['loc'] = test.apply(lambda row: 1 if not pd.isnull(row.location) else 0,axis=1)\ntest['link']=test.apply(lambda row:  haslink(row.text),axis=1)\ntest['upletter']=test.apply(lambda row: hasupperletter(row.text),axis=1)\ntest['ap']=test.apply(lambda row: hasap(row.text),axis=1)\ntest['tag']=test.apply(lambda row: hastags(row.text),axis=1)\ntest['at']=test.apply(lambda row: hasat(row.text),axis=1)\ntest['money']=test.apply(lambda row: hasmoney(row.text),axis=1)\ntest['found_loc']=test.apply(lambda row: haslocations(row.text),axis=1)\ntest['date']=test.apply(lambda row: hasdates(row.text),axis=1)\ntest['dbk']=test.apply(lambda row: 0 if pd.isnull(row.keyword) else hasdoublekey(row.keyword),axis=1)\ntest['number']=test.apply(lambda row: hasnumber(row.text),axis=1)\ntest['photo']=test.apply(lambda row: hasphoto(row.text),axis=1)\ntest['religion']=test.apply(lambda row: hasreligions(row.text),axis=1)\ntest['fun']=test.apply(lambda row: hasfun(row.text),axis=1)\ntest['sex']=test.apply(lambda row: hassex(row.text),axis=1)\ntest['saut']=test.apply(lambda row: hassaut(row.text),axis=1)\ntest['punct']=test.apply(lambda row: haspunct(row.text),axis=1)\n\n","505d67b2":"sns.set_style(\"whitegrid\")\n#print(train[['key','keyword']].head(100).to_string())\n#print(train.describe(include='all'))\nNDtrain=X_train[X_train.target.eq(0)]\nDtrain=X_train[X_train.target.eq(1)]\nsns.distplot(NDtrain['key'],kde=False, label=\"Non disaster tweet\")\nsns.distplot(Dtrain['key'],kde=False, label=\"Disaster tweet\")\nplt.legend(prop={'size': 12})\nplt.xlabel(\"Presence of a keyword in the tweet\")\nplt.title(\"Impact of the keyword presence on tweets nature\")\nplt.show()\n","6746888d":"sns.set_context(\"paper\", font_scale=2)\nsns.set_style(\"white\")\nplt.rc('text', usetex=False)\nfig, ax = plt.subplots(figsize=(8,6))\nsns.despine(left=True)\n\nB=['Non disaster tweet','Disaster tweet']\nax.hist([NDtrain['nb_keyword'], Dtrain['nb_keyword']],  histtype='bar', \n        align='mid', label=B, alpha=0.4)#, rwidth=0.6)\n\nplt.legend(prop={'size': 12})\nplt.xlabel(\"Number of keywords in a tweet\")\nplt.title(\"Impact of the number of keywords on target\")\nplt.show()\n","27b6ebce":"sns.set_context(\"paper\", font_scale=2)\nsns.set_style(\"white\")\nplt.rc('text', usetex=False)\nfig, ax = plt.subplots(figsize=(6,6))\nsns.despine(left=True)\n\nB=['Non disaster tweet','Disaster tweet']\nax.hist([NDtrain['link'], Dtrain['link']],  histtype='bar', \n        align='mid', label=B, alpha=0.4)#, rwidth=0.6)\n\nplt.legend(prop={'size': 12})\nplt.xlabel(\"Link filled in a tweet\")\nplt.title(\"Impact of the links on target\")\nplt.show()","73e61384":"sns.set_context(\"paper\", font_scale=2)\nsns.set_style(\"white\")\nplt.rc('text', usetex=False)\nfig, ax = plt.subplots(figsize=(6,6))\nsns.despine(left=True)\n\nB=['Non disaster tweet','Disaster tweet']\nax.hist([NDtrain['tag'], Dtrain['tag']],  histtype='bar', \n        align='mid', label=B, alpha=0.4)#, rwidth=0.6)\n\nplt.legend(prop={'size': 12})\nplt.xlabel(\"Tags filled in a tweet\")\nplt.title(\"Impact of the tags on target\")\nplt.show()","42e3ef91":"sns.set_context(\"paper\", font_scale=2)\nsns.set_style(\"white\")\nplt.rc('text', usetex=False)\nfig, ax = plt.subplots(figsize=(6,6))\nsns.despine(left=True)\n\nB=['Non disaster tweet','Disaster tweet']\nax.hist([NDtrain['found_loc'], Dtrain['found_loc']],  histtype='bar', \n        align='mid', label=B, alpha=0.4)#, rwidth=0.6)\n\nplt.legend(prop={'size': 12})\nplt.xlabel(\"Locations in a tweet\")\nplt.title(\"Impact of the locations inside a tweet on target\")\nplt.show()","29692dd8":"sns.set_context(\"paper\", font_scale=2)\nsns.set_style(\"white\")\nplt.rc('text', usetex=False)\nfig, ax = plt.subplots(figsize=(6,6))\nsns.despine(left=True)\n\nB=['Non disaster tweet','Disaster tweet']\nax.hist([NDtrain['loc'], Dtrain['loc']],  histtype='bar', \n        align='mid', label=B, alpha=0.4)#, rwidth=0.6)\n\nplt.legend(prop={'size': 12})\nplt.xlabel(\"Location filled in a tweet\")\nplt.title(\"Impact of the location on target\")\nplt.show()\n","8f78c795":"print_full(X_train.loc[X_train['loc']==1, ['location', 'target', 'text']].head(40))","f1ce7ff4":"fig, ax = plt.subplots(figsize=(6,6))\nsns.despine(left=True)\n\nB=['Non disaster tweet','Disaster tweet']\nax.hist([NDtrain['nbcar'], Dtrain['nbcar']],  histtype='bar', \n        align='mid', label=B, alpha=0.4)#, rwidth=0.6)\n\nplt.legend(prop={'size': 12})\nplt.xlabel(\"Number of caracters in a tweet\")\nplt.title(\"Impact of the number of caracters on target\")\nplt.show()\n","ac92c7e2":"fig, ax = plt.subplots(figsize=(8,6))\nsns.despine(left=True)\n\nB=['Non disaster tweet','Disaster tweet']\nax.hist([NDtrain['wordcount'], Dtrain['wordcount']],  histtype='bar', \n        align='mid', label=B, alpha=0.4)#, rwidth=0.6)\n\nplt.legend(prop={'size': 12})\nplt.xlabel(\"Words number in a tweet\")\nplt.title(\"Impact of the words number on target\")\nplt.show()\n","9c32efd3":"fig, ax = plt.subplots(figsize=(8,6))\nsns.despine(left=True)\n\nB=['Non disaster tweet','Disaster tweet']\nax.hist([NDtrain['av_wordlength'], Dtrain['av_wordlength']],  histtype='bar', \n        align='mid', label=B, alpha=0.4)#, rwidth=0.6)\n\nplt.legend(prop={'size': 12})\nplt.xlabel(\"Words number in a tweet\")\nplt.title(\"Impact of the words average length on target\")\nplt.show()\n","1e7910ff":"sns.set_style(\"whitegrid\")\nax=sns.catplot(x=\"wordcount\", y=\"nbcar\", kind=\"box\",hue=\"target\",data=X_train, height=10)\nax.set(xlabel='Number of words', ylabel='Tweet Caracter count')\nplt.title(\"Tweets Caracter count by number of words and Result\")\nplt.show()\n","4a76ba4a":"sns.set_style(\"whitegrid\")\nax=sns.catplot(x=\"nb_keyword\", y=\"nbcar\", kind=\"box\",hue=\"target\",data=X_train, height=10)\nax.set(xlabel='Number of keyword', ylabel='Tweet Caracter count')\nplt.title(\"Tweets Caracter count by number of keywords and Result\")\nplt.show()","7b730f4f":"sns.set_style(\"whitegrid\")\nax=sns.catplot(x=\"loc\", y=\"nbcar\", kind=\"box\",hue=\"target\",data=X_train, height=10)\nax.set(xlabel='Location filled', ylabel='Tweet Caracter count')\nplt.title(\"Tweets Caracter count by filled location and Result\")\nplt.show()","69695639":"#clf=svm.SVC()\n\nclf=RandomForestClassifier()\n\n#Tags addition reduce accuracy :-(\nfeatures=['nbcar','wordcount','av_wordlength','nb_keyword','loc', 'link', 'upletter', 'ap', 'tag', 'at', \n          'money',  'found_loc','date', 'dbk', 'number', 'photo', 'religion', 'sex', 'fun', 'saut', 'punct', 'score']\nclf.fit(X_train[features],X_train[['target']])\nres_val=clf.predict(X_val[features])\nX_val['pred']=res_val\nerreur_pred=X_val[X_val['pred']!=X_val['target']]\nprint(clf.score(X_val[features], y_val))\nprint_full(erreur_pred[['target', 'pred','nb_keyword' ,'link', 'keyword','found_loc','date',  'number' ,'text' ]])\n\n","5ba52f4e":"y_test=clf.predict(test[features])\nmy_submission=pd.DataFrame({'id': test.id, 'target': y_test})\nmy_submission.to_csv('submission.csv', index=False)","e8115414":"The table above shows the descriptive statistics on the numerical columns.\n\nThe target column is only filled with 0 and 1, with:\n\n* 0: tweet **is not** related to **disaster**\n* 1: tweet **is** related to **disaster**\n\nThe mean shows us that around 43% of the tweets are disasters tweets. So, the distribution of the tweets is slightly biased.","374c961b":"This plot is pretty interesting, the orange distribution which represents the disaster tweets is nearly always above the blue one. The orange variance is smaller too. \n","b101499c":"Watching this, it seems that between 15 and 20 (included), the probability of either target is nearly the same. Under or over that, most of the tweets are non disaster tweets.\n\n### Average_wordlength\n","db413ac9":"So with this, the datasets should be clean. \n## Data Analysis\n\nFirst, we will create on both datasets some features about word counts, average word length, presence of a keyword and caracters count.\n","1283cd7e":"The differences spotted on the whole table (not only the first 15 ): \n* keyword missing in some duplicates\n* location missing or different location\n* target not identical in all duplicates\nSo the datasets need obviously cleaning either to remove the duplicates or to harmonize them. **Cleaning action 1**: determine the strategy to deal with that.","baff076a":"Apparently there are so many keywords filled now that we can't even see on the graph cases where it is not filled. \nWe will just check if the filling is right with the next code section. \nAs most of the keywords are filled now, we can't really say anything about this column anymore. We will have to analyze whether the number of  keywords have an impact on the target column. \nAction: Analyze later whether the keywords value has an impact on the target.","34648601":"## Two variables analysis\n### Nbcar ","b75a01ab":"A few things stands out already, based on the observations of more than the 15 first rows:\n\n* column keyword is not always filled even in disasters tweets. Is this column nicely filled?\n* column location is not filled either\n* the tone of the disaster tweets is more serious or formal, sentences seem longer\n\nSo, let us use some of the descriptive stats to understand better the training dataset. ","79bae4f4":"The table above shows the descriptive statistics on the columns containing text.\n\nHere, we notice that :\n\n* <strong>Tweets text column<\/strong>: the only column always filled. Some tweets appears multiple times, while around 99% are unique. **Investigate action 1**: It might be interesting to see if the duplicated tweets have the same information.\n* <strong>Disaster keyword in a tweet<\/strong>: 99% or 7552 are filled, 221 different disaster keywords are employed there, the one appearing the most being \"Fatalities\", observed 45 times in the dataset. **Investigate action 2**: We should try to see if the unfilled keywords can be filled automatically or if there is a reason explaining those missing values.\n<strong>Location of the tweet<\/strong>: 67% are filled, with a lot of different locations. **Investigate action 3**: We should investigate how can there be so many different locations.\n","a02ef6cc":"Here we can see that the shorter tweets are mostly non disaster tweet. When the number of caracters go over 100 this tendency is not so clear.\n\n### wordcount","51addd6a":"Maintenant, il convient d'appliquer les scores pour cr\u00e9er une colonne score global","b57c7eb7":"Then we create a function which takes two parameters: a sentence and an array of keywords. If a keyword is found in the sentence, it is returned.\nA new field is created, countaining a kind of list of all keywords in the sentence. This field is going to be useful to create a ratings of keywords and then weight the sentences by them\n\n","cdc75c55":"Phase d'analyse \u00e0 finir","aecef242":"### Unfilled keywords\nThe table below lists the first 15 unfilled keywords: ","54f252f8":"\n# Introduction \nThe goal of this kernel is to share a way to tackle NLP tasks learnt from Andrew Ng in the Machine Learning course. \nA general way to describe this methodo : \n1. Create standard NLP features\n2. Develop a basic machine learning algorythm. The iterative approach described on steps 3 to 5 is based on this basic algorythm.\n3. Analyze the predictions error on the validation dataset\n4. Create new features. From this point on, either run again the whole algorythm to go back to point 3 and then 4 iteratively OR go to 5\n5. Generate an Analysis Plot to decide what to do next to improve your accuracy\n \n First things first, the following lines of code install the required libraries, load the datasets and create a train, validation and test datasets","d802f136":"## First level Analysis\n\nFirst thing first, let us get a look at the data by target type.\nWe will display the 15 first non disaster tweets.","830338ac":"This here is a very useful function to print out the results without cutting the text field : ","981c956a":"## Further investigation\n### Columns of duplicated tweets text\nThe following code isolate the texts duplicates : \n","7a06cf9a":"Not much can be said here, most of the locations are well defined independently of target. \n\nLet us go to the next column: nb_car","7066fc12":"We will do a few basics feature graphical analysis.","c60a3a79":"And now with the disaster tweets:\n","994fd143":"The following points seems obvious: \n* when the target equals 1, there should keywords, for example with id 1, the keyword should be earthquake. **Cleaning action 2**: cleaning required to find the keywords and add them.\n* when the target equals 0, there can be a keywords or not. Remember the introduction talking about setting the streets on fire with songs? In this tweet, the keyword would be fire although this is not a disaster tweet.\n###  Location versatility\nThe tables above listed some locations. We saw for example the following values: \"Nigeria\", \"Pedophile hunting ground\", \"Jubail IC, Saudi Arabia\",  \"?????? ???? ??????\".\nIt seems that there were no quality control on this column entry. **Analysis action 1**: It might be interesting to see if the more formal tweets have good quality fills on the location column, but then again, how can we figure that out? \n## Data Cleaning\n### Duplicates rows\nThe gains here seems very low as there are not a lot of rows affected. So the cleaning has not been done.\n### Missing keywords\nIf we go back to the table of first 15 unfilled keywords of train dataset, we find that the words a human would point out as disaster keywords are on that keywords list, see column manual keyword. So, those 15 cases have easily been filled manually. \n\n| id   | keyword | manual keyword | text                                                         | target |\n| ---- | ------- | -------------- | ------------------------------------------------------------ | ------ |\n| 1    |         | earthquake     | Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all | 1      |\n| 4    |         | fire           | Forest fire near La Ronge Sask. Canada                       | 1      |\n| 5    |         | evacuation     | All residents asked to 'shelter in place' are being notified by officers.  No other evacuation or shelter in place orders are expected | 1      |\n| 6    |         | wildfire       | 13,000 people receive #wildfires evacuation orders in California | 1      |\n| 7    |         | wildfire       | Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours  into a school | 1      |\n| 8    |         | fire           | #RockyFire Update => California Hwy. 20 closed in both directions due  to Lake County fire - #CAfire #wildfires | 1      |\n| 10   |         | flood          | #flood #disaster Heavy rain causes flash flooding of streets in Manitou,  Colorado Springs areas | 1      |\n| 13   |         | fire           | I'm on top of the hill and I can see a fire in the woods...  | 1      |\n| 14   |         | evacuation     | There's an emergency evacuation happening now in the building across the  street | 1      |\n| 15   |         | tornado        | I'm afraid that the tornado is coming to our area...         | 1      |\n| 16   |         | heat wave      | Three people died from the heat wave so far                  | 1      |\n| 17   |         | flood          | Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH  TAMPA WHAT AM I GONNA DO WHAT AM I GONNA DO FVCK #flooding | 1      |\n| 18   |         | flood          | #raining #flooding #Florida #TampaBay #Tampa 18 or 19 days. I've lost  count | 1      |\n| 19   |         | flood          | #Flood in Bago Myanmar #We arrived Bago                      | 1      |\n| 20   |         | crash          | Damage to school bus on 80 in multi car crash #BREAKING      | 1      |\n\nLet us see how well this can be generalized with the following automation. \nWe will build an algorithm that will \n* go through every sentence of the tweets datasets where the keyword column is empty. \n* look for words in the keywords list. \n* If one keyword is found, the keyword column will be filled.\nThe first thing is to create a list with all keywords which are already listed in the initial datasets. The nan are deleted from the arrays.\n\nAfter a few iterations, it became clear that some words had to be added to the keyword list.","1b21ced7":"Those two libraries are required to find dates in a string and the second one to recognize cities and countries in a string.","3614232e":"The location doesn't seem to have much impact on the target. \nLet's just check that by checking the first location value that are not null.  ","367663ae":"**So, if the number of keywords is <=1, the probability is much higher that the tweet is a non disaster tweet. \nAbove that, there are more disaster tweets.\n\nLet us check the location column.","475b709d":"We then loop over the datasets train and test to add the found keywords when the field is empty.","784d9e9e":"In this plot, we can see easily that for every number of keyword the Disaster tweets are longer and have less variance. "}}