{"cell_type":{"982c820c":"code","bf90e655":"code","ac35c9af":"code","e9624fc2":"code","79693b49":"code","8946e88e":"code","73391a0f":"code","fa0fd273":"code","9eb2e37e":"code","ed9766c2":"code","c2991303":"code","b930e090":"code","9ce47259":"code","5a8b25c0":"code","957b7eaf":"code","6a68aed8":"code","1eb6d2f4":"code","a873c616":"code","21d4a386":"code","2c9bd6db":"code","69c9d627":"code","1b501808":"code","04c4fa16":"code","13673fee":"code","dfac6b36":"code","cb16f6a8":"code","e91119a9":"code","e26355c1":"code","448af7b9":"code","b615d7db":"code","8c577f37":"code","2e5322bd":"code","e526e38f":"code","dd88c4cc":"code","ddeb668e":"code","a4c81ddd":"code","b8446732":"code","1f6443d2":"code","8137ee6e":"code","0fd3e028":"code","d715a25f":"code","101cf859":"code","5df964d1":"code","e6639bd0":"code","f1495e2c":"code","98c6f9c3":"code","5425e04f":"code","b5edf44e":"code","1e25351e":"code","525f3502":"code","3295ed9b":"code","ad2c5a68":"code","90d7fff7":"code","387f818b":"code","0ef180e7":"code","dadd0d67":"code","c64b5d87":"code","c83852ea":"markdown","c525bf69":"markdown","18cc6c75":"markdown","52add71d":"markdown","d85531e0":"markdown","3e6eb916":"markdown","686dc265":"markdown","07ea2ed5":"markdown","6a67683a":"markdown","68da00c4":"markdown","6f34e8b9":"markdown","f352f8bd":"markdown"},"source":{"982c820c":"# Uploading the packages we'll need\n\nimport pandas as pd\nimport numpy as np\n\nimport json\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout, Activation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","bf90e655":"# Reading in the json file\n\nwith open('..\/input\/ships-in-satellite-imagery\/shipsnet.json') as data_file:\n    dataset = json.load(data_file)","ac35c9af":"# Turning the json information into a numpy array and then assign it as our X and y values\n\nX = np.array(dataset['data']).astype('uint8')\ny = np.array(dataset['labels']).astype('uint8')","e9624fc2":"# Checking the amount of ship and not a ship labels for the images\n\npd.Series(y).value_counts()","79693b49":"# Identifying the class names\n\nclass_names = ['Not A Ship', 'Ship']","8946e88e":"# The current data for each image is one long row of flattened data points representing the RGB values of each pixel\n\nX.shape","73391a0f":"# Each item in X_reshaped will now be 3 lists, each list will be the RBG values for each pixel for the length and the width \n# of the image\n\nX_reshaped = X.reshape([-1, 3, 80, 80])\n\nX_reshaped.shape","fa0fd273":"# We need to change the order of the dimensions to get the correct format to plot the images \n\nX_reshaped = X.reshape([-1, 3, 80, 80]).transpose([0,2,3,1])\n\nX_reshaped.shape","9eb2e37e":"# The current data for y is a single integer representing the class (1 = ship, 0 = not a ship)\n\ny.shape","ed9766c2":"# Converts the data for y to a binary class matrix\n\ny_reshaped = tf.keras.utils.to_categorical(y, num_classes=2)\n\ny_reshaped.shape","c2991303":"# Separating X_reshaped to correspond with the different labels\n\nimgs_0 = X_reshaped[y==0]\nimgs_1 = X_reshaped[y==1]","b930e090":"# Taking a quick look at a the Not a Ship pictures\n\ndef plot(a,b):\n\n    plt.figure(figsize=(15, 15))\n    plt.subplot(2,4,1)\n    plt.title('Not A Ship')\n    plt.imshow(a[0])\n    plt.subplot(2,4,2)\n    plt.title('Not A Ship')\n    plt.imshow(a[1])\n    plt.subplot(2,4,3)\n    plt.title('Not A Ship')\n    plt.imshow(a[2])\n    plt.subplot(2,4,4)\n    plt.title('Not A Ship')\n    plt.imshow(a[3])\n    plt.subplot(2,4,5)\n    plt.title('Ship')\n    plt.imshow(b[0])\n    plt.subplot(2,4,6)\n    plt.title('Ship')\n    plt.imshow(b[1])\n    plt.subplot(2,4,7)\n    plt.title('Ship')\n    plt.imshow(b[2])\n    plt.subplot(2,4,8)\n    plt.title('Ship')\n    plt.imshow(b[3])\n    plt.subplots_adjust(bottom=0.3, top=0.7, hspace=0.25)\n    \nplot(imgs_0, imgs_1)","9ce47259":"# Normalizing the X values\n\nX_reshaped = X_reshaped \/ 255","5a8b25c0":"# Doing the initial train\/test split on the reshaped values\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(X_reshaped, y_reshaped, test_size=0.20, random_state=42)\n\n# Creating the validation set\n\nX_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)","957b7eaf":"X_train.shape","6a68aed8":"y_train.shape","1eb6d2f4":"X_val.shape","a873c616":"y_val.shape","21d4a386":"X_test.shape","2c9bd6db":"y_test.shape","69c9d627":"# Using Sequential and then adding the layers after\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape=[80, 80, 3]))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(2, activation='sigmoid'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])","1b501808":"# Training the model\n\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))","04c4fa16":"# Finding our final accuracy on the test set\n\nmodel.evaluate(X_test, y_test)","13673fee":"# Plotting the loss and accuracy\n\npd.DataFrame(history.history).plot();","dfac6b36":"# The loss function still looks like it's going down so testing to see if adding more epochs makes a difference\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape=[80, 80, 3]))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(2, activation='sigmoid'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])","cb16f6a8":"# Training the model\n\nhistory = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))","e91119a9":"# Finding our final accuracy on the test set\n\nmodel.evaluate(X_test, y_test)","e26355c1":"# Plotting the loss and accuracy\n\npd.DataFrame(history.history).plot();","448af7b9":"# Trying out more layers to see if we can get it even higher and adding in the softmax function for more interpretability\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape=[80, 80, 3]))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])","b615d7db":"# Training the model\n\nhistory = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))","8c577f37":"# Finding our final accuracy on the test set\n\nmodel.evaluate(X_test, y_test)","2e5322bd":"# Plotting the loss and accuracy\n\npd.DataFrame(history.history).plot();","e526e38f":"# Finding the percentage predictions for each item in the test set\n\ny_pred = model.predict(X_test)","dd88c4cc":"# Taking a look at the first prediction\n\npd.Series(y_pred[0], index=class_names)","ddeb668e":"# Looks like a ship to me!\n\nplt.imshow(X_test[0])","a4c81ddd":"# Putting all of the predictions in a dataFrame\n\ndf = pd.DataFrame(y_pred, columns=class_names)\n\ndf = df.round(6)\n\ndf","b8446732":"# Adding the information from y_test to the predictions dataFrame\n\ndf['Test is a Ship'] = y_test[:, 1]\n\ndf","1f6443d2":"# Finding the images where the model was the most certain that is wasn't a ship when it was\n\ndf['Difference'] = df['Ship'] - df['Test is a Ship']\n\ndf.sort_values('Difference', ascending=True).head(10)","8137ee6e":"# Finding the images where the model was the most certain that is was a ship when it wasn't\n\ndf.sort_values('Difference', ascending=False).head(10)","0fd3e028":"# Plotting the top 4 from each incorrect guess\n\nplt.figure(figsize=(15, 15))\nplt.subplot(2,4,1)\nplt.title('Not A Ship')\nplt.imshow(X_test[537])\nplt.subplot(2,4,2)\nplt.title('Not A Ship')\nplt.imshow(X_test[518])\nplt.subplot(2,4,3)\nplt.title('Not A Ship')\nplt.imshow(X_test[322])\nplt.subplot(2,4,4)\nplt.title('Not A Ship')\nplt.imshow(X_test[206])\nplt.subplot(2,4,5)\nplt.title('Ship')\nplt.imshow(X_test[528])\nplt.subplot(2,4,6)\nplt.title('Ship')\nplt.imshow(X_test[628])\nplt.subplot(2,4,7)\nplt.title('Ship')\nplt.imshow(X_test[662])\nplt.subplot(2,4,8)\nplt.title('Ship')\nplt.imshow(X_test[3])\nplt.subplots_adjust(bottom=0.3, top=0.7, hspace=0.25)","d715a25f":"# Plotting a histogram plot of the pixel intensities to see if I can spot a common pattern as to why they are misclassifying\n# those images as ships\n\n# I couldn't really notice any pattern\n\ndef plotHistogram(a):\n\n    plt.figure(figsize=(10,5))\n    plt.subplot(1,2,1)\n    plt.imshow(a)\n    plt.axis('off')\n    plt.title('Ship' if y[1] else 'Not A Ship')\n    histo = plt.subplot(1,2,2)\n    histo.set_ylabel('Count')\n    histo.set_xlabel('Pixel Intensity')\n    n_bins = 30\n    plt.hist(a[:,:,0].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.5);\n    plt.hist(a[:,:,1].flatten(), bins= n_bins, lw = 0, color='g', alpha=0.5);\n    plt.hist(a[:,:,2].flatten(), bins= n_bins, lw = 0, color='b', alpha=0.5);","101cf859":"plotHistogram(X_test[537])","5df964d1":"plotHistogram(X_test[518])","e6639bd0":"plotHistogram(X_test[322])","f1495e2c":"plotHistogram(X_test[206])","98c6f9c3":"# Using a 3x3 kernel and a pooling size of 2x2 at the start of the sequence\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), input_shape=(80, 80, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])","5425e04f":"# Training the model\n\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))","b5edf44e":"# Finding our final accuracy on the test set\n\nmodel.evaluate(X_test, y_test)","1e25351e":"# Plotting the loss and accuracy\n\npd.DataFrame(history.history).plot();","525f3502":"# Using a 5x5 kernel and a pooling size of 3x3 at the start of the sequence\n\n# Increasing the number of nodes in the first convolution layer \n\n# Increasing the pool size in the first pool layer\n\nmodel = Sequential()\nmodel.add(Conv2D(64, (5, 5), input_shape=(80, 80, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Conv2D(32, (3, 3), input_shape=(80, 80, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])","3295ed9b":"# Training the model\n\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))","ad2c5a68":"# Finding our final accuracy on the test set\n\nmodel.evaluate(X_test, y_test)","90d7fff7":"# Plotting the loss and accuracy\n\npd.DataFrame(history.history).plot();","387f818b":"# Using a 5x5 kernel and a pooling size of 3x3 at the start of the sequence\n\n# Increasing the number of nodes in the first convolution layer \n\n# Increasing the pool size in the first pool layer\n\n# Adding an extra convolutional layer + drop out layers to prevent over fitting\n\n# Adding an extra dense layer of 50 neurons towards the end\n\n# Increasing the epochs to 50\n\n# Setting my computer on fire\n\nmodel = Sequential()\nmodel.add(Conv2D(64, (5, 5), input_shape=(80, 80, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(32, (3, 3), input_shape=(80, 80, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(32, (3, 3), input_shape=(80, 80, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])","0ef180e7":"# Training the model\n\nhistory = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))","dadd0d67":"# Finding our final accuracy on the test set\n\nmodel.evaluate(X_test, y_test)","c64b5d87":"# Plotting the loss and accuracy\n\npd.DataFrame(history.history).plot();","c83852ea":"<center>\n    <img src=\"https:\/\/i.insider.com\/605f4ca58e71b30018519272?width=1136&format=jpeg\" width='100%'> \n<\/center>\n\n----------\n\n<h1 align=\"center\"> Ship Satellite Image Classification <\/h1>\n<br>\n<center align=\"center\"> <font size='4'>  Developed by: <\/font><font size='4' color='#33AAFBD'>Paul O'Neill <\/font><\/center>\n<br>\n\n----------","c525bf69":"# Creating the Train\/Test Split + Validation Set","18cc6c75":"# Trying a Convolutional Neural Network Model","52add71d":"# Importing the Libraries","d85531e0":"# Exploring the Images","3e6eb916":"# Adding More Epochs","686dc265":"# Uploading the Images","07ea2ed5":"# Reshaping the Data","6a67683a":"# Adding More Layers + Softmax Activation","68da00c4":"# Adding Another Convolution and Pooling Layer","6f34e8b9":"# Setting up a Baseline Model","f352f8bd":"# Adding More Layers + Drop Out Layers + Increasing Epochs"}}