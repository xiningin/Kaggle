{"cell_type":{"ed118026":"code","28662790":"code","270cf679":"code","d0babaff":"code","32ed6ed8":"code","88883be7":"code","ed50fb61":"code","8ba67e44":"code","48fed1da":"code","343f2ac8":"code","cf289c33":"code","2d0611d3":"code","6fcd88d5":"code","f11466bc":"code","349edcd2":"code","acf78908":"code","10dc8bf8":"code","852ffe04":"code","8a1602dd":"code","0891f4d7":"code","8d367be2":"code","4b3370bb":"code","de5d130a":"code","e8c3cb74":"code","7fc94f54":"markdown","ae2fa0e9":"markdown","8b424494":"markdown","0fd2c9f8":"markdown","3db7e264":"markdown","088871fb":"markdown","761c053b":"markdown","eee3ebdb":"markdown","2556fc70":"markdown","49643056":"markdown","da227170":"markdown","49196d40":"markdown","3e8fed85":"markdown"},"source":{"ed118026":"!pip install git+https:\/\/github.com\/keras-team\/keras-tuner.git -q","28662790":"import os, sys, gc\nimport time, random\nimport numpy as np\nimport pandas as pd\nimport logging\nimport typing as tp\nfrom pathlib import Path\nfrom contextlib import contextmanager\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.callbacks import *\n\nprint('TF version:', tf.__version__)\nprint('GPU devices:', tf.config.list_physical_devices('GPU'))\nprint(\"GPU available: \", tf.test.is_gpu_available())","270cf679":"TARGET = \"target\"\n# IDVAR = 'id'\nN_OUTS = 1\nBS = 128\nEPOCHS = 10\n\nDIR = \"..\/input\/tabular-playground-series-jan-2021\/\"\nWORK = \".\/\"\n\n\nnum_cols = [f'cont{i}' for i in range(1,15)]\nall_cols = num_cols  # + string_cols + categorical_cols","d0babaff":"train = pd.read_csv(DIR+\"train.csv\")\ntrain['id'] = train.index\n# train_labels = train[TARGET].values\n\ntest = pd.read_csv(DIR+\"test.csv\")\ntest_index = test['id']\n\nsub = pd.read_csv(DIR+\"sample_submission.csv\")\n\nprint('Raw data loaded!')\nprint(\"Train: {}, Test: {}, Sample sub: {}\".format(train.shape, test.shape, sub.shape))\n\n\n# split to train\/valid sets\nprint('Split to train\/valid sets:\\n')\nval_df = train[all_cols].sample(frac=0.2, random_state=2020)\ntrain_df = train[all_cols].drop(val_df.index)\nprint('Train shape:', train_df.shape)    \nprint('Valid shape:', val_df.shape)  ","32ed6ed8":"display(train.sample(5))","88883be7":"# train[[TARGET]].plot(figsize=(16, 8));","ed50fb61":"from tensorflow.keras.layers.experimental.preprocessing import Normalization\n\ndef encode_numerical_feature(feature, name, dataset):\n    normalizer = Normalization()                    # Create a Normalization layer for each feature\n    feature_ds = dataset.map(lambda x, y: x[name])  # Prepare a TF-Dataset that only yields our feature\n    normalizer.adapt(feature_ds)                    # Learn the statistics of the data\n    encoded_feature = normalizer(feature)           # Normalize the input feature\n    return encoded_feature\n","8ba67e44":"# code part coppied from: https:\/\/www.kaggle.com\/nicapotato\/keras-nn-tabular-regression-problem\n\ndef dataframe_to_dataset(dataframe, labels, role, BATCHSIZE):\n    dataframe = dataframe.copy()\n    if role != \"test\":\n        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    else: \n        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if role == \"train\":\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(BATCHSIZE)\n    return ds","48fed1da":"train_ds = dataframe_to_dataset(train_df, train.loc[train_df.index, TARGET], \"train\", BS)\nval_ds = dataframe_to_dataset(val_df, train.loc[val_df.index, TARGET], \"val\", BS)\ntest_ds = dataframe_to_dataset(test[all_cols], np.zeros((test.shape[0], N_OUTS)), \"test\", BS)\n\n# full dataset\nfull_train_ds = dataframe_to_dataset(train[all_cols], train[TARGET], \"train\", BS)\n\nprint('Training ds steps:', int(train_ds.cardinality()))\nprint('Validation ds steps:', int(val_ds.cardinality()))\nprint('Test ds steps:', int(test_ds.cardinality()))\nprint()\nprint('Full Training ds steps:', int(full_train_ds.cardinality()))\n\n# train_ds = train_ds.shuffle(1024).batch(BS).prefetch(8)\n# val_ds = val_ds.batch(BS).prefetch(8)\n# test_ds = test_ds.batch(BS).prefetch(8)","343f2ac8":"# sanity check \n\n# import pprint as pp\n\n# print('Look at Data')\n# for x, y in val_ds.take(1):\n#     pp.pprint(x)\n#     pp.pprint(y)","cf289c33":"# We use TF Functional API to create aour NN model\n# For more info see here: \n\n\ndef base_model():\n\n    num_inputs = [Input(shape=(1,), name=x) for x in num_cols]\n    encoded_nums = [encode_numerical_feature(var_input, var_name, train_ds)\n                   for var_input, var_name in zip(num_inputs, num_cols)]\n    \n    all_feats = Concatenate()(encoded_nums)\n\n    x = Dropout(0.2)(all_feats)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(32, activation=\"relu\")(x)\n    x = Dropout(0.2)(x)\n    out = Dense(1, activation='linear')(x)\n    base_model = tf.keras.Model(num_inputs, out)\n    \n    # compile model \n    base_model.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=2e-3),\n        loss=\"mse\",\n        metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n\n    return base_model","2d0611d3":"base_model = base_model()\nbase_model.summary()","6fcd88d5":"# set callbacks \nes = EarlyStopping(monitor='val_loss', min_delta=0.0001,patience=5, verbose=1, mode='min',restore_best_weights=True)\nrlr = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=0.00001, verbose=0)\n# ckp = callbacks.ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\nhist = base_model.fit(train_ds, \n                      batch_size=BS, \n                      epochs=EPOCHS,\n                      validation_data=val_ds, \n                      verbose=1, \n                      callbacks=[es, rlr])","f11466bc":"# from nb: https:\/\/www.kaggle.com\/nicapotato\/keras-nn-tabular-regression-problem\/\n\nplot_metrics = ['loss', 'rmse']\n\nf, ax = plt.subplots(1,2,figsize = [12,4])\nfor p_i,metric in enumerate(plot_metrics):\n    ax[p_i].plot(hist.history[metric], label='Train ' + metric, )\n    ax[p_i].plot(hist.history['val_' + metric], label='Val ' + metric)\n    ax[p_i].set_title(\"Loss Curve - {}\".format(metric))\n    ax[p_i].set_ylabel(metric.title())\n    ax[p_i].legend()\nplt.show()","349edcd2":"import kerastuner as kt\n\ndef make_model(hp):\n    \n    num_inputs = [Input(shape=(1,), name=x) for x in num_cols]\n    encoded_nums = [encode_numerical_feature(var_input, var_name, train_ds)\n                   for var_input, var_name in zip(num_inputs, num_cols)]\n    all_feats = Concatenate()(encoded_nums)\n    x = all_feats\n    \n    num_layers = hp.Int('num_layers', min_value=2, max_value=5, step=1)\n    for i in range(num_layers):\n        units = hp.Int(f'units_{i}', min_value=128, max_value=512, step=64)\n        dp = hp.Float(f'dp_{i}', min_value=0., max_value=0.5)\n        x = Dropout(dp)(x)\n        x = Dense(units, activation='relu')(x)\n    \n    dp = hp.Float('final_dp', min_value=0.05, max_value=0.5)\n    x = Dropout(dp)(x)\n    outputs = Dense(1, activation='linear')(x)\n    model = tf.keras.Model(num_inputs, outputs)\n\n    lr = hp.Float('learning_rate', min_value=1e-4, max_value=5e-2)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) # 1e-3\n    model.compile(loss='mse',\n                  optimizer=optimizer,\n                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n    #     model.summary()\n    return model\n\n","acf78908":"# set KerasTurner\n\nMAX_TRIALS = 10  # 5  \n# Set to 5 for a quick run, but need 100+ for good results\n\n\ntuner = kt.tuners.BayesianOptimization(\n    make_model,\n    objective=kt.Objective('val_rmse', direction=\"min\"),  # 'val_loss',\n    max_trials=MAX_TRIALS,          \n    overwrite=True)\n\ntuner.search(train_ds, \n             validation_data=val_ds, \n             callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=5)], \n             epochs=60)","10dc8bf8":"def get_trained_model(hp):\n    model = make_model(hp)\n    # First, find the best number of epochs to train for\n    callbacks=[EarlyStopping(monitor='val_rmse', mode='min', patience=5, restore_best_weights=True)]\n    hist = model.fit(train_ds, validation_data=val_ds, epochs=50, callbacks=callbacks)\n    val_loss_per_epoch = hist.history['val_rmse']\n    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n    print('Best epoch: %d' % (best_epoch,))\n    # Increase epochs by 20% when training on the full dataset\n    model = make_model(hp)\n    model.fit(full_train_ds, epochs=int(best_epoch * 1.2), verbose=0)\n    return model","852ffe04":"# select top-N models\n\nn = 3     # e.g. n=10 for top ten models\nbest_hps = tuner.get_best_hyperparameters(n)\n\nall_preds = []\nfor hp in best_hps:\n    model = get_trained_model(hp)\n    preds = model.predict(test_ds)\n    all_preds.append(preds)","8a1602dd":"for i in range(n):\n    sns.distplot(all_preds[i])\nplt.show()","0891f4d7":"preds = np.zeros(shape=(len(test), 1))\nfor p in all_preds:\n    preds += p\npreds \/= len(all_preds)","8d367be2":"sub['target'] = preds\nsub.to_csv('nn_model.csv', index=False)\n\nprint('Submit!')\nsub.head(10)","4b3370bb":"sub['target'].plot(figsize=(16,4));","de5d130a":"sub['target'].iloc[:1000].plot(figsize=(16,4));","e8c3cb74":"sns.distplot(sub['target'])","7fc94f54":"# Optimize with Keras Tuner","ae2fa0e9":"### WiP - The notebook will be updated constantly with new tasks if there is interest. \n\n### Feel free to ask any questions you might have in the comments bellow \n\nNext steps: \n\n- Try different SoA optimizers\n\n- Try different LR schedulers \n\n- Tune different sets of hyperparams \n\n","8b424494":"# Training a baseline model","0fd2c9f8":"Here we use KerasTuner to make a hyperparameter search for our NN configuration. \n\nFor demo we use the following hyperparams: \n\n- `num_layers` (`Int`): The no. of layers in our NN (shallow or deep NN)\n\n- `units_i` (`Int`): The no. of dense neurons for each layer-i \n\n- `dp_i` (`Float`): The dropout rate for each layer-i \n\n- `final_dp` (`Float`): The dropout rate for at the last layer before output\n\n- `learning_rate` (`Float`): The learning rate for our optimizer","3db7e264":"# Ensemble predictions from top-N models","088871fb":"# Submit","761c053b":"# Import libraries","eee3ebdb":"## NN Starter + Keras Tuner Ensemble\n\nThis notebook will show you steb-by-step how to:\n\n- Use a TF-Keras neural network for tabular data (regression)\n- Use `KerasTuner` to find high-performing model configurations\n- Ensemble a few of the top models to generate final predictions\n\nReferences:\n\n- https:\/\/www.kaggle.com\/fchollet\/moa-keras-kerastuner-best-practices\n- https:\/\/github.com\/keras-team\/keras-tuner\n\nNote: This notebook is addressed more to newers\/mid-level in deep learning, aligned with the purpose of this Playground competition. \nExperienced kagglers probably won't learn anything new. ","2556fc70":"1) Encode our features\n\nFirst we need to encode our input variables before passing them to the NN. \nSince we have only numerical features we use a single `Normalization` layer to encode each feature separately.\nThen, we concatenate the entire feature space into a single vector. \n\nWe wrap all the above steps in the following python method: `encode_numerical_feature` ","49643056":"Let's turn our dataframes into `tf.data.Dataset`, which we will use to train our Keras models in the next step.\nThe following method: `dataframe_to_dataset` does exactly that. ","da227170":"# Prepare Dataset","49196d40":"# Load data","3e8fed85":"## Reinstantiate the top N models and train them on the full dataset"}}