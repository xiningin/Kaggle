{"cell_type":{"4aee1b22":"code","55cd76fb":"code","70ae2fad":"code","f2821eed":"code","89156af2":"code","7c033923":"code","3cd725f0":"code","5abef389":"code","a2df6b8c":"code","f45306f9":"code","00db7f65":"code","12a3db85":"code","7372d763":"code","1d7e9780":"code","9d623883":"code","8a16cf36":"code","b7399f23":"code","d66a6a2b":"code","06ea7047":"code","e8be2b31":"markdown","d1d98ea9":"markdown","985f7089":"markdown","744095f4":"markdown","599d130f":"markdown","5386b52d":"markdown"},"source":{"4aee1b22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","55cd76fb":"#Firstly we read our data\ndata = pd.read_csv(\"..\/input\/SPAM text message 20170820 - Data.csv\")","70ae2fad":"data.head()\n","f2821eed":"#We must change category 1 or 0\ndata[\"Category\"] = [1 if each == \"spam\" else 0 for each in data[\"Category\"]]","89156af2":"data.head()","7c033923":"#We choose 1 row.And we throw punctuation\nimport re\nnlp_data = str(data.iloc[2,:])\nnlp_data = re.sub(\"[^a-zA-Z]\",\" \",nlp_data)","3cd725f0":"#After return lower case\nnlp_data = nlp_data.lower()","5abef389":"#we have two choice we can use split methot or tokenize\nimport nltk as nlp\nnlp_data = nlp.word_tokenize(nlp_data)\n#nlp_data = nlp_data.split() or we can do so","a2df6b8c":"#we have to find word root\nlemma = nlp.WordNetLemmatizer()\nnlp_data = [lemma.lemmatize(word) for word in nlp_data]","f45306f9":"#We join our data\nnlp_data = \" \".join(nlp_data)","00db7f65":"import nltk as nlp\nimport re\ndescription_list = []\nfor description in data[\"Message\"]:\n    description = re.sub(\"[^a-zA-Z]\",\" \",description)\n    description = description.lower()   # buyuk harftan kucuk harfe cevirme\n    description = nlp.word_tokenize(description)\n    #description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n    lemma = nlp.WordNetLemmatizer()\n    description = [ lemma.lemmatize(word) for word in description]\n    description = \" \".join(description)\n    description_list.append(description) #we hide all word one section","12a3db85":"#We make bag of word it is including number of all word's info\nfrom sklearn.feature_extraction.text import CountVectorizer \nmax_features = 3000 #We use the most common word\ncount_vectorizer = CountVectorizer(max_features = max_features, stop_words = \"english\")\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()\nprint(\"the most using {} words: {}\".format(max_features,count_vectorizer.get_feature_names()))","7372d763":"#We separate our data is train and test\ny = data.iloc[:,0].values   # male or female classes\nx = sparce_matrix\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)","1d7e9780":"#We make model for predict\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"the accuracy of our model: {}\".format(nb.score(x_test,y_test)))","9d623883":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter = 200)\nlr.fit(x_train,y_train)\nprint(\"our accuracy is: {}\".format(lr.score(x_test,y_test)))","8a16cf36":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train,y_train)\n#print('Prediction: {}'.format(prediction))\nprint('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test))","b7399f23":"x_test = x_test.reshape(558,3000,1)\n","d66a6a2b":"x_train = x_train.reshape(5014,3000,1)","06ea7047":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\n\n# Initialising the RNN\nregressor = Sequential()\n\n# Adding the first LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))\nregressor.add(Dropout(0.2))\n\n# Adding a second LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a third LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a fourth LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50))\nregressor.add(Dropout(0.2))\n\n# Adding the output layer\nregressor.add(Dense(units = 1))\n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error',metrics=[\"accuracy\"])\n\n# Fitting the RNN to the Training set\nregressor.fit(x_test, y_test, epochs = 3, batch_size = 32)\n","e8be2b31":"**Conclusion**\n\nI hope you like our kernel.If you have questions you ask me i back you.If you UPVOTE me \u0131 will be happy.Stay you good","d1d98ea9":"**NLP**\n\nWe do this process to all data with for loop","985f7089":"**How can do NLp**\n\n* Firstly we throw punctuation(\"\",.:;)\n* Then we split all word\n* Then we return all word lower case\n* We find word root with lemmatizer\n* We make bag of word(It is including the most using word and sentence's info)\n* We make prediction model ","744095f4":"**Welcome to NLP**\n\nToday we will have done nlp algorithm and we talk about \"how we do nlp from 0\".Firstly I talk you how we do after we do this.PLEASE UPVOTE and FEEDBACK.I hope it is benefit for you.\n","599d130f":"**Example**\n\nIt is example for you learn.I will do one row but \u0131 will have all row in below","5386b52d":"And we have list is including all word now we have to prepare predict model but we will do the rest in below"}}