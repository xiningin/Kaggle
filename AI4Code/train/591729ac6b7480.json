{"cell_type":{"68d7d0c4":"code","f236ac9b":"code","49d36632":"code","357b2af3":"code","074bb923":"code","48647fdd":"code","189078ef":"code","540ced16":"code","185809c3":"code","8e0c713f":"code","fc06110e":"code","59dbd856":"code","fcea9124":"code","d97564e4":"code","cdee1c63":"code","6ebc61d6":"code","af966603":"code","ae32c0bf":"code","4f4b43b2":"code","ad9535af":"code","d6b9d62e":"code","6cd4e7f8":"code","eafc1689":"code","63b1ce0e":"code","c286e3f6":"code","f1843bc9":"code","447e07bb":"code","f4134579":"code","a0d4aab4":"code","d3aae982":"code","43e925de":"code","52263b02":"code","5d10d02e":"code","3058b5a7":"code","6593c2b6":"code","f74accf3":"code","4ca05cd2":"markdown","eb4d64ca":"markdown","90fded88":"markdown","312a07cf":"markdown","a35de3c8":"markdown","711c03e2":"markdown","2fe573f0":"markdown","de6b58ed":"markdown","45cf6f5c":"markdown","94dc0413":"markdown","44fd7c64":"markdown","2e8413d6":"markdown"},"source":{"68d7d0c4":"import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt, seaborn as sns","f236ac9b":"df=pd.read_csv(\"..\/input\/mall-customers-kmeans-pcacsv\/Mall_Customers_Kmeans_PCA.csv\")\ndf.head()","49d36632":"df.rename({'Annual Income (k$)': 'Income',\n          'Spending Score (1-100)':'Spend_Score'}, axis=1,inplace=True)","357b2af3":"df.plot.scatter(x='Income', y='Spend_Score', color='gray')\nplt.show()","074bb923":"mall_scaled = df.copy()\ncols_to_scale = ['Age', 'Income', 'Spend_Score']\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nmall_scaled[cols_to_scale] = scaler.fit_transform(mall_scaled[cols_to_scale])","48647fdd":"mall_scaled[cols_to_scale].describe()","189078ef":"from sklearn.cluster import KMeans\ncluster_cols = ['Income', 'Spend_Score']\nmarkers = ['x', '*', '.', '|', '_', '1', '2']","540ced16":"plt.figure(figsize=[18,12])\nfor n in range(2,8):\n    model = KMeans(n_clusters=n, random_state=42)\n    mall_scaled['Cluster']= model.fit_predict(mall_scaled[cluster_cols])\n    \n\n    plt.subplot(2,3, n-1)\n    for clust in range(n):\n        temp = mall_scaled[mall_scaled.Cluster == clust]\n        plt.scatter(temp.Income, temp.Spend_Score, \\\n                marker=markers[clust], \\\n                label=\"Cluster \"+str(clust), color='gray')\n        plt.title(\"N clusters: \"+str(n))\n        plt.xlabel('Income')\n        plt.ylabel('Spend_score')\n        plt.legend()\n\nplt.show()","185809c3":"K = 3\nmodel = KMeans(n_clusters=K, random_state=42)\nmodel.fit(mall_scaled[cluster_cols])","8e0c713f":"print(model.inertia_)\n\nprint(\"You will see that inertia is 157.70. Note that this number by itself does not mean much to us. We are more interested in how this number changes with the number of clusters\")","fc06110e":"X = mall_scaled[cluster_cols]\ninertia_scores = []\nfor K in range(2,11):\n    inertia = KMeans(n_clusters=K, random_state=42).fit(X).inertia_\n    inertia_scores.append(inertia)","59dbd856":"plt.figure(figsize=[7,5])\nplt.plot(range(2,11), inertia_scores, color='gray')\nplt.title(\"SSE\/Inertia vs. number of clusters\")\nplt.xlabel(\"Number of clusters: K\")\nplt.ylabel('SSE\/Inertia')\nplt.show()\n\n\nprint(\"By observing the preceding plot, you will notice that there is an elbow in the plot at K=5. So, we take five as the optimal number of clusters, the best value of K for the KMeans algorithm. Before that, every additional cluster gives us big gains in reducing the sum of squared errors. Beyond five, we seem to be getting extremely low returns.\")","fcea9124":"from sklearn.cluster import MeanShift, estimate_bandwidth\nbandwidth=0.9","d97564e4":"ms=MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(mall_scaled[cluster_cols])\n\nmall_scaled['Cluster1'] = ms.predict(X)","cdee1c63":"markers = ['x', '*', '.', '|', '_', '1', '2']\nplt.figure(figsize=[8,6])\nfor clust in range(mall_scaled.Cluster1.nunique()):\n    temp = mall_scaled[mall_scaled.Cluster1 == clust]\n    plt.scatter(temp.Income, temp.Spend_Score, \\\n                marker=markers[clust], \\\n                label=\"Cluster\"+str(clust), \\\n                color='gray')\n\nplt.xlabel(\"Income\")\nplt.ylabel(\"Spend_score\")\nplt.legend()\nplt.show()\n\n\nprint(\"The model has found five unique clusters. They are very much aligned with the clusters you arrived at earlier using K-means where you specified 5 clusters. But notice that the clusters on the right have areas of very low density. The choice of bandwidth has led to such loose clusters. \")","6ebc61d6":"print(\"\"\"Estimate the required bandwidth using the estimate_bandwidth method. \nUse the estimate_bandwidth function with a quantile value of 0.1 (an \narbitrary choice) to estimate the best bandwidth to use.\"\"\") \n\nbandwidth = estimate_bandwidth(mall_scaled[cluster_cols], \\\n                               quantile=0.1)\nprint(bandwidth)","af966603":"ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(mall_scaled[cluster_cols])\nmall_scaled['Cluster']= ms.predict(mall_scaled[cluster_cols])\nmall_scaled.Cluster.nunique()","ae32c0bf":"plt.figure(figsize=[8,6])\nfor clust in range(mall_scaled.Cluster.nunique()):\n    temp = mall_scaled[mall_scaled.Cluster == clust]\n    plt.scatter(temp.Income, temp.Spend_Score, \\\n                marker=markers[clust], \\\n                label=\"Cluster\"+str(clust), \\\n                color='gray')\nplt.xlabel(\"Income\")\nplt.ylabel(\"Spend_score\")\nplt.legend()\nplt.show()","4f4b43b2":"## Changint eh quantile to 0.15\n\nbandwidth = estimate_bandwidth(mall_scaled[cluster_cols], \\\n                               quantile=0.15)\nprint(bandwidth)","ad9535af":"ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(mall_scaled[cluster_cols])\nmall_scaled['Cluster']= ms.predict(mall_scaled[cluster_cols])\nmall_scaled.Cluster.nunique()","d6b9d62e":"plt.figure(figsize=[8,6])\nfor clust in range(mall_scaled.Cluster.nunique()):\n    temp = mall_scaled[mall_scaled.Cluster == clust]\n    plt.scatter(temp.Income, temp.Spend_Score, \\\n                marker=markers[clust], \\\n                label=\"Cluster\"+str(clust), \\\n                color='gray')\nplt.xlabel(\"Income\")\nplt.ylabel(\"Spend_score\")\nplt.legend()\nplt.show()\n\nprint(\"\"\"In this exercise, you successfully used mean-shift clustering with varying parameters \nto make your understanding more concrete. When you used a quantile value of \n0.15, which means you looked at more points to estimate the bandwidth required, \nyou ended up with a bandwidth of about 0.86 and obtained 5 clusters. When you \nused a quantile value of 0.1, though, the estimated bandwidth was about 0.65 and \nobtained 7 clusters. This demonstrates the impact of the bandwidth parameter, \nalternatively, the quantile parameter used to estimate the bandwidth.\"\"\")","6cd4e7f8":"bank0 = pd.read_csv(\"..\/input\/segmentation\/Bank_Personal_Loan_Modelling-1.csv\")\n\n\n\nprint(\"\"\"An important feature for business is the education level of the customer and needs to \nbe included in the segmentation. The values in the data are Primary, Secondary, \nand Tertiary. Since this is a categorical feature, K-means is not a suitable \napproach. You need to create customer segmentation with this data by applying \nk-prototype clustering to data that has a mix of categorical (education) and \ncontinuous (income) variables\"\"\")","eafc1689":"bank0.head()","63b1ce0e":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nbank_scaled = bank0.copy()\nbank_scaled['Income'] = scaler.fit_transform(bank0[['Income']])","c286e3f6":"from kmodes.kprototypes import KPrototypes\ncluster_cols = ['Income', 'Education']\n\nkp = KPrototypes(n_clusters=3, random_state=42)\n\nbank_scaled['Cluster'] = kp.fit_predict(bank_scaled[cluster_cols],categorical=[1])","f1843bc9":"res = bank_scaled.groupby('Cluster')['Education'].value_counts(normalize=True)\n\nres.unstack().plot.barh(figsize=[9,6],color=['black','lightgray','dimgray'])\nplt.show()\n\n\nprint(\"\"\"cluster 2 is dominated by customers with \nprimary education. In cluster 1, the number of primary educated customers \nroughly equals the number of secondary and tertiary educated customers \ntogether. In cluster 0, customers with higher education (secondary or \ntertiary) significantly outnumber those with primary education.\"\"\")","447e07bb":"cluster_cols = ['Income', 'Spend_Score']\nX = mall_scaled[cluster_cols]\nmodel = KMeans(n_clusters=3, random_state=42)\ncluster_assignments = model.fit_predict(X)","f4134579":"from sklearn.metrics import silhouette_score\nsilhouette_avg = silhouette_score(X, cluster_assignments)\nprint(silhouette_avg)","a0d4aab4":"silhouette_scores = []\nfor K in range(2, 11):\n    model = KMeans(n_clusters=K, random_state=42)\n    cluster_assignments = model.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_assignments)\n    silhouette_scores.append(silhouette_avg)","d3aae982":"silhouette_scores","43e925de":"plt.figure(figsize=[7,5])\nplt.plot(range(2,11), silhouette_scores, color='gray')\nplt.xlabel(\"Number of clusters: K\")\nplt.ylabel('Avg. Silhouette Score')\nplt.show()\n\nprint(\"\"\"you can infer that K=5 has the best silhouette score and is therefore the optimal number of clusters.\"\"\")\n","52263b02":"from sklearn.model_selection import train_test_split\n\ndf_train, df_test=train_test_split(df, train_size=0.75, random_state=42)","5d10d02e":"print(df_train.shape)\nprint(df_test.shape)","3058b5a7":"model = KMeans(n_clusters=6, random_state=42)\ndf_train['Cluster'] = model.fit_predict(df_train[cluster_cols])\nsilhouette_avg = silhouette_score(df_train[cluster_cols], df_train['Cluster'])\nprint(silhouette_avg)","6593c2b6":"df_test['Cluster'] = model.predict(df_test[cluster_cols])\nsilhouette_avg = silhouette_score(df_test[cluster_cols], df_test['Cluster'])\nprint(silhouette_avg)","f74accf3":"for clust in range(df_test.Cluster.nunique()):\n    temp = df_test[df_test.Cluster == clust]\n    plt.scatter(temp.Income, temp.Spend_Score, \\\n                marker=markers[clust], \\\n                color='gray')\nplt.xlabel(\"Income\")\nplt.ylabel(\"Spend_score\")\nplt.show()","4ca05cd2":"## Test_Train_Split","eb4d64ca":"## More CLustering Techniques\n\n**Mean-shift clustering** is an interesting algorithm in contrast to the k-means \nalgorithm because unlike k-means, it does not require you to specify the number of \nclusters. The intuition of its working is rather simple \u2013 it works by starting at each \ndata point and shifting the data points (assigning them to clusters) toward the area \nof greatest density \u2013 that is, towards a natural cluster centroid. When all the data \npoints have found their local density peak, the algorithm is complete. This tends to \nbe computationally expensive, so this method does not scale well to large datasets \n(k-means clustering, on the other hand, scales very well).\n\nWhile not needing to choose the number of clusters sounds great, there is another \nhyper-parameter that strongly influences the behavior of the algorithm - bandwidth. \nAlso referred to as window size, bandwidth defines how far each data point will look \nwhen searching for a higher density area. As you can expect, a higher bandwidth \nwould allow points to look farther and get linked to farther away clusters and can \nlead to fewer, looser, larger clusters. \n\nA common method (which we will use shortly) for determining the best bandwidth \nis to estimate it based on the distances between nearby points (using a quantile \nparameter which specifies the proportion of data points to look across), but this \nmethod requires you to choose a quantile that determines the proportion of points to \nlook at. This is non-trivial. In practice, this ends up being a very similar problem to the \nproblem of choosing a number of clusters where at some point you, the user, have to \nchoose what hyperparameter to use.\n\n\n\n\n\n\n\n","90fded88":"## Benefits and Drawbacks of the Mean-Shift Technique \n\nIn the previous exercise, we saw that the mean-shift algorithm too had its own key \nhyper-parameters. This is again a choice to be made by the user. Why, then, bother \nwith mean-shift clustering? To answer this let us understand the benefits and \ndrawbacks of the mean-shift algorithm. \nBenefits of mean-shift algorithm\n1. We don't need to pre-specify the number of clusters.\n\n2. The single parameter, bandwidth, has a physical meaning and its effects are easy to interpret.\n\n3. It can identify complex-shaped clusters (k-means only gave spherical\/globular clusters).\n\n4. Robust to outliers.\n\nDrawbacks of mean-shift algorithm\n\n1. Computationally expensive, doesn't scale well to large datasets.\n\n2. Does not work well with a high number of dimensions (leads to unstable clusters).\n\n3. No direct control over the number of clusters, which is problematic when we have business constraints on the number of clusters.\n\nWe can see that the **mean-shift** algorithm is another powerful, density-based \napproach to clustering. While it has its own hyper-parameter and some issues with \nscalability, it does have its merits which can be extremely useful in certain situations. \nBut both approaches we saw so far work only for quantitative data. In practice, we \ncome across many situations that need us to work with non-numeric data. Let us \nnow explore another technique for clustering, that helps us handle different types of \ndata better.\n","312a07cf":"you have successfully used k-prototypes clustering to segment \npeople based on their Income and Education levels. A visual analysis gave insight into \nthe nature of the clusters. Visual analysis is good but brings a lot of subjectivity and \nisn't always a good idea when dealing with high-dimensional data. It is always good to \nhave quantitative measures for evaluating clusters. ","a35de3c8":"### Scaling the attributes","711c03e2":"### The Elbow Method with Sum of Squared Errors (inertia)","2fe573f0":"#### The silhouette score is 0.495, which is a big drop from 0.545 on the train set. To understand the cause for this drop, you'll need to visualize the clusters on the test data","de6b58ed":"## Choosing the Number of Clusters\n\n**1) Choosing the Number of Clusters Based on Visual Inspection**\n\nChoosing the number of clusters based on visual inspection is often appealing \nbecause it is a decision based on looking at what is happening with the data most \ndirectly. People are usually quite good at looking at how much different clusters \noverlap and deciding whether a given number of clusters leads to too much overlap. \nThis is not a quantitative method; however, as it leaves a lot to subjectivity and \nindividual judgment, for many simple problems, it is a great way to decide how many \nclusters to use.\n\n**2) The Elbow Method with Sum of Squared Errors (inertia)**\n\nOften, it is difficult to tell by visualization alone how many clusters should be used \nto solve a particular problem. Different people may disagree about the number \nof clusters to use, and there may not be a clear, unanimous answer. With higher \ndimensional data, there is an additional issue: dimensionality-reduction techniques \nare not perfect. They attempt to take all the information in multiple dimensions \nand reduce it to only two. In some cases, this can work well, but as the number of \ndimensions increases, the data becomes more complex, and these visual methods \nquickly reach their limitations. When this happens, it's not easy to determine through \nvisual inspection what the right number of clusters to use is. In these cases, it's often \nbetter to reach for a more quantitative measure. One such classic measure is to look \nfor an elbow in a plot of the sum of squared errors, also called an Inertia Plot.\n\nThe sum of squared errors (SSE) is the sum of the \"errors\" (the difference between \na data point and the centroid of its assigned cluster) for all data points, squared. \nAnother term for it is inertia. The tighter the clusters, the closer the constituent points \nto their respective clusters, and the lower the SSE\/inertia. The sum of squared errors \nfor the model can be calculated using the following equation:\nFigure 4.5: Equation for calculating the sum of squared errors \nof data points in a dataset\nHere, \u03bck\n is the location of the centroid of cluster k, and each xi\n is a data point assigned \nto cluster k. When all the entities are treated as a single cluster, this SSE value is at \nits maximum for the dataset. As we increase k, we should expect the sum of squared \nerrors to decrease since there are more centroids. In the extreme case, when each \npoint is a cluster, the SSE\/inertia value is 0, as each point is the centroid for its own \ncluster. In scikit-learn, you will use the 'inertia_' attribute that is available after \nfitting a model.","45cf6f5c":"### Choosing the Number of Clusters Based on Visual Inspection","94dc0413":"What do you gather from the plot? First, the top right cluster doesn't seem to be \na good one. There are two points that are far from the dense part of the cluster. \nThis is not a tight cluster. Second, the bottom right cluster contains just two \npoints, both of which are very close to another cluster. This cluster should be \nmerged with the adjacent cluster.","44fd7c64":"## k-modes and k-prototypes Clustering\n\n\nk-means clustering is great when you are dealing exclusively with quantitative data. \nHowever, when you have categorical data (that is, data that can't be converted into \nnumerical order, such as race, language, and country) with more than two categories, \nthe representation of this data using numbers becomes a key consideration. In \nstatistics, one common strategy for dealing with categorical data is to use dummy \nvariables\u2014the practice of creating a new **indicator** variable for each category - so \nthat each of these dummy variables is a binary. When clustering, this can lead to \ncomplications, because if you have many different categories, you are adding many \ndifferent dimensions for each categorical variable and the result will often not \nproperly reflect the kinds of groupings you're looking for.\n\n\nTo handle such situations, two related methods make dealing with categorical data \nmore natural. **k-modes** is a clustering algorithm that uses the mode of a cluster rather \nthan the mean, but otherwise performs just like the k-means algorithm. Like mean is \na good measure for the typical\/ central value for a continuous variable, 'mode' or the \nmost commonly occurring category is the typical value for a categorical variable. The \nK-modes algorithm is a great choice for categorical data.\n\n**k-prototypes** clustering allows you to deal with cases where there is a mix of \ncategorical and continuous variables. Instead of defining a centroid for each cluster \nlike k-means or k-modes, k-prototypes clustering chooses a data point to be the \nprototype and uses that as if it is the centroid of the cluster, updating to a new data \npoint closer to the center of all data points assigned to that cluster using the same \nprocess as k-means or k-modes\n\n","2e8413d6":"## Evaluating Clustering\n\n\nWe have seen various ways of performing clustering so far, each approach having \nits merits. For the same task, we saw that the approaches provided varying results. \nWhich of them is better? Before we answer that, we need to be able to evaluate how \ngood the results from clustering are. Only then can we compare across segmentation \napproaches. We need to have, therefore, ways to evaluate the quality of clustering. \n\nAnother motivation for cluster evaluation methods is the reiteration that clustering is \na part of a bigger segmentation exercise, of which clustering is a key part, but far from \nthe whole. Recall from the discussion in the previous chapter that in segmentation \nexercises, business is often the end consumer of the segments and acts on them. The \nsegments, therefore, need to make sense to the business as well and be actionable. \nThat is why we need to be able to evaluate clusters from a business perspective as \nwell. We have discussed this aspect in the previous chapter and stated the involved \nconsiderations. Let us further the discussion on the technical evaluation of clusters.\n\nA principled, objective way of evaluating clusters is essential. Subjective methods, \nsuch as visual inspection, can always be used, but we acknowledge that they have \nserious limitations. Quantitative methods for cluster evaluation remove subjectivity \nand have the added advantage of enabling some level of automation. One such \nmeasure is the silhouette score - a powerful objective method that can be used \nwith data that is more difficult to visualize. \n\n\n\nNote that the silhouette score is a general measure of how well a clustering fits \nthe data, so it can be used to not only compare two different models of different \ntypes but also choose hyperparameters, such as the number of clusters or choice of \nquantile for calculating bandwidth for mean-shift clustering\n\n\n\n## Silhouette Score\n\nA natural way to evaluate clusters is as follows: if the clusters are well-separated, then \nany point in a cluster should be closer to most of the points in the same cluster than \nto a point from another cluster.\n\nThis intuition is quantified through the silhouette score. The silhouette score is a \nformal measure of how well a clustering fits the data. The higher the score, the \nbetter the clusters are. The score is calculated for each data point separately, \nand the average is taken as a measure of how well the model fits the whole \ndataset altogether. \n\nLet us understand the score better. There are two main components to the score. \nThe first component measures how well the data point fits into the cluster that \nit is assigned to. This is defined as the average distance between it and all other \nmembers of that same cluster. The second component measures how well the data \npoint fits into the next nearest cluster. It is calculated in the same way by measuring \nthe average distance between the data point and all the data points assigned to the \nnext nearest cluster. The difference between these two numbers can be taken as a \nmeasure of how well the data point fits into the cluster it is assigned to as opposed \nto a different cluster. Therefore, when calculated for all data points, it's a measure of \nhow well each data point fits into the particular cluster it's been assigned to. \nMore formally, given data point xi, where axi is the average distance between that \ndata point and all other data points in the same cluster and bxi is the average distance \nbetween data point xi and the data points in the next nearest cluster\n\nNote that since we divide by the maximum of axi and bxi, we end up with a number \nbetween \u22121 and 1. A negative score means that this data point is actually on average \ncloser to the other cluster, whereas a high positive score means it's a much better \nfit to the cluster it is assigned to. A value close to 0 would mean that the sample is \nclose to both clusters. When we take the average score across all data points, we will \ntherefore still get a number between \u22121 and 1, where the closer we are to one the \nbetter the fit.\n"}}