{"cell_type":{"b5124842":"code","10de6a3d":"code","ed669682":"code","64d37375":"code","05b85248":"code","12414280":"code","dbb5f1a7":"code","6dacc55c":"code","8ce317f6":"code","c2250819":"code","3a40e2cd":"code","894d5d2b":"code","193751d4":"code","5a1a3e4e":"markdown","d151e29a":"markdown","b4bcc275":"markdown","4c465d61":"markdown","6d1f2087":"markdown","34868cce":"markdown"},"source":{"b5124842":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","10de6a3d":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n","ed669682":"# Importing the dataset\ndataset = pd.read_csv('..\/input\/credit-card-applications\/Credit_Card_Applications.csv')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","64d37375":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range = (0, 1))\nX = sc.fit_transform(X)","05b85248":"\n# from shutil import copyfile\n# copyfile(src = \"..\/input\/minisom1.py\", dst = \"..\/working\/minisom.py\")\nfrom minisom1 import MiniSom\n# # Training the Self Organizing Map\nsom = MiniSom(x = 10, y = 10, input_len = 15, sigma = 1.0, learning_rate = 0.5)\nsom.random_weights_init(X)\nsom.train_random(data = X, num_iteration = 100)","12414280":"# Visualizing the results\nfrom pylab import bone, pcolor, colorbar, plot, show\nbone()\npcolor(som.distance_map().T)\ncolorbar()\nmarkers = ['o', 's']\ncolors = ['r', 'g']\nfor i, x in enumerate(X):\n    w = som.winner(x)\n    plot(w[0] + 0.5,\n         w[1] + 0.5,\n         markers[y[i]],\n         markeredgecolor = colors[y[i]],\n         markerfacecolor = 'None',\n         markersize = 10,\n         markeredgewidth = 2)\nshow()","dbb5f1a7":"# Finding the frauds\nmappings = som.win_map(X)\nfrauds = mappings[(8,5)]\nfrauds = sc.inverse_transform(frauds)\n","6dacc55c":"# Creating the matrix of features\ncustomers = dataset.iloc[:, 1:].values\n\n# Creating the dependent variable\nis_fraud = np.zeros(len(dataset))\nfor i in range(len(dataset)):\n    if dataset.iloc[i,0] in frauds:\n        is_fraud[i] = 1","8ce317f6":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ncustomers = sc.fit_transform(customers)","c2250819":"# Importing the Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense","3a40e2cd":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 2, kernel_initializer = 'uniform', activation = 'relu', input_dim = 15))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","894d5d2b":"# Fitting the ANN to the Training set\nclassifier.fit(customers, is_fraud, batch_size = 1, epochs = 2)","193751d4":"# Predicting the probabilities of frauds\ny_pred = classifier.predict(customers)\ny_pred = np.concatenate((dataset.iloc[:, 0:1].values, y_pred), axis = 1)\ny_pred = y_pred[y_pred[:, 1].argsort()]","5a1a3e4e":"## Part 3 - Detect the Frauds with the Artificial Neural Networks.","d151e29a":"## Part 1 - Identify the Frauds with the Self-Organizing Map","b4bcc275":"# SOMs Working\nSOM algorithm starts from initializing the values of weight vectors to small numbers close to 0. Then, a sample vector is selected randomly and the map of weight vectors is searched to find which weight best represents that sample. Each weight vector has neighboring weights that are close to it. The weight that is chosen is updated to become more like the randomly selected sample vector. The neighbors of that weight are also updated to become more like the chosen sample vector. This allows the map to grow and form different shapes (square\/rectangular\/hexagonal\/L)in 2D feature space.\n\n**Algorithm**:\n1. Each node\u2019s weights are randomly initialized.\n2. An input vector is chosen at random from the set of training data.\n3. Every node is examined to calculate which one\u2019s weights are most like the input vector. The winning node is commonly known as the Best Matching Unit (BMU is a technique which calculates the distance from each weight to the sample vector, by running through all weight vectors. The weight with the shortest distance is the winner.).\n4. Then the neighbourhood of the BMU is calculated. The amount of neighbors decreases over time.\n5. The winning weight is rewarded with becoming more like the sample vector. The neighbors also become more like the sample vector. The closer a node is to the BMU, the more its weights get altered and the farther away the neighbor is from the BMU, the less it learns.\n6. Repeat step 2 to 5 for N iterations.","4c465d61":"## Part 2 - Going from Unsupervised to Supervised Deep Learning\n","6d1f2087":"**Observations:**\n* If the average distance is high, then the surrounding weights are very different and a light color is assigned to the location of the weight. \n* If the average distance is low, a darker color is assigned. The resulting maps show that the concentration of different clusters of species are more predominant in three zones. Figure tells us about ***different density of data types*** (darker and lighter regions)and ***how they are clustered***.","34868cce":"# Introduction\n### Artificial Neural Networks\nAn Artificial Neural Networks or ANN is an information processing paradigm that is inspired by the way the biological nervous system such as brain process information. It is composed of large number of highly interconnected processing elements(neurons) working in unison to solve a specific problem.\n\n![](https:\/\/miro.medium.com\/max\/1400\/0*2AMCbOiRQfpOmmkn.png)\n### Self Organizing Maps\nA self-organizing map (SOM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map.\n- SOMs retain topology of input dataset\n- SOMs apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent) used in other ANNs. \n\n# Objective\n    1. Detect fraud applicants from given credit card application dataset using SOM model(unsupervised). \n    - All 16 features will be used in modelling.\n    \n    2. Predict the fraud applications from given credit card application dataset using ANN model(supervised).\n    - 15 features (except Customer id) will be used in modelling.    "}}