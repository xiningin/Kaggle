{"cell_type":{"ec0f3fb9":"code","ad4af2d8":"code","d6fc3f7b":"code","722d261d":"code","81aefaef":"code","56ec59c3":"code","de05975b":"code","eaceeca5":"code","3e6adefe":"code","08a00002":"code","049dc635":"code","4de11a7f":"code","0456e36d":"code","7f383b4d":"code","2c0ba4d5":"code","06429402":"code","854a550d":"code","4c594dc4":"code","42ad1f27":"code","e3f416dd":"code","0aa60ce3":"code","21067a32":"code","cd9c88f0":"code","42fe6fd5":"code","771b75be":"code","a5792dc7":"code","46e202e5":"code","1f9fa8ba":"code","60b5c4b2":"markdown","8a1c2e92":"markdown","062aa1ea":"markdown","ed3208e2":"markdown","234c12af":"markdown","5e818fa6":"markdown","c29e9660":"markdown","c096ed65":"markdown","5393330a":"markdown","7d4767d6":"markdown","fb8d3067":"markdown","2f4ec766":"markdown","b5e0b209":"markdown","6642d928":"markdown","58862610":"markdown","1963ffcb":"markdown","31d84c0d":"markdown","ed73bccc":"markdown","bcce8b2c":"markdown","669d3346":"markdown","77f0f943":"markdown","c7a720bd":"markdown"},"source":{"ec0f3fb9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n# Neural networks:\nfrom keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D\nfrom keras import models\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nimport pickle\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad4af2d8":"np.random.seed(1)\n# Load Dataset\ndf_train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\") \n# Random permutation(the seed is used to resample the same permutation evey time)\ndf_train = df_train.iloc[np.random.permutation(len(df_train))]","d6fc3f7b":"df_train.head(5)","722d261d":"df_train.shape","81aefaef":"sample_size = df_train.shape[0] # Training size\nvalidation_size = int(df_train.shape[0] * 0.1) # Testing size\n\n# train_x y train_y\n# Take all the columns except for the 0th one\ntrain_x = np.asarray(df_train.iloc[:sample_size - validation_size:, 1:]).reshape([sample_size - validation_size, 28, 28, 1])\ntrain_y = np.asarray(df_train.iloc[:sample_size - validation_size:, 0]).reshape([sample_size - validation_size, 1])# 0th column\n\n# val_x y val_y\nval_x = np.asarray(df_train.iloc[sample_size - validation_size:,1:]).reshape([validation_size,28,28,1])\nval_y = np.asarray(df_train.iloc[sample_size - validation_size:, 0]).reshape([validation_size, 1])","56ec59c3":"# Training set size:\ntrain_x.shape, train_y.shape","de05975b":"df_test = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\n#Reshape it as a numpy array:\ntest_x = np.asarray(df_test.iloc[:, :]).reshape([-1, 28, 28, 1])","eaceeca5":"train_x = train_x\/255\nval_x = val_x\/255\ntest_x = test_x\/255","3e6adefe":"# First we check the frequency of digits in trainin and validation set\ncounts = df_train.iloc[:sample_size - validation_size, :].groupby('label')['label'].count()\n\nf = plt.figure(figsize = (10, 6))\nf.add_subplot(111)\n\nplt.bar(counts.index, counts.values, width = 0.8, color = \"orange\")\nfor i in counts.index:\n    plt.text(i, counts.values[i] + 50, str(counts.values[i]), horizontalalignment = 'center', fontsize = 14)\n\nplt.tick_params(labelsize = 14)\nplt.xticks(counts.index)\nplt.xlabel(\"Digits\", fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.title(\"Frequency Graph training set\", fontsize = 20)\nplt.savefig('digit_frequency_train.png')\nplt.show()","08a00002":"counts = df_train.iloc[sample_size - validation_size:, :].groupby('label')['label'].count()\n\nf = plt.figure(figsize = (10, 6))\nf.add_subplot(111)\n\nplt.bar(counts.index, counts.values, width = 0.8, color = \"orange\")\nfor i in counts.index:\n    plt.text(i, counts.values[i] + 5, str(counts.values[i]), horizontalalignment = 'center', fontsize = 14)\n\nplt.tick_params(labelsize = 14)\nplt.xticks(counts.index)\nplt.xlabel(\"Digits\", fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.title(\"Frequency Graph Validation set\", fontsize = 20)\nplt.savefig('digit_frequency_Val.png')\nplt.show()","049dc635":"rows = 5\ncols = 6\n\nf = plt.figure(figsize = (2*cols, 2*rows))\n\nfor i in range(rows*cols):\n    f.add_subplot(rows, cols, i + 1)#Adding a suubplot on each iteration\n    plt.imshow(train_x[i].reshape([28, 28]), cmap = \"Blues\")\n    plt.axis(\"off\")\n    plt.title(str(train_y[i]), y = -0.15, color = \"green\")\nplt.savefig(\"digits.png\")","4de11a7f":"#Define the model sequential\nmodel = models.Sequential()","0456e36d":"# Block 1\nmodel.add(Conv2D(32,3, padding  =\"same\",input_shape=(28, 28, 1)))\nmodel.add(LeakyReLU())\nmodel.add(Conv2D(32,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n# Block 2\nmodel.add(Conv2D(64,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(Conv2D(64,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(10,activation=\"sigmoid\"))","7f383b4d":"initial_lr = 0.001\nloss = \"sparse_categorical_crossentropy\"\nmodel.compile(Adam(lr = initial_lr), loss = loss, metrics = ['accuracy'])\nmodel.summary()","2c0ba4d5":"epochs = 20 # epochs\nbatch_size = 256 # batch size\nhistory_1 = model.fit(train_x, train_y, batch_size = batch_size, epochs = epochs, validation_data = (val_x, val_y))","06429402":"#Deffining figure:\nf = plt.figure(figsize = (20, 7))\n\n#Adding accuracy subplot\nf.add_subplot(121)\n\n#Accuracy curve for training set\nplt.plot(history_1.epoch, history_1.history['accuracy'], label = \"accuracy\")\n#Accuracy curve for test set\nplt.plot(history_1.epoch, history_1.history['val_accuracy'], label = \"val_accuracy\")\n\nplt.title(\"Accuracy Curve\", fontsize = 18)\nplt.xlabel(\"Epochs\", fontsize = 15)\nplt.ylabel(\"Accuracy\", fontsize = 15)\nplt.grid(alpha = 0.3)\nplt.legend()\n\n#Adding loss subplot\nf.add_subplot(122)\n\n#Loss curve for the training set\nplt.plot(history_1.epoch,history_1.history['loss'],label=\"loss\")\n#Loss curve for the test set\nplt.plot(history_1.epoch,history_1.history['val_loss'],label=\"val_loss\")\n\nplt.title(\"Loss Curve\", fontsize = 18)\nplt.xlabel(\"Epochs\", fontsize = 15)\nplt.ylabel(\"Loss\", fontsize = 15)\nplt.grid(alpha = 0.3)\nplt.legend()\n\nplt.show()","854a550d":"val_p = np.argmax(model.predict(val_x), axis = 1)\n\n#Fill the confusion matrix and sumarize the error:\nerror = 0\nconfusion_matrix = np.zeros([10, 10])\nfor i in range(val_x.shape[0]):\n    confusion_matrix[val_y[i], val_p[i]] += 1\n    if val_y[i] != val_p[i]:\n        error += 1\n        \nprint(\"Confusion Matrix: \\n\\n\", confusion_matrix)\nprint(\"\\nErrors in validation set: \", error)\nprint(\"\\nError Persentage: \", (error * 100) \/ val_p.shape[0])\nprint(\"\\nAccuracy: \", 100 - (error * 100) \/ val_p.shape[0])\nprint(\"\\nValidation set Shape: \", val_p.shape[0])","4c594dc4":"# Ploting confusion matrix:\nf = plt.figure(figsize = (10, 10))\nf.add_subplot(111)\n\nplt.imshow(np.log2(confusion_matrix + 1), cmap = \"Blues\")\nplt.colorbar()\nplt.tick_params(size = 5, color = \"white\")\nplt.xticks(np.arange(0, 10), np.arange(0, 10))\nplt.yticks(np.arange(0, 10), np.arange(0, 10))\n\nthreshold = confusion_matrix.max()\/2\n\nfor i in range(10):\n    for j in range(10):\n        plt.text(j, i, int(confusion_matrix[i, j]), horizontalalignment = \"center\", color = \"white\" if confusion_matrix[i, j] > threshold else \"black\")\n        \n\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"Confusion_matrix1.png\")\nplt.show()","42ad1f27":"# Function of keras for image augmentation\ndatagen = ImageDataGenerator(\n    featurewise_center = False,\n    samplewise_center = False,\n    featurewise_std_normalization = False,\n    samplewise_std_normalization = False,\n    zca_whitening = False,\n    rotation_range = 10, #Rotate images in range\n    zoom_range = 0.1, #Zoom image\n    width_shift_range = 0.1, #Shift images horizontaly\n    height_shift_range = 0.1, #Shift images vertically\n    horizontal_flip = False,\n    vertical_flip = False)\ndatagen.fit(train_x)","e3f416dd":"# Keras function used tu reduce the learning rate when a metric has stopped improving\nlrr = ReduceLROnPlateau(monitor = 'val_accuracy', patience = 2, verbose = 1, factor = 0.5, min_lr = 0.00001)","0aa60ce3":"epochs = 30\nhistory_2 = model.fit_generator(datagen.flow(train_x, train_y, batch_size = batch_size), steps_per_epoch = int(train_x.shape[0]\/batch_size) + 1, epochs = epochs, validation_data = (val_x, val_y), callbacks = [lrr])\n","21067a32":"# Deffining figure\nf = plt.figure(figsize = (20, 7))\nf.add_subplot(121)\n\n# Adding accuracy subplot\n#Train\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['accuracy']+history_2.history['accuracy'],label = \"accuracy\")\n#Test\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['val_accuracy']+history_2.history['val_accuracy'],label = \"val_accuracy\")\n\nplt.title(\"Accuracy Curve\", fontsize = 18)\nplt.xlabel(\"Epochs\", fontsize = 15)\nplt.grid(alpha = 0.3)\nplt.legend()\n\n# Adding loss subplot\nf.add_subplot(122)\n#Train\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['loss']+history_2.history['loss'],label=\"loss\") \n#Test\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['val_loss']+history_2.history['val_loss'],label=\"val_loss\")\n\nplt.title(\"Loss Curve\", fontsize = 18)\nplt.xlabel(\"Epochs\", fontsize = 15)\nplt.ylabel(\"Loss\", fontsize = 15)\nplt.grid(alpha = 0.3)\nplt.legend()\n\nplt.show()","cd9c88f0":"val_p = np.argmax(model.predict(val_x), axis = 1)\n\nerror = 0\nconfusion_matrix = np.zeros([10, 10])\nfor i in range(val_x.shape[0]):\n    confusion_matrix[val_y[i], val_p[i]] += 1\n    if val_y[i] != val_p[i]:\n        error += 1\n        \nprint(\"Confusion Matrix: \\n\\n\", confusion_matrix)\nprint(\"\\nErrors in validation set: \", error)\nprint(\"\\nError Persentage: \", (error * 100) \/ val_p.shape[0])\nprint(\"\\nAccuracy: \", 100 - (error * 100) \/ val_p.shape[0])\nprint(\"\\nValidation set Shape: \", val_p.shape[0])","42fe6fd5":"f = plt.figure(figsize = (10, 10))\nf.add_subplot(111)\n\nplt.imshow(np.log2(confusion_matrix + 1), cmap = \"Blues\")\nplt.colorbar()\nplt.tick_params(size = 5, color = \"white\")\nplt.xticks(np.arange(0, 10), np.arange(0, 10))\nplt.yticks(np.arange(0, 10), np.arange(0, 10))\n\nthreshold = confusion_matrix.max()\/2\n\nfor i in range(10):\n    for j in range(10):\n        plt.text(j, i, int(confusion_matrix[i, j]), horizontalalignment = \"center\", color = \"white\" if confusion_matrix[i, j] > threshold else \"black\")\n        \n\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"Confusion_matrix2.png\")\nplt.show()","771b75be":"rows = 4\ncols = 9\n\nf = plt.figure(figsize = (2*cols, 2*rows))\nsubplot = 1\nfor i in range(val_x.shape[0]):\n    if val_y[i] != val_p[i]:\n        f.add_subplot(rows, cols, subplot)\n        subplot += 1\n        plt.imshow(val_x[i].reshape([28, 28]), cmap = \"Blues\")\n        plt.axis(\"off\")\n        plt.title(\"T: \" + str(val_y[i]) + \"P: \" + str(val_p[i]), y = -0.15, color = \"Red\")\n        \nplt.savefig(\"error_plots.png\")\nplt.show()","a5792dc7":"test_y = np.argmax(model.predict(test_x), axis = 1)","46e202e5":"rows = 5\ncols = 10\n\nf = plt.figure(figsize = (2*cols, 2*rows))\n\nfor i in range(rows*cols):\n    f.add_subplot(rows, cols, i+1)\n    plt.imshow(test_x[i]. reshape([28, 28]), cmap = \"Blues\")\n    plt.axis(\"off\")\n    plt.title(str(test_y[i]))","1f9fa8ba":"df_submission = pd.DataFrame([df_test.index + 1, test_y], [\"ImageId\", \"Label\"]).transpose()\ndf_submission.to_csv(\"MySubmission.csv\", index = False)","60b5c4b2":"## ","8a1c2e92":"As we see, the accuracy and error of the test set are worse than the ones from the train set, it means the algorithm is overfiting.\n\nLet's analize the performance of the algorithm on the test set more deeper.\n\n## Confusion Matrix:\nEach row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes.","062aa1ea":"## Separate training and test sets\nIn this case i'll be using 90% of the set for training and the other 10% for testing","ed3208e2":"# Predictions on the test set:","234c12af":"**The training set has 42000 images, with 784 pixels each(the other col is the label)**","5e818fa6":"## Creating submission","c29e9660":"**Frequency plot for the training set:**","c096ed65":"# Load and visualize the Dataset\n#### MNIST Dataset:\n* 60000 labeled images of handwritten numbers.\n* Image size is 28 * 28.\n* The images have only one color chanel(grayscale).\n* Labeled from 0 to 9.","5393330a":"# Improve results by image augmentation\n![image.png](attachment:1f2da43c-aa9c-40fa-9095-31004d05a34f.png)\n\nFor improving the results we'll be using image augmentation, which is a technique widely used in machine learning, where we pick examples of data and we slightly change them by roting them or flipping them, etc.","7d4767d6":"### **Load test.csv**","fb8d3067":"### **Normlalize the data:**\nEach pixel values lies between [0, 255], This range is too high and can be difficult  for the model to learn, what we do is scale the range of pixels to [0, 1]","2f4ec766":"## Training\nWe'll use now the model.fit_generator() funcion, which is the function used for training on a generated batch.","b5e0b209":"## Visualize digits dataset frequency\nAn important thing to do is is check the frequency of classes in the dataset, is always better to work with a balanced dataset.\n\n**Frequency plot for the training set:**","6642d928":"It seems that there's a good balance of classes on the Train and Test sets","58862610":"## Training performance","1963ffcb":"### Training performance\nAfter the training with the new generated data, let's see the results.","31d84c0d":"## **Confusion Matrix**","ed73bccc":"# Visualizing results\n## Errors in the validation set:","bcce8b2c":"### **Training**","669d3346":"## Compile the model:\nI'll be using \"Sparse categorical crossentropy\" as a loss, accuracy as the performance metric and Adam as optimization algorithm. ","77f0f943":"### **Visualizing the digits**","c7a720bd":"# Building the model\n## Convolutional neural network\n**Description of the model:**\n* 2 convolutional blocks(with leaky relu) with 2 MaxPool layers and a dropout layer.\n* The output layer has 10 nodes with sigmoid activation"}}