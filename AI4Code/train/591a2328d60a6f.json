{"cell_type":{"487c81a2":"code","696cc6d4":"code","e0b68009":"code","19db0a0d":"code","cc2015ed":"code","08e9e302":"code","533a0ed3":"code","0ac5d89b":"code","9d2dad51":"code","359fcd16":"code","0a9ddbb0":"code","cb230d94":"code","7512bc69":"code","f904f027":"code","64204a06":"code","a1c14304":"code","c74ae843":"code","bb0068b0":"code","4f4e262b":"code","1b74aacb":"code","65594e29":"code","798fe507":"code","24ada240":"code","77cce081":"code","78eaf3e9":"code","2894fe7b":"code","52329cc3":"code","63031f21":"code","87664938":"code","e40ba281":"code","4215ea07":"code","f1a26c10":"code","b59c87e9":"markdown","4d1e0d4e":"markdown","a996d39f":"markdown","e4b3102d":"markdown","eb06d32b":"markdown","253deaa1":"markdown","657eddd0":"markdown","5488ea37":"markdown","ab7c6832":"markdown","847bc449":"markdown","5106717a":"markdown","f54e3c59":"markdown","a75e85df":"markdown","416e31f7":"markdown","5e59d851":"markdown"},"source":{"487c81a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","696cc6d4":"df = pd.read_csv('\/kaggle\/input\/predicting-divorce\/divorce.csv')","e0b68009":"df.head()","19db0a0d":"df.describe()","cc2015ed":"df.info()","08e9e302":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","533a0ed3":"sns.heatmap(df.corr())","0ac5d89b":"#Let's visualize this more\nplt.figure(figsize=(10,6))\ndf[df['Divorce_Y_N']==1]['2_strangers'].hist(color='red',label='Divorced')\ndf[df['Divorce_Y_N']==0]['2_strangers'].hist(color='blue',label='Not Divorced')\nplt.legend()\nplt.xlabel('2_strangers')\nplt.title('Histogram of column 2_strangers')","9d2dad51":"sns.countplot(data=df,x='Divorce_Y_N',palette='coolwarm')","359fcd16":"df.isnull().sum()","0a9ddbb0":"from sklearn.model_selection import train_test_split","cb230d94":"X = df.drop('Divorce_Y_N',axis=1)\ny = df['Divorce_Y_N']","7512bc69":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","f904f027":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn import metrics","64204a06":"RFC = RandomForestClassifier()","a1c14304":"RFC.fit(X_train,y_train)","c74ae843":"forest_pred = RFC.predict(X_test)","bb0068b0":"print(confusion_matrix(y_test,forest_pred))\nprint(classification_report(y_test,forest_pred))\nmetrics.accuracy_score(y_test,forest_pred)","4f4e262b":"ada = AdaBoostClassifier(n_estimators=100)","1b74aacb":"ada.fit(X_train,y_train)","65594e29":"ada_pred = ada.predict(X_test)","798fe507":"print(confusion_matrix(y_test,ada_pred))\nprint(classification_report(y_test,ada_pred))\nmetrics.accuracy_score(y_test,ada_pred)","24ada240":"gb = GradientBoostingClassifier()","77cce081":"gb.fit(X_train,y_train)","78eaf3e9":"gb_pred = gb.predict(X_test)","2894fe7b":"print(confusion_matrix(y_test,gb_pred))\nprint(classification_report(y_test,gb_pred))\nmetrics.accuracy_score(y_test,gb_pred)","52329cc3":"from sklearn.model_selection import GridSearchCV","63031f21":"\nparams = {'n_estimators':[5,100,200,300],\n         'max_depth':[1,3,5,7,9]\n         ,'learning_rate':[0.01,0.1,1,10,100]}\ngrid_gb = GridSearchCV(gb,param_grid=params,scoring='accuracy',cv=5)\ngrid_gb.fit(X_train,y_train)\ngrid_gb.best_params_","87664938":"gb = GradientBoostingClassifier(n_estimators=100,learning_rate=0.01,max_depth=1)","e40ba281":"gb.fit(X_train,y_train)","4215ea07":"gb_pred = gb.predict(X_test)","f1a26c10":"print(confusion_matrix(y_test,gb_pred))\nprint(classification_report(y_test,gb_pred))\nmetrics.accuracy_score(y_test,gb_pred)","b59c87e9":"# Train Test Split","4d1e0d4e":"# Data Exploration","a996d39f":"Let's see how the GradientBoosting Classifier model does with the hyper-parameters above","e4b3102d":"All the columns contain integer data types","eb06d32b":"# Preprocessing Data","253deaa1":"# Predicting Divorce using AdaBoost Classifier","657eddd0":"Looks like the most positively correlated column to Divorce_Y_N is 2_strangers, which contains the answer to the following question:\n\n*We are like two strangers who share the same environment at home rather than family.*\n","5488ea37":"Conclusion - RFC\/AdaBoost models results in a higher accuracy of 98%","ab7c6832":"Looks like using AdaBoost did not increase accuracy","847bc449":"# Predicting Divorce using Random Forest Classifier","5106717a":"Are there any missing values?","f54e3c59":"# Predicting Divorce using GradientBoosting Classifier","a75e85df":"Looks like gradient boosting classifier did worse than the random forest\/adaboost classifier!Let's see if fine tuning the hyper-parameters helps to increase the accuracy","416e31f7":"# Finding the optimal Hyper-Parameters using GridSearchCV for the Gradient Boosting Classifier","5e59d851":"Let's see the count of divorced vs still married "}}