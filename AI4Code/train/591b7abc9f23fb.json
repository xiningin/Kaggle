{"cell_type":{"3dead061":"code","73950873":"code","0d39aa32":"code","51ad6bac":"code","98932894":"code","c50b8e2c":"code","aca9c28e":"code","f7be02ff":"code","f0600132":"code","06310ce2":"code","c5d35c1c":"code","f8977bc6":"code","2a6df06a":"code","36e2e0be":"code","132e518b":"code","24acad66":"code","deab5271":"code","025ecb42":"code","4f0a1a88":"code","79ecc7d5":"code","a9755daa":"code","53ae416e":"markdown","0ee99855":"markdown","934fe2eb":"markdown","5d3a9187":"markdown","a7b57fac":"markdown","4bfb533a":"markdown","7c9ca7bf":"markdown","720888b5":"markdown","c013945b":"markdown","15ae2cea":"markdown","aab8d934":"markdown","b077806a":"markdown","9262e26c":"markdown","a07fb221":"markdown","e8938621":"markdown","17aff850":"markdown"},"source":{"3dead061":"import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","73950873":"data = load_breast_cancer()\ndf = pd.DataFrame(data['data'],columns=data['feature_names'])","0d39aa32":"df","51ad6bac":"X,y = data['data'],data['target']","98932894":"X_scaled = (X-X.min(axis=0)) \/ (X.max(axis=0) - X.min(axis=0))\nX = X_scaled\nX_b = np.c_[np.ones((len(X),1)),X]\nX = X_b","c50b8e2c":"scaler = MinMaxScaler()\n\nX_scaled_test = scaler.fit_transform(X)","aca9c28e":"X","f7be02ff":"X_scaled_test","f0600132":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)","06310ce2":"def sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))","c5d35c1c":"thetas = np.zeros(df.shape[1]+1)\nm = len(y)","f8977bc6":"def cost_function(X,y,thetas):\n    predictions = sigmoid(np.dot(X,thetas.T))\n    J = -(1\/m) * np.sum(y*np.log(predictions) + (1-y)*np.log(1-predictions))\n    return J","2a6df06a":"cost_function(X,y,thetas)","36e2e0be":"def batch_gradient_descent(X,y,thetas,alpha=0.5,n_iters=2500):\n    c_hist = [0] * n_iters\n    \n    for i in range(n_iters):\n        prediction = sigmoid(np.dot(X,thetas.T))\n        thetas = thetas - (alpha\/m) * np.dot(prediction-y,X)\n        c_hist[i] = cost_function(X,y,thetas)\n        \n    return thetas,c_hist","132e518b":"batch_gd_thetas,batch_gd_cost = batch_gradient_descent(X_train,y_train,thetas)","24acad66":"import matplotlib.pyplot as plt\n\nplt.plot(range(2500),batch_gd_cost)\nplt.xlabel('No. of Iterations')\nplt.ylabel('J (\u03b8)')\nplt.title('Batch Gradient Descent')\nplt.show()","deab5271":"threshold = 0.5\n\nfinal_predictions = pd.Series(sigmoid(np.dot(X_test,batch_gd_thetas.T)) >= threshold).astype('int')\n\nprint(classification_report(y_test,final_predictions))","025ecb42":"from sklearn.linear_model import LogisticRegression","4f0a1a88":"lg = LogisticRegression()","79ecc7d5":"lg.fit(X_train,y_train)","a9755daa":"print(classification_report(y_test,lg.predict(X_test)))","53ae416e":"<h3 align='center'>Feature Scaling<\/h3>\n\n<p align='center'>We will make a custom implementation of MinMaxScaler using the following formula: \n\n![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/min-max-normalisation.jpg)    \n    \nMost ML Algorithms require feature scaling, iuncluding Logistic Regression + KNN. Also, Batch Gradient Descent requires feature scaling too. We will also add a bias row\n<\/p>","0ee99855":"<h3 align='center'>Define our features and label<\/h3>","934fe2eb":"<h3 align='center'>Initialize thetas and m(number of training examples)<\/h3>","5d3a9187":"<h3 align='center'>Import necessary libraries<\/h3>","a7b57fac":"<h3 align='center'>Create the code for batch gradient descent<\/h3>\n\n![](https:\/\/datascienceplus.com\/wp-content\/uploads\/2017\/02\/4.png)","4bfb533a":"<h3 align='center'>View the data<\/h3>","7c9ca7bf":"<h1 align='center'>Welcome to My Kernel!<\/h1>\n\n<h3 align='center'>In this kernel, I would like to show you how to implement Logistic Regression from scratch using batch gradient descent on the famous breast cancer dataset. Enjoy and please upvote if you liked it!\n    <\/h3>","720888b5":"<h3 align='center'>Make predictions based on the threshold and get a classification report of our model's performance on validation set<\/h3>","c013945b":"<h3 align='center'>Use MinMaxScaler from Sklearn to make sure our scaling is properly functioning<\/h3>","15ae2cea":"<h3>Thanks for reading!<\/h3>","aab8d934":"<h3 align='center'>Using the Logistic Regression class from sklearn to compare our custom model to sklearn's implmentation<\/h3>","b077806a":"<h3 align='center'>Define the sigmoid activation function<\/h3>\n\n![](https:\/\/miro.medium.com\/max\/970\/1*Xu7B5y9gp0iL5ooBj7LtWw.png)","9262e26c":"<h3 align='center'>Plot the cost function for batch gradient descent<\/h3>","a07fb221":"<h3 align='center'>Load the data<\/h3>","e8938621":"<h3 align='center'>Ok, so I think we passed the test. Now let's split the data into train + validation<\/h3>\n\n","17aff850":"<h3 align='center'>Define the logistic cost function<\/h3>\n\n![](https:\/\/raw.githubusercontent.com\/ritchieng\/machine-learning-stanford\/master\/w3_logistic_regression_regularization\/logistic_regression_simple2.png)"}}