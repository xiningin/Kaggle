{"cell_type":{"225568b3":"code","235035b7":"code","25960183":"code","a85b37b0":"code","440ac867":"code","9396e3e3":"code","7bd6fb3f":"code","b575e687":"code","52827a12":"code","1e5bcff8":"code","a69620eb":"code","52e46194":"code","d933e672":"code","16fbecd8":"code","9ae4d910":"code","d341a362":"code","bbea762d":"code","854b6495":"code","e4c84f61":"code","4dad7326":"code","73e5b1c1":"code","89b73f24":"code","e0d4584b":"code","3e14e0ec":"code","2960f155":"code","24e72262":"code","e909e779":"code","dfe0a300":"code","d0e11f73":"code","e0d605e9":"code","fadb822c":"code","d5caea6f":"code","db5cbb91":"code","69754046":"code","e880d4ee":"code","0be34b44":"code","2aa9c2b1":"code","c80ab8d0":"code","7c5a18b0":"code","97f14e29":"code","5c34d6c5":"code","c4d55e5e":"code","1ab99194":"code","4cc26ccd":"code","cc5e6c49":"code","a1a367c8":"code","e9477dd0":"code","87caa0f9":"code","4806c8b7":"code","4006360e":"code","307012bb":"markdown","f80ecca2":"markdown","41794eef":"markdown","8c7662fa":"markdown","8536d5d7":"markdown","e8d9a159":"markdown","689222ce":"markdown","5c63ba15":"markdown","b274e3ad":"markdown","4d24ef92":"markdown","00cbe7d7":"markdown","116b7419":"markdown"},"source":{"225568b3":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom datetime import datetime\n%matplotlib inline\n#tf.enable_eager_execution()","235035b7":"TRAIN_FILE_NAME = '..\/input\/train.csv'\nTEST_FILE_NAME = '..\/input\/test.csv'\nNUM_RECORDS = 6000000\ndatatypes = {'key': 'str', \n              'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'int',\n            }","25960183":"df = pd.read_csv(TRAIN_FILE_NAME, \n                 nrows=NUM_RECORDS, \n                 dtype=datatypes, \n                 parse_dates=['pickup_datetime'])\nlen(df)","a85b37b0":"lat_border = (40.63, 40.85)\nlong_border = (-74.03, -73.75)\ndf.plot(kind='scatter', x='dropoff_longitude', y='dropoff_latitude', \n       color='green', s=.02, alpha=.6)\nplt.title(\"Dropoffs\")\nplt.ylim(lat_border)\nplt.xlim(long_border)\n\ndf.plot(kind='scatter', x='pickup_longitude', y='pickup_latitude', \n       color='blue', s=.02, alpha=.6)\nplt.title(\"Pickups\")\nplt.ylim(lat_border)\nplt.xlim(long_border)\nplt.show()","440ac867":"df.head()","9396e3e3":"df.isnull().sum()","7bd6fb3f":"print(f'Count before:{len(df)}')\ndf = df.dropna(how='any', axis=0)\nprint(f'Count after:{len(df)}')","b575e687":"df.describe()","52827a12":"df[df.passenger_count == 0].shape","1e5bcff8":"df[['fare_amount','passenger_count']].corr()","a69620eb":"_ = df['passenger_count'].hist(bins=10, figsize = (8,4) )","52e46194":"df[df.fare_amount < 0].shape","d933e672":"df[df.fare_amount > 250].shape","16fbecd8":"_ = df[df.fare_amount < 250]['fare_amount'].hist(bins=10, figsize = (8,4) )","9ae4d910":"def clean(df):\n    df = df[(-76 <= df['pickup_longitude']) & (df['pickup_longitude'] <= -72)]\n    df = df[(-76 <= df['dropoff_longitude']) & (df['dropoff_longitude'] <= -72)]\n    df = df[(38 <= df['pickup_latitude']) & (df['pickup_latitude'] <= 42)]\n    df = df[(38 <= df['dropoff_latitude']) & (df['dropoff_latitude'] <= 42)]\n    # Remove possible outliers\n    df = df[(0 < df['fare_amount']) & (df['fare_amount'] <= 250)]\n    # Remove inconsistent values\n    df = df[(df['dropoff_longitude'] != df['pickup_longitude'])]\n    df = df[(df['dropoff_latitude'] != df['pickup_latitude'])]\n    return df","d341a362":"print(f'Count before cleaning:{len(df)}')\ndf = clean(df)\nprint(f'Count after cleaning:{len(df)}')","bbea762d":"def check_night(row):\n    hour = row['hour']\n    hclass = 5\n    if hour > 6 and hour <= 11:\n        hclass = 1\n    elif hour >11 and hour <= 16:\n        hclass = 2\n    elif hour > 16 and hour <= 20:\n        hclass = 3\n    elif hour > 20 and hour <= 23:\n        hclass = 4\n    else:\n        hclass = 5\n    return hclass","854b6495":"def process_date(df):\n    df['year'] = df.pickup_datetime.apply(lambda x: x.year)\n    df['month'] = df.pickup_datetime.apply(lambda x: x.month)\n    df['day'] = df.pickup_datetime.apply(lambda x: x.day)\n    df['hour'] = df.pickup_datetime.apply(lambda x: x.hour)\n    df['weekday'] = df.pickup_datetime.apply(lambda x: x.weekday())\n    df['hclass'] = df.apply (lambda x: check_night(x), axis=1)\n    return df","e4c84f61":"boroughs = {\n    'manhattan':{\n        'min_lon':-74.0479, 'min_lat':40.6829,\n        'max_lon':-73.9067, 'max_lat':40.8820\n    },\n    'queens':{\n        'min_lon':-73.9630, 'min_lat':40.5431,\n        'max_lon':-73.7004, 'max_lat':40.8007\n    },\n    'brooklyn':{\n        'min_lon':-74.0421, 'min_lat':40.5707,\n        'max_lon':-73.8334, 'max_lat':40.7395\n    },\n    'bronx':{\n        'min_lon':-73.9339, 'min_lat':40.7855,\n        'max_lon':-73.7654, 'max_lat':40.9176\n    },\n    'staten_island':{\n        'min_lon':-74.2558, 'min_lat':40.4960,\n        'max_lon':-74.0522, 'max_lat':40.6490\n    }\n}\nairports = {\n    'JFK':{\n        'min_lon':-73.8352, 'min_lat':40.6195,\n        'max_lon':-73.7401, 'max_lat':40.6659\n    },          \n    'EWR':{\n        'min_lon':-74.1925, 'min_lat':40.6700, \n        'max_lon':-74.1531, 'max_lat':40.7081\n    },\n    'LaGuardia':{\n        'min_lon':-73.8895, 'min_lat':40.7664, \n        'max_lon':-73.8550, 'max_lat':40.7931\n    }\n}","4dad7326":"boroughs_list = list(boroughs.keys())\nboroughs_list.append('others')\nairport_list = list(airports.keys())\nairport_list.append('others')\nprint(boroughs_list)\nprint(airport_list)","73e5b1c1":"def getBorough(lat,lon):\n    locs=boroughs.keys()\n    for loc in locs:\n        if lat>=boroughs[loc]['min_lat'] and lat<=boroughs[loc]['max_lat'] and lon>=boroughs[loc]['min_lon'] and lon<=boroughs[loc]['max_lon']:\n            return loc\n    return 'others'","89b73f24":"def getAirport(lat,lon):\n    locs=airports.keys()\n    for loc in locs:\n        if lat>=airports[loc]['min_lat'] and lat<=airports[loc]['max_lat'] and lon>=airports[loc]['min_lon'] and lon<=airports[loc]['max_lon']:\n            return loc\n    return 'others'","e0d4584b":"def process_distance(df):\n    lat1 = df.pickup_latitude\n    lat2 = df.dropoff_latitude\n    lon1 = df.pickup_longitude\n    lon2 = df.dropoff_longitude\n    df['pickup_borough'] = df.apply(lambda row:getBorough(row['pickup_latitude'],\n                                                          row['pickup_longitude']),\n                                     axis=1)\n    df['dropoff_borough'] = df.apply(lambda row:getBorough(row['dropoff_latitude'],\n                                                           row['dropoff_longitude']),\n                                     axis=1)\n    df['pickup_airport'] = df.apply(lambda row:getAirport(row['pickup_latitude'],\n                                                          row['pickup_longitude']),\n                                     axis=1)\n    df['dropoff_airport'] = df.apply(lambda row:getAirport(row['dropoff_latitude'],\n                                                           row['dropoff_longitude']),\n                                     axis=1)\n    df['latdiff'] = lat1 - lat2\n    df['londiff'] = lon1 - lon2\n    df['euclid'] = np.sqrt((df['latdiff'] * df['latdiff']) + (df['londiff'] * df['londiff']) )\n    df['manhattan'] =  np.abs(lat2 - lat1) + np.abs(lon2 - lon1)\n    p = np.pi \/ 180\n    d =  0.5 - np.cos((lat2 - lat1) * p)\/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) \/ 2\n    df['haversine'] = 0.6213712 * 12742 * np.arcsin(np.sqrt(d))\n    return df","3e14e0ec":"def process(df):\n    df = process_date(df)\n    df = process_distance(df)\n    return df\n    ","2960f155":"df = process(df)\ndf.head()","24e72262":"df[df.hclass < 1].shape","e909e779":"df[['hour','month','day','hclass','euclid','manhattan', 'haversine']].describe()","dfe0a300":"corr_matrix = df[['fare_amount','euclid','manhattan', 'haversine']].corr()\n_ = sns.heatmap(corr_matrix)","d0e11f73":"train_df, validation_df = train_test_split(df, test_size=0.1, random_state=1, shuffle=True)\nprint('Training examples:', len(train_df))\nprint('Validation examples:', len(validation_df))","e0d605e9":"scale_cols = ['pickup_longitude', 'pickup_latitude','dropoff_longitude', \n              'dropoff_latitude','year', 'latdiff', 'londiff',\n              'euclid', 'manhattan', 'haversine']\nscaler = preprocessing.MinMaxScaler()\ntrain_df[scale_cols] = scaler.fit_transform(train_df[scale_cols])\nvalidation_df[scale_cols] = scaler.transform(validation_df[scale_cols])","fadb822c":"train_df.head()","d5caea6f":"validation_df.head()","db5cbb91":"train_df.describe()","69754046":"def get_required_data_df(df, include_label=False):\n    required_cols = ['pickup_longitude', \n                     'pickup_latitude',\n                     'dropoff_longitude',\n                     'dropoff_latitude',\n                     'month',\n                     'day',\n                     'hour',\n                     'weekday',\n                     'hclass',\n                     'latdiff',\n                     'londiff',\n                     'euclid',\n                     'manhattan',\n                     'haversine',\n                     'pickup_borough',\n                     'dropoff_borough',\n                     'pickup_airport',\n                     'dropoff_airport',\n                    ]\n    if include_label:\n        required_cols.append('fare_amount')\n    data_df = df[required_cols]\n    return(data_df)","e880d4ee":"validation_data_df = get_required_data_df(validation_df, include_label=True)\nvalidation_data_df.head()","0be34b44":"train_data_df = get_required_data_df(train_df, include_label=True)\ntrain_data_df.head()","2aa9c2b1":"INPUT_COLUMNS = [\n    # Define features\n    tf.feature_column.categorical_column_with_identity('month', num_buckets = 13),\n    tf.feature_column.categorical_column_with_identity('day', num_buckets = 32),\n    tf.feature_column.categorical_column_with_identity('weekday', num_buckets = 7),\n    tf.feature_column.categorical_column_with_identity('hour', num_buckets = 24),\n    tf.feature_column.categorical_column_with_identity('hclass', num_buckets = 6),\n    tf.feature_column.categorical_column_with_vocabulary_list('pickup_borough', \n                                                              vocabulary_list=boroughs_list),\n    tf.feature_column.categorical_column_with_vocabulary_list('dropoff_borough', \n                                                              vocabulary_list=boroughs_list),\n    tf.feature_column.categorical_column_with_vocabulary_list('pickup_airport', \n                                                              vocabulary_list=airport_list),\n    tf.feature_column.categorical_column_with_vocabulary_list('dropoff_airport', \n                                                              vocabulary_list=airport_list),\n\n    # Distance columns\n    tf.feature_column.numeric_column('pickup_latitude'),\n    tf.feature_column.numeric_column('pickup_longitude'),\n    tf.feature_column.numeric_column('dropoff_latitude'),\n    tf.feature_column.numeric_column('dropoff_longitude'),\n    tf.feature_column.numeric_column('latdiff'),\n    tf.feature_column.numeric_column('londiff'),\n    tf.feature_column.numeric_column('euclid'),\n    tf.feature_column.numeric_column('manhattan'),\n    tf.feature_column.numeric_column('haversine'),\n]","c80ab8d0":"def build_estimator(input_columns, \n                    nbuckets,\n                    hidden_units,\n                    linear_optimizer='Ftrl',\n                    dnn_optimizer = 'Adagrad',\n                    run_config=None\n                   ):\n    ( month, day, dayofweek, \n     hourofday, hclass, \n      pbor, dbor, pair, dair,\n     plat, plon, dlat, dlon, latdiff, londiff, \n     euclidean ,manhattan, haversine, \n    ) = input_columns\n    \n     # Bucketize the lats & lons\n    latbuckets = np.linspace(38.0, 42.0, nbuckets).tolist()\n    lonbuckets = np.linspace(-76.0, -72.0, nbuckets).tolist()\n    b_plat = tf.feature_column.bucketized_column(plat, latbuckets)\n    b_dlat = tf.feature_column.bucketized_column(dlat, latbuckets)\n    b_plon = tf.feature_column.bucketized_column(plon, lonbuckets)\n    b_dlon = tf.feature_column.bucketized_column(dlon, lonbuckets)\n    \n    # Feature cross\n    ploc = tf.feature_column.crossed_column([b_plat, b_plon], nbuckets * nbuckets)\n    dloc = tf.feature_column.crossed_column([b_dlat, b_dlon], nbuckets * nbuckets)\n    pd_pair = tf.feature_column.crossed_column([ploc, dloc], nbuckets ** 4 )\n    day_hr =  tf.feature_column.crossed_column([dayofweek, hourofday], 24 * 7)\n    \n    # Wide columns\n    wide_columns = [\n        # Feature crosses\n        dloc, ploc, pd_pair,\n        pbor, dbor, pair, dair,\n        day_hr,\n        # Sparse columns\n        month, day, dayofweek, hourofday, hclass,\n    ]\n    \n    # deep columns\n    deep_columns = [\n        # Embedding_column to \"group\" together ...\n        tf.feature_column.embedding_column(pd_pair, 10),\n        tf.feature_column.embedding_column(day_hr, 10),\n        tf.feature_column.embedding_column(pbor, 10),\n        tf.feature_column.embedding_column(dbor, 10),\n        tf.feature_column.embedding_column(pair, 10),\n        tf.feature_column.embedding_column(dair, 10),\n  \n        # Numeric columns\n        plat, plon, dlat, dlon,\n        latdiff, londiff, euclidean, manhattan, haversine\n    ]\n    estimator = tf.estimator.DNNLinearCombinedRegressor(\n        #model_dir = model_dir,\n        linear_feature_columns = wide_columns,\n        dnn_feature_columns = deep_columns,\n        dnn_hidden_units = hidden_units,\n        dnn_optimizer = dnn_optimizer,\n        linear_optimizer = linear_optimizer,\n        loss_reduction = tf.losses.Reduction.MEAN\n    )\n    \n    return estimator","7c5a18b0":"BATCH_SIZE = 512\nMAX_STEPS = 200000\nNBUCKETS = 10\nLEARNING_RATE = 0.0001\nHIDDEN_UNITS = [64, 64, 64, 4]","97f14e29":"optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\nestimator = build_estimator(INPUT_COLUMNS, \n                            NBUCKETS, \n                            HIDDEN_UNITS, \n                            dnn_optimizer=optimizer)","5c34d6c5":"def pandas_input_fun(df, batch_size=None, mode=tf.estimator.ModeKeys.TRAIN):\n    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n        label = df['fare_amount']\n        return tf.estimator.inputs.pandas_input_fn(x = df, \n                                                   y = label, \n                                                   batch_size=batch_size, \n                                                   num_epochs=100,\n                                                   shuffle=True\n                                                )\n    elif mode == tf.estimator.ModeKeys.EVAL:\n        label = df['fare_amount']\n        return tf.estimator.inputs.pandas_input_fn(x = df, \n                                                   y = label, \n                                                   batch_size=batch_size, \n                                                   num_epochs=100\n                                                )\n    elif mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.inputs.pandas_input_fn(x = df, \n                                                   y = None, \n                                                   num_epochs=1,\n                                                   shuffle=False\n                                                )","c4d55e5e":"train_spec = tf.estimator.TrainSpec(input_fn=pandas_input_fun(train_data_df, \n                                                              BATCH_SIZE, \n                                                              tf.estimator.ModeKeys.TRAIN), \n                                    max_steps=MAX_STEPS\n                                   )\neval_spec = tf.estimator.EvalSpec(input_fn=pandas_input_fun(validation_data_df, \n                                                             BATCH_SIZE, \n                                                             tf.estimator.ModeKeys.EVAL), \n                                  steps=500\n                                 )","1ab99194":"tf.estimator.train_and_evaluate(estimator, train_spec=train_spec, eval_spec=eval_spec)","4cc26ccd":"predictions = estimator.predict(input_fn=pandas_input_fun(validation_data_df, \n                                                          None, \n                                                          tf.estimator.ModeKeys.PREDICT)\n                               )\npredictions_pd = pd.DataFrame(predictions)\nmetrics.mean_squared_error(validation_data_df.fare_amount, predictions_pd.predictions)","cc5e6c49":"plt.scatter(validation_data_df.euclid,validation_data_df.fare_amount, color='red')\nplt.scatter(validation_data_df.euclid,predictions_pd.predictions, color='blue')\n#plt.plot(validation_data_df.euclid, predictions_pd.predictions, color='blue')\n_ = plt.show()","a1a367c8":"test_df = pd.read_csv(TEST_FILE_NAME, dtype=datatypes, parse_dates=['pickup_datetime'])\ntest_df = process(test_df)\n#test_df = process_date(test_df)\n#test_df = process_distance(test_df)\ntest_df[scale_cols] = scaler.transform(test_df[scale_cols])\ntest_df.head()","e9477dd0":"test_data_df = get_required_data_df(test_df)\ntest_data_df.head()","87caa0f9":"predictions = estimator.predict(input_fn=pandas_input_fun(test_data_df, \n                                                          None, \n                                                          tf.estimator.ModeKeys.PREDICT)\n                               )","4806c8b7":"predictions_df = pd.DataFrame(predictions)\npredictions_df = pd.DataFrame(predictions_df['predictions'].apply(lambda x: x[0]))\nsubmission = pd.DataFrame(\n    {'key': test_df.key, 'fare_amount': predictions_df.predictions},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission.csv', index = False)","4006360e":"submission.head()","307012bb":"We can see there are some null values so lets drop them for now","f80ecca2":"After adding new columns lets see if there is any outliers","41794eef":"Feature engineering","8c7662fa":"Predictions","8536d5d7":"Start building the model","e8d9a159":"There are outliers in fare amout so lets clean up those as well","689222ce":"There are some outliers in on passenger cout so lets clean up those values","5c63ba15":"There are some outliers latitude and longidute so we need to filter out those outliers","b274e3ad":"Validation data predictions","4d24ef92":"Estimator using Tensorflow","00cbe7d7":"Lets add some engineered features","116b7419":"Split dataframe into training and validation set and scale the data"}}