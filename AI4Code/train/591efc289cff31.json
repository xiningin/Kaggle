{"cell_type":{"462aecbf":"code","1e9769c5":"code","abb0b912":"code","d5f2ee23":"code","05761804":"code","84775f97":"code","85bb829f":"code","14fd8301":"code","d298eaa4":"code","134709c6":"code","5fed63dc":"code","027aa3a2":"code","0ee13d81":"code","9dc484cd":"code","ece310c7":"code","1cf1995a":"code","e62e3166":"code","c636ff59":"code","cd0be007":"code","7bd4e4ff":"code","f4b1322f":"code","8848cc1a":"code","ac5442c2":"code","3c5665fa":"code","1e53ca2e":"code","85df5138":"code","a8424d46":"code","ba9fc633":"code","bf5ff2e1":"code","c12fd04d":"code","be14022c":"code","ab572d10":"code","aba39420":"code","f8216dcb":"code","44f3cbbd":"code","1a500df3":"code","8777a295":"code","3e1b8f6e":"code","d452ef3a":"code","2e3b9a71":"code","5e349b72":"code","5b8616f7":"code","019aa69e":"code","28580ed1":"code","131b758a":"code","7e9ec50d":"code","2a498a3c":"code","da64e3f8":"code","f4574f68":"code","e3cd10ac":"code","d8cdbb47":"code","b6ea0829":"code","51f33706":"code","dccff354":"code","7cee7472":"code","f8c593db":"code","f1797970":"code","ad224131":"code","6d9e4a95":"code","d641153a":"code","ac6b3a08":"code","f7142292":"code","3d24b430":"code","cd9ac589":"code","17628cae":"code","dac5c2f2":"code","6d16efe3":"code","0c6d8324":"code","c89ad5c7":"code","1bf005a2":"code","2680a978":"code","3e6ec7c4":"code","03893534":"code","327dfb63":"code","32931034":"code","0f4f8ea4":"code","a8aed11d":"code","98ee4658":"code","f337fb88":"code","d9f308f0":"code","b451dd3b":"code","12d70b7f":"code","03b788ff":"code","756953fc":"markdown","c5949845":"markdown","2e88c61a":"markdown","a05d1c58":"markdown","1e735e31":"markdown","3b39dd36":"markdown","8ae24592":"markdown","ed388f0f":"markdown","252ce1b6":"markdown","e7bc2c1e":"markdown","055e3fb6":"markdown","e57ad677":"markdown","ec820444":"markdown","824760a1":"markdown","b5d8604c":"markdown","60e19c1d":"markdown","30124f2a":"markdown","ef59e3ae":"markdown","fefba3ca":"markdown","834dde1d":"markdown","31f6abc7":"markdown","3b531f8a":"markdown","60c82eec":"markdown","e5727212":"markdown","b398fff6":"markdown","3687f941":"markdown","5780531e":"markdown","e623a5e8":"markdown","60489fb9":"markdown","b45eec08":"markdown","965544f2":"markdown","43a7459a":"markdown","d25ad8e4":"markdown","1c81ae86":"markdown","bbafd9e7":"markdown","e0b9bb71":"markdown","f7859f88":"markdown","83fc01b4":"markdown","945cefd4":"markdown","f9a8fc02":"markdown","b81fbeca":"markdown","ae8a622b":"markdown","57b184c4":"markdown","9f9694c5":"markdown","2baa0e39":"markdown","5a12d0d5":"markdown","5e917558":"markdown","917995d6":"markdown","d58b726e":"markdown","1b4666a0":"markdown","608d449a":"markdown","a1ec1de1":"markdown","2441c425":"markdown","baaab490":"markdown","dd566f9f":"markdown","1d4fe840":"markdown","176c974e":"markdown","9bce035a":"markdown","a14b145e":"markdown","ce625bea":"markdown","038542c8":"markdown","eb472360":"markdown","0c9e8547":"markdown"},"source":{"462aecbf":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\nplt.rc(\"font\", size=14)\nsns.set(rc={'figure.figsize':(12,5),'figure.dpi':100})","1e9769c5":"# Read CSV train data file into DataFrame\ntrain_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\n# Read CSV test data file into DataFrame\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")","abb0b912":"# preview train data\ntrain_df.head()","d5f2ee23":"# preview test data\ntest_df.head()","05761804":"pd.DataFrame([train_df.shape,test_df.shape],index=['Train Dataset','Test Dataset'],columns=['Rows','Columns'])","84775f97":"train_miss=train_df.isna().sum().sort_values(ascending=False)\ntrain_miss[train_miss>0]","85bb829f":"test_miss=test_df.isna().sum().sort_values(ascending=False)\ntest_miss[test_miss>0]","14fd8301":"pd.DataFrame([[train_df.isna().sum().sum(),test_df.isna().sum().sum()]],columns=['Train Dataset','Test Dataset'],index=['Missing Values'])","d298eaa4":"def missing_percent(df):\n    nan_precent=100*(df.isnull().sum().sort_values(ascending=False)\/len(df))\n    # Filter to find the missing count > 0\n    return nan_precent[nan_precent>0]","134709c6":"nan_percent_train=missing_percent(train_df)\npd.DataFrame({'Column_Name':nan_percent_train.index, 'Missing_Percentage':nan_percent_train.values}).style.background_gradient(cmap='Reds')","5fed63dc":"nan_percent_test=missing_percent(test_df)\npd.DataFrame({'Column_Name':nan_percent_test.index, 'Missing_Percentage':nan_percent_test.values}).style.background_gradient(cmap='Reds')","027aa3a2":"train_data=train_df.copy()","0ee13d81":"test_data=test_df.copy()","9dc484cd":"def draw_barplot(data):\n    bar_plot=sns.barplot(x=data.index,y=data)\n    plt.xticks(rotation=45)\n    bar_plot.bar_label(bar_plot.containers[0],fmt='%.2f')\n    plt.show()","ece310c7":"draw_barplot(nan_percent_train)","1cf1995a":"draw_barplot(nan_percent_test)","e62e3166":"train_data.drop('Cabin', axis=1, inplace=True)","c636ff59":"test_data.drop('Cabin', axis=1, inplace=True)","cd0be007":"sns.distplot(train_data['Age'],bins=15,color='teal');\nplt.xlim(-10,85)\nplt.show()","7bd4e4ff":"train_data[\"Age\"].median()","f4b1322f":"train_data[\"Age\"].fillna(train_data[\"Age\"].median(skipna=True), inplace=True)","8848cc1a":"test_data[\"Age\"].fillna(train_data[\"Age\"].median(skipna=True), inplace=True)","ac5442c2":"(1\/len(train_data))*100","3c5665fa":"train_data[\"Embarked\"].mode()","1e53ca2e":"train_data[\"Embarked\"].fillna(train_data[\"Embarked\"].mode()[0], inplace=True)","85df5138":"test_data[\"Embarked\"].isna().sum()","a8424d46":"(1\/len(test_data))*100","ba9fc633":"train_data['Fare'].median(skipna=True)","bf5ff2e1":"test_data['Fare'].fillna(train_data['Fare'].median(skipna=True),inplace=True)","c12fd04d":"train_data[\"Fare\"].isna().sum()","be14022c":"train_data.isna().sum().sum()","ab572d10":"test_data.isna().sum().sum()","aba39420":"plt.figure(figsize=(15,8))\nax = train_df[\"Age\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\ntrain_df[\"Age\"].plot(kind='density', color='teal')\nax = train_data[\"Age\"].hist(bins=15, density=True, stacked=True, color='orange', alpha=0.5)\ntrain_data[\"Age\"].plot(kind='density', color='orange')\nax.legend(['Raw Age','Adjusted Age'])\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","f8216dcb":"train_data['TravelAlone']=np.where((train_data[\"SibSp\"]+train_data[\"Parch\"])>0, 0, 1)\ntrain_data.drop('SibSp', axis=1, inplace=True)\ntrain_data.drop('Parch', axis=1, inplace=True)","44f3cbbd":"test_data['TravelAlone']=np.where((test_data[\"SibSp\"]+test_data[\"Parch\"])>0, 0, 1)\ntest_data.drop('SibSp', axis=1, inplace=True)\ntest_data.drop('Parch', axis=1, inplace=True)","1a500df3":"sns.distplot(train_data['Pclass']);","8777a295":"train_data['Pclass'] = train_data['Pclass'].astype(object)\ntest_data['Pclass'] = test_data['Pclass'].astype(object)","3e1b8f6e":"train_data.drop('PassengerId', axis=1, inplace=True)\ntrain_data.drop('Name', axis=1, inplace=True)\ntrain_data.drop('Ticket', axis=1, inplace=True)\n\n## Now, apply the same changes to the test data\n\ntest_data.drop('PassengerId', axis=1, inplace=True)\ntest_data.drop('Name', axis=1, inplace=True)\ntest_data.drop('Ticket', axis=1, inplace=True)","d452ef3a":"# Finding numerical features\nnumeric_train_data=train_data.select_dtypes(include=[np.number])\n## Survived , Age , Fare ,TravelAlone\n\n# Finding categorical features\ncategorical_train_data=train_data.select_dtypes(include='object')\n## Pclass , Sex , Embarked","2e3b9a71":"training=pd.get_dummies(categorical_train_data,drop_first=True)","5e349b72":"training.head()","5b8616f7":"final_train=pd.concat([training,numeric_train_data],axis=1)","019aa69e":"final_train.head()","28580ed1":"# Finding numerical features\nnumeric_test_data=test_data.select_dtypes(include=[np.number])\n## Survived , Age , Fare ,TravelAlone\n\n# Finding categorical features\ncategorical_test_data=test_data.select_dtypes(include='object')\n## Pclass , Sex , Embarked","131b758a":"testing=pd.get_dummies(categorical_test_data,drop_first=True)","7e9ec50d":"testing.head()","2a498a3c":"final_test=pd.concat([testing,numeric_test_data],axis=1)","da64e3f8":"final_test.head()","f4574f68":"sns.barplot('Pclass', 'Survived', data=train_df);\nplt.show()","e3cd10ac":"ax = sns.kdeplot(final_train[\"Age\"][final_train[\"Survived\"] == 1], color=\"darkturquoise\", shade=True)\nsns.kdeplot(final_train[\"Age\"][final_train[\"Survived\"] == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Age for Surviving Population and Deceased Population')\n# plt.axvline(x=28,color='g')\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","d8cdbb47":"sns.set(rc={'figure.figsize':(16,8),'figure.dpi':100})\navg_survival_byage = final_train[[\"Age\", \"Survived\"]].groupby(['Age'], as_index=False).mean()\nsns.set(font_scale = 0.75)\nsns.barplot(x='Age', y='Survived', data=avg_survival_byage, color=\"LightSeaGreen\")\nplt.xticks(rotation=90)\nplt.show()","b6ea0829":"final_train['IsMinor']=np.where(final_train['Age']<=16, 1, 0)\n## apply the same changes to the test data\nfinal_test['IsMinor']=np.where(final_test['Age']<=16, 1, 0)","51f33706":"sns.set(font_scale = 1)\nsns.barplot('Embarked', 'Survived', data=train_df);\nplt.show()","dccff354":"sns.barplot('Pclass', 'Survived', data=train_df,hue='Sex')\nplt.show()","7cee7472":"barplot_data=train_df[['Survived','Sex']]\nbarplot_data['TravelAlone']=np.where((train_df[\"SibSp\"]+train_df[\"Parch\"])>0, 0, 1)\n\nsns.barplot('TravelAlone', 'Survived', data=barplot_data, hue=\"Sex\");\nplt.show()","f8c593db":"sns.set(rc={'figure.figsize':(8,5),'figure.dpi':100})\nsns.barplot('Sex', 'Survived', data=train_df);\nplt.show()","f1797970":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV","ad224131":"X=final_train.drop('Survived',axis=1)\ny=final_train['Survived']","6d9e4a95":"log_model=LogisticRegression(solver='liblinear')\n# define the method\nrfe = RFE(estimator=log_model,n_features_to_select=7)\n# fit the model\nrfe.fit(X,y)","d641153a":"print('Selected features: %s' % list(X.columns[rfe.support_]))","ac6b3a08":"# Create the RFECV object\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfe_cv=RFECV(log_model, cv=5, scoring='accuracy')\nrfe_cv.fit(X,y)","f7142292":"print(f'Optimal number of feature : {rfe_cv.n_features_} ')","3d24b430":"Selected_features=list(X.columns[rfe_cv.support_])\nprint(f'All features : {list(X.columns)} \\n')\nprint(f'Selected features : {Selected_features}')","cd9ac589":"sns.set(rc={'figure.figsize':(8,5),'figure.dpi':100})\nsns.lineplot(data={\n    'N_features':range(1, len(rfe_cv.grid_scores_) + 1),\n    'CV_score':rfe_cv.grid_scores_\n}, x=\"N_features\", y=\"CV_score\")","17628cae":"X = final_train[Selected_features]","dac5c2f2":"from sklearn.model_selection import train_test_split, cross_val_score","6d16efe3":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","0c6d8324":"from sklearn.preprocessing import StandardScaler","c89ad5c7":"scaler=StandardScaler()","1bf005a2":"scaler.fit(X_train)","2680a978":"scaled_X_train=scaler.transform(X_train)\nscaled_X_test=scaler.transform(X_test)","3e6ec7c4":"from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score,plot_confusion_matrix,confusion_matrix,classification_report","03893534":"# train the model\nlog_model.fit(scaled_X_train, y_train)","327dfb63":"pd.DataFrame(log_model.coef_, columns=X.columns,index=['Coefficients']).head()","32931034":"# predicting test data\ny_pred = log_model.predict(scaled_X_test)","0f4f8ea4":"# evaluating the model\naccuracy=accuracy_score(y_test,y_pred)\nrecall=recall_score(y_test,y_pred)\nprecision=precision_score(y_test,y_pred)\nf1=f1_score(y_test,y_pred)","a8aed11d":"pd.DataFrame({'Logistic Metrics': [accuracy, recall, precision,f1]}, index=['accuracy', 'recall', 'precision','f1'])","98ee4658":"confusion_matrix(y_test, y_pred)","f337fb88":"\nplot_confusion_matrix(log_model, scaled_X_test, y_test);","d9f308f0":"print(classification_report(y_test, y_pred))","b451dd3b":"from sklearn.metrics import plot_roc_curve","12d70b7f":"plot_roc_curve(log_model, scaled_X_test, y_test)","03b788ff":"final_test['Survived'] = log_model.predict(final_test[Selected_features])\nfinal_test['PassengerId'] = test_df['PassengerId']\n\nsubmission = final_test[['PassengerId','Survived']]\n\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission.tail()","756953fc":"<a id=\"t4.2.\"><\/a>\n## 4.2. Exploration of Age","c5949845":"If \"Age\" is missing for a given row, I'll impute with 28 (median age).","2e88c61a":"##### Create categorical variable for traveling alone","a05d1c58":"Apply to same imputation for \"Age\" in the Test data as I did for my Training data (if missing, Age = 28)","1e735e31":"<a id=\"t4.3.\"><\/a>\n## 4.3. Exploration of Embarked Port","3b39dd36":"There were no missing values in the \"Embarked\" port variable in test data","8ae24592":"Remove the \"Cabin\" variable from the test data, as I've decided not to include it in my analysis.","ed388f0f":"##### Add the dummy variables to finalize the test set.","252ce1b6":"This is a very obvious difference.  Clearly being female greatly increased your chances of survival.","e7bc2c1e":"There are only 1 (0.24%) missing value for \"Fare\" in test data.\n","055e3fb6":"According to the Kaggle data dictionary, both SibSp and Parch relate to traveling with family.  For simplicity's sake (and to account for possible multicollinearity), I'll combine the effect of these variables into one categorical predictor: whether or not that individual was traveling alone.","e57ad677":"As we see, 7 variables were kept. ","ec820444":"Considering the survival rate of passengers under 16, I'll also include another categorical variable in my dataset: \"Minor\"","824760a1":"<a id=\"t3.4.\"><\/a>\n## 3.4. Fare - Missing Values","b5d8604c":"<font color=#e6005c>  Note: there is no target variable into test data (i.e. \"Survival\" column is missing), so the goal is to predict this target using different machine learning algorithms such as logistic regression. <\/font>","60e19c1d":"<a id=\"t4.6.\"><\/a>\n## 4.6. Exploration of Gender Variable","30124f2a":"<a id=\"t5.2.\"><\/a>\n## 5.2. Feature scaling","ef59e3ae":"<a id=\"t5.3.\"><\/a>\n## 5.3. Model evaluation","fefba3ca":"<a id=\"t3.5.\"><\/a>\n## 3.5. Additional Variables","834dde1d":"### Plotting missing value in train dataset","31f6abc7":"<a id=\"t3.3.\"><\/a>\n## 3.3. Embarked - Missing Values","3b531f8a":"##### check missing values in test data","60c82eec":"<a id=\"t2.\"><\/a>\n# 2. Import Data & Libraries","e5727212":"##### check missing values in train data","b398fff6":"# **Introduction**\nThe following kernel contains the steps enumerated below for assessing the Titanic survival dataset:\n\n1. [About Dataset](#t1.)\n2. [Import Data & Libraries](#t2.)\n3. [Data Quality & Missing Values](#t3.)\n    * 3.1. [Cabin - Missing Values](#t3.1.)\n    * 3.2. [Age - Missing Values](#t3.2.)\n    * 3.3. [Embarked - Missing Values](#t3.3.)\n    * 3.4. [Fare - Missing Values](#t3.4.)\n    * 3.5  [Additional Variables](#t3.5.)\n4. [Exploratory Data Analysis (EDA)](#t4.)\n    * 4.1.  [Exploration of Cabin](#t4.1.)\n    * 4.2.  [Exploration of Age](#t4.2.)\n    * 4.3.  [Exploration of Embarked Port](#t4.3.)\n    * 4.4.  [Exploration of Passenger Class](#t4.4.)\n    * 4.5.  [Exploration of Traveling Alone vs. With Family](#t4.5.)\n    * 4.6.  [Exploration of Gender Variable](#t4.6.)\n5. [Logistic Regression and Results](#t5.)\n    * 5.1. [Feature selection](#t5.1.)\n       * 5.1.1. [Recursive feature elimination](#t5.1.1.)\n       * 5.1.2. [Feature ranking with recursive feature elimination and cross-validation](#t5.1.2.)\n    * 5.2. [Feature scaling](#t5.2.)\n    * 5.3. [Model evaluation](#t5.3.)\n    * 5.4. [Evaluating curves and AUC](#t5.4.)\n    * 5.5. [Submit test predictions](#t5.5.)","3687f941":"<a id=\"t1.\"><\/a>\n# 1. About Dataset","5780531e":"Individuals traveling without family were more likely to die in the disaster than those with family aboard.<br>\nUnsurprisingly, most of the people who traveled alone were women","e623a5e8":"<a id=\"t5.4.\"><\/a>\n## 5.4. Evaluating curves and AUC","60489fb9":"<a id=\"t4.4.\"><\/a>\n## 4.4. Exploration of Passenger Class","b45eec08":"Now, apply the same changes to the test data","965544f2":"##### First let's see how many missing values are there in train and test dataset","43a7459a":"<a id=\"t5.1.2.\"><\/a>\n### 5.1.2. Feature ranking with recursive feature elimination and cross-validation\nIt is also possible to automatically select the number of features chosen by RFE.\nThis can be achieved by performing cross-validation evaluation of different numbers of features as we did in the previous section and automatically selecting the number of features that resulted in the best mean score.<br>\nThe RFECV is configured just like the RFE class regarding the choice of the algorithm that is wrapped. Additionally, the minimum number of features to be considered can be specified via the \u201cmin_features_to_select\u201d argument (defaults to 1) and we can also specify the type of cross-validation and scoring to use via the \u201ccv\u201d (defaults to 5) and \u201cscoring\u201d arguments (uses accuracy for classification) <br>\nHereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation.","d25ad8e4":"<a id=\"t5.\"><\/a>\n# 5. Logistic Regression and Results","1c81ae86":"<a id=\"t5.5.\"><\/a>\n## 5.5. Submit test predictions ","bbafd9e7":"##### Now Age feature in train dataset is normally distributed","e0b9bb71":"Since \"Age\" is (right) skewed, using the mean might give us biased results by filling in ages that are older than desired. To deal with this, we'll use the median to impute the missing values. ","f7859f88":"~20% of entries for passenger age are missing. Let's see what the 'Age' variable looks like in general.","83fc01b4":"As we see, **Fare** & **Age** features not selected","945cefd4":"There were no missing values in the \"Fare\" variable in train data","f9a8fc02":"Passengers who boarded in Cherbourg, France, appear to have the highest survival rate.<br>\nPassengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown.","b81fbeca":"### Plotting missing value in train dataset","ae8a622b":"The age distribution for survivors and deceased is actually very similar.  One notable difference is that, of the survivors, a larger proportion were children.  The passengers evidently made an attempt to save children by giving them a place on the life rafts. ","57b184c4":"Unsurprisingly, being a first class passenger was safest.","9f9694c5":"<a id=\"t3.\"><\/a>\n# 3. Data Quality & Missing Value Assessment","2baa0e39":"##### We see that **PClass** feature having int data type consists of discrete values","5a12d0d5":"<a id=\"t4.1.\"><\/a>\n## 4.1. Exploration of Cabin","5e917558":"77% of records are missing, which means that imputing information and using this variable for prediction is probably not wise.  We'll ignore this variable in our model.","917995d6":"<a id=\"t3.2.\"><\/a>\n## 3.2.    Age - Missing Values","d58b726e":"<a id=\"t5.1.\"><\/a>\n## 5.1. Feature selection\n\n<a id=\"t5.1.1.\"><\/a>\n### 5.1.1. Recursive feature elimination\nFeature selection refers to techniques that select a subset of the most relevant features (columns) for a dataset. Fewer features can allow machine learning algorithms to run more efficiently (less space or time complexity) and be more effective. Some machine learning algorithms can be misled by irrelevant input features, resulting in worse predictive performance.<br>\nTechnically, RFE is a wrapper-style feature selection algorithm that also uses filter-based feature selection internally.<br>\nRFE works by searching for a subset of features by starting with all features in the training dataset and successfully removing features until the desired number remains<br><br>","1b4666a0":"This Dataset contains the information about Titanic ship and included **891** Rows and **12** columns.<br>\nIn this notebook We try to find the best ML model to predict which passengers survived the Titanic shipwreck\n##### Columns info :\n* **PassengerId :** ID of Passengers\n\n* **Survival :**  Survival (1) or Not (0)\n\n* **Pclass :**  Class of Travel ( 1 = 1st, 2 = 2nd, 3 = 3rd )\n\n* **Name :** Name of Passengers\n\n* **Sex :**  Gender\n\n* **Age :**  Age in years\n\n* **SibSp :**  Number of Siblings \/ Spouses aboard\n\n* **Parch :**  Number of parents \/ Children aboard \n\n* **Fare :**  Passenger fare\n\n* **Cabin :**  Cabin number\n\n* **Embarked :**  Port of Embarkation(C = Cherbourg, Q = Queenstown, S = Southampton)\n","608d449a":"#### check missing values in adjusted train and test data","a1ec1de1":"Summarize the selection of the attributes","2441c425":"<a id=\"t4.5.\"><\/a>\n## 4.5. Exploration of Traveling Alone vs. With Family","baaab490":"\n<font color=#e6005c> <h4> If you liked this Notebook, please do upvote :)<\/h4> <\/font>","dd566f9f":"Unsurprisingly, being a first class passenger was safest.<br>\nIn each class passenger, the number of women was more than men.","1d4fe840":"#### Data Overview","176c974e":"If \"Embarked\" is missing for a riven row, I'll impute with \" S ( Southampton ) \" (the most common boarding port).","9bce035a":"<a id=\"t3.1.\"><\/a>\n## 3.1. Cabin - Missing Values","a14b145e":"There are only 2 (0.22%) missing values for \"Embarked\", so we can just impute with the port where most people boarded.","ce625bea":"So we can just impute with 14.45 (median Fare).","038542c8":"<a id=\"t4.\"><\/a>\n# 4. Exploratory Data Analysis (EDA)","eb472360":"Determine the Features & Target Variable","0c9e8547":"##### Plot number of features VS cross-validation scores"}}