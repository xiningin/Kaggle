{"cell_type":{"f72cb48c":"code","ed713c87":"code","c9e6614d":"code","20aadfdb":"code","29a7d69c":"code","0a526e8e":"code","b31d11a2":"code","56cccc03":"code","0cb90cc5":"code","cca46df7":"code","ed539c10":"code","31613950":"code","624c3f5e":"code","fdc35a63":"code","4f8a6dd5":"code","1a7333f3":"code","103c36a4":"code","1880a28d":"code","6e5e0bc3":"code","f92004b0":"code","f5c2270b":"code","5addb305":"code","9867ca86":"code","72a38277":"code","3615a5c4":"code","5a03eb45":"code","0e24a2d5":"code","648e1a5d":"code","9b59f7b0":"code","ca123465":"code","58f8971f":"code","52be75e7":"code","cd1297ac":"code","52d21433":"code","e40ffe81":"code","035bfa6a":"code","a0bd95c2":"code","19378db4":"code","099064df":"code","be37b60b":"code","6c02b159":"code","850a647d":"code","2f323a3d":"code","fe4f616d":"code","4dd55cc0":"code","52c128a6":"code","3063c65b":"code","52670247":"code","6f4a572a":"code","eb15c739":"code","832491c7":"code","a8523ba0":"code","a77958ea":"code","f4c62f4f":"code","125dd5e2":"code","292f754a":"markdown","c3ba639c":"markdown","da678ce1":"markdown","ab5c1a3b":"markdown","feec8816":"markdown","f41e2643":"markdown","518f4f52":"markdown","2a0f6d86":"markdown","dd83fc83":"markdown","3a58c8d2":"markdown","c2a3ca50":"markdown","bf395749":"markdown","6d25cb2c":"markdown","d6950ff4":"markdown"},"source":{"f72cb48c":"\nimport torch\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nfrom tqdm.notebook import tqdm\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport pickle\n\n\n# set numpy seed for reproductibility\nnp.random.seed(seed=42)\n%matplotlib inline","ed713c87":"project_name='credit-card-GAN'","c9e6614d":"DATA_DIR = '..\/input\/creditcardfraud\/creditcard.csv'","20aadfdb":"df = pd.read_csv(DATA_DIR)\ndf.head()","29a7d69c":"df.describe()","0a526e8e":"df.isnull().sum()","b31d11a2":"df.shape","56cccc03":"mask = df['Class'] < 1","0cb90cc5":"y_train = df[mask]","cca46df7":"y_train.shape","ed539c10":"y_train.describe()","31613950":"y_test = df[~mask]","624c3f5e":"y_test.shape","fdc35a63":"y_test.describe()","4f8a6dd5":"y_train = y_train.drop('Class', axis=1)","1a7333f3":"y_train = y_train.drop('Time', axis=1)","103c36a4":"y_train.shape","1880a28d":"y_test = y_test.drop('Class', axis=1)","6e5e0bc3":"y_test = y_test.drop('Time', axis=1)","f92004b0":"y_test.shape","f5c2270b":"We transform the dataframes to numpy array to be able to tranform them to tensor.","5addb305":"y_train_array = y_train.to_numpy()\ny_test_array = y_test.to_numpy()","9867ca86":"dataset = torch.from_numpy(y_train_array).float()\ntest_fraud_ds = torch.from_numpy(y_test_array).float()","72a38277":"torch.manual_seed(42)\nsize1 = 28431\nsize2 = len(dataset) - size1","3615a5c4":"train_ds, test_real_ds = random_split(dataset, [size2, size1])\nlen(train_ds), len(test_real_ds)","5a03eb45":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","0e24a2d5":"device = get_default_device()\ndevice","648e1a5d":"batch_size = 256","9b59f7b0":"train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)","ca123465":"train_dl = DeviceDataLoader(train_dl, device)","58f8971f":"dim_fake_in = 4096\ndim_fake_out = 29\ndim_real_in = 29\ndim_real_out = 1","52be75e7":"discriminator = nn.Sequential(\n            nn.Linear(dim_real_in, 4096),\n            nn.Sigmoid(),\n            nn.Linear(4096, 2048),\n            nn.ReLU(),\n            nn.Linear(2048, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, dim_real_out),\n            nn.Sigmoid()\n            )","cd1297ac":"discriminator = to_device(discriminator, device)","52d21433":"generator = nn.Sequential(\n            nn.Linear(dim_fake_in, 4096),\n            nn.Sigmoid(),\n            nn.Linear(4096, 2048),\n            nn.ReLU(),\n            nn.Linear(2048, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, dim_fake_out)\n        )","e40ffe81":"generator = to_device(generator, device)","035bfa6a":"def train_discriminator(real_data, opt_d):\n    # Clear discriminator gradients\n    opt_d.zero_grad()\n\n    # Pass real data through discriminator\n    real_preds = discriminator(real_data)\n    real_targets = torch.ones(real_data.size(0), 1, device=device)\n    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n    real_score = torch.mean(real_preds).item()\n    \n    # Generate fake data\n    latent = torch.randn(batch_size, latent_size, device=device)\n    fake_data = generator(latent)\n\n    # Pass fake data through discriminator\n    fake_targets = torch.zeros(fake_data.size(0), 1, device=device)\n    fake_preds = discriminator(fake_data)\n    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n    fake_score = torch.mean(fake_preds).item()\n\n    # Update discriminator weights\n    loss = real_loss + fake_loss\n    loss.backward()\n    opt_d.step()\n    return loss.item(), real_score, fake_score","a0bd95c2":"latent_size = dim_fake_in","19378db4":"def train_generator(opt_g):\n    # Clear generator gradients\n    opt_g.zero_grad()\n    \n    # Generate fake data\n    latent = torch.randn(batch_size, latent_size, device=device)\n    fake_data = generator(latent)\n    \n    # Try to fool the discriminator\n    preds = discriminator(fake_data)\n    targets = torch.ones(batch_size, 1, device=device)\n    loss = F.binary_cross_entropy(preds, targets)\n    \n    # Update generator weights\n    loss.backward()\n    opt_g.step()\n    \n    return loss.item()","099064df":"def fit(epochs, lr):\n    torch.cuda.empty_cache()\n    \n    # Losses & scores\n    losses_g = []\n    losses_d = []\n    real_scores = []\n    fake_scores = []\n    \n    # Create optimizers\n    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n    \n    for epoch in range(epochs):\n        for real_data in train_dl:\n            # Train discriminator\n            loss_d, real_score, fake_score = train_discriminator(real_data, opt_d)\n            # Train generator\n            loss_g = train_generator(opt_g)\n            \n        # Record losses & scores\n        losses_g.append(loss_g)\n        losses_d.append(loss_d)\n        real_scores.append(real_score)\n        fake_scores.append(fake_score)\n        \n        # Log losses & scores (last batch).\n        # We print the losses and scores from the last batch of data for every 100th epoch, to track the progress of training.\n        if (epoch+1) % 100 == 0 or epoch == epochs-1:\n            print(\"Epoch [{}\/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n    \n    return losses_g, losses_d, real_scores, fake_scores","be37b60b":"lr = 0.0001\nepochs = 300","6c02b159":"%%time\nhistory = fit(epochs, lr)","850a647d":"losses_g, losses_d, real_scores, fake_scores = history","2f323a3d":"plt.plot(losses_d, '-')\nplt.plot(losses_g, '-')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['Discriminator', 'Generator'])\nplt.title('Losses');","fe4f616d":"plt.plot(real_scores, '-')\nplt.plot(fake_scores, '-')\nplt.xlabel('epoch')\nplt.ylabel('score')\nplt.legend(['Real', 'Fake'])\nplt.title('Scores');","4dd55cc0":"X_fake_g = torch.randn(dim_fake_in).float()\nX_fake_d = torch.randn(dim_fake_out)\nX_real_d = test_real_ds[0]\nX_fraud = test_fraud_ds[0]","52c128a6":"X_fake_g = to_device(X_fake_g, device)\nX_fake_d = to_device(X_fake_d, device)\nX_real_d = to_device(X_real_d, device)\nX_fraud = to_device(X_fraud, device)","3063c65b":"# generate a sequence from random values with generator\nprint(generator(X_fake_g))\n\n# evaluate wrong seq with discriminator\nprint(discriminator(X_fake_d))\n\n# evaluate fraud with discriminator\nprint(discriminator(X_fraud))\n\n# evaluate good seq with discriminator`\ndiscriminator(X_real_d)","52670247":"# Save models\npickle.dump(discriminator, open(\".\/d_gan.p\", \"wb\"))\npickle.dump(generator, open(\".\/g_gan.p\", \"wb\"))\n\n# load model\nloaded_D = pickle.load(open(\".\/d_gan.p\", \"rb\"))\nloaded_G = pickle.load(open(\".\/g_gan.p\", \"rb\"))","6f4a572a":"# generate a sequence from random values with generator\nprint(generator(X_fake_g))\n\n# generate a sequence from random values with loaded generator\nprint(loaded_G(X_fake_g))","eb15c739":"# evaluate wrong seq with discriminator\nprint(discriminator(X_fake_d))\n\n# evaluate fraud seq with discriminator\nprint(discriminator(X_fraud))\n\n# evaluate wrong seq with loaded discriminator\nprint(loaded_D(X_fake_d))\n\n# evaluate fraud seq with discriminator\nprint(discriminator(X_fraud))","832491c7":"# evaluate good seq with discriminator\nprint (discriminator(X_real_d))\n\n# evaluate good seq with loaded discriminator\nprint (loaded_D(X_real_d))","a8523ba0":"torch.save(discriminator.state_dict(), 'discriminator.pth')\ntorch.save(generator.state_dict(), 'generator.pth')","a77958ea":"!pip install jovian --upgrade --quiet","f4c62f4f":"import jovian","125dd5e2":"jovian.commit(project=project_name, environment=None, outputs=['discriminator.pth', 'generator.pth'])","292f754a":"We can now move our training data loader using DeviceDataLoader for automatically transferring batches of data to the GPU (if available).\n\n\n\n\n\n\n","c3ba639c":"We drop the variables 'Class' and 'Time' from the dataset. ","da678ce1":"# Using a GPU\n\nTo use a GPU, if one is available, we define a couple of helper functions (get_default_device & to_device) and a helper class DeviceDataLoader to move our model & data to the GPU, if one is available.\n\n","ab5c1a3b":"We move the generator to the existing device","feec8816":"# Generator Training","f41e2643":"We are now ready to train the model.","518f4f52":"Separating the non-fraud data with the fraud data. We conduct the training only on the non-fraud data to make the model be able to detect normal transactions.","2a0f6d86":"# Generator Network","dd83fc83":"# Discriminator Training","3a58c8d2":"# Full Training Loop\n\nLet's define a fit function to train the discriminator and generator in tandem for each batch of training data. We'll use the Adam optimizer with some custom parameters (betas) that are known to work well for GANs.","c2a3ca50":"# Discriminator Network","bf395749":"We can also visualize how the loss changes over time. Visualizing losses is quite useful for debugging the training process. For GANs, we expect the generator's loss to reduce over time, without the discriminator's loss getting too high.","6d25cb2c":"Dataloader","d6950ff4":"We move the discriminator to the existing device"}}