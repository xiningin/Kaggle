{"cell_type":{"665ded88":"code","c72eb204":"code","6861c623":"code","00808e37":"code","82a0c8dc":"code","1fc170ab":"code","5bd104b7":"code","2b163c9b":"code","fcde1aae":"code","354da3bb":"code","2e0b8012":"code","06ff0ea4":"code","20e47e08":"code","313466b3":"code","93ac87f9":"code","4661b348":"code","7a6a8e3b":"code","7a36894d":"code","ef6eae1b":"code","321cd236":"code","c948432e":"code","1981d39d":"code","da2364de":"code","4eb0b118":"code","120febeb":"code","057e8790":"code","b166b46c":"code","e8fa5beb":"code","9bbb8578":"code","05756c6a":"code","11dc6433":"code","6fa9ed5e":"code","22da7be9":"code","030a32c7":"code","c6235ad1":"code","fd64b429":"code","fdc7ae5a":"code","2daab7fa":"code","7b579794":"code","68ff0c97":"code","51478472":"code","a4c668a1":"code","bf28e69d":"code","3cdabde3":"code","266f57a7":"code","b201ae57":"code","f05647a7":"code","430a6232":"code","ca0f5f0e":"code","b4dab514":"code","ac67a121":"code","ee09b323":"code","881b7c93":"code","3c3c28d0":"code","2f33b5c6":"markdown","3d8cf5f9":"markdown","e3039c0c":"markdown","a7ac6b79":"markdown","6a4463ec":"markdown","07137b32":"markdown","934ee25b":"markdown","5ad67b44":"markdown","59784ca6":"markdown","5c9460e6":"markdown","f3b23892":"markdown","c497a46d":"markdown","9b1f3642":"markdown","b30b8a3c":"markdown","5212f516":"markdown","6b3ad50f":"markdown","ad751072":"markdown","dca84e3e":"markdown","a26c9fb0":"markdown","f90208c8":"markdown","73710469":"markdown","59c4ab3d":"markdown","b3045288":"markdown","8fa44554":"markdown","7099171d":"markdown","a50bc45e":"markdown","4b454a7f":"markdown","2a1b713f":"markdown","74648ca9":"markdown","44d75759":"markdown","1c50d81f":"markdown","f85fcd4c":"markdown","f7967dc6":"markdown","539fecb7":"markdown","839ad315":"markdown","a512ccc5":"markdown","2b19b293":"markdown","1ffdc53c":"markdown","7bb37547":"markdown","bb877d1b":"markdown","753bb061":"markdown","4da5fbe8":"markdown","1f830b30":"markdown","0a7444d5":"markdown","4c809973":"markdown","1fa79b67":"markdown","471d1c84":"markdown","11be6923":"markdown","2348eb1a":"markdown","9b9f9054":"markdown","b784a502":"markdown","412437dc":"markdown","30e85cae":"markdown","bf0eba7b":"markdown","4c144710":"markdown","4464e896":"markdown","d8a67812":"markdown","0fd9df81":"markdown","1cb415f3":"markdown","97b83a2e":"markdown"},"source":{"665ded88":"import pandas as pd\r\nimport seaborn as sns\r\nimport numpy as np\r\nimport sklearn.datasets\r\nfrom matplotlib import pyplot as plt\r\nfrom factor_analyzer import FactorAnalyzer","c72eb204":"lol = pd.read_csv(\"..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv\")\r\nlol.info()\r\npd.options.display.max_columns = None\r\nlol.head(20)","6861c623":"#We'll drop missing values rows. No need to as there are 9879 non-null vlaues for each column for the total 9879 entries. Still good practice:\r\nlol.dropna(inplace=True)","00808e37":"#setting up our new DataFrame for Factor Analysis\r\ndf_clean = lol.copy()\r\ndf_clean.head(10)","82a0c8dc":"#We're going to eliminate variables with reasons why:\r\n#1. gameId is an obvious candidate to eliminate since the unique ID and how it varies plays no role. \r\ndf_clean = df_clean.drop('gameId',axis=1)\r\n#2. For the purpose of FA, we can drop blueWins (although we will retain it for regression later).\r\ndf_clean = df_clean.drop('blueWins',axis=1)\r\ny = lol['blueWins']\r\n#3. We can eliminate elite monster kills as it can be derived from dragon kills + herald kills.\r\ndf_clean = df_clean.drop('blueEliteMonsters',axis=1)\r\ndf_clean = df_clean.drop('redEliteMonsters',axis=1)\r\n#4. blueFirstBlood has a perfect negative correlation with redFirstBlood, so we'll drop one of them.\r\ndf_clean = df_clean.drop('redFirstBlood',axis=1)\r\n#5. blueGoldDiff\/blueExperienceDiff are calculated variables too -> blueTotalGold - redTotalGold & blueTotalExperience - redTotalExperience.\r\ndf_clean = df_clean.drop('blueGoldDiff',axis=1)\r\ndf_clean = df_clean.drop('redGoldDiff',axis=1)\r\ndf_clean = df_clean.drop('blueExperienceDiff',axis=1)\r\ndf_clean = df_clean.drop('redExperienceDiff',axis=1)\r\n#6. Because the dataset's context is game stats until the 10 minute mark, GPM and XPM are literally just total gold\/exp divided by 10. We can drop them.\r\ndf_clean = df_clean.drop('blueCSPerMin',axis=1)\r\ndf_clean = df_clean.drop('redCSPerMin',axis=1)\r\ndf_clean = df_clean.drop('blueGoldPerMin',axis=1)\r\ndf_clean = df_clean.drop('redGoldPerMin',axis=1)\r\n#7. blueKills has a perfect negative correlation with redDeaths & blueDeaths has a perfect negative correlation with blueDeaths.\r\ndf_clean = df_clean.drop('redKills',axis=1)\r\ndf_clean = df_clean.drop('redDeaths',axis=1)\r\n#8. While not perfectly correlated, avglevel and totalExperience are highly correlated. We will drop one of them to avoid colinearity. Because leveling up becomes incrementally more difficult as you grow in levels, the average level is not the best indication of experience. Better to keep total experience as a better indication.\r\ndf_clean = df_clean.drop('blueAvgLevel',axis=1)\r\ndf_clean = df_clean.drop('redAvgLevel',axis=1)","1fc170ab":"df_clean.info()","5bd104b7":"fig, ax = plt.subplots(figsize=(25,25))\r\nax = sns.heatmap(df_clean.corr(), annot=True, ax=ax, cmap=\"YlGnBu\", linewidths = .5); ax.set_title(\"Correlation between LoL variables\", fontsize = 50)\r\nplt.show()\r\n#we can see there is still high colinearity between some variables, but for now we've eliminated most of the variables which would've had correlations > 0.9 with our retained variables. We'll continue to proceed with what we have, but we'll look to eliminate a lot more variables!","2b163c9b":"win = pd.DataFrame({'blueWins':df_clean.corrwith(y,axis=0)})\r\nfig, ax = plt.subplots()\r\nx_labels = ['blueWins']\r\ny_labels = df_clean.columns.tolist()\r\nsns.set(font_scale=0.5)\r\nplt.title('Correlation with winning')\r\nsns.heatmap(win,cmap=\"YlGnBu\", xticklabels = x_labels, yticklabels = y_labels, center=0, square=True, linewidths=0.05,cbar_kws={\"shrink\": 0.5}, annot = True, annot_kws={\"fontsize\":1})","fcde1aae":"#Calculating eigenvalues and seeing which ones are greater than 1:\r\nfa = FactorAnalyzer()\r\nfa.fit(df_clean)\r\nev,v = fa.get_eigenvalues()\r\ncount = sum(1 for i in ev if i > 1)\r\nprint(count)\r\nbig_evs = sum(i for i in ev if i > 1)\r\ntotal_evs = sum(ev)\r\nprint(big_evs)\r\nprint(float(big_evs\/total_evs))","354da3bb":"#plotting scree-plot:\r\nplt.scatter(range(1,df_clean.shape[1]+1),ev)\r\nplt.plot(range(1,df_clean.shape[1]+1),ev) \r\nplt.title('Scree Plot')\r\nplt.xlabel('Factors')\r\nplt.ylabel('Eigenvalue')\r\nplt.axhline(1.0, color = 'black') #visualise which eigenvalues are above and below 1\r\nplt.grid(b=True, which='major', color='#666666', linestyle='-')\r\n# Show the minor grid lines with very faint and almost transparent grey lines\r\nplt.minorticks_on()\r\nplt.grid(b=True, which='minor', color='#999999', linestyle='-.')\r\nplt.show()\r\n#visualisation of the 9 eigenvalues > 1.","2e0b8012":"#Performing Factor Analysis:\r\nfa2 = FactorAnalyzer(count, rotation=\"varimax\")\r\nfa2.fit(df_clean)\r\nx_labels = ['Factor ' + str(i) for i in range(1,count+1)]\r\ny_labels = df_clean.columns.tolist()\r\nsns.set(font_scale=0.5)\r\nplt.title('Loading Factors - ' + str(count))\r\nload = sns.heatmap(fa2.loadings_,cmap=\"coolwarm\", xticklabels = x_labels, yticklabels = y_labels, center=0, square=True, linewidths=.2,cbar_kws={\"shrink\": 0.5}, annot = True, annot_kws={\"fontsize\":1})","06ff0ea4":"var_check = np.vstack((fa2.get_communalities(), fa2.get_uniquenesses(),np.array(fa2.get_communalities() + fa2.get_uniquenesses()))).tolist()\r\ny_labels = ['Communality','Uniqueness', 'Total Variance']\r\nx_labels = df_clean.columns.tolist()\r\nsns.set(font_scale=0.5)\r\nplt.title('Communality-Uniqueness of Variables')\r\nload = sns.heatmap(var_check,cmap=\"RdBu\", xticklabels = x_labels, yticklabels = y_labels, center=0, square=True, linewidths=.2,cbar_kws={\"shrink\": 0.5}, annot = True, annot_kws={\"fontsize\":1})","20e47e08":"#dropping mentioned variables:\r\ncols = ['blueWardsPlaced','blueWardsDestroyed','redWardsPlaced','redWardsDestroyed','blueFirstBlood', 'blueHeralds', 'redHeralds', 'blueTowersDestroyed', 'redTowersDestroyed', 'blueTotalJungleMinionsKilled', 'redTotalJungleMinionsKilled']\r\ndf_fa = df_clean.drop(cols,axis=1)","313466b3":"df_fa.head(10)","93ac87f9":"#Time to observe the correlation matrix of our dataset:\r\nfig, ax = plt.subplots(figsize=(25,25))\r\nax = sns.heatmap(df_fa.corr(), annot=True, ax=ax, cmap=\"YlGnBu\", linewidths = .5); ax.set_title(\"Correlation between LoL variables v2\", fontsize = 50)\r\nplt.show()\r\n#We can see there is still high colinearity between some variables, but from the initial Factor Analysis, I can with good conscience say these variables have high communality and are relevant.","4661b348":"from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\r\ncalculate_bartlett_sphericity(df_fa)","7a6a8e3b":"from factor_analyzer.factor_analyzer import calculate_kmo\r\nkmo_all,kmo_model=calculate_kmo(df_fa)\r\nkmo_model","7a36894d":"from sklearn.preprocessing import StandardScaler\r\nsc = StandardScaler()\r\ndf_fas = sc.fit_transform(df_fa)","ef6eae1b":"#Calculating eigenvalues and seeing which ones are greater than 1:\r\nfa = FactorAnalyzer()\r\nfa.fit(df_fas)\r\nev,v = fa.get_eigenvalues()\r\ncount = sum(1 for i in ev if i > 1)\r\nprint(count)\r\nbig_evs = sum(i for i in ev if i > 1)\r\ntotal_evs = sum(ev)\r\nprint(big_evs)\r\nprint(float(big_evs\/total_evs))\r\n","321cd236":"df_fas.shape","c948432e":"#plotting scree-plot:\r\nplt.scatter(range(1,df_fas.shape[1]+1),ev)\r\nplt.plot(range(1,df_fas.shape[1]+1),ev) \r\nplt.title('Scree Plot v2')\r\nplt.xlabel('Factors')\r\nplt.ylabel('Eigenvalue')\r\nplt.axhline(1.0, color = 'black') #visualise which eigenvalues are above and below 1\r\nplt.grid(b=True, which='major', color='#666666', linestyle='-')\r\n# Show the minor grid lines with very faint and almost transparent grey lines\r\nplt.minorticks_on()\r\nplt.grid(b=True, which='minor', color='#999999', linestyle='-.')\r\nplt.show()\r\n#visualisation of the 4 eigenvalues > 1.","1981d39d":"def _HornParallelAnalysis(data, K=10, printEigenvalues=False):\n# Create a random matrix to match the dataset\n    n,m = data.shape\n    # Set the factor analysis parameters\n    fa = FactorAnalyzer(n_factors=1, method='minres', rotation=None, use_smc=True)\n    # Create arrays to store the values\n    sumComponentEigens = np.empty(m)\n    sumFactorEigens = np.empty(m)\n    # Run the fit 'K' times over a random matrix\n    for runNum in range(0, K):\n        fa.fit(np.random.normal(size=(n, m)))\n        sumComponentEigens = sumComponentEigens + fa.get_eigenvalues()[0]\n        sumFactorEigens = sumFactorEigens + fa.get_eigenvalues()[1]\n    # Average over the number of runs\n    avgComponentEigens = sumComponentEigens \/ K\n    avgFactorEigens = sumFactorEigens \/ K\n\n    # Get the eigenvalues for the fit on supplied data\n    fa.fit(data)\n    dataEv = fa.get_eigenvalues()\n    # Set up a scree plot\n    plt.figure(figsize=(8, 6))\n\n\n    # Print results\n    if printEigenvalues:\n        print('Principal component eigenvalues for random matrix:\\n', avgComponentEigens)\n        print('Factor eigenvalues for random matrix:\\n', avgFactorEigens)\n        print('Principal component eigenvalues for data:\\n', dataEv[0])\n        print('Factor eigenvalues for data:\\n', dataEv[1])\n    # Find the suggested stopping points\n    suggestedFactors = sum((dataEv[1] - avgFactorEigens) > 0)\n    suggestedComponents = sum((dataEv[0] - avgComponentEigens) > 0)\n    print('Parallel analysis suggests that the number of factors = ', suggestedFactors , ' and the number of components = ', suggestedComponents)\n\n    # Plot the eigenvalues against the number of variables\n\n    # Line for eigenvalue 1\n    plt.plot([0, m+1], [1, 1], 'k--', alpha=0.3)\n    # For the random data - Components\n    plt.plot(range(1, m+1), avgComponentEigens, 'b', label='PC - random', alpha=0.4)\n    # For the Data - Components\n    plt.scatter(range(1, m+1), dataEv[0], c='b', marker='o')\n    plt.plot(range(1, m+1), dataEv[0], 'b', label='PC - data')\n    # For the random data - Factors\n    plt.plot(range(1, m+1), avgFactorEigens, 'g', label='FA - random', alpha=0.4)\n    # For the Data - Factors\n    plt.scatter(range(1, m+1), dataEv[1], c='g', marker='o')\n    plt.plot(range(1, m+1), dataEv[1], 'g', label='FA - data')\n    plt.title('Parallel Analysis Scree Plots', {'fontsize': 20})\n    plt.xlabel('Factors\/Components', {'fontsize': 15})\n    plt.xticks(ticks=range(1, m+1), labels=range(1, m+1))\n    plt.ylabel('Eigenvalue', {'fontsize': 15})\n    plt.legend()\n    plt.show();\n_HornParallelAnalysis(df_fas)","da2364de":"#Performing Factor Analysis:\r\nfa2 = FactorAnalyzer(count+1, rotation=\"equamax\")\r\nfa2.fit(df_fas)\r\nx_labels = ['Factor ' + str(i) for i in range(1,count+2)]\r\ny_labels = df_fa.columns.tolist()\r\nsns.set(font_scale=0.5)\r\nplt.figure(figsize=(20,20))\r\nplt.title('Loading Factors - ' + str(count+1) + ' v2')\r\nload = sns.heatmap(fa2.loadings_,cmap=\"coolwarm\", xticklabels = x_labels, yticklabels = y_labels, center=0, square=True, linewidths=.2,cbar_kws={\"shrink\": 0.5}, annot = True, annot_kws={\"fontsize\":1})","4eb0b118":"var_check = np.vstack((fa2.get_communalities(), fa2.get_uniquenesses(),np.array(fa2.get_communalities() + fa2.get_uniquenesses()))).tolist()\r\ny_labels = ['Communality','Uniqueness', 'Total Variance']\r\nx_labels = df_fa.columns.tolist()\r\nsns.set(font_scale=0.5)\r\nplt.title('Communality-Uniqueness of Variables')\r\nload = sns.heatmap(var_check,cmap=\"RdBu\", xticklabels = x_labels, yticklabels = y_labels, center=0, square=True, linewidths=.2,cbar_kws={\"shrink\": 0.5}, annot = True, annot_kws={\"fontsize\":1})","120febeb":"#Transform the data with our 5 new Factors:\r\ntransformed_df_fa = pd.DataFrame(fa2.transform(df_fas), columns = ['F1','F2','F3','F4','F5'])\r\ntransformed_df_fa.head()","057e8790":"def cronbach_alpha(df):\r\n    # 1. Transform the df into a correlation matrix\r\n    df_corr = df.corr()\r\n    \r\n    # 2.1 Calculate N\r\n    # The number of variables equals the number of columns in the df\r\n    N = df.shape[1]\r\n    \r\n    # 2.2 Calculate R\r\n    # For this, we'll loop through the columns and append every\r\n    # relevant correlation to an array calles \"r_s\". Then, we'll\r\n    # calculate the mean of \"r_s\"\r\n    rs = np.array([])\r\n    for i, col in enumerate(df_corr.columns):\r\n        sum_ = df_corr[col][i+1:].values\r\n        rs = np.append(sum_, rs)\r\n    mean_r = np.mean(rs)\r\n    \r\n   # 3. Use the formula to calculate Cronbach's Alpha \r\n    cronbach_alpha = (N * mean_r) \/ (1 + (N - 1) * mean_r)\r\n    return cronbach_alpha","b166b46c":"df_fa['blueDragonsReversed'] = df_fa['blueDragons'].multiply(-1)\r\ndf_fa.head()","e8fa5beb":"print(cronbach_alpha(df_fa[['blueKills','blueAssists','blueTotalGold']]))\r\nprint(cronbach_alpha(df_fa[['blueDeaths','redAssists','redTotalGold','redTotalExperience']]))\r\nprint(cronbach_alpha(df_fa[['blueDragonsReversed','redDragons']]))\r\nprint(cronbach_alpha(df_fa[['blueTotalExperience','blueTotalMinionsKilled']]))\r\nprint(cronbach_alpha(df_fa[['redTotalExperience','redTotalMinionsKilled']]))","9bbb8578":"from sklearn.model_selection import train_test_split\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.metrics import classification_report","05756c6a":"x_train,x_test,y_train,y_test=train_test_split(transformed_df_fa,y,test_size=0.2,random_state=1)\r\nfinal_LR=LogisticRegression()","11dc6433":"final_LR.fit(x_train,y_train)","6fa9ed5e":"result_LR=final_LR.predict(x_test)\r\nprint(accuracy_score(result_LR,y_test))","22da7be9":"from sklearn.metrics import confusion_matrix\r\nconfusion_matrix = confusion_matrix(y_test, result_LR)\r\nprint(confusion_matrix)","030a32c7":"import statsmodels.api as sm\r\nlogit_model=sm.Logit(y,transformed_df_fa)\r\nresult=logit_model.fit()\r\nprint(result.summary2())","c6235ad1":"#scaling our data similar to when doing FA:\r\ndf_clean_scaled = sc.fit_transform(df_clean)","fd64b429":"#converting it back into a panda:\r\ndf_clean_scaled = pd.DataFrame(df_clean_scaled, columns = list(df_clean.columns))\r\ndf_clean_scaled.head()","fdc7ae5a":"#computing the eigenvalues & eigenvectors and seeing how many will fall between the 70-90% variance:\r\neig_vals, eig_vecs = np.linalg.eig(df_clean_scaled.corr())\r\ncount_pca_floor = float('inf')\r\ncount_pca_ceiling = 0\r\ntotal_pca = sum(eig_vals)\r\ntotal_pca_running = 0\r\ni = 0\r\neig_vals = np.sort(eig_vals)[::-1]\r\nprint(eig_vals)\r\nwhile total_pca_running <= 0.90:\r\n    if total_pca_running >= 0.70 and count_pca_floor == float('inf'):\r\n        count_pca_floor = min(count_pca_floor,i)\r\n    if total_pca_running + eig_vals[i]\/total_pca > 0.90:\r\n        break\r\n    total_pca_running += eig_vals[i]\/total_pca\r\n    count_pca_ceiling += 1\r\n    i += 1\r\n#the set calculated tells us the range of components we should keep to maximise variance but also minimise our dimensions\r\n(count_pca_floor,count_pca_ceiling)","2daab7fa":"from sklearn.decomposition import PCA\r\npca = PCA()\r\npca.fit(df_clean_scaled)\r\npca_data = pca.transform(df_clean_scaled)\r\npercent_var = np.round(pca.explained_variance_ratio_.cumsum()*100, decimals=1)\r\nlabels = ['PC' + str(x) for x in range(1, len(percent_var)+1)]\r\nplt.figure(figsize=(20,20))\r\n#cumulative variance explained:\r\nplt.bar(x=range(1, len(percent_var)+1), height = percent_var, tick_label = labels, color='midnightblue', edgecolor='aqua', label = 'cumulative variance explained')\r\nsns.lineplot(x=range(1, len(percent_var)+1), y = percent_var)\r\n#benchmark of 70-90% variance explained:\r\nplt.axhline(70.0, color = 'orange', linestyle=\"--\")\r\nplt.axhline(90.0, color = 'orange', linestyle=\"--\")\r\nplt.ylabel('Percentage of Explained Variance')\r\nplt.xlabel('Principal Component')\r\nplt.title('Cumulative Sum of Variance')\r\nplt.show()","7b579794":"aa = pd.DataFrame({'feature':df_clean_scaled.columns, 'eigenvalue': abs(pca.components_[0])})\r\naa.sort_values(by='eigenvalue', ascending=False).head(10)","68ff0c97":"aa = pd.DataFrame({'feature':df_clean_scaled.columns, 'eigenvalue': abs(pca.components_[1])})\r\naa.sort_values(by='eigenvalue', ascending=False).head(10)","51478472":"aa = pd.DataFrame({'feature':df_clean_scaled.columns, 'eigenvalue': abs(pca.components_[2])})\r\naa.sort_values(by='eigenvalue', ascending=False).head(10)","a4c668a1":"#apply PCA to the number of above components:\r\npca9 = PCA(n_components=9)\r\npca9_fit = pca9.fit_transform(df_clean_scaled)\r\ndf_pca = pd.DataFrame(data = pca9_fit, columns = ['PCA' + str(i) for i in range(1,10)])\r\ndf_pca.head()","bf28e69d":"px_train,px_test,py_train,py_test=train_test_split(pca9_fit,y,test_size=0.2,random_state=1)\r\nPCA_LR=LogisticRegression() ","3cdabde3":"PCA_LR.fit(px_train,py_train)","266f57a7":"result_PCALR=PCA_LR.predict(px_test)\r\nprint(accuracy_score(result_PCALR,py_test))","b201ae57":"from sklearn.metrics import confusion_matrix\r\nconfusion_matrix_pca = confusion_matrix(py_test, result_PCALR)\r\nprint(confusion_matrix_pca)","f05647a7":"import statsmodels.api as sm\r\nlogit_model=sm.Logit(y,pca9_fit)\r\nresult2=logit_model.fit()\r\nprint(result2.summary2())","430a6232":"#Attempting first step of B2 method to delete the highest-loading variable of the least important PC:\r\naa = pd.DataFrame({'feature':df_clean_scaled.columns, 'eigenvalue': abs(pca.components_[22])})\r\naa.sort_values(by='eigenvalue', ascending=False).head(10)","ca0f5f0e":"cols = []\r\nfor i in range(9):\r\n    aa = pd.DataFrame({'feature':df_clean_scaled.columns, 'eigenvalue': abs(pca.components_[i])})\r\n    x = aa.values.tolist()\r\n    x.sort(key = lambda x: x[1], reverse = True)\r\n    cols.append(x[0][0])\r\n    print(x[0][0] + \" : \" + str(x[0][1]))\r\ndf_b4 = df_clean_scaled[cols]","b4dab514":"bx_train,bx_test,by_train,by_test=train_test_split(df_b4,y,test_size=0.2,random_state=1)\r\nB4_LR=LogisticRegression()","ac67a121":"B4_LR.fit(bx_train,by_train)","ee09b323":"result_B4LR=B4_LR.predict(bx_test)\r\nprint(accuracy_score(result_B4LR,by_test))","881b7c93":"from sklearn.metrics import confusion_matrix\r\nconfusion_matrix_b4 = confusion_matrix(by_test, result_B4LR)\r\nprint(confusion_matrix_b4)","3c3c28d0":"import statsmodels.api as sm\r\nlogit_model=sm.Logit(y,df_b4)\r\nresult3=logit_model.fit()\r\nprint(result3.summary2())","2f33b5c6":"<b>Performing Logistic Regression after PCA:<\/b>","3d8cf5f9":"<b>Factor Analysis consists of:<\/b>\n\n    1. Kaiser Criterion\/Scree Plot of eigenvalues to determine number of Factors\n\n    2. Factor Loadings of variables by Factor Analysis Rotation\n\n    3. Communality-Uniqueness Analysis","e3039c0c":"In an official and thorough Factor Analysis, we should perform a few tests to validate the justification in performing FA. We will perform two tests on our dataset (Hadi, Abdullah & Sentosa, 2016):\n\n1. <b>Barlett's Test of Sphericity<\/b> - Checks if our correlation matrix is statistically similar to the Identity matrix. In other words, do our variables essentially have 0 relations with each other or not?\n\n        We want to see a p-value < 0.05 to justify the suitability of FA.\n\n2. <b>Kaiser-Meyer-Olkin (KMO) Test<\/b> - Checks if the inter-correlations between our variables are strong enough. Similar to Bartlett's test in checking if our variable correlations are sufficient.\n\n        We want to see a KMO measure > 0.6 to justify the suitability of FA.","a7ac6b79":"We can repeat the above steps to obtain the rest of the interpretations of our Components:\r\n\r\nPC1: Gold\/Exp Diff\r\n\r\nPC2: K\/D\/A\r\n\r\nPC3: Dragons Killed\r\n\r\nPC4: Experience\r\n\r\nPC5: Heralds Killed\r\n\r\nPC6: Wards destroyed\r\n\r\nPC7: Towers Destroyed\r\n\r\nPC8: Wards Placed\r\n\r\nPC9: Jungle Minions killed\r\n\r\nPC10: (Eventually, the Compoments become less and less interpretable...)\r\n\r\nWith at least 70% of cumulative variance explained, the variance being < 1 after the 9th PC & the uninterpretability after the 9th PC and the 10th PC onwards having eigenvalues < 1, there is evidence to choose 9 Principal Components.","6a4463ec":"When communalities are all high, it indicates that the extracted components represent the Factors well. In our example, most communalities are very high, with the lowest communality in blueDragons = 0.46.","07137b32":"Looks like our cronbach alphas for our 5 factors is looking good! Therefore our factors are deemed acceptable.","934ee25b":"<b>Component 1:<\/b>\r\n\r\nThe output below suggests Component 1 is about Gold and Experience Difference\r\n","5ad67b44":"Before we decide how many PCA components, we need to know the criteria for how many components we should maintain. The cumulative variance to preserve our data seems to vary, but we'll choose the amount of components where 70-90% of the total variance is explained (Siswadi, Muslim & Bakhtiar, 2016). We'll explain in the discussion at the end of this of the alternative methods that determine how many PCA components to choose.","59784ca6":"<u><b>Conclusion:<\/b><\/u>\r\n\r\nGold accumulation through early kills\/solid last-hitting as well as securing early dragon kills maximises your chances in winning your League of Legends games.\r\n\r\nThe first 10 minutes of almost 10000 diamond-ranked games were analyzed through Factor Analysis and Principal Component Analysis. While both have their unique pros & cons - Factor Analysis proved to be more effective at analysing our dataset for latent variables while still achieving dimensionality reduction.","5c9460e6":"<u><b> FA vs PCA Discussion:<\/b><\/u>\r\n\r\nPCA compared to FA seems to be much more rigorously studied - with other researchers disputing the variance cutoff of our Kaiser Criterion (while we based our selections on \u03bb<sub>x<\/sub> < 1, other researchers suggest cutoffs of 0.6-0.7) as well as introducing other criteria such as the broken-stick distribution (King & Jackson, 1999). I did perform a comparison of our PCA's explained variance ratio vs the broken-stick distribution. However when comparing the two, only the first two variances\/eigenvalues were greater than the first two values, suggesting we only retain two variables. While this may be of discussion, we ignored the results in this notebook as two PCs could only explain a maximum of around 33%. Furthermore, Bartkowiak with an empirical comparison shows us that most of the rules lead to similar decisions on how many components\/variables to keep - except the broken stick rule (Jolliffe, 2002 p. 132).\r\n\r\nBecause FA considers communality (while PCA doesn't), it makes it easier to distinguish variables which we should inherently remove after one iteration. We can see that using logistic regression after the B4 method results in some variables with higher p-values (blueWardsDestroyed, blueWardsPlaced, redTotalJungleMinionsKilled) - variables FA has already analyzed and has deemed unfit to use. While there are ways around this in PCA by analyzing multiple criteria and subsets, it seems like FA has been overall more effective in our notebook.","f3b23892":"<u>Communality-Uniqueness Analysis:<\/u>\n\nUniqueness is the variance that is 'unique' to the variable (not shared), while Communalities is the variance that is common to the variable (shared). Uniqueness + Communalities = 1.\n\nLow Uniqueness\/High Communality = Good. High Uniqueness\/Low Communality = bad and should be under consideration for elimination. (Princeton University, 2020)","c497a46d":"The first necessary step in PCA is to scale our data. If this isn't done first, then we'll have trouble when dealing with variances.","9b1f3642":"<b>Component 2:<\/b>\r\n\r\nThe output below suggests Component 2 is about Kills\/Deaths\/Assists (K\/D\/A)","b30b8a3c":"As mentioned before, we shouldn't rely only on the Kaiser Criterion. There are a lot of methods in retaining factors but there are three of them to utilise (Hayton, Allen & Scarpello, 2004):\n\n    1. Kaiser criterion - if a factor's eigenvalues is above 1.0, we should retain that factor.\n\n    2. Scree plot - when is there a substantial decline in the magnitude of the eigenvalues?\n\n    3. Parallel analysis - given the eigenvalue of the sample and eigenvalue of synthetic data with exact dimensionality of sample, count(the no. of eigenvalues of sample > no. of eigenvalues from synthetic) = retained number of factors.\n\nUsing the Kaiser Criterion alone is not good practice as overrelying on it can result in overestimating\/underestimating the number of factors. Normally selecting 4 factors would suffice, but the Scree Plot suggests the 5th factor being close to 1 and not being the 'drop-off' point (significant drop-off at 6th factor, where 6th factor onwards plateaus) means there's an argument to actually keep 5 factors. (Li, Yang & Liu, 2018).","5212f516":"<u>FA Rotation selection:<\/u>\n\nGenerally, varimax rotation is chosen to maximise sum(variance of squared loadings) and ensure factors created are orthogonal (uncorrelated). However, imposing orthogonality on highly correlated factors is more 'unnatural'. Therefore, we will do an oblique rotation. The drawback of doing oblimin rotation is diminished interpretability of factors due to higher eigenvalues (IBM, 2014).).","6b3ad50f":"The cumulative variance explained shows there is evidence to keep at least 9 Principal Components. Becuase of this high number, we'll be unable to utilise one of PCA's advantages: its visualisations (best used for components under 4 since the human eye sees in 3D).","ad751072":"<b>Performing Logistic Regression after Factor Analysis:<\/b>","dca84e3e":"    We're going to start looking at the eigenvalues in the first few compoments and start to identify the features that correspond with the Components:","a26c9fb0":"**Parallel Analysis:**\n\nAlthough multiple sources put Parallel Analysis as the most promising method over Kaiser criterion\/Scree plot for the number of factors to retain, it could be problematic over 500 subjects (Myzziah, 2021) as well as underestimate the number of factors to retain when the first eigenvalue is large (Beauducel, 2001).\n\nParallel Analysis is still shown below and still suggests maintaining 4 factors, however we'll still continue keeping the 5 factors.","f90208c8":"An eigenvalue > 1 means that the factor explains more variance than a unique variable. Total eigenvalues > 1 = 9.\n\nThe total of eigenvalues = 16.43, with the 4 eigenvalues > 1 accounting for 71.44% of the total variance.","73710469":"The very low Communality\/high Uniqueness of variables strongly suggests the removal and the analysis be rerun (cut-off being 0.5). These variables for consideration include:\n\n    1. Wards placed\/destroyed for blue and red side (0.98, 0.79, 0.97, 0.88 Uniqueness)\n\n    2. Who got first blood (0.84 Uniqueness)\n\n    3. How many Heralds (0.66, 0.73 Uniqueness)\n\n    4. Towers destroyed (0.73, 0.7 Uniqueness)\n\n    5. Jungle minions destroyed (0.65, 0.65 Uniqueness)\n\nThere is enough justification to remove the mentioned variables:\n\n    1. Low correlations in correlation matrix and with blueWins.\n\n    2. None of the 9 proposed factors have high enough loadings for these mentioned variables above.\n\n    3. Very low Communality","59c4ab3d":"**Performing Factor Analysis and choosing rotation method:**\n\nBecause we have variables that have high loadings in factors (blueKills, blueDeaths, blueAssists, blueTotalGold, redAssists, redDragons, RedTotalGold) as well as variables that bleed into other factors (blueTotalExperience, redTotalExperience), we use the 'equamax' rotation method that minimizes both (IBM, 2014).","b3045288":"Returning the confusion matrix (by row) of True Positives, False Negatives, False Positives & True Negatives:","8fa44554":"<u><b>Principal Component Analysis<\/b><\/u>\n\nDefintion: <i>PCA can be defined as the orthogonal projection of the data to a lower dimensional linear space (principal subspace), while minimizing the mean squared distance between datapoints and the orthogonal projection while maximising the spread throughout the orthogonal projection (variance).<\/i> Extremely great writeup explaining PCA here: https:\/\/stats.stackexchange.com\/questions\/2691\/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n\nHow is Principal Component Analysis (PCA) different from FA?\n\n1. Both methods are looking to make an approximation of a given covariance matrix. The fundamental mathematical difference between the two methods is this (Bishop, 2014):\n\n   <b>PCA: C \u2248 WW<sup>\u22a4<\/sup><\/b>\n\n   <b>FA: C \u2248 WW<sup>\u22a4<\/sup> + \u03a8<\/b>\n\n    Where C represents the covariance matrix of our dataset, W being a matrix of our data after deciding how many components to keep (like how we retained <i>'k'<\/i> factors in    FA) and \u03a8 the diagonal matrix. This diagonal matrix is the representation of uniqueness (total variance - communalities).  \n\n2. In PCA, because of the absence of \u03a8, we're not looking to determine a causal statement about the relationships. In FA, we're seek latent causes that will eventually be interpreted as the reason for our observed, measured variables. This is why in FA, we try to interpret and categorize the Factors. In PCA, not so much. We're simply finding a linear combination of our variables that create the 'Component', so there's no guarantee the Components are interpretable.\n\n3. In FA, there are a lot of possible algorithms to choose from (depending on different assumptions). This can result in some algorithms resulting in a dead end (Heywood Case) and is a more of an art form than PCA. In PCA, just perform eigen decomposition and you're done.\n\nIt is worth noting that as the dimensionality (n) of our dataset increases, the differences between PCA and FA decreases. However for a small <i>'n'<\/i>, the difference is significant (Carine, 2014).\n","7099171d":"<b>Performing Logistic Regression after B4 Method:<\/b>","a50bc45e":"Summarizing regression results:","4b454a7f":"<b><u>References:<\/u><\/b>\n\nBeauducel, A. (2001) <i>Problems with parallel analysis in data sets with oblique simple structure<\/i>. Institute for Science Education\n\nBishop, C.M. (2006) <i>Pattern Recognition and Machine Learning<\/i>. pp. 584\n\nCadima, J.F.C.L & Jolliffe, I.T (1995) <i>Variable selection and the interpretation of principal subspaces<\/i>. - Journal of Agricultural Biological and Environmental Statistics\n\nCarine (2014) <i>Is there any good reason to use PCA instead of EFA? Also, can PCA be a substitute for factor analysis?<\/i>  Stack Exchange - Stats\n\nCentre for Academic Success (2017) <i>Advice on Exploratory Factor Analysis<\/i>. Birmingham City University pp.2\n\nFerrari, S. (2013) <i>From Generative to Conventional Play: MOBA and League of Legends<\/i>. Georgia Institute of Technology\n\nHadi, N.U., Abdullah, N. & Sentosa, I. (2016) <i>An Easy Approach to Exploratory Factor Analysis: Marketing Perspective<\/i>. Rome-Italy: Journal of Education and Social Research, MSCER Publishing\n\nHayton, J.C., Allen D.G. & Scarpello, V. (2004) <i>Factor Retention Decisions in Exploratory Factor Analysis: A Tutorial on Parallel Analysis<\/i>. Sage Publications\n\nHilsdorf, M. (2020) <i>Cronbach\u2019s Alpha: Theory and Application in Python<\/i>. Towards data science\n\nIBM (2014) <i>Factor Analysis Rotation<\/i>. SPSS Statistics - 23.0.0\n\nJoliffe, I.T. (2002) <i>Principal Component Analysis, Second Edition<\/i>. Springer\n\nKing, J.R. & Jackson, D.A. (1999) <i>Variable Selection in Large Environmental Data Sets using Principal Components Analysis<\/i>. Environmetrics\n\nLi, J., Yang, M. & Liu, B. (2018) <i>Factor Analysis<\/i>. jbhender - github\n\nMyzziah (2021) <i>Horn's Parallel Analysis in Python: Am I doing it correctly?<\/i> - reddit\n\nPrinceton University (2020) <i>Manuals - mvfactor<\/i>. stata.com\n\nSilva, A.P.D (2000) <i>Discarding Variables in Principal Component Analysis: Algorithms for All-Subsets Comparisons<\/i>. The Catholic University of Portugal\n\nSiswadi, Muslim, A. & Bakhtiar, T. (2012) <i>Variable Selection Using Principal Component and Procrustes Analyses and its Application in Educational Data<\/i>. Journal of Asian Scientific Research\n\nType2 (2013) <i>Should one remove highly correlated variables before doing PCA?<\/i>  Stack Exchange - Stats","2a1b713f":"There is one last assessment to perform called Cronbach's Alpha. It will measure how interanlly consistent our factors are in relation to how they are as a group. Our Factors will be acceptable when our Cronbach's Alpha is > 0.7 (Hilsdorf, 2020).","74648ca9":"Ideally, we would like to see that each variable is highly correlated with only 1 factor. We can see from above that the wards placed\/destroyed and first blood do not have a high enough factor loading for any of the 9 Factors (cut-off being 0.5). Furthermore, Factors 8 & 9 do not have a high enough loading for any variables.","44d75759":"    In this KMO test, the measure is 0.74. This value is considered good, which justifies suitability of factor analysis for the dataset.","1c50d81f":"Observing the correlation matrix with reduced variables:","f85fcd4c":"<b>We've now finished our 'psuedo' Factor Analysis. From the above, we will look to drop the mentioned variables, and we will perform a proper Factor Analysis after this:<\/b>","f7967dc6":"    In this Bartlett's test, the p-value is 0. This means the test was statistically significant and the observed correlation matrix is not an identity matrix.","539fecb7":"We could naively conclude (simply from the correlation matrix analysis) that the more kills, gold and experience Blue team has, the more likely they are going to win. Similarly, the more kills, gold and experience Red team has, the more likely Blue Team will lose. But we want more than just that!","839ad315":"Firstly, any variables that are going in an opposite direction need to be reversed for the test:","a512ccc5":"We're going to perform a <i>'psuedo'<\/i> Factor Analysis. This will not be the Final Factor Analysis because of the very high colinearity of variables. We will decide on the appropriate method and rotation, run the FA, remove any items with low communalities and re-run (Centre of Academic Success, 2017).","2b19b293":"<u><b>PCA Selection methods<\/b><\/u>\r\n\r\nThe B2 procedure begins by doing PCA over our <i> n &#215; p<\/i> data matrix. If we choose to retain <i>q<\/i> Principal Components, then associate one variable with each of the last <i>p - q<\/i> variables. The <i>p - q<\/i> variables are then removed (Siswadi, Muslim & Bekhtiar, 2012). This can be done as a whole <i>p - q<\/i> block or one by one and performing multiple PCAs. Jolliffe explores the distinctive ways of deleting the variables and determines deleting en bloc consistently failed to select an appropriate subset for some simple correlation structures (Joliffe, 2002, p. 138).\r\n\r\nThe B4 procedure is associating one variable with each of the first <i>m<\/i> PCs, namely the variable not already chosen with the highest loading. This approach is complimentary to the B2 method, but also when there exists groups of highly correlated variables (which is almost all of our PCs) it is designed to select just one variable from each group.\r\n\r\n","1ffdc53c":"Finally, return the cronbach alphas:","7bb37547":"<b>Highest eigenvalue of first 9 Components:<\/b>","bb877d1b":"Attempting to predict the variables that determine if blue side wins: https:\/\/www.kaggle.com\/bobbyscience\/league-of-legends-diamond-ranked-games-10-min.\n\nThe goal of analysing this dataset is to inform high-elo League of Legends players the key metrics to focus on in the first 10 minutes ie. the 'Laning Phase' (Ferrari, 2013) in order to maximise your chances of winning the game overall.\n\nThe goal is also to use two statistical methods on a single dataset then potentially use it to train a logistic regression model: \n\n    1. Factor Analysis\n\n    2. Principal Component Analysis\n\nBoth of these methods (in my opinion) aren't well documented, so hopefully this will be relevant for some who were as lost as I was! Throughout this notebook, I've placed some references and consolidated pieces of their methodologies here.\n\n    Please note that this data captures the first 10 minutes of a game. In a 30-40 minute game, a lot of 'comebacks' can happen, so our final accuracy will not be 'overly' magnificent. \n    \n    From other individuals who have analysed this dataset, a 70-75% final accuracy seems to be the concensus. \n    \n    Therefore, having a strong advantage in the 1st 10 minutes of the game would mean your team had a 70-75% chance of winning the game overall.","753bb061":"Performing the KMO Test - a measure of sampling adequacy (of the strength of inter-correlations among variables) which ranges from 0 to 1:","4da5fbe8":"What does the above mean?\n\nOur naive Factor Analysis Approach involves us making a rough estimate on the number of Factors required in FA. The rough criteria is that the number of eigenvalues > 1 is the number of Factors we keep (Kaiser Criterion). However this criteria is <b>highly<\/b> disputed and shouldn't be gospel (Hayton, Allen & Scarpello, 2004).\n\nFor now, we'll perform FA on 9 factors.","1f830b30":"Performing Barlett's Test of Sphericity:","0a7444d5":"Observing the correlation matrix: (Note that 23 dimensions is still extremely high, and we are looking to reduce this by a substantial amount while minimising the loss of information by removing variables)","4c809973":"Cleaning up our dataframe:","1fa79b67":"Unfortunately, what we see is that the last eigenvalue has variables which have high loadings in our first (most variance explained) & last (least variance explained) PC, making the B2 method not very viable. \r\n\r\nTherefore, we'll first start by using the B4 method to assign variables to the first 9 PCs, then assign the remaining variables (for rejection) that did not appear in the B4 method:","471d1c84":"Quickly check the correlation between our chosen variables and 'blueWins':","11be6923":"<u><b>PCA retrospect and reconsiderations<\/b><\/u>\r\n\r\nWe've found a problem with our logistic regression - unsatisfactory p-values\/z-scores in our Principal Components. There are clear signs of overemphasis on some contributions and underemphasis in others. The following attempts to explain why:\r\n\r\nPCA is trying to achieve two things at the same time: Minimising the mean-squared error (MSE) of data points while also maximising the spread of variance. In other words - our resultant components should not only be an accurate interpretation\/reconstruction of our variables (minimising MSE) but also construct new characteristics that strongly differ across variables (maximise variance). With that being said, let's pose two hypotheticals: What if our original dataset's variables had low colinearity? What if our original dataset's variables had high colinearity? (which our dataset has both).\r\n\r\nIf a dataset's features had little colinearity, PCA would be of almost no use to reduce dimensionality. If our dataset had 100 very unique and uncorrelated variables, then it would take 100 Principal Components to explain those 100 very unique and uncorrelated variables (Phan, 2016 pp. 6). This is why it's 'standard' practice to remove variables with low correlations, although this practice may be misleading and other approaches such as finding small subsets of original variables that approximate the various PCs in some optimal way (Silva, 2000). If a dataset's features had high colinearity, then the effect of adding more and more nearly correlated variables increases the contribution of the common underlying factor in PCA, causing PCA to overemphasise the contribution (Type2, 2013).\r\n\r\nWhen analyzing how other individuals used PCA in this particular dataset in Kaggle, it seems like very little is addressed on these matters. Therefore, it may be preferable to search for small subsets of the original variables that approximate the relevant PCs in some optimal way (Silva, 2000). There are proposed algorithms and new criteria of what subsets of original variables will approximate the relevant PCs found in both Silva's <i>Discarding Variables in Principal Component Analysis: Algorithms for All-Subsets Comparisons<\/i> and Cadima & Jolliffe's <i>Variable selection and the interpretation of principal subspaces<\/i>.\r\n\r\nHowever due to the computational intensity of either working out the criteria of each possible subset (2<sup>N<\/sup>-1 of non-empty subset possibilities), the ambiguity of choosing the specific subsets or the complexity of working out algorithms such as Yanai's GCD\/McGabe's 4 Criterion from above, we won't touch this realm of PCA (at least in this notebook, maybe another one in the future?). Instead, we're going to narrow in to a much simpler method suggested by one of the same authors Jolliffe: the B2\/B4 method (Joliffe, 2002, p. 138).\r\n\r\n","2348eb1a":"<b>Component 3:<\/b>\r\n\r\nThe output below suggests Component 3 is about Elite Monster Kills. Interestingly, the eigenvalue for Herald kills is relatively small compared to Dragon kills, so we could conclude Component 3 is actually about Dragons taken instead.","9b9f9054":"<b>Factor Analysis part 2:<\/b>","b784a502":"<u><b>Factor Analysis<\/b><\/u>\r\n\r\nThe first method we will try out is Exploratory Factor Analysis (FA). FA will determine a 'cluster' of factors that are significant to the dataset. FA can be difficult compared to Principal Component Analysis (PCA) due to the ambiguity and 'art' of FA.\r\n\r\nGenerally as a rule of thumb, we run FA if we assume or wish to test a theoretical model of latent factors causing observed variables.\r\n\r\n    What I mean by latent factors is similar to causation - winning your lane is a latent cause of more gold, exp, CS, kills and assists for example.","412437dc":"From the above information, we can interpret the 5 Factors as so:\n\n    1. How many kills has Blue side had on Red?\n\n    2. How many kills has Red side had on Blue?\n\n    3. How many dragons has Blue side killed compared to Red side?\n\n    4. How many last hits does Blue side have?\n\n    5. How many last hits does Red side have?","30e85cae":"Splitting our training and test data:","bf0eba7b":"First, load packages.","4c144710":"Some of our p-values\/z-scores are still unsatisfactory but much better than our initial PCA regression model.","4464e896":"Manually performing feature scaling before running the FA by ensuring our columns have an average mean of 0 and standard deviation of 1:","d8a67812":"That means with the first 10 minutes of League data, we're able to predict the final outcome with a 74.04% accuracy.","0fd9df81":"Assessing the Accuracy of our model:","1cb415f3":"An eigenvalue > 1 means that the factor explains more variance than a unique variable. Total eigenvalues > 1 = 4.\n\nThe total of eigenvalues = 9.76, with the 4 eigenvalues > 1 accounting for 81.36% of the total variance.","97b83a2e":"Next, load dataset."}}