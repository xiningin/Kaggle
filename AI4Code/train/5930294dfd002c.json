{"cell_type":{"5277165b":"code","22bf95d7":"code","095f80ef":"code","e0ea48d5":"code","1f0462f3":"code","03f70da4":"code","958b4388":"code","7767e90e":"code","05fa51c0":"code","8d43b419":"code","b74ee409":"markdown","f343c90a":"markdown","c0d442bc":"markdown","a7f091ed":"markdown","d94915fa":"markdown","15ca6c67":"markdown"},"source":{"5277165b":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader\nfrom tqdm import tqdm\nfrom torch.optim import SGD\nfrom torch.optim.lr_scheduler import StepLR\nfrom torchvision import transforms","22bf95d7":"!ls -lh \/kaggle\/input\/fashionmnist","095f80ef":"data_dir = \"\/kaggle\/input\/fashionmnist\"\ntraindata = np.loadtxt(os.path.join(data_dir,\"fashion-mnist_train.csv\"),delimiter=\",\",skiprows=1,dtype=np.uint8)\ntestdata = np.loadtxt(os.path.join(data_dir,\"fashion-mnist_test.csv\"),delimiter=\",\",skiprows=1,dtype=np.uint8)\ntrain_images,train_labels = traindata[:,1:].reshape(-1,1,28,28),torch.LongTensor(traindata[:,0])\ntest_images,test_labels = testdata[:,1:].reshape(-1,1,28,28),torch.LongTensor(testdata[:,0])","e0ea48d5":"plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.imshow(train_images[i][0],cmap=\"gray\")\nplt.tight_layout()","1f0462f3":"# # normalize based on training set\n# train_mean,train_std = train_images.mean(),train_images.std()\n\ndo_augmentation = True\nif do_augmentation:\n    # define transformation for dataloader\n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.RandomHorizontalFlip(p=0.5), # symmetry\n        transforms.RandomAffine(degrees=5,translate=(0.05,0.05)), # angle sharpness & xy position\n        transforms.RandomResizedCrop(size=(32,32),scale=(9\/10,11\/10),ratio=(9\/10,11\/10)), # size\n        transforms.ToTensor()\n    ])\nelse:\n    transform = transforms.ToTensor()\n\nclass FashionMNISTDataset(Dataset):\n    def __init__(self,images,labels, transform=None):\n        self.images = images.transpose(0,2,3,1) # channel last for numpy\n        self.labels = labels\n        self.transform = transform\n    def __getitem__(self,idx):\n        if self.transform is not None:\n            return self.transform(self.images[idx]),self.labels[idx]\n        else:\n            return self.images[idx],self.labels[idx]\n    def __len__(self):\n        return len(self.labels)\n\ntrainset = FashionMNISTDataset(train_images,train_labels,transform=transform)\ntestset = FashionMNISTDataset(test_images,test_labels,transform=transform)\ntrainloader = DataLoader(trainset,batch_size=512,shuffle=True,pin_memory=True)\ntestloader = DataLoader(testset,batch_size=512,pin_memory=True)","03f70da4":"plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.imshow(trainset[i][0][0],cmap=\"gray\")\nplt.tight_layout()","958b4388":"class BasicBlock(nn.Module):\n    \n    def __init__(self,inchan,expansion=False):\n        super(BasicBlock,self).__init__()\n        self.inchan = inchan\n        self.expansion = expansion\n        if self.expansion:\n            stride = 2\n            self.outchan = self.inchan*2\n            self.project = nn.Sequential(\n                nn.Conv2d(self.inchan,self.outchan,1,2),\n                nn.BatchNorm2d(self.outchan)\n            )\n        else:\n            stride = 1\n            self.outchan = self.inchan\n            self.project = nn.Identity()\n            \n        self.conv1 = nn.Conv2d(self.inchan, self.outchan,3,stride,1,bias=False)\n        self.bn1 = nn.BatchNorm2d(self.outchan)\n        self.conv2 = nn.Conv2d(self.outchan, self.outchan,3,1,1,bias=False)\n        self.bn2 = nn.BatchNorm2d(self.outchan)\n        \n        # following resnetv1.5 by nvidia\n        nn.init.ones_(self.bn2.weight)\n        nn.init.zeros_(self.bn2.bias)\n        \n    def forward(self,x):\n        proj = self.project(x)\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.conv2(out)\n        out = F.relu(self.bn2(out)+proj)\n        return out\n        \nclass ResNet20(nn.Module):\n    def __init__(self):\n        super(ResNet20,self).__init__()\n        self.conv0 = nn.Conv2d(1,16,3,1,1,bias=False)\n        self.bn0 = nn.BatchNorm2d(16)\n        self.layer1 = self._make_layer(16,2)\n        self.layer2 = self._make_layer(16,2,expansion=True)\n        self.layer3 = self._make_layer(32,2,expansion=True)\n        self.fc = nn.Linear(64,10)\n        \n    def _make_layer(self,inchan,num_rep=0, expansion=False):\n        blocks = [BasicBlock(inchan,expansion),]\n        outchan = inchan*2 if expansion else inchan\n        for _ in range(num_rep):\n            blocks.append(BasicBlock(outchan))\n        return nn.Sequential(*blocks)\n        \n    def forward(self,x):\n        out = F.relu(self.bn0(self.conv0(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.adaptive_avg_pool2d(out,(1,1)) # global average pooling\n        out = self.fc(out.flatten(start_dim=1))\n        return out\n    \nclass ResNet56(nn.Module):\n    def __init__(self):\n        super(ResNet56,self).__init__()\n        self.conv0 = nn.Conv2d(1,16,3,1,1)\n        self.bn0 = nn.BatchNorm2d(16)\n        self.layer1 = self._make_layer(16,8)\n        self.layer2 = self._make_layer(16,8,expansion=True)\n        self.layer3 = self._make_layer(32,8,expansion=True)\n        self.fc = nn.Linear(64,10)\n        \n    def _make_layer(self,inchan,num_rep=0, expansion=False):\n        blocks = [BasicBlock(inchan,expansion),]\n        outchan = inchan*2 if expansion else inchan\n        for _ in range(num_rep):\n            blocks.append(BasicBlock(outchan))\n        return nn.Sequential(*blocks)\n        \n    def forward(self,x):\n        out = F.relu(self.bn0(self.conv0(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.adaptive_avg_pool2d(out,(1,1)) # global average pooling\n        out = self.fc(out.flatten(start_dim=1))\n        return out","7767e90e":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef train(model,train_set,opt,loss_fn,epochs,scheduler=None,eval_set=None,device=device):\n    for ep in range(epochs):\n        running_loss = 0.0\n        running_correct = 0\n        running_total = 0\n        model.train() # switch back to train mode\n        with tqdm(train_set,desc=f\"{ep+1}\/{epochs} epochs:\") as t:\n            it = 0\n            for images,labels in t:\n                it += 1\n                opt.zero_grad()\n                out = model(images.to(device))\n                loss = loss_fn(out,labels.to(device))\n                loss.backward()\n                opt.step()\n                \n                pred = out.max(dim=1)[1]\n                running_loss += loss.item()*len(labels)\n                running_correct += (pred==labels.to(device)).sum().item()\n                running_total += len(labels)\n                train_metrics = {\"train_loss\":running_loss\/running_total,\"train_acc\":running_correct\/running_total}\n                if it < len(train_set) or eval_set is None:\n                    t.set_postfix(train_metrics)\n                else:\n                    eval_metrics = evaluate(model,loss_fn,eval_set,device)\n                    t.set_postfix({**train_metrics,**eval_metrics})\n        if scheduler is not None:\n            scheduler.step()\n\ndef evaluate(model,loss_fn,eval_set,device):\n    model.eval() # swithch to evaluation mode\n    with torch.no_grad():\n        running_loss = 0.0\n        running_correct = 0\n        running_total = 0\n        for images,labels in eval_set:\n            out = model(images.to(device))\n            loss = loss_fn(out,labels.to(device))\n            pred = out.max(dim=1)[1]            \n            running_loss += loss.item()*len(labels)\n            running_correct += (pred==labels.to(device)).sum().item()\n            running_total += len(labels)\n    return {\"eval_loss\":running_loss\/running_total,\"eval_acc\":running_correct\/running_total}","05fa51c0":"model = ResNet56()\nmodel.to(device)\n\nepochs = 50\nloss_fn = nn.CrossEntropyLoss()\nopt = SGD(model.parameters(),lr=0.1,momentum=0.9,weight_decay=1e-4)\nscheduler = StepLR(opt,step_size=30,gamma=0.1) \n\ntrain(model, trainloader, opt, loss_fn, epochs, scheduler, testloader)","8d43b419":"# count model parameters\ntotal_params = 0\nfor p in model.parameters():\n    total_params += np.prod(p.size())\nprint(\"Total params is {:6e}\".format(total_params))","b74ee409":"# Architecture","f343c90a":"![layer_composition](https:\/\/i.imgur.com\/Vj5Jwzh.png)","c0d442bc":"# Import modules","a7f091ed":"# Prepare data","d94915fa":"# Train","15ca6c67":"## Layer composition (n=9)\n\n*Figure source [He et al.](https:\/\/arxiv.org\/abs\/1512.03385)*"}}