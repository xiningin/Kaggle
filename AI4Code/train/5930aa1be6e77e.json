{"cell_type":{"89d9a803":"code","34bbab8c":"code","b3d8954a":"code","a5e7f358":"code","597adb71":"code","20208c92":"code","4d3719ce":"code","74b21976":"code","bd665b5a":"code","e3c62bd4":"code","d3ff6972":"code","98a167ad":"code","5bb02756":"code","1ce187c5":"markdown","61327623":"markdown","9f507830":"markdown","aea15552":"markdown","7d8d8165":"markdown","cbf9e935":"markdown"},"source":{"89d9a803":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer","34bbab8c":"comments_df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nvalid_df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\n\ncomments_df","b3d8954a":"prev_train_df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\nprev_train_df","a5e7f358":"prev_train_df[prev_train_df[\"toxic\"] > 0]","597adb71":"for col in [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"] :\n    print(col, prev_train_df[col].sum())","20208c92":"prev_train_df[\"severe_toxic\"] = prev_train_df[\"severe_toxic\"] * 2.5\nprev_train_df[\"threat\"] = prev_train_df[\"threat\"] *3\nprev_train_df[\"identity_hate\"] = prev_train_df[\"identity_hate\"] * 2\n\nprev_train_df[\"total_toxic\"] = prev_train_df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\",\n                                              \"identity_hate\"]].sum(axis = 1).astype(np.int64)","4d3719ce":"train_df = prev_train_df[[\"comment_text\", \"total_toxic\"]]\ntrain_df","74b21976":"prev_train_df.loc[prev_train_df[\"comment_text\"].str.contains(\"fuckyou\"), \"comment_text\"]","bd665b5a":"#tfvec = TfidfVectorizer(stop_words=\"english\")\ntfvec = TfidfVectorizer(analyzer = 'char_wb', ngram_range = (3,5))\ntfv = tfvec.fit_transform(train_df[\"comment_text\"])","e3c62bd4":"lgb_params = {\n    \"objective\": \"regression\",\n    \"n_estimators\" : 2000,\n    \"random_seed\" : 1234}\n\nmodel = lgbm.LGBMRegressor(**lgb_params)","d3ff6972":"model.fit(tfv, train_df[\"total_toxic\"])","98a167ad":"tfv_comments = tfvec.transform(comments_df[\"text\"])\npred = model.predict(tfv_comments)","5bb02756":"sub = pd.DataFrame()\nsub[\"comment_id\"] = comments_df[\"comment_id\"]\nsub[\"score\"] = pred\nsub.to_csv('submission.csv',index=False)\nsub","1ce187c5":"Try to see how the score will be improved by changing ngram.  \nFirst, tried to change ngram=(1, 3) with analyzer=default, meaning \"word\". Then the score became slightly worse from 0.763 to 0.760.  \nNext, tried to change ngram=(3,5) with analyzer=**char_wb**, then score got much improved to **0.815**  \nI guess, in case that multiple words are concatinated for example \"gofuckyou\" in below rows, analyzer=char_wb(ngram=3~5) can pick up \"fuck\" and it would improve the accuracy, while analyzer=word will capture the entire as a single word. \n\nFurthermore, the public score was improved from 0.815 to **0.821** by increasing n_estimatos from 1000 to 1500.\n\nHowever, when increasing n_estimators from 1500 to 2000, the score got worse from 0.821 to 0.819. Looks like the number of learning times is too much and resulted in overfitting. I think approx 1500 would be the best for the number of learning.  \n\nngram\u3092\u5909\u66f4\u3057\u3066\u3069\u3046\u30b9\u30b3\u30a2\u304c\u5909\u308f\u308b\u304b\u3092\u8a66\u3057\u3066\u307f\u308b\u3002\u5358\u8a9e\u5358\u4f4d\u3067ngram\u7bc4\u56f2\u30921\uff5e3\u3067\u8a66\u3057\u3066\u307f\u308b\u3068\u3001\u30b9\u30b3\u30a2\u306f0.763\u21920.760\u3078\u3068\u50c5\u304b\u306b\u4f4e\u4e0b\u3002  \n\u4e00\u65b9\u3001\u5358\u8a9e\u5358\u4f4d\u3067\u306f\u306a\u304f\u3001\u5358\u8a9e\u5185\u6587\u5b57\u5358\u4f4d(**char_wb**\uff09\u3067ngram\u7bc4\u56f2\u30923\uff5e5\u3067\u8a66\u3057\u3066\u307f\u308b\u3068\u3001\u30b9\u30b3\u30a2\u306f**0.815**\u3078\u3068\u5927\u304d\u304f\u6539\u5584\u3002  \n\u304a\u305d\u3089\u304f\u3001\u4ee5\u4e0b\u306b\u898b\u3089\u308c\u308b\u3088\u3046\u306a\"gofuckyourelf\"\u306e\u3088\u3046\u306b\u8907\u6570\u5358\u8a9e\u304c\u5207\u308c\u76ee\u306a\u304f\u7e4b\u304c\u3063\u3066\u3044\u308b\u5834\u5408\u306b\u3001analyzer=word\u3060\u3068\u5168\u4f53\u3092\u4e00\u8a9e\u3068\u3057\u3066\u6349\u3048\u3066\u3057\u307e\u3046\u306e\u306b\u5bfe\u3057\u3001analyzer=char_wb (ngram=3~5)\u306b\u3059\u308b\u3068fuck\u90e8\u5206\u3092\u3046\u307e\u304f\u62fe\u3048\u308b\u306e\u3067\u7cbe\u5ea6\u304c\u4e0a\u304c\u308b\u305f\u3081\u3067\u306f\u306a\u3044\u304b\u3068\u601d\u3046\u3002\n\n\u3053\u3053\u304b\u3089\u3055\u3089\u306b\u3001n_estimators\u30921000\u21921500\u306b\u5897\u3084\u3057\u3066\u307f\u308b\u3068\u3001\u30b9\u30b3\u30a2\u306f0.815\u304b\u3089**0.821**\u306b\u6539\u5584\u3002\n\n\u4e00\u65b9\u3001n_estimators\u3092\u3055\u3089\u306b1500\u21922000\u306b\u5897\u3084\u3059\u3068\u3001\u30b9\u30b3\u30a2\u306f0.821\u304b\u30890.819\u3078\u3068\u60aa\u5316\u3002\u5b66\u7fd2\u56de\u6570\u304c\u591a\u3059\u304e\u3066\u904e\u5b66\u7fd2\u306b\u306a\u3063\u305f\u3082\u306e\u3068\u601d\u308f\u308c\u308b\u306e\u3067\u3001\u5b66\u7fd2\u56de\u6570\u306f1500\u7a0b\u5ea6\u304c\u826f\u3044\u3082\u306e\u3068\u601d\u308f\u308c\u308b\u3002","61327623":"Toxic rows are 15,294, which is apprx 10% out of total training data 159,571 rows. It seems almost rows are non-toxic. \"threat\" is only 478 rows, which is 0.3%.   \nTry to weighten some rare items, then newly creat \"total_toxic\". \n\n\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf159,571\u884c\u306e\u3046\u3061\u3001toxic\u306f15,294\u30671\u5272\u7a0b\u5ea6\u3002\u6b86\u3069\u304c\u300c\u5bb3\u306e\u306a\u3044\u300d\u6587\u7ae0\u306e\u3088\u3046\u3060\u3002threat\uff08\u8105\u3057\uff09\u306b\u81f3\u3063\u3066\u306f478\u3068\u50c5\u304b0.3%\u3002\u30ec\u30a2\u5ea6\u304c\u9055\u3046\u306e\u306b\u3001\u30ab\u30a6\u30f3\u30c8\u3068\u3057\u3066\u306f\u540c\u3058\u300c1\u300d\u3068\u3044\u3046\u306e\u3082\u9055\u548c\u611f\u3042\u308b\u306e\u3067\u3001\u30ec\u30a2\u306a\u9805\u76ee\u306e\u91cd\u307f\u3092\u5897\u3057\u3066\u307f\u308b\u3002  \n\u305d\u306e\u4e0a\u3067\u3001\u300ctotal_toxic\u300d\u3092\u65b0\u8a2d\u3059\u308b\u3002","9f507830":"From a quick look, we can see \"non-toxic\" sentences only.  \nLet's take a look at \"toxic\" sentences.  \n\n\u4e0a\u8a18\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u306b\u306f\u3001\u5bb3\u304c\u306a\u3044\u6587\u7ae0\u3057\u304b\u898b\u3048\u306a\u3044\u3002\u5bb3\u306e\u3042\u308b\u6587\u7ae0\u306f\u3069\u306e\u3088\u3046\u306a\u3082\u306e\u304b\u898b\u3066\u307f\u308b\u3002","aea15552":"Reading the training data from past \"jigsaw-toxic-comment-classification-challenge\".\n\n\u5b66\u7fd2\u7528\u306b\u3001\u904e\u53bb\u306e\u300cjigsaw-toxic-comment-classification-challenge\u300d\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u3002","7d8d8165":"# [Jigsaw] Simple LGBM starter using TF-IDF\n\nThis is a simple LightGBM starter using TF-IDF only.  \nThe reason why used TF-IDF only is based on the thought that wheter \"toxic\" or not would be depending on the existance of the specific violent or obscene words in the sentences.  \n\nJust FYI, I attached improvement history as below.  \nAs Data description says, there is no training data for this competition. Hence, please add the training data from \"jigsaw-toxic-comment-classification-challenge\".\n\n\nTF-IDF\u3092\u4f7f\u3063\u305fLightGBM\u306b\u3088\u308b\u30b7\u30f3\u30d7\u30eb\u306aStarter\u3067\u3059\u3002  \nTF-IDF\u306e\u307f\u3092\u4f7f\u3063\u305f\u306e\u306f\u3001\u300c\u5bb3\u306e\u3042\u308b\u6587\u7ae0\u300d\u304b\u3069\u3046\u304b\u3068\u3044\u3046\u306e\u306f\u6216\u308b\u7279\u5b9a\u306e\u8a9e\uff08\u66b4\u529b\u7684\u30fb\u5351\u7325\u7b49\uff09\u304c\u6587\u7ae0\u4e2d\u306b\u5b58\u5728\u3059\u308b\u304b\u3069\u3046\u304b\u306b\u3088\u3063\u3066\u6c7a\u307e\u308b\u3053\u3068\u304c\u591a\u3044\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u3044\u3046\u8003\u3048\u304b\u3089\u3067\u3059\u3002  \n\u3054\u53c2\u8003\u307e\u3067\u306b\u3001\u5f53\u521d\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u304b\u3089\u306e\u6539\u4fee\u5c65\u6b74\u3092\u4ed8\u3051\u307e\u3057\u305f\u3002  \n\n\u672c\u30b3\u30f3\u30da\u306eData description\u306b\u8a18\u8f09\u306e\u901a\u308a\u3001\u3053\u306e\u30b3\u30f3\u30da\u306b\u306fTraining\u30c7\u30fc\u30bf\u304c\u3042\u308a\u307e\u305b\u3093\u306e\u3067\u3001\u904e\u53bb\u306e\"jigsaw-toxic-comment-classification-challenge\"\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8ffd\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n![image.png](attachment:646d43d9-0049-4bf7-a110-a672d0f8861d.png)","cbf9e935":"Then, let's count each factor,meaning, toxic, sever_toxic, obscene, threat, insult and identity_hate.\n\n\u6b21\u306b\u3001toxic(\u5bb3\u304c\u3042\u308b)\u3001severe_toxic\uff08\u975e\u5e38\u306b\u5bb3\u304c\u3042\u308b\uff09\u3001obscene\uff08\u7325\u893b\u306a\uff09\u3001threat\uff08\u8105\u3057\u306e\uff09\u3001insult\uff08\u4fae\u8511\u7684\u306a\uff09\u3001identity_hate\uff08\u5dee\u5225\u7684\u306a\u3001\u3068\u6349\u3048\u308b\u306e\u304c\u3088\u3044\u306e\u3067\u3057\u3087\u3046\u304b\uff09\u304c\u3069\u306e\u7a0b\u5ea6\u3042\u308b\u306e\u304b\u898b\u3066\u307f\u308b\u3002"}}