{"cell_type":{"870a883e":"code","be2970b0":"code","bf2f7011":"code","3f11ca69":"code","cca7273f":"code","4592deed":"code","3b4893e3":"code","1b355aca":"code","2fdb9ec1":"code","6da72b38":"code","6592f400":"code","17eef1a0":"code","9ab92b03":"code","32dc8b49":"code","5b45b100":"code","a567e03e":"code","032dbc96":"code","897c35b4":"code","a97c5675":"code","7ec97e55":"code","ec0c7774":"code","b5380288":"code","774dfcf6":"code","9bbf7881":"code","428b50bc":"code","5e4e0619":"code","9f70afb0":"code","105bc76e":"code","f6c26b96":"code","313d8a29":"code","13aef88e":"markdown","e4475048":"markdown","440af9d0":"markdown","d3252e2b":"markdown","24408d4e":"markdown","7294ef87":"markdown","e9433010":"markdown","1c0958f6":"markdown","f4324377":"markdown","63f432c7":"markdown","d1e5f36a":"markdown","00c4d2dc":"markdown","62e5b7f5":"markdown","67055447":"markdown","188fdffe":"markdown","0948b31c":"markdown","040ca6cc":"markdown","2ac60a6d":"markdown","65d0de37":"markdown","cc222114":"markdown","656734d9":"markdown","cca835f4":"markdown","9bb57c50":"markdown"},"source":{"870a883e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for splitting data to train and test partition\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\n\n# Classifier for modelling\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Evaluates model\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\n# Sample data provided by sklearn\nfrom sklearn.datasets import load_iris","be2970b0":"# load data from scikit learn datasets\niris = load_iris()\nprint(iris.DESCR)\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                     columns= iris['feature_names'] + ['target'])\ndf.head()","bf2f7011":"# observe data shape and target unique values\nprint(\"Data frame shape :\", df.shape)\nprint(\"target unique values :\", df['target'].unique())","3f11ca69":"# checking null value\nfor attribute in df.columns:\n    print(\"column\", attribute, \"null \\t:\", df[attribute].isnull().sum())","cca7273f":"# load data from external file\ntennis = pd.read_csv('..\/input\/weather.nominal.csv')\ntennis.head()","4592deed":"# Gaussian\nclf_gnb = GaussianNB()\n\nclf_gnb.fit(iris.data, iris.target)","3b4893e3":"# fitting\nclf_tree = tree.DecisionTreeClassifier(criterion='entropy', max_leaf_nodes=6)\n\nclf_tree.fit(iris.data, iris.target)","1b355aca":"# visualize model\nimport graphviz\ndot_data = tree.export_graphviz(clf_tree, out_file=None, \n                         feature_names=iris.feature_names,  \n                         class_names=iris.target_names,  \n                         filled=True, rounded=True,  \n                         special_characters=True)  \ngraph = graphviz.Source(dot_data) \ngraph\n# graph.render(\"iris\") ","2fdb9ec1":"clf_neigh = KNeighborsClassifier(n_neighbors=3)\n\nclf_neigh.fit(iris.data, iris.target)","6da72b38":"clf_neuron = MLPClassifier(hidden_layer_sizes=(5, 2), random_state=42, max_iter=1000, solver='lbfgs')\n\nclf_neuron.fit(iris.data, iris.target)","6592f400":"# Spltting\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, random_state=42)\nclass_names = iris.target_names","17eef1a0":"# plotting confusion matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","9ab92b03":"# Gaussian\nclf_gnb = GaussianNB()\n\nclf_gnb.fit(X_train, y_train)\n\npred_gnb = clf_gnb.predict(X_test)\nprint(\"accuracy score \\t=\", accuracy_score(y_test, pred_gnb, normalize=True))\nprint(\"recall score \\t=\", recall_score(y_test, pred_gnb, average='micro'))\nprint(\"f1 score \\t=\", f1_score(y_test, pred_gnb, average='micro'))","32dc8b49":"cnf_matrix_gnb = confusion_matrix(y_test, pred_gnb)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix_gnb, classes=class_names,normalize=True,\n                      title='GaussianNB Confusion Matrix')\n\nplt.show()","5b45b100":"# fitting\nclf_tree = tree.DecisionTreeClassifier(criterion='entropy', max_leaf_nodes=3)\n\nclf_tree.fit(X_train, y_train)\n\npred_tree = clf_tree.predict(X_test)\nprint(\"accuracy score =\", accuracy_score(y_test, pred_tree, normalize=True))\nprint(\"recall score \\t=\", recall_score(y_test, pred_tree, average='micro'))\nprint(\"f1 score \\t=\", f1_score(y_test, pred_tree, average='micro'))","a567e03e":"cnf_matrix_tree = confusion_matrix(y_test, pred_tree)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix_tree, classes=class_names,normalize=True,\n                      title='DecisionTree Confusion Matrix')\n\nplt.show()","032dbc96":"clf_neigh = KNeighborsClassifier(n_neighbors=3)\n\nclf_neigh.fit(X_train, y_train)\n\npred_neigh = clf_neigh.predict(X_test)\nprint(\"accuracy score \\t=\", accuracy_score(y_test, pred_neigh, normalize=True))\nprint(\"recall score \\t=\", recall_score(y_test, pred_neigh, average='micro'))\nprint(\"f1 score \\t=\", f1_score(y_test, pred_neigh, average='micro'))","897c35b4":"cnf_matrix_neigh = confusion_matrix(y_test, pred_neigh)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix_neigh, classes=class_names,normalize=True,\n                      title='kNN Confusion Matrix')\n\nplt.show()","a97c5675":"clf_neuron = MLPClassifier(hidden_layer_sizes=(5, 2), random_state=42, max_iter=1000, solver='lbfgs')\n\nclf_neuron.fit(X_train, y_train)\n\npred_neuron = clf_neuron.predict(X_test)\nprint(\"accuracy score \\t=\", accuracy_score(y_test, pred_neuron, normalize=True))\nprint(\"recall score \\t=\", recall_score(y_test, pred_gnb, average='micro'))\nprint(\"f1 score \\t=\", f1_score(y_test, pred_gnb, average='micro'))","7ec97e55":"cnf_matrix_neuron = confusion_matrix(y_test, pred_neuron)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix_neuron, classes=class_names,normalize=True,\n                      title='NeuralNetwork Confusion Matrix')\n\nplt.show()","ec0c7774":"## splitting and cross validate\nfrom sklearn.model_selection import KFold, cross_val_score, cross_val_predict, cross_validate\n\nX = iris.data\ny = iris.target\nkf = KFold(n_splits=10, random_state=False)\nprint(kf.get_n_splits())\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]","b5380288":"# Gaussian\nclf_gnb = GaussianNB()\n\nclf_gnb.fit(X_train, y_train)\n\ngnb_cv_score = cross_val_score(clf_gnb, X_train, y_train, cv = 10)\nprint(\"cross validate score =\", gnb_cv_score)\npred_gnb = cross_val_predict(clf_gnb, X_test, y_test, cv=10)\nprint(\"accuracy score =\", accuracy_score(y_test, pred_gnb, normalize=True))","774dfcf6":"# fitting\nclf_tree = tree.DecisionTreeClassifier(criterion='entropy', max_leaf_nodes=6)\n\nclf_tree.fit(X_train, y_train)\n\ntree_cv_score = cross_val_score(clf_tree, X_train, y_train, cv = 10)\nprint(\"cross validate score =\", tree_cv_score)\npred_tree = cross_val_predict(clf_tree, X_test, y_test, cv=10)\nprint(\"accuracy score =\", accuracy_score(y_test, pred_tree, normalize=True))","9bbf7881":"clf_neigh = KNeighborsClassifier(n_neighbors=3)\n\nclf_neigh.fit(X_train, y_train)\n\nneigh_cv_score = cross_val_score(clf_neigh, X_train, y_train, cv = 10)\nprint(\"cross validate score =\", neigh_cv_score)\npred_neigh = cross_val_predict(clf_neigh, X_test, y_test, cv=10)\nprint(\"accuracy score =\", accuracy_score(y_test, pred_neigh, normalize=True))","428b50bc":"clf_neuron = MLPClassifier(hidden_layer_sizes=(5, 2), random_state=42, max_iter=1000, solver='lbfgs')\n\nclf_neuron.fit(X_train, y_train)\n\nneuron_cv_score = cross_val_score(clf_neuron, X_train, y_train, cv = 10)\nprint(\"cross validate score =\", neuron_cv_score)\npred_neuron = cross_val_predict(clf_neuron, X_test, y_test, cv=10)\nprint(\"accuracy score =\", accuracy_score(y_test, pred_neuron, normalize=True))","5e4e0619":"# Save the model using Pickle Library\nimport pickle\n\nmodels = []\nmodels.append(clf_gnb)\nmodels.append(clf_tree)\nmodels.append(clf_neigh)\nmodels.append(clf_neuron)\n\npkl_filename = 'model-iris.pkl'\nwith open(pkl_filename, 'wb') as file:  \n    for model in models:\n        pickle.dump(model, file)","9f70afb0":"models = []\npkl_filename = 'model-iris.pkl'\nwith open(pkl_filename, 'rb') as file:\n    while True:\n        try:\n            models.append(pickle.load(file))\n        except EOFError:\n            break","105bc76e":"loaded_gnb = models[0]\nloaded_tree = models[1]\nloaded_neigh = models[2]\nloaded_neuron = models[3]","f6c26b96":"import random as rd\n\n# get extreme value from each attribute, and make a random instance\nmax_v = np.amax(X, axis = 0)\nmin_v = np.amin(X, axis = 0)\nnew_instance = [round(rd.uniform(min_v[0],max_v[0]),1),\n               round(rd.uniform(min_v[1],max_v[1]),1),\n               round(rd.uniform(min_v[2],max_v[2]),1),\n               round(rd.uniform(min_v[3],max_v[3]),1)]\nprint(new_instance)","313d8a29":"# predicting the new instance \nnew_i_pred1 = loaded_gnb.predict([new_instance])\nprint('new instance prediction using Naive bayes:', iris.target_names[new_i_pred1])\n\nnew_i_pred2 = loaded_tree.predict([new_instance])\nprint('new instance prediction using DecisionTree:', iris.target_names[new_i_pred2])\n\nnew_i_pred3 = loaded_neigh.predict([new_instance])\nprint('new instance prediction using kNN:', iris.target_names[new_i_pred3])\n\nnew_i_pred4 = loaded_neuron.predict([new_instance])\nprint('new instance prediction using NeuralNetwork:', iris.target_names[new_i_pred4])\n","13aef88e":"### kNN","e4475048":"### Neural Network (MLP)","440af9d0":"### Naive Bayes","d3252e2b":"## Save the Model","24408d4e":"## Load Model","7294ef87":"## Adding an Instance","e9433010":"### Neural Network (MLP)","1c0958f6":"### Naive Bayes","f4324377":"### Decision Tree","63f432c7":"# Import Modules","d1e5f36a":"# Modelling","00c4d2dc":"## 3. Cross-Validation","62e5b7f5":"### Decision Tree","67055447":"### kNN","188fdffe":"## 1. Full training","0948b31c":"### k-Nearest Neighbors (kNN)","040ca6cc":"### Naive Bayes","2ac60a6d":"### Neural Network (MLP)","65d0de37":"<h1><center> Introduction to Scikit-Learn and Pandas <\/center><\/h1>\n<center>**November 2018**<\/center>\n<br><br>\n+ Scikit-learn <br>\n[Scikit-learn](http:\/\/scikit-learn.org\/stable\/) (formerly scikits.learn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n<br>\n+ Matplotlib <br>\n[Matplotlib](https:\/\/matplotlib.org\/) is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+. There is also a procedural \"pylab\" interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged.\n<br>\n+ Pandas <br>\n[Pandas](https:\/\/pandas.pydata.org\/) (Python Data Analysis Library) is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. pandas is a NumFOCUS sponsored project. This will help ensure the success of development of pandas as a world-class open-source project, and makes it possible to donate to the project.","cc222114":"### Decision Tree","656734d9":"# Load, Preprocessing and Preview Data","cca835f4":"In this notebook, we will use 3 modelling method: \n* Full Training : all of the sample data will be used as training data. Prediction is not a part of this method\n* Hold-out : split the data sample to two parts (training data and test data)\n* Cross-validation : the data will be split randomly into *k* group, one group will be used as train data, and the rest are used as test data","9bb57c50":"## 2. Hold-Out"}}