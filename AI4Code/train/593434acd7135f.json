{"cell_type":{"90a091af":"code","141226ac":"code","6248e68f":"code","ace470f7":"code","8ac5d7c9":"code","12d35e8a":"code","e03d2398":"code","c24c46ec":"code","d38e621c":"code","e6e4daf8":"code","4f52c58d":"code","83b83f5c":"code","dd6d61f9":"code","910f676f":"code","4f3d9b4d":"code","6300236d":"code","66de3444":"code","69b79f3d":"code","d0999054":"code","747ad390":"code","179a99a0":"markdown","fe3f94b7":"markdown","71ab61ec":"markdown","eb48a9ec":"markdown","5b7d0b30":"markdown","b35c1829":"markdown","2a55b1d4":"markdown","7108fa85":"markdown","791285c5":"markdown","543508a0":"markdown","b55c51a6":"markdown","4c846763":"markdown","17042112":"markdown","9a285147":"markdown","fc329966":"markdown","808bd65a":"markdown"},"source":{"90a091af":"import os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\nimport zipfile\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom functools import partial\nfrom tensorflow.keras.optimizers import Nadam\nimport pandas as pd\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns","141226ac":"image_count = 0\nlabels = []\ntrain_counts = []\nfor dirname in os.listdir('..\/input\/split-garbage-dataset\/train'):\n    labels.append(dirname)\n    image_count = 0\n    for img in os.listdir(os.path.join('..\/input\/split-garbage-dataset\/train',dirname)):\n        image_count +=1\n    train_counts.append(image_count)\n\nprint(list(zip(train_counts,labels)))","6248e68f":"plt.pie(train_counts,\n        explode=(0, 0, 0, 0, 0, 0) , \n        labels=labels,\n        autopct='%1.1f%%');\nplt.title('-- TRAINING SET --');","ace470f7":"class_weights = []\ntotal_samples = train_generator.samples              #1768\ntotal_classes = len(train_generator.class_indices)   #6\nfor ele in train_counts:\n    result = round(total_samples \/ (total_classes * ele),2)\n    class_weights.append(result)\nprint(dict(zip(labels,class_weights)))\n\nclass_weights = dict(zip(train_generator.class_indices.values(),class_weights))","8ac5d7c9":"# Add our data-augmentation parameters to ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255.,\n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True,\n                                   vertical_flip = True,\n                                  )\n\n# Note that the validation data should not be augmented!\nvalid_datagen = ImageDataGenerator( rescale = 1.0\/255. )\ntest_datagen = ImageDataGenerator( rescale = 1.0\/255. )\n\n# Flow training images in batches of 8 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory('..\/input\/split-garbage-dataset\/train',\n                                                    batch_size =8,\n                                                    class_mode = 'categorical', \n                                                    target_size = (150,150))     \n\n# Flow validation images in batches of 4 using valid_datagen generator\nvalidation_generator =  valid_datagen.flow_from_directory( '..\/input\/split-garbage-dataset\/valid',\n                                                          batch_size  = 4,\n                                                          class_mode  = 'categorical', \n                                                          target_size = (150,150))     \n\n# Flow test images using test_datagen generator\ntest_generator = test_datagen.flow_from_directory('..\/input\/split-garbage-dataset\/test',\n                                                  batch_size = 1,\n                                                  class_mode = 'categorical',\n                                                  target_size = (150,150),shuffle = False)     \n","12d35e8a":"tf.keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nxception_model = keras.applications.xception.Xception(weights=\"imagenet\",\n                                                      include_top=False,\n                                                      input_shape = (150,150,3))\navg = keras.layers.GlobalAveragePooling2D()(xception_model.output)\noutput = keras.layers.Dense(6, activation=\"softmax\")(avg)\nmodel = keras.models.Model(inputs=xception_model.input, outputs=output)","e03d2398":"optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)\nmodel.compile(optimizer =optimizer ,loss = 'categorical_crossentropy',metrics =['accuracy'])","c24c46ec":"early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience = 10,restore_best_weights = True)\nlr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor = 0.7,patience = 2)\nmodel_checkpoint =tf.keras.callbacks.ModelCheckpoint('GarbageClassifier_Xce.h5', save_best_only=True)\n\nclass CustomCallBack(tf.keras.callbacks.Callback):\n        def on_epoch_end(self,epoch,logs={}):\n            if(logs.get('accuracy')>0.98):\n                print(\"\\nReached 98.0% accuracy so cancelling training!\")\n                self.model.stop_training = True\n                \nmycallback = CustomCallBack()","d38e621c":"history = model.fit(\n            train_generator,\n            steps_per_epoch=train_generator.samples\/train_generator.batch_size,\n            epochs = 30,\n            validation_data = validation_generator,\n            validation_steps= validation_generator.samples\/validation_generator.batch_size,\n            class_weight = class_weights ,\n            callbacks= [early_stopping_cb,model_checkpoint,lr_scheduler,mycallback],\n            verbose=1)","e6e4daf8":" fig = plt.figure(figsize=(10,10))\n\n# Plot accuracy\nplt.subplot(221)\nplt.plot(history.history['accuracy'],'bo-', label = \"acc\")\nplt.plot(history.history['val_accuracy'], 'ro-', label = \"val_acc\")\nplt.title(\"train_accuracy vs val_accuracy\")\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"epochs\")\nplt.grid(True)\nplt.legend()\n\n# Plot loss function\nplt.subplot(222)\nplt.plot(history.history['loss'],'bo-', label = \"loss\")\nplt.plot(history.history['val_loss'], 'ro-', label = \"val_loss\")\nplt.title(\"train_loss vs val_loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(223)\nplt.plot(history.epoch,history.history['lr'],'o-')\nplt.title(\"train_loss vs val_loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\")\nplt.grid(True)\nplt.legend()","4f52c58d":"model.evaluate(test_generator,batch_size = 1) \n","83b83f5c":"y_pred = model.predict(test_generator)\ny_pred = np.argmax(y_pred, axis=1)\nprint(classification_report(test_generator.classes, y_pred))\ncf_matrix = confusion_matrix(test_generator.classes, y_pred)\nplt.figure(figsize=(10,10))\nheatmap = sns.heatmap(cf_matrix, xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys(), annot=True, fmt='d', color='blue')\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.title('Confusion matrix of model')","dd6d61f9":"tf.keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nxception_model = keras.applications.xception.Xception(weights=\"imagenet\",\n                                                      include_top=False,\n                                                      input_shape = (150,150,3))\navg = keras.layers.GlobalAveragePooling2D()(xception_model.output)\noutput = keras.layers.Dense(6, activation=\"softmax\")(avg)\nmodel2 = keras.models.Model(inputs=xception_model.input, outputs=output)","910f676f":"optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)\nmodel2.compile(optimizer =optimizer ,loss = 'categorical_crossentropy',metrics =['accuracy'])","4f3d9b4d":"def exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1**(epoch \/ s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(lr0=0.001, s=10)\n\n\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience = 10,restore_best_weights = True)\nlr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\nmc =tf.keras.callbacks.ModelCheckpoint('XceptionLR_Xce2exp.h5', save_best_only=True)\nclass CustomCallBack(tf.keras.callbacks.Callback):\n        def on_epoch_end(self,epoch,logs={}):\n            if(logs.get('accuracy')>0.99):\n                print(\"\\nReached 99.0% accuracy so cancelling training!\")\n                self.model.stop_training = True","6300236d":"mycallback = CustomCallBack()\nhistory1 = model2.fit(\n            train_generator,\n            steps_per_epoch=train_generator.samples\/train_generator.batch_size,\n            epochs = 30,\n            validation_data = validation_generator,\n            validation_steps= validation_generator.samples\/validation_generator.batch_size,\n            callbacks= [early_stopping_cb,mc,lr_scheduler,mycallback],\n            verbose=1)","66de3444":"fig = plt.figure(figsize=(10,10))\n\n# Plot accuracy\nplt.subplot(221)\nplt.plot(history1.history['accuracy'],'bo-', label = \"acc\")\nplt.plot(history1.history['val_accuracy'], 'ro-', label = \"val_acc\")\nplt.title(\"train_accuracy vs val_accuracy\")\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"epochs\")\nplt.grid(True)\nplt.legend()\n\n# Plot loss function\nplt.subplot(222)\nplt.plot(history1.history['loss'],'bo-', label = \"loss\")\nplt.plot(history1.history['val_loss'], 'ro-', label = \"val_loss\")\nplt.title(\"train_loss vs val_loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(223)\nplt.plot(history1.epoch,history1.history['lr'],'o-')\nplt.title(\"train_loss vs val_loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\")\nplt.grid(True)\nplt.legend()","69b79f3d":"model2.evaluate(test_generator,batch_size = 1)\n","d0999054":"test_generator.class_indices.keys()","747ad390":"y_pred = model2.predict(test_generator)\ny_pred = np.argmax(y_pred, axis=1)\nprint(classification_report(test_generator.classes, y_pred))\ncf_matrix = confusion_matrix(test_generator.classes, y_pred)\nplt.figure(figsize=(10,10))\nheatmap = sns.heatmap(cf_matrix, xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys(), annot=True, fmt='d', color='red')\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.title('Confusion matrix of model')","179a99a0":"##### That brings us to the end of the notebook and please let me know if I can help you understand something in the notebook and give me an upvote if you learnt something new today! ;) ","fe3f94b7":"##### Well! The loss curve looks pretty good now and the model performs better!","71ab61ec":"### Thank you!! :)","eb48a9ec":"##### Let's use a different learning rate scheduler (Exponential)  to smoothen the curve ","5b7d0b30":"##### A Decent Model? ;)\n\n","b35c1829":"# Building ImageGenerator for augmentation ","2a55b1d4":"#### Callbacks and Learning Rate Scheduler","7108fa85":"##### Let's go ahead and find out class weights that could be used to slightly mitigate our class imbalance issue while training our model","791285c5":"## Xception Model","543508a0":"# Model","b55c51a6":"##### Looks like we have an imbalanced dataset!","4c846763":"# Let's have a look at our Data","17042112":"# MODEL 2","9a285147":"# Importing necessary modules ","fc329966":"##### The class with lesser number of samples gets more weight and gets penalized accordingly while training!\n","808bd65a":"> class_weight = n_total_samples \/ (n_total_classes * n_class_samples)"}}