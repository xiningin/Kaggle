{"cell_type":{"1c96e5ad":"code","f66a9d90":"code","41fc1daf":"code","bc4838a4":"code","7ec0cf6b":"code","e1e26c29":"code","8d3f7517":"code","b484a0c0":"code","5907e9b7":"code","6ce2a4e3":"code","c97c1791":"code","a14fba6a":"code","087fe9fd":"code","d56b2d27":"code","4848099a":"code","cd753e9a":"code","3d07e832":"code","3a770018":"code","7ca33650":"code","a15f8f5d":"code","fe9b075d":"code","c3c0982d":"code","56b607d9":"code","99473c78":"code","8dc96f46":"code","9c0c76fe":"code","03e8ace0":"code","d551725a":"code","612a551b":"code","27c7c477":"markdown","21d22beb":"markdown","4a7c8d4d":"markdown","a20ee963":"markdown","a4b931ab":"markdown","f7078c84":"markdown","d5b0e39c":"markdown","67ad540a":"markdown","e6ee3570":"markdown","e71d6a3f":"markdown","c85c9dca":"markdown","e28cabb3":"markdown","65d02e4d":"markdown","28c17693":"markdown","c131f8cc":"markdown","bcbcedfd":"markdown","e285dda7":"markdown","c0e22807":"markdown","cb8f4115":"markdown","bd71f9f0":"markdown"},"source":{"1c96e5ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f66a9d90":"# Import numpy, pandas, matpltlib.pyplot, sklearn modules and seaborn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)\nplt.style.use('ggplot')\n\n# Import KNeighborsClassifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc","41fc1daf":"# Import the data\ndf = pd.read_csv('\/kaggle\/input\/us-accidents\/US_Accidents_May19.csv')\ndf.info()","bc4838a4":"# Convert Start_Time and End_Time to datetypes\ndf['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\ndf['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')\n\n# Extract year, month, day, hour and weekday\ndf['Year']=df['Start_Time'].dt.year\ndf['Month']=df['Start_Time'].dt.strftime('%b')\ndf['Day']=df['Start_Time'].dt.day\ndf['Hour']=df['Start_Time'].dt.hour\ndf['Weekday']=df['Start_Time'].dt.strftime('%a')\n\n# Extract the amount of time in the unit of minutes for each accident, round to the nearest integer\ntd='Time_Duration(min)'\ndf[td]=round((df['End_Time']-df['Start_Time'])\/np.timedelta64(1,'m'))\ndf.info()","7ec0cf6b":"# Check if there is any negative time_duration values\ndf[td][df[td]<=0]","e1e26c29":"# Drop the rows with td<0\n\nneg_outliers=df[td]<=0\n\n# Set outliers to NAN\ndf[neg_outliers] = np.nan\n\n# Drop rows with negative td\ndf.dropna(subset=[td],axis=0,inplace=True)\ndf.info()","8d3f7517":"# Double check to make sure no more negative td\ndf[td][df[td]<=0]","b484a0c0":"# Remove outliers for Time_Duration(min): n * standard_deviation (n=3), backfill with median\n\nn=3\n\nmedian = df[td].median()\nstd = df[td].std()\noutliers = (df[td] - median).abs() > std*n\n\n# Set outliers to NAN\ndf[outliers] = np.nan\n\n# Fill NAN with median\ndf[td].fillna(median, inplace=True)\ndf.info()","5907e9b7":"# Print time_duration information\nprint('Max time to clear an accident: {} minutes or {} hours or {} days; Min to clear an accident td: {} minutes.'.format(df[td].max(),round(df[td].max()\/60), round(df[td].max()\/60\/24), df[td].min()))","6ce2a4e3":"# Set the list of features to include in Machine Learning\nfeature_lst=['Source','TMC','Severity','Start_Lng','Start_Lat','Distance(mi)','Side','City','County','State','Timezone','Temperature(F)','Humidity(%)','Pressure(in)', 'Visibility(mi)', 'Wind_Direction','Weather_Condition','Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal','Turning_Loop','Sunrise_Sunset','Hour','Weekday', 'Time_Duration(min)']","c97c1791":"# Select the dataset to include only the selected features\ndf_sel=df[feature_lst].copy()\ndf_sel.info()","a14fba6a":"# Check missing values\ndf_sel.isnull().mean()","087fe9fd":"df_sel.dropna(subset=df_sel.columns[df_sel.isnull().mean()!=0], how='any', axis=0, inplace=True)\ndf_sel.shape","d56b2d27":"# Set state\nstate='PA'\n\n# Select the state of Pennsylvania\ndf_state=df_sel.loc[df_sel.State==state].copy()\ndf_state.drop('State',axis=1, inplace=True)\ndf_state.info()","4848099a":"# Map of accidents, color code by county\n\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=df_state, hue='County', legend=False, s=20)\nplt.show()","cd753e9a":"# Generate dummies for categorical data\ndf_state_dummy = pd.get_dummies(df_state,drop_first=True)\n\ndf_state_dummy.info()","3d07e832":"# Assign the data\ndf=df_state_dummy\n\n\n# Set the target for the prediction\ntarget='Severity'\n\n\n\n# Create arrays for the features and the response variable\n\n# set X and y\ny = df[target]\nX = df.drop(target, axis=1)\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)","3a770018":"# List of classification algorithms\nalgo_lst=['Logistic Regression',' K-Nearest Neighbors','Decision Trees','Random Forest']\n\n# Initialize an empty list for the accuracy for each algorithm\naccuracy_lst=[]","7ca33650":"# Logistic regression\nlr = LogisticRegression(random_state=0)\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\nprint(\"[Logistic regression algorithm] accuracy_score: {:.3f}.\".format(acc))","a15f8f5d":"# Create a k-NN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n\n# Predict the labels for the training data X\ny_pred = knn.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\nprint('[K-Nearest Neighbors (KNN)] knn.score: {:.3f}.'.format(knn.score(X_test, y_test)))\nprint('[K-Nearest Neighbors (KNN)] accuracy_score: {:.3f}.'.format(acc))","fe9b075d":"# Setup arrays to store train and test accuracies\nneighbors = np.arange(3, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, n_neighbor in enumerate(neighbors):\n    \n    # Setup a k-NN Classifier with n_neighbor\n    knn = KNeighborsClassifier(n_neighbors=n_neighbor)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train,y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n","c3c0982d":"# Decision tree algorithm\n\n# Instantiate dt_entropy, set 'entropy' as the information criterion\ndt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n\n\n# Fit dt_entropy to the training set\ndt_entropy.fit(X_train, y_train)\n\n# Use dt_entropy to predict test set labels\ny_pred= dt_entropy.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_entropy = accuracy_score(y_test, y_pred)\n\n\n# Print accuracy_entropy\nprint('[Decision Tree -- entropy] accuracy_score: {:.3f}.'.format(accuracy_entropy))\n\n\n\n# Instantiate dt_gini, set 'gini' as the information criterion\ndt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=1)\n\n\n# Fit dt_entropy to the training set\ndt_gini.fit(X_train, y_train)\n\n# Use dt_entropy to predict test set labels\ny_pred= dt_gini.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_gini = accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\nacc=accuracy_gini\naccuracy_lst.append(acc)\n\n# Print accuracy_gini\nprint('[Decision Tree -- gini] accuracy_score: {:.3f}.'.format(accuracy_gini))","56b607d9":"# Random Forest algorithm\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)\n\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"[Randon forest algorithm] accuracy_score: {:.3f}.\".format(acc))\n","99473c78":"feature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\n\n# Creating a bar plot, displaying only the top k features\nk=10\nsns.barplot(x=feature_imp[:10], y=feature_imp.index[:k])\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","8dc96f46":"# List top k important features\nk=20\nfeature_imp.sort_values(ascending=False)[:k]","9c0c76fe":"# Create a selector object that will use the random forest classifier to identify\n# features that have an importance of more than 0.03\nsfm = SelectFromModel(clf, threshold=0.03)\n\n# Train the selector\nsfm.fit(X_train, y_train)\n\nfeat_labels=X.columns\n\n# Print the names of the most important features\nfor feature_list_index in sfm.get_support(indices=True):\n    print(feat_labels[feature_list_index])","03e8ace0":"# Transform the data to create a new dataset containing only the most important features\n# Note: We have to apply the transform to both the training X and test X data.\nX_important_train = sfm.transform(X_train)\nX_important_test = sfm.transform(X_test)\n\n# Create a new random forest classifier for the most important features\nclf_important = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n\n# Train the new classifier on the new dataset containing the most important features\nclf_important.fit(X_important_train, y_train)","d551725a":"# Apply The Full Featured Classifier To The Test Data\ny_pred = clf.predict(X_test)\n\n# View The Accuracy Of Our Full Feature Model\nprint('[Randon forest algorithm -- Full feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_pred)))\n\n# Apply The Full Featured Classifier To The Test Data\ny_important_pred = clf_important.predict(X_important_test)\n\n# View The Accuracy Of Our Limited Feature Model\nprint('[Randon forest algorithm -- Limited feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_important_pred)))","612a551b":"# Make a plot of the accuracy scores for different algorithms\n\n# Generate a list of ticks for y-axis\ny_ticks=np.arange(len(algo_lst))\n\n# Combine the list of algorithms and list of accuracy scores into a dataframe, sort the value based on accuracy score\ndf_acc=pd.DataFrame(list(zip(algo_lst, accuracy_lst)), columns=['Algorithm','Accuracy_Score']).sort_values(by=['Accuracy_Score'],ascending = True)\n\n# Make a plot\nax=df_acc.plot.barh('Algorithm', 'Accuracy_Score', align='center',legend=False,color='0.5')\n\n# Add the data label on to the plot\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+0.02, i.get_y()+0.2, str(round(i.get_width(),2)), fontsize=10)\n\n# Set the limit, lables, ticks and title\nplt.xlim(0,1.05)\nplt.xlabel('Accuracy Score')\nplt.yticks(y_ticks, df_acc['Algorithm'], rotation=0)\nplt.title('[{}] Which algorithm is better predicting severity?'.format(state))\n\nplt.show()","27c7c477":"### Step 8. Deal with categorical data: pd.get_dummies()\n","21d22beb":"### Step 7. Select the state of interest: PA\n\n\nDue to the limitation of personal laptop, the whole US dataset is too big to handle","4a7c8d4d":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n#### Data preparation: train_test_split","a20ee963":"### Step 3. Extract year, month, day, hour, weekday, and time to clear accidents","a4b931ab":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n#### Plot the accuracy score versus algorithm","f7078c84":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm B. The K-Nearest Neighbors (KNN) algorithm\n   ##### KNN with 6 neighors","d5b0e39c":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm D. Random Forest   \n   ##### n_estimators=100                 ","67ad540a":"### Step 4. Deal with outliers\n\n#### B. Fill outliers with median values","e6ee3570":"### Step 6. Drop rows with missing values","e71d6a3f":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm B. The K-Nearest Neighbors (KNN) algorithm\n   ##### Optmize the number of neighors: plot the accuracy versus number of neighbors","c85c9dca":"### Step 2. Import the dataset","e28cabb3":"This cell below takes long time to run, so I decided not to run here. Basically it will generate a plot to show the accuracy for each number of neighbors to guide your optimization.","65d02e4d":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm D. Random Forest   \n   ##### Select the top important features, set the threshold      ","28c17693":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm C. Decision Tree                 ","c131f8cc":"### Step 5. Select a list of features for machine learning algorithms\n\n Only select relavant columns without overwhelming the computer","bcbcedfd":"### Step 4. Deal with outliers\n\n#### A. Drop rows with negative time_duration","e285dda7":"### Step 1. Import libraries","c0e22807":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm D. Random Forest   \n   ##### Visualize important features      ","cb8f4115":"### About me\nRonghui Zhou\nzhou.uf@gmail.com\nhttps:\/\/github.com\/RonghuiZhou\/us-accidents","bd71f9f0":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm A. Logistic regression              "}}