{"cell_type":{"10719d17":"code","e2a42ce0":"code","37ae939a":"code","224d5f33":"code","31e00cc7":"code","9f15a282":"code","8cfa60af":"code","e15698d6":"code","1fb8f72b":"code","0487f7d6":"code","fa503f26":"code","bbccf7eb":"code","8925af4c":"code","89970837":"code","03492b47":"code","f96b8649":"code","7da2beee":"code","13fbf8ca":"code","9a0d3a90":"code","89d41c62":"code","073f6982":"code","113f0fb9":"code","7e6d66a0":"code","cd2896f7":"code","e8d1c65e":"code","68992d51":"code","e7ad94e6":"code","0e713e1f":"code","bec07343":"code","2dccab88":"code","ce21d17d":"code","f13a6134":"code","f195841a":"code","62665d4d":"code","42a33474":"code","b32fcc85":"code","c6141070":"code","25aea11a":"code","1e55952a":"code","706f40bb":"code","ee907006":"code","d4703a70":"code","aa95e013":"code","e168ea00":"code","a4f78a23":"code","e690ec5e":"code","6b6c7dd4":"code","a7a45d5e":"code","fdb647f9":"markdown","12749e6a":"markdown","3d8888af":"markdown","ded97f2f":"markdown","b165cb53":"markdown","c80d2367":"markdown","8764796c":"markdown","6cacef92":"markdown","a7229f7e":"markdown","7477446e":"markdown","cfaf0558":"markdown","ea1d2243":"markdown","59e344d7":"markdown","b8f7b60a":"markdown","3f8ffc33":"markdown","4725165d":"markdown","92ce1b8e":"markdown","c75499f6":"markdown","9893fc7a":"markdown","20151d4e":"markdown","31e648d2":"markdown","315af6f9":"markdown","e1128327":"markdown","b5654146":"markdown","52682612":"markdown","98a621cb":"markdown"},"source":{"10719d17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e2a42ce0":"df = pd.read_csv('..\/input\/wine-quality\/winequalityN.csv')","37ae939a":"df.head()","224d5f33":"df.shape","31e00cc7":"df.info()","9f15a282":"df.isnull().sum().sort_values(ascending=False)","8cfa60af":"missing_val_cols = [\"fixed acidity\", \"pH\", \"volatile acidity\", \"sulphates\", \"citric acid\", \"residual sugar\", \"chlorides\"]","e15698d6":"for col in missing_val_cols:\n    mean = df[col].mean()\n    df[col].fillna(mean, inplace=True)\n    ","1fb8f72b":"df.isnull().sum()","0487f7d6":"df['type'].value_counts()","fa503f26":"df['quality'].value_counts()","bbccf7eb":"import seaborn as sns\nimport matplotlib.pyplot as plt","8925af4c":"sns.pairplot(df, hue='quality', corner=True)","89970837":"sns.catplot(x=\"type\", y=\"citric acid\", kind=\"box\", hue=\"quality\", data=df)","03492b47":"sns.countplot(x=\"quality\", hue=\"type\", data=df)","f96b8649":"sns.countplot(x=\"type\", hue=\"quality\", data=df)","7da2beee":"corr = df.corr()","13fbf8ca":"plt.subplots(figsize=(10, 10))\nsns.heatmap(corr, annot=True,cmap=sns.diverging_palette(200, 10, as_cmap=True), vmin=-1, vmax=1, linewidths=.5, fmt=\".2f\")","9a0d3a90":"df = pd.get_dummies(df, drop_first=True)","89d41c62":"df.head()","073f6982":"df = df.rename(columns={\"type_white\": \"wine_type\"})","113f0fb9":"df[\"wine_quality\"] = [1 if x>6 else 0 for x in df.quality]","7e6d66a0":"df.head()","cd2896f7":"y = df[\"wine_quality\"]","e8d1c65e":"y.value_counts()","68992d51":"y","e7ad94e6":"x = df.drop([\"quality\", \"wine_quality\"], axis=1)","0e713e1f":"x","bec07343":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix","2dccab88":"log = pd.DataFrame(columns=[\"model\", \"accuracy\"])","ce21d17d":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","f13a6134":"clf = LogisticRegression(solver='liblinear')\n\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n\nclf.score(X_train, y_train)\n\nscore = accuracy_score(y_test, y_pred)\n\nconfusion_matrix(y_test, y_pred)","f195841a":"log = log.append({\"model\": \"logistic regression\", \"accuracy\": score}, ignore_index=True)","62665d4d":"from sklearn.svm import SVC","42a33474":"model = SVC(kernel=\"rbf\",C=1)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nscore = accuracy_score(y_test, y_pred)","b32fcc85":"confusion_matrix(y_test, y_pred)","c6141070":"log = log.append({\"model\": \"SVC\", \"accuracy\": score}, ignore_index=True)","25aea11a":"log","1e55952a":"from sklearn.ensemble import RandomForestClassifier","706f40bb":"clf = RandomForestClassifier(random_state = 1)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nlog = log.append({\"model\": \"Random Forest\", \"accuracy\": score}, ignore_index=True)","ee907006":"log","d4703a70":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(criterion='entropy',random_state=7)\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nlog = log.append({\"model\": \"Decision tree\", \"accuracy\": score}, ignore_index=True)","aa95e013":"log","e168ea00":"from sklearn.neighbors import KNeighborsClassifier","a4f78a23":"clf = KNeighborsClassifier(n_neighbors=3)\nclf.fit(X_train, y_train)\nclf.predict(X_test)\ny_pred = clf.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nlog = log.append({\"model\": \"K nearest neighbours\", \"accuracy\": score}, ignore_index=True)","e690ec5e":"log","6b6c7dd4":"from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\nclf.predict(X_test)\ny_pred = clf.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nlog = log.append({\"model\": \"GaussianNB\", \"accuracy\": score}, ignore_index=True)\nprint(log)","a7a45d5e":"plt.figure(figsize=(10, 5))\nplt.bar(log[\"model\"], log[\"accuracy\"], width=0.4)\nplt.xlabel(\"Classifier\")\nplt.ylabel(\"accuracy\")\nfor index, value in enumerate(log[\"accuracy\"]):\n    plt.text(index,value, str(round(value, 2)))\nplt.show()","fdb647f9":"## **Data exploration**","12749e6a":"<font size=\"4\">Since the column ```type``` of object type, we convert it into integer by assigning values 0 and 1 to wine types<\/font>","3d8888af":"<font size=\"4\">**Gaussian Naive Bayes**<\/font>","ded97f2f":"<font size=\"5\">**Random Forest has shown the best result (accuracy score = 88%) in comparison to other classifiers**<\/font>","b165cb53":"<font size=\"5\">In the next version, I will try to balance the dataset using SMOTEENN and see if there's any improvement in accuracy scores.<\/font>\n\n<font size=\"4\">If you found this helpful, please feel free to upvote \ud83d\udc4d<\/font>","c80d2367":"<font size=\"4\">Few columns have missing values, the columns \"fixed acidity\" has the highest number of missing values<\/font>","8764796c":"<font size=\"4\">Store the label in ```y```<\/font>","6cacef92":"<font size=\"4\">```Density``` and ```alcohol``` are highly negatively correlated and ```free sulfur dioxide``` and ```total sulfur dioxide``` are highly positively correlated<\/font>","a7229f7e":"<font size=\"4\">We have an unbalanced dataset, more number of white wine than red <\/font>","7477446e":"<font size=\"4\">**C-support Vector Classification**<\/font>","cfaf0558":"<font size=\"4\">**Logistic Regeression**<\/font>","ea1d2243":"<font size=\"4\">**K nearest neighbour**<\/font>","59e344d7":"<font size=\"4\">Store the feature columns in ```x```<\/font>","b8f7b60a":"<font size=\"4\">Rename the column ```type_white``` to ```wine_type```<\/font>","3f8ffc33":"<font size=\"4\">We will replace the missing the values with the column mean<\/font>","4725165d":"## **Data Viz**","92ce1b8e":"## **Read the data**","c75499f6":"<font size=\"4\">Now, there are no more missing values<\/font>","9893fc7a":"<font size=\"4\">The dataset is unbalanced with respect to the wine quality as well.<\/font>","20151d4e":"<font size=\"4\">White wine has many outliers for quality 6<\/font>","31e648d2":"<font size=\"4\">Since, we had an unbalanced dataset with respect to wine quality, we divide the qualities into two  groups - 1 (for wine quality 7, 8, 9) and 0 (for 6 and below)<\/font>","315af6f9":"## **Replace missing values**","e1128327":"<font size=\"4\">**Decision Tree Classifier**<\/font>","b5654146":"<font size=\"4\">The data has 6497 rows and 13 columns<\/font>","52682612":"## **Model Training**","98a621cb":"<font size=\"4\">**Random Forest Classifier**<\/font>"}}