{"cell_type":{"b47f5dc2":"code","18fdd708":"code","1b71be31":"code","7ed7dc86":"code","f1eccf19":"code","23119bfa":"code","54a12c40":"code","4fa9b9f2":"code","4260089f":"code","34572e6b":"code","db42a2b1":"code","24af349b":"code","67313627":"code","ae1f39d6":"code","5b3ffa28":"markdown"},"source":{"b47f5dc2":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, re, time, warnings, pickle, psutil, random\n\nfrom math import ceil\nimport lightgbm as lgb\nfrom typing import Union\n\nwarnings.filterwarnings('ignore')","18fdd708":"########################### Helpers\n#################################################################################\n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n## RMSE metric for occasions \ndef rmse(y, y_pred):\n    return np.sqrt(np.mean(np.square(y - y_pred)))","1b71be31":"########################### Vars\n#################################################################################\nTARGET    = 'sales'    # Our Target     \nEND_TRAIN = 1913       # Last day of training set     \nSEED      = 42         # Seed for deterministic processes  \nseed_everything(SEED)  # Seeder\n\nVERBOSE   = False      # Verbosity on all wrmsse levels   \n\nremove_features = ['id','t_block','date','d', 'wm_yr_wk',TARGET]","7ed7dc86":"########################### Metric and Evaluators\n########################### get_evaluators(items_ids=None)\n########################### items_ids -> list of subset of item_id ","f1eccf19":"########################### Init Metric\n########################### https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/133834\n#################################################################################\nclass WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(self.group_ids):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight \/ lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def get_scale(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        return getattr(self, f'lv{lv}_scale')\n        \n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')       \n        return (score \/ scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            all_scores.append(lv_scores.sum())\n        if VERBOSE:\n            print(np.round(all_scores,3))\n        return np.mean(all_scores)\n\n    def full_score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            all_scores.append(lv_scores.sum())\n        print(np.round(all_scores,3))\n        return np.mean(all_scores)\n    \nclass WRMSSEForLightGBM(WRMSSEEvaluator):\n\n    def feval(self, preds, dtrain):\n        preds = preds.reshape(self.valid_df[self.valid_target_columns].shape, order='F') #.transpose()\n        score = self.score(preds)\n        return 'WRMSSE', score, False\n    \n    def full_feval(self, preds, dtrain):\n        preds = preds.reshape(self.valid_df[self.valid_target_columns].shape, order='F') #.transpose()\n        score = self.full_score(preds)\n        return 'WRMSSE', score, False\n    \n########################### Lgb evaluators\n#################################################################################\ndef get_evaluators(items_ids):\n    prices = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sell_prices.csv')\n    calendar = pd.read_csv('..\/input\/m5-forecasting-accuracy\/calendar.csv')\n    train_fold_df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\n    train_fold_df = train_fold_df[train_fold_df['item_id'].isin(items_ids)].reset_index(drop=True)\n\n    lgb_evaluators = []\n    for i in range(2):\n        temp_train = train_fold_df.iloc[:,:-28*(i+1)]\n        if i==0:\n            temp_valid = train_fold_df.iloc[:, -28*(i+1):]\n        else:\n            temp_valid = train_fold_df.iloc[:, -28*(i+1):-28*i]\n\n        lgb_evaluator = WRMSSEForLightGBM(temp_train, temp_valid, calendar, prices)\n        lgb_evaluators.append(lgb_evaluator)\n\n    del train_fold_df, temp_train, temp_valid, prices, calendar\n    return lgb_evaluators","23119bfa":"########################### Model\n#################################################################################\nimport lightgbm as lgb\nlgb_params = {\n                    'boosting_type': 'gbdt',                      \n                    'metric': ['rmse'],           \n                    'subsample': 0.5,                \n                    'subsample_freq': 1,\n                    'learning_rate': 0.05,           \n                    'num_leaves': 2**8-1,            \n                    'min_data_in_leaf': 2**8-1,     \n                    'feature_fraction': 0.8,\n                    'n_estimators': 1500,            \n                    'early_stopping_rounds': 30,     \n                    'seed': SEED,\n                    'verbose': -1,\n                } \n\ndef make_fast_test(df, block=0, loss_func='tweedie'):\n    df = df.copy()\n    local_params = lgb_params.copy()           \n\n    # To be sure that some features are categoricals\n    icols = ['item_id','dept_id','cat_id','store_id','state_id', \n             'event_name_1','event_type_1','event_name_2','event_type_2',]\n    for col in icols:\n        try:\n            df[col] = df[col].astype('category')\n        except:\n            pass\n    \n    # Our features\n    features_columns = [col for col in list(df) if col not in remove_features]\n    print(features_columns)\n    \n    train_mask = (df['t_block']<(df['t_block'].max()-block))\n    valid_mask = (df['t_block']==(df['t_block'].max()-block))\n    \n    train_data = lgb.Dataset(df[train_mask][features_columns], label=df[train_mask][TARGET])\n    valid_data = lgb.Dataset(df[valid_mask][features_columns], label=df[valid_mask][TARGET])\n    \n    print('Train time block', df[train_mask]['t_block'].max(), df[train_mask]['d'].min(), df[train_mask]['d'].max())\n    print('Valid time block', df[valid_mask]['t_block'].max(), df[valid_mask]['d'].min(), df[valid_mask]['d'].max())\n    \n    temp_df = df[valid_mask]\n    del df\n    \n    if loss_func=='custom':\n        seed_everything(SEED)\n        estimator = lgb.train(\n                                local_params,\n                                train_data,\n                                valid_sets = [valid_data],\n                                verbose_eval = 20,\n                                fobj = custom_loss, \n                                feval=lgb_evaluators[block].feval,\n                            )\n        \n    elif loss_func=='tweedie':\n        local_params['objective'] = 'tweedie'             \n        local_params['tweedie_variance_power'] = 1.1  \n        seed_everything(SEED)\n        estimator = lgb.train(\n                                local_params,\n                                train_data,\n                                valid_sets = [valid_data],\n                                verbose_eval = 20,\n                                feval=lgb_evaluators[block].feval,\n                            ) \n        \n    else:\n        local_params['objective'] = 'regression'             \n        seed_everything(SEED)\n        estimator = lgb.train(\n                                local_params,\n                                train_data,\n                                valid_sets = [valid_data],\n                                verbose_eval = 20,\n                                feval=lgb_evaluators[block].feval,\n                            )\n        \n    temp_df['preds'] = estimator.predict(temp_df[features_columns])\n    temp_df = temp_df[['id','d',TARGET,'preds']]\n    return estimator, temp_df","54a12c40":"########################### Make grid\n#################################################################################\ngrid_df = pd.concat([pd.read_pickle('..\/input\/m5-simple-fe\/grid_part_1.pkl'),\n                     pd.read_pickle('..\/input\/m5-simple-fe\/grid_part_3.pkl').iloc[:,2:]],\n                     axis=1)\ngrid_df['t_block'] = grid_df['d'].apply(lambda x: ceil((x+19)\/(28))).astype(np.int8)\ngrid_df = grid_df[grid_df['d']<=END_TRAIN]\ngrid_df = grid_df[grid_df['t_block']>(grid_df['t_block'].max()-24)].reset_index(drop=True) #last 2 years\nlgb_evaluators = get_evaluators(list(grid_df['item_id'].unique()))\ngc.collect()","4fa9b9f2":"########################### Unrealistic lag 1+\n#################################################################################\ngrouper = grid_df.groupby(['id'])[TARGET]\nfor i in range(1,8):\n    print(i)\n    grid_df['normal_lag_'+str(i)] = grouper.transform(lambda x: x.shift(i)).astype(np.float32)\n    \ndel grouper","4260089f":"########################### Make simple model with tweedie\n#################################################################################\nmodel_tweedie, preds_tweedie_df = make_fast_test(grid_df, 0, 'tweedie')\nprint(lgb_evaluators[0].full_feval(preds_tweedie_df['preds'].values, pd.DataFrame())) \nlgb.plot_importance(model_tweedie, figsize=(15,15))","34572e6b":"########################### Some custom loss here\n########################### The place to make own one\n#################################################################################\ndef custom_loss(y_pred, y_true):\n    grad = y_pred - y_true.get_label()\n    hess = np.ones_like(y_pred)\n    return grad, hess","db42a2b1":"########################### Make simple model with custom loss\n#################################################################################\nmodel_custom, preds_custom_df = make_fast_test(grid_df, 0, 'custom')\nprint(lgb_evaluators[0].full_feval(preds_custom_df['preds'].values, pd.DataFrame())) \nlgb.plot_importance(model_custom, figsize=(15,15))","24af349b":"########################### Lets test several folds\n#################################################################################\nall_preds = pd.DataFrame()\nfor i in [0,1]:\n    model_tweedie, preds_tweedie_df = make_fast_test(grid_df, i, 'tweedie')\n    all_preds = pd.concat([all_preds, preds_tweedie_df]).reset_index(drop=True) \n    print('-'*10)\n    \nall_preds","67313627":"########################### Lets test limited item ids\n#################################################################################\nkeep_id = np.array_split(list(grid_df['item_id'].unique()), 20)[0]\nlgb_evaluators = get_evaluators(keep_id)\ntemp_df = grid_df[grid_df['item_id'].isin(keep_id)]\n\nall_preds = pd.DataFrame()\nfor i in [0,1]:\n    model_tweedie, preds_tweedie_df = make_fast_test(temp_df, i, 'tweedie')\n    all_preds = pd.concat([all_preds, preds_tweedie_df]).reset_index(drop=True) \n    print('-'*10)\n    \nall_preds","ae1f39d6":"########################### Lets do better item_id split\n#################################################################################\nfrom sklearn.model_selection import train_test_split\n\nkeep_id,_ = train_test_split(list(grid_df['item_id'].unique()), test_size=0.7, random_state=SEED)\nlgb_evaluators = get_evaluators(keep_id)\ntemp_df = grid_df[grid_df['item_id'].isin(keep_id)]\n\nall_preds = pd.DataFrame()\nfor i in [0,1]:\n    model_tweedie, preds_tweedie_df = make_fast_test(temp_df, i, 'tweedie')\n    all_preds = pd.concat([all_preds, preds_tweedie_df]).reset_index(drop=True) \n    print('-'*10)\n    \nall_preds","5b3ffa28":"![https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F2405813%2F80a132ad861f29bb185658445b1c5425%2Fkaggle_5.jpg?generation=1590187236288048&alt=media](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F2405813%2F80a132ad861f29bb185658445b1c5425%2Fkaggle_5.jpg?generation=1590187236288048&alt=media)"}}