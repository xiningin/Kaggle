{"cell_type":{"1d1c1dec":"code","a711539e":"code","39bff570":"code","f8f3fba1":"code","738efac4":"code","7ce33a5e":"code","ff9587fb":"code","f67f498a":"code","f3129da1":"code","5a867c5f":"code","b67d4cb1":"code","eeed96bc":"code","1d7b4cf3":"code","ef380060":"code","98918ca4":"code","56e75844":"code","10b55374":"code","b9dd98cf":"code","d6c4b1aa":"code","97051b70":"code","ffcbb8bd":"code","cd2c876d":"code","d5992710":"code","3c3f5d37":"code","f9dd6033":"code","641fd695":"code","325ad246":"code","615e60dc":"code","ed81df93":"code","f2a5f915":"code","cc679b1f":"code","05f80e13":"code","9f76879f":"code","e829c934":"code","e2ae5d94":"code","4cd71317":"code","c3e0f8ea":"code","b7461956":"code","6c75a0d4":"code","326f580c":"code","d01ed29c":"code","6ccbec40":"code","ebc33b72":"code","e343eaeb":"code","a871e884":"code","98ed22e5":"code","c62d608d":"code","8dbc3f8e":"code","a5cc7952":"code","fef849cd":"code","92bc0466":"code","bdf2d4f1":"code","393f7f28":"code","600f6c8f":"code","e394668f":"code","256325ed":"code","a4f0cd26":"code","9a0e467b":"code","f34d6b7d":"code","1cd9b25c":"code","481578fa":"code","1fa5a236":"code","a10844ec":"code","3d9b1d84":"markdown","ecc66547":"markdown"},"source":{"1d1c1dec":"!pip install -q -U pip\n!pip install -q -U opencv-python\n!pip install -q tensorflow\n!pip install -q keras\n!pip install -q scikit-learn\n!pip install -q seaborn\n!pip install --quiet vit-keras","a711539e":"import tensorflow as tf\nimport keras\n\nfrom vit_keras import vit\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, jaccard_score\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom scipy import stats\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPool2D, UpSampling2D, GlobalMaxPool2D, GlobalAveragePooling2D, Conv2DTranspose, concatenate\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, Flatten, Input\nfrom tensorflow.keras.models import Model, load_model\n\n# from tensorflow.keras.applications.resnet50 import ResNet50\n# from tensorflow.keras.applications import InceptionV3, InceptionResNetV2, DenseNet201\n# from tensorflow.keras.applications import densenet, inception_v3, inception_resnet_v2, resnet50\n\nfrom tensorflow.keras.preprocessing.image import load_img\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import metrics\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tqdm.notebook import tqdm\nfrom tqdm import tqdm, notebook\nfrom datetime import datetime\n\nimport numpy as np\nimport os\nimport cv2\nimport pandas as pd\n# import imutils\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt","39bff570":"!pip install gdown","f8f3fba1":"tf.config.list_physical_devices()","738efac4":"# !gdown --id 1xKv6x4hkyT9xsdqynlzIKM2XEpP9i_iW","7ce33a5e":"# !mkdir Meeting1\/Data\/wazzadu_images\/\n# !unzip wazzadu_images.zi\u0e22","ff9587fb":"# !unzip wazzadu.zip","f67f498a":"dim_cat_subcat = pd.read_csv(\"dim_cat_subcat_tag_key.csv\", header = 1)\ndim_cat_subcat.head()\n\ntrain = pd.read_csv(\"train.csv\")\ntrain['kaggle_path'] = [os.path.basename(os.path.normpath(path)) for path in (list(train.image_path))]\ntrain['kaggle_path'] = 'wazzadu_images\/' + train['kaggle_path']\n\ntrain = train.merge(dim_cat_subcat, how = 'left', \n                    left_on=['category_id', 'subcategory_id', 'tag_name'], \n                    right_on=[\"cate_id\", \"subcate_id\", \"tag\"]).iloc[:,[0,1,2,3,4,5,6,7,8,14]]\n\ntrain.head()","f3129da1":"pd.concat([train,pd.get_dummies(train.key)],axis=1)","5a867c5f":"list_model = [\n        \n#     {'model':vit.vit_b32(image_size = 224, activation = 'softmax', pretrained = True, include_top = False, pretrained_top = False,\n#                          classes = 24),\n#      'preprocessing':vit.preprocess_inputs,\n#      'size':(224,224),\n#      'name':'VisionTransformer_b32'},\n    \n    {'model':vit.vit_b16(image_size = 384, activation = 'softmax', pretrained = True, include_top = False, pretrained_top = False,\n                         classes = 24),\n     'preprocessing':vit.preprocess_inputs,\n     'size':(384,384),\n     'name':'VisionTransformer_b16'}\n    \n#     {'model':vit.vit_l16(image_size = 384, activation = 'softmax', pretrained = True, include_top = False, pretrained_top = False,\n#                          classes = 24),\n#      'preprocessing':vit.preprocess_inputs,\n#      'size':(384,384),\n#      'name':'VisionTransformer_l16'}\n]","b67d4cb1":"with tf.device('\/device:GPU:0'):\n    def get_model(index, list_model=list_model, dropout=0.2):\n        basemodel = list_model[index]['model']\n        preprocessing = list_model[index]['preprocessing']\n        size = list_model[index]['size']\n        \n        inputs = basemodel.input\n        \n        for layer in basemodel.layers:\n            layer.trainable = False\n        \n        x = basemodel(inputs)\n        x = Flatten()(x)\n        x = BatchNormalization()(x)\n        \n#         x = Dense(512)(x)\n\n\n#         x = Dropout(dropout)(x)\n#         x = Activation('gelu')(x)\n#         x = BatchNormalization()(x)\n        \n#         x = Dense(512)(x)\n#         x = Dropout(dropout)(x)\n#         x = Activation('gelu')(x)\n#         x = BatchNormalization()(x)\n        \n        x = Dense(256)(x)\n        x = Dropout(dropout)(x)\n        x = Activation('gelu')(x)\n        x = BatchNormalization()(x)\n        \n        x = Dense(128)(x)\n        x = Dropout(dropout)(x)\n        x = Activation('gelu')(x)\n        x = BatchNormalization()(x)\n        \n        x = Dense(24)(x)\n        x = Dropout(dropout)(x)\n        x = Activation('softmax')(x)\n        \n        outputs = x\n        \n        model = Model(inputs=inputs, outputs=outputs)\n        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n        return model, preprocessing, size\n    \nget_model(0)[0].summary()","eeed96bc":"def preprocessing_batch_size(X, preprocessing, batch_size):\n    new_X = None\n    used_X = False\n    size = len(X)\n    for i in tqdm(range(0,size,batch_size)):\n        start = i\n        end = i+batch_size if i+batch_size <= size else size\n        if not used_X:\n            new_X = preprocessing(X[start:end])\n            used_X = True\n        else:\n            new_X = np.append(new_X, preprocessing(X[start:end]), axis=0)\n        \n    new_X = np.array(new_X)\n    return new_X\n\ndef predict_batch_size(X, model, batch_size):\n    new_X = None\n    used_X = False\n    size = len(X)\n    for i in tqdm(range(0,size,batch_size)):\n        start = i\n        end = i+batch_size if i+batch_size <= size else size\n        if not used_X:\n            new_X = model.predict(X[start:end])\n            used_X = True\n        else:\n            new_X = np.append(new_X, model.predict(X[start:end]), axis=0)\n        \n    new_X = np.array(new_X)\n    return new_X\n\ndef make_border(crop_img):\n    scale_x,scale_y, c = crop_img.shape\n    bottom = scale_y - scale_x if scale_y - scale_x >= 0 else 0\n    right = scale_x - scale_y if scale_x - scale_y >= 0 else 0\n    crop_img = cv2.copyMakeBorder(crop_img, 0, bottom, 0, right, cv2.BORDER_DEFAULT)\n    \n    return crop_img","1d7b4cf3":"list_image = []\nlist_key = []\n\nfor index, value in tqdm(train.iterrows(), total=train.shape[0]):\n    path = value['kaggle_path']\n    key = value['key']\n    \n    img = np.array(load_img(path))\n    img = make_border(img)\n#     print(abs(img.shape[0]-img.shape[1])\/max(img.shape[0],img.shape[1]))\n    img = cv2.resize(img, (384,384))\n    img = np.array(img)\n    \n#     plt.imshow(img); plt.show()\n    \n#     if index == 10:\n#         break\n    \n    list_key.append(key)\n    list_image.append(img)\n    \nlist_image = np.array(list_image)\n# list_image.shape","ef380060":"batch_size = 3000\n\nmodel, preprocessing, (x_size, y_size) = get_model(0)\n\nX = np.array([cv2.resize(i, (x_size, y_size)) for i in list_image])\nX = preprocessing_batch_size(X, preprocessing, batch_size)\n\nle = LabelEncoder()\ny =le.fit_transform(list_key)\n\nX.shape, y.shape","98918ca4":"# batch_size = 5000\n\n# for i in range(len(list_model)):\n#     model, preprocessing, (x_size, y_size) = get_model(i)\n\n#     X = np.array([cv2.resize(j, (x_size, y_size)) for j in list_image])\n#     X = preprocessing_batch_size(X, preprocessing, batch_size)\n#     y_pred = predict_batch_size(X, model, batch_size)\n\n#     model_name = list_model[i]['name']\n#     np.save('Predicted_Pretrained_'+str(model_name), y_pred)","56e75844":"import tensorflow as tf\nimport keras\n\nfrom vit_keras import vit\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, jaccard_score\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom scipy import stats\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPool2D, UpSampling2D, GlobalMaxPool2D, GlobalAveragePooling2D, Conv2DTranspose, concatenate\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, Flatten, Input\nfrom tensorflow.keras.models import Model, load_model\n\n# from tensorflow.keras.applications.resnet50 import ResNet50\n# from tensorflow.keras.applications import InceptionV3, InceptionResNetV2, DenseNet201\n# from tensorflow.keras.applications import densenet, inception_v3, inception_resnet_v2, resnet50\n\nfrom tensorflow.keras.preprocessing.image import load_img\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import metrics\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tqdm.notebook import tqdm\nfrom tqdm import tqdm, notebook\nfrom datetime import datetime\n\nimport numpy as np\nimport os\nimport cv2\nimport pandas as pd\n# import imutils\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt","10b55374":"list_model = [\n        \n#     {'model':vit.vit_b32(image_size = 224, activation = 'softmax', pretrained = True, include_top = False, pretrained_top = False,\n#                          classes = 24),\n#      'preprocessing':vit.preprocess_inputs,\n#      'size':(224,224),\n#      'name':'VisionTransformer_b32'},\n    \n    {'model':vit.vit_b16(image_size = 384, activation = 'softmax', pretrained = True, include_top = False, pretrained_top = False,\n                         classes = 24),\n     'preprocessing':vit.preprocess_inputs,\n     'size':(384,384),\n     'name':'VisionTransformer_b16'}\n    \n#     {'model':vit.vit_l16(image_size = 384, activation = 'softmax', pretrained = True, include_top = False, pretrained_top = False,\n#                          classes = 24),\n#      'preprocessing':vit.preprocess_inputs,\n#      'size':(384,384),\n#      'name':'VisionTransformer_l16'}\n]","b9dd98cf":"with tf.device('\/device:GPU:0'):\n    def get_model(index, list_model=list_model, dropout=0.2):\n        basemodel = list_model[index]['model']\n        preprocessing = list_model[index]['preprocessing']\n        size = list_model[index]['size']\n        \n        inputs = basemodel.input\n        \n        for layer in basemodel.layers:\n            layer.trainable = False\n        \n        x = basemodel(inputs)\n        x = Flatten()(x)\n        x = BatchNormalization()(x)\n        \n#         x = Dense(512)(x)\n#         x = Dropout(dropout)(x)\n#         x = Activation('gelu')(x)\n#         x = BatchNormalization()(x)\n        \n#         x = Dense(512)(x)\n#         x = Dropout(dropout)(x)\n#         x = Activation('gelu')(x)\n#         x = BatchNormalization()(x)\n        \n        x = Dense(256)(x)\n        x = Dropout(dropout)(x)\n        x = Activation('gelu')(x)\n        x = BatchNormalization()(x)\n        \n        x = Dense(128)(x)\n        x = Dropout(dropout)(x)\n        x = Activation('gelu')(x)\n        x = BatchNormalization()(x)\n        \n        x = Dense(24)(x)\n        x = Dropout(dropout)(x)\n        x = Activation('softmax')(x)\n        \n        outputs = x\n        \n        model = Model(inputs=inputs, outputs=outputs)\n        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n        return model, preprocessing, size\n    \nget_model(0)[0].summary()","d6c4b1aa":"learning_rate = 1e-4\n\n# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n#                                                  factor = 0.2,\n#                                                  patience = 2,\n#                                                  verbose = 1,\n#                                                  min_delta = 1e-4,\n#                                                  min_lr = 1e-6,\n#                                                  mode = 'max')\n\n# earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n#                                                  min_delta = 1e-4,\n#                                                  patience = 5,\n#                                                  mode = 'max',\n#                                                  restore_best_weights = True,\n#                                                  verbose = 1)\n\ncheckpointer = tf.keras.callbacks.ModelCheckpoint(filepath = '.\/model.hdf5',\n                                                  monitor = 'accuracy', \n                                                  verbose = 1, \n                                                  save_best_only = True,\n                                                  save_weights_only = True,\n                                                  mode = 'max')\n\ncallbacks = [checkpointer]\n\n","97051b70":"# # np.save('X_224_border', X)\n# # np.save('y_224_border', y)\n# np.save('X_384_border', X)\n# np.save('y_384_border', y)","ffcbb8bd":"# X = np.load('X_224_border.npy')\n# y = np.load('y_224_border.npy')\nX = np.load('X_384_border.npy')\ny = np.load('y_384_border.npy')","cd2c876d":"X.shape, y.shape","d5992710":"####\n# list_key to labalencoder \n####\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","3c3f5d37":"epochs = 25\nbatch_train = 128\n\nmodel, preprocessing, (x_size, y_size) = get_model(0)\n\nhistory = model.fit(X, y,\n                    batch_size = batch_train,\n#                     validation_data = (X_test, y_test),\n                    epochs = epochs,\n                    callbacks = callbacks\n                   )\n\nmodel.save('model-test-gpu-7-border-gelu-256-128-no-trainable')","f9dd6033":"# model.save('model-test-gpu-7-border-gelu-256-128-no-trainable.h5')","641fd695":"model = get_model(0)[0]\n\nmodel.summary()","325ad246":"model, preprocessing, _ = get_model(0)\n\nmodel.load_weights('.\/model-test-gpu-7-border-gelu-256-128-no-trainable.h5')","615e60dc":"model.summary()","ed81df93":"model.evaluate(X,y, batch_size = 64)","f2a5f915":"model.","cc679b1f":"epochs = 60\nbatch_train = 64\n\nmodel, preprocessing, (x_size, y_size) = get_model(0)\n\nhistory = model.fit(X, y,\n                    batch_size = batch_train,\n#                     validation_data = (X_test, y_test),\n                    epochs = epochs,\n#                     callbacks = callbacks\n                   )\n\nmodel.save('model-gpu-7-border-gelu-256-128-no-trainable')","05f80e13":"\n# le.inverse_transform(y_test)","9f76879f":"predictions_X_test = model.predict(X[x_test_ids], batch_size = batch_train)\npredictions_X = model.predict(X, batch_size = batch_train)\n\nnp.save('Keys-name-model-3',le.inverse_transform(y_test))\nnp.save('predictions-model-3-y', predictions_X_test)\nnp.save('predictions-model-3', predictions_X)","e829c934":"\nprint(classification_report(np.argmax(predictions_X_test, axis = 1), y_test))","e2ae5d94":"import numpy as np\nimport pandas as pd\n\ny = np.load('y.npy')\npd.Series(y).value_counts()\n\nkeys = np.load('key_pretrained_1.npy')\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(keys)\nnp.all(le.inverse_transform(y) == keys)\n\n\n# pd.Series(keys).value_counts()","4cd71317":"%%time\n# np.save('X', X)\n# np.save('y', y)\n\nX = np.load('X.npy')\ny = np.load('y.npy')\n\n# this is about 40G, 1 minute to load\n\nX = np.array([cv2.resize(i, (224,224)) for i in X])\n\nX.shape, y.shape","c3e0f8ea":"list_model = [\n        \n#     {'model':vit.vit_b32(image_size = 224, activation = 'softmax', pretrained = True, include_top = False, pretrained_top = False,\n#                          classes = 24),\n#      'preprocessing':vit.preprocess_inputs,\n#      'size':(224,224),\n#      'name':'VisionTransformer_b32'},\n    \n    {'model':vit.vit_b16(image_size = 224, activation = 'softmax', pretrained = True, include_top = False, pretrained_top = False,\n                         classes = 24),\n     'preprocessing':vit.preprocess_inputs,\n     'size':(224,224),\n     'name':'VisionTransformer_b16'}\n    \n#     {'model':vit.vit_l16(image_size = 384, activation = 'softmax', pretrained = True, include_top = False, pretrained_top = False,\n#                          classes = 24),\n#      'preprocessing':vit.preprocess_inputs,\n#      'size':(384,384),\n#      'name':'VisionTransformer_l16'}\n]","b7461956":"# from sklearn.model_selection import train_test_split\n\n# x_ids = list(range(len(X)))\n# x_train_ids, x_test_ids, y_train, y_test = train_test_split(x_ids, y, test_size = 0.2, random_state=42)\n","6c75a0d4":"with tf.device('\/device:GPU:0'):\n    def get_model(index, list_model=list_model, dropout=0.2):\n        basemodel = list_model[index]['model']\n        preprocessing = list_model[index]['preprocessing']\n        size = list_model[index]['size']\n        \n        inputs = basemodel.input\n        \n        for layer in basemodel.layers:\n            layer.trainable = False\n        \n        x = basemodel(inputs)\n        x = Flatten()(x)\n        x = BatchNormalization()(x)\n        \n#         x = Dense(512)(x)\n#         x = Dropout(dropout)(x)\n#         x = Activation('gelu')(x)\n#         x = BatchNormalization()(x)\n        \n#         x = Dense(512)(x)\n#         x = Dropout(dropout)(x)\n#         x = Activation('gelu')(x)\n#         x = BatchNormalization()(x)\n        \n        x = Dense(256)(x)\n        x = Dropout(dropout)(x)\n        x = Activation('gelu')(x)\n        x = BatchNormalization()(x)\n        \n        x = Dense(128)(x)\n        x = Dropout(dropout)(x)\n        x = Activation('gelu')(x)\n        x = BatchNormalization()(x)\n        \n#         x = Dense(64)(x)\n#         x = Dropout(dropout)(x)\n#         x = Activation('gelu')(x)\n#         x = BatchNormalization()(x)\n        \n        x = Dense(24)(x)\n        x = Dropout(dropout)(x)\n        x = Activation('softmax')(x)\n        \n        outputs = x\n        \n        model = Model(inputs=inputs, outputs=outputs)\n        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n        return model, preprocessing, size\n    \nget_model(0)[0].summary()","326f580c":"model, preprocessing, _ = get_model(0)\n\nmodel = load_model('model-gpu-6-all-35-gelu-256-128-64-no-trainable')\n\n# model.load_weights('.\/model-test-gpu-7-border-gelu-256-128-no-trainable.h5')","d01ed29c":"model, preprocessing, (x_size, y_size) = get_model(0)\n\nlearning_rate = 1e-4\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n                                                 factor = 0.2,\n                                                 patience = 2,\n                                                 verbose = 1,\n                                                 min_delta = 1e-4,\n                                                 min_lr = 1e-6,\n                                                 mode = 'max')\n\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n                                                 min_delta = 1e-4,\n                                                 patience = 5,\n                                                 mode = 'max',\n                                                 restore_best_weights = True,\n                                                 verbose = 1)\n\ncheckpointer = tf.keras.callbacks.ModelCheckpoint(filepath = '.\/model.hdf5',\n                                                  monitor = 'val_accuracy', \n                                                  verbose = 1, \n                                                  save_best_only = True,\n                                                  save_weights_only = True,\n                                                  mode = 'max')\ncallbacks = [reduce_lr, earlystopping, checkpointer]","6ccbec40":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)","ebc33b72":"epochs = 20\nbatch_train = 256\n\nhistory = model.fit(X_train, \n                    y_train,\n                    batch_size = batch_train,\n                    validation_data = (X_test, y_test),\n                    epochs = epochs,\n                    callbacks = callbacks\n                   )\n\n# model.save('model-gpu-8-fuckshit.h5')","e343eaeb":"epochs = 10\nbatch_train = 128\n\nhistory = model.fit(X, \n                    y,\n                    batch_size = batch_train,\n#                     validation_data = (X[x_test_ids], y_test),\n                    epochs = epochs,\n#                     callbacks = callbacks\n                   )\n\nmodel.save('model-gpu-8-fuckshit.h5')","a871e884":"model_path = '.\/model-gpu-6-all-35-gelu-256-128-64-no-trainable'\n# model_path = '.\/model-gpu-5-gelu-256-128-no-trainable'\n# model_path = 'model-gpu-3-relu-256-no-trainable'\nmodel = load_model(model_path)\n\nmodel.summary()","98ed22e5":"import cv2\nimport matplotlib.pyplot as plt\nfrom skimage import io\nimport pandas as pd\nfrom tqdm import tqdm\nimport requests","c62d608d":"df_test = pd.read_csv('.\/Predict\/test.csv')\ndf_test.head()","8dbc3f8e":"df_test['url'][0].split('\/')","a5cc7952":"# plt.imshow(cv2.resize(img, (384, 3)))\nimg = test_img_list[13]\nscale_x, scale_y, _ = img.shape\nbottom = scale_y - scale_x if scale_y - scale_x >= 0 else 0\nright = scale_x - scale_y if scale_x - scale_y >= 0 else 0\nplt.imshow(cv2.copyMakeBorder(img, 0, bottom, 0, right, cv2.BORDER_DEFAULT))","fef849cd":"plt.imshow(test_img_list[147])","92bc0466":"plt.imshow(test_img_list[50][int(tl_y * scale_y):int(bl_y * scale_y), int(tl_x * scale_x):int(tr_x * scale_x)] )","bdf2d4f1":"rows['url'].split('\/')","393f7f28":"test_img_list = []\n\n\nfor idx, rows in tqdm(df_test.iterrows(), total = df_test.shape[0]):\n    img = io.imread('.\/Predict\/pic\/' + rows['url'].split('\/')[8])\n    scale_x,scale_y, c = img.shape\n#     plt.imshow(img); plt.show()\n    \n    tl_y, bl_y, tl_x, tr_x = rows['tl_y'], rows['bl_y'], rows['tl_x'], rows['tr_x']\n    try:\n        crop_img = img[int(tl_y * scale_y):int(bl_y * scale_y), int(tl_x * scale_x):int(tr_x * scale_x)] \n        crop_img = cv2.resize(crop_img, (384, 384))\n    except:\n        crop_img = img\n        crop_img = cv2.resize(crop_img, (384, 384))\n        \n#     if idx == 147: print(rows['url'].split('\/')[8])\n#     plt.imshow(crop_img); plt.show()\n\n#     crop_img = make_border(crop_img)\n    # make square\n#     max_dimension = max(scale_x, scale_y)\n    \n#     plt.imshow(crop_img); plt.show()\n\n    test_img_list.append(crop_img)\n\n    \n# test_img_list = np.array([cv2.resize(i, (384, 384)) for i in test_img_list])\ntest_img_list = np.array(test_img_list)","600f6c8f":"list_key = np.load('key_pretrained_1.npy')\n\nle = LabelEncoder()\ny = le.fit_transform(list_key)\ny","e394668f":"test_img_list = preprocessing(test_img_list)","256325ed":"predictions = le.inverse_transform(np.argmax(model.predict(test_img_list), axis = 1))\n","a4f0cd26":"df = pd.DataFrame({'id': range(1,1218), 'Answer':pd.Series(predictions.tolist())})\n# df = df.reindex(range(1,1218))\n# df = df.reset_index()\n# df['id'] = list(range(1,1218))\n\n# df.columns = ['id','Answer']\n# df = df.set_index('id')\n# df = df.reindex(range(1,1218))\n# df.loc[12:,:]=None\n# df = df.reset_index()\ndf.head()","9a0e467b":"df.to_csv('submissions_13.csv', index = False)","f34d6b7d":"for i, img in zip(predictions, test_img_list):\n    print(i)\n    plt.imshow(img); plt.show()","1cd9b25c":"np.argmax(model.predict(test_img_list), axis = 1)","481578fa":"img.shape","1fa5a236":"fig, ax = plt.subplots(figsize = (12,8))\npd.DataFrame(history.history).iloc[:,:4].plot(title = 'Metrics over Time (B\/16)', ax = ax, xticks = range(20), ylim = (0.2, 2))\nplt.show()","a10844ec":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")","3d9b1d84":"## Loading and Training","ecc66547":"## Predictions"}}