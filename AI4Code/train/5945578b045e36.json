{"cell_type":{"16703ea5":"code","fea457c1":"code","ea3c59b4":"code","a35d6dff":"code","702fc54f":"code","91202779":"code","6a75d9cf":"code","1636bf4d":"code","6f30587e":"code","d6568283":"code","9a9581ab":"code","dbe983f5":"code","2faae211":"code","9536da55":"code","87d370b6":"code","6819a0f6":"code","4625a802":"code","bbc2c805":"code","fae98a78":"code","60549041":"code","61ad79c9":"code","747fad20":"code","f7b017b4":"code","68e1c1e9":"code","330634df":"code","dfbcaa11":"code","3c368a61":"code","1775f0d5":"code","df4a29c8":"code","2c7d48ff":"code","25e1f5b9":"code","061a0791":"code","1a20bbd5":"code","23af95d2":"code","fd480ebe":"code","539bae7b":"code","d4d3be18":"code","d6179d17":"code","6b16ae26":"code","4e1c9d64":"code","822ed3fb":"code","14a7b67b":"code","d29ca4f0":"code","41b0bd22":"code","21c2c245":"code","73436235":"code","2393eb55":"code","96ca0c93":"code","29fe255a":"code","dedfe3aa":"code","4814b982":"code","af492c48":"code","b4d94eaf":"code","512de683":"code","00398ff9":"code","de8e2cdc":"code","218361b8":"code","49da2678":"code","29e305cf":"code","189eebe3":"markdown","b7b73baa":"markdown","4dc3a7c3":"markdown","933a54f7":"markdown","1f6e2ccd":"markdown","7ead07f4":"markdown","31c63944":"markdown","6e60b50c":"markdown","3af5c668":"markdown","a4112ff4":"markdown","bc629bce":"markdown","d629b1a8":"markdown","78f3c837":"markdown","84f14a82":"markdown","2f12c95f":"markdown","caf9b57c":"markdown","6195f71e":"markdown","eaa2c907":"markdown","628ede94":"markdown","d7acfe59":"markdown","af31979a":"markdown","f913cfa4":"markdown","fc93f2a3":"markdown","8e6ef786":"markdown","dc05d28b":"markdown","0a99c62a":"markdown","95f882c7":"markdown","511dbaac":"markdown","8fdf6c1f":"markdown","8e12a83c":"markdown","61a8a0b8":"markdown","6d908827":"markdown","ad3ae60e":"markdown","eba58edf":"markdown","28901309":"markdown","e269d07b":"markdown","9bde627b":"markdown","f8c9ce31":"markdown","5132d652":"markdown","b730fbda":"markdown","49900546":"markdown","cc20095e":"markdown","8677405e":"markdown","e7f970ad":"markdown","16be6bf5":"markdown","4f2819a0":"markdown","1a3ed51b":"markdown","12353fad":"markdown","a35bc58a":"markdown","c77c7ceb":"markdown"},"source":{"16703ea5":"import numpy as np \nimport pandas as pd \n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set_style(\"whitegrid\")\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os \nprint(os.listdir(\"..\/input\"))","fea457c1":"training = pd.read_csv(\"..\/input\/train.csv\")\ntesting = pd.read_csv(\"..\/input\/test.csv\")","ea3c59b4":"training.head()","a35d6dff":"testing.head()","702fc54f":"print(training.keys())\nprint(testing.keys())","91202779":"types_train = training.dtypes\nnum_values = types_train[(types_train == float)]\n\nprint(\"These are the numerical features:\")\nprint(num_values)","6a75d9cf":"training.describe()","1636bf4d":"def null_table(training, testing):\n    print(\"Training Data Frame\")\n    print(pd.isnull(training).sum()) \n    print(\" \")\n    print(\"Testing Data Frame\")\n    print(pd.isnull(testing).sum())\n\nnull_table(training, testing)","6f30587e":"training.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\ntesting.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\n\nnull_table(training, testing)","d6568283":"copy = training.copy()\ncopy.dropna(inplace = True)\nsns.distplot(copy[\"Age\"])","9a9581ab":"#the median will be an acceptable value to place in the NaN cells\ntraining[\"Age\"].fillna(training[\"Age\"].median(), inplace = True)\ntesting[\"Age\"].fillna(testing[\"Age\"].median(), inplace = True) \ntraining[\"Embarked\"].fillna(\"S\", inplace = True)\ntesting[\"Fare\"].fillna(testing[\"Fare\"].median(), inplace = True)\n\nnull_table(training, testing)","dbe983f5":"training.head()","2faae211":"testing.head()","9536da55":"#can ignore the testing set for now\nsns.barplot(x=\"Sex\", y=\"Survived\", data=training)\nplt.title(\"Distribution of Survival based on Gender\")\nplt.show()\n\ntotal_survived_females = training[training.Sex == \"female\"][\"Survived\"].sum()\ntotal_survived_males = training[training.Sex == \"male\"][\"Survived\"].sum()\n\nprint(\"Total people survived is: \" + str((total_survived_females + total_survived_males)))\nprint(\"Proportion of Females who survived:\") \nprint(total_survived_females\/(total_survived_females + total_survived_males))\nprint(\"Proportion of Males who survived:\")\nprint(total_survived_males\/(total_survived_females + total_survived_males))","87d370b6":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Distribution of Survival Based on Class\")\nplt.show()\n\ntotal_survived_one = training[training.Pclass == 1][\"Survived\"].sum()\ntotal_survived_two = training[training.Pclass == 2][\"Survived\"].sum()\ntotal_survived_three = training[training.Pclass == 3][\"Survived\"].sum()\ntotal_survived_class = total_survived_one + total_survived_two + total_survived_three\n\nprint(\"Total people survived is: \" + str(total_survived_class))\nprint(\"Proportion of Class 1 Passengers who survived:\") \nprint(total_survived_one\/total_survived_class)\nprint(\"Proportion of Class 2 Passengers who survived:\")\nprint(total_survived_two\/total_survived_class)\nprint(\"Proportion of Class 3 Passengers who survived:\")\nprint(total_survived_three\/total_survived_class)","6819a0f6":"sns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")","4625a802":"sns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")","bbc2c805":"survived_ages = training[training.Survived == 1][\"Age\"]\nnot_survived_ages = training[training.Survived == 0][\"Age\"]\nplt.subplot(1, 2, 1)\nsns.distplot(survived_ages, kde=False)\nplt.axis([0, 100, 0, 100])\nplt.title(\"Survived\")\nplt.ylabel(\"Proportion\")\nplt.subplot(1, 2, 2)\nsns.distplot(not_survived_ages, kde=False)\nplt.axis([0, 100, 0, 100])\nplt.title(\"Didn't Survive\")\nplt.subplots_adjust(right=1.7)\nplt.show()","fae98a78":"sns.stripplot(x=\"Survived\", y=\"Age\", data=training, jitter=True)","60549041":"sns.pairplot(training)","61ad79c9":"training.sample(5)","747fad20":"testing.sample(5)","f7b017b4":"set(training[\"Embarked\"])","68e1c1e9":"from sklearn.preprocessing import LabelEncoder\n\nle_sex = LabelEncoder()\nle_sex.fit(training[\"Sex\"])\n\nencoded_sex_training = le_sex.transform(training[\"Sex\"])\ntraining[\"Sex\"] = encoded_sex_training\nencoded_sex_testing = le_sex.transform(testing[\"Sex\"])\ntesting[\"Sex\"] = encoded_sex_testing\n\nle_embarked = LabelEncoder()\nle_embarked.fit(training[\"Embarked\"])\n\nencoded_embarked_training = le_embarked.transform(training[\"Embarked\"])\ntraining[\"Embarked\"] = encoded_embarked_training\nencoded_embarked_testing = le_embarked.transform(testing[\"Embarked\"])\ntesting[\"Embarked\"] = encoded_embarked_testing\n\n#Here's how to do it manually in Python without packages\n\"\"\"\ntraining.loc[training[\"Sex\"] == \"male\", \"Sex\"] = 0\ntraining.loc[training[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntraining.loc[training[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntraining.loc[training[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntraining.loc[training[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n\ntesting.loc[testing[\"Sex\"] == \"male\", \"Sex\"] = 0\ntesting.loc[testing[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntesting.loc[testing[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntesting.loc[testing[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntesting.loc[testing[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n\"\"\"","330634df":"training.sample(5)","dfbcaa11":"testing.sample(5)","3c368a61":"training[\"FamSize\"] = training[\"SibSp\"] + training[\"Parch\"] + 1\ntesting[\"FamSize\"] = testing[\"SibSp\"] + testing[\"Parch\"] + 1","1775f0d5":"training[\"IsAlone\"] = training.FamSize.apply(lambda x: 1 if x == 1 else 0)\ntesting[\"IsAlone\"] = testing.FamSize.apply(lambda x: 1 if x == 1 else 0)","df4a29c8":"for name in training[\"Name\"]:\n    training[\"Title\"] = training[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n    \nfor name in testing[\"Name\"]:\n    testing[\"Title\"] = testing[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)","2c7d48ff":"training.head() #Title column added","25e1f5b9":"titles = set(training[\"Title\"]) #making it a set gets rid of all duplicates\nprint(titles)","061a0791":"title_list = list(training[\"Title\"])\nfrequency_titles = []\n\nfor i in titles:\n    frequency_titles.append(title_list.count(i))\n    \nprint(frequency_titles)","1a20bbd5":"titles = list(titles)\n\ntitle_dataframe = pd.DataFrame({\n    \"Titles\" : titles,\n    \"Frequency\" : frequency_titles\n})\n\nprint(title_dataframe)","23af95d2":"title_replacements = {\"Mlle\": \"Other\", \"Major\": \"Other\", \"Col\": \"Other\", \"Sir\": \"Other\", \"Don\": \"Other\", \"Mme\": \"Other\",\n          \"Jonkheer\": \"Other\", \"Lady\": \"Other\", \"Capt\": \"Other\", \"Countess\": \"Other\", \"Ms\": \"Other\", \"Dona\": \"Other\"}\n\ntraining.replace({\"Title\": title_replacements}, inplace=True)\ntesting.replace({\"Title\": title_replacements}, inplace=True)\n\nle_title = LabelEncoder()\nle_title.fit(training[\"Title\"])\n\nencoded_title_training = le_title.transform(training[\"Title\"])\ntraining[\"Title\"] = encoded_title_training\nencoded_title_testing = le_title.transform(testing[\"Title\"])\ntesting[\"Title\"] = encoded_title_testing\n\n#Again, here's how to do it manually\n\"\"\"\ntraining.loc[training[\"Title\"] == \"Miss\", \"Title\"] = 0\ntraining.loc[training[\"Title\"] == \"Mr\", \"Title\"] = 1\ntraining.loc[training[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntraining.loc[training[\"Title\"] == \"Master\", \"Title\"] = 3\ntraining.loc[training[\"Title\"] == \"Dr\", \"Title\"] = 4\ntraining.loc[training[\"Title\"] == \"Rev\", \"Title\"] = 5\ntraining.loc[training[\"Title\"] == \"Other\", \"Title\"] = 6\n\ntesting.loc[testing[\"Title\"] == \"Miss\", \"Title\"] = 0\ntesting.loc[testing[\"Title\"] == \"Mr\", \"Title\"] = 1\ntesting.loc[testing[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntesting.loc[testing[\"Title\"] == \"Master\", \"Title\"] = 3\ntesting.loc[testing[\"Title\"] == \"Dr\", \"Title\"] = 4\ntesting.loc[testing[\"Title\"] == \"Rev\", \"Title\"] = 5\ntesting.loc[testing[\"Title\"] == \"Other\", \"Title\"] = 6\n\"\"\"","fd480ebe":"training.drop(\"Name\", axis = 1, inplace = True)\ntesting.drop(\"Name\", axis = 1, inplace = True)","539bae7b":"training.sample(5)","d4d3be18":"testing.sample(5)","d6179d17":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n#We need to reshape our data since the Scaler takes in arrays\nages_train = np.array(training[\"Age\"]).reshape(-1, 1)\nfares_train = np.array(training[\"Fare\"]).reshape(-1, 1)\nages_test = np.array(testing[\"Age\"]).reshape(-1, 1)\nfares_test = np.array(testing[\"Fare\"]).reshape(-1, 1)\n\ntraining[\"Age\"] = scaler.fit_transform(ages_train)\ntraining[\"Fare\"] = scaler.fit_transform(fares_train)\ntesting[\"Age\"] = scaler.fit_transform(ages_test)\ntesting[\"Fare\"] = scaler.fit_transform(fares_test)\n\n#You can try with MinMaxScaler as well to see how it performs in comparison, just replace StandardScaler with MinMaxScaler","6b16ae26":"training.head()","4e1c9d64":"testing.head()","822ed3fb":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","14a7b67b":"from sklearn.metrics import make_scorer, accuracy_score ","d29ca4f0":"from sklearn.model_selection import GridSearchCV","41b0bd22":"X_train = training.drop(labels=[\"PassengerId\", \"Survived\"], axis=1) #define training features set\ny_train = training[\"Survived\"] #define training label set\nX_test = testing.drop(\"PassengerId\", axis=1) #define testing features set\n#we don't have y_test, that is what we're trying to predict with our model","21c2c245":"X_train.head()","73436235":"from sklearn.model_selection import train_test_split #to create validation data set\n\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets","2393eb55":"svc_clf = SVC() \n\nparameters_svc = {\"kernel\": [\"rbf\", \"linear\"], \"probability\": [True, False], \"verbose\": [True, False]}\n\ngrid_svc = GridSearchCV(svc_clf, parameters_svc, scoring=make_scorer(accuracy_score))\ngrid_svc.fit(X_training, y_training)\n\nsvc_clf = grid_svc.best_estimator_\n\nsvc_clf.fit(X_training, y_training)\npred_svc = svc_clf.predict(X_valid)\nacc_svc = accuracy_score(y_valid, pred_svc)","96ca0c93":"print(\"The Score for SVC is: \" + str(acc_svc))","29fe255a":"linsvc_clf = LinearSVC()\n\nparameters_linsvc = {\"multi_class\": [\"ovr\", \"crammer_singer\"], \"fit_intercept\": [True, False], \"max_iter\": [100, 500, 1000, 1500]}\n\ngrid_linsvc = GridSearchCV(linsvc_clf, parameters_linsvc, scoring=make_scorer(accuracy_score))\ngrid_linsvc.fit(X_training, y_training)\n\nlinsvc_clf = grid_linsvc.best_estimator_\n\nlinsvc_clf.fit(X_training, y_training)\npred_linsvc = linsvc_clf.predict(X_valid)\nacc_linsvc = accuracy_score(y_valid, pred_linsvc)\n\nprint(\"The Score for LinearSVC is: \" + str(acc_linsvc))","dedfe3aa":"rf_clf = RandomForestClassifier()\n\nparameters_rf = {\"n_estimators\": [4, 5, 6, 7, 8, 9, 10, 15], \"criterion\": [\"gini\", \"entropy\"], \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n                 \"max_depth\": [2, 3, 5, 10], \"min_samples_split\": [2, 3, 5, 10]}\n\ngrid_rf = GridSearchCV(rf_clf, parameters_rf, scoring=make_scorer(accuracy_score))\ngrid_rf.fit(X_training, y_training)\n\nrf_clf = grid_rf.best_estimator_\n\nrf_clf.fit(X_training, y_training)\npred_rf = rf_clf.predict(X_valid)\nacc_rf = accuracy_score(y_valid, pred_rf)\n\nprint(\"The Score for Random Forest is: \" + str(acc_rf))","4814b982":"logreg_clf = LogisticRegression()\n\nparameters_logreg = {\"penalty\": [\"l2\"], \"fit_intercept\": [True, False], \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n                     \"max_iter\": [50, 100, 200], \"warm_start\": [True, False]}\n\ngrid_logreg = GridSearchCV(logreg_clf, parameters_logreg, scoring=make_scorer(accuracy_score))\ngrid_logreg.fit(X_training, y_training)\n\nlogreg_clf = grid_logreg.best_estimator_\n\nlogreg_clf.fit(X_training, y_training)\npred_logreg = logreg_clf.predict(X_valid)\nacc_logreg = accuracy_score(y_valid, pred_logreg)\n\nprint(\"The Score for Logistic Regression is: \" + str(acc_logreg))","af492c48":"knn_clf = KNeighborsClassifier()\n\nparameters_knn = {\"n_neighbors\": [3, 5, 10, 15], \"weights\": [\"uniform\", \"distance\"], \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\"],\n                  \"leaf_size\": [20, 30, 50]}\n\ngrid_knn = GridSearchCV(knn_clf, parameters_knn, scoring=make_scorer(accuracy_score))\ngrid_knn.fit(X_training, y_training)\n\nknn_clf = grid_knn.best_estimator_\n\nknn_clf.fit(X_training, y_training)\npred_knn = knn_clf.predict(X_valid)\nacc_knn = accuracy_score(y_valid, pred_knn)\n\nprint(\"The Score for KNeighbors is: \" + str(acc_knn))","b4d94eaf":"gnb_clf = GaussianNB()\n\nparameters_gnb = {}\n\ngrid_gnb = GridSearchCV(gnb_clf, parameters_gnb, scoring=make_scorer(accuracy_score))\ngrid_gnb.fit(X_training, y_training)\n\ngnb_clf = grid_gnb.best_estimator_\n\ngnb_clf.fit(X_training, y_training)\npred_gnb = gnb_clf.predict(X_valid)\nacc_gnb = accuracy_score(y_valid, pred_gnb)\n\nprint(\"The Score for Gaussian NB is: \" + str(acc_gnb))","512de683":"dt_clf = DecisionTreeClassifier()\n\nparameters_dt = {\"criterion\": [\"gini\", \"entropy\"], \"splitter\": [\"best\", \"random\"], \"max_features\": [\"auto\", \"sqrt\", \"log2\"]}\n\ngrid_dt = GridSearchCV(dt_clf, parameters_dt, scoring=make_scorer(accuracy_score))\ngrid_dt.fit(X_training, y_training)\n\ndt_clf = grid_dt.best_estimator_\n\ndt_clf.fit(X_training, y_training)\npred_dt = dt_clf.predict(X_valid)\nacc_dt = accuracy_score(y_valid, pred_dt)\n\nprint(\"The Score for Decision Tree is: \" + str(acc_dt))","00398ff9":"from xgboost import XGBClassifier\n\nxg_clf = XGBClassifier()\n\nparameters_xg = {\"objective\" : [\"reg:linear\"], \"n_estimators\" : [5, 10, 15, 20]}\n\ngrid_xg = GridSearchCV(xg_clf, parameters_xg, scoring=make_scorer(accuracy_score))\ngrid_xg.fit(X_training, y_training)\n\nxg_clf = grid_xg.best_estimator_\n\nxg_clf.fit(X_training, y_training)\npred_xg = xg_clf.predict(X_valid)\nacc_xg = accuracy_score(y_valid, pred_xg)\n\nprint(\"The Score for XGBoost is: \" + str(acc_xg))","de8e2cdc":"model_performance = pd.DataFrame({\n    \"Model\": [\"SVC\", \"Linear SVC\", \"Random Forest\", \n              \"Logistic Regression\", \"K Nearest Neighbors\", \"Gaussian Naive Bayes\",  \n              \"Decision Tree\", \"XGBClassifier\"],\n    \"Accuracy\": [acc_svc, acc_linsvc, acc_rf, \n              acc_logreg, acc_knn, acc_gnb, acc_dt, acc_xg]\n})\n\nmodel_performance.sort_values(by=\"Accuracy\", ascending=False)","218361b8":"svc_clf.fit(X_train, y_train)","49da2678":"submission_predictions = svc_clf.predict(X_test)","29e305cf":"submission = pd.DataFrame({\n        \"PassengerId\": testing[\"PassengerId\"],\n        \"Survived\": submission_predictions\n    })\n\nsubmission.to_csv(\"titanic.csv\", index=False)\nprint(submission.shape)","189eebe3":"<a id=\"p2\"><\/a>\n# **2. Loading and Viewing Data Set**\nWith Pandas, we can load both the training and testing set that we wil later use to train and test our model. Before we begin, we should take a look at our data table to see the values that we'll be working with. We can use the head and describe function to look at some sample data and statistics. We can also look at its keys and column names.","b7b73baa":"Gender appears to be a very good feature to use to predict survival, as shown by the large difference in propotion survived. Let's take a look at how class plays a role in survival as well.","4dc3a7c3":"<a id=\"p3\"><\/a>\n# **3. Dealing with NaN Values (Imputation)**\nThere are NaN values in our data set in the age column. Furthermore, the Cabin column has a lot of missing values as well. These NaN values will get in the way of training our model. We need to fill in the NaN values with replacement values in order for the model to have a complete prediction for every row in the data set. This process is known as **imputation** and we will show how to replace the missing data.","933a54f7":"> **Note:** The numbers printed above are the proportion of male\/female survivors of all the surviviors ONLY. The graph shows the propotion of male\/females out of ALL the passengers including those that didn't survive.","1f6e2ccd":"**DecisionTree Model**","7ead07f4":"**RandomForest Model**","31c63944":"This IsAlone feature also may work well with the data we're dealing with, telling us whether the passenger was along or not on the ship.","6e60b50c":"<a id=\"p9\"><\/a>\n# **9. Submission**\nLet's create a dataframe to submit to the competition with our predictions of our model.","3af5c668":"**Defining Features in Training\/Test Set**","a4112ff4":"<a id=\"p8\"><\/a>\n# **8. Evaluating Model Performances**\nAfter making so many models and predictions, we should evaluate and see which model performed the best and which model to use on our testing set.","bc629bce":">**Note:** Only Age and Fare are the actual numerical values above and that the other features are just represented with numbers.","d629b1a8":"## **Classification vs. Regression**\nAs you know, predicting Titanic survivors is a supervised classification Machine Learning problem, where you classify a passenger as either survived, or not survived. Whereas in regression, you predict a continuous value like house price. If you would like to see my regression kernel on predicting housing price based on house features, click [here](https:\/\/www.kaggle.com\/samsonqian\/house-price-prediction-preprocessing-validation).","78f3c837":"We take a look at the distribution of the Age column to see if it's skewed or symmetrical. This will help us determine what value to replace the NaN values.","84f14a82":"To evaluate our model performance, we can use the make_scorer and accuracy_score function from sklearn metrics.","2f12c95f":"**LinearSVC Model**","caf9b57c":"# **Machine Learning to Predict Titanic Survivors **\nHi, I'm a current undergraduate student interested in the Data Science and Machine Learning field. In this Kernel, I will step by step process messy data and build a ML model  to predict the survival of each passenger aboard the Titanic. This guide is meant for people starting with data visualization, analysis and Machine Learning. If that sounds like you, then you're in the right place! It is not as difficult as you think to understand.\n\n*Please upvote and share if this helps you!! Also, feel free to fork this kernel to play around with the code and test it for yourself. If you plan to use any part of this code, please reference this kernel!* I will be glad to answer any questions you may have in the comments. Thank You! \n\n*Make sure to follow me for Future Kernels even better than this one!*","6195f71e":"**SVC Model**","eaa2c907":"Cool! All the features are in numerical form now. It is ready to be fed into our model. Before we do that however, there's something else that we should notice when looking at the preprocessed data. Particularly, the Age and Fare feature values.","628ede94":"<a id=\"p6\"><\/a>\n# **6. Feature Rescaling**\nIf you take a look at the Age and Fare features above, you can see that the values deviate heavily from the other features. This may cause some problems when we are modelling, since it may make these features seem more important than others. It would be beneficial to scale them so they are more representative. We can do this with both a MinMaxScaler or a StandardScaler. I will do this with a StandardScaler. The steps are shown below.","d7acfe59":"<a id=\"p7\"><\/a>\n# **7. Model Fitting, Optimizing, and Predicting**\nNow that our data has been processed and formmated properly, and that we understand the general data we're working with as well as the trends and associations, we can start to build our model. We can import different classifiers from sklearn. We will try different types of models to see which one gives the best accuracy for its predictions.","af31979a":"This data looks very messy! We're going to have to preprocess it before it's ready to be used in Machine Learning models.","f913cfa4":"## *Creating Synthetic Features*\nSometimes it is useful to create synthetic features that we think may help us predict the target value. ","fc93f2a3":"Nice! No more missing values. Now let's take a look at our imputed data.","8e6ef786":"**KNeighbors Model**","dc05d28b":"**Age**","0a99c62a":"**XGBoost Model**","95f882c7":"We can also use a GridSearch cross validation to find the optimal parameters for the model we choose to work with and use to predict on our testing set.","511dbaac":"Here is one final cumulative graph of a pair plot that shows the relations between all of the different features","8fdf6c1f":"<a id=\"p5\"><\/a>\n# **5. Feature Engineering**\nBecause values in the Sex and Embarked columns are categorical values, we have to represent these strings as numerical values in order to perform our classification with our model. We can also do this process through **One-Hot-Encoding**.","8e12a83c":"> ### Note:\nYou might see some people in this competition get incredibly high accuracies, sometimes even 100%. These people are likely extremely overfitting their models, or cheating since the test set survivors are available publicly online. You can always improve your model, but overfitting it just for higher score isn't good practice. ","61a8a0b8":"**Gender**","6d908827":"# **Contents**\n1. [Importing Libraries and Packages](#p1)\n2. [Loading and Viewing Data Set](#p2)\n3. [Dealing with NaN Values (Imputation)](#p3)\n4. [Plotting and Visualizing Data](#p4)\n5. [Feature Engineering](#p5)\n6. [Feature Rescaling](#p6)\n7. [Modeling and Predicting with sklearn](#p7)\n8. [Evaluating Model Performances](#p8)\n9. [Submission](#p9)","ad3ae60e":"**LogisiticRegression Model**","eba58edf":"Now, the next step for you would be to take a look at regression, the other type of supervised Machine Learning. I have written a kernel on another competition hosted by Kaggle, House Price Prediction. If you would like to read it and gain an understanding of regression, please click [here](https:\/\/www.kaggle.com\/samsonqian\/house-price-prediction-preprocessing-validation).","28901309":"Although it may not seem like it, we can also extract some useful information from the name column. Not the actual names themselves, but the title of their names like Ms. or Mr. This may also provide a hint as to whether the passenger survived or not. Therefore we can extract this title and then encode it like we did for Sex and Embarked.","e269d07b":"If you made it this far, congratulations!! You have gotten a glimpse at an introduction to data visualization, analysis and Machine Learning. You are well on your way to become a Data Science expert! Keep learning and trying out new things, as one of the most important things for Data Scientists is to be creative and perform analysis hands-on. Please upvote and share if this kernel helped you!","9bde627b":"**sklearn Models to Test**","f8c9ce31":"There are 3 values for Embarked: *S*, *C*, and *Q*. We will represent these with numbers as well.","5132d652":"Looks like the distribution of ages is slightly skewed right. Because of this, we can fill in the null values with the median for the most accuracy. \n> **Note:** We do not want to fill with the mean because the skewed distribution means that very large values on one end will greatly impact the mean, as opposed to the median, which will only be slightly impacted.","b730fbda":"We change Sex to binary, as either 1 for female or 0 for male. We do the same for Embarked. We do this same process on both the training and testing set to prepare our data for Machine Learning.","49900546":"**Class**","cc20095e":"**Validation Data Set**\n\nAlthough we already have a test set, it is generally easy to overfit the data with these classifiers. It is therefore useful to have a third data set called the validation data set to ensure that our model doesn't overfit with the data. We can make this third data set with sklearn's train_test_split function. We can also use the validation data set to test the general accuracy of our model.","8677405e":"It appears that class also plays a role in survival, as shown by the bar graph. People in Pclass 1 were more likely to survive than people in the other 2 Pclasses.","e7f970ad":"It appears as though passengers in the younger range of ages were more likely to survive than those in the older range of ages, as seen by the clustering in the strip plot, as well as the survival distributions of the histogram.","16be6bf5":"Wow! Cabin has a lot of missing values. Also, it seems as though the Ticket feature is too noisy to be useful. We can probably drop both features without it impacting the performance of our model.","4f2819a0":"**GaussianNB Model**","1a3ed51b":"<a id=\"p4\"><\/a>\n# **4. Plotting and Visualizing Data**\nIt is very important to understand and visualize any data we are going to use in a machine learning model. By visualizing, we can see the trends and general associations of variables like Sex and Age with survival rate. We can make several different graphs for each feature we want to work with to see the entropy and information gain of the feature. ","12353fad":"This feature scaling may allow for higher accuracy for our models because of the reduced weight of their magnitudes!","a35bc58a":"We can combine SibSp and Parch into one synthetic feature called family size, which indicates the total number of family members on board for each member. ","c77c7ceb":"<a id=\"p1\"><\/a>\n# **1. Importing Libraries and Packages**\nWe will use these packages to help us manipulate the data and visualize the features\/labels as well as measure how well our model performed. Numpy and Pandas are helpful for manipulating the dataframe and its columns and cells. We will use matplotlib along with Seaborn to visualize our data."}}