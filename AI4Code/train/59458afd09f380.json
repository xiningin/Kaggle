{"cell_type":{"cd278739":"code","2a68b18d":"code","541542dd":"code","3564aa5d":"code","b781669a":"code","b3476b4d":"code","d6e6b1e9":"code","f17ee5e5":"code","f1d786d8":"code","9ee5d3d9":"code","75849f18":"code","8d2a64a4":"code","707f7e1c":"code","252f81cf":"code","69f2a266":"code","570994f5":"code","5f0b9ac0":"code","0b78e60e":"code","3d8f3642":"markdown","6a61b763":"markdown","d9122615":"markdown","3964798f":"markdown","8e5c4cf8":"markdown","ca889e67":"markdown","2e440279":"markdown","294e075d":"markdown","b5756030":"markdown","ed44b5cd":"markdown","676e72b1":"markdown","ad38dc65":"markdown","8a0cd1f2":"markdown","341a21b4":"markdown","1c2bc102":"markdown","7da2ecfd":"markdown","0f730820":"markdown","2b7da39f":"markdown"},"source":{"cd278739":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import cross_val_score,cross_validate,train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom mlxtend.classifier import StackingCVClassifier","2a68b18d":"X = pd.read_csv(\"..\/input\/learn-together\/train.csv\", index_col='Id')\nX_test = pd.read_csv(\"..\/input\/learn-together\/test.csv\", index_col='Id')\nX_train = X.iloc[:,:54]\ny_train = X.loc[:,'Cover_Type']","541542dd":"lgbc = LGBMClassifier(n_estimators=500, learning_rate= 0.1, \n               objective= 'multiclass', num_class=7,\n               random_state= 12345, n_jobs=-1)\nlgbc.fit(X_train, y_train)\nlgbc_feature_importances = pd.DataFrame(lgbc.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=[\"importance\"])\nprint(lgbc_feature_importances.sort_values(\"importance\",ascending=False))\n","3564aa5d":"print(X_train.columns[lgbc_feature_importances[\"importance\"] == 0])","b781669a":"def get_LGBC():\n    return LGBMClassifier(n_estimators=500, learning_rate= 0.1, \n               objective= 'multiclass', num_class=7,\n               random_state= 12345, n_jobs=-1)\n\n\nfor thre in [0,50,100,200,500]:\n    print(np.mean(cross_val_score(get_LGBC(), \n                                  X_train.drop(X_train.columns[lgbc.feature_importances_<thre], axis=1), \n                                  y_train, cv=5)))\n","b3476b4d":"X_train.drop(X_train.columns[lgbc.feature_importances_ == 0], axis=1, inplace=True)\nX_test.drop(X_test.columns[lgbc.feature_importances_ == 0], axis=1, inplace=True)","d6e6b1e9":"fr = DecisionTreeClassifier(random_state=12345).fit(X_train, y_train)\nfr_feature_importances = pd.DataFrame(fr.feature_importances_, \n                            index = X_train.columns,\n                             columns=['importance'])\nprint(fr_feature_importances.sort_values(\"importance\",ascending=False))","f17ee5e5":"print(X_train.columns[fr_feature_importances[\"importance\"] == 0])","f1d786d8":"for thre in [0,0.0001,0.001,0.005,0.01]:\n    print(np.mean(cross_val_score(DecisionTreeClassifier(random_state=12345), \n                                    X_train.drop(X_train.columns[fr.feature_importances_<thre], axis=1), \n                                    y_train, cv=5)))","9ee5d3d9":"X_train.drop(X_train.columns[fr.feature_importances_ == 0], axis=1, inplace=True)\nX_test.drop(X_test.columns[fr.feature_importances_ == 0], axis=1, inplace=True)","75849f18":"X_train.columns","8d2a64a4":"len(X_train.columns)","707f7e1c":"ab_clf = AdaBoostClassifier(n_estimators=200,\n                            base_estimator=DecisionTreeClassifier(\n                                min_samples_leaf=2,\n                                random_state=12345),\n                            random_state=12345)\n   \nrf_clf = RandomForestClassifier(n_estimators=300,\n                                random_state=12345,\n                                n_jobs=1)\n\nxgb_clf = XGBClassifier(n_estimators = 500, \n                        booster='gbtree', \n                        colsample_bylevel=1, \n                        colsample_bynode=1, \n                        colsample_bytree=0.8, \n                        gamma=5,\n                        nthread=1, \n                        learning_rate=0.1,\n                        max_delta_step=0, \n                        max_depth=10,\n                        min_child_weight=10, \n                        missing=None, \n                        random_state= 12345,\n                        n_jobs=1)                     \n\net_clf = ExtraTreesClassifier(n_estimators=300,\n                              min_samples_leaf=1,\n                              min_samples_split=2,\n                              max_depth=50,\n                              max_features=0.3,\n                              bootstrap = False,\n                              random_state=12345,\n                              n_jobs=1)\n\nlg_clf = LGBMClassifier(n_estimators=300,\n                        num_leaves=128,\n                        learning_rate= 0.1,\n                        verbose=-1,\n                        num_class=7,\n                        random_state=12345,\n                        n_jobs=1)\n\nensemble = [(\"AdaBoostClassifier\", ab_clf),\n            (\"RandomForestClassifier\", rf_clf),\n            (\"XGBClassifier\", xgb_clf),\n            (\"ExtraTreesClassifier\", et_clf),\n            (\"LGBMClassifier\", lg_clf)]","252f81cf":"for label, clf in ensemble:\n    score = cross_val_score(clf, X_train, y_train,\n                            cv=5,\n                            scoring='accuracy',\n                            verbose=0,\n                            n_jobs=-1)\n\n    print('  -- {: <24} : {:.3f} : {}'.format(label, np.mean(score), np.around(score, 3)))","69f2a266":"stack = StackingCVClassifier(classifiers=[ab_clf, rf_clf, xgb_clf, et_clf, lg_clf],\n                             meta_classifier=rf_clf,\n                             cv=5,\n                             stratify=True,\n                             shuffle=True,\n                             use_probas=True,\n                             use_features_in_secondary=True,\n                             verbose=1,\n                             random_state=12345,\n                             n_jobs=-1)\nX_train = np.array(X_train)\ny_train = np.array(y_train)\nstack = stack.fit(X_train, y_train)","570994f5":"X_test = np.array(X_test)\npred = stack.predict(X_test)","5f0b9ac0":"pred[:10]","0b78e60e":"X_test = pd.read_csv(\"..\/input\/learn-together\/test.csv\", index_col='Id')\npredictions = pd.Series(pred, index=X_test.index, dtype=y_train.dtype)\npredictions.to_csv('submission.csv', header=['Cover_Type'], index=True, index_label='Id')","3d8f3642":"### Stacked model","6a61b763":"From above results, I will also delect the features which feature_importances index is zero.","d9122615":"This is my first formal kaggle competition. Before the competition, I just use the kaggle datasets as my ML training datasets. When I got an email from keggle who told me that there is a good competition for kaggle beginners, I realized that i could try to join in kaggle competition to learn more.\n\nAs a new learner, I have read some excellent notebook in this competition. So in this notebook, I refer to some code and methods from other participants, such as:\n\n[Learn Kaggle Users - Classify forest types](https:\/\/www.kaggle.com\/xwolf12\/learn-kaggle-users-classify-forest-types)  \n[Top 6% Roosevelt National Forest competition](https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition\/comments)\n[Forest Cover: Stacking Multiple Classifiers](https:\/\/www.kaggle.com\/kwabenantim\/forest-cover-stacking-multiple-classifiers)  \n[Basic ensemble model](https:\/\/www.kaggle.com\/jakelj\/basic-ensemble-model)  \n[2-Layer K-fold learning Forest Cover](https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover)\n\nFrom beginning of this competition, I first understand the meaning of this dataset and do some exploratory studys so that i could find some useful features. This contents has been recorded in a notebook: [Exploratory study in forest types with R](https:\/\/www.kaggle.com\/gukai1212\/exploratory-study-in-forest-types-with-r)\n\n","3964798f":"### Import python packages and kaggle datasets.","8e5c4cf8":"Making predictions","ca889e67":"# Code","2e440279":"#### Then I want to use DecisionTreeClassifier to filter some bad features to improve prediction accuracy","294e075d":"Fitting stack","b5756030":"# Prelusion","ed44b5cd":"From above results, I will delect the features which feature_importances index is zero.","676e72b1":"This is a work in progress and I accept all critique.\n\nI have try lots exploratory research in this competition. But I want to just use ML functions to predict the forest types, not including some preprocessings and discovering new correlated variables.","ad38dc65":"Done!","8a0cd1f2":"Cross-validating classifiers","341a21b4":"Export predictions and submission","1c2bc102":"#### First, I use LGBMClassifier to filter some bad features.","7da2ecfd":"Show the best fiter threshold in this training datasets:","0f730820":"Show the best fiter threshold in above training datasets:","2b7da39f":"### Straightforward features selection"}}