{"cell_type":{"371d0b6d":"code","9b494f8e":"code","24a1c521":"code","c3f185db":"code","df30d6ee":"code","f7b4124d":"code","c1eea7c8":"code","da8356fe":"code","2fdf4ec9":"code","90c6a6b7":"code","32bb6168":"code","52b318f1":"code","60141fce":"code","4643a25a":"code","20f06ce2":"code","2864e2ad":"code","1859ad46":"code","1dc997ff":"code","67d1e036":"code","c0237c14":"code","6a7d363e":"code","e4913c1c":"code","126527f7":"code","d8c1bbe5":"code","850349a4":"code","dcc8b25d":"code","46647888":"code","1fc38122":"code","18c0b3c1":"code","731b1a2f":"code","a05efc69":"code","c0846a50":"code","3c64df56":"code","1452ae84":"code","9a06fc16":"code","a896bb31":"code","77b46ced":"code","7f10ace0":"code","4a897016":"code","f625e775":"code","bb767c30":"code","b89b6eca":"code","3dcd14bb":"code","f09a6919":"code","ed9e4d58":"code","99ed7346":"code","a9970a15":"code","4c789481":"code","52736b3e":"code","c9c53b5b":"code","ec128af1":"code","524e3c4a":"code","30d2eb3d":"code","1f31ba0a":"code","4bd2d499":"code","22fb4a30":"code","2c4f0286":"code","7db82085":"code","bb6328f0":"code","79b6d20b":"code","de84a6f7":"code","f4ffd873":"code","9be71f18":"code","47a9190d":"code","89f4cca2":"code","c1e3d295":"code","f1b5587e":"code","63ea80c1":"code","a33d3c53":"code","7f278175":"code","a3139d34":"code","f7a23e44":"code","c421200e":"code","8db7d16d":"code","1c859af0":"code","57e50c84":"code","0cb22b85":"code","966c6f29":"code","3416cf48":"code","ea062e33":"code","c1cd675d":"code","e5e0fa86":"code","f00caf44":"code","e86cd7e0":"code","96148186":"code","61c7dc24":"code","11f18700":"code","99f22e8e":"code","79fe081b":"code","5fa5a767":"markdown","9b2bb131":"markdown","08e6048b":"markdown","f4da0949":"markdown","2271a23d":"markdown","5ef0582b":"markdown","ea31881c":"markdown","7c36ca4a":"markdown","3539d1c7":"markdown","4097293a":"markdown","eafb60b6":"markdown","3ab7c0ee":"markdown","3efb01ab":"markdown","4d97fe2b":"markdown","0091edfb":"markdown","7116b7b4":"markdown","d59941d9":"markdown","07e5651f":"markdown","ff0fab0b":"markdown","bca0b50d":"markdown","e54df6c1":"markdown","8072334c":"markdown","64937576":"markdown","f3f6dd19":"markdown","d6adb337":"markdown","41e647fd":"markdown","a84863d1":"markdown","23d21c52":"markdown","8aaa7bb9":"markdown","ddbe6e90":"markdown","0488b73f":"markdown","26c1bd84":"markdown","609705ff":"markdown","9509aeb4":"markdown","37952167":"markdown","2073f0e6":"markdown","65c00af0":"markdown","ad6415dd":"markdown","39062e3f":"markdown","b918afe1":"markdown","df4cfc3f":"markdown","43a614a0":"markdown","2581fa41":"markdown","0098873a":"markdown","138f76e9":"markdown","7aa1d3d9":"markdown","363c61a2":"markdown","f09f289d":"markdown","c29aacf8":"markdown","e2c0a269":"markdown","fea04fb5":"markdown","ca83f3b5":"markdown","209e13a8":"markdown","91f56a6a":"markdown","a86809ba":"markdown","2677ddcb":"markdown","78afbe2c":"markdown","d09565ea":"markdown","9a75abe8":"markdown","5032441a":"markdown","c96958dc":"markdown"},"source":{"371d0b6d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\n\nimport seaborn as sns\n\nfrom collections import Counter\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9b494f8e":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_PassengerId = test_data[\"PassengerId\"]\n","24a1c521":"train_data.info # We can get basic information about our data ","c3f185db":"train_data.columns # We can get columns of train data","df30d6ee":"train_data.head() # This code show us initial five line","f7b4124d":"train_data.describe","c1eea7c8":"def barPlot(var):\n    \"\"\"\n    input: variable ex:\"Sex\"\n    output: bar plot & value count\n    \"\"\"\n    var_traind  = train_data[var] # get feature\n    var_value = var_traind.value_counts() # value_counts() count number of categorical variable \n    \n    #visualize\n    plt.figure(figsize = (8,4))\n    plt.bar(var_value.index,var_value)\n    plt.xticks(var_value.index, var_value.index.values)\n    plt.ylabel('Frequency')\n    plt.title(var)\n    plt.show()\n    print(\"{}: \\n\".format(var,var_value))","da8356fe":"c1 = [\"Survived\",\"Sex\",\"Pclass\",\"Embarked\",\"SibSp\",\"Parch\"]\nfor x in c1:\n    barPlot(x)","2fdf4ec9":"#  When we run shows us how many categories about Cabin, Name and Ticket\nc2 = [\"Cabin\",\"Name\",\"Ticket\"]\nfor x in c2:\n    print(\"{} \\n\".format(train_data[x].value_counts()))","90c6a6b7":"def plt_hist(variable):\n    plt.figure(figsize = (8,4))\n    plt.hist(train_data[variable], bins = 40)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","32bb6168":"num_var = [\"Fare\", \"Age\", \"PassengerId\"]\nfor y in num_var:\n    plt_hist(y)","52b318f1":"train_data[[\"Pclass\",\"Survived\"]]","60141fce":"#Pclass - Survived\ntrain_data[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"], as_index=False).mean().sort_values(by=\"Survived\",ascending= False )","4643a25a":"#Embarked - Survived\ntrain_data[[\"Embarked\",\"Survived\"]].groupby([\"Embarked\"], as_index=False).mean().sort_values(by=\"Survived\",ascending= False )","20f06ce2":"#Sex - Survived\ntrain_data[[\"Sex\",\"Survived\"]].groupby([\"Sex\"], as_index=False).mean().sort_values(by=\"Survived\",ascending= False )\n","2864e2ad":"def detect_outliers(df,features):\n    outlier_indices = []\n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 =np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        #Outlier Step\n        outlier_step = IQR * 1.5\n        # Detect outlier and their indices\n        outlier_list_col = df[(df[c] <Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        #store indices\n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i,v in outlier_indices.items() if v > 2) \n    \n    return multiple_outliers","1859ad46":"train_data.loc[detect_outliers(train_data,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])]","1dc997ff":"train_df_len = len(train_data)\ntrain_data = pd.concat([train_data,test_data], axis=0).reset_index(drop = True)","67d1e036":"train_data.head()","c0237c14":"# Which feature have null values?\ntrain_data.columns[train_data.isnull().any()]","6a7d363e":"# How many have? \ntrain_data.isnull().sum()","e4913c1c":"# Where is the null value for Embarked?\ntrain_data[train_data[\"Embarked\"].isnull()]","126527f7":"# Analysis use the Fare\n\ntrain_data.boxplot(column='Fare',by='Embarked')\nplt.show()","d8c1bbe5":"train_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(\"C\")","850349a4":"# Check again null values for Embarked\ntrain_data[train_data[\"Embarked\"].isnull()]","dcc8b25d":"# For fare\ntrain_data[train_data[\"Fare\"].isnull()]\n","46647888":"train_data[\"Fare\"] = train_data[\"Fare\"].fillna(np.mean(train_data[train_data[\"Pclass\"] == 3][\"Fare\"]))","1fc38122":"train_data[train_data[\"Fare\"].isnull()]","18c0b3c1":"train_data.info()","731b1a2f":"list1 = [\"SibSp\",\"Parch\", \"Age\" , \"Fare\" ,\"Survived\"]\nsns.heatmap(train_data[list1].corr(),annot = True, fmt = \".2f\")\nplt.show()","a05efc69":"sib_surv_factor = sns.factorplot(x = \"SibSp\",y = \"Survived\", data = train_data, kind = \"bar\", size = 7)\nsib_surv_factor.set_ylabels(\"Survived Probability\");","c0846a50":"parch_surv_factor = sns.factorplot(x = \"Parch\", y =  \"Survived\", kind = \"bar\",data=train_data, size=6)\nparch_surv_factor.set_ylabels(\"Survived Probability\");","3c64df56":"pclass_surv_factor = sns.factorplot(x = \"Pclass\", y =  \"Survived\", kind = \"bar\",data=train_data, size=6)\npclass_surv_factor.set_ylabels(\"Survived Probability\");","1452ae84":"age_surv_factor = sns.FacetGrid(train_data,col = 'Survived' )\nage_surv_factor.map(sns.distplot, \"Age\",bins = 30);\n","9a06fc16":"pclass_surv_age = sns.FacetGrid(train_data, col = 'Survived',row = 'Pclass')\npclass_surv_age.map(plt.hist, \"Age\",bins = 25)\npclass_surv_age.add_legend();","a896bb31":"em_sex_pclass_surv = sns.FacetGrid(train_data, row= 'Embarked',size=4)\nem_sex_pclass_surv.map(sns.pointplot, \"Pclass\",\"Survived\",\"Sex\")\nem_sex_pclass_surv.add_legend();","77b46ced":"#Sex - Survived\ntrain_data[[\"Sex\",\"Survived\"]].groupby([\"Sex\"], as_index=False).mean().sort_values(by=\"Survived\",ascending= False )","7f10ace0":"em_sex_fare_surv = sns.FacetGrid(train_data, row= 'Embarked',col='Survived',size=3)\nem_sex_fare_surv.map(sns.barplot,\"Sex\",'Fare')\nem_sex_fare_surv.add_legend();","4a897016":" train_data[train_data[\"Age\"].isnull()]","f625e775":"sns.factorplot(x = \"Sex\", y = \"Age\", data=train_data, kind = \"box\");","bb767c30":"sns.factorplot(x = \"Sex\", y = \"Age\",hue = \"Pclass\", data=train_data, kind = \"box\");","b89b6eca":"sns.factorplot(x = \"Parch\", y = \"Age\", data=train_data, kind = \"box\");\nsns.factorplot(x = \"SibSp\", y = \"Age\", data=train_data, kind = \"box\");","3dcd14bb":"# Sex feature is object not int. We have to convert int.\ntrain_data[\"Sex\"] = [1 if i == \"male\" else 0 for i in train_data[\"Sex\"]]","f09a6919":"sns.heatmap(train_data[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot = True);","ed9e4d58":"#fill age feature \nindex_nan_age = list(train_data[\"Age\"][train_data[\"Age\"].isnull()].index)\nfor i in index_nan_age:\n    age_pred = train_data[\"Age\"][((train_data[\"SibSp\"] == train_data.iloc[i][\"SibSp\"]) &\n                                 (train_data[\"Parch\"] == train_data.iloc[i][\"Parch\"]) & \n                                 (train_data[\"Pclass\"] == train_data.iloc[i][\"Pclass\"]))].median()\n    age_median  = train_data[\"Age\"].median()\n    if not np.isnan(age_pred):\n        train_data[\"Age\"].iloc[i]  =age_pred\n    else:\n        train_data[\"Age\"].iloc[i]  =age_median","99ed7346":" train_data[train_data[\"Age\"].isnull()]","a9970a15":"train_data[\"Name\"].head(10)","4c789481":"name = train_data[\"Name\"]\ntrain_data[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in name]","52736b3e":"train_data[\"Title\"].head(10)","c9c53b5b":"sns.countplot(x=\"Title\", data = train_data)\nplt.xticks(rotation = 60)\nplt.show()","ec128af1":"#convert to categorical\ntrain_data[\"Title\"] = train_data[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"],\"other\")\ntrain_data[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\"or i == \"Ms\" or i== \"Mlle\" or i == \"Mrs\" else 2 if i== \"Mr\" else 3 for i in train_data[\"Title\"]]\ntrain_data[\"Title\"].head()","524e3c4a":"sns.countplot(x=\"Title\", data = train_data)\nplt.xticks(rotation = 60)\nplt.show()","30d2eb3d":"g = sns.factorplot(x = \"Title\", y=\"Survived\" , data=train_data, kind=\"bar\")\ng.set_xticklabels([\"Master\",\"Mrs\",\"Mr\",\"Other\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","1f31ba0a":"train_data.drop(labels = [\"Name\"], axis=1, inplace=True)","4bd2d499":"train_data.head()","22fb4a30":"train_data = pd.get_dummies(train_data, columns = [\"Title\"])\ntrain_data.head()","2c4f0286":" train_data[\"Fsize\"] = train_data[\"SibSp\"]  + train_data[\"Parch\"] + 1","7db82085":"train_data.head()","bb6328f0":"# Is there any relations between Fsize and Survived\ng = sns.factorplot(x = \"Fsize\", y= \"Survived\", data=train_data, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","79b6d20b":"train_data[\"Family_size\"] = [1 if i<5 else 0 for i in train_data[\"Fsize\"]]","de84a6f7":"train_data.head(10)","f4ffd873":"sns.countplot(x = \"Family_size\" , data = train_data)\nplt.show()","9be71f18":"#Relations between survived\ng = sns.factorplot(x = \"Family_size\", y= \"Survived\", data=train_data, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","47a9190d":"#Family_size occurs 0 and 1. We split two feature\ntrain_data = pd.get_dummies(train_data, columns = [\"Family_size\"])\ntrain_data.head()","89f4cca2":"train_data[\"Embarked\"].head()","c1e3d295":"sns.countplot(x = \"Embarked\", data = train_data)\nplt.show()","f1b5587e":"train_data = pd.get_dummies(train_data, columns = [\"Embarked\"])\n\ntrain_data.head()","63ea80c1":"train_data[\"Ticket\"].head(10)","a33d3c53":"y = \" A\/5. 2152 \"\ny.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0]","7f278175":"tickets = []\nfor i in list(train_data.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"x\")\n        \ntrain_data[\"Ticket\"] = tickets","a3139d34":"train_data[\"Ticket\"].head()","f7a23e44":"#prefix for the use T instead of use Ticket\ntrain_data = pd.get_dummies(train_data, columns = [\"Ticket\"], prefix = \"T\")","c421200e":"train_data.head()\n","8db7d16d":"sns.countplot(x = \"Pclass\", data = train_data)\nplt.show()","1c859af0":"train_data[\"Pclass\"] = train_data[\"Pclass\"].astype(\"category\")\ntrain_data = pd.get_dummies(train_data, columns = [\"Pclass\"])\ntrain_data.head()","57e50c84":"train_data[\"Sex\"] = train_data[\"Sex\"].astype(\"category\")\ntrain_data = pd.get_dummies(train_data, columns = [\"Sex\"])\ntrain_data.head()","0cb22b85":"train_data.drop(labels = [\"PassengerId\", \"Cabin\"], axis = 1, inplace = True)","966c6f29":"train_data.columns","3416cf48":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","ea062e33":"train_df_len","c1cd675d":"test = train_data[train_df_len:]\ntest.drop(labels = [\"Survived\"], axis = 1, inplace = True)","e5e0fa86":"test.head()","f00caf44":"#we split our data as test and train \n# test_size = 0.33 mean %33 test_size, the rest train data\ntrain = train_data[:train_df_len]\nX_train = train.drop(labels = \"Survived\",axis = 1)\ny_train = train[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.33, random_state = 42)\nprint(\"x_train\", len(X_train))\nprint(\"x_test\", len(X_test))\nprint(\"y_train\", len(y_train))\nprint(\"y_test\", len(y_test))\nprint(\"test\", len(test))","e86cd7e0":"logReg = LogisticRegression()\nlogReg.fit(X_train, y_train)\nacc_log_train = round(logReg.score(X_train, y_train)*100,2) # as %\nacc_log_test = round(logReg.score(X_test, y_test)*100,2) # as %\nprint(\"Trainin Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))","96148186":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC (random_state = random_state),\n             RandomForestClassifier (random_state = random_state),\n             LogisticRegression (random_state = random_state),\n             KNeighborsClassifier()]\n\n#Select necessary parameters for Hyperparameter Parameters\ndecisionTree_param_grid = {\"min_samples_split\": range(10,500,20),\n                          \"max_depth\": range(1,20,2)}\n\nsvc_param_grid = {\"kernel\": [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]} #values that parameters can take\n\nrandomForest_param_grid = {\"max_features\": [1,3,10],\n                          \"min_samples_split\":[2,3,10],\n                          \"min_samples_leaf\":[1,3,10],\n                          \"bootstrap\": [False],\n                          \"n_estimators\": [100,300],\n                          \"criterion\": [\"gini\"]}\n\n\nlogReg_param_grid = {\"C\": np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\":[\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]} \n\nclassifier_param = [decisionTree_param_grid,\n                   svc_param_grid,\n                   randomForest_param_grid,\n                   logReg_param_grid,\n                   knn_param_grid]","61c7dc24":"#train\ncross_validation_result = [] #keeps results\nbest_estimators = [] #select best values in ML models\nfor i in range(len(classifier)):\n    # n_jobs = 1 mean provides our code to run in parallel\n    # verbose = 1 shows results when to run code\n    clf = GridSearchCV(classifier[i], param_grid = classifier_param[i], cv= StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1 , verbose = 1)\n    clf.fit(X_train, y_train)\n    cross_validation_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cross_validation_result[i])","11f18700":"cross_validation_result = pd.DataFrame({\"Cross Validation Means\": cross_validation_result, \"ML Models\": [\n    \"DecisionTreeClassifier\",\n    \"SVC\",\n    \"RandomForestClassifier\",\n    \"LogisticRegression\",\n    \"KNeighborsClassifier\"]})\n\ng= sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cross_validation_result)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")\n","99f22e8e":"# use voting classifier type to do ensemble modeling.\nvoting_classifier = VotingClassifier(estimators = [(\"dt\", best_estimators[0]),\n                                                  (\"rfc\", best_estimators[2]),\n                                                  (\"lr\", best_estimators[3])],\n                                                    voting  = \"soft\", n_jobs = -1)\nvoting_classifier = voting_classifier.fit(X_train,y_train)\nprint(accuracy_score(voting_classifier.predict(X_test),y_test))","79fe081b":"test_survived = pd.Series(voting_classifier.predict(test), name=\"Survived\").astype(int)\nresults = pd.concat([test_PassengerId,test_survived],axis=1)\nresults.to_csv(\"Titanic.csv\",index=False)\n","5fa5a767":"<a id=24><\/a>\n## Ticket","9b2bb131":"* Parch = Parent or Children\n* K\u00fc\u00e7\u00fck aileler b\u00fcy\u00fck ailelere g\u00f6re hayatta kalma olas\u0131l\u0131\u011f\u0131 daha y\u00fcksek\n* The black line in the middle is the standard deviation\n* SibSp and Parch can be used for new feature extraction with th=3","08e6048b":"Age is not correlated with sex but it is correlated with parch,sibsp and pclass.","f4da0949":"<a id=8><\/a>\n# Find Missing Value","2271a23d":"<a id = \"6\"><\/a>\n# Outlier Detection","5ef0582b":"<a id=26><\/a>\n## Sex","ea31881c":"We created 4 categories by revealing the name feature and named it a feature named Title. So we made a brand new feature extraction. Now we can extract the Name function from our data.","7c36ca4a":"<a id = 17><\/a>\n## Embarked - Sex - Pclass - Survived","3539d1c7":"<a id=18><\/a>\n## Embarked - Sex - Fare - Survived","4097293a":"<a id=23><\/a>\n## Embarked","eafb60b6":"    PassengerId :  id of each passenger\n    Survived : alive(1) or dead(0)\n    Pclass : class of each passenger (Which class)\n    Name : Name of each passenger\n    Sex : Sex of each passenger\n    Age : Age of each passenger\n    SibSp : number of siblings\/spouses\n    Parch : number of parents\/children\n    Ticket : Ticket number\n    Fare : amount of money of ticket\n    Cabin : cabin category\n    Embarked : port where passenger embarked (C = cherbourg, Q = Queenstown, S = Southamton)\n","3ab7c0ee":"<a id=30><\/a>\n## Simple Logistic Regression","3efb01ab":"The Titanic was a steamship that sank of April 15, 1912 after striking an iceberg, leading to the deaths of more than 1,500 passengers and crew.","4d97fe2b":"<a id = 13><\/a>\n## Parch - Survived","0091edfb":"### Now lets examine correlation","7116b7b4":"* Passengers who pay higher fare have better survival.\n\n* C embarked has a higher probability of survival than other embarked.","d59941d9":"* Female survive has a higher probability of survival  than male (Embarked S and Q).\n* Also We can see female survive has a higher probability of survival  than male with the Sex-Survived analysis.\n* male have better survival rate in pclass 3 in embarked C.\n* Embarked and sex will be used in training\n","07e5651f":"<a id = 16><\/a>\n##  Pclass - Survived - Age","ff0fab0b":"<a id=31><\/a>\n## Hyperparameter Tuning -- Grid Search -- Cross Validation","bca0b50d":"* Pclass is important feature for model training","e54df6c1":"<a id=22><\/a>\n# Family Size ","8072334c":"Small families more chance to survive than large families.","64937576":"<a id = 10><\/a>\n# Visualization","f3f6dd19":"##### What we learn?\n1. [Load Data](#1)\n1. [Variable Description](#2)\n    * [Categorical Variable](#3)\n    * [Numerical Variable](#4)\n1. [Data Analysis](#5)\n1. [Outlier Detection](#6)\n1. [Missing Value](#7)\n    * [Find Missing Value](#8)\n    * [Fill Missing Value](#9)\n1. [Visualization](#10)\n    * [Correlation between SibSp - Parch - Age - Fare - Survived](#11)\n    * [ SibSp - Survived](#12)\n    * [ Parch - Survived](#13)\n    * [ Pclass - Survived](#14)\n    * [ Age - Survived](#15)\n    * [ Pclass - Survived - Age](#16)\n    * [ Embarked - Sex - Pclass - Survived](#17)\n    * [ Embarked - Sex - Fare - Survived](#18)\n    * [ Fill Missing : Age Feature](#19)\n1. [Feature Engineering](#20)\n    * [Name -- Title](#21)\n    * [Family Size](#22)\n    * [Embarked](#23)\n    * [Ticket](#24)\n    * [Pclass](#25)\n    * [Sex](#26)\n    * [Drop Passenger ID and Cabin](#27)\n1. [Modeling](#28)\n    * [Train-Test Split](#29)\n    * [Simple Logistic Regression](#30)\n    * [Hyperparameter Tuning -- Grid Search -- Cross Validation ](#31)\n    * [Ensemble Modeling](#32)\n    * [Prediction and Submission](#33)\n","d6adb337":"<a id=27><\/a>\n## Drop Passenger ID and Cabin","41e647fd":"<a id=21><\/a>\n## Name -- Title","a84863d1":"Outliers are extreme values that deviate from other observations on data.","23d21c52":"## Age - Survived","8aaa7bb9":"* Survived = 0.0 means distribution of the not survive\n* Survived = 1.0 means distribution of the survive\n* Oldest passenger (80) survived\n* most passengers are in 15-35 age range","ddbe6e90":"<a id=7><\/a>\n# **Missing Value**\n\nNeed the concatenation each two data frame. We have to make sure there are no nulls in the other dataframe","0488b73f":"We will make the tickets categorical. We make new features for each categorical. ","26c1bd84":"<a id = \"4\"><\/a>\n# Numerical Variable","609705ff":"We compare 5 different ml models. \nWhile comparing these models we will be doing Hyperparameter Tuning. So we're going to look for the best parameters found in these models. We will use the Grid Seaarcgh method when searching for this parameter and we will use the Cross Validation method when comparing the best values of the parameters we find.\n\n* Decision Tree\n* SVM \n* Random Forest\n* KNN\n* Logistic Regression","9509aeb4":"Sex is not informative for age prediction, age distribution seems to be same.","37952167":"<a id = 11><\/a>\n## Correlation between Sibsp - Parch - Age - Fare - Survived","2073f0e6":"<a id=20><\/a>\n# Feature Engineering\n   ","65c00af0":"<a id = 14><\/a>\n## Pclass - Survived","ad6415dd":"1st class passengers are older than 2nd, and 2nd is older tan 3rd class.","39062e3f":"**Explain plot:** <br>\nC's median is near 100. Q's median value is quite low this mean paid less than others probably the class value is also low. S embark's median is low according to C may be class 2. So We can fill with C the blank values.","b918afe1":"* Pclass : class of each passenger (Which class)\n* 1st class has a higher probability of survival than other classes","df4cfc3f":"<a id = \"2\"><\/a>\n# Variable Description","43a614a0":"We will drop the featurs we do not use for ML.","2581fa41":"We split A\/5, PC like those, if there any sign we push X. ","0098873a":"# <font color=\"blue\">**ABOUT TITANIC**","138f76e9":"* We have 263 rows missing age feature.\n* How can we fill it?\n    * Sex : We can look at the average of the male and female\n    * Pclass : We can fill according to class\n    * Parch : We can fill according to number of child\n    * Sex - Pclass - Pclass : We can also fill in a hybrid way","7aa1d3d9":"The best models: Decision Tree, Random Forest and Logistic Regression. Because that models bigger than 0.8","363c61a2":"Fare feature seems to have correlation with survived feature (0.26)","f09f289d":"<a id = 12><\/a>\n## SibSp - Survived","c29aacf8":"<a id=25><\/a>\n## Pclass","e2c0a269":"<a id=9><\/a>\n# Fill Missing Value\n*    Embarked <br> \n* Fare","fea04fb5":"<a id=29><\/a>\n## Train-Test Split","ca83f3b5":"According to the values here, we will divide it into two categories as greater than 4.5 and smaller.","209e13a8":"<a id=28><\/a>\n# Modeling","91f56a6a":"<a id=33><\/a>\n## Prediction and Submission","a86809ba":"*  We can say that if the number of siblings - spouses  is increasing, the probability of survival is lower. \n*  Having a lot of SibSp have less chance to survive. If SibSp 0 or 2, passenger has more chance to survive. \n*  We can consider a new feature describing these categories","2677ddcb":"<a id = 19><\/a>\n## Fill Missing : Age Feature","78afbe2c":"<a id = \"5\"><\/a>\n# Data Analysis\n<br> We will examine the connections of some categories with each other.\n\n<br>Exp:\n<br> \n* Pclass - Survived","d09565ea":"<a id=32><\/a>\n## Ensemble Modeling","9a75abe8":"<a id = \"1\"><\/a>\n# Load Data ","5032441a":"We will try to make better predictions by combining the ml models we have implemented.","c96958dc":"<a id = \"3\"><\/a>\n# Categorical Variable"}}