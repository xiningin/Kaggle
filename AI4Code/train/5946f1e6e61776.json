{"cell_type":{"43faaedf":"code","269fdf88":"code","fdc1989d":"code","e30691ea":"code","54cfc969":"code","417bd7fa":"code","fec37365":"code","c22636a8":"code","aa7e242c":"code","47870ce1":"code","88815ee6":"code","20bdf267":"code","d3b5c1ad":"code","e144acdf":"markdown","97a3b8ec":"markdown","3569610c":"markdown","b2781c27":"markdown","fce69ac9":"markdown","34a95bcd":"markdown","fe80e1d6":"markdown","7bca2fec":"markdown","ac5b5e1f":"markdown","662b7954":"markdown"},"source":{"43faaedf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O\nimport os\nimport datetime\nimport warnings\nimport gc\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb\n\nfrom tqdm.notebook import tqdm\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","269fdf88":"data_path = '..\/input\/random-forest-with-embeddings-tutorial-part-1'\n\nfor dirname, _, filenames in os.walk(data_path):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fdc1989d":"X_train = pd.read_feather(f'{data_path}\/X_train.feather')\ny_train = pd.read_feather(f'{data_path}\/y_train.feather').meter_reading","e30691ea":"y_train","54cfc969":"def RF_wrapper(Xt, yt, Xv, yv, fold=-1):\n    \n    model = RandomForestRegressor(n_jobs=-1, n_estimators=40,\n                              max_samples=200000, max_features=0.5,\n                              min_samples_leaf=5, oob_score=False).fit(Xt, yt)\n    print(f'Training fold {fold}...')\n    \n    score_train = np.sqrt(mean_squared_error(model.predict(Xt), yt))\n    oof = model.predict(Xv)\n    score = np.sqrt(mean_squared_error(oof, yv))\n    print(f'Fold {fold}: training RMSLE: {score_train},   validation RMSLE: {score}\\n')\n    return model, oof, score","417bd7fa":"def perform_CV(model_wrap, xs, ys, n_splits=3):\n    \n    kf = KFold(n_splits=n_splits, shuffle=False)\n\n    models = []\n    scores = []\n    oof_total = np.zeros(xs.shape[0])\n\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(xs), start=1):\n        Xt, yt = xs.iloc[train_idx], ys[train_idx]\n        Xv, yv = xs.iloc[val_idx], ys[val_idx]\n        model, oof, score = model_wrap(Xt, yt, Xv, yv, fold)\n\n        models.append(model)\n        scores.append(score)\n        oof_total[val_idx] = oof\n\n    print('Training completed.')\n    print(f'> Mean RMSLE across folds: {np.mean(scores)}, std: {np.std(scores)}')\n    print(f'> OOF RMSLE: {np.sqrt(mean_squared_error(ys, oof_total))}')\n    return models, scores, oof_total","fec37365":"%%time\nn_splits = 3\nmodels, _, _ = perform_CV(RF_wrapper, X_train, y_train, n_splits=n_splits)","c22636a8":"importance = pd.DataFrame([model.feature_importances_ for model in models],\n                          columns=X_train.columns,\n                          index=[f'Fold {i}' for i in range(1, n_splits + 1)])\nimportance = importance.T\nimportance['Average importance'] = importance.mean(axis=1)\nimportance = importance.sort_values(by='Average importance', ascending=False)\n\nplt.figure(figsize=(10,7))\nsns.barplot(x='Average importance', y=importance.index, data=importance);","aa7e242c":"del X_train\ngc.collect()","47870ce1":"X_embeds = pd.read_feather(f'{data_path}\/X_embeds.feather')","88815ee6":"%%time\nmodels_emb, _, _ = perform_CV(RF_wrapper, X_embeds, y_train, n_splits=n_splits)","20bdf267":"importance = pd.DataFrame([model.feature_importances_ for model in models_emb],\n                          columns=X_embeds.columns,\n                          index=[f'Fold {i}' for i in range(1, n_splits + 1)])\nimportance = importance.T\nimportance['Average importance'] = importance.mean(axis=1)\nimportance = importance.sort_values(by='Average importance', ascending=False)\n\nplt.figure(figsize=(10,7))\nsns.barplot(x='Average importance', y=importance.index, data=importance);","d3b5c1ad":"del X_embeds, y_train\ngc.collect()","e144acdf":"We compare the validation set performance of a random forest on two version of the ASHRAE dataset (preprocessed [in Part 1](https:\/\/www.kaggle.com\/michelezoccali\/random-forest-with-embeddings-tutorial-part-1)), differing in the treatment of categorical variables. These are treated:\n\n1. with **standard ordinal encoding** (discrete levels), and\n2. with **Entity Embeddings**, i.e. vectors of continuous values previously learned by a neural net.","97a3b8ec":"Let us perform k-fold CV, without shuffling as this is a time series. An alternative would be to do a single train\/validation split, possibly with a gap to mimic training\/private split. Otherwise, one could try something like Time-series split CV.","3569610c":"# Modeling","b2781c27":"So, using these mysterious embeddings actually worked! Even if, in this case, it only helped performance a little (and was quite a bit slower too!). However, it is well worth knowing that this method exists, as in general it leads to much better performance. For more information, check out the paper [Entity Embeddings of Categorical Variables](https:\/\/arxiv.org\/pdf\/1604.06737.pdf).\n\nThat's it. Do upvote this kernel if you found it of any use! \ud83d\udd96","fce69ac9":"Let's train the random forest **without** embeddings.","34a95bcd":"# Random forest with Entity Embeddings: Training the model","fe80e1d6":"Let's see the average feature importance across models. We can use this to retroactively drop further superfluous features during preprocessing.","7bca2fec":"# Load data","ac5b5e1f":"Let us write a small wrapper function for the Random Forest, to be passed to a CV routine.","662b7954":"Now let's repeat **with** embeddings."}}