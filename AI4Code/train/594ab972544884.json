{"cell_type":{"d48b54b3":"code","0eaf2638":"code","ee5bbfa0":"code","f3bb2456":"code","54279277":"code","b9c781a3":"code","85c520f3":"code","05971157":"code","056a822a":"code","fb7778d8":"code","9c2e9459":"code","73fb35bc":"code","fff15814":"code","1553e6cf":"code","59754612":"code","5586ca73":"code","e945fbbc":"code","69bf6e0b":"code","a1b808b1":"code","84cefb7b":"code","c226812f":"code","b72240b4":"markdown","0140b295":"markdown","493878c6":"markdown","07d9815f":"markdown","8bd07e89":"markdown","47ab9ad0":"markdown","e5cd14e2":"markdown","804b091b":"markdown","24cdbafa":"markdown"},"source":{"d48b54b3":"# install Segmentation Models \n!pip install -U segmentation-models","0eaf2638":"# required libraries \nimport numpy as np \nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport cv2\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nimport segmentation_models\nsegmentation_models.set_framework('tf.keras')\nfrom segmentation_models import Unet\nfrom segmentation_models import get_preprocessing\nfrom segmentation_models.losses import bce_jaccard_loss\nfrom segmentation_models.metrics import iou_score\n\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt","ee5bbfa0":"def rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background.\n    ref: https:\/\/www.kaggle.com\/inversion\/run-length-decoding-quick-start\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros((shape[0] * shape[1]), dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n    return img.reshape(shape)\n\n\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    ref: https:\/\/www.kaggle.com\/dragonzhang\/positive-score-with-detectron-3-3-inference\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\ndef get_mask(image_id, df):\n    '''\n    Uses rle_decode() to get ndarray from mask using image_id in dataframe (df).\n    ref: https:\/\/www.kaggle.com\/barteksadlej123\/sartors-tf-starter\n    '''\n    current = df[df[\"id\"] == image_id]\n    labels = current[\"annotation\"].tolist()\n    \n    mask = np.zeros((HEIGHT, WIDTH))\n    for label in labels:\n        mask += rle_decode(label, (HEIGHT, WIDTH))\n    mask = mask.clip(0, 1)\n    \n    return mask\n\n\n#  fix overlaps: \n\ndef check_overlap(msk):\n    '''\n    Checks if there are overlap in a mask (msk).\n    ref: https:\/\/www.kaggle.com\/awsaf49\/sartorius-fix-overlap\n    '''\n    msk = msk.astype(np.bool).astype(np.uint8)\n    return np.any(np.sum(msk, axis=-1)>1)\n\n\ndef fix_overlap(msk):\n    '''\n    Args:\n        mask: multi-channel mask, each channel is an instance of cell, shape:(520,704,None)\n    Returns:\n        multi-channel mask with non-overlapping values, shape:(520,704,None) \n    ref: https:\/\/www.kaggle.com\/awsaf49\/sartorius-fix-overlap\n    '''\n    msk = np.array(msk)\n    msk = np.pad(msk, [[0,0],[0,0],[1,0]])\n    ins_len = msk.shape[-1]\n    msk = np.argmax(msk,axis=-1)\n    msk = tf.keras.utils.to_categorical(msk, num_classes=ins_len)\n    msk = msk[...,1:]\n    msk = msk[...,np.any(msk, axis=(0,1))]\n    return msk\n\n\n# make predictions for test set: \n\ndef make_predictions(dataset, num, keras_model, check_overlaps=False):\n    '''\n    For a tf.Dataset, makes predictions for n=num (num =-1 or all_images takes all images in the dataset), \n    images using a keras_model. Returns a list of predicted masks, each as ndarray. \n    '''\n    predictions = []\n    if dataset:\n        for image in dataset.take(num):\n            image = image[None]\n            pred_mask = keras_model.predict(image)\n            # changes shape from (1,512,512,1) to (512,512)\n            pred_mask = pred_mask[0, :, :, 0]\n            # fix overlaps\n            if check_overlaps:\n                if check_overlap(msk=pred_mask):\n                    pred_mask = pred_mask[None]\n                    pred_mask = fix_overlap(msk=pred_mask)\n            # transforms ndarray values to 0s and 1s\n            pred_mask =  np.where( pred_mask > 0.5, 1, 0)\n            predictions.append(pred_mask)\n    return predictions\n\n\ndef display(display_list):\n    '''\n    Displays an example image and mask \n    '''\n    plt.figure(figsize=(20, 20))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n    plt.show()\n\n    \n# functions to visualize predictions:\n\ndef create_mask(pred_mask):\n    '''Converts predicted mask values to 0s and 1s'''\n    pred_mask = tf.where(pred_mask > 0.5,1,0)\n    return pred_mask\n\ndef show_predictions(keras_model, dataset=None, num=1):\n    '''\n    Shows N=num predictions examples from dataset and a keras_model \n    '''\n    if dataset:\n        for image, mask in dataset.take(num):\n            pred_mask = keras_model.predict(image)\n            display([image[0], mask[0], create_mask(pred_mask[0])])\n    else:\n        display([sample_image, sample_mask,\n                 create_mask(model.predict(sample_image[tf.newaxis, ...])[0])])\n","f3bb2456":"# constants\n\nDEBUG = False\n\nSEED = 123\nWIDTH, HEIGHT = 704, 520\nRESIZE_WIDTH, RESIZE_HEIGHT = 512, 512\nBATCH_SIZE = 4\nBUFFER_SIZE = 32\n\n## in a segmentation task each pixel is given a class \n## OUTPUT_CLASSES: number of classes that can be assigned to each pixel \nOUTPUT_CLASSES = 1\n\n# architecture backbone for segmentation model\nBACKBONE = 'densenet121'\n\nVAL_SPLIT = 0.2\n\nAUTO = tf.data.AUTOTUNE\n\nEPOCHS = 100","54279277":"# paths\n\n# input\nDIR = '..\/input\/sartorius-cell-instance-segmentation'\ntrain_csv = os.path.join(DIR,'train.csv') \ntrain_path =  os.path.join(DIR, 'train\/')\ntest_path = os.path.join(DIR, 'test\/')\n\n# output \ncsv_output = os.path.join('.\/', 'submission.csv') \nmodel_output = os.path.join('.\/', 'unet_keras_' + BACKBONE + '_backbone.h5')","b9c781a3":"# train and validation split\ntrain = pd.read_csv(train_csv)\ntrain.head()\n\nn_ids = train.id.nunique()\n\nif DEBUG:\n    unique_ids_train = list(set(train['id'].tolist()))[:BATCH_SIZE]\n    unique_ids_valid = list(set(train['id'].tolist()))[BATCH_SIZE:2*BATCH_SIZE]\nelse:\n    unique_ids_train = list(set(train['id'].tolist()))[:int(n_ids * (1 - VAL_SPLIT))]\n    unique_ids_valid = list(set(train['id'].tolist()))[int(n_ids * (1 - VAL_SPLIT)):]\n\n\ntemp = pd.DataFrame()\nfor sample_id in unique_ids_train:\n    query = train[train.id == sample_id]\n    temp = pd.concat([temp, query])\ntrain = temp\ntrain = train.reset_index(drop=True)\n\ntemp = pd.DataFrame()\nfor sample_id in unique_ids_valid:\n    query = train[train.id == sample_id]\n    temp = pd.concat([temp, query])\nvalid = temp\nvalid = train.reset_index(drop=True)\n    \nTRAIN_LENGTH = train['id'].nunique()\nSTEPS_PER_EPOCH = TRAIN_LENGTH \/\/ BATCH_SIZE\n\nVALID_LENGTH = valid['id'].nunique()\nVALIDATION_STEPS = VALID_LENGTH \/\/ BATCH_SIZE","85c520f3":"# training\/validation data generator \n\npreprocess_input = get_preprocessing(BACKBONE)\n\ndef train_generator(df):\n    image_ids = set(df['id'].tolist())\n    \n    for image_id in image_ids:\n        \n        image = cv2.imread(os.path.join(train_path, image_id) + '.png')\n        image = preprocess_input(image)\n        image = cv2.resize(image, (RESIZE_HEIGHT, RESIZE_WIDTH))\n\n        mask = get_mask(image_id, df)        \n        mask = cv2.resize(mask, (RESIZE_HEIGHT, RESIZE_WIDTH))\n        \n        mask = mask.reshape((*mask.shape, 1))\n        \n        image = image.astype(np.float32)\n        mask = mask.astype(np.float32)\n        \n        yield image, mask","05971157":"# use the generator to get training and validation sets\ntrain_ds = tf.data.Dataset.from_generator(\n    lambda : train_generator(train), \n    output_types=(tf.float32, tf.float32),\n    output_shapes=((RESIZE_HEIGHT, RESIZE_WIDTH, 3), (RESIZE_HEIGHT, RESIZE_WIDTH, 1)))\n\nvalid_ds = tf.data.Dataset.from_generator(\n    lambda : train_generator(valid), \n    output_types=(tf.float32, tf.float32),\n    output_shapes=((RESIZE_HEIGHT, RESIZE_WIDTH, 3), (RESIZE_HEIGHT, RESIZE_WIDTH, 1)))\n","056a822a":"# \"the following class performs a simple augmentation by randomly-flipping an image\"\nclass Augment(tf.keras.layers.Layer):\n    def __init__(self, seed=SEED):\n        super().__init__()\n        \n        self.augment_inputs = preprocessing.RandomFlip('horizontal', seed=seed)\n        self.augment_labels = preprocessing.RandomFlip('horizontal', seed=seed)\n        \n    def call(self, inputs, labels):\n        inputs = self.augment_inputs(inputs)\n        labels = self.augment_labels(labels)\n        return inputs, labels","fb7778d8":"# \"build the input pipeline, applying the augmentation after batching the inputs\"\n# for augmentation only add .map(Augment()) after .repeat() \n\ntrain_ds = (\n    train_ds\n    .shuffle(BUFFER_SIZE)\n    .batch(BATCH_SIZE)\n    .repeat()\n    .map(Augment())\n    .prefetch(AUTO))\n\nvalid_ds = (\n    valid_ds\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO))","9c2e9459":"# \"visualize an image example and its corresponding mask from the dataset\"    \nfor images, masks in train_ds.take(1):\n    sample_image, sample_mask = images[0], masks[0]\n    display([sample_image, sample_mask])","73fb35bc":"# model defininition\n\n# \"encoder_freeze: if True set all layers of encoder (backbone model) as non-trainable\"  \nmodel = Unet(backbone_name=BACKBONE, \n                encoder_weights='imagenet', \n                activation='sigmoid',\n                classes=OUTPUT_CLASSES,\n                encoder_freeze=True)","fff15814":"# compile model \n\n# optimizer \nopt = keras.optimizers.Adam(learning_rate=1e-3)\n\n# loss from Segmentation Models\n# can be customized using sm.segmentation_models.losses.JaccardLoss()\n# metrics from Segmentation Models, \n# can be customized using sm.segmentation_models.metrics.IOUScore() function\nmodel.compile(optimizer=opt,\n              loss=bce_jaccard_loss,\n              metrics=[iou_score])","1553e6cf":"# training\n\n# \"observe how the model improves while it is training\"\nclass DisplayCallback(tf.keras.callbacks.Callback):\n    def __init__(self):\n        super().__init__()\n    def on_epoch_end(self, epoch, logs=None):\n        clear_output(wait=False)\n        show_predictions(keras_model=model)\n        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n# display callback defined above\ndisplay_cb = DisplayCallback()\n\n\n# \"save the Keras model or model weights at some frequency\"\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    model_output,\n    save_best_only=True,\n    save_weights_only=False,\n)\n\n# \"reduce learning rate when a metric has stopped improving\"\n# documentation: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/ReduceLROnPlateau\nlr_reduce = tf.keras.callbacks.ReduceLROnPlateau( monitor='val_loss', factor=0.1, patience=10, verbose=0,\n    mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n\nmodel_history = model.fit(train_ds, epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_steps=VALIDATION_STEPS,\n                          validation_data=valid_ds,\n                          callbacks=[display_cb, model_checkpoint, lr_reduce])","59754612":"# plot training curve\nloss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\nplt.figure()\nplt.plot(model_history.epoch, loss, 'r', label='Training loss')\nplt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss Value')\nplt.ylim([0, 1])\nplt.legend()\nplt.show()","5586ca73":"# test data generator \ntest_ids = [  os.path.join(test_path, each)  for each in os.listdir(test_path) if each.endswith('.png')]\ndef test_generator(image_ids):\n    for image_id in image_ids:\n        image = cv2.imread(image_id) \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)        \n        image = cv2.resize(image, (RESIZE_HEIGHT, RESIZE_WIDTH))\n        image = image.astype(np.float32)\n        yield image","e945fbbc":"# test dataset from test data generator \ntest_ds = tf.data.Dataset.from_generator(\n    lambda : test_generator(test_ids), \n    output_types=(tf.float32),\n    output_shapes=((RESIZE_HEIGHT, RESIZE_WIDTH, 3)) )","69bf6e0b":"# test image ids and predictions\ntest_predictions = make_predictions(dataset=test_ds, \n                                    num=len(test_ids), \n                                    keras_model=model,\n                                    check_overlaps=True)","a1b808b1":"# encode predections in the RL format\ntest_predictions = [rle_encode(mask) for mask in test_predictions] ","84cefb7b":"# transform full image paths to ids \ntest_ids = [Path(ID).stem for ID in test_ids]","c226812f":"# generate submission data frame \nsubmisssion = pd.DataFrame.from_dict({'id': test_ids, 'predicted': test_predictions} )\nsubmisssion = submisssion.sort_values( ['id'], ascending=True )\nprint(submisssion.head(), 'n')\nsubmisssion.to_csv(csv_output, index=False)","b72240b4":"<a id=\"section-2\"><\/a>\n### 2. Functions","0140b295":"<a id=\"section-5\"><\/a>\n### 5. Define the model ","493878c6":"<a id=\"section-1\"><\/a>\n### 1. Libraries and paths","07d9815f":"## Description \nThis notebook contains a basic implementation of a Keras image segmentation model for the [Sartorius - Cell Instance Segmentation competition](https:\/\/www.kaggle.com\/c\/sartorius-cell-instance-segmentation\/data) using \n[Segmentation Models: \"Python library with Neural Networks for Image Segmentation based on Keras and TensorFlow. \"](https:\/\/github.com\/qubvel\/segmentation_models). This library contains four models architectures for image segmentation: UNet, LinkNet, PSPNet, and FPN.\n\nHere a UNet with a DenseNet121 backbone is implemented, but you could try the other models (LinkNet, PSPNet, or FPN) using the 25 available backbones (such as ResNet, MobileNet, DenseNet, etc.). By now, this implementation is unable to achieve a good score (> 0.0), it only provides a way for test different segmentation models with the competition's dataset. It also uses Jaccard loss and IOU score for training. \n\n## Problem definition\nData: \n\n[Phase-contrast microscopy](https:\/\/en.wikipedia.org\/wiki\/Phase-contrast_microscopy) images of human neuronal cell  types along with annotations (labels) representing cell segmentations. \n\nAim: \n\nThe trained model should be able to predict the annotations for cell segmentation, including rare cell types (such as neuroblastoma cell line SH-SY5Y as discussed in the [competition description](https:\/\/www.kaggle.com\/c\/sartorius-cell-instance-segmentation\/overview)). Annotations should be provided in run-length format (see functions rle_decode() and rle_encode below).\n\n## Approach \n[Segmentation models library](https:\/\/github.com\/qubvel\/segmentation_models): \n\n\"The main features of this library are:\n\n*     High level API (just two lines of code to create model for segmentation)\n*     4 models architectures for binary and multi-class image segmentation (including legendary Unet)\n*     25 available backbones for each architecture\n*     All backbones have pre-trained weights for faster and better convergence\n*     Helpful segmentation losses (Jaccard, Dice, Focal) and metrics (IoU, F-score)\"\n\n\nReference notebooks:\n1. [Cell Segmentation - Run Length Decoding](https:\/\/www.kaggle.com\/ihelon\/cell-segmentation-run-length-decoding).\n2. [Sartors TF starter](https:\/\/www.kaggle.com\/barteksadlej123\/sartors-tf-starter).\n3. [Positive score with Detectron 3\/3 - Inference](https:\/\/www.kaggle.com\/dragonzhang\/positive-score-with-detectron-3-3-inference).\n\nOther references: \n1. [Tensorflow Tutorials: Image segmentation ](https:\/\/www.tensorflow.org\/tutorials\/images\/segmentation).\n2. [Segmentation models documentation](https:\/\/segmentation-models.readthedocs.io\/en\/latest\/).\n","8bd07e89":"<a id=\"section-4\"><\/a>\n### 4. Generate train and validation data sets","47ab9ad0":"## Workflow\n\n[1. Imports](#section-1)\n\n[2. Functions](#section-2)\n\n[3. Constants](#section-3)\n\n[4. Generate train and validation data sets](#section-4)\n\n[5. Define the model](#section-5)\n\n[6. Training](#section-6)\n\n[7. Test set predictions](#section-7)\n\n","e5cd14e2":"<a id=\"section-7\"><\/a>\n### 7. Test set predictions","804b091b":"<a id=\"section-6\"><\/a>\n### 6. Training","24cdbafa":"<a id=\"section-3\"><\/a>\n### 3. Constants "}}