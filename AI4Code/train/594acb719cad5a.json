{"cell_type":{"cb668072":"code","7326be97":"code","6adf51d7":"code","a09c67b2":"code","7851f7a8":"code","dafff403":"code","e9222c21":"code","8363e200":"code","0d5ffc76":"code","f3713a69":"code","37152bf1":"code","12af2029":"code","d39e9e2c":"code","33a00197":"code","9e8a05b8":"code","7d90680d":"code","3e978640":"code","8a26a7d4":"code","32f88570":"code","e0a030b3":"code","e01f3f3a":"code","95785add":"code","34ce24ad":"code","5a6b2e81":"code","36231255":"code","3b98b385":"code","4060f3b7":"code","bd70fdd8":"markdown","b8024da3":"markdown","5f07b5e4":"markdown","c0b37003":"markdown","e41301ae":"markdown"},"source":{"cb668072":"\"\"\"\ntorch modules\nwith ofc  numpy  and pandas\n\n\"\"\"\n\nimport pandas as pd\nimport numpy as np \n\nfrom torch import nn\nimport torch\nfrom torchtext import data\nfrom torch.nn  import functional as F\nimport torch.optim as  optim \nif torch.cuda.is_available():  \n  dev = \"cuda:0\" \n\n  print(\"gpu up\")\nelse:  \n  dev = \"cpu\"  \ndevice = torch.device(dev)","7326be97":"import random\nSEED= 32","6adf51d7":"\"\"\"\nregex and the tokenizers\n\"\"\"\n\nimport re\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.lang.en import English\nfrom spacy.lang.ar import Arabic\nfrom nltk.translate.bleu_score import sentence_bleu\n\nenNLP = English()\narNLP = Arabic()\n\nenTokenizer = Tokenizer(enNLP.vocab)\narTokenizer =  Tokenizer(arNLP.vocab)\n\n\n","a09c67b2":"df = pd.read_csv(\"\/kaggle\/input\/arabic-to-english-translation-sentences\/ara_eng.txt\",delimiter=\"\\t\",names=[\"eng\",\"ar\"])","7851f7a8":"df","dafff403":"\"\"\"\ndefining the tokenizers for arabic and english  \n\ncreating the fields for the dataset from torchtext \nthat class is the simple way I could find for turning a df into a torch dataset\n\n\u0646\u0647\u0647\u0627 and \u0628\u0628\u062f\u0623 are just arbitrary words for init and end of sentence tokens  \nfor some reason when I choose an arabic word for the unknown token  the vocab doesn't replace words that are not in the vocab  \n\"\"\"\n\ndef myTokenizerEN(x):\n return  [word.text for word in \n          enTokenizer(re.sub(r\"\\s+\\s+\",\" \",re.sub(r\"[\\.\\'\\`\\\"\\r+\\n+]\",\" \",x.lower())).strip())]\ndef myTokenizerAR(x):\n return  [word.text for word in \n          arTokenizer(re.sub(r\"\\s+\\s+\",\" \",re.sub(r\"[\\.\\'\\`\\\"\\r+\\n+]\",\" \",x.lower())).strip())]\n\nSRC = data.Field(tokenize=myTokenizerEN,batch_first=False,init_token=\"<sos>\",eos_token=\"<eos>\")\nTARGET = data.Field(tokenize=myTokenizerAR,batch_first=False,tokenizer_language=\"ar\",init_token=\"\u0628\u0628\u062f\u0623\",eos_token=\"\u0646\u0647\u0647\u0627\")\n\nclass DataFrameDataset(data.Dataset):\n\n    def __init__(self, df, src_field, target_field, is_test=False, **kwargs):\n        fields = [('eng', src_field), ('ar',target_field)]\n        examples = []\n        for i, row in df.iterrows():\n            eng = row.eng \n            ar = row.ar\n            examples.append(data.Example.fromlist([eng, ar], fields))\n\n        super().__init__(examples, fields, **kwargs)\n\n        \ntorchdataset = DataFrameDataset(df,SRC,TARGET)\n","e9222c21":"train_data, valid_data = torchdataset.split(split_ratio=0.8, random_state = random.seed(SEED))","8363e200":"SRC.build_vocab(train_data,min_freq=2)\nTARGET.build_vocab(train_data,min_freq=2)  ","0d5ffc76":"\n\n#Commonly used words\nprint(TARGET.vocab.freqs.most_common(10))  ","f3713a69":"\n\"\"\"\nwe are using batches for validation and test set because of memory usage we can't pass the whole set at once\n\ntry lowering the batch size if you are out of memory \n\"\"\"\nBATCH_SIZE = 64\n\n\n\ntrain_iterator,valid_iterator = data.BucketIterator.splits(\n    (train_data,valid_data), \n    batch_size = BATCH_SIZE,\n    device = device,\n    sort=False,\n    sort_within_batch=False,\nshuffle=True)","37152bf1":"\"\"\"\nto point out one thing about the transformer what it could do is to enable \ntraining on the whole sequence at once but on really using it for translation it predicts the next word \nthen it feeds the prediction into the sequence again until the model predict <eos> token (with a max length ofc)\n\n\"\"\"\nclass TranslateTransformer(nn.Module):\n    def __init__(\n        self,\n        embedding_size,\n        src_vocab_size,\n        trg_vocab_size,\n        src_pad_idx,\n        num_heads,\n        num_encoder_layers,\n        num_decoder_layers,\n        max_len,\n    ):\n        super(TranslateTransformer, self).__init__()\n        self.srcEmbeddings = nn.Embedding(src_vocab_size,embedding_size)\n        self.trgEmbeddings= nn.Embedding(trg_vocab_size,embedding_size)\n        self.srcPositionalEmbeddings= nn.Embedding(max_len,embedding_size)\n        self.trgPositionalEmbeddings= nn.Embedding(max_len,embedding_size)\n        self.transformer = nn.Transformer(\n            embedding_size,\n            num_heads,\n            num_encoder_layers,\n            num_decoder_layers,\n        )\n        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n        self.dropout = nn.Dropout(0.1)\n        self.src_pad_idx = src_pad_idx\n        self.max_len = max_len\n    \n    def make_src_mask(self, src):\n        src_mask = src.transpose(0,1) == self.src_pad_idx\n\n        return src_mask.to(device)\n\n    def forward(self,x,trg):\n        src_seq_length = x.shape[0]\n        N = x.shape[1]\n        trg_seq_length = trg.shape[0]\n        #adding zeros is an easy way\n        src_positions = (\n            torch.arange(0, src_seq_length)\n            .reshape(src_seq_length,1)  + torch.zeros(src_seq_length,N) \n        ).to(device)\n        \n        trg_positions = (\n            torch.arange(0, trg_seq_length)\n            .reshape(trg_seq_length,1)  + torch.zeros(trg_seq_length,N) \n        ).to(device)\n\n\n        srcWords = self.dropout(self.srcEmbeddings(x.long()) +self.srcPositionalEmbeddings(src_positions.long()))\n        trgWords = self.dropout(self.trgEmbeddings(trg.long())+self.trgPositionalEmbeddings(trg_positions.long()))\n        \n        src_padding_mask = self.make_src_mask(x)\n        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(device)\n        \n        \n        out = self.transformer(srcWords,trgWords, src_key_padding_mask=src_padding_mask,tgt_mask=trg_mask )\n        out= self.fc_out(out)\n        return out\n        \n\n","12af2029":"#No. of unique tokens in text\nsrc_vocab_size  = len(SRC.vocab)\nprint(\"Size of english vocabulary:\",src_vocab_size)\n\n#No. of unique tokens in label\ntrg_vocab_size =len(TARGET.vocab)\nprint(\"Size of arabic vocabulary:\",trg_vocab_size)\n\nnum_heads = 8\nnum_encoder_layers = 3\nnum_decoder_layers = 3\n\nmax_len= 227\nembedding_size= 256\nsrc_pad_idx =SRC.vocab.stoi[\"<pad>\"]\n\n\nmodel = TranslateTransformer(\n    embedding_size,\n    src_vocab_size,\n    trg_vocab_size,\n    src_pad_idx,\n    num_heads,\n    num_encoder_layers,\n    num_decoder_layers,\n    max_len\n).to(device)","d39e9e2c":"loss_track = []\nloss_validation_track= []","33a00197":"\"\"\"\nI'm using adagrad because it assigns bigger updates to less frequently updated weights so \nso thought it could be useful for words not used a lot \n\"\"\"\n\noptimizer = optim.Adagrad(model.parameters(),lr = 0.003)\nEPOCHS = 60\n\n\npad_idx = SRC.vocab.stoi[\"<pad>\"]\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx) \n\nfor i in range(50,EPOCHS):\n    stepLoss=[]\n    model.train() # the training mode for the model (applies dropout and batchnorms)\n    for batch  in train_iterator:\n        input_sentence = batch.eng.to(device)\n        trg = batch.ar.to(device)\n\n        optimizer.zero_grad()\n        out = model(input_sentence,trg[:-1])\n        out = out.reshape(-1,trg_vocab_size)\n        trg = trg[1:].reshape(-1)\n        loss = criterion(out,trg)\n        \n        \n        loss.backward()\n        optimizer.step()\n        stepLoss.append(loss.item())\n        \n\n    loss_track.append(np.mean(stepLoss))\n    print(\"train crossentropy at epoch {} loss: \".format(i),np.mean(stepLoss))\n    \n    stepValidLoss=[]\n    model.eval() # the evaluation mode for the model (doesn't apply dropout and batchNorm)\n    for batch  in valid_iterator:\n        input_sentence = batch.eng.to(device)\n        trg = batch.ar.to(device)\n\n        optimizer.zero_grad()\n        out = model(input_sentence,trg[:-1])\n        out = out.reshape(-1,trg_vocab_size)\n        trg = trg[1:].reshape(-1)\n        loss = criterion(out,trg)\n        \n        stepValidLoss.append(loss.item())\n  \n    loss_validation_track.append(np.mean(stepValidLoss))\n    print(\"validation crossentropy at epoch {} loss: \".format(i),np.mean(stepValidLoss))\n    \n    \n        ","9e8a05b8":"import matplotlib.pyplot as plt \n\n#the train loss after 50 epoch\nplt.figure(figsize=(10,5))\nplt.plot(range(60),loss_track,label=\"train loss\")\nplt.plot(range(60),loss_validation_track,label=\"valiadtion loss\")\nplt.legend()\nplt.show()","7d90680d":"\"\"\"\nthis function takes some arguments and returns the translated arabic sentence \n\n\"\"\"\n\ndef translate(model,sentence,srcField,targetField,srcTokenizer):\n    model.eval()\n    processed_sentence = srcField.process([srcTokenizer(sentence)]).to(device)\n    trg = [\"\u0628\u0628\u062f\u0623\"]\n    for _ in range(60):\n        \n        trg_indecies = [targetField.vocab.stoi[word] for word in trg]\n        outputs = torch.Tensor(trg_indecies).unsqueeze(1).to(device)\n        outputs = model(processed_sentence,outputs)\n        \n        if targetField.vocab.itos[outputs.argmax(2)[-1:].item()] == \"<unk>\":\n            continue \n        trg.append(targetField.vocab.itos[outputs.argmax(2)[-1:].item()])\n        if targetField.vocab.itos[outputs.argmax(2)[-1:].item()] == \"\u0646\u0647\u0647\u0627\":\n            break\n    return \" \".join([word for word in trg if word != \"<unk>\"][1:-1])\n    \n    ","3e978640":"translate(model,\"I'm happy\" ,SRC,TARGET,myTokenizerEN)","8a26a7d4":"translate(model,\"what do you want\" ,SRC,TARGET,myTokenizerEN) ","32f88570":"translate(model,\"what do you like to have \" ,SRC,TARGET,myTokenizerEN) ","e0a030b3":"translate(model,\"I am going outside\" ,SRC,TARGET,myTokenizerEN) ","e01f3f3a":"translate(model,\"he is here\" ,SRC,TARGET,myTokenizerEN) ","95785add":"translate(model,\"he is not here\" ,SRC,TARGET,myTokenizerEN) ","34ce24ad":"translate(model,\"I need help\" ,SRC,TARGET,myTokenizerEN)","5a6b2e81":"translate(model,\"I'm not at home\" ,SRC,TARGET,myTokenizerEN)  # it totally get negation right \n","36231255":"translate(model,\"I'm ready\" ,SRC,TARGET,myTokenizerEN)\n","3b98b385":"print(translate(model,\"it's not important\" ,SRC,TARGET,myTokenizerEN) )","4060f3b7":"\"\"\"\nand the model also can't handle any slightly complicated sentences \n\"\"\"\ntranslate(model,\"he apologized for the mistake he did\" ,SRC,TARGET,myTokenizerEN)","bd70fdd8":"for this notebook we will build a pytorch transformer to translate english sentences into arabic,\nmain limitation here is lack of publicly available data","b8024da3":"as the model doesn't have a significant performance, I think it needs more epochs and much more data (maybe it needs to be bigger also but we have limited ram ) ","5f07b5e4":"I will be using torchtext it's really good for preparing the data, it has a good documentation also those are links about an example using torchtext and a tutorial\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/first-text-classification-in-pytorch\/\n\nhttp:\/\/mlexplained.com\/2018\/02\/08\/a-comprehensive-tutorial-to-torchtext\/","c0b37003":"to understand how the transformer works, I recommend these videos \n\nhttps:\/\/www.youtube.com\/watch?v=U0s0f995w14&t=2522s\n\nhttps:\/\/www.youtube.com\/watch?v=M6adRGJe5cQ&t=1781s","e41301ae":"I tried doing more than 60 epochs but it turned into copying and pasting the training examples "}}