{"cell_type":{"cf3a5737":"code","9727eb65":"code","508dc868":"code","b4a6b633":"code","0b848755":"code","d1aee633":"code","fd1f817c":"code","8afc5667":"code","b5bddad5":"code","aa8c9ec6":"code","eb181087":"code","158176f7":"code","eae4078d":"code","7b56427f":"code","4fbfbf5d":"code","db0865cc":"code","75f55923":"code","319ef8cc":"code","fdb7c022":"code","442bd4f5":"code","3dc93665":"code","f51f0c32":"code","4d772f51":"code","53643e22":"code","0b84d9eb":"code","0c22e4a5":"code","f2ca8fb7":"code","2631345d":"code","2dde648f":"code","a5314020":"code","490cf817":"code","c4baf2ae":"code","216344f8":"code","bd2d1ea4":"code","b6a3c894":"code","466098fd":"code","8a0355b9":"code","0d263a84":"markdown"},"source":{"cf3a5737":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9727eb65":"from __future__ import absolute_import, division, print_function, unicode_literals\ntry:\n  # %tensorflow_version only exists in Colab.\n  %tensorflow_version 2.x\nexcept Exception:\n  pass\nimport tensorflow as tf\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nmpl.rcParams['axes.grid'] = False","508dc868":"tf.__version__","b4a6b633":"train = pd.read_csv('\/kaggle\/input\/ltfs-2\/train_fwYjLYX.csv')\ntest = pd.read_csv('\/kaggle\/input\/ltfs-2\/test_1eLl9Yf.csv')","0b848755":"train['application_date']=pd.to_datetime(train['application_date'],format=\"%Y-%m-%d\")\ntest['application_date']=pd.to_datetime(test['application_date'],format=\"%Y-%m-%d\")","d1aee633":"train=train.groupby(['application_date','segment']).sum().reset_index()\n\n","fd1f817c":"train['weekday']=train['application_date'].apply(lambda x : x.weekday())\ntrain['day']=train['application_date'].apply(lambda x : x.day)\ntrain['month']=train['application_date'].apply(lambda x : x.month)\ntrain['year']=train['application_date'].apply(lambda x : x.year)\n\ntest['weekday']=test['application_date'].apply(lambda x : x.weekday())\ntest['day']=test['application_date'].apply(lambda x : x.day)\ntest['month']=test['application_date'].apply(lambda x : x.month)\ntest['year']=test['application_date'].apply(lambda x : x.year)","8afc5667":"train_seg1 = train[train['segment']==1]\ntrain_seg2 = train[train['segment']==2]","b5bddad5":"idx = pd.date_range('2017-04-01', '2019-07-05')\ntrain_seg1.index=train_seg1.application_date\ntrain_seg1.drop(['application_date'],axis=1,inplace=True)\ntrain_seg1=train_seg1.reindex(idx,method='ffill')\ntrain_seg1.head()","aa8c9ec6":"idx = pd.date_range('2017-04-01', '2019-07-23')\ntrain_seg2.index=train_seg2.application_date\ntrain_seg2.drop(['application_date'],axis=1,inplace=True)\ntrain_seg2=train_seg2.reindex(idx,method='ffill')\ntrain_seg2.head()","eb181087":"idx = pd.date_range('2019-07-06', '2019-10-24')\ntest.index=test.application_date\ntest.drop(['application_date'],axis=1,inplace=True)\n#test=test.reindex(idx,method='ffill')\ntest.head()","158176f7":"features_considered = ['case_count','weekday','day','month','year']\ntarget = ['case_count']","eae4078d":"\ndef split_sequence(sequence, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n    # find the end of this pattern\n        end_ix = i + n_steps\n    # check if we are beyond the sequence\n        if end_ix > len(sequence)-1:\n            break\n    # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return np.array(X), np.array(y)","7b56427f":"raw_seq_s1 = list(train_seg1['case_count'])\nn_steps = 9\nX_s1, y_s1 = split_sequence(raw_seq_s1, n_steps)","4fbfbf5d":"raw_seq_s2 = list(train_seg2['case_count'])\nn_steps = 9\nX_s2, y_s2 = split_sequence(raw_seq_s2, n_steps)","db0865cc":"initial_input_s1 = list(X_s1[-1])\ninitial_input_s2 = list(X_s2[-1])","75f55923":"initial_input_s2","319ef8cc":"test_s1 = test[test['segment']==1]\ntest_s2 = test[test['segment']==2]","fdb7c022":"test_s1.shape","442bd4f5":"X_s1.shape","3dc93665":"n_features = 1\nn_seq = 3\nn_steps = 3\nX_s1 = X_s1.reshape((X_s1.shape[0], n_seq, n_steps, n_features))\nX_s2 = X_s2.reshape((X_s2.shape[0], n_seq, n_steps, n_features))","f51f0c32":"X_s1.shape","4d772f51":"X_s2.shape","53643e22":"train_seg1.head()","0b84d9eb":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(filters=64, kernel_size=1, activation='relu'),input_shape=(None, n_steps, n_features)))\nmodel.add(tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(pool_size=2)))\nmodel.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()))\nmodel.add(tf.keras.layers.LSTM(50, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1))\n\nmodel.compile(optimizer='adam', loss='mse')\n\nmodel.fit(X_s1, y_s1, epochs=1000,verbose=0)","0c22e4a5":"test_s1.shape[0]","f2ca8fb7":"results_s1 = []\nn_steps_1=9\nn_steps_2 = 3\nfor i in range(test_s1.shape[0]):\n    val = np.array(initial_input_s1[i:i+n_steps_1])\n    #print(val)\n    #val = val.reshape((1, n_steps, n_features))\n    val = val.reshape((1, n_seq, n_steps_2, n_features))\n    yhat = model.predict(val)\n    initial_input_s1.append(yhat[0][0])\n    results_s1.append(yhat[0][0])","2631345d":"results_s1","2dde648f":"train_seg1.tail(10)","a5314020":"model2 = tf.keras.Sequential()\nmodel2.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(filters=64, kernel_size=1, activation='relu'),input_shape=(None, n_steps, n_features)))\nmodel2.add(tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling1D(pool_size=2)))\nmodel2.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()))\nmodel2.add(tf.keras.layers.LSTM(50, activation='relu'))\nmodel2.add(tf.keras.layers.Dense(1))\n\nmodel2.compile(optimizer='adam', loss='mse')\n\nmodel2.fit(X_s2, y_s2, epochs=1000,verbose=0)","490cf817":"results_s2 = []\nn_steps_1=9\nn_steps_2 = 3\nfor i in range(test_s2.shape[0]):\n    val = np.array(initial_input_s2[i:i+n_steps_1])\n    #print(val)\n    #val = val.reshape((1, n_steps, n_features))\n    val = val.reshape((1, n_seq, n_steps_2, n_features))\n    yhat = model2.predict(val)\n    initial_input_s2.append(yhat[0][0])\n    results_s2.append(yhat[0][0])","c4baf2ae":"results_s2","216344f8":"train_seg2.tail()","bd2d1ea4":"test_s1['case_count']= results_s1\ntest_s2['case_count']= results_s2\nsubmission = pd.concat([test_s1,test_s2])","b6a3c894":"submission = submission.reset_index().sort_values('id')[['id','application_date','segment','case_count']]\nsubmission.head()","466098fd":"submission.to_csv('submission.csv',index=False)","8a0355b9":"submission.shape","0d263a84":"# Time series forecasting"}}