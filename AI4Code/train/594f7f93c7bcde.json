{"cell_type":{"f5c60c01":"code","4e6d97a8":"code","449423a5":"code","0a8e6f6d":"code","2f475025":"code","e791d230":"code","0642605d":"code","b6ab6540":"code","30c3e72b":"code","5e67a74e":"code","c5b58af8":"code","f2b777f5":"code","563b15ef":"code","0b1649ec":"code","8bf18ef3":"code","921c01b2":"code","b0b60b39":"code","674bdbae":"code","355fc13d":"code","247e2bbc":"code","693134a2":"code","458b514a":"markdown","c739655f":"markdown","3765f67b":"markdown","778e247b":"markdown","ca6ae2cb":"markdown","192f0391":"markdown","6f9d949b":"markdown","ad491053":"markdown","3672353b":"markdown","41c34a14":"markdown","9a0f9e3e":"markdown","5093d069":"markdown","ec230c06":"markdown","fcea5736":"markdown","cdb3e761":"markdown","c417db60":"markdown"},"source":{"f5c60c01":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import beta, bernoulli\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\nimport random\nimport math\n\nRANDOM_SEED = 123\nnp.random.seed(RANDOM_SEED)\ninit_notebook_mode(connected=True)","4e6d97a8":"def algorithm_performance():\n    \"\"\"\n    Function that will show the performance of each algorithm we will be using in this tutorial.\n    \"\"\"\n    \n    ## calculate how many time each Ad has been choosen\n    count_series = pd.Series(index_list).value_counts(normalize=True)\n    print('Ad #0 has been shown', count_series[0]*100, '% of the time.')\n    print('Ad #1 has been shown', count_series[1]*100, '% of the time.')\n    \n    print('Total Reward (Number of Clicks):', total_reward) ## print total Reward\n    \n    x = np.arange (0, n, 1)\n\n    ## plot the calculated CTR for Ad #0\n    data1 = go.Scatter(x=x,\n                       y=ctr[0],\n                       name='Calculated CTR #0',\n                       line=dict(color=('rgba(10, 108, 94, .7)'),\n                                 width=2))\n\n    ## plot the line with actual CTR for Ad #0\n    data2 = go.Scatter(x=[0, n],\n                       y=[ACTUAL_CTR[0]] * 2,\n                       name='Actual CTR #0 value',\n                       line = dict(color = ('rgb(205, 12, 24)'),\n                                   width = 1,\n                                   dash = 'dash'))\n\n    ## plot the calculated CTR for Ad #1\n    data3 = go.Scatter(x=x,\n                       y=ctr[1],\n                       name='Calculated CTR #1',\n                       line=dict(color=('rgba(187, 121, 24, .7)'),\n                                 width=2))\n\n    ## plot the line with actual CTR for Ad #0\n    data4 = go.Scatter(x=[0, n],\n                       y=[ACTUAL_CTR[1]] * 2,\n                       name='Actual CTR #1 value',\n                       line = dict(color = ('rgb(205, 12, 24)'),\n                                   width = 1,\n                                   dash = 'dash'))\n\n    ## plot the Regret values as a function of trial number\n    data5 = go.Scatter(x=x,\n                       y=regret_list,\n                       name='Regret')\n\n    layout = go.Layout(title='Simulated CTR Values and Algorithm Regret',\n                       xaxis={'title': 'Trial Number'},\n                       yaxis1={'title': 'CTR value'},\n                       yaxis2={'title': 'Regret Value'}\n                       )\n    fig = tools.make_subplots(rows=2, cols=1, print_grid=False, shared_yaxes=True, shared_xaxes=True)\n\n    fig.append_trace(data1, 1, 1)\n    fig.append_trace(data2, 1, 1)\n    fig.append_trace(data3, 1, 1)\n    fig.append_trace(data4, 1, 1)\n    fig.append_trace(data5, 2, 1)\n\n    fig['layout'].update(layout)\n    iplot(fig, show_link=False)","449423a5":"ACTUAL_CTR = [.45, .65]\nprint('Actual CTR for Ad #0 is:', ACTUAL_CTR[0])\nprint('Actual CTR for Ad #1 is:', ACTUAL_CTR[1])","0a8e6f6d":"## For each alrorithm we will perform 1000 trials\nn = 1000","2f475025":"regret = 0 \ntotal_reward = 0\nregret_list = [] ## list for collecting the regret values for each impression (trial)\nctr = {0: [], 1: []} ## lists for collecting the calculated CTR \nindex_list = [] ## list for collecting the number of randomly choosen Ad\n\n## set the initial values for impressions and clicks \nimpressions = [0,0] \nclicks = [0,0]\n\nfor i in range(n):    \n    \n    random_index = np.random.randint(0,2,1)[0] ## randomly choose the value between [0,1]\n    index_list.append(random_index) ## add the value to list\n    \n    impressions[random_index] += 1 ## add 1 impression value for the choosen Ad\n    did_click = bernoulli.rvs(ACTUAL_CTR[random_index]) ## simulate if the person clicked on the ad usind Actual CTR value\n    \n    if did_click:\n        clicks[random_index] += did_click ## if person clicked add 1 click value for the choosen Ad\n    \n    ## calculate the CTR values and add them to list\n    if impressions[0] == 0:\n        ctr_0 = 0\n    else:\n        ctr_0 = clicks[0]\/impressions[0]\n        \n    if impressions[1] == 0:\n        ctr_1 = 0\n    else:\n        ctr_1 = clicks[1]\/impressions[1]\n        \n    ctr[0].append(ctr_0)\n    ctr[1].append(ctr_1)\n    \n    ## calculate the regret and reward\n    regret += max(ACTUAL_CTR) - ACTUAL_CTR[random_index]\n    regret_list.append(regret)\n    total_reward += did_click","e791d230":"algorithm_performance()","0642605d":"## save the reward and regret values for future comparison\nrandom_dict = {'reward':total_reward, \n               'regret_list':regret_list, \n               'ads_count':pd.Series(index_list).value_counts(normalize=True)}","b6ab6540":"e = .05 ## set the Epsilon value\nn_init = 100 ## number of impressions to choose the winning Ad\nimpressions = [0,0]\nclicks = [0,0]\n\nfor i in range(n_init):\n    random_index = np.random.randint(0,2,1)[0]\n    \n    impressions[random_index] += 1\n    did_click = bernoulli.rvs(ACTUAL_CTR[random_index])\n    if did_click:\n        clicks[random_index] += did_click\n        \nctr_0 = clicks[0] \/ impressions[0]\nctr_1 = clicks[1] \/ impressions[1]\nwin_index = np.argmax([ctr_0, ctr_1]) ## select the Ad number with the highest CTR\n\nprint('After', n_init, 'initial trials Ad #', win_index, 'got the highest CTR =', round(np.max([ctr_0, ctr_1]), 2), \n      '(Real CTR value is', ACTUAL_CTR[win_index], ').'\n      '\\nIt will be shown', (1-e)*100, '% of the time.')","30c3e72b":"regret = 0 \ntotal_reward = 0\nregret_list = [] \nctr = {0: [], 1: []}\nindex_list = [] \nimpressions = [0,0] \nclicks = [0,0]\n\nfor i in range(n):    \n    \n    epsilon_index = random.choices([win_index, 1-win_index], [1-e, e])[0]\n    index_list.append(epsilon_index)\n    \n    impressions[epsilon_index] += 1\n    did_click = bernoulli.rvs(ACTUAL_CTR[epsilon_index])\n    if did_click:\n        clicks[epsilon_index] += did_click\n    \n    if impressions[0] == 0:\n        ctr_0 = 0\n    else:\n        ctr_0 = clicks[0]\/impressions[0]\n        \n    if impressions[1] == 0:\n        ctr_1 = 0\n    else:\n        ctr_1 = clicks[1]\/impressions[1]\n        \n    ctr[0].append(ctr_0)\n    ctr[1].append(ctr_1)\n    \n    regret += max(ACTUAL_CTR) - ACTUAL_CTR[epsilon_index]\n    regret_list.append(regret)\n    total_reward += did_click","5e67a74e":"algorithm_performance()","c5b58af8":"epsilon_dict = {'reward':total_reward, \n                'regret_list':regret_list, \n                'ads_count':pd.Series(index_list).value_counts(normalize=True)}","f2b777f5":"regret = 0 \ntotal_reward = 0\nregret_list = [] \nctr = {0: [], 1: []}\nindex_list = [] \nimpressions = [0,0] \nclicks = [0,0]\npriors = (1, 1)\nwin_index = np.random.randint(0,2,1)[0] ## randomly choose the first shown Ad\n\nfor i in range(n):    \n    \n    impressions[win_index] += 1\n    did_click = bernoulli.rvs(ACTUAL_CTR[win_index])\n    if did_click:\n        clicks[win_index] += did_click\n    \n    ctr_0 = random.betavariate(priors[0]+clicks[0], priors[1] + impressions[0] - clicks[0])\n    ctr_1 = random.betavariate(priors[0]+clicks[1], priors[1] + impressions[1] - clicks[1])\n    win_index = np.argmax([ctr_0, ctr_1])\n    index_list.append(win_index)\n    \n    ctr[0].append(ctr_0)\n    ctr[1].append(ctr_1)\n    \n    regret += max(ACTUAL_CTR) - ACTUAL_CTR[win_index]\n    regret_list.append(regret)    \n    total_reward += did_click","563b15ef":"## plot the Beta distributions\nx = np.arange (0, 1, 0.01)\ny = beta.pdf(x, priors[0]+clicks[0], priors[1] + impressions[0] - clicks[0])\ny \/= y.max() ## normalize\n\ndata1 = go.Scatter(x=x,\n                   y=y,\n                   name='Beta Distribution (Ad #0)',\n                   marker = dict(color=('rgba(10, 108, 94, 1)')),\n                   fill='tozeroy',\n                   fillcolor = 'rgba(10, 108, 94, .7)')\n\ndata2 = go.Scatter(x = [ACTUAL_CTR[0]] * 2,\n                   y = [0, 1],\n                   name = 'Actual CTR #0 Value',\n                   mode='lines',\n                   line = dict(\n                       color = ('rgb(205, 12, 24)'),\n                       width = 2,\n                       dash = 'dash'))\n\ny = beta.pdf(x, priors[0]+clicks[1], priors[1] + impressions[1] - clicks[1])\ny \/= y.max()\n\ndata3 = go.Scatter(x=x,\n                   y=y,\n                   name='Beta Distribution (Ad #1)',\n                   marker = dict(color=('rgba(187, 121, 24, 1)')),\n                   fill='tozeroy',\n                   fillcolor = 'rgba(187, 121, 24, .7)')\n\ndata4 = go.Scatter(x = [ACTUAL_CTR[1]] * 2,\n                   y = [0, 1],\n                   name = 'Actual CTR #1 Value',\n                   mode='lines',\n                   line = dict(\n                       color = ('rgb(205, 12, 24)'),\n                       width = 2,\n                       dash = 'dash'))\n\nlayout = go.Layout(title='Beta Distributions for both Ads',\n                   xaxis={'title': 'Possible CTR values'},\n                   yaxis={'title': 'Probability Density'})\n\nfig = go.Figure(data=[data1, data2, data3, data4], layout=layout)\n\n# fig = tools.make_subplots(rows=1, cols=2, print_grid=False, shared_xaxes=False,\n#                           subplot_titles=('Beta Distribution (Ad #0)','Beta Distribution (Ad #1)'))\n\n# fig.append_trace(data1, 1, 1)\n# fig.append_trace(data2, 1, 1)\n# fig.append_trace(data3, 1, 2)\n# fig.append_trace(data4, 1, 2)\n\n# fig['layout'].update(showlegend=False)\n\niplot(fig, show_link=False)","0b1649ec":"algorithm_performance()","8bf18ef3":"thompson_dict = {'reward':total_reward, \n                 'regret_list':regret_list, \n                 'ads_count':pd.Series(index_list).value_counts(normalize=True)}","921c01b2":"regret = 0 \ntotal_reward = 0\nregret_list = [] \nindex_list = [] \nimpressions = [0,0] \nclicks = [0,0]\nctr = {0: [], 1: []}\ntotal_reward = 0\n\nfor i in range(n):\n    \n    index = 0\n    max_upper_bound = 0\n    for k in [0,1]:\n        if (impressions[k] > 0):\n            CTR = clicks[k] \/ impressions[k]\n            delta = math.sqrt(2 * math.log(i+1) \/ impressions[k])\n            upper_bound = CTR + delta\n            ctr[k].append(CTR)\n        else:\n            upper_bound = 1e400\n        if upper_bound > max_upper_bound:\n            max_upper_bound = upper_bound\n            index = k\n    index_list.append(index)\n    impressions[index] += 1\n    reward = bernoulli.rvs(ACTUAL_CTR[index])\n    \n    clicks[index] += reward\n    total_reward += reward\n    \n    regret += max(ACTUAL_CTR) - ACTUAL_CTR[index]\n    regret_list.append(regret)","b0b60b39":"algorithm_performance()","674bdbae":"ucb1_dict = {'reward':total_reward, \n             'regret_list':regret_list, \n             'ads_count':pd.Series(index_list).value_counts(normalize=True)}","355fc13d":"data1 = go.Bar(x=['Random Selection', 'Epsilon Greedy', 'Thompson Sampling', 'UCB1'],\n               y=[random_dict['ads_count'][0], \n                  epsilon_dict['ads_count'][0], \n                  thompson_dict['ads_count'][0],\n                  ucb1_dict['ads_count'][0]],\n               name='Ad #0',\n               marker=dict(color='rgba(10, 108, 94, .7)'))\n\ndata2 = go.Bar(x=['Random Selection', 'Epsilon Greedy', 'Thompson Sampling', 'UCB1'],\n               y=[random_dict['ads_count'][1], \n                  epsilon_dict['ads_count'][1], \n                  thompson_dict['ads_count'][1],\n                  ucb1_dict['ads_count'][1]],\n               name='Ad #1',\n               marker=dict(color='rgba(187, 121, 24, .7)'))\n\ndata = [data1, data2]\nlayout = go.Layout(title='Ratio of appearance of both Ads throughout the trials',\n                   xaxis={'title': 'Algorithm'},\n                   yaxis={'title': 'Ratio'},\n                   barmode='stack')\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","247e2bbc":"data1 = go.Scatter(\n    x=np.arange (0, n, 1),\n    y=random_dict['regret_list'],\n    name='Random Selection',\n    marker=dict(color='#ffcc66')\n)\ndata2 = go.Scatter(\n    x=np.arange (0, n, 1),\n    y=epsilon_dict['regret_list'],\n    name='e-Greedy',\n    marker=dict(color='#0099ff')\n)\ndata3 = go.Scatter(\n    x=np.arange (0, n, 1),\n    y=thompson_dict['regret_list'],\n    name='Thompson Sampling',\n    marker=dict(color='#ff3300')\n)\ndata4 = go.Scatter(\n    x=np.arange (0, n, 1),\n    y=ucb1_dict['regret_list'],\n    name='UCB1',\n    marker=dict(color='#33cc33')\n)\n\nlayout = go.Layout(\n    title='Regret by the Algorithm',\n    xaxis={'title': 'Trial'},\n    yaxis={'title': 'Regret'}\n)\n\ndata = [data1, data2, data3, data4]\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","693134a2":"data = go.Bar(\n    x=[ucb1_dict['reward'], thompson_dict['reward'], epsilon_dict['reward'], random_dict['reward']],\n    y=['UCB1', 'Thompson Sampling', 'e-Greedy','Random Selection'],\n    orientation = 'h',\n    marker=dict(color=['#33cc33', '#ff3300', '#0099ff', '#ffcc66']),\n    opacity=0.7\n)\n\ntext = go.Scatter(\n    x=[ucb1_dict['reward'], thompson_dict['reward'], epsilon_dict['reward'], random_dict['reward']],\n    y=['UCB1', 'Thompson Sampling', 'e-Greedy', 'Random Selection'],\n    mode='text',\n    text=[ucb1_dict['reward'], thompson_dict['reward'], epsilon_dict['reward'], random_dict['reward']],\n    textposition='middle left',\n    line = dict(\n        color = ('rgba(255,141,41,0.6)'),\n        width = 1\n    ),\n    textfont=dict(\n        family='sans serif',\n        size=16,\n        color='#000000'\n    )\n)\n\ndata = [data,text]\n\nlayout = go.Layout(\n    title='Total Reward by Algorithms',\n    xaxis={'title': 'Total Reward (Clicks)'},\n    margin={'l':200},\n    showlegend=False\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","458b514a":"Both Ads were shown equal amount of times and the more trials, the closer the CTR values are to their known values. However, the Regret is continually increasing since the algorithm doesn't learn anything and doesn't do any updates according to gained information. This ever-increasing regret is exactly what we\u2019re hoping to minimize with \u201csmarter\u201d methods.\n\nI would use this algorithm in two cases:\n\n1. I want to be confident about the estimated CTR value for each Ad (the more impression each Ad get, the more confindet I am that estimated CTR equals to real CTR).\n\n2. I have unlimited Marketing Budget ;)","c739655f":"## <a id='ucb'>Upper Confidence Bound (UCB1)<\/a> \n\n* *50% - Exploration*\n* *50% - Exploitation*\n\nUnlike the Thompson Sampling algorithm, the Upper Confidence Bound cares more about the uncertainty (high variation) of each variant. The more uncertain we are about one variant, the more important it is to explore. \n\nAlgorithm chooses the variant with the highest upper confidence bound value (UCB) which represents the highest reward guess for the variant. It is defind as follows:\n\n$UCB = \\bar x_i + \\sqrt{\\frac{2 \\cdot \\log{t}}{n}}$ ,\n\nwhere $\\bar x_i$ - the (CTR rate) for $i$-th step,\n\n$t$ - total number of (impressions) for all variants,\n\n$n$ - total number of (impressions) for choosen variant\n\nThe logic is rather straightforward:\n\n1. Calculate the UCB for all variants.\n2. Choose the variant with the highest UCB.\n3. Go to 1.","3765f67b":"Note the intersection area. There might be the cases that value of Beta distribution for Ad #0 will be higher, than for Ad #1, so algorithm will choose Ad #0 (which performs worse).","778e247b":"# <center> How to communicate effectively with your users before you know much about them <br> A data science tutorial that\u2019s easy to understand! <\/center>\n***\n*by [Ruslan Klymentiev](https:\/\/github.com\/ruslan-kl) and [Zank Bennett](https:\/\/github.com\/zankbennett)*\n\n*Originally created for Bennett Data Science [blog post](https:\/\/bennettdatascience.com\/know-your-options-when-it-comes-to-a-b-testing\/)*\n\n[GitHub Repo](https:\/\/github.com\/ruslan-kl\/mab_problem) with Flask app which could also help with an intuition of dealing with MAB problem.\n\n<img src=\"https:\/\/i.ibb.co\/zskN3Xs\/carl-raw-563328-unsplash.jpg\" alt=\"carl-raw-563328-unsplash\" border=\"0\">\n\n#### Table of Contents\n- <a href='#mab'>What is Multi-Armed Bandit Problem?<\/a>  \n- <a href='#random'>Random Selection<\/a>\n- <a href='#epsilon'>Epsilon-Greedy<\/a> \n- <a href='#ts'>Thompson Sampling<\/a> \n- <a href='#ucb'>Upper Confidence Bound<\/a> \n- <a href='#comparison'>Comparison and Conclusions<\/a> ","ca6ae2cb":"The regret is the lowest we've seen so far. Each uptick in regret happened when the Ad #0 was chosen. In the CTR Value plot, you can see that in the beginning, the green (Thompson sampled CTR value for Ad#0) values were often greater than the tan (Thompson sampled CTR value for Ad#1), resulting in Ad#0 being shown.\n\nNote the differences in variations for each ad. The algorithm explores constantly, then naturally exploits the ad variant with the highest sample taken from the appropriate Beta distribution. This can be shown in the top plot of the distributions. The Beta distribution for Ad#1 is much higher and narrower than that of Ad#0, meaning its sampled values will be consistently higher than those of Ad#0, which is exactly what we want!","192f0391":"## <a id='comparison'>Comparison and Conclusions<\/a> \n\nNow let's compare four of this methods and see which one performed better for our problem.\n\nFirst of all, it's obvious that we want to show the Ad #1 more often since its actual CTR is 0.65. Let's take a look at the ratio how many time the right Ad has been chosen for each algorithm.","6f9d949b":"You can see that regret went up when the algorithm tried to decrease uncertainty of CTR for Ad#0 by choosing it.\n\nIt might be useful when you want the model to choose the best variant more often, but are still interested in reducing the uncertainty of both variants.","ad491053":"Taking to account that Thompson Sampling and Epsilon-Greedy algorithms chose ad with the higher CTR (#1) most of the time, it shouldn't come as surprise that their regret values are the lowest.","3672353b":"It can be the case that total reward for the algorithm with the lowest regret value will not be the highest. It is caused by the fact that even if the algorithm chooses the right ad it doesn't guarantee that the user will click on it.\n\nAs was told from the beginning, the Thompson Sampling is generally the best choice, but we also looked at other algorithms and discussed how and when they might be useful. The method you choose depends on your unique problem, prior information you have and what information you want to receive afterwards. ","41c34a14":"## <a id='ts'>Thompson Sampling<\/a> \n\n* *50% - Exploration*\n* *50% - Exploitation*\n\nThe Thompson Sampling exploration part is more sophisticated than e-Greedy algorithm. We have been using **Beta distribution**\\* here, however Thompson Sampling can be generalized to sample from any distributions over parameters.\n\n> \\*In probability theory and statistics, the **beta distribution** is a family of continuous probability distributions defined on the interval [0, 1] parametrized by two positive shape parameters, denoted by $\\alpha$ and $\\beta$, that appear as exponents of the random variable and control the shape of the distribution. \n\n*If you want to know more about Beta distribution here is an [article](http:\/\/varianceexplained.org\/statistics\/beta_distribution_and_baseball\/) I found extremely useful.*\n\nLogic:\n\n1. Choose prior distributions for parameters $\\alpha$ and $\\beta$.\n2. Calculate the $\\alpha$ and $\\beta$ values as: $\\alpha = prior + hits$, $\\beta = prior + misses$. * In our case hits = number of clicks, misses = number of impressions without a click. Priors are useful if you have some prior information about the actual CTR\u2019s of your ads. Here, we do not, so we\u2019ll use 1.0.*\n3. Estimate actual CTR\u2019s by sampling values of Beta distribution for each variant $B(\\alpha_i, \\beta_i)$ and choose the sample with the highest value (estimated CTR).\n4. Repeat 2-3.","9a0f9e3e":"## <a id='mab'>What is Multi-Armed Bandit Problem?<\/a> \n\nThis is a classic case that comes up various areas, from marketing to medical trials: How do you give a user what they want before you have a relationship with them? What about the case where you don\u2019t even know much about the affinity for a user to an item or treatment your giving the user, as in a medical trial of several drugs or supplements?\n\nWe\u2019ll show you how to approach these problems, and how a Multi-Armed Bandit with Thompson Sampling is generally the best choice. That\u2019s a lot of words. Let\u2019s slow down and start with something simple.\n\nAs an example, imagine you are running a marketing campaign for your website and have two ads to choose from. And say the objective is  to show the ad with highest click through rate (CTR) value to drive the highest traffic possible. But you don't have any prior information about how either of these ads will perform. What would be your approach? What about typical A\/B testing? How about showing both of the ads equally, then, at some point, switching to the ad with the highest measured CTR? How long do you have to show both ads before settling on a \u201cwinner\u201d? In these cases, it seems like guessing might be our best bet. In fact, that\u2019s not far off!\n\n\nThere\u2019s a method for this very problem; it\u2019s called **the multi armed bandit (MAB)**.\n\nThere\u2019s a lot of information explaining what [the MAB algorithm](https:\/\/en.wikipedia.org\/wiki\/Multi-armed_bandit) is, what it does and how it works, so we\u2019ll keep it brief. Essentially, the MAB algorithm is a near-optimal method for solving the explore exploit trade-off dilemma that occurs when we don\u2019t know whether to explore possible options to find the one with the best payoff or exploit an option that we feel is the best, from the limited exploration done so far.\n\nIn this tutorial we will look at different algorithms to solve the MAB problem. They all have different approaches and different Exploration vs Exploitation ratios.\n\nHere, we\u2019ll define **CTR** as the ratio of how many times an ad was clicked vs. the number of impressions. For example, if an ad has been shown 100 times and clicked 10 times, CTR = 10\/100 = 0.1\n\nWe\u2019ll define **regret** as the difference between the highest possible CTR and the CTR shown. For example, if ad A has a known CTR or 0.1 and ad B has a known CTR of 0.3, each time we show ad A, we have a regret equal to 0.3 - 0.1 = 0.2. This seems like a small difference until we consider that an ad may be shown 1MM times in only hours.\n\nIn this tutorial, we want to demonstrate which algorithm implementation performs best in terms of minimizing regret. The four implementations we\u2019ll use are:\n\n1. Random Selection\n2. Epsilon Greedy\n3. Thompson Sampling\n4. Upper Confidence Bound (UCB1)\n\nEach method will be described below in the simulations.\n\nTo perform this experiment, we have to assume we know the CTR\u2019s in advance. That way, we can simulate a click (or not) of a given ad. For example, if we show ad A, with a known CTR of 28%, we can assume the ad will be clicked on 28% of the time and bake that into our simulation.","5093d069":"That\u2019s much better; Notice how the regret has decreased by an order of magnitude! The Epsilon-Greedy algorithm seems to perform much better than Random Selection. But can you see the problem here? The winning variant from exploration period will not necessarily be the optimal variant, and you can actually exploit the suboptimal variant. This increases regret and decreases reward. According to the **Law of Large Numbers**\\* the more initial trials you do, the more likely you will choose the winning variant correctly. But in Marketing you don't usually want to rely on chance and hope that you have reached that 'large number of trials'.\n\n> \\*In probability theory, the **law of large numbers (LLN)** is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed.\n\nThe good point is that you can adjust the ratio that controls how often to show the winning ad after initial trials by choosing different $\\epsilon$ values. ","ec230c06":"This never(?) happens in real life, but for we will assume that we know the actual CTR values for both Ads for simulation purposes.  ","fcea5736":"The winners here are the Thompson Sampling and Epsilon-Greedy since they showed the right Ad#1 the most of the time.","cdb3e761":"## <a id='random'>Random Selection<\/a> \n* *0% - Exploration*\n* *100% - Exploitation*\n\nLet's start with the most na\u00efve approach - Random Selection. The Random Selection algorithm doesn't do any Exploration, it just chooses randomly the Ad to show. \n\nYou can think of it as coin flip - if you get heads you show Ad #0, if you get tails you show Ad #1. So if you have 2 ads, each add will appear ~50% (=100%\/2) of the time. I guess you can tell already what are the disadvantages of this model, but let's look on simulation.","c417db60":"## <a id='epsilon'>Epsilon Greedy<\/a> \n\n* *~15% - Exploration*\n* *~85% - Exploitation*\n\nThe next approach is Epsilon-Greedy algorithm. Its logic can be defined as follows:\n1. Run the experiment for some initial number of times (**Exploration**).\n2. Choose the winning variant with the highest score for initial period of exploration.\n3. Set the Epsilon value, **$\\epsilon$**.\n4. Run experiment with choosing the winning variant for **$(1-\\epsilon)\\% $** of the time and other options for **$\\epsilon\\%$** of the time (**Exploitation**).\n\nLet's take a look at this algorithm in practice:"}}