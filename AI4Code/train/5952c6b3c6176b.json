{"cell_type":{"5b45c714":"code","ebbfb59d":"code","13434ef1":"code","7b43ff44":"code","c2ecd272":"code","ed48ab4e":"code","2c7d0e55":"code","30a330e1":"code","bc993428":"code","1f20fdc8":"code","f5dba3ae":"code","80c04fc3":"code","a9ee887b":"code","a6900c8c":"code","45531dee":"code","c2bdf8ef":"code","34703f13":"markdown","120aae92":"markdown","bb9df4f5":"markdown","d9ede58f":"markdown","082ff064":"markdown","20b5d045":"markdown","d671ee36":"markdown","84195a7e":"markdown","a54875c9":"markdown","dc8ac610":"markdown","785c5033":"markdown","0411fa1e":"markdown","2f3f9412":"markdown","50e1d2d4":"markdown","92e064a8":"markdown"},"source":{"5b45c714":"import os\nfrom collections import defaultdict\nfrom time import perf_counter\n\nimport numpy as np # linear algebra\nnp.random.seed(42)\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv\nfrom sklearn.datasets import load_breast_cancer,load_iris,load_digits\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.base import BaseEstimator\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble.weight_boosting import ClassifierMixin,BaseEnsemble\nfrom sklearn.model_selection import train_test_split,cross_validate,RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","ebbfb59d":"iris = load_iris()\nXiris,Yiris= iris.data[:,2:],iris.target\nplt.scatter(Xiris[:,0],Xiris[:,1],c=Yiris);","13434ef1":"T = 50                 #Total number of training rounds\nX,Y = Xiris,Yiris       #Using whole dataset for training for now\n\nn_classes = len(np.unique((Y))) #No of classes to predict\nN,M = X.shape           #Training and feature size respectively \nWt = np.full((N,),1\/N)  #weight of training samples, all initially equal. This will always sum to 1\nmodels = []             #Store trained models","7b43ff44":"log = defaultdict(list)\nfor t in range(T):\n    \"\"\"Train & predict with Base Classifier (Decision Tree here)\"\"\"\n    clf = DecisionTreeClassifier(max_depth=1).fit(X,Y,sample_weight=Wt)\n    preds = clf.predict(X)\n    e = 1 - accuracy_score(Y,preds,sample_weight=Wt)            \n    if e==0 or e>= 1 - 1\/n_classes : # Discard too weak classifiers \n        break\n        \n    \"\"\"Change a instance-weight based on if it was correctly classified\"\"\"\n    alpha = np.log((1-e)\/e) + np.log(n_classes - 1)       #Weight of the newly trained tree\n    match = preds==Y                 #Find indices of correctly classified instances \n    Wt[~match] *= np.exp(alpha)      #Increase weight of mis-classified instances\n    Wt \/= Wt.sum()                   #Normalize the weights to sum upto 1\n    models.append((alpha,clf))       #Store the newly trained model (clf) and its weight (alpha)\n    \n    \"\"\"Log several training info for further analysis\"\"\"\n    log['err'].append(e)             #Log weighted error\n    log['alpha'].append(alpha)       #Log tree weights\n    log['match'].append(match.sum()) #Log total no of instances correctly classified, related to log['e']\n    log['ins-weight'].append(Wt.copy())   #Log instance-weights\n\nm = len(models)\nprint(\"Total models trained:\",m)","c2ecd272":"def smooth(a):\n    \"\"\"Helper to smooth curves\"\"\"\n    return np.convolve(a,[1,1,1,1,1,1,1],'valid') \/ 7\n\nplt.plot(smooth(log['err']),label='error smoothed')    # Smoothed error of trained models\nplt.plot([.5]*m,label='random guess');                 # Vs a random classifier\nplt.legend(loc=\"best\");\n","ed48ab4e":"plt.plot(smooth(log['alpha']));","2c7d0e55":"plt.plot([w.std() for w in log['ins-weight']]);","30a330e1":"idx = np.argsort(-Wt)[:20]\ntop_10 = np.zeros_like(Wt)\ntop_10[idx] = 1.0\n\nplt.figure(figsize=(10,5))\nplt.subplot(121)\nplt.scatter(Xiris[:,0],Xiris[:,1],c=Yiris);\nplt.title(\"Original Data\")\n\nplt.subplot(122)\nplt.scatter(X[:,0],X[:,1],c=top_10,cmap=plt.cm.coolwarm,);\nplt.title(\"Hardest Instances (Red colored)\");\n\n","bc993428":"N = len(X)\nYpred = np.zeros((N,n_classes))\nfor alpha,clf in models:\n    yp = clf.predict(X)\n    Ypred[range(N),yp] += alpha\nYpred = np.argmax(Ypred,axis=1)  #Final prediction is `Ypred`\n","1f20fdc8":"class SimpleAdaBoost(BaseEstimator,ClassifierMixin):  #Inheritance is to allow using sklearn's cross_validate on this\n    def __init__(self,T=50,max_tree_depth=1):\n        self.T = T\n        self.models = []\n        self.max_tree_depth = max_tree_depth\n\n    def fit(self,X,Y):\n        self.n_classes = len(np.unique(Y))  \n        self.models = []               #Throw away if there are trained models from any previous fit()\n        N = len(X)\n        Wt = np.full((N,),1\/N)\n        for t in range(self.T):\n            clf = DecisionTreeClassifier(max_depth=self.max_tree_depth).fit(X,Y,sample_weight=Wt)\n            preds = clf.predict(X)\n            e = 1 - accuracy_score(Y,preds,sample_weight=Wt)\n            if e==0 or e>= 1-1\/self.n_classes:\n                break\n            alpha = np.log((1-e)\/e) + np.log(self.n_classes - 1)\n            match = preds==Y\n            Wt[~match] *= np.exp(alpha)\n            Wt \/= Wt.sum()\n            self.models.append((alpha,clf))\n        return self\n\n    def predict(self,X,start=0,end=99999):\n        N = len(X)\n        Ypred = np.zeros((N,self.n_classes))        \n        for alpha,clf in self.models[start:end]:\n            yp = clf.predict(X)\n            Ypred[range(N),yp] += alpha\n        return np.argmax(Ypred,axis=1)","f5dba3ae":"%matplotlib inline\nplt.rcParams[\"animation.html\"] = \"jshtml\"\nimport matplotlib.animation\n\nxp = np.arange(Xiris[:,0].min()-1,Xiris[:,0].max()+1,.02)\nyp = np.arange(Xiris[:,1].min()-1,Xiris[:,1].max()+1,.02)\nxx,yy = np.meshgrid(xp,yp) \n\nfig, ax = plt.subplots()\nplt.close()\nclf = SimpleAdaBoost(T=30,max_tree_depth=2).fit(Xiris,Yiris)\nm = len(clf.models)\nprint(m)\n\nindividual = False     #Change this to True if you want to just visualize the individual tree boundaries\ndef animate(i):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()],start=individual and i,end=i+1).reshape(xx.shape)\n    ax.contourf(xx,yy,Z,cmap=plt.cm.coolwarm,alpha=.99);\n    ax.scatter(Xiris[:,0],Xiris[:,1],c=Yiris);\n    \nani = matplotlib.animation.FuncAnimation(fig, animate, frames=m,interval=400)\nani","80c04fc3":"def validate(model,X,Y,rounds=100):\n    start = perf_counter()\n    acc = cross_validate(model,X,Y,cv=RepeatedStratifiedKFold(n_repeats=10))['test_score'].mean()\n    end = perf_counter()\n    print(f\"{model.__class__.__name__} : {acc*100:.1f} % in {end-start:.2f} seconds\")","a9ee887b":"\"\"\"Diabetes Dataset \"\"\"\ndf = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\nXdiab,Ydiab = df.values[:,:-1],df.values[:,-1].astype('int32')\n\nvalidate(SimpleAdaBoost(),Xdiab,Ydiab)\nvalidate(AdaBoostClassifier(algorithm='SAMME'),Xdiab,Ydiab)\nvalidate(DecisionTreeClassifier(),Xdiab,Ydiab);","a6900c8c":"\"\"\"Breast Cancer Dataset \"\"\"\nXcan,Ycan = load_breast_cancer(return_X_y=True)\nvalidate(SimpleAdaBoost(),Xcan,Ycan);validate(AdaBoostClassifier(algorithm='SAMME'),Xcan,Ycan),validate(DecisionTreeClassifier(),Xcan,Ycan);","45531dee":"\"\"\"Full Iris Dataset \"\"\"\nXiris,Yiris = load_iris(return_X_y=True)\nvalidate(SimpleAdaBoost(max_tree_depth=3),Xiris,Yiris)\nvalidate(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),algorithm='SAMME'),Xiris,Yiris)\nvalidate(DecisionTreeClassifier(),Xiris,Yiris);","c2bdf8ef":"\"\"\"Digits Dataset \"\"\"\nXdig,Ydig = load_digits(return_X_y=True)\nvalidate(SimpleAdaBoost(max_tree_depth=3,T=100),Xdig,Ydig)\nvalidate(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),n_estimators=100,algorithm='SAMME'),Xdig,Ydig)\nvalidate(DecisionTreeClassifier(),Xdig,Ydig);","34703f13":"## Testing Our Implementation\n* Against Sklearn's AdaBoost and our base learner i.e. DecisionTreeClassifier.\n* On 4 different datasets, Breast Cancer Wisconsin, Pima Indians Diabetes dataset, Iris and Digits.\n\nFirst, lets write a helper to evaluate a model.","120aae92":"Not surprisingly, hardest instances are all near the decision boundary or the outliers.\n\n**This is the reason behind AdaBoost's performance, it starts with learning to classify simpler instances, and then gradually shifts attention towards harder ones, ones previuos models performed poorly on. So in the final ensemble of trained models, we have both models that perform great with simple instances, and the ones that work well mainly for harder instances.  **","bb9df4f5":"## Fitting the model : A single iteration\n* Build a new training set by sampling from original set, according to instance-weight `Wt`\n* Fit a simple Decision Tree Classifier (of small depth) with the new training set\n* Predict class labels for whole training set using trained model\n* Increase the weights of misclassified instances, decrease for the correctly classified ones\n* We also log different information in `log` dictionary below","d9ede58f":"Now we'll see how the decision boundary of our `SimpleAdaBoost` changes as more base learners are added to it...","082ff064":"Now lets start eveluations.`SimpleAdaBoost` is our implemented AdaBoost, `AdaBoostClassifier` is scikit-learn's official implementation. We also include base learner Decision Tree here, to see how much boost in performace came from these AdaBoosts.","20b5d045":"Thats it.<br> \nThank you for reading this kernel. <br> \nAny feedback is welcome.","d671ee36":"## AdaBoost\n* This kernel attempt to produce simplest possible pure python implementation of AdaBoost, using sklearn's Decision Tree as base learner.<br>\n* Despite the simplicity, performance seems to be on par with Scikit-learn's official implementation, at least on simple datasets tested here.","84195a7e":"# AdaBoost\n* Initialize all training samples with equal weight (sum to 1) (array `Wt` below)\n* For `T` no of rounds, create `T` of models, weight of each model is proportional to its performance\n* During prediction, class with highest (weighted) votes from trained models will be chosen.","a54875c9":"## Putting It All Together\n(Please expand the code below for complete code in sklearn-like API)","dc8ac610":"## Predicting \n* Take weighted predictions from each trained model\n* Find the class with highest votes \n","785c5033":"Now, lets see where are the top 20 hardest instances of our training set. ","0411fa1e":"The plot above shows that latter models struggle more to classify compared to earlier models. The plateau at the end suggests we could do with fewer trees.\nThis comparatively poorer performance naturally corresponds to lower weights for latter models, as the plot below shows.","2f3f9412":"Lets prepare our dataset first. This will be classic Iris dataset, but now both sepal_* features dropped, to bring the dataset to 2D for visualization. Sepal features are relatively less important in Iris dataset. Besides maximizing accuracy isn't the goal of this notebook.","50e1d2d4":"Sometimes, few classifiers may fail to achieve the minimum accuracy for it to be included into final classifier. This can be atrributed to extreme simplicity of base classifier.\n\nBelow, we plot the logged information and see what they reveal.","92e064a8":"But why does latter models struggle more? The answer lies in instance weight distrbution, which gradually focuses more on harder instances, at the expanse of easier ones. This means relative weights among instances, as calculated by standard deviation here, also gradually increase."}}