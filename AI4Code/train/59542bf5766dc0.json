{"cell_type":{"a0d0adbd":"code","3dc6746d":"code","1284010e":"code","6066916a":"code","51c55ecb":"code","e21fe2c1":"code","5c380ad7":"code","3e256ea7":"code","96f8ebc5":"code","412ab580":"code","d9ce6995":"code","2defd7e9":"code","ef9c1206":"code","0a367c9c":"code","079ae626":"code","386abaed":"code","a3acf672":"code","36fc58ee":"code","632749dc":"code","c216773a":"code","f3558304":"code","0ba42958":"code","46c5374e":"code","5806bea5":"code","78e435c0":"code","82e2e219":"code","9a5447e8":"code","717e61d2":"code","da610934":"code","350fa052":"code","9d3ad686":"code","573d7b3e":"code","b7086010":"markdown","f85da3e7":"markdown","3286c84b":"markdown","08a2a8d1":"markdown","1adf0b5e":"markdown","1e2148b1":"markdown","6c1e8bab":"markdown","317382d0":"markdown","cf89e8ca":"markdown","ab517eca":"markdown","4413d30d":"markdown","299e0a63":"markdown","8cb178ea":"markdown","a90d8951":"markdown","cc786fdc":"markdown","1e2b13b0":"markdown","16303c5a":"markdown","8950480c":"markdown","923a48fb":"markdown","218ff6b1":"markdown","a6039bf0":"markdown","0a5128d1":"markdown","42cc4569":"markdown","143a9e47":"markdown","06ff4654":"markdown","05522881":"markdown","71bd44e0":"markdown","fcd0b9a3":"markdown","a5e97a17":"markdown","b02e3a13":"markdown","f1ede87f":"markdown","11be842f":"markdown","13641646":"markdown","3e00ac4b":"markdown","7468cc6f":"markdown","6bd8a103":"markdown","3b4a3ffa":"markdown","bb94f877":"markdown","09156c6f":"markdown","c77c5007":"markdown","88ad07bd":"markdown","6d792a17":"markdown","0e3d4f29":"markdown","103817a2":"markdown","36a86607":"markdown","5cebeb1e":"markdown","7311b006":"markdown","b4e7c2b4":"markdown","1bcd6ea1":"markdown","30062219":"markdown","0d8f42f6":"markdown","ffa46755":"markdown","b17291dd":"markdown","acaa35f0":"markdown","1db8736e":"markdown","c3d791f2":"markdown","d542b353":"markdown","ab3a267f":"markdown","0a8319b5":"markdown","2ca7474f":"markdown","5ca04cf3":"markdown","144a6e5a":"markdown","1a68c8a7":"markdown","95faa571":"markdown","2fbb9b1e":"markdown","55f52d05":"markdown","794165ba":"markdown","d2975880":"markdown","431d5ba6":"markdown","ccf34457":"markdown","1e5dcb37":"markdown","68d1a096":"markdown","45664f3c":"markdown","56855d97":"markdown","28b5e61e":"markdown","f8e671d4":"markdown"},"source":{"a0d0adbd":"# Dependencies\n\n# Standard Dependencies\nimport os\nimport numpy as np\nimport pandas as pd\nfrom math import sqrt\n\n# Visualization\nfrom pylab import *\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistics\nfrom statistics import median\nfrom scipy import signal\nfrom scipy.misc import factorial\nimport scipy.stats as stats\nfrom scipy.stats import sem, binom, lognorm, poisson, bernoulli, spearmanr\nfrom scipy.fftpack import fft, fftshift\n\n# Scikit-learn for Machine Learning models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Seed for reproducability\nseed = 12345\nnp.random.seed(seed)\n\n# Kaggle Directory for Kernels\nKAGGLE_DIR = '..\/input\/'\n\n# Read in csv of Toy Dataset\n# We will use this dataset throughout the tutorial\ndf = pd.read_csv(KAGGLE_DIR + 'toy_dataset.csv')\n\n# Files and file sizes\nprint('\\n# Files and file sizes')\nfor file in os.listdir(KAGGLE_DIR):\n    print('{}| {} MB'.format(file.ljust(30), \n                             str(round(os.path.getsize(KAGGLE_DIR + file) \/ 1000000, 2))))","3dc6746d":"# PMF Visualization\nn = 100\np = 0.5\n\nfig, ax = plt.subplots(1, 1, figsize=(17,5))\nx = np.arange(binom.ppf(0.01, n, p), binom.ppf(0.99, n, p))\nax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='Binomial PMF')\nax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\nrv = binom(n, p)\n#ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1, label='frozen PMF')\nax.legend(loc='best', frameon=False, fontsize='xx-large')\nplt.title('PMF of a binomial distribution (n=100, p=0.5)', fontsize='xx-large')\nplt.show()","1284010e":"# Plot normal distribution\nmu = 0\nvariance = 1\nsigma = sqrt(variance)\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\nplt.figure(figsize=(16,5))\nplt.plot(x, stats.norm.pdf(x, mu, sigma), label='Normal Distribution')\nplt.title('Normal Distribution with mean = 0 and std = 1')\nplt.legend(fontsize='xx-large')\nplt.show()","6066916a":"# Data\nX  = np.arange(-2, 2, 0.01)\nY  = exp(-X ** 2)\n\n# Normalize data\nY = Y \/ (0.01 * Y).sum()\n\n# Plot the PDF and CDF\nplt.figure(figsize=(15,5))\nplt.title('Continuous Normal Distributions', fontsize='xx-large')\nplot(X, Y, label='Probability Density Function (PDF)')\nplot(X, np.cumsum(Y * 0.01), 'r', label='Cumulative Distribution Function (CDF)')\nplt.legend(fontsize='xx-large')\nplt.show()","51c55ecb":"# Uniform distribution (between 0 and 1)\nuniform_dist = np.random.random(1000)\nuniform_df = pd.DataFrame({'value' : uniform_dist})\nuniform_dist = pd.Series(uniform_dist)","e21fe2c1":"plt.figure(figsize=(18,5))\nsns.scatterplot(data=uniform_df)\nplt.legend(fontsize='xx-large')\nplt.title('Scatterplot of a Random\/Uniform Distribution', fontsize='xx-large')","5c380ad7":"plt.figure(figsize=(18,5))\nsns.distplot(uniform_df)\nplt.title('Random\/Uniform distribution', fontsize='xx-large')","3e256ea7":"# Generate Normal Distribution\nnormal_dist = np.random.randn(10000)\nnormal_df = pd.DataFrame({'value' : normal_dist})\n# Create a Pandas Series for easy sample function\nnormal_dist = pd.Series(normal_dist)\n\nnormal_dist2 = np.random.randn(10000)\nnormal_df2 = pd.DataFrame({'value' : normal_dist2})\n# Create a Pandas Series for easy sample function\nnormal_dist2 = pd.Series(normal_dist)\n\nnormal_df_total = pd.DataFrame({'value1' : normal_dist, \n                                'value2' : normal_dist2})","96f8ebc5":"# Scatterplot\nplt.figure(figsize=(18,5))\nsns.scatterplot(data=normal_df)\nplt.legend(fontsize='xx-large')\nplt.title('Scatterplot of a Normal Distribution', fontsize='xx-large')","412ab580":"# Normal Distribution as a Bell Curve\nplt.figure(figsize=(18,5))\nsns.distplot(normal_df)\nplt.title('Normal distribution (n=1000)', fontsize='xx-large')","d9ce6995":"# Change of heads (outcome 1)\np = 0.6\n\n# Create Bernoulli samples\nbern_dist = bernoulli.rvs(p, size=1000)\nbern_df = pd.DataFrame({'value' : bern_dist})\nbern_values = bern_df['value'].value_counts()\n\n# Plot Distribution\nplt.figure(figsize=(18,4))\nbern_values.plot(kind='bar', rot=0)\nplt.annotate(xy=(0.85,300), \n             s='Samples that came up Tails\\nn = {}'.format(bern_values[0]), \n             fontsize='large', \n             color='white')\nplt.annotate(xy=(-0.2,300), \n             s='Samples that came up Heads\\nn = {}'.format(bern_values[1]), \n             fontsize='large', \n             color='white')\nplt.title('Bernoulli Distribution: p = 0.6, n = 1000')","2defd7e9":"x = np.arange(0, 20, 0.1)\ny = np.exp(-5)*np.power(5, x)\/factorial(x)\n\nplt.figure(figsize=(15,8))\nplt.title('Poisson distribution with lambda=5', fontsize='xx-large')\nplt.plot(x, y, 'bs')\nplt.show()","ef9c1206":"# Specify standard deviation and mean\nstd = 1\nmean = 5\n\n# Create log-normal distribution\ndist=lognorm(std,loc=mean)\nx=np.linspace(0,15,200)\n\n# Visualize log-normal distribution\nplt.figure(figsize=(15,6))\nplt.xlim(5, 10)\nplt.plot(x,dist.pdf(x), label='log-normal PDF')\nplt.plot(x,dist.cdf(x), label='log-normal CDF')\nplt.legend(fontsize='xx-large')\nplt.title('Visualization of log-normal PDF and CDF', fontsize='xx-large')\nplt.show()","0a367c9c":"# Summary\nprint('Summary Statistics for a normal distribution: ')\n# Median\nmedi = median(normal_dist)\nprint('Median: ', medi)\ndisplay(normal_df.describe())\n\n# Standard Deviation\nstd = sqrt(np.var(normal_dist))\n\nprint('The first four calculated moments of a normal distribution: ')\n# Mean\nmean = normal_dist.mean()\nprint('Mean: ', mean)\n\n# Variance\nvar = np.var(normal_dist)\nprint('Variance: ', var)\n\n# Return unbiased skew normalized by N-1\nskew = normal_df['value'].skew()\nprint('Skewness: ', skew)\n\n# Return unbiased kurtosis over requested axis using Fisher\u2019s definition of kurtosis \n# (kurtosis of normal == 0.0) normalized by N-1\nkurt = normal_df['value'].kurtosis()\nprint('Kurtosis: ', kurt)","079ae626":"# Take sample\nnormal_df_sample = normal_df.sample(100)\n\n# Calculate Expected Value (EV), population mean and bias\nev = normal_df_sample.mean()[0]\npop_mean = normal_df.mean()[0]\nbias = ev - pop_mean","386abaed":"print('Sample mean (Expected Value): ', ev)\nprint('Population mean: ', pop_mean)\nprint('Bias: ', bias)","a3acf672":"from math import sqrt\n\nY = 100 # Actual Value\nYH = 94 # Predicted Value\n\n# MSE Formula \ndef MSE(Y, YH):\n     return np.square(YH - Y).mean()\n\n# RMSE formula\ndef RMSE(Y, YH):\n    return sqrt(np.square(YH - Y).mean())\n\n\nprint('MSE: ', MSE(Y, YH))\n\nprint('RMSE: ', RMSE(Y, YH))","36fc58ee":"# Standard Error (SE)\nuni_sample = uniform_dist.sample(100)\nnorm_sample = normal_dist.sample(100)\n\nprint('Standard Error of uniform sample: ', sem(uni_sample))\nprint('Standard Error of normal sample: ', sem(norm_sample))\n\n# The random samples from the normal distribution should have a higher standard error","632749dc":"# Note that we take very small samples just to illustrate the different sampling methods\n\nprint('---Non-Representative samples:---\\n')\n# Convenience samples\ncon_samples = normal_dist[0:5]\nprint('Convenience samples:\\n\\n{}\\n'.format(con_samples))\n\n# Haphazard samples (Picking out some numbers)\nhap_samples = [normal_dist[12], normal_dist[55], normal_dist[582], normal_dist[821], normal_dist[999]]\nprint('Haphazard samples:\\n\\n{}\\n'.format(hap_samples))\n\n# Purposive samples (Pick samples for a specific purpose)\n# In this example we pick the 5 highest values in our distribution\npurp_samples = normal_dist.nlargest(n=5)\nprint('Purposive samples:\\n\\n{}\\n'.format(purp_samples))\n\nprint('---Representative samples:---\\n')\n\n# Simple (pseudo)random sample\nrand_samples = normal_dist.sample(5)\nprint('Random samples:\\n\\n{}\\n'.format(rand_samples))\n\n# Systematic sample (Every 2000th value)\nsys_samples = normal_dist[normal_dist.index % 2000 == 0]\nprint('Systematic samples:\\n\\n{}\\n'.format(sys_samples))\n\n# Stratified Sampling\n# We will get 1 person from every city in the dataset\n# We have 8 cities so that makes a total of 8 samples\ndf = pd.read_csv(KAGGLE_DIR + 'toy_dataset.csv')\n\nstrat_samples = []\n\nfor city in df['City'].unique():\n    samp = df[df['City'] == city].sample(1)\n    strat_samples.append(samp['Income'].item())\n    \nprint('Stratified samples:\\n\\n{}\\n'.format(strat_samples))\n\n# Cluster Sampling\n# Make random clusters of ten people (Here with replacement)\nc1 = normal_dist.sample(10)\nc2 = normal_dist.sample(10)\nc3 = normal_dist.sample(10)\nc4 = normal_dist.sample(10)\nc5 = normal_dist.sample(10)\n\n# Take sample from every cluster (with replacement)\nclusters = [c1,c2,c3,c4,c5]\ncluster_samples = []\nfor c in clusters:\n    clus_samp = c.sample(1)\n    cluster_samples.extend(clus_samp)\nprint('Cluster samples:\\n\\n{}'.format(cluster_samples))    \n","c216773a":"# Covariance between Age and Income\nprint('Covariance between Age and Income: ')\n\ndf[['Age', 'Income']].cov()","f3558304":"# Correlation between two normal distributions\n# Using Pearson's correlation\nprint('Pearson: ')\ndf[['Age', 'Income']].corr(method='pearson')","0ba42958":"# Using Spearman's rho correlation\nprint('Spearman: ')\ndf[['Age', 'Income']].corr(method='spearman')","46c5374e":"# Generate data\nx = np.random.uniform(low=20, high=260, size=100)\ny = 50000 + 2000*x - 4.5 * x**2 + np.random.normal(size=100, loc=0, scale=10000)\n\n# Plot data with Linear Regression\nplt.figure(figsize=(16,5))\nplt.title('Well fitted but not well fitting: Linear regression plot on quadratic data', fontsize='xx-large')\nsns.regplot(x, y)","5806bea5":"# Linear regression from scratch\nimport random\n# Create data from regression\nxs = np.array(range(1,20))\nys = [0,8,10,8,15,20,26,29,38,35,40,60,50,61,70,75,80,88,96]\n\n# Put data in dictionary\ndata = dict()\nfor i in list(xs):\n    data.update({xs[i-1] : ys[i-1]})\n\n# Slope\nm = 0\n# y intercept\nb = 0\n# Learning rate\nlr = 0.0001\n# Number of epochs\nepochs = 100000\n\n# Formula for linear line\ndef lin(x):\n    return m * x + b\n\n# Linear regression algorithm\nfor i in range(epochs):\n    # Pick a random point and calculate vertical distance and horizontal distance\n    rand_point = random.choice(list(data.items()))\n    vert_dist = abs((m * rand_point[0] + b) - rand_point[1])\n    hor_dist = rand_point[0]\n\n    if (m * rand_point[0] + b) - rand_point[1] < 0:\n        # Adjust line upwards\n        m += lr * vert_dist * hor_dist\n        b += lr * vert_dist   \n    else:\n        # Adjust line downwards\n        m -= lr * vert_dist * hor_dist\n        b -= lr * vert_dist\n        \n# Plot data points and regression line\nplt.figure(figsize=(15,6))\nplt.scatter(data.keys(), data.values())\nplt.plot(xs, lin(xs))\nplt.title('Linear Regression result')  \nprint('Slope: {}\\nIntercept: {}'.format(m, b))","78e435c0":"# scikit-learn bootstrap package\nfrom sklearn.utils import resample\n\n# data sample\ndata = df['Income']\n\n# prepare bootstrap samples\nboot = resample(data, replace=True, n_samples=5, random_state=1)\nprint('Bootstrap Sample: \\n{}\\n'.format(boot))\nprint('Mean of the population: ', data.mean())\nprint('Standard Deviation of the population: ', data.std())\n\n# Bootstrap plot\npd.plotting.bootstrap_plot(data)","82e2e219":"# Perform t-test and compute p value of two random samples\nprint('T-statistics and p-values of two random samples.')\nfor _ in range(10):\n    rand_sample1 = np.random.random_sample(10)\n    rand_sample2 = np.random.random_sample(10)\n    print(stats.ttest_ind(rand_sample1, rand_sample2))","9a5447e8":"# To-do\n# Equivalence testing","717e61d2":"# q-q plot of a normal distribution\nplt.figure(figsize=(15,6))\nstats.probplot(normal_dist, dist=\"norm\", plot=plt)\nplt.show()","da610934":"# q-q plot of a uniform\/random distribution\nplt.figure(figsize=(15,6))\nstats.probplot(uniform_dist, dist=\"norm\", plot=plt) \nplt.show()","350fa052":"# Detect outliers on the 'Income' column of the Toy Dataset\n\n# Function for detecting outliers a la Tukey's method using z-scores\ndef tukey_outliers(data) -> list:\n    # For more information on calculating the threshold check out:\n    # https:\/\/medium.com\/datadriveninvestor\/finding-outliers-in-dataset-using-python-efc3fce6ce32\n    threshold = 3\n    \n    mean = np.mean(data)\n    std = np.std(data)\n    \n    # Spot and collect outliers\n    outliers = []\n    for i in data:\n        z_score = (i - mean) \/ std\n        if abs(z_score) > threshold:\n            outliers.append(i)\n    return outliers\n\n# Get outliers\nincome_outliers = tukey_outliers(df['Income'])\n\n# Visualize distribution and outliers\nplt.figure(figsize=(15,6))\ndf['Income'].plot(kind='hist', bins=100, label='Income distribution')\nplt.hist(income_outliers, bins=20, label='Outliers')\nplt.title(\"Outlier detection a la Tukey's method\", fontsize='xx-large')\nplt.xlabel('Income')\nplt.legend(fontsize='xx-large')","9d3ad686":"# Inverse logit function (link function)\ndef inv_logit(x):\n    return 1 \/ (1 + np.exp(-x))\n\nt1 = np.arange(-10, 10, 0.1)\nplt.figure(figsize=(15,6))\nplt.plot(t1, inv_logit(t1), \n         t1, inv_logit(t1-2),   \n         t1, inv_logit(t1*2))\nplt.title('Inverse logit functions', fontsize='xx-large')\nplt.legend(('Normal', 'Changed intercept', 'Changed slope'), fontsize='xx-large')","573d7b3e":"# Simple example of Logistic Regression in Python\nfrom sklearn.datasets import load_iris\nX, y = load_iris(return_X_y=True)\n\n# Logistic regression classifier\nclf = LogisticRegression(random_state=0, \n                         solver='lbfgs',\n                         multi_class='multinomial').fit(X, y)\n\nprint('Accuracy score of logistic regression model on the Iris flower dataset: {}'.format(clf.score(X, y)))\n","b7086010":"**MSE (Mean Squared Error)** is a formula to measure how much estimators deviate from the true distribution. This can be very useful with for example, evaluating regression models.\n\n\n![](https:\/\/i.stack.imgur.com\/iSWyZ.png)\n\n\n**RMSE (Root Mean Squared Error)** is just the root of the MSE.\n\n\n![](http:\/\/file.scirp.org\/Html\/htmlimages\/5-2601289x\/fcdba7fc-a40e-4019-9e95-aca3dc2db149.png)\n\n","f85da3e7":"### Uniform Distribution","3286c84b":"![](https:\/\/www.researchgate.net\/profile\/Arch_Woodside2\/publication\/286454889\/figure\/fig3\/AS:669434310037526@1536616985820\/Anscombes-quartet-of-different-XY-plots-of-four-data-sets-having-identical-averages.png)","08a2a8d1":"## Sampling methods <a id=\"6\"><\/a>","1adf0b5e":"## Covariance <a id=\"7\"><\/a>","1e2148b1":"### q-q plot (quantile-quantile plot)\n\nMany statistical techniques require that data is coming from a normal distribution (for example, t-test). Therefore, it is important to verify this before applying statistical techniques.\n\nOne approach is to visualize and make a judgment about the distribution. A q-q plot is very helpful for determining if a distribution is normal. There are other tests for testing 'normality', but this is beyond the scope of this tutorial.\n\nIn the first plot we can easily see that the values line up nicely. From this we conclude that the data is normally distributed.\n\nIn the second plot we can see that the values don't line up. Our conclusion is that the data is not normally distributed. In this case the data was uniformly distributed.\n","6c1e8bab":"## Bonus: Free statistics courses <a id=\"15\"><\/a>","317382d0":"A Link Function is used in Generalized Linear Models (GLMs) to apply linear models for a continuous response variable given continuous and\/or categorical predictors. A link function that is often used is called the inverse logit or logistic sigmoid function.\n\nThe link function provides a relationship between the linear predictor and the mean of a distribution.","cf89e8ca":"## Sources <a id=\"16\"><\/a>","ab517eca":"## Generalized Linear Models (GLMs) <a id=\"13\"><\/a>","4413d30d":"The Poisson distribution is a discrete distribution and is popular for modelling the number of times an event occurs in an interval of time or space. \n\nIt takes a value lambda, which is equal to the mean of the distribution.\n\nPMF: \n\n![](https:\/\/study.com\/cimages\/multimages\/16\/poisson1a.jpg)\n\nCDF: \n![](http:\/\/www.jennessent.com\/images\/cdf_poisson.gif)","299e0a63":"A Uniform distribution is pretty straightforward. Every value has an equal change of occuring. Therefore, the distribution consists of random values with no patterns in them. In this example we generate random floating numbers between 0 and 1.\n\nThe PDF of a Uniform Distribution:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/648692e002b720347c6c981aeec2a8cca7f4182f)\n\nCDF:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/eeeeb233753cfe775b24e3fec2f371ee8cdc63a6)","8cb178ea":"### PDF (Probability Density Functions)","a90d8951":"A log-normal distribution is continuous. The main characteristic of a log-normal distribution is that it's logarithm is normally distributed. It is also referred to as Galton's distribution.\n\nPDF: \n\n![](https:\/\/www.mhnederlof.nl\/images\/lognormaldensity.jpg)\n\nCDF:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/29095d9cbd6539833d549c59149b9fc5bd06339b)\n\nWhere Phi is the CDF of the standard normal distribution.","cc786fdc":"### Prevention of Overfitting\n\n1. Split data into training data and test data.\n2. Regularization: limit the flexibility of the model.\n3. Cross Validation","1e2b13b0":"### Tukey's method","16303c5a":"A Binomial Distribution has a countable number of outcomes and is therefore discrete.\n\nBinomial distributions must meet the following three criteria:\n\n1. The number of observations or trials is fixed. In other words, you can only figure out the probability of something happening if you do it a certain number of times.\n2. Each observation or trial is independent. In other words, none of your trials have an effect on the probability of the next trial.\n3. The probability of success is exactly the same from one trial to another.\n\nAn intuitive explanation of a binomial distribution is flipping a coin 10 times. If we have a fair coin our chance of getting heads (p) is 0.50. Now we throw the coin 10 times and count how many times it comes up heads. In most situations we will get heads 5 times, but there is also a change that we get heads 9 times. The PMF of a binomial distribution will give these probabilities if we say N = 10 and p = 0.5. We say that the x for heads is 1 and 0 for tails.\n\nPMF:\n\n![](http:\/\/reliabilityace.com\/formulas\/binomial-pmf.png)\n\nCDF:\n\n![](http:\/\/reliabilityace.com\/formulas\/binomial-cpf.png)\n\n\nA **Bernoulli Distribution** is a special case of a Binomial Distribution.\n\nAll values in a Bernoulli Distribution are either 0 or 1. \n\nFor example, if we take an unfair coin which falls on heads 60 % of the time, we can describe the Bernoulli distribution as follows:\n\np (change of heads) = 0.6\n\n1 - p (change of tails) = 0.4\n\nheads = 1\n\ntails = 0\n\nFormally, we can describe a Bernoulli distribution with the following PMF (Probability Mass Function):\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/a9207475ab305d280d2958f5c259f996415548e9)\n","8950480c":"The formula for Pearson's correlation coefficient consists of the covariance between the two random variables divided by the standard deviation of the first random variable times the standard deviation of the second random variable.\n\nFormula for Pearson's correlation coefficient:\n\n![](http:\/\/sherrytowers.com\/wp-content\/uploads\/2013\/09\/correlation_xy-300x97.jpg)","923a48fb":"### Moments\n\nA moment is a quantitative measure that says something about the shape of a distribution. There are central moments and non-central moments. This section is focused on the central moments.\n\nThe 0th central moment is the total probability and is always equal to 1.\n\nThe 1st moment is the mean (expected value).\n\nThe 2nd central moment is the variance.\n\n**Variance** = The average of the squared distance of the mean. Variance is interesting in a mathematical sense, but the standard deviation is often a much better measure of how spread out the distribution is.\n\n![](http:\/\/www.visualmining.com\/wp-content\/uploads\/2013\/02\/analytics_formula_variance.png)\n\n**Standard Deviation** = The square root of the variance\n\n![](http:\/\/www.visualmining.com\/wp-content\/uploads\/2013\/02\/analytics_formula_std_dev.png)\n\nThe 3rd central moment is the skewness.\n\n**Skewness** = A measure that describes the contrast of one tail versus the other tail. For example, if there are more high values in your distribution than low values then your distribution is 'skewed' towards the high values.\n\n![](http:\/\/www.visualmining.com\/wp-content\/uploads\/2013\/02\/analytics_formula_skewness.png)\n\nThe 4th central moment is the kurtosis.\n\n**Kurtosis** = A measure of how 'fat' the tails in the distribution are.\n\n![](http:\/\/www.visualmining.com\/wp-content\/uploads\/2013\/02\/analytics_formula_kurtosis.png)\n\nThe higher the moment, the harder it is to estimate with samples. Larger samples are required in order to obtain good estimates.","218ff6b1":"# Statistics Tutorial","a6039bf0":"A Probability distribution tells us something about the likelihood of each value of the random variable.\n\nA random variable X is a function that maps events to real numbers.\n\nThe visualizations in this section are of discrete distributions. Many of these distributions can however also be continuous.","0a5128d1":"**If you like this Kaggle kernel, feel free to give an upvote and leave a comment! I will try to implement your suggestions in this kernel!**","42cc4569":"Hi!\n\nThis notebook is a gentle tutorial to essential concepts in statistics. I try to present the concepts in a fun and interactive way and I encourage you to play with the code to get a better grasp of the concepts.\n\nI will be using a \"[Toy Dataset](https:\/\/www.kaggle.com\/carlolepelaars\/toy-dataset)\" to illustrate concepts in this kernel.\n\nThe Jupyter Notebook and dataset are also available as a [Github repository](https:\/\/github.com\/CarloLepelaars\/stats_tutorial).\n\n![](https:\/\/i.stack.imgur.com\/c88K3.png)","143a9e47":"A discrete variable is a variable that can only take on a \"countable\" number of values. If you can count a set of items, then it\u2019s a discrete variable. An example of a discrete variable is the outcome of a dice. It can only have 1 of 6 different possible outcomes and is therefore discrete. A discrete random variable can have an infinite number of values. For example, the whole set of natural numbers (1,2,3,etc.) is countable and therefore discrete. \n\nA continuous variable takes on an \"uncountable\" number of values. An example of a continuous variable is length. Length can be measured to an arbitrary degree and is therefore continuous.\n\nIn statistics we represent a distribution of discrete variables with PMF's (Probability Mass Functions) and CDF's (Cumulative Distribution Functions). We represent distributions of continuous variables with PDF's (Probability Density Functions) and CDF's. \n\nThe PMF defines the probability of all possible values x of the random variable. A PDF is the same but for continuous values.\nThe CDF represents the probability that the random variable X will have an outcome less or equal to the value x. The name CDF is used for both discrete and continuous distributions.\n\nThe functions that describe PMF's, PDF's and CDF's can be quite daunting at first, but their visual counterpart often looks quite intuitive.","06ff4654":"Our model is overfitting if it is also modeling the 'noise' in the data. This implies that the model will not generalize well to new data even though the error on the training data becomes very small. Linear models are unlikely to overfit, but as models become more flexible we have to be wary of overfitting. Our model can also underfit which means that it has a large error on the training data. \n\nFinding the sweet spot between overfitting and underfitting is called the [Bias Variance Trade-off](https:\/\/en.wikipedia.org\/wiki\/Bias%E2%80%93variance_tradeoff). It is nice to know this theorem, but more important to understand how to prevent it. I will explain some techniques for how to do this below.","05522881":"## Summary Statistics and Moments <a id=\"4\"><\/a>","71bd44e0":"Linear Regression can be performed through Ordinary Least Squares (OLS) or Maximum Likelihood Estimation (MLE).\n\nMost Python libraries use OLS to fit linear models.\n\n![](https:\/\/image.slidesharecdn.com\/simplelinearregressionpelatihan-090829234643-phpapp02\/95\/simple-linier-regression-9-728.jpg?cb=1251589640)","fcd0b9a3":"### Logistic regression\n\nWith logistic regression we use a link function like the inverse logit function mentioned above to model a binary dependent variable. While a linear regression model predicts the expected value of y given x directly, a GLM uses a link function. \n\nWe can easily implement logistic regression with [sklearn's Logistic Regression function.](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)","a5e97a17":"Here we visualize a PMF of a binomial distribution. You can see that the possible values are all integers. For example, no values are between 50 and 51. \n\nThe PMF of a binomial distribution in function form:\n\n![](http:\/\/reliabilityace.com\/formulas\/binomial-pmf.png)\n\nSee the \"[Distributions](#3)\" sections for more information on binomial distributions.","b02e3a13":"### Log-Normal Distribution","f1ede87f":"The PDF is the same as a PMF, but continuous. It can be said that the distribution has an infinite number of possible values. Here we visualize a simple normal distribution with a mean of 0 and standard deviation of 1.\n\nPDF of a normal distribution in formula form:\n\n![](https:\/\/www.mhnederlof.nl\/images\/normalpdf.jpg)","11be842f":"Tukey suggested that an observation is an outlier whenever an observation is 1.5 times the interquartile range below the first quartile or 1.5 times the interquartile range above the third quartile. This may sound complicated, but is quite intuitive if you see it visually.\n\nFor normal distributions, Tukey\u2019s criteria for outlier observations is unlikely if no outliers are present, but using Tukey\u2019s criteria for other distributions should be taken with a grain of salt.\n\nThe formula for Tukey's method:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/2a103bbd9233d9f8f711a7c76dfeb9694446f860)\n\nYa is the larger of two means being compared. SE is the standard error of the sum of the means.\n\n[Source](https:\/\/en.wikipedia.org\/wiki\/Tukey%27s_range_test)","13641646":"### PMF (Probability Mass Function)","3e00ac4b":"## Overfitting <a id=\"20\"><\/a>","7468cc6f":"Covariance is a measure of how much two random variables vary together. variance is similar to covariance in that variance shows you how much one variable varies. Covariance tells you how two variables vary together.\n\nIf two variables are independent, their covariance is 0. However, a covariance of 0 does not imply that the variables are independent.","6bd8a103":"### Poisson Distribution","3b4a3ffa":"In Grubbs test, the null hypothesis is that no observation is an outlier, while the alternative hypothesis is that there is one observation an outlier. Thus the Grubbs test is only searching for one outlier observation.\n\nThe formula for Grubbs test:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/bafc310f1dbca658728c73256fed19b6a7f11130)\n\nWhere Y_hat is the sample mean and s is the standard deviation. The Grubbs test statistic is the largest absolute deviation from the sample mean in units of the sample standard deviation.\n\n[Source](https:\/\/en.wikipedia.org\/wiki\/Grubbs%27_test_for_outliers)","bb94f877":"## Outliers <a id=\"12\"><\/a>","09156c6f":"### Normal Distribution","c77c5007":"**Mean, Median and Mode** \n\nNote: The mean is also called the first moment.\n\n\n![](https:\/\/qph.fs.quoracdn.net\/main-qimg-29a4925034e075f16e1c743a4b3dda8b)","88ad07bd":"# The end!","6d792a17":"An outlier is an observation which deviates from other observations. An outlier often stands out and could be an error.\n\nOutliers can mess up you statistical models. However, outliers should only be removed when you have established good reasons for removing the outlier.\n\nSometimes the outliers are the main topic of interest. This is for example the case with fraud detection. There are many outlier detection methods, but here we will discuss Grubbs test and Tukey\u2019s method. Both tests assume that the data is normally distributed.","0e3d4f29":"Frequentist:\n\n1. Fixed parameters (Processes are fixed)\n2. Repeated sampling -> Probabilities\n\nBayes:\n\n1. Probability as \"degree of belief\"\n2. P(parameter) -> All plausible values of the parameter\n3. Updates degree of belief based on a prior belief\n\n\nFrequentists and Bayesians agree that Bayes' Theorem is valid. See figure below for explanation of this theorem.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*LB-G6WBuswEfpg20FMighA.png)\n\n\nBayes theorem extends to distributions and random variables.\n","103817a2":"## Bootstrapping <a id=\"10\"><\/a>","36a86607":"## Bias, MSE and SE <a id=\"5\"><\/a>","5cebeb1e":"The coefficients of a linear model can also be computed using MSE (Mean Squared Error) without an iterative approach. I implemented Python code for this technique as well. The code is in [the second cell of this Github repository](https:\/\/github.com\/CarloLepelaars\/linreg\/blob\/master\/linreg_from_scratch.ipynb)","7311b006":"## Correlation <a id=\"8\"><\/a>","b4e7c2b4":"We can also implement linear regression with a bare-bones approach. In the following example we measure the vertical distance and horizontal distance between a random data point and the regression line. \n\nFor more information on implementing linear regression from scratch [I highly recommend this explanation by Luis Serrano](https:\/\/aitube.io\/video\/introduction-to-linear-regression).","1bcd6ea1":"### Grubbs Test","30062219":"We establish two hypotheses, H0 (Null hypothesis) and Ha (Alternative Hypothesis). \n\nWe can make four different decisions with hypothesis testing:\n1. Reject H0 and H0 is not true (no error)\n2. Do not reject H0 and H0 is true (no error)\n3. Reject H0 and H0 is true (Type 1 Error)\n4. Do not reject H0 and H0 is not true (Type 2 error)\n\nType 1 error is also called Alpha error.\nType 2 error is also called Beta error.\n\n![](https:\/\/qph.fs.quoracdn.net\/main-qimg-84121cf5638cbb5919999b2a8d928c91)\n\n![](https:\/\/i.stack.imgur.com\/x1GQ1.png)","0d8f42f6":"## Hypothesis testing <a id=\"11\"><\/a>","ffa46755":"## Linear regression <a id=\"9\"><\/a>","b17291dd":"## Distributions <a id=\"3\"><\/a>","acaa35f0":"- [Preparation](#1)\n- [Discrete and Continuous Variables](#2)\n  - PMF (Probability Mass Function)\n  - PDF (Probability Density Function)\n  - CDF (Cumulative Distribution Function)\n- [Distributions](#3)\n  - Uniform Distribution\n  - Normal Distribution\n  - Binomial Distribution\n  - Poisson Distribution\n  - Log-normal Distribution\n- [Summary Statistics and Moments](#4)\n- [Bias, MSE and SE](#5)\n- [Sampling Methods](#6)\n- [Covariance](#7)\n- [Correlation](#8)\n- [Linear Regression](#9)\n  - Anscombe's Quartet\n- [Bootstrapping](#10)\n- [Hypothesis Testing](#11)\n  - p-value\n  - q-q plot\n- [Outliers](#12)\n  - Grubbs Test\n  - Tukey's Method\n- [Overfitting](#20)\n  - Prevention of Overfitting\n  - Cross-Validation\n- [Generalized Linear Models (GLMs)](#13)\n  - Link Functions\n  - Logistic Regression\n- [Frequentist vs. Bayes](#14)\n- [Bonus: Free Statistics Courses](#15)\n- [Sources](#16)","1db8736e":"The CDF maps the probability that a random variable X will take a value of less than or equal to a value x (P(X \u2264  x)). CDF's can be discrete or continuous. In this section we visualize the continuous case. You can see in the plot that the CDF accumulates all probabilities and is therefore bounded between 0 \u2264 x \u2264 1.\n\nThe CDF of a normal distribution as a formula:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/187f33664b79492eedf4406c66d67f9fe5f524ea)\n\n*Note: erf means \"[error function](https:\/\/en.wikipedia.org\/wiki\/Error_function)\".*","c3d791f2":"### Binomial Distribution","d542b353":"Anscombe's quartet is a set of four datasets that have the same descriptive statistics and linear regression fit. The datasets are however very different from each other.\n\nThis sketches the issue that although summary statistics and regression models are really helpful in understanding your data, you should always visualize the data to see whats really going on. It also shows that a few outliers can really mess up your model.\n\n[More information on Anscombe's Quartet](https:\/\/en.wikipedia.org\/wiki\/Anscombe%27s_quartet)","ab3a267f":"**Bias** is a measure of how far the sample mean deviates from the population mean. The sample mean is also called **Expected value**.\n\nFormula for Bias:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/82a9c6501a54260ed0edd2f03923719b9f2db906)\n\nThe formula for expected value (EV) makes it apparent that the bias can also be formulated as the expected value minus the population mean:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/12828b1f927b39d2fa9d75f82c02b91209182911)","0a8319b5":"The **Standard Error (SE)** measures how spread the distribution is from the sample mean.\n\n![](http:\/\/desktopia.net\/p\/2018\/07\/standard-deviation-biology-for-life-in-standard-error-of-the-mean-formula.gif)\n\nThe formula can also be defined as the standard deviation divided by the square root of the number of samples.\n\n![](https:\/\/toptipbio.com\/wp-content\/uploads\/2017\/07\/Standard-error-formula.jpg)","2ca7474f":"### Link functions","5ca04cf3":"Here we observe that the linear model is well-fitted. However, a linear model is probably not ideal for our data, because the data follows a quadratic pattern. A [polynomial regression model](https:\/\/en.wikipedia.org\/wiki\/Polynomial_regression) would better fit the data, but this is outside the scope of this tutorial.","144a6e5a":"## Discrete and Continuous Variables <a id=\"2\"><\/a>","1a68c8a7":"### CDF (Cumulative Distribution Function)","95faa571":"## Frequentist vs. Bayes <a id=\"14\"><\/a>","2fbb9b1e":"https:\/\/dataconomy.com\/2015\/02\/introduction-to-bayes-theorem-with-python\n\nhttps:\/\/www.statisticshowto.datasciencecentral.com\/discrete-variable\/\n\nhttps:\/\/www.statisticshowto.datasciencecentral.com\/probability-and-statistics\/binomial-theorem\/binomial-distribution-formula\/\n\nhttps:\/\/www.statisticshowto.datasciencecentral.com\/probability-and-statistics\/descriptive-statistics\/sample-variance\/\n\nhttps:\/\/machinelearningmastery.com\/a-gentle-introduction-to-the-bootstrap-method\n\nhttps:\/\/machinelearningmastery.com\/k-fold-cross-validation\/\n\nhttps:\/\/en.wikipedia.org\/wiki\/Poisson_distribution\n\nhttps:\/\/www.tutorialspoint.com\/python\/python_p_value.htm\n\nhttps:\/\/towardsdatascience.com\/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f\n\nhttps:\/\/www.slideshare.net\/dessybudiyanti\/simple-linier-regression\n\nhttps:\/\/www.youtube.com\/channel\/UCgBncpylJ1kiVaPyP-PZauQ\n\nhttps:\/\/en.wikipedia.org\/wiki\/Bias%E2%80%93variance_tradeoff","55f52d05":"Bootstrapping is a resampling technique to quantify the uncertainty of an estimator given sample data. In other words, we have a sample of data and we take multiple samples from that sample. For example, with bootstrapping we can take means for each bootstrap sample and thereby make a distribution of means.\n\nOnce we created a distribution of estimators we can use this to make decisions. \n\nBootstrapping can be:\n1. Non-parametric (Take random samples from sample)\n2. Parametric (Take from a (normal) distribution with sample mean and variance).\n    Downside: You are making assumptions about the distribution.\n    Upside: Computationally more light\n3. Online bootstrap (Take samples from a stream of data)\n\nThe following code implements a simple non-parametric bootstrap to create a distribution of means, medians and midranges of the Income distribution in our Toy Dataset. We can use this to deduce which income means will make sense for subsequent samples.\n","794165ba":"## Table of contents","d2975880":"**Non-Representative Sampling:**\n\nConvenience Sampling = Pick samples that are most convenient, like the top of a shelf or people that can be easily approached.\n\nHaphazard Sampling = Pick samples without thinking about it. This often gives the illusion take you are picking out samples at random. \n\nPurposive Sampling = Pick samples for a specific purpose. An example is to focus on extreme cases. This can be useful but is limited because it doesn't allow you to make statements about the whole population.\n\n**Representative Sampling:**\n\nSimple Random Sampling = Pick samples (psuedo)randomly.\n\nSystematic Sampling = Pick samples with a fixed interval. For example every 10th sample (0, 10, 20, etc.).\n\nStratified Sampling = Pick the same amount of samples from different groups (strata) in the population.\n\nCluster Sampling = Divide the population into groups (clusters) and pick samples from those groups.","431d5ba6":"Another method for calculating a correlation coefficient is 'Spearman's Rho'. The formula looks different but it will give similar results as Pearson's method. In this example we see almost no difference, but this is partly because it is obvious that the Age and Income columns in our dataset have no correlation.\n\nFormula for Spearmans Rho:\n\n![](http:\/\/s3.amazonaws.com\/hm_120408\/fa\/3d86\/yhf5\/9dwq\/4m6e2kcav\/original.jpg?1447778688)","ccf34457":"### Anscombe's Quartet","1e5dcb37":"### P-Value\n\nA p-value is the probability of finding equal or more extreme results when the null hyptohesis (H0) is true. In other words, a low p-value means that we have compelling evidence to reject the null hypothesis.\n\nIf the p-value is lower than 5% (p < 0.05). We often reject H0 and accept Ha is true. We say that p < 0.05 is statistically significant, because there is less than 5% chance that we are wrong in rejecting the null hypothesis.\n\nOne way to calculate the p-value is through a T-test. We can use [Scipy's ttest_ind function](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.ttest_ind.html) to calculate the t-test for the means of two independent samples of scores. In this example we calculate the t-statistic and p-value of two random samples 10 times. \n\nWe see that the p-value is sometimes very low, but this does not mean that these two random samples are correlated. This is why you have to be careful with relying too heavily of p-values. If you repeat an experiment multiple times you can get trapped in the illusion that there is correlation where there is only randomness.\n\n[This xkcd comic perfectly illustrates the hazards of relying too much on p-values](https:\/\/xkcd.com\/882\/).","68d1a096":"## Preparation <a id=\"1\"><\/a>","45664f3c":"Correlation is a standardized version of covariance. Here it becomes more clear that Age and Income do not have a strong correlation in our dataset.","56855d97":"There is a lot of free material online for people who want to dive deeper into statistics. Here is a selection from the Internet.\n\nUdacity's \"Intro to Statistics\": https:\/\/eu.udacity.com\/course\/intro-to-statistics--st101\n\nUdacity's \"Intro to Descriptive Statistics\": https:\/\/eu.udacity.com\/course\/intro-to-descriptive-statistics--ud827\n\nUdacity's \"Intro to Inferential Statistics\": https:\/\/eu.udacity.com\/course\/intro-to-inferential-statistics--ud201\n\nedX's \"Introduction to Probability - The Science of Uncertainty\" : https:\/\/www.edx.org\/course\/introduction-probability-science-mitx-6-041x-2\n\nKhan Academy's videos on statistics and probability: https:\/\/www.khanacademy.org\/math\/statistics-probability\n\n(Kaggle Kernel) Mathematics of Linear Regression by Nathan Lauga: https:\/\/www.kaggle.com\/nathanlauga\/mathematics-of-linear-regression","28b5e61e":"A normal distribution (also called Gaussian or Bell Curve) is very common and convenient. This is mainly because of the [Central Limit Theorem (CLT)](https:\/\/en.wikipedia.org\/wiki\/Central_limit_theorem), which states that as the amount independent random samples (like multiple coin flips) goes to infinity the distribution of the sample mean tends towards a normal distribution.\n\nPDF of a normal distribution:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/2ce7e315b02666699e0cd8ea5fb1a3e0c287cd9d)\n\nCDF:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/187f33664b79492eedf4406c66d67f9fe5f524ea)\n","f8e671d4":"### Cross Validation\n\nCross validation is a technique to estimate the accuracy of our statistical model. It is also called out-of-sample testing or rotation estimation. Cross validation will help us to recognize overfitting and to check if our model generalizes to new (out-of-sample) data.\n\nA popular cross validation technique is called k-fold cross validation. The idea is simple, we split our dataset up in k datasets and out of each dataset k we pick out a few samples. We then fit our model on the rest of k and try to predict the samples we picked out. We use a metric like Mean Squared Error to estimate how good our predictions are. This procedure is repeated and then we look at the average of the predictions over multiple cross-validation data sets. \n\nA special case where we pick out one samples is called 'Leave-One-Out Cross Validation (LOOCV)'. However, the variance of LOOCV is high.\n\nFor more information about cross validation [check out this blog](https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/).\n\n"}}