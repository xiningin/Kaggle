{"cell_type":{"a1f9fd3c":"code","7fd7e811":"code","1a03480d":"code","062ec4cc":"code","8446e60e":"code","a4913b4f":"code","dc8bcb0c":"code","5c338e26":"code","b521c163":"code","d24e0484":"code","f5722d8c":"code","20512a01":"code","5a7b5f2f":"code","09f95cd8":"code","d05282a6":"code","50ec3f72":"code","2fab35af":"code","abcaa423":"code","4b70be76":"code","80e6e5ae":"code","954328cc":"code","c061b928":"code","caad2932":"code","1d42332b":"code","1c675fed":"code","aea2a08b":"code","84bc30b9":"code","86f32455":"code","b3804ad7":"code","7dfb49cf":"code","357d6eee":"code","533af777":"code","483c1267":"code","18673ad1":"markdown"},"source":{"a1f9fd3c":"import re\nimport string\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport nltk\nnltk.download('wordnet')\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport transformers\n\nimport tensorflow as tf\n","7fd7e811":"from tensorflow.keras.layers import  Dense, Input, Dropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam, SGD\n# from keras.preprocessing.text import Tokenizer\n# from keras.preprocessing.sequence import pad_sequences\n# from keras.models import Sequential\n# from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n# \n# from keras.initializers import Constant\n","1a03480d":"# Detect hardware, return appropriate distribution strategy\n\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS available: \", strategy.num_replicas_in_sync)","062ec4cc":"print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")","8446e60e":"train = pd.read_json('..\/input\/githubbug\/embold_train.json')\ntest = pd.read_json('..\/input\/githubbug\/embold_test.json')","a4913b4f":"train.head()","dc8bcb0c":"print('Train Data shape: ', train.shape)","5c338e26":"train.columns = train.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\ntest.columns = test.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')","b521c163":"train.head()","d24e0484":"train['label'].value_counts(normalize=True)","f5722d8c":"def remove_stopwords(string):\n    word_list = [word.lower() for word in string.split()]\n    stopwords_list = list(stopwords.words(\"english\"))\n    for word in word_list:\n        if word in stopwords_list:\n            word_list.remove(word)\n    return ' '.join(word_list)","20512a01":"for column in ['body', 'title']:\n    train[column] = train[column].map(lambda x: re.sub('\\\\n',' ',str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'\\W',' ',str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'https\\s+|www.\\s+',r'', str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'http\\s+|www.\\s+',r'', str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+',' ',str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'\\^[a-zA-Z]\\s+',' ',str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'\\s+',' ',str(x)))\n    train[column] = train[column].str.lower()\n\n    train[column] = train[column].map(lambda x: re.sub(r\"\\\u2019\", \"\\'\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"n\\\u2019t\", \" not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'s\", \" is\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\\u2019d\", \" would\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\d\", \" would\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"[0-9]\", \"digit\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'[?|!|\\'|\"|#]',r'', str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'[.|,|)|(|\\|\/]',r' ', str(x)))\n    train[column] = train[column].apply(lambda x: remove_stopwords(x))","5a7b5f2f":"train['description'] = train['title'] + \" \" + train['body']\ntrain.head()\n","09f95cd8":"def regular_encode(texts, tokenizer, maxlen = 512):\n    enc_di = tokenizer.batch_encode_plus(texts, return_attention_masks = False, return_token_type_ids = False, pad_to_max_length = True, max_length = maxlen)\n    return np.array(enc_di['input_ids'])\n","d05282a6":"tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')\n\n","50ec3f72":"X_train, X_test, y_train, y_test = train_test_split(train['description'], train['label'], random_state = 42, test_size = 0.3, stratify=train.label.values)\n","2fab35af":"y_train.value_counts(normalize=True)","abcaa423":"y_test.value_counts(normalize=True)","4b70be76":"X_train.head()","80e6e5ae":"Xtrain_encoded = regular_encode(X_train.astype('str'), tokenizer, maxlen = 128) \nytrain_encoded = tf.keras.utils.to_categorical(y_train, num_classes = 3, dtype = 'int32')\nXtest_encoded = regular_encode(X_test.astype('str'), tokenizer, maxlen = 128)\nytest_encoded = tf.keras.utils.to_categorical(y_test, num_classes = 3, dtype = 'int32')\n\n","954328cc":"def build_model(transformer, params, loss = 'categorical_crossentropy', max_len = 512):\n    input_word_ids = tf.keras.layers.Input(shape = (max_len,), dtype = tf.int32, name = \"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n\n    #adding dropout layer\n    x = tf.keras.layers.Dropout(params[\"droupout\"])(cls_token)\n\n    #using a dense layer of 3 neurons as the number of unique categories is 3. \n    out = tf.keras.layers.Dense(3, activation = 'sigmoid')(x)\n\n    model = tf.keras.Model(inputs = input_word_ids, outputs = out)\n    optimizer = SGD(learning_rate=params[\"learning_rate\"], momentum=params[\"momentum\"])\n    model.compile(optimizer=optimizer, loss = loss, metrics = ['accuracy'])\n#     model.compile(tf.keras.optimizers.Adam(lr = 3e-5), loss = loss, metrics = ['accuracy'])\n    return model","c061b928":"# with strategy.scope():\n#     transformer_layer = transformers.TFAutoModel.from_pretrained('bert-large-uncased')\n#     model = build_model(transformer_layer, max_len = 128)\n# model.summary()\n#with strategy.scope():\n# with tf.device('\/GPU:0'): \n#     transformer_layer = transformers.TFAutoModel.from_pretrained('bert-large-uncased')\n#     model = build_model(transformer_layer, max_len = 128)\n# model.summary()","caad2932":"BATCH_SIZE = 32*strategy.num_replicas_in_sync\n# BATCH_SIZE = 16\nAUTO = tf.data.experimental.AUTOTUNE \ntrain_dataset = (tf.data.Dataset.from_tensor_slices((Xtrain_encoded, ytrain_encoded)).repeat().shuffle(2048).batch(BATCH_SIZE).prefetch(AUTO))\ntest_dataset = (tf.data.Dataset.from_tensor_slices(Xtest_encoded).batch(BATCH_SIZE))\n","1d42332b":"def objective(trial):\n    params={\n        \"droupout\": trial.suggest_uniform(\"droupout\", 0.1, 0.7),\n        \"momentum\": trial.suggest_uniform(\"momentum\", 0.1, 0.9),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-3)\n    }\n    \n#     with tf.device('\/GPU:0'): \n    with strategy.scope():\n        transformer_layer = transformers.TFAutoModel.from_pretrained('bert-large-uncased')\n        model = build_model(transformer_layer, params, max_len = 128)\n    \n    #training for 20 epochs\n\n    n_steps = Xtrain_encoded.shape[0] \/\/ BATCH_SIZE\n    train_history = model.fit(train_dataset, steps_per_epoch = n_steps, epochs = 20)\n    \n    return train_history[\"loss\"]\n\n\n\n\n    \nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=20)\n\nprint(\"best trial:\")\ntrial_=study.best_trial\nprint(trial_.values)\nprint(trial_.params)","1c675fed":"# #training for 20 epochs\n\n# n_steps = Xtrain_encoded.shape[0] \/\/ BATCH_SIZE\n# train_history = model.fit(train_dataset, steps_per_epoch = n_steps, epochs = 20)\n\n","aea2a08b":"# #making predictions \n\n# preds = model.predict(test_dataset, verbose = 1)\n\n# #converting the one hot vector output to a linear numpy array.\n# pred_classes = np.argmax(preds, axis = 1)\n","84bc30b9":"print('Prediction Accuracy on Validation dataset: ', np.round(100*accuracy_score(pred_classes, y_test), 2), '%')\n","86f32455":"for column in ['body', 'title']:\n    test[column] = test[column].map(lambda x: re.sub('\\\\n',' ',str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r'\\W',' ',str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r'https\\s+|www.\\s+',r'', str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r'http\\s+|www.\\s+',r'', str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+',' ',str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r'\\^[a-zA-Z]\\s+',' ',str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r'\\s+',' ',str(x)))\n    test[column] = test[column].str.lower()\n\n    test[column] = test[column].map(lambda x: re.sub(r\"\\\u2019\", \"\\'\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"n\\\u2019t\", \" not\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"\\'s\", \" is\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"\\\u2019d\", \" would\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"\\d\", \" would\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"[0-9]\", \"digit\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r'[?|!|\\'|\"|#]',r'', str(x)))\n    test[column] = test[column].map(lambda x: re.sub(r'[.|,|)|(|\\|\/]',r' ', str(x)))\n    test[column] = test[column].apply(lambda x: remove_stopwords(x))","b3804ad7":"test['description'] = test['title'] + \" \" + test['body']\ntest.head()\n","7dfb49cf":"test['description'].head()","357d6eee":"test_encoded = regular_encode(test['description'].astype('str'), tokenizer, maxlen = 128)\npredict_dataset = (tf.data.Dataset.from_tensor_slices(test_encoded).batch(BATCH_SIZE))","533af777":"#making predictions \n\nprediction = model.predict(predict_dataset, verbose = 1)\n\n#converting the one hot vector output to a linear numpy array.\nprediction_classes = np.argmax(prediction, axis = 1)\n","483c1267":"submission = pd.DataFrame({'label':prediction_classes })\nsubmission.to_csv('submission.csv', index=False)","18673ad1":"# Predict on test data"}}