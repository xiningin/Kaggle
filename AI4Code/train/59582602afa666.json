{"cell_type":{"8a65bcd4":"code","dfba979b":"code","bca1f161":"code","362e27e6":"code","8201afc2":"code","94d33b61":"code","301ee5ab":"code","0f9b695c":"code","819bbed1":"code","99444707":"code","b97d5264":"code","b97075ae":"code","f52d553d":"code","50f13a3e":"code","fef1c7b6":"code","15537fb5":"code","a01be5cd":"code","1b8528bb":"code","394fb30e":"code","f5893fb0":"code","cd100463":"code","385887a6":"code","aea77657":"code","4559c656":"code","8a2d2efa":"markdown","3ac1597c":"markdown","5eb0c279":"markdown","d241011e":"markdown","30deda72":"markdown","4f19cec7":"markdown","f11b3bb1":"markdown","f900c552":"markdown","c9fd076b":"markdown","6462a50e":"markdown","caad4f1d":"markdown","0de110ea":"markdown","9c8dc90e":"markdown","47a0217f":"markdown","f15f4254":"markdown"},"source":{"8a65bcd4":"#Importing the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix","dfba979b":"path = '\/kaggle\/input\/mushroom-classification\/mushrooms.csv'\n\ndf = pd.read_csv(path)","bca1f161":"df.head()","362e27e6":"df.describe()","8201afc2":"df.info()","94d33b61":"#Replacing e with 1\n#Replacing  p with 0\ndf['class'] = df['class'].map({'e':1, 'p':0})","301ee5ab":"#Note that all our independent varibles are categorical variables\n#Using attribute get_dummies to convert conver our category variables into dummy indicator\ndf_dummies = pd.get_dummies(df, drop_first = True)","0f9b695c":"df_dummies.head()","819bbed1":"#Declaring our target variable as y\n#Declaring our independent variables as x\nx = df_dummies.drop('class', axis = 1)\ny = df_dummies['class']","99444707":"#Selecting the model\nreg = LogisticRegression()","b97d5264":"#Splitting our dataset into train and test datasets \nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 24)","b97075ae":"#We train the model with x_train and y_train\nreg.fit(x_train, y_train)","f52d553d":"#Predicting with our already trained model using x_test\ny_hat = reg.predict(x_test)","50f13a3e":"#Mesuring the accuracy of our model\nacc = metrics.accuracy_score(y_hat, y_test)\nacc","fef1c7b6":"#The intercept for our regression\nreg.intercept_","15537fb5":"#Coefficient for all our variables\nreg.coef_","a01be5cd":"cm = confusion_matrix(y_hat, y_test)\ncm","1b8528bb":"# Format for easier understanding\ncm_df = pd.DataFrame(cm)\ncm_df.columns = ['Predicted 0','Predicted 1']\ncm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\ncm_df","394fb30e":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier # for K nearest neighbours\nfrom sklearn import svm #for Support Vector Machine (SVM) ","f5893fb0":"dt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny1 = dt.predict(x_test)\nacc1 = metrics.accuracy_score(y1, y_test)\nacc1","cd100463":"kk = KNeighborsClassifier()\nkk.fit(x_train,y_train)\ny2 = kk.predict(x_test)\nacc2 = metrics.accuracy_score(y2, y_test)\nacc2","385887a6":"sv = svm.SVC()\nsv.fit(x_train,y_train)\ny3 = sv.predict(x_test)\nacc3 = metrics.accuracy_score(y3, y_test)\nacc3","aea77657":"pd.options.display.max_rows = 999\nresult = pd.DataFrame(data = x.columns, columns = ['Features'])\nresult['BIAS'] = np.transpose(reg.coef_)\nresult['odds'] = np.exp(np.transpose(reg.coef_))\nresult","4559c656":"#T be able to identify our refernce model\ndf['cap-shape'].unique()","8a2d2efa":"#### Our target column 'class' contains two unique values 'e' and 'p' which we will map to '1' and '0' respectively\n#### note 'e' stands for edible while 'p' for  poisonous","3ac1597c":"#### After comparison with some other model we see that Decision tree and KNeigbors both gave us 100% accuracy but our model was close enough with ~99.9% accuracy ","5eb0c279":"###  CONCLUSION\n#### Let's try to make a table and interpret what weight(BIAS) and odds means","d241011e":"### CONFUSION MATRIX","30deda72":"### DEALING WITH MISSING VALUES","4f19cec7":"### LOGISTIC REGRESSION\n#### Here we will create, fit and train our model in addition to that we will try to predict using the already trained model","f11b3bb1":"#### Using feature cap-shape as an example cap-shape_b is our baseline model i.e all cap-shape feature will be referenced to it and note that it is the only one that doesn't appear in the table above","f900c552":"#### If you find this notebook useful don't forget to upvote. #Happycoding","c9fd076b":"#### This dataset is clean it does not have any missing value","6462a50e":"### OTHER MODELS","caad4f1d":"#### Our model predicted '0' correctly 755 times while NEVER predicting '0' incorrectly \n#### Also it predicted  '1'  correctly 869 times while predicting '1' incorrectly  ONCE","0de110ea":"### INTRODUCTION\n\n#### Predicting the mushrooms mainly through logistic regression\n#### Mushroom class was classified into two categories  poisonous(0) and edible(1)\n#### Steps taken in preprocessing includes Data cleaning,etc\n#### All our variables in this dataset are categorical\n\n### SIDE NOTE\n#### You can leave your question about any unclear part in the comment section\n#### Any correction will be highly welcomed","9c8dc90e":"#### From the dataset description which can be found on kaggle 'b' represent bell while 's' represent sunken\n#### To interpret cap-shape_s in terms of odds we can say that  cap-shape_s  is almost twice more likely to cause a change in our target varable than cap-shape_b(our baseline model)","47a0217f":"### LOADING THE DATASET","f15f4254":"### DUMMY VARIABLES"}}