{"cell_type":{"58d1a664":"code","e959c0ac":"code","6e2a572e":"code","e47171d4":"code","ee52b42d":"code","2b3d3b8e":"code","684cacde":"code","19a0b463":"code","fbda2b86":"code","0b3d6723":"code","5e3238ab":"code","3050465c":"code","1915a651":"code","5d615117":"code","874aaf89":"code","09e12599":"code","9dc2df1e":"code","4f64212a":"code","e0b8cd4c":"code","de0131ad":"markdown","fff62ad7":"markdown","5a18ac9b":"markdown","669421a0":"markdown","fd1e882f":"markdown","4885a03f":"markdown","15b41163":"markdown","548f933e":"markdown","502f7fc4":"markdown","cc88db27":"markdown","6c90ddfb":"markdown","bfd5ebca":"markdown","ac4bd24e":"markdown"},"source":{"58d1a664":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.datasets import mnist\nfrom keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport math\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e959c0ac":"tf.__version__","6e2a572e":"#dataset\n#test data,test label and trainin data and trainin label\n(X_train_orig,y_train_orig),((X_test_orig,y_test_orig))=mnist.load_data()","e47171d4":"index=100\nfig,axs=plt.subplots(1,5,figsize=(20,10))\n\nfor i in range(5):\n    digit=X_train_orig[index]\n    #tahmin i\u00e7in kullan\u0131lacak resmin yeniden boyutland\u0131rmesi\n    digit=digit.reshape(28,28)\n    axs[i].imshow(digit,plt.cm.binary)\n    axs[i].set_title(\"number {}\".format(y_train_orig[index]))\n    index+=16\n                     ","ee52b42d":"def convert_to_one_hot(Y, C):\n    \"\"\"\n    Y=test or train data label\n    c=number of  classes\n    \"\"\"\n    Y = np.eye(C)[Y.reshape(-1)].T\n    return Y","2b3d3b8e":"#flatten test and train images\nX_train_flatten=X_train_orig.reshape(X_train_orig.shape[0], -1).T\nX_test_flatten=X_test_orig.reshape(X_test_orig.shape[0],-1).T\n\n#normalize images\nX_train = X_train_flatten\/255\nX_test = X_test_flatten\/255\n\ny_train=convert_to_one_hot(y_train_orig,10)\ny_test=convert_to_one_hot(y_test_orig,10)\n\nprint (\"number of training examples = \" + str(X_train.shape[1]))\nprint (\"number of test examples = \" + str(X_test.shape[1]))\nprint (\"X_train shape: \" + str(X_train.shape))\nprint (\"Y_train shape: \" + str(y_train.shape))\nprint (\"X_test shape: \" + str(X_test.shape))\nprint (\"Y_test shape: \" + str(y_test.shape))","684cacde":"def create_placeholders(n_x,n_y):\n    \"\"\"\n    n_x=size of image vector[28*28]\n    n_y=number of classes[0 to 9:so 10]\n    \n    X: placeholder for data input, shape[n_x,None]\n    Y: placeholder for input classes, shape[n_y,None]\n    \"\"\"\n    X=tf.placeholder(shape=(n_x,None),dtype=tf.float32)\n    Y=tf.placeholder(shape=(n_y,None),dtype=tf.float32)\n    \n    return X,Y","19a0b463":"X,Y=create_placeholders(784,10)\nprint(\"X:\",str(X))\nprint(\"Y:\",str(Y))","fbda2b86":"def initialize_parameters():\n    \"\"\"\n        parameters shape ara\n        W1=[128,X.shape[0]]\n        b1=[128,1]\n        W2=[64,128]\n        b2=[64,1]\n        W3=[20,64]\n        b3=[20,1]\n        W4=[10,20]\n        b4=[10,1]\n    \"\"\"\n    \n    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n    \n    #using tensorflow define parameter\n    W1 = tf.get_variable(\"W1\", [128,784], initializer =  tf.contrib.layers.xavier_initializer(seed = 1))\n    b1 = tf.get_variable(\"b1\", [128,1], initializer = tf.zeros_initializer())\n    W2 = tf.get_variable(\"W2\", [64,128], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b2 = tf.get_variable(\"b2\", [64,1], initializer = tf.zeros_initializer())\n    W3 = tf.get_variable(\"W3\", [20,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b3 = tf.get_variable(\"b3\", [20,1], initializer = tf.zeros_initializer())\n    W4 = tf.get_variable(\"W4\", [10,20], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b4 = tf.get_variable(\"b4\", [10,1], initializer = tf.zeros_initializer())\n   \n    \n    parameters = { \"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2,\n                  \"W3\": W3,\n                  \"b3\": b3,\n                  \"W4\": W4,\n                  \"b4\": b4,}\n    \n    return parameters","0b3d6723":"tf.reset_default_graph()\nwith tf.Session() as sess:\n    parameters = initialize_parameters()\n    print(\"W1 = \" + str(parameters[\"W1\"]))\n    print(\"b1 = \" + str(parameters[\"b1\"]))\n    print(\"W2 = \" + str(parameters[\"W2\"]))\n    print(\"b2 = \" + str(parameters[\"b2\"]))\n    print(\"W3 = \" + str(parameters[\"W3\"]))\n    print(\"b3 = \" + str(parameters[\"b3\"]))\n    print(\"W4 = \" + str(parameters[\"W4\"]))\n    print(\"b4 = \" + str(parameters[\"b4\"]))","5e3238ab":"def forward_propagation(X,parameters):\n    \n    \"\"\"\n    forward_propagataion?linear->relu->linear->relu->linear->relu->linear->softmax\n    \"\"\"\n    W1=parameters[\"W1\"]\n    b1=parameters[\"b1\"]\n    W2=parameters[\"W2\"]\n    b2=parameters[\"b2\"]\n    W3=parameters[\"W3\"]\n    b3=parameters[\"b3\"]\n    W4=parameters[\"W4\"]\n    b4=parameters[\"b4\"]\n    \n    Z1=tf.add(tf.matmul(W1,X),b1)       #np.dot(W1,x)+b1\n    A1=tf.nn.relu(Z1)\n    Z2=tf.add(tf.matmul(W2,A1),b2)      #np.dot(W2,A1)+b2\n    A2=tf.nn.relu(Z2)\n    Z3=tf.add(tf.matmul(W3,A2),b3)      #np.dot(W3,A2)+b3\n    A3=tf.nn.relu(Z3)\n    Z4=tf.add(tf.matmul(W4,A3),b4)       #np.dot(W4,A3)+b4\n    \n    return Z4\n    ","3050465c":"tf.reset_default_graph()\n\nwith tf.Session() as sess:\n    X, Y = create_placeholders(784, 10)\n    parameters = initialize_parameters()\n    Z4 = forward_propagation(X, parameters)\n    print(\"Z4 = \" + str(Z4))","1915a651":"def compute_cost(Z4,Y):\n    \"\"\"\n     Z3=output of forward propagation for  last linear layer\n    \"\"\"\n    logits = tf.transpose(Z4)\n    labels = tf.transpose(Y)\n    \n    cost=  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits =logits, labels = labels))\n    \n    return cost\n  ","5d615117":"tf.reset_default_graph()\n\nwith tf.Session() as sess:\n    X, Y = create_placeholders(784, 10)\n    parameters = initialize_parameters()\n    Z3 = forward_propagation(X, parameters)\n    cost=compute_cost(Z3,Y)\n    print(\"Z3 = \" + str(Z3))","874aaf89":"def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0): \n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    \n    permutation = list(np.random.permutation(m))#Randomly permute a sequence, or return a permuted range.\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n\n    \n    num_complete_minibatches = math.floor(m\/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]#x[:,0:64],x[:,64:128]...\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    \n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","09e12599":"def model(X_train,y_train,X_test,y_test,learning_rate=0.0001,\n          num_epochs=50,minibatch_size=32):\n    \n    \n    ops.reset_default_graph()\n    tf.set_random_seed(1)    \n    (n_x,m)=X_train.shape\n    n_y=y_train.shape[0]\n    \n    seed=1\n    \n    costs=[] #for plotting cost fuction decleraed costs list\n    \n    #call placeholder fonction\n    X,Y=create_placeholders(n_x,n_y)\n    \n    #initialize parameter\n    parameters=initialize_parameters()\n    \n    #forward propagation\n    Z3=forward_propagation(X,parameters)\n    \n    #compute cost\n    cost=compute_cost(Z3,Y)\n    \n    #Backward propagation and define optimizer \n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n    \n    #initialize all parameters\n    init=tf.global_variables_initializer()\n    \n    with tf.Session() as sess:\n        sess.run(init)\n        \n        \n        \n        for epoch in range(num_epochs):\n            \n            epoch_cost=0\n            \n            number_of_minibatches=m\/minibatch_size\n            \n            #chosing mini-batch\n            seed=seed+1#for np.andom.seed()\n            \n            minibatches=random_mini_batches(X_train,y_train,minibatch_size,seed)\n            \n            for minibatch in minibatches:\n                (minibatch_X,minibatch_Y)=minibatch\n                \n                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n                \n                epoch_cost += minibatch_cost \/ minibatch_size\n            \n            if (epoch % 5 == 0):\n                print (\"Cost   after epoch %i: %f\" % (epoch, epoch_cost))\n            if (epoch % 2 == 0):\n                costs.append(epoch_cost)\n                \n                    \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per fives)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # lets save the parameters in a variable\n        parameters = sess.run(parameters)\n        print (\"Parameters have been trained!\")\n\n        # Calculate the correct predictions\n        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: y_train}))\n        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: y_test}))\n           \n        \n        \n        return parameters\n                  ","9dc2df1e":"### Run the Model and ploting Cost\nparameters = model(X_train, y_train, X_test, y_test)","4f64212a":"def predict(X, parameters):\n    \n    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n    W4 = tf.convert_to_tensor(parameters[\"W4\"])\n    b4 = tf.convert_to_tensor(parameters[\"b4\"])\n    \n    params = {\"W1\": W1,\n              \"b1\": b1,\n              \"W2\": W2,\n              \"b2\": b2,\n              \"W3\": W3,\n              \"b3\": b3,\n              \"W4\": W4,\n              \"b4\": b4}\n    \n    x = tf.placeholder(\"float\", [784, 1])\n    \n    #for predict forward propagation\n    z4 = forward_propagation(x, params)\n    p = tf.argmax(z4)\n    \n    sess = tf.Session()\n    prediction = sess.run(p, feed_dict = {x: X})\n        \n    return prediction","e0b8cd4c":"index=100\nfig,axs=plt.subplots(1,5,figsize=(20,10))\n\nfor i in range(5):\n    digit=X_test_orig[index]\n    digit_pred=digit.reshape(784,-1)\n    #tahmin i\u00e7in kullan\u0131lacak resmin yeniden boyutland\u0131rmesi\n    digit=digit.reshape(28,28)\n    axs[i].imshow(digit,plt.cm.binary)\n    axs[i].set_title(\"predict {}\".format(predict(digit_pred,parameters)))\n    index+=16\n                ","de0131ad":"### Build The Model ","fff62ad7":"### Predict Function","5a18ac9b":"#### visualizing some data","669421a0":"### Initializing Parameters\n\nnetwork has 4 layers and layer1 has 128 node, layer2 has 64 node,layer3 has 20 node,layer4 has 10(number of classes) node.we are using xavier initalization for weihgts and zero initialization for bias.\n","fd1e882f":"<h1>Implementation Four Layer Neural Network Step by Step with MNIST Dataset for  Digit Recognition<\/h1>","4885a03f":"### Tets Model","15b41163":"### Create placeholder for X and Y","548f933e":"<h2>ANN Model<\/h2>\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/610146\/1092304\/network.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1589627019&Signature=VMDeBYsjmm3CzADXsBvJ6Fvs%2Btb%2BEB63mDUJXsW4IXaSknakLwK3Ks2mkcC0JeS%2Flhjf3m4meuSXY2Z1QCvf0cT10bDjkCO3OWVfg7UwAc5GjDWqBPPhEduzIGrKtju1CBFDbS35gqmowXvqudZVwkEkCbw2e3w21Fdvt1JCq%2FTG%2Fno3jwpUvb3uyJsNY12fPp7axLeqvZiEK2beayyFs7T793Tkgddcvq2gCNT0htHLMUTRWXVuKIOc0I0%2BOjFWmgxFFI3WL%2Byw%2BX2e87eGMVdhxEpV5GylV%2BG%2F2wM01ta%2BL4pOgyaMtEKioPmxJBWCnGqUVgWgzAiKBGGhwaDGIQ%3D%3D\" width=\"1200\" height=\"1200\">","502f7fc4":"<h2>Prepare Data For Model<\/h2>","cc88db27":"### Define Function for Choosing Random mini-batches","6c90ddfb":"### Compute Cost","bfd5ebca":"###  Backward propagation & parameter updates\n\nTensorflow handles all backpropagation and parameter updates.","ac4bd24e":"### Forward Propagation"}}