{"cell_type":{"759fa04c":"code","e7de23ba":"code","f7625802":"code","ab646a34":"code","24ca9dd6":"code","3096ae72":"code","0079d3d2":"code","0cdd49c3":"code","290c6bff":"code","4b597b7b":"code","a763113d":"code","15c942bf":"code","a6ad645e":"code","f66f21f1":"code","f661f786":"code","20dbd4f6":"code","efcf7609":"code","4dede3b8":"code","a85ebd02":"code","a4c728a5":"code","a089b18d":"code","f7df5518":"code","620e46b0":"code","b4f95512":"code","11152afb":"code","d6520c30":"code","3ee04a10":"code","6c846331":"code","62cf4653":"code","5bfe243c":"code","7d2ecab6":"code","3f8d5d7c":"code","4e53f7c2":"code","cd00c043":"code","612dd6a8":"code","a7fbe82a":"code","40096b35":"code","0fcd3684":"markdown","4996d16b":"markdown","7a1e0b88":"markdown","c826f8f0":"markdown","07ceeda1":"markdown","264be7e3":"markdown","3ccd0ef7":"markdown","5a6e3356":"markdown","e711aca3":"markdown","343e7cf7":"markdown","de5a0e46":"markdown","c478f2dc":"markdown","50a4f083":"markdown"},"source":{"759fa04c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e7de23ba":"import numpy as np \r\nimport pandas as pd\r\n\r\nfrom scipy.stats import skew\r\nfrom scipy.stats.stats import pearsonr\r\n\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nplt.style.use('fivethirtyeight')\r\n\r\nimport warnings\r\nwarnings.filterwarnings('ignore')\r\n%matplotlib inline","f7625802":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\ntest_ids = test.Id","ab646a34":"train.shape","24ca9dd6":"test.shape","3096ae72":"train.head()","0079d3d2":"all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\r\n                      test.loc[:,'MSSubClass':'SaleCondition']))","0cdd49c3":"null_columns = all_data.columns[all_data.isna().any()].tolist()\nnull_columns_data = all_data[null_columns].isnull().sum()\nnull_columns_data","290c6bff":"all_data.drop(columns = null_columns_data[null_columns_data > 500].index, axis=1, inplace=True)","4b597b7b":"null_columns = all_data.columns[all_data.isna().any()].tolist()\r\nnull_columns_data = all_data[null_columns].isnull().sum()\r\nnull_columns_data","a763113d":"all_cat_cols = all_data.select_dtypes(include=['object']).columns.to_list()\nall_num_cols = all_data.select_dtypes(exclude=['object']).columns.to_list()\n\nnull_cat_cols = []\nfor col in all_cat_cols:\n    if col in null_columns:\n        null_cat_cols.append(col)\n        \nnul_num_cols = []\nfor col in all_num_cols:\n    if col in null_columns:\n        nul_num_cols.append(col)","15c942bf":"for col in null_cat_cols:\n     all_data[col].fillna(all_data[col].mode()[0], inplace=True)","a6ad645e":"all_data[nul_num_cols].head()","f66f21f1":"all_data[nul_num_cols].describe()","f661f786":"def fillna_num_col(cols, type):\n    for col in cols:\n        if type == \"mode\":\n            all_data[col].fillna(all_data[col].mode()[0], inplace=True)\n        elif type == \"median\":\n            all_data[col].fillna(all_data[col].median(), inplace=True)\n        elif type == \"mean\":\n            all_data[col].fillna(all_data[col].mean(), inplace=True)","20dbd4f6":"fillna_num_col(cols=[\"BsmtFullBath\", \"BsmtHalfBath\"], type=\"mode\")\nfillna_num_col(cols=[\"GarageCars\"], type=\"median\")\nfillna_num_col(cols=all_data.columns[all_data.isna().any()].tolist(), type=\"mean\")","efcf7609":"all_cat_cols = all_data.select_dtypes(include=['object']).columns.to_list()\nall_cat_cols","4dede3b8":" all_data = pd.get_dummies(all_data)","a85ebd02":"X = all_data.iloc[:train.shape[0], :]\ny = train.iloc[:, -1].tolist()\n\ntest = all_data.iloc[train.shape[0]:, :]","a4c728a5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","a089b18d":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\n\nfrom sklearn.metrics import r2_score\n\nMLA = [\n       DecisionTreeRegressor(),\n       LinearRegression(),\n       XGBRegressor()\n]\n\nrow_index = 0\nMLA_compare = pd.DataFrame()\n\nclassifier_names = []\nr2_scores = []\n\nfor classifier in MLA:\n    classifier.fit(X_train, y_train)\n\n    y_pred = classifier.predict(X_test)\n    classifier_r2_score = r2_score(y_test, y_pred)\n\n    MLA_name = classifier.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'R2 Score'] = classifier_r2_score\n\n    row_index+=1","f7df5518":"MLA_compare","620e46b0":"params = {\r\n    # Parameters that we are going to tune.\r\n    'max_depth':6,\r\n    'min_child_weight': 1,\r\n    'eta':.3,\r\n    'subsample': 1,\r\n    'colsample_bytree': 1,\r\n    # Other parameters\r\n    'objective':'reg:squarederror',\r\n}\r\n\r\ndtrain = xgb.DMatrix(X_train, label=y_train)\r\ndtest = xgb.DMatrix(X_test, label=y_test)\r\n\r\nparams['eval_metric'] = \"mae\"\r\n\r\nnum_boost_round = 999\r\n\r\nclassifier = XGBRegressor(\r\n    params,\r\n    dtrain,\r\n    num_boost_round=num_boost_round,\r\n    evals=[(dtest, \"Test\")],\r\n    early_stopping_rounds=10\r\n)","b4f95512":"cv_results = xgb.cv(\r\n    params,\r\n    dtrain,\r\n    num_boost_round=num_boost_round,\r\n    seed=42,\r\n    nfold=5,\r\n    metrics={'mae'},\r\n    early_stopping_rounds=10\r\n)\r\ncv_results","11152afb":"cv_results['test-mae-mean'].min()","d6520c30":"gridsearch_params = [\r\n    (max_depth, min_child_weight)\r\n    for max_depth in range(9,12)\r\n    for min_child_weight in range(5,8)\r\n]","3ee04a10":"min_mae = float(\"Inf\")\r\nbest_params = None\r\nfor max_depth, min_child_weight in gridsearch_params:\r\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\r\n                             max_depth,\r\n                             min_child_weight))\r\n    # Update our parameters\r\n    params['max_depth'] = max_depth\r\n    params['min_child_weight'] = min_child_weight\r\n    # Run CV\r\n    cv_results = xgb.cv(\r\n        params,\r\n        dtrain,\r\n        num_boost_round=num_boost_round,\r\n        seed=42,\r\n        nfold=5,\r\n        metrics={'mae'},\r\n        early_stopping_rounds=10\r\n    )\r\n    # Update best MAE\r\n    mean_mae = cv_results['test-mae-mean'].min()\r\n    boost_rounds = cv_results['test-mae-mean'].argmin()\r\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\r\n    if mean_mae < min_mae:\r\n        min_mae = mean_mae\r\n        best_params = (max_depth,min_child_weight)\r\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","6c846331":"params['max_depth'] = 10\r\nparams['min_child_weight'] = 6","62cf4653":"gridsearch_params = [\r\n    (subsample, colsample)\r\n    for subsample in [i\/10. for i in range(7,11)]\r\n    for colsample in [i\/10. for i in range(7,11)]\r\n]","5bfe243c":"min_mae = float(\"Inf\")\r\nbest_params = None\r\n# We start by the largest values and go down to the smallest\r\nfor subsample, colsample in reversed(gridsearch_params):\r\n    print(\"CV with subsample={}, colsample={}\".format(\r\n                             subsample,\r\n                             colsample))\r\n    # We update our parameters\r\n    params['subsample'] = subsample\r\n    params['colsample_bytree'] = colsample\r\n    # Run CV\r\n    cv_results = xgb.cv(\r\n        params,\r\n        dtrain,\r\n        num_boost_round=num_boost_round,\r\n        seed=42,\r\n        nfold=5,\r\n        metrics={'mae'},\r\n        early_stopping_rounds=10\r\n    )\r\n    # Update best score\r\n    mean_mae = cv_results['test-mae-mean'].min()\r\n    boost_rounds = cv_results['test-mae-mean'].argmin()\r\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\r\n    if mean_mae < min_mae:\r\n        min_mae = mean_mae\r\n        best_params = (subsample,colsample)\r\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","7d2ecab6":"params['subsample'] = .8\r\nparams['colsample_bytree'] = 1.","3f8d5d7c":"%time\r\n# This can take some time\u2026\r\nmin_mae = float(\"Inf\")\r\nbest_params = None\r\nfor eta in [.3, .2, .1, .05, .01, .005]:\r\n    print(\"CV with eta={}\".format(eta))\r\n    # We update our parameters\r\n    params['eta'] = eta\r\n    # Run and time CV\r\n    %time cv_results = xgb.cv(params, dtrain, num_boost_round=num_boost_round, seed=42, nfold=5, metrics=['mae'], early_stopping_rounds=10)\r\n    # Update best score\r\n    mean_mae = cv_results['test-mae-mean'].min()\r\n    boost_rounds = cv_results['test-mae-mean'].argmin()\r\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\r\n    if mean_mae < min_mae:\r\n        min_mae = mean_mae\r\n        best_params = eta\r\nprint(\"Best params: {}, MAE: {}\".format(best_params, min_mae))","4e53f7c2":"params['eta'] = .01","cd00c043":"model = xgb.train(\r\n    params,\r\n    dtrain,\r\n    num_boost_round=num_boost_round,\r\n    evals=[(dtest, \"Test\")],\r\n    early_stopping_rounds=10\r\n)","612dd6a8":"y_pred = model.predict(dtest)\r\nr2_score(y_test, y_pred)","a7fbe82a":"test = xgb.DMatrix(test)\r\ny_pred = model.predict(test)","40096b35":"output = pd.DataFrame({'Id': test_ids, 'SalePrice': y_pred})\noutput.to_csv('submission.csv', index=False)","0fcd3684":"# House Price Prediction","4996d16b":"### Filling Null Values in Categorical Columns by Mode (Most Repeated Value)","7a1e0b88":"### List of Columns with Null Values","c826f8f0":"## Model","07ceeda1":"## Preprocessing","264be7e3":"## Hyper Parameter Tuning XGBModel","3ccd0ef7":"## Importing Libraries and Setting Default Style","5a6e3356":"### List of Categorical Columns with Null Values","e711aca3":"## Merging Data for Easier Preprocessing","343e7cf7":"### Dealing With Null Values in Numeric Columns","de5a0e46":"## Importing Dataset","c478f2dc":"### Encoding Categorical Columns","50a4f083":"### Droping Columns with High Number of Null Values"}}