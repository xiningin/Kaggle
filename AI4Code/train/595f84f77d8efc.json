{"cell_type":{"ab795cf5":"code","ceb80f90":"code","cee1c208":"code","ac6aa4b0":"code","764d5d8b":"code","b9530526":"code","42e751b1":"code","6407562c":"code","e70e0501":"code","06dabf24":"code","32a45bd3":"code","cae2d80a":"code","c0ff8acc":"markdown","14e28bfe":"markdown","459474b2":"markdown","ad3df0a8":"markdown","2c23c890":"markdown","88b7ad09":"markdown","1ae51696":"markdown"},"source":{"ab795cf5":"import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder","ceb80f90":"def softmax(x, axis=1):\n    # Stable Softmax\n    # from https:\/\/eli.thegreenplace.net\/2016\/the-softmax-function-and-its-derivative\/\n    y = x- np.max(x, axis=axis, keepdims=True)\n    return np.exp(y) \/ np.sum(np.exp(y), axis=axis, keepdims=True)\n\ndef softmax_derivatives(s):\n    s = s.reshape(-1,1)\n    return np.diagflat(s) - np.dot(s, s.T)\n\ndef softmax_second_derivative(s, C=4):\n    d2 = np.zeros((C, C))\n    for i in range(C):\n        for j in range(C):\n            if i == j:\n                d2[i, j] = (1. - s[i]) * s[i] * (1. - 2. * s[i])\n            else:\n                d2[i, j] = s[i] * s[j] * (2. * s[i] - 1.)\n    return d2","cee1c208":"def kappa_grads_lightgbm(y_true, y_pred):\n    \n    number_classes = 4\n    C = number_classes\n    labels = y_true \n\n    batch_size = y_true.shape[0]\n    labels_one_hot = OneHotEncoder(categories=\n                           [range(C)]*1, sparse=False).fit_transform(y_true.reshape(-1, 1))\n    y_pred = np.reshape(y_pred, (batch_size, C), order='F')\n\n    y_pred = softmax(y_pred)\n    eps = 1e-12\n\n    wtc = (y_true.reshape(-1, 1) - range(C))**2 \/ ((C - 1)**2)\n    N = np.sum(wtc * y_pred)\n    dN_ds = wtc\n\n    Ni = np.sum(labels_one_hot, 0) \/ batch_size \n    repeat_op = np.tile(np.reshape(range(0, C), [C, 1]), [1, C])\n    repeat_op_sq = np.square((repeat_op - np.transpose(repeat_op)))\n    wij = repeat_op_sq \/ ((C - 1) ** 2)\n    \n    hist_preds = np.sum(y_pred, axis=0)\n    D = np.sum(Ni.reshape(-1, 1) * (wij * hist_preds.reshape(1, -1)))\n    dD_ds = np.tile(np.dot(wij, Ni), (batch_size, 1))\n    \n    dL_ds = dN_ds \/ (N + eps) - dD_ds \/ (D + eps)\n\n    dL_da = np.zeros_like(dL_ds)\n    ds_da = np.zeros((batch_size, C, C))\n    for i in range(batch_size):\n        ds_da[i] = softmax_derivatives(y_pred[i])\n        dL_da[i] = np.dot(ds_da[i], dL_ds[i])\n\n    d2L_da2 = np.zeros_like(dL_da)\n    for b in range(batch_size):\n        d2s_da2 = softmax_second_derivative(y_pred[b])\n        d2N = -np.dot(dN_ds[b].reshape([C, 1]), dN_ds[b].reshape(1, C)) \/ (N * N + eps)\n        d2D = np.dot(dD_ds[b].reshape([C, 1]), dD_ds[b].reshape(1, C)) \/ (D * D + eps)\n        d2L_ds2 = d2N + d2D\n        for c in range(C):\n            AA = ds_da[b,0,c]*(ds_da[b,0,c] * d2L_ds2[0,0] + \n                                ds_da[b,1,c] * d2L_ds2[0,1] + \n                                ds_da[b,2,c] * d2L_ds2[0,2] +\n                                ds_da[b,3,c] * d2L_ds2[0,3]\n                                ) + dL_ds[b,0] * d2s_da2[c, 0] \n\n            BB = ds_da[b,1,c]*(ds_da[b,0,c] * d2L_ds2[1,0] + \n                                ds_da[b,1,c] * d2L_ds2[1,1] + \n                                ds_da[b,2,c] * d2L_ds2[1,2] +\n                                ds_da[b,3,c] * d2L_ds2[1,3]\n                                ) + dL_ds[b,1] * d2s_da2[c, 1] \n\n            CC = ds_da[b,2,c]*(ds_da[b,0,c] * d2L_ds2[2,0] + \n                                ds_da[b,1,c] * d2L_ds2[2,1] + \n                                ds_da[b,2,c] * d2L_ds2[2,2] +\n                                ds_da[b,3,c] * d2L_ds2[2,3]\n                                ) + dL_ds[b,2] * d2s_da2[c, 2] \n\n            DD = ds_da[b,3,c]*(ds_da[b,0,c] * d2L_ds2[3,0] + \n                                ds_da[b,1,c] * d2L_ds2[3,1] + \n                                ds_da[b,2,c] * d2L_ds2[3,2] +\n                                ds_da[b,3,c] * d2L_ds2[3,3]\n                                ) + dL_ds[b,3] * d2s_da2[c, 3] \n\n            d2L_da2[b, c] = AA + BB + CC + DD\n    \n\n    return [dL_da.flatten('F'), np.abs(d2L_da2.flatten('F'))]","ac6aa4b0":"!pip install autograd","764d5d8b":"import autograd.numpy as np\nfrom autograd import grad\nfrom autograd import jacobian\nfrom autograd import hessian","b9530526":"# Test data\ny_pred = np.array([[ 0.89912265,  0.79084255,  0.32162871, -0.99296229],\n       [ 0.81017273,  0.18127493,  0.13865968, -0.32750946],\n       [ 1.24011418,  0.94562047, -0.19091468, -0.68148713],\n       [-1.10852297, -0.5044101 ,  0.41754522,  2.25403507],\n       [ 1.41286477, -0.43752987, -0.34757177, -0.35673979],\n       [ 3.19812033,  0.15338679, -1.12337413, -1.27692332],\n       [ 2.4100999 ,  0.23849515, -0.79835829, -1.22977774],\n       [-1.20118161, -1.21880044,  0.59375328,  3.13776406],\n       [ 0.35147176,  0.03808217, -0.14641639, -0.22850701],\n       [ 2.8068574 ,  0.30871835, -0.84488727, -1.28116301]])\n\ny_true = np.array([3, 0, 3, 3, 0, 0, 0, 3, 3, 0])","42e751b1":"def weighted_kappa(y_true, y_pred, C=4):\n    \n    batch_size = y_true.shape[0]\n    labels_one_hot = OneHotEncoder(categories=\n                           [range(C)]*1, sparse=False).fit_transform(y_true.reshape(-1, 1))\n    y_pred = softmax(y_pred)\n    eps = 1e-12\n\n    wtc = (y_true.reshape(-1, 1) - range(C))**2 \/ ((C - 1)**2)\n    N = np.sum(wtc * y_pred)\n    Ni = np.sum(labels_one_hot, 0) \/ batch_size \n\n    repeat_op = np.tile(np.reshape(range(0, C), [C, 1]), [1, C])\n    repeat_op_sq = np.square((repeat_op - np.transpose(repeat_op)))\n    wij = repeat_op_sq \/ ((C - 1) ** 2)\n\n    histp = np.sum(y_pred, axis=0)\n    D = np.sum(Ni.reshape(-1, 1) * (wij * histp.reshape(1, -1)))\n    \n    return np.log(N \/ (D + eps))","6407562c":"kappa_grad = grad(weighted_kappa, 1)\nkappa_hess = hessian(weighted_kappa, 1)","e70e0501":"[custom_grad, custom_hess] = kappa_grads_lightgbm(y_true, y_pred)\nautograd_grad = kappa_grad(y_true, y_pred).flatten('F')","06dabf24":"np.allclose(custom_grad, autograd_grad)","32a45bd3":"autograd_all_hess = kappa_hess(y_true, y_pred)\n# just need the diagonals to compare\nautograd_diag_hess = np.array([\n    autograd_all_hess[i, j, i, j]\n    for j in range(autograd_all_hess.shape[1])\n    for i in range(autograd_all_hess.shape[0])\n    ])","cae2d80a":"# use absolute val of autograd_hess since we used it in the custom loss\nnp.allclose(custom_hess, np.abs(autograd_diag_hess))","c0ff8acc":"## Introduction\nThis code in this kernel is for a custom loss function for LightGBM based on the paper https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0167865517301666.\n\nWilliam Wu published a kernel, https:\/\/www.kaggle.com\/wuwenmin\/dnn-and-effective-soft-quadratic-kappa-loss, using the same paper with an application to tensorflow.  He asks in this thread, https:\/\/www.kaggle.com\/c\/data-science-bowl-2019\/discussion\/121606, if anyone has used the weighted kappa loss and this is a what I have used.\n\nUsing this loss performs worse than regression in my limited tests.  It performs slightly better on validation when compared to LightGBM's multiclass objective.\n\nThe kernel is organized as follows:\n1. Background on the objective function\n2. Calculations of the gradient and hessian\n3. The code for custom objective for the scikit learn fit function\n4. Check derivatives using autograd to see if the gradients and second derivatives match\n","14e28bfe":"### Helper Functions","459474b2":"## Check with autograd","ad3df0a8":"## Code for LightGBM function\n\nA couple of notes. This isn't a convex function as far as I can tell by checking the hessian for positive semi-definiteness.  I could only get results using this by taking the absolute value of the hessian as a hack.  The scikit-learn api gives the predictions and takes the gradient and hessian in column order.","2c23c890":"## Gradient\n\nSince we are plugging the output of the LightGBM model and running it through softmax before the loss function, we will need the derivatives with respect to that LightGBM predictions.  \n\nFor the gradient, we need to calculate \n\n$$\\frac{\\partial \\mathscr{L}}{\\partial a_m}$$ \n\nwhere \n\n$m \\in \\{1, 2,..., C \\}$ , C is the number of classes \n\n$a_m$: is the output of the LightGBM model\n\nHere are the partials for the weighted kappa loss:\n\n$$\n\\frac{\\partial \\mathscr{L}}{\\partial s_m} = \\frac{1}{\\mathscr{N}}\\frac{\\partial \\mathscr{N}}{\\partial s_m} - \n\\frac{1}{\\mathscr{D}}\\frac{\\partial \\mathscr{D}}{\\partial s_m}\n$$\n\nwith \n\n$$\n\\frac{\\partial \\mathscr{N}}{\\partial s_m(X_k)} = \\omega_{{t_k} m}\n$$\n\n$$\n\\frac{\\partial \\mathscr{D}}{\\partial s_m(X_k)} = \\sum_{i=1}^{C} \\hat{N}_{i}\\omega_{i, m}\n$$\n\n$s_m$: softmax function of m-th class\n\nThe jacobian of the softmax is derived at this link https:\/\/eli.thegreenplace.net\/2016\/the-softmax-function-and-its-derivative\/.  The partial derivative for the i-th output for the j-th input is:\n\n$$\n\\frac{\\partial s_i}{\\partial a_j} = \\left\\{\n                \\begin{array}{ll}\n                  s_i(1 - s_j) \\quad i = j\\\\\n                  -s_i s_j  \\quad i \\ne j\n                \\end{array}\n              \\right.\n$$\n\nwhere\n\n$s_i$: i-th softmax output\n\n$a_j$: output of LightGBM and input to softmax function\n\nPutting the above together we have for each model output $a_{ij}$, batch b, and class m:\n\n$$\n\\frac{\\partial \\mathscr{L}}{\\partial a_{i j}} = \\sum_{b=1}^{Batch Size} \\sum_{m=1}^{Number Classes} \\frac{\\partial \\mathscr{L}}{\\partial s_{bm}} \\frac{\\partial s_{bm}}{\\partial a_{i j}}\n$$\n\nAs a concrete example, for batch size of 2 and 4 classes and starting the index at 0, we have:\n\n$$\n\\frac{\\partial \\mathscr{L}}{\\partial a_{00}} = \n\\frac{\\partial \\mathscr{L}}{\\partial s_{00}} \\frac{\\partial s_{00}}{\\partial a_{00}} + \n\\frac{\\partial \\mathscr{L}}{\\partial s_{01}} \\frac{\\partial s_{01}}{\\partial a_{00}} + \n\\frac{\\partial \\mathscr{L}}{\\partial s_{02}} \\frac{\\partial s_{02}}{\\partial a_{00}} + \n\\frac{\\partial \\mathscr{L}}{\\partial s_{03}} \\frac{\\partial s_{03}}{\\partial a_{00}} + \n\\frac{\\partial \\mathscr{L}}{\\partial s_{10}} \\frac{\\partial s_{10}}{\\partial a_{00}} + \n\\frac{\\partial \\mathscr{L}}{\\partial s_{11}} \\frac{\\partial s_{11}}{\\partial a_{00}} + \n\\frac{\\partial \\mathscr{L}}{\\partial s_{12}} \\frac{\\partial s_{12}}{\\partial a_{00}} + \n\\frac{\\partial \\mathscr{L}}{\\partial s_{13}} \\frac{\\partial s_{13}}{\\partial a_{00}}  \n$$\n\nwhere the last 4 terms are 0 since the softmax out is not a function of $a_{00}$.","88b7ad09":"## Background\nWilliam has an excellent summary of the weighted kappa loss in his kernel using the notation from the paper.  This will be using similar notation.  Here is a quick overview of the loss function.  \n\n$$\n\\text{ minimize  } \\mathscr{L}=\\log (1-\\kappa) \n$$\nwhere\n$$\n\\kappa = 1 - \\frac{\\mathscr{N}}{\\mathscr{D}}\n$$\n\nThe paper redefines the numerator and denominator in terms of softmax output probabilities.\n\n$\\mathscr{N}=\\sum_{i, j} \\omega_{i, j} O_{i, j}=\\sum_{k=1}^{N} \\sum_{c=1}^{C} \\omega_{t_{k}, c} P_{c}(X_{k})$\n\n$\\mathscr{D}=\\sum_{i, j} \\omega_{i, j} E_{i, j}=\\sum_{i=1}^{C} \\hat{N}_{i} \\sum_{j=1}^{C}\\left(\\omega_{i, j} \\sum_{k=1}^{N} P_{j}(X_{k})\\right)$\n\nwhere\n\n$X_{k}$: input data of the k-th sample\n\n$E_{i, j}=\\frac{N_{i} \\sum_{k=1}^{N} P_{j}(X_{k})}{N}= \\hat{N}_{i} \\sum_{k=1}^{N} P_{j}(X_{k})$\n\n$N$: number of samples\n\n$N_i$: number of samples of the i-th class \n\n$\\hat{N}_{i}=\\frac{N_{i}}{N}$\n\n$t_k$: correct class number for sample k\n\n$P_{c}\\left(X_{k}\\right)$: conditional probability that the k-th sample belongs to class c given that the true class is $t_k$\n\n$w_{ij} = \\frac{(i-j)^2}{(C-1)^2}$: C is number classes (same formula for $w_{t_kc}$)\n","1ae51696":"## Hessian\nFrom my understanding, LightGBM only requires the second derivatives for the ouput predictions and not\nthe full hessian.\n\nThis is an indexing nightmare, so I will continue with the concrete example above, where we now need to calculate:\n\n$$\n\\frac{\\partial}{\\partial a_{00}} \\frac{\\partial \\mathscr{L}}{\\partial a_{00}}\n$$\n\nUsing the non zero terms above:\n\n$$\n\\frac{\\partial}{\\partial a_{00}} (\\frac{\\partial \\mathscr{L}}{\\partial s_{00}} \\frac{\\partial s_{00}}{\\partial a_{00}} + \n\\frac{\\partial \\mathscr{L}}{\\partial s_{01}} \\frac{\\partial s_{01}}{\\partial a_{00}} + \n\\frac{\\partial \\mathscr{L}}{\\partial s_{02}} \\frac{\\partial s_{02}}{\\partial a_{00}} + \n\\frac{\\partial \\mathscr{L}}{\\partial s_{03}} \\frac{\\partial s_{03}}{\\partial a_{00}}) = \n\\frac{\\partial}{\\partial a_{00}} \\left(A + B + C + D \\right)\n$$\n\nFor the first term:\n$$ \n\\frac{\\partial}{\\partial a_{00}} A = \\frac{\\partial}{\\partial a_{00}} \\left(\\frac{\\partial \\mathscr{L}} {\\partial s_{00}} \\frac{\\partial s_{00}}{\\partial a_{00}} \\right) = \\frac{\\partial}{\\partial a_{00}} \\left( \\frac{\\partial \\mathscr{L}} {\\partial s_{00}}\\right)\\frac{\\partial s_{00}}{\\partial a_{00}} + \\frac{\\partial \\mathscr{L}} {\\partial s_{00}}\\frac{\\partial}{\\partial a_{00}} \\left(\\frac{\\partial s_{00}}{\\partial a_{00}} \\right)\n$$\n\nwhich results in:\n$$\n\\frac{\\partial}{\\partial a_{00}} A = \\frac{\\partial s_{00}}{\\partial a_{00}} \\left(\\frac{\\partial s_{00}}{\\partial a_{00}} \\frac{\\partial^2 \\mathscr{L}}{\\partial s_{00}^2} + \n\\frac{\\partial s_{01}}{\\partial a_{00}} \\frac{\\partial^2 \\mathscr{L}}{\\partial s_{01} \\partial s_{00}} + \n\\frac{\\partial s_{02}}{\\partial a_{00}} \\frac{\\partial^2 \\mathscr{L}}{\\partial s_{02} \\partial s_{00}} +\n\\frac{\\partial s_{03}}{\\partial a_{00}} \\frac{\\partial^2 \\mathscr{L}}{\\partial s_{03} \\partial s_{00}} \\right) +\n\\frac{\\partial \\mathscr{L}}{\\partial s_{00}} \\frac{\\partial^2 s_{00}}{\\partial a_{00}^2}\n$$\n\nB, C, and D are similarly derived as well as each model output $a_{ij}$.\n\nFor the second derivative of the loss wrt the softmax, we have:\n\n$$\n\\frac{\\partial^2 \\mathscr{L}}{\\partial s_{bi} \\partial s_{bj}} = \\frac{\\partial \\mathscr{D}}{\\partial s_{bj}}\\frac{\\partial \\mathscr{D}}{\\partial s_{bi}}\\frac{1}{\\mathscr{D}^2}-\\frac{\\partial \\mathscr{N}}{\\partial s_{bj}}\\frac{\\partial \\mathscr{N}}{\\partial s_{bi}}\\frac{1}{\\mathscr{N}^2} \n$$\n\nThe hessian for the softmax function is mostly zeros since values outside of each batch are unrelated.  Within each batch, the second derivaties we want are:\n\n$$\n\\frac{\\partial^2 s_{bi}}{\\partial a_{bi}\\partial a_{bj}} = \\left\\{\n                \\begin{array}{ll}\n                  s_{bi}(1 - s_{bi})(1 - 2 s_{bi}) \\quad i = j\\\\\n                  s_{bi} s_{bj}(2s_{bi} - 1)  \\quad i \\ne j\n                \\end{array}\n              \\right.\n$$\n\nFor batch b and classes i and j."}}