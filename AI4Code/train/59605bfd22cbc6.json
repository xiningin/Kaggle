{"cell_type":{"0c824ab5":"code","4832d84c":"code","f607c460":"code","74285308":"code","75a55473":"code","f52af114":"code","9d65bff3":"code","51e708ec":"code","143d72fb":"code","34b5da22":"code","c767bc0d":"code","dc326123":"code","0cf2c23a":"code","389a3d67":"markdown","802f5701":"markdown","20120fc3":"markdown","308c354a":"markdown","350ac04a":"markdown","51f2ebec":"markdown"},"source":{"0c824ab5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nprint(tf.__version__)\n\nimport warnings\nwarnings.filterwarnings('ignore')","4832d84c":"# It is a simple function to plot time x axis values y axis\n\ndef plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)","f607c460":"# Inputs: data series, size of the window,The size of the batches to use when training,\n# the size of the shuffle buffer, which determines how the data will be shuffled.\n#\n# Expand the dimensions of the series to work with the LSTM's\n# Create dataset ds from the series\n# Slice the data up into the appropriate windows, shifted by one time set.\n# keep them all the same size by setting drop remainder to true.\n# flatten the data into chunks in the size of our window_size + 1.\n# Shuffle it with shuffle buffer that speeds things up with large datasets\n# Return the dataset that batched into the selected batch size \n\ndef windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)","74285308":"# We can use this function to make some prediction by using the trained model \n\ndef model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","75a55473":"df_sun = pd.read_csv('\/kaggle\/input\/sunspots\/Sunspots.csv')\nprint(df_sun.shape)\ndf_sun.head()\n","f52af114":"# Let's visualize the data\nseries = df_sun['Monthly Mean Total Sunspot Number']\ntime = df_sun['Unnamed: 0']\n\nplt.figure(figsize=(10, 6))\nplot_series(time, series)","9d65bff3":"# Let's take 80% of the data as train set\nsplit_time = 2500\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n","51e708ec":"# Let's define random_seeds and the variables\n# Clear keras session \ntf.keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nshuffle_buffer_size = 1000\nwindow_size = 64\nbatch_size = 128\n\n# Use windowed_dataset function to make dataset suitable\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\nprint(train_set)\nprint(x_train.shape)\n","143d72fb":"# We use \"lr_schedule\" to see which \"learning rate\" is optimum \n# Run the model with less epoch to visualize \"learning rate\" vs \"loss\"\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n                    lambda epoch: 1e-8 * 10**(epoch\/20))\n# Optimizer and loos parameters\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\nloss = tf.keras.losses.Huber()","34b5da22":"# Build and Fit the model\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=64, kernel_size=5,\n                          strides=1, padding='causal',\n                          activation='relu',\n                          input_shape=[None, 1]),\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    tf.keras.layers.LSTM(128, return_sequences=True),\n    tf.keras.layers.Dense(32, activation=\"relu\"),\n    tf.keras.layers.Dense(16, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n\nmodel.compile(loss=loss, optimizer=optimizer, metrics=['mae'])\n\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","c767bc0d":"# Let's visualize \"learning rate\" vs \"loss\"\nplt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-3, 0, 80])","dc326123":"# Let's rerun the model with the optimul learning rate\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nshuffle_buffer_size = 1000\nwindow_size = 64\nbatch_size = 64\n\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\noptimizer = tf.keras.optimizers.SGD(lr=7e-6, momentum=0.9)\nloss = tf.keras.losses.Huber()\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                          strides=1, padding='causal',\n                          activation='relu',\n                          input_shape=[None, 1]),\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    tf.keras.layers.Dense(32, activation=\"relu\"),\n    tf.keras.layers.Dense(16, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n\nmodel.compile(loss=loss, optimizer=optimizer, metrics=['mae'])\n\nhistory = model.fit(train_set, epochs=200)","0cf2c23a":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\nprint(\"First 10 Predictions :\",\"\\n\", rnn_forecast[:10])\nprint('')\nprint(\"mae : \", tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy())\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","389a3d67":"# Libraries","802f5701":"# Model ","20120fc3":"# The Functions\n\n- Let's built some useful function to use in the next steps ","308c354a":"# Split Train-Test Dataset ","350ac04a":"### This suggests the best learning rate for this network will be around \"7e-5\"","51f2ebec":"# Data Exploration and Preparation\n\n###  This data tracks sunspots on a monthly basis from 1749 until 2018. \n### Sunspots do have seasonal cycles approximately every 11 years."}}