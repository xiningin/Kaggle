{"cell_type":{"e7be14c1":"code","b94d5601":"code","ccedc88a":"code","55b513d9":"code","d2200e42":"code","c2c75e16":"code","84d896a8":"code","e6aa5017":"code","d882d817":"code","a1adf119":"code","b753b09f":"code","a75c2616":"code","850b8408":"code","aa41ddd7":"code","7b48defa":"code","f95f8e7e":"code","8841f67a":"code","0afa66c8":"code","08c4e3e7":"code","6e080cbd":"code","10afa018":"code","16b1b400":"code","7cec7397":"code","c045be58":"code","0f918511":"code","6e66b1ce":"code","db392dd0":"code","53833c94":"code","e7efad54":"code","eb0206cc":"code","09691a77":"code","7894d0ec":"code","60a35a23":"code","1c6d533c":"code","30004463":"code","13e03f94":"code","28fda33d":"code","b25456d0":"code","aacffcf4":"code","e0526a6c":"code","cb231a29":"code","760603b4":"code","02c8a842":"code","71ee9e46":"code","111b229a":"code","da5fdd93":"code","17de3d84":"code","de21e83f":"code","a4e03c89":"code","815e19e8":"code","89bf20a9":"code","8aa30efa":"code","e226eacf":"code","8bdea8ae":"code","77c9739e":"code","f6a7e142":"code","71590ef0":"code","84884630":"code","aeb25c73":"code","b77924c9":"code","a324a1cc":"code","c217e3e1":"code","fa1e9820":"code","835da4a3":"code","2f916939":"code","13fedf5c":"code","f2363827":"code","3b891157":"code","c09fff8a":"code","0bbbea22":"code","57b11d8b":"code","38351580":"code","57360e22":"code","ad8a0c41":"code","e581a881":"code","811445a3":"markdown","833b61fa":"markdown","205fe27e":"markdown","e6f505be":"markdown","7f6860df":"markdown"},"source":{"e7be14c1":"# Importing the libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn.linear_model as linear_model\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer, r2_score\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\n\n\n# Ignorar warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","b94d5601":"'''\nSteps:\n\n1. Collecting data\n2. Cleaning data\n3. Exploratory data analysis\n4. Model building\n\n'''","ccedc88a":"# loading the train and test data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","55b513d9":"# initilizing the target variable\nSalePrice = train['SalePrice']","d2200e42":"SalePrice.shape","c2c75e16":"## concatenating train and test\ndf = pd.concat((train, test))\nprint(\"Shape of df: \", df.shape)","84d896a8":"df.head(10)","e6aa5017":"df.shape","d882d817":"df.columns","a1adf119":"# Counting the Numeric columns\nnumericalFeatures = df.select_dtypes(include = [np.number])\nprint(\"The number of numerical features is: {}\".format(numericalFeatures.shape[1]))","b753b09f":"numericalFeatures.columns","a75c2616":"# Counting the Categorical columns\ncategoricalFeatures = df.select_dtypes(exclude = [np.number])\nprint(\"The number of categorical features is: {}\".format(categoricalFeatures.shape[1]))","850b8408":"categoricalFeatures.columns","aa41ddd7":"## Checking data distribution only in the training set\n\nplt.subplots(figsize=(12,9))\nsns.distplot(train['SalePrice'], fit=stats.norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = stats.norm.fit(train['SalePrice'])\n\n# Plot with the distribution\nplt.legend(['Normal dist. ($\/mu=$ {:.2f} and $\/sigma=$ {:.2f})'.format(mu, sigma)], loc='best')\n\n# Probability plot\nfig=plt.figure()\nstats.probplot(train['SalePrice'], plot=plt)\nplt.show()","7b48defa":"corr = numericalFeatures.corr()\n\nsns.set(style=\"white\")\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 10))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","f95f8e7e":"## Correlation greater than 0.5\n\ntop_feature = corr.index[abs(corr['SalePrice']>0.5)]\nplt.subplots(figsize=(12,8))\ntop_corr = df[top_feature].corr()\nsns.heatmap(top_corr, annot=True)\nplt.show()","8841f67a":"catFeatures = categoricalFeatures.columns\ntrain[catFeatures] = train[catFeatures].fillna('Missing')\n\n# Making the ANOVA\nanova = {'feature':[], 'f':[], 'p':[]}\nfor cat in catFeatures:\n  group_prices = []\n  for group in train[cat].unique():\n      group_prices.append(train[train[cat] == group]['SalePrice'].values)\n  f, p = stats.f_oneway(*group_prices)\n  anova['feature'].append(cat)\n  anova['f'].append(f)\n  anova['p'].append(p)\nanova = pd.DataFrame(anova)\nanova = anova[['feature','f','p']]\nanova.sort_values('p', inplace = True)","0afa66c8":"anova","08c4e3e7":"df = df.drop(['SalePrice', 'Id'], axis=1)","6e080cbd":"# Calculating the percentage of the missing values\nnullValues = (df.isnull().sum() \/ len(df)) * 100\nnullValues = round(nullValues.drop(nullValues[nullValues == 0].index).sort_values(ascending=False)[:30],2)\nmissingData = pd.DataFrame({'Percente of null values' :nullValues})\nmissingData.head(30)","10afa018":"df.shape","16b1b400":"### graph of Percentage of null values\n\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=nullValues.index, y=nullValues)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of null values', fontsize=15)\nplt.title('Percent null values by feature', fontsize=15)\nplt.show()","7cec7397":"# Changing the categorical features related to the quality of the house\n\nnumber = LabelEncoder()\ndf['Alley'] = number.fit_transform(df['Alley'].astype('str'))\ndf['LotShape'] = number.fit_transform(df['LotShape'].astype('str'))\ndf['LandContour'] = number.fit_transform(df['LandContour'].astype('str'))\ndf['Utilities'] = number.fit_transform(df['Utilities'].astype('str'))\ndf['LandSlope'] = number.fit_transform(df['LandSlope'].astype('str'))\ndf['ExterQual'] = number.fit_transform(df['ExterQual'].astype('str'))\ndf['BsmtQual'] = number.fit_transform(df['BsmtQual'].astype('str'))\ndf['BsmtCond'] = number.fit_transform(df['BsmtCond'].astype('str'))\ndf['BsmtExposure'] = number.fit_transform(df['BsmtExposure'].astype('str'))\ndf['BsmtFinType1'] = number.fit_transform(df['BsmtFinType1'].astype('str'))\ndf['BsmtFinType2'] = number.fit_transform(df['BsmtFinType2'].astype('str'))\ndf['HeatingQC'] = number.fit_transform(df['HeatingQC'].astype('str'))\ndf['KitchenQual'] = number.fit_transform(df['KitchenQual'].astype('str'))\ndf['Functional'] = number.fit_transform(df['Functional'].astype('str'))\ndf['FireplaceQu'] = number.fit_transform(df['FireplaceQu'].astype('str'))\ndf['GarageFinish'] = number.fit_transform(df['GarageFinish'].astype('str'))\ndf['GarageQual'] = number.fit_transform(df['GarageQual'].astype('str'))\ndf['GarageFinish'] = number.fit_transform(df['GarageFinish'].astype('str'))\ndf['GarageCond'] = number.fit_transform(df['GarageCond'].astype('str'))\ndf['PavedDrive'] = number.fit_transform(df['PavedDrive'].astype('str'))\ndf['PoolQC'] = number.fit_transform(df['PoolQC'].astype('str'))\n","c045be58":"train.groupby(['YrSold', 'MoSold']).Id.count().plot(kind='bar', figsize=(14,4))\nplt.title(\"Sale date\")\nplt.show()","0f918511":"# Conversion from numeric feature to Category features\n\ndf['MSSubClass'] = df.MSSubClass.apply(lambda x: str(x))\ndf['MoSold'] = df.MoSold.apply(lambda x: str(x))\ndf['YrSold'] = df.YrSold.apply(lambda x: str(x))","6e66b1ce":"df['MSSubClass'] = number.fit_transform(df['MSSubClass'].astype('str'))\ndf['MoSold'] = number.fit_transform(df['MoSold'].astype('str'))\ndf['YrSold'] = number.fit_transform(df['YrSold'].astype('str'))","db392dd0":"df.columns[df.isnull().any()]","53833c94":"# Lot Frontage-  we replace it with the median value\ndf.LotFrontage = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# Garage Year Built, if missing we fill it to zero\ndf.GarageYrBlt.fillna(0, inplace=True)\n\n# Masonary Veneer Area here most values are zero\ndf.MasVnrArea.fillna(0, inplace=True)","e7efad54":"df.columns[df.isnull().any()]","eb0206cc":"df.Electrical.value_counts()","09691a77":"df.Electrical.fillna(df.Electrical.mode()[0], inplace=True)","7894d0ec":"df.MasVnrType.value_counts()","60a35a23":"# correcting the assignment\ndf.MasVnrType.replace({'Missing':'None'}, inplace=True)\n\n# we are going to replace them with the mean value\ndf.loc[(df.MasVnrType == 'None') & (df.MasVnrArea > 1), 'MasVnrType'] = 'BrkFace' \ndf.loc[(df.MasVnrType == 'None') & (df.MasVnrArea == 1), 'MasVnrType'] = 0 \n\nfor vnr_type in df.MasVnrType.unique():\n    # so here we set area equal to the mean of the given veneer type\n    df.loc[(df.MasVnrType == vnr_type) & (df.MasVnrArea == 0), 'MasVnrArea'] = df[df.MasVnrType == vnr_type].MasVnrArea.mean()\n    \ndf.MasVnrType.fillna(df.MasVnrType.mode()[0], inplace=True)    ","1c6d533c":"df.GarageType.value_counts()","30004463":"df.GarageType.fillna(df.GarageType.mode()[0], inplace=True)","13e03f94":"df.Fence.value_counts()","28fda33d":"df.Fence.fillna(0, inplace=True)","b25456d0":"df.MiscFeature.value_counts()","aacffcf4":"df.MiscFeature.fillna(0, inplace=True)","e0526a6c":"df.GarageArea.fillna(df.GarageArea.mean(), inplace=True)","cb231a29":"df.SaleType.value_counts()","760603b4":"df.SaleType.fillna(df.SaleType.mode()[0], inplace=True)","02c8a842":"df.GarageCars.value_counts()","71ee9e46":"df.GarageCars.fillna(df.GarageCars.mode()[0], inplace=True)","111b229a":"df.BsmtFinSF1.value_counts()","da5fdd93":"df.BsmtFinSF1.fillna(df.BsmtFinSF1.mean(), inplace=True)\ndf.BsmtFinSF2.fillna(df.BsmtFinSF2.mean(), inplace=True)","17de3d84":"df.BsmtFullBath.value_counts()","de21e83f":"df.BsmtFullBath.fillna(df.BsmtFullBath.mode()[0], inplace=True)\ndf.BsmtHalfBath.fillna(df.BsmtHalfBath.mode()[0], inplace=True)\ndf.Exterior1st.fillna(df.Exterior1st.mode()[0], inplace=True)\ndf.Exterior2nd.fillna(df.Exterior2nd.mode()[0], inplace=True)\ndf.BsmtUnfSF.fillna(df.BsmtUnfSF.mode()[0], inplace=True)\ndf.MSZoning.fillna(df.MSZoning.mode()[0], inplace=True)\ndf.TotalBsmtSF.fillna(df.TotalBsmtSF.mean(), inplace=True)","a4e03c89":"df.columns[df.isnull().any()]","815e19e8":"# Calculating total square feet (area)\n\ndf['Total_SF'] = df.TotalBsmtSF + df.GrLivArea\ndf['TotalFloorSF'] = df['1stFlrSF'] + df['2ndFlrSF']\ndf['TotalPorchSF'] = df.OpenPorchSF + df.EnclosedPorch + df['3SsnPorch'] + df['ScreenPorch']","89bf20a9":"# Now let's create some boolean features (Yes-No type)\n\ndf['HasBasement'] = df.TotalBsmtSF.apply(lambda x: 1 if x>0 else 0)\ndf['HasGarage'] = df.GarageArea.apply(lambda x: 1 if x>0 else 0)\ndf['HasPorch'] = df.TotalPorchSF.apply(lambda x: 1 if x>0 else 0)\ndf['HasPool'] = df.PoolArea.apply(lambda x: 1 if x>0 else 0)\ndf['WasRemodeled'] = (df.YearRemodAdd != df.YearBuilt).astype(np.int64)\ndf['IsNew'] = (df.YearBuilt > 2000).astype(np.int64)\ndf['WasCompleted'] = (df.SaleCondition != 'Partial').astype(np.int64)","8aa30efa":"booleanFeatures = ['HasBasement','HasGarage','HasPorch','HasPool','WasRemodeled','IsNew','WasCompleted']","e226eacf":"numericalFeatures = numericalFeatures.drop(['Id','SalePrice'], axis=1)\nnumFeatures = numericalFeatures.columns\ncatFeatures = categoricalFeatures.columns","8bdea8ae":"numFeatures = [f for f in numFeatures if f not in booleanFeatures]","77c9739e":"# Total Bathrooms\n\ndf['TotalBathrooms'] = df.FullBath + 0.5*df.HalfBath + df.BsmtFullBath + 0.5*df.BsmtHalfBath","f6a7e142":"for f in numFeatures:\n  df.loc[:,f] = np.log1p(df[f])","71590ef0":"SalePrice = np.log1p(SalePrice)","84884630":"df = pd.get_dummies(df).copy()","aeb25c73":"dfColumns = df.columns","b77924c9":"df.head()","a324a1cc":"plt.subplots(figsize=(12,9))\nsns.distplot(SalePrice, fit=stats.norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = stats.norm.fit(SalePrice)\n\n# Plot with the distribution\nplt.legend(['Normal dist. ($\/mu=$ {:.2f} and $\/sigma=$ {:.2f})'.format(mu, sigma)], loc='best')\n\n# Probability plot\nfig=plt.figure()\nstats.probplot(SalePrice, plot=plt)\nplt.show()","c217e3e1":"# scaling dataset with robust scaler\n\nscaler = StandardScaler()\n\ndf.loc[:, numFeatures] = scaler.fit_transform(df[numFeatures])","fa1e9820":"trainLen = len(train)\ny_train = SalePrice\nx_train = df[:trainLen]\nx_test = df[trainLen:]\n\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(len(y_train))","835da4a3":"def test_model(model, x_train, y_train):\n    cv = KFold(n_splits = 3, shuffle=True, random_state = 45)\n    r2 = make_scorer(r2_score)\n    r2_val_score = cross_val_score(model, x_train, y_train, cv=cv, scoring = r2)\n    score = [r2_val_score.mean()]\n    return score","2f916939":"def rsme(model, x, y):\n  cv_scores = -cross_val_score(model, x, y, scoring='neg_mean_squared_error', cv=10)\n  return np.sqrt(cv_scores)","13fedf5c":"## Tuning parameters\n\nparam_grid = {'alpha':[0.0001,0.001,0.01,1.,5.,10.,25.],'max_iter':[50000]}\nlasso = GridSearchCV(Lasso(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error')\nlasso.fit(x_train, y_train)\nalpha = lasso.best_params_['alpha']\n\n# Home in\nparam_grid = {'alpha':[x\/100. * alpha for x in range(50,150,5)],'max_iter':[50000]}\nlasso = GridSearchCV(Lasso(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error')\nlasso.fit(x_train, y_train)\nalpha = lasso.best_params_['alpha']\nlasso = lasso.best_estimator_\n\nprint('Lasso -> Train RSME: {:,.5f}| alpha {:,.5f}'.format(rsme(lasso,x_train,y_train).mean(),alpha))","f2363827":"coefs = pd.DataFrame({'coefs':lasso.coef_,'Positive':lasso.coef_>0}, index=dfColumns)\ncoefs['coefs_abs'] = np.abs(coefs.coefs)\nprint('Lasso dropped {} of {} features.'.format(sum(coefs.coefs==0), coefs.shape[0]))\n\ntop_coefs = coefs.sort_values('coefs_abs', ascending=False).head(30)\nplt.figure(figsize=(8,10))\nsns.barplot(top_coefs.coefs_abs, top_coefs.index, orient='h', hue=top_coefs.Positive)\nplt.title=('Lasso Regression: Top Features')\nplt.xlabel('Absolute Coeficient')\nplt.show()","3b891157":"# Linear Regression\n\nLR = linear_model.LinearRegression()\nacc_LR = test_model(LR, x_train, y_train)\n\nLR_rsme = rsme(LR, x_train, y_train)\n\nprint('Score: {:.5f}'.format((acc_LR[0])))\nprint('RSME: {:.5f}'.format(LR_rsme.mean()))","c09fff8a":"# Support Vector Regressor\n\nsvr_reg = SVR(kernel='rbf')\nacc_SVR = test_model(svr_reg, x_train, y_train)\n\nsvr_rsme = rsme(svr_reg, x_train, y_train)\nprint('Score: {:.5f}'.format((acc_SVR[0])))\nprint('RSME: {:.5f}'.format(svr_rsme.mean()))","0bbbea22":"#Decision Tree\ndt_reg = DecisionTreeRegressor(random_state=21)\nacc_tree = test_model(dt_reg, x_train, y_train)\n\ndt_rsme = rsme(dt_reg, x_train, y_train)\nprint('Score: {:.5f}'.format((acc_tree[0])))\nprint('RSME: {:.5f}'.format(dt_rsme.mean()))","57b11d8b":"# Random Forest\nrf_reg = RandomForestRegressor(n_estimators = 1000, random_state=51)\nacc_rf = test_model(rf_reg, x_train, y_train)\n\nrf_rsme = rsme(rf_reg, x_train, y_train)\nprint('Score: {:.5f}'.format((acc_rf[0])))\nprint('RSME: {:.5f}'.format(rf_rsme.mean()))","38351580":"# Bagging Regressor\nbr_reg = BaggingRegressor(n_estimators=1000, random_state=51)\nacc_br = test_model(br_reg, x_train, y_train)\n\nbr_rsme = rsme(br_reg, x_train, y_train)\nprint('Score: {:.5f}'.format((acc_br[0])))\nprint('RSME: {:.5f}'.format(br_rsme.mean()))","57360e22":"# Gradient Boosting Regressor\ngbr_reg = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, loss='ls', random_state=51)\nacc_gbr = test_model(gbr_reg, x_train, y_train)\n\ngbr_rsme = rsme(gbr_reg, x_train, y_train)\nprint('Score: {:.5f}'.format((acc_gbr[0])))\nprint('RSME: {:.5f}'.format(gbr_rsme.mean()))","ad8a0c41":"# XGBoost\n\nxgb_reg = xgb.XGBRegressor(colsample_bytree=0.2, \n                        gamma=0.0,\n                        learning_rate=0.05,\n                        max_depth=6,\n                        min_child_weight=1.5,\n                        n_estimators=7200,\n                        reg_alpha=0.9,\n                        reg_lambda=0.6,\n                        subsample=0.2,\n                        seed=42,\n                        silent=1)\n\nacc_xgb = test_model(xgb_reg,x_train[top_coefs.index], y_train)\nxgb_rsme = rsme(xgb_reg, x_train[top_coefs.index], y_train)\n\nprint('Score: {:.5f}'.format((acc_xgb[0])))\nprint('RSME: {:.5f}'.format(xgb_rsme.mean()))","e581a881":"results = pd.DataFrame({\n    'Model': ['Linear Regression', 'Support Vector Regressor', \n              'Decision Tree', 'Random Forest', 'Bagging Regressor', 'Gradient Boosting Regressor ','XGBoost'],\n    'Score': [acc_LR[0], acc_SVR[0], acc_tree[0], acc_rf[0], acc_br[0], acc_gbr[0], acc_xgb[0]],\n    'RSME': [LR_rsme[0], svr_rsme[0], dt_rsme[0], rf_rsme[0], br_rsme[0], gbr_rsme[0], xgb_rsme[0]]\n})\n\nresult = results.sort_values(by='RSME', ascending=True)\nresult = result.set_index('Model')\ndisplay(result.head(8))","811445a3":"# Building models","833b61fa":"# Loading data","205fe27e":"# Data distribution","e6f505be":"Null hypothesis (H0): There is no difference\n\nIf P<0.05 we can reject H0\n\nThe features Street, LandSlope and Utilities have P>0.05, which means, they make difference in sales price.","7f6860df":"# Correlation\n"}}