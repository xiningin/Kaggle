{"cell_type":{"f0eefabf":"code","4506e61c":"code","234acee6":"code","4eb18540":"code","6cd3a1e5":"code","06d67e56":"code","1d024384":"code","971e1929":"code","3e7867b0":"code","620e4425":"code","dc027bc7":"code","1b7089e5":"code","ffffce02":"code","e471c997":"code","e75aecdb":"code","b47f91fc":"code","0719bc2f":"code","657240c5":"code","28c084f3":"code","d301910a":"code","0bc5eb95":"code","66a39de4":"code","bc1a2358":"code","1e7720be":"code","d4e9aa45":"code","24b5dfa6":"markdown","cc9665da":"markdown","6bac651c":"markdown","52a2b4c0":"markdown","af8efb5e":"markdown","196074f3":"markdown","626ddf64":"markdown","f564c6f8":"markdown","1a0cf83c":"markdown","e39815d0":"markdown","8e724161":"markdown","0a426c56":"markdown"},"source":{"f0eefabf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.display import Image\nimport os\n!ls ..\/input\/","4506e61c":"df = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","234acee6":"df.info()","4eb18540":"df.shape","6cd3a1e5":"df.describe()","06d67e56":"df.isna().sum()","1d024384":"df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","971e1929":"df.isna().sum()","3e7867b0":"df['Glucose'].fillna(df['Glucose'].median(), inplace = True)\ndf['BloodPressure'].fillna(df['BloodPressure'].median(), inplace = True)\ndf['SkinThickness'].fillna(df['SkinThickness'].median(), inplace = True)\ndf['Insulin'].fillna(df['Insulin'].median(), inplace = True)\ndf['BMI'].fillna(df['BMI'].median(), inplace = True)","620e4425":"ax = sns.catplot('Outcome', data=df, kind='count')\nax.set(xticklabels = [\"No\", \"Yes\"])\n\nplt.ylabel('Number of Case')\nplt.title('Diabete Distribution')\n\n#the data is good? should we get an equal distribution of the sick people","dc027bc7":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndf.hist(figsize=(10,20))","1b7089e5":"corr=df.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nsns.set(font_scale=1.15)\nplt.figure(figsize=(16, 9))\n\nsns.heatmap(corr, mask=mask,square=True,annot=True,cmap='YlGnBu',)\n\nplt.title('Correlation between features')","ffffce02":"#### add some graph that compare the outcome to the most important feature\n#pregnancy, glucose, BMI, age","e471c997":"X = df.drop(['Outcome'], axis = 1)\ny = df['Outcome']","e75aecdb":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","b47f91fc":"column = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']\n\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler(with_mean=True, with_std=True)\ndfnumss = pd.DataFrame(ss.fit_transform(X_train[column]), columns=['ss_'+x for x in column], index = X_train.index)\nX_train = pd.concat([X_train, dfnumss], axis=1)\nX_train = X_train.drop(column, axis=1)","0719bc2f":"dfnumss = pd.DataFrame(ss.transform(X_test[column]), columns=['ss_'+x for x in column], index = X_test.index)\nX_test = pd.concat([X_test, dfnumss], axis=1)\nX_test = X_test.drop(column, axis=1)","657240c5":"from sklearn.svm import SVC\n\nsvc = SVC()     \nsvc.fit(X_train, y_train)                                                        \ny_pred = svc.predict(X_test)    \n","28c084f3":"from sklearn import metrics\n\nprint (metrics.accuracy_score(y_test, y_pred))\nprint (metrics.confusion_matrix(y_test, y_pred))\nprint (metrics.classification_report(y_test, y_pred))","d301910a":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test) \n","0bc5eb95":"print (metrics.accuracy_score(y_test, y_pred))\nprint (metrics.confusion_matrix(y_test, y_pred))\nprint (metrics.classification_report(y_test, y_pred))","66a39de4":"from sklearn.neighbors import KNeighborsClassifier                                                            \n\nknn = KNeighborsClassifier() \nknn.fit(X_train, y_train)                                                                                                            \ny_pred = knn.predict(X_test) ","bc1a2358":"print (metrics.accuracy_score(y_test, y_pred))\nprint (metrics.confusion_matrix(y_test, y_pred))\nprint (metrics.classification_report(y_test, y_pred))\n","1e7720be":"from sklearn.feature_selection import SelectFromModel","d4e9aa45":"\nfrom sklearn.inspection import permutation_importance\nfrom matplotlib import pyplot\n\n\nknn.fit(X_train, y_train)\nresults = permutation_importance(knn, X_train, y_train, scoring='accuracy')\nimportance = results.importances_mean\n\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n\npyplot.barh([x for x in range(len(importance))],importance, color = 'green', align='center')\nplt.ylabel(\"The Variables\")\nplt.xlabel(\"Score\")\nplt.title(\"Feature Importance from KNN\")\npyplot.show()\n\n","24b5dfa6":"In case of outlier, we decide to replace the missing value with the median","cc9665da":"**What is K-nearest neighbors(KNN)?**\n\nKNN is a model that classifies data points based on the points that are most similar to it, the neighbor more similar to you. It uses test data to make an \"educated guess\" on what an unclassified point should be classified as.\n\n![KNN.png](attachment:KNN.png)\n\n\n**Analogy**\n\nFor instance, a book store will recommend you a book by associating your taste with similar customers.  You share similar tastes with Thomas, another customer. Thomas just bought and loved his new book. Next time you go to the shop, the bookseller will suggest the same book as Thomas. KNN is frequently used as a recommendation system. In our example, the algorithms will look at the profile of those with diabetes and test if one person has a similar profile to those with diabetes. \n\n**Pros & Cons**\n\nThis algorithm's main advantages are that it does not need assumption to process, and with its memory-based approach, it always evolves with a new dataset. However, as the variables grow, the model will be slower to process and less accurate. Also, the data need to be homogenous, which means a feature from another needs the same scale.\n","6bac651c":"**What is Random Forest?**\n\nThe random forest consists of a large number of individual decision trees that operate as an ensemble. Each tree in the random forest spits out a class prediction, and the class with the most votes becomes our model's prediction.\n\n![Random%20Forest.png](attachment:Random%20Forest.png)\n\n**Analogy**\n\nYou are looking for a new book based on your friend's recommendation. The first time, your relative will ask multiple questions about your taste until he advised you a book. Then, you repeat this process multiples times with similar and different questions through other friends. In the end, you decide your book according has been recommended most of the time.\n\n**Pros & Cons**\n\nImplementing an RFC is its ability to reduce errors, and it has a good performance with imbalanced data. However, it tends to overfit when it dealings with irrelevant data, and its results may be biased when you have categorical data.\n","52a2b4c0":"There are not as many diabetes patients as non-diabetes patients. We called it **Imbalanced Data**. It is common in medical diagnosis and fraud due to a small number of cases.","af8efb5e":"In 2014, the World Health Organization (WHO) estimated 422 million people are suffering from diabetes. Diabetes is an illness that occurs when your blood glucose is too high. Its effects can damage your body\u2019s organs. It is caused by either an issue with your immune system or a bad lifestyle, such as overweight or physical inactivity.\n\n\nThis **dataset** is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n\nThe dataset is composed of:\n\n\n* Pregnancies: Number of time pregnant\n* Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* Blood Pressure: Diastolic blood pressure (mm Hg)\n* SkinThickness: Triceps skinfold thickness (mm)\n* Insulin: 2-Hour serum insulin (mu U\/ml)\n* BMI: Body mass index (weight in kg\/(height in m)^2)\n* Diabetes pedigree function: Diabetes pedigree function\n* Age: Age(years)\n* Outcome: Class variable (0 or 1) 268 of 768 are 1, the others are 0","196074f3":"# # Exploratory Data Analysis","626ddf64":"\n**What is  Support Vector Machine(SVM)?**\n\nSVM is a supervised machine for classification and regression models. SVMs are based on the idea of finding a line or called hyperplane when you have more than two variables that best divide a dataset into X classes, where X is the number of Target. (i.e. to have or not diabetes).\n\n![SVM.png](attachment:SVM.png)\n\n\n**Analogy**\n\nFor instance, you are at the zoo and want to see a crocodile. However, you are confused with two similar animals, crocodile and alligator. After looking at the patterns, you will notice which one is the crocodile and start looking at it. Same as you, support vector machines work. It looks at data and sorts it into one of the two categories. In our case, the algorithm will try to find which characteristics make you more likely to have or not diabetes. Then the hyperplane is like a cut-off that places you in a side or another.\n\n**Pros & Cons**\n\nThis algorithm's advantage is that it performs better in a higher dimension and barely impacts outliers. However, it takes time to process with a large dataset, and difficult to target appropriate the hyperparameters.","f564c6f8":"**Supervised Machine Learning**, is where you have a target variable(output) and predictor variable(input). You use an algorithm to obtain the map function that lets you know which variables and in which ranges have located the probability of getting diabetes. \n\nIn our case, we use **eager learning** algorithm with a support vector machine and random forest tree, and lazy learning algorithm, with K-nearest neighbors. What is all of this? The first one attempts to create an explicit, concise, and generalized description of a phenome from the training set. Whereas, **lazy learning**, also called instance-based learning, compared instance to instance, which has been stored and memorized train data. It infers hypotheses directly from the training instances themselves. Let\u2019s see their philosophy with some graphs!","1a0cf83c":"![](http:\/\/)","e39815d0":"**Feature Importance**\n\nFeature importance refers to a method that evaluates the impact of your variable in your prediction model. It can be either used for the numerical or categorical value. Its goal is to understand better the model and data, which are the most relevant and in the opposite way, which are not contributed. As a next step, you may gather more or different data.","8e724161":"Glucose is the variable that explains the most of our model.","0a426c56":"**Dataframe.desccribe()** is used to get a data frame summary. It only counts numerical values, excluding categorical and NaN values. From this approach, we have a high-level summary of the dataset and understand each column's distribution.\nMoreover, with a deeper look at the tale, we noticed some columns with a minimum value of 0, such as glucose blood pressure, skin, insulin, and BMI. These values are not realistic. To make it more logical, we will replace them with Nan value."}}