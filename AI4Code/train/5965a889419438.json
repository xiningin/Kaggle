{"cell_type":{"bc09ac72":"code","03ef75cb":"code","91bb610c":"code","57eebd86":"code","6f0a9e4a":"code","b5883cdf":"code","ad63146f":"code","2f779402":"code","58315c71":"code","593a04c7":"code","cb8dcb13":"markdown","f43b7c77":"markdown","bff7b8fc":"markdown","dbd4e9ce":"markdown","e61aee64":"markdown","a5976dbc":"markdown","dc949fce":"markdown","9acf22d0":"markdown","38cf32da":"markdown","e193e44c":"markdown"},"source":{"bc09ac72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03ef75cb":"import pandas as pd\ndf = pd.read_csv(\"\/kaggle\/input\/drug-classification\/drug200.csv\")","91bb610c":"df","57eebd86":"df[\"Sex\"] = pd.factorize(df['Sex'])[0].astype(np.uint8)\ndf[\"BP\"] = pd.factorize(df['BP'])[0].astype(np.uint8)\ndf[\"Cholesterol\"] = pd.factorize(df['Cholesterol'])[0].astype(np.uint8)\ndf[\"Drug\"] = pd.factorize(df['Drug'])[0].astype(np.uint8)\ndf[\"Na_to_K\"] = (df[\"Na_to_K\"] - df[\"Na_to_K\"].min()) \/ (df[\"Na_to_K\"].max() - df[\"Na_to_K\"].min())\ndf","6f0a9e4a":"X = df.drop(columns = ['Drug'])\ny = df[[\"Drug\"]]\nX[0:5], y[0:5]","b5883cdf":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","ad63146f":"import tensorflow as tf","2f779402":"tf.random.set_seed(42)\n\n# Create the model\nmodel = tf.keras.Sequential([\n  #tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)\n  #tf.keras.layers.Dense(100, activation=\"relu\"),\n  tf.keras.layers.Dense(50, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(5, activation=\"softmax\") # output shape is 10, activation is softmax\n])\n\n# Compile the model\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                 optimizer=tf.keras.optimizers.Adam(lr=0.01), # ideal learning rate (same as default)\n                 metrics=[\"accuracy\"])\n\n# Fit the model\nhistory = model.fit(X_train,\n                       y_train, epochs=432, validation_data=(X_test, y_test))","58315c71":"pd.DataFrame(history.history).plot()","593a04c7":"model.evaluate(X_test, y_test)","cb8dcb13":"# BOOM!! Acheived 100% Accuracy.","f43b7c77":"# Create the Model for Training ","bff7b8fc":"# It looks very good. Lets evaluate the model","dbd4e9ce":"# Import TensorFlow","e61aee64":"# Create the Data and Labels","a5976dbc":"# Lets plot the model hisory, like loss values and accuracies","dc949fce":"# Split the Datasets into Train and Test Dataset (80%-20%)","9acf22d0":"# We preprocess the columns for Better Accuracy ","38cf32da":"# Lets check the Data","e193e44c":"# Load the CSV File"}}