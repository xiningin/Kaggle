{"cell_type":{"2e2e8aaa":"code","9bf58dcb":"code","5f2cc8b2":"code","15320e20":"code","c17bb894":"code","51cd78e5":"code","bf7c027b":"code","50317c52":"code","d1975f94":"code","c626a8f8":"code","53529f36":"code","f9bc331c":"code","96ee8286":"code","ba383195":"code","41ebe4dd":"code","01758ba9":"code","80d10201":"code","971eabe5":"code","f0241d5a":"code","e44d7fec":"code","300495ca":"code","a4f36095":"code","d55b5c51":"code","5abc904c":"code","995465ae":"code","bd8a925c":"code","71a2f1bc":"code","d463e6c5":"code","fbd7879e":"code","d430d42c":"code","f5774b16":"code","b4c958b7":"code","5742dc26":"code","0ee22b54":"code","185b5af6":"code","43a372dd":"code","f5c6db01":"code","0f5f170c":"code","ad25dc4b":"code","d6e844fd":"code","00a8542a":"code","f875ec82":"markdown","3f3798fa":"markdown","d0dc33dc":"markdown","b6a4cb53":"markdown","936e74ae":"markdown","7e46ce67":"markdown","883478e2":"markdown","00155a9d":"markdown","4190df45":"markdown","3d16e3ea":"markdown","7a092619":"markdown","17bd4d9d":"markdown","cc06218e":"markdown","30dc39c0":"markdown","55ff55cc":"markdown","58e82d3e":"markdown","84314931":"markdown","b032390d":"markdown","840675ef":"markdown","c9493592":"markdown","19473f3f":"markdown","601095a7":"markdown","21ec0577":"markdown"},"source":{"2e2e8aaa":"# My default libs\nimport numpy as np\nimport pandas as pd\n\n# Data viz libs\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\nimport plotly.graph_objs as go\n\n# Sklearn libs\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Models Libs\nfrom xgboost import XGBRegressor\n\n# Disable warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","9bf58dcb":"# Files path\ntrain_data_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\ntest_data_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'","5f2cc8b2":"# Getting the dataset's\ntrain_df = pd.read_csv(train_data_path)\ntest_df = pd.read_csv(test_data_path)","15320e20":"# Look to shape's\nprint('Train dataset have ', train_df.shape[0], ' lines and ', train_df.shape[1], ' columns')\nprint('Test dataset have ', test_df.shape[0], ' lines and ', test_df.shape[1], ' columns')","c17bb894":"# See the raw data, first 10 lines\ntrain_df.head()","51cd78e5":"year_seasons_df = train_df[['SalePrice','MoSold']].copy()\n\ndef setSeason(month):\n    if month in (6,7,8):\n        return \"Summer\"\n    if month in (11,10,9):\n        return \"Autumn\"\n    if month in (12,1,2):\n        return \"Winter\"\n    return \"Spring\"\n    \n\nyear_seasons_df['yearSeason'] = year_seasons_df.MoSold.apply(lambda x: setSeason(x));\n\nyear_seasons_df.sort_values(by='SalePrice', inplace=True)\n\ntrace = go.Box(\n    x = year_seasons_df.yearSeason,\n    y = year_seasons_df.SalePrice\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Prices x Year Season\",\n                  yaxis={'title':'Sale Price'},\n                  xaxis={'title':'Year Season'})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","bf7c027b":"year_seasons_gp_df = year_seasons_df.groupby('yearSeason')['SalePrice'].count().reset_index()\n\nyear_seasons_gp_df = pd.DataFrame({'yearSeason': year_seasons_gp_df.yearSeason,\n                                   'CountHouse': year_seasons_gp_df.SalePrice})\n\nyear_seasons_gp_df.sort_values(by='CountHouse', inplace=True)","50317c52":"trace = go.Bar(\n    x = year_seasons_gp_df.yearSeason,\n    y = year_seasons_gp_df.CountHouse\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Count House x Year Station\",\n                  yaxis={'title':'Count House'},\n                  xaxis={'title':'Year Station'})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","d1975f94":"def labelSeason(x):\n    if x == \"Summer\":\n        return 1\n    if x == \"Autumn\":\n        return 2\n    if x == \"Winter\":\n        return 3\n    return 4\n\n\nyear_seasons_df['labelSeason'] = year_seasons_df.yearSeason.apply(lambda x: labelSeason(x))\n\ndf_corr_year_seasons = year_seasons_df.corr()\n\ndf_corr_year_seasons","c626a8f8":"year_seasons_sorted_df = year_seasons_df.sort_values(by='MoSold')\n\nyear_seasons_sorted_gp_df = year_seasons_df.groupby('MoSold')['SalePrice'].count().reset_index();","53529f36":"df = year_seasons_sorted_gp_df\n\ntrace = go.Scatter(\n    x = df.MoSold,\n    y = df.SalePrice,\n    mode = 'markers+lines',\n    line_shape='spline'\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Sales by month's\",\n                  yaxis={'title':'Count House'},\n                  xaxis={'title':'Month sold', 'zeroline':False})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","f9bc331c":"trace = go.Scatter(\n    x = train_df.LotArea,\n    y = train_df.SalePrice,\n    mode = 'markers'\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Lot Area x Sale Price\",\n                  yaxis={'title':'Sale Price'},\n                  xaxis={'title':'Lot Area'})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","96ee8286":"trace = go.Box(\n    y = train_df.SalePrice,\n    name = 'Sale Price'\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Distribuiton Sale Price\",\n                  yaxis={'title':'Sale Price'})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","ba383195":"trace = go.Box(\n    y = train_df.LotArea,\n    name = 'Lot Area'\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Distribuiton Lot Area\",\n                  yaxis={'title':'Lot Area'})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","41ebe4dd":"lotarea_saleprice_df = train_df[['SalePrice', 'LotArea']]\n\nlotarea_saleprice_df.corr()","01758ba9":"train_df = train_df.drop(train_df.loc[(train_df['LotArea'] > 70000)].index)\ntrain_df = train_df.drop(train_df.loc[(train_df['SalePrice'] > 500000)].index)","80d10201":"trace = go.Scatter(\n    x = train_df.LotArea,\n    y = train_df.SalePrice,\n    mode = 'markers'\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Lot Area x Sale Price\",\n                  yaxis={'title':'Sale Price'},\n                  xaxis={'title':'Lot Area'})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","971eabe5":"# Set y (Target)\ny = np.log(train_df.SalePrice)\n\nX = train_df.copy()\n\nX_test = test_df.copy();","f0241d5a":"X['AreaUtil'] = X['LotArea'] - (X['MasVnrArea'] + X['GarageArea'] + X['PoolArea'])\nX_test['AreaUtil'] = X_test['LotArea'] - (X_test['MasVnrArea'] + X_test['GarageArea'] + X_test['PoolArea'])","e44d7fec":"X['HavePool'] = X['PoolArea'] > 0\nX_test['HavePool'] = X_test['PoolArea'] > 0","300495ca":"X['GarageCars2'] = X['GarageCars']**2\nX['GarageCarsSQRT'] = np.sqrt(X['GarageCars'])\nX['GarageArea'] = X['GarageArea']**2\nX['GarageAreaSQRT'] = np.sqrt(X['GarageArea'])\nX['LotArea2'] = X['LotArea']**2\nX['LotAreaSQRT'] = np.sqrt(X['LotArea'])\nX['AreaUtil2'] = X['AreaUtil']**2\nX['AreaUtilSQRT'] = np.sqrt(X['AreaUtil'])\nX['GrLivArea2'] = X['GrLivArea']**2\nX['GrLivAreaSQRT'] = np.sqrt(X['GrLivArea'])\n\nX_test['GarageCars2'] = X_test['GarageCars']**2\nX_test['GarageCarsSQRT'] = np.sqrt(X_test['GarageCars'])\nX_test['GarageArea'] = X_test['GarageArea']**2\nX_test['GarageAreaSQRT'] = np.sqrt(X_test['GarageArea'])\nX_test['LotArea2'] = X_test['LotArea']**2\nX_test['LotAreaSQRT'] = np.sqrt(X_test['LotArea'])\nX_test['AreaUtil2'] = X_test['AreaUtil']**2\nX_test['AreaUtilSQRT'] = np.sqrt(X_test['AreaUtil'])\nX_test['GrLivArea2'] = X_test['GrLivArea']**2\nX_test['GrLivAreaSQRT'] = np.sqrt(X_test['GrLivArea'])","a4f36095":"corrmat = X.corr()\n\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0]\n\n# most correlated features\nif 1 == 1:\n    plt.figure(figsize=(30,15))\n    g = sns.heatmap(X[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","d55b5c51":"# Remove row with missing target\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\n# Drop target.\nX.drop(['SalePrice'], axis=1, inplace=True)\n\nX.drop(['OverallQual'], axis=1, inplace=True)\n","5abc904c":"X.head()","995465ae":"#cols_sem_muitos_dados = [col for col in X.columns if X[col].isnull().sum() * 100 \/ len(X) > 50.00]\n\ncols_sem_muitos_dados = [col for col in X.columns if X[col].isnull().any()]\n\nfor col in cols_sem_muitos_dados:\n    X[col].fillna(\"None\")\n    X_test[col].fillna(\"None\")\n\n#X.drop(cols_sem_muitos_dados, axis=1, inplace=True)\n#X_test.drop(cols_sem_muitos_dados, axis=1, inplace=True)","bd8a925c":"X.head()","71a2f1bc":"print('Train Shape:', X.shape)\nprint('Test Shape:', X_test.shape)","d463e6c5":"# Get categorical cols WITH type OBJECT (like string)\ncategorical_cols = [cname for cname in X.columns\nif X[cname].dtype == \"object\"            \n]\n\n# Get numerical cols\nnumerical_cols = [cname for cname in X.columns\nif X[cname].dtype in ['int64', 'float64']]","fbd7879e":"# Merge all cols\nmy_cols = categorical_cols + numerical_cols\nX = X[my_cols].copy()\nX_test = X_test[my_cols].copy()\n\n# Let's see the first 5 cols\nX.head()","d430d42c":"one_hot_cols = categorical_cols\n\nif 1 == 0:\n    \n    x_cat_unique_values  = [col for col in X[categorical_cols].columns if len(X[col].unique()) <= 10]\n\n    dict_diff_onehot = set(categorical_cols) - set(x_cat_unique_values)\n\n    one_hot_cols = x_cat_unique_values\n\n    labelEncoder = LabelEncoder()\n\n    for col in list(dict_diff_onehot):\n        x_unique = X[col].unique();\n        x_test_unique = X_test[col].unique();\n\n        union_uniques = list(x_unique) + list(x_test_unique)\n\n        uniques = list(dict.fromkeys(union_uniques));\n\n        labelEncoder.fit(uniques);\n\n        X[col] = labelEncoder.transform(X[col].astype(str))\n        X_test[col] = labelEncoder.transform(X_test[col].astype(str))\n","f5774b16":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import learning_curve\nfrom math import sqrt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom catboost import CatBoostRegressor","b4c958b7":"X.head()","5742dc26":"# Numerical data\nnumerical_transformer = Pipeline(steps=[\n  ('imputer', SimpleImputer(strategy='mean')),\n  ('scaler', MaxAbsScaler())\n])\n\n# Categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, one_hot_cols)\n    ]\n)\n\n# Bundle preprocessing and modeling the pipeline\npipeline = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor),\n    ]\n)\n\n# Preprocessing of training data\nX_train_fit = pipeline.fit_transform(X)\nX_test_fit = pipeline.transform(X_test)","0ee22b54":"#print(\"How much features we have now =  \"+str(len(X_train_fit[0])))","185b5af6":"if 1 == 1:\n    from sklearn.feature_selection import RFE\n    \n    nfeature = 200\n    \n    trans = RFE(XGBRegressor(random_state=0,objective=\"reg:squarederror\"), n_features_to_select=nfeature)\n    \n    X_train_fit = trans.fit_transform(X_train_fit, y)\n    X_test_fit = trans.transform(X_test_fit)","43a372dd":"if 1 == 0:\n    from sklearn.model_selection import RandomizedSearchCV\n   \n    try:\n        nfeature\n    except NameError:\n        nfeature = len(X_train_fit[0])\n        \n    n_estimators = range(100,2000,200)\n    max_depth = [2, 3, 5, 10, 15]\n    booster=['gbtree','gblinear','dart']\n    learning_rate=[0.05,0.1,0.012,0.013,0.015,0.016,0.020,0.025,0.2,0.3,0.5]\n    min_child_weight=[1,2,3,4]\n    base_score=[0.25,0.5,0.75,1]\n    max_features=range(1, nfeature)\n\n    # Define the grid of hyperparameters to search\n    hyperparameter_grid = {\n        'n_estimators': n_estimators,\n        'max_depth':max_depth,\n        'learning_rate':learning_rate,\n        'min_child_weight':min_child_weight,\n        'booster':booster,\n        'base_score':base_score,\n        'max_features':max_features\n        }\n\n    random_cv = RandomizedSearchCV(estimator=XGBRegressor(random_state=0,objective=\"reg:squarederror\"),\n                param_distributions=hyperparameter_grid,\n                cv=5, n_iter=25,\n                scoring = 'neg_mean_squared_error',n_jobs = 4,\n                verbose = 5,\n                return_train_score = False,\n                random_state=0)\n\n    random_cv.fit(X_train_fit,y)\n    \n    model = random_cv.best_estimator_\n    \n    print(random_cv.best_estimator_)","f5c6db01":"if 1 == 1:\n    model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0,\n             importance_type='gain', learning_rate=0.015, max_delta_step=0,\n             max_depth=3, max_features=97, min_child_weight=1, missing=None,\n             n_estimators=1100, n_jobs=1, nthread=None,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n             subsample=1, verbosity=1)\n    ","0f5f170c":"#len(X_train_fit[0])","ad25dc4b":"kf = KFold(5, shuffle=True, random_state=0)\n\nfor linhas_treino, linhas_valid in kf.split(X_train_fit):\n    X_train, X_valid = X_train_fit[linhas_treino], X_train_fit[linhas_valid];\n    y_train, y_valid = y.iloc[linhas_treino], y.iloc[linhas_valid];\n    \n    # Define Model\n    model.fit(X_train, y_train);\n\n    # Preprocessing of validation data, get predictions\n    \n    preds = model.predict(X_valid)\n    \n    print('MAE:', mean_absolute_error(np.exp(y_valid), np.exp(preds)),'\\n');\n    print('RMSE:', np.sqrt(mean_squared_error(y_valid, preds)),'\\n');","d6e844fd":"if 1 == 1:\n    # Split data with validation data and train data\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train_fit, y, random_state = 0, train_size=0.8)\n    \n    model.fit(X_train, y_train);\n\n    # Preprocessing of validation data, get predictions\n    preds = model.predict(X_valid)\n\n    print('MAE:', mean_absolute_error(np.exp(y_valid), np.exp(preds)),'\\n');\n    print('RMSE:', np.sqrt(mean_squared_error(y_valid, preds)),'\\n');","00a8542a":"preds_test = model.predict(X_test_fit)\n\n# Create OutPut Data\noutput = pd.DataFrame({'Id': X_test.Id, 'SalePrice': np.exp(preds_test)})\n\n# To CSV\noutput.to_csv('submission.csv', index=False)\n\n# Show me the data!\noutput.head()","f875ec82":"# Feature engineering","3f3798fa":"## Distribution of Sale Price\n\nI used the boxplot to get some idea about the distribution, and we can note 2 outliers, prices greater than **600K**","d0dc33dc":"# Pipeline","b6a4cb53":"## Correlation by Lot Area and Price\n\n\nIt's a good correlation 0.26, so that's means the \"The lot area don't impact the price.\" hypotesys it's wrong, so the alternative hypothesys is 'The lot area impact in the house price' *(omg :o that's so unbelievable)*","936e74ae":"# My idea\n\n\nWas get any knowledge about buying and selling houses, so i got some insights from this two websites: [8 Steps to Buying a House in Iowa](https:\/\/listwithclever.com\/real-estate-blog\/8-steps-to-buying-a-house-in-iowa\/), [10 Important Features to Consider When Buying a House](https:\/\/homeia.com\/10-important-features-to-consider-when-buying-a-house\/) and [Will It Be a Seller's or Buyer's Market in 2019? Find Out Here](https:\/\/listwithclever.com\/real-estate-blog\/sellers-or-buyers-market\/).\n\nAnd i learned about the US houses market, and wat it's important, when is needed to sell a house. I understand why the seasons it's imporant to sell a house, and the quality off the [appliance](https:\/\/homeia.com\/10-important-features-to-consider-when-buying-a-house\/#6-The-age-style-and-condition-of-home-appliances), and others factors.","7e46ce67":"## Lot Area by Price\n\nSee the distribution of houses price and the lot area, we can note 4 big's outliers, with a vast lot area","883478e2":"# EDA - Exploratory Data Analysis","00155a9d":"## Sales by month's\n\nHow many houes was sold by month ?","4190df45":"## Now i count how many houses was sold by year station","3d16e3ea":"# ..........................................................","7a092619":"### Conflict with the domain knowledge\n\nTake a look for the correlation by station, month with SalePrice, and for my surprise, the month and seasons dont have a higher correlation with the sale price.\n\nBut the **\"The seasons don't impact the price\"** hyphotesys was overturned hypothesis? No, becouse the diference of the houses prices between the seasons it's not so big, but it's ture, more house it's solded on the 'cold' seasons.","17bd4d9d":"## I create this new data frame, were i select the columns 'SalePrice' and 'MoSold'.\n\nThe idea is 'Use the month's to get the seasons and using boxplot, compare the distribuiton of the house's price's by seasons.","cc06218e":"## Removing Outiliers\n\n- According to sale price box plot, the dataset had some houses with values greater than 70000, so i removed this houses.\n- According to sale price by lot area scatter plot, the dataset had some lot area greater than 500000, so i removed this houses to.","30dc39c0":"# Hypotheses\n\n- [X] The seasons don't impact the price.\n- [X] The lot area don't impact the price.","55ff55cc":"## The Polinominal idea\n\nIn the Andrew Ng course i learned about this [polinominal regression](https:\/\/towardsdatascience.com\/introduction-to-linear-regression-and-polynomial-regression-f8adc96f31cb) and a get the idea about create a new squared features and that's features are the most positive correlation with the sale price. And the square root feature, i got from a netflix post (but i lost the link :c), were the post says 'square features it's good for linear regression' something like that, so i tryed it to.","58e82d3e":"I created the area util feature, my idea is 'Lot Area it's the house total area, and others it's non 'raw area', so i sum others area's and subtracted from the total area'. That's is like 'The main area'\n","84314931":"## Heatmap\n\nTake a look to correlations with the sale price","b032390d":"The Have Pool it's a boolean feature, if the pool area it's greater than 0 means that house have a pool","840675ef":"## Distribution of lot area\n\n*yes i love boxplot*\n\nIn the lot area the outliers it's the house with the lot area greater than **70k**\n","c9493592":"## Targets and features\n\n\nSetting the target *y* = *Sale Price*\n\nSetting the features *X* = *Others columns* (?) ","19473f3f":"## Finaly, prepare submit","601095a7":"## Import Libraries","21ec0577":"### New distribution sale price by lot area"}}