{"cell_type":{"119ca8d3":"code","da220179":"code","4227ae97":"code","3179cf66":"code","9a949e37":"code","940cd4c9":"code","0046d8f1":"code","491c8366":"code","438eca77":"code","da3976cb":"code","92be8555":"code","cafa4e4a":"code","be241f40":"code","855b6799":"code","5d0f33c0":"code","3ea598a2":"code","b8a85cb7":"code","18f16068":"code","f86b9a12":"code","46a4590a":"code","9b0cc844":"code","2eeb8a78":"code","ee891499":"code","7fdbd86e":"code","8684ca92":"code","43e88399":"code","435c04ac":"code","b6e82d58":"code","bb2caef3":"code","c44afb5a":"code","ea87447d":"code","f8251410":"code","2d746994":"code","42bd2360":"code","d96ee66d":"code","481b7056":"code","464843b7":"markdown","39b29819":"markdown","ee9aae06":"markdown","74ea2644":"markdown","8a2473f0":"markdown","9ae46f3c":"markdown","88d264af":"markdown","a178ab09":"markdown","89810af1":"markdown","e162c629":"markdown","70359b03":"markdown","ba8f8780":"markdown","0e7b2d29":"markdown","94063800":"markdown","7e0bfd44":"markdown","bcc9dc6d":"markdown","44ed3ffc":"markdown","082bdd88":"markdown","06d3315c":"markdown","7d26b16e":"markdown","e4d5c56f":"markdown","94399732":"markdown","ca2732da":"markdown","01f9b823":"markdown","37d813f8":"markdown"},"source":{"119ca8d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","da220179":"#importing other necessary modules for visualization and label-encoding\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#warning suppressing section\nimport warnings\nwarnings.filterwarnings('ignore')","4227ae97":"data_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","3179cf66":"data1 = data_train.copy()\ncombo = [data1, test]\nprint(data1.isnull().sum())\nprint(data1.describe())","9a949e37":"print(test.isnull().sum())\nprint(test.describe())","940cd4c9":"data1.head(10)","0046d8f1":"sns.boxplot(data1.Age)\n","491c8366":"sns.boxplot(data1.Fare)","438eca77":"for i,d in enumerate(combo):\n    d.Age.fillna(d.Age.median(), inplace=True)\n    d.Embarked.fillna(d.Embarked.mode()[0], inplace=True)\n    d.Fare.fillna(d.Fare.median(),inplace=True)\n    d = d.drop(columns=['Cabin','PassengerId','Ticket'], axis=1)              \n    print(d.isnull().sum())\n    print(\"-\"*20)\n    combo[i]=d","da3976cb":"data1=combo[0]\ndata1.groupby('Sex')['Survived'].value_counts(normalize=True)\n","92be8555":"sns.set_style('darkgrid')\nsns.distplot(data1.Age,hist=True,kde=True,bins=25)","cafa4e4a":"print(\"Total passengers within age group below Five:\",data1[data1.Age<5]['Survived'].count())\nprint(\"Survived:\",data1[(data1.Age<5)&(data1.Survived==1)]['Survived'].count())","be241f40":"data1['title']=d['Name'].str.split(',',expand=True)[1].str.split('.',expand=True)[0]\ndata1.title.value_counts()","855b6799":"for i,d in enumerate(combo):\n    d = d.copy()\n    d['family_size']= d['Parch']+d['SibSp']+1\n    #name_breakdown\n    d['title']=d['Name'].str.split(',',expand=True)[1].str.split('.',expand=True)[0]\n    title_tags = d.title.value_counts() <10\n    d['title']=d.title.apply(lambda x:'Misc' if title_tags.loc[x]==True else x)\n    combo[i]= d","5d0f33c0":"for i,d in enumerate(combo):\n    d= d.copy()\n    d['Farebin']=pd.qcut(d.Fare.astype('int'), 4)\n    d['Agebin']=pd.cut(d.Age.astype('int'), 5)\n    combo[i]=d","3ea598a2":"#label encoding for the train and test data\nfrom sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\nfor i,d in enumerate(combo):\n    d=d.copy()\n    d['sex_code']=enc.fit_transform(d.Sex)\n    d['embarked_code']=enc.fit_transform(d.Embarked)\n    d['title_code']=enc.fit_transform(d.title)\n    d['agebin_code']=enc.fit_transform(d.Agebin)\n    d['farebin_code']=enc.fit_transform(d.Farebin)\n    combo[i]=d\n","b8a85cb7":"data_enc = combo[0]\nprint(data_enc.columns)","18f16068":"#dummy variable creation\ny_feature=['Survived']\nX_ft=data_enc.columns\nX_ft_copy = X_ft.copy()\nX_ft_copy = X_ft_copy.drop(['Survived', 'Name','Farebin','Agebin'])\nX_dummy = pd.get_dummies(data_enc[X_ft_copy])\nprint(X_ft_copy)\nprint(X_dummy.columns)","f86b9a12":"X_aug_y = X_dummy; X_aug_y['y']= data1['Survived']\nplt.figure(figsize=(15,10))\nsns.heatmap(X_aug_y.corr(), annot=True, cmap='coolwarm')","46a4590a":"to_drop = ['SibSp','Parch','Sex_female','Sex_male','Embarked_Q','embarked_code','title_Misc','y']\nX_dummy = X_dummy.drop(to_drop, axis=1)\n#X_dummy.columns\n","9b0cc844":"y=data1['Survived']","2eeb8a78":"#GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as gbc\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n\ndef evaluate(estimator, data, features, performCV=True, printFeatureImportance=True, cv=5):\n    #Fit the algorithm on the data\n    estimator.fit(data[features], y)\n        \n    #Predict training set:\n    predictions = estimator.predict(data[features])\n    predprob = estimator.predict_proba(data[features])[:,1]\n    \n    #Perform cross-validation:\n    if performCV:\n        cv_score = cross_val_score(estimator, data[features], y, cv=cv, scoring='roc_auc')\n    \n    #Print model report:\n    print(\"Model Report\")\n    print(\"Accuracy : {}\".format(accuracy_score(y,predictions)))\n    print(\"F1 Score : {}\".format(f1_score(y,predictions)))\n    print(\"AUC Score (Train): {}\".format(roc_auc_score(y, predprob)))\n    \n    if performCV:\n        print(\"CV Score : Mean - {:.7f} | Std - {:.7f} | Min - {:.7f} | Max - {:.7f}\" \\\n              .format(np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n        \n    #Print Feature Importance:\n    if printFeatureImportance:\n        feat_imp = pd.Series(estimator.feature_importances_, features).sort_values(ascending=False)\n        feat_imp.plot(kind='bar',title='Feature Importances')\n        plt.ylabel('Feature Importance Score')","ee891499":"#baseline gbm model\nfeatures = X_dummy.columns\ngbm0=gbc(random_state=20)\nevaluate(gbm0, X_dummy, features)","7fdbd86e":"#tuning1:\ngbm1=gbc(min_samples_split=10,min_samples_leaf=5,max_depth=3, max_features='sqrt', subsample=0.8, random_state=20)\nparam_tune1 = {'n_estimators':range(10,51,10)}\ngridCV1=GridSearchCV(estimator=gbm1, param_grid=param_tune1, scoring='roc_auc', n_jobs=-1, iid=False, cv=5)\ngridCV1.fit(X_dummy,y)\n","8684ca92":"print(gridCV1.best_params_, gridCV1.best_score_)","43e88399":"#tuning2:\ngbm2=gbc(n_estimators=40,min_samples_leaf=5,random_state=20,subsample=0.8)\nparam_tune2 = {'max_depth':range(2,11),\n               'min_samples_split':range(5,20),\n                'max_features':['auto','sqrt']}\ngridCV2 = GridSearchCV(estimator=gbm2,param_grid=param_tune2, scoring='roc_auc',iid=False, n_jobs=-1,cv=5)\ngridCV2.fit(X_dummy,y)\nprint(gridCV2.best_params_, gridCV2.best_score_)","435c04ac":"#tuning3:\ngbm3=gbc(n_estimators=40,max_depth=3,max_features='sqrt',random_state=20,subsample=0.8)\nparam_tune3 = {'min_samples_split':range(20,81),\n                'min_samples_leaf':range(5,31)}\ngridCV3 = GridSearchCV(estimator=gbm3,param_grid=param_tune3, scoring='roc_auc',iid=False, n_jobs=-1,cv=5)\ngridCV3.fit(X_dummy,y)\nprint(gridCV3.best_params_, gridCV3.best_score_)\n","b6e82d58":"#evaluation based on current parameter\ngbm4=gbc(n_estimators=40,min_samples_split=58, min_samples_leaf=5, max_features='sqrt',\n        max_depth=3,subsample=0.8, random_state=20)\nevaluate(gbm4, X_dummy, features)","bb2caef3":"gbm5 = gbc(min_samples_split=58, min_samples_leaf=5, max_features='sqrt',\n          max_depth=3, subsample=0.8, random_state=20)\nparam_tune4 = {'learning_rate':np.arange(0.01,0.1,0.01),\n              'n_estimators':range(40,400,10)}\ngridCV4 = GridSearchCV(estimator=gbm5,param_grid=param_tune4, scoring='roc_auc',iid=False, n_jobs=-1,cv=5)\ngridCV4.fit(X_dummy, y)\nprint(gridCV4.best_params_, gridCV4.best_score_)\n","c44afb5a":"gbm6=gbc(learning_rate=0.02,n_estimators=220, min_samples_split=58, min_samples_leaf=5, max_features='sqrt',\n          max_depth=3, subsample=0.8, random_state=20)\n#param_tune4={'n_estimators':range(100,1001,100)}\n#gridCV4=GridSearchCV(estimator=gbm6, param_grid=param_tune4, scoring='roc_auc', iid=False, n_jobs=-1,\n                    #cv=5)\n#gridCV4.fit(X_dummy, y)\nevaluate(gbm6, X_dummy, features)","ea87447d":"test1=combo[1]\n#test1.head(10)\nft=test1.columns\nft_copy = ft.copy()\nft_copy = ft_copy.drop(['Name','Farebin','Agebin'])\nX_test = pd.get_dummies(test1[ft_copy])","f8251410":"drop = ['SibSp','Parch','Sex_female','Sex_male','Embarked_Q','embarked_code','title_Misc']\nX_test=X_test.drop(drop, axis=1)\nprint(X_dummy.columns)\nprint(X_test.columns)\n","2d746994":"#Prediction of Survival Labels\ntest['Survived'] = gbm6.predict(X_test)\n","42bd2360":"data = {'PassengerId':test.PassengerId.values,\n                          'Survived':test.Survived.values}\nsubmission = pd.DataFrame(data)\nsubmission.to_csv('submission.csv', index=False)","d96ee66d":"test.groupby('Sex')['Survived'].value_counts(normalize=True)\n","481b7056":"print(\"Total passengers within age group below Five:\",test[test.Age<5]['Survived'].count())\nprint(\"Survived:\",test[(test.Age<5)&(test.Survived==1)]['Survived'].count())","464843b7":"**Target Variable Preparation**","39b29819":"First we create a baseline gradient boosting model with hyperparameters not properly tuned and  prone to overfittin then we will reach upto a properly tuned model for our classification purpose. In the following steps we tune the model and evaluate its result so that eventually we reach upto a better classification model and showing important features step by step after proper tuning. Grid Search Cross Validation is somewhat a lengthy process if multiple parameters are tuned simulteneously so it had broken to several steps. ","ee9aae06":"**Dummy Variable Introduction**\n\nThe more features we have the more we have insights about the data, hence dummy variable introduction is a good thing to practice plus it helps us to show more details about categorical variable regarding which categorical value is contributing more to the data","74ea2644":"**Module Import Section**\n\n\nHere we import main two main visualization libraries for some basic EDA, and also to suppress all warnings. It is to mention suppressing warnings is just a method to show some cleaner output, but one may avoid it if they want","8a2473f0":"**Model Fitting **\n\nWe approach towards the Gradient Boosting Classifier to solve our problem. It is quite a helpful algorithm in terms of binary classification. Scikit-Learn library provides the classifier model pre-trained, yet hyperparameters are tuned to for better result and avoid overfitting\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html\n","9ae46f3c":"The following function fits the model and evaluates necessary metrics for model evaluation and also shows the important features suggested by the model as of high priority. This following tutorial gave me the idea for building this and it is quite helpful in terms of a compact model performance evaluation\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/","88d264af":"Here we see a high survival rate for female passengers and a very low survival rate for male passengers so we may expect this pattern to retain over test dataset too.\n\nNext another important factor comes into consideration is **Age** of the passenger. So we check the histogram for age of the passengers to have an idea regarding its distribution","a178ab09":"Here we also see the same pattern in survival. A very high survival rate for female passengers and that of very low for male passengers","89810af1":"Well, we go for the majority and mark the rest ones with tag 'Misc.'","e162c629":"We notice that more than 50% of the children in the age group below Five survived, hence this pattern will retain in test dataset too\n","70359b03":"Next We have two continuous variables which may cut into different groups for some quick insights about age and fare. \n* 1. Farebin: It splits the Fare column into different groups which can easily identify whether a passenger is within high , low or some intermediate medium range\n* 2. Agebin: Similar to Farebin it splits the entire age into different age groups for quick identification","ba8f8780":"**Correlation Matrix Visualization**\n\nSince we are using many features , it is possible that there are some redundant features which are not contributing that much. It also show if there is multicollinearity between two features, which must be avoided. We use heatmap to visualize them quickly and easily","0e7b2d29":"As of the conclusion from heatmap, the following features are dropped because of multicollinearity and also some of them are not contributing much to towards target variable classification","94063800":"After fitting gridsearch CV model properly to our data we check best parameters and corresponding scores after every step. This parameter will be used in the model. Since this is a binary classification problem hence ROC-AUC is a good evaluation metric for evaluation of classification power of the model. ","7e0bfd44":"Here we see that age and and Fare are not evenly distributed through out the entire range but we will try to retain this structure after the missing value treatments. Also Ticket column will not contribute much since ther is no numbering or any kind of tagging cannot be assigned with that ticket number. PassengerId is dropped because we already have passenger name details for reference purpose.","bcc9dc6d":"**Treatments of Categorical Variables**\n\nIn order to deal with categorical variables we encode them with different labels with integer values. Since rest of the values are numeric it is a good idea to bring all of them under similar system. As for later purpose we may need to use some feature scaling and then all variables may be treated the same way. Though in this problem we may not require that but many other machine learning problem requires some feature scaling for better results ","44ed3ffc":"Here we have 100% survival rate for children which is improved as compared to train set.","082bdd88":"**Data Import Section**\n\n","06d3315c":"**Conclusion**\n\nHere we do some basic analysis of predicted labels of survial of passengers. During basic EDA, we found the following facts\n* 1. There is high survival rate in case of female passengers as compared to male passengers\n* 2. Children below the age of Five had a survival rate of more than 50%\n\nSo, we may expect the same pattern in case of test dataset too. The following part focuses on that","7d26b16e":"**Feature Engineering**\n\nIt is always a good idea to generate new features, sometimes it helps to improve the model greatly. But we need to avoid collinearity before fitting to the model.\nWe Introduce two new features\n* 1. Family Size: since we have information about number of siblings ,spouse, parent so it is easy to calculate the family size of an aboarded individual.\n* 2. Title Tags: Since every name comes with it title tags (Mr., Mrs. etc) it may play some role with the survival values in our data","e4d5c56f":"**Introduction with the data**\n\nIt is important to know about the given features of our dataset and missing values for each feature. We also check basic statistics of the data using the describe method. ","94399732":"We have so far reached the final step towards model training. The last model will be used for our classification purpose but before that we need to provide the exact same treatment we provided for the case of train dataset, which will make our test dataset to contain the same features as of  the train dataset.\n\n**Preparaing Test Data for Survival Prediction**\n\nHere we will use the same dummy variable introduction technique as we used previously and remove all those features we had removed for training data case. This will enable us to use our model to predict using test data.","ca2732da":"From the above results we found that **Age, Embarked , Cabin, Fare** contains missing values and hence must undergo some missing value treatment. Fare column contains a large number of missing values hence it is better to drop it entirely, rest are to be filled in","01f9b823":"**Basic EDA**\n\nSince **Survived** is the target variable hence it is important know the effects of other variables on Target Variable. From a first glance at the data it is obvious to look for the answer how survival is affected by gender of the passengers","37d813f8":"It is to notice that there were a good number of children ( Age < 5) among passengers so , it is better to look for their survival rate and there are a few older people too ( Age > 65) but they are very less in number. Also the distribution almost following a properly distributed normal curve with slight high density in the starting region and very mildly skewed towards lower age side, but it will not make much impact. "}}