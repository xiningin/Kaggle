{"cell_type":{"f8319eea":"code","8350ffba":"code","706f71ad":"code","878ce9b7":"code","df907eb1":"code","f1ebce26":"code","a2655736":"code","08ef80e9":"code","286fcc29":"code","5fb7bdb9":"code","d32330a1":"code","fef17e7a":"code","158fddd6":"code","bd157baf":"code","d5f363e3":"code","0aa22509":"code","1a8ccfd6":"code","cac99326":"code","88147c56":"code","832a05b3":"code","b76cef41":"code","b7c18692":"code","c1a575f8":"code","1dcfe299":"code","068531f7":"code","ea40d99f":"code","f845939d":"code","9ffd486d":"code","35337aca":"code","4a2e6a2e":"code","59373e62":"code","9bee4a83":"code","f2306a0b":"code","22ec1bf2":"code","2baad50b":"code","b0befc28":"code","4e7e5cfa":"code","f8f380ba":"code","290c7801":"code","a300bc06":"code","32be7955":"code","f23b0058":"code","f3d22337":"code","7f3303f8":"code","0568adaa":"code","50420556":"code","25c3d366":"code","1af743aa":"code","fdaab1c4":"code","4b489d5d":"code","2a82ec28":"code","ec4c0f4e":"code","77ea885d":"code","5a44ceab":"code","f0458ba0":"code","9ba5d617":"code","8da99b91":"code","ba44a626":"code","2bcbef32":"code","4af8675c":"code","c28a0528":"code","9bdb6bb7":"code","b1874f08":"code","82af9cff":"markdown","9d5f6611":"markdown","d51b1af7":"markdown","bdd7ddfe":"markdown"},"source":{"f8319eea":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom torchvision import transforms, models\n\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom PIL import Image\nimport PIL\nfrom io import BytesIO\nimport requests\n\nimport cv2","8350ffba":"import warnings\nwarnings.filterwarnings('ignore')\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","706f71ad":"def binarize(img):\n    img[img<0.5]=0\n    img[img>=0.5]=1\n    return img[:,:,0]\n\ndef load_image(img_path,transform_img=True):\n    if \"http\" in img_path:\n        response = requests.get(img_path)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    else:\n        image = Image.open(img_path).convert('RGB')\n    return image_transform_vgg(image,transform_img)\n    \ndef image_transform_vgg(image, transform_img=True):\n    if transform_img:\n        # pre-processed for the pre-trained model\n        normalize = transforms.Normalize(mean = (0.485, 0.456, 0.406),\n                                         std = (0.229, 0.224, 0.225))\n        \n        transform = transforms.Compose([transforms.ToTensor(),\n                                        normalize\n                                        ])\n    else:\n        transform = transforms.Compose([transforms.ToTensor(),\n                                        lambda x: x.transpose(0,2),\n                                        lambda x: x.transpose(0,1),\n                                        binarize,\n                                        ])\n    return transform(image)\n\ndef image_pure(img,):\n    normalize = transforms.Normalize(mean = (-0.485\/0.229, -0.456\/0.224, -0.406\/0.255),\n                                     std = (1\/0.229, 1\/0.224, 1\/0.255))\n    pure_image = transforms.Compose([normalize,\n                                     lambda x: x.cpu().numpy(),\n                                     lambda x: x.clip(0.0, 1.0),\n                                     lambda x: x.transpose(1,2,0),\n                                     torch.from_numpy,\n                                    ])\n    return pure_image(img)\n","878ce9b7":"img_path = 'https:\/\/raw.githubusercontent.com\/luanfujun\/deep-painterly-harmonization\/master\/data\/'\nimg_index = 16","df907eb1":"style_img = load_image(f'{img_path}{img_index}_target.jpg')\ncontent_img = load_image(f'{img_path}{img_index}_naive.jpg')\nmask_img = load_image(f'{img_path}{img_index}_c_mask.jpg',0)\ndilated_mask_img = load_image(f'{img_path}{img_index}_c_mask_dilated.jpg',0)","f1ebce26":"style_img.shape, content_img.shape, mask_img.shape, dilated_mask_img.shape","a2655736":"fig,(ax1,ax2,ax3,ax4) = plt.subplots(nrows=1,ncols=4,figsize=(20,10))\nax1.imshow(image_pure(style_img))\nax2.imshow(image_pure(content_img))\nax3.imshow(mask_img)\nax4.imshow(dilated_mask_img)\nplt.show()","08ef80e9":"fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(10,10))\nax1.imshow( image_pure(content_img) * mask_img[:,:,None])\nax2.imshow(dilated_mask_img[:,:,None] * image_pure(content_img))\nplt.show()","286fcc29":"model = models.vgg16_bn(pretrained=True).features.to(device)\nmodel.eval()\nfor param in model.parameters():\n    param.requires_grad_(False)","5fb7bdb9":"def vgg_features(data,index=None):\n    layers_index = {'2' : 'conv1_1', # 'layer' : 'layer_name'\n                    '9' : 'conv2_1',\n                    '16' : 'conv3_1',\n                    '26' : 'conv4_1',\n                    '29' : 'conv4_2',  ## content representation\n                    '36' : 'conv5_1'}\n    layers = index if index else layers_index  \n    features={}\n    for layer_index, layer in model._modules.items():\n        data = layer(data)\n        if layer_index in layers:\n            features[layers_index[layer_index]]=data\n    return features","d32330a1":"content_img_vggfeatures = vgg_features(content_img[None].to(device), index=['16','26','36'])\nstyle_img_vggfeatures = vgg_features(style_img[None].to(device), index=['16','26','36'])","fef17e7a":"[i.shape for i in content_img_vggfeatures.values()]","158fddd6":"class MaskFeatures(nn.Module):\n    \"\"\"\n    change the dimension of the mask image by applying the AvgPool \n    corresponding to each convolutional layer of\n    vgg model\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.convol_mask = nn.AvgPool2d(kernel_size=3, padding=1, stride=1)\n    \n    def apply_mask(self,data,loop):\n        for j in range(loop):\n            data = self.convol_mask(data)\n        return data\n   \n    def forward(self,data):\n        features=[]\n        temp = [2,2,1,2,1,2,1]\n        data = data[None,None,:,:]\n        for k in temp:\n            if k==1:\n                data = self.convol_mask(data)\n                features.append(data.squeeze().to(device))\n            else:\n#                 data = self.apply_mask(data,loop=k)\n                data = F.interpolate(data, size=(data.shape[2]\/\/2, data.shape[3]\/\/2))\n        return features","bd157baf":"extract_maskfeatures = MaskFeatures().to(device)\ndilated_mask_imgfeatures = extract_maskfeatures((dilated_mask_img).float())","d5f363e3":"print([i.shape for i in dilated_mask_imgfeatures])\nfig,(ax1,ax2,ax3) = plt.subplots(nrows=1,ncols=3,figsize=(20,10))\nax1.imshow((dilated_mask_imgfeatures[0].cpu()),cmap='gray')\nax2.imshow((dilated_mask_imgfeatures[1].cpu()),cmap='gray')\nax3.imshow((dilated_mask_imgfeatures[2].cpu()),cmap='gray')\nplt.show()","0aa22509":"def get_patch(img, kernel_size=3 ,stride=1 ,padding=1):\n    \"\"\"\n    returns the array of values by applying the filter of \n    kernel_size * kernel_size at each pixel of the image\n    \"\"\"\n    patcher = nn.Unfold(kernel_size = kernel_size, padding=padding, stride=stride)\n    patches = patcher(img)\n    return patches.transpose(1,2).squeeze()","1a8ccfd6":"def match_features(content_img, style_img):\n    \"\"\"\n    Use the cosine similarity concept to find out how much\n    two image patched look alike return the indexes\n    \"\"\"\n    result = []\n    for input_layerfeature, style_layerfeature in zip(content_img, style_img):\n        \n        input_layerpatch = get_patch(input_layerfeature)\n        style_layerpatch = get_patch(style_layerfeature)\n        \n#         input_dot_style = input_layerpatch @ style_layerpatch.T\n#         mod_input_layerpatch = torch.sqrt(torch.sum(input_layerpatch ** 2, 1)).unsqueeze(1)\n#         mod_style_layerpatch = torch.sqrt(torch.sum(style_layerpatch ** 2 ,1)).unsqueeze(0)\n#         down = (1e-15 + torch.mm(mod_input_layerpatch, mod_style_layerpatch))\n#         cosine_similarity = input_dot_style \/ down\n#         _, index = cosine_similarity.max(1)\n        cosine = cosine_similarity(input_layerpatch.cpu(), style_layerpatch.cpu())\n        index = np.argmax(cosine,1)        \n        result.append(torch.tensor(index, device=device))\n    return result","cac99326":"%%time\ncontent_mapped_stylefeatures = match_features(content_img_vggfeatures.values(), \n                                 style_img_vggfeatures.values())","88147c56":"print(len(content_mapped_stylefeatures),[i.shape for i in content_mapped_stylefeatures])\nfig,(ax1,ax2,ax3) = plt.subplots(nrows=1,ncols=3,figsize=(20,10))\nax1.imshow(content_mapped_stylefeatures[0].cpu().reshape(-1,175))\nax2.imshow(content_mapped_stylefeatures[1].cpu().view(-1,87),)\nax3.imshow(content_mapped_stylefeatures[2].cpu().view(-1,43),)\nplt.show()","832a05b3":"def gram_mapped_style(mapped_index,imgfeatures, mask_features):\n    result = []\n    for index, layer, mask in zip(mapped_index, imgfeatures, mask_features):\n        layer = layer.reshape(layer.shape[1],-1)\n        layer = layer[:,index] * mask.view(1,-1)\n        result.append(torch.tensor(layer @ layer.T))\n    return result","b76cef41":"style_mappedGramContent = gram_mapped_style(content_mapped_stylefeatures, style_img_vggfeatures.values(),\n                                            dilated_mask_imgfeatures)\n[i.shape for i in style_mappedGramContent]","b7c18692":"target_img = torch.tensor(content_img[None,:,:,:], requires_grad=True,device=device)\ndef content_loss(imgfeature): \n    img_layer4_1 = imgfeature * dilated_mask_imgfeatures[1][None,None,:,:]\n    content_layer4_1 = content_img_vggfeatures['conv4_1'] * dilated_mask_imgfeatures[1][None,None,:,:]\n    return F.mse_loss(img_layer4_1, content_layer4_1, reduction='sum')\/ float(imgfeature.shape[1] * dilated_mask_imgfeatures[1].sum())\n\ndef style_loss(imgfeatures):\n    loss_val = 0\n    for img_layer, gram_style_layer, mask_layer in zip(imgfeatures, style_mappedGramContent, dilated_mask_imgfeatures):\n        \n        img_layer = img_layer.reshape(img_layer.size(1),-1)        \n        img_layer = img_layer * mask_layer.view(1,-1)\n\n#         style_layer = style_layer * mask_layer.view(1,-1)\n        \n        gram_img_layer = img_layer @ img_layer.T\n#         gram_style_layer = style_layer @ style_layer.T\n\n        loss_val += F.mse_loss(gram_img_layer, gram_style_layer)\n    return loss_val*10\n\ndef total_loss(imgfeatures):\n    content_loss_val = content_loss(imgfeatures[1])\n    style_loss_val = style_loss(imgfeatures)\n    total_loss = content_loss_val + style_loss_val\n    global loop\n    loop+=1\n    if loop%100==0 or loop==1:\n        print(f'{loop:>7d}{total_loss:>15.8f}{style_loss_val:>15.8f}{content_loss_val:>15.8f}')\n    return total_loss","c1a575f8":"%%time\noptimizer = torch.optim.LBFGS([target_img], lr=1)\nepoch = 1000\nloop = 0\nprint(f'{\"loop\":>7}{\"total_loss\":>15}{\"style_loss\":>15}{\"content_loss\":>15}')\nwhile loop<=epoch:\n    def closure():\n        optimizer.zero_grad()\n        output = list(vgg_features(target_img, index=['16','26','36']).values())\n        loss = total_loss(output)\n        loss.backward()\n        return loss\n    optimizer.step(closure)\ntarget_img = target_img.detach().squeeze()   ","1dcfe299":"phase1_img = (target_img.cpu()*mask_img + content_img*(1-mask_img)).squeeze()\n\nfig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(20,20))\nax1.imshow(image_pure(target_img))\nax2.imshow(image_pure(phase1_img))\nplt.show()","068531f7":"content_img_vggfeatures = vgg_features(content_img[None].to(device), index=['2','9','16','26'])\nstyle_img_vggfeatures = vgg_features(style_img[None].to(device), index=['2','9','16','26'])\nphase1_img_vggfeatures = vgg_features(phase1_img[None].to(device), index=['2','9','16','26'])","ea40d99f":"[i.shape for i in phase1_img_vggfeatures.values()]","f845939d":"class MaskFeatures2(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.convol_mask = nn.AvgPool2d(kernel_size=3, padding=1, stride=1)\n        \n    def resize2half(self,data):\n        return F.interpolate(data, size=(data.shape[2]\/\/2, data.shape[3]\/\/2))\n    \n    def forward(self,data):\n        features=[]\n        data = data[None,None,:,:]\n        data = self.convol_mask(data)\n        features.append(data.squeeze().to(device))\n        data = self.resize2half(self.convol_mask(data))\n        data = self.convol_mask(data)\n        features.append(data.squeeze().to(device))\n        data = self.resize2half(self.convol_mask(data))\n        data = self.convol_mask(data)\n        features.append(data.squeeze().to(device))\n        data = self.convol_mask(data)\n        data = self.resize2half(self.convol_mask(data))\n        data = self.convol_mask(data)\n        features.append(data.squeeze().to(device))\n        return features","9ffd486d":"dilated_mask_img.shape","35337aca":"phase2_maskfeatures = MaskFeatures2()\ndilated_mask_imgfeatures = phase2_maskfeatures(dilated_mask_img)","4a2e6a2e":"[i.shape for i in dilated_mask_imgfeatures]","59373e62":"fig,ax = plt.subplots(nrows=1,ncols=4,figsize=(20,10))\nfor i in range(4):\n    ax[i].imshow(dilated_mask_imgfeatures[i].detach().cpu())","9bee4a83":"phase1imgfeatures_mapped_stylefeatures = match_features([list(phase1_img_vggfeatures.values())[-1]], \n                                                        [list(style_img_vggfeatures.values())[-1]])","f2306a0b":"[i.shape for i in phase1imgfeatures_mapped_stylefeatures]","22ec1bf2":"plt.imshow(phase1imgfeatures_mapped_stylefeatures[0].reshape(-1,87).cpu())","2baad50b":"# resize style image according to the conv4_1\nconv4_1_size = style_img_vggfeatures['conv4_1'].shape\nprint(f'conv4_1_size: {conv4_1_size}')\nstyle_img_resized = cv2.resize((image_pure(style_img)).numpy(), (conv4_1_size[-1], conv4_1_size[-2]))\nstyle_img_resized = image_transform_vgg(style_img_resized).to(device)\nstyle_img_resized.shape","b0befc28":"plt.imshow(image_pure(style_img_resized))","4e7e5cfa":"# resize mask image according to the conv4_1\nmask_img_resized = cv2.resize(mask_img.numpy(), (conv4_1_size[-1], conv4_1_size[-2]), interpolation=cv2.INTER_NEAREST)\nmask_img_resized = torch.from_numpy(mask_img_resized).to(device)\nmask_img_resized.shape","f8f380ba":"plt.imshow(mask_img_resized.view(-1,87).cpu())","290c7801":"def neighbour_index(centered_index, neighbour, img_size):\n    height,width = img_size\n    co_ordinated_index = np.array([centered_index\/\/width, centered_index%width]) + neighbour\n    if 0<=co_ordinated_index[0]<height and 0<=co_ordinated_index[1]<width:\n        return co_ordinated_index[0] * width + co_ordinated_index[1]\n    else:\n        return -1\n\ndef phase2_match_features(mappedfeatures, style_features, mask, kernel_size=5):\n    ch,height,width = style_features.shape\n    mask = mask.view(-1) # 1-d array\n    style_features = style_features.view(ch,-1).T\n    result = mappedfeatures.clone()\n    grid = [np.array([i,j]) for i in range(-(kernel_size\/\/2),kernel_size\/\/2+1)\n                            for j in range(-(kernel_size\/\/2),kernel_size\/\/2+1)]\n    for img_pixel in range(height*width):\n        if mask[img_pixel]<=0.1:continue\n        candidates = set()\n        style_neighbour = []\n        for pixel_cordinate in grid:\n            to_index = neighbour_index(img_pixel, pixel_cordinate, (height, width))\n            if to_index != -1:\n                index_to = neighbour_index(mappedfeatures[to_index], -pixel_cordinate,(height, width))\n                if index_to != -1:\n                    candidates.add(index_to)\n                    style_neighbour.append(index_to)\n        candidates = torch.tensor([*candidates])\n        if len(candidates)<=1:continue\n        candiadate_features = np.take(style_features, candidates, axis=0)\n        neighbour_features = np.take(style_features,style_neighbour ,axis=0)\n        dists = (((candiadate_features[:,None] - neighbour_features[None])**2).sum(axis=2)).sum(axis=1)\n        result[img_pixel] = candidates[dists.argmin()]\n    return result","a300bc06":"mapped_phase2 = phase2_match_features(phase1imgfeatures_mapped_stylefeatures[0].cpu(),\n                                      style_img_resized.cpu(), mask_img_resized.reshape(-1)).to(device)","32be7955":"plt.imshow(mapped_phase2.view(-1,87).cpu())","f23b0058":"def upsample(mapped, orig_size, new_size):\n    orig_height, orig_width = orig_size\n    new_height, new_width = new_size\n    TOTAL_NewMap_PIXEL = new_height * new_width\n    new_map = torch.zeros(TOTAL_NewMap_PIXEL, dtype=torch.int32)#.to(device)\n    scale_factor_height, scale_factor_width = new_height\/orig_height, new_width\/orig_width\n    \n    for map_pixel in range(TOTAL_NewMap_PIXEL):\n        new_map_x_index , new_map_y_index = divmod(map_pixel, new_width)\n        orig_x_index = min((0.5+new_map_x_index)\/\/scale_factor_height, orig_height-1)\n        orig_y_index = min((0.5+new_map_y_index)\/\/scale_factor_width, orig_width-1)\n        mapped_pixelIndex = mapped[int(orig_x_index*orig_width + orig_y_index)]\n        \n        mapped_x_index = int(new_map_x_index + (mapped_pixelIndex\/\/orig_width - orig_x_index)*scale_factor_height + 0.5)\n        mapped_y_index = int(new_map_y_index + (mapped_pixelIndex%orig_width - orig_y_index)*scale_factor_width + 0.5)\n        \n        mapped_x_index = min(mapped_x_index,new_height-1)\n        mapped_y_index = min(mapped_y_index,new_width-1)\n        \n        new_map[map_pixel] = mapped_x_index*new_width + mapped_y_index\n    return new_map","f3d22337":"%%time\nmapped_features2 = [upsample(mapped_phase2, dilated_mask_imgfeatures[-1].shape, mask_layer.shape)\n                    for mask_layer in dilated_mask_imgfeatures[:-1]] + [mapped_phase2]","7f3303f8":"[i.shape for i in mapped_features2]","0568adaa":"mask_imgfeatures = [mask_img_resized]\nfor layer in list(style_img_vggfeatures.values())[-2::-1]:\n    new_layer = cv2.resize((mask_imgfeatures[0].cpu()).numpy(), (layer.size(-1),layer.size(-2)), interpolation=cv2.INTER_NEAREST)\n    mask_imgfeatures.insert(0,torch.tensor(new_layer).to(device))\n[i.shape for i in mask_imgfeatures]","50420556":"fig,ax = plt.subplots(nrows=1,ncols=4,figsize=(20,10))\nfor i in range(4):\n    ax[i].imshow(mask_imgfeatures[i].cpu())","25c3d366":"def mapped_style2(style_features, map_features, mask_features):\n    res=[]\n    mask_=[]\n    for style_ftr, map_ftr, mask_ftr in zip(style_features, map_features, mask_features):\n        style_ftr = style_ftr.reshape(style_ftr.size(1),-1)\n        mask_ftr = mask_ftr.reshape(-1)\n        map_ftr = map_ftr.cpu().numpy()\n        \n        style_ftr = style_ftr[:,map_ftr]\n        map_count = Counter(map_ftr)\n        for index, pixel in enumerate(map_ftr):\n            if mask_ftr[index] >=0.1 and map_count[pixel]>1:\n                mask = (map_ftr==pixel)\n                mask[index] = False\n                mask_ftr[mask]=0\n        res.append(style_ftr)\n        mask_.append(mask_ftr)\n        \n    return res,mask_","1af743aa":"%%time\nstyle_ftrs, mask_ftrs = mapped_style2(style_img_vggfeatures.values(), mapped_features2, mask_imgfeatures)","fdaab1c4":"[i.shape for i in style_ftrs]","4b489d5d":"def histogram_mask(style_features, mask_features):\n    masked = style_features * mask_features\n    return torch.cat([torch.histc(masked[i][mask_features>=0.1], n_bins).unsqueeze(0) for i in range(masked.size(0))]).to(device)","2a82ec28":"n_bins=256\nstyle_hist_mask = [histogram_mask(style_ftrs[i],mask_ftrs[i]) for i in (0,3)]","ec4c0f4e":"[i.shape for i in style_hist_mask]","77ea885d":"def style_loss(imgfeatures):\n    loss_val=0\n    for img_layer, style_layer, mask_layer, mask_resized_layer in zip(imgfeatures, style_ftrs, dilated_mask_imgfeatures, mask_ftrs):\n        img_layer = img_layer.reshape(img_layer.size(1),-1)\n        img_layer = img_layer * mask_layer.view(1,-1)\n\n        style_layer = style_layer * mask_resized_layer[None].view(1,-1)\n        style_layer = style_layer * float(torch.sqrt(mask_layer.sum()\/mask_resized_layer.sum()))\n\n        img_layer = img_layer @ img_layer.T\n        style_layer = style_layer @ style_layer.T\n\n        loss_val += F.mse_loss(img_layer, style_layer)\n    return loss_val\n\ndef content_loss(imgfeature):\n    img_layer4_1 = imgfeature * dilated_mask_imgfeatures[-1][None,None,:,:]\n    content_layer4_1 = content_img_vggfeatures['conv4_1'] * dilated_mask_imgfeatures[-1][None,None,:,:]\n    return F.mse_loss(img_layer4_1, content_layer4_1, reduction='sum')\/ float(imgfeature.shape[1] * dilated_mask_imgfeatures[-1].sum())\n","5a44ceab":"def select_index(tensor, index):\n    channel = tensor.size(0)\n    return tensor.view(-1)[index.view(-1)].view(channel,-1)","f0458ba0":"def map_histogram(img, histogram_ref):\n    channel, depth = img.size()\n    sorted_img, sorted_index = img.sort(1)\n    ymin, ymax = img.min(1)[0].unsqueeze(1), img.max(1)[0].unsqueeze(1)\n    histogram_normalized = histogram_ref * depth \/ histogram_ref.sum(1).unsqueeze(1)\n    cumsum_ref = histogram_normalized.cumsum(1) # cumulative sum https:\/\/pytorch.org\/docs\/stable\/torch.html#torch.cumsum\n\n    cumsum_previousIndex = torch.cat([torch.zeros(channel,1).to(device), cumsum_ref[:,:-1]], 1)\n    step = (ymax - ymin)\/256\n    range_ = torch.arange(1,depth+1).unsqueeze(0).cuda()\n    index = ((cumsum_ref.unsqueeze(1) - range_.unsqueeze(2))<0).sum(2).long()\n    ratio = (range_ - select_index(cumsum_previousIndex, index))\/(select_index(histogram_normalized, index)+ 1e-8)\n    ratio = ratio.squeeze().clamp(0,1)\n\n    new_x = ymin + (ratio + index.float() * step)\n    _, remap = sorted_index.sort()\n    new_x = select_index(new_x, index)\n    return new_x","9ba5d617":"def histogram_loss(imgfeatures):\n    hist_loss = 0\n    mask_features = [dilated_mask_imgfeatures[0], dilated_mask_imgfeatures[3]]\n    for img_layer, mask_layer, style_hist_layer in zip(imgfeatures, mask_features, style_hist_mask):\n        img_layer = img_layer.view(img_layer.size(1),-1)\n        mask_layer = mask_layer.view(1,-1)\n        img_layer = img_layer * mask_layer\n        temp_mask = mask_layer>=0.1\n        temp_mask = temp_mask[0,:]\n        img_layer = torch.cat([layer[temp_mask].unsqueeze(0) for layer in img_layer])\n        hist_loss = F.mse_loss(img_layer, map_histogram(img_layer, style_hist_layer))\n    return hist_loss\/2","8da99b91":"def total_variation_loss(imgfeatures):\n    return ((imgfeatures[:,:-1,:] - imgfeatures[:,1:,:])**2).sum() + ((imgfeatures[:,:,:-1] - imgfeatures[:,:,1:])**2).sum()","ba44a626":"def median_total_variation_loss(img):\n    channel,height, width = img.shape\n    tensor1 = torch.cat([torch.zeros((channel,width))[:,None,:], img], axis=1)\n    tensor2 = torch.cat([torch.zeros((channel,height))[:,:,None], img], axis=2)\n\n    return torch.median((tensor1[:,:-1,:] - tensor1[:,1:,:]) ** 2 + (tensor2[:,:,:-1] - tensor2[:,:,1:]) ** 2)","2bcbef32":"median_totalvariation_loss = median_total_variation_loss(style_img)\nmedian_totalvariation_loss","4af8675c":"def final_loss(imgfeatures):\n    content_loss_val = content_loss(imgfeatures[-1])\n    style_loss_val = style_loss(imgfeatures)\n    histogram_loss_val = histogram_loss([imgfeatures[0], imgfeatures[3]])\n    total_variation_loss_val = total_variation_loss(imgfeatures[0])\n    final_loss = content_loss_val + 0.01*style_loss_val + histogram_loss_val +\\\n                float(10 \/ (1 + torch.exp(median_totalvariation_loss * 10**4 -25)))*total_variation_loss_val\n    global loop\n    loop+=1\n    if loop%100==0 or loop==1:\n        print(f'{loop:>7d}{final_loss:>15.8f}{style_loss_val:>15.8f}{content_loss_val:>15.8f}{histogram_loss_val:>20.8f}{total_variation_loss_val:>25.8f}')\n    return final_loss","c28a0528":"final_target_img = torch.tensor(phase1_img[None], requires_grad=True, device=device)","9bdb6bb7":"%%time\noptimizer = torch.optim.LBFGS([final_target_img],lr=0.1)\nepoch = 1000\nloop = 1\nprint(f'{\"loop\":>7}{\"total_loss\":>15}{\"style_loss_val\":>15}{\"content_loss\":>15}{\"histogram_loss_val\":>20}{\"total_variation_loss_val\":>25}')\nwhile loop<=epoch:\n    def closure():\n        optimizer.zero_grad()\n        output = vgg_features(final_target_img, index=['2','9','16','26'])\n        output = list(output.values())\n        loss = final_loss(output)\n        loss.backward()\n        return loss\n    optimizer.step(closure)","b1874f08":"final_img = final_target_img.detach().squeeze()\nphase2_img = (final_img.cpu()*mask_img + content_img*(1-mask_img)).squeeze()\n\nfig,(ax1,ax2,ax3) = plt.subplots(nrows=1,ncols=3,figsize=(20,20))\nax1.imshow(image_pure(content_img))\nax2.imshow(image_pure(phase1_img))\nax3.imshow(image_pure(phase2_img))\nplt.show()","82af9cff":"# Reconstruction 1","9d5f6611":"# Mapping","d51b1af7":"# Phase 2","bdd7ddfe":"**Helpful links:**\n1. https:\/\/sgugger.github.io\/deep-painterly-harmonization.html\n2. https:\/\/github.com\/sgugger\/Deep-Learning\/blob\/master\/DeepPainterlyHarmonization.ipynb\n3. https:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2016\/papers\/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\n4. https:\/\/arxiv.org\/abs\/1804.03189\n5. https:\/\/github.com\/luanfujun\/deep-painterly-harmonization"}}