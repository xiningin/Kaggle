{"cell_type":{"1246ba07":"code","09b3e899":"code","cbdc675f":"code","ae6c4bfe":"code","cfbbb398":"code","c6ca8a82":"code","ec704040":"code","de40306d":"code","ea948ddf":"code","55f18cf5":"code","c0431649":"code","2629f0d1":"code","abfbeb2c":"code","29739b30":"code","7bcde62b":"code","f6bff864":"code","b684a5a0":"code","caf5a8b0":"code","97f2c5da":"code","2f42dccc":"code","dcbd8b7c":"code","cb14a428":"code","7778c960":"code","8d80644f":"code","aa89019b":"code","424e553c":"code","eadd52d9":"code","04fb0f67":"code","14724046":"code","2abf79c7":"code","5da50c9e":"code","2bbb693a":"code","65759b86":"code","869f53ce":"code","4849623f":"code","74b1db69":"code","cb8f54d5":"code","145729e8":"code","70e7ff7d":"code","c18f32dd":"code","73ab9ab4":"code","7be51fbb":"code","4baa58d9":"code","e7769c9a":"code","18ac23ad":"code","9e95c92c":"code","985f134d":"code","0ce51a3d":"code","8bb03006":"code","27b840b0":"code","8422605b":"code","70f6cae7":"code","0af7629d":"code","d708e28e":"code","f5124712":"code","24e24072":"code","9abc8611":"code","7be5de49":"code","77a96778":"code","5732dd2b":"code","ab376cfa":"code","99e98f56":"code","6eb6135e":"code","4f1e3c9b":"code","ff7d4131":"code","1abd9eb0":"code","07516f90":"code","0dce6c81":"code","674e83e9":"code","daf4b123":"code","0cd9e5a5":"code","63d5bf72":"code","3356c5e0":"code","60f7ae0f":"code","2b67c475":"code","b2c3bf70":"code","8b11c280":"code","0bcc057e":"markdown","96341d30":"markdown","20d800dd":"markdown","561b9db6":"markdown","7394fd06":"markdown","f4b7cbef":"markdown","217d3be8":"markdown","d3bdfd91":"markdown","3435a782":"markdown","dcc0a5f4":"markdown","7ea84fdf":"markdown","4de8449b":"markdown","367bbd54":"markdown","c9ca9413":"markdown","0cb935c8":"markdown","ec50f584":"markdown","0f140c45":"markdown","c30bee44":"markdown","ec0a88b8":"markdown","6c2c2ad6":"markdown","6e034b29":"markdown","d94241fc":"markdown","2c38aaf7":"markdown","97440e6b":"markdown","480f5905":"markdown","3e8a039e":"markdown","4353c223":"markdown","2efd84f7":"markdown","9ffeca82":"markdown","b084ff6a":"markdown","c1fca0e5":"markdown","0bb0d519":"markdown","170532fb":"markdown","bbaaedb6":"markdown","fe9fe46f":"markdown","4dd55e55":"markdown","130cf290":"markdown","b5f1013f":"markdown","845ac4e4":"markdown","290d1cbb":"markdown"},"source":{"1246ba07":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09b3e899":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use(\"fivethirtyeight\")\nsns.set_style(\"darkgrid\")","cbdc675f":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport xgboost\nfrom xgboost import XGBClassifier\n\nimport lightgbm\nfrom lightgbm import LGBMClassifier\n\n# Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.metrics import accuracy_score","ae6c4bfe":"iris = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")\niris.head()","cfbbb398":"iris.shape","c6ca8a82":"# Import scikit-learn dataset library\nfrom sklearn import datasets\n\n# Load dataset\niris = datasets.load_iris()","ec704040":"# print the label species(setosa, versicolor,virginica)\nprint(iris.target_names)\n\n# print the names of the four features\nprint(iris.feature_names)","de40306d":"# print the iris data (top 5 records)\nprint(iris.data[0:5])\n\n# print the iris labels (0:setosa, 1:versicolor, 2:virginica)\nprint(iris.target)","ea948ddf":"# Creating a DataFrame of given iris dataset.\n\ndata = pd.DataFrame({\n    'sepal length':iris.data[:,0],\n    'sepal width':iris.data[:,1],\n    'petal length':iris.data[:,2],\n    'petal width':iris.data[:,3],\n    'species':iris.target\n})\n\ndata.head()","55f18cf5":"X = data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\ny = data['species']                                                       # Labels","c0431649":"# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)    # 70% training and 30% test","2629f0d1":"# Create a Gaussian Classifier\nclf = RandomForestClassifier(n_estimators=100)\n\n# Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train, y_train)","abfbeb2c":"y_pred = clf.predict(X_test)","29739b30":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))","7bcde62b":"feature_imp = pd.Series(clf.feature_importances_, index=iris.feature_names).sort_values(ascending=False)","f6bff864":"# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","b684a5a0":"# Split dataset into features and labels\nX = data[['petal length', 'petal width','sepal length']]  # Removed feature \"sepal length\"\ny = data['species']                                       \n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.70, random_state=5) # 70% training and 30% test","caf5a8b0":"# Create a Gaussian Classifier\nclf = RandomForestClassifier(n_estimators=100)\n\n# Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\n# prediction on test set\ny_pred=clf.predict(X_test)\n\n# Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","97f2c5da":"train = pd.read_csv(\"\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/sample_submission.csv\")","2f42dccc":"display(train.head(3))\ndisplay(test.head(3))\ndisplay(sub.head(3))","dcbd8b7c":"display(train.shape)\ndisplay(test.shape)\ndisplay(sub.shape)","cb14a428":"display(train.info())\ndisplay(test.info())","7778c960":"trainoriginal = train.copy()\ntestoriginal = test.copy()","8d80644f":"train['target'].value_counts()","aa89019b":"x = ['Looking for job change', 'Not looking for job change']\ny = train['target'].value_counts()\nplt.bar(x, y, color='orangered')\nplt.title('Survived Distribution', fontweight='bold', fontsize=20)\nplt.xlabel('Survived', fontweight='bold', fontsize=15)\nplt.ylabel('Frequency', fontweight='bold', fontsize=15)\nplt.show();","424e553c":"train['gender'].value_counts()","eadd52d9":"x = ['Male', 'Female', 'Other']\ny = train['gender'].value_counts()\nplt.bar(x, y, color='orangered')\nplt.title('gender Distribution', fontweight='bold', fontsize=20)\nplt.xlabel('gender', fontweight='bold', fontsize=15)\nplt.ylabel('Frequency', fontweight='bold', fontsize=15)\nplt.show();","04fb0f67":"train['education_level'].value_counts()","14724046":"plt.figure(figsize=(8,6))\nx = ['Graduate', 'Masters', 'High School', 'Phd', 'Primary School']\ny = train['education_level'].value_counts()\nplt.bar(x, y, color='orangered')\nplt.title('education_level Distribution', fontweight='bold', fontsize=20)\nplt.xlabel('education_level', fontweight='bold', fontsize=15)\nplt.ylabel('Frequency', fontweight='bold', fontsize=15)\nplt.show();","2abf79c7":"train['enrolled_university'].value_counts()","5da50c9e":"plt.figure(figsize=(8,6))\nx = ['no_enrollment', 'Full time course', 'Part time course']\ny = train['enrolled_university'].value_counts()\nplt.bar(x, y, color='orangered')\nplt.title('enrolled_university Distribution', fontweight='bold', fontsize=20)\nplt.xlabel('enrolled_university', fontweight='bold', fontsize=15)\nplt.ylabel('Frequency', fontweight='bold', fontsize=15)\nplt.show();","2bbb693a":"train['relevent_experience'].value_counts()","65759b86":"plt.figure(figsize=(8,6))\nx = ['Has relevent experience', 'No relevent experience']\ny = train['relevent_experience'].value_counts()\nplt.bar(x, y, color='orangered')\nplt.title('relevent_experience Distribution', fontweight='bold', fontsize=20)\nplt.xlabel('relevent_experience', fontweight='bold', fontsize=15)\nplt.ylabel('Frequency', fontweight='bold', fontsize=15)\nplt.show();","869f53ce":"train['experience'].value_counts()","4849623f":"train['company_size'].value_counts()","74b1db69":"train = train.drop(['enrollee_id', 'city'], axis=1)\ntest = test.drop(['enrollee_id', 'city'], axis=1)","cb8f54d5":"import missingno as msno","145729e8":"display(train.isnull().sum())\nprint('-'*40)\ndisplay(test.isnull().sum())","70e7ff7d":"# Visualize missing values as a matrix\n# msno.matrix(train,figsize=(11,7), sparkline=False, fontsize=12, color=(0.27, 0.52, 1.0));\n# msno.matrix(train,figsize=(11,7), sparkline=False, fontsize=12, color=(0,.3,.3));\nmsno.matrix(train,figsize=(11,7), fontsize=12, color=(1, 0.38, 0.27));","c18f32dd":"# Visualize the number of missing values as a bar chart\n# color=\"dodgerblue\" \"orangered\"\nmsno.bar(train, color=\"dodgerblue\", sort=\"ascending\", figsize=(13,7), fontsize=12);","73ab9ab4":"fig = plt.figure(figsize=(15,7))\n\nax1 = fig.add_subplot(1,2,1)\nmsno.bar(train, color=\"tomato\", fontsize=12, ax=ax1);\n\nax2 = fig.add_subplot(1,2,2)\nmsno.bar(train, log=True, color=\"tab:green\", fontsize=12, ax=ax2);\n\nplt.tight_layout()","7be51fbb":"# Missing Values\nNaN = np.sum(train.isnull())\nNaN_Col = NaN.loc[(NaN != 0)].sort_values(ascending=False)\n\nplt.figure(figsize=(8,6))\nsns.barplot(x = NaN_Col.index, y = NaN_Col)\nplt.ylabel(\"Missing Value Count\", size=20);\nplt.xlabel(\"Feature Name\", size=20);\nplt.xticks(rotation=90)\nplt.show()","4baa58d9":"# Visualize the correlation between the number of missing values in different columns as a heatmap\nmsno.heatmap(train, cmap=\"RdYlGn\", figsize=(10,5), fontsize=12)","e7769c9a":"msno.dendrogram(train, figsize=(12,7), fontsize=12)","18ac23ad":"fig = plt.figure(figsize=(15,7))\n\nax1 = fig.add_subplot(1,2,1)\nmsno.dendrogram(train, orientation=\"right\", method=\"centroid\", fontsize=12, ax=ax1);\n\nax2 = fig.add_subplot(1,2,2)\nmsno.dendrogram(train, orientation=\"top\", method=\"ward\", fontsize=12, ax=ax2);\n\nplt.tight_layout()","9e95c92c":"f, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(train.corr(), vmax=.8, square=True, annot=True, cmap='Blues');","985f134d":"# Transform discrete values to columns with 1 and 0s\ntrain_OHE = pd.get_dummies(train)\n\n# Do the same for competition data\ntest_OHE = pd.get_dummies(test)","0ce51a3d":"X1 = train_OHE.drop('target', axis=1)  # Features\ny1 = train_OHE['target']               # Labels","8bb03006":"# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2) # 80% training and 20% test","27b840b0":"# Create a Gaussian Classifier\nrf = RandomForestClassifier(n_estimators=100)\n\n# Train the model using the training sets y_pred=clf.predict(X_test)\nrf.fit(X_train, y_train)","8422605b":"y_pred = rf.predict(X_test)\ny_pred_test_rf = rf.predict(test_OHE)","70f6cae7":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))","0af7629d":"feature_imp1 = pd.Series(rf.feature_importances_,index=X1.columns).sort_values(ascending=False)\n\nplt.figure(figsize=(15,20))\n\n# Creating a bar plot\nsns.barplot(x=feature_imp1, y=feature_imp1.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","d708e28e":"lgbm_parameters = {\n    'reg_alpha': 0.00388218567052311,\n    'reg_lambda': 8.972335390951376e-05,\n    'colsample_bytree': 0.18375780999902297,\n    'subsample': 0.013352256062576087,\n    'learning_rate': 0.002597839272059483,\n    'max_depth': 44,\n    'num_leaves': 15,\n    'min_child_samples': 89,\n    'cat_smooth': 56, \n    'cat_l2': 22.375773634793603,\n    'max_bin': 33, \n    'min_data_per_group': 89\n}","f5124712":"lgbm_parameters['metric'] = 'binary_logloss'\nlgbm_parameters['objective'] = 'binary'\nlgbm_parameters['n_estimators'] = 15000","24e24072":"lgbm_model = LGBMClassifier(**lgbm_parameters)","9abc8611":"lgbm_model.fit(X_train, y_train)","7be5de49":"y_pred = lgbm_model.predict(X_test)\ny_pred_test_rf = lgbm_model.predict(test_OHE)","77a96778":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))","5732dd2b":"plt.rcParams[\"figure.figsize\"] = (12, 22)\nlightgbm.plot_importance(lgbm_model, max_num_features = 60, height=.9)","ab376cfa":"import shap","99e98f56":"from sklearn.datasets import load_boston\n\nfrom sklearn.inspection import permutation_importance\n\nfrom xgboost import XGBRegressor","6eb6135e":"boston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)","4f1e3c9b":"X.head(3)","ff7d4131":"xgb = XGBRegressor(n_estimators=100)\nxgb.fit(X_train, y_train)","1abd9eb0":"# Random Forest we would do the same to get importances\nprint(xgb.feature_importances_)","07516f90":"# plot\nplt.figure(figsize=(9,8))\nplt.bar(range(len(xgb.feature_importances_)), xgb.feature_importances_)\nplt.show()","0dce6c81":"plt.figure(figsize=(9,8))\nplt.barh(boston.feature_names, xgb.feature_importances_);","674e83e9":"# To have even better plot, let\u2019s sort the features based on importance value:\n\nplt.figure(figsize=(9,8))\nsorted_idx = xgb.feature_importances_.argsort()\nplt.barh(boston.feature_names[sorted_idx], xgb.feature_importances_[sorted_idx]);\nplt.xlabel(\"Xgboost Feature Importance\")\nplt.show()","daf4b123":"# This permutation method will randomly shuffle each feature and compute the change in the model\u2019s performance. The features which impact the performance the most are the most important one.\n\nperm_importance = permutation_importance(xgb, X_test, y_test)","0cd9e5a5":"plt.figure(figsize=(9,10))\nsorted_idx = perm_importance.importances_mean.argsort()\nplt.barh(boston.feature_names[sorted_idx], perm_importance.importances_mean[sorted_idx])\nplt.xlabel(\"Permutation Importance\")","63d5bf72":"def correlation_heatmap(train):\n    correlations = train.corr()\n\n    fig, ax = plt.subplots(figsize=(13,13))\n    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', cmap=\"YlGnBu\",\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70}\n                )\n    plt.show();\n    \ncorrelation_heatmap(X_train[boston.feature_names[sorted_idx]])","3356c5e0":"explainer = shap.TreeExplainer(xgb)\nshap_values = explainer.shap_values(X_test)","60f7ae0f":"# To visualize the feature importance we need to use summary_plot method:\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","2b67c475":"shap.summary_plot(shap_values, X_test)","b2c3bf70":"shap.dependence_plot(\"LSTAT\", shap_values, X_test)","8b11c280":"submission = pd.DataFrame({\"enrollee_id\": testoriginal[\"enrollee_id\"], \"target\": y_pred_test_rf})\nsubmission.to_csv('submission.csv', index=False)","0bcc057e":"accuracy = accuracy_score(y_test, predictions)\naccuracy","96341d30":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 2.1) Finding Important Features <\/h1>\n\n - Random forest uses **gini** importance or **mean decrease in impurity (MDI)** to calculate the importance of each feature.","20d800dd":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> Table of Contents <\/h1>\n\n#### 1) Load Required Libraries\n\n#### 2) iris\n\n>       2.1) Finding Important Features\n\n>       2.2) Generating the Model on Selected Features\n\n#### 3) HR Analytics\n\n>       3.1) Read Data\n\n>       3.2) EDA (Exploratory Data Analysis)\n\n>            3.2.1) Drop Unwanted Features\n\n>       3.3) Model building and Evaluation \n\n>            3.3.1) Random Forest\n\n>            3.3.2) LGBM\n\n#### 4) Boston\n\n>       a) Xgboost Built-in Feature Importance\n\n>       b) Permutation Based Feature Importance\n\n>       c) Feature Importance Computed with SHAP Values","561b9db6":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2.2.2) Visualize missing values as a barplot <\/h1>","7394fd06":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2.1) Drop Unwanted Features <\/h1>","f4b7cbef":"# make predictions for test set\ny_pred_xgb1 = xgb1.predict(X_test)\npredictions = [round(value) for value in y_pred_xgb1]","217d3be8":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2.2.1) Visualize missing values as a matrix <\/h1>","d3bdfd91":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left;\"> a) Xgboost Built-in Feature Importance <\/h1>","3435a782":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.4) enrolled_university <\/h1>","dcc0a5f4":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.7) company_size <\/h1>","7ea84fdf":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.3.1) Random Forest <\/h1>","4de8449b":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 3) HR Analytics <\/h1>","367bbd54":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.3.3) LGBM <\/h1>","c9ca9413":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2.2.4) Visualize missing values as a dendogram <\/h1>","0cb935c8":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.3) education_level <\/h1>","ec50f584":"- Random forests can also handle missing values. ","0f140c45":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2.2.3) Visualize missing values as a heatmap <\/h1>","c30bee44":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.3) Model building and Evaluation <\/h1>","ec0a88b8":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.2) gender <\/h1>","6c2c2ad6":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 4) Boston <\/h1>","6e034b29":"#### The permutation based importance is computationally expensive (for each feature there are several repeast of shuffling). The permutation based method can have problem with highly-correlated features. Let\u2019s check the correlation in our dataset:","d94241fc":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.5) relevent_experience <\/h1>","2c38aaf7":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:180%; text-align:left;\"> 4.1) Xgboost Feature Importance Computed in 3 Ways with Python <\/h1>\n\n#### a) Feature Importance built-in the Xgboost algorithm,\n\n#### b) Feature Importance computed with Permutation method,\n\n#### c) Feature Importance computed with SHAP values.","97440e6b":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left;\"> b) Permutation Based Feature Importance <\/h1>","480f5905":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 2.2) Generating the Model on Selected Features <\/h1>\n\n- Here, we can remove the **\"sepal width\"** feature because it has **very low importance,** and select the 3 remaining features.","3e8a039e":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.1) target <\/h1>","4353c223":"- Based on above results, I would say that it is **safe to remove: ZN, CHAS, AGE, INDUS.**\n\n- Their importance based on permutation is very low and they are **not highly correlated with other features (abs(corr) < 0.8).**","2efd84f7":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 2) iris <\/h1>","9ffeca82":"### About Xgboost Built-in Feature Importance\n\n- There are several types of importance in the Xgboost. It can be computed in several different ways. The **default** type is **gain** if you construct model with scikit-learn like API (docs). When you access Booster object and get the importance with **get_score** method, then default is **weight**. You can check the type of the importance with **xgb.importance_type.**\n\n- The **gain** type shows the average gain across all splits where feature was used.\n\n- The **weight** shows the number of times the feature is used to split data. This type of feature importance can favourize **numerical and high cardinality features**.\n\n- There are also **cover, total_gain, total_cover** types of importance.","b084ff6a":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:170%; text-align:left;\"> 1) Load Required Libraries <\/h1>","c1fca0e5":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2) EDA (Exploratory Data Analysis) <\/h1>","0bb0d519":"<h1 style=\"background-color:magenta; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1) Read Data <\/h1>","170532fb":"xgb1 = XGBClassifier()\nxgb1.fit(X_train, y_train)","bbaaedb6":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Submission <\/h1>","fe9fe46f":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.1.6) experience <\/h1>","4dd55e55":"|Feature                   |Description                                                |\n|--------------------------|-----------------------------------------------------------|\n|enrollee_id               |Unique ID for candidate                                    |\n|city                      |City code                                                  |\n|city_development_index    |Development index of the city (scaled)                     |\n|gender                    |Gender of candidate                                        |\n|relevent_experience       |Relevant experience of candidate                           |\n|enrolled_university       |Type of University course enrolled if any                  |\n|education_level           |Education level of candidate                               |\n|major_discipline          |Education major discipline of candidate                    |\n|experience                |Candidate total experience in years                        |             \n|company_size              |Number of employees in current employer's company          |\n|company_type              |Type of current employer                                   |\n|lastnewjob                |Difference in years between previous job and current job   |\n|training_hours            |Training hours completed                                   |\n|target                    |0 \u2013 Not looking for job change                             |\n|                          |1 \u2013 Looking for a job change                               |","130cf290":"<h1 style=\"background-color:orange; font-family:newtimeroman; font-size:180%; text-align:left;\"> c) Feature Importance Computed with SHAP Values <\/h1>\n\n- The third method to compute feature importance in Xgboost is to use SHAP package. It is model-agnostic and using the Shapley values from game theory to estimate the how does each feature contribute to the prediction.","b5f1013f":"- We can see that after **removing** the least important features (sepal length), the accuracy **increased.** This is because you removed misleading data and noise, resulting in an increased accuracy. A lesser amount of features also reduces the training time.","845ac4e4":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.2.2) Missing Values <\/h1>\n\n#### Visualize missing values (NaN) values using Missingno Library\n\na) Visualize missing values as a matrix\n\nb) Visualize missing values as a barplot\n\nc) Visualize missing values as a heatmap\n\nd) Visualize missing values as a dendrogram","290d1cbb":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:170%; text-align:left;\"> 3.3.2) XGBoost <\/h1>"}}