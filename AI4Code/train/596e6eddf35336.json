{"cell_type":{"11ccad6b":"code","782d92e0":"code","1687b62d":"code","8c795e16":"code","25d48343":"code","93893d6f":"code","398ef0b0":"code","bd1c81d8":"code","84935229":"code","5ac5d967":"code","60e7f814":"code","a7a8a85b":"code","65d79edd":"code","1f0200ac":"code","fbb0b226":"code","488c3abc":"markdown","95aaeaa2":"markdown","1862092c":"markdown","85e34419":"markdown","2e679c81":"markdown","f341bbf6":"markdown","e3790e77":"markdown","b715cc0a":"markdown","533feec2":"markdown","600931c7":"markdown","c7ac54fb":"markdown","33ecf3c9":"markdown","0b5fd17d":"markdown","249da9f5":"markdown","cde18b0f":"markdown","e9628689":"markdown"},"source":{"11ccad6b":"import os\nprint(os.listdir(\"..\/input\"))","782d92e0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport matplotlib.cbook as cbook\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, roc_auc_score, log_loss\nimport gc\nimport time\nfrom pandas.core.common import SettingWithCopyWarning\nimport warnings\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, GroupKFold\n\nwarnings.simplefilter('error', SettingWithCopyWarning)\ngc.enable()\n%matplotlib inline","1687b62d":"train = pd.read_csv('..\/input\/create-extracted-json-fields-dataset\/extracted_fields_train.gz', \n                    dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ntest = pd.read_csv('..\/input\/create-extracted-json-fields-dataset\/extracted_fields_test.gz', \n                   dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ntrain.shape, test.shape","8c795e16":"y_clf = (train['totals.transactionRevenue'].fillna(0) > 0).astype(np.uint8)\ny_reg = train['totals.transactionRevenue'].fillna(0)\ndel train['totals.transactionRevenue']\ny_clf.mean(), y_reg.mean()","25d48343":"for df in [train, test]:\n    df['date'] = pd.to_datetime(df['date'])\n    df['vis_date'] = pd.to_datetime(df['visitStartTime'])\n    df['sess_date_dow'] = df['vis_date'].dt.dayofweek\n    df['sess_date_hours'] = df['vis_date'].dt.hour\n    df['sess_date_dom'] = df['vis_date'].dt.day","93893d6f":"excluded_features = [\n    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n    'visitId', 'visitStartTime', 'non_zero_proba', 'vis_date'\n]\n\ncategorical_features = [\n    _f for _f in train.columns\n    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n]\n\nif 'totals.transactionRevenue' in train.columns:\n    del train['totals.transactionRevenue']\n\nif 'totals.transactionRevenue' in test.columns:\n    del test['totals.transactionRevenue']","398ef0b0":"for f in categorical_features:\n    train[f], indexer = pd.factorize(train[f])\n    test[f] = indexer.get_indexer(test[f])","bd1c81d8":"folds = GroupKFold(n_splits=5)\n\ntrain_features = [_f for _f in train.columns if _f not in excluded_features]\nprint(train_features)\noof_clf_preds = np.zeros(train.shape[0])\nsub_clf_preds = np.zeros(test.shape[0])\nfor fold_, (trn_, val_) in enumerate(folds.split(y_clf, y_clf, groups=train['fullVisitorId'])):\n    trn_x, trn_y = train[train_features].iloc[trn_], y_clf.iloc[trn_]\n    val_x, val_y = train[train_features].iloc[val_], y_clf.iloc[val_]\n    \n    clf = lgb.LGBMClassifier(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=1000,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    clf.fit(\n        trn_x, trn_y,\n        eval_set=[(val_x, val_y)],\n        early_stopping_rounds=50,\n        verbose=50\n    )\n    \n    oof_clf_preds[val_] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n    print(roc_auc_score(val_y, oof_clf_preds[val_]))\n    sub_clf_preds += clf.predict_proba(test[train_features], num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits\n    \nroc_auc_score(y_clf, oof_clf_preds)","84935229":"train['non_zero_proba'] = oof_clf_preds\ntest['non_zero_proba'] = sub_clf_preds","5ac5d967":"train_features = [_f for _f in train.columns if _f not in excluded_features] + ['non_zero_proba']\nprint(train_features)\n\noof_reg_preds = np.zeros(train.shape[0])\nsub_reg_preds = np.zeros(test.shape[0])\nimportances = pd.DataFrame()\n\nfor fold_, (trn_, val_) in enumerate(folds.split(y_reg, y_reg, groups=train['fullVisitorId'])):\n    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_].fillna(0)\n    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_].fillna(0)\n    \n    reg = lgb.LGBMRegressor(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=1000,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(val_x, np.log1p(val_y))],\n        early_stopping_rounds=50,\n        verbose=50\n    )\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    \n    imp_df['fold'] = fold_ + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_reg_preds += np.expm1(_preds) \/ folds.n_splits\n    \nmean_squared_error(np.log1p(y_reg.fillna(0)), oof_reg_preds) ** .5","60e7f814":"import warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nimportances['gain_log'] = np.log1p(importances['gain'])\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 12))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","a7a8a85b":"test['PredictedLogRevenue'] = sub_reg_preds\ntest[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum()['PredictedLogRevenue'].apply(np.log1p).reset_index()\\\n    .to_csv('test_clf_reg_log_of_sum.csv', index=False)","65d79edd":"# Go to actual revenues\ntrain['PredictedRevenue'] = np.expm1(oof_reg_preds)\ntest['PredictedRevenue'] = sub_reg_preds\ntrain['totals.transactionRevenue'] = y_reg\n\n# Sum by date on train and test\ntrn_group = train[['date', 'PredictedRevenue', 'totals.transactionRevenue']].groupby('date').sum().reset_index()\nsub_group = test[['date', 'PredictedRevenue']].groupby('date').sum().reset_index()\n\n# Now plot all this\nyears = mdates.YearLocator()   # every year\nmonths = mdates.MonthLocator()  # every month\nyearsFmt = mdates.DateFormatter('%Y-%m')\n\nfig, ax = plt.subplots(figsize=(15, 6))\nax.set_title('Actual Dollar Revenues - we are way off...', fontsize=15, fontweight='bold')\nax.plot(pd.to_datetime(trn_group['date']).values, trn_group['totals.transactionRevenue'].values)\nax.plot(pd.to_datetime(trn_group['date']).values, trn_group['PredictedRevenue'].values)\nax.plot(pd.to_datetime(sub_group['date']).values, sub_group['PredictedRevenue'].values)\n\n# # format the ticks\nax.xaxis.set_major_locator(months)\nax.xaxis.set_major_formatter(yearsFmt)\nax.xaxis.set_minor_locator(months)\n\nax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n# # ax.format_ydata = price\nax.grid(True)\n\n# rotates and right aligns the x labels, and moves the bottom of the\n# axes up to make room for them\nfig.autofmt_xdate()","1f0200ac":"# Go to actual revenues\ntrain['PredictedRevenue'] = np.expm1(oof_reg_preds)\ntest['PredictedRevenue'] = sub_reg_preds\ntrain['totals.transactionRevenue'] = y_reg\n\n# Sum by date on train and test\ntrn_group = train[['date', 'PredictedRevenue', 'totals.transactionRevenue']].groupby('date').sum().reset_index()\nsub_group = test[['date', 'PredictedRevenue']].groupby('date').sum().reset_index()\n\nyears = mdates.YearLocator()   # every year\nmonths = mdates.MonthLocator()  # every month\nyearsFmt = mdates.DateFormatter('%Y-%m')\n\nfig, ax = plt.subplots(figsize=(15, 6))\nax.set_title('We are also off in logs... or am I just stupid ?', fontsize=15, fontweight='bold')\nax.plot(pd.to_datetime(trn_group['date']).values, np.log1p(trn_group['totals.transactionRevenue'].values))\nax.plot(pd.to_datetime(trn_group['date']).values, np.log1p(trn_group['PredictedRevenue'].values))\nax.plot(pd.to_datetime(sub_group['date']).values, np.log1p(sub_group['PredictedRevenue'].values))\n\n# # format the ticks\nax.xaxis.set_major_locator(months)\nax.xaxis.set_major_formatter(yearsFmt)\nax.xaxis.set_minor_locator(months)\n\nax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n# # ax.format_ydata = price\nax.grid(True)\n\n# rotates and right aligns the x labels, and moves the bottom of the\n# axes up to make room for them\nfig.autofmt_xdate()","fbb0b226":"# Keep amounts in logs\ntrain['PredictedRevenue'] = oof_reg_preds\ntest['PredictedRevenue'] = np.log1p(sub_reg_preds)\ntrain['totals.transactionRevenue'] = np.log1p(y_reg)\n\n# You really mean summing up the logs ???\ntrn_group = train[['date', 'PredictedRevenue', 'totals.transactionRevenue']].groupby('date').sum().reset_index()\nsub_group = test[['date', 'PredictedRevenue']].groupby('date').sum().reset_index()\n\nyears = mdates.YearLocator()   # every year\nmonths = mdates.MonthLocator()  # every month\nyearsFmt = mdates.DateFormatter('%Y-%m')\n\nfig, ax = plt.subplots(figsize=(15, 6))\nax.set_title('Summing up logs looks a lot better !?! Is the challenge to find the correct metric ???', fontsize=15, fontweight='bold')\nax.plot(pd.to_datetime(trn_group['date']).values, trn_group['totals.transactionRevenue'].values)\nax.plot(pd.to_datetime(trn_group['date']).values, trn_group['PredictedRevenue'].values)\nax.plot(pd.to_datetime(sub_group['date']).values, sub_group['PredictedRevenue'].values)\n\n# # format the ticks\nax.xaxis.set_major_locator(months)\nax.xaxis.set_major_formatter(yearsFmt)\nax.xaxis.set_minor_locator(months)\n\nax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n# # ax.format_ydata = price\nax.grid(True)\n\n# rotates and right aligns the x labels, and moves the bottom of the\n# axes up to make room for them\nfig.autofmt_xdate()","488c3abc":"### Add classification to dataset","95aaeaa2":"### Get targets","1862092c":"### Classify non-zero revenues","85e34419":"### Get data","2e679c81":"### Factorize categoricals","f341bbf6":"### Display using np.log1p","e3790e77":"### Add date features","b715cc0a":"## Introduction\n\nI believe the main issue we have in this challenge is not to predict revenues but more to get these zeros right since less than 1.3 % of the sessions have a non-zero revenue.\n\nThe idea in this kernel is to classify non-zero transactions first and use that to help our regressor get better results.\n\nThe kernel only presents one way of doing it. No special feature engineering or set of hyperparameters, just a code shell\/structure ;-) ","533feec2":"### Save predictions\n\nMaybe one day Kaggle will support file compression for submissions from kernels...\n\nI'm aware I sum the logs instead of summing the actual revenues...","600931c7":"### Create list of features","c7ac54fb":"### Import packages","33ecf3c9":"### Using sum of logs - no really ?","0b5fd17d":"### Check file structure","249da9f5":"### Plot Actual Dollar estimates per dates","cde18b0f":"### Predict revenues at session level","e9628689":"The issue is that the model highly underestimates the log revenues. As a consequence, going back to actual dollar revenues underestimates even more transactions. Now if you sum up a high number of transactions (like we do to display things at date level) will get estimates really off. This is what the 1st plot demonstrates.\n\nThe 2nd plot shows the same thing but in log. Again this is due to the number of transactions we need to aggregate at date level.\n\nDue to the underestimation, summing up log revenues (i.e. in fact multiplying them) will get revenues on the same range as actual revenues. This is just a proof of our underestimation at session level.\n\nAs it's been said on the forum we don't have a lot of Visitors with lots of sessions and this is why Visitor RMSE and Session RMSE are on the same scale.\n\n"}}