{"cell_type":{"66f177eb":"code","a6567275":"code","b3aff848":"code","dc3be3d7":"code","a48e2374":"code","96d3b0c8":"code","ea72185d":"code","7b4cc74e":"code","3cabe8a9":"code","544b9415":"code","9e5bfe00":"code","ed77c43e":"code","79f6485f":"code","f5febb58":"code","639fcbe7":"code","883d9a52":"code","b315847a":"code","7c8aa767":"code","17a6d33a":"code","4390783c":"code","06838a3b":"code","3d719b19":"code","277f3b4b":"code","161ef48d":"code","ff180f9a":"code","da1eccaa":"code","71b74bdc":"code","d6bf2e9c":"code","368f07e1":"code","2c2e508e":"code","33b7c379":"code","5a433f2a":"code","ff6b4c2f":"code","99d67742":"code","25d7d2cd":"code","72c142e2":"code","1adfff18":"code","dab61356":"code","7433999a":"code","4d46376b":"code","59a2e695":"code","aebd4241":"code","cbcd6a24":"code","241e8f7d":"code","8dacd5f9":"code","fea3c113":"code","b229f9d6":"code","2ac26081":"code","2e30dd72":"code","fc9b0528":"code","8cbad47e":"code","40eca70b":"code","1202a498":"code","ff7cee2f":"code","2ec5b3ff":"code","361578c0":"code","75a5bda6":"code","3cf61e72":"code","5508d239":"code","e2967fa1":"code","2b6c0090":"code","d45d0d5e":"code","ffbfd946":"code","19393d31":"code","d04c2ffc":"code","70a81c5f":"code","75024502":"code","89523576":"code","c2aeb32a":"code","7910c8df":"code","2bd87b98":"code","b25d66f5":"code","94054e1b":"code","ce2b254b":"code","30a0cc96":"code","67419cf6":"code","4eb1ee2a":"code","66e16913":"code","7770a072":"code","2b9aeab0":"code","84b8dbc7":"code","39e25fc4":"code","6168fc44":"code","5f31a2db":"code","d71003ca":"code","c4fc9466":"code","34e4405b":"code","f254e89b":"code","768752aa":"code","19250ba4":"code","0870c704":"code","ea0a34ed":"code","87475796":"code","ae22d6f5":"code","e82c725a":"code","e2462b7c":"code","c1d76a75":"code","f8283bc6":"code","a655810b":"code","3f3a8446":"code","cc64af96":"code","412f9843":"code","b38192da":"code","1a65ef03":"code","699e8cf1":"code","ee0cb94e":"code","c4818be2":"code","c2fbc437":"code","d8800a0d":"code","9dd851b4":"code","28d5612a":"code","86e891cc":"code","33948202":"code","439d9ee3":"code","c4717c53":"code","2619a062":"code","8ff96313":"markdown","a16f79a3":"markdown","1259466c":"markdown","99bf0482":"markdown","c19ecb3d":"markdown","83d81b36":"markdown","beeffe7e":"markdown","bcdf32e1":"markdown","24240ffc":"markdown","9b325d43":"markdown","7a093704":"markdown","fb168c78":"markdown","97b18a4b":"markdown","4fa249f3":"markdown","e9eba2e4":"markdown","b75ec475":"markdown","12f37ca7":"markdown","c52c5f79":"markdown","84234f96":"markdown","8d41c935":"markdown","1345f25e":"markdown","85332928":"markdown","7e4d8c16":"markdown","88ef7900":"markdown","14ce07c7":"markdown","663fd1b9":"markdown","fdce1ecd":"markdown","c54f98ee":"markdown","0688fbb4":"markdown","19c23ba3":"markdown","252a3a76":"markdown","34197719":"markdown","912a5ed5":"markdown","827e3344":"markdown","34e98b2c":"markdown","aa634da4":"markdown","236cb0c3":"markdown","69952ebb":"markdown","a196320d":"markdown"},"source":{"66f177eb":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai.io import *\nfrom fastai.conv_learner import *\n\nfrom fastai.column_data import *","a6567275":"PATH='\/kaggle\/working\/nietzsche\/'","b3aff848":"get_data(\"https:\/\/s3.amazonaws.com\/text-datasets\/nietzsche.txt\", f'{PATH}nietzsche.txt')\ntext = open(f'{PATH}nietzsche.txt').read()\nprint('corpus length:', len(text))","dc3be3d7":"text[:400]","a48e2374":"chars = sorted(list(set(text)))\nvocab_size = len(chars)+1\nprint('total chars:', vocab_size)","96d3b0c8":"chars.insert(0, \"\\0\")\n\n''.join(chars[1:-6])","ea72185d":"char_indices = {c: i for i, c in enumerate(chars)}\nindices_char = {i: c for i, c in enumerate(chars)}","7b4cc74e":"idx = [char_indices[c] for c in text]\n\nidx[:10]","3cabe8a9":"''.join(indices_char[i] for i in idx[:70])","544b9415":"cs=3\nc1_dat = [idx[i]   for i in range(0, len(idx)-cs, cs)]\nc2_dat = [idx[i+1] for i in range(0, len(idx)-cs, cs)]\nc3_dat = [idx[i+2] for i in range(0, len(idx)-cs, cs)]\nc4_dat = [idx[i+3] for i in range(0, len(idx)-cs, cs)]","9e5bfe00":"x1 = np.stack(c1_dat)\nx2 = np.stack(c2_dat)\nx3 = np.stack(c3_dat)","ed77c43e":"y = np.stack(c4_dat)","79f6485f":"x1[:4], x2[:4], x3[:4]","f5febb58":"y[:4]","639fcbe7":"x1.shape, y.shape","883d9a52":"n_hidden = 256","b315847a":"n_fac = 42","7c8aa767":"class Char3Model(nn.Module):\n    def __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n\n        # The 'green arrow' from our diagram - the layer operation from input to hidden\n        self.l_in = nn.Linear(n_fac, n_hidden)\n\n        # The 'orange arrow' from our diagram - the layer operation from hidden to hidden\n        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n        \n        # The 'blue arrow' from our diagram - the layer operation from hidden to output\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, c1, c2, c3):\n        in1 = F.relu(self.l_in(self.e(c1)))\n        in2 = F.relu(self.l_in(self.e(c2)))\n        in3 = F.relu(self.l_in(self.e(c3)))\n        \n        h = V(torch.zeros(in1.size()).cuda())\n        h = F.tanh(self.l_hidden(h+in1))\n        h = F.tanh(self.l_hidden(h+in2))\n        h = F.tanh(self.l_hidden(h+in3))\n        \n        return F.log_softmax(self.l_out(h))","17a6d33a":"md = ColumnarModelData.from_arrays('.', [-1], np.stack([x1,x2,x3], axis=1), y, bs=512)","4390783c":"m = Char3Model(vocab_size, n_fac).cuda()","06838a3b":"it = iter(md.trn_dl)\n*xs,yt = next(it)\nt = m(*V(xs))","3d719b19":"opt = optim.Adam(m.parameters(), 1e-2)","277f3b4b":"fit(m, md, 1, opt, F.nll_loss)","161ef48d":"set_lrs(opt, 0.001)","ff180f9a":"fit(m, md, 1, opt, F.nll_loss)","da1eccaa":"def get_next(inp):\n    idxs = T(np.array([char_indices[c] for c in inp]))\n    p = m(*VV(idxs))\n    i = np.argmax(to_np(p))\n    return chars[i]","71b74bdc":"get_next('y. ')","d6bf2e9c":"get_next('ppl')","368f07e1":"get_next(' th')","2c2e508e":"get_next('and')","33b7c379":"cs=8","5a433f2a":"c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(len(idx)-cs)]","ff6b4c2f":"c_out_dat = [idx[j+cs] for j in range(len(idx)-cs)]","99d67742":"xs = np.stack(c_in_dat, axis=0)","25d7d2cd":"xs.shape","72c142e2":"y = np.stack(c_out_dat)","1adfff18":"xs[:cs,:cs]","dab61356":"y[:cs]","7433999a":"val_idx = get_cv_idxs(len(idx)-cs-1)","4d46376b":"md = ColumnarModelData.from_arrays('.', val_idx, xs, y, bs=512)","59a2e695":"class CharLoopModel(nn.Module):\n    # This is an RNN!\n    def __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.l_in = nn.Linear(n_fac, n_hidden)\n        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(bs, n_hidden).cuda())\n        for c in cs:\n            inp = F.relu(self.l_in(self.e(c)))\n            h = F.tanh(self.l_hidden(h+inp))\n        \n        return F.log_softmax(self.l_out(h), dim=-1)","aebd4241":"m = CharLoopModel(vocab_size, n_fac).cuda()\nopt = optim.Adam(m.parameters(), 1e-2)","cbcd6a24":"fit(m, md, 1, opt, F.nll_loss)","241e8f7d":"set_lrs(opt, 0.001)","8dacd5f9":"fit(m, md, 1, opt, F.nll_loss)","fea3c113":"class CharLoopConcatModel(nn.Module):\n    def __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.l_in = nn.Linear(n_fac+n_hidden, n_hidden)\n        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(bs, n_hidden).cuda())\n        for c in cs:\n            inp = torch.cat((h, self.e(c)), 1)\n            inp = F.relu(self.l_in(inp))\n            h = F.tanh(self.l_hidden(inp))\n        \n        return F.log_softmax(self.l_out(h), dim=-1)","b229f9d6":"m = CharLoopConcatModel(vocab_size, n_fac).cuda()\nopt = optim.Adam(m.parameters(), 1e-3)","2ac26081":"it = iter(md.trn_dl)\n*xs,yt = next(it)\nt = m(*V(xs))","2e30dd72":"fit(m, md, 1, opt, F.nll_loss)","fc9b0528":"set_lrs(opt, 1e-4)","8cbad47e":"fit(m, md, 1, opt, F.nll_loss)","40eca70b":"def get_next(inp):\n    idxs = T(np.array([char_indices[c] for c in inp]))\n    p = m(*VV(idxs))\n    i = np.argmax(to_np(p))\n    return chars[i]","1202a498":"get_next('for thos')","ff7cee2f":"get_next('part of ')","2ec5b3ff":"get_next('queens a')","361578c0":"class CharRnn(nn.Module):\n    def __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNN(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(1, bs, n_hidden))\n        inp = self.e(torch.stack(cs))\n        outp,h = self.rnn(inp, h)\n        \n        return F.log_softmax(self.l_out(outp[-1]), dim=-1)","75a5bda6":"# Pytorch RNN is broken in this setup due to this issue: https:\/\/github.com\/pytorch\/pytorch\/issues\/5667 \n# m = CharRnn(vocab_size, n_fac).cuda()\n# opt = optim.Adam(m.parameters(), 1e-3)","3cf61e72":"# it = iter(md.trn_dl)\n# *xs,yt = next(it)","5508d239":"# t = m.e(V(torch.stack(xs)))\n# t.size()","e2967fa1":"# ht = V(torch.zeros(1, 512,n_hidden))\n# outp, hn = m.rnn(t, ht)\n# outp.size(), hn.size()","2b6c0090":"# t = m(*V(xs)); t.size()","d45d0d5e":"# fit(m, md, 4, opt, F.nll_loss)","ffbfd946":"# set_lrs(opt, 1e-4)","19393d31":"# fit(m, md, 2, opt, F.nll_loss)","d04c2ffc":"# def get_next(inp):\n#     idxs = T(np.array([char_indices[c] for c in inp]))\n#     p = m(*VV(idxs))\n#     i = np.argmax(to_np(p))\n#     return chars[i]","70a81c5f":"# get_next('for thos')","75024502":"# def get_next_n(inp, n):\n#     res = inp\n#     for i in range(n):\n#         c = get_next(inp)\n#         res += c\n#         inp = inp[1:]+c\n#     return res","89523576":"# get_next_n('for thos', 40)","c2aeb32a":"c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(0, len(idx)-cs-1, cs)]","7910c8df":"c_out_dat = [[idx[i+j] for i in range(cs)] for j in range(1, len(idx)-cs, cs)]","2bd87b98":"xs = np.stack(c_in_dat)\nxs.shape","b25d66f5":"ys = np.stack(c_out_dat)\nys.shape","94054e1b":"xs[:cs,:cs]","ce2b254b":"ys[:cs,:cs]","30a0cc96":"val_idx = get_cv_idxs(len(xs)-cs-1)","67419cf6":"md = ColumnarModelData.from_arrays('.', val_idx, xs, ys, bs=512)","4eb1ee2a":"class CharSeqRnn(nn.Module):\n    def __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNN(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(1, bs, n_hidden))\n        inp = self.e(torch.stack(cs))\n        outp,h = self.rnn(inp, h)\n        return F.log_softmax(self.l_out(outp), dim=-1)","66e16913":"# m = CharSeqRnn(vocab_size, n_fac).cuda()\n# opt = optim.Adam(m.parameters(), 1e-3)","7770a072":"# it = iter(md.trn_dl)\n# *xst,yt = next(it)","2b9aeab0":"# def nll_loss_seq(inp, targ):\n#     sl,bs,nh = inp.size()\n#     targ = targ.transpose(0,1).contiguous().view(-1)\n#     return F.nll_loss(inp.view(-1,nh), targ)","84b8dbc7":"# fit(m, md, 4, opt, nll_loss_seq)","39e25fc4":"# set_lrs(opt, 1e-4)","6168fc44":"# fit(m, md, 1, opt, nll_loss_seq)","5f31a2db":"# m = CharSeqRnn(vocab_size, n_fac).cuda()\n# opt = optim.Adam(m.parameters(), 1e-2)","d71003ca":"# m.rnn.weight_hh_l0.data.copy_(torch.eye(n_hidden))","c4fc9466":"# fit(m, md, 4, opt, nll_loss_seq)","34e4405b":"# set_lrs(opt, 1e-3)","f254e89b":"# fit(m, md, 4, opt, nll_loss_seq)","768752aa":"from torchtext import vocab, data\n\nfrom fastai.nlp import *\nfrom fastai.lm_rnn import *\n\nPATH='\/kaggle\/working\/nietzsche\/'\n\nTRN_PATH = 'trn\/'\nVAL_PATH = 'val\/'\nTRN = f'{PATH}{TRN_PATH}'\nVAL = f'{PATH}{VAL_PATH}'\n\n# Note: The student needs to practice her shell skills and prepare her own dataset before proceeding:\n# - trn\/trn.txt (first 80% of nietzsche.txt)\n# - val\/val.txt (last 20% of nietzsche.txt)\n\n%ls {PATH}","19250ba4":"%ls {PATH}trn","0870c704":"TEXT = data.Field(lower=True, tokenize=list)\nbs=64; bptt=8; n_fac=42; n_hidden=256\n\nFILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\nmd = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n\nlen(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)","ea0a34ed":"class CharSeqStatefulRnn(nn.Module):\n    def __init__(self, vocab_size, n_fac, bs):\n        self.vocab_size = vocab_size\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNN(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        self.init_hidden(bs)\n        \n    def forward(self, cs):\n        bs = cs[0].size(0)\n        if self.h.size(1) != bs: self.init_hidden(bs)\n        outp,h = self.rnn(self.e(cs), self.h)\n        self.h = repackage_var(h)\n        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n    \n    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))","87475796":"# m = CharSeqStatefulRnn(md.nt, n_fac, 512).cuda()\n# opt = optim.Adam(m.parameters(), 1e-3)","ae22d6f5":"# fit(m, md, 4, opt, F.nll_loss)","e82c725a":"# set_lrs(opt, 1e-4)\n\n# fit(m, md, 4, opt, F.nll_loss)","e2462b7c":"# From the pytorch source\n\ndef RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n    return F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))","c1d76a75":"class CharSeqStatefulRnn2(nn.Module):\n    def __init__(self, vocab_size, n_fac, bs):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNNCell(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        self.init_hidden(bs)\n        \n    def forward(self, cs):\n        bs = cs[0].size(0)\n        if self.h.size(1) != bs: self.init_hidden(bs)\n        outp = []\n        o = self.h\n        for c in cs: \n            o = self.rnn(self.e(c), o)\n            outp.append(o)\n        outp = self.l_out(torch.stack(outp))\n        self.h = repackage_var(o)\n        return F.log_softmax(outp, dim=-1).view(-1, self.vocab_size)\n    \n    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))","f8283bc6":"m = CharSeqStatefulRnn2(md.nt, n_fac, 512).cuda()\nopt = optim.Adam(m.parameters(), 1e-3)","a655810b":"fit(m, md, 4, opt, F.nll_loss)","3f3a8446":"# class CharSeqStatefulGRU(nn.Module):\n#     def __init__(self, vocab_size, n_fac, bs):\n#         super().__init__()\n#         self.vocab_size = vocab_size\n#         self.e = nn.Embedding(vocab_size, n_fac)\n#         self.rnn = nn.GRU(n_fac, n_hidden)\n#         self.l_out = nn.Linear(n_hidden, vocab_size)\n#         self.init_hidden(bs)\n        \n#     def forward(self, cs):\n#         bs = cs[0].size(0)\n#         if self.h.size(1) != bs: self.init_hidden(bs)\n#         outp,h = self.rnn(self.e(cs), self.h)\n#         self.h = repackage_var(h)\n#         return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n    \n#     def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))","cc64af96":"# From the pytorch source code - for reference\n\ndef GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n    gi = F.linear(input, w_ih, b_ih)\n    gh = F.linear(hidden, w_hh, b_hh)\n    i_r, i_i, i_n = gi.chunk(3, 1)\n    h_r, h_i, h_n = gh.chunk(3, 1)\n\n    resetgate = F.sigmoid(i_r + h_r)\n    inputgate = F.sigmoid(i_i + h_i)\n    newgate = F.tanh(i_n + resetgate * h_n)\n    return newgate + inputgate * (hidden - newgate)","412f9843":"# m = CharSeqStatefulGRU(md.nt, n_fac, 512).cuda()\n\n# opt = optim.Adam(m.parameters(), 1e-3)","b38192da":"# fit(m, md, 6, opt, F.nll_loss)","1a65ef03":"# set_lrs(opt, 1e-4)","699e8cf1":"# fit(m, md, 3, opt, F.nll_loss)","ee0cb94e":"from fastai import sgdr\n\nn_hidden=512","c4818be2":"class CharSeqStatefulLSTM(nn.Module):\n    def __init__(self, vocab_size, n_fac, bs, nl):\n        super().__init__()\n        self.vocab_size,self.nl = vocab_size,nl\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        self.init_hidden(bs)\n        \n    def forward(self, cs):\n        bs = cs[0].size(0)\n        if self.h[0].size(1) != bs: self.init_hidden(bs)\n        outp,h = self.rnn(self.e(cs), self.h)\n        self.h = repackage_var(h)\n        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n    \n    def init_hidden(self, bs):\n        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n                  V(torch.zeros(self.nl, bs, n_hidden)))","c2fbc437":"m = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()\nlo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)","d8800a0d":"os.makedirs(f'{PATH}models', exist_ok=True)","9dd851b4":"fit(m, md, 2, lo.opt, F.nll_loss)","28d5612a":"on_end = lambda sched, cycle: save_model(m, f'{PATH}models\/cyc_{cycle}')\ncb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\nfit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)","86e891cc":"on_end = lambda sched, cycle: save_model(m, f'{PATH}models\/cyc_{cycle}')\ncb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\nfit(m, md, 2**6-1, lo.opt, F.nll_loss, callbacks=cb)","33948202":"def get_next(inp):\n    idxs = TEXT.numericalize(inp)\n    p = m(VV(idxs.transpose(0,1)))\n    r = torch.multinomial(p[-1].exp(), 1)\n    return TEXT.vocab.itos[to_np(r)[0]]","439d9ee3":"get_next('for thos')","c4717c53":"def get_next_n(inp, n):\n    res = inp\n    for i in range(n):\n        c = get_next(inp)\n        res += c\n        inp = inp[1:]+c\n    return res","2619a062":"print(get_next_n('for thos', 400))","8ff96313":"### Create and train model","a16f79a3":"Our inputs","1259466c":"...and this is the next character after each sequence.","99bf0482":"For each of 0 through 7, create a list of every 8th character with that starting point. These will be the 8 inputs to our model.","c19ecb3d":"### Create inputs","83d81b36":"### Identity init!","beeffe7e":"### Create and train model","bcdf32e1":"### Test model","24240ffc":"## Setup","9b325d43":"This is the size of our unrolled RNN.","7a093704":"### RNN loop","fb168c78":"Then create a list of the next character in each of these series. This will be the labels for our model.","97b18a4b":"Then create the exact same thing, offset by 1, as our labels","4fa249f3":"Let's take non-overlapping sets of characters this time","e9eba2e4":"So each column below is one series of 8 characters from the text.","b75ec475":"Sometimes it's useful to have a zero value in the dataset, e.g. for padding","12f37ca7":"### Create inputs","c52c5f79":"## Stateful model","84234f96":"Map from chars to indices and back again","8d41c935":"The first 4 inputs and outputs","1345f25e":"## Three char model","85332928":"The number of latent factors to create (i.e. the size of the embedding matrix)","7e4d8c16":"### Setup","88ef7900":"### RNN","14ce07c7":"## Our first RNN!","663fd1b9":"### GRU","fdce1ecd":"Our output","c54f98ee":"### Create and train model","0688fbb4":"### Test model","19c23ba3":"### Putting it all together: LSTM","252a3a76":"### Setup","34197719":"### Test","912a5ed5":"Create a list of every 4th character, starting at the 0th, 1st, 2nd, then 3rd characters","827e3344":"## RNN with pytorch","34e98b2c":"We're going to download the collected works of Nietzsche to use as our data for this class.","aa634da4":"## Multi-output model","236cb0c3":"### Test model","69952ebb":"*idx* will be the data we use from now on - it simply converts all the characters to their index (based on the mapping above)","a196320d":"Pick a size for our hidden state"}}