{"cell_type":{"eefd987f":"code","5ff7c728":"code","88669eb6":"code","ea1295f5":"code","8a075a7d":"code","bb39f82f":"code","af1c3661":"code","dab12479":"code","d44fae9f":"code","8c21e78f":"code","a95089ea":"code","c678e9d0":"code","90190cef":"code","26df1705":"code","7aa1a4de":"code","2f3172ce":"code","3e5e710b":"code","e426408c":"code","85f41208":"code","943dead3":"code","7c7d3793":"code","8f9e3f48":"markdown","ad27564d":"markdown","45dcba67":"markdown","c095c877":"markdown","f0d210e9":"markdown","1f1aab26":"markdown"},"source":{"eefd987f":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport time\nfrom pathlib import Path\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import train_test_split\n\nimport xgboost as xgb\nfrom xgboost import plot_importance\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor as cbr\n\n%matplotlib inline","5ff7c728":"dataset_path = r'\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv'\n\ndf = pd.read_csv(dataset_path, parse_dates=['date'])\ndf.head()","88669eb6":"#calculating CPM\n#calculating the value that the Advertisers Bid for the month of June\n# CPM(the value which was the winning bid value) = \n#((revenue of the publisher*100)\/revenue_share_percentage)\/measurable_impressions)*1000\n\ndef weird_division(n, d):\n    return n \/ d if d else 0\n\ndf['CPM'] = df.apply(lambda x: weird_division(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , axis=1)\ndf.drop('total_revenue', axis=1, inplace=True)","ea1295f5":"df.head()","8a075a7d":"def multi_collinearity_heatmap(df, figsize=(11,9)):\n    \n    \"\"\"\n    Creates a heatmap of correlations between features in the df. \n    A figure size can optionally be set.\n    \"\"\"\n    \n    # Set the style of the visualization\n    sns.set(style=\"white\")\n\n    # Create a covariance matrix\n    corr = df.corr()\n\n    # Generate a mask the size of our covariance matrix\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=figsize)\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(\n        corr, mask=mask, cmap=cmap, center=0, square=True, \n        linewidths=.5, cbar_kws={\"shrink\": .5}, \n        vmax=corr[corr != 1.0].max().max()\n    );","bb39f82f":"cols_num = []\n\nfor col in df.columns:\n    if df[col].dtype == 'int64' or df[col].dtype == 'float64':\n        cols_num.append(col)\n","af1c3661":"for col in cols_num:\n    sns.violinplot(x=df[col], title=col)\n    plt.show()","dab12479":"cols_to_drop = ['integration_type_id', 'ad_type_id', 'revenue_share_percent']\ndf.drop(cols_to_drop, axis=1, inplace=True) #.reset_index(drop=True)","d44fae9f":"cols_to_log = ['total_impressions', 'measurable_impressions', 'viewable_impressions']\nfor col in cols_to_log:\n    df[col] = df[col].astype('float64').replace(0.0, 0.01)\n    df[col] = np.log(df[col])","8c21e78f":"multi_collinearity_heatmap(df, figsize=(10, 10))","a95089ea":"cols_to_drop = ['measurable_impressions', 'viewable_impressions']\ndf.drop(cols_to_drop, axis=1, inplace=True) #.reset_index(drop=True)","c678e9d0":"def x_y_split(X, target='CPM'):\n    return X.drop(target, axis=1), X[target]\n\nAFTER_DATE = '2019-06-22'\n\ndf = df.loc[lambda x: x.CPM >= 0]\ndf_train = df.loc[lambda x: x.date < AFTER_DATE].drop('date', axis=1)\ndf_train = df_train.loc[lambda x: x.CPM < x.CPM.quantile(.95)]\n\ndf_test = df.loc[lambda x: x.date >= AFTER_DATE].drop('date', axis=1)\ndf_test = df_test.loc[lambda x: (x.CPM < x.CPM.quantile(.95))] \n\nX_train_all, y_train_all = x_y_split(df_train)\nX_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.1, random_state=242)\n\nX_test, y_test = x_y_split(df_test)\n","90190cef":"log_target = False","26df1705":"y_train.hist()","7aa1a4de":"def do_prediction(model, X_train, y_train, X_val, y_val, validate=True, print_time=True, log_target=False):\n    _start = time.time()    \n\n    if log_target:\n        model.fit(X_train, np.log(y_train))\n        y_train_pred = np.exp(model.predict(X_train))\n    else:\n        model.fit(X_train, y_train)\n        y_train_pred = model.predict(X_train)\n\n    if validate:\n        if log_target:\n            y_val_pred = np.exp(model.predict(X_val))\n        else:\n            y_val_pred = model.predict(X_val)\n        \n    \n        \n    _end = time.time()\n    if print_time:\n        print(f\"Time taken to run: {round((_end - _start)\/60,1)} minutes\")\n\n    mse_train = mse(y_train, y_train_pred)\n    if validate:\n        mse_val = mse(y_val, y_val_pred)\n        print(f\"MSE train: {mse_train:.4f}\\tMSE val: {mse_val}\\tdelta: {abs(mse_train - mse_val):.4f}\")\n    else:    \n        print(f\"MSE train: {mse_train:.4f}\")\n    \n    return y_train_pred, y_val_pred","2f3172ce":"def show_pred_vs_true(y_train, y_train_pred, y_val, y_val_pred):\n    # Scatterplot of predicted vs. actual values\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    fig.suptitle('Predicted vs. actual values', fontsize=14, y=1)\n    plt.subplots_adjust(top=0.93, wspace=0)\n\n    ax1.scatter(y_val, y_val_pred, s=2, alpha=0.7)\n    ax1.plot(list(range(0, int(min(y_val.max(), y_val_pred.max())))),\n             list(range(0, int(min(y_val.max(), y_val_pred.max())))), color='black', linestyle='--')\n    ax1.set_title('Test set')\n    ax1.set_xlabel('Actual values')\n    ax1.set_ylabel('Predicted values')\n\n    ax2.scatter(y_train, y_train_pred, s=2, alpha=0.7)\n    ax2.plot(list(range(0, int(min(y_train.max(), y_train_pred.max())))),\n             list(range(0, int(min(y_train.max(), y_train_pred.max())))), color='black', linestyle='--')\n    ax2.set_title('Train set')\n    ax2.set_xlabel('Actual values')\n    ax2.set_ylabel('')\n    ax2.set_yticklabels(labels='')\n\n    plt.show()","3e5e710b":"model_lgb = lgb.LGBMRegressor(num_leaves=41, n_estimators=200)\n# num_leaves=41, n_estimators=100\n\ny_train_pred, y_val_pred = do_prediction(model_lgb, X_train, y_train, X_val, y_val, log_target=log_target)\nshow_pred_vs_true(y_train, y_train_pred, y_val, y_val_pred)","e426408c":"model_xgb = xgb.XGBRegressor(objective='reg:squarederror')\n\ny_train_pred, y_val_pred = do_prediction(model_xgb, X_train, y_train, X_val, y_val, log_target=log_target)\nshow_pred_vs_true(y_train, y_train_pred, y_val, y_val_pred)","85f41208":"model_cbr = cbr(random_seed=242, verbose=0, early_stopping_rounds=10)\ny_train_pred, y_val_pred = do_prediction(model_cbr, X_train, y_train, X_val, y_val, log_target=log_target)\nshow_pred_vs_true(y_train, y_train_pred, y_val, y_val_pred)","943dead3":"best_model = model_cbr\nbest_model.fit(X_train_all, y_train_all)","7c7d3793":"y_pred = best_model.predict(X_test)\nmse(y_test, y_pred)","8f9e3f48":"# Load Data","ad27564d":"# EDA","45dcba67":"# Calculate on test","c095c877":"# Choose and train best model","f0d210e9":"# Model","1f1aab26":"# Split for train\/val\/test data"}}