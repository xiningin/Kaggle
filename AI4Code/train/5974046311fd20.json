{"cell_type":{"5403cd86":"code","de8f57d2":"code","19741e1b":"code","a4ae0e09":"code","d1ebe6e3":"code","2eb524d7":"code","4ca55d6c":"code","f5026ac8":"code","88c9d986":"code","22f04dd3":"code","be131419":"code","84f74f12":"code","c3dfbddf":"code","8d95ae2a":"code","77a35176":"code","e0842c8d":"code","c4e6b2a4":"code","8cbedf59":"code","87ab9460":"markdown","c72222fa":"markdown","eb64aa69":"markdown"},"source":{"5403cd86":"!pip install --upgrade pip\n!pip install numpyro==0.6.0\n!pip install --upgrade jax==0.2.10 jaxlib==0.1.62+cuda110 -f https:\/\/storage.googleapis.com\/jax-releases\/jax_releases.html","de8f57d2":"from jax.lib import xla_bridge\nprint(xla_bridge.get_backend().platform)","19741e1b":"import time\nimport pandas as pd\nimport numpy as onp\n\nimport jax.numpy as np\nfrom jax import random\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.examples.datasets import COVTYPE, load_dataset\nfrom numpyro.infer import HMC, MCMC, NUTS\n\nfrom jax.random import PRNGKey\n\nnumpyro.util.set_platform('gpu') #Use GPU for MCMC","a4ae0e09":"import pandas as pd\nbaseball = pd.read_csv('..\/input\/mlb-pitch-data-20152018\/atbats.csv')\nd = baseball","d1ebe6e3":"d = d[(d.ab_id > 2018000000) & (d.ab_id < 2019000000)]\nlen(d)","2eb524d7":"# d = d[:200]","4ca55d6c":"hit_event = ['Single','Double','Triple','Home Run']\nd['Hits'] = d.event.isin(hit_event).astype(int)\nd['pitcher_code'] = d['pitcher_id'].astype('category').cat.codes\nd['batter_code'] = d['batter_id'].astype('category').cat.codes","f5026ac8":"numpyro.set_host_device_count(2)\n\ndat_list = {\"hits\": np.array(d.Hits),\n            \"pitcher\": np.array(d.pitcher_code),\n            \"batter\": np.array(d.batter_code)}\n\ndef model(pitcher, batter, hits=None, link=False):\n    a_bar = numpyro.sample(\"a_bar\", dist.Normal(0, 10))\n    sigma_a = numpyro.sample(\"sigma_a\", dist.HalfCauchy(5))\n    b_bar = numpyro.sample(\"b_bar\", dist.Normal(0, 10))\n    sigma_b = numpyro.sample(\"sigma_b\", dist.HalfCauchy(5))\n\n\n    a = numpyro.sample(\"a\", dist.Normal(a_bar, sigma_a), sample_shape=(len(d['pitcher_code'].unique()),))\n    b = numpyro.sample(\"b\", dist.Normal(b_bar, sigma_b), sample_shape=(len(d['batter_code'].unique()),))\n\n    # non-centered paramaterization\n#     a = numpyro.sample('a',  dist.TransformedDistribution(dist.Normal(0., 1.), dist.transforms.AffineTransform(a_bar, sigma_a)), sample_shape=(len(d['pitcher_code'].unique()),))\n#     b = numpyro.sample('b',  dist.TransformedDistribution(dist.Normal(0., 1.), dist.transforms.AffineTransform(b_bar, sigma_b)), sample_shape=(len(d['batter_code'].unique()),))\n\n    logit_p = a[pitcher] + b[batter]\n    if link:\n        p = expit(logit_p)\n        numpyro.sample(\"p\", dist.Delta(p), obs=p)\n    numpyro.sample(\"hits\", dist.Binomial(logits=logit_p), obs=hits)\n\nmcmc = MCMC(NUTS(model), 1000, 1000, num_chains=1)\nmcmc.run(PRNGKey(0), np.array(d.pitcher_code), np.array(d.batter_code), hits=np.array(d.Hits), extra_fields=('potential_energy','mean_accept_prob',))\nmcmc.print_summary(0.89)","88c9d986":"np.mean( mcmc.get_extra_fields()['mean_accept_prob'])","22f04dd3":"import pymc3 as pm\nfrom pymc3 import sample, Normal, HalfCauchy, Uniform\nimport numpy as np\nimport pymc3.sampling_jax","be131419":"dat_list = {\"hits\": np.array(d.Hits),\n            \"pitcher\": np.array(d.pitcher_code),\n            \"batter\": np.array(d.batter_code)}\nwith pm.Model() as model:\n\n    # Priors\n    mu_a = Normal('mu_a', mu=0., tau=0.01)\n    sigma_a = HalfCauchy('sigma_a', 5)\n    mu_b = Normal('mu_b', mu=0., tau=0.01)\n    sigma_b = HalfCauchy('sigma_b', 5)\n\n\n    a = Normal('a', mu=mu_a, sigma=sigma_a, shape=len(d['pitcher_code'].unique()))\n\n    b = Normal('b', mu=mu_b, sigma=sigma_b, shape=len(d['batter_code'].unique()))\n\n    # Expected value\n    logit_p = a[dat_list['pitcher']] + b[dat_list['batter']]\n\n    # Data likelihood\n    p = pm.Bernoulli('y', logit_p=logit_p, observed=dat_list['hits'])","84f74f12":"%%time\n\nwith model:\n    hierarchical_trace_jax = pm.sampling_jax.sample_numpyro_nuts(\n        50_000, target_accept=0.9, chains=1\n    )","c3dfbddf":"!pip install git+https:\/\/github.com\/blackjax-devs\/blackjax","8d95ae2a":"import jax\nimport numpy as np\nimport pymc3 as pm\nimport pymc3.sampling_jax\n\nimport blackjax.nuts as nuts\nimport blackjax.stan_warmup as stan_warmup","77a35176":"from theano.graph.fg import FunctionGraph\nfrom theano.link.jax.jax_dispatch import jax_funcify\n\nseed = jax.random.PRNGKey(1234)\nchains = 1\n\n# Get the FunctionGraph of the model.\nfgraph = FunctionGraph(model.free_RVs, [model.logpt])\n\n# Jax funcify builds Jax variant of the FunctionGraph.\nfns = jax_funcify(fgraph)\nlogp_fn_jax = fns[0]\n\n# Now we build a Jax variant of the initial state\/inputs to the model.\nrv_names = [rv.name for rv in model.free_RVs]\ninit_state = [model.test_point[rv_name] for rv_name in rv_names]\ninit_state_batched = jax.tree_map(\n    lambda x: np.repeat(x[None, ...], chains, axis=0), init_state\n)","e0842c8d":"# Then we transform the Jaxified input and FunctionGraph to a BlackJax NUTS sampler\npotential = lambda x: -logp_fn_jax(*x)\ninitial_position = init_state\ninitial_state = nuts.new_state(initial_position, potential)","c4e6b2a4":"%%time\n\nkernel_factory = lambda step_size, inverse_mass_matrix: nuts.kernel(\n    potential, step_size, inverse_mass_matrix\n)\n\nlast_state, (step_size, inverse_mass_matrix), _ = stan_warmup.run(\n    seed, kernel_factory, initial_state, 1000\n)\n\n\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    def one_step(state, rng_key):\n        state, info = kernel(rng_key, state)\n        return state, (state, info)\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n\n    return states, infos\n\n\n# Build the kernel using the step size and inverse mass matrix returned from the window adaptation\nkernel = kernel_factory(step_size, inverse_mass_matrix)\n\n# Sample from the posterior distribution\nstates, infos = inference_loop(seed, kernel, last_state, 50_000)","8cbedf59":"# with model:\n#     trace = sample(1000, tune=1000)","87ab9460":"The data here consist of all at bats of MLB game, specifically we would like to model the batting average of players in 2018 season. Baseball consist of one batter and one pitcher on each at bat, and we hope to find how they affact the batting average by grouping batter and pitcher played in the season.","c72222fa":"For computation speed of calculation, using kaggle hardware numpyro with GPU use about 80 seconds on calculation, which is about 10x speedup for numpyro with CPU and another 2x speedup for 2 chains MCMC compare to pymc3 (Codes below). However, the r_hat (Gelman Rubin diagnostic) in calculation is generally greater than 1, means that the chain has not fully converged. It can be solved by [non-centered paramaterization](https:\/\/mc-stan.org\/docs\/2_21\/stan-users-guide\/reparameterization-section.html) (codes commented out), but as noted in the stan users guide, it is not possible here due to 180k+ sample size. \n\nAnother possible solution is use large number of chains [noted by Michael Betancourt's talk](https:\/\/youtu.be\/DJ0c7Bm5Djk?t=19129). Numpyro currently do not vectorized accross chains unlike [PyMC4](https:\/\/github.com\/pymc-devs\/pymc4) so fitting this type of model is still hopeless, but as R\u00e9mi Louf point out, having the ability to quickly sample multiple chains could be a breakthrough on MCMC modelling.","eb64aa69":"[Numpyro](https:\/\/github.com\/pyro-ppl\/numpyro) is a probabilistic programming language with Numpy backend powered by JAX for autograd and JIT compilation to GPU\/TPU\/CPU. Inspired by [R\u00e9mi Louf's blogpost](https:\/\/rlouf.github.io\/post\/jax-random-walk-metropolis\/), here is a speed benchmark for numpyro (GPU\/CPU) vs PyMC3 MCMC on medium size real life dataset."}}