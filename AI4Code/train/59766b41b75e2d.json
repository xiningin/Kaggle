{"cell_type":{"2670c06e":"code","33ee56f3":"code","755c0095":"code","81fc30c4":"code","582c3879":"code","686aca7b":"code","b2c914fc":"code","1b2df728":"code","e08b5291":"code","d9dc0e37":"code","83817d00":"code","089e6a69":"code","d806c666":"code","e3b4f761":"code","3985a423":"code","1d3de4e2":"code","379a266b":"code","5cb19684":"code","a0469fce":"code","625791de":"code","c81d4b0e":"code","1990c232":"code","f9cd5ccc":"code","6ab7c516":"code","e090a245":"code","6e16a376":"code","b73edaad":"code","dafef4ae":"code","6f2949e6":"code","311a1da4":"code","d532a840":"code","7de0ddd5":"code","85494e9e":"code","538a3604":"code","72852305":"code","9b676500":"code","576d9d51":"code","2326e167":"code","9718d9ba":"code","f24ce016":"code","96b0ae91":"code","f9952750":"code","16a1c7b3":"code","6925ef24":"code","ba02a608":"code","d1c3f815":"code","f89e16fd":"code","2a6b657e":"markdown","8c9bccb8":"markdown","7a350ffd":"markdown","6e7b90c9":"markdown","ff372fc3":"markdown","2eb517fb":"markdown","05686df2":"markdown","652b2b64":"markdown","5404bb8d":"markdown","20d7b464":"markdown","d7951a05":"markdown","32ea7b61":"markdown","406f7cbd":"markdown","20ad204b":"markdown","783d8f85":"markdown","f975047f":"markdown","66d4b232":"markdown","71cb523c":"markdown","83b60e75":"markdown","fca7b456":"markdown","841974a9":"markdown","df94445c":"markdown","ee869a59":"markdown","6dd7f301":"markdown","5cab7412":"markdown","fe8bb29a":"markdown","e65fd7eb":"markdown","47437fed":"markdown","536a5fa4":"markdown","a053e9df":"markdown","1c6bdf1e":"markdown","c5f4972e":"markdown"},"source":{"2670c06e":"from keras.utils import np_utils \nfrom keras.datasets import mnist \nimport seaborn as sns\nfrom keras.initializers import RandomNormal\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time","33ee56f3":"# https:\/\/gist.github.com\/greydanus\/f6eee59eaf1d90fcb3b534a25362cea4\n# https:\/\/stackoverflow.com\/a\/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()","755c0095":"# the data, shuffled and split between train and test sets \n(X_train, y_train), (X_test, y_test) = mnist.load_data()","81fc30c4":"type(X_train)","582c3879":"plt.imshow(X_train[218])","686aca7b":"print(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d, %d)\"%(X_train.shape[1], X_train.shape[2]))\nprint(\"Number of training examples :\", X_test.shape[0], \"and each image is of shape (%d, %d)\"%(X_test.shape[1], X_test.shape[2]))","b2c914fc":"# if you observe the input shape its 2 dimensional vector\n# for each image we have a (28*28) vector\n# we will convert the (28*28) vector into single dimensional vector of 1 * 784 \n\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]) \nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]) ","1b2df728":"# after converting the input images from 3d to 2d vectors\n\nprint(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d)\"%(X_train.shape[1]))\nprint(\"Number of training examples :\", X_test.shape[0], \"and each image is of shape (%d)\"%(X_test.shape[1]))","e08b5291":"# An example data point\nprint(X_train[0])","d9dc0e37":"# if we observe the above matrix each cell is having a value between 0-255\n# before we move to apply machine learning algorithms lets try to normalize the data\n# X => (X - Xmin)\/(Xmax-Xmin) = X\/255\n\nX_train = X_train\/255\nX_test = X_test\/255","83817d00":"# example data point after normlizing\nprint(X_train[0])","089e6a69":"# here we are having a class number for each image\nprint(\"Class label of first image :\", y_train[0])\n\n# lets convert this into a 10 dimensional vector\n# ex: consider an image is 5 convert it into 5 => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n# this conversion needed for MLPs \n\nY_train = np_utils.to_categorical(y_train, 10) \nY_test = np_utils.to_categorical(y_test, 10)\n\nprint(\"After converting the output into a vector : \",Y_train[0])","d806c666":"from keras.models import Sequential \nfrom keras.layers import Dense, Activation ","e3b4f761":"# some model parameters\n\noutput_dim = 10\ninput_dim = X_train.shape[1]\n\nbatch_size = 128 \nnb_epoch = 20","3985a423":"# start building a model\nmodel = Sequential()\nmodel.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))","1d3de4e2":"model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])","379a266b":"history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = nb_epoch, verbose = 1, validation_data = (X_test, Y_test)) ","5cb19684":"print(history.history.keys())","a0469fce":"score = model.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])","625791de":"fig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","c81d4b0e":"# Multilayer perceptron\n\nmodel_sigmoid = Sequential()\nmodel_sigmoid.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\nmodel_sigmoid.add(Dense(128, activation='sigmoid'))\nmodel_sigmoid.add(Dense(output_dim, activation='softmax'))\n\nmodel_sigmoid.summary()","1990c232":"model_sigmoid.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_sigmoid.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","f9cd5ccc":"score = model_sigmoid.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","6ab7c516":"w_after = model_sigmoid.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","e090a245":"model_sigmoid = Sequential()\nmodel_sigmoid.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\nmodel_sigmoid.add(Dense(128, activation='sigmoid'))\nmodel_sigmoid.add(Dense(output_dim, activation='softmax'))\n\nmodel_sigmoid.summary()","6e16a376":"model_sigmoid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_sigmoid.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","b73edaad":"score = model_sigmoid.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","dafef4ae":"w_after = model_sigmoid.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","6f2949e6":"model_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\n\nmodel_relu.summary()","311a1da4":"model_relu.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","d532a840":"score = model_relu.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","7de0ddd5":"w_after = model_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","85494e9e":"model_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\n\nprint(model_relu.summary())","538a3604":"model_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","72852305":"score = model_relu.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","9b676500":"w_after = model_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","576d9d51":"from keras.layers.normalization import BatchNormalization\n\nmodel_batch = Sequential()\n\nmodel_batch.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_batch.add(BatchNormalization())\n\nmodel_batch.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_batch.add(BatchNormalization())\n\nmodel_batch.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_batch.summary()","2326e167":"model_batch.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_batch.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","9718d9ba":"score = model_batch.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","f24ce016":"w_after = model_batch.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","96b0ae91":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_drop = Sequential()\n\nmodel_drop.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\n\nmodel_drop.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\n\nmodel_drop.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_drop.summary()","f9952750":"model_drop.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","16a1c7b3":"score = model_drop.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","6925ef24":"w_after = model_drop.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","ba02a608":"from keras.optimizers import Adam,RMSprop,SGD\ndef best_hyperparameters(activ):\n\n    model = Sequential()\n    model.add(Dense(512, activation=activ, input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\n    model.add(Dense(128, activation=activ, kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\n    model.add(Dense(output_dim, activation='softmax'))\n\n\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n    \n    return model","d1c3f815":"# https:\/\/machinelearningmastery.com\/grid-search-hyperparameters-deep-learning-models-python-keras\/\n\nactiv = ['sigmoid','relu']\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = KerasClassifier(build_fn=best_hyperparameters, epochs=nb_epoch, batch_size=batch_size, verbose=0)\nparam_grid = dict(activ=activ)\n\n# if you are using CPU\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n# if you are using GPU dont use the n_jobs parameter\n\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid_result = grid.fit(X_train, Y_train)","f89e16fd":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","2a6b657e":"Lets see whats in the output of the fit function","8c9bccb8":"We are changing the optimizer to `ADAM` which has a better performance than the `stocastic gradient descent`","7a350ffd":"- To evaluate the model's performance call the model.evaluate method","6e7b90c9":"-  Before training a model, you need to configure the learning process, which is done via the compile method\n\n- It receives three arguments:\n- [An optimizer](https:\/\/keras.io\/optimizers\/). This could be the string identifier of an existing optimizer \n- [A loss function](https:\/\/keras.io\/losses\/). This is the objective that the model will try to minimize.\n- [A list of metrics]( https:\/\/keras.io\/metrics\/). For any classification problem you will want to set this to metrics=['accuracy']. \n\n\n- Note: when using the categorical_crossentropy loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except for a 1 at the index corresponding to the class of the sample). That is why we converted out labels into vectors","ff372fc3":"We can see a difference in the distribution of the weights in the second and third layer","2eb517fb":"Now we are applying [Dropout](https:\/\/keras.io\/layers\/core\/#dropout). Dropout is a technique for avoiding overfit. Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.","05686df2":"We are adding normalization to the end layers to improve performace. From the above plots we can see that the variance of the last layer is more. This can be reduced using [batch notmalization](https:\/\/keras.io\/layers\/normalization\/#batchnormalization)\n\nhttps:\/\/intoli.com\/blog\/neural-network-initialization\/ \nIf we sample weights from a normal distribution N(0,\u03c3) we satisfy this condition with \u03c3=\u221a(2\/(ni+ni+1). \nh1 =>  \u03c3=\u221a(2\/(ni+ni+1) = 0.039  => N(0,\u03c3) = N(0,0.039)\nh2 =>  \u03c3=\u221a(2\/(ni+ni+1) = 0.055  => N(0,\u03c3) = N(0,0.055)\nh1 =>  \u03c3=\u221a(2\/(ni+ni+1) = 0.120  => N(0,\u03c3) = N(0,0.120)","652b2b64":"<h2>MLP + Sigmoid activation + ADAM <\/h2>","5404bb8d":"## Importing Libraries","20d7b464":"We can see that the distribution of the weights is not uniform. Now for first two layers its similar to normal distribution.","d7951a05":"An attempt to introduce hyper parameter tuning using sklearns famous [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html)","32ea7b61":"![image.png](attachment:image.png)","406f7cbd":"We can see the distribution of weights here\nWeights have a uniform distribution in all three layers with very small variance","20ad204b":"<h2> MLP + ReLU + ADAM <\/h2>","783d8f85":"<h2> MLP + Batch-Norm on hidden Layers + AdamOptimizer <\/2>","f975047f":" <h3>  MLP + Sigmoid activation + SGDOptimizer <\/h3>","66d4b232":"<h2>  Softmax classifier  <\/h2>","71cb523c":"<h2> Hyper-parameter tuning of Keras models using Sklearn <\/h2>","83b60e75":"- The model needs to know what input shape it should expect. \n- For this reason, the first layer in a Sequential model \n(and only the first, because following layers can do automatic shape inference)\nneeds to receive information about its input shape. \n- you can use input_shape and input_dim to pass the shape of input\n- output_dim represent the number of nodes need in that layer here we have 10 nodes","fca7b456":"Adding the stocastic gradient as optimizer","841974a9":"- Using imshow to see one image in the dataset. The number in the 218th position is 6","df94445c":"Changing the activation function to `Rectified Linear Unit` aka [relu](https:\/\/keras.io\/activations\/#relu)\n-  https:\/\/arxiv.org\/pdf\/1707.09725.pdf#page=95 for relu layers\n- If we sample weights from a normal distribution N(0,\u03c3) we satisfy this condition with \u03c3=\u221a(2\/(ni). \n- h1 =>  \u03c3=\u221a(2\/(fan_in) = 0.062  => N(0,\u03c3) = N(0,0.062)\n- h2 =>  \u03c3=\u221a(2\/(fan_in) = 0.125  => N(0,\u03c3) = N(0,0.125)\n- out =>  \u03c3=\u221a(2\/(fan_in+1) = 0.120  => N(0,\u03c3) = N(0,0.120)","ee869a59":"- The X_tran and X_test are three dimentional numpy array. The 0th dimention is the number of images, 1st dimension is the width and 2nd dimension is the height","6dd7f301":"We will get val_loss and val_acc only when you pass the paramter validation_data\n1. val_loss : validation loss\n2. val_acc : validation accuracy\n3. loss : training loss\n4. acc : train accuracy\n\nFor each key in histrory.histrory we will have a list of length equal to number of epochs","5cab7412":"- The Sequential model is a linear stack of layers.\n- You can create a [Sequential model](https:\/\/keras.io\/getting-started\/sequential-model-guide\/) by passing a list of layer instances to the constructor:\n- We can create a densely connected neural network using the Dense Constructor\n- [Dense](https:\/\/keras.io\/layers\/core\/) implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n- [Activations](https:\/\/keras.io\/activations\/) can either be used through an Activation layer, or through the activation argument supported by all forward layers","fe8bb29a":"Two of the top numerical platforms in Python that provide the basis for Deep Learning research and development are Theano and TensorFlow.\n\nBoth are very powerful libraries, but both can be difficult to use directly for creating deep learning models.\n\nIn this kernel, I am trying to give an introduction to deep learning using keras which is a front end of tensorflow. Enjoy reading","e65fd7eb":"- Keras models are trained on Numpy arrays of input data and labels. \n- For training a model, you will typically use the  fit function\n\n fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, \n validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, \n validation_steps=None)\n\n- fit() function Trains the model for a fixed number of epochs (iterations on a dataset).\n\n- it returns A History object. Its History.history attribute is a record of training loss values and \n metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).","47437fed":"<h2> MLP + ReLU +SGD <\/h2>","536a5fa4":"We have three layers here\n1. Input layer with 512 neurons\n2. Second layer has 128 neurons\n3. Third layer\/output layer has 10 neurons ","a053e9df":"<h2> 5. MLP + Dropout + AdamOptimizer <\/h2>","1c6bdf1e":"In the previous example we have used the `tanh` activation function. No optimmizer was used. Lets use a different activation function called the `sigmoid` and `stocastic gradient optimizer`","c5f4972e":"Here we can see that there are two batch notmalization layers."}}