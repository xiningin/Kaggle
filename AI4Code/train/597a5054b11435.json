{"cell_type":{"f946edb3":"code","b2823dce":"code","4864b2f8":"code","58fa7d2d":"code","9b0e8d0f":"code","8ccb8763":"code","058ec0f1":"code","b2db9823":"markdown","05a7cd1a":"markdown","2c448959":"markdown","b1b99d63":"markdown"},"source":{"f946edb3":"import pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\n\ntqdm.pandas()\n\ntrain = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')","b2823dce":"def build_vocab(texts):\n    sentences = texts.progress_apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n    \nembedding_index = load_embeddings(\"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\")","4864b2f8":"tokenizer = Tokenizer(lower=False)\ntokenizer.fit_on_texts(train.comment_text.tolist() + test.comment_text.tolist())","58fa7d2d":"%%time\ndef check_coverage_new(word_counts, wanted_keys):\n    a = {key: val for key, val in word_counts.items() if key not in wanted_keys}\n    print(f'Found embeddings for {1-len(a)\/len(word_counts):.2%} of vocablen')\n    print(f'Found embeddings for {1-sum(a.values())\/sum(word_counts.values()):.2%} of all text')\n    return sorted(a.items(), key= lambda x : x[1], reverse=True)\n\nwanted_keys = embedding_index.keys()\nsort_x = check_coverage_new(tokenizer.word_counts, wanted_keys)","9b0e8d0f":"%%time\n\nimport operator \n\ndef check_coverage(vocab, embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\nsort_y = check_coverage(tokenizer.word_counts, embedding_index)","8ccb8763":"%%time\ndef check_coverage_new(word_counts, wanted_keys):\n    a = {key: val for key, val in word_counts.items() if key not in wanted_keys}\n    print(f'Found embeddings for {1-len(a)\/len(word_counts):.2%} of vocablen')\n    print(f'Found embeddings for {1-sum(a.values())\/sum(word_counts.values()):.2%} of all text')\n    return sorted(a.items(), key= lambda x : x[1], reverse=True)\n\nsort_x = check_coverage_new(tokenizer.word_counts, wanted_keys)","058ec0f1":"sort_x[:10], sort_y[:10]","b2db9823":"Hello. \n\nIn the process of studying the kernels of a given competitions, I often saw a code that is used for estimate cover by a dictionary of vector representations.\n\nThis method is good, but it looks \"not pythonically\" outwardly. For example, I do not understand the point of using \"try-except\". Below is my solution.","05a7cd1a":"In the course of work, you will use the \"check_coverage_new\" function several times, but you no longer need to override the \"wanted_keys\" (if you use one set of vector views), which speeds up the definition.","2c448959":"This is my solution (I hope.. I have not met a kernel with something similar)","b1b99d63":"Total:\n1. Execution speed at ~ 100ms- ~ 200ms faster. On this dataset this is not critical, but in larger datasets this can be a significant plus;\n2. The function has become shorter;\n3. One less import.\n\nI hope my decision will be useful for you."}}