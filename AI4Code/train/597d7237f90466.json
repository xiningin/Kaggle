{"cell_type":{"e273ad63":"code","52a95491":"code","8d2ee580":"code","3140d80d":"code","b32c640c":"code","a77d02fe":"code","ef280faf":"code","a40a7c72":"code","3dd77a26":"code","d9b46bb4":"code","d21b71f1":"code","4d05c037":"code","7b66174a":"code","810d5051":"code","48fe682f":"code","0c96d587":"code","c7667a05":"code","0d66ac7a":"code","c20bd4ac":"code","17136df2":"code","41130776":"code","d247a24c":"code","4c5fa69d":"code","235886c7":"code","755cd517":"code","7bd0ccbf":"code","6f24994a":"code","24b561c9":"markdown","b0027748":"markdown","32e042b4":"markdown","377e70c0":"markdown","18117c2b":"markdown","969f1069":"markdown","9681f2b4":"markdown","56dde027":"markdown","9041be87":"markdown","9106b27d":"markdown","35ea8cb2":"markdown","bb8a245b":"markdown","0d0e3a03":"markdown","79971410":"markdown","8227af1f":"markdown","274efbf4":"markdown","fd35c1f6":"markdown","348972de":"markdown","855d82ef":"markdown","7154ed38":"markdown"},"source":{"e273ad63":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly as py\nimport plotly.graph_objs as go","52a95491":"import numpy as np\n\nX=np.array([\n    [1,0,0],\n    [1,0,1],\n    [1,1,0],\n    [1,1,1]]\n)\n\n# Y = a and b\nY=np.array(X[:,1] & X[:,2])\n\n# Randomly initialize weight vector\nW=np.random.rand(1,3)","8d2ee580":"# Converge if y_pred == Y\ndef converge(X,Y):\n    p = np.inner(X,W) > 0\n    y_pred = np.array(p, dtype='int64').reshape(Y.shape)\n    return all(y_pred == Y)","3140d80d":"while not converge(X,Y):\n    \n    # Full batch\n    for i in range(4):\n\n        if Y[i] == 1 and np.inner(X[i],W) < 0:\n            W[0] += 0.1*X[i]\n\n        elif Y[i] == 0 and np.inner(X[i],W) >= 0:\n            W[0] -= 0.1*X[i]","b32c640c":"np.inner(X,W) > 0","a77d02fe":"import numpy as np\nimport matplotlib.pyplot as plt\n\n\nX=np.random.random((100,2))*10\nX[:,0]=1  # Bias term : always 1\nW = np.array([-3.6,0.25])\n\n#Y = -3.6 + 0.25x + noise\nY=np.dot(X,W.T)+np.random.random((100))\/5\n\nplt.plot(X[:,1],Y,'+')\nplt.title('data points')\nplt.show()","ef280faf":"new_W = np.linalg.inv(np.dot(X.T, X)).dot(X.T).dot(Y)","a40a7c72":"x_plot = np.linspace(0,10,100)\nplt.plot(x_plot, new_W[0]+new_W[1]*x_plot, label='Best fit line', color='r')\nplt.plot(X[:,1],Y,'+', label='data')\nplt.legend()\nplt.show()","3dd77a26":"# Initialize the feature space\nm=100\nbias = np.linspace(-10,10,m)\nweight = np.linspace(-10,10,m)\n\ndef mse(b,w):\n    y_pred = X[:,0]*b + X[:,1]*w  \n    return np.mean(np.sum((y_pred.ravel()-Y)**2))\n\no =[[0 for i in range(m)] for e in range(m)]\n\n# Compute loss for each feature\nfor i in range(m):\n    for j in range(m):\n        o[i][j] = mse(bias[j],weight[i])","d9b46bb4":"info =  'bias: %{x:.4f}<br>' + 'weight: %{y:.4f}<br>' +   'Loss: %{z:.4f}'\n\nfig = go.Figure(data=[\n                      go.Surface(z=o, x=bias, y=weight, hovertemplate = info, name='Loss'),\n                      go.Scatter3d(x=[new_W[0]] ,y=[new_W[1]], z=[mse(new_W[0],new_W[1])], hovertemplate=info, name='Minimum'),\n                      ])\n\nfig.update_layout(scene = dict(\n                                            xaxis_title='weight space',\n                                            yaxis_title='bias space',\n                                            zaxis_title='MSE'\n                                            ),\n                            width = 700,\n                            title = 'MSE of Simple linear regression')\nfig.show()","d21b71f1":"eta = 0.001\niter = 5000\nm = 100\n\nX = 2*np.random.rand(m,1)  \nX = np.c_[np.ones((m,1)),X]   # bias term : always 1\n\nY = X[:,0]*3 + X[:,1]*4 + np.random.rand(m,1).ravel()   # Linear Relationship\n#y = X[:,0]*3 + X[:,1]**6 + 2\/X[:,1] + np.random.rand(m,1).ravel()  # Non-linear relationship\nY = Y.reshape(-1,1)","4d05c037":"plt.plot(X[:,1],Y,'+')\nplt.show()","7b66174a":"W = np.random.randn(2,1)\nW_initial = W.copy()\nmse_history = []\n\nfrom sklearn.metrics import mean_squared_error as mse\n\nprint(\"MSE before training :\", mse(y_pred = X.dot(W).ravel(), y_true = Y))\nfor i in range(iter):\n    gradient = 2\/m *X.T.dot(X.dot(W)-Y) \n    W = W - eta*gradient.reshape(W.shape)\nprint(\"MSE after training :\", mse(y_pred = X.dot(W).ravel(), y_true = Y))","810d5051":"plt.clf()\nplt.plot(X[:,1],Y,'+', label='Data points')\nplt.plot(X[:,1], X.dot(W_initial), label = 'Initial weight', color='k')\nplt.plot(X[:,1], X.dot(W), label = 'Trained weight', color='r')\nplt.legend()\nplt.show()","48fe682f":"X = tf.random.uniform([100,2])\nX = tf.concat([X, tf.ones([100,1])], axis=1)\n\nY = tf.matmul(X, tf.constant([[4],[3],[5]], dtype=tf.float32))","0c96d587":"W = tf.Variable(tf.random.normal([3,1]), trainable=True)\nb = tf.random.normal([1])\n\nn_iter=100\neta=0.0001\nhistory = []\n\ndef current_MSE():\n    return tf.reduce_mean(tf.losses.mse(Y, tf.matmul(X, W) + b)).numpy()\n\nprint(\"MSE before training =\",current_MSE())\n\nfor i in range(n_iter):\n    history.append(current_MSE())\n    with tf.GradientTape() as tape:\n        y_pred = tf.matmul(X, W) + b\n        loss = tf.losses.mse(y_pred=y_pred, y_true=Y)\n    \n    grads = tape.gradient(loss,W)\n    W.assign(W-eta*grads)\n\nprint(\"MSE after training =\",current_MSE())","c7667a05":"plt.figure(figsize=(5,4))\nplt.plot(np.arange(n_iter), history)\nplt.xlabel('iteration')\nplt.ylabel('MSE')\nplt.show()","0d66ac7a":"model = tf.keras.Sequential([\n  tf.keras.layers.Dense(3, activation=tf.nn.relu, input_shape=(3,)),  # input shape required\n  tf.keras.layers.Dense(2, activation=tf.nn.relu),\n  tf.keras.layers.Dense(1)\n])","c20bd4ac":"model(X)[:5]","17136df2":"def loss(model, x, y, training):\n\n    # training=training is needed only if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    y_ = model(x, training=training)\n\n    return tf.keras.losses.mse(y_true=y, y_pred=y_)\n\n\nl = loss(model, X, Y, training=False)\nprint(\"Loss test: {}\".format(tf.reduce_mean(l)))","41130776":"def grad(model, inputs, targets):\n    with tf.GradientTape() as tape:\n        loss_value = loss(model, inputs, targets, training=True)\n    return loss_value, tape.gradient(loss_value, model.trainable_variables)","d247a24c":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)","4c5fa69d":"loss_value, grads = grad(model, X, Y)\n\nprint(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n                                          tf.reduce_mean(loss_value.numpy())))\n\noptimizer.apply_gradients(zip(grads, model.trainable_variables))\n\nprint(\"Step: {},          Loss: {}\".format(optimizer.iterations.numpy(),\n                                          tf.reduce_mean(loss(model, X, Y, training=True).numpy())))","235886c7":"print(\"Slightly different in slicing\")\nprint(X[0,None])\nprint(X[0,:])","755cd517":"train_loss_results = []\ntrain_accuracy_results = []\n\nnum_epochs = 31\n\nfor epoch in range(num_epochs):\n    \n    # Full batch\n    for i in range(X.shape[0]):\n        loss_value, grads = grad(model, X[i,None], Y[i,None])\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n    # End epoch\n    train_loss_results.append(tf.reduce_mean(loss(model,X,Y, training=True)))\n    train_accuracy_results.append( tf.reduce_mean(tf.keras.metrics.mse(Y, model(X))))\n\n    if epoch % 5 == 0:\n        print(\"Epoch {:03d}: Loss: {:.3f}\".format(epoch,\n                                                                    train_loss_results[-1]))","7bd0ccbf":"plt.figure(figsize=(5,4))\nplt.ylabel(\"MSE\")\nplt.plot(train_loss_results)\nplt.xlabel('iterations')\nplt.show()","6f24994a":"pred = model(X).numpy()\n\ndat = pd.DataFrame({'y-pred':pred.ravel(), 'y-true':Y.numpy().ravel()})\ndat","24b561c9":"## 1) Closed form","b0027748":"Initialize the optimizer.","32e042b4":"Example: close form gradient descent simple Linear regression","377e70c0":"### Closed form\n\nA common $Loss$ function for regression : $MSE$\n$$MSE(X,W) = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y} - y_i)^2$$ \nWhere $\\hat{y}$ is predicted output : $\\hat{y} = \\sum w_ix_i = X \\cdot W$ <br>\nThus,\n$$MSE(X,W) = \\frac{1}{m}\\sum_{i=1}^{m}(x_i \\cdot W - y_i)^2 $$\n$$= \\frac{1}{m} (XW - y)^T(XW - y)$$\nIts gradient with respect to weight :\n$$\\leadsto \\nabla_W MSE(W) = \\frac{2}{m} X^T(XW - y)$$\n\n---\nfor each iteration $do$: <br>\n$\\to W = W - \\eta \\nabla_W MSE(W)$ \n\n---","18117c2b":"# Classification\n","969f1069":"$$y = 4x_1 + 3x_2 + 5$$","9681f2b4":"Only single one neuron won't perform well if data is non-linear...","56dde027":"# Linear regression","9041be87":"Build the data points to learn","9106b27d":"Use TF autodiff to comput the gradients for us.","35ea8cb2":"#### Multilayer perceptron but higher level API\n\ntf.keras make the construction phase very easy. We don't have to initialize the variables manually, perform matrix multiply, and so on.","bb8a245b":"Sample of training","0d0e3a03":"We want to find **\"the good weight\"** vector $W$ that $W \\cdot X$ gives us the correct answer <br>\nLinear equation written in matrix form : $\\hat{y} = XW$ <br>\n**\"the good weight\"** vector : $W = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y$ <br>\n\n$\\hat{y}$ is y_pred\n\n### Derivation \ny_pred is calculated by $$\\hat{y} =  XW$$ <br>\n\n<br>\nLoss function : Sum of squared residual $$SSR(W) = \\sum (y-\\hat{y})^2$$ <br>\n$$ =  \\sum (y-XW)^2$$ <br>\n$$ = (y-XW)^T (y-XW) $$ <br>\n\n<br>\nWe need to minimize it. So, <br>\n$$ \\frac{\\partial}{\\partial W}( (y-XW)^T (y-XW)) = 0$$\n$$ X^T(y-XW) = 0 $$\n$$ X^Ty - X^TXW = 0 $$<br>\n\n$$ \\leadsto W = (X^TX)^{-1}X^Ty $$","79971410":"## 1) Perceptron algorithm\n\n$P$ := inputs with 1 class <br>\n$N$ := inputs with 0 class <br>\nInitialize $W$ randomly <br>\n\n$while$ !converge $do$: <br>\n$\\mapsto$ for all $x \\in P\\cup N$ <br>\n$\\longmapsto$ if ($x \\in P$ and $W \\cdot x < 0$) then {$W = W+\\mu x$}  <br>\n$\\longmapsto$ else if ($x \\in N$ and $W \\cdot x \\geq 0$) then {$W = W-\\mu x$}\n\n;Converge = when all inputs are classified correctly.\n### Classification\n$if (x_i \\cdot w_i > 0)$ then $\\hat{y_i} = 1$ <br>\n$if (x_i \\cdot w_i \\leq 0)$ then $\\hat{y_i} = 0$","8227af1f":"## 2) Gradient descent","274efbf4":"Make a prediction by simply passing the data to the model.","fd35c1f6":"First, let's see the **MSE** of **Linear regression**.","348972de":"#### Low level Perceptron","855d82ef":"We see that the loss of a simple linear regression is a regular bowl. Obviously, it has only one minimum point which is exactly global(i.e. no local minimum points)","7154ed38":"### TF's autodiff\n\nfor each iterations $do$: <br>\n$\\to$ (full batch) compute the $gradient$ of the $Loss$ with respect to the $trainables$   <br>\n$\\to$ update the weight matrix by the $gradient*\\eta$"}}