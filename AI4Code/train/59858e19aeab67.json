{"cell_type":{"0298b6f2":"code","1b740af9":"code","a163cc44":"code","83dce06b":"code","34c58f49":"code","2dca437d":"code","4a9adc4d":"code","7f53febf":"code","f0cfeda0":"code","178cc52a":"code","902e281f":"code","27bbd99f":"code","9d9f389f":"code","18315dac":"code","2a17c1d4":"code","3f88a2f4":"code","b96ae2e7":"code","c2f833e7":"code","668825f2":"markdown","ffe5f3e1":"markdown","bee018ac":"markdown"},"source":{"0298b6f2":"import matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nimport time\nimport os\nimport PIL.Image as Image\nfrom IPython.display import display\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nprint(torch.cuda.get_device_name(device))","1b740af9":"dataset_dir = \"..\/input\/car_data\/car_data\/\"\n","a163cc44":"indices = np.arange(8144)\nnp.random.shuffle(indices)\ntrain_tfms = transforms.Compose([transforms.Resize((400, 400)),\n                                 transforms.RandomHorizontalFlip(),\n                                 transforms.RandomRotation(15),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\nval_tfms = transforms.Compose([transforms.Resize((400, 400)),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntest_tfms = transforms.Compose([transforms.Resize((400, 400)),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n\ntrain_loader = torch.utils.data.DataLoader(dataset = torchvision.datasets.ImageFolder(root=dataset_dir+\"train\", transform = train_tfms),\nbatch_size=32, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[:7500]))\n\n\nval_loader =torch.utils.data.DataLoader(dataset = torchvision.datasets.ImageFolder(root=dataset_dir+\"train\", transform = val_tfms),\nbatch_size=32, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[7500:]))\n\n\ndataset2 = torchvision.datasets.ImageFolder(root=dataset_dir+\"test\", transform = test_tfms)\ntestloader = torch.utils.data.DataLoader(dataset2, batch_size = 32, shuffle=False, num_workers = 2)","83dce06b":"# get some random training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nprint(images[1].shape)","34c58f49":" grid = torchvision.utils.make_grid(images, nrow=10)\nplt.figure(figsize=(15,15))\nplt.imshow(np.transpose(grid, (1,2,0)))\n","2dca437d":"train_dir = \"..\/input\/car_data\/car_data\/train\"\n#os.listdir(train_dir)\nos.listdir(train_dir + '\/' + 'Acura ZDX Hatchback 2012')","4a9adc4d":"train_dir = \"..\/input\/car_data\/car_data\/train\"\ntest_dir = \"..\/input\/car_data\/car_data\/test\"\nos.listdir(train_dir)\n#Dictionary that has name of car class as key and image name as its values\ncar_names_train = {}\n\nfor i in os.listdir(train_dir):\n    #print()\n    car_names_train[i] = os.listdir(train_dir + '\/' + i)\ncar_names_train","7f53febf":"for i in os.listdir(test_dir):\n    #print()\n    car_names_test[i] = os.listdir(test_dir + '\/' + i)\ncar_names_test","f0cfeda0":"#Code to create two lists for class name and image directories corresponding to it\ncar_images_ls = []\ncar_names_ls = []\ncar_classes = []\ncar_directories = []\n\nfor i in car_names_train:\n    car_classes.append(i)\n\nfor i,j in enumerate(car_names_train.values()):\n    for img in j:\n        car_images_ls.append(img)\n        car_names_ls.append(car_classes[i])\n        \n        \nfor i in range(len(car_names_ls)):\n    car_directories.append(train_dir + '\/' + car_names_ls[i] + '\/' + car_images_ls[i])\n    \ncar_images_ls[1]","178cc52a":"def train_model(model_ft, criterion, optimizer, scheduler, n_epochs = 5):\n    \n    losses = []\n    accuracies = []\n    val_accuracies = []\n    # set the model to train mode initially\n    model_ft.train()\n    for epoch in range(n_epochs):\n        since = time.time()\n        running_loss = 0.0\n        running_correct = 0.0\n        for i, data in enumerate(train_loader, 0):\n\n            # get the inputs and assign them to cuda\n            inputs, labels = data\n            #inputs = inputs.to(device).half() # uncomment for half precision model\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            \n            # forward + backward + optimize\n            outputs = model_ft(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n             # calculate the loss\/acc later\n            running_loss += loss.item()\n            running_correct += (labels==predicted).sum().item()\n\n        epoch_duration = time.time()-since\n        epoch_loss = running_loss\/len(train_loader)\n        epoch_acc = 100\/32*running_correct\/len(train_loader)\n        print(\"Epoch %s, duration: %d s, loss: %.4f, acc: %.4f\" % (epoch+1, epoch_duration, epoch_loss, epoch_acc))\n        \n        losses.append(epoch_loss)\n        accuracies.append(epoch_acc)\n        \n        # switch the model to eval mode to evaluate on validation data\n        model_ft.eval()\n        val_acc = eval_model(model_ft)\n        val_accuracies.append(val_acc)\n        \n        # re-set the model to train mode after validating\n        model_ft.train()\n        scheduler.step(val_acc)\n        since = time.time()\n    print('Finished Training')\n    return model_ft, losses, accuracies, val_accuracies","902e281f":"def eval_model(model_ft):\n    correct = 0.0\n    total = 0.0\n    with torch.no_grad():\n        for i, data in enumerate(val_loader, 0):\n            images, labels = data\n            #images = images.to(device).half() # uncomment for half precision model\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            outputs = model_ft(images)\n            _, predicted = torch.max(outputs.data, 1)\n            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_acc = 100.0 * correct \/ total\n    print('Accuracy of the network on the validation images: %d %%' % (\n        val_acc))\n    return val_acc","27bbd99f":"model_ft = models.resnet34(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\n\n# replace the last fc layer with an untrained one (requires grad by default)\nmodel_ft.fc = nn.Linear(num_ftrs, 196)\nmodel_ft = model_ft.to(device)\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n\n\nlrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, threshold = 0.9)","9d9f389f":"model_ft, training_losses, training_accs, val_accs = train_model(model_ft, criterion, optimizer, lrscheduler, n_epochs=10)","18315dac":"colnames=['Name'] \nuser1 = pd.read_csv('..\/input\/names.csv',names=colnames)\ndict1={}\nfor i in user1.index:\n    dict1[i]=user1['Name'][i]\n    \ndict1","2a17c1d4":"###Testing with random validation samples###\nwith torch.no_grad():\n    model_ft.eval()\n    model_ft.eval()\n    print (\"Evaluating random validation samples:\")\n    val_data, val_targets = next(iter(val_loader))\n    outputs = model_ft(val_data[0].unsqueeze(0).to(device))\n    _, pred = torch.max(outputs, 1)\n    \n    print (\"Actual Class - {0}\".format(dict1[int(val_targets[0]) ]))\n    print (\"Predicted Class - {0}\".format(dict1[int(pred)]))","3f88a2f4":"# plot the stats\nf, axarr = plt.subplots(2,2, figsize = (12, 8))\naxarr[0, 0].plot(training_losses)\naxarr[0, 0].set_title(\"Training loss\")\naxarr[0, 1].plot(training_accs)\naxarr[0, 1].set_title(\"Training acc\")\naxarr[1, 0].plot(val_accs)\n\naxarr[1, 0].set_title(\"Val acc\")","b96ae2e7":"#Testing on Test loader.Getting list of image names and class labels.\nlabels1=[]\nimage_nm=[]\nimg_indx=0\nwith torch.no_grad():\n    for i, (images, labels) in enumerate(testloader, 0):\n        images = images.to(device)\n        outputs = model_ft(images)\n        _, predicted = torch.max(outputs.data, 1)\n        #print(len(predicted))\n        \n        for j in range(len(predicted)):\n                #print(idx_class_mapping[i.item()])\n                labels1.append(int(predicted[j]) + 1)\n                sample_fname, _ = testloader.dataset.samples[img_indx]\n                img_indx+=1\n                sample_fname_file=os.path.splitext(os.path.basename(sample_fname))[0]\n                image_nm.append(sample_fname_file)\n                #image_nm.append()\n        \n      #print(\"{}, {}\\n\".format(sample_fname[i], predicted[i].item())) ","c2f833e7":"#Making Dataframe for class labels and image names\npredicted_df = pd.DataFrame(\n    {'Image': image_nm,\n     'Prediction': labels1     \n    })\n\npredicted_df.to_csv()","668825f2":"**Stanford car dataset Classification**","ffe5f3e1":"We have to make a model that can tell the difference between cars by type or colour? Which cars are manufactured by Tesla vs BMW?","bee018ac":"My work approach:: I used pytorch.imageloader becuase it directly loads dataset from given path like input\/flower_data\/train or input\/flower_data\/valid. Earlier i tried to load it using numpy\/pandas, then i need to write code for making train and valid set, each having images of 196 categories with proper labeling. Like.. 1 folder will have all images inside 1 floder and label as 1, which we will replace with name(help from json cat_name). But, using pytorch, it was easy to do so with only few lines of code.\n\nAlgoritm Optimization:: We iincreased model accuracy by changing parameters like epoc.We changed epoc from 3 to 9.Accurancy increased slowly slowly.\n\nThen we pre-process the dataset, using pytorch, rotate,resize , normailze etc methods.\n\nAlgorithm used for this image classification:: Transfer learning becuase making model from scratch would be very complex and computation expensive, would require lot of time. Transfer learning allows us to train deep networks using significantly less data then we would need if we had to train from scratch. In recent years, a number of models have been created for reuse in computer vision problems. Using these pre-trained models is known as transfer learning These pre-trained models allow others to quickly obtain cutting edge results in computer vision without needing the large amounts of compute power, time, and patience in finding the right training technique to optimize the weights. "}}