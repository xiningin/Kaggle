{"cell_type":{"f02873c1":"code","145aa9af":"code","4cd44892":"code","4cb14bb8":"code","53260302":"code","d8f7ff49":"code","46843ed0":"code","80f2e456":"code","9636a329":"code","feded683":"code","0645c69c":"code","0c6c0fbd":"code","d567e531":"code","a069c357":"code","51d40338":"code","198c706f":"code","4753ea88":"code","f16708e2":"code","aab11755":"code","9715be11":"code","c267a50b":"code","cb66b30d":"code","cb335827":"code","be5823cc":"code","dc9ebaa3":"code","7505a513":"code","febf700d":"code","3440490e":"code","c9b9bb1a":"code","7d2d4676":"code","526667c4":"code","5d956ece":"markdown","48001f71":"markdown","122a5955":"markdown","54cb4fae":"markdown"},"source":{"f02873c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","145aa9af":"import tensorflow as tf \nimport os \n# walk through 10 classes of food image data \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nsns.set()\nfor dirpath, dirnames, filenames in os.walk(\"\/kaggle\/input\/shoe-dataset\/shoeTypeClassifierDataset\/training\/\"):\n    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}' .\")","4cd44892":"# let's get the classnames \nimport pathlib\nimport numpy as np \ntrain_dir = \"\/kaggle\/input\/shoe-dataset\/shoeTypeClassifierDataset\/training\/\"\ntest_dir= \"\/kaggle\/input\/shoe-dataset\/shoeTypeClassifierDataset\/validation\/\"\ndata_dir = pathlib.Path(train_dir)\nclass_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\nprint(class_names)","4cb14bb8":"# let's get the subdirectories ( these are our class names)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport random \n\ndef view_random_image(target_dir, target_class):\n    # Setup the target directory (we will view images from here )\n    target_folder = target_dir+target_class \n\n    # get random images path \n    random_image = random.sample(os.listdir(target_folder), 1)\n    print(random_image)\n\n    # Read in the image and plot it using matplotlib\n    img = mpimg.imread(target_folder+'\/'+random_image[0])\n\n    plt.imshow(img)\n    plt.title(target_class, size=22)\n    plt.axis(\"off\")\n\n    print(f'Image shape: {img.shape}')\n\n    return img","53260302":"img = view_random_image(target_dir=train_dir,\n                        target_class = random.choice(class_names))","d8f7ff49":"from tensorflow.keras.preprocessing.image import ImageDataGenerator \n\n# rescaling images \ntrain_data_gen = ImageDataGenerator(rescale=1\/255.)\ntest_data_gen = ImageDataGenerator(rescale=1\/255.)\n\n# load data in from directories and turn it into batches \ntrain_data = train_data_gen.flow_from_directory(train_dir, \n                                               target_size=(224, 224), \n                                               batch_size = 32, \n                                               class_mode=\"categorical\")\n\n\ntest_data = test_data_gen.flow_from_directory(\n    test_dir, \n    target_size = (224, 224), \n    batch_size = 32, \n    class_mode=\"categorical\"\n)","46843ed0":"import tensorflow as tf \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Activation","80f2e456":"# inserting random seed \ntf.random.set_seed(420)\n\nmodel_1 = Sequential([\n    Conv2D(10, 3, input_shape=(224, 224, 3), activation=\"relu\"),\n    Conv2D(10, 3, activation=\"relu\"),\n    MaxPool2D(),\n    Conv2D(10, 3, activation=\"relu\"),\n    Conv2D(10, 3, activation=\"relu\"),\n    MaxPool2D(),\n    Flatten(), \n    Dense(6, activation=\"softmax\")\n    \n])\n\n\nmodel_1.compile(\n    loss = tf.keras.losses.CategoricalCrossentropy(), \n    optimizer = tf.keras.optimizers.Adam(),\n    metrics = [\"accuracy\"]\n)\n","9636a329":"model_1.summary()","feded683":"hitory_1 = model_1.fit(train_data, \n            epochs=5,\n            steps_per_epoch = len(train_data),\n            validation_data=test_data,\n            validation_steps = len(test_data)\n)","0645c69c":"model_1.evaluate(test_data)\n","0c6c0fbd":"# Let's create a function to plot loss curve and accuracy \ndef plot_loss_curves(history):\n    \"\"\"\n    function created to print loss curve aswell as accuracy curve from the history variable \n    \n    args : \n    history object \n    \n    return : \n    None \n    \n    Plots : two graphs\n    \"\"\"\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']\n    \n    \n    epochs = range(len(history.history['loss']))\n    \n    # plotting loss \n    plt.plot(epochs, loss, label=\"training loss\")\n    plt.plot(epochs, val_loss, label=\"validation loss\")\n    plt.title(\"loss\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n    plt.show()\n    \n    \n    # plotting accuracy \n    plt.plot(epochs, accuracy, label=\"training accuracy\")\n    plt.plot(epochs, val_accuracy, label=\"validation Accuracy\")\n    plt.title(\"Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n    plt.show()","d567e531":"plot_loss_curves(hitory_1)","a069c357":"# set a random seed \ntf.random.set_seed(42)\n\nmodel_2 = Sequential([\n    Conv2D(10, 3, input_shape=(224, 224, 3), activation=\"relu\"),\n    MaxPool2D(), \n    Conv2D(10, 3, input_shape=(224, 224, 3), activation=\"relu\"),\n    MaxPool2D(),\n    Flatten(),\n    Dense(6, activation=\"softmax\")\n])","51d40338":"model_2.compile(\n    loss = tf.keras.losses.CategoricalCrossentropy(),\n    optimizer = tf.keras.optimizers.Adam(),\n    metrics = ['accuracy']\n)","198c706f":"model_2.fit(train_data, \n           epochs=5, \n           steps_per_epoch = len(train_data), \n           validation_data = test_data, \n           validation_steps = len(test_data)\n           )","4753ea88":"model_2.summary()","f16708e2":"history_2 = model_2.fit(train_data, \n           epochs=5, \n           steps_per_epoch = len(train_data), \n           validation_data = test_data, \n           validation_steps = len(test_data)\n)","aab11755":"plot_loss_curves(history_2)","9715be11":"# here I am going to try to model one is resenet and another is effieicient net and further I am \n#going to fine tune the effieiceint ","c267a50b":"# resnet url \nresnet_url = \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v2_50\/feature_vector\/4\"\n\n# effiecient net \nefficientnet_url = \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b0\/feature-vector\/1\"\n","cb66b30d":"\nimport tensorflow as tf \nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\n\n\n# let's create_model() function \n\ndef create_model(model_url, num_classes=10):\n    \"\"\"\n    Takes a Tensorflow hub URL and creates a keras Sequential model with it.\n    \n\n    Args : \n        model_url : (str) A tensorflow Hub features extraction URL. \n        num_classes (int) : Number of output neurons in the op layer,\n        should be equal to number of target clases , default 10.\n\n    Returns : \n    A uncompiled Keras Sequential Model with model_url as feature extractor \n    layer and Dense output layer with number classes output neurons \n\n\n    \"\"\"\n    # Download the pretrained model and save it as keras layer \n\n    feature_extractor_layer = hub.KerasLayer(\n        model_url, \n        trainable=False, \n        name = \"FEL\", \n        input_shape = (224, 224, 3)\n    )\n\n    # create out seq model \n    model = tf.keras.Sequential([\n                                 feature_extractor_layer,\n                                 tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"outputlayers\")\n    ])\n\n    return model","cb335827":"# create ResNet Model \n\nresnet_model = create_model(\n    resnet_url, \n    num_classes = train_data.num_classes,\n    \n\n)","be5823cc":"resnet_model.summary()","dc9ebaa3":"# compile resnet model \n\nresnet_model.compile(\n    loss = tf.keras.losses.CategoricalCrossentropy(),\n    optimizer = tf.keras.optimizers.Adam(),\n    metrics = [\"accuracy\"]\n)","7505a513":"resent_history = resnet_model.fit(\n    train_data, \n    epochs=5, \n    steps_per_epoch=len(train_data),\n    validation_data = test_data)","febf700d":"# create ResNet Model \n\nefficientnet_model = create_model(\n    efficientnet_url, \n    num_classes = train_data.num_classes,\n    \n\n)","3440490e":"# compile resnet model \n\nefficientnet_model.compile(\n    loss = tf.keras.losses.CategoricalCrossentropy(),\n    optimizer = tf.keras.optimizers.Adam(),\n    metrics = [\"accuracy\"]\n)","c9b9bb1a":"efficientnet_model = efficientnet_model.fit(\n    train_data, \n    epochs=5, \n    steps_per_epoch=len(train_data),\n    validation_data = test_data)","7d2d4676":"plot_loss_curves(efficientnet_model)","526667c4":"plot_loss_curves(resent_history)","5d956ece":"## Building CNN Model ","48001f71":"## Using Transfer Learning ","122a5955":"## What if we try to simplify our model a little \n\n","54cb4fae":"# Preprocess the dataset \n## Preparing it for a model "}}