{"cell_type":{"e93be5d9":"code","80034429":"code","bb3e7245":"code","a3232a19":"code","8ad608fe":"code","74d2c598":"code","c28d8fb8":"code","65605c5b":"code","b337c806":"code","fc72bb3a":"code","43e4e7ca":"code","8517e569":"code","e95669b4":"code","5c344ea0":"code","dd1f6a8a":"code","f922bd97":"code","1c9864bd":"code","207dfbae":"code","61257e94":"code","247ed8a7":"code","25703517":"code","07396faa":"code","afd586b5":"code","072e6740":"code","344e28e7":"code","31c5aad8":"code","0e29e388":"code","c9534410":"code","6b2239f7":"code","b04f8520":"code","ac607b29":"code","7d1a0f2e":"code","d06be668":"code","2b3d26d5":"code","b8acf217":"code","6dd7d1ef":"code","d10f9e95":"code","66db7134":"code","fe0b1b5c":"code","83d85e53":"code","ba084ed1":"code","bc8975a3":"code","498d80db":"code","66eca785":"code","3dc4c483":"code","4a3212ff":"code","738a4eda":"code","bdbbe7c8":"code","6e723361":"code","42a5225d":"code","b2339146":"code","10dce281":"code","e0fd96b0":"code","02cde210":"code","07c01556":"code","782da25c":"code","9f61f914":"code","66450bae":"code","cb6d0ecd":"code","979dfe00":"code","fad277f0":"code","73f6630a":"code","59d5f9ea":"code","f787823f":"code","083aed61":"code","c2aab032":"code","f37f046b":"code","d862b36e":"code","10b02b64":"code","1f72e7dc":"code","abbca54b":"code","281ae91b":"code","05c3f5f4":"code","48b9ecc7":"code","9e47cb62":"code","95330e28":"code","59e70ad4":"code","0a329917":"code","50a5fb67":"code","d93968f9":"code","d6563275":"code","b8d8deb5":"code","bce10f6c":"code","dbe107bb":"code","4d120c19":"code","5ad46bc4":"code","fdaf5469":"code","979c4102":"code","2b3b5a4a":"code","30eea9c4":"code","dc2501d6":"code","72acb14b":"code","ad5b6a95":"code","4bc0e787":"code","a9ba3b5a":"code","f115d0b6":"code","011f02f1":"code","aae45a6b":"code","460acab1":"code","cd635ca6":"code","ab55cd01":"code","5e5c331b":"markdown","9934380d":"markdown","f9f0637d":"markdown","58e7d1b1":"markdown","6e41b1ee":"markdown","7e8c58a7":"markdown","1843e0f6":"markdown","1fd2d00f":"markdown","f6dec562":"markdown","58163294":"markdown","c0aec90c":"markdown","547b9a79":"markdown","b19aa3c5":"markdown","599a4b2b":"markdown","5e40cf79":"markdown","0951b4d1":"markdown","3b3552af":"markdown","d204c930":"markdown","4ec3d19d":"markdown","004f1b24":"markdown","cbc5d1b3":"markdown","70cc7434":"markdown","d54cc423":"markdown","c09d0542":"markdown","6a0579a5":"markdown","ed5134b0":"markdown","5fe5d158":"markdown","95d18943":"markdown","19aeb96c":"markdown","2da2a6a7":"markdown","e892d640":"markdown","7d560331":"markdown","d1d152ed":"markdown","e9aad46e":"markdown","31a5be43":"markdown","aac213f2":"markdown","04edea34":"markdown","565444c4":"markdown","d1a21f59":"markdown","4b8d072c":"markdown","936e9e02":"markdown","b0c10af9":"markdown","655a1e98":"markdown","e806329a":"markdown","0ead6372":"markdown","67fcd749":"markdown","99f50e44":"markdown","7faf0898":"markdown","4cb52356":"markdown","b7e36f35":"markdown","820077bb":"markdown","2d1e0965":"markdown","fb0995af":"markdown","815ca0d8":"markdown","dcdad32e":"markdown","a2cda4c3":"markdown","0307d66c":"markdown","18caaa9b":"markdown","e91c0814":"markdown","a6f70fd2":"markdown","def115c2":"markdown","6eeaa074":"markdown","477a79e7":"markdown","1a14a159":"markdown","cfe283d2":"markdown","e38982f4":"markdown","fb557ee3":"markdown","84757323":"markdown","376b32ac":"markdown","a40b2d51":"markdown","7dd5eef2":"markdown","49c45ba3":"markdown","fc4eaf57":"markdown","4a3b0518":"markdown","28313acd":"markdown","6faaef86":"markdown","50802b82":"markdown","32c904f1":"markdown"},"source":{"e93be5d9":"!pip install comet_ml","80034429":"import comet_ml","bb3e7245":"!pip install spacy\n!pip install NLTK","a3232a19":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom numpy import arange\nimport seaborn as sns\n\n#Natural Language Processing\nimport re\nimport spacy.cli\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\n#Matrix measurement\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\n#Resampling techniques\nfrom sklearn.utils import resample\nfrom imblearn.over_sampling import SMOTE\n\n#Machine Learning Models\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.linear_model import SGDClassifier, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import  KFold\n\n\nfrom wordcloud import WordCloud \n\n# from google.colab import drive","8ad608fe":"spacy.cli.download('en_core_web_sm')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nstop = nltk.corpus.stopwords.words('english')","74d2c598":"nlp = spacy.load('en_core_web_sm')","c28d8fb8":"train = pd.read_csv('https:\/\/raw.githubusercontent.com\/Stephane-Masamba\/Team_4_CPT_ML-Classification\/Mikael\/train%20(1).csv')\nprint(train.head())","65605c5b":"test = pd.read_csv('https:\/\/raw.githubusercontent.com\/Stephane-Masamba\/Team_4_CPT_ML-Classification\/Mikael\/test%20(1).csv')\nprint(test.head())","b337c806":"def clean_text(df):\n  i = 0\n  for tweet in df['message']:\n    tweet = tweet.lower()\n    tweet = re.sub(r'http\\S+', 'LINK', tweet)\n    tweet = re.sub(r'@\\S+', 'USER_REF', tweet)\n    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n    tweet = tweet.lstrip()\n    tweet = tweet.rstrip()\n    tweet = tweet.replace('  ', ' ')\n    df.loc[i, 'message'] = tweet\n    i += 1\n\n","fc72bb3a":"clean_text(train)\ntrain","43e4e7ca":"clean_text(test)\ntest","8517e569":"def remove_stopwords(df):\n    my_stop_words = stopwords.words('english')\n    my_stop_words.append('LINK')\n    my_stop_words.append('USER_REF')\n\n    df_index = 0\n\n    for tweet in df['message']:\n      tweet = word_tokenize(tweet)\n      tweet = [word for word in tweet if not word in my_stop_words]\n      tweet = ' '.join(tweet)\n\n      df.loc[df_index, 'message'] = tweet\n      df_index += 1\n\n    return df","e95669b4":"remove_stopwords(train)","5c344ea0":"remove_stopwords(test)","dd1f6a8a":"def entities(df):\n    df_index = 0\n\n    for tweet in df['message']:\n      tweet = nlp(tweet)\n\n      for entity in tweet.ents:\n        df.loc[df_index, 'message'] = df.loc[df_index, 'message'].replace(str(entity.text), str(entity.label_))\n\n      df_index += 1\n\n      return df","f922bd97":"entities(train)","1c9864bd":"entities(test)","207dfbae":"def lem_text(df):\n    df_index = 0\n\n    for tweet in df['message']:\n      tweet = nlp(tweet)\n      \n      for token in tweet:\n        df.loc[df_index, 'message'] = df.loc[df_index, 'message'].replace(str(token.text), str(token.lemma_))\n\n      df_index += 1\n\n      return df","61257e94":"lem_text(train)","247ed8a7":"lem_text(test)","25703517":"train.isnull().sum()\n","07396faa":"test.isnull().sum()","afd586b5":"train.sentiment.value_counts()","072e6740":"counts = train[\"sentiment\"].value_counts()\nplt.bar(range(len(counts)), counts)\nplt.xticks([0, 1, 2, 3], ['Pro', 'News', 'Neutral', 'Anti'])\n\nplt.ylabel(\"Total per class\")\nplt.xlabel(\"Sentiment Classes\")\nplt.show()","344e28e7":"#Percentage of the major class\nlen(train[train.sentiment==1])\/len(train.sentiment)","31c5aad8":"#word clouds\nnews = train[train['sentiment'] == 2]['message']\npro = train[train['sentiment'] == 1]['message']\nneutral =train[train['sentiment'] == 0]['message']\nAnti = train[train['sentiment'] ==-1]['message']\n\n\nnews = [word for line in news for word in line.split()]\npro = [word for line in pro for word in line.split()]\nneutral = [word for line in neutral for word in line.split()]\nAnti= [word for line in Anti for word in line.split()]\n\nnews = WordCloud(\n    background_color='white',\n    max_words=20,\n    max_font_size=40,\n    scale=5,\n    random_state=1,\n    collocations=False,\n    normalize_plurals=False\n).generate(' '.join(news))\n\npro = WordCloud(\n    background_color='white',\n    max_words=20,\n    max_font_size=40,\n    scale=5,\n    random_state=1,\n    collocations=False,\n    normalize_plurals=False\n).generate(' '.join(pro))\n\n\n\nneutral = WordCloud(\n    background_color='white',\n    max_words=20,\n    max_font_size=40,\n    scale=5,\n    random_state=1,\n    collocations=False,\n    normalize_plurals=False\n).generate(' '.join(neutral))\n\n\nAnti = WordCloud(\n    background_color='white',\n    max_words=20,\n    max_font_size=40,\n    scale=5,\n    random_state=1,\n    collocations=False,\n    normalize_plurals=False\n).generate(' '.join(Anti))\n\n\nfig, axs = plt.subplots(2, 2, figsize = (20, 10))\nfig.tight_layout(pad = 0)\n\naxs[0, 0].imshow(news)\naxs[0, 0].set_title('Words from news tweets', fontsize = 20)\naxs[0, 0].axis('off')\n\naxs[0, 1].imshow(pro)\naxs[0, 1].set_title('Words from pro tweets', fontsize = 20)\naxs[0, 1].axis('off')\n\n\naxs[1, 0].imshow(Anti)\naxs[1, 0].set_title('Words from anti tweets', fontsize = 20)\naxs[1, 0].axis('off')\n\naxs[1, 1].imshow(neutral)\naxs[1, 1].set_title('Words from neutral tweets', fontsize = 20)\naxs[1, 1].axis('off')\n\nplt.savefig('joint_cloud.png')","0e29e388":"X = train['message']\nX","c9534410":"y = train['sentiment']\ny","6b2239f7":"tf_vecto = TfidfVectorizer(lowercase = True,stop_words = 'english',ngram_range=(1, 2))\nX = tf_vecto.fit_transform(X)","b04f8520":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","ac607b29":"#X_test = test['message']","7d1a0f2e":"#X_test = tf_vect.transform(X_test)","d06be668":"def accuracy(model):\n    features = train['message']\n    target = train['sentiment']\n\n    train_scores = []\n    test_scores = []\n\n    #tf_vect = TfidfVectorizer(ngram_range=(1, 2))\n    tf_vecto = TfidfVectorizer(lowercase = True,stop_words = 'english',ngram_range=(1, 2))\n\n\n    folds = KFold(n_splits=5, shuffle=True)\n\n    for train_index, test_index in folds.split(features):\n        x_train, x_test = features.iloc[train_index], features.iloc[test_index]    \n        y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n\n        x_train = tf_vecto.fit_transform(x_train)\n        x_test = tf_vecto.transform(x_test)\n     \n        model.fit(x_train, y_train)\n        train_predictions = model.predict(x_train)\n        test_predictions = model.predict(x_test)\n\n        train_score = accuracy_score(y_train, train_predictions)\n        train_scores.append(train_score)\n\n        test_score = accuracy_score(y_test, test_predictions)\n        test_scores.append(test_score)\n\n    avg_train_accuracy = np.mean(train_scores)\n    avg_test_accuracy = np.mean(test_scores)\n\n    return [avg_train_accuracy, avg_test_accuracy]\n","2b3d26d5":"sv = SVC()\n\nsv_accuracy = accuracy(sv)\nsv_accuracy","b8acf217":"bernoulli = BernoulliNB()\n\nbernoulli_accuracy = accuracy(bernoulli)\nbernoulli_accuracy","6dd7d1ef":"mnb = MultinomialNB()\n\nmnb_accuracy = accuracy(mnb)\nmnb_accuracy","d10f9e95":"sgd = SGDClassifier()\n\nsgd_accuracy = accuracy(sgd)\nsgd_accuracy","66db7134":"rand_forest = RandomForestClassifier()\n\nrand_forest_accuracy = accuracy(rand_forest)\nrand_forest_accuracy","fe0b1b5c":"knn = KNeighborsClassifier()\n\nknn_accuracy = accuracy(knn)\nknn_accuracy","83d85e53":"grad_booster = GradientBoostingClassifier()\n\ngrad_booster_accuracy = accuracy(grad_booster)\ngrad_booster_accuracy","ba084ed1":"extra_trees = ExtraTreesClassifier()\n\nextra_trees_accuracy = accuracy(extra_trees)\nextra_trees_accuracy","bc8975a3":"bagging = BaggingClassifier()\n\nbagging_accuracy = accuracy(bagging)\nbagging_accuracy","498d80db":"dec_tree = DecisionTreeClassifier()\n\ndec_tree_accuracy = accuracy(dec_tree)\ndec_tree_accuracy","66eca785":"linear_sv = LinearSVC()\n\nlinear_sv_accuracy = accuracy(linear_sv)\nlinear_sv_accuracy","3dc4c483":"models = ['SVC', 'Bernoulli', 'Multinomial Naive Bayes', 'SGDClassifier', 'Random Forest', 'KNearestNeighbours', 'Gradient Booster', 'Extra Trees', 'Bagging', 'Decision Tree', 'Linear SV']\nbar_widths = [sv_accuracy[1], bernoulli_accuracy[1], mnb_accuracy[1], sgd_accuracy[1], rand_forest_accuracy[1], knn_accuracy[1], grad_booster_accuracy[1], extra_trees_accuracy[1], bagging_accuracy[1], dec_tree_accuracy[1], linear_sv_accuracy[1]]\nbar_positions = arange(11) + 0.75\ntick_positions = range(1,12)\n\nfig, ax = plt.subplots()\nax.barh(bar_positions, bar_widths, 0.5)\nax.set_yticks(tick_positions)\nax.set_yticklabels(models)\n\nax.set_ylabel('Model')\nax.set_xlabel('Accuracy')\nax.set_title('Accuracy For Each Model Trained')\n\nplt.show()","4a3212ff":"linear_sv.fit(X_train, y_train)","738a4eda":"#confusion matrix and classification_report\ny_pred = linear_sv.predict(X_test)\n\nprint(confusion_matrix(y_test,y_pred))\n\nprint('\\n\\nAccuracy score: ' + str(accuracy_score(y_test, y_pred)))\nprint(\"\\n\\nClassification Report:\\n\\n\", classification_report(y_test,y_pred,target_names=['Anti', 'Neutral','Pro','News']))","bdbbe7c8":"sentiment_code = {1:'Pro', 2:'News', 0:'Neutral', -1:'Anti'}","6e723361":"train['sentiment_code'] = train['sentiment'].map(sentiment_code)","42a5225d":"aux_train = train[['sentiment', 'sentiment_code']].drop_duplicates().sort_values('sentiment_code')\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(12.8,6))\nsns.heatmap(conf_matrix, \n            annot=True,\n            cbar=False,\n            fmt='g',\n            xticklabels=aux_train['sentiment'].values, \n            yticklabels=aux_train['sentiment'].values,\n            cmap=\"Blues\")\nplt.ylabel('Predicted')\nplt.xlabel('Actual')\nplt.title('Confusion matrix')\nplt.show()","b2339146":"sgd.fit(X_train,y_train)","10dce281":"y_predict = sgd.predict(X_test)\n\nprint(confusion_matrix(y_test,y_predict))\n\nprint('\\n\\nAccuracy score: ' + str(accuracy_score(y_test, y_pred)))\nprint(\"\\n\\nClassification Report:\\n\\n\", classification_report(y_test,y_predict,target_names=['Anti', 'Neutral','Pro','News']))\n\n","e0fd96b0":"sentiment_code = {1:'Pro', 2:'News', 0:'Neutral', -1:'Anti'}","02cde210":"train['sentiment_code'] = train['sentiment'].map(sentiment_code)","07c01556":"aux_train = train[['sentiment', 'sentiment_code']].drop_duplicates().sort_values('sentiment_code')\nconf_matrix = confusion_matrix(y_test, y_predict)\nplt.figure(figsize=(12.8,6))\nsns.heatmap(conf_matrix, \n            annot=True,\n            cbar=False,\n            fmt='g',\n            xticklabels=aux_train['sentiment'].values, \n            yticklabels=aux_train['sentiment'].values,\n            cmap=\"Blues\")\nplt.ylabel('Predicted')\nplt.xlabel('Actual')\nplt.title('Confusion matrix')\nplt.show()","782da25c":"train = pd.read_csv('https:\/\/raw.githubusercontent.com\/Stephane-Masamba\/Team_4_CPT_ML-Classification\/Mikael\/train%20(1).csv')\nprint(train.head())","9f61f914":"test = pd.read_csv('https:\/\/raw.githubusercontent.com\/Stephane-Masamba\/Team_4_CPT_ML-Classification\/Mikael\/test%20(1).csv')\nprint(test.head())","66450bae":"clean_text(train)\nremove_stopwords(train)\nentities(train)\nlem_text(train)","cb6d0ecd":"train_majority = train[train.sentiment== 1]\ntrain_0 = train[train.sentiment== 0]\ntrain_2 = train[train.sentiment== 2]\n\ntrain_minority = train[train.sentiment==-1]\n\n","979dfe00":"# Downsample majority classes\ntrain_majority_downsampled = resample(train_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=1296,     # to match minority class\n                                 random_state=123) # reproducible results\n\n\ntrain_0_downsampled = resample(train_0, \n                                 replace=False,    \n                                 n_samples=1296,     \n                                 random_state=123) \n\ntrain_2_downsampled = resample(train_2, \n                                 replace=False,    \n                                 n_samples=1296,     \n                                 random_state=123) \n\n                      \n\n\n\n# Combine minority class with downsampled majority class\ntrain_downsampled1 = pd.concat([train_0_downsampled,train_2_downsampled])\n\ntrain_downsampled2 = pd.concat([train_majority_downsampled, train_minority])\n\ntrain_downsampled =  pd.concat([train_downsampled1, train_downsampled2])","fad277f0":"train_downsampled","73f6630a":"train_downsampled['sentiment'].value_counts()","59d5f9ea":"counts = train[\"sentiment\"].value_counts()\ncounti = train_downsampled['sentiment'].value_counts()\n\nplt.bar(range(len(counts)), counts)\nplt.bar(range(len(counts)),counti,color='red')\nplt.xticks([0, 1, 2, 3], ['Pro', 'News', 'Neutral', 'Anti'])\n\nplt.ylabel(\"Total per class\")\nplt.xlabel(\"Sentiment Classes\")\nplt.legend(['original','resampled'])\nplt.show()","f787823f":"X_down = train['message']\nX_down","083aed61":"y_down = train['sentiment']\ny_down","c2aab032":"X_down = tf_vecto.fit_transform(X_down)\nX_down","f37f046b":"#train_test_split\nX_train1,X_test1,y_train1,y_test1 = train_test_split(X_down,y_down,test_size=0.2,random_state=0)","d862b36e":"\nlsvm = LinearSVC()\nlsvm.fit(X_train1, y_train1)","10b02b64":"#confusion matrix and classification_report\ny_pred1 = lsvm.predict(X_test1)\n\nprint(confusion_matrix(y_test1,y_pred1))\n\nprint('\\n\\nAccuracy score: ' + str(accuracy_score(y_test1, y_pred1)))\nprint(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test1,y_pred1,target_names=['Anti', 'Neutral','Pro','News']))","1f72e7dc":"sentiment_code = {1:'Pro', 2:'News', 0:'Neutral', -1:'Anti'}","abbca54b":"train['sentiment_code'] = train['sentiment'].map(sentiment_code)","281ae91b":"aux_train = train[['sentiment', 'sentiment_code']].drop_duplicates().sort_values('sentiment_code')\nconf_matrix = confusion_matrix(y_test1, y_pred1)\nplt.figure(figsize=(12.8,6))\nsns.heatmap(conf_matrix, \n            annot=True,\n            cbar=False,\n            fmt='g',\n            xticklabels=aux_train['sentiment'].values, \n            yticklabels=aux_train['sentiment'].values,\n            cmap=\"Blues\")\nplt.ylabel('Predicted')\nplt.xlabel('Actual')\nplt.title('Confusion matrix')\nplt.show()","05c3f5f4":"print(X_train.shape,y_train.shape)","48b9ecc7":"smote = SMOTE(\"minority\")\nX_sm , y_sm = smote.fit_resample(X_train,y_train)","9e47cb62":"print(X_sm.shape,y_sm.shape)","95330e28":"ls= LinearSVC()\nls.fit(X_sm, y_sm)\n\n\n#confusion matrix and classification_report\ny_predsm = ls.predict(X_test)\n\nprint(confusion_matrix(y_test,y_predsm))\n\nprint('\\n\\nAccuracy score: ' + str(accuracy_score(y_test, y_predsm)))\nprint(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test,y_predsm,target_names=['Anti', 'Neutral','Pro','News']))","59e70ad4":"sentiment_code = {1:'Pro', 2:'News', 0:'Neutral', -1:'Anti'}","0a329917":"train['sentiment_code'] = train['sentiment'].map(sentiment_code)","50a5fb67":"aux_train = train[['sentiment', 'sentiment_code']].drop_duplicates().sort_values('sentiment_code')\nconf_matrix = confusion_matrix(y_test, y_predsm)\nplt.figure(figsize=(12.8,6))\nsns.heatmap(conf_matrix, \n            annot=True,\n            cbar=False,\n            fmt='g',\n            xticklabels=aux_train['sentiment'].values, \n            yticklabels=aux_train['sentiment'].values,\n            cmap=\"Blues\")\nplt.ylabel('Predicted')\nplt.xlabel('Actual')\nplt.title('Confusion matrix')\nplt.show()","d93968f9":"train = pd.read_csv('https:\/\/raw.githubusercontent.com\/Stephane-Masamba\/Team_4_CPT_ML-Classification\/Mikael\/train%20(1).csv')\nprint(train.head())","d6563275":"test = pd.read_csv('https:\/\/raw.githubusercontent.com\/Stephane-Masamba\/Team_4_CPT_ML-Classification\/Mikael\/test%20(1).csv')\nprint(test.head())\n","b8d8deb5":"sample = pd.read_csv('https:\/\/raw.githubusercontent.com\/Stephane-Masamba\/Team_4_CPT_ML-Classification\/Mikael\/sample_submission.csv')\nprint(sample.head())\n","bce10f6c":"clean_text(train)\nremove_stopwords(train)","dbe107bb":"clean_text(test)\nremove_stopwords(test)","4d120c19":"X_min = train['message']\nX_min\n","5ad46bc4":"y_min = train['sentiment']\ny_min\n","fdaf5469":"X_min = tf_vecto.fit_transform(X_min)\n","979c4102":"#train_test_split\nX_train2,X_test2,y_train2,y_test2 = train_test_split(X_min,y_min,test_size=0.2,random_state=0)","2b3b5a4a":"sgd = SGDClassifier()\n\nsgd_accuracy = accuracy(sgd)\nsgd_accuracy\n\n","30eea9c4":"linear_sv = LinearSVC()\n\nlinear_sv_accuracy = accuracy(linear_sv)\nlinear_sv_accuracy\n","dc2501d6":"linear_sv.fit(X_train2, y_train2)\n","72acb14b":"#confusion matrix and classification_report\ny_pred2 = linear_sv.predict(X_test2)\n\n\nprint(confusion_matrix(y_test1,y_pred1))\n\nprint('\\n\\nAccuracy score: ' + str(accuracy_score(y_test2, y_pred1)))\nprint(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test2,y_pred2,target_names=['Anti', 'Neutral','Pro','News']))","ad5b6a95":"sentiment_code = {1:'Pro', 2:'News', 0:'Neutral', -1:'Anti'}","4bc0e787":"train['sentiment_code'] = train['sentiment'].map(sentiment_code)","a9ba3b5a":"aux_train = train[['sentiment', 'sentiment_code']].drop_duplicates().sort_values('sentiment_code')\nconf_matrix = confusion_matrix(y_test1, y_pred1)\nplt.figure(figsize=(12.8,6))\nsns.heatmap(conf_matrix, \n            annot=True,\n            cbar=False,\n            fmt='g',\n            xticklabels=aux_train['sentiment'].values, \n            yticklabels=aux_train['sentiment'].values,\n            cmap=\"Blues\")\nplt.ylabel('Predicted')\nplt.xlabel('Actual')\nplt.title('Confusion matrix')\nplt.show()","f115d0b6":"# import comet_ml in the top of your file\nfrom comet_ml import Experiment\n    \n# Add the following code anywhere in your machine learning file\nexperiment = Experiment(api_key=\"kyaDe1YHDUV60KbpzF3dVpIuk\",\n                        project_name=\"general\", workspace=\"rachel-ramonyai\")","011f02f1":"f1 = f1_score(y_test, y_predsm,average='macro')\nprecision = precision_score(y_test, y_pred,average='macro')\nrecall = recall_score(y_test, y_pred,average='macro')","aae45a6b":"params = {\"kernel\": 'linear',\n          \"model_type\": \"SVC\",\n          \"stratify\": True\n          }","460acab1":"params = {\n          \"model_type\": \"Best LinearSVC\",\n          \"stratify\": True\n          }\n\nmetrics = {\"f1\": f1,\n           \"recall\": recall,\n           \"precision\": precision\n           }","cd635ca6":"# Log our parameters and results\nexperiment.log_parameters(params)\nexperiment.log_metrics(metrics)","ab55cd01":"experiment.end()","5e5c331b":"There are no missing values in both the train and test datasets.","9934380d":"#*COMET*","f9f0637d":"###Gradient Boosting Classifier","58e7d1b1":"###Features and Modelling","6e41b1ee":"###Random Forest","7e8c58a7":"A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. ... If None, then the base estimator is a decision tree.","1843e0f6":"##Plot all models and respective accuracy scores","1fd2d00f":"Here we add 'LINK' and 'USER_REF' to our list of stop words and remove all stop words from each tweet","f6dec562":"The objective of a Linear SVC (Support Vector Classifier) is to fit to the data provided, returning a \"best fit\" hyperplane that divides, or categorizes, data. After getting the hyperplane,the features are fed to the classifier to see what the \"predicted\" class is. ","58163294":"Below is a visual of the confusion matrix","c0aec90c":"###Sentiment Classes","547b9a79":"SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes","b19aa3c5":"#                                       *Climate* *Change* *Belief* *Analysis*\n\n\n\n---\n\n\n\n\n\n\n\n\n\n","599a4b2b":"##Modelling with resampled data\n","5e40cf79":"###Word Cloud","0951b4d1":"###Extra Trees Classifier","3b3552af":"Below is a graph of all models which allows for simplified comparison\n","d204c930":"###Decision Tree Classifier","4ec3d19d":"Now that the data is balanced, we go ahead and build a model. This time we check the performance with onlyy the two best performing models as per the above graph.","004f1b24":"#Read in the datasets","cbc5d1b3":"###Convert features to machine language","70cc7434":"###Import Libraries","d54cc423":"##Modelling with maximum data cleaning","c09d0542":"###Splitting the data","6a0579a5":"#Modelling","ed5134b0":"Knowing that we are dealing with text data, we decided to first clean the data by making all tweets lower-case, removing punctuation marks and removing white spaces before doing anything else. Also, replacing all links with the word 'LINK' and all user handles with 'USER_REF'","5fe5d158":"###Lemmatize every word in each tweet","95d18943":"###K-Nearest Neighbors (KNN)","19aeb96c":"Clean the data by calling the cleaning functions","2da2a6a7":"#Data Cleaning","e892d640":"The 'Pro' climate change class accounts for 54% of the data set and the remaining 46% is shared amongst the three other classes. This imbalance indicates how biased our model might me thus exploring resampling techniques might be required in order to improve the model accuracy.","7d560331":"Many companies are built around lessening their environmental impact and thus they offer products that are environmentally friendly and sustainable. With that, they would like to determine how people perceive climate change.\n\n\n\n\n","d1d152ed":"In summary, Naive Bayes classifier is a general term which refers to conditional independence of each of the features in the model, while Multinomial Naive Bayes classifier is a specific instance of a Naive Bayes classifier which uses a multinomial distribution for each of the features","e9aad46e":"The decision tree classifier (Pang-Ning et al., 2006) creates the classification model by building a decision tree. Each node in the tree specifies a test on an attribute, each branch descending from that node corresponds to one of the possible values for that attribute.","31a5be43":"#Results and ending comet experiment","aac213f2":"#Conclusion","04edea34":"The function below takes in a model and splits our training data into a train and test set. Once this is done, the function calculates the accuracy score for both the train and test set and returns the avergace accuracy score for both.","565444c4":"#Exploratory Data Analysis","d1a21f59":"Our model predicted the news category best, while the other categories did improve, with our model being able to predict anti sentiment the second best. Although only more accurate by a couple of percent. So overall the model performed as anticipated.\n\nThrough the classification techniques, companies will be able to  access a broad base of consumer sentiments, spanning multiple demographic and geographic categories, this will increase their insights and inform  strategies.\n\nThis includes adding to their market research efforts in gauging how their product or service may be received.\n\nBased on the findings it can be concluded that more customers will be receptive to eco-friendly products and services provided by companies. \n\nThe results prove that more people believe that climate change is a threat to our lives and that it is up to us to reduce our carbon footprint.\n","4b8d072c":"###Classes and description\n\nClass 2 : News-the tweet links to factual news about climate change\n\nClass 1 : Pro-the tweet supports the belief of man-made climate change\n\nClass 0 : Neutral-the tweet neither supports nor refutes the belief of man-made climate change\n\nClass -1 : Anti-the tweet does not believe in man-made climate change","936e9e02":"###Variables definitions\n\nSentiment: Sentiment of tweet\n\nMessage: Tweet body\n\nTweetid: Twitter unique id","b0c10af9":"An extra-trees classifier is a class that implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The number of trees in the forest.","655a1e98":"The following are some of the few classification techniques explored in this notebook:\n\n\n1.Support Vector Machines\n\n2.Naive Bayes Classfier\n\n3.K-Nearest Neighbors\n\n4.Random Forest\n\n5.Tree based models","e806329a":"#Imports","0ead6372":"###Linear SVC","67fcd749":"This time we only call one cleaning function which is the clean_text() function. This does not include tokenization as well as lemmatization","99f50e44":"#Model Analysis and Insights","7faf0898":"###Convert to machine language","4cb52356":"###Find all named entities for each tweet","b7e36f35":"Because this is **sentiment analysis**,the interpretation and classification of emotions within text data, we will use some natural language preprocessing techniques libraries like nltk,spacy together with scikit learn.","820077bb":"Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.","2d1e0965":"###Stochastic Gradient Descent (SGD)","fb0995af":"###Stochastic Gradient Descent","815ca0d8":"#Introduction","dcdad32e":"To conclude the modelling section, we will explore what one could have done by intuition. This is just to check how accurate our model would be if we did not introduce the 'fancy' cleaning functions.","a2cda4c3":"The Bernoulli naive Bayes classifier assumes that all our features are binary such that they take only two values (e.g. a nominal categorical feature that has been one-hot encoded)","0307d66c":"###Bernoulli Naive Bayes","18caaa9b":"###Missing values","e91c0814":"###Fit best models and use the metrics module","a6f70fd2":"###Downsampling","def115c2":" SGD is a simple,efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression.It has received a considerable amount of attention just recently in the context of large-scale learning.","6eeaa074":"##Modelling with minimal data cleaning","477a79e7":"We worked with various models, and at first the linear SV model and the SGD classifier performed the best, with around 70% accuracy. \n\nWe then decided to fit these two models and increase their performance. Both of the models achieved different precision,recall & F1 scores.\n\nWith the SV model performing better with precision, but acheiving an accuarcy of 72%. While the SGD model achieved better scores in the other categories, however both models achieved the same accuracy at first.\n\nThe last model we made after extensive cleaning achieved a lower accuracy by 1%, but the predictions between anti,pro,neutral and news were closer together. Without a clear leader. This better reflects the real world findings.\n\n\nThe 'Pro' climate change class accounts for 54% of the data set and the remaining 46% is shared amongst the three other classes. \n\nThis 54% count for 'Pro' class means more people agree that climate change is a real threat to our eco-system. Companies that offer products and services which are environmentally friendly and sustainable are more likely to receive support from people based on the shared sentiments and beliefs about climate change. \n\nThe market reasearch done by these companies has positive results which is an indication for prospects of goood profit margins for eco-friendly products and services.\n\nAccuracy on holdout data set is always lower and this shows that our model tends to be slightly overfitting the training data...best performing model is linear svc.\n\n","1a14a159":"In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method proposed by Thomas Cover used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. ... In k-NN classification, the output is a class membership.","cfe283d2":"#Define our feature and target variable to use when modelling","e38982f4":"The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.","fb557ee3":"Now it is time to analyse the data and how it is structured.","84757323":"#Data description","376b32ac":"Below we show some of the words used in the different tweets under each class.","a40b2d51":"###SMOTE ","7dd5eef2":"Since there has already been an observation that the data is imbalanced, one must attempt improving the accuracy of the above modelling. \n\n\nThis will be done by introducing resampling techniques, Downsampling and SMOTE.","49c45ba3":"###Multi Nomial Naive Bayes","fc4eaf57":"###Bagging","4a3b0518":"This notebook aims to build Machine Learning Classification model(s) that is able to classify accurately whether or not a person believes in climate change based on their novel tweet.\n \n\nThis will be done by importing necesarry libraries as well as the training and test datasets. Data cleaning follows together with exploratory data analysis.We then wrap up the notebook by diving into different classification techniques under the Modelling section which will be followed by insights and a conclusion.\n\n\n","28313acd":"###Support Vector Machines(SVM)","6faaef86":"The data available aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. \n\nIn total, 43943 tweets were collected. \n\nEach tweet is labelled as one of four classes.\n\n","50802b82":"Using data from the first data split","32c904f1":"###Linear SVC"}}