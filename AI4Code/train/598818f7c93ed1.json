{"cell_type":{"4d1933de":"code","9839d88e":"code","a16db006":"code","813e479f":"code","af6d1228":"code","2e2fe1df":"code","2e105dfa":"code","f030cd7a":"code","0ea22a91":"code","00d6a674":"code","6b788a1a":"code","00c9b84e":"code","ffc7428b":"code","cb7ce071":"code","627be1bc":"code","94bcdfe5":"code","0461ff34":"code","a4d4ee82":"code","359ac9df":"code","11d55c7d":"code","1b7e72e4":"code","bbfbf898":"code","c31a9723":"code","c4029a9f":"code","c15961c8":"code","8d2d70ce":"code","98e496df":"code","3d886503":"code","f7edffee":"code","cba09a7b":"code","ad63efe3":"code","d76c75cf":"code","d62036d0":"code","7c5b70a9":"code","6e740c07":"code","247bc8e3":"code","60cd6d8a":"code","ffe32463":"code","8bff2a9f":"code","8064cc33":"code","70948af9":"code","501f03d4":"code","893a0ff5":"code","c2276c47":"code","95f35da0":"code","74428edc":"code","1a2589cb":"code","0105cdc6":"code","9dddd554":"code","723b45da":"code","33eeec59":"code","c4ad1093":"code","57c81817":"code","7ea75f14":"code","59281569":"code","29e61a39":"code","c635d585":"code","4fd8862a":"code","54b763e8":"code","109d4955":"code","06261cfd":"code","afd6cb36":"code","0d4a36a5":"code","6d95bea4":"code","ebbbf374":"code","506a579c":"code","058107b2":"code","e34e1434":"code","d6507d99":"code","d0737fcc":"code","edb6696e":"code","45b0da5a":"code","ded5fdab":"markdown","2816a49d":"markdown","4b68894d":"markdown","5ce12645":"markdown","33041814":"markdown","47c7610f":"markdown","5c089cd4":"markdown","71e936f5":"markdown","163ef493":"markdown","b2c6c2f3":"markdown","a9dfc0ee":"markdown","e0dde4f3":"markdown","18886322":"markdown","87de8479":"markdown","eba69bda":"markdown","2a0b9d1a":"markdown","2661dfa6":"markdown","e6097ae2":"markdown","3ff49e43":"markdown","5f5b0912":"markdown","3b93afa6":"markdown","2fd56ded":"markdown","eadb9192":"markdown","6b4385b8":"markdown","e4538e72":"markdown","6142d5dc":"markdown","928e6976":"markdown","957eeb45":"markdown","693633e5":"markdown","87b601e3":"markdown","75459a44":"markdown","8de6a983":"markdown","1d1cf9ef":"markdown","ba0abf44":"markdown","4ec2ed41":"markdown","aecf7208":"markdown","29ad035d":"markdown","f2105a46":"markdown"},"source":{"4d1933de":"# Libs to deal with tabular data\nimport numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\n\n# Statistics\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats.contingency import expected_freq\n\n# Plotting packages\nimport seaborn as sns\nsns.axes_style(\"darkgrid\")\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\n# Machine Learning\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom boruta import BorutaPy\n\nfrom lightgbm import LGBMClassifier\n\n# Optimization\n!pip uninstall optuna -y\n!pip uninstall typing -y\n!pip install optuna==2.3.0\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.visualization import plot_contour, plot_optimization_history\nfrom optuna.visualization import plot_param_importances, plot_slice\n\n\n# To display stuff in notebook\nfrom IPython.display import display, Markdown\n\n# Misc tqdm.notebook.tqdm\nfrom tqdm.notebook import tqdm\nimport time","9839d88e":"train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')","a16db006":"train.sample(10)","813e479f":"train.shape","af6d1228":"train.info()","2e2fe1df":"train.nunique()","2e105dfa":"def chi2_test_cramers_v(var1, var2):\n    cont_freq = pd.crosstab(train[var1], train[var2]).values\n    n_obs = cont_freq.sum().sum()\n    chi2_test = chi2_contingency(cont_freq)\n    cramers_v = np.sqrt(chi2_test[0] \/ (n_obs * (min(cont_freq.shape) - 1)))\n    print(\"Cramer's V:\", cramers_v)\n    print('P-value:', chi2_test[1]) ","f030cd7a":"ax = train['Survived'].replace({\n    0: 'No',\n    1: 'Yes'\n}).value_counts().plot.bar(rot=0)\nax.set_title('Survived', fontsize=16)\nplt.show()","0ea22a91":"train['Survived'].replace({\n    0: 'No',\n    1: 'Yes'\n}).value_counts()\/train.shape[0]","00d6a674":"ax = train['Pclass'].value_counts().sort_index().plot.bar(rot=0)\nax.set_title('Class', fontsize=16)\nplt.show()","6b788a1a":"sns.countplot(data = train, x = 'Pclass', hue = 'Survived')\nplt.show()","00c9b84e":"chi2_test_cramers_v('Pclass', 'Survived')","ffc7428b":"train['Name'].value_counts()","cb7ce071":"ax = train['Sex'].value_counts().plot.bar(rot = 0)\nax.set_title('Sex', fontsize=16)\nplt.show()","627be1bc":"sns.countplot(data = train, x = 'Sex', hue = 'Survived')\nplt.show()","94bcdfe5":"chi2_test_cramers_v('Sex', 'Survived')","0461ff34":"(train['Age']%1).value_counts()","a4d4ee82":"sns.histplot(train['Age'])\nplt.title('Age', fontsize=16)\nplt.xlabel('Years')\nplt.show()","359ac9df":"train['Age'].describe()","11d55c7d":"train['Age'].corr(train['Survived'])","1b7e72e4":"sns.boxplot(data=train, x = 'Survived', y='Age')","bbfbf898":"train.loc[train['Age'].isnull(), 'Survived'].value_counts(normalize=True).sort_index()","c31a9723":"train['age_null'] = train['Age'].isnull()\nchi2_test_cramers_v('age_null', 'Survived')","c4029a9f":"ax = train['SibSp'].value_counts().sort_index().plot.bar(rot=0)\nplt.title('Number of siblings or spouse', fontsize=16)\nplt.show()","c15961c8":"train['SibSp'].corr(train['Survived'], 'spearman')","8d2d70ce":"sns.countplot(data = train, x = 'SibSp', hue = 'Survived')\nplt.show()","98e496df":"ax = train['Parch'].value_counts().sort_index().plot.bar(rot=0)\nplt.title('Number of parents or children', fontsize=16)\nplt.show()","3d886503":"train['Parch'].corr(train['Survived'], 'spearman')","f7edffee":"sns.countplot(data = train, x = 'Parch', hue = 'Survived')\nplt.show()","cba09a7b":"train['Ticket'].sample(10)","ad63efe3":"train['Ticket'].value_counts().head(10)","d76c75cf":"# number of tickets with only numbers\ntrain['Ticket'].notnull().sum() - train['Ticket'].str.contains('[^\\d]').sum()","d62036d0":"train['ticket_null'] = train['Ticket'].isnull()\nchi2_test_cramers_v('age_null', 'Survived')","7c5b70a9":"train.loc[train['ticket_null'].eq(True), 'Survived'].value_counts(normalize=True).sort_index()","6e740c07":"sns.histplot(train['Fare'])\nplt.title('Fare', fontsize=16)\nplt.xlabel('Money')\nplt.show()","247bc8e3":"train['Fare'].describe()","60cd6d8a":"sns.histplot(train['Fare'].apply(np.log10))\nplt.title('log(Fare)', fontsize=16)\nplt.xlabel('Money')\nplt.show()","ffe32463":"sns.boxplot(data=train, x = 'Survived', y='Fare')","8bff2a9f":"train['Fare'].corr(train['Survived'], 'spearman')","8064cc33":"train['cabin_number'] = train['Cabin'].str.extract('(\\d+)').astype('float64')\ntrain['cabin_letter'] = train['Cabin'].str.extract('([A-Za-z])')\ntrain['cabin_null'] = train['Cabin'].isnull()","70948af9":"train.loc[train['Cabin'].notnull(), 'Cabin'].sample(10)","501f03d4":"train['Cabin'].value_counts().describe([.25, .5, .75, .9])","893a0ff5":"pd.crosstab(train['cabin_null'], train['Survived']).div(train['cabin_null'].value_counts().sort_index(), axis='rows')","c2276c47":"chi2_test_cramers_v('cabin_null', 'Survived')","95f35da0":"train['cabin_letter'].value_counts().sort_index().plot.bar(rot=0)\nplt.title('Cabin letter', fontsize=16)\nplt.show()","74428edc":"sns.countplot(data = train, x = 'cabin_letter', hue = 'Survived')\nplt.show()","1a2589cb":"chi2_test_cramers_v('cabin_letter', 'Survived')","0105cdc6":"sns.histplot(train['cabin_number'])","9dddd554":"sns.boxplot(data=train, x = 'Survived', y = 'cabin_number')","723b45da":"train['cabin_number'].corr(train['Survived'])","33eeec59":"ax = train['Embarked'].replace({\n    'S': 'Southampton',\n    'C': 'Cherbourg',\n    'Q': 'Queenstown'\n}).value_counts().plot.bar(rot=0)\nax.set_title('Embarked', fontsize=16)\nplt.show()","c4ad1093":"sns.countplot(data = train, x = 'Embarked', hue = 'Survived')\nplt.show()","57c81817":"chi2_test_cramers_v('Embarked', 'Survived')","7ea75f14":"train['embarked_null'] = train['Embarked'].isnull()\nchi2_test_cramers_v('embarked_null', 'Survived')","59281569":"# Drop useless columns or the ones that are hard to input in the model \ntrain = train.drop(columns = [\n    'PassengerId', 'Name', 'Cabin', 'cabin_number', 'Ticket',\n    'cabin_null', 'age_null', 'ticket_null', 'embarked_null'\n])\n\n# Replace missing values of cabin letter with a new category M, standing for missing\ntrain['cabin_letter'] = train['cabin_letter'].fillna('M')\n\n# Applying transformations\ntrain['Fare'] = train['Fare'].apply(np.log10)\n\ny_train = train['Survived']\n\nx_train_raw = train.drop(columns=['Survived'])\n\nx_train_cat_codes = x_train_raw.copy()\nencoding_dict = {}\nfor col in ['Sex', 'Embarked', 'cabin_letter']:\n    encoding_dict[col] = {val: idx for idx, val in enumerate(x_train_cat_codes[col].unique())}\n    x_train_cat_codes[col] = x_train_cat_codes[col].replace(encoding_dict[col])\n\nx_train_filled = x_train_cat_codes.copy()\nmedian_imp = SimpleImputer(strategy='median').fit(x_train_filled.values[:,[2, 5]])\nmode_imp = SimpleImputer(strategy='most_frequent').fit(x_train_filled.values[:,[6]])\nx_train_filled.iloc[:,[2, 5]] = median_imp.transform(x_train_filled.values[:,[2, 5]])\nx_train_filled.iloc[:,[6]] = mode_imp.transform(x_train_filled.values[:,[6]])","29e61a39":"x_train_cat_codes","c635d585":"x_train_cat_codes","4fd8862a":"def cramers_v(var1, var2):\n    cont_freq = pd.crosstab(var1, var2).values\n    n_obs = cont_freq.sum().sum()\n    chi2_test = chi2_contingency(cont_freq)\n    cramers_v = np.sqrt(chi2_test[0] \/ (n_obs * (min(cont_freq.shape) - 1)))\n    return cramers_v","54b763e8":"train[['Pclass', 'Sex', 'Embarked', 'cabin_letter']].astype(\"category\").apply(lambda x: x.cat.codes).corrwith(train['Survived'], method = cramers_v)","109d4955":"train[['Age', 'SibSp', 'Parch', 'Fare']].corrwith(train['Survived'], method = 'spearman')","06261cfd":"rf = RandomForestClassifier(n_jobs=-1, max_depth=5, random_state=42)\nfeat_selector = BorutaPy(\n    rf, \n    verbose=2, \n    random_state=42,\n    n_estimators = 'auto', \n    two_step=False\n).fit(x_train_filled.values, y_train.values)","afd6cb36":"mutu_info = mutual_info_classif(x_train_filled, y_train, discrete_features=[0, 1, 3, 4, 6, 7])\nmutu_info = pd.Series(mutu_info, index=x_train_filled.columns).sort_values(ascending=False)\n\nsns.barplot(x = mutu_info.values, y = mutu_info.index, color='cornflowerblue')\nplt.title('Mutual information', fontsize=16)\nplt.show()","0d4a36a5":"class Light_GBM_RF_CV:\n    def __init__(self, x, y, folds=5, random_state=42):\n        # Hold this implementation specific arguments as the fields of the class.\n        self.x = x\n        self.y = y\n        self.folds = folds\n        self.random_state = random_state\n\n    def __call__(self, trial):\n        cv = KFold(\n            self.folds, \n            random_state = self.random_state, \n            shuffle=True\n        )\n        \n        clf = LGBMClassifier(\n            # task params\n            boosting_type = 'rf',\n            objective = 'binary',\n            metric = 'auc',\n            random_state = self.random_state,\n            n_jobs = -1,\n            # controlling tree growth\n            num_leaves = trial.suggest_int('num_leaves', 16, 256),\n            max_depth = trial.suggest_int('max_depth', 4, 8),\n            min_child_samples = trial.suggest_int('min_child_samples', 5, 1000),\n            # random forest params\n            n_estimators = trial.suggest_int('n_estimators', 10, 500),\n            subsample = trial.suggest_float('subsample', 0.5, 1),\n            subsample_freq = 1,\n            colsample_bytree = 1,\n            feature_fraction_bynode = trial.suggest_float('feature_fraction_bynode', 0.1, 1),\n            # learning parameters\n            learning_rate = 1,\n            reg_alpha = trial.suggest_loguniform('reg_alpha', 1e-5, 1.0),\n            reg_lambda = trial.suggest_loguniform('reg_lambda', 1e-5, 1.0),\n            # features params\n            max_bin = trial.suggest_int('max_bin', 50, 256)\n        )\n        \n        scores = []\n\n        for array_idxs in cv.split(self.x):\n            train_index, val_index = array_idxs[0], array_idxs[1]\n            x_train, x_val = self.x[train_index], self.x[val_index]\n            y_train, y_val = self.y[train_index], self.y[val_index]\n            \n            # Filling missing values\n            median_imp = SimpleImputer(strategy='median').fit(x_train[:,[2, 5]])\n            mode_imp = SimpleImputer(strategy='most_frequent').fit(x_train[:,[6]])\n            x_train[:,[2, 5]] = median_imp.transform(x_train[:,[2, 5]])\n            x_train[:,[6]] = mode_imp.transform(x_train[:,[6]])\n            x_val[:,[2, 5]] = median_imp.transform(x_val[:,[2, 5]])\n            x_val[:,[6]] = mode_imp.transform(x_val[:,[6]])\n\n            clf.fit(x_train, y_train, verbose = False)\n            scores.append(roc_auc_score(y_val, clf.predict_proba(x_val)[:,1]))\n\n        return sum(scores) \/ self.folds","6d95bea4":"lgbm_cv = Light_GBM_RF_CV(x_train_cat_codes.values, y_train.values)\nstudy = optuna.create_study(sampler=TPESampler(seed = 42), direction='maximize')\nstudy.optimize(lgbm_cv, n_trials=100)","ebbbf374":"print('Best model')\nprint('***********')\nprint('Mean validation AUC: ', study.best_value, '\\n')\nprint('Best hyperparameters')\nprint('***********')\nfor param, val in study.best_params.items():\n    print(param + ':', val)","506a579c":"plot_optimization_history(study)","058107b2":"plot_param_importances(study)","e34e1434":"lgbm_final_clf = LGBMClassifier(\n    boosting_type = 'rf',\n    objective = 'binary',\n    metric = 'auc',\n    random_state = 42,\n    n_jobs = -1,\n    subsample_freq = 1,\n    colsample_bytree = 1,\n    learning_rate = 1,\n    **study.best_params\n)\n\nlgbm_final_clf.fit(\n    x_train_filled.values, \n    y_train,\n    verbose = False\n)","d6507d99":"perm_importances = permutation_importance(\n    lgbm_final_clf,\n    x_train_filled,\n    y_train,\n    scoring = 'roc_auc',\n    n_repeats = 10,\n    n_jobs = -1\n)\ndf_perm_importances = pd.DataFrame(perm_importances.importances, index=x_train_filled.columns).T\ndf_perm_importances = df_perm_importances.melt(\n    value_vars = df_perm_importances.columns,\n    var_name = 'feature',\n    value_name = 'importance'\n)\n\nplt.figure(figsize=(10,5))\nsns.boxplot(data = df_perm_importances, x = 'importance', y = 'feature')\nplt.title('Permutation importance using AUC (train set)', fontsize=16)\nplt.show()","d0737fcc":"test = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')","edb6696e":"test['cabin_letter'] = test['Cabin'].str.extract('([A-Za-z])')\ntest['cabin_letter'] = test['cabin_letter'].fillna('M')\ntest = test.drop(columns = ['Name', 'Cabin', 'Ticket'])\ntest['Fare'] = test['Fare'].apply(np.log10)\n\nfor col in ['Sex', 'Embarked', 'cabin_letter']:\n    test[col] = test[col].replace(encoding_dict[col])\n\ntest.iloc[:,[2, 5]] = median_imp.transform(test.values[:,[2, 5]])\ntest.iloc[:,[6]] = mode_imp.transform(test.values[:,[6]])","45b0da5a":"submission_df = pd.concat([\n    test['PassengerId'], \n    pd.Series(lgbm_final_clf.predict(test.drop(columns='PassengerId').values), name='Survived')\n], axis=1, copy=False)\n\nsubmission_df.to_csv('.\/submission.csv', index=False)","ded5fdab":"Notice that almost 5 thousand passengers doesn't have a ticket. One hypothesis is that these people are employees of the cruise ship. Looking at the relationship between suviving and lack of tickets, it's not strong but, according to the chi2 test, they are dependent.","2816a49d":"After 100 trials, we got an AUC of almost 0.85, which is good. Regarding our hyperparameters, the best combination resulted in a set of 218 trees of max depth equal to 8. Notice that both regularization parameters ended up being close to 0, indicating that they didn't help our algorithm. Bagging and feature fraction also had similiar values of about 0.65, that is, in each tree we use 65% of our training samples and in each node split we consider only 65% of the available features.\n\nBelow we can see two graphs showing how was our progress and which hyperparameters were the most important while tuning the random forest mode.","4b68894d":"## EDA + Light GBM - Tabular Series Apr 2021\n\nIn this kernel I will explore and create a predictive model with the T\n\n## Import libs and data","5ce12645":"Mutual information is a good way to assess feature importance regardless of the variable type, so that we can put categorical and numerical variables under the same scale. However we see that the overall ordering of impotance didn't change significantly.\n\n## Modelling\n\nIn this section I will be using Light GBM with Optuna, which is an automatic hyperparameter search tool. Differently from the standard use of the Light GBM library, I'm going to experiment with the random forest mode. Despite it's a poorly documentated mode, it's equivalent to a boosting algorithm with 1 estimator and a number of parallel trees. The learning (shrinkage) rate is fixed to 1 and we must set the parameters that control the size of the training set to use in each tree and the number of features to be used in each split.\n\nBelow we specify a range of values for a the model hyperparameters and use Optuna to explore which combination yields the best result of the metric AUC.","33041814":"In this part the order of importance changed a bit from the correlation coefficients and the mutual information values. For example, according to the permutation importance, age is the least important feature. \n\n### Submission\n\nNow let's read the test set and make predicitions using the standard threshold of 0.5 to binary classification. Of course we could fine tune this value to increase our accuracy.","47c7610f":"#### Survived","5c089cd4":"#### Cabin\n\nCabin has the type object and can be split in two parts: the letter and the number. One problem of this variable is the high number of missing values. Notice that the same cabin can have multiple passengers.","71e936f5":"## Feature selection\n\nIn this section I will test a number of options regarding feature selection. \n\n- Correlation coefficients\n- Boruta framework\n- Mutual information\n\n### Correlation coefficients\n\nFirst, let's put the correlation coefficients in a single table and compare them. To do so, I will have to change the Cramer's V function a little bit.","163ef493":"One solution to better visualize and understand the variable is to apply the log function. Notice how the peaks are more evident.","b2c6c2f3":"The variable seems important for prediction as it has a chi2 correlation of 0.35. Regarding missing values, they don't have a predictive power. ","a9dfc0ee":"Using a chi2 independency test we see that chances are 0 that this pair of variables is independent. Also, computing the Cramer's V correlation coefficient we get a modest value, which indicates that there is correlation but it's not that high.\n\n#### Name\n\nName is a text column that at first sight don't give us much information wether a passenger will survive or not. Thus, I'll drop it before start modelling.","e0dde4f3":"Also, the Spearman coefficient is higher than the other coefficients analyzed so far.","18886322":"#### Sex\n\nAnalyzing the variable sex, we conclude that survived is very dependent on the sex. Although male has a higher frequency, womans are much more prone to survive.","87de8479":"### Retraining with the best parameters\n\nNow I need to retrain the model with the best configuration and the whole dataset in order to evaluate feature importance and make predictions.","eba69bda":"Although it can have letters, most tickets (over 70,000) have only numbers.","2a0b9d1a":"Regarding the dependent variable, the variable has a little bit higher Spearman correlation than the number of siblings and spouse, but it's still very low. ","2661dfa6":"### Data preparation\n\nIn this section I will make a simple data preparation, so that we can create a simple model without fancy feature engineering.","e6097ae2":"By analyzing the graphs below it's clear that having a cabin often implies in surviving the accident. The Cramer's V is also high.","3ff49e43":"All categorical features have a correlation coeffient greater than 0.3, which is a moderate result and it's not negligible. But the numerical features doesn't have a high correlation even when we use the Spearman coefficient, which analyzes ranks instead of raw values. One variable we could drop immediatly is SibSp, which is very close to 0. But for rest, I think it's best to look at other methods to get another perspective.\n\n### Boruta\n\nBoruta is an all-relevant feature selection algorithm based on random forests, feature importance and random variables. In our context, Boruta is a usefull algorithm because it's capable of evaluating the relationship between more than one feature and the target. Thus, we are not limited to assess importance only with bivariate analysis between a feature and our target. \n\nIt works by first creating a shadow variable for each input feature. This shadow variable is a permuted version of the original one, so that it has the distribution but no correlation with the target. Then, a random forest is fitted using both original and shadow features. The algorithm's key ideia is that important features are those that have an importance metric greater than the importance of the best shadow feature. Such important features are recorded with a \"hit\" and this process of creating shadow features and fitting RFs is repeated a number of times. Repetition is important because the algorithm is random in its nature. \n\nAfter that, we will have a list of how many times each variable was a hit and we can use the binomial distribution to assess the probability of getting an amount of hits merely by chance. The variables with low probabilities are considered important and the ones with high probabilities are discarded. The variables with probability between the low and high thresholds are considered tentative and it's up to the user to discard or utilize them. \n\n","5f5b0912":"### Feature importance\n\nTo assess feature importance I'm going to use a method called permutation importance. It evaluates the importance of a variable by shuffling it and computing how much the objective metric reduces. This process is repeated to each variable a number of times to generate a confidence interval.  ","3b93afa6":"#### Ticket\n\nTicket is a mixed variable because it can have numbers as well as letters. Also, we have 75,331 unique tickets, which means that there are a lot of people who hold identical tickets.","2fd56ded":"#### Siblings or spouse\n\nOver 90% of the passengers have at most one sibling or spouse.","eadb9192":"Now analyzing the cabin letters, \"A\", \"B\" and \"C\" are the most popular types.","6b4385b8":"Regarding the dependent variable, we can draw two conclusions:\n- People who survived are older. The means of the distributions shift just a bit. \n- Correlation is 0.10, which is low.\n- The presence of missing rows doesn't influence the distribution of survived.","e4538e72":"As we can see, Boruta considered all features relevant to the problem. So, even though the number of siblings and spouse has a low correlation with the target, it has an interaction with other variables which makes it important to predict the target.\n\n### Mutual information\n\nMutual information is a metric from the information theory field which assess much much the observed joint distribution deviates from an independent joint distribution (assuming both variables are independent). Values close to 0 indicate that the distributions are almost independent.","6142d5dc":"#### Fare\n\nAs we can see below, fare is heavily skewed, so that most people spent little money and a few of them spent a lot.","928e6976":"Notice that the first and second class are almost equal in frequency while the third class, which is probably the cheaper one, has more passengers than the others. When we compare this variable to the frequency of the positive event, it's clear that being in the first two classes gives passengers an advantage in terms of survival. On the other hand, if a passenger is in the third class, it will likely not survive.","957eeb45":"Again our chi2 test indicates dependency and a correlation of 0.5.\n\n#### Age\n\nAge has about 3400 missing values and a few estimated ages. According to the metadata, rows with decimal ages are estimated. Below we can see that most ages aren't estimates, but there is 1100 rows with this aspect. ","693633e5":"### Analyzing distributions and characteristics\n\nHere I analyze each variable individually and with respect to the target. Before we start, I define an important function to measure the Pearson's Chi2 correlation coefficient and assess the strenght of the relationship between two categorical variables.","87b601e3":"Finally, as we expected, the cabin number is just random and the Pearson correlation coefficient is close to 0.","75459a44":"One could say that having a sibling or spouse would help in surviving because you have someone to rely on. On the other hand, it could be bad because you have to worry about the other person. As we can see below by the spearman correlation, none of these hypothesis is true because the correlation is very close to 0. ","8de6a983":"The distribution doesn't look like a normal curve. Instead age is very spread and there are some peaks around 8, 25 and 55 years. Also, some bins of the histogram are more populated than its neighbors. ","1d1cf9ef":"Analyzing the graph below, people who survived tend to pay more expensive fares. The distribution of the people who survived is more skewed than the other.","ba0abf44":"## Data analysis and preparation\n\nBefore we start analyzing individual variable, it's essential to take a look at the overall features of the dataset.","4ec2ed41":"#### Embarked\n\nThe two most popular embaking options are Southampton and Cherbourg.","aecf7208":"Notice that our dataset is a little bit unbalanced. When we have this kind of situation, we can create a new sample from this dataset where the frequency of survived is more similar to the negative event one. This technique is very usefull when the positive class corresponds to 10% or less of the dataset. In this case, since the it's not so severe, we can keep it untouched.\n\n#### Passenger class ","29ad035d":"#### Parents or children\n\nMost people don't have a parent or children on board, but when compared to the variable above the number of parents and children seems to be shifted and there is a considerable group of people with 2 relatives.","f2105a46":"First, analyzing the relationship between missing cabin and suviving, we see that the Cramer's V is modest, but high compared to the other variables. That's because people who have a cabin are much more prone to survive. "}}