{"cell_type":{"7d4958f3":"code","ef208962":"code","3129478b":"code","3d533959":"code","0766a9c2":"code","d2cb1772":"code","cb951fe9":"code","b17fcd2f":"code","9ea98c14":"code","73db5a80":"code","af8cad5e":"code","54d22f8d":"code","e3e1d5ff":"code","afe7a8e7":"code","71d7f08d":"code","59717b5e":"code","e45762f7":"code","4a34b981":"code","b4d99b64":"code","d11f4869":"code","0678550b":"code","1a7823ae":"code","d46caafc":"code","d29f1bbc":"code","691d6911":"code","5aa21f1c":"code","8d2080e6":"code","b66dac72":"code","54d7e870":"code","3728d723":"code","0f34826d":"code","14fb7049":"code","c5d63b82":"code","5cf0f28d":"code","9919d1bf":"code","347f1421":"code","b25cdfc2":"code","c8319ad9":"code","6fc1ae64":"code","cf2bb0d6":"markdown","d87f1296":"markdown","a3af1927":"markdown","c208798c":"markdown","2f663c06":"markdown","d64cfe72":"markdown","40cdf4ee":"markdown","a843e88f":"markdown","ba8f8dd0":"markdown","e261160e":"markdown","dae23111":"markdown","d970e10b":"markdown","2cd2cbc8":"markdown","52d81997":"markdown","2cbd882a":"markdown","ce3ed95c":"markdown","7a9e2889":"markdown","0df57a49":"markdown","6fab2e42":"markdown","96d17e28":"markdown","b1028c43":"markdown","30bafbb3":"markdown","29d9c8b7":"markdown","59a5e970":"markdown","d8ce2a91":"markdown","668a16b6":"markdown","8b06e4a5":"markdown","d7541cc3":"markdown","76332225":"markdown","84f70664":"markdown"},"source":{"7d4958f3":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport category_encoders as ce\nimport os\nimport plotly.figure_factory as ff\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ef208962":"df = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\nfig = ff.create_table(df.head())\nfig.show()","3129478b":"coun_sum = df.isnull().sum().values\npercent = (df.isnull().sum() \/ df.shape[0]).values\nnull_ = pd.DataFrame()\nnull_[\"counts\"] = coun_sum\nnull_['percent_null'] = percent\nnull_.index = df.columns\nnull_","3d533959":"def feature_plot(data,target,feature,cat=True,text=True):\n    \n    df = data.copy()\n    backcolor='#EAEAEA'\n    if cat == True:\n        palette_1 = [\"#F8003D\",\"#08D9D6\",\"#a1d4e2\"]\n        sns.set_palette(palette_1)\n        sm = df.shape[0]\n        fig,ax = plt.subplots(1,2,figsize=(12,6))\n        g = sns.countplot(data=df,x=feature,hue=target,edgecolor=\"black\",linewidth=3,ax=ax[0])\n        g.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        g.set_ylabel(feature+\"_counts\", weight='bold', size=13,alpha=0.65)\n        g.set_facecolor(backcolor)\n        g.spines[['top', 'right','bottom']].set_visible(False)\n        g.set_title(feature+\"_counts and percent\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        if  text == True:\n            for patch in g.patches:\n                x, height, width = patch.get_x(), patch.get_height(), patch.get_width()\n                g.text(x + width \/ 2, height + 10, f'{height} \/ {height \/ sm * 100:2.2f}%', va='center', ha='center', size=10, alpha=0.5,bbox={'facecolor': 'w', 'boxstyle': 'round4'}\n          )\n            \n        dt = df[feature].value_counts(normalize=True).reset_index()\n        p = sns.barplot(data=dt,x=\"index\",y=feature,edgecolor=\"black\",linewidth=3,ax=ax[1])\n        sns.set_palette(palette_1)\n        p.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        p.set_ylabel(feature+\"_counts\", weight='bold', size=13,alpha=0.65)\n        p.set_facecolor(backcolor)\n        p.spines[['top', 'right','bottom']].set_visible(False)\n        p.set_title(feature+\"_percent\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        \n        plt.show()\n    \n    else:\n        fig,ax = plt.subplots(1,2,figsize=(10,6))\n        g = sns.kdeplot(data=df,x=feature,hue=target,ax=ax[0])\n        g.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        g.set_ylabel(feature+\"_KDE\", weight='bold', size=13,alpha=0.65)\n        g.set_facecolor(backcolor)\n        g.spines[['top', 'right','bottom']].set_visible(False)\n        g.set_title(feature+\"_KDE_FIGURE\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        \n        p=sns.boxplot(y=df[feature],ax=ax[1])\n        p.set_facecolor(backcolor)\n        p.set_xlabel(feature)\n        p.set_ylabel(feature+\"_VALUES\")\n        p.set_title(feature+\"_BOX\")\n        plt.show()","0766a9c2":"feature_plot(df,\"Survived\",\"Embarked\")","d2cb1772":"feature_plot(df,\"Survived\",\"Pclass\")","cb951fe9":"feature_plot(df,\"Survived\",\"Sex\")","b17fcd2f":"feature_plot(df,\"Survived\",\"Fare\",cat=False)","9ea98c14":"feature_plot(df,\"Survived\",\"Age\",cat=False)","73db5a80":"df[df['Age'] < 0]","af8cad5e":"feature_plot(df,\"Survived\",\"SibSp\",text=False)","54d22f8d":"feature_plot(df,\"Survived\",\"Parch\",text=False)","e3e1d5ff":"df['Age'] = df['Age'].fillna(df[\"Age\"].mean())\ndf_test['Age'] = df_test['Age'].fillna(df[\"Age\"].mean())","afe7a8e7":"df[\"Number_companions\"] = df[\"SibSp\"] + df[\"Parch\"]\ndf_test[\"Number_companions\"] = df_test[\"SibSp\"] + df_test[\"Parch\"]","71d7f08d":"df[\"Cabin_type\"] = df[\"Cabin\"].apply(lambda x:str(x)[0])\ndf_test[\"Cabin_type\"] = df_test[\"Cabin\"].apply(lambda x:str(x)[0])\n\ndf[\"Name_split\"] = df[\"Name\"].apply(lambda a: a.split(\".\")[0].split(\",\")[1].strip())\ndf_test[\"Name_split\"] = df_test[\"Name\"].apply(lambda a: a.split(\".\")[0].split(\",\")[1].strip())","59717b5e":"df = df.drop(columns=[\"Name\",\"Cabin\",\"Ticket\"])\ndf_test = df_test.drop(columns=[\"Name\",\"Cabin\",\"Ticket\"])","e45762f7":"def age_split(x):\n    if x < 18:\n        return \"Children\"\n    elif  18 <= x < 44:\n        return \"Youth\"\n    elif  44 <= x < 59:\n        return \"middle-aged\"\n    else:\n        return \"elderly\"","4a34b981":"df['Age_type'] = df[\"Age\"].map(age_split)\ndf_test['Age_type'] = df_test[\"Age\"].map(age_split)","b4d99b64":"#woe columns [\"Sex\",\"Plcass\",\"Embarked\",\"Age_type\"]\n#leave_one [\"Cabin_type\",\"Name_split\"]\n#others  [\"SibSp\",\"Parch\",\"Number_companions\",\"Pclass\"]\nwoe_col = [\"Embarked\",\"Age_type\"]\n\ndef Woe_encoder(data,data_test,target,list_col):\n    df = data.copy()\n    df_test = data_test.copy()\n    for col in list_col:\n        woe = ce.WOEEncoder()\n        woe.fit(df[col],df[target])\n        df[col] = woe.transform(df[col])\n        df_test[col] = woe.transform(df_test[col])\n    return df,df_test\ndf,df_test= Woe_encoder(df,df_test,\"Survived\",woe_col)","d11f4869":"le = [\"Cabin_type\",\"Name_split\"]\ndef lea_encoder(data,data_test,target,list_col):\n    df = data.copy()\n    df_test = data_test.copy()\n    for col in le:\n        woe = ce.LeaveOneOutEncoder()\n        woe.fit(df[col],df[target])\n        df[col] = woe.transform(df[col])\n        df_test[col] = woe.transform(df_test[col])\n    return df,df_test\ndf,df_test = lea_encoder(df,df_test,\"Survived\",le)","0678550b":"df = pd.get_dummies(data=df,columns=[\"Sex\"],drop_first=True)\ndf_test = pd.get_dummies(data=df_test,columns=[\"Sex\"],drop_first=True)\n\n\ndf[\"Pclass\"] = df[\"Pclass\"].replace({1:\"p_1\",2:\"p_2\",3:\"p_3\"})\ndf_test[\"Pclass\"] = df_test[\"Pclass\"].replace({1:\"p_1\",2:\"p_2\",3:\"p_3\"})\ndf = pd.get_dummies(data=df,columns=[\"Pclass\"],drop_first=True)\ndf_test = pd.get_dummies(data=df_test,columns=[\"Pclass\"],drop_first=True)\n","1a7823ae":"age_mean = df[\"Age\"].mean()\nage_std = df[\"Age\"].std()\nfare_mean = df[\"Fare\"].mean()\nfare_std = df[\"Fare\"].std()\ndf[\"Age\"] = df[\"Age\"].apply(lambda x:(x-age_mean)\/age_std)\ndf_test[\"Age\"] = df_test[\"Age\"].apply(lambda x:(x-age_mean)\/age_std)\ndf[\"Fare\"] = df[\"Fare\"].apply(lambda x:(x-fare_mean)\/fare_std)\ndf_test[\"Fare\"] = df_test[\"Fare\"].apply(lambda x:(x-fare_mean)\/fare_std)","d46caafc":"plt.figure(figsize=(10,6))\nsns.heatmap(df.corr(method=\"spearman\"),annot=True,cmap=\"Blues_r\")","d29f1bbc":"df = df.drop(columns=[\"Cabin_type\"])\ndf_test =df_test.drop(columns=[\"Cabin_type\"])\ndf = df.drop(columns=[\"Name_split\"])\ndf_test =df_test.drop(columns=[\"Name_split\"])","691d6911":"from sklearn.model_selection import StratifiedKFold,cross_val_score,learning_curve,validation_curve\nfrom sklearn.metrics import recall_score,precision_score,f1_score,ConfusionMatrixDisplay,confusion_matrix\nx = df.drop(columns=[\"PassengerId\",\"Survived\"])\ny = df[[\"Survived\"]]\nids = df_test[[\"PassengerId\"]]\ndf_test = df_test.drop(columns=[\"PassengerId\"])\nstf = StratifiedKFold(n_splits=5,shuffle=True,random_state=123)\nfor train_index,test_index in stf.split(x,y):\n    x_train,y_train = x.iloc[train_index],y.iloc[train_index]\n    x_test,y_test = x.iloc[test_index],y.iloc[test_index]","5aa21f1c":"def learn_curve(model,x,y):\n    train_sizes,train_score,test_score = learning_curve(model,x,y,cv=5,scoring=\"recall\")\n    plt.figure(figsize=(10,6))\n    \n    train_score_mean = np.mean(train_score,axis=1)\n    test_score_mean = np.mean(test_score,axis=1)\n    \n    plt.plot(train_sizes,train_score_mean,linestyle='--',color=\"red\",marker=\"o\",markersize=5,label=\"train_score\")\n    plt.plot(train_sizes,test_score_mean,linestyle=':',color=\"blue\",marker=\"s\",markersize=5,label=\"test_score\")\n    plt.xlabel(\"train_sizes\")\n    plt.ylabel(\"recall_score_mean\")\n    plt.legend()\n    plt.title(\"learning_curve for data_sizes\")\n    plt.show()\n\ndef valid_curve(model,feature,target,pname,pname_range,c,metrics):\n    \n    train_score,test_score = validation_curve(model,feature,target,param_name=pname,param_range=pname_range,cv=c,scoring=metrics)\n    train_score_mean = np.mean(train_score,axis=1)\n    test_score_mean = np.mean(test_score,axis=1)\n    plt.plot(pname_range,train_score_mean,linestyle='--',color=\"red\",marker=\"o\",markersize=5,label=\"train_score\")\n    plt.plot(pname_range,test_score_mean,linestyle=':',color=\"red\",marker=\"o\",markersize=5,label=\"test_score\")\n    plt.xlabel(\"params_sizes\")\n    plt.ylabel(metrics+\"_score\")\n    plt.title(\"validation_\"+pname+\"_curve\")\n    plt.legend()\n    plt.show()\n    ","8d2080e6":"from xgboost import XGBClassifier\nxgb = XGBClassifier(eval_metric=\"logloss\",random_state=123,use_label_encoder=False)\nlearn_curve(xgb,x,y.values.ravel())","b66dac72":"from sklearn.model_selection import RandomizedSearchCV\nparams = {\n    \"n_estimators\":(100,500),\n    \"max_depth\":(4,8),\n    \"learning_rate\":(0.3,0.7),\n    \"subsample\":(0.5,0.8),\n    \"gamma\":(0.1,0.4),\n    \"colsample_bytree\":(0.5,0.8)\n}\n\nrf_xgb = RandomizedSearchCV(xgb,params,scoring=\"recall\",cv=5)\nrf_xgb.fit(x_train,y_train.values.ravel())\nrf_xgb_pre = rf_xgb.predict(x_test)\ncmxgb = confusion_matrix(y_test,rf_xgb_pre)\ndis_xgb = ConfusionMatrixDisplay(confusion_matrix=cmxgb)\nprint(\"best_params:\",rf_xgb.best_params_)\nprint(\"rf_xgb_recall:\",recall_score(y_test,rf_xgb_pre))\nprint(\"rf_xgb_precision:\",precision_score(y_test,rf_xgb_pre))\nprint(\"rf_xgb_f1_score:\",f1_score(y_test,rf_xgb_pre))\nprint(\"---------------------------------learn_curve--------------------------------\")\nprint(learn_curve(rf_xgb,x,y.values.ravel()))\nprint(\"--------------------------------confusion_matrix---------------------------\")\ndis_xgb.plot()\nplt.show()","54d7e870":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(x_train,y_train.values.ravel())\nsvc_pre = svc.predict(x_test)\ncmsvc = confusion_matrix(y_test,svc_pre)\ndis_svc = ConfusionMatrixDisplay(confusion_matrix=cmsvc)\nprint(\"svc_recall:\",recall_score(y_test,svc_pre))\nprint(\"svc_precision:\",precision_score(y_test,svc_pre))\nprint(\"svc_f1_score:\",f1_score(y_test,svc_pre))\nprint(\"---------------------------------learn_curve--------------------------------\")\nprint(learn_curve(svc,x,y.values.ravel()))\nprint(\"--------------------------------confusion_matrix---------------------------\")\ndis_svc.plot()\nplt.show()","3728d723":"# params_scv = {\n#     \"C\":(0.1,0.8),\n#     \"gamma\":(0.1,0.8),\n# }\n\n# rf_svc = RandomizedSearchCV(svc,params_scv,cv=5,scoring=\"recall\")\n# rf_svc.fit(x_train,y_train.values.ravel())\n# rf_svc_pre = rf_svc.predict(x_test)\n# cmsvc = confusion_matrix(y_test,rf_svc_pre)\n# dis_svc = ConfusionMatrixDisplay(confusion_matrix=cmsvc)\n# print(\"best_params:\",rf_xgb.best_params_)\n# print(\"rf_svc_recall:\",recall_score(y_test,rf_svc_pre))\n# print(\"rf_svc_precision:\",precision_score(y_test,rf_svc_pre))\n# print(\"rf_svc_f1_score:\",f1_score(y_test,rf_svc_pre))\n# print(\"---------------------------------learn_curve--------------------------------\")\n# print(learn_curve(rf_svc,x,y.values.ravel()))\n# print(\"--------------------------------confusion_matrix---------------------------\")\n# dis_svc.plot()\n# plt.show()","0f34826d":"from lightgbm import LGBMClassifier\nlgb = LGBMClassifier()\nlgb.fit(x_train,y_train.values.ravel())\nlgb_pre = lgb.predict(x_test)\ncmlgb = confusion_matrix(y_test,lgb_pre)\ndis_lgb = ConfusionMatrixDisplay(confusion_matrix=cmlgb)\nprint(\"lgb_recall:\",recall_score(y_test,lgb_pre))\nprint(\"lgb_precision:\",precision_score(y_test,lgb_pre))\nprint(\"lgb_f1_score:\",f1_score(y_test,lgb_pre))\nprint(\"---------------------------------learn_curve--------------------------------\")\nprint(learn_curve(lgb,x,y.values.ravel()))\nprint(\"--------------------------------confusion_matrix---------------------------\")\ndis_lgb.plot()\nplt.show()","14fb7049":"params = {\n    \"n_estimators\":(100,500),\n    \"max_depth\":(4,8),\n    \"learning_rate\":(0.3,0.7),\n    \"subsample\":(0.5,0.8),\n    \"colsample_bytree\":(0.5,0.8)\n}\n\nrf_lgb = RandomizedSearchCV(lgb,params,scoring=\"recall\",cv=5)\nrf_lgb.fit(x_train,y_train.values.ravel())\nrf_lgb_pre = rf_lgb.predict(x_test)\ncmlgb = confusion_matrix(y_test,rf_lgb_pre)\ndis_lgb = ConfusionMatrixDisplay(confusion_matrix=cmlgb)\nprint(\"best_params:\",rf_lgb.best_params_)\nprint(\"rf_lgb_recall:\",recall_score(y_test,rf_lgb_pre))\nprint(\"rf_lgb_precision:\",precision_score(y_test,rf_lgb_pre))\nprint(\"rf_lgb_f1_score:\",f1_score(y_test,rf_lgb_pre))\nprint(\"---------------------------------learn_curve--------------------------------\")\nprint(learn_curve(rf_lgb,x,y.values.ravel()))\nprint(\"--------------------------------confusion_matrix---------------------------\")\ndis_lgb.plot()\nplt.show()","c5d63b82":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train,y_train.values.ravel())\nrf_pre = rf.predict(x_test)\ncmrf = confusion_matrix(y_test,rf_pre)\ndis_rf = ConfusionMatrixDisplay(confusion_matrix=cmrf)\nprint(\"rf_recall:\",recall_score(y_test,rf_pre))\nprint(\"rf_precision:\",precision_score(y_test,rf_pre))\nprint(\"rf_f1_score:\",f1_score(y_test,rf_pre))\nprint(\"---------------------------------learn_curve--------------------------------\")\nprint(learn_curve(rf,x,y.values.ravel()))\nprint(\"--------------------------------confusion_matrix---------------------------\")\ndis_rf.plot()\nplt.show()","5cf0f28d":"from sklearn.model_selection import GridSearchCV\nparams = {\n    \"max_depth\":np.arange(3,6,1),\n    \"n_estimators\":np.arange(50,300,50),\n    \"min_impurity_decrease\":np.arange(0,0.3,0.1) \n}\n\nrf_rf = GridSearchCV(rf,params,scoring=\"recall\",cv=5)\nrf_rf.fit(x_train,y_train.values.ravel())\nrf_rf_pre = rf_rf.predict(x_test)\ncmrf_rf = confusion_matrix(y_test,rf_rf_pre)\ndis_rf_rf = ConfusionMatrixDisplay(confusion_matrix=cmrf_rf)\nprint(\"best_params:\",rf_rf.best_params_)\nprint(\"rf_rf_lgb_recall:\",recall_score(y_test,rf_rf_pre))\nprint(\"rf_rf_lgb_precision:\",precision_score(y_test,rf_rf_pre))\nprint(\"rf_rf_lgb_f1_score:\",f1_score(y_test,rf_rf_pre))\nprint(\"---------------------------------learn_curve--------------------------------\")\nprint(learn_curve(rf_rf,x,y.values.ravel()))\nprint(\"--------------------------------confusion_matrix---------------------------\")\ndis_lgb.plot()\nplt.show()","9919d1bf":"xgb = XGBClassifier(subsample= 0.5, n_estimators=100, max_depth=8, learning_rate=0.7, gamma=0.1\n                    , colsample_bytree=0.8,eval_metric=\"logloss\",random_state=123,use_label_encoder=False)\nsvc = SVC(random_state=123)\nlgb = LGBMClassifier(subsample=0.5, n_estimators=100, max_depth=8, learning_rate=0.7, colsample_bytree=0.8,random_state=123)\nrf = RandomForestClassifier(max_depth=5, min_impurity_decrease=0.0, n_estimators=50,random_state=123)","347f1421":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nestima = [(\"xgb\",xgb),(\"svc\",svc),(\"lgb\",lgb),(\"rf\",rf)]\nmetaestima = LogisticRegression()\nstack_clf = StackingClassifier(estimators=estima,final_estimator=metaestima,cv=5)\nstack_clf.fit(x_train,y_train.values.ravel())\nstack_clf_pre = stack_clf.predict(x_test)\ncmstack_clf = confusion_matrix(y_test,stack_clf_pre)\ndis_stack_clf = ConfusionMatrixDisplay(confusion_matrix=cmstack_clf)\nprint(\"stack_clf_recall:\",recall_score(y_test,stack_clf_pre))\nprint(\"stack_clf_precision:\",precision_score(y_test,stack_clf_pre))\nprint(\"stack_clf_f1_score:\",f1_score(y_test,stack_clf_pre))","b25cdfc2":"print(\"---------------------------------learn_curve--------------------------------\")\nprint(learn_curve(stack_clf,x,y.values.ravel()))\nprint(\"--------------------------------confusion_matrix---------------------------\")\ndis_stack_clf.plot()\nplt.show()","c8319ad9":"df_test['Fare'] = df_test[\"Fare\"].fillna(df[\"Fare\"].mode().values[0])\ndf_test_pre = stack_clf.predict(df_test)\nsubmission = pd.DataFrame()\nsubmission[\"PassengerId\"] = ids\nsubmission[\"Survived\"] = df_test_pre\nsubmission","6fc1ae64":"submission.to_csv('submission.csv',index=None)","cf2bb0d6":"- Fare\n- Passenger fare\n- The fare is mainly concentrated in 0-100. It can be seen that the fare has a rightward deviation and there are outliers.","d87f1296":"- There are some columns with greater relevance, and we will not consider excluding them for the time being. The first correlation calculation uses spearman, which is aimed at categorical and continuous variables, and may not be interpretable.\n- !!!!!!!!\n- After subsequent model calculations, I found that the recall rate and accuracy are basically 74%-78%. Now I decided to delete the feature of the structure: Cabin_type, and finally see if the effect is improved.","a3af1927":"- **For more than 5 category features, I use leave-one encoding, otherwise, I use WOE encoding or one_hot_encoders.**\n- **Here, I gave up the more commonly used one-hot encoding. And to introduce you to a good third-party library-category_encoders**","c208798c":"<div style=\"color: #fff7f7;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#8ac6d1 ;\n           font-size:20px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center> <h1>Titanic machine learning \ud83d\udea9<\/h1> <\/center> \n<\/div>","2f663c06":"- Pclass\n- Ticket class . A proxy for socio-economic status (SES)\n- The ticket class can reflect the personal economic situation to a certain extent. It can be seen that most people's class is category 3, and the number of class 1 cabins is relatively small, indicating that there are still a few wealthy families. It is worth noting that people with a class of 1 class have a relatively high survival rate.","d64cfe72":"- Age\n- The age is mainly concentrated in the 20-40 years old.\n- It's a bit unusual, the age is less than 0? Let's take a look.","40cdf4ee":"- Sex\n- What happened? There are more men than women, but the survival rate of women is higher than that of men. By watching the famous movie Titanic, you can actually know that most men have partners, or some men are gentlemen, and give priority to women in times of crisis.","a843e88f":"\n<div style=\"color: #fff7f7;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#8ac6d1 ; \n           font-size:20px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center>Thinking about the effect of the model:  <\/center> \n <hr>\n <ul>\nWell, although the recall rate is lower than the single model, we also see that f1_score is higher than the single model. <br>It shows that the effect of model stacking has been improved a little. At the same time, we have also seen that the score of a single model is always below 80%, indicating that our feature engineering is not very good, or the distribution of training set and test set is not good. Same, leading to poor model fitting effect.\n <\/ul>\n <hr>\n<\/div>","ba8f8dd0":"- Looking at f1_score, the overall effect of random forest tuning has increased to a certain extent.","e261160e":"- **As the data set increases, the model has three stages: over-fitting-under-fitting-over-fitting.** \n- **We need to increase the intensity of regularization for overfitting. Let's start tuning parameters.**","dae23111":"- Through the distribution of age characteristics, you can consider filling in missing values from the mean or median of age.","d970e10b":"- Combine the previous models, and use the logistic regression model for the final base model to see the final effect.","2cd2cbc8":"- LightGBM","52d81997":"- SVM","2cbd882a":"# Dealing with missing values","ce3ed95c":"- According to the background information, the first letter in the Cabin indicates the type of cabin number, so we can temporarily ignore the following numbers, as long as the cabin number is the same, the type is the same, but the spatial location is different.","7a9e2889":"# Dealing with category characteristics","0df57a49":"- Our recall rate has increased by one point","6fab2e42":"# EDA","96d17e28":"- Parch\n-  of parents \/ children aboard the Titanic\n-  The upper column represents the number of siblings and spouses, and now this column represents the number of parents and children. We can combine these two columns into one column to indicate the number of accompanying persons.\n- We can also see that people without parents or children have the largest number and the highest survival rate.","b1028c43":"- SibSp\n-  of siblings \/ spouses aboard the Titanic\n- Indicates the number of siblings and spouses","30bafbb3":"- Add a column to indicate whether the person boarded the Titanic alone.","29d9c8b7":"- XGBoost","59a5e970":"- **Fare has a missing value, the problem is not big, we can temporarily fill it.**","d8ce2a91":"- **After the above-commented code tuning, it was found that the effect was worse, so we decided to use the default parameters.**","668a16b6":"- Okay, there is no age less than 0. We don't need to worry.","8b06e4a5":"# Modeling","d7541cc3":"- RF","76332225":"- Port of Embarkation\n- C = Cherbourg, Q = Queenstown, S = Southampton. \n- People who boarded in Southampton had a higher survival rate, and it can be seen that most of them also boarded in Southampton.","84f70664":"# Model stacking"}}