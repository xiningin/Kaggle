{"cell_type":{"ce134bc9":"code","720d639e":"code","fae75ee0":"code","0d007417":"code","7e3606cc":"code","857964eb":"code","86e2a3a2":"code","38c43734":"code","b0bdded3":"code","96820c77":"code","3045af47":"code","3cfa08de":"code","a4a970ba":"code","2a0e56fb":"markdown","e9b21b5b":"markdown","1ae6370b":"markdown","399889df":"markdown","09d51828":"markdown","6007688e":"markdown","1f5fc814":"markdown","58e720ec":"markdown","78f84f97":"markdown","815d9bf6":"markdown","8039bf91":"markdown","23a816cc":"markdown"},"source":{"ce134bc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)import matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os  \nimport json\nimport seaborn as sb\n\n# This import helps visualizing in a more appealing way e.g. data_frame.head()\nimport glob\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# Regular Expressions library\nimport re\n# Import natural language processing library\nimport nltk\nfrom nltk.corpus import stopwords\n# nltk.download(\"stopwords\")\nfrom nltk.stem.porter import PorterStemmer\n\nps_obj = PorterStemmer()\n\nroot_path = \"\/kaggle\/input\/CORD-19-research-challenge\/\"\n","720d639e":"##\n# Stems words of input string, removes non-alphabetical characters\n# \\param str_ input string to be normalized\n# \\return normalized string\n#\ndef normalize_str(str_):\n    # Keep only alphabets and remove any other numbers or symbols replacing them with spaces\n    str_ = re.sub('[^a-zA-Z]', ' ', str_)\n    # all to lower case\n    str_ = str_lower()\n    # split in list format to remove stop words\n    str_ = str_.split()\n    # Remove stop words\n    str_ = [ps_ob(word) for word in str_ if word not in set(stopwords.words('english'))]\n    # Revert to strig format\n    str_ = ' '.join(str_)\n    return str_","fae75ee0":"# Get all Json files\nall_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nlen(all_json)","0d007417":"meta_file = root_path + \"metadata.csv\"\n# Read CSV file as pandas dataframe, dtype to specify the data type for the specified keys\nmeta_df = pd.read_csv(meta_file, dtype={\n'pubmed_id': str,\n'Microsoft Academic Paper ID': str, \n'doi': str, 'arxiv_id': str\n    })\n# This function returns the first n (n=5 by default) rows for the object based on position. \nmeta_df.head()","7e3606cc":"meta_df.info()","857964eb":"##\n# Class to store meta data about papers and does some exploratory operations\n#\nclass PaperMeta:\n    def __init__(self, dir_path):\n        meta_file = dir_path + \"metadata.csv\"\n        # Read CSV file as pandas dataframe, dtype to specify the data type for the specified keys\n        self.df = pd.read_csv(meta_file, dtype={\n        'pubmed_id': str,\n        'Microsoft Academic Paper ID': str, \n        'doi': str, 'arxiv_id': str\n    })\n\n    ##\n    # Removes duplicate of the values in the input column\n    # \\col the column to inspect for duplicates\n    # \\str_normalize Normalizes string by stemming, removing stop words before checking for duplicates\n    # \\show_duplicates print url of duplicates for inspection\n    # \\update_df If true the duplictes are eleminated from the dataframe keeping only the first duplicate\n    # \\show_duplicates prints repeated values and urls of the repititions for inspection\n    # \\return Dictionary with keys as col, and values are the corresponding rows\n    #\n    def remove_duplicates(self, col, show_duplicates=False, update_df=False, str_normalize=False):\n        # Cleaning data by replacing meaningless values to be None, this is important when deciding duplicates or outliers\n        # In case of abstract some abstracts are string = 'none' or 'Unknown' or empty strings thus replace them by None\n        clean_df = self.replace_meaningless_values(col)\n        duplicates = clean_df[clean_df.duplicated(subset=col, keep=False) & ~clean_df[col].isnull()]\n        duplicate_less_df = self.df[~clean_df.duplicated(subset=col, keep='first') | clean_df[col].isnull()]\n        print(f'{col} Length of duplicates: {len(duplicates)} - possible removals keeping first: {len(self.df) - len(duplicate_less_df)}')\n        if show_duplicates:\n            # Sorting duplicated to show corresponding duplicates in order\n            duplicates = duplicates.sort_values(by=col, axis='index')\n            # Keeping track of previous index for printing lines between different column values\n            prev_index = None\n            for index, row in duplicates.iterrows():\n                if prev_index is not None and duplicates[col][prev_index] != duplicates[col][index]:\n                    print(f'----------------------')\n                prev_index = index\n                # In case of long column values, print only first n characters for printing purposes\n                dup_value = row[col] if col not in [\"abstract\"] else row[col][:35]\n                print (f'{index} - {col} :{dup_value}- Url: {row[\"url\"]}')\n        if update_df:\n            num_rows_original = len(self.df.index)\n            # Keep one instance of column duplicates and all null column values\n            self.df = duplicate_less_df\n            num_rows_filtered = len(self.df.index)\n            print(f'Num rows before\/after {col} duplicate removal keeping first:  {num_rows_original}\/{num_rows_filtered}, removed rows: {num_rows_original - num_rows_filtered}')\n        return duplicates  \n    \n    ##\n    # Replaces meaningless values e.g. empty strings in some cases by None.\n    # This is important for duplicate removal or outlier analysis.\n    # \\param col the column for which its values are to be cleaned\n    # \\return dataframe after replacing meaningless values.\n    #\n    def replace_meaningless_values(self, col):\n        clean_df = self.df.copy()\n        if col == \"abstract\":\n            meaningless_vals = ['[Image: see text]', 'none', '', 'Unknown', '[Figure: see text]']\n            clean_df[col] = clean_df[\"abstract\"].replace(to_replace=meaningless_vals, value=None)\n        return clean_df\n\n    ##\n    # Returns a dict constructed from the data frame with keys as the input chosen column\n    # The values are list of rows to accomodate repititions.\n    # \\param key_col key of the dict\n    # \\return constructed dictionary \n    #\n    def to_dict(self, key_col):\n        tmp_dict = {}\n        for index, row in self.df.iterrows():\n            if row[col] not in tmp_dict:\n                tmp_dict[row[col]] = [row]\n            else:\n                tmp_dict[row[col]].append(row)\n        return tmp_dict","86e2a3a2":"meta = PaperMeta(root_path);\ndup_cols = [\"cord_uid\", \"doi\", \"pmcid\", \"pubmed_id\", \"arxiv_id\", \"who_covidence_id\", \"mag_id\", \"abstract\", \n           \"authors\", \"sha\", \"title\", \"pdf_json_files\", \"pmc_json_files\"]\n\nfor col_ in dup_cols:\n    meta.remove_duplicates(col_, show_duplicates=False, update_df=False);\n# meta_dict_sha = meta.df.to_dict(\"sha\");","38c43734":"##\n# Class to store and inspect json file objects\n#\nclass Paper:\n    def __init__(self, file_path, metadata_dict=None):\n        with open(file_path) as file:\n            json_obj = json.load(file)\n            self.json_keys = list(json_obj.keys())\n            self.metadata_keys = list(json_obj['metadata'].keys())\n            # The ID changes with kaggle data updates but as of 12\/5 it seems they mean sha hash\n            self.paper_id = json_obj['paper_id']\n            self.title = json_obj['metadata']['title']\n            # List of dicts each representing an author\n            self.authors = json_obj['metadata']['authors']\n            # Dictionary with section titles as keys and correspondig text as values\n            self.sections = {}\n            # Back matter with similar structure to body text\n            self.back_matter_sections = {}\n            # Dict of dictionaries to references\n            self.references = json_obj['bib_entries']\n            # list of dictionary to references\n            self.figures = json_obj['ref_entries']\n            # Url to paper to be extracted from meta data file\n            self.url = None\n            # Article's source to be extracted from meta data file\n            self.source_x = None\n            # cord_uid unique paper identifier for CORD-19 to be extracted from meta data file\n            self.cord_uid = None\n            self.doi = None\n            self.pmcid = None\n            self.pubmed_id = None\n            self.publish_time = None\n            self.journal = None\n            self.MAP_id = None # Microsoft Academic Paper\n            self.arxiv_id = None\n            # self.found_in_metadata = self.compliment_with_dict(metadata_dict)\n            \n            self.organize_in_sections(json_obj)\n            \n    ##\n    # Organize paper contents into sections including abstract, body_text and back_matter\n    #\n    def organize_in_sections(self, json_obj):\n        # if abstract key does not exist pass (meaning the paper does not have an abstract)\n        # Note that this different from json_schema explanation that says that abstract is found in metada dictionary\n        if 'abstract' in json_obj:\n            for paragraph in json_obj['abstract']:\n                if 'abstract' not in self.sections:\n                    self.sections['abstract'] = [paragraph['text']]\n                else:\n                    self.sections['abstract'].append(paragraph['text'])\n\n        # Body text\n        for paragraph in json_obj['body_text']:\n            # Sections and their contents\n            if paragraph['section'] in self.sections:\n                self.sections[paragraph['section']].append(paragraph['text'])\n            else:\n                self.sections[paragraph['section']] = [paragraph['text']]\n        # Concatenate paragraphs from list to continuous string separated by new line.\n        for sec, paragraphs_list in self.sections.items():\n            self.sections[sec] = '\\n'.join(self.sections[sec])\n\n        # Back matter text\n        for paragraph in json_obj['back_matter']:\n            if paragraph['section'] in self.back_matter_sections:\n                self.back_matter_sections[paragraph['section']].append(paragraph['text'])\n            else:\n                self.back_matter_sections[paragraph['section']] = [paragraph['text']]\n        # Concatenate paragraphs from list to continuous string separated by new line.\n        for sec, paragraphs_list in self.back_matter_sections.items():\n            self.back_matter_sections[sec] = '\\n'.join(self.back_matter_sections[sec])\n    \n    ##\n    # Complement Paper info from sha based metadata dict, the dictionary has key to list of rows\n    #\n    def compliment_with_dict(self, metadata_dict):\n        self_sha = self.paper_id\n        if self_sha in metadata_dict:\n            for df_row in metada_dict[self_sha]:\n                if df_row['title'] == self.title and df_row['authors'] == self.authors:\n                    # Url to paper to be extracted from meta data file\n                    self.url = df_row['url']\n                    # Article's source to be extracted from meta data file\n                    self.source_x = df_row['source_x']\n                    # cord_uid unique paper identifier for CORD-19 to be extracted from meta data file\n                    self.cord_uid = df_row['cord_uid']\n                    self.doi = df_row['doi']\n                    self.pmcid = df_row['pcmid']\n                    self.pubmed_id = df_row['pubmed_id']\n                    self.publish_time = df_row['publish_time']\n                    self.journal = df_row['journal']\n                    self.MAP_id = df_row['Microsoft Academic Paper ID'] # Microsoft Academic PAper\n                    self.arxiv_id = df_row['arxiv_id']\n                    return True\n        return False\n    \n    ##\n    # Returns the number of words in the title\n    #\n    def title_length(self):\n        if len(self.title) > 0:\n            title_words = self.title.split()\n            return len(title_words)\n        return 0\n\n    ##\n    # Returns Word count in paper sections\n    #\n    def word_count(self):\n        counter = 0\n        for sec, text in self.sections.items():\n            words_per_sec = text.split()\n            counter += len(words_per_sec)\n        return counter\n    \n    ##\n    # Returns info related to the input criteria\n    #\n    def get_info(self, criteria):\n        if criteria == \"words_count\":\n            return self.word_count()\n        elif criteria == \"sections_count\":\n            return len(self.sections)\n        elif criteria == \"title_len\":\n            return self.title_length()\n        elif criteria == \"figs_count\":\n            return len(self.figures)\n        elif criteria == \"refs_count\":\n            return len(self.references)\n        elif criteria == \"has_abstract\":\n            return int('abstract' in self.sections)\n        elif criteria == \"authors_count\":\n            return len(self.authors)\n        elif criteria == \"has_back_matter\":\n            return int(len(self.back_matter_sections) > 0)\n        else:\n            raise Exception(\"Unsupported paper critera: \" + criteria)\n    \n    # Get authors names\n    def get_authors(self):\n        authors = []\n        for auth_dict in self.authors:\n            tmp_dict = {}\n            tmp_dict['order'] = len(authors)\n            tmp_dict['first'] = auth_dict['first']\n            tmp_dict['last'] = auth_dict['last']\n            tmp_dict['email'] = auth_dict['email']\n            tmp_dict['affiliation'] = auth_dict['affiliation']\n            authors.append(tmp_dict)\n        return authors\n    # Text representation of the class\n    def __repr__(self):\n        summary = f'ID: {self.paper_id}\\nTitle: {self.title}\\n'\n        letters_per_sec = 100\n        for sec, text in self.sections.items():\n            summary += f'{sec} -> {text[:letters_per_sec]}\\n'\n        summary += \" --- Fig.\/ Tables ---\\n\"\n        for fig_key, fig_dict in self.figures.items():\n            summary += f'{fig_dict[\"type\"]} -> {fig_dict[\"text\"]}\\n'\n        summary += \" --- Back Matter ---\\n\"\n        for sec, text in self.back_matter_sections.items():\n            summary += f'{sec} -> {text[:letters_per_sec]}\\n'\n        summary += \" --- References ---\\n\"\n        for ref_key, ref_dict in self.references.items():\n            # Some references do not have reference ID's maybe this mean the paper is not in the dataset\n            if \"ref_id\" in ref_dict:\n                summary += f'{ref_dict[\"ref_id\"]} -> '\n            else:\n                summary += f'{ref_key} -> '\n            summary += f'{ref_dict[\"title\"]}\\n'\n        return summary\n        ","b0bdded3":"paper_ex = Paper(all_json[15236])\npaper_ex = Paper(all_json[5005])\n\n\nprint(f'======= Json object keys: {paper_ex.json_keys}')\nprint(f'======= Metadata keys: {paper_ex.metadata_keys}')\n\nprint(f\"======= Paper Sections =====\\n {paper_ex.sections.keys()}\")\nprint(f\"======= Paper Summary =====\\n {paper_ex} \")\n\nprint(f\"======= Authors =====\\n {paper_ex.get_authors()}\")","96820c77":"stats_dict = {'title_len': [], 'words_count': [], 'figs_count': [], 'refs_count': [], 'has_abstract':[]\n               , 'authors_count':[], 'sections_count': [], 'has_back_matter': []}\nfor json_paper in all_json:\n    paper_ = Paper(json_paper)\n    for key, _ in stats_dict.items():\n        stats_dict[key].append(paper_.get_info(criteria=key))\n        \nstats_df = pd.DataFrame(stats_dict)\n# stats_df.head()\nstats_df.info()\nstats_df.describe()","3045af47":"def plot_1D_hist(df, col, bins, min_threshold=None, max_threshold=None):\n    max_threshold = max(df[col]) if max_threshold is None else max_threshold\n    min_threshold = min(df[col]) if min_threshold is None else min_threshold\n    df_reduced = stats_df.loc[stats_df[col] < max_threshold]\n    retained_percent = float(df_reduced.shape[0])\/ df.shape[0]\n    plt.hist(df_reduced[col], label=col, bins=bins, range=(min_threshold, max_threshold), histtype='stepfilled')\n    plt.legend()\n    plt.ylabel(\"Frequency\")\n    plt.title(f'Percentage of data {round(retained_percent, 3)}')\n    plt.show()\n    return df_reduced","3cfa08de":"col_to_thresholds = {'title_len': (0, 80), 'words_count': (0, 20000), 'figs_count': (0, 50), 'refs_count': (0, 300)\n               , 'authors_count':(0, 50), 'sections_count': (0, 75)}\ncol_to_bins = {'title_len': range(col_to_thresholds['title_len'][1]), 'words_count': 80, 'figs_count': range(col_to_thresholds['figs_count'][1]),\n               'refs_count': range(col_to_thresholds['refs_count'][1]), 'authors_count':range(col_to_thresholds['authors_count'][1]),\n               'sections_count': range(col_to_thresholds['sections_count'][1])}\nstats_df = stats_df.drop_duplicates()\nreduced_df_all_cols = stats_df.copy()\nfor col, min_max_thresholds in col_to_thresholds.items():\n    reduced_df_by_col = plot_1D_hist(stats_df, col, bins=col_to_bins[col], min_threshold=min_max_thresholds[0], max_threshold=min_max_thresholds[1])\n    # reduced_df_all_cols = reduced_df_all_cols.merge(reduced_df_by_col, how='inner')\n    reduced_df_all_cols = reduced_df_all_cols.merge(reduced_df_by_col)\n\n    print(f'{col} - Complete stats_df shape: {stats_df.shape} , reduced stats_df shape: {reduced_df_all_cols.shape}')","a4a970ba":"pd.plotting.scatter_matrix(reduced_df_all_cols, figsize=(15, 15), hist_kwds={'bins':80});","2a0e56fb":"# Per Papers Statistics","e9b21b5b":"# Helper string related methods","1ae6370b":"# PaperMeta Class to explore metadata file\n\n- ### Found 36 cord_uid duplicates. By checking few urls the papers appear to be the same e.g. cord_uid jsk1oztb [dup1](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7161675\/pdf\/s1057-6290_2010_0000011014.pdf), [dup2](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7162272\/pdf\/s1057-6290_2010_0000011014.pdf).\n- ### Found 1211 title duplicates and with few url samples check\n    - Some cases papers appear to be the same paper e.g. Interleukin-18 expression and the response to treatment in patients with psoriasis [dup1](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3258774\/pdf\/AMS-7-4-713.pdf), [dup2](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3302712\/pdf\/AMS-6-6-964.pdf)\n    - Some cases appear to be extensions of the same paper from the same authors e.g. SARS Control and Psychological ... ([dup1](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3323345\/pdf\/03-0703.pdf), [dup2](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3320456\/pdf\/04-0760.pdf)) , e.g. Conflict and Emerging\nInfectious Diseases  ([dup1](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3375795\/pdf\/06-1093_finalP.pdf), [dup2](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC2600301\/pdf\/08-0027_finalL.pdf)), e.g. Vicarious traumatization in the general public ... ([dup1](https:\/\/reader.elsevier.com\/reader\/sd\/pii\/S0889159120303093?token=3B77B9570D8D743F19178AFC2B757A76ED5343F9DBB852FB4E3F383899927A4142AD5E2E1A952C93B6FB76A57A04A0B2), [dup2](https:\/\/www.medrxiv.org\/content\/10.1101\/2020.02.29.20029322v1.full.pdf)).\n    - Some cases are unrelated with general titles e.g News in Brief ([dup1](https:\/\/www.thelancet.com\/action\/showPdf?pii=S0140-6736%2804%2915673-0), [dup2](https:\/\/www.thelancet.com\/action\/showPdf?pii=S0140-6736%2804%2916542-2)) , e.g. **Public health round-up {15}** ([dup1](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4431563\/pdf\/BLT.15.010415.pdf) , [dup2](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5985429\/pdf\/BLT.18.010518.pdf)), other general titles: Reseach highlights, Biomedical Briefing, **Correction {5}** , **..News..** , Letter from the editor, Respiratory Infections, Department of Error, Corona, Instructions for Authors, Articles of Significant Interest in This Issue, Pharmaceutical applications, Announcement, Patent reports, ..**volume**.., **nan {158}{Index  docs, non-English, Url not found}**, authors, **Infectious disease surveillance update {24}**, briefing, Reviews and comment from the nature publishing group, ..**Brief**.., Erratum, Lung, Vaccines, Pandemic, Acronyms and Abbreviations, Appendix, Acronyms and Abbreviations, Glossary, Keyword Index, Contents List, Letters, Feedback, Editorial Board and Contents, Full Issue PDF, In Case You Haven't Heard, Sore throat, A Message from the Editor, In this issue, Commentary, References, Summaries, Editor's Choice, Economic review, Posters.\n    - Titles leading to non-English articles: \n        - German: Panorama, Aus der Branche, Infektiologie, Corona, Erreger, Abk\u00fcrzungsverzeichnis, Inhaltsverzeichnis, Infektionen\n        - French: Fiche, chapitre, table, mots, auteurs, \u00c9pid\u00e9miologie des diarrh\u00e9es aigu\u00ebs, Textes juridiques, Posters\n        - Spanish: Controversias sobre, Miocarditis fulminante \n        - Chinese:  Pregnant women with new coronavirus infection\n    - Car related articles! People+companies also in german Personen + Unternehmen [link1](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7149276\/pdf\/38314_2020_Article_201.pdf), [link2](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7149237\/pdf\/38311_2020_Article_232.pdf), [link3](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7149276\/pdf\/38314_2020_Article_201.pdf), [link4](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7149237\/pdf\/38311_2020_Article_232.pdf)\n    ","399889df":"# Paper Class to store research paper json files in a code friendly manner\n[Starter code](https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering\/notebook)\n","09d51828":"## Is it a proper research paper?\n- Decent title length (more than 1 or 2 words?)\n- By certain key words, Title content does not have some words: news, highlights, volume, brief, letter, comment, glossary, index, editorial ?\n- In English Language\n- Has abstract, introduction and conculsion\/future work\/implications sections?\n- Has figures? All figures are references in text?\n- Has references? Decent number of references?\n\n## Is the paper relevant to covid-19 topic?\n- By date?\n- By keywords?\n\n## What is the quality of the paper?\n- Decent number of pages (more than 1 ?) (by number of words)\n- Has figures? All figures are references in text?\n- Referenced by other papers\n\n\n### Current Tasks\n\n- Survey other data cleaning submissions.\n- Attempt duplicate filtering both in json documents and metadata. By IDs first (validate first manually), then by title or title + abstract or other methods.\n- Matching metadata entries to json files (65k entries in metadata and 85k documents) -> paper_id value json files is similar to sha value in metadat and some is similar pmc_id, using title + abstract matching sounds decent.\n","6007688e":"### Other sumbissions\n\nMost submissions are Literature clustering and search engines to search for articles:\n\n1-[ A search engine](https:\/\/www.kaggle.com\/danielwolffram\/discovid-ai-a-search-and-recommendation-engine) that takes some key words in like google and gets you relevant results, also divides papers into different categories, when you click on a category it will show you the most repeated topic keyords in that category. It is submitted to risk factors task but I feel it is very general.\n\n2- [Simple no AI search engine](https:\/\/www.kaggle.com\/mlconsult\/transmission-incubation-and-environment-2-0) submitted for tasks 1 and 3, some manual keywords input and the output us a set of columns with result sentences in a paper and its link.\n\n3- [Transmission analysis & its relation to temperation](https:\/\/www.kaggle.com\/sixteenpython\/covid-19-temperature-air-travel-transmission): this study is not mainly depending on the CORD-19 dataset but rather on some files on the number of deaths per country, flights from China to the rest of the world and average temperature per country.\n\n4- [Interesting answering question submission](https:\/\/www.kaggle.com\/sandyvarma\/covid-19-bert-mesh-enabled-knowledge-graph) based on [BERT](https:\/\/ai.googleblog.com\/2018\/11\/open-sourcing-bert-state-of-art-pre.html) (Google's AI solution to answer literature related question). BERT  ranks 18's in Stanford's [Question-Answers dataset](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/), question and answers samples can be found [here](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/explore\/v2.0\/dev\/).\n","1f5fc814":"# Inspection of the metadata.csv file\n\n### Metadata has an entry for each paper containing some info e.g. url, ID based on the paper's source, title etc.","58e720ec":"# [Video](https:\/\/www.youtube.com\/watch?v=oNW4jXkc1eY&t=303s) Explaining the approach\n\nThis notebook aims at filtering the dataset provided by Kaggle differentiating proper research papers and other documents that would not be useful for a public health specialist\/ scientist to explore.\n\n## Steps to realize outlier document removal\n\n - Extracting duplicate papers by ID , title etc. from metadata\n - Parametrizing papers with different criteria e.g. words count, references count, authors count, figures count, has abstract, has back matter etc.\n - These parameters along side the content of the paper are then used to extract outlier documents.","78f84f97":"## Scatter plots (Matrix) for per paper stats","815d9bf6":"# Importing Libraries and Dataset","8039bf91":"## Duplicate Removal\n- Keep only one duplicate by {\"cord_uid\", \"doi\", \"pmcid\", \"pubmed_id\", \"arxiv_id\", \"who_covidence_id\", \"mag_id\"} in order.\n    - when removing by pubmed_id: cases where duplicates where extension of one another [dup1](https:\/\/www.thelancet.com\/action\/showPdf?pii=S0140-6736%2803%2914131-1), [dup2](https:\/\/www.thelancet.com\/action\/showPdf?pii=S0140-6736%2803%2914130-X)\n    - By pubmed_id: one duplicate in English, another in other language [dup1](https:\/\/reader.elsevier.com\/reader\/sd\/pii\/S0300289605706876?token=FA9EA4F3C01731B7A2FA07BAC2B9BBCCBCC63A2292D51BF79BE524CCBAC635C6193809598279841125DF7994EC4F38AA), [dup2](https:\/\/reader.elsevier.com\/reader\/sd\/pii\/S1579212906602747?token=2461CCFCDA22B431518D7165223D9EFE85ACF1424029F3AF0E775ABF8B8BBA31B2C5679166A073437B62E94524324556)\n    - By abstract: some abstract content had meaningless values -> \n        - Taking one randomly is not sufficient becaue there were 4 duplicates: where 3 were indices and one is a 32 page long document! [dup1](https:\/\/reader.elsevier.com\/reader\/sd\/pii\/B9780123814685000129?token=EB1CD3361AE3771D17A0E2D3A4E532F068A7C04FCA63565FB6EEBF8AE5AD2C0C53CB2BB51F2AFA2A398059CE300F34E5), [dup2](https:\/\/reader.elsevier.com\/reader\/sd\/pii\/B9780123820044100251?token=BECB784A3CC09B1C4EA8479D1FEE093D8B7C95CA2B1781F7134E43B8F5D4F288D472C6EC3475A16642581EA3BAA33AE1), [dup3](https:\/\/reader.elsevier.com\/reader\/sd\/pii\/B9780123820068000499?token=DE934B5B3298552244AA1197B23DBBFB8FDCA01E314D36366810583FC0D2AE29CBDA4B89E18923E0CF516742D198B297), [dup4](https:\/\/reader.elsevier.com\/reader\/sd\/pii\/B9780123820068000426?token=D30673E5E6E41365B42EE8C0F5EE9101C92B3280417B4D9FB7A9549C6CC7DFC368AB7341B0500FA256029E02BC5DCA79). Also, 165 page document and the other two are indiex pages [dup1](https:\/\/reader.elsevier.com\/reader\/sd\/pii\/B9780702066979000030?token=0E5C7E568FF183E8519A4898801E5EB69B9FF1732620693033B3E565F5D5224464C5B083AEE7B0973B408DAD6B26A509), [dup2](C9AFAF4A4C7259FF135A285A9A2927F45BA22FBBAF2ADF267DBB09CBC2E7FA6C74EB969AA7456E3669BD3A626DD33EC1), [dup3](https:\/\/reader.elsevier.com\/reader\/sd\/pii\/B9780721636955500791?token=5BAFA13D50E3B07C5445325B822AC392D3E54FC7154E92333A372BE60CB03A73EA4553A9B1F326229D2935C4E95226F4)\n        - By authors, sha and titles do not make sense.\n        - No duplicates found when finding duplicates using pmc_json and pdf_json","23a816cc":"# Traversing all Research paper json files"}}