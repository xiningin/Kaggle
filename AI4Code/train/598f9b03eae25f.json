{"cell_type":{"a3a29b28":"code","8bf5b521":"code","e37fd4c1":"code","90f70045":"code","a55616fd":"code","77da8a4d":"code","a17775a0":"code","7df2197b":"code","7109dd4f":"code","ace9b86b":"code","43fe2f62":"code","b895f56f":"code","29508816":"markdown","db358618":"markdown","721d10e8":"markdown","dbfe8d94":"markdown","de01fb80":"markdown","6cc7d9cb":"markdown","e816f82f":"markdown","eeb44e40":"markdown"},"source":{"a3a29b28":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","8bf5b521":"train = pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/test.csv')\n# We need timestamps from test to create submission file\ntime = test['datetime']","e37fd4c1":"# Make datetime column date time format\ntrain['datetime'] = pd.to_datetime(train['datetime'])","90f70045":"# Extrac day of the week, hout and month\n\ntrain['day_of_week'] = train.datetime.dt.day_name()\ntrain['hour'] = train.datetime.dt.hour\ntrain['month'] = train.datetime.dt.month","a55616fd":"# Since temp and atemp  have high correlation \n# It make sense to dervice a single feature\n# Let`s use average value between two\n\ntrain['temp'] = (train['temp']+train['atemp'])\/2\ntrain = train.drop('atemp',axis=1)","77da8a4d":"# here we choose columns that later on will be transformed \n# By OneHotEncoder\n# By OrdinalEncoder\n# Let`s also try to use bins\n# And scale numeric values\n\ncolumns_ohe = ['season','holiday','workingday','day_of_week','month']\ncolumns_bin = ['humidity','windspeed']\ncolumns_num = ['temp','humidity','windspeed']","a17775a0":"# Let`s use log(1+x) of target values\n# Because there is postitive skewnees\ntrain['count']=np.log1p(train['count'])\n\ndef remove_outliers(df_in, col_name):\n    q1 = df_in[col_name].quantile(0.25)\n    q3 = df_in[col_name].quantile(0.75)\n    iqr = q3-q1 #Interquartile range\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n    print('Removed: ',(len(df_in)-len(df_out))\/len(df_in)*100,' % of initial dataset')\n    return df_out\n\ntrain = remove_outliers(train,'count')\n","7df2197b":"from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error,mean_squared_log_error\n\n# Column tranformer is usefull function from sklearn library\n# It help to make tranformation of pandas dataframes and put in pipeline if necessary\n\ntrans = make_column_transformer(\n    (OrdinalEncoder(),['weather']),\n    (OneHotEncoder(),columns_ohe),\n    (RobustScaler(),columns_num),\n    (KBinsDiscretizer(n_bins = 4,encode='ordinal'), columns_bin),\n    remainder = 'passthrough'\n)\n\n\n# We use RandomForesrRegressor with already adjusted values by GridSearchCV\nrf_reg = RandomForestRegressor(n_estimators = 3000,\n                               max_depth = 40,\n                               random_state = 42)\n\n# We use GradientBoosting Regressor with already adjusted values by GridSearchCV\ngb_reg = GradientBoostingRegressor(n_estimators=1000, \n                                   min_samples_leaf=6, \n                                   random_state=42)\n\n\n# Create training and validation datasets\nX = train.drop(['count','datetime','casual','registered'],axis=1)\ny = train['count']\n\n# transform X values\nX = trans.fit_transform(X)\n\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n# Split set on traing and test\nX_train,X_val,y_train,y_val = train_test_split(X,y,\n                                               random_state=0,\n                                               test_size = 0.25,\n                                               shuffle=True)\n\n# Fit by two regressors\nrf_reg.fit(X_train,y_train)\n\ngb_reg.fit(X_train,y_train)\n\n\nprint('(RFR) RMSE: ',np.sqrt(mean_squared_error(y_val,rf_reg.predict(X_val))))\nprint('(GBR) RMSE: ',np.sqrt(mean_squared_error(y_val,gb_reg.predict(X_val))))","7109dd4f":"# we can blend results from different regressors to improve the score\n# weight parameters are adjusted manualy\n# As you can see score has improved slightly\n\ndef blend_pred(X):\n    pred = 0.3*rf_reg.predict(X) + 0.7*gb_reg.predict(X)\n    return pred\n\nprint('Blended model RMSE: ',np.sqrt(mean_squared_error(y_val,blend_pred(X_val))))","ace9b86b":"# We must prepare test file in the same way as train\n\ntest['datetime'] = pd.to_datetime(test['datetime'])\ntest['day_of_week'] = test.datetime.dt.day_name()\ntest['hour'] = test.datetime.dt.hour\ntest['month'] = test.datetime.dt.month\ntest['temp'] = (test['temp']+test['atemp'])\/2\ntest = test.drop(['datetime','atemp'],axis=1)\n","43fe2f62":"# then we use the same transformer and predict values\nX_test = trans.transform(test)\n\n# dont forget to transform predicted values with np.exp\npred = np.expm1(rf_reg.predict(X_test)).round()\n","b895f56f":"sub = pd.DataFrame({'datetime':time,'count':pred})\nsub.to_csv('sub.csv',index = False)","29508816":"### Submission","db358618":"### Remove outliers ","721d10e8":"### Modeling","dbfe8d94":"### Import Datasets","de01fb80":"### Import modules","6cc7d9cb":"### Blending","e816f82f":"### Prepare submision file","eeb44e40":"### Feature Engineering"}}