{"cell_type":{"b979139d":"code","129869ba":"code","a79212cd":"code","f5260382":"code","3132bde6":"code","109fd32e":"code","fd5f8909":"code","51795f8a":"code","64151549":"code","720b882f":"markdown","1b0408d4":"markdown","65ad1248":"markdown","a7c69e94":"markdown","a0bdc3fd":"markdown","21ef1ab4":"markdown","ed0a0126":"markdown"},"source":{"b979139d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/test.csv\")\ntrain_y=np.array((train[[\"label\"]]).T)\ntrain_x=np.array(train)\ntrain_x=np.delete(train_x,0,1).T\ntest_x=np.array(test).T","129869ba":"train_x=train_x\/255  #Normalizing the input data\ntest_x=test_x\/255","a79212cd":"digits = 10\nexamples = train_y.shape[1]\nY_new = np.eye(digits)[train_y.astype('int32')]\nY_new = Y_new.T.reshape(digits, examples)","f5260382":"i = 40\nplt.imshow(train_x[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\nplt.axis(\"off\")\nplt.show()\nY_new[:,i]","3132bde6":"def compute_cost(Y,Y_hat):\n    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n    m = Y.shape[1]\n    L = -(1\/m) * L_sum\n    \n    return L","109fd32e":"def sigmoid(Z):\n    return 1\/(1+np.exp(-Z))","fd5f8909":"n_x=train_x.shape[0]\nn_h=64                #number of nodes in hidden layer\nlearning_rate=0.75\nW1 = np.random.randn(n_h, n_x)\nb1 = np.zeros((n_h, 1))\nW2 = np.random.randn(digits, n_h)\nb2 = np.zeros((digits, 1))\nm=train_x.shape[1]\nX=train_x\nY=Y_new\n\nfor i in range(3000):  \n    #Implementation of forward propagation\n    Z1=np.dot(W1,X) +b1     \n    A1=sigmoid(Z1)\n    Z2=np.dot(W2,A1)+b2\n    A2 = np.exp(Z2) \/ np.sum(np.exp(Z2), axis=0)\n    \n    cost=compute_cost(Y,A2)\n    \n    #Implementation of backward propagation\n    dZ2=A2-Y              \n    dW2=(np.dot(dZ2,A1.T))\/m\n    db2 = (1\/m) * np.sum(dZ2, axis=1, keepdims=True)\n    dA1=np.dot(W2.T,dZ2)\n    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n    dW1 = (1\/m) * np.dot(dZ1, X.T)\n    db1 = (1\/m) * np.sum(dZ1, axis=1, keepdims=True)\n    \n    #Implementation of Gradient Descent\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    \n    if (i % 100 == 0):\n        print(\"Epoch\", i, \"cost: \", cost)\n    \nprint(\"Final cost:\", cost)","51795f8a":"Z1 = np.matmul(W1, test_x) + b1\nA1 = sigmoid(Z1)\nZ2 = np.matmul(W2, A1) + b2\nA2 = np.exp(Z2) \/ np.sum(np.exp(Z2), axis=0)\npredictions = np.argmax(A2, axis=0)    #Selecting the index with the max value which'll correspond to the prediction","64151549":"submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"DR.csv\", index=False, header=True)","720b882f":"**Doing sanity check**","1b0408d4":"Now I'll initialise the hyperparameters for learning and do **forward** and **backward** propagation, followed by **Gradient** **Descent**.","65ad1248":"The below code is to create a csv file \"DR\" which'll contain all predictions","a7c69e94":"Below I'll be using a cost function defined for softmax function","a0bdc3fd":" This'll be my first notebook on Kaggle and I'm open to all the feedback that you guys can provide me .\n \n In this notebook I'll be creating a 2 layer neural network using *sigmoid* as activation in **hidden** layer and *softmax* in *output* layer.\n \n We'll only be getting an accuracy of 91.6% as I'm not using **CNN** and a shallow network and I'll soon implement a **CNN** ** from scratch** in a later version.\n \n I have avoided libraries like **Keras** for creating the network and have written both the forward and back prop.\n ","21ef1ab4":"Here I'll be changing the labels to correspond for multiclass classification\nIf Y=1, Y_new=[0 1 0 0 0 0 0 0 0 0]\n","ed0a0126":"The model is ready and now we'll make predictions on the test data\n'"}}