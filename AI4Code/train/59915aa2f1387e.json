{"cell_type":{"a5bca724":"code","45be1df2":"code","dfff42fa":"code","e5be9b81":"code","dffd92e3":"code","22be0a2f":"code","50821c45":"code","5165c5b3":"code","d93dc158":"code","a56c4c68":"code","6e576216":"code","576c0d9e":"code","e0c21dea":"code","93877ca6":"code","b57abca9":"code","752af4ca":"code","c5441b97":"code","566f1769":"code","6ea6506b":"code","df0ea83e":"code","7660028b":"code","d1e62b87":"markdown","bc77dee7":"markdown","93fdb6f1":"markdown","68034373":"markdown","2988dbc0":"markdown","dde57c0f":"markdown","17867851":"markdown"},"source":{"a5bca724":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","45be1df2":"from matplotlib.pyplot import imread,imshow\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.optimizers import *\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport glob\nfrom tensorflow.keras.callbacks import EarlyStopping\n","dfff42fa":"# loading images\n\ntrain_cats_list = glob.glob(\"..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg\")\ntrain_dogs_list = glob.glob(\"..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg\")\n\ntest_cats_list = glob.glob(\"..\/input\/cat-and-dog\/test_set\/test_set\/cats\/*.jpg\")\ntest_dogs_list = glob.glob(\"..\/input\/cat-and-dog\/test_set\/test_set\/dogs\/*.jpg\")","e5be9b81":"#collect images in the same dataset\ndogs_list= train_dogs_list + test_dogs_list\ncats_list= train_cats_list + test_cats_list","dffd92e3":"#dog and cat image numbers \nprint(len(dogs_list))\nprint(len(cats_list))","22be0a2f":"#Editing data and reading pixel values\nX = []\nY = []\n\nfor image in cats_list:\n    X.append(resize(imread(image),(64,64,1)))\n    \nfor image in dogs_list:\n    X.append(resize(imread(image),(64,64,1)))\n    \nY += [0 for _ in range(len(cats_list))]\nY += [1 for _ in range(len(dogs_list))]\n\nX=np.array(X)\nY=np.array(Y)","50821c45":"# Resize\n\nY=Y.reshape(X.shape[0],1)\nX=X.reshape(X.shape[0],64,64)","5165c5b3":"# Images from the dataset\nimg_size=64\nplt.subplot(2,2,1)\nplt.imshow(X[260])\nplt.axis(\"off\")\nplt.colorbar()\nplt.subplot(2,2,2)\nplt.imshow(X[9500])\nplt.axis(\"off\")\nplt.colorbar()\nplt.subplot(2,2,3)\nplt.imshow(X[6550])\nplt.axis(\"off\")\nplt.colorbar()\nplt.subplot(2,2,4)\nplt.imshow(X[4500])\nplt.axis(\"off\")\nplt.colorbar()","d93dc158":"print(\"X Shape: \", X.shape)\nprint(\"Y Shape: \", Y.shape)","a56c4c68":"# Separate Training and Test sets\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.30, random_state=200)\nnumber_of_train=X_train.shape[0]\nnumber_of_test=X_test.shape[0]\n\n\nprint(number_of_train)\nprint(number_of_test)","6e576216":"# resize, write all pixels in one line\n\nX_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test.reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)","576c0d9e":"x_train= X_train_flatten.T\nx_test=X_test_flatten.T\ny_train=Y_train.T\ny_test=Y_test.T\n\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","e0c21dea":"#set initial parameters\n\ndef initialize(dimension):\n    w=np.full((dimension,1),0.01)\n    #np.full --> Verilen \u015fekil ve t\u00fcrde fill_value (0.01) ile doldurulmu\u015f yeni bir dizi d\u00f6nd\u00fcr\u00fcr. \n    #yani pixel say\u0131s\u0131 kadar 0.01 de\u011ferli array olu\u015ftur\n    b=0.0\n    return w,b","93877ca6":"#sigmoid Function\n# z= np.dot(w.T, x_train)+b\n\ndef sigmoid(z):\n    y_head= 1\/(1+ np.exp(-z))  #sigmoid function\n    \n    return y_head","b57abca9":"#forward and Backward Propagation\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # Forward Propagation\n    z= np.dot(w.T, x_train)+b\n    y_head= sigmoid(z)   #propabilistiv 0-1 aras\u0131nda\n    loss= -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)  #loss function\n    cost= (np.sum(loss))\/x_train.shape[1]   #x_train.shape[1] ----> number of samples in x_train\n    \n    # Backward Propagation\n    \n    derivative_weight= (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]  # x_trian.sahape[1] is scaling\n    derivative_bias= (np.sum(y_head-y_train))\/ x_train.shape[1]  # x_trian.sahape[1] is scaling\n    gradients= {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost, gradients","752af4ca":"# Uptading (Learning) Paramaters\n\ndef update (w, b, x_train, y_train, learning_rate, number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index= []\n    \n    for i in range (number_of_iteration):\n        #cost ve graidentleri finding\n        cost, gradients= forward_backward_propagation (w,b,x_train, y_train)\n        cost_list.append(cost)\n        \n        #uptading\n        w= w- learning_rate*gradients[\"derivative_weight\"]\n        \n        b= b- learning_rate*gradients[\"derivative_bias\"]\n        \n        if i%100==0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" % (i, cost))\n    \n    parameters= {\"weight\":w, \"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","c5441b97":"#prediction\n\ndef predict(w,b,x_test):\n    y_head= sigmoid(np.dot(w.T, x_test)+b)\n    y_prediction= np.zeros((1, x_test.shape[1]))\n    \n    for i in range (y_head.shape[1]):\n        if y_head[0,i] <= 0.5 :\n            y_prediction[0,i]=0\n        else:\n            y_prediction[0,i]=1\n    \n    return y_prediction","566f1769":"#Logistic Regression\n\ndef logistic_regression( x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    dimension= x_train.shape[0] #number of pixel\n    w,b = initialize (dimension)\n    \n    parameters, gradients, cost_list= update ( w,b, x_train, y_train, learning_rate, num_iterations)\n    \n    y_prediction_test= predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    y_prediction_train= predict(parameters[\"weight\"], parameters[\"bias\"], x_train)\n    \n    print(\"train accuracy: {} %\".format(100-np.mean(np.abs(y_prediction_train- y_train))*100))  #np.abs-- mutlak de\u011fer\n    print(\"test accuracy: {} %\".format(100-np.mean(np.abs(y_prediction_test- y_test))*100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate=0.01, num_iterations=1500)","6ea6506b":"#Logistic regression modeli\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg= LogisticRegression(random_state=200, max_iter=1500, solver='liblinear')\n\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","df0ea83e":"#reshaping \nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","7660028b":"# Evaluating the ANN\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndef build_classifier():\n    \n    classifier= Sequential() # initialize neural network\n    classifier.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\n\nclassifier = KerasClassifier(build_fn= build_classifier, epochs=100)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))\n    ","d1e62b87":"# Edit dataset","bc77dee7":"# Logistic Regression","93fdb6f1":"# Logistic Regression with Sklearn","68034373":"# Deep Neural Networks","2988dbc0":"# Loading the dataset","dde57c0f":"# Installing libraries","17867851":"I trained a logistic regression model and deep neural network using a dataset of cat and dog photos. The results were not what I expected but this process was quite educational for me.Looking forward to your suggestions to improve model performance (accuracy)"}}