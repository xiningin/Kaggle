{"cell_type":{"54eaa2d1":"code","c5aaccfc":"code","2473f40f":"code","101ac9f4":"code","aa8b7960":"code","4d54052b":"code","b1e69aba":"code","920f829d":"code","80f9c16c":"code","709359f2":"code","728feece":"code","20609791":"code","f8420edb":"code","db51a44d":"code","31697546":"code","2d820b4e":"code","0baad754":"code","ad213858":"code","cd0130d1":"code","7ead0e8d":"code","bf728e67":"code","7474f9b7":"code","74e54634":"code","59f654bb":"code","3bd42062":"markdown","d74dcf80":"markdown","df95be48":"markdown","095cca07":"markdown","f91a3c07":"markdown","69f21e55":"markdown","26e8f902":"markdown","b7ba675c":"markdown"},"source":{"54eaa2d1":"# to delete any directory. Don't use it now.\n# import shutil\n# shutil.rmtree(\"..\/dog-cat-small\")","c5aaccfc":"import os, shutil\nimport os.path\nfrom os import path\n# our dataset (only train portion)\noriginal_dataset_dir = \"..\/input\/dogs-vs-cats\/train\/train\"    # we are asked to work with train part only.\n# Create a Directory where we\u2019ll store our smaller dataset\nbase_dir = \"\/kaggle\/working\/dog-cat-small\"\nif not path.exists(base_dir):\n    os.mkdir(\"\/kaggle\/working\/dog-cat-small\")\n","2473f40f":"# Directories for the training, validation, and test splits\ntrain_dir = os.path.join(base_dir, 'train')\nif not path.exists(train_dir):\n    os.mkdir(train_dir)\nvalidation_dir = os.path.join(base_dir, 'validation')\nif not path.exists(validation_dir):\n    os.mkdir(validation_dir)\ntest_dir = os.path.join(base_dir, 'test')\nif not path.exists(test_dir):\n    os.mkdir(test_dir)","101ac9f4":"#Directory with training cat pictures\ntrain_cats_dir = os.path.join(train_dir, 'cats')\nif not path.exists(train_cats_dir):\n    os.mkdir(train_cats_dir)","aa8b7960":"# Directory with training dog pictures\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\nif not path.exists(train_dogs_dir):\n    os.mkdir(train_dogs_dir)","4d54052b":"# now same work for validation and test sets\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\nif not path.exists(validation_cats_dir):\n    os.mkdir(validation_cats_dir)\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\nif not path.exists(validation_dogs_dir):\n    os.mkdir(validation_dogs_dir)\n\ntest_cats_dir = os.path.join(test_dir, 'cats')\nif not path.exists(test_cats_dir):\n    os.mkdir(test_cats_dir)\ntest_dogs_dir = os.path.join(test_dir, 'dogs')\nif not path.exists(test_dogs_dir):\n    os.mkdir(test_dogs_dir)","b1e69aba":"len(os.listdir(original_dataset_dir))","920f829d":"# Copies the first 70% cat images to train_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(0, int(12500*0.70), 1)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_cats_dir, fname)\n    shutil.copyfile(src, dst)","80f9c16c":"# Copies the next 20% cat images to validation_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(int(12500*0.70), int(12500*0.90))]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_cats_dir, fname)\n    shutil.copyfile(src, dst)","709359f2":"# Copies the rest 10% cat images to test_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(int(12500*0.90), 12500)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_cats_dir, fname)\n    shutil.copyfile(src, dst)","728feece":"# now same work for dog pictures\n\nfnames = ['dog.{}.jpg'.format(i) for i in range(int(12500*0.70))]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_dogs_dir, fname)\n    shutil.copyfile(src, dst)\n\nfnames = ['dog.{}.jpg'.format(i) for i in range(int(12500*0.70), int(12500*0.90))]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_dogs_dir, fname)\n    shutil.copyfile(src, dst)\n\nfnames = ['dog.{}.jpg'.format(i) for i in range(int(12500*0.90), 12500)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_dogs_dir, fname)\n    shutil.copyfile(src, dst)","20609791":"# sanity check\nimport pandas as pd\nd = {\n        \"Set\": [\"Train\",\"\", \"Validation\",\"\", \"Test\",\"\"],\n        \"Type\": [\"Cats\", \"Dogs\", \"Cats\", \"Dogs\", \"Cats\", \"Dogs\"],\n        \"Number\": [len(os.listdir(train_cats_dir)),\n                    len(os.listdir(train_dogs_dir)),\n                    len(os.listdir(validation_cats_dir)),\n                    len(os.listdir(validation_dogs_dir)),\n                    len(os.listdir(test_cats_dir)),\n                    len(os.listdir(test_dogs_dir))]\n    }\n\np = pd.DataFrame(d)\np","f8420edb":"from keras import layers\nfrom keras import models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',\ninput_shape=(150, 150, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","db51a44d":"model.summary()","31697546":"from keras import optimizers\nmodel.compile(loss='binary_crossentropy',\n                optimizer=optimizers.RMSprop(lr=1e-4),\n                metrics=['acc'])","2d820b4e":"from keras.preprocessing.image import ImageDataGenerator\n\n# Rescale all images by 1\/255\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(150, 150),\n    batch_size=50,\n    class_mode='binary')\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(150, 150),\n    batch_size=50,\n    class_mode='binary') #Because we use binary_crossentropy loss, we need binary labels.","0baad754":"for data_batch, labels_batch in train_generator:\n    print('data batch shape:', data_batch.shape)\n    print('labels batch shape:', labels_batch.shape)\n    break","ad213858":"history = model.fit(\n    train_generator,\n    steps_per_epoch=100,\n    epochs=30,\n    validation_data=validation_generator,\n    validation_steps=50)\n# it took 1hour to train in my pc. But with GPU, only about 10minutes. Only the 1st epoch was lengthy.","cd0130d1":"#### Save the model for future use:\nmodel.save('\/kaggle\/working\/cats_and_dogs_small_1.h5')","7ead0e8d":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","bf728e67":"test_generator = test_datagen.flow_from_directory(\n        test_dir,\n        target_size=(150, 150),\n        batch_size=50,\n        class_mode='binary',\n        shuffle=False)\n\ntest_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\nprint('Test Accuracy:', test_acc)","7474f9b7":"from sklearn.metrics import accuracy_score, confusion_matrix\nimport numpy as np\n# load model\n# from tensorflow.keras.models import load_model\n# model = load_model('\/kaggle\/working\/cats_and_dogs_small_1.h5')\n# preds = model.predict_generator(test_generator, steps=len(test_generator))\n# preds = model.predict(test_generator)\npreds = model.predict_generator(test_generator,steps = len(test_generator.labels\/\/50))\n\ny=test_generator.classes # shape=(2500,)\ny_test =y.reshape(2500,1)\n\nacc = accuracy_score(test_generator.labels, np.round(preds))*100\ncm = confusion_matrix(test_generator.labels, np.round(preds))\n\ntn, fp, fn, tp = cm.ravel()\n\nprint('\\n============TEST METRICS=============')\nprecision = tp\/(tp+fp)*100\nrecall = tp\/(tp+fn)*100\nprint('Accuracy: {}%'.format(acc))\nprint('Precision: {}%'.format(precision))\nprint('Recall: {}%'.format(recall))\nprint('F1-score: {}'.format(2*precision*recall\/(precision+recall)))","74e54634":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_cm(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots()\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n    \nplot_cm(test_generator.labels, np.round(preds))","59f654bb":"# %matplotlib inline\n# import numpy as np\n# import os\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n\n# ax = plt.axes()\n# sns.heatmap(cm, annot=True, fmt=\"d\")\n\n# ax.set_title('Confusion Matrix')\n# plt.show()","3bd42062":"#### Configure the Model for Training","d74dcf80":"## Evaluation","df95be48":"## Data Preprocessing\n- Read the picture files.\n- Decode the JPEG content to RGB grids of pixels.\n- Convert these into floating-point tensors.\n- Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as we know, neural networks prefer to deal with small input values).","095cca07":"## Separation of data","f91a3c07":"## Fit the model","69f21e55":"## Convnet","26e8f902":"### Confusion Matrix","b7ba675c":"So, we have 12500 dogs and 12500 cats images. Now let's split them."}}