{"cell_type":{"69eddff6":"code","b34ad660":"code","ed18a0f8":"code","9a138f72":"code","619f66a6":"code","77e055ab":"code","231f8d86":"code","f51b197a":"code","7da4a0b8":"code","593ae113":"code","f14ec87c":"code","602f667f":"code","78aa2441":"code","f1575e47":"code","89a336f5":"code","67767540":"code","b07d9d33":"code","7b45d802":"markdown","75d2d419":"markdown","a30b8036":"markdown","645e6b88":"markdown","48733b90":"markdown","52c7735f":"markdown","5786b9d5":"markdown","8f0b6577":"markdown","b4453333":"markdown","fe909735":"markdown","85f9a58d":"markdown"},"source":{"69eddff6":"!pip install -qq tensorflow_decision_forests\n\nfrom scipy.optimize import minimize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.utils import shuffle\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport glob\n\nimport gc\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import initializers\nimport tensorflow_decision_forests as tfdf\n\nfrom keras.models import Model\n\nimport warnings\nwarnings.filterwarnings('ignore')","b34ad660":"dfs = []\nfor path in glob.glob('..\/input\/emg-4\/*'):\n    df = pd.read_csv(path, header=None)\n    dfs.append(df)\n    \ndf = shuffle(pd.concat(dfs))","ed18a0f8":"for i, col in enumerate(list(df.columns[:-1])):\n    df[i] += (-1 * df[i].min())\n    df[i] = pd.to_numeric(df[i], downcast='integer')","9a138f72":"df.head()","619f66a6":"# train\/test - split\ntrain, test = train_test_split(df, test_size=0.3, random_state=42)","77e055ab":"# one hot encoding for ANN\ntargets = pd.get_dummies(train[64])\n\n# normal target for Boosted Trees\ntarget = train[64]","231f8d86":"cce = tf.keras.losses.CategoricalCrossentropy()\n\n# custom loss function\ndef custom_metric(y_true, y_pred):\n    y_pred = K.clip(y_pred, 1e-15, 1-1e-15)\n    loss = K.mean(cce(y_true, y_pred))\n    return loss\n\n# early stop function\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_custom_metric', min_delta=1e-05, patience=5, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True)\n\n# reduce learning rate\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_custom_metric', factor=0.7, patience=2, verbose=0,\n    mode='min')","f51b197a":"def conv_model():\n\n    conv_inputs = layers.Input(shape = (64))\n    \n    embed = layers.Embedding (input_dim = 256, \n                  output_dim = 7,\n                  embeddings_regularizer='l2')(conv_inputs)\n    embed = layers.Conv1D(12,1,activation = 'relu')(embed)        \n    embed = layers.Flatten()(embed)\n    hidden = layers.Dropout(0.3)(embed)\n\n    hidden = tfa.layers.WeightNormalization(\n        layers.Dense(\n        units=32,\n        activation ='selu',\n        kernel_initializer = \"lecun_normal\"))(hidden)\n\n    output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden]))\n    output = tfa.layers.WeightNormalization(\n    layers.Dense(\n        units = 32,\n        activation='relu',\n        kernel_initializer = \"lecun_normal\"))(output) \n    output = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden, output]))\n    output1 = tfa.layers.WeightNormalization(\n    layers.Dense(\n        units = 32, \n        activation = 'relu',\n        kernel_initializer = \"lecun_normal\"))(output)\n\n    conv_outputs = layers.Dense(\n        units = 4, \n        activation ='softmax',\n        kernel_initializer =\"lecun_normal\")(output1)\n\n    model_conv = Model(conv_inputs,conv_outputs)\n    \n    return model_conv, output1","7da4a0b8":"def get_params(model_conv, output1):\n    nn_model_without_head = tf.keras.models.Model(inputs=model_conv.inputs,\n                            outputs=output1)\n    \n    param = {\n                'preprocessing': nn_model_without_head,\n                'use_hessian_gain':True,\n                'selective_gradient_boosting_ratio':0.2,\n                'categorical_algorithm':'RANDOM', \n                'num_trees': 300,\n                'selective_gradient_boosting_ratio':0.2,\n                'subsample': 0.7870499728626467,\n                'shrinkage': 0.018653897565237845,\n                'max_depth' : 3,\n                'min_examples' : 11,\n                'l1_regularization': 3.5480988121992953,\n                'l2_categorical_regularization': 0.11074398839677566\n            }\n    \n    return param","593ae113":"y_valids = []\n\noof_NN_a = np.zeros((train.shape[0],4))\npred_NN_a = np.zeros((test.shape[0],4))\n\noof_NN_g = np.zeros((train.shape[0],4))\npred_NN_g = np.zeros((test.shape[0],4))\n\nNN_g_train_preds = []\nNN_g_test_preds = []\n\nNN_a_train_preds = []\nNN_a_test_preds = []\n\nN_FOLDS = 3\nSEED = 41\nEPOCH = 60\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state= SEED)","f14ec87c":"for fold, (tr_idx, ts_idx) in enumerate(skf.split(train,target)):\n    print(f\"\\n - - - - Training fold: {fold + 1} - - - -\\n\")\n\n    X_train = train.iloc[:,:-1].iloc[tr_idx]\n    y_train = targets.iloc[tr_idx]\n    y_train1 = target.iloc[tr_idx]\n    X_test = train.iloc[:,:-1].iloc[ts_idx]\n    y_test = targets.iloc[ts_idx]\n    y_test1 = target.iloc[ts_idx]\n\n    K.clear_session()\n    \n    # ANN\n    model_conv, output1 = conv_model()\n    \n    print(\"\\nTraining of ANN model\\n\")\n    model_conv.compile(loss='categorical_crossentropy', \n    optimizer = keras.optimizers.Adam(learning_rate=2e-4), \n    metrics=custom_metric)\n    model_conv.fit(X_train, y_train,\n                batch_size = 256, epochs = EPOCH,\n                validation_data=(X_test, y_test),\n                callbacks=[es, plateau],\n                verbose=0)\n    \n    # predict\n    pred_a = model_conv.predict(X_test)\n    oof_NN_a[ts_idx] += pred_a \n    score_NN_a = log_loss(y_test, pred_a)\n    pred_NN_a += model_conv.predict(test.iloc[:,:-1]) \/ N_FOLDS\n    \n    # GBT\n    param = get_params(model_conv, output1)\n    \n    model_Gradient_with_NN = tfdf.keras.GradientBoostedTreesModel(**param)\n    \n    print(\"\\nTraining of GBT with ANN model\\n\")\n    metrics = [tf.keras.metrics.CategoricalCrossentropy()]\n    model_Gradient_with_NN.compile(metrics=metrics)\n    model_Gradient_with_NN.fit(np.array(X_train),np.array(y_train1))\n    \n    pred_g = model_Gradient_with_NN.predict(X_test)\n    oof_NN_g[ts_idx] += pred_g\n    score_GBT_NN = log_loss(y_test1, pred_g)\n    print(f\"\\nFOLD {fold + 1} Score of ANN Model: {score_NN_a}\")\n    print(f\"\\nFOLD {fold + 1} Score of GBT after ANN: {score_GBT_NN}\\n\")\n    \n    # prepare weight optimization\n    y_valid = target.iloc[ts_idx]\n    y_valids.append(y_valid)\n    \n    NN_a_train_preds.append(pred_a)\n    NN_a_test_preds.append(model_conv.predict(test.iloc[:,:-1]))\n\n    NN_g_train_preds.append(pred_g)\n    NN_g_test_preds.append(model_Gradient_with_NN.predict(test.iloc[:,:-1]))","602f667f":"scores = []\nweights = []\n\nfor y, NN_a_pred,NN_g_pred in zip(y_valids, NN_a_train_preds, NN_g_train_preds):\n    preds = []\n    preds.append(NN_a_pred)\n    preds.append(NN_g_pred)\n    \n    starting_values = [0]*len(preds)\n    \n    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n    \n    def log_weight_loss(weights):\n        weighted_pred = ((weights[0]*preds[0]) + (weights[1]*preds[1]))\n        return log_loss(y, weighted_pred)\n    \n    res = minimize(log_weight_loss, \n                   starting_values, \n                   method='Nelder-Mead',\n                   constraints=cons)\n    \n    weights.append(res['x'])\n    scores.append(res['fun'])","78aa2441":"results = pd.DataFrame(weights, columns=['Model ANN','Model GBT'])\nresults['better model'] = results.idxmax(axis=1, skipna=True)\nresults ['max_value'] = results.max(axis=1)\nresults['scores'] = scores\ndisplay(results)","f1575e47":"folds = N_FOLDS\nfinal_weights = sum(weights)\/(folds)\n\n# weighted preds\nweighted_preds = np.array((final_weights[0] * sum(np.array(NN_a_test_preds)\/(folds)))\n                           +(final_weights[1] * sum(np.array(NN_g_test_preds)\/(folds))))\n\n# blends preds\nblends = (pred_NN_g + pred_NN_a)\/2","89a336f5":"def get_accuracy(df, preds):\n    return len(np.where(df.iloc[:,-1].values == np.argmax(preds, axis=1))[0]) \/ len(df)","67767540":"weighted_acc = get_accuracy(test, weighted_preds)\nprint(\"Weighted Accuracy:%1.3f\" % weighted_acc)","b07d9d33":"blends_acc = get_accuracy(test, blends)\nprint(\"Blends Accuracy:%1.3f\" % blends_acc)","7b45d802":"<h1 id=\"train\" style=\"color:#131315; background:#cec6c4; border:0.5px dotted #b1a9c0;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#train\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","75d2d419":"<div>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/88836\/204662\/70c8793a1e85f544086a1d4b19951b51\/dataset-cover.jpg\"\/>\n<\/div>","a30b8036":"<h1 id=\"ensemble\" style=\"color:#131315; background:#cec6c4; border:0.5px dotted #b1a9c0;\"> \n    <center>Ensemble\n        <a class=\"anchor-link\" href=\"#ensemble\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","645e6b88":"<h1 id=\"reference\" style=\"color:#131315; background:#cec6c4; border:0.5px dotted #b1a9c0;\"> \n    <center>Reference\n        <a class=\"anchor-link\" href=\"#reference\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","48733b90":"Tabular June 2021 notebooks (models) + [Laurent Pourchot Gradient Boosting Notebook](https:\/\/www.kaggle.com\/pourchot\/a-neural-network-improved-by-a-gradient-boosting)","52c7735f":"<h1 id=\"optimization\" style=\"color:#131315; background:#cec6c4; border:0.5px dotted #b1a9c0;\"> \n    <center>Optimization\n        <a class=\"anchor-link\" href=\"#optimization\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","5786b9d5":"<h1 id=\"model\" style=\"color:#131315; background:#cec6c4; border:0.5px dotted #b1a9c0;\"> \n    <center>Model\n        <a class=\"anchor-link\" href=\"#model\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","8f0b6577":"<h1 id=\"dataset\" style=\"color:#131315; background:#cec6c4; border:0.5px dotted #b1a9c0;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","b4453333":"<h1 id=\"adds\" style=\"color:#131315; background:#cec6c4; border:0.5px dotted #b1a9c0;\"> \n    <center>Additionals\n        <a class=\"anchor-link\" href=\"#adds\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","fe909735":"<h1 id=\"accuracy\" style=\"color:#131315; background:#cec6c4; border:0.5px dotted #b1a9c0;\"> \n    <center>Accuracy\n        <a class=\"anchor-link\" href=\"#accuracy\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","85f9a58d":"<h1 id=\"boosted\" style=\"color:#131315; background:#cec6c4; border:0.5px dotted #b1a9c0;\"> \n    <center>Boosted Trees Parameters\n        <a class=\"anchor-link\" href=\"#boosted\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}