{"cell_type":{"61a3d5b4":"code","77e9e93a":"code","9376846e":"code","10ea8c59":"code","c1f5bf93":"code","240b255e":"markdown","72974e97":"markdown","9b68df7e":"markdown","f340b2bc":"markdown","f4cf8fe5":"markdown","f1f57ff0":"markdown","71ec5c7d":"markdown","fa82483b":"markdown","1b7ff650":"markdown"},"source":{"61a3d5b4":"import math\nimport pandas as pd\nfrom operator import itemgetter","77e9e93a":"class DecisionTree:\n    def __init__(self, df, target, positive, parent_val, parent):\n        self.data = df\n        self.target = target\n        self.positive = positive\n        self.parent_val = parent_val\n        self.parent = parent\n        self.childs = []\n        self.decision = ''\n\n    def _get_entropy(self, data):\n        p = sum(data[self.target]==self.positive)\n        n = data.shape[0] - p\n        p_ratio = p\/(p+n)\n        n_ratio = 1 - p_ratio\n        entropy_p = -p_ratio*math.log2(p_ratio) if p_ratio != 0 else 0\n        entropy_n = - n_ratio*math.log2(n_ratio) if n_ratio !=0 else 0\n        return entropy_p + entropy_n\n    \n    def _get_gain(self, feat):\n        avg_info=0\n        for val in self.data[feat].unique():\n            avg_info+=self._get_entropy(self.data[self.data[feat] == val])*sum(self.data[feat]==val)\/self.data.shape[0]\n        return self._get_entropy(df) - avg_info\n    \n    def _get_splitter(self):\n        self.splitter = max(self.gains, key = itemgetter(1))[0] \n    \n    def update_nodes(self):\n        self.features = [col for col in self.data.columns if col != self.target]\n        self.entropy = self._get_entropy(self.data)\n        if self.entropy != 0:\n            self.gains = [(feat, self._get_gain(feat)) for feat in self.features]\n            self._get_splitter()\n            residual_columns = [k for k in self.data.columns if k != self.splitter]\n            for val in self.data[self.splitter].unique():\n                df_tmp = self.data[self.data[self.splitter]==val][residual_columns]\n                tmp_node = DecisionTree(df_tmp, self.target, self.positive, val, self.splitter)\n                tmp_node.update_nodes()\n                self.childs.append(tmp_node)","9376846e":"def print_tree(n):\n    for child in n.childs:\n        if child:\n            print(child.__dict__.get('parent', ''))\n            print(child.__dict__.get('parent_val', ''), '\\n')\n            print_tree(child)","10ea8c59":"df = pd.read_csv('..\/input\/dt-id3-examplecsv\/dt_id3_example.csv')\ndf","c1f5bf93":"dt = DecisionTree(df, 'Play', 'Yes', '', '')\ndt.update_nodes()\nprint_tree(dt)","240b255e":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*c0KNcY0rkSLDYPlLuHOLOQ.png)","72974e97":"### Sample run","9b68df7e":"# A simple explanation and implementation of DTs ID3 algorithm in python","f340b2bc":"### Import Libraries","f4cf8fe5":"### Printing the tree","f1f57ff0":"### Simple Decision Tree Class","71ec5c7d":"![](https:\/\/images.unsplash.com\/photo-1570912084442-fe51bd643e5f?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=2068&q=80)","fa82483b":"Decision trees are one of the simplest non-linear supervised algorithms in the machine learning world. As the name suggests they are used for making decisions in ML terms we call it classification (although they can be used for regression as well).\nThe decision trees have a unidirectional tree structure i.e. at every node the algorithm makes a decision to split into child nodes based on certain stopping criteria. Most commonly DTs use entropy, information gain, Gini index, etc.\nThere are a few known algorithms in DTs such as ID3, C4.5, CART, C5.0, CHAID, QUEST, CRUISE. In this article, we would discuss the simplest and most ancient one: ID3.","1b7ff650":"Original article can be found [here](https:\/\/ankit-malik-vi.medium.com\/ml-from-scratch-decision-trees-da68cdaa13bc)"}}