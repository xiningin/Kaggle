{"cell_type":{"899a4d90":"code","8b8ecb8e":"code","0cde84a5":"code","3fee233d":"code","d1e3715f":"code","c31a0dff":"code","a265cba4":"code","2cb2aa57":"code","f67ec6ba":"code","457d3872":"code","b2ab5283":"code","e590dc29":"code","4263be8f":"code","d79eadac":"code","977e199e":"code","c5bfa5c4":"code","d5dc79b2":"code","be7a9f22":"code","0e325d7d":"code","40a78a45":"markdown","04d290a7":"markdown","bf64bf4a":"markdown","6c4d38be":"markdown","e648ea53":"markdown","c58fd74c":"markdown","aecfb4b8":"markdown","a4a55ce7":"markdown","0103ca5f":"markdown","efe9d57e":"markdown","1e9e34c8":"markdown","c435bc66":"markdown","fa55d6c5":"markdown","815f8d1f":"markdown","250867db":"markdown","c1a4cace":"markdown","c0ad414e":"markdown","fa4e9f6b":"markdown","609e2356":"markdown"},"source":{"899a4d90":"import os\nimport shutil\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nfrom os import listdir, makedirs, getcwd, remove\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\n\nfrom keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers","8b8ecb8e":"# Check for the directory and if it doesn't exist, make one.\ncache_dir = expanduser(join('~', '.keras'))\nif not exists(cache_dir):\n    makedirs(cache_dir)\n    \n# make the models sub-directory\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    makedirs(models_dir)","0cde84a5":"# original dataset folder, you can see above\ninput_path = Path('\/kaggle\/input\/flowers-recognition\/flowers')\nflowers_path = input_path \/ 'flowers'","3fee233d":"# Each species of flower is contained in a separate folder. Get all the sub directories\nflower_types = os.listdir(flowers_path)\nprint(\"Types of flowers found: \", len(flower_types))\nprint(\"Categories of flowers: \", flower_types)","d1e3715f":"# A list that is going to contain tuples: (species of the flower, corresponding image path)\nflowers = []\n\nfor species in flower_types:\n    # Get all the file names\n    all_flowers = os.listdir(flowers_path \/ species)\n    # Add them to the list\n    for flower in all_flowers:\n        flowers.append((species, str(flowers_path \/species) + '\/' + flower))\n\n# Build a dataframe        \nflowers = pd.DataFrame(data=flowers, columns=['category', 'image'], index=None)\nflowers.head()","c31a0dff":"# feel free to edit \"0\" (corresponds 0. image)\n# flowers['image'][0]","a265cba4":"# Let's check how many samples for each category are present\nprint(\"Total number of flowers in the dataset: \", len(flowers))\nfl_count = flowers['category'].value_counts()\nprint(\"Flowers in each category: \")\nprint(fl_count)","2cb2aa57":"# Let's do some visualization and see how many samples we have for each category\n\nf, axe = plt.subplots(1,1,figsize=(14,6))\nsns.barplot(x = fl_count.index, y = fl_count.values, ax = axe)\naxe.set_title(\"Flowers count for each category\", fontsize=16)\naxe.set_xlabel('Category', fontsize=14)\naxe.set_ylabel('Count', fontsize=14)\nplt.show()","f67ec6ba":"# Let's visualize some flowers from each category\n\n# A list for storing names of some random samples from each category\nrandom_samples = []\n\n# Get samples fom each category \nfor category in fl_count.index:\n    samples = flowers['image'][flowers['category'] == category].sample(4).values\n    for sample in samples:\n        random_samples.append(sample)\n\n# Plot the samples\nf, ax = plt.subplots(5,4, figsize=(15,10))\nfor i,sample in enumerate(random_samples):\n    ax[i\/\/4, i%4].imshow(mimg.imread(random_samples[i]))\n    ax[i\/\/4, i%4].axis('off')\nplt.show()    ","457d3872":"# Make a parent directory `data` and two sub directories `train` and `valid`\n%mkdir -p data\/train\n%mkdir -p data\/valid\n\n# Inside the train and validation sub=directories, make sub-directories for each catgeory\n%cd data\n%mkdir -p train\/daisy\n%mkdir -p train\/tulip\n%mkdir -p train\/sunflower\n%mkdir -p train\/rose\n%mkdir -p train\/dandelion\n\n%mkdir -p valid\/daisy\n%mkdir -p valid\/tulip\n%mkdir -p valid\/sunflower\n%mkdir -p valid\/rose\n%mkdir -p valid\/dandelion\n\n%cd ..\n\n# You can verify that everything went correctly using ls command","b2ab5283":"for category in fl_count.index:\n    samples = flowers['image'][flowers['category'] == category].values\n    perm = np.random.permutation(samples)\n    # Copy first 100 samples to the validation directory and rest to the train directory\n    for i in range(100):\n        name = perm[i].split('\/')[-1]\n        shutil.copyfile(perm[i],'.\/data\/valid\/' + str(category) + '\/'+ name)\n    for i in range(101,len(perm)):\n        name = perm[i].split('\/')[-1]\n        shutil.copyfile(perm[i],'.\/data\/train\/' + str(category) + '\/' + name)","e590dc29":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',\ninput_shape=(240, 240, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.4))\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(5, activation='softmax'))\n\nmodel.summary()","4263be8f":"model.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-4),\n              metrics=['acc'])","d79eadac":"# Define the generators\nbatch_size = 32\n# this is the augmentation configuration we will use for training\ntrain_datagen = ImageDataGenerator(\n        rescale=1.\/255,\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\n# this is the augmentation configuration we will use for testing:\n# only rescaling\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\n# this is a generator that will read pictures found in\n# subfolers of 'data\/train', and indefinitely generate\n# batches of augmented image data\ntrain_generator = train_datagen.flow_from_directory(\n        'data\/train',  # this is the target directory\n        target_size=(240, 240),  # all images will be resized to 150x150\n        batch_size=batch_size,\n        class_mode='categorical')  # more than two classes\n\n# this is a similar generator, for validation data\nvalidation_generator = test_datagen.flow_from_directory(\n        'data\/valid',\n        target_size=(240, 240),\n        batch_size=batch_size,\n        class_mode='categorical')","977e199e":"from keras.preprocessing import image\nfnames = [os.path.join('data\/train\/dandelion', fname) for\nfname in os.listdir('data\/train\/dandelion')]\nimg_path = fnames[22]\nimg = image.load_img(img_path, target_size=(240, 240))\n\nx = image.img_to_array(img)\nx = x.reshape((1,) + x.shape)\ni = 0\nf, axes = plt.subplots(1,4,figsize=(14,4))\nfor batch in train_datagen.flow(x, batch_size=1):\n    imgplot = axes[i].imshow(image.array_to_img(batch[0]))\n    i += 1\n    if i % 4 == 0:\n        break\nplt.show()","c5bfa5c4":"history = model.fit_generator(\n          train_generator,\n          steps_per_epoch=100,\n          epochs=100,\n          validation_data=validation_generator,\n          validation_steps=50)","d5dc79b2":"model.save('flowers_recognition_v2.h5')","be7a9f22":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nf, axes = plt.subplots(1,2,figsize=(14,4))\n\naxes[0].plot(epochs, acc, 'bo', label='Training acc')\naxes[0].plot(epochs, val_acc, 'b', label='Validation acc')\naxes[0].legend()\n\naxes[1].plot(epochs, loss, 'bo', label='Training loss')\naxes[1].plot(epochs, val_loss, 'b', label='Validation loss')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].legend()\n\nplt.show()","0e325d7d":"# deleting train and test sets, because kaggle is trying to show all\n# images that we created as output\nshutil.rmtree(\"\/kaggle\/working\/data\")","40a78a45":"If you are familiar with **cd** and **ls** commands, you can see new folders and image files by using them.","04d290a7":"### <span id=\"5\"><\/span> ** Displaying Some Augmented Images **","bf64bf4a":"## <span id=\"2\"><\/span> ** 2. Building the Network **","6c4d38be":"Original folder list is here as you can see below. But we need a training and test set. Let's get them.\n```\n\/kaggle\/input\/flowers-recognition\/\n    flowers\/\n        tulip\n        daisy\n        sunflower\n        rose\n        dandelion\n```","e648ea53":"Now, it's time to prepare our training and test sets. This code below copying files into new folders and creating new images stored like below.\n```\ndata\/\n    train\/\n        daisy  \n        dandelion\n        rose\n        sunflower\n        tulip\n            \n    validation\/\n        daisy  \n        dandelion\n        rose\n        sunflower\n        tulip\n```","c58fd74c":"In this kernel, I have built a ConvNet from scratch using [Flowers Recognition](https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition) data. My goal wasn't to get the best accuracy, but to learn about ConvNet in Keras. Using pre-trained models (transfer learning) can give better results.\n\n<b><font color=\"red\">Don't forget to <\/font><\/b> <b><font color=\"green\">UPVOTE <\/font><\/b> if you liked this kernel, thank you. \ud83d\ude42\ud83d\udc4d\n","aecfb4b8":"### <span id=\"6\"><\/span> ** Fitting the Model **","a4a55ce7":"### <span id=\"3\"><\/span> ** Configuring the Model for Training **","0103ca5f":"I have designed a diagram in order to visualize the architecture of model. If you're comfortable about theoretical knowledge of Convolutional Neural Networks, hope this make sense. But I know that there are many practitioners on Kaggle, hey guys, just scroll down! :)\n<img src=\"https:\/\/docs.google.com\/uc?id=1aipvkky5CyHQx2vyYoUdqAdRuaX1YSJi\" width=\"850px\">","efe9d57e":"### <span id=\"8\"><\/span> ** Displaying curves and accuracy during training **","1e9e34c8":"## <span id=\"1\"><\/span> ** 1. Importing Libraries and Reading the Dataset **\nThe images dataset are not seperated into training and test set. In this section, we are going to copy images and divide into two folder (train and validation). I have added detailed folder list that you can see later. But simply, the folder will bel like this:\n```\ndata\/\n    train\/\n        category1\/(contains all images related to category1)  \n        category2\/(contains all images related to category2)\n        ...\n        ...\n            \n    validation\/\n        category1\/(contains all images related to category1)  \n        category2\/(contains all images related to category2)\n        ...\n        ...\n```","c435bc66":"### <span id=\"4\"><\/span> ** Data Augmentation **","fa55d6c5":"### <span id=\"7\"><\/span> ** Saving the model **","815f8d1f":"<img src=\"https:\/\/drive.google.com\/uc?export=download&id=1L7ZFYFXCzc01DOKnbaDd5YV5GZgo_htn\" width=\"750px\">\n<hr\/>\n[**Tolgahan \u00c7epel**](https:\/\/www.kaggle.com\/tolgahancepel)\n<hr\/>\n\n<font color=green>\n* 1. [Importing Libraries and Reading the Dataset](#1)\n* 2. [Building the Network](#2)\n    * [Configuring the Model for Training](#3)\n    * [Data Augmentation](#4)\n    * [Displaying Some Augmented Images](#5)\n    * [Fitting the model](#6)\n    * [Saving the model](#7)\n    * [Displaying curves of loss and accuracy during training](#8)\n* 3. [Conclusion](#9)\n<hr\/>","250867db":"Building a ConvNet from scratch is not the best method to get the best accuracy. However, we are learning, so be cool!","c1a4cace":"This cell above, creating a dataframe which contains all images and their path. If we run this command to see first row's image path:\n```\n>> flowers['image'][0]\n```\n\nThe output:\n```\n'\/kaggle\/input\/flowers-recognition\/flowers\/tulip\/122450705_9885fff3c4_n.jpg'\n```\n","c0ad414e":"Because we have a very small dataset (4242 samples), data augmentation can help to improve accuracy. But also we need to rescale images. All these processes coded in this cell below.","fa4e9f6b":"## <span id=\"9\"><\/span> ** 4. Conclusion **","609e2356":"Let's visualize an augmanted image and see what we got!"}}