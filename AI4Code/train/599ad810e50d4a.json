{"cell_type":{"e3de5fc5":"code","8c1a9fb9":"code","ac520275":"code","4542e6b3":"code","b0b3bea5":"code","d0f9a1c4":"code","bcc355bf":"code","b6ccf198":"code","5e1e5e39":"code","0eacdbff":"code","09c37aab":"code","65f2829a":"code","3c1de99b":"code","031f2ae9":"code","3f02e848":"code","95842faa":"code","c759e902":"code","a8e2b687":"code","b5ba8a22":"code","427d9205":"code","76fffe1d":"code","6c2280c1":"code","6e529192":"code","e1fce41f":"code","eb031815":"code","e4a2e851":"code","29b2d602":"code","7f798ad3":"code","ffe3e94f":"code","1f572134":"code","3eaaea61":"code","4424ddc1":"code","630efe04":"code","3ccb6d1d":"code","0f6d4602":"code","0f9b6d2c":"code","d758fedd":"code","3eec7fc0":"code","f983b860":"code","200a9eb2":"code","d60ebcfc":"code","b4a15985":"code","c3cd578f":"code","52c3d61e":"code","91640782":"code","f108c154":"code","78eca344":"code","955453a4":"code","424fc0c8":"code","9e398cce":"code","4d815a45":"code","450b3395":"code","e7a8b94a":"code","5da18d60":"code","9113de93":"code","c219ff87":"code","7c88f973":"code","55fd5fd6":"code","4d321f7c":"code","eeaee017":"code","68be0cfe":"code","39341e5c":"code","dc7bd4b7":"code","ffee85eb":"code","e398e0ab":"code","0c20f11a":"code","87fe25ac":"code","9276a271":"code","9bfe12c4":"code","56595793":"code","1ebd52bb":"code","af221e49":"code","a8bb030a":"markdown","da95c814":"markdown","993feffa":"markdown","9b546a64":"markdown","0e817e34":"markdown","4679043e":"markdown","bcba15eb":"markdown","6187a2b8":"markdown","5d9cdb3e":"markdown","de4ef151":"markdown","92f3e960":"markdown","03ba77ac":"markdown","060864d9":"markdown","079627e8":"markdown","6c7ce6b1":"markdown","41297ec9":"markdown","347bf80c":"markdown","c5a701f8":"markdown","fee1e2e6":"markdown","24125200":"markdown","ca3bd3cb":"markdown","4000527e":"markdown","89173e42":"markdown","2df11550":"markdown","c553b7be":"markdown","aaddc741":"markdown","7da9ca89":"markdown","6b7f3c72":"markdown","f7735a64":"markdown","1e9d7608":"markdown","fa32dfbd":"markdown","2e0a6995":"markdown","98704754":"markdown","17b0296a":"markdown","f939d0d0":"markdown"},"source":{"e3de5fc5":"from sklearn.datasets import load_boston\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8c1a9fb9":"boston = load_boston()\nx = boston.data\ny = boston.target\ncolumns = boston.feature_names\n\n#create the dataframe\nboston_df = pd.DataFrame(boston.data)\nboston_df.columns = columns\nboston_df['TARGET'] = y\nboston_df.head()","ac520275":"fig = px.scatter(boston_df, x='CRIM', y='TARGET')\nfig.show()","4542e6b3":"fig = px.box(boston_df, y='CRIM')\nfig.show()","b0b3bea5":"fig = px.histogram(boston_df, x='CRIM', marginal=\"box\")\nfig.show()","d0f9a1c4":"fig = px.scatter(boston_df, x='INDUS', y='TARGET')\nfig.show()","bcc355bf":"fig = px.box(boston_df, y='INDUS')\nfig.show()","b6ccf198":"from scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(boston_df))\nthreshold = 3\nprint('list of outliers:', '\\n', np.where(z > 3)[0], '\\n', np.where(z > 3)[1])\nprint('number of outliers:', np.where(z > 3)[0].shape)\nprint('row: 466, column: 11', z[466][11])","5e1e5e39":"# First we will calculate IQR\nboston_df_CRIM = boston_df['CRIM']\nQ1 = boston_df_CRIM.quantile(0.25)\nQ3 = boston_df_CRIM.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","0eacdbff":"# create filter to detect values below and above outlier thresholds\n(boston_df_CRIM < (Q1 - 1.5 * IQR)) |(boston_df_CRIM > (Q3 + 1.5 * IQR))","09c37aab":"# Values below or above outlier thresholds\nprint(boston_df_CRIM[(boston_df_CRIM < (Q1 - 1.5 * IQR)) | (boston_df_CRIM > (Q3 + 1.5 * IQR))])\nprint(boston_df_CRIM[(boston_df_CRIM < (Q1 - 1.5 * IQR)) | (boston_df_CRIM > (Q3 + 1.5 * IQR))].shape)","65f2829a":"# IQR values for each variable\nQ1 = boston_df.quantile(0.25)\nQ3 = boston_df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","3c1de99b":"(boston_df < (Q1 - 1.5 * IQR)) |(boston_df > (Q3 + 1.5 * IQR))","031f2ae9":"# boolean to detect any value equal to true in one of the columns\nboston_df.where((boston_df < (Q1 - 1.5 * IQR)) | (boston_df > (Q3 + 1.5 * IQR))).any(True)","3f02e848":"from sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\n\ny = boston_df['TARGET']\nX = boston_df.drop(['TARGET'], axis=1)\n\nclf = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.10, random_state=42)\nclf.fit(X)\ny_pred = clf.predict(X)\n\n# the model will predict an inlier with a label of +1 and an outlier with a label of -1\n\noutliers_values = X[clf.predict(X) == -1]\noutliers_values","95842faa":"from sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\ncolumns = boston_df.columns\n\n#note that we transform the data with MinMaxScaler\nboston_df_scaled = scaler.fit_transform(boston_df)\nboston_df_scaled = pd.DataFrame(boston_df_scaled, columns=columns)\n\ny = boston_df_scaled['TARGET']\nX = boston_df_scaled.drop(['TARGET'], axis=1)\n\n# fit the model for outlier detection (default)\nclf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n\n# use fit_predict to compute the predicted labels of the training samples\n# (when LOF is used for outlier detection, the estimator has no predict,\n# decision_function and score_samples methods).\n\ny_pred = clf.fit_predict(X)\nX_scores = clf.negative_outlier_factor_","c759e902":"X_scores[0:10]","a8e2b687":"np.sort(X_scores)[:50]","b5ba8a22":"# threshold\nthreshold = np.sort(X_scores)[24]\nthreshold","427d9205":"# filter to detect to outliers\ninliers = X_scores > threshold\ninliers","76fffe1d":"X[(X_scores > threshold)==False]","6c2280c1":"boston_df_new = boston_df[~((boston_df < (Q1 - 1.5 * IQR)) |(boston_df > (Q3 + 1.5 * IQR))).any(axis=1)]\nprint(boston_df.shape)\nprint(boston_df_new.shape)\n# if we want to remove all the rows that are below or above the outlier threshold for each variable, we lose a lot of data","6e529192":"boston_df2 = boston_df.copy()\n\nQ1 = boston_df.CRIM.quantile(0.25)\nQ3 = boston_df.CRIM.quantile(0.75)\nIQR = Q3 - Q1\nprint(f'IQR: {IQR}')\nprint(f'Mean: {boston_df.CRIM.mean()}')\nprint(f'Median: {boston_df.CRIM.median()}')\n\nboston_df2.CRIM[((boston_df2.CRIM < (Q1 - 1.5 * IQR)) |(boston_df2.CRIM > (Q3 + 1.5 * IQR)))] = boston_df.CRIM.mean()","e1fce41f":"fig = px.box(boston_df2, y='CRIM')\nfig.show()","eb031815":"boston_df_new = boston_df[(X_scores > threshold)==True]\n# delete observations based on the threshold decided through isolation forest","e4a2e851":"boston_df.shape","29b2d602":"boston_df_new.shape","7f798ad3":"boston_df[(X_scores == threshold)]\n# the threshold observation","ffe3e94f":"threshold_row = boston_df[(X_scores == threshold)]","1f572134":"outliers = boston_df[(X_scores < threshold)]\noutliers","3eaaea61":"outliers.to_records(index = False)\n\n# We got rid of the indexes of outliers and transformed them into array, \n# because indexes may lead to problems in this process.","4424ddc1":"outliers_array = outliers.to_records(index = False)\n# we define a variable for outlier array","630efe04":"outliers_array[:] = threshold_row.to_records(index = False)\n# We replace all the outliers with the threshold row","3ccb6d1d":"outliers_array","0f6d4602":"boston_df_o = boston_df.copy()\nboston_df_o[(X_scores < threshold)] = pd.DataFrame(outliers_array, index = boston_df_o[(X_scores < threshold)].index)\nboston_df_o[(X_scores < threshold)]\n# the outlier observations after replacement by the threshold observation","0f9b6d2c":"fig = px.box(boston_df, y='CRIM')\nfig.show()","d758fedd":"fig = px.box(boston_df_o, y='CRIM')\nfig.show()","3eec7fc0":"# sns.load_dataset('planets')\n","f983b860":"import seaborn as sns\ndf_planets = pd.read_csv(\"\/kaggle\/input\/seaborn-practice\/planets.csv\")\ndf_planets.head()","200a9eb2":"# number of nan values in each column\ndf_planets.isnull().sum()","d60ebcfc":"# observations with NaN values\ndf_planets[df_planets.isnull().any(axis=1)]","b4a15985":"import missingno as msno\n\nmsno.bar(df_planets);","c3cd578f":"msno.matrix(df_planets);","52c3d61e":"msno.heatmap(df_planets);","91640782":"msno.dendrogram(df_planets);","f108c154":"df_planets_no_nan = df_planets.dropna()\ndf_planets_no_nan.isnull().sum()","78eca344":"df_planets2 = df_planets.copy()\ndf_planets2.isnull().sum()","955453a4":"df_planets2['orbital_period'].fillna(df_planets2['orbital_period'].mean(), inplace=True)\ndf_planets2.isnull().sum()","424fc0c8":"df_planets2[\"mass\"].fillna(df_planets2.groupby(\"method\")[\"mass\"].transform(\"mean\"), inplace=True) \ndf_planets2.isnull().sum()","9e398cce":"df_planets2.groupby(\"method\")[\"mass\"].mean()","4d815a45":"df_planets3 = df_planets.copy()\ndf_planets3 = pd.get_dummies(df_planets3, drop_first=True, columns = [\"method\"], prefix = [\"method\"])\ndf_planets3.head()","450b3395":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nindex = df_planets3.index\ncolumns = df_planets3.columns\n\nimp_mean = IterativeImputer(random_state=0)\nimp_mean.fit(df_planets3)\ndf_planets3_imputed = imp_mean.transform(df_planets3)\ndf_planets3_imputed","e7a8b94a":"df_planets3_imputed = pd.DataFrame(df_planets3_imputed, index=index, columns=columns)\ndf_planets3_imputed.isnull().sum()","5da18d60":"from sklearn.tree import DecisionTreeRegressor\n\ndf_planets4 = df_planets.copy()\ndf_planets4 = pd.get_dummies(df_planets4, drop_first=True, columns = [\"method\"], prefix = [\"method\"])\nindex = df_planets4.index\ncolumns = df_planets4.columns\n\nimp_mean = IterativeImputer(random_state=0, estimator=DecisionTreeRegressor())\nimp_mean.fit(df_planets4)\ndf_planets4_imputed = imp_mean.transform(df_planets4)\ndf_planets4_imputed","9113de93":"df_planets4_imputed = pd.DataFrame(df_planets4_imputed, index=index, columns=columns)\ndf_planets4_imputed.isnull().sum()","c219ff87":"from sklearn.neighbors import KNeighborsRegressor\n\ndf_planets5 = df_planets.copy()\ndf_planets5 = pd.get_dummies(df_planets5, drop_first=True, columns = [\"method\"], prefix = [\"method\"])\nindex = df_planets5.index\ncolumns = df_planets5.columns\n\nimp_mean = IterativeImputer(random_state=0, estimator=KNeighborsRegressor())\nimp_mean.fit(df_planets5)\ndf_planets5_imputed = imp_mean.transform(df_planets5)\ndf_planets5_imputed","7c88f973":"df_planets5_imputed = pd.DataFrame(df_planets5_imputed, index=index, columns=columns)\ndf_planets5_imputed.isnull().sum()","55fd5fd6":"# spliting training and testing data\nfrom sklearn.model_selection import train_test_split\n\n# data normalization with sklearn\nfrom sklearn.preprocessing import MinMaxScaler\n\ny = boston_df['TARGET']\nX = boston_df.drop(['TARGET'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n\n# fit scaler on training data\nnorm = MinMaxScaler().fit(X_train)\n\n# transform training data\nX_train_norm = norm.transform(X_train)\n\n# transform testing dataabs\nX_test_norm = norm.transform(X_test)\n\nX_train_norm","4d321f7c":"index = X_train.index\ncolumns = X_train.columns\n\ndf_train_norm = pd.DataFrame(X_train_norm, columns = columns, index = index)\ndf_train_norm","eeaee017":"fig = px.box(boston_df, y='CRIM')\nfig.show()","68be0cfe":"fig = px.box(df_train_norm, y='CRIM')\nfig.show()","39341e5c":"from sklearn.preprocessing import StandardScaler\n\n# copy of datasets\nX_train_stand = X_train.copy()\nX_test_stand = X_test.copy()\n\nindex = X_train.index\ncolumns = X_train.columns\n\n# fit scaler on training data\nscaler = StandardScaler().fit(X_train)\n\n# transform training data\nX_train_stand = scaler.transform(X_train)\n\n# transform testing dataabs\nX_test_stand = scaler.transform(X_test)\n\nX_train_stand","dc7bd4b7":"df_train_stand = pd.DataFrame(X_train_stand, columns = columns, index = index)\ndf_train_stand","ffee85eb":"fig = px.box(df_train_stand, y='CRIM')\nfig.show()","e398e0ab":"import seaborn as sns\ndf_tips = pd.read_csv(\"\/kaggle\/input\/seaborn-practice\/tips.csv\")\ndf_tips.head()","0c20f11a":"from sklearn.preprocessing import LabelEncoder\n\nlbe = LabelEncoder()\nlbe.fit_transform(df_tips[\"sex\"])","87fe25ac":"df_tips[\"sex_label\"] = lbe.fit_transform(df_tips[\"sex\"])\ndf_tips.head()","9276a271":"df_tips[\"day\"].str.contains(\"Sun\")","9bfe12c4":"df_tips[\"day_new\"] = np.where(df_tips[\"day\"].str.contains(\"Sun\"), 1, 0)\ndf_tips.head()","56595793":"lbe = LabelEncoder()\ndf_tips[\"day_label\"] = lbe.fit_transform(df_tips[\"day\"])\ndf_tips.day_label.unique()","1ebd52bb":"df_tips_one_hot = pd.get_dummies(df_tips, columns = [\"day\"], prefix = [\"day\"])\ndf_tips_one_hot.head()","af221e49":"# drop the first column option \n\ndf_tips_one_hot_2 = pd.get_dummies(df_tips, drop_first=True, columns = [\"day\"], prefix = [\"day\"])\ndf_tips_one_hot_2.head()","a8bb030a":"* https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3668100\/\n* https:\/\/towardsdatascience.com\/how-to-handle-missing-data-8646b18db0d4\n* https:\/\/medium.com\/@danberdov\/dealing-with-missing-data-8b71cd819501","da95c814":"# Standardization and Normalization\n\n* Some machine learning algorithms are sensitive to feature scaling while others are virtually invariant to it.\n    * __Machine learning algorithms__ like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled.\n    * __Distance algorithms__ like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.\n    * __Tree-based algorithms__, on the other hand, are fairly insensitive to the scale of the features. Think about it, a decision tree is only splitting a node based on a single feature. The decision tree splits a node on a feature that increases the homogeneity of the node. This split on a feature is not influenced by other features.\n*  You should consider the scaling method as an important hyperparameter of your model.\n\n## Normalization\n* Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n* X' = (X - Xmin) \/ (Xmax - Xmin)\n    * Here, Xmax and Xmin are the maximum and the minimum values of the feature respectively. \n    * When the value of X is the minimum value in the column, the numerator will be 0, and hence X' is 0.\n    * On the other hand, when the value of X is the maximum value in the column, the numerator is equal to the denominator and thus the value of X' is 1.\n    * If the value of X is between the minimum and the maximum value, then the value of X' is between 0 and 1.\n\n## Standardization \n* Standardization (Z-score normalization) is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation. In other words, it rescales data to have a mean of 0 and a standard deviation of 1 (unit variance).\n    * X' = (X - \u03bc) \/ \u03c3\n    * Mu is the mean of the feature values and Sigma is the standard deviation of the feature values. Note that in this case, the values are not restricted to a particular range.\n* __Log transformation__ is a data transformation method in which it replaces each observation x with a log(x).\n* __Binary transformation__ is a data transformation method in which it replaces each observation with 1 if it is above a certain threshold, and with 0 if it is below a certain threshold. \n\n## Normalization vs Standardization\n* There is no obvious answer to this question: it really depends on the application. \n* __Normalization__ is a good technique to use when you do not know the distribution of your data or when you know the distribution is not Gaussian (a bell curve). Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks.\n* __Standardization__ assumes that your data has a Gaussian (bell curve) distribution. This does not strictly have to be true, but the technique is more effective if your attribute distribution is Gaussian. Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression, and linear discriminant analysis. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.","993feffa":"### Standardization","9b546a64":"### Imputation through predictive models","0e817e34":"* The below code will give an output with some true and false values. The data point where we have False that means these values are valid whereas True indicates presence of an outlier.","4679043e":"## Methods to detect outliers\n\n### Sector and Field Knowledge\n* The interpretability of an outlier model is very important, and decisions seeking to tackle an outlier need some context or rationale. In fact, outliers sometimes can be helpful indicators. For example, in some applications of data analytics like credit card fraud detection, outlier analysis becomes important because here, the exception rather than the rule may be of interest to the analyst.","bcba15eb":"# Problems with the Data\n* __Noisy and inconsistent data:__ Data can be noisy, having incorrect attribute values. Owing to the following, the data collection instruments used may be fault. Maybe human or computer errors occurred at data entry. Errors in data transmission can also occur. This can also include duplicates or semi-duplicates of the data records; data segments, which have no value for a particular research; unnecessary information fields for each of the variables.\n* __Outlier:__ The outliers are the singular data points dissimilar to the rest of the domain. An outlier is an observation point that is distant from other observations.\n* __Missing values:__ In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. ","6187a2b8":"# Outlier\n* The outliers can be a result of a mistake during data collection or it can be just an indication of variance in your data.\n    * Outliers badly affect mean and standard deviation of the dataset. These may statistically give erroneous results.\n    * It increases the error variance and reduces the power of statistical tests.\n    * If the outliers are non-randomly distributed, they can decrease normality.\n    * Most machine learning algorithms do not work well in the presence of outlier. So it is desirable to detect and remove outliers.\n    * They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions.\n* If they are the result of a mistake, then we can ignore them, but if it is just a variance in the data we would need think a bit further. Before we try to understand whether to ignore the outliers or not, we need to know the ways to identify them.","5d9cdb3e":"### Normalization","de4ef151":"### Label Encoding","92f3e960":"## Approaches to handle outliers\n* Deleting observations\n* Replacing with the mean or the median of the variable\n* Replacing with a threshold value","03ba77ac":"* https:\/\/towardsdatascience.com\/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n* https:\/\/machinelearningmastery.com\/how-to-one-hot-encode-sequence-data-in-python\/#:~:text=A%20one%20hot%20encoding%20is,is%20marked%20with%20a%201.","060864d9":"### Univariate Approach","079627e8":"### Multivariate approach","6c7ce6b1":"* https:\/\/towardsdatascience.com\/ways-to-detect-and-remove-the-outliers-404d16608dba\n* boston data set as an example\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_boston.html\n* https:\/\/machinelearningmastery.com\/how-to-use-statistics-to-identify-outliers-in-data\/\n* https:\/\/medium.com\/analytics-vidhya\/how-to-remove-outliers-for-machine-learning-24620c4657e8\n* https:\/\/machinelearningmastery.com\/one-class-classification-algorithms\/","41297ec9":"### Multivariate approaches to detect outliers:\n* Instead of treating each variable to detect and handle outliers separately, this approach considers the data as a whole. \n* __One-Class Classification__, or OCC for short, involves fitting a model on the \u201cnormal\u201d data and predicting whether new data is normal or an outlier\/anomaly.\n* The scikit-learn library provides a handful of common one-class classification algorithms intended for use in outlier or anomaly detection and change detection, such as One-Class SVM, Isolation Forest, Elliptic Envelope, and Local Outlier Factor.\n* __Isolation Forest:__ Isolation Forest, or iForest for short, is a tree-based anomaly detection algorithm. \n    * It is based on modeling the normal data in such a way to isolate anomalies that are both few in number and different in the feature space. \n    * Tree structures are created to isolate anomalies. The result is that isolated examples have a relatively short depth in the trees, whereas normal data is less isolated and has a greater depth in the trees.\n![title](isolation_forest.png)\n* __Local Outlier Factor:__ A simple approach to identifying outliers is to locate those examples that are far from the other examples in the feature space. \n    * This can work well for feature spaces with low dimensionality (few features), although it can become less reliable as the number of features is increased, referred to as the curse of dimensionality.\n    * The local outlier factor, or LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier detection.\n![title](sphx_glr_plot_lof_outlier_detection_001.png)","347bf80c":"### Z-score\n* The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.\n* The intuition behind Z-score is to describe any data point by finding their relationship with the Standard Deviation and Mean of the group of data points. Z-score is finding the distribution of data where mean is 0 and standard deviation is 1 i.e. normal distribution.\n* The data points which are way too far from zero will be treated as the outliers. In most of the cases a threshold of 3 or -3 is used i.e if the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers.","c5a701f8":"# Missing Data\n\n* One of the most common problems in Data Science is handling the missing values. Firstly, there is NO good way to deal with missing data and it is difficult to provide a general solution. Thus, it is important the understand the nature of the data, the purpose of the task and treat the missing values accordingly. \n\n## Types of Missing Data\n\n### Missing Completely at Random (MCAR): \n* The fact that a certain value is missing has nothing to do with its hypothetical value and with the values of other variables.\n* Values in a data set are missing completely at random (MCAR) if the events that lead to any particular data-item being missing are independent both of observable variables and of unobservable parameters of interest, and occur entirely at random.\n\n### Missing at Random (MAR): \n* The data is missing relative to the observed data. It is not related to the specific missing values. \n* The data is not missing across all observations but only within sub-samples of the data. It is not known if the data should be there; instead, it is missing given the observed data. The missing data can be predicted based on the complete observed data.\n\n### Missing Not at Random (MNAR)\n* The MNAR category applies when the missing data has a structure to it. In other words, there appear to be reasons the data is missing.\n\n## Methods to Detect the Randomness of Missing Values\n* __Visualisation methods e.g. missingno__\n* __The Independent-Samples T Test__ procedure compares means for two groups of cases. Ideally, for this test, the subjects should be randomly assigned to two groups, so that any difference in response is due to the treatment (or lack of treatment) and not to other factors.\n* __Correlation test__\n* __Little's MCAR Test:__ Tests the null hypothesis that the missing data is Missing Completely At Random (MCAR). A p.value of less than 0.05 is usually interpreted as being that the missing data is not MCAR (i.e., is either Missing At Random or non-ignorable). \n\n## Methods to Handle Missing Values\n\n### Deletion\n* __Listwise deletion:__ (complete-case analysis) removes all data for an observation that has one or more missing values. Particularly if the missing data is limited to a small number of observations, you may just opt to eliminate those cases from the analysis. However in most cases, it is often disadvantageous to use listwise deletion. This is because the assumptions of MCAR (Missing Completely at Random) are typically rare to support. As a result, listwise deletion methods produce biased parameters and estimates.\n* __Pairwise deletion:__ analyses all cases in which the variables of interest are present and thus maximizes all data available by an analysis basis. A strength to this technique is that it increases power in your analysis but it has many disadvantages. It assumes that the missing data are MCAR. If you delete pairwise then you\u2019ll end up with different numbers of observations contributing to different parts of your model, which can make interpretation difficult.\n* __Dropping variable:__ There are situations when the variable has a lot of missing values, in this case, if the variable is not a very important predictor for the target variable, the variable can be dropped completely. As a rule of thumb, when the data goes missing on 60\u201370 percent of the variable, dropping the variable should be considered.\n\n### Imputation\n* Replacing the missing values with mean, mode, median. Such imputation is the most basic approach but it decreases the variance in the data. \n* Using a categorical variable and using the mean values for imputation based on the categories of this variable. \n* For categorical variables, the most common approach is to use mode for imputation. Also, the previous or the next value can be also used as a method. An alternative approach is to treat the missing values as another category by replacing them with \"Other\".\n* Hot deck imputation is a method for handling missing data in which each missing value is replaced with an observed response from a \u201csimilar\u201d unit. \n\n### Predictive models\n* The machine learning models can be used to predict missing values. This approach is also called multivariate approach because the other variables are used to predict the missing values in a specific variable or all the missing values in each variable.\n* Scikitlearn library has a recent method called __Iterative Imputer__ to implement multivariate imputation. ","fee1e2e6":"### Local outlier factor","24125200":"### Replacing outliers with threshold","ca3bd3cb":"# Data Preprocessing\n* Data preprocessing is transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, and\/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues.\n* __Data cleansing or data cleaning__ is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.","4000527e":"### Binary transformation","89173e42":"### IQR\n* In this method by using Inter Quartile Range(IQR), we detect outliers. IQR tells us the variation in the data set. Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR treated as outliers.\n    * Q1 represents the 1st quartile\/25th percentile of the data.\n    * Q2 represents the 2nd quartile\/median\/50th percentile of the data.\n    * Q3 represents the 3rd quartile\/75th percentile of the data.\n    * (Q1\u20131.5IQR) represent the smallest value in the data set and (Q3+1.5IQR) represent the largest value in the data set.","2df11550":"### Delete the observations with NaN values","c553b7be":"# Data Transformation\n\n## Transformation in Categorical Variables\n* __Binary transformation:__ Replace the variables having two categories with the values of 0 and 1. \n* Giving 1 to one category and 0 to the other categories. \n\n### Label encoding \n* This approach is very simple and it involves converting each value in a column to a number.* \n* __Ordinality problem:__ But depending upon the data values and type of data, label encoding induces a new problem since it uses number sequencing. Thus, it has the disadvantage that the numeric values can be misinterpreted by algorithms as having some sort of hierarchy\/order in them. \n* If there is a hierarchy between the categories such as 'Very High', 'High', 'Medium' and 'Low', it is more logical to use label encoding. \n\n### One-hot encoding\n*  This ordering issue is addressed in another common alternative approach called \u2018One-Hot Encoding\u2019. In this strategy, each category value is converted into a new column and assigned a 1 or 0 (notation for true\/false) value to the column.\n* Though this approach eliminates the hierarchy\/order issues but does have the downside of adding more columns to the data set. It can cause the number of columns to expand greatly if you have many unique values in a category column.\n\n## Transformation in Continuous Variables\n* Continuous variable can be divided into categories based on certain intervals such as dividing age variable into categories to see difference between various age groups. ","aaddc741":"* https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/\n* https:\/\/towardsai.net\/p\/data-science\/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff\n* https:\/\/www.geeksforgeeks.org\/normalization-vs-standardization\/","7da9ca89":"### Deleting observations","6b7f3c72":"### Deleting observations","f7735a64":"### Filling the NaN values with mean","1e9d7608":"### Isolation Forest","fa32dfbd":"### MissingNo Analysis","2e0a6995":"### Giving 1 to one category and 0 to the others","98704754":"### Replacing with the mean or the median of the variable","17b0296a":"### Data visualisation\n* __Scatter plot:__ Scatter plot graph points on two axes using Cartesian coordinates. By graphing the points this way, we can visually identify points that fall outside the expected grouping. These points are likely to be outliers. \n* __Box plot:__  In descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles. Outliers will appear separate from the plot.","f939d0d0":"### One-hot encoding"}}