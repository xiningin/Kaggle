{"cell_type":{"dffc13eb":"code","a2ba9ec5":"code","6ab5798f":"code","ee47e690":"code","b29a1ee5":"code","cadc4d90":"code","8e287e9c":"code","281bdff1":"code","f1d4a448":"code","ff2820ac":"code","424385d4":"code","0011a926":"code","3f5a1b59":"code","a1661916":"code","1f9536d9":"code","53629ae0":"code","3cae2576":"code","aefae855":"code","d8d9f82e":"code","b3225d8c":"code","bf940237":"code","15d8b906":"code","fcfee5cd":"code","e4e7b2cb":"code","acc48416":"code","3b6f925f":"code","7a75cdc8":"code","938e3ee1":"markdown","3fa0238a":"markdown","a6d51818":"markdown","a1849974":"markdown","6a2b6f58":"markdown","4258d1f0":"markdown","f0f93e87":"markdown","4e9e8ea3":"markdown","20d7dfd1":"markdown","ee4e6a8d":"markdown","c5aa74e3":"markdown","bf178254":"markdown","ef529cd9":"markdown","00c69b74":"markdown","37661323":"markdown","bf359ddd":"markdown","f9c2d8b5":"markdown","1facbc75":"markdown","a3add4a6":"markdown","ed0ccfa9":"markdown","a7b64a56":"markdown","799bdcce":"markdown","9eb6a9dd":"markdown","080d8fbd":"markdown","f4ec1820":"markdown","339873d8":"markdown","9ae4d32f":"markdown"},"source":{"dffc13eb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2ba9ec5":"df=pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\ndf.head()","6ab5798f":"cols = np.arange(21,df.shape[1])\ndf.drop(df.columns[cols],axis=1,inplace=True)\ndf.describe()","ee47e690":"df_frequency=pd.concat([df['Customer_Age'],df['Total_Trans_Ct'],df['Total_Trans_Amt'],df['Months_Inactive_12_mon'],df['Credit_Limit'],df['Attrition_Flag']],axis=1)\nfig,ax=plt.subplots(ncols=4,figsize=(20,5))\nsns.scatterplot(data=df_frequency,y=\"Total_Trans_Ct\",x=\"Total_Trans_Amt\",hue=\"Attrition_Flag\",ax=ax[0])\nsns.scatterplot(data=df_frequency,y=\"Total_Trans_Ct\",x=\"Months_Inactive_12_mon\",hue=\"Attrition_Flag\",ax=ax[1])\nsns.scatterplot(data=df_frequency,y=\"Total_Trans_Ct\",x=\"Credit_Limit\",hue=\"Attrition_Flag\",ax=ax[2])\n_scat=sns.scatterplot(data=df_frequency,y=\"Total_Trans_Ct\",x=\"Customer_Age\",hue=\"Attrition_Flag\",ax=ax[3])","b29a1ee5":"df_demographic=pd.concat([df['Customer_Age'],df['Gender'],df['Education_Level'],df['Marital_Status'],df['Income_Category'],df['Attrition_Flag']],axis=1)","cadc4d90":"fig,(ax, ax2, ax3, ax4)=plt.subplots(ncols=4,figsize=(20,5))\npd.crosstab(df['Attrition_Flag'],df['Gender']).plot(kind='bar',ax=ax, ylim=[0,5000])\npd.crosstab(df['Attrition_Flag'],df['Education_Level']).plot(kind='bar',ax=ax2, ylim=[0,5000])\npd.crosstab(df['Attrition_Flag'],df['Marital_Status']).plot(kind='bar',ax=ax3, ylim=[0,5000])\npd.crosstab(df['Attrition_Flag'],df['Income_Category']).plot(kind='bar',ax=ax4, ylim=[0,5000])\n\nfig,(ax, ax2, ax3)=plt.subplots(ncols=3,figsize=(20,5))\npd.crosstab(df['Attrition_Flag'],df['Dependent_count']).plot(kind='bar',ax=ax, ylim=[0,5000])\npd.crosstab(df['Attrition_Flag'],df['Card_Category']).plot(kind='bar',ax=ax2, ylim=[0,10000])\n_box=sns.boxplot(data = df_demographic,x='Attrition_Flag',y='Customer_Age', ax=ax3)","8e287e9c":"churn=df['Attrition_Flag'].value_counts()\nchurn\n_piechart=plt.pie(x=churn,labels=churn.keys(),autopct=\"%.1f%%\")","281bdff1":"df_categorical=df.loc[:,df.dtypes==np.object]\ndf_categorical = df_categorical[['Gender', 'Education_Level', 'Marital_Status', 'Income_Category','Card_Category','Attrition_Flag']]\ndf_categorical.head()","f1d4a448":"df_numerical=df.loc[:,df.dtypes!=np.object]\ndf_numerical['Attrition_Flag']=df.loc[:,'Attrition_Flag']\noh=pd.get_dummies(df_numerical['Attrition_Flag'])\ndf_numerical=df_numerical.drop(['Attrition_Flag'],axis=1)\ndf_numerical=df_numerical.drop(['CLIENTNUM'],axis=1)\ndf_numerical=df_numerical.join(oh)\ndf_numerical.head()","ff2820ac":"from sklearn import preprocessing\n\nlabel = preprocessing.LabelEncoder()\ndf_categorical_encoded = pd.DataFrame() \n\nfor i in df_categorical.columns :\n  df_categorical_encoded[i]=label.fit_transform(df_categorical[i])\n\nfrom scipy.stats import chi2_contingency\n\ndef cramers_V(var1,var2) :\n  crosstab =np.array(pd.crosstab(var1,var2, rownames=None, colnames=None)) # Cross table building\n  stat = chi2_contingency(crosstab)[0] # Keeping of the test statistic of the Chi2 test\n  obs = np.sum(crosstab) # Number of observations\n  mini = min(crosstab.shape)-1 # Take the minimum value between the columns and the rows of the cross table\n  return (stat\/(obs*mini))\n\nrows= []\n\nfor var1 in df_categorical_encoded:\n  col = []\n  for var2 in df_categorical_encoded :\n    cramers =cramers_V(df_categorical_encoded[var1], df_categorical_encoded[var2]) # Cramer's V test\n    col.append(round(cramers,2)) # Keeping of the rounded value of the Cramer's V  \n  rows.append(col)\n  \ncramers_results = np.array(rows)\ncramerv_matrix = pd.DataFrame(cramers_results, columns = df_categorical_encoded.columns, index =df_categorical_encoded.columns)\nmask = np.triu(np.ones_like(cramerv_matrix, dtype=np.bool))\ncat_heatmap = sns.heatmap(cramerv_matrix, mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\ncat_heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","424385d4":"##Point Biserial Correlation\nfrom scipy import stats\nnum_corr=df_numerical.corr()\nplt.figure(figsize=(16, 6))\nmask = np.triu(np.ones_like(num_corr, dtype=np.bool))\nnum_heatmap = sns.heatmap(num_corr, mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\nnum_heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","0011a926":"fig, ax=plt.subplots(ncols=2,figsize=(15, 5))\n\nheatmap = sns.heatmap(num_corr[['Existing Customer']].sort_values(by='Existing Customer', ascending=False), ax=ax[0],vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with Existing Customers', fontdict={'fontsize':18}, pad=16);\nheatmap = sns.heatmap(num_corr[['Attrited Customer']].sort_values(by='Attrited Customer', ascending=False), ax=ax[1],vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with Attrited Customers', fontdict={'fontsize':18}, pad=16);\n\nfig.tight_layout(pad=5)","3f5a1b59":"df_model=df\ndf_model=df_model.drop(['CLIENTNUM','Credit_Limit','Customer_Age','Avg_Open_To_Buy','Months_on_book','Dependent_count'],axis=1)\ndf_model.head()","a1661916":"df_model['Attrition_Flag'] = df_model['Attrition_Flag'].map({'Existing Customer': 1, 'Attrited Customer': 0})\ndf_oh=pd.get_dummies(df_model)\ndf_oh['Attrition_Flag'] = df_oh['Attrition_Flag'].map({1: 'Existing Customer', 0: 'Attrited Customer'})\nlist(df_oh.columns)","1f9536d9":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n\nX = df_oh.loc[:, df_oh.columns != 'Attrition_Flag']\ny = df_oh['Attrition_Flag']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","53629ae0":"sm = SMOTE(sampling_strategy='minority', k_neighbors=20, random_state=42)\nX_train_res, y_train_res = sm.fit_resample(X_train, y_train)","3cae2576":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train_res, y_train_res)","aefae855":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nsvm_clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\nsvm_clf.fit(X_train_res, y_train_res)","d8d9f82e":"from sklearn.ensemble import GradientBoostingClassifier\ngb_clf=GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=42)\ngb_clf.fit(X_train_res, y_train_res)","b3225d8c":"y_rf=rf_clf.predict(X_test)\ny_svm=svm_clf.predict(X_test)\ny_gb=gb_clf.predict(X_test)","bf940237":"from sklearn.metrics import plot_confusion_matrix\nfig,ax=plt.subplots(ncols=3, figsize=(20,6))\nplot_confusion_matrix(rf_clf, X_test, y_test, ax=ax[0])\nax[0].title.set_text('Random Forest')\nplot_confusion_matrix(svm_clf, X_test, y_test, ax=ax[1])\nax[1].title.set_text('Support Vector Machine')\nplot_confusion_matrix(gb_clf, X_test, y_test, ax=ax[2])\nax[2].title.set_text('Gradient Boosting')\nfig.tight_layout(pad=5)","15d8b906":"from sklearn.metrics import classification_report, recall_score, precision_score, f1_score\nprint('Random Forest Classifier')\nprint(classification_report(y_test, y_rf))\nprint('------------------------')\nprint('Support Vector Machine')\nprint(classification_report(y_test, y_svm))\nprint('------------------------')\nprint('Gradient Boosting')\nprint(classification_report(y_test, y_gb))","fcfee5cd":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","e4e7b2cb":"#rf_random = RandomizedSearchCV(estimator = rf_clf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\n#rf_random.fit(X_train_res, y_train_res)\n#print(rf_random.best_params_)","acc48416":"rf_clf_opt= RandomForestClassifier(n_estimators=750, min_samples_split=2, min_samples_leaf=1, \n                            max_features='auto', max_depth=50, bootstrap=False)\nrf_clf_opt.fit(X_train_res,y_train_res)\ny_rf_opt=rf_clf_opt.predict(X_test)\nprint('Random Forest Classifier (Optimized)')\nprint(classification_report(y_test, y_rf_opt))\n_rf_opt=plot_confusion_matrix(rf_clf_opt, X_test, y_test)","3b6f925f":"from sklearn.model_selection import GridSearchCV\n\nparam_test1 = {'n_estimators':range(20,81,10)}\n#gsearch1 = GridSearchCV(\n#estimator = GradientBoostingClassifier(learning_rate=1.0, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10), \n#param_grid = param_test1, scoring='roc_auc',n_jobs=4, cv=5)\n#gsearch1.fit(X_train_res,y_train_res)\n#print(gsearch1.best_params_)","7a75cdc8":"gb_clf_opt=GradientBoostingClassifier(n_estimators=80,learning_rate=1.0, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10)\ngb_clf_opt.fit(X_train_res,y_train_res)\ny_gb_opt=gb_clf_opt.predict(X_test)\nprint('Gradient Boosting (Optimized)')\nprint(classification_report(y_test, y_gb_opt))\nprint(recall_score(y_test,y_gb_opt,pos_label=\"Attrited Customer\"))\n_gbopt=plot_confusion_matrix(gb_clf_opt, X_test, y_test)","938e3ee1":"## 5.1 Balancing the Training Dataset\n\nBefore we train our prediction model, we will have to tinker with our imbalanced dataset first.\nIn section 4.1 we see that churned customer takes only 16.1% of the data.\n\nTo balance the dataset, we will utilize SMOTE.\nSMOTE (Synthetic Minority Oversampling Technique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n\nFirst we split our training and test data. SMOTE will be applied only to training dataset so that the prediction will use original dataset with no synthetic datapoints.","3fa0238a":"## 6.2 First Evaluation","a6d51818":"We see here that the categorical columns are NOT CORRELATED with customer churn by themselves. This supports the previous figures that show similar distribution between existing and churned customers across all demographic. Building an accurate model without considering the numerical values is impossible.\n\nNow let's check the correlation of the numerical values.","a1849974":"# 4. Feature Engineering and Selection\n## 4.1 Balance\/Imbalance\n\nFirst we check the event ratio of the dataset.","6a2b6f58":"# 7. Conclusion\n\nAfter tuning the hyperparameters and reevaluating, there were no significant change to the recall score and accuracy.\nHowever, the model can be considered good enough as it is 95% accurate and has 84% recall score.\n\nNotebook Banner Image Source: https:\/\/miro.medium.com\/max\/875\/1*k0aH2ikjVKpXNOIDFKXdTg.png","4258d1f0":"We now see better correlation measurements to the customer churn. In the heatmap above, the correlation coefficient of attrited and existing customer to all feature columns are identical in numbers, with mirroring signs (+ and -).\nLet's view them better:","f0f93e87":"## 6.3 Tuning Hyperparameters\n\nNow we will attempt to raise the model accuracy even more. We can achieve this by tuning the algorithm hyperparameters. Since SVM produced very low accuracy (<60%), we will not consider this algorithm any further.","4e9e8ea3":"We see clearly above that we for all 4 features, we can point out the following hypothesis:\n1. The higher they spend annually, the customers are more likely to remain.\n2. After 2-3 months of inactivity, the customers are more likely to leave.\n3. The higher the credit limit is, the customers are more likely to remain.\n4. Age distribution does not really matter in this case, because the clusters are largely overlapped.\n5. Almost all churned customers used their cards below 100 times.","20d7dfd1":"## 5.4 Gradient Boosting","ee4e6a8d":"## 4.4 One Hot Encoding\n\nNow we convert the categorical features into binary with one hot encoding","c5aa74e3":"# 2. Importing Libraries and Dataset","bf178254":"## 3.2 Based On Demographics","ef529cd9":"We see in both the confusion matrix and classification report that random forest and gradient boosting works best with recall score above 85%.","00c69b74":"## 5.2 Random Forest Classifier","37661323":"# 3. Exploratory Data Analysis","bf359ddd":"# 1. Introduction\n\nCustomer attrition, also known as customer churn, customer turnover, or customer defection, is the loss of clients or customers.\n\nBanks, telephone service companies, Internet service providers, pay TV companies, insurance firms, and alarm monitoring services, often use customer attrition analysis and customer attrition rates as one of their key business metrics (along with cash flow, EBITDA, etc.) because the cost of retaining an existing customer is far less than acquiring a new one.[1] Companies from these sectors often have customer service branches which attempt to win back defecting clients, because recovered long-term customers can be worth much more to a company than newly recruited clients. https:\/\/en.wikipedia.org\/wiki\/Customer_attrition","f9c2d8b5":"## 4.3 Removing Non-Correlating Columns\n\nWe discard the features mentioned above to build an accurate model.","1facbc75":"## 5.3 Support Vector Machine","a3add4a6":"## 4.2 Correlation\n\nSince we have a dataset with mixed features: categorical and numerical, we cannot simply use standard correlation function (Pearson coefficient). We will have to split the features into separate tables, and we measure numerical data correlation via Pearson, and categorical data via Cramer's V function.","ed0ccfa9":"## 3.1 Based On Frequency of Use and Numeral Characteristics","a7b64a56":"Not a great ratio to have. This shows an imbalance in the dataset which can result in predictors making false positives\/negatives because the attrition data is undersampled. We will need to counter this issue later when building our model.","799bdcce":"## 6.1 First Prediction","9eb6a9dd":"# Credit Card Customer Churn EDA & Prediction\n\n![image.png](attachment:image.png)","080d8fbd":"While the figures indeed show some disparity in numbers between churned and existing customers, the distribution of each category is very similar. Also to support the claims of previous figure, we see again that age does not play a major factor in churned and existing customers.\n\n![similar_dist.png](attachment:similar_dist.png)\n\nMeaning: \nThe figures above shows that each category cannot be used alone as a factor to decide customer churn.\nA married female with higher income and education is a different demographic than a single female with medium income and education.\nAs such, we will need to weigh in all these features to build our model.","f4ec1820":"# 5. Model Training\n\nNow we train different classifiers and try to obtain the most accurate model.\nWe will utilize:\n1. Random Forest\n2. Support Vector Machine\n3. Gradient Boosting","339873d8":"# 6. Prediction Models","9ae4d32f":"We see above that the following features are not correlated with customer churn (between -0.1 and +0.1) :\n1. Credit Limit\n2. Average Open To Buy\n3. Months On Book\n4. Age\n5. Dependent Count"}}