{"cell_type":{"7a7b55d2":"code","3a84febf":"code","7ed1e00c":"code","fe745cb8":"code","7cfc3669":"code","4f19f760":"code","6ac57e00":"code","9fcab7c4":"code","f186fb64":"code","af6e4cd1":"code","18cd64b9":"code","7e5a0c56":"code","46058735":"code","0b0e29b4":"code","ea52c811":"code","7c9a70d7":"code","8cda603b":"code","3fd4b1f1":"code","b26b954b":"code","80c86ad6":"code","cbc47cd8":"code","b1f6aa0e":"code","bd700fa9":"markdown","096bcc00":"markdown","2b8cfa8e":"markdown","3335a813":"markdown","5a352515":"markdown","ebebc44e":"markdown","512fe636":"markdown","44ef2c0b":"markdown","e1fc9c5b":"markdown","9d8b3695":"markdown","7056ba29":"markdown","1ed02760":"markdown","af2c898c":"markdown","1c6224cd":"markdown"},"source":{"7a7b55d2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport gc\nimport os\nimport sys\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom timeit import default_timer as timer\n\nimport lightgbm as lgb\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint","3a84febf":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","7ed1e00c":"def state(message,start = True, time = 0):\n    if(start):\n        print(f'Working on {message} ... ')\n    else :\n        print(f'Working on {message} took ({round(time , 3)}) Sec \\n')","fe745cb8":"# Import dataset\ndf_train = pd.read_csv('..\/input\/pubg-finish-placement-prediction\/train_V2.csv')\ndf_test = pd.read_csv('..\/input\/pubg-finish-placement-prediction\/test_V2.csv')\n\n# Reduce memory use\ndf_train=reduce_mem_usage(df_train)\ndf_test=reduce_mem_usage(df_test)\n\n# Show some data\ndf_train.head()\ndf_train.describe()","7cfc3669":"# # Reference link: https:\/\/www.kaggle.com\/melonaded\/a-beginner-guide-to-top-35-lasso-rf-lgbm\n\n# # Drop features\n# df_train = df_train.drop(['longestKill', 'rankPoints', 'numGroups'], axis=1)\n# df_test = df_test.drop(['longestKill', 'rankPoints', 'numGroups'], axis=1)\n\n# # Check row with NaN value\n# df_train[df_train['winPlacePerc'].isnull()]\n# # Drop row with NaN 'winPlacePerc' value\n# df_train.drop(2744604, inplace=True)\n\n# df_train['kills'].value_counts()\n# df_train['DBNOs'].value_counts()\n# df_train['weaponsAcquired'].value_counts()","4f19f760":"# # Reference link: https:\/\/www.kaggle.com\/deffro\/eda-is-fun\n# # Reference link: https:\/\/www.kaggle.com\/carlolepelaars\/pubg-data-exploration-rf-funny-gifs\n\n# # Get alldata for feature engineering\n# all_data = df_train.append(df_test, sort=False).reset_index(drop=True)\n\n# # Map the matchType\n# all_data['matchType'] = all_data['matchType'].map({\n#     'crashfpp':1,\n#     'crashtpp':2,\n#     'duo':3,\n#     'duo-fpp':4,\n#     'flarefpp':5,\n#     'flaretpp':6,\n#     'normal-duo':7,\n#     'normal-duo-fpp':8,\n#     'normal-solo':9,\n#     'normal-solo-fpp':10,\n#     'normal-squad':11,\n#     'normal-squad-fpp':12,\n#     'solo':13,\n#     'solo-fpp':14,\n#     'squad':15,\n#     'squad-fpp':16\n#     })\n\n# # Normalize features\n# all_data['playersJoined'] = all_data.groupby('matchId')['matchId'].transform('count')\n# all_data['killsNorm'] = all_data['kills']*((100-all_data['playersJoined'])\/100 + 1)\n# all_data['damageDealtNorm'] = all_data['damageDealt']*((100-all_data['playersJoined'])\/100 + 1)\n# all_data['maxPlaceNorm'] = all_data['maxPlace']*((100-all_data['playersJoined'])\/100 + 1)\n# all_data['matchDurationNorm'] = all_data['matchDuration']*((100-all_data['playersJoined'])\/100 + 1)\n\n# all_data['healsandboosts'] = all_data['heals'] + all_data['boosts']\n# all_data['totalDistance'] = all_data['rideDistance'] + all_data['walkDistance'] + all_data['swimDistance']\n# all_data['killsWithoutMoving'] = ((all_data['kills'] > 0) & (all_data['totalDistance'] == 0))\n\n# all_data=reduce_mem_usage(all_data)\n\n# # Split the train and the test\n# df_train = all_data[all_data['winPlacePerc'].notnull()].reset_index(drop=True)\n# df_test = all_data[all_data['winPlacePerc'].isnull()].drop(['winPlacePerc'], axis=1).reset_index(drop=True)\n\n# target = 'winPlacePerc'\n# features = list(df_train.columns)\n# features.remove(\"Id\")\n# features.remove(\"matchId\")\n# features.remove(\"groupId\")\n# features.remove(\"matchType\")\n\n# y_train = np.array(df_train[target])\n# features.remove(target)\n# x_train = df_train[features]\n\n# x_test = df_test[features]","6ac57e00":"def feature_engineering(df,is_train=True):\n    if is_train: \n        df = df[df['maxPlace'] > 1]\n\n    state('totalDistance')\n    s = timer()\n    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n    e = timer()\n    state('totalDistance', False, e - s)\n          \n\n    state('rankPoints')\n    s = timer()\n    df['rankPoints'] = np.where(df['rankPoints'] <= 0 ,0 , df['rankPoints'])\n    e = timer()                                  \n    state('rankPoints', False, e-s)\n    \n\n    target = 'winPlacePerc'\n    features = list(df.columns)\n    \n    # Remove some features from the features list :\n    features.remove(\"Id\")\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n    features.remove(\"matchDuration\")\n    features.remove(\"matchType\")\n    \n    y = None\n    if is_train: \n        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n        # Remove the target from the features list :\n        features.remove(target)\n    \n    # Make new features indicating the mean of the features ( grouped by match and group ) :\n    print(\"get group mean feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    \n    # If we are processing the training data let df_out = the grouped  'matchId' and 'groupId'\n    if is_train: \n        df_out = agg.reset_index()[['matchId','groupId']]\n    else: \n        df_out = df[['matchId','groupId']]\n    \n    # Merge agg and agg_rank (that we got before) with df_out :\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # Make new features indicating the max value of the features for each group ( grouped by match )\n    print(\"get group max feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    # Merge the new (agg and agg_rank) with df_out :\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # Make new features indicating the minimum value of the features for each group ( grouped by match )\n    print(\"get group min feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    # Merge the new (agg and agg_rank) with df_out :\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # Make new features indicating the number of players in each group ( grouped by match )\n    print(\"get group size feature\")\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n     \n    # Merge the group_size feature with df_out :\n    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n    \n    # Make new features indicating the mean value of each features for each match :\n    print(\"get match mean feature\")\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    \n    # Merge the new agg with df_out :\n    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n    \n    # Make new features indicating the number of groups in each match :\n    print(\"get match size feature\")\n    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n    \n    # Merge the match_size feature with df_out :\n    df_out = df_out.merge(agg, how='left', on=['matchId'])\n    \n    # Drop matchId and groupId\n    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n    df_out = reduce_mem_usage(df_out)\n    \n    # X is the output dataset (without the target) and y is the target :\n    X = np.array(df_out, dtype=np.float64)\n    \n    \n    del df, df_out, agg, agg_rank\n    gc.collect()\n\n    return X, y\n","9fcab7c4":"x_train, y_train = feature_engineering(df_train,True)\nx_test,_ = feature_engineering(df_test,False)","f186fb64":"# Split the train and the validation set for the fitting\nrandom_seed=1\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.05, random_state=random_seed)","af6e4cd1":"# Random Forest\nRF = RandomForestRegressor(n_estimators=10, min_samples_leaf=3, max_features=0.5, n_jobs=-1)","18cd64b9":"%%time\nRF.fit(x_train, y_train)","7e5a0c56":"mae_train_RF = mean_absolute_error(RF.predict(x_train), y_train)\nmae_val_RF = mean_absolute_error(RF.predict(x_val), y_val)\nprint('mae train RF: ', mae_train_RF)\nprint('mae val RF: ', mae_val_RF)","46058735":"# Reference link: https:\/\/www.kaggle.com\/chocozzz\/lightgbm-baseline\ndef run_lgb(train_X, train_y, val_X, val_y, x_test):\n    params = {\"objective\" : \"regression\", \n              \"metric\" : \"mae\", \n              'n_estimators':20000, \n              'early_stopping_rounds':200,\n              \"num_leaves\" : 31, \n              \"learning_rate\" : 0.05, \n              \"bagging_fraction\" : 0.7,\n              \"bagging_seed\" : 0, \n              \"num_threads\" : 4,\n              \"colsample_bytree\" : 0.7\n             }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, valid_sets=[lgtrain, lgval], early_stopping_rounds=200, verbose_eval=1000)\n    \n    pred_test_y = model.predict(x_test, num_iteration=model.best_iteration)\n    return pred_test_y, model","0b0e29b4":"%%time\n# Training the model #\npred_test_lgb, model = run_lgb(x_train, y_train, x_val, y_val, x_test)","ea52c811":"mae_train_lgb = mean_absolute_error(model.predict(x_train, num_iteration=model.best_iteration), y_train)\nmae_val_lgb = mean_absolute_error(model.predict(x_val, num_iteration=model.best_iteration), y_val)\n\nprint('mae train lgb: ', mae_train_lgb)\nprint('mae val lgb: ', mae_val_lgb)","7c9a70d7":"# Reference link: https:\/\/www.kaggle.com\/qingyuanwu\/deep-neural-network\ndef run_DNN(x_train, y_train, x_val, y_val, x_test):\n    NN_model = Sequential()\n    NN_model.add(Dense(x_train.shape[1],  input_dim = x_train.shape[1], activation='relu'))\n    NN_model.add(Dense(136, activation='relu'))\n    NN_model.add(Dense(136, activation='relu'))\n    NN_model.add(Dense(136, activation='relu'))\n    NN_model.add(Dense(136, activation='relu'))\n\n    # output Layer\n    NN_model.add(Dense(1, activation='linear'))\n\n    # Compile the network :\n    NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n    NN_model.summary()\n    \n    checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n    checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n    callbacks_list = [checkpoint]\n    \n    NN_model.fit(x=x_train, \n                 y=y_train, \n                 batch_size=1000,\n                 epochs=30, \n                 verbose=1, \n                 callbacks=callbacks_list,\n                 validation_split=0.15, \n                 validation_data=None, \n                 shuffle=True,\n                 class_weight=None, \n                 sample_weight=None, \n                 initial_epoch=0,\n                 steps_per_epoch=None, \n                 validation_steps=None)\n\n    pred_test_y = NN_model.predict(x_test)\n    pred_test_y = pred_test_y.reshape(-1)\n    return pred_test_y, NN_model","8cda603b":"%%time\n# Training the model #\npred_test_DNN, model = run_DNN(x_train, y_train, x_val, y_val, x_test)","3fd4b1f1":"mae_train_DNN = mean_absolute_error(model.predict(x_train), y_train)\nmae_val_DNN = mean_absolute_error(model.predict(x_val), y_val)\nprint('mae train dnn: ', mae_train_DNN)\nprint('mae val dnn: ', mae_val_DNN)","b26b954b":"pred_test_RF = RF.predict(x_test)\ndf_test['winPlacePerc_RF'] = pred_test_RF\nsubmission = df_test[['Id', 'winPlacePerc_RF']]\nsubmission.to_csv('submission_RF.csv', index=False)","80c86ad6":"df_test['winPlacePerc_lgb'] = pred_test_lgb\nsubmission = df_test[['Id', 'winPlacePerc_lgb']]\nsubmission.to_csv('submission_lgb.csv', index=False)","cbc47cd8":"df_test['winPlacePerc_DNN'] = pred_test_DNN\nsubmission = df_test[['Id', 'winPlacePerc_DNN']]\nsubmission.to_csv('submission_DNN.csv', index=False)","b1f6aa0e":"weight_DNN = (1 - mae_val_DNN) \/ (3 - mae_val_DNN - mae_val_RF - mae_val_lgb)\nweight_RF = (1 - mae_val_RF) \/ (3 - mae_val_DNN - mae_val_RF - mae_val_lgb)\nweight_lgb = (1 - mae_val_lgb) \/ (3 - mae_val_DNN - mae_val_RF - mae_val_lgb)\n\ndf_test['winPlacePerc'] = df_test.apply(lambda x: x['winPlacePerc_RF'] * weight_RF + x['winPlacePerc_DNN'] * weight_DNN + x['winPlacePerc_lgb'] * weight_lgb, axis=1)\nsubmission = df_test[['Id', 'winPlacePerc']]\nsubmission.to_csv('submission.csv', index=False)","bd700fa9":"## 4. Model ensembling(RF + DNN)","096bcc00":"### 2. feature_v2","2b8cfa8e":"## 2. LightGBM","3335a813":"## 2. LightGBM","5a352515":"# I. Load and show data","ebebc44e":"# V. Use the model for prediction","512fe636":"# III. Feature engineering","44ef2c0b":"### 1. feature_v1","e1fc9c5b":"# IV. Create model for train","9d8b3695":"## 1. Random Forest","7056ba29":"# II. Clean the data","1ed02760":"## 1. Random Forest","af2c898c":"## 3. DNN","1c6224cd":"## 3. DNN"}}