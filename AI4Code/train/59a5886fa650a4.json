{"cell_type":{"dca7583d":"code","195d5e60":"code","2475df68":"code","df14bdce":"code","b401d646":"code","3bf98e8f":"code","a01cd35d":"code","94e30dd6":"code","e9aefa37":"code","4ac15520":"code","71989120":"code","03fb7057":"code","66d88ab4":"code","adb12c8d":"code","dd7c9bae":"code","690f911d":"code","ee59bc5c":"code","9a1e92eb":"code","ad6b34db":"code","1352b200":"code","a4640098":"code","3f94e12e":"code","f0b6d7fa":"code","8ede0f34":"code","d7719a4f":"code","8f8c185e":"code","5b097b11":"code","e1bcb064":"code","798b5ca1":"code","9f65a190":"code","c00538da":"code","eb8d713c":"code","38e62b41":"code","79aa6843":"code","bb73b3f8":"code","69d6fb8d":"code","4902985b":"code","2decdd1b":"code","5a0c748d":"code","7318d2d0":"markdown","b56461b0":"markdown","84a98c19":"markdown","3d1ab850":"markdown","7ef5ef35":"markdown"},"source":{"dca7583d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","195d5e60":"data = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndata","2475df68":"data.info()","df14bdce":"data.isnull().sum()","b401d646":"data.bmi.replace(to_replace=np.nan, value=data.bmi.mean(), inplace=True)","3bf98e8f":"data.describe()","a01cd35d":"labels =data['stroke'].value_counts(sort = True).index\nsizes = data['stroke'].value_counts(sort = True)\n\ncolors = [\"lightblue\",\"red\"]\nexplode = (0.05,0) \n \nplt.figure(figsize=(7,7))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90,)\n\nplt.title('Customer Churn Breakdown')\nplt.show()","94e30dd6":"plt.figure(figsize=(10,5))\nsns.countplot(data=data,x='gender');","e9aefa37":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\ndata.plot(kind='scatter', x='age', y='avg_glucose_level', alpha=0.5, color='green', ax=axes[0], title=\"Age vs. avg_glucose_level\")\ndata.plot(kind='scatter', x='bmi', y='avg_glucose_level', alpha=0.5, color='red', ax=axes[1], title=\"bmi vs. avg_glucose_level\")\nplt.show()","4ac15520":"sns.set(style=\"ticks\");\npal = [\"#FA5858\", \"#58D3F7\"]\n\nsns.pairplot(data, hue=\"stroke\", palette=pal);\nplt.title(\"stroke\");","71989120":"plt.figure(figsize=(15,7))\nsns.heatmap(data.corr(),annot=True);\n","03fb7057":"plt.figure(figsize=(10,5))\nstrok=data.loc[data['stroke']==1]\nsns.countplot(data=strok,x='ever_married',palette='inferno');\n","66d88ab4":"plt.figure(figsize=(10,5))\nsns.countplot(data=strok,x='work_type',palette='cool');\n","adb12c8d":"plt.figure(figsize=(10,5))\nsns.countplot(data=strok,x='smoking_status',palette='autumn');\n","dd7c9bae":"plt.figure(figsize=(10,5))\nsns.countplot(data=strok,x='Residence_type',palette='Greens');","690f911d":"plt.figure(figsize=(10,5))\nsns.countplot(data=strok,x='heart_disease',palette='Reds');","ee59bc5c":"plt.figure(figsize=(10,5))\nsns.countplot(data=strok,x='hypertension',palette='Pastel2');","9a1e92eb":"cat_features = ['work_type', 'gender', 'Residence_type', 'smoking_status', 'ever_married'] # categorical features\nnum_features = ['age', 'avg_glucose_level', 'bmi']                                         # numerical features","ad6b34db":"# Assigning categorical variables to a numerical value\ncat_maps = {'work_type': {'Private':0, 'Self-employed': 1, 'Govt_job':2, 'children':3, 'Never_worked':4},\n            'gender': {'Male':0, 'Female':1},\n            'Residence_type': {'Urban':0, 'Rural':1},\n            'smoking_status': {'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3},\n            'ever_married': {'Yes':0, 'No':1}\n}","1352b200":"data['work_type'] = data['work_type'].map(cat_maps['work_type'])\ndata['gender'] = data['gender'].map(cat_maps['gender'])\ndata['Residence_type'] = data['Residence_type'].map(cat_maps['Residence_type'])\ndata['smoking_status'] = data['smoking_status'].map(cat_maps['smoking_status'])\ndata['ever_married'] = data['ever_married'].map(cat_maps['ever_married'])","a4640098":"data","3f94e12e":"# Split into input data and target variable\n\nfeatures = ['age',\n 'hypertension',\n 'heart_disease',\n 'ever_married',\n 'Residence_type',\n 'avg_glucose_level',\n 'bmi',\n 'gender',\n 'work_type',\n 'smoking_status']\n\nlabel = ['stroke']\n\nX = data[features]\ny = data[label]","f0b6d7fa":"X.gender=(X.gender.fillna(1))\nX.isnull().sum()","8ede0f34":"# Split into training and validation sets. Stratified split of 80-20 ratio\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test, y_train,y_test=train_test_split(X, y,test_size=0.2,random_state=42, stratify = y)","d7719a4f":"\"\"\"\nSynthetic Minority Oversampling Technique (SMOTE) for handling class imbalance.\nOnly numerical features should be inputted with categorical features untouched\n\n\"\"\"\n\nfrom imblearn.over_sampling import SMOTENC\n\nsmote = SMOTENC([1,2,3,4,7,8,9]) # we pass the index of the input numerical features\nX_train , y_train = smote.fit_resample(X_train, y_train)","8f8c185e":"X_train = pd.DataFrame(data = X_train, columns = features)\nX_test = pd.DataFrame(data = X_test, columns = features)","5b097b11":"!pip install category_encoders","e1bcb064":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, RandomizedSearchCV\nfrom matplotlib import pyplot\nimport category_encoders as ce","798b5ca1":"folds = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001) # Stratified K-Fold Cross Validation","9f65a190":"\"\"\"\nNumerical features are scaled using Standard Scaling. \nCategorical features are encoded in different ways for different models:\n    > For tree-based models, catboost encoding is used\n    > For linear models, one hot encoding is used\n    > For KNN, categorical features are dropped\n\"\"\"\n\n\ntree_mapper = ColumnTransformer(transformers=[('num', StandardScaler(), num_features),\n                                         ('ce', ce.CatBoostEncoder(), cat_features)\n                                         ], remainder= 'passthrough')\n\nlinear_mapper = ColumnTransformer(transformers=[('num', StandardScaler(), num_features),\n                                         ('ce', ce.OneHotEncoder(), cat_features)\n                                         ], remainder= 'passthrough')\n\nnum_mapper = ColumnTransformer(transformers=[('num', StandardScaler(), num_features)\n                                         ], remainder= 'drop')","c00538da":"models = [\n          Pipeline([('mapper', linear_mapper), ('classifier', LogisticRegression())]),\n          Pipeline([('mapper', num_mapper), ('classifier', KNeighborsClassifier())]),\n          Pipeline([('classifier', GaussianNB())]),\n          Pipeline([('classifier', BernoulliNB())]),\n          Pipeline([('mapper', tree_mapper), ('classifier', DecisionTreeClassifier())]),\n          Pipeline([('mapper', tree_mapper), ('classifier', RandomForestClassifier())]),\n          Pipeline([('mapper', tree_mapper), ('classifier', XGBClassifier())]),\n          Pipeline([('mapper', tree_mapper), ('classifier', GradientBoostingClassifier())]),\n          Pipeline([('mapper', linear_mapper), ('classifier', SVC(probability= True))]),\n          Pipeline([('mapper', tree_mapper), ('classifier', AdaBoostClassifier(\n              base_estimator=DecisionTreeClassifier(criterion='entropy',\n                                                      max_depth = None,\n                                                      max_features = None,\n                                                      min_samples_leaf = 1,\n                                                      min_samples_split = 2,\n                                                      random_state = 0)))])\n]\n\nmodel_grids = [\n               [{'classifier__C':[1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1, 5, 1e1, 5e1, 1e2, 5e2, 1e3],\n                 'classifier__random_state':[0]}],                                 #logistic regression\n               \n               [{'classifier__n_neighbors':[5,7,9,11, 13, 15, 17, 19], \n                 'classifier__metric': ['euclidean', 'manhattan', 'minkowski']}],  #KNN\n               \n               [{'classifier__var_smoothing': [1e-10, 1e-09, 1e-8, 1e-7]}],        #GaussianNB\n\n               [{'classifier__alpha': [1e-2, 1e-1, 1, 1e1, 1e2]}],                 #BernoulliNB\n\n               [{'classifier__criterion':['gini','entropy'],\n                 'classifier__random_state':[0], \n                 'classifier__max_depth' : [3, 5, 8, 10, 15, None], \n                 'classifier__min_samples_split' : [1,2,5,10,15,30],\n                 'classifier__min_samples_leaf': [1,2,5,10], \n                 'classifier__max_features': ['log2', 'sqrt', None]}],             #Decision Tree\n               \n               [{'classifier__criterion':['gini','entropy'],\n                 'classifier__n_estimators': [1000],\n                 'classifier__random_state':[0], \n                 'classifier__max_depth' : [3, 5, 8, 10, 15, None], \n                 'classifier__min_samples_split' : [1,2,5,10,15,30],\n                 'classifier__min_samples_leaf': [1,2,5,10], \n                 'classifier__max_features': ['log2', 'sqrt', None]}],             #Random Forest\n\n               [{'classifier__n_estimators':[1000],\n                 'classifier__criterion':['gini','entropy'],\n                 'classifier__random_state':[0],\n                 'classifier__max_depth': [3, 5, 8, 10, 15, 30],\n                 'classifier__min_child_weight': [2,4,6,8,10],\n                 'classifier__gamma': [0, 0.1, 0.2, 0.3],\n                 'classifier__reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n                 'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n                 'classifier__eta': [0.1, 0.2, 0.3, 0.4, 0.5]}],                   #XGBoost\n               \n               [{'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2], \n                 'classifier__n_estimators': [1000],\n                 'classifier__random_state':[0],\n                 'classifier__max_depth' : [5, 8, 15, None], \n                 'classifier__min_samples_split' : [1,2,5,10],\n                 'classifier__min_samples_leaf': [1,2,5,10], \n                 'classifier__max_features': ['log2', 'sqrt', 'auto', 'None']}],   #Gradient Bossting Decision Tree\n                        \n\n               [{'classifier__C':[1e-1, 1, 1e1] ,\n                 'classifier__random_state':[0],\n                 'classifier__kernel': ['rbf', 'poly']\n                }],                                                                #SVM\n               \n               [{'classifier__n_estimators' : [800, 1000, 1200], \n                 'classifier__learning_rate' : [1e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1, 1e1],\n                 'classifier__random_state':[0]}]                                  #AdaBoost\n]            \n          ","eb8d713c":"# Hyperparameter Tuning. Random Search of 100 iterations is used. Uncomment cell to run\n\"\"\"\nfor i,j in zip(models, model_grids):\n    grid = RandomizedSearchCV(estimator=i, param_distributions=j, n_iter = 100, scoring='f1_weighted', cv = skf)\n    grid.fit(X_train, y_train)\n    best_f1 = grid.best_score_\n    best_param = grid.best_params_\n    print('{}:\\nBest F1 : {:.4f}'.format(i.steps[-1],best_f1))\n    print('Best Parameters : ',best_param)\n    print('')\n    print('----------------')\n    print('')\n\"\"\"","38e62b41":"models_tuned = []\nmodels_tuned.append(Pipeline([('mapper', linear_mapper),  ('classifier', LogisticRegression(random_state = 0))]))\nmodels_tuned.append(Pipeline([('mapper', num_mapper),  ('classifier', KNeighborsClassifier(n_neighbors=5, \n                                                                                           metric = 'manhattan'))]))\nmodels_tuned.append(Pipeline([('classifier', GaussianNB(var_smoothing= 1e-7))]))\nmodels_tuned.append(Pipeline([('classifier', BernoulliNB(alpha= 100))]))\nmodels_tuned.append(Pipeline([('mapper', linear_mapper),  ('classifier', SVC(C=1, random_state = 0, probability= True))]))\n\nmodels_tuned.append(Pipeline([('mapper', tree_mapper),  \n                              ('classifier', \n                               DecisionTreeClassifier(criterion='entropy',\n                                                      max_depth = None,\n                                                      max_features = None,\n                                                      min_samples_leaf = 1,\n                                                      min_samples_split = 2,\n                                                      random_state = 0)\n                               )]))\nmodels_tuned.append(Pipeline([('mapper', tree_mapper),  \n                              ('classifier', \n                               RandomForestClassifier(n_estimators = 1000,\n                                                      criterion='entropy',\n                                                      max_depth = None,\n                                                      max_features = 'sqrt',\n                                                      min_samples_leaf = 1,\n                                                      min_samples_split = 10,\n                                                      random_state = 0)\n                               )]))\n\nmodels_tuned.append(Pipeline([('mapper', tree_mapper),  \n                              ('classifier', \n                               AdaBoostClassifier(base_estimator= DecisionTreeClassifier(criterion='gini',\n                                                      max_depth = 30,\n                                                      max_features = 'log2',\n                                                      min_samples_leaf = 5,\n                                                      min_samples_split = 15,\n                                                      random_state = 0),\n                                                  learning_rate = 0.1,\n                                                  n_estimators = 500)\n                               )]))\n\nmodels_tuned.append(Pipeline([('mapper', tree_mapper),  \n                              ('classifier', XGBClassifier(criterion = 'gini',\n                                                           eta = 0.1,\n                                                           max_depth = 8,\n                                                           n_estimators = 500,\n                                                           random_state = 0\n                                                           ))]))\n\nmodels_tuned.append(Pipeline([('mapper', tree_mapper),  \n                              ('classifier', GradientBoostingClassifier(n_estimators = 1000,\n                                                                        learning_rate= 0.2,\n                                                                        max_depth = 8,\n                                                                        min_samples_split = 10,\n                                                                        min_samples_leaf = 1,\n                                                                        max_features = 'auto',\n                                                                        random_state = 0)\n                              )]))\n","79aa6843":"lst_1_tuned= []\n\nfor m in range(len(models_tuned)):\n    lst_2_tuned= []\n    model = models_tuned[m]\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = skf)   #K-Fold Validation\n    \n    test_acc = accuracy_score(y_test, y_pred)\n\n    cr = classification_report(y_test, y_pred)\n      \n    cm = confusion_matrix(y_test, y_pred)\n\n    precision = precision_score(y_test, y_pred, average= 'weighted')\n    recall = recall_score(y_test, y_pred, average= 'weighted')\n    f1 = f1_score(y_test, y_pred, average= 'weighted')\n    roc = roc_auc_score(y_test, y_pred)\n\n    predicted_probab = model.predict_proba(X_test)\n    predicted_probab = predicted_probab[:, 1]\n\n    fpr, tpr, _ = roc_curve(y_test, predicted_probab)\n    pyplot.plot(fpr, tpr, marker='.', label = type(models_tuned[m][-1]).__name__)\n    pyplot.xlabel('False Positive Rate')\n    pyplot.ylabel('True Positive Rate')\n    pyplot.legend()\n    pyplot.show()\n   \n    print(type(models_tuned[m][-1]).__name__ , ':')\n    \n    print('Accuracy Score: {:.4f}'.format(test_acc))\n    print('')\n    \n    print(\"K-Fold Validation Mean Accuracy: {:.4f} %\".format(accuracies.mean()*100))\n    print('')\n\n    print(\"Classification report: \")\n    print(cr)\n    print('')\n\n    print(\"Confusion matrix: \")\n    print(cm)\n    print('')\n\n    print('Precision Score: {:.4f}'.format(precision))\n    print('')\n\n    print('Recall Score: {:.4f}'.format(recall))\n    print('')\n\n    print('F1 score: {:.4f}'.format(f1))\n    \n    print('-----------------------------------')\n    print('')\n    lst_2_tuned.append(type(models_tuned[m][-1]).__name__)\n    lst_2_tuned.append(accuracies.mean())\n    lst_2_tuned.append(test_acc)\n    lst_2_tuned.append(precision)\n    lst_2_tuned.append(recall)\n    lst_2_tuned.append(f1)\n    lst_1_tuned.append(lst_2_tuned)","bb73b3f8":"df_tuned = pd.DataFrame(lst_1_tuned, columns= ['Model','Cross-val acc','Test Accuracy','Precision','Recall', 'F1'])\ndf_tuned.sort_values(by= ['F1'], inplace= True, ascending= False)\ndf_tuned","69d6fb8d":"from sklearn.ensemble import StackingClassifier, VotingClassifier, BaggingClassifier\n\ndef get_stacking_model():\n    # define the base models\n    level0 = []\n    level0.append(['Logistic Regression', Pipeline([('mapper', linear_mapper),  \n                                                    ('classifier', LogisticRegression(random_state = 0))])])\n    \n    level0.append(['KNN', Pipeline([('mapper', num_mapper),  \n                                    ('classifier', KNeighborsClassifier(n_neighbors=5, metric = 'manhattan'))])])\n    \n    # level0.append(['Gaussian NB', Pipeline([('mapper', linear_mapper), \n    #                                         ('classifier', GaussianNB(var_smoothing= 1e-7))])])\n    \n    # level0.append(['Bernoulli NB', Pipeline([('mapper', linear_mapper), \n    #                                          ('classifier', BernoulliNB(alpha=100))])])\n    \n    level0.append(['SVM', Pipeline([('mapper', linear_mapper),  \n                                    ('classifier', SVC(C=1, random_state = 0, probability= True))])])\n    \n    # level0.append(['DT', Pipeline([('mapper', tree_mapper),  \n    #                                ('classifier', \n    #                                 DecisionTreeClassifier(criterion='entropy',\n    #                                                   max_depth = None,\n    #                                                   max_features = None,\n    #                                                   min_samples_leaf = 1,\n    #                                                   min_samples_split = 2,\n    #                                                   random_state = 0)\n    #                            )])])\n    level0.append(['Random Forest', Pipeline([('mapper', tree_mapper),  \n                              ('classifier', RandomForestClassifier(n_estimators = 1000,\n                                                      criterion='entropy',\n                                                      max_depth = None,\n                                                      max_features = 'sqrt',\n                                                      min_samples_leaf = 1,\n                                                      min_samples_split = 10,\n                                                      random_state = 0)\n                               )])\n    ])\n    level0.append(['AdaBoost', Pipeline([('mapper', tree_mapper),  \n                              ('classifier', \n                               AdaBoostClassifier(base_estimator= DecisionTreeClassifier(criterion='gini',\n                                                      max_depth = 30,\n                                                      max_features = 'log2',\n                                                      min_samples_leaf = 5,\n                                                      min_samples_split = 15,\n                                                      random_state = 0),\n                                                  learning_rate = 0.1,\n                                                  n_estimators = 500)\n                               )])\n    ])\n    level0.append(['XGBoost', Pipeline([('mapper', tree_mapper),  \n                              ('classifier', XGBClassifier(criterion = 'gini',\n                                                           eta = 0.1,\n                                                           max_depth = 8,\n                                                           n_estimators = 500,\n                                                           random_state = 0\n                                                           )\n                              )])])\n    level0.append(['GBT', Pipeline([('mapper', tree_mapper),  \n                              ('classifier', GradientBoostingClassifier(n_estimators = 1000,\n                                                                        learning_rate= 0.2,\n                                                                        max_depth = 8,\n                                                                        min_samples_split = 10,\n                                                                        min_samples_leaf = 1,\n                                                                        max_features = 'auto',\n                                                                        random_state = 0)\n                              )])\n    ])\n    \n\n    # define meta learner model\n    # level1 = KNeighborsClassifier(n_neighbors= 7)\n    level1 = RandomForestClassifier(criterion='entropy', n_estimators= 1000, random_state= 0)\n    # define the stacking ensemble\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=skf)\n    return model","4902985b":"model = get_stacking_model()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = skf)   #K-Fold Validation\n\ntest_acc = accuracy_score(y_test, y_pred)\n\ncr = classification_report(y_test, y_pred)\n  \ncm = confusion_matrix(y_test, y_pred)\n\nprecision = precision_score(y_test, y_pred, average= 'weighted')\nrecall = recall_score(y_test, y_pred, average= 'weighted')\nf1 = f1_score(y_test, y_pred, average= 'weighted')\n\nprint('Stacking Ensemble:')\n\nprint('Accuracy Score: {:.4f}'.format(test_acc))\nprint('')\n\nprint(\"Classification report: \")\nprint(cr)\nprint('')\n\nprint(\"Confusion matrix: \")\nprint(cm)\nprint('')\n\nprint('Precision Score: {:.4f}'.format(precision))\nprint('')\n\nprint('Recall Score: {:.4f}'.format(recall))\nprint('')\n\nprint('F1 score: {:.4f}'.format(f1))\n\nprint('-----------------------------------')\nprint('')","2decdd1b":"list_stack=[]\nlist_stack.append(\"Stacking ensemble\")\nlist_stack.append(np.nan) # Only testing ensemble on test set\nlist_stack.append(test_acc)\nlist_stack.append(precision)\nlist_stack.append(recall)\nlist_stack.append(f1)\nfinal_list = lst_1_tuned\nfinal_list.append(list_stack)","5a0c748d":"df_tuned = pd.DataFrame(lst_1_tuned, columns= ['Model','Cross-val acc','Test Accuracy','Precision','Recall', 'F1'])\ndf_tuned.sort_values(by= ['F1'], inplace= True, ascending= False)\ndf_tuned","7318d2d0":"# Ensembling","b56461b0":"# Testing tuned models","84a98c19":"# Preprocessing","3d1ab850":"# Visualization","7ef5ef35":"# Base Models + Hyperparameter Tuning"}}