{"cell_type":{"ed112ce1":"code","f5a7cc4d":"code","268dfee4":"code","276dc4d9":"code","d20376ea":"code","b835359f":"code","98cdf5e6":"code","16ef9acc":"code","e938f723":"code","d7f6c855":"code","fd6b68d6":"code","f031e140":"code","26758939":"code","b1ba2d38":"code","e0f2a524":"code","bdd9571c":"code","3cc55953":"code","d44d763b":"code","8043667a":"code","d33e1227":"code","0b4e2c62":"code","d6229d3a":"code","2ee0a055":"code","c8e8bf0d":"code","dbb36efa":"code","37c5d5b0":"code","067ca19d":"code","ad57fd7e":"code","f5b7c535":"code","d8c24d6b":"code","44989b7a":"code","a9f8bbcc":"code","323d8856":"code","f01bac18":"code","3cdc7ead":"code","a2de638a":"code","49b924af":"code","4ff846bc":"markdown","7d1cff5d":"markdown","7e4474e9":"markdown","fe824c64":"markdown","53eb5cae":"markdown","15a79272":"markdown","38a4d167":"markdown","3726905e":"markdown","5d1b6250":"markdown","a3f1ea60":"markdown","94b6be48":"markdown","61559b72":"markdown","ed20aba6":"markdown","f16b5d29":"markdown","f01eb097":"markdown"},"source":{"ed112ce1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nimport math\nimport argparse\nimport pickle\nimport tokenizers\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import layers as L\nfrom transformers import TFBertMainLayer,TFBertModel,TFBertPreTrainedModel,BertConfig,BertTokenizer\nfrom transformers import TFRobertaModel, RobertaTokenizer,RobertaConfig\nfrom transformers.modeling_tf_utils import get_initializer\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","f5a7cc4d":"parser = argparse.ArgumentParser()\n\nparser.add_argument('--seed', type=int, default=88888)\n\n#First stage model arguments\nparser.add_argument(\"--model_type\", default=\"roberta\", type=str)\nparser.add_argument(\"--model_path\", default=\"..\/input\/tweet-senti-pretrained-roberta-cnn\/\", type=str)\nparser.add_argument(\"--max_seq_length\", default=96, type=int)\nparser.add_argument(\"--train_batch_size\", default=32, type=int)\nparser.add_argument(\"--eval_batch_size\", default=32, type=int)\nparser.add_argument(\"--num_train_epochs\", default=3, type=int)\nparser.add_argument(\"--cv_splits\", default=5, type=int)\nparser.add_argument(\"--warmup\", default=0.1, type=float)\nparser.add_argument(\"--learning_rate\", default=3e-5, type=float)\nparser.add_argument(\"--epoches_1\", default=3, type=float)\n\n#Second stage model arguments\nparser.add_argument(\"--max_char_length\", default=150, type=int)\nparser.add_argument(\"--cv_splits_2\", default=5, type=int)\nparser.add_argument(\"--learning_rate_2\", default=4e-3, type=float)\nparser.add_argument(\"--epoches_2\", default=5, type=float)\n\nargs, _ = parser.parse_known_args()","268dfee4":"def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    tf.random.set_seed(args.seed)\n\nset_seed(args)","276dc4d9":"test_df=pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\")\ntrain_df=pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")\ntrain_df.dropna(inplace=True)\ntrain_df=train_df.reset_index()","d20376ea":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nf, axes = plt.subplots(1, 3,figsize=(15,6))\nsns.countplot(x=train_df[train_df['sentiment']=='neutral']['text'].astype(str).str.split().str.len(),ax=axes[0])\nsns.countplot(x=train_df[train_df['sentiment']=='positive']['text'].astype(str).str.split().str.len(),ax=axes[1])\nsns.countplot(x=train_df[train_df['sentiment']=='negative']['text'].astype(str).str.split().str.len(),ax=axes[2])\naxes[0].title.set_text('lenght of neutral')\naxes[1].title.set_text('length of positive')\naxes[2].title.set_text('length of negative')\naxes[0].set_xticklabels(axes[0].get_xticklabels(), fontsize=8)\naxes[1].set_xticklabels(axes[0].get_xticklabels(), fontsize=8)\naxes[2].set_xticklabels(axes[0].get_xticklabels(), fontsize=8)\nplt.tight_layout()","b835359f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nf, axes = plt.subplots(1, 3,figsize=(15,6))\nsns.countplot(x=train_df[train_df['sentiment']=='neutral']['selected_text'].astype(str).str.split().str.len(),ax=axes[0])\nsns.countplot(x=train_df[train_df['sentiment']=='positive']['selected_text'].astype(str).str.split().str.len(),ax=axes[1])\nsns.countplot(x=train_df[train_df['sentiment']=='negative']['selected_text'].astype(str).str.split().str.len(),ax=axes[2])\naxes[0].title.set_text('lenght of neutral sentiment')\naxes[1].title.set_text('lenght of positive sentiment')\naxes[0].title.set_text('lenght of neutral sentiment')\n\naxes[2].title.set_text('lenght of neutral')\naxes[0].set_xticklabels(axes[0].get_xticklabels(), fontsize=8)\naxes[1].set_xticklabels(axes[0].get_xticklabels(), fontsize=8)\naxes[2].set_xticklabels(axes[0].get_xticklabels(), fontsize=8)\nplt.tight_layout()","98cdf5e6":"#Load the data for first stage model\ndef load_data_roberta(df,tokenizer, is_train_eval):\n    sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n    ct = df.shape[0]\n    MAX_LEN_WORD=args.max_seq_length\n    input_ids = np.ones((ct,MAX_LEN_WORD),dtype='int32')\n    attention_mask = np.zeros((ct,MAX_LEN_WORD),dtype='int32')\n    token_type_ids = np.zeros((ct,MAX_LEN_WORD),dtype='int32') #need it or not?\n    start_tokens = np.zeros((ct,MAX_LEN_WORD),dtype='int32')\n    end_tokens = np.zeros((ct,MAX_LEN_WORD),dtype='int32')\n    offsets=[]\n    tweets=[]\n\n    for k in range(ct):\n        text1 = \" \"+\" \".join(df.loc[k,'text'].split()) \n        enc = tokenizer.encode(text1)\n        sentiment_token = sentiment_id[df.loc[k,'sentiment']]\n        input_ids[k,:len(enc.ids)+3] = [0, sentiment_token] + enc.ids + [2]\n        attention_mask[k,:len(enc.ids)+3] = 1\n        offsets.append([(0,0)]* 2 + enc.offsets + [(0,0)])\n        tweets.append(text1)\n                \n        if is_train_eval:            \n            chars = np.zeros((len(text1)))\n            text2 = \" \".join(df.loc[k,'selected_text'].split())\n            # start index of selceted text in the raw text at char level\n            start_idx = text1.find(text2) \n            chars[start_idx:start_idx+len(text2)]=1\n            #add the white space before the beginning of selected text\n            if text1[start_idx-1]==' ': \n                chars[start_idx-1] = 1         \n\n            selected_toks_index = []\n            for i,(a,b) in enumerate(enc.offsets):\n                sm = np.sum(chars[a:b])\n                if sm>0: selected_toks_index.append(i) \n            if len(selected_toks_index)>0:\n                start_tokens[k,selected_toks_index[0]+2] = 1\n                end_tokens[k,selected_toks_index[-1]+2] = 1\n    \n    if is_train_eval:           \n        dataset={'input_ids':input_ids,\n                 'attention_mask':attention_mask,\n                 'token_type_ids':token_type_ids,\n                 'start_tokens':start_tokens,\n                 'end_tokens':end_tokens,\n                 'tweets':tweets,\n                 'offsets':offsets}\n\n    #for pred\n    else:\n        dataset={'input_ids':input_ids,\n                 'attention_mask':attention_mask,\n                 'token_type_ids':token_type_ids,\n                 'tweets':tweets,\n                 'offsets':offsets}\n            \n    return dataset ","16ef9acc":"model_path=\"..\/input\/robertatransformer\/roberta-base-tf_model.h5\"\nmodel_class=TFRobertaModel\nconfig = RobertaConfig.from_pretrained(\"..\/input\/robertatransformer\/roberta-base-config.json\")\ntokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file=\"..\/input\/robertatransformer\/roberta-base-vocab.json\", \n                                            merges_file=\"..\/input\/robertatransformer\/roberta-base-merges.txt\", \n                                            lowercase=True,\n                                            add_prefix_space=True)","e938f723":"trainset=load_data_roberta(train_df,tokenizer, is_train_eval=True) \ntestset=load_data_roberta(test_df,tokenizer, is_train_eval=False) ","d7f6c855":"PAD_ID=1\nlr=args.learning_rate\ndef build_model_cnn(model_class,model_path,config):\n    MAX_LEN_WORD=args.max_seq_length\n    ids = tf.keras.layers.Input((MAX_LEN_WORD,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN_WORD,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN_WORD,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN_WORD - tf.reduce_sum(padding, -1)\n    max_len_word = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len_word]\n    att_ = att[:, :max_len_word]\n    tok_ = tok[:, :max_len_word]\n\n    bertmodel = model_class.from_pretrained(model_path,config=config)\n    x = bertmodel(ids_,attention_mask=att_,token_type_ids=tok_)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(768,2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n    model.compile(loss=loss_fn,optimizer=optimizer)\n    \n    \n    # this is required as `model.predict` needs a fixed size!\n    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN_WORD - max_len_word]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN_WORD - max_len_word]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    return model, padded_model","fd6b68d6":"#loss function for first stage model\nLABEL_SMOOTHING=0.1\ndef loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss\n","f031e140":"def save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)","26758939":"def load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\n","b1ba2d38":"#The metrics to measure the perfomance of the predict results for both stage1 and stage2 model\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","e0f2a524":"#def train_pred_cv():\njac = []; \nVER='v0'; \nDISPLAY=1\nn_splits=args.cv_splits\nlr=args.learning_rate\nMAX_LEN=args.max_seq_length\nn_best=3\n\ninput_ids=trainset['input_ids']\nattention_mask=trainset['attention_mask']\ntoken_type_ids=trainset['token_type_ids']\nstart_tokens=trainset['start_tokens']\nend_tokens=trainset['end_tokens']\n\ninput_ids_test=testset['input_ids']\nattention_mask_test=testset['attention_mask']\ntoken_type_ids_test=testset['token_type_ids']\n\nct_train=input_ids.shape[0]\nct_test=input_ids_test.shape[0]\noof_start = np.zeros((ct_train,MAX_LEN))\noof_end = np.zeros((ct_train,MAX_LEN))\npreds_start = np.zeros((ct_test,MAX_LEN))\npreds_end = np.zeros((ct_test,MAX_LEN))\n\njac_valiation=np.zeros((ct_train,))\nout_of_range=[]\nabnormal=[]\n\nskf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=args.seed)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n    \n    print('### FOLD %i'%(fold+1))\n    \n    K.clear_session()\n    #set_seed(args)\n    model, padded_model = build_model_cnn(model_class,model_path,config)\n    \n   \n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n        \n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = f'roberta-base-{fold+1}.h5'\n\n    for epoch in range(1, args.num_train_epochs + 1):\n        BATCH_SIZE=args.train_batch_size\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) \/ BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]        \n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, \n                  callbacks=[],validation_data=(inpV, targetV), shuffle=False)  \n    \n    save_weights(model, weight_fn)\n        \n    print('Loading model for validation and pred...')\n        # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    #load_weights(model, weight_fn)\n    padded_model.set_weights(model.get_weights()) \n\n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n\n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_test,attention_mask_test,token_type_ids_test],verbose=DISPLAY)\n    preds_start+=preds[0]\/n_splits\n    preds_end+=preds[1]\/n_splits\n\n    fold_jac_val=[]\n    for k in idxV:        \n        a= np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])        \n        if a>b:\n            st=train_df.loc[k,'text']\n        else:\n            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-2:b-1])\n        jac_val=jaccard(st,train_df.loc[k,'selected_text'])\n        fold_jac_val.append(jac_val)\n    jac.append(np.mean(fold_jac_val))\n    print(f'>>>> FOLD {fold+1} Jaccard ={np.mean(fold_jac_val)}')   ","bdd9571c":"print(f'The CV jaccard is {np.mean(jac)}')","3cc55953":"roberta_models=[\"roberta-base-1.h5\",\n                \"roberta-base-2.h5\",\n                \"roberta-base-3.h5\",\n                \"roberta-base-4.h5\",\n                \"roberta-base-5.h5\"]","d44d763b":"MAX_LEN_WORD=args.max_seq_length\nDISPLAY=1\n                                    \ninput_ids_test=testset['input_ids']\nattention_mask_test=testset['attention_mask']\ntoken_type_ids_test=testset['token_type_ids']\npreds_test_start = np.zeros((ct_test,MAX_LEN_WORD))\npreds_test_end = np.zeros((ct_test,MAX_LEN_WORD))\n\ninput_ids_train=trainset['input_ids']\nattention_mask_train=trainset['attention_mask']\ntoken_type_ids_train=trainset['token_type_ids']\npreds_oof_start = np.zeros((ct_train,MAX_LEN_WORD))\npreds_oof_end = np.zeros((ct_train,MAX_LEN_WORD))","8043667a":"model_number=len(roberta_models)\nfor i in range(model_number):\n    print(f\"predict roberta model----{i+1}\")\n    K.clear_session()\n    weight_fn=roberta_models[i]\n    _, padded_model = build_model_cnn(model_class,model_path,config)   \n    load_weights(padded_model, weight_fn)\n    preds_test = padded_model.predict([input_ids_test,attention_mask_test,token_type_ids_test],verbose=DISPLAY)\n    preds_test_start+=preds_test[0]\/model_number\n    preds_test_end+=preds_test[1]\/model_number\n    \n    preds_oof = padded_model.predict([input_ids_train,attention_mask_train,token_type_ids_train],verbose=DISPLAY)\n    preds_oof_start+=preds_oof[0]\/model_number\n    preds_oof_end+=preds_oof[1]\/model_number","d33e1227":"def token_level_to_char_level(text, offsets, preds):\n    probas_char = np.zeros(len(text))\n    for i, offset in enumerate(offsets):\n        if offset[0] or offset[1]:\n            probas_char[offset[0]:offset[1]] = preds[i]\n    return probas_char","0b4e2c62":"tweets_train=trainset['tweets']\noffsets_train=trainset['offsets']\nprobas_train_start=[]\nprobas_train_end=[]\n\n\nfor idx, tweet in enumerate(tweets_train):\n    probas_train_start.append(token_level_to_char_level(tweet, offsets_train[idx], preds_oof_start[idx]))\n    probas_train_end.append(token_level_to_char_level(tweet, offsets_train[idx], preds_oof_end[idx]))\n","d6229d3a":"tweets_test=testset['tweets']\noffsets_test=testset['offsets']\nprobas_test_start=[]\nprobas_test_end=[]\nfor idx, tweet in enumerate(tweets_test):\n    probas_test_start.append(token_level_to_char_level(tweet, offsets_test[idx], preds_test_start[idx]))\n    probas_test_end.append(token_level_to_char_level(tweet, offsets_test[idx], preds_test_end[idx]))","2ee0a055":"with open('char_pred_train_start.pkl', 'wb') as handle:\n    pickle.dump(probas_train_start, handle)\nwith open('char_pred_train_end.pkl', 'wb') as handle:\n    pickle.dump(probas_train_end, handle)","c8e8bf0d":"#convert text to char-level tokens\ntokenizer_char = Tokenizer(num_words=None, char_level=True, oov_token='UNK', lower=True)\ntokenizer_char.fit_on_texts(train_df['text'].values)\nlen_voc = len(tokenizer_char.word_index) + 1\n\nX_train = tokenizer_char.texts_to_sequences(train_df['text'].values)\nX_test = tokenizer_char.texts_to_sequences(test_df['text'].values)","dbb36efa":"def get_start_end_string(text, selected_text):\n    len_selected_text = len(selected_text)\n    idx_start, idx_end = 0, 0\n    \n    candidates_idx = [i for i, e in enumerate(text) if e == selected_text[0]]\n    for idx in candidates_idx:\n        if text[idx : idx + len_selected_text] == selected_text:\n            idx_start = idx\n            idx_end = idx + len_selected_text-1\n            break\n    assert text[idx_start: idx_end+1] == selected_text, f'\"{text[idx_start: idx_end+1]}\" instead of \"{selected_text}\" in \"{text}\"'\n\n    char_targets = np.zeros(len(text))\n    char_targets[idx_start: idx_end+1] = 1\n    \n    return idx_start, idx_end","37c5d5b0":"def load_data_second_model(df, X, char_start_probas, char_end_probas, max_len_char, train=True):\n    \n    ct=len(df)\n    X = pad_sequences(X, maxlen=max_len_char, padding='post', truncating='post')\n\n    start_probas = np.zeros((ct, max_len_char), dtype=float)\n    for i, p in enumerate(char_start_probas):\n        len_ = min(len(p), max_len_char)\n        start_probas[i, :len_] = p[:len_]\n    start_probas=np.expand_dims(start_probas, axis=2)\n\n    end_probas = np.zeros((ct, max_len_char), dtype=float)\n    for i, p in enumerate(char_end_probas):\n        len_ = min(len(p), max_len_char)\n        end_probas[i, :len_] = p[:len_]\n    end_probas=np.expand_dims(end_probas, axis=2)\n\n    sentiments_list = ['positive', 'neutral', 'negative']\n\n    texts = df['text'].values\n    selected_texts = df['selected_text'].values if train else [''] * len(df)\n    sentiments = df['sentiment'].values\n    sentiments_input = [[sentiments_list.index(s)]*max_len_char for s in sentiments]\n\n\n    if train:\n        start_idx = np.zeros((ct,max_len_char),dtype='int32')\n        end_idx = np.zeros((ct,max_len_char),dtype='int32')\n        for i, (text, sel_text) in enumerate(zip(df['text'].values, df['selected_text'].values)):\n            start, end = get_start_end_string(text, sel_text.strip())\n            start_idx[i,start]=1\n            end_idx[i,end]=1\n    else:\n        start_idx = [0] * len(df)\n        end_idx = [0] * len(df)\n\n    dataset= {\n        'ids': X, #np.array(len,max_len_char)\n        'probas_start': start_probas,  #np.array(len,max_len_char,1)\n        'probas_end': end_probas, #np.array(len,max_len_char,1)\n        'target_start': start_idx, #np.array\n        'target_end': end_idx, #np.array\n        'text': texts, #pd.series\n        'selected_text': selected_texts, #pd.series\n        'sentiment': sentiments,\n        'sentiment_input': np.array(sentiments_input),#list\n        }\n        \n    return dataset","067ca19d":"char_dataset_train=load_data_second_model(train_df, \n                                          X_train, \n                                          probas_train_start, \n                                          probas_train_end, \n                                          max_len_char=args.max_char_length, \n                                          train=True)\nchar_dataset_test=load_data_second_model(test_df, \n                                          X_test, \n                                          probas_test_start, \n                                          probas_test_end, \n                                          max_len_char=args.max_char_length, \n                                          train=False)","ad57fd7e":"LABEL_SMOOTHING=0.1\ndef loss_fn_2(y_true, y_pred):\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss","f5b7c535":"class ConvBlock(L.Layer):\n  def __init__(self,out_dim,kernel_size,padding=\"same\"):\n    super(ConvBlock, self).__init__()\n    self.conv=L.Conv1D(out_dim,kernel_size, padding=padding)\n    self.bn=L.BatchNormalization()\n            \n  def call(self, inputs):\n    x=self.conv(inputs)\n    x=self.bn(x)\n    x=tf.keras.activations.relu(x)\n    return x","d8c24d6b":"class Logits(L.Layer):\n    def __init__(self,dim1=32,dim2=2):\n        super(Logits, self).__init__()\n        self.dense1=L.Dense(dim1,activation='relu')\n        self.dense2=L.Dense(dim2)\n\n    def call(self, inputs):\n        x=self.dense1(inputs)\n        x=self.dense2(x)\n\n        return x","44989b7a":"class CharCNN(tf.keras.Model):\n\n    def __init__(self,len_voc,cnn_dim=32, char_embed_dim=16, sent_embed_dim=16, \n               proba_cnn_dim=16, kernel_size=3,max_len_char=args.max_char_length):\n        super(CharCNN, self).__init__()\n        self.CharEmbedding = L.Embedding(input_dim=len_voc, output_dim=char_embed_dim)\n        self.SentimentEmbedding = L.Embedding(input_dim=3, output_dim=sent_embed_dim)\n        self.ProbasCNN = ConvBlock(proba_cnn_dim,kernel_size)\n        self.CNN1=ConvBlock(cnn_dim,kernel_size)\n        self.CNN2=ConvBlock(cnn_dim*2,kernel_size)\n        self.CNN3=ConvBlock(cnn_dim*4,kernel_size)\n        self.CNN4=ConvBlock(cnn_dim*8,kernel_size)\n        self.Dropout = L.Dropout(0.5)\n        self.dense_logits=Logits(32,2)\n        \n\n    def call(self, inputs):\n        char_ids,sentiment,start_probas,end_probas=inputs\n        probas = tf.concat([start_probas, end_probas], -1) #(b,max_len_char,2)\n        probas_fts = self.ProbasCNN(probas) \n        #print(\"probas_fts\",probas_fts.shape)    \n        char_fts = self.CharEmbedding (char_ids) #(b,max_len_char,16)\n        #print(\"char_fts\",char_fts.shape)\n        sentiment_fts = self.SentimentEmbedding(sentiment) #(b,max_len_char,16)\n        #print(\"sentiment_fts\",sentiment_fts.shape)\n\n        x = tf.concat([char_fts, sentiment_fts, probas_fts], -1) \n        features = self.CNN1(x)\n        features = self.CNN2(features)\n        features = self.CNN3(features)\n        features = self.CNN4(features)    \n        #print(\"features\",features.shape)\n\n        logits=self.Dropout(features)\n        logits=self.dense_logits(logits) \n        #print(\"logits\",logits.shape)\n\n        start_logits, end_logits = logits[:, :, 0], logits[:, :, 1]\n        start_logits= tf.keras.activations.softmax(start_logits)\n        end_logits= tf.keras.activations.softmax(end_logits)\n        #print(\"start_logits\",start_logits.shape)\n\n        return start_logits,end_logits","a9f8bbcc":"inpTest=[char_dataset_test['ids'],\n           char_dataset_test['sentiment_input'],\n           char_dataset_test['probas_start'],\n           char_dataset_test['probas_end']]","323d8856":"n_splits=args.cv_splits_2\nchar_pred_oof_start=np.zeros((ct_train,args.max_char_length))\nchar_pred_oof_end=np.zeros((ct_train,args.max_char_length))\nchar_pred_test_start=np.zeros((ct_test,args.max_char_length))\nchar_pred_test_end=np.zeros((ct_test,args.max_char_length))\njac_2=[]\nsplits = list(StratifiedKFold(n_splits=n_splits).split(X=train_df, y=train_df['sentiment']))\nfor i, (train_idx, val_idx) in enumerate(splits):\n    print(\"-----------------------------------------------------------------------\")\n    print(f\"start train fold---{i+1}\")\n    inp_train=[char_dataset_train['ids'][train_idx],\n               char_dataset_train['sentiment_input'][train_idx],\n               char_dataset_train['probas_start'][train_idx],\n               char_dataset_train['probas_end'][train_idx]]\n    target_train=[char_dataset_train['target_start'][train_idx],char_dataset_train['target_end'][train_idx]]\n    \n    inp_val=[char_dataset_train['ids'][val_idx],\n               char_dataset_train['sentiment_input'][val_idx],\n               char_dataset_train['probas_start'][val_idx],\n               char_dataset_train['probas_end'][val_idx]]\n    target_val=[char_dataset_train['target_start'][val_idx],char_dataset_train['target_end'][val_idx]]\n    model=CharCNN(len_voc)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=args.learning_rate_2)\n    model.compile(loss=loss_fn_2,optimizer=optimizer)\n    model.fit(inp_train, target_train, epochs=args.epoches_2, batch_size=128, verbose=1, \n              validation_data=(inp_val,target_val),\n                  callbacks=[])\n    print('preditc the oof')\n    pred_val_start,pred_val_end=model.predict(inp_val,batch_size=128, verbose=1, callbacks=[])\n    char_pred_oof_start[val_idx]=pred_val_start\n    char_pred_oof_end[val_idx]=pred_val_end\n    print(\"calcuate the jaccard for valset\")\n    fold_jac=[]\n    for k in val_idx:\n        a=np.argmax(char_pred_oof_start[k,])\n        b=np.argmax(char_pred_oof_end[k,])\n        orig_text=char_dataset_train['text'][k,]\n        orig_selected=char_dataset_train['selected_text'][k,]\n        if a>b: \n            pred_selected=orig_text\n        else:            \n            pred_selected=orig_text[a:b+1]\n        jac_val=jaccard(pred_selected,orig_selected)\n        fold_jac.append(jac_val)\n    jac_2.append(fold_jac)\n    print(\"fold jac is:\", np.mean(fold_jac))\n        \n    print('predict the testset')\n    pred_test_start,pred_test_end=model.predict(inpTest,batch_size=128, verbose=1, callbacks=[])\n    char_pred_test_start+=pred_test_start\/n_splits\n    char_pred_test_end+=pred_test_end\/n_splits","f01bac18":"print(f'The mean Jaccard value is {np.mean(jac_2)}')","3cdc7ead":"def convert_prob_to_string(dataset, pred_start, pred_end):\n    ct=len(dataset['text'])\n    pred=[]\n    for k in range(ct):        \n        start_idx=np.argmax(pred_start[k])\n        end_idx=np.argmax(pred_end[k])\n        if start_idx>end_idx:\n            pred_selected_text=dataset['text'][k]\n        else:\n            pred_selected_text=dataset['text'][k][start_idx:end_idx+1]\n        pred.append(pred_selected_text.strip())\n    return pred      ","a2de638a":"pred=convert_prob_to_string(char_dataset_test,char_pred_test_start, char_pred_test_end)\ntest_df['selected_text']=pred\ntest_df.to_csv('submission.csv',columns=['textID','selected_text'], index=False)\n","49b924af":"test_df.sample(10)","4ff846bc":"## 2.2 Build the RoBERTa transformer with one layer CNN on the top\n\nFor faster training time, I sorted the input dataset according to the text length. Thus, the data in the same batch has almost the same lengh, and I could pad them to the shortter sequence to decrease the process time for the batch. That is why there are two models in the following code. The \"model\" is for traning, and the \"padded_model\" is for predicting in which every batch has the same sequece length. I choose the categorical cross entropy function as the loss funciton and Adam as the optimizer. And since this is finetuning to a pretrained transfomer, I choose the learning rate as 3e-5 which is smaller than other model training. I also tried to apply learning rate scheduler to imprvoe the performance. But it didn't help significantly. So I kept the simple way.","7d1cff5d":"# 1. Tweet sentiment extraction\n\n## 1.1. The goal and metrics\n\nThe goal of this kaggle competition is to extract sentiment summarization from tweet texts which are denoted by sentiment type with neural,positive or negative. The data format is as following. The column \"selected_text\" is the target we should predict with the infomation of \"text\" and \"setiment\".\n\n![image.png](attachment:image.png)","7e4474e9":"# 2. First stage model\n\n![image.png](attachment:image.png)\n","fe824c64":"## 2.3 Training the model with k-Folds\n\nAfter some experiments, I choose 5 folds and 3 epoches for every fold to train the model. The result turns out not too bad. The local CV jaccard is 0.705, and the LB is 0.712.","53eb5cae":"\nThe metrics is [world level jaccard](http:\/\/en.wikipedia.org\/wiki\/Jaccard_index), The function of jaccard is as below:\n![image.png](attachment:image.png)\n\nThe overall metric is: \n$$score=\\sum_{i=1}^n jaccard(gt_i,dt_i)$$\nWhere n is the number of examples in the dataset, $gt_i$ is the ith ground truth, $dt_i$ is the ith prediction.\n","15a79272":"## 1.2 The archetecture\n\nIn this notebook, I present a solution which build two_stage models to extract the sentiment. \n\nThe first stage model is based on WordPiece level tokens. I apply Roberta transformer with a CNN layer on the top to predict the probabilities of the start and end index for the tokens in the sequence. I then build the second stage model based on character_level tokens to futhur compute the probilities of start and end index of sentiment summarization.\n\nThe frameworks I use are tensorflow and transfomer. And I use kaggle GPU to run my code. \n\nI am grateful that many people generously shared their excellent ideas and solutions. I learned a lot from this competition, and this solution borrowed largely from the following posts:\n\nhttps:\/\/www.kaggle.com\/khoongweihao\/tse2020-roberta-cnn-random-seed-distribution\n\nhttps:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/159477\n ","38a4d167":"## 3.2 Build and train the character level model (CNN)\n\nIn this step, I still choose the categorical_crossentropy function with label_smoothing as the loss function. And I train the model with 5 folds cross validation. Every fold has 5 epoches. The CV jaccard is around 0.78 and the LB result is around 0.728. I expected the CV was close to the LB. What caused the big difference between them? I guess the trainset was trained by 2 stages that might be the reason why the validation data got much better score.    ","3726905e":"I tried bert_base, bert_large, Alber base, Albert large, RoBERTa large during building the WordPiece token level model. Ultimately, I choose RoBERTa_base as the transfomer for my first stage model. My choice was made based on two reasons. First, the jaccard value from outcome of the RoBERTa base transfomer is slightly better than other transfomers. Second, the computation speed is significently quicker than others. I think the Byte-Pair-Encoding of the RoBERTa helps get better results and remove the next sentence prediction objective in RoBERTa decreases the trainning time.","5d1b6250":"# 4. Some thoughts\n\nAs I previously mentioned, I have tried to apply BERT and Albert transformer to the first stage model and it didn't work well. At this point, I am wondering that training variety of models and emsembling the results should have improved the overall score. \n\nBesides, do more experiements with learning rate, optimizer should have helped to boost the score.\n\nParticipating in this competition took quite a few efforts, but I gained many hands-on skills and state_of_art technologies in the field of NLP and have a lot of fun.\n","a3f1ea60":"## 2.4 Predict the test set and train set with pretrained first level model","94b6be48":"## 3.3 Predict the testset and convet probabilities of start and end index to target string","61559b72":"# 3. Second stage model\n\nI then build a second level model to futhur compute the probility of start and end index of sentiment summary. This model is based on character_level tokens, and it has three inputs:\n<br>\n1) The output of first stage model. \n<br>\n2) The sequence of character_level tokens of tweets.\n<br>\n3) The sentiment type(neutral, positive, negative)\n\n![image.png](attachment:image.png)","ed20aba6":"## 2.5 Convert the WordPiece level probabilities to char level probabilities\n\nThe prediction from first level model is based on word peice level tokens. In order to feed this prediction to the second level model which is based on charecter_level tokens, I convert the prediction to character level. The following is a example\uff1a\n\n![image.png](attachment:image.png)","f16b5d29":"## 2.1 Prepare the data and quick EDA\n\nTake a glimpse of the data. It is obviously that the lengh of targets for negative and positive tweets are much shortter than the neutral tweets. The length of the targets for neutral tweets is almost the same as the raw tweets. Let us feed this information and allow the model learning the pattern. ","f01eb097":"## 3.1 Prepre the data for second stage model\n\n"}}