{"cell_type":{"4ed5fc17":"code","d98e7157":"code","0d174633":"code","c8e5ccc2":"code","c1f81408":"code","1ee501a2":"code","93c936cc":"code","c9b9228f":"code","c0731d8f":"code","ba4189e7":"code","003444c4":"code","e8fcd179":"code","a54d6f14":"code","c4e09773":"code","034360a6":"code","8c8b5940":"code","3788503a":"code","b9f356fc":"code","2eeb4756":"code","7e0fc607":"code","36875525":"markdown","9ba630b5":"markdown","926bd3de":"markdown","93b446bb":"markdown","76a03418":"markdown"},"source":{"4ed5fc17":"# Let's install Feature-engine\n# an open-source Python library for feature engineering\n# it allows us to select which features we want to transform\n# straight-away from within each transformer\n\n# https:\/\/feature-engine.readthedocs.io\/en\/latest\/index.html\n\n!pip install feature-engine","d98e7157":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# for the model\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\n\n# for feature engineering\nfrom feature_engine import imputation as mdi\nfrom feature_engine import encoding as ce","0d174633":"data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\ndata.head()","c8e5ccc2":"# the aim of this notebook is to show how to optimize hyperparameters of an\n# entire machine learning pipeline.\n\n# So I will take a shortcut and remove some features to make things simpler\n\ncols = [\n    'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin',\n    'Embarked', 'Survived'\n]\n\ndata = data[cols]\n\ndata.head()","c1f81408":"# Cabin: extract numerical and categorical part and delete original variable\n\ndata['cabin_num'] = data['Cabin'].str.extract('(\\d+)') # captures numerical part\ndata['cabin_num'] = data['cabin_num'].astype('float')\ndata['cabin_cat'] = data['Cabin'].str[0] # captures the first letter\n\ndata.drop(['Cabin'], axis=1, inplace=True)\n\ndata.head()","1ee501a2":"# make list of variables types\n# we need these lists to indicate Feature-engine which variables it should modify\n\n# numerical: discrete\ndiscrete = [\n    var for var in data.columns if data[var].dtype != 'O' and var != 'Survived'\n    and data[var].nunique() < 10\n]\n\n# numerical: continuous\ncontinuous = [\n    var for var in data.columns\n    if data[var].dtype != 'O' and var != 'Survived' and var not in discrete\n]\n\n# categorical\ncategorical = [var for var in data.columns if data[var].dtype == 'O']\n\nprint('There are {} discrete variables'.format(len(discrete)))\nprint('There are {} continuous variables'.format(len(continuous)))\nprint('There are {} categorical variables'.format(len(categorical)))","93c936cc":"# discrete variables\n\ndiscrete","c9b9228f":"# continuous variables\n\ncontinuous","c0731d8f":"# categorical variables\n\ncategorical","ba4189e7":"# separate into training and testing set\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop('Survived', axis=1),  # predictors\n    data['Survived'],  # target\n    test_size=0.1,  # percentage of obs in test set\n    random_state=0)  # seed to ensure reproducibility\n\nX_train.shape, X_test.shape","003444c4":"titanic_pipe = Pipeline([\n\n    # missing data imputation - we replace na in numerical variables\n    # with an arbitrary value. \n    ('imputer_num',\n     mdi.ArbitraryNumberImputer(arbitrary_number=-1,\n                                variables=['Age', 'Fare', 'cabin_num'])),\n    \n    # for categorical variables, we can either replace na with the string\n    # missing or with the most frequent category\n    ('imputer_cat',\n     mdi.CategoricalImputer(variables=['Embarked', 'cabin_cat'])),\n\n    # categorical encoding - we will group rare categories into 1\n    ('encoder_rare_label', ce.RareLabelEncoder(\n        tol=0.01,\n        n_categories=2,\n        variables=['Embarked', 'cabin_cat'],\n    )),\n    \n    # we replace category names by numbers\n    ('categorical_encoder', ce.OrdinalEncoder(\n        encoding_method='ordered',\n        variables=['cabin_cat', 'Sex', 'Embarked'],\n    )),\n\n    # Gradient Boosted machine\n    ('gbm', GradientBoostingClassifier(random_state=0))\n])","e8fcd179":"# now we create the hyperparameter space that we want to sample\n# that is, the hyperparameter values that we want to test.\n\n# to perform Grid search, we need to specifically provide the hyperparameter\n# values that we want to test\n\n# to opimize hyperparameters within a pipeline, we assemble the space\n# as follows:\n\nparam_grid = {\n    # try different feature engineering parameters:\n    \n    # test different parameters to replace na with numbers\n    'imputer_num__arbitrary_number': [-1, 99],\n    \n    # test imputation with frequent category or string missing\n    'imputer_cat__imputation_method': ['missing','frequent'],\n    \n    # test different thresholds to group rare labels\n    'encoder_rare_label__tol': [0.1, 0.2],\n    \n    # test 2 different encoding strategies\n    'categorical_encoder__encoding_method': ['ordered', 'arbitrary'],\n    \n    # try different gradient boosted tree model paramenters\n    'gbm__max_depth': [None, 1, 3],\n    'gbm__n_estimators': [10, 20, 50, 100, 200]\n}\n\n# (note how we call the step name in the pipeline followed by __\n# followed by the name of the hyperparameter that we want to modify)\n\n# for more details on the Feature-engine transformers hyperparameters, visit\n# Feature-engine documentation","a54d6f14":"# now we set up the grid search with cross-validation\n\n# we are optimizing over few hyperparameters altogether, so \n# a GridSearch should be more than enough\n\ngrid_search = GridSearchCV(\n    titanic_pipe, # the pipeline\n    param_grid, # the hyperparameter space\n    cv=3, # the cross-validation\n    scoring='roc_auc', # the metric to optimize\n)\n\n# for more details in the grid parameters visit:\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html","c4e09773":"# and now we train over all the possible combinations of the parameters\n# specified above\ngrid_search.fit(X_train, y_train)\n\n# and we print the best score over the train set\nprint((\"best roc-auc from grid search: %.3f\"\n       % grid_search.score(X_train, y_train)))","034360a6":"# and finally let's check the performance over the test set\n\nprint((\"best linear regression from grid search: %.3f\"\n       % grid_search.score(X_test, y_test)))","8c8b5940":"# we can find the best pipeline with its parameters like this\n\ngrid_search.best_estimator_","3788503a":"# and find the best fit parameters like this\n\ngrid_search.best_params_","b9f356fc":"# we also find the data for all models evaluated\n\nresults = pd.DataFrame(grid_search.cv_results_)\n\nprint(results.shape)\n\nresults.head()","2eeb4756":"# we can order the different models based on their performance\n\nresults.sort_values(by='mean_test_score', ascending=False, inplace=True)\n\nresults.reset_index(drop=True, inplace=True)\n\n\n# plot model performance and the generalization error\n\nresults['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)\n\nplt.ylabel('Mean test score - ROC-AUC')\nplt.xlabel('Hyperparameter combinations')","7e0fc607":"# and to wrap up:\n# let's explore the importance of the features\n\nimportance = pd.Series(grid_search.best_estimator_['gbm'].feature_importances_)\nimportance.index = data.drop('Survived', axis=1).columns\nimportance.sort_values(inplace=True, ascending=False)\nimportance.plot.bar(figsize=(12,6))","36875525":"## Assembling a Feature Engineering Pipeline with Hyperparameter Optimization\n\nIn this notebook, I will assemble a feature engineering pipeline followed by a Gradient Boosting Classifier. I will search for the best hyperparameters both for the machine learning model and the feature engineering steps using Grid Search.\n\nIn summary, we will:\n\n- set up a series of feature engineering steps using [Feature-engine](https:\/\/feature-engine.readthedocs.io\/en\/latest\/index.html)\n- train a Gradient Boosting Classifier\n- train the pipeline with cross-validation, looking over different feature-engineering and model hyperparameters\n\nFor more details on feature engineering and hyperparameter optimization feel free to check my [online courses](https:\/\/www.trainindata.com\/).","9ba630b5":"If you liked this notebook and would like to know more about feature engineering and hyperparameter optimization feel free to check my [online courses](https:\/\/www.trainindata.com\/).","926bd3de":"## Load the data","93b446bb":"## Grid Search with Cross-validation\n\nFor hyperparameter search we need:\n\n- the machine learning pipeline that we want to optimize (in the previous cell)\n- the hyperparamter space that we want to sample (next cell)\n- a metric to optimize\n- an algorithm to sample the hyperparameters (Grid Search in this case)\n\nLet's do that.","76a03418":"### Set up the pipeline\n\nWe assemble a pipeline with default or some hyperparameters for each step. But we will modify this during the hyperparameter search later on."}}