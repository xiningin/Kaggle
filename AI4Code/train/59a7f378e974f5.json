{"cell_type":{"6c1ef528":"code","a56f6bb4":"code","5323e543":"code","7de7c755":"code","fac5bcf2":"code","626b4ddf":"code","ff7bb030":"code","b5af6da8":"code","ffe5ef99":"markdown","8afafdb3":"markdown","67cd8252":"markdown","1f6da917":"markdown","dcc20542":"markdown","fac16cb4":"markdown"},"source":{"6c1ef528":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n","a56f6bb4":"home = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv',index_col='Id')\ntest = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv',index_col='Id')\n\n#home.head()\n#test.head()","5323e543":"#Generate feature correlation visualization\n\nplt.figure(figsize=(18,18))\nsns.heatmap(home.corr(), annot=True, fmt=\".2f\", vmin=-1, vmax=1, center= 0, cmap= 'coolwarm', cbar_kws= {'orientation': 'horizontal'} )\n\nhome.drop(['MSSubClass','OverallCond','LowQualFinSF','MiscVal','PoolArea','MoSold','YrSold','GarageYrBlt','TotRmsAbvGrd','GarageCars'], axis=1, inplace=True)\ntest.drop(['MSSubClass','OverallCond','LowQualFinSF','MiscVal','PoolArea','MoSold','YrSold','GarageYrBlt','TotRmsAbvGrd','GarageCars'], axis=1, inplace=True)","7de7c755":"#Identify and report count - columns with missing values\nmissing_data = home.isnull().sum()\ncol_with_missing = missing_data[missing_data>0]\ncol_with_missing.sort_values(inplace=True)\nprint(col_with_missing)\n\nmissing_data2 = test.isnull().sum()\ncol_with_missing2 = missing_data2[missing_data2>0]\ncol_with_missing2.sort_values(inplace=True)\nprint(col_with_missing2)\n\nhome.drop(['PoolQC','MiscFeature','Alley','Fence'], axis=1, inplace=True)\ntest.drop(['PoolQC','MiscFeature','Alley','Fence'], axis=1, inplace=True)\n\nfor df in [home, test]:\n#Encode missing categorical features with most common type\/quality, grouping by neighborhood\n    for col in (\"Electrical\",\"MasVnrType\", \"GarageType\",\"BsmtQual\",\"BsmtCond\",\"BsmtFinType1\",\"BsmtFinType2\",\"BsmtExposure\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\"FireplaceQu\",\"KitchenQual\",\"SaleType\"\n               ,\"Exterior1st\",\"Exterior2nd\",\"Utilities\",\"Functional\",\"MSZoning\"):\n         df[col] = df.groupby(\"Neighborhood\")[col].transform(lambda x: x.fillna(x.mode()[0]))\n#Encode missing numerical features with average values, grouping by neighborhood\n    for col2 in (\"MasVnrArea\",\"LotFrontage\",\"TotalBsmtSF\",\"GarageArea\",\"BsmtUnfSF\",\"BsmtFinSF2\",\"BsmtFinSF1\",\"BsmtFullBath\",\"BsmtHalfBath\"):\n        df[col2] = df.groupby('Neighborhood')[col2].transform(lambda x: x.fillna(x.mean()))","fac5bcf2":"#Identify Numerical Data\nnumerical_data = home.select_dtypes(exclude=['object']).drop('SalePrice', axis=1)\n\n#Explore Numerical Data Distribution\nfig = plt.figure(figsize=(20,18))\nfor i in range(len(numerical_data.columns)):\n    fig.add_subplot (9,4,i+1)\n    sns.distplot(a=numerical_data.iloc[:,i].dropna(), kde=False)\n    plt.xlabel(numerical_data.columns[i])\nplt.tight_layout()\nplt.show()","626b4ddf":"#Identify Categorical Data\ncategorical_data = home.select_dtypes(['object'])\n\n#Explore Categorical Data Distribution\nfig = plt.figure(figsize=(20,18))\nfor i in range(len(categorical_data.columns)):\n    fig.add_subplot(12,4,i+1)\n    sns.countplot(x=categorical_data.iloc[:,i])\nplt.tight_layout()\nplt.show()\n","ff7bb030":"#Fit the Model on Training Data\ny = home.SalePrice\nX = home.drop(['SalePrice'],axis=1)\n# Get list of categorical variables\na = (home.dtypes == 'object')\nobject_cols = list(a[a].index)\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(home[object_cols]))\n# One-hot encoding removed index; put it back\nOH_cols_train.index = home.index\n# Remove categorical columns (will replace with one-hot encoding)\nX = X.drop(object_cols, axis=1)\n\nmy_model = XGBRegressor()\nmy_model.fit(X, y)\n\npredictions = my_model.predict(X)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y)))\n\n\n#Apply the model to Test Data\nX_test = test\n# Get list of categorical variables\nb = (test.dtypes == 'object')\nobject_cols2 = list(b[b].index)\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_test = pd.DataFrame(OH_encoder.fit_transform(test[object_cols2]))\n# One-hot encoding removed index; put it back\nOH_cols_test.index = test.index\n# Remove categorical columns (will replace with one-hot encoding)\nX_test = test.drop(object_cols, axis=1)\n\n\npredictions2 = my_model.predict(X_test)\n\nprint(predictions2)","b5af6da8":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': predictions2})\noutput.to_csv('submission.csv', index=False)","ffe5ef99":"# Modelling the Data","8afafdb3":"# **Reading the Data**","67cd8252":"# Explore the data - Missing Values\nIdentify columns with missing values and decide how to best treat these variables\n\n**1. Dropped Columns** - We wish to drop variables with a very high number of null values.\n      <br>  a)<font color=blue> **PoolQC**:<\/font> Pool Quality 1453 null values out of 1460\n      <br>  b)<font color=blue> **MiscFeature**:<\/font> Miscellaneous features (e.g. Elevators\/Shed) 1406 null values out of 1460\n      <br>  c)<font color=blue> **Alley**:<\/font> Type of alley access to property 1369 null values out of 1460\n      <br>  d)<font color=blue> **Fence**:<\/font> Fence Quality on the property 1179 null values out of 1460\n\n**2. Categorical Features:** - For categorical columns with missing values we will encode missing values with the most common type reported in that neighbourhood.\n\n**3. Numerical Features:** - For numerical columns with missing values we will encode with averages value reported in that neighbourhood.\n","1f6da917":"# Explore the data - Correlations\nIdentify property features that have \"very Low\" correlation with Sale Price and property features highly correlated with one another (multicollinearity)\n\n**1. Low Correlation Features (-0.1<x<0.1)** - We wish to drop features that appear to have low value in predicting a home's sale price\n<br>\n      <br>  a)<font color=blue> **MSSubClass**:<\/font> Identifies the type of dwelling involved in the sale (-0.08)\n      <br>  b)<font color=blue> **OverallCond**:<\/font> Rates the overall condition of the house (-0.08)\n      <br>  c)<font color=blue> **LowQualFinSF**:<\/font> Low quality finished square feet (all floors) (-0.03)\n      <br>  d)<font color=blue> **MiscVal**:<\/font> Dollar value of miscellaneous feature (-0.02)\n      <br>  e)<font color=blue> **PoolArea**:<\/font> Month Sold (0.09)\n      <br>  f)<font color=blue> **MoSold**:<\/font> Month Sold (0.05)\n      <br>  g)<font color=blue> **YrSold**:<\/font> Year Sold (-0.03)\n\n**2. Highly Correlated Features |x|>0.8** - We wish to drop features highly similar features to predict sale price when one feature will do\n<br>\n      <br>  a)<font color=blue> **GarageYrBlt**:<\/font> GarageYrBlt & YearBuilt (0.83), drop GarageYrBlt because property without garage more likely than garage without property\n      <br>  b)<font color=blue> **TotRmsAbvGrd**:<\/font> GrLivArea & TotRmsAbvGrd (0.83), drop TotRmsAbvGrd because this variable does not count bathrooms and squarefeet differences is more detailed\n      <br>  c)<font color=blue> **GarageCars**:<\/font> GarageArea and GarageCars (0.88), drop GarageCars because squarefeet differences is more detailed\n\n**3. Other Features**\n<br>\n      <br> As for BsmtFinSF2 (-0.01) and BsmtHalfBath (-0.02) having weak correlation with Sales Price.\n      <br> Also, the strong correlation between TotalBsmtSF vs 1stFlrSF (0.82).\n      <br>\n      <br> I am retaining these features for feature engineering.","dcc20542":"# Submit for Evaluation\n","fac16cb4":"# **My Housing Price Competition Notebook**"}}