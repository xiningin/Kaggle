{"cell_type":{"5577f1d9":"code","efd45694":"code","5a0201fd":"code","f627bf19":"code","86069757":"code","16fcc2eb":"markdown","7efd01ce":"markdown"},"source":{"5577f1d9":"import numpy as np\nimport pandas as pd\n# train = pd.read_csv('..\/input\/train.csv').sample(frac=0.1).reset_index(drop=True)\n# test = pd.read_csv('..\/input\/test.csv').sample(frac=0.1).reset_index(drop=True)\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n# train = return_preproc_data(train)\n# test = return_preproc_data(test)\n","efd45694":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\n\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n# \ntrain_text = train['comment_text']\ntest_text = test['comment_text']\nall_text = pd.concat([train_text, test_text])\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    max_features=10000)\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\ntest_word_features = word_vectorizer.transform(test_text)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(2, 5),#2-6\n    max_features=50000)\nchar_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_text)\ntest_char_features = char_vectorizer.transform(test_text)\n\ntrain_features = hstack([train_char_features, train_word_features])\ntest_features = hstack([test_char_features, test_word_features])\n\n","5a0201fd":"from sklearn.svm import LinearSVC\nfrom sklearn.ensemble import AdaBoostClassifier\n# import xgboost as xgb\nfrom sklearn.calibration import CalibratedClassifierCV\n","f627bf19":"scores = []\nsubmission = pd.DataFrame.from_dict({'id': test['id']})\nfor class_name in class_names:\n    train_target = train[class_name]\n    classifier = LogisticRegression(solver='sag')\n#     svm = LinearSVC(dual=False)\n#     classifier = AdaBoostClassifier(cv, algorithm='SAMME', n_estimators=100)\n#     svm = LinearSVC()\n#     classifier = CalibratedClassifierCV(svm) \n#     classifier = xgb.XGBClassifier(objective='multi:softprob', learning_rate=1,\n#                                    tree_method='auto',\n#                                    max_depth = 12,    \n#                                    silent=True, \n#                                    n_estimators=100, \n#                                    num_class=2, )\n\n#     print('class')\n    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n    scores.append(cv_score)\n    print('CV score for class {} is {}'.format(class_name, cv_score))\n\n    classifier.fit(train_features, train_target)\n    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n\nprint('Total CV score is {}'.format(np.mean(scores)))\n\n# # submission.to_csv('submission.csv', index=False)","86069757":"# def return_preproc_data(data):\n#     import string     \n#     from nltk.stem import WordNetLemmatizer\n#     from nltk.corpus import stopwords\n    \n#     #\u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0437\u043d\u0430\u043a\u0438 \u043f\u0440\u0438\u043f\u0438\u043d\u0430\u043d\u0438\u044f, \u0446\u0438\u0444\u0440\u044b\n#     to_exclude = set(string.punctuation)\n#     to_exclude.update(set(string.digits))\n#     to_exclude.update(\"\\n\")    \n\n#     data['comment_text'] = [''.join([ch for ch in x.strip().lower() if ch not in to_exclude]).strip() for x in data['comment_text']]\n#     #\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f \n#     lemmatizer = WordNetLemmatizer()\n#     data['comment_text'] = [' '.join([lemmatizer.lemmatize(lemma) for lemma in x.split()]) for x in data['comment_text']]\n#     #\u0423\u0434\u0430\u043b\u0438\u043c \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u0430 \n#     stopWords = set(stopwords.words('english'))\n#     data['comment_text'] = [' '.join([word for word in x.split() if word not in stopWords]) for x in data['comment_text']]\n\n#     #\u0443\u0434\u0430\u043b\u0438\u043c \u0441\u043b\u043e\u0432\u0430, \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0434\u043b\u0438\u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u0435 13\n#     data['comment_text'] = [' '.join([word for word in x.split() if len(word) <14]) for x in data['comment_text']]\n\n# #     \u0412\u044b\u043f\u043e\u043b\u043d\u0438\u043c \u0441\u0442\u0435\u043c\u043c\u0438\u043d\u0433(\u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u043a \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435)\n#     from nltk.stem import PorterStemmer\n#     ps = PorterStemmer()\n#     data['comment_text'] = [' '.join([ps.stem(word) for word in x.split()]) for x in data['comment_text']]\n#     return data\n# #     \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \n# #     from keras.preprocessing.text import Tokenizer\n# #     tokenizer = Tokenizer()\n# #     tokenizer.fit_on_texts(data['comment_text'])\n# #     comment_matrix = tokenizer.texts_to_matrix(data['comment_text'])\n","16fcc2eb":"Machine Learning","7efd01ce":"Preprocessing"}}