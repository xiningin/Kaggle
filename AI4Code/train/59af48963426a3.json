{"cell_type":{"3a033515":"code","e324cfcb":"code","583f4d9e":"code","a910a703":"code","625414fb":"code","326b2e12":"code","f7fb9110":"code","83976fb8":"code","44f49de2":"code","22af4db3":"code","45d1ea61":"code","e6af1c57":"code","51145868":"code","c13a2352":"code","35aa7ed2":"code","22a9e5db":"code","25974e11":"code","7b483345":"code","4dd2fee8":"code","5c538fd7":"code","81d3a1dd":"code","cd78af9c":"code","5481bb7a":"code","7e9ce997":"code","3d48af94":"code","3bd274a1":"code","ce98b55f":"code","d79ec009":"code","49ee153f":"code","f92a07cc":"code","5c1df5de":"code","25c71a1e":"code","19ad854e":"code","24db2f57":"code","cb3be7da":"code","9caf109a":"code","a7463f25":"code","5a2ab930":"code","5b45507f":"code","8f3e772b":"code","94debafe":"code","ab868371":"code","5b31a42f":"code","3c6ecf20":"code","3818754c":"code","aa649a69":"code","e5af1344":"code","f8770241":"code","9591332d":"code","2aa6c865":"code","521768ba":"code","f8a0d802":"code","903ac07b":"code","2cddf835":"code","4c8a086e":"code","90fb6a13":"code","25c4c3ff":"code","d51abe84":"code","c6f184cb":"code","d92291ca":"markdown","c9b2a3b3":"markdown","9fc977cd":"markdown","217bf569":"markdown","dc7cdbb3":"markdown","a7b485fa":"markdown","76c8ed22":"markdown","e644963f":"markdown","539998d7":"markdown","3e09c814":"markdown","b203ebeb":"markdown","3297c7bf":"markdown","e4ad5120":"markdown","3b624dcb":"markdown","af74b65b":"markdown"},"source":{"3a033515":"# Importing the Required Libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport warnings; warnings.simplefilter('ignore')\nimport nltk\nimport re\nimport string\nfrom string import punctuation\nfrom nltk import ngrams\nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import SnowballStemmer\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMModel,LGBMClassifier, plot_importance\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\n\nstop_words = set(stopwords.words('english'))\npunctuation = punctuation + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'","e324cfcb":"# Importing the datasets\n\ndf_train = pd.read_csv(\"\/kaggle\/input\/kuc-hackathon-winter-2018\/drugsComTrain_raw.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/kuc-hackathon-winter-2018\/drugsComTest_raw.csv\") \n\nprint (\"The shape of the train set given is : \", df_train.shape)\nprint (\"The shape of the test set given is : \", df_test.shape)\n\ndf_train.head()","583f4d9e":"# Data types\ndf_train.dtypes","a910a703":"# Merging the test and train data \nmerge = [df_train, df_test]\ndf_data = pd.concat(merge)\n\nprint (df_data.shape)\n\ndf_data.head(10)","625414fb":"df_data.describe()","326b2e12":"# Null values\nprint (\"Null values in the dataset : \", df_data.isnull().sum(axis = 0))","f7fb9110":"# Calculating what percentage of data is null\nsize = df_data.shape[0]\n\nprint (\"Total Size of the dataset : \", size)\n\ntotal_na = df_data.isnull().sum(axis = 0)['condition']\nprint (\"Null values : \", total_na)\n\nprint (\"PERCENTAGE : \", (total_na\/size)*100)","83976fb8":"# Dropping the data points with null values as it's very much less than 5% of the whole dataset\ndf_data = df_data.dropna(how = 'any', axis = 0)\n\nprint (\"The shape of the dataset after null values removal :\", df_data.shape)","44f49de2":"# lowercasing the column names so it will be easier for access ^^\ndf_data.columns = df_data.columns.str.lower()","22af4db3":"# Sorting the dataframe\ndf_data.sort_values(['uniqueid'], ascending = True, inplace = True)\ndf_data.reset_index(drop = True, inplace = True)\ndf_data.head(10)","45d1ea61":"# Total unique conditions in the dataset\nprint (df_data['condition'].nunique(), \"\\n\")\n\nprint (\"some of the conditions are : \", df_data['condition'].unique()[0:10])","e6af1c57":"# top 10 drugs with rating equals 1\ndf_data.loc[df_data['rating'] == 1, :]['drugname'].value_counts().head(10)","51145868":"df_data.loc[df_data.usefulcount == 0, 'drugname'].value_counts()","c13a2352":"# Minimum rating in the dataset\ndf_data['rating'].min()","35aa7ed2":"# Converting the date in to date time format \ndf_data['date'] = pd.to_datetime(df_data['date'])","22a9e5db":"# This barplot shows the top 20 drugs with the 10\/10 rating\n\n# Setting the Parameters\nsns.set(font_scale = 1.2, style = 'darkgrid')\nplt.rcParams['figure.figsize'] = [15, 8]\n\nrating = dict(df_data.loc[df_data.rating == 10, \"drugname\"].value_counts())\ndrugname = list(rating.keys())\ndrug_rating = list(rating.values())\n\nsns_rating = sns.barplot(x = drugname[0:20], y = drug_rating[0:20])\n\nsns_rating.set(title = 'Top 20 drugs with 10\/10 rating', ylabel = 'Number of Ratings', xlabel = \"Drug Names\")\nplt.setp(sns_rating.get_xticklabels(), rotation=90);","25974e11":"# This barplot shows the Top 20 drugs with the 1\/10 rating\n\n# Setting the Parameter\nsns.set(font_scale = 1.2, style = 'whitegrid')\nplt.rcParams['figure.figsize'] = [15, 8]\n\nrating = dict(df_data.loc[df_data.rating == 1, \"drugname\"].value_counts())\ndrugname = list(rating.keys())\ndrug_rating = list(rating.values())\n\nsns_rating = sns.barplot(x = drugname[0:20], y = drug_rating[0:20], palette = 'winter')\n\nsns_rating.set(title = 'Top 20 drugs with 1\/10 rating', ylabel = 'Number of Ratings', xlabel = \"Drug Names\")\n\nplt.setp(sns_rating.get_xticklabels(), rotation=90);","7b483345":"# A countplot of the ratings so we can see the distribution of the ratings\nplt.rcParams['figure.figsize'] = [20,8]\nsns.set(font_scale = 1.4, style = 'whitegrid')\nfig, ax = plt.subplots(1, 2)\n\nsns_1 = sns.countplot(df_data['rating'], palette = 'spring', order = list(range(10, 0, -1)), ax = ax[0])\nsns_2 = sns.distplot(df_data['rating'], ax = ax[1])\nsns_1.set_title('Count of Ratings')\nsns_1.set_xlabel(\"Rating\")\n\nsns_2.set_title('Distribution of Ratings')\nsns_2.set_xlabel(\"Rating\");","4dd2fee8":"# Word cloud of the reviews with rating equal to 10\n\ndf_rate_ten = df_data.loc[df_data.rating == 10, 'review']\nk = (' '.join(df_rate_ten))\n\nwordcloud = WordCloud(width = 1000, height = 500, background_color = 'white').generate(k)\nplt.figure(figsize=(15, 10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off');","5c538fd7":"# Word cloud of the reviews with rating equal to 1\n\ndf_rate_one = df_data.loc[df_data.rating == 1, 'review']\nk1 = (' '.join(df_rate_one))\n\nwordcloud = WordCloud(width = 1000, height = 500).generate(k1)\nplt.figure(figsize=(15, 10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off');","81d3a1dd":"# This barplot shows the mean rating of the drugs per year\n\nmean_rating = dict(df_data.groupby(df_data['date'].dt.year)['rating'].mean())\nplt.rcParams['figure.figsize'] = [12, 7]\nsns.set(font_scale = 1.2, style = 'whitegrid')\nsns_ = sns.barplot(x = list(mean_rating.keys()), y = list(mean_rating.values()), color = 'orange');\nsns_.set_xlabel(\"Year\")\nsns_.set_ylabel(\"Rating\");","cd78af9c":"# This barplot show the top 10 conditions the people are suffering.\n\ncond = dict(df_data['condition'].value_counts())\ntop_condition = list(cond.keys())[0:10]\nvalues = list(cond.values())[0:10]\nsns.set(style = 'darkgrid', font_scale = 1.3)\nplt.rcParams['figure.figsize'] = [18, 7]\n\nsns_ = sns.barplot(x = top_condition, y = values, palette = 'winter')\nsns_.set_title(\"Top 10 conditions\")\nsns_.set_xlabel(\"Conditions\")\nsns_.set_ylabel(\"Count\");","5481bb7a":"# Top 10 drugs which are used for the top condition, that is Birth Control\n\ndf = df_data[df_data['condition'] == 'Birth Control']['drugname'].value_counts()[0: 10]\nsns.set(font_scale = 1.2, style = 'darkgrid')\n\nsns_ = sns.barplot(x = df.index, y = df.values, palette = 'summer')\nsns_.set_xlabel('Drug Names')\nsns_.set_title(\"Top 10 Drugs used for Birth Control\")\nplt.setp(sns_.get_xticklabels(), rotation = 90);","7e9ce997":"# Distribution of the useful count\nsns.set(style = 'whitegrid', font_scale = 1.3)\nplt.rcParams['figure.figsize'] = [12, 7]\nsns.distplot(df_data['usefulcount'].dropna())","3d48af94":"# This barplot shows the number of reviews per year\ndf = df_data['date'].dt.year.value_counts()\ndf = df.sort_index()\n\nsns_ = sns.barplot(x = df.index, y = df.values, color = 'mediumaquamarine')\nsns_.set_title(\"Number of reviews per year\")\nsns_.set_xlabel(\"Year\");","3bd274a1":"# Heatmap of the correlation matrix\nplt.rcParams['figure.figsize'] = [7,5]\nsns.set(font_scale = 1.2)\ncorr = df_data.select_dtypes(include = 'int64').corr()\nsns_heat = sns.heatmap(corr, annot = True, vmin=-1, vmax=1, center=0,\n            cmap=sns.diverging_palette(20, 220, n=200), square=True);\nplt.setp(sns_heat.get_xticklabels(), rotation = 45);","ce98b55f":"# Top 20 unigrams according to the rating\ndf_ = df_data[['rating', 'review']]\ndf_['review'] = df_data['review'].str.replace(\"&#039;\", \"\")\ndf_['review'] = df_['review'].str.replace(r'[^\\w\\d\\s]',' ')\n\ndf_review_5 = \" \".join(df_.loc[df_.rating <= 5, 'review'])\ndf_review_10 = \" \".join(df_.loc[df_.rating > 5, 'review'])\n\ntoken_review_5 = word_tokenize(df_review_5)\ntoken_review_10 = word_tokenize(df_review_10)\n\nunigrams_5 = ngrams(token_review_5, 1)\nunigrams_10 = ngrams(token_review_10, 1)\n\nfrequency_5 = Counter(unigrams_5)\nfrequency_10 = Counter(unigrams_10)\n\ndf_5 = pd.DataFrame(frequency_5.most_common(20))\ndf_10 = pd.DataFrame(frequency_10.most_common(20))\n\n# Barplot that shows the top 20 unigrams\nplt.rcParams['figure.figsize'] = [20,11]\nfig, ax = plt.subplots(1,2)\nsns.set(font_scale = 1.5, style = 'whitegrid')\n\nsns_5 = sns.barplot(x = df_5[1], y = df_5[0], color = 'lightsteelblue', ax = ax[0])\nsns_10 = sns.barplot(x = df_10[1], y = df_10[0], color = 'lightsteelblue', ax = ax[1])\n\n# Setting axes labels\nsns_5.set_title(\"Top 20 unigrams according for rating <= 5\")\nsns_10.set_title(\"Top 20 unigrams according for rating > 5\")\nsns_5.set_ylabel(\"Unigrams\");","d79ec009":"# Top 20 bigrams according to the rating\nbigrams_5 = ngrams(token_review_5, 2)\nbigrams_10 = ngrams(token_review_10, 2)\n\nfrequency_5 = Counter(bigrams_5)\nfrequency_10 = Counter(bigrams_10)\n\ndf_5 = pd.DataFrame(frequency_5.most_common(20))\ndf_10 = pd.DataFrame(frequency_10.most_common(20))\n\n# Barplot that shows the top 20 bigrams\nplt.rcParams['figure.figsize'] = [22,11]\nfig, ax = plt.subplots(1,2)\nsns.set(font_scale = 1.3, style = 'whitegrid')\n\nsns_5 = sns.barplot(x = df_5[1], y = df_5[0], color = 'red', ax = ax[0])\nsns_10 = sns.barplot(x = df_10[1], y = df_10[0], color = 'red', ax = ax[1])\n\n# Setting axes labels\nsns_5.set_title(\"Top 20 bigrams according for rating <= 5\")\nsns_10.set_title(\"Top 20 bigrams according for rating > 5\")\nsns_5.set_ylabel(\"bigrams\");","49ee153f":"# Top 20 trigrams according to the rating\ntrigrams_5 = ngrams(token_review_5, 3)\ntrigrams_10 = ngrams(token_review_10, 3)\n\nfrequency_5 = Counter(trigrams_5)\nfrequency_10 = Counter(trigrams_10)\n\ndf_5 = pd.DataFrame(frequency_5.most_common(20))\ndf_10 = pd.DataFrame(frequency_10.most_common(20))\n\n# Barplot that shows the top 20 trigrams\nplt.rcParams['figure.figsize'] = [25,13]\nfig, ax = plt.subplots(1,2)\nsns.set(font_scale = 1.3, style = 'whitegrid')\n\nsns_5 = sns.barplot(x = df_5[1], y = df_5[0], color = 'orange', ax = ax[0])\nsns_10 = sns.barplot(x = df_10[1], y = df_10[0], color = 'orange', ax = ax[1])\n\n# Setting axes labels\nsns_5.set_title(\"Top 20 trigrams according for rating <= 5\")\nsns_10.set_title(\"Top 20 trigrams according for rating > 5\")\nsns_5.set_ylabel(\"trigrams\");","f92a07cc":"# Removing the stop words before plotting\nstop_words = set(stopwords.words('english'))\ndf_['review'] = df_['review'].str.lower()\ndf_['review_1'] = df_['review'].apply(lambda x: \" \".join(word for word in x.split() if word not in stop_words))\ndf_review = \" \".join(df_['review_1'])\ntokenize = word_tokenize(df_review)\nfrequency = Counter(tokenize)\ndf = pd.DataFrame(frequency.most_common(30))\n\nplt.rcParams['figure.figsize'] = [12, 15]\nsns.set(font_scale = 1.3, style = 'whitegrid')\n\n# plotting\nword_count = sns.barplot(x = df[1], y = df[0], color = 'darkcyan')\nword_count.set_title(\"Word Count Plot\")\nword_count.set_ylabel(\"Words\")\nword_count.set_xlabel(\"Count\");","5c1df5de":"# Giving the Sentiment according to the ratings\ndf_data['sentiment_rate'] = df_data['rating'].apply(lambda x: 1 if x > 5 else 0)","25c71a1e":"def review_clean(review): \n    # changing to lower case\n    lower = review.str.lower()\n    \n    # Replacing the repeating pattern of &#039;\n    pattern_remove = lower.str.replace(\"&#039;\", \"\")\n    \n    # Removing all the special Characters\n    special_remove = pattern_remove.str.replace(r'[^\\w\\d\\s]',' ')\n    \n    # Removing all the non ASCII characters\n    ascii_remove = special_remove.str.replace(r'[^\\x00-\\x7F]+',' ')\n    \n    # Removing the leading and trailing Whitespaces\n    whitespace_remove = ascii_remove.str.replace(r'^\\s+|\\s+?$','')\n    \n    # Replacing multiple Spaces with Single Space\n    multiw_remove = whitespace_remove.str.replace(r'\\s+',' ')\n    \n    # Replacing Two or more dots with one\n    dataframe = multiw_remove.str.replace(r'\\.{2,}', ' ')\n    \n    return dataframe","19ad854e":"# Contraction Dictionary for the expansion\n\ncontractions_dict = {\n    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n    \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n    \"doesn\u2019t\": \"does not\", \"don't\": \"do not\", \"don\u2019t\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\",\n    \"haven't\": \"have not\", \"he'd\": \"he had\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n    \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\"might've\": \"might have\",\n    \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n    \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y\u2019all\": \"you all\", \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"ain\u2019t\": \"am not\", \"aren\u2019t\": \"are not\",\n    \"can\u2019t\": \"cannot\", \"can\u2019t\u2019ve\": \"cannot have\", \"\u2019cause\": \"because\", \"could\u2019ve\": \"could have\", \"couldn\u2019t\": \"could not\", \"couldn\u2019t\u2019ve\": \"could not have\",\n    \"didn\u2019t\": \"did not\", \"doesn\u2019t\": \"does not\", \"don\u2019t\": \"do not\", \"don\u2019t\": \"do not\", \"hadn\u2019t\": \"had not\", \"hadn\u2019t\u2019ve\": \"had not have\",\n    \"hasn\u2019t\": \"has not\", \"haven\u2019t\": \"have not\", \"he\u2019d\": \"he had\", \"he\u2019d\u2019ve\": \"he would have\", \"he\u2019ll\": \"he will\", \"he\u2019ll\u2019ve\": \"he will have\",\n    \"he\u2019s\": \"he is\", \"how\u2019d\": \"how did\", \"how\u2019d\u2019y\": \"how do you\", \"how\u2019ll\": \"how will\", \"how\u2019s\": \"how is\", \"i\u2019d\": \"i would\", \"i\u2019d\u2019ve\": \"i would have\",\n    \"i\u2019ll\": \"i will\", \"i\u2019ll\u2019ve\": \"i will have\", \"i\u2019m\": \"i am\", \"i\u2019ve\": \"i have\", \"isn\u2019t\": \"is not\", \"it\u2019d\": \"it would\", \"it\u2019d\u2019ve\": \"it would have\",\n    \"it\u2019ll\": \"it will\", \"it\u2019ll\u2019ve\": \"it will have\", \"it\u2019s\": \"it is\", \"let\u2019s\": \"let us\", \"ma\u2019am\": \"madam\", \"mayn\u2019t\": \"may not\",\n    \"might\u2019ve\": \"might have\", \"mightn\u2019t\": \"might not\", \"mightn\u2019t\u2019ve\": \"might not have\", \"must\u2019ve\": \"must have\", \"mustn\u2019t\": \"must not\",\n    \"mustn\u2019t\u2019ve\": \"must not have\", \"needn\u2019t\": \"need not\", \"needn\u2019t\u2019ve\": \"need not have\", \"o\u2019clock\": \"of the clock\",\n    \"oughtn\u2019t\": \"ought not\", \"oughtn\u2019t\u2019ve\": \"ought not have\", \"shan\u2019t\": \"shall not\", \"sha\u2019n\u2019t\": \"shall not\", \"shan\u2019t\u2019ve\": \"shall not have\",\n    \"she\u2019d\": \"she would\", \"she\u2019d\u2019ve\": \"she would have\", \"she\u2019ll\": \"she will\", \"she\u2019ll\u2019ve\": \"she will have\", \"she\u2019s\": \"she is\",\n    \"should\u2019ve\": \"should have\", \"shouldn\u2019t\": \"should not\", \"shouldn\u2019t\u2019ve\": \"should not have\", \"so\u2019ve\": \"so have\", \"so\u2019s\": \"so is\",\n    \"that\u2019d\": \"that would\", \"that\u2019d\u2019ve\": \"that would have\", \"that\u2019s\": \"that is\", \"there\u2019d\": \"there would\", \"there\u2019d\u2019ve\": \"there would have\",\n    \"there\u2019s\": \"there is\", \"they\u2019d\": \"they would\", \"they\u2019d\u2019ve\": \"they would have\", \"they\u2019ll\": \"they will\", \"they\u2019ll\u2019ve\": \"they will have\",\n    \"they\u2019re\": \"they are\", \"they\u2019ve\": \"they have\", \"to\u2019ve\": \"to have\", \"wasn\u2019t\": \"was not\", \"we\u2019d\": \"we would\", \"we\u2019d\u2019ve\": \"we would have\",\n    \"we\u2019ll\": \"we will\", \"we\u2019ll\u2019ve\": \"we will have\", \"we\u2019re\": \"we are\", \"we\u2019ve\": \"we have\", \"weren\u2019t\": \"were not\", \"what\u2019ll\": \"what will\",\n    \"what\u2019ll\u2019ve\": \"what will have\", \"what\u2019re\": \"what are\", \"what\u2019s\": \"what is\", \"what\u2019ve\": \"what have\", \"when\u2019s\": \"when is\",\n    \"when\u2019ve\": \"when have\", \"where\u2019d\": \"where did\", \"where\u2019s\": \"where is\", \"where\u2019ve\": \"where have\", \"who\u2019ll\": \"who will\",\n    \"who\u2019ll\u2019ve\": \"who will have\", \"who\u2019s\": \"who is\", \"who\u2019ve\": \"who have\",\"why\u2019s\": \"why is\", \"why\u2019ve\": \"why have\", \"will\u2019ve\": \"will have\",\n    \"won\u2019t\": \"will not\", \"won\u2019t\u2019ve\": \"will not have\", \"would\u2019ve\": \"would have\", \"wouldn\u2019t\": \"would not\", \"wouldn\u2019t\u2019ve\": \"would not have\",\n    \"y\u2019all\": \"you all\", \"y\u2019all\": \"you all\", \"y\u2019all\u2019d\": \"you all would\", \"y\u2019all\u2019d\u2019ve\": \"you all would have\", \"y\u2019all\u2019re\": \"you all are\",\n    \"y\u2019all\u2019ve\": \"you all have\", \"you\u2019d\": \"you would\", \"you\u2019d\u2019ve\": \"you would have\", \"you\u2019ll\": \"you will\", \"you\u2019ll\u2019ve\": \"you will have\",\n    \"you\u2019re\": \"you are\", \"you\u2019re\": \"you are\", \"you\u2019ve\": \"you have\"\n}\ncontractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n\n# Function expand the contractions if there's any\ndef expand_contractions(s, contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, s)","24db2f57":"#df_data['review_clean'] = df_data['review'].apply(review_clean)\ndf_data['review_clean'] = review_clean(df_data['review'])\n\n# Expanding the contractions\ndf_data['review_clean'] = df_data['review_clean'].apply(lambda x: expand_contractions(x))\n\n# Removing punctuations\ndf_data['review_clean'] = df_data['review_clean'].apply(lambda x: ''.join(word for word in x if word not in punctuation))","cb3be7da":"df_data.head()","9caf109a":"# Removing the stopwords\ndf_data['review_clean'] = df_data['review_clean'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))","a7463f25":"# Removing the word stems using the Snowball Stemmer\nSnow_ball = SnowballStemmer(\"english\")\ndf_data['review_clean'] = df_data['review_clean'].apply(lambda x: \" \".join(Snow_ball.stem(word) for word in x.split()))","5a2ab930":"df_data['review_clean'].head(20)","5b45507f":"# Separating the day, month and year from the Date\n\ndf_data['day'] = df_data['date'].dt.day\ndf_data['month'] = df_data['date'].dt.month\ndf_data['year'] = df_data['date'].dt.year","8f3e772b":"df_data.head()","94debafe":"def sentiment(review):\n    # Sentiment polarity of the reviews\n    pol = []\n    for i in review:\n        analysis = TextBlob(i)\n        pol.append(analysis.sentiment.polarity)\n    return pol","ab868371":"df_data['sentiment'] = sentiment(df_data['review'])","5b31a42f":"df_data['sentiment_clean'] = sentiment(df_data['review_clean'])","3c6ecf20":"np.corrcoef(df_data['sentiment'], df_data['rating'])","3818754c":"np.corrcoef(df_data['sentiment_clean'], df_data['rating'])","aa649a69":"# Cleaning the reviews without removing the stop words and using snowball stemmer\n\ndf_data['review_clean_ss'] = review_clean(df_data['review'])\n\ndf_data['review_clean_ss'] = df_data['review_clean_ss'].apply(lambda x: expand_contractions(x))\n\ndf_data['review_clean_ss'] = df_data['review_clean_ss'].apply(lambda x: ''.join(word for word in x if word not in punctuation))\n\ndf_data['sentiment_clean_ss'] = sentiment(df_data['review_clean_ss'])","e5af1344":"np.corrcoef(df_data['sentiment_clean_ss'], df_data['rating'])","f8770241":"df_data.head(10)","9591332d":"#Word count in each review\ndf_data['count_word']=df_data[\"review_clean_ss\"].apply(lambda x: len(str(x).split()))\n\n#Unique word count \ndf_data['count_unique_word']=df_data[\"review_clean_ss\"].apply(lambda x: len(set(str(x).split())))\n\n#Letter count\ndf_data['count_letters']=df_data[\"review_clean_ss\"].apply(lambda x: len(str(x)))\n\n#punctuation count\ndf_data[\"count_punctuations\"] = df_data[\"review\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n#upper case words count\ndf_data[\"count_words_upper\"] = df_data[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n#title case words count\ndf_data[\"count_words_title\"] = df_data[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n#Number of stopwords\ndf_data[\"count_stopwords\"] = df_data[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n\n#Average length of the words\ndf_data[\"mean_word_len\"] = df_data[\"review_clean_ss\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","2aa6c865":"df_data.columns","521768ba":"# Correlation Heatmap of the features engineered\nplt.rcParams['figure.figsize'] = [17,15]\nsns.set(font_scale = 1.2)\ncorr = df_data.select_dtypes(include = 'int64').corr()\nsns_ = sns.heatmap(corr, annot = True, cmap = 'YlGnBu')\nplt.setp(sns_.get_xticklabels(), rotation = 45);","f8a0d802":"# Label Encoding Drugname and Conditions\nlabel_encoder_feat = {}\nfor feature in ['drugname', 'condition']:\n    label_encoder_feat[feature] = LabelEncoder()\n    df_data[feature] = label_encoder_feat[feature].fit_transform(df_data[feature])","903ac07b":"# Defining Features and splitting the data as train and test set\n\nfeatures = df_data[['condition', 'usefulcount', 'sentiment', 'day', 'month', 'year',\n                   'sentiment_clean_ss', 'count_word', 'count_unique_word', 'count_letters',\n                   'count_punctuations', 'count_words_upper', 'count_words_title',\n                   'count_stopwords', 'mean_word_len']]\n\ntarget = df_data['sentiment_rate']\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42)\nprint (\"The Train set size \", X_train.shape)\nprint (\"The Test set size \", X_test.shape)","2cddf835":"# Training Model - I\n\nclf = LGBMClassifier(\n        n_estimators=10000,\n        learning_rate=0.10,\n        num_leaves=30,\n        subsample=.9,\n        max_depth=7,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01,\n        min_child_weight=2,\n        silent=-1,\n        verbose=-1,\n        )\nmodel = clf.fit(X_train, y_train)\n\n# Predictions\npredictions = model.predict(X_test)\nprint (\"The Accuracy of the model is : \", accuracy_score(y_test, predictions), '\\n')\nprint (\"The confusion Matrix is \\n\")\nprint (confusion_matrix(y_test, predictions), '\\n')\n\nprint (classification_report(y_test, predictions))","4c8a086e":"# Feature Importance Plot using LGBM\nplt.rcParams['figure.figsize'] = [12, 9]\nsns.set(style = 'whitegrid', font_scale = 1.2)\nplot_importance(model);","90fb6a13":"# Training Model - II\n\nxgb_clf = XGBClassifier(n_estimator = 10000,\n                    learning_rate=0.10,\n                    num_leaves=30)\n\nmodel_xgb = xgb_clf.fit(X_train, y_train)\n\n# Predictions\npredictions_2 = model_xgb.predict(X_test)\nprint (\"The Accuracy of the model is : \", accuracy_score(y_test, predictions_2), '\\n')\nprint (\"The confusion Matrix is \\n\")\nprint (confusion_matrix(y_test, predictions_2), '\\n')\n\nprint (classification_report(y_test, predictions_2))","25c4c3ff":"# Feature Importance Plot using XGBClassifier\nfrom xgboost import plot_importance # plot_importance for xgboost\nplt.rcParams['figure.figsize'] = [12, 9]\nplot_importance(model_xgb);","d51abe84":"# Training Model - III\n\ncat_clf = CatBoostClassifier(iterations = 10000,\n                            learning_rate = 0.5);\n\nmodel_cat = cat_clf.fit(X_train, y_train);","c6f184cb":"# Predictions\npredictions_3 = model_cat.predict(X_test)\nprint (\"The Accuracy of the model is : \", accuracy_score(y_test, predictions_3), '\\n')\nprint (\"The confusion Matrix is \\n\")\nprint (confusion_matrix(y_test, predictions_3), '\\n')\n\nprint (classification_report(y_test, predictions_3))","d92291ca":"Model - III CatBoostClassifier","c9b2a3b3":"Model - I LightGBM","9fc977cd":"The Correlation coefficient between the sentiment of the uncleaned review with rating is greater than the cleaned review, so now lets try without the snowball stemmer and without removing the stopwords.","217bf569":"## Feature Engineering","dc7cdbb3":"#### Trigrams","a7b485fa":"#### n-grams","76c8ed22":"## Preprocessing","e644963f":"#### Unigrams","539998d7":"We will merge both the datasets and do the preprocessing and exploratory data analysis for the whole dataset and after that, we will split the dataset later for training and testing","3e09c814":"Model - II XGBClassifier","b203ebeb":"## Exploratory Data Analysis","3297c7bf":"## Modelling","e4ad5120":"#### Word count plot","3b624dcb":"#### Bigrams","af74b65b":"We are going to use the threshold rating of 5 for giving the sentiment, The review will have a positive sentiment (1) if rating > 5 and negative sentiment otherwise."}}