{"cell_type":{"8fca4a10":"code","ae62faed":"code","46860241":"code","dcc1df53":"code","beeb46a6":"code","acb44c8d":"code","2496ead0":"code","7c182d9f":"code","b6d293c7":"code","799d4731":"code","6dfcb3d5":"code","43d797de":"code","30d186cf":"code","de181467":"code","68e10f9c":"code","c8664213":"code","3abb320d":"markdown","79f403f3":"markdown","ac3b1123":"markdown"},"source":{"8fca4a10":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport datetime\nimport joblib\n\nfrom bayes_opt import BayesianOptimization\nimport lightgbm as lgb\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ae62faed":"##### Download of files.\n\nprint('Downloading datasets...')\nprint(' ')\ndf_train = pd.read_pickle('\/kaggle\/input\/3-fraud-detection-preprocessing\/train.pkl')\nprint('Train has been downloaded... (1\/2)')\ndf_test = pd.read_pickle('\/kaggle\/input\/3-fraud-detection-preprocessing\/test.pkl')\nprint('Test has been downloaded... (2\/2)')\nprint(' ')\nprint('All files are downloaded')","46860241":"##### Feature selection for the model\n# To prevent overfitting, we choose the elimination of some columns.\n\nfeatures = list(df_train)\nfeatures_to_remove = ['TransactionID','isFraud','TransactionDT','DT','uid1', 'uid2', 'uids', 'D1','D2','D4','D6','D10','D11','D12',\n                     'D13','D14','D15']\n\nfor col in features_to_remove:\n    features.remove(col)\n\n# Get the feature list\n\nprint(f'Columns selected: {features}')\nprint(f'Lenght: {len(features)}')","dcc1df53":"##### Train + val separation\n# We separate train (6 months) into train (4 months and 15 days) + skip (15 days) + val (1 month)\n\ntrain = df_train.query('DT <= 135').copy()\nval = df_train.query('DT >  150').copy()","beeb46a6":"##### Defining X and target\n\n# Train\nX_train = train[features]\ny_train = train.isFraud\nprint('X_train:', X_train.shape)\nprint('y_train:', y_train.shape)\n\n# Val\nX_val = val[features]\ny_val = val.isFraud\nprint('X_val:', X_val.shape)\nprint('y_val:', y_val.shape)\n\n# Test\nX_test = df_test[features]\nprint('X_test:', X_test.shape)","acb44c8d":"##### Bayesian Optimization\n# https:\/\/github.com\/fmfn\/BayesianOptimization\n\ndef objective(learning_rate,num_leaves,max_depth,colsample_bytree,subsample):\n    \n    # get X_train and X_val that are defined outside the functions that's why 'global' label\n    global X_train\n    global y_train\n    global X_val\n    global y_val\n    \n    # transform dtype of integers to avoid errors\n    num_leaves = int(num_leaves)\n    max_depth = int(max_depth)\n    \n    # check dtype if not: stop and execute an error\n    assert type(num_leaves) == int\n    assert type(max_depth) == int\n    \n    # create the params\n    parameters = {'objective': 'binary',\n             'boosting_type': 'gbdt',\n             'metric': 'auc',\n             'n_jobs': -1,\n             'tree_learner': 'serial',\n             'learning_rate': learning_rate,\n             'num_leaves': num_leaves,\n             'max_depth': max_depth,\n             'colsample_bytree': colsample_bytree,\n             'subsample': subsample,\n             'verbosity': -1,\n             'random_state': 27}\n    \n    # create the dataset\n    categorical_columns = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'M1', 'M3', 'M4', 'M5', 'M6', 'M8', 'M9']\n    \n    train_data = lgb.Dataset(X_train, label = y_train, categorical_feature = categorical_columns)\n    val_data = lgb.Dataset(X_val, label = y_val, categorical_feature = categorical_columns)\n    \n    # train the model\n    model_lgb = lgb.train(parameters, train_data , valid_sets = [train_data, val_data], num_boost_round = 200, categorical_feature = categorical_columns,\n                          early_stopping_rounds = 200, verbose_eval = 0)\n    \n    score  = roc_auc_score(y_val, model_lgb.predict(X_val))\n                          \n    return score","2496ead0":"params = {'learning_rate': (0.02,0.03),\n         'num_leaves': (225,275),\n         'max_depth': (12,16),\n         'colsample_bytree': (0.3,0.9),\n         'subsample': (0.3,0.9)}\n\nclf_BO = BayesianOptimization(objective, params, random_state = 27)\n\nclf_BO.maximize(init_points = 3, n_iter = 20)","7c182d9f":"print(f'Best Parameters: {clf_BO.max[\"params\"]}')\nprint(' ')\nprint('Best AUC:',{clf_BO.max['target']})","b6d293c7":"##### LightGBM with #3\n\nparameters = {'objective': 'binary',\n             'boosting_type': 'gbdt',\n             'metric': 'auc',\n             'n_jobs': -1,\n             'tree_learner': 'serial',\n             'learning_rate': 0.027832622205538907,\n             'num_leaves': 268,\n             'max_depth': 15,\n             'colsample_bytree': 0.3740990054742914,\n             'subsample': 0.589171882228726,\n             'verbosity': -1,\n             'random_state': 27}\n\ncategorical_columns = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'M1', 'M3', 'M4', 'M5', 'M6', 'M8', 'M9']\n\ntrain_data = lgb.Dataset(X_train, label = y_train, categorical_feature = categorical_columns)\nval_data   = lgb.Dataset(X_val,   label = y_val, categorical_feature = categorical_columns)\n\nclf = lgb.train(parameters, train_data , valid_sets = [train_data, val_data], num_boost_round = 200, categorical_feature = categorical_columns,\n                          early_stopping_rounds = 200, verbose_eval = 100)","799d4731":"##### AUC\n\nauc = roc_auc_score(y_val, clf.predict(X_val))\nprint('AUC:',auc)","6dfcb3d5":"##### ROC Curve\n\ny_pred = clf.predict(X_val)\nr_curve = roc_curve(y_val, y_pred)\n\nplt.figure(figsize=(8,8))\nplt.title('ROC')\nplt.plot(r_curve[0], r_curve[1], 'b', label = 'AUC = %0.2f' % auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","43d797de":"##### Feature Importance\n\nfeatures_importance = clf.feature_importance()\nfeatures_array = np.array(features)\nfeatures_array_ordered = features_array[(features_importance).argsort()[::-1]]\nfeatures_array_ordered\n\nplt.figure(figsize=(16,10))\nsns.barplot(y=features_array, x=features_importance, orient='h', order=features_array_ordered[:50])\n\nplt.show()","30d186cf":"##### Saving this results into an Excel file\n\ntry:\n    model_results = pd.read_excel('model_results.xlsx')\n    \n    model_results = model_results.append({'datetime' : datetime.datetime.now(), 'clf':clf.__class__, 'features': features_array, 'parameters' : clf.params,\n                                       'AUC': auc, 'features_importance': list(features_importance), 'features_ordered': list(features_array_ordered), \n                                       }, ignore_index = True)\n        \nexcept:\n    model_results = pd.DataFrame(columns = ['summary', 'datetime', 'clf', 'features', 'parameters', 'entry1', 'entry2', \n                                            'AUC', 'features_importance', 'features_ordered', 'output1', 'output2'])\n    \n    model_results = model_results.append({'datetime' : datetime.datetime.now(), 'clf':clf.__class__, 'features': features_array, 'parameters' : clf.params, \n                                       'AUC': auc, 'features_importance': list(features_importance), 'features_ordered': list(features_array_ordered),\n                                       }, ignore_index = True)\nfinally:\n    model_results.to_excel('model_results.xlsx', index= False)","de181467":"##### Saving this model\n\njoblib.dump(clf, 'model_lightgbm.pkl')","68e10f9c":"pred = clf.predict(X_test)\nsubmission = pd.DataFrame()\n\nsubmission['TransactionID'] = df_test['TransactionID']\nsubmission['isFraud'] = pred\n\nsubmission\nsubmission.to_csv('submission.csv', index = False)","c8664213":"val.to_pickle('val_lgb.pkl')\nprint('Done!')","3abb320d":"|     #     | learning_rate |    num_leaves   |    max_depth    |colsample_bytree |   subsample     |       AUC       |\n|:----------:|:--------------:|:----------------:|:----------------:|:----------------:|:----------------:|----------------:|\n| 1         |0.03215518473989215|             294 |             14 |             0.4 |             0.8 |0.9194457912401148|\n| 2         |            0.04 |             278 |             13 |             0.4 |             0.9 |0.9164233793906151|\n| 3         |0.027832622205538907|             268 |             15 |0.3740990054742914|0.589171882228726|0.9188929389573968|\n| 4         |0.029827755272285032|             262 |             15 |0.7823490596970961|0.8631490082348747|0.92719808846762|","79f403f3":"# Modelling with LightGBM","ac3b1123":"# Submission to Kaggle"}}