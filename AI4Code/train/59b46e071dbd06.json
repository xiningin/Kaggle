{"cell_type":{"d37bf357":"code","55a22824":"code","896d2074":"code","82e7f9e7":"code","505b7ca5":"code","4dcd6e5d":"code","d6fa4c88":"code","a9d2b0ce":"code","87fcbf09":"code","345f3bc0":"code","1d1e5d7a":"code","d525dcea":"code","250b672f":"code","a47dab12":"code","38ffcde4":"code","bbec952a":"code","9894d46e":"code","f30167ae":"code","065ddb10":"code","2446247d":"code","a96e88ba":"code","9bf6e07b":"code","04883928":"code","c1a42123":"code","954b753e":"code","f557f001":"code","0aa26798":"code","35c95b82":"code","94b3a553":"code","7f911c79":"code","1dd231f4":"code","9a873999":"code","a962ada7":"code","d6d78452":"code","799bcff9":"code","3318cf09":"code","d3c63dc6":"code","726f4ade":"code","f95c0a4d":"code","9208ffd6":"code","07dc9b95":"code","9837af2e":"code","4c4b6a17":"code","016ed3b2":"code","fe51097d":"code","fbe7322e":"code","36766d6d":"code","0a6b0cc3":"code","b32e2df6":"code","4d86cc9c":"code","e8e41dc4":"code","1e9cd69d":"code","5a03594c":"code","1a997b8c":"code","2ea8dbf7":"code","5755ab3f":"code","c6c148c7":"code","e7eb0380":"code","23e2a846":"code","41071624":"code","29283823":"code","b6f0a464":"code","ed1243c4":"code","392ed728":"code","e6c5f3e9":"code","a3df5dc2":"code","3dd23e79":"code","ffd1ed28":"code","6b1ac03e":"code","33bdc8de":"code","5add274a":"code","b20c9c58":"code","72049978":"code","ead4477c":"code","d09d5e61":"code","930927d0":"code","26aa73df":"code","fffb799f":"code","aa7ca7e3":"code","c539695d":"code","27c3bb3a":"code","e6adefb5":"code","1d71f6ec":"code","4a241195":"code","81c9f115":"code","0631e9c0":"code","41171771":"code","9e6ea4c5":"code","7b2001bc":"code","77896135":"code","38417bcd":"code","264fa73a":"code","f713a517":"code","6bec92b1":"code","7b5a587f":"code","e937e30e":"code","425e8ae4":"code","e366ebc5":"code","983f3986":"code","cf07a344":"code","da366980":"code","ace15793":"code","f9eb5d10":"code","56172bf1":"code","25171dd5":"code","ac910293":"code","a448a47d":"code","e476c715":"code","7586bdb7":"markdown","baa1bd9d":"markdown","76515414":"markdown","27cf3bb9":"markdown","540e4d2c":"markdown","b722cb71":"markdown","9463800c":"markdown","d4ebe98d":"markdown","e3aed6f6":"markdown","10d196ef":"markdown","34d32401":"markdown","54abbf41":"markdown","e5320d00":"markdown","d116513d":"markdown","7a2f6926":"markdown","8ffa16a0":"markdown","7e276fe6":"markdown","48cd2d3b":"markdown","57779a30":"markdown","66438ebe":"markdown","cdbe58da":"markdown","579a931d":"markdown","6fafa30d":"markdown","154c3145":"markdown","9e51884a":"markdown","669fe5e5":"markdown","7f0ad97f":"markdown","118bb6f9":"markdown","6ad19694":"markdown","d134b494":"markdown","75abb0ea":"markdown","eaea5511":"markdown","c608e517":"markdown","f55348cb":"markdown","e2b7141c":"markdown","7458877e":"markdown","6ca2cb3c":"markdown","0dfc296c":"markdown","b8562c1e":"markdown","3c7f5b29":"markdown","42bc3a71":"markdown","19230cc6":"markdown","974d0740":"markdown","16ad1e36":"markdown","ad4a23d8":"markdown","67ffcc9b":"markdown","29fa6a15":"markdown","3af8fa99":"markdown","c4672446":"markdown","aaa28f7e":"markdown","2d1413d6":"markdown","f028d6e3":"markdown","a1f1a563":"markdown","048d4143":"markdown","f83123a4":"markdown","68ac5b48":"markdown","f2429f85":"markdown","bfd318cb":"markdown","22d0abe4":"markdown","dd125f26":"markdown","e51a39f4":"markdown","c465e3f9":"markdown","06adebf4":"markdown","ecda7580":"markdown","61f19850":"markdown","fbd63eb3":"markdown","b37988a1":"markdown","c5e8fda5":"markdown","ae7b8451":"markdown","cc8dbbd3":"markdown","f62e70d6":"markdown","e4e5550e":"markdown","2cecef1e":"markdown","94c49e06":"markdown","9301abfb":"markdown","e0cf4583":"markdown","7f60a562":"markdown","7f15014b":"markdown","48dc5a84":"markdown","ca8a3016":"markdown","3123d84f":"markdown","515bacc7":"markdown","cec7eb51":"markdown","cc9d4545":"markdown"},"source":{"d37bf357":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split ,GridSearchCV ,RandomizedSearchCV","55a22824":"Data_train=pd.read_csv('..\/input\/titanic\/train.csv')\nData_test=pd.read_csv('..\/input\/titanic\/test.csv')\nData=pd.concat([Data_train,Data_test],sort=False)","896d2074":"Data_train.shape","82e7f9e7":"Data_test.shape","505b7ca5":"Data.shape","4dcd6e5d":"Data_train.columns.values","d6fa4c88":"Data[\"Sex\"].value_counts(normalize=False)*100","a9d2b0ce":"ax=sns.countplot(x=Data.Sex,data=Data)\nfor p, label in zip(ax.patches, Data[\"Sex\"].value_counts()):\n    ax.annotate(label, (p.get_x()+0.375, p.get_height()+0.15))\nplt.title(\"Males and Females in Titanic \")\n","87fcbf09":"Data[\"Survived\"].value_counts(normalize=False)","345f3bc0":"Data_train[\"Survived\"]=Data_train[\"Survived\"].astype(int)","1d1e5d7a":"ax=sns.countplot(x=Data_train.Survived,data=Data_train)\nfor p, label in zip(ax.patches, Data_train[\"Survived\"].value_counts()):\n    ax.annotate(label, (p.get_x()+0.375, p.get_height()+0.15))\nplt.title(\"Number of survivors in Titanic\")","d525dcea":"Data[\"Sex\"][Data[\"Survived\"]==1].value_counts(normalize=True)*100","250b672f":"ax=sns.countplot(x=Data_train.Survived,data=Data_train,hue=Data_train.Sex)\nplt.title(\"The proportion of females and males to the total number of survivors\")","a47dab12":"Data_train[\"Survived\"][Data_train[\"Sex\"]==\"female\"].value_counts(normalize=True)*100","38ffcde4":"Data_train[\"Survived\"][Data_train[\"Sex\"]==\"male\"].value_counts(normalize=True)*100","bbec952a":"ax=sns.countplot(x=Data_train.Sex,data=Data_train,hue=Data_train.Survived)\nplt.title(\"The proportion of survivors to the number of Females & Males\")","9894d46e":"Data_train[\"Pclass\"].value_counts(dropna=False,normalize=True)*100","f30167ae":"ax=sns.countplot(Data[\"Pclass\"]) ","065ddb10":"Classes=np.unique(Data_train[\"Pclass\"])","2446247d":"Data_train[\"Pclass\"][Data_train[\"Survived\"]==1].value_counts(normalize=True)*100","a96e88ba":"Data_train[(Data_train[\"Sex\"]==\"female\") & (Data_train[\"Survived\"]==1)][\"Pclass\"].value_counts(dropna=False,normalize=True)*100","9bf6e07b":"Data_train[(Data_train[\"Sex\"]==\"male\") & (Data_train[\"Survived\"]==1)][\"Pclass\"].value_counts(dropna=False,normalize=True)*100","04883928":"sns.catplot(x=\"Sex\", col=\"Pclass\",hue='Survived', col_wrap=3,data=Data_train[Data_train['Pclass'].notnull()],kind=\"count\", height=3, aspect=0.8)","c1a42123":"SibSp=np.unique(Data[\"SibSp\"])\nData[\"SibSp\"][Data[\"Survived\"]==1].value_counts(normalize=True)*100","954b753e":"Data[\"Parch\"].value_counts(normalize=True)*100","f557f001":"Data[\"Parch\"][Data[\"Survived\"]==1].value_counts(normalize=True)*100","0aa26798":"Data[\"Embarked\"].value_counts(normalize=True)*100","35c95b82":"Data[\"Embarked\"][Data[\"Survived\"]==1].value_counts(normalize=True)*100","94b3a553":"Embarked = sns.FacetGrid(Data_train, row='Embarked', size=2.2, aspect=1.6)\nEmbarked.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\nEmbarked.add_legend()","7f911c79":"Data.describe(include=\"all\")","1dd231f4":"Data_train.info()","9a873999":"Data_test.info()","a962ada7":"Data.isnull().sum().sort_values(ascending=False)","d6d78452":"PassengersID_test=Data_test[\"PassengerId\"]\nData_train=Data_train.drop(columns=[\"PassengerId\",\"Ticket\",\"Cabin\",\"Name\"])\nData_test=Data_test.drop(columns=[\"PassengerId\",\"Ticket\",\"Cabin\",\"Name\"])","799bcff9":"Data_train.shape","3318cf09":"Data_test.shape","d3c63dc6":"Data_train.info()","726f4ade":"Data_test.info()","f95c0a4d":"Data_train[\"SipSp_Parch\"]=Data_train[\"Parch\"]+Data_train[\"SibSp\"]\nData_test[\"SipSp_Parch\"]=Data_test[\"Parch\"]+Data_train[\"SibSp\"] ","9208ffd6":"SibSp_Parch_train= np.unique(Data_train[\"SipSp_Parch\"])\nSibSp_Parch_train","07dc9b95":"SibSp_Parch_test= np.unique(Data_test[\"SipSp_Parch\"])\nSibSp_Parch_test","9837af2e":"Data_test[\"SipSp_Parch\"]=Data_test[\"SipSp_Parch\"].astype(int)","4c4b6a17":"Data_train=Data_train.drop(columns=[\"SibSp\",\"Parch\"])\nData_test=Data_test.drop(columns=[\"SibSp\",\"Parch\"])","016ed3b2":"sns.catplot(x=\"Sex\", col=\"SipSp_Parch\",hue='Survived', col_wrap=3,data=Data_train[Data_train['SipSp_Parch'].notnull()],kind=\"count\", height=3, aspect=1)\n","fe51097d":"freq=Data[\"Embarked\"].dropna().mode()[0]\nfreq","fbe7322e":"Data_train[\"Embarked\"]=Data_train[\"Embarked\"].fillna(freq)","36766d6d":"for data in [Data_train,Data_test]:\n    data[\"Sex\"]=data[\"Sex\"].map({\"female\":1,\"male\":0})\n    data[\"Embarked\"]=data[\"Embarked\"].map({\"S\":0,\"C\":1,\"Q\":2}).astype(int)","0a6b0cc3":"Data_train[(Data_train[\"Sex\"]==0)&(Data_train[\"SipSp_Parch\"]==10)][\"Age\"].dropna()","b32e2df6":"grid = sns.FacetGrid(Data_train, row='SipSp_Parch', col='Sex', size=2.2, aspect=1.8)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","4d86cc9c":"Data_train[(Data_train[\"Sex\"]==0)&(Data_train[\"SipSp_Parch\"]==10)]","e8e41dc4":"ar=np.zeros((2,11))\nfor data in [Data_train,Data_test]:\n    for i in range(0,2):\n        for j in range(0,11):\n            predicted_Age=data[(data[\"Sex\"]==i) & (data[\"SipSp_Parch\"]==j)][\"Age\"].dropna()\n            predicted=predicted_Age.mean()\n            ar[i,j]=round(predicted,1)\n\n        arr=ar[np.logical_not(np.isnan(ar))] # to ignore the nan values in ar\n        print(np.mean(arr))\n\n        \n        for val in ar:\n\n            if np.isnan(np.nan):\n                ar[np.isnan(ar)]=round(np.mean(arr),1)\n\n\n    print(ar)          \n    for i in range(0,2):\n        for j in range(0,11):\n            data.loc[(data.Age.isnull()) & (data.Sex == i) & (data.SipSp_Parch == j),\"Age\"]=ar[i,j]\n            \n       \nfor data in [Data_train,Data_test]:\n    for i in range(0,2):\n        for j in SibSp_Parch_train:\n            data.loc[(data.Age.isnull()) & (data.Sex==i) & (data.SipSp_Parch==j),\"Age\"]=ar[i,j]\n\n#Fill the Fare feature in test data:\nData_test[\"Fare\"]=Data_test[\"Fare\"].fillna(np.mean(Data_test.Fare))\nData_test[\"Fare\"]=Data_test[\"Fare\"].astype(int)\nData_train[\"Fare\"]=Data_train[\"Fare\"].astype(int)","1e9cd69d":"Data_train[\"Age_band\"]=pd.cut(Data_train[\"Age\"],[0,16,32,48,64,80],labels=[\"0-16\",\"16-32\",\"32-48\",\"48-64\",\"64-80\"])\nData_test[\"Age_band\"]=pd.cut(Data_train[\"Age\"],[0,16,32,48,64,80],labels=[\"0-16\",\"16-32\",\"32-48\",\"48-64\",\"64-80\"])\nData_train[\"Age_band\"][Data_train[\"Survived\"]==1].value_counts(normalize=True)*100","5a03594c":"for i in [\"0-16\",\"16-32\",\"32-48\",\"48-64\",\"64-80\"]:\n    print(\"When ages between {}:\\n{}\".format(i,Data_train[\"Survived\"][Data_train[\"Age_band\"]==i].value_counts(normalize=True)*100))","1a997b8c":"sns.catplot(x=\"Survived\", col=\"Age_band\", col_wrap=3,data=Data_train,kind=\"count\", height=3, aspect=1)","2ea8dbf7":"Data[\"Age\"].describe()","5755ab3f":"for data in [Data_train,Data_test]:    \n    data.loc[ data['Age'] <= 16, 'Age'] = 0\n    data.loc[(data['Age'] > 16) & (data['Age'] <= 32), 'Age'] = 1\n    data.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age'] = 2\n    data.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age'] = 3\n    data.loc[(data['Age'] > 64) & (data['Age'] <= 80), 'Age'] = 4 ","c6c148c7":"for data in [Data_train,Data_test]:\n    data[\"Age\"]=data[\"Age\"].astype(int)","e7eb0380":"Data_train.columns","23e2a846":"Data_train=Data_train.drop(columns=[\"Age_band\"])\nData_test=Data_test.drop(columns=[\"Age_band\"])","41071624":"Data_train.info()","29283823":"Data_test.info()","b6f0a464":"Data_train.columns","ed1243c4":"features=['Pclass','Sex','Age','Fare','Embarked','SipSp_Parch']\nrows=2\ncols=3\nfig,axes = plt.subplots(rows,cols,figsize=(cols*4,rows*4))\nfor i in range (rows):\n    for j in range(cols):\n        r=i*cols+j\n        ax=axes[i][j]\n        sns.countplot(x=Data_train[features[r]],data=Data_train,hue=Data_train.Survived,ax=ax)","392ed728":"corrmat=Data_train.corr()\nk=6\ncols=corrmat.nlargest(k,\"Survived\")[\"Survived\"] \ncols","e6c5f3e9":"ytrain_validation=Data_train.pop(\"Survived\")\nxtrain_validation=Data_train\nx_test=Data_test","a3df5dc2":"xtrain_validation.shape","3dd23e79":"ytrain_validation.shape","ffd1ed28":"x_test.shape","6b1ac03e":"x_train,x_valid,y_train,y_valid=train_test_split(xtrain_validation,ytrain_validation,shuffle=False,test_size=0.2,random_state=2020)","33bdc8de":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\ncv_method=RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=999)\nparams_KNN={\"n_neighbors\":[3,5,6,7,10],\"p\":[1,2,5],\"weights\":[\"uniform\",\"distance\"]}\ngs_KNN=GridSearchCV(estimator=KNeighborsClassifier(),param_grid=params_KNN,verbose=1,scoring=\"accuracy\",\n                   cv=cv_method, return_train_score=True)\ngs_KNN.fit(x_train,y_train)","5add274a":"gs_KNN.best_params_","b20c9c58":"gs_KNN.best_score_","72049978":"model_KNN=KNeighborsClassifier(n_neighbors=10,p=1,weights=\"distance\")\nfit_KNN=model_KNN.fit(x_train,y_train)\ny_KNN_predict=fit_KNN.predict(x_valid)\nscore_KNN = fit_KNN.score(x_valid,y_valid)\n\"Score_KNN : {} %\".format(round(score_KNN *100,3))","ead4477c":"round(fit_KNN.score(x_train,y_train)*100,3)","d09d5e61":"from sklearn.svm import SVC\nparams_SVC={\"kernel\":[\"rbf\",\"linear\"],\"gamma\":[\"scale\",\"auto\"]}\ngs_SVC=GridSearchCV(estimator=SVC(),param_grid=params_SVC,verbose=1,scoring=\"accuracy\",\n                   cv=cv_method, return_train_score=True)\n\ngs_SVC.fit(x_train,y_train)","930927d0":"gs_SVC.best_params_","26aa73df":"gs_SVC.best_score_","fffb799f":"model_SVC=SVC(C=1,kernel=\"linear\",gamma=\"scale\")\nfit_SVC=model_SVC.fit(x_train,y_train)\ny_SVC_predict=fit_SVC.predict(x_valid)\nscore_SVC=fit_SVC.score(x_valid,y_valid)\n\"Score_SVC : {} %\".format(round(score_SVC *100,3))","aa7ca7e3":"from sklearn.linear_model import SGDClassifier\nparams_SGDClassifier={\"loss\":[\"hinge\",\"log\"]}\ngs_SGDClassifier=GridSearchCV(estimator=SGDClassifier(),param_grid=params_SGDClassifier,verbose=1,scoring=\"accuracy\",\n                   cv=cv_method, return_train_score=True)\n\ngs_SGDClassifier.fit(x_train,y_train)","c539695d":"gs_SGDClassifier.best_params_","27c3bb3a":"gs_SGDClassifier.best_score_","e6adefb5":"model_SGDClassifier=SGDClassifier(loss=\"log\",random_state=400)\nfit_SGDClassifier=model_SGDClassifier.fit(x_train,y_train)\ny_SGDClassifier_predict=model_SGDClassifier.predict(x_valid)\nscore_SGDClassifier=fit_SGDClassifier.score(x_valid,y_valid)\n\"Score_SGDClassifier : {} %\".format(round(score_SGDClassifier *100,3))","1d71f6ec":"from sklearn.tree import DecisionTreeClassifier\nimport altair as alt\ncv_method=RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=999)\nparams_DecisionTreeClassifier={\"min_impurity_decrease\":[0.0009,0.001,0.009],\"min_samples_split\":[2,3,4],\"max_features\":[4,5,6]}\ngs_DecisionTreeClassifier=GridSearchCV(estimator=DecisionTreeClassifier(),param_grid=params_DecisionTreeClassifier,verbose=1,scoring=\"accuracy\",\n                   cv=cv_method, return_train_score=True)\ngs_DecisionTreeClassifier.fit(x_train,y_train)","4a241195":"gs_DecisionTreeClassifier.best_params_","81c9f115":"gs_DecisionTreeClassifier.best_score_","0631e9c0":"results_DecisionTreeClassifier=pd.DataFrame(gs_DecisionTreeClassifier.cv_results_[\"params\"])\nresults_DecisionTreeClassifier[\"test_score\"]=gs_DecisionTreeClassifier.cv_results_[\"mean_test_score\"]\nalt.Chart(results_DecisionTreeClassifier,title=\"Decision Tree Performance Comparison\").mark_line(point=True).encode(\n        alt.X(\"max_features\",title=\"max_features\"),alt.Y(\"test_score\",title=\"Mean CV Score\",aggregate=\"average\",scale=alt.Scale(zero=False)))","41171771":"model_DecisionTreeClassifier=DecisionTreeClassifier(min_impurity_decrease=0.001,max_features=5,min_samples_split=3)\nfit_DecisionTreeClassifier=model_DecisionTreeClassifier.fit(x_train,y_train)\ny_DecisionTreeClassifier_predict=fit_DecisionTreeClassifier.predict(x_valid)\nscore_DecisionTreeClassifier = fit_DecisionTreeClassifier.score(x_valid,y_valid)\n\"Score_DecisionTreeClassifier : {} %\".format(round(score_DecisionTreeClassifier *100,3))","9e6ea4c5":"round(fit_DecisionTreeClassifier.score(x_train,y_train)*100,3)","7b2001bc":"from sklearn.ensemble import RandomForestClassifier\ncv_method=RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=999)\nn_estimators=[int(x) for x in np.linspace(start=200,stop=2000,num=10)]  # [200,400,600,800,1000,1200,1400,1600,1800,2000]\nmax_depth=[int(x) for x in np.linspace(10,100,11)]  #[10,20,30,40,50,60,70,80,90,100,None]\nmax_features=[\"auto\",\"sqrt\"]\nmin_samples_split=[2,5,7,10]\nbootstrap=[True,False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'bootstrap': bootstrap}","77896135":"rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\nrf_random.fit(x_train,y_train)","38417bcd":"rf_random.best_params_","264fa73a":"rf_random.best_score_","f713a517":"params_RandomForestClassifier={\n    'max_depth': [40,45],\n    'min_samples_split': [5,7],\n    'n_estimators': [500,600]}\ngs_RandomForestClassifier=GridSearchCV(estimator=RandomForestClassifier(),param_grid=params_RandomForestClassifier,verbose=1,scoring=\"accuracy\",\n                   cv=cv_method, return_train_score=True)\ngs_RandomForestClassifier.fit(x_train,y_train)","6bec92b1":"gs_RandomForestClassifier.best_params_","7b5a587f":"gs_RandomForestClassifier.best_score_","e937e30e":"model_RandomForestClassifier=RandomForestClassifier(n_estimators=500,max_features=\"sqrt\",min_samples_split=7,max_depth=45,bootstrap=True)\nfit_RandomForestClassifierr=model_RandomForestClassifier.fit(x_train,y_train)\ny_RandomForestClassifierr_predict=fit_RandomForestClassifierr.predict(x_valid)\nscore_RandomForestClassifierr = fit_RandomForestClassifierr.score(x_valid,y_valid)\n\"score_RandomForestClassifierr : {} %\".format(round(score_RandomForestClassifierr *100,3))","425e8ae4":"round(fit_RandomForestClassifierr.score(x_train,y_train)*100,3)","e366ebc5":"import tensorflow as tf\n\nmodel_keras_tf=tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n                                        tf.keras.layers.Dense(512,activation=tf.nn.relu),\n                                        tf.keras.layers.Dense(256,activation=tf.nn.relu),\n                                        tf.keras.layers.Dense(128,activation=tf.nn.sigmoid),\n                                        tf.keras.layers.Dense(64,activation=tf.nn.sigmoid),\n                                        tf.keras.layers.Dense(2,activation=tf.nn.sigmoid)])\nmodel_keras=model_keras_tf.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel_keras_tf.fit(x_train,y_train)\ntest_loss,score_keras=model_keras_tf.evaluate(x_valid,y_valid)\nscore_keras","983f3986":"from sklearn.linear_model import LogisticRegression\ncv_method=RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=999)","cf07a344":"penalty=[\"l1\",\"l2\"]\nC=[0.5,1]\nsolver=[\"liblinear\",\"lbfgs\",\"sag\"]\nmax_iter=[int(x) for x in np.linspace(start=10,stop=500,num=10)]\nmulti_class=[\"auto\", \"ovr\", \"multinomial\"]\n\nRS_params_LogisticRegression={\"penalty\":penalty,\"C\":C,\"solver\":solver,\"max_iter\":max_iter,\"multi_class\":multi_class}\nrf_random = RandomizedSearchCV(estimator = LogisticRegression(), param_distributions = RS_params_LogisticRegression, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\nrf_random.fit(x_train,y_train)","da366980":"rf_random.best_params_","ace15793":"rf_random.best_score_","f9eb5d10":"model_LogisticRegression=LogisticRegression(penalty=\"l2\",C=0.25,solver=\"lbfgs\",max_iter=100,multi_class=\"ovr\")\nfit_LogisticRegression=model_LogisticRegression.fit(x_train,y_train)\ny_LogisticRegression_predict=fit_LogisticRegression.predict(x_valid)\nscore_LogisticRegression= fit_LogisticRegression.score(x_valid,y_valid)\n\"score_LogisticRegression : {} %\".format(round(score_LogisticRegression *100,3))","56172bf1":"round(fit_LogisticRegression.score(x_train,y_train)*100,3)","25171dd5":"conclusion = pd.DataFrame({\"Algorithm\":[\"LR\",\"SVC\",\"KNN\",\"DT\",\"RF\",\"keras\",\"SGDC\"],\n                          \"The Accuracy\":[score_LogisticRegression,score_SVC,score_KNN,score_DecisionTreeClassifier,score_RandomForestClassifierr,score_keras,score_SGDClassifier]})\nresult=conclusion.sort_values(by=\"The Accuracy\",ascending=False)\nresult=result.set_index(\"The Accuracy\")\nresult ","ac910293":"from sklearn.ensemble import VotingClassifier\nmodel_voting=VotingClassifier(estimators=[(\"LR\",model_LogisticRegression),(\"RF\",model_RandomForestClassifier),(\"KNN\",model_KNN),\n                                          (\"DT\",model_DecisionTreeClassifier)])\n\nmodel_voting_fitting=model_voting.fit(x_train,y_train)\ny_voting_predict=model_voting_fitting.predict(x_valid)\nscore_voting= model_voting_fitting.score(x_valid,y_valid)\n\"score_LogisticRegression : {} %\".format(round(score_voting *100,3))","a448a47d":"round(model_voting_fitting.score(x_train,y_train)*100,3)","e476c715":"y_RandomForestClassifierr_predict_test=fit_RandomForestClassifierr.predict(x_test)\nsubmit=pd.DataFrame({\"PassengerId\":PassengersID_test,\"Survived\":y_RandomForestClassifierr_predict_test})\nsubmit.to_csv(\"prediction.csv\",index=False)","7586bdb7":"### Extract some features and information from (train and test) data in each column:","baa1bd9d":"> #### we can now use those best parameters in KNN algorithm:","76515414":"### More iterations will cover a wider search space and more cv folds reduces the chances of overfitting, but raising each will increase the run time.","27cf3bb9":"### On the other hand, the percentage of survivals who didn't traveled with parents and\/or children about 68%, and less than 20% of survivals were with only one parent and\/or child. ","540e4d2c":"### All the features of training set as a list:","b722cb71":"#### the number of total samples is 1309 so it's about 58 % of total passengers and crew of Titanic ship.\n#### 40% of them as training data and about 18% as a testing data.\n#### We have about 68% training data of passengers of Titanic ship and 32% for testing data from the passengers \n#### who were in Titanic.","9463800c":"### on the other hand, there are about 74% females from all females in Titanic ship are survived. ","d4ebe98d":"### By using the Correlation function , we find that Sex features has the largest impact in \"Survived\" label:","e3aed6f6":"### The percentage of suvivals according to each Class:\n* About 40% of Survivals were in Class_1.\n* And about 25% Survivals were in Class_2.\n* Only about 35% of Survivals were in Class_3.","10d196ef":"###  We should also check the score of training data to figure out if we have overfitting or not. And And as it is evident it has shown us good value.","34d32401":"#### 68% of survived passengers were females and 32 % were males.","54abbf41":"### Distributions of passengers in each class:\n* 24% in Class_1\n* 21% in Class_2\n* 55% in Class_3","e5320d00":"#### Number of survivals in the ship was 342 passengers from 891 passengers in Training set(about 38% of passengers).","d116513d":"> The percentage of survived males in each class:","7a2f6926":"### About 61% of survivals didn't have any siblings and\/or spouse , 33% had one siblings and\/or spouse, less than 4% had two , about 1% had three and less than 1% had 4 siblings and\/or spouse.","8ffa16a0":"### In RandomForestClassifier with a huge numbers of hyperparameters and many different values therefore, a huge number of process fits, i want to start using RandomSearch and then for more specific values we can use GridSearch.","7e276fe6":"## 5. RandomForestClassifier Algorithm:","48cd2d3b":"### Again we need Random Search to randomrly choose a wide range of the best parameters.","57779a30":"### Check dimensions of data now:","66438ebe":"### we should also check the score of training data to figure out if we have overfitting or not. And And as it is evident it has shown us good value.","cdbe58da":"### 1.2. Fill the Embarked feature:","579a931d":"#### 843 males and 466 females were on Titanic ship (About 64% of Passengers males,and 33% were females).","6fafa30d":"* ### In this task i will not use the maximum depth of the tree hyperparameter \"max_depth\" because in this task is less flexible compared to min_impurity_decrease .\n* ### The minimum number of samples required to split an internal node \"min_samples_split\"\n* ### The minimum number of samples required to be at a leaf node \"min_samples_leaf\".The number of features to consider when looking for the best split \"max_features\"\n* ### \"creterion\" : 'gini': to measure of impurity. 'entropy':measure of uncertainty or randomness (The more randomness a variable has, the higher the entropy is).","154c3145":"### The best parameters for best score by SGDClassifier Al:","9e51884a":"## 1. KNN Algorithm:","669fe5e5":"### from next three commands we found many things:\n1. Ticket column combine two types of data(Numerical, Alphanumeric) so the type is object.\n2. [PassengerId,Pclass,Age,SibSp,Parch,Fare,Survived,Fare] are integer or float.\n3. [Sex,Ticket,Cabin,Embarked] have object types.\n4. Age, Cabin, Embarked and Fare features contain a null values in training data.\n\n also, Age and Cabin features have a null values in testing data.","7f0ad97f":"### On the other hand, around 90% of the passengers who had ages between [64-80] died in the accident.","118bb6f9":"### We can plot the all the computations to see by plot the optimal parameters:","6ad19694":"### Hyperparameters:\n#### \"loss\": Loss function.","d134b494":"### Hyperparameters:","75abb0ea":"### We should also check the score of training data to figure out if we have overfitting or not. And And as it is evident it has shown us good value.","eaea5511":"## 3. SGDClassifier:","c608e517":"## 7. Logistic Regression Al:","f55348cb":"### The best parameters for best score by SVC AL:","e2b7141c":"Survived column in Data has 418 Null value because Data consists of Train and Test data.","7458877e":"### So at first, we will divide the training dataset into two parts (Train & Validation) data to fit the model many times an choose the appropriate algorithms algorithms. Then we will evaluate the model by using the test dataset to see the final accuracy.","6ca2cb3c":"## 4. Decision Tree Classifier:","0dfc296c":"## **Workflow steps**  :\n    1. upload the datasets as a training, validation and testing datasets.\n    \n    2. In this step, we will split the data into training and testing.\n    \n    3. Find the correlation between the features and survived column.\n    \n    4. Prepare the data by converting the text categorical values to numerical values. So, the machine learning algorithms and neural networks require all features to be converted to numerical values. \n    \n    5. Continue preparing dataset by estimate the missing values in features.So, the optimal \n    performace of algorithms require a no missing values.\n    \n    6. scale the datasets and correct and exclude the samples containing the errors by detecting the \n    outliers and remove them to avoid misleading.\n    \n    7. we may need to create a new features when we analyze and divide the categorical features.\n    \n    8. use the right vesualization plots and charts to check the correlation and comparison\n    between categories.\n    \n    9. trying many algorithms to fit the dataset and choose the best algorithm.\n    \n    10. Evaluate the model.","b8562c1e":"### On the other hand, When SipSp_Parch feature =10 in training data, all the corresponding values in Age features are NaN.\n\n\n### So, we can replace the nan with mean of row (the same gender).","3c7f5b29":"### Finally we should evaluate the model by test it with testing data:\n### I will use the best Algorithm i got in process \"RandomForestClassifier\":","42bc3a71":"### We can see the best parameters and the best score by GridSearchCV:","19230cc6":"### we can now use those best parameters in DT algorithm:","974d0740":"## Some statistics in the trip:","16ad1e36":"### In dividing the data we should also see the min and max values in data:","ad4a23d8":"## 2. SVC (Support Vector Classification):","67ffcc9b":"### Convert the string values to integers (Encoding) in {Sex,Embarked} columns:","29fa6a15":"### The model is ready to train and predict the output. The task is under supervised ML and it is performing a classification task to predict the survivors in Titanic.","3af8fa99":"### We should also check the score of training data to figure out if we have overfitting or not. And And as it is evident it has shown us good value.","c4672446":"### we will use GridSerachCV for spicific values of hyperparameters:","aaa28f7e":"> ### The percentage of survivved females in each class:","2d1413d6":"## 1. Missing Data:\n### 1.1. Fill the missing data in both training and testing data and convert the data to integer:\n### Still have missing data in [Age, Emarked] features (Train data), [Age, Fare] features (Test data).\n\n### Let's check them again:","f028d6e3":"## 6. Keras:","a1f1a563":"### Now we can evaluate all the results to choose the best algorithm for our task:","048d4143":"### Now we can remove the Parch and SibSp features from train and test data:","f83123a4":"### Convert the Age feature in train and test data into integer:","68ac5b48":"### The age of passengers are related to many other features like the number of children, parrents, siblings and also the gender. so, what we will do in this section is combining the featuers (SibSp and Parch) in one columns)","f2429f85":"### We can summarize the final data with the label \"Survived\":","bfd318cb":"### Upload the data and combine train and test data to use it in Preprocessing data:","22d0abe4":"### Embarked takes three possible values. 'S' port used by most passengers:","dd125f26":"### Hyperparameters:","e51a39f4":"### We don't need \"Age_band\" feature in data anymore so we can remove it from both {train,test} data.","c465e3f9":"### In this section we will deal with Age feature , so we will divide the Age feature into 3 bands: [0-16],[16-32],[32-48],[48-64],[64-80].","06adebf4":"### We have cleaned and full data in all the features.\n### Let's Check it :","ecda7580":"### And it is clear that the percentage of survivals who their ages were between 16-32 was the highest with 44%.","61f19850":"### **In this script we will use Random and Grid search techniques to tune the Hyper parameters of the algorithms of ML.**","fbd63eb3":"## Voting method:","b37988a1":"### More than 75% of passengers traveled without parents and\/or children, and about 13% of them were with only one parent and\/or child .","c5e8fda5":"### In this sectio and after checking the optimum algorithms for our task. I will use those algorithms to work together and vote after fitting to gave us the final prediction\n> ### \"Even if this is will not help to evalate the performance and will not gave us more advantage but let's learn it together\":","ae7b8451":"> #### we should at the begining figure out the most frequent in all Data:","cc8dbbd3":"### But in fact, This number does not reflect reality where 55% of passengers with ages[0-16] survived and 40% for passengers[32-48], while only about 37% of passengers[16-32] survived and that is because the almost all passengers were young passengers.","f62e70d6":"### We can show the data by :","e4e5550e":"### By using the unique function, we found that the range of SibSp_Parch feature is [0-10].","2cecef1e":"* ### I will focus in hyperparameters of SVM algorithm about 'C' which represents misclassification or error term and how you can control the trade-off between decision boundary and misclassification term. \n* ### \"Kernel\" to transform the given dataset input data into the required form.\n* ### \"Gamma\"  whereas a higher value of gamma will exactly fit the training dataset, which causes over-fitting.","94c49e06":"## Hyperparameters:\n* ### \"n_estimators\": the number of trees in the forest. \"max_features\":the number of features considered for splitting at each leaf node.\n* ### \"min_samples_split\": min number of data points placed in a node before the node is split.\n* ### \"max_depth\" : max number of levels in each decision tree.\n* ### \"bootstrap\": methods for sampling data point (with or without replacement).","9301abfb":"#### We should also check the score of training data to figure out if we have overfitting or not. And And as it is evident it has shown us good value.","e0cf4583":"### The most important arguments in RandomizedSearchCV are n_iter, which controls the number of different combinations to try,and cv which is the number of folds to use for cross validation (we use 100 and 3 respectively)","7f60a562":"### Now we can determine {xtrain,ytest,xtest}:\n#### for sure we should use a validation data and at the end we will test the model with data the model has not seen before.","7f15014b":"### We can see the best parameters and the best score by GridSearchCV:","48dc5a84":"## Correlations between features:","ca8a3016":"#### We can see the best parameters and the best score by GridSearchCV:","3123d84f":"### So, we can see that for out task and according to the chosen hyperparameters: RandomForestClassifier, DecisionTree, LogisticRegression are the best.","515bacc7":"\n### **<p style=\"text-align:center;\"> This task is a normal Titanic Competition exluding the names of passengers feature. <\/p>**\n  \n Even though there are so much solutions for this dataset entries but i want to put some of my \nideas supported with the required explanations to solve competition like this.","cec7eb51":"### If we start studying the corellation between the features and survived column, we should realize many things:\n1. Cabin features has a huge number of null values.\n2. Ticket feature describe the number of ticket for each passenger and that is will not contribute directly to survival like PassengerID feature.\n3. Name features is a personal info so it will not help to predict the survivals.\n\n#### From all before we can see that we can drop a group of features which may not have a corellation with Survuved label.","cc9d4545":"## Task definition:\nAll the information about the competition can be found in \"Kaggle competition description page [here](https:\/\/www.kaggle.com\/c\/titanic).\n#### But as a summary:\nOn April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg,\n killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough \nlifeboats for the passengers and crew.\n\nAlthough there was some element of luck involved in surviving the sinking,\n some groups of people were more likely to survive than others, such as women, children,\n and the upper-class.\n\nIn this Kernel , you are going to find a solution for normal Titanik kaggle competition where the survived column is a label."}}