{"cell_type":{"3d249e8c":"code","647a0865":"code","fa73a99d":"code","ec7dc0c9":"code","37a35ab4":"code","74d2a982":"code","21afc211":"code","25af8cc8":"code","a7540b69":"code","92f880ee":"code","0757ee33":"markdown"},"source":{"3d249e8c":"from __future__ import division\nfrom __future__ import print_function\n\nPREF = '016a05'","647a0865":"import os\nfrom os import listdir\nfrom os.path import isfile, join\nimport time\nimport warnings\nimport traceback\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport scipy.signal as sg\nimport multiprocessing as mp\nfrom scipy.signal import hann\nfrom scipy.signal import hilbert\nfrom scipy.signal import convolve\nimport scipy\nimport multiprocessing\nimport numba\nimport copy\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.base import TransformerMixin\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.base import BaseEstimator\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\nfrom keras.models import Sequential, Model\nfrom keras.models import model_from_json\nfrom keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization, GaussianNoise\nfrom keras import callbacks\nimport keras.backend as K\nimport tensorflow as tf\nfrom keras.layers import * \nfrom keras.optimizers import Adam\nfrom keras.utils import Sequence\nfrom keras.models import load_model\n\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n","fa73a99d":"OUTPUT_DIR = '.\/'  \nDATA_DIR = '..\/input\/train'  \nINPUT_DATA_PATH = '..\/input\/'\n\nSCALE = 32\nSIG_LEN = 4096\nBATCH_SIZE = 20\nNUM_THREADS = 2\nSEG_LENGHT_HAT = 1\nY_SCALE = 16\n\n\n# To reduce load\nREDUCED = True\nMAX_START_IDX = 300000000\n\nrandom_state=21","ec7dc0c9":"if not os.path.exists('{}\/{}__start_indices.npy'.format(OUTPUT_DIR,PREF)) or True:\n    START_INDICES_SIZE = int(629e6 \/ 4096 \/ 4*10*1.25)\n    limits = [5656574, 50085878, 104677356, 138772453, 187641820, 218652630, 245829585, 307838917, 338276287, 375377848, 419368880, 461811623, 495800225, 528777115, 585568144, 621985673]\n    rnd_idxs = np.zeros(shape=(START_INDICES_SIZE,), dtype=np.int32)\n\n    if REDUCED:\n        max_start_idx = MAX_START_IDX - 150000\n    else:\n        max_start_idx = 629000000 # From train.csv shape[0]-150000\n    \n    np.random.seed(random_state)\n    max_start_binned = int(max_start_idx\/(4096*SEG_LENGHT_HAT))\n\n    rnd_idxs = np.random.randint(0, max_start_binned, size=START_INDICES_SIZE, dtype=np.int32)\n    rnd_idxs *= 4096 * SEG_LENGHT_HAT\n    rnd_idxs = np.unique(rnd_idxs) # Borro los duplicados\n    for limit in limits: # Borro los que estan cerca de la trnasicion.\n        rnd_idxs = rnd_idxs[np.logical_not(np.logical_and(rnd_idxs>(limit-200000), rnd_idxs<(limit+200000)))]\n\n    print('-'*64)\n    print('Created:{}, from {} (limit: {}),  min:{}, and max:{}'.format(rnd_idxs.shape[0], START_INDICES_SIZE, max_start_idx, np.min(rnd_idxs), np.max(rnd_idxs) ))\n\n    rnd_idxs = np.asarray(rnd_idxs)\n    for _ in range(50):\n        np.random.shuffle(rnd_idxs)\n    print(rnd_idxs[:8])\n    print(rnd_idxs[-8:])\n\n    np.save('{}{}__start_indices.npy'.format(OUTPUT_DIR,PREF), rnd_idxs)\n    print('{}{}__start_indices.npy saved!'.format(OUTPUT_DIR,PREF))\n    print('-'*64)\n    \n    start_indices = rnd_idxs\nelse:\n    start_indices = np.load('{}{}__start_indices.npy'.format(OUTPUT_DIR,PREF))\n\n\ngtr_idxs = start_indices[:28000]\ngts_idxs = start_indices[28000:40000]\n\nif REDUCED:\n    train_df = pd.read_csv(os.path.join(INPUT_DATA_PATH, 'train.csv'), \n                                dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32}, nrows=MAX_START_IDX)\nelse:\n    train_df = pd.read_csv(os.path.join(INPUT_DATA_PATH, 'train.csv'), \n                                dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32}, nrows=MAX_START_IDX)\n","37a35ab4":"import pywt\n\ndef create_features(seg, X_hat):\n    global SIG_LEN, SCALE\n    scales = range(1,SCALE+1)\n    waveletname = 'mexh'\n    \n    X = pywt.cwt(seg['acoustic_data'].values[2:-2], np.asarray(scales), waveletname)[0].reshape(1,SCALE,SIG_LEN-4)\n    \n    X -= np.min(X, axis=1)\n    X \/= np.max(X,axis=1)\n    \n    if X_hat is None:\n        X_hat = X\n    else:\n        X_hat = np.concatenate([X_hat, X], axis=0)\n\n    \n    return X_hat","74d2a982":"class DataGen(Sequence):\n\n    def __init__(self, X=None, batch_size=12, PREF=PREF,OUTPUT_DIR=OUTPUT_DIR, DATA_DIR='..\/input\/',\n                 start_indices=[2,4,8], load_raw_data=True):\n        self.start_indices=start_indices\n        self.X = X\n        self.PREF = PREF\n        self.OUTPUT_DIR = OUTPUT_DIR\n        self.DATA_DIR = DATA_DIR\n        self.batch_size=batch_size\n        self.load_raw_data = load_raw_data\n        self.train_df = None\n            \n        if self.load_raw_data:\n            print('Loading train.csv ...')\n            self.train_df = pd.read_csv(os.path.join(self.DATA_DIR, 'train.csv'), \n                                dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32}) #, nrows=100000)\n        return\n\n    def __len__(self):\n        return int(np.floor(1.0 * len(self.start_indices)\/self.batch_size))\n\n    def get_raw_data(self):\n        \n        return self.train_df.copy()\n\n    def set_raw_data(self, train_df):\n        self.train_df = train_df\n        return\n    \n    def __getitem__(self, idx):\n        p_id, seg_st = 0, 0\n\n        return self.process(p_id, idx,seg_st)\n    \n\n    def generate(self):\n        idx = 0\n        while True:\n\n            if idx ==  self.__len__():\n                idx = 0\n            \n            yield(self.__getitem__(idx))\n            idx += 1\n\n        return\n    \n    def process(self, p_id=0, idx=[0], seg_st=0): # idx = batched pos.\n        try:\n            start_indices = self.start_indices[int(self.batch_size * idx):int(self.batch_size * (idx+1))]\n            seg_st += self.batch_size * idx\n\n            X1, X2 = None, None\n            train_y = pd.DataFrame(dtype=np.float64, columns=['time_to_failure'])\n\n            for seg_id, start_idx in zip(range(int(seg_st), int(seg_st + self.batch_size)), start_indices):\n                end_idx = np.int32(start_idx + SIG_LEN)\n                seg = self.train_df.iloc[start_idx: end_idx,:].copy()\n                X1 = create_features(seg, X1)\n                train_y.loc[seg_id, 'time_to_failure'] = seg['time_to_failure'].values[-1] \/ Y_SCALE\n\n            return X1.reshape(-1,X1.shape[1],X1.shape[2],1), train_y\n        except:\n            print('Error working with: %d, %d, %d to %d of 629e6' % (p_id, 0, start_idx, end_idx))\n            print(traceback.format_exc())\n            success = 0\n            print('Error working with: %d, %d, %d to %d of 629e6' % (p_id, 0, start_idx, end_idx))\n        \n        return\n","21afc211":"Gtr = DataGen(start_indices=gtr_idxs, batch_size=BATCH_SIZE, load_raw_data=False ) \nGtr.set_raw_data(train_df)\n\nGval = DataGen(start_indices=gts_idxs, batch_size=BATCH_SIZE, load_raw_data=False ) \nGval.set_raw_data(train_df)\n","25af8cc8":"# Capsule definition\ndef squash(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    scale = K.sqrt(s_squared_norm) \/ (0.5 + s_squared_norm)\n    return scale * x\n\n\n# define our own softmax function instead of K.softmax\n# because K.softmax can not specify axis.\ndef softmax(x, axis=-1):\n    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n    return ex \/ K.sum(ex, axis=axis, keepdims=True)\n\n\n# define the margin loss like hinge loss\ndef margin_loss(y_true, y_pred):\n    lamb, margin = 0.5, 0.1\n    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n\n\nclass Capsule(Layer):\n    def __init__(self,**kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = 12\n        self.dim_capsule = 16\n        self.routings = 3\n        self.share_weights = True\n        self.activation = squash\n\n\n    def build(self, input_shape):\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.kernel = self.add_weight(\n                name='capsule_kernel',\n                shape=(1, input_dim_capsule,\n                       self.num_capsule * self.dim_capsule),\n                initializer='glorot_uniform',\n                trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.kernel = self.add_weight(\n                name='capsule_kernel',\n                shape=(input_num_capsule, input_dim_capsule,\n                       self.num_capsule * self.dim_capsule),\n                initializer='glorot_uniform',\n                trainable=True)\n\n    def call(self, inputs):\n        if self.share_weights:\n            hat_inputs = K.conv1d(inputs, self.kernel)\n        else:\n            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n\n        batch_size = K.shape(inputs)[0]\n        input_num_capsule = K.shape(inputs)[1]\n        hat_inputs = K.reshape(hat_inputs,\n                               (batch_size, input_num_capsule,\n                                self.num_capsule, self.dim_capsule))\n        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n\n        b = K.zeros_like(hat_inputs[:, :, :, 0])\n        for i in range(self.routings):\n            c = softmax(b, 1)\n            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n            if i < self.routings - 1:\n                b = K.batch_dot(o, hat_inputs, [2, 3])\n                if K.backend() == 'theano':\n                    o = K.sum(o, axis=1)\n\n        return o\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","a7540b69":"# MODEL DEF\ndef _Model(shape):\n    units=16\n    guess_dim=True\n    if guess_dim == True:\n        n = SCALE\n        dma = 50000\n    inp = Input(shape=(n,dma,))\n\n    main_input = Input(shape=(shape[1], shape[2], shape[3]), name='main_input') \n    x = Conv2D(72, (3, 3), activation='relu', name='Conv_0')(main_input)\n    x = Conv2D(32, (3, 3), activation='relu', name='Conv_1')(x)\n    x = Reshape((-1, 128))(x)\n    \n    x = Capsule()(x)\n    x = Flatten()(x)\n\n    x = Dense(units=128)(x)\n    x = Dense(units=128)(x)\n    x = Dropout(0.2)(x)    \n    # The output layer\n    preds = Dense(units=1, activation='linear')(x)\n    model = Model(inputs=main_input, outputs=preds)\n\n    model.summary()\n\n    # Compile and fit model\n    model.compile(optimizer=Adam(0.001),loss='mae', metrics=['mean_squared_error']) \n    \n    return model\n\nmodel = _Model([BATCH_SIZE,SCALE,SIG_LEN-4,1])","92f880ee":"print('training ...')\nmodel.fit_generator(Gtr, epochs=1, verbose=1, steps_per_epoch=2, #len(Gtr), \n            validation_steps=None,\n            max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)","0757ee33":"**Each bin of 4096 samples @ 4Mhz contains enough information to makes satisfactory predictions or obtains features for a metamodel.**\n\n**Dataset**: wavelet transform coefficients of the bin. (mexh & morl works well enough).\n\n**NN Model**: CapsuNet implemented in keras \/ tensorflow."}}