{"cell_type":{"5639718a":"code","515a305e":"code","29b32d48":"code","0ffebbf3":"code","24a3bd15":"code","d083850f":"code","2ebf9e4a":"code","59396eb7":"code","ec051b22":"code","f28690f4":"code","258feee1":"code","d3c87617":"code","145c457f":"code","af4d7e91":"code","d1e02108":"markdown","8cc43263":"markdown","eb00505b":"markdown","09f506e7":"markdown","bd052cbb":"markdown","c70a9bf4":"markdown","4c29fbb9":"markdown","6e17722e":"markdown","8e73570d":"markdown","dca92789":"markdown","843b39b3":"markdown","31ed6f0b":"markdown","fed0a448":"markdown"},"source":{"5639718a":"pip install hyperopt","515a305e":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\nfrom sklearn.model_selection import train_test_split\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classifier\/Regressor\nfrom xgboost import XGBRegressor, DMatrix\nfrom lightgbm import LGBMRegressor\n# Model selection\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, DeltaYStopper\nfrom skopt.space import Real, Categorical, Integer\n\n# Data processing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport pandas as pd\nimport numpy as np\n\n# Unfortunately I'm still using matplotlib for graphs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom hyperopt import hp","29b32d48":"# Loading data \ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","0ffebbf3":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","24a3bd15":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()\n\n","d083850f":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n","2ebf9e4a":"space={'learning_rate': hp.lognormal(\"learning_rate\",1e-4, 1e-1),\n        'max_depth': hp.choice('max_depth', np.arange(3, 20, dtype=int)),\n        'reg_alpha' : hp.uniform('reg_alpha', 1e-6, 100),\n        'reg_lambda' : hp.uniform('reg_lambda', 1e-6, 100),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.choice('min_child_weight', np.arange(0,10, dtype=int)),\n        'n_estimators':  hp.choice('n_estimators', np.arange(500,20000, dtype=int)),\n        \n    }","59396eb7":"from hyperopt import STATUS_OK\nfrom sklearn.model_selection import KFold\n\n\ndef hyperparameter_tuning(space):\n    \n    model= XGBRegressor(learning_rate= (space['learning_rate']), n_estimators =int(space['n_estimators']), max_depth = int(space['max_depth']), \n                         reg_alpha = (space['reg_alpha']), min_child_weight=int(space['min_child_weight']),\n                         colsample_bytree=space['colsample_bytree'], reg_lambda=space['reg_lambda'], tree_method = 'gpu_hist')\n    \n    evaluation = [( X_train, y_train), ( X_valid, y_valid)]\n    \n    model.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"rmse\",\n            early_stopping_rounds=10,verbose=False)\n\n    pred = model.predict(X_valid)\n    mse= mean_squared_error(y_valid, pred)\n    print (\"SCORE:\", mse)\n    #change the metric if you like\n    return {'loss':mse,'status':STATUS_OK}","ec051b22":"from hyperopt import tpe\n\n# Create the algorithms\ntpe_algo = tpe.suggest","f28690f4":"from hyperopt import Trials\n\n# Create two trials objects\ntpe_trials = Trials()","258feee1":"from hyperopt import fmin\nbest = fmin(fn=hyperparameter_tuning,\n            space=space,\n            algo=tpe_algo,\n            max_evals=10,\n            trials=tpe_trials, rstate= np.random.RandomState(50))\n\nprint (best)","d3c87617":"kf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\noof_preds = np.zeros((X.shape[0],))\nfinal_predictions = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(kf.split(X)):\n    X_train, X_valid = X.loc[train_id], X.loc[valid_id]\n    y_train, y_valid = y.loc[train_id], y.loc[valid_id]\n    \n    model = XGBRegressor(**best,tree_method = 'gpu_hist')\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 50)\n    \n    #Mean of the predictions\n    final_predictions += model.predict(X_test).ravel() # Splits\n    \n    \n\n    #Out of Fold predictions\n    oof_preds[valid_id] = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_id]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ 10\n    \nfinal_predictions \/= 10    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","145c457f":"# Preparing the submission\nsubmission = pd.DataFrame({'id':X_test.id, \n                           'target': final_predictions})\n\nsubmission.to_csv(\"submission.csv\", index = False)","af4d7e91":"submission","d1e02108":"### Step3: Hyperparameter optimization Algorithm - TPE","8cc43263":"# Train with best parameters and predict on test data","eb00505b":"# 1. Install HyperOpt","09f506e7":"# Prepare data","bd052cbb":"### Step5: Run Hyperopt Function for optimization","c70a9bf4":"### This notebook mainly focuses on bayesian optimization using Tree Parzen Estimator or TPE for XGBoost model. 'HyperOpt' library is specially designed for that.\n\nThis notebook demonstrates following steps: \n1. Install Hyperopt\n2. Encode categorical data using Ordinal encoding\n3. Setup for optimization:\n    a. Domain space of hyper-parameters\n    b. Objective function\n    c. TPE Algorithm\n    d. History\n4. Find best hyperparameters for XGBoost by running optimization\n5. Train XGBoost with best hyperparameters with KFold cross validation\n6. Predict on test data\n7. Submit","4c29fbb9":"### Step4: History","6e17722e":"### Step1: Domain space","8e73570d":"As first steps:\n\nwe load the train and test data \nwe separate the target from the training data\nwe separate the ids from the data\nwe convert integer variables to categories (thus our machine learning algorithm can pick them as categorical variables and not standard numeric one)\n\nYou can add further processing, for instance by feature engineering, in order to succeed in this competition","dca92789":"Once, we get optimal hyperparameters from TPE algo, it's time to train XGBoost model with kFolds.","843b39b3":"# Setting up optimization","31ed6f0b":"# Submit notebook","fed0a448":"### Step2: Objective Function"}}