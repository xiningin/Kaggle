{"cell_type":{"5ff4e2dd":"code","2eaa0d13":"code","ddfbea3e":"code","532429f5":"code","b188f86c":"code","e2989715":"code","d1c0fc4d":"code","e553f9f9":"code","6e11e820":"code","cc0f5066":"code","377f1bdf":"code","5748f129":"code","a35ff301":"code","ba021bed":"code","00c7523d":"code","ae3a4bb7":"code","e01eab37":"code","387310c7":"code","497534e0":"code","d11c0de3":"code","69b894e3":"code","121e3d84":"code","685c0822":"code","0beeba12":"code","9d911d38":"code","ba98c08c":"code","69852968":"code","9a4e8128":"code","d3309868":"code","df0c40f8":"code","86584883":"code","2e16744b":"code","3cd26fbf":"code","3122a3bf":"code","32ca39cd":"code","722088d2":"code","3d3c1de4":"code","d1bb756d":"code","8144cd7b":"code","c32d9c88":"code","f1e3bef6":"code","73ea57cf":"code","f54f8994":"code","187a8034":"code","9625d4db":"code","86962610":"code","a4b9ff88":"code","2047d77f":"code","e73ce4af":"code","75b63df4":"code","812226fc":"code","af0456f7":"code","fd5b5999":"code","80a602cd":"code","07ba8c1b":"code","fb71c589":"code","60c3439e":"code","80b74538":"code","0cb82d07":"code","b88bdf69":"code","9dc96b0b":"code","fe0ff2c6":"code","5d09f56b":"code","90dfb480":"code","2ddce64e":"code","16c260f1":"code","b4b2dc74":"code","4e8ace3b":"code","14e8e431":"code","ad28201e":"code","f82236a5":"code","758a841e":"code","30243f7f":"code","4203d7fc":"code","df01b9fc":"code","4d2f431a":"code","8bc6a70a":"code","3c9f0f0e":"code","ea58e401":"code","acb78dd0":"code","ebd65efe":"code","2d0a2aae":"code","34bae155":"markdown","8427ad69":"markdown","dd720e4e":"markdown","f23ac1d0":"markdown","0a57b8a1":"markdown","121fc0f7":"markdown","89f77c2b":"markdown","b3cce811":"markdown","aa754277":"markdown","821f995e":"markdown","b7813140":"markdown","fb1af210":"markdown","50e98417":"markdown","fb25558e":"markdown","78f3fc9d":"markdown","05db4aba":"markdown","0625cd95":"markdown","8f27774f":"markdown","42714795":"markdown"},"source":{"5ff4e2dd":"import matplotlib.pyplot as plt\n\nimport torch\nfrom torchvision import datasets, transforms\nimport pandas as pd\nimport os\nfrom PIL import Image\nimport shutil\nimport torchvision\nimport numpy as np\nimport os.path\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport random","2eaa0d13":"#define the transformations\ntransform_ship = transforms.Compose([transforms.ToTensor()])","ddfbea3e":"SEED = 200\nbase_dir = '..\/input\/'\n\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    os.environ['PYHTONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(SEED)","532429f5":"#read the train csv\n\ntrain_data = pd.read_csv(base_dir+'\/train\/train.csv')","b188f86c":"train_data.head()","e2989715":"### map imge names to labels\nmap_img_class_dict = {k:v for k, v in zip(train_data.image, train_data.category)}","d1c0fc4d":"#read the test csv\n\ntest_data = pd.read_csv(base_dir+'\/test_ApKoW4T.csv')\ntest_data.head()","e553f9f9":"from PIL import Image\n\nclass ShipDataLoader(torch.utils.data.DataLoader):\n    def __init__(self, CSVfolder, process='train', transform = transforms.Compose([transforms.Resize(size=(224, 224)),transforms.ToTensor()]), imgFolder='..\/input\/train\/images\/',labelsDict = {}, y_labels = list(train_data.category)):\n        \n        self.process = process\n        self.imgFolder = imgFolder\n        self.CSVfolder = CSVfolder\n        self.y = y_labels\n        self.FileList = pd.read_csv(self.CSVfolder)['image'].tolist()\n        self.transform = transform\n        self.labelsDict = labelsDict\n        \n        if self.process =='train':\n            self.labels = [labelsDict[i] for i in self.FileList]\n        else:\n            self.labels = [0 for i in range(len(self.FileList))]\n\n    def __len__(self):\n        return len(self.FileList)\n    \n    def __getitem__(self,idx):\n        file_name =  self.FileList[idx]\n        image_data=self.pil_loader(self.imgFolder+\"\/\"+file_name)\n        \n        if self.transform:\n            image_data = self.transform(image_data)\n        \n        if self.process == 'train':\n            label = self.y[idx]\n        else:\n            label = file_name\n            \n        return image_data, label\n    \n    def pil_loader(self,path):\n        with open(path, 'rb') as f:\n            img = Image.open(f)\n            return img.convert('RGB')","6e11e820":"#define the batchsize\ntraining_batchsize = 5","cc0f5066":"#retrieve the full data\nfull_data = ShipDataLoader(base_dir+'\/train\/train.csv',process = \"train\", imgFolder = base_dir+\"\/train\/images\", labelsDict = map_img_class_dict)","377f1bdf":"#create a dataloader\ntrainfull_loader = torch.utils.data.DataLoader(full_data, batch_size=training_batchsize, shuffle=True)","5748f129":"# dictionary ship encoding \nship = {1: 'Cargo', \n        2: 'Military', \n        3: 'Carrier', \n        4: 'Cruise', \n        5: 'Tankers'}","a35ff301":"#custom function to display images\n\ndef imshow(img, title):\n    \n    #convert image from tensor to numpy for visualization\n    npimg = img.numpy()\n    #define the size of a figure\n    plt.figure(figsize = (15, 15))\n    plt.axis(\"off\")\n    \n    #interchaging the image sizes - transposing\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.title(title, fontsize=15)\n    plt.show()","ba021bed":"#function to get images and feed into our custom function 'imshow'\n\ndef show_batch_images(dataloader):\n    \n    #getting the images\n    images, labels = next(iter(dataloader))\n    #make a grid from those images\n    img = torchvision.utils.make_grid(images)\n    imshow(img, \"classes: \" + str([str(x.item())+ \" \"+ ship[x.item()] for x in labels]))","00c7523d":"show_batch_images(trainfull_loader)","ae3a4bb7":"show_batch_images(trainfull_loader)","e01eab37":"#checking for available gpu\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nfrom torchvision import models\nimport torch.optim as optim\n\n#define the number of classes for the final layer\nnum_classes = 5","387310c7":"Training_transforms = transforms.Compose([\n\ttransforms.Resize((224,224)),\n\ttransforms.RandomHorizontalFlip(),\n\ttransforms.ColorJitter(brightness=0.1,contrast=0.1,saturation=0.1, hue=0.1),\n\ttransforms.RandomAffine(degrees=15, translate=(0.3,0.3), scale=(0.5,1.5), shear=None, resample=False, fillcolor=0),\n\ttransforms.ToTensor(),\n\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])","497534e0":"validation_transforms = transforms.Compose([\n\ttransforms.Resize((224,224)),\n\ttransforms.RandomHorizontalFlip(),\n\ttransforms.ToTensor(),\n\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n\t\t])","d11c0de3":"from torch.utils.data.sampler import SubsetRandomSampler","69b894e3":"#### 90-10 train-validation split\ntr, val = train_test_split(train_data.category, stratify=train_data.category, test_size=0.15, random_state=10)\n\n### Batchsize and parallelize\n\ntraining_batchsize = 16\nnum_workers = 8\n\n#### Idx for train and valid\ntrain_sampler = SubsetRandomSampler(list(tr.index)) \nvalid_sampler = SubsetRandomSampler(list(val.index))","121e3d84":"len(list(tr.index))","685c0822":"len(list(val.index))","0beeba12":"#### Train dataloader ####\ntraindataset = ShipDataLoader('..\/input\/train\/train.csv',\"train\", Training_transforms, '..\/input\/train\/images', map_img_class_dict)\ntrain_loader = torch.utils.data.DataLoader(traindataset, batch_size= training_batchsize ,sampler=train_sampler,num_workers=num_workers)","9d911d38":"show_batch_images(train_loader)","ba98c08c":"#### Valid dataloader ####\n\nvaldataset = ShipDataLoader('..\/input\/train\/train.csv',\"train\", validation_transforms, '..\/input\/train\/images', map_img_class_dict)\nval_loader = torch.utils.data.DataLoader(valdataset, batch_size=training_batchsize,sampler=valid_sampler,num_workers=num_workers)","69852968":"#### Test dataloader ####\n\ntestdataset = ShipDataLoader('..\/input\/train\/train.csv',\"test\", validation_transforms, '..\/input\/train\/images', map_img_class_dict)\ntest_loader = torch.utils.data.DataLoader(testdataset, batch_size=training_batchsize,num_workers=num_workers)","9a4e8128":"import torch.nn as nn\nimport copy","d3309868":"#create a iterator\n\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\n\n#shape of images bunch\nprint(images.shape)\n\n#shape of first image in a group of 4\nprint(images[1].shape)\n\n#class label for first image\nprint(labels[1])\n","df0c40f8":"num_classes","86584883":"dataloaders = {\"train\": train_loader, \"val\": val_loader} \ndataset_sizes = {\"train\": len(list(tr.index)), \"val\": len(list(val.index))}","2e16744b":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    train_losses, test_losses = [], []\n    train_acc, test_acc = [], []\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device) - 1\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n            \n            if phase == \"train\":\n                train_losses.append(epoch_loss)\n                train_acc.append(epoch_acc)\n            elif phase == \"val\":\n                test_losses.append(epoch_loss)\n                test_acc.append(epoch_acc)\n                \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n    \n    del inputs, labels\n    torch.cuda.empty_cache()\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    #saving the best model\n    torch.save(model.state_dict(best_model_wts),\"saved.pth\")\n    print(\"best model saved\")\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, train_losses, test_losses, train_acc, test_acc","3cd26fbf":"def make_predictions(dataloader, trained_model):\n\n    pred = dict()\n    trained_model.eval()\n\n    for  data in dataloader:\n        images, labels = data\n        images = images.cuda()\n        \n        #push model to cuda\n        trained_model.cuda()\n        outputs = trained_model(images)\n        \n        #print(outputs)\n        for i in range(len(images)):\n            #print(torch.argmax(outputs[i]))\n            detect_class = torch.argmax(outputs[i]).item() + 1\n            pred[labels[i]] = detect_class   \n\n    df = pd.DataFrame(list(pred.items()), columns=['image', 'category'])\n    return(df)","3122a3bf":"import time","32ca39cd":"model_ft = models.resnet50(pretrained = True)\n\nprint(model_ft)","722088d2":"#number of trainable parameters in resent101 - before freezing\n\nprint(\"Number of trainable parameters: \", sum(p.numel() for p in model_ft.parameters() if p.requires_grad))","3d3c1de4":"### Let's print the names of the layer stacks for our model\nfor name, child in model_ft.named_children():\n    print(name)","d1bb756d":"model_ft.fc.in_features","8144cd7b":"# Parameters of newly constructed modules have requires_grad=True by default\nfc = nn.Sequential(nn.Linear(model_ft.fc.in_features, 720),\n                                nn.ReLU(),\n                                nn.Dropout(0.5),\n                                nn.Linear(720, 256),\n                                nn.ReLU(),\n                                nn.Dropout(0.4),\n                                nn.Linear(256, 64),\n                                nn.ReLU(),\n                                nn.Dropout(0.3),\n                                nn.Linear(64, 5),\n                                nn.Softmax(dim = 1))","c32d9c88":"model_ft.fc = fc","f1e3bef6":"print(model_ft)","73ea57cf":"criterion = nn.CrossEntropyLoss()\n\nmodel_ft = model_ft.to(device)\n\noptimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model_ft.parameters()), lr=0.0001, weight_decay=1e-3)\n\n# Decay LR by a factor of 0.15 every 7 epochs\nexp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.15)\n\nscheduler_cosineAL = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, len(train_loader), eta_min=1e-6)","f54f8994":"model_trained, train_lr, test_lr,train_acc, test_acc = train_model(model_ft, criterion, optimizer_ft, scheduler_cosineAL, num_epochs=80)","187a8034":"#plot the losses\n\nplt.plot(train_lr, label='Training loss')\nplt.plot(test_lr, label='Validation loss')\nplt.legend(frameon=False)\nplt.show()","9625d4db":"plt.plot(train_acc, label = \"Training acc\")\nplt.plot(test_acc, label = \"Validation acc\")\nplt.legend(frameon=False)\nplt.show()","86962610":"test_df_res50 = make_predictions(test_loader, model_trained)","a4b9ff88":"test_df_res50.rename(columns = {'category':'rs50_category'}, inplace = True) \ntest_df_res50.head()","2047d77f":"test_df_res50.rs50_category.value_counts()","e73ce4af":"model_ft = models.resnet101(pretrained = True)\n\nprint(model_ft)","75b63df4":"#number of trainable parameters in resent101 - before freezing\n\nprint(\"Number of trainable parameters: \", sum(p.numel() for p in model_ft.parameters() if p.requires_grad))","812226fc":"### Let's print the names of the layer stacks for our model\nfor name, child in model_ft.named_children():\n    print(name)","af0456f7":"model_ft.fc.in_features","fd5b5999":"# Parameters of newly constructed modules have requires_grad=True by default\nfc = nn.Sequential(nn.Linear(model_ft.fc.in_features, 720),\n                                nn.ReLU(),\n                                nn.Dropout(0.5),\n                                nn.Linear(720, 256),\n                                nn.ReLU(),\n                                nn.Dropout(0.4),\n                                nn.Linear(256, 64),\n                                nn.ReLU(),\n                                nn.Dropout(0.3),\n                                nn.Linear(64, 5),\n                                nn.Softmax(dim = 1))","80a602cd":"model_ft.fc = fc","07ba8c1b":"print(model_ft)","fb71c589":"criterion = nn.CrossEntropyLoss()\n\nmodel_ft = model_ft.to(device)\n\noptimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model_ft.parameters()), lr=0.0001, weight_decay=1e-3)\n\nscheduler_cosineAL = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, len(train_loader), eta_min=1e-6)","60c3439e":"model_trained, train_lr, test_lr,train_acc, test_acc = train_model(model_ft, criterion, optimizer_ft, scheduler_cosineAL, num_epochs=80)","80b74538":"#plot the losses\n\nplt.plot(train_lr, label='Training loss')\nplt.plot(test_lr, label='Validation loss')\nplt.legend(frameon=False)\nplt.show()","0cb82d07":"plt.plot(train_acc, label = \"Training acc\")\nplt.plot(test_acc, label = \"Validation acc\")\nplt.legend(frameon=False)\nplt.show()","b88bdf69":"test_df_res101 = make_predictions(test_loader, model_trained)","9dc96b0b":"test_df_res101.rename(columns = {'category':'rs101_category'}, inplace = True) \n\ntest_df_res101.head()","fe0ff2c6":"test_df_res101.rs101_category.value_counts()","5d09f56b":"model_ft = models.resnet152(pretrained = True)\n\nprint(model_ft)","90dfb480":"#number of trainable parameters in resent101 - before freezing\n\nprint(\"Number of trainable parameters: \", sum(p.numel() for p in model_ft.parameters() if p.requires_grad))","2ddce64e":"### Let's print the names of the layer stacks for our model\nfor name, child in model_ft.named_children():\n    print(name)","16c260f1":"model_ft.fc.in_features","b4b2dc74":"# Parameters of newly constructed modules have requires_grad=True by default\nfc = nn.Sequential(nn.Linear(model_ft.fc.in_features, 720),\n                                nn.ReLU(),\n                                nn.Dropout(0.5),\n                                nn.Linear(720, 256),\n                                nn.ReLU(),\n                                nn.Dropout(0.4),\n                                nn.Linear(256, 64),\n                                nn.ReLU(),\n                                nn.Dropout(0.3),\n                                nn.Linear(64, 5),\n                                nn.Softmax(dim = 1))","4e8ace3b":"model_ft.fc = fc","14e8e431":"print(model_ft)","ad28201e":"criterion = nn.CrossEntropyLoss()\n\nmodel_ft = model_ft.to(device)\n\noptimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model_ft.parameters()), lr=0.0001, weight_decay=1e-3)\n\nscheduler_cosineAL = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, len(train_loader), eta_min=1e-6)","f82236a5":"model_trained, train_lr, test_lr,train_acc, test_acc = train_model(model_ft, criterion, optimizer_ft, scheduler_cosineAL, num_epochs=80)","758a841e":"#plot the losses\n\nplt.plot(train_lr, label='Training loss')\nplt.plot(test_lr, label='Validation loss')\nplt.legend(frameon=False)\nplt.show()","30243f7f":"plt.plot(train_acc, label = \"Training acc\")\nplt.plot(test_acc, label = \"Validation acc\")\nplt.legend(frameon=False)\nplt.show()","4203d7fc":"test_df_res152 = make_predictions(test_loader, model_trained)","df01b9fc":"test_df_res152.rename(columns = {'category':'res152_category'}, inplace = True) \n\ntest_df_res152.head()","4d2f431a":"test_df_res152.res152_category.value_counts()","8bc6a70a":"merge_results = [df.set_index(['image']) for df in [test_df_res50, test_df_res101, test_df_res152]]\nmerge_results = pd.concat(merge_results, axis=1).reset_index()","3c9f0f0e":"merge_results.head()","ea58e401":"merged_pred = merge_results.drop(\"image\", axis = 1)\n\n#finding the most frequent value in a row\nmerged_pred = merged_pred.mode(axis = 1)[0]","acb78dd0":"final_df = pd.DataFrame({\"image\": list(merge_results.image)})\nfinal_df[\"category\"] = merged_pred","ebd65efe":"final_df.head()","2d0a2aae":"final_df.to_csv(\"final_submission.csv\")","34bae155":"# Resnet 101 Model","8427ad69":" # Training resnet152 Model","dd720e4e":"# Custom class for loading data\n","f23ac1d0":"# Modeling","0a57b8a1":"There are 5 classes of ships to be detected which are as follows:\n\n* Cargo\n* Military\n* Carrier\n* Cruise\n* Tankers\n\n\n![](https:\/\/datahack.analyticsvidhya.com\/media\/__sized__\/contest_cover\/god_2-thumbnail-1200x1200-90.jpg)","121fc0f7":"# Resnet 50 Model","89f77c2b":"# Making Predictions - Resnet101","b3cce811":"# Approach\n - Created a Custom Pytorch Data loader function\n - Used Pytorch Pretrained Models\n - Used Resnet50, Resnet152 and Resnet101\n - Final Submission is result of votings based on 3 submission files created from 3 different models:\n     - Image size 224x224\n     - Data Augmenation\n     - Stratified Sampling split for training and validation data because of data imbalance\n     - Pre-trained network(resnet50, Resnet152 and Resnet101) - Unfreezed All Layers\n     - Saved the best model with good accuracy and used that to make predictions","aa754277":"# Merge Results","821f995e":"# resnet152 Model","b7813140":"# Training Resnet50 Model","fb1af210":"# Analytics Vidhya - Game of Deep Learning || Computer Vision Hackathon\n - Competition Link: https:\/\/datahack.analyticsvidhya.com\/contest\/game-of-deep-learning\/","50e98417":"# Visualization of Data","fb25558e":"# Import Libraries","78f3fc9d":"# Problem Statement\nShip or vessel detection has a wide range of applications, in the areas of maritime safety, fisheries management, marine pollution, defence and maritime security, protection from piracy, illegal migration, etc.\n\nKeeping this in mind, a Governmental Maritime and Coastguard Agency is planning to deploy a computer vision based automated system to identify ship type only from the images taken by the survey boats. You have been hired as a consultant to build an efficient model for this project.","05db4aba":"### Data Augmentation\n\nA common strategy for training neural networks is to introduce randomness in the input data itself. For example, you can randomly rotate, mirror, scale, and\/or crop your images during training. This will help your network generalize as it's seeing the same images but in different locations, with different sizes, in different orientations, etc.\n\nTo randomly rotate, scale and crop, then flip your images you would define your transforms like this:\n\n```python\ntrain_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.5, 0.5, 0.5], \n```\n\nYou'll also typically want to normalize images with `transforms.Normalize`. You pass in a list of means and list of standard deviations, then the color channels are normalized like so\n\n```input[channel] = (input[channel] - mean[channel]) \/ std[channel]```\n\nSubtracting `mean` centers the data around zero and dividing by `std` squishes the values to be between -1 and 1. Normalizing helps keep the network work weights near zero which in turn makes backpropagation more stable. Without normalization, networks will tend to fail to learn.\n\nYou can find a list of all [the available transforms here](http:\/\/pytorch.org\/docs\/0.3.0\/torchvision\/transforms.html). When you're testing however, you'll want to use images that aren't altered (except you'll need to normalize the same way). So, for validation\/test images, you'll typically just resize and crop.","0625cd95":"# Making Predictions - resnet152","8f27774f":"# Making Predictions - Resnet50","42714795":"# Training Resnet101 Model"}}