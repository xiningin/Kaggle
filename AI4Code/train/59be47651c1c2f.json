{"cell_type":{"f4c51288":"code","8539b1bf":"code","e213e4e6":"code","8d5193c3":"code","34ccf1ef":"code","bd374fb5":"code","6954486e":"markdown"},"source":{"f4c51288":"!pip install kaggle-environments --upgrade","8539b1bf":"%%writefile submission.py\n\n\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime, math\nfrom collections import defaultdict\n\ntotal_reward = 0\nbandit_dict = {}\n\n\ndef get_next_bandit():\n    \"\"\"Get Next Bandit\n    \n    Choose the best bandit based on some logics. \n    \n    Honestly, don't really understand the logic T.T\n    \"\"\"\n    best_bandit = 0\n    best_bandit_expected = 0\n    \n    for bnd in bandit_dict:\n        # define some things\n        num_wins = bandit_dict[bnd]['win']\n        num_losses = bandit_dict[bnd]['loss']\n        num_opt_choices = bandit_dict[bnd]['opp']\n        num_opt_redraws = bandit_dict[bnd]['op_continue']\n        # calculate expectation\n        expect = (\n            num_wins - num_losses        # subtract the losses?! \n            + num_opt_choices            # add the num draws of opponent\n            - (num_opt_choices>0)*1.5    # subtract if opponent has ever drawn (rate up things you've never drawn)\n            + num_opt_redraws            # adding number of opt redraws (rate up something that's commonly drawn in a row)\n        ) \/ (\n            num_wins + num_losses + num_opt_choices  # divide by total plays\n        ) \\\n        * math.pow(0.97, num_wins + num_losses + num_opt_choices)  # decay\n        \n        \n        # find the best bandit\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n            \n    return best_bandit\n\nmy_action_list = []\nop_action_list = []\n\nop_continue_cnt_dict = defaultdict(int)\n\ndef multi_armed_probabilities(observation, configuration):\n    \"\"\"Multi Armed Probabilities\n    \n    Track the moves and rewards of the game, as well as the repeated actions\n    by players.\n    \n    Logic: \n     1. If you have a successful pull, do that again. (over-exploit)\n     2. If you've drawn something 3 times in a row, redo that 50% of the time\n     3. Else choose agent based on best estimate of returns\n    \"\"\"\n    global total_reward, bandit_dict\n\n    # initialise randomly\n    my_pull = random.randrange(configuration['banditCount'])\n    \n    # update the internal data\n    if 0 == observation['step']:\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0, 'my_continue': 0, 'op_continue': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if 0 < last_reward:\n            bandit_dict[my_last_action]['win'] += 1\n        else:\n            bandit_dict[my_last_action]['loss'] += 1\n        bandit_dict[op_last_action]['opp'] += 1\n        \n        # if someone redraws the same, then increment\n        if observation['step'] >= 3:\n            if my_action_list[-1] == my_action_list[-2]:\n                bandit_dict[my_last_action]['my_continue'] += 1\n            else:\n                bandit_dict[my_last_action]['my_continue'] = 0\n            if op_action_list[-1] == op_action_list[-2]:\n                bandit_dict[op_last_action]['op_continue'] += 1\n            else:\n                bandit_dict[op_last_action]['op_continue'] = 0\n        \n        # if I had successful last pull, do that again\n        # this probably breaks various follow agents since it's not 'bayesian'\n        if last_reward > 0:\n            my_pull = my_last_action\n        # if I've mad three in a row the same, do it again 50% of the \n        # time, otherwise get the best bandit\n        elif observation['step'] >= 4 \\\n            and (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]) \\\n            and random.random() < 0.5:\n            my_pull = my_action_list[-1]\n        # otherwise use bandit choice logics\n        else:\n            my_pull = get_next_bandit()\n    \n    return my_pull","e213e4e6":"%%writefile opponent_agent.py\n\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime, math\n\ntotal_reward = 0\nbandit_dict = {}\n\ndef set_seed(my_seed=42):\n    os.environ['PYTHONHASHSEED'] = str(my_seed)\n    random.seed(my_seed)\n    np.random.seed(my_seed)\n\ndef get_next_bandit():\n    best_bandit = 0\n    best_bandit_expected = 0\n    for bnd in bandit_dict:\n        expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5) \\\n                 \/ (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\n                * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n    return best_bandit\n\nmy_action_list = []\nop_action_list = []\n\n\n\ndef multi_armed_probabilities(observation, configuration):\n    global total_reward, bandit_dict\n\n    my_pull = random.randrange(configuration['banditCount'])\n    if 0 == observation['step']:\n        set_seed()\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if 0 < last_reward:\n            bandit_dict[my_last_action]['win'] = bandit_dict[my_last_action]['win'] +1\n        else:\n            bandit_dict[my_last_action]['loss'] = bandit_dict[my_last_action]['loss'] +1\n        bandit_dict[op_last_action]['opp'] = bandit_dict[op_last_action]['opp'] +1\n        \n        if last_reward > 0:\n            my_pull = my_last_action\n        else:\n            if observation['step'] >= 4:\n                if (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]):\n                    if random.random() < 0.5:\n                        my_pull = my_action_list[-1]\n                    else:\n                        my_pull = get_next_bandit()\n                else:\n                    my_pull = get_next_bandit()\n            else:\n                my_pull = get_next_bandit()\n    \n    return my_pull","8d5193c3":"from kaggle_environments import make\nenv = make(\"mab\", debug=True)","34ccf1ef":"import datetime\n\nenv.reset()\nstart_time = datetime.datetime.now()\nenv.run([\"opponent_agent.py\", \"submission.py\"])\nstop_time = datetime.datetime.now()\nprint('Completed agent vs new model:', stop_time-start_time)\nenv.render(mode=\"ipython\", width=800, height=400)","bd374fb5":"import datetime\n\nenv.reset()\nstart_time = datetime.datetime.now()\nenv.run([\"submission.py\", \"submission.py\"])\nstop_time = datetime.datetime.now()\nprint('Completed sub vs sub:', stop_time-start_time)\nenv.render(mode=\"ipython\", width=800, height=400)","6954486e":"# Keep pulling same bandit as long as reward keeps coming!\n\nCleaned up a bit and added coments \/ explanations for functions.\n\nFull credit to Lindada's notebook.\nNotebook: https:\/\/www.kaggle.com\/a763337092\/pull-vegas-slot-machines-add-weaken-rate-continue5\nKaggler: https:\/\/www.kaggle.com\/a763337092"}}