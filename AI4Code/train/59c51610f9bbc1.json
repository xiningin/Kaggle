{"cell_type":{"c5b16f1e":"code","4af84114":"code","cb206cf1":"code","ed3ce924":"code","744b4fcc":"code","fb1e3443":"code","edf8f4bf":"code","8af990bc":"code","e33d8b7c":"code","9e233cd9":"code","041e1bd7":"code","f6fd650b":"code","6d5b3b6f":"code","ef0a8e8a":"code","05fdaf23":"code","a19c4cf1":"code","436af13c":"code","c40081c7":"code","6c3668d5":"code","b97abbd0":"code","41f9e8d0":"code","4c439768":"code","667d6eb7":"code","4e0413f6":"code","3dedf95e":"code","4b4d3555":"code","64b52571":"code","891ef688":"code","c330581b":"code","0eaddb2c":"code","96a132b3":"code","1b6f6efb":"code","7613a0dd":"code","00ba4b9f":"code","4c5fd470":"code","2b5422f2":"code","5f24fa0f":"code","fcb625b0":"code","01598384":"code","5dfa1013":"code","f3c7da21":"code","5c9dd225":"markdown","392278bc":"markdown","2ef0beec":"markdown","baf610d5":"markdown","663dbc25":"markdown","4e8bccf0":"markdown","2b5345e2":"markdown","166db37b":"markdown","c5b17e22":"markdown","5ae8f3be":"markdown","d3ee8f31":"markdown","f0c93621":"markdown","4103ab2b":"markdown","1e27bded":"markdown","6f26c22a":"markdown","c97d627b":"markdown","8e9b02b7":"markdown","f7e7212d":"markdown","dd39a21a":"markdown","ea612986":"markdown","2bcf1609":"markdown","901f1727":"markdown","3452edd9":"markdown","553c4589":"markdown","0e8ac3a6":"markdown","8d109d3e":"markdown","ae51f171":"markdown","8f05aa65":"markdown","3fa8e6f0":"markdown","7120b1da":"markdown","0673e470":"markdown","3f0011a7":"markdown","ad7cc101":"markdown","2bcf0958":"markdown","c4e699c3":"markdown","3e5e6a1c":"markdown","8d71c45b":"markdown","b7972cfa":"markdown","b36cf92d":"markdown"},"source":{"c5b16f1e":"!pip install tubesml==0.2.0","4af84114":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport string\nimport random\n\nfrom tubesml.base import BaseTransformer, self_columns, reset_columns\nimport tubesml as tml\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.datasets import make_classification, make_regression\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport xgboost as xgb\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","cb206cf1":"def create_data(n_samples, n_features, flip_y=0.1, noise=0, regression=False, random_state=26):\n    if not regression:\n        df, target = make_classification(n_samples=n_samples, n_features=n_features, flip_y=flip_y, n_redundant=0, random_state=random_state)\n    else:\n        df, target = make_regression(n_samples=n_samples, n_features=n_features, noise=noise, random_state=random_state)\n    \n    i = 0\n    random_names = []\n    # generate n_features random strings of 5 characters\n    while i < n_features:\n        random_names.append(''.join([random.choice(string.ascii_lowercase) for _ in range(5)]))\n        i += 1\n        \n    df = pd.DataFrame(df, columns=random_names)\n    df['target'] = target\n    \n    return df\n\n\ndef plot_tree_prediction(data, model=DecisionTreeClassifier(random_state=34)):\n    tree = model\n    y = data['target']\n    train = data.drop('target', axis=1)\n\n    tree.fit(train, y)\n    \n    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n    \n    ax[0].scatter(x=data[data.target==0][data.columns[0]], \n                  y=data[data.target==0][data.columns[1]], \n                  alpha=0.7, label='y=0')\n    ax[0].scatter(x=data[data.target==1][data.columns[0]], \n                  y=data[data.target==1][data.columns[1]], \n                  alpha=0.7, label='y=1')\n    ax[0].legend()\n    ax[0].set_title('Original data', fontsize=14)\n    ax[0].set_ylabel(data.columns[0])\n    ax[0].set_xlabel(data.columns[1])\n    \n    plot_step = 0.02\n\n    x_min, x_max = train[train.columns[0]].min() - 1, train[train.columns[0]].max() + 1\n    y_min, y_max = train[train.columns[1]].min() - 1, train[train.columns[1]].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    Z = tree.predict(pd.DataFrame(np.c_[xx.ravel(), yy.ravel()], columns=train.columns))\n    Z = Z.reshape(xx.shape)\n    ax[1].contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n    \n    to_plot = train   # placeholder for other versions    \n\n    ax[1].scatter(x=to_plot[data.target==0][to_plot.columns[0]], \n                  y=to_plot[data.target==0][to_plot.columns[1]], \n                  alpha=0.5, color='k', label='y=0')\n    ax[1].scatter(x=to_plot[data.target==1][to_plot.columns[0]], \n                  y=to_plot[data.target==1][to_plot.columns[1]], \n                  alpha=0.5, color='w', label='y=1')\n    ax[1].legend()\n    ax[1].set_title(f'Tree-based model predictions', fontsize=14)\n    plt.show()\n    \n    \ndef rotate_data(data, angle):\n    df = data.copy()\n    \n    df[df.columns[0]] = data[data.columns[0]]*np.cos(angle) - data[data.columns[1]]*np.sin(angle)\n    df[df.columns[1]] = data[data.columns[0]]*np.sin(angle) + data[data.columns[1]]*np.cos(angle)\n    \n    return df","ed3ce924":"df = create_data(1000, 2, flip_y=0, random_state=26)\ndf.head()","744b4fcc":"tree = DecisionTreeClassifier(random_state=34)\ntree.fit(df.drop('target', axis=1), df['target'])\nplt.figure(figsize=(15,8))\nplot_tree(tree, filled=True)\nplt.show()","fb1e3443":"plt.figure(figsize=(15,8))\nplot_tree(tree, max_depth=2, filled=True)\nplt.show()","edf8f4bf":"plot_tree_prediction(df)","8af990bc":"df_rot = rotate_data(df, angle=-3.14\/6)\nplot_tree_prediction(df_rot)","e33d8b7c":"df_rot = rotate_data(df, angle=-3.14\/3)\nplot_tree_prediction(df_rot)","9e233cd9":"df = create_data(1000, 2, flip_y=0.1, random_state=26)\nplot_tree_prediction(df)","041e1bd7":"df_rot = rotate_data(df, angle=-3.14\/6)\nplot_tree_prediction(df_rot)","f6fd650b":"class DfPCA(BaseTransformer):\n    def __init__(self, n_components, svd_solver='auto', compress=False, random_state=0):\n        super().__init__()\n        self.n_components = n_components\n        self.svd_solver = svd_solver\n        self.n_components_ = 0\n        self.random_state = random_state\n        self.PCA = PCA(n_components=self.n_components, svd_solver=self.svd_solver, random_state=self.random_state)\n        self.compress = compress\n        self.original_columns = []\n        \n    @reset_columns\n    def fit(self, X, y=None):\n        \n        self.PCA.fit(X)\n        self.n_components_ = self.PCA.n_components_\n        \n        return self\n    \n    @self_columns\n    def transform(self, X, y=None):\n                \n        X_tr = self.PCA.transform(X)\n        X_tr = pd.DataFrame(X_tr, columns=[f'pca_{i}' for i in range(self.n_components_)])\n        \n        self.original_columns = X.columns\n        \n        if self.compress:\n            X_tr = self.inverse_transform(X_tr)\n        \n        return X_tr\n    \n    \n    def inverse_transform(self, X, y=None):\n        \n        try:\n            X_tr = self.PCA.inverse_transform(X)\n        except ValueError:  # if self.compress=True, the inverse_transform called externally would throw an error\n            return X\n        X_tr = pd.DataFrame(X_tr, columns=self.original_columns)\n        \n        return X_tr","6d5b3b6f":"df = create_data(1000, 2, flip_y=0, random_state=26)\n\ntree_pca = Pipeline([('pca', tml.DfPCA(n_components=0.95, random_state=24)), \n                     ('tree', DecisionTreeClassifier(random_state=34))])\n\nplot_tree_prediction(df, tree_pca)","ef0a8e8a":"df_rot = rotate_data(df, angle=-3.14\/3)\nplot_tree_prediction(df_rot, tree_pca)","05fdaf23":"df = create_data(1000, 2, flip_y=0.1, random_state=26)\nplot_tree_prediction(df, tree_pca)","a19c4cf1":"df_rot = rotate_data(df, angle=-3.14\/6)\nplot_tree_prediction(df_rot, tree_pca)","436af13c":"df = create_data(1000, 2, flip_y=0, random_state=26)\n\nforest = RandomForestClassifier(n_estimators=200, random_state=32, n_jobs=-1)\n\nplot_tree_prediction(df, forest)","c40081c7":"df = create_data(1000, 2, flip_y=0.1, random_state=26)\nplot_tree_prediction(df, forest)","6c3668d5":"df_rot = rotate_data(df, angle=-3.14\/3)\nplot_tree_prediction(df_rot, forest)","b97abbd0":"df = create_data(1000, 2, flip_y=0, random_state=26)\n\nforest_pca = Pipeline([('pca', tml.DfPCA(n_components=0.95)), \n                     ('forest', forest)])\n\nplot_tree_prediction(df, forest_pca)","41f9e8d0":"boost = xgb.XGBClassifier(n_estimators=300, n_jobs=-1, \n                          random_state=343, use_label_encoder=False, eval_metric='logloss')\n\nplot_tree_prediction(df, boost)","4c439768":"df_rot = rotate_data(df, angle=-3.14\/3)\nplot_tree_prediction(df_rot, boost)","667d6eb7":"boost_pca = Pipeline([('pca', tml.DfPCA(n_components=0.95)), \n                     ('xgb', boost)])\n\nplot_tree_prediction(df, boost_pca)","4e0413f6":"df = create_data(1000, 10)\ndf.head(10)","3dedf95e":"pca = tml.DfPCA(n_components=0.8)\ndf_pca = pca.fit_transform(df.drop('target', axis=1))\ndf_pca.head(10)","4b4d3555":"df_inv = pca.inverse_transform(df_pca)\ndf_inv.head(10)","64b52571":"digits = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nnum = 10\ntrain = digits.drop('label', axis=1)[:num]\nlabels = digits['label'][:num]\n\nnum_row = 2\nnum_col = 5\n# plot images\nfig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\nfor i in range(num):\n    ax = axes[i\/\/num_col, i%num_col]\n    ax.imshow(np.array(train.iloc[i]).reshape((28,28)), cmap='gray')\n    ax.set_title(f'Label: {labels[i]}')\nplt.tight_layout()\nplt.show()","891ef688":"pca = tml.DfPCA(n_components=0.7, compress=True, random_state=234)\ntrain = digits.drop('label', axis=1)\ntrain = pca.fit_transform(train)[:num]\nfig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\nfor i in range(num):\n    ax = axes[i\/\/num_col, i%num_col]\n    ax.imshow(np.array(train.iloc[i]).reshape((28,28)), cmap='gray')\n    ax.set_title(f'Label: {labels[i]}')\nplt.tight_layout()\nplt.show()","c330581b":"df = create_data(1000, 2, flip_y=0.1, random_state=26)\n\ntree_pca = Pipeline([('pca', tml.DfPCA(n_components=0.95, compress=True)), \n                     ('tree', DecisionTreeClassifier(random_state=34))])\n\nplot_tree_prediction(df, tree_pca)","0eaddb2c":"df_rot = rotate_data(df, angle=-3.14\/6)\nplot_tree_prediction(df_rot, tree_pca)","96a132b3":"df = create_data(1000, 2, regression=True)\ndf.head()","1b6f6efb":"data= df.copy()\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 6))\n    \nax[0].scatter(x=data[data.columns[0]], \n              y=data[data.columns[1]], \n              alpha=0.9, c=data['target'], cmap='Reds')\n\nax[0].set_title('Original data', fontsize=14)\nax[0].set_ylabel(data.columns[0])\nax[0].set_xlabel(data.columns[1])\n\ntree = DecisionTreeRegressor(random_state=443)\ny = data['target']\ntrain = data.drop('target', axis=1)\n\ntree.fit(train, y)\n\nplot_step = 0.02\n\nx_min, x_max = train[train.columns[0]].min() - 1, train[train.columns[0]].max() + 1\ny_min, y_max = train[train.columns[1]].min() - 1, train[train.columns[1]].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                     np.arange(y_min, y_max, plot_step))\n\nZ = tree.predict(pd.DataFrame(np.c_[xx.ravel(), yy.ravel()], columns=train.columns))\nZ = Z.reshape(xx.shape)\nax[1].contourf(xx, yy, Z, cmap='Reds')\n\nax[1].scatter(x=data[data.columns[0]], \n              y=data[data.columns[1]], \n              alpha=0.5, c=data['target'], cmap='Blues')\nax[1].set_title(f'Decision Tree predictions', fontsize=14)\n\nplt.show()","7613a0dd":"df_rot = rotate_data(df, 3.14\/2)\ndata= df_rot.copy()\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 6))\n    \nax[0].scatter(x=data[data.columns[0]], \n              y=data[data.columns[1]], \n              alpha=0.9, c=data['target'], cmap='Reds')\n\nax[0].set_title('Original data', fontsize=14)\nax[0].set_ylabel(data.columns[0])\nax[0].set_xlabel(data.columns[1])\n\ntree = DecisionTreeRegressor(random_state=443)\ny = data['target']\ntrain = data.drop('target', axis=1)\n\ntree.fit(train, y)\n\nplot_step = 0.02\n\nx_min, x_max = train[train.columns[0]].min() - 1, train[train.columns[0]].max() + 1\ny_min, y_max = train[train.columns[1]].min() - 1, train[train.columns[1]].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                     np.arange(y_min, y_max, plot_step))\n\nZ = tree.predict(pd.DataFrame(np.c_[xx.ravel(), yy.ravel()], columns=train.columns))\nZ = Z.reshape(xx.shape)\nax[1].contourf(xx, yy, Z, cmap='Reds')\n\nax[1].scatter(x=data[data.columns[0]], \n              y=data[data.columns[1]], \n              alpha=0.5, c=data['target'], cmap='Blues')\nax[1].set_title(f'Decision Tree predictions', fontsize=14)\n\nplt.show()","00ba4b9f":"house = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhouse.head()","4c5fd470":"numeric_pipe = Pipeline([('fs', tml.DtypeSel('numeric')), \n                         ('imp', tml.DfImputer('mean'))])\n\ncategorical_pipe = Pipeline([('fs', tml.DtypeSel('category')), \n                             ('imp', tml.DfImputer('most_frequent')), \n                             ('dummify', tml.Dummify(drop_first=True))])\n\nunion_pipe = tml.FeatureUnionDf(transformer_list=[('num', numeric_pipe), ('cat', categorical_pipe)])\n\nfull_pipe = Pipeline([('proc', union_pipe), \n                      ('scl', tml.DfScaler())])","2b5422f2":"train = house.drop(['Id', 'SalePrice'], axis=1)\ntarget = house['SalePrice']\n\nfull_pipe.fit_transform(train).head()","5f24fa0f":"train = house.drop(['Id', 'SalePrice'], axis=1)\ntarget = house['SalePrice']\n\nnumeric_pipe = Pipeline([('fs', tml.DtypeSel('numeric')), \n                         ('imp', tml.DfImputer('mean'))])\n\ncategorical_pipe = Pipeline([('fs', tml.DtypeSel('category')), \n                             ('imp', tml.DfImputer('most_frequent')), \n                             ('dummify', tml.Dummify(drop_first=True))])\n\nunion_pipe = tml.FeatureUnionDf(transformer_list=[('num', numeric_pipe), ('cat', categorical_pipe)])\n\nfull_pipe = Pipeline([('proc', union_pipe), \n                      ('scl', tml.DfScaler()), \n                      ('pca', tml.DfPCA(n_components=100))])\n\nfull_pipe.fit_transform(train).head()","fcb625b0":"train = house.drop(['Id', 'SalePrice'], axis=1)\ntarget = house['SalePrice']\n\nnumeric_pipe = Pipeline([('fs', tml.DtypeSel('numeric')), \n                         ('imp', tml.DfImputer('mean'))])\n\ncategorical_pipe = Pipeline([('fs', tml.DtypeSel('category')), \n                             ('imp', tml.DfImputer('most_frequent')), \n                             ('dummify', tml.Dummify(drop_first=True))])\n\nunion_pipe = tml.FeatureUnionDf(transformer_list=[('num', numeric_pipe), ('cat', categorical_pipe)])\n\nfull_pipe = Pipeline([('proc', union_pipe), \n                      ('scl', tml.DfScaler()), \n                      ('pca', tml.DfPCA(n_components=100, compress=True))])\n\nfull_pipe.fit_transform(train).head()","01598384":"train = house.drop(['Id', 'SalePrice'], axis=1)\ntarget = house['SalePrice']\n\nnumeric_pipe = Pipeline([('fs', tml.DtypeSel('numeric')), \n                         ('imp', tml.DfImputer('mean'))])\n\ncategorical_pipe = Pipeline([('fs', tml.DtypeSel('category')), \n                             ('imp', tml.DfImputer('most_frequent')), \n                             ('dummify', tml.Dummify(drop_first=True))])\n\nunion_pipe = tml.FeatureUnionDf(transformer_list=[('num', numeric_pipe), ('cat', categorical_pipe)])\n\nfull_pipe = Pipeline([('proc', union_pipe), \n                      ('scl', tml.DfScaler()), \n                      ('pca', tml.DfPCA(n_components=100)), \n                      ('tree', DecisionTreeRegressor(max_depth=5, random_state=32))])\n\nfull_pipe.fit(train, target)","5dfa1013":"full_pipe = Pipeline([('proc', union_pipe), \n                      ('scl', tml.DfScaler()), \n                      ('pca', tml.DfPCA(n_components=100, compress=False)), \n                      ('tree', DecisionTreeRegressor(max_depth=5, random_state=32))])\n\nparam_grid = {'pca__n_components': [50, 100], \n              'pca__compress': [True, False]}\n\nres, bp, be = tml.grid_search(data=train, target=target, cv=5, estimator=full_pipe, \n                param_grid=param_grid, scoring='neg_mean_squared_error')\n\nres.head()","f3c7da21":"test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndel test['Id']\n\nbe.predict(test)[:10]","5c9dd225":"We immediately see how the decision boundary looks simpler than before. However, **the biggest impact of doing so is not simplicity, but invariance**. If we now rotate the data as we did before we see how the decision boundary is now rotating with the data by maintaining the same shape, e.g. **the set of rules is now invariant to changes in the data representation**.","392278bc":"However, we can see that while we got the same features we had at the beginning, the numbers do not match with the original data. This is because **in compressing the data we have lost some information**. \n\nIn a normal situation, one would hope that the information we have lost was the least relevant one according to the PCA. However, it is good to be aware of the consequences. Let's take the famous MNIST digit dataset and see some of the labeled data\n\n","2ef0beec":"But, most importantly, it is now invariant under rotations","baf610d5":"# Decision boundaries and rotations\n\nWhat the tree is actually doing is defining a **decision boundary** that cuts across the data and say: everything on this side has target = 0, the rest has target = 1.\n\nThis is easy to visualize in 2 dimensions since we have only 2 features but, in principle, you can always expect this boundary to be defined by the set of rules the tree has learned, no matter how many dimensions and features you have. This is one of the positive aspects of this kind of algorithms.\n\nA visualization of the data and of the decision boundary is given by","663dbc25":"The advantage of this is that we can use the pipeline inside of any cross validation scheme and it will guarantee to not have any information leak between train and test, something very important to correctly assess the quality of the model.","4e8bccf0":"If we were to compress (to maintain 70% of the variance) and reverse transform the dataset, we would observe a loss in how defined the images are.","2b5345e2":"# Decision trees and how do they work\n\nWhile an extensive description of the algorithm can be found [in this notebook](https:\/\/www.kaggle.com\/lucabasa\/fantastic-trees-and-how-to-tune-them) (or pretty much anywhere on the internet), let's take a simple dataset and train a model to have a quick idea of its functioning.","166db37b":"# PCA for data compression\n\nAs mentioned in the introduction, PCA is mostly used to **reduce the dimensionality** of our data. For example, let's take a dataset with 10 features and a target variable","c5b17e22":"Or, you can use it in a grid search to find the better strategy for manipulating your data before the training.","5ae8f3be":"For more complex operations that can be done in a pipeline for this dataset, you can go [here](https:\/\/www.kaggle.com\/lucabasa\/houseprice-end-to-end-project)\n\n# Conclusions\n\nTree-based models are flexible and can capture very complex patterns in a dataset but they are susceptible to small perturbation of the data.\nPCA can be used to avoid overfitting caused by the (unknown) data representation used during training. Since we don't know a priori what data representation we are using, the risk is to overfit the training set by creating a needlessly complicated decision boundary, which would be nearly impossible to spot if the number of training features is high.\n\nPCA can reduce this risk by making sure the training set representation that goes into the model is standardized along with its principal components.\n\nMoreover, it is always possible to recover the original features at the cost of a small loss of information due to the compression associated with the dimensionality reduction. \n\nIncluding PCA into a pipeline is not difficult and it will both ensure your model validation is leak-free and your pipeline is ready to be used on truly unseen data without spending any extra line of code.","d3ee8f31":"Or to use the compression trick","f0c93621":"As before, we can fix this unwanted behavior running a PCA before the RandomForestClassifier","4103ab2b":"This will save us some memory and can help our model to train faster as it has to learn from fewer features. \n\nIt is also possible to then reconstruct the original data by calling the `inverse_transform`","1e27bded":"However, we see that even in this case the boundary can change shape if we simply rotate the data","6f26c22a":"Running PCA before a model can be tricky: categorical and numerical features, missing values, data on different scales (PCA assumes the features are on the same scale). However, making use of sklearn's pipelines (with [**tubesML**](https:\/\/pypi.org\/project\/tubesml\/) to be sure you can still run complex grid searches or cross validation for model inspection with minimal coding), this can become fairly easy and give you control on the effects of PCA.","c97d627b":"An ideal situation would see that the entire dataset and the decision boundaries rotated by 90 degrees but we again observe how the shape of the boundary changed. Therefore, we can again use PCA to make our problem rotation invariant.\n\nIncluding PCA in your pipeline can be a valuable tool and, with the use of the right classes, it is not a difficult task. For example, let's take the training set of the housing price competition","8e9b02b7":"We have a binary classification problem and 2 features to use to define some rules to predict our target column.\n\nA Decision Tree will try to recursively split the data in 2 by using all the features and all the possible values to split on, find the best split, repeat until some condition is met.\n\nEven with this simple dataset, letting a tree grow can lead to a very complex set of rules as we can see by visualizing the tree.","f7e7212d":"While an arguably more complex version is given by rotating the data just a bit more","dd39a21a":"# What about regression?\n\nTo not fall into the trap of many text that explain classifiers (or regressors) without mentioning that regressors (or classifiers) exist, let's take a simple dataset suitable for a regression problem","ea612986":"This does not necessarily mean that any model trained on this data will perform worse since it can also be that the information loss was about irrelevant noise in the data.\n\nComing back to our original example, we can use this trick of data compression in the pipeline with a decision tree","2bcf1609":"Let's now take the same classification problem but, this time, instead of directly feeding the decision tree with our data, **we first perform a PCA that captures at least 95% of the variance of the data**.","901f1727":"But in this case we lose the benefits of having a decision boundary that is invariant under rotations","3452edd9":"Decision Trees are a very simple and powerful algorithm to learn complex patterns in a dataset. They are the base of many other algorithms that combine the predictions of several trees.\n\nAn overview and analysis of tree-based algorithms can be found [here](https:\/\/www.kaggle.com\/lucabasa\/fantastic-trees-and-how-to-tune-them).\n\nIn this notebook, we will see how decision trees build rules to make their decisions (decision boundaries) and how a simple perturbation of the data can change such rules. As a consequence, it is not difficult to build arbitrarily more complex models to fit the data that eventually will have little generalization power.\n\nWhile every algorithm here used has already methods to regularize the learning and prevent overfitting, we will focus on how [Principal Component Analysis](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) can be used to make sure that no data transformation can fundamentally change the rules that our algorithm finds to take decisions.\n\nSecondly, we will also see how PCA can be used to compress the data and how this can be useful not only in saving memory but also to train ML models.\n\n\n*Note: we will make use of the [**tubesML**](https:\/\/pypi.org\/project\/tubesml\/) package as it makes working with pipelines more convenient. For an overview about pipelines, which eventually gave the base to the creation of the package, you can see [this notebook](https:\/\/www.kaggle.com\/lucabasa\/understand-and-use-a-pipeline)*","553c4589":"We see how the boundary is made of **orthogonal lines** since it is a glorified collection of simple if-statements (if this is bigger than this value, go left, else go right). \n\nHere comes the issue: a tree-based algorithm will always try to make the decision boundary as a collection of orthogonal lines, no matter how the data is oriented.\n\nA consequence of this is that **with a simple rotation of the data we can make the boundary arbitrarily simple or complex**.\n\nA simpler version of the very same classification problem is ","0e8ac3a6":"Still a pretty good fit but now we can't see if the decision boundary is changing because of the noise (e.g. the model is learning the noise, another way of saying it is overfitting) or because of the (unknown) representation of the data.","8d109d3e":"This remains true if we add noise to our artificial dataset. The original boundary looks again simpler than before","ae51f171":"Zooming into the first 3 layers, we see that the first split is done on the second column with -0.029 as a threshold, then it either split on the same column with a different threshold or on the first one, and so on.","8f05aa65":"We also see how the decision boundary is less susceptible to the introduction of some noise in the data with respect to the individual decision tree, even if this means misclassifying more observations.","3fa8e6f0":"Another set of algorithms, called boosting algorithms, train a sequence of decision trees to then combine the result. In particular, XGBoost, among other things, trains each tree on the residuals (e.g. how much the predictions were wrong) of the previous one. These algorithms are capable to reach very high accuracy. In our case, with this very simple dataset, the decision boundary will look very similar to the one we have seen for the Random Forest.","7120b1da":"# PCA to the rescue\n\nThe Principal Component Analysis is a very common dimensionality reduction technique that changes the basis of the data to identify the principal components. The goal is to capture as much variance in the data with as few dimensions as possible.\n\nTo see how this can help us with our problem, let's first define our transformer (recently included in [tubesML](https:\/\/pypi.org\/project\/tubesml\/)). Like most things in that package, it is a wrapper around sklearn classes that aims to preserve the pandas DataFrame structure, which is very convenient to build very complex pipelines and, for example, tune them, processing included, with grid searches and similar methods.","0673e470":"We can use PCA to make this dataset smaller. If we require PCA to preserve at least 80% of the variance of the original dataset, we get a dataset with 8 features (and a target column not displayed here)","3f0011a7":"Either way, we are ready to use it on entirely unseen data","ad7cc101":"In other words, with simple data manipulation that TubesML provides we can get a dataset ready to be fed to a ML model. Moreover, it is not difficult to add the PCA step mentioned above","2bcf0958":"Thus we see how the model is predicting a low target value on the bottom left and a high one on the top right. In between, we see several decision boundaries to signal when the model is predicting a bigger target value.\n\nThis problem and this model is also susceptible to rotations, exactly like the classification problem we played with so far","c4e699c3":"With the expected useful correction coming from the use of PCA","3e5e6a1c":"The Decision Tree will function very similarly to before, the only difference is that the decision boundaries will define areas of prediction for several target values. We display this in scales of red","8d71c45b":"In other words, we train models to define a set of rules and make predictions but that **set of rules is not invariant to changes to the representation of the data**.\n\nAnother thing that influences the decision boundary shape is the presence of noise in the data. For example, by just flipping the label of 10% of the observations, the decision boundary becomes like this","b7972cfa":"Similarly to the Random Forest, we find that XGBoost is susceptible to data rotation as well","b36cf92d":"# Ensemble and Boosting\n\nAs we know, it is possible to overcome some drawbacks of Decision Trees by using algorithms that combine the prediction of many trees. The most popular algorithm of this type is arguably **Random Forest**. \n\nThe goal is to train a large number of trees in parallel and then combine their predictions (in the sklearn implementation, by taking the average of their probabilistic predictions) to obtain a model less prone to overfitting. The randomness comes from the fact that each tree is trained on a random sample of observations and features.\n\nIf we use such an algorithm, we find a somewhat smoother decision boundary"}}