{"cell_type":{"152d787a":"code","536efb38":"code","76538c7a":"code","9d5637ba":"code","f6e57775":"code","b4bebaa2":"code","381d1ce8":"code","6cc923b8":"code","68f37387":"code","28938c72":"code","c1212c7b":"code","f04c6d39":"code","0725ec90":"code","635ffc19":"code","01c36c8e":"code","91975a50":"code","48780436":"code","2721ebc7":"code","5693492e":"code","98d98bcd":"code","7ab7bb4a":"code","a92ce139":"code","6b535607":"code","3ca93380":"code","8b304e61":"code","1a6a00cf":"code","36ba8b3e":"code","74b9edd4":"code","002963ce":"code","ab62567d":"code","a969ad21":"code","8dabe350":"code","a69ee9a9":"code","fafa6b59":"code","10ac924f":"code","c40e2d0e":"code","8b60d65d":"code","66a28bc0":"code","db270e3a":"code","68d2dae9":"code","e597ef94":"markdown","dc1c32d7":"markdown","b092bc56":"markdown","ecae282c":"markdown","fb4c9533":"markdown","4a405f89":"markdown","bcfc8ea2":"markdown","437247c0":"markdown","6b66178e":"markdown","69fca386":"markdown","3ddffc1d":"markdown","62716883":"markdown","fdb3927a":"markdown","f8aaecb9":"markdown","b1eb59c6":"markdown","3ff8dc23":"markdown","4862be4b":"markdown"},"source":{"152d787a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","536efb38":"print(os.listdir(\"..\/input\"))","76538c7a":"train_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","9d5637ba":"train_df.head()","f6e57775":"train_df.shape","b4bebaa2":"test_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","381d1ce8":"test_df.head()","6cc923b8":"train_df.info()","68f37387":"train_df.describe()","28938c72":"test_df.describe()","c1212c7b":"all_data=pd.concat((train_df,test_df)).reset_index(drop=True)\nx_saleprice=train_df[\"SalePrice\"]\nall_data.drop([\"SalePrice\"],axis=1,inplace=True)\nall_data.shape","f04c6d39":"from scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n","0725ec90":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\nsns.distplot(train_df['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","635ffc19":"# log(1+x) transform\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])","01c36c8e":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train_df['SalePrice'] , fit=norm, color=\"b\");\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\n\nplt.show()","91975a50":"# Remove outliers\ntrain_df.drop(train_df[(train_df['OverallQual']<5) & (train_df['SalePrice']>200000)].index, inplace=True)\ntrain_df.drop(train_df[(train_df['GrLivArea']>4500) & (train_df['SalePrice']<300000)].index, inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)","48780436":"sns.heatmap(all_data.isnull(),yticklabels=False,cbar=False)","2721ebc7":"all_data['LotFrontage']=all_data['LotFrontage'].fillna(all_data['LotFrontage'].mean())\nall_data.drop(['Alley'],inplace=True,axis=1)\nall_data['BsmtQual']=all_data['BsmtQual'].fillna(all_data['BsmtQual'].mode()[0])\nall_data['BsmtCond']=all_data['BsmtCond'].fillna(all_data['BsmtCond'].mode()[0])\nall_data['FireplaceQu']=all_data['FireplaceQu'].fillna(all_data['FireplaceQu'].mode()[0])\nall_data['GarageType']=all_data['GarageType'].fillna(all_data['GarageType'].mode()[0])\nall_data['GarageQual']=all_data['GarageQual'].fillna(all_data['GarageQual'].mode()[0])\nall_data['GarageCond']=all_data['GarageCond'].fillna(all_data['GarageCond'].mode()[0])\n\nall_data.drop(['PoolQC','Fence','MiscFeature','Id'],inplace=True,axis=1)\nall_data['MasVnrType']=all_data['MasVnrType'].fillna(all_data['MasVnrType'].mode()[0])\nall_data['MasVnrArea']=all_data['MasVnrArea'].fillna(all_data['MasVnrArea'].mode()[0])\nall_data['BsmtExposure']=all_data['BsmtExposure'].fillna(all_data['BsmtExposure'].mode()[0])\nall_data['BsmtFinType1']=all_data['BsmtFinType1'].fillna(all_data['BsmtFinType1'].mode()[0])\nall_data['BsmtFinType2']=all_data['BsmtFinType2'].fillna(all_data['BsmtFinType2'].mode()[0])\nall_data.drop(['GarageYrBlt'],inplace=True,axis=1)\nall_data['GarageFinish']=all_data['GarageFinish'].fillna(all_data['GarageFinish'].mode()[0])\nall_data['MSZoning']=all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\n\n\n\n\n\n\n                                                ","5693492e":"all_data['Utilities']=all_data['Utilities'].fillna(all_data['Utilities'].mode()[0])\nall_data['Exterior1st']=all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd']=all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['BsmtFinType1']=all_data['BsmtFinType1'].fillna(all_data['BsmtFinType1'].mode()[0])\nall_data['BsmtFinSF1']=all_data['BsmtFinSF1'].fillna(all_data['BsmtFinSF1'].mean())\nall_data['BsmtFinSF2']=all_data['BsmtFinSF2'].fillna(all_data['BsmtFinSF2'].mean())\nall_data['BsmtUnfSF']=all_data['BsmtUnfSF'].fillna(all_data['BsmtUnfSF'].mean())\nall_data['TotalBsmtSF']=all_data['TotalBsmtSF'].fillna(all_data['TotalBsmtSF'].mean())\nall_data['BsmtFullBath']=all_data['BsmtFullBath'].fillna(all_data['BsmtFullBath'].mode()[0])\nall_data['BsmtHalfBath']=all_data['BsmtHalfBath'].fillna(all_data['BsmtHalfBath'].mode()[0])\nall_data['KitchenQual']=all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Functional']=all_data['Functional'].fillna(all_data['Functional'].mode()[0])\nall_data['GarageCars']=all_data['GarageCars'].fillna(all_data['GarageCars'].mean())\nall_data['GarageArea']=all_data['GarageArea'].fillna(all_data['GarageArea'].mean())\nall_data['SaleType']=all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n","98d98bcd":"sns.heatmap(all_data.isnull(),yticklabels=False,cbar=False)","7ab7bb4a":"\ncategorical_feature_mask = all_data.dtypes==object\n# filter categorical columns using mask and turn it into alist\ncategorical_cols = all_data.columns[categorical_feature_mask].tolist()","a92ce139":"categorical_cols","6b535607":"len(categorical_cols)","3ca93380":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nall_data[categorical_cols] = all_data[categorical_cols].apply(lambda col: labelencoder.fit_transform(col.astype(str)))","8b304e61":"all_data.shape","1a6a00cf":"all_data.head()","36ba8b3e":"train_df = all_data.iloc[:1460,:]  \ntest_df = all_data.iloc[1460 :,:]  ","74b9edd4":"train_df[\"SalePrice\"] = x_saleprice","002963ce":"train_df.info()","ab62567d":"X_train=train_df.drop(['SalePrice'],axis=1)\ny_train=train_df['SalePrice']\nX_test=test_df","a969ad21":"X_train.shape","8dabe350":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import make_scorer, r2_score, mean_squared_log_error\n\nn_folds = 5\n\ncv = KFold(n_splits = 5, shuffle=True, random_state=42).get_n_splits(X_train.values)\n\ndef test_model(model):   \n    msle = make_scorer(mean_squared_log_error)\n    rmsle = np.sqrt(cross_val_score(model, X_train, y_train, cv=cv, scoring = msle))\n    score_rmsle = [rmsle.mean()]\n    return score_rmsle\n\ndef test_model_r2(model):\n    r2 = make_scorer(r2_score)\n    r2_error = cross_val_score(model, X_train, y_train, cv=cv, scoring = r2)\n    score_r2 = [r2_error.mean()]\n    return score_r2","a69ee9a9":"from sklearn.ensemble import GradientBoostingRegressor\nreg_gbr = GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.05, loss='ls', max_depth=3,\n                          max_features='sqrt', max_leaf_nodes=None,\n                          min_impurity_decrease=0.0, min_impurity_split=None,\n                          min_samples_leaf=9, min_samples_split=8,\n                          min_weight_fraction_leaf=0.0, n_estimators=1250,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=None, subsample=0.8, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\nrmsle_ggr = test_model(reg_gbr)\nprint (rmsle_ggr, test_model_r2(reg_gbr))","fafa6b59":"reg_gbr.fit(X_train, y_train)\ny_pred  = reg_gbr.predict(test_df) ","10ac924f":"y_pred","c40e2d0e":"y_pred = pd.DataFrame(y_pred, columns=['SalePrice'])","8b60d65d":"y_pred.head()","66a28bc0":"y_pred.tail()","db270e3a":"sub_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nresults=pd.concat([sub_df['Id'],y_pred],axis=1)","68d2dae9":"results.columns=['Id','SalePrice']\nresults.to_csv('submission.csv',index=False)\nprint(\"Your submission was successfully saved!\")\n","e597ef94":"With 79 exploratory variables describing almost every features of residential homes in Ames,Iowa our taskis to predict the final price of each home. If you are a beginner on our platform come along with me through the following steps;\n1.  First of all Let's read about the data that is given.In the train.csv there are 1460 rows and 81 columns whereas in test.csv there are 1459 and 80 columns.\n2. After reading the file,Let us analyse the trends in both the datasets. Before moving ahead we are combining both the dataframes so that changes can be done in both the dfs together.\n3. Now let us take our target variables distribution into account and make it normal thus removing the outliers.\n4. Then, with the help of heatmap we can check number of nulls in each feature. So, based on the column name and after reading the description in the file data_description.txt of that column, we can decide if we want to replace nulls by mean(), mode(), median() or 'NA' or something else.\n5. Now, we have to replace all the object\/string values to numerical type using labelencoder function\n6. Now I am separating the train and test data from all_data and appending SalePrice column to the train data and the training models starts from here.\n7. While training the model, we are using Kfold cross validation for the better root mean squared log error as it is the evaluation criteria on the leaderboard.\n8. Training with Gradient Boosting Regressor model: rmsle, r2_score = [0.12633287769093945] [0.881143550810682] respectively\n","dc1c32d7":"## Data Analysis\n***Let us do some analysis on the dataset***","b092bc56":"***With 79 exploratory variables describing almost every features of residential homes in Ames,Iowa . Let us predict the final price of each home. To begin with let us prepare our environment and import the data and necessary libraries***","ecae282c":"Let us check the distribution again","fb4c9533":" ### Awesome!! we are good to go","4a405f89":"Here , the SalePrice is skewed to the right. This can be a problem because normally machine learning models don't do well with data that are not normally distributed. We can apply a log(1+x) tranform to fix the skew.","bcfc8ea2":"# **Ames house price prediction through advanced regression techniques**","437247c0":"## Creating and tuning the model","6b66178e":"***Examining the dataset for null values with the help of a heatmap from seaborn ,we will then deal with the missing values accordingly.***","69fca386":"## Data cleaning","3ddffc1d":"Woww! our saleprice feature is now normally distributed.","62716883":"## Preprocessing data for model selection","fdb3927a":"\nLet us import some modules and libraries for statistical modelling and then a take a look at the saleprice distribution.Since it is really important to make your target variables ready to perform training.","f8aaecb9":"## Handling categorical features","b1eb59c6":"### ***Hyper-Parameter tuning***","3ff8dc23":"Let us now ensure that there are not any missing values","4862be4b":"## Dealing with missing values"}}