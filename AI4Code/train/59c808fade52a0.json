{"cell_type":{"57b41432":"code","b5c1a33b":"code","a5495c9d":"code","38ca140e":"code","91948157":"code","6ffbcd30":"code","bca30a08":"code","6cc27696":"code","55f353c6":"code","838a0326":"code","89944be3":"code","d0ed7f65":"code","1d328b34":"code","f7d6f343":"code","b5f2e5ce":"code","56d89a16":"code","dd7db67c":"code","55b6c359":"code","e30e9d66":"code","49563e5d":"code","ee613780":"code","759a6a1d":"code","c6b7a11d":"code","7c295203":"code","8fbb65bb":"code","7dbae24c":"code","4bd8324d":"code","0f1b9882":"code","970c406f":"code","b1a8b41f":"code","59a41a9d":"code","b9bed129":"code","014800a0":"code","d3635b2b":"code","62d0a239":"code","ec1f88e0":"code","f3334bd1":"code","f9d5cba5":"code","aa9950bd":"code","cbcd8376":"code","41022f50":"code","e61876c4":"code","c7a73264":"code","f983e59b":"code","9e2d54fb":"code","13b3d75b":"code","bd451c6a":"code","5bb0b406":"code","036bfe9f":"markdown","3be60ba3":"markdown","9cd2d025":"markdown","366f994a":"markdown","72ca110a":"markdown","91ba41a5":"markdown","cc3b071b":"markdown","c817be99":"markdown","8bef320d":"markdown","b9b2b67e":"markdown","4d89fbc0":"markdown","2a87dc9f":"markdown","d35905bb":"markdown","31247c24":"markdown","b419f8e7":"markdown","d229b369":"markdown","1d7f1c34":"markdown","aa98acef":"markdown","fe6dddc6":"markdown","4cbd7675":"markdown"},"source":{"57b41432":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","b5c1a33b":"data = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","a5495c9d":"data.head(10)","38ca140e":"data.shape","91948157":"data.columns","6ffbcd30":"data.isna().sum()","bca30a08":"# removing id and unnamed: 32 column which is not necessary for our model\ndata = data.drop(['id','Unnamed: 32'],axis = 1)","6cc27696":"data.head(10)","55f353c6":"# As our dataset is balanced (around 60-40 ratio), there is no need to balance our data\ndata.diagnosis.value_counts(normalize = True)","838a0326":"# Mapping our target variable to 1 and 0\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata['diagnosis'] = le.fit_transform(data['diagnosis'])","89944be3":"data.diagnosis.value_counts(normalize=True)","d0ed7f65":"# Finding correlation among features using sns' heatmap\nplt.figure(figsize=(20,20))\nsns.heatmap(data.corr(),annot=True,cmap='coolwarm')","1d328b34":"# removing features that are less correlated with our target variable\ndata.corr().diagnosis[data.corr().diagnosis<=0.2]","f7d6f343":"less_corr = data.corr().diagnosis[data.corr().diagnosis<=0.2].index","b5f2e5ce":"data = data.drop(less_corr,axis=1)","56d89a16":"data.shape","dd7db67c":"# Standardizing our features except target variable\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler\nstand_scale = data.drop(['diagnosis'],axis = 1)\ncol_trans = make_column_transformer(\n            (StandardScaler(), stand_scale.columns),\n            remainder = 'passthrough')","55b6c359":"from sklearn.model_selection import train_test_split\nX = data.drop(['diagnosis'], axis = 1)\ny = data['diagnosis']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","e30e9d66":"col_trans.fit_transform(X_train)","49563e5d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nlogreg = LogisticRegression(solver='lbfgs')\npipe = make_pipeline(col_trans,logreg)","ee613780":"from sklearn.model_selection import cross_val_score\nprint('Accuracy score on Train data: {}'.format(cross_val_score(pipe, X_train, y_train, cv=10, scoring='accuracy').mean()*100))","759a6a1d":"pipe = make_pipeline(col_trans,logreg)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nfrom sklearn import metrics\nprint('Accuracy score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","c6b7a11d":"from sklearn.neighbors import KNeighborsClassifier\nknn_scores = []\nfor k in range(1,31):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    pipe = make_pipeline(col_trans,knn_classifier)\n    knn_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","7c295203":"plt.figure(figsize=(16,16))\nplt.plot([k for k in range(1, 31)], knn_scores, color = 'red')\nfor i in range(1,31):\n    plt.text(i, knn_scores[i-1], (i, round(knn_scores[i-1]*100,2)))\nplt.xticks([i for i in range(1, 31)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","8fbb65bb":"print('Accuracy score on Train data: {}'.format(knn_scores[4]*100))","7dbae24c":"knn_classifier = KNeighborsClassifier(n_neighbors = 4)\npipe = make_pipeline(col_trans,knn_classifier)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint('Accuracy score on Test Data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","4bd8324d":"from sklearn.svm import SVC\nsvc_scores = []\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nfor i in range(len(kernels)):\n    svc_classifier = SVC(kernel = kernels[i])\n    pipe = make_pipeline(col_trans,svc_classifier)\n    svc_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","0f1b9882":"from matplotlib.cm import rainbow\nimport numpy as np\ncolors = rainbow(np.linspace(0, 1, len(kernels)))\nplt.figure(figsize=(10,7))\nplt.bar(kernels, svc_scores, color = colors)\nfor i in range(len(kernels)):\n    plt.text(i, svc_scores[i], svc_scores[i])\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')","970c406f":"print('Accuracy score on Train data: {}'.format(svc_scores[2]*100))","b1a8b41f":"svc_classifier = SVC(kernel = 'rbf')\npipe = make_pipeline(col_trans,svc_classifier)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint('Accuracy score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","59a41a9d":"from sklearn.tree import DecisionTreeClassifier\ndt_scores = []\nfor i in range(1, len(X.columns) + 1):\n    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n    pipe = make_pipeline(col_trans,dt_classifier)\n    dt_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","b9bed129":"plt.figure(figsize=(10,10))\nplt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\nfor i in range(1, len(X.columns) + 1):\n    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\nplt.xticks([i for i in range(1, len(X.columns) + 1)])\nplt.xlabel('Max features')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different number of maximum features')","014800a0":"print('Accuracy score on Train data: {}'.format(dt_scores[3]*100))","d3635b2b":"dt_classifier = DecisionTreeClassifier(max_features = 4, random_state = 0)\npipe = make_pipeline(col_trans,dt_classifier)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint('Accuracy  score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","62d0a239":"from sklearn.ensemble import RandomForestClassifier\nrf_scores = []\nestimators = [10, 100, 200, 500, 1000]\nfor i in estimators:\n    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n    pipe = make_pipeline(col_trans,rf_classifier)\n    rf_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","ec1f88e0":"plt.figure(figsize=(10,7))\ncolors = rainbow(np.linspace(0, 1, len(estimators)))\nplt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\nfor i in range(len(estimators)):\n    plt.text(i, rf_scores[i], round(rf_scores[i],5))\nplt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\nplt.xlabel('Number of estimators')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for different number of estimators')","f3334bd1":"print('Accuracy score on Train data: {}'.format(rf_scores[4]*100))","f9d5cba5":"rf_classifier = RandomForestClassifier(n_estimators = 1000, random_state = 0)\npipe = make_pipeline(col_trans,rf_classifier)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint('Accuracy score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","aa9950bd":"pipe = make_pipeline(col_trans,logreg)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)","cbcd8376":"print('Confusion Matrix - Training Dataset')\nprint(pd.crosstab(y_train, pipe.predict(X_train), rownames = ['True'], colnames = ['Predicted'], margins = True))","41022f50":"4\/165","e61876c4":"print('Confusion Matrix - Testing Dataset')\nprint(pd.crosstab(y_test, y_pred, rownames = ['True'], colnames = ['Predicted'], margins = True))","c7a73264":"# Checking False Negative Rate\n2\/47","f983e59b":"from sklearn.metrics import precision_score,recall_score,f1_score\nprint('Precision Score: {}'.format(precision_score(y_test,y_pred)));\nprint('Recall Score: {}'.format(recall_score(y_test,y_pred)))\nprint('F1 Score: {}'.format(f1_score(y_test,y_pred)))","9e2d54fb":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, y_pred)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","13b3d75b":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom inspect import signature\n\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\n\n# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\nstep_kwargs = ({'step': 'post'}\n               if 'step' in signature(plt.fill_between).parameters\n               else {})\nplt.step(recall, precision, color='b', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n          average_precision))","bd451c6a":"from sklearn.metrics import roc_auc_score\nprint('ROC AUC Score: {}'.format(roc_auc_score(y_test, y_pred)))","5bb0b406":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test,y_pred)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for breast cancer prediction')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","036bfe9f":"4.25% of observations in testing data are misclassified as Benign","3be60ba3":"### Precision, Recall and F1 Score","9cd2d025":"## Train Test Split","366f994a":"### ROC Curve","72ca110a":"## Classification Models\n\n### 1.Logistic Regression","91ba41a5":"Let's take a look at confusion matrix in out testing dataset","cc3b071b":"## Data Cleaning","c817be99":"## Other Evaluation Metrics","8bef320d":"## Importing Libraries","b9b2b67e":"### 5. Random Forest Classifier","4d89fbc0":"### Precision-Recall Plot","2a87dc9f":"Around 2.42% of observations are misclassified as Benign which actually is Malignant","d35905bb":"### Average Precision Score","31247c24":"You can improve the performance of the model to some more extent by performing Hyper parameter tuning in KNN, Random forest, etc. and append it to the pipeline model and test the model.\n\nThanks for viewing. Please do upvote, if you like my notebook. Any Suggestions are welcome!","b419f8e7":"### 4. Decision Tree Classifier","d229b369":"### 3. Support Vector Classifier (SVC)","1d7f1c34":"### 2. K Nearest Neighbors Classifier","aa98acef":"Logisitc Regression performed well compared with other models. Let's run our model using Logistic Regression and have a look at its confusion matrix","fe6dddc6":"### ROC-AUC Score","4cbd7675":"Let's check our False Negative rate which makes sense especially in health care field. "}}