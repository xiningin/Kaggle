{"cell_type":{"aa078f14":"code","4a95cc04":"code","05472a34":"code","5f8df14b":"code","baf8a2cd":"code","23ecb9c1":"code","9e08fd85":"code","312c851e":"code","6c6fe377":"code","2b722630":"code","011a8974":"markdown","2ee4c43f":"markdown","e54770ba":"markdown","f615514d":"markdown","f5f5cce2":"markdown","740c7bb5":"markdown","f95260c6":"markdown","707507e3":"markdown","1a35e107":"markdown","b116dbb8":"markdown","81852fc2":"markdown"},"source":{"aa078f14":"\"\"\"\nWe need to download the following packages to be used \nin our news scrapping as well as in our \nLanguage Processing task :-\n\n1. BeautifulSoup4 (bs4) - An awesome web scrapping \n  and DOM Data extraction library.\n\n\"\"\"\n!pip3 install bs4","4a95cc04":"\"\"\"\nImporting the following packages - \n\n1. requests - Requests is an inbuilt python package \n  for making a call to a web URL. It allows HTTP\/1.1 \n  requests to be carried out in an easy manner.\n\n2. bs4 - An awesome web scrapping \n  and DOM Data extraction library.\n\n3. nltk - For processing text, NLTK provides us with \n  lots of great functionality built into it.\n\n4. sklearn - It is a popular library having a \n  collection of a numerous Machine Learning Algorithms \n  implemented into it that are just ready to use.\n\n5. collections - It is an inbuilt python library that\n  contains a great collection of special container\n  datatypes.\n\n6. textwrap - It is also an inbuilt package for \n  wrapping up the long text so that it doesn't \n  go out of screen width. (For those who hate \n  horizontal scrolling :p).\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nimport nltk\n\n# Downloading the stopwords and punkt Tokenizer to be used later\nnltk.download('stopwords')\nnltk.download('punkt')\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\nimport collections\nimport textwrap","05472a34":"\"\"\"\nBuilding a word tokenizer to extract all the \ntokens in the given sentence\/text.\n\"\"\"\n\ndef word_tokenizer(text):\n\n        # NLTK provides the way to break text into tokens.\n        # Example - Tokenizing the text \"Man beats coronavirus\"\n        # results in [\"Man\", \"beats\", \"coronavirus\"].\n        # It is more than just a space separator.\n\n        tokenizer = nltk.word_tokenize(text)\n\n        # While analyzing the english words, since we know that\n        # grow, grew, growing, grown and many such words are possible\n        # that mean the same. So we stem the word to its minimal form\n        # in order to get the least length token which is same in all \n        # such word stemming. Eg - stem('grows') => 'grow' ideally\n\n        porter_stemmer = nltk.stem.PorterStemmer()\n\n        # Stopwords is a list of those words which are very commonly used\n        # in the english library and there is not much use of them in\n        # clustering. Eg - The articles do not make much sense and so\n        # they are included in stopwords\n\n        stopwords = nltk.corpus.stopwords.words('english')\n        tokens = []\n\n        for token in tokenizer:\n              if token not in stopwords:\n                    # Stemming the token, if not present in stopwords\n                    # and appending it in tokens list.\n                    stemmed_token = porter_stemmer.stem(token)\n                    tokens.append(stemmed_token)\n\n        # Returning the list of all the tokens\n        return tokens","5f8df14b":"\"\"\"\nBased on features, we will cluster the sentences whose features are \nmost similar to one another.\n\"\"\"\n\ndef cluster_sentences(sentences, nb_of_clusters=10):\n        # A TF-IDF vectorizer is a bag of words model. It works \n        # on the basis of occurence of a term in a document and\n        # the number of documents containing the same term. It \n        # converts a collection of raw text \n        # to a matrix of Term frequency and Inverse \n        # Document Frequency and features.\n\n        tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenizer,\n                                        lowercase=True)\n\n        # Fitting all the sentences\/documents in TF-IDF vectorizer\n        # for text to numerical feature matrix.\n        \n        news_tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n        \n        # Initializing KMeans clustering to segregate the documents\n        # on the basis of TF-IDF scores\n        \n        kmeans = KMeans(n_clusters=nb_of_clusters)\n        cost = []\n\n        # Fitting the TF-IDF matrix in the feature space to split out the\n        # clusters on the basis of pattern between text.\n        \n        kmeans.fit(news_tfidf_matrix)\n\n        # Initializing a dictionary\/map object for storing all the\n        # news clusters.\n\n        clusters = collections.defaultdict(list)\n        \n        # Since, we got a news index and its corresponding cluster label,\n        # we will use it to create the dictionary for clustering the news\n        # by their labels and indices.\n\n        for index, label in enumerate(kmeans.labels_):\n                clusters[label].append(index)\n\n        # Returning the clusters\n\n        return clusters","baf8a2cd":"\"\"\"\nIn precious step, we have created the clusters. Now its time \nto get back news articles using the clusters mapping.\n\"\"\"\n\ndef create_cluster(news_articles, num_clusters = 10):\n    # Creating a list of all the headlines by using the keys of\n    # news articles mapping object.\n\n    headlines = list(news_articles.keys())\n\n    # Creating a list of all the news bodies by using the values of\n    # news articles mapping object.\n\n    news_body = list(news_articles.values())\n\n    # Calling the clustering function we wrote earlier for \n    # getting the cluster labels\n\n    clusters = cluster_sentences(headlines, num_clusters)\n\n    # Creating a map for storing the news articles in order of the\n    # cluster they are assigned to.\n    clustering_results = collections.defaultdict(list)\n\n    news_articles_mapping = {}\n\n    # Creating a map for storing the news articles in order of the\n    # cluster they are assigned to.\n    for cluster_index in range(num_clusters):\n            for index, headline in enumerate(clusters[cluster_index]):\n                  # Appending all the similar grouped articles \n                  # in a single clustered map object.\n                  clustering_results[\"Cluster \"+ \n                                     str(cluster_index+1)].append(headlines[headline])\n                  news_articles_mapping[headlines[headline]] = news_body[headline]\n    return (clustering_results, news_articles_mapping)","23ecb9c1":"# Declaring the url on which we are going to apply scrapping.\n\nurl_to_crawl = 'https:\/\/inshorts.com\/en\/read\/'\n\n# Since, we will be dealing with HTML content, we will be using HTML parser\nparser = 'html.parser'\n\n# Getting the crawled webpage.\nresponse = requests.get(url_to_crawl)\n\n# Converting response to BeatifulSoup object for DOM manipulations\n# and extracting text from it\nbs4obj = BeautifulSoup(response.text, parser)","9e08fd85":"# Printing the bs4 data\n\nprint(bs4obj.prettify()[:1000])","312c851e":"# Now since we have the whole page HTML,\n# we can extract the any text we would wish to like.\n\nall_news_map = {}\n\n# Observing the pattern of the web page, we can see that \n# all news in the webpage are covered under their div tags \n# which contain news-card class.\n\nall_news_cards = bs4obj.findAll('div', {\"class\": \"news-card\"})\n\n# We loop through all news object in order to find the \n# news heading and its corresponding news body.\n\nfor newsObj in all_news_cards:\n    # Finding span tag under newsObj with attribute itemprop \n    # set to description and extracting the value of its content.\n    \n    news_headline = newsObj.find('span', {\"itemprop\": \"description\"})['content']\n    \n    # Finding the div under newsObj having attribute itemprop \n    # set to articleBody and extracting its inner text.\n    \n    news_body = newsObj.find('div', {\"itemprop\": \"articleBody\"}).text\n    \n    # Adding a key with news heading and it body as its value.\n\n    all_news_map[news_headline] = news_body","6c6fe377":"# Its time to get the news articles clustered. So \n# calling the create_cluster function we created \n# earlier for this. For passing on, we have news map \n# object and the maximum number of clusters, we wish\n# the algorithm to output. \n\nclustering_result, news_article_mapping = create_cluster(all_news_map, \n                                                         num_clusters=10)","2b722630":"# Creating a text wrapper object to display the cluster results\n# and prevents it from overflowing.\n\nwrapper = textwrap.TextWrapper(width=100, initial_indent='\\t\\t', subsequent_indent='\\t\\t')\n\n# Looping over all the items in the clustering_result\n\nfor cluster, news_headlines in clustering_result.items():\n    print(cluster)\n    for headline in news_headlines:\n      print('\\t', headline)\n      for text in wrapper.wrap(news_article_mapping[headline]):\n        print(text)\n      print('\\n')","011a8974":"## Clustering the news articles [Headlines and Mapping body]","2ee4c43f":"## Output","e54770ba":"## Starting the crawler","f615514d":"## Building a function for tokenizing text","f5f5cce2":"## Checking the crawled object","740c7bb5":"## Importing the required packages","f95260c6":"## Downloading the additional Packages","707507e3":"# **Web Scrapping**","1a35e107":"## Getting the clustered results","b116dbb8":"## Extracting the headlines and news body from crawled HTML","81852fc2":"## Clustering Sentences"}}