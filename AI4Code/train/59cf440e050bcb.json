{"cell_type":{"f5f8be7a":"code","9cac390f":"code","c8e3ee1e":"code","62a34ce8":"code","9f7e1882":"code","0e3fe79f":"code","0c749eef":"code","98f11152":"code","f60b7e25":"code","5a6b26a3":"code","23264a2a":"code","1802fa57":"code","eab4b72e":"code","ed43c79c":"code","1462d03d":"code","61dd32a7":"code","bf7d69cc":"code","852f09a1":"code","af9fdd83":"code","fbc4ed52":"code","dc3906b5":"code","cf74f591":"code","0b4e0a90":"code","7017bcf0":"markdown","e8bfd40c":"markdown","92148020":"markdown","db508799":"markdown","e6e182f0":"markdown","aea1fe54":"markdown","9d7efe5c":"markdown","3d9a1fd4":"markdown","d10663b9":"markdown","8e6a5b46":"markdown","27aec61c":"markdown"},"source":{"f5f8be7a":"# %%capture is to hide the output\n%%capture\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9cac390f":"%%capture\n!unzip '..\/input\/dogs-vs-cats\/train'\n!unzip '..\/input\/dogs-vs-cats\/test1'","c8e3ee1e":"!mkdir train_","62a34ce8":"import shutil\nimport os\n\nos.mkdir('train_\/cat\/')\nos.mkdir('train_\/dog\/')\n\nfor f in os.listdir('train'):\n    if f.split('.')[0] == 'cat':\n        shutil.move('train\/'+f,'train_\/cat\/'+f)\n    else:\n        shutil.move('train\/'+f,'train_\/dog\/'+f)","9f7e1882":"os.rmdir('train')","0e3fe79f":"print(f\"Number of testing imgaes : {len(os.listdir('test1'))}\")","0c749eef":"import torch\nfrom torchvision import transforms, datasets,models\nimport numpy as np\nimport pandas as pd\nfrom torch import nn,optim\nfrom PIL import Image","98f11152":"train_dir = 'train_'\n\n\n# We create the transforms for train (Data Augmentation)\ntrain_transform = transforms.Compose([\n  transforms.Resize(256),\n  transforms.CenterCrop(224),\n  transforms.RandomRotation(30),\n  transforms.RandomHorizontalFlip(),\n  transforms.ToTensor(),\n  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n\n# Creating Train DataSet\ntrain_dataset = datasets.ImageFolder(train_dir,transform=train_transform)\n\n# Creating a Data Generator (to obtain data in batches)\ntrain_dataloader = torch.utils.data.DataLoader(\n  train_dataset,  \n  batch_size = 128,\n  shuffle = True\n)","f60b7e25":"import matplotlib.pyplot as plt\n\ndef imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax\n\n\n\nimages, labels = next(iter(train_dataloader))\n\ntitle = 'Dog' if labels[0].item() == 1 else 'Cat'\nimshow(images[0])","5a6b26a3":"model = models.vgg16(pretrained=True)\nmodel","23264a2a":"# Freeze our feature parameters as we don't wanna retrain them to the new data\nfor param in model.parameters():\n  param.requires_grad = False","1802fa57":"from collections import OrderedDict\n\nclassifier = nn.Sequential(OrderedDict([\n  # Layer 1\n  ('dropout1',nn.Dropout(0.3)),\n  ('fc1', nn.Linear(25088,500)),\n  ('relu', nn.ReLU()),\n  # output layer\n  ('fc2', nn.Linear(500,2)),\n  ('output', nn.LogSoftmax(dim=1))\n]))\n\n# Attach the classifier to the model\nmodel.classifier = classifier","eab4b72e":"# Loss\ncriterion = nn.NLLLoss()\n\n# Optimizer \noptimizer = optim.Adam(model.classifier.parameters(),lr =0.001)","ed43c79c":"# Lets use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","1462d03d":"# Moving model to GPU\nmodel = model.to(device)","61dd32a7":"from tqdm import tqdm\nepochs = 5\n\n\nfor e in range(epochs):\n  running_loss, total, correct = 0, 0 , 0\n\n  model.train()\n    \n  for images,labels in tqdm(train_dataloader):\n    \n    # Moving input to GPU\n    images, labels = images.to(device), labels.to(device)\n\n    # Forward prop\n    outputs = model(images)\n    loss = criterion(outputs,labels)\n\n    # Backward prop\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Metrics \n    running_loss += loss.item()\n    total += labels.size(0)\n\n    _, predicted = torch.max(torch.exp(outputs).data,1)\n    correct += (predicted == labels).sum().item()\n  else:\n    # Logs \n    print(f'Epoch {e} Training: Loss={running_loss:.5f} Acc={correct\/total * 100:.2f}')\n","bf7d69cc":"# The checkpoints dictionary will consist of necessary details for rebuilding the model with pretrained weights\ncheckpoints = {\n     'pre-trained':'vgg16',\n     'classifier':nn.Sequential(OrderedDict([\n          # Layer 1\n          ('dropout1',nn.Dropout(0.3)),\n          ('fc1', nn.Linear(25088,500)),\n          ('relu', nn.ReLU()),\n          # output layer\n          ('fc2', nn.Linear(500,2)),\n          ('output', nn.LogSoftmax(dim=1))\n    ])),\n    'state_dict':model.state_dict()\n}\n\ntorch.save(checkpoints,'vgg16_catsVdogs.pth')","852f09a1":"# Loading and Rebuilding the saved model\n\ndef load_saved_model(path):\n    \n    # Loading the checkpoint dictionary\n    checkpoint = torch.load(path)\n    \n    # Loading features of the pretrained vgg16\n    model = models.vgg16(pretrained=True)\n    for param in model.parameters():\n        param.requires_grad = False\n        \n    # Reconstruct the classifier by loading the structure from checkpoint\n    model.classifier = checkpoint['classifier']\n    \n    # Loading the weights\n    model.load_state_dict(checkpoints['state_dict'])\n    \n    # Set model to Evaluation mode to avoid training\n    model.eval()\n    \n    return model","af9fdd83":"loaded_model = load_saved_model('vgg16_catsVdogs.pth')\nloaded_model.to(device)","fbc4ed52":"loaded_model","dc3906b5":"# Test Transform\ntest_transform = transforms.Compose([\n          transforms.Resize(256),\n          transforms.CenterCrop(224),\n          transforms.ToTensor(),\n          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n\npredictions = []\n\nfor i in tqdm(range(1,12501)):\n    path = 'test1\/'+str(i)+'.jpg'\n    X = Image.open(path).convert('RGB')\n    X = test_transform(X)[:3,:,:]\n    X = X.unsqueeze(0)\n    X = X.to(device)\n    outputs = loaded_model(X)\n    predictions.append(torch.argmax(outputs).item())\n    ","cf74f591":"# Making the subimission\ndata = {'id':list(range(1,12501)),'label':predictions}\ndf = pd.DataFrame(data)\ndf.shape","0b4e0a90":"df.to_csv('cats-dogs-submission.csv',index=False)","7017bcf0":"# 1. Pre-setup","e8bfd40c":"**Prediction**","92148020":"# 4. Train the Model","db508799":"**Save the Model**","e6e182f0":"# 3. Loading the Pre-Trained model","aea1fe54":"We can see that the model has to major components which are the *Features* and the *Classifier*.\n\nThe *Features* are obtained by passing the image throught the many Convolutions \n\nThe *Classifier* is built and trained to the \"Imagenet\" dataset so it has about 1000 neurons in the output layer as it can map upto 1000 different classes.\n\nFor out Cats-Dogs classifier we need to train a binary classifier and can train a new classifier from scratch which uses the features learnt from the \"Imagenet\" dataset of the Densenet121","9d7efe5c":"Lets now build our binary classifier","3d9a1fd4":"We are going to be using the Densenet 121 model","d10663b9":"**Testing and creating submission**","8e6a5b46":"# 2. Loading the Data","27aec61c":"Lets Create DataLoaders along with some Data Augmentation on the Input data"}}