{"cell_type":{"bd2cd372":"code","8f129e0f":"code","64624c3f":"code","255a287d":"code","87f32d77":"code","4ed07936":"code","434770f7":"code","9a95a9ea":"code","0e00729e":"code","8aac9bc0":"code","472f31ef":"code","ca0f0052":"code","dcbb07c5":"code","716f0896":"code","f2a6c0e6":"code","1cc2a850":"code","94699070":"code","fb22bc6c":"code","0da7f1e5":"code","80e12b67":"code","6d93d746":"code","c66695a2":"code","6e0de4ba":"code","0989c562":"code","24b815c1":"code","f30c5dd9":"code","1b321829":"code","27d2d2fc":"code","f020ccee":"code","ff6f1731":"code","e4823691":"code","fbc69a9e":"code","31c35536":"code","7ad3b8f9":"code","9d98e6cb":"code","b9168163":"code","2b646110":"code","aab1aefe":"code","d71910ae":"code","48d7e392":"code","4f5d0468":"code","68c1370d":"code","90a32e39":"code","f1de8ac1":"code","d45a6949":"code","3c07e2e2":"code","8986d6bf":"code","67f7c8fb":"code","c92aa827":"code","5af3cac4":"code","f2e3881c":"code","47b17d77":"code","2307f3c1":"code","a5337f62":"markdown","dc03d947":"markdown","6ea29e84":"markdown","650884cd":"markdown","0122dc0c":"markdown","97a2a719":"markdown","cf91e41d":"markdown","81c4ff9b":"markdown","39799a9b":"markdown","cab06b80":"markdown","19f80b4e":"markdown","7c8e6042":"markdown"},"source":{"bd2cd372":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom matplotlib import pyplot\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 15, 6","8f129e0f":"data = pd.read_csv('..\/input\/air-passengers\/AirPassengers.csv')\nprint (data.head())\nprint ('\\n Data Types:')\nprint (data.dtypes)","64624c3f":"# The data contains a particular month and number of passengers travelling in that month. \n#In order to read the data as a time series, we have to pass special arguments to the \n#read_csv command:\ndateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m')\ndata = pd.read_csv('..\/input\/air-passengers\/AirPassengers.csv', parse_dates=['Month'], \n                   index_col='Month',date_parser=dateparse)\nprint ('\\n Parsed Data:')\nprint (data.head())","255a287d":"data.index","87f32d77":"#Convert to timeseries\nts = data['#Passengers']\nts.head(10)","4ed07936":"#Indexing time series arrays\nts['1949-01-01']","434770f7":"#Import datetime library and use 'datetime' function\nfrom datetime import datetime\nts[datetime(1949,1,1)]","9a95a9ea":"#GET RANGE\n#PLOTTING THE TIME SERIES\nplt.plot(ts)\n","0e00729e":"from pandas import read_csv\n#series = read_csv('international-airline-passengers.csv', header=0, index_col=0)\nX_new = data.values\nsplit = len(X_new) \/ 2\nX1, X2 = X_new[0:int(split)], X_new[int(split):]\nmean1, mean2 = X1.mean(), X2.mean()\nvar1, var2 = X1.var(), X2.var()\nprint('mean1=%f, mean2=%f' % (mean1, mean2))\nprint('variance1=%f, variance2=%f' % (var1, var2))","8aac9bc0":"from pandas import read_csv\nfrom matplotlib import pyplot\ndata.hist()\npyplot.show()","472f31ef":"from numpy import log\nX_log = log(data.values)\npyplot.hist(X_log)\npyplot.show()\npyplot.plot(X_log)\npyplot.show()","ca0f0052":"#We can now calculate the mean and standard deviation of the values of the log transformed dataset.\nX_new_log = data.values\nX_new_log = log(X_new_log)\nsplit_log = len(X_new_log) \/ 2\nX1_log, X2_log = X_new_log[0:int(split_log)], X_new_log[int(split_log):]\nmean1_log, mean2_log = X1_log.mean(), X2_log.mean()\nvar1_log, var2_log = X1_log.var(), X2_log.var()\nprint('mean1=%f, mean2=%f' % (mean1_log, mean2_log))\nprint('variance1=%f, variance2=%f' % (var1_log, var2_log))","dcbb07c5":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n    \n    #Determing rolling statistics\n    #rolmean = pd.rolling_mean(timeseries, window=12)\n    rolmean = pd.Series(timeseries).rolling(window=12).mean()\n    #rolstd = pd.rolling_std(timeseries, window=12)\n    rolstd = pd.Series(timeseries).rolling(window=12).std()\n\n    #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)","716f0896":"test_stationarity(ts)","f2a6c0e6":"#MAKING TS STATIONARY\n\nts_log = np.log(ts)\nplt.plot(ts_log)\n","1cc2a850":"test_stationarity(ts_log)","94699070":"#Smoothing\nmoving_avg = pd.Series(ts_log).rolling(window=12).mean()\nplt.plot(ts_log)\nplt.plot(moving_avg, color='red')","fb22bc6c":"ts_log_moving_avg_diff = ts_log - moving_avg\nts_log_moving_avg_diff.head(5)","0da7f1e5":"ts_log_moving_avg_diff.dropna(inplace=True)\nts_log_moving_avg_diff.head(5)","80e12b67":"#TEST STATIONARITY AGAIN\ntest_stationarity(ts_log_moving_avg_diff)","6d93d746":"expwighted_avg = ts_log.ewm(span=12).mean() \nplt.plot(ts_log)\nplt.plot(expwighted_avg, color='red')","c66695a2":"ts_log_ewma_diff = ts_log - expwighted_avg\ntest_stationarity(ts_log_ewma_diff)","6e0de4ba":"ts_log_diff = ts_log - ts_log.shift()\nplt.plot(ts_log_diff)","0989c562":"ts_log_diff.dropna(inplace=True)\ntest_stationarity(ts_log_diff)","24b815c1":"#DECOMPOSITION\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts_log)","f30c5dd9":"trend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid","1b321829":"plt.subplot(411)\nplt.plot(ts_log, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","27d2d2fc":"ts_log_decompose = residual\nts_log_decompose.dropna(inplace=True)\ntest_stationarity(ts_log_decompose)","f020ccee":"#FINAL FORECASTING\nfrom statsmodels.tsa.arima_model import ARIMA","ff6f1731":"#ACF\/PACF PLOTS\n\n#ACF and PACF plots:\nfrom statsmodels.tsa.stattools import acf, pacf  ","e4823691":"lag_acf = acf(ts_log_diff, nlags=20)\nlag_pacf = pacf(ts_log_diff, nlags=20, method='ols')","fbc69a9e":"#Plot ACF:    \nplt.subplot(121)    \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')","31c35536":"#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","7ad3b8f9":"ts_values=ts_log.values","9d98e6cb":"X = ts_values\nsize = int(len(X) * 0.667)\ntrain, test = X[0:size], X[size:len(X)]","b9168163":"#MODEL BUILDING\n#training will be 66%, test will be 33% as per our model\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom numpy.linalg import LinAlgError\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2b646110":"history = [x for x in train]\npredictions = list()\n#test.reset_index()","aab1aefe":"for t in range(len(test)):\n    try:\n        model = ARIMA(history, order=(4,1,2))\n        model_fit = model.fit(disp=0)\n        output = model_fit.forecast()\n        yhat = output[0]\n        predictions.append(yhat)\n        obs = test[t]\n        history.append(obs)\n    except (ValueError, LinAlgError):\n        pass\n    print('predicted=%f, expected=%f' % (yhat, obs))\nerror = mean_squared_error(test, predictions)\nrmse = mean_squared_error(test, predictions)**0.5\nprint('Test MSE: %.3f' % rmse)","d71910ae":"#CHECKING ERROR\nfrom math import sqrt\nrms = sqrt(mean_squared_error(test, predictions))","48d7e392":"# plot\npyplot.plot(test, color = 'blue', label='test')   \npyplot.plot(predictions, color='red', label='pred')\npyplot.show()","4f5d0468":"# plot\npyplot.plot(np.exp(test), color = 'blue', label='test')   \npyplot.plot(np.exp(predictions), color='red', label='pred')\npyplot.show()","68c1370d":"from math import sqrt\nrms = sqrt(mean_squared_error(np.exp(test), np.exp(predictions)))\nprint('Mean Squarred Error: %.2f'% rms)","90a32e39":"!pip install pmdarima\nfrom pmdarima.arima import auto_arima","f1de8ac1":"arima_model = auto_arima(train, start_p=1, start_q=1, d=1, max_p=4, max_q=4, start_P=1, \n                         D=None, start_Q=1, max_P=4, max_D=1, max_Q=4, max_order=5, m=1, \n                         seasonal=True, stationary=False, information_criterion='aic', \n                         alpha=0.05, test='kpss', seasonal_test='ocsb', stepwise=True, \n                         n_jobs=1, start_params=None, trend=None, method='lbfgs', \n                         maxiter=50, offset_test_args=None, seasonal_test_args=None, \n                         suppress_warnings=True, error_action='trace', trace=False, \n                         random=False, random_state=None, n_fits=10, \n                         return_valid_fits=False, out_of_sample_size=0, \n                         scoring='mse', scoring_args=None, with_intercept='auto', \n                         sarimax_kwargs=None)\n","d45a6949":"arima_model.summary()","3c07e2e2":"prediction_arima_model = pd.DataFrame(arima_model.predict(n_periods=len(test)))\nprediction_arima_model.columns = ['predicted_sales']","8986d6bf":"np.exp(prediction_arima_model)  ","67f7c8fb":"plt.figure(figsize=(8,5))\n#plt.plot(train, label='Training')\nplt.plot(np.exp(test), label='Test')\nplt.plot(np.exp(prediction_arima_model), label='Predictions')\nplt.legend(loc = 'upper left')\nplt.show()\n","c92aa827":"import warnings\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error","5af3cac4":"# evaluate an ARIMA model for a given order (p,d,q)\ndef evaluate_arima_model(X, arima_order):\n    # prepare training dataset\n    train_size = int(len(X) * 0.8)\n    train, test = X[0:train_size], X[train_size:]\n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    for t in range(len(test)):\n        model = ARIMA(history, order=arima_order)\n        model_fit = model.fit(disp=0)\n        yhat = model_fit.forecast()[0]\n        predictions.append(yhat)\n        history.append(test[t])\n    # calculate out of sample error\n    error = mean_squared_error(test, predictions)\n    return error","f2e3881c":"# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype('float32')\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mse = evaluate_arima_model(dataset, order)\n                    if mse < best_score:\n                        best_score, best_cfg = mse, order\n                    print('ARIMA%s MSE=%.3f' % (order,mse))\n                except:\n                    continue\n    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))","47b17d77":"# load dataset\ndef parser(x):\n    return datetime.strptime('190'+x, '%Y-%m')","2307f3c1":"import datetime\nprint(datetime.datetime.now())\np_values = [1,2,3,4,5]\nd_values = [0,1]\nq_values = [1,2,3]\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(train, p_values, d_values, q_values)\nprint(datetime.datetime.now())","a5337f62":"## STATISTICAL TEST","dc03d947":"- Reviewing the plot of the time series again, we can see that there is an obvious seasonality component, \n    and it looks like the seasonality component is growing.\n- This may suggest an exponential growth from season to season. \n- A log transform can be used to flatten out exponential change back to a linear relationship.","6ea29e84":"## Grid Search","650884cd":"- Values not looking like Gaussian, therefore mean & variance values are less meaningful\n- This squashed distribution of the observations may be another indicator of a non-stationary time series.","0122dc0c":"## AUTO ARIMA","97a2a719":"- This is a quick and dirty method that may be easily fooled.","cf91e41d":"## SUMMARY STATISTICS","81c4ff9b":"# Autoregressive Integrated Moving Average (ARIMA)\nAn autoregressive integrated moving average model is a form of regression analysis that gauges the strength of one dependent variable relative to other changing variables. The model's goal is to predict future securities or financial market moves by examining the differences between values in the series instead of through actual values.","39799a9b":"### Augmented Dickey Fuller Test","cab06b80":"#### To identify the best combination of (p,d,q), please run the last part of this code (Grid Search)","19f80b4e":"##########################################################################\n##########################################################################\n####################DETERMINE PDQ VALUES - DONOT RUN###################\n##########################################################################\n##########################################################################","7c8e6042":"- We can use a statistical test to check if the difference between two samples of Gaussian random \n    variables is real or a statistical fluke. We could explore statistical significance tests, like the \n    Student t-test, but things get tricky because of the serial correlation between values."}}