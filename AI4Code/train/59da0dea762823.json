{"cell_type":{"51a6bc13":"code","8e4a4313":"code","7c5ba4bb":"code","86fe2bb0":"code","30520e78":"code","049ad659":"code","a34a3c86":"code","e676f924":"code","13f04b52":"code","7efc5745":"code","6f059a63":"code","da3a4f8e":"code","b38b41c2":"code","84cccfe4":"code","5a51fc15":"code","b7738680":"code","a379c7cf":"code","eb2c1113":"code","c3b40b79":"code","cff65426":"code","2a0a02dd":"code","9e8ac13b":"code","7834351d":"code","295f7fac":"code","c14e4c19":"code","7ec93a1e":"code","98c4dd17":"code","3db6e1ce":"code","eb3c768d":"markdown","5d0eda97":"markdown","865f710e":"markdown","61c86083":"markdown","cc3bcfdc":"markdown","26cd3871":"markdown","6fba5e3a":"markdown","0e028ab2":"markdown","f02ba2b2":"markdown","0506663d":"markdown","e0bf151e":"markdown","ef076588":"markdown","525d13d9":"markdown","57f5c3af":"markdown","76a1eca9":"markdown","9e1cc800":"markdown","880e0365":"markdown","3cc4d87e":"markdown","8b59f4d8":"markdown","0f959144":"markdown","43c68edb":"markdown","58a66b39":"markdown"},"source":{"51a6bc13":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport os\n%matplotlib inline","8e4a4313":"# 8 int64 values\nmy_integers=tf.train.Feature(int64_list=tf.train.Int64List(value=np.arange(8)))\nmy_integers","7c5ba4bb":"# A single float value\nmy_floats=tf.train.Feature(float_list=tf.train.FloatList(value=[3.14]))\nmy_floats","86fe2bb0":"# A range of bytes. Note that strings must be .encode()'ed\ntxt='This string must be encoded'\nmy_txt=tf.train.Feature(bytes_list=tf.train.BytesList(value=[txt.encode()]))\nmy_txt","30520e78":"features=tf.train.Features(feature={\n    'integers': my_integers,\n    'pi': my_floats,\n    'description': my_txt})\nfeatures","049ad659":"tf_record = tf.train.Example(features=features)\ntf_record","a34a3c86":"fname='example1.tfrecord'\nwith tf.io.TFRecordWriter(fname) as writer:\n    writer.write(tf_record.SerializeToString())\nprint(\"Size of {} is {}bytes\".format(fname, os.path.getsize(fname)))","e676f924":"dataset = tf.data.TFRecordDataset(fname)\nraw_example = next(iter(dataset)) # only one example in this file\nparsed = tf.train.Example.FromString(raw_example.numpy())\nparsed","13f04b52":"# strings must be decoded\nparsed.features.feature['description'].bytes_list.value[0].decode() ","7efc5745":"parsed.features.feature['integers'].int64_list.value[:] # get all data","6f059a63":"parsed.features.feature['integers'].int64_list.value[5] # get a single value","da3a4f8e":"parsed.features.feature['pi'].float_list.value[0] # get the value","b38b41c2":"parsed.features.feature['pi'].float_list.value[:] # get the value as a list","84cccfe4":"import hashlib\nfrom io import BytesIO\nfrom PIL import Image, ImageFont, ImageDraw\nARTAXOR_PATH = '\/kaggle\/input\/arthropod-taxonomy-orders-object-detection-dataset\/'\n\npickles='\/kaggle\/input\/starter-arthropod-taxonomy-orders-data-exploring\/'\nobjectdf=pd.read_pickle(pickles+'ArTaxOr_objects.pkl')\nlabels=pd.read_pickle(pickles+'ArTaxOr_labels.pkl')\nobjectdf.sample(5)","5a51fc15":"# Fetch attribution string from image EXIF data\ndef get_attribution(file):\n    with Image.open(file) as img:\n        exif_data = img._getexif()\n    s='Photo: unknown'\n    if exif_data is not None:\n        if 37510 in exif_data:\n            if len(exif_data[37510]) > 0:\n                s = exif_data[37510][8:].decode('ascii')\n        if 315 in exif_data:\n            if len(exif_data[315]) > 0:\n                s = 'Photo: ' + exif_data[315]\n    return s\n\n# Create example for TensorFlow Object Detection API\ndef create_tf_example(imagedf, longest_edge=1024):  \n    fname = ARTAXOR_PATH+imagedf.file.iloc[0]\n    filename=fname.split('\/')[-1] # exclude path\n    by = get_attribution(fname)\n    img = Image.open(fname, \"r\")\n    # resize image if larger that longest edge while keeping aspect ratio\n    if max(img.size) > longest_edge:\n        img.thumbnail((longest_edge, longest_edge), Image.ANTIALIAS)\n    height = img.size[1] # Image height\n    width = img.size[0] # Image width\n    buf= BytesIO()\n    img.save(buf, format= 'JPEG') # encode to jpeg in memory\n    encoded_image_data= buf.getvalue()\n    image_format = b'jpeg'\n    source_id = filename.split('.')[0]\n    license = 'CC BY-NC-SA 4.0'\n    # A hash of the image is used in some frameworks\n    key = hashlib.sha256(encoded_image_data).hexdigest()   \n    # object bounding boxes \n    xmins = imagedf.left.values # List of normalized left x coordinates in bounding box (1 per box)\n    xmaxs = imagedf.right.values # List of normalized right x coordinates in bounding box\n    ymins = imagedf.top.values # List of normalized top y coordinates in bounding box (1 per box)\n    ymaxs = imagedf.bottom.values # List of normalized bottom y coordinates in bounding box\n    # List of string class name & id of bounding box (1 per box)\n    object_cnt = len(imagedf)\n    classes_text = []\n    classes = []\n    for i in range(object_cnt):\n        classes_text.append(imagedf.label.iloc[i].encode())\n        classes.append(1+imagedf.label_idx.iloc[i])\n    # unused features from Open Image \n    depiction = np.zeros(object_cnt, dtype=int)\n    group_of = np.zeros(object_cnt, dtype=int)\n    occluded = imagedf.occluded.values #also Pascal VOC\n    truncated = imagedf.truncated.values # also Pascal VOC\n    # Pascal VOC\n    view_text = []\n    for i in range(object_cnt):\n        view_text.append('frontal'.encode())\n    difficult = np.zeros(object_cnt, dtype=int)\n\n    tf_record = tf.train.Example(features=tf.train.Features(feature={\n        'image\/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n        'image\/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n        'image\/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename.encode()])),\n        'image\/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[source_id.encode()])),\n        'image\/license': tf.train.Feature(bytes_list=tf.train.BytesList(value=[license.encode()])),\n        'image\/by': tf.train.Feature(bytes_list=tf.train.BytesList(value=[by.encode()])),\n        'image\/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_image_data])),\n        'image\/key\/sha256': tf.train.Feature(bytes_list=tf.train.BytesList(value=[key.encode()])),\n        'image\/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n        'image\/object\/bbox\/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n        'image\/object\/bbox\/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n        'image\/object\/bbox\/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n        'image\/object\/bbox\/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n        'image\/object\/class\/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n        'image\/object\/class\/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n        'image\/object\/depiction': tf.train.Feature(int64_list=tf.train.Int64List(value=depiction)),\n        'image\/object\/group_of': tf.train.Feature(int64_list=tf.train.Int64List(value=group_of)),\n        'image\/object\/occluded': tf.train.Feature(int64_list=tf.train.Int64List(value=occluded)),\n        'image\/object\/truncated': tf.train.Feature(int64_list=tf.train.Int64List(value=truncated)),\n        'image\/object\/difficult': tf.train.Feature(int64_list=tf.train.Int64List(value=difficult)),\n        'image\/object\/view': tf.train.Feature(bytes_list=tf.train.BytesList(value=view_text))\n    }))\n    return tf_record","b7738680":"sample_file='ArTaxOr\/Lepidoptera\/002b37ac08e1.jpg'\nimagedf=objectdf[objectdf.file == sample_file]\ntfr=create_tf_example(imagedf)\nfname='.\/image_ex1.tfrecord'\nwith tf.io.TFRecordWriter(fname) as writer:\n    writer.write(tfr.SerializeToString())\nprint(\"Size of {} is {}kbytes\".format(fname, os.path.getsize(fname)\/\/1024))","a379c7cf":"# Some helper functions to draw image with object boundary boxes\nfontname = '\/usr\/share\/fonts\/truetype\/dejavu\/DejaVuSans.ttf'\nfont = ImageFont.truetype(fontname, 40) if os.path.isfile(fontname) else ImageFont.load_default()\n\ndef bbox(img, xmin, ymin, xmax, ymax, color, width, label, score):\n    draw = ImageDraw.Draw(img)\n    xres, yres = img.size[0], img.size[1]\n    box = np.multiply([xmin, ymin, xmax, ymax], [xres, yres, xres, yres]).astype(int).tolist()\n    txt = \" {}: {}%\" if score >= 0. else \" {}\"\n    txt = txt.format(label, round(score, 1))\n    ts = draw.textsize(txt, font=font)\n    draw.rectangle(box, outline=color, width=width)\n    if len(label) > 0:\n        if box[1] >= ts[1]+3:\n            xsmin, ysmin = box[0], box[1]-ts[1]-3\n            xsmax, ysmax = box[0]+ts[0]+2, box[1]\n        else:\n            xsmin, ysmin = box[0], box[3]\n            xsmax, ysmax = box[0]+ts[0]+2, box[3]+ts[1]+1\n        draw.rectangle([xsmin, ysmin, xsmax, ysmax], fill=color)\n        draw.text((xsmin, ysmin), txt, font=font, fill='white')\n\ndef plot_img(img, axes, xmin, ymin, xmax, ymax, classes, class_label, by):\n    for i in range(len(xmin)):\n        color=labels.color[class_label[i]-1]\n        bbox(img, xmin[i], ymin[i], xmax[i], ymax[i], color, 5, classes[i].decode(), -1)\n    plt.setp(axes, xticks=[], yticks=[])\n    axes.set_title(by)\n    plt.imshow(img)","eb2c1113":"# load tfrecord\nfname='image_ex1.tfrecord'\ndataset = tf.data.TFRecordDataset(fname)\nimg_example = next(iter(dataset)) \nimg_parsed = tf.train.Example.FromString(img_example.numpy())\n# only extract features we will actually use\nxmin=img_parsed.features.feature['image\/object\/bbox\/xmin'].float_list.value[:]\nxmax=img_parsed.features.feature['image\/object\/bbox\/xmax'].float_list.value[:]\nymin=img_parsed.features.feature['image\/object\/bbox\/ymin'].float_list.value[:]\nymax=img_parsed.features.feature['image\/object\/bbox\/ymax'].float_list.value[:]\nby=img_parsed.features.feature['image\/by'].bytes_list.value[0].decode()\nclasses=img_parsed.features.feature['image\/object\/class\/text'].bytes_list.value[:]\nclass_label=img_parsed.features.feature['image\/object\/class\/label'].int64_list.value[:]\nimg_encoded=img_parsed.features.feature['image\/encoded'].bytes_list.value[0]","c3b40b79":"fig = plt.figure(figsize=(10,10))\naxes = axes = fig.add_subplot(1, 1, 1)\nimg = Image.open(BytesIO(img_encoded))\nplot_img(img, axes, xmin, ymin, xmax, ymax, classes, class_label, by)","cff65426":"def create_tf_example2(imagedf, longest_edge=1024):  \n    # Filename of the image (full path is useful when there are multiple image directories)\n    fname = ARTAXOR_PATH+imagedf.file.iloc[0]\n    filename=fname.split('\/')[-1] # exclude path\n    by = get_attribution(fname)\n    img = Image.open(fname, \"r\")\n    source_id = filename.split('.')[0]\n    # resize image if larger that longest edge while keeping aspect ratio\n    if max(img.size) > longest_edge:\n        img.thumbnail((longest_edge, longest_edge), Image.ANTIALIAS)\n    image_data = np.asarray(img)\n    # storing shape will make it easy to reconstruct image later\n    image_shape = np.array(image_data.shape)\n    # convert to float\n    image_data = image_data.reshape(image_data.shape[0]*image_data.shape[1]*image_data.shape[2])\n    image_data = image_data.astype(float)\/255. # normalize to [0,1]\n    # object bounding boxes \n    xmins = imagedf.left.values # List of normalized left x coordinates in bounding box (1 per box)\n    xmaxs = imagedf.right.values # List of normalized right x coordinates in bounding box\n    ymins = imagedf.top.values # List of normalized top y coordinates in bounding box (1 per box)\n    ymaxs = imagedf.bottom.values # List of normalized bottom y coordinates in bounding box\n    # List of string class name & id of bounding box (1 per box)\n    classes_text = []\n    classes = []\n    for i in range(len(imagedf)):\n        classes_text.append(imagedf.label.iloc[i].encode())\n        classes.append(1+imagedf.label_idx.iloc[i])\n\n    tf_record = tf.train.Example(features=tf.train.Features(feature={\n        'image\/shape': tf.train.Feature(int64_list=tf.train.Int64List(value=image_shape)),\n        'image\/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename.encode()])),\n        'image\/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[source_id.encode()])),\n        'image\/by': tf.train.Feature(bytes_list=tf.train.BytesList(value=[by.encode()])),\n        'image\/data': tf.train.Feature(float_list=tf.train.FloatList(value=image_data)),\n        'image\/object\/bbox\/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n        'image\/object\/bbox\/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n        'image\/object\/bbox\/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n        'image\/object\/bbox\/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n        'image\/object\/class\/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n        'image\/object\/class\/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes))\n    }))\n    return tf_record","2a0a02dd":"sample_file2='ArTaxOr\/Hymenoptera\/ab30b4c2f70c.jpg'\nimagedf=objectdf[objectdf.file == sample_file2]\ntfr2 = create_tf_example2(imagedf)\nfname2 = 'image_ex2.tfrecord'\nwith tf.io.TFRecordWriter(fname2) as writer:\n    writer.write(tfr2.SerializeToString())\nprint(\"Size of {} is {}kbytes\".format(fname2, os.path.getsize(fname2)\/\/1024))","9e8ac13b":"dataset2 = tf.data.TFRecordDataset(fname2)\nimg_example2 = next(iter(dataset2)) \nimg_parsed2 = tf.train.Example.FromString(img_example2.numpy())\n# extract features\nxmin=img_parsed2.features.feature['image\/object\/bbox\/xmin'].float_list.value[:]\nxmax=img_parsed2.features.feature['image\/object\/bbox\/xmax'].float_list.value[:]\nymin=img_parsed2.features.feature['image\/object\/bbox\/ymin'].float_list.value[:]\nymax=img_parsed2.features.feature['image\/object\/bbox\/ymax'].float_list.value[:]\nby=img_parsed2.features.feature['image\/by'].bytes_list.value[0].decode()\nclasses=img_parsed2.features.feature['image\/object\/class\/text'].bytes_list.value[:]\nclass_label=img_parsed2.features.feature['image\/object\/class\/label'].int64_list.value[:]\nimg_shape=img_parsed2.features.feature['image\/shape'].int64_list.value[:]\nimg_data=img_parsed2.features.feature['image\/data'].float_list.value[:]","7834351d":"image2=np.array(img_data).reshape(img_shape) # reshape\nimage2=image2*255. # scale back to [0, 255] and convert to int\nimage2=image2.astype(int)\nimg=Image.fromarray(np.uint8(image2))\nfig = plt.figure(figsize=(10,10))\naxes = axes = fig.add_subplot(1, 1, 1)\nplot_img(img, axes, xmin, ymin, xmax, ymax, classes, class_label, by)","295f7fac":"filelist=pd.read_pickle(pickles+'ArTaxOr_filelist.pkl')\nfilelist=filelist.sample(frac=1)\nfilelist.head()","c14e4c19":"%%time\nimport contextlib2\n\ndef open_sharded_tfrecords(exit_stack, base_path, num_shards):\n    tf_record_output_filenames = [\n        '{}-{:05d}-of-{:05d}.tfrecord'.format(base_path, idx, num_shards)\n        for idx in range(num_shards)\n        ]\n    tfrecords = [\n        exit_stack.enter_context(tf.io.TFRecordWriter(file_name))\n        for file_name in tf_record_output_filenames\n    ]\n    return tfrecords\n\nnum_shards=50\noutput_filebase='.\/ArTaxOr'\n\nwith contextlib2.ExitStack() as tf_record_close_stack:\n    output_tfrecords = open_sharded_tfrecords(tf_record_close_stack, output_filebase, num_shards)\n    for i in range(len(filelist)):\n        ldf=objectdf[objectdf.id == filelist.id.iloc[i]].reset_index()\n        tf_record = create_tf_example(ldf, longest_edge=1280)\n        output_shard_index = i % num_shards\n        output_tfrecords[output_shard_index].write(tf_record.SerializeToString())","7ec93a1e":"!ls -lh ArTaxOr*.tfrecord","98c4dd17":"labels=pd.read_pickle(pickles+'ArTaxOr_labels.pkl')\npbfile=open('.\/ArTaxOr.pbtxt', 'w') \nfor i in range (len(labels)): \n    pbfile.write('item {{\\n id: {}\\n name:\\'{}\\'\\n}}\\n\\n'.format(i+1, labels.name[i])) \npbfile.close()","3db6e1ce":"fname='.\/ArTaxOr-00029-of-00050.tfrecord' \ndataset3 = tf.data.TFRecordDataset(fname)\nfig = plt.figure(figsize=(16,18))\nidx=1\nfor raw_record in dataset3.take(6):\n    axes = fig.add_subplot(3, 2, idx)\n    example = tf.train.Example()\n    example.ParseFromString(raw_record.numpy())\n    xmin=example.features.feature['image\/object\/bbox\/xmin'].float_list.value[:]\n    xmax=example.features.feature['image\/object\/bbox\/xmax'].float_list.value[:]\n    ymin=example.features.feature['image\/object\/bbox\/ymin'].float_list.value[:]\n    ymax=example.features.feature['image\/object\/bbox\/ymax'].float_list.value[:]\n    by=example.features.feature['image\/by'].bytes_list.value[0].decode()\n    classes=example.features.feature['image\/object\/class\/text'].bytes_list.value[:]\n    class_label=example.features.feature['image\/object\/class\/label'].int64_list.value[:]\n    img_encoded=example.features.feature['image\/encoded'].bytes_list.value[0]\n    img = Image.open(BytesIO(img_encoded))\n    plot_img(img, axes, xmin, ymin, xmax, ymax, classes, class_label, by)\n    idx=idx+1","eb3c768d":"# TensorFlow TFRecords Demystified\nTensorFlow TFRecords have a reputation of being both clunky and very large. But that is just a reputation. In this kernel we will learn how to master writing and reading TFRecords, and control exactly how large or small these files are.  \nIn this kernel we will:\n  * Create basic TFRecords (write & read)\n  * Create TFRecords with image data (write & read)\n  * Create TFRecords for a large image dataset using sharding","5d0eda97":"Finally, we can write this record to a file using `tf.io.TFRecordWriter`. ","865f710e":"## Sharding large datasets\nSharding is a method of splitting and storing a large dataset into multiple files. The last part of this kernel we will convert the entire ArTaxOr dataset into TFRecords.","61c86083":"Right! Storing image data as float gives a filesize of almost 10MB while the jpeg-encoded one was only 104kB. But now all preprocessing steps are complete, and the image can be fed directly to training (or possibly agumentation). Now, let's read it back in and display. ","cc3bcfdc":"If we are going to use TF Object Detection API, a label definition file is also needed:","26cd3871":"## Summary\nWe have seen how TFRecords can store just about any type of data in a format that makes it easy to read using key-value pairs. And new keys (or features) can be added without breaking backwards compatibility. When image data is written into TFRecords, the size of the TFRecord is directly dependent on how the image data is stored (encoded jpeg vs. float arrays).","6fba5e3a":"Awesome! We just created a TFRecord, wrote it to a file, read it back in and accessed the features. If you are writing you own training code and input function, you can name the features to anything. If you want to create TFRecords as input to someone else's code, make sure all the required keys are present, otherwise there will be plenty of errors. Unfortunately, documentation of the expected TFRecord format is often hard to find.","0e028ab2":"Again, read a few records from one of the `.tfrecord` files to check that everything is OK.","f02ba2b2":"Yes - it works! But putting jpeg-encoded images in TFRecords does not release the full potential of TFRecords. If you are training on a machine with fast I\/O and lots of disk space, it is better to do all the preprocessing before storing the image as normalized float array. Let's define another function for this:","0506663d":"Create an example record and save it to file. The example file happens to be the ArTaxOr dataset logo which contains a butterfly and a bumblebee.","e0bf151e":"## Using TFRecords with image data\nRight, time to create a more advanced TFRecord, this time with image data. There is kind of an established way to name keys for image data. To make input function efficient, all preprocessing steps should be performed when creating TFRecords, otherwise the input function must repeat preprocessing each time the training passed over the TFRecord. Typical preprocessing steps are:  \n  * Decode image from jpg etc.\n  * Resize image\n  * Convert to float\n  * Normalize to [0,1] range  \n  \nAugumentation is *not* something to do in a TFRecord, since agumentation could be different each time training passes over the data. We will make a TFrecord that is compatible with [TensorFlow Object Detection API](https:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/object_detection).  \nWe are going to get images from the [ArTaxOr dataset](https:\/\/www.kaggle.com\/mistag\/arthropod-taxonomy-orders-object-detection-dataset), and the object data is imported from a pickled dataframe created in the [starter kernel](https:\/\/www.kaggle.com\/mistag\/starter-arthropod-taxonomy-orders-data-exploring).","ef076588":"Now we can access each feature using `parsed.features.feature['<feature name>']`:","525d13d9":"## Protocol buffers\nTFRecords are [Protocol buffers](https:\/\/developers.google.com\/protocol-buffers\/docs\/overview). Protocol buffers are a way of serializing structured data in a compact and efficient way. A Protocol buffer message is defined by a `.proto` file. Example:  \n```\nsyntax = \"proto3\"; \/\/ define protocol version\n\n\/\/ Define \"Employee\" message with 3 fields\nmessage Employee {\n string name = 1; \/\/ a unique number is assigned to each field\n int32 company_id = 2;\n string address = 3;\n}\n```  \nA single `.proto`-file can define multiple messages and messages can be nested. The beauty of protocol buffers is that the `.proto`-files can be compiled into a library (language of choice) with access functions using the `protoc` compiler. But for TFRecords the access functions we need are already part of TensorFlow, so no need to worry about compiling `.proto` files.","57f5c3af":"Now that the `.tfrecord` file is stored, let's load it back in and visualize the contents.","76a1eca9":"Define a function that creates a `tf.train.Example` from an image and the objects contained within. Note that TensorFlow Object Detection API expects jpeg-encoded image data, so the only preprocessing to be done is resize (optional) before the image is re-encoded (via a memory buffer using `BytesIO`. The input to this function is a DataFrame that contains one row per object and the columns shown above. This record is generous with features, but that is to make sure that the TF Object Detection API will successfully run the evaluation phase during training.","9e1cc800":"Next we can combine these individual features into a list of features, using `tf.train.Features`. Each feature is given a name (or key):","880e0365":"## TFRecord format\nTFRecords are made to support just about any type of data, and that means nesting basic features into a hierarchy of features. Look at the definition of [TFExample protobuffer here.](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/r2.0\/tensorflow\/core\/example\/feature.proto) TFRecords supports only three different data types:  \n  * bytes\n  * float\n  * int64\n\nSo any data must be converted into one of these three types. `tf.train.Feature` can be used to convert data into lists of these types. Let's create a few lists (or **features**).","3cc4d87e":"Next, read the file back in and access the data using `tf.data.TFRecordDataset` and `tf.train.Example.FromString`:","8b59f4d8":"The `contextlib2` library is used to automatically close all TFRecords files after writing is finished.","0f959144":"The last step is to create an `example` TFRecord. So, `example` is not a very good name, but that is what the TFRecord is called. We simply pass the list of features to `tf.train.Example`.","43c68edb":"OK, we have seen different ways of how images can be stored in TFRecords. Now, let's consider the case when we have a huge dataset, and need to create multiple TFRecords.","58a66b39":"## References\n*  [Protocol buffers](https:\/\/developers.google.com\/protocol-buffers\/)  \n*  [TensorFlow TFRecord tutorial](https:\/\/www.tensorflow.org\/tutorials\/load_data\/tfrecord)"}}