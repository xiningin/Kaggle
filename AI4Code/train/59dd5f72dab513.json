{"cell_type":{"eadc61cd":"code","b629a86d":"code","738bab69":"code","728ea1f3":"code","c1eff0ad":"code","f394428a":"code","4c33b3a7":"code","0718446f":"code","f05f420b":"code","67bb0fcf":"code","79a50652":"code","4104e084":"code","4a29c29a":"code","a954cae9":"code","ba95cfbd":"code","487d79be":"code","34b35751":"code","52e12fd7":"code","9f66a93d":"code","dedd6dad":"code","05fb84f6":"code","82c4262b":"code","1ea146bb":"code","cddb7060":"code","b5963dd9":"code","e27f829b":"code","c7e3c75b":"code","1e347afa":"code","a2f80af4":"code","1dea925f":"code","5ac2247e":"code","547d0905":"code","77459355":"code","1f743c62":"code","00e7fba0":"code","56ad8c90":"code","4262f3a3":"code","82270a20":"code","badc8d79":"code","f16b33ee":"code","f18de5a8":"code","bd4c14b7":"code","70c469f6":"code","6802e59c":"code","b12582b3":"code","0abc26ed":"code","20d8d8e9":"code","529bb3df":"code","ce07d287":"code","d2502c00":"code","038df3f3":"code","c46f3745":"code","aae77871":"code","565b0ab6":"code","a260fa9e":"code","4b4e7031":"code","91166a07":"code","03ec8704":"code","45ab08dc":"code","310eeab7":"code","5f9a335c":"code","8496d0e6":"code","d3faa16d":"code","a5cdc811":"code","faa3feb5":"code","a6acff67":"code","fdf5cc07":"code","5a5eda15":"code","95ca9bcc":"code","4959e3b8":"code","34c74c6b":"code","89f625f7":"code","b44a67ac":"code","1a7bdc50":"code","b57d3281":"code","d9f84369":"code","34882a87":"code","8579f308":"code","a7337549":"code","f7b8d4bc":"code","3b1912ef":"code","d4666d5b":"code","a81b038b":"code","d64f0381":"code","06a471cb":"code","03578516":"code","180ea925":"markdown","e36ef7e5":"markdown","2ec5e22e":"markdown","259d710a":"markdown","3ec78ca6":"markdown","d356b1f1":"markdown","5fda7c34":"markdown","53bb946e":"markdown","3c9272c3":"markdown","4af75ea0":"markdown","0d0e8be8":"markdown","c81b15f6":"markdown","21c139e1":"markdown"},"source":{"eadc61cd":"%pylab inline\n\nimport seaborn as sns\nimport pandas as pd\nimport lifetimes","b629a86d":"np.random.seed(42)\n\nimport random\nrandom.seed(42)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (12, 8) \nplt.rcParams[\"figure.dpi\"] = 60 \n\n# sns.set(style=\"ticks\")\n# sns.set_context(\"poster\", font_scale = .5, rc={\"grid.linewidth\": 5})","738bab69":"orders = pd.read_csv('..\/input\/olist_orders_dataset.csv')\ncustomers = pd.read_csv('..\/input\/olist_customers_dataset.csv')\npayments = pd.read_csv('..\/input\/olist_order_payments_dataset.csv')\norder_items = pd.read_csv(\"..\/input\/olist_order_items_dataset.csv\")\n\ncols = ['customer_id', 'order_id', 'order_purchase_timestamp']\norders = orders[cols]\norders = orders.set_index('customer_id')\norders.drop_duplicates(inplace=True)\norders[\"order_purchase_timestamp\"] = pd.to_datetime(orders[\"order_purchase_timestamp\"])\n\n# aggregate cost of items\ncosts = order_items.groupby(\"order_id\")[\"price\"].sum()\n\ncols = ['customer_id', 'customer_unique_id']\ncustomers = customers[cols]\ncustomers = customers.set_index('customer_id')\n\ntransactions = pd.concat([orders,customers], axis=1, join='inner')\ntransactions.reset_index(inplace=True)\n\ncols = ['customer_unique_id', 'order_id','order_purchase_timestamp']\ntransactions = transactions[cols]\n\ntransactions['order_purchase_timestamp'] = pd.to_datetime(transactions['order_purchase_timestamp'])\ntransactions['order_date'] = transactions.order_purchase_timestamp.dt.date\ntransactions['order_date'] = pd.to_datetime(transactions['order_date'])\n\ncols = ['customer_unique_id', 'order_id', 'order_date']\ntransactions = transactions[cols]","728ea1f3":"products = pd.read_csv(\"..\/input\/olist_products_dataset.csv\")\nsellers = pd.read_csv(\"..\/input\/olist_sellers_dataset.csv\")","c1eff0ad":"transactions = transactions.merge(costs.to_frame(\"total_cost\").reset_index(), on='order_id')\ntransactions.columns = [\"customer_id\", \"order_id\", \"order_date\", \"total_cost\"]","f394428a":"transactions.order_date.value_counts().plot()","4c33b3a7":"vc_num_purchases_by_customer = transactions.groupby(\"customer_id\")[\"order_id\"].nunique().value_counts()\nvc_num_purchases_by_customer = vc_num_purchases_by_customer.sort_index()\nnunique_items_per_customer = transactions.merge(order_items).groupby(\"customer_id\")[\"product_id\"].nunique()\nvc_num_items_per_customer = nunique_items_per_customer.value_counts()\nvc_num_items_per_customer = vc_num_items_per_customer.sort_index()\n\nprint(\"Average number of purchases: {:.2f}\".format(transactions.groupby(\"customer_id\")[\"order_id\"].nunique().mean()))\nprint(\"Average number of items bought per customer: {:.2f}\".format(\n    nunique_items_per_customer.mean()))\n\n\n# plot of number of orders per customer\nax = plt.figure().add_subplot(211)\nvc_num_purchases_by_customer.plot.bar(color='dodgerblue', ax=ax)\n\npct_cumsum_vc = vc_num_purchases_by_customer.cumsum() \/ vc_num_purchases_by_customer.sum()\n\nax2=ax.twinx()\nax.set_ylabel(\"Number of customers\")\nax.set_xlabel(\"Number of purchases\")\nax2.set_ylabel(\"Cumulative percent\")\nax2.plot(range(len(vc_num_purchases_by_customer)), pct_cumsum_vc, linestyle='--', color='salmon')\n\n# plot of number of items bought per customer\nax = plt.figure().add_subplot(212)\nvc_num_items_per_customer.plot.bar(color='dodgerblue', ax=ax)\n\npct_cumsum_vc = vc_num_items_per_customer.cumsum() \/ vc_num_items_per_customer.sum()\n\nax2=ax.twinx()\nax.set_ylabel(\"Number of customers\")\nax.set_xlabel(\"Number of items bought\")\nax2.set_ylabel(\"Cumulative percent\")\nax2.plot(range(len(vc_num_items_per_customer)), pct_cumsum_vc, linestyle='--', color='salmon')","0718446f":"np.max(transactions.order_date) - np.min(transactions.order_date)","f05f420b":"from lifetimes.utils import summary_data_from_transaction_data\n\n\n\n# summary_data_from_transaction_data(transactions, \n#                                                    customer_id_col = 'customer_id', \n#                                                    datetime_col = 'order_date', \n#                                                    freq = 'D',\n#                                         observation_period_end='2018-09-28', monetary_value_col='total_cost' )","67bb0fcf":"timestamps = [d.strftime('%Y-%m-%d') for d in pd.date_range(start='2018-04-01', end='2018-10-01', freq='M')]","79a50652":"summary_cal_holdout = None\n\nfor timestamp in timestamps:\n    summary_cal_holdout_partial = summary_data_from_transaction_data(transactions, \n                                                   customer_id_col = 'customer_id', \n                                                   datetime_col = 'order_date', \n                                                   freq = 'D',\n                                                observation_period_end=timestamp, monetary_value_col='total_cost' )\n\n    summary_cal_holdout_partial['date'] = timestamp\n    \n    if summary_cal_holdout is None:\n        summary_cal_holdout = summary_cal_holdout_partial\n    else:\n        summary_cal_holdout = summary_cal_holdout.append([summary_cal_holdout_partial])","4104e084":"summary_cal_holdout['date'].unique()","4a29c29a":"from sklearn.model_selection import train_test_split\n\nx = summary_cal_holdout[summary_cal_holdout['date'] != '2018-09-30']\ny = summary_cal_holdout[summary_cal_holdout['date'] == '2018-09-30']","a954cae9":"x = x.reset_index(drop=False)\ny = y.reset_index(drop=False)","ba95cfbd":"x = x[x['customer_id'].isin(y['customer_id'])]","487d79be":"sorted_x = x.sort_values(['customer_id', 'date'])","34b35751":"customers = x['customer_id'].unique()\ndates = x['date'].unique()","52e12fd7":"index = pd.MultiIndex.from_product([customers, dates], names = [\"customer_id\", \"date\"])\nall_customers_and_dates = pd.DataFrame(index = index).reset_index()","9f66a93d":"merged = pd.merge(all_customers_and_dates, x,  how='left', left_on=['customer_id', 'date'], right_on = ['customer_id', 'date'])","dedd6dad":"merged.loc[(merged['date'] == '2018-04-30') & (pd.isna(merged['frequency'])), 'frequency'] = 0\nmerged.loc[(merged['date'] == '2018-04-30') & (pd.isna(merged['recency'])), 'recency'] = 0\nmerged.loc[(merged['date'] == '2018-04-30') & (pd.isna(merged['T'])), 'T'] = 0\nmerged.loc[(merged['date'] == '2018-04-30') & (pd.isna(merged['monetary_value'])), 'monetary_value'] = 0","05fb84f6":"merged = merged.fillna(axis = 0, method = 'ffill')","82c4262b":"dates = merged['date'].values.reshape(95420, 5)\nfrequencies = merged['frequency'].values.reshape(95420, 5)\nrecencies = merged['recency'].values.reshape(95420, 5)\nTs = merged['T'].values.reshape(95420, 5)\nmonetary_values = merged['monetary_value'].values.reshape(95420, 5)","1ea146bb":"reshaped = numpy.hstack((frequencies, recencies, Ts, monetary_values)).reshape(95420, 4, 5)","cddb7060":"sorted_y = y.sort_values(['customer_id'])","b5963dd9":"output_y = sorted_y[['frequency', 'recency', 'T', 'monetary_value']]","e27f829b":"X_train, X_test, y_train, y_test = train_test_split(reshaped, output_y, test_size=0.2)","c7e3c75b":"y_train","1e347afa":"from keras import layers\nfrom keras.layers import recurrent\nfrom keras.models import Sequential","a2f80af4":"model = Sequential([\n    layers.Dense(60, input_shape=(4,5)),\n    layers.Activation('relu'),\n    layers.SimpleRNN(60),\n    layers.Activation('relu'),\n    layers.Dense(4),\n    layers.Activation('linear')\n])","1dea925f":"model.compile(optimizer='rmsprop',\n              loss='mse')","5ac2247e":"for layer in model.layers:\n    print(layer.output_shape)","547d0905":"#TODO normalize input\nmodel.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)","77459355":"from lifetimes import BetaGeoFitter, ModifiedBetaGeoFitter\nmbgf = ModifiedBetaGeoFitter()\nmbgf.fit(summary_cal_holdout[\"frequency_cal\"], summary_cal_holdout[\"recency_cal\"], summary_cal_holdout[\"T_cal\"], \n         iterative_fitting=3, verbose=True)","1f743c62":"from lifetimes.plotting import plot_period_transactions\nax = plot_period_transactions(mbgf, max_frequency=7)\nax.set_yscale('log')\nsns.despine();","00e7fba0":"from lifetimes.plotting import plot_frequency_recency_matrix, plot_probability_alive_matrix, plot_expected_repeat_purchases\n\n\nplt.figure(figsize=(10, 10))\nplot_frequency_recency_matrix(mbgf, T=120, );\n\nplt.figure(figsize=(10, 10))\nplot_probability_alive_matrix(mbgf);","56ad8c90":"n = 7\n\nsummary = summary_cal_holdout.copy()\nduration_holdout = summary.iloc[0]['duration_holdout']\n\nsummary['model_predictions'] = summary.apply(lambda r: mbgf.conditional_expected_number_of_purchases_up_to_time(\n    duration_holdout, r['frequency_cal'], r['recency_cal'], r['T_cal']), axis=1)\nagg_data = summary.groupby(\"frequency_cal\")[['frequency_holdout', 'model_predictions']].mean()\nax = agg_data.iloc[:n].plot(title=\"Pearson correlation (calibration & holdout): {:.4f}\".format(agg_data.corr().min().min()))\nax.set_xticks(agg_data.iloc[:n].index);","4262f3a3":"t = 120\npredicted_purchases = mbgf.conditional_expected_number_of_purchases_up_to_time(t, \n                                                                               summary_cal_holdout['frequency_cal'], \n                                                                               summary_cal_holdout['recency_cal'],\n                                                                               summary_cal_holdout['T_cal'])\npredicted_purchases.sort_values().tail(4)","82270a20":"from lifetimes.plotting import plot_history_alive\n\nfig = plt.figure(figsize=(15, 10))\n\nfor idx, customer_id in enumerate(predicted_purchases.sort_values().tail(4).index, 1):\n    # all days\n    days_since_birth = (max(transactions.order_date - min(transactions.order_date))).days\n    sp_trans = transactions.loc[transactions['customer_id'] == customer_id]\n    \n    plot_history_alive(mbgf, days_since_birth, sp_trans, 'order_date', ax=fig.add_subplot(2, 2, idx))","badc8d79":"# weak correlation between monetary and frequency\nreturning_customers_summary = summary_cal_holdout[summary_cal_holdout[\"frequency_cal\"] > 0]\nprint(returning_customers_summary[[\"frequency_cal\", \"monetary_value_cal\"]].corr())\n\nfig, axes = plt.subplots(1,2,figsize=(12, 5))\nsns.distplot(returning_customers_summary[\"monetary_value_cal\"], ax=axes[0], )\nsns.distplot(np.log(returning_customers_summary[\"monetary_value_cal\"] + 1), ax=axes[1], axlabel='$log(monetary\\_value)$')","f16b33ee":"from lifetimes import GammaGammaFitter\n\ngg = GammaGammaFitter()\ngg.fit(returning_customers_summary[\"frequency_cal\"], \n       returning_customers_summary[\"monetary_value_cal\"], verbose=True)","f18de5a8":"expected_average_profit_validation = gg.conditional_expected_average_profit(\n    summary_cal_holdout['frequency_holdout'], summary_cal_holdout['monetary_value_holdout'])\n\nexpected_average_profit = gg.conditional_expected_average_profit(\n    summary_cal_holdout['frequency_cal'], summary_cal_holdout['monetary_value_cal'])\n\nprint(\"With non-repeat buyers\")\nprint(\"Train correlation\")\nprint(pd.Series.corr(summary_cal_holdout[\"monetary_value_cal\"], expected_average_profit).round(4))\n\nprint(\"Validation correlation\")\nprint(pd.Series.corr(summary_cal_holdout[\"monetary_value_holdout\"], expected_average_profit_validation).round(4))","bd4c14b7":"expected_average_profit_validation = gg.conditional_expected_average_profit(\n    returning_customers_summary['frequency_holdout'], returning_customers_summary['monetary_value_holdout'])\n\nexpected_average_profit = gg.conditional_expected_average_profit(\n    returning_customers_summary['frequency_cal'], returning_customers_summary['monetary_value_cal'])\n\nprint(\"Repeat buyers only\")\nprint(\"Train correlation\")\nprint(pd.Series.corr(returning_customers_summary[\"monetary_value_cal\"], expected_average_profit).round(4))\n\nprint(\"Validation correlation\")\nprint(pd.Series.corr(returning_customers_summary[\"monetary_value_holdout\"], expected_average_profit_validation).round(4))","70c469f6":"from sklearn.metrics import mean_squared_error, mean_absolute_error\nmean_absolute_error(summary_cal_holdout[\"monetary_value_holdout\"], expected_average_profit_validation)","6802e59c":"print(\"Expected average profit validation: {:.2f}, Average profit validation: {:.2f}\".format(\n    gg.conditional_expected_average_profit(\n        summary_cal_holdout['frequency_cal'],\n        summary_cal_holdout['monetary_value_cal']\n    ).mean(),\n    summary_cal_holdout[summary_cal_holdout['frequency_cal']>0]['monetary_value_cal'].mean()))","b12582b3":"returning_customers_summary[:2]","0abc26ed":"from sklearn.model_selection import train_test_split\n\ntrain_cols = [\"frequency_cal\", \"recency_cal\", \"T_cal\", \"monetary_value_cal\"]\nX_train = returning_customers_summary[train_cols].values\ny_train = returning_customers_summary[\"frequency_cal\"].values\n\ntest_cols = [\"frequency_cal\", \"recency_cal\", \"T_cal\", \"monetary_value_holdout\"]\nX_test = returning_customers_summary[test_cols].values\ny_test = returning_customers_summary[\"frequency_holdout\"].values","20d8d8e9":"import statsmodels.api as sm\n\nX_train_constant = sm.add_constant(X_train, prepend=False)\n\nmod = sm.OLS(y_train, X_train_constant)\nres = mod.fit()\nprint(res.summary())","529bb3df":"from sklearn.svm import SVR\n\nsvr_model = SVR()\n\nsvr_model.fit(X_train, y_train)\npreds = svr_model.predict(X_test)\n\nn = 30\n\nsummary = returning_customers_summary.copy()\nduration_holdout = summary.iloc[0]['duration_holdout']\n\nsummary['model_predictions'] = preds\nagg_data = summary.groupby(\"frequency_cal\")[['frequency_holdout', 'model_predictions']].mean()\nax = agg_data.iloc[:n].plot(title=\"Pearson correlation (calibration & holdout): {:.4f}\".format(agg_data.corr().min().min()))\nax.set_xticks(agg_data.iloc[:n].index);","ce07d287":"orders = pd.read_csv('..\/input\/olist_orders_dataset.csv')\ncustomers = pd.read_csv('..\/input\/olist_customers_dataset.csv')\npayments = pd.read_csv('..\/input\/olist_order_payments_dataset.csv')\norder_items = pd.read_csv(\"..\/input\/olist_order_items_dataset.csv\")\n","d2502c00":"print(\"Orders\")\ndisplay(orders[:2])\nprint(\"Customers\")\ndisplay(customers[:2])\nprint(\"Payments\")\ndisplay(payments[:2])\nprint(\"Orders-Items\")\ndisplay(order_items[:2])\nprint(\"Items\")\ndisplay(products[:2])\nprint(\"Sellers\")\ndisplay(sellers[:2])","038df3f3":"import featuretools as ft\n\nes = ft.EntitySet(id = 'customers')\n\nes = es.entity_from_dataframe(entity_id = 'customers', dataframe = customers[[\"customer_id\", \"customer_city\", \"customer_state\"]], \n                              index = 'customer_id',)\n\nes = es.entity_from_dataframe(entity_id = 'orders', dataframe = orders.reset_index(),\n                              index = 'order_id', \n                              time_index = 'order_purchase_timestamp')\n\nes = es.entity_from_dataframe(entity_id = 'order_products', \n                              dataframe = order_items[[\"order_id\", \"product_id\", \"seller_id\", \"price\", \"freight_value\"]],\n                              index = 'order_product_id', make_index=True)\n\nes = es.entity_from_dataframe(entity_id = 'products', \n                              dataframe = products,\n                              index = 'product_id', )\n\nes = es.entity_from_dataframe(entity_id = 'sellers', \n                              dataframe = sellers[[\"seller_id\", \"seller_city\", \"seller_state\"]],\n                              index = 'seller_id', )\n\nes = es.entity_from_dataframe(entity_id = 'payments', \n                              dataframe = payments[[\"order_id\", \"payment_type\", \"payment_installments\", \"payment_value\"]],\n                              index = 'payment_id', make_index=True)","c46f3745":"# Add the relationship to the entity set\n# customer to orders\nes = es.add_relationship(ft.Relationship(es['customers']['customer_id'],\n                                    es['orders']['customer_id']))\n# orders to order products\nes = es.add_relationship(ft.Relationship(es['orders']['order_id'],\n                                    es['order_products']['order_id']))\n# products to order products\nes = es.add_relationship(ft.Relationship(es['products']['product_id'], \n                                         es['order_products']['product_id']))\n# sellers to order products\nes = es.add_relationship(ft.Relationship(es['sellers']['seller_id'],\n                                         es['order_products']['seller_id']))\n\n# orders to payments\nes = es.add_relationship(ft.Relationship(es['orders']['order_id'],\n                                         es['payments']['order_id']))\n","aae77871":"es.add_last_time_indexes()","565b0ab6":"es","a260fa9e":"# We want to know if the customer will buy anything again after cutoff\ncutoff_time = pd.Timestamp('July 1, 2018')","4b4e7031":"# training window of only 2 months, then experiment with 4 months, 6 months\n# find out if the customer will buy anything after \n# turns out, sliding windows are not supported yet\nfeatures, feature_names = ft.dfs(entityset = es, target_entity = 'customers', verbose=True, \n                                 cutoff_time=cutoff_time,\n                                 training_window = ft.Timedelta(\"60 days\"))","91166a07":"import missingno as msno\nfeatures_null_filtered = msno.nullity_filter(features, p=0.75)","03ec8704":"features_encoded, features_names_encoded = ft.encode_features(features_null_filtered, feature_names)","45ab08dc":"orders[\"order_purchase_timestamp\"] = pd.to_datetime(orders[\"order_purchase_timestamp\"])\nlast_timestamp_per_customer = orders.reset_index().groupby(\"customer_id\")[\"order_purchase_timestamp\"].max()\ncustomer_bought_after_cutoff = last_timestamp_per_customer > cutoff_time","310eeab7":"from sklearn.model_selection import train_test_split\n\nX = features_encoded\ny = customer_bought_after_cutoff\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","5f9a335c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import make_pipeline\n\nimputer = Imputer(strategy='most_frequent')\nt_svd = TruncatedSVD(n_components=100)\nlog_res = LogisticRegression(C=0.1)\n\npipeline = make_pipeline(imputer, t_svd, log_res)\npipeline.fit(X_train, y_train)","8496d0e6":"from sklearn.metrics import log_loss, accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, average_precision_score\n\ndef complete_evaluation(y_test, y_probs, y_preds, positive_class=1):\n    \"\"\"\n    Complete evaluation\n    \"\"\"\n    acc = accuracy_score(y_test, y_preds)\n    average_prec = average_precision_score(y_test, y_probs[:, positive_class])\n    precs, recs, fscore, support = precision_recall_fscore_support(y_test, y_preds)\n    prec_0, prec_1 = precs\n    rec_0, rec_1 = recs\n    fscore_0, fscore_1 = fscore \n    support_0, support_1 = support\n    \n    balanced_accuracy = (rec_0 + rec_1) \/ 2\n\n    log_loss_value = log_loss(y_test, y_probs)\n    auc_value = roc_auc_score(y_test, y_probs[:, positive_class])\n\n    return pd.DataFrame([{\"accuracy\" : acc, \"loss\" : log_loss_value, \"auc\" : auc_value, \"average_precision\" : average_prec,\n                  \"precision_0\" : prec_0, \"precision_1\" : prec_1, \n                  \"recall_0\" : rec_0, \"recall_1\" : rec_1, \"fscore_0\" : fscore_0, \"fscore_1\" : fscore_1,\n                  \"support_0\" : support_0, \"support_1\" : support_1, \"balanced_accuracy\" : balanced_accuracy}])","d3faa16d":"y_proba = pipeline.predict_proba(X_test)\n\nlist_results = []\nfor threshold in [0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2]:\n    y_preds = y_proba[:, 1] > threshold\n    result = complete_evaluation(y_test, y_proba, y_preds)\n    result[\"threshold\"] = threshold\n    result[\"number_predictions\"] = (y_preds == 1).sum()\n    list_results.append(result)\n    \ndf_results_thresholds = pd.concat(list_results)","a5cdc811":"df_results_thresholds","faa3feb5":"width = 0.3\nplt.figure(figsize=(15,5))\ndf_results_thresholds.plot(y=['number_predictions', \"precision_1\"], x=\"threshold\", position=0, kind='bar',\n                                    width=width, secondary_y= 'number_predictions', rot=0, )","a6acff67":"# 120 days training window\nfeatures, feature_names = ft.dfs(entityset = es, target_entity = 'customers', verbose=True, \n                                 cutoff_time=cutoff_time,\n                                 training_window = ft.Timedelta(\"120 days\"))","fdf5cc07":"import missingno as msno\nfeatures_null_filtered = msno.nullity_filter(features, p=0.75)","5a5eda15":"features_encoded, features_names_encoded = ft.encode_features(features_null_filtered, feature_names)","95ca9bcc":"last_timestamp_per_customer = orders.reset_index().groupby(\"customer_id\")[\"order_purchase_timestamp\"].max()\ncustomer_bought_after_cutoff = last_timestamp_per_customer > cutoff_time","4959e3b8":"from sklearn.model_selection import train_test_split\n\nX = features_encoded\ny = customer_bought_after_cutoff\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","34c74c6b":"imputer = Imputer(strategy='most_frequent')\nt_svd = TruncatedSVD(n_components=100)\nlog_res = LogisticRegression(C=0.1)\n\npipeline = make_pipeline(imputer, t_svd, log_res)\npipeline.fit(X_train, y_train)","89f625f7":"y_proba = pipeline.predict_proba(X_test)\n\nlist_results = []\nfor threshold in [0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25]:\n    y_preds = y_proba[:, 1] > threshold\n    result = complete_evaluation(y_test, y_proba, y_preds)\n    result[\"threshold\"] = threshold\n    result[\"number_predictions\"] = (y_preds == 1).sum()\n    result[\"average f-score\"] = ((result[\"fscore_0\"] + result[\"fscore_1\"])\/2).iloc[0]\n    list_results.append(result)\n    \ndf_results_thresholds = pd.concat(list_results)","b44a67ac":"df_results_thresholds","1a7bdc50":"width = 0.3\nplt.figure(figsize=(15,5))\ndf_results_thresholds.plot(y=['number_predictions', \"precision_1\"], x=\"threshold\", position=0, kind='bar',\n                                    width=width, secondary_y= 'number_predictions', rot=0, )","b57d3281":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.pipeline import make_pipeline\n\nimputer = Imputer(strategy='most_frequent')\nrandom_forest = RandomForestClassifier(n_estimators=50, max_depth=3, min_samples_split=10, \n                                       class_weight='balanced_subsample')\n# gbt = GradientBoostingClassifier(subsample=0.5, n_iter_no_change=3, verbose=True,)\n\npipeline = make_pipeline(imputer, random_forest)\npipeline.fit(X_train, y_train)","d9f84369":"def name_scores(featurecoef, col_names, label=\"Score\", sort=False):\n    \"\"\"\n    Generates a DataFrame with all the independent variables used in the model with their corresponding coefficient\n    :param featurecoef: model.coef_ | model.feature_importances_\n    :param column: string to be anonymized\n    :param label: Name of the column where coefficients will be added\n    :param sort: False = Decending, True = Ascending\n    :return: pandas DataFrame\n    \"\"\"\n    \n    df_feature_importance = pd.DataFrame([dict(zip(col_names, featurecoef))]).T.reset_index()\n    df_feature_importance.columns = [\"Feature\", label]\n    if sort:\n        return df_feature_importance.sort_values(ascending=False, by=label)\n    return df_feature_importance","34882a87":"pd.set_option('display.width', 1000)\npd.set_option('display.max_colwidth', -1)","8579f308":"name_scores(random_forest.feature_importances_, X_train.columns, sort=True)[:20]","a7337549":"y_proba = pipeline.predict_proba(X_test)\nsns.distplot(y_proba[:, 1])\n\nlist_results = []\nfor threshold in [0.5, 0.525, 0.55, 0.575, 0.6, 0.625, 0.65]:\n    y_preds = y_proba[:, 1] > threshold\n    result = complete_evaluation(y_test, y_proba, y_preds)\n    result[\"threshold\"] = threshold\n    result[\"number_predictions\"] = (y_preds == 1).sum()\n    result[\"average f-score\"] = ((result[\"fscore_0\"] + result[\"fscore_1\"])\/2).iloc[0]\n    list_results.append(result)\n    \ndf_results_thresholds = pd.concat(list_results)","f7b8d4bc":"df_results_thresholds","3b1912ef":"width = 0.3\nplt.figure(figsize=(15,5))\ndf_results_thresholds.plot(y=['number_predictions', \"precision_1\"], x=\"threshold\", position=0, kind='bar',\n                                    width=width, secondary_y= 'number_predictions', rot=0, )","d4666d5b":"imputer = Imputer(strategy='most_frequent')\ngbt = GradientBoostingClassifier(verbose=True,)\n\npipeline = make_pipeline(imputer, gbt)\npipeline.fit(X_train, y_train)","a81b038b":"name_scores(gbt.feature_importances_, X_train.columns, sort=True)[:20]","d64f0381":"y_proba = pipeline.predict_proba(X_test)\nsns.distplot(y_proba[:, 1])\n\nlist_results = []\nfor threshold in [0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25]:\n    y_preds = y_proba[:, 1] > threshold\n    result = complete_evaluation(y_test, y_proba, y_preds)\n    result[\"threshold\"] = threshold\n    result[\"number_predictions\"] = (y_preds == 1).sum()\n    result[\"average f-score\"] = ((result[\"fscore_0\"] + result[\"fscore_1\"])\/2).iloc[0]\n    list_results.append(result)\n    \ndf_results_thresholds = pd.concat(list_results)","06a471cb":"df_results_thresholds","03578516":"width = 0.3\nplt.figure(figsize=(15,5))\ndf_results_thresholds.plot(y=['number_predictions', \"precision_1\"], x=\"threshold\", position=0, kind='bar',\n                                    width=width, secondary_y= 'number_predictions', rot=0, )","180ea925":"## Gradient Boosted Trees","e36ef7e5":"# 120 days","2ec5e22e":"# NBD Model","259d710a":"# 60 days","3ec78ca6":"## A more complex model -- Random Forests","d356b1f1":"# Regression, finally","5fda7c34":"# Adding the nature of items\n- X attributes: \n - number of items purchased, type of items purchased, city, payment history, type of seller, cost of items, \n- y attributes: how many times the customer returned to purchase an order","53bb946e":"# Objective:\n1. Predict the number of purchases a customer will do in 2018 H2.\n2. Predict the total cash credited by the customer in 2018 H2.","3c9272c3":"# Regression, finally","4af75ea0":"# Adding Monetary, GammaGamma Filter","0d0e8be8":"# ModifiedBetaGeoFitter\n- With this technique, we are abstracting:\n  - the nature of the items in the order\n  - the ratings of the users in the order\n  - the credit payment history of the items in the user\n  - the freight rate","c81b15f6":"# Regression with item types\n## Target: Frequency Holdout\nNot good!","21c139e1":"# Conclusions:\n- Repeat purchases could be predicted (holdout set's pearson correlation with predictions) although the probability is very low (expected number of future purchases).\n- Monetary value predictions has good correlation although this includes the non-repeat buyers. \n- For the repeat buyers, monetary value predictions' correlation is low. The exact monetary value is hard to predict since for an e-commerce site, the variety of items is very large."}}