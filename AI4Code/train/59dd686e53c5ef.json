{"cell_type":{"b1894cb9":"code","93479f8b":"code","ee5f4798":"code","50e5629b":"code","0467ecce":"code","d83ed941":"code","fb19c760":"code","776e7bef":"code","815f7cb9":"code","c052f89b":"markdown","715b00a8":"markdown","680a41f0":"markdown","3993f097":"markdown","6f4e3e31":"markdown","49257acf":"markdown"},"source":{"b1894cb9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nfrom statsmodels.api import  qqplot\n\nfrom sklearn import preprocessing as preprocessing\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.metrics import mean_absolute_error\nimport types\n\n\nfrom scipy.stats import shapiro, normaltest, anderson\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\ndef load_data(path):\n    return pd.read_csv(path+'train.csv', index_col='Id'), pd.read_csv(path+'test.csv', index_col='Id')\n\ndef init_sns():\n    # set seaborn style dark\n    sns.set_style('dark')\n    \ndef qq_plot(x, **kwargs):\n    qqplot(x, ax=plt.gca(), **kwargs)\n\ndef cat_cols(df):\n    \"\"\"return categorical columns\n    \"\"\"\n    return [cname for cname in df.columns if\n                    df[cname].dtype == \"O\"]\n\ndef num_cols(df):\n    \"\"\"return numerical columns\n\n        total columns = categorical columns + numerical columns\n    \"\"\"\n    return df.columns[~df.columns.isin(cat_cols(df))]\n\ndef get_cols_with_(x, data):\n    \"\"\" return columns of DataFrame 'data', which contains str 'x'\n    \"\"\"\n    cols = []\n    for c in data.columns:\n        if x in c:\n            cols.append(c)\n    return cols\n\ndef save(index, preds):\n    # Save test predictions to file\n    output = pd.DataFrame({'Id': index,\n                           'SalePrice': preds})\n    output.to_csv('submission.csv', index=False)\n    \ndef nomality_tests(data):\n    \"\"\"\n    return shpiro test result(s1, p1),\n    D\u2019Agostino\u2019s K^2 Test result(s2,p2), \n    Anderson-Darling Test result\n    in tuple\n    \"\"\"\n    s1, p1 = shapiro(data)\n    s2, p2 = normaltest(data)\n    result = anderson(data)\n    return (s1, p1), (s2, p2), result\n\ndef linear_regression(data):\n    lr = LinearRegression()\n    x_train, y_train = data[n_feats], data['SalePrice']\n    lr.fit(x_train, y_train)\n    return lr\n\ndef fill_garageyrblt(data, c):\n    rows = data['GarageYrBlt'].isnull()\n    data.loc[rows, 'GarageYrBlt']= \\\n    data.loc[rows, 'YearBuilt']\n    \ndef miss_val_handler(data):\n    \n    def _handle_(data, c, v):\n        if isinstance(v, types.FunctionType):\n            v(data, c)\n        else:\n            null_r = data[c].isnull()\n            if null_r.sum() == 0: return\n            data.loc[null_r, c] = v        \n\n    for c in MISS_VAL:\n        v = MISS_VAL[c]\n        if isinstance(v, tuple):\n            for e in v:\n                _handle_(data, c, e)\n        else:\n            _handle_(data, c, v)\n            \n    return data\n            \ndef BsmtExposure_no(data,c):\n    #data.loc[949, 'BsmtExposure']='No'\n    pass\n\ndef median_impute(data, c):\n    null_r = data[c].isnull()\n    if null_r.sum() == 0: return\n    data.loc[null_r, c] = data[c].median()\n    \ndef most_freq_impute(data, c):\n    null_r = data[c].isnull()\n    if null_r.sum() == 0: return\n    data.loc[null_r, c] = data[c].mode()[0]\n    \n# load data to TRAIN_DATA, TEST_DATA\nPATH = '\/kaggle\/input\/home-data-for-ml-course\/'\nTRAIN_DATA, TEST_DATA = load_data(PATH)\n\n## Chosen numeric features\nN_FEATS = ['BedroomAbvGr', 'GrLivArea', 'LowQualFinSF', 'YearBuilt', 'FullBath',\n           'EnclosedPorch', '1stFlrSF', 'BsmtFinSF2', 'OverallQual', 'YearRemodAdd',\n           'BsmtUnfSF', 'LotArea', 'KitchenAbvGr', 'Fireplaces', 'OverallCond',\n           'TotRmsAbvGrd', 'PoolArea', 'LotFrontage', 'OpenPorchSF', 'BsmtFinSF1',\n           'GarageYrBlt', 'TotalBsmtSF', 'GarageCars', '3SsnPorch', 'BsmtHalfBath',\n           'WoodDeckSF', '2ndFlrSF', 'GarageArea', 'BsmtFullBath', 'MiscVal', 'ScreenPorch',\n           'HalfBath', 'MasVnrArea', 'YrSold']\n\n## Chosen categrical featrues\nC_FEATS = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n           'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', \n           'BldgType','HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n           'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n           'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n           'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n           'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n           'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType',\n           'SaleCondition', 'MoSold', 'MSSubClass']\n\n## Doc of Values for missing value to fill.\n## where key is column name and value to fill missing value\nMISS_VAL = {'PoolQC': 'NA',\n            'FireplaceQu': 'NA',\n            'GarageCond': 'NA',\n            'GarageType': 'NA',\n            'GarageFinish': 'NA',\n            'GarageQual': 'NA',\n            'BsmtExposure': ('NA', BsmtExposure_no),\n            'BsmtFinType2': 'NA',\n            'BsmtFinType1': 'NA',\n            'BsmtQual': 'NA',\n            'BsmtCond': 'NA',\n            'MasVnrType': 'None',\n            'Electrical': 'SBrkr',\n            'MasVnrArea': 0,\n            'LotFrontage': median_impute,\n            'GarageYrBlt': fill_garageyrblt,\n            'BsmtFullBath':0,\n            'BsmtHalfBath':0,\n            'TotalBsmtSF': 0,\n            'BsmtFinSF1': 0,\n            'BsmtUnfSF': 0, \n            'BsmtFinSF2': 0,\n            'Exterior1st': most_freq_impute,\n            'Exterior2nd': most_freq_impute,\n            'MSZoning': most_freq_impute,\n            'Functional': most_freq_impute,\n            'Utilities': most_freq_impute,\n            'KitchenQual': most_freq_impute,\n            'SaleType': most_freq_impute,\n            'GarageArea': 0,\n            'GarageCars': 0}\n\n## Ordinal features\nORDINAL = ['ExterQual','BsmtQual', 'KitchenQual',\n           'GarageQual', 'HeatingQC', 'PoolQC',\n           'GarageType', 'GarageFinish', 'GarageCond',\n           'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n           'BsmtFinType2', 'FireplaceQu', 'Electrical',\n           'LotShape', 'Functional', 'ExterCond',\n           'Utilities', 'PavedDrive']\n\n## Features to be one-hot encoded\nONE_HOT = ['LandSlope', 'LotConfig', 'Foundation', 'MSSubClass', 'SaleType', 'MSZoning',\n           'Neighborhood', 'MoSold', 'Condition1', 'Exterior2nd', 'BldgType', 'RoofStyle',\n           'HouseStyle', 'CentralAir', 'Heating', 'MasVnrType', 'LandContour', 'Street',\n           'Exterior1st', 'Condition2', 'SaleCondition', 'RoofMatl']\n\nQT = preprocessing.QuantileTransformer(output_distribution='normal', random_state=0)\nOHE =  preprocessing.OneHotEncoder(dtype='int', drop='first')\nORDE = preprocessing.OrdinalEncoder()\nCT = ColumnTransformer([('ohe', OHE, ONE_HOT),\n                        ('orde', ORDE, ORDINAL),\n                        ('qt', QT, N_FEATS)],\n                       remainder='drop')\n\nFEATS = N_FEATS + ORDINAL + ONE_HOT\n\nREGRESSOR = TransformedTargetRegressor(regressor=LinearRegression(),\n                                       transformer=QT)\n\nMISS_VAL_IMPUTER = preprocessing.FunctionTransformer(miss_val_handler, validate=False)\n\nPIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('col_trans', CT),\n                           ('linear_reg', REGRESSOR)],\n                    verbose=True)\n\ndef ols_regression(x_train, x_test):\n    X_train = x_train.copy()\n    X_test = x_test.copy()\n\n    # test + train data merge\n    merged = pd.concat([X_train, X_test], sort=True)\n    miss_val_handler(merged)\n    # missing value handle\n    X_train[949, 'BsmtExposure'] = 'No'\n\n    PIPELINE.fit(merged.loc[:1460,:].copy(), merged.loc[:1460,'SalePrice'].copy())\n    print(mean_absolute_error(X_train['SalePrice'],\n                              PIPELINE.predict(merged.loc[:1460,:].copy())))","93479f8b":"RidgeCV()","ee5f4798":"RIDGE_REGRESSOR = TransformedTargetRegressor(regressor=RidgeCV(alphas=[1e-6, 1e-3, 1e-2, 1e-1, 1]),\n                                       transformer=QT)\n\nRIDGE_PIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('col_trans', CT),\n                           ('ridge_reg', RIDGE_REGRESSOR)],\n                    verbose=True)\n\ndef ridge_regression(x_train, x_test, save_test=False):\n    X_train = x_train.copy()\n    X_test = x_test.copy()\n\n    # test + train data merge\n    merged = pd.concat([X_train, X_test], sort=True)\n    miss_val_handler(merged)\n    # missing value handle\n    X_train[949, 'BsmtExposure'] = 'No'\n\n    RIDGE_PIPELINE.fit(merged.loc[:1460,:].copy(), merged.loc[:1460,'SalePrice'].copy())\n    print(mean_absolute_error(X_train['SalePrice'],\n                              RIDGE_PIPELINE.predict(merged.loc[:1460,:].copy())))\n    \n    if save_test:\n        preds = RIDGE_PIPELINE.predict(merged.loc[1460:,:].copy())\n        save(X_test.index, preds)\n","50e5629b":"ridge_regression(TRAIN_DATA, TEST_DATA, save_test=True)","0467ecce":"MSSUBCLASS_OHE =  preprocessing.OneHotEncoder(dtype='int', handle_unknown='ignore')","d83ed941":"MSSUBCLASS_OHE","fb19c760":"CT = ColumnTransformer([('mssubclass_ohe', MSSUBCLASS_OHE, ['MSSubClass']),\n                        ('ohe', OHE, list(set(ONE_HOT) - set(['MSSubClass']))),\n                        ('orde', ORDE, ORDINAL),\n                        ('qt', QT, N_FEATS)],\n                       remainder='drop')","776e7bef":"RIDGE_PIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('col_trans', CT),\n                           ('ridge_reg', RIDGE_REGRESSOR)],\n                    verbose=True)\n\ndef ridge_regression(x_train, x_test, save_test=False):\n    X_train = x_train.copy()\n    X_test = x_test.copy()\n\n    # test + train data merge\n    merged = pd.concat([X_train, X_test], sort=True)\n    miss_val_handler(merged)\n    # missing value handle\n    X_train[949, 'BsmtExposure'] = 'No'\n\n    RIDGE_PIPELINE.fit(merged.loc[:1460,:].copy(), merged.loc[:1460,'SalePrice'].copy())\n    print(mean_absolute_error(X_train['SalePrice'],\n                              RIDGE_PIPELINE.predict(merged.loc[:1460,:].copy())))\n    \n    if save_test:\n        preds = RIDGE_PIPELINE.predict(merged.loc[1461:,:].copy())\n        save(X_test.index, preds)","815f7cb9":"ridge_regression(TRAIN_DATA, TEST_DATA)","c052f89b":"## MSSubClass: 150\n\nFix one hot encoding for 'MSSubClass'.\n\n    20  1-STORY 1946 & NEWER ALL STYLES\n    30  1-STORY 1945 & OLDER\n    40  1-STORY W\/FINISHED ATTIC ALL AGES\n    45  1-1\/2 STORY - UNFINISHED ALL AGES\n    50  1-1\/2 STORY FINISHED ALL AGES\n    60  2-STORY 1946 & NEWER\n    70  2-STORY 1945 & OLDER\n    75  2-1\/2 STORY ALL AGES\n    80  SPLIT OR MULTI-LEVEL\n    85  SPLIT FOYER\n    90  DUPLEX - ALL STYLES AND AGES\n    120  1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n    150  1-1\/2 STORY PUD - ALL AGES\n    160  2-STORY PUD - 1946 & NEWER\n    180  PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\n    190  2 FAMILY CONVERSION - ALL STYLES AND AGES","715b00a8":"# Goal\n\nUse Redge regression instead OLS.\n\nReference:\n[Ridge Regression](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#ridge-regression)","680a41f0":"When predict with test, 'unknown categories' was found. Becasue of column transformation with CT was performed only usiing train data ( ***merged\\[:1460,:\\]*** ).","3993f097":"# Try","6f4e3e31":"# Ridge regression","49257acf":"# Data load and utility functions\n\nThese utility came from previous notebooks.\n\n* [Housing price: OLS Linear Regression - 1](https:\/\/www.kaggle.com\/angelndevil2\/housing-price-ols-linear-regression-1)\n* [Housing price: OLS Linear Regression - 2](https:\/\/www.kaggle.com\/angelndevil2\/housing-price-ols-linear-regression-2)\n* [Housing price: OLS Linear Regression - 3](https:\/\/www.kaggle.com\/angelndevil2\/housing-price-ols-linear-regression-3)\n* [Housing price: OLS Linear Regression - 4](https:\/\/www.kaggle.com\/angelndevil2\/housing-price-ols-linear-regression-4)"}}