{"cell_type":{"05de738c":"code","78022f21":"code","f08c3221":"code","65391609":"code","c3679c13":"code","90692066":"code","7a1ba238":"code","1a1bc713":"code","a2197332":"code","49a69050":"code","d7b767ae":"code","bd907089":"code","2cfb4c3b":"code","47b72a44":"code","7724e33b":"code","9d55ec17":"code","a4d02867":"code","17c49294":"code","d74e93ee":"markdown","deed35b3":"markdown","95c8f728":"markdown","d957d282":"markdown","bda724e4":"markdown","6db6bdad":"markdown","01b37d22":"markdown","150a4881":"markdown","6044f88d":"markdown","584fff5f":"markdown","4bccbc93":"markdown","a09e3cdc":"markdown","8afab189":"markdown","e05f74c8":"markdown","e9913d73":"markdown","67c9d572":"markdown","14f0495b":"markdown","90e78559":"markdown","ecbde784":"markdown"},"source":{"05de738c":"import pandas as pd\nfrom sklearn.model_selection  import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\n","78022f21":"df = pd.read_csv('\/kaggle\/input\/wdbc.csv', header=None)","f08c3221":"df.head()","65391609":"from sklearn.preprocessing import LabelEncoder\nX = df.loc[:,2:].values\ny = df.loc[:,1].values\nle = LabelEncoder()\ny = le.fit_transform(y)","c3679c13":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20, random_state=1)","90692066":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline","7a1ba238":"pipe_lr = Pipeline([('scl', StandardScaler()),('pca', PCA(n_components=2)),('lr',LogisticRegression(random_state=1))])\npipe_lr.fit(X_train,y_train)\nprint('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))","1a1bc713":"from sklearn.model_selection  import StratifiedKFold\nkfold = StratifiedKFold(n_splits=3)\nscores = []\nfor train,test in kfold.split(X_train, y_train):\n    pipe_lr.fit(X_train[train],y_train[train])\n    score = pipe_lr.score(X_train[test],y_train[test])\n    scores.append(score)\n    print('Fold: , Class dist.: %s, Acc: %.3f' % ( np.bincount(y_train[train]), score))","a2197332":"print('CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))","49a69050":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(estimator=pipe_lr, X = X_train, y = y_train, cv=10,n_jobs=1 )\nprint('CV accuracy Scores %s' % scores)","d7b767ae":"from sklearn.model_selection import learning_curve\npipe_lr = Pipeline([('scl',StandardScaler()), ('clf', LogisticRegression(penalty = 'l2', random_state=0))])\ntrain_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, X=X_train, y= y_train, train_sizes=np.linspace(0.1,1.0,10.0), cv=10, n_jobs=1)\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')\nplt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\nplt.plot(train_sizes, test_mean, color='green', marker='o', markersize=5, label='Testing Accuracy')\nplt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\nplt.grid()\nplt.xlabel('Number of training samples')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.8, 1.0])\nplt.show()\n","bd907089":"from sklearn.model_selection  import validation_curve\nparam_range = [0.01,0.1,1,10,100]\ntrain_scores, test_scores = validation_curve(estimator = pipe_lr, X=X_train, y=y_train, param_name = 'clf__C', param_range=param_range, cv=10)\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.plot(param_range, train_mean,color='blue', marker='o',markersize=5,label='training accuracy')\nplt.fill_between(param_range, train_mean + train_std,train_mean - train_std, alpha=0.15,color='blue')\nplt.plot(param_range, test_mean,color='green', linestyle='--',marker='s', markersize=5,label='validation accuracy')\nplt.fill_between(param_range,test_mean + test_std,test_mean - test_std,alpha=0.15, color='green')\nplt.grid()\nplt.xscale('log')\nplt.legend(loc='lower right')\nplt.xlabel('Parameter C')\nplt.ylabel('Accuracy')\nplt.ylim([0.8, 1.0])\nplt.show()","2cfb4c3b":"from sklearn.model_selection  import GridSearchCV\nfrom sklearn.svm import SVC\npipe_svc = Pipeline([('cls', StandardScaler()), ('clf', SVC(random_state=1))])\nparam_range = [0.0001,0.001,0.01,0.1,1,10,100,1000]\nparam_grid = [{'clf__C' : param_range,\n              'clf__kernel': ['linear']},\n             {'clf__C': param_range,\n             'clf__gamma': param_range,\n             'clf__kernel': ['rbf']}]\ngs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid,scoring='accuracy', cv=10,n_jobs=-1)\ngs.fit(X_train,y_train)\nprint(gs.best_score_)\nprint(gs.best_params_)","47b72a44":"clf = gs.best_estimator_\nclf.fit(X_train,y_train)\nprint('Test score : %.3f' %clf.score(X_test,y_test))","7724e33b":"gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=2, n_jobs=-1)\nscores = cross_val_score(gs, X_train,y_train, scoring='accuracy', cv=5)\nprint('CV accuracy %.3f +- %.3f' % (np.mean(scores), np.std(scores)))","9d55ec17":"from sklearn.tree import DecisionTreeClassifier\ngs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=1), param_grid=[{'max_depth' : [1,2,3,4,5,6,7,None]}], scoring='accuracy', cv = 5)\nscores = cross_val_score(gs, X_train,y_train, scoring='accuracy', cv=2)\nprint('CV accuracy %.3f +- %.3f' % (np.mean(scores), np.std(scores)))\n\n","a4d02867":"from sklearn.metrics import confusion_matrix\npipe_svc.fit(X_train,y_train)\ny_pred = pipe_svc.predict(X_test)\nconf_mat = confusion_matrix(y_true = y_test, y_pred= y_pred)\nprint(conf_mat)\n","17c49294":"from sklearn.metrics import precision_score,recall_score, f1_score\n\nprint('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\nprint('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\nprint('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))","d74e93ee":"Similar to the learning_curve function, the validation_curve function uses strati ed k-fold cross-validation by default to estimate the performance of the model if we are using algorithms for classi cation. Inside the validation_curve function, we speci ed the parameter that we wanted to evaluate. In this case, it is C, the inverse regularization parameter of the LogisticRegression classi er, which we wrote as 'clf__C' to access the LogisticRegression object inside the scikit-learn pipeline for a speci ed value range that we set via the param_range parameter. Similar to the learning curve example in the previous section, we plotted the average training and cross-validation accuracies and the corresponding standard deviations.\nAlthough the differences in the accuracy for varying values of C are subtle, we can see that the model slightly under ts the data when we increase the regularization strength (small values of C). However, for large values of C, it means lowering the strength of regularization, so the model tends to slightly over t the data. In this case, the sweet spot appears to be around C=0.1.","deed35b3":"# Combining transformers and estimators in a Pipeline\nIn the previous chapter, you learned that many learning algorithms require input features on the same scale for optimal performance. Thus, we need to standardize the columns in the Breast Cancer Wisconsin dataset before we can feed them to a linear classi er, such as logistic regression. Furthermore, let's assume that we want to compress our data from the initial 30 dimensions onto a lower two-dimensional subspace via principal component analysis (PCA), a feature extraction technique for dimensionality reduction that we introduced in Chapter 5, Compressing Data via Dimensionality Reduction. Instead of going through the  tting and transformation steps for the training and test dataset separately, we can chain the StandardScaler, PCA, and LogisticRegression objects in a pipeline:","95c8f728":"### The K-fold cross-validation\nIn k-fold cross-validation, we randomly split the training dataset into k folds without replacement, where k \u22121 folds are used for the model training and one fold is used for testing. This procedure is repeated k times so that we obtain k models and performance estimates.","d957d282":"## Fine-tuning machine learning models via grid search\nIn machine learning, we have two types of parameters: those that are learned from the training data, for example, the weights in logistic regression, and the parameters of a learning algorithm that are optimized separately. The latter are the tuning parameters, also called hyperparameters, of a model, for example, the regularization parameter in logistic regression or the depth parameter of a decision tree.\nIn the previous section, we used validation curves to improve the performance of a model by tuning one of its hyperparameters. In this section, we will take a look at a powerful hyperparameter optimization technique called grid search that can further help to improve the performance of a model by  nding the optimal combination of hyperparameter values.\n\n### Tuning hyperparameters via grid search","bda724e4":"The returned average cross-validation accuracy gives us a good estimate of what\nto expect if we tune the hyperparameters of a model and then use it on unseen data. For example, we can use the nested cross-validation approach to compare an\nSVM model to a simple decision tree classi er; for simplicity, we will only tune\nits depth parameter:","6db6bdad":"In this chapter, we will learn how to:\n\n\u2022 Obtain unbiased estimates of a model's performance\n\n\u2022 Diagnose the common problems of machine learning algorithms\n\n\u2022 Fine-tune machine learning models\n\n\u2022 Evaluate predictive models using different performance metrics\n","01b37d22":"## Diagnosing bias and variance problems with learning curves\n\nIf a model is too complex for a given training dataset\u2014there are too many degrees\nof freedom or parameters in this model\u2014the model tends to over t the training\ndata and does not generalize well to unseen data. Often, it can help to collect more training samples to reduce the degree of over tting. However, in practice, it can often be very expensive or simply not feasible to collect more data. By plotting the model training and validation accuracies as functions of the training set size, we can easily detect whether the model suffers from high variance or high bias, and whether the collection of more data could help to address this problem.But before we discuss how to plot learning curves in sckit-learn\n","150a4881":"Using the preceding code, we initialized a GridSearchCV object from the sklearn.grid_search module to train and tune a support vector machine (SVM) pipeline. We set the param_grid parameter of GridSearchCV to a list of dictionaries to specify the parameters that we'd want to tune. For the linear SVM, we only evaluated the inverse regularization parameter C; for the RBF kernel SVM, we tuned both the C and gamma parameter. Note that the gamma parameter is speci c to kernel SVMs. After we used the training data to perform the grid search, we obtained the score of the best-performing model via the best_score_ attribute and looked at its parameters, that can be accessed via the best_params_ attribute. In this particular case, the linear SVM model with 'clf__C'= 0.1' yielded the best k-fold cross- validation accuracy: 97.8 percent.\n\nFinally, we will use the independent test dataset to estimate the performance of the best selected model, which is available via the best_estimator_ attribute of the GridSearchCV object:","6044f88d":"Although the previous code example was useful to illustrate how k-fold cross-validation works, scikit-learn also implements a k-fold cross-validation scorer, which allows us to evaluate our model using strati ed k-fold cross-validation more ef ciently","584fff5f":"Via the train_sizes parameter in the learning_curve function, we can control the absolute or relative number of training samples that are used to generate the learning curves. Here, we set train_sizes=np.linspace(0.1, 1.0, 10) to use 10 evenly spaced relative intervals for the training set sizes. By default, the learning_curve function uses strati ed k-fold cross-validation to calculate the cross-validation accuracy, and we set k =10 via the cv parameter. Then, we simply calculate the average accuracies from the returned cross-validated training and test scores for the different sizes of the training set, which we plotted using matplotlib's plot function. Furthermore, we add the standard deviation of the average accuracies to the plot using the fill_between function to indicate the variance of the estimate.","4bccbc93":"The intermediate steps in a pipeline constitute scikit-learn transformers, and the\nlast step is an estimator. In the preceding code example, we built a pipeline that consisted of two intermediate steps, a StandardScaler and a PCA transformer, and a logistic regression classi er as a  nal estimator. When we executed the fit method on the pipeline pipe_lr, the StandardScaler performed fit and transform on the training data, and the transformed training data was then passed onto the next object in the pipeline, the PCA. Similar to the previous step, PCA also executed fit and transform on the scaled input data and passed it to the  nal element of the pipeline, the estimator. We should note that there is no limit to the number of intermediate steps in this pipeline. The concept of how pipelines work is summarized in the following  gure:","a09e3cdc":"As we can see here, the nested cross-validation performance of the SVM\nmodel (97.8 percent) is notably better than the performance of the decision tree (90.8 percent). Thus, we'd expect that it might be the better choice for classifying new data that comes from the same population as this particular dataset.","8afab189":"An extremely useful feature of the cross_val_score approach is that we can distribute the evaluation of the different folds across multiple CPUs on our machine. If we set the n_jobs parameter to 1, only one CPU will be used to evaluate the performances just like in our StratifiedKFold example previously. However, by setting n_jobs=2 we could distribute the 10 rounds of cross-validation to two CPUs (if available on our machine), and by setting n_jobs=-1, we can use all available CPUs on our machine to do the computation in parallel.\n","e05f74c8":"### The holdout method\n\nA better way of using the holdout method for model selection is to separate the data into three parts: a training set, a validation set, and a test set. The training set is used to  t the different models, and the performance on the validation set is then used for the model selection. The advantage of having a test set that the model hasn't seen before during the training and model selection steps is that we can obtain a less biased estimate of its ability to generalize to new data","e9913d73":"Using LabelEncoder, we transform the class labels from their original string representation\n(M and B) into integers:","67c9d572":"### Optimizing the precision and recall of a classi cation model","14f0495b":"### Addressing overfitting and underfitting with validation curves","90e78559":"## Looking at different performance evaluation metrics\n### Reading a confusion matrix","ecbde784":"Although grid search is a powerful approach for  nding the optimal set of parameters, the evaluation of all possible parameter combinations is also computationally very expensive. An alternative approach to sampling different parameter combinations using scikit-learn is randomized search. Using the RandomizedSearchCV class in scikit-learn, we can draw random parameter combinations from sampling distributions with a speci ed budget. More details and examples for its usage can be found\nat http:\/\/scikit-learn.org\/stable\/modules\/grid_search.html #randomized-parameter-optimization"}}