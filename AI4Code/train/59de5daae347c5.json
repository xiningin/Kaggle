{"cell_type":{"32251cae":"code","dc441344":"code","33f4f266":"code","6e840896":"code","ebe23aa5":"code","65a35cb6":"code","ef1ca821":"code","67f343bc":"code","5b15976d":"code","d737ef10":"code","9f7c944c":"code","b22e3470":"code","0cede81b":"code","b036018b":"code","bf77c702":"code","4c85809d":"code","58dd145b":"code","16fd614d":"code","af980619":"code","c21a1c10":"code","7608b4d4":"code","1df5e727":"code","babafef7":"code","bdd2e05d":"code","987a971d":"code","c891a89a":"code","abf0bfd0":"code","d0bce6af":"code","b2cb4de8":"code","c882034a":"code","a6d311b1":"code","f9bb9271":"code","b8262973":"code","9afb26d9":"code","f5f79f96":"code","862c5fa4":"markdown","d367e050":"markdown","eea4f2e1":"markdown"},"source":{"32251cae":"# by Grossmend, 2018","dc441344":"import pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import ensemble, tree, linear_model\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers","33f4f266":"# train data\ntrain_data = pd.read_csv('\/kaggle\/input\/train.csv')\n\n# test data\ntest_data = pd.read_csv('\/kaggle\/input\/test.csv')\n\n# concat train and test data in one DataFrame with keys\nall_data = pd.concat([train_data, test_data], keys=['train', 'test'], axis=0, sort=False)\n\n# set option display number columns\npd.set_option('display.max_columns', train_data.shape[1])\n\n# show first 10 row data\nall_data.head(7)","6e840896":"# view size datasets\nprint('Size train_data:', all_data.loc['train'].shape)\nprint('Size test data:', all_data.loc['test'].shape)","ebe23aa5":"# view missing data in train and test data by percentage\nnan_values = pd.concat([(train_data.isnull().sum() \/  train_data.isnull().count())*100,\n                        (test_data.isnull().sum() \/ test_data.isnull().count())*100], axis=1, keys=['Train', 'Test'], sort=False)\nprint('true')\nnan_values[nan_values.sum(axis=1) > 0].sort_values(by=['Train'], ascending=False)","65a35cb6":"# view info without 'SalePrices'\nall_data[all_data.columns.difference(['SalePrice'])].info()","ef1ca821":"# show counts each types in data\nall_data.get_dtype_counts()","67f343bc":"# check duplecated field \"id\"\nany(all_data['Id'].duplicated())","5b15976d":"# show correlation of data\ncorrmat = train_data.drop('Id', axis=1).corr()\nplt.subplots(figsize=(13,13))\nmask = np.zeros_like(corrmat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corrmat, mask=mask, vmax=0.9, cmap=\"YlGnBu\", square=True, cbar_kws={\"shrink\": .5}, linewidths=0.6);\nplt.title('correlation of data');\n\n# more settings: 'https:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html'","d737ef10":"# show most correlated features from field 'SalePrice'\ncorr=train_data.corr()[\"SalePrice\"]\ncorr[np.argsort(corr, axis=0)[::-1]]","9f7c944c":"# show descriptive statistics summary field \"SalePrice\"\ntrain_data['SalePrice'].describe()","b22e3470":"# show distribution field 'SalePrice'\nplt.subplots(figsize=(17,7));\nsns.distplot(train_data['SalePrice'], color='black', bins=100);\nplt.title('distribution \"SalePrice\"');","0cede81b":"# show skewness and kurtosis (\u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u0430\u0441\u0441\u0438\u043c\u0435\u0442\u0440\u0438\u0438 \u0438 \u0430\u043a\u0441\u0446\u0435\u0441\u0441\u0430)\nprint('Skewness:', train_data['SalePrice'].skew())\nprint('Kurtosis:', train_data['SalePrice'].kurt())","b036018b":"# log transformation of train labels (SalePrice)\ntrain_labels = np.log(train_data['SalePrice'])\nplt.subplots(figsize=(17,7));\nsns.distplot(train_labels, color='green', bins=100);\nplt.title('Log transformation \"SalePrice\"');","bf77c702":"# delete field \"SalePrice\" from data\nif 'SalePrice' in all_data:\n    all_data.drop('SalePrice', inplace=True, axis=1)\nelse:\n    print('no field \"SalePrice\"')\nall_data.shape","4c85809d":"# delete do not need fields (another analysis, see more: https:\/\/blog.grossmend.com\/blog)\n\ndrop_list = [\n    '3SsnPorch',\n    'BsmtFinSF1',\n    'BsmtFinSF2', \n    'BsmtFullBath',\n    'BsmtHalfBath',\n    'BsmtUnfSF',\n    'EnclosedPorch',\n    'Fence',\n    'Functional',\n    'GarageArea',\n    'GarageCond',\n    'GarageYrBlt',\n    'Heating',\n    'LowQualFinSF',\n    'MasVnrArea',\n    'MiscFeature',\n    'MiscVal',\n    'OpenPorchSF',\n    'PoolArea',\n    'PoolQC',\n    'RoofMatl',\n    'ScreenPorch',\n    'Utilities',\n    'WoodDeckSF',\n]\n\nall_data.drop(drop_list, axis=1, errors='ignore', inplace=True)\nall_data.shape","58dd145b":"# show data first 7 rows\nall_data.head(7)","16fd614d":"# show missing values each field\nnan_values = pd.concat([(all_data.isnull().sum() \/  all_data.isnull().count())*100], axis=1, keys=['all_data'], sort=False)\nnan_values[nan_values.sum(axis=1) > 0].sort_values(by=['all_data'], ascending=False)","af980619":"# missing nan values\nif all_data.isnull().values.any():\n    all_data['Alley'] = all_data['Alley'].fillna('no_access')\n    all_data['FireplaceQu'] = all_data['FireplaceQu'].fillna('no_fp')\n    all_data['LotFrontage'] = all_data['LotFrontage'].fillna(all_data['LotFrontage'].mean())\n    all_data['MasVnrType'] = all_data['MasVnrType'].fillna(all_data['MasVnrType'].mode()[0])\n    all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n    all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n    all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n    if 'TotalBsmtSF' in all_data:\n        all_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna(0)\n    all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior1st'].mode()[0])\n    all_data['GarageCars'] = all_data['GarageCars'].fillna(0.0)\n    all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n    all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n    for col in ('GarageType', 'GarageFinish', 'GarageQual'): all_data[col] = all_data[col].fillna('no_garage') \n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'): all_data[col] = all_data[col].fillna('no_bsmt')\nelse:\n    print('all values is not missing')","c21a1c10":"# add need columns and drop do not need columns after missing\nif set(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']).issubset(all_data.columns):\n    all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n    all_data.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], axis=1, inplace=True)\nelse:\n    print(\"no found fields ('TotalBsmtSF', '1stFlrSF', '2ndFlrSF')\")","7608b4d4":"# view missing data in train and test data by percentage\nnan_values = pd.concat([(all_data.isnull().sum() \/  all_data.isnull().count())*100], axis=1, keys=['all_data'], sort=False)\nnan_values = nan_values[nan_values.sum(axis=1) > 0]\nif nan_values.shape[0] > 0:\n    nan_values.sort_values(by=['all_data'], ascending=False)\nelse:\n    print('no missing values')","1df5e727":"# convert fields to categorical\nall_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\nall_data['KitchenAbvGr'] = all_data['KitchenAbvGr'].astype(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","babafef7":"# getting number fields from object\nfor col in all_data.dtypes[all_data.dtypes == 'object'].index:\n    all_data[col] = all_data[col].astype('category')\n    all_data[col] = all_data[col].cat.codes","bdd2e05d":"# delete \"Id\" field\nall_data = all_data.drop('Id', axis=1)","987a971d":"# normalize all values\nall_data=(all_data-all_data.mean())\/all_data.std()\nprint(all_data.shape)\nall_data.head(7)","c891a89a":"# split dataset train data and test data for ML\nX_model = all_data.loc['train'].select_dtypes(include=[np.number])\ny_model = np.log(train_data['SalePrice'])\n\nY_finish = all_data.loc['test'].select_dtypes(include=[np.number])","abf0bfd0":"# split data train and test\nX_train, X_test, y_train, y_test = train_test_split(X_model, y_model, test_size=0.15)","d0bce6af":"def build_model(insh):\n    \n    model = models.Sequential()\n    model.add(layers.Dense(32, activation='relu', input_shape=(insh,)))\n    model.add(layers.Dropout(0.3))\n    model.add(layers.Dense(16, activation='sigmoid'))\n    model.add(layers.Dense(1))\n    \n    opt = optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.99, epsilon=None, decay=0.0, amsgrad=False)\n    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n    \n    return model","b2cb4de8":"# fit NN model\ninput_shape = X_train.shape[1]\nDL_model = build_model(input_shape)\n\n# get initial weights model\ninitial_weights = DL_model.get_weights()\n\nhistory = DL_model.fit(X_train.values,\n                       y_train.values,\n                       epochs=300,\n                       batch_size=128,\n                       verbose=0,\n                       validation_data=(X_test, y_test))","c882034a":"# check scores model\nDL_model.evaluate(X_test.values, y_test.values)","a6d311b1":"n = 100\n\nplt.subplots(figsize=(17,7));\nplt.plot(history.history['mean_absolute_error'][n:])\nplt.plot(history.history['val_mean_absolute_error'][n:])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.subplots(figsize=(17,7));\nplt.plot(history.history['loss'][n:])\nplt.plot(history.history['val_loss'][n:])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","f9bb9271":"# # kross k validation\n\n# kfold = KFold(n_splits=5, shuffle=True)\n\n# scores = []\n\n# nn_model = None\n# nn_model = build_model(X_model.shape[1])\n\n# for _ in range(5):\n#     shf = shuffle(X_model, y_model)\n#     for train, test in kfold.split(shf[0].reset_index(drop=True), shf[1].reset_index(drop=True)):\n#         nn_model.set_weights(initial_weights)\n#         nn_model.fit(X_model.iloc[train].values,\n#                      y_model.iloc[train].values,\n#                      epochs=300,\n#                      batch_size=128,\n#                      verbose=0)\n#         acc = nn_model.evaluate(X_model.iloc[test].values, y_model.iloc[test].values, verbose=0)[1]\n#         scores.append(acc)\n#         print('accuracy step ' + str(len(scores)) + ': ', acc)\n# print('mean:', np.mean(scores))    ","b8262973":"# create finish model and train on complete train data\ninput_shape = X_model.shape[1]\nDL_model_finish = build_model(input_shape)\nDL_model_finish.set_weights(initial_weights)\n\nhistory = DL_model_finish.fit(X_model.values,\n                              y_model.values,\n                              epochs=300,\n                              batch_size=128,\n                              verbose=0)","9afb26d9":"# get finish predict values\nprediction = np.exp(DL_model_finish.predict(Y_finish.values))","f5f79f96":"# save to CSV\npd.DataFrame({'Id': test_data['Id'], 'SalePrice': prediction.flatten()}).to_csv('submission_nn_keras.csv', index=False)","862c5fa4":"## <b><font color='3C89F9'>1. Data preparation<\/font><\/b>","d367e050":"### <b><font color='green'>Deep Learning<\/font> by Keras<\/b>","eea4f2e1":"## <b><font color='3C89F9'>2. Machine Learning<\/font><\/b>"}}