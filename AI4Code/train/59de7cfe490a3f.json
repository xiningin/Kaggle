{"cell_type":{"dd1ab0fd":"code","765108e5":"code","876abe20":"code","a81c158d":"code","b4a3ce0b":"code","3ca448a9":"code","7750bdd6":"code","88a0ffd4":"code","b8b05f6c":"code","7c9d596d":"code","c5108f97":"code","9c9576b9":"code","d22486da":"code","73dc8059":"code","617dd68b":"code","180d8fa2":"code","97c566d9":"code","2f5a822f":"code","43b89e85":"code","6cbc7d7b":"code","d5162288":"code","8f8e5a18":"code","59185325":"code","c5dee64c":"code","3c401845":"code","ca7ac4de":"code","0810c1a5":"code","28f10239":"code","42db421f":"code","4ac031f5":"code","832662c0":"code","c2b84ef2":"code","1dbe25c9":"code","f28d5700":"code","174d5af7":"code","f62a217d":"code","59fb1ce6":"code","cbef1321":"code","a036b6e5":"code","fa7dcc26":"code","9af7abb1":"code","06f2a631":"code","6d286d04":"code","c39a5bb7":"code","02b0d344":"code","9b052ba8":"code","41dca4f7":"code","9e1980e8":"code","c38ba927":"code","129a4b46":"code","aed35a1f":"code","986ad901":"code","fc0edc77":"code","16a8016f":"code","26acb1c1":"code","9bf6e869":"code","d6c3db8b":"code","9b36b1f2":"code","f2e30221":"code","e3924269":"code","6bc64b90":"code","2b4ff939":"code","cfa72ceb":"code","a72b070e":"code","4b13a106":"code","d56cd0aa":"code","42ff2882":"code","7efd16b3":"code","36452599":"code","2ec7c572":"code","6a2dc261":"code","b3f08d58":"code","7f7b3d72":"code","f8d59781":"code","48de440b":"code","a1892e5d":"code","02fd74f9":"code","5f34e57b":"code","95cd209a":"code","d011c7e2":"code","4561432b":"code","51b661ef":"code","46741b8d":"code","17bb8749":"code","cbfa332e":"code","4cd8ac72":"code","dec4a462":"code","25aeb252":"code","3ff6f075":"code","9f56def0":"code","51bbd374":"code","abbf4906":"code","1d138e4c":"code","40993892":"code","c5e4f6e3":"code","37bdf25c":"code","35866d4c":"code","71c13eca":"code","6ffa9302":"code","4e77018d":"code","652d8df6":"markdown","8ba1d03f":"markdown","8013c8ed":"markdown","17786a90":"markdown","91ec6a25":"markdown","79116785":"markdown","0a75e100":"markdown","deafbb4d":"markdown","19417bbd":"markdown","07a1d139":"markdown","57b2b21e":"markdown","df9cc827":"markdown","cfc3cf2d":"markdown","b4eafd5b":"markdown","0e7bb956":"markdown","50a5f9ad":"markdown","673ba214":"markdown","ff250b02":"markdown","c3db741b":"markdown","a1bbd111":"markdown","437f553d":"markdown","96cbe97f":"markdown","aaf5af6e":"markdown","bbb12bfb":"markdown","d2a49c70":"markdown","5c430e4c":"markdown","e3170cdf":"markdown","048c4dd7":"markdown","ca8f1c2b":"markdown","5e59cb3b":"markdown","ce1b7940":"markdown","fff63b9e":"markdown","f515f883":"markdown","593a9b90":"markdown","936110c6":"markdown","63cec2e4":"markdown","d41d3cd4":"markdown","4d4e3918":"markdown","eac1b08a":"markdown","a5985f06":"markdown","13e858d5":"markdown","13393deb":"markdown","bbf45356":"markdown","584d06dc":"markdown","8452be42":"markdown","8f9d8406":"markdown","a1a57550":"markdown"},"source":{"dd1ab0fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","765108e5":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom scipy import stats\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import RobustScaler\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\npd.pandas.set_option(\"display.max_columns\",None)\nprint(\"all necessary libraries are imported\")","876abe20":"train=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain.head()","a81c158d":"test=pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest.head()","b4a3ce0b":"train.shape,test.shape","3ca448a9":"train_cat=list(train.select_dtypes(include='object'))\ntrain_cat\n","7750bdd6":"train_num=list(train.select_dtypes(exclude='object'))\ntrain_num","88a0ffd4":"train['Survived'].value_counts().plot.bar(color=['r','g'])","b8b05f6c":"plt.figure(1)\nplt.subplot(221)\ntrain['Sex'].value_counts(normalize=True).plot.bar(figsize=(10,10),color=['b','r'])\nplt.subplot(223)\ntrain['Embarked'].value_counts(normalize=True).plot.bar(figsize=(10,10),color=['y','g','r'])","7c9d596d":"plt.figure(1)\nplt.subplot(221)\ntrain['Pclass'].value_counts(normalize=True).plot.bar(figsize=(10,10),title=\"Pclass\")\nplt.subplot(223)\ntrain['Pclass'].plot.box(figsize=(10,10))","c5108f97":"plt.figure(1)\nplt.subplot(221)\ntrain['SibSp'].value_counts(normalize=True).plot.bar(figsize=(10,10),title=\"SibSp\")\nplt.subplot(223)\ntrain['SibSp'].plot.box(figsize=(10,10))","9c9576b9":"plt.figure(1)\nplt.subplot(221)\ntrain['Parch'].value_counts(normalize=True).plot.bar(figsize=(10,10),title=\"Parch\")\nplt.subplot(223)\ntrain['Parch'].plot.box(figsize=(10,10))","d22486da":"train_cat","73dc8059":"sex=pd.crosstab(train['Sex'],train['Survived'])\nsex.div(sex.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(4,4))","617dd68b":"embarked=pd.crosstab(train['Embarked'],train['Survived'])\nembarked.div(embarked.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(4,4))","180d8fa2":"train_num","97c566d9":"pclass=pd.crosstab(train['Pclass'],train['Survived'])\npclass.div(pclass.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(4,4))","2f5a822f":"sibsp=pd.crosstab(train['SibSp'],train['Survived'])\nsibsp.div(sibsp.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(4,4))","43b89e85":"parch=pd.crosstab(train['Parch'],train['Survived'])\nparch.div(parch.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(4,4))","6cbc7d7b":"train.isnull().any()","d5162288":"train_missing_obj_col=[]\nfor col in train_cat:\n    if train[col].isnull().any():\n        train_missing_obj_col.append(col)\ntrain_missing_obj_col        ","8f8e5a18":"train_missing_num_col=[]\nfor col in train_num:\n    if train[col].isnull().any():\n        train_missing_num_col.append(col)\ntrain_missing_num_col     ","59185325":"temp=train[['Age']]\ntemp","c5dee64c":"train.drop(['Age'],inplace=True,axis=1)","3c401845":"from sklearn.impute import SimpleImputer\nmy_imputer=SimpleImputer()\nimputed_temp = pd.DataFrame(my_imputer.fit_transform(temp))\nimputed_temp.columns = temp.columns","ca7ac4de":"train=pd.concat([train,imputed_temp],axis=1)","0810c1a5":"train","28f10239":"train['Embarked'].fillna(train['Embarked'].mode(),inplace=True)","42db421f":"dummy1=pd.get_dummies(train[['Sex','Embarked']],drop_first=True)\ndummy1","4ac031f5":"train.drop(train_missing_obj_col ,axis=1,inplace=True)","832662c0":"train=pd.concat([train,dummy1],axis=1)","c2b84ef2":"train.drop(['Name','PassengerId','Sex','Ticket'],axis=1,inplace=True)","1dbe25c9":"train","f28d5700":"train.shape","174d5af7":"train['Age']=np.log(train['Age']+1)","f62a217d":"train['Fare']=np.log(train['Fare']+1)","59fb1ce6":" (train['Age']).plot(kind = 'density', title = 'Log Age distribution', fontsize=14, figsize=(10, 6))","cbef1321":" (train['Survived']).plot(kind = 'density', title = 'Survived', fontsize=14, figsize=(10, 6))","a036b6e5":"test.head()","fa7dcc26":"test_cat_col=list(test.select_dtypes(include='object'))\ntest_cat_col","9af7abb1":"test_num_col=list(test.select_dtypes(exclude='object'))\ntest_num_col","06f2a631":"test_missing_cat_col=[col for col in test_cat_col if test[col].isnull().any()]\ntest_missing_cat_col","6d286d04":"test_missing_num_col=[col for col in test_num_col if test[col].isnull().any()]\ntest_missing_num_col","c39a5bb7":"temp1=test[['Age','Fare']]","02b0d344":"imputer=SimpleImputer()\nimputed_temp1=pd.DataFrame(imputer.fit_transform(temp1))\nimputed_temp1.columns=temp1.columns","9b052ba8":"test.drop(['Age','Cabin','Name','Fare'],axis=1,inplace=True)","41dca4f7":"test=pd.concat([test,imputed_temp1],axis=1)\ntest","9e1980e8":"dummy2=pd.get_dummies(test[['Sex','Embarked']],drop_first=True)\ndummy2","c38ba927":"test.drop(['PassengerId','Sex','Embarked','Ticket'],axis=1,inplace=True)","129a4b46":"test=pd.concat([test,dummy2],axis=1)\ntest","aed35a1f":"test.shape","986ad901":"test['Age']=np.log(test['Age'])","fc0edc77":"test['Fare']=np.log(test['Fare']+1)","16a8016f":" (test['Age']).plot(kind = 'density', title = 'Log Age distribution', fontsize=14, figsize=(10, 6))","26acb1c1":"y=train['Survived']","9bf6e869":"train.drop(['Survived'],axis=1,inplace=True)","d6c3db8b":"train.head()","9b36b1f2":"test.head()","f2e30221":"train.shape,test.shape","e3924269":"x_train,x_test,y_train,y_test=train_test_split(train,y,random_state=42)","6bc64b90":"logistic=LogisticRegression(max_iter=100,random_state=1,n_jobs=-1)\nlogistic.fit(x_train,y_train)\npred1=logistic.predict(x_test)\npred1","2b4ff939":"print(f'The train accuracy is {logistic.score(x_train,y_train)}')\nfrom sklearn.metrics import accuracy_score\nprint('Logistic Regresson model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, pred1)))","cfa72ceb":"# param_grid = { \"criterion\" : [\"gini\",\"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \n#               \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\n# rf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n# clf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\n# clf.fit(x_train, y_train)\n# clf.best_params_","a72b070e":"random_forest = RandomForestClassifier(criterion = \"entropy\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 12,\n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=42, \n                                       n_jobs=-1)\n\nrandom_forest.fit(x_train, y_train)\npred2 = random_forest.predict(x_test)\n\nrandom_forest.score(x_train, y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","4b13a106":"print('Random Forest Classifier model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, pred2)))","d56cd0aa":"sgd = SGDClassifier(random_state=42)\nsgd.fit(x_train, y_train)\npred3 = sgd.predict(x_test)\n\nsgd.score(x_train, y_train)\n\nacc_sgd = round(sgd.score(x_train, y_train) * 100, 2)\nacc_sgd","42ff2882":"print('Stochastic Gradient Descent (SGD) model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, pred3)))","7efd16b3":"\nsgdc=SGDClassifier(random_state=0)\n\nparameters = {\n            'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n             'loss':['hinge','log'], 'penalty':['l1','l2']}\nsearcher = GridSearchCV(sgdc, parameters, cv=10)\nsearcher.fit(x_train, y_train)\nprint(searcher.best_params_)\nprint(searcher.best_score_)","36452599":"sgd = SGDClassifier(random_state=0,alpha=0.01,loss='log',penalty='l2')\nsgd.fit(x_train, y_train)\npred3 = sgd.predict(x_test)\npred3","2ec7c572":"print('Stochastic Gradient Descent (SGD) tuned model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, pred3)))","6a2dc261":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train, y_train) \npred4 = knn.predict(x_test)  \nacc_knn = round(knn.score(x_train, y_train) * 100, 2)\nprint('Stochastic Gradient Descent (SGD) model train accuracy score: {0:0.4f}'. format(acc_knn))\nprint('Stochastic Gradient Descent (SGD) model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, pred4)))","b3f08d58":"linear_svc = LinearSVC(max_iter=100000,dual=True)\nlinear_svc.fit(x_train, y_train)\n\npred5 = linear_svc.predict(x_test)\n\nacc_linear_svc = round(linear_svc.score(x_train, y_train) * 100, 2)\nprint('Linear Support Vector Machine model train accuracy score: {0:0.4f}'. format(acc_linear_svc))\nprint('Linear Support Vector Machine model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, pred5)))","7f7b3d72":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(x_train, y_train)\npred6 = decision_tree.predict(x_test)\nacc_decision_tree = round(decision_tree.score(x_train,y_train) * 100, 2)\nprint('Decision Tree model train accuracy score: {0:0.4f}'. format(acc_decision_tree))\nprint('Decision Tree model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, pred6)))","f8d59781":"grid_param = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10],\n    'splitter' : ['best', 'random'],\n    'min_samples_leaf' : [1, 2, 3, 5, 7],\n    'min_samples_split' : [1, 2, 3, 5, 7],\n    'max_features' : ['auto', 'sqrt', 'log2']\n}\n\ndecision = GridSearchCV(decision_tree, grid_param, cv = 5, n_jobs = -1, verbose = 1)\ndecision.fit(x_train, y_train)\nprint(decision.best_params_)\nprint(decision.best_score_)","48de440b":"decision_tree = DecisionTreeClassifier(criterion='entropy',\n                                      max_depth=7,\n                                      splitter='best',\n                                      min_samples_leaf=5,\n                                      min_samples_split=3,\n                                      max_features='log2')\ndecision_tree.fit(x_train, y_train)\npred_6 = decision_tree.predict(x_test)\nprint('Decision Tree model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, pred_6)))","a1892e5d":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = decision_tree)\nadb.fit(x_train,y_train)\nadb_pred=adb.predict(x_test)\nacc_adb=adb.score(x_train,y_train)\nprint('Adaboost Classifier model train accuracy score: {0:0.4f}'. format(acc_adb))\nprint('Adaboost Classifier model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, adb_pred)))","02fd74f9":"grid_param = {\n    'n_estimators' : [100, 120, 150, 180, 200],\n    'learning_rate' : [0.01, 0.1, 1, 10],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\n\nada = GridSearchCV(adb, grid_param, cv = 5, n_jobs = -1, verbose = True)\nada.fit(x_train, y_train)\nprint(ada.best_params_)\nprint(ada.best_score_)","5f34e57b":"adb = AdaBoostClassifier(n_estimators=200,\n                        learning_rate=0.01,\n                        algorithm='SAMME.R')\nadb.fit(x_train,y_train)\npred7=adb.predict(x_test)\nprint('Adaboost Classifier model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, pred7)))","95cd209a":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\ngb.fit(x_train, y_train)\npred8=gb.predict(x_test)\nacc_gb=gb.score(x_train,y_train)\nprint('Gradient Boosting model train accuracy score: {0:0.4f}'. format(acc_gb))\nprint('Gradient Boosting model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, pred8)))","d011c7e2":"import lightgbm as lgb\nlgbm= lgb.LGBMClassifier()\nlgbm.fit(x_train,y_train)\nlgbm_pred=lgbm.predict(x_test)\nacc_lgbm=lgbm.score(x_train,y_train)\nprint('lgbm model train accuracy score: {0:0.4f}'. format(acc_lgbm))\nprint('lgbm model accuracy score: {0:0.4f}'. format(accuracy_score(y_test,lgbm_pred)))","4561432b":"fit_params={\"early_stopping_rounds\":30, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(x_test,y_test)],\n            'eval_names': ['valid'],\n            'verbose': 100,\n            'categorical_feature': 'auto'}","51b661ef":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nparam_test ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(50, 100), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}","46741b8d":"from sklearn.model_selection import RandomizedSearchCV\nclf = lgb.LGBMClassifier(max_depth=-1, random_state=42, silent=True, metric='None', n_jobs=4, n_estimators=100)\ngs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=100,\n    scoring='roc_auc',\n    cv=3,\n    refit=True,\n    random_state=314,\n    verbose=True)","17bb8749":"gs.fit(x_train, y_train, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","cbfa332e":"opt_params={'colsample_bytree':  0.952164731370897, 'min_child_samples': 61, \n            'min_child_weight': 0.01, 'num_leaves': 38, 'reg_alpha': 0, \n            'reg_lambda': 0.1, 'subsample':0.3029313662262354} ","4cd8ac72":"clf_sw = lgb.LGBMClassifier(**clf.get_params())\n#set optimal parameters\nclf_sw.set_params(**opt_params)","dec4a462":"clf_sw.fit(x_train,y_train)\npred9=clf_sw.predict(x_test)\nprint('lgbm model accuracy score: {0:0.4f}'. format(accuracy_score(y_test,pred9)))","25aeb252":"from catboost import CatBoostClassifier\n\ncat = CatBoostClassifier(iterations=10)\ncat.fit(x_train, y_train)\npred10=cat.predict(x_test)\nacc_cat=cat.score(x_train,y_train)\nprint('CatBoostClassifier model accuracy score: {0:0.4f}'. format(accuracy_score(y_test,pred10)))","3ff6f075":"from sklearn.ensemble import VotingClassifier \nclassifiers = [('Gradient Boosting Classifier', gb), ('Stochastic Gradient Boosting', sgd),  ('Cat Boost Classifier', cat), \n                ('Decision Tree', decision_tree), ('Light Gradient', clf_sw),\n               ('Random Forest', random_forest), ('Ada Boost', adb), ('Logistic', logistic)]\nvc = VotingClassifier(estimators = classifiers)\nvc.fit(x_train, y_train) \nvoting_pred = vc.predict(x_test)\naccuracy = accuracy_score(y_test,voting_pred)\nprint('Voting Classifier: {:.3f}'.format(accuracy))","9f56def0":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nprint(f\"{confusion_matrix(y_test, vc.predict(x_test))}\\n\")\nprint(classification_report(y_test, vc.predict(x_test)))","51bbd374":"importances = pd.DataFrame({'feature':train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)","abbf4906":"importances.plot.bar()","1d138e4c":"models = pd.DataFrame({\n    'Model' : ['Logistic Regression','Random Forest Classifier','Stochastic Gradient Boosting', 'KNN','LSVM', 'Decision Tree Classifier','Ada Boost Classifier',\n             'Gradient Boosting Classifier','Lightgbm', 'Cat Boost',  'Voting Classifier'],\n    'Score' : [accuracy_score(y_test,pred1), accuracy_score(y_test,pred2), accuracy_score(y_test,pred3),\n               accuracy_score(y_test,pred4),accuracy_score(y_test,pred5), accuracy_score(y_test,pred_6),\n               accuracy_score(y_test,pred7), accuracy_score(y_test,pred8), accuracy_score(y_test,pred9)\n               ,accuracy_score(y_test,pred10),accuracy]\n})\n\n\nmodels.sort_values(by = 'Score', ascending = False)","40993892":"final_pred=vc.predict(test)","c5e4f6e3":"submission=pd.read_csv('..\/input\/titanic\/gender_submission.csv')","37bdf25c":"submission","35866d4c":"submission['Survived']=final_pred","71c13eca":"submission","6ffa9302":"pd.DataFrame(submission,columns=['PassengerId','Survived']).to_csv('titanic_new.csv',index=False)","4e77018d":"pd.read_csv('titanic_new.csv')","652d8df6":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : orange; border-radius: 5px 5px;\"><strong>Imputation In Categorical Col<\/strong><\/p>","8ba1d03f":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>CatBoost Classifier<\/strong><\/p>","8013c8ed":"<p style = \"font-size : 35px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightblue; border-radius: 5px 5px;\"><strong>Missing Values Imputation In Train Dataset<\/strong><\/p>","17786a90":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : orange; border-radius: 5px 5px;\"><strong>Numerical Feature<\/strong><\/p> ","91ec6a25":"# Now buliding the model with best parameters","79116785":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : orange; border-radius: 5px 5px;\"><strong>Imputation In Numerical Col<\/strong><\/p>","0a75e100":"<p style = \"font-size : 35px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightblue; border-radius: 5px 5px;\"><strong>Removing the Skewness In the Train Data Col<\/strong><\/p>","deafbb4d":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>LightGBM<\/strong><\/p>","19417bbd":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : orange; border-radius: 5px 5px;\"><strong>Target Feature<\/strong><\/p> ","07a1d139":"<p style = \"font-size : 50px; color : blue ; font-family : 'Comic Sans MS'; text-align : center; background-color : yellow; border-radius: 5px 5px;\"><strong>Titanic EDA and Prediction<\/strong><\/p>","57b2b21e":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Model Comaprison<\/strong><\/p>","df9cc827":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Decision Tree<\/strong><\/p>","cfc3cf2d":"> class 1 has most survival rate and class 3 has least survival rate","b4eafd5b":"# Now buliding the model with best parameters","0e7bb956":"it says that most of the people have dead","50a5f9ad":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Linear Support Vector Machine<\/strong><\/p># Linear Support Vector Machine","673ba214":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Logistic Regression<\/strong><\/p>","ff250b02":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : orange; border-radius: 5px 5px;\"><strong>Categorical Feature<\/strong><\/p> ","c3db741b":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Adaboost Classifier<\/strong><\/p>","a1bbd111":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Final Prediction<\/strong><\/p>","437f553d":"<p style = \"font-size : 35px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightblue; border-radius: 5px 5px;\"><strong>Missing Values Imputation In Test Dataset<\/strong><\/p>","96cbe97f":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Importing all necessary libraries<\/strong><\/p> ","aaf5af6e":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Random Forest Classifier<\/strong><\/p>","bbb12bfb":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong> K Nearest Neighbor (KNN)<\/strong><\/p>","d2a49c70":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : orange; border-radius: 5px 5px;\"><strong>Numerical vs Target Feature<\/strong><\/p> ","5c430e4c":"# Hyperparameter Tuning","e3170cdf":"<p style = \"font-size : 35px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightblue; border-radius: 5px 5px;\"><strong>Univariate Analysis<\/strong><\/p> ","048c4dd7":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : orange; border-radius: 5px 5px;\"><strong>Categorical vs Target Feature<\/strong><\/p> ","ca8f1c2b":"> More males have dead comapre to females\nas if 1 shows alive and 0 shows death","5e59cb3b":"# Hyperparameter tuning","ce1b7940":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong> Stochastic Gradient Descent (SGD)<\/strong><\/p>","fff63b9e":"* > Around 80% of the passangers are male\n* > Around 75% of passangers have embarked as S","f515f883":"<p style = \"font-size : 35px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightblue; border-radius: 5px 5px;\"><strong>Removing the Skewness In the Test Data Col<\/strong><\/p>","593a9b90":"# Hyperparameter Tuning","936110c6":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : orange; border-radius: 5px 5px;\"><strong>Removing Target Variable from the Train Dataset<\/strong><\/p>","63cec2e4":"# Hyperparameter tuning","d41d3cd4":"<p style = \"font-size : 35px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightblue; border-radius: 5px 5px;\"><strong>Important Features<\/strong><\/p>","4d4e3918":"# **Hyperparameter tuning**","eac1b08a":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Gradient Boosting<\/strong><\/p>","a5985f06":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : orange; border-radius: 5px 5px;\"><strong>Imputation In Categorical Col<\/strong><\/p>","13e858d5":"<p style = \"font-size : 35px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightblue; border-radius: 5px 5px;\"><strong>Model Building<\/strong><\/p>","13393deb":"<p style = \"font-size : 35px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightblue; border-radius: 5px 5px;\"><strong>Feature Engineering on Test Dataset<\/strong><\/p>","bbf45356":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : orange; border-radius: 5px 5px;\"><strong>Imputation In Numerical Col<\/strong><\/p> ","584d06dc":"> The survival rate of embarked 'C' is greater than the rest two and the lowest survival rate is associated with embarked 'S'","8452be42":"# Now buliding the model with best parameters","8f9d8406":"<p style = \"font-size : 25px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Voting Classifier<\/strong><\/p>","a1a57550":"<p style = \"font-size : 35px; color : #34656d ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightblue; border-radius: 5px 5px;\"><strong>Bivariate Analysis<\/strong><\/p> "}}