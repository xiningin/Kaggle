{"cell_type":{"6d79e0cd":"code","6ecb9514":"code","f653d2a6":"code","3565c1be":"code","59514e25":"code","335faa73":"code","ca015e54":"code","b7a497ac":"code","49761d25":"code","6adfb2d8":"code","5b25b207":"code","1f1f5bb9":"code","2736b26a":"code","6a694a53":"code","85825c4c":"code","bb46bc01":"code","6eb43734":"code","119e66f5":"code","987bff03":"code","3cbd904b":"code","b5b5651c":"code","9a1a984b":"code","61efeb32":"code","84c9425d":"code","aea9e798":"code","9aa6d275":"code","9b13368b":"code","629b918a":"code","d1a7a60c":"code","4ee1e065":"code","d905eccb":"code","0b6cc8df":"code","71aef7f3":"code","176d946f":"code","3588d58c":"code","e9467ea1":"code","888e42d6":"code","748c3bf2":"code","bbfaffdc":"code","c86c814e":"code","d77332be":"code","75a7160a":"code","2f78f43f":"code","8fe2ee86":"code","90f3ba38":"code","691028e9":"code","de14a300":"code","21b5de36":"code","cf9b823a":"code","b5f9fbe6":"code","00cee92e":"code","d7fa1a0b":"code","fe24c640":"markdown","a55b0186":"markdown","5f70b76d":"markdown","bf3dbb57":"markdown","c8902f18":"markdown","8c2615b3":"markdown","25b69f42":"markdown","2068a981":"markdown","54d8364e":"markdown","c3c10def":"markdown","d09f48be":"markdown","14622b34":"markdown","f15e0d21":"markdown","8a06c3ce":"markdown"},"source":{"6d79e0cd":"# necessary imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nplt.style.use('fivethirtyeight')","6ecb9514":"from sklearn.datasets import load_boston\n\ndata = load_boston() # reading data","f653d2a6":"data","3565c1be":"# creating dataframe \n\ndf = pd.DataFrame(data.data, columns = data.feature_names)","59514e25":"# adding target value to the data\n\ndf['MEDV'] = data.target","335faa73":"df.head()","ca015e54":"df.describe()","b7a497ac":"df.info()","49761d25":"# looking at null values \n\ndf.isna().sum()","6adfb2d8":"# Let's see how data is distributed for every column\n\nplt.figure(figsize = (20, 15))\nplotnumber = 1\n\nfor column in df:\n    if plotnumber <= 14:\n        ax = plt.subplot(3, 5, plotnumber)\n        sns.distplot(df[column])\n        plt.xlabel(column, fontsize = 15)\n        \n    plotnumber += 1\n    \nplt.tight_layout()\nplt.show()","5b25b207":"# Plotting `Price` with remaining columns\n\nplt.figure(figsize = (20, 15))\nplotnumber = 1\n\nfor column in df:\n    if plotnumber <= 14:\n        ax = plt.subplot(3, 5, plotnumber)\n        sns.scatterplot(x = df['MEDV'], y = df[column])\n        \n    plotnumber += 1\n\nplt.tight_layout()\nplt.show()","1f1f5bb9":"# looking for outliers using box plot\n\nplt.figure(figsize = (20, 8))\nsns.boxplot(data = df, width = 0.8)\nplt.show()","2736b26a":"# creating features and label variable\n\nX = df.drop(columns = 'MEDV', axis = 1)\ny = df['MEDV']","6a694a53":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","85825c4c":"X_scaled","bb46bc01":"# checking for multicollinearity using `VIF` and `correlation matrix`\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\n\nvif['VIF'] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]\nvif['Features'] = X.columns\n\nvif","6eb43734":"# dropping 'TAX' column from data\n\nX.drop(columns = ['TAX'], axis = 1)","119e66f5":"# Heatmap\n\nfig, ax = plt.subplots(figsize = (16, 8))\nsns.heatmap(df.corr(), annot = True, fmt = '1.2f', annot_kws = {'size' : 10}, linewidth = 1)\nplt.show()","987bff03":"import statsmodels.formula.api as smf\n\nlm = smf.ols(formula = 'MEDV ~ RAD', data = df).fit()\nlm.summary()","3cbd904b":"lm = smf.ols(formula = 'MEDV ~ TAX', data = df).fit()\nlm.summary()","b5b5651c":"# removing \"RAD\" column\n\ndf.drop(columns = 'RAD', axis = 1, inplace = True)","9a1a984b":"df.head()","61efeb32":"# splitting data into training asnd test set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.30, random_state = 0)","84c9425d":"# fitting training data to model\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)","aea9e798":"# prediction of model\n\ny_pred = lr.predict(X_test)","9aa6d275":"# training accuracy of model\n\nlr.score(X_train, y_train)","9b13368b":"# test accuracy of model\n\nlr.score(X_test, y_test)","629b918a":"# creating a function to create adhusted R-Squared\n\ndef adj_r2(X, y, model):\n    r2 = model.score(X, y)\n    n = X.shape[0]\n    p = X.shape[1]\n    adjusted_r2 = 1 - (1 - r2) * (n - 1) \/ (n - p - 1)\n    \n    return adjusted_r2","d1a7a60c":"print(adj_r2(X_train, y_train, lr))","4ee1e065":"print(adj_r2(X_test, y_test, lr))","d905eccb":"from sklearn.linear_model import Lasso, LassoCV\n\nlasso_cv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\nlasso_cv.fit(X_train, y_train)","0b6cc8df":"# best alpha parameter\n\nalpha = lasso_cv.alpha_\nalpha","71aef7f3":"lasso = Lasso(alpha = lasso_cv.alpha_)\nlasso.fit(X_train, y_train)","176d946f":"lasso.score(X_train, y_train)","3588d58c":"lasso.score(X_test, y_test)","e9467ea1":"print(adj_r2(X_train, y_train, lasso))","888e42d6":"print(adj_r2(X_test, y_test, lasso))","748c3bf2":"from sklearn.linear_model import Ridge, RidgeCV\n\nalphas = np.random.uniform(0, 10, 50)\nridge_cv = RidgeCV(alphas = alphas, cv = 10, normalize = True)\nridge_cv.fit(X_train, y_train)","bbfaffdc":"# best alpha parameter\n\nalpha = ridge_cv.alpha_\nalpha","c86c814e":"ridge = Ridge(alpha = ridge_cv.alpha_)\nridge.fit(X_train, y_train)","d77332be":"ridge.score(X_train, y_train)","75a7160a":"ridge.score(X_test, y_test)","2f78f43f":"print(adj_r2(X_train, y_train, ridge))","8fe2ee86":"print(adj_r2(X_test, y_test, ridge))","90f3ba38":"from sklearn.linear_model import ElasticNet, ElasticNetCV\n\nelastic_net_cv = ElasticNetCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\nelastic_net_cv.fit(X_train, y_train)","691028e9":"# best alpha parameter\n\nalpha = elastic_net_cv.alpha_\nalpha","de14a300":"# l1 ratio \n\nelastic_net_cv.l1_ratio","21b5de36":"elastic_net = ElasticNet(alpha = elastic_net_cv.alpha_, l1_ratio = elastic_net_cv.l1_ratio)\nelastic_net.fit(X_train, y_train)","cf9b823a":"elastic_net.score(X_train, y_train)","b5f9fbe6":"elastic_net.score(X_test, y_test)","00cee92e":"print(adj_r2(X_train, y_train, elastic_net))","d7fa1a0b":"print(adj_r2(X_test, y_test, elastic_net))","fe24c640":"<p style = \"color : #f54748; font-size : 35px; font-family : 'Comic Sans MS';\"><strong>EDA<\/strong><\/p>","a55b0186":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>We still are getting the same r2 score. That means our Regression model has been well trained over the training data and there is no overfitting.<\/strong><\/p> ","5f70b76d":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>From OLS Regression Results we can conclude that removing \"RAD\" column will be good.<\/strong><\/p> ","bf3dbb57":"<p style = \"color : #f54748; font-size : 35px; font-family : 'Comic Sans MS';\"><strong>Lasso Regression<\/strong><\/p>","c8902f18":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>There are some outliers in data, so StandardScaler can help in scaling data.<\/strong><\/p> ","8c2615b3":"<p style = \"color : #f54748; font-size : 40px; font-family : 'Comic Sans MS'; text-align : center;\"><strong>Linear Regression and Regularization<\/strong><\/p>","25b69f42":"<p style = \"color : #f54748; font-size : 35px; font-family : 'Comic Sans MS';\"><strong>Elastic Net<\/strong><\/p>","2068a981":"<p style = \"font-size : 30px; color : #4e8d7c ; font-family : 'Comic Sans MS';  \"><strong>Data Description :-<\/strong><\/p>\n\n<ul>\n    <li style = \"color : #03506f; font-size : 18px; font-family : 'Comic Sans MS';\"><strong>Data :- Independent Variables also known as the x values.<\/strong><\/li>\n    <li style = \"color : #03506f; font-size : 18px; font-family : 'Comic Sans MS';\"><strong>feature_names :- The column names of the data.<\/strong><\/li>\n    <li style = \"color : #03506f; font-size : 18px; font-family : 'Comic Sans MS';\"><strong>target :- The target variable or the price of the houses(dependent variable) alse known as y value.<\/strong><\/li>\n<\/ul>","54d8364e":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>\"RAD\" and \"TAX\" columns are highly correlated which means multicollinearity is present so we have to remove one column.<\/strong><\/p> ","c3c10def":"<p style = \"color : #f54748; font-size : 35px; font-family : 'Comic Sans MS';\"><strong>Ridge Regression<\/strong><\/p>","d09f48be":"<p style = \"color : #f55c47; font-size : 35px; font-family : 'Comic Sans MS';\"><strong>If you like this kernel, Please do upvote.<\/strong><\/p>","14622b34":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>There are no missing values in the data.<\/strong><\/p> ","f15e0d21":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>It seems there are no missing values in data.<\/strong><\/p> ","8a06c3ce":"<p style = \"font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; \"><strong>Model r2 score is less on the test data so there is chance of overfitting, let's check this using regularization.<\/strong><\/p> "}}