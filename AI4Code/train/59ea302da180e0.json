{"cell_type":{"e0c0dc12":"code","3521d3e8":"code","1da12fcf":"code","93b27f46":"code","6b47a14f":"code","da8f520a":"code","3f76eb10":"code","b31b1f18":"code","548a4469":"code","e8a8b427":"code","d5bf878e":"code","5c482560":"code","f609d37a":"code","654b2a98":"code","60303ceb":"code","620e2027":"code","790415c2":"code","540f3402":"code","0dca5950":"code","549879f7":"code","ee70383a":"code","dd157efb":"code","9667628a":"code","4779d0e6":"code","f9932e4e":"code","c57d6a28":"code","b8e1d524":"code","fcf83dc5":"code","8276d11e":"code","5090d0f3":"code","f124aeef":"code","4c150ade":"code","2d026b97":"code","9b488e5f":"code","9cae6342":"code","df8e5639":"markdown","666b4f1e":"markdown","b2448f4b":"markdown","34afaf60":"markdown","06edad18":"markdown","a6aec54f":"markdown","21cfa287":"markdown","02e9deb6":"markdown","f75f8cdb":"markdown","db732a7e":"markdown","4aa9be09":"markdown"},"source":{"e0c0dc12":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization, Dense, Dropout, GlobalAveragePooling2D, GlobalMaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers.experimental import preprocessing","3521d3e8":"df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')","1da12fcf":"df['Id'] = df['Id'].apply(lambda x: '..\/input\/petfinder-pawpularity-score\/train\/'+ x +'.jpg')\ntest_df['Id'] = test_df['Id'].apply(lambda x: '..\/input\/petfinder-pawpularity-score\/test\/'+ x +'.jpg')","93b27f46":"# scaler = StandardScaler()\n\n# scaled_X = scaler.fit_transform(X)\n# scaled_X","6b47a14f":"df['Pawpularity'].hist(figsize = (10, 5))\nprint(f\"The mean Pawpularity score is {df['Pawpularity'].mean()}\")\nprint(f\"The median Pawpularity score is {df['Pawpularity'].median()}\")\nprint(f\"The standard deviation of the Pawpularity score is {df['Pawpularity'].std()}\")","da8f520a":"print(f\"There are {len(df['Pawpularity'].unique())} unique values of Pawpularity score\")","3f76eb10":"df['norm_score'] = df['Pawpularity']\/100\ndf['norm_score']","b31b1f18":"df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\ndf.head()","548a4469":"import math\n#Rice rule\nnum_bins = int(np.ceil(2*((len(df))**(1.\/3))))\nnum_bins","e8a8b427":"df['bins'] = pd.cut(df['norm_score'], bins=num_bins, labels=False)\ndf['bins'].hist()","d5bf878e":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\n\ndf['fold'] = -1\n\n\nN_FOLDS = 10\nstrat_kfold = StratifiedKFold(n_splits=N_FOLDS, random_state=365, shuffle=True)\nfor i, (_, train_index) in enumerate(strat_kfold.split(df.index, df['bins'])):\n    df.iloc[train_index, -1] = i\n    \ndf['fold'] = df['fold'].astype('int')\n\ndf.fold.value_counts().plot.bar()","5c482560":"df['bins'].value_counts()","f609d37a":"df","654b2a98":"X = df.drop(['Id', \"Pawpularity\", \"fold\", \"norm_score\", \"bins\"], axis=1)\ny = df['Pawpularity']\n\nX.shape","60303ceb":"IMG_SIZE = (224, 224)\nBATCH_SIZE = 64\nimg_path_train = '..\/input\/petfinder-pawpularity-score\/train\/'\nimg_path_test = '..\/input\/petfinder-pawpularity-score\/test'","620e2027":"def img_read(is_labelled):\n    def read_img(image_path):\n        img = tf.io.read_file(image_path)\n        img = tf.io.decode_jpeg(img, channels=3)\n        img = tf.cast(img, tf.float32)\n        # normalizing image by calculating (img - mean) \/ adjusted_std\n        # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/image\/per_image_standardization\n#         img = tf.image.per_image_standardization(img)\n        img = tf.image.resize(img, IMG_SIZE)\n        return img\n    \n    def can_be_readed(path, label):\n        return read_img(path), label\n\n    return can_be_readed if is_labelled else read_img","790415c2":"def creat_dataset_metadata(df, batch_size, is_labelled = True):\n    # function to convert images \n    image_read = img_read(is_labelled)\n    \n    # creating dataset of image path and pawpularity score\n    if is_labelled:\n        meta_data = df.drop(['Id', \"Pawpularity\", \"fold\", \"norm_score\", \"bins\"], axis=1)\n#         scaled_meta_data = scaler.transform(meta_data)\n        \n        input_dataset = tf.data.Dataset.from_tensor_slices((df[\"Id\"].values, meta_data))\n        output_dataset = tf.data.Dataset.from_tensor_slices((df[\"Pawpularity\"].values))\n        \n        # converting images to tensors\n        input_dataset = input_dataset.map(image_read, num_parallel_calls=tf.data.AUTOTUNE)\n        \n        # creating final dataset\n        dataset = tf.data.Dataset.zip((input_dataset, output_dataset))\n        \n        # spliting in batches\n        dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n        return dataset\n        \n    else :\n        meta_data = df.drop('Id', axis=1)\n#         saled_meta_data = scaler.transform(meta_data)\n        \n        input_dataset_1 = tf.data.Dataset.from_tensor_slices((df[\"Id\"].values))\n        input_dataset_2 = tf.data.Dataset.from_tensor_slices((meta_data))\n        \n        # converting images to tensors\n        input_dataset_1 = input_dataset_1.map(image_read, num_parallel_calls=tf.data.AUTOTUNE)\n        \n#         dataset = tf.data.Dataset.from_tensor_slices((input_dataset_1, input_dataset_2))\n        dataset = tf.data.Dataset.zip(((input_dataset_1, input_dataset_2),)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n        \n        return dataset","540f3402":"def creat_dataset_image(df, batch_size, is_labelled = True):\n    # function to convert images \n    image_read = img_read(is_labelled)\n    \n    # creating dataset of image path and pawpularity score\n    if is_labelled:\n        input_dataset = tf.data.Dataset.from_tensor_slices((df[\"Id\"].values, df[\"Pawpularity\"].values))\n        \n        # converting images to tensors\n        dataset = input_dataset.map(image_read, num_parallel_calls=tf.data.AUTOTUNE)\n        \n        # spliting in batches\n        dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n        return dataset\n        \n    else :\n        input_dataset_1 = tf.data.Dataset.from_tensor_slices((df[\"Id\"].values))\n        \n        # converting images to tensors\n        input_dataset_1 = input_dataset_1.map(image_read, num_parallel_calls=tf.data.AUTOTUNE)\n        dataset = input_dataset_1.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n        \n        return dataset","0dca5950":"test_dataset_metadata = creat_dataset_metadata(test_df, BATCH_SIZE, False)","549879f7":"test_dataset_metadata","ee70383a":"test_dataset_image = creat_dataset_image(test_df, BATCH_SIZE, False)","dd157efb":"test_dataset_image","9667628a":"# data augmentation stage with horizontal flipping, rotation, zooms, etc....\nimage_augmentation = tf.keras.Sequential([\n    preprocessing.RandomFlip('horizontal'),\n    preprocessing.RandomRotation(0.3),\n    preprocessing.RandomZoom(0.3),\n    preprocessing.RandomHeight(0.2),\n    preprocessing.RandomWidth(0.2),\n    preprocessing.RandomContrast(0.2),\n    preprocessing.Rescaling(1.\/255)\n], name='data_augmentation')","4779d0e6":"from tensorflow.keras import layers\n\nxception_weights = '..\/input\/keras-pretrained-models\/xception_weights_tf_dim_ordering_tf_kernels_notop.h5'\ninception_v3 = '..\/input\/keras-pretrained-models\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\nefficientnet_b2 = '..\/input\/d\/aeryss\/keras-pretrained-models\/EfficientNetB2_NoTop_ImageNet.h5'\nbase_model = tf.keras.applications.InceptionV3(include_top=False, weights=inception_v3)\nbase_model.trainable = True","f9932e4e":"for layer in base_model.layers[:-28]:\n    layer.trainable = False","c57d6a28":"def create_model_1():\n    # image input model\n    img_input = tf.keras.layers.Input(shape=(229, 229, 3), name='image_input')\n    x = image_augmentation(img_input)\n    x = base_model(x)\n    x = GlobalMaxPooling2D()(x)\n    x = layers.Dense(526, activation='relu')(x)\n    output_layer = layers.Dense(1, activation='linear')(x)\n\n    model = tf.keras.Model(inputs = img_input, outputs=output_layer)\n    \n    return model","b8e1d524":"def create_model_2():\n    # image input model\n    img_input = tf.keras.layers.Input(shape=(224, 224, 3), name='image_input')\n    x = image_augmentation(img_input)\n    x = base_model(x)\n    x = GlobalMaxPooling2D()(x)\n    img_output = layers.Dense(526, activation='relu')(x)\n    img_model = tf.keras.Model(img_input, img_output)\n \n    # other data Model\n    data_input = layers.Input(shape=X.shape[1], name='data_input')\n#     x = layers.Dense(64, activation='relu')(data_input)\n#     x = layers.Dropout(0.5)(x)\n#     x = layers.BatchNormalization()(x)\n    data_output = layers.Dense(32, activation='relu')(data_input)\n    data_model = tf.keras.Model(data_input, data_output)\n    \n    # concatinating Model layers\n    concat_layer = layers.Concatenate(name = 'concat_layer')([img_model.output, data_model.output])\n\n    combined_dropout = layers.Dropout(0.5)(concat_layer)\n    combined_dence = layers.Dense(128, activation='relu')(combined_dropout)\n    final_dropout = layers.Dropout(0.5)(combined_dence)\n\n    output_layer = layers.Dense(1, activation='relu')(final_dropout)\n\n    model = tf.keras.Model(inputs = [img_model.input, data_model.input], outputs=output_layer)\n    \n    return model","fcf83dc5":"test_model = create_model_2()\ntest_model.summary()","8276d11e":"from tensorflow.keras.utils import plot_model\nplot_model(test_model, show_shapes=True)","5090d0f3":"train_callbacks = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\", patience=5,\n        restore_best_weights=True\n    ),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor=\"val_loss\", factor=0.5,\n        patience=2, verbose=1\n    ),\n]","f124aeef":"# list to store outputs of each fold\nfinal_results = []","4c150ade":"for fold in range(10):\n    print(f'Fold {fold+1}')\n    train_df =  df[df.fold != fold].reset_index(drop=True)\n    val_df = df[df.fold == fold].reset_index(drop=True)\n    \n    # ccreating train and validation dataset\n    train_dataset = creat_dataset_metadata(train_df, BATCH_SIZE, is_labelled=True)\n    val_dataset = creat_dataset_metadata(val_df, BATCH_SIZE, is_labelled=True)\n    \n    # creating model\n    model = create_model_2()\n    \n    # compiling model\n    model.compile(\n        loss = 'mse',\n        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n        metrics = [tf.keras.metrics.RootMeanSquaredError()],\n    )\n    \n    # fitting Model\n    print('model training \\n')\n    model.fit(\n        train_dataset,\n        epochs = 25,\n        steps_per_epoch = len(train_dataset),\n        validation_data = val_dataset,\n        validation_steps = len(val_dataset),\n        callbacks = train_callbacks,\n    )\n    \n    # making predictions and storing them in list\n    submit = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')\n    pred = model.predict(test_dataset_metadata)\n    pred = np.squeeze(pred, axis=1)\n\n    final = pd.DataFrame()\n    final['Id'] = submit['Id']\n    final['Pawpularity'] = pred\n    \n    final_results.append(final)\n    print(final[:6])","2d026b97":"np.array(final_results[:]).shape[0]","9b488e5f":"n = np.array(final_results[:]).shape[0]\npred_final = np.dot(np.array([1]*n), np.array(final_results[:])[:,:,1] )\npred_final \/= n\npred_final","9cae6342":"test_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\n\nsubmission = pd.DataFrame()\nsubmission['Id']= test_df['Id']\nsubmission['Pawpularity']=  (pred_final) + 7\nsubmission.to_csv('submission.csv', index=False)\nsubmission","df8e5639":"## Data set form images only model","666b4f1e":"## Datasets for model.2","b2448f4b":"# Calculating final Predictions","34afaf60":"# Training callbacks","06edad18":"# Augmentation layer and Model\n\n* **For base model we are using inception v3 with pretrained weights and unfreezing last 28 layers**\n* **create_model_1 --> creates model for only image inputs**\n* **create_model_2 --> reates model for image and metadata**","a6aec54f":"## Finding optimial bin formula","21cfa287":"# Creating Input Datasets\n\n* **Funtion creat_dataset_metadata --> creates dataset of images and metadata**\n* **Function creat_dataset_image --> creates dataset with only images**\n* **Function img_read --> reads image from image folder and resize it to (229, 229)**","02e9deb6":"## Changing Test Data","f75f8cdb":"# EDA","db732a7e":"# Importing and spliting  data\n\n* **20% data split**","4aa9be09":"# Model Training"}}