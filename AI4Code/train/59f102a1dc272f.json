{"cell_type":{"1c0535fd":"code","8714d25e":"code","7b17180f":"code","ea61369b":"code","cf5f9723":"code","ed23182f":"code","5879bfbd":"code","566f27ba":"code","abf08c85":"code","e5f38eda":"code","deaeefe2":"code","1246cdb4":"code","4680d485":"code","9c107a75":"code","b92ba594":"code","add06b67":"code","cfaf6025":"code","86b990e7":"code","63151270":"code","75ec3ea6":"code","60460e60":"code","85fffc8b":"code","97fb69d0":"code","4cbe2d31":"code","85b4f252":"code","1b639b32":"code","b2345276":"code","4b74f70f":"code","b3f3918f":"code","bfb3cf74":"code","fe1959e3":"markdown","7fcae896":"markdown","3b23349f":"markdown","7d2ef5f3":"markdown","b3d1fbf2":"markdown","cb2df2ae":"markdown","13869b0a":"markdown","75d8731a":"markdown","262682dd":"markdown","b7d803d4":"markdown","78066456":"markdown","7315a130":"markdown","27020d4f":"markdown","2a1ad4a5":"markdown","ac989a43":"markdown","5c671ada":"markdown","a5bdd6d9":"markdown","146ca88d":"markdown","c86f3684":"markdown","44be9144":"markdown","1d6181f3":"markdown","7c1e07c1":"markdown","94e37227":"markdown","92059e0b":"markdown","3726ebf7":"markdown","a4500cc2":"markdown","1876ad87":"markdown","9af963b3":"markdown","06b0a0e7":"markdown","5ef6ae37":"markdown","2d83f1da":"markdown","96f15571":"markdown","9eb8ff82":"markdown","214453d0":"markdown","dad5a41a":"markdown","c2f05557":"markdown","d6ed7528":"markdown","a6453576":"markdown","09325177":"markdown","a8cca7ff":"markdown"},"source":{"1c0535fd":"import pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelBinarizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import utils\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom xgboost import XGBClassifier\n\nimport shap","8714d25e":"# Load the data in format of Pandas DataFrame\ndf = pd.read_csv(\"\/kaggle\/input\/atpdata\/ATP.csv\")\ndf.shape","7b17180f":"df.info()","ea61369b":"df.head()","cf5f9723":"df.columns","ed23182f":"dimensions = ['winner_rank','loser_rank','winner_age','loser_age','winner_ht','loser_ht']\n\nplt.figure(1, figsize=(20,12))\n\nfor i in range(1,7):\n    plt.subplot(2,3,i)\n    df[dimensions[i-1]].plot(kind='hist', title=dimensions[i-1])","5879bfbd":"plt.figure(figsize=(9,5))\ndf['winner_name'].value_counts()[:10].plot(kind='barh')\nplt.title('10 Top players')","566f27ba":"correlation_matrix = df.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(correlation_matrix, annot= True, linewidth=0.1, cmap= 'Oranges')","abf08c85":"df = df.drop(columns=['score','tourney_name','winner_name',\n                      'loser_name','minutes', 'l_1stIn', 'l_1stWon', 'l_2ndWon','l_ace', 'l_svpt',\n                      'l_SvGms','l_bpFaced','l_df', 'l_bpSaved',  'w_1stIn', 'w_1stWon','w_2ndWon', 'w_SvGms',\n                      'w_ace', 'w_svpt','w_bpFaced', 'w_bpSaved','w_df'])\nprint(\"remained features \", df.columns)","e5f38eda":"none_percent = df.isnull().sum() * 100 \/ len(df)\nnone_values_df = pd.DataFrame({'Feature name': df.columns, 'None values count': df.isnull().sum(),\n                                 'None values percent': none_percent})\nprint(none_values_df.reset_index().drop(columns=['index']))","deaeefe2":"features_to_remove = none_values_df[none_values_df[\"None values percent\"] > 85]\nprint(features_to_remove)\ndf = df.drop(columns=features_to_remove['Feature name'].values)","1246cdb4":"df.dropna(subset=['winner_rank_points', 'loser_rank_points','winner_rank','loser_rank', 'surface'], inplace=True)\ndf = df.reset_index(drop=True)\ndf","4680d485":"numeric_columns = ['winner_rank', 'loser_rank', 'winner_age', 'loser_age', 'winner_ht', 'loser_ht']\ndf[numeric_columns] = df[numeric_columns].astype(float)","9c107a75":"df.tourney_date","b92ba594":"#\"tourney_date\" is in the format of YYYYMMDD\ndf['tourney_year'] = df.tourney_date.astype(str).str[:4].astype(int)\ndf['tourney_month'] = df.tourney_date.astype(str).str[4:6].astype(int)\n#Now drop \"tourney_date\"\ndf = df.drop(columns=['tourney_date'])","add06b67":"df = df.rename(columns={\"loser_age\": \"first_age\", \"loser_entry\": \"first_entry\", \"loser_hand\": \"first_hand\",\n                        \"loser_ht\": \"first_ht\", \"loser_id\": \"first_id\", \"loser_ioc\": \"first_ioc\",\n                        \"loser_rank\": \"first_rank\", \"loser_rank_points\": \"first_rank_points\",\n                        \"loser_seed\": \"first_seed\",\n                \n                        \"winner_age\": \"second_age\", \"winner_entry\": \"second_entry\",\"winner_hand\": \"second_hand\",\n                        \"winner_ht\": \"second_ht\", \"winner_id\": \"second_id\", \"winner_ioc\": \"second_ioc\",\n                        \"winner_rank\": \"second_rank\", \"winner_rank_points\": \"second_rank_points\",\n                        \"winner_seed\": \"second_seed\",\n                       },)","cfaf6025":"copy_2_df = df.copy()\ncopy_2_df[[ 'first_age','first_hand','first_ht','first_id','first_ioc','first_rank','first_rank_points','first_seed',\n            'second_age','second_hand','second_ht','second_id','second_ioc','second_rank','second_rank_points','second_seed']]\\\n=copy_2_df[['second_age','second_hand','second_ht','second_id','second_ioc','second_rank','second_rank_points','second_seed',\n             'first_age','first_hand','first_ht','first_id','first_ioc','first_rank','first_rank_points','first_seed']]\n","86b990e7":"winner_player2 = np.zeros(df.shape[0]) # second player wins so label=0\ndf['label'] = winner_player2\n\n\nwinner_player1 = np.ones(copy_2_df.shape[0]) # first player wins so label=1\ncopy_2_df['label'] = winner_player1 \n\ndf = pd.concat([df,copy_2_df])\n#shuffle data\ndf = df.sample(frac=1).reset_index(drop=True)\ndf","63151270":"# first_player = []\n# second_player = []\n# labels = []\n# for winner, looser in zip(df['winner_id'], df['loser_id']):\n#     number = np.random.choice([0,1],1)[0] #the number of the winner\n#     if number == 0: #the winner is player 0 and the loser is player 1 => label = 0\n#         first_player.append(winner)\n#         second_player.append(looser)\n        \n#     else: #the loser is player 0 and the winner is player 1 => label = 1\n#         second_player.append(winner)\n#         first_player.append(looser)\n    \n#     labels.append(number)\n# df['first_player_id'] = first_player\n# df['second_player_id'] = second_player\n# df['label'] = labels\n# df = df.drop(columns=['loser_id', 'winner_id'])","75ec3ea6":"hand_encoder = LabelEncoder()\ndf['first_hand'] = hand_encoder.fit_transform(df['first_hand'].astype(str))\ndf['second_hand'] = hand_encoder.transform(df['second_hand'].astype(str))\n\ndf['first_ioc'] = LabelEncoder().fit_transform(df['first_ioc'].astype(str))\ndf['second_ioc'] = LabelEncoder().fit_transform(df['second_ioc'].astype(str))\n\ndf['surface'] = LabelBinarizer().fit_transform(df['surface'].astype(str))\ndf['tourney_level'] = LabelEncoder().fit_transform(df['tourney_level'].astype(str))\ndf['tourney_id'] = LabelEncoder().fit_transform(df['tourney_id'].astype(str))\ndf['round'] = LabelEncoder().fit_transform(df['round'].astype(str))\n\ndf.info()","60460e60":"df_imputed = pd.DataFrame(SimpleImputer().fit_transform(df))\ndf_imputed.columns = df.columns\ndf_imputed.index = df.index\ndf = df_imputed.copy()","85fffc8b":"print('Final shape of data after cleaning and preprocessing: ', df.shape)","97fb69d0":"y = df['label']\ndf_X = df.drop(columns='label')","4cbe2d31":"# split data : 80% for train and 20% for test.\nX_train, X_test, y_train, y_test = train_test_split(df_X, y, test_size=0.2)","85b4f252":"#Call the classifier\nRF_classifier = RandomForestClassifier(n_estimators=100)\n#fit the data\nRF_classifier.fit(X_train, y_train)\n#predict \nRF_predictions = RF_classifier.predict(X_test)","1b639b32":"print('Confusion matrix')\nprint(confusion_matrix(y_test,RF_predictions))\nprint('Classification report')\nprint(classification_report(y_test,RF_predictions))\nprint('Accuracy= ', accuracy_score(y_test, RF_predictions))","b2345276":"#Call the classifier\nXGB_classifier = XGBClassifier()\n#fit the data\nXGB_classifier.fit(X_train, y_train)\n#predict \nXGB_predictions = XGB_classifier.predict(X_test)","4b74f70f":"print('Confusion matrix')\nprint(confusion_matrix(y_test,XGB_predictions))\nprint('Classification report')\nprint(classification_report(y_test,XGB_predictions))\nprint('Accuracy= ', accuracy_score(y_test, XGB_predictions))","b3f3918f":"# load JS visualization code to notebook\nshap.initjs()\n\n# explain the model's predictions using SHAP\nexplainer = shap.TreeExplainer(XGB_classifier)\nshap_values = explainer.shap_values(X_train)\n\n# visualize the first prediction's explanation\nshap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:], matplotlib=True)","bfb3cf74":"# summarize the effects of all the features\nshap.summary_plot(shap_values, X_train)","fe1959e3":"Let's now train and execute our prediction model. For this we will use 2 differents models : RandomForest and XGBoost.","7fcae896":"For our supervised prediction model, we have to define our target feature!\n\nWe will transform our data so that we  have 2 players (first player & second player), their respective personal informations (id, hand, age, etc) and general informations about the match and the tourney.\n\nThen we will create a column \"label\" which is equal to 1 if player 1 wins, 0 if player 2 wins.\n\nto do so, we will create a first copy of our dataset where the winner is considered as first player so **label=0**. Then a second copy where we inverse the places of the players so **label=1**.\n","3b23349f":"Time needed to finish this test: about 6 hours.","7d2ef5f3":"XGBoost gives slightly better results than RandomForest.","b3d1fbf2":"Let's now count the number and the percentage of None values in our data set and treat features and entries that contain a lot of them.","cb2df2ae":"Finally, let's handle the few remaing None values using SimpleImputer.","13869b0a":"# Load the data","75d8731a":"With more time we can:\n* Make more in-depth cleaning of the data and more accurate feature engineering.\n* Fine tune the hyperparameters of the classifiers in order to improve the quality of prediction.\n*  Use Time Series to  better exploit the time features.\n*  We could suggest making predictions during the match by including information about the current situation (number of minutes spent, number of faults, ect.).","262682dd":"The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue.","b7d803d4":"# Further work","78066456":"The aim of this work is to explore data from ATP tennis competitions in order to predict the results of tennis matches.","7315a130":"First copy of the data where the loser is considered as first player and the winner as second player","27020d4f":"Let's now plot the correclation matrix and try to explore relations between features.","2a1ad4a5":"# Exploratory Data Analysis","ac989a43":"Author: Ahlem JOUIDI","5c671ada":"## Feature importance ","a5bdd6d9":"Create second copy with inverse of positions","146ca88d":"The problem with this method is that we have duplicated the dataset to be able to consider the two players in the two positions. \n\nI tried another method where the idea was **to assign randomly** the winner and loser players to the first and second players features and if :\n* winner player is assigned to player_1 and loser to player_2 then label=0\n* winner player is assigned to player_2 and loser to player_1 then label=1\n\nThis method preserve the original size of data and enable as to create the label target. However after further work and execution of the classifiers we got overfitting problem. So we will keep the first method.\n\nBelow is the source code of this method.","c86f3684":"We proceed to the removal of features that are redundant\/ crrelated with each other, and those that are obviously not available before a match to predict. \n* \"winner_name\", \"loser_name\" & \"tourney_name\" because they are almost redundant and replaceable by respectively \"winner_id\", \"loser_id\" & \"tourney_id\". \n\n* \"minutes\", \"score\", \"1stIn\", \"_1stWon\" ,\"SvGms\", \"l_bpFaced\", \"l_bpSaved\", \"svpt\" features because cannot be known in advance.","44be9144":"* We can see clearly that \"minutes\", \"score\", \"1stIn\", \"_1stWon\" ,\"SvGms\", \"l_bpFaced\", \"l_bpSaved\" and \"svpt\" features are highly correlated to each other. This is expected because they represent the result of a match.\n* Features like rank, rank_points, age and ht are important ones.","1d6181f3":"The players who won the most matches are the likely to win again.","7c1e07c1":"To avoid analytical errors, we will convert numerical values of string format to float type.","94e37227":"We can see that we have a lot of null values, so we have to handle them.\n\nWe will remove feature that have more than 85% of None values.","92059e0b":"### RandomForest classifier","3726ebf7":"# Prediction model","a4500cc2":"# Feature engineering","1876ad87":"As first step, let's clean the data and retain only important features for our prediction task.","9af963b3":"This plot allow as to get an overview of which features are most important for a model. \"First_rank\", \"second_rank\", \"first_rank_points\" and \"second_rank_points\" are the most important features. This is expected as those features express directly the level of performance of each player. \n\nThe color represents the feature value (red high, blue low). This reveals for example that a high first_rank (rank of first player) lowers the predicted labels (label tends vers 0).","06b0a0e7":"The objective of this analysis is to come up with insights regarding the tournaments using different types of visualizations as tools to facilitate interpretation of the data and variables and communication of the results.\n\nLet's start with histograms to better understand the distribution of the key variables of our dataset based on multiple dimensions.","5ef6ae37":"Since \"rank_points\" and \"rank\", which contain past performance of players, and \"surface\" features are important ones, we will delete the entries that do not contain information about them.","2d83f1da":"### XGBoost classifier","96f15571":"We will use SHAP (SHapley Additive exPlanations) library which is a game theoretic approach to explain the output of any machine learning model. It will allow as visualize the behaviour of the classifer as well as the most important features used.\n\nWe will visualize the output of XGBoost model.","9eb8ff82":"Construct label feature","214453d0":"## Import librairies","dad5a41a":"We will expand the \"tourney_date\" feature to new columns storing year and month attributes.","c2f05557":"# Prediction of the winning player in a tennis match","d6ed7528":"Random Forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes of the individual trees.\n","a6453576":"Like Random Forests,  Extreme Gradient Boosting is a set of decision trees. The two main differences are:\n\n**How trees are built:** random forests builds each tree independently while gradient boosting builds one tree at a time. This additive model (ensemble) works in a forward stage-wise manner, introducing a weak learner to improve the shortcomings of existing weak learners. \n**Combining results:** random forests combine results at the end of the process (by averaging or \"majority rules\") while gradient boosting combines results along the way.","09325177":"# Preporcessing ","a8cca7ff":"For our prediction model, we have to encode categorical features. We will use LaberEncoder of Sklearn library."}}