{"cell_type":{"3a7261b8":"code","cf071e96":"code","4066add5":"code","38f0585a":"code","94386691":"code","f829264d":"code","35907a35":"code","48d472f7":"code","61f2f77c":"code","e1fa10c8":"code","df0c0907":"code","2bfea117":"code","388ed490":"code","5797e908":"code","245c254c":"code","614dc48c":"code","7a38b705":"code","f7a44e1e":"code","9e334597":"code","3599d3a0":"code","557a6826":"code","db79ff22":"code","9e40a518":"code","60d0e2f2":"code","f59820b9":"code","d102d4c4":"code","90e0276d":"code","6b6e43a7":"code","bd6756ba":"code","09b8ad3e":"code","9069f6f2":"code","a1432e1b":"code","725ae1d6":"code","f8459411":"code","8f586055":"code","e290935c":"code","11beb8ca":"code","25e56838":"code","904e279e":"code","f6eeeff5":"code","c42459a6":"code","a0c6c37e":"code","bb03af5f":"code","7092d5d3":"code","f2963362":"code","c92a913c":"code","a272bc2a":"code","0dbe243e":"code","8d0caf51":"code","36833fcb":"code","63dd53a0":"code","16f21356":"code","c7714b91":"code","5ea6665c":"code","94dbd79d":"code","7f456e55":"code","fcbddfe8":"code","4e376ed8":"code","f721035b":"code","7093c25e":"code","b5c9d58e":"code","457cc783":"code","b13717b3":"code","0c1f17a7":"code","d39852aa":"code","ecd2d868":"code","21a6c05b":"code","986c2c97":"code","6aba7dea":"code","b2ba962a":"code","fe400cd0":"code","43106afc":"code","d0e1294a":"code","c2e3d903":"code","eb94fcd8":"code","d133b925":"code","a9db581f":"code","eb60daff":"code","7051d13c":"code","18bc59a1":"code","edcecc8f":"code","dcbd448b":"code","7e8cce20":"code","9c8a5f1f":"code","5c315ffc":"code","56ca227f":"code","e9b7b925":"code","660f718d":"code","dc266b0d":"code","40220599":"code","d571e46a":"code","e5bc160e":"code","4dfe341c":"code","af1308f7":"code","01ec8bb1":"code","409217e7":"code","6c61a18a":"code","e064d772":"code","659dd55a":"code","aa7a8f8e":"code","a49ec7ff":"code","996057df":"code","bf5d3e28":"code","1d672662":"code","b6702351":"code","3b7ec29b":"code","9611386e":"code","bb8c349f":"code","1524fc93":"code","cdeba622":"code","530ac407":"code","33b6f3b0":"code","51dc5969":"code","1c88e92e":"code","1171be7b":"code","d6909519":"code","0c1ae73f":"code","d64aa43a":"markdown","bedea654":"markdown","3016b67b":"markdown","9da8335f":"markdown","6b5eb67b":"markdown","09e4a2a7":"markdown","4e8699bd":"markdown","fb05b86a":"markdown","f937a54f":"markdown","a22a1ec8":"markdown","a75246d8":"markdown","da4bdad9":"markdown","ea46552f":"markdown","61deaa0d":"markdown","3af5913b":"markdown","6cbd1312":"markdown","43c42c59":"markdown","ef8a7af9":"markdown","aa91e931":"markdown","9bd5cb25":"markdown","d93b7577":"markdown","7c5badd8":"markdown","87da1e8f":"markdown","b6ad7291":"markdown","648072a1":"markdown","8595f60d":"markdown","1a600e72":"markdown","a355087f":"markdown","8e9d6412":"markdown","d7dc0e34":"markdown","fc72bd25":"markdown","49044bb8":"markdown","4961f18d":"markdown","63851e94":"markdown","843d6931":"markdown","619374d6":"markdown","ecd79b0c":"markdown","dcedd2d9":"markdown","277e88aa":"markdown","46236137":"markdown","c12fe7ee":"markdown","b4eb5bda":"markdown","a18ca3cc":"markdown","a8f6b198":"markdown","44521972":"markdown","ae974001":"markdown","5463e234":"markdown","73033149":"markdown","1ec1e32a":"markdown","11d09f9a":"markdown","2d2ccf1a":"markdown","d58e47b1":"markdown","18b8a308":"markdown","9125521f":"markdown","fe9618b1":"markdown","6355b88d":"markdown","b7e52727":"markdown","0a95057a":"markdown","4f21e672":"markdown","73ef3f6c":"markdown","0c4499b8":"markdown","8fd2bb15":"markdown","554e3dc7":"markdown","fb179c8b":"markdown","c0efa55e":"markdown","ffe146a9":"markdown","744af8fe":"markdown","656c30a0":"markdown","03ca0776":"markdown"},"source":{"3a7261b8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_column\",100)\npd.set_option(\"display.max_rows\",100)","cf071e96":"#Reading the file\ndata = pd.read_csv(r'..\/input\/xyzcorp-lendingdataprediction\/XYZCorp_LendingData.txt',\n                      index_col=0, delimiter='\\t',parse_dates=['issue_d'])","4066add5":"''''\n#generating the pandas profiling report for better understanding the dataset what we got and making sense toward better approach..\nimport pandas_profiling as pf\nimport pandas as pd \ndf=pf.ProfileReport(data)\n\n''''","38f0585a":"''''\n#Exporting the generated pandas-profinling report into html extention.....\ndf.to_file(r\"C:\\Users\\bank Lending\\lending_data.html\")\n''''","94386691":"train_df=data[data['issue_d']<'2015-6-01']\ntest_df=data[data['issue_d']>='2015-6-01']","f829264d":"print(train_df.shape,test_df.shape)","35907a35":"train_df.isnull().sum().sort_values(ascending=False)","48d472f7":"print(train_df.info())","61f2f77c":"print(train_df.describe())","e1fa10c8":"train_df=train_df.drop(['inq_last_12m','total_cu_tl','inq_fi','all_util','max_bal_bc','open_rv_24m','open_rv_12m',\n                          'il_util','total_bal_il','mths_since_rcnt_il','open_il_24m','open_il_12m','open_il_6m',\n                          'open_acc_6m','verification_status_joint','next_pymnt_d','mths_since_last_record',\n                          'mths_since_last_major_derog','mths_since_last_delinq',\n                          'dti_joint','desc','annual_inc_joint'],axis=1)","df0c0907":"train_df.isnull().sum()","2bfea117":"def missing_data(train_df):\n    total=train_df.isnull().sum().sort_values(ascending=False)\n    percent=(train_df.isnull().sum()\/train_df.isnull().count()).sort_values(ascending=False)\n    missing_data=pd.concat([total,percent],axis=1,keys=['total','percent'])\n    return(missing_data.head(20))","388ed490":"missing_data(train_df)","5797e908":"print(train_df.columns)","245c254c":"train_df['pymnt_plan'].value_counts()","614dc48c":"train_df['pymnt_plan'].value_counts().plot.bar()","7a38b705":"\ndel train_df['pymnt_plan']","f7a44e1e":"''''\n#generating again profiling report to know the current dataframe description after treating the dataframe with cut off \n#unnecessary things....\nimport pandas_profiling as pf\nimport pandas as pd\ndf=pf.ProfileReport(train_df)\n''''","9e334597":"''''\n#Exporting it in to html format...\ndf.to_file(r'C:\\Users\\AKASH\\OneDrive\\Desktop\\PYTHON IMARTICUS\\Python Project - Bank Lending\\lending_train_data.html')\n''''","3599d3a0":"for x in train_df.columns[:]:\n    if train_df[x].dtype=='object':\n        train_df[x].fillna(train_df[x].mode()[0],inplace=True)\n    elif train_df[x].dtype=='int64' or train_df[x].dtype=='float64':\n        train_df[x].fillna(train_df[x].mean(),inplace=True)","557a6826":"print(train_df.isnull().sum()) ","db79ff22":"print(train_df['last_pymnt_d'].value_counts())","9e40a518":"train_df=train_df.drop(['last_pymnt_d'],axis=1)","60d0e2f2":"print(train_df.dtypes)","f59820b9":"colname=[]\nfor x in train_df.columns:\n    if train_df[x].dtype=='float64':\n        colname.append(x)\n        ","d102d4c4":"print(colname)","90e0276d":"#Here we go with transform the variable into numerical form...\nfrom sklearn import preprocessing\n\nle=preprocessing.LabelEncoder()\n\nfor x in colname:\n    train_df[x]=le.fit_transform(train_df[x])","6b6e43a7":"print(train_df.dtypes)    ","bd6756ba":"print(train_df['issue_d'].value_counts())","09b8ad3e":"del train_df['issue_d']","9069f6f2":"print(train_df['zip_code'].value_counts())","a1432e1b":"del train_df['zip_code']","725ae1d6":"print(train_df['initial_list_status'].value_counts())","f8459411":"sns.countplot('initial_list_status',data=train_df,hue='default_ind')","8f586055":"train_df['initial_list_status'] = np.where(train_df['initial_list_status']=='f', 0, train_df['initial_list_status'])\ntrain_df['initial_list_status'] = np.where(train_df['initial_list_status']=='w', 1, train_df['initial_list_status'])","e290935c":"print(train_df['initial_list_status'].value_counts())","11beb8ca":"train_df['int_rate'].value_counts()","25e56838":"plt.figure(figsize=(10,5))\nsns.distplot(train_df['int_rate'])\nplt.show()","904e279e":" train_df['default_ind'].value_counts().plot.bar()","f6eeeff5":"print(train_df['title'].value_counts())","c42459a6":" del train_df['title']","a0c6c37e":"colname=[]\nfor x in train_df.columns:\n    if train_df[x].dtype=='object':\n        colname.append(x)","bb03af5f":"print(colname)","7092d5d3":"from sklearn import preprocessing\n\nle=preprocessing.LabelEncoder()\n\nfor x in colname:\n    train_df[x]=le.fit_transform(train_df[x])  ","f2963362":"print(train_df.dtypes)","c92a913c":"print(train_df['emp_length'].value_counts())","a272bc2a":"train_df=train_df.drop(['member_id', 'emp_title', 'addr_state', 'earliest_cr_line', 'out_prncp_inv', 'total_rec_late_fee',\n                       'last_credit_pull_d', 'policy_code'],axis=1)","0dbe243e":"train_df=train_df.drop(['application_type'],axis=1)","8d0caf51":"# Plotting histogram of all variables and check out the frequency distribution for remaining variables in the dataset\ntrain_df.hist(figsize=(15,20))","36833fcb":"lis=train_df[train_df.columns].corr()['default_ind'][:]\nprint(train_df[train_df.columns].corr()['default_ind'][:])","63dd53a0":"corr=train_df.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","16f21356":"plt.figure(figsize=(20,20))\nsns.heatmap(train_df.corr(),annot=False,cmap='magma')","c7714b91":"fig, ax = plt.subplots(1, 3, figsize=(16,5))\nsns.distplot(train_df['loan_amnt'],ax=ax[0])\nsns.distplot(train_df['funded_amnt'], ax=ax[1])\nsns.distplot(train_df['funded_amnt_inv'], ax=ax[2])\n\nax[1].set_title(\"Amount Funded by the Lender\")\nax[0].set_title(\"Loan Applied by the Borrower\")\nax[2].set_title(\"Total committed by Investors\")","5ea6665c":"train_df=train_df.drop(['funded_amnt_inv','funded_amnt','delinq_2yrs','collections_12_mths_ex_med','acc_now_delinq',\n                        'tot_coll_amt'],axis=1)","94dbd79d":"X=train_df.iloc[:,0:-1].values\nY=train_df.iloc[:,-1].values","7f456e55":"print(X.shape)\nprint(Y.shape)","fcbddfe8":"count_classes = pd.value_counts(train_df['default_ind'], sort = True)\n\ncount_classes.plot(kind = 'bar', rot=0)\n\nplt.title(\"Transaction Class Distribution\")\n\nplt.xticks(range(2))\n\n# Interpretation - It is observed that the y varibale is imbalanced ","4e376ed8":"approved = train_df[train_df['default_ind']==1]\n\nreject = train_df[train_df['default_ind']==0]","f721035b":"print(approved.shape,reject.shape)","7093c25e":"''''\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import NearMiss\n# Implementing Oversampling for Handling Imbalanced \nsmk = SMOTETomek(random_state=10)\nX_res,y_res=smk.fit_sample(X,Y)\n''''","b5c9d58e":"''''\nprint(X_res.shape,y_res.shape)\n''''","457cc783":"''''\nfrom collections import Counter\nprint('Original dataset shape {}'.format(Counter(Y)))\nprint('Resampled dataset shape {}'.format(Counter(y_res)))\n''''","b13717b3":"from imblearn.combine import SMOTETomek\nfrom imblearn.over_sampling import RandomOverSampler\nos =  RandomOverSampler()\nX_o_res, Y_o_res = os.fit_sample(X, Y)","0c1f17a7":"print(X_o_res.shape,Y_o_res.shape)","d39852aa":"from collections import Counter\nprint('Original dataset shape {}'.format(Counter(Y)))\nprint('Resampled dataset shape {}'.format(Counter(Y_o_res)))","ecd2d868":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaler.fit(X_o_res)\n","21a6c05b":"X_o_res=scaler.transform(X_o_res)\nprint(X_o_res)\n","986c2c97":"\nfrom sklearn.model_selection import train_test_split\nX_train,X_val,Y_train,Y_val=train_test_split(X_o_res,Y_o_res,test_size=0.3, random_state=10)\n","6aba7dea":"from sklearn.linear_model import LogisticRegression\nclassfier=LogisticRegression(solver='liblinear', C=10.0, random_state=0,dual=False)\nclassfier.fit(X_train,Y_train)\n\nY_pred=classfier.predict(X_val)\n\nprint(list(zip(Y_val,Y_pred)))","b2ba962a":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncfm=confusion_matrix(Y_val,Y_pred)\nprint(cfm)\nprint(\"classification report: \")\nprint(classification_report(Y_val,Y_pred))\n\nacc=accuracy_score(Y_val,Y_pred)\nprint(\"Accuracy of the model: \",acc)","fe400cd0":"Y_pred_prob=classfier.predict_proba(X_val)\nprint(Y_pred_prob)","43106afc":"Y_pred_class=[]\nfor value in Y_pred_prob[:,1]:\n    if value>0.55:\n        Y_pred_class.append(1)\n    else:\n        Y_pred_class.append(0)\nprint(Y_pred_class)","d0e1294a":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncfm=confusion_matrix(Y_val,Y_pred_class)\nprint(cfm)\nprint(\"classification report: \")\nprint(classification_report(Y_val,Y_pred_class))\n\nacc=accuracy_score(Y_val,Y_pred_class)\nprint(\"Accuracy of the model: \",acc)","c2e3d903":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncfm=confusion_matrix(Y_val,Y_pred_class)\nprint(cfm)\nacc=print(accuracy_score(\"Accuracy of the model\",acc))\nprint(classification_report(Y_val,Y_pred_class))","eb94fcd8":"from sklearn import metrics\n\nfpr,tpr,z=metrics.roc_curve(Y_val,Y_pred_class)\nauc=metrics.auc(fpr,tpr)\nprint(auc)","d133b925":"import matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nplt.show()","a9db581f":"#Using cross validation\n\nclassifier=LogisticRegression()\n\n#performing kfold_cross_validation\nfrom sklearn.model_selection import KFold\nkfold_cv=KFold(n_splits=10)\nprint(kfold_cv)\n\nfrom sklearn.model_selection import cross_val_score\n#running the model using scoring metric as accuracy\nkfold_cv_result=cross_val_score(estimator=classifier,X=X_train,\n                                y=Y_train, cv=kfold_cv)\nprint(kfold_cv_result)\n#finding the mean\nprint(kfold_cv_result.mean())","eb60daff":"for train_value, test_value in kfold_cv.split(X_train):\n    classifier.fit(X_train[train_value], Y_train[train_value]).predict(X_train[test_value])\n\n\nY_pred_cv=classifier.predict(X_val)\nprint(list(zip(Y_val,Y_pred_cv)))","7051d13c":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n\ncfm=confusion_matrix(Y_val,Y_pred_cv)\nprint(cfm)\nprint(\"classification report: \")\nprint(classification_report(Y_val,Y_pred_cv))\n\nacc=accuracy_score(Y_val,Y_pred_cv)\nprint(\"Accuracy of the model: \",acc)","18bc59a1":"from sklearn import metrics\n\nfpr,tpr,z=metrics.roc_curve(Y_val,Y_pred_cv)\nauc=metrics.auc(fpr,tpr)\nprint(auc)","edcecc8f":"import matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nplt.show()","dcbd448b":"#prediction using decision tree classfier\nfrom sklearn.tree import DecisionTreeClassifier\nmodel_DecisionTree=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n                                          max_features=None, max_leaf_nodes=None,\n                                          min_impurity_split=1e-07, min_samples_leaf=1,\n                                          min_samples_split=2, min_weight_fraction_leaf=0.0,\n                                          presort=False, random_state=None, splitter='best')\n#fit the model on the data and predict the values\nmodel_DecisionTree.fit(X_train,Y_train)\nY_pred=model_DecisionTree.predict(X_val)","7e8cce20":"from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\ncfm=confusion_matrix(Y_val,Y_pred)\nprint(cfm)\n\nprint(\"Classification_Report\")\nprint(classification_report(Y_val,Y_pred))\n\nacc=accuracy_score(Y_val,Y_pred)\nprint(\"Accuracy of the model : \",acc)","9c8a5f1f":"#predicting using the Random_Forest_Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_RandomForest=RandomForestClassifier(n_estimators=30, random_state=10)\n\n#fit the model on the data and predict the values\nmodel_RandomForest.fit(X_train,Y_train)\n\nY_pred=model_RandomForest.predict(X_val)","5c315ffc":"from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\ncfm=confusion_matrix(Y_val,Y_pred)\nprint(cfm)\n\nprint(\"Classification_Report\")\nprint(classification_report(Y_val,Y_pred))\n\nacc=accuracy_score(Y_val,Y_pred)\nprint(\"Accuracy of the model : \",acc)","56ca227f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('log', model1))\nmodel2 = DecisionTreeClassifier(criterion='gini',random_state=10)\nestimators.append(('cart', model2))\n\n\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nensemble.fit(X_train,Y_train)\nY_pred=ensemble.predict(X_val)\n#print(Y_pred)","e9b7b925":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nprint(\"Confusion matrix\")\nprint(confusion_matrix(Y_val,Y_pred))\nprint(\"Accuracy of the model\")\nprint(accuracy_score(Y_val,Y_pred))\nprint(\"Classification report\")\nprint(classification_report(Y_val,Y_pred))","660f718d":"test_df.head()","dc266b0d":"test_df.shape","40220599":"test_df.dtypes","d571e46a":"print(test_df.info())","e5bc160e":"test_df.describe()","4dfe341c":"test_df=test_df.drop([\"addr_state\",\"annual_inc_joint\",\"application_type\",\"desc\",\"dti_joint\",\"earliest_cr_line\",\n                      \"emp_title\",\"member_id\",\"last_credit_pull_d\",\"mths_since_last_delinq\",\n                      \"mths_since_last_major_derog\",\"mths_since_last_record\",\"next_pymnt_d\",\n                      \"out_prncp_inv\",\"policy_code\",\"total_rec_late_fee\",\"verification_status_joint\",\n                      \"total_bal_il\",\"il_util\",\"open_rv_12m\",\"open_rv_24m\",\"max_bal_bc\",\"all_util\",\n                      \"inq_fi\",\"total_cu_tl\",\"inq_last_12m\",\"inq_last_12m\"],axis=1)","af1308f7":"print(test_df.isnull().sum())","01ec8bb1":"test_df=test_df.drop(['default_ind'],axis=1)","409217e7":"test_df=test_df.drop(['open_acc_6m','open_il_6m','open_il_12m','open_il_24m','mths_since_rcnt_il'],axis=1)","6c61a18a":"print(test_df.isnull().sum())","e064d772":"for x in test_df.columns[:]:\n    if test_df[x].dtype=='object':\n        test_df[x].fillna(test_df[x].mode()[0],inplace=True)\n    elif test_df[x].dtype=='int64' or test_df[x].dtype=='float64':\n        test_df[x].fillna(test_df[x].mean(),inplace=True)","659dd55a":"print(test_df.isnull().sum())","aa7a8f8e":"colname=[]\nfor x in test_df.columns:\n    if test_df[x].dtype=='object':\n        colname.append(x)\nprint(colname) ","a49ec7ff":"from sklearn import preprocessing \nle=preprocessing.LabelEncoder()\nfor x in colname:\n    test_df[x]=le.fit_transform(test_df[x])","996057df":"colname=[]\nfor x in test_df.columns:\n    if test_df[x].dtype=='float64':\n        colname.append(x)\nprint(colname)","bf5d3e28":"from sklearn import preprocessing\nle=preprocessing.LabelEncoder()\n\nfor x in colname:\n    test_df[x]=le.fit_transform(test_df[x])   ","1d672662":"print(test_df.dtypes)  ","b6702351":"test_df=test_df.drop(['issue_d','last_pymnt_d','funded_amnt_inv','funded_amnt','delinq_2yrs','collections_12_mths_ex_med',\n                      'acc_now_delinq','tot_coll_amt'],axis=1)","3b7ec29b":"test_df=test_df.drop(['title','zip_code','pymnt_plan'],axis=1)","9611386e":"print(test_df.shape)","bb8c349f":"#doing prediction for actual one means on decision tree \ntest=test_df.values\n#test=scaler.transform(test)","1524fc93":"classifier = LogisticRegression(solver='liblinear', C=30.0, random_state=0,fit_intercept=True,\n                               verbose=0)\nclassifier.fit(X_train,Y_train)","cdeba622":"test_pred=classifier.predict(test)","530ac407":"test_df[\"pred\"]=test_pred\nprint(test_df.head())","33b6f3b0":"print(test_df.shape)","51dc5969":"test_df1=test_df.drop(['total_rev_hi_lim','tot_cur_bal','last_pymnt_amnt','collection_recovery_fee','recoveries',\n                       'total_rec_int','total_rec_prncp','total_pymnt_inv','total_pymnt','out_prncp','initial_list_status',\n                       'total_acc','revol_util','revol_bal','pub_rec','open_acc','inq_last_6mths','dti','purpose',\n                       'verification_status','annual_inc','home_ownership','emp_length','sub_grade','grade','installment',\n                       'int_rate','term','loan_amnt'],axis=1)","1c88e92e":"test_df1.head","1171be7b":"test_df1[\"pred\"]= test_df1.pred.map({0:\"potential non defaulter\", 1:\"potential defaulter\" })","d6909519":"test_df1.to_excel(\"XYZcorp_prediction.xlsx\",header=\"True\")","0c1ae73f":"print(test_df1['pred'].value_counts())","d64aa43a":"### Missing values treatment","bedea654":"#### Now take a view of remaining columns in the dataset using below function..","3016b67b":"#### This variable is used for splitting the data and is not significant for model building","9da8335f":"#### checking the unique record for this particular column","6b5eb67b":"#### We'll first check unique record for this particular column","09e4a2a7":"#### will check the unique record for this particular column.","4e8699bd":"#### Plotting histogram of all variables and check out the frequency distribution for remaining variables in the dataset","fb05b86a":"#### Dropping the variables on domain knowledge","f937a54f":"#### Now will check the shape of stored 0's and 1's in diffrent-diffrent variable","a22a1ec8":"#### Dropping the variables in sense of domain knowledge and generating pandas_profiling report.","a75246d8":"### Imputing the missing values for all the variables...","da4bdad9":"#### Checking out it's converted or not","ea46552f":"#### Transforming..","61deaa0d":"#Printing the concise summary about 'train_df' data frame, and will show the information such as- dtypes,non-null value, memory usage.","3af5913b":"#### Ploting the count plot for the dependent variable and identifying the imbalanced data and perform oversampling technique.","6cbd1312":"#### Dropped as the same is in the dateformat","43c42c59":"#### Checking missing values","ef8a7af9":"#### this variable is our target varaible ploting and checking out the record and got the result as imbalanced data, that's gonna be make mistake when we'll try to fit the model and mistake will be read the same values again and again in ration of 9:1, and falsly predict the default values as well...","aa91e931":" ### BANK LENDING DATA PREDICTION NOTEBOOK!","9bd5cb25":"# K-FOLD CROSS VALIDATION","d93b7577":"#Printing the floating point variable","7c5badd8":"#### Plotting the heatmap for checking out the correlation in betwwen the variables...","87da1e8f":"#checking out the continuous variable ploting density plots thats used to observe the distribution of a variable in a dataset.\n#It plots the graph on a continuous interval or time-period, An advantage of Density Plots over Histograms is that they're better at determining the distribution shape because they're not affected by the number of bins.","b6ad7291":"*** we'll split data split as per given problem statement condition..","648072a1":"# Decision Tree","8595f60d":"#### Ploting the graph for checking out the normal distribution of the variable, and this variable got perfect skewness means it's not right skewed or left skewed so keeping it....","1a600e72":"#### checking the correlation for identify of enstablished relationship in between variables ","a355087f":"#### Dropping as per above explanation","8e9d6412":"#### performing standardScaler for removes the mean and scales each feature\/variable to unit variance. This operation is performed feature-wise in an independent way. StandardScaler can be influenced by outliers(if they exist in the dataset) since it involves the estimation of the empirical mean and standard deviation of each feature","d7dc0e34":"#### Checking out the missing values for every variable that present in the dataset.","fc72bd25":"#We'll check out the unique record for this particular variable..","49044bb8":"#### Zip code is removed as the same is varchar variable hence not significant for model building","4961f18d":"#### We are printing the original dataset values and after oversampling values","63851e94":"### Data Pre-processing","843d6931":"#Created the manual function for find out the missing values and sorted out as most occured missing data on top and categoriesed the counting form and percentage....","619374d6":"### Resampling of the data","ecd79b0c":"#### Appending all variables with dtype object","dcedd2d9":"#We'll view some basics stastical details such as percentile, standard deviation,mean etc.","277e88aa":"### Scaling the data","46236137":"#### implemeting the accuracy for one.","c12fe7ee":"### Pandas Profiling","b4eb5bda":"### PREDICTION ON TEST DATA","a18ca3cc":"#### Count of y variable is insignificant hence dropping the same ","a8f6b198":"### Reading the file","44521972":"#### After ploting the heatmap found variable with very low correlation and had some variable with same distribution\n#### we'll treat the model as unnecessary sampled performing again and again, found some highly skewed variable too.","ae974001":"#### adjusting the threshold, default point is 0.5 always for camparison with matrix values store the predicted probabilities.","5463e234":"### Spliting X and Y into Train & Test","73033149":"#### We'll first split the classified binary numbers in to 'approved', and 'reject' two seperated variable","1ec1e32a":"#### Plotting the graph for check the most occurance of record in graphical way...","11d09f9a":"#### doing the lable encoding manually for this particular column","2d2ccf1a":"#### We'll checking out the unique record of particular variable.","d58e47b1":"#### auc mostly checks for getting model proficiancy is good or bad or very good","18b8a308":"# Logistic Regression","9125521f":"## AUC","fe9618b1":"### Dropping variables","6355b88d":"# Random Forest","b7e52727":"#### Spiting the dataset into X and Y","0a95057a":"#### Using lable encoding function we'll fit the 'object' data type variables and transform the data in to numerical form...","4f21e672":"#Printing the shape of both splitted dataset to get counts of variables and observations...","73ef3f6c":"#### Dropped as per above","0c4499b8":"### Converting all variables to numerical","8fd2bb15":"#### Performing RandomOverSampler technique to handle imbalanced data, imblearn package that help to identfy the imbalanced data and performed oversampling technique for the model","554e3dc7":"### importing the libraries ","fb179c8b":"#### Plotting the graph for the graphical representation...","c0efa55e":"### Splitting data into Independent & Dependent Dataframe","ffe146a9":"# Model Building ","744af8fe":"### Data Splitting","656c30a0":"## ROC","03ca0776":"#Conversion of float point variables in to numerical form, first we'll pull out all floating point variable and see.."}}