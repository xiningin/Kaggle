{"cell_type":{"80352e98":"code","783563fb":"code","0a6121fb":"code","eb8a7431":"code","189d901a":"code","6a09e7d4":"code","ca1bcb20":"code","86a7b7d6":"code","d1540964":"code","a6dd2e8d":"code","8e318e90":"code","903a3f3c":"code","0345ab1f":"code","67f869cd":"code","6b8dbcca":"code","5454b19f":"code","9e1d6f98":"code","945d9e5b":"code","4185c233":"code","e44a9004":"code","20e82881":"code","e7683a39":"code","e1b1e68b":"code","17d3953e":"code","f00193ce":"code","7d42858a":"code","e6110378":"code","3f2bfe66":"code","938b7582":"code","0e915e2f":"code","139176a0":"code","bb1ce6f3":"code","adc5fe2b":"code","12bc7b48":"code","05816390":"code","3783946a":"code","f6a15720":"code","14b1db7a":"markdown","556b1d16":"markdown","459f8bf7":"markdown","3a4c9b99":"markdown","705c391e":"markdown","987f6ae2":"markdown","cc8eb862":"markdown","954c81da":"markdown","c13b720a":"markdown","d32a2fdf":"markdown","899b53bb":"markdown","2201fa8d":"markdown","2c28f3a6":"markdown"},"source":{"80352e98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport math\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout\nfrom keras.models import load_model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport gc\n\nfrom sklearn import preprocessing\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.?","783563fb":"train = pd.read_csv(\"..\/input\/train_V2.csv\")\ntest = pd.read_csv(\"..\/input\/test_V2.csv\")","0a6121fb":"train = train.dropna()","eb8a7431":"train['rideDistance'] = (train['rideDistance']\/10).round(0)\ntrain['swimDistance'] = (train['swimDistance']\/10).round(0)\ntrain['walkDistance'] = (train['walkDistance']\/10).round(0)","189d901a":"test['rideDistance'] = (test['rideDistance']\/10).round(0)\ntest['swimDistance'] = (test['swimDistance']\/10).round(0)\ntest['walkDistance'] = (test['walkDistance']\/10).round(0)","6a09e7d4":"#a Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df,display=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    if display:\n        start_mem = df.memory_usage().sum() \/ 1024**2\n        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    if display:\n        end_mem = df.memory_usage().sum() \/ 1024**2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","ca1bcb20":"test[\"winPlacePerc\"] = -1","86a7b7d6":"df = pd.concat([train, test])","d1540964":"del train\ndel test","a6dd2e8d":"df[\"Id\"] = df.index ","8e318e90":"#squad_fpp = df[(df['matchType']=='squad-fpp') | (df['matchType']=='normal-squad-fpp')]\n#duo = df[(df['matchType']=='duo') | (df['matchType']=='normal-duo')]\n#solo_fpp = df[(df['matchType']=='solo-fpp') | (df['matchType']=='normal-solo-fpp')]\n#squad = df[(df['matchType']=='squad') | (df['matchType']=='normal-squad')]\n#duo_fpp = df[(df['matchType']=='duo-fpp') | (df['matchType']=='normal-duo-fpp')]\n#solo = df[(df['matchType']=='solo') | (df['matchType']=='normal-solo')]\n#flare = df[(df['matchType']=='flaretpp')|(df['matchType']=='flarefpp')]\n#crash = df[(df['matchType']=='crashtpp') | (df['matchType']=='crashfpp')]","903a3f3c":"#dp_by_type = {'squad_fpp':[squad_fpp,39525],\n#              'flare':[flare,62],\n#              'crash':[crash,149],\n#              'solo_fpp':[solo_fpp,12098],\n#              'duo_fpp':[duo_fpp,22586],\n#              'squad':[squad,14110],\n#              'solo':[solo,4067],\n#              'duo':[duo,7105]\n#              }","0345ab1f":"duo = df[(df['matchType']=='duo') | (df['matchType']=='normal-duo')|(df['matchType']=='duo-fpp') | (df['matchType']=='normal-duo-fpp')]\nsquad = df[(df['matchType']=='squad') | (df['matchType']=='normal-squad')|(df['matchType']=='squad-fpp') | (df['matchType']=='normal-squad-fpp')]\nsolo = df[(df['matchType']=='solo') | (df['matchType']=='normal-solo')|(df['matchType']=='solo-fpp') | (df['matchType']=='normal-solo-fpp')]\nflare = df[(df['matchType']=='flaretpp')|(df['matchType']=='flarefpp')]\ncrash = df[(df['matchType']=='crashtpp') | (df['matchType']=='crashfpp')]","67f869cd":"dp_by_type = {'flare':[flare,62],\n              'crash':[crash,149],\n              'squad':[squad,53635],\n              'solo':[solo,16165],\n              'duo':[duo,29691]\n              }","6b8dbcca":"for name,ele in dp_by_type.items():\n    print(name + \" : \" + str(len(ele[0])))\n    ele[0].to_csv(name+'.csv', index=False)\n    ele[0] = 0","5454b19f":"#del squad_fpp,duo,solo_fpp,squad,duo_fpp,solo,flare,crash","9e1d6f98":"del duo,squad,solo,flare,crash","945d9e5b":"del df\ngc.collect()","4185c233":"def featureEngineering(df):\n    return featureEngineeringSecond(reduce_mem_usage(featureEngineeringFirst(df)))","e44a9004":"def items(df):\n    df['items'] = df['heals'] + df['boosts']\n    return df\n\ndef survival(df):\n    df[\"survival\"] = df[\"revives\"] + df[\"boosts\"] + df[\"heals\"]\n    return df\n\ndef players_in_team(df):\n    agg = df.groupby(['groupId']).size().to_frame('players_in_team')\n    return df.merge(agg, how='left', on=['groupId'])\n\ndef total_distance(df):\n    df['total_distance'] = df['rideDistance'] + df['swimDistance'] + df['walkDistance']\n    return df\n\ndef total_time_by_distance(df):\n    df[\"total_time_by_distance\"] = df[\"rideDistance\"]\/5+df[\"walkDistance\"]+df[\"swimDistance\"]*5\n    return df\n\ndef headshotKills_over_kills(df):\n    df['headshotKills_over_kills'] = df['headshotKills'] \/ df['kills']\n    df['headshotKills_over_kills'].fillna(0, inplace=True)\n    return df\n\ndef teamwork(df):\n    df['teamwork'] = df['assists'] + df['revives']\n    return df\n\ndef total_items_acquired(df):\n    df['total_items_acquired'] = df[\"boosts\"] + df[\"heals\"] + df[\"weaponsAcquired\"]\n    return df\n\ndef killPlace_over_maxPlace(df):\n    df['killPlace_over_maxPlace'] = df['killPlace'] \/ df['maxPlace']\n    df['killPlace_over_maxPlace'].fillna(0, inplace=True)\n    df['killPlace_over_maxPlace'].replace(np.inf, 0, inplace=True)\n    return df\n\ndef distance_over_heals(df):\n    df['walkDistance_over_heals'] = df['total_distance'] \/ df['heals']\n    df['walkDistance_over_heals'].fillna(0, inplace=True)\n    df['walkDistance_over_heals'].replace(np.inf, 0, inplace=True)\n    return df\n\ndef distance_over_kills(df):\n    df['walkDistance_over_kills'] = df['total_distance'] \/ df['kills']\n    df['walkDistance_over_kills'].fillna(0, inplace=True)\n    df['walkDistance_over_kills'].replace(np.inf, 0, inplace=True)\n    return df\n\ndef headshot_kill_rate(df):\n    df['headshot_kill_rate'] = (df['headshotKills']+1)\/(df['kills']+1)\n    return df\n","20e82881":"def featureEngineeringFirst(df):\n    print(\"        Feature Engineering First started...\")\n        \n    df = items(df)\n    gc.collect()\n        \n    df = survival(df)\n    gc.collect()\n    \n    df = players_in_team(df)\n    gc.collect()\n    \n    df = total_distance(df)\n    gc.collect()\n    \n    df = total_time_by_distance(df)\n    gc.collect()\n    \n    #df = headshotKills_over_kills(df)\n    gc.collect()\n\n    df = teamwork(df)\n    gc.collect()\n    \n    df = total_items_acquired(df)\n    gc.collect()\n    \n    #df = killPlace_over_maxPlace(df)\n    gc.collect()\n    \n    #df = distance_over_heals(df)\n    gc.collect()\n    \n    #df = distance_over_kills(df)\n    gc.collect()\n    \n    #df = headshot_kill_rate(df)\n    gc.collect()\n    \n    df = reduce_mem_usage(df)\n    \n    print(\"        Feature Engineering First fished \")\n    \n    return df","e7683a39":"def min_by_team(df):\n    features = list(df.columns)\n    features.remove('Id')\n    features.remove('groupId')\n    features.remove('matchId')\n    agg = df.groupby(['matchId','groupId'])[features].min()\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    return agg, agg_rank\n\ndef max_by_team(df):\n    features = list(df.columns)\n    features.remove('Id')\n    features.remove('groupId')\n    features.remove('matchId')\n    agg = df.groupby(['matchId', 'groupId'])[features].max()\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    return agg, agg_rank\n\ndef sum_by_team(df):\n    features = list(df.columns)\n    features.remove('Id')\n    features.remove('groupId')\n    features.remove('matchId')\n    agg = df.groupby(['matchId', 'groupId'])[features].sum()\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    return agg, agg_rank\n\ndef median_by_team(df):\n    features = list(df.columns)\n    features.remove('Id')\n    features.remove('groupId')\n    features.remove('matchId')\n    agg = df.groupby(['matchId', 'groupId'])[features].median()\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    return agg, agg_rank\n\ndef mean_by_team(df):\n    features = list(df.columns)\n    features.remove('Id')\n    features.remove('groupId')\n    features.remove('matchId')\n    agg = df.groupby(['matchId', 'groupId'])[features].agg('mean')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    return agg, agg_rank","e1b1e68b":"def mergeWithAgg(df,agg,agg_rank,name):\n    print(\"            Merge \"+name)\n    df = df.merge(agg, suffixes=[\"\", \"_\"+name], how='left', on=['matchId', 'groupId'])\n    df = df.merge(agg_rank, suffixes=[\"\", \"_\"+name+\"_rank\"], how='left', on=['matchId', 'groupId'])\n    return reduce_mem_usage(df)","17d3953e":"def featureEngineeringSecond(df):\n    print(\"        Feature Engineering Second started...\")\n    \n    print(\"            Min\")\n    min_, min_rank = min_by_team(df)\n    gc.collect()\n    \n    print(\"            Max\")\n    max_, max_rank = max_by_team(df)\n    gc.collect()\n    \n    print(\"            Sum\")\n    sum_, sum_rank = sum_by_team(df)\n    gc.collect()\n    \n    print(\"            Median\")\n    median_, median_rank = median_by_team(df)\n    gc.collect()\n    \n    print(\"            Mean\")\n    mean_, mean_rank = mean_by_team(df)\n    gc.collect()\n    \n    df = mergeWithAgg(df,min_, min_rank,\"min\")\n    del min_, min_rank\n    df = mergeWithAgg(df,max_, max_rank,\"max\")\n    del max_, max_rank\n    df = mergeWithAgg(df,sum_, sum_rank,\"sum\")\n    del sum_, sum_rank\n    df = mergeWithAgg(df,median_, median_rank,\"median\")\n    del median_, median_rank\n    df = mergeWithAgg(df,mean_, mean_rank,\"mean\")\n    del mean_, mean_rank\n        \n    print(\"        Feature Engineering Second finished\")\n    \n    return df","f00193ce":"def baseline_model(input_dim):\n    model = Sequential()\n    # create model\n    model.add(Dense(32, kernel_initializer='he_normal',input_dim=input_dim , activation='selu'))\n    model.add(Dense(64, kernel_initializer='he_normal', activation='selu'))\n    model.add(Dense(128, kernel_initializer='he_normal', activation='selu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(128, kernel_initializer='he_normal', activation='selu'))\n    model.add(BatchNormalization())\n    model.add(Dense(64, kernel_initializer='he_normal', activation='selu'))\n    model.add(BatchNormalization())\n    model.add(Dense(32, kernel_initializer='he_normal', activation='selu'))\n    model.add(Dense(8, kernel_initializer='he_normal', activation='selu'))\n    model.add(BatchNormalization())\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n    return model","7d42858a":"def normalize(df):\n    result = df.copy()\n    for feature_name in df.columns:\n        if df[feature_name].dtype != object:\n            max_value = df[feature_name].max()\n            min_value = df[feature_name].min()\n            result[feature_name] = (df[feature_name] - min_value) \/ (max_value - min_value)\n    return result","e6110378":"def learningPart(df,batch_size):\n    print(\"    Data processing started...\")\n    df_winPlacePerc = df.winPlacePerc\n    df = df.drop('winPlacePerc', axis=1)\n    df = df.drop('matchType', axis=1)\n    df = featureEngineering(df)\n    df = df.drop('matchId', axis=1)\n    df = df.drop('groupId', axis=1)\n    gc.collect()\n    df['winPlacePerc'] =  df_winPlacePerc.values\n    \n    del df_winPlacePerc \n    gc.collect()\n    \n    print(\"    Data scaling started...\")\n    df = normalize(df)\n    print(\"    Data scaling finised\")\n    \n    train_df = df[df[\"winPlacePerc\"] != -1]\n    train_df = train_df.dropna()\n    gc.collect()\n    test_df = df[df[\"winPlacePerc\"] == -1]\n    test_df = test_df.drop('winPlacePerc', axis=1)\n    del df\n    gc.collect()\n    \n    train_df_Y = train_df.winPlacePerc\n    train_df_X = train_df.drop('winPlacePerc', axis=1)\n    del train_df\n    print(\"    Data processing finished\")\n    \n\n    \n    print(\"    Data spliting started...\")\n    X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(train_df_X, train_df_Y, test_size=0.2)\n    del train_df_Y\n    del train_df_X\n    print(\"    Data spliting finished\")\n    \n    print(\"    Model training started...\")\n    epochs = 80\n    \n    callbacks = [ModelCheckpoint('best_model_df.h5', verbose=0, monitor='val_loss',save_best_only=True, mode='auto')]\n    \n    model_df = baseline_model(X_train_df.shape[1])\n    history_df = model_df.fit(X_train_df, y_train_df, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_test_df, y_test_df),callbacks=callbacks)\n    gc.collect()    \n    print(\"    Model training finished\")\n    \n    print(\"    Prediction started...\")\n    best_model_df = load_model('best_model_df.h5')\n    \n    predict_y_df = best_model_df.predict(t_test_df)\n    predict_y_df = pd.DataFrame({'Id':test_df['Id'].values,'winPlacePerc':predict_y_df.flatten()})\n    print(\"    Prediction finished\")\n    \n    return predict_y_df, history_df","3f2bfe66":"def minmax(df):\n    df_minmax = pd.DataFrame()\n    for feature_name in df.columns:\n        v_min = df[feature_name].min()\n        v_max = df[feature_name].max()\n        df_minmax[feature_name] = pd.Series([v_min,v_max])\n    \n    return df_minmax","938b7582":"def learningPart(df,batch_size):\n    print(\"    Data processing started...\")\n    df = df.drop('matchType', axis=1)\n    train_df = df[df[\"winPlacePerc\"] != -1]\n    gc.collect()\n    test_df = df[df[\"winPlacePerc\"] == -1]\n    test_df = test_df.drop('winPlacePerc', axis=1)\n    del df\n    gc.collect()\n    \n    train_df_Y = train_df.winPlacePerc\n    train_df_X = train_df.drop('winPlacePerc', axis=1)\n    del train_df\n    \n    train_df_X = featureEngineering(train_df_X)\n    train_df_X = train_df_X.drop('matchId', axis=1)\n    train_df_X = train_df_X.drop('groupId', axis=1)\n       \n    print(\"    Data processing finished\")\n    \n    \n    print(\"    Data spliting started...\")\n    X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(train_df_X, train_df_Y, test_size=0.2)\n    del train_df_Y\n    del train_df_X\n    gc.collect()\n    print(\"    Data spliting finished\")\n    \n\n    print(\"    Data test processing started...\")\n    test_df = featureEngineering(test_df)\n    test_df = test_df.drop('matchId', axis=1)\n    test_df = test_df.drop('groupId', axis=1) \n    \n    print(\"    Data test processing finished\")\n    \n    \n    print(\"    Data scaling started...\")\n    minmaxv = pd.concat([minmax(test_df),minmax(X_train_df),minmax(X_test_df)])\n    scaler_df = preprocessing.MinMaxScaler(feature_range=(-1, 1),copy=False).fit(minmaxv)\n    del minmaxv\n    gc.collect()\n    X_train_df = scaler_df.transform(X_train_df)\n    gc.collect()\n    X_test_df = scaler_df.transform(X_test_df)\n    gc.collect()\n    t_test_df = scaler_df.transform(test_df)\n    print(\"    Data scaling finised\")\n    \n    print(\"    Model training started...\")\n    epochs = 80\n    \n    callbacks = [ModelCheckpoint('best_model_df.h5', verbose=0, monitor='val_loss',save_best_only=True, mode='auto')]\n    \n    model_df = baseline_model(X_train_df.shape[1])\n    history_df = model_df.fit(X_train_df, y_train_df, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_test_df, y_test_df),callbacks=callbacks)\n    gc.collect()    \n    print(\"    Model training finished\")\n    \n    print(\"    Prediction started...\")\n    best_model_df = load_model('best_model_df.h5')\n    \n    predict_y_df = best_model_df.predict(t_test_df)\n    predict_y_df = pd.DataFrame({'Id':test_df['Id'].values,'winPlacePerc':predict_y_df.flatten()})\n    print(\"    Prediction finished\")\n    \n    return predict_y_df, history_df","0e915e2f":"result = {}\nfor name,typeGame in dp_by_type.items():\n    print(\"Start \"+ name)\n    predict, history = learningPart(pd.read_csv(name+\".csv\"),typeGame[1])\n    result[name] = [predict,history]\n    print(\"End \"+ name)\n    dp_by_type[name] = 0","139176a0":"def displayHistory(history):\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation mae values\n    plt.plot(history.history['mean_absolute_error'])\n    plt.plot(history.history['val_mean_absolute_error'])\n    plt.title('Mean Abosulte Error')\n    plt.ylabel('Mean absolute error')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()","bb1ce6f3":"for key,ele in result.items():\n    print(\"History for \" + str(key))\n    displayHistory(ele[1])","adc5fe2b":"data = []\nfor key,ele in result.items():\n    data.append(ele[0])\n    ","12bc7b48":"df = pd.concat(data, ignore_index=True)\ndf = df.sort_values(by='Id', ascending=[True])\ndf = df.reset_index(drop=True)","05816390":"submission = pd.read_csv(\"..\/input\/sample_submission_V2.csv\")","3783946a":"submission[\"winPlacePerc\"] = df.winPlacePerc","f6a15720":"submission.to_csv('submission.csv', index=False)","14b1db7a":"# PART concat","556b1d16":"This part is for training and predicting. I use Sequential model from keras. ","459f8bf7":"Thank you for reading. If you have any questions\/remarks, ask me.","3a4c9b99":"this part is for feature engineering. there are two part, one for make new feartures and the second to coralate data together.","705c391e":"# Learning part","987f6ae2":"# Result","cc8eb862":"# Split by matchType","954c81da":"This part is to merge result and make submission.","c13b720a":"# prediction group by match type\nHello, it's my first public kernel. i will try to make prediction for each match type. ","d32a2fdf":"# FeatureEngineering","899b53bb":"In this part, I merge test and train value to create each matchType dataset. To identified test value, I add winPlacePerc value at -1.","2201fa8d":"This part is for visualied training evolution.","2c28f3a6":"I tried with different combination, second is better. I make dict with name of type and batch size. I serialize each dataset to release RAM."}}