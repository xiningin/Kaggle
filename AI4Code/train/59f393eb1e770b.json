{"cell_type":{"be014139":"code","d19d7b21":"code","d8f5c5e3":"code","9bc0dcfa":"code","a89136f7":"code","a2ebf9d8":"code","d59a3800":"code","eb9e7cdc":"code","463e4c61":"code","330211bc":"code","e597fa3e":"code","a89f17cd":"code","3b64f5f5":"code","4cce34a6":"code","72a51eba":"code","eeaabedb":"code","a8e5794d":"code","f840fc01":"code","44b943d3":"code","5980bed8":"code","9697f534":"code","db59efbf":"code","8215e116":"code","b1932d5e":"code","d165889d":"code","f7cfbc26":"code","b6dc2ebf":"code","f60e08ae":"code","30b1286b":"code","7cc75565":"code","e90f8ae0":"code","e18ce3a5":"code","eec3197a":"code","a17918d8":"code","ccfcea13":"code","433c0a02":"code","d580810b":"code","e0343198":"code","5226b453":"code","db083eb4":"code","47a39a7c":"code","49111ed2":"code","192ef9dd":"code","15fe5374":"code","34543682":"code","d2c9445e":"code","4a947282":"code","68d1860a":"code","12c33e96":"code","b8a86d02":"code","6784883b":"code","612d5cfb":"code","aaa8008c":"code","d5a24c71":"code","d517b618":"markdown","59646f48":"markdown","280d84f2":"markdown","66429b2e":"markdown","d6fbb99f":"markdown","f442e708":"markdown","95587b4a":"markdown","f63e087c":"markdown","83293c6e":"markdown","f1791c54":"markdown","296c346c":"markdown","618db5d9":"markdown","d1dc7c81":"markdown","1a063f13":"markdown","daa0d18d":"markdown","eb8e9aa4":"markdown"},"source":{"be014139":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d19d7b21":"from IPython.display import Image","d8f5c5e3":"import matplotlib.pyplot as plt","9bc0dcfa":"df = pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\ndf","a89136f7":"plt.hist(df[\"age\"])","a2ebf9d8":"def age_group(x):\n    if x < 38:\n        return 0\n    if x < 52:\n        return 1\n    if x < 67:\n        return 2\n    return 3","d59a3800":"df[\"age\"] = df[\"age\"].apply(lambda x : age_group(x))\ndf","eb9e7cdc":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer","463e4c61":"categoricals = [\"age\", \"cp\", \"slp\", \"caa\", \"thall\"]","330211bc":"df[\"age\"].unique()","e597fa3e":"df[\"cp\"].unique()","a89f17cd":"dummys = [0]\nfor i in categoricals:\n    dummys.append(dummys[-1] + len(df[i].unique()))","3b64f5f5":"dummys","4cce34a6":"ct = ColumnTransformer(transformers= [ ( \"encoder\", OneHotEncoder(), categoricals ) ], remainder=\"passthrough\")","72a51eba":"df = pd.DataFrame(ct.fit_transform(df))\ndf","eeaabedb":"df","a8e5794d":"df = df.drop(columns = dummys)\ndf","f840fc01":"from sklearn.preprocessing import StandardScaler","44b943d3":"sc = StandardScaler()","5980bed8":"from sklearn.model_selection import train_test_split","9697f534":" X_train, X_test, y_train, y_test = train_test_split(df.iloc[ : , :-1], df.iloc[: , -1:], test_size = 0.2)","db59efbf":"for i in (21,22,25,27):\n    X_train[i] = sc.fit_transform(X_train[i].values.reshape(-1,1))\n    X_test[i] = sc.transform(X_test[i].values.reshape(-1,1))","8215e116":"X_train","b1932d5e":"from sklearn.linear_model import LogisticRegression","d165889d":"reg = LogisticRegression()","f7cfbc26":"from sklearn.metrics import accuracy_score","b6dc2ebf":"reg.fit(X_train, y_train)","f60e08ae":"accuracy_score(y_test, reg.predict(X_test))","30b1286b":"from sympy import *","7cc75565":"Image('..\/input\/image1\/1.png')","e90f8ae0":"from sklearn.decomposition import PCA\npca = PCA(n_components=5)","e18ce3a5":"X_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","eec3197a":"X_train[0:1]","a17918d8":"p_num = 5","ccfcea13":"theta_symbols = []\nx_symbols = []\ny = Symbol(\"y\")\nfor i in range(p_num + 1):\n    theta = \"t\" + str(i)\n    x = \"x\" + str(i)\n    theta_symbols.append(Symbol(theta))\n    x_symbols.append(Symbol(x))","433c0a02":"h_function = theta_symbols[0]\nfor i in range(1,p_num + 1):\n    h_function += theta_symbols[i] * x_symbols[i]","d580810b":"h_function","e0343198":"cost_function = (h_function - y)**2","5226b453":"Image('..\/input\/image1\/2.png')","db083eb4":"Image('..\/input\/image1\/3.jpg')","47a39a7c":"learning_rate = 0.1\nm = 242 # X_train has 242 rows","49111ed2":"Image('..\/input\/image2\/1.png')","192ef9dd":"Image('..\/input\/image3\/Untitled.png')","15fe5374":"X_test","34543682":"vals = [0.01] * (p_num+1)","d2c9445e":"def get_subs(k, index):\n    subs = []\n    for i in range(m):\n        subs.append( (k[i], X_train[i][index-1] ) )\n    return subs","4a947282":"def get_thetas():\n    subs = []\n    for i in range(p_num + 1):\n        subs.append( (theta_symbols[i], vals[i]) )\n    return subs","68d1860a":"def get_x(index, t):\n    subs = []\n    if t == \"train\":\n        for i in range(1,p_num+1):\n            subs.append( (x_symbols[i], X_train[index][i-1] ))\n        subs.append( (y, float(y_train.iloc[index]) ) )\n    else:\n        for i in range(1,p_num+1):\n            subs.append( (x_symbols[i], X_test[index][i-1]) )\n        subs.append( (y, float(y_test.iloc[index]) ) )\n    return subs","12c33e96":"def sum_of_loss():\n    sum = 0\n    for k in range(61):\n        sum += cost_function.subs(get_thetas() + get_x(k, \"test\"))\n    return sum","b8a86d02":"helper_function = (h_function- y)\nhelper_function","6784883b":"losses = []\nfor k in range(25):\n    helper_function_2 = helper_function.subs(get_thetas())\n    sum_loss = 0\n    k = []\n    for j in range(m):\n        symbol = Symbol(\"k\" + str(j))\n        sum_loss += helper_function_2.subs(get_x(j, \"train\")) * symbol\n        k.append(symbol)\n    temp = []\n    temp.append(sum_loss.subs(list(zip(k, [1] * m))))\n    for i in range(1,p_num+1):\n        print(\".....\")\n        sum_loss_2 = sum_loss.subs(get_subs(k, i))\n        temp.append(sum_loss_2)\n    for i in range(p_num+1):\n        vals[i] -= temp[i] \/ (m \/ learning_rate)\n    print(vals)\n    losses.append(sum_of_loss())","612d5cfb":"plt.plot(range(25),losses)","aaa8008c":"y_pred = []\nfor i in range(61):\n    x = h_function.subs(get_thetas() + get_x(i, \"test\"))\n    if x > 0.5:\n        y_pred.append(1)\n    else:\n        y_pred.append(0)","d5a24c71":"accuracy_score(y_test, y_pred)","d517b618":"## First try sklearn library","59646f48":"### Categoricals -> \"age\", \"cp\", \"slp\", \"caa\", \"thall\"\n\n### And since we use linear regression we must avoid dummy trap","280d84f2":"## Standard Scaling","66429b2e":"#### And now we can tune parameters using gradient decent algorithm","d6fbb99f":"Initially we choose theta's 0.01","f442e708":"### We need to scale column 21,22,25,27","95587b4a":"# Avoid Dummy Trap","f63e087c":"##### We can avoid dummy trap by doing drop first column for every categoricals","83293c6e":"It takes some time (we didn't use fast library like numpy)","f1791c54":"##### Our data has 5 parameters","296c346c":"#### h = t0 + t1*x1 + t2*x2 + t3*x3 + t4*x4 + t5*x5","618db5d9":"We can use PCA for our algorithm","d1dc7c81":"### Let's look at the formulas above and create our hypothesis","1a063f13":"## Our Algorithm","daa0d18d":"![image.png](attachment:80c726ca-5c88-4834-8d57-ee3d30ac2782.png)","eb8e9aa4":"![Untitled.png](attachment:6891cea1-943d-4757-94fe-11dde89f4607.png)"}}