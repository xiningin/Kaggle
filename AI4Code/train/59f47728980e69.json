{"cell_type":{"393e14ec":"code","2b7e2c33":"code","7dac7277":"code","386d2005":"code","be815469":"code","ee480db6":"code","62cfcc53":"code","0f01e2eb":"code","fceba011":"code","979baa63":"code","47bf1832":"code","dabd4a51":"code","ce97d5f3":"code","785e76cd":"code","ae2aed83":"code","8b99243c":"code","44eea49f":"code","3d18ec7e":"code","78aa6d6f":"code","6d8e3aca":"code","5190f6d6":"code","eb7f5aac":"markdown","abded5d8":"markdown","fea1f2dd":"markdown"},"source":{"393e14ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport re\nfrom bs4 import BeautifulSoup\nimport os\nimport random\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import KFold, cross_val_score\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\n\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b7e2c33":"TEST_DATA_PATH = '..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv'\nVALID_DATA_PATH = '..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv'\nTRAIN_DATA_PATH = '..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv'","7dac7277":"SEED = 10\nMAX_FEATURES = 10_000","386d2005":"def set_seed(seed=42):\n    \"\"\"Utility function to use for reproducibility.\n    :param seed: Random seed\n    :return: None\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\ndef set_display():\n    \"\"\"Function sets display options for charts and pd.DataFrames.\n    \"\"\"\n    # Plots display settings\n    plt.style.use('fivethirtyeight')\n    plt.rcParams['figure.figsize'] = 12, 8\n    plt.rcParams.update({'font.size': 14})\n    # DataFrame display settings\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.max_rows', None)\n    pd.options.display.float_format = '{:.4f}'.format\n    \n    \ndef text_cleaning(text: str) -> str:\n    \"\"\"Function cleans text removing special characters,\n    extra spaces, embedded URL links, HTML tags and emojis.\n    Code source: https:\/\/www.kaggle.com\/manabendrarout\/pytorch-roberta-ranking-baseline-jrstc-infer\n    :param text: Original text\n    :return: Preprocessed text\n    \"\"\"\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+')  # website links\n    text = template.sub(r'', text)\n\n    soup = BeautifulSoup(text, 'lxml')  # HTML tags\n    only_text = soup.get_text()\n    text = only_text\n\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n\n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text)  # special characters\n    text = re.sub(' +', ' ', text)  # extra spaces\n    text = text.strip()  # spaces at the beginning and at the end of string\n\n    return text","be815469":"set_seed(SEED)\nset_display()","ee480db6":"# Extract classified text samples and clean the texts.\ndata_train = pd.read_csv(TRAIN_DATA_PATH)\ndata_train['comment_text'] = data_train['comment_text'].apply(text_cleaning)\ndata_train.head()","62cfcc53":"categories = data_train.loc[:, 'toxic':'identity_hate'].sum()\nplt.title('Category Frequency')\nplt.bar(categories.index, categories.values)\nplt.show()","0f01e2eb":"scores = data_train.loc[:, 'toxic':'identity_hate'].sum(axis=1).value_counts()\nplt.bar(scores.index, scores.values)\nplt.title('Scores Distribution: Simple Sum')\nplt.show()","fceba011":"# Multiplication factors for categories.\ncat_mtpl = {'toxic': 1, 'severe_toxic': 1.75, 'obscene': 0.95,\n            'threat': 2, 'insult': 1.6, 'identity_hate': 1.95}\n\nfor category in cat_mtpl:\n    data_train[category] = data_train[category] * cat_mtpl[category]\n\ndata_train['score'] = data_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)","979baa63":"plt.hist(data_train['score'])\nplt.title('Scores Distribution: Adjusted Sum')\nplt.show()","47bf1832":"n_samples_toxic = len(data_train[data_train['score'] != 0])\nn_samples_normal = len(data_train) - n_samples_toxic\n\nidx_to_drop = data_train[data_train['score'] == 0].index[n_samples_toxic\/\/5:]\ndata_train = data_train.drop(idx_to_drop)\n\nprint(f'Reduced number of neutral text samples from {n_samples_normal} to {n_samples_toxic\/\/5}.')\nprint(f'Total number of training samples: {len(data_train)}')","dabd4a51":"print(f'Mean toxicity score: {data_train[\"score\"].mean()}\\n'\n      f'Standard deviation: {data_train[\"score\"].std()}')","ce97d5f3":"# Candidate models\nkridge = make_pipeline(\n    TfidfVectorizer(decode_error='ignore', stop_words='english', max_features=MAX_FEATURES),\n    KernelRidge()\n)\n\nrandforest = make_pipeline(\n    TfidfVectorizer(decode_error='ignore', stop_words='english', max_features=MAX_FEATURES),\n    RandomForestRegressor(n_jobs=-1)\n)","785e76cd":"models = [\n    ('KernelRidge', kridge),\n    ('RandomForest', randforest)\n]","ae2aed83":"# New data for validation: text pairs.\ndata_valid = pd.read_csv(VALID_DATA_PATH)\n\n# Clean the texts\ndata_valid['less_toxic'] = data_valid['less_toxic'].apply(text_cleaning)\ndata_valid['more_toxic'] = data_valid['more_toxic'].apply(text_cleaning)\n\ndata_valid.head()","8b99243c":"# Train each model on all available samples from previous competition.\nfor name, model in models:\n    print('-' * 50)\n    model.fit(data_train['comment_text'], data_train['score'])\n    print(f'{name} model completed training.')\n\n    # Estimate toxicity score for text pairs.\n    data_valid[f'less_toxic_score_{name}'] = model.predict(data_valid['less_toxic'])\n    data_valid[f'more_toxic_score_{name}'] = model.predict(data_valid['more_toxic'])\n    print(f'{name} model completed prediction.')\n\n    # Compare scores for all text pairs.\n    data_valid[f'result_{name}'] = \\\n        data_valid[f'more_toxic_score_{name}'] > data_valid[f'less_toxic_score_{name}']\n\n    # Ratio of correctly scored text pairs.\n    print('Correct predictions:', data_valid[f'result_{name}'].sum() \/ len(data_valid))\n    \n    joblib.dump(model, f'{name}.joblib')","44eea49f":"# Check the accuracy of averaged scores from the best models.\ndata_valid['less_toxic_score'] = data_valid[['less_toxic_score_KernelRidge', 'less_toxic_score_RandomForest']].mean(axis=1)\n\ndata_valid['more_toxic_score'] = data_valid[['more_toxic_score_KernelRidge', 'more_toxic_score_RandomForest']].mean(axis=1)\n\ndata_valid[f'result'] = data_valid[f'more_toxic_score'] > data_valid[f'less_toxic_score']\nprint('Correct averaged predictions:', data_valid[f'result'].sum() \/ len(data_valid))","3d18ec7e":"# New data for text scoring.\ndata_test = pd.read_csv(TEST_DATA_PATH)\ndata_test['text'] = data_test['text'].apply(text_cleaning)\ndata_test.head()","78aa6d6f":"# Get prediction from the best models.\nfor name, model in models[0:]:\n    data_test[f'score_{name}'] = model.predict(data_test['text'])\n    print(f'{name} model completed prediction.')","6d8e3aca":"# Average the result.\ndata_test['score'] = data_test[['score_KernelRidge', 'score_RandomForest']].mean(axis=1)","5190f6d6":"data_test[['comment_id', 'score']].to_csv('submission.csv', index=False)\ndata_test[['comment_id', 'score']].head()","eb7f5aac":"# Predictions","abded5d8":"# Import Libraries","fea1f2dd":"# Model "}}