{"cell_type":{"9f2f6540":"code","af5510c7":"code","349680c8":"code","4bc8a1c3":"code","fd5b6c4d":"code","4ee14d4c":"code","a89dce06":"code","fbb31138":"code","c54341f5":"code","959f5571":"code","dd99c9c4":"code","546e130d":"code","4ab3cb68":"code","8ccca8b3":"code","97ed3098":"code","2fda06fa":"code","dc55fde5":"code","52126585":"code","1496538e":"code","896cab10":"code","a9a25906":"code","f739a66d":"code","5283fb6d":"code","3b2b2a6f":"code","51f508c4":"code","75e3a28e":"code","b2feb996":"code","c12b88fe":"markdown","677bd61b":"markdown","89164ba1":"markdown","641c6fa6":"markdown","8020b438":"markdown","c1e3aae4":"markdown","b8edabb8":"markdown","905e9c0f":"markdown","85d1ddca":"markdown","f91004af":"markdown","8395179a":"markdown","52dd6937":"markdown","0e4f1a81":"markdown","79b82a59":"markdown","d418dda1":"markdown","bcc9944a":"markdown","bbd2fb34":"markdown","a34d39ed":"markdown","f70a8994":"markdown","53eb8034":"markdown","4cc6ccb8":"markdown","c1628ed6":"markdown"},"source":{"9f2f6540":"# import the necessary liberaries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # visuals\nimport seaborn as sns # advanced visuals\n\nimport warnings # ignore warnings\nwarnings.filterwarnings('ignore')","af5510c7":"# read the data\ndf = pd.read_csv(\"..\/input\/iris\/Iris.csv\")","349680c8":"# display the first 5 rows\ndf.head()","4bc8a1c3":"# main characteristics of the dataframe\ndf.info()","fd5b6c4d":"# drop Id, axis = 1: tells python to drop the entire column\n# Do not run this cell more than once\ndf = df.drop(\"Id\", axis = 1)\ndf.head()","4ee14d4c":"# summary statistics\ndf.describe()","a89dce06":"# How many species in our dataframe?\n# is the data balanced?\ndf[\"Species\"].value_counts()","fbb31138":"fig, axes = plt.subplots(2, 2, figsize=(10,5), dpi = 100)\nfig.suptitle('Distribution of (sepal length, sepal width, petal length, petal width) per Species')\n\n# Distribution of sepal length per Species\nsns.kdeplot(ax = axes[0,0], data = df, x = 'SepalLengthCm', hue = \"Species\", alpha = 0.5, shade = True)\naxes[0,0].set_xlabel(\"Sepal Length CM\")\naxes[0,0].get_legend().remove()\n\n# Distribution of sepal width per Species\nsns.kdeplot(ax = axes[0,1], data = df, x = 'SepalWidthCm', hue = \"Species\", alpha = 0.5, shade = True)\naxes[0,1].set_xlabel(\"Sepal width CM\")\naxes[0,1].get_legend().remove()\n\n# Distribution of petal length per Species\nsns.kdeplot(ax = axes[1,0], data = df, x = 'PetalLengthCm', hue = \"Species\", alpha = 0.5, shade = True)\naxes[1,0].set_xlabel(\"Petal Length CM\")\naxes[1,0].get_legend().remove()\n\n# Distribution of petal width per Species\nsns.kdeplot(ax = axes[1,1], data = df, x = 'PetalWidthCm', hue = \"Species\", alpha = 0.5, shade = True)\naxes[1,1].set_xlabel(\"Petal Width CM\")\n\nplt.tight_layout()","c54341f5":"# Scatter plot od petal length vs petal width\nplt.figure(figsize = (7, 3), dpi = 100)\nsns.scatterplot(data = df, x = 'PetalLengthCm', y = 'PetalWidthCm', hue = \"Species\")\nplt.title(\"Species clusters based on Sepal length and width\")\nplt.xlabel(\"Petal Length Cm\")\nplt.ylabel(\"Petal Width Cm\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()","959f5571":"# Scatter plot od sepal length vs petal width\nplt.figure(figsize = (7, 3), dpi = 100)\nsns.scatterplot(data = df, x = 'SepalLengthCm', y = 'SepalWidthCm', hue = \"Species\")\nplt.title(\"Species clusters based on Sepal length and width\")\nplt.xlabel(\"Sepal Length Cm\")\nplt.ylabel(\"Sepal Width Cm\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()","dd99c9c4":"#box plots\nfig, axes = plt.subplots(2, 2, figsize=(10,5), dpi = 100)\n\n#Mean Sepal Length\nsns.boxplot(ax = axes[0,0], data = df, x = \"Species\", y = 'SepalLengthCm')\naxes[0,0].set_xlabel(None)\naxes[0,0].set_ylabel(None)\naxes[0,0].set_title(\"Mean Sepal Length\")\n\n\n#Mean Sepal Width\nsns.boxplot(ax = axes[0,1], data = df, x = \"Species\", y = 'SepalWidthCm')\naxes[0,1].set_xlabel(None)\naxes[0,1].set_ylabel(None)\naxes[0,1].set_title(\"Mean Sepal Width\")\n\n#Mean Petal Length\nsns.boxplot(ax = axes[1,0], data = df, x = \"Species\", y = 'PetalLengthCm')\naxes[1,0].set_xlabel(None)\naxes[1,0].set_ylabel(None)\naxes[1,0].set_title(\"Mean Petal Length\")\n\n#Mean Petal Width\nsns.boxplot(ax = axes[1,1], data = df, x = \"Species\", y = 'PetalWidthCm')\naxes[1,1].set_xlabel(None)\naxes[1,1].set_ylabel(None)\naxes[1,1].set_title(\"Mean Petal Width\")\n\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.5)","546e130d":"# Correlation map\nplt.figure(figsize = (8, 4), dpi = 100)\nsns.heatmap(df.corr(), annot = True, cmap = \"viridis\", vmin = -1, vmax = 1)\nplt.title(\"Correlation map between variables\")\n#plt.xticks(rotation = 90)\nplt.show()","4ab3cb68":"# 1. Seprate the dependent variable from the independent ones.\n\nX = df.drop(\"Species\", axis = 1)\ny = df[\"Species\"]","8ccca8b3":"# 2. Perform a train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","97ed3098":"# 3. Feature scaling\nfrom sklearn.preprocessing import StandardScaler # import the scaler\nscaler = StandardScaler() # initiate it\nScaled_X_train = scaler.fit_transform(X_train) #fit the parameters and use it to trannsform the traning data\nScaled_X_test = scaler.transform(X_test) #transform the test data","2fda06fa":"# Logestic Regression \nfrom sklearn.linear_model import LogisticRegression # import the classifier\nlog_model = LogisticRegression() #initiate it\nlog_model.fit(Scaled_X_train, y_train) #fit the model to the training data","dc55fde5":"# creating predictions \ny_pred = log_model.predict(Scaled_X_test)","52126585":"# import evaluation metrics \nfrom sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, classification_report","1496538e":"# create the confusion matrix\nconfusion_matrix(y_test, y_pred)","896cab10":"# plot the confusion matrix\nfig, ax = plt.subplots(dpi = 120)\nplot_confusion_matrix(log_model, Scaled_X_test, y_test, ax = ax);","a9a25906":"# measure the accuracy of our model\nacc_score = accuracy_score(y_test, y_pred)\nround(acc_score, 2)","f739a66d":"# generate the classification report \nprint(classification_report(y_test, y_pred)) # Hint: try it without using the print() method","5283fb6d":"# import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV \n\n# set the range of paprameters\npenalty = ['l1', 'l2', 'elasticnet']\nC = np.logspace(0,20,50)\nsolver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\nmulti_class = ['ovr', 'multinomial']\nl1_ratio = np.linspace(0, 1, 20)\n\n# build the parameter grid\nparam_grid = {\n   'penalty': penalty,\n    'C': C,\n    'solver': solver,\n    'multi_class': multi_class, \n    'l1_ratio': l1_ratio\n}\n\n# initiate and fit the Grid Search Model\ngrid_model = GridSearchCV(log_model, param_grid = param_grid)\ngrid_model.fit(Scaled_X_train, y_train)","3b2b2a6f":"# best parameters \ngrid_model.best_params_","51f508c4":"# creating predictions \ny_pred = grid_model.predict(Scaled_X_test)\n\n# plot the confusion matrix\nfig, ax = plt.subplots(dpi = 120)\nplot_confusion_matrix(grid_model, Scaled_X_test, y_test, ax = ax);","75e3a28e":"# measure the accuracy of our model\nacc_score = accuracy_score(y_test, y_pred)\nround(acc_score, 2)","b2feb996":"# generate the classification report \nprint(classification_report(y_test, y_pred)) # Hint: try it without using the print() method","c12b88fe":"Have you noticed that we used .fit_transform() with the traning data and only used .transform() with the test data? we did it to aviod data leakage. Read more about it [from here](https:\/\/machinelearningmastery.com\/data-preparation-without-data-leakage\/) ","677bd61b":"As we expected before, the model did a perfect job predicting Setosa. It only misclassified one observation as versicolor, where in fact it is virginica. However, the model performance is near perfect and we could not have done better than that.","89164ba1":"**Why feature scaling?** \n\nReal Life Datasets have many features with a wide range of values like for example let\u2019s consider the house price prediction dataset. It will have many features like no. of. bedrooms, square feet area of the house, etc\n\nAs you can guess, the no. of bedrooms will vary between 1 and 5, but the square feet area will range from 500-2000. This is a huge difference in the range of both features.\n\nMany machine learning algorithms that are using Euclidean distance as a metric to calculate the similarities will fail to give a reasonable recognition to the smaller feature, in this case, the number of bedrooms, which in the real case can turn out to be an actually important metric.\n\n![](https:\/\/i.imgflip.com\/2rcqrd.png)\n\nTo aviod this problem we need to scale the features so that they all have the same scale, i.e the same range of values. We can normalize all features so that have values between (-1, 1) or standardize them to have values between (0, 1). \n\nThe important thing to note here is that feature scaling does not affect the relative importance of features, scaled features will still have the same orginal information and importance relative to each other, this can be clearly demonstated from the image below: despite feature scaling they are still strawberry and apple, they did not lose their meaning.\n\n![](https:\/\/miro.medium.com\/max\/2000\/1*yR54MSI1jjnf2QeGtt57PA.png)","641c6fa6":"### **<a id = \"prepare\">Feature engineering: Data prep for the model<\/a>**","8020b438":"The dataframe has 150 non-null values. It has 6 variables, all of them are in the right data type.\nthe first variable \"Id\" seems to be redundant and unnecessary for the our analysis, we can drop it and keep the rest of variables. ","c1e3aae4":"This section focuses on how to produce and analyze charts that meets the best practices in both academia and industry. we will try to meet the following criteria in each graph:\n1. **Chose the right graph that suits the variable type:** to display the distribution of categorical variables we might opt for count or bar plot. As for continuous variables we might go with a histogram. If we wan to study the distribution of a continuous variable per each calss of other categorical variable we can use a box plots or a kde plot with hue parameter... etc.\n2. **Maximize Dagt-Ink Ration:** it equals to the ink used to display the data devided by the total ink used in the graph. Try not to use so many colors without a good reason for that. Aviod using backround colors, or borders or any other unnecessary decorations.\n3. **Use clear well written Titles, labels, and tick marks.**","b8edabb8":"In this section we will make sure that the data is well prepared for training the model. We will:\n\n1. Seprate the dependent variable from the independent ones. \n2. Perform a train test split \n3. Scale the data (feature scaling).","905e9c0f":"**Why train test split ?** we need to split the data into two parts: \n1. Training part, we will use it to train the model.\n2. Test part: this is unseen data (the model has never seen it before), we will use it the test the real performance of the model.\n\n**Why we need to test on unseen data?** why we do not simply train the model on the whole data and then reuse some of it for evaluation? because this will be like giving the student the answers before entring the exam, the model will be very familiar with the evaluation data because he has seen them before and he will get a full mark. In order for the test to be real, the model has to be evaluated on unseen data.","85d1ddca":"From the summary statistics we can notice that Sepal leafs are wider and longer than Petal lefs, this can be clearly demonstrated in the following image:\n\n![](https:\/\/www.integratedots.com\/wp-content\/uploads\/2019\/06\/iris_petal-sepal-e1560211020463.png)","f91004af":"The data is clean and balanced with exactly the same number of flowers per species: 50 flowers. but why do we care about the balance between number of observations per class? \n\nImbalanced classifications pose a challenge for predictive modeling as most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class.\n\nFor example, an imbalanced multiclass classification problem may have 80 percent examples in the first class, 18 percent in the second class, and 2 percent in a third class.\n\nThe minority class is harder to predict because there are few examples of this class, by definition. This means it is more challenging for a model to learn the characteristics of examples from this class, and to differentiate examples from this class from the majority class (or classes).\n\nThis is a problem because typically, the minority class is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class. \n\nMore detailed explanation can be found [here](https:\/\/machinelearningmastery.com\/what-is-imbalanced-classification\/).","8395179a":"### **<a id = \"eval2\">Model re-evaluation<\/a>**\nWe will evaluate the optimized version of our model and see if it does better than the base model","52dd6937":"### **<a id = 'visual'>Data visualization and explanatory data analysis<\/a>**","0e4f1a81":"Scatter and box plots confirmed the aforementioned conclusion, setosa is easily separable based on petal length and width.","79b82a59":"Congratulations! you have made it to the end of the tutorial. Please leave your feedback and suggestions of improvement","d418dda1":"**Main conclusions from the graph:**\n1. Setosa is easily separable from the other species, this means that the model will be able to classify it accurately.\n2. Petal length and width is expected to be  better predictors of Species than Sepal lenght and width.\n\nBoth conclusions can be demonstrated in the following picture where Setosa is clearly different from other sepcies especially when it comes to its petal leefs, it has a very small sepal width and length comapred to other species.\n\n![](https:\/\/miro.medium.com\/max\/900\/0*Uw37vrrKzeEWahdB)","bcc9944a":"The data set includes three iris species with 50 samples each as well as some properties about each flower.\n\nIn this notebook I will explain How to effectively use logistic regression to solve classification problems. I will try to explain each and every step in a concise and clear manner. we will go through the following, step by step:\n- [Reading and understanding the data](#read)\n- [Data visualization and explanatory data analysis](#visual)\n- [Feature engineering: Data prep for the model](#prepare)\n- [Model building](#build)\n- [Model evaluation](#eval1)\n- [Model optimization: hyper parameter tuning](#hyper)\n- [Model re-evaluation](#eval2)","bbd2fb34":"### **<a id = \"eval1\">Model evaluation<\/a>**\nFirst we will make predictions using the model on the test data, and then evaluate its performance using the following metrics:\n1. **Confusion matrix:** A summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. Here is an example of a confusion matrix: \n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/04\/Example-Confusion-matrix.png)\n\n2. **Accuracy score:** the fraction of predictions our model got right (number of correct predictions devided by total number of predictions).\n3. **Classification report:** used to measure the quality of predictions from a classification algorithm. How many predictions are True and how many are False. The report shows the main classification metrics precision, recall and f1-score on a per-class basis. **Precision:** What percent of your predictions were correct? - **Recall:** What percent of the positive cases did you catch? - **F1 score:** What percent of positive predictions were correct?.\n\nFor more info, click [here](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#model-evaluation) and [here](https:\/\/muthu.co\/understanding-the-classification-report-in-sklearn\/).","a34d39ed":"The optimized model did a completely perfect job. I correctly classified all the examples in the test data. The accuracy of the model is 100 percent. Accuracy improved from 97 percent for the base model to 100 percent for the optimized model.","f70a8994":"### **<a id = \"build\">Model building<\/a>** \nWe will use logestic regression, but the same methodology can be applied to any other classifier","53eb8034":"[Correlation is](https:\/\/www.jmp.com\/en_in\/statistics-knowledge-portal\/what-is-correlation.html) a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate). It's a common tool for describing simple relationships without making a statement about cause and effect.\n\nCorrelation coefficient ranges between -1 (perfect negative correlation) and 1 (perfect positive correlation). As you can notice, there is a strong positive correlation between petal width and length on one hand and sepal length on the other hand. ","4cc6ccb8":"### **<a id = \"read\">Reading and understanding the data<\/a>**","c1628ed6":"### **<a id = \"hyper\">Model optimization: hyper parameter tuning<\/a>**\nHyperparameter tuning [is](https:\/\/neptune.ai\/blog\/hyperparameter-tuning-in-python-a-complete-guide-2020) the process of determining the right combination of parameters that allows us to maximize model performance. We will try different values for each parameter and choose the ones that give us the best predictions."}}