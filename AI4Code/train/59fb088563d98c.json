{"cell_type":{"760d80aa":"code","40776805":"code","a3fa8928":"code","d57ecda6":"code","6b8a4525":"code","63f648f6":"code","b66b6dc4":"code","a8d6829a":"code","b869303b":"code","9b128db9":"code","8f1839ed":"code","0302269b":"code","5ab22b0b":"code","7ebd8e13":"code","426782d2":"code","df67a760":"code","d2d6ca78":"code","e846c560":"code","5d6810e0":"code","60a63f65":"code","429eee8b":"code","e30df6f2":"code","4a65fecc":"code","16e9e9da":"code","286c5cc9":"code","c551596f":"code","1def30c7":"code","1614c885":"code","b29ab293":"code","1e541e56":"code","cf426d72":"code","fc7db495":"code","6cbaaf38":"code","9ecdd287":"code","3f693bdc":"code","2f7d4756":"code","fd80d092":"code","bf65f328":"code","987f066e":"code","4984b742":"code","7dfdd686":"code","d02e25d5":"code","6422108f":"code","addbea98":"code","647b8dee":"code","3be84608":"code","fbd0c762":"code","e090fad9":"code","cf44e742":"code","80248b47":"code","71741345":"code","67fdc61d":"code","e66f3586":"code","92f6d481":"code","a33aeb34":"code","180513f6":"code","f5ab0a84":"code","388686c1":"code","a84f264d":"code","d39c8c03":"code","2ddb359f":"code","692d9511":"code","9f493a44":"code","67fef7ac":"code","9f9bf243":"code","58abb684":"code","fe690599":"code","aad998c2":"code","111a8889":"code","00f8aa77":"code","79c8cbec":"code","33486185":"code","0c3917fd":"code","c2c35d9b":"code","68e63ce9":"code","6bb43465":"code","10649f6c":"code","48b76d25":"code","580bda68":"code","fb3b9a42":"code","d475e51e":"code","305f7d71":"code","25bfc8ee":"code","0fc1bf98":"code","96df610c":"code","d2d9d673":"code","588298d3":"code","bbaa8c26":"code","86646de2":"code","e4903797":"code","a87ae24d":"code","39d1182f":"markdown","55385ebf":"markdown","31aa6291":"markdown","be296bad":"markdown","90f0c9aa":"markdown","3fc4a8a4":"markdown","8f43672f":"markdown","ec631326":"markdown","0c0967b9":"markdown"},"source":{"760d80aa":"## ----------- Part 1: Define and categorize the problem statement --------------\n#### The problem statement is to analyze the cause of absenteeism and predict the every month losses in 2011 due to absenteism.\n##### This is clearly a 'Supervised machine learning regression problem' to predict a number based on the input features\n\n## ----------- Part 1 ends here ----------------- ","40776805":"##------------- Import all the required libraries--------------\n\n## Import all the required libraries\nimport os\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n\n#------ for model evaluation -----\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n#----- for preprocessing\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import StandardScaler\n\n#---- for model building\nimport sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n#---- for cross validation\n#from sklearn.cross_validation import train_test_split\n\n#---- for visualization---\nimport matplotlib.pyplot as plt \nimport seaborn as sn\n\n#------ for model evaluation -----\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n#---- For handling warnings\nimport sys\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")","a3fa8928":"## ------------------- Part 2: Gather the data -----------------\n\n### Here data is provided as .csv file with the problem.\n### Let's import the data \nemp_abntsm=pd.read_excel('..\/input\/Absenteeism_at_work_Project.xls')\nemp_abntsm.head()\n##---------- Part 2 ends here --------------------------","d57ecda6":"# For ease of operations, lets change the names of the columns to short versions\n#emp_abntsm.rename(columns=lambda x: x.replace(' ', '_'))\nemp_abntsm=emp_abntsm.rename(columns = {'Reason for absence':'Absence_Reason','Month of absence':'Absence_Month','Day of the week':'Absence_Day','Transportation expense': 'Transportation_Expense','Distance from Residence to Work':'Work_Distance','Service time':'Service_Time','Work load Average\/day ':'Average_Workload','Hit target': 'Hit_Target','Disciplinary failure':'Disciplinary_Failure','Social drinker':'Drinker','Social smoker':'Smoker','Body mass index':'BMI','Absenteeism time in hours':'Absent_Hours'})","6b8a4525":"emp_abntsm.head()","63f648f6":"#-- Here the target feature is: 'Absenteeism time in hour' and other 20 columns, which are mix of continous and categorical(although defined as int\/floats) features are predictors.\n#-- Lets analyze them further","b66b6dc4":"# ------------Part 3 : Prepare the data for consumption(Data Cleaning) ---------------\n#### 3a.) Check the shape\/properties of the data\n#### 3b.) Completing -- Perform missing value analysis and impute missing values if necessary\n#### 3c.) Correcting -- Check for any invalid data inputs , for outliers or for any out of place data\n#### 3d.) Creating -- Feature extraction . Extract any new features from existing features if required\n#### 3e.) Converting -- Converting data to proper formats","a8d6829a":"#### --------3a.) Check the shape\/properties of the data\n## Check the shape of the data\nemp_abntsm.shape\n\n# what we can infer:\n## ->the dataset has 740 observations and 21 features","b869303b":"## Check the properties of the data\nemp_abntsm.info()\n# what we can infer:\n# ->There are null values in the dataset\n# -> The datatypes are int and float","9b128db9":"#### ------------------3b.) Correcting -- Check for any invalid data inputs \n# From above observations data doesnot seem to have any invalid datatypes to be handled\n\n# However feature 'Absence_Month' have an imvalid value 0. Lets drop it.\n# ALso, as we can see, 'Absent_Hours' are 0 in some places.\n# This could be result of cancelled or withdrwan leaves. Lets drop these\n\nemp_abntsm = emp_abntsm[(emp_abntsm.Absent_Hours > 0)]\nemp_abntsm = emp_abntsm[(pd.notnull(emp_abntsm.Absence_Month)) & ~(emp_abntsm.Absence_Month == 0)] \n# Let's check for the outliers in EDA step","8f1839ed":"# -------------- 3c.) Completing -- Perform missing value analysis and impute missing values if necessary\n#-- Calculating % of nulls\n(emp_abntsm.isna().sum() \/ emp_abntsm.shape[0])*100\n# what we can infer:\n# ->There are  null values in almost all the columns of the dataset, although in small amount.\n# -> We'll drop all the null value rows for target variable and \n# -> We'll will impute null values for all other features.","0302269b":"#-- impute missing values in all the independent featues(exept Average_Workload)\n#-- Replace missing of any any employee with  information of same employee from other instances\n#-- example if 'Age' of employee 1 is missing, then impute it with 'Age' from other instance of employee 1.\nfinal_col = ['Transportation_Expense','Work_Distance','Service_Time','Age','BMI','Drinker','Smoker','Height','Weight','Pet','Son','Education','Disciplinary_Failure','Hit_Target']\n#----impute missing values and Nas --------\nfor i in emp_abntsm['ID'].unique(): \n    for j in final_col :\n        emp_abntsm.loc[((emp_abntsm['ID'] == i) & (emp_abntsm[j].isna())), j] = emp_abntsm[(emp_abntsm.ID==i)][j].max()","5ab22b0b":"#--- Now for 'Average_Workload' missing values, let's analyze which is the best way to impute ","7ebd8e13":"emp_abntsm[['ID','Absence_Month','Average_Workload']].sort_values(['Absence_Month','ID','Average_Workload'])","426782d2":"plt.scatter(x='ID', y='Average_Workload', s=None, c=None, marker=None, data=emp_abntsm)","df67a760":"plt.scatter(x='Absence_Month', y='Average_Workload', s=None, c=None, marker=None, data=emp_abntsm)","d2d6ca78":"#From above, we can deduce that 'Average_Workload' is distributed mostly by month.\n#So, let's impute missing 'Average_Workload' by mode of that month","e846c560":"# update workload with the mode of corresponding month's workload\nfor i in emp_abntsm['Absence_Month'].unique(): \n    frequent_wrkld = stats.mode(emp_abntsm[emp_abntsm['Absence_Month']==i]['Average_Workload'])[0][0]\n    emp_abntsm.loc[((emp_abntsm['Absence_Month']==i) & pd.isna(emp_abntsm['Average_Workload'])),'Average_Workload'] = frequent_wrkld","5d6810e0":"#Fill missing values of 'Absent_Hours' with 0\nemp_abntsm.Absent_Hours = emp_abntsm.Absent_Hours.fillna(0)","60a63f65":"#---- Missing Value handling ENDS here ------------------","429eee8b":"#### 3d.) ------- Converting -- Converting data to proper formats\n# features like 'Absence_Month','Education' are categories here. Lets convert to categories\ncategorical_var = ['Absence_Reason','Absence_Month','Absence_Day','Seasons','Disciplinary_Failure','Education','Son','Drinker','Smoker','Pet']\ncontinous_var = ['ID','Transportation_Expense','Work_Distance','Service_Time','Age','Average_Workload','Hit_Target','Weight','Height','BMI']\ntarget_var = ['Absent_Hours']\n\nfor i in categorical_var:\n    emp_abntsm[i] = emp_abntsm[i].astype(\"category\")\nemp_abntsm.info()","e30df6f2":"#### -----------------3e.) Creating -- Feature extraction . Extract any new features from existing features if required\n\n# Here we do not need any feature extraction.\n# However, before feeding to model, we might need to aggregate the data","4a65fecc":"# ------------Part 3 : Prepare the data for consumption(Data Cleaning) ENDS here------------------------------------------------","16e9e9da":"# ---------------------------Part 4 : Exploratory Data Analysis(EDA) STARTS here -----------------------------------------------","286c5cc9":"#----- 4 a.) Outlier Analysis -----------","c551596f":"box = ['Transportation_Expense','Work_Distance','Service_Time','Age','Average_Workload','Hit_Target','Weight','Height','BMI','Absent_Hours']\nrow = 5\ncol = 2\nr = 0\nc=0\ni=0\nfig,ax = plt.subplots(nrows=row,ncols=col)\nfig.set_size_inches(20,20)\n\nwhile r < row:\n    c =0\n    while c < col:\n        sn.boxplot(x=box[i], y=None, hue=None, data=emp_abntsm, order=None, hue_order=None, orient=None, color=None, palette=None,ax=ax[r,c])\n        c=c+1\n        i=i+1\n    r=r+1","1def30c7":"fig,ax = plt.subplots(nrows=1,ncols=1)\nfig.set_size_inches(10,8)\nsn.boxplot(x=emp_abntsm['Absence_Month'], y='Absent_Hours', hue=None, data=emp_abntsm, order=None, hue_order=None, orient=None, color=None, palette=None,ax=ax)","1614c885":"# what we can infer from above boxplots:\n# --> Target feature 'Absent_hours', has many outliers. It needs to be handled( will handle it after exploratory analysis)\n# -> Not many outliers in independent features. Data seems balanced.\n","b29ab293":"#---- 4b.) Correlation Analysis\n#--- Explore continous features\n#--- Explore categorical features","1e541e56":"#------------- Explore continous features -----------------\n##Explore the correlation btwn the independent continous features with target variabe\ncorr=emp_abntsm[continous_var].corrwith(emp_abntsm.Absent_Hours)\ncorr.plot.bar(figsize=(8,6), title='Correlation of features with the response variable Absent_Hours', grid=True, legend=False, style=None, fontsize=None, colormap=None, label=None)","cf426d72":"##------heatmap for correlation matrix---------##\n##to check multicollinearity ---##\n\ncorr = emp_abntsm[continous_var].corr()\n#correlation matrix\nsn.set(style='white')\n#compute correlation matrix\n#corr =bike.drop(columns=['cnt']).corr()\n#generate a mask for upper triangle#\nmask =np.zeros_like(corr, dtype=np.bool)\nmask[np.tril_indices_from(mask)]=True\n#setuop the matplotlab figure\nf,ax=plt.subplots(figsize=(10,10))\n#generate a custom diverging colormap\ncmap=sn.diverging_palette(220, 10, s=75, l=50, sep=10, n=6, center='light', as_cmap=True)\n#heatmap\nsn.heatmap(corr, vmin=None, vmax=None, cmap=cmap, center=0, robust=False, fmt='.2g', linewidths=0, linecolor='white', square=True, mask=mask, ax=None)\n#correlation matrix","fc7db495":"# This shows that there is multicollinearity in the dataset. BMI and Weight are highly correlated. 'Service_Time' and 'Age' are also correlated\n#Will have to deal with multi collinearity by removing few features from the dataset.","6cbaaf38":"#Visualize the relationship among all continous variables using pairplots\nNumericFeatureList=['Transportation_Expense','Work_Distance','Service_Time','Age','Average_Workload','Hit_Target','Weight','Height','BMI']\nsn.pairplot(emp_abntsm,vars=NumericFeatureList)","9ecdd287":"#Lets explore some more, the relationship btwn independent continous variables and dependent variable using JOINT PLOTs\n#graph individual numeric features by 'Absent_Hours'\nfor i in continous_var:\n    sn.jointplot(i, \"Absent_Hours\", data=emp_abntsm, kind='reg', color='g', size=4, ratio=2, space=0.2, dropna=True, xlim=None, ylim=None, joint_kws=None, marginal_kws=None, annot_kws=None)","3f693bdc":"#--- Checking the effect of 'Age' on 'Absence'\n#--- Aggregate data by 'Age' and by total hours of absence\nemp_hours = emp_abntsm[['ID','Absent_Hours']].groupby('ID').sum().reset_index()\nemp_age = emp_abntsm[['ID','Age']].groupby('ID').max().reset_index()\nabsence_by_age = emp_hours.merge(emp_age, how='inner',left_on='ID', right_on='ID')\n\nplt.scatter('Age', 'Absent_Hours', data=absence_by_age)","2f7d4756":"# Clearly, people over 40+ years of age tends to take less leaves compare to others","fd80d092":"#--- Checking the effect of 'Transportation_Expense' on 'Absence'\n#--- Aggregate data by 'Transportation_Expense' and by total hours of absence\n\nemp_transport = emp_abntsm[['ID','Transportation_Expense']].groupby('ID').max().reset_index()\nabsence_by_transport = emp_hours.merge(emp_transport, how='inner',left_on='ID', right_on='ID')\n\nplt.scatter('Transportation_Expense', 'Absent_Hours', data=absence_by_transport)","bf65f328":"# This clearly shows concentration of leaves more whre the Transportation_Expense is between 150-300","987f066e":"#--- Checking the effect of 'Work_Distance' on 'Absence'\n#--- Aggregate data by 'Work_Distance' and by total hours of absence\n\nemp_distance = emp_abntsm[['ID','Work_Distance']].groupby('ID').max().reset_index()\nabsence_by_distance = emp_hours.merge(emp_distance, how='inner',left_on='ID', right_on='ID')\n\nplt.scatter('Work_Distance', 'Absent_Hours', data=absence_by_distance)","4984b742":"# This clearly shows concentration of leaves more where the distance from work is between 10-30 km","7dfdd686":"#--- Checking the effect of 'Service_Time' on 'Absence'\n#--- Aggregate data by 'Service_Time' and by total hours of absence\n\nemp_service = emp_abntsm[['ID','Service_Time']].groupby('ID').max().reset_index()\nabsence_by_service = emp_hours.merge(emp_service, how='inner',left_on='ID', right_on='ID')\n\nplt.scatter('Service_Time', 'Absent_Hours', data=absence_by_service)","d02e25d5":"# Evident from above, employees with service years < 8 and >18 tends to take less leaves","6422108f":"# Checking the distribution of target feature\nsn.distplot(emp_abntsm['Absent_Hours']\/1000, bins=None, hist=True, kde=True, rug=False, fit=None, hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, color=None, vertical=False, norm_hist=False, axlabel=None, label=None, ax=None)","addbea98":"# what we can infer from above analysis of continous variables:\n# -> Target variable 'Absent_Hours' is not normally distributed, which is not a good thing. \n# -> We have to look in to this, before feeding the data to model.\n\n# -> 'Work_Distance','Age','Average_Workload' has good correlation with target feature 'Absent_Hours'.\n# -> Let's drop others from further analysis.\n\n# -> There is multi collinearity in dataset. 'Work_Distance' and 'Transportation_Expense' are correlated. \n# -> However, since p(Transportation_Expense) > p(Work_Distance), we'll drop Transportation_Expense from further analysis.","647b8dee":"#------------- Explore categorical features ------------------","3be84608":"##checking the pie chart distribution of categorical variables\nemp_piplot=emp_abntsm[categorical_var]\nplt.figure(figsize=(15,12))\nplt.suptitle('pie distribution of categorical features', fontsize=20)\nfor i in range(1,emp_piplot.shape[1]+1):\n    plt.subplot(4,3,i)\n    f=plt.gca()\n    f.set_title(emp_piplot.columns.values[i-1])\n    values=emp_piplot.iloc[:,i-1].value_counts(normalize=True).values\n    index=emp_piplot.iloc[:,i-1].value_counts(normalize=True).index\n    plt.pie(values,labels=index,autopct='%1.1f%%')\n#plt.tight_layout()","fbd0c762":"# --- Now let's analyze the absence by total hours of absence (not by frequency) ----","e090fad9":"#--- Checking for the reason of Absence---\n#checking the top reasons for absence as per the total numbers of absence\nemp_reason_tot_hours = emp_abntsm[['Absence_Reason','Absent_Hours']].groupby('Absence_Reason').sum().sort_values('Absent_Hours').reset_index()\nfig,ax = plt.subplots(nrows=1,ncols=1)\nfig.set_size_inches(10,5)\nsn.barplot(x='Absence_Reason', y='Absent_Hours', hue=None, data=emp_reason_tot_hours, order=None, hue_order=None, units=None, orient=None, color=None, palette=None,errcolor='.26', ax=ax)","cf44e742":"#Analyzing absence dependency of no of kids\nemp_son_tot = emp_abntsm[['Son','Absent_Hours']].groupby('Son').sum().sort_values('Absent_Hours').reset_index()\nfig,ax = plt.subplots(nrows=1,ncols=1)\nfig.set_size_inches(10,5)\nsn.barplot(x='Son', y='Absent_Hours', hue=None, data=emp_son_tot, order=None, hue_order=None, units=None, orient=None, color=None, palette=None,errcolor='.26', ax=ax)","80248b47":"# Clearly, employee with 3-4 kids tend to take less hours of absence","71741345":"#Analyzing absence dependency of month of year\n\n#--leaves by frquency \nemp_month_frequent = emp_abntsm[['Absence_Month','Absent_Hours']].groupby('Absence_Month').count().sort_values('Absent_Hours').reset_index()\n\n#--Leaves by total hours\nemp_month_tot = emp_abntsm[['Absence_Month','Absent_Hours']].groupby('Absence_Month').sum().sort_values('Absent_Hours').reset_index()\nfig,ax = plt.subplots(nrows=2,ncols=1)\nfig.set_size_inches(10,6)\nsn.barplot(x='Absence_Month', y='Absent_Hours', hue=None, data=emp_month_frequent, order=None, hue_order=None, units=None, orient=None, color=None, palette=None,errcolor='.26', ax=ax[0])\nsn.barplot(x='Absence_Month', y='Absent_Hours', hue=None, data=emp_month_tot, order=None, hue_order=None, units=None, orient=None, color=None, palette=None,errcolor='.26', ax=ax[1])","67fdc61d":"#------ Exploratory Data Analysis ENDS Here------------------\n# Final observations:\n#1.) \n#------------------------------------------------------------","e66f3586":"#----------------------Prepare data for modelling ------------------\n","92f6d481":"#---- Drop the features which are not very relevant based on above analyses\nemp_df  = emp_abntsm[['ID','Absence_Month','Son','Drinker','Work_Distance','Service_Time','Age','Average_Workload','Absent_Hours']]","a33aeb34":"#----Lets aggregate the data on 'Month' and 'Id'\nemp_num = emp_df[['ID','Absence_Month','Work_Distance','Service_Time','Age','Average_Workload']].groupby(['ID','Absence_Month']).max().reset_index()\nemp_tgt = emp_df[['ID','Absence_Month','Absent_Hours']].groupby(['ID','Absence_Month']).sum().reset_index()\nemp_cat = emp_abntsm[['ID','Absence_Month','Son','Drinker']].groupby(['ID','Absence_Month']).max().reset_index()\nemp = emp_num.merge(emp_cat, how='inner',left_on=['ID','Absence_Month'], right_on=['ID','Absence_Month']).merge(emp_tgt, how='inner',left_on=['ID','Absence_Month'], right_on=['ID','Absence_Month'])\nemp.head()","180513f6":"#--- Lets deal with Nans introduced(same way already done above, by imputing)\n\n#---- imputing Nan values with max each value present for a particular id. eg. Age will always be same for any id.\nfinal_col = ['Work_Distance','Service_Time','Age','Drinker','Son']\n#----impute missing values and Nas --------\nfor i in emp['ID'].unique(): \n    for j in final_col :\n        emp.loc[((emp['ID'] == i) & (emp[j].isna())), j] = emp[(emp.ID==i)][j].max()\n        \n# update workload with the mode of corresponding month's workload\nfor i in emp['Absence_Month'].unique(): \n    frequent_wrkld = stats.mode(emp[emp['Absence_Month']==i]['Average_Workload'])[0][0]\n    emp.loc[((emp['Absence_Month']==i) & pd.isna(emp['Average_Workload'])),'Average_Workload'] = frequent_wrkld\n\n#update NA 'Absent_Hours' with 0\nemp.Absent_Hours = emp.Absent_Hours.fillna(0)","f5ab0a84":"emp.head()","388686c1":"#----- Lets check for any outliers in the aggregated data -----\ncontinous_var = ['Work_Distance','Service_Time','Age','Average_Workload','Absent_Hours','Son']\nrow = 3\ncol = 2\nr = 0\nc=0\ni=0\nfig,ax = plt.subplots(nrows=row,ncols=col)\nfig.set_size_inches(20,20)\n\nwhile r < row:\n    c =0\n    while c < col:\n        sn.boxplot(x=continous_var[i], y=None, hue=None, data=emp, order=None, orient=None, ax=ax[r,c])\n        c=c+1\n        i=i+1\n    r=r+1","a84f264d":"# Clearly, 'Absent_Hours' has so many outliers, this will affect model. So, extreme outliers needs to be removed to make the model more generic.\n# We are not removing outliers in service time, since the input data for 2011 is going to be same as 2010(except 'Age' and 'ServiceTime')","d39c8c03":"#----- Create a function to remove outliers from any column, from any database\ndef remove_outlier(df_in, col_name):\n    q1 = np.percentile(df_in[col_name],25)\n    q3 = np.percentile(df_in[col_name],75)\n    iqr = q3-q1 #Interquartile range\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n    return df_out","2ddb359f":"#--- remove out liers\ncontinous_var = ['Service_Time','Age','Absent_Hours']\nfor i in continous_var:\n    emp = remove_outlier(emp,i)","692d9511":"# Check the distribution of target feature.\n#It seems better distributed then previous\nsn.distplot(emp['Absent_Hours']\/1000, bins=None, hist=True, kde=True)","9f493a44":"emp.head()","67fef7ac":"#--- As we can clearly see that the dataset has different features of differenr range\/scale.\n#--- Lets standardise the range\/scale for better performance of model\n#--We can use scikit-learn preprocessing library functions StandardScaler\/Normalizer for the same.\n# -- However, here I am using simple formula to standardize the scale\n# ---------->>value(new) = (value(max) - value) \/ (value(max) - value(min))","9f9bf243":"def Standardize_Values(df):\n    df_new = df\n    var = ['Work_Distance','Service_Time','Age','Average_Workload']\n    for i in var:\n        df_new[i] = (np.max(df_new[i]) - df_new[i]) \/ (np.max(df_new[i]) - np.min(df_new[i]))\n    return df_new","58abb684":"#--- Standardize the values ---\nemp_final = Standardize_Values(emp)\nemp_final.head()","fe690599":"#------------ Done preparing data for modelling -------------","aad998c2":"#----------Part 5 : Model Builing starts here ----------------------","111a8889":"# 1.) I am selecting 3 models to test and evaluate\n #   -> Linear Regression Model\n #   -> Random Forrest (ensemble method using bagging technique)\n #   -> Gradient Boosting (ensemble method using boosting technique)\n#2.) Cross validation    \n#3.) All these 3 models will be compared and evaluated\n#4.) We'll choose the best out of 3","00f8aa77":"#--- define a function which takes model, predicted and test values and returns evalution matrix: R-squared value,RootMeanSquared,MeanAbsoluteError\ndef model_eval_matrix(model,X_test,Y_test,Y_predict):\n    r_squared = model.score(X_test, Y_test)\n    mse = mean_squared_error(Y_predict, Y_test)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(Y_predict, Y_test)\n    return r_squared,mse,rmse, mae","79c8cbec":"#train,test = train_test_split(emp_final, test_size=0.20, random_state=1)\ntrain = emp_final[:80]\ntest = emp_final[20:]\nX_train = train.drop(columns = ['Absent_Hours','ID'])\n#Y_train = np.log(train.Absent_Hours)\nY_train = train.Absent_Hours\/1000\nX_test = test.drop(columns = ['Absent_Hours','ID'])\n#Y_test = np.log(test.Absent_Hours)\nY_test = test.Absent_Hours\/1000","33486185":"#--Define Linear regession model --\nlrm_regressor = LinearRegression()\nlrm_regressor.fit(X_train, Y_train)\nY_predict_lrm =lrm_regressor.predict(X_test)","0c3917fd":"#------- Random Forest Model (Ensemble method using Bagging technique) --------------\nforest_reg = RandomForestRegressor(n_estimators=2000, criterion='mse', max_depth=10, min_samples_split=5, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=20, min_impurity_decrease=0.00, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=1, verbose=0, warm_start=False)\nforest_reg.fit(X_train, Y_train)\nY_predict_forest =forest_reg.predict(X_test)","c2c35d9b":"## ----------- Building XGBoost Model (Ensemble method using Boosting technique) ---------------\n#xgb_reg = GradientBoostingRegressor(random_state=1) # without parameter hypertuning\n# Following model is with parameter hypertuning\nxgb_reg = GradientBoostingRegressor(loss='ls', learning_rate=0.2, n_estimators=2000, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=1, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=15, warm_start=False, presort='auto')\nxgb_reg.fit(X_train, Y_train)\nY_predict_xgb = xgb_reg.predict(X_test)","68e63ce9":"#---Stroring all model performances in dataframe to compare----\nmetric=[]\nml_models=['Linear Reg','Random Forest','GradientBoost']\nfitted_models= [lrm_regressor,forest_reg,xgb_reg]\nY_Predict =[Y_predict_lrm,Y_predict_forest,Y_predict_xgb]\ni=0\nfor mod in ml_models:\n    R_SQR,MSE,RMSE,MAE = model_eval_matrix(fitted_models[i],X_test,Y_test,Y_Predict[i])\n    metric.append([mod,R_SQR,MSE,RMSE,MAE])\n    i=i+1\ndf_mod_performance=pd.DataFrame(metric,columns =['Model','R-Squared','MeanSquaredError','RootMeanSquaredError','MeanAbsoluteError'])\ndf_mod_performance[['Model','RootMeanSquaredError']]","6bb43465":"#Clearly, Random Forest proves to be best model here -----\n# We'll use Random forest as our final model to predict 2011 losses due to absence\n# FINAL MODE :: RANDOM FOREST","10649f6c":"absence_prediction=X_test\nabsence_prediction['Absent_Hours'] = 1000*Y_test\nabsence_prediction['Predicted_Absent_Hours'] = 1000*Y_predict_forest\n#final_bike_prediction_df['Predicted_Absent_Hours'] = round(final_bike_prediction_df['Predicted_Absent_Hours'])\n#--- Sample output(with actual counts and predicted counts) ---\nabsence_prediction","48b76d25":"#Predicted absence hours of 2010\nabsence_prediction.Predicted_Absent_Hours.sum()","580bda68":"#Actual absence hours of 2010\nabsence_prediction.Absent_Hours.sum()","fb3b9a42":"#--- Predicted absence hours per month \nabsence_prediction.groupby('Absence_Month').sum().reset_index()[['Absence_Month','Absent_Hours','Predicted_Absent_Hours']]","d475e51e":"#--------data fo 2011\n#--- service and age will be added by 1\n\nemp_2011 = emp\nemp_2011.Service_Time = emp.Service_Time + 1\nemp_2011.Age = emp.Age + 1","305f7d71":"emp_2011= emp_2011.drop(columns = ['Absent_Hours','ID'])","25bfc8ee":"#-------- Standardise the scale, before passing the input to model\nemp_2011 = Standardize_Values(emp_2011)","0fc1bf98":"predict_2011_absence =forest_reg.predict(emp_2011)","96df610c":"absence_prediction_2011=emp_2011\nabsence_prediction_2011['Predicted_Absent_Hours'] = predict_2011_absence*1000\n\nabsence_prediction_2011","d2d9d673":"monthly_absence= absence_prediction_2011.groupby('Absence_Month').sum().reset_index()[['Absence_Month','Predicted_Absent_Hours']]\nmonthly_absence","588298d3":"#lets say in a month excluding weekend 22 days are working days. total working hours of 36 employees will be 22*8*36.\n# total losses % = (absent_hours \/ Total_Hours)*100\ntot_Monthly_hours = 22*8*36\nmonthly_absence['monthly_loss_percentage'] = (monthly_absence['Predicted_Absent_Hours']\/tot_Monthly_hours) * 100","bbaa8c26":"#---------MONTHLY LOSSES PREDICTED FOR YEAR 2011 PER MONTH -----------","86646de2":"monthly_absence","e4903797":"#---------------------------------------------------------------------","a87ae24d":"# Hereby, concluding the project with above predictions ","39d1182f":"#---- Longest hours of absences for reason 13,19,23,28\n#--------> #23 - medical consultation (23),\n#--------> #24 - blood donation (24),  \n#--------> #27- physiotherapy (27), \n#--------> #28 - dental consultation (28)\n#--------> #13 - Diseases of the musculoskeletal system and connective tissue \n#--------> #19 - Injury, poisoning and certain other consequences of external causes","55385ebf":"#### Overall, \n#---> Seems like employee takes most absences for medical consulations\/dental consultation\/physiotherapy.\n#---> these hours can be rduced by setting up a medical consultation\/dental consultation\/physiotherapy booth(with visiting doctors may be) at office\/facility\n#---> In long term, introducing exercise\/yoga sessions in office once\/twice a week will help reduce physiotherapy issues","31aa6291":"# This python code is for project : Employee Absenteeism\n\n#### Problem Statement: XYZ is a courier company. As we appreciate that human capital plays an important role in collection, transportation and delivery. The company is passing through genuine issue of Absenteeism. The company has shared it dataset and requested to have an answer on the following areas: \n\n##### 1. What changes company should bring to reduce the number of absenteeism?  \n##### 2. How much losses every month can we project in 2011 if same trend of absenteeism continues","be296bad":"To prepare data for 2011,assuming that all the employees are retained in 2011 and all other condition remains and same trends continues, we need to add +1 to 'Service_Time' and 'Age'(keeping all other features same)","90f0c9aa":"### These pie distributions are based on the frequency of the 'leaves' taken , not on the tot no. of leaves taken.\n\n#What we can infer from above piplot:\n\n#-> From 'Reason' distribution, we can see that most frequent leaves are taken for the reason 23,28,27\n#--------> #23 - medical consultation (23),\n#--------> #28 - dental consultation (28)\n#--------> #27- physiotherapy (27), \n#--------> #13 - Diseases of the musculoskeletal system and connective tissue \n#--------> #19 - Injury, poisoning and certain other consequences of external causes\n#--------> #10 - Diseases of the respiratory system\n\n#->From, 'Month' distribution, we can see that frquency of leaves are more or less uniformally distributed over months, with highest no. of leaves taken in March, Feb and July(holiday season)\n\n#->From, 'Education' distribution, we can see that frquency of leaves are highest for education = 1(highschool)\n\n#->From, 'Weekday' distribution, we can see that frquency of leaves are mostly distributed, with most frequent leaves on 'Monday', which makes sense as most people travel\/party over weekend and the mood spills over to Monday :)\n\n#-> From, 'Son' and 'Pet', we can see that people having no kids and no pets(no family responsibilities) tend to take frequent leaves.\n\n#-> 'Social Drinker' takes little more leaves than non drinker.\n","3fc4a8a4":"#--> Clearly, March tops the month for most absences. This makes sense as this is peak holiday season due to change of weather and clear and sunny sky\n#--> Second one is July, which again is the 'holiday' season ","8f43672f":"#---- Now, since we need to predict the losses per month, Lets aggregate the data on month(and ID, since the features category is different for each ID) before feeding the data to model.","ec631326":"Since, random forest model is our final model to be used for prediction, We'll use this model to predict the losses of 2011.\nLet's prepare data for 2011","0c0967b9":"I am going to divide whole project in to 8 parts:\n1.) Define and categorize problem statement\n2.) Gather the data\n3.) Prepare data for consumption\n4.) Perform Exploratory Data Analysis\n5.) Models Building\n6.) Evaluate and compare Model performances and choose the best model\n7.) Hypertune the selected model\n8.) Produce sample output with tuned model"}}