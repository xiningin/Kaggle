{"cell_type":{"fe5c4429":"code","454d7b0d":"code","1792e47e":"code","f74d91f0":"code","e6ab9d87":"code","c4ba85c0":"code","cf465a2d":"code","50042198":"code","50528434":"code","36a08430":"code","512f6855":"code","faeda97c":"code","0ea33d2c":"code","c1906999":"code","924d3093":"code","d039e67b":"code","b917e2ac":"code","f41ec37f":"code","5db55da5":"code","181ac898":"code","23b0782a":"code","56265b50":"code","e42a09c5":"code","5bd6c5ef":"code","9c83fa32":"code","09efc624":"code","94478970":"code","ef0d5ee9":"code","ffc3549f":"code","54a07c5f":"code","e76f057f":"code","d2d7aa0c":"code","d6124765":"code","687e9304":"code","37670fd9":"code","897878ee":"code","ccbb37da":"code","54322bfe":"code","f2bea0c1":"code","7d62ac92":"code","9f3f413f":"code","9da4d72f":"code","44323bfc":"code","e5a46f1f":"code","485d1f52":"code","032e47ab":"code","4919ca2f":"code","1cda0025":"code","82665e46":"code","d805ba02":"code","6b3fe3c5":"code","5920fbba":"code","94364267":"code","b02b1d3f":"code","825bc554":"markdown","61d259f5":"markdown","1cd3fa45":"markdown","7a0b063d":"markdown","093b9b05":"markdown","f2b2e7a0":"markdown","d4dec7c9":"markdown","edb5561e":"markdown","9c40f621":"markdown","c663fec4":"markdown","bf903386":"markdown","efbf7105":"markdown","25b9becf":"markdown","d5df1695":"markdown","c9a65963":"markdown","c1538be8":"markdown","7e0939b9":"markdown","01a513c6":"markdown","9ceeaac1":"markdown","21157026":"markdown","f592ba8c":"markdown","f16c59d3":"markdown","6b12a924":"markdown","9b7abb96":"markdown","7454ddfb":"markdown","1b8bfd41":"markdown","cffebb1a":"markdown","f69e8270":"markdown","88c17973":"markdown","d981986b":"markdown","7b5e200e":"markdown","115dd322":"markdown","1d5fcbae":"markdown","6fac76b1":"markdown","0d676380":"markdown","de75303b":"markdown","68638857":"markdown","a70c2e02":"markdown"},"source":{"fe5c4429":"import numpy as np \nimport pandas as pd\nimport os\nimport re\nimport scipy.stats\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# %matplotlib inline\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\n\nSEED = 7\n\nprint(\"Setup complete.\")","454d7b0d":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndatasets = [train, test]\n\ntrain","1792e47e":"train.info()","f74d91f0":"test.info()","e6ab9d87":"sns.countplot(data=train, x='Pclass', hue='Survived')\nplt.show() # To remove extra message shown on screen","c4ba85c0":"def get_title(name):\n    reg_exp = re.search(' ([A-Za-z]+)\\.', name)\n    return reg_exp.group(1) if reg_exp else \"\"\n\n# Number of different titles\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), sharex=True)\n\nsns.countplot(data=train, y=train['Name'].apply(get_title), ax=ax[0])\nax[0].set(title='Train', ylabel='')\n\nsns.countplot(data=test, y=test['Name'].apply(get_title), ax=ax[1])\nax[1].set(title='Test', ylabel='')\nplt.show()","cf465a2d":"for ds in datasets:\n    # Add Title column\n    ds['Title'] = ds['Name'].apply(get_title)\n    # Replacement\n    ds['Title'] = \\\n        ds['Title'].replace(['Mlle', 'Ms'], 'Miss').replace('Mme', 'Mrs')\n    rare = ['Capt', 'Col', 'Countess', 'Don', 'Dona', 'Dr', 'Jonkheer', \n            'Lady', 'Major', 'Rev', 'Sir']\n    ds['Title'] = ds['Title'].replace(rare, 'Rare')\n\n# Number of different titles\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), sharex=True)\n\nsns.countplot(data=train, y='Title', ax=ax[0])\nax[0].set(title='Train', ylabel='')\n\nsns.countplot(data=test, y='Title', ax=ax[1])\nax[1].set(title='Test', ylabel='')\nplt.show()","50042198":"sns.countplot(data=train, x='Sex', hue='Survived')\nplt.show()","50528434":"for ds in datasets:\n    # Imputate (fill missing values)\n    def rand_ages():\n        np.random.seed(SEED)\n        return np.random.randint(low=ds['Age'].mean() - ds['Age'].std(),\n                                 high=ds['Age'].mean() + ds['Age'].std(),\n                                 size=ds['Age'].isnull().sum())\n    ds.loc[ds['Age'].isnull(), 'Age'] = rand_ages()\n\n    # Categorise\n    ds['Age'] = pd.cut(ds['Age'], 5, labels=range(5))\n    ds.loc[:, 'Age'] = ds['Age'].astype(int)\n\n# Plot\nax = sns.countplot(data=train, x='Age', hue='Survived')\nax.set_xlabel('Age category')\nplt.show()","36a08430":"for ds in datasets:\n    # Family size is based on number of siblings\/spouses and parent\/children\n    family_size = ds['SibSp'] + ds['Parch'] + 1\n    \n    # Categorise\n    ds['FamilySize'] = pd.cut(\n        family_size, bins=[-np.inf, 1, 2, 4, np.inf], labels=range(4))\n    ds.loc[:, 'FamilySize'] = ds['FamilySize'].astype(int)\n\n# Plot\nsns.countplot(data=train, x='FamilySize', hue='Survived')\nplt.show()","512f6855":"median = train['Fare'].median()\nfor ds in datasets:\n    #Impute\n    ds['Fare'] = ds['Fare'].fillna(median)\n    \n    # Categorise\n    ds['Fare'] = pd.qcut(ds['Fare'], q=4, labels=range(4))\n    ds.loc[:, 'Fare'] = ds['Fare'].astype(int)\n\n# Plot\nsns.countplot(data=train, x='Fare', hue='Survived')\nplt.show()","faeda97c":"for ds in datasets:\n    ds['HasCabin'] = ds['Cabin'].notnull().astype(int)\n\n# Plot\nsns.countplot(data=train, x='HasCabin', hue='Survived')\nplt.show()","0ea33d2c":"# Count of values\nfig, ax = plt.subplots(1, 2, sharey=True)\n\nsns.countplot(data=train, x='Embarked', ax=ax[0])\nax[0].set_title('Train')\n\nsns.countplot(data=test, x='Embarked', ax=ax[1], order=['S', 'C', 'Q'])\nax[1].set_title('Test')\nplt.show()","c1906999":"# Impute\nfor ds in datasets:\n    ds['Embarked'] = ds['Embarked'].fillna('S')\n\n# Plot\nsns.countplot(data=train, x='Embarked', hue='Survived')\nplt.show()","924d3093":"train.head()","d039e67b":"# Encoding\ndef encode_freq_sorted(feature):\n    sorted_indices = feature.value_counts().index\n    sorted_dict = dict(zip(sorted_indices, range(len(sorted_indices))))\n    return feature.map(sorted_dict).astype(int)\n    \nfor ds in datasets:\n    ds['Sex'] = encode_freq_sorted(ds['Sex']) # Sex\n    ds['Embarked'] = encode_freq_sorted(ds['Embarked']) # Embarked\n    ds['Title'] = encode_freq_sorted(ds['Title']) # Title\n\ntrain.head()","b917e2ac":"drop_features = ['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin']","f41ec37f":"correlation = train.drop(columns=drop_features).corr()\n\nplt.figure(figsize=(11, 9))\nsns.heatmap(correlation, annot=True)\nplt.show()","5db55da5":"drop_features.extend(['Pclass'])\n\ntrain = train.drop(columns=drop_features)\ntest = test.drop(columns=drop_features)\n\nX = train.drop(columns=['PassengerId', 'Survived'])\ny = train['Survived']\n\nX.head()","181ac898":"# Split training and validation data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, \n                                                  random_state=SEED)\nX_test = test.drop(columns=['PassengerId']) # For consistency in naming variables","23b0782a":"# Cross-validator\nfrom sklearn.model_selection import StratifiedKFold\n\ncross_valid = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n\n# Define randomized search as a function for later use\ndef random_search(X, y, estimator, params, score=\"accuracy\", cv=cross_valid, \n                  n_iter=100, random_state=SEED, n_jobs=-1):\n    \"\"\"\n    Randomized search of parameters, using \"cv\" fold cross validation, search \n    across \"n_iter\" different combinations, and use all available cores\n    \"\"\"\n    print(\"# Tuning hyper-parameters for {} by randomized search\".format(score))\n    \n    classifier = RandomizedSearchCV(estimator=estimator, param_distributions=params, \n                             scoring=score, cv=cv, n_iter=n_iter, n_jobs=n_jobs, \n                             random_state=random_state)\n    classifier.fit(X, y)\n    \n    print(\"Best parameters by random search:\\n\", classifier.best_params_)\n    return classifier","56265b50":"from sklearn.ensemble import RandomForestClassifier\n\n# By setting random_state, we get the same result every time we run the command\nrandom_forest = RandomForestClassifier(random_state=SEED)\n\nrandom_forest.get_params()","e42a09c5":"# Hyper-parameter tuning\n# Note: running this cell takes a few minutes\n\n# Create the parameter grid\n# The keys are ordered alphabetically\nparams = {\n    # Method of selecting samples for training each tree\n    'bootstrap': [True, False],\n    # Maximum number of levels in tree\n    'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n    # Number of features to consider at every split\n    'max_features': ['auto', 'sqrt'],\n    # Minimum number of samples required at each leaf node\n    'min_samples_leaf': [1, 2, 4],\n    # Minimum number of samples required to split a node\n    'min_samples_split': [2, 5, 10],\n    # Number of trees in random forest\n    'n_estimators': [int(x) for x in np.linspace(200, 2000, num = 10)]\n}\n\n# Apply randomized search cross validation\nrandom_forest_tuned = random_search(\n    X_train, y_train, estimator=random_forest, params=params)","5bd6c5ef":"# Define a function to plot learning curve for later use\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator, X, y, ylim=None, cv=None, n_jobs=-1,\n                        train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Plot the test and training learning curves.\n    \"\"\"\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, scoring='accuracy', cv=cv, n_jobs=n_jobs, \n                       train_sizes=train_sizes, return_times=True)\n    \n    # Plot learning curve\n    fig, ax = plt.subplots()\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", \n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.grid()\n    plt.legend(loc=\"best\")\n    if ylim is not None:\n        plt.ylim(*ylim)\n    ax.set(title=\"Learning curves\", \n           xlabel=\"Training examples\", ylabel=\"Score\")\n \n    return plt","9c83fa32":"# Learning curve\nplot_learning_curve(random_forest_tuned.best_estimator_, \n                    X, y, cv=cross_valid)\nplt.show()","09efc624":"# Prediction on validation data\ny_pred = random_forest_tuned.predict(X_val)\n\n# Accuracy of prediction\naccuracy_random_forest = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_random_forest)","94478970":"# Default params of an svm\nfrom sklearn.svm import SVC\n\nsvc = SVC(probability=True, random_state=SEED)\n\nsvc.get_params()","ef0d5ee9":"# Hyper-parameter tuning\nparams = {\n    'C': scipy.stats.expon(scale=100),\n    'class_weight':['balanced', None],\n    'gamma': scipy.stats.expon(scale=.1),\n    'kernel':['rbf', 'linear']\n}\n\nsvc_tuned = random_search(X_train, y_train, estimator=svc, params=params)","ffc3549f":"# Learning curve\nplot_learning_curve(svc_tuned.best_estimator_, X, y, cv=cross_valid)\nplt.show()","54a07c5f":"# Prediction on vlaidation data\ny_pred = svc_tuned.predict(X_val)\n\n# Accuracy\naccuracy_svc = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_svc)","e76f057f":"# Default params of an svm\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier(random_state=SEED, verbosity=0)\n\nxgb.get_params()","d2d7aa0c":"# Hyper-parameter tuning\nparams = {\n    'colsample_bytree': list(np.arange(0.6, 1.0, step=0.05)),\n    'gamma': list(np.arange(0.1, 15, step=0.2)),\n    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n    'max_depth': list(range(2, 12)),\n    'min_child_weight': list(range(1, 12)),\n    'n_estimators': [10, 100, 500, 1000],\n    'reg_alpha': [10**i for i in range(-5, 1)],\n    'subsample': list(np.arange(0.6, 1.0, step=0.05))\n}\n\nxgb_tuned = random_search(X_train, y_train, estimator=xgb, params=params)","d6124765":"# Learning curve\nplot_learning_curve(xgb_tuned.best_estimator_, X, y, cv=cross_valid)\nplt.show()","687e9304":"# Prediction on vlaidation data\ny_pred = xgb_tuned.predict(X_val)\n\n# Accuracy\naccuracy_xgboost = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_xgboost)","37670fd9":"# Default params of an svm\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(random_state=SEED)\n\ndecision_tree.get_params()","897878ee":"# Hyper-parameter tuning\nparams = {\n    'criterion': [\"gini\", \"entropy\"],\n    'max_depth': list(range(1, 32)),\n    'max_features': list(range(1, X_train.shape[1]+1)),\n    'min_samples_leaf': list(range(1, 9)),\n    'min_samples_split': list(np.arange(0.1, 1.1, step=0.1))\n}\n\ndecision_tree_tuned = random_search(\n    X_train, y_train, estimator=decision_tree, params=params)","ccbb37da":"# Learning curve\nplot_learning_curve(decision_tree_tuned.best_estimator_, X, y, cv=cross_valid)\nplt.show()","54322bfe":"# Prediction on vlaidation data\ny_pred = decision_tree_tuned.predict(X_val)\n\n# Accuracy\naccuracy_decision_tree = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_decision_tree)","f2bea0c1":"# Default params\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\n\nknn.get_params()","7d62ac92":"# Hyper-parameter tuning\n# Note: running this cell takes a few minutes\nparams = {\n    'leaf_size': list(range(20, 50)),\n    'n_neighbors': list(range(3, 30)),\n    'p': [1, 2]\n}\n\nknn_tuned = random_search(X_train, y_train, estimator=knn, params=params)","9f3f413f":"# Learning curve\nplot_learning_curve(knn_tuned.best_estimator_, X, y, cv=cross_valid)\nplt.show()","9da4d72f":"# Prediction on vlaidation data\ny_pred = knn_tuned.predict(X_val)\n\n# Accuracy\naccuracy_knn = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_knn)","44323bfc":"# Default params\nfrom sklearn.linear_model import LogisticRegression\n\nlogistic_regression = LogisticRegression(random_state=SEED)\n\nlogistic_regression.get_params()","e5a46f1f":"# Hyper-parameter tuning\nparams = {\n    'C': scipy.stats.loguniform(1e-5, 100),\n    'penalty': ['l1', 'l2', 'elasticnet'],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear']\n}\n\nlogistic_regression_tuned = random_search(\n    X_train, y_train, estimator=logistic_regression, params=params)","485d1f52":"# Learning curve\nplot_learning_curve(logistic_regression_tuned.best_estimator_, \n                    X, y, cv=cross_valid)\nplt.show()","032e47ab":"# Prediction on vlaidation data\ny_pred = logistic_regression_tuned.predict(X_val)\n\n# Accuracy\naccuracy_logistic_regression = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_logistic_regression)","4919ca2f":"# Default params\nfrom sklearn.naive_bayes import GaussianNB\n\nnaive_bayes = GaussianNB()\n\nnaive_bayes.get_params()","1cda0025":"# Hyper-parameter tuning\nparams = {\n    'var_smoothing': [np.exp(-i) for i in range(1, 15)]\n}\n\nnaive_bayes_tuned = random_search(\n    X_train, y_train, estimator=naive_bayes, params=params, n_iter=15-1)","82665e46":"# Learning curve\nplot_learning_curve(naive_bayes_tuned.best_estimator_, \n                    X, y, cv=cross_valid)\nplt.show()","d805ba02":"# Prediction on vlaidation data\ny_pred = naive_bayes_tuned.predict(X_val)\n\n# Accuracy\naccuracy_naive_bayes = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_naive_bayes)","6b3fe3c5":"accuracy_df = pd.DataFrame({\n    'Model': ['Random forest', 'SVM', 'XGBoost', 'Decision Tree', 'KNN', \n              'Logistic regression', 'Naive Bayes'],\n    'Accuracy': [accuracy_random_forest, accuracy_svc, accuracy_xgboost,\n                 accuracy_decision_tree, accuracy_knn,\n                 accuracy_logistic_regression, accuracy_naive_bayes]\n})\n\n# Sort\naccuracy_df_sorted = accuracy_df.sort_values(by='Accuracy', ascending=False)\n\n# Plot\ng = sns.barplot(data=accuracy_df_sorted, y='Model', x='Accuracy')\ng.set(ylabel='', xlim=(0.7, 0.85))\nplt.show()","5920fbba":"from sklearn.ensemble import VotingClassifier\n\nvoting = VotingClassifier(\n    estimators=[('rf', random_forest_tuned),\n                ('xgb', xgb_tuned),\n                ('knn', knn_tuned),\n                ('svc', svc_tuned),\n                ('lr', logistic_regression_tuned),\n                ('dt', decision_tree_tuned),\n                ('nb', naive_bayes_tuned)],\n    voting='soft', \n    n_jobs=-1)\n\nvoting = voting.fit(X_train, y_train)","94364267":"# Prediction on vlaidation data\ny_pred = voting.predict(X_val)\n\n# Accuracy\naccuracy_voting = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_voting)","b02b1d3f":"# The best model\nmodel = voting\n\n# Prediction\npredictions = model.predict(X_test)\n\n# Save the results\noutput = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"The results successfully saved!\")","825bc554":"Now, we try different models, for which we use randomized search to optimize hyper-parameters based on prediction accuracy. The randomized search randomly samples from a grid of paramaters and performs K-fold cross validation with each combination of values.","61d259f5":"## Logistic regression\nThe logistic model is used to model the probability of a certain class or event existing such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc [[link](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression)].\n\nThe material is borrowed from [here](https:\/\/machinelearningmastery.com\/hyperparameter-optimization-with-random-search-and-grid-search\/).","1cd3fa45":"### 4. Sex\nThere is no NaN in this column.","7a0b063d":"It's seen that the more the fare, the more the number of survived passengers.","093b9b05":"### 9. Fare\nTo impute, we use median of training data. Then, we categorise it into 4 categories.","f2b2e7a0":"### 8. Ticket\nThe ticket number doesn't affect the survival.","d4dec7c9":"## Random forest\nRandom forest is an ensemble learning method for classification that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean\/average prediction of the individual trees [[link](https:\/\/en.wikipedia.org\/wiki\/Random_forest#:~:text=Random%20forests%20or%20random%20decision,average%20prediction%20(regression)%20of%20the)].\n\nFirst, we define the model in the simplest form to see what the default parameters are.\n\nThe material is inspired by [Hyperparameter Tuning the Random Forest in Python](http:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74).","edb5561e":"We see that \"S\" is the most frequent port of embarkation.","9c40f621":"Now that the data cleaning, feature engineering, data encoding and feature selection is done, the dataset is up and ready to be fed to machine learning models.\n\n# Classification\nHere we apply different machine learning techniques to model the training data and later predicting who survived the shipwreck.\n\nWe don't have access to the actual survived values for the test data, since this the competition's rule, until competitors don't overfit the test data. Therefore, we split the training data into training and validation, and use validation data to evaluate our model. ","c663fec4":"Now that the best parameters are found, the model can be tested on validation data.","bf903386":"# Feature selection\nAt this step, we drop the features that wouldn't affect (much) the target.","efbf7105":"### 1. Survived\nThis is the target column.","25b9becf":"### 7. Parch\nThis is the number of parents\/children. We can consider it along with \"SibSp\" to make a new feature, \"FamilySize\", that can be categorized into \n 4 categories comprising 1, 2, {3, 4} and >= 5 members.","d5df1695":"### 10. Cabin\nMost values of this column are NaN, meaning that most passengers didn't have cabins. We add a feature to show if a passenger had cabin or not.","c9a65963":"This is because we used \n* \"Name\" -> \"Title\", \n* \"SibSp\" and \"Parch\" -> \"FamilySize\", \n* \"Ticket\" -> drop,\n* \"Cabin\" -> \"HasCabin\".\n\nTo explore the data, we can visualize the correlation between features. The correlation coefficient shows the linear relation between features. Strong correlation between two features, which is shown by values close to +1 or -1, means there is redundancy in data, therefore one of the features can be dropped.","c1538be8":"As can be seen, there is a (rather) strong correlation between \"HasCabin\" and \"Pclass\", because the values are close to -1. We drop one of them to remove redundancy.\n\nThe values of \"HasCabin\" are closer to 0 for different features, which means it is less correlated to other features rather than \"Pclass\"; so, we drop \"Pclass\".","7e0939b9":"### Learning curve\na learning curve (or training curve) plots the optimal value of a model's loss function for a training set against this loss function evaluated on a validation data set with same parameters as produced the optimal function. It is a tool to find out how much a machine model benefits from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, it will not benefit much from more training data [[link](https:\/\/en.wikipedia.org\/wiki\/Learning_curve_(machine_learning))].","01a513c6":"## Decision Tree\nA decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes) [[link](https:\/\/en.wikipedia.org\/wiki\/Decision_tree#:~:text=A%20decision%20tree%20is%20a%20flowchart%2Dlike%20structure%20in%20which,taken%20after%20computing%20all%20attributes)].","9ceeaac1":"# Feature engineering","21157026":"Note that if we didn't care about the frequency of values, we could use \"sklearn.preprocessing.LabelEncoder\". At the moment, that library doesn't support arbitrary ordering of values of the features.","f592ba8c":"## Naive Bayes\nNaive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong independence assumptions between the features [[link](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier)].","f16c59d3":"Here, we encode features based on frequency of occurance of their values, i.e. the most frequent value in each feature is encoded to 0, the second most frequent to 1 and so on.","6b12a924":"# Results\nThe final results are saved in \"my_submission.csv\".","9b7abb96":"# Data encoding\nLooking into the datasets, there are categorical features, that need to be converted to numberical ones in order for modelling. This is called \"encoding\".","7454ddfb":"The features are listed as above.\n\n### 0. PassengerId\nThis column includes IDs and doesn't affrect the survival.","1b8bfd41":"### 6. SibSp\nThis is the number of siblings\/spouses aboard the Titanic and can be considered along with \"Parch\" feature.","cffebb1a":"Some titles can be replaced by another one, e.g. 'Mlle'=Mademoiselle (French) with 'Miss'. Also, most titles are rare, e.g. 'Col' that is used only for 2 passengers, so it'd be helpful to replace them with 'Rare'.","f69e8270":"### 2. Pclass\n\nThis is the ticket class, with the numeric values 1=1st, 2=2nd, 3=3rd and no NaN.","88c17973":"## Conclusion\nThe following is the result of running different models on the Titanic dataset.","d981986b":"## Ensemble modelling\n","7b5e200e":"### 5. Age\nIn this column, there is plenty of missing values. To impute, we can use random numbers in the range of mean - std ($\\mu - \\sigma$) and mean + std ($\\mu + \\sigma$). Statistically speaking, 68% of data is in this range.\n\nThen, we categorise \"Age\" into 5 categories.","115dd322":"### 11. Embarked\nThis column has missing values. We impute them by the most frequent value.","1d5fcbae":"# Introduction\nThis notebook explores the Titanic dataset, using Python language, with the aim of predicting which passengers survived the shipwreck.\n\nThe material is inspired by [Titanic best working Classifier](https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier\/comments) and [Introduction to Ensembling\/Stacking in Python](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python).","6fac76b1":"Here, we check each and every feature.\n\nRunning the following commands, we see there are 12 columns in the training data, 3 of which having NaN -- \"Age\", \"Cabin\" and \"Embarked\". Also, the 3 columns of \"Age\", \"Fare\" and \"Cabin\" in the test dataset include missing values. We'll take care of these columns later.","0d676380":"## Support vector machine (SVM)\nA support vector machine is a supervised machine learning model that uses classification algorithms for two-group classification problems.","de75303b":"## XGBoost\nXGBoost is an open-source software library that provides a gradient boosting framework.","68638857":"## K-Nearest Neighbors\nThe k-nearest neighbors algorithm (k-NN) is a non-parametric method used that assumes similar things exist in close proximity.","a70c2e02":"### 3. Name\nEach name in the dataset is associated with a title. Instead of 'Name' that is observed once, probably, for each passenger, we can use a common feature, title, to group passengers."}}