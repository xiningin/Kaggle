{"cell_type":{"051a90ba":"code","241261fb":"code","2613d2c2":"code","83010850":"code","5e015347":"code","b7998f71":"code","6acb931a":"code","eccb5516":"code","c7b3bffb":"code","e8681ade":"code","ee97580a":"code","68e0f16a":"markdown","5ee14083":"markdown","a38a967c":"markdown","962017be":"markdown","65cabbcf":"markdown","f9ff5d80":"markdown"},"source":{"051a90ba":"import numpy as np \nimport matplotlib.pyplot as plt \nimport cv2\nimport glob\n\n# This is a bit of magic to make matplotlib figures appear inline in the notebook\n# rather than in a new window.\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (19.0, 17.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n# Some more magic so that the notebook will reload external python modules;\n# see http:\/\/stackoverflow.com\/questions\/1907993\/autoreload-of-modules-in-ipython\n%load_ext autoreload\n%autoreload 2","241261fb":"data_dir = r'\/kaggle\/input\/dataset\/dataset\/'\nclasses = ['broadleaf', 'grass', 'soil', 'soybean'] \n\nnum_file = 1100 \nall_files = [] \nnum_data =num_file*len(classes)\nY = np.zeros(num_data)\n\n\nfor i, cls in enumerate(classes):\n    all_files += [f for f in glob.glob(data_dir+cls+'\/*.tif')][:num_file]\n    Y[i*num_file:(i+1)*num_file] = i # label all classes with int [0.. len(classes)]\n\n    \n# Image dimension\nim_width = 230\nim_height = 230 \nim_channel = 3\ndim = im_width * im_height * im_channel\n\nX = np.ndarray(shape=(num_data, im_width, im_height, im_channel), dtype=np.uint8)\n\nfor idx, file in enumerate(all_files):\n    X[idx] = cv2.resize(cv2.imread(file), (im_width, im_height))\n\nX_train = np.empty(shape=(4000,im_width, im_height, im_channel), dtype=np.uint8)\nX_val = np.empty(shape=(200,im_width, im_height, im_channel), dtype=np.uint8)\nX_test = np.empty(shape=(200,im_width, im_height, im_channel), dtype=np.uint8)\n\ny_train = np.empty(4000)\ny_val = np.empty(200)\ny_test = np.empty(200) \n\nfor i, cls in enumerate(classes): \n    X_test[50*i:50*(i+1)] = X[np.where(Y == i)[0][:50]]\n    X_val[50*i:50*(i+1)] = X[np.where(Y == i)[0][50:100]]\n    X_train[1000*i:1000*(i+1)] = X[np.where(Y == i)[0][100:]]\n    \n    y_test[50*i:50*(i+1)] = i\n    y_val[50*i:50*(i+1)] = i\n    y_train[1000*i:1000*(i+1)] = i\n    \ndel Y \ndel X\n\n# Extract features \n#Shuffle training index\ntrain_idxs = np.random.permutation(X_train.shape[0])\ny_train  = y_train[train_idxs].astype(int)\nX_train = X_train[train_idxs]\n\nX_train = np.reshape(X_train, (X_train.shape[0], -1)).astype('float64')\nX_test = np.reshape(X_test, (X_test.shape[0], -1)).astype('float64')\nX_val = np.reshape(X_val, (X_val.shape[0], -1)).astype('float64')\n\nX_tiny = X_train[100:110].astype('float64')\ny_tiny = y_train[100:110].astype(int)\nnum_dev = 500\n\nX_dev = X_train[0:num_dev].astype('float64')\ny_dev = y_train[0:num_dev].astype(int)\nprint(\"X_train shape\", X_train.shape, \"| y_train shape:\", y_train.shape)\nprint(\"X_test shape\", X_test.shape, \"| y_test shape:\", y_test.shape)\nprint(\"X_val shape\", X_val.shape, \"| y_val shape:\", y_val.shape)\nprint(\"X_dev shape\", X_dev.shape, \"| y_dev shape:\", y_dev.shape)\nprint(\"X_tiny shape\", X_tiny.shape, \"| y_tiny shape:\", y_tiny.shape)\n\n#Subtract out the mean image \n#first: compute the mean image\nmean_image = np.mean(X_train, axis=0) #axis=0. stack horizontally\n#Second subtract the mean image from train and test data \nX_train -= mean_image\nX_val -= mean_image \nX_test -= mean_image\nX_dev -= mean_image\nX_tiny -= mean_image\n\n#Third append the bias dimension using linear algebra trick\nX_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\nX_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\nX_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\nX_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\nX_tiny = np.hstack([X_tiny, np.ones((X_tiny.shape[0], 1))])\n\nprint('=====STACK BIAS term=====')\nprint(\"X_train shape\", X_train.shape)\nprint(\"X_test shape\", X_test.shape)\nprint(\"X_val shape\", X_val.shape)\nprint(\"X_dev shape\", X_dev.shape)\nprint(\"X_tiny shape\", X_tiny.shape)","2613d2c2":"# Visualize some images \n# Make sure that everything when OK\nclasses = ['broadleaf', 'grass', 'soil', 'soybean']\nn_class = len(classes)\nsamples_per_class = 4\n\n\nfor y, cls in enumerate(classes):\n    idxes = np.flatnonzero(y == y_train)\n    idxes = np.random.choice(idxes, samples_per_class, replace = False)\n    for i, idx in enumerate(idxes):\n        plt_idx = i * n_class + y + 1\n        plt.subplot(samples_per_class,n_class, plt_idx)\n        plt.imshow(X_train[idx][:-1].reshape(im_width, im_height, im_channel).astype('uint8'))\n        if(i==0): plt.title(cls)\n\nplt.show()","83010850":"act = {'relu': lambda x: np.maximum(0,x), 'sigmoid': lambda x:1.0\/(1.0 + np.exp(-x))}\n\ndef LLloss(X, W, y, reg = 0.0):# Log likelyhood loss\n    '''\n    X: nd array shape(NxD)\n    W: nd array shape(DxC)\n    y: nd array shape(NxC) #one hot coded\n    https:\/\/ljvmiranda921.github.io\/notebook\/2017\/08\/13\/softmax-and-the-negative-log-likelihood\/\n    '''\n    #convert y_batch to one hot coded\n    num_classes = W.shape[1]\n    num_train = X.shape[0]\n    y_hot = np.zeros((num_train, num_classes))\n    y_hot[np.arange(num_train), y.astype(int)] = 1.0 \n    \n    grad = np.zeros(W.shape)\n    scores = act['sigmoid'](np.dot(X, W)) #shape:(NxC)\n    error = scores - y_hot\n    grad = np.dot(X.T, error)\n    loss = 0.0\n    \n    #zeros out correct class score\n    scores[np.arange(num_train),y.astype(int)] = 0 \n    loss = np.sum(scores)\/num_train\n    loss += reg * np.sum(W * W)\n    grad += reg * W\n    return loss, grad\n\n'''\n#Tested model on soft max with result.\n'''\ndef softmax(X, W, y, reg=0.0):\n    num_classes = W.shape[1]\n    num_train, dim = X.shape\n    loss = 0.0 \n    dW = np.zeros(W.shape)\n    \n    scores = act['relu'](X.dot(W)) #Shape: (N x C) \n    \n    \n    #normalize trick to avoid over flow when compute exp \n    scores -= scores.max(axis=1).reshape(scores.shape[0],1)\n    \n    correct_score = scores[np.arange(num_train), y]\n    \n    scores = np.exp(scores)\n    \n    \n    \n    scores_sumexp = np.sum(scores, axis=1) #stack vertically\n    \n    loss = (-1.0) * np.sum(np.log(np.exp(correct_score)\/scores_sumexp))\n    #Normalize each score with all class score \n    scores \/= scores_sumexp.reshape(num_train, 1) #turn (num_train,) -> (num_train, 1) shape\n    #turn correct score to (sumexp - S)\/sumexp \n    scores[np.arange(num_train), y] = (-1.0) * (scores_sumexp - np.exp(correct_score))\/scores_sumexp\n\n    dW = X.T.dot(scores)\n    #normalize and add regularization term\n    loss \/= num_train\n    dW \/= num_train\n    \n    loss += reg * np.sum(W * W)\n    dW += reg * W\n    \n    return loss, dW\n\n\nclass LogitRegression(): \n    def __init__(self, lossFunction, activation='sidmoid'): \n        '''\n        lossFunction: function pointer\n        self.W = assume that the bias is stacked \n        '''\n        self.W = None\n        self.lossf = lossFunction\n        if(activation not in act): print('invalid actiavtion function')\n        self.act = act[activation]\n    def predict(self, x): \n        if(self.W is None): print('get that weight init')\n        scores = self.act(np.dot(x, self.W)) #Score shape: (N x C)\n        y_pred = np.argmax(scores, axis=1) #stack verticlly\n        return y_pred\n    def train(self, X, y, X_val, y_val, iter_num = 100, batch_size = 200, \n                    learn_rate = 1, reg = 0.0, it_verbose = 100, verbose = False):\n        '''\n        X: nd array shape(N x D) \n        self.W: nd array shape(D x C) \n        '''\n        num_train, dim = X.shape\n        num_classes = int(np.max(y) + 1) \n        if(self.W is None): \n            self.W = 0.01 * np.random.rand(dim, num_classes) \n            \n        # Use SGD to optimize hyperparameter\n        loss_hist = [] \n        train_acc_hist = []\n        val_acc_hist = [] \n        \n        iter_per_epoch = max(num_train\/batch_size, 1) \n        \n        for it in range(iter_num): \n            #Sampling and split trainning into small batch \n            sampling_idxs = np.random.choice(np.arange(num_train), \n                                    batch_size, replace=False)\n            X_batch = X[sampling_idxs]#shape (batch_size x dim)\n            y_batch = y[sampling_idxs]#shape (batch_size x 1)\n            \n            # compute loss and grad using mini batch\n            loss, grad = self.lossf(X_batch, self.W, y_batch, reg)\n            loss_hist.append(loss)\n            \n            # update rule \n            self.W += (-1.0) * learn_rate * grad \n            \n            if(verbose and it%it_verbose==0):\n                print('iteration: %d \/ %d | Loss: %f' % (it, iter_num, loss)) \n                \n            #Every epoch check accuracy of the model \n            if(it%iter_per_epoch==0):#finish one time model read through all datas \n                train_acc = (self.predict(X_batch) == y_batch).mean()\n                val_acc = (self.predict(X_val) == y_val).mean()\n                train_acc_hist.append(train_acc)\n                val_acc_hist.append(val_acc)\n        return {\n            'loss_hist': loss_hist, \n            'train_acc_hist': train_acc_hist,\n            'val_acc_hist' : val_acc_hist\n        }\n            ","5e015347":"LRlikely = LogitRegression(LLloss, activation='sigmoid')\nstats = LRlikely.train(X_train, y_train, X_val, y_val, iter_num = 1000, batch_size = 100, \n                    learn_rate = 1e-7, reg = 1e-3, it_verbose = 100, verbose = True)\n\n# LRsoftmax = LogisticRegression(softmax)\n# loss_hist_softmax = LRsoftmax.train(x_dev, y_dev, iter_num=1000, it_verbose=100)","b7998f71":"# plot loss history and train\/ validation accuracies history\nplt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\nplt.subplot(2,1,1) \nplt.plot(stats['loss_hist'])\nplt.title('Loss History')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\n\nplt.subplot(2,1,2)\nplt.plot(stats['train_acc_hist'], label='train')\nplt.plot(stats['val_acc_hist'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Classfication Accuracies')\nplt.legend()\nplt.show()","6acb931a":"learning_rates = [ 0.55]\n\n#Note: higher regularization term\nregularization_strength = [0.075] \nbest_val = 0.0 \nbest_LLmodel = None\nbest_stats = None\nresult = {}\n\n'''Good search on hyperparameter\nlearn: 3.6e-05 r: 1 train_acc 0.635 val_acc 0.48 test_acc 0.58\nlearn: 3.5e-06 r: 0.1 train_acc 0.54 val_acc 0.48 test_acc 0.58\nlearn: 3.6e-05 r: 0.01 train_acc 0.57 val_acc 0.475 test_acc 0.595\nlearn: 0.55 r: 0.075 train_acc 0.595 val_acc 0.45 test_acc 0.595\n'''\n\nfor learn in learning_rates:\n    for r in regularization_strength: \n        tune_model = LogitRegression(LLloss, activation='sigmoid')\n        stats_tune = tune_model.train(X_train, y_train, X_val, y_val, iter_num = 150, batch_size = 500, \n                    learn_rate = 1e-7, reg = 1e-3, it_verbose = 100, verbose = False)\n        \n        train_acc = stats_tune['train_acc_hist'][-1]\n        val_acc = stats_tune['val_acc_hist'][-1]\n        test_acc = (tune_model.predict(X_test) == y_test).mean()\n        result[(train_acc, val_acc, test_acc)] = (learn, r) \n        #print log \n        print('learn:',learn,'r:',r,'train_acc',train_acc,'val_acc',val_acc,'test_acc', test_acc)\n        if(test_acc > best_val):\n            best_val = test_acc \n            best_LLmodel = tune_model\n            best_stats = stats_tune\n        del tune_model \n","eccb5516":"# plot loss history and train\/ validation accuracies history\nplt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\nplt.subplot(2,1,1) \nplt.plot(best_stats['loss_hist'])\nplt.title('Loss History')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\n\nplt.subplot(2,1,2)\nplt.plot(best_stats['train_acc_hist'], label='train')\nplt.plot(best_stats['val_acc_hist'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Classfication Accuracies')\nplt.legend()\nplt.show()","c7b3bffb":"LRsoftmax = LogitRegression(softmax, activation='relu')\nstats_softmax = LRsoftmax.train(X_train, y_train.astype(int), X_val, y_val.astype(int), iter_num = 10, batch_size = 10, \n                    learn_rate =1e-7, reg =0.0, it_verbose = 1, verbose = True)","e8681ade":"# plot loss history and train\/ validation accuracies history\nplt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\nplt.subplot(2,1,1) \nplt.plot(stats_softmax['loss_hist'])\nplt.title('Loss History')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\n\nplt.subplot(2,1,2)\nplt.plot(stats_softmax['train_acc_hist'], label='train')\nplt.plot(stats_softmax['val_acc_hist'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Classfication Accuracies')\nplt.legend() \nplt.show()\n","ee97580a":"# print(LRsoftmax.W[158700])\n# Image dimension\nim_width = 230\nim_height = 230 \nim_channel = 3\ndim = im_width * im_height * im_channel\nplt.imshow(X_val[2][:-1].reshape(im_width, im_height, im_channel))\n\nprint(X_val[2][:-1][128000:129000])","68e0f16a":"### Hyperparameter tuning using crosvalidation ","5ee14083":"# Debug model with Dev set\n## Loglikelyhood loss ","a38a967c":"## Softmax loss with relu ","962017be":"## Apply Logistic regression Model on Data \n","65cabbcf":"# Preprocessing the data","f9ff5d80":"### First: Define 2 Loss function that can possibly apply on Logit regression \n- Softmax loss \n- Loglikely hood loss\n\n##### two Activation function that will be test on\n- RELU\n- Sigmoid"}}