{"cell_type":{"3c2f53cc":"code","58a4344f":"code","306aef8e":"code","a3f21a75":"code","89a46c56":"code","f64aa71e":"code","2a0f9707":"code","0190be75":"code","edf2e7df":"code","57c16310":"code","0c46d5d7":"code","d0962c48":"code","2d6532b8":"code","c9c40463":"code","1b847037":"code","17ca4555":"code","e4064378":"code","cbb58887":"code","9c1e63f2":"code","40a64243":"code","a86970ec":"code","da081dec":"code","a6f515b3":"code","4301ccce":"code","2b5c4067":"code","f22814f6":"code","c1d2c138":"code","78261402":"code","6a2fcc84":"code","a99e8eb1":"code","0ff7fbe5":"code","355a77ee":"code","1e825f7a":"code","a795ee4f":"code","00b32324":"code","c3a2d614":"code","c881a260":"code","88071f53":"code","0b21fed1":"code","f9236af9":"code","5b03e930":"code","3091bffe":"code","c3a58102":"code","bcf0e32a":"code","78f17cac":"code","05e73e82":"code","8e254a88":"code","73861c15":"markdown","31e2a341":"markdown","7673928b":"markdown","424fe976":"markdown","1154fa85":"markdown","e888436d":"markdown","dd505b40":"markdown","f08935c5":"markdown","b6f12ff2":"markdown","6ee1e224":"markdown","2edb31ed":"markdown","87c81c2a":"markdown","7b017d32":"markdown","6c50a6ee":"markdown","53ee8271":"markdown","6b118b42":"markdown","ee6d2f8c":"markdown","4a3485a5":"markdown","a87c538e":"markdown","084e00ee":"markdown","9efd0ce0":"markdown","5a458b98":"markdown","20cb5e23":"markdown","36925526":"markdown","9e7f2207":"markdown","4935563b":"markdown","a08c6384":"markdown","1d70bbca":"markdown","eb41b93b":"markdown","f0930dfc":"markdown","bf45b62b":"markdown","3be962bf":"markdown","81bd493a":"markdown","99c9810d":"markdown","68fda931":"markdown","be7b8355":"markdown","ade5ed66":"markdown","96d58486":"markdown","1b874ba2":"markdown","3bf7d880":"markdown","ce448376":"markdown","95e81c65":"markdown","3628856e":"markdown","d0ecdb6a":"markdown","0acee270":"markdown","15acf146":"markdown","6a71420a":"markdown","6b182fad":"markdown","99f099a8":"markdown","5d3a8c8e":"markdown","b2d3cb3e":"markdown","2eb16256":"markdown","a58f5cf0":"markdown","09856971":"markdown","b7bd5159":"markdown","5346636b":"markdown","1e565058":"markdown","02486ec3":"markdown","aabc545b":"markdown","a6e999e8":"markdown"},"source":{"3c2f53cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import median_absolute_error\nimport random\nimport datetime\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.python.keras.losses import LossFunctionWrapper\nfrom tensorflow.python.keras.utils import losses_utils\nimport tensorflow_probability as tfp\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\ntfd = tfp.distributions\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\n!pip install -q -U keras-tuner\n\nimport keras_tuner as kt\nimport IPython\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58a4344f":"\"\"\"Functions to assess loss.\"\"\"\n\ndef quantile_loss(y_true, y_pred, quantiles):\n    \n    \"\"\"Calculate the quantile loss function, summed across all quantile outputs.\n\n    Parameters\n    ----------\n    y_true : ndarray or dataframe or list or Tensor of shape `[batch_size, d0, .. dN]`.\n        Ground truth values.\n\n    y_pred : ndarray or dataframe or list or Tensor of shape `[batch_size, d0, .. dN]`.\n        The predicted values.\n\n    n_quantiles : ndarray or dataframe or list or Tensor of shape `[batch_size, d0, .. dN]`.\n        The set of output n_quantiles on which is calculated the quantile loss.\n\n    Returns\n    -------\n    error : Tensor of shape `[batch_size, d0, .. dN]`.\n        The error in %.\n    \"\"\"\n\n    y_true = tf.cast(y_true, dtype=tf.float32)\n    y_pred = tf.cast(y_pred, dtype=tf.float32)\n    quantiles = tf.convert_to_tensor(quantiles)\n    diff = tf.transpose(y_true - y_pred)\n\n    quantile_loss = (quantiles * tf.clip_by_value(diff, 0., np.inf) +\n                    (1 - quantiles) * tf.clip_by_value(-diff, 0., np.inf))\n    \n    M = tf.cast(tf.shape(y_true)[0], dtype=tf.float32)\n    error = quantile_loss \/ M\n    \n    sum_quantiles = tf.reduce_sum(error, axis=-1)\n    return tf.reduce_sum(sum_quantiles, axis=tf.range(sum_quantiles.shape.rank-1))\n\nclass QuantileLossError(LossFunctionWrapper):\n    \"\"\"Calculate the quantile loss error between `y_true`and `y_pred` across all examples.\n    Standalone usage:\n    >>> y_true = [[0., 1., 2.], [0., 0., 4.]]\n    >>> y_pred = [[1., 1., 2.], [1., 0., 3.]]\n    >>> # Using 'auto'\/'sum_over_batch_size' reduction type.\n    >>> ql = QuantileLossError(quantiles=[0.5])\n    >>> ql(y_true, y_pred).numpy()\n    0.5\n    >>> # Calling with 'sample_weight'.\n    >>> ql(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()\n    0.25\n    >>> # Using 'AUTO' reduction type.\n    >>> ql = QuantileLossError(quantiles=[0.5],\n    ...     reduction=tf.keras.losses.Reduction.AUTO)\n    >>> ql(y_true, y_pred).numpy()\n    0.25\n    >>> # Using 'none' reduction type.\n    >>> ql = QuantileLossError(quantiles=[0.5],\n    ...     reduction=tf.keras.losses.Reduction.NONE)\n    >>> ql(y_true, y_pred).numpy()\n    array([0.25, 0.25], dtype=float32)\n    >>> # Using multiple quantiles.\n    >>> ql = QuantileLossError(quantiles=[0.1, 0.5, 0.9])\n    >>> ql(y_true, y_pred).numpy()\n    1.5\n\n    Usage with the `compile()` API:\n    ```python\n    model.compile(optimizer='sgd', loss=tf.keras.losses.QuantileLossError())\n    ```\n    \"\"\"\n\n    def __init__(self,\n                 quantiles,\n                 reduction=losses_utils.ReductionV2.SUM,\n                 name='quantile_loss'):\n        super(QuantileLossError, self).__init__(\n            quantile_loss, quantiles=quantiles, name=name, reduction=reduction)","306aef8e":"class Stack(tf.keras.layers.Layer):\n    \"\"\"A stack is a series of blocks where each block produce two outputs, the horizon and the back_horizon. \n    All of the outputs are sum up which compose the stack output while each residual back_horizon is given to the following block.\n    \n    Parameters\n    ----------\n    blocks: list of `TrendBlock`, `SeasonalityBlock` or `GenericBlock`.\n        Define blocks in a stack.\n    \"\"\"\n    def __init__(self, blocks, **kwargs):\n        \n        super().__init__(**kwargs)\n\n        self._blocks = blocks\n\n    def call(self, inputs):\n\n        y_horizon = 0.\n        for block in self._blocks:\n            residual_y, y_back_horizon = block(inputs) # shape: (n_quantiles, Batch_size, horizon), (Batch_size, back_horizon)\n            inputs = tf.subtract(inputs, y_back_horizon)\n            y_horizon = tf.add(y_horizon, residual_y) # shape: (n_quantiles, Batch_size, horizon)\n\n        return y_horizon, inputs","a3f21a75":"class N_BEATS(tf.keras.Model):\n    \"\"\"This class compute the N-BEATS model. This is a univariate model which can be\n     interpretable or generic. It's strong advantage is its internal structure which allows us \n     to extract the trend and the seasonality of a temporal serie. It's available from the attributes\n     `seasonality` and `trend`. This is an unofficial implementation.\n\n     `@inproceedings{\n        Oreshkin2020:N-BEATS,\n        title={{N-BEATS}: Neural basis expansion analysis for interpretable time series horizoning},\n        author={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\n        booktitle={International Conference on Learning Representations},\n        year={2020},\n        url={https:\/\/openreview.net\/forum?id=r1ecqn4YwB}\n        }`\n    \n    Parameter\n    ---------\n    stacks: list of `Stack` layer.\n        Define the stack to use in nbeats model. It can be full of `TrendBlock`, `SeasonalityBlock` or `GenereicBlock`.\n    \"\"\"\n    def __init__(self, \n                 stacks,\n                 **kwargs):\n                \n        super().__init__(**kwargs)\n\n        self._stacks = stacks\n\n    def call(self, inputs):\n        self._residuals_y = tf.TensorArray(tf.float32, size=len(self._stacks)) # Stock trend and seasonality curves during inference\n        y_horizon = 0.\n        for idx, stack in enumerate(self._stacks):\n            residual_y, inputs = stack(inputs)\n            self._residuals_y.write(idx, residual_y)\n            y_horizon = tf.add(y_horizon, residual_y)\n\n        return y_horizon\n\n    @property\n    def seasonality(self):\n        return self._residuals_y.stack()[1]\n\n    @property\n    def trend(self):\n        return self._residuals_y.stack()[0]","89a46c56":"class TrendBlock(tf.keras.layers.Layer):\n    \"\"\" Trend block definition. Output layers are constrained which define polynomial function of small degree p.\n    Therefore it is possible to get explanation from this block.\n    \n    Parameter\n    ---------\n    p_degree: integer\n        Degree of the polynomial function.\n    horizon: integer\n        Horizon time to horizon.\n    back_horizon: integer\n        Past to rebuild.\n    n_neurons: integer\n        Number of neurons in Fully connected layers.\n    n_quantiles: Integer.\n        Number of quantiles in `QuantileLossError`.\n    \"\"\"\n    def __init__(self, \n                 horizon, \n                 back_horizon,\n                 p_degree,   \n                 n_neurons, \n                 n_quantiles, \n                 dropout_rate,\n                 **kwargs):\n\n        super().__init__(**kwargs)\n        \n        self._p_degree = tf.reshape(tf.range(p_degree + 1, dtype='float32'), shape=(-1, 1)) # Shape (-1, 1) in order to broadcast horizon to all p degrees\n        self._horizon = tf.cast(horizon, dtype='float32') \n        self._back_horizon = tf.cast(back_horizon, dtype='float32')\n        self._n_neurons = n_neurons \n        self._n_quantiles = n_quantiles\n\n        self._FC_stack = [tf.keras.layers.Dense(n_neurons, \n                                            activation='relu', \n                                            kernel_initializer=\"glorot_uniform\") for _ in range(4)]\n        \n        self._dropout = tf.keras.layers.Dropout(dropout_rate)\n        \n        self._FC_back_horizon = self.add_weight(shape=(n_neurons, p_degree + 1), \n                                           trainable=True,\n                                           initializer=\"glorot_uniform\",\n                                           name='FC_back_horizon_trend')\n        \n        self._FC_horizon = self.add_weight(shape=(n_quantiles, n_neurons, p_degree + 1),\n                                           trainable=True,\n                                           initializer=\"glorot_uniform\",\n                                           name='FC_horizon_trend')\n\n        self._horizon_coef = (tf.range(self._horizon) \/ self._horizon) ** self._p_degree\n        self._back_horizon_coef = (tf.range(self._back_horizon) \/ self._back_horizon) ** self._p_degree\n        \n    def call(self, inputs):\n\n        for dense in self._FC_stack:\n            x = dense(inputs) # shape: (Batch_size, n_neurons)\n            x = self._dropout(x, training=True) # We bind first layers by a dropout \n            \n        theta_back_horizon = x @ self._FC_back_horizon # shape: (Batch_size, p_degree)\n        theta_horizon = x @ self._FC_horizon # shape: (n_quantiles, Batch_size, p_degree)\n\n        y_back_horizon = theta_back_horizon @ self._back_horizon_coef # shape: (Batch_size, back_horizon)\n        y_horizon = theta_horizon @ self._horizon_coef # shape: (n_quantiles, Batch_size, horizon)\n        \n        return y_horizon, y_back_horizon\n    ","f64aa71e":"class SeasonalityBlock(tf.keras.layers.Layer):\n    \"\"\"Seasonality block definition. Output layers are constrained which define fourier series. \n    Each expansion coefficent then become a coefficient of the fourier serie. As each block and each \n    stack outputs are sum up, we decided to introduce fourier order and multiple seasonality periods.\n    Therefore it is possible to get explanation from this block.\n    \n    Parameters\n    ----------\n    horizon: integer\n        Horizon time to horizon.\n    back_horizon: integer\n        Past to rebuild.\n    n_neurons: integer\n        Number of neurons in Fully connected layers.\n    periods: Integer.\n        fourier serie period. The paper set this parameter to `horizon\/2`.\n    back_periods: Integer.\n        fourier serie back period. The paper set this parameter to `back_horizon\/2`.\n    horizon_fourier_order: Integer.\n        Higher values signifies complex fourier serie\n    back_horizon_fourier_order: Integer.\n        Higher values signifies complex fourier serie\n    n_quantiles: Integer.\n        Number of quantiles in `QuantileLossError`.\n    \"\"\"\n    def __init__(self,\n                 horizon,\n                 back_horizon,\n                 n_neurons, \n                 periods, \n                 back_periods, \n                 horizon_fourier_order,\n                 back_horizon_fourier_order,\n                 n_quantiles,\n                 dropout_rate,\n                 **kwargs):\n        \n        super().__init__(**kwargs)\n\n        self._horizon = horizon\n        self._back_horizon = back_horizon\n        self._periods = tf.cast(tf.reshape(periods, (1, -1)), 'float32') # Broadcast horizon on multiple periods\n        self._back_periods = tf.cast(tf.reshape(back_periods, (1, -1)), 'float32')  # Broadcast back horizon on multiple periods\n        self._horizon_fourier_order = tf.reshape(tf.range(horizon_fourier_order, dtype='float32'), shape=(-1, 1)) # Broadcast horizon on multiple fourier order\n        self._back_horizon_fourier_order = tf.reshape(tf.range(back_horizon_fourier_order, dtype='float32'), shape=(-1, 1)) # Broadcast horizon on multiple fourier order\n\n        # Workout the number of neurons needed to compute seasonality coefficients\n        horizon_neurons = tf.reduce_sum(2 * horizon_fourier_order)\n        back_horizon_neurons = tf.reduce_sum(2 * back_horizon_fourier_order)\n        \n        self._FC_stack = [tf.keras.layers.Dense(n_neurons, \n                                               activation='relu', \n                                               kernel_initializer=\"glorot_uniform\") for _ in range(4)]\n        \n        self._dropout = tf.keras.layers.Dropout(dropout_rate)   \n        \n        self._FC_back_horizon = self.add_weight(shape=(n_neurons, back_horizon_neurons), \n                                           trainable=True,\n                                           initializer=\"glorot_uniform\",\n                                           name='FC_back_horizon_seasonality')\n\n        self._FC_horizon = self.add_weight(shape=(n_quantiles, n_neurons, horizon_neurons), \n                                           trainable=True,\n                                           initializer=\"glorot_uniform\",\n                                           name='FC_horizon_seasonality')\n        \n        # Workout cos and sin seasonality coefficents\n        time_horizon = tf.range(self._horizon, dtype='float32') \/ self._periods\n        horizon_seasonality = 2 * np.pi * self._horizon_fourier_order * time_horizon\n        self._horizon_coef = tf.concat((tf.cos(horizon_seasonality), \n                                          tf.sin(horizon_seasonality)), axis=0)\n\n        time_back_horizon = tf.range(self._back_horizon, dtype='float32') \/ self._back_periods\n        back_horizon_seasonality = 2 * np.pi * self._back_horizon_fourier_order * time_back_horizon\n        self._back_horizon_coef = tf.concat((tf.cos(back_horizon_seasonality), \n                                        tf.sin(back_horizon_seasonality)), axis=0)\n        \n    def call(self, inputs):\n\n        for dense in self._FC_stack:\n            X = dense(inputs) # shape: (Batch_size, nb_neurons)\n            X = self._dropout(X, training=True) # We bind first layers by a dropout \n\n        theta_horizon = X @ self._FC_horizon # shape: (n_quantiles, Batch_size, 2 * fourier order)\n        theta_back_horizon = X @ self._FC_back_horizon # shape: (Batch_size, 2 * fourier order)\n        \n        y_horizon = theta_horizon @ self._horizon_coef # shape: (n_quantiles, Batch_size, horizon)\n        y_back_horizon = theta_back_horizon @ self._back_horizon_coef # shape: (Batch_size, back_horizon)\n    \n        return y_horizon, y_back_horizon","2a0f9707":"class GenericBlock(keras.layers.Layer):\n    \"\"\"\n    Generic block definition as described in the paper. \n    We can't have explanation from this kind of block because g coefficients are learnt.\n    \n    Parameter\n    ---------\n    horizon: integer\n        Horizon time to horizon.\n    back_horizon: integer\n        Past to rebuild.\n    nb_neurons: integer\n        Number of neurons in Fully connected layers.\n    back_neurons: integer\n        Number of back_horizon expansion coefficients.\n    fore_neurons: integer\n        Number of horizon expansion coefficients.\n    \"\"\"\n    def __init__(self, horizon, \n                       back_horizon, \n                       n_neurons, \n                       n_quantiles,\n                       dropout_rate,\n                        **kwargs):\n        \n        super().__init__(**kwargs)\n        \n        self._FC_stack = [keras.layers.Dense(n_neurons, \n                                            activation='relu', \n                                            kernel_initializer=\"glorot_uniform\") for _ in range(4)]\n        \n        self._dropout = tf.keras.layers.Dropout(dropout_rate)\n\n        \n        self._FC_back_horizon = self.add_weight(shape=(n_neurons, n_neurons), \n                                           trainable=True,\n                                           initializer=\"glorot_uniform\",\n                                           name='FC_back_horizon_generic')\n\n        self._FC_horizon = self.add_weight(shape=(n_quantiles, n_neurons, n_neurons), \n                                           trainable=True,\n                                           initializer=\"glorot_uniform\",\n                                           name='FC_horizon_generic')\n        \n        self._back_horizon = keras.layers.Dense(back_horizon, \n                                           kernel_initializer=\"glorot_uniform\")\n        \n        self._horizon = keras.layers.Dense(horizon, \n                                           kernel_initializer=\"glorot_uniform\")\n        \n    def call(self, inputs):\n        X = inputs # shape: (Batch_size, back_horizon)\n        for dense_layer in self._FC_stack:\n            X = dense_layer(X) # shape: (Batch_size, nb_neurons)\n            X = self._dropout(X, training=True) # We bind first layers by a dropout \n            \n        theta_horizon = X @ self._FC_horizon # shape: (n_quantiles, Batch_size, 2 * fourier order)\n        theta_back_horizon = X @ self._FC_back_horizon # shape: (Batch_size, 2 * fourier order)\n        \n        y_back_horizon = self._back_horizon(theta_back_horizon) # shape: (Batch_size, back_horizon)\n        y_horizon = self._horizon(theta_horizon) # shape: (n_quantiles, Batch_size, horizon)\n        \n        return y_horizon, y_back_horizon\n    ","0190be75":"def confidence_interval(time_series, quantile):\n    \n    mean = tf.reduce_mean(time_series, axis=0)\n    standard_deviation = tf.math.reduce_std(time_series, axis=0)\n    \n    # (-1, 1, 1, 1) Broadcast std with shape (quantile, timesteps, horizon)\n    max_interval = mean + tf.reshape(tfd.Normal(loc=0, scale=1).quantile(quantile), (-1, 1, 1, 1)) * standard_deviation \n    min_interval = mean - tf.reshape(tfd.Normal(loc=0, scale=1).quantile(quantile), (-1, 1, 1, 1)) * standard_deviation \n    \n    return mean, min_interval, max_interval\n\ndef fig_add_trace(fig, y, x, row, col):\n    for data, fill, name, color, showlegend in y:\n        fig.add_trace(\n            go.Scatter(\n                name=name,\n                x=x,\n                y=data,\n                fill=fill,\n                line=dict(color=color),\n                fillcolor=color,\n                showlegend=showlegend),\n            row=row, col=col)\n        \ndef plot_results_nbeats(y_pred,\n                        date_outputs,\n                        y_true=None,\n                        date_history=None,\n                        seasonality=None,\n                        trend=None):\n    \n    date = pd.to_datetime(date_outputs[0])\n    date_history = date_history if date_history is not None else pd.to_datetime(date_outputs[0])\n    y_pred_mean, y_pred_min_interval, y_pred_max_interval = confidence_interval(y_pred, [0.99])\n    \n    if trend is None:\n        fig = make_subplots(\n        subplot_titles=['True Vs Predicted','Trend','Seasonality', 'Overall trend', 'Overall seasonality'],\n        rows=1, cols=1,\n        vertical_spacing=0.1,\n        horizontal_spacing=0.05,\n        specs=[[{\"type\": \"scatter\"}]])\n        \n        if y_true is not None:\n    \n            # Trace ground truth\n            fig.add_trace(\n                    go.Scatter(\n                        name=\"y_true\",\n                        x=date_history,\n                        y=y_true[0],\n                        line=dict(color=\"green\")),\n                    row=1, col=1\n                )  \n    \n    else:\n        fig = make_subplots(\n        subplot_titles=['True Vs Predicted','Trend','Seasonality', 'Overall trend', 'Overall seasonality'],\n        rows=3, cols=2,\n        vertical_spacing=0.1,\n        horizontal_spacing=0.05,\n        column_widths=[0.8, 0.6],\n        row_heights=[0.8, 0.8, 0.8],\n        specs=[[{\"type\": \"scatter\", \"rowspan\": 2}, {\"type\": \"scatter\"}],\n               [        None      , {\"type\": \"scatter\"}], \n               [{\"type\": \"scatter\", \"colspan\": 2}, None]])\n        \n        if y_true is not None:\n    \n            # Trace ground truth\n            fig.add_trace(\n                    go.Scatter(\n                        name=\"y_true\",\n                        x=date_history,\n                        y=y_true[0],\n                        line=dict(color=\"green\")),\n                    row=1, col=1\n                )  \n            \n        trend_mean, trend_min_interval, trend_max_interval = confidence_interval(trend, [0.99])\n\n        fig_add_trace(fig, zip([trend_min_interval[0, 1, 0], trend_max_interval[0, 1, 0], \n                                trend_mean[1, 0]], \n                              ['none','tonextx', 'none'],\n                              ['Confidence interval', 'Confidence interval', 'Average'],\n                              ['rgba(153,50,204, 0.7)', 'rgba(153,50,204, 0.7)', \n                               'darkorange'], \n                              [ False, False, False]), date, row=1, col=2)\n        \n            # Trace mean\n        fig_add_trace(fig, zip([tf.reduce_mean(trend_min_interval, axis=2)[0, 1], tf.reduce_mean(trend_max_interval, axis=2)[0, 1], \n                            tf.reduce_mean(trend_mean, axis=1)[1]], \n                          ['none','tonextx', 'none'],\n                          ['Confidence interval', 'Confidence interval', 'Average'],\n                          ['rgba(153,50,204, 0.7)', 'rgba(153,50,204, 0.7)', \n                           'darkorange'], \n                          [False, False, False]), date, row=3, col=1)\n    \n    # Trace mean\n    fig_add_trace(fig, zip([y_pred_min_interval[0, 0, 0], y_pred_max_interval[0, 2, 0], \n                            y_pred_min_interval[0, 1, 0], y_pred_max_interval[0, 1, 0], \n                            y_pred_mean[1, 0]], \n                          ['none', 'tonextx', 'none','tonextx', 'none'],\n                          ['Prediction interval', 'Prediction interval', 'Confidence interval', 'Confidence interval', 'Average'],\n                          ['rgba(255, 0, 0, 0.2)', 'rgba(255, 0, 0, 0.2)', \n                           'rgba(153,50,204, 0.7)', 'rgba(153,50,204, 0.7)', \n                           'darkorange'], \n                          [False, True, False, True, True]), date, row=1, col=1)\n    \n    # Trace seasonality\n    if seasonality is not None:\n        seasonality_mean, seasonality_min_interval, seasonality_max_interval = confidence_interval(seasonality, [0.99])\n        fig_add_trace(fig, zip([seasonality_min_interval[0, 1, 0], seasonality_max_interval[0, 1, 0], \n                                seasonality_mean[1, 0]], \n                              ['none','tonextx', 'none'],\n                              ['Confidence interval', 'Confidence interval', 'Average'],\n                              ['rgba(153,50,204, 0.7)', 'rgba(153,50,204, 0.7)', \n                               'darkorange'], \n                              [False, False, False]), date, row=2, col=2)\n    \n    return fig","edf2e7df":"class DataSet:\n    \"\"\"\n    Preprocessing.\n    \"\"\"\n    def __init__(self, horizon, back_horizon):\n        self.horizon = horizon\n        self.back_horizon = back_horizon\n    \n    def preprocessing(self, y, date, train_size=0.7, val_size=0.2):\n        \n        y = y.copy().astype('float')\n\n        train = y[:int(train_size*len(y))]\n        val = y[int(train_size*len(y))-self.back_horizon:int((train_size+val_size)*len(y))]\n        test = y[int((train_size+val_size)*len(y))-self.back_horizon:]\n        train_date = date[:int(train_size*len(y))]\n        val_date = date[int(train_size*len(y))-self.back_horizon:int((train_size+val_size)*len(y))]\n        test_date = date[int((train_size+val_size)*len(y))-self.back_horizon:]\n\n        # Training set\n        self.X_train, self.y_train, self.train_date = self.create_sequences(train, \n                                                                            train, \n                                                                            train_date,\n                                                                            self.horizon, \n                                                                            self.back_horizon)\n        # Validation set\n        self.X_val, self.y_val, self.val_date = self.create_sequences(val,\n                                                                      val,\n                                                                      val_date,\n                                                                      self.horizon,\n                                                                      self.back_horizon)\n        # Testing set\n        self.X_test, self.y_test, self.test_date = self.create_sequences(test,\n                                                                         test,\n                                                                         test_date,\n                                                                         self.horizon,\n                                                                         self.back_horizon)\n\n        # training on all database\n        self.X_train_all, self.y_train_all, self.train_all_date = self.create_sequences(y, \n                                                                                        y,\n                                                                                        date,\n                                                                                        self.horizon,\n                                                                                        self.back_horizon)\n            \n    @staticmethod\n    def create_sequences(X, y, d, horizon, time_steps):\n        Xs, ys, ds = [], [], []\n        for col in range(X.shape[1]):\n            for i in range(0, len(X)-time_steps-horizon, 1):\n                Xs.append(X[i:(i+time_steps), col])\n                ys.append(y[(i+time_steps):(i+time_steps+horizon), col])\n                ds.append(d[(i+time_steps):(i+time_steps+horizon)])\n\n        return np.array(Xs), np.array(ys), np.array(ds)","57c16310":"def create_interpretable_nbeats(horizon, \n                                   back_horizon,\n                                   p_degree,   \n                                   trend_n_neurons, \n                                   seasonality_n_neurons, \n                                   periods, \n                                   back_periods, \n                                   horizon_fourier_order,\n                                   back_horizon_fourier_order,\n                                   n_quantiles, \n                                   share=True,\n                                   dropout_rate=0.1,\n                                   **kwargs):\n    \n    \"\"\"Wrapper to create interpretable model. check nbeats doc to know more about parameters.\"\"\"\n    \n    if share is True:\n        trend_block = TrendBlock(horizon=horizon, \n                                 back_horizon=back_horizon, \n                                 p_degree=p_degree, \n                                 n_neurons=trend_n_neurons, \n                                 n_quantiles=n_quantiles,\n                                 dropout_rate=dropout_rate,\n                                 **kwargs)\n        \n        seasonality_block = SeasonalityBlock(horizon=horizon, \n                                 back_horizon=back_horizon, \n                                 periods=periods, \n                                 back_periods=back_periods, \n                                 horizon_fourier_order=horizon_fourier_order,\n                                 back_horizon_fourier_order = back_horizon_fourier_order,\n                                 n_neurons=seasonality_n_neurons, \n                                 n_quantiles=n_quantiles,dropout_rate=dropout_rate, **kwargs)\n        \n        trendblocks = [trend_block for _ in range(3)]\n        seasonalityblocks = [seasonality_block for _ in range(3)]\n    else:\n        trendblocks = [TrendBlock(horizon=horizon, \n                                 back_horizon=back_horizon, \n                                 p_degree=p_degree, \n                                 n_neurons=trend_n_neurons, \n                                 n_quantiles=n_quantiles, \n                                  dropout_rate=dropout_rate,\n                                 **kwargs) for _ in range(3)]\n        seasonalityblocks = [SeasonalityBlock(horizon=horizon, \n                                 back_horizon=back_horizon, \n                                 periods=periods, \n                                 back_periods=back_periods, \n                                 horizon_fourier_order=horizon_fourier_order,\n                                 back_horizon_fourier_order = back_horizon_fourier_order,\n                                 n_neurons=seasonality_n_neurons, \n                                 n_quantiles=n_quantiles,dropout_rate=dropout_rate, **kwargs) for _ in range(3)]\n        \n    trendstacks = Stack(trendblocks)\n    seasonalitystacks = Stack(seasonalityblocks)\n    \n    return N_BEATS([trendstacks, seasonalitystacks])\n\ndef create_generic_nbeats(horizon,\n                          back_horizon, \n                           n_neurons, \n                           n_quantiles,\n                           n_blocks,\n                           n_stacks,\n                           share=True,\n                           dropout_rate=0.1,\n                           **kwargs):\n\n    \"\"\"Wrapper to create interpretable model. check nbeats doc to know more about parameters.\"\"\"\n    generic_stacks = []\n    if share is True:\n        for stack in range(n_stacks):\n            generic_block = GenericBlock(horizon=horizon, \n                              back_horizon=back_horizon, \n                               n_neurons=n_neurons, \n                               n_quantiles=n_quantiles,\n                               dropout_rate=0.1,\n                               **kwargs)\n\n            generic_blocks = [generic_block for _ in range(n_blocks)]\n            generic_stacks.append(Stack(generic_blocks))\n            \n    else:\n        for stack in range(n_stacks):\n            generic_blocks = [GenericBlock(horizon=horizon, \n                              back_horizon=back_horizon, \n                               n_neurons=n_neurons, \n                               n_quantiles=n_quantiles, \n                               dropout_rate=0.1,\n                               **kwargs) for _ in range(n_blocks)]\n            \n            generic_stacks.append(Stack(generic_blocks))\n    \n    return N_BEATS(generic_stacks)","0c46d5d7":"data = pd.read_csv('\/kaggle\/input\/temperature-change\/Environment_Temperature_change_E_All_Data_NOFLAG.csv', encoding='latin', sep=',')\ndate = pd.date_range('1961-01-01', '2020-01-01', freq='M')","d0962c48":"# We are filtering on valid countries and months and rearranging the data to get df with shape (countries, months)\n\ndef preprocessing(element, data=data.copy()):\n    df = []\n    col = []\n    for country in data['Area'].unique():\n        new_data = data.dropna().query(f'Area==\"{country}\" & 7012>=`Months Code` & Element==\"{element}\"')    \n        if len(new_data) == 12:\n            col.append(country)\n            df.append(new_data.iloc[:,7:].values.flatten('F'))\n\n    df = pd.DataFrame(np.array(df).T, index=date, columns=col)\n    return df\n        \ndf = preprocessing(\"Temperature change\")\ndf_std = preprocessing(\"Standard Deviation\")","2d6532b8":"fig = px.line(df, x=df.index, y=df.columns[0:],\n              title='Temperature in \u00b0C over countries', width=1500)\n\nfig.update_layout(\n    updatemenus=[\n        dict(\n            active=0,\n            buttons=list([dict(label=\"None\",\n                     method=\"update\",\n                     args=[{\"visible\": [True for _ in range(186)]},\n                           {\"title\": \"Temperature in \u00b0C over countries\",\n                            \"annotations\": []}])]) + list([\n                dict(label=f\"{j}\",\n                     method=\"update\",\n                     args=[{\"visible\": [True if i==idx else False for i in range(186)]},\n                           {\"title\": f\"{j}\",\n                            \"annotations\": []}]) for idx,j in enumerate(df.columns[0:])])\n            )])\n\nfig.show()","c9c40463":"back_horizon = 2 * 120\nhorizon = 120\n\ndataset = DataSet(horizon, back_horizon)\ndataset.preprocessing(df.values, date, train_size=0.60, val_size=0.20)","1b847037":"def model_builder(hp):\n    \n    hp_n_neurons_trend = hp.Int('neurons_trend', min_value = 4, max_value=16, step=2)\n    hp_n_neurons_seas = hp.Int('neurons_seas', min_value = 4, max_value=16, step=2)\n    hp_periods = hp.Int('periods', min_value = 1, max_value = horizon, step = 30)\n    hp_back_periods = hp.Int('back_periods', min_value = 1, max_value = back_horizon, step = 30)\n    hp_share = hp.Boolean('share')\n    \n    model_nbeats = create_interpretable_nbeats(horizon=horizon, \n                                               back_horizon=back_horizon,\n                                               p_degree=1,   \n                                               trend_n_neurons=hp_n_neurons_trend, \n                                               seasonality_n_neurons=hp_n_neurons_seas, \n                                               periods=hp_periods, \n                                               back_periods=hp_back_periods, \n                                               horizon_fourier_order=hp_periods,\n                                               back_horizon_fourier_order=hp_back_periods,\n                                               n_quantiles=3, \n                                               share=hp_share)\n\n    model_nbeats.compile(loss=QuantileLossError([0.1, 0.5, 0.9]), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                         metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\n    return model_nbeats\n\ntuner = kt.Hyperband(model_builder,\n                     objective = 'val_loss', \n                     max_epochs = 10,\n                     factor = 3,\n                     directory = 'my_dira',\n                     project_name = 'temp')\n\nclass ClearTrainingOutput(tf.keras.callbacks.Callback):\n    def on_train_end(*args, **kwargs):\n        IPython.display.clear_output(wait = True)\n\ntuner.search(dataset.X_train, dataset.y_train, epochs = 20, \n             validation_data = (dataset.X_val, dataset.y_val), callbacks = [ClearTrainingOutput()])","17ca4555":"best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n\n# Build the model with the optimal hyperparameters and train it on the data\nmodel = tuner.hypermodel.build(best_hps)\nmodel.build((None, back_horizon))\nmodel.summary()\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.001)\nes_cb = EarlyStopping(monitor='val_loss', min_delta=0,  patience=10, verbose=0, mode='auto', restore_best_weights=True)\n\nhistory = model.fit(dataset.X_train, dataset.y_train, batch_size=64, epochs=20, \n                    callbacks = [reduce_lr, es_cb], validation_data = (dataset.X_val, dataset.y_val))\n","e4064378":"fig = plt.figure(figsize=(20,5))\n\nax = plt.subplot(131)\n\nepochs = [i for i in range(len(history.history['loss']))]\nax.plot(epochs, history.history['loss'], label='loss')\nax.plot(epochs, history.history['val_loss'], label='val_loss')\nplt.legend()\n\nax = plt.subplot(132)\nax.plot(epochs, history.history['mean_absolute_error'], label='mean_absolute_error')\nax.plot(epochs, history.history['val_mean_absolute_error'], label='val_mean_absolute_error')\nplt.legend()\n\nax = plt.subplot(133)\nax.plot(epochs, history.history['lr'], label='lr')\nb = plt.legend()","cbb58887":"outputs = []\nseasonality = []\ntrend = []\nfor sample in range(30):\n    results = model(dataset.X_test)\n    outputs.append(results)\n    seasonality.append(model.seasonality)\n    trend.append(model.trend)\n    \nplot_results_nbeats(y_pred=tf.stack(outputs),\n                    date_outputs=dataset.test_date,\n                    y_true=dataset.y_test,\n                    date_history=None,\n                    seasonality=tf.stack(seasonality),\n                    trend=tf.stack(trend))","9c1e63f2":"outputs = []\nseasonality = []\ntrend = []\nfor sample in range(50):\n    results = model(df.iloc[-240:].T.values)\n    outputs.append(results)\n    seasonality.append(model.seasonality)\n    trend.append(model.trend)","40a64243":"date_forecasting = np.expand_dims(pd.date_range('2020-01-01', '2030-01-01', freq='M'), axis=0)\n\nplot_results_nbeats(y_pred=tf.stack(outputs),\n                    date_outputs=date_forecasting,\n                    y_true=df.T.values,\n                    date_history=date,\n                    seasonality=tf.stack(seasonality),\n                    trend=tf.stack(trend))","a86970ec":"data_gaz = pd.read_csv('\/kaggle\/input\/us-gasoline-and-diesel-retail-prices-19952021\/PET_PRI_GND_DCUS_NUS_W.csv')\ndata_gaz.iloc[:, 0] = data_gaz.iloc[:, 0].astype('datetime64[ns]').dropna()\ndata = data_gaz.iloc[:, 1:]\ndate = data_gaz.iloc[:, 0]","da081dec":"import plotly.express as px\n\nfig = px.line(data_gaz, x='Date', y=data_gaz.columns[1:],\n              hover_data={\"Date\": \"|%B %d, %Y\"},\n              title='U.S Gaz over years')\n\nfig.update_layout(\n    updatemenus=[\n        dict(\n            active=0,\n            buttons=list([dict(label=\"None\",\n                     method=\"update\",\n                     args=[{\"visible\": [True for _ in range(12)]},\n                           {\"title\": \"all U.S gaz over years\",\n                            \"annotations\": []}])]) + list([\n                dict(label=f\"{j}\",\n                     method=\"update\",\n                     args=[{\"visible\": [True if i==idx else False for i in range(12)]},\n                           {\"title\": f\"{j}\",\n                            \"annotations\": []}]) for idx,j in enumerate(data.columns[1:])])\n            )])\n\nfig.show()","a6f515b3":"back_horizon = 3 * 52\nhorizon = 52\n\ndataset = DataSet(horizon, back_horizon)\ndataset.preprocessing(data.values, date, train_size=0.6, val_size=0.2)","4301ccce":"def model_builder(hp):\n    \n    hp_n_neurons_trend = hp.Int('neurons_trend', min_value = 4, max_value=16, step=2)\n    hp_n_neurons_seas = hp.Int('neurons_seas', min_value = 4, max_value=16, step=2)\n    hp_periods = hp.Int('periods', min_value = 1, max_value = horizon, step = 30)\n    hp_back_periods = hp.Int('back_periods', min_value = 1, max_value = back_horizon, step = 30)\n    hp_share = hp.Boolean('share')\n    \n    model_nbeats = create_interpretable_nbeats(horizon=horizon, \n                                               back_horizon=back_horizon,\n                                               p_degree=1,   \n                                               trend_n_neurons=hp_n_neurons_trend, \n                                               seasonality_n_neurons=hp_n_neurons_seas, \n                                               periods=hp_periods, \n                                               back_periods=hp_back_periods, \n                                               horizon_fourier_order=hp_periods,\n                                               back_horizon_fourier_order=hp_back_periods,\n                                               n_quantiles=3, \n                                               share=hp_share)\n\n    model_nbeats.compile(loss=QuantileLossError([0.1, 0.5, 0.9]), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                         metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\n    return model_nbeats\n\ntuner = kt.Hyperband(model_builder,\n                     objective = 'val_mean_absolute_error', \n                     max_epochs = 10,\n                     factor = 3,\n                     directory = 'my_dir',\n                     project_name = 'gaz')\n\nclass ClearTrainingOutput(tf.keras.callbacks.Callback):\n    def on_train_end(*args, **kwargs):\n        IPython.display.clear_output(wait = True)\n\ntuner.search(dataset.X_train, dataset.y_train, epochs = 20, \n             validation_data = (dataset.X_val, dataset.y_val), callbacks = [ClearTrainingOutput()])","2b5c4067":"best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n\n# Build the model with the optimal hyperparameters and train it on the data\nmodel = tuner.hypermodel.build(best_hps)\nmodel.build((None, back_horizon))\nmodel.summary()\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.001)\nes_cb = EarlyStopping(monitor='val_loss', min_delta=0,  patience=10, verbose=0, mode='auto', restore_best_weights=True)\n\nhistory = model.fit(dataset.X_train, dataset.y_train, batch_size=64, epochs=10, \n                    callbacks = [reduce_lr, es_cb], validation_data = (dataset.X_val, dataset.y_val))\n","f22814f6":"fig = plt.figure(figsize=(20,5))\n\nax = plt.subplot(131)\n\nepochs = [i for i in range(len(history.history['loss']))]\nax.plot(epochs, history.history['loss'], label='loss')\nax.plot(epochs, history.history['val_loss'], label='val_loss')\nplt.legend()\n\nax = plt.subplot(132)\nax.plot(epochs, history.history['mean_absolute_error'], label='mean_absolute_error')\nax.plot(epochs, history.history['val_mean_absolute_error'], label='val_mean_absolute_error')\nplt.legend()\n\nax = plt.subplot(133)\nax.plot(epochs, history.history['lr'], label='lr')\nb = plt.legend()","c1d2c138":"outputs = []\nseasonality = []\ntrend = []\nfor sample in range(50):\n    results = model(dataset.X_test)\n    outputs.append(results)\n    seasonality.append(model.seasonality)\n    trend.append(model.trend)\n    \nplot_results_nbeats(y_pred=tf.stack(outputs),\n                    date_outputs=np.apply_along_axis(lambda x: pd.to_datetime(x, format='%Y-%M-%d'), -1, dataset.test_date),\n                    y_true=dataset.y_test,\n                    date_history=None,\n                    seasonality=tf.stack(seasonality),\n                    trend=tf.stack(trend))","78261402":"outputs = []\nseasonality = []\ntrend = []\nfor sample in range(50):\n    results = model(data.iloc[-156:].T.values.astype('float'))\n    outputs.append(results)\n    seasonality.append(model.seasonality)\n    trend.append(model.trend)","6a2fcc84":"date_forecasting = np.expand_dims(pd.date_range(date.iloc[-1], date.iloc[-1] + pd.Timedelta(52, 'W'), freq='W'), axis=0)\n\nplot_results_nbeats(y_pred=tf.stack(outputs),\n                    date_outputs=date_forecasting,\n                    y_true=data.T.values,\n                    date_history=date,\n                    seasonality=tf.stack(seasonality),\n                    trend=tf.stack(trend))","a99e8eb1":"import seaborn as sns\n\ndata = pd.read_csv('\/kaggle\/input\/covid-world-vaccination-progress\/country_vaccinations.csv')\ndata = data.pivot(index='date', columns='country', values='daily_vaccinations_per_million')\n\nx = data.notna().sum().sort_values()\ny = x.index\n\nsns.set_theme(style=\"whitegrid\")\n\n# Initialize the matplotlib figure\nf, (ax1, ax2)  = plt.subplots(1,2,figsize=(12, 35), sharey='all')\n\n# Plot the available data\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=x, y=y, color=\"b\",label=\"available day data\", ax=ax1)\n\nax1.legend(ncol=2, frameon=True)\nax1.set(xlim=(0, 74), ylabel=\"Countries\")\n\nc = [ 'r' if val in data.sum().sort_values().values[-5:] else 'b' for val in data.sum().loc[y]]\n\nb = sns.barplot(x=data.sum().loc[y], y=y.values,\n            palette=c, ax=ax2, label=\"daily_vaccinations_per_million\")\nax2.legend(ncol=2, frameon=True)\n\nsns.despine(left=True, bottom=True)\n    ","0ff7fbe5":"df = data.loc[:,x.index[-5:]].dropna().reset_index()\nfig = px.line(df, x='date', y=df.columns[1:],\n              hover_data={\"date\": \"|%B %d, %Y\"},\n              title='Covid vaccinations over days')\n\nfig.update_layout(\n    updatemenus=[\n        dict(\n            active=0,\n            buttons=list([dict(label=\"None\",\n                     method=\"update\",\n                     args=[{\"visible\": [True for _ in range(5)]},\n                           {\"title\": \"all U.S gaz over years\",\n                            \"annotations\": []}])]) + list([\n                dict(label=f\"{j}\",\n                     method=\"update\",\n                     args=[{\"visible\": [True if i==idx else False for i in range(5)]},\n                           {\"title\": f\"{j}\",\n                            \"annotations\": []}]) for idx,j in enumerate(df.columns[1:])])\n            )])\n\nfig.show()\n","355a77ee":"data = data.loc[:,x.index[-5:]].dropna()\ndate = pd.to_datetime(data.index)\n\nback_horizon = 1 * 7\nhorizon = 7\n\ndataset = DataSet(horizon, back_horizon)\ndataset.preprocessing(data.values, date, train_size=0.7, val_size=0.2)","1e825f7a":"def model_builder(hp):\n    \n    hp_n_neurons_trend = hp.Int('neurons_trend', min_value = 4, max_value=16, step=2)\n    hp_n_neurons_seas = hp.Int('neurons_seas', min_value = 4, max_value=16, step=2)\n    hp_periods = hp.Int('periods', min_value = 1, max_value = horizon, step = 30)\n    hp_back_periods = hp.Int('back_periods', min_value = 1, max_value = back_horizon, step = 30)\n    hp_share = hp.Boolean('share')\n    \n    model_nbeats = create_interpretable_nbeats(horizon=horizon, \n                                               back_horizon=back_horizon,\n                                               p_degree=1,   \n                                               trend_n_neurons=hp_n_neurons_trend, \n                                               seasonality_n_neurons=hp_n_neurons_seas, \n                                               periods=hp_periods, \n                                               back_periods=hp_back_periods, \n                                               horizon_fourier_order=hp_periods,\n                                               back_horizon_fourier_order=hp_back_periods,\n                                               n_quantiles=3, \n                                               share=hp_share)\n\n    model_nbeats.compile(loss=QuantileLossError([0.1, 0.5, 0.9]), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                         metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\n    return model_nbeats\n\ntuner = kt.Hyperband(model_builder,\n                     objective = 'val_mean_absolute_error', \n                     max_epochs = 10,\n                     factor = 3,\n                     directory = 'my_dir',\n                     project_name = 'covid')\n\nclass ClearTrainingOutput(tf.keras.callbacks.Callback):\n    def on_train_end(*args, **kwargs):\n        IPython.display.clear_output(wait = True)\n\ntuner.search(dataset.X_train, dataset.y_train, epochs = 20, \n             validation_data = (dataset.X_val, dataset.y_val), callbacks = [ClearTrainingOutput()])","a795ee4f":"best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n\n# Build the model with the optimal hyperparameters and train it on the data\nmodel = tuner.hypermodel.build(best_hps)\nmodel.build((None, back_horizon))\nmodel.summary()\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.001)\nes_cb = EarlyStopping(monitor='val_loss', min_delta=0,  patience=10, verbose=0, mode='auto', restore_best_weights=True)\n\nhistory = model.fit(dataset.X_train, dataset.y_train, batch_size=64, epochs=10, \n                    callbacks = [reduce_lr, es_cb], validation_data = (dataset.X_val, dataset.y_val))\n","00b32324":"fig = plt.figure(figsize=(20,5))\n\nax = plt.subplot(131)\n\nepochs = [i for i in range(len(history.history['loss']))]\nax.plot(epochs, history.history['loss'], label='loss')\nax.plot(epochs, history.history['val_loss'], label='val_loss')\nplt.legend()\n\nax = plt.subplot(132)\nax.plot(epochs, history.history['mean_absolute_error'], label='mean_absolute_error')\nax.plot(epochs, history.history['val_mean_absolute_error'], label='val_mean_absolute_error')\nplt.legend()\n\nax = plt.subplot(133)\nax.plot(epochs, history.history['lr'], label='lr')\nb = plt.legend()","c3a2d614":"outputs = []\nseasonality = []\ntrend = []\nfor sample in range(50):\n    results = model(dataset.X_test)\n    outputs.append(results)\n    seasonality.append(model.seasonality)\n    trend.append(model.trend)\n    \nplot_results_nbeats(y_pred=tf.stack(outputs),\n                    date_outputs=np.apply_along_axis(lambda x: pd.to_datetime(x, format='%Y-%M-%d'), -1, dataset.test_date),\n                    y_true=dataset.y_test,\n                    date_history=None,\n                    seasonality=tf.stack(seasonality),\n                    trend=tf.stack(trend))","c881a260":"outputs = []\nseasonality = []\ntrend = []\nfor sample in range(50):\n    results = model(data.iloc[-7:].T.values.astype('float'))\n    outputs.append(results)\n    seasonality.append(model.seasonality)\n    trend.append(model.trend)","88071f53":"date_forecasting = np.expand_dims(pd.date_range(date[-1], date[-1] + pd.Timedelta(7, 'D'), freq='D'), axis=0)\n\nplot_results_nbeats(y_pred=tf.stack(outputs),\n                    date_outputs=date_forecasting,\n                    y_true=data.T.values,\n                    date_history=date,\n                    seasonality=tf.stack(seasonality),\n                    trend=tf.stack(trend))","0b21fed1":"data = pd.read_csv('\/kaggle\/input\/40-years-of-air-quality-index-from-the-epa-daily\/aqi_daily_1980_to_2021.csv')\ndata = pd.pivot_table(data.query('`Defining Parameter`==\"Ozone\"')[['State Name','AQI', 'County Name', 'Date']], \n               index='Date', columns=['State Name', 'County Name'])\n\ndata_final = data.loc[:, data.notna().sum().sort_values().index[-1]].dropna()","f9236af9":"data_to_plot = data_final.to_frame().reset_index()\ndata_to_plot.columns = ['Date', 'AQI']\n\nfig = px.line(data_to_plot, x='Date', y='AQI',\n              hover_data={\"Date\": \"|%B %d, %Y\"},\n              title='Ozone index per day')\n\nfig.show()","5b03e930":"back_horizon = 3 * 120\nhorizon = 120\n\ndataset = DataSet(horizon, back_horizon)\ndataset.preprocessing(data_final.values.reshape(-1, 1), data_final.index, train_size=0.7, val_size=0.2)","3091bffe":"def model_builder(hp):\n    \n    hp_n_neurons_trend = hp.Int('neurons_trend', min_value = 4, max_value=16, step=2)\n    hp_n_neurons_seas = hp.Int('neurons_seas', min_value = 4, max_value=16, step=2)\n    hp_periods = hp.Int('periods', min_value = 1, max_value = horizon, step = 30)\n    hp_back_periods = hp.Int('back_periods', min_value = 1, max_value = back_horizon, step = 30)\n    hp_share = hp.Boolean('share')\n    \n    model_nbeats = create_interpretable_nbeats(horizon=horizon, \n                                               back_horizon=back_horizon,\n                                               p_degree=1,   \n                                               trend_n_neurons=hp_n_neurons_trend, \n                                               seasonality_n_neurons=hp_n_neurons_seas, \n                                               periods=hp_periods, \n                                               back_periods=hp_back_periods, \n                                               horizon_fourier_order=hp_periods,\n                                               back_horizon_fourier_order=hp_back_periods,\n                                               n_quantiles=3, \n                                               share=hp_share)\n\n    model_nbeats.compile(loss=QuantileLossError([0.1, 0.5, 0.9]), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                         metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\n    return model_nbeats\n\ntuner = kt.Hyperband(model_builder,\n                     objective = 'val_mean_absolute_error', \n                     max_epochs = 10,\n                     factor = 3,\n                     directory = 'my_directory',\n                     project_name = 'index')\n\nclass ClearTrainingOutput(tf.keras.callbacks.Callback):\n    def on_train_end(*args, **kwargs):\n        IPython.display.clear_output(wait = True)\n\ntuner.search(dataset.X_train, dataset.y_train, epochs = 70, \n             validation_data = (dataset.X_val, dataset.y_val), callbacks = [ClearTrainingOutput()])","c3a58102":"best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n\n# Build the model with the optimal hyperparameters and train it on the data\nmodel = tuner.hypermodel.build(best_hps)\nmodel.build((None, back_horizon))\nmodel.summary()\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.001)\nes_cb = EarlyStopping(monitor='val_loss', min_delta=0,  patience=10, verbose=0, mode='auto', restore_best_weights=True)\n\nhistory = model.fit(dataset.X_train, dataset.y_train, batch_size=32, epochs=30, \n                    callbacks = [reduce_lr, es_cb], validation_data = (dataset.X_val, dataset.y_val))\n","bcf0e32a":"fig = plt.figure(figsize=(20,5))\n\nax = plt.subplot(131)\n\nepochs = [i for i in range(len(history.history['loss']))]\nax.plot(epochs, history.history['loss'], label='loss')\nax.plot(epochs, history.history['val_loss'], label='val_loss')\nplt.legend()\n\nax = plt.subplot(132)\nax.plot(epochs, history.history['mean_absolute_error'], label='mean_absolute_error')\nax.plot(epochs, history.history['val_mean_absolute_error'], label='val_mean_absolute_error')\nplt.legend()\n\nax = plt.subplot(133)\nax.plot(epochs, history.history['lr'], label='lr')\nb = plt.legend()","78f17cac":"outputs = []\nseasonality = []\ntrend = []\nfor sample in range(50):\n    results = model(dataset.X_test)\n    outputs.append(results)\n    seasonality.append(model.seasonality)\n    trend.append(model.trend)\n    \nplot_results_nbeats(y_pred=tf.stack(outputs),\n                    date_outputs=dataset.test_date,\n                    y_true=dataset.y_test,\n                    date_history=None,\n                    seasonality=tf.stack(seasonality),\n                    trend=tf.stack(trend))","05e73e82":"outputs = []\nseasonality = []\ntrend = []\nfor sample in range(50):\n    results = model(data_final.iloc[-360:].values.astype('float').reshape(1, -1))\n    outputs.append(results)\n    seasonality.append(model.seasonality)\n    trend.append(model.trend)","8e254a88":"date = data_final.dropna().index\n\ndate_forecasting = np.expand_dims(pd.date_range(date[-1], pd.to_datetime(date[-1]) + pd.Timedelta(120, 'D'), freq='D'), axis=0)\n\nplot_results_nbeats(y_pred=tf.stack(outputs),\n                    date_outputs=date_forecasting,\n                    y_true=data_final.T.values.reshape(1, -1),\n                    date_history=date,\n                    seasonality=tf.stack(seasonality),\n                    trend=tf.stack(trend))","73861c15":"![image.png](attachment:image.png) \n\n<div class=\"alert alert-info\" role=\"alert\"> <a style=\"font-weight: bold;\">Alert:<\/a> image taken from the research paper.<\/div>","31e2a341":"We try to forecast 120 days","7673928b":"Let's dive into the example, here we took temperature change over 50 years and 186 countries.","424fe976":"In the same way as the trend we constrain $g^f$ and $g^b$. A natural choice for the basis to model periodic function is the Fourier series:\n\n<p style=\"text-align: center\" align='justify'>$\\widehat{y}_{s,l}$$=\\sum_{i=0}^{fourier\\ order}{\\theta_{s,l,i}^{f}\\times \\cos{2\\pi i T}+\\theta_{s,l,i+H\/2}^{f}\\times \\sin{2\\pi i T}}$<\/p>\n\n\n$T=\\frac{[0,1...,H-1]}{H}$ of shape $(, horizon)$\n    <br>\n\nMatrix form: \n<p style=\"text-align: center\" align='justify'>\n$\\widehat{y}_{s,l}$$=\\theta_{s,l}^fS$<\/p>\n\n\\begin{equation*}\n\\theta^f_{s,l} = \n\\begin{pmatrix}\n\\theta_{0,s,l,0} & \\theta_{0,s,l,1} & \\cdots & \\theta_{0,s,l,2 \\times Fourier\\ order} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\theta_{batch\\ size,s,l,0} & \\theta_{batch\\ size,s,l,1} & \\cdots & \\theta_{batch\\ size,s,l,2 \\times Fourier\\ order} \\\\\n\\end{pmatrix}\n\\end{equation*}\n\n\\begin{equation*}\nS = \n\\begin{pmatrix}\n\\cos(2\\pi\\ \\times 0\\ \\times 0\/H) & \\cdots & \\cdots & \\cos(2\\pi\\ \\times 0\\ \\times H-1\/H \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\cos(2\\pi\\ \\times [\\frac{H}{2}-1]\\ \\times0\/H) & \\cdots & \\cdots & \\cos(2\\pi\\ \\times [\\frac{H}{2}-1]\\ \\times H-1\/H) \\\\\n\\sin(2\\pi\\ \\times 0\\ \\times 0\/H) & \\cdots & \\cdots & \\sin(2\\pi\\ \\times 0\\ \\times H-1\/H) \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\sin(2\\pi\\ \\times [\\frac{H}{2}-1]\\ \\times 0\/H) & \\cdots & \\cdots & \\sin(2\\pi\\ \\times [\\frac{H}{2}-1]\\ \\times H-1\/H) \\\\\n\\end{pmatrix}\n\\end{equation*}\n\n<br>\nwith $\\theta_{s,l}^f$ of shape $(batch\\ size, fourier\\ order)$ (In the paper fourier order = H\/2).\n    <br>\n\nand S of shape $(2 \\times fourier\\ order, horizon)$. $2 \\times fourier\\ order$ because the matrix holds sin and cos coefficents. \n\nLeads to $\\widehat{y}$ of shape $(batch\\ size, horizon)$\n\nWe have modified the code so that the model integrates the possibility of adding seasonality periods and to modify the fourier order for each period. The linear nature of this model let us adding these features. Let's define p as the period and fourier_order the fourier order computed. \n\nMatrix become:\n\n\\begin{equation*}\n\\theta^f_{s,l} = \n\\begin{pmatrix}\n\\theta_{0,s,l,period\\ 1,0} & \\cdots & \\theta_{0,s,l,period\\ 1,2 \\times Fourier\\ order}  & \\cdots & \\theta_{0,s,l,period\\ n,2 \\times Fourier\\ order} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\theta_{batch\\ size,s,l,period\\ 1,0} & \\cdots & \\theta_{batch\\ size,s,l, period\\ 1, 2 \\times Fourier\\ order} & \\cdots & \\theta_{batch\\ size,s,l, period\\ n, 2 \\times Fourier\\ order} \\\\\n\\end{pmatrix}\n\\end{equation*}\n\n\\begin{equation*}\nS = \n\\begin{pmatrix}\n\\cos(2\\pi\\ \\times 0\\ \\times 0\/H)_{period\\ 1} & \\cdots & \\cdots & \\cos(2\\pi\\ \\times 0\\ \\times H-1\/H)_{period\\ 1} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\cos(2\\pi\\ \\times [\\frac{H}{2}-1]\\ \\times 0\/H)_{period\\ 1}  & \\cdots & \\cdots & \\cos(2\\pi\\ \\times [\\frac{H}{2}-1]\\ \\times H-1\/H)_{period\\ 1}  \\\\\n\\sin(2\\pi\\ \\times 0\\ \\times 0\/H)_{period\\ 1}  & \\cdots & \\cdots & \\sin(2\\pi\\ \\times 0\\ \\times H-1\/H)_{period\\ 1}  \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\sin(2\\pi\\ \\times [\\frac{H}{2}-1]\\ \\times 0\/H)_{period\\ 1}  & \\cdots & \\cdots & \\sin(2\\pi\\ \\times [\\frac{H}{2}-1]\\ \\times H-1\/H)_{period\\ 1}  \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\sin(2\\pi\\ \\times [\\frac{H}{2}-1]\\ \\times 0\/H)_{period\\ n}  & \\cdots & \\cdots & \\sin(2\\pi\\ \\times [\\frac{H}{2}-1]\\ \\times H-1\/H)_{period\\ n}  \\\\\n\\end{pmatrix}\n\\end{equation*}","1154fa85":"It could be obvious but in case of few data it's better to use back_horizon = horizon","e888436d":"<h2 style=\"text-align: center; font-size:20px;\"> What is the trend ? <\/h2><a id=b><\/a>","dd505b40":"<div class=\"panel panel-default\">\n   <div class=\"panel-heading\">Authors defined multiple loss functions to use with the model.<\/div>\n    <div class=\"panel-body\"> <p>However, we used the quantile loss error in order to estimate a prediction interval. This loss function is not really adapted here as it tends to zeros our weights, in order to prevent this phenomenom we squared the error.\n<\/p>\n<div class=\"list-group\">\n  <a href=\"https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_percentage_error#:~:text=The%20mean%20absolute%20percentage%20error%20(MAPE),%20also%20known,loss%20function%20for%20regression%20problems%20in%20machine%20learning\" class=\"list-group-item\">MAPE<\/a>\n  <a href=\"https:\/\/en.wikipedia.org\/wiki\/Symmetric_mean_absolute_percentage_error\" class=\"list-group-item\">SMAPE<\/a>\n  <a href=\"https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_scaled_error\" class=\"list-group-item\">MASE<\/a>\n  <a href=\"https:\/\/arxiv.org\/pdf\/1905.10437.pdf\" class=\"list-group-item\">OWA<\/a>\n  <a href=\"https:\/\/arxiv.org\/abs\/1912.09363v2\" class=\"list-group-item\">Quantile loss<\/a>\n<\/div><\/div><\/div>","f08935c5":"The following cell defines the implementation of the plot function.","b6f12ff2":"Let's infer","6ee1e224":"We now plot the results.","2edb31ed":"\n<center><img src=\"https:\/\/help.xlstat.com\/servlet\/rtaImage?eid=ka0080000002LJS&feoid=00N1p00000JrJra&refid=0EM08000002m1SU\">\n    \nIn statistical terms, the demand for - or sales of - a given product is said to exhibit seasonality when the underlying time series undergoes a predictable cyclical variation depending on the time of year. Seasonality is one of the most frequently used statistical behaviours to improve the accuracy of demand horizons.\n","87c81c2a":"<h2 style=\"text-align: center; font-size:50px; background-color: #428bca; color:#FFFFFF\"> Generic <\/h2><a id=d><\/a>\n<hr>","7b017d32":"Training and test phase","6c50a6ee":"\n<center><img src=\"https:\/\/1.bp.blogspot.com\/-R7Ts4x5l6ew\/X37AGoLiU1I\/AAAAAAAAQWs\/cM0W6FUOM34WzaAKppG5amRERuXVYgLWwCLcBGAsYHQ\/s1200\/1.jpeg\">\n\nWe introduce dropout in order to regularize our model and to calculate uncertainty by keeping it active during inference. This is called confidence interval and it catched the epistemic errors which occur because our sample is not enough representative of the reality. This can be decreased thanks to new examples. \n\nHowever, the aleotoric error still exists. This is an irreductible fluctuation due to variables that we can't have access to, like error due to sensors. We can catch it by creating prediction intervals which can be estimated by using the Quantile loss error. I took it from the TFT paper: https:\/\/arxiv.org\/abs\/1912.09363v2","53ee8271":"<h2 style=\"text-align: center; font-size:50px; background-color: #428bca; color:#FFFFFF\"> Dataset preprocessing <\/h2><a id=e><\/a>\n<hr>","6b118b42":"<h2 style=\"text-align: center; font-size:50px; background-color: #428bca; color:#FFFFFF\"> NBEATS wrapper<\/h2><a id=e><\/a>\n<hr>","ee6d2f8c":"Hypertuning","4a3485a5":"We can visualize our test data","a87c538e":"<div class=\"panel panel-default\">\n   <div class=\"panel-heading\">Don't miss my other notebooks<\/div>\n<div class=\"list-group\">\n  <a href=\"https:\/\/www.kaggle.com\/gatandubuc\/california-traffic-collision-covid-impact\/comments\" class=\"list-group-item\">california traffic collision covid impact<\/a>\n  <a href=\"https:\/\/www.kaggle.com\/gatandubuc\/xgbclassifier-and-model-explainability\" class=\"list-group-item\">xgbclassifier and model explainability<\/a>\n    <a class=\"list-group-item\">Coming soon: horizon with TFT || Interpretable model<\/a>\n<\/div><\/div><\/div>","084e00ee":"Hypertunng of the model","9efd0ce0":"<h2 style=\"text-align: center; font-size:50px; background-color: #428bca; color:#FFFFFF\"> Trend <\/h2><a id=b><\/a>\n<hr>","5a458b98":"Hypertuning of our model","20cb5e23":"<h2 style=\"text-align: center; font-size:50px; color:#428bca\"> Example 2: U.S gaz <\/h2><a id=h><\/a>\n<hr>","36925526":"This part defines wrapper to create generic model and interpretable model","9e7f2207":"<h1 style=\"background-color: #428bca; color:#FFFFFF\">Introduction<\/h1>\n\nWe will see in this notebook an implementation of N-BEATS model with keras and tensorflow and 3 examples!\n\nThis notebook is based on [N-BEATS: NEURAL BASIS EXPANSION ANALYSIS FOR INTERPRETABLE TIME SERIES horizonING](https:\/\/arxiv.org\/abs\/1905.10437)\n\n> N-BEATS model is based on backward and forward residual links and a very deep stack of fully-connected layers. \n> Our architecture design methodology relies on a few key principles. First, the base architecture\nshould be simple and generic, yet expressive (deep). Second, the architecture should not rely on timeseries-specific feature engineering or input scaling. These prerequisites let us explore the potential\nof pure DL architecture in TS horizoning. Finally, as a prerequisite to explore interpretability, the\narchitecture should be extendable towards making its outputs human interpretable. We now discuss\nhow those principles converge to the proposed architecture.\n\n# Why this model?\n\nInterpretable models are essential tools for the engineer. they can help to better understand interactions between variables and can discover really cool insights. This one is the deep learning version of other more traditional models based on trend and seasonality but gives better results. \n\n# Some informations\nIt has been declined in two versions: generic model and interpretable model. The first one works as a black box while the second describes the serie as a function of trend and seasonality. They give approximately same results but they offer better performances when coupled by bagging method.\n\n# What will we do in this notebook?\nWe will demonstrate how to use the interpretable model (noted I) and how to tune it. Before diving into code details, let's focus on mathematics details and the overall structure.\n\n<div class=\"alert alert-info\" role=\"alert\"> <a style=\"font-weight: bold;\">Alert:<\/a> We slightly changed the implementation in order to integrate some additional stuffs.<\/div>\n\n","4935563b":"Loss curves of the training","a08c6384":"<h2 style=\"text-align: center; font-size:50px; background-color: #428bca; color:#FFFFFF\"> Epistemic and aleotoric error <\/h2><a id=e><\/a>\n<hr>","1d70bbca":"Let's do inference","eb41b93b":"<h1 style=\"text-align: center; font-size:50px; background-color: #428bca; color:#FFFFFF\"> Overall Structure <\/h1><a id=a><\/a>\n<hr>","f0930dfc":"Loss error","bf45b62b":"Load data","3be962bf":"Few words for generic block. Contrary to trend and seasonality blocks, this one doesn't have contrained g function and then the output can't be interpreted.","81bd493a":"<center><img src=\"https:\/\/www.ezyeducation.co.uk\/images\/Lexicon_Images\/trend_rate_growth.jpg\"\/><\/center>\nThe trend is often the first thing to be detected when analysing a time series. If you work in the fashion industry, you should know that this \"trend\" is the opposite of the ones you are familiar with, as it is an enduring phenomenon.\n\nThis is the general direction of a series of observations upwards or downwards over a fairly long period of time. When there is no direction, we say that there is no trend, which obviously does not mean that all values are the same...\n\nLet us recall a basic rule for the study of economic time series. It is the analytical decomposition: chronicle = trend + periodic variations + random residual variations (+ possible economic cycle). This assumes a certain regularity. In a company, it is often an evolution of volumes observed over time. We will not mention here the particular case of stock market trends, studied in the context of technical analysis and on which statistically unpredictable movements are grafted...","99c9810d":"This notebook shows you the implementation of nbeats model and a way to visualize the outputs decomposition with plotly.\nThanks to trend and seasonality blocks, authors propose an intepretable deep residual neural network. Through this 4 examples, we can see that the model is efficient on multiple kind of data and also with few instances.\n\nNot every time series fit very well with this kind of model. For exemple, Gaz US series are dependent of non predictable events called black swann.\n\nAlso, it is possible that the prediction interval is between the confidence interval... i'm working on it, if you see the problem text me :) \n\nTo reach 100% the last thing you need to do is to upvote my notebook! \n<div class=\"progress\">\n  <div class=\"progress-bar\" role=\"progressbar\" aria-valuenow=\"98\" aria-valuemin=\"0\" aria-valuemax=\"100\" style=\"width: 98%;\">\n    98%\n  <\/div>\n<\/div>\n\n\n","68fda931":"test plot","be7b8355":"Bayesian model could have been used here, this may be the subject of a new version.","ade5ed66":"We define horizon and back_horizon (1 year = 52 weeks)","96d58486":"Let's see our loss curves. It can be a bit messy if you take back_horizon = n>1 * horizon.","1b874ba2":"We can then decompose our outputs. Here, the overall trend doesn't mean anything because multiple periods are taken into account.","3bf7d880":"We load data and we take the 5 first countries with more values. The following graphs give us a quick overview of the available data.","ce448376":"The following cells gives you the overall structure code","95e81c65":"<div class=\"container-fluid\">\n    <img src=\"https:\/\/cdn.tv-programme.com\/pic\/episodes\/38\/387767.jpg\" class=\"center-block\">\n    <div class=\"carousel-caption\">\n        <div class=\"page-header\">\n          <h1><p style=\"text-align: center; font-size:50px; color:#FFFFFF\">N-BEATS <\/p><small STYLE='color:#FFFFFF; font-size:30px' >NEURAL BASIS EXPANSION ANALYSIS FOR\n        INTERPRETABLE TIME SERIES horizonING\n        <\/small><\/h1><\/div><\/div>\n<\/div>","3628856e":"Here is another of the NBeats model. I chose this example because the time series seems to be a function of a seasonality and trend pattern. For simplicity, we take the longest example (Ozone, louisiana and St. John the Baptist).","d0ecdb6a":"Our final plot :)","0acee270":"Train and test","15acf146":"<p align='justify'>\n    As we have now the intuition about the trend, we understand why they are constraining layers $g^f$ and $g^b$. We have to mimic a monotonic function, or at least a slowly varying one. Authors then take a polynomial function of small degree p:\n<\/p>\n\n<p style=\"text-align: center\" align='justify'>$\\widehat{y}_{s,l}$$=\\sum_{i=0}^p{\\theta_{s,l,i}^i\\times t^i}$\n<\/p>\n\nMatrix form: \n<p style=\"text-align: center\" align='justify'>\n$\\widehat{y}_{s,l}$$=\\theta_{s,l}^f T$<\/p>\n    <br>\nOur coefficents $\\theta_{s,l}^f$ of shape $(batch\\ size, p+1)$. \n    <br>\nand T of shape $(p+1, horizon)$\n\n\\begin{equation*}\n\\theta^f_{s,l} = \n\\begin{pmatrix}\n\\theta_{0,s,l,0} & \\cdots & \\theta_{0,s,l,p\\ degree} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\theta_{batch\\ size,s,l,0} & \\cdots & \\theta_{batch\\ size,s,l,p\\ degree} \\\\\n\\end{pmatrix}\n\\end{equation*}\n\n\\begin{equation*}\nT = \n\\begin{pmatrix}\n\\frac{0^0}{H} \\cdots & \\cdots & \\frac{(H-1)^0}{H}\\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n\\frac{0^p}{H} \\cdots & \\cdots & \\frac{(H-1)^p}{H}\\\\\n\\end{pmatrix}\n\\end{equation*}\n\nFinally, our function is a matrix multiplication between a constrained g layers defined as the time steps and coefficients theta known as polynomial coefficients.\n\nthat's lead to $\\widehat{y}$ of shape $(batch\\ size, horizon)$\n","6a71420a":"<h2 style=\"text-align: center; font-size:50px; background-color: #428bca; color:#FFFFFF\"> Conclusion <\/h2><a id=k><\/a>\n<hr>","6b182fad":"<h2 style=\"text-align: center; font-size:50px; background-color: #428bca; color:#FFFFFF\"> Seasonality <\/h2><a id=c><\/a>\n<hr>","99f099a8":"Loss curve during training","5d3a8c8e":"From the previous graphs, we can see an overall increasing of the temperature of almost 0.5 degrees over 10 years. We can also check up the trend and seasonality curves of each country.","b2d3cb3e":"Let's take 120 months of horizon and a double of history.","2eb16256":"<h1 style=\"background-color: #428bca; color:#FFFFFF\">Summary<\/h1>\n\n<h3 style=\"text-indent: 5vw; font-family: Verdana; font-size: 15px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"> <a href=\"#a\">I  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Overall structure <\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 5vw; font-family: Verdana; font-size: 15px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"> <a href=\"#b\">II &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Trend <\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 5vw; font-family: Verdana; font-size: 15px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"> <a href=\"#c\">III&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Seasonality <\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 5vw; font-family: Verdana; font-size: 15px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"> <a href=\"#d\">IV &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Generic <\/a><\/h3>\n\n--- \n\n<h3 style=\"text-indent: 5vw; font-family: Verdana; font-size: 15px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"> <a href=\"#e\">V  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dropout <\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 5vw; font-family: Verdana; font-size: 15px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"> <a href=\"#f\">   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Example 1: Temperature Change <\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 5vw; font-family: Verdana; font-size: 15px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"> <a href=\"#h\">   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Example 2: U.S gaz <\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 5vw; font-family: Verdana; font-size: 15px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"> <a href=\"#i\">   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Example 3: Covid vaccination <\/a><\/h3>\n\n\n---\n\n<h3 style=\"text-indent: 5vw; font-family: Verdana; font-size: 15px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"> <a href=\"#j\">   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Example 4: Ozone index <\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 5vw; font-family: Verdana; font-size: 15px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"> <a href=\"#k\">VI &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Conclusion <\/a><\/h3>","a58f5cf0":"<h2 style=\"text-align: center; font-size:50px;color:#428bca\"> Example 4: Index quality <\/h2><a id=j><\/a>\n<hr>","09856971":"\n\n<div align=\"justify\">\n    \n# Let's start the notebook with the overall structure of the model.\n    \nAs you can see in the previous image, we're dealing with a **deep residual Fully Connected model**. \n    \nThis is called **deep** because its internal layers are aggregated in blocks which themselves create stacks and all follow each other. \n    \nThis is called **residual** because every outputs are substract and used in a deeper part of the model but don't panic as we are introducing the interpretable model the structure won't be so deep.\n    \n# The input\nThe model is fed with a backcast period and predict a forecast period (also called back horizon and horizon). The recommanded equation between back horizon and horizon is:\n<\/div>\n\n<br>\n<p style=\"text-align: center\">$Back horizon = n \\times horizon$\n<br> With $n \\in [1,...,7]$<\/p>\n\nIn case of few data, it's better to use back horizon = horizon (Personal experience).\n\nWe can define our input matrix with shape $(Batch\\ size,\\ back\\ horizon)$\n\n# Internal layers\n\n![image.png](attachment:d9e3d242-b3e1-4632-a7de-d8d0e4e2554b.png)\n\n## What is a block ?\n\nFor sake of simplicity let's name a block **l**. We will use it to explain all the mathematical details as it is the same for every blocks.\n\nA block is just 4 Fully Connected layers (called FC) which gives rise to two forks. The first way is trying to rebuild the back_horizon input while the second one is trying to predict the horizon. We find for each way a FC layer which gives birth to $\\theta^f$ and $\\theta^b$ coefficients.\n\nThey corresponds to expansion coefficients and they can be seen as the trend and seasonality coefficients. $g^b$ and $g^f$ are basis vectors. then a linear combination between coefficents and basis vectors is enough to create a prediction.  \n\n## What is a stack ?\n\nOnly one block is enough to have a prediction so why is there more than one?\n    \nAs explained before this is its residual property which gives its robustness. Multiple blocks aligned give better results, the following ones are trying to predict the missing part of their predecessors and finally outputs are sum up. \n    \nThese blocks give a stack and this is the sum of multiple stacks which gives the final output. \nSo we end up with a global horizon of shape $(Batch\\ size,\\ horizon)$.\n\n# Recommandations\n\nAuthors recommand to use 2 stacks with 3 blocks each (for I model). One stack representing the trend and a second corresponding to the seasonality. Also, the weights can be shared inside a same stack. We will focus on the composition of each interpretable block.<\/div>","b7bd5159":"Test part","5346636b":"<h2 style=\"text-align: center; font-size:50px; background-color: #428bca; color:#FFFFFF\"> Plot <\/h2><a id=e><\/a>\n<hr>","1e565058":"It's time for inference","02486ec3":"<h2 style=\"text-align: center; font-size:50px;color:#428bca\"> Example 3: Covid vaccination <\/h2><a id=i><\/a>\n<hr>","aabc545b":"<h2 style=\"text-align: center; font-size:50px; color:#428bca\"> Example 1: Temperature change <\/h2><a id=f><\/a>\n<hr>","a6e999e8":"let's infer"}}