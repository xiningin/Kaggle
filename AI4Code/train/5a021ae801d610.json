{"cell_type":{"c1e7fe6b":"code","31d5a95c":"code","b8b50a36":"code","ee30b027":"code","688ef79f":"code","b949abfa":"code","68fa7511":"code","eb6cbee6":"code","3795f72e":"code","2acb6b7c":"code","72852606":"code","d955f78e":"code","e2fc1964":"markdown","79584dc5":"markdown","ab4d6bbf":"markdown","ec44b0ec":"markdown","c8aad3da":"markdown","00784dad":"markdown","6b0e774d":"markdown","4ae9cce8":"markdown","4cae37fd":"markdown","65fa14f5":"markdown","0e0ae63d":"markdown","ded24ee0":"markdown"},"source":{"c1e7fe6b":"!pip -q install vit_pytorch ","31d5a95c":"from __future__ import print_function, division\n\nimport numpy as np\nimport os\nimport glob\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom einops import rearrange\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision import models\nfrom torchvision import transforms\nfrom torch.utils.data import  WeightedRandomSampler\nfrom vit_pytorch import ViT\n\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport time\n  \n\nuse_gpu = torch.cuda.is_available()\nif use_gpu:\n    print(\"Using CUDA\")\n\ntorch.manual_seed(42)","b8b50a36":"# Data Parameters\nDATA_PATH = '..\/input\/kermany2018\/oct2017\/OCT2017 '\nTRAIN = 'train'\nVAL = 'val'\nTEST = 'test'\nDATASET_TYPE = [TRAIN, VAL, TEST]\n\n# Training parameters\nBATCH_SIZE = 32\nEPOCHS = 20\nLR = 0.0003\n\n# Model parameters\nIMAGE_SIZE = 224\nPATCH_SIZE = 32 \nDEPTH = 8","ee30b027":"# calculate training data mean and standard dev. to normalize the data\n\n# train_data = datasets.ImageFolder(os.path.join(DATA_PATH, TRAIN), \n#                                   transform = transforms.ToTensor())\n\n# means = torch.zeros(3)\n# stds = torch.zeros(3)\n\n# for img, _ in train_data:\n#     means += torch.mean(img, dim = (1,2))\n#     stds += torch.std(img, dim = (1,2))\n\n# means \/= len(train_data)\n# stds \/= len(train_data)\n    \n# print(f'Calculated means: {means}')\n# print(f'Calculated stds: {stds}')","688ef79f":"data_transforms = {\n    TRAIN: transforms.Compose([\n        # Data augmentation for the train set\n        # randomly crop the image to 224x224\n        # randomly flip it horizontally. \n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n    ]),\n    VAL: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ]),\n    TEST: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ]),\n}\n\nimage_datasets = {\n    x: datasets.ImageFolder(\n        os.path.join(DATA_PATH, x), \n        transform=data_transforms[x]\n    )\n    for x in DATASET_TYPE\n}\n\n\ndataset_sizes = {x: len(image_datasets[x]) for x in DATASET_TYPE}\n\nprint(\"Classes: \")\nprint(image_datasets[TRAIN].classes)","b949abfa":"# computing class weights for imbalanced data set\n\ntrain_targets = [sample[1] for sample in image_datasets[TRAIN].imgs]\ncounter = Counter(train_targets)\nclass_count = [i for i in counter.values()]\nclass_weights = 1.\/torch.tensor(class_count, dtype=torch.float)\ntrain_samples_weight = [class_weights[class_id] for class_id in train_targets]\ntrain_weighted_sampler = torch.utils.data.sampler.WeightedRandomSampler(\n    train_samples_weight, dataset_sizes[TRAIN])\n\n\n# plot training data imbalance\n\ncolors = ['blue', 'red', 'pink', 'turquoise']\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(image_datasets[TRAIN].classes, class_count, color=colors)\nax.set_title('Distribution of training set');","68fa7511":"train_loader = torch.utils.data.DataLoader(\n     image_datasets[TRAIN], \n     batch_size=BATCH_SIZE, \n     sampler=train_weighted_sampler,\n#     shuffle=True, \n     num_workers=4\n )\n\ntest_loader = torch.utils.data.DataLoader(\n    image_datasets[TEST], \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    num_workers=4\n)\n\n\n# print dataset size, class\nfor x in DATASET_TYPE:\n    print(\"Loaded {}  {} images\".format(dataset_sizes[x], x))\n","eb6cbee6":"# # import torch\n# # import torch.nn.functional as F\n# from einops import rearrange, repeat\n# # from torch import nn\n\n# MIN_NUM_PATCHES = 16\n\n# class Residual(nn.Module):\n#     def __init__(self, fn):\n#         super().__init__()\n#         self.fn = fn\n#     def forward(self, x, **kwargs):\n#         return self.fn(x, **kwargs) + x\n\n# class PreNorm(nn.Module):\n#     def __init__(self, dim, fn):\n#         super().__init__()\n#         self.norm = nn.LayerNorm(dim)\n#         self.fn = fn\n#     def forward(self, x, **kwargs):\n#         return self.fn(self.norm(x), **kwargs)\n\n# class FeedForward(nn.Module):\n#     def __init__(self, dim, hidden_dim, dropout = 0.):\n#         super().__init__()\n#         self.net = nn.Sequential(\n#             nn.Linear(dim, hidden_dim),\n#             nn.GELU(),\n#             nn.Dropout(dropout),\n#             nn.Linear(hidden_dim, dim),\n#             nn.Dropout(dropout)\n#         )\n#     def forward(self, x):\n#         return self.net(x)\n\n# class Attention(nn.Module):\n#     def __init__(self, dim, heads = 8, dropout = 0.):\n#         super().__init__()\n#         self.heads = heads\n#         self.scale = dim ** -0.5\n\n#         self.to_qkv = nn.Linear(dim, dim * 3, bias = False)\n#         self.to_out = nn.Sequential(\n#             nn.Linear(dim, dim),\n#             nn.Dropout(dropout)\n#         )\n\n#     def forward(self, x, mask = None):\n#         b, n, _, h = *x.shape, self.heads\n#         qkv = self.to_qkv(x).chunk(3, dim = -1)\n#         q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n\n#         dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n\n#         if mask is not None:\n#             mask = F.pad(mask.flatten(1), (1, 0), value = True)\n#             assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n#             mask = mask[:, None, :] * mask[:, :, None]\n#             dots.masked_fill_(~mask, float('-inf'))\n#             del mask\n\n#         attn = dots.softmax(dim=-1)\n\n#         out = torch.einsum('bhij,bhjd->bhid', attn, v)\n#         out = rearrange(out, 'b h n d -> b n (h d)')\n#         out =  self.to_out(out)\n#         return out\n\n# class Transformer(nn.Module):\n#     def __init__(self, dim, depth, heads, mlp_dim, dropout):\n#         super().__init__()\n#         self.layers = nn.ModuleList([])\n#         for _ in range(depth):\n#             self.layers.append(nn.ModuleList([\n#                 Residual(PreNorm(dim, Attention(dim, heads = heads, dropout = dropout))),\n#                 Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)))\n#             ]))\n#     def forward(self, x, mask = None):\n#         for attn, ff in self.layers:\n#             x = attn(x, mask = mask)\n#             x = ff(x)\n#         return x\n\n# class ViT(nn.Module):\n#     def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dropout = 0., emb_dropout = 0.):\n#         super().__init__()\n#         assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n#         num_patches = (image_size \/\/ patch_size) ** 2\n#         patch_dim = channels * patch_size ** 2\n#         assert num_patches > MIN_NUM_PATCHES, f'your number of patches ({num_patches}) is way too small for attention to be effective. try decreasing your patch size'\n\n#         self.patch_size = patch_size\n\n#         self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n#         self.patch_to_embedding = nn.Linear(patch_dim, dim)\n#         self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n#         self.dropout = nn.Dropout(emb_dropout)\n\n#         self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout)\n\n#         self.to_cls_token = nn.Identity()\n\n#         self.mlp_head = nn.Sequential(\n#             nn.LayerNorm(dim),\n#             nn.Linear(dim, mlp_dim),\n#             nn.GELU(),\n#             nn.Dropout(dropout),\n#             nn.Linear(mlp_dim, num_classes)\n#         )\n\n#     def forward(self, img, mask = None):\n#         p = self.patch_size\n\n#         x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n#         x = self.patch_to_embedding(x)\n#         b, n, _ = x.shape\n\n#         cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n#         x = torch.cat((cls_tokens, x), dim=1)\n#         x += self.pos_embedding[:, :(n + 1)]\n#         x = self.dropout(x)\n\n#         x = self.transformer(x, mask)\n\n#         x = self.to_cls_token(x[:, 0])\n#         return self.mlp_head(x)\n","3795f72e":"def train_epoch(model, optimizer, data_loader, loss_history):\n    \n    total_samples = len(data_loader.dataset)\n    model.train()\n    total_loss = 0\n\n\n    for i, (data, target) in enumerate(data_loader):\n        optimizer.zero_grad()\n        output = F.log_softmax(model(data), dim=1)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        if i % 1000 == 0:\n            print('[' +  '{:5}'.format(i * len(data)) + '\/' + '{:5}'.format(total_samples) +\n                  ' (' + '{:3.0f}'.format(100 * i * len(data) \/ total_samples) + '%)]  Loss: ' +\n                  '{:6.4f}'.format(loss.item()))\n            # loss_history.append(loss.item())\n    \n    # average loss per epoch\n    avg_loss = total_loss \/ total_samples\n    loss_history.append(avg_loss)","2acb6b7c":"def evaluate(model, data_loader, loss_history):\n    model.eval()\n    \n    total_samples = len(data_loader.dataset)\n    correct_samples = 0\n    total_loss = 0\n\n    with torch.no_grad():\n        for data, target in data_loader:\n            output = F.log_softmax(model(data), dim=1)\n            loss = F.nll_loss(output, target, reduction='sum')\n            _, pred = torch.max(output, dim=1)\n            \n            total_loss += loss.item()\n            correct_samples += pred.eq(target).sum()\n\n    avg_loss = total_loss \/ total_samples\n    loss_history.append(avg_loss)\n    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n          '  Accuracy:' + '{:5}'.format(correct_samples) + '\/' +\n          '{:5}'.format(total_samples) + ' (' +\n          '{:4.2f}'.format(100.0 * correct_samples \/ total_samples) + '%)\\n')","72852606":"start_time = time.time()\n\nmodel = ViT(image_size=IMAGE_SIZE, patch_size=PATCH_SIZE, num_classes=4, channels=3,\n            dim=64, depth=DEPTH, heads=8, mlp_dim=128)\noptimizer = optim.Adam(model.parameters(), lr=LR)\n\ntrain_loss_history, test_loss_history = [], []\nfor epoch in range(1, EPOCHS + 1):\n    print('Epoch:', epoch)\n    train_epoch(model, optimizer, train_loader, train_loss_history)\n    evaluate(model, test_loader, test_loss_history)\n\nprint('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')","d955f78e":"# plot loss function\n\n# plt.plot(np.array(train_loss_history), label='train loss') # need to fix\nplt.plot(np.array(test_loss_history), label='test loss')\nplt.xticks(range(EPOCHS), range(1, EPOCHS+1))\nplt.title('Test Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend();","e2fc1964":"## IV. Training","79584dc5":"### Loading the data set","ab4d6bbf":"## OCT retinal data classification using the ViT vision transformer","ec44b0ec":"### Image augmentation","c8aad3da":"## III. Model","00784dad":"### Imbalanced taining data","6b0e774d":"The paper \u201cAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\u201d, is the first paper to describe a pure Transformer (one which does not rely on CNNs) for Image classification tasks. It is currently (October 2020) under review at ICLR but a PyTorch implementation by Phil Wang is available on Github: lucidrains\/vit-pytorch.  \n\nIn this notebook, we will apply this Vision Transformer model to classify Optical Coherence Tomography (OCT) retinal data. This exercise is inspired by a [recent article](https:\/\/towardsdatascience.com\/a-demonstration-of-using-vision-transformers-in-pytorch-mnist-handwritten-digit-recognition-407eafbc15b0) which applied the Vision Transformer to the MNIST dataset.\n\n**Data:** The OCT dataset consists of about 83K high resolution training images. There are four classes of data: DME, CNV, Drusen and Normal. The data distribution is highly imbalanced so this is one of the challenges to classification. \n\n**Model:** In the Vision Transformer, an image is divided into patches. These patches are analogous to a sequence of tokens\/words in a language transformer model. The patch size is one of several tunable hyperparameters.  Here we will experiment with this parameter as well as the learning rate, batch size, and depth to get a reasonable baseline. The model we use is available on Github as previously stated.\n\n**Training:** We will train the model from scratch.\n\n**Evaluation:** We will evaluate the model on the test data. ","4ae9cce8":"## II. Preprocessing image data","4cae37fd":"### Imports","65fa14f5":"### Globals","0e0ae63d":"## I. Setup","ded24ee0":"The [model architecture](https:\/\/www.youtube.com\/watch?v=TrdevFK_am4) is described by Yannic Kilcher.\nThe model implementation we use is taken from the [Github repository by Phil Wang](https:\/\/github.com\/lucidrains\/vit-pytorch). \n\nUsage:\n```\n$ pip install vit-pytorch   \nfrom vit_pytorch import ViT  \n\nv = ViT(\n    image_size = 256,  \n    patch_size = 32,  # num_patches = (image_size \/\/ patch_size) ** 2  \n    num_classes = 1000,  \n    dim = 1024,  \n    depth = 6,  \n    heads = 8,  \n    mlp_dim = 2048,   \n    dropout = 0.1,   \n    emb_dropout = 0.1  \n)  \n```\nThe model is reproduced below."}}