{"cell_type":{"f420756d":"code","db93c4c0":"code","41e61698":"code","794c49ed":"code","4e5b2249":"code","e92b4208":"code","c782de0a":"code","69028424":"code","2c725637":"code","045faa59":"code","83dfb47c":"code","b23fec65":"code","6b140106":"code","09e8b10c":"code","0d68555a":"code","ad62eca3":"code","ef94c92f":"code","eb377ed4":"code","ae623cb5":"code","d08ffb3f":"code","5b9388c6":"code","5d84188f":"code","8b6b5c00":"code","d3a0c52b":"code","c45d7c6e":"code","65f2fc22":"code","32972ca7":"code","29e16cf0":"code","b558a0a4":"code","96226e9a":"code","be187290":"code","e2f08fc0":"code","ee188418":"code","0acfa58e":"code","49dacc37":"code","bbcd9f23":"code","e1a6793f":"code","e604fd6b":"code","5744178d":"code","2da284d0":"code","156a80e7":"code","72162aaa":"code","628b2b9b":"code","f5e5b5ce":"code","3a11d882":"code","e3973df2":"code","60baa57d":"code","6b813dab":"code","4cb71657":"code","c401a4fa":"code","33c8da65":"code","0c254826":"code","a0598f9e":"code","47167bf3":"code","0083e217":"code","9caeb8ef":"code","db615ce8":"code","7e03a0f4":"code","e7e12265":"code","a0c3cf76":"code","97ac8c56":"code","af908ab8":"code","7aa90318":"code","6426058e":"code","fc288b9a":"code","7efd91d7":"code","a463f268":"markdown","08862ecc":"markdown","8c225c1a":"markdown","53a09ae0":"markdown","7d3387fb":"markdown","cfd7b245":"markdown","45027730":"markdown","6e6100ec":"markdown","41bdaf2e":"markdown","e81d2169":"markdown","37bcc3cd":"markdown","21b7d076":"markdown","18c66590":"markdown","5ce7c51e":"markdown","21e5b951":"markdown","d675d5a0":"markdown","c960fffe":"markdown","3fb6bebb":"markdown","88c002e8":"markdown","fe26b76d":"markdown","48343ed4":"markdown","5e669935":"markdown","39d67a78":"markdown","12779c7b":"markdown","265b6abb":"markdown","f40c5456":"markdown","298a092b":"markdown","0218116c":"markdown","f10b6d36":"markdown","dc9b7c82":"markdown","ac013592":"markdown","5ec7b11c":"markdown"},"source":{"f420756d":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.metrics import mutual_info_score, accuracy_score, mean_squared_error\nfrom sklearn.linear_model import LogisticRegression, Ridge\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","db93c4c0":"cols = ['neighbourhood_group','room_type','latitude','longitude','price','minimum_nights',\n        'number_of_reviews','reviews_per_month','calculated_host_listings_count','availability_365']\n\ndf = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv', usecols=cols)\ndf.head()","41e61698":"df.shape","794c49ed":"df.isna().sum()","4e5b2249":"df = df.fillna(0)","e92b4208":"df.isna().sum()","c782de0a":"df.describe()","69028424":"df.mode()","2c725637":"df['neighbourhood_group'].value_counts()","045faa59":"df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2","83dfb47c":"len(df_train), len(df_val), len(df_test)","b23fec65":"df_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\ny_train = df_train.price.values\ny_val = df_val.price.values\ny_test = df_test.price.values\n\ndel df_train['price']\ndel df_val['price']\ndel df_test['price']","6b140106":"df_full_train = df_full_train.reset_index(drop=True)","09e8b10c":"# check for missing values\ndf_full_train.isnull().sum()","0d68555a":"df_full_train.info()","ad62eca3":"train_mean = round(df_full_train.price.mean(),2)\ntrain_mean","ef94c92f":"numerical = df_full_train.select_dtypes(exclude=['object']).columns.tolist()\ncategorical = df_full_train.select_dtypes(include=['object']).columns.tolist()","eb377ed4":"numerical","ae623cb5":"categorical","d08ffb3f":"corr = df_full_train.corr()\n\nf, ax = plt.subplots(figsize=(12, 10))\n# mask the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)","5b9388c6":"y_train_above_average = np.where(y_train >= 152, 1, 0)\ny_val_above_average = np.where(y_val >= 152, 1, 0)\ny_test_above_average = np.where(y_test >= 152, 1, 0)  ","5d84188f":"print('The mutual information score for neighbourhood group and binarized price is {0}.'.format(round(mutual_info_score(y_train_above_average, df_train.neighbourhood_group),2)))","8b6b5c00":"print('The mutual information score for room type and binarized price is {0}.'.format(round(mutual_info_score(y_train_above_average, df_train.room_type),2)))","d3a0c52b":"df_train.info()","c45d7c6e":"dv = DictVectorizer(sparse=False)\n\ntrain_dict = df_train.to_dict(orient='records')\nX_train = dv.fit_transform(train_dict)\n\nval_dict = df_val.to_dict(orient='records')\nX_val = dv.transform(val_dict)","65f2fc22":"model = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\nmodel.fit(X_train, y_train_above_average)","32972ca7":"model.intercept_[0]","29e16cf0":"model.coef_[0].round(3)","b558a0a4":"dict(zip(dv.get_feature_names(), model.coef_[0].round(3)))","96226e9a":"y_pred = model.predict_proba(X_val)[:, 1]","be187290":"above_avg_price_decision = (y_pred >= 0.5)","e2f08fc0":"(y_val == above_avg_price_decision).mean()","ee188418":"above_avg_price_decision.astype(int)","0acfa58e":"df_pred = pd.DataFrame()\ndf_pred['probability'] = y_pred\ndf_pred['prediction'] = above_avg_price_decision.astype(int)\ndf_pred['actual'] = y_val_above_average\ndf_pred['correct'] = df_pred.prediction == df_pred.actual","49dacc37":"df_pred.head()","bbcd9f23":"round(df_pred.correct.mean(),2)","e1a6793f":"# model trained with all variables\nmodel = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\nmodel.fit(X_train, y_train_above_average)\ny_pred = model.predict_proba(X_val)[:, 1]\nabove_avg_price_decision = (y_pred >= 0.5)\ndf_pred_all = pd.DataFrame()\ndf_pred_all['probability'] = y_pred\ndf_pred_all['prediction'] = above_avg_price_decision.astype(int)\ndf_pred_all['actual'] = y_val_above_average\ndf_pred_all['correct'] = df_pred_all.prediction == df_pred_all.actual\nmodel_1_acc = round(df_pred_all.correct.mean(),2)\nprint('Accuracy for the model with all features is {0}.'.format(model_1_acc))","e604fd6b":"# removed room_type\nmodel_2_features = ['availability_365','calculated_host_listings_count','latitude','longitude','minimum_nights','neighbourhood_group','number_of_reviews','reviews_per_month']\n# removed reviews_per_month\nmodel_3_features = ['availability_365','calculated_host_listings_count','latitude','longitude','minimum_nights','neighbourhood_group','number_of_reviews','room_type']\n# removed number_of_reviews\nmodel_4_features = ['availability_365','calculated_host_listings_count','latitude','longitude','minimum_nights','neighbourhood_group','reviews_per_month','room_type']\n# removed neighbourhood_group\nmodel_5_features = ['availability_365','calculated_host_listings_count','latitude','longitude','minimum_nights','number_of_reviews','reviews_per_month','room_type']\n# removed minimum_nights\nmodel_6_features = ['availability_365','calculated_host_listings_count','latitude','longitude','neighbourhood_group','number_of_reviews','reviews_per_month','room_type']\n# removed longitude\nmodel_7_features = ['availability_365','calculated_host_listings_count','latitude','minimum_nights','neighbourhood_group','number_of_reviews','reviews_per_month','room_type']\n# removed latitude\nmodel_8_features = ['availability_365','calculated_host_listings_count','longitude','minimum_nights','neighbourhood_group','number_of_reviews','reviews_per_month','room_type']\n# removed calculated_host_listings_count\nmodel_9_features = ['availability_365','latitude','longitude','minimum_nights','neighbourhood_group','number_of_reviews','reviews_per_month','room_type']\n# removed availability_365\nmodel_10_features = ['calculated_host_listings_count','latitude','longitude','minimum_nights','neighbourhood_group','number_of_reviews','reviews_per_month','room_type']","5744178d":"dicts_train_2 = df_train[model_2_features].to_dict(orient='records')\ndicts_val_2 = df_val[model_2_features].to_dict(orient='records')\ndv_2 = DictVectorizer(sparse=False)\ndv_2.fit(dicts_train_2)\nX_train_2 = dv_2.transform(dicts_train_2)\nX_val_2 = dv_2.transform(dicts_val_2)\nmodel_2 = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\nmodel_2.fit(X_train_2, y_train_above_average)\ndict(zip(dv_2.get_feature_names(), model_2.coef_[0].round(3)))","2da284d0":"y_pred_2 = model_2.predict_proba(X_val_2)[:, 1]\nabove_avg_price_decision_2 = (y_pred_2 >= 0.5)\ndf_pred_2 = pd.DataFrame()\ndf_pred_2['probability'] = y_pred_2\ndf_pred_2['prediction'] = above_avg_price_decision_2.astype(int)\ndf_pred_2['actual'] = y_val_above_average\ndf_pred_2['correct'] = df_pred_2.prediction == df_pred_2.actual\nmodel_2_acc = round(df_pred_2.correct.mean(),2)\nprint('Model 2 accuracy is {0}.'.format(model_2_acc))","156a80e7":"dicts_train_3 = df_train[model_3_features].to_dict(orient='records')\ndicts_val_3 = df_val[model_3_features].to_dict(orient='records')\ndv_3 = DictVectorizer(sparse=False)\ndv_3.fit(dicts_train_3)\nX_train_3 = dv_3.transform(dicts_train_3)\nX_val_3 = dv_3.transform(dicts_val_3)\nmodel_3 = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\nmodel_3.fit(X_train_3, y_train_above_average)\ndict(zip(dv_3.get_feature_names(), model_3.coef_[0].round(3)))","72162aaa":"y_pred_3 = model_3.predict_proba(X_val_3)[:, 1]\nabove_avg_price_decision_3 = (y_pred_3 >= 0.5)\ndf_pred_3 = pd.DataFrame()\ndf_pred_3['probability'] = y_pred_3\ndf_pred_3['prediction'] = above_avg_price_decision_3.astype(int)\ndf_pred_3['actual'] = y_val_above_average\ndf_pred_3['correct'] = df_pred_3.prediction == df_pred_3.actual\nmodel_3_acc = round(df_pred_3.correct.mean(),2)\nprint('Model 3 accuracy is {0}.'.format(model_3_acc))","628b2b9b":"dicts_train_4 = df_train[model_4_features].to_dict(orient='records')\ndicts_val_4 = df_val[model_4_features].to_dict(orient='records')\ndv_4 = DictVectorizer(sparse=False)\ndv_4.fit(dicts_train_4)\nX_train_4 = dv_4.transform(dicts_train_4)\nX_val_4 = dv_4.transform(dicts_val_4)\nmodel_4 = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\nmodel_4.fit(X_train_4, y_train_above_average)\ndict(zip(dv_4.get_feature_names(), model_4.coef_[0].round(3)))","f5e5b5ce":"y_pred_4 = model_4.predict_proba(X_val_4)[:, 1]\nabove_avg_price_decision_4 = (y_pred_4 >= 0.5)\ndf_pred_4 = pd.DataFrame()\ndf_pred_4['probability'] = y_pred_4\ndf_pred_4['prediction'] = above_avg_price_decision_4.astype(int)\ndf_pred_4['actual'] = y_val_above_average\ndf_pred_4['correct'] = df_pred_4.prediction == df_pred_4.actual\nmodel_4_acc = round(df_pred_4.correct.mean(),2)\nprint('Model 4 accuracy is {0}.'.format(model_4_acc))","3a11d882":"dicts_train_5 = df_train[model_5_features].to_dict(orient='records')\ndicts_val_5 = df_val[model_5_features].to_dict(orient='records')\ndv_5 = DictVectorizer(sparse=False)\ndv_5.fit(dicts_train_5)\nX_train_5 = dv_5.transform(dicts_train_5)\nX_val_5 = dv_5.transform(dicts_val_5)\nmodel_5 = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\nmodel_5.fit(X_train_5, y_train_above_average)\ndict(zip(dv_5.get_feature_names(), model_5.coef_[0].round(3)))","e3973df2":"y_pred_5 = model_5.predict_proba(X_val_5)[:, 1]\nabove_avg_price_decision_5 = (y_pred_5 >= 0.5)\ndf_pred_5 = pd.DataFrame()\ndf_pred_5['probability'] = y_pred_5\ndf_pred_5['prediction'] = above_avg_price_decision_5.astype(int)\ndf_pred_5['actual'] = y_val_above_average\ndf_pred_5['correct'] = df_pred_5.prediction == df_pred_5.actual\nmodel_5_acc = round(df_pred_5.correct.mean(),2)\nprint('Model 5 accuracy is {0}.'.format(model_5_acc))","60baa57d":"dicts_train_6 = df_train[model_6_features].to_dict(orient='records')\ndicts_val_6 = df_val[model_6_features].to_dict(orient='records')\ndv_6 = DictVectorizer(sparse=False)\ndv_6.fit(dicts_train_6)\nX_train_6 = dv_6.transform(dicts_train_6)\nX_val_6 = dv_6.transform(dicts_val_6)\nmodel_6 = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\nmodel_6.fit(X_train_6, y_train_above_average)\ndict(zip(dv_6.get_feature_names(), model_6.coef_[0].round(3)))","6b813dab":"y_pred_6 = model_6.predict_proba(X_val_6)[:, 1]\nabove_avg_price_decision_6 = (y_pred_6 >= 0.5)\ndf_pred_6 = pd.DataFrame()\ndf_pred_6['probability'] = y_pred_6\ndf_pred_6['prediction'] = above_avg_price_decision_6.astype(int)\ndf_pred_6['actual'] = y_val_above_average\ndf_pred_6['correct'] = df_pred_6.prediction == df_pred_6.actual\nmodel_6_acc = round(df_pred_6.correct.mean(),2)\nprint('Model 6 accuracy is {0}.'.format(model_6_acc))","4cb71657":"dicts_train_7 = df_train[model_7_features].to_dict(orient='records')\ndicts_val_7 = df_val[model_7_features].to_dict(orient='records')\ndv_7 = DictVectorizer(sparse=False)\ndv_7.fit(dicts_train_7)\nX_train_7 = dv_7.transform(dicts_train_7)\nX_val_7 = dv_7.transform(dicts_val_7)\nmodel_7 = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\nmodel_7.fit(X_train_7, y_train_above_average)\ndict(zip(dv_7.get_feature_names(), model_7.coef_[0].round(3)))","c401a4fa":"y_pred_7 = model_7.predict_proba(X_val_7)[:, 1]\nabove_avg_price_decision_7 = (y_pred_7 >= 0.5)\ndf_pred_7 = pd.DataFrame()\ndf_pred_7['probability'] = y_pred_7\ndf_pred_7['prediction'] = above_avg_price_decision_7.astype(int)\ndf_pred_7['actual'] = y_val_above_average\ndf_pred_7['correct'] = df_pred_7.prediction == df_pred_7.actual\nmodel_7_acc = round(df_pred_7.correct.mean(),2)\nprint('Model 7 accuracy is {0}.'.format(model_7_acc))","33c8da65":"dicts_train_8 = df_train[model_8_features].to_dict(orient='records')\ndicts_val_8 = df_val[model_8_features].to_dict(orient='records')\ndv_8 = DictVectorizer(sparse=False)\ndv_8.fit(dicts_train_8)\nX_train_8 = dv_8.transform(dicts_train_8)\nX_val_8 = dv_8.transform(dicts_val_8)\nmodel_8 = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\nmodel_8.fit(X_train_8, y_train_above_average)\ndict(zip(dv_8.get_feature_names(), model_8.coef_[0].round(3)))","0c254826":"y_pred_8 = model_8.predict_proba(X_val_8)[:, 1]\nabove_avg_price_decision_8 = (y_pred_8 >= 0.5)\ndf_pred_8 = pd.DataFrame()\ndf_pred_8['probability'] = y_pred_8\ndf_pred_8['prediction'] = above_avg_price_decision_8.astype(int)\ndf_pred_8['actual'] = y_val_above_average\ndf_pred_8['correct'] = df_pred_8.prediction == df_pred_8.actual\nmodel_8_acc = round(df_pred_8.correct.mean(),2)\nprint('Model 8 accuracy is {0}.'.format(model_8_acc))","a0598f9e":"dicts_train_9 = df_train[model_9_features].to_dict(orient='records')\ndicts_val_9 = df_val[model_9_features].to_dict(orient='records')\ndv_9 = DictVectorizer(sparse=False)\ndv_9.fit(dicts_train_9)\nX_train_9 = dv_9.transform(dicts_train_9)\nX_val_9 = dv_9.transform(dicts_val_9)\nmodel_9 = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\nmodel_9.fit(X_train_9, y_train_above_average)\ndict(zip(dv_9.get_feature_names(), model_9.coef_[0].round(3)))","47167bf3":"y_pred_9 = model_9.predict_proba(X_val_9)[:, 1]\nabove_avg_price_decision_9 = (y_pred_9 >= 0.5)\ndf_pred_9 = pd.DataFrame()\ndf_pred_9['probability'] = y_pred_9\ndf_pred_9['prediction'] = above_avg_price_decision_9.astype(int)\ndf_pred_9['actual'] = y_val_above_average\ndf_pred_9['correct'] = df_pred_9.prediction == df_pred_9.actual\nmodel_9_acc = round(df_pred_9.correct.mean(),2)\nprint('Model 9 accuracy is {0}.'.format(model_9_acc))","0083e217":"dicts_train_10 = df_train[model_10_features].to_dict(orient='records')\ndicts_val_10 = df_val[model_10_features].to_dict(orient='records')\ndv_10 = DictVectorizer(sparse=False)\ndv_10.fit(dicts_train_10)\nX_train_10 = dv_10.transform(dicts_train_10)\nX_val_10 = dv_10.transform(dicts_val_10)\nmodel_10 = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\nmodel_10.fit(X_train_10, y_train_above_average)\ndict(zip(dv_10.get_feature_names(), model_10.coef_[0].round(3)))","9caeb8ef":"y_pred_10 = model_10.predict_proba(X_val_10)[:, 1]\nabove_avg_price_decision_10 = (y_pred_10 >= 0.5)\ndf_pred_10 = pd.DataFrame()\ndf_pred_10['probability'] = y_pred_10\ndf_pred_10['prediction'] = above_avg_price_decision_10.astype(int)\ndf_pred_10['actual'] = y_val_above_average\ndf_pred_10['correct'] = df_pred_10.prediction == df_pred_10.actual\nmodel_10_acc = round(df_pred_10.correct.mean(),2)\nprint('Model 10 accuracy is {0}.'.format(model_10_acc))","db615ce8":"data = {'model': ['Model 1','Model 2','Model 3','Model 4','Model 5','Model 6','Model 7','Model 8','Model 9','Model 10'],\n        'desc': ['All','No room_type','No reviews_per_month','No number_of_reviews','No neighbourhood_group','No minimum_nights','No longitude','No latitude','No calculated_host_listings_count','No availability_365'],\n        'accuracy': [model_1_acc, model_2_acc, model_3_acc, model_4_acc, model_5_acc, model_6_acc, model_7_acc, model_8_acc, model_9_acc, model_10_acc],\n        'diff_model_1_acc': [model_1_acc - model_1_acc, model_2_acc - model_1_acc, model_3_acc - model_1_acc, model_4_acc - model_1_acc, model_5_acc - model_1_acc,\n                             model_6_acc - model_1_acc, model_7_acc - model_1_acc, model_8_acc - model_1_acc, model_9_acc - model_1_acc, model_10_acc - model_1_acc]\n       }\nsummary = pd.DataFrame(data)\nsummary","7e03a0f4":"plt.plot(df['price'])\nplt.show()","e7e12265":"df.isin([0]).sum()","a0c3cf76":"print('y_train has {0} zeros.'.format(len(y_train) - np.count_nonzero(y_train)))","97ac8c56":"print('y_val has {0} zero.'.format(len(y_val) - np.count_nonzero(y_val)))","af908ab8":"print('y_test has {0} zeros.'.format(len(y_test) - np.count_nonzero(y_test)))","7aa90318":"train_mean","6426058e":"# fill in zeros\ny_train[y_train == 0] = train_mean\ny_val[y_val == 0] = train_mean\ny_test[y_test == 0] = train_mean","fc288b9a":"# take the log\ny_train = np.log(y_train)\ny_val = np.log(y_val)\ny_test = np.log(y_test)","7efd91d7":"alpha_values = [0, 0.01, 0.1, 1, 10]\nrmses = []\n\nfor alpha in alpha_values:\n    model = Ridge(alpha)\n    model.fit(X_train, y_train)\n    pred = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, pred))\n    rmses.append(rmse)\n    print('Ridge Regression with alpha = {0}, RMSE on validation set is {1}'.format(alpha, round(rmse,3)))\n","a463f268":"<h3>Model 3 - Removed reviews_per_month<\/h3>","08862ecc":"<h3>Model 2 - Removed room_type<\/h3>","8c225c1a":"*If there are multiple options, select the smallest alpha.*","53a09ae0":"<h3>Model 6 - Removed minimum_nights<\/h3>","7d3387fb":"<h3>A6. When rounding all the RMSEs on the validation set become the same, so the smallest alpha is 0.<\/h3>","cfd7b245":"<h3>Model 7 - Removed longitude<\/h3>","45027730":"<h3>A1. The mode of neighbourhood_group is Manhattan.<\/h3>","6e6100ec":"<h3>Question 4: One Hot Encoding and Logistic Regression<\/h3>\n\n*     Now let's train a logistic regression\n*     Remember that we have two categorical variables in the data. Include them using one-hot encoding.\n*     Fit the model on the training dataset.\n*         To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n*         model = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\n*     Calculate the accuracy on the validation dataset and round it to 2 decimal digits.\n","41bdaf2e":"<h3>Model 10 - Removed availability_365<\/h3>","e81d2169":"<h3>Model 8 - Removed latitude<\/h3>","37bcc3cd":"<h3>Question 3: Mutual Information<\/h3>\n\n*     Calculate the mutual information score with the (binarized) price for the two categorical variables that we have. Use the training set only.\n*     Which of these two variables has bigger score?\n*     Round it to 2 decimal digits using round(score, 2)","21b7d076":"<h3>Model 5 - Removed neighbourhood_group<\/h3>","18c66590":"<h3>Since the values of price can go from 0 to 10,000, we have to handle 0's prior to applying logarithmic transformation<\/h3>","5ce7c51e":"*note: the difference doesn't have to be positive*","21e5b951":"<h3>A5. Both number_of_reviews and reviews_per_month have the same accuracy as the model with all features.<\/h3>","d675d5a0":"<h3>A3: The room type offers more information (0.14 > 0.05) than neighbourhood type.<\/h3>","c960fffe":"<h3>Question 6<\/h3>\n\n*     For this question, we'll see how to use a linear regression model from Scikit-Learn\n*     We'll need to use the original column 'price'. Apply the logarithmic transformation to this column.\n*     Fit the Ridge regression model on the training data.\n*     This model has a parameter alpha. Let's try the following values: [0, 0.01, 0.1, 1, 10]\n*     Which of these alphas leads to the best RMSE on the validation set? Round your RMSE scores to 3 decimal digits.\n","3fb6bebb":"<h3>Model 9 - Removed calculated_host_listings_count<\/h3>","88c002e8":"<h3>Model 4 - Removed number_of_reviews<\/h3>","fe26b76d":"<h3>Make price binary<\/h3>\n\n*     We need to turn the price variable from numeric into binary.\n*     Let's create a variable above_average which is 1 if the price is above (or equal to) 152.\n","48343ed4":"<h3>Features<\/h3>\n\nFor the rest of the homework, you'll need to use the features from the previous homework with additional two 'neighbourhood_group' and 'room_type'. So the whole feature set will be set as follows:\n\n    'neighbourhood_group',\n    'room_type',\n    'latitude',\n    'longitude',\n    'price',\n    'minimum_nights',\n    'number_of_reviews',\n    'reviews_per_month',\n    'calculated_host_listings_count',\n    'availability_365'\n\nSelect only them and fill in the missing values with 0.","5e669935":"<h4>Split the data<\/h4>\n\n*     Split your data in train\/val\/test sets, with 60%\/20%\/20% distribution.\n*     Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.\n*     Make sure that the target value ('price') is not in your dataframe.\n","39d67a78":"<h4>A4. The accuracy is 0.79.<\/h4>","12779c7b":"<h3>To keep things simple, we will fill the 0's with the mean calculated above to impute the data.<\/h3>","265b6abb":"# Examining the New York City Airbnb data for Classification ","f40c5456":"<h3>Model 1 - All Features Included<\/h3>","298a092b":"<h3>Dataset<\/h3>\n\nIn this homework, we will continue the New York City Airbnb Open Data. You can take it from Kaggle or download from here if you don't want to sign up to Kaggle.\n\nWe'll keep working with the 'price' variable, and we'll transform it to a classification task.","0218116c":"<h3>A2.  The highest correlation is between reviews_per_month and number_of_reviews (0.58)<\/h3>","f10b6d36":"<h3>Question 5<\/h3>\n\n*     We have 9 features: 7 numerical features and 2 categorical.\n*     Let's find the least useful one using the feature elimination technique.\n*     Train a model with all these features (using the same parameters as in Q4).\n*     Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n*     For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n*     Which of following features has the smallest difference?\n<ul>\n    <li>neighbourhood_group\n    <li>room_type\n    <li>number_of_reviews\n    <li>reviews_per_month\n <\/ul>","dc9b7c82":"<h3>Question 2: Correlation<\/h3>\n\n*     Create the correlation matrix for the numerical features of your train dataset.\n*     What are the two features that have the biggest correlation in this dataset?\n","ac013592":"<h4>Read in the data and fill missing values with zeros<\/h4>","5ec7b11c":"<h3>Question 1: EDA<\/h3>\n\nWhat is the most frequent observation (mode) for the column 'neighbourhood_group'?"}}