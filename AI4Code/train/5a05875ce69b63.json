{"cell_type":{"967998ec":"code","a92d8399":"code","804e1cb4":"code","fc64e5fe":"code","f27c59d1":"code","b06cf337":"code","7469bd2e":"code","f9ac58fb":"code","fc81e256":"code","82f9b12f":"code","c40414ba":"code","9fa46715":"code","4a04f153":"code","1eb53da6":"code","f3b50269":"code","8f0e85cb":"code","165fb2b0":"code","5a9ae0a3":"code","045b9a30":"code","7144405f":"code","8c389e7b":"code","e8605333":"code","eecb60ff":"code","1ce9ab7a":"code","9e3876b9":"code","28fc83fb":"code","9f007af8":"code","e85f53a8":"code","5820a568":"code","dde5273d":"code","ca063f13":"code","27399783":"code","fb522df2":"code","272ae6cd":"code","cdd01a1e":"code","1259adf5":"code","590d3849":"code","31b2fbdd":"code","c11fd3fa":"code","ba51744a":"code","562b375d":"markdown","e178cc55":"markdown","c6028977":"markdown","c9ecf369":"markdown","bbb8d190":"markdown","c47108d6":"markdown","5c4feb40":"markdown","a1a9dcea":"markdown","cb8c87bc":"markdown","7138955a":"markdown","ff611e59":"markdown","1c88a92c":"markdown","1f0dee86":"markdown","cedadc76":"markdown","9c36e7a0":"markdown","1b8eb331":"markdown","c4c6c154":"markdown","696baf75":"markdown","11a484c4":"markdown","8ac2f928":"markdown","611f4fca":"markdown","eb1dac24":"markdown","d234b55e":"markdown","dd22dc04":"markdown","cf680117":"markdown","dcd67c95":"markdown","9c365932":"markdown","5fb711e9":"markdown","5ce285bf":"markdown","7719d7d2":"markdown","ddedcb98":"markdown","6f2906ad":"markdown","b108b482":"markdown","6b3b58a4":"markdown","8ef8c3fb":"markdown","5189c186":"markdown","10641b36":"markdown","21fe7e53":"markdown","d613f46d":"markdown","06af7b4f":"markdown","c33947db":"markdown","c421c149":"markdown","bc4e3776":"markdown","c5249762":"markdown"},"source":{"967998ec":"import random\n\nimport cv2\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n# import tensorflow_addons as tfa\nfrom tensorflow.keras import layers\n\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\n\nimport os","a92d8399":"LABEL_PATH = \"..\/input\/blood-cells\/dataset-master\/dataset-master\/labels.csv\"\nIMAGE_PATH = \"..\/input\/blood-cells\/dataset-master\/dataset-master\/JPEGImages\"\n\nSEED = 42\n\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\nos.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)","804e1cb4":"def load_image(path):\n    try:\n        return cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n    except:\n        return None\n\n\ndata = pd.read_csv(LABEL_PATH)[[\"Image\", \"Category\"]]\ndata = data.rename(columns={\"Category\": \"label\", \"Image\": \"id\"})\ndata = data[-pd.isna(data[\"label\"])]\n\ndata[\"path\"] = [f\"{IMAGE_PATH}\/BloodImage_{id:05}.jpg\" for id in data[\"id\"]]\n\ndata[\"image\"] = [load_image(path) for path in data[\"path\"]]\ndata = data[[im is not None for im in data[\"image\"]]]\n\ndata[\"label\"].value_counts()","fc64e5fe":"def plot_grid(images, titles=None, title=None, cmap=None, top=1.15, size=(3, 3)):\n    if titles is None:\n        titles = [None] * len(images)\n        padding = 0.1\n    else:\n        padding = (0.1, 0.35)\n\n    fig = plt.figure(figsize=(11, 11))\n    if title is not None:\n        fig.suptitle(title)\n        fig.subplots_adjust(top=top)\n    grid = ImageGrid(fig, 111, nrows_ncols=size, axes_pad=padding)\n    for ax, img, title in zip(grid, images, titles):\n        if title is not None:\n            ax.set_title(title)\n        ax.axis(\"off\")\n        ax.imshow(img, cmap=cmap)\n\n\n# The following are used for demostration purposes troughout this notebook\nn = 9\nIMAGES, TITLES = data[\"image\"][55 : 55 + n], data[\"label\"][55 : 55 + n]\n\nplot_grid(images=IMAGES, titles=TITLES)","f27c59d1":"def relabel(label):\n    if \",\" in label:\n        l1, l2 = label.split(\",\")\n        return l1 if l1 == l2 else None\n    else:\n        return label\n\n\ndata[\"label\"] = [relabel(l) for l in data[\"label\"]]\ndata = data[[l is not None for l in data[\"label\"]]]\n\nCLASS_LABELS = data[\"label\"].unique()\ndata[\"target\"] = [np.argmax(l == CLASS_LABELS) for l in data[\"label\"]]\ndata[\"label\"].value_counts()","b06cf337":"from sklearn.model_selection import train_test_split\n\n\ndef split_dataset(data, targets, test_size=0.25, train_size=0.8):\n    data_rest, data_test, targets_rest, targets_test = train_test_split(\n        data, targets, test_size=test_size, stratify=targets\n    )\n\n    data_train, data_dev, targets_train, targets_dev = train_test_split(\n        data_rest, targets_rest, train_size=train_size, stratify=targets_rest\n    )\n\n    return (\n        tf.data.Dataset.from_tensor_slices((data_train, targets_train)),\n        tf.data.Dataset.from_tensor_slices((data_dev, targets_dev)),\n        tf.data.Dataset.from_tensor_slices((data_test, targets_test)),\n    )","7469bd2e":"def residual_block(input, channels, momentum, strides=1):\n    block = layers.Conv2D(channels * strides, 3, padding=\"same\", strides=strides)(input)\n    block = layers.BatchNormalization(momentum=momentum)(block)\n    block = layers.ReLU()(block)\n    block = layers.Dropout(0.2)(block)\n    block = layers.Conv2D(channels * strides, 3, padding=\"same\")(block)\n    block = layers.ReLU()(block)\n    block = layers.BatchNormalization(momentum=momentum)(block)\n\n    if strides > 1:\n        connection = layers.Conv2D(\n            channels * strides, 1, strides=strides, padding=\"same\"\n        )(input)\n        connection = layers.BatchNormalization(momentum=momentum)(connection)\n    else:\n        connection = input\n\n    output = tf.keras.layers.Add()([block, connection])\n\n    return output\n\n\ndef create_model(blocks=[1, 2], init_channels=16, momentum=0.99):\n    input = layers.Input((None, None, 3))\n\n    first = layers.Conv2D(init_channels, 7, padding=\"same\")(input)\n    hidden = layers.MaxPool2D()(first)\n\n    channels = init_channels\n    for strides in blocks:\n        hidden = residual_block(hidden, channels, momentum, strides=strides)\n        channels = strides * channels\n\n    last = layers.GlobalAvgPool2D()(hidden)\n    last = layers.Dense(512, activation=\"relu\")(last)\n    output = layers.Dense(len(CLASS_LABELS), activation=\"softmax\")(last)\n\n    return tf.keras.Model(inputs=[input], outputs=[output])","f9ac58fb":"def compile_model(model, train_dataset, batch_size=32, lr_final=0.001, lr_init=0.01):\n    steps = int(tf.data.experimental.cardinality(train_dataset).numpy() \/ batch_size)\n    decay_factor = (lr_final \/ lr_init) ** (1 \/ steps)\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=lr_init,\n        decay_steps=steps,\n        decay_rate=decay_factor,\n        staircase=True,\n    )\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n    )","fc81e256":"class Print(tf.keras.callbacks.Callback):\n    log_each = 10\n    epoch = 0\n\n    def __init__(self, log_each):\n        self.log_each = log_each\n\n    def on_epoch_begin(self, epoch, logs=None):\n        self.epoch = epoch\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.epoch % self.log_each == 0:\n            print(\n                \"Epoch: {}, loss: {}, acc: {}, val loss: {}, val accuracy: {}\".format(\n                    self.epoch,\n                    logs[\"loss\"],\n                    logs[\"sparse_categorical_accuracy\"],\n                    logs[\"val_loss\"],\n                    logs[\"val_sparse_categorical_accuracy\"],\n                )\n            )\n\n\ndef fit_model(\n    model,\n    train_dataset,\n    dev_dataset,\n    model_name,\n    early_stop,\n    batch_size=32,\n    epochs=50,\n    log_each=10,\n    shuffle=True,\n):\n\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=f\"models\/{model_name}.hdf5\",\n        save_best_only=True,\n        save_weights_only=False,\n    )\n\n    callbacks = [Print(log_each=log_each), checkpoint]\n\n    if early_stop is not None:\n        early_stop = tf.keras.callbacks.EarlyStopping(\n            monitor=\"val_sparse_categorical_accuracy\",\n            patience=early_stop,\n            verbose=1,\n            mode=\"min\",\n            restore_best_weights=True,\n        )\n        callbacks.append(early_stop)\n\n    return model.fit(\n        (train_dataset.shuffle(5000) if shuffle else train_dataset)\n        .batch(batch_size)\n        .prefetch(tf.data.AUTOTUNE),\n        validation_data=dev_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n        epochs=epochs,\n        callbacks=callbacks,\n        verbose=0,\n    )","82f9b12f":"from sklearn.metrics import ConfusionMatrixDisplay, cohen_kappa_score, confusion_matrix\n\n\ndef evaluate_model(model, history, test_dataset):\n    acc = history.history[\"sparse_categorical_accuracy\"]\n    val_acc = history.history[\"val_sparse_categorical_accuracy\"]\n    loss = history.history[\"loss\"]\n    val_loss = history.history[\"val_loss\"]\n\n    x = range(len(acc))\n    _, ((ax, ax2), (ax3, ax4)) = plt.subplots(figsize=(12, 12), ncols=2, nrows=2)\n\n    ax.plot(x, acc, label=\"Training Accuracy\")\n    ax.plot(x, val_acc, label=\"Validation Accuracy\")\n    ax.legend(loc=\"lower right\")\n    ax.set_title(\"Training and Validation Accuracy\")\n\n    ax2.plot(x, loss, label=\"Training Loss\")\n    ax2.plot(x, val_loss, label=\"Validation Loss\")\n    ax2.legend(loc=\"upper right\")\n    ax2.set_title(\"Training and Validation Loss\")\n\n    y_pred = np.argmax(\n        model.predict(test_dataset.batch(16).prefetch(tf.data.AUTOTUNE)), axis=1\n    )\n    y_true = np.array(tuple(zip(*test_dataset))[1])\n    cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n    cmd = ConfusionMatrixDisplay(cm)\n    cmd.plot(\n        ax=ax3,\n        cmap=plt.cm.Blues,\n        xticks_rotation=\"vertical\",\n    )\n    ax3.set_xticklabels(CLASS_LABELS)\n    ax3.set_yticklabels(CLASS_LABELS)\n    ax3.set_title(\"Confusion Matrix on Test Set\")\n\n    score = cohen_kappa_score(y_true, y_pred)\n    ax4.pie(\n        [np.max([score, 0]), np.min([1 - score, 1])],\n        wedgeprops=dict(width=0.5),\n        startangle=90,\n    )\n    ax4.text(\n        0,\n        0,\n        np.round(score, decimals=3),\n        horizontalalignment=\"center\",\n        verticalalignment=\"center\",\n    )\n    ax4.set_title(\"Cohen's Kappa Score on the Test Set\")\n","c40414ba":"baseline_train, baseline_dev, baseline_test = split_dataset(\n    data=[np.asarray(cv2.resize(i, dsize=(256, 192))) for i in data[\"image\"]],\n    targets=[np.argmax(l == CLASS_LABELS) for l in data[\"label\"]],\n)","9fa46715":"baseline_model = create_model(blocks=[1, 2])\ncompile_model(baseline_model, baseline_train)\n\n\nbaseline_history = fit_model(\n    baseline_model,\n    train_dataset=baseline_train,\n    dev_dataset=baseline_dev,\n    epochs=50,\n    log_each=5,\n    model_name=\"baseline\",\n    early_stop=None\n)","4a04f153":"baseline_model.load_weights(f\"models\/baseline.hdf5\")\nevaluate_model(baseline_model, baseline_history, baseline_test)","1eb53da6":"def balance_dataset(dataset, ratio):\n    data, targets = [np.array(a) for a in zip(*dataset)]\n    class_count = [np.count_nonzero(targets == l) for l in range(len(CLASS_LABELS))]\n    largest = np.max(class_count)\n\n    indices = []\n    for l, count in zip(range(len(CLASS_LABELS)), class_count):\n        class_indices = np.where(targets == l)[0]\n        indices += list(\n            random.choices(class_indices, k=int(np.round(largest * ratio)))\n            if count < largest * ratio\n            else class_indices\n        )\n\n    return tf.data.Dataset.from_tensor_slices(\n        (\n            data[indices],\n            targets[indices],\n        )\n    )","f3b50269":"balanced_model = create_model(blocks=[1, 2])\nbalanced_train = balance_dataset(baseline_train, ratio=1)\ncompile_model(balanced_model, balanced_train)\n\n\nbalanced_history = fit_model(\n    balanced_model,\n    train_dataset=balanced_train,\n    dev_dataset=baseline_dev,\n    epochs=50,\n    log_each=5,\n    model_name=\"balanced\",\n    early_stop=None\n)","8f0e85cb":"balanced_model.load_weights(\"models\/balanced.hdf5\")\nevaluate_model(balanced_model, balanced_history, baseline_test)","165fb2b0":"# Some custom colors for labelling\nRED, GREEN, BLUE = (200, 55, 0), (0, 200, 55), (0, 55, 200)\n\ndef visualize_mask(img, mask, color=(255, 255, 255), no_bg=False):\n    i = img.copy()\n    i[mask(img)] = color\n    if no_bg:\n        i[~mask(img)] = (0, 0, 0)\n    return i\n\ndef is_background(img):\n    return (img[:, :, 0] > 180) & (img[:, :, 1] > 180) & (img[:, :, 2] > 180)\n\n\nplot_grid(\n    images=[visualize_mask(img, is_background, BLUE) for img in IMAGES],\n    title=\"The background, masked with blue\",\n)","5a9ae0a3":"def is_erytrocyte(img):\n    return (img[:, :, 2] < 150) | (img[:, :, 0] + 3 > img[:, :, 2])\n\n\nplot_grid(\n    images=[\n        visualize_mask(visualize_mask(img, is_background, BLUE), is_erytrocyte, RED)\n        for img in IMAGES\n    ],\n    title=\"The background, masked with blue, and the erytrocytes, masked with red\",\n)","045b9a30":"def is_leukocyte(img):\n    return ~is_background(img) & ~is_erytrocyte(img)\n\n\nplot_grid(\n    images=[visualize_mask(img, is_leukocyte, no_bg=True) for img in IMAGES],\n    title=\"The leukocyte mask\",\n)","7144405f":"def leukocyte_mask(img):\n    i = cv2.GaussianBlur(img, (21, 21), 2)\n    i = visualize_mask(img, is_leukocyte, no_bg=True)\n\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n    i = cv2.erode(i, kernel, iterations=2)\n    i = cv2.dilate(i, kernel, iterations=7)\n\n    return np.max(i.astype(np.uint8), axis=2)\n\n\nplot_grid(\n    images=[leukocyte_mask(img) for img in IMAGES],\n    title=\"The leukocyte mask, after opening ops\",\n    cmap=\"gray\"\n)","8c389e7b":"def area(c):\n    rect = cv2.boundingRect(c)\n    return rect[2] * rect[3]\n\n\ndef find_contours(img_thr):\n    contours, _ = cv2.findContours(\n        img_thr, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE\n    )\n    return sorted(contours, key=area, reverse=True)\n\n\ndef draw_contours(img, contours, which=-1, color=GREEN):\n    im = img.copy()\n    return cv2.drawContours(\n        im, contours, contourIdx=which, color=color, thickness=2, lineType=cv2.LINE_AA\n    )\n\n\nCONTOURS = [find_contours(leukocyte_mask(img)) for img in IMAGES]\n\nplot_grid(\n    images=[draw_contours(img, cont, which=0) for img, cont in zip(IMAGES, CONTOURS)],\n    title=\"Image segmentation: The largest leukocytes in the sample\",\n)\n","e8605333":"def box(contour):\n    return np.int0(cv2.boxPoints(cv2.minAreaRect(contour)))\n\n\nplot_grid(\n    images=[\n        draw_contours(img, [box(cs[0])], color=(255, 0, 0))\n        for img, cs in zip(IMAGES, CONTOURS)\n    ],\n    title=\"Bounding boxes of the selected leukocytes\",\n)","eecb60ff":"def crop(src, contour):\n    # Get center, size, and angle from rect\n    center, size, theta = cv2.minAreaRect(contour)\n    # Convert to int \n    center, size = tuple(map(int, center)), tuple(map(int, size))\n    # Get rotation matrix for rectangle\n    M = cv2.getRotationMatrix2D(center, 4 * theta, 1)\n    # Perform rotation on src image\n    dst = cv2.warpAffine(src, M, (src.shape[1] ,src.shape[0]))\n    out = cv2.getRectSubPix(dst, size, center)\n    return out\n\nplot_grid(\n    images=[\n        cv2.resize(crop(img, cs[0]), (128, 128))\n        for img, cs in zip(IMAGES, CONTOURS)\n    ],\n    title=\"Isolated leukocytes\",\n    top=0.95\n)","1ce9ab7a":"def isolate_cell(image, size=128):\n    contours = find_contours(leukocyte_mask(image))\n    padded = cv2.resize(crop(image, contours[0]), (size, size))\n    return padded\n\ndata[\"cell\"] = [isolate_cell(im) for im in data[\"image\"]]","9e3876b9":"cell_train, cell_dev, cell_test = split_dataset(\n    data=np.array([np.asarray(c) for c in data[\"cell\"]]),\n    targets=data[\"target\"],\n)\ncell_train_balanced = balance_dataset(cell_train, ratio=1)\n\ncell_model = create_model(blocks=[1, 2])\ncompile_model(cell_model, cell_train_balanced)\n\ncell_history = fit_model(\n    cell_model,\n    train_dataset=cell_train_balanced,\n    dev_dataset=cell_dev,\n    epochs=50,\n    log_each=5,\n    model_name=\"cells\",\n    early_stop=None\n)","28fc83fb":"cell_model.load_weights(f\"models\/cells.hdf5\")\nevaluate_model(cell_model, cell_history, cell_test)","9f007af8":"# Just as a reminder, the evaluation of the previous model\nevaluate_model(balanced_model, balanced_history, baseline_test)","e85f53a8":"CELLS = [np.asarray(c) for c in data[\"cell\"][10:13]]","5820a568":"# def transform(img):\n#     img = tf.image.random_flip_left_right(img)\n#     img = tf.image.random_flip_up_down(img)\n#     # with tf.device(\"\/cpu:0\"):\n#     #     img = tf.keras.preprocessing.image.random_rotation(img, 360, row_axis=0, col_axis=1, channel_axis=2)\n#     return img\n\n\ndef transform(img):\n    with tf.device(\"\/cpu:0\"):\n        return tf.keras.Sequential(\n            [\n                layers.RandomFlip(\"horizontal_and_vertical\"),\n                layers.RandomRotation(1, fill_mode=\"constant\"),\n            ]\n        )(img)\n\n\nplot_grid(\n    images=CELLS + [transform(c) for c in CELLS],\n    size=(2, 3),\n    title=\"Before\/After Flip & Rotate\",\n    top=1.25,\n)","dde5273d":"def shear(img):\n    return tf.keras.preprocessing.image.random_shear(\n        img,\n        10,\n        row_axis=0,\n        col_axis=1,\n        channel_axis=2,\n        fill_mode=\"constant\",\n    )\n\n\nplot_grid(\n    images=CELLS + [shear(c) for c in CELLS],\n    size=(2, 3),\n    title=\"Before\/After Random Shear\",\n    top=1.25,\n)","ca063f13":"def brightness(img):\n    return tf.image.random_brightness(img, max_delta=0.2, seed=SEED)\n\nplot_grid(\n    images=CELLS + [brightness(c) for c in CELLS],\n    size=(2, 3),\n    title=\"Before\/After Bridghtness Adjustments\",\n    top=1.25,\n)","27399783":"def contrast(img):\n    return tf.image.random_contrast(img, lower=0.8, upper=1.2)\n\nplot_grid(\n    images=CELLS + [contrast(c) for c in CELLS],\n    size=(2, 3),\n    title=\"Before\/After Contrast Adjustments\",\n    top=1.25,\n)","fb522df2":"def augment(image, p=0.25):\n    if random.random() < p:\n        image = shear(image)\n    if random.random() < p:\n        image = brightness(image)\n    if random.random() < p:\n        image = contrast(image)\n    if random.random() < p:\n        image = transform(image)\n\n    return image\n\n\nplot_grid(\n    images=CELLS + [augment(c, p=1) for c in CELLS],\n    size=(2, 3),\n    title=\"Before\/After Augmentations\",\n    top=1.25,\n)","272ae6cd":"balanced_data, augmented_target = [\n    np.array(x) for x in zip(*balance_dataset(cell_train, ratio=4))\n]","cdd01a1e":"augmented_data = np.array([augment(d, p=1) for d in balanced_data])","1259adf5":"cell_data, cell_target = [np.array(x) for x in zip(*cell_train)]\naugmented_data_2 = np.concatenate([cell_data, augmented_data])\naugmented_target_2 = np.concatenate([cell_target, augmented_target])","590d3849":"augmented_model = create_model(blocks=[1, 2, 2])\naugmented_train = tf.data.Dataset.from_tensor_slices(\n    (augmented_data, augmented_target)\n)\ncompile_model(augmented_model, augmented_train, lr_init = 0.05, lr_final=0.004)\n\naugmented_history = fit_model(\n    augmented_model,\n    train_dataset=augmented_train,\n    dev_dataset=cell_dev,\n    epochs=140,\n    log_each=5,\n    model_name=\"augmented\",\n    early_stop=None\n)","31b2fbdd":"augmented_model.load_weights(f\"models\/augmented.hdf5\")\nevaluate_model(augmented_model, augmented_history, cell_test)","c11fd3fa":"best_model = tf.keras.models.load_model(\"..\/input\/locally-trained-models-for-leukocytes\/augmented.hdf5\")\nbest_model.summary()","ba51744a":"best_model.evaluate(cell_test.batch(32))","562b375d":"Then, a function to detect the erytrocytes. I'll use the fact that their colors have a really weak blue component, or a strong red component.","e178cc55":"Now that I know a little bit about the data I'll be working with, I can build the training, validation, and testing datasets.\nI'll use 25% of the images for testing, and 80% of the rest for training. ","c6028977":"_Takeaways:_ Compared with the previous model (see the evaluation below), the model trained on isolated cells has a slightly better performance, converges slightly faster, and its training is much quicker thanks to the images being much smaller.\n\n_Next, and the final, step:_ I'm quite happy with the model as it is; I will now add some layers to the model and lenghten the training process to get even better results. To make sure the model won't overfit, I will perform data augmentation.","c9ecf369":"Increasing the model size without increasing the number of training samples would result in overfitting. That is why I will augment the dataset with additional observations that are made by modifying the existing ones.","bbb8d190":"As everyone seems to be using accuracy, I calculated it for my model as well to have a comparison point. Here is a comparison with three most upvoted models that are publicly available on Kaggle.\n\n| Notebook                                                                                                                  | Accuracy on the test set | # of parameters |\n|---------------------------------------------------------------------------------------------------------------------------|--------------------------|-----------------|\n|this model| 92% | 134,853 |\n| [Identify Blood Cell Subtypes From Images](https:\/\/www.kaggle.com\/paultimothymooney\/identify-blood-cell-subtypes-from-images) | 90%                      | not provided    |\n| [Blood Cell Keras Inception](https:\/\/www.kaggle.com\/kartiksharma522\/blood-cell-keras-inception)                               | 86% (on validation set)  | 363,825         |\n| [Deep Learning From Scratch + Insights](https:\/\/www.kaggle.com\/placidpanda\/deep-learning-from-scratch-insights)              | 83%                      | 16,732          |","c47108d6":"This model is much better that the one I started with!","5c4feb40":"All of the models in this notebook are based on [ResNet](https:\/\/arxiv.org\/abs\/1512.03385), although much smaller, because the dataset is simpler and smaller. \n\nThe model contains a configurable number of blocks that are each made from two CNN layers with `3 x 3` kernels followed by a batch normalisation layer. For added regularization I apply a 20% dropout between the two CNN layers. The blocks are connected sequentially, and there are residual connections between their connection points.","a1a9dcea":"## Model building, fitting, and evaluation","cb8c87bc":"## Conclusion\n\nMain takeaways:\n- residual connections are very effective and help bring the number of parameters down while maintaining the perfomance characteristics\n- balancing the dataset improved generalization of the model\n- isolating the cells with OpenCV improved the training speed tremendously, and even improved the model itself\n- combining data augmentation with adding additional layers and epochs brought some nice small improvements as well\n\nAs is, the model isn't suitable for clinical use. However, it performs very well considering we only had less than 400 labeled examples.","7138955a":"To finalize the leukocyte masks I will:\n\n1. Get rid of the little scraps and make the masks more round.\n2. Find the bounding boxes of the leukocytes.\n3. Finally, crop the leukocytes according to their bounding boxes.\n\nI'll do (1) by _opening_ the image, that is erosion followed by dilation. ","ff611e59":"## Preparation of training, validation, and testing datasets","1c88a92c":"## Final model, augmented and balanced dataset of isolated leukocytes","1f0dee86":"Because I don't have information about the positions of leukocytes in the training samples, I can't train a neural network to peform the image segmentation and isolate the leukocytes. Luckily, the white blood cells in the samples have certain characteristics that make it possible to detect them using only Open CV:\n\n1. The background (i.e. area that is not an erytrocyte nor a leukocyte) is gray.\n2. The colour of erytrocytes leans toward red hues.\n3. The colour of leukocytes has a strong blue component.\n\nI will construct simple masks to filter out (1) and (2) to be left only with the leukocytes. \n\nFirstly, a function to detect the background.","cedadc76":"As you can see above, the dataset is very unbalanced. It also seems that some images contain more than one leukocyte. Let's check that by looking at some of the images.","9c36e7a0":"__Takeaways:__ The model clearly suffers from the dataset being imbalanced. Almost every cell is labelled as a neutrophile.\n\n__Next step:__ It might be useful to resample the data to balance the number of different leukocytes.","1b8eb331":"I will add these images to the original datasets and use them from now on to train the models.","c4c6c154":"## Third model, balanced dataset with isolated cells","696baf75":"The baseline model has two blocks, the first with 16 channels and the second with 32.","11a484c4":"# Using TF and OpenCV to differentiate between leukocytes","8ac2f928":"I use a custom fit function to supply callbacks for the training phase:\n\n- `Print` to only log every `n`-th epoch\n- `ModelCheckpoint` to save the best performing model after the training\n- optionally, `EarlyStop` to stop the training if validation loss hasn't changed for a long time\n\nI also shuffle the training set before the start of the training to ensure the batches are (pseudo)random.","611f4fca":"Contrast adjustments.","eb1dac24":"## Dataset overview and cleaning","d234b55e":"And everything put together.","dd22dc04":"_This project aims to provide a model that will be able to identify different types of leukocytes in an image of a blood sample. It is presented as a simple case study._\n\nAlthough all leukocytes participate on the protection of our bodies from pathogens in some way or another, the exact function of a leukocyte depends on its type. It is important to be able to discern between these types if we want to study immune responses. In this notebook I devise a model that could be used to analyse samples quickly and accurately.\n\n__You can find the final model in the last section of this notebook.__","cf680117":"I crop the images and seize each cell to a `128 x 128` square. A little bit of stretching doesn't hurt, on the contrary, it helps the model to generalize well.","dcd67c95":"Finally, I use a custom evaluation function to quickly visualize main insights about the training of the model and its performance on the test set. I use [Cohen's Kappa](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3900052\/) as the performance metric. Compared to accuracy and F1-score, Cohen's kappa statistic is a better fit for problems with unbalanced label distributions such as this one. It can be interpreted as the magnitude of agreement between the reality (the golden labels) and the model (the predicted labels) that can't be attributed tu pure chance.","9c365932":"Other people on Kaggle mostly use the other dataset supplied with this problem, that already contains augmented data. This means that the metrics between my and other models can't be directly compared.\n\nHowever, we might still gain some insight from comparison with the other models, especially regarding their size and performance.\n\nI will now upload a model that I trained locally on a different seed and with slightly different training hyperparameters.","5fb711e9":"Shear transformation.","5ce285bf":"Flipping and rotation.","7719d7d2":"(2) It is now trivial to find the contours of the leukocytes, because they are currently the biggest unmasked area of the image.","ddedcb98":"## Data augmentation","6f2906ad":"## Leukocyte isolation by image segmentation with OpenCV","b108b482":"The important detail in the compile function is that I'm applying exponential decay to the learning rate. I'm using the Adam optimizer which seems to be the industry standard.","6b3b58a4":"## First model, baseline","8ef8c3fb":"Brightness adjustments.","5189c186":"## Comparison with other models on Kaggle","10641b36":"There's only a few images that contain more than one cell'; for the sake of simplicity, I removed them from the dataset.","21fe7e53":"## Second model, balanced dataset\n\nThere are many methods of resampling data. In some areas, [SMOTE](https:\/\/arxiv.org\/pdf\/1106.1813.pdf) has been gaining some popularity \u2014 a smart resampling method that generates new observations by sampling the feature space near existing observations. In our case, a simple oversampling of underrepresented classes should be enough.","d613f46d":"The leukocyte masks (i.e. `not background & not erytrocyte`) themselves look like the following.","06af7b4f":"It is also easy to find their bounding boxes.","c33947db":"First, I scale the images from 640x480 to 256x192 to make the training faster. The model won't care about the dimenstions of the input because it's made from CNNs.","c421c149":"The model has 134,853 parameters.","bc4e3776":"I got the [dataset](https:\/\/www.kaggle.com\/paultimothymooney\/blood-cells) from Kaggle. Main characteristics:\n\n- it contains 367 annotated `640 x 480` images of blood samples\n- most of the images contain exactly one white blood cell of a certain type\n    - types: eosinophils, lymphocytes, monocytes, neutrophils, and basophils\n    - each image in the dataset has an accompanying label specifying which type of leukocyte(s) it shows\n\n\n_Note that there is an alternative dataset on Kaggle with augmented data and some additional annotations, but we won't use it to simulate more typical research conditions._\n\nFirst, let's load the dataset and look at the number of each leukocyte type.","c5249762":"__Takeaways:__ Training the _same_ model for the _same_ time on a more balanced dataset has yielded a __dramatic improvement__ in performance.\n\n__Next step:__ The model now has to find the cell in the image and then identify its type. Let's speed up the training (and mabye vene improve the performance) by isolating the cell in the image manually using open CV and feeding only the cells to the model."}}