{"cell_type":{"e6d9b1b5":"code","1f52217c":"code","d09c8b63":"code","19f3a307":"code","dfc654f8":"code","c7166819":"code","528bf13e":"code","489b039e":"code","13fc3318":"code","0cbd006b":"code","09d3a5e7":"code","359ebe55":"code","1fd009e7":"code","78517573":"markdown","c0390e41":"markdown","1fe8a2e8":"markdown","565f99b0":"markdown","d7a7a8db":"markdown","57097974":"markdown","0be376b9":"markdown"},"source":{"e6d9b1b5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.metrics import accuracy_score\nfrom kaggle.competitions import twosigmanews","1f52217c":"env = twosigmanews.make_env()\n(market_train, news_train) = env.get_training_data()\n","d09c8b63":"# news_train['subjects'].nunique()\n# news_train['provider'].nunique()\n# print(news_train['audiences'].nunique())\n# print(news_train['time'].nunique())\n# print(news_train['sourceTimestamp'].nunique())\n# print(news_train['firstCreated'].nunique())\nprint(market_train['time'].nunique())\n# print(market_train['time'].head(10000))\n# news_sub = pd.DataFrame()\n# news_sub[['A','B','C']] = news_train[['assetName','sentimentNegative','sentimentNeutral','sentimentPositive']].groupby('assetName').count()\n# news_sub[['a','b','c']] = news_train[['assetName','sentimentNegative','sentimentNeutral','sentimentPositive']].groupby('assetName').mean()\n# market_train = market_train.join(news_sub,on='assetName',how='left')","19f3a307":"cat_cols = ['assetCode']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n","dfc654f8":"from sklearn.model_selection import train_test_split\n\n# market_train = market_train.loc[pd.to_datetime(market_train['time']) >= datetime(2009, 1, 1)]\nmarket_train = market_train.loc[pd.to_datetime(market_train['time']) >= pd.to_datetime('2009-01-01').tz_localize('UTC')]\n\ntrain_indices, val_indices = train_test_split(market_train.index.values,test_size=0.25, random_state=23)\n","c7166819":"def encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\n\nencoders = [{} for cat in cat_cols]\n\n\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(market_train.loc[train_indices, cat].astype(str).unique())}\n    market_train[cat] = market_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets\n","528bf13e":"from sklearn.preprocessing import StandardScaler\n# print(market_train['A'].describe())\nmarket_train[num_cols] = market_train[num_cols].fillna(0)\nprint('scaling numerical columns')\n\nscaler = StandardScaler()\n\n#col_mean = market_train[col].mean()\n#market_train[col].fillna(col_mean, inplace=True)\nscaler = StandardScaler()\nfrom datetime import datetime\nprint(market_train['time'].dtypes)\nmarket_train[num_cols] = scaler.fit_transform(market_train[num_cols])\n","489b039e":"def get_input(market_train, indices):\n    X_num = market_train.loc[indices, num_cols].values\n    X = {'num':X_num}\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat_cols].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train, train_indices)\nX_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train, val_indices)","13fc3318":"import warnings\nwarnings.filterwarnings(action ='ignore',category = DeprecationWarning)\nfrom xgboost import XGBClassifier\n\nfrom functools import partial\nfrom hyperopt import hp, fmin, tpe\nalgo = partial(tpe.suggest, n_startup_jobs=10)\ndef auto_turing(args):\n    model = XGBClassifier(n_jobs = 4, n_estimators = args['n_estimators'],max_depth=6)\n    model.fit(X_train['num'],y_train.astype(int))\n    confidence_valid = model.predict(X_valid['num'])*2 -1\n    score = accuracy_score(confidence_valid>0,y_valid)\n    print(args,score)\n    return -score\n# space = {\"n_estimators\":hp.choice(\"n_estimators\",range(10,50))}\n# print(fmin)\n# best = fmin(auto_turing, space, algo=algo,max_evals=30)\n# print(best)\n\n#\u5355\u673axgb\u7a0b\u5e8f\n# model = XGBClassifier(n_jobs = 4, n_estimators = 47,max_depth=6)\n# model.fit(X_train['num'],y_train.astype(int))\n# confidence_valid = model.predict(X_valid['num'])*2 -1\n# score = accuracy_score(confidence_valid>0,y_valid)\n# print(score)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=None,min_samples_split=2, min_samples_leaf=1, \n                               min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_split=1e-07, \n                               bootstrap=True,  oob_score=False, n_jobs=1,random_state=None, verbose=0,warm_start=False, class_weight=None)\nmodel.fit(X_train['num'], y_train.astype(int))\nconfidence_valid = model.predict(X_valid['num'])*2 -1\nscore = accuracy_score(confidence_valid>0,y_valid)\nprint(score)\n\n# distribution of confidence that will be used as submission\n# plt.hist(confidence_valid, bins='auto')\n# plt.title(\"predicted confidence\")\n# plt.show()\n","0cbd006b":"# calculation of actual metric that is used to calculate final score\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint(score_valid)","09d3a5e7":"days = env.get_prediction_days()","359ebe55":"n_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    \n    t = time.time()\n\n    market_obs_df['assetCode_encoded'] = market_obs_df[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n\n    market_obs_df[num_cols] = market_obs_df[num_cols].fillna(0)\n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    X_num_test = market_obs_df[num_cols].values\n    X_test = {'num':X_num_test}\n    X_test['assetCode'] = market_obs_df['assetCode_encoded'].values\n    \n    prep_time += time.time() - t\n    \n    t = time.time()\n    market_prediction = model.predict(X_test['num'])*2 -1\n    #print(type(market_prediction))\n    #print(type(predicted_confidences))\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\nenv.write_submission_file()\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')","1fd009e7":"# distribution of confidence as a sanity check: they should be distributed as above\nplt.hist(predicted_confidences, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","78517573":"Result validation","c0390e41":"# Prepare data","1fe8a2e8":"# Market Data Only Baseline\n\nUsing a lot of ideas from NN Baseline Kernel.\nsee. https:\/\/www.kaggle.com\/christofhenkel\/market-data-nn-baseline\nIf this help you, please vote, thankyou!","565f99b0":"# Handling categorical variables","d7a7a8db":"# Handling numerical variables","57097974":"# Train  model using hyperopt to auto hyper_parameters turing","0be376b9":"# Prediction"}}