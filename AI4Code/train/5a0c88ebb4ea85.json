{"cell_type":{"aa7d2bcf":"code","6f6079c3":"code","4488ce05":"code","8c60e0ec":"code","e358dc76":"code","cc6dbe16":"code","cb69aae6":"code","e0c3a56e":"code","cc7c6ef6":"code","2748d549":"code","3653672d":"code","1532ae13":"code","d5f922e0":"code","e59ab454":"code","59660542":"code","bde352f9":"code","e826eb97":"code","ce3e5c16":"code","f85d1070":"code","d897f7ea":"code","b3e072a0":"code","4ad29fa4":"code","5321977b":"code","4b3c263d":"code","d232cd8f":"code","c535562d":"code","182157c7":"code","f298e162":"code","a7068c34":"code","e0447c81":"code","09d7d20c":"code","1025cc64":"code","165c2417":"markdown","b053c441":"markdown","579f4fb0":"markdown","5bccdae9":"markdown","0c4ab3d3":"markdown","e9421b5c":"markdown","c7936149":"markdown","60dd3928":"markdown","617db701":"markdown","a478f7c7":"markdown","e401bca0":"markdown","03b32dea":"markdown","13626013":"markdown","d4bb5c05":"markdown","7f63ee41":"markdown","d3f4896d":"markdown","4eddae9e":"markdown"},"source":{"aa7d2bcf":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport torch\nimport torchvision\nimport torch.nn.functional as F\nfrom torch import nn, optim\n# from torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models, utils\nfrom torch.optim.lr_scheduler import MultiplicativeLR\n# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n# from pytorch_lightning import Trainer\n# from ignite.handlers import EarlyStopping\n# from pytorchtools import EarlyStopping\n\n# import matplotlib.image as Image\nfrom skimage import io\nfrom PIL import Image\nfrom os import listdir\nfrom os.path import isfile, join\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score, accuracy_score, roc_curve, auc, roc_auc_score, precision_score, recall_score\nfrom sklearn.utils import class_weight\n\nimport matplotlib.pyplot as plt\n\nfrom collections import defaultdict \n\nfrom shutil import copyfile\nimport zipfile\n\nfrom tqdm import tqdm\n# from tqdm.notebook import tqdm\nfrom time import sleep\n\nimport warnings\nwarnings.filterwarnings('ignore')","6f6079c3":"GPU = True\ndevice = \"cuda\" if GPU and torch.cuda.is_available() else \"cpu\"\n\nprint(f'Using device {device}')","4488ce05":"df = pd.read_csv('..\/input\/aerial-cactus-identification\/train.csv')\ndf.head()","8c60e0ec":"df.shape","e358dc76":"cmap = plt.get_cmap('Blues')\ncolors = [cmap(i) for i in np.linspace(0, 0.7, df['has_cactus'].unique().shape[0])]\n\nplt.title('\u0421lass distribution')\ndf['has_cactus'].value_counts().plot(kind='pie', figsize=(6, 6), autopct='%1.2f%%', shadow=True, colors=colors)\nplt.show()","cc6dbe16":"submission = pd.read_csv('..\/input\/aerial-cactus-identification\/sample_submission.csv')\nsubmission.head()","cb69aae6":"train_link, test_link = '..\/input\/aerial-cactus-identification\/train.zip', '..\/input\/aerial-cactus-identification\/test.zip'","e0c3a56e":"with zipfile.ZipFile(train_link, \"r\") as z:\n    z.extractall(\".\")\n    \nwith zipfile.ZipFile(test_link, \"r\") as z:\n    z.extractall(\".\")","cc7c6ef6":"Y_train, Y_valid = train_test_split(df, stratify=df['has_cactus'], random_state=42, test_size=0.2)","2748d549":"Y_valid.head()","3653672d":"current_path = '.\/train\/'\nneed_path = '.\/valid\/'\ntest_path = '.\/test\/'\n\ntry:\n    os.makedirs(need_path)\nexcept FileExistsError:\n    pass\n\nfor link in Y_valid['id']:\n    copyfile(current_path + link, need_path + link)","1532ae13":"len(os.listdir(current_path)), len(os.listdir(need_path)), len(os.listdir(test_path))","d5f922e0":"class CactusDataset(Dataset):\n    # \u043f\u0430\u043f\u043a\u0430, \u043c\u0435\u0442\u043a\u0438 \u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f\n    def __init__(self, folder, labels, transform=None):\n        self.transform = transform\n        self.folder = folder\n        self.labels = labels\n    \n    # \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\n    def __len__(self):\n#         return len(self.data)\n        return self.labels.shape[0]\n    \n    # \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043e\u0431\u0440\u0430\u0437\u0435\u0446\n    def __getitem__(self, index):\n        image_path = os.path.join(self.folder, self.labels['id'].iloc[index])\n        label =  self.labels['has_cactus'].iloc[index]\n        \n        image = Image.open(image_path).convert('RGB')\n#         image = cv2.imread(image_path)\n#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        \n        if self.transform is not None:\n            image = self.transform(image)\n\n        return image, label\n    \n    # \u0430\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0438 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435\n    def get_mean_std(self):\n        image_all = np.array([np.array(self[ind][0]) for ind in tqdm(range(self.__len__()))])\n        \n        return image_all.mean(axis=(0, 1, 2)) \/ 255, image_all.std(axis=(0, 1, 2)) \/ 255\n    \n    # \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043d\u0430\u0431\u043e\u0440, \u0438\u043d\u0434\u0435\u043a\u0441\u044b \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0431\u0443\u0434\u0443\u0442 \u043f\u0435\u0440\u0435\u0434\u0430\u043d\u044b                         \n    def view_sample(self, indices, mean, std, count=8):\n        plt.figure(figsize=(count * 3, 3))\n\n        for i, ind in enumerate(indices):\n            image, label = self[ind]\n            \n            if self.transform is not None:\n                image = image.squeeze().permute(1, 2, 0).numpy()\n                image = std * image + mean\n#                 image = np.clip(image, 0, 1)\n            \n            plt.subplot(1, count, i + 1)\n            plt.imshow(image)\n            plt.axis('off')\n            plt.title(f'Cactus: {label == 1}')","e59ab454":"train_dataset = CactusDataset(folder=current_path, labels=Y_train)","59660542":"mean, std = train_dataset.get_mean_std()\nmean, std","bde352f9":"# indices = np.random.choice(np.arange(self.__len__()), count, replace=False)\nindices = list(range(8))\n\ntrain_dataset.view_sample(indices=indices, mean=mean, std=std)","e826eb97":"transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n#     transforms.Resize(50),\n#     transforms.RandomAffine(5, shear=(2, 2)),\n#     transforms.Resize(30),\n    \n    transforms.RandomRotation(10),\n    transforms.CenterCrop(28),\n    \n#     transforms.RandomCrop(28),\n    \n    transforms.Pad(padding=2, padding_mode='symmetric'),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ntrain_dataset = CactusDataset(folder=current_path, labels=Y_train, transform=transform)\nvalid_dataset = CactusDataset(folder=need_path, labels=Y_valid, transform=transform)","ce3e5c16":"train_dataset.view_sample(indices=indices, mean=mean, std=std)","f85d1070":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # \u0441\u0432\u0451\u0440\u0442\u043e\u0447\u043d\u044b\u0435\n        # torch.Size([16, 3, 3, 3]) = torch.Size([out_channels, in_channels, kernel_size[0], kernel_size[1]])\n        # in_channels, out_channels: \u043a\u043e\u043b-\u0432\u043e \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u0438 \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0445 \u043a\u0430\u043d\u0430\u043b\u043e\u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e\n        # kernel_size: \u0440\u0430\u0437\u043c\u0435\u0440 \u044f\u0434\u0440\u0430\n        # padding: \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0443\u0441\u0442\u044b\u0445 (0) \u043f\u0438\u043a\u0441\u0435\u043b\u0435\u0439 \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u0441\u044f\n        # stride: \u043e\u0442\u0441\u0442\u0443\u043f\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n        \n        # \u0438\u0437\u0431\u0435\u0436\u0430\u0442\u044c \u043a\u043e\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u0434\u0432\u0438\u0433\u0430 \u0438 \u043d\u0435 \u043d\u0430\u0441\u044b\u0449\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438\n        # num_features: \u043a\u043e\u043b-\u0432\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0432 \u043f\u0440\u0435\u0434. \u0441\u043b\u043e\u0435\n        self.dense_1 = nn.BatchNorm2d(16)\n        \n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.dense_2 = nn.BatchNorm2d(32)\n        \n        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.dense_3 = nn.BatchNorm2d(64)\n        \n        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.dense_4 = nn.BatchNorm2d(128)\n        \n        # \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u044b\u0435\n        # \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 (batch_size, 512) \u0438 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0442\u0435\u043d\u0437\u043e\u0440 (batch_size, 128). y = x * W^T + b, W.shape=(out_features, in_features), b.shape=(out_features)\n        self.fc1 = nn.Linear(in_features=512, out_features=128)\n        self.fc_dense1 = nn.BatchNorm1d(128)\n        self.out = nn.Linear(in_features=128, out_features=2)\n         \n        # \u043f\u0440\u043e\u0440\u0435\u0436\u0438\u0432\u0430\u043d\u0438\u0435\n        # p: \u0432\u0435\u0440-\u0441\u0442\u044c \u043e\u0442\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u0430\n        self.d1 = nn.Dropout(0.5)\n        # \u0441\u0438\u0433\u043c\u043e\u0438\u0434\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f (\u043b\u043e\u0433\u0438\u0442)\n        self.f = nn.Sigmoid()\n\n        \n    def forward(self, t):\n        x = t\n        x = self.conv1(x)\n        x = self.dense_1(x)\n        # LRELU = max{0, x} + neg * min{0, x}, neg == 0.01\n        x = F.leaky_relu(x)\n        # \u0432\u0437\u044f\u0442\u0438\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0443\u043c\u0430 \u0438\u0437 \u043f\u0440\u044f\u043c\u043e\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a\u043e\u0432 (kernel_size[0], kernel_size[1])\n        # stride: \u043e\u0442\u0441\u0442\u0443\u043f \u043f\u0438\u043a\u0441\u0435\u043b\u0435\u0439 \u043f\u0440\u0438 \u0441\u043e\u0441\u0435\u0434\u043d\u0438\u0445 \u0432\u0437\u044f\u0442\u0438\u044f\u0445 \u043c\u0430\u043a\u0441\u0438\u043c\u0443\u043c\u0430\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n\n\n        x = self.conv2(x)\n        x = self.dense_2(x)\n        x = F.leaky_relu(x)\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n\n        x = self.conv3(x)\n        x = self.dense_3(x)\n        x = F.leaky_relu(x)\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n\n        x = self.conv4(x)\n        x = self.dense_4(x)\n        x = F.leaky_relu(x)\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n\n        # keras -> flatten(): \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0441\u043b\u043e\u0439 \u043f\u043b\u043e\u0442\u043d\u044b\u043c (\u0432\u044b\u0442\u044f\u043d\u0443\u0442\u044c)\n        x = x.reshape(-1, 512)\n\n        x = self.fc1(x)\n        x = self.fc_dense1(x)\n        x = F.leaky_relu(x)\n#         x = self.d1(x)\n        \n        x = self.out(x)\n        x = self.f(x)\n\n        return x","d897f7ea":"# \u043d\u043e\u0440\u043c\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\nlr = 0.02\n\n# \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445 \nepochs = 100\n\n# \u0440\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430\nbatch_size = 64\n\n# \u0432\u0441\u0451 \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435\nnum_workers = 0\n\n# \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043c\u0435\u0442\u0440\u0438\u043a\nmetrics = defaultdict(list)\n\n# \u0441\u0430\u043c\u0430 \u043c\u043e\u0434\u0435\u043b\u044c\nmodel = Model().to(device)\n\n# \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044f \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u043d\u043e\u0440\u043c\u044b \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\nscheduler = MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.9)\n\n# \u0437\u0430\u0433\u0440\u0443\u0437\u0447\u0438\u043a\u0438\ntrain_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\nvalid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)","b3e072a0":"train_data_loader","4ad29fa4":"# \u0438\u0442\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u043e \u0437\u0430\u0433\u0440\u0443\u0437\u0447\u0438\u043a\u0443\ndataiter = iter(train_data_loader)\n\n# \u043e\u0434\u0438\u043d \u0431\u0430\u0442\u0447\nimages, labels = dataiter.next()\n\nplt.figure(figsize=(8, 8))\nplt.axis('off')\nplt.title(\"\u0418\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430\")\nplt.imshow(np.transpose(utils.make_grid(images.to(device), padding=2, normalize=True).cpu(), (1, 2, 0)))\nplt.show()","5321977b":"weight = class_weight.compute_class_weight('balanced', np.unique(Y_train['has_cactus']), Y_train['has_cactus'])\nweight = {i : weight[i] for i in np.unique(Y_train['has_cactus'])}\nweight","4b3c263d":"def other_metrics(y_true, y_pred, weights):\n    balanced_accuracy = balanced_accuracy_score(y_true, y_pred, sample_weight=[weights[i] for i in y_true])\n    accuracy = accuracy_score(y_true, y_pred)\n    precision= precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    roc_auc = roc_auc_score(y_true, y_pred)\n    \n    return np.array([balanced_accuracy, accuracy, precision, recall, roc_auc])","d232cd8f":"# \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445 \u0431\u0435\u0437 \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0448\u0438\u0431\u043a\u0438 (\u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438)\nearly_stopping = 25\n\n# \u0441\u0447\u0451\u0442\u0447\u0438\u043a\nearly_count = 0\n\n# \u043f\u0430\u043f\u043a\u0430 \u0434\u043b\u044f \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\ncheckpoint_dirr = '.\/checkpoint\/'\ntry:\n    os.makedirs(checkpoint_dirr)\nexcept FileExistsError:\n    pass\n\n# \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u0430\u044f \u043b\u0443\u0447\u0448\u0430\u044f \u043f\u043e\u0442\u0435\u0440\u044f\nbest_loss = np.inf\n\n# \u043c\u0430\u0442\u0440\u0438\u043a\u0438\nbalanced_accuracy, accuracy, precision, recall, roc_auc = [0] * 5\n\n# \u043f\u043e \u043a\u0430\u0436\u0434\u043e\u0439 \u044d\u043f\u043e\u0445\u0435\nfor epoch in range(epochs):\n    # \u0448\u0430\u0433 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f learning rate\n    scheduler.step()\n    \n    # \u043e\u0431\u043d\u0443\u043b\u0438\u0442\u044c loss \u043e\u0448\u0438\u0431\u043a\u0443 \u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0438\n    train_loss = 0\n    metric = 0\n    model.train()\n    \n    with tqdm(train_data_loader, unit=\"batch\") as tepoch:\n        tepoch.set_description(f\"Epoch {epoch}\")\n        # \u043f\u043e \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u0431\u0430\u0442\u0447\u0443\n        for i, batch in enumerate(tepoch):\n\n            # \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0438\u0437\u043e \u0438 \u043c\u0435\u0442\u043a\u0438 \u0438\u0437 \u0431\u0430\u0442\u0447\u0430\n            images, labels = batch\n            \n            # \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u043e\u0434 \u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u043e\n            images = images.to(device)\n            labels = labels.to(device) \n\n            # \u0432 64 \u0431\u0438\u0442\u0430\n            labels = labels.long()\n            \n            # \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u044b\n            preds = model(images)\n            pred = preds.argmax(dim=1)\n            \n            # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c: https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.CrossEntropyLoss.html\n            loss = F.cross_entropy(preds, labels)\n\n            # \u043e\u0431\u043d\u0443\u043b\u0438\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043f\u0435\u0440\u0435\u0434 \u043e\u0431\u0440\u0430\u0442\u043d\u044b\u043c \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u0435\u043c\n            optimizer.zero_grad()\n            \n            # \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043f\u043e\u0442\u0435\u0440\u044c\n            loss.backward()\n            \n            # \u043e\u0431\u043d\u043e\u0432\u0438\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b\n            optimizer.step()\n            \n            # \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043a\u0438\n            train_loss += loss.item()\n            \n            # \u0434\u0440\u0443\u0433\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438\n            metric += other_metrics(labels.tolist(), pred.tolist(), weight)\n            balanced_accuracy, accuracy, precision, recall, roc_auc = metric\n            \n            tepoch.set_postfix(loss=train_loss \/ (i + 1), balanced_accuracy=balanced_accuracy \/ (i + 1), accuracy=accuracy \/ (i + 1), precision=precision \/ (i + 1), \n                               recall=recall \/ (i + 1), roc_auc=roc_auc \/ (i + 1), lr=scheduler.get_lr()[0])\n        \n        # \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u044c\n        metrics['loss'].append(train_loss \/ (i + 1))\n        metrics['balanced_accuracy'].append(balanced_accuracy \/ (i + 1))\n        metrics['accuracy'].append(accuracy \/ (i + 1))\n        metrics['precision'].append(precision \/ (i + 1))\n        metrics['recall'].append(recall \/ (i + 1))\n        metrics['roc_auc'].append(roc_auc \/ (i + 1))\n    \n    sleep(0.1)\n    \n    model.eval()\n    \n    # \u0442\u043e\u0436\u0435 \u0441\u0430\u043c\u043e\u0435, \u043d\u043e \u0431\u0435\u0437 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043e\u0432 (\u0442.\u043a. \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f)\n    with torch.no_grad():\n        valid_loss = 0\n        metric = 0\n        with tqdm(valid_data_loader, unit=\"batch\") as tepoch:\n            tepoch.set_description(f\"Epoch {epoch}\")\n        \n            for i, valid_batch in enumerate(tepoch):\n                valid_images, valid_labels = valid_batch\n                valid_images = valid_images.to(device)\n                valid_labels = valid_labels.to(device)\n                valid_labels = valid_labels.long()\n\n                valid_preds = model(valid_images)\n                valid_pred = valid_preds.argmax(dim=1)\n\n                loss_valid = F.cross_entropy(valid_preds ,valid_labels)\n                \n                valid_loss += loss_valid.item()\n                \n                metric += other_metrics(valid_labels.tolist(), valid_pred.tolist(), weight)\n                balanced_accuracy, accuracy, precision, recall, roc_auc = metric\n                \n                tepoch.set_postfix(loss=valid_loss \/ (i + 1), balanced_accuracy=balanced_accuracy \/ (i + 1), accuracy=accuracy \/ (i + 1), precision=precision \/ (i + 1), \n                               recall=recall \/ (i + 1), roc_auc=roc_auc \/ (i + 1))\n\n            metrics['val_loss'].append(valid_loss \/ (i + 1))\n            metrics['val_balanced_accuracy'].append(balanced_accuracy \/ (i + 1))\n            metrics['val_accuracy'].append(accuracy \/ (i + 1))\n            metrics['val_precision'].append(precision \/ (i + 1))\n            metrics['val_recall'].append(recall \/ (i + 1))\n            metrics['val_roc_auc'].append(roc_auc \/ (i + 1))\n\n    sleep(0.1)\n    \n    # \u0435\u0441\u043b\u0438 \u043e\u0448\u0438\u0431\u043a\u0430 \u0443\u043b\u0443\u0447\u0448\u0438\u043b\u0430\u0441\u044c\n    if valid_loss < best_loss:\n        # \u043f\u0435\u0440\u0435\u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c\n        best_loss = valid_loss\n        \n        # \u0437\u0430\u043d\u0443\u043b\u0438\u0442\u044c \u0441\u0447\u0451\u0442\u0447\u0438\u043a\n        early_count = 0\n        \n        # \u0437\u0430\u043f\u0438\u0441\u0430\u0442\u044c \n        torch.save(model.state_dict(), f'{checkpoint_dirr}epoch:{epoch}.pt')\n    else:\n        early_count += 1\n\n        if early_count > early_stopping:\n            print(f\"Loss did not improve over {early_stopping} epochs => early stopping\")\n            break","c535562d":"def graph_plot(history, typ=False):\n    if typ:\n        for i in history.keys():\n            print(f'{i} = [{min(history[i])}; {max(history[i])}]\\n')\n    \n    epoch = len(history['loss'])\n    # \u043d\u0430 \u043a\u0430\u0436\u0434\u0443\u044e: (train, val) + lr\n    size = len(history.keys()) \/\/ 2 + 1\n    \n    ncols = 3\n    nrows = np.ceil(size \/ ncols)\n    \n    fig = plt.figure(figsize=(30, 20))\n    i = 1\n    for k in list(history.keys()):\n        if 'val' not in k:\n            fig.add_subplot(nrows, ncols, i)\n            plt.plot(history[k][2:], marker='o', markersize=5)\n            if k != 'lr':\n                plt.plot(history['val_' + k][2:], marker='o', markersize=5)\n            plt.title(k, fontsize=10)\n\n            plt.ylabel(k)\n            plt.xlabel('epoch')\n            plt.grid()\n\n            plt.yticks(fontsize=10, rotation=30)\n            plt.xticks(fontsize=10, rotation=30)\n            plt.legend(['train', 'valid'], loc='upper left', fontsize=10, title_fontsize=15)\n            i += 1\n#         plt.show()\n\ngraph_plot(metrics)","182157c7":"np.argmax(-np.array(metrics['val_loss']))","f298e162":"[name for name in sorted(os.listdir(checkpoint_dirr)) if str(np.argmax(-np.array(metrics['val_loss']))) in name][0]","a7068c34":"model = Model().to(device)\nmodel.load_state_dict(torch.load(f\"{checkpoint_dirr}{[name for name in sorted(os.listdir(checkpoint_dirr)) if str(np.argmax(-np.array(metrics['val_loss']))) in name][0]}\"))\nmodel.eval()","e0447c81":"test_dataset = CactusDataset(folder=test_path, labels=submission, \n                             transform=transforms.Compose([transforms.ToTensor(), \n                                                           transforms.Normalize(mean=mean, std=std)]))\n\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)","09d7d20c":"predict = []\n\nfor test_batch in test_loader:\n    data, target = test_batch\n    data, target = data.to(device), target.to(device)\n    output = model(data)\n\n    predict += output.argmax(dim=1).tolist()\n\nsubmission['has_cactus'] = predict\nsubmission.to_csv('submission.csv', index=False)","1025cc64":"submission.head()","165c2417":"## \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b","b053c441":"## \u041d\u0435\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0441\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u043e\u0432","579f4fb0":"## Model Class","5bccdae9":"## \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430 \u0441\u0435\u0442\u0438","0c4ab3d3":"## \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445","e9421b5c":"## \u0411\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438","c7936149":"## GPU","60dd3928":"## \u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430","617db701":"https:\/\/www.aiworkbox.com\/lessons\/how-to-define-a-convolutional-layer-in-pytorch\n\nhttps:\/\/github.com\/vdumoulin\/conv_arithmetic\/blob\/master\/README.md\n\nhttps:\/\/wandb.ai\/authors\/ayusht\/reports\/Implementing-and-Tracking-the-Performance-of-a-CNN-in-Pytorch-An-Example--VmlldzoxNjEyMDU\n\nhttps:\/\/www.aiworkbox.com\/lessons\/batchnorm2d-how-to-use-the-batchnorm2d-module-in-pytorch\n\nhttps:\/\/stackoverflow.com\/questions\/53419474\/using-dropout-in-pytorch-nn-dropout-vs-f-dropout\n\nhttps:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU\n\n\u043f\u043e\u0447\u0435\u043c\u0443 leaky_relu\n\nhttps:\/\/www.quora.com\/What-are-the-advantages-of-using-Leaky-Rectified-Linear-Units-Leaky-ReLU-over-normal-ReLU-in-deep-learning","a478f7c7":"## \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0447\u0430\u0441\u0442\u0435\u0439","e401bca0":"## \u041f\u0440\u043e\u0433\u043d\u043e\u0437","03b32dea":"## DataFrame","13626013":"## \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043d\u043e\u0432\u043e\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430","d4bb5c05":"## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043b\u0443\u0447\u0448\u0435\u0439","7f63ee41":"## \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438","d3f4896d":"## Dataset Class","4eddae9e":"## \u0413\u0440\u0430\u0444\u0438\u043a\u0438"}}