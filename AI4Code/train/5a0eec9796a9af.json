{"cell_type":{"7a0f6ca4":"code","ffcb7ada":"code","e40c2664":"code","4fe2cea1":"code","5bd3a488":"code","b3af350c":"code","593f1c2b":"code","990e2cf9":"code","9d83d923":"code","d801d2b5":"code","70fae529":"code","8ff7b0f0":"code","06e25822":"code","cf642aa0":"code","fcf7abf1":"code","eaf76fe5":"code","77b9ba48":"markdown","cb67852f":"markdown","5e63353d":"markdown","ef82e144":"markdown","8b662015":"markdown","e9c53bdc":"markdown","4724f09f":"markdown","94d7d05b":"markdown","0c3583f3":"markdown"},"source":{"7a0f6ca4":"!ls ..\/input\n!ls ..\/input\/ogawa4-6","ffcb7ada":"# \u30d1\u30c3\u30b1\u30fc\u30b8\u306eimport\nimport random\nimport math\nimport time\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n","e40c2664":"# \u521d\u671f\u8a2d\u5b9a\n# Setup seeds\ntorch.manual_seed(1234)\nnp.random.seed(1234)\nrandom.seed(1234)\n","4fe2cea1":"dir_ogawa = \"\/kaggle\/input\/ogawa4\/\"\ndir_weights = \"\/kaggle\/input\/ogawa4-6\/weights\/\"\ndir_data = dir_ogawa + \"data\/\"\n#dir_utils = dir_ogawa + \"utils\/\"\n#dir_weights = dir_ogawa + \"weights\/\"\n\nimport sys\n\n#if sys.path[0] != utils_weights_dir:\n#    sys.path.insert(0,utils_weights_dir)\nif not dir_ogawa in sys.path:\n    sys.path.append(dir_ogawa)\n#sys.path.remove('..\/input\/ogawa4\/')\nsys.path\n!mkdir -p weights\n!ls -l $dir_weights","5bd3a488":"from utils.dataloader import make_datapath_list, DataTransform, COCOkeypointsDataset\n\n# MS COCO\u306e\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u30ea\u30b9\u30c8\u4f5c\u6210\ntrain_img_list, train_mask_list, val_img_list, val_mask_list, train_meta_list, val_meta_list = make_datapath_list(\n    rootpath=dir_data)\n#    rootpath=\".\/data\/\")\n\n#\n# make_datapath_list\u3092\u4fee\u6b63\u3057\u306a\u304b\u3063\u305f\u306e\u3067\u3053\u3053\u3067\u518d\u8abf\u6574\n#\nval_mask_list = [dir_ogawa + m[2:] for m in val_mask_list]\ntrain_mask_list = [dir_ogawa + m[2:] for m in train_mask_list]\n\n# Dataset\u4f5c\u6210\n# \u672c\u66f8\u3067\u306f\u30c7\u30fc\u30bf\u91cf\u306e\u554f\u984c\u304b\u3089\u3001train\u3092val_list\u3067\u4f5c\u6210\u3057\u3066\u3044\u308b\u70b9\u306b\u6ce8\u610f\ntrain_dataset = COCOkeypointsDataset(\n    val_img_list, val_mask_list, val_meta_list, phase=\"train\", transform=DataTransform())\n\n# \u4eca\u56de\u306f\u7c21\u6613\u306a\u5b66\u7fd2\u3068\u3057\u691c\u8a3c\u30c7\u30fc\u30bf\u306f\u4f5c\u6210\u3057\u306a\u3044\n#val_dataset = COCOkeypointsDataset(val_img_list, val_mask_list, val_meta_list, phase=\"val\", transform=DataTransform())\n\n# DataLoader\u4f5c\u6210\nbatch_size = 32\n\ntrain_dataloader = data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True)\n\n#val_dataloader = data.DataLoader(\n#    val_dataset, batch_size=batch_size, shuffle=False)\n\n# \u8f9e\u66f8\u578b\u5909\u6570\u306b\u307e\u3068\u3081\u308b\n#dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\ndataloaders_dict = {\"train\": train_dataloader, \"val\": None}\n","b3af350c":"import cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nindex = 24\n\nprint(val_meta_list[index])\nprint(val_img_list[index])\nprint(val_mask_list[index])\n\nimg = cv2.imread(val_img_list[index])\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(img)\nplt.show()\n\n\nmask_miss = cv2.imread(val_mask_list[index])\nmask_miss = cv2.cvtColor(mask_miss,cv2.COLOR_BGR2RGB)\nplt.imshow(mask_miss)\nplt.show()\n\nblend_img = cv2.addWeighted(img,0.4,mask_miss,0.6,0)\nplt.imshow(blend_img)\nplt.show()\n\nitem=train_dataset.__getitem__(index)\nprint(item[0].shape)  # img\nprint(item[1].shape)  # heatmaps,\nprint(item[2].shape)  # heat_mask\nprint(item[3].shape)  # pafs \nprint(item[4].shape)  # paf_mask\n","593f1c2b":"from utils.openpose_net import OpenPoseNet\nnet = OpenPoseNet()\n","990e2cf9":"# \u5b66\u7fd2\u6e08\u307f\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30ed\u30fc\u30c9\u3059\u308b\nnet_weights = torch.load(\n    dir_weights+'openpose_net_30.pth', map_location={'cuda:0': 'cpu'})\nkeys = list(net_weights.keys())\n\nweights_load = {}\n\n# \u30ed\u30fc\u30c9\u3057\u305f\u5185\u5bb9\u3092\u3001\u672c\u66f8\u3067\u69cb\u7bc9\u3057\u305f\u30e2\u30c7\u30eb\u306e\n# \u30d1\u30e9\u30e1\u30fc\u30bf\u540dnet.state_dict().keys()\u306b\u30b3\u30d4\u30fc\u3059\u308b\nfor i in range(len(keys)): # newkey <- oldkey dictionary \u306flist\u5316\u3057\u3066index\u5bfe\u5fdc \n    weights_load[list(net.state_dict().keys())[i]\n                 ] = net_weights[list(keys)[i]]\n\n# \u30b3\u30d4\u30fc\u3057\u305f\u5185\u5bb9\u3092\u30e2\u30c7\u30eb\u306b\u4e0e\u3048\u308b\nstate = net.state_dict()\nstate.update(weights_load)\nnet.load_state_dict(state)\n\nprint('\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8a2d\u5b9a\u5b8c\u4e86\uff1a\u5b66\u7fd2\u6e08\u307f\u306e\u91cd\u307f\u3092\u30ed\u30fc\u30c9\u3057\u307e\u3057\u305f')\nprint(net)","9d83d923":"# \u640d\u5931\u95a2\u6570\u306e\u8a2d\u5b9a\nclass OpenPoseLoss(nn.Module):\n    \"\"\"OpenPose\u306e\u640d\u5931\u95a2\u6570\u306e\u30af\u30e9\u30b9\u3067\u3059\u3002\"\"\"\n\n    def __init__(self):\n        super(OpenPoseLoss, self).__init__()\n\n    def forward(self, saved_for_loss, heatmap_target, heat_mask, paf_target, paf_mask):\n        \"\"\"\n        \u640d\u5931\u95a2\u6570\u306e\u8a08\u7b97\u3002\n\n        Parameters\n        ----------\n        saved_for_loss : OpenPoseNet\u306e\u51fa\u529b(\u30ea\u30b9\u30c8)\n\n        heatmap_target : [num_batch, 19, 46, 46]\n            \u6b63\u89e3\u306e\u90e8\u4f4d\u306e\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u60c5\u5831\n\n        heatmap_mask : [num_batch, 19, 46, 46]\n            heatmap\u753b\u50cf\u306emask\n\n        paf_target : [num_batch, 38, 46, 46]\n            \u6b63\u89e3\u306ePAF\u306e\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u60c5\u5831\n\n        paf_mask : [num_batch, 38, 46, 46]\n            PAF\u753b\u50cf\u306emask\n\n        Returns\n        -------\n        loss : \u30c6\u30f3\u30bd\u30eb\n            \u640d\u5931\u306e\u5024\n        \"\"\"\n\n        total_loss = 0\n        # \u30b9\u30c6\u30fc\u30b8\u3054\u3068\u306b\u8a08\u7b97\u3057\u307e\u3059\n        for j in range(6):\n\n            # PAFs\u3068heatmaps\u306b\u304a\u3044\u3066\u3001\u30de\u30b9\u30af\u3055\u308c\u3066\u3044\u308b\u90e8\u5206\uff08paf_mask=0\u306a\u3069\uff09\u306f\u7121\u8996\u3055\u305b\u308b\n            # PAFs\n            pred1 = saved_for_loss[2 * j] * paf_mask\n            gt1 = paf_target.float() * paf_mask\n\n            # heatmaps\n            pred2 = saved_for_loss[2 * j + 1] * heat_mask\n            gt2 = heatmap_target.float()*heat_mask\n\n            total_loss += F.mse_loss(pred1, gt1, reduction='mean') + \\\n                F.mse_loss(pred2, gt2, reduction='mean')\n\n        return total_loss\n\n\ncriterion = OpenPoseLoss()\n","d801d2b5":"update_params = []\nfor name, param in net.named_parameters():    \n    if 'model0' in name:\n        print(name)\n        param.requires_grad = False\n    else:\n        param.requires_grad = True\n        update_params.append(param)","70fae529":"optimizer = optim.SGD(update_params,#net.parameters(), \n                      lr=1e-2,\n                      momentum=0.9,\n                      weight_decay=0.0001,\n                    nesterov= True)\n","8ff7b0f0":"#\n# W = W - lr*DW\n#     |W|*0.05(5%)> lr*|DW|\n#      W\/DW*rate*lower_factor > lr -> lr*2\n#                upper_factor < lr -> lr\/2\n#\ndef schedule_lr(optimizer,rate=0.05,lower_factor=0.1,upper_factor=10.):\n    r = sys.maxsize\n    for g in optimizer.param_groups:\n        for p in g['params']:\n            if p.grad is None:\n                print(f\"parameter is None, and requires_grad={p.requires_grad}\")\n                return\n            W_DW=p.abs()\/(p.grad.abs()+1.e-6)\n            #W_DW=p.norm()\/(p.grad.norm()+1.e-6)\n            #print(\"min\", W_DW.min())\n            r = min(r,W_DW.min())\n    \n    r *= rate\n    \n    for i, g in enumerate(optimizer.param_groups):\n        lr = g['lr']\n#        print(\"lr=\", lr)\n        new_lr = None\n        \n        if r*lower_factor > lr:\n            new_lr = 2.0*lr\n        elif r*upper_factor < lr:\n            new_lr = lr\/2.\n        if new_lr is not None:\n            g['lr'] = new_lr\n            print(f\"group[{i}] {lr:.3e} is changed to {new_lr:.3e}.\")\n            print(f\"(r={r:.2e} rate={rate:.2e} lower={lower_factor:.2e} upper={upper_factor:.2e})\")\n\n    return\n","06e25822":"def visualize_tmasks(masks,img=None):\n    f, axis = plt.subplots(4, 5, figsize=(8, 8))\n    for k in range(len(masks)):\n        i = k\/\/5\n        j = k%5\n        ax = axis[i,j]\n        m = masks[k].detach().numpy()\n        ax.imshow(m)\n        ax.set_title(f\"{k} {np.max(m):.2e}\")\n    if img is not None:\n        axis[2,3].imshow(img.detach().numpy().astype(int))\n        axis[2,3].set_title(f\"Org {torch.max(img):.2e}\")\n    plt.show()\n","cf642aa0":"from torch.optim.lr_scheduler import ReduceLROnPlateau\n\ndef hotstart_setup(model):\n    print(\"HOT START!!!\")\n    for param in model.parameters():\n        param.requires_grad = True\n\n    trainable_vars = [param for param in model.parameters() if param.requires_grad]\n    optimizer = torch.optim.SGD(trainable_vars,\n                      lr=1e-2,\n                      momentum=0.9,\n                      weight_decay=0.0001)\n                                                    \n    lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=3, min_lr=0, eps=1e-08)\n    return optimizer, lr_scheduler","fcf7abf1":"# \u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3055\u305b\u308b\u95a2\u6570\u3092\u4f5c\u6210\n\n\ndef train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n\n    # GPU\u304c\u4f7f\u3048\u308b\u304b\u3092\u78ba\u8a8d\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"\u4f7f\u7528\u30c7\u30d0\u30a4\u30b9\uff1a\", device)\n\n    # \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092GPU\u3078\n    net.to(device)\n\n    # \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u3042\u308b\u7a0b\u5ea6\u56fa\u5b9a\u3067\u3042\u308c\u3070\u3001\u9ad8\u901f\u5316\u3055\u305b\u308b\n    torch.backends.cudnn.benchmark = True\n\n    # \u753b\u50cf\u306e\u679a\u6570\n    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n    batch_size = dataloaders_dict[\"train\"].batch_size\n    \n    #num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n\n    # \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u30ab\u30a6\u30f3\u30bf\u3092\u30bb\u30c3\u30c8\n    iteration = 1\n\n    early_stop_count = 0\n    early_stop_loss = 9999\n    # epoch\u306e\u30eb\u30fc\u30d7\n    for epoch in range(num_epochs):\n\n        # \u958b\u59cb\u6642\u523b\u3092\u4fdd\u5b58\n        t_epoch_start = time.time()\n        t_iter_start = time.time()\n        epoch_train_loss = 0.0  # epoch\u306e\u640d\u5931\u548c\n        epoch_val_loss = 0.0  # epoch\u306e\u640d\u5931\u548c\n\n        print('-------------')\n        print('Epoch {}\/{}'.format(epoch+1, num_epochs))\n        print('-------------')\n\n        # epoch\u3054\u3068\u306e\u8a13\u7df4\u3068\u691c\u8a3c\u306e\u30eb\u30fc\u30d7\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                net.train()  # \u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u30e2\u30fc\u30c9\u306b\n                optimizer.zero_grad()\n                print('\uff08train\uff09')\n\n            # \u4eca\u56de\u306f\u691c\u8a3c\u306f\u30b9\u30ad\u30c3\u30d7\n            else:\n                continue\n                #net.eval()   # \u30e2\u30c7\u30eb\u3092\u691c\u8a3c\u30e2\u30fc\u30c9\u306b\n                #print('-------------')\n                #print('\uff08val\uff09')\n\n            # \u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\u304b\u3089minibatch\u305a\u3064\u53d6\u308a\u51fa\u3059\u30eb\u30fc\u30d7\n            for imges, heatmap_target, heat_mask, paf_target, paf_mask in dataloaders_dict[phase]:\n                # \u30df\u30cb\u30d0\u30c3\u30c1\u304c\u30b5\u30a4\u30ba\u304c1\u3060\u3068\u3001\u30d0\u30c3\u30c1\u30ce\u30fc\u30de\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3\u3067\u30a8\u30e9\u30fc\u306b\u306a\u308b\u306e\u3067\u3055\u3051\u308b\n                if imges.size()[0] == 1:\n                    continue\n\n                # GPU\u304c\u4f7f\u3048\u308b\u306a\u3089GPU\u306b\u30c7\u30fc\u30bf\u3092\u9001\u308b\n                imges = imges.to(device)\n                heatmap_target = heatmap_target.to(device)\n                heat_mask = heat_mask.to(device)\n                paf_target = paf_target.to(device)\n                paf_mask = paf_mask.to(device)\n\n                # optimizer\u3092\u521d\u671f\u5316\n                optimizer.zero_grad()\n\n                # \u9806\u4f1d\u642c\uff08forward\uff09\u8a08\u7b97\n                with torch.set_grad_enabled(phase == 'train'):\n                    # (out6_1, out6_2)\u306f\u4f7f\u308f\u306a\u3044\u306e\u3067 _ \u3067\u4ee3\u66ff\n                    _, saved_for_loss = net(imges)\n                    \n                    saved_for_loss = [torch.nn.functional.relu(x) for x in saved_for_loss]\n\n                    loss = criterion(saved_for_loss, heatmap_target,\n                                     heat_mask, paf_target, paf_mask)\n\n                    # \u8a13\u7df4\u6642\u306f\u30d0\u30c3\u30af\u30d7\u30ed\u30d1\u30b2\u30fc\u30b7\u30e7\u30f3\n                    if phase == 'train':\n                        loss.backward()\n                        #schedule_lr(optimizer) # be called before zero_grad\n                        optimizer.step()\n\n                        if (iteration % 10 == 0):  # 10iter\u306b1\u5ea6\u3001loss\u3092\u8868\u793a\n                            t_iter_finish = time.time()\n                            duration = t_iter_finish - t_iter_start\n                            print('\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3 {} || Loss: {:.4f} || 10 iter: {:.4f} sec.'.format(\n                                iteration, loss.item()\/batch_size, duration))\n                            t_iter_start = time.time()\n                            \n                            #visualize_tmasks(saved_for_loss[5][0].cpu())\n                            #visualize_tmasks(heatmap_target[0].cpu())\n\n\n                        epoch_train_loss += loss.item()\n                        iteration += 1\n\n                    # \u691c\u8a3c\u6642\n                    else:\n                        epoch_val_loss += loss.item()\n\n        # epoch\u306ephase\u3054\u3068\u306eloss\u3068\u6b63\u89e3\u7387\n        t_epoch_finish = time.time()\n        \n        visualize_tmasks(saved_for_loss[5][0].cpu())\n        visualize_tmasks(heatmap_target[0].cpu())\n\n        print('-------------')\n        print('epoch {} || Epoch_TRAIN_Loss:{:.4e} ||Epoch_VAL_Loss:{:.4e}'.format(\n            epoch+1, epoch_train_loss\/num_train_imgs, \n            0.0))#epoch_val_loss\/num_val_imgs))\n        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n        t_epoch_start = time.time()\n        \n        if epoch_train_loss <= early_stop_loss:\n            early_stop_loss = epoch_train_loss\n            early_stop_count = 0\n        else:\n            early_stop_count += 1\n        #if early_stop_count > 3:\n        #    break\n        if epoch == 5:\n            optimizer, lr_scheduler = hotstart_setup(net)\n        if epoch > 5:\n            lr_scheduler.step(epoch_train_loss)\n\n    # \u6700\u5f8c\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4fdd\u5b58\u3059\u308b\n    torch.save(net.state_dict(), 'weights\/openpose_net_' +\n               str(epoch+1) + '.pth')\n","eaf76fe5":"# \u5b66\u7fd2\u30fb\u691c\u8a3c\u3092\u5b9f\u884c\u3059\u308b\nnum_epochs = 40 #30#2\ntrain_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)\n","77b9ba48":"# 4.6 \u5b66\u7fd2\u3068\u691c\u8a3c\u306e\u5b9f\u65bd\n\n- \u672c\u30d5\u30a1\u30a4\u30eb\u3067\u306f\u3001OpenPose\u306e\u5b66\u7fd2\u3068\u691c\u8a3c\u306e\u5b9f\u65bd\u3092\u884c\u3044\u307e\u3059\u3002AWS\u306eGPU\u30de\u30b7\u30f3\u3067\u8a08\u7b97\u3057\u307e\u3059\u3002\n- p2.xlarge\u306745\u5206\u307b\u3069\u304b\u304b\u308a\u307e\u3059\u3002\n","cb67852f":"# \u4e8b\u524d\u6e96\u5099\n\n- \u3053\u308c\u307e\u3067\u306e\u7ae0\u3067\u5b9f\u88c5\u3057\u305f\u30af\u30e9\u30b9\u3068\u95a2\u6570\u3092\u30d5\u30a9\u30eb\u30c0\u300cutils\u300d\u5185\u306b\u7528\u610f\u3057\u3066\u3044\u307e\u3059\n","5e63353d":"# \u6700\u9069\u5316\u624b\u6cd5\u3092\u8a2d\u5b9a","ef82e144":"# \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u4f5c\u6210","8b662015":"# \u640d\u5931\u95a2\u6570\u3092\u5b9a\u7fa9","e9c53bdc":"# \u5b66\u7fd2\u3092\u5b9f\u65bd","4724f09f":"\u4ee5\u4e0a","94d7d05b":"# \u5b66\u7fd2\u76ee\u6a19\n\n1.\tOpenPose\u306e\u5b66\u7fd2\u3092\u5b9f\u88c5\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b","0c3583f3":"# DataLoader\u4f5c\u6210"}}