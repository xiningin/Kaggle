{"cell_type":{"fa6dae49":"code","dfbce202":"code","adbe16df":"code","ed81cccd":"code","1ab4418e":"code","01822c13":"code","a4f52786":"code","99b9b2d2":"code","5ba62e83":"code","fab47309":"code","8605d9af":"code","ae5769d8":"code","c719220a":"code","7091c81a":"code","cc131ef5":"code","683c2bb2":"code","d8f6230d":"code","115def1b":"code","ea800372":"code","1bfee149":"code","e893ed5f":"code","5119e4c3":"code","a9b9cc53":"code","824de4fa":"code","275d3dca":"code","a6bb1d7e":"code","72affbd5":"code","767d0788":"code","608d2851":"code","ddf6a69f":"code","625dc796":"code","0175dc8b":"code","9930049b":"markdown","d48203d2":"markdown","4a0f8caf":"markdown","9d5b3265":"markdown","fa9c400c":"markdown","a71878cf":"markdown","de8a1636":"markdown","3af49522":"markdown","9528e8da":"markdown","4b6f0715":"markdown","fe9acc75":"markdown","1bdc8114":"markdown","59a2450a":"markdown","aaf841c5":"markdown","35c33c74":"markdown","b94d0108":"markdown","538f0122":"markdown","0f4f7320":"markdown","89b36dac":"markdown","a4f01f30":"markdown","79c033d0":"markdown","a5a76580":"markdown","b515b8bf":"markdown","13e0e555":"markdown","6b97a32b":"markdown","b96f06f9":"markdown","315e4a4d":"markdown","377d583b":"markdown","b169417a":"markdown","119a0586":"markdown"},"source":{"fa6dae49":"from fastai.text import *\nfrom fastai.text.all import *\nimport nltk\nimport spacy\nimport pandas as pd\nimport numpy as np\nimport fastai\nimport torch\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import classification_report\ntqdm.pandas()\nprint(f\"fastai version: {fastai.__version__}\")\nif torch.cuda.is_available():\n    print(f\"GPU which is used : {torch.cuda.get_device_name(0)}\")\n    \n\n## defining directories\nroot = Path().absolute()\ndata_dir = root \/ \"..\/input\/feedback-prize-2021\"\ntrain_text_dir = data_dir \/ \"train\"\ntest_text_dir = data_dir \/ \"test\"\nout = root \/ \"out\"\nout.mkdir(exist_ok=True)\n# data_dir.ls()","dfbce202":"df = pd.read_csv(data_dir \/ \"train.csv\", dtype={'discourse_id': int, 'discourse_start': int, 'discourse_end': int})\nprint('\\nHere is the data sample : \\n\\n')\ndf.head(2)","adbe16df":"def visualize_classes(file_name):\n    ents = []\n    for i, row in df[df['id'] == file_name].iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n    with open(f'{train_text_dir}\/{file_name}.txt', 'r') as file: data = file.read()\n\n    doc2 = {\n        \"text\": data,\n        \"ents\": ents,\n        \"title\": file_name\n    }\n    colors = {'Lead': '#c1dbd5',\n            'Position': '#fcf2b6',\n            'Claim': '#bbceae',\n            'Evidence': '#c8f1bf',\n            'Counterclaim': '#d3b88a',\n            'Concluding Statement': '#ed9a8b',\n            'Rebuttal': '#ef8c9d'}\n\n    options = {\"ents\": df.discourse_type.unique().tolist(), \"colors\": colors}\n    spacy.displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)\n    print('\\n')","ed81cccd":"for i in df['id'].sample(n=4, random_state=2).values.tolist():\n    visualize_classes(i)","1ab4418e":"ID2CLASS = dict(enumerate(df['discourse_type'].unique().tolist() + ['No Class']))\nCLASS2ID = {v: k for k, v in ID2CLASS.items()}\n# print(ID2CLASS)\nCLASS2ID","01822c13":"def get_text(source_dir,a_id):\n    a_file = f\"{source_dir}\/{a_id}.txt\"\n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\ndef fill_gaps(elements, text):\n    \"\"\"Add \"No Class\" elements to a list of elements (see get_elements) \"\"\"\n    initial_idx = 0\n    final_idx = len(text)\n\n    # Add element at the beginning if it doesn't in index 0\n    new_elements = []\n    if elements[0][0] != initial_idx:\n        starting_element = (0, elements[0][0]-1, 'No Class')\n        new_elements.append(starting_element)\n\n\n    # Add element at the end if it doesn't in index \"-1\"\n    if elements[-1][1] != final_idx:\n        closing_element = (elements[-1][1]+1, final_idx, 'No Class')\n        new_elements.append(closing_element)\n\n    elements += new_elements\n    elements = sorted(elements, key=lambda x: x[0])\n\n    # Add \"No class\" elements inbetween separated elements \n    new_elements = []\n    for i in range(1, len(elements)-1):\n        if elements[i][0] != elements[i-1][1] + 1 and elements[i][0] != elements[i-1][1]:\n            new_element = (elements[i-1][1] + 1, elements[i][0]-1, 'No Class')\n            new_elements.append(new_element)\n\n    elements += new_elements\n    elements = sorted(elements, key=lambda x: x[0])\n    return elements\n\n\ndef get_elements(df, text_id, do_fill_gaps=True, text=None):\n    \"\"\"Get a list of (start, end, class) elements for a given text_id\"\"\"\n    text = get_text(text_id) if text is None else text\n    df_text = df[df['id'] == text_id]\n    elements = df_text[['discourse_start', 'discourse_end', 'discourse_type']].to_records(index=False).tolist()\n    if do_fill_gaps:\n        elements = fill_gaps(elements, text)\n    return elements\n\ndef get_x_samples(df, text_id, do_fill_gaps=True):\n    \"\"\"Create a dataframe of the sentences of the text_id, with columns text, label \"\"\"\n    text = get_text(train_text_dir,text_id)\n    elements = get_elements(df, text_id, do_fill_gaps, text)\n    sentences = []\n    for start, end, class_ in elements:\n        elem_sentences = nltk.sent_tokenize(text[start:end])\n        sentences += [(sentence, class_) for sentence in elem_sentences]\n    df = pd.DataFrame(sentences, columns=['text', 'label'])\n    df['label'] = df['label'].map(CLASS2ID)\n    return df","a4f52786":"text_ids = df['id'].unique().tolist()","99b9b2d2":"x = []\nfor text_id in tqdm(text_ids):\n    x.append(get_x_samples(df, text_id))\n\ndf_sentences = pd.concat(x)\ndf_sentences = df_sentences[df_sentences.text.str.split().str.len() >= 3]\ndf_sentences.to_csv(\"df_sentence.csv\", index=False)","5ba62e83":"df_sample = df_sentences.sample(n=3000, random_state=42)\nprint('class distribution in sample set :')\ndf_sample['label'].value_counts()","fab47309":"dls_lm = TextDataLoaders.from_df(df_sample, path=data_dir,is_lm=True)\ndls_lm.show_batch(max_n=3)","8605d9af":"torch.save(dls_lm, out \/ 'dls_lm.pkl')\n# dls_lm = torch.load(out \/ 'dls_lm.pkl')","ae5769d8":"learn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=data_dir, wd=0.1, model_dir=\"\/tmp\/model\/\").to_fp16()","c719220a":"learn.lr_find()","7091c81a":"learn.fit_one_cycle(1, 1e-3)","cc131ef5":"learn.unfreeze()\nlearn.fit_one_cycle(7, 1e-4)","683c2bb2":"learn.save_encoder(out \/ 'finetuned')","d8f6230d":"dls_clas = TextDataLoaders.from_df(df_sentences, path=data_dir, text_col='text', label_col='label',text_vocab=dls_lm.vocab)\ndls_clas.show_batch(max_n=3)","115def1b":"torch.save(dls_clas, out \/ 'dls_clas.pkl')\n# dls_clas = torch.load(out \/ 'dls_clas.pkl')","ea800372":"metrics=[accuracy,F1Score(average='micro')]\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=metrics, model_dir=\"\/tmp\/model\/\")","1bfee149":"learn = learn.load_encoder(out \/ 'finetuned')","e893ed5f":"learn.lr_find()","5119e4c3":"lr = 1e-2\nlearn.fit_one_cycle(1, lr)","a9b9cc53":"learn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(lr\/(2.6**4),lr))","824de4fa":"learn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(lr\/2\/(2.6**4),lr\/2))","275d3dca":"learn.unfreeze()\nlearn.fit_one_cycle(2, slice(lr\/10\/(2.6**4),lr\/10))","a6bb1d7e":"def create_df_test():\n    test_ids = [f[:-4] for f in os.listdir(test_text_dir)]\n    test_data = []\n    for test_id in test_ids:\n        text = get_text(test_text_dir, test_id)\n        sentences = nltk.sent_tokenize(text)\n        id_sentences = []\n        idx = 0 \n        for sentence in sentences:\n            id_sentence = []\n            words = sentence.split()\n            # I created this heuristic for mapping words in senteces to \"word indexes\"\n            # This is not definitive and might have strong drawbacks and problems\n            for w in words:\n                id_sentence.append(idx)\n                idx+=1\n            id_sentences.append(id_sentence)\n        test_data += list(zip([test_id] * len(sentences), sentences, id_sentences))\n    df_test = pd.DataFrame(test_data, columns=['id', 'text', 'ids'])\n    return df_test","72affbd5":"df_test = create_df_test()\ndf_test.head()","767d0788":"def predict(txt):\n    with learn.no_bar(), learn.no_logging():\n        return int(learn.predict(txt)[0])","608d2851":"df_test['predictions'] = df_test[\"text\"].progress_apply(lambda x: predict(x))\n\ndf_test['class'] = df_test['predictions'].map(ID2CLASS)\ndf_test.head()","ddf6a69f":"df_test['predictionstring'] = df_test['ids'].apply(lambda x: ' '.join([str(i) for i in x]))\ndf_test.head()","625dc796":"df_test = df_test[df_test['class'] != 'No Class']\ndf_test.head()","0175dc8b":"# submit\ndf_test[['id', 'class', 'predictionstring']].to_csv(\"submission.csv\", index=False)","9930049b":"### Prepare test data","d48203d2":"Here is abit of information about using `slice` in learning rate","4a0f8caf":"\n\n<!-- ![](https:\/\/cdn.dribbble.com\/users\/2353146\/screenshots\/9073115\/media\/eadd1ef050b49fbdf20d42e7c504458d.png)\nImage is made by [\u0410\u043b\u0435\u043d\u0430 \u041c\u043e\u043b\u0447\u0430\u043d\u043e\u0432\u0430](https:\/\/dribbble.com\/shots\/9073115-WRITING?utm_source=Clipboard_Shot&utm_campaign=mimipig&utm_content=WRITING&utm_medium=Social_Share&utm_source=Clipboard_Shot&utm_campaign=mimipig&utm_content=WRITING&utm_medium=Social_Share) -->\n\n# Here is a baseline notebook for [Feedback Prize - Evaluating Student Writing](https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/)\n\n\n**The notebook outline:** \n1. spacy EDA \n2. Text preprocessing and transforming data to sentences using this great notebook: [\ud83d\udcd6Feedback- Baseline\ud83e\udd17 Sentence Classifier [0.226]](https:\/\/www.kaggle.com\/julian3833\/feedback-baseline-sentence-classifier-0-226)\n3. Train model with [ULMFIT](https:\/\/arxiv.org\/pdf\/1708.02182v1.pdf) using fastai library for sentence classficiation\n4. Submit\n\n\n","9d5b3265":"Next, we finetune the model. By default, a pretrained Learner is in a frozen state, meaning that only the head of the model will train while the body stays frozen.","fa9c400c":"Then we can define our text classifier like before:\n\nDefing metrics: we use [accuracy](https:\/\/docs.fast.ai\/metrics.html#accuracy) and [F1Score](https:\/\/docs.fast.ai\/metrics.html#F1Score)","a71878cf":"Visualize 4 sample of data","de8a1636":"Applying different lr for different groups is a technique called \u201cdiscriminative layer training\u201d that is introduced in part 1. This technique is commonly used in both computer vision and natural language processing.\nyou can find some good info by reading this blog post: [The 1cycle policy](https:\/\/sgugger.github.io\/the-1cycle-policy.html#the-1cycle-policy)","3af49522":"### preprocessing","9528e8da":"Here we instantiate our [language_model_learner](https:\/\/docs.fast.ai\/text.learner.html#language_model_learner) and using [Mixed precision training](https:\/\/docs.fast.ai\/callback.fp16.html#Learner.to_fp16) by adding `.to_fp16()` at the end of code","4b6f0715":"and save dataloader for later use","fe9acc75":"As said before we load our encoder that we saved so that we use the exact same vocabulary as when we were fine-tuning our language model.","1bdc8114":"Here we gather our data for text classification almost exactly like before.\n\nThe main difference is that we have to use the exact same vocabulary as when we were fine-tuning our language model, or the weights learned won't make any sense. We pass that vocabulary with vocab by adding `text_vocab=dls_lm.vocab`","59a2450a":"And here are some helper functions to transform data to sentences\n* fill_gaps(), classifies parts of the texts that has not label as \"No Class\" \n* get_elements(), creates a list of text sections for a each text\n* get_x_samples(), maps each sentences to their labels.","aaf841c5":"### Finetune AWD-LSTM on our corpus to train [ULMFIT](https:\/\/docs.fast.ai\/tutorial.text.html) ","35c33c74":"We can then fine-tune the model after unfreezing","b94d0108":"Drop the sentences with label \"No class\" ","538f0122":"Slice trainset, in order to train the kernel faster\n\nIt should be removed when training model for submission","0f4f7320":"Turn the word ids into predictionstring ","89b36dac":"### Make prediction","a4f01f30":"Therefore, in your last line, `slice(5e-3\/(2.6**4),5e-3)` is equivalent to `slice(start = 5e-3\/(2.6**4), stop = 5e-3, step = None)`","79c033d0":"### Visualize classes","a5a76580":"The last step is to train with discriminative learning rates and gradual unfreezing. In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference.","b515b8bf":"Create [TextDataLoaders](https:\/\/docs.fast.ai\/text.data.html#TextDataLoaders.from_df) from our df_sentences. keep in mind that using `is_lm` to `True` because later on we want to use it for finetuning our language model.","13e0e555":"Convert classes into ints, also we put `No Class` for sentences with no label","6b97a32b":"### First we do necessary imports and load the data","b96f06f9":"And here we build our sentence dataframe","315e4a4d":"Saving encoder for later use in text classification\nNOTE THAT\n\n`\nEncoder: The model not including the task-specific final layer(s). It means much the same thing as body when applied to vision CNNs, but tends to be more used for NLP and generative models\n`","377d583b":"**Refrences:**\n\n- [Transfer learning in text](https:\/\/docs.fast.ai\/tutorial.text.html)\n- [Efficient multi-lingual language model fine-tuning](https:\/\/nlp.fast.ai\/)\n- [Universal Language Model Fine-tuning for Text Classification](https:\/\/arxiv.org\/abs\/1801.06146)","b169417a":"slice() can be passed 1 or 2 arguments only. Below is a snippet of experiments for your reference:\n\n```python\nIn [9]: slice(5)\nOut[9]: slice(None, 5, None)\n\nIn [10]: slice(1, 5)\nOut[10]: slice(1, 5, None)\n```","119a0586":"Using learning rate finder of fastai. Here we plot the loss versus the learning rates. We're interested in finding a good order of magnitude of the learning rate, so we plot with a log scale. Then, we choose a value that is approximately in the middle of the sharpest downward slope.\n\nFor more information on the finding the good learning rate you can refer to this post: [how do you find a good learning rate](https:\/\/sgugger.github.io\/how-do-you-find-a-good-learning-rate.html)"}}