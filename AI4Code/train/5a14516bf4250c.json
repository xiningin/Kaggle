{"cell_type":{"06fd9d08":"code","60909f55":"code","3d4293bc":"code","31ec5489":"code","6258e01b":"code","18682b88":"code","5b50acc8":"code","fac11afe":"code","38bfbc11":"code","5956e2c1":"code","c50e2e26":"code","b21c6297":"code","0e05df45":"code","3dc72c69":"code","9cdb7529":"code","d8617945":"code","346e4acb":"code","0546579c":"code","31c9fc0c":"markdown","4c05669c":"markdown","9a5ccdb4":"markdown","657c528f":"markdown","0df8afc7":"markdown","bef73741":"markdown","179e6279":"markdown","30fa593a":"markdown","918ddbd0":"markdown","6a83122a":"markdown","14c00b6e":"markdown"},"source":{"06fd9d08":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\n\ntrain_sales = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')","60909f55":"#To reduce memory usage\ndef downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\n#Reduce memory usage and compare with the previous one to be sure\ntrain_sales = downcast_dtypes(train_sales)","3d4293bc":"train_sales.head()","31ec5489":"# Preprocess: remove id, item_id, dept_id, cat_id, store_id, state_id columns\nstartDay = 350  # Remove the first 350 days in train sales data due to zero_inflated data\ntrain_sales = train_sales.T\ntrain_sales = train_sales[6 + startDay:]\ntrain_sales.head(5)","6258e01b":"# Initialize a dataframe with zeros for 1969 days in the calendar\n\ndaysBeforeEvent1 = pd.DataFrame(np.zeros((1969,1)))\ndaysBeforeEvent2 = pd.DataFrame(np.zeros((1969,1)))\n\nsnap_CA = pd.DataFrame(np.zeros((1969,1)))\nsnap_TX = pd.DataFrame(np.zeros((1969,1)))\nsnap_WI = pd.DataFrame(np.zeros((1969,1)))\n\n\n# Label 1 to one day before the event_name_1 \n# Label 1 to one day before the event_name_2\n# Sales are likely to increase one day before events like superbowl etc.\n\n# Label 1 to days on snap_CA\n# Label 1 to days on snap_TX\n# Label 1 to days on snap_WI\n\nfor x,y in calendar.iterrows():\n    if((pd.isnull(calendar[\"event_name_1\"][x])) == False):\n           daysBeforeEvent1[0][x-1] = 1 \n            \n    if((pd.isnull(calendar[\"event_name_2\"][x])) == False):\n           daysBeforeEvent2[0][x-1] = 1    \n    \n    \n    if((pd.isnull(calendar[\"snap_CA\"][x])) == False):\n           snap_CA[0][x] = 1    \n        \n    if((pd.isnull(calendar[\"snap_TX\"][x])) == False):\n           snap_TX[0][x] = 1    \n        \n    if((pd.isnull(calendar[\"snap_WI\"][x])) == False):\n           snap_WI[0][x] = 1","18682b88":"# split dataset into evaluation (last 2 weeks), validation (first 2 weeks), training  \n# input for predicting validation period day 1941 to 1969\n\ndaysBeforeEvent1_eval = daysBeforeEvent1[1941:]\ndaysBeforeEvent2_eval = daysBeforeEvent2[1941:]\n\nsnap_CA_eval = snap_CA[1941:]\nsnap_TX_eval = snap_TX[1941:]\nsnap_WI_eval = snap_WI[1941:]\n\n\n# input for predicting validation period day 1913 to 1941\n\ndaysBeforeEvent1_valid = daysBeforeEvent1[1913:1941] \ndaysBeforeEvent2_valid = daysBeforeEvent2[1913:1941]\n\nsnap_CA_valid = snap_CA[1913:1941] \nsnap_TX_valid = snap_TX[1913:1941]\nsnap_WI_valid = snap_WI[1913:1941]\n\n# input for training as a feature\n# daysBeforeEvent1 = daysBeforeEvent1[startDay:1913] \n# daysBeforeEvent2 = daysBeforeEvent2[startDay:1913] \ndaysBeforeEvent1 = daysBeforeEvent1[startDay:1941] \ndaysBeforeEvent2 = daysBeforeEvent2[startDay:1941]\n\nsnap_CA = snap_CA[startDay:1941] \nsnap_TX = snap_TX[startDay:1941] \nsnap_WI = snap_WI[startDay:1941] ","5b50acc8":"#Before concatanation with our main data \"dt\", indexes are made same and column name is changed to \"oneDayBeforeEvent\"\ndaysBeforeEvent1.columns = [\"oneDayBeforeEvent1\"]\ndaysBeforeEvent1.index = train_sales.index\n\ndaysBeforeEvent2.columns = [\"oneDayBeforeEvent2\"]\ndaysBeforeEvent2.index = train_sales.index\n\n\nsnap_CA.columns = [\"snap_CA\"]\nsnap_CA.index = train_sales.index\n\nsnap_TX.columns = [\"snap_TX\"]\nsnap_TX.index = train_sales.index\n\nsnap_WI.columns = [\"snap_WI\"]\nsnap_WI.index = train_sales.index","fac11afe":"train_sales = pd.concat([train_sales, daysBeforeEvent1, daysBeforeEvent2,\n                        snap_CA, snap_TX, snap_WI], axis = 1, sort=False)\ntrain_sales.head()  # additional features (event1, event2, SNAP) are added","38bfbc11":"#Feature Scaling: Scale features using min-max scaler in range 0-1\nfrom sklearn.preprocessing import MinMaxScaler\n\nsc = MinMaxScaler(feature_range = (0, 1))\n\ntrain_sales_scaled = sc.fit_transform(train_sales)","5956e2c1":"timesteps = 28  # use the last 28 days to predict the next day's sales\nX_train = []\ny_train = []\n\nfor i in range(timesteps, 1941 - startDay):\n    X_train.append(train_sales_scaled[i-timesteps:i])\n    y_train.append(train_sales_scaled[i][0:30490])","c50e2e26":"#Convert to np array to be able to feed the LSTM model\nX_train = np.array(X_train)\ny_train = np.array(y_train)\nprint(X_train.shape)\nprint(y_train.shape)","b21c6297":"import tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM,Dropout\n\ntf.random.set_seed(51)\nnp.random.seed(51)\n\nn_timesteps = X_train.shape[1]\nn_products = X_train.shape[2]\n\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=128, kernel_size=7,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=(n_timesteps, n_products)),\n    tf.keras.layers.MaxPooling1D(),\n    tf.keras.layers.Conv1D(filters=64, kernel_size=7, \n                           strides=1, activation='relu', padding=\"causal\"),\n    tf.keras.layers.MaxPooling1D(),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(30490)\n])\n\n\nopt_adam = tf.keras.optimizers.Adam(clipvalue=0.5)\n\nmodel.compile(loss='mse',\n              optimizer=opt_adam, \n              metrics=[tf.keras.metrics.RootMeanSquaredError()])\nmodel.summary()","0e05df45":"# Define a Callback class that stops training when the val_loss degrades above 150 epochs\n# and also saves the model as checkpoints\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)","3dc72c69":"# Fitting the RNN to the Training set\nepochs = 150\nbatch_size = 100\nmodel.fit(X_train, y_train, epochs = epochs, batch_size = batch_size)","9cdb7529":"inputs_eval = train_sales[-timesteps:]\ninputs_eval = sc.transform(inputs_eval)\n\ninputs = train_sales[-timesteps*2:-timesteps]\ninputs = sc.transform(inputs)","d8617945":"X_test = []\nX_test.append(inputs[0:timesteps])\nX_test = np.array(X_test)\npredictions = []\n\nfor j in range(timesteps,timesteps + 28):\n    predicted_stock_price = model.predict(X_test[0,j - timesteps:j].reshape(1, timesteps, 30495))\n    \n    testInput = np.column_stack((np.array(predicted_stock_price),\n                                 daysBeforeEvent1_valid.loc[1913 + j - timesteps],\n                                 daysBeforeEvent2_valid.loc[1913 + j - timesteps],\n                                 snap_CA_valid.loc[1913 + j - timesteps],\n                                snap_TX_valid.loc[1913 + j - timesteps],\n                                snap_WI_valid.loc[1913 + j - timesteps]))\n\n    X_test = np.append(X_test, testInput).reshape(1,j + 1,30495)\n    predicted_stock_price = sc.inverse_transform(testInput)[:,0:30490]\n    predictions.append(predicted_stock_price)","346e4acb":"X_eval = []\nX_eval.append(inputs_eval[0:timesteps])\nX_eval = np.array(X_eval)\npredictions_eval = []\n\nfor j in range(timesteps,timesteps + 28):\n    predicted_stock_price = model2.predict(X_eval[0,j - timesteps:j].reshape(1, timesteps, 30495))\n    \n    testInput = np.column_stack((np.array(predicted_stock_price),\n                                 daysBeforeEvent1_eval.loc[1941 + j - timesteps],\n                                 daysBeforeEvent2_eval.loc[1941 + j - timesteps],\n                                 snap_CA_eval.loc[1941 + j - timesteps],\n                                snap_TX_eval.loc[1941 + j - timesteps],\n                                snap_WI_eval.loc[1941 + j - timesteps]))\n\n    X_eval = np.append(X_eval, testInput).reshape(1,j + 1,30495)\n    predicted_stock_price = sc.inverse_transform(testInput)[:,0:30490]\n    predictions_eval.append(predicted_stock_price)","0546579c":"import time\n\nsubmission = pd.DataFrame(data=np.array(predictions).reshape(28,30490))\nsubmission = submission.T\n\nsubmission_eval = pd.DataFrame(data=np.array(predictions_eval).reshape(28,30490))\nsubmission_eval = submission_eval.T\n\nsubmission = pd.concat((submission, submission_eval), ignore_index=True)\n\nsample_submission = pd.read_csv(\"sample_submission.csv\")\n    \nidColumn = sample_submission[[\"id\"]]\n    \nsubmission[[\"id\"]] = idColumn  \n\ncols = list(submission.columns)\ncols = cols[-1:] + cols[:-1]\nsubmission = submission[cols]\n\ncolsdeneme = [\"id\"] + [f\"F{i}\" for i in range (1,29)]\n\nsubmission.columns = colsdeneme\n\ncurrentDateTime = time.strftime(\"%d%m%Y_%H%M%S\")\n\ncols = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n       'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20',\n       'F21', 'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']\n\nsubmission[cols] = submission[cols].mask(submission[cols] < 0, 0)\nresult.to_csv(\"submission.csv\", index=False)","31c9fc0c":"### Creating training, validation, evaluation dataset\n\n**Training Dataset**\n- To train a timeseries dataset, we will use the training dataset, which comprises of features (sales, event 1, event 2, SNAP) from day 350 to 1912. \n\n- From this training dataset, we will predict the future sales of each product for 56 days in 2 week windows (predict day 1913 to 1940, and predict day 1941 to 1969).\n\n**Validation Dataset**\n- The dataset with the features (sales, event 1, event 2, SNAP) from day 1913 to 1940 is used to validate the predicted values from trained dataset. \n\n**Evaluation Dataset**\n- The dataset with the features (sales, event 1, event 2, SNAP) from day 1941 to 1969 is used to evaluate the predicted values from trained dataset. \n","4c05669c":"### Standardizing features\n\n- It is also important to scale our features across the columns. Each columns represents the sales values of a particular day. This helps to ensure that the sales values are between 0 and 1, and this helps the gradient descent optimization algorithm in LSTM model.","9a5ccdb4":"## Introduction\n\nThe Makridakis competitions (or M-competitions), organised by forecasting expert Spyros Makridakis, aim to provide a better understanding and advancement of forecasting methodology by comparing the performance of different methods in solving a well-defined, real-world problem. The first M-competition was held in 1982. According to forecasting researcher and practitioner Rob Hyndman the M-competitions \u201chave had an enormous influence on the field of forecasting. They focused attention on what models produced good forecasts, rather than on the mathematical properties of those models\u201d. This empirical approach is very similar to Kaggle\u2019s trade-mark way of having the best machine learning algorithms engage in intense competition on diverse datasets. M5 is the first M-competition to be held on Kaggle.\n\n\n## Objective\n\nTo predict sales data provided by the retail giant Walmart 4 weeks into the future (two 2-week windows). \n\n\n## Dataset\n\nThe data: We are working with 42,840 hierarchical time series. The data were obtained in the 3 US states of California (CA), Texas (TX), and Wisconsin (WI). \u201cHierarchical\u201d here means that data can be aggregated on different levels: item level, department level, product category level, and state level. The sales information reaches back from Jan 2011 to June 2016. In addition to the sales numbers, we are also given corresponding data on prices, promotions, and holidays. The dataset is zero-inflated, that is entries with empty information are imputed with a zero value. Hence, most of the time series contain zero values.\n\nThe data comprises 3049 individual products from 3 categories and 7 departments, sold in 10 stores in 3 states. The hierachical aggregation captures the combinations of these factors. For instance, we can create 1 time series for all sales, 3 time series for all sales per state, and so on. The largest category is sales of all individual 3049 products per 10 stores for 30490 time series.\n\n\n## Dataset files\nThe training data comes in the shape of 3 separate files:\n\n`sales_train.csv`: this is our main training data. It has 1 column for each of the 1941 days from 2011-01-29 and 2016-05-22; not including the validation period of 28 days until 2016-06-19. It also includes the IDs for item, department, category, store, and state. The number of rows is 30490 for all combinations of 30490 items and 10 stores.\n\n`sell_prices.csv`: the store and item IDs together with the sales price of the item as a weekly average.\n\n`calendar.csv`: dates together with related features like day-of-the week, month, year, and an 3 binary flags for whether the stores in each state allowed purchases with SNAP food stamps at this date (1) or not (0).\n\n\n## Metrics\n\nThe point forecast submission are being evaluated using the **Root Mean Squared Scaled Error (RMSSE)**, which is derived from the Mean Absolute Scaled Error (MASE) that was designed to be scale invariant and symmetric. In a similar way to the MASE, the RMSSE is scale invariant and symmetric, and measures the prediction error (i.e. forecast - truth) relative to a \u201cnaive forecast\u201d that simply assumes that step i = step i-1. In contrast to the MASE, here both prediction error and naive error are scaled to account for the goal of estimating average values in the presence of many zeros.\n\nThe metric is computed for each time series and then averaged accross all time series including weights. The weights are proportional to the sales volume of the item, in dollars, to give more importance to high selling products. Note, that the weights are based on the last 28 days of the training data, and that those dates will be adjusted for the ultimate evaluation data, as confirmed by the organisers.","657c528f":"### Additional features: event 1, event 2 and SNAP\n\nBesides using sales as a time series feature, festive or sports events can have a strong positive influence on the sales. For example, shoppers are more likely to purchase more snacks or food a day before Thanksgiving or Superbowl. Similarly, we expect more sales during the days with SNAP program. Hence, we inserted 5 additional features: event 1, event 2, SNAP_CA, SNAP_WI, SNAP_TX. The days of SNAP program is different for each store location and therefore we need 3 separate features for SNAP.  ","0df8afc7":"### Table of Contents\n\n* [ Load data and downcasting ](#downcast)\n\n* [ Preprocessing ](#preprocess)\n\n* [ CNN-LSTM Model](#cnnlstm)","bef73741":"## Callbacks\n\nCallbacks are inserted to monitor the performance of model after 100 epochs. If the performance of the model degrades, then the model is stopped.","179e6279":"<a name=\"cnnlstm\"><\/a>\n## CNN-LSTM Model\n\nThe model used is CNN-LSTM, with regularization parameters such as batch normalization. There are 2 units of CNN (conv1d followed by max pooling layer). The conv1d layer helps to analyse and extract the patterns of the sales in 1 week window (just like extracting \"edges\" patterns in a typcial conv2d on an image). The max pooling layer then summarizes these patterns into broader, generalizable patterns, which  seeks to remove noises in the spikes or troughs in daily sales.\n\nThe outputs from CNN is then fed into a LSTM model with 3 layers of bidirectional LSTM. We use an inverted number of LSTM units from 512 to 256 to 128. This has the property of reducing and summarizing the sales patterns for better prediction. The batch normalization helps to improve the robustness of the model by reducing the likelihood of overfitting. ","30fa593a":"<a name=\"downcast\"><\/a>\n## Load data and downcasting\n\nDowncasting the dataframes helps to reduce the amount of storage used by them and also to expidite the operations performed on them.\n\nNumerical Columns: Depending on your environment, pandas automatically creates int32, int64, float32 or float64 columns for numeric ones. If you know the min or max value of a column, you can use a subtype which is less memory consuming. You can also use an unsigned subtype if there is no negative value. In this dataset, we convert features with a float64 data type into float32 data type, and convert features with int64, int32 to in16 data type.","918ddbd0":"## Post processing of future sales\n\nFor sales with predicted negative values, they are converted into zeros. ","6a83122a":"<a name=\"preprocess\"><\/a>\n## Preprocess dataset\n\nBased on the sell price dataset, there are multiple products which has zero prices from day 1 to day 350. This is due to the product not being listed in the store yet. Additionally, on the sales_train dataset, there are multiple products which has zero sales, This could be due to zero prices or that the sales is not recorded. \n\nSince our LSTM model predicts the future sales of multiple products instead of the sales of individual products, hence we need to set a cutoff data instead of trimming the dataset by the first non-zero prices. Hence we set this cutoff date on day 350 ","14c00b6e":"## Predicting the future sales for validation and evaluation periods"}}