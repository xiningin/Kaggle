{"cell_type":{"f6ba8465":"code","36cbc3ec":"code","b4dd4b33":"code","d6bd67f1":"code","52602c1b":"code","8f2cb930":"code","f63b8681":"code","1e279c46":"code","2feb3341":"code","3511e2e3":"code","033bfb6d":"code","8fd485d0":"code","08924563":"code","960604ed":"markdown","e293c79b":"markdown","efe41b62":"markdown","a37e575f":"markdown","8246c194":"markdown","be095c7d":"markdown","73229f62":"markdown","c6dec4f0":"markdown","275272e0":"markdown","46f0aab3":"markdown"},"source":{"f6ba8465":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\nfrom spacy.lang.hi import Hindi\nfrom spacy.lang.ta import Tamil\nfrom spacy.lang.hi import STOP_WORDS as hindi_stopwords\nfrom spacy.lang.ta import STOP_WORDS as tamil_stopwords\n\n\nseed=111\nnp.random.seed(seed)\n\n%config IPCompleter.use_jedi = False","36cbc3ec":"# Path to the data diectory\ndata_dir = Path(\"..\/input\/chaii-hindi-and-tamil-question-answering\/\")\n\n# Read the training and test csv files\ntrain_df = pd.read_csv(data_dir \/ \"train.csv\", encoding=\"utf8\")\ntest_df = pd.read_csv(data_dir \/ \"test.csv\", encoding=\"utf8\")\n\n# How many training and test samples have been provided?\nprint(\"Number of training samples: \", len(train_df))\nprint(\"Number of test samples: \", len(test_df))","b4dd4b33":"train_df.head()","d6bd67f1":"test_df.head()","52602c1b":"plt.figure(figsize=(8, 5))\nsns.countplot(data=train_df, x=\"language\")\nplt.show()","8f2cb930":"# Get the actual count values\ntrain_df[\"language\"].value_counts()","f63b8681":"train_df[\"question\"] = train_df[\"question\"].str.replace(\"?\", \"\", regex=False).str.strip()\ntrain_df.head()","1e279c46":"# Get the text for both the languages\ntamil_text = \" \".join(train_df[train_df[\"language\"]==\"tamil\"][\"question\"])\nhindi_text = \" \".join(train_df[train_df[\"language\"]==\"hindi\"][\"question\"])","2feb3341":"# Download and extract the fonts\n!wget -q http:\/\/www.lipikaar.com\/sites\/www.lipikaar.com\/themes\/million\/images\/support\/fonts\/Devanagari.zip\n!wget -q http:\/\/www.lipikaar.com\/sites\/www.lipikaar.com\/themes\/million\/images\/support\/fonts\/Tamil.zip\n\n!unzip -qq Devanagari.zip\n!unzip -qq Tamil.zip","3511e2e3":"# Get the tokens and frequencies for Hindi language\n\nhindi_nlp = Hindi()\nhindi_doc = hindi_nlp(hindi_text)\nhindi_tokens = set([token.text for token in hindi_doc])\nhindi_tokens_counter = Counter(hindi_tokens)\n\n\n# Get the tokens and frequencies for Tamil language\ntamil_nlp = Tamil()\ntamil_doc = hindi_nlp(tamil_text)\ntamil_tokens = set([token.text for token in tamil_doc])\ntamil_tokens_counter = Counter(tamil_tokens)","033bfb6d":"def plot_wordcloud(\n    font_path,\n    frequencies,\n    stopwords,\n    width=500,\n    height=500,\n    background_color=\"white\",\n    collocations=True,\n    min_font_size=5,\n):\n    \"\"\"Generates wordcloud from word frequencies.\"\"\"\n    \n    wordcloud = WordCloud(font_path=font_path,\n                      width=width,\n                      height=height,\n                      background_color=background_color,\n                      stopwords=stopwords,\n                      collocations=collocations,\n                      min_font_size=min_font_size).generate_from_frequencies(frequencies)\n\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","8fd485d0":"# Plot the wordcloud for hindi langauge\nplot_wordcloud(font_path=\"Devanagari\/Lohit-Devanagari.ttf\",\n               frequencies=hindi_tokens_counter,\n               stopwords=hindi_stopwords\n              )","08924563":"# Plot the wordcloud for tamil language\nplot_wordcloud(font_path=\"Tamil\/Lohit-Tamil.ttf\",\n               frequencies=tamil_tokens_counter,\n               stopwords=tamil_stopwords\n              )","960604ed":"# The Task\n\nYou are given questions in Tamil\/Hindi about some Wikipedia articles, and you have to generate the answers for those questions using your model.\n\n## Dataset\n\nWe have been provided with a new question-answering dataset with question-answer pairs, and it goes by the name **`chaii-1`**.\n\n## Evaluation Metric\n\nThe predictions would be evaluated using **`word-level Jaccard score`**. A sample code has also been provided for the same.\n\n```python\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n```\n\nLet's statrt diving into the data!","e293c79b":"This suggests that the number of instances for `Hindi` language is almost double the number of instance of `Tamil` language in the training dataset. Let's also get the actual count to see the difference ","efe41b62":"# WordCloud\n\nWe will generate two wordclouds, one for each language. ","a37e575f":"# chaii - Hindi and Tamil Question Answering\n\nHello Kagglers! This competition is pretty cool and a bit harder than the Q&A datasets we generally work on. Most of the datasets and the research done focuses on the English language. Although these models show good performance on the English language datasets, they don't work very well on the Indian languages. The [Indian languages ecosystem](https:\/\/en.wikipedia.org\/wiki\/Languages_of_India#Prominent_languages_of_India) is as diverse as India is. If you consider all the languages and dialects, then almost **19,000** languages or dialects are spoken by Indians daily.","8246c194":"For generating the wordlcoud, we need the right `font`\n\n1. [Font for Hindi](http:\/\/www.lipikaar.com\/support\/download-unicode-fonts-for-hindi-marathi-sanskrit-nepali)\n2. [Font for Tamil](http:\/\/www.lipikaar.com\/support\/download-unicode-fonts-for-tamil)\n\n**Note:** I haven't checked how accurate the gven stopwords are, this is something you need to cross-check!","be095c7d":"That's it for the EDA! We will build a LM in the next notebook!","73229f62":"There are only ~1100 training samples, meaning we are in a low data regime, suggesting that transfer-learning and fine-tuning are the best shots if we are going to use DNNs for this task. This doesn't mean you shouldn't build your models!\n\nLet's take a look at the training data and the test data","c6dec4f0":"# Distribution of the languages in the training dataset\n\nLet's see how many samples we have for each language in the training dataset. For this we can use `countplot(..)`","275272e0":"A few things to note:\n\n1. There can be English words as well in the given questions\n2. `answer_start` column isn't in the test dataset, but it gives important information about the training dataset, the starting character for the context\n3. The `language` column is present in both `train` and `test`. One of the things that we can try is to build two separate models, one for `Hindi` and one for `Tamil`, and then make the predictions accordingly using the values in this column","46f0aab3":"# Remove punctuation\n\nAll the questions presented here are represented with a question mark. We will simply remove it and along with it, we will alos strip any whitespace around the text"}}