{"cell_type":{"cf0199ca":"code","a9d4cf70":"code","490a2e38":"code","1ea72724":"code","d0c3149a":"code","ab90a899":"code","b6f485db":"code","bf29a9d3":"code","7f41373b":"code","cbeccdae":"code","52037c78":"code","329695d7":"code","edd25640":"code","b5026a02":"code","318d1cf0":"code","d33cbf0b":"code","09272e8c":"code","61c04d25":"code","11c69e0f":"code","a8406938":"code","571496dc":"code","9c6eb56f":"code","ee1cb76f":"code","bd3eaf1e":"code","67aeed34":"code","cc54a3f3":"code","e1313b2e":"code","5e41d854":"code","db5616c3":"code","c7e9ef06":"code","7d46b36b":"code","b7c907ba":"code","3519f39e":"code","612c1d95":"code","a4f15ae5":"code","44c38fe8":"code","be6b0ff8":"code","47166bb3":"code","f92279de":"code","042b3e47":"code","8a6654eb":"code","b316abec":"code","99434614":"code","ba9697b7":"code","12aa5744":"code","82696a62":"code","2df25c24":"code","d03e0a35":"code","ed79bb84":"code","f61c91c1":"code","e4172213":"code","63121bd7":"code","81b28f52":"code","00367e83":"markdown","409bbf7c":"markdown","b7d88395":"markdown","f755381f":"markdown","ec0ca592":"markdown","6999a339":"markdown","a06d66e1":"markdown","bcaa6db0":"markdown","139285d5":"markdown","b402c6e3":"markdown","4d1c72bb":"markdown","33235607":"markdown","32c2d8ac":"markdown","fddd6ead":"markdown","3c0c59fb":"markdown"},"source":{"cf0199ca":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport os","a9d4cf70":"path = \"..\/input\/m5-forecasting-accuracy\"\ncalendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\nselling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\nsample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))\nsales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))","490a2e38":"calendar.head(3)","1ea72724":"for i, var in enumerate([\"year\", \"weekday\", \"month\", \n                          \"snap_CA\", \"snap_TX\", \"snap_WI\"]):\n    plt.figure()\n    g = sns.countplot(calendar[var])\n    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n    g.set_title(var)","d0c3149a":"from sklearn.preprocessing import OrdinalEncoder\n\ndef prep_calendar(df):\n    df = df.copy()\n    temp = [\"wday\", \"month\", \"year\", \"event_name_1\", \"event_type_1\"]\n    df = df[[\"wm_yr_wk\", \"d\"] + temp]\n    df.fillna(\"missing\", inplace=True)\n    df[temp] = OrdinalEncoder().fit_transform(df[temp])\n    for v in temp:\n        df[temp] = df[temp].astype(\"uint8\")\n    df.wm_yr_wk = df.wm_yr_wk.astype(\"uint16\")\n    return df\n\ncalendar = prep_calendar(calendar)","ab90a899":"calendar.head(3)","b6f485db":"sales.head(3)","bf29a9d3":"for i, var in enumerate([\"state_id\", \"store_id\", \"cat_id\", \"dept_id\"]):\n    plt.figure()\n    g = sns.countplot(sales[var])\n    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n    g.set_title(var)","7f41373b":"sales.item_id.value_counts()","cbeccdae":"sales.drop([\"d_\" + str(i+1) for i in range(800)], axis=1, inplace=True)","52037c78":"sales[sales.item_id==\"HOBBIES_1_001\"]","329695d7":"def melt_sales(df):\n    df = df.drop([\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"], axis=1).melt(\n        id_vars=['id'], var_name='d', value_name='demand')\n    return df\n\nsales = melt_sales(sales)","edd25640":"sales.head()","b5026a02":"sample_submission.head()","318d1cf0":"# Turn strings like \"F1\" to \"d_1914\"\ndef map_f2d(d_col, id_col):\n    eval_flag = id_col.str.endswith(\"evaluation\")\n    return \"d_\" + (d_col.str[1:].astype(\"int\") + 1913 + 28 * eval_flag).astype(\"str\")\n\n# Reverse\ndef map_d2f(d_col, id_col):\n    eval_flag = id_col.str.endswith(\"evaluation\")\n    return \"F\" + (d_col.str[2:].astype(\"int\") - 1913 - 28 * eval_flag).astype(\"str\")\n\n# Example\nmap_f2d(pd.Series([\"F1\", \"F2\", \"F28\", \"F1\", \"F2\", \"F28\"]), \n        pd.Series([\"validation\", \"validation\", \"validation\", \"evaluation\", \"evaluation\", \"evaluation\"]))","d33cbf0b":"submission = sample_submission.melt(id_vars=\"id\", var_name=\"d\", value_name=\"demand\").assign(\n    demand=np.nan,\n    d = lambda df: map_f2d(df.d, df.id))\nsubmission.head()","09272e8c":"sales = pd.concat([sales, submission])\nsales.tail()","61c04d25":"sales.id = sales.id.str.replace(\"evaluation\", \"validation\")","11c69e0f":"from sklearn.preprocessing import StandardScaler\n\ndef add_lagged_features(df):\n    df['lag_t56'] = df.groupby('id')['demand'].transform(lambda x: x.shift(56))\n    df['rolling_mean_t30'] = df.groupby('id')['demand'].transform(lambda x: x.shift(56).rolling(30, min_periods=1).mean())\n  \n    temp = [\"lag_t56\", \"rolling_mean_t30\"]\n    df.dropna(subset=temp, inplace=True)    \n    df[temp] = StandardScaler().fit_transform(df[temp])\n    for v in temp:\n        df[v] = df[v].astype(\"float32\")\n    return df\n\nsales = add_lagged_features(sales)","a8406938":"sales.head()","571496dc":"def expand_id(id):\n    return id.str.split(\"_\", expand=True).assign(\n        dept_id=lambda df: df.iloc[:,0] + \"_\" + df.iloc[:,1], \n        item_id=lambda df: df.iloc[:,0] + \"_\" + df.iloc[:,1] + \"_\" + df.iloc[:, 2],\n        store_id=lambda df: df.iloc[:,3] + \"_\" + df.iloc[:,4]).drop(np.arange(6), axis=1)\n\n# Example\nexpand_id(sales[\"id\"].head())","9c6eb56f":"uid = pd.Series(sales[\"id\"].unique())\nid_lookup = expand_id(uid)\nid_lookup[\"id\"] = uid\n\nencode_item_id = OrdinalEncoder()\nencode_dept_id = OrdinalEncoder()\nencode_store_id = OrdinalEncoder()\nid_lookup[\"item_id\"] = encode_item_id.fit_transform(id_lookup[[\"item_id\"]]).astype(\"uint16\")\nid_lookup[\"dept_id\"] = encode_dept_id.fit_transform(id_lookup[[\"dept_id\"]]).astype(\"uint8\")\nid_lookup[\"store_id\"] = encode_store_id.fit_transform(id_lookup[[\"store_id\"]]).astype(\"uint8\")\n\nid_lookup.head()","ee1cb76f":"sales = sales.merge(id_lookup, on=\"id\", how=\"left\")\ndel sales[\"id\"]","bd3eaf1e":"sales.head()","67aeed34":"selling_prices.head()","cc54a3f3":"# Add relative change\ndef prep_selling_prices(df):\n    df = df.copy()\n    df[\"store_id\"] = encode_store_id.transform(df[[\"store_id\"]]).astype(\"uint8\")\n    df[\"item_id\"] = encode_item_id.transform(df[[\"item_id\"]]).astype(\"uint16\")\n    df[\"wm_yr_wk\"] = df[\"wm_yr_wk\"].astype(\"uint16\")\n    \n    df[\"sell_price_rel_diff\"] = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"].pct_change()\n    sell_price_cummin = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"].cummin()\n    sell_price_cummax = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"].cummax()\n    df[\"sell_price_cumrel\"] = (df[\"sell_price\"] - sell_price_cummin) \/ (sell_price_cummax - sell_price_cummin)\n    df.fillna({\"sell_price_rel_diff\": 0, \"sell_price_cumrel\": 1}, inplace=True)\n    floats = [\"sell_price_cumrel\", \"sell_price_rel_diff\", \"sell_price\"]\n    sc = StandardScaler()\n    df[floats] = sc.fit_transform(df[floats])\n    for v in floats:\n        df[v] = df[v].astype(\"float32\")\n    return df\n\nselling_prices = prep_selling_prices(selling_prices)","e1313b2e":"selling_prices.head()","5e41d854":"gc.collect()\nsales = sales.merge(calendar, how=\"left\", on=\"d\")\ndel sales[\"d\"]","db5616c3":"gc.collect()\nsales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\ndel sales[\"wm_yr_wk\"]","c7e9ef06":"sales.fillna({\"sell_price\": 0, \"sell_price_rel_diff\": 0, \"sell_price_cumrel\": 0}, inplace=True)","7d46b36b":"sales.head()","b7c907ba":"sales.tail()","3519f39e":"gc.collect()","612c1d95":"training_flag = pd.notna(sales.demand)","a4f15ae5":"def make_Xy(df, ind=None, return_y = True):\n    if ind is not None:\n        df = df[ind]\n    X = {\"dense1\": df[[\"lag_t56\", \"rolling_mean_t30\", \"sell_price\", \"sell_price_rel_diff\", \n                       \"sell_price_cumrel\"]].to_numpy(dtype=\"float32\"),\n         \"item_id\": df[[\"item_id\"]].to_numpy(dtype=\"uint16\")}\n    for i, v in enumerate([\"wday\", \"month\", \"year\", \"event_name_1\", \"event_type_1\", \"dept_id\", \"store_id\"]):\n        X[v] = df[[v]].to_numpy(dtype=\"uint8\")\n    if return_y:\n        return X, df.demand.to_numpy(dtype=\"float32\")\n    else:\n        return X","44c38fe8":"# sales.to_csv(\"sales.csv\", index=False)","be6b0ff8":"X_train, y_train = make_Xy(sales, training_flag) # make_Xy(sales[0:1000000])\ny_train.shape","47166bb3":"X_test = make_Xy(sales, ~training_flag, return_y=False)","f92279de":"del sales\ngc.collect()","042b3e47":"import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ntf.keras.backend.clear_session()  # For easy reset of notebook state.\n\nfrom tensorflow.keras.layers import Dense, Input, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import concatenate, Flatten\nfrom tensorflow.keras import regularizers","8a6654eb":"# Dense part\ndense_input = Input(shape=(5, ), name='dense1')\ndense_branch = Dense(30, activation=\"relu\")(dense_input)\ndense_branch = Dense(30, activation=\"relu\")(dense_branch)\n\n# Embedded input\nwday_input = Input(shape=(1,), name='wday')\nmonth_input = Input(shape=(1,), name='month')\nyear_input = Input(shape=(1,), name='year')\nevent_name_1_input = Input(shape=(1,), name='event_name_1')\nevent_type_1_input = Input(shape=(1,), name='event_type_1')\nitem_id_input = Input(shape=(1,), name='item_id')\ndept_id_input = Input(shape=(1,), name='dept_id')\nstore_id_input = Input(shape=(1,), name='store_id')\n\n# Embedding layers\nwday_emb = Flatten()(Embedding(7, 3, )(wday_input))\nmonth_emb = Flatten()(Embedding(12, 3)(month_input))\nyear_emb = Flatten()(Embedding(6, 3)(year_input))\nevent_name_1_emb = Flatten()(Embedding(31, 5)(event_name_1_input))\nevent_type_1_emb = Flatten()(Embedding(5, 2)(event_type_1_input))\nitem_id_emb = Flatten()(Embedding(len(encode_item_id.categories_[0]), 50)(item_id_input))\nitem_id_emb = Dense(20, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(item_id_emb)\ndept_id_emb = Flatten()(Embedding(7, 3)(dept_id_input))\nstore_id_emb = Flatten()(Embedding(10, 4)(store_id_input))\n\nx = concatenate([dense_branch, wday_emb, month_emb, year_emb, event_name_1_emb,\n                event_type_1_emb, item_id_emb, dept_id_emb, store_id_emb])\nx = Dense(100, activation=\"relu\")(x)\nx = Dense(20, activation=\"relu\")(x)\nprediction = Dense(1, activation=\"linear\", name='output')(x)\n\nmodel = Model(inputs={\"dense1\": dense_input, \"wday\": wday_input, \"month\": month_input,\n                      \"year\": year_input, \"event_name_1\": event_name_1_input, \"event_type_1\": event_type_1_input,\n                      \"item_id\": item_id_input, \"dept_id\": dept_id_input, \"store_id\": store_id_input},\n              outputs=prediction)","b316abec":"model.summary()\n\nkeras.utils.plot_model(model, 'model.png', show_shapes=True)","99434614":"model.compile(loss=keras.losses.mean_squared_error,\n              optimizer=keras.optimizers.RMSprop(),\n              metrics=['mse'])\n\nhistory = model.fit(X_train, \n                    y_train,\n                    batch_size=4096,\n                    epochs=10,\n                    validation_split=0.1)","ba9697b7":"# Plot the evaluation metrics over epochs","12aa5744":"model.save('modelM5.h5')","82696a62":"pred = model.predict(X_test, batch_size=4096)","2df25c24":"pred.shape","d03e0a35":"submission.shape","ed79bb84":"submission.tail()","f61c91c1":"submission = submission.assign(\n    demand = np.clip(pred, 0, None),\n    d = lambda df: map_d2f(df.d, df.id))\nsubmission.head()","e4172213":"# Right column order\ncol_order = [\"id\"] + [\"F\" + str(i + 1) for i in range(28)]\nsubmission = submission.pivot(index=\"id\", columns=\"d\", values=\"demand\").reset_index()[col_order]\n\n# Right row order\nsubmission = sample_submission[[\"id\"]].merge(submission, how=\"left\", on=\"id\")","63121bd7":"submission.head()","81b28f52":"submission.to_csv(\"submission.csv\", index=False)","00367e83":"### Selling prices\n\nContains selling prices for each store_id, item_id_wm_yr_wk combination.","409bbf7c":"## Modelling\n\nWe will now use Tensorflow & Keras to model sales demand as a function of the prepared input. Key pieces are the categorical predictors prepared above. They will be fed through embedding layers and combined to dense numeric features.\n\nFor simplicity, we use MSE as evaluation criterion. This will most certainly change in future commits.","b7d88395":"#### Notes for modeling\n\n\n- wday -> integer coding & embedding\n\n- year(?) -> integer coding & embedding\n\n- month(?) -> integer coding & embedding\n\n- \"event_name_1\", \"event_type_1\"\": simple imputer & integer coding & embedding\n","f755381f":"#### Notes for modeling\n\n**Features**:\n\n- sell_price: numeric\n\n- relative change to last date (per store and item): numeric\n\n- price position between cummin and cummax (per store and item): numeric\n\n**Reshape**: No\n\n**Merge key(s)**: to sales data by store_id, item_id, wm_yr_wk (through calendar data)","ec0ca592":"Derive some time related features:","6999a339":"#### Add relevant id information\n\nAfter combination of training and test data, we can join further info.","a06d66e1":"#### Change \"evaluation\" to \"validation\"...","bcaa6db0":"#### Drop dates to save space. Offline, this is not required.","139285d5":"## Load data","b402c6e3":"## Submission","4d1c72bb":"#### Reshaping\n\nWe now reshape the data from wide to long, using \"id\" as fixed and swapping \"d_1\", to \"d_1913\". ","33235607":"### Combine all","32c2d8ac":"### Create input dicts for multi-input","fddd6ead":"### The model","3c0c59fb":"#### Add reshaped submission file\n\nSo that it sneaks through data preprocessing easily."}}