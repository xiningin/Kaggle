{"cell_type":{"80ad6329":"code","f42ef444":"code","df419995":"code","aa76a4ef":"code","026dfc89":"code","cafbe877":"code","ea2938d4":"code","f72270c7":"code","f0e6f50d":"code","0dc6bad8":"code","2c300bb6":"code","21b76659":"code","2bcd5ef7":"code","315a6538":"code","83bdda52":"code","967c3dc6":"code","26b22b78":"code","e64b1254":"code","fbb09a01":"code","0a91b98e":"code","02901ac3":"code","29581dbc":"code","a393c71b":"code","21cb9820":"code","1883c220":"code","8a61a41e":"code","6df27361":"markdown","7000ed74":"markdown","27338b45":"markdown","8c97f1f5":"markdown","38aa0537":"markdown","d3409de2":"markdown","36a93ad2":"markdown","f74a3270":"markdown","caf77afb":"markdown","39a0a00a":"markdown","c3b7531f":"markdown","2d8940fa":"markdown","b9d40dc8":"markdown"},"source":{"80ad6329":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f42ef444":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ntest['Survived']= submission['Survived']\n\n# Combine Train and Test Datasets for Data Cleaning\ndf = pd.concat([test.assign(ind=\"test\"), train.assign(ind=\"train\")])","df419995":"df.head(2)","aa76a4ef":"[train.shape , test.shape , submission.shape]","026dfc89":"df.info()","cafbe877":"sns.barplot(data=df, x='Sex', y= 'Survived')","ea2938d4":"sns.countplot(data=df, x='Survived')","f72270c7":"sns.scatterplot(data=df, x='Age', y='Fare', hue='Survived')","f0e6f50d":"sns.pairplot(data = train,hue='Survived')","0dc6bad8":"# Deal with \"Cabin\" Column\ndf = df.drop(['Cabin'] , axis=1)\n\n# Some columns have no effect on survival so we remove them:\ndf = df.drop(['PassengerId'] , axis=1)\ndf = df.drop(['Ticket'] , axis=1)\ndf = df.drop(['Name'] , axis=1)\n\n# Deal with \"Age\" Column\ndf = df.dropna(axis=0, subset=['Age'])\n\n# We have some null in Embarked and Fare columns:\ndf = df.dropna(axis=0, subset=['Embarked'])\ndf = df.dropna(axis=0, subset=['Fare'])\n\n# Dealing with Categorical Data\ndf['Pclass'] = df['Pclass'].apply(str)\n\n# Convert All Object type to One hot encoding\n\n# START ONE HOT ENCODING\ndf_num = df.select_dtypes(exclude='object')\ndf_obj = df.select_dtypes(include='object')\nnon_dummy_cols = ['ind']\ndummy_cols = list(set(df_obj.columns) - set(non_dummy_cols))\ndf_obj = pd.get_dummies(df_obj, columns=dummy_cols, drop_first=True)\ndf = pd.concat([df_num, df_obj], axis = 1)\n# END ONE HOT ENCODING\n\n# Split Test data from df\ntest, train = df[df[\"ind\"].eq(\"test\")], df[df[\"ind\"].eq(\"train\")]\n\n# We should Drop indicator Column from test and train dataframes:\ntest= test.drop(['ind'], axis=1)\ntrain= train.drop(['ind'], axis=1)","2c300bb6":"# Split the Data to Train & Test\nX_test, y_test = test.drop(columns='Survived').copy(), test['Survived'].copy()\nX_train, y_train = train.drop(columns='Survived').copy(), train['Survived'].copy()","21b76659":"from sklearn.svm import SVC","2bcd5ef7":"model= SVC()","315a6538":"model.fit(X_train, y_train)","83bdda52":"y_pred= model.predict(X_test)","967c3dc6":"from sklearn.metrics import classification_report, confusion_matrix","26b22b78":"confusion_matrix(y_test, y_pred)","e64b1254":"print(classification_report(y_test, y_pred))","fbb09a01":"from sklearn.model_selection import GridSearchCV","0a91b98e":"svm = SVC()\nparam_grid = {'C':[0.01,0.1,1, 10, 100, 1000],'gamma':[1, 0.1, 0.01, 0.001, 0.0001]}\ngrid = GridSearchCV(svm,param_grid, cv=5)","02901ac3":"grid.fit(X_train, y_train)","29581dbc":"grid.best_estimator_","a393c71b":"grid.best_params_","21cb9820":"y_pred_grid= grid.predict(X_test)","1883c220":"confusion_matrix(y_test, y_pred_grid)","8a61a41e":"print(classification_report(y_test, y_pred_grid))","6df27361":"**Accuracy increased well!**","7000ed74":"# Evaluating the Model","27338b45":"# Predicting Test data","8c97f1f5":"# Hyperparameters","38aa0537":"# Data Cleaning\nI did this part with details on [this notebook](https:\/\/www.kaggle.com\/sajjadnajafi\/logistic-regression-titanic\/).","d3409de2":"# Import all Necessary Libraries","36a93ad2":"# Import Dataset","f74a3270":"# Data Overview","caf77afb":"# Determine the Features & Target Variable","39a0a00a":"# Train the Model","c3b7531f":"# Titanic SVM\n\n# Support Vector Machines (SVM)\n\nSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n\nThe advantages of support vector machines are:\n\n* Effective in high dimensional spaces.\n\n* Still effective in cases where number of dimensions is greater than the number of samples.\n\n* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n\n* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n\nThe disadvantages of support vector machines include:\n\n* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n\n* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.\n\n(Read Full article on [scikit-learn](https:\/\/scikit-learn.org\/stable\/modules\/svm.html))","2d8940fa":"# EDA","b9d40dc8":"Accuracy of training isn't good so i'm going to set Hyperparameters:"}}