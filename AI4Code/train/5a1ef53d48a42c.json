{"cell_type":{"4fe89093":"code","6e134077":"code","61555bb7":"code","39cadf6d":"code","eee83323":"code","60f5b767":"code","0a761cba":"code","6e17b3e5":"code","cb37a8d2":"code","0a196429":"code","25aea863":"code","e873e0cc":"code","b16d475a":"code","fbaf13f1":"code","36737e50":"code","e0111f9e":"code","f7c2d71f":"code","cca4c348":"code","3cbece3b":"code","df17b80e":"code","914ddcaa":"code","49aa9665":"code","69af0c62":"code","7cc6756f":"code","daf7aca8":"code","7a81b527":"code","807e783b":"code","28bcee29":"code","51d821ee":"code","b90fb494":"code","7c4fa015":"code","fea55d88":"code","71b9df84":"code","c118a356":"markdown","855da9ed":"markdown","b37c954e":"markdown","02122fed":"markdown","431073d6":"markdown","24fab321":"markdown","b52732b1":"markdown","5021bb83":"markdown","47cf3c0d":"markdown","22e51e6f":"markdown","807bc992":"markdown","79d217a6":"markdown","c39b5a73":"markdown","b4b2d02d":"markdown","4cebd9c2":"markdown","286fdce6":"markdown","1031c504":"markdown","82d11927":"markdown","4877d966":"markdown","47ef3f8e":"markdown","8a2349c5":"markdown","2dcd279c":"markdown","2bd80968":"markdown","1e6a4b47":"markdown"},"source":{"4fe89093":"#basic\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport gc\n\n#Model imports\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\n\n#optuna\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom sklearn.metrics import roc_auc_score\n\n# You can only call make_env() once, so don't lose it!\nimport riiideducation\n\nimport os","6e134077":"%%time\n\nused_data_types_dict = {\n    'question_id': 'int16',\n    'bundle_id': 'int16',\n    'correct_answer': 'int8',\n    'part': 'int8',\n    'tags': 'str',\n}\n\nquestions = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/questions.csv',\n                       usecols = used_data_types_dict.keys(), dtype=used_data_types_dict)\n\nlectures_df = pd.read_csv('..\/input\/riiid-test-answer-prediction\/lectures.csv')\n#ex = pd.read_csv('..\/input\/riiid-test-answer-prediction\/example_test.csv')","61555bb7":"%%time\nfeatures_df = pd.read_pickle('..\/input\/riiid-splitting-train-and-test-data\/features_q_only.pkl.zip')\ntrain_df = pd.read_pickle('..\/input\/riiid-splitting-train-and-test-data\/train_q_only.pkl.zip')","39cadf6d":"def add_seen_before_to_train_df(features_df, train_df):\n    train_questions_only_df = features_df[features_df['answered_correctly']!=-1]\n\n    # fill dictionary with default values\n    state = dict()\n    for user_id in train_questions_only_df['user_id'].unique():\n        state[user_id] = {}\n    total = len(state.keys())\n\n    # add user content attempts\n    user_content = train_questions_only_df.groupby('user_id')['content_id'].apply(np.array).apply(np.sort).apply(np.unique)\n    user_attempts = train_questions_only_df.groupby(['user_id', 'content_id'])['content_id'].count().astype(np.uint8).groupby('user_id').apply(np.array).values\n\n    for user_id, content, attempt in tqdm(zip(state.keys(), user_content, user_attempts),total=total):\n        state[user_id] = dict(zip(content, attempt))\n\n    del user_content, user_attempts, train_questions_only_df\n\n    big_list=[]\n\n    for pair in tqdm(train_df[['user_id','content_id']].values):\n        if pair[0] in state:\n            if pair[1] in state[pair[0]]:\n                big_list.append(state[pair[0]][pair[1]])\n                state[pair[0]][pair[1]]+=1\n            else:\n                big_list.append(0)\n                state[pair[0]][pair[1]]=1\n        else:\n            big_list.append(0)\n            state[pair[0]]={pair[1]:1}\n\n    del state\n\n    train_df['seen_before']=big_list\n    train_df.seen_before = train_df.seen_before.clip(upper=5)\n\n    del big_list\n    gc.collect()\n    \n    return(train_df)\n\ntrain_df = add_seen_before_to_train_df(features_df, train_df)","eee83323":"#messing with lectures before we remove them from df\nlects_df = features_df[features_df['answered_correctly']==-1]\n\nlect_seen_df = pd.DataFrame(data=lects_df.user_id.value_counts())\nlect_seen_df.columns=['lectures_seen']\nlect_seen_df.lectures_seen = lect_seen_df.lectures_seen.astype(float)\n\n#some new lect feature stuff\nlectures_df['type_of'] = lectures_df['type_of'].replace('solving question', 'solving_question')\n\nlectures_df = pd.get_dummies(lectures_df, columns=['part', 'type_of'])\n\npart_lectures_columns = [column for column in lectures_df.columns if column.startswith('part')]\ntypes_of_lectures_columns = [column for column in lectures_df.columns if column.startswith('type_of_')]\n\nlectures_df = lectures_df.set_index('lecture_id')\n\n# merge lecture features to train dataset\ntrain_lectures = features_df[features_df.answered_correctly == -1].merge(lectures_df, left_on='content_id', right_on='lecture_id', how='left')\n\n# collect per user stats\nuser_lecture_stats_part = train_lectures.groupby('user_id')[part_lectures_columns + types_of_lectures_columns].sum()\n\nuser_lecture_stats_part = user_lecture_stats_part.merge(lect_seen_df, left_index=True, right_index=True)\nuser_lecture_stats_part.index.name = 'user_id'\nuser_lecture_stats_part['lectures_seen'] = user_lecture_stats_part['lectures_seen'].astype(int)\n\ndel lects_df, lect_seen_df, train_lectures\n\nuser_lecture_stats_part","60f5b767":"#removes rows that are lectures and adds tags and part to each interaction\ntrain_questions_only_df = features_df[features_df['answered_correctly']!=-1]\n\ntrain_questions_only_df = pd.merge(train_questions_only_df, questions[['part','tags']], \n                                   left_on='content_id', right_index=True, how = 'left')\ndel features_df","0a761cba":"#getting the mean accuracy, question count of each user and other math stuff\ngrouped_by_user_df = train_questions_only_df.groupby('user_id')\n\nuser_answers_df = grouped_by_user_df.agg({'answered_correctly': ['mean', 'count', 'sum']}).copy()\nuser_answers_df.columns = [\n    'user_mean_accuracy', \n    'user_questions_answered',\n    'user_questions_correct',\n]\nuser_answers_df.user_questions_correct = user_answers_df.user_questions_correct.astype('int32')\n\nuser_answers_df","6e17b3e5":"user_lagtime_max_dict = grouped_by_user_df.agg({'lag_time': ['max']}).copy()\nuser_lagtime_max_dict.columns = [\n    'user_lag_time_max',\n]\n\nuser_lagtime_max_dict = user_lagtime_max_dict.to_dict('index')\n\n#adds in the last lagtime as sometimes mutliple q's have the same timestamp\nfor pair in grouped_by_user_df.tail(1)[['user_id','lag_time']].values:\n    user_lagtime_max_dict[pair[0]]['last_lagtime'] = pair[1]\n\ngc.collect()","cb37a8d2":"#grouping by content_id\ngrouped_by_tags_df = train_questions_only_df.groupby('tags')\n\ntags_answers_df = grouped_by_tags_df.agg({'answered_correctly': ['mean', 'count', 'std', 'skew']}).copy()\n\ntags_answers_df.columns = [\n    'tags_mean_accuracy', \n    'tags_question_asked', \n    'tags_std_accuracy', \n    'tags_skew_accuracy'\n]\n\ntags_answers_df","0a196429":"grouped_by_part_df = train_questions_only_df.groupby('part')\n\npart_answers_df = grouped_by_part_df.agg({'answered_correctly': ['mean', 'count']}).copy()\npart_answers_df.columns = [\n    'part_mean_accuracy', \n    'part_questions_answered', \n]\n\npart_answers_df","25aea863":"del grouped_by_user_df\ndel grouped_by_tags_df\ndel grouped_by_part_df","e873e0cc":"content_answers_df = pd.read_pickle('..\/input\/riiid-content-answers-df-preprocessing\/content_answers_df.pkl.zip')\n\ncontent_answers_df","b16d475a":"features = [\n    'user_mean_accuracy', \n    'user_questions_answered',\n    'user_questions_correct',\n    'q_mean_accuracy', \n    'q_question_asked',\n    'q_question_correct',\n    'community',\n    'num_in_bundle',\n    'tags_mean_accuracy', \n    'tags_question_asked', \n    'tags_std_accuracy', \n    'tags_skew_accuracy',\n    'part_mean_accuracy', \n    'part_questions_answered', \n    'prior_question_elapsed_time', \n    #'prior_question_had_explanation',\n    'part_1',\n    'part_2',\n    'part_3',\n    'part_4',\n    'part_5',\n    'part_6',\n    'part_7',\n    'type_of_concept',\n    'type_of_intention',\n    'type_of_solving_question',\n    #'type_of_starter',\n    'lectures_seen',\n    'seen_before',\n    'avg_q_time',\n    'lag_time',\n]\n\ntarget = 'answered_correctly'","fbaf13f1":"del train_questions_only_df","36737e50":"def add_and_update_user_lects(user_lecture_stats_part, lectures_df, train_df):\n    \n    lect_dict = lectures_df.to_dict('index')\n    lect_stats_part_dict = user_lecture_stats_part.to_dict('index')\n    \n    part_1_list = []\n    part_2_list = []\n    part_3_list = []\n    part_4_list = []\n    part_5_list = []\n    part_6_list = []\n    part_7_list = []\n    type_of_concept_list = []\n    type_of_intention_list = []\n    type_of_solving_question_list = []\n    type_of_starter_list = []\n    lectures_seen_list = []\n    \n    for pair in tqdm(train_df[['content_id','user_id','answered_correctly']].values):\n        if pair[1] in lect_stats_part_dict:\n            if pair[2]!=-1:\n                part_1_list.append(lect_stats_part_dict[pair[1]]['part_1'])\n                part_2_list.append(lect_stats_part_dict[pair[1]]['part_2'])\n                part_3_list.append(lect_stats_part_dict[pair[1]]['part_3'])\n                part_4_list.append(lect_stats_part_dict[pair[1]]['part_4'])\n                part_5_list.append(lect_stats_part_dict[pair[1]]['part_5'])\n                part_6_list.append(lect_stats_part_dict[pair[1]]['part_6'])\n                part_7_list.append(lect_stats_part_dict[pair[1]]['part_7'])\n                type_of_concept_list.append(lect_stats_part_dict[pair[1]]['type_of_concept'])\n                type_of_intention_list.append(lect_stats_part_dict[pair[1]]['type_of_intention'])\n                type_of_solving_question_list.append(lect_stats_part_dict[pair[1]]['type_of_solving_question'])\n                type_of_starter_list.append(lect_stats_part_dict[pair[1]]['type_of_starter'])\n                lectures_seen_list.append(lect_stats_part_dict[pair[1]]['lectures_seen'])\n                \n            else:\n                part_1_list.append(lect_stats_part_dict[pair[1]]['part_1'])\n                part_2_list.append(lect_stats_part_dict[pair[1]]['part_2'])\n                part_3_list.append(lect_stats_part_dict[pair[1]]['part_3'])\n                part_4_list.append(lect_stats_part_dict[pair[1]]['part_4'])\n                part_5_list.append(lect_stats_part_dict[pair[1]]['part_5'])\n                part_6_list.append(lect_stats_part_dict[pair[1]]['part_6'])\n                part_7_list.append(lect_stats_part_dict[pair[1]]['part_7'])\n                type_of_concept_list.append(lect_stats_part_dict[pair[1]]['type_of_concept'])\n                type_of_intention_list.append(lect_stats_part_dict[pair[1]]['type_of_intention'])\n                type_of_solving_question_list.append(lect_stats_part_dict[pair[1]]['type_of_solving_question'])\n                type_of_starter_list.append(lect_stats_part_dict[pair[1]]['type_of_starter'])\n                lectures_seen_list.append(lect_stats_part_dict[pair[1]]['lectures_seen'])\n                \n                lect_stats_part_dict[pair[1]]['part_1'] += lect_dict[pair[0]]['part_1']\n                lect_stats_part_dict[pair[1]]['part_2'] += lect_dict[pair[0]]['part_2']\n                lect_stats_part_dict[pair[1]]['part_3'] += lect_dict[pair[0]]['part_3']\n                lect_stats_part_dict[pair[1]]['part_4'] += lect_dict[pair[0]]['part_4']\n                lect_stats_part_dict[pair[1]]['part_5'] += lect_dict[pair[0]]['part_5']\n                lect_stats_part_dict[pair[1]]['part_6'] += lect_dict[pair[0]]['part_6']\n                lect_stats_part_dict[pair[1]]['part_7'] += lect_dict[pair[0]]['part_7']\n                lect_stats_part_dict[pair[1]]['type_of_concept'] += lect_dict[pair[0]]['type_of_concept']\n                lect_stats_part_dict[pair[1]]['type_of_intention'] += lect_dict[pair[0]]['type_of_intention']\n                lect_stats_part_dict[pair[1]]['type_of_solving_question'] += lect_dict[pair[0]]['type_of_solving_question']\n                lect_stats_part_dict[pair[1]]['type_of_starter'] += lect_dict[pair[0]]['type_of_starter']\n                lect_stats_part_dict[pair[1]]['lectures_seen'] += 1\n        else:\n            part_1_list.append(0)\n            part_2_list.append(0)\n            part_3_list.append(0)\n            part_4_list.append(0)\n            part_5_list.append(0)\n            part_6_list.append(0)\n            part_7_list.append(0)\n            type_of_concept_list.append(0)\n            type_of_intention_list.append(0)\n            type_of_solving_question_list.append(0)\n            type_of_starter_list.append(0)\n            lectures_seen_list.append(0)\n            \n            \n            if pair[2]==-1:\n                lect_stats_part_dict[pair[1]]={}\n                lect_stats_part_dict[pair[1]]['part_1'] = lect_dict[pair[0]]['part_1']\n                lect_stats_part_dict[pair[1]]['part_2'] = lect_dict[pair[0]]['part_2']\n                lect_stats_part_dict[pair[1]]['part_3'] = lect_dict[pair[0]]['part_3']\n                lect_stats_part_dict[pair[1]]['part_4'] = lect_dict[pair[0]]['part_4']\n                lect_stats_part_dict[pair[1]]['part_5'] = lect_dict[pair[0]]['part_5']\n                lect_stats_part_dict[pair[1]]['part_6'] = lect_dict[pair[0]]['part_6']\n                lect_stats_part_dict[pair[1]]['part_7'] = lect_dict[pair[0]]['part_7']\n                lect_stats_part_dict[pair[1]]['type_of_concept'] = lect_dict[pair[0]]['type_of_concept']\n                lect_stats_part_dict[pair[1]]['type_of_intention'] = lect_dict[pair[0]]['type_of_intention']\n                lect_stats_part_dict[pair[1]]['type_of_solving_question'] = lect_dict[pair[0]]['type_of_solving_question']\n                lect_stats_part_dict[pair[1]]['type_of_starter'] = lect_dict[pair[0]]['type_of_starter']\n                lect_stats_part_dict[pair[1]]['lectures_seen'] = 1\n                \n    train_df['part_1'] =  part_1_list\n    train_df['part_2'] = part_2_list\n    train_df['part_3'] = part_3_list\n    train_df['part_4'] = part_4_list\n    train_df['part_5'] = part_5_list\n    train_df['part_6'] = part_6_list\n    train_df['part_7'] = part_7_list\n    train_df['type_of_concept'] = type_of_concept_list\n    train_df['type_of_intention'] = type_of_intention_list\n    train_df['type_of_solving_question'] = type_of_solving_question_list\n    train_df['type_of_starter'] = type_of_starter_list\n    train_df['lectures_seen'] = lectures_seen_list\n    \n    lect_stats_part = pd.DataFrame.from_dict(lect_stats_part_dict, orient='index')\n    \n    return(lect_stats_part, train_df)","e0111f9e":"def add_and_update_user_stats(user_answers_df, train_df):\n    \n    my_dict=user_answers_df.to_dict('index')\n    user_acc_list=[]\n    user_answered_list=[]\n    user_correct_list=[]\n    \n    for pair in tqdm(train_df[['user_id','answered_correctly']].values):\n        if pair[0] in my_dict:\n            user_acc_list.append(my_dict[pair[0]]['user_mean_accuracy'])\n            user_answered_list.append(my_dict[pair[0]]['user_questions_answered'])\n            user_correct_list.append(my_dict[pair[0]]['user_questions_correct'])\n            my_dict[pair[0]]['user_questions_answered']+=1\n            my_dict[pair[0]]['user_questions_correct']+=pair[1]\n            my_dict[pair[0]]['user_mean_accuracy'] = my_dict[pair[0]]['user_questions_correct']\/my_dict[pair[0]]['user_questions_answered']\n            \n        else:\n            my_dict[pair[0]]={'user_mean_accuracy':0.645,'user_questions_answered': 1, 'user_questions_correct': pair[1]}\n            user_acc_list.append(0.645)\n            user_answered_list.append(0)\n            user_correct_list.append(0)\n    \n    train_df['user_mean_accuracy']=user_acc_list\n    train_df['user_questions_answered']=user_answered_list\n    train_df['user_questions_correct']=user_correct_list\n    \n    user_answers_df = pd.DataFrame.from_dict(my_dict, orient='index')\n            \n    return(user_answers_df, train_df)","f7c2d71f":"def add_and_update_content_stats(content_answers_df, train_df):\n    \n    my_dict=content_answers_df.to_dict('index')\n    \n    q_mean_accuracy_list=[]\n    q_question_asked=[]\n    q_question_correct=[]\n    community_list=[]\n    num_in_bundle_list=[]\n    avg_q_time_list=[]\n    \n    for pair in tqdm(train_df[['content_id','answered_correctly']].values):\n        q_mean_accuracy_list.append(my_dict[pair[0]]['q_mean_accuracy'])\n        q_question_asked.append(my_dict[pair[0]]['q_question_asked'])\n        q_question_correct.append(my_dict[pair[0]]['q_question_correct'])\n        community_list.append(my_dict[pair[0]]['community'])\n        num_in_bundle_list.append(my_dict[pair[0]]['num_in_bundle'])\n        avg_q_time_list.append(my_dict[pair[0]]['avg_q_time'])\n        \n        my_dict[pair[0]]['q_question_asked']+=1\n        my_dict[pair[0]]['q_question_correct']+=pair[1]\n        my_dict[pair[0]]['q_mean_accuracy'] = my_dict[pair[0]]['q_question_correct']\/my_dict[pair[0]]['q_question_asked']\n        \n    train_df['q_mean_accuracy'] = q_mean_accuracy_list\n    train_df['q_question_asked'] = q_question_asked\n    train_df['q_question_correct'] = q_question_correct\n    train_df['community'] = community_list\n    train_df['num_in_bundle'] = num_in_bundle_list\n    train_df['avg_q_time'] = avg_q_time_list\n    \n    content_answers_df = pd.DataFrame.from_dict(my_dict, orient='index')\n    \n    return(content_answers_df, train_df)","cca4c348":"def update_user_max_timestamp(user_lagtime_max_dict, train_df):\n    \n    for pair in train_df[['user_id','timestamp','lag_time']].values:\n        if pair[0] in user_lagtime_max_dict:\n            user_lagtime_max_dict[pair[0]]['user_lag_time_max'] = pair[1]\n            user_lagtime_max_dict[pair[0]]['last_lagtime'] = pair[2]\n        else:\n            user_lagtime_max_dict[pair[0]] = {}\n            user_lagtime_max_dict[pair[0]]['user_lag_time_max'] = pair[1]\n            user_lagtime_max_dict[pair[0]]['last_lagtime'] = pair[2]\n   \n    return(user_lagtime_max_dict)","3cbece3b":"%%time\nlect_stats_part, train_df = add_and_update_user_lects(user_lecture_stats_part, lectures_df, train_df)\n\ntrain_df = train_df[train_df[target] != -1]\n\nuser_lagtime_max_dict = update_user_max_timestamp(user_lagtime_max_dict, train_df)\n\nuser_answers_df, train_df = add_and_update_user_stats(user_answers_df, train_df)\ncontent_answers_df, train_df = add_and_update_content_stats(content_answers_df, train_df)\n\ntrain_df = pd.merge(train_df, questions[['part','tags']], left_on='content_id', right_index=True, how = 'left')\ntrain_df = train_df.merge(part_answers_df, how='left', left_on='part', right_index=True)\ntrain_df = train_df.merge(tags_answers_df, how='left', left_on='tags', right_index=True)\n\ntrain_df['part_1'].fillna(0, inplace = True)\ntrain_df['part_2'].fillna(0, inplace = True)\ntrain_df['part_3'].fillna(0, inplace = True)\ntrain_df['part_4'].fillna(0, inplace = True)\ntrain_df['part_5'].fillna(0, inplace = True)\ntrain_df['part_6'].fillna(0, inplace = True)\ntrain_df['part_7'].fillna(0, inplace = True)\ntrain_df['type_of_concept'].fillna(0, inplace = True)\ntrain_df['type_of_intention'].fillna(0, inplace = True)\ntrain_df['type_of_solving_question'].fillna(0, inplace = True)\ntrain_df['type_of_starter'].fillna(0, inplace = True)\ntrain_df['lectures_seen'].fillna(0, inplace = True)\ntrain_df['prior_question_elapsed_time'].fillna(25423, inplace = True)\ntrain_df['avg_q_time'].fillna(25423, inplace = True)\ntrain_df['lag_time'].fillna(26161758, inplace = True)\n\ntrain_df[types_of_lectures_columns + part_lectures_columns] = train_df[types_of_lectures_columns + part_lectures_columns].astype(int)\ntrain_df['lectures_seen'] = train_df['lectures_seen'].astype(int)\n\ntrain_df['prior_question_had_explanation'] = train_df['prior_question_had_explanation'].fillna(value=False).astype(bool)\ntrain_df = train_df.fillna(value=0.5)\n\ntrain_df = train_df[features + [target]]\ntrain_df = train_df.replace([np.inf, -np.inf], np.nan)\ntrain_df['prior_question_elapsed_time'].fillna(25423, inplace = True)\ntrain_df['avg_q_time'].fillna(25423, inplace = True)\ntrain_df['lag_time'].fillna(26161758, inplace = True)\n\ntrain_df = train_df.fillna(0)","df17b80e":"#train_df, test_df = train_test_split(train_df, random_state=314, test_size=0.2)","914ddcaa":"sampler = TPESampler(seed=314)\n\ndef create_model(trial):\n    num_leaves = trial.suggest_int(\"num_leaves\", 20, 40)\n    n_estimators = trial.suggest_int(\"n_estimators\", 50, 400)\n    max_depth = trial.suggest_int('max_depth', 3, 8)\n    min_child_samples = trial.suggest_int('min_child_samples', 100, 1200)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0001, 0.30)\n    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 5, 90)\n    bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.50, 1.0)\n    feature_fraction = trial.suggest_uniform('feature_fraction', 0.50, 1.0)\n    \n    model = LGBMClassifier(\n        num_leaves=num_leaves,\n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        min_child_samples=min_child_samples, \n        min_data_in_leaf=min_data_in_leaf,\n        learning_rate=learning_rate,\n        feature_fraction=feature_fraction,\n        random_state=314\n    )\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(train_df[features], train_df[target])\n    score = roc_auc_score(test_df[target].values, model.predict_proba(test_df[features])[:,1])\n    return score","49aa9665":"#study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n#study.optimize(objective, n_trials=50)\n#params = study.best_params\n#params['random_state'] = 314","69af0c62":"params = {'num_leaves': 30,\n          'n_estimators': 300,\n          'max_depth': 5,\n          'min_child_samples': 371,\n          'learning_rate': 0.28285171125399805,\n          'min_data_in_leaf': 23,\n          'bagging_fraction': 0.8057106694835638,\n          'feature_fraction': 0.5688885590495344,\n         }\n\nmodel = LGBMClassifier(**params)\nmodel.fit(train_df[features], train_df[target])\n#print('LGB score: ', roc_auc_score(test_df[target].values, model.predict_proba(test_df[features])[:,1]))","7cc6756f":"print(model.feature_importances_)\nprint(train_df.columns[:-1])\n\n#we can use train_df.columns[:-1] only because the target column is at the end of the dataframe\n\npd.DataFrame({'col_name': model.feature_importances_},\n                index=train_df.columns[:-1]).sort_values(by='col_name', ascending=False)","daf7aca8":"del train_df\n\nall_data = pd.read_pickle('..\/input\/riiid-train-df\/train_df.pkl.gzip')\nall_data = all_data[all_data[target] != -1]","7a81b527":"# fill dictionary with all the questions that users have seen before\ndef get_me_all_seen_befores(all_data):\n    state = dict()\n    for user_id in all_data['user_id'].unique():\n        state[user_id] = {}\n    total = len(state.keys())\n\n    # add user content attempts\n    user_content = all_data.groupby('user_id')['content_id'].apply(np.array).apply(np.sort).apply(np.unique)\n    user_attempts = all_data.groupby(['user_id', 'content_id'])['content_id'].count().astype(np.uint8).groupby('user_id').apply(np.array).values\n\n    for user_id, content, attempt in tqdm(zip(state.keys(), user_content, user_attempts),total=total):\n        state[user_id] = dict(zip(content, attempt))\n\n    del user_content, user_attempts, all_data\n\n    return(state)\n\nstate = get_me_all_seen_befores(all_data)","807e783b":"def update_content_stats(content_answers_df, previous_test_df):\n    for row in previous_test_df[['content_id','answered_correctly','content_type_id']].values:\n        if row[2] == 0:\n            content_answers_df.at[row[0],'q_question_correct'] += row[1]\n            content_answers_df.at[row[0],'q_question_asked'] += 1\n    content_answers_df['q_mean_accuracy']= content_answers_df['q_question_correct']\/content_answers_df['q_question_asked']","28bcee29":"def update_user_stats(user_answers_df, previous_test_df):\n    for row in previous_test_df[['user_id','answered_correctly','content_type_id']].values:\n        if row[2] == 0:\n            try:\n                user_answers_df.at[row[0],'user_questions_correct'] += row[1]\n                user_answers_df.at[row[0],'user_questions_answered'] += 1\n            except:\n                user_answers_df.at[row[0]]=(0,1,row[1])\n    user_answers_df['user_mean_accuracy']= user_answers_df['user_questions_correct']\/user_answers_df['user_questions_answered']","51d821ee":"def update_lect_stats(user_lecture_stats_part, lectures_df, previous_test_df):\n    for row in previous_test_df[['user_id','content_type_id', 'content_id']].values:\n        if row[1] == 1:\n            y = lectures_df.query('lecture_id == {}'.format(row[2])).drop('tag', 1).reset_index(drop=True)\n            y = y.loc[:, (y != 0).any(axis=0)]\n            if row[0] in user_lecture_stats_part.index:\n                for i in y.columns:\n                    user_lecture_stats_part.at[row[0], i] +=1\n                user_lecture_stats_part.at[row[0], 'lectures_seen'] +=1\n            else:\n                user_lecture_stats_part.loc[row[0]] = 0\n                for i in y.columns:\n                    user_lecture_stats_part.at[row[0], i] +=1\n                user_lecture_stats_part.at[row[0], 'lectures_seen'] +=1","b90fb494":"def add_and_update_seen_before(train_df, state):\n    big_list=[]\n    for pair in train_df[['user_id','content_id','content_type_id']].values:\n        if pair[2] == 0:\n            if pair[0] in state:\n                if pair[1] in state[pair[0]]:\n                    big_list.append(state[pair[0]][pair[1]])\n                    state[pair[0]][pair[1]]+=1\n                else:\n                    big_list.append(0)\n                    state[pair[0]][pair[1]]=1\n            else:\n                big_list.append(0)\n                state[pair[0]]={pair[1]:1}\n        else:\n            big_list.append(0)\n\n    train_df['seen_before']= big_list\n    train_df.seen_before = train_df.seen_before.clip(upper=5)\n    return(train_df, state)","7c4fa015":"def add_and_update_lag_time(test_df, user_lagtime_max_dict):\n    \n    lag_time_list = []\n    \n    for pair in test_df[['user_id','timestamp']].values:\n        if pair[0] in user_lagtime_max_dict:\n            if pair[1] != user_lagtime_max_dict[pair[0]]['user_lag_time_max']:\n                lag_time_list.append(pair[1] - (user_lagtime_max_dict[pair[0]]['user_lag_time_max']))\n                user_lagtime_max_dict[pair[0]]['last_lagtime'] = pair[1] - (user_lagtime_max_dict[pair[0]]['user_lag_time_max'])\n                user_lagtime_max_dict[pair[0]]['user_lag_time_max'] = pair[1]\n            else:\n                lag_time_list.append(user_lagtime_max_dict[pair[0]]['last_lagtime'])\n        else:\n            lag_time_list.append(0)\n            user_lagtime_max_dict[pair[0]] = {}\n            user_lagtime_max_dict[pair[0]]['user_lag_time_max'] = pair[1]\n            user_lagtime_max_dict[pair[0]]['last_lagtime'] = 0\n            \n    test_df['lag_time'] = lag_time_list\n            \n    return(test_df, user_lagtime_max_dict)","fea55d88":"env = riiideducation.make_env()\niter_test = env.iter_test()\n\nprevious_test_df = None","71b9df84":"for (test_df, sample_prediction_df) in iter_test:\n    if previous_test_df is not None:\n        previous_test_df[target] = eval(test_df[\"prior_group_answers_correct\"].iloc[0])\n        update_content_stats(content_answers_df, previous_test_df)\n        update_user_stats(user_answers_df, previous_test_df)\n        update_lect_stats(user_lecture_stats_part, lectures_df, previous_test_df)\n    \n    train_df, state = add_and_update_seen_before(test_df, state)\n    test_df, user_lagtime_max_dict = add_and_update_lag_time(test_df, user_lagtime_max_dict)\n    \n    test_df = pd.merge(test_df, questions[['part','tags']], left_on='content_id', right_index=True, how = 'left')\n    \n    test_df = test_df.merge(user_answers_df, how='left', left_on='user_id',right_index=True)\n    test_df = test_df.merge(content_answers_df, how='left', left_on='content_id', right_index=True)\n    test_df = test_df.merge(part_answers_df, how='left', left_on='part', right_index=True)\n    test_df = test_df.merge(tags_answers_df, how='left', left_on='tags', right_index=True)\n    test_df = test_df.merge(user_lecture_stats_part,  how='left', left_on='user_id', right_index=True)\n    \n    test_df['part_1'].fillna(0, inplace = True)\n    test_df['part_2'].fillna(0, inplace = True)\n    test_df['part_3'].fillna(0, inplace = True)\n    test_df['part_4'].fillna(0, inplace = True)\n    test_df['part_5'].fillna(0, inplace = True)\n    test_df['part_6'].fillna(0, inplace = True)\n    test_df['part_7'].fillna(0, inplace = True)\n    test_df['type_of_concept'].fillna(0, inplace = True)\n    test_df['type_of_intention'].fillna(0, inplace = True)\n    test_df['type_of_solving_question'].fillna(0, inplace = True)\n    test_df['type_of_starter'].fillna(0, inplace = True)\n    test_df['lectures_seen'].fillna(0, inplace = True)\n    test_df['prior_question_elapsed_time'].fillna(25423, inplace = True)\n    test_df['avg_q_time'].fillna(0, inplace = True)\n    test_df['lag_time'].fillna(26161758, inplace = True)\n\n    test_df['lectures_seen'] = test_df['lectures_seen'].astype(int)\n    \n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(value=False).astype(bool)\n    test_df.fillna(value = 0.6, inplace = True)\n\n    test_df['answered_correctly'] = model.predict_proba(test_df[features])[:,1]\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n    previous_test_df = test_df.copy()","c118a356":"The next kernel is a dataframe that keeps track of what type of lectures each user had seen. This was another great dataframe, and provided numerous features for the model to train on. ","855da9ed":"Specifying the datatypes of the dataset that I was loading in really helped keep the size of the files down. I did so in the following kernel on the question metadata, and did so on the training dataframe in the riiid-splitting-train-and-test-data before loading them into this notebook.","b37c954e":"Uncommenting the following cell gets optuna up and searching for even better parameters. The one thing I could not figure out, but would be helpful is to be able to set the starting parameters that it uses. I could only figure out how to randomize the parameter starting point.","02122fed":"### Calling environment to Make Preds","431073d6":"A dataframe with the number of questions answered, the number of questions correct, and the mean_accuracy of each user.","24fab321":"The train_test_split was only used when training the hyperparameters, and selecting features as it allowed me to evaluate the predictiveness of the model. ","b52732b1":"### Final Preds on Test Data\n\nTraditionally it is frowned upon to add loops into the test_data pipeline, but I agree with Tito's notebook in that in this instance updating features does actually increase model performance. I did test this out, and with looping my score was much better.\n\n\nThe following loops are essentially performing the same methods as I did in the training dataset preprocessing functions. There are a couple more functions that I added here which updated features which were brought in from another notebook for the training data. \n\nFunctions:\n\n* update_content_stats\n* update_user_stats\n* update_lect_stats\n* add_and_update_seen_before\n* add_and_update_lag_time","5021bb83":"I then deleted the groupby objects to free up some space before importing the preprocessed statistics of each content by its id. I will note that I was very strict on making sure to get these statistics from the exact same questions as I did in this notebook.\n\nThis is important as I would otherwise have had some data leakage!","47cf3c0d":"### Features not created in this notebook\n\n**Community**\n\nThis feature was actually really interesting, and I learnt alot from Alex Bader who was the brains behind this feature. It basically groups the tags into five groups. Some of the tags were \"connected\" by a specific tag number, others were by themselves, and others were in pairs. He did this using a framework called networkx which provided great visualizations of these connections. I encourage you to check out his analysis linked at the top of this notebook.\n\n**num_in_bundle**\n\nThis feature came about when I realized that we dont see some of the questions in the entire training dataset. I realized that three consecutive questions werent seen, and it occured to me that some questions are connected and always answered together. I created a feature that says how many parts to the question there is.\n\n**avg_q_time**\n\nThis feature was brought in from the content_answers_df and was basically the total average amount of time spent on each question. I was able to get this variable by shifting the prior_question_elapsed_time variable and taking the average of all the times. I ran into some difficulty here as there were a number of numpy inf and pandas NaN values which could not be dropped easily.\n\n**lag_time**\n\nI probably could have added created this feature in this notebook, but it did take a while to loop through and add this feature. I mentioned a little bit about this variable above, but it is essentially the time between the current interaction and the previous interaction of the specific user. This was one of the last features I came across and bumped up my score by a decent margin.\n\n\nNOTE: It is also a great help to list the column names of all the features, and the column that you will be predicting as it makes the pipeline more readable and easy to debug.\n\n","22e51e6f":"### Preprocessing functions\n\nThe following functions are crucial to this notebook. They allow the datapipeline to go through and provide updated statistics on: \n\n* lectures that users attend\n* user statistics\n* content statistics\n\nFirst of all I converted the dataframes to dictionarues, and converted the columns to numpy arrays and looped through those as this was much more memory efficient and was a much quicker solution. I added the values I needed to a list and then added these lists as columns at the end. \n\nThe final function I have written here is taking the max user lagtime so that if by chance a user is midway through a question set, we can input the correct lagtime to that interaction.\n","807bc992":"### Imports and Loading Data","79d217a6":"A dataframe with the number of questions answered, number of questions correct, mean_accuracy, and skew of each question tag.","c39b5a73":"# Closing Notes\n\n### Things I would do differently\n\nWith more time I would have liked to have implemented a model that trained on the entire dataset. I experiemented with this a little bit near the end and found thtat by dropping the features with low importance and increasing the size of the training dataset I can see improvenements in my model score.\n\nI would also have liked to experiment more with deep learning models as this was such a large dataset and these kind of models perform very well on large datasets. I did attempt to create a deep learning model, but it was not as accurate as the LGB I trained.. Still unsure if this was due to hyperparamter tuning, selected layers, or data preprocessing.\n\n### Things I would like to improve on\n\nThe big takeway from this competition is that I really need to become more proficcient in using deep learning models on tabular datasets. I have spent a good portion of time on this already, and am learning something new everyday.\n\nFurthermore, I have been exploring automated layer optimization frameworks in order to give myy deep learning models the best structure. Other than fine hyperparameter tuning, it seems seems that there is no framework out there that will optimize layers. Maybe once I understand a bit more about the topic, maybe I could create a package myself?","b4b2d02d":"# Opening Notes \n\nThis competition was really fun for me and I learned so much as the competition progressed. Here are a few of the things I have learnt along the way.\n\n- Manipulating\/transforming\/loading large datasets with compute limits \n- Advanced data manipulation and analysis using pandas (Pandas is now my top tag on Stack Overflow!)\n- Learnt how to use an Automized hyperparameter tuner called Optuna\n- Feature engineering with tabular time series data\n- and so much more!\n\n### Data Sources\nI used 4 different data sources to load into this one notebook, and they are as follows.\n\n1. riid-test-answer-prediction\n\nThis was the actual competition data which contained csv file with 100 million rows of users and their interactions with quesitons. There was also metadata referring to each question_id and lecture_id, and example_test data, and a sample submission. I will go into more detail regarding the features provided throughout the notebook.\n\n\n2. riiid-splitting-train-and-test-data\n\nI created a seperate notebook to split the notebook into two seperate datasets because I was unable to stay within CPU and RAM notebook limits. I also wanted to save the dataset to pickle as otherwise it took forever to load in. This enabled me to use a portion of the data to get specific user features and statistics on each question type. I go into into more detail in that notebook about the split and encourage you to check it out. \n\n3. riiid-content-answers-df-preperation\n\nThis is essentially a csv file of question statistics that I put together, and I looped through this and updated the question accuracy as I preprocessed the training dataset. I could not do this in this notebook as training the LGB model would just force the Notebook to go over the CPU limits.\n\n4. riid-train-df\n\nPickled 100 million rows so I could load in all the data. I had to do this as some of the dictionaries necessary to create features were to large to keep when training the model. I found a good solution to this was to load in all the data at the end of the notebook after deleting it all to get those dictionaries back before making predictions on the test dataset.\n\n### Helpful Notebooks\n\nTito's Looping Strategy - [link](https:\/\/www.kaggle.com\/its7171\/lgbm-with-loop-feature-engineering)\n\nVopani's Notebook on Large Datasets -  [link](https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets)\n\nAlex Bader's Notebook on Quesion Tags - [link](https:\/\/www.kaggle.com\/spacelx\/2020-r3id-clustering-question-tags)\n\nMark wijkhuizen's Feature Eng - [link](https:\/\/www.kaggle.com\/markwijkhuizen\/riiid-training-and-prediction-using-a-state)\n\nTakamotoki's Data Manipulation Techniques [link](https:\/\/www.kaggle.com\/takamotoki\/lgbm-iii-part3-adding-lecture-features)\n\n\n\n","4cebd9c2":"One of the features I created in a different notebook was called lag_time. Each interaction had a timestamp, and I was able to create a feature which was basically the amount of time since they had last answered a question. \n\nWhen creating this feature I found that some questions had multiple parts, and the timestamps were identical. If I did not create this dictionary I would have all but one of the connected questions with a lag_time of 0.\n\nCreating this dataframe enabled me to loop through the train and test data and input the proper lag time value.","286fdce6":"Once I had got all the information needed from the lecture interactions, I removed all of these rows from the data and focused on creating features from the questions. Before doing this I made sure merge the question tag and the question ID with the dataframe in order to merge with the training dataset down the line. ","1031c504":"### Preprocessing Training DataSet\n\nWe start off by adding the updating the user lecture stats, and then removing all of the lecture rows from the data as we cant make predictions on these rows. \n\nThen come the next three functions specified above:\n\n* update_user_max_timestamp\n* add_and_update_user_stats\n* add_and_update_content_stats\n\nI then merged the questions metadata on the content_id so that I could add specific part and tag statistics. \n\nThen came filling all null values mostly with zeros, with the excpetion of time features which I filled with their respective means. \n\nRe-ordered the columns so that the predicted column is found at the end of the dataframe.\n\nRemoved numpy inf values and filled them with zeros. \n","82d11927":"The next few cells are another aspect of this competition where I gained some very useful skills. Before this compettion I was used to manually altering hyperparameters through loops and gridsearch methods. Optuna is really helpful as it provides a intuitive method to optimize hyperparameters with ease. ","4877d966":"Deleting the portion of the dataset which we got our statistics from to free up some memory.","47ef3f8e":"### Feature Engineering\n\nI did quite a bit of feature engineering in a seperate notebook to stay withing the CPU limits but nonetheless still go most of it done below.\n\nIn the next kernel I created a feature which basically told the model wether this specific user had seen this specific quesiton before. It turned out to be a great feature and provided a good boost in my score when I added it in.","8a2349c5":"This is a pretty relaxed way of checking how important the features are in the model. The feature importance is calculated by the amount that each attribute split point improves the performance measure of the model. This seems to be a good indicator of feature importance for LGB models.","2dcd279c":"If a train_test_split was performed here you can uncomment the print line to see the roc_auc_score of the model, and evaluate how effective it is.","2bd80968":"As I mentioned at the start of the notebook, I could not keep dictionary's that I used in preprocessing in memory while I trained the LGB model. This wasnt too much of a problem as I easily loaded the entire dataset back into memory and got the necessary dictionaries back in memory for use on the test data.","1e6a4b47":"A dataframe with the number of questions answered and mean_accuracy grouped by question part. \n\nI ended up leaving out the idea of a loop updating this stat as it did little to the accuracy, and the part_questions_correct was an underwhelming feature."}}