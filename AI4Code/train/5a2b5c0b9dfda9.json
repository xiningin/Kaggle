{"cell_type":{"35be7bef":"code","ccc71ef3":"code","cbc9f7e5":"code","b53a0a5d":"code","2f2f8691":"code","8ec77467":"code","ed3011eb":"code","1468d989":"code","983a285e":"code","cfd62a77":"code","da329829":"code","af600ebc":"code","1ee34784":"code","e6f692c5":"code","e4759842":"code","9180f679":"code","cf0d1bcc":"code","2046b466":"code","a6bf3f67":"code","4f1ca83b":"code","bc82b85e":"code","b59e94ae":"code","892de593":"code","ff0c1f52":"code","8f11ba1b":"code","063c3cc0":"code","cb998673":"code","6805788c":"code","c4372a80":"code","aa62b3f4":"code","fb988306":"code","302ea62b":"code","3ed7aba3":"code","07999bf3":"code","8dfd4dba":"code","640ba9f0":"code","bf4f15a2":"code","b38fa032":"code","e966824e":"code","ffd121d4":"code","1e62ac64":"code","535bcab2":"code","9bae8569":"code","ef8c2d2d":"code","1ab708fe":"code","32488214":"code","ad565cfb":"code","92234e41":"code","7b2864c2":"code","5fdaa65c":"code","5e90338b":"code","30aaa724":"code","132e81e8":"code","fd17a951":"code","619d0bc0":"code","c1026351":"code","581eb86a":"code","953c8abb":"code","e09250f2":"code","2c296d35":"code","223ac681":"code","ba638d08":"code","d47fe0fa":"code","60ffe260":"code","ae7e9e64":"markdown","25000df3":"markdown","b540bcfb":"markdown","e7caa9f2":"markdown","270ddc49":"markdown","2025dabc":"markdown","c57890b4":"markdown","bf99c93b":"markdown","4e222c12":"markdown","0bb2b958":"markdown","d2b59ebb":"markdown","1a780d51":"markdown","f36929d7":"markdown","fee9fbae":"markdown","6676c2ff":"markdown","14d32bd8":"markdown","7f01f722":"markdown","804a3aff":"markdown","0ea881c5":"markdown","1de30cb6":"markdown","a2066782":"markdown","9e0c23d2":"markdown","017f40bb":"markdown","96263823":"markdown","9a44b6e7":"markdown","68bbba41":"markdown","ec910704":"markdown","6b822b5d":"markdown","f7ba8310":"markdown","ba1e14f8":"markdown","f6436163":"markdown","d1d4c887":"markdown","44315d32":"markdown","c913e669":"markdown"},"source":{"35be7bef":"# Data Analysis #\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Model stuff #\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\nplt.style.use(\"seaborn\")\nplt.rcParams['figure.figsize'] = (12,5)","ccc71ef3":"# Import data #\ndpath = '..\/input\/'\ndiamonddf = pd.read_csv(dpath + \"diamonds.csv\")","cbc9f7e5":"diamonddf.head()","b53a0a5d":"diamonddf.info()","2f2f8691":"diamonddf.drop(\"Unnamed: 0\", axis = 1, inplace = True) # drop weird column","8ec77467":"diamonddf.isnull().sum()","ed3011eb":"diamonddf.dtypes","1468d989":"# Are there any weird values? #\ndiamonddf.describe(include=['O'])","983a285e":"# Quantitative description #\ndiamonddf.describe()","cfd62a77":"numcols = diamonddf.select_dtypes(include = ['float64','int64']).columns.tolist()","da329829":"colors = sns.color_palette(\"deep\")\nfig,axes = plt.subplots(3,3, figsize = (12,8)) # up to 9 quant vars\nsns.distplot(diamonddf[\"carat\"], color = colors[0], ax = axes[0,0])\nsns.distplot(diamonddf[\"depth\"], color = colors[1], ax = axes[0,1])\nsns.distplot(diamonddf[\"table\"], color = colors[2], ax = axes[0,2])\nsns.distplot(diamonddf[\"price\"], color = colors[3], ax = axes[1,0])\nsns.distplot(diamonddf[\"x\"], color = colors[4], ax = axes[1,1])\nsns.distplot(diamonddf[\"y\"], color = colors[0], ax = axes[1,2])\nsns.distplot(diamonddf[\"z\"], color = colors[1], ax = axes[2,0])\nplt.suptitle(\"Distribution of Quantitative Data\", size = 16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","af600ebc":"colors = sns.color_palette(\"deep\")\nfig,axes = plt.subplots(3,3, figsize = (12,8)) # up to 9 quant vars\nsns.boxplot(y= diamonddf[\"carat\"], color = colors[0], ax = axes[0,0])\nsns.boxplot(y = diamonddf[\"depth\"], color = colors[1], ax = axes[0,1])\nsns.boxplot(y = diamonddf[\"table\"], color = colors[2], ax = axes[0,2])\nsns.boxplot(y = diamonddf[\"price\"], color = colors[3], ax = axes[1,0])\nsns.boxplot(y = diamonddf[\"x\"], color = colors[4], ax = axes[1,1])\nsns.boxplot(y = diamonddf[\"y\"], color = colors[0], ax = axes[1,2])\nsns.boxplot(y = diamonddf[\"z\"], color = colors[1], ax = axes[2,0])\nplt.suptitle(\"Distribution of Quantitative Data (boxplots)\", size = 16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","1ee34784":"# diamonds that are probably errors\nzero_df = diamonddf[(diamonddf['x'] == 0) |\n           (diamonddf['y'] == 0) |\n           (diamonddf['z'] == 0)]\nzero_df.head()","e6f692c5":"zero_df.shape","e4759842":"# Drop the rows with zero as any x, y, or z\ndiamonddf.drop(zero_df.index, inplace = True)","9180f679":"cat_vars = diamonddf.select_dtypes(include = 'object').columns.tolist()\nfig, axes = plt.subplots(1,3, figsize = (12,5))\ni = 0\nfor var_name in cat_vars:\n    diamonddf[var_name].value_counts().sort_values().plot(kind = 'barh', color = 'C0', ax = axes[i])\n    axes[i].set_title(var_name)\n    i += 1\nplt.tight_layout()\nplt.show()","cf0d1bcc":"train_df, test_df = train_test_split(diamonddf, test_size=0.2, random_state=12)","2046b466":"Y_test = test_df['price']\nX_test = test_df.drop('price', axis = 1)","a6bf3f67":"print(\"Total dataset size: {}\".format(diamonddf.shape))\nprint(\"Training set size (80%): {}\".format(train_df.shape))\nprint(\"Test set size (20%): {}\".format(test_df.shape))","4f1ca83b":"diamonds = train_df.copy()","bc82b85e":"# Pair plot#\nsns.pairplot(diamonds)\nplt.show()","b59e94ae":"ol1 = diamonds[diamonds['z'] > 20].index\nol2 = diamonds[diamonds['y'] > 20].index\n\nfig, axes = plt.subplots(1,3, figsize = (12,4))\nsns.scatterplot(x = diamonds['carat'], y = diamonds['z'], ax = axes[0]) \naxes[0].annotate(ol1[0], (diamonds['carat'].loc[ol1], diamonds['z'].loc[ol1]), size = 12)\n\nsns.scatterplot(x = diamonds['x'], y = diamonds['y'], ax = axes[1])\naxes[1].annotate(ol2[0], (diamonds['x'].loc[ol2], diamonds['y'].loc[ol2]), size = 12)\n\nsns.scatterplot(x = diamonds['y'], y = diamonds['z'], ax = axes[2])\naxes[2].annotate(ol1[0], (diamonds['y'].loc[ol1], diamonds['z'].loc[ol1]), size = 12)\naxes[2].annotate(ol2[0], (diamonds['y'].loc[ol2] - 4, diamonds['z'].loc[ol2] + 1), size = 12)\n\nplt.suptitle(\"Outliers in 3 sample plots\", size = 14)\nplt.show()","892de593":"diamonds[diamonds['z'] > 20]","ff0c1f52":"diamonds[diamonds['y'] > 20]","8f11ba1b":"diamonds['z'].describe()","063c3cc0":"cond = (diamonds['y'] > 20) | (diamonds['z'] > 20) \ndiamonds.drop(diamonds[cond].index, inplace = True)","cb998673":"fig, axes = plt.subplots(1,3, figsize = (12,4))\nsns.scatterplot(x = diamonds['carat'], y = diamonds['z'], ax = axes[0]) \n\nsns.scatterplot(x = diamonds['x'], y = diamonds['y'], ax = axes[1])\n\nsns.scatterplot(x = diamonds['y'], y = diamonds['z'], ax = axes[2])\n\nplt.suptitle(\"3 Sample Plots without Outliers\", size = 16)\nplt.show()","6805788c":"sns.heatmap(diamonds.corr(), cmap = \"RdBu_r\", square = True, annot=True, cbar=True)\nplt.title(\"Correlation Between Variables\")\nplt.show()","c4372a80":"# Drop x, y, and z #\ndiamonds.drop(['x','y','z'], axis = 1, inplace = True)","aa62b3f4":"clar_order = [\"I1\", \"SI2\", \"SI1\", \"VS2\", \"VS1\", \"VVS2\", \"VVS1\", \"IF\"]\ncut_order = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\ncolor_order = sorted(diamonds['color'].unique().tolist(), reverse = True)","fb988306":"fig, axes = plt.subplots(1,2, figsize = (12,5))\nsns.boxplot(x = \"cut\", y = \"price\", data = diamonds, order = cut_order, ax = axes[0], palette = 'Blues')\nsns.boxplot(x = 'clarity', y = 'price', data = diamonds, order = clar_order, ax = axes[1], palette = 'Blues')\nplt.suptitle(\"Diamond Price by Cut and Clarity\", size = 14)\nplt.show()","302ea62b":"fig, axes = plt.subplots(1,2, figsize = (12,5))\nsns.scatterplot(x = 'carat', y = 'price', hue = \"cut\", palette = 'Blues', hue_order = cut_order,\n                size = 10, data = diamonds, ax = axes[0])\nsns.scatterplot(x = 'carat', y = 'price', hue = \"clarity\", palette = 'Blues', hue_order = clar_order,\n                size = 10, data = diamonds, ax = axes[1])\nplt.suptitle(\"Diamond Price vs. 2 predictors\", size = 14)\nplt.show()","3ed7aba3":"fig, axes = plt.subplots(1,2, figsize = (12,5))\nsns.boxplot(x = \"color\", y = \"price\", data = diamonds, order = color_order, palette = 'Blues', ax = axes[0])\nsns.scatterplot(x = 'carat', y = 'price', hue = \"color\", palette = 'Blues', hue_order = color_order,\n                size = 10, data = diamonds, ax = axes[1])\nplt.suptitle(\"Diamond Price by Color\", size = 14)\nplt.show()","07999bf3":"fig, axes = plt.subplots(2, 3, figsize = (12,6))\nsns.kdeplot(np.log(diamonds['price']), shade=True , color='r', ax = axes[0,0])\naxes[0,0].set_title(\"Log transform\")\nsns.kdeplot(np.sqrt(diamonds['price']), shade=True , color='b', ax = axes[0,1])\naxes[0,1].set_title(\"Square root transform\")\nsns.kdeplot((diamonds['price']**(1\/3)), shade=True , color='coral', ax = axes[0,2])\naxes[0,2].set_title(\"Cube root transform\")\nsns.boxplot(y = np.log(diamonds['price']), ax = axes[1,0], color = 'coral')\nsns.boxplot(y = np.sqrt(diamonds['price']), ax = axes[1,1], color = 'coral')\nsns.boxplot(y = (diamonds['price']**(1\/3)), ax = axes[1,2], color = 'coral')\nplt.tight_layout()\nplt.show()","8dfd4dba":"def error_metrics(y_true, y_pred):\n    mean_abs = \"Mean Absolute Error: {}\".format(mean_absolute_error(y_true, y_pred))\n    mean_squared = \"Mean Square Error: {}\".format(mean_squared_error(y_true, y_pred))\n    r2 = \"r2 score: {}\".format(r2_score(y_true, y_pred))\n    return mean_abs, mean_squared, r2","640ba9f0":"# Remove the label #\nX_train = diamonds.drop('price', axis = 1)\nY_train = diamonds['price'].copy()","bf4f15a2":"def cat_mapper(categories):\n    \"create a dictionary that maps integers to the ordered categories\"\n    i = 0\n    mapped = {}\n    for cat in categories:\n        mapped[cat] = i\n        i += 1\n    return mapped","b38fa032":"cat_mapper(color_order)","e966824e":"cat_mapper(cut_order)","ffd121d4":"cat_mapper(clar_order)","1e62ac64":"X_train[cat_vars].head()","535bcab2":"X_train_mapped = X_train.copy()\nX_train_mapped['cut'] = X_train_mapped['cut'].map(cat_mapper(cut_order))\nX_train_mapped['color'] = X_train_mapped['color'].map(cat_mapper(color_order))\nX_train_mapped['clarity'] = X_train_mapped['clarity'].map(cat_mapper(clar_order))","9bae8569":"minmaxscaler = MinMaxScaler()\nnumcols = ['carat','depth','table']","ef8c2d2d":"X_train_mapped[numcols] = minmaxscaler.fit_transform(X_train_mapped[numcols])","1ab708fe":"X_train_mapped.head()","32488214":"# Adjusting the test datasets #\nX_test.drop(['x','y','z'], axis = 1, inplace = True)\nX_test['cut'] = X_test['cut'].map(cat_mapper(cut_order))\nX_test['color'] = X_test['color'].map(cat_mapper(color_order))\nX_test['clarity'] = X_test['clarity'].map(cat_mapper(clar_order))","ad565cfb":"X_test[numcols] = minmaxscaler.transform(X_test[numcols])","92234e41":"alphas = [.01,.1,1,10,100,1000,10000]","7b2864c2":"ridge = RidgeCV(alphas = alphas, cv = 5)\nridge_fit = ridge.fit(X_train_mapped, Y_train)","5fdaa65c":"yhat_ridge = ridge_fit.predict(X_test)","5e90338b":"sns.distplot(Y_test - yhat_ridge)\nplt.title(\"Distribution of Errors (Ridge Regression)\")\nplt.show()","30aaa724":"x = np.linspace(0, 30000, 1000)\nsns.scatterplot(x = Y_test, y = yhat_ridge)\nplt.plot(x,x, color = 'red', linestyle = 'dashed')\nplt.xlim(-100, 36000)\nplt.ylim(-100, 36000)\nplt.title(\"Actual vs. Predicted (Ridge Regression)\")\nplt.show()","132e81e8":"# Ridge error metrics #\nerror_metrics(Y_test, yhat_ridge)","fd17a951":"lasso = LassoCV(cv=5, random_state=12, alphas = alphas)\nlasso_fit = lasso.fit(X_train_mapped, Y_train)\nyhat_lasso = lasso_fit.predict(X_test)","619d0bc0":"error_metrics(Y_test, yhat_lasso)","c1026351":"sns.distplot(Y_test - yhat_lasso)\nplt.title(\"Distribution of Errors (LASSO Regression)\")\nplt.show()","581eb86a":"sns.scatterplot(x = Y_test, y = yhat_lasso)\nplt.plot(x,x, color = 'red', linestyle = 'dashed')\nplt.xlim(-100, 36000)\nplt.ylim(-100, 36000)\nplt.title(\"Actual vs. Predicted (LASSO Regression)\")\nplt.show()","953c8abb":"elasticnet = ElasticNetCV(cv=5, random_state=12,\n                          l1_ratio = 0.9,\n                          alphas = alphas)\nelastic_fit = elasticnet.fit(X_train_mapped, Y_train)\nyhat_elastic = elastic_fit.predict(X_test)","e09250f2":"error_metrics(Y_test, yhat_elastic)","2c296d35":"randomforest = RandomForestRegressor(max_depth=5, \n                                     random_state=12, \n                                     n_estimators = 1000)\n                                     ","223ac681":"rf_fit = randomforest.fit(X_train_mapped, Y_train)\nyhat_rf = rf_fit.predict(X_test)","ba638d08":"error_metrics(Y_test, yhat_rf)","d47fe0fa":"sns.distplot(Y_test - yhat_rf)\nplt.title(\"Distribution of Errors (Random Forest Regression)\")\nplt.show()","60ffe260":"sns.scatterplot(x = Y_test, y = yhat_rf)\nplt.plot(x,x, color = 'red', linestyle = 'dashed')\nplt.xlim(-100,20000)\nplt.ylim(-100, 20000)\nplt.title(\"Actual vs. Predicted (LASSO Regression)\", size = 14)\nplt.show()","ae7e9e64":"## ElasticNet Regression\n\nThe results of both models are similar, but let's see if a mix of both types of penalties ($L_1$ and $L_2$)improve the prediction accuracy.","25000df3":"# Predicting Diamond Prices \n\nIn this notebook we will go through a project, from analyzing data to testing different regression models. The objective is to predict the price of a diamond based on different attributes of the diamond.\n\n1. Data Overview\n2. Split train \/ test + EDA\n3. Normalization \/ Modeling","b540bcfb":"It looks like the model underpredicts for diamonds with a price of 10,000, similar to the ridge regression model. This is evident in the histogram (mean is less than zero) and in the scatterplot (the trend is a curve under $y = x$ line)","e7caa9f2":"There is very apparent collinearity here. x,z, and z are all correlated with each other (and should either be combined, or one should be used to model the price). There is also a very strong relationship between carat and x, y, and z. **This is probably because carat is a unit of weight**. \n\nWhile not always the case, it does appear that carat is a function of the dimensions with some density coefficient [(source)](https:\/\/www.jewelrynotes.com\/how-to-calculate-a-diamonds-weight-in-carats\/).\n\nSo we will drop x, y, and z.","270ddc49":"After a couple of tests with `l1_ratio` equaling 0.5, 0.3, 0.8, and 0.9 we see that using an L1 penalty reduces our errors and increases our $R^2$ score. However having a bit of the $L_2$ penalty decreased our errors.","2025dabc":"## Investigate strange outlier\n\nWe tried ignoring the outlier but it looks like an obvious error. Let's take a look at the weird outlier that shows up in the y and z plots. ","c57890b4":"## Categorical Variables","bf99c93b":"We're also going to have to remove the outlier from out X_train. The models will learn more useful information this way.\n\n**Missing Values:**\n\nWe removed the observations where x, y, or z were zero. There aren't any other missing values.","4e222c12":"Error metrics:\n\n- Mean Absolute Error (how far away are my predictions from ground truth?)\n- Mean Squared Error (is my model making large errors?)\n- $R^2$ Score (goodness-of-fit)\n\nSince all of our models contain the same amount of predictors $p$, we won't have to refer to the $R^2_{adj}$ score.","0bb2b958":"It looks like there aren't any extra categories as the result of spelling or fat finger errors. ","d2b59ebb":"# 2. Create a test set\n\nBefore going any deeper into the analysis we will create a test set for model evaluation. We don't want to add any bias to the way we create models.","1a780d51":"The distribution of depth, table, y, and z all have long tails. There is a particular value in $z$ that looks like an error or extreme outlier, and there are other outliers we can explore too.\n\nx, y, and z shouldn't have 0 as a value, since that wouldn't make physical sense. We can remove all rows where any of these variables are zero, or we can impute them.","f36929d7":"There is an interaction between carat, price, and color. Same goes with carat, price, and clarity. Carat and cut don't interact as distinctly.","fee9fbae":"Looks good.","6676c2ff":"# Transformations on Price\n\nWhen looking at a summary of our data, it appeared that the response was skewed. While this isn't crucial, we can visualize the effect of different transformations (square root, log, cube root, etc.) on the response. We'll also experiment with this in our modeling stage.","14d32bd8":"# Final Models\n\nThe best models in terms of mean absolute error are:\n\n1. Random Forest Regression: **\\$538.72** \n2. ElasticNet (with 0.9 L1 penalty): **\\$854.20**\n3. Ridge: **\\$860.88**\n4. LASSO: **\\$861.01**\n","7f01f722":"* We will stray as much as possible from dropping any outliers since we lack context. That wonky z value could actually be completely valid. Next we will briefly check the categorical variables for any blatant errors.","804a3aff":"# Correlation categorical predictors and price\n\nWe will now investigate the relationship between a diamond's attributes and its price using categorical variables, and a combination of continuous\/categorical. First, we order the categories.","0ea881c5":"# EDA\n\n- Which predictors are correlated to the price of a diamond?\n- Are high quality diamonds worth more than low quality diamonds?\n- Are there any immediate interaction effects between a categorical predictor, numerical predictor, and the response?\n- Is there collinearity\/multicollinearity in the dataset?\n- Are there any clear outliers that we should investigate?\n","1de30cb6":"# 3. Modeling\n\nWe will consider a few regression models:\n\n- Ridge Regression\n- LASSO Regression\n- ElasticNet\n- Random Forest Regression\n\nXGBoost, Support Vector Regression and a stacked ensemble were considered, but SVR was too slow due to the number of features and simple models were performing well which indicated that more complex models weren't necessary.\n\n\nWe have split our training and test set already, so we can start by one-hot encoding our categorical variables, and then normalizing our numerical variables.\n\nWe provide an interpretable model, as well as a model that (likely) performs better at the loss of some interpretability.","a2066782":"## LASSO Regression","9e0c23d2":"The two outliers might be 3.18 instead of 31.8, this is an assumption but it seems reasonable.\n\nThese are two values out of 50,000 that look really weird, and will definitely impact regression models that aren't robust to outliers. Iterative methods like random forest and gradient boosting will be able to handle them, but I will assume that these are errors and that the chances that they will occur naturally in the real world are slim. \n\nIt's important to note that this isn't the way to handle outliers every time though; imagine if it was a type of observation that wasn't recorded because someone lost all of the diamonds of this type (or something like that). ","017f40bb":"## Ridge Regression","96263823":"Looks great. ","9a44b6e7":"## Column descriptions\n\n- **carat:** The weight of the diamond, equivalent to 200mg (should be a good indicator)\n- **cut:** Quality of the cut\n- **color:** Color of the diamond from J to D (worst to best)\n- **clarity:** How clear the diamond is; I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)\n- **depth:** Total depth percentage (relative to x and y). **Will likely be collinear.**\n- **table:** Width of top of diamond relative to widest point (43--95)\n- **price:** In US dollars ($)\n- **x, y, z:** Dimensions of the diamond","68bbba41":"# 1. Data Overview","ec910704":"### Scaling Features\n\nWe will use MinMax scaling, with the possibility of trying StandardScaler in case we want all of the predictors to be roughly normal. We don't have many outliers so robust transformers won't be necessary.","6b822b5d":"It looks like price is skewed right, so we should log transform it for better predictions. There aren't any clear predictors here, so we will have to experiment to find best combinations. There's also a pretty obvious outlier that we should investigate further.\n\nIt also looks like $x$ is related to the carat of the diamond, so these might cause a collinearity issue. We will check the correlation matrix just to be certain.","f7ba8310":"## Random Forest Regression\n\nWe use random forest regression to test a non-linear model on the data. Support Vector Regression was initially considered, but was very slow due to the number of observations (see the [SVR documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVR.html) on sklearn for other approaches).\n\nFirst we will train a model with 1000 estimators and then use grid search if it looks like the model can be improved","ba1e14f8":"There are roughly 54,000 examples of diamonds, each with 11 qualities. One of the columns looks like an extra index. We'll drop this column.","f6436163":"## Drop two outliers","d1d4c887":"## Format data types and clean up data\n\nBefore going into the analysis, we want to remove \/ fill null entries, and change the data type of misspecified columns. We don't go into any feature engineering or anything here yet.","44315d32":"### Encoding\n\nWe use encoding since our categorical variables are actually ordinal and not purely categorical. We will use an ordered version of the categories and then map those with corresponding integers for our new columns.","c913e669":"The model was tuned with max_depth = 2 and max_depth = 5 with a large improvement when max_depth = 5. This is the best model in terms of our three error metrics."}}