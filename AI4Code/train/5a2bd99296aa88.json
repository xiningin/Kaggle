{"cell_type":{"075341ea":"code","f2fadf54":"code","a45e1506":"code","518e791b":"code","2916f7d3":"code","192d8d11":"code","fb7c9d1a":"code","90dc604a":"code","4c9fb884":"code","8e11add1":"code","48ca4102":"code","0f91861f":"code","0f5182fd":"code","11faede6":"code","f5547441":"code","fdd1a2d5":"code","318abf7d":"code","a8aeaf0c":"code","c845c13d":"code","8645ee6b":"code","835620f9":"code","1d0cbac7":"code","7c836f51":"code","d7981e50":"code","19a44a16":"code","7eab2ad5":"code","591f5735":"code","50161b5f":"code","a129c0ff":"code","8d3a9641":"code","c6ba2cbe":"code","85834a65":"code","a7c9b9c9":"code","9e68e519":"code","2edbc572":"code","ace874e7":"code","df11bbf9":"code","64c43e6b":"code","c47a0f68":"code","cce8bc24":"code","86d35998":"markdown","4c927126":"markdown","8d8b28a4":"markdown","a81c2803":"markdown","ec711d27":"markdown","674dd66b":"markdown","183295d6":"markdown","daf1ae1d":"markdown","1875757a":"markdown","ef22c0fc":"markdown","c5af12d8":"markdown","1ccc8cb7":"markdown","82e7230a":"markdown","bded741e":"markdown","b097908d":"markdown"},"source":{"075341ea":"!pip install vncorenlp\n!pip install sklearn_crfsuite\n!pip install Distance\n\nimport json\nfrom pprint import pprint\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom vncorenlp import VnCoreNLP\nimport pandas as pd\nfrom sklearn_crfsuite import CRF\nimport random\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport fasttext\nimport xgboost as xgb\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import plot_roc_curve, jaccard_score, classification_report\nfrom sklearn import svm\nimport distance","f2fadf54":"vncorenlp = VnCoreNLP(\"..\/input\/vncorenlp\/VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,ner,parse\", max_heap_size='-Xmx2g')","a45e1506":"f = open('..\/input\/law-qa\/question.train.json', 'r')\ndata_question = json.loads(f.read())\nf.close()\n\nf = open('..\/input\/law-qa\/law.json')\ndata_law = json.loads(f.read())\nf.close()","518e791b":"print(f'Length of data_question: {len(data_question)}')\nprint(f'Length of data_law: {len(data_law)}')","2916f7d3":"for single_law in data_law:\n    if single_law.get('id') == '03\/VBHN-VPQH':\n        print('Found single law')\n        for single_article in single_law.get('articles'):\n            if single_article.get('id') == str(21):\n                print(single_article)","192d8d11":"print(f'Number of question: {len(data_question)}')\nprint(f'Number of law: {len(data_law)}')","fb7c9d1a":"num_of_article = 0\ntotal_word = 0\ntotal_character = 0\nfor single_law in data_law:\n    num_of_article += len(single_law.get('articles'))\n    for article in single_law.get('articles'):\n        total_word += len(article.get('text').split(' '))\n        total_character += len(article.get('text'))\n\nprint(num_of_article)\nprint(total_word \/ num_of_article)\nprint(total_character \/ num_of_article)","90dc604a":"# 2314 c\u1ee7a\n# 1926 \u0111\u01b0\u1ee3c\n# 1874 quy_\u0111\u1ecbnh\n# 1674 trong\n# 1646 c\u00f3\n# 1528 n\u00e0y\n# 1477 v\u00e0\n# 1274 ho\u1eb7c\n# 1242 c\u00e1c\n# 1210 theo\n# 1197 ng\u01b0\u1eddi\n# 1169 th\u00ec\n# 1159 kh\u00f4ng\n# 1063 v\u1ec1\n# 1062 tr\u01b0\u1eddng_h\u1ee3p\n# 1057 \u0111\u00e3\n# 1035 kh\u00e1c\n# 1021 cho\n\nstop_word = ['c\u1ee7a', 'v\u00e0', 'ng\u01b0\u1eddi', 'c\u00f3', '\u0111\u01b0\u1ee3c', 'lu\u1eadt_s\u01b0', \n             'quy_\u0111\u1ecbnh', 'trong', 'kh\u00f4ng', 'quy\u1ec1n', 'trong', \n             'n\u00e0y', 'ho\u1eb7c', 'c\u00e1c', 'theo', 'th\u00ec', 'v\u1ec1', 'tr\u01b0\u1eddng_h\u1ee3p', '\u0111\u00e3', 'kh\u00e1c', 'cho'\n            ]\n# stop_word = []","4c9fb884":"a = np.array([1,2,3,4])\nb = np.array([True, False, True, False])\na[b]","8e11add1":"f = open('..\/input\/law-qa\/question.train.json', 'r')\ndata_question = json.loads(f.read())\nf.close()\n\nf = open('..\/input\/law-qa\/law.json')\ndata_law = json.loads(f.read())\nf.close()\n\nlaw_corpus = []\nfor single_law in data_law:\n    for article in single_law.get('articles'):\n        law_paragraph = article.get('text').replace('\\n', '')\n        law_paragraph = ''.join([i for i in law_paragraph if not i.isdigit()])\n\n        tokenized_paragraph = vncorenlp.tokenize(law_paragraph)\n        filtered_token = []\n        for tok_sentence in tokenized_paragraph:\n            for tok in tok_sentence:\n                if tok not in stop_word:\n                    filtered_token.append(tok)\n\n        tokenized_paragraph = ' '.join(filtered_token)\n        article['text'] = tokenized_paragraph\n        article['corpus_id'] = len(law_corpus)\n        law_corpus.append(tokenized_paragraph)\n\nfor question in data_question:\n    question_txt = question.get('text').replace('\\n', '')\n    question_txt = ''.join([i for i in question_txt if not i.isdigit()])\n    \n    tokenized_question = vncorenlp.tokenize(question_txt)\n\n    filtered_token = []\n    for tok_sentence in tokenized_question:\n        for tok in tok_sentence:\n            if tok not in stop_word:\n                filtered_token.append(tok)\n\n    tokenized_question = ' '.join(filtered_token)\n    question['text'] = tokenized_question\n    question['corpus_id'] = len(law_corpus)\n    law_corpus.append(tokenized_question)","48ca4102":"for quest in data_question:\n    print(quest.get('text'))\n    print('=====')","0f91861f":"law_corpus[2210]","0f5182fd":"def find_corpus_id(law_id, article_id):\n    for biglaw in data_law:\n        if biglaw.get('id') == law_id:\n            for article in biglaw.get('articles'):\n                if article.get('id') == article_id:\n                    return article.get('corpus_id')\n    return None","11faede6":"def adding_limit_neg(data, ques_corpus_id, set_relar, LIMIT_NEG=64):\n    limit = LIMIT_NEG * len(set_relar)\n    for law in data_law:\n        law_id = law.get('id')\n        list_article = law.get('articles')\n        random.shuffle(list_article)\n        for art in list_article:\n            art_id = art.get('id')\n            if (law_id, art_id) not in set_relar:\n                law_corpus_id = art.get('corpus_id')\n                data.append((ques_corpus_id, law_corpus_id, 0))\n                limit -= 1\n                if limit == 0:         \n                    return","f5547441":"def build_queart_pair():\n    data = []\n\n    for quest in data_question:\n        quest_id = quest.get('id')\n        quest_txt = quest.get('text')\n        ques_corpus_id = quest.get('corpus_id')\n        set_relar = set()\n\n        for relar in quest.get('relevant_articles'):\n            law_id = relar.get('law_id')\n            art_id = relar.get('article_id')\n#             print(law_id, art_id)            \n            law_corpus_id = find_corpus_id(law_id, art_id)\n            for i in range(10):\n                data.append((ques_corpus_id, law_corpus_id, 1))\n            set_relar.add((law_id, art_id))\n\n        adding_limit_neg(data=data, ques_corpus_id=ques_corpus_id, set_relar=set_relar)\n    return data\n\n\ndata = build_queart_pair()","fdd1a2d5":"def create_tfidf_feature(law_corpus):\n    vectorizer = TfidfVectorizer(use_idf=True)\n    tfIdf = vectorizer.fit_transform(law_corpus)\n    return vectorizer.get_feature_names(), tfIdf, vectorizer","318abf7d":"def create_feature_for_text():\n    features, tfidf_matrix = create_tfidf_feature(law_corpus)\n    \n    list_keyword = []\n    for i, txt in enumerate(law_corpus):\n        df = pd.DataFrame(\n#                 [tfidf_matrix[i].T.todense(), np.array(features).reshape(1, -1)], \n                {\n                    'TF-IDF': pd.Series(tfidf_matrix[i].toarray().flatten()),\n                    'WORD':  pd.Series(np.array(features).flatten())\n                },\n                index=[i for i in range(len(features))],\n                #     index=list_feature,\n#                     columns=[[\"TF-IDF\"],['word']]\n        )\n        notis_stopword = ~df['WORD'].isin(stop_word)\n\n        df = df[notis_stopword]\n        df = df.sort_values('TF-IDF', ascending=False)\n        keywords = df.iloc[[i for i in range(10)]].index\n        list_keyword.append(keywords)\n    return list_keyword, features","a8aeaf0c":"def create_onehot_tfidf(list_feature, most_tfidf_keyword):\n    X = []\n    for sent in most_tfidf_keyword:\n        x = np.zeros(len(list_feature))\n        for i in range(len(x)):\n            if list_feature[i] in sent:\n                x[i] = 1\n        X.append(x)\n        \n    return np.array(X)","c845c13d":"def create_tfidf_data():\n    most_tfidf_keyword, list_feature = create_feature_for_text()\n#     enc_tfidf_feature = OneHotEncoder(handle_unknown='ignore')\n#     enc_tfidf_feature.fit(most_tfidf_keyword)\n#     X = enc_tfidf_feature.transform(most_tfidf_keyword).toarray()\n    X = create_onehot_tfidf(list_feature, most_tfidf_keyword)\n#     return X, list_feature, enc_tfidf_feature\n    return X, list_feature","8645ee6b":"def create_tfidf_data2():\n    list_feature , tfidf_lawcorpus, vectorizer = create_tfidf_feature(law_corpus)\n    flatten_tfidf = []\n    for i in range(tfidf_lawcorpus.shape[0]):\n        flatten_tfidf.append(tfidf_lawcorpus[i].toarray().flatten())\n    return list_feature , np.array(flatten_tfidf), vectorizer","835620f9":"# tfidf_lawcorpus, list_feature = create_tfidf_data()\nlist_feature , tfidf_lawcorpus, vectorizer = create_tfidf_data2()","1d0cbac7":"# tfidf_lawcorpus[0].toarray().flatten()","7c836f51":"def count_df(word):\n    res = 0\n    for corpus in law_corpus:\n        if word in corpus.split(' '):\n           res += 1\n    return res","d7981e50":"# To finding Stop-word\n\n# countword = []\n\n# for word in list_feature:\n#     countword.append((count_df(word), word))\n\n# for corpus in law_corpus:\n#     if 'ng' in corpus.split(' '):\n#         print(corpus)\n\n# countword.sort(reverse=True)\n# for count, word in countword[:100]:\n#     print(count, word)","19a44a16":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef calculate_Jaccard(x, y):\n    return distance.jaccard(x, y)\n\ndef calculate_cosine(x, y):\n    return cosine_similarity(x,y)","7eab2ad5":"X = []\ny = []\n\nfor i, (corpusid1, corpusid2, label) in enumerate(data):\n    jacard_distance = calculate_Jaccard(law_corpus[corpusid1].split(' '), law_corpus[corpusid2].split(' '))\n    cosin_distance = calculate_cosine(tfidf_lawcorpus[corpusid1].reshape(1, -1), tfidf_lawcorpus[corpusid2].reshape(1, -1))\n    X.append(np.concatenate((tfidf_lawcorpus[corpusid1], tfidf_lawcorpus[corpusid2], [jacard_distance, cosin_distance[0][0]])))\n    y.append(label)\n#     if label == 1:\n#         y.append([0, 1])\n#     else:\n#         y.append([1, 0])\n\nX = np.array(X)\ny = np.array(y)\n# y = y.reshape((-1, 1)).astype('bytes')","591f5735":"y.shape","50161b5f":"sum(X)","a129c0ff":"# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","8d3a9641":"# y_train.sum() + y_test.sum()","c6ba2cbe":"# clf = svm.SVC(probability=True)\n# clf.fit(X_train, y_train)","85834a65":"# from sklearn.metrics import classification_report\n# ypred = clf.predict(X_test)","a7c9b9c9":"# from sklearn.metrics import classification_report\n# print(classification_report(y_test, ypred))","9e68e519":"# plot_roc_curve(clf, X_test, y_test)","2edbc572":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)","ace874e7":"skf = GroupKFold(n_splits=6)\nclf = xgb.XGBClassifier(\n    n_estimators=1000,\n    max_depth=12,\n    learning_rate=0.02,\n    subsample=0.8,\n    colsample_bytree=0.4,\n    missing=-1,\n    eval_metric='auc',\n    # USE CPU\n    nthread=4,\n    tree_method='hist'\n    # USE GPU\n#         tree_method='gpu_hist' \n)\n\nh = clf.fit(X_train, y_train, \n        eval_set=[(X_val,y_val)],\n        verbose=100, early_stopping_rounds=200)","df11bbf9":"plot_roc_curve(clf, X_test, y_test)","64c43e6b":"pred = clf.predict(X_test)\nprint(classification_report(y_test, pred))","c47a0f68":"def inference(statement):\n    tokenized_list = vncorenlp.tokenize(statement)\n    filtered_token = []\n    for tok_sentence in tokenized_list:\n        for tok in tok_sentence:\n            if tok not in stop_word:\n                filtered_token.append(tok)\n                \n    tokenized_statement = ' '.join(filtered_token)\n    print(tokenized_statement)\n    statement_vector = vectorizer.transform([tokenized_statement])\n    statement_vector = statement_vector.toarray().flatten()\n#     list_keyword = []\n#     for i, feature in enumerate(list_feature):\n#         if feature in tokenized_statement:\n#             list_keyword.append(i)\n\n#     while len(list_keyword) < 10:\n#         list_keyword.append(-1)\n#     statement_vector = np.array(enc_tfidf_feature.transform([list_keyword]).toarray()[0])\n    \n    list_pred = []\n    for corpusid, txt in enumerate(law_corpus):\n        jacard_distance = calculate_Jaccard(tokenized_statement.split(' '), law_corpus[corpusid].split(' '))\n        cosine_distance = calculate_cosine(statement_vector.reshape(1,-1), tfidf_lawcorpus[corpusid].reshape(1, -1))\n        x = np.concatenate((statement_vector, tfidf_lawcorpus[corpusid], [jacard_distance, cosine_distance[0][0]]))\n        proba = clf.predict_proba(np.array([x]))\n        label = clf.predict(np.array([x]))\n        list_pred.append((proba[0], label[0], corpusid, cosine_distance[0][0]))\n    list_pred.sort(key=lambda tup: float(tup[0][1]), reverse=True)\n\n    for i, (proba, label, corpusid, cosine_distance) in enumerate(list_pred[:100]):\n#         if float(cosine_distance) > 0.2:\n        print(proba, ' - ', label, ' - ' ,  law_corpus[corpusid], ' - ', cosine_distance)\n        print('==============')\n\n# inference('C\u01b0\u1edbp c\u1ee7a gi\u1ebft ng\u01b0\u1eddi l\u00e0 h\u00e0nh vi c\u00f3 th\u1ec3 b\u1ecb t\u1eed h\u00ecnh.')\n# inference('H\u1ecdc t\u1eadp l\u00e0 quy\u1ec1n v\u00e0 ngh\u0129a v\u1ee5 c\u1ee7a m\u1ed7i c\u00f4ng d\u00e2n.')\ninference('Sau khi k\u1ebft h\u00f4n , v\u1ee3 ph\u1ea3i c\u1ea3i \u0111\u1ea1o ch\u1ed3ng .')\n# inference('Nh\u00e0 n\u01b0\u1edbc ch\u00ednh s\u00e1ch bi\u1ec7n ph\u00e1p \u0111\u1ed3ng b\u1ed9 \u0111\u1ec3 b\u1ea3o \u0111\u1ea3m vi\u1ec7c th\u1ef1c hi\u1ec7n t\u1ef1 do c\u01b0 tr\u00fa c\u00f4ng d\u00e2n .')","cce8bc24":"for quest in data_question:\n    print('question: ', quest.get('text'), ':')\n    print(len(quest.get('relevant_articles')))\n    for relar in quest.get('relevant_articles'):\n        law_id = relar.get('law_id')\n        art_id = relar.get('article_id')\n        law_corpus_id = find_corpus_id(law_id, art_id)\n        print('\\t *', law_corpus[law_corpus_id])\n    print('===')","86d35998":"# Build Dataset","4c927126":"# II. Explore data","8d8b28a4":"### Build question-document pair","a81c2803":"## 1. Print out some example of data","ec711d27":"### Create Data From Feature","674dd66b":"## 2. Statistical","183295d6":"# I. Load files","daf1ae1d":"### TF-IDF Feature","1875757a":"# Feature Extraction","ef22c0fc":"# Classification ","c5af12d8":"### Tokenize Data","1ccc8cb7":"### SVM Classification","82e7230a":"# Inference","bded741e":"### Stop Word of Law","b097908d":"### XGBoost Classfication"}}