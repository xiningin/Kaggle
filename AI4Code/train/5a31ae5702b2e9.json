{"cell_type":{"35f52bff":"code","f83ce9ed":"code","8afdee3c":"code","4f1e76d1":"code","96d7a0f2":"code","0075b2c5":"code","718b1a1c":"code","f7ca98e5":"code","88b6c7d0":"code","0f65d5c4":"code","22f8ab41":"code","57ff5c41":"code","b74a5add":"code","8c32053e":"code","0ac3c19a":"code","2ba9b577":"code","a750d66e":"code","72c78dc3":"code","d2b747a0":"code","f2a6bfe8":"code","e0d026c7":"code","881fa9f8":"code","846101ad":"code","684e9be1":"code","4997e899":"code","16fbf0f2":"code","429aeb57":"code","99449a83":"code","4c64adc1":"code","5c7b892a":"code","b48a2a38":"code","7fd67713":"code","c0e45dd7":"code","9053bcbb":"code","f5ddf2de":"code","95a15902":"code","da3fce18":"markdown","34470ea1":"markdown","ed68a1b5":"markdown","4ca56cf7":"markdown","36a85301":"markdown","1c52f705":"markdown","daeeaac3":"markdown","575310c8":"markdown","a73a510d":"markdown","97eded8a":"markdown","a5b72b2c":"markdown","213709b4":"markdown","09612dc8":"markdown","3dd8b9fa":"markdown","50661df5":"markdown","89819507":"markdown","d2eb3e91":"markdown","557ea1e7":"markdown","e97d2195":"markdown","2647e7ee":"markdown","90174101":"markdown","a6e726ee":"markdown","f1ac9247":"markdown","d3bb292b":"markdown","6780ce9e":"markdown","70731b7d":"markdown","5b650421":"markdown","3e79af54":"markdown","23ea6204":"markdown","83bc54f6":"markdown","3240b767":"markdown","506b0ce2":"markdown","18534828":"markdown"},"source":{"35f52bff":"import pandas as pd\nimport pandasql as ps\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom fastprogress import master_bar, progress_bar\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","f83ce9ed":"# create features and clean data from portfolio.json\ndef process_portfolio(portfolio):\n    portfolio[\"num_channel\"] = portfolio.channels.map(lambda x: len(x))\n\n    channels = ['web', 'email', 'mobile', 'social']\n    for channel in channels:\n        col_name = 'channel_' + channel\n        portfolio[col_name] = portfolio.channels.map(lambda x: 1 if channel in x else 0)\n\n    portfolio = portfolio.drop(['channels'], axis=1)\n\n    return portfolio\n\n# create features and clean data from transcript.json\ndef process_transcript(transcript):\n\n    # split dataframe by each event\n    offer_received_transcript = transcript[transcript.event == 'offer received']\n    offer_viewed_transcript = transcript[transcript.event == 'offer viewed']\n    offer_completed_transcript = transcript[transcript.event == 'offer completed']\n    transaction_transcript = transcript[transcript.event == 'transaction']\n\n    # extract offer id and amount from value\n    offer_received_transcript['offer_id'] = offer_received_transcript.value.map(lambda x: x['offer id'])\n    offer_viewed_transcript['offer_id'] = offer_viewed_transcript.value.map(lambda x: x['offer id'])\n    offer_completed_transcript['offer_id'] = offer_completed_transcript.value.map(lambda x: x['offer_id'])\n    transaction_transcript['amount'] = transaction_transcript.value.map(lambda x: x['amount'])\n\n    return transaction_transcript, offer_received_transcript, offer_viewed_transcript, offer_completed_transcript\n\ndef preprocess(portfolio, transcript):\n\n    # process portfolio, transcript\n    portfolio = process_portfolio(portfolio)\n    transaction_transcript, offer_received_transcript, offer_viewed_transcript, offer_completed_transcript = process_transcript(transcript)\n\n    # create offer deadline\n    offer_received_transcript = offer_received_transcript.merge(\n        portfolio[['id', 'duration']], how='left', left_on='offer_id', right_on='id'\n        ).drop(['id','value'], axis=1)\n    offer_received_transcript[\"offer_deadline\"] = offer_received_transcript[\"time\"] + 24 * offer_received_transcript[\"duration\"]\n\n    # add offer id to transactions which related with offer\n    transaction_transcript = transaction_transcript.drop('value', axis=1)\n    query1 = '''\n        SELECT\n            L.person,\n            L.time,\n            L.amount,\n            R.time AS offer_received_at,\n            R.offer_id,\n            R.offer_deadline\n        FROM\n            transaction_transcript AS L\n        LEFT JOIN\n            offer_received_transcript AS R\n        ON L.person = R.person\n        AND L.time BETWEEN R.time AND R.offer_deadline\n    '''\n    transaction_transcript = ps.sqldf(query1, locals())\n\n    # add when the offer viewed\n    offer_viewed_transcript = offer_viewed_transcript[['person', 'time', 'offer_id']]\n    offer_viewed_transcript = offer_viewed_transcript.rename(columns={'time': 'offer_viewed_at'})\n    transaction_transcript = transaction_transcript.merge(offer_viewed_transcript, how='left', on=['person', 'offer_id'])\n\n    # create the flag which show the transactions were influenced by the offer\n    query2 = '''\n        SELECT\n            *,\n            CASE\n                WHEN offer_id IS NULL THEN 0\n                WHEN offer_viewed_at IS NULL THEN 0\n                WHEN time < offer_viewed_at THEN 0\n                ELSE 1\n            END AS is_influenced_by_offer\n        FROM\n            transaction_transcript\n    '''\n    transaction_transcript = ps.sqldf(query2, locals())\n\n    return transaction_transcript","8afdee3c":"def create_feature(transaction, portfolio, profile):\n\n    # split data to offer related and not related\n    offer_related_transaction = transaction[transaction.is_influenced_by_offer == 1]\n    offer_not_related_transaction = transaction[transaction.is_influenced_by_offer == 0]\n\n    # created target valuable\n    offer_related_transaction = offer_related_transaction.groupby(['person', 'offer_id'])['amount'].sum().reset_index()\n\n    # merge other data and add features\n    offer_related_transaction = offer_related_transaction.merge(profile, how='left', left_on='person', right_on='id').drop('id', axis=1)\n    offer_related_transaction = offer_related_transaction.merge(portfolio, how='left', left_on='offer_id', right_on='id').drop('id', axis=1)\n\n    # create feature from offer not related transactions\n    offer_not_related_transaction = offer_not_related_transaction[['person', 'amount']]\n    aggfunc = {\n        'amount': ['min', 'max', 'sum', 'mean', 'count', 'std', 'median']\n    }\n    offer_not_related_transaction = offer_not_related_transaction.groupby('person')['amount'].agg(aggfunc).reset_index()\n    offer_not_related_transaction.columns = ['_'.join(col) for col in offer_not_related_transaction.columns]\n    offer_related_transaction = offer_related_transaction.merge(offer_not_related_transaction, how='left', left_on='person', right_on='person_').drop('person_', axis=1)\n    offer_related_transaction = offer_related_transaction.fillna(0)\n\n    # drop income, gender, age is null users\n    offer_related_transaction = offer_related_transaction[offer_related_transaction.age != 118]\n\n    # create other feature\n    offer_related_transaction['became_member_on'] = pd.to_datetime(offer_related_transaction.became_member_on.map(lambda x: str(x)), format='%Y%m%d')\n    offer_related_transaction['registar_year'] = offer_related_transaction['became_member_on'].dt.year\n    #offer_related_transaction['registar_month'] = offer_related_transaction['became_member_on'].dt.month\n    offer_related_transaction['scaled_difficulty'] = offer_related_transaction.difficulty \/ offer_related_transaction.duration\n    offer_related_transaction['age_group'] = np.floor(offer_related_transaction.age\/10) *10\n    offer_related_transaction['buy_rate'] = offer_related_transaction.amount_sum \/ offer_related_transaction.income\n    offer_related_transaction['reward_inpact'] = offer_related_transaction.reward \/ offer_related_transaction.income\n    offer_related_transaction['income_growth_potential'] = offer_related_transaction.income \/ offer_related_transaction.age\n    offer_related_transaction['income_high'] = offer_related_transaction.income.map(lambda x: 1 if x > 75000 else 0)\n    offer_related_transaction['discount_ratio'] = offer_related_transaction.reward \/ offer_related_transaction.difficulty\n    offer_related_transaction.discount_ratio = offer_related_transaction.discount_ratio.fillna(0)\n\n    # get dummy valuables\n    offer_related_transaction = pd.get_dummies(offer_related_transaction, columns=['gender', 'registar_year'])\n\n    # drop not needed columns\n    offer_related_transaction = offer_related_transaction.drop([\n        'became_member_on',\n        'channels'], axis=1\n    )\n\n    return offer_related_transaction","4f1e76d1":"ls ..\/input\/","96d7a0f2":"portfolio = pd.read_json('..\/input\/portfolio.json', orient='records', lines=True)\nprofile = pd.read_json('..\/input\/profile.json', orient='records', lines=True)\ntranscript = pd.read_json('..\/input\/transcript.json', orient='records', lines=True)","0075b2c5":"# process data\ncleaned_data = preprocess(portfolio, transcript)\ncleaned_data = create_feature(cleaned_data, portfolio, profile)\ncleaned_data.head()","718b1a1c":"# check the shape\ncleaned_data.shape","f7ca98e5":"plt.figure(figsize=(15, 6))\nsns.distplot(cleaned_data.income, kde = True)\nplt.show()","88b6c7d0":"plt.figure(figsize=(15, 6))\nsns.scatterplot(x='income', y='amount', data=cleaned_data, alpha=.25)\nplt.show()","0f65d5c4":"cleaned_data['log_amount'] = np.log(cleaned_data.amount)","22f8ab41":"plt.figure(figsize=(15, 6))\nsns.scatterplot(x='income', y='log_amount', data=cleaned_data, alpha=.25)\nplt.show()","57ff5c41":"plt.figure(figsize=(15, 6))\nsns.scatterplot(x='income', y='log_amount', data=cleaned_data, alpha=.5)\nplt.xlim((70000, 80000))\nplt.show()","b74a5add":"plt.figure(figsize=(15, 6))\nsns.scatterplot(x='income', y='log_amount', data=cleaned_data, alpha=.25, hue='offer_type')\nplt.show()","8c32053e":"for col in ['amount_sum', 'amount_mean', 'amount_min', 'amount_std']:\n    col_name = 'log_' + col\n    cleaned_data[col_name] = np.log(cleaned_data[col])","0ac3c19a":"plt.figure(figsize=(15, 8))\nfor i, x in enumerate(['log_amount_sum', 'log_amount_mean', 'log_amount_min', 'log_amount_std']):\n    plt.subplot(2, 2, i+1)\n    sns.scatterplot(x=x, y='log_amount', data=cleaned_data, alpha=.25, hue='offer_type')\nplt.show()","2ba9b577":"# drop columns created to visualize\ncleaned_data = cleaned_data.drop(['log_amount_sum', 'log_amount_mean', 'log_amount_min', 'log_amount_std'], axis=1)","a750d66e":"# split data for each offer type\ndata_informational = cleaned_data[cleaned_data.offer_type == 'informational']\ndata_bogo = cleaned_data[cleaned_data.offer_type == 'bogo']\ndata_discount = cleaned_data[cleaned_data.offer_type == 'discount']\n\nprint('informational data length: ', len(data_informational))\nprint('bogo data length: ', len(data_bogo))\nprint('discount data length: ', len(data_discount))","72c78dc3":"def cross_validation(data):\n    model = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=0)\n    X = data.drop([\n        'person', \n        'offer_id', \n        'amount',\n        'log_amount',\n        'offer_type'\n    ], axis=1)\n\n    y = data['log_amount']\n    scores = cross_val_score(model, X, y, cv=5)\n\n    return scores","d2b747a0":"for data, offer_type in zip([data_bogo, data_discount, data_informational], ['bogo', 'discount', 'informational']):\n    scores = cross_validation(data)\n    print('{} mean score: {}'.format(offer_type, scores.mean()))\n    print('{} scores: {} \\n'.format(offer_type, scores))","f2a6bfe8":"def model_predict(data):\n    model = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=0)\n    X = data.drop([\n        'person', \n        'offer_id', \n        'amount',\n        'log_amount',\n        'offer_type'\n    ], axis=1)\n\n    y = data['log_amount'].values\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n    model.fit(X_train, y_train)\n    \n    y_pred_test = model.predict(X_test)\n    y_pred_train = model.predict(X_train)\n    \n    return y_test, y_pred_test, y_train, y_pred_train","e0d026c7":"plt.figure(figsize=(17, 5))\nfor i, data in enumerate([data_bogo, data_discount, data_informational]): \n    y_true, y_pred, _, _ = model_predict(data)  \n    plt.subplot(1, 3, i+1)\n    plt.plot((-2, 7), (-2, 7), color='gray')\n    sns.scatterplot(x=y_pred, y=y_true, alpha=.5)\n    plt.xlabel('y_pred')\n    plt.ylabel('y_true')\nplt.show()","881fa9f8":"def stacking(data):\n    y_test, y_pred_test, y_train, y_pred_train = model_predict(data)\n    model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=0)\n    input_train = np.stack([y_pred_train, y_pred_train**2]).T\n    input_test = np.stack([y_pred_test, y_pred_test**2]).T\n    model.fit(input_train, y_train)\n    y_pred_stack = model.predict(input_test)\n    return y_test, y_pred_stack","846101ad":"plt.figure(figsize=(17, 5))\nfor i, data in enumerate([data_bogo, data_discount, data_informational]): \n    y_true, y_pred = stacking(data)  \n    plt.subplot(1, 3, i+1)\n    plt.plot((-2, 7), (-2, 7), color='gray')\n    sns.scatterplot(x=y_pred, y=y_true, alpha=.5)\n    plt.xlabel('y_pred')\n    plt.ylabel('y_true')\nplt.show()","684e9be1":"for data, offer_type in zip([data_bogo, data_discount, data_informational], ['bogo', 'discount', 'informational']):\n    y_true, y_pred = stacking(data)\n    score = r2_score(y_true, y_pred)\n    print('{} test score: {}'.format(offer_type, score))","4997e899":"def ensembling(model1, model2, data):\n    X = data.drop(['person', 'offer_id', 'amount', 'log_amount', 'offer_type'], axis=1)\n    y = data['log_amount'].values\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n    \n    model1.fit(X_train, y_train)\n    model2.fit(X_train, y_train)\n    \n    y_pred_test = (model1.predict(X_test) + model2.predict(X_test)) \/ 2\n    y_pred_train = (model1.predict(X_train) + model2.predict(X_train)) \/ 2\n    \n    return y_test, y_pred_test, y_train, y_pred_train","16fbf0f2":"for data, offer_type in zip([data_bogo, data_discount, data_informational], ['bogo', 'discount', 'informational']):\n    model1 = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=100)\n    model2 = GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=0, learning_rate=0.1)\n    y_test, y_pred_test, y_train, y_pred_train = ensembling(model1, model2, data)\n    score_train = r2_score(y_train, y_pred_train)\n    score_test = r2_score(y_test, y_pred_test)\n    \n    print('{} train score: {}'.format(offer_type, score_train))\n    print('{} test score: {} \\n'.format(offer_type, score_test))","429aeb57":"# define param grid\nrf_param_grid = {\n    'max_depth' : [5, 8, 10],\n    'min_samples_split' : [2, 5, 10]\n}\n\ngb_param_grid = {\n    'learning_rate' : [0.01, 0.05, 0.1],\n    'min_samples_split' : [2, 5, 10] \n}","99449a83":"# apply grid search\nmodels = {}\nfor offer_type, data in zip(['bogo', 'discount', 'informational'], [data_bogo, data_discount, data_informational]):\n    X = data.drop(['person', 'offer_id', 'amount', 'log_amount', 'offer_type'], axis=1)\n    y = data['log_amount'].values\n    \n    rf_grid_search = GridSearchCV(RandomForestRegressor(n_estimators=100), rf_param_grid, cv=5)\n    gb_grid_search = GridSearchCV(GradientBoostingRegressor(n_estimators=100, max_depth=5), gb_param_grid, cv=5)\n    rf_grid_search.fit(X, y)\n    gb_grid_search.fit(X, y)\n    \n    models[offer_type] = [rf_grid_search.best_estimator_, gb_grid_search.best_estimator_]\n    \nmodels","4c64adc1":"# eval performance\nfor data, offer_type in zip([data_bogo, data_discount, data_informational], ['bogo', 'discount', 'informational']):\n    model1 = models[offer_type][0]\n    model2 = models[offer_type][1]\n    y_test, y_pred_test, y_train, y_pred_train = ensembling(model1, model2, data)\n    score_train = r2_score(y_train, y_pred_train)\n    score_test = r2_score(y_test, y_pred_test)\n    \n    print('{} train score: {}'.format(offer_type, score_train))\n    print('{} test score: {} \\n'.format(offer_type, score_test))","5c7b892a":"cleaned_data.head()","b48a2a38":"# create dataframe for predict\nimport_data = cleaned_data.drop(['offer_id', 'difficulty', 'duration', 'offer_type', 'reward', 'num_channel', \n                                 'channel_web', 'channel_email', 'channel_mobile', 'channel_social', 'amount',\n                                 'log_amount', 'reward_inpact', 'discount_ratio', 'scaled_difficulty'], axis=1).drop_duplicates()\n\nimport_data['key'] = 1\nportfolio['key'] = 1\nimport_data = import_data.merge(portfolio.drop('channels', axis=1), how='left', on='key')\nimport_data['reward_inpact'] = import_data.reward \/ import_data.income\nimport_data['discount_ratio'] = import_data.reward \/ import_data.difficulty\nimport_data['scaled_difficulty'] = import_data.difficulty \/ import_data.duration\nimport_data = import_data.fillna(0)\ndel import_data['key']\nimport_data.head()","7fd67713":"# make predictions for each offer\npredictions = pd.DataFrame()\nfor _, row in import_data.iterrows():\n    person = row.person\n    offer_type = row.offer_type\n    offer_id = row.id\n    row = row.drop(['person', 'id', 'offer_type']).values.reshape(1, -1)\n    model1 = models[offer_type][0]\n    model2 = models[offer_type][1]\n    prediction = (model1.predict(row) + model2.predict(row)) \/ 2\n\n    new_row = pd.DataFrame([[person, offer_id, offer_type, prediction]], columns=['person', 'offer_id', 'offer_type', 'predicted_log_amount'])\n    predictions = predictions.append(new_row)\n    \npredictions = predictions.reset_index().drop('index', axis=1)\npredictions['predicted_log_amount'] = predictions.predicted_log_amount.map(lambda x: x[0])","c0e45dd7":"# culcurate max amount for each person (after predicted)\noptimized_amount = predictions.groupby('person')['predicted_log_amount'].max().reset_index()\noptimized_amount['predicted_amount'] = np.exp(optimized_amount.predicted_log_amount)\noptimized_amount = optimized_amount.drop_duplicates()\noptimized_amount.tail()","9053bcbb":"# culcurate max amount for each person (before predicted)\nbefore_optimized_amount = cleaned_data.groupby('person')['amount'].max().reset_index()\nbefore_optimized_amount.tail()","f5ddf2de":"# culcurate total difference\njoin_result = optimized_amount.merge(before_optimized_amount, on='person')\nsum(join_result.predicted_amount - join_result.amount)","95a15902":"# culcurate ratio of predicted amount is larger than actual amount\nsum(join_result.predicted_amount > join_result.amount) \/ len(join_result)","da3fce18":"## Data Cleaning\n\nIn this part, I will clean data and create some features. When finish the process, the data will look like below.\n\nperson | offer_id | offer_related_purchase($) | other features...\n-- | -- | -- | --\n1 | A | 10 | ...\n1 | B | 20 | ...\n2 | A | 30 | ...\n2 | C | 40 | ...\n... | ... | ... | ...\n\nThe rows are unique in combination of person and offer_id, and the target valuable is the amount of offer related purchase.  \nI processed the data paying attention to below parts.\n\n* There is the case a user received offer but never open the offer and buy products. It should not be thought offer related purchase.\n* There is the case a user didn't receive offer but purchase product. It also should not be thought offer related purchase.\n* Offer has expiration date, So if there is purchases which over offer deadline, It also should not be thought offer related purchase.\n\nBecause I want to prevent the notebook becoming too complicated, I divided the cleaning of the data in .py file.\nSo, if you are interested in the processing details, please see [preprocess.py](https:\/\/github.com\/shyaginuma\/udacity_capstone_starbucks\/blob\/master\/py\/preprocess.py) and [feature_engineering.py](https:\/\/github.com\/shyaginuma\/udacity_capstone_starbucks\/blob\/master\/py\/feature_engineering.py).","34470ea1":"It looks like the model has improved a bit. Let's culcurate R2 score.","ed68a1b5":"Model performances can be said little good.\nIn the next section, I will try to improve the models.","4ca56cf7":"### Parameter Tuning\n\nParameter tuning is almost last part of building machine learning models. I think the sklearn's [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) is often used in many case, I will use it to models for each offer type. This step spent many many time.","36a85301":"The bogo and discount model improved little, but the informational model seems to be overfit.  \nThere is other method such as change weights of each model or use theree or more models or use several type of argorithm.  \nBut the more increasing ensemble complex, the more calcuration time is needed. Because this is not a competition, I will use two model ensembling. ","1c52f705":"It has no relation with offer_type. I can't understand why the boundary exist, but we can use this information when building machine learning model.   \nIf I use tree-based model, the information will be useful because if income is higer than \\\\$75000, the target values will be higher than 2.","daeeaac3":"### Ensembling\n\nensembling is also the method of often using in the competition. Ensembling build some model and predict target values on the each model, then take a weighted average of those predicted values. [more detail](https:\/\/towardsdatascience.com\/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f)","575310c8":"The boundary lies near \\\\$75000.  \nWhy this boundary exist? It may be relate with `offer_type`. Let's cross the dimension.","a73a510d":"## Conclusion\n\nI tried to maxmize the offer related purchase amount through building machine learning model to predict amount.  \nAs a result, I can't achieve this but I discovered some hints which may be key to solve this problem.\n\n**1. The person who purchases when offer doesn't send purchases when offer sends.**  \nThe person who usually uses Starbucks purchases also offer sended situation. These customer are thought as Fixed customers of Starbucks. Their sales are very important, so we should monitor their sales monthly or weekly so that we can notice when the change occured.\n\n**2. The varience of the target valuable is very large.**  \nBecause of this, it is very hard to explain by machine learning model for this problem. The probability model may be efficient for this case because the model can predict estimated Interval. In other words, the prediction of Machine learning model is point estimation for example, \"the prediction value is 90.\" but probability model can predict values for example,  \"The prediction value are 80~95 with 90%\".","97eded8a":"From above density plot, we can see almost person has \\\\$40000 ~ \\\\$80000 income and some person get over \\\\$100000.","a5b72b2c":"This figure is hard to see because almost `amount` has small values and it is too dense in small value area.  \nIn such cases let's try logarithmic conversion. It will improve the plot.","213709b4":"## Modeling\n\nIn this part, I will building the machine learning model. I will use [Random Forest Regressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html) because through the EDA step, the target has non-linear relation with each features and may not be able to explain simple model. Random Forest Regressor can treat many small decision tree model so that it can adopt complex data and non-linear relation.\n\nI will split model with each offer type. This is because I prevent the model overfit a offer type.   \nFor example the informational offer don't have `discount`, so the offer related purchases seem to be no relation with `discount` in informationa offer.  \nBut, the discount offer has `discount` value, and this may be related with the offer related purchases. If we build model with all data, we will ignore things like this.","09612dc8":"## Project Overview\n\nHere, I will analyze the subject of offer optimization. What is **offer**? The offer means such as coupon or information about new product. We are provided the data of customer purchases and offer related activity (receive offer, open offer, complete offer) on the Starbucks rewards mobile app.   \nBelow, I will show the information of provided data.\n\n* portfolio.json - containing offer ids and meta data about each offer (duration, type, etc.)\n* profile.json - demographic data for each customer\n* transcript.json - records for transactions, offers received, offers viewed, and offers completed (main data)\n\nIf you are interested more detail, please see [docs from udacity](https:\/\/github.com\/shyaginuma\/udacity_capstone_starbucks\/blob\/master\/docs). This directry contains detail information about overview and data.","3dd8b9fa":"### Income\n\nThe person's income seems to be an important information because it relate with purchasing ability.  \nLet's look at the distribution and corration with target.","50661df5":"First, let's see how well the model predicts target values.","89819507":"## Problem Statement \/ Metrics\n\nThe problem I will solve here is **Which offer should we send to each customer?**. I aim to maximize sales by solving this problem.  \nSpeaking more, if we can send offer which maxmize sales of customer, We can achieve that. So, I will try to predict purchases of customer when we sent each offer with machine learning model.  \nIt is important to note here that the offer has expiration date. So, data has purchases which is not related to offer and we need to process data.\n\nI will solve this problem for below steps.\n\n* Data Cleaning : clean, combine and aggregate provided data\n* EDA : grasp data charactoristics through visualizing and easy aggregate\n* Modeling : build machine learning model and I will use random forest regressor\n* Evaluating : evaluate ML model's performance and try to improve by some method\n* Deploy : try to show impact with comparing original data with when ML model used\n\nI will use [R2 score](https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination) to measure performance of a model.  \nIn the competition, the mean squared error (mse) or mean absolute error (mae) are often used because in the competition, participants can compete by the values. But in this case I have to show model performance. MSE or MAE can show the size of loss but they don't have clear criterion, so I can't understand whether model is good with these metrics.\nR2 score takes values from -1 to 1 and this value shows how well the prediction fits the actual data.","d2eb3e91":"## Evaluating\n\nIn this part, I will try below things to improve model performances.\n\n* Model Stacking\n* Model Ensembling\n* Parameter Tuning","557ea1e7":"Ensembling is used model improvement, but it is used mainly to increase robustness of the predicted values.  \nLet's try.","e97d2195":"# Udacity Data Scientist Nanodegree Capstone Project - Starbucks","2647e7ee":"Because of almost targets are low but a little of targets are high, it may be can't enough fitting normal scale.  \nSo, I will apply log transformation to target and treat it as target same as when I visualize.","90174101":"Here, each model is optimized for each offer type data. And performance of each models are best in this notebook.  \nNow, It's ready to deploy.","a6e726ee":"From here, it looks like model working well, but there is also a part that model can't explain.","f1ac9247":"## Improvement\n\nI didn't try here, but it seems to be efficient the down sampling method and probability model in this case.\n\n**probability model**  \nI already explained above.\n\n**down sampling**  \nBecause the target valuable concentrated on the same section and It looks like the\u3000prediction of the model was pulled by that interval, if I do down sampling of the section, it seems to be well.","d3bb292b":"There are corrations with amout_sum, amount_mean, amount_min, but amount_std may not be corrated with target.  \nAnd from here, it looks like the target variance is large. It seems to be hard to explain the variance with simple model.","6780ce9e":"As a result, the R2 score became a little bad. I think this is because the model became too complex.  \nStacking is not effective in this case, next I will try **Ensembling** method.","70731b7d":"In the previous part, I decided model parameters with no evidences.  \nNext part, I try to optimize model parameters.","5b650421":"Surprisingly, the profit decrease after the prediction. I think this is because the model seems to tend to predict lower.  \nTo improve this, the down sampling may be efficient.","3e79af54":"## Deploy\n\nIn this step, I will predict offer related purchase amount for each offer with models which I built.  \nThen, I show the profit increase if sent optimized offer.","23ea6204":"Looks good, and we can realize there are something like a boundary near the \\\\$80000.  \nWhat's this? Let's close look at plot to find out where the boundaries are.","83bc54f6":"Now, Let's start analysis with import necessary library!!","3240b767":"## EDA\n\nIn this part, I will look the data distribution and the corration of some features and target valuable.","506b0ce2":"### Offer not related purchase amount\n\nIn the data cleaning step, I split data to offer related purchases and offer not related purchases, and I use offer related purchases to create target valuable, offer not related purchases to create features.\n\nOffer not related purchase amount seems to be related with target valuable because it show customer's purchase ability and royality.  \nLet's look at the corration with target.\n\nI created some features from offer not related purchases, and I will look at below features.\n\n* sum\n* mean\n* min\n* std\n\nI will apply log transformation to features same as target.","18534828":"### Stacking\n\nStacking is the method of often using in the competition.  \nThe charactoristics of Stacking is that use predicted result as a feature and predict again. [more detail](http:\/\/blog.kaggle.com\/2016\/12\/27\/a-kagglers-guide-to-model-stacking-in-practice\/)\n\nIn the above plot, it looks like it has a more steeper linear relationship than `y=x`. The Stacking method may work well, so let's try."}}