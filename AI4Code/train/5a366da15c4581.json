{"cell_type":{"5ba57e41":"code","792805ba":"code","9e0dc61c":"code","0e6b13a6":"code","7f319e77":"code","302dba01":"code","2a808321":"code","557521d1":"code","838ea2de":"code","d688de70":"code","98ecdb1a":"code","f96756d8":"code","0801ffe4":"code","1535e0ec":"code","da0f6339":"code","bf5f8736":"code","356e3d08":"code","7db4b470":"code","c0a43632":"markdown","28df5b88":"markdown","386b49ae":"markdown"},"source":{"5ba57e41":"import pandas as pd\nimport numpy as np\nimport keras\n\nimport spacy\n\nfrom collections import defaultdict\nfrom sklearn.metrics import log_loss\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","792805ba":"nlp = spacy.load('en_core_web_lg')","9e0dc61c":"class Graph():\n    def __init__(self):\n        \"\"\"\n        self.edges is a dict of all possible next nodes\n        e.g. {'X': ['A', 'B', 'C', 'E'], ...}\n        self.weights has all the weights between two nodes,\n        with the two nodes as a tuple as the key\n        e.g. {('X', 'A'): 7, ('X', 'B'): 2, ...}\n        \"\"\"\n        self.edges = defaultdict(list)\n        self.weights = {}\n    \n    def add_edge(self, from_node, to_node, weight, back_penalty=1):\n        # Note: assumes edges are bi-directional\n        self.edges[from_node].append(to_node)\n        self.edges[to_node].append(from_node)\n        self.weights[(from_node, to_node)] = weight\n        self.weights[(to_node, from_node)] = weight*back_penalty\n\ndef dijsktra(graph, initial, end):\n    # shortest paths is a dict of nodes\n    # whose value is a tuple of (previous node, weight)\n    shortest_paths = {initial: (None, 0)}\n    current_node = initial\n    visited = set()\n    \n    while current_node != end:\n        visited.add(current_node)\n        destinations = graph.edges[current_node]\n        weight_to_current_node = shortest_paths[current_node][1]\n\n        for next_node in destinations:\n            weight = graph.weights[(current_node, next_node)] + weight_to_current_node\n            if next_node not in shortest_paths:\n                shortest_paths[next_node] = (current_node, weight)\n            else:\n                current_shortest_weight = shortest_paths[next_node][1]\n                if current_shortest_weight > weight:\n                    shortest_paths[next_node] = (current_node, weight)\n        \n        next_destinations = {node: shortest_paths[node] for node in shortest_paths if node not in visited}\n        if not next_destinations:\n            raise Exception(\"Something is wrong\")\n        # next node is the destination with the lowest weight\n        current_node = min(next_destinations, key=lambda k: next_destinations[k][1])\n    \n    # Work back through destinations in shortest path\n    path = []\n    dist = 0\n    while current_node is not None:\n        path.append(current_node)\n        next_node = shortest_paths[current_node][0]\n        dist += shortest_paths[current_node][1]\n        current_node = next_node\n    # Reverse path\n    path = path[::-1]\n    return path, dist\n        ","0e6b13a6":"def get_rank(token):\n    \"\"\"Step up with token.head until it reaches the root. Returns with step number and root\"\"\"\n    i = 0\n    next_token = token\n    while(next_token!=next_token.head):\n        i+=1\n        next_token=next_token.head\n    return i, next_token\n\ndef child_count(token):\n    cc = 0\n    for child in token.children:\n        cc+=1\n    return cc","7f319e77":"def build_answers(data):\n    answers = []\n    for i in range(len(data)):\n        dataNext = data.loc[i]\n        Acoref = dataNext[\"A-coref\"]\n        Bcoref = dataNext[\"B-coref\"]\n        answerNext = [int(Acoref), int(Bcoref), 1-int(Acoref or Bcoref)]\n        answers.append(answerNext)\n    return np.vstack(answers)","302dba01":"def build_features(data):\n    \"\"\"Generates features from input data\"\"\"\n    features = []\n    sum_good = 0\n    for i in range(0,len(data)):\n        fi = []\n        dataNext = data.loc[i]\n        text = dataNext[\"Text\"]\n        #print(visualise(dataNext))\n        doc=nlp(text)\n        Aoff = dataNext[\"A-offset\"]\n        Boff = dataNext[\"B-offset\"]\n        Poff = dataNext[\"Pronoun-offset\"]\n        lth = len(text)\n        \n        for token in doc:\n            if(token.idx==Aoff):\n                Atoken = token\n            if(token.idx==Boff):\n                Btoken = token\n            if(token.idx==Poff):\n                Ptoken=token\n        Arank, Aroot = get_rank(Atoken)\n        Brank, Broot = get_rank(Btoken)\n        Prank, Proot = get_rank(Ptoken)\n        \n        graph = Graph()\n        \n        for token in doc:\n            graph.add_edge(token, token.head, 1, 4)\n        \n        sent_root = []\n        for sent in doc.sents:\n            sent_root.append(sent.root)\n        for j in range(len(sent_root)-1):\n            graph.add_edge(sent_root[j], sent_root[j+1],1, 4)\n        try:\n            _, Alen = dijsktra(graph, Atoken, Ptoken)\n        except:\n            Alen = 300\n        try:\n            _, Blen = dijsktra(graph, Btoken, Ptoken)\n        except:\n            Blen = 300\n        \n        \n        sent_num = len(sent_root)\n        for i in range(len(sent_root)):\n            if Aroot == sent_root[i]:\n                Atop = i\n            if Broot == sent_root[i]:\n                Btop = i\n            if Proot == sent_root[i]:\n                Ptop = i\n        \n        fi.append(Aoff\/lth)#0\n        fi.append(Boff\/lth)#1\n        fi.append(Poff\/lth)#2\n\n        fi.append(1.0*Atop\/sent_num)#3\n        fi.append(1.0*Btop\/sent_num)#4\n        fi.append(1.0*Ptop\/sent_num)#5\n\n        fi.append(Arank\/10)#6\n        fi.append(Brank\/10)#7\n        fi.append(Prank\/10)#8\n        \n        #fi.append(Atoken.similarity(Ptoken))#9\n        #fi.append(Btoken.similarity(Ptoken))#10\n        \n        #fi.append(Alen\/300)#9\n        #fi.append(Blen\/300)#10\n        \n        #fi.append(child_count(Aroot))#11\n        #fi.append(child_count(Broot))#12\n        #fi.append(child_count(Proot))#13\n        \n        features.append(fi)\n    return np.vstack(features)\n\ndef swap_raws(data, i, j):\n    \"\"\"Swap the ith and jth column of the data\"\"\"\n    new_data = np.copy(data)\n    temp = np.copy(new_data[:, i])\n    new_data[:,i] = new_data[:,j]\n    new_data[:,j] = temp\n    return new_data","2a808321":"def build_model(featnum):\n    model = keras.models.Sequential()\n\n    model.add(keras.layers.Dense(units=64, activation='relu', input_dim=featnum))\n    model.add(keras.layers.Dense(units=64, activation='relu'))\n    model.add(keras.layers.Dense(units=64, activation='relu'))\n\n    \n    model.add(keras.layers.Dense(units=3, activation='softmax'))\n    sgd = keras.optimizers.SGD(lr=0.02, decay=1e-4, momentum=0.7, nesterov=True)\n    model.compile(loss='categorical_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n    return model","557521d1":"!wget https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-development.tsv\n!wget https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-validation.tsv\n!wget https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-test.tsv\n!ls","838ea2de":"test_data = pd.read_csv('gap-development.tsv', sep='\\t')\nval_data = pd.read_csv('gap-validation.tsv', sep='\\t')\ndev_data = pd.read_csv('gap-test.tsv', sep='\\t')","d688de70":"dev_answ = build_answers(dev_data)\nval_answ = build_answers(val_data)\ntest_answ = build_answers(test_data)","98ecdb1a":"print(\"Feature building started \", time.ctime())\ndev_feat = build_features(dev_data)\nprint(\"Developement ready\", time.ctime())\nval_feat = build_features(val_data)\nprint(\"Validation ready\", time.ctime())\ntest_feat = build_features(test_data)\nprint(\"Test ready\", time.ctime())","f96756d8":"#Flip A and B in dev\ndev_feat_p = swap_raws(dev_feat, 0, 1)\ndev_feat_p = swap_raws(dev_feat_p, 3, 4)\ndev_feat_p = swap_raws(dev_feat_p, 6, 7)\n#dev_feat_p = swap_raws(dev_feat_p, 9, 10)\n#dev_feat_p = swap_raws(dev_feat_p, 11, 12)\n\ndev_feat = np.concatenate((dev_feat, dev_feat_p),0)","0801ffe4":"dev_answ_p = swap_raws(dev_answ,0,1)\ndev_answ = np.concatenate((dev_answ, dev_answ_p),0)","1535e0ec":"model = build_model(np.shape(dev_feat)[1])\nearlyStopping = keras.callbacks.EarlyStopping(patience=100, restore_best_weights=True)\nhistory = model.fit(dev_feat, dev_answ,\n                    validation_data = (val_feat, val_answ),\n                    epochs=100000, batch_size=2000, verbose=0,\n                    callbacks=[earlyStopping],\n                    shuffle=True)","da0f6339":"plt.plot(history.history['loss'], label=\"loss\")\nplt.plot(history.history['val_loss'], label=\"val_loss\")\nplt.legend()\nplt.show()","bf5f8736":"test_predict = model.predict(test_feat)","356e3d08":"print(\"Test score:\", log_loss(test_answ,test_predict))","7db4b470":"submission = pd.read_csv(\"..\/input\/sample_submission_stage_1.csv\", index_col = \"ID\")\nsubmission[\"A\"]=test_predict[:,0]\nsubmission[\"B\"]=test_predict[:,1]\nsubmission[\"NEITHER\"]=test_predict[:,2]\nsubmission.to_csv(\"submission_11f.csv\")","c0a43632":"See for Dijkstra: https:\/\/gist.github.com\/econchick\/4666413","28df5b88":"# Extracting own features using spacy\n\nRoot: recursive spacy token.head until it is itself\nRank: steps until root\n\n - Offset \/ text length (A, B, P)\n - Root position in sentence root list (A, B, P)\n - Rank (A, B, P)\n - Distance (AP, BP) , weighted dijkstra\n - ( Spacy token similarity (AP, BP))\n \nScore: ~0.70\n\nAlso: doubling the input data by swaping A and B","386b49ae":"Doubling training data by changing A to B and B to A"}}