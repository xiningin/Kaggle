{"cell_type":{"0a5a10ff":"code","fa9320cf":"code","7e25bf40":"code","017a0416":"code","e5b61403":"code","fa08f38d":"code","52d70aae":"code","12afe01a":"code","2f54a31b":"code","5896cd33":"code","4b97ae5e":"code","b5a6dca3":"code","2b325f95":"code","aca84a27":"code","3bab6636":"code","0ca2611b":"code","9f281f75":"code","e4fa40f4":"code","73945d39":"code","c936f02b":"code","4d9557fb":"code","66f3830c":"code","d08beade":"code","1b3ba1bb":"code","3fc3fc2a":"code","da838a55":"code","716c75d1":"code","f55badf1":"code","a6f4c526":"code","91c72c2f":"code","04d2fec7":"code","c3f984b4":"code","cdcc557a":"code","a7743eac":"code","bb533802":"code","e2bfd50b":"code","d6b0a611":"code","6f76c662":"code","777cdb8c":"markdown","c57f7d95":"markdown"},"source":{"0a5a10ff":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa9320cf":"df = pd.read_csv('\/kaggle\/input\/for-simple-exercises-time-series-forecasting\/Alcohol_Sales.csv', index_col = 'DATE', parse_dates=True)\ndf.index.freq = 'MS'\ndf.head()","7e25bf40":"# Renaming the only column present to Sales\ndf.columns = ['Sales']","017a0416":"df.plot(figsize=(12,8))","e5b61403":"from statsmodels.tsa.seasonal import seasonal_decompose","fa08f38d":"# Decompose the Sales data\nresults = seasonal_decompose(df['Sales'])","52d70aae":"results.plot();","12afe01a":"results.seasonal.plot(figsize=(10,6))","2f54a31b":"len(df)","5896cd33":"# We'll predict sales for next one year, as it is monthly data, so we'll have test size of 12 and train size of len(df) - 12\n# Splitting the data into train and test\n\ntrain = df.iloc[:313]\ntest = df.iloc[313:]","4b97ae5e":"len(test)","b5a6dca3":"# We need to scale or normalize the data to pass it into RNN (recurrent neural network)\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n","2b325f95":"# fit the training data\nscaler.fit(train) # finds the max value in train data","aca84a27":"# transform the training data\nscaled_train = scaler.transform(train)\nscaled_test = scaler.transform(test)","3bab6636":"from keras.preprocessing.sequence import TimeseriesGenerator","0ca2611b":"# Create TimeseriesGenerator object for training dataset (define how long the trainig sequence to be (n_input) and the very next point to predict)\nn_input = 12\nn_features = 1 # number of columns in your dataset\n\ntrain_generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)","9f281f75":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM","e4fa40f4":"model = Sequential()\n\nmodel.add(LSTM(150, activation='relu', input_shape=(n_input, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')","73945d39":"model.summary()","c936f02b":"model.fit_generator(train_generator, epochs=25)","4d9557fb":"model.history.history.keys()","66f3830c":"# plotting loss versus their range of values\nmyloss = model.history.history['loss']\nplt.plot(range(len(myloss)), myloss)","d08beade":"# For predicting the first data in test set we need the last 12 data points from the test dataset\nfirst_eval_batch = scaled_train[-12:]","1b3ba1bb":"first_eval_batch","3fc3fc2a":"first_eval_batch.shape","da838a55":"# reshaping first_eval_batch as the input which is being passed to TimeseriesGenerator must be 3D (batch_size, n_input, n_features) \nfirst_eval_batch = first_eval_batch.reshape((1, n_input, n_features))","716c75d1":"model.predict(first_eval_batch)","f55badf1":"# holding my predictions\ntest_predictions = [] \n\n# last n_input points from training set\nfirst_eval_batch = scaled_train[-n_input:]\n# Reshape this to the format RNN wants (same as TimeseriesGenerator)\ncurrent_batch = first_eval_batch.reshape((1, n_input, n_features))","a6f4c526":"# how far into future will I forecast\nfor i in range(len(test)):\n    # one timestep ahead of historical 12 points\n    current_pred = model.predict(current_batch)[0]\n    # store that prediction\n    test_predictions.append(current_pred)\n    # update the current batch to include prediction\n    current_batch = np.append(current_batch[:, 1:, :], [[current_pred]], axis=1)","91c72c2f":"len(test_predictions)","04d2fec7":"# Performing reverse scaling on the test predictions\ntrue_predictions = scaler.inverse_transform(test_predictions)","c3f984b4":"true_predictions","cdcc557a":"# Add prediction column to dataset\ntest['Predictions'] = true_predictions","a7743eac":"# Plotting original sales with predicted sales\ntest.plot(figsize=(12,8))","bb533802":"# Saving the trained model for future reference\nmodel.save('timeseriesmodel.h5')","e2bfd50b":"from keras.models import load_model","d6b0a611":"# loading the model\ntimeseries_rnn_model = load_model('timeseriesmodel.h5')","6f76c662":"timeseries_rnn_model.summary()","777cdb8c":"In the above n_feature should be decided based on seasonality, if seasonality is based over the year, then as it is monthly dataset, so, n_feature should be at least 12, in order for RNN to pickup at least seasonality","c57f7d95":"#### Forecast using RNN model"}}