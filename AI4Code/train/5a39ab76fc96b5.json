{"cell_type":{"d5077396":"code","9a057f22":"code","ad7efa00":"code","976d9e9f":"code","31081b97":"code","c8dadfc1":"code","4d0b55a9":"code","0cec0113":"code","e93fec8d":"code","2222e925":"code","1acdca91":"code","261443e0":"code","eb9a6c7f":"code","00a29161":"code","9d81633c":"code","31c75775":"code","1bda3e08":"code","0880d81d":"code","eff69dca":"markdown","bcb02065":"markdown","cb3f5f1b":"markdown","10c06f8a":"markdown","b11f01f5":"markdown","e0105a27":"markdown","58448b9c":"markdown","378c5138":"markdown","46a0ed3e":"markdown","ae86de2f":"markdown","aba3bef6":"markdown"},"source":{"d5077396":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\nimport gc\n\nfrom PIL import Image\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt\n\nDIR_INPUT = '\/kaggle\/input\/global-wheat-detection'\nDIR_TRAIN = f'{DIR_INPUT}\/train'\nDIR_TEST = f'{DIR_INPUT}\/test'","9a057f22":"#Install + import Weights and Biases\n!pip install --upgrade wandb\nimport wandb","ad7efa00":"train_df = pd.read_csv(f'{DIR_INPUT}\/train.csv')\ntrain_df.shape\ntrain_df.head()","976d9e9f":"#Extract the bbox column and explode it into x,y,w,h\ntrain_df['x'] = -1\ntrain_df['y'] = -1\ntrain_df['w'] = -1\ntrain_df['h'] = -1\n\ndef expand_bbox(x):\n    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n    if len(r) == 0:\n        r = [-1, -1, -1, -1]\n    return r\n\ntrain_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\ntrain_df.drop(columns=['bbox'], inplace=True)\ntrain_df['x'] = train_df['x'].astype(np.float)\ntrain_df['y'] = train_df['y'].astype(np.float)\ntrain_df['w'] = train_df['w'].astype(np.float)\ntrain_df['h'] = train_df['h'].astype(np.float)","31081b97":"image_ids = train_df['image_id'].unique()\n#Original: 3373 total images divided into 2708 for training, 665 for validation\n#valid_ids = image_ids[-665:]\n#train_ids = image_ids[:-665]\n\n#Select smaller set: 677 for training, 166 for validation (1\/4 size original) to avoid any CUDA out of memory issues\ntrain_ids = image_ids[:677]\nvalid_ids = image_ids[677:677+166]","c8dadfc1":"print(image_ids.shape)\nprint(valid_ids.shape)\nprint(train_ids.shape)","4d0b55a9":"#take the bounding boxes and put them into their respective category (train, valid) by id \nvalid_df = train_df[train_df['image_id'].isin(valid_ids)]\ntrain_df = train_df[train_df['image_id'].isin(train_ids)]","0cec0113":"valid_df.shape, train_df.shape","e93fec8d":"class WheatDataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique() #all images\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n        \n        #load image and normalize image pixels to 0-1\n        image = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        \n        boxes = records[['x', 'y', 'w', 'h']].values\n        #turn each bounding box into format [x_start, y_start, x_end, y_end]\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        \n        #calculate areas of each bounding box\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n\n        # there is only one class\n        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels #all boxes are labelled as 1 (wheat head)\n        # target['masks'] = None\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        \n        #apply transformations to this image\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","2222e925":"# Albumentations (pretty much image data augmentation)\ndef get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n#Collate function\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n","1acdca91":"#IOU Metric Calculation\nfrom collections import namedtuple\nfrom typing import List, Union\n\nBox = namedtuple('Box', 'xmin ymin xmax ymax')\n\n\ndef calculate_iou(gt: List[Union[int, float]],\n                  pred: List[Union[int, float]],\n                  form: str = 'pascal_voc') -> float:\n    \"\"\"Calculates the IoU.\n    \n    Args:\n        gt: List[Union[int, float]] coordinates of the ground-truth box\n        pred: List[Union[int, float]] coordinates of the prdected box\n        form: str gt\/pred coordinates format\n            - pascal_voc: [xmin, ymin, xmax, ymax]\n            - coco: [xmin, ymin, w, h]\n    Returns:\n        IoU: float Intersection over union (0.0 <= iou <= 1.0)\n    \"\"\"\n    if form == 'coco':\n        bgt = Box(gt[0], gt[1], gt[0] + gt[2], gt[1] + gt[3])\n        bpr = Box(pred[0], pred[1], pred[0] + pred[2], pred[1] + pred[3])\n    else:\n        bgt = Box(gt[0], gt[1], gt[2], gt[3])\n        bpr = Box(pred[0], pred[1], pred[2], pred[3])\n        \n\n    overlap_area = 0.0\n    union_area = 0.0\n\n    # Calculate overlap area\n    dx = min(bgt.xmax, bpr.xmax) - max(bgt.xmin, bpr.xmin)\n    dy = min(bgt.ymax, bpr.ymax) - max(bgt.ymin, bpr.ymin)\n\n    if (dx > 0) and (dy > 0):\n        overlap_area = dx * dy\n\n    # Calculate union area\n    union_area = (\n            (bgt.xmax - bgt.xmin) * (bgt.ymax - bgt.ymin) +\n            (bpr.xmax - bpr.xmin) * (bpr.ymax - bpr.ymin) -\n            overlap_area\n    )\n\n    return overlap_area \/ union_area","261443e0":"#MAP Calculation\ndef find_best_match(gts, predd, threshold=0.5, form='pascal_voc'):\n    \"\"\"Returns the index of the 'best match' between the\n    ground-truth boxes and the prediction. The 'best match'\n    is the highest IoU. (0.0 IoUs are ignored).\n    \n    Args:\n        gts: Coordinates of the available ground-truth boxes\n        pred: Coordinates of the predicted box\n        threshold: Threshold\n        form: Format of the coordinates\n        \n    Return:\n        Index of the best match GT box (-1 if no match above threshold)\n    \"\"\"\n    best_match_iou = -np.inf\n    best_match_idx = -1\n    \n    for gt_idx, ggt in enumerate(gts):\n        iou = calculate_iou(ggt, predd, form=form)\n        \n        if iou < threshold:\n            continue\n        \n        if iou > best_match_iou:\n            best_match_iou = iou\n            best_match_idx = gt_idx\n\n    return best_match_idx\n\n\ndef calculate_precision(preds_sorted, gt_boxes, threshold=0.5, form='coco'):\n    \"\"\"Calculates precision per at one threshold.\n    \n    Args:\n        preds_sorted: \n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n\n    fn_boxes = []\n\n    for pred_idx, pred in enumerate(preds_sorted):\n        best_match_gt_idx = find_best_match(gt_boxes, pred, threshold=threshold, form=form)\n\n        if best_match_gt_idx >= 0:\n            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n            tp += 1\n\n            # Remove the matched GT box\n            gt_boxes = np.delete(gt_boxes, best_match_gt_idx, axis=0)\n\n        else:\n            # No match\n            # False positive: indicates a predicted box had no associated gt box.\n            fn += 1\n            fn_boxes.append(pred)\n\n    # False negative: indicates a gt box had no associated predicted box.\n    fp = len(gt_boxes)\n    precision = tp \/ (tp + fp + fn)\n    return precision, fn_boxes, gt_boxes\n\n\ndef calculate_image_precision(preds_sorted, gt_boxes, thresholds=(0.5), form='coco', debug=False):\n    \n    n_threshold = len(thresholds)\n    image_precision = 0.0\n    \n    for threshold in thresholds:\n        precision_at_threshold, _, _ = calculate_precision(preds_sorted,\n                                                           gt_boxes,\n                                                           threshold=threshold,\n                                                           form=form\n                                                          )\n        if debug:\n            print(\"@{0:.2f} = {1:.4f}\".format(threshold, precision_at_threshold))\n\n        image_precision += precision_at_threshold \/ n_threshold\n    \n    return image_precision","eb9a6c7f":"\n\ndef train(args, model, device, train_data_loader, optimizer, epoch, iteration):\n    print(\"Epoch: \", epoch)\n    \n    #Train Loop\n    for batch_idx, (images, targets, image_ids) in enumerate(train_data_loader):\n        \n        #Train and calculate loss\n        model.train()\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        loss_dict = model(images, targets)\n        \n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            #Log the train loss and iteration\n            wandb.log({\"train_loss\": loss_value, \"iteration_train\": iteration})\n        \n        iteration += 1 #iteration increases for every batch\n        \n    \n    return iteration #returns the updated new iteration\n            ","00a29161":"#[xmin, ymin, xmax, ymax] => [x, y, width, height]\ndef convert_boxes_format(boxes):\n    boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n    boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n    return boxes\n\nclass_id_to_label = {\n    0: \"target\",\n    1: \"wheat\"\n}\n\n#Test will run as validation and log bounding boxes with confidence\/prediction scores\ndef test(args, model, device, test_data_loader, epoch, iteration):\n    model.eval()\n    \n    epoch_precision_score = []\n    for batch_idx, (images, targets, image_ids) in enumerate(test_data_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        outputs = model(images)\n        \n        #Calculate Precision Score Per Batch\n        batch_precision_score = 0\n        for i, image in enumerate(images):\n            \n            #predicted bounding boxes\n            boxes = outputs[i]['boxes'].data.cpu().numpy()\n            boxes = convert_boxes_format(boxes)\n            \n            scores = outputs[i]['scores'].data.cpu().numpy()\n            \n            #ground truth bounding boxes\n            gt_boxes = targets[i][\"boxes\"].cpu().numpy()\n            gt_boxes = convert_boxes_format(gt_boxes)\n\n            image_id = image_ids[i]\n\n            # Sort highest confidence -> lowest confidence\n            preds_sorted_idx = np.argsort(scores)[::-1]\n            preds_sorted = boxes[preds_sorted_idx]\n\n            iou_thresholds = [x for x in np.arange(0.5, 0.76, 0.05)]\n            image_precision = calculate_image_precision(preds_sorted, gt_boxes,\n                                            thresholds=iou_thresholds,\n                                            form='coco', debug=True)\n            \n            print(\"Average Precision of image: {0:.4f}\".format(image_precision))\n            batch_precision_score +=  image_precision\n        \n        batch_precision_score = batch_precision_score \/ len(images) #average precision score for the batch\n        epoch_precision_score.append(batch_precision_score)\n        \n        if batch_idx % args.log_interval == 0:\n            #logs the precision score per batch and also the iteration\n            wandb.log({\"batch_score_validation\": batch_precision_score, \"iteration_validation\": iteration})\n        \n        #Log bounding boxes\n        if batch_idx % args.image_log_interval == 0:\n            \n            #Log 1 image with bounding boxes for this batch\n            for i, image in enumerate(images[:1]): \n\n                image_id = image_ids[i]\n                scores = outputs[i]['scores'].data.cpu().numpy().astype(np.float64)\n\n                #predicted bounding boxes\n                boxes = outputs[i]['boxes'].data.cpu().numpy().astype(np.float64)\n                predicted_boxes = []\n                for b_i, box in enumerate(boxes):\n    \n                    box_data = {\"position\" : {\n                          \"minX\" : box[0],\n                          \"maxX\" : box[2],\n                          \"minY\" : box[1],\n                          \"maxY\" : box[3] \n                        },\n                      \"class_id\" : 1,\n                      \"box_caption\" : \"wheat: (%.3f)\" % (scores[b_i]),\n                      \"domain\": \"pixel\",\n                      \"scores\" : { \"score\" : scores[b_i] }\n                    }\n                    predicted_boxes.append(box_data)\n                    \n\n                #ground truth bounding boxes\n                gt_boxes = targets[i][\"boxes\"].cpu().numpy().astype(np.float64)\n                target_boxes = [] \n                for b_i, box in enumerate(gt_boxes):\n  \n                    box_data = {\"position\" : {\n                          \"minX\" : box[0],\n                          \"maxX\" : box[2],\n                          \"minY\" : box[1],\n                          \"maxY\" : box[3] \n                        },\n                      \"class_id\" : 0,\n                      \"domain\": \"pixel\",\n                      \"box_caption\" : \"ground_truth\"\n                    }\n                    target_boxes.append(box_data)\n                \n                \n                image = image.permute(1,2,0).cpu().numpy().astype(np.float64)\n                \n                #create image object and log\n                img = wandb.Image(image, boxes = \n                                  {\"predictions\": \n                                   {\"box_data\": predicted_boxes, \n                                    \"class_labels\" : class_id_to_label},\"ground_truth\": {\"box_data\": target_boxes}})\n                \n                wandb.log({\"bounding_boxes\": img})\n            \n        iteration += 1\n    \n    epoch_precision_score = sum(epoch_precision_score) \/ len(epoch_precision_score) \n    wandb.log({\"epoch_score_validation\": batch_precision_score, \"epoch\": epoch})\n    return iteration\n        \n    ","9d81633c":"#Load dataset\ntrain_dataset = WheatDataset(train_df, DIR_TRAIN, get_train_transform())\nvalid_dataset = WheatDataset(valid_df, DIR_TRAIN, get_valid_transform())","31c75775":"#Hyperparameter sweep configuration (more info the the W&B docs)\nsweep_config = {\n    'method': 'grid', #grid, random (random will continue running until you terminate it or specify a target field for metric)\n    'metric': {\n      'name': 'train_loss',\n      'goal': 'minimize'   \n    },\n    'parameters': {\n        'epochs': {\n            'values': [2, 4]\n        },\n        'batch_size': {\n            'values': [4, 8]\n        },\n        'lr': {\n            'values': [1e-3, 5e-4] \n        }\n    }\n}\n\n#initialize the sweep\n#Running this line will ask you to log into your W&B account\nsweep_id = wandb.sweep(sweep_config, project=\"GWD-fasterRCNN\")","1bda3e08":"def run():\n    \n    #Default Hyperparameter values if no sweep is defined\n    config_default = {         \n    \"batch_size\": 8,          # input batch size for training (default: 64)\n    \"test_batch_size\": 8,    # input batch size for testing (default: 1000)\n    \"epochs\": 2,             # number of epochs to train (default: 10)\n    \"lr\": 0.005,               # learning rate (default: 0.01)\n    \"momentum\": 0.9,          # SGD momentum (default: 0.5) \n    \"no_cuda\": False,         # whether to disable CUDA training\n    \"seed\": 42,               # random seed (default: 42)\n    \"log_interval\": 1,      #how many batches to wait before logging in train\/test loops\n    \"image_log_interval\": 10,\n    \"decay\": 0.0005\n    }\n    wandb.init(config=config_default, project=\"GWD-fasterRCNN\")\n    config = wandb.config\n    \n    # load a model; pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    num_classes = 2  # wheat (1)....or not wheat (0)\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    use_cuda = not config[\"no_cuda\"] and torch.cuda.is_available()\n    device = torch.device('cuda') if use_cuda else torch.device('cpu')\n\n    # Set random seeds and deterministic pytorch for reproducibility\n    # random.seed(config.seed)       # python random seed\n    torch.manual_seed(config[\"seed\"]) # pytorch random seed\n    np.random.seed(config[\"seed\"]) # numpy random seed\n    torch.backends.cudnn.deterministic = True\n\n\n    #Create dataloaders\n    train_data_loader = DataLoader(\n        train_dataset,\n        batch_size=config[\"batch_size\"],\n        shuffle=False,\n        num_workers=4,\n        collate_fn=collate_fn\n    )\n\n    valid_data_loader = DataLoader(\n        valid_dataset,\n        batch_size=config[\"test_batch_size\"],\n        shuffle=False,\n        num_workers=4,\n        collate_fn=collate_fn\n    )\n\n\n    model.to(device)\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=config[\"lr\"], momentum=config[\"momentum\"], weight_decay=config[\"decay\"])\n    \n    ####\n    wandb.watch(model, log=\"all\") #this line only needs to be run once to hook W&B to your model (comment out for future runs)\n    ####\n    \n    #Keep track of train and test's iterations seperatedly (this allows for easier plotting because you can use iteration_train or iteration_test for your X axis)\n    iteration_train = 0\n    iteration_test = 0\n    for epoch in range(1, config[\"epochs\"] + 1):\n        #Runs training and returns the new training iteration counter\n        iteration_train = train(config, model, device, train_data_loader, optimizer, epoch, iteration_train)\n        #Runs testing\/valiadtion and returns the new test iteration counter\n        iteration_test = test(config, model, device, valid_data_loader, epoch, iteration_test)\n        print(\"Iteration Train: \", iteration_train)\n        print(\"Iteration Test: \", iteration_test)\n\n    torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')\n    wandb.save('fasterrcnn_resnet50_fpn.pth')\n    \n    #Clear memory\n    del model\n    del train_data_loader\n    del valid_data_loader\n    gc.collect()\n    torch.cuda.empty_cache()","0880d81d":"#Run sweep\nwandb.agent(sweep_id, run)\nprint(\"FINISHED\")","eff69dca":"# Logging metrics and predicted bounding boxes\nHere we define train and test functions that will use the W&B logging features as well as setting up the hyperparameter sweep configuration.","bcb02065":"### Select a small subset for demo purposes.","cb3f5f1b":"### Hyperparameter sweep results\nThe HP sweep will produce 2x2x2=8 runs (because in the sweep config, there are 2 values being tested for each of epochs,batch size,and lr)\n\nhttps:\/\/app.wandb.ai\/kshen\/GWD-fasterRCNN\/sweeps\/uuo3vref\n","10c06f8a":"Weights and Biases provides visualizations to compare different HP configurations. Each run can be plotted on a parallel chart. \n\nHere we can see that the run highlighted in purple has a hyperparameter config that results in the smallest train loss.\n\n![Screenshot_99.png](attachment:Screenshot_99.png)","b11f01f5":"### Bounding Boxes\n\nWe can also see the bounding boxes that we logged for our runs (a sample image from a run is shown below). Visualizing bounding boxes can help you see what your model is getting right and wrong by visualizing the ground truth boxes compared with our predictions. \n\nIn the sample below, we can see that one major source of error is that leaves can be detected as wheat heads. However, as mentioned at the beginning, since this is only trained on a small subset for demo purposes, the conclusions made here shouldn't be taken at face value.\n\n![](https:\/\/i.imgur.com\/yP2sF1k.png)","e0105a27":"\n\nBy metrics batch_score_validation or train_loss we can see that certain runs\/configurations can produce better scores:\n\n![Screenshot_98.png](attachment:Screenshot_98.png)","58448b9c":"# Setup","378c5138":"# Hyperparameter Search for FasterRCNN and Visualizing Predictions\n\nIn this notebook, I'll demonstrate the [Weights and Biases library](https:\/\/docs.wandb.com\/)'s automated hyperparameter optimization functionality on FasterRCNN. We'll use it to measure our model's performance quickly, find optimal hyperparameters, monitor system stats like GPU utilization and log the bounding boxes predicted by our FasterRCNN models.\n\n## [Explore results in a live dashboard \u2192](https:\/\/app.wandb.ai\/kshen\/GWD-fasterRCNN?workspace=user-lavanyashukla)\n![](https:\/\/i.imgur.com\/Jt8EKf0.png)\n\nOnly a small subset of the data will be used to demonstrate training and validation to avoid GPU out of memory error due to Kaggle's memory limit. This is only to demonstrate how to integrate W&B and the code can be modified to suit your purpose or run on a local machine.\n\nIn this example, we are trying to find the best combination of epochs, batch size, and learning rate to minimize the training loss. You can set what metric to optimize for, we will just use training loss as an example. Other metrics\/scores such as the Average Precision Score will also be logged for the validation set (technically the Mean Average Precision Score if evaluated on entire set, see [this kernel](https:\/\/www.kaggle.com\/pestipeti\/competition-metric-details-script)).\n\n### If you like this kernel please give it an upvote :)\n\n- [Docs for W&B hyperparameter sweeps](https:\/\/docs.wandb.com\/sweeps)\n\n- [W&B for Kaggle](https:\/\/www.wandb.com\/kaggle)\n\n- Original FasterRCNN notebooks (@pestipeti):\n\n    - [Kernel 1](https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train)\n\n    - [Kernel 2](https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-inference)","46a0ed3e":"In the test function we will also log the bounding boxes so we can see visualizations.","ae86de2f":"# Define Sweep Configuration and Run\nA sweep configuration defines all the values that W&B will try and the metric that it will be using to judge which set of values do the best.\n","aba3bef6":"The run function is the main function.\n\nNote that we are keeping track of the iterations separately for train and test. \n\nThis will allow for easier plotting of values in your W&B dashboard.\n\nYou can set the X axis for charts to use iteration_train for displaying training loss\/accuracy, or iteration_test for displaying values such as validation\/test loss\/accuracy.\n\nThe wandb.watch() function only needs to be called once, so in subsequent runs we can comment it out."}}