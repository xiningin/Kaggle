{"cell_type":{"a834b2f2":"code","83a283c3":"code","8e3f1324":"code","13ca35ee":"code","857cc89d":"code","2341c535":"code","cd414f6a":"code","d26ed9d3":"code","6ee5d0af":"code","cc072f44":"code","493f9dab":"code","439fb4fc":"code","12af066f":"code","fadc2ea5":"code","5080b7d1":"code","75fd38be":"code","56168db7":"code","43917fc4":"code","a22cde64":"code","6eab49da":"code","5ba6a0c8":"code","36458860":"code","f26e1930":"code","599d9871":"code","997fd31e":"code","5e954edf":"markdown"},"source":{"a834b2f2":"import fastbook\nfastbook.setup_book()\nfrom fastbook import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\nfrom sklearn.model_selection import train_test_split","83a283c3":"titanic_train_input_data=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nprocs = [Categorify, FillMissing]\ninput_columns=['PassengerId','Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\nX_train, X_cv, y_train, y_cv = train_test_split(titanic_train_input_data[input_columns], titanic_train_input_data[\"Survived\"], test_size=0.2, random_state=0)","8e3f1324":"titanic_train_input_data=titanic_train_input_data.append(titanic_train_input_data.tail(1))","13ca35ee":"last_added_row=titanic_train_input_data.tail(1)","857cc89d":"titanic_train_input_data.loc[last_added_row.index.values,'Fare']=float(\"NaN\")","2341c535":"titanic_train_input_data.tail(1)","cd414f6a":"splits = (list(X_train.index.values),list(X_cv.index.values))","d26ed9d3":"#Declare dependent variable and automatically deduce the continuous vs categorical variables\ndep_var=['Survived']\ncont,cat = cont_cat_split(titanic_train_input_data, 1, dep_var=dep_var)","6ee5d0af":"cont,cat","cc072f44":"#Tabular Pandas\nto = TabularPandas(titanic_train_input_data, procs, cat, cont, y_names=dep_var, splits=splits)","493f9dab":"#Display train and validation lengths\nlen(to.train),len(to.valid)\n","439fb4fc":"to.show(3)","12af066f":"to.items.head(3)","fadc2ea5":"titanic_train_input_data.columns","5080b7d1":"#Build simple decision tree classifier\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_model=DecisionTreeClassifier(max_leaf_nodes=4)\ndecision_model.fit(xs,y)","75fd38be":"#Lets see the tree that was built\ndraw_tree(decision_model, xs, size=10, leaves_parallel=True, precision=2)","56168db7":"#Dtree visualization\nsamp_idx = np.random.permutation(len(y))[:500]\ndtreeviz(decision_model, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')","43917fc4":"from sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score\ndef calc_accuracy(pred,y):\n    tn,fp,fn,tp=confusion_matrix(y,pred).ravel()\n    print(tn,fp,fn,tp)\n    return round(accuracy_score(y,pred), 6)\ndef model_accuracy(m, x, y): return calc_accuracy(m.predict(x), y)\nmodel_accuracy(decision_model, valid_xs, valid_y)","a22cde64":"decision_model.get_n_leaves(), len(xs)","6eab49da":"xs","5ba6a0c8":"#Decision modelwith no max leaf nodes\ndecision_model_nomax=DecisionTreeClassifier()\ndecision_model_nomax.fit(xs,y)\nacc=model_accuracy(decision_model_nomax, valid_xs, valid_y)\nprint(acc)\ndecision_model_nomax.get_n_leaves(), len(xs)","36458860":"decision_model_minsamplesleaf=DecisionTreeClassifier(min_samples_leaf=15)\ndecision_model_minsamplesleaf.fit(xs,y)\nacc=model_accuracy(decision_model_minsamplesleaf, valid_xs, valid_y)\nprint(acc)\ndecision_model_nomax.get_n_leaves(), len(xs)","f26e1930":"titanic_test_data=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nto_tst = to.new(titanic_test_data)\nto_tst.process()\nto_tst.items.head(3)","599d9871":"print(\"Writing submission file:\")\ny_test=decision_model_minsamplesleaf.predict(to_tst.items)\ntitanic_test_data[\"Survived\"]=y_test\ntitanic_test_data[[\"PassengerId\",\"Survived\"]].to_csv(\"titanic_decision_tree.csv\",index=False)","997fd31e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport numpy as np \nimport pandas as pd \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.datasets import make_classification \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import accuracy_score\nimport scipy.stats\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nimport math\nfrom scipy.stats import pearsonr\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder_sex = LabelEncoder()\n\ndef map_risk_categorical(data,cv_data,test_data,columns,target_column):\n    for column in columns:\n        print(column+\" unique values:\")\n        print(data[column].unique())\n        mean_encodings=data.groupby(column)[target_column].mean()\n        print(mean_encodings)\n        mean_encodings['']=data[column].mode()\n        data[column]=data[column].map(mean_encodings)\n        data[column]=round(data[column].astype(float),2)\n        cv_data[column]=cv_data[column].map(mean_encodings)\n        cv_data[column]=round(cv_data[column].astype(float),2)\n        test_data[column]=test_data[column].map(mean_encodings)\n        test_data[column]=round(test_data[column].astype(float),2)\n        imp.fit(data[[column]])\n        cv_data[[column]]=imp.transform(cv_data[[column]])\n        test_data[[column]]=imp.transform(test_data[[column]])\n    return (data.copy(),cv_data.copy(),test_data.copy())\n\ndef print_correlation(data,columns,class_column):\n    dist={}\n    for column in columns:\n        print(\"Correlation of \"+column+\" with \"+class_column)\n        print(pearsonr(data[column],data[class_column]))\n        \ndef print_summary(data,columns,filter_query):\n    normal_stats={}\n    for column in columns:\n        normal_stats[column]={}\n        if filter_query != \"\":\n            total_count=data.query(filter_query)[column].count()\n            print(\"Total count\"+str(total_count))\n            print(data.query(filter_query)[[column]].describe())\n            normal_stats[column]=(data.query(filter_query)[column].mean(),data.query(filter_query)[column].std())\n        else:\n            print(data[column].describe())\n    return normal_stats.copy()\n\ndef map_data(data): \n    gender_mapping={\"male\":1,\"female\":0} \n    embarked_mapping={\"S\":1,\"C\":2,\"Q\":3} \n    cabin_mapping={\"A\":1,\"B\":2,\"C\":3,\"D\":4,\"E\":5,\"F\":6,\"G\":7} \n    #data[\"Sex\"]=data[\"Sex\"].map(gender_mapping,na_action='ignore')\n    #data[\"Sex\"]=label_encoder_sex.fit_transform(data[\"Sex\"])\n    data[\"Embarked\"]=data[\"Embarked\"].map(embarked_mapping,na_action='ignore') \n    data[\"Cabin\"]=data[\"Cabin\"].map(lambda x:x[0],na_action='ignore') \n    data[\"Cabin\"]=data[\"Cabin\"].map(cabin_mapping,na_action='ignore')\n    data[[\"Pclass\",\"Sex\",\"Fare\",\"Embarked\",\"Age\"]]=imp.fit_transform(data[[\"Pclass\",\"Sex\",\"Fare\",\"Embarked\",\"Age\"]])\n    data[\"Cabin\"]=data[\"Cabin\"].fillna(0)\n    return data.copy()\n\ndef map_risk_numeric(data,columns,risk_map,complement_flag):\n    for column in columns:\n        norm_stats_mean,norm_stats_stdev=risk_map[column]\n        print(\"mean:\"+str(norm_stats_mean)+\" std:\"+str(norm_stats_stdev)+\" for \"+column)\n        norm_stats=scipy.stats.norm(norm_stats_mean,norm_stats_stdev)\n        data[column]=norm_stats.cdf(data[column])\n        data[column]=round(data[column],2)\n        #if complement_flag==1:\n        #    data[new_column]=round(1-data[new_column],2)\n    return data.copy()\n\ntitanic_train_input_data=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic_test_data=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n#train_columns=[\"Pclass\",\"Age\",\"Sex\",\"Fare\",\"Embarked\",\"Cabin\",\"SibSp\",\"Parch\"]\ntrain_columns=[\"Sex\",\"Pclass\",\"Pclass_Sex\",\"Cabin\",\"Age\",\"Pclass_Age\",\"Sex_Fare\",\"Family_Size\"]\ncategorical_columns=[\"Pclass\",\"Sex\",\"Cabin\",\"Pclass_Sex\",\"Age\",\"Pclass_Age\",\"Family_Size\",\"SibSp\",\"Parch\",\"Fare_Per_Person\",\"Sex_Fare\"]\nnumeric_columns=[]\ninput_columns=categorical_columns+numeric_columns\n\n\ntitanic_train_input_data=map_data(titanic_train_input_data.copy())\ntitanic_test_data=map_data(titanic_test_data.copy())\n\ntitanic_train_input_data['Family_Size']=titanic_train_input_data['SibSp']+titanic_train_input_data['Parch']+1\ntitanic_train_input_data['Fare_Per_Person']=titanic_train_input_data['Fare']\/(titanic_train_input_data['Family_Size'])\ntitanic_train_input_data['Fare_Per_Person']=round(titanic_train_input_data['Fare_Per_Person'],2)\ntitanic_train_input_data['Pclass_Sex']=titanic_train_input_data['Pclass'].map(str)+titanic_train_input_data['Sex'].map(str)\ntitanic_train_input_data['Age']=round(titanic_train_input_data['Age']\/5)\ntitanic_train_input_data['Fare_Per_Person']=round(titanic_train_input_data['Fare_Per_Person']\/20)\ntitanic_train_input_data['Pclass_Age']=titanic_train_input_data['Pclass'].map(str)+titanic_train_input_data['Age'].map(str)\ntitanic_train_input_data['Sex_Fare']=titanic_train_input_data['Sex'].map(str)+titanic_train_input_data['Fare_Per_Person'].map(str)\ntitanic_train_input_data['Sex_Age']=titanic_train_input_data['Sex'].map(str)+titanic_train_input_data['Age'].map(str)\ntitanic_train_input_data['Age_Cabin']=titanic_train_input_data['Age'].map(str)+titanic_train_input_data['Cabin'].map(str)\n\nX_train, X_cv, y_train, y_cv = train_test_split(titanic_train_input_data[input_columns], titanic_train_input_data[\"Survived\"], test_size=0.2, random_state=0)\n\ntitanic_train_data=titanic_train_input_data.iloc[X_train.index.values].copy()\n\ntitanic_test_data['Family_Size']=titanic_test_data['SibSp']+titanic_test_data['Parch']\ntitanic_test_data['Fare_Per_Person']=titanic_test_data['Fare']\/(titanic_test_data['Family_Size']+1)\ntitanic_test_data['Fare_Per_Person']=round(titanic_test_data['Fare_Per_Person']\/20)\ntitanic_test_data['Pclass_Sex']=titanic_test_data['Pclass'].map(str)+titanic_test_data['Sex'].map(str)\ntitanic_test_data['Age']=round(titanic_test_data['Age']\/5)\ntitanic_test_data['Pclass_Age']=titanic_test_data['Pclass'].map(str)+titanic_test_data['Age'].map(str)\ntitanic_test_data['Sex_Fare']=titanic_test_data['Sex'].map(str)+titanic_test_data['Fare_Per_Person'].map(str)\ntitanic_test_data['Sex_Age']=titanic_test_data['Sex'].map(str)+titanic_test_data['Age'].map(str)\ntitanic_test_data['Age_Cabin']=titanic_test_data['Age'].map(str)+titanic_test_data['Cabin'].map(str)\n\n#Over sampling\n#count_survived_0, count_survived_1 = titanic_train_data.Survived.value_counts()\n\n#titanic_survived_0 = titanic_train_data[titanic_train_data[\"Survived\"] == 0]\n#titanic_survived_1 = titanic_train_data[titanic_train_data[\"Survived\"] == 1]\n#titanic_train_data_survived_1_over = titanic_survived_1.sample(count_survived_0, replace=True)\n#titanic_train_data_over = pd.concat([titanic_survived_0, titanic_train_data_survived_1_over], axis=0)\n\n\nprint(\"Survived class numeric vals:\")\n#print_unique_vals(titanic_train_data,numeric_columns,\"Survived==1\")\nsurvived_normal_stats=print_summary(titanic_train_data,numeric_columns,\"Survived==1\")\nprint(\"-------------------------\") \nprint(\"Not Survived class numeric vals:\")\n#print_unique_vals(titanic_train_data,numeric_columns,\"Survived==0\")\nnot_survived_normal_stats=print_summary(titanic_train_data,numeric_columns,\"Survived==0\")\nprint(\"--------------------------\")\nprint(\"Correlation:\")\n#print_correlation(titanic_train_data,categorical_columns,\"Survived\")\n#print_correlation(titanic_train_data,numeric_columns,\"Survived\")\n\ntitanic_train_data,X_cv,titanic_test_data=map_risk_categorical(titanic_train_data,X_cv,titanic_test_data,categorical_columns,\"Survived\")\n#titanic_train_data=map_risk_numeric(titanic_train_data,numeric_columns,survived_normal_stats,0)\n#X_cv=map_risk_numeric(X_cv,numeric_columns,survived_normal_stats,0)\n#titanic_test_data=map_risk_numeric(titanic_test_data,numeric_columns,survived_normal_stats,0)\n#sc = StandardScaler()\n#titanic_train_data[numeric_columns] = sc.fit_transform(titanic_train_data[numeric_columns])\n#X_cv[numeric_columns] = sc.fit_transform(X_cv[numeric_columns])\ntitanic_test_original=titanic_test_data.copy()\n#titanic_test_data[numeric_columns] = sc.fit_transform(titanic_test_data[numeric_columns])\n\ntitanic_train_data[train_columns+[\"PassengerId\",\"Name\",\"Survived\"]].to_csv(\"titanic_mapped_risk.csv\",index=False)\n\n#from sklearn.naive_bayes import ComplementNB\n#clf = ComplementNB()\n#clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,100), random_state=7,max_iter=5000,activation='relu')\n#clf = RandomForestClassifier(max_depth=5, random_state=0,max_features=2)\nclf = LogisticRegression(random_state=7,C=0.1)\n\n#clf=Sequential()\n#clf.add(Dense(units=10, activation=\"relu\",kernel_initializer='glorot_uniform',input_dim=6))\n#clf.add(Dense(units=10, activation=\"relu\",kernel_initializer='glorot_uniform'))\n#clf.add(Dense(units=1, activation = 'sigmoid',kernel_initializer='glorot_uniform'))\n#clf.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n#clf.fit(tf.constant(titanic_train_data[train_columns].values,'float32'), tf.constant(y_train.values,'float32'), epochs = 100)\n#y_pred=clf.predict(tf.constant(X_cv[train_columns].values,'float32'))\nclf.fit(titanic_train_data[train_columns], y_train)\ny_pred=clf.predict_proba(X_cv[train_columns])\ny_pred_df=pd.DataFrame(y_cv)\n#y_pred_df[\"Survived_Prob\"]=y_pred[:,0]\ny_pred_df[\"Not_Survived_Prob\"]=y_pred[:,0]\ny_pred_df[\"Survived_Prob\"]=y_pred[:,1]\ny_pred_df[\"Not_Survived_Prob\"]=round(y_pred_df[\"Not_Survived_Prob\"],2)\ny_pred_df[\"Survived_Prob\"]=round(y_pred_df[\"Survived_Prob\"],2)\ny_pred_df.columns=[\"Survived\",\"Not_Survived_Prob\",\"Survived_Prob\"]\n#y_pred_df.columns=[\"Survived\",\"Survived_Prob\"]\npredictions=X_cv.merge(y_pred_df,left_index=True,right_index=True)\npredictions=predictions.merge(titanic_train_input_data,left_index=True,right_index=True)\npredictions[\"Survived_Prediction\"]=predictions[\"Survived_Prob\"].map(lambda x:(1 if x>=0.5 else 0))\npredictions.query(\"Survived_x==1 & Survived_Prediction!=1\").to_csv(\"predictions_fn.csv\",index=False)\npredictions.query(\"Survived_x==0 & Survived_Prediction==1\").to_csv(\"predictions_fp.csv\",index=False)\n#print(predictions.query(\"Survived_Prediction!=Survived\"))\n#predictions.query(\"Survived_x==1 & Survived_Prediction!=1\").Family_Size.hist()\n\ntn,fp,fn,tp=confusion_matrix(y_cv,predictions[\"Survived_Prediction\"]).ravel()\n\nprint(tn,fp,fn,tp)\nprint(accuracy_score(y_cv,predictions[\"Survived_Prediction\"]))\n#print(clf.coef_[0])\n\nprint(\"Writing submission file:\")\ny_test=clf.predict(titanic_test_data[train_columns])\n#y_test=clf.predict(tf.constant(titanic_test_data[train_columns].values,'float32'))\ntitanic_test_original[\"Survived\"]=y_test\ntitanic_test_original[\"Survived\"]=titanic_test_original[\"Survived\"].map(lambda x:(1 if x>=0.5 else 0))\ntitanic_test_original[[\"PassengerId\",\"Survived\"]].to_csv(\"titanic_random_2.csv\",index=False)","5e954edf":"### This notebook is applying fast.ai Tabular data lesson concepts on titanic training data competition"}}