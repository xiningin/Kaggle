{"cell_type":{"df3146f0":"code","fa45f787":"code","b47ea371":"code","6fc33487":"code","f02d22de":"code","b2154250":"code","301c154d":"code","718f3c15":"code","6ed7725c":"code","7e545172":"code","7e899ed9":"code","df3e8eb7":"code","95b30d76":"code","4b7985b7":"code","7fe8f74c":"code","0985e5ee":"code","dfa55ebe":"code","b5a9b300":"code","f56b042b":"code","44b2f146":"code","a1e59f4e":"code","e65c63bf":"markdown","28777e56":"markdown","fe484883":"markdown","441b0004":"markdown","e206e679":"markdown","ae9bccfb":"markdown","b6026fb1":"markdown","9dfeb956":"markdown","0a020b63":"markdown","bf960e67":"markdown","01bee824":"markdown","add87a6d":"markdown","cecc9867":"markdown","7d176260":"markdown"},"source":{"df3146f0":"#import libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n#read data\ndata=pd.read_csv(\"..\/input\/youtube-new\/USvideos.csv\")\n#Datam\u0131z\u0131n bilgilerine bakal\u0131m \ndata.info()","fa45f787":"data.head()","b47ea371":"#data.describe()\ndata.corr()\nf,ax = plt.subplots(figsize=(8,8)) #this command create 8x8 subplot\nsns.heatmap(data.corr(),annot=True, linewidths=1, fmt= '.1f',ax=ax) #user seaborn library for virtualization\nplt.show()","6fc33487":"#sklearn library \nfrom sklearn.linear_model import LinearRegression\n\n#linear regression model\nlinear_reg = LinearRegression();\n\nx = data.views.values.reshape(-1,1)\ny = data.likes.values.reshape(-1,1)\n\nlinear_reg.fit(x, y)\nplt.scatter(x, y)\n\ny_head = linear_reg.predict(x)\nplt.plot(x, y_head, color = \"red\")\n\nplt.xlabel('Views')\nplt.ylabel('likes')\nplt.show()\n\nprint('Accuracy : lineer regression score :{} %'.format(linear_reg.score(x, y)*100))","f02d22de":"#sklearn library \nfrom sklearn.linear_model import LinearRegression\n\n#linear regression model\nmultiple_linear_reg = LinearRegression();\n\n#x = data.views.values.reshape(-1,1)\nx = data.iloc[:,[7,10]].values\n\ny = data.likes.values.reshape(-1,1)\n\nmultiple_linear_reg.fit(x, y)\n\nprint('Accuracy : multiple lineer regression score :{} %'.format(multiple_linear_reg.score(x, y)*100))","b2154250":"# %%\n# polynomial regression =  y = b0 + b1*x +b2*x^2 + b3*x^3 + ... + bn*x^n\n\nfrom sklearn.preprocessing import PolynomialFeatures\npolynomial_regression = PolynomialFeatures(degree = 2)\n\nx_polynomial = polynomial_regression.fit_transform(x)\n\n\n# %% fit\nlinear_regression2 = LinearRegression()\nlinear_regression2.fit(x_polynomial,y)\n\n# %%\n\ny_head2 = linear_regression2.predict(x_polynomial)\n\nplt.plot(x,y_head2,color= \"green\",label = \"poly\")\nplt.legend()\nplt.show()","301c154d":"#import libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n#read data\ndata_redwine=pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\n#Datam\u0131z\u0131n bilgilerine bakal\u0131m \ndata_redwine.info()\ndata_redwine.head(10)","718f3c15":"data_redwine['quality'].value_counts()\n","6ed7725c":"#data.describe()\ndata_redwine.corr()\nf,ax = plt.subplots(figsize=(12,12)) #this command create 8x8 subplot\nsns.heatmap(data_redwine.corr(),annot=True, linewidths=1, fmt= '.1f',ax=ax) #user seaborn library for virtualization\nplt.show()","7e545172":"#Here we see that its quite a downing trend in the volatile acidity as we go higher the quality \nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'volatile acidity', data = data_redwine)","7e899ed9":"#Composition of citric acid go higher as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'citric acid', data = data_redwine)","df3e8eb7":"#Composition of chloride also go down as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'chlorides', data = data_redwine)","95b30d76":"\nx = data_redwine.drop('quality', axis = 1)\ny = data_redwine['quality']\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.4,random_state=42)","4b7985b7":"from sklearn.tree import DecisionTreeClassifier\nreg = DecisionTreeClassifier()\n\nreg.fit(x_train,y_train)\nprint(\"Accuracy Score : Decision Tree classification {}\".format(reg.score(x_test, y_test)))","7fe8f74c":"from sklearn.ensemble import RandomForestClassifier#RandomForestRegressor\nreg = RandomForestClassifier(n_estimators = 200, random_state = 42)\n\nreg.fit(x_train,y_train)\nprint(\"Accuracy Score : Random Forest classification {}\".format(reg.score(x_test, y_test)))","0985e5ee":"from sklearn.metrics import confusion_matrix\n\ny_pretest = reg.predict(x_test)\ncm = confusion_matrix(y_test, y_pretest)\n\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\n\nsns.heatmap(cm, annot=True, linewidth = 0.5, linecolor = \"red\", fmt = \".0f\", ax=ax)\nplt.xlabel(\"Quality-test\")\nplt.ylabel(\"Quality-pred\")\nplt.show()","dfa55ebe":"data_redwine['quality'].value_counts()\n","b5a9b300":"#KNN model our dataset\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 10) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))\n","f56b042b":"\n# %%\n# find k value\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","44b2f146":" from sklearn.svm import SVC\n \n svm = SVC(random_state = 1)\n svm.fit(x_train,y_train)\n \n # %% test\n print(\"accuracy of svm algo: \",svm.score(x_test,y_test))\n","a1e59f4e":" # %% Naive bayes \n from sklearn.naive_bayes import GaussianNB\n nb = GaussianNB()\n nb.fit(x_train,y_train)\n \n # %% test\n print(\"print accuracy of naive bayes algo: \",nb.score(x_test,y_test))\n","e65c63bf":"Multiple lineer regression\n\nx = views and comments\ny = likes\n\nand result is wonderful, accuracy = %84   ","28777e56":"Polynomial Lineer regression\n\nour data is not fit polynomial regression model because our data proportional, I can understand . You can see this graphic.","fe484883":"our classification quality,\n\nlet's look correlation","441b0004":"We use Random Forest Classifier model, and we will look our score.","e206e679":"there is 0.8 corr with views and likes.\n\nI will do lineer regression sklearn model and I will look accurancy my model.","ae9bccfb":"Support Vector Machine (SVM) classification Model","b6026fb1":"let's look all models accuracy percent\n\ndecision tree : % 58.2\nrandom forest : % 66.2\nknn           : % 50.3\nsvm           : % 49.5\nnaive bayes   : % 53\n\nRandom forest looking best model predict our data.\n\nThis is my homework so I'm training :) I'm glad if you comment or invoke\n\n","9dfeb956":"we need ideal K value you can find with this code ","0a020b63":"Random forest accuracy more than decision tree model, maybe We can loon confusion matrix.","bf960e67":"Continue data analyze, let's look correlation ","01bee824":"I will classification algorithm, decision tree, random forest, knn algorithms so I need another dataset.\n\nI think Red Wine Quality dataset really good for classification samples.\n\nlet's look this dataset","add87a6d":"Hello everyone\n\ntoday I will do homework about supervised classification algorithms. Firstly, I have to choose good dataset, I guess hotel-booking-demand is good dataset for classification models.\n\nLet's look nearly this dataset","cecc9867":"We use Decision Tree Classifier model, and we will look our score.","7d176260":"naive bayes classification Model"}}