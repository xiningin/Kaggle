{"cell_type":{"0f7a964f":"code","f93146e5":"code","cac61e1f":"code","23d1c60b":"code","b056e939":"code","324dfdff":"code","2af54a47":"code","da303f06":"code","2ed3d439":"code","24c2f67c":"code","40d1d1ed":"code","cd2eb11f":"code","27967d23":"code","b787e0e7":"code","cf6cc5ed":"code","4c51f415":"code","4a5eb853":"code","9ed0f9a7":"code","2c508447":"code","61340a5f":"code","1838d860":"code","cf3180f4":"code","c0477f30":"code","c70d9856":"code","acc7584b":"code","4dcbc870":"code","532679aa":"code","26fdd9b0":"code","230593ca":"code","6fef6a48":"markdown","0807ab15":"markdown","47187d44":"markdown","3f7f77e3":"markdown","36146498":"markdown","ec9b376e":"markdown","ab553112":"markdown","01c5611e":"markdown","33fe3829":"markdown","aa7b7419":"markdown","e06fccb0":"markdown","3d1f23b8":"markdown","d03b675b":"markdown","7ee41e67":"markdown","7a050f6a":"markdown","8f9b271e":"markdown","cd874661":"markdown","72cf4bdc":"markdown","d609b852":"markdown","05f0f1e5":"markdown","37dee58e":"markdown","0e89d81d":"markdown","cd132cd3":"markdown","171815ce":"markdown","eb2c4947":"markdown","b3a4d6e6":"markdown","4c5217ba":"markdown","29f1a412":"markdown"},"source":{"0f7a964f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # using for plots\nimport seaborn as sns #using for plots\n%matplotlib inline \n\nfrom sklearn.model_selection import train_test_split # split train and test sets\nfrom sklearn.preprocessing import StandardScaler # for scaling \n\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n# Gradient Boosting Machine\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Cross Validation Score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\n# Any results you write to the current directory are saved as output.","f93146e5":"iris_ds= pd.read_csv(\"..\/input\/Iris.csv\")\niris_ds.columns,iris_ds.shape\n#print(iris_ds.shape)","cac61e1f":"iris_ds.head()","23d1c60b":"iris_ds.tail()","b056e939":"iris_ds.isnull().sum()","324dfdff":"iris_ds.dtypes","2af54a47":"iris_ds.Species.value_counts()","da303f06":"iris_ds.drop(columns=['Id'],axis=1,inplace=True)\n","2ed3d439":"iris_ds.columns.values","24c2f67c":"iris_ds.hist(figsize=(20,10))","40d1d1ed":"sns.pairplot(iris_ds,hue='Species')","cd2eb11f":"sns.boxplot(data=iris_ds)","27967d23":"iris_ds.describe()","b787e0e7":"iris_ds.info()","cf6cc5ed":"iris_ds.Species = iris_ds.Species.astype('category')\n","4c51f415":"iris_ds.info()","4a5eb853":"iris_ds.Species.cat.codes.head()","9ed0f9a7":"iris_ds.Species = iris_ds.Species.cat.codes","2c508447":"iris_ds.Species.tail()","61340a5f":"iris_ds.columns.values","1838d860":"X = iris_ds[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\ny = iris_ds.Species","cf3180f4":"x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nx_train.shape[0],y_train.shape[0]","c0477f30":"scalar = StandardScaler()\nx_train = scalar.fit_transform(x_train)\nx_test = scalar.transform(x_test)","c70d9856":"x_train","acc7584b":"def normal_prediction():\n    logis = LogisticRegression()\n    logis.fit(x_train,y_train)\n    print(\"logistic regression::\\n\",confusion_matrix(y_test,logis.predict(x_test)),\"\\n\")\n    \n    svm = SVC()\n    svm.fit(x_train,y_train)\n    print(\"SVM ::\\n\",confusion_matrix(y_test,logis.predict(x_test)),\"\\n\")\n    \n    knn = KNeighborsClassifier()\n    knn.fit(x_train,y_train)\n    print(\"KNN ::\\n\",confusion_matrix(y_test,knn.predict(x_test)),\"\\n\")\n    \n    dTmodel = DecisionTreeClassifier()\n    dTmodel.fit(x_train,y_train)\n    print(\"DecisionTree ::\\n\",confusion_matrix(y_test,dTmodel.predict(x_test)),\"\\n\")\n    \n    rForest = RandomForestClassifier()\n    rForest.fit(x_train,y_train)\n    print(\"RandomForest ::\\n\",confusion_matrix(y_test,rForest.predict(x_test)),\"\\n\")\n\n    grBoosting = GradientBoostingClassifier()\n    grBoosting.fit(x_train,y_train)\n    print(\"GradientBoosting ::\\n\",confusion_matrix(y_test,grBoosting.predict(x_test)),\"\\n\")","4dcbc870":"normal_prediction()","532679aa":"#using cross_val_score\nlogis = LogisticRegression()\nsvm = SVC()\nknn = KNeighborsClassifier()\ndTmodel = DecisionTreeClassifier()\nrForest = RandomForestClassifier()\ngrBoosting = GradientBoostingClassifier()\n    \nscores = cross_val_score(logis,x_train,y_train,cv=5)\nprint(\"Accuracy for logistic regresion: mean: {0:.2f} 2sd: {1:.2f}\".format(scores.mean(),scores.std() * 2))\nprint(\"Scores::\",scores)\nprint(\"\\n\")\n\nscores2 = cross_val_score(svm,x_train,y_train,cv=5)\nprint(\"Accuracy for SVM: mean: {0:.2f} 2sd: {1:.2f}\".format(scores2.mean(),scores2.std() * 2))\nprint(\"Scores::\",scores)\nprint(\"\\n\")\n\nscores3 = cross_val_score(knn,x_train,y_train,cv=5)\nprint(\"Accuracy for KNN: mean: {0:.2f} 2sd: {1:.2f}\".format(scores3.mean(),scores3.std() * 2))\nprint(\"Scores::\",scores)\nprint(\"\\n\")\n\nscores4 = cross_val_score(dTmodel,x_train,y_train,cv=5)\nprint(\"Accuracy for Decision Tree: mean: {0:.2f} 2sd: {1:.2f}\".format(scores4.mean(),scores4.std() * 2))\nprint(\"Scores::\",scores4)\nprint(\"\\n\")\n\nscores5 = cross_val_score(rForest,x_train,y_train,cv=5)\nprint(\"Accuracy for Random Forest: mean: {0:.2f} 2sd: {1:.2f}\".format(scores5.mean(),scores5.std() * 2))\nprint(\"Scores::\",scores5)\nprint(\"\\n\")\n\nscores6 = cross_val_score(grBoosting,x_train,y_train,cv=5)\nprint(\"Accuracy for Gradient Boosting: mean: {0:.2f} 2sd: {1:.2f}\".format(scores6.mean(),scores6.std() * 2))\nprint(\"Scores::\",scores6)\nprint(\"\\n\")","26fdd9b0":"clf = RandomForestClassifier()\n#Random Forest\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": sp_randint(1, 4),\n              \"min_samples_split\": sp_randint(2, 4),\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# run randomized search\nn_iter_search = 5\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=n_iter_search, cv=5)\n\nrandom_search.fit(x_train, y_train)\nprint(random_search.best_params_)\nprint(random_search.best_estimator_)\nconfusion_matrix(y_test,random_search.predict(x_test))\n","230593ca":"# use a full grid over all parameters\nparam_grid = {\"max_depth\": [3, None],\n              \"max_features\": [1, 3, 4],\n              \"min_samples_split\": [2, 3, 4],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# run grid search\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)\n\ngrid_search.fit(x_train, y_train)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)\nconfusion_matrix(y_test,grid_search.predict(x_test))","6fef6a48":"Using StandardScalar function scale all numarical variables. If we have any categorival variables we use OneHotEncoder() for create dummy variables. We can use pandas get_dummies also for to create dummy variables.","0807ab15":"GridSearchCV and RandomizedSearchCV are ways to tune hyper parameters.","47187d44":"Using describe we can see min\/max\/std\/quartail distribution for all numarical columns","3f7f77e3":"We can access converted values using codes property.","36146498":"Check any null values in the datase using isnull and sum. Out data set not have any null\/empty values.","ec9b376e":"Info shows more info like how many observations there in the dataset\n\nIt is also very powerfull. It show how many missing values also in each column level.\n\nIt shows column data type also. Lot information with single command","ab553112":"Box plot for all numarical variables","01c5611e":"Split dataset for train and test. We use train for training purpose and test for validation.","33fe3829":"Placing converted values back to Species column.","aa7b7419":"checking column data types using dtypes. Our dataset contains one int, 4 float and 1 object variables. 'Species' is our target varible.","e06fccb0":"**Reading Iris flower dataset using pandas read_csv menthod**","3d1f23b8":"Now I am copying target variable to y and all remaining columns to X variable. without this also you can pass directly to train_test_split method. ","d03b675b":"I written normal_prediction() fuction for predicting output using different algorithems. \nFollowing algorithems I used for predict\n* LogisticRegression\n* SVM\n* KNN\n* DecisionTree\n* ReandomForest\n* GradientBoosting","7ee41e67":"Using Cross_val_score() function to predict output. This way we can use KFold cross validation. Here I used cv=5. This creates 5 folds.","7a050f6a":"Calling normal_prediction() function","8f9b271e":"Using GridSearchCV to tune parameters","cd874661":"**Dataset contains 6 columns and 150 observations.**\n\nChecking top 5 rows of the dataset using head. We can show number of rows based on parameter. By default it shows 5 rows.","72cf4bdc":"Displaying columns in the dataset using columns property","d609b852":"> **Import required libraries using import**","05f0f1e5":"We have total 6 columns in the data set. Id column we can delete. 'Species' is the catagorical column\n\nLet check distribution of target vriable uisng value_counts\n\n","37dee58e":"Now I am going to convert target variable as category.\nwe can follow different approaches for this convertion. different approaches as follows\n* LableEncoder()\n* map (We use map for ordinal category columns. You can specify the order)\n* type conversion using astype","0e89d81d":"Using seaboarns package we can show rich plots. I used pairplot function to show scatter plots.","cd132cd3":"We have equal distribution values in Target variable. This is balenced dataset. In case of imbalenced datasets we have to use other techniques to handle. \n\nI am deleting Id column. This wont help our model. using drop function I am deleting Id variable from dataset. \n\naxis=1 indicated column level and inplace=True applies dataset.","171815ce":"using hist function from pandas we can check numarical column distribution.","eb2c4947":"Id column has been deleted from dataset. Now you can check how many columns present in the dataset","b3a4d6e6":"Now we check again data types using info command. now it shows type as category","4c5217ba":"checking last 5 rows using tail. By default tail shows last 5 rows.","29f1a412":"***Iris Flower Model*** \n\nIn this kernal I am going to explore following\n* Basic ploting\n* different classification algorithems\n* Cross_validation\n* GridSearchCV\n* RandamizedSearchCV\n* Converting categorical variable (different approaches)\n"}}