{"cell_type":{"ac85afd0":"code","dcf0eb43":"code","37592f12":"code","5f652186":"code","03a7d897":"code","87f82e48":"code","0a7a5e0d":"code","5e335b7e":"code","85d3ef27":"code","08eed878":"code","a7bc62b0":"code","6675ccbc":"code","66eee268":"code","1554038e":"code","3402f3ed":"code","4e7fdf4b":"code","f44ad8fa":"code","eeaf2e51":"code","79f35a7c":"code","94fa918c":"markdown","d6ec0bc9":"markdown","1e9b7ad3":"markdown"},"source":{"ac85afd0":"#### Just use the load_kaggle() utility script to load kaggle datasets ##\n### Add the load_kaggle() utility script at the top under \"File\" menu\nfrom load_kaggle import load_kaggle","dcf0eb43":"subm, train, test = load_kaggle()\nprint(train.shape, test.shape, subm.shape)\ntrain.head()","37592f12":"train['keyword'] = train['keyword'].fillna('missing').values\ntest['keyword'] = test['keyword'].fillna('missing').values","5f652186":"train['location'] = train['location'].fillna('missing').values\ntest['location'] = test['location'].fillna('missing').values","03a7d897":"train['text'] = train[['keyword','location','text']].apply(lambda x :' '.join(x.astype(str)),1)\ntest['text'] = test[['keyword','location','text']].apply(lambda x :' '.join(x.astype(str)),1)\ntrain.head(3)","87f82e48":"train1 = train.drop(['keyword','location'],axis=1)\ntest1 = test.drop(['keyword','location'],axis=1)\ntrain1.head(2)","0a7a5e0d":"### you can do either pip install from pypi or from the github\n!pip install deep_autoviml\n#!pip install git+https:\/\/github.com\/AutoViML\/deep_autoviml.git","5e335b7e":"#### Just make sure you have version 0.0.57 or Higher #######\nimport numpy as np\nimport pandas as pd\nfrom deep_autoviml import deep_autoviml as deepauto","85d3ef27":"target = 'target'","08eed878":"project_name = \"Disaster\"\nmodel_options = {'nlp_char_limit':10, 'cat_feat_cross_flag':False,\n                 'max_trials': 10}\nkeras_options = {\"patience\":10, \"epochs\":100}","a7bc62b0":"keras_model_type =  \"NNLM\" ## \"auto\" is an automatic way to build NLP pipelines. Also, \"USE\" stands for Universal Sentence Encoder","6675ccbc":"#### Make sure you run this on train1 since it has been combined into one text ##\noutput = deepauto.fit(train1, target, keras_model_type,project_name=project_name, \n                      keras_options=keras_options,\n                 model_options=model_options, save_model_flag=True, \n                      use_my_model='', verbose=1)","66eee268":"model = output[0]\ncat_vocab_dict = output[1]","1554038e":"### Make sure you use train1 and test1 since they have combined text ##\ntest1.head(1)","3402f3ed":"predictions = deepauto.predict(model, project_name, test_dataset=test1,\n                                 keras_model_type=keras_model_type, \n                                 cat_vocab_dict=cat_vocab_dict)","4e7fdf4b":"y_preds = predictions[-1]\nprint(y_preds.shape)\ny_preds[:4]","f44ad8fa":"print(subm.shape)\nsubm.head(2)","eeaf2e51":"subm[target] = y_preds\nsubm.head()","79f35a7c":"subm.to_csv('submission.csv', index=False)","94fa918c":"# Run Deep_AutoViML using the following options","d6ec0bc9":"## Swivel Encoder + Keras preprocessing layers = Winning Combination!","1e9b7ad3":"# deep_autoviml is a tensorflow 2.5-powered, keras-ready, model and pipeline building utility\n##  You just have to run deep_autoviml using a couple of lines of code. It will automatically do the following:\n\n- Load a preselected BERT model from Tensorflow Hub\n- Load train, test files containing training and testing data\n- Load a proper preprocessor + tokenizer for BERT\n- It will make predictions \n## Keras preprocessing layers enable you to encapsulate feature engineering and hidden layers into the model itself. This makes the process for training and predictions the same: just feed input data (in the form of files or dataframes) and the model will take care of all preprocessing before predictions."}}