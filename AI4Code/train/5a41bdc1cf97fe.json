{"cell_type":{"c3b8566f":"code","6e2bf744":"code","5b4733ad":"code","195b2ca5":"code","632b3241":"code","2f1e64d5":"code","8f5108b6":"code","f7264040":"code","f1b5b349":"code","f6bfb1d4":"code","77ccc161":"code","92a4185f":"code","0e2eeb25":"code","aaecdf80":"code","33401054":"code","7c28537b":"code","a0e24347":"code","a58f4700":"code","91549d9f":"code","3124f6f9":"code","30af69af":"code","1e2e12aa":"code","7cf3d607":"markdown","4920002c":"markdown","72333b42":"markdown","d2a23336":"markdown","9eddd8cc":"markdown","9058a599":"markdown","00d7a446":"markdown","1f790d0f":"markdown","c7ec031b":"markdown","a52af241":"markdown","8ec04300":"markdown","8b2b122f":"markdown"},"source":{"c3b8566f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6e2bf744":"import pandas as pd\nimport numpy as np\nimport os\nimport xgboost as xgb\nimport catboost as cb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import ParameterGrid\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\nseed = 123","5b4733ad":"train_data = pd.read_csv(\"..\/input\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/test.csv\")\nsample = pd.read_csv(\"..\/input\/SampleSubmission.csv\")\nlabel = \"NESHER\"\nbad_features = [\"DAPAR\",\"KABA\",\"TZADAK\",\"TZIYUN_HATAMA_MM\",\"IND_MATIM_MM\",\"IND_OVER_MINIMAL_REQUIREMENTS_MM\",\n                \"TZIYUN_HATAMA_MP\",\"IND_MATIM_MP\",\"IND_OVER_MINIMAL_REQUIREMENTS_MP\",\"MADAD_EITANUT\"]\napp = pd.read_csv(\"..\/input\/applications.csv\")","195b2ca5":"print(\"Train shape: \"+str(train_data.shape))\nprint(\"Test shape: \"+str(test_data.shape))","632b3241":"def add_wills_features(df_train):\n    ##adds wills features, returns a dataframe that contains the new columns\n    ##need to join with the original dataframe and keep the original values \n    dests_wills_cols = [col for col in df_train.columns if \"_WILL\" in col and \"DESTINATION\" not in col and \"LOHEM\" not in col]\n    df_train['dests_wills_max'] = df_train[dests_wills_cols].max(axis = 1)\n    df_train['dests_wills_sum'] = df_train[dests_wills_cols].sum(axis = 1)\n    df_train['dests_wills_all_nan'] = df_train['dests_wills_max'].apply(lambda x: int(x==0))\n    df_train['dests_wills_non_zero_count'] = df_train[dests_wills_cols].astype(bool).sum(axis=1)\n    df_train['dests_wills_mean_without0'] = df_train[['dests_wills_sum','dests_wills_non_zero_count']].apply(lambda x: x[0]\/x[1], axis = 1)\n    df_train['dests_wills_mean_with0'] = df_train[dests_wills_cols].mean(axis = 1)\nadd_wills_features(train_data)\nadd_wills_features(test_data)","2f1e64d5":"cols = [\"DESTINATION\", \"KAHAS_TWO_MONTHS_BEFORE_GIYUS\", 'LOHEM_WILL', 'DESTINATION_WILL']\ndests_details = pd.concat([train_data[cols],test_data[cols]])\\\n    .groupby(by = \"DESTINATION\", sort = False).agg({\"KAHAS_TWO_MONTHS_BEFORE_GIYUS\":\"mean\",\n                                                    \"LOHEM_WILL\":\"mean\",\n                                                    \"DESTINATION_WILL\":\"mean\"})\ndests_details.columns = [\"dest_details_dest_will\", \"dest_details_lohem_will\", \"dest_details_kahas\"]\ntrain_data = train_data.join(dests_details, on = 'DESTINATION')\ntest_data = test_data.join(dests_details, on = 'DESTINATION')","8f5108b6":"app.Begin_date = pd.to_datetime(app.Begin_date.apply(lambda x: '2'+x[1:]), format='%Y-%m-%d %H:%M:%S')\napp.End_date = pd.to_datetime(app.End_date.apply(lambda x: '2'+x[1:]), format='%Y-%m-%d %H:%M:%S')\napp['date_diff'] = app.End_date - app.Begin_date\n\ndef f(x):\n    d = {}\n    d['num_apps'] = x.shape[0]\n    d['num_in'] = sum(x.Incident_direct == '\u05e0\u05db\u05e0\u05e1')\n    d['num_out'] = sum(x.Incident_direct == '\u05d9\u05d5\u05e6\u05d0')\n    d['in_percentage'] = d['num_in'] \/ d['num_apps']\n    d['arotz_phone_percentage'] = sum(x.ArotzPnia == '\u05d8\u05dc\u05e4\u05d5\u05df') \/ d['num_apps']\n    d['arotz_fax_percentage'] = sum(x.ArotzPnia == '\u05e4\u05e7\u05e1') \/ d['num_apps']\n    d['arotz_mail_percentage'] = sum(x.ArotzPnia == '\u05de\u05d9\u05d9\u05dc') \/ d['num_apps']\n    d['arotz_internet_percentage'] = sum(x.ArotzPnia == '\u05d0\u05d9\u05e0\u05d8\u05e8\u05e0\u05d8') \/ d['num_apps']\n    d['arotz_personal_percentage'] = sum(x.ArotzPnia == '\u05de\u05e1\u05d9\u05e8\u05d4 \u05d0\u05d9\u05e9\u05d9\u05ea') \/ d['num_apps']\n    d['arotz_other_percentage'] = 1 - (d['arotz_phone_percentage']+d['arotz_fax_percentage']+d['arotz_mail_percentage']+\n                                       d['arotz_internet_percentage']+d['arotz_personal_percentage'])\n    d['mean_date_diff'] = x.date_diff.mean().seconds\n    d['max_date_diff'] = x.date_diff.max().seconds\n    return pd.Series(d, index=d.keys())\napp_features = app.groupby(\"CustomerID\").apply(f)\n\ntrain_data = train_data.set_index(\"TZ\").join(app_features)\ntest_data = test_data.set_index(\"TZ\").join(app_features)\ntrain_data = train_data.reset_index()\ntest_data = test_data.reset_index()","f7264040":"def kaba_as_feature(df, quality_col_name, other_col_name):\n#     df[other_col_name] = df[other_col_name] + 1 \n    new_col_name = quality_col_name + '_' + other_col_name\n    df[new_col_name] = df[quality_col_name] \/ df[other_col_name] + 1\n    df = df.replace([np.inf, -np.inf], np.nan)\n    df = df.fillna(df[new_col_name].max())\n    return df","f1b5b349":"train_data = kaba_as_feature(train_data, \"KABA\", \"LOHEM_WILL\")\ntest_data = kaba_as_feature(test_data, \"KABA\", \"LOHEM_WILL\")","f6bfb1d4":"print(\"Train shape: \"+str(train_data.shape))\nprint(\"Test shape: \"+str(test_data.shape))","77ccc161":"features = list(set(train_data.columns) -\\\n                set([\"MISPAR_ISHI\", \"TZ\", \"DESTINATION\", \"MAHZOR_ACHARON\", \"NESHER\"]) -\\\n                set(bad_features))","92a4185f":"n_splits = 10\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\nauc = []\n\nparam = {'max_depth': 5,\n         'eta': 0.2,\n         'silent': 1,\n         'objective': 'binary:logistic', \n         'nthread':8,\n         \"eval_metric\":'auc',\n         \"seed\": seed}\nnum_round = 100\n\nfor i,(train,val) in enumerate(kf.split(train_data)):\n    print(\"Fold {} out of {}\".format(i+1, n_splits))\n    dtrain = xgb.DMatrix(train_data.loc[train, features], label=train_data.loc[train, label])\n    dval = xgb.DMatrix(train_data.loc[val, features], label=train_data.loc[val, label])\n    evallist = [(dtrain, 'train'), (dval, 'eval')]\n    model = xgb.train(param, dtrain, num_round, evallist)\n    auc.append(float(model.eval(dval).split(':')[1]))\nprint(\"{}-fold cross validation AUC: {:.4f}\".format(n_splits, np.mean(auc)))","0e2eeb25":"from sklearn.model_selection import GridSearchCV\nparams = {\n         'gamma': [0, 0.5, 1],\n         'learning_rate': [0.1, 0.2, 0.5],\n         'colsample_bytree': [0.6, 0.8, 1.0],\n         'max_depth': [3, 4, 5],\n         'n_estimators': [100, 200]\n        }\n\nmodel = XGBClassifier(n_jobs = 8)\ngscv = GridSearchCV(model, params, cv=5, scoring='roc_auc')\ngscv.fit(train_data[features], train_data[label])\ngs_best = gscv.best_estimator_","aaecdf80":"bst = gscv.best_estimator_\nbst.fit(train_data[features], train_data[label])\nsample[label] = [x[1] for x in bst.predict_proba(test_data[features])]\nsample.to_csv(\"..\/data\/submission.csv\", index=False)","33401054":"sorted(list(model.get_fscore().items()), key=lambda x: x[1], reverse=True)","7c28537b":"features = list(set([col for col in train_data.columns if not col.endswith(\"_y\")]) - set([\"MISPAR_ISHI\", \"TZ\", \"NESHER\"]) - set(bad_features))\ncat_features = [\"MAHZOR_ACHARON\", \"DESTINATION_x\"]\ncat_features = [i for (i,x) in  enumerate(features) if x in cat_features]","a0e24347":"n_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\nauc = []\n\nparams = {\n    'iterations': 500,\n    'learning_rate': 0.1,\n    'depth':10,\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'random_seed': seed,\n    'use_best_model': False,\n    'verbose':1\n}\n\n\n\nfor i,(train,val) in enumerate(kf.split(train_data)):\n    print(\"Fold {} out of {}\".format(i+1, n_splits))\n    train_pool = cb.Pool(train_data.loc[train, features], train_data.loc[train, label],\n                      cat_features=cat_features)\n    validate_pool = cb.Pool(train_data.loc[val, features], train_data.loc[val, label],\n                         cat_features=cat_features)\n\n    model = cb.CatBoostClassifier(**params)\n    model.fit(train_pool, eval_set=validate_pool)\n    \n    auc.append(roc_auc_score(validate_pool.get_label(), [x[1] for x in model.predict_proba(validate_pool)]))\nprint(\"{}-fold cross validation AUC: {:.2f}\".format(n_splits, np.mean(auc)))","a58f4700":"n_splits = 10\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\np_scores = []\nparams = {\n    'iterations': [400, 500, 700],\n    'learning_rate': [0.03 ,0.05, 0.07],\n    'depth':[4,5,6],\n    'rsm': [0.7, 0.8, 0.9],\n    'loss_function': ['Logloss'],\n    'eval_metric': ['AUC'],\n    'random_seed': [seed],\n    'use_best_model': [False]\n}\n\n\nfor i,p in enumerate(ParameterGrid(params)):\n    print(\"Model number {}\".format(i))\n    auc = []\n    for i,(train,val) in enumerate(kf.split(train_data)):\n        print(\"Fold {} out of {}\".format(i+1, n_splits))\n        train_pool = cb.Pool(train_data.loc[train, features], train_data.loc[train, label],\n                          cat_features=cat_features)\n        validate_pool = cb.Pool(train_data.loc[val, features], train_data.loc[val, label],\n                             cat_features=cat_features)\n\n        model = cb.CatBoostClassifier(**p)\n        model.fit(train_pool, eval_set=validate_pool)\n\n        auc.append((roc_auc_score(validate_pool.get_label(), [x[1] for x in model.predict_proba(validate_pool)])))\n    p_scores.append([np.mean(auc), p])","91549d9f":"sorted(auc, key=lambda x: x[0], reverse=True)[0]","3124f6f9":"params = {'depth': 5,\n 'eval_metric': 'AUC',\n 'iterations': 500,\n 'learning_rate': 0.05,\n 'loss_function': 'Logloss',\n 'random_seed': 123,\n 'rsm': 0.8,\n 'use_best_model': False}","30af69af":"train_pool = cb.Pool(train_data[features], train_data[label],\n                      cat_features=cat_features)\ntest_pool = cb.Pool(test_data[features],\n                      cat_features=cat_features)\nmodel = cb.CatBoostClassifier(**params)\nmodel = model.fit(train_pool)\n                     \nsample[label] = [x[1] for x in model.predict_proba(test_pool)]\nsample.to_csv(\"..\/data\/submission1000.csv\", index=False)","1e2e12aa":"sorted(zip(features,model.feature_importances_), key=lambda x: x[1], reverse=True)","7cf3d607":"## Feature importance","4920002c":"# Catboost","72333b42":"## Grid search","d2a23336":"## Predict","9eddd8cc":"## Feature importance","9058a599":"# XGBoost","00d7a446":"# Add features","1f790d0f":"## Grid search","c7ec031b":"# Read data","a52af241":"## Cross validate","8ec04300":"## Predict test","8b2b122f":"## Cross validate"}}