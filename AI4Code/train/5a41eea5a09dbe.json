{"cell_type":{"2b278122":"code","d30a663b":"code","76b305e5":"code","95259ee8":"code","07c8c61c":"code","079fee8c":"code","7c17bc4d":"code","d120e7b5":"code","44434ff6":"code","81be5eba":"code","33cb7830":"code","5908ce7a":"code","a2a73272":"code","123a2724":"code","303875a3":"code","1f8cd366":"code","8fd68f96":"code","6e7578cd":"markdown","33c59eb2":"markdown","45a2316c":"markdown","32d7ef61":"markdown","b56a8a24":"markdown"},"source":{"2b278122":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \n\n# gradient boosting for regression in scikit-learn\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom matplotlib import pyplot\n\n\nfrom xgboost import XGBRegressor\n\nfrom lightgbm import LGBMRegressor\n\nfrom catboost import CatBoostRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d30a663b":"train=pd.read_csv(r'\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n# define dataset\ntrain.head(2)","76b305e5":"test=pd.read_csv(r'\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n# define dataset\ntest.head(2)","95259ee8":"df_samp=pd.read_csv(r'\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n# define dataset\ndf_samp.head(2)","07c8c61c":"li_col=train.columns.to_list()\nli_cat=[]\n# print(li_col)\nfor i in range(len(li_col)):\n    if train[li_col[i]].dtypes=='O':\n        li_cat.append(li_col[i])\n        \nprint(li_cat)   ","079fee8c":"for i in range(len(li_cat)):\n    train[li_cat[i]]=train[li_cat[i]].astype(str)\n    train[li_cat[i]]=label_encoder.fit_transform(train[li_cat[i]]) \n    test[li_cat[i]]=test[li_cat[i]].astype(str)\n    test[li_cat[i]]=label_encoder.fit_transform(test[li_cat[i]]) \n    ","7c17bc4d":"train = train.fillna(train.mean())\ntest = test.fillna(test.mean())\n","d120e7b5":"y=train[['SalePrice']]\nX=train.drop(columns=['SalePrice'])\n# # X_train, X_test, y_train, y_test = train_test_split(\n# #     X, y, test_size=0.33, random_state=42)","44434ff6":"# evaluate the model\nmodel = GradientBoostingRegressor()\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\nprint('R2: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","81be5eba":"# fit the model on the whole dataset\nmodel = GradientBoostingRegressor()\nmodel.fit(X, y)","33cb7830":"# y_pred = model.predict(test)\n","5908ce7a":"# n_scores = cross_val_score(model, test, y_pred, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n# print('R2: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n","a2a73272":"# df=pd.DataFrame(y_pred,columns={'SalePrice'})\n# df=pd.concat([df_samp[['Id']],df],axis=1)\n# # df=df.rename(columns={'0':'SalePrice'})\n# df","123a2724":"# df.to_csv('GBM.csv',index=False)","303875a3":"# evaluate the model\nmodel = XGBRegressor(objective='reg:squarederror')\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\nprint('R2: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","1f8cd366":"# evaluate the model\nmodel = LGBMRegressor()\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\nprint('R2: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","8fd68f96":"# evaluate the model\nmodel = CatBoostRegressor(verbose=0, n_estimators=200)\ncv = RepeatedKFold(n_splits=20, n_repeats=6, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\nprint('R2: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","6e7578cd":"# XG Boost","33c59eb2":"# Light GBM","45a2316c":"# CatBoost","32d7ef61":"# GBM","b56a8a24":"# Source: \nhttps:\/\/machinelearningmastery.com\/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost\/"}}