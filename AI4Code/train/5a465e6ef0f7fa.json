{"cell_type":{"ad1eaac2":"code","6534c52e":"code","3f648428":"code","10218601":"code","b872daf0":"code","551a2729":"code","5eede3bd":"code","64eb74c3":"code","9f3daa80":"code","7008eaf7":"code","c504530e":"code","c4da9dbe":"code","5023ce25":"code","59b2207c":"code","fd8f6acf":"code","8b5f558d":"code","49183703":"code","3313b181":"code","ef2508c1":"code","edb4cbec":"code","b5ab715e":"code","50abda56":"code","82f4fcd6":"code","c7342885":"code","8dd0d536":"code","466b0ac7":"code","1e5c19b5":"code","96ffec31":"code","05ed8858":"code","7bc5cc95":"code","24e18fbb":"code","5c1692c0":"code","4c3a229d":"code","16386e18":"code","47897e28":"code","8b86972f":"markdown","4d820e0d":"markdown","ca97df03":"markdown","8a23a2e2":"markdown","bf4e345f":"markdown","09fd19e8":"markdown","b2a444e9":"markdown","93ac2ba5":"markdown","0e29f055":"markdown","19392736":"markdown","e657513e":"markdown"},"source":{"ad1eaac2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6534c52e":"#To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\n\n#Common imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n#To plot pretty figures\n%matplotlib inline\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\nnp.random.seed(42)","3f648428":"training_set = pd.read_csv('..\/input\/titanic\/train.csv')\ntesting_set = pd.read_csv('..\/input\/titanic\/test.csv')\n\nprint(training_set.head())\nprint('-'*80)\nprint(testing_set)","10218601":"training_set.info()","b872daf0":"#There are null values in attributes - Age, Cabin, Embarked\ntraining_set.describe()","551a2729":"#In the training set only 38.38% survived the titanic accident\ntraining_set.corr()","5eede3bd":"#Survival is strongly related to Fare and Pclass attributes than other numeric attributes\ntraining_set[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","64eb74c3":"#1st class passengers have better survival rate than the other 2 classes\ntraining_set[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9f3daa80":"training_set['Split'] = 1\ntesting_set['Split'] = 0\ntesting_set['Survived'] = np.NaN\ndata = pd.concat([training_set, testing_set])\nprint(data.shape)","7008eaf7":"#As the Name attribute is a string we can use the name title in the string to categorize the attribute.\ndata['Title'] = data.Name.str.extract('([a-zA-Z]+)\\.', expand=False)\npd.crosstab(data['Title'], data['Sex'])","c504530e":"data['Title'] = data['Title'].replace(['Capt', 'Col', 'Countess', 'Don', 'Dona', 'Dr', \n                                       'Jonkheer', 'Lady', 'Major', 'Rev', 'Sir'], 'Rare')\ndata['Title'] = data['Title'].replace(['Mme'], 'Mrs')\ndata['Title'] = data['Title'].replace(['Ms', 'Mlle'], 'Miss')\ndata['Title'] = data['Title'].map({'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5})\ndata['Title'] = data['Title'].fillna(0)","c4da9dbe":"data['Fare'].fillna(data.groupby('Pclass')['Fare'].transform('median'), inplace=True)\ndata['Embarked'].fillna(data.Embarked.dropna().mode()[0], inplace=True)\ndata['Age'].fillna(data.groupby('Title')['Age'].transform('median'), inplace=True)\nprint(data.Age.isna().value_counts())","5023ce25":"data['Sex'] = data['Sex'].map({'female': 0, 'male': 1}).astype(int)\ndata['Embarked'] = data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\ndata['Age_cat'] = pd.cut(data['Age'], 5)\ndata['Fare_cat'] = pd.qcut(training_set['Fare'], 4)","59b2207c":"X = data[data.Split==1]\nX[['Age_cat', 'Survived']].groupby(['Age_cat'], as_index=False).mean().sort_values(by='Survived', ascending=False)","fd8f6acf":"data['Age_Group'] = np.NaN\ndata.loc[data['Age']<=16, 'Age_Group'] = 0\ndata.loc[(data['Age']>16) & (data['Age']<=32), 'Age_Group'] = 1\ndata.loc[(data['Age']>32) & (data['Age']<=48), 'Age_Group'] = 2\ndata.loc[(data['Age']>48) & (data['Age']<=64), 'Age_Group'] = 3\ndata.loc[(data['Age']>64) & (data['Age']<=80), 'Age_Group'] = 4","8b5f558d":"data.Age_Group.value_counts()","49183703":"X[['Fare_cat', 'Survived']].groupby(['Fare_cat'], as_index=False).mean().sort_values(by='Survived', ascending=False)","3313b181":"data['Fare_Group'] = np.NaN\ndata.loc[data['Fare']<=7.91, 'Fare_Group'] = 0\ndata.loc[(data['Fare']>7.91) & (data['Fare']<=14.454), 'Fare_Group'] = 1\ndata.loc[(data['Fare']>14.454) & (data['Fare']<=31.0), 'Fare_Group'] = 2\ndata.loc[(data['Fare']>31.0) & (data['Fare']<=512.3292), 'Fare_Group'] = 3","ef2508c1":"data['Cabin'] = data.Cabin.str[:1]\nprint(data.Cabin.value_counts())","edb4cbec":"cabin_mapping = {'A': 0, 'B': 0.4, 'C': 0.8, 'D': 1.2, 'E': 1.6, 'F': 2, 'G': 2.4, 'T': 2.8}\ndata['Cabin'] = data['Cabin'].map(cabin_mapping)\ndata['Cabin'].fillna(data.groupby('Pclass')['Cabin'].transform('median'), inplace=True)\nprint(data['Cabin'].isna().value_counts())","b5ab715e":"print(data.Fare_cat.value_counts())\nprint(data.Fare_Group.value_counts())","50abda56":"data['FamilySize'] = data['Parch']+data['SibSp']+1\ndata['IsAlone'] = 0\ndata.loc[data.FamilySize==1, 'IsAlone'] = 1\ndata.drop('FamilySize', axis=1, inplace=True)\ndata['Age*Class'] = data['Age_Group']*data['Pclass']","82f4fcd6":"prepared_data = data.drop(['Age', 'Fare', 'Age_cat', 'Fare_cat', 'Name', 'Ticket', 'Parch', 'SibSp', 'Cabin'], axis=1)\nprint(prepared_data.info())","c7342885":"X_train  = prepared_data[data.Split==1].drop(['PassengerId', 'Survived', 'Split'], axis=1)\ny_train = prepared_data[data.Split==1].Survived\nX_test = prepared_data[data.Split==0].drop(['PassengerId', 'Survived', 'Split'], axis=1)\nprint(X_train.head(10))","8dd0d536":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","466b0ac7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nLR = LogisticRegression(solver='liblinear', max_iter=1000, penalty='l2')\nLR.fit(X_train, y_train)\npredictions = LR.predict(X_train)\nscores = cross_val_score(LR, X_train, y_train, cv=10, scoring='accuracy')\nprint('Training set accuracy is : ', accuracy_score(y_train, predictions)*100)\nprint('Validation set accuracy is :', scores.mean()*100)","1e5c19b5":"from sklearn.linear_model import SGDClassifier\n\nSGD = SGDClassifier(learning_rate='adaptive', eta0=0.01)\nSGD.fit(X_train, y_train)\npredictions = SGD.predict(X_train)\nscores = cross_val_score(SGD, X_train, y_train, cv=10, scoring='accuracy')\nprint('Training set accuracy is : ', accuracy_score(y_train, predictions)*100)\nprint('Validation set accuracy is :', scores.mean()*100)","96ffec31":"from sklearn.naive_bayes import GaussianNB\n\nGNB = GaussianNB()\nGNB.fit(X_train, y_train)\npredictions = GNB.predict(X_train)\nscores = cross_val_score(GNB, X_train, y_train, cv=10, scoring='accuracy')\nprint('Training set accuracy is : ', accuracy_score(y_train, predictions)*100)\nprint('Validation set accuracy is :', scores.mean()*100)","05ed8858":"from sklearn.tree import DecisionTreeClassifier\n\nDTC = DecisionTreeClassifier(min_samples_split=5, random_state=42)\nDTC.fit(X_train, y_train)\npredictions = DTC.predict(X_train)\nscores = cross_val_score(DTC, X_train, y_train, cv=10, scoring='accuracy')\nprint('Training set accuracy is : ', accuracy_score(y_train, predictions)*100)\nprint('Validation set accuracy is :', scores.mean()*100)","7bc5cc95":"from sklearn.neighbors import KNeighborsClassifier\n\nKNN = KNeighborsClassifier(n_neighbors=16)\nKNN.fit(X_train, y_train)\npredictions = KNN.predict(X_train)\nscores = cross_val_score(KNN, X_train, y_train, cv=10, scoring='accuracy')\nprint('Training set accuracy is : ', accuracy_score(y_train, predictions)*100)\nprint('Validation set accuracy is :', scores.mean()*100)","24e18fbb":"from sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier(n_estimators=99, min_samples_split=9, random_state=42)\nRFC.fit(X_train, y_train)\npredictions = RFC.predict(X_train)\nscores = cross_val_score(RFC, X_train, y_train, cv=10, scoring='accuracy')\nprint('Training set accuracy is : ', accuracy_score(y_train, predictions)*100)\nprint('Validation set accuracy is :', scores.mean()*100)","5c1692c0":"from sklearn.svm import SVC\n\nSVM = SVC(kernel='poly', degree=3, tol=0.01, probability=True)\nSVM.fit(X_train, y_train)\npredictions = SVM.predict(X_train)\nscores = cross_val_score(SVM, X_train, y_train, cv=10, scoring='accuracy')\nprint('Training set accuracy is : ', accuracy_score(y_train, predictions)*100)\nprint('Validation set accuracy is :', scores.mean()*100)","4c3a229d":"from sklearn.svm import LinearSVC\n\nLSVC = LinearSVC(max_iter=2000, dual=False)\nLSVC.fit(X_train, y_train)\npredictions = LSVC.predict(X_train)\nscores = cross_val_score(LSVC, X_train, y_train, cv=10, scoring='accuracy')\nprint('Training set accuracy is : ', accuracy_score(y_train, predictions)*100)\nprint('Validation set accuracy is :', scores.mean()*100)","16386e18":"from sklearn.ensemble import VotingClassifier\n\nVC = VotingClassifier([\n    ('LogisticRegression', LR),\n    ('GaussianNB', GNB),\n    ('SGDClassifier', SGD),\n    ('RandomForestClassifier', RFC),\n    ('SupportVectorMachines', SVM),\n    ('LinearSVC', LSVC),\n    ('KNeighborsClassifier', KNN),\n    ('DecisionTreeClassifier', DTC)\n])\nVC.fit(X_train, y_train)\npredictions = VC.predict(X_train)\nscores = cross_val_score(VC, X_train, y_train, cv=10, scoring='accuracy')\nprint('Training set accuracy is : ', accuracy_score(y_train, predictions)*100)\nprint('Validation set accuracy is :', scores.mean()*100)","47897e28":"predictions = RFC.predict(X_test).astype(int)\nsubmission = pd.DataFrame({'PassengerId': testing_set.PassengerId, 'Survived': predictions})\nsubmission.to_csv('.\/final_result.csv', index=False)","8b86972f":"# **Data Exploration**","4d820e0d":"# **Feature Engineeing**","ca97df03":"# **Importing the datasets**","8a23a2e2":"**8. LinearSVC**","bf4e345f":"**2. SGDClassifier**","09fd19e8":"**6. RandomForestClassifier**","b2a444e9":"**7. SVM**","93ac2ba5":"# **Training different classifiers**\n**1. Logistic Regression**","0e29f055":"**5. KNeighborsClassifier**","19392736":"**4. DecisionTreeClassifier**","e657513e":"**3. GaussianNB**"}}