{"cell_type":{"a4666d18":"code","2f060120":"code","d467884d":"code","e3390494":"code","1b3e6be5":"code","96f98da2":"code","d82f838b":"code","c639f9d5":"code","08182e68":"code","aa74c55a":"code","eba942ae":"code","a2cadc70":"code","1bec2c19":"code","81f3213f":"code","41a0546b":"code","34acb6df":"code","0c219cdf":"code","d9c74285":"code","1525153e":"code","0084cdaa":"code","d954352d":"code","2767ed17":"code","5ad96396":"code","cfe73f8d":"code","0e003f55":"code","22ac889b":"code","80e49fe1":"code","7ed09ebc":"code","363fe672":"markdown","d678d9e8":"markdown","fef904b7":"markdown","25cbaba0":"markdown","4feb0a2a":"markdown","a73e1ce9":"markdown","1aad66f4":"markdown"},"source":{"a4666d18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f060120":"sample_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","d467884d":"sample_df","e3390494":"train_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")","1b3e6be5":"train_df","96f98da2":"train_df.info()","d82f838b":"# get_dummies\u3067\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092\u30c0\u30df\u30fc\u5909\u6570\u306b\u5909\u63db\u3059\u308b\ntrain_df = pd.get_dummies(train_df, columns=['RoofMatl', 'KitchenQual', 'HouseStyle'])","c639f9d5":"train_df.shape","08182e68":"# \u6b20\u640d\u5024\u3092\u5e73\u5747\u5024\u3067\u57cb\u3081\u308b\ntrain_df['LotFrontage'] = train_df['LotFrontage'].fillna(train_df['LotFrontage'].mean())","aa74c55a":"# \u7cbe\u5ea6\u4e0a\u6607\u306b\u5bc4\u4e0e\u3059\u308b\u7279\u5fb4\u91cf\u3092\u9078\u629e\u3059\u308b\nfeatures = [\n    \"MSSubClass\",\n    \"OverallQual\",\n    \"LotArea\",\n    \"YearBuilt\",\n    \"YearRemodAdd\",\n    'KitchenQual_Ex', \n    'KitchenQual_Fa',\n    'KitchenQual_Gd',\n    'KitchenQual_TA',\n    # 'RoofMatl_ClyTile',\n    # 'RoofMatl_CompShg',\n    # 'RoofMatl_Membran',\n    # 'RoofMatl_Metal', \n    # 'RoofMatl_Roll',\n    # 'RoofMatl_Tar&Grv', \n    # 'RoofMatl_WdShake', \n    # 'RoofMatl_WdShngl',\n    'HouseStyle_1.5Fin',\n    'HouseStyle_1.5Unf', \n    'HouseStyle_1Story',\n#     'HouseStyle_2.5Fin',\n    'HouseStyle_2.5Unf',\n    'HouseStyle_2Story',\n    'HouseStyle_SFoyer',\n    'HouseStyle_SLvl',\n    # 'LotFrontage'\n    # 'Utilities_AllPub',\n    # 'Utilities_NoSeWa',\n    # 'PoolArea',\n    # 'SaleType_COD', \n    # 'SaleType_CWD', \n    # 'SaleType_Con',\n    # 'SaleType_ConLD',\n    # 'SaleType_ConLI', \n    # 'SaleType_ConLw', \n    # 'SaleType_New',\n    # 'SaleType_Oth',\n    # 'SaleType_WD',\n    '1stFlrSF',\n    '2ndFlrSF',\n    'TotalBsmtSF',\n    'OverallCond',\n]","eba942ae":"X = train_df[features].values\ny = train_df[\"SalePrice\"].values","a2cadc70":"X","1bec2c19":"y","81f3213f":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nr2_scores = []\nrmse_scores = []\ntree_scores = []\n\nkf = KFold(n_splits=10, shuffle=True, random_state=1)\nfor tr_idx, va_idx in kf.split(X):\n    tr_x, va_x = X[tr_idx], X[va_idx]\n    tr_y, va_y = y[tr_idx], y[va_idx]\n\n    lr = LinearRegression()\n    lr.fit(tr_x, tr_y)\n    va_pred = lr.predict(va_x)\n    rmse_score = np.sqrt(mean_squared_error(va_y, va_pred))\n    rmse_scores.append(rmse_score)\n\n    r2_score = lr.score(va_x, va_y)\n    r2_scores.append(r2_score)\n\n    tree = DecisionTreeRegressor(max_depth=6)\n    tree.fit(tr_x, tr_y)\n    tree_score = tree.score(va_x, va_y)\n    tree_scores.append(tree_score)\n\n    \n\nprint(np.mean(rmse_scores))\nprint(np.mean(r2_scores))\nprint(np.mean(tree_scores))\n","41a0546b":"\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\n\nrmses = []\nr2s = []\n\nkf = KFold(n_splits=10, shuffle=True, random_state=1)\nfor tr_idx, va_idx in kf.split(X):\n    tr_x, va_x = X[tr_idx], X[va_idx]\n    tr_y, va_y = y[tr_idx], y[va_idx]\n    model = RandomForestRegressor(\n        n_estimators=1000,\n        criterion='mse',\n        random_state=1,\n        n_jobs=-1\n    )\n    model.fit(tr_x, tr_y)\n    va_pred = model.predict(va_x)\n    mse = mean_squared_error(va_y, va_pred)\n    rmse = np.sqrt(mse)\n    rmses.append(rmse)\n\n    r2 = r2_score(va_y, va_pred)\n    r2s.append(r2)\n       \n\nprint(np.mean(rmses))\nprint(np.mean(r2s))\n","34acb6df":"\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\n\n\nrmses = []\nr2s = []\n\nkf = KFold(n_splits=10, shuffle=True, random_state=1)\nfor tr_idx, va_idx in kf.split(X):\n    tr_x, va_x = X[tr_idx], X[va_idx]\n    tr_y, va_y = y[tr_idx], y[va_idx]\n    lgb_train = lgb.Dataset(tr_x, tr_y)\n    lgb_eval = lgb.Dataset(va_x, va_y)\n    params = {\n        # 'task': 'train',\n        # 'boosting_type': 'gbdt',\n        # 'objective': 'rmse',\n        'seed': 1,\n        'verbose': 1000,\n        'metric': {'rmse'},\n        'learning_rate': 0.05,\n        'num_iterations': 1000,\n        'num_leaves': 23,\n        'max_depth': 15,\n        'early_stopping_rounds': 100,\n    }\n    num_round = 1000\n    model = lgb.train(\n        params,\n        lgb_train,\n        num_boost_round=num_round,\n        # categorical_feature=features,\n        valid_names=['train', 'valid'],\n        valid_sets=[lgb_train, lgb_eval]\n    )\n    va_pred = model.predict(va_x)\n    mse = mean_squared_error(va_y, va_pred)\n    rmse = np.sqrt(mse)\n    rmses.append(rmse)\n\n    r2 = r2_score(va_y, va_pred)\n    r2s.append(r2)\n       \n\nprint(np.mean(rmses))\nprint(np.mean(r2s))\n","0c219cdf":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.1,\n    random_state=1\n)","d9c74285":"\nfrom keras.layers import Dense, Dropout\nfrom keras.models import Sequential\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom keras.callbacks import EarlyStopping\n\nmodel = Sequential()\nmodel.add(Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.compile(\n    loss='mse', \n    optimizer='adam',\n    metrics=['mae', 'mse'],\n)\nbatch_size = 128\nepochs = 300\nearly_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\nhistory = model.fit(\n    X_train, \n    y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=100,\n    validation_data=(X_test, y_test),\n)\nva_pred = model.predict(X_test)\nscore = np.sqrt(mean_squared_error(va_pred, y_test))\nprint(score)\nr2 = r2_score(va_pred, y_test)\nprint(r2)","1525153e":"import lightgbm as lgb\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test)\n\nparams = {\n    # 'task': 'train',\n    # 'boosting_type': 'gbdt',\n    # 'objective': 'rmse',\n    'seed': 1,\n    'verbose': 1000,\n    'metric': {'rmse'},\n    'learning_rate': 0.05,\n    'num_iterations': 1000,\n    'num_leaves': 23,\n    'max_depth': 15,\n    'early_stopping_rounds': 100,\n}\nnum_round = 1000\nmodel = lgb.train(\n    params,\n    lgb_train,\n    num_boost_round=num_round,\n    # categorical_feature=features,\n    valid_names=['train', 'valid'],\n    valid_sets=[lgb_train, lgb_eval]\n)","0084cdaa":"test_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","d954352d":"# get_dummies\u3067\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092\u30c0\u30df\u30fc\u5909\u6570\u306b\u5909\u63db\u3059\u308b\ntest_df = pd.get_dummies(test_df, columns=['RoofMatl', 'KitchenQual', 'HouseStyle'])","2767ed17":"my_submission = pd.DataFrame()","5ad96396":"my_submission['Id'] = test_df['Id']","cfe73f8d":"model.predict(test_df[features])","0e003f55":"np.exp(model.predict(test_df[features]))","22ac889b":"my_submission['SalePrice'] = model.predict(test_df[features])","80e49fe1":"my_submission.head()","7ed09ebc":"my_submission.to_csv('submission.csv', index=False)","363fe672":"MLP","d678d9e8":"lightgbm","fef904b7":"\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3067\u8272\u3005\u306a\u30e2\u30c7\u30eb\u3092\u8a66\u3059","25cbaba0":"\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8","4feb0a2a":"# \u6a19\u6e96\u5316\nfrom sklearn.preprocessing import StandardScaler\nstdsc = StandardScaler()\nstdsc.fit(X)\nX = stdsc.transform(X)","a73e1ce9":"\u7dda\u5f62\u56de\u5e30\uff0c\u6c7a\u5b9a\u6728","1aad66f4":"# \u63d0\u51fa\n\u5fae\u5dee\u3060\u304c\uff0clightgbm\u304c\u6700\u3082\u7cbe\u5ea6\u304c\u9ad8\u3044\u305f\u3081\uff0clightgbm\u3092\u63a1\u7528\u3059\u308b\uff0e"}}