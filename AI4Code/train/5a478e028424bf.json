{"cell_type":{"f9c3bc3c":"code","7e8f912a":"code","e8465a5d":"code","9188af39":"code","3b74acd8":"code","45c86fb1":"code","e248c668":"markdown","7c4c46a2":"markdown","99281f4b":"markdown","fcd8ecba":"markdown","05093b12":"markdown","e19df3b9":"markdown"},"source":{"f9c3bc3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.tree import DecisionTreeClassifier as skTree\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.tree import export_graphviz\nimport graphviz\nimport os\nfrom sklearn import datasets","7e8f912a":"#iris2 = datasets.load_iris()\n#pandas learn strat also .Catogorical\n\nprint(os.listdir(\"..\/input\/iris\"))\niris = pd.read_csv('..\/input\/iris\/Iris.csv')\nprint(iris.columns.values)","e8465a5d":"#Arranges data so that it can be inputed into sklearn using pandas\nx = (iris[['PetalLengthCm', 'PetalWidthCm']]).values\ny = (pd.Series(data = (iris[\"Species\"]).values)).map(lambda i: 0 if i == \"Iris-virginica\" else (1 if i == \"Iris-versicolor\" else 2)).values\n\n#Split the data into training and test blocks\n#y is the outcome and x is the predictor\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 2, stratify = y)\n\n#specify options for the tree (these can be adjusted to account for overfitting etc.) \ntree = skTree(criterion=\"gini\", max_depth=3, random_state = 1)\n#create the tree using the training data\nmodel = tree.fit(x_train, y_train)","9188af39":"#below is plotting\n\n#this combines training and test data\nx_combined = np.vstack((x_train,x_test))\ny_combined = np.hstack((y_train,y_test))\nplot_decision_regions(x_combined, y_combined, clf=tree)\n\n#these are labels for the plot\nplt.xlabel('PetalWidthCm')\nplt.ylabel('PetalLengthCm')\nplt.legend(loc = 'upper left')\nplt.show()","3b74acd8":"#ignore this stuff\n#from pydotplus import graph_from_dot_data\n#from sklearn.tree import export_graphviz\n#'SepalLengthCm' 'SepalWidthCm' 'PetalLengthCm' 'PetalWidthCm'\n#dont ignore the stuff below this comment tho\n\n#print treeplot\ndot_data = export_graphviz(tree, out_file=None)\ngraph = graphviz.Source(dot_data) \ngraph\n\n\n# Any results you write to the current directory are saved as output.","45c86fb1":"#this uses the 30% testing data to output predictions\npredict = model.predict(x_test)\n\n#this shows how many are correct or not correct\n#It is not labeled, but there are three outcome variables\n#100% accuracy (over-fitting) would have all the numbers on the diagonal (in the middle)\npd.crosstab(predict,y_test)\n\npredict2 = model.predict(x_train)\npd.crosstab(predict2,y_train)","e248c668":"Iris Classification Tree Example\nEthan Bartlett\n\nThis example uses a notebook to predict flower species from a flower dataset.\n\n\nImporting necessary libraries below.","7c4c46a2":"Above is a graphical representation of the tree showing sample partions, loss values, logic operands, branches, etc.","99281f4b":"The codeblocks below plot the decision tree -- so they go through all of the values.","fcd8ecba":"The codeblock below does a scatter-plot of two x variables. The classification variables are color coded.","05093b12":"The clodeblock below outputs predictions and then tabulates the success and failure. You want both succes and failure to be around the same proportion correct.","e19df3b9":"Download dataset from kaggle. This is the \"iris\" dataset that is commonly used. It is a dataset with 3 different species of flowers and measurements for certain flowers. Sepal and Petal length and width.\n\nIt is a small dataset, but it runs fast and the characteristics (features) of the flowers are biological, so it is easy to demonstrate whehter your model is working correctly or not."}}