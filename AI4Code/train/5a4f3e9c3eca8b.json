{"cell_type":{"5fc1e021":"code","5cb0d997":"code","fd6cf151":"code","91780e6a":"code","dfd3b0de":"code","9af4b4e4":"code","edfdfd64":"code","6e80871d":"code","e40f788a":"code","9850ea53":"code","9041fc3e":"code","4920963a":"code","31b18334":"code","5c380904":"code","6d915fbe":"code","8e282ce6":"code","c3dc5736":"code","c208b1aa":"code","542bb4db":"code","943e55af":"code","9f537b7e":"code","7e36703d":"code","3b492aaf":"code","59347b64":"code","3e549875":"code","d255d45b":"code","010a3506":"code","6250a316":"code","d5189cf4":"code","7b1d9225":"code","a56decfc":"code","63f5ac39":"code","362dd661":"code","43a59f34":"code","35d3d62a":"code","1c46918a":"code","fb328b7c":"code","6ac45d07":"code","1618e942":"code","da41cbbd":"code","f86810f8":"code","bf1e1f2d":"code","e5e86e57":"code","e185f784":"code","fd3bc938":"code","bd0af5e8":"code","d0a0a467":"code","1e4a2349":"code","30ab4816":"code","aedf98c7":"code","2e761df3":"code","f5fc9ded":"code","b14190cc":"code","729a6e2e":"code","7e61ea6d":"code","1873c9ac":"code","e08ce064":"code","7caee8ae":"code","8bba0b71":"code","65d2fdf8":"code","f66bef80":"code","636365e1":"code","6dc983c6":"code","7bfcdde5":"code","1b9a3174":"code","4fdf048f":"code","ab06d0a5":"code","b002d44e":"code","8d132073":"code","e8a27074":"code","64491e32":"code","a72b1532":"code","66f181e0":"code","9b8ddcfb":"code","69ca7631":"code","2dcbc19d":"code","d585940a":"code","8a887cae":"code","4ac99c5a":"code","1ecee8b2":"code","a292f09b":"code","0aa9a543":"code","e90ac8d4":"code","b2497aa5":"code","cd87e437":"code","a23b9380":"code","8441e2c6":"code","b5c40f39":"markdown","48923410":"markdown","1e3f7da5":"markdown","fffcbe00":"markdown","7e787bad":"markdown","0e47cb71":"markdown","6a047b0f":"markdown","95a30c78":"markdown","4eb4721d":"markdown","e858b984":"markdown","31cf88b6":"markdown","959b8a3f":"markdown","4edc9d22":"markdown","0c06aa6e":"markdown","cb53cba0":"markdown","c772715b":"markdown","13b86075":"markdown","35065775":"markdown","d5f4be65":"markdown","091aa5af":"markdown","92be9c35":"markdown","856f5b12":"markdown","14389554":"markdown","0c01bdb6":"markdown","659440fa":"markdown","003523f9":"markdown","212b0149":"markdown","8bf1f738":"markdown","64515a28":"markdown","2875aafc":"markdown","c0fe8f4e":"markdown","4ce49be4":"markdown","a68788c4":"markdown","14c0821e":"markdown","dd24fb71":"markdown","c806bdad":"markdown","82f8edfa":"markdown","47fb7f9d":"markdown","ec0c50de":"markdown","51d8bc5d":"markdown","36d2f32b":"markdown","19dfbe57":"markdown","dd22580a":"markdown","b2e34bba":"markdown","a0cbea94":"markdown","4a56a0cd":"markdown","2ad844b3":"markdown","5c6870eb":"markdown","f04dceca":"markdown","2e2d8b54":"markdown","8e25654d":"markdown","eda01530":"markdown","682984d9":"markdown","afdfbf95":"markdown","8b295da8":"markdown","68e4732c":"markdown","54ddb827":"markdown","cb00ab51":"markdown","9fd3dba6":"markdown","d8316f0d":"markdown","fa9ef6dd":"markdown","27195d4e":"markdown","c6eb8ec4":"markdown","7d386789":"markdown","8455b386":"markdown","0bf5ddd8":"markdown"},"source":{"5fc1e021":"import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import roc_auc_score, f1_score, fbeta_score, precision_score, recall_score\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom tqdm.notebook import tqdm\nimport csv\nfrom gensim.models.callbacks import CallbackAny2Vec\nfrom itertools import groupby\nfrom gensim.models import Word2Vec\nimport gensim\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport os\nimport random\npd.options.display.max_columns = 999","5cb0d997":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fd6cf151":"PATH = Path('\/kaggle\/input\/imdb-dataset\/')","91780e6a":"def load_data(start_year, min_minutes, min_votes):\n    title_basics = pd.read_csv(PATH \/ 'title.basics.tsv' \/ 'title.basics.tsv', sep='\\t')\n    title_ratings = pd.read_csv(PATH \/ 'title.ratings.tsv'\/ 'title.ratings.tsv', sep='\\t')\n    title_basics.genres = title_basics.genres.apply(\n                        lambda x: x.split(',') if ((type(x)!=float) & (x!=r'\\N')) else ['no_genre'])\n\n    title_basics.runtimeMinutes = (\n     title_basics.runtimeMinutes.apply(lambda x: np.nan if not x.isdigit() else x).astype(float)\n                                )\n    \n    title_basics = title_basics[\n        title_basics.titleType.isin(['movie'])\n        & ~title_basics.runtimeMinutes.isna()\n        & (title_basics.runtimeMinutes <= 3.5 * 60)\n        & title_basics.genres.apply(lambda x: 'Short' not in x)\n        ]\n    \n    movies = pd.merge(title_basics, title_ratings, on='tconst', how='left')\n    movies['startYear'] = movies['startYear'].apply(lambda x: np.nan if x == r'\\N' else int(x))\n    \n    # MY CONDITIONS:\n    movies = movies[movies.startYear > start_year].dropna(subset=['averageRating'])\n    movies = movies[movies.runtimeMinutes >= min_minutes]\n    movies = movies[movies.numVotes>=min_votes]\n    return movies","dfd3b0de":"movies = load_data(start_year = 1960, min_minutes = 60, min_votes = 15)","9af4b4e4":"principals = pd.read_csv(PATH \/'title.principals.tsv'\/'title.principals.tsv',delimiter=\"\\t\")","edfdfd64":"cast = principals[principals['category'].isin(['actor', 'actress'])]\ncrew = principals[~principals['category'].isin(['actor', 'actress'])]","6e80871d":"ordered_cast = pd.DataFrame(cast.groupby(by='tconst').apply(lambda x: [x for x in x['nconst']]))\nordered_cast = ordered_cast.reset_index().rename(columns={0: 'cast'})\nordered_cast.head(2)","e40f788a":"# If it went right, we should have no duplicated movies (tconst)\nordered_cast.duplicated('tconst').any()","9850ea53":"ordered_crew = pd.DataFrame(crew.groupby(by='tconst').apply(lambda x: [x for x in x['nconst']]))\nordered_crew = ordered_crew.reset_index().rename(columns={0: 'crew'})\nordered_crew.head(2)","9041fc3e":"# If it went right, we should have no duplicated movies (tconst)\nordered_crew['tconst'].value_counts().max()","4920963a":"movies = pd.merge(movies, ordered_cast, on='tconst', how='left').merge(ordered_crew, on='tconst', how='left')\n\n# If it went right, we should have no duplicated movies (tconst)\nmovies.duplicated('tconst').any()","31b18334":"# CHECK: Back to The Future should have Fox and C. Lloyd within Cast, and Robert Zemeckis within Crew.\nmovies[movies['tconst']=='tt0088763']","5c380904":"movies['cast'].isna().sum()","6d915fbe":"movies['crew'].isna().sum()","8e282ce6":"movies[movies['cast'].isna()].head(2)","c3dc5736":"movies['cast'].fillna('Unknown', inplace=True)\nmovies['crew'].fillna('Unknown', inplace=True)","c208b1aa":"to_wtv = pd.DataFrame(principals.groupby(by='tconst').apply(lambda x: [x for x in x['nconst']])).reset_index().iloc[:, 1]\nto_wtv.head(3)","542bb4db":"wtv = Word2Vec(window=10, iter=10, min_count=5)\nwtv.build_vocab(to_wtv)\nwtv.train(to_wtv, total_words=wtv.corpus_total_words, epochs=10)","943e55af":"from IPython.display import HTML, display\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef get_name(id):\n    response = requests.get(f'https:\/\/www.imdb.com\/name\/{id}\/')\n    soup = BeautifulSoup(response.content)\n    return soup.select('.header .itemprop')[0].text\n\ndef get_image(id):\n    response = requests.get(f'https:\/\/www.imdb.com\/name\/{id}\/')\n    soup = BeautifulSoup(response.content)\n    candidates = soup.select('#name-poster')\n    return candidates[0].attrs['src'] if candidates else 'https:\/\/m.media-amazon.com\/images\/G\/01\/imdb\/images\/nopicture\/medium\/name-2135195744._CB466677935_.png'\n\ndef render_person(id):\n    name = get_name(id)\n    picture = get_image(id)\n    return f\"\"\"\n    <div style=\"width: 150px; text-align: center\">\n        <h4 style='margin-top: -5px'>{name}<\/h4>\n        <div style='font-size:75%; margin-bottom: 5px'>{id}<\/div>\n        <a href=\"https:\/\/www.imdb.com\/name\/{id}\" target=\"_blank\">\n            <img style=\"width: 100px; display: block; margin-left: auto; margin-right: auto;\" src=\"{picture}\"\/>\n        <\/a>\n    <\/div>\n    \"\"\"\n\ndef show_similars(id, n=10):\n    if id in wtv.wv: \n        display(HTML(render_person(id)))\n    renders = []\n    for similar_id, score in wtv.wv.most_similar(id, topn=n):\n        renders.append(render_person(similar_id))\n        \n    carousel = ''.join(\n        [\n            f'<div style=\"margin-left: 10px; float: left\">{p}<\/div>' \n            for p in renders\n        ]\n        )\n    display(HTML(f'<div style=\"width: 1800px\">{carousel}<\/div>'))\n\ndef show_similars_tovector(id, n=10):\n    renders = []\n    \n    for similar_id, score in wtv.wv.most_similar(id, topn=n):\n        renders.append(render_person(similar_id))\n        \n    carousel = ''.join(\n        [\n            f'<div style=\"margin-left: 10px; float: left\">{p}<\/div>' \n            for p in renders\n        ]\n    )\n    display(HTML(f'<div style=\"width: 1800px\">{carousel}<\/div>'))\n","9f537b7e":"show_similars('nm0000138')","7e36703d":"show_similars('nm0000849')","3b492aaf":"show_similars('nm1297015')","59347b64":"random_actor = random.sample(list(principals.nconst.unique()), 1)[0]","3e549875":"show_similars('nm4643289')","d255d45b":"wtv.wv['nm0000138']","010a3506":"def who_doesnt_match(person1, person2, person3, person4):\n    \n    p1 = str(person1); p2=str(person2); p3=str(person3); p4=str(person4); \n    result = wtv.wv.doesnt_match([p1, p2, p3, p4])\n    \n    if result in wtv.wv: \n        display(HTML(render_person(result)))","6250a316":"angelina_jolie= 'nm0001401'\nsean_penn = 'nm0000576'\nryan_gosling = 'nm0331516'\nrandom1 = random.sample(list(principals.nconst.unique()), 1)[0]\nrandom2 = random.sample(list(principals.nconst.unique()), 1)[0]\nrandom3 = random.sample(list(principals.nconst.unique()), 1)[0]","d5189cf4":"print(random1, random2, random3)","7b1d9225":"who_doesnt_match(angelina_jolie, random1, random2, random3)","a56decfc":"who_doesnt_match(sean_penn, random1, random2, random3)","63f5ac39":"who_doesnt_match(ryan_gosling, random1, random2, random3)","362dd661":"paul_newman = 'nm0000056'\njoseph_gordon_levitt = 'nm0330687'\njennifer_lawrence = 'nm2225369'\nrooney_mara = 'nm1913734'","43a59f34":"who_doesnt_match(paul_newman, joseph_gordon_levitt, jennifer_lawrence, rooney_mara)","35d3d62a":"christian_bale = 'nm0000288'\nwill_ferrell = 'nm0002071'\nseth_rogen = 'nm0736622'\nadam_sandler = 'nm0001191'","1c46918a":"who_doesnt_match(christian_bale, will_ferrell, seth_rogen, adam_sandler)","fb328b7c":"default_vector = np.mean(wtv.wv.vectors, axis=0)\nshow_similars_tovector([default_vector])","6ac45d07":"default_vector = np.zeros(100)\nshow_similars_tovector([default_vector])","1618e942":"np.percentile(movies.numVotes, 99)","da41cbbd":"movies_docs = movies.to_dict(orient='records')\nmovies['Classic'] = [1 if x['numVotes']>np.percentile(movies.numVotes, 99) else 0 for x in movies_docs]","f86810f8":"len(movies[movies['Classic']==1]) \/ len(movies)","bf1e1f2d":"movies[movies['Classic']==1].sample(5)","e5e86e57":"class W2VFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, wtv, category, min_cnt_movies=2):\n        self.category = category\n        self.min_cnt_movies = min_cnt_movies\n        self.wtv = wtv\n\n    def fit(self, X, y):\n        self.default_vector = np.zeros(100)\n        #self.default_vector = np.mean(wtv.wv.vectors, axis=0)\n        return self\n    \n    def _get_movie_vector(self, x_i):\n        vectors = []\n        for person in x_i[self.category]:\n            if person not in self.wtv.wv or self.wtv.wv.vocab[person].count < self.min_cnt_movies: continue\n            vectors.append(self.wtv.wv[person])\n            \n        if len(vectors) == 0:\n            return self.default_vector\n        else:\n            return np.mean(vectors, axis=0)\n            \n    def transform(self, X):\n        return np.asarray([self._get_movie_vector(x_i) for x_i in X])","e185f784":"movies.runtimeMinutes.corr(np.log(movies.numVotes))","fd3bc938":"class RunTime(BaseEstimator, TransformerMixin):\n    def fit(self, X, y): return self\n    def transform(self, X):\n        res = []\n        for e in X:\n            res.append({'runTime': int(e['runtimeMinutes'])})\n        return res","bd0af5e8":"class GenreDummies(BaseEstimator, TransformerMixin):\n    def fit(self, X, y): return self\n    def transform(self, X):\n        res = []\n        for e in X:\n            res.append({g: 1 for g in e['genres']})\n        return res  ","d0a0a467":"v = DictVectorizer(sparse=False)\ndummies_genre = v.fit_transform(GenreDummies().transform(movies_docs))","1e4a2349":"df_genres = pd.DataFrame(dummies_genre, columns=v.feature_names_)\ndf_genres = df_genres.astype(int)\ngenres_analysis = pd.concat([movies, df_genres], axis=1)\ngraph1 = pd.DataFrame(genres_analysis[genres_analysis['numVotes'] > np.percentile(genres_analysis.numVotes,97)].iloc[:,14:-1] \\\n                     .sum(axis=0) \/ len(genres_analysis[genres_analysis['numVotes'] > \\\n                    np.percentile(genres_analysis.numVotes,97)])).reset_index().rename(columns={'index': 'genre', 0: 'pct'})\ngraph1['classic']=\"Classic\"\ngraph2 = pd.DataFrame(genres_analysis.iloc[:,14:-1] \\\n                     .sum(axis=0) \/ len(genres_analysis)).reset_index().rename(columns={'index': 'genre', 0: 'pct'})\ngraph2['classic'] = \"All\"\ngraph = pd.concat([graph1, graph2], axis=0) ","30ab4816":"import seaborn as sns\nsns.set_style(\"white\")\nfig, ax = plt.subplots(figsize=(14,20))\nax = sns.barplot(y='genre', x='pct', hue='classic', data=graph, palette='viridis')\nax.grid(color='grey', linestyle='-', linewidth=0.1, axis='x')\nax.set_xticks([0.05, 0.10, .15, .20, .25, .30, .35, .40, .45, .50])\nax.set_yticklabels(graph.genre[:23], size = 13, fontfamily='serif')\nax.set_xlabel('% of total movies', fontsize=15)\nax.set_ylabel('Genre', fontsize=15, fontfamily='serif')\nax.tick_params(labelbottom=True,labeltop=True)\nplt.title('% of total movies per genre (1960-2020)', fontsize=20)","aedf98c7":"genres_prog = genres_analysis.groupby(by='startYear').agg({'Horror': 'sum', 'Comedy': 'sum', \\\n                                                           'Drama':'sum', 'Sci-Fi':'sum', 'tconst': 'count'})\ngenres_prog.reset_index(inplace=True)\ngenres_prog['%_horror'] = genres_prog['Horror'] \/ genres_prog['tconst'] * 100\ngenres_prog['%_comedy'] = genres_prog['Comedy'] \/ genres_prog['tconst'] * 100\ngenres_prog['%_drama'] = genres_prog['Drama'] \/ genres_prog['tconst'] * 100\ngenres_prog['%_scifi'] = genres_prog['Sci-Fi'] \/ genres_prog['tconst'] * 100\ngenres_prog = genres_prog.iloc[:-1,:]","2e761df3":"ax = plt.subplots(figsize=(18,8))\nax = sns.lineplot(x=genres_prog['startYear'], y=genres_prog['%_horror'], legend='brief')\nax.set_xlabel('Year', fontsize=15)\nax.set_ylabel('# of Horror films per 100 movies', fontsize=15)\nax.grid(color='grey', linestyle='-', linewidth=0.1)\nax.set_xticks([1965, 1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020])\nplt.title('Progression of Horror films through time', fontsize=16)","f5fc9ded":"ax = plt.subplots(figsize=(18,8))\nax = sns.lineplot(x=genres_prog['startYear'], y=genres_prog['%_scifi'], legend='brief')\nax.set_xlabel('Year', fontsize=15)\nax.set_ylabel('# of Sci-Fi films per 100 movies', fontsize=15)\nax.grid(color='grey', linestyle='-', linewidth=0.1)\nax.set_xticks([1965, 1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020])\nplt.title('Progression of Sci-Fi films through time', fontsize=16)","b14190cc":"class ReleaseYear(BaseEstimator, TransformerMixin):\n    def fit(self, X, y): return self\n    def transform(self, X):\n        res = []\n        for e in X:\n            res.append({'release_year': int(e['startYear'])})\n        return res","729a6e2e":"ax = plt.subplots(figsize=(18,8))\nax = sns.lineplot(x=movies['startYear'], y=np.log(movies['numVotes']))\nax.grid(color='grey', linestyle='-', linewidth=0.1)\nplt.title('Year vs. logVotes')","7e61ea6d":"train_df = movies[movies.startYear.isin(range(1975,2017))]\ntest_df = movies[movies.startYear.isin(range(2017,2020))]\nlen(train_df), len(test_df), len(test_df) \/ len(train_df)","1873c9ac":"print(len(train_df[train_df['Classic']==1]) \/ len(train_df))\nprint(len(test_df[test_df['Classic']==1]) \/ len(test_df))","e08ce064":"train_docs = train_df.to_dict(orient='records')\ntest_docs = test_df.to_dict(orient='records')","7caee8ae":"y_train = (train_df.Classic).values\ny_test = (test_df.Classic).values","8bba0b71":"def test_pipe(pipe):\n    precision, recall, _ = precision_recall_curve(y_test, pipe.predict_proba(test_docs)[:, 1])\n    pr_auc_score = auc(recall, precision)\n    return {\n        'train_auc': roc_auc_score(y_train, pipe.predict_proba(train_docs)[:, 1]),\n        'test_auc': roc_auc_score(y_test, pipe.predict_proba(test_docs)[:, 1]),\n        'f1':f1_score(y_test, pipe.predict(test_docs)),\n        'precision':precision_score(y_test, pipe.predict(test_docs)),\n        'recall':recall_score(y_test, pipe.predict(test_docs)),\n        'pr_auc_score_testing': pr_auc_score\n        }\n\ndef see_preds(pipe):\n    preds = pipe.predict_proba(test_docs)\n    vis = test_df[['tconst','primaryTitle','startYear','runtimeMinutes','genres','numVotes','averageRating', 'Classic']]\n    vis['prob_True'] = [preds[i][1] for i in range(len(preds))]\n    return vis","65d2fdf8":"def get_features_pipe():\n    steps = []\n    steps.append(make_pipeline(W2VFeatures(wtv, category='cast', min_cnt_movies=3)))\n    steps.append(make_pipeline(W2VFeatures(wtv, category='crew', min_cnt_movies=3)))\n    steps.append(make_pipeline(RunTime(), DictVectorizer(sparse=False)))\n    steps.append(make_pipeline(GenreDummies(), DictVectorizer(sparse=False)))\n    steps.append(make_pipeline(ReleaseYear(), DictVectorizer(sparse=False)))\n    res = make_union(*steps)\n    return res\n\ndef get_model_pipe(features_pipe, scaler, estimator):\n    return make_pipeline(features_pipe, scaler, estimator)","f66bef80":"features_pipe=get_features_pipe()\nlogistic_model = get_model_pipe(\n                        features_pipe,\n                        scaler = StandardScaler(), \n                        estimator= LogisticRegression(max_iter=400\n                                                     ))\nlogistic_model.fit(train_docs, y_train)","636365e1":"results = test_pipe(logistic_model)\nresults","6dc983c6":"print('We get an outstanding ROC AUC score of {}% in our testing set. We have a Precision of {}% and a Recall of {}%, both very good values considering that only 0.6% of the examples in the testing set are positive. So there are very few movies that got this number of votes in our test data, but we can correctly identify {}% of them with our simple Logistic Regression model. Finally, we get a pr-auc in testing of {}.'.format(round(results['test_auc']*100,1), round(results['precision']*100,1), \n        round(results['recall']*100,1), round(results['recall']*100,1), round(results['pr_auc_score_testing'], 3)))","7bfcdde5":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.metrics import roc_auc_score, average_precision_score, precision_score, recall_score, fbeta_score, precision_recall_curve, auc\nimport lightgbm as lgbm\n\ndef evaluate_model(params):\n   \n    parameters = {\n                    'num_leaves':params['num_leaves'], \n                    'objective':'binary',\n                    'max_depth':params['max_depth'],\n                    'learning_rate':params['learning_rate'],\n                    'max_bin':params['max_bin'], \n                    'metric': ['auc', 'binary_logloss']\n                     }\n\n    pipe = get_model_pipe(features_pipe,\n                        scaler = StandardScaler(), \n                        estimator= lgbm.LGBMClassifier(**parameters)\n                          )\n    pipe.fit(train_docs, y_train)\n    \n    precision, recall, _ = precision_recall_curve(y_test, pipe.predict_proba(test_docs)[:, 1])\n    pr_auc_score = auc(recall, precision)\n    \n    return {\n        'num_leaves': params['num_leaves'],\n        'max_depth': params['max_depth'],\n        'learning_rate': params['learning_rate'],\n        'max_bin': params['max_bin'],\n        'Training ROC-AUC': round(roc_auc_score(y_train, pipe.predict_proba(train_docs)[:, 1]),3),\n        'Testing ROC-AUC':round(roc_auc_score(y_test, pipe.predict_proba(test_docs)[:, 1]),3),\n        'Testing PR-AUC':round(pr_auc_score,3),\n        'Precision': round(precision_score(y_test, pipe.predict(test_docs), zero_division=1), 3),\n        'Recall': round(recall_score(y_test, pipe.predict(test_docs)), 3)     ,\n        }\n\ndef objective(params):\n    res = evaluate_model(params)\n    res['loss'] = - res['Testing PR-AUC']\n    res['status'] = STATUS_OK\n    return res \n\nhyperparameter_space = {\n        'learning_rate': hp.uniform('learning_rate', 0.001, 0.3),\n        'num_leaves': hp.choice('num_leaves', range(30, 270)),\n        'max_depth': hp.choice('max_depth', range(3, 15)),\n        'max_bin': hp.choice('max_features', range(20, 380)),\n}","1b9a3174":"trials = Trials()\nbest = fmin(\n    objective,\n    space=hyperparameter_space,\n    algo=tpe.suggest,\n    max_evals=50,\n    trials=trials\n);","4fdf048f":"lgbm_results = pd.DataFrame(trials.results)\nlgbm_results.sort_values(by='loss').head(5)","ab06d0a5":"# Final LGBM Model:\nparameters = {'num_leaves': 108, \n                'objective':'binary',\n                'max_depth': 12,\n                'learning_rate': 0.019,\n                'max_bin': 363, \n                'metric': ['auc', 'binary_logloss']}\n\nlgbm_model = get_model_pipe(features_pipe,\n                        scaler = StandardScaler(), \n                        estimator= lgbm.LGBMClassifier(**parameters))\nlgbm_model.fit(train_docs, y_train)","b002d44e":"def plot_roc_curve(y_test, naive_probs, log_model_probs, lgbm_model_probs):\n    sns.set_style('white')\n    fig, ax = plt.subplots(figsize=(14,8))\n    # plot naive skill roc curve\n    precision, recall, _ = precision_recall_curve(y_test, naive_probs)\n    ax = sns.lineplot(recall, precision, label='No Skill')\n    # plot log model roc curve\n    precision, recall, _ = precision_recall_curve(y_test, log_model_probs)\n    ax = sns.lineplot(recall, precision, markers=True, ci=False, label='Logistic Regression')\n    # plot lgbm model roc curve\n    precision, recall, _ = precision_recall_curve(y_test, lgbm_model_probs)\n    ax = sns.lineplot(recall, precision, markers=True, ci=False, label='LightGBM')\n    # axis labels\n    ax.set_xlabel('Recall')\n    ax.set_ylabel('Precision')\n    ax.set_yticks(np.arange(0,1.1,0.1))\n    ax.set_xticks(np.arange(0,1.1,0.1))\n    ax.grid(color='black', linestyle='-', linewidth=0.1)\n    # show the legend\n    plt.legend()\n    plt.title('Precision-Recall Curves', fontsize=15)\n    # show the plot\n    plt.show()","8d132073":"from sklearn.dummy import DummyClassifier\nnaive_model = DummyClassifier(strategy='stratified')\nnaive_model.fit(train_docs, y_train)\nnaive_probs = naive_model.predict_proba(test_docs)[:, 1]\nlog_model_probs = logistic_model.predict_proba(test_docs)[:, 1]\nlgbm_model_probs = lgbm_model.predict_proba(test_docs)[:, 1]","e8a27074":"plot_roc_curve(y_test, naive_probs, log_model_probs, lgbm_model_probs)","64491e32":"see_preds(lgbm_model).sort_values(by='numVotes', ascending=False).head(10)","a72b1532":"see_preds(lgbm_model).sample(5)","66f181e0":"def get_features_pipe():\n    steps = []\n    steps.append(make_pipeline(RunTime(), DictVectorizer(sparse=False)))\n    steps.append(make_pipeline(GenreDummies(), DictVectorizer(sparse=False)))\n    steps.append(make_pipeline(ReleaseYear(), DictVectorizer(sparse=False)))\n    res = make_union(*steps)\n    return res\n\nfeatures_pipe=get_features_pipe()\nmodel = get_model_pipe(\n                        features_pipe,\n                        scaler = StandardScaler(), \n                        estimator= lgbm.LGBMClassifier(**parameters))\n                                                   \nmodel.fit(train_docs, y_train)\n\nresults = test_pipe(model)\nresults","9b8ddcfb":"def get_features_pipe():\n    steps = []\n    steps.append(make_pipeline(W2VFeatures(wtv, category='cast', min_cnt_movies=3)))\n    steps.append(make_pipeline(W2VFeatures(wtv, category='crew', min_cnt_movies=3)))\n    res = make_union(*steps)\n    return res\n\nfeatures_pipe=get_features_pipe()\nmodel = get_model_pipe(\n                        features_pipe,\n                        scaler = StandardScaler(), \n                        estimator= lgbm.LGBMClassifier(**parameters))\n                                                     \nmodel.fit(train_docs, y_train)\n\ntest_pipe(model)","69ca7631":"y_train = train_df.numVotes\ny_test = test_df.numVotes","2dcbc19d":"thresholds = [\n        np.percentile(movies.numVotes, 10), np.percentile(movies.numVotes, 25), np.percentile(movies.numVotes, 40), \n        np.percentile(movies.numVotes, 55), np.percentile(movies.numVotes, 70), np.percentile(movies.numVotes, 80), \n        np.percentile(movies.numVotes, 85), np.percentile(movies.numVotes, 87.5), np.percentile(movies.numVotes, 90), \n        np.percentile(movies.numVotes, 91.5), np.percentile(movies.numVotes, 93), np.percentile(movies.numVotes, 95),\n        np.percentile(movies.numVotes, 96.5), np.percentile(movies.numVotes, 98), \n        np.percentile(movies.numVotes, 99), np.percentile(movies.numVotes, 99.25)\n             ]","d585940a":"def get_bools(y):\n    res = []\n    for t in thresholds:\n        res.append(y >= t)\n    return res\n\nys_train = get_bools(y_train)\nys_test = get_bools(y_test)","8a887cae":"models = [ get_model_pipe(\n        features_pipe=get_features_pipe(),\n        scaler=StandardScaler(),\n        estimator=lgbm.LGBMClassifier(**parameters))  \n    for _ in range(len(thresholds)) ]","4ac99c5a":"for i, m in enumerate(models):\n    m.fit(train_docs, ys_train[i])","1ecee8b2":"from random import randint\nfrom itertools import compress\nfrom PIL import Image\nfrom io import BytesIO\n\ndef get_movie_image(id):\n    response = requests.get(f'https:\/\/www.imdb.com\/title\/{id}\/')\n    soup = BeautifulSoup(response.content)\n    candidates = soup.find('img',)\n    return candidates.attrs['src'] if candidates else 'https:\/\/i2.wp.com\/www.fryskekrite.nl\/wordpress\/wp-content\/uploads\/2017\/03\/No-image-available.jpg'\n\ndef prob_dist(tconst):\n    sns.set_style(\"white\")\n    filt = [x['tconst']==tconst for x in test_docs]\n    movie = list(compress(test_docs, filt))[0]\n    \n    # We plot the movie image on the corner\n    response = requests.get(get_movie_image(tconst))\n    im = Image.open(BytesIO(response.content))\n    im = im.resize((int(im.size[0]*0.40), int(im.size[1]*0.36)))\n    height = im.size[1]\n    fig, ax = plt.subplots(figsize=(13,8))\n    fig.figimage(im, 50, fig.bbox.ymax-height*0.9)\n    \n    # The following is to make sure the prob distribution starts and ends with zero, and to remove the inconsistencies in which\n    # we can end up with Prob<0 if for example Prob(votes>Xi) < Prob(votes>X(i-1).\n    preds = np.asarray([m.predict_proba([movie])[0,1] for m in models])\n    preds = preds[:-1] - preds[1:]\n    preds = np.where(preds<0, 0, preds)\n    preds = np.insert(preds, 0,0)\n    thresholds_start = np.insert(thresholds,0,10)\n    \n    # We add more data points to the end of the numVotes range if this is a movie with a lot of votes:\n    if movie['numVotes']>np.percentile(movies.numVotes,99):\n        y_train_extra = (train_df.numVotes>np.percentile(movies.numVotes,99.5)).values\n        y_train_extra2 = (train_df.numVotes>np.percentile(movies.numVotes,99.9)).values\n        extra_model = get_model_pipe(\n                                        features_pipe=get_features_pipe(),\n                                        scaler=StandardScaler(),\n                                        estimator=LogisticRegression(max_iter=400)   \n                                    )\n        extra_model.fit(train_docs, y_train_extra)\n        preds = np.insert(preds, len(preds), extra_model.predict_proba([movie])[:,1])\n        extra_model.fit(train_docs, y_train_extra2)\n        preds = np.insert(preds, len(preds), extra_model.predict_proba([movie])[:,1])\n        preds = np.insert(preds, len(preds),0)\n        thresholds_start = np.insert(thresholds_start,len(thresholds_start),np.percentile(movies.numVotes,99.5))\n        thresholds_start = np.insert(thresholds_start,len(thresholds_start),np.percentile(movies.numVotes,99.9))\n        thresholds_start = np.insert(thresholds_start,len(thresholds_start),1500000)\n    \n    mids = [(x1 + x2) \/ 2 for x1, x2 in zip(thresholds_start[:-1], thresholds_start[1:])]\n    \n    ax = sns.lineplot(mids, preds, label='predicted distribution', marker=\"o\")\n    plt.plot([movie['numVotes'],movie['numVotes']], [0, preds.max()], color='seagreen', linewidth=1.5, label='true numVotes')\n    ax.grid(color='black', linestyle='-', linewidth=0.1)\n    plt.xscale(\"log\")\n    #plt.yticks(np.arange(0,0.75,0.05))\n    plt.legend(loc='best', fontsize=13)\n    plt.xlabel('Log(numVotes)', fontsize=14, fontfamily='serif')\n    plt.ylabel('Probabilty', fontsize=14, fontfamily='serif')\n    plt.suptitle('{originalTitle} \\n'.format(**movie), fontsize=24, fontfamily='serif')\n    plt.title(' \\n \\n \\n ({startYear:.00f}, {genres})\\n'.format(**movie), fontsize=16, fontfamily='serif')\n    ","a292f09b":"prob_dist('tt7131622')","0aa9a543":"prob_dist('tt4123430')","e90ac8d4":"prob_dist('tt5095030')","b2497aa5":"prob_dist('tt6323858')","cd87e437":"prob_dist('tt8106596')","a23b9380":"prob_dist('tt5563334')","8441e2c6":"prob_dist('tt7923374')","b5c40f39":"### **FAME**\n\nWe'll try with three random actors\/actresses and a famous one, who should be the one that does not belong if our model is capturing \"fame\" correctly hidden in those mysterious numbers of the vectors.","48923410":"We see that the number of votes increases until the beginning of the 00's and then slowly goes down. It seems to make sense to control for Year of Release of the movies.","1e3f7da5":"In both cases we get kind of obscure people, which is exactly what we would expect. Not sure what to decide here. For now let's use the np.zeros vector and later we check if we do better the other way.","fffcbe00":"Nice to see that results seem to make a lot of sense in most cases :) Let's now look for a random actor\/actress. If our model is coherent, we should get other random not well-known people in return.","7e787bad":"To end this notebook, we'll make use of the fact that our predictive model seems to work pretty well for drawing some approximations of probability distributions regarding movies' expected IMDB votes. To do this, we treat it as several different classification problems, in which the target in each case will be numVotes>Xi.","0e47cb71":"First let's see how balanced\/imbalanced the genres are in our data according to whether they are \"classic\" movies:","6a047b0f":"To better understand what our W2V Cast vectors are doing, let's see how well we would do without the Word2Vec features.","95a30c78":"# Feature Engineering","4eb4721d":"Manually checking these movies, I see that this is indeed the case. These movies have some people involved listed as \"self\", but nobody listed as actor\/actresses. Therefore it makes sense. Let's just fill these movies with 'Unknown' cast\/crew.","e858b984":"We'll use the following thresholds, one for each classification model we'll make. I don't use many thresholds among the first percentiles because they represent very low numbers of votes (apparently there are a lot of movies with less than 100 votes on IMDB). I use smaller intervals later, as the difference between the 95th and 96th percentile can be tens of thousands of votes.","31cf88b6":"Indeed, we get a bunch of random unknown people (at least to me), just as we hoped. Our random reference actor is actually called Jon Snow, which I find pretty funny.","959b8a3f":"We divide our \"principals\" dataset into \"cast\" (actors and actresses) and the rest.\nThe idea is to add to our Movies dataframe two columns: one with a list containing all actors\/actresses in the movie, and another including all other crew members who were involved in the movie.","4edc9d22":"We see that there is a weak\/moderate correlation between runtime and the number of votes, even though I really doubt this relation is linear (which is what the coefficient above measures). Anyway, looks like we should add RunTime to our model.","0c06aa6e":"Now we should decide what vector to estimate for cast\/crew that do not have a vector (because they did not appear in a sufficient minimum number of movies).\nI consider two obvious options:\n\n1- We take the np.mean vector of all people.\n\n2- We use a vector full of zeros.\n\nLet's see who are the most similar people to each of these two default vectors:","cb53cba0":"# Choosing proxy for movies' popularity","c772715b":"### RunTime","13b86075":"# Predictive Model","35065775":"**Let's check a few similarities**. For each actor\/actress, we see which others have the most similar vectors (cosine similarity). If we give the name of a famous person, it would make sense to get back other well-known actors\/actresses of around the same magnitude in return.","d5f4be65":"### Some examples!","091aa5af":"We should take into account that these are only approximations and not real probability distributions. For some reason sometimes the model may think that the probability of a movie receiving, let's say 50,000 votes is lower than receiving 100,000 votes but also lower than getting 20,000 votes. This is a bit weird and make some plots look like something that is not quite what one expects of a probability density. But does it make sense? Maybe it's not so crazy to predict that a movie will either get very low attention, or a lot of it, and the probability of receiving average number of votes or anything in between is low. With this being said, we can still see that the model sometimes is spot-on, predicting the right number of votes a movie gets almost perfectly.","92be9c35":"There may be some other things, like the Genre of a movie, its Runtime and the Year it was released, that affect the number of votes it gets. For this reason, it is better to include these control variables in order to better understand the effect that the cast has on the movie's popularity.","856f5b12":"We could keep trying things endlessly but let's end for now.\n\nIn summary, we have seen that using word2vec to embedd actors into vectors seem to work pretty well. It looks like these numbers contain information such as how famous the actor\/actress is, in which era he\/she was a star, and the kind of movies he\/she is usually in.\n\nRemember that these vectors were assembled by looking at who worked with whom across all movies in our data. So it seems that you can describe an actor\/actress pretty well by knowing who he\/she has worked with in the past!","14389554":"The following functions let us turn imdb id's into pictures and names of the cast\/crew:","0c01bdb6":"Now we add this information to our movies data.","659440fa":"The idea is to estimate one vector for each person who is involved in a movie. We use the other cast\/crew members of a movie as context for the algorithm to learn a vector for a certain person. We need to feed the Word2Vec with a list of sublists in which each of the latter contains the id's of the people involved in each movie. For example, something like:\n\n[nm0000138, nm0000701, nm0000708, nm0000870, nm0365239, nm0000116, nm0484457, nm0000035]","003523f9":"Our performance is reduced, so it seems that the *Genre* dummies, the *Runtime* and the *Release Year* were helping at least somewhat. But the lead stars of this predictive model are the vectors extracted from Word2Vec, as we can see we do decently well using those and nothing else.","212b0149":"The class below takes, for each movie, its entire cast or crew vectors, and then computes the mean of all of them. The result of this will be one 100-length vector that summarizes the cast (category='cast') or crew (category='crew') that was involved in the movie. We make the assumption that the mean of these vectors illustrates the movie's cast\/crew quality\/popularity as a whole.","8bf1f738":"# Model Selection, Predictions & Feature Importances","64515a28":"I'll use only movies after 1975 for training to reduce our data a bit and the needed computation power. To achieve a more realistic predictive scenario, I'll use movies from 1980-2016 for training, and movies from 2017-2019 for testing.","2875aafc":"Let's play a bit more. W2V has a method called \"doesn't match\" that returns which element from a list is the one that does not belong in the group. More technically, it computes the center of the group (the mean of all vectors) and then returns the one who is furthest away from this center. I think this is a great way to check which parts of the cast spectrum are covered by these vectors.\n\nLet's start by checking whether how \"famous\" the actors are is covered. We have seen already from the previous examples that this indeed seems to be covered.","c0fe8f4e":"### Cast & Crew","4ce49be4":"# For the future...","a68788c4":"We do however have some movies with no cast or crew. Let's check a couple.","14c0821e":"# Probability Distributions","dd24fb71":"Again we see it works pretty well.","c806bdad":"Let's see which are some of these *classic* movies we have.","82f8edfa":"We see that the production of Horror films decreased heavily during the 90's after the golden age of the genre in the 80's. It then started slowly increasing again after year 2000 and nowadays it seems to be quite popular again.","47fb7f9d":"I impose a couple of conditions to make the dataset more manageable:\n\n1- Only load movies from after 1960, as I suspect films older than that have a different voting pattern. The choice of 1960 as the cutoff is kind of subjective though.\n\n2- Only load movies > 60 minutes long. It appears that the convention of the minimum duration to be considered a movie is either 40 or 80 minutes depending on the source. So I take 60 minutes which is in the middle of both and makes sense to me.\n\n3- Only load movies with at least 15 votes. With this we get rid of entries that are not even worth looking into (less than 15 votes means that not even the people involved in the movie appears to have voted for it).","ec0c50de":"## Train\/Test Split","51d8bc5d":"We find some unknown movies which the model correctly identified as films with zero chance of getting >125,000 votes.","36d2f32b":"## LightGBM (Gradient Boosting Machine)","19dfbe57":"We have 1.2% of positive examples in our training set, and less than 0.6% in our testing set. This is not ideal, but I still prefer to train on older movies and test on more recent ones, as it is a more useful scenario in practical terms, so I'll keep it this way.","dd22580a":"### Year of Release","b2e34bba":"We find some very clear differences, mostly in the genres Action and Adventure. Action movies comprise around 12% of total movies, but almost 30% of these so-called *Classic* movies belong to the Action genre. We find a similar situation with the Adventure genre movies. This suggests that we must control for a movie's genre in the model.\n\nLet's see now how certain genres' popularity has evolved through time.","a0cbea94":"Of course looking at the vectors themselves make little sense to us humans. But they do appear to make sense to a computer, which is what makes this technique so interesting.\nHere's for example how Di Caprio's vector looks like:","4a56a0cd":"# Word2Vec","2ad844b3":"It would be interesting to try the following:\n\n- See how much our predictions could improve by adding more predictors that can easily be extracted from the data.\n\n- Try to predict other things, such as the films' ratings.\n\n- Identify clusters among actors\/actresses according to their vectors resulted from Word2Vec. \n\n- Consider alternatives other than simply taking the np.mean of all the cast vectors to use as predictors. For example, it would be interesting to try a weighted average, in which for example the vector of the *lead actor* is given double importance compared to the rest of the cast. The *director*'s vector can also have twice the weight compared to other crew members.\n","5c6870eb":"# Results from Word2Vec","f04dceca":"### Other features","2e2d8b54":"It worked.","8e25654d":"Now we build the Word2Vec model with *gensim*, which requires only a couple of lines of code. We use a window of 10, and 100 length vectors will be estimated for each person using the other people involved in each movie as context.","eda01530":"Maybe not all of these movies can really be considered actual \"classics\", but they do have a very large number of votes and are truly famous movies.","682984d9":"# Load Data","afdfbf95":"## Logistic Regression","8b295da8":"### Genre","68e4732c":"The movies above are the ones with the highest number of votes in the test data. Our model correctly identified all of them as movies that would get >125,000 votes, except for **Dunkirk**, in which case it largely failed to recognize it as a future *Classics*, probably because it casts mostly unknown actors despite being a Nolan movie. It also made the wrong prediction for **Logan**, but in that case the model was very close to making the right prediction (it predicted 49% chance of becoming a *classic*). Now let's check some random predictions.","54ddb827":"### **GENRE**\nTo end with, let's see if the models also picks up which kind of genres the actor\/actress is usually in. I'll include three (mostly) comedy actors and one more serious-type actor such as Christian Bale, who should not belong here.","cb00ab51":"I will use the number of IMDB votes as a proxy to measure a movie's popularity. The goal will be to predict which movies will be among the Top 1% in terms of IMDB votes ( > ~125,000 votes) based on its cast and crew. We'll call these kinds of movies a \"Classic\" in our dataset to easily differentiate them from the rest.","9fd3dba6":"## Using Word2Vec in non-NLP context\n\nIn this notebook, we enconde movies' cast and crew into vectors and use them  to predict the probability of any given movie being among the 1% with most IMDB votes.\n\nWhen a categorical variable can take such a huge number of possible values, such as the actors\/actresses involved in movies, it becomes hard to apply one-hot encoding and use every person as a dummy variable and not overfit badly. There are ways around this though. Another option is to compute features regarding each actor\/actress (such as avg number of votes in his\/her previous movies) and include those interval variables in a model. In this case we do something different, and try to learn representations for each actor\/actress (100 length vectors) using **Word2Vec**.\n\nEven though Word2Vec was originally designed to produce word embeddings, you can use it for any case in which the context matters. In this case, we use actors' and actresses' names (or imdb id's) as the words that would usually be the input of a **Word2Vec** model, and the other actors and actresses who appeared in the same movies, as *context words*.","d8316f0d":"And here I found a couple in which the model fails, just to show that even though we created a model that is pretty decent at identifying top 1% movies, we do not always make good predictions.","fa9ef6dd":"It sure looks like our model is pretty good at differentiating famous actors from the rest.","27195d4e":"There is no doubt that we at least do way better than random guessing. It also appears that our LGBM model outperforms the simple linear model from before. To summarize all this, let us pick the best values from hyperopt to confirm how our LGBM model looks like and then we plot the Precision-Recall curves of both LGBM and Log. Regression, as well as a Naive model to compare the performance of them all.","c6eb8ec4":"Now let's get a bit more serious and try a more complex and powerful algorithm, such as LGMB's Gradient Boosting Machine. We use hyperopt for adjusting the hyperparameters in a smart way. We'll use the Testing PR-AUC, instead of the ROC-AUC, as the metric we want to maximize here. I make this decision because (1) we've seen before that we can easily get a ROC-AUC of 99% in Testing already with a simple linear model, and (2) it is a more representative metric considering the huge imbalance in our data. Only 0.6% of the testing examples belong to the positive class. So then the PR-AUC we would get by random guessing is 0.6%. Let's see how much better than that we can do with our model.","7d386789":"We do much worse, as expected. Despite the deceiving 94% ROC AUC in testing, our recall drops to a poor 12%. This reveals that the cast and crew representations obtained with Word2Vec are fundamental to our model. Let's see if we can confirm this by seeing how well we do with those features only.","8455b386":"It looks clear from the plot above that LGBM gives us the best performance. This is particularly true if we are looking for a model with good Precision. We see that we can obtain higher Precision with LGBM without sacrificing as much Recall as with the linear model. If, on the other hand, we were more interested in obtaining high Recall, then both models are equally good. We see that for anything over 50% in Recall we get around the same Precision with both models. In this particular case, I would be more interested in high Precision indeed. It would be better for our model to be very accurate when it predicts that a certain movie will be among the Top 1% in votes, and I don't mind so much if there are many movies that actually achieve this status without our model being able to recognize them as such, which is what Recall measures.\n\nOn the other hand, we see that our Naive model performs very poorly. We can get a Precision of 100% with a 0% Recall or vice versa, and not much we can do in the middle.\n\nNow let's see some of the predictions to get a more practical feel for what our model is predicting:","0bf5ddd8":"### **AGE**\n\nI take 3 kind of new young stars and one star from the past. Despite being all famous, let's see if the model can differentiate according to which era they belong to."}}