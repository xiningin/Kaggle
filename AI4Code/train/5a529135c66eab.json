{"cell_type":{"60c62b63":"code","281d51d3":"code","9e438404":"code","f384bb7f":"code","25c65902":"code","b479e926":"code","03876948":"code","e3add101":"code","983effb2":"code","29fa01b8":"code","9546f9bd":"code","caefa629":"code","15fdf087":"code","d50ec4dd":"code","e031f790":"code","76f573c1":"code","db6f4427":"code","c45d9d82":"code","ed6a2610":"code","b1d0e3a3":"code","a40a5211":"code","99d4b6e7":"code","f48a9eb7":"code","1a2c763b":"code","0f7021f6":"code","76a92004":"code","e9bb55fc":"code","904b29d4":"code","6eb6c116":"code","d09442fc":"code","6805c1dc":"code","3856902e":"code","409a17d1":"code","74c878f9":"code","a52bb71f":"code","33d51019":"code","62c32793":"code","e1131e4c":"code","4805c584":"code","dc321485":"code","6532114f":"code","17a9c545":"code","367a835d":"code","6dd2bfba":"code","a8c25121":"code","a809a8e7":"code","b7aab089":"code","1b6a1128":"code","cad87b22":"code","5cf08f72":"code","3fc3e145":"code","d3ee8ccf":"code","9d53cc12":"code","5999632e":"code","6a0aaa86":"code","280f485d":"code","f902526e":"code","bd8ccd0e":"code","685dd213":"markdown","20ac85f1":"markdown","82f365d1":"markdown","8617b3fc":"markdown","a74d9261":"markdown","47c398c2":"markdown","508d407a":"markdown","34ea2fce":"markdown","55e28534":"markdown","fa4a9874":"markdown","5381599a":"markdown","8e71f9ef":"markdown","341968f1":"markdown","b72d586b":"markdown","208e592a":"markdown","07521cd2":"markdown","590c4aad":"markdown","9c66a8bc":"markdown","c3266c0c":"markdown","5965a014":"markdown","43bbdc8c":"markdown","d673f47d":"markdown","fbb49b19":"markdown","7dfdc235":"markdown","6e5aff53":"markdown"},"source":{"60c62b63":"!pip install xlrd\n!pip install openpyxl","281d51d3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats.mstats import winsorize\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","9e438404":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f384bb7f":"df = pd.read_csv('\/kaggle\/input\/car-data\/CarPrice_Assignment.csv')\ndata_dict = pd.read_excel('\/kaggle\/input\/car-data\/Data Dictionary - carprices.xlsx', skiprows=3)\ndata_dict = data_dict.iloc[:-2, [7, 11]].rename({'Unnamed: 7': 'Column', 'Unnamed: 11': 'Description'}, axis=1)","25c65902":"pd.set_option('max_colwidth', 250)\ndisplay(data_dict)","b479e926":"df.head()","03876948":"df['brand'] = df['CarName'].apply(lambda x: x.split(' ')[0])\ndf['brand'] = df['brand'].replace({'vokswagen': 'volkswagen', 'vw': 'volkswagen', 'maxda': 'mazda',\n                                   'Nissan': 'nissan', 'porcshce': 'porsche', 'toyouta': 'toyota'})","e3add101":"df['brand'].unique()","983effb2":"df['symboling'] = df['symboling'].astype('category')","29fa01b8":"df.columns","9546f9bd":"df.info()","caefa629":"df.describe()","15fdf087":"df.describe(include='O')","d50ec4dd":"df.select_dtypes('object').nunique()","e031f790":"categorical_features = list(df.select_dtypes(['O', 'category']).drop('CarName', axis=1).columns)","76f573c1":"plt.figure(figsize=(15, 20))\n\nfor i in range(len(categorical_features) - 1):\n    plt.subplot(4, 3, i+1)\n    plt.title(categorical_features[i] + ' (Value Counts)')\n    plt.bar(df[categorical_features[i]].value_counts().index,\n            df[categorical_features[i]].value_counts())\n    \nplt.subplot(4, 3, 11)\nplt.title('brand (Value Counts)')\nplt.bar(df['brand'].value_counts().index,\n        df['brand'].value_counts())\nplt.xticks(rotation=90)\n\nplt.show()","db6f4427":"for i in range(len(categorical_features)):\n    print(df.groupby(categorical_features[i])['price'].mean(), '\\n')","c45d9d82":"plt.figure(figsize=(18, 24))\n\nfor i in range(len(categorical_features)-1):\n    plt.subplot(4, 3, i+1)\n    plt.title(categorical_features[i] + ' (mean price)')\n    sns.barplot(categorical_features[i], 'price', data=df,\n                order=df.groupby(categorical_features[i])['price'].mean().sort_values(ascending=False).index, ci=None)\n    \nplt.subplot(4, 3, 11)\nplt.title('brand (Value Counts)')\nsns.barplot('brand', 'price', data=df, ci=None,\n           order=df.groupby('brand')['price'].mean().sort_values(ascending=False).index)\nplt.xticks(rotation=90)\n\nplt.show()","ed6a2610":"numeric_features = list(df.select_dtypes([np.int64, np.float64]).drop('car_ID', axis=1).columns)","b1d0e3a3":"plt.figure(figsize=(25, 15))\n\nfor i in range(len(numeric_features)):\n    plt.subplot(3, 5, i+1)\n    plt.title(numeric_features[i], fontsize=20)\n    plt.hist(df[numeric_features[i]], bins=15)\n    \nplt.show()","a40a5211":"plt.figure(figsize=(25, 15))\n\nfor i in range(len(numeric_features)):\n    plt.subplot(3, 5, i+1)\n    plt.title(numeric_features[i], fontsize=20)\n    plt.boxplot(df[numeric_features[i]])\n    \nplt.show()","99d4b6e7":"plt.figure(figsize=(10, 10))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","f48a9eb7":"plt.figure(figsize=(15, 10))\n\nplt.subplot(2, 3, 1)\nplt.scatter(df['enginesize'], df['price'])\nplt.title('enginesize')\n\nplt.subplot(2, 3, 2)\nplt.scatter(df['curbweight'], df['price'])\nplt.title('curbweight')\n\nplt.subplot(2, 3, 3)\nplt.scatter(df['horsepower'], df['price'])\nplt.title('horsepower')\n\nplt.subplot(2, 3, 4)\nplt.scatter(df['carwidth'], df['price'])\nplt.title('carwidth')\n\nplt.subplot(2, 3, 5)\nplt.scatter(df['highwaympg'], df['price'])\nplt.title('highwaympg')\n\nplt.subplot(2, 3, 6)\nplt.scatter(df['citympg'], df['price'])\nplt.title('citympg')\n\nplt.show()","1a2c763b":"plt.figure(figsize=(20, 20))\n\nfor i in range(len(numeric_features)):\n    plt.subplot(4, 4, i+1)\n    plt.scatter(df[numeric_features[i]], df['price'])\n    plt.title(numeric_features[i])\n\nplt.show()","0f7021f6":"plt.figure(figsize=(15, 7))\n\nplt.subplot(1, 2, 1)\nplt.hist(df['price'], bins=30)\nplt.title('Price (Histogram)', fontsize=15)\n\nplt.subplot(1, 2, 2)\nplt.boxplot(df['price'])\nplt.title('Price (Boxplot)', fontsize=15)\n\nplt.show()","76a92004":"df['carwidth_winsorize'] = winsorize(df['carwidth'], limits=[0, 0.1])\ndf['enginesize_winsorize'] = winsorize(df['enginesize'], limits=[0, 0.1])\ndf['stroke_winsorize'] = winsorize(df['stroke'], limits=[0.1, 0.1])\ndf['compressionratio_winsorize'] = winsorize(df['compressionratio'], limits=[0.1, 0.1])","e9bb55fc":"display(df[['carwidth','carwidth_winsorize']].describe())\ndisplay(df[['enginesize','enginesize_winsorize']].describe())\ndisplay(df[['stroke','stroke_winsorize']].describe())\ndisplay(df[['compressionratio','compressionratio_winsorize']].describe())","904b29d4":"plt.figure(figsize=(8, 20))\n\nplt.subplot(4, 2, 1)\nplt.boxplot(df['carwidth'])\nplt.title('cardwidth')\n\nplt.subplot(4, 2, 2)\nplt.boxplot(df['carwidth_winsorize'])\nplt.title('cardwidth_winsorize')\n\nplt.subplot(4, 2, 3)\nplt.boxplot(df['enginesize'])\nplt.title('enginesize')\n\nplt.subplot(4, 2, 4)\nplt.boxplot(df['enginesize_winsorize'])\nplt.title('enginesize_winsorize')\n\nplt.subplot(4, 2, 5)\nplt.boxplot(df['stroke'])\nplt.title('stroke')\n\nplt.subplot(4, 2, 6)\nplt.boxplot(df['stroke_winsorize'])\nplt.title('stroke_winsorize')\n\nplt.subplot(4, 2, 7)\nplt.boxplot(df['compressionratio'])\nplt.title('compressionratio')\n\nplt.subplot(4, 2, 8)\nplt.boxplot(df['compressionratio_winsorize'])\nplt.title('compressionratio_winsorize')\n\nplt.show()","6eb6c116":"df = df.drop(['carwidth', 'enginesize', 'stroke', 'compressionratio'], axis=1)","d09442fc":"df = df[['car_ID', 'symboling', 'CarName', 'fueltype', 'aspiration',\n         'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'wheelbase',\n         'carlength', 'carheight', 'curbweight', 'enginetype', 'cylindernumber',\n        'fuelsystem', 'boreratio', 'horsepower', 'peakrpm', 'citympg',\n        'highwaympg', 'brand', 'carwidth_winsorize',\n        'enginesize_winsorize', 'stroke_winsorize',\n        'compressionratio_winsorize', 'price']]","6805c1dc":"plt.figure(figsize=(10, 10))\nsns.heatmap(df.corr(), annot=True)\nplt.show()","3856902e":"df = pd.get_dummies(df.drop('CarName', axis=1))","409a17d1":"y = df['price']\nX = df.drop(['price', 'car_ID'], axis=1)","74c878f9":"scaler = StandardScaler()\n\ndf_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\nX_scaled = df_scaled.drop(['price', 'car_ID'], axis=1)","a52bb71f":"normalizer = Normalizer()\n\ndf_normalized = pd.DataFrame(normalizer.fit_transform(df), columns=df.columns)\nX_normalized = df_normalized.drop(['price', 'car_ID'], axis=1)","33d51019":"print('X:')\ndisplay(X.head())\n\nprint('X_scaled:')\ndisplay(X_scaled.head())\n\nprint('X_normalized:')\ndisplay(X_normalized.head())","62c32793":"def fit_predict_score(Model, X_train, y_train, X_test, y_test):\n    \"\"\"Fit the model of your choice,\n    predict for test data,\n    returns MAE, MSE, RMSE.\"\"\"\n    model = Model\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n\n    return (train_score, test_score, metrics.mean_absolute_error(y_test, y_pred),\n            metrics.mean_squared_error(y_test, y_pred), np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n\ndef model_comparison(X, y):\n    \"\"\"Creates a DataFrame comparing Linear Regression, Lasso, Ridge, SVR (kernel: linear),\n    XBRegressor, and LGBMRegressor scores and errors.\"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    \n    lrm_train_score, lrm_test_score, lrm_mae, lrm_mse, lrm_rmse = fit_predict_score(LinearRegression(), X_train, y_train, X_test, y_test)\n    lasso_train_score, lasso_test_score, lasso_mae, lasso_mse, lasso_rmse = fit_predict_score(Lasso(), X_train, y_train, X_test, y_test)\n    ridge_train_score, ridge_test_score, ridge_mae, ridge_mse, ridge_rmse = fit_predict_score(Ridge(), X_train, y_train, X_test, y_test)\n    svr_train_score, svr_test_score, svr_mae, svr_mse, svr_rmse = fit_predict_score(SVR(kernel='linear'), X_train, y_train, X_test, y_test)\n    xgbr_train_score, xgbr_test_score, xgbr_mae, xgbr_mse, xgbr_rmse = fit_predict_score(XGBRegressor(), X_train, y_train, X_test, y_test)\n    lgbr_train_score, lgbr_test_score, lgbr_mae, lgbr_mse, lgbr_rmse = fit_predict_score(LGBMRegressor(), X_train, y_train, X_test, y_test)\n    \n    models = ['Linear Regression', 'Lasso Regression', 'Ridge Regression',\n          'SVM (kernel:linear)', 'XGBoost (Regressor)', 'LightGBM (Regressor)']\n    train_score = [lrm_train_score, lasso_train_score, ridge_train_score, svr_train_score, xgbr_train_score, lgbr_train_score]\n    test_score = [lrm_test_score, lasso_test_score, ridge_test_score, svr_test_score, xgbr_test_score, lgbr_test_score]\n    mae = [lrm_mae, lasso_mae, ridge_mae, svr_mae, xgbr_mae, lgbr_mae]\n    mse = [lrm_mse, lasso_mse, ridge_mse, svr_mse, xgbr_mse, lgbr_mse]\n    rmse = [lrm_rmse, lasso_rmse, ridge_rmse, svr_rmse, xgbr_rmse, lgbr_rmse]\n    \n    model_comparison = pd.DataFrame(data=[models, train_score, test_score, mae, mse, rmse]).T.rename({0: 'Model', 1:'Training Score',\n                                                                                    2: 'Test Score',\n                                                                                    3:'Mean Absolute Error',\n                                                                                    4: 'Mean Squared Error',\n                                                                                    5:'Root Mean Squared Error'}, axis=1)\n    \n    return model_comparison","e1131e4c":"print(\"Default:\")\ndisplay(model_comparison(X, y))\nprint(\"Scaled:\")\ndisplay(model_comparison(X_scaled, y))\nprint(\"Normalized:\")\ndisplay(model_comparison(X_normalized, y))","4805c584":"params = {'alpha': [10**i for i in range(1, 5)] + [round(0.1**i,5) for i in range(5)]}\n\nlasso_grid = GridSearchCV(estimator = Lasso(),\n                        param_grid = params,                        \n                        cv = 5)\n\nlasso_grid.fit(X_scaled, y)","dc321485":"print('Best Score: ', lasso_grid.best_score_)\nprint('Best Params: ', lasso_grid.best_params_)","6532114f":"display(pd.DataFrame(pd.DataFrame(lasso_grid.cv_results_)[['param_alpha', 'mean_test_score']].groupby(['param_alpha'])['mean_test_score'].mean()).reset_index().sort_values('param_alpha'))","17a9c545":"params = {'alpha': [10**i for i in range(1, 5)] + [round(0.1**i,5) for i in range(5)]}\n\nridge_grid = GridSearchCV(estimator = Ridge(),\n                        param_grid = params,                        \n                        cv = 5)\n\nridge_grid.fit(X_scaled, y)","367a835d":"print('Best Score: ', ridge_grid.best_score_)\nprint('Best Params: ', ridge_grid.best_params_)","6dd2bfba":"display(pd.DataFrame(pd.DataFrame(ridge_grid.cv_results_)[['param_alpha', 'mean_test_score']].groupby(['param_alpha'])['mean_test_score'].mean()).reset_index().sort_values('param_alpha'))","a8c25121":"params = {'C': [10**i for i in range(1, 2)] + [round(0.1**i,5) for i in range(5)],\n          'kernel': ['linear']}\n\nsvr_grid = GridSearchCV(estimator = SVR(),\n                        param_grid = params,                        \n                        cv = 5)\n\nsvr_grid.fit(X, y)","a809a8e7":"display(pd.DataFrame(pd.DataFrame(svr_grid.cv_results_)[['param_C', 'mean_test_score']].groupby(['param_C'])['mean_test_score'].mean()).reset_index().sort_values('param_C'))","b7aab089":"print('Best Score: ', svr_grid.best_score_)\nprint('Best Params: ', svr_grid.best_params_)","1b6a1128":"params = {\n        'learning_rate': [0.1, 0.3, 0.5],\n        'max_depth': [1, 3, 5],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.1, 0.3, 0.5],\n        'colsample_bytree': [0.1, 0.3, 0.5],\n        'n_estimators' : [100, 200, 500, 750, 1000],\n        'objective': ['reg:squarederror']\n}\n\nxgbr_grid = GridSearchCV(estimator = XGBRegressor(),\n                         param_grid = params,\n                         cv = 3)\n\nxgbr_grid.fit(X_normalized, y)","cad87b22":"pd.DataFrame(xgbr_grid.cv_results_)[['param_colsample_bytree', 'param_learning_rate', 'param_max_depth',\n       'param_min_child_weight', 'param_n_estimators',\n       'param_subsample', 'mean_test_score']].sort_values('mean_test_score', ascending=False).head()","5cf08f72":"print('Best Score: ', xgbr_grid.best_score_)\nprint('Best Params: ', xgbr_grid.best_params_)","3fc3e145":"params = {\n    'learning_rate': [0.001, 0.01, 0.1, 1],\n    'n_estimators': [100, 200, 500, 750, 1000]\n}\n\nlgbr_grid = GridSearchCV(estimator = LGBMRegressor(),\n                        param_grid = params,                        \n                        cv = 3)\n\nlgbr_grid.fit(X_normalized, y)","d3ee8ccf":"pd.DataFrame(lgbr_grid.cv_results_)[['param_learning_rate', 'param_n_estimators', 'mean_test_score']].sort_values('mean_test_score', ascending=False).head()","9d53cc12":"print('Best Score: ', lgbr_grid.best_score_)\nprint('Best Params: ', lgbr_grid.best_params_)","5999632e":"print('Lasso: X_scaled')\nprint('Lasso Best Params: {}\\n'.format(lasso_grid.best_params_))\n\nprint('Ridge: X_scaled')\nprint('Ridge Best Params: {}\\n'.format(ridge_grid.best_params_))\n\nprint('SVR: X' )\nprint('SVR Best Params: {}\\n'.format(svr_grid.best_params_))\n\nprint('XGBRegressor: X_normalized')\nprint('XGBRegressor Best Params: {}\\n'.format(xgbr_grid.best_params_))\n\nprint('LGBMRegressor: X_normalized')\nprint('LGBMRegressor Best Params: {}'.format(lgbr_grid.best_params_))","6a0aaa86":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n\n# Lasso\nlasso = Lasso(alpha=100)\nlasso.fit(X_train, y_train)\n\ny_pred = lasso.predict(X_test)\nlasso_train_score = lasso.score(X_train, y_train)\nlasso_test_score = lasso.score(X_test, y_test)\n\nlasso_mae = metrics.mean_absolute_error(y_test, y_pred)\nlasso_mse = metrics.mean_squared_error(y_test, y_pred)\nlasso_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n\n# Ridge\nridge = Ridge(alpha=1000)\nridge.fit(X_train, y_train)\n\ny_pred = ridge.predict(X_test)\nridge_train_score = ridge.score(X_train, y_train)\nridge_test_score = ridge.score(X_test, y_test)\n\nridge_mae = metrics.mean_absolute_error(y_test, y_pred)\nridge_mse = metrics.mean_squared_error(y_test, y_pred)\nridge_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))","280f485d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Linear Regression\nlrm = LinearRegression()\nlrm.fit(X_train, y_train)\n\ny_pred = lrm.predict(X_test)\nlrm_train_score = lrm.score(X_train, y_train)\nlrm_test_score = lrm.score(X_test, y_test)\n\nlrm_mae = metrics.mean_absolute_error(y_test, y_pred)\nlrm_mse = metrics.mean_squared_error(y_test, y_pred)\nlrm_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n\n# SVR\nsvr = SVR(C=0.001, kernel='linear')\nsvr.fit(X_train, y_train)\n\ny_pred = svr.predict(X_test)\nsvr_train_score = svr.score(X_train, y_train)\nsvr_test_score = svr.score(X_test, y_test)\n\nsvr_mae = metrics.mean_absolute_error(y_test, y_pred)\nsvr_mse = metrics.mean_squared_error(y_test, y_pred)\nsvr_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))","f902526e":"X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.33, random_state=42)\n\n# XGBRegressor\nxgbr = XGBRegressor(colsample_bytree=0.3, learning_rate=0.1, max_depth=5, min_child_weight=1,\n                    n_estimators=500, objective='reg:squarederror', subsample=0.5)\nxgbr.fit(X_train, y_train)\n\ny_pred = xgbr.predict(X_test)\nxgbr_train_score = xgbr.score(X_train, y_train)\nxgbr_test_score = xgbr.score(X_test, y_test)\n\nxgbr_mae = metrics.mean_absolute_error(y_test, y_pred)\nxgbr_mse = metrics.mean_squared_error(y_test, y_pred)\nxgbr_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n\n# Ridge\nlgbr = LGBMRegressor(learning_rate=0.01, n_estimators=1000)\nlgbr.fit(X_train, y_train)\n\ny_pred = lgbr.predict(X_test)\nlgbr_train_score = lgbr.score(X_train, y_train)\nlgbr_test_score = lgbr.score(X_test, y_test)\n\nlgbr_mae = metrics.mean_absolute_error(y_test, y_pred)\nlgbr_mse = metrics.mean_squared_error(y_test, y_pred)\nlgbr_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))","bd8ccd0e":"models = ['Linear Regression', 'Lasso Regression', 'Ridge Regression',\n      'SVM (kernel:linear)', 'XGBoost (Regressor)', 'LightGBM (Regressor)']\ntrain_score = [lrm_train_score, lasso_train_score, ridge_train_score, svr_train_score, xgbr_train_score, lgbr_train_score]\ntest_score = [lrm_test_score, lasso_test_score, ridge_test_score, svr_test_score, xgbr_test_score, lgbr_test_score]\nmae = [lrm_mae, lasso_mae, ridge_mae, svr_mae, xgbr_mae, lgbr_mae]\nmse = [lrm_mse, lasso_mse, ridge_mse, svr_mse, xgbr_mse, lgbr_mse]\nrmse = [lrm_rmse, lasso_rmse, ridge_rmse, svr_rmse, xgbr_rmse, lgbr_rmse]\n\nmodel_comparison = pd.DataFrame(data=[models, train_score, test_score, mae, mse, rmse]).T.rename({0: 'Model', 1:'Training Score',\n                                                                                2: 'Test Score',\n                                                                                3:'Mean Absolute Error',\n                                                                                4: 'Mean Squared Error',\n                                                                                5:'Root Mean Squared Error'}, axis=1)\n\ndisplay(model_comparison)","685dd213":"## Ridge","20ac85f1":"### Correlations between Features & Target Variable","82f365d1":"### Columns and Descriptions","8617b3fc":"Highest positive correlations with **price** column:\n- **enginesize**: 0.87\n- **curbweight**: 0.84\n- **horsepower**: 0.81\n- **carwidth**: 0.76\n\nHighest negative correlations **price** column:\n- **highwaympg**: -0.7\n- **citympg**: -0.69","a74d9261":"## Lasso","47c398c2":"## Target Variable (Price)","508d407a":"# One-Hot-Encoding","34ea2fce":"# Hyperparameter Tuning","55e28534":"## Numerical Features","fa4a9874":"### Scatter Plot of All Features and Target Variable","5381599a":"# Car Data - Data Description & Cleaning","8e71f9ef":"## Support Vector Machine","341968f1":"# GridSearchCV Results & Final Comparison","b72d586b":"# Winsorization\n\nFeatures with outliers:\n- **carwidth**\n- **enginesize**\n- **stroke**\n- **compressionratio**","208e592a":"# Standardization","07521cd2":"### Best Results:\n\n- Normalize the data.\n- Use **XGBRegressor**.","590c4aad":"## Categorical Features","9c66a8bc":"# Building Machine Learning Models\n\n- Linear Regression\n- Lasso Regression\n- Ridge Regression\n- Support Vector Regressor\n- XGBoost\n- LightGBM","c3266c0c":"## XGBoost","5965a014":"# Heatmap (Numerical features only)","43bbdc8c":"# Normalization","d673f47d":"# Car Price Prediction with Regression\n\nIn this notebook, I will apply different regression algorithms on [Car Price data from Kaggle](https:\/\/www.kaggle.com\/goyalshalini93\/car-data). My aim will be to:\n1. Understand the data by exploring and visualizing.\n2. Clean and make any necessary feature engineering.\n3. Try different regression algorithms on different versions (unaltered, standardized & normalized) of the data.\n4. Tune hyperparameters to get better results.\n\n### Sections:\n- Data Description & Cleaning\n- Exploratory Data Analysis\n        o Categorical Features\n        o Numerical Features\n        o Target Variable\n- Winsorization\n- Heatmap\n- One-Hot-Encoding\n- Standardization\n- Normalization\n- Building Machine Learning Models\n        o Linear Regression\n        o Lasso Regression\n        o Ridge Regression\n        o Support Vector Regressor\n        o XGBoost\n        o LightGBM\n- Hyperparameter Tuning\n        o Lasso Regression\n        o Ridge Regression\n        o Support Vector Regressor\n        o XGBoost\n        o LightGBM\n- GridSearchCV Results & Final Comparison","fbb49b19":"## LightGBM","7dfdc235":"### Scatter Plot of Features with Highest Correlations and Target Variable","6e5aff53":"# Exploratory Data Analysis"}}