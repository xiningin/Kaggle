{"cell_type":{"4c495098":"code","a1c46a9a":"code","a30aa650":"code","d20ecdd9":"markdown"},"source":{"4c495098":"import cv2\nimport os\nimport subprocess\nimport tempfile\nimport shutil\n\nimport pandas as pd\n\nfrom tqdm.auto import tqdm","a1c46a9a":"def annotate_video(\n        video_path: str,\n        video_labels: pd.DataFrame,\n        output_path: str,\n        convert: bool = False) -> str:\n    '''\n    Create a function to annotate the video at the provided path using labels\n    from the provided dataframe, return the path of the video\n    '''\n\n    VIDEO_CODEC = \"MP4V\"\n    HELMET_COLOR = (0, 0, 0)    # Black\n    IMPACT_COLOR = (0, 0, 255)  # Red\n    video_name = os.path.basename(video_path)\n\n    vidcap = cv2.VideoCapture(video_path)\n    fps = vidcap.get(cv2.CAP_PROP_FPS)\n    width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        tmp_output_path = os.path.join(tmp_dir, video_name)\n        output_video = cv2.VideoWriter(tmp_output_path,\n                                       cv2.VideoWriter_fourcc(*VIDEO_CODEC),\n                                       fps, (width, height))\n        frame = 0\n        while True:\n            it_worked, img = vidcap.read()\n            if not it_worked:\n                break\n\n            # We need to add 1 to the frame count to match the label frame index\n            # that starts at 1\n            frame += 1\n\n            # Let's add a frame index to the video so we can track where we are\n            img_name = f\"{video_name}_frame{frame}\"\n            cv2.putText(img, img_name, (0, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0,\n                        HELMET_COLOR, thickness=2)\n\n            # Now, add the boxes\n            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n            for box in boxes.itertuples(index=False):\n                # Filter for definitive head impacts and turn labels red\n                if box.impact == 1 and box.confidence > 1 and box.visibility > 0:\n                    color, thickness = IMPACT_COLOR, 2\n                else:\n                    color, thickness = HELMET_COLOR, 1\n                # Add a box around the helmet\n                cv2.rectangle(img, (box.left, box.top),\n                            (box.left + box.width, box.top + box.height),\n                            color, thickness=thickness)\n                cv2.putText(img, box.label, (box.left, max(0, box.top - 5)),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, thickness=1)\n            output_video.write(img)\n        output_video.release()\n\n        if os.path.exists(output_path):\n            os.remove(output_path)\n        # Not all browsers support the codec, we will re-load the file at\n        # tmp_output_path and convert to a codec that is more broadly\n        # readable using ffmpeg\n        if convert:\n            subprocess.run([\"ffmpeg\", \"-i\", tmp_output_path,\n                            \"-crf\", \"18\", \"-preset\", \"veryfast\",\n                            \"-vcodec\", \"libx264\", output_path])\n        else:\n            shutil.copy(tmp_output_path, output_path)\n\n    return output_path","a30aa650":"# Define input and output paths\ndata_path = '\/kaggle\/input\/nfl-impact-detection'\ndest_video_path = '.\/output\/annotated_videos\/'\n\nvideo_path = os.path.join(data_path, 'train')\nvideo_files = os.listdir(video_path)\nvideo_labels = pd.read_csv(os.path.join(data_path, 'train_labels.csv'))\n\nos.makedirs(dest_video_path, exist_ok=True)\n\nfor video_file in tqdm(video_files):\n    annotate_video(os.path.join(video_path, video_file), video_labels,\n                   os.path.join(dest_video_path, video_file),\n                   True)","d20ecdd9":"Code for video annotation from @samhuddleston https:\/\/www.kaggle.com\/samhuddleston\/nfl-1st-and-future-getting-started kernel"}}