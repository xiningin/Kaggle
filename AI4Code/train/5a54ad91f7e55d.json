{"cell_type":{"1b09f214":"code","5d3cdc37":"code","b9cbb5b8":"code","6f225b0c":"code","9cc7c817":"code","a2648141":"code","16827cc5":"code","e2e4b5eb":"code","21be9ff0":"code","cd1efc33":"code","0d38c5e6":"code","19e99815":"code","e68ce593":"code","e30b6e86":"code","10de38a2":"code","eb16369d":"code","5821d3d4":"code","605fd364":"code","b12b3ff1":"code","06207ff1":"code","6f311649":"code","256193a0":"code","b01b51c3":"code","9311d0b0":"code","381e9ca9":"code","d548fc47":"code","c44cce29":"code","0a718cb0":"code","47300fb9":"code","1c474a73":"code","3894e530":"code","26df2284":"code","3506ab23":"code","005ccc23":"code","6f434fb0":"code","b44536dd":"code","20fa5567":"code","714f5f4a":"markdown","40ebf654":"markdown","f191f067":"markdown","3aa97117":"markdown","e5a29f13":"markdown","cbcc00fa":"markdown","d94a15b1":"markdown","1a2a3309":"markdown","079f34b0":"markdown","1ddeedb4":"markdown","83dd34cf":"markdown","b841289a":"markdown","c8f35f68":"markdown","18a33a11":"markdown","32df47b1":"markdown","f28f376a":"markdown","ec4022f6":"markdown","e80f8ebc":"markdown"},"source":{"1b09f214":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d3cdc37":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nprint(f'SHAPE  rows, cols\\ntrain: {train.shape}\\ntest: {test.shape}')","b9cbb5b8":"def match_list(lista1, lista2):\n    for i in lista1:\n        if i not in lista2:\n            print(f'{i}: not exists list 2')\n    for z in lista2:\n        if z not in lista1:\n            print(f'{z}: not exists list 1')\n\nmatch_list(train.columns, test.columns)","6f225b0c":"# Concatenate the train and test\nX_full = pd.concat([train.drop(\"SalePrice\", axis=1), test]) \ny_full = train[['SalePrice']]\n\nprint(f'SHAPE  rows, cols\\nX_full: {X_full.shape}\\ny_full: {y_full.shape}')","9cc7c817":"# Missing values\nmiss_values = X_full.isna().sum()\nprint(f'Total of missing values: {miss_values.sum()}')\ncols_miss = miss_values[miss_values.values > 0].index\nprint(f'Total of Columns missing values: {len(cols_miss)}\\nList columns missing values:\\n{cols_miss}')\n\nmiss_values = X_full.isna().sum().sort_values(ascending = False).head(22).reset_index()\nmiss_values.rename(columns={'index': 'features', 0: 'miss_values'}, inplace=True)","a2648141":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\nwith plt.style.context('Solarize_Light2'):\n    plt.figure(figsize=(14,5))\n    color = sns.dark_palette(\"deeppink\", reverse=True, n_colors=22)\n    plt.bar(miss_values['features'], round(miss_values.miss_values*100 \/ len(X_full), 1), color=color, width=0.9)\n    xs = list(range(0,120,20))\n    ys = list(map(lambda x: str(x)+'%', xs))\n    plt.yticks(xs, ys)\n    plt.xticks(rotation=90)\n    plt.title('Columns Missing Values TOP: 22', color='#073642', style='italic')\n    plt.xlabel('Features')\n    plt.ylabel('% Of Missing Values')\nplt.show()\n","16827cc5":"miss_drop_cols = miss_values.loc[0:5].features.values.tolist() # add list columns % top 6 missing values\nprint('Total of Features Full: {}'.format(len(X_full.columns)))\nX_full.drop(columns=miss_drop_cols, inplace=True)\nprint('Total of Features Full drop missing: {}'.format(len(X_full.columns)))","e2e4b5eb":"# Categorical Features\ncat_features = X_full.select_dtypes(include='object').copy()\ncat_features['MSSubClass'] = X_full['MSSubClass'].apply(str)\n\n# Numeric Features\nnumeric_features = X_full.select_dtypes(exclude='object').copy()\nnumeric_features.drop(columns=['MSSubClass'], inplace=True)\n#drop(['MSSubClass'])\n\nprint('Total of features category: {}'.format(len(cat_features.columns)))\nprint('Total of features numeric: {}'.format(len(numeric_features.columns)))\n#cat_features.nunique().sort_values(ascending = False).head(3)","21be9ff0":"with plt.style.context('Solarize_Light2'):\n    fig = plt.figure(figsize=(18,24))\n    for index, i in enumerate(numeric_features.columns[1:]):\n        plt.subplot(9,4,index+1)\n        sns.distplot(numeric_features[i].dropna(), color='#268bd2', kde=False, hist_kws={'alpha': .8}) \n    \n    fig.suptitle('Histograms Features Numerical', y=1.02, fontsize=18, color='#002b36',  weight='bold')\n    fig.tight_layout(pad=1.0)","cd1efc33":"numeric_overfit = []\nfor i in numeric_features.columns:\n    z = numeric_features[i].value_counts(normalize=True)*100\n    if z.iloc[0] >= 85:\n        numeric_overfit.append(i)\n\nprint(f\"Numerical Features with > 85% of the same value: {numeric_overfit}\")","0d38c5e6":"with plt.style.context('Solarize_Light2'):\n    fig = plt.figure(figsize=(18,20))\n    for index, i in enumerate(numeric_features.drop(columns=numeric_overfit).columns[1:]):\n        plt.subplot(7,4,index+1)\n        sns.regplot(x=i, y='SalePrice', data=train, color='#268bd2')\n        p = plt.xticks()[0]\n        var = p[-1]- p[-2]\n        plt.xlim(xmax=train[i].max()+var\/4)\n    \n    fig.suptitle('Regression Features Numerical', y=1.02, fontsize=18, color='#002b36',  weight='bold')\n    fig.tight_layout(pad=1.0)","19e99815":"b = numeric_features.drop(columns=numeric_overfit)\nb = b.drop(b.columns[[0]], axis=1)\n\ndf_corr = b\ndf_corr['SalePrice'] = train['SalePrice']\ncorr = df_corr.corr()\n\nwith plt.style.context('Solarize_Light2'):\n    fig, (heat1, heat2) = plt.subplots(2, 1, figsize=(16, 20), sharex=True, gridspec_kw={'hspace': .08})\n    sns.heatmap(round(corr,1), linewidth=0.5, annot=True, ax=heat1)\n    sns.heatmap(round(corr,1), mask = corr <=0.8, linewidth=0.5, annot=True, ax=heat2)\n    heat1.set_title('Heatmap 1: Correlated all', color='#073642', style='italic')\n    heat2.set_title('Heatmap 2: Highly Correlated variables > 0.8', color='#073642', style='italic')\n    fig.suptitle('Heatmaps Features Numerical', x=0.17, y=0.92, fontsize=18, color='#002b36',  weight='bold')\n    plt.show()","e68ce593":"# Missing values features numerical\nfrom sklearn.impute import KNNImputer\n\nprint('Total of missing values features numerical: {}'.format(numeric_features.isna().sum().sum()))\nprint('Filling missing values using the k-Nearest Neighbors....')\ncols_name1 = list(numeric_features.columns)\nimputer = KNNImputer(n_neighbors=2, weights=\"distance\")\nimp_knn = imputer.fit_transform(numeric_features)\nnumeric_features = pd.DataFrame(imp_knn, columns=cols_name1, index=numeric_features.index)\n\nprint('Total of missing values features numerical transform k-Nearest Neighbors: {}'.format(numeric_features.isna().sum().sum()))","e30b6e86":"h = []\nfor c in corr.columns:\n    high = corr[c].mask(corr[c] < 0.8).dropna().index\n    if len(high) > 1:\n        h.append(high[0]+' '+'and'+' '+high[1])\n\nprint('Highly Correlated variables:')\nfor o in set(h):\n    print(o)","10de38a2":"cat_features['YrSold'] = numeric_features['YrSold'].astype(str)\ncat_features['MoSold'] = numeric_features['MoSold'].astype(str)\nnumeric_features.drop(columns=['YrSold', 'MoSold'], inplace=True)\nnumeric_features.drop(columns=numeric_overfit, inplace=True)","eb16369d":"with plt.style.context('Solarize_Light2'):\n    fig = plt.figure(figsize=(18,30))\n    for index, i in enumerate(cat_features.columns):\n        plt.subplot(11,4,index+1)\n        sns.countplot(x=cat_features[i])\n        plt.xticks(rotation=90)\n    \n    fig.suptitle('Counts Features Categorical', y=1.02, fontsize=18, color='#002b36',  weight='bold')\n    fig.tight_layout(pad=1.0)","5821d3d4":"print('Total of missing values features categorical: {}'.format(cat_features.isna().sum().sum()))\n# Features missing fill 'NA'\nmiss_na = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond',\n           'BsmtFinType1','BsmtFinType2', 'BsmtExposure', 'GarageFinish']\n\nfor i in miss_na:\n    cat_features[i] = cat_features[i].fillna('NA')\n\n# Features missing fill 'mode'\nfor i in cat_features.columns:\n    if i not in miss_na:\n        cat_features[i] = cat_features[i].fillna(cat_features[i].mode()[0])\n\nprint('Total of missing values features categorical transform: {}'.format(cat_features.isna().sum().sum()))","605fd364":"ord_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond']\nfin_col = ['BsmtFinType1','BsmtFinType2']\n\nordinal_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\nfintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\nexpose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\n\nlotshape_map = {'Reg': 3, 'IR1': 2, 'IR2': 1, 'IR3': 0}\nlandslope_map = {'Gtl': 2, 'Mod': 1, 'Sev': 0}\ngaragefinish_map = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'NA': 0}\npaved_map = {'Y': 2, 'P': 1, 'N': 0}\n\n\nfor col in ord_col:\n    cat_features[col] = cat_features[col].map(ordinal_map)\n    \n\nfor col in fin_col:\n    cat_features[col] = cat_features[col].map(fintype_map)\n\ncat_features['BsmtExposure'] = cat_features['BsmtExposure'].map(expose_map)\ncat_features['LotShape'] = cat_features['LotShape'].map(lotshape_map)\ncat_features['LandSlope'] = cat_features['LandSlope'].map(landslope_map)\ncat_features['GarageFinish'] = cat_features['GarageFinish'].map(garagefinish_map)\ncat_features['PavedDrive'] = cat_features['PavedDrive'].map(paved_map)\n\ncat_overfit = []\nfor i in cat_features.columns:\n    z = cat_features[i].value_counts(normalize=True)*100\n    if z.iloc[0] > 96:\n        cat_overfit.append(i)\n\nprint(f\"Categorical Features with > 96% of the same value: {cat_overfit}\")\ncat_features.drop(columns=cat_overfit, inplace=True)\n","b12b3ff1":"numeric_features['TotalSF'] = numeric_features['TotalBsmtSF'] + numeric_features['1stFlrSF'] + numeric_features['2ndFlrSF']\nnumeric_features['TotalBath'] = numeric_features['FullBath'] + numeric_features['HalfBath'] \nnumeric_features['TotalPorch'] = numeric_features['OpenPorchSF'] + numeric_features['EnclosedPorch']","06207ff1":"# Merge features numerical anda categorical\nfeatures = pd.concat([numeric_features, cat_features], axis=1)\nfeatures.drop(columns='Id', inplace=True)\nfeatures.shape, X_full.shape\ny_target = np.log1p(y_full) # transform y log to target predict","6f311649":"with plt.style.context('Solarize_Light2'):\n    fig, (hist, hist_log) = plt.subplots(1, 2, figsize=(16, 4))\n    sns.distplot(y_full, color='#268bd2', ax=hist)\n    sns.distplot(y_target, color='#268bd2', ax=hist_log)\n    hist.set_title('SalePrice', color='#073642', style='italic')\n    hist_log.set_title('Log(SalePrice)', color='#073642', style='italic')\n    \n    fig.suptitle('Histograms', y=1.02, fontsize=18, color='#002b36',  weight='bold')\n    plt.show()","256193a0":"# OneHotEncoder\nfeatures = pd.get_dummies(features)\n\n# Split train and test\nX_train_new = features.iloc[:len(train), :]\nX_test_new = features.iloc[len(X_train_new):, :]\n\nprint('SHAPE  rows, cols\\ntrain_new: {}\\ntest_new: {}'.format(X_train_new.shape, X_test_new.shape))","b01b51c3":"from sklearn.preprocessing import RobustScaler, QuantileTransformer\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import LassoCV, ElasticNetCV, HuberRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error\n\n# ExtraTrees\nextra_model = ExtraTreesRegressor(max_depth=200, n_estimators=570, random_state=1)\n\n# GradientBoosting\ngrad_model = GradientBoostingRegressor(n_estimators=2900, learning_rate=0.0161, max_depth=3,\n                                       max_features='sqrt', min_samples_leaf=17, loss='huber', random_state=1)\nhist_model = HistGradientBoostingRegressor(min_samples_leaf=40, max_depth=3, max_iter=225, learning_rate=0.15,\n                                           loss='least_absolute_deviation', random_state=1)\n# xgboost\nxgboost_model = XGBRegressor(learning_rate=0.0139, n_estimators=2000, max_depth=4, min_child_weight=0,\n                             subsample=0.7968, colsample_bytree=0.4064, nthread=-1, scale_pos_weight=2,\n                             seed=42, random_state=1)\n# lightgbm\nlgbm_model = LGBMRegressor(objective='regression', n_estimators=6500, num_leaves=10, learning_rate=0.005,\n                           max_bin=163, bagging_fraction=0.85, n_jobs=-1, bagging_seed=42, \n                           feature_fraction_seed=42, bagging_freq=7, feature_fraction=0.1294, \n                           min_data_in_leaf=8, random_state=1)\n# LassoCV\nlasso_model = LassoCV(n_alphas=150, max_iter=1e4, random_state=1)\n\n# ElasticNetCV\nelasticnet_model = ElasticNetCV(n_alphas=150, max_iter=1e4, l1_ratio=1.15, random_state=1)\n\n# Huber\nhuber_model = HuberRegressor(max_iter=2000)\n\n# SVR\nrbf_model = SVR(kernel='rbf', C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121)","9311d0b0":"# Transformer\ntransformer = QuantileTransformer(output_distribution='normal')\n\n# Models\nextratree = make_pipeline(transformer, extra_model)\ngrad = make_pipeline(transformer, grad_model)\nhist = make_pipeline(transformer, hist_model)\nxgboost = make_pipeline(transformer, xgboost_model)\nlgbm = make_pipeline(transformer, lgbm_model)\nlasso = make_pipeline(transformer, lasso_model)\nelasticnet = make_pipeline(transformer, elasticnet_model)\nhuber = make_pipeline(transformer, huber_model)\nsvr = make_pipeline(RobustScaler(), rbf_model)\n\nmodels = [('ExtraTrees', extratree),\n          ('GradientBoosting', grad),\n          ('HistGradientBoosting', hist),\n          ('XGBoost', xgboost), \n          ('LightGBM', lgbm),\n          ('LassoCV', lasso),\n          ('ElasticNetCV', elasticnet),\n          ('Huber', huber),\n          ('SVR', svr)]","381e9ca9":"def storm_model(x, y, models, cv, scoring):\n    df_evaluation = pd.DataFrame()\n    df_predict = pd.DataFrame()\n    row_index = 0\n    for name, model in models:\n        # score\n        scores = cross_validate(model, np.array(x), np.array(y).ravel(), cv=cv, scoring=scoring, n_jobs=-1, verbose=0)\n        # predict\n        y_pred = cross_val_predict(model, np.array(x), np.array(y).ravel(), cv=cv, verbose=0)\n        df_predict[name] = y_pred\n        df_evaluation.loc[row_index, 'Model_Name'] = name\n        for i in scoring:\n            text = 'test_'+i\n            df_evaluation.loc[row_index, i] = -1*scores[text].mean()\n        row_index += 1\n    df_evaluation.rename(columns = {'neg_mean_absolute_error': 'MAE', 'neg_median_absolute_error': 'MEAE',\n                                    'neg_mean_squared_error': 'MSE', 'neg_root_mean_squared_error': 'RMSE'}, inplace = True)\n    df_evaluation.sort_values(by=['MAE'], ascending=True, inplace=True)\n    df_evaluation.reset_index(drop=True, inplace=True)\n    return (df_evaluation, df_predict)","d548fc47":"from sklearn.model_selection import cross_validate, cross_val_predict, KFold\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=1)\nscoring = ['neg_mean_absolute_error',\n           'neg_median_absolute_error', \n           'neg_mean_squared_error', \n           'neg_root_mean_squared_error']\n# cross validate\ndf_score, df_preds = storm_model(X_train_new, y_target, models, kfolds, scoring)","c44cce29":"df_score.style.background_gradient(cmap='jet')","0a718cb0":"# Compare val with preds\ndf_preds['LogSalePrice'] = y_target['SalePrice']\ndf_teste = pd.DataFrame({'SalePrice': np.floor(np.expm1(df_preds['LogSalePrice'])),\n                         'Preds': np.floor(np.expm1(df_preds['GradientBoosting']))})\ndf_teste['dif_val_pred'] = df_teste['Preds'] - df_teste['SalePrice']\ndf_teste.tail(3)","47300fb9":"# StackingRegressor\nbest_models = [('GradientBoosting', grad),\n               ('XGBoost', xgboost),\n               ('LightGBM', lgbm)]\nstack = StackingRegressor(estimators=best_models, final_estimator=huber_model)\nscores = cross_validate(stack, np.array(X_train_new), np.array(y_target).ravel(), \n                        cv=kfolds, scoring=scoring, n_jobs=-1, verbose=0)","1c474a73":"print(f\"MAE score: {-1*scores['test_neg_mean_absolute_error'].mean()}\")","3894e530":"y_pred = cross_val_predict(stack, np.array(X_train_new), np.array(y_target).ravel(), \n                           cv=kfolds, verbose=0, n_jobs=-1)","26df2284":"with plt.style.context('Solarize_Light2'):\n    fig = plt.figure(figsize=(18,10))\n    df_preds['Stacking'] = y_pred\n    rows = 40\n    g = df_preds.head(rows)\n    for index, i in enumerate(df_preds.drop(columns='LogSalePrice').columns):\n        plt.subplot(3,4,index+1)\n        plt.scatter(g.index, g['LogSalePrice'], edgecolors='black')\n        plt.plot(g.index, g[i], color='Red')\n        plt.title(i, color='#073642', style='italic')\n    fig.suptitle('Single predictors versus stacked predictors', y=1.06, fontsize=18, color='#002b36',  weight='bold')\n    fig.tight_layout(pad=1.0)","3506ab23":"from datetime import datetime\n\n# Fitting the models on train data\nprint('=' * 20, 'START Fitting', '=' * 20)\nprint('=' * 55)\nprint(datetime.now(), '\\n')\n\nprint(datetime.now(), 'ExtraTrees')\nextratree_fit = extratree.fit(np.array(X_train_new), np.array(y_target).ravel())\nprint(datetime.now(), 'GradientBoosting')\ngrad_fit = grad.fit(np.array(X_train_new), np.array(y_target).ravel())\nprint(datetime.now(), 'HistGradientBoosting')\nhist_fit = hist.fit(np.array(X_train_new), np.array(y_target).ravel())\nprint(datetime.now(), 'XGBoost')\nxgboost_fit = xgboost.fit(np.array(X_train_new), np.array(y_target).ravel())\nprint(datetime.now(), 'LightGBM')\nlgbm_fit = lgbm.fit(np.array(X_train_new), np.array(y_target).ravel())\nprint(datetime.now(), 'LassoCV')\nlasso_fit = lasso.fit(np.array(X_train_new), np.array(y_target).ravel())\nprint(datetime.now(), 'ElasticNetCV')\nelasticnet_fit = elasticnet.fit(np.array(X_train_new), np.array(y_target).ravel())\nprint(datetime.now(), 'Huber')\nhuber_fit = huber.fit(np.array(X_train_new), np.array(y_target).ravel())\nprint(datetime.now(), 'SVR')\nsvr_fit = svr.fit(np.array(X_train_new), np.array(y_target).ravel())\nprint(datetime.now(), 'StackingCVRegressor')\nstack_fit = stack.fit(np.array(X_train_new), np.array(y_target).ravel())\n\nprint(datetime.now(), '\\n')\nprint('=' * 20, 'FINISHED Fitting', '=' * 20)\nprint('=' * 58)","005ccc23":"def blend_models_predict(X):\n    return ((0.04* extratree_fit.predict(X)) + \n            (0.15 * grad_fit.predict(X)) + \n            (0.04 * hist_fit.predict(X)) + \n            (0.15 * xgboost_fit.predict(X)) + \n            (0.15 * lgbm_fit.predict(X)) + \n            (0.05 * lasso_fit.predict(X)) + \n            (0.05 * elasticnet_fit.predict(X)) + \n            (0.13 * huber_fit.predict(X)) +\n            (0.04 * svr_fit.predict(X)) +\n            (0.20 * stack_fit.predict(X)))","6f434fb0":"submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission['SalePrice'] = np.floor(np.expm1(blend_models_predict(np.array(X_test_new))))\nsubmission = submission[['Id', 'SalePrice']]","b44536dd":"submission.head()","20fa5567":"submission.to_csv('my_submission-v002.csv', index=False)\nprint('Save submission', datetime.now())","714f5f4a":"### StackingRegressor\nStack of estimators with a final regressor.\n* StackingRegressor","40ebf654":"Thanks","f191f067":"# Submission\n\nOur models are tuned, stacked and fitted we are ready to predict and submit our results","3aa97117":"# Exploratory data analysis (EDA)","e5a29f13":"# Preprocessing data\n**Resume:**\n * Target transformer ***log(y_target)***\n * OneHotEncoder\n * RobustScaler in ***model SVR*** \n * QuantileTransformer","cbcc00fa":"# Cross-validation: evaluating estimator performance","d94a15b1":"<p style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #fdf6e3; color :#002b36   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Blend & Stack Regression: Dataset Housing Prices<\/p>\n\n\n[Housing Prices](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course)\n\n![Ames Housing dataset image](https:\/\/i.imgur.com\/lTJVG4e.png)","1a2a3309":"### Features Categorical","079f34b0":"## OneHotEncoder","1ddeedb4":"# Introduction\n\nObjectives:\n* Applying exploratory data analysis dataset\n* Feature engineering\n* Preprocessing\n* Predicting housing prices","83dd34cf":"# Feature Engineering","b841289a":"Kaggle community of examples inspired.\n\nSource: [Alex Lekov](https:\/\/www.kaggle.com\/itslek\/stack-blend-lrs-xgb-lgb-house-prices-k-v17) and [Ertu\u011frul Demir](https:\/\/www.kaggle.com\/datafan07\/my-top-1-approach-eda-new-models-and-stacking\/notebook) great script  approach were great guides for me!","c8f35f68":"## Missing Values","18a33a11":"**Resume:**\n * TotalSF = TotalBsmtSF + 1stFlrSF + 2ndFlrSF\n * TotalBath = FullBath + HalfBath\n * TotalPorch = OpenPorchSF + EnclosedPorch","32df47b1":"## Model Results","f28f376a":"# Models Regressions\n**Resume:**\n * ExtraTreesRegressor\n * GradientBoostingRegressor\n * HistGradientBoostingRegressor\n * XGBRegressor\n * LGBMRegressor\n * LassoCV\n * ElasticNetCV\n * HuberRegressor\n * SVR","ec4022f6":"### Features Numerical","e80f8ebc":"## Blend Models"}}