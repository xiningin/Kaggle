{"cell_type":{"da4851a3":"code","f9ea90d7":"code","e90b1e8b":"code","d3818258":"code","b76340d5":"code","2510d7f6":"code","60b020f0":"code","1896998f":"code","6a92dfda":"code","10565014":"code","f093fbe7":"code","148ddc9d":"code","b29dcdba":"code","37748546":"code","a3f65660":"code","abe78dae":"markdown","a1e88019":"markdown","147fe695":"markdown","8ed6fc12":"markdown","5c88caca":"markdown","bb040d4b":"markdown"},"source":{"da4851a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nfrom time import time\nfrom torch import nn, optim\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport itertools\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f9ea90d7":"df_train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\ny = df_train.iloc[:,0].values.astype('int32')\nx = df_train.iloc[:,1:].values.astype('float32')\ntest_data = Variable(torch.from_numpy(df_test.values.astype('float32')))","e90b1e8b":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.005, random_state=2)","d3818258":"x_train = Variable(torch.from_numpy(x_train))\nx_test = Variable(torch.from_numpy(x_test))\ny_train = Variable(torch.from_numpy(y_train)).type(torch.LongTensor)\ny_test = Variable(torch.from_numpy(y_test)).type(torch.LongTensor)","b76340d5":"input_size = 784\nhidden_sizes = [128, 64, 32]\noutput_size = 20\n\nmodel = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[2], output_size),\n                      nn.LogSoftmax(dim=1))\nprint(model)","2510d7f6":"criterion = nn.NLLLoss()\n\nlogps = model(x_train) \nloss = criterion(logps, y_train) #calculate the NLL loss","60b020f0":"print('Before backward pass: \\n', model[0].weight.grad)\nloss.backward()\nprint('After backward pass: \\n', model[0].weight.grad)","1896998f":"optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\ntime0 = time()\nepochs = 20\nfor e in range(epochs):\n    running_loss = 0\n    for i in range(100):\n        # Resetting the grad\n        optimizer.zero_grad()\n        \n        y_pred = model(x_train)\n        loss = criterion(y_pred, y_train)\n        \n        #This is where the model learns by backpropagating\n        loss.backward()\n        \n        #And optimizes its weights here\n        optimizer.step()\n        \n        running_loss += loss.item()\n        if i % 100 == 0:\n            print(\"Epoch {} - Training loss: {}\".format(e, running_loss\/len(df_train)))\n        \nprint(\"\\nTraining Time (in minutes) =\",(time()-time0)\/60)","6a92dfda":"with torch.no_grad():\n    logps = model(x_test)\n\ny_pred = torch.exp(logps)\nprobab = list(y_pred.numpy()[0])\nprint(\"Predicted Digit =\", probab.index(max(probab)))\ny_pred = logps\ny_pred_classes = np.argmax(y_pred,axis = 1) ","10565014":"\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","f093fbe7":"confusion_mtx = confusion_matrix(y_test, y_pred_classes) \nplot_confusion_matrix(confusion_mtx, classes = range(10))","148ddc9d":"accuracy_score(y_test, y_pred_classes)","b29dcdba":"with torch.no_grad():\n    logps = model(test_data)\n\ny_pred = torch.exp(logps)\nprobab = list(y_pred.numpy()[0])\nprint(\"Predicted Digit =\", probab.index(max(probab)))","37748546":"y_pred = np.argmax(y_pred,axis = 1)\ny_pred = pd.Series(y_pred,name=\"Label\")","a3f65660":"submission = pd.concat([pd.Series(range(1,len(test_data)),name = \"ImageId\"), y_pred],axis = 1)\nsubmission.to_csv('submission.csv', index=False)","abe78dae":"### Training Model","a1e88019":"### Defining Model","147fe695":"### Defining Loss Function","8ed6fc12":"### Testing with Given Test Data","5c88caca":"### Reading Data","bb040d4b":"### Testing Model"}}