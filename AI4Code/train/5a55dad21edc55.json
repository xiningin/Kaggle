{"cell_type":{"40530ae6":"code","37d3633f":"code","63972247":"code","24ba1b9d":"code","b81983ee":"code","b976dd51":"code","95060ede":"code","56478bf4":"code","91449a76":"code","13d60a64":"code","c31b4f3a":"code","07deceed":"code","ea204ec3":"code","c03a3f75":"code","111e2a2e":"code","279deea0":"code","47780115":"code","41a7b1e9":"code","ad0b5029":"code","920423f1":"code","f23d1077":"code","e419e030":"code","101aae34":"code","ac186147":"code","202d5253":"code","4993d855":"code","1374bdaf":"code","5534f971":"code","996e4c89":"code","da30d932":"code","1ea55039":"code","f19bc96d":"code","f0876eaf":"code","456a0f0d":"code","42f0ebc1":"code","779eef63":"code","0c0b6a34":"code","a6018347":"code","4568d476":"code","cc6d0cf2":"code","b99951e0":"code","d9e0867c":"code","7a41454b":"code","86e1d58d":"code","f3b9449a":"code","1d6dc8c6":"code","9bea3e23":"code","419b3700":"code","75ffd3db":"code","bd322d17":"code","15b6e2ba":"code","d0cd5f75":"code","bee6bf2a":"code","ab01eabb":"code","cf46a7f9":"code","fc4ea030":"code","c93f0723":"code","ff9ef857":"code","d820fef2":"code","0ce94b31":"code","790c10e5":"code","4e0a6807":"code","aab33359":"code","c0a5682e":"code","7490273b":"code","689a9f56":"code","abd09809":"code","d3968823":"code","6c9279e6":"code","48777ce0":"code","8bad0471":"code","b4a349c9":"code","def3e3f7":"code","65fa3d52":"code","904ff857":"code","7e6dfc4d":"code","7649fd42":"code","928858a5":"code","d7ecbf89":"code","21884e43":"code","af5f2816":"code","f7c70f5a":"code","8dedfb0b":"code","2413ae3b":"code","a28ee719":"code","1ca5ce1d":"code","d172d15f":"code","dfe35b6d":"code","71d4a849":"markdown","259c90db":"markdown","a4e27bf9":"markdown","65678421":"markdown","0a7f3e77":"markdown","bdd950e1":"markdown","438aee40":"markdown","be87f090":"markdown","feafedf9":"markdown","53caf3ac":"markdown","1b94a539":"markdown","00fa36ec":"markdown","8faf9bb9":"markdown","3267fced":"markdown","89fda4b7":"markdown","a03d85d8":"markdown","628e3189":"markdown","f1ee1b54":"markdown","54a513d5":"markdown","d86b45f7":"markdown","33e75312":"markdown","557be72e":"markdown","90c9f75b":"markdown","f7bd6194":"markdown","1475e139":"markdown","e66d1f24":"markdown","ad9fc27c":"markdown","902c1c82":"markdown","0b927bd3":"markdown","399587e0":"markdown","8dce0809":"markdown","e91cece8":"markdown","2a4efc1c":"markdown"},"source":{"40530ae6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37d3633f":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder","63972247":"# Check datasets\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","24ba1b9d":"# Load our data\npath = \"\/kaggle\/input\/competitive-data-science-predict-future-sales\/\"\n\nitems = pd.read_csv(path + 'items.csv')\nshops = pd.read_csv(path + 'shops.csv')\ncats = pd.read_csv(path + 'item_categories.csv')\ntrain = pd.read_csv(path + 'sales_train.csv')\n\n# Set index to ID to avoid droping it later\ntest = pd.read_csv(path + 'test.csv').set_index('ID')","b81983ee":"train.head()","b976dd51":"test.head()","95060ede":"items.head()","56478bf4":"cats.head()","91449a76":"# There are items with strange prices and sales. After detailed exploration \n# I decided to remove items with price > 100000 and sales > 1001 (1000 is ok)\nplt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","13d60a64":"# Let's look at the size of our training data\ntrain.shape","c31b4f3a":"# Let us remove two rows of outlier data\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]","07deceed":"# Let us check any rows where item_price is < 0\ntrain.loc[train.item_price<0]\n# Found one! We should fix it because it will affect our predictions","ea204ec3":"# Let us replace this with the average of two prices of the same item\n# Select the entry with same shop_id, item_id, and date_block_num with price > 0 \ntrain[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)]","c03a3f75":"# Compute the median of the two prices\nmedian = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\nmedian","111e2a2e":"# Substitute the negative price with the median\ntrain.loc[train.item_price<0, 'item_price'] = median","279deea0":"shops","47780115":"shops.shape","41a7b1e9":"shops['shop_name'].value_counts()","ad0b5029":"shops['shop_name'].nunique()","920423f1":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\n# Set shop_id to 57 because the name is duplicated\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\n# Set shop_id to 58 because the name is duplicated\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\n# Set shop_id to 11 because the name is duplicated\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","f23d1077":"# The below code return empty since we have replaced these shop_ids-\n# with the same id as the duplicated shops\ntrain.loc[(train.shop_id == 0) | (train.shop_id == 1) | (train.shop_id == 10)]","e419e030":"shops.head()","101aae34":"# Apparently, the shop_name start with the city name\n# We will use this to create a label encoded feature called 'city_code'\nshops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]","ac186147":"shops.head()","202d5253":"cats.head()","4993d855":"# The categories contains the types as well as its subtype\n# (e.g., type: \u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b subtype: PS2)\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]","1374bdaf":"cats.head()","5534f971":"test.shape","996e4c89":"test.head()","da30d932":"test.nunique()","1ea55039":"# Let us compare test items and train items\n# We convert them to set first and then find set 'intersection'\ntest_item_list = set(test.item_id)\ntrain_item_list = set(train.item_id)\n\nintersection_item_list = test_item_list.intersection(train_item_list)\nlen(intersection_item_list)","f19bc96d":"# Items in the test item which are not in the train list. \n# Playing with the idea of extending zero sales for such items?\nlen(list(test_item_list - intersection_item_list))","f0876eaf":"# Returns number of seconds pass since Epoch, 1970\n# Create new matrix\/data frame for monthly sales aggregates using train set\nimport time\nts = time.time()\n\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']","456a0f0d":"for i in range(34):\n    # Get train set data and plug it into sales\n    sales = train[train.date_block_num == i]\n    # Get the unique shop and item id to plug into our monthly matrix data\n    # Dtype set to avoid downcasting\/changing type after concatenation \n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))","42f0ebc1":"matrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)","779eef63":"time.time() - ts","0c0b6a34":"matrix.head()","a6018347":"matrix.shape","4568d476":"# Add new feature revenue which is product of item price and item count per day\n# We will use this to create a trend feature\ntrain['revenue'] = train['item_price'] *  train['item_cnt_day']","cc6d0cf2":"train.head()","b99951e0":"# Note: We can manipulate our dataframe by grouping by a specific feature and aggregating data\n# Example shows grup of shops with total price\ntrain.groupby(['shop_id']).agg({'item_price': ['sum']})","d9e0867c":"# Shows aggregate amount of item count per month\n# This was done using date_block_num grouping as well as shop and item IDs, aggregate SUM\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True); group","7a41454b":"# Combine our matrix data with the item_cnt_month\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0) # Label encoding\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))","86e1d58d":"matrix.head()","f3b9449a":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","1d6dc8c6":"matrix = pd.concat([matrix, test], ignore_index=True, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month","9bea3e23":"matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)","419b3700":"# We will create lag features, which will be used later for mean-encoding values. This is a key function\n# After applying lag features, we can see that on month 34. Values for lag 1, 2, 3, 6, and 12 as being used in our prediction!\n# The respective months refer to values value for month(item_cnt_mean), 33(0.568359), 32(2.511719), 31(2.833984), 28(1.977539), and 22(1.299805) \n# (i.e., from last year 12 months ago)\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","75ffd3db":"matrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')","bd322d17":"matrix.shape","15b6e2ba":"matrix.head()","d0cd5f75":"matrix.columns","bee6bf2a":"matrix.describe()","ab01eabb":"# It seems like we will aggregate the item_cnt_month per date_block_num (monthly) column\nmatrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})","cf46a7f9":"# Same code as above, asign to group\n# We take the mean values for all items per month and add that as a lag feature \ngroup = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\n# This aggregate will be added as date_avg_item_cnt\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\n\n# Apply lag using date_avg_item_count column\n# This basically means that we will check the number of items last month when making predictions!\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\n# After apply lag feature, drop the original column\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)","fc4ea030":"matrix.head()","c93f0723":"matrix[matrix.item_id == 5037].groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\n# As shown in code output for item_id 5037, we have entries from month 20 to 34 (target month)","ff9ef857":"# After applying lag features, we can see that on month 34. Values for lag 1, 2, 3, 6, and 12 as being used in our prediction!\n# The respective months refer to values value for month(item_cnt_mean), 33(0.568359), 32(2.511719), 31(2.833984), 28(1.977539), and 22(1.299805) \n# (i.e., from last year 12 months ago)\nmatrix[matrix.date_block_num == 34]","d820fef2":"# We do the same for all items and months as mentioned above\n# Take note that we are taking the mean values per month and using those as lag features\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)","0ce94b31":"group = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)","790c10e5":"group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)","4e0a6807":"group = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_type_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)","aab33359":"group = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)","c0a5682e":"group = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)","7490273b":"group = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)","689a9f56":"group = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)","abd09809":"group = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)","d3968823":"group = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\n# Compute delta price lag = (average monthly price per item lag - average price per item) \/ (average price per item)\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\n# https:\/\/stackoverflow.com\/questions\/31828240\/first-non-null-value-per-row-from-a-list-of-pandas-columns\/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)","6c9279e6":"matrix.head()","48777ce0":"# Select dataset with 'delta' columns\nmatrix.filter(regex='delta')","8bad0471":"# Compute revenue (revenue = item count * item price)\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)","b4a349c9":"# Make sure month is 1 to 12\nmatrix['month'] = matrix['date_block_num'] % 12\n\n# Days in a month, account for leap years\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","def3e3f7":"cache = {}\nmatrix['item_shop_last_sale'] = -1\nmatrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num","65fa3d52":"cache = {}\nmatrix['item_last_sale'] = -1\nmatrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = row.item_id\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        if row.date_block_num>last_date_block_num:\n            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n            cache[key] = row.date_block_num","904ff857":"matrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')","7e6dfc4d":"matrix = matrix[matrix.date_block_num > 11]","7649fd42":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)","928858a5":"matrix.columns","d7ecbf89":"matrix.info()","21884e43":"# Save our matrix\nmatrix.to_pickle('data.pkl')\n\n# # Delete unused variables\n# del matrix\n# del cache\n# del group\n# del items\n# del shops\n# del cats\n# del train\n\n# don't delete test and leave it for submission\n# garbage collection\ngc.collect();","af5f2816":"data = pd.read_pickle('data.pkl')","f7c70f5a":"data.columns","8dedfb0b":"data = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    'item_cnt_month_lag_1',\n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    #'date_shop_cat_avg_item_cnt_lag_1',\n    #'date_shop_type_avg_item_cnt_lag_1',\n    #'date_shop_subtype_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    #'date_type_avg_item_cnt_lag_1',\n    #'date_subtype_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month',\n    'days',\n    'item_shop_last_sale',\n    'item_last_sale',\n    'item_shop_first_sale',\n    'item_first_sale',\n]]","2413ae3b":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","a28ee719":"del data\ngc.collect();","1ca5ce1d":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ntime.time() - ts","d172d15f":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","dfe35b6d":"plot_features(model, (10,14))","71d4a849":"### Average item count per month and city","259c90db":"### Average item count per month and shop","a4e27bf9":"## Select perfect features","65678421":"### Average item count per month","0a7f3e77":"As per above, we have 60 unique shop names, but upon visual inspection that is not the case and we have to set same shop_id for same shops","bdd950e1":"Fit our XGBoost model and feed our train and validation data","438aee40":"Months since the first sale for each shop\/item pair and for item only.","be87f090":"## Pre-processing for shops, categories, and items ","feafedf9":"## Final preparations\n\nBecause of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).","53caf3ac":"### Average item count per month, item, and city","1b94a539":"## Check for outliers","00fa36ec":"### Average item count per month and item category","8faf9bb9":"Price trends for the last six months","3267fced":"## Month Sales\n\nTest set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to the train. Hence, for the most of the items in the test set target value should be zero. \n\nIn the other hand train set contains only pairs which were sold or returned in the past. Tha main idea is to calculate monthly sales and extend it with zero sales for each unique pair within the month. This way train data will be similar to test data.","89fda4b7":"# Test set processing\n\nTo use time tricks append test pairs to the matrix!","a03d85d8":"Validation strategy is 34 month for the test set, 33 month for the validation set and 13-33 months for the train.","628e3189":"# XGBoost\n\n## Load saved pickle data","f1ee1b54":"Last month shop revenue trend","54a513d5":"## Trends features","d86b45f7":"Months since the last sale for each shop\/item pair and for item only. I use programing approach.\n\nCreate HashTable with key equals to {shop_id,item_id} and value equals to date_block_num. Iterate data from the top. Foreach row if {row.shop_id,row.item_id} is not present in the table, then add it to the table and set its value to row.date_block_num. if HashTable contains key, then calculate the difference beteween cached value and row.date_block_num.","33e75312":"## Target Lags","557be72e":"Producing lags brings a lot of nulls.","90c9f75b":"### Average item count per month and item type","f7bd6194":"## Check for duplicate shops","1475e139":"### Average item count per month, shop, and item sub-type","e66d1f24":"## Shops, items, categories feature processing","ad9fc27c":"### Average item count per month and item","902c1c82":"A lag features is a fancy name for a variable which contains data from prior time steps. If we have time-series data-, \nwe can convert it into rows. Every row contains data about one observation and includes all previous occurrences of that-\nobservation.","0b927bd3":"# Load modules and data\n\n[Predict Future Sales](https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales) is one of the earlier contests I entered, a very simple regression model can be used but you will likely get a lower score compared to another model create using xgboost with good feature engineering, you can to this notebook as standalone. Credits to [dlarionov](https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost) for straightforward feature engineering tips: ","399587e0":"### Average item count per month, shop and item category","8dce0809":"Aggregate train set by shop\/item pairs to calculate target aggreagates, then clip(0,20) target value. This way train target will be similar to the test predictions.\n\nI use floats instead of ints for item_cnt_month to avoid downcasting it after concatination with the test set later. If it would be int16, after concatination with NaN values it becomes int64, but foat16 becomes float16 even with NaNs.\n","e91cece8":"## Mean-encoded features","2a4efc1c":"## Special features\n\nNumber of days in a month. There are no leap years."}}