{"cell_type":{"c2374d85":"code","07c95430":"code","c000876c":"code","29013b72":"code","f1558e06":"code","73c1d78f":"code","5d56466a":"code","b85aa53d":"code","6c19b7ec":"code","c63829a6":"code","a1ede0de":"code","46372f53":"code","9f6acde7":"code","c51414e2":"code","f009da44":"code","7d0f3521":"code","04e2861b":"code","e967002d":"code","7d5ba3a4":"code","08555b77":"code","c06ff8c6":"code","00437d0f":"code","694f6cfc":"code","85d18ade":"code","1f0ea7ee":"code","d1782c8a":"code","5272e627":"code","6e27cd20":"code","1f4bb7fc":"code","959c4111":"code","24750c78":"code","b6c996ac":"code","1ff74e5d":"code","3663fcb0":"code","cc8109f2":"code","91d10e19":"code","f03c276f":"code","2b95e94e":"code","fa420960":"code","dc72c7aa":"code","1cf2780d":"code","a83381a3":"code","7c48196a":"code","df156e76":"code","37e47b2f":"code","15e2f0a4":"code","fe21c1a6":"code","88f750d5":"code","f0a21047":"code","1dbaef7d":"code","feec7ab6":"code","597ec455":"code","5bc7f731":"code","02db4f3f":"markdown","90f744a6":"markdown","7a51872c":"markdown","15553083":"markdown","578902ec":"markdown","8f921818":"markdown","ba85049b":"markdown","e0b1711b":"markdown","b7f7f24e":"markdown","04feb8df":"markdown","4f5ce01c":"markdown","0252bbea":"markdown","2af16b6b":"markdown","45cbc00c":"markdown","7aed5d99":"markdown","279a8e10":"markdown"},"source":{"c2374d85":"#!pip install multiprocess\n#!pip install torchsummary\n#!pip install nltk\n#!pip install torchtext\n#!pip install spacy","07c95430":"import sys\nsys.path.append('..')\nimport os\nimport shutil\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport PIL\nfrom PIL import Image\nimport random\nimport seaborn as sns\nimport pandas as pd\nimport pickle\nimport nltk\n\n#import program.model\n#import program.data\n\nimport torch\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, ToPILImage\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms.functional import to_pil_image\nfrom torch.nn.utils.rnn import pack_padded_sequence\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import models\nfrom torchtext.data.metrics import bleu_score\nfrom torch.utils.tensorboard import SummaryWriter","c000876c":"sns.set_palette('Paired')\nsns.set_style('whitegrid')","29013b72":"#program.data.show_random_images(image_dir='\/content\/drive\/MyDrive\/data\/images', captions_path='\/content\/drive\/MyDrive\/data\/val_captions.csv', count=9)","f1558e06":"image_captions = pd.read_table('..\/input\/split-captions\/train_captions.csv', sep=',')\nimage_captions.head(2)","73c1d78f":"def generate_image_splits(image_folder, test_size=1000):\n    images = np.array(os.listdir('.\/data\/Images\/'))\n    np.random.shuffle(images)\n    train_size = images.shape[0] - test_size\n    train_images = images[:train_size]\n    assert len(train_images) == train_size\n    images = images[train_size:]\n\n    test_images = images\n    assert len(test_images) == test_size\n    \n    return (train_images, test_images)","5d56466a":"generate_splits = False\n\nif generate_splits:\n    train_images, test_images = generate_image_splits(image_folder='.\/data\/train\/', \n                                                                     test_size=1000, validation_size=1000)\n\n    # prepare files to copy with multiprocessing\n    train_images = ['.\/data\/Images\/'+img for img in train_images]\n    train_images = [img + '__' + '.\/data\/train\/' + img.split('\/')[-1] for img in train_images]\n\n    test_images = ['.\/data\/Images\/'+img for img in test_images]\n    test_images = [img + '__' + '.\/data\/test\/' + img.split('\/')[-1] for img in test_images]","b85aa53d":"# Check if works\nfunction_check = False\nif function_check:\n    amount_test_images = 100\n    test_img = train_images[:amount_test_images]\n    program.data.copy_all_files(fp_from_to=test_img)\n\n    for img in tqdm(test_img, desc='Deleting Images again'):\n        assert os.path.isfile(img.split('__')[-1])\n        os.remove(img.split('__')[-1])","6c19b7ec":"copy_files = False\nif copy_files:\n    program.data.copy_all_files(fp_from_to=train_images)\n    program.data.copy_all_files(fp_from_to=validation_images)\n    program.data.copy_all_files(fp_from_to=test_images)","c63829a6":"save_caps_per_split = False\n\nif save_caps_per_split:\n    train_caps = image_captions[image_captions['image'].isin(os.listdir('.\/data\/train\/'))]\n    assert 5*len(os.listdir('.\/data\/train\/')) == train_caps.shape[0]\n    train_caps.to_csv('.\/data\/train_captions.csv', index=False)\n\n    test_caps = image_captions[image_captions['image'].isin(os.listdir('.\/data\/test\/'))]\n    assert 5*len(os.listdir('.\/data\/test\/')) == test_caps.shape[0]\n    test_caps.to_csv('.\/data\/test_captions.csv', index=False)","a1ede0de":"from multiprocess import Pool\nimport re\nfrom collections import Counter","46372f53":"class TextPreprocessor(object):\n    \"\"\"Class handles text data.\"\"\"\n    \n    def __init__(self, n_processes, max_sequence_length=None,\n                 start_token='<START>', end_token='<END>', fill_token='<PAD>', unknown_token='<UNK>', \n                 pad_fill=True, disable_pbar=True):\n        \"\"\"\n        Handles text from csvs, extract tokens with padding or without. Can create a dictionary with words\n        to idx.        \n        \"\"\"\n        self.n_processes = n_processes\n        self.max_sequence_length = max_sequence_length\n        self.start_token = start_token\n        self.end_token = end_token\n        self.fill_token = fill_token\n        self.pad_fill = pad_fill\n        self.unknown_token = unknown_token\n        self.disable_pbar = disable_pbar\n        self.longest_word = None\n        self.tokenized_text = None\n        self.text_dict = None\n        self.text_dict_r = None\n    \n    @staticmethod\n    def _longest_word(X):\n        \"\"\"Extracts longest word from all given sentences\"\"\"\n        X_ = pd.Series(X)\n        sentence_lengths = X_.map(lambda x: len(x.split(' ')))\n        longest_word = max(sentence_lengths)\n        \n        return (sentence_lengths, longest_word)\n\n    def tokenize(self, X, return_tokenized=False):\n        \"\"\"Tokenizes Sentence by the help of multiprocessing\"\"\"\n        self.sentence_lengths, self.longest_word = self._longest_word(X=X)\n        if self.max_sequence_length:\n            # Overwrite props\n            self.longest_word = self.max_sequence_length\n        \n        def __tokenize_words(s):\n            s = re.sub(pattern=r'[^\\w\\s]', repl='', string=s)\n            s = s.lower()\n            #s = s.strip()\n            w = s.split(' ')  \n            if len(w) > self.longest_word:\n                w = w[:self.longest_word]\n            w.insert(0, self.start_token)\n            w.append(self.end_token)\n            if self.pad_fill:\n                while len(w)-2 < self.longest_word:\n                    w.append('<PAD>')\n            \n            return w\n        \n        with Pool(processes=self.n_processes) as pool:\n            tokenized_text = pool.map(__tokenize_words, X)\n        \n        self.tokenized_text = tokenized_text\n        \n        if return_tokenized:\n            return tokenized_text\n    \n    def create_txt_dict(self):\n        \"\"\"Creates a dictionary object which can be used to translate from word to idx\"\"\"\n        self.text_dict = {self.start_token:1, self.end_token:2, self.unknown_token:3, self.fill_token:4}\n        idx = 5\n\n        for text in self.tokenized_text:\n            for word in text:\n                if not self.text_dict.get(word):\n                    self.text_dict[word] = idx\n                    idx += 1\n        \n        self.zero_index()\n            \n        return self\n    \n    def zero_index(self):\n        val_range = np.arange(len(list(self.text_dict.values())))\n        dict_keys = list(self.text_dict.keys())\n        self.text_dict = dict(zip(dict_keys, val_range))\n    \n    def idx_to_word(self, idx: list):\n        \"\"\"Creates a reverse dictionary which can be used to translate from idx back to word\"\"\"\n        if not self.text_dict:\n            self.create_txt_dict()\n        \n        # Create dict to transform words back\n        if not self.text_dict_r:\n            self.text_dict_r = dict(zip(list(self.text_dict.values()), list(self.text_dict.keys())))\n        \n        return ' '.join([self.text_dict_r[id_] for id_ in idx.tolist()])\n    \n    def word_to_idx(self, word):\n        \"\"\"Takes a word and returns its index\"\"\"\n        if not self.text_dict:\n            self.create_txt_dict()\n        \n        return self.text_dict[word]\n    \n    def count_words(self):\n        \"\"\"Counts length of tokenized sentences.\"\"\"\n        concat_lists = []\n\n        for L in tqdm(processor.tokenized_text, disable=self.disable_pbar):\n            for element in L:\n                concat_lists.append(element)\n                \n        return Counter(concat_lists)\n    \nclass TextHandler(TextPreprocessor):\n    \n    def __init__(self, n_processes: int, max_sequence_length: int =None, unknown_threshold: float = None, start_token: str ='<START>', \n                 end_token: str ='<END>', fill_token: str='<PAD>', unknown_token: str ='<UNK>', pad_fill: bool =True, disable_pbar=True):\n        \"\"\"\n        \n        arguments:\n        ------------------\n        n_processes: int\n            Amount processed to use for the tokenization\n            \n        unknown_threshold: int or float\n            Less frequent words to remove from data\n            \n        start_token: str\n            default: '<START>' Start token\n        \n        end_token: str\n            default: '<END>', End token\n        \n        fill_token: str \n            default:'<PAD>', Pad token\n        \n        unknown_token: str\n            default:'<UNK>', unknown token\n        \n        pad_fill: bool\n            default=True, add_idx_for_new_word=False\n        \"\"\"\n        super().__init__(n_processes=n_processes, max_sequence_length=max_sequence_length,\n                         start_token=start_token, end_token=end_token, \n                         fill_token=fill_token, unknown_token=unknown_token, pad_fill=pad_fill, \n                         disable_pbar=disable_pbar)\n        self.unknown_threshold = unknown_threshold\n        self.disable_pbar = disable_pbar\n        \n        \n    def fit(self, X: list):\n        \"\"\"\"\"\"\n        self.tokenize(X, return_tokenized=False)\n        self.create_txt_dict()\n        if self.unknown_threshold:\n            self._tokenize_unknown()\n        \n        return self\n        \n    def fit_transform(self, X: list):\n        \"\"\"\"\"\"\n        self.fit(X)\n        X = self.transform(X)\n        \n        return X\n    \n    def transform(self, X: list):\n        \"\"\"\"\"\"\n        converted_tensors = []\n        for s in tqdm(X, desc='Converting Sentences to Tensors', disable=self.disable_pbar):\n            converted_sentence = self.convert_words_to_tensor(sentence=s)\n            converted_tensors.append(converted_sentence)\n            \n        if len(converted_tensors)==1:\n            converted_tensors = converted_tensors[0]\n        \n        return converted_tensors\n    \n    def inverse_transform(self, X: list):\n        converted_sentences = []\n        for t in tqdm(X, desc='Converting Tensors to Sentences', disable=self.disable_pbar):\n            sentence = self.idx_to_word(t)\n            converted_sentences.append(sentence)\n        \n        return converted_sentences\n        \n    def _tokenize_unknown(self):\n        \"\"\"Replaces less appearing words with unknown token\"\"\"\n        word_counts = pd.Series(self.count_words())\n        words = word_counts[word_counts > self.unknown_threshold].index.to_list()\n        \n        self.text_dict = {self.start_token:1, self.end_token:2, self.unknown_token:3, self.fill_token:4}\n        token = 5\n        for w in words:\n            if w not in list(self.text_dict.keys()):\n                self.text_dict[w] =  token\n                token += 1\n        self.text_dict_r = None\n        \n        self.zero_index()\n\n        return self\n        \n    def convert_words_to_tensor(self, sentence: str):\n        \"\"\"Converts given sentence into a tensor.\"\"\"\n        if not self.text_dict:\n            self.create_txt_dict()\n            \n        # Inner func for tokenization\n        def __tokenize_words(s):\n            s = re.sub(pattern=r'[^\\w\\s]', repl='', string=s)\n            s = s.lower()\n            #s = s.strip()\n            w = s.split(' ')  \n            if len(w) > self.longest_word:\n                w = w[:self.longest_word]\n            w.insert(0, self.start_token)\n            w.append(self.end_token)\n            if self.pad_fill:\n                while len(w)-2 < self.longest_word:\n                    w.append('<PAD>')\n            \n            return w\n        \n        tensor_convert = []\n        for w in __tokenize_words(sentence):\n            try:\n                tensor_convert.append(self.text_dict[w])\n            except:\n                # if word is not in dict\n                tensor_convert.append(self.text_dict[self.unknown_token])\n                \n        return torch.LongTensor(tensor_convert)","9f6acde7":"processor = TextPreprocessor(n_processes=8, max_sequence_length=10, start_token='<START>', end_token='<END>', pad_fill=True)","c51414e2":"processor.tokenize(X = image_captions['caption'].to_list())","f009da44":"word_counts = processor.count_words()\nword_counts = pd.Series(word_counts).sort_values(ascending=False)\nword_counts = word_counts.drop([processor.end_token, processor.start_token, processor.fill_token])\n#word_counts = np.log(word_counts)\n\nfig = plt.subplots(figsize=(10, 4))\np = sns.kdeplot(word_counts, log_scale=True)\np.set_title('Distribution of Word Counts', loc='left')\np.set_xlabel('log(Word Counts)')\nsns.despine()\n\nplt.show()","7d0f3521":"n = 30\nword_counts = processor.count_words()\nword_counts = pd.Series(word_counts).sort_values(ascending=False)\nword_counts = word_counts.drop([processor.end_token, processor.start_token, processor.fill_token])\nfrequent_words = word_counts.head(n)\n\nfig = plt.subplots(figsize=(20, 4))\nplt.subplot(1,2,1)\np = sns.barplot(x=frequent_words.index, y=frequent_words, palette=sns.color_palette('Blues_r', n+10)[:n])\np.set_title(f'{n} most frequent Words', loc='left')\np.set_ylabel('Word Count')\nplt.xticks(rotation=45)\nsns.despine()\n\nplt.subplot(1,2,2)\nnot_frequent_words = word_counts.tail(n)\np = sns.barplot(x=not_frequent_words.index, y=not_frequent_words, color=sns.color_palette('Blues_r', n)[-4])\np.set_title(f'{n} less frequent Words', loc='left')\np.set_ylabel('Word Count')\nplt.xticks(rotation=45)\nsns.despine()\n\nplt.show()","04e2861b":"word_counts[word_counts == 1].shape","e967002d":"processor = TextHandler(n_processes=7, max_sequence_length=None, pad_fill=False, unknown_threshold=1)","7d5ba3a4":"processor.fit(image_captions['caption'].tolist())","08555b77":"save_as_pickle = False\n\nif save_as_pickle:\n    with open('.\/data\/wtoi_reduced.pkl', 'wb') as pkl_file:\n        pickle.dump(processor.text_dict, pkl_file)","c06ff8c6":"#with open('.\/data\/wtoi_reduced.pkl', 'rb') as pkl_file:\n#    processor.text_dict = pickle.load(pkl_file)","00437d0f":"fig = plt.subplots(figsize=(10, 4))\np = sns.histplot(processor.sentence_lengths)\nplt.vlines(x=processor.longest_word, ymin=p.axis()[-2], ymax=p.axis()[-1], \n           color='grey', linestyles='--')\nplt.text(x=processor.longest_word, y=p.axis()[-1]*.90, s=r'$\\leftarrow$ Cropped Here')\np.set_title('Sentence Lengths in Dataset')\np.set_xlabel('Sentence Length')\nsns.despine()\nplt.show()","694f6cfc":"assert all([len(l) for l in processor.tokenized_text]) == True","85d18ade":"image_captions['caption'].tolist()[:2]","1f0ea7ee":"transformed_sentences = processor.transform(image_captions['caption'].tolist())\ntransformed_sentences[:2]","d1782c8a":"transformed_sentences[3].shape","5272e627":"assert all([len(l) for l in transformed_sentences]) == True","6e27cd20":"len(transformed_sentences[0])","1f4bb7fc":"processor.inverse_transform(transformed_sentences)[:2]","959c4111":"example = processor.transform(['My name is Simon and i like deep learning .'])\nexample","24750c78":"processor.inverse_transform([example])","b6c996ac":"torch.cuda.is_available()","1ff74e5d":"import os\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator","3663fcb0":"from torch.utils.data import Dataset\nfrom torchvision.io import read_image\nimport torch\nimport pandas as pd\nimport os\n\nclass CustomDataset(Dataset):\n    \n    def __init__(self, annotations_file, img_dir, annot_transform=None, image_transform=None):\n        self.annotation_file = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.annot_transform = annot_transform\n        self.image_transform = image_transform\n\n    def __len__(self):\n        return len(self.annotation_file)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.to_list()\n                \n        img_path = os.path.join(self.img_dir, self.annotation_file.iloc[idx, 0])\n        image = read_image(img_path)\n        label = self.annotation_file.iloc[idx, 1]\n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.annot_transform:\n            label = self.annot_transform([label])\n            \n        return image, label","cc8109f2":"\"\"\"image_transforms = Compose([ToPILImage(),\n                            Resize((224, 224)), \n                            ToTensor(),\n                            Normalize(mean=[0.485, 0.456, 0.406],\n                                      std=[0.229, 0.224, 0.225])])\"\"\"\nimage_transforms = Compose([ToPILImage(),\n                            Resize((224, 224)), \n                            ToTensor()])","91d10e19":"train_dataset = CustomDataset(annotations_file='..\/input\/split-captions\/train_captions.csv', img_dir='..\/input\/flickr8k\/Images\/', \n                              annot_transform=processor.transform, image_transform=image_transforms)\ntest_dataset = CustomDataset(annotations_file='..\/input\/split-captions\/test_captions.csv', img_dir='..\/input\/flickr8k\/Images\/', \n                              annot_transform=processor.transform, image_transform=image_transforms)","f03c276f":"def collate_fn(data):\n    \"\"\"Function to preprocess Batches after loading it with the Dataloader\"\"\"\n    data.sort(key=lambda x: len(x[1]), reverse=True)\n    images, captions = zip(*data)\n\n    images = torch.stack(images, 0)\n\n    # Extract length per caption\n    lengths = [len(caption) for caption in captions]\n    # Init zero tensor with amx length for each target \n    targets = torch.zeros(len(captions), max(lengths)).long() + 3\n    for i, caption in enumerate(captions):\n        end = lengths[i]\n        # Replace part with target values and fill remaining with zeros\n        targets[i, :end] = caption[:end]  \n        \n    return images, targets, lengths","2b95e94e":"def train_network(encoder, decoder, criterion, optimizer,n_epochs, \n                  dataloader_train, lr_scheduler=None, epsilon=.001,\n                  writer=None, save_checkpoint_at=None, debug_run=False):\n    \"\"\"\n    Trains a neural Network with given inputs and parameters.\n    \n    params:\n    --------------------\n    encoder: torch.Network\n        Image Encoder Network to extract image features from images.\n        \n    decoder: torch.Network\n        Decoder Network for Generating Image Captions\n        \n    criterion: \n        Cost-Function used for the network optimizatio\n        \n    optimizer: torch.Optimizer\n        Optmizer for the network\n        \n    n_epochs: int\n        Defines how many times the whole dateset should be fed through the network\n        \n    dataloader_train: torch.Dataloader \n        Dataloader with the batched dataset\n        \n    dataloader_val: torch.Dataloader\n        Dataloader with validation set to calculate validation loss\n        if None: No validation Loop will be performed during training\n        \n    lr_scheduler: float\n        Learning Rate Scheduler, to adapt LR during training\n        Will be multiplied at n-steps with a given Gamma Hyperparam\n    \n    writer:\n        Tensorboard writer from from torchsummary SummaryWriter Instance\n        \n    epsilon: float\n        Stopping Criterion regarding to change in cost-function between two epochs.\n        \n    save_checkpoint_at: str\n        If this parameter is not None it will save the model at epoch.\n\n    debug_run:\n        If true than only one batch will be put through network.\n        \n    returns:\n    ---------------------\n    encoder:\n        Trained Torch Encoder Model\n\n    decoder:\n        Trained Torch Decoder Model\n        \n    losses: dict\n        dictionary of losses of all batches and Epochs.\n        \n    \"\"\"\n    print(20*'=', 'Start Training', 20*'=')\n    if save_checkpoint_at:\n        try:\n            os.mkdir(save_checkpoint_at)\n        except FileExistsError as ae:\n            print(ae)\n    batch_losses, epoch_losses = [], []\n    dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    encoder.to(dev), decoder.to(dev)\n    criterion.to(dev)\n\n    encoder.train(), decoder.train()\n    overall_length = len(dataloader_train)\n    try: # To interrupt training whenever necessary wihtout loosing progress\n        with tqdm(total=n_epochs*overall_length, disable=debug_run) as pbar:\n            for epoch in range(n_epochs):  # loop over the dataset multiple times\n                running_loss = 0.0\n                for i, data in enumerate(dataloader_train):\n                    # get the inputs\n                    images, captions, lengths = data\n                    images, captions = images.to(dev), captions.to(dev)\n                    # zero the parameter gradients\n                    encoder.zero_grad(), decoder.zero_grad()\n                    \n                    # forward + backward + optimize\n                    features = encoder(images)\n                    out = decoder(features, captions, lengths)\n                    #print('Predicted for first batch', out[:len(captions[0])].argmax(1))\n                    #print('True for first batch', captions[0])\n                    \n                    loss = criterion(out, captions.reshape(-1)) # Targets can be labels as documented for CE-Loss\n                    loss.backward()\n                    optimizer.step()\n\n                    # calc and print stats\n                    batch_losses.append(loss.item())\n                    if writer:\n                        writer.add_scalar('Loss\/batch', loss.item(), i)\n                    running_loss += loss.item()                \n                    pbar.set_description(f'Epoch: {epoch+1}\/{n_epochs} \/\/ Running Loss: {np.round(running_loss, 3)} ')\n                    pbar.update(1)\n                    if debug_run:\n                        print('- Training Iteration passed. -')\n                        break\n                    \n                if debug_run:\n                    # Breaks loop \n                    print('Finished Debug Run')\n                    break\n                \n                if save_checkpoint_at and not debug_run:\n                    torch.save(encoder.state_dict(), save_checkpoint_at + f'checkpoint_encoder_ep{epoch}.ckpt')\n                    torch.save(decoder.state_dict(), save_checkpoint_at + f'checkpoint_decoder_ep{epoch}.ckpt')\n\n                if lr_scheduler:\n                    lr_scheduler.step()\n\n                print(f'Epoch {epoch+1} \/\/ Train Loss: {round(running_loss, 2)}')\n                epoch_losses.append(running_loss)\n                if writer:\n                    writer.add_scalar('Loss\/epoch', running_loss, epoch)\n                if epoch > 0:\n                    diff = np.abs(epoch_losses[-2] - running_loss)\n                    if diff < epsilon:\n                        print('- Network Converged. Stopping Training. -')\n                        break\n                        \n    except KeyboardInterrupt as ke:\n        # Handles Keyboardinterrupt and returns trained network and results\n        print(ke)\n        \n    print(20*'=', 'Finished Training', 20*'=')\n                                                 \n    return encoder, decoder, dict(batch=batch_losses, \n                                    epoch=epoch_losses)","fa420960":"class EncoderCNN(nn.Module):\n    \"\"\"\n    Encoder CNN Network for the encoding of images and generation of hidden state inputs for \n    RNN Decoder Model\n    \"\"\"\n    def __init__(self, embed_size, pretrained=True):\n        \"\"\"\n        \n        arguments:\n        --------------\n        embed_size: int\n            Size of the Word Embedding. (Vector Dimensionality of a Single Word)\n            \n        pretrained: bool\n            If true uses pretrained CNN Model. Freezes all Convolutional and Pooling Layers of\n            CNN Network.\n        \n        \"\"\"\n        super(EncoderCNN, self).__init__()\n        self.pretrained = pretrained\n        self.embed_size = embed_size\n        resnet = models.resnet50(pretrained=pretrained)\n        modules = list(resnet.children())[:-1]      \n        self.resnet = nn.Sequential(*modules)\n        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n        self.bn = nn.BatchNorm1d(embed_size)\n        \n    def forward(self, images):\n        if self.pretrained:\n            # Freezing of the Gradients does not create Graph Structure for autograd\n            with torch.no_grad():\n                features = self.resnet(images)\n        else:\n            # Creates Graph Structure\n            features = self.resnet(images)\n        # Reshape feature maps as a single feature vector\n        features = features.reshape(features.size(0), -1)\n        features = self.linear(features)\n        features = self.bn(features)\n        \n        return features\n    \n\nclass DecoderRNN(nn.Module):\n    \"\"\"\n    Decoder RNN Network for the encoding of image features and generation of Image Captioning    \n    \"\"\"\n    def __init__(self, embed_size, hidden_size, vocab_size, \n                 verbose=False, **kwargs):\n        \"\"\"\n        \n        arguments:\n        ---------------\n        embed_size: int\n            Dimensionality of the Word Embedding. This is used to init the Weigh Matrix for the inputs of the RNN\n            \n        hidden_size: int\n            Dimensionality of the hidden Size. Dimensionality of the hidden states appended behind each other (unrolled)\n            \n        vocab_size: int\n            Size of the vocabulary = Amount of Words given for training.\n            \n        num_layers; int\n             Number of layers of the LSTM to stack on top of each other taking the hidden states as input from the previous layer.       \n        \"\"\"\n        super(DecoderRNN, self).__init__()\n        self.vocab_size = vocab_size \n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.verbose = verbose\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTMCell(embed_size, hidden_size)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.dev = kwargs.get('device') if  kwargs.get('device') else self.__create_device()\n        \n    \n    def __create_device(self):\n        return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n        \n    def forward(self, features, captions, lengths):\n        \"\"\"\n        Forward Prop with Teacher-Forcer Method. \n        Forces the Target to be the next input and not predicted label of previous.\n        \"\"\"\n        hidden = None\n        max_sequence_length = max(lengths)\n        outputs = torch.empty((features.shape[0], captions.shape[1], self.vocab_size)).to(self.dev)\n     \n        # iteration N-1\n        hidden, cell = self.lstm(features, hidden)\n        feature_out = self.linear(hidden)\n        outputs[:,0,:] = feature_out        \n        \n        captions = self.embed(captions)\n        \n        for t in range(max_sequence_length-1): # Not including last word as it need to be predicted by the previous\n            # Step t used to predict step t+1\n            inputs = captions[:, t, :]\n            \n            hidden, cell = self.lstm(inputs, (hidden, cell))\n            out = self.linear(hidden)\n            \n            # Append each prediction to step t+1\n            outputs[:, t+1, :] = out\n                        \n        return outputs.reshape(-1, self.vocab_size)\n    \n    \n    def predict(self, features, max_sentence_length, end_token_id=1):\n        predictions = []\n        hidden = None\n        \n        # Iter N-1\n        hidden, cell = self.lstm(features.unsqueeze(0), hidden)\n        \n        # Create Start Token as Tensor for first input\n        inputs = torch.LongTensor([0])\n\n        for t in range(max_sentence_length):\n            inputs = self.embed(inputs)\n            hidden, cell = self.lstm(inputs, (hidden, cell))\n            out = self.linear(hidden)\n            out = out.argmax(dim=1)\n            if out == end_token_id:\n                break\n            predictions.append(out.item())\n            inputs = out\n\n        return predictions","dc72c7aa":"BATCH_SIZE = 32\nNUM_WORKERS = 2\nEPOCHS = 10\n\nEMBED_SIZE = 512\nHIDDEN_SIZE = 256\n\ndataloader_train = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, \n                              num_workers=NUM_WORKERS, shuffle=True, pin_memory=True, \n                              collate_fn=collate_fn)\ndataloader_test = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, \n                             num_workers=NUM_WORKERS, shuffle=True, pin_memory=True, \n                             collate_fn=collate_fn)","1cf2780d":"encoder = EncoderCNN(embed_size=EMBED_SIZE, pretrained=True)\ndecoder = DecoderRNN(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE,\n                    vocab_size=max(processor.text_dict.values())+1, num_layers=1)\n","a83381a3":"criterion = nn.CrossEntropyLoss()\nparams = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n#params = list(decoder.parameters()) + list(encoder.parameters()) ","7c48196a":"optimizer = torch.optim.Adam(params, lr=.001)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=5, gamma=.9)","df156e76":"encoder, decoder, res = train_network(encoder=encoder, decoder=decoder, \n                                      criterion=criterion, optimizer=optimizer, \n                                       n_epochs=EPOCHS, \n                                      dataloader_train=dataloader_train, \n                                      debug_run=False)","37e47b2f":"save_model = True\n\nif save_model:\n    torch.save(obj=encoder, f='encoder{}eps_{}ed_{}hs_4_pretrained_notransforms'.format(EPOCHS, EMBED_SIZE, HIDDEN_SIZE))\n    torch.save(obj=decoder, f='decoder{}eps_{}ed_{}hs_4_pretrained_notransforms'.format(EPOCHS, EMBED_SIZE, HIDDEN_SIZE))\n    with open('results{}_eps.pkl'.format(EPOCHS), 'wb') as pkl_file:\n        pickle.dump(res, pkl_file)","15e2f0a4":"encoder.cpu() \nencoder.eval()\ndecoder.cpu()\ndecoder.eval()","fe21c1a6":"fig = plt.subplots(figsize=(15, 4))\nplt.subplot(1,2,1)\nplt.plot(res['epoch'])\n\nplt.subplot(1,2,2)\nplt.plot(res['batch'])\n\nplt.show()","88f750d5":"def generate_captions(encoder, decoder, test_batch, limit=None, \n                      end_token_id: int = 1, start_token_id: int = 0, pad_token_id: int = 3,\n                      **kwargs):    \n    position_tokens = [start_token_id, end_token_id, pad_token_id]\n    images, captions, _ = test_batch\n    \n    assert images.shape[0] >= limit\n    \n    ncols = 2\n    nrows = (images.shape[0] \/\/ ncols ) + 1\n    fig = plt.subplots(figsize=(20, nrows*4))\n    \n    with torch.no_grad():\n        for i in range(images.shape[0] if not limit else limit):\n            features = encoder(images[i].unsqueeze(0))\n            predictions = decoder.predict(features.squeeze(), 30, 1)\n            inverse_transform = Compose([ToPILImage()])\n            \n            predictions = [tok for tok in predictions if tok not in position_tokens]\n            predictions = processor.idx_to_word(np.array(predictions))\n            \n            actual = [tok for tok in captions[i] if tok not in position_tokens]\n            actual = processor.idx_to_word(np.array(actual))            \n            \n            plt.subplot(nrows, ncols, i+1)\n            plt.imshow(inverse_transform(images[i]))\n            plt.grid(False)\n            plt.axis('off')\n            \n            plt.title('Predicted: ' + predictions + \n                      '\\nActual: ' + actual, fontdict=dict(fontsize=10))\n            \n        plt.subplots_adjust(hspace=.5)\n\n        plt.show()","f0a21047":"test_batch = next(iter(dataloader_test))\ngenerate_captions(encoder=encoder, decoder=decoder, test_batch=test_batch, max_sequence_length=30, \n                  limit=10, end_token_id=processor.word_to_idx('<END>'), start_token_id=processor.word_to_idx('<START>'), \n                  pad_token_id=processor.word_to_idx('<PAD>'))","1dbaef7d":"def evaluate_bleu(encoder, decoder, test_size=20, \n                  test_images_dir='.\/data\/test\/', test_captions_path = '.\/data\/test_captions.csv', \n                  position_tokens=[0, 1, 3], **kwargs):\n    \"\"\"\n    Takes Encoder and Decoder as input and returns Bleu-Score on images from given set of captions. \n    Will always take all captions in the testset as reference corpus and the predicted caption as reference corpus.\n    \n    params:\n    ----------\n    encoder\n        Encoder Network\n        \n    decoder\n        Decoder Network\n    \n    test_images_dir='.\/data\/test\/'\n        Path to image dir with all relevant images.\n        \n    test_captions_path = '.\/data\/test_captions.csv'\n        path to captions file with all test_captions nevers seen by the network.\n    \n    position_tokens=[0, 1, 3] :\n        Postional Tokens used in tokenization of captions\n    \n    **kwargs:\n        'random_state' for equal sampling\n    \n    returns:\n    ------------\n    bleu_scores: list\n        list of all calculated bleu_scores\n    \n    \"\"\"\n    test_images = os.listdir(test_images_dir)\n    test_captions = pd.read_csv(test_captions_path)\n    np.random.seed(seed=kwargs.get('random_state'))\n    position_tokens = [0, 1, 3]\n    \n    if test_size:\n        sample_image_names = test_captions['image'].unique()\n        np.random.shuffle(sample_image_names)\n        sample_image_names = sample_image_names[:test_size]\n    else:\n        sample_image_names = test_captions['image'].unique()\n    sample_captions = test_captions[test_captions['image'].isin(sample_image_names)]\n    \n    scores = []\n    with torch.no_grad():\n        for img_name in tqdm(sample_image_names, desc='Calculating BLEU-Scores'):\n            \n            img_path = os.path.join(test_images_dir, img_name)\n            image = read_image(img_path)\n\n            all_captions = sample_captions.loc[sample_captions['image'] == img_name, 'caption'].to_list()\n            all_captions = processor.transform(all_captions)\n            all_captions = [caption[1:-1] for caption in all_captions]\n            all_captions = processor.inverse_transform(all_captions)\n            all_captions = [caption.strip().split(' ') for caption in all_captions]\n            max_sequence_length = max([len(cap) for cap in all_captions])\n\n            image = image_transforms(image).unsqueeze(0)\n            features = encoder(image)\n            prediction = decoder.predict(features=features.squeeze(), max_sentence_length=max_sequence_length, end_token_id=1)\n            prediction = [tok for tok in prediction if tok not in position_tokens]\n            prediction = processor.idx_to_word(np.array(prediction))\n            prediction = prediction.strip().split(' ')\n\n            score = bleu_score(candidate_corpus=[prediction], references_corpus=[all_captions])\n            scores.append(score)\n    \n    return scores","feec7ab6":"scores = evaluate_bleu(encoder=encoder, decoder=decoder, test_size=None, \n                       test_images_dir='..\/input\/flickr8k\/Images', \n                       test_captions_path = '..\/input\/split-captions\/test_captions.csv', \n                       position_tokens=[0, 1, 3])","597ec455":"np.mean(scores)","5bc7f731":"plt.hist(scores)\nplt.show()","02db4f3f":"## Preprocess Text-Data","90f744a6":"## Sampling Images","7a51872c":"# Modeling","15553083":"Die Daten habe ich gem\u00e4ss dem Beispiel aus dem Paper von [Vinyals et al.](https:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf) vorgenommen. -> Train: 6000, Validierung: 1000, 1000","578902ec":"**Beschreibung:**\n\nAuc hier kann man die Rechtsshiefe Form der oberen Verteilung bereits erkennen. Dabei sieht man dass F\u00fcllw\u00f6rter und Pr\u00e4ppositionen an erster Stelle auftauchen, was auch so zu erwarten war. Sehr interessant sind W\u00f6rter die danach auftauche wie: man, Two, black, woman, girl etc. Dies gibt bereits einen kleinen Aufschluss dar\u00fcber was auf den Bildern mehrheitlih zu sehen ist. Ein Entfernen der oft vorkommenden W\u00f6rter macht denke ich nicht viel sinn, da diese W\u00f6rter gebraucht werden, um anst\u00e4ndige S\u00e4tze zu bilden.\n\nIm rechten Plot sind die seltensten W\u00f6rter zu sehen, die sich nat\u00fcrlich im Plot stets unterscheiden, da alle meistens nur einmal auftauchen. Die W\u00f6rter sind wurden sehr selten zur Beschreibung der Bilder gebraucht. Daher ist daraus zu schliessen, dass diese Bilder nur einmal im Datensatz auftauchen. Es k\u00f6nnte durchaus sinn machen die wenigvorkommenden W\u00f6rter zu l\u00f6schen, da vermutlich die Datengrundlage zu klein ist, um hier ein Modell anzulernen. Ich werde somit W\u00f6rter entfernen die nur einmal vorkommen.","8f921818":"**Beschreibung**:\n\nMan kann hier eine Long-Tail Distribution sehen, die \u00fcber eine starke Rechtsschiefe verf\u00fcgt. Diese Rechtssschiefe ist ein Zeichen daf\u00fcr, dass die meisten W\u00f6rter wenige Wordcounts haben, wobei es vereinzelte W\u00f6rter gibt die sehr oft vorkommen und die Verteilung so in die breite ziehen. Bei diesem Beispiel habe ich eine Logtransformation angewendet um zu schauen.","ba85049b":"**Beschreibung:**\n\nDas Bild zeigt einige Beispielbilder mit den dazugeh\u00f6rigen Captions. Pro Bild gibt es grunds\u00e4tzlich 5 verschiedene Captions.","e0b1711b":"Der Datensatz mit den Captions kann als CSV eingelesen werden. Pro Bild exisiteren mehrere Captions.","b7f7f24e":"## Generate Train-Validate-Test","04feb8df":"Wir sehen, dass wir sehr viele W\u00f6rter haben, die nur einmal in den Daten vorkommen, deshalb m\u00f6chte ich diese W\u00f6rter mit dem Tag UNK kennzeichnen, da die Datenmenge f\u00fcr jene zu klein ist um passend angelernt zu werden. Ein Vorteil hierbei ist auch die Reduktion der Datenmenge.","4f5ce01c":"Man kann sehen, dass die meisten L\u00e4ngen eher Normalverteilt sind mit Mittelwert um 10. Es hat einige Satzl\u00e4ngen, die sehr lange sind und als Ausreisser bezeichnet werden k\u00f6nnen.\nIch denke, dass es sinn macht diese S\u00e4tze zu reduzieren oder auch zu entfernen in unserem Datensatz. Vorerst werde ich diese Gr\u00f6sse jedoch beibehalten.","0252bbea":"## Loading Data","2af16b6b":"# Sources\n\nhttps:\/\/arxiv.org\/pdf\/1411.4555.pdf\n\nhttps:\/\/www.kaggle.com\/mdteach\/image-captioning-with-out-attention-pytorchhttps:\/\/www.kaggle.com\/mdteach\/image-captioning-with-out-attention-pytorch\n\nhttps:\/\/medium.com\/@stepanulyanin\/captioning-images-with-pytorch-bc592e5fd1a3\n\nhttps:\/\/pytorch.org\/tutorials\/beginner\/nlp\/word_embeddings_tutorial.html\n\nhttps:\/\/medium.com\/@deepeshrishu09\/automatic-image-captioning-with-pytorch-cf576c98d319\n\nhttps:\/\/discuss.pytorch.org\/t\/image-captioning-example-doubt-in-input-size-of-decoder-lstm\/14296\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/04\/solving-an-image-captioning-task-using-deep-learning\/","45cbc00c":"Wie man an dieser Aussage sehen kann wurde das Neue Wort, dass noch nicht in den Daten drinnen war als `<UNK>` gekennzeichnet, was die Abk. unbekannt kennzeichnet.","7aed5d99":"# Data","279a8e10":"# Deep Learning: Image Captioning"}}