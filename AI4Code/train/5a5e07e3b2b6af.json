{"cell_type":{"d6e000cc":"code","57a608a4":"code","b1ac271b":"code","377270fc":"code","9506ff53":"code","67c97567":"code","1af7a681":"code","3589c87d":"code","9e385931":"code","2dd3eec9":"code","be82463b":"code","46be1d8e":"code","4a90e9ec":"code","6d916bd1":"code","7a0c7d68":"code","2517b06c":"code","64a6dec3":"code","6a367b70":"code","aee44ded":"code","d25e93c9":"markdown","e5c11da9":"markdown","64fd0256":"markdown","0ec03960":"markdown","a9ad31c7":"markdown","819384b8":"markdown","c52c80b1":"markdown","99d4ef12":"markdown","2b2e2302":"markdown","85bda27b":"markdown","082180cb":"markdown","4e99592a":"markdown"},"source":{"d6e000cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","57a608a4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\n","b1ac271b":"mnist_train=pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nmnist_test=pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","377270fc":"mnist_train.head()","9506ff53":"mnist_train.info()","67c97567":"#train_data\ntotal = mnist_train.isnull().sum().sort_values(ascending=False)\npercent = (mnist_train.isnull().sum()\/mnist_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","1af7a681":"#test_data\ntotal = mnist_test.isnull().sum().sort_values(ascending=False)\npercent = (mnist_test.isnull().sum()\/mnist_test.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","3589c87d":"Y_train = mnist_train[\"label\"]\nsns.countplot(Y_train)","9e385931":"#creating the X variable for train and test\nX_train = mnist_train.drop(labels = [\"label\"],axis = 1) \nX_test=mnist_test","2dd3eec9":"X_train, X_valid, y_train, y_valid = \\\n    train_test_split(X_train, Y_train, stratify=Y_train, train_size=0.75)","be82463b":"X_train.shape[1]\n","46be1d8e":"input_shape = [X_train.shape[1]]","4a90e9ec":"model = keras.Sequential([\n    layers.Dense(256, activation='relu', input_shape=input_shape),\n    layers.Dropout(0.5),\n    layers.BatchNormalization(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.BatchNormalization(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10)\n])","6d916bd1":"model.compile(\noptimizer=\"adam\",\nloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\nmetrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n)","7a0c7d68":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=4,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=200,\n    callbacks=[early_stopping],\n)\n","2517b06c":"history_df = pd.DataFrame(history.history)\nhistory_df","64a6dec3":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy']].plot(title=\"Accuracy\")","6a367b70":"# predict results\nresults = model.predict(mnist_test)\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")","aee44ded":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"msnit_predictions.csv\",index=False)","d25e93c9":"I am also using an early stopping","e5c11da9":"## 3 NN\nIt is now time to define the model. I will use a simple neural network with 4 Dense layers; the activation will be \"relu\" and I will also use Batchnormalization and Dropout","64fd0256":"We can now split the train data into train and validation data. I will use 25% for validation.","0ec03960":"## 1.Importing libraries and data","a9ad31c7":"## 2. Preprocessing","819384b8":"Let's see if there are missing values","c52c80b1":"Let's take a look","99d4ef12":"I am using \"adam\" as optimizer and sparse categorical cross entropy as the loss function","2b2e2302":"All the digits are, more or less, evenly represented. \nNow it's time to create the X variable; as it is known, the columns contain values from 0 to 255 but I won't normalize them now as I will later use Batchnormalization.","85bda27b":"As expected, no missing values","082180cb":"It's time to prepare the variables for modelling.\nFirstly I'll prepare the target variable to check if all the digits are evenly represented","4e99592a":"Let's take a look at the data"}}