{"cell_type":{"5a4435c6":"code","7834c5d1":"code","eaab335d":"code","c55c3115":"code","75a227e9":"code","113f48c5":"code","418c03c2":"code","fca13d36":"code","264eca0c":"code","8932827d":"code","a5d7f765":"code","f4230bd0":"code","3ed405ba":"code","060ade77":"code","0034b020":"markdown","6cb47bf7":"markdown","84f0107e":"markdown","8430d33f":"markdown","7f079a84":"markdown","6d68b031":"markdown","ee77e0ef":"markdown","ab1b3d5c":"markdown","bf53ba8d":"markdown","7f6edca9":"markdown","992d298a":"markdown","af474ebb":"markdown","ba4f5d95":"markdown","25b95955":"markdown","a22fe602":"markdown","df1f9ce8":"markdown","02e78daf":"markdown","02fe3158":"markdown","836b1d4d":"markdown","77cad9d5":"markdown","bcf65a7b":"markdown","e2001957":"markdown","b3f320d1":"markdown","f5a2b4ae":"markdown","95511a0e":"markdown","344cfc88":"markdown"},"source":{"5a4435c6":"import warnings\nwarnings.filterwarnings(\"ignore\")\n%autosave 150\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pylab as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt","7834c5d1":"file='..\/input\/the-movies-dataset\/'\n\ncols_ratings = ['userId', 'movieId', 'rating']\nratings = pd.read_csv(file+'ratings_small.csv', usecols=cols_ratings)\nratings=ratings.rename(columns={\"userId\": \"user_id\",\"movieId\":\"movie_id\"})\nratings['movie_id'] = ratings['movie_id'].astype('str')\n\ncols_titles = ['id', 'original_title']\ntitles = pd.read_csv(file+'movies_metadata.csv', usecols=cols_titles)\ntitles=titles.rename(columns={\"id\": \"movie_id\",\"original_title\":\"title\"})","eaab335d":"data = pd.merge(ratings, titles)","c55c3115":"def overview(df):\n    print('SHAPE:\\n',df.shape)\n    print('COLUMN NAMES:\\n', df.columns.tolist())\n    print('UNIQUE VALUES PER COLUMN:\\n', df.nunique())\n    print('COLUMNS WITH MISSING DATA:\\n',df.isnull().sum())\n    print('SAMPLE:\\n',df.sample(10))\n    print('INFO:\\n',df.info())","75a227e9":"overview(data)","113f48c5":"def assign_to_testset(df):\n    sampled_ids = np.random.choice(df.index,size=np.int64(np.ceil(df.index.size * 0.2)),replace=False)\n    df.loc[sampled_ids, 'for_testing'] = True\n    return df\n\ndata['for_testing'] = False\ngrouped = data.groupby('user_id', group_keys=False).apply(assign_to_testset)\ndata_train = data[grouped.for_testing == False]\ndata_test = data[grouped.for_testing == True]\nprint(data_train.shape)\nprint(data_test.shape)\nprint(data_train.index & data_test.index)\n\nprint(\"Training data_set has \"+ str(data_train.shape[0]) +\" ratings\")\nprint(\"Test data set has \"+ str(data_test.shape[0]) +\" ratings\")","418c03c2":"def SimPearson(df,user1,user2,items_min=1):\n    #movies rated by of user1\n    data_user1=df[df['user_id']==user1]\n    #movies rated by of user2\n    data_user2=df[df['user_id']==user2]\n    \n    #movies rated by both\n    both_rated=pd.merge(data_user1,data_user2,on='movie_id')\n    \n    if len(both_rated)<2:\n        return 0\n    if len(both_rated)<items_min:\n        return 0\n    res=pearsonr(both_rated.rating_x,both_rated.rating_y)[0]\n    if(np.isnan(res)):\n        return 0\n    return res","fca13d36":"minidata = data[data['user_id']<100] # get only data from 100 users\nprint(minidata.shape)\n\nminidata.loc[:,'for_testing'] = False\ngrouped = minidata.groupby('user_id', group_keys=False).apply(assign_to_testset)\nminidata_train = minidata[grouped.for_testing == False]\nminidata_test = minidata[grouped.for_testing == True]\n\nprint(minidata_train.shape )\nprint(minidata_test.shape )\n\nprint('users:', minidata.user_id.nunique() )\nprint('movies:',minidata.movie_id.nunique() )","264eca0c":"class CF: #Collaborative Filtering\n    def __init__ (self,df,simfunc):\n        self.df=df\n        self.simfunc=simfunc\n        self.sim = pd.DataFrame(np.sum([0]),columns=data_train.user_id.unique(), index=data_train.user_id.unique())\n        \n    def compute_similarities(self):\n        allusers=set(self.df.user_id)\n        self.sim = {} #we are going to create a dictionary with the calculated similarities between users\n        for user1 in allusers:\n            self.sim.setdefault(user1, {})\n            #we take all the movies whatched by user1\n            movies_user1=data_train[data_train['user_id']==user1]['movie_id']\n            #we take all the users that have whatched any of the movies user1 has\n            data_mini=pd.merge(data_train,movies_user1,on='movie_id')\n            \n            for user2 in allusers:\n                if user1==user2:continue\n                self.sim.setdefault(user2, {})\n                if (user1 in self.sim[user2]):continue\n                # we calculate the similarity between user1 and user2\n                simi=self.simfunc(data_mini,user1,user2)\n                if (simi<0):\n                    self.sim[user1][user2]=0\n                    self.sim[user2][user1]=0\n                else: # we store the similarity in the dictionary\n                    self.sim[user1][user2]=simi\n                    self.sim[user2][user1]=simi\n        return self.sim\n    \n    def predict(self,user,movie):\n        allratings=self.df[(self.df['movie_id']==movie)]\n        allusers_movie=set(allratings.user_id)\n        \n        numerator=0.0\n        denominator=0.0\n        \n        for u in allusers_movie:\n            if u==user:continue\n            #we calculate the numerator and denominator of the prediction formula we saw at the beginning\n            numerator+=self.sim[user][u]*float(allratings[allratings['user_id']==u]['rating'])\n            denominator+=self.sim[user][u]\n                \n        if denominator==0: \n            if self.df.rating[self.df['movie_id']==movie].mean()>0:\n            # if the sum of similarities is 0 we use the mean of ratings for that movie\n                return self.df.rating[self.df['movie_id']==movie].mean()\n            else:\n            # else return mean rating for that user\n                return self.df.rating[self.df['user_id']==user].mean()\n        \n        return numerator\/denominator","8932827d":"CF_userbased=CF(df=minidata_train,simfunc=SimPearson)\ndicsim=CF_userbased.compute_similarities()","a5d7f765":"dicsim[2]","f4230bd0":"example_pred=CF_userbased.predict(user=13,movie=1)\nprint(example_pred)","3ed405ba":"def evaluation(dftest):\n    \n    preds_test=[]\n    for user in set(dftest.user_id):\n        for movie in set(dftest[dftest['user_id']==user]['movie_id'].tolist()):\n            pred=CF_userbased.predict(user=user,movie=movie)\n            preds_test.append(\n            {\n                'user_id':user,\n                'movie_id':movie,\n                'pred_rating':pred\n            }\n        )\n            \n    pred_ratings=pd.DataFrame(preds_test)\n    valid_union=pd.merge(pred_ratings,dftest,on=['user_id','movie_id'])\n    \n    real_rating=valid_union.rating.values\n    estimated=valid_union.pred_rating.values\n    \n    rms = sqrt(mean_squared_error(real_rating, estimated))\n    return rms","060ade77":"error=evaluation(minidata_test)\nerror","0034b020":"There's no missing data so no worries about that. I won't do EDA for this exercise, I will jump right into the implemantation of the RECSYS although I do encourage everyone to take some time and get a better understanding of the data","6cb47bf7":"#### Let's evaluate now the predictions measuring the RMSE with the test set.","84f0107e":"We import the csv files with the users ratings and the movie titles. There's more features in the datasets but for this exercise were are gonna keep it simple and just import the necessary ones.","8430d33f":"We can now merge the two data frames into one and let's take general look at what we have","7f079a84":"### Making predictions","6d68b031":"![RECSYS-2.jpg](attachment:RECSYS-2.jpg)","ee77e0ef":"There's no right or wrong option here. It's a good idea to try them all and see what works best for our particular problem. Once we have calculated the similarities between users we can start making predictions","ab1b3d5c":"## Other resources","bf53ba8d":"For this exercise I will use rmse to evaluate the performance:\n\n* $RMSE = \\sqrt{(\\frac{\\sum(\\hat{y}-y)^2}{n})}$\n","7f6edca9":"To compute the predictions we will use the following formula:\n\n$$pred(a,m) = \\frac{\\sum_{b \\in N}{sim(a,b)*(r_{b,m})}}{\\sum_{b \\in N}{sim(a,b)}}$$\n\nWhere: \n\n* $pred(a,m)$ is the predicted rating of user \"a\" for movie \"m\"\n* $sim(a,b)$ is the similarity between user \"a\" and user \"b\"\n* $M$ is the set of common rated movies by user \"a\" and \"b\"\n* $r_{b,m}$ is the rating of movie \"m\" by user \"b\"\n* $\\bar{r_a}$ is the mean rating given by user \"a\"\n","992d298a":"## IMPLEMENTATION","af474ebb":"* # RECOMMENDER SYSTEM - COLLABORATIVE FILTERING (USER-BASED)","ba4f5d95":"We import the necessary packages:","25b95955":"For every user we stored in a dictionary the value of the similarity between him\/her and every other user. Let's take a look at the similarities between user_id=2 and all the others for example:","a22fe602":"## COLLABORATIVE FILTERING (USER-BASED)","df1f9ce8":"Divide the data in two sets: **training and test**","02e78daf":"This type of recommender systems relies on this concept: **I will recommend you one item that other users that are similar to you liked.** . So the underlying question is: \u00bfHow do we measure how similiar two users are?. For our particular case, movie recommendations, we want to recommend a user \"u\" movies that users similar to \"u\" have liked.","02fe3158":"For furher readings on this topic **I would recommend the following notebooks and links** that have been useful for me. \nGood luck!!","836b1d4d":"### Similarity computation","77cad9d5":"\n* https:\/\/www.kaggle.com\/ibtesama\/getting-started-with-a-movie-recommendation-system#Content-Based-Filtering\n* https:\/\/www.kaggle.com\/rounakbanik\/movie-recommender-systems\n* https:\/\/towardsdatascience.com\/recommender-systems-in-practice-cef9033bb23a","bcf65a7b":"### Evaluation metric","e2001957":"I will use **Pearson correlation** to calculate the similarity between two users:","b3f320d1":"The basic idea in similarity computation between two users <i>a<\/i> and <i>b<\/i> is to first isolate the items commonly rated by both users (set <i>M<\/i>), and then to apply a similarity computation technique to determine the similarity.\n    <ul>\n    <li>Euclidean distance<\/li>\n    $$sim(a,b) = \\sqrt{\\sum_{m \\in M}{(r_{a,m} - r_{b,m})^2}}$$\n    <br>\n    <li>Pearson Correlation<\/li>\n    $$sim(a,b) = \\frac{\\sum_{m\\in M} (r_{a,m}-\\bar{r_a})(r_{b,m}-\\bar{r_b})}{\\sqrt{\\sum_{m \\in M}(r_{a,m}-\\bar{r_a})\u00b2}\\sqrt{\\sum_{m \\in M}(r_{b,m}-\\bar{r_b})\u00b2}}$$\n    <br>\n    <li>Cosine distance<\/li>\n    $$ sim(a,b) = \\frac{\\vec{a}\u00b7 \\vec{b}}{|\\vec{a}| * |\\vec{b}|}$$\n    <br>\n    <\/ul>\n  \n<br>\nWhere: \n\n* $sim(a,b)$ is the similarity between user \"a\" and user \"b\"\n* $M$ is the set of common rated movies by user \"a\" and \"b\"\n* $r_{a,m}$ is the rating of movie \"m\" by user \"a\"\n* $\\bar{r_a}$ is the mean rating given by user \"a\"\n","f5a2b4ae":"**This next step is optional:** To avoid spending too much time computing the similarities, we are going to select a small sample of the dataset. We'll take the first 99 users.","95511a0e":"We are now ready to predict the rating of any user for any movie. Let's predict, for example, the rating that user_id 13 would give the movie_id number 1:","344cfc88":"**The goal for this notebook is to build a simple recommender system using the collaborative filtering approach (user-based)**. I will use a dataset with 100k movie ratings."}}