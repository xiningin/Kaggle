{"cell_type":{"6eef1c46":"code","896d4fbf":"code","46f4f510":"code","02f4101f":"code","507ea93a":"code","58999add":"code","54096dd6":"code","bf862458":"code","6d5dbe1b":"code","af3a88a3":"code","5a95ebfc":"code","daa08f5c":"code","795b32ff":"code","e9663225":"code","ce7203b1":"code","58bcbb41":"code","a1d2e416":"code","6a7122d0":"code","c2975185":"code","0f1b12ed":"code","a0fe5943":"code","def49339":"code","32d46afd":"code","9fe9af69":"code","9a10cb12":"code","4dc27e4e":"code","d513ddd9":"code","fd96eedd":"code","e26cca42":"code","da6e9214":"code","82f642c9":"code","ea0f4643":"code","832bc73e":"code","24d664c5":"code","e5f0e033":"code","c9034c6a":"code","dd93a553":"code","50a39d97":"code","bf199a1b":"code","6c1eded4":"code","74eca623":"code","cf1ac72a":"code","c5599925":"code","3bef6602":"code","0dabcfa8":"code","cc9d2ce5":"code","b0a0f7d6":"code","fe5b8921":"code","8fbcca40":"code","ccedda43":"code","28f0b64a":"code","e22eb1a3":"code","db51beb4":"code","7dcb557a":"code","cf1391b2":"code","be08172b":"code","72683ee9":"markdown","6bff874b":"markdown","822859c5":"markdown","2504a0a6":"markdown","2bfcc0d2":"markdown","52782d1e":"markdown","959ed4b0":"markdown","304ff8f6":"markdown","b3f70389":"markdown","638c2339":"markdown","0c916545":"markdown"},"source":{"6eef1c46":"import matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nimport matplotlib.style as stl\nimport seaborn as sns","896d4fbf":"data=pd.read_csv(\"..\/input\/heart-disease-and-stroke-prevention\/dataset.csv\")\npd.set_option('display.max_columns', None)","46f4f510":"data.head()","02f4101f":"data.columns","507ea93a":"data.describe()","58999add":"data.isna().sum()","54096dd6":"filler = data[\"Data_Value\"].mean()\ndata[\"Data_Value\"] = data[\"Data_Value\"].fillna(filler)","bf862458":"data[\"Data_Value_Unit\"].unique()","6d5dbe1b":"data.drop(\"Data_Value_Unit\", axis=1,inplace=True)","af3a88a3":"data.isna().sum()","5a95ebfc":"filler = data[\"Confidence_Limit_High\"].mean()\ndata[\"Confidence_Limit_High\"] = data[\"Confidence_Limit_High\"].fillna(filler)","daa08f5c":"data[\"Data_Value_Footnote\"].unique()","795b32ff":"data.isna().sum()","e9663225":"data.drop(\"Data_Value_Footnote_Symbol\", axis=1,inplace=True)","ce7203b1":"filler = data[\"Confidence_Limit_Low\"].mean()\ndata[\"Confidence_Limit_Low\"] = data[\"Confidence_Limit_Low\"].fillna(filler)","58bcbb41":"data.isna().sum()","a1d2e416":"data.info()","6a7122d0":"numerical=[\"Year\",\"Data_Value\",\"Data_Value_Alt\",\"Confidence_Limit_Low\",\"Confidence_Limit_High\",\"LocationID\"]","c2975185":"import matplotlib.style as style\nstyle.available","0f1b12ed":"style.use(\"ggplot\")\nfor col in numerical:\n    plt.figure(figsize=(12,6))\n    sns.histplot(data[col],color=\"lime\")\n    plt.show()","a0fe5943":"categoricals = []\nfor col in data.columns:\n    if not col in numerical:\n        categoricals.append(col)","def49339":"for col in categoricals:\n    plt.figure(figsize=(12,8))\n    sns.countplot(x=data[col])\n    plt.show()","32d46afd":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\ndata.plot(kind=\"hist\", y=\"Data_Value\", bins=70, color=\"b\", ax=axes[0][0])\ndata.plot(kind=\"hist\", y=\"Confidence_Limit_Low\", bins=100, color=\"r\", ax=axes[0][1])\ndata.plot(kind=\"hist\", y=\"Data_Value_Alt\", bins=6, color=\"g\", ax=axes[1][0])\ndata.plot(kind=\"hist\", y=\"Confidence_Limit_High\", bins=100, color=\"orange\", ax=axes[1][1])\n","9fe9af69":"data.head()","9a10cb12":"data.columns\n","4dc27e4e":"data.info()","d513ddd9":"from category_encoders import CountEncoder\nenc = CountEncoder(normalize=True, cols=['Break_Out_Category', 'BreakoutCategoryID'])\ndata = enc.fit_transform(data)","fd96eedd":"from sklearn.preprocessing import OneHotEncoder\n\ndata= pd.get_dummies(data, columns = ['LocationAbbr', 'Datasource'])\n","e26cca42":"import category_encoders as ce\nordenc=ce.OrdinalEncoder(cols=['LocationDesc','Topic'])\ndata=ordenc.fit_transform(data)","da6e9214":"data.head()","82f642c9":"data.shape","ea0f4643":"data.drop([\"PriorityArea1\",\"PriorityArea2\",\"PriorityArea3\",\"PriorityArea4\"], axis=1, inplace=True)","832bc73e":"data.drop([\"Data_Value_Footnote\"], axis=1, inplace=True)","24d664c5":"data[\"Category\"].unique()","e5f0e033":"maap={\"Cardiovascular Diseases\":0,\"Risk Factors\":1}\nfor i in range(data.shape[0]):\n    status=data[\"Category\"][i]\n    data[\"Category\"][i]=maap[status]\n    ","c9034c6a":"data[\"Category\"].unique()","dd93a553":"data.head()","50a39d97":"data[\"Data_Value_Type\"].unique()","bf199a1b":"map={\"Age-Standardized\":0,\"Crude\":1}\nfor i in range(data.shape[0]):\n    status=data[\"Data_Value_Type\"][i]\n    data[\"Data_Value_Type\"][i]=map[status]\n    ","6c1eded4":"data[\"Data_Value_Type\"].unique()","74eca623":"import category_encoders as ce\nordenc=ce.OrdinalEncoder(cols=['CategoryID','TopicID','Indicator','Data_Value_TypeID','GeoLocation','Break_out','BreakOutID','IndicatorID'])\ndata=ordenc.fit_transform(data)","cf1ac72a":"data.head()","c5599925":"y = data[\"Topic\"]\nx = data.drop('Topic',axis=1)\nfrom sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=0)\n","3bef6602":"data.info()","0dabcfa8":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\nypred = gnb.fit(xtrain, ytrain).predict(xtest)\nypred","cc9d2ce5":"from sklearn.model_selection import cross_validate\ncv_results = cross_validate(gnb, x, y, cv=5)","b0a0f7d6":"cv_results","fe5b8921":"cv_results['test_score']","8fbcca40":"scores = cross_validate(gnb, x, y, cv=5,scoring=('r2', 'neg_mean_squared_error'),return_train_score=True)","ccedda43":"print(scores['train_r2'])","28f0b64a":"from sklearn.tree import DecisionTreeClassifier\nmytree=DecisionTreeClassifier(max_depth=8)","e22eb1a3":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\ncv=KFold(5,random_state=0)\ncross_val_score(mytree,x,y, cv=cv)","db51beb4":"#from sklearn.model_selection import LeaveOneOut\n#cv=LeaveOneOut()\n#cross_val_score(mytree,x,y, cv=cv)","7dcb557a":"from sklearn.model_selection import ShuffleSplit\ncv=ShuffleSplit(6,test_size=0.3)\ncross_val_score(mytree,x,y, cv=cv)","cf1391b2":"from sklearn.model_selection import StratifiedKFold\ncv=StratifiedKFold(6)\ncross_val_score(mytree,x,y, cv=cv)","be08172b":"from sklearn.model_selection import GroupKFold\ncv=StratifiedKFold(6)\ncross_val_score(mytree,x,y, cv=cv)","72683ee9":"#  Encoding","6bff874b":"# Leave One Out Cross Validation.\nThis technique is a special case of the K-Fold. In fact, this is the case where K = \"number of samples in the Dataset\". For example, if a Dataset contains 100 samples, then K = 100. The machine therefore trains on 99 samples and evaluates itself on the last. It thus performs 100 training sessions (out of the 100 possible combinations) which can take a considerable amount of time for the machine.\nThis technique is NOT RECOMMENDED.","822859c5":"__Consiste \u00e0 m\u00e9langer le Dataset, puis \u00e0 le d\u00e9couper en K parties \u00e9gales (K-Fold). Par exemple si le Dataset contient 100 \u00e9chantillons et que K=5, alors nous aurons 5 paquets de 20 \u00e9chantillons. Ensuite, la machine s'entraine sur 4 paquet, puis s'\u00e9value sur le paquet restant, et alterne les diff\u00e9rentes combinaisons de paquet possibles. Au Final, elle effectue donc un nombre K d'entrainement (5 entra\u00eenements dans cette situation).\nCette technique est LARGEMENT UTILIS\u00c9E, mais elle a un l\u00e9ger d\u00e9savantage: si le dataset est h\u00e9t\u00e9rog\u00e8ne et comprend des classes d\u00e9s\u00e9quilibr\u00e9es, alors il se peut que certain splits de Cross-Validation ne contiennent pas les classes minoritaires. Par exemple, si un dataset de 100 \u00e9chantillons contient seulement 10 \u00e9chantillons de la classe 0, et 90 \u00e9chantillons de la classe 1, alors il est possible que sur 5 Folds, certains ne contiennent pas d'\u00e9chantillon de la Classe 0.__","2504a0a6":"# ShuffleSplit Cross-Validation :\nThis technique consists of mixing and then cutting the Dataset into two parts: Part of Train, and part of Test. Once the training and then the evaluation is completed, we collect our data, we remix it, then we re-cut the DataSet in the same proportions as before. We repeat the action for as many Cross-Validation iterations as we want. We can thus find the same data several times in the validation set through the Iterations.\nThis technique is a GOOD ALTERNATIVE to K-FOLD, but it has the same disadvantage: if the classes are unbalanced, then we risk missing information in the Validation game!","2bfcc0d2":"# STRATIFIED K-FOLD\nThis technique is a default choice (but consumes a little more resource than K-FOLD). It consists in mixing the dataset, then letting the machine sort the data in \"Strata\" (that is to say in different classes) before forming a number K of packets (K-Fold) which all contain a little data from each Strata (of each Class).","52782d1e":"# Cross Validation","959ed4b0":"# CountEncoding\/Frequency","304ff8f6":"# OrdinalEncoding","b3f70389":"# KFold Cross-Validation ","638c2339":"# GROUP K-FOLD\nThis Cross-Validation technique is VERY IMPORTANT TO KNOW!\nIn Data Science, we often assume that our data is independent and drawn from the same distribution. For example, the apartments of a Real Estate DataSet are all independent (from each other) and identically distributed.\nBut it's not always the case ! For example, the data in a medical Dataset can depend on each other: if people from the same family are diagnosed with cancer, then the genetic factor creates a dependency between the different data. It is therefore necessary to split the Dataset into an Influence Group, which is why GROUP K-FOLD exists.\nGroupKfold (5) .split (X, y, groups)","0c916545":"# OneHotEncoder"}}