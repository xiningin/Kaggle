{"cell_type":{"92bbfd0b":"code","10abf5cf":"code","80f6a12e":"code","fd01e967":"code","7baf5bb0":"code","fa004893":"code","af3a9560":"code","f244cf36":"code","330194f4":"code","45b1d6b6":"code","f2144584":"code","fbdd9653":"markdown","4062f342":"markdown","eba77119":"markdown","16a50589":"markdown","0a5c7f3c":"markdown","303b4fe0":"markdown","854091ae":"markdown","261bc59a":"markdown","73e4bed5":"markdown","be185f19":"markdown"},"source":{"92bbfd0b":"!pip install transformers","10abf5cf":"import random\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import BatchSampler, SequentialSampler\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntokenizer = AutoTokenizer.from_pretrained(\"joeddav\/xlm-roberta-large-xnli\", padding=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(\"joeddav\/xlm-roberta-large-xnli\").to(device)\nmodel.config","80f6a12e":"train_data = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv', encoding='utf8')\ntest_data = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv', encoding='utf8')\ntrain_data.head()","fd01e967":"train_data['label'] = train_data['label'].replace([0, 2], [2, 0])\ntrain_data.head()","7baf5bb0":"class MyDataset(Dataset):\n    def __init__(self, df, tokenizer, labels = False):\n        self.inputs = df.loc[:,['premise', 'hypothesis']].values\n        self.tokenizer = tokenizer\n        self.labels = labels\n        if self.labels:\n            self.tgt = df['label'].values\n    def __len__(self):\n        return len(self.inputs)\n    def __getitem__(self, idx):\n        inputs = tokenizer(self.inputs[idx].tolist(), add_special_tokens=True, padding=True, return_tensors='pt')\n        if self.labels:\n            inputs['labels'] = self.tgt[idx]\n            return inputs\n        return inputs","fa004893":"def eval_model(model, dataloader, device):\n    correct = 0\n    eval_loss = 0\n    model.eval()\n    for i, batch in enumerate(dataloader):\n        out = model(input_ids=batch['input_ids'].squeeze().to(device),\n                    attention_mask=batch['attention_mask'].squeeze().to(device),\n                    labels=batch['labels'].squeeze().to(device))\n        eval_loss += out[0].item()\n        correct += (out[1].argmax(dim=1)==batch['labels'].squeeze().to(device)).float().sum()\n    accu = correct \/ train_dataloader.dataset.__len__()\n    eval_loss \/= len(dataloader)\n    return eval_loss, accu","af3a9560":"train_dataset = MyDataset(train_data.loc[0:1000], tokenizer, labels=True)\ntrain_dataloader = DataLoader(dataset=train_dataset,\n                              sampler=BatchSampler(\n                                    SequentialSampler(train_dataset), \n                                    batch_size=8, drop_last=True))\nloss, accuracy = eval_model(model, train_dataloader, device)\nprint(\"Loss: {}, Accuracy: {}\".format(loss, accuracy))","f244cf36":"def submission_predict(model, dataloader, device):\n    model.eval()\n    predicts = np.array([])\n    for i, batch in enumerate(dataloader):\n        inp_ids = batch['input_ids'].squeeze().to(device)\n        mask = batch['attention_mask'].squeeze().to(device)\n        out = model(input_ids=inp_ids, attention_mask=mask)\n        batch_preds = out[0].argmax(dim=1)\n        predicts = np.concatenate((predicts, batch_preds.cpu().detach().numpy()))\n    return predicts","330194f4":"test_dataset = MyDataset(test_data, tokenizer, labels=False)\ntest_dataloader = DataLoader(dataset=test_dataset,\n                              sampler=BatchSampler(\n                                    SequentialSampler(test_dataset), \n                                    batch_size=8, drop_last=False), shuffle=False)\ntest_preds = submission_predict(model, test_dataloader, device)","45b1d6b6":"submission = np.concatenate((test_data['id'].values.reshape(-1,1), np.int32(test_preds.reshape(-1,1))), axis=1)\nsubmission = pd.DataFrame(submission, columns=['id', 'prediction'])\nsubmission['prediction'] = submission['prediction'].astype(np.int32).replace([0,2], [2,0])\nsubmission.to_csv('submission.csv', index=False)\nmodel.save_pretrained('.\/model.pt')\ntokenizer.save_pretrained('..\/tokenizer')","f2144584":"submission","fbdd9653":"The model we are using already a fine-tune model. So let us see how it work on our train dataset.","4062f342":"Remember we swipe our entailment and contradiction ids in train data. We will do the same with our test data predicts.","eba77119":"Choose first 1000 sample for evaluating our model and create PyTorch Dataset, DataLoader object.","16a50589":"Import libraries and download xlm-robrta-large-xnli model.","0a5c7f3c":"![image.png](attachment:image.png)<br>\nLeft side image taken from competition data describe part and right side image taken from model.config output which you can see in model loading code cell output. Hopefully you catch it already by seeing the picture that entailment and contradiction label ids are swiped. So we need to swipe it in our DataFrame label column too which can be done easily by run following code cell.","303b4fe0":"Install transformers library","854091ae":"0.935 accuracy is pretty good. Now let us submit our prediction.","261bc59a":"Create a PyTorch Dataset class which will provide us the desire input format for our model.","73e4bed5":"Loading data....","be185f19":"This is an inference Notebook using [**joeddav\/xlm-roberta-large-xnli**](https:\/\/huggingface.co\/joeddav\/xlm-roberta-large-xnli) pre-trained model.<br>\n<br>\n**About Model:**<br>\nThis model was fine-tune on [XNLI](https:\/\/arxiv.org\/pdf\/1809.05053.pdf) dataset which contains 15 different languages and its base model [**xlm-roberta-large**](https:\/\/huggingface.co\/xlm-roberta-large) was trained on 100 different languages. Which mention in this [paper](https:\/\/arxiv.org\/pdf\/1911.02116.pdf)"}}