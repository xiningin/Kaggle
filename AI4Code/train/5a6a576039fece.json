{"cell_type":{"c95393df":"code","9665c2c3":"code","f30f09c8":"code","c921c1c0":"code","99f9b0b5":"code","08449c7d":"code","70cb1691":"code","ea989b9a":"code","27910b55":"code","0c3d8782":"code","3ed67eeb":"code","520dd346":"code","94cea235":"code","62cd2e15":"code","cb69571d":"code","c3f079ad":"code","c4a6bee2":"code","3096286c":"code","d83a0c3a":"code","78de6399":"code","827711c4":"markdown","9834eac7":"markdown","b6812081":"markdown","ad78779f":"markdown","22f5441e":"markdown","b0545d3f":"markdown","b6d3829c":"markdown","04dfbe41":"markdown","d1e7f420":"markdown","27b555f9":"markdown","6372f953":"markdown","d576cb94":"markdown","2043aec9":"markdown","b5381413":"markdown","c93a4139":"markdown","77790a9f":"markdown","54cc43c8":"markdown"},"source":{"c95393df":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9665c2c3":"##Import Data\ndf=pd.read_csv('\/kaggle\/input\/hr-analytics\/HR_comma_sep.csv')\ndf.head()","f30f09c8":"df.describe()","c921c1c0":"##Data Inspection\nprint('Existence of null values: ',df.isnull().values.any())\nprint('Existence of NaN values: ',df.isna().values.any())","99f9b0b5":"df['left'].value_counts(normalize=True)","08449c7d":"department_list=df['Department'].value_counts()\nret_ratio=df.groupby('Department')['left'].value_counts()\nratio_arr=np.zeros(len(department_list))\n\ni=0\nfor j in department_list.keys():\n    #print(j,'--> Stay: ',ret_ratio[j][0],'Left: ',ret_ratio[j][1])\n    ratio_arr[i]=100*ret_ratio[j][1]\/(ret_ratio[j][0]+ret_ratio[j][1])\n    i=i+1\n\nsalary_list=df['salary'].value_counts()\n\nsal_ratio=df.groupby('salary')['left'].value_counts()\nsal_arr=np.zeros(len(salary_list))\n\ni=0\nfor j in salary_list.keys():\n    #print(j,'--> Stay: ',ret_ratio[j][0],'Left: ',ret_ratio[j][1])\n    sal_arr[i]=100*sal_ratio[j][1]\/(sal_ratio[j][0]+sal_ratio[j][1])\n    i=i+1\n\n\nfig,ax = plt.subplots(ncols=2,figsize=(20,5))\n\nplt.sca(ax[0])\n_rt_bar=sns.barplot(x=department_list.keys(),y=ratio_arr)\n_rt_title=plt.title('Resignation Rate Per Department')\nfor bar in _rt_bar.patches:\n    _rt_bar.annotate(format(bar.get_height(), '.2f'),  \n                   (bar.get_x() + bar.get_width() \/ 2,  \n                    bar.get_height()), ha='center', va='center', \n                   size=12, xytext=(0, 8), \n                   textcoords='offset points') \n_rt_xtick=plt.xticks(rotation=45)\n_rt_ylim=plt.ylim(0,100)\n_rt_ylabel=plt.ylabel('%')\n\nplt.sca(ax[1])\n_rt_bar=sns.barplot(x=salary_list.keys(),y=sal_arr)\n_rt_title=plt.title('Resignation Rate Per Salary Level')\nfor bar in _rt_bar.patches:\n    _rt_bar.annotate(format(bar.get_height(), '.2f'),  \n                   (bar.get_x() + bar.get_width() \/ 2,  \n                    bar.get_height()), ha='center', va='center', \n                   size=12, xytext=(0, 8), \n                   textcoords='offset points') \n_rt_xtick=plt.xticks(rotation=45)\n_rt_ylim=plt.ylim(0,100)\n_rt_ylabel=plt.ylabel('%')","70cb1691":"fig,ax = plt.subplots(ncols=3,figsize=(20,5))\n_box=sns.boxplot(data = df,y='satisfaction_level',x='left',showmeans=True,ax=ax[0])\n_box=sns.boxplot(data = df,y='last_evaluation',x='left',showmeans=True,ax=ax[1])\n_box=sns.boxplot(data = df,y='average_montly_hours',x='left',showmeans=True,ax=ax[2])\nfor n in range(0,3):\n    ax[n].set_xticklabels(labels=['Stayed','Left'])\n    ax[n].set_xlabel(None)\n\nfig,ax = plt.subplots(ncols=2,figsize=(10,5))\n_box=sns.boxplot(data = df,y='time_spend_company',x='left',showmeans=True,ax=ax[0])\n_box=sns.boxplot(data = df,y='number_project',x='left',showmeans=True,ax=ax[1])\nfor n in range(0,2):\n    ax[n].set_xticklabels(labels=['Stayed','Left'])\n    ax[n].set_xlabel(None)","ea989b9a":"sns.pairplot(df, hue=\"left\")","27910b55":"fig,ax=plt.subplots(ncols=3,figsize=(20,5))\nsns.scatterplot(data=df,x='satisfaction_level',y='last_evaluation',hue='left', ax=ax[0])\nsns.scatterplot(data=df,x='satisfaction_level',y='average_montly_hours',hue='left', ax=ax[1])\nsns.scatterplot(data=df,x='last_evaluation',y='average_montly_hours',hue='left', ax=ax[2])","0c3d8782":"fig=plt.figure(figsize=(10,5))\nsns.scatterplot(data=df,x='satisfaction_level',y='last_evaluation',hue='left')\nplt.vlines(0.675,0.75,1.0,'red')\nplt.hlines(0.75,0.675,0.95,'red')\nplt.vlines(0.95,0.75,1.0,'red')","3ed67eeb":"df_x=df.loc[(df[\"left\"] == 1) & (df[\"last_evaluation\"] > 0.7) & (df[\"satisfaction_level\"]>0.6)]\nprint('Resigned Cluster Average - Overall Average')\nprint(df_x.mean()-df.mean())\nprint('-----------------------------')\nprint('Resigned Cluster Salary Range')\nprint(df_x['salary'].value_counts())","520dd346":"fig,ax=plt.subplots(ncols=2,figsize=(20,8))\nresign_corr=df.corr()\nmask = np.triu(np.ones_like(resign_corr, dtype=np.bool))\ncat_heatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG',ax=ax[0])\ncat_heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);\n\nheatmap = sns.heatmap(resign_corr[['left']].sort_values(by='left', ascending=False),vmin=-1, vmax=1, annot=True, cmap='BrBG',ax=ax[1])\nheatmap.set_title('Features Correlating with Resignation', fontdict={'fontsize':18}, pad=16);","94cea235":"df_lr=df.copy()\ndf_lr=pd.get_dummies(df_lr, columns = ['Department','salary'])\ndf_lr.head()","62cd2e15":"# fit a logistic regression model on an imbalanced classification dataset\nfrom numpy import mean\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\n\nX = np.asarray(df_lr.loc[:, df_lr.columns != 'left'])\ny = np.asarray(df_lr.loc[:, df_lr.columns == 'left'])","cb69571d":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler().fit(X)","c3f079ad":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","c4a6bee2":"from sklearn.metrics import roc_auc_score,roc_curve\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train,y_train.ravel())\n\ny_pred=model.predict(X_test)\ny_proba=model.predict_proba(X_test)\n\nns_probs = [0 for _ in range(len(y_test))]\nns_auc = roc_auc_score(y_test, ns_probs)\nprint(\"ROC AUC SCORE: \",roc_auc_score(y_test, y_proba[:, 1]))\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, y_proba[:,1])\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\nplt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')","3096286c":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n\ncf_matrix = confusion_matrix(y_test, y_pred)\n\nsns.heatmap(cf_matrix, annot=True, cmap='Blues')\n\nprint(classification_report(y_test, y_pred))","d83a0c3a":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=42)\ngb_clf.fit(X_train, y_train.ravel())\n\ny_gb=gb_clf.predict(X_test)","78de6399":"gb_matrix=confusion_matrix(y_test, y_gb)\nsns.heatmap(gb_matrix, annot=True, cmap='Blues')\nprint(classification_report(y_test, y_gb))","827711c4":"We see above that employee evaluation is almost irrelevant to resignations\/retentions.","9834eac7":"# Gradient Boosting\n\nNow we attempt to model another predictor with gradient boosting classifier, to find out whether it produces a more accurate model.","b6812081":"# Conclusion\n\nWhile Gradient Boosting is the preferred prediction algorithm, it is prone to overfitting due to the imbalanced dataset which is on a 76:23 ratio. A more accurate prediction can be made by gathering more resignee data and using continuous variable for the salary column instead of categorical (low-medium-high).","ad78779f":"We see in the figures above, that HR department has the highest resignation rate, followed by ....\nAlso, in terms of salary, employees that are on the lower group are very likely to leave.\n\nAs for the numerical aspects, we see that employees that handle a large amount of projects, have low evaluation and satisfaction rate on the company, tends to leave. The same applies to employee with high monthly working hours and time employed by the company.\nOn the other hand, promotion does not seem to be a major factor that causes resignation.\n\nNext, we observe and measure the correlation between each features.","22f5441e":"We see some interesting charts above. 3 charts contain clustered datapoints:\n1. Satisfaction vs Last Evaluation\n2. Satisfaction vs Average Monthly Hours\n3. Last Evaluation vs Average Monthly Hours","b0545d3f":"## One Hot Encoding\n\nWe start by assigning numbers to categorical features such as department and salary range.\nFor department separation, we will use one hot encoding, and for salary range we will assign between 1 (low) to 3 (high).","b6d3829c":"# Exploratory Data Analysis\n\nIn this section, we will explore the dataset and calculate some metrics that can illustrate factors that can cause an employee to resign. ","04dfbe41":"# Importing Library and Dataset","d1e7f420":"The clusters are very similar. On the first scatter plot, resigned employees are high performing-dissatisfied, low performing-dissatisfied, or high performing and satisfied people.\n\nSimilarly on the second chart: A lot of resignees are people who worked the highest amount of hours with either low\/high satisfaction level. Then there is a cluster where the resignees worked normal to low monthly hours with low satisfaction level.\n\nHowever on the third chart: The largest resignee cluster comprises of people who performed good and clocked the most hours. Naturally, there is another cluster containing people that performed below average and logged fewer hours than average.","27b555f9":"We see above that gradient boosting yields a far more accurate prediction in both 0 (\"Stayed\") and 1 (\"Resigned\").\nAs such, this should be the preferred prediction model.","6372f953":"# Introduction\n\nEmployee resignation happens everyday. Resignation is a difficult decision because it has a huge impact on an employee's livelihood, especially if they have a family. Despite that, the number of resignation increases every year. An analysis conducted by Compdata, the consulting practice at Salary.com, showed that, based on data from nearly 25,000 organizations of varying sizes in the United States, employee quits increased from 13.5 percent in October 2017 to 14.2 percent in October 2018.\n\nThere are many factors that can influence resignation such as: imbalanced work division, high sum of work hours, dissatisfaction against corporate, salary range, career prospects, etc. This is where HR plays a huge role. Letting high performing employee leave can be more damaging to the organization compared to saving cost for a cheaper but worse performing employee.\n\nIn this notebook we will analyze a sample HR analytics dataset to find out some facts on employee resignation, and will attempt to create a classifier model to predict whether an employee with a specific profile may resign or not.","d576cb94":"# Logistic Regression\n\nIn this section we will explore the correlation coefficient between features and target, and we will utilize a weighted class logistic regression to build our prediction model.\n\n## Correlation\n\nWe analyze the correlation between numerical features and the target.","2043aec9":"# Data Inspection\n\nWe check for missing values, and dataset damages.","b5381413":"Now we can clearly see why. The resignee in this particular cluster, on average are evaluated 0.2 higher, have worked on 0.74 more projects and 42.57 more hours, is employed 1.6 year longer. Most importantly, 562 of them are on the lower salary range. In terms of work accident and promotion, the gaps are negligible (<0.1).","c93a4139":"Logistic Regression produces a really bad prediction considering 1 or \"RESIGNED\" is the important class.","77790a9f":"Here we can see that the target values are imbalanced, with only 23.8% of the data being resignees.","54cc43c8":"The most interesting cluster is the one marked red above. Why did they resign even though their performance and satisfaction level are above average? Let's try to isolate this group and compare with the average metric of the entire dataset."}}