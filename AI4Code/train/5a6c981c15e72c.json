{"cell_type":{"d90f87a9":"code","241c05f9":"code","32fd7a58":"code","72051942":"code","67fca67c":"code","d0b02923":"code","fd3dff46":"code","690aa6a6":"code","0444f3f3":"code","a679d6c8":"code","731e82a3":"code","6fc1a473":"code","113474e1":"code","0925b7bc":"code","791bada5":"code","59a343fa":"code","d75f9720":"code","09b4cc38":"code","071b7deb":"code","ce3d5fd9":"code","66e2db53":"code","89c65055":"code","caa258c3":"code","763cffe1":"code","d626e93c":"code","0a70c2f4":"code","f615e49d":"code","17d19488":"code","5f20af73":"code","d5b0198a":"code","894b97ad":"code","89412be7":"code","1ee6057e":"code","ad1a843b":"code","f084b644":"code","76a077e9":"code","f2a04c72":"code","0e32b5be":"code","53f29e21":"code","d7d3676f":"code","261aeb63":"code","4e8758b9":"code","7ebd15c5":"code","8fe8c68d":"markdown","90e94f22":"markdown","d2f6a2ba":"markdown","12f3e9a7":"markdown","1166817a":"markdown","16085c23":"markdown","c123b10b":"markdown","33e2b822":"markdown","e42236f2":"markdown","f21fd7fa":"markdown","98492f89":"markdown"},"source":{"d90f87a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","241c05f9":"os.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray')","32fd7a58":"original_dataset_dir = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray'\n\nbase_dir ='.\/chest_xray_postprocessed'\nos.mkdir(base_dir)","72051942":"train_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\nval_dir = os.path.join(base_dir, 'val')\nos.mkdir(val_dir)\ntest_dir = os.path.join(base_dir, 'test')\nos.mkdir(test_dir)\n\ntrain_pneumonia_dir = os.path.join(train_dir, 'pneumonia')\nos.mkdir(train_pneumonia_dir)\n\ntrain_normal_dir = os.path.join(train_dir, 'normal')\nos.mkdir(train_normal_dir)\n\nval_pneumonia_dir = os.path.join(val_dir, 'pneumonia')\nos.mkdir(val_pneumonia_dir)\n\nval_normal_dir = os.path.join(val_dir, 'normal')\nos.mkdir(val_normal_dir)\n\ntest_pneumonia_dir = os.path.join(test_dir, 'pneumonia')\nos.mkdir(test_pneumonia_dir)\n\ntest_normal_dir = os.path.join(test_dir, 'normal')\nos.mkdir(test_normal_dir)","67fca67c":"import glob\n\npaths_pneumonia = glob.glob(original_dataset_dir+'\/train\/PNEUMONIA\/*')\npaths_normal = glob.glob(original_dataset_dir+'\/train\/NORMAL\/*')","d0b02923":"print(len(paths_pneumonia))\nprint(len(paths_normal))","fd3dff46":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8,6))\nsns.barplot(['pneumonia','normal'],[len(paths_pneumonia),len(paths_normal)])\nplt.title('Class distribution')\n","690aa6a6":"fname_pneumonia = [x.split('\/')[-1] for x in paths_pneumonia]\nfname_pneumonia[:2]","0444f3f3":"fname_normal = [x.split('\/')[-1] for x in paths_normal]\nfname_normal[:2]","a679d6c8":"import shutil\n\ndef train_val_data_split(pneumonia_list, normal_list, validation_split=0.1):\n    \n    n_pneumonia = len(pneumonia_list)\n    n_normal = len(normal_list)\n    \n    shuffle_idx_pneumonia = [x for x in range(n_pneumonia)]\n    shuffle_idx_normal = [x for x in range(n_normal)]\n    \n    num_train_pneumonia = int((1-validation_split)*n_pneumonia)\n    num_val_pneumonia = n_pneumonia - num_train_pneumonia\n    \n    num_train_normal = int((1-validation_split)*n_normal)\n    num_val_normal = n_normal - num_train_normal\n    \n    for i in range(num_train_pneumonia):\n        src = os.path.join(original_dataset_dir+'\/train\/PNEUMONIA\/', pneumonia_list[i])\n        dst = os.path.join(train_pneumonia_dir, pneumonia_list[i])\n        shutil.copyfile(src,dst)\n        \n    for i in range(num_train_pneumonia,n_pneumonia):\n        src = os.path.join(original_dataset_dir+'\/train\/PNEUMONIA\/', pneumonia_list[i])\n        dst = os.path.join(val_pneumonia_dir, pneumonia_list[i])\n        shutil.copyfile(src,dst)\n        \n    for i in range(num_train_normal):\n        src = os.path.join(original_dataset_dir+'\/train\/NORMAL\/', normal_list[i])\n        dst = os.path.join(train_normal_dir, normal_list[i])\n        shutil.copyfile(src,dst)\n        \n    for i in range(num_train_normal,n_normal):\n        src = os.path.join(original_dataset_dir+'\/train\/NORMAL\/', normal_list[i])\n        dst = os.path.join(val_normal_dir, normal_list[i])\n        shutil.copyfile(src,dst)","731e82a3":"train_val_data_split(fname_pneumonia, fname_normal, validation_split=0.1)","6fc1a473":"def test_data_copy():\n    \n    test_data_pneumonia = glob.glob(original_dataset_dir+'\/test\/PNEUMONIA\/*')\n    test_data_normal = glob.glob(original_dataset_dir+'\/test\/NORMAL\/*')\n    \n    test_fname_pneumonia = [x.split('\/')[-1] for x in test_data_pneumonia]\n    test_fname_normal = [x.split('\/')[-1] for x in test_data_normal]\n    \n    n_pneumonia = len(test_fname_pneumonia)\n    n_normal = len(test_fname_normal)\n    \n    for i in range(n_pneumonia):\n        src = os.path.join(original_dataset_dir+'\/test\/PNEUMONIA\/', test_fname_pneumonia[i])\n        dst = os.path.join(test_pneumonia_dir, test_fname_pneumonia[i])\n        shutil.copyfile(src,dst)\n        \n    for i in range(n_normal):\n        src = os.path.join(original_dataset_dir+'\/test\/NORMAL\/', test_fname_normal[i])\n        dst = os.path.join(test_normal_dir, test_fname_normal[i])\n        shutil.copyfile(src,dst)","113474e1":"test_data_copy()","0925b7bc":"import keras\nkeras.__version__","791bada5":"#os.listdir('.\/chest_xray_postprocessed\/train\/normal\/')","59a343fa":"from PIL import Image\nimport matplotlib.pyplot as plt\n\nfname = '.\/chest_xray_postprocessed\/train\/normal\/IM-0382-0001.jpeg'\nimage = Image.open(fname).convert(\"L\")\narr = np.asarray(image)\nplt.imshow(arr, cmap='gray', vmin=0, vmax=255)\nplt.title('Normal')\nplt.show()","d75f9720":"#os.listdir('.\/chest_xray_postprocessed\/train\/pneumonia\/')","09b4cc38":"fname = '.\/chest_xray_postprocessed\/train\/pneumonia\/person1000_bacteria_2931.jpeg'\nimage = Image.open(fname).convert(\"L\")\narr = np.asarray(image)\nplt.imshow(arr, cmap='gray', vmin=0, vmax=255)\nplt.title('Pneumonia')\nplt.show()","071b7deb":"print(arr.shape)\nprint(np.max(arr))","ce3d5fd9":"from keras import layers\nfrom keras import models\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras import metrics","66e2db53":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128,128,1)))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4),metrics=['acc',metrics.Recall()])","89c65055":"model.summary()","caa258c3":"# Here I'm specifying values for data augmentation during training\ntrain_datagen = ImageDataGenerator(rescale=1.\/255, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n\n# Validation and test data should not be augmented\nval_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory('.\/chest_xray_postprocessed\/train', target_size=(128, 128), batch_size=16, color_mode='grayscale', class_mode='binary')\n\nval_generator = val_datagen.flow_from_directory('.\/chest_xray_postprocessed\/val', target_size=(128, 128), batch_size=16, color_mode='grayscale', class_mode='binary')\n\ntest_generator = test_datagen.flow_from_directory('.\/chest_xray_postprocessed\/test', target_size=(128, 128), batch_size=16, color_mode='grayscale', class_mode='binary', shuffle=False)","763cffe1":"history = model.fit_generator(train_generator, steps_per_epoch = 326, epochs=30, validation_data = val_generator, validation_steps=1)","d626e93c":"# Saving the history data as a dict file\n\nimport pickle \n    \nwith open('.\/trainHistoryDict', 'wb') as file_pi:\n    pickle.dump(history.history, file_pi)","0a70c2f4":"# Displaying curves of loss and accuracy during training\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc)+1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","f615e49d":"model.save_weights('CNN_part2_model.h5')","17d19488":"model.metrics_names","5f20af73":"# evaluate the model\nscores = model.evaluate_generator(test_generator)\nprint(\"{}: {:.3f}\".format(model.metrics_names[1], scores[1]))\nprint(\"{}: {:.3f}\".format(model.metrics_names[2], scores[2]))","d5b0198a":"y_test_prob = model.predict_generator(test_generator, verbose=True)","894b97ad":"index_array = y_test_prob>0.5\nindex_array_1d = np.reshape(index_array,-1)\ny_test_pred = np.zeros(len(y_test_prob), dtype=int)\ny_test_pred[index_array_1d] = 1 #y_test_pred is a binary value (0 or 1) ","89412be7":"y_test_pred[-10:]","1ee6057e":"y_test_true = test_generator.classes # the test_generator classes attribute provides an array of classes for for each instance","ad1a843b":"type(y_test_true)","f084b644":"from sklearn.metrics import roc_auc_score,roc_curve,accuracy_score,recall_score,confusion_matrix,f1_score,precision_score","76a077e9":"print(\"Accuracy on the test set: {:.3f}\".format(accuracy_score(y_test_true,y_test_pred)))","f2a04c72":"print(\"Recall on the test set: {:.3f}\".format(recall_score(y_test_true,y_test_pred)))","0e32b5be":"print(\"Precision on the test set: {:.3f}\".format(precision_score(y_test_true,y_test_pred)))","53f29e21":"fpr, tpr, thresholds = roc_curve(y_test_true,y_test_prob)\nplt.plot(fpr, tpr, label=\"ROC Curve\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR (recall)\")\nplt.legend(loc=4)","d7d3676f":"print(\"ROC AUC score on the test set: {:.3f}\".format(roc_auc_score(y_test_true,y_test_prob)))","261aeb63":"confusion_matrix(y_test_true, y_test_pred)","4e8758b9":"print(\"f1 score on the test set: {:.3f}\".format(f1_score(y_test_true,y_test_pred)))","7ebd15c5":"shutil.rmtree('.\/chest_xray_postprocessed')","8fe8c68d":"Much of this code was stolen from [Deep Learning with Python](https:\/\/www.manning.com\/books\/deep-learning-with-python).","90e94f22":"Now I have successfully done the 90\/10 split of the training\/validation data.  I also copied the test data.  All of this data now sits at '.\/chest_xray_postprocessed'.  I will need to delete this data before committing those notebook, since Kaggle has a limit of 500 output files.\n\nOkay, let's now work on building a model.","d2f6a2ba":"While we're at it, let's quickly display a barplot to show this distribution.  This is obviously an unbalanced dataset, weighted heavily to the chest X-rays with pneumonia.","12f3e9a7":"## Chest X-ray Images Dataset\n\nBen Sturm <br\/>\n11\/22\/19\n\nThe Chest X-ray Images Dataset is avaliable [here](https:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia).  It consists of 5,863 Chest X-ray images consisting of two classes (Pneumonia\/Normal).  The dataset is broken up into a train\/validation\/test sets.  My task is to develop a classifier that takes in the images and outputs 1 (Pneumonia) or 0 (Normal).\n\nThe problem with this dataset is that the validation set is very small (18 examples total), which isn't enough to use to validate the model during training.  So, what I chose to do is split the training data using a 90\/10 split of training\/validation, which I will implement in this notebook.  I also plan to use data augmentation when training my model.  ","1166817a":"### Model evaluation","16085c23":"Using predict_generator to obtain probabilities of each test example with the probability representing the likelihood of the image example belonging to the pneumonia class (class=1).","c123b10b":"Since y_test_prob represents probabilities, we will use the standard decision boundary that a probability>0.5 belongs to the positive class (pneumonia class).","33e2b822":"### Model building using Keras","e42236f2":"I'm going to do a 90\/10 split of training and validation.  This means of the positive class (Pneumonia), we'll have 3489 Train and 387 Validation.  Of the negative class (Normal), we'll have 1208 Train and 134 Validation.","f21fd7fa":"Since the images are grayscale images, which means they consist of 1 channel, I'm going to use a simple convnet which I've stolen from the [Deep Learning with Python](https:\/\/www.manning.com\/books\/deep-learning-with-python) book.","98492f89":"### Copying images to training, validation, and test directories\n\nHere I'm specifying the original dataset directory and the base directory where the postprocessed data will be saved.  The base dictory is located in the working directory of the kernel.  "}}