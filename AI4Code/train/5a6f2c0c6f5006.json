{"cell_type":{"5d364a26":"code","1cbfb57d":"code","dd6101c2":"code","f96030a0":"code","fc91071b":"code","19a9a43e":"code","29d50b5b":"code","7587bd98":"code","7ef5e9f3":"code","1297655a":"code","33e7c6cd":"code","947fd8f7":"code","1bd81a01":"code","ab8fa2a3":"code","4b2a952d":"code","396f76e4":"code","024818f2":"code","b2e3f99a":"code","fdc448d4":"code","0343fd8b":"code","74ea5aba":"code","68f302a3":"code","259e55bc":"code","207ca81b":"code","ba1b109f":"code","0e3296d4":"code","acf2ec0f":"code","1a13de03":"code","3ebe6034":"markdown","b52f2d0a":"markdown","0c8ecd21":"markdown","e56d41d5":"markdown","98b16ef2":"markdown","7161ad01":"markdown","3dfeca7f":"markdown","f731c378":"markdown","17085c9e":"markdown","fe030858":"markdown","642486e5":"markdown","50b71bd6":"markdown","9d5ed459":"markdown","a05cf2bb":"markdown","1c73f1a2":"markdown","6c6654ea":"markdown","9a423889":"markdown","e706205f":"markdown","d6a2d1b9":"markdown","c79d6991":"markdown","0de5af46":"markdown","ff81985a":"markdown","e938509e":"markdown","d51a2c77":"markdown","081f2754":"markdown","0b19cbef":"markdown","ca161290":"markdown","4f07640c":"markdown","6aa1d861":"markdown","8debceb2":"markdown","c4606977":"markdown","5503441c":"markdown","70be58f4":"markdown","e3bbdc77":"markdown","56f88033":"markdown","905f311a":"markdown","d58b9a53":"markdown","bc007ca3":"markdown","2a1e0b13":"markdown","e2eb73f7":"markdown","efcabcaa":"markdown","dcb3eacd":"markdown","f8bdd8d8":"markdown","bc8465d8":"markdown","33a6fe6e":"markdown","6299386a":"markdown","abc532ce":"markdown","54658fda":"markdown","8d331c22":"markdown"},"source":{"5d364a26":"import gc\nimport os\nimport random\nimport wandb\nimport math\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 18})\nplt.style.use('fivethirtyeight')\n\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom termcolor import colored\nfrom IPython import display\n\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\ndevice = torch.device(\"cuda\")","1cbfb57d":"# W&B for experiment tracking\nimport wandb\nwandb.login()","dd6101c2":"class config:\n    DIRECTORY_PATH = \"..\/input\/ventilator-pressure-prediction\"\n    \n    INPUT = \"\/kaggle\/input\/ventilator-pressure-prediction\"\n    TRAIN_FILE_PATH = DIRECTORY_PATH + \"\/train.csv\"\n    TEST_FILE_PATH = DIRECTORY_PATH + \"\/test.csv\"\n    SAMPLE_FILE_PATH = DIRECTORY_PATH + \"\/sample_submission.csv\"\n    OUTPUT = \"\/kaggle\/working\"\n    N_FOLD = 5\n    SKIP_FOLDS = [1, 2, 3, 4]  # only fold-0\n    SEED = 0\n    \n    LR = 2.5e-2\n    N_EPOCHS = 50\n    HIDDEN_SIZE = 64\n    BS = 2048\n    WEIGHT_DECAY = 1e-5\n    \n    NOT_WATCH_PARAM = ['INPUT']\n    \n    EXP_NAME = \"google-brain-ventilator\"\n\n# wandb config\nWANDB_CONFIG = {\n     'competition': 'google-brain', \n              '_wandb_kernel': 'neuracort'\n    }","f96030a0":"def set_seed(seed=config.SEED):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","fc91071b":"train = pd.read_csv(config.TRAIN_FILE_PATH,index_col=0)\ntest  = pd.read_csv(config.TEST_FILE_PATH, index_col=0)\nsample = pd.read_csv(config.SAMPLE_FILE_PATH)","19a9a43e":"train.head()","29d50b5b":"test.head()","7587bd98":"train.info()","7ef5e9f3":"print(f\"Training Dataset Shape: {colored(train.shape, 'yellow')}\")\nprint(f\"Test Dataset Shape: {colored(test.shape, 'yellow')}\")","1297655a":"for col in train.columns:\n    print(col + \":\" + colored(str(len(train[col].unique())), 'yellow'))","33e7c6cd":"def plot_distribution(x, title):\n    \n    \"\"\"\n    Function to obtain the distribution plot of given data.\n    \n    params: x(string)     : Name of the Column for the Plot.\n            title(string) : Title of the Plot\n    \"\"\"\n    sns.displot(train, x = x, kind=\"kde\", bw_adjust=2)\n\n    plt.title(title, fontsize = 15)\n    plt.show()","947fd8f7":"plot_list = [(\"R\", \"Lung Airway Restricted\"),\n             (\"C\", \"Lung Compliance\"),\n             (\"u_in\", \"Control input for the inspiratory solenoid\"),\n             (\"u_out\", \"Control input for the exploratory solenoid valve\"),\n             (\"pressure\", \"Airway pressure\")\n            ]","1bd81a01":"for column, title in plot_list:\n    plot_distribution(x = column, title = title)   ","ab8fa2a3":"fig, ax = plt.subplots(figsize = (12, 8))\nplt.subplot(2, 2, 1)\nsns.countplot(x='R', data=train)\nplt.title('Counts of R in train');\nplt.subplot(2, 2, 2)\nsns.countplot(x='R', data=test)\nplt.title('Counts of R in test');\nplt.subplot(2, 2, 3)\nsns.countplot(x='C', data=train)\nplt.title('Counts of C in train');\nplt.subplot(2, 2, 4)\nsns.countplot(x='C', data=test)\nplt.title('Counts of C in test');","4b2a952d":"#for train set\npair_rc = train.groupby([\"R\", \"C\"]).size().reset_index(name=\"Counts\")\npair_rc[\"R\"] = pair_rc[[\"R\",\"C\"]].apply(lambda cols: (cols[0],cols[1]),axis=1)\npair_rc.drop(\"C\",axis=1,inplace=True)\npair_rc.rename(columns={'R':'R-C pair'},inplace=True)\nfig,ax = plt.subplots(1,2,figsize=(16,4))\nsns.barplot(x=\"R-C pair\",y=\"Counts\",data=pair_rc,ax=ax[0]);\nax[0].set_title(\"Counts of R-C pairs train set\");\n\n#for test set\npair_rc = test.groupby([\"R\", \"C\"]).size().reset_index(name=\"Counts\")\npair_rc[\"R\"] = pair_rc[[\"R\",\"C\"]].apply(lambda cols: (cols[0],cols[1]),axis=1)\npair_rc.drop(\"C\",axis=1,inplace=True)\npair_rc.rename(columns={'R':'R-C pair'},inplace=True)\nsns.barplot(x=\"R-C pair\",y=\"Counts\",data=pair_rc,ax=ax[1]);\nax[1].set_title(\"Counts of R-C pairs test set\");","396f76e4":"corr = train.corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig, ax = plt.subplots(figsize=(14, 14))\n\n# plot heatmap\nsns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8}, vmin=0, vmax=1)\n# yticks\nplt.yticks(rotation=0)\nplt.show()","024818f2":"corr = test.corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig, ax = plt.subplots(figsize=(14, 14))\n\n# plot heatmap\nsns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8}, vmin=0, vmax=1)\n# yticks\nplt.yticks(rotation=0)\nplt.show()","b2e3f99a":"for i in range(1,5,1):\n    one_breath = train[train[\"breath_id\"]==i]\n\n    plt.figure(figsize=(8,6));\n    sns.lineplot(x = 'id',y='pressure',data=one_breath[one_breath['u_out']==0],color='green',label='pressure inhale');\n    sns.lineplot(x = 'id',y='pressure',data=one_breath[one_breath['u_out']==1],color='orange',label='pressure exhale');\n    sns.lineplot(x = 'id',y='u_in',data=one_breath,color='blue',label='valve position')\n    plt.title(f\"Variation of Pressure and Input valve position during breath {i}\");\n    plt.legend();","fdc448d4":"display.Image(\"..\/input\/transformer-architecture\/Transformer.png\")","0343fd8b":"display.Image(\"..\/input\/transformer-architecture\/Attention.png\")","74ea5aba":"# Initialise wandb\nwandb.init(project='google-brain', config=WANDB_CONFIG)","68f302a3":"class VentilatorDataset(Dataset):\n    \n    def __init__(self, df):\n        self.dfs = [_df for _, _df in df.groupby(\"breath_id\")]\n        \n    def __len__(self):\n        return len(self.dfs)\n    \n    def __getitem__(self, item):\n        df = self.dfs[item]\n        \n        X = df[['R_cate', 'C_cate', 'u_in', 'u_out']].values\n        y = df['pressure'].values\n        d = {\n            \"X\": torch.tensor(X).float(),\n            \"y\": torch.tensor(y).float(),\n        }\n        return d","259e55bc":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) \/ d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n    \nclass VentilatorModel(nn.Module):\n    \n    def __init__(self):\n        super(VentilatorModel, self).__init__()\n        # This embedding method from: https:\/\/www.kaggle.com\/theoviel\/deep-learning-starter-simple-lstm\n        self.r_emb = nn.Embedding(3, 2, padding_idx=0)\n        self.c_emb = nn.Embedding(3, 2, padding_idx=0)\n        self.seq_emb = nn.Sequential(\n            nn.Linear(9, config.HIDDEN_SIZE),\n            nn.LayerNorm(config.HIDDEN_SIZE),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n        )\n        self.pos_encoder = PositionalEncoding(d_model=config.HIDDEN_SIZE, dropout=0.2)\n        encoder_layers = nn.TransformerEncoderLayer(d_model=config.HIDDEN_SIZE, nhead=8, dim_feedforward=2048, dropout=0.2, )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=2)\n        self.head = nn.Linear(config.HIDDEN_SIZE, 1)\n        \n        # Encoder\n        initrange = 0.1\n        self.r_emb.weight.data.uniform_(-initrange, initrange)\n        self.c_emb.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, X, y=None):\n        bs = X.shape[0]\n        r_emb = self.r_emb(X[:,:,0].long()).view(bs, 80, -1)\n        c_emb = self.c_emb(X[:,:,1].long()).view(bs, 80, -1)\n        seq_x = torch.cat((r_emb, c_emb, X[:, :, 2:]), 2)\n        h = self.seq_emb(seq_x)\n        h = self.pos_encoder(h)\n        h = self.transformer_encoder(h)\n        regr = self.head(h)\n        \n        if y is None:\n            loss = None\n        else:\n            loss = self.loss_fn(regr.squeeze(2), y)\n            \n        return regr, loss\n    \n    def loss_fn(self, y_pred, y_true):\n        loss = nn.L1Loss()(y_pred, y_true)\n        return loss","207ca81b":"def train_loop(model, optimizer, loader):\n    losses, lrs = [], []\n    model.train()\n    optimizer.zero_grad()\n    for d in loader:\n        out, loss = model(d['X'].to(device), d['y'].to(device))\n        \n        losses.append(loss.item())\n        step_lr = np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean()\n        lrs.append(step_lr)\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    return np.array(losses).mean(), np.array(lrs).mean()\n\ndef valid_loop(model, loader):\n    losses, predicts = [], []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, loss = model(d['X'].to(device), d['y'].to(device))\n        losses.append(loss.item())\n        predicts.append(out.cpu())\n\n    return np.array(losses).mean(), torch.vstack(predicts).squeeze(2).numpy().reshape(-1)\n\ndef test_loop(model, loader):\n    predicts = []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, _ = model(d['X'].to(device))\n        predicts.append(out.cpu())\n\n    return torch.vstack(predicts).squeeze(2).numpy().reshape(-1)","ba1b109f":"from torch.optim.optimizer import Optimizer\nclass Lamb(Optimizer):\n    # Reference code: https:\/\/github.com\/cybertronai\/pytorch-lamb\n\n    def __init__(\n        self,\n        params,\n        lr: float = 1e-3,\n        betas = (0.9, 0.999),\n        eps: float = 1e-6,\n        weight_decay: float = 0,\n        clamp_value: float = 10,\n        adam: bool = False,\n        debias: bool = False,\n    ):\n        if lr <= 0.0:\n            raise ValueError('Invalid learning rate: {}'.format(lr))\n        if eps < 0.0:\n            raise ValueError('Invalid epsilon value: {}'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                'Invalid beta parameter at index 0: {}'.format(betas[0])\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                'Invalid beta parameter at index 1: {}'.format(betas[1])\n            )\n        if weight_decay < 0:\n            raise ValueError(\n                'Invalid weight_decay value: {}'.format(weight_decay)\n            )\n        if clamp_value < 0.0:\n            raise ValueError('Invalid clamp value: {}'.format(clamp_value))\n\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.clamp_value = clamp_value\n        self.adam = adam\n        self.debias = debias\n\n        super(Lamb, self).__init__(params, defaults)\n\n    def step(self, closure = None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    msg = (\n                        'Lamb does not support sparse gradients, '\n                        'please consider SparseAdam instead'\n                    )\n                    raise RuntimeError(msg)\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(\n                        p, memory_format=torch.preserve_format\n                    )\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(\n                        p, memory_format=torch.preserve_format\n                    )\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                # Paper v3 does not use debiasing.\n                if self.debias:\n                    bias_correction = math.sqrt(1 - beta2 ** state['step'])\n                    bias_correction \/= 1 - beta1 ** state['step']\n                else:\n                    bias_correction = 1\n\n                # Apply bias to lr to avoid broadcast.\n                step_size = group['lr'] * bias_correction\n\n                weight_norm = torch.norm(p.data).clamp(0, self.clamp_value)\n\n                adam_step = exp_avg \/ exp_avg_sq.sqrt().add(group['eps'])\n                if group['weight_decay'] != 0:\n                    adam_step.add_(p.data, alpha=group['weight_decay'])\n\n                adam_norm = torch.norm(adam_step)\n                if weight_norm == 0 or adam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm \/ adam_norm\n                state['weight_norm'] = weight_norm\n                state['adam_norm'] = adam_norm\n                state['trust_ratio'] = trust_ratio\n                if self.adam:\n                    trust_ratio = 1\n\n                p.data.add_(adam_step, alpha=-step_size * trust_ratio)\n\n        return loss","0e3296d4":"def main():\n    train_df = pd.read_csv(f\"{config.INPUT}\/train.csv\")\n    test_df = pd.read_csv(f\"{config.INPUT}\/test.csv\")\n    sub_df = pd.read_csv(f\"{config.INPUT}\/sample_submission.csv\")\n    oof = np.zeros(len(train_df))\n    test_preds_lst = []\n\n    gkf = GroupKFold(n_splits=config.N_FOLD).split(train_df, train_df.pressure, groups=train_df.breath_id)\n    for fold, (_, valid_idx) in enumerate(gkf):\n        train_df.loc[valid_idx, 'fold'] = fold\n\n    train_df['C_cate'] = train_df['C'].map({10: 0, 20: 1, 50:2})\n    train_df['R_cate'] = train_df['R'].map({5: 0, 20: 1, 50:2})\n    test_df['C_cate'] = test_df['C'].map({10: 0, 20: 1, 50:2})\n    test_df['R_cate'] = test_df['R'].map({5: 0, 20: 1, 50:2})\n\n    test_df['pressure'] = -1\n    test_dset = VentilatorDataset(test_df)\n    test_loader = DataLoader(test_dset, batch_size=config.BS,\n                             pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n    \n    for fold in range(config.N_FOLD):\n#         if fold in []\n        print(f'Fold-{fold}')\n        train_dset = VentilatorDataset(train_df.query(f\"fold!={fold}\"))\n        valid_dset = VentilatorDataset(train_df.query(f\"fold=={fold}\"))\n\n        set_seed()\n        train_loader = DataLoader(train_dset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),\n                                  worker_init_fn=lambda x: set_seed())\n        valid_loader = DataLoader(valid_dset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n\n        model = VentilatorModel()\n        model.to(device)\n\n        optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n\n        uniqe_exp_name = f\"{config.EXP_NAME}_f{fold}\"\n\n        wandb_config.fold = fold\n        \n        for k, v in dict(vars(config)).items():\n            if k[:2] == \"__\" or k in config.NOT_WATCH_PARAM:\n                continue\n            wandb_config[k] = v\n        wandb.watch(model)\n        \n        os.makedirs(f'{config.OUTPUT}\/{config.EXP_NAME}', exist_ok=True)\n        model_path = f\"{config.OUTPUT}\/{config.EXP_NAME}\/ventilator_f{fold}_best_model.bin\"\n        \n        valid_best_loss = float('inf')\n        for epoch in tqdm(range(config.N_EPOCHS)):\n\n            train_loss, lrs = train_loop(model, optimizer, train_loader)\n            valid_loss, valid_predict = valid_loop(model, valid_loader)\n            valid_score = np.abs(valid_predict - train_df.query(f\"fold=={fold}\")['pressure'].values).mean()\n\n            if valid_loss < valid_best_loss:\n                valid_best_loss = valid_loss\n                torch.save(model.state_dict(), model_path)\n                oof[train_df.query(f\"fold=={fold}\").index.values] = valid_predict\n\n            wandb.log({\n                \"train_loss\": train_loss,\n                \"valid_loss\": valid_loss,\n                \"valid_best_loss\": valid_best_loss,\n                \"valid_score\": valid_score,\n                \"learning_rate\": lrs,\n            })\n            \n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        model.load_state_dict(torch.load(model_path))\n        test_preds = test_loop(model, test_loader)\n        test_preds_lst.append(test_preds)\n        \n        sub_df['pressure'] = test_preds\n        sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/sub_f{fold}.csv\", index=None)\n        \n    train_df['oof'] = oof\n    train_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/oof.csv\", index=None)\n    \n    if len(config.SKIP_FOLDS) == 0:\n        sub_df['pressure'] = np.stack(test_preds_lst).mean(0)\n        sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission.csv\", index=None)\n    \n        cv_score = train_df.apply(lambda x: abs(x['oof'] - x['pressure']), axis=1).mean()\n        print(\"CV:\", cv_score)","acf2ec0f":"if __name__ == \"__main__\":\n    main()","1a13de03":"wandb.finish()","3ebe6034":"### Train Dataset","b52f2d0a":"<center><img src = \"https:\/\/i.guim.co.uk\/img\/media\/60bba82aaeedb75bb5d1d50e51f5e64283ae491a\/0_325_4879_2928\/master\/4879.jpg?width=445&quality=45&auto=format&fit=max&dpr=2&s=21baed785ce44a9e9ca8687e2edf7b04\" width = \"750\" height = \"500\"\/><\/center>\n","0c8ecd21":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents<\/center><\/h2>","e56d41d5":"## Understanding Transformers\n\n### Introduction\n\n- The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. \n- The best performing models also connect the encoder and decoder through an attention mechanism.\n- The Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n- Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train\n---","98b16ef2":"<a id=\"individual-breath-analysis\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Individual Breath Analysis<\/center><\/h2>","7161ad01":"- [AgingCare](https:\/\/www.agingcare.com\/articles\/ventilators-can-help-your-elderly-parent-breath-easier-136879.htm)\n- [Ventilator Pressure Prediction: EDA, FE and models](https:\/\/www.kaggle.com\/artgor\/ventilator-pressure-prediction-eda-fe-and-models)\n- [Simple EDA Beginner](https:\/\/www.kaggle.com\/zhaodianwen\/simple-eda-beginner)\n- [ventilator pressure prediction: EDA](https:\/\/www.kaggle.com\/bibhash123\/ventilator-pressure-prediction-eda)","3dfeca7f":"<a id=\"dataset-distribution\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Dataset Distribution<\/center><\/h2>","f731c378":"## HeatMap","17085c9e":"<a id=\"global-config\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Global Config<\/center><\/h2>","fe030858":"<a id=\"tabular-exploration\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Tabular Exploration<\/center><\/h2>","642486e5":"For the implementation I have referred to [this](https:\/\/www.kaggle.com\/takamichitoda\/ventilator-train-transformer) notebook by Takamichi Toda.","50b71bd6":"### Model Architecture\n\n- Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). \n\n- Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n\n- The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.","9d5ed459":"### Encoder and Decoder\n\n**Encoder:**   \n- The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. \n\n- A residual connection is employed around each ofthe two sub-layers, followed by layer normalization. That is, the output of each sub-layer is  \n`LayerNorm(x + Sublayer(x))`,   \nwhere `Sublayer(x)` is the function implemented by the sub-layer itself. \n\n- To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n\n--- \n**Decoder:**   \n- The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. \n- Similar to the encoder, residual connections are employed around each of the sub-layers, followed by layer normalization. \n- The self-attention sub-layer is modified in the decoder stack to prevent positions from attending to subsequent positions. \n- This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n---","a05cf2bb":"1. [Competition Overview](#competition-overview)  \n2. [Ventilator Understanding](#ventilator-understanding)\n3. [Libraries](#libraries)  \n4. [Weights and Biases](#weights-and-biases)\n4. [Load Datasets](#load-datasets)  \n5. [Tabular Exploration](#tabular-exploration)  \n6. [Dataset Distribution](#dataset-distribution) \n7. [Feature Correlation](#feature-correlation)\n8. [Individual Breath Analysis](#individual-breath-analysis)\n9. [Transformer Model Understanding](#transformer-model-understanding)  \n10. [Transformer Model Implementation](#transformer-model-implementation)\n11. [References](#references)","1c73f1a2":"<a id=\"competition-overview\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Competition Overview<\/center><\/h2>","6c6654ea":"## Training, Test and Validation Loops","9a423889":"### Evaluation Criteria\n\nThe competition will be scored as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath. The expiratory phase is not scored.","e706205f":"<a id=\"libraries\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries<\/center><\/h2>","d6a2d1b9":"### Test Dataset","c79d6991":"## About the Dataset\n\n### Files\n\n- **train.csv** - the training set\n- **test.csv** - the test set\n- **sample_submission.csv** - a sample submission file in the correct format\n\n### Columns\n- `id` - globally-unique time step identifier across an entire file\n- `breath_id` - globally-unique time step for breaths\n- `R` - lung attribute indicating how restricted the airway is (in cmH2O\/L\/S). Physically, this is the change in pressure per change in flow (air volume per time). Intuitively, one can imagine blowing up a balloon through a straw. We can change R by changing the diameter of the straw, with higher R being harder to blow.\n- `C` - lung attribute indicating how compliant the lung is (in mL\/cmH2O). Physically, this is the change in volume per change in pressure. Intuitively, one can imagine the same balloon example. We can change C by changing the thickness of the balloon\u2019s latex, with higher C having thinner latex and easier to blow.\n- `time_step` - the actual time stamp.\n- `u_in` - the control input for the inspiratory solenoid valve. Ranges from 0 to 100.\n- `u_out` - the control input for the exploratory solenoid valve. Either 0 or 1.\n- `pressure` - the airway pressure measured in the respiratory circuit, measured in cmH2O.","0de5af46":"<h1><center>Ventilator: InDepth EDA + Understanding + Model + W&B<\/center><\/h1>\n<h2><center>One Stop for all your needs!<\/center><\/h2>\n                                                      \n<center><img src = \"https:\/\/cdn.dribbble.com\/users\/1083804\/screenshots\/5841972\/google_brain_conceptartboard_1.png\" width = \"750\" height = \"500\"\/><\/center>                                                                                               ","ff81985a":"### Column-wise Unique Values","e938509e":"### Reach Out to me on [LinkedIn](https:\/\/www.linkedin.com\/in\/ishandutta0098)","d51a2c77":"### Dataset Size","081f2754":"<a id=\"transformer-model-understanding\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Transformer Model Understanding<\/center><\/h2>","0b19cbef":"---","ca161290":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.<\/center><\/h3>","4f07640c":"## PyTorch Dataset Class","6aa1d861":"<a id=\"weights-and-biases\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Weights and Biases<\/center><\/h2>","8debceb2":"<a id=\"references\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>References<\/center><\/h2>","c4606977":"## Lamb Optimizer","5503441c":"## Model","70be58f4":"<h1><center>More Plots and Models coming soon!<\/center><\/h1>\n                                                      \n<center><img src = \"https:\/\/static.wixstatic.com\/media\/5f8fae_7581e21a24a1483085024f88b0949a9d~mv2.jpg\/v1\/fill\/w_934,h_379,al_c,q_90\/5f8fae_7581e21a24a1483085024f88b0949a9d~mv2.jpg\" width = \"750\" height = \"500\"\/><\/center> ","e3bbdc77":"### What is a Ventilator?\n\n- A ventilator is a machine that helps people breathe (ventilate). \n- These machines are often used in hospitals as life support for patients who have difficulty breathing or who have lost all ability to breathe on their own. Mechanical ventilation may be either invasive or noninvasive (e.g. using a tight-fitting external mask). \n- Invasive modes require the insertion of internal tubes\/devices through endotracheal intubation or tracheostomy.\n\n### When Are Ventilators Used?\n\nMany diseases and other factors can affect lung function and cause difficulty breathing to the point that a person may need a ventilator to stabilize their condition. Examples include:\n\n- Respiratory infections like pneumonia, influenza (flu) and coronavirus (COVID-19)\n- Lung diseases like asthma, COPD (chronic obstructive pulmonary disease), cystic fibrosis and lung cancer\n- Acute respiratory distress syndrome (ARDS)\n- Damage to the nerves and\/or muscles involved in breathing (can be caused by upper spinal cord injuries, polio, amyotrophic lateral sclerosis, myasthenia gravis, etc.)\n- Brain injury\n- Stroke\n- Drug overdose\n\nPatients who can\u2019t breathe on their own at all also use ventilators while undergoing treatment for the underlying condition(s) that caused respiratory failure or respiratory arrest. Long-term ventilator care may be needed if a patient cannot regain the ability to breathe independently.\n\n### How does a ventilator work?\n\nA ventilator moves air into and out of the lungs (oxygen in and carbon dioxide out). It can be inserted through the mouth or nose, and down the trachea, or through a surgical opening, via tracheostomy. Depending on the patient\u2019s medical condition, they may be able to use a respiratory mask in lieu of the breathing tubes. This is known as non-invasive mechanical ventilation.\n\nThe amount of oxygen the patient receives can be controlled through a monitor connected to the ventilator. If the patient\u2019s condition is particularly delicate, the monitor will be set up to send an alarm to the caregiver indicating an increase in air pressure.\n\nThe machine works by bringing oxygen to the lungs and taking carbon dioxide out of the lungs. This allows a patient who has trouble breathing to receive the proper amount of oxygen. It also helps the patient\u2019s body to heal, since it eliminates the extra energy of labored breathing.","56f88033":"---","905f311a":"The Explanations and Images belong to the original research paper [Attention is All you Need](https:\/\/arxiv.org\/abs\/1706.03762)","d58b9a53":"<a id=\"transformer-model-implementation\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Transformer Model Implementation<\/center><\/h2>","bc007ca3":"### Applications of Attention in the Model\nThe Transformer uses multi-head attention in three different ways:  \n\n- In **encoder-decoder attention** layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models.  \n\n- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.  \n\n- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. This is implemented inside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input of the softmax which correspond to illegal connections.","2a1e0b13":"---","e2eb73f7":"## CountPlots","efcabcaa":"<center><img src = \"https:\/\/i.imgur.com\/1sm6x8P.png\" width = \"750\" height = \"500\"\/><\/center>  ","dcb3eacd":"## Distribution Plot","f8bdd8d8":"<a id=\"ventilator-understanding\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Ventilator Understanding<\/center><\/h2>","bc8465d8":"### Description\n\nIn this competition, you\u2019ll simulate a ventilator connected to a sedated patient's lung. The best submissions will take lung attributes compliance and resistance into account.\n\nIf successful, you'll help overcome the cost barrier of developing new methods for controlling mechanical ventilators. \n\nThis will pave the way for algorithms that adapt to patients and reduce the burden on clinicians during these novel times and beyond. As a result, ventilator treatments may become more widely available to help patients breathe.","33a6fe6e":"## Attention\n\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.","6299386a":"---","abc532ce":"<a id=\"feature-correlation\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Feature Correlation<\/center><\/h2>","54658fda":"<a id=\"load-datasets\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load Datasets<\/center><\/h2>","8d331c22":"**Weights & Biases** is the machine learning platform for developers to build better models faster. \n\nYou can use W&B's lightweight, interoperable tools to \n- quickly track experiments, \n- version and iterate on datasets, \n- evaluate model performance, \n- reproduce models, \n- visualize results and spot regressions, \n- and share findings with colleagues. \n\nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.\n\nIn this notebook I will use Weights and Biases's amazing features to perform wonderful visualizations and logging seamlessly. "}}