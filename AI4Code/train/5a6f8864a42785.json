{"cell_type":{"1a067a4c":"code","839393c4":"code","bb1838ee":"code","d72dde28":"code","9d990726":"code","aebf5f9b":"code","54919454":"code","c857c8b4":"code","26bbd32c":"code","bb0cf2c4":"code","143d3349":"code","a5e2f20f":"code","1391afde":"code","a4c81146":"code","221d30e5":"code","6c4bd466":"code","3d825131":"code","aeeb5a37":"code","dbf58b00":"code","efd09eb9":"code","70729773":"code","8a075534":"code","a8587018":"code","aaae6e9a":"code","d61fb7f4":"code","4f74d783":"code","5b2ce74a":"code","7e20cd4d":"code","4e73c296":"code","2a55ec67":"code","b102de9a":"code","de897e04":"code","14b6c44f":"code","719d0cc5":"code","614f8da8":"code","c03171fa":"code","df155565":"code","41aac1f1":"code","6484b6e7":"code","bc9ff0ab":"code","8df4fb4f":"code","0b834302":"code","ea0980c4":"code","0888ce31":"code","b05e323e":"code","bcd3ac6b":"code","a144c973":"code","03069206":"code","435c4163":"code","4f397239":"code","71536fbf":"code","a668064b":"code","dd61feed":"code","7d5513b2":"code","7739ba35":"code","14e73202":"code","c823b406":"markdown","f4361a72":"markdown","469dff0d":"markdown","7c4be186":"markdown","a744b450":"markdown","35c03a30":"markdown","7e98d9f6":"markdown","13a6d9f4":"markdown","0e806800":"markdown","24052b5b":"markdown","ae8fe47f":"markdown","826d39f8":"markdown","794ebcce":"markdown","a597ac1b":"markdown","f85588bb":"markdown","6f440992":"markdown","2e90b671":"markdown","e9fca012":"markdown","ee1f00fb":"markdown","2313aa08":"markdown","6c7b6184":"markdown","8ba2d711":"markdown","346d3b9d":"markdown","20b26e28":"markdown","967d76d3":"markdown","c8452ddd":"markdown","3ea74e7c":"markdown","82f927e8":"markdown","bb8b38d7":"markdown","3f6408ba":"markdown","eb7792ed":"markdown","612689b6":"markdown","5af23e32":"markdown","ec860122":"markdown","166b215d":"markdown","48318621":"markdown","fae8dc0f":"markdown","e688e996":"markdown"},"source":{"1a067a4c":"import numpy as np;\nimport pandas as pd;\nimport seaborn as sns;\nimport matplotlib.pyplot as plt;\n%matplotlib inline\n\nfrom scipy.stats import zscore\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom scipy.stats import randint\nfrom sklearn.model_selection import train_test_split","839393c4":"raw_data= pd.read_csv('..\/input\/concrete-compressive-strength\/concrete.csv');\nraw_data.head()","bb1838ee":"raw_data.info();","d72dde28":"# doesn't look like any NaN values\n# let's confirm this by using the below technique.\n\nraw_data.isnull().sum()","9d990726":"# finding columns which contains value as 0\ncol_with_zeros=[]\nfor col in raw_data.columns:\n    zero_count = raw_data[raw_data[col]==0].count()[col];\n    if (zero_count > 0):\n        col_with_zeros.append([col, zero_count]);\n        \npd.DataFrame(col_with_zeros,columns=['Columns Name','Count of Zeroes'])","aebf5f9b":"features=raw_data.columns\nfeatures","54919454":"#helper function - to provide index for 2 array.\ndef i_j_counter(rows,columns):\n    i=0\n    j=0\n    indexes=[]\n    while(1>0):\n        indexes.append([i,j])\n        if((j+1)%n_columns==0):\n            j=0\n            i=i+1\n            if(i%n_rows == 0):\n                break;\n        else:\n            j=j+1\n    return indexes;","c857c8b4":"raw_data_desc=pd.DataFrame(raw_data.describe().transpose())\nraw_data_desc['IQR']= raw_data_desc['75%'] - raw_data_desc['25%']\n\nraw_data_desc","26bbd32c":"n_rows=3;\nn_columns=3;\nskew= pd.DataFrame(raw_data.skew(),columns=['value'])\nfig, axes = plt.subplots(n_rows,n_columns, figsize=(16,10))\nfor col,index,skew in zip(features,i_j_counter(n_rows,n_columns),skew['value']):\n    sns.distplot(raw_data[col], ax=axes[index[0],index[1]],label=f'skew {skew: .2f}', color='c')\n    axes[index[0],index[1]].legend(loc='upper right')","bb0cf2c4":"# Normalizing the data\nz_data= raw_data.apply(zscore);","143d3349":"#boxplot has better visulalization from the normalized data.\nplt.figure(figsize=(18,7))\nsns.boxplot(data=z_data[features[0:-1]]);","a5e2f20f":"# cleaning outlier\ndef clean_outliers(data):\n    for col in data.columns[0:-1]:\n        Q1= data[col].quantile(0.25)\n        Q3= data[col].quantile(0.75)\n        IQR=Q3-Q1\n        c1=Q1-(1.5*IQR)\n        c2=Q3+(1.5*IQR)\n        data.loc[data[col] < c1, col] = c1\n        data.loc[data[col] > c2, col] = c2\n    return data","1391afde":"z_data=clean_outliers(z_data);\nraw_data=clean_outliers(raw_data);","a4c81146":"#boxplot has better visulalization from the normalized data.\nplt.figure(figsize=(18,7))\nsns.boxplot(data=z_data[features[0:-1]]);","221d30e5":"corr= raw_data.corr();\nsns.set_context('notebook', font_scale=1.0, rc={\"lines.linewidth\":3.5});\nplt.figure(figsize=(18,7));\n\n#create a mask with all value as 1 of size of corr matrix\nmask= np.zeros_like(corr);\n#makring all elements one above the main diagonal as True\nmask[np.triu_indices_from(mask,1)] =True\nsns.heatmap(corr, mask=mask, fmt='0.2f', annot=True);","6c4bd466":"sns.pairplot(z_data, diag_kind='kde');","3d825131":"raw_data.head()","aeeb5a37":"# we can derive different type of composite features from the given features. Currently we are going to use ratio method to do that.\n# our feautres are ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg','fineagg', 'age']\ncomp_data= pd.DataFrame()\ncomp_data['strength'] = raw_data['strength']\nfor col1 in features[0:-1]:\n    for col2 in features[0:-1]:\n        if col1 != col2:\n            col=f'{col1}\/{col2}'\n            new_col=raw_data[col1]\/raw_data[col2]\n            comp_data[col]=new_col\n            \ncorr=pd.DataFrame(comp_data.corr())\nclean_corr= pd.DataFrame(corr[abs(corr)> 0.50]['strength'])\nclean_corr.dropna(inplace=True)\nclean_corr","dbf58b00":"comp_raw_data= pd.DataFrame(raw_data).copy()\ncomp_raw_data['cement\/water'] = raw_data['cement']\/raw_data['water']\ncomp_raw_data['cement\/coarseagg'] = raw_data['cement']\/raw_data['coarseagg']\ncomp_raw_data['water\/cement'] = raw_data['water']\/raw_data['cement']\ncomp_raw_data['water\/age'] = raw_data['water']\/raw_data['age']\ncomp_raw_data['fineagg\/age'] = raw_data['fineagg']\/raw_data['age']\ncomp_raw_data['age\/water'] = raw_data['age']\/raw_data['water']\ncomp_raw_data.head()","efd09eb9":"def get_poly_pca_dataset(dataset,degree,explained_variance):\n    poly= PolynomialFeatures(degree=degree, interaction_only=True)\n    poly_data=poly.fit_transform(dataset.drop('strength',axis=1))\n    poly_data= pd.DataFrame(poly_data, columns=['feat_'+str(x) for x in range(poly_data.shape[1])])\n    poly_data=poly_data.join(dataset.drop('strength',axis=1)) # joinig back the oroginal columns of the dataset passed.\n    print(f'-Dataset with polynomial degree: {degree} has shape: {poly_data.shape}\\n')\n    \n    pca=PCA(n_components=explained_variance)\n    pca.fit(poly_data)\n    pca_data=pca.transform(poly_data)\n    pca_data=pd.DataFrame(pca_data)\n    print(f'-After PCA, as parameter to explaine variance by {explained_variance*100 : .2f}%.\\n')\n    print(f'-Percentage of the variance explained by each column is:\\n {pca.explained_variance_ratio_*100}')\n    \n    return pca_data","70729773":"# with degree 2\npca_data_2 =get_poly_pca_dataset(comp_raw_data,2,.99)\npca_data_2.head()","8a075534":"# with degree 3\npca_data_3 =get_poly_pca_dataset(comp_raw_data,3,.99)\npca_data_3.head()","a8587018":"# with degree 4\npca_data_4 =get_poly_pca_dataset(comp_raw_data,4,.99)\npca_data_4.head()","aaae6e9a":"comp_z_data= comp_raw_data.apply(zscore)\nclusters=range(2,20)\nmean_distortions=[]\nfor val in clusters:\n    kmeans =KMeans(n_clusters=val)\n    kmeans.fit(comp_z_data)\n    mean_distortions.append(sum(np.min(cdist(comp_z_data,kmeans.cluster_centers_),axis=1))\/comp_z_data.shape[0])","d61fb7f4":"plt.plot(clusters, mean_distortions,'bx-')\nplt.xlabel('No. Of Clusters')\nplt.ylabel('Distortion')\nplt.title('Elbow Method')","4f74d783":"labeled_data=pd.DataFrame(comp_raw_data).copy()\nkmeans= KMeans(n_clusters=5)\nkmeans.fit(comp_z_data)\nlabeled_data['labels']=kmeans.labels_","5b2ce74a":"labeled_data.head()","7e20cd4d":"labeled_data.boxplot(by='labels', layout=(6,3), figsize=(20,20));","4e73c296":"labeled_data.groupby('labels').mean()","2a55ec67":"labeled_data.groupby('labels').count()","b102de9a":"for val in features[0:-1]:\n    with sns.axes_style(\"white\"):\n        plot = sns.lmplot(val,'strength',data=labeled_data,hue='labels');\n    ","de897e04":"pipelines=[]\npipelines.append(('LinearRegression',Pipeline([('Scaler', StandardScaler()), ('LinearRegression',LinearRegression())])))\npipelines.append(('DecisionTreeRegressor',Pipeline([('Scaler', StandardScaler()), ('DecisionTreeRegressor',DecisionTreeRegressor(random_state=1))])))\npipelines.append(('RandomForestRegressor',Pipeline([('Scaler', StandardScaler()), ('RandomForestRegressor',RandomForestRegressor(random_state=1))])))\npipelines.append(('SVR',Pipeline([('Scaler', StandardScaler()), ('SVR',SVR())])))\npipelines.append(('GradientBoostingRegressor',Pipeline([('Scaler', StandardScaler()), ('GradientBoostingRegressor',GradientBoostingRegressor(random_state=1))])))","14b6c44f":"def get_pipeline_with_degree(degree):\n    pipelines_pca=[]\n    pipelines_pca.append(('LinearRegression',Pipeline([('Scaler', StandardScaler()),('polynomialFeature',PolynomialFeatures(degree=degree,interaction_only=True)),('PCA',PCA(n_components=.99)), ('LinearRegression',LinearRegression())])))\n    pipelines_pca.append(('DecisionTreeRegressor',Pipeline([('Scaler', StandardScaler()),('polynomialFeature',PolynomialFeatures(degree=degree,interaction_only=True)),('PCA',PCA(n_components=.99)), ('DecisionTreeRegressor',DecisionTreeRegressor(random_state=1))])))\n    pipelines_pca.append(('RandomForestRegressor',Pipeline([('Scaler', StandardScaler()), ('polynomialFeature',PolynomialFeatures(degree=degree,interaction_only=True)),('PCA',PCA(n_components=.99)), ('RandomForestRegressor',RandomForestRegressor(random_state=1))])))\n    pipelines_pca.append(('SVR',Pipeline([('Scaler', StandardScaler()),('polynomialFeature',PolynomialFeatures(degree=degree,interaction_only=True)),('PCA',PCA(n_components=.99)), ('SVR',SVR())])))\n    pipelines_pca.append(('GradientBoostingRegressor',Pipeline([('Scaler', StandardScaler()),('polynomialFeature',PolynomialFeatures(degree=degree,interaction_only=True)),('PCA',PCA(n_components=.99)), ('GradientBoostingRegressor',GradientBoostingRegressor(random_state=1))])))\n    \n    return pipelines_pca;","719d0cc5":"def cv_results(X,y,pipelines):\n    results = pd.DataFrame(columns=['Name','Mean Variance Explained', 'Standard Devivation'])\n    for index,pipeline in enumerate(pipelines):\n        name,pipeline = pipeline\n        kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n        cv_results = cross_val_score(pipeline,X,y,cv=kfold, scoring='explained_variance')\n        results.loc[index]= [name,cv_results.mean()*100, cv_results.std()*100]\n    return results","614f8da8":"X,X_test,y,y_test = train_test_split(raw_data.drop('strength',axis=1), raw_data['strength'], test_size=0.20, random_state=1)\nX_train,X_val,y_train,y_val= train_test_split(X,y, test_size=0.20, random_state=1)\n\ncv_results(X_train,y_train,pipelines)","c03171fa":"X,X_test,y,y_test = train_test_split(comp_raw_data.drop('strength',axis=1), comp_raw_data['strength'], test_size=0.20, random_state=1)\nX_train,X_val,y_train,y_val= train_test_split(X,y, test_size=0.20, random_state=1)\n\ncv_results(X_train,y_train,pipelines)","df155565":"X,X_test,y,y_test = train_test_split(pca_data_2, comp_raw_data['strength'], test_size=0.20, random_state=1)\nX_train,X_val,y_train,y_val= train_test_split(X,y, test_size=0.20, random_state=1)\n\ncv_results(X_train,y_train,pipelines)","41aac1f1":"X,X_test,y,y_test = train_test_split(pca_data_3, comp_raw_data['strength'], test_size=0.20, random_state=1)\nX_train,X_val,y_train,y_val= train_test_split(X,y, test_size=0.20, random_state=1)\n\ncv_results(X_train,y_train,pipelines)","6484b6e7":"X,X_test,y,y_test = train_test_split(pca_data_4, comp_raw_data['strength'], test_size=0.20, random_state=1)\nX_train,X_val,y_train,y_val= train_test_split(X,y, test_size=0.20, random_state=1)\n\ncv_results(X_train,y_train,pipelines)","bc9ff0ab":"X,X_test,y,y_test = train_test_split(comp_raw_data.drop('strength',axis=1), comp_raw_data['strength'], test_size=0.20, random_state=1)\nX_train,X_val,y_train,y_val= train_test_split(X,y, test_size=0.20, random_state=1)","8df4fb4f":"cv_results(X_train,y_train,get_pipeline_with_degree(2))","0b834302":"cv_results(X_train,y_train,get_pipeline_with_degree(3))","ea0980c4":"cv_results(X_train,y_train,get_pipeline_with_degree(4))","0888ce31":"grb= GradientBoostingRegressor()\ngrb.get_params()","b05e323e":"tunning_pipeline= Pipeline([('scaler',StandardScaler()),('gradientboostinregressor', GradientBoostingRegressor(random_state=1))])","bcd3ac6b":"params={'gradientboostinregressor__learning_rate': [0.1,0.2,0.3],\n       'gradientboostinregressor__loss': ['ls', 'lad', 'huber', 'quantile'],\n       'gradientboostinregressor__max_depth':range(1,3) ,\n        'gradientboostinregressor__max_features': [None,2,3],\n        'gradientboostinregressor__max_leaf_nodes': [None,2,3],\n       }","a144c973":"X,X_test,y,y_test = train_test_split(comp_raw_data.drop('strength',axis=1), comp_raw_data['strength'], test_size=0.20, random_state=1)\nX_train,X_val,y_train,y_val= train_test_split(X,y, test_size=0.20, random_state=1)\n\nsearchGrid= GridSearchCV(tunning_pipeline, param_grid=params, cv=10, scoring='explained_variance');\nsearchGrid.fit(X_train,y_train);\n","03069206":"searchGrid.best_params_","435c4163":"print(f'Explained variance by the model on trainning data is {searchGrid.score(X_train,y_train)*100: .2f}%');\nprint(f'Explained variance by the model on validation data is {searchGrid.score(X_val,y_val)*100: .2f}%');","4f397239":"# Using the best params given by the Grid Search\nfinal_pipeline = Pipeline([('scaler',StandardScaler()),\n                           ('gradientboostinregressor', GradientBoostingRegressor(random_state=1, \n                                                                                learning_rate=0.3,\n                                                                                loss='ls',\n                                                                                max_depth=2,\n                                                                                max_features=None,\n                                                                                max_leaf_nodes=None))])\n","71536fbf":"final_pipeline.fit(X_train,y_train);","a668064b":"print(f'Explained variance by the model on trainning data is {final_pipeline.score(X_train,y_train)*100: .2f}%');\nprint(f'Explained variance by the model on validation data is {final_pipeline.score(X_val,y_val)*100: .2f}%');","dd61feed":"print(f'Explained variance by the model on test data is {final_pipeline.score(X_test,y_test)*100: .2f}%');","7d5513b2":"X=X_test\ny=y_test\nkfold=KFold(n_splits=10, shuffle=True, random_state=1)\ncv_results=cross_val_score(final_pipeline,X,y, cv=kfold, scoring='explained_variance')","7739ba35":"print(f'The cross validation result mean is: {cv_results.mean()*100: .2f}% and standard deviation: {cv_results.std()*100: .2f}%');","14e73202":"\nplt.plot(range(1,11),cv_results);\nplt.xlabel('Split Count');\nplt.ylabel('Explained variance');\nplt.title('CV-KFold chart for Gradient Boosting Algo');\nprint(f'The algorithm can explain variance in range of {(cv_results.mean()-(2*cv_results.std()))*100: .2f}% to {(cv_results.mean()+(2*cv_results.std()))*100: .2f}% with 95% confidence.');","c823b406":"|         | cement     | slag       | ash             | water          | superplastic | coarseagg       | fineagg    | age           |\n|---------|------------|------------|-----------------|----------------|--------------|-----------------|------------|---------------|\n| Group-0 | Useful     | Useful     | Not Useful      | Highly Useful  | Useful       | Not Useful      | Not Useful | Highly Useful |\n| Group-1 | Useful     | Not Useful | Useful          | Slightly Useful | Useful       | Not Useful      | Useful     | Not Useful    |\n| Group-2 | Useful     | Not Useful | Not Useful      | Useful         | Useful       | Slightly Useful | Useful     | Not Useful    |\n| Group-3 | Useful     | Useful     | Not Useful      | Not Useful     | Useful       | Not Useful      | Useful     | Highly Useful |\n| Group-4 | Not Useful | Not Useful | Slightly Useful | Not Useful     | Not Useful   | Not Useful      | Not Useful | Highly Useful |","f4361a72":"### Import dataset","469dff0d":"- No trace of Null values.","7c4be186":"## Creating Models","a744b450":"## Data Description:\nThe actual concrete compressive strength (MPa) for a given mixture under a\nspecific age (days) was determined from laboratory. Data is in raw form (not\nscaled). The data has 8 quantitative input variables, and 1 quantitative output\nvariable, and 1030 instances (observations).\n## Domain:\nCement manufacturing\n## Context:\nConcrete is the most important material in civil engineering. The concrete\ncompressive strength is a highly nonlinear function of age and ingredients.\nThese ingredients include cement, blast furnace slag, fly ash, water,\nsuperplasticizer, coarse aggregate, and fine aggregate.\n## Attribute Information:\n- Cement : measured in kg in a m3 mixture\n- Blast : measured in kg in a m3 mixture\n- Fly ash : measured in kg in a m3 mixture\n- Water : measured in kg in a m3 mixture\n- Superplasticizer : measured in kg in a m3 mixture\n- Coarse Aggregate : measured in kg in a m3 mixture\n- Fine Aggregate : measured in kg in a m3 mixture\n- Age : day (1~365)\n- Concrete compressive strength measured in MPa","35c03a30":"#### New features which indicates high correlation with the target variable\n- **cement\/water** 0.559664\n- **cement\/coarseagg** 0.511801\n- **water\/cement** -0.501118\n- **water\/age** -0.515822\n- **fineagg\/age** -0.503051\n- **age\/water** 0.528081","7e98d9f6":"- By seeing the pairplot it is cler that there are different gussians mixed in the dataset.\n- We can find these different gussians in dataset by using clustering mechanism.","13a6d9f4":"### Multivariate Analysis","0e806800":"## Exploratory Data Analysis","24052b5b":"- **Cement**: The graph is skew to right(+ve), doesn't have a normal distribution.\n- **slag**: The graph is skewed to right(+ve), the second gussain in it looks like normal. Few ouliers on the higher end.\n- **ash**: This also has +ve skew. The second guassin still looks normal but +vely skewed.\n- **water**: It slightly +vely skewed, but has multiple peaks inside it. Might indicate multiple gussian are merged\/ multiple clusters can be formed, at least 3. Outliers present at both ends of wiskers.\n- **superplastic**: This also has +ve skewness. The second guassian look like highly +vely skewed as a logn tail on right. Outliers on higher end.\n- **coarseagg**: A bit -vely skewed and had multiple peaks, suggest multiple gussians mixed, at least 3.\n- **fineagg**: A bit -vely skewed and has multiple peaks, suggests multiple gussians mixed, at least 3. Few outliers at higher end.\n- **age**: This is has a long range, this feature mainly tell us about the age of the concerte, has long tails towards right, highly +vely skewed. Outliers at the higher ends, we can't say if we can drop this, because there could be cases in which concerte is of higher age.\n- **strength**: Normally distributed and +vely skewed.","ae8fe47f":"- As suggested in the description, the strength is non-linear function to the features given, we will try different polynomical degrees as no specific pattern is seen in the pair plot.","826d39f8":"- Strenght is higly correlated with cement,  age.\n- Water is higly correlated with superplastic.","794ebcce":"#### Dataset with compostie features (Dataset-2)","a597ac1b":"- We can clearly see that outliers exists. We need to fix this.","f85588bb":"### Correlation Analysis","6f440992":"# Featurization and Model Tuning Project","2e90b671":"## Explorer Gussians present in the data","e9fca012":"### Polynomial Features with PCA data with simple Pipeline\n#### ({comp_data}=> {poly_data} then ({poly_data} + {comp_data})=> {pca_data})","ee1f00fb":"### Univariate Analysis","2313aa08":"### THE END","6c7b6184":"### Dataset-1","8ba2d711":"- From this we can conclude that The random forest and the gradient boosting regressor have the best variance explained approx to 90% with the raw data + compostite features value.","346d3b9d":"- We will consider elbow point at 5.","20b26e28":"- The data is clearly mix of gussians and the optimal no. of clusters are 5.\n- Each clusters contains approx. 200 records.","967d76d3":"## Composite Feature Engineering","c8452ddd":"#### Parameter tunning for the GradientBoostingRegressor","3ea74e7c":"### Raw and Compostite Data With Simple Pipelines","82f927e8":"## Creating Polynomial Features (Dataset-3,4,5)","bb8b38d7":"- For better visualization of box plot we have used z-score normilized data, we also done the outlier cleanup in the raw_data.","3f6408ba":"### Final model (GradientBoostingRegressor)","eb7792ed":"- The expected number of cluster are 3 from the pairplot observations.\n- We will try from range 2-20 to obtain minimum distortion.","612689b6":"- Datasets we have\n   - **raw_data** - Dataset without any composite features\n   - **comp_raw_data** - Dataset with composite features in it.\n   - **pca_data_2** - Dataset with polynomial degree 2 and PCA of 99%\n   - **pca_data_3** - Dataset with polynomial degree 3 and PCA of 99%\n   - **pca_data_4** - Dataset with polynomial degree 4 and PCA of 99%","5af23e32":"### Distribution Plots","ec860122":"### Composite Data Send to Poly-PCA pipeline\n#### no comp_data re-addition to ploy_data after poly_data generation","166b215d":"## Performance Evaluation","48318621":"### After Tunning test the final model on test set.","fae8dc0f":"### Import essentials libaries","e688e996":"- There are 3 columns which contains zero as value, as the context is mixture that we make to produce concerete, we can have slag or ash or superplastic values as zero. No imputation is required for these."}}