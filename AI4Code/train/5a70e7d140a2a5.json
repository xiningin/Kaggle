{"cell_type":{"5ad3624c":"code","58009fe6":"code","9255af03":"code","1f4c2ab6":"code","f915e0d0":"code","a068aef2":"code","22a056ab":"code","d4943eed":"code","7aea9ae5":"code","d942c72d":"code","d314e138":"code","e035fb5c":"code","860a7adb":"code","00a98faa":"code","a1026aa0":"code","ab238e73":"code","b48764c9":"code","cf1906ad":"code","e7b33f54":"code","6bed30cb":"code","e1528d99":"code","819ecbfc":"code","98aa912b":"code","7e29b382":"code","10ec3744":"markdown","2122423f":"markdown","2eac8524":"markdown","031b2707":"markdown","9aaafa38":"markdown","d0a84a88":"markdown","0824630b":"markdown","04586c4d":"markdown","f1675160":"markdown"},"source":{"5ad3624c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","58009fe6":"#import the required libraries\nimport keras\nfrom keras.layers import Embedding\nfrom keras.layers import Dense, Flatten, LSTM\nfrom keras.layers import Input, GlobalMaxPool1D, Dropout\nfrom keras.layers import Activation\nfrom keras.models import Model, Sequential\nfrom keras import optimizers\n\nimport os\nimport pandas as pd","9255af03":"#read the data files\ntrain = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv')","1f4c2ab6":"#lets look at the data\ntrain.head()","f915e0d0":"#let's look at the test data\ntest.head()","a068aef2":"#check for any null values in our dataset\nprint(train.isnull().any())","22a056ab":"#again checking for any null values in the test dataset\nprint(test.isnull().any())","d4943eed":"#divide our training data into features X and label Y\nX_train = train['comment_text'] #will be used to train our model on\nX_test = test['comment_text'] #will be used to predict the output labels to see how well our model has trained\ny_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values","7aea9ae5":"y_train.shape #as expected total 6 columns for 6 predictor classes","d942c72d":"#import the tokenizer class from the keras api\nfrom keras.preprocessing.text import Tokenizer","d314e138":"#let's calculate the vocabulary size as well which will be given as an input to the Embedding layer\ntokens = Tokenizer() #tokenizes our data\ntokens.fit_on_texts(X_train)\nvocab_size = len(tokens.word_index) + 1 #size of the total number of uniques tokens in our dataset\ntokenized_train = tokens.texts_to_sequences(X_train) #converting our tokens into sequence of integers\ntokenized_test = tokens.texts_to_sequences(X_test)","e035fb5c":"print(X_train[0]) #the first text\nprint(100 * '-')\nprint(tokenized_train[0]) #the correspondin first comment in the vectorized form","860a7adb":"#import the pad_sequences class from the keras api\nfrom keras.preprocessing.sequence import pad_sequences","00a98faa":"max_len = 300 #maximum length of the padded sequence that we want (one of the hyperparameter that can be tuned)\npadded_train = pad_sequences(tokenized_train, maxlen = max_len, padding = 'post') #post padding our sequences with zeros\npadded_test = pad_sequences(tokenized_test, maxlen = max_len)","a1026aa0":"padded_train[:10] #as you can observe once our sentence ends the padding starts and continues until we have a vector of max_len","ab238e73":"input_model = Input(shape = (max_len, ))\nx = Embedding(input_dim = vocab_size, output_dim = 120)(input_model)\nx = LSTM(60, return_sequences = True)(x)\nx = Activation('relu')(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.2)(x)\n\n# x = LSTM(100, return_sequences = True)(x)\n# x = Activation('relu')(x)\n# x = GlobalMaxPool1D()(x)\n# x = Dropout(0.2)(x)\n\nx = Dense(150, activation = 'relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(100, activation = 'relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(6, activation = 'softmax')(x)","b48764c9":"model = Model(inputs = input_model, outputs = x)\noptim = optimizers.Adam(lr = 0.01, decay = 0.01 \/ 64) #defining our optimizer\nmodel.compile(loss = 'categorical_crossentropy', optimizer = optim, metrics = ['accuracy']) #compiling the model","cf1906ad":"model.summary()","e7b33f54":"#fit the model onto our dataset\nhistory = model.fit(padded_train, y_train, batch_size = 64, epochs = 5, validation_split = 0.3)","6bed30cb":"import numpy as np\n\nembedding_dim = 50\n#def create_embedding_matrix(filepath, embedding_dim):\nvocab_size = len(tokens.word_index) + 1  # Adding again 1 because of reserved 0 index\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt', encoding = 'utf-8') as f:\n    for line in f:\n        word, *vector = line.split()\n        if word in tokens.word_index:\n            idx = tokens.word_index[word] \n            embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]","e1528d99":"model1 = Sequential()\nmodel1.add(Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], input_length = max_len, trainable = False))\nmodel1.add(LSTM(60, return_sequences = True))\nmodel1.add(Activation('relu'))\nmodel1.add(GlobalMaxPool1D())\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(16, activation = 'relu'))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(6, activation = 'softmax'))\nmodel1.summary()","819ecbfc":"#compile the model\nmodel1.compile(optimizer = optimizers.Adam(lr = 0.01, decay = 0.01 \/ 32), loss = 'categorical_crossentropy', metrics = ['accuracy'])","98aa912b":"#fit the model on the dataset\nhistory1 = model1.fit(padded_train, y_train, epochs = 5, batch_size = 32, validation_split = 0.3)","7e29b382":"list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\ny_pred = model1.predict(padded_test)\n\nsample_submission = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv\")\n\nsample_submission[list_classes] = y_pred\n\nsample_submission.to_csv(\"model_submission.csv\", index = False)","10ec3744":"## PREPROCESSING USING KERAS","2122423f":"\n**1. Now if you observe the vector representation of each comment, the size of each vector is different.**\n\n**2. But our machine learning model expects the size of our input data to be same throughout.**\n\n**3. Hence we will be doing padding.**","2eac8524":"**While building our model using the functional API we have used some new layers in our network**\n\n**1. Starting few layers are same as the previous, the layer called Dropout is been added here. Now Dropout is a regularisation technique which we implement to prevent out model from overfitting to our training data. This technique randomly drops (or switches off) nodes from a particular layer with the given probability. So that the input and output both are blocked for the node which is been dropped.**\n\n**2. Here we have stacked two (LSTM-->Activation-->GlobalMaxPool1D-->Dropout) onto each other followed by few fully connected layers and finally another fully connected layer with 6 neurons as the output layer of our model.**","031b2707":"\n**KERAS is a high level deep learning API which is also very easy when it comes to applying deep learning algorithms. Keras offers a 'Tokenizer' class which carries out tokenisation for us and also convert all the text data into lowercase.**","9aaafa38":"**Let's break down our input features and the output labels that we have to predict**\n\n**1. Our input features which will be our inputs to our DNN architecture will be the column consisting of the comments text.**\n\n**2. As our input features are in the form of a text and not in the form of numbers and integers which is usually prefferd by the ML algorithms we will have to find a way to convert the raw text in the form of integers so that they can be fed to our model. (how to do this explained later)**\n\n**3. Also the input texts are of different lengths. But our model expects the input sequences to have the same length. So we need to find a way to make all the lenght of input sequences equal.**\n\n**4. Coming to our labels that we have to predict, we got in total six predictor classes. Depending on the input text we have to predict to which class our input text can be classified.**\n\n**5. The values of our output labels are 0 and 1, corresponding to whether a particular statement can be classified in that class or not.**\n\n\n**LET'S SEE FURTHER HOW TO SOLVE OUR PROBLEM**","d0a84a88":"## USING PRE-TRAINED WORD EMBEDDINGS","0824630b":"**Pre-trained word embeddings are the ones which are usually trained on a large amount of corpus. Using such pre-trained word embeddings on your model can help achieve the model very good amount of accuracy. Using pre-trained word embeddings helps you to avoid training your own word embedding from scratch. This method can be employed when the available amount of dataset is not too much. Word2Vec(by google) and  Glove(by stanford NLP group) are the two most commonly use pre-trained word embeddings.**","04586c4d":"## LSTM MODEL USING FUNCTIONAL API","f1675160":"**Preprocessing is the most important step when it comes to any of the dataset before it is passed to a Machine Learning model. This fact is no less true when it comes to dealing with text dataset.**\n\n**Infact for text dataset preprocessing is a must and without which we cannot move forward in training our model.**\n\n**The fact that our algorithm only understand numbers and integers and cannot understand strings or text data makes preprocessing even more valuable and essential in Natural Language Processing.**\n\n**When it comes to preprocessing a text data the most common steps are:**\n\n**1. The entire sentence needs to be converted in the form of small tokens or into individual words. By doing this it helps us to assign each word with a unique integer value called as index or ID of that word.**\n\n**2. Tokenization also helps us to find the total number of unique words in our vocabulary which is later to be fed in our model.**\n\n**3. Once the tokenization is done, the next common step is to remove stopwords. Stopwords are the the most commonly occuring words in our vocabulary. Words that won't generally matter even if they are taken out from our vocabulary. For example words like 'the', 'and', 'so', 'if' etc are the stopwords. But we don't always have to remove the stopwords. Stopwords are only removed when their absence won't affect our model prediction much, in cases like classification of documents according to the topics, stopwords can be removed as we only have to focus on words that carry more importance in our documents.**\n\n**4. There are various other ways in which preprocessing of the text data can be done.**"}}