{"cell_type":{"55caf2a1":"code","78161c5e":"code","8970d189":"code","f394c0e4":"code","577ef852":"code","38eae772":"code","0dac5851":"code","132a74a1":"code","7b39ba3b":"code","dbf24a7d":"code","64c031ae":"code","c550f21b":"code","bc2ce858":"code","36cea697":"code","2aa7835d":"code","b77783be":"markdown"},"source":{"55caf2a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","78161c5e":"import gc\nimport json\nimport pickle\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport transformers\nfrom transformers import RobertaModel, RobertaTokenizerFast","8970d189":"with open(\"\/kaggle\/input\/k\/lollol222\/k\/lollol222\/review-context-data\/data.pkl\", \"rb\") as f:\n  DATA = pickle.load(f)\n\n\nwith open(\"\/kaggle\/input\/k\/lollol222\/k\/lollol222\/review-context-data\/train_data.pkl\", \"rb\") as f:\n    TRAIN_DATA = pickle.load(f)\n    TRAIN_UIDS = TRAIN_DATA.keys()\n    \nwith open(\"\/kaggle\/input\/k\/lollol222\/k\/lollol222\/review-context-data\/test_data.pkl\", \"rb\") as f:\n    TEST_DATA = pickle.load(f)\n    TEST_UIDS = TEST_DATA.keys()\n    \n\nfor uid in DATA.keys():\n    DATA[uid][\"queries\"] = np.array(DATA[uid][\"queries\"])\n    DATA[uid][\"answers\"] = np.array(DATA[uid][\"answers\"])\n    DATA[uid][\"neg_cls_uids\"] = list(set(DATA[uid][\"neg_cls_uids\"]) - set([43349]))\n    ","f394c0e4":"len(DATA[14813][\"queries\"]),len(DATA[14813][\"neg_cls_uids\"])","577ef852":"#PARAMS\nMAX_SEQ_LENGTH = 512\nMODEL = \"roberta-base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSAMPLE_CLASS = 10\nSAMPLE_DATA = 4\nEMBEDDING_DIM = 512\nBATCH_SIZE = 1\nACCUMULATION_STEPS=8\nLR=1e-5\nTOTAL_STEPS = 1500\nTOKENIZER = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\nTEST_STEPS = 1000","38eae772":"special_tokens_dict = {'additional_special_tokens': ['[ANIME_NAME]','[MAIN_CHAR]','[MALE_CHAR]','[FEMALE_CHAR]']}\nTOKENIZER.add_special_tokens(special_tokens_dict)","0dac5851":"class SampleClass():\n\n  def __init__(self, uid, sample_neg_class=True,p=SAMPLE_CLASS,k=SAMPLE_DATA,\n      max_seq_length=MAX_SEQ_LENGTH,embedding_dim=EMBEDDING_DIM,device=DEVICE):\n\n    self.uid = uid\n    self.k = k\n    self.p = p\n    self.max_seq_length = max_seq_length\n    self.device = DEVICE\n    self.embedding_dim = embedding_dim\n    self.sample_data_idx = random.sample(range(len(DATA[self.uid][\"answers\"])), self.k)\n    self.tokenized_data = None\n\n    if sample_neg_class:\n      self.neg_classes = [SampleClass(neg_uid, sample_neg_class=False)\n                                for neg_uid in random.sample(DATA[self.uid][\"neg_cls_uids\"], self.p)]\n      self.anchors_idx = random.sample(range(len(DATA[self.uid][\"queries\"])), self.k)\n      self.tokenized_anchors = None\n      self.triplets = []\n\n  @property\n  def data(self):\n    if self.tokenized_data is None:\n      sampled_data = DATA[self.uid][\"answers\"][self.sample_data_idx]\n      tokenized_data = TOKENIZER(list(sampled_data), max_length=self.max_seq_length,\n                                return_tensors=\"pt\",padding=transformers.file_utils.PaddingStrategy(\"max_length\"),truncation=True)[\"input_ids\"]\n      self.tokenized_data = tokenized_data.long().to(self.device)\n    return self.tokenized_data\n\n  @property\n  def anchors(self):\n    if self.tokenized_anchors is None:\n      sampled_anchors = DATA[self.uid][\"queries\"][self.anchors_idx]\n      tokenized_anchors = TOKENIZER(list(sampled_anchors), max_length=self.max_seq_length,\n                                return_tensors=\"pt\",padding=transformers.file_utils.PaddingStrategy(\"max_length\"),truncation=True)[\"input_ids\"]\n      self.tokenized_anchors = tokenized_anchors.long().to(self.device)\n    return self.tokenized_anchors\n\n  def get_data(self,idx):\n    return self.tokenized_data[idx]\n\n  def get_anchors(self,idx):\n    return self.tokenized_anchors[idx]\n\n  def pairwise_distance(self, x, is_squared=False):\n    m1,m2 = x    \n    diff_mat = torch.abs(m1.unsqueeze(1) - m2)\n    scores_mat = torch.sum(diff_mat,dim=-1)\n    return scores_mat\n\n\n  def hard_triplets(self,model):\n    anchors = self.anchors\n    neg_data = torch.cat([neg_class.data for neg_class in self.neg_classes], 0)\n    pos_data = self.data\n\n    mini_batch_data = torch.cat((anchors, pos_data, neg_data),0)\n\n    with torch.no_grad():\n      embeddings = model(calc_triplets=True,x = mini_batch_data)\n\n    anchors_embeddings = embeddings[:self.k]\n    pos_embeddings = embeddings[self.k:self.k*2]\n    neg_embeddings = embeddings[self.k*2:]\n\n    pos_distances = self.pairwise_distance((anchors_embeddings,pos_embeddings))\n    hard_positives = torch.max(pos_distances,1)\n\n    anchors = torch.cat((anchors_embeddings, torch.zeros(\n                              neg_embeddings.shape[0]-anchors.shape[0], self.embedding_dim).to(self.device)))\n\n    neg_distances  = self.pairwise_distance((anchors_embeddings, neg_embeddings))\n    neg_distances = neg_distances[:self.k]\n    hard_negatives = torch.min(neg_distances, 1)\n\n    for i in range(self.k):\n      anchor = self.get_anchors(i)\n      positive = self.get_data(hard_positives[1][i])\n      negative = self.neg_classes[hard_negatives[1][i]\/\/self.k].get_data(hard_negatives[1][i] % self.k)\n      self.triplets.append((anchor, positive, negative))\n    return self.triplets\n\n","132a74a1":"class DanEncoder(nn.Module):\n  def __init__(self, input_dim:int, embedding_dim:int, n_hidden_layers:int, n_hidden_units:[int], dropout_prob:int):\n    super(DanEncoder, self).__init__()\n\n    assert n_hidden_layers != 0\n    assert len(n_hidden_units) + 1 == n_hidden_layers\n\n    encoder_layers = []\n    for i in range(n_hidden_layers):\n      if i == n_hidden_layers - 1:\n        out_dim = embedding_dim\n        encoder_layers.extend(\n          [\n            nn.Linear(input_dim, out_dim),\n          ])\n        continue\n      else:\n        out_dim = n_hidden_units[i]\n\n      encoder_layers.extend(\n        [\n          nn.Linear(input_dim, out_dim),\n          nn.Tanh(),\n        ]\n      )\n      input_dim = out_dim\n    self.encoder = nn.Sequential(*encoder_layers)\n\n  def forward(self, x_array):\n      return self.encoder(x_array)\n\n\n\n#MODEL\nclass Model(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.roberta = RobertaModel.from_pretrained(\"\/kaggle\/input\/review-emddings-pt2\/roberta_base_anime_finetuned.h5\", output_hidden_states=True)\n    self.roberta.resize_token_embeddings(len(TOKENIZER))\n    self.hid_mix = 5\n    self.feats = self.roberta.pooler.dense.out_features\n    self.embedding_layers = DanEncoder(self.feats, EMBEDDING_DIM, 2, [768], 0.1)\n    self.tanh = nn.Tanh()\n\n  def forward(self, calc_triplets=False, **kwargs):\n    if calc_triplets:\n        x = kwargs[\"x\"]\n        return self.forward_once(x)\n    x1 = kwargs[\"x1\"]\n    x2 = kwargs[\"x2\"]\n    x3 = kwargs[\"x3\"]\n    return self.forward_once(x1), self.forward_once(x2), self.forward_once(x3)\n\n  def forward_once(self, x):\n    outputs = self.roberta(x)\n    hidden_states = outputs[2]\n    hmix = []\n    for i in range(1, self.hid_mix+1):\n      hmix.append(hidden_states[-i][:,0].reshape((-1,1,self.feats)))\n\n    hmix_tensor = torch.cat(hmix,1)\n    pool_tensor = torch.mean(hmix_tensor,1)\n    pool_tensor = self.tanh(pool_tensor)\n    embeddings = self.embedding_layers(pool_tensor)\n    return embeddings","7b39ba3b":"def sample_data(model, data_size=BATCH_SIZE, is_test=False):\n  if not is_test:\n    \n    sampled_uids = random.sample(TRAIN_UIDS, data_size)\n  else:\n    sampled_uids = random.sample(TEST_UIDS, data_size)\n  data = []\n  for uid in sampled_uids:\n    x = SampleClass(uid)\n    data.extend(x.hard_triplets(model))\n  random.shuffle(data)\n\n  anchors = torch.cat([i[0].unsqueeze(0) for i in data],0).to(DEVICE)\n  positives = torch.cat([i[1].unsqueeze(0) for i in data],0).to(DEVICE)\n  negatives = torch.cat([i[2].unsqueeze(0) for i in data],0).to(DEVICE)\n#   print(anchors.shape,positives.shape,negatives.shape)\n  return (anchors, positives, negatives)","dbf24a7d":"def criterion(a,p,n):\n  t_loss = F.triplet_margin_loss(a,p,n,margin=2,p=1)\n  return t_loss","64c031ae":"model = Model().to(DEVICE)\nmodel.load_state_dict(torch.load(\"\/kaggle\/input\/k\/lollol222\/k\/iamparadox\/yuno-models\/embedding_model_weights.h5\"))\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\noptim = torch.optim.AdamW(optimizer_grouped_parameters, lr=LR)","c550f21b":"def train(model, criterion, optim, scheduler=None, device=DEVICE):\n  model.train()\n  avg_loss = []\n  acc_loss = []\n  i = 1\n  step = 0\n  tbar = tqdm(total=TOTAL_STEPS)\n\n  while True:\n    if step == TOTAL_STEPS:\n      tbar.close()\n      break\n    data = sample_data(model)\n    anchors = data[0]\n    positives = data[1]\n    negatives = data[2]\n\n    ebd1,ebd2,ebd3 = model(x1=anchors,x2=positives,x3=negatives)\n    loss = criterion(ebd1,ebd2,ebd3)\n    tbar.set_description(f\"AVG_LOSS: {np.average(avg_loss):.5f}, LOSS:{loss.item():.5f}, STEP: {step}\")\n    loss = loss \/ ACCUMULATION_STEPS\n    loss.backward()\n    acc_loss.append(loss.item())\n    i += 1\n    if i % ACCUMULATION_STEPS == 0:\n      step += 1\n      optim.step()\n      if scheduler is not None:\n        scheduler.step()\n      model.zero_grad()\n      avg_loss.append(sum(acc_loss))\n      tbar.update(1)\n      acc_loss = []\n  torch.save(model.state_dict(), \"\/kaggle\/working\/embedding_model_weights.h5\")","bc2ce858":"train(model, criterion, optim)","36cea697":"def test_model(model, criterion):\n    model.eval()\n    avg_loss = []\n    step = 0\n    tbar = tqdm(total=TEST_STEPS)\n    while True:\n       if step == TEST_STEPS: \n           tbar.close()\n           break\n       data = sample_data(model, is_test=True)\n       anchors = data[0]\n       positives = data[1]\n       negatives = data[2]\n       with torch.no_grad():\n           ebd1,ebd2,ebd3 = model(x1=anchors,x2=positives,x3=negatives)\n           loss = criterion(ebd1,ebd2,ebd3)\n           avg_loss.append(loss.item())\n           tbar.set_description(f\"AVG_LOSS: {np.average(avg_loss):.5f}, LOSS:{loss.item():.5f}, STEP: {step}\")\n       tbar.update(1) \n       step += 1     ","2aa7835d":"print(\"ENTERING EVAL MODE\")\ntest_model(model, criterion)","b77783be":"# Roberta Base v3 with l1 Norm(m=2)"}}