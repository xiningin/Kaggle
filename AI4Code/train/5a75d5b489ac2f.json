{"cell_type":{"379b7fa9":"code","1b21a00e":"code","ffdebd5c":"code","d070d849":"code","98165e65":"code","750fa8d4":"code","df27cfbc":"code","ce681302":"code","ba8d6391":"code","ed01a673":"code","9ef772b6":"code","8cc4908b":"markdown","f0d49191":"markdown","f979da55":"markdown","7ccafec1":"markdown","53d1b4b9":"markdown","f5f424da":"markdown","6754b1b1":"markdown","75750e98":"markdown","c281ff27":"markdown","ce350500":"markdown","54015435":"markdown","02801363":"markdown","ed49fe32":"markdown","ac2139c3":"markdown","ef55170b":"markdown","2006e293":"markdown","3d5d094b":"markdown","e8fc0fe4":"markdown","323aba8d":"markdown","37e67538":"markdown","39de6b1a":"markdown","6ec3f325":"markdown","c03909e5":"markdown"},"source":{"379b7fa9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b21a00e":"import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA","ffdebd5c":"class AttentionLayer(nn.Module):\n    \n    def __init__(self, in_dim, out_dim, k=None):\n        super(AttentionLayer, self).__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.k = k\n        \n        self.w = nn.Parameter(torch.Tensor(out_dim))\n        self.w.data.uniform_(0, 1)\n        \n        self.graph = None\n        \n    def forward(self, input):\n        N = input.shape[0]\n        \n        feature_weight = nn.functional.softmax(self.w, dim=0) \n        out = input * feature_weight #calculating feature attention\n                \n        # calculating the matrix of distances between nodes    \n        dist = torch.norm(out.unsqueeze(1) - out.unsqueeze(0), p=2, dim=-1)\n        _, graph = dist.sort()\n        self.graph = graph\n        \n        y = torch.zeros(N, self.in_dim)\n        for i in range(N):\n            # for each node we find k nearest neighbours\n            neighbor_idxs = graph[i][:self.k]\n            # then we calculate an output using inverse euclidean distance between weighted vectors as an attentin kernel\n            a = -torch.sum((out[i] - out[neighbor_idxs])**2, dim=1)\n            a = nn.functional.softmax(a, dim=0)\n            y[i] = torch.sum(out[neighbor_idxs] * a.unsqueeze(1), dim=0)\n            \n        return y","d070d849":"class AffinityNet(nn.Module):\n    \n    def __init__(self, in_dim, out_dim, num_classes, hidden_dim = 100, k = 10):\n        super(AffinityNet, self).__init__()\n        # AffinityNet has one AttentionLayer and 2 fully conected layers on the top\n        self.attention_layer = AttentionLayer(in_dim, out_dim, k)\n        self.fc1 = nn.Linear(out_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n        \n    def forward(self, input):\n        out = self.attention_layer(input)\n        out = self.fc1(out)\n        out = nn.functional.relu(out)\n        out = self.fc2(out)\n        return out","98165e65":"means = np.array([[0,0], [1,0], [0,1], [1,1]]) * 5\nnum_clusters = 4\nsignal_dim = 1\nclu_size = 200\nfigsize = 10\nnoise_channels = 40\n\nassert num_clusters == len(means)\nmeans = [m*signal_dim for m in means]\nsigmas = 1.0*np.ones(len(means))\nx = []\ny = []\nfor i, (mean, sigma) in enumerate(zip(means, sigmas)):\n    x.append(\n        np.concatenate([\n            np.random.multivariate_normal(mean, sigma*np.eye(len(mean)), size=clu_size),\n            np.random.multivariate_normal([2.5] * noise_channels, 10 * np.eye(noise_channels), size=clu_size)\n        ], axis = 1)\n    )\n    y.append(i*np.ones(clu_size, dtype=int))\nx = np.concatenate(x, axis=0)\ny = np.concatenate(y, axis=0)\n\ntitle = 'input'\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(x[:,0], x[:,1], c=y)\n\nplt.title(title)\nplt.show()","750fa8d4":"pca = PCA(n_components = 2)\nx_pca = pca.fit_transform(x)\n\ntitle = 'PCA transformed'\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(x_pca[:,0], x_pca[:,1], c=y)\n\nplt.title(title)\nplt.show()","df27cfbc":"model = AffinityNet(noise_channels + 2, noise_channels + 2, num_clusters)","ce681302":"class Dataset(torch.utils.data.Dataset):\n    \n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n    \ndataset = Dataset(x, y)\n\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len(dataset) \/ 2), int(len(dataset) \/ 2)])\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset,\n                                        batch_size = 256,\n                                        shuffle = True)   \nval_dataloader = torch.utils.data.DataLoader(val_dataset,\n                                        batch_size = int(len(dataset) \/ 2))   ","ba8d6391":"epochs = 150\nlr = 0.01\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\ntrain_acc_log = []\nval_acc_log = []\n\nfor epoch in range(epochs):\n    for idx, (data, label) in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        y_pred = model(data)\n        loss = criterion(y_pred, label)\n        loss.backward()\n        optimizer.step()\n\n        y_hat = torch.argmax(y_pred, dim = 1)\n        correct = (y_hat == label).sum()\n        \n        for val_data, val_label in val_dataloader:\n            val_pred = model(val_data)\n            val_y_hat = torch.argmax(val_pred, dim = 1)\n            val_correct = (val_y_hat == val_label).sum()\n        train_acc = correct \/ label.shape[0]    \n        val_acc = val_correct \/ val_label.shape[0]\n        \n    train_acc_log.append(train_acc)\n    val_acc_log.append(val_acc)\n    if epoch % 10 == 0:    \n        print(f\"Epoch {epoch} Loss = {loss.data:.03f}, acc = {train_acc:.02f}, val acc = {val_acc:.02f}\")","ed01a673":"title = 'Training and validation accuracy'\nplt.figure(figsize=(figsize, figsize))\n\nplt.plot(train_acc_log, '--', label = 'Train acc', color = 'red')\nplt.plot(val_acc_log, label = 'Validation acc', color = 'green')\nplt.xlabel('Epoch', fontsize=18)\nplt.ylabel('Accuracy', fontsize=16)\nplt.legend()\nplt.title(title)\nplt.show()","9ef772b6":"title = 'Feature weights learned by AffinityNet'\nplt.figure(figsize=(figsize, figsize))\n\nplt.scatter(x = range(42), y = nn.functional.softmax(model.attention_layer.w, dim = 0).detach().numpy())\nplt.title(title)\nplt.show()","8cc4908b":"In a such model we have a sentence as input, for example 'I am a student'. Each word is encoded by some vector - called word embedding. These vectors have an important property - vectors of words that are close in meaning are themselves located close in the embeddings vector space.<br>\nNext, for every word embedding $x_i$ we calculate 3 vectors:\n$$k_i = W_k * x_i$$\n$$q_i = W_q * x_i$$\n$$v_i = W_v * x_i$$\nwhere $W_k$, $W_q$ and $W_v$ are the model parameters.","f0d49191":"![attention.PNG](attachment:581cc143-c43b-49b4-beaa-3168ebb525c8.PNG)","f979da55":"Building a model","7ccafec1":"**If my kernel was helpful, please consider upvoting**","53d1b4b9":"<h1>AffinityNet<\/h1>","f5f424da":"![AffinityNet.PNG](attachment:7eeaa14b-7fd7-4f6d-9a29-62f09654a9bf.PNG)","6754b1b1":"Stages 1-4 then we repeat for every input word $x_1,...,x_n$.<\/br>\nThus, the main advantage of this model is that it takes into account the interactions between words in a sentence, which allows it, for example, to catch semantic connections between an adjective and a noun to which it refers and to match the words by their forms at the output in the case of translation of the text into another language","75750e98":"Preparing a datasets","c281ff27":"As you can see, AffinyNet does a good job with 42-dimensional data, in which only 2 features have useful information, weeding out the remaining noisy ones<\/br>\n**If you liked this post - please feel free to upvote**<\/br>\nIn the next one I'll show how to use AffinityNet for images for the task of the weakly-supervised semantic segmentation<\/br>\np.s. And sorry for my english ;)","ce350500":"Now, lets generete some data. Each node has 2 channels, which has useful information, according to which we can easily split all the data into 4 clusters, and 40 noise chanels, which have no useful information and are just generated randomly.","54015435":"To calculate output for the $i$-th word we need:\n1. Calculate dot products between vectors $q_i$ and $k_j$ where $j = 1...n$, where $n$ - count of words in the sentence. Thus we get weights $a_1,..., a_n$ which represent interactions between the $i$-th word and every other word in a sentense.\n2. Divide $a_1,..., a_n$ by some constant, usually the square root of the dimension of the vector\n3. Calculate $w_1,..., w_n$ by just taking softmax function from $a_1,..., a_n$\n4. Calculate the weighted sum $y_i = \\displaystyle\\sum_{j=1}^{n} v_j * w_j$","02801363":"Here you can see all point projected into 2-dimensional space using PCA","ed49fe32":"In this kernel we will be learning about the idea AffinityNet, which was introduced in a paper [AffinityNet: Semi-supervised Few-shot Learning for\nDisease Type Prediction\n](https:\/\/arxiv.org\/pdf\/1805.08905.pdf)\nThe basic idea of this model is to apply an attention mechanism to learning node representations, which allows for every node get its representation based on its K-nearest neighbors, assuming that similar nodes will have similar learned representations.","ac2139c3":"<h1>Introduction<\/h1>","ef55170b":"<h2>Attention mechanism<\/h2>","2006e293":"<h1>Experiment<\/h1>","3d5d094b":"While, for example, for images, the input features - pixels, has a naturally ordered structure and we can learn a good feature extractor using convolutional and pooling layers, in other applications features are usually unordered and we have some feature selector. So, in a typical AffinityNet model the input layer is followed by a feature attention layer, which plays the role of such feature selector.","e8fc0fe4":"Next, AffinityNet has one or more **kNN attention pooling layers**. For each input object it takes $h_i$ - a vector got from the feature attention layer, and calculates distances to others nodes:\n$$d_{ij} = ||h_i - h_j||^2$$\nBased on these distances, kNN layer finds $i$-th node's k nearest neighbours and calculates output as\n$$y_i = f(\\displaystyle\\sum_{j\\in\\mathcal{N}(i)} a(h_i, h_j) \\cdot h_j)$$\nwhere $\\mathcal{N}(i)$ is k nearest neighbours, $f(x)$ is a nonlinear transformation, for example $f(x) = ReLU(W\\cdot x + b)$ and $a(h_i, h_j)$ is an **attention kernel** - some similarity metric, such as\n* Cosine similarity $a_{ij}=\\frac{h_i \\cdot h_j}{||h_i|| \\cdot ||h_j||}$\n* Inner product $a_{ij} = h_i \\cdot h_j$\n* Inverse euclidean distance between weighted vectors $a_{ij} = - ||w \\odot h_i - w \\odot h_j||$","323aba8d":"**Feature attention** layer consists of a vector $w$ which have the same size as input vectors. It transforms the input vectors by just element-wise multiplication:\n$$f(x_i) = w * x_i$$\n","37e67538":"Here you can see learned weights in the Feature attention layer. Notice, that the weights of the first 2 features, which really have useful information, are much more bigger then the weigth of the noise features.","39de6b1a":"Learning the model for 150 epochs","6ec3f325":"Let's briefly recall the essence of the attention mechanism.<br>\nIt came from the Natural Language Processing and allows to consider different relationships between words in a sentence.","c03909e5":"Lets reproduce an experiment from tha paper<\/br>\nBelow you can see the code of AttentionLayer, which combines both the Feature attention layer and the kNN attention layer, and AffinityNet."}}