{"cell_type":{"8deed106":"code","89464a58":"code","2fad5369":"code","7f041072":"code","8cbc6eaf":"code","f3eac484":"code","2a60f13f":"code","96b3aaa3":"code","7cee755d":"code","e03aa84a":"code","888bc335":"code","7973938e":"code","b3b0341b":"code","47b08418":"code","c5988cee":"code","6fada7f1":"code","3d5b88ee":"code","f7c290eb":"code","ac6f72a1":"code","189a3194":"code","2cb6afec":"code","a5ccc8fd":"code","3a89525a":"code","b9d657e6":"code","63f98f21":"code","60c18143":"code","921569a8":"code","7aae959c":"code","74881113":"code","850b5989":"code","7bc84a62":"code","c211b537":"code","cbe7aa17":"code","ee3495d6":"code","c26c8e0d":"code","2d8884af":"code","673ee9ae":"code","1566b35f":"code","c33b8ed2":"code","3c7721b6":"code","1e8e7b4e":"code","8357d434":"code","a67d783a":"code","db7cb200":"code","47b5349d":"code","725a42b9":"code","93fbacd6":"code","77ab9c12":"code","2e680104":"code","fec122a0":"code","6dd36438":"code","a3e84dbe":"markdown","12c6d519":"markdown","0b3ee8b6":"markdown","e2509df6":"markdown","6b81b4a4":"markdown","021868fd":"markdown","8ce3c2b3":"markdown","c9d5d2cc":"markdown","e157b617":"markdown","0c72fdb8":"markdown","21949acd":"markdown","cde744aa":"markdown","f1787e8c":"markdown","978aa488":"markdown","95a66ec9":"markdown","d5e53e4e":"markdown","1cecebc3":"markdown","46ac7a78":"markdown","519dd5cb":"markdown"},"source":{"8deed106":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","89464a58":"pd.read_csv?","2fad5369":"!ls ..\/input\/male-daan-schnell-mal-klassifizieren","7f041072":"!ls ..\/input\/male-daan-schnell-mal-klassifizieren\/train.csv","8cbc6eaf":"!head ..\/input\/male-daan-schnell-mal-klassifizieren\/train.csv","f3eac484":"!dir ..\/input\/male-daan-schnell-mal-klassifizieren","2a60f13f":"fn = '..\/input\/male-daan-schnell-mal-klassifizieren\/train.csv'\nimport pandas as pd\ndf = pd.read_csv(fn)\ndf.head(5)","96b3aaa3":"df = pd.read_csv(fn,index_col='Id')\ndf.head()","7cee755d":"df.y.values.dtype","e03aa84a":"df.values","888bc335":"data = df.values\ndata","7973938e":"data.shape,data.dtype","b3b0341b":"nTrain=5000\nXtrain = data[:nTrain,:-1] \nytrain = data[:nTrain,-1] \nXtest = data[nTrain:,:-1] \nytest = data[nTrain:,-1] \nXtrain.shape,ytrain.shape,Xtest.shape,ytest.shape","47b08418":"ytrain","c5988cee":"ytrain = ytrain.astype('int')\nytest = ytest.astype('int')\nytrain, ytrain.dtype","6fada7f1":"Xtrain","3d5b88ee":"k=7","f7c290eb":"#np.random.randint? #Tipp: entkommentieren Sie diese Zeile und lernen Sie, was die Funktion randint ist.\nzuf\u00e4lliger_Index = np.random.randint(low=0,high=len(ytest))\nzuf\u00e4lliger_Index","ac6f72a1":"Testzeile = Xtest[zuf\u00e4lliger_Index,:]\nTestlabel = ytest[zuf\u00e4lliger_Index]\nTestzeile,Testlabel","189a3194":"Xtrain.shape, Testzeile.shape","2cb6afec":"((Xtrain - Testzeile)**2)","a5ccc8fd":"np.sqrt?","3a89525a":"distanz = np.sqrt(((Xtrain - Testzeile)**2).sum(axis=1))","b9d657e6":"distanz","63f98f21":"distanz","60c18143":"np.sort(distanz)[:k]","921569a8":"np.argsort(distanz)[:k]","7aae959c":"np.argsort([1,7,4,9])","74881113":"ytrain","850b5989":"distanz[np.argsort(distanz)[:k]]\nnp.min(distanz)","7bc84a62":"ytrain[np.argsort(distanz)[:k]]","c211b537":"[1,0,1,0,1,1,0]","cbe7aa17":"np.around(np.mean(np.array([1,0,1,0,1,1,0])))","ee3495d6":"np.median([1,0,1,0,1,1,0])","c26c8e0d":"distanz[:k]","2d8884af":"a= np.array([1,4,8,2])\nsorted_indices = np.argsort(a)\nsorted_indices, a[sorted_indices]\n","673ee9ae":"sorted_indices = np.argsort(distanz)\ndistanz[sorted_indices]","1566b35f":"distanz[sorted_indices[:k]], sorted_indices[:k], ytrain[sorted_indices[:k]]","c33b8ed2":"Auftretende_Trainingslabels = ytrain[sorted_indices[:k]]","3c7721b6":"if np.mean(Auftretende_Trainingslabels)>=0.5:\n    yhat = 1\nelse:\n    yhat = 0\nyhat","1e8e7b4e":"print('H\u00e4ufigstes Label in [1,0,1,0,0]:',np.argmax(np.bincount([1,0,1,0,0])))\nprint('H\u00e4ufigstes Label in [1,0,1,0,1]:',np.argmax(np.bincount([1,0,1,0,1])))","8357d434":"np.bincount([0,3,3,2,1,1])","a67d783a":"np.bincount([0,3,3,3,2,2,2,2,2,2,2,1,1,4,4,4,4,5,6])","db7cb200":"np.argmax(np.bincount([0,3,3,3,2,2,2,2,2,2,2,1,1,4,4,4,4,5,6]))","47b5349d":"def kNN_Vorhersage(Xtrain,Testzeile,k):\n    distanz = (((Xtrain - Testzeile)**2).sum(axis=1))**0.5\n    sorted_indices = np.argsort(distanz)\n    Auftretende_Trainingslabels = ytrain[sorted_indices[:k]]\n    return np.argmax(np.bincount(Auftretende_Trainingslabels))\n\nzuf\u00e4lliger_Index = np.random.randint(low=0,high=len(ytest))\nTestzeile = Xtest[zuf\u00e4lliger_Index,:]\n\nyhat = kNN_Vorhersage(Xtrain,Testzeile,k)\n\nprint(f'Vorhersage f\u00fcr {Testzeile}:{yhat}')","725a42b9":"#kNN-Klassifikator in Scikit-Learn\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(max_depth=5)\n#clf = KNeighborsClassifier(n_neighbors=k)\n#KNeighborsClassifier? #Bitte entkommentieren!","93fbacd6":"#Laden der Daten:\nimport pandas as pd\ndf_train = pd.read_csv('..\/input\/male-daan-schnell-mal-klassifizieren\/train.csv',index_col='Id')\ndf_test = pd.read_csv('..\/input\/male-daan-schnell-mal-klassifizieren\/test.csv',index_col='Id')\ndf_train.head(3)","77ab9c12":"# Bringe Daten in X-y-Form:\nXtrain = df_train.values[:,:-1]\nytrain = df_train.values[:,-1]\nXtest = df_test.values","2e680104":"#Trainiere den k-NN:\nclf.fit(Xtrain,ytrain)","fec122a0":"yhat = clf.predict(Xtest)","6dd36438":"#Erstelle eine .csv-Datei \"Submission.csv\"\nser = pd.Series(yhat,name='y').astype('int')\nser.index.name='Id'\nser.to_csv('Submission.csv',header=True)\n!head Submission.csv","a3e84dbe":"F\u00fcr `Testzeile` ben\u00f6tigen wir die `k` kleinsten Abst\u00e4nde zu Trainingsdatenpunkten (in `Xtrain`). Diese zu berechnen ist eine der St\u00e4rken von Numpy: Mit einer Zeile berechnen wir alle Abst\u00e4nde der Testzeile zu *allen* Trainingszeilen. Es lohnt sich, \u00fcber *broadcasting* nachzulesen, aber grunds\u00e4tzlich: Alle Numpy-Operationen werden Elementweise durchgef\u00fchrt. Die Funktion np.sum und viele weitere haben ein Argument axis=..., mit welchem sich angeben l\u00e4sst, wor\u00fcber summiert wird: None (\u00fcber alles), 0 (zeilenweise), 1 (spaltenweise), 2 etc.","12c6d519":"Welches dieser Label ist das h\u00e4ufigste? Da nur die Werte 0 und 1 vorkommen, k\u00f6nnen wir dazu den Mittelwert bestimmen: Ist er gr\u00f6sser als 0.5, dann kommt 1 h\u00e4ufiger vor als 0.","0b3ee8b6":"Dieses Notebook laden wir nun als Eingabe f\u00fcr die Kaggle-Competition hoch! Dazu wird das Notebook gespeichert. Unter Output findet man dann den Knopf \"Submit to Competition\"","e2509df6":"Das Training ist trivial! Wir m\u00fcssen nur die Trainingsdaten abspeichern (schon geschehen). \nF\u00fcr die Vorhersage: Wir beginnen mit der Vorhersage f\u00fcr eine Testdatenzeile, welche wir zuf\u00e4llig ausw\u00e4hlen:","6b81b4a4":"nun brauchen wir \"nur\" noch die Labels der $k$ kleinsten Distanzen. Dazu\n- berechnen wir die Indices der $k$ kleinsten Distanzen mittels np.argsort\n- suchen uns an Hand dieser Indices in `ytrain` die entsprechenden Labels der n\u00e4chsten Trainingsdatenpunkte\n- Bestimmen darunter das h\u00e4ufigere Label. Dieses ist dann unsere k-NN-Vorhersage f\u00fcr die `Testzeile`.\n    ","021868fd":"Damit lassen sich die $k$ kleinsten Distanzen, deren Indices und die zugeh\u00f6rigen Testlabels ausgeben:","8ce3c2b3":"Besser w\u00e4re es, wenn `ytrain` eine Ganzzahl w\u00e4re: ","c9d5d2cc":"![](http:\/\/)Um eine .csv-Datei zu laden, benutzen wir Pandas, eine Bibliothek f\u00fcr Data Science. Wir laden ein *DataFrame* (eine Tabelle mit benannten Spalten), werden anschliessend aber nur den Numpy-Array verwenden, welcher unter dem Attribut df.values abgespeichert ist. ","e157b617":"Etwas eleganter w\u00e4re die folgende Zeile. Bincount gibt einen Array zur\u00fcck, der an der 0. Stelle die Anzahl Nullen, an der 1. Stelle die Anzahl Einsen (etc.) wiedergibt. Also aus [1,0,1,0,0] wird [3,2]. ArgMax gibt anschliessend den Index der Stelle raus, welche das (erste) Maximum enth\u00e4lt).","0c72fdb8":"Bauen wir einen k-NN Klassifikator f\u00fcr folgendes $k$.","21949acd":"Numpy enth\u00e4lt eine n-dimensionale Matrix (hier tats\u00e4chlich eine Matrix, n=2), wobei alle Matrixelemente vom selben Typ sind (hier 'float64', also Gleitkommazahl mit 64 Bit, in C w\u00e4re dies der Typ 'double'):","cde744aa":"benutzen wir eine etwas passendere Notation: Unsere Daten sollten wir eigentlich in Trainings- und Testdaten aufteilen. Hier machen wir's uns (methodisch ein grosser Fehler!) der Einfachheit halber- einfach: Die ersten *nTrain* Zeilen sollen Trainingsdaten sein, der Rest Testdaten.\nAber: die letzte Spalte enth\u00e4lt das Klassifikationslabel- diese gliedern wir in ein Symbol $y$ aus: $f(X)\\sim y$","f1787e8c":"F\u00fcr die Differenz wird die Testzeile so oft repliziert, damit die entstehende Shape mit jener von Xtrain kompatibel ist f\u00fcr elementweises Subtrahieren. Auch das Quadrieren (\"\\*\\*2\") wird Elementweise ausgef\u00fchrt. Hier wird also \n$$\\sqrt{(xt_1-x_1)^2+(xt_2-x_2)^2} \\qquad\\forall\\, xt \\mathrm{~in~} Xtest$$\n(mit $xt$ einer Zeile aus dem Trainingsdatensatz, und x der `Testzeile`)","978aa488":"Um zu verstehen, wie argsort funktioniert:","95a66ec9":"## Bestimmen darunter das h\u00e4ufigere Label. Dieses ist dann unsere k-NN-Vorhersage f\u00fcr die `Testzeile`.","d5e53e4e":"## 1. berechne die Indices der $k$ kleinsten Distanzen mittels np.argsort","1cecebc3":"## suchen an Hand dieser Indices in `ytrain` die entsprechenden Labels der n\u00e4chsten Trainingsdatenpunkte","46ac7a78":"Zusammenfassend:","519dd5cb":"# k-NN mit Scikit-Learn\nNat\u00fcrlich geht das alles einfacher mit Scikit-Learn! Doku zum KNN [hier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)"}}