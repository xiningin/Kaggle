{"cell_type":{"2df7bbc9":"code","eb7b9f1f":"code","197b720d":"code","9f8ac7db":"code","a2bcbff3":"code","7b624bba":"code","3bd1e7ae":"code","291f6726":"code","d262fad9":"code","18db489f":"code","6d26de38":"code","bd8dc7e6":"code","d5d08ee1":"code","178992c9":"code","3d77cfa6":"code","81ed6d9a":"code","5d8665e2":"code","948bd1e8":"markdown","ccaca7d0":"markdown","548845e0":"markdown","f7e9f839":"markdown","c12621b1":"markdown","7bcd3aa1":"markdown"},"source":{"2df7bbc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb7b9f1f":"!pip install category_encoders","197b720d":"!pip install xgboost","9f8ac7db":"import category_encoders as ce\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBRegressor as xgb","a2bcbff3":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nprint(df_train.shape)\ndf_train.info()","7b624bba":"# Used to initialize the Target Encoder for transforming categorical features into numerical features\n# One of the steps of the 'pre-process function'\n\n# Declaring a Global Variable\nencoder = None\n\ndef init_encoder():\n    cols = ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n       'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'BsmtExposure', 'BsmtFinType1', \n       'BsmtFinType2', 'Heating', 'CentralAir', 'Electrical', 'Functional', 'GarageType', 'GarageFinish',\n       'PavedDrive', 'SaleType', 'SaleCondition']\n    global encoder\n    encoder = ce.target_encoder.TargetEncoder(cols=cols)","3bd1e7ae":"# Declaring the global variables in order to plot the Correlation Matrix\ndf_res_6th = None\ndf_sp_6th = None\n\n# This is the pre-processing function, which contains all the pre-processing steps\ndef pre_process(df, is_train):\n    # 1st Step\n    ids = df[\"Id\"]\n    if is_train:\n        y = df[\"SalePrice\"]\n        # Storing the SalePrice for plotting the correlation matrix\n        global df_sp_6th\n        df_sp_6th = y\n        y = np.log(y)\n        df.drop([\"Id\", \"SalePrice\"], axis=1, inplace=True)\n    else:\n        df.drop([\"Id\"], axis=1, inplace=True) \n    \n    # 2nd Step\n    df.drop(columns=['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)\n    \n    # Pre-requisites for 3rd and 4th steps\n    dtypes = df.dtypes\n    cols = df.columns\n    med_cols = []\n    fre_cols = []\n    for i in range(len(cols)):\n        if(dtypes[i] == 'int64' or dtypes[i] == 'float64'):\n            med_cols.append(cols[i])\n        else:\n            fre_cols.append(cols[i])\n    \n    # 3rd Step\n    for col in med_cols:\n        df[col] = df[col].fillna(df[col].median()) \n    \n    # 4th Step\n    for col in fre_cols:\n        df[col] = df[col].fillna(df[col].value_counts().index[0])\n        \n    # 5th Step\n    labels = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\n    cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', \n            'FireplaceQu', 'GarageQual', 'GarageCond']\n    for col in cols:\n        df[col] = [labels[df[col][i]] for i in range(df.shape[0])]\n    \n    # 6th Step\n    if is_train:\n        init_encoder()\n        df = encoder.fit_transform(df, y)\n        # Storing the dataset for plotting the correlation matrix\n        global df_res_6th\n        df_res_6th = df\n    else:\n        df = encoder.transform(df)\n    \n    # Comment from 7th step to the last step to obtain the pre-processed data after the 6th step.\n    # 7th Step\n    cols = ['GrLivArea', 'GarageArea', '1stFlrSF']\n    df = df.drop(columns=cols, axis=1)\n    \n    if is_train:\n        return df, y, ids\n    else:\n        return df, ids","291f6726":"X, y, train_id =  pre_process(df_train, is_train = True)\nprint(train_id.shape)\nX.info()","d262fad9":"# In order to find out the further pre-processing steps (after 6th), we need to look at the \n# pre-processed data we obtained after the 6th step.\n\ndf_6th = pd.concat([df_res_6th, df_sp_6th], axis=1)\ndf_6th.head()\n\n# We are trying to find PCC (Pearson Co-relation Coefficient) between features, so that\n# we can eliminate some of the redundant features\ncor_mat = df_6th.corr(method='pearson', min_periods=50)\nprint(cor_mat.shape)\n\n# We will find out the 20 pair of features which have the largest value of PCC \ncols = cor_mat.nlargest(20, \"SalePrice\")\nprint(cols.shape)\n\nplt.figure(figsize = (15,15))\nsns.set(font_scale = 1.1)\nsns.heatmap(\n    cols[cols.index], cbar = True, annot = True, square = True, cmap = \"RdPu\" , \n    fmt = \".2f\", annot_kws = {\"size\": 10}, yticklabels = cols.index, \n    xticklabels = cols.index\n)\n\n# Based on this co-relation plot, we will be eliminating one feature from the pair of features \n# having value of PCC >= 0.80. We will be eliminating 'GrLivArea', 'GarageArea' and '1stFlrSF'","18db489f":"# Defining the custom metric [RMSE on Log]\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","6d26de38":"# Random Forest Model\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# reg = RandomForestRegressor(n_estimators = 100, criterion=\"mae\", min_samples_split=5)\n# reg.fit(X_train, y_train)\n# y_pred = reg.predict(X_test)\n# print(rmse_log(y_test, y_pred))","bd8dc7e6":"# Gradient Boosting Regressor Model\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ngbr = GradientBoostingRegressor(\n    learning_rate=0.05, n_estimators=3000, min_samples_split=15\n)\ngbr.fit(X_train, y_train)\ny_pred = gbr.predict(X_test)\nprint(rmse(y_test, y_pred))","d5d08ee1":"# XGB Regressor Model\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# xgb = xgb(\n#     learning_rate=0.05, n_estimators=7200, max_depth=6,\n#     random_state=42\n# )\n# xgb.fit(X_train, y_train)\n# y_pred = xgb.predict(X_test)\n# print(rmse(y_test, y_pred))","178992c9":"df_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(df_test.shape)\ndf_test.info()","3d77cfa6":"X, test_id =  pre_process(df_test, is_train = False)\nprint(X.shape, test_id.shape)","81ed6d9a":"y_sub = gbr.predict(X)\ny_sub = np.exp(y_sub) # Converting back from log scale to normal scale\nsubmission = pd.DataFrame({\"Id\": test_id, \"SalePrice\": y_sub})\nsubmission.head(5)","5d8665e2":"submission.to_csv(\"submission.csv\", index = False)","948bd1e8":"# House Prices - Advanced Regression Techniques\n- Hola fello enthusiasts! In this notebook, you can find my submission for the **House Prices - Advanced Regression Techniques** competition, which can be found [here](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques).\n- Before beginning with this notebook, make sure to read about the dataset itself. I didn't included the description of the dataset to keep this notebook as concise as possible, but one can read bout it [here](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data).\n- Reference Notebooks\n    - [Predicting House Prices XGBoost + GBM Models](https:\/\/www.kaggle.com\/bserdogan\/predicting-house-prices-xgboost-gbm-models\/notebook)\n    - [Gradient Boosting Regressor | Scikit-Learn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html)\n    - [Random Forest Regressor | Scikit-Learn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)","ccaca7d0":"# Installing & Importing Packages","548845e0":"# Submitting the Predictions","f7e9f839":"# Training the Model","c12621b1":"# Pre-processing the Dataset\n\n### ***1st Step***\n- If we are using the pre_process function on the train set, we will save the ids and sale price into separate columns, and remove them from the dataset.\n- Otherwise, we will only save the ids into a separate column, and remove them from the dataset.\n- Also, if used on training set, then we are taking the logarithmic of the SalePrice values, and then storing it into the separate column.\n- The reason behind the use of logarithmic is not so theoretically justified as to my knowledge. It leads to a bit better results though.\n- One of the probable explanations could be trying to normalize the SalePrice values\n\n### ***2nd Step***\n- From the above information, we can easily conclude that the features numbered **3, 6, 25, 26, 30-33, 35, 42, 57-60, 63, 64, and 72-74** consist of NULL values\n- Among these features, we will be dropping the ones, which have a very high missing ratio. These include features numbered **6, 72, 73, and 74**\n\n### ***3rd Step***\n- For the numerical features, we will be doing the **median-based** imputation. These include features numbered **3, 26 and 59**.  \n- But since, it might happen that in the test dataset some other numerical feature contains a NULL value, hence, we will be applying this on all the numerical features.\n\n### ***4th Step***\n- For the categorical features, we will be replacing NaN(s) with the most frequent values in the respective columns. These include features numbered **25, 30-33, 35, 42, 57, 58, 60, 63, and 64**.\n\n### ***5th Step***\n- Now, we will be converting the categorical features with inherenet ordinality into numerical features. These inlcude features numbered 26, 27, 29, 30, 39, 52, 56, 62, and 63\n\n### ***6th Step***\n- Now, we will be converting some of the categorical features into numerical features, with the help of Target Encoding. These include all the remaining categorical features.\n- I have used the Target Encoder implementation of Category Encoders Package, which can be found [here](https:\/\/contrib.scikit-learn.org\/category_encoders\/targetencoder.html). \n- After this step, all the features will be converted into numerical features.\n\n### ***7th Step***\n- Based on the co-relation plot that we plotted on the pre-processed data after the 6th step, we will be eliminating one feature from the pair of features having value of PCC >= 0.75, i.e. 'GrLivArea', 'OverallQual' and 'YearBuilt'","7bcd3aa1":"# Describing the Dataset"}}