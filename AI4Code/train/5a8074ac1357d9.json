{"cell_type":{"4d3300c9":"code","4e3fe5af":"code","10247b99":"code","f7c1e977":"code","69e3f68c":"code","08d50483":"code","06fb5ac6":"code","5e769454":"code","add6b13e":"code","f9bb38f6":"code","27c89791":"code","bcefe001":"code","855ea077":"code","b386c8a0":"code","a8bad621":"code","20a2669a":"code","240432b8":"code","605fc023":"code","046f9540":"code","b7542636":"code","0d0adc1b":"code","c164848b":"code","9b2fb3dc":"code","86427736":"code","8297f828":"code","21975a27":"code","ab9bb990":"code","d18cca69":"code","563247f2":"code","747c6380":"code","f8de1a8a":"code","cfe138c0":"code","b99ea005":"code","ec52b6c8":"code","dfbd00bf":"code","76f0349e":"code","ab06c499":"code","42ef75c5":"code","89c7ed37":"code","dae9f968":"code","1a75008b":"code","fb9eefc4":"code","9d8ab0b1":"code","d5ed85e0":"code","48735ca1":"code","db8e6467":"code","4dbff1b3":"code","20ad4fc2":"code","2d0269de":"code","8a8e613d":"code","9d78d866":"code","ec064326":"code","9201f5a8":"code","aaf70dbd":"code","3431fcc3":"code","ecf0c9bf":"code","a90f8391":"code","c4e3cd03":"code","9cc1d8f8":"code","b39ef27f":"code","99b0a704":"code","1f6774f3":"code","f44494ac":"code","3a20493f":"code","c75c4f6b":"code","dec1840c":"code","a0443706":"code","aa2965be":"code","0d37e092":"markdown","87434571":"markdown","5a64f3b1":"markdown","3da7df39":"markdown","53178a94":"markdown","4b9682fe":"markdown","cba5c15f":"markdown","1e0ea655":"markdown","869cec57":"markdown","d41bb921":"markdown","3aff8df3":"markdown","34567a32":"markdown","22a0ba4c":"markdown","66ab40c5":"markdown","b12f97dc":"markdown","dfd54058":"markdown","b6c67032":"markdown","24f985a1":"markdown","fb3959a9":"markdown","e5356a49":"markdown","d7992e96":"markdown","555610be":"markdown","a49762c9":"markdown","0fa3af40":"markdown","b8e9e4b8":"markdown","cdb796d1":"markdown","bbb1d3c1":"markdown","b730e1af":"markdown","22ff3688":"markdown","4e2cddc7":"markdown","4dd3b301":"markdown"},"source":{"4d3300c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 24})\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","4e3fe5af":"mcr = pd.read_csv('\/kaggle\/input\/kaggle-survey-2019\/multiple_choice_responses.csv')\nqs = pd.read_csv('\/kaggle\/input\/kaggle-survey-2019\/questions_only.csv')","10247b99":"print(mcr.shape, qs.shape)","f7c1e977":"mcr.head()","69e3f68c":"for i in qs.columns:\n    print(i,' : ',qs[i][0])","08d50483":"mcr['Q10'].value_counts()[:-1].plot(kind='bar', figsize=(15, 6), rot=90)","06fb5ac6":"q10_ranking = []\nq10_values = mcr['Q10']\nprint(\"Percentage of missing values: {} %\".format(round(100*q10_values.isna().sum()\/len(q10_values),2)))\nq10_values = q10_values.fillna('0')\nfor i in range(1,len(mcr)):\n    t=q10_values[i]\n    if True:\n        x10 = int(''.join(t.split('$')[-1].split('-')[-1].split(',')))\n        q10_ranking.append(x10)\n        #print(x10)\nmcr_q10_ranking = mcr[1:].copy()\nmcr_q10_ranking['max_income_level'] = q10_ranking\nvalue_cnts = mcr_q10_ranking['max_income_level'].value_counts()\naxx = (value_cnts.sort_index(ascending = True)[1:]).plot(kind='bar', figsize=(15, 6), rot=90)\naxx.set_xlabel(\"Income group maximum: USD $\")\naxx.set_ylabel(\"# of Respondents\")","5e769454":"q10_group = []\nfor i in range(len(q10_ranking)):\n    if q10_ranking[i]>=150000:\n        q10_group.append(3)\n    elif q10_ranking[i]>=50000 and q10_ranking[i]<=149999:\n        q10_group.append(2)\n    elif q10_ranking[i]>=1 and q10_ranking[i]<=49999:\n        q10_group.append(1)\n    else:\n        q10_group.append(0)","add6b13e":"mcr_q10_ranking.head()","f9bb38f6":"mcr_q10_ranking['q10_group'] = q10_group","27c89791":"mcr_q10_ranking.head()","bcefe001":"value_cnts = mcr_q10_ranking['q10_group'].value_counts()\naxx = (value_cnts.sort_index(ascending = True)[1:]).plot(kind='bar', figsize=(15, 6), rot=90)\naxx.set_xlabel(\"Income group: USD $\")\naxx.set_ylabel(\"# of Respondents\")\nprint('Group 3 is the highest earning group')","855ea077":"print('There are {} % of respondents who are earning more than 150,000$'.format(round(100*value_cnts[3]\/len(mcr_q10_ranking),2)))\nprint('{} % of respondents are earning between 50,000$ and 149,999$'.format(round(100*value_cnts[2]\/len(mcr_q10_ranking),2)))\nprint('{} % of respondents are earning < 50,000$'.format(round(100*value_cnts[1]\/len(mcr_q10_ranking),2)))\nprint('{} % of respondents are not earning'.format(round(100*value_cnts[0]\/len(mcr_q10_ranking),2)))","b386c8a0":"mcr_g3 = mcr_q10_ranking[mcr_q10_ranking['q10_group']==3]\nlen(mcr_g3)\/len(mcr_q10_ranking)","a8bad621":"#Reset_index\nmcr_g3 = mcr_g3.reset_index().drop('index', axis=1)\nmcr_g3.head()","20a2669a":"for i in mcr_g3.isna().sum().index:\n    print(i,' : ', mcr_g3[i].isna().sum())","240432b8":"# Time taken by the G3 people\nduration_only = mcr_g3[mcr_g3.columns[0]]\nimport seaborn as sns\nlst = pd.to_numeric(duration_only, errors='coerce')\/60\nlst = pd.DataFrame(lst)\nlst.columns = ['Time from Start to Finish (minutes)']\nlst = lst['Time from Start to Finish (minutes)']\nsns.distplot(lst, bins = 5000).set(xlim=(0, 60))","605fc023":"print('There are {} total G3 respondents out of whom {} respondents have spent more than 10 minutes taking this survey and {} respondents really took more than an hour.'.format(lst.shape[0],lst[lst>10].shape[0],lst[lst>60].shape[0]))","046f9540":"def plotMcqHist(i):\n    plt.rcParams.update({'font.size': 24})\n    print(mcr_g3.columns[i], '  : ', mcr[mcr_g3.columns[i]][0])\n    temp = mcr_g3[mcr_g3.columns[i]].value_counts()\n    temp = temp.fillna(-10)\n    plt.figure(figsize=(20,5))\n    #plt.xlabel(mcr_g3.columns[i])\n    plt.ylabel('# Respondents')\n    plt.bar(list(temp.index),list(temp))\n    \n    plt.xticks(rotation=90)\n    plt.show()\n    \n    print('='*50)","b7542636":"for i in enumerate(mcr_g3.columns):\n    print(i)","0d0adc1b":"lst1 = [1,2,4,5,6,8,9,10,20,21,48,55,95,116,117]","c164848b":"plotMcqHist(lst1[0])","9b2fb3dc":"# Reordering the plot about X axis\ntemp = mcr_g3[mcr_g3.columns[1]].value_counts()\n#temp = temp.fillna(-10)\ntemp_list=[]\nfor i in range(len(temp)):\n    temp_list.append(int(temp.index[i].split('-')[0].split('+')[0]))\ntmp = pd.DataFrame(temp)\ntmp['index'] = temp_list\ntmp = tmp.sort_values(by=['index'], ascending = True)\nplt.figure(figsize=(20,5))\n#plt.xlabel(mcr_g3.columns[1])\nplt.ylabel('# Respondents')\nplt.bar(list(tmp.index),list(tmp['Q1']))\n\nplt.xticks(rotation=90)\nplt.show()\nprint('='*50)","86427736":"print(\"There are {} % of G3 respondents who belong to 18 to 21 Age group\".format(round((((mcr_g3[mcr_g3.columns[lst1[0]]]=='18-21')).sum()\/len(mcr_g3))*100,2)))\nprint(\"There are {} % of G3 respondents who belong to 22 to 24 Age group\".format(round((((mcr_g3[mcr_g3.columns[lst1[0]]]=='22-24')).sum()\/len(mcr_g3))*100,2)))\nprint(\"There are {} % of G3 respondents who belong to 25 to 29 Age group\".format(round((((mcr_g3[mcr_g3.columns[lst1[0]]]=='25-29')).sum()\/len(mcr_g3))*100,2)))\nprint(\"There are {} % of G3 respondents who belong to 30 to 34 Age group\".format(round((((mcr_g3[mcr_g3.columns[lst1[0]]]=='30-34')).sum()\/len(mcr_g3))*100,2)))","8297f828":"plotMcqHist(lst1[1])","21975a27":"print(\"{} % of G3 respondents are Male\".format(round((((mcr_g3[mcr_g3.columns[lst1[1]]]=='Male')).sum()\/len(mcr_g3))*100,2)))\nprint(\"{} % of G3 respondents are Female\".format(round((((mcr_g3[mcr_g3.columns[lst1[1]]]=='Female')).sum()\/len(mcr_g3))*100,2)))","ab9bb990":"print(\"{} % of Total Male respondents belong to G3\".format(round((((mcr_g3[mcr_g3.columns[lst1[1]]]=='Male')).sum()\/((mcr[mcr.columns[lst1[1]]]=='Male')).sum())*100,2)))\nprint(\"{} % of Total Female respondents belong to G3\".format(round((((mcr_g3[mcr_g3.columns[lst1[1]]]=='Female')).sum()\/((mcr[mcr.columns[lst1[1]]]=='Female')).sum())*100,2)))","d18cca69":"plotMcqHist(lst1[2])","563247f2":"print(\"{} % of high income group (Group 3) respondents belong to United States of America\".format(round((mcr_g3[mcr_g3.columns[lst1[2]]]=='United States of America').sum()\/(len(mcr_g3))*100,2)))\nprint(\"While {} % of high income group (Group 3) respondents belong to India\".format(round((mcr_g3[mcr_g3.columns[lst1[2]]]=='India').sum()\/(len(mcr_g3))*100,2)))","747c6380":"plotMcqHist(lst1[3])","f8de1a8a":"print(\"{} % of high income group (Group 3) respondents have Bachelors as Highest level of Formal education\".format(round((mcr_g3[mcr_g3.columns[lst1[3]]]=='Bachelor\u2019s degree').sum()\/(len(mcr_g3))*100,2)))\nprint(\"{} % of high income group (Group 3) respondents have Masters as Highest level of Formal education\".format(round((mcr_g3[mcr_g3.columns[lst1[3]]]=='Master\u2019s degree').sum()\/(len(mcr_g3))*100,2)))\nprint(\"{} % of high income group (Group 3) respondents have Doctoral as Highest level of Formal education\".format(round((mcr_g3[mcr_g3.columns[lst1[3]]]=='Doctoral degree').sum()\/(len(mcr_g3))*100,2)))","cfe138c0":"plotMcqHist(lst1[4])","b99ea005":"print(\"{} % of G3 respondents identify themselves as Data Scientists\".format(round((((mcr_g3[mcr_g3.columns[lst1[4]]]=='Data Scientist')).sum()\/len(mcr_g3))*100,2)))\nprint(\"While {} % of total respondents identify themselves as Data Scientists\".format(round((((mcr[mcr.columns[lst1[4]]]=='Data Scientist')).sum()\/len(mcr))*100,2)))","ec52b6c8":"print(\"{} % of G3 respondents identify themselves as Data Scientists and have Master's Degree\".format(round((((mcr_g3[mcr_g3.columns[lst1[4]]]=='Data Scientist')&(mcr_g3[mcr_g3.columns[lst1[3]]]=='Master\u2019s degree')).sum()\/len(mcr_g3))*100,2)))\nprint(\"{} % of G3 respondents identify themselves as Data Scientists and have Doctoral Degree\".format(round((((mcr_g3[mcr_g3.columns[lst1[4]]]=='Data Scientist')&(mcr_g3[mcr_g3.columns[lst1[3]]]=='Doctoral degree')).sum()\/len(mcr_g3))*100,2)))\n\nprint(\"{} % of G3 respondents identify themselves as Data Scientists, have Master's Degree and belong to United States of America\".format(round((((mcr_g3[mcr_g3.columns[lst1[4]]]=='Data Scientist')&(mcr_g3[mcr_g3.columns[lst1[3]]]=='Master\u2019s degree') & (mcr_g3[mcr_g3.columns[lst1[2]]]=='United States of America')).sum()\/len(mcr_g3))*100,2)))\nprint(\"{} % of G3 respondents identify themselves as Data Scientists, have Doctoral Degree and belong to United States of America\".format(round((((mcr_g3[mcr_g3.columns[lst1[4]]]=='Data Scientist')&(mcr_g3[mcr_g3.columns[lst1[3]]]=='Doctoral degree') & (mcr_g3[mcr_g3.columns[lst1[2]]]=='United States of America')).sum()\/len(mcr_g3))*100,2)))\n\nprint(\"{} % of G3 Male respondents identify themselves as Data Scientists, have Master's Degree and belong to United States of America\".format(round((((mcr_g3[mcr_g3.columns[lst1[4]]]=='Data Scientist')&(mcr_g3[mcr_g3.columns[lst1[3]]]=='Master\u2019s degree') & (mcr_g3[mcr_g3.columns[lst1[2]]]=='United States of America')&(mcr_g3[mcr_g3.columns[lst1[1]]]=='Male')).sum()\/len(mcr_g3))*100,2)))\nprint(\"{} % of G3 Male respondents identify themselves as Data Scientists, have Doctoral Degree and belong to United States of America\".format(round((((mcr_g3[mcr_g3.columns[lst1[4]]]=='Data Scientist')&(mcr_g3[mcr_g3.columns[lst1[3]]]=='Doctoral degree') & (mcr_g3[mcr_g3.columns[lst1[2]]]=='United States of America')&(mcr_g3[mcr_g3.columns[lst1[1]]]=='Male')).sum()\/len(mcr_g3))*100,2)))\n\n","dfbd00bf":"plotMcqHist(lst1[5])","76f0349e":"plotMcqHist(lst1[6])","ab06c499":"plotMcqHist(lst1[7])","42ef75c5":"plotMcqHist(lst1[8])","89c7ed37":"print(\"If anyone follows the recommendations from this notebook, they have a {} % probability of earning an income >$500,000 and {} % of propbability of earning an income greater than $250,000 \".format(round(((mcr_g3[mcr_g3.columns[lst1[8]]]=='> $500,000').sum()\/len(mcr_g3))*100,2), round((((mcr_g3[mcr_g3.columns[lst1[8]]]=='250,000-299,999')|(mcr_g3[mcr_g3.columns[lst1[8]]]=='300,000-500,000')|(mcr_g3[mcr_g3.columns[lst1[8]]]=='> $500,000')).sum()\/len(mcr_g3))*100,2)))","dae9f968":"plotMcqHist(lst1[9])","1a75008b":"print(\"You are {}% likely to spend more than 100,000$ for cloud computing products.\".format(round(((mcr_g3[mcr_g3.columns[lst1[9]]]=='> $100,000 ($USD)').sum()\/len(mcr_g3))*100,2)))","fb9eefc4":"plotMcqHist(lst1[10])","9d8ab0b1":"print(\"You are {}% likely to use JupyterLab or RStudio type local development environments at work or school\".format(round(((mcr_g3[mcr_g3.columns[lst1[10]]]=='Local development environments (RStudio, JupyterLab, etc.)').sum()\/len(mcr_g3))*100,2)))","d5ed85e0":"plotMcqHist(lst1[11])","48735ca1":"# Reordering the plot about X axis\ntemp = mcr_g3[mcr_g3.columns[lst1[11]]].value_counts()\ntemp_list=[]\n\ntmp = pd.DataFrame(temp)\ntmp['index'] = [4,5,6,3,2,1,0]\nprint(tmp)\ntmp = tmp.sort_values(by=['index'], ascending = True)\nplt.figure(figsize=(20,5))\n#plt.xlabel(mcr_g3.columns[1])\nplt.ylabel(\"# Respondents\")\nplt.bar(list(tmp.index),list(tmp['Q15']))\n\nplt.xticks(rotation=90)\nplt.show()\nprint('='*50)","db8e6467":"print(\"You are {}% likely to have 10+ years of data analysis related coding experience if you are earning more than 150,000$.\".format(round((((mcr_g3[mcr_g3.columns[lst1[11]]]=='10-20 years')|(mcr_g3[mcr_g3.columns[lst1[11]]]=='20+ years')).sum()\/len(mcr_g3))*100,2)))","4dbff1b3":"plotMcqHist(lst1[12])","20ad4fc2":"print(\"Hahaa! {}% respondents earning more than 150,000$ recommend to learn Python first.\".format(round((((mcr_g3[mcr_g3.columns[lst1[12]]]=='Python')).sum()\/len(mcr_g3))*100,2)))\nprint(\"{}% respondents recommending to learn Python first are earning more than 150,000$.\".format(round((((mcr_g3[mcr_g3.columns[lst1[12]]]=='Python')).sum()\/(mcr[mcr.columns[lst1[12]]]=='Python').sum())*100,2)))","2d0269de":"plotMcqHist(lst1[14])","8a8e613d":"print('Q13', '  : ', mcr[mcr_g3.columns[35]][0].split('-')[0])\ntemp=[]\nind=[]\nfor i in [i for i in range(35,45)]:\n    temp.append(mcr_g3[mcr_g3.columns[i]].value_counts()[0])\n    ind.append(mcr_g3[mcr_g3.columns[i]].value_counts().index[0])\n#print(ind,temp)\nlst_tmp = pd.DataFrame(temp)\nlst_tmp.index=ind\n#print(lst_tmp)\nplt.figure(figsize=(20,5))\n#plt.xlabel('Q13')\nplt.ylabel('# Respondents')\nplt.bar(list(lst_tmp.index),list(lst_tmp[0]))\nplt.xticks(rotation=90)\nplt.show()\nprint('='*50)","9d78d866":"print('Q25', '  : ', mcr[mcr_g3.columns[118]][0].split('-')[0])\ntemp=[]\nind=[]\nfor i in [i for i in range(118,129)]:\n    temp.append(mcr_g3[mcr_g3.columns[i]].value_counts()[0])\n    ind.append(mcr_g3[mcr_g3.columns[i]].value_counts().index[0])\n#print(ind,temp)\nlst_tmp = pd.DataFrame(temp)\nlst_tmp.index=ind\n#print(lst_tmp)\nplt.figure(figsize=(20,5))\n#plt.xlabel('Q25')\nplt.ylabel('# Respondents')\nplt.bar(list(lst_tmp.index),list(lst_tmp[0]))\nplt.xticks(rotation=90)\nplt.show()\nprint('='*50)","ec064326":"print('Top 3 ML algorithms you need to be equiped with are: \\n',ind[:3])","9201f5a8":"print('Q29', '  : ', mcr[mcr_g3.columns[168]][0].split('-')[0])\ntemp=[]\nind=[]\nfor i in [i for i in range(169,178)]:\n    temp.append(mcr_g3[mcr_g3.columns[i]].value_counts()[0])\n    ind.append(mcr_g3[mcr_g3.columns[i]].value_counts().index[0])\n#print(ind,temp)\nlst_tmp = pd.DataFrame(temp)\nlst_tmp.index=ind\n#print(lst_tmp)\nplt.figure(figsize=(20,5))\n#plt.xlabel('Q29')\nplt.ylabel('# Respondents')\nplt.bar(list(lst_tmp.index),list(lst_tmp[0]))\nplt.xticks(rotation=90)\nplt.show()\nprint('='*50)","aaf70dbd":"print('Q30', '  : ', mcr[mcr_g3.columns[168]][0].split('-')[0])\ntemp=[]\nind=[]\nfor i in [i for i in range(181,192)]:\n    temp.append(mcr_g3[mcr_g3.columns[i]].value_counts()[0])\n    ind.append(mcr_g3[mcr_g3.columns[i]].value_counts().index[0])\n#print(ind,temp)\nlst_tmp = pd.DataFrame(temp)\nlst_tmp.index=ind\n#print(lst_tmp)\nplt.figure(figsize=(20,5))\n#plt.xlabel('Q30')\nplt.ylabel('# Respondents')\nplt.bar(list(lst_tmp.index),list(lst_tmp[0]))\nplt.xticks(rotation=90)\nplt.show()\nprint('='*50)","3431fcc3":"mcr","ecf0c9bf":"mcr_q10_ranking.head()","a90f8391":"# If respondent took more than 10 minutes: Category 0, else category 1\ntime_encoding = []\nfor i in mcr_q10_ranking[mcr_q10_ranking.columns[0]]:\n    if int(i) >600:\n        time_encoding.append(0)\n    elif int(i)<=600:\n        time_encoding.append(1)\n","c4e3cd03":"X = mcr_q10_ranking.copy()\n\nX = X.reset_index()\nY = mcr_q10_ranking['q10_group']\nX = X.drop([mcr_q10_ranking.columns[0],mcr_q10_ranking.columns[-2], 'index', 'q10_group', 'Q10'], axis=1)\nX.head()","9cc1d8f8":"# As there will be no new category we will proceed with Label Encoder. Data Leakage is not a problem here.\n# All columns are categorical\n\nfrom sklearn import preprocessing\n# Label Encoding\nfor f in X.columns:\n    lbl = preprocessing.LabelEncoder()\n    X[f] = lbl.fit_transform(list(X[f].values))","b39ef27f":"X.head()","99b0a704":"Y.head()","1f6774f3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, stratify=Y)\nX_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size = 0.2, stratify = y_train)\nprint(X_train.shape, y_train.shape)\nprint(X_cv.shape, y_cv.shape)\nprint(X_test.shape, y_test.shape)","f44494ac":"import xgboost as xgb\nclf = xgb.XGBClassifier(n_estimators=500,\n                        n_jobs=4,\n                        max_depth=10,\n                        learning_rate=0.05,\n                        subsample=0.9,\n                        colsample_bytree=0.9)\n\nclf.fit(X_train, y_train)","3a20493f":"predict_train = clf.predict(X_train)\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nprint(confusion_matrix(y_train,predict_train))\nprint(accuracy_score(y_train,predict_train))","c75c4f6b":"predict_cv = clf.predict(X_cv)\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nprint(confusion_matrix(y_cv,predict_cv))\nprint(accuracy_score(y_cv,predict_cv))","dec1840c":"predict = clf.predict(X_test)\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nprint(confusion_matrix(y_test,predict))\nprint(accuracy_score(y_test,predict))","a0443706":"# plot feature importance\nfrom xgboost import plot_importance\nplot_importance(clf, max_num_features=10)\nplt.show()","aa2965be":"print(mcr.columns[4], mcr[mcr.columns[4]][0])\nprint(mcr.columns[1], mcr[mcr.columns[1]][0])\nprint(mcr.columns[6], mcr[mcr.columns[6]][0])\nprint(mcr.columns[9], mcr[mcr.columns[9]][0])\nprint(mcr.columns[8], mcr[mcr.columns[8]][0])\nprint(mcr.columns[10], mcr[mcr.columns[10]][0])","0d37e092":"Competition Source: https:\/\/www.kaggle.com\/c\/kaggle-survey-2019\/overview <br>\nData Source: https:\/\/www.kaggle.com\/c\/kaggle-survey-2019\/data","87434571":"### Taking this challenge I present to you 'The road to a successful career in Machine Learning and Data Science'.\n- With simple bar plots","5a64f3b1":"There are 821 G3 respondents. More than 50% of them completed Data Science courses on Cousera. With personal experience I would also suggest to go through Kaggle Learn in addition to Coursera.","3da7df39":"#### Group 3 or G3 indicates a successful career. Let us start by analysing their responses and what is important to them.","53178a94":"A *successful career* can easily be indicated by the amount you earn and the amount you pay from and for ML and DS. This has been asked in <br>\nQuestion 10 : What is your current yearly compensation (approximate $USD)? and in <br>\nQuestion 11 :  Approximately how much money have you spent on machine learning and\/or cloud computing products at your work in the past 5 years?","4b9682fe":"This model is consistent with test set and validation set both giving an accuracy of 85%. Sadly, the model has overfitted on the train set.","cba5c15f":"Even though all these people belong to 'Group 3' they have different set of responses for different questions.","1e0ea655":"Generally 20+ individuals are responsible for Data Science workloads at their place of business.","869cec57":"There were 34 questions asked in the survey.","d41bb921":"Huh, why does USA lead every technology! We already know the answer.","3aff8df3":"I will end this quest with a summary of important points.\n\n### Summary: \nThere are 4.16% respondents who are earning more than 150,000 USD. The analysis is limited to this group. Well that is what we need. A high income!\n\n1. Average time spent by these people is 5- 10 minutes on this survey\n2. Most of them belong to 30 to 50 years\n3. The dataset is silently imbalanced by respondents who identify themselves as Male\n4. 70% of respondents belong to United States of America due to the availability of infrastructure and opportunities\n5. 44% have Master's degree\n6. 40% of them are Data Scientists\n7. Most of them work with large sized companies\n8. 20+ individuals are responsible for Data Science workloads at their business\n9. Companies with well established ML methods most likely pay a fat cheque\n10. 27% respondents of this group are earning more than 250,000 USD while 10% are earning more than 500,000 USD\n11. 33% respondents of this group have spent more than 100,000 USD on machine learning and\/or cloud computing products at work in past 5 years\n12. 43% use JupyterLab or RStudio\n13. 42% of respondents of this group have 10+ years of experience\n14. 62% of respondents of this group recommend learning Python first\n15. More than 50 % have completed Data Science courses on Coursera.\n16. You need to be equiped with Linear Regression, Logistic Regression, Decision Trees, Random Forests and GBM (XGB, LGBM)\n17. A sizeable percentage use Amazon Web Services for Cloud Computing","34567a32":"You will also be able to spend more than 100,000(USD) on cloud computing products.","22a0ba4c":"In addition, I will build an XGBoost Classifier to see what determines your pay.","66ab40c5":"Women in technology need more initiatives. Their participation in the field of Data Science and Machine Learning is low and their share of income is very low in this field. ","b12f97dc":"### Key: <br>\n#### Income(USD) >=150000 : Group '3'\n#### Income(USD) in 50000 and 149999 : Group '2'\n#### Income(USD) <=49999 : Group '1'\n#### Income not specified : Group '0'","dfd54058":"Kernel Credits: Thanks to Paul Mooney, the developer advocate at Kaggle for his kernel: https:\/\/www.kaggle.com\/paultimothymooney\/2018-kaggle-machine-learning-data-science-survey and thanks to Amin for his kernel: https:\/\/www.kaggle.com\/amiiiney\/student-community-in-kaggle","b6c67032":"Average time spent by high income respondents taking this Survey: 5 - 10 minutes","24f985a1":"Do get acquainted with Amazon Web Services for cloud computing purposes. You can productionize models, in addition to training ML models. This will also fetch you a good fat income.","fb3959a9":"## G3 respondent is a respondent whose income is greater than 150,000 USD.","e5356a49":"Train, Test, Validation split:","d7992e96":"Companies with well established ML methods most likely pay a fat cheque.","555610be":"It is obviuos that higher the age, the respondent most likely has higher experience and higher income. 90% of Data Scientists generally become successful when they reach age 30. Other constraints do play key role in determining the success in this field.","a49762c9":"People working at companies with large size earn a better income.","0fa3af40":"## 2019 Kaggle ML & DS Survey: ","b8e9e4b8":"## Notes for a succesful career:<br> \n- For an income higher than 150,000 USD\n\n### 1. Learn Python first <br> 2. Complete Data Science courses on Coursera and Kaggle Learn <br> 3. Be equipped with Linear Regression, Logistic Regression, Decision Trees, Random Forests, XGBoost and LightGBM <br> 4. Use Amazon Web Services for Cloud Computing <br> 5. Earn 10 years of experience<br> 6. Work in USA like countries where infrastructure is available<br> 7. Earn a Master's degree - Doctoral is a +<br> 8. Look out for Data Scientist roles <br> 9. Work with Large size companies <br> 10. Try spending on Cloud Computing products at work <br> 11. Use JupyterLab or RStudio <br> ","cdb796d1":"### Summary: \n1. Data Scientists are more capable of earning higher incomes. \n2. Most of them come from United States of America. \n3. Respondents who identify themselves as Male are quitely creating an imbalance in the dataset. \n4. A master's degree will fetch you more income.","bbb1d3c1":"### The challenge objective: \nTell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A \u201cstory\u201d could be defined any number of ways, and that\u2019s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about!","b730e1af":"This is my final submission for this competition.","22ff3688":"The G3 Data Scientist community is highly educated. Over 90% of G3 respondents have a degree equal to or above bachelor's level.","4e2cddc7":"### The following few blocks of code are for warming up the Kernel.","4dd3b301":"### For determining your income, your country is the most important attribute. The other important features are Age, Role, Size of data science team, company size and whether the employer incorporates ML methods into their business. Interestingly your degree stands at 9th position of importance. And your Gender is not an important attribute to determine your pay."}}