{"cell_type":{"667735a2":"code","873dae66":"code","3e60dd9d":"code","0d694f50":"code","d2fb63d3":"code","ee7b7477":"markdown"},"source":{"667735a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.mixture import GaussianMixture as GMM\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport itertools\nimport json\n\nfrom scipy import linalg\nimport matplotlib as mpl\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","873dae66":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n\ntrn = train.filter(regex='var.+')\ntst = test.filter(regex='var.+')","3e60dd9d":"trn.head()","0d694f50":"#Fit GMM based on number of components, returns labels of predictions\ndef fit_gmm(X,col,components):\n    \n    X = trn[col].values\n    Xr = X.reshape(-1,1)\n    gmm = GMM(n_components=components).fit(Xr)\n    labels = gmm.predict(Xr)\n    \n    return labels\n\n\n    \n#Find best fit for GMM\ndef calc_gmm(X,col,components):\n    \n    X = trn[col].values\n    X = X.reshape(-1,1)\n    \n    lowest_bic = np.infty\n    best_component = 0\n    \n    bic = []\n    n_components_range = range(1, components)\n    cv_types = ['spherical', 'tied', 'diag', 'full']\n    \n    for cv_type in cv_types:\n        for n_components in n_components_range:\n            # Fit a Gaussian mixture with EM\n            gmm = GMM(n_components=n_components,\n                                      covariance_type=cv_type)\n            gmm.fit(X)\n            bic.append(gmm.bic(X))\n            if bic[-1] < lowest_bic:\n                lowest_bic = bic[-1]\n                best_gmm = gmm\n                best_component = n_components\n\n    bic = np.array(bic)\n    color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',\n                              'darkorange'])\n    bars = []\n\n    print ('Column: {} Lowest BIC: {} Components:{}'.format(col,lowest_bic,best_component))\n           \n    # Plot the BIC scores\n    plt.figure(figsize=(20, 7))\n    spl = plt.subplot(2, 1, 1)\n    for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n        xpos = np.array(n_components_range) + .2 * (i - 2)\n        bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n                                  (i + 1) * len(n_components_range)],\n                        width=.2, color=color))\n    plt.xticks(n_components_range)\n    plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\n    plt.title('BIC score per model')\n    xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\n        .2 * np.floor(bic.argmin() \/ len(n_components_range))\n    plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\n    spl.set_xlabel('Number of components')\n    spl.legend([b[0] for b in bars], cv_types)\n\n    \"\"\"\n    # Plot the winner\n    splot = plt.subplot(2, 1, 2)\n    Y_ = clf.predict(X)\n    for i, (mean, cov, color) in enumerate(zip(clf.means_, clf.covariances_,\n                                           color_iter)):\n        v, w = linalg.eigh(cov)\n        if not np.any(Y_ == i):\n            continue\n        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n\n        # Plot an ellipse to show the Gaussian component\n        angle = np.arctan2(w[0][1], w[0][0])\n        angle = 180. * angle \/ np.pi  # convert to degrees\n        v = 2. * np.sqrt(2.) * np.sqrt(v)\n        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n        ell.set_clip_box(splot.bbox)\n        ell.set_alpha(.5)\n        splot.add_artist(ell)\n\n    plt.xticks(())\n    plt.yticks(())\n    plt.title('Selected GMM: full model, 2 components')\n    plt.subplots_adjust(hspace=.35, bottom=.02)\n    plt.show()\n    \"\"\"\n\n    \n    return best_gmm, best_component    \n    \n\n\n","d2fb63d3":"gmm,best_component = calc_gmm(trn,'var_126',15) \n\n","ee7b7477":"Since some variable distributions look like Gaussian Mixtures, I have been playing around with the sklearn GMM library to try and extract the number of components using the BIC score: https:\/\/scikit-learn.org\/stable\/auto_examples\/mixture\/plot_gmm_selection.html\n\nBelow kernel shows example for one var, but most vars show a different number of expected components. That doesnt make sense intuitively for me, because I would have expected similar noise functions to be added to add noise to something like a categorical variable. \n\nThoughts and comments are welcome!"}}