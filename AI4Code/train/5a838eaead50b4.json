{"cell_type":{"c7955c1c":"code","38e4bfc6":"code","f86e1f75":"code","0b6409d0":"code","9114377b":"code","59f6013d":"code","9ce92d71":"code","ebc97c08":"code","455884d0":"code","3902bd34":"code","7eafc34e":"code","e285a94e":"code","fce5ec4d":"code","d3ce74f3":"code","fbced0e8":"code","504c1cce":"code","3759ff9d":"code","b142ed6b":"code","a69462b8":"code","58192c3d":"code","ee27e349":"code","7ed4807f":"code","24ecb337":"code","9804529b":"code","6d9a0015":"code","5ac9ff77":"code","773a4b4d":"code","57ef235e":"code","b9ed02c6":"code","4d5baf15":"code","e959c418":"code","08cc404d":"code","ceb49169":"code","2c7810c0":"code","9eadd5f5":"code","b4b3363e":"code","66c5d9d3":"code","22782a90":"code","1a89fc42":"code","364308c0":"code","a11d9144":"markdown","b2550963":"markdown","a2a436bf":"markdown","59eac4fd":"markdown","0908f94e":"markdown","755b7d35":"markdown","2eeca6e9":"markdown","2ce45afd":"markdown","d61ecb04":"markdown","c31f4a2a":"markdown","169be3c5":"markdown","3ae3aa69":"markdown","f92da282":"markdown","0a313cb8":"markdown","015503ae":"markdown"},"source":{"c7955c1c":"#importing packages for data Analysis\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n#importing wordcloud for visual\n\nfrom wordcloud import WordCloud, STOPWORDS","38e4bfc6":"import spacy\n\nnlp = spacy.load('en_core_web_sm')","f86e1f75":"#importing packages for data visuals\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#importing packages for interactive data visuals\n\nfrom plotly import __version__\n\nimport cufflinks as cf\n\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()\n\n\nfrom IPython.display import HTML\n\n\n\nimport plotly.express as px","0b6409d0":"# importing natural language packages\/library to process text\n\nimport nltk\n\nnltk.download('vader_lexicon')\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","9114377b":"from nltk.sentiment.vader import SentimentIntensityAnalyzer","59f6013d":"#importing data\n\ndf = pd.read_csv('..\/input\/covid19-tweets\/covid19_tweets.csv')","9ce92d71":"df.head()","ebc97c08":"# let us start with missing values\n\n\nMissing_values = df.isnull().sum() \/ len(df) *100\n\nMissing_values.iplot(kind='bar',title='Missing values in each columns in %',theme='white',color='#3DD8AD')\n\n\n","455884d0":"# we can how many rows are empty in each cloumn through visual\n\nplt.figure(figsize=(14,6))\nplt.title('Missing values in each columns')\nsns.set_context(context='notebook',font_scale=1.5)\nsns.heatmap(df.isnull(),cmap='Set3',cbar=False,yticklabels=False)","3902bd34":"df['user_location'].value_counts()[0:5]\n\n#below results seems to be fine but after deep cleaning the location column found that United states has the higer number of tweets","7eafc34e":"# cleaning user location column\n\ndf['user_location'] = df['user_location'].str.replace('Stuck in the Middle','None')\n\n\n# well we can create a list to each sub location merge with Main location\n\nIndia_list = ['\u0906\u0930\u094d\u092f\u093e\u0935\u0930\u094d\u0924','. \u092d\u093e\u0930\u0924','New Delhi, India','Mumbai, India','Chennai, India',\n              'Hyderabad, India','Bhubaneshwar, India','Bengaluru, India',\n              'New Delhi','Mumbai','Chennai','Bhubaneswar','Jammu And Kashmir'\n             'Guwahati, India','Odisha','Pune, India','Bangalore','India, India',\n             'India India . India . ','India, India','Jammu And Kashmir','Hyderabad',\n             'Jaipur, India','INDIA','Maharashtra, India','Delhi','India','Kolkata, India',\n              'BENGALURU','Ahmadabad City, India','Guwahati  India']\n\ndf['user_location'] = df['user_location'].str.replace('|'.join(India_list),'India')\ndf['user_location'] = df['user_location'].str.replace('India  India','India')\n\nU_S_list = ['Washington, DC','New York, NY','Los Angeles, CA','USA','California, USA','Atlanta, GA','Chicago, IL',\n            'Boston, MA','New York','San Francisco, CA','Texas, United States',\n            'Houston, TX','Florida, United States','Seattle, WA','San Diego, CA',\n           'Washington, D.C.','Austin, TX','New York, United States','Text RESIST to 50409',\n            'Texas','United States, United States','NYC','astroworld','New Jersey, United States','Las Vegas, NV',\n           'United States City','Philadelphia, PA','Los Angeles','California','New Jersey, United States',\n            'United States, United States','United States City', 'Pewee Valley, KY','Global','Earth','Worldwide',\n            'WORLDWIDE','Stuck in the Middle','535 Radio Lane, Henderson, NC','Shoreview, MN','Nashville, TN | Tucson','United States|United States, AZ',\n            'Everywhere! ','Florida, United States','Florida, United States','United States City','Planet Location Unknown',\n           'New Jersey, United States','Florida','Brooklyn, NY','United States','Baltimore, MD','New Orleans, Louisiana',\n           'Virginia, United States','Raleigh, NC','Memphis, TN','Ohio, United States','North Carolina, United States',\n           'Buffalo, NY','Ohio, United States','Sacramento, CA','Sacramento, CA','Oakland, CA',\n           'United States, United States','Planet United States','United States']\n\ndf['user_location'] = df['user_location'].str.replace('|'.join(U_S_list),'United States')\n\ndf['user_location'] = df['user_location'].str.replace('United States  United States','United States')\n\n\nU_K_list =['London, England','London','UK','London, UK','England, United Kingdom','United Kingdom, United Kingdom','United Kingdom  United Kingdom']\n\ndf['user_location'] = df['user_location'].str.replace('|'.join(U_K_list),'United Kingdom')\n\n\nCanada_list = ['Toronto, Ontario','Toronto','Ontario, Canada','Canada, Canada','Vancouver',\n              'Vancouver, British Columbia','Canada  Canada']\n\ndf['user_location'] = df['user_location'].str.replace('|'.join(Canada_list),'Canada')\n\n\nAustralia_list = ['Melbourne, Victoria','Sydney, Australia','Sydney, New South Wales',\n                  'Sydney, New South Wales','Melbourne, Australia','Melbourne','Canberra']\n\ndf['user_location'] = df['user_location'].str.replace('|'.join(Australia_list),'Australia')\n\n\n\nPhilippines_list = ['Manila, Philippines']\n\ndf['user_location'] = df['user_location'].str.replace('|'.join(Philippines_list),'Philippines')\n\nSouth_Africa_list = ['Johannesburg  South Africa','Cape Town  South Africa','Johannesburg  South Africa']\n\ndf['user_location'] = df['user_location'].str.replace('|'.join(South_Africa_list),'South Africa')\n\ndf['user_location'] = df['user_location'].str.replace(r'\\W',' ')","e285a94e":"# this is after cleaning the location column\ndf['user_location'].value_counts()[0:5]","fce5ec4d":"# cleaning user_created column\n\n# converting from object to datetime\n\ndf['user_created'] = pd.to_datetime(df['user_created'])\n\n","d3ce74f3":"#cleaning data column\n\ndf['date'] = pd.to_datetime(df['date'])\n\n# let us create a new column for Month and year from the date column\n\ndf['Month'] = df['date'].dt.month\ndf['Year'] = df['date'].dt.year","fbced0e8":"# cleaning df['hashtags']\n\n# let us remove the non-Alphanumeric\ndf['hashtags'] = df['hashtags'].str.replace(r'\\W',' ')# using Regural expression code to remove the +-=\n\nhastags_list_clean = ['nan','nan, ',' ...,']\ndf['hashtags'] = df['hashtags'].str.replace('|'.join(hastags_list_clean),'')","504c1cce":"# cleaning the text column\n\ndf['text'] = df['text'].str.replace(r'\\W',' ')\ndf['text'] = df['text'].str.replace(r'\\d+',' ')\ndf['text'] = df['text'].str.replace('@',\" \")","3759ff9d":"#let see what we have in the words\nimport spacy\n\nnlp = spacy.load('en_core_web_sm')\n\n\n\nhastag_token = ''\n\nfor words in df['hashtags'].value_counts().index[0:100]:\n    hastag_token += words\n\ndoc = nlp(hastag_token)","b142ed6b":"from spacy import displacy\n\n\ndisplacy.render(doc, style='ent',jupyter=True)\n\n# Hastags \n# using spacy-displacy analysing the Hastag words ","a69462b8":"# Top 15 user location by number of tweets\n\n\nimport plotly.io as pio\n\npio.templates.default = \"plotly_white\"\n\n\ndata_plot = df.groupby('user_location')['user_name'].count().reset_index()\n\ndata_plot = data_plot.sort_values(['user_name'])\n\ndata_plot = data_plot.tail(15)\n\nfig = px.bar(data_plot, x='user_location', y='user_name',\n             color='user_name',labels={'user_name':'Number of Twitter users'},\n             title='Top 15 user location by number of tweets',\n             height=600)\nfig.show()\n\n","58192c3d":"# top 15  user name by number of tweets \n\nimport plotly.io as pio\n\npio.templates.default = \"simple_white\"\n\ndata_plot_Top15_user = df.groupby('user_name')['user_location'].count().reset_index()\n\ndata_plot_Top15_user = data_plot_Top15_user.sort_values(['user_location'])\n\ndata_plot_Top15_user = data_plot_Top15_user.tail(15)\n\n\nfig = px.bar(data_plot_Top15_user, x='user_name', y='user_location',color='user_location',\n            labels={'user_name':'Users','user_location':'Number of tweets'},\n            title='Top 15 user by number of tweets',height=600)\nfig.show()\n\n\n","ee27e349":"# no. of user is verified or not\n\nplt.figure(figsize=(10,6))\nsns.set_context(context='notebook',font_scale=1.5)\nsns.countplot(df['user_verified'],palette='gist_rainbow')\nplt.title('Verified user and unverified user')","7ed4807f":"# let us plot hastag\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in df['hashtags']: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 1800, height = 1200, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = '#40e0d0') \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title('hashtags')  \nplt.show() ","24ecb337":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n\nsid = SentimentIntensityAnalyzer()","9804529b":"# creating the new column for pol score\n\ndf['Sentiment_scores'] = df['text'].apply(lambda tweet: sid.polarity_scores(tweet))","6d9a0015":"# creating new column for the compound sentiment Analysis Score\n\ndf['compound'] = df['Sentiment_scores'].apply(lambda x:x['compound'])","5ac9ff77":"# using lambda function to mark 'neg' and 'pos'  compound score\n\ndf['compound_result'] = df['compound'].apply(lambda score: 'pos' if score >=0 else 'neg')","773a4b4d":"plt.figure(figsize=(10,6))\nsns.set_context(context='notebook',font_scale=1.5)\nsns.countplot(df['compound_result'],palette='Set2')\nplt.title('Over all Tweet Sentiment score results');","57ef235e":"# Top 12 country with their tweet sentiment score\n\nUS_compound_score    = df[df['user_location'].str.contains('United States',na=False)]['compound_result']\nIndia_compound_score = df[df['user_location'].str.contains('India',na=False)]['compound_result']\nChina_compound_score = df[df['user_location'].str.contains('China',na=False)]['compound_result']\nUnited_Kingdom_score = df[df['user_location'].str.contains('United Kingdom',na=False)]['compound_result']\nAustralia_score       = df[df['user_location'].str.contains('Australia',na=False)]['compound_result']\nSouth_Africa_score     =  df[df['user_location'].str.contains('South Africa',na=False)]['compound_result']\nLagos_Nigeria_score     =  df[df['user_location'].str.contains('Lagos  Nigeria',na=False)]['compound_result']\nPhilippines_score     =  df[df['user_location'].str.contains('Philippines',na=False)]['compound_result']\nNairobi_Kenya_score     =  df[df['user_location'].str.contains('Nairobi  Kenya',na=False)]['compound_result']\nSwitzerland_score     =  df[df['user_location'].str.contains('Switzerland',na=False)]['compound_result']\nIreland_score     =  df[df['user_location'].str.contains('Ireland',na=False)]['compound_result']\nSingapore_score     =  df[df['user_location'].str.contains('Singapore',na=False)]['compound_result']\n","b9ed02c6":"sa = pd.DataFrame({'United_States':US_compound_score, 'India':India_compound_score, 'China':China_compound_score,'United_Kingdom':United_Kingdom_score,\n                   'Australia':Australia_score, 'South_Africa':South_Africa_score,\n                   'Philippines':Philippines_score,'Switzerland':Switzerland_score,'Ireland':Ireland_score,\n                   'Singapore':Singapore_score})\n                   \ndf_sa = pd.melt(sa)                 \n                   \ndf_sa.columns = ['Country','positive_Negative_sentiment']","4d5baf15":"plt.figure(figsize=(10,10))\nplt.title('Tweets Sentiment Analysis score results for top 10 countries')\n\nsns.countplot(data=df_sa, y='Country', hue='positive_Negative_sentiment');","e959c418":"pip install NRCLex","08cc404d":"# import of package\nimport nltk\nfrom nrclex import NRCLex\n\nfrom nltk.corpus import stopwords","ceb49169":"df['text'] = df['text'].str.strip()","2c7810c0":"# function for convert the text and return tweets to emotions affects as dataframe\n\ndef emo_tweet(word):\n    word = [word for word in word if word not in stopwords.words('english')]\n    word = str([cell.encode('utf-8') for cell in word])# to convert the text into utf-8 unicode\n    str_text = NRCLex(word) \n    str_text = str_text.raw_emotion_scores\n    str_text = pd.DataFrame(str_text,index=[0])\n    str_text = pd.melt(str_text)\n    str_text.columns = ('Emotions','Count')\n    str_text = str_text.sort_values('Count')\n    return str_text","9eadd5f5":"Ind_emo = df[df['user_location'].str.contains('India',na = False)]['text']\nUS_emo = df[df['user_location'].str.contains('United States',na=False)]['text']\nchina_emo = df[df['user_location'].str.contains('China',na=False)]['text']","b4b3363e":"\nIndia_emotion = emo_tweet(Ind_emo)\nUS_emotion = emo_tweet(US_emo)","66c5d9d3":"# tweets emotions affects\n\nplt.figure(figsize=(10,6))\nplt.title('Tweet from India and its emotional affects')\nsns.set_style('white')\nsns.set_context(context='notebook',font_scale=1.2)\nsns.barplot(y='Emotions',x='Count',data=India_emotion[0:8],palette='viridis');","22782a90":"# tweets from USA and its emotional affects\n\nplt.figure(figsize=(10,6))\nplt.title('Tweet from USA and its emotional affects')\nsns.set_style('dark')\nsns.set_context(context='notebook',font_scale=1.2)\nsns.barplot(y='Emotions',x='Count',data = US_emotion[0:8],palette='ocean_r');","1a89fc42":"Overall_tweet_emo = emo_tweet(df['text'])","364308c0":"plt.figure(figsize=(12,6))\nplt.title('overall tweets and its emotional affects')\nsns.set_style('dark')\nsns.set_context(context='notebook',font_scale=1.5)\nsns.barplot(x='Emotions',y='Count',data = Overall_tweet_emo[0:8],palette='viridis');","a11d9144":"# Agenda\n\n* <a href=\"#Import-library\">Import library<\/a>\n* <a href=\"#Missing-data\">Missing data<\/a>\n* <a href=\"#Data-Cleaning\">Data Cleaning<\/a>\n* <a href=\"#Data-Visualization\">Data Visualization<\/a>\n* <a href=\"#Sentiment-Analysis-using-nltk-VADER\">Sentiment Analysis using nltk VADER<\/a>\n","b2550963":"# Thank you!!!","a2a436bf":"# <a href=\"#Let us see tweets sentiment score for each country\">Let us see tweets sentiment score for each country<\/a>","59eac4fd":"# <a href=\"#Missing-data\">Missing data<\/a>","0908f94e":"![](https:\/\/i.imgur.com\/z67zou3.jpg)","755b7d35":"* Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n\nRef-wikipedia.org\n\n**VADER**\n* A SentimentAnalyzer is a tool to implement and facilitate Sentiment Analysis tasks using NLTK features and classifiers,\nespecially for teaching and demonstrative purposes.","2eeca6e9":"# NRCLex\n\n* NRCLex will measure emotional affect from a body of text. Affect dictionary contains approximately 27,000 words, and is based on the National Research Council Canada (NRC) affect lexicon.\n\nhttp:\/\/sentiment.nrc.ca\/lexicons-for-research\/\n","2ce45afd":"# <a href=\"#Sentiment Analysis using nltk VADER\">Sentiment Analysis using nltk VADER<\/a>","d61ecb04":"# <a href=\"#Data Visualization\">Data Visualization<\/a>","c31f4a2a":"# <a href=\"#Agenda\"> Back to Index<\/a>","169be3c5":"* It seems tweets are more positive ","3ae3aa69":"# **Import library**\n\n* Importing packages for data analysis\n* Importing pacakges for data visuals","f92da282":"# we can analyse the tweets emotional affects\n\nEmotional affects measured include the following :\n\n* fear\n* anger\n* anticipation\n* trust\n* surprise\n* positive\n* negative\n* sadness\n* disgust\n* joy","0a313cb8":"![](https:\/\/i.imgur.com\/ZyS1hUp.jpg)","015503ae":"# <a href=\"#Data Cleaning\">Data Cleaning<\/a>\n\nData cleansing or data cleaning is the process of detecting and correcting corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.\n*  Ref Wikipedia"}}