{"cell_type":{"845c6f3f":"code","a370d6f7":"code","745e87f6":"code","d3aa5996":"code","1f12759a":"code","4a4a4f6d":"code","2c7043e0":"code","44ea7550":"code","17d98bf7":"code","4bbbd1a7":"code","bc3ae9dc":"code","764f2981":"code","0d49e5e3":"code","7220088d":"code","b47950b0":"code","efe0594b":"code","f806292b":"code","1d5618f4":"code","1f3b4d90":"code","ad7c9eec":"code","e364f0be":"code","40a4fce9":"code","6d196803":"code","809ac7c1":"code","9ac4a1f9":"code","f932994b":"code","690f97c9":"code","67c9874f":"code","c3850abd":"code","8a9aecfc":"code","369882a7":"code","05c41b4a":"code","8e60af80":"code","a443dc7f":"code","67198698":"code","73ab2004":"code","00d9a2ca":"code","990709e5":"code","1ed3e963":"code","d9b2d5e3":"code","35f152d7":"code","86926068":"code","bbfeffe1":"code","2a168c2c":"code","a4e17a13":"code","5d1744ef":"code","2bf0e5ba":"code","2cc89a31":"code","2778e400":"code","fcc4922c":"code","d8888a38":"code","168d4d0f":"code","37a3bea7":"code","b8ea99f1":"code","94c2e7b6":"code","42fa79f9":"code","1ad6181c":"markdown","ff21bdb5":"markdown","00b4210b":"markdown","abf15029":"markdown","f4055555":"markdown","1f9cf94c":"markdown","6340e94c":"markdown","76a91c96":"markdown","a70fce75":"markdown","8538c99b":"markdown","6b19ec05":"markdown"},"source":{"845c6f3f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\n\nimport nltk\nimport spacy\n#from glove import Corpus, Glove\n#from gensim.utils import deaccen\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n##Importing necessary files \nimport pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport time\nimport re\nfrom sklearn.decomposition import PCA\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nimport string\nimport gensim\nfrom sklearn.manifold import TSNE\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.models import Word2Vec, FastText\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","a370d6f7":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport nltk\n#nltk.download('stopwords')\nimport gensim\n\nfrom nltk.corpus import stopwords \n\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# spacy for lemmatization\nimport spacy\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nimport gensim.downloader as api\nfrom gensim.models import TfidfModel\nfrom gensim.corpora import Dictionary","745e87f6":"stop_words = stopwords.words('english')","d3aa5996":"meta_df = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv', usecols=['cord_uid', 'sha', 'title', 'doi', 'abstract', \n                                                 'authors', 'publish_time','Microsoft Academic Paper ID','pubmed_id',\n                                              'has_pdf_parse', 'has_pmc_xml_parse'], dtype={'pubmed_id': str,\n                                                                                           \"Microsoft Academic Paper ID\":str})","1f12759a":"meta_df.abstract.isnull().sum()","4a4a4f6d":"meta_df_cp = meta_df.copy()\n\nmeta_df_cp = meta_df_cp[pd.notnull(meta_df_cp.abstract)]","2c7043e0":"def sentence_clean(tokens):\n    for token in tokens:\n        yield(gensim.utils.simple_preprocess(str(token), deacc=True))\n\ndatatokens = list(sentence_clean(meta_df_cp.abstract.values.tolist()))","44ea7550":"def review_preprocess(review):\n    \n    en_stops = set(stopwords.words('english'))\n    \n    ##Removing any tags\n    \n    clean = re.compile('<.*?>')\n    review_without_tag = re.sub(clean, '', review) \n    \n    ###Remove any unwanted url\n    \n    review_without_tag_and_url = re.sub(r\"http\\S+\", \"\", review_without_tag)\n    review_without_tag_and_url = re.sub(r\"www\\S+\", \"\", review_without_tag)\n    \n    ##Convert to lower case\n    \n    review_lowercase = review_without_tag_and_url.lower()\n    \n    ##split into words\n    \n    list_of_words = word_tokenize(review_lowercase)\n    \n    ##Remove punctuation\n\n    list_of_words_without_punctuation=[''.join(this_char for this_char in this_string if (this_char in string.ascii_lowercase))for this_string in list_of_words]\n     \n    ## Remove any empty string\n    \n    list_of_words_without_punctuation = list(filter(None, list_of_words_without_punctuation))\n    \n    ###Remove any stop words\n\n    filtered_word_list = [w for w in list_of_words_without_punctuation if w not in en_stops] \n    \n    ### join all cleaned data\n    return ' '.join(filtered_word_list)","17d98bf7":"meta_df_cp.tail()","4bbbd1a7":"import time\nstart_time=time.time()\nmeta_df_cp['new']=meta_df_cp['abstract'].apply(review_preprocess)\nprint('Elapsed time for review preprocessing : ',((time.time()-start_time)\/60),' in minutes')","bc3ae9dc":"meta_df_cp['new'].tail()","764f2981":"data = meta_df_cp.new.str.cat(sep=' ')\n\n###tokenize words\ntokens = word_tokenize(data)\n\nvocabulary = set(tokens)\nprint('Number of vocabulary : {}'.format(len(vocabulary)))\n\n##frequency distribution of words\nfrequency_distribution = nltk.FreqDist(tokens)\nsorted(frequency_distribution,key=frequency_distribution.__getitem__, reverse=True)[0:5]","0d49e5e3":"###plotting wordcloud from frequency distribution\nwordcloud = WordCloud().generate_from_frequencies(frequency_distribution)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","7220088d":"###calculating words vs frequency distribution \nwords_frequency= pd.Series(tokens).value_counts().reset_index()\nwords_frequency.columns = ['Word','Frequency']\n\n\nwords_frequency.head()","b47950b0":"### Bar plot of frequency distribution\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(style=\"whitegrid\")\nsns.set(rc={'figure.figsize':(17,9)})\n### Fill the required inputs in the below code ###\nax = sns.barplot(x=\"Word\", y=\"Frequency\", data=words_frequency[words_frequency.index<20])\n\nplt.show()","efe0594b":"### appending in list for word2vec\nlist_of_tokenized = []\nfor one_sentence in meta_df_cp.new:\n    list_of_tokenized.append(gensim.utils.simple_preprocess(one_sentence))\n    ","f806292b":"### usning word2vec model \nmodel = Word2Vec(list_of_tokenized, size=150, window=10, min_count=2, workers=10)","1d5618f4":"list_of_tokenized[0]","1f3b4d90":"w1 = \"population\"\nmodel.wv.most_similar(positive=w1)","ad7c9eec":"w1 = [\"respiratory\"]\nmodel.wv.most_similar (positive=w1,topn=3)","e364f0be":"model.wv.doesnt_match([\"china\",\"coronavirus\",\"pandemic\",\"table\"])","40a4fce9":"###using pca and t-sne to plot the word embedding space\ndef word_vectors_plot(model, input_word, word_list):\n    \n    word_arrays = np.empty((0, 150), dtype='f')\n    word_tags = [input_word]\n    color_list  = ['blue']\n\n    \n    word_arrays = np.append(word_arrays, model.wv.__getitem__([input_word]), axis=0)\n    \n    \n    similar_words = model.wv.most_similar([input_word],topn=8)\n    \n\n    for word_score in similar_words:\n        word_vector = model.wv.__getitem__([word_score[0]])\n        word_tags.append(word_score[0])\n        color_list.append('green')\n        word_arrays = np.append(word_arrays, word_vector, axis=0)\n    \n   \n    for word in word_list:\n        word_vector = model.wv.__getitem__([word])\n        word_tags.append(word)\n        color_list.append('red')\n        word_arrays = np.append(word_arrays, word_vector, axis=0)\n        \n    \n    reduce = PCA(n_components=17).fit_transform(word_arrays)\n    \n    \n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduce)\n    \n    \n    \n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_tags,\n                       'color': color_list})\n    fig, _ = plt.subplots()\n    fig.set_size_inches(9, 9)\n    \n    \n    \n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 40,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    \n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n\n    \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE viz for input: {}'.format(input_word.title()),fontsize=16)","6d196803":"word_vectors_plot(model, 'coronavirus', ['china', 'infection', 'ncov', 'respiratory', 'covid', 'transmission', 'wuhan', 'severe'])","809ac7c1":"bigram = gensim.models.Phrases(datatokens, min_count=5, threshold=100)\nbigram_mod = gensim.models.phrases.Phraser(bigram)\n\ndef remove_stopwords(words):\n    return [[word for word in simple_preprocess(str(row)) if word not in stop_words] for row in words]\n\ndef lemmatize_text(words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    text = []\n    for sent in words:\n        row = nlp(\" \".join(sent)) \n        text.append([token.lemma_ for token in row if token.pos_ in allowed_postags])\n    return text\n\ndef create_bigram(text):\n     return [bigram_mod[doc] for doc in text]","9ac4a1f9":"data_word = remove_stopwords(datatokens)\n\ndata_word = create_bigram(data_word)","f932994b":"# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\nlemmatized_text = lemmatize_text(data_word, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])","690f97c9":"lemmatized_text[:1]","67c9874f":"data_word[0]","c3850abd":"# Create the dictionary all the tokens or words we have preprocessed using the above data \n\nid2word = corpora.Dictionary(lemmatized_text)\n\n# create a corpus \ncorpus = [id2word.doc2bow(text) for text in lemmatized_text]","8a9aecfc":"[[(id2word[id], freq) for id, freq in corp]for corp in corpus[:1]]","369882a7":"def compute_coherence(dictionary, corpus, texts, limit, start=2, step=3):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n    \"\"\"\n    \n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","05c41b4a":"model_list, coherence_values = compute_coherence(dictionary=id2word, corpus=corpus, texts=lemmatized_text, start=2, limit=40, step=6)","8e60af80":"# TF-IDF Document Frequency\ntfidf = TfidfModel(corpus)\ntfidf_corpus = tfidf[corpus]","a443dc7f":"# Show graph\nlimit=40; start=2; step=6#\nx = range(start, limit, step)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","67198698":"# # Print the coherence scores\nfor m, cv in zip(x, coherence_values):\n    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))","73ab2004":"# Select the model and print the topics\noptimal_model = model_list[4]\nmodel_topics = optimal_model.show_topics(formatted=False)\nprint(optimal_model.print_topics(num_words=10))","00d9a2ca":"import collections\n \ntry:\n    from collections import OrderedDict\nexcept ImportError:\n    OrderedDict = dict","990709e5":"data_lda = {i: OrderedDict(optimal_model.show_topics(i,5)) for i in range(5)}\n","1ed3e963":"df_lda = pd.DataFrame(data_lda)\ndf_lda = df_lda.fillna(0).T\nprint(df_lda.shape)","d9b2d5e3":"# g=sns.clustermap(df_lda.corr(), center=0, standard_scale=1, cmap=\"RdBu\", metric='cosine', linewidths=.75, figsize=(15, 15))\n# plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n# plt.show()\n#plt.setp(ax_heatmap.get_yticklabels(), rotation=0)  # For y axis","35f152d7":"def dominat_topic_ident(ldamodel, corpus, text):\n    # output\n    dominat_topic_sent = pd.DataFrame()\n    \n    # get the dominat topic for all the sentences in the dataset\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x : (x[1]), reverse=True)\n        # Get the dominat topic, it's percentage contribution and Keywords for each document \n        for j, (topic_num, prop_topic) in enumerate (row):\n            # dominat topic \n            if j == 0:\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([w for w, p in wp])\n                dominat_topic_sent = dominat_topic_sent.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), \n                                                          ignore_index=True)\n            else:\n                break\n    dominat_topic_sent.columns = ['Domiant_topic', 'Percentage_contribution', 'Keywords']\n    \n    # Add original text to the end of the output\n    contents = pd.Series(text)\n    dominat_topic_sent = pd.concat([dominat_topic_sent, contents], axis=1)\n    return(dominat_topic_sent)","86926068":"df_topic_sents_keywords = dominat_topic_ident(ldamodel=optimal_model, corpus=corpus, text=meta_df_cp.abstract.values.tolist())\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Percentage_Contrib', 'Keywords', 'Text']","bbfeffe1":"# Show\ndf_dominant_topic.head(10)","2a168c2c":"import seaborn as sns\nimport matplotlib.colors as mcolors\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\nfig, axes = plt.subplots(2,2,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):    \n    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n    ax.hist(doc_lens, bins = 1000, color=cols[i])\n    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n    ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n    ax.set_ylabel('Number of Documents', color=cols[i])\n    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.90)\nplt.xticks(np.linspace(0,1000,9))\nfig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\nplt.show()","a4e17a13":"# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=26, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","5d1744ef":"import matplotlib.colors as mcolors\n# Get topic weights and dominant topics ------------\nfrom sklearn.manifold import TSNE\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\n\n# Get topic weights\ntopic_weights = []\nfor i, row_list in enumerate(lda_model[corpus]):\n    topic_weights.append([w for i, w in row_list[0]])\n\n# Array of topic weights    \narr = pd.DataFrame(topic_weights).fillna(0).values\n\n# Keep the well separated points (optional)\narr = arr[np.amax(arr, axis=1) > 0.35]\n\n# Dominant topic number in each doc\ntopic_num = np.argmax(arr, axis=1)\n\n# tSNE Dimension Reduction\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(arr)\n\n# Plot the Topic Clusters using Bokeh\noutput_notebook()\nn_topics = 26\nmycolors = np.array([color for name, color in mcolors.XKCD_COLORS.items()])\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n              plot_width=900, plot_height=700)\nplot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\nshow(plot)","2bf0e5ba":"def prediction_unseen_doc(lda_model, doc, threshold=0.1):\n    \"\"\"\n    Get the most representative topic of a new documment\n    \"\"\"\n    doc_preprocessed = doc.split()\n    doc_tokens = bigram_mod[doc_preprocessed]\n    bow_tokens = id2word.doc2bow(doc_tokens)\n\n    rows = []\n    for i, score in sorted(\n        lda_model.get_document_topics(bow_tokens), key=lambda x: x[1], reverse=True\n    ):\n        if score > threshold:\n            words, _ = zip(*lda_model.show_topic(i))\n            rows.append([f\"Topic_{i+1}\", score, \"; \".join(words)])\n            break\n\n    return pd.DataFrame(rows, columns=[\"Topic\", \"Score\", \"Words\"])","2cc89a31":"def document_same_topic(df_topic, df_dominant_topic, df_merged):\n    \"\"\"\n    Obtain documents that have the same topic as df_topic\n    \"\"\"\n\n    for index, row in df_topic.iterrows():\n        topic = int(row[\"Topic\"].split(\"_\")[-1])\n\n        doc_same_topic = list(\n            df_dominant_topic[df_dominant_topic[\"Dominant_Topic\"] == topic][\n                \"Document_No\"\n            ]\n        )\n\n        doc_detail = df_merged.loc[doc_same_topic]\n\n    return doc_detail","2778e400":"QUERY1 = '''\nTransmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n'''\nQUERY2 = '''\nSeverity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n'''\nQUERY3 = '''\nSusceptibility of populations\n         '''","fcc4922c":"# Preprocess the queries\nquery_1_preprocessed = review_preprocess(QUERY1)\nquery_2_preprocessed = review_preprocess(QUERY2)\nquery_3_preprocessed = review_preprocess(QUERY3)\n","d8888a38":"unseen_doc_q1 = prediction_unseen_doc(lda_model=optimal_model,doc=query_1_preprocessed)\nunseen_doc_same_topic_q1 = document_same_topic(unseen_doc_q1, df_dominant_topic, meta_df_cp).head(10)","168d4d0f":"unseen_doc_q1.style.background_gradient(cmap='viridis')","37a3bea7":"unseen_doc_same_topic_q1.head()","b8ea99f1":"import pyLDAvis.gensim\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(optimal_model, corpus, dictionary=optimal_model.id2word)\nvis","94c2e7b6":"# Creating a corpus object\n#corpus = Corpus()","42fa79f9":"# Fit the corpus with a list of tokens\n#corpus.fit(docs_tokens, window=10)","1ad6181c":"### Tokenize the words and clean the abstract read from the metacsv file","ff21bdb5":"> ***Identify the dominat topic for all the senetences we have in our dataset from the optimal LDA created ***","00b4210b":"**Clustering of topics **","abf15029":"*In the next step we are going to identiy the optimal number of topics this is computed using coherence value*","f4055555":"LDA","1f9cf94c":"# What do we know about COVID-19 risk factors?\n\n## COVID-19 Open Research Dataset Challenge (CORD-19)\n\n\n### Task Details\n\nWhat do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\n\nSpecifically, we want to know what the literature reports about:\n\n\n\t\n1. Data on potential risks factors\n\t\t\n    * Smoking, pre-existing pulmonary disease\n    * \t\tCo-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities\n    * \t\tNeonates and pregnant women\n    * \t\tSocio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n\t\n\t\n2. Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n    * \tSeverity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n    * \tSusceptibility of populations\n    * Public health mitigation measures that could be effective for control\n\n\n#### Pros\n*Word2Vec has several advantages over bag of words and IF-IDF scheme. Word2Vec retains the semantic meaning of different words in a document. The context information is not lost. Another great advantage of Word2Vec approach is that the size of the embedding vector is very small. Each dimension in the embedding vector contains information about one aspect of the word. We do not need huge sparse vectors, unlike the bag of words and TF-IDF approaches.*\n\n#### Cons\n*We still need to create a huge sparse matrix, which also takes a lot more computation than the simple bag of words approach*\n\n#### Approach\nOur approach has been to perfom a text analysis from the meta data file provided and identify the most important topics from the text and map all the json files or documents related with that topic \nFor example if we have a document in the meta file if identify the dominant topic that contribute highest in the document detection and relate it to all the json files that fall inline with that topic","6340e94c":"**Collaborator for this notebook**\n\n**Collaborator 1: Anamika Jha \nKaggle ID: anamikajha**\n\n**Collaborator 2: Pratishtha Kapoor\nKaggle ID: Pratishtha**\n\n**We woluld like to thank Affine Analytics for all there support and guidance provided in the completion of this chllenge**","76a91c96":"3.5.1 Train a Glove model","a70fce75":"![image.png](attachment:image.png)","8538c99b":"#### Gensim simple_preporcess method is being used to tokenize and clean data by removing stopwords and punctuation if any found in the abstract","6b19ec05":" ****Create a Dictionary and corpus which is required for topic modeliing****\n The main inpyts for topic modelling is the dictionary and the corpu "}}