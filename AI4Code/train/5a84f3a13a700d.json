{"cell_type":{"9a128d62":"code","885c493f":"code","2c66beb7":"code","202edfdb":"code","b66d0bbc":"code","328375b2":"code","c493ba46":"code","97c320d9":"code","515e7777":"code","6b53410e":"code","e1755efc":"code","e173f329":"markdown","5159104d":"markdown","5fa2c8ab":"markdown","eb259f5e":"markdown","c91ced47":"markdown","3a36ad6a":"markdown","a6c523e4":"markdown","69138697":"markdown","22e5be2b":"markdown","8b98d6f2":"markdown"},"source":{"9a128d62":"import torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nimport pickle\nimport numpy as np\nfrom skimage import io\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom PIL import Image\nfrom pathlib import Path\n\nfrom torchvision import transforms\nfrom multiprocessing.pool import ThreadPool\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nfrom matplotlib import colors, pyplot as plt\n%matplotlib inline\n\n# \u0432 sklearn \u043d\u0435 \u0432\u0441\u0435 \u0433\u043b\u0430\u0434\u043a\u043e, \u0447\u0442\u043e\u0431\u044b \u0432 colab \u0443\u0434\u043e\u0431\u043d\u043e \u0432\u044b\u0432\u043e\u0434\u0438\u0442\u044c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \n# \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0433\u043d\u043e\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c warnings\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)","885c493f":"from sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nclass SimpsonsDataset(Dataset):\n    def __init__(self, files, mode):\n        super().__init__()\n        # \u0441\u043f\u0438\u0441\u043e\u043a \u0444\u0430\u0439\u043b\u043e\u0432 \u0434\u043b\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438\n        self.files = sorted(files)\n        # \u0440\u0435\u0436\u0438\u043c \u0440\u0430\u0431\u043e\u0442\u044b\n        self.mode = mode\n        self.samples = []\n        self.len_ = len(self.files)\n        self.label_encoder = LabelEncoder()\n\n        if self.mode != 'test':\n            self.labels = [path.parent.name for path in self.files]\n            self.label_encoder.fit(self.labels)\n\n        with tqdm(desc=f\"Loading {self.mode} files\", total=self.len_) as pbar:\n            p = ThreadPool()\n            for sample in p.imap(self.load_sample, self.files):\n                self.samples.append(sample)\n                pbar.update()\n    \n    def __len__(self):\n        return self.len_\n\n      \n    def load_sample(self, file):\n        image = Image.open(file)\n        image.load()\n        return image\n\n    def __getitem__(self, index):\n        # \u0434\u043b\u044f \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u044b PyTorch \u0438 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0432\u0445\u043e\u0434\u0430\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n        ])\n        x = self.samples[index]\n        x = self._prepare_sample(x)\n        x = np.array(x \/ 255, dtype='float32')\n        x = transform(x)\n        if self.mode == 'test':\n            return x\n        else:\n            label = self.labels[index]\n            label_id = self.label_encoder.transform([label])\n            y = label_id.item()\n            return x, y\n        \n    def _prepare_sample(self, image):\n        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n        return np.array(image)","2c66beb7":"path = '..\/input\/train\/simpsons_dataset\/'","202edfdb":"from pathlib import Path\ntrain_val_files = sorted(list(Path(path).rglob('*.jpg')))","b66d0bbc":"len(train_val_files)","328375b2":"ds = SimpsonsDataset(files=train_val_files, mode='train')","c493ba46":"from sklearn.preprocessing import LabelEncoder\nimport PIL\nimport PIL.Image as Image\nclass SimpsonsDataset(Dataset):\n    def __init__(self, files, mode, transform):\n        assert mode in ['train', 'test', 'val']\n        self.files = sorted(files)\n        self.mode = mode\n        self.transform = transform\n        self.len_ = len(self.files)\n        \n        if self.mode !='test':\n            self.labels = [path.parent.name for path in self.files]\n            self.label_encoder = LabelEncoder()\n            self.label_encoder.fit(self.labels)\n        \n        \n    def __len__(self):\n        return self.len_\n    \n    @staticmethod\n    def load_img(fname):\n        img = Image.open(fname)\n        return img\n    \n    \n    def __getitem__(self, idx):\n        x = SimpsonsDataset.load_img(self.files[idx])\n        x = self.transform(x)\n        if self.mode == 'test':\n            return x\n        else:\n            y = self.labels[idx]\n            y = self.label_encoder.transform([y]).item()\n            return x, y","97c320d9":"ds = SimpsonsDataset(files=train_val_files, mode='train', transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n]))","515e7777":"trainset = SimpsonsDataset(files=train_val_files, mode='train', transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n]))","6b53410e":"trainloader = DataLoader(trainset, batch_size=32, shuffle=True)","e1755efc":"EPOCHS = 0\nfor epoch in range(EPOCHS):\n\n    for X_batch, y_batch in trainloader:\n        #X_batch \u0438 y_batch - \u0442\u0435\u043d\u0437\u043e\u0440\u044b\n        #\u0435\u0441\u043b\u0438 \u0432\u044b \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0435 \u043d\u0430 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0435, \u0442\u043e \u0438\u0445 \u0435\u0449\u0451 \u043d\u0443\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u0432 \u0432\u0438\u0434\u0435\u043e\u043f\u0430\u043c\u044f\u0442\u044c\n        # X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()","e173f329":"PS. \u043c\u043d\u043e\u0433\u0438\u0435 \u0434\u043e \u0441\u0438\u0445 \u043f\u043e\u0440 \u0441\u043f\u0440\u0430\u0448\u0438\u0432\u0430\u044e\u0442, \u043a\u0430\u043a \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\u043c\u0438 \u0438 \u0434\u0430\u0442\u0430\u043b\u043e\u0430\u0434\u0435\u0440\u0430\u043c\u0438, \u0447\u0442\u043e \u0441\u0442\u0440\u0430\u043d\u043d\u043e\n\n\u0434\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u043a\u043e\u043d\u0441\u0442\u0440\u0443\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442","5159104d":"\u043f\u0430\u043c\u044f\u0442\u0438 \u044d\u0442\u043e\u0442 \u043e\u0431\u044a\u0435\u043a\u0442 \u0437\u0430\u043d\u0438\u043c\u0430\u0435\u0442 \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043c\u0435\u043d\u044c\u0448\u0435, \u0432\u0435\u0434\u044c \u043c\u044b \u0445\u0440\u0430\u043d\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0443\u0442\u0438 \u043a \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c, \u0430 \u043d\u0435 \u0441\u0430\u043c\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f","5fa2c8ab":"\u043c\u043d\u043e\u0433\u0438\u0435 \u0436\u0430\u043b\u0443\u044e\u0442\u0441\u044f \u043d\u0430 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0441 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u043e\u0439 \u0434\u0430\u043d\u043d\u044b\u0445 - \u043f\u0440\u0438 \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0437\u0430\u043a\u0430\u043d\u0447\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043f\u0430\u043c\u044f\u0442\u044c\n\n\u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b - \u043d\u0435\u0432\u0435\u0440\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0435 \u0434\u0430\u043d\u043d\u044b\u0445","eb259f5e":"\u0437\u0430\u0442\u0435\u043c \u043c\u044b \u043f\u0440\u043e\u0441\u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u044d\u0442\u043e\u0442 \u043b\u043e\u0430\u0434\u0435\u0440, \u0438\u0442\u0435\u0440\u0438\u0440\u0443\u044f\u0441\u044c \u043f\u043e \u043d\u0435\u043c\u0443","c91ced47":"\u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u043a\u0430\u043a \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u044e\u0442\u0441\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432 \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435-\u0431\u0435\u0439\u0437\u043b\u0430\u0439\u043d\u0435\n\n\u043a\u043b\u0430\u0441\u0441 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0442\u0430\u043c \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u0442\u0430\u043a","3a36ad6a":"\u0442\u0443\u0442 \u043c\u044b \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432 \u043f\u0430\u043c\u044f\u0442\u044c\n\n\u043f\u0440\u043e\u0431\u0435\u043c\u0430 \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e \u043c\u044b \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0438\u0445 \u0432\u0441\u0435 \u0441\u0440\u0430\u0437\u0443 - \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 jpg \u0432\u0435\u0441\u044f\u0442 \u0443\u0436\u0435 \u0431\u043e\u043b\u0435\u0435 500 \u043c\u0431. \u0412 \u043f\u0430\u043c\u044f\u0442\u0438 \u044d\u0442\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441\u0443\u0434\u044f \u043f\u043e \u0432\u0441\u0435\u043c\u0443 \u0445\u0440\u0430\u043d\u044f\u0442\u0441\u044f \u0432 \u043d\u0435\u0441\u0436\u0430\u0442\u043e\u043c \u0432\u0438\u0434\u0435.","a6c523e4":"---","69138697":"\u043f\u0440\u0438\u0447\u0438\u043d\u0430, \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0437\u0430\u043a\u0430\u043d\u0447\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043f\u0430\u043c\u044f\u0442\u044c - \u0432\u043e\u0442 \u0432 \u044d\u0442\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u043a\u043e\u0434\u0430\n\n> with tqdm(desc=f\"Loading {self.mode} files\", total=self.len_) as pbar:\n\n>             p = ThreadPool()\n\n>             for sample in p.imap(self.load_sample, self.files):\n\n>                 self.samples.append(sample)\n\n>                 pbar.update()","22e5be2b":"\u0441\u0438\u0441\u0442\u0435\u043c\u0430 \u0441\u0435\u0441\u0441\u0438\u0439 kaggle \u043a\u0440\u0438\u0447\u0438\u0442 \u0432 \u0430\u0433\u043e\u043d\u0438\u0438 \u0438 \u0441\u043e\u043e\u0431\u0449\u0430\u0435\u0442, \u0447\u0442\u043e \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u0437\u0430\u043d\u044f\u0442\u043e 15.6 \u0413\u0411 \u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u043f\u0430\u043c\u044f\u0442\u0438 (\u0447\u0442\u043e \u044f\u0432\u043d\u043e \u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e)\n\n\u0447\u0442\u043e\u0431\u044b \u0440\u0435\u0448\u0438\u0442\u044c \u044d\u0442\u0443 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443, \u044f \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e \u043f\u043e\u0441\u0442\u0443\u043f\u0430\u0442\u044c \u0441 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438 \u0438\u043d\u0430\u0447\u0435 - \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0442\u044c \u0438\u0445 \u043d\u0435 \u0432\u0441\u0435 \u0441\u0440\u0430\u0437\u0443, \u0430 \u0432\u043c\u0435\u0441\u0442\u043e \u044d\u0442\u043e\u0433\u043e \u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0443\u0442\u0438 \u043a \u0444\u0430\u0439\u043b\u0430\u043c, \u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0442\u044c \u043f\u043e \u043c\u0435\u0440\u0435 \u043e\u0431\u0440\u0430\u0449\u0435\u043d\u0438\u044f, \u043f\u043e \u043e\u0434\u043d\u043e\u043c\u0443\n\n\u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e \u0432\u0430\u043c \u0441\u0432\u043e\u044e \u0432\u0435\u0440\u0441\u0438\u044e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430","8b98d6f2":"\u043d\u0430 \u044d\u0442\u043e\u043c \u044d\u0442\u0430\u043f\u0435 \u043c\u044b \u0438\u043c\u0435\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442, \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u044b\u0439 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438 (\u0435\u0441\u043b\u0438 \u043d\u0443\u0436\u043d\u043e) \u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n\n\u0442\u0435\u043f\u0435\u0440\u044c \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u043e\u0431\u0435\u0440\u043d\u0443\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0432 \u0434\u0430\u0442\u0430\u043b\u043e\u0430\u0434\u0435\u0440, \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u044b\u0439 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c \u0433\u043e\u0442\u043e\u0432\u044b\u0435 \u0431\u0430\u0442\u0447\u0438"}}