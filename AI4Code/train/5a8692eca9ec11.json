{"cell_type":{"f4be6c55":"code","31dbeeef":"code","858ca1cb":"code","c34d09f9":"code","33cef5ff":"code","fce51c1e":"code","bc55a8db":"code","fc399a51":"code","792ad25e":"code","eca57c0e":"code","d33bb894":"code","5ab5651a":"code","f48ab767":"code","13da2547":"code","b35954aa":"code","ea0c1b84":"code","9e2711b7":"code","423e1503":"code","22eb3ffa":"code","1536f04c":"code","76406165":"code","c6d454df":"code","fa62d090":"code","f214a6cd":"code","429a2d9d":"code","95694f24":"code","2bade6b6":"code","443fd833":"code","e8ce871c":"code","02af6940":"code","ec9b8bc7":"code","abb3027f":"code","7f778105":"code","2a2ba5ea":"code","71a10d73":"code","af3c7ef2":"code","6ee171b9":"code","6a73e7b9":"code","342b4398":"code","36dd65b9":"code","10be07f5":"code","60a23aae":"code","a0f89780":"code","d38972e0":"code","5cbce537":"code","88a2717d":"code","1d3afd89":"code","eb618aba":"code","be71a6a8":"code","2246b1aa":"code","b689f975":"code","b7fb62a3":"code","7f28a74b":"code","625e81a8":"markdown","b04293a0":"markdown","390a4a61":"markdown","5e87d2f3":"markdown","1868f557":"markdown","79d93760":"markdown","09a45ad5":"markdown","4f0fc923":"markdown","ca24a2e1":"markdown","b274c9cb":"markdown","d0ef2c5d":"markdown","b91cb813":"markdown","faabcf28":"markdown","c817f56a":"markdown","f952bada":"markdown","0487df4a":"markdown","2ec12b91":"markdown","e3641182":"markdown","0211981c":"markdown","65f8ccc3":"markdown","5bc0cfb8":"markdown","ec146713":"markdown","4d23e787":"markdown","d8c440a0":"markdown","195dc975":"markdown","92339dd0":"markdown","6916a934":"markdown","490b080d":"markdown","16e04cfb":"markdown","e181f963":"markdown","7d063d45":"markdown","89c0efb5":"markdown","a1650b14":"markdown","10a5ce26":"markdown","d1bad4ff":"markdown","2c5df9fb":"markdown","fa6c02b7":"markdown","f6518b52":"markdown","a8efdff2":"markdown","2e51f52d":"markdown","84589151":"markdown"},"source":{"f4be6c55":"%%time\n# times the whole cell\n\n# import pandas\nimport pandas as pd\n\n# set the path to read df_raw\npath = '..\/input\/df_raw'\n\n# read the data into a pandas DataFrame\ndf_raw = pd.read_feather(path)","31dbeeef":"# import proc_df and the functions it depends on\nfrom fastai.structured import numericalize, fix_missing, proc_df\n\nX, y, nas_dict = proc_df(df_raw, 'SalePrice')","858ca1cb":"# create a function for splitting X and y into train and test sets of customizable sizes\ndef split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\n# validation set size: 12000.\nvalidation = 12000\n\n# split point: length of dataset minus validation set size.\nsplit_point = len(X)-validation\n\n# split X\nX_train, X_valid = split_vals(X, split_point)\n\n# split y\ny_train, y_valid = split_vals(y, split_point)\n\n# dimensions (row, columns) of X_train, y_train and X_valid\nX_train.shape, y_train.shape, X_valid.shape","c34d09f9":"# import numpy\nimport numpy as np\n\n# create a function that takes the RMSE\ndef rmse(pred,known): return np.sqrt(((pred-known)**2).mean())\n\n# create a function that rounds to 5 decimal places (like kaggle leaderboard)\ndef rounded(value): return np.round(value, 5)\n\n# create a function that prints a list of 4 scores, rounded:\n# [RMSE of X_train, RMSE of X_valid, R Squared of X_train, R Squared of X_valid]\ndef print_scores(model):\n    RMSE_train = rmse(model.predict(X_train), y_train)\n    RMSE_valid = rmse(model.predict(X_valid), y_valid)\n    R2_train = model.score(X_train, y_train)\n    R2_valid = model.score(X_valid, y_valid)\n    scores = [rounded(RMSE_train), rounded(RMSE_valid), rounded(R2_train), rounded(R2_valid)]\n    if hasattr(m, 'oob_score_'): scores.append(m.oob_score_) # appends OOB score (if any) to the list \n    print(scores)","33cef5ff":"# print 5 first rows\ndf_raw.head()","fce51c1e":"# import set_rf_samples\nfrom fastai.structured import set_rf_samples\n\n# set random subsample size to 50,000 rows\nset_rf_samples(50000)","bc55a8db":"# import the class\nfrom sklearn.ensemble import RandomForestRegressor\n\n# instantiate the model with the parameters of the last lesson\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True,\n                          random_state=17190)\n\n# fit the model with data and calculate the running time\n%time m.fit(X_train, y_train)\n\n# [RMSE of X_train, RMSE of X_valid, R Squared of X_train, R Squared of X_valid, OOB score]\nprint_scores(m)","fc399a51":"# use a list comprehension to loop through the random forest and concatenates the predictions of each individual tree on a new axis\npreds = np.stack([tree.predict(X_valid) for tree in m.estimators_])\n\n# dimensions (rows, columns)\npreds.shape","792ad25e":"# split df_raw and conserve the part of the validation set\n_, raw_valid = split_vals(df_raw, split_point)\n\n# make a copy\nvalidation = raw_valid.copy()\n\n# add new column: calculated standard deviation over row axis\nvalidation['pred_std'] = np.std(preds, axis=0)\n\n# add new column: calculated mean over row axis\nvalidation['pred'] = np.mean(preds, axis=0)","eca57c0e":"# plots the counts of the unique values of YearMade in the validation set\nvalidation.YearMade.value_counts(dropna=False).plot.bar(figsize=(15,4))","d33bb894":"# list of selected columns\ncolumns = ['YearMade', 'SalePrice', 'pred', 'pred_std']\n\n# dataframe of selected columns with the rows grouped by the values in YearMade [index 0]\n# and with the calculated mean of 'SalePrice', 'pred', 'pred_std'\nyear = validation[columns].groupby(columns[0]).mean()\n\n# dimensions (rows, columns)\nprint(year.shape)\n\n# 10 first rows sorted descendingly\nyear.sort_values(by=['pred_std'],ascending=False).head(10)","5ab5651a":"# plots the counts of the unique values of ProductSize in the validation set\nvalidation.ProductSize.value_counts(dropna=False).plot.barh()","f48ab767":"# list of selected columns\ncolumns = ['ProductSize', 'SalePrice', 'pred', 'pred_std']\n\n# dataframe of selected columns with the rows grouped by the values in ProductSize [index 0]\n# and with the calculated mean of 'SalePrice', 'pred', 'pred_std'\nsize = validation[columns].groupby(columns[0]).mean()\n\n# calculates the ratio between mean standard deviation and mean prediction\n(size.pred_std\/size.pred).sort_values(ascending=False)","13da2547":"# import rf_feat_importance\nfrom fastai.structured import rf_feat_importance\n\n# create a dataframe of the feature importance by passing the model and the training set\nfi = rf_feat_importance(m, X_train)\n\n# dimension (rows, columns)\nprint(fi.shape)\n\n# first 5 rows\nfi.head()","b35954aa":"# plot the feature importance of the column names in 'cols'\nfi.plot('cols', 'imp', 'bar', figsize=(15,4))","ea0c1b84":"# create a Series with the column names in 'cols' with a greater 'imp' value than 0.005\nto_keep = fi[fi.imp>0.005].cols\n\n# dimensions (rows,)\nlen(to_keep)","9e2711b7":"# plot the feature importance of the first 10 columns\nfi[:10].plot('cols', 'imp', 'barh', figsize=(15,7))","423e1503":"# create a dataframe with the selected columns\nX_keep = X[to_keep].copy()\n\n# split X\nX_train, X_valid = split_vals(X_keep, split_point)","22eb3ffa":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_scores(m)","1536f04c":"# create a dataframe of the feature importance\nfi = rf_feat_importance(m, X_train)\n\n# plot the feature importance of the first 10 columns \nfi[:10].plot('cols', 'imp', 'barh', figsize=(15,7))","76406165":"# dtypes in the DataFrame\nprint(df_raw[['YearMade','ProductSize','Coupler_System','fiProductClassDesc']].dtypes)\n\n# generate descriptive statistics of all columns\ndf_raw[['YearMade','ProductSize','Coupler_System','fiProductClassDesc']].describe(include='all')","c6d454df":"X_one_hot, _, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(X_one_hot, split_point)\n\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_scores(m)","fa62d090":"# create a dataframe of the feature importance\nfi = rf_feat_importance(m, X_train)\n\n# plot the feature importance of the first 10 columns\nfi[:10].plot('cols', 'imp', 'barh', figsize=(15,7))","f214a6cd":"# import stats\nfrom scipy import stats\n\n# create distance matrix\nmatrix = stats.spearmanr(X_keep).correlation\n\n# show distance matrix as a dataframe for demostration\npd.DataFrame(matrix)","429a2d9d":"# import average linkage\nfrom scipy.cluster.hierarchy import average\n\n# create linkage matrix\nlinkage = average(matrix)\n\n# show linkage matrix as a dataframe for demostration\npd.DataFrame(linkage)","95694f24":"# import dendrogram\nfrom scipy.cluster.hierarchy import dendrogram\n\n# import matplotlib\nimport matplotlib.pyplot as plt\n\n# set the size of the figure (plot container)\nplt.figure(figsize=(15,7))\n\n# plot dendrogram (inside figure)\ndendrogram(linkage, labels=X_keep.columns, orientation='left', leaf_font_size=16)\n\n# display figure\nplt.show()","2bade6b6":"# create a function that takes a dataframe as argument and returns the oob_score of a random forest trained on that fataframe\ndef get_oob(dataframe):\n    m = RandomForestRegressor(n_estimators=30, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n    X, _ = split_vals(dataframe, split_point)\n    m.fit(X, y_train)\n    return m.oob_score_","443fd833":"# baseline to compare to\nget_oob(X_keep)","e8ce871c":"# loop through the selected columns and print the oob_score with that column removed from the dataframe\nfor column in ('saleYear', 'saleElapsed',\n               'ProductGroup' ,'ProductGroupDesc',\n               'fiBaseModel','fiModelDesc',\n               'Grouser_Tracks', 'Coupler_System', 'Hydraulics_Flow'):\n    print(column, get_oob(X_keep.drop(column, axis=1)))","02af6940":"# list of columns names\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Hydraulics_Flow']\n\n# returns oob_score with the selected columns removed\nget_oob(X_keep.drop(to_drop, axis=1))","ec9b8bc7":"# drop inplace the selected columns from X_keep\nX_keep.drop(to_drop, axis=1, inplace=True)\n\n# split X_keep\nX_train, X_valid = split_vals(X_keep, split_point)","abb3027f":"# import reset_rf_samples\nfrom fastai.structured import reset_rf_samples\n\n# use full bootstrap sample\nreset_rf_samples()","7f778105":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_scores(m)","2a2ba5ea":"set_rf_samples(50000)","71a10d73":"X_one_hot, _, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(X_one_hot, split_point)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(X_train, y_train)","af3c7ef2":"fi = rf_feat_importance(m, X_train)\nfi[:10].plot('cols', 'imp', 'barh', figsize=(15,4))","6ee171b9":"# import get_sample\nfrom fastai.structured import get_sample\n\n# random sample of 1000 rows from X_train\nX_sample = get_sample(X_train, 1000)","6a73e7b9":"# import partial dependence calculator and plotter\nfrom pdpbox.pdp import pdp_isolate, pdp_plot\n\n# list of features names\nfeatures = ['Enclosure_EROPS', 'Enclosure_EROPS AC', 'Enclosure_EROPS w AC', 'Enclosure_NO ROPS', 'Enclosure_None or Unspecified', 'Enclosure_OROPS']\n\n# calculate partial dependence plot (model, dataframe, dataframe.columns, 'feature') \npdp = pdp_isolate(m, X_sample, X_sample.columns, features)\n\n# plot partial dependent plot (pdp_isolate, 'name')\npdp_plot(pdp, 'Enclosure')","342b4398":"pd.set_option('display.max_columns', None)\nX_train[X_train.YearMade == 1000].head()","36dd65b9":"# pick only rows that have a YearMade value larger than 1000\nX_sample = get_sample(X_train[X_train.YearMade>1000], 1000)\n\n# calculate partial dependence plot (model, dataframe, dataframe.columns, 'feature') \npdp = pdp_isolate(m, X_sample,X_sample.columns,'YearMade')\n\n# plot partial dependent plot (pdp_isolate, 'name')\npdp_plot(pdp, 'YearMade')","10be07f5":"# make a copy\nX = X_keep.copy()\n\n# create a new target column (empty)\nX['in_validation_set'] = None\n\n# set rows in the training set to False (up to last 12000 rows)\nX.in_validation_set[:split_point] = False\n\n# set rows in the validation set to True (last 12000 rows)\nX.in_validation_set[split_point:] = True\n\n# split X, y\nX, y, nas = proc_df(X, 'in_validation_set')","60a23aae":"# import the class\nfrom sklearn.ensemble import RandomForestClassifier\n\nm = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X, y)\nm.oob_score_ # if the validation set was random (not time dependent), this experiment would not work and the oob error would be bad (far from 1).","a0f89780":"# create a dataframe of the feature importance\nfi = rf_feat_importance(m, X)\n\n# plot the feature importance of first 5 columns\nfi.head().plot('cols', 'imp', 'barh')","d38972e0":"X['SalesID'].plot()","5cbce537":"X['saleElapsed'].plot()","88a2717d":"X['MachineID'].plot()","1d3afd89":"# baseline to compare to\n\nX_train, X_valid = split_vals(X_keep, split_point)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_scores(m)","eb618aba":"# loop through the selected columns and print the scores of a random forest trained with that column removed from the training set.\nfor column in ('SalesID', 'saleElapsed', 'MachineID'):\n    X_train, X_valid = split_vals(X_keep.drop(column, axis=1), split_point)\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(X_train, y_train)\n    print(column)\n    print_scores(m)","be71a6a8":"# drop the columns from the dataframe and split X\nX_train, X_valid = split_vals(X_keep.drop(['SalesID', 'MachineID'], axis=1), split_point)\n\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_scores(m)","2246b1aa":"# use full bootstrap sample\nreset_rf_samples()","b689f975":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_scores(m)","b7fb62a3":"# plot the feature importance of the column names in 'cols'\nrf_feat_importance(m, X_train).plot('cols', 'imp', 'barh', figsize=(12,7))","7f28a74b":"m = RandomForestRegressor(n_estimators=160, max_features=0.5, n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_scores(m)","625e81a8":"# Lecture 5\nIt is recommended to [watch the video-lecture first](https:\/\/youtu.be\/3jl2h9hSRvc), then follow the notebook.","b04293a0":"# Extrapolation","390a4a61":"# Final model","5e87d2f3":"#  Intro to Machine Learning\nThis is a free course offered by [fast.ai](http:\/\/www.fast.ai\/) (currently [unlisted](http:\/\/forums.fast.ai\/t\/another-treat-early-access-to-intro-to-machine-learning-videos\/6826)). There's a github [repository](https:\/\/github.com\/fastai\/fastai\/tree\/master\/courses\/ml1).\n\n## About this course\nSome machine learning courses can leave you confused by the enormous range of techniques shown and can make it difficult to have a practical understanding of how to apply them.\n\nThe good news is that modern machine learning can be distilled down to a couple of key techniques that are of very wide applicability. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods:\n\n- **Ensembles of decision trees** (i.e. Random Forests and Gradient Boosting Machines), mainly for **structured data** (such as you might find in a database table at most companies)\n- **Multi-layered neural networks learnt with Stochastic Gradient Descent** (SGD) (i.e. shallow and\/or deep learning), mainly for **unstructured data** (such as audio, vision, and natural language)\n\n### The lessons\nIn this course we'll be learning about:\n- **Random Forests** \n- **Stochastic Gradient Descent**.\n- **Gradient Boosting** \n- **Deep Learning**\n\n### The dataset\nWe will be teaching the course using the [Blue Book for Bulldozers Kaggle Competition](https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers): \n- \"The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuration. The data is sourced from auction result postings and includes information on usage and equipment configurations.\"\n\n### Note:\nThese are personal notes. For the original code, check the github repository of the course. Also, I will be importing things as I need them.\n","1868f557":"# Removing redundant features","79d93760":"## Load the data\nWe'll be loading our feather file from the [last lesson](https:\/\/www.kaggle.com\/ailobe\/fastai-ml1-lesson1-rf\/output).\n- feather-format is really **fast**.","09a45ad5":"More important that a slight decrease or increase in accuracy are the **insights gathered** through this approach.\n- Now we know which **categorical values** are the most important and we'll use this information in further analysis when we look at partial dependence.","4f0fc923":"Removing any of these columns doesn't affect drastically the accuracy of the model, so it looks like we can try to remove one from each group. Let's see what that does.","ca24a2e1":"# Random Forest Model interpretation","b274c9cb":"# Lecture 4\nIt is recommended to [watch the video-lecture first](https:\/\/youtu.be\/0v93qHDqq_g), then follow the notebook.","d0ef2c5d":"It seems that our model is almost as good as before, although now it's a simpler model.\n- Previous score:\n- Let's **use this dataframe** from here. \n\nNow we'll see if the feature importances have changed after dropping redundant columns from the training set.","b91cb813":"There are **40** sets of predictions (**trees**) with **12,000** values (**predictions**), which corresponds to the size of the validation set.\n\nNow we can **calculate the mean and the standard deviation of the predictions**, and more interestingly, we can **add that information to the dataset**.\n- We'll use the **df_raw**, because after `proc_df`, the dataset is all numbers and we want to be able to interpret its values.\n- We need to **split it again** because we are only interested in the rows of the **validation set**.\n- We'll **add two new columns** to the dataset: the **predictions** and the **standard deviation** of the predictions.","faabcf28":"Out of 55 possible values, these are the YearMade values that have the rows with more standard deviation in the prediction (so are less accurate).\n- Except for YearMade 1000, all the other rows have very **little unique value counts**.","c817f56a":"Now we can create a **new dataset** that contain only the **selected columns**.\n- We'll need to **split** the dataframe again into separate training and validation sets.\n\nBefore fitting our model with the new training set we'll plot one more time the feature importance.\n- Chances are that those values will vary when we rerun `rf_feat_importance` on the newly trained model.\n- We' plot only the 10 most important columns so it's easier to see the changes happening there.","f952bada":"We'll plot partial dependence plots to know more about Enclosure and YearMade.","0487df4a":"Clearly there is a trend in the validation set that the model has learned which suggest that these variables have an underlying temporal component.\n- Let's try to remove them one at a time and see how the model perfoms without them.","2ec12b91":"Considering that has rows with Enclosure_EROPS w AC values, it can't be a category for machines so old that don't have any record.\n- It seems more likely to be a marker for missing values.\n- We'll just ignore it in the plot.","e3641182":"It seems we did a good job forcing the model to use other variables instead of the time related ones.\n- Let's try the model on the **full dataset**.","0211981c":"The ProductSize values that have the rows with more standard deviation in the prediction are: Large, Compact and Small, and they are also the smallest groups in the validation set.\n- In general, the random forest does a less good job predicting **small groups of data**.\n\n## Feature importance\nFeature importance tells us **which columns affect the predictions  the most**.\n-  We'll use a method from the **fastai library** called **`rf_feat_importance`** that uses the `feature_importances_` attribute from the [RandomForestRegressor](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html) to return a dataframe with the columns and their importance in descending order.\n- It's an **easy way** to know which features are the most important. There are [other methods](http:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html) to do it.","65f8ccc3":"# Tree interpreter","5bc0cfb8":"## Confidence based on tree variance\nTo give us a sense of **relative confidence** of predictions, we can use the **standard deviation** of the predictions.\n- The standard deviation is a measure that quantifies the **amount of variation** of a set of values ([source](https:\/\/en.wikipedia.org\/wiki\/Standard_deviation)).\n - A **low** standard deviation indicates that the data points tend to be **close to the mean**.\n - A **high** standard deviation indicates that the data points are spread out over a **wider range of values**. \n- We want to be more **cautious** with rows that have a higher standard deviation because they make our model give **inconsistent predictions**.\n\nWe'll create a function to store **the predictions for each individual tree**.","ec146713":"# Partial dependence\nTo investigate the interactions between the features and the target, we can use graphical tools like [partial dependence plots](https:\/\/www.kaggle.com\/dansbecker\/partial-dependence-plots).\n- Partial dependence plots show **how** each **variable affects** the model's **predictions**.\n- We won't use the whole dataframe for the partial dependence plots because we don't need that much information.\n - We'll use a method from the fastai library, **`get_sample`**, to \"get a random sample of n rows from df, without replacement\".\n- We'll use 1-hot encoding (`max_n_cat` parameter in `proc_df`) and subsampling (`set_rf_samples`).\n\nWe've [added a custom package](https:\/\/www.kaggle.com\/docs\/kernels#modifying-the-default-environment) to the kernel enviroment, **pdpbox**, because it makes tha task easier, but there are [other ways](https:\/\/www.kaggle.com\/dansbecker\/partial-dependence-plots) to create partial dependence plots.","4d23e787":"Now we can use that information, but to **what columns** do we have **to pay attention**?\n- We could try with the features that got selected in our **single small deterministic tree** from the [last lesson](https:\/\/www.kaggle.com\/ailobe\/fastai-ml1-lesson1-rf):\n - ProductSize, YearMade, fiSecondaryDesc, Hydraulics_Flow and ModelID.\n\nConsidering we are dealing with pieces of **heavy equipment**,  it's probably safe to assume that the **year** of **manufacture** and the **size** of the **machine** might be the most important features out of the selected in deciding the prize.\n- So let's take a look at YearMade and ProductSize.","d8c440a0":"It seems that our model is almost as good as before, although now it's a simpler model.\n- Let's **use this dataframe** from here. ","195dc975":"When **redundant columns** are **removed** the **relationships** between those redundant columns and the most **informative columns** are also are removed .\n- Those relationships (**collinearity**) between columns can have an effect on the **feature importance** of some of the columns. \n\nThe are 4 columns that stand out from the rest in terms of its feature importance. Let's take a closer look at them.","92339dd0":"And let's see how this model looks on the **full dataset**.","6916a934":"To know more about the following pieces of code, check the [first lesson](https:\/\/www.kaggle.com\/ailobe\/fastai-ml1-lesson1-rf).\n- We'll use a method from the fastai library , **`proc_df`**,  to get the dataset ready for the random forest.\n- We'll create the function **`split_vals`** to split the dataset into training and validation sets.\n- We'll create the function **`print_scores`** to evaluate our model.","490b080d":"Random Forest **do not extrapolate**, wich basically means they cannot predict future trends. \n-  \"In a general sense, to extrapolate is to infer something that is not explicitly stated from existing information\" ([source](https:\/\/whatis.techtarget.com\/definition\/extrapolation-and-interpolation)).\n- Unlike linear models, random forests will never be able to predict values bigger or smaller than the max and min in the training data ([source](https:\/\/www.quantopian.com\/posts\/random-forest-unable-to-predict-outside-of-training-data)).\n- That means random forests are limited to short-range prediction, assuming the new data won't be much different from the original training data (sources: [1](http:\/\/www.breakingbayes.com\/2017\/05\/30\/detemporalized-random-forest-time-series-modeling\/), [2](http:\/\/freerangestats.info\/blog\/2016\/12\/10\/extrapolation)).\n\nLuckily, we are making **short-range predictions**, but even there extrapolation can be problematic.\n- One way to improve the ability of our model of predicting future prices is to try to detemporalize the training set.\n- That is to **avoid using time related variables** as predictors in order to force the model to find other variables that have a strong relationship with the oucome and that might work better when trying to predict the future.\n\nWe can use random forest **interpretation** to figure out the best way to **detemporalize** the training set.\n- We'll build a model that is the **opposite** of what we want: it will rely heavily on time related variables to make predictions.\n- Next, we'll use what it has learned as a **guide** of which variables not to use.\n- The way we do it is by performing a little **experiment**:\n - instead of training the model to predict the **price** (regression problem),\n - we are going to train it to predict if a given row can be found or not **in the validation set** (classification problem).\n- Since our validation set is new data in the near future, the model will need to learn which variables are more likely to appear in the **future**.\n- Essentially, this will force the model to recognize **temporal patterns** in the data that may be hidden to us.\n- Next, we'll use **feature importance** to know which variables have a stronger temporal component.","16e04cfb":"## Subsampling\nWhen we are **interpreting our model**, we are not striving for prediction accuracy but for insights into the data.\n- We want a model that indicates the nature of the **relationships betwen features**.\n- We want it to be **reliable enough** so we can trust our interpetratons of its results.\n- There's **no need** to use the **full dataset** on each tree if we just want to get an accurate enough random forest.\n- Using a **subset** will be faster and will allow us to do more iterations.\n- We should use a **big enough subsample** to get similar results each time we run the same analysis.\n","e181f963":"We can see clearly that 'Enclosure_EROPS w AC' drives the prices up.\n- Now maybe the time to know [what that term means](http:\/\/articles.extension.org\/pages\/66325\/rollover-protective-structures).\n- We'll discover that EROPS means that the machine has an **enclosed cabin** and that makes it possible to have heating and **air conditioning**.\n\nBefore we look at YearMade, we need to fix the value YearMade 1000 that we saw earlier so it doesn't mess up with the plot.","7d063d45":"We can use cluster analysis to better understand the relationships between variables. Clustering essentially means grouping objects by similarity in groups called clusters.\n\nWe'll use **hierarchical clustering** (also called agglomerative clustering) which is a method of cluster analysis that builds **nested clusters** represented as a tree diagram (or dendrogram).\n- We'll use the [scipy's hierarchical clustering](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/cluster.hierarchy.html): **`scipy.cluster.hierarchy`**. \n- All that is used is a **matrix** of distances, that reflects cluster similarity, and a **method** to calculate the distances between clusters which produces a **linkage matrix**.\n- To create a matrix of distances we need a **distance metric** or correlation coefficient.\n- We'll use Spearman correlation coefficient because is a **nonparametric** metric, apropiate for our model (decision trees are nonparametric).\n - We'll use the [scipy's Spearman correlation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.spearmanr.html): **`scipy.stats.spearmanr`**.\n- The [method](https:\/\/en.wikipedia.org\/wiki\/UPGMA) used to perform the clustering it's called [average linkage](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.cluster.hierarchy.average.html#scipy.cluster.hierarchy.average): **`scipy.cluster.hierarchy.average`**.\n- Finally, we'll plot the [dendrogram](https:\/\/docs.scipy.org\/doc\/scipy-0.14.0\/reference\/generated\/scipy.cluster.hierarchy.dendrogram.html): **`scipy.cluster.hierarchy.dendrogram`**.\n\nSources: [1](https:\/\/en.wikipedia.org\/wiki\/Cluster_analysis), [2](https:\/\/en.wikipedia.org\/wiki\/Hierarchical_clustering), [3](https:\/\/joernhees.de\/blog\/2015\/08\/26\/scipy-hierarchical-clustering-and-dendrogram-tutorial\/).","89c0efb5":"Here the trend is also clear. The newer the machine, the more expensive it is.\n- Obvious if we consider that  'Enclosure_EROPS w AC' category won't comprise the older machines.","a1650b14":"Let's drop the ones that improved our score and compare it one more time.","10a5ce26":"There are **4 groups** of very similar variables (the numbers measure dissimiliraty) compared to the rest, so there's a chance some of them might be redundant:\n- saleYear and saleElapsed\n- ProductGroup and ProductGroupDesc\n- fiBaseModel and fiModelDesc\n- Grouser_Tracks, Coupler_System and Hydraulics_Flow\n\nLet's try removing some of these related features to see if the model can be simplified without impacting the accuracy.\n- We'll create a function that give us the only the oob_score and we'll remove each variables one at a time to see what impacts does it make.","d1bad4ff":"Let's take a look at these columns.\n- SalesID: \"unique identifier of a particular sale of a machine at auction\".\n- saleElapsed:  It's a [unix timestamp](https:\/\/en.wikipedia.org\/wiki\/Unix_time) (time elapsed in seconds since a universal point in the past).\n - column created by add_datepart method from original column saledate:  \"time of sale\".\n- MachineID: \"identifier for a particular machine;  machines may have multiple sales\".","2c5df9fb":"# Lecture 3\nIt is recommended to [watch the lecture first](https:\/\/youtu.be\/YSFG_W8JxBo?), then follow the notebook.\n- The lesson material starts at [51:07](https:\/\/youtu.be\/YSFG_W8JxBo?t=51m7s) because the time before that is spent talking about [another kaggle competition](https:\/\/www.kaggle.com\/c\/favorita-grocery-sales-forecasting) and [how to deal with really large datasets](https:\/\/www.kaggle.com\/jagangupta\/memory-optimization-and-eda-on-entire-dataset).","fa6c02b7":"## One-hot encoding\nOne-hot encoding, like dummy encoding for lineal models, works by recoding all the categorical values in **different columns** with **zeroes** and **ones**. The difference is that one-hot encoding will create n number columns and dummy encoding will create n-1 ([source](https:\/\/stats.stackexchange.com\/questions\/224051\/one-hot-vs-dummy-encoding-in-scikit-learn)).\n- If new binary variables are added for each unique categorical value, it takes the random forest only **one step to decide** if it's the best split point.\n- We can use the optional parameter `max_n_cat` in **`proc_df`**, to do **selective one-hot encoding**.\n- It will turn categorical variables into new columns only if their number of categories is **less or equal to chosen value**.\n- Considering the three categorical variables we are interested have 6, 2 and 74 unique values, we might want set **`max_n_cat` to 7** so these two columns are included.\n\nNow some of these new columns may prove to have more important features than in the earlier situation, where all categories were in one column.\n- We'll use `proc_df` with the new parameter and split the dataframe again.","f6518b52":"Now that we know that 3 of the 4 most informative columns are categorical variables, we might want to make some modifications so our model can use the information in this categories a little better.\nThe way the categories are encoded right now, there are lots of possible values encoded in the same column.\n- That has worked fine until now, but we can try a different approach so it doesn't take the random forest so many decisions to find valuable insights.","a8efdff2":"Now that we found the best settings for our model, let's train it one last time using lots of trees.\n\nWe cannot submit to the [leaderboard](https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/leaderboard) anymore because it's and old competition, but we can compare our RMSLE score of the validation set.","2e51f52d":"I've been unable to add a second custom package to the kernel enviroment so I'll **skip this section** for now. \n- [Tree interpreter](https:\/\/github.com\/andosa\/treeinterpreter) is a package for interpreting scikit-learn's decision tree and random forest predictions.\n- Allows decomposing each prediction into bias and feature contribution components as described in this [blog post](http:\/\/blog.datadive.net\/random-forest-interpretation-with-scikit-learn\/).\n- For the original code of this section, check out the [github repo](https:\/\/github.com\/fastai\/fastai\/blob\/master\/courses\/ml1\/lesson2-rf_interpretation.ipynb); for the explanations, watch the [video-lecture](https:\/\/youtu.be\/3jl2h9hSRvc?t=44m26s).","84589151":"As one would expect, not all the features are equally important.\n- There are a **handful** of columns that are really **informative**, while most of them are not.\n\nWe want to **concentrate efforts** on the features that matter the most, so we might want to dispense with the least informative columns and **include** only the **most informative** ones in the **training set**.\n- The number of columns to include\/exclude will depend on the **prediction performance**.\n- We want to make **just as good** a model than the model with all the columns, **but** hopefully **simpler** so it generalizes a little better.\n- If the model **gets worse**, we have **exluded too many** columns that weren't redundant after all.\n- We are going to **filter the columns** by importance value."}}