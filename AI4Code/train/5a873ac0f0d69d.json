{"cell_type":{"acc6c024":"code","64899deb":"code","73bbb19e":"code","cf9808d5":"code","4ea4a0a1":"code","f78b1960":"code","3f0a07e7":"code","a3b9f19e":"code","45d192a9":"code","7567f3f6":"code","0e1484d7":"code","2e118e87":"code","6352dd9e":"code","89776f3e":"code","d2fb692e":"code","ecc0be29":"code","6e17fa5e":"code","19524845":"code","8376ae95":"code","a6edf812":"code","753a839f":"code","8f6569e6":"code","1b441f98":"code","7ac2bfc3":"code","b84fa938":"code","be7ea6d4":"code","3f40ae00":"code","0d7e0e9e":"code","06891e39":"code","83343b06":"code","e4d6dd87":"code","8f7546cf":"code","1015a778":"code","49b9a2cc":"code","eb8bd4f2":"code","fa38fa0e":"code","bcc94c57":"code","9356f41c":"code","31e1a98f":"code","474437ff":"code","2587bf4e":"code","695821ea":"code","47989ccc":"code","bab8e6d8":"code","41ea06b3":"code","1ab3b31b":"code","721e6289":"code","a38aa9ad":"code","045e4223":"code","fa9917a5":"code","79c60654":"code","7aae9fa8":"code","de593fe7":"code","6e0e163a":"code","6ead8442":"code","14e33911":"code","3ab71b2f":"code","ecbeb1dc":"code","1eac3aa4":"code","40176d27":"code","f94e6046":"code","e8c76e47":"code","4bac1908":"code","0bf5d883":"code","3a54b00c":"code","69e6d299":"code","d4c1e50d":"code","e068623f":"code","e1f01d08":"code","a9c0af1c":"code","22cb7654":"code","d89968b1":"code","a3252d73":"code","ae2dd06a":"code","001eba57":"code","78d5d361":"code","dd71b7d7":"code","b97f4ae7":"code","ed2e37fa":"code","f2b5dae2":"code","564f89bc":"code","35dfe5c7":"code","ed65a268":"code","c188a239":"code","32bbaa8b":"code","98c82e9d":"code","62344c0c":"code","2562dc32":"code","58f061f7":"code","c3e6a6f2":"code","187e7e77":"code","51ddfce3":"code","82978e9d":"code","1b4a5991":"code","3885eb59":"code","03089315":"code","41e82209":"markdown","4d03a70c":"markdown","854cb100":"markdown","2b89a859":"markdown","24895f2a":"markdown","c730b873":"markdown","13b59fbe":"markdown","abfdde83":"markdown","4bf7f3d5":"markdown","2dfa1194":"markdown","5b32275d":"markdown","eba94376":"markdown","c3b04873":"markdown","668878fa":"markdown","2ed09dcc":"markdown","5166795c":"markdown"},"source":{"acc6c024":"# Import libraries to store data\nimport pandas as pd\nimport numpy as np\n\n# Import libraries to visualize data\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Import libraries to process data\nfrom tsfresh import extract_relevant_features, extract_features, select_features\nfrom tsfresh.utilities.dataframe_functions import impute\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\n\n# Import libraries to classify data and score results\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# Import libraries used in functions and for feedback\nimport os\nimport gc\nimport logging\nimport itertools\nimport warnings","64899deb":"# Settings\npath = os.getcwd()\ngc.enable()\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","73bbb19e":"# Kaggle kernel: IS_LOCAL = False\nIS_LOCAL = False\nif(IS_LOCAL):\n    PATH='..\/input\/'\nelse:\n    PATH='..\/input\/competicao-dsa-machine-learning-sep-2019\/'","cf9808d5":"print(os.listdir(PATH))","4ea4a0a1":"# Logger\ndef get_logger():\n    FORMAT = '[%(levelname)s] %(asctime)s: %(name)s: %(message)s'\n    logging.basicConfig(format=FORMAT)\n    logger = logging.getLogger('Main')\n    logger.setLevel(logging.DEBUG)\n    return logger\n\nlogger = get_logger()","f78b1960":"logger.info('Start load data')","3f0a07e7":"# Read in data into a dataframe\nX_train = pd.read_csv(os.path.join(PATH, 'X_treino.csv'))\ny_train = pd.read_csv(os.path.join(PATH, 'y_treino.csv'))\nX_test = pd.read_csv(os.path.join(PATH, 'X_teste.csv'))","a3b9f19e":"logger.info('Start exploratory data analysis')","45d192a9":"# Show dataframe columns\nprint(X_train.columns)","7567f3f6":"# Display top of dataframe\nX_train.head()","0e1484d7":"# Display the shape of dataframe\nX_train.shape","2e118e87":"# See the column data types and non-missing values\nX_train.info()","6352dd9e":"# Unique values by features\nX_train.nunique(dropna=False, axis=0)","89776f3e":"# Missing values by features\nX_train.isnull().sum(axis=0)","d2fb692e":"# Statistics of numerical features\nX_train.describe().T","ecc0be29":"# Boxplots for each column\nX_train.plot(kind='box', subplots=True, layout=(4,3), figsize=(14,10))","6e17fa5e":"# Show dataframe columns\nprint(X_test.columns)","19524845":"# Display top of dataframe\nX_test.head()","8376ae95":"# Display the shape of dataframe\nX_test.shape","a6edf812":"# See the column data types and non-missing values\nX_test.info()","753a839f":"# Unique values by features\nX_test.nunique(dropna=False, axis=0)","8f6569e6":"# Missing values by features\nX_test.isnull().sum(axis=0)","1b441f98":"# Statistics of numerical features\nX_test.describe().T","7ac2bfc3":"# Boxplots for each column\nX_test.plot(kind='box', subplots=True, layout=(4,3), figsize=(14,10))","b84fa938":"# Show dataframe columns\nprint(y_train.columns)","be7ea6d4":"# Display top of dataframe\ny_train.head()","3f40ae00":"# Display the shape of dataframe\ny_train.shape","0d7e0e9e":"# See the column data types and non-missing values\ny_train.info()","06891e39":"# Unique values by features\ny_train.nunique(dropna=False, axis=0)","83343b06":"# Missing values by features\ny_train.isnull().sum(axis=0)","e4d6dd87":"# Statistics of numerical features\ny_train.describe().T","8f7546cf":"# Boxplots for each column\ny_train.plot(kind='box', subplots=True, layout=(1,2), figsize=(5,4))","1015a778":"# Levels distribution of categorical\ny_train.groupby('surface').size().sort_values(ascending=False)","49b9a2cc":"# Distribution of target feature (surface)\nf, ax = plt.subplots(1,1, figsize=(16,4))\ntotal = float(len(y_train))\ng = sns.countplot(y_train['surface'], order = y_train['surface'].value_counts().index, color='steelblue')\ng.set_title(\"Number and percentage of surface\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(100*height\/total),\n            ha=\"center\") \nplt.show()","eb8bd4f2":"# Distribution of group_id\nf, ax = plt.subplots(1,1, figsize=(18,8))\ntotal = float(len(y_train))\ng = sns.countplot(y_train['group_id'], order = y_train['group_id'].value_counts().index, color='steelblue')\ng.set_title(\"Number and percentage of group_id\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.1f}%'.format(100*height\/total),\n            ha=\"center\", rotation='90') \nplt.show()","fa38fa0e":"# Density plots of features\ndef plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2, 5 ,figsize=(16,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2, 5, i)\n        sns.kdeplot(df1[feature], bw=0.5, label=label1)\n        sns.kdeplot(df2[feature], bw=0.5, label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show()","bcc94c57":"features = X_train.columns.values[3:]\nplot_feature_distribution(X_train, X_test, 'train', 'test', features)","9356f41c":"def plot_feature_class_distribution(classes, tt, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(5, 2, figsize=(16,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5, 2, i)\n        for clas in classes:\n            ttc = tt[tt['surface']==clas]\n            sns.kdeplot(ttc[feature], bw=0.5, label=clas)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show()","31e1a98f":"classes = (y_train['surface'].value_counts()).index\ntt = X_train.merge(y_train, on='series_id', how='inner')\nplot_feature_class_distribution(classes, tt, features)","474437ff":"# Target feature - surface and group_id distribution\nfig, ax = plt.subplots(1,1,figsize=(24,6))\ntmp = pd.DataFrame(y_train.groupby(['group_id', 'surface'])['series_id'].count().reset_index())\nm = tmp.pivot(index='surface', columns='group_id', values='series_id')\ns = sns.heatmap(m, linewidths=.1, linecolor='black', annot=True, cmap=\"YlGnBu\")\ns.set_title('Number of surface category per group_id', size=16)\nplt.show()","2587bf4e":"# Correlation map for train dataset\ncorr = X_train.corr()\n_ , ax = plt.subplots( figsize =( 12 , 10 ) )\ncmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n_ = sns.heatmap(corr, cmap = cmap, square=True, cbar_kws={ 'shrink' : .9 }, ax=ax, annot = True, annot_kws = {'fontsize' : 12 })","695821ea":"# Correlation map for test dataset\ncorr = X_test.corr()\n_ , ax = plt.subplots( figsize =( 12 , 10 ) )\ncmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n_ = sns.heatmap(corr, cmap = cmap, square=True, cbar_kws={ 'shrink' : .9 }, ax=ax, annot = True, annot_kws = {'fontsize' : 12 })","47989ccc":"logger.info('Start feature engineering')","bab8e6d8":"logger.info('Start processing missing values')","41ea06b3":"# Function to calculate missing values by column\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","1ab3b31b":"missing_values_table(X_train)","721e6289":"missing_values_table(X_test)","a38aa9ad":"missing_values_table(y_train)","045e4223":"logger.info('Start Euler factors and additional features')","fa9917a5":"# https:\/\/stackoverflow.com\/questions\/53033620\/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z","79c60654":"# Calculate euler factors and several additional features starting from the original features\ndef perform_euler_factors_calculation(df):\n    df['total_angular_velocity'] = np.sqrt(np.square(df['angular_velocity_X']) + np.square(df['angular_velocity_Y']) + np.square(df['angular_velocity_Z']))\n    df['total_linear_acceleration'] = np.sqrt(np.square(df['linear_acceleration_X']) + np.square(df['linear_acceleration_Y']) + np.square(df['linear_acceleration_Z']))\n    df['total_xyz'] = np.sqrt(np.square(df['orientation_X']) + np.square(df['orientation_Y']) +\n                              np.square(df['orientation_Z']))\n    df['acc_vs_vel'] = df['total_linear_acceleration'] \/ df['total_angular_velocity']\n    \n    x, y, z, w = df['orientation_X'].tolist(), df['orientation_Y'].tolist(), df['orientation_Z'].tolist(), df['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    df['euler_x'] = nx\n    df['euler_y'] = ny\n    df['euler_z'] = nz\n    \n    df['total_angle'] = np.sqrt(np.square(df['euler_x']) + np.square(df['euler_y']) + np.square(df['euler_z']))\n    df['angle_vs_acc'] = df['total_angle'] \/ df['total_linear_acceleration']\n    df['angle_vs_vel'] = df['total_angle'] \/ df['total_angular_velocity']\n    return df","7aae9fa8":"X_train = perform_euler_factors_calculation(X_train)","de593fe7":"X_test = perform_euler_factors_calculation(X_test)","6e0e163a":"X_train.shape","6ead8442":"X_test.shape","14e33911":"features = X_train.columns.values[13:]\nplot_feature_distribution(X_train, X_test, 'train', 'test', features)","3ab71b2f":"classes = (y_train['surface'].value_counts()).index\ntt = X_train.merge(y_train, on='series_id', how='inner')\nplot_feature_class_distribution(classes, tt, features)","ecbeb1dc":"logger.info('Start aggregated feature extraction')","1eac3aa4":"orientations = ['orientation_X', 'orientation_Y', 'orientation_Z', 'orientation_W']","40176d27":"angular_velocity = ['angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z']","f94e6046":"linear_acceleration = ['linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z']","e8c76e47":"params = {'abs_energy':None,\n          'absolute_sum_of_changes':None,\n          'agg_autocorrelation':[{'f_agg':'var','maxlag':32}],\n          'change_quantiles':[{'ql':0.25,'qh':0.75,'isabs':True, 'f_agg':'mean'},\n                             {'ql':0.25,'qh':0.75,'isabs':True, 'f_agg':'std'}],\n          'cid_ce':[{'normalize':True},{'normalize':False}],\n          'fft_aggregated':[{'aggtype': 'centroid'},\n                            {'aggtype': 'variance'},\n                            {'aggtype': 'skew'},\n                            {'aggtype': 'kurtosis'}],\n          'c3': [{'lag': 1}, {'lag': 2}, {'lag': 3}],\n          'standard_deviation': None,\n          'variance': None,\n          'skewness': None,\n          'kurtosis': None,\n          'maximum': None,\n          'minimum': None,\n          'sample_entropy':None,\n          'mean_abs_change':None,\n          'sum_values':None,\n          'quantile': [{'q': 0.1},\n                       {'q': 0.2},\n                       {'q': 0.3},\n                       {'q': 0.4},\n                       {'q': 0.6},\n                       {'q': 0.7},\n                       {'q': 0.8},\n                       {'q': 0.9}],\n          'large_standard_deviation': [{'r': 0.25},{'r':0.35}],\n          'fft_coefficient': [{'coeff': 0, 'attr': 'real'},\n                              {'coeff': 1, 'attr': 'real'},\n                              {'coeff': 2, 'attr': 'real'},\n                              {'coeff': 3, 'attr': 'real'},\n                              {'coeff': 4, 'attr': 'real'},\n                              {'coeff': 5, 'attr': 'real'},\n                              {'coeff': 6, 'attr': 'real'},\n                              {'coeff': 7, 'attr': 'real'},\n                              {'coeff': 8, 'attr': 'real'},\n                              {'coeff': 9, 'attr': 'real'},\n                              {'coeff': 10, 'attr': 'real'},\n                              {'coeff': 11, 'attr': 'real'},\n                              {'coeff': 12, 'attr': 'real'},\n                              {'coeff': 13, 'attr': 'real'},\n                              {'coeff': 14, 'attr': 'real'},\n                              {'coeff': 15, 'attr': 'real'},\n                              {'coeff': 16, 'attr': 'real'},\n                              {'coeff': 17, 'attr': 'real'},\n                              {'coeff': 18, 'attr': 'real'},\n                              {'coeff': 19, 'attr': 'real'},\n                              {'coeff': 20, 'attr': 'real'},\n                              {'coeff': 21, 'attr': 'real'},\n                              {'coeff': 22, 'attr': 'real'},\n                              {'coeff': 23, 'attr': 'real'},\n                              {'coeff': 24, 'attr': 'real'},\n                              {'coeff': 25, 'attr': 'real'},\n                              {'coeff': 26, 'attr': 'real'},\n                              {'coeff': 27, 'attr': 'real'},\n                              {'coeff': 28, 'attr': 'real'},\n                              {'coeff': 29, 'attr': 'real'},\n                              {'coeff': 30, 'attr': 'real'},\n                              {'coeff': 31, 'attr': 'real'},\n                              {'coeff': 32, 'attr': 'real'},\n                              {'coeff': 33, 'attr': 'real'},\n                              {'coeff': 34, 'attr': 'real'},\n                              {'coeff': 35, 'attr': 'real'},\n                              {'coeff': 36, 'attr': 'real'},\n                              {'coeff': 37, 'attr': 'real'},\n                              {'coeff': 38, 'attr': 'real'},\n                              {'coeff': 39, 'attr': 'real'},\n                              {'coeff': 40, 'attr': 'real'},\n                              {'coeff': 41, 'attr': 'real'},\n                              {'coeff': 42, 'attr': 'real'},\n                              {'coeff': 43, 'attr': 'real'},\n                              {'coeff': 44, 'attr': 'real'},\n                              {'coeff': 45, 'attr': 'real'},\n                              {'coeff': 46, 'attr': 'real'},\n                              {'coeff': 47, 'attr': 'real'},\n                              {'coeff': 48, 'attr': 'real'},\n                              {'coeff': 49, 'attr': 'real'},\n                              {'coeff': 50, 'attr': 'real'},\n                              {'coeff': 51, 'attr': 'real'},\n                              {'coeff': 52, 'attr': 'real'},\n                              {'coeff': 53, 'attr': 'real'},\n                              {'coeff': 54, 'attr': 'real'},\n                              {'coeff': 55, 'attr': 'real'},\n                              {'coeff': 56, 'attr': 'real'},\n                              {'coeff': 57, 'attr': 'real'},\n                              {'coeff': 58, 'attr': 'real'},\n                              {'coeff': 59, 'attr': 'real'},\n                              {'coeff': 60, 'attr': 'real'},\n                              {'coeff': 61, 'attr': 'real'},\n                              {'coeff': 62, 'attr': 'real'},\n                              {'coeff': 63, 'attr': 'real'},\n                              {'coeff': 64, 'attr': 'real'}],\n         }","4bac1908":"# The package TSFRESH was developed to automate feature extraction and selection from time series data.\n# https:\/\/tsfresh.readthedocs.io\/en\/latest\/text\/introduction.html\ntsfresh_train = extract_features(X_train.drop(['row_id'], axis=1),\n                                 column_id='series_id',\n                                 column_sort='measurement_number',\n                                 default_fc_parameters=params)\nimpute(tsfresh_train)","0bf5d883":"relevant_train_features = set()\nfor label in y_train['surface'].unique():\n    y_train_binary = (y_train['surface'].values == label).astype(int)\n    X_train_filtered = select_features(tsfresh_train, y_train_binary, fdr_level=0.382)\n    print(\"Number of relevant features for class {}: {}\/{}\".format(\n                    label, X_train_filtered.shape[1], tsfresh_train.shape[1]))\n    relevant_train_features = relevant_train_features.union(set(X_train_filtered.columns))","3a54b00c":"tsfresh_test = extract_features(X_test.drop(['row_id'], axis=1),\n                                column_id='series_id', \n                                column_sort='measurement_number',\n                                default_fc_parameters=params)\nimpute(tsfresh_test)","69e6d299":"len(relevant_train_features)","d4c1e50d":"tsfresh_train = tsfresh_train[list(relevant_train_features)]\ntsfresh_test = tsfresh_test[list(relevant_train_features)]","e068623f":"tsfresh_train.shape","e1f01d08":"tsfresh_test.shape","a9c0af1c":"logger.info('Prepare models')","22cb7654":"# LabelEncoder\nle = LabelEncoder()\ny_train['surface'] = le.fit_transform(y_train['surface'])","d89968b1":"y_train['surface'].head()","a3252d73":"# Create the scaler object\nscaler = StandardScaler()\n# Fit on the training data\nscaler.fit(tsfresh_train)\n# Transform both the training and testing data\ntsfresh_train = scaler.transform(tsfresh_train)\ntsfresh_test = scaler.transform(tsfresh_test)","ae2dd06a":"folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)","001eba57":"logger.info('Start running model')","78d5d361":"RFC = RandomForestClassifier(n_estimators=500, n_jobs=-1)","dd71b7d7":"sub_preds_rf = np.zeros((tsfresh_test.shape[0], 9))\noof_preds_rf = np.zeros((tsfresh_train.shape[0]))\nscore = 0\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(tsfresh_train, y_train['surface'])):\n    clf = RFC\n    clf.fit(tsfresh_train[trn_idx], y_train['surface'][trn_idx])\n    oof_preds_rf[val_idx] = clf.predict(tsfresh_train[val_idx])\n    sub_preds_rf += clf.predict_proba(tsfresh_test) \/ folds.n_splits\n    score += clf.score(tsfresh_train[val_idx], y_train['surface'][val_idx])\n    print('Fold: {} score: {}'.format(fold_, clf.score(tsfresh_train[val_idx], y_train['surface'][val_idx])))\nprint('Avg Accuracy', score \/ folds.n_splits)","b97f4ae7":"logger.info('Prepare confusion matrix')","ed2e37fa":"def plot_confusion_matrix(actual, predicted, classes, title='Confusion Matrix'):\n    conf_matrix = confusion_matrix(actual, predicted)\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(title, size=12)\n    plt.colorbar(fraction=0.05, pad=0.05)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    thresh = conf_matrix.max() \/ 2.\n    for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n        plt.text(j, i, format(conf_matrix[i, j], 'd'),\n        horizontalalignment=\"center\", color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.grid(False)\n    plt.tight_layout()","f2b5dae2":"plot_confusion_matrix(y_train['surface'], oof_preds_rf, le.classes_)","564f89bc":"logger.info('Start running model')","35dfe5c7":"param_grid = {\n    'max_depth': [3, 4, 5, 6],  # the maximum depth of each tree\n    'min_child_weight': np.linspace(0.8, 1.2, 4),\n    'gamma': np.linspace(0, 0.2, 4),\n}","ed65a268":"XGB = xgb.sklearn.XGBClassifier(learning_rate = 0.025,\n                                objective = 'multi:softmax',\n                                n_estimators = 150,\n                                max_depth = 5,\n                                min_child_weight = 1.2,\n                                subsample=0.8,\n                                colsample_bytree = 0.8,\n                                gamma = 0.066,\n                                n_jobs = -1,\n                                silent = True,\n                                seed = 42)","c188a239":"sub_preds_xgb = np.zeros((tsfresh_test.shape[0], 9))\noof_preds_xgb = np.zeros((tsfresh_train.shape[0]))\nscore = 0\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(tsfresh_train, y_train['surface'])):\n    clf =  XGB\n    clf.fit(tsfresh_train[trn_idx], y_train['surface'][trn_idx])\n    oof_preds_xgb[val_idx] = clf.predict(tsfresh_train[val_idx])\n    sub_preds_xgb += clf.predict_proba(tsfresh_test) \/ folds.n_splits\n    score += clf.score(tsfresh_train[val_idx], y_train['surface'][val_idx])\n    print('Fold: {} score: {}'.format(fold_, clf.score(tsfresh_train[val_idx], y_train['surface'][val_idx])))\nprint('Avg Accuracy', score \/ folds.n_splits)","32bbaa8b":"logger.info('Prepare confusion matrix')","98c82e9d":"plot_confusion_matrix(y_train['surface'], oof_preds_xgb, le.classes_)","62344c0c":"logger.info('Start running model')","2562dc32":"SCLF = StackingCVClassifier(classifiers=[XGB, RFC],\n                            meta_classifier=RFC,\n                            use_features_in_secondary=True,\n                            n_jobs=-1,\n                            random_state=42)","58f061f7":"sub_preds_sclf = np.zeros((tsfresh_test.shape[0], 9))\noof_preds_sclf = np.zeros((tsfresh_train.shape[0]))\nscore = 0\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(tsfresh_train, y_train['surface'])):\n    clf =  SCLF\n    clf.fit(tsfresh_train[trn_idx], y_train['surface'][trn_idx])\n    oof_preds_sclf[val_idx] = clf.predict(tsfresh_train[val_idx])\n    sub_preds_sclf += clf.predict_proba(tsfresh_test) \/ folds.n_splits\n    score += clf.score(tsfresh_train[val_idx], y_train['surface'][val_idx])\n    print('Fold: {} score: {}'.format(fold_, clf.score(tsfresh_train[val_idx], y_train['surface'][val_idx])))\nprint('Avg Accuracy', score \/ folds.n_splits)","c3e6a6f2":"logger.info('Prepare confusion matrix')","187e7e77":"plot_confusion_matrix(y_train['surface'], oof_preds_sclf, le.classes_)","51ddfce3":"logger.info(\"Prepare submission\")","82978e9d":"submission = pd.read_csv(os.path.join(PATH, 'sample_submission.csv'))","1b4a5991":"submission['surface'] = le.inverse_transform(sub_preds_sclf.argmax(axis=1))","3885eb59":"submission.to_csv('submission.csv', index=False)","03089315":"submission.head(10)","41e82209":"## Import Packages and Initial Settings","4d03a70c":"## End","854cb100":"### Aggregated feature extraction","2b89a859":"### Prepare for cross-validation","24895f2a":"### StackingCVClassifier","c730b873":"### XGBoost","13b59fbe":"# Competi\u00e7\u00e3o DSA de Machine Learning - Set\/2019\n# Maicon Moda","abfdde83":"## Load Data","4bf7f3d5":"### Scaling Features","2dfa1194":"### Processing Missing Values","5b32275d":"### Random Forest Classifier","eba94376":"## Feature Engineering","c3b04873":"## Machine Learning","668878fa":"### Euler factors and additional features","2ed09dcc":"## Exploratory Data Analysis","5166795c":"## Submission"}}