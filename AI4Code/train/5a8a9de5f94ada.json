{"cell_type":{"0ec50df2":"code","1696c870":"code","b472c0b4":"code","fe1e7406":"code","5953801e":"code","50b3af62":"code","3ddd2954":"code","badc088e":"code","108b1abd":"code","41ab43e2":"code","ca1ab215":"code","28df6089":"code","bd9f9998":"code","dafe2cab":"code","ca935052":"code","f609df7f":"code","5f67e1e7":"code","aec10057":"code","ca55fae7":"code","56dcec57":"code","eae2b55b":"code","b4974adc":"code","fc9dd2c6":"code","82f2271b":"code","e4e48f59":"code","68279af7":"code","4ce227de":"code","93e9eb87":"code","e04e4141":"code","3d60eade":"code","0d1f5116":"code","124d3ac0":"code","5419e4b4":"code","35e6bb8a":"code","7b9d263f":"code","45a9e3fd":"code","5bd1c1bd":"code","98c494c8":"code","163868aa":"code","975fcb7c":"code","0e83d3ae":"code","73375bf9":"code","b2cfb743":"code","891abaea":"code","dcf2d592":"code","d11262ca":"code","50052bdf":"code","638848ad":"code","fea3345a":"code","e9dd1040":"code","e8f25f2f":"code","9917ccb1":"code","0b4cda31":"code","7c9f3539":"code","38303548":"code","b35a0fad":"code","ec95d82e":"code","7efce969":"code","faf7d820":"code","b9f12118":"code","05f17cbe":"code","c2c680c0":"code","3d17d664":"code","60797e49":"code","17cb3637":"code","ba500552":"code","c25a54ea":"code","ec8eb743":"code","7cd2d8b0":"code","d444cdac":"code","5f9cb6b6":"code","5a0f4b24":"code","f9053155":"code","9479e335":"code","d9e3b64c":"code","78e009d3":"code","eb4c75ce":"code","f1ec480d":"code","e5284dca":"code","620ab8de":"code","0459ef6c":"code","f6032ef2":"code","e9dc8c00":"code","a5bd96a2":"code","9b30baa1":"code","2d45f9ad":"code","47b7a150":"code","4d50c2d3":"code","8e18aae1":"code","74acacb1":"code","1f02868e":"code","34e09f4a":"code","9227e850":"code","868bb16c":"code","550732cf":"markdown","b1903988":"markdown","0c2dffb2":"markdown","372fa865":"markdown","fbbed9d4":"markdown","df93302b":"markdown","01de25bb":"markdown","e85e6b4d":"markdown","4c11cf47":"markdown","aca9d811":"markdown","882b4f3b":"markdown","e8532761":"markdown","6194a9e9":"markdown","cef9fa4c":"markdown","0283bdfa":"markdown","4e9cff41":"markdown","70e3283b":"markdown","6a8de2f3":"markdown","c7cc1d31":"markdown","574c58ca":"markdown","f8a659fa":"markdown","9309bee6":"markdown","f382060a":"markdown","d677883b":"markdown","4f6e2a8e":"markdown","aabdf4c8":"markdown","d83cb8a0":"markdown","95b967e0":"markdown","18da1af4":"markdown","1326d60f":"markdown","d9d37215":"markdown","d8b4690a":"markdown","b21e0290":"markdown","cec071e0":"markdown","c3939ce4":"markdown","4223d664":"markdown","3a2bf6f5":"markdown","2bfa8dee":"markdown","ea1fb747":"markdown","06c6773a":"markdown","6af4e602":"markdown","aff7796e":"markdown","187dab35":"markdown","e1fb6ae4":"markdown","c5824948":"markdown","7972be72":"markdown","02908d65":"markdown","7348ba1c":"markdown","fafdb946":"markdown","1a1182a8":"markdown","a553153d":"markdown","a6ba0c5a":"markdown","3f9e8db9":"markdown","2ce7a823":"markdown","542ab71a":"markdown","3c1c3347":"markdown","496fd39a":"markdown","31ab3f7c":"markdown","23f46f82":"markdown"},"source":{"0ec50df2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","1696c870":"df = pd.read_csv('..\/input\/students-performance-in-exams\/StudentsPerformance.csv')","b472c0b4":"df.head()","fe1e7406":"df.info()","5953801e":"df.describe()","50b3af62":"sns.distplot(df['math score'])\nplt.title('Distribution of Math scores')","3ddd2954":"sns.distplot(df['reading score'])\nplt.title('Distribution of Reading scores')","badc088e":"sns.distplot(df['writing score'])\nplt.title('Distribution of Writing Scores')","108b1abd":"exam_list = ['math score', 'reading score', 'writing score']\nfor exam in exam_list:\n    print(exam + ':')\n    print('Percentage of students scoring between 0 & 50: {}%'.format(100 * len(df[df[exam] <= 50]) \/ len(df)))\n    print('Percentage of students scoring between 51 & 60: {}%'.format(100 * len(df[(df[exam] >= 51) & (df[exam] <= 60)]) \/ len(df)))\n    print('Percentage of students scoring between 61 & 70: {}%'.format(100 * len(df[(df[exam] >= 61) & (df[exam] <= 70)]) \/ len(df)))\n    print('Percentage of students scoring between 71 & 80: {}%'.format(100 * len(df[(df[exam] >= 71) & (df[exam] <= 80)]) \/ len(df)))\n    print('Percentage of students scoring between 81 & 90: {}%'.format(100 * len(df[(df[exam] >= 81) & (df[exam] <= 90)]) \/ len(df)))\n    print('Percentage of students scoring between 91 & 100: {}%'.format(100 * len(df[(df[exam] >= 91)]) \/ len(df)))\n    print('-' * 40)\n  ","41ab43e2":"sns.pairplot(df[['math score','writing score','reading score']])","ca1ab215":"sns.heatmap(df[['math score','reading score','writing score']].corr(), annot=True)","28df6089":"df[df['gender'] == 'male'].describe()","bd9f9998":"df[df['gender'] == 'female'].describe()","dafe2cab":"sns.boxplot(x='gender',y='math score',data=df)","ca935052":"sns.boxplot(x='gender',y='reading score',data=df)","f609df7f":"sns.boxplot(x='gender',y='writing score',data=df)","5f67e1e7":"df[df['test preparation course'] == 'completed'].describe()","aec10057":"df[df['test preparation course'] == 'none'].describe()","ca55fae7":"plt.figure(figsize=(12,8))\nplt.subplot(1,3,1)\nsns.boxplot(x='test preparation course', y='math score', data=df)\n\nplt.subplot(1,3,2)\nsns.boxplot(x='test preparation course', y='reading score', data=df)\n\nplt.subplot(1,3,3)\nsns.boxplot(x='test preparation course', y='writing score', data=df)\n\nplt.suptitle('How does the Test Preparation Course effect Test Scores?')","56dcec57":"order = ['group A', 'group B', 'group C', 'group D', 'group E']\n\nplt.figure(figsize=(15,8))\nplt.subplot(1,3,1)\nsns.boxplot(x='race\/ethnicity', y='math score', data=df,order=order)\n\nplt.subplot(1,3,2)\nsns.boxplot(x='race\/ethnicity', y='reading score', data=df, order=order)\n\nplt.subplot(1,3,3)\nsns.boxplot(x='race\/ethnicity', y='writing score', data=df,order=order)\n\nplt.suptitle('How does race\/ethnicity effect Test Scores?')","eae2b55b":"sns.countplot(x='race\/ethnicity',data=df, order=order)","b4974adc":"plt.figure(figsize=(10,6))\nsns.countplot(x='parental level of education',data=df)","fc9dd2c6":"df['parental level of education'] = df['parental level of education'].apply(lambda x: 'high school' if 'high school' in x else x)","82f2271b":"plt.figure(figsize=(10,6))\nsns.countplot(x='parental level of education',data=df)","e4e48f59":"education_order = [\"high school\", \"some college\", \"associate's degree\", \"bachelor's degree\", \"master's degree\"]\nplt.figure(figsize=(10,6))\nsns.boxplot(x='parental level of education', y='math score', data=df, order=education_order)","68279af7":"plt.figure(figsize=(10,6))\nsns.boxplot(x='parental level of education', y='reading score', data=df, order=education_order)","4ce227de":"plt.figure(figsize=(10,6))\nsns.boxplot(x='parental level of education', y='writing score', data=df,order=education_order)","93e9eb87":"df[df['lunch'] == 'standard'].describe()","e04e4141":"df[df['lunch'] == 'free\/reduced'].describe()","3d60eade":"plt.figure(figsize=(15,8))\nplt.subplot(1,3,1)\nsns.boxplot(x='lunch', y='math score', data=df)\n\nplt.subplot(1,3,2)\nsns.boxplot(x='lunch', y='reading score', data=df)\n\nplt.subplot(1,3,3)\nsns.boxplot(x='lunch', y='writing score', data=df)\n\nplt.suptitle('How does the lunch package each student recieves effect Test Scores?')","0d1f5116":"race_v_pared = df.groupby(['parental level of education','race\/ethnicity']).size().reset_index(name=\"Count\").pivot(index='parental level of education',columns='race\/ethnicity',values='Count')\nrace_v_pared.index = pd.CategoricalIndex(race_v_pared.index, categories=[\"high school\", \"some college\", \"associate's degree\", \"bachelor's degree\", \"master's degree\"])\nrace_v_pared.sort_index(level=0, inplace=True)\nsns.heatmap(race_v_pared,annot=True,fmt='d')","124d3ac0":"for group in ['group A', 'group B', 'group C', 'group D', 'group E']:\n    print(group)\n    for edu in education_order:\n        print(\"Percentage with {} education: {}%\".format(edu, 100 * race_v_pared[group][edu]\/race_v_pared[group].sum()))\n    print('-' * 25)","5419e4b4":"race_v_lunch = df.groupby(['race\/ethnicity','lunch']).size().reset_index(name=\"Count\").pivot(index='lunch',columns='race\/ethnicity',values='Count')\nrace_v_lunch.index = pd.CategoricalIndex(race_v_lunch.index, categories=['free\/reduced','standard'])\nrace_v_lunch.sort_index(level=0, inplace=True)\nsns.heatmap(race_v_lunch,annot=True, fmt='d')","35e6bb8a":"for group in ['group A', 'group B', 'group C', 'group D', 'group E']:\n    print(group)\n    for lunch in ['free\/reduced','standard']:\n        print(\"Percentage with {} lunch: {}%\".format(lunch,100 * race_v_lunch[group][lunch] \/ race_v_lunch[group].sum()))\n    print('-' * 40)","7b9d263f":"race_v_prep = df.groupby(['race\/ethnicity','test preparation course']).size().reset_index(name=\"Count\").pivot(index='test preparation course',columns='race\/ethnicity',values='Count')\nrace_v_prep.index = pd.CategoricalIndex(race_v_prep.index, categories=['none','completed'])\nrace_v_prep.sort_index(level=0, inplace=True)\nsns.heatmap(race_v_prep,annot=True,fmt='d')","45a9e3fd":"for group in ['group A', 'group B', 'group C', 'group D', 'group E']:\n    print(\"Percentage of Students in ethnic {} who completed the test preparation course: {}%\".format(group, 100 * race_v_prep[group]['completed'] \/ race_v_prep[group].sum()))","5bd1c1bd":"parvprep = df.groupby(['parental level of education', 'test preparation course']).size().reset_index(name='Count').pivot(index='parental level of education',columns='test preparation course',values='Count')\nparvprep.index = pd.CategoricalIndex(parvprep.index, categories=[\"high school\", \"some college\", \"associate's degree\", \"bachelor's degree\", \"master's degree\"])\nparvprep.sort_index(level=0, inplace=True)\nsns.heatmap(parvprep,annot=True,fmt='d')","98c494c8":"for edu in [\"high school\", \"some college\", \"associate's degree\", \"bachelor's degree\", \"master's degree\"]:\n    print(\"Percentage of students who parents achieved {} level education that completed the test preparation course: {}%\".format(edu, (100 * parvprep['completed'][edu] \/ (parvprep['completed'][edu] + parvprep['none'][edu]) )))","163868aa":"full_marks = df[(df['math score'] == 100) & (df['reading score'] == 100) & (df['writing score'] == 100)]\nfull_marks","975fcb7c":"zero_marks = df[(df['math score'] == 0) & (df['reading score'] == 0) & (df['writing score'] == 0)]\nzero_marks","0e83d3ae":"lessthan40 = df[(df['math score'] < 20) & (df['reading score'] < 20) & (df['writing score'] < 20)]\nlessthan40","73375bf9":"df.describe().T","b2cfb743":"m_and_r = df[((df['math score'] > 77) & (df['reading score'] < 59)) | ((df['reading score'] > 79) & (df['math score'] < 57))]\nm_and_r","891abaea":"m_and_w = df[((df['math score'] > 77) & (df['writing score'] < 58)) | ((df['writing score'] > 79) & (df['math score'] < 57))]\nm_and_w","dcf2d592":"r_and_w = df[((df['reading score'] > 79) & (df['writing score'] < 58)) | ((df['writing score'] > 79) & (df['reading score'] < 59))]\nr_and_w","d11262ca":"def outside_range(df, column):\n    global lower,upper\n    q25, q75 = np.quantile(df[column], 0.25), np.quantile(df[column], 0.75)\n    \n    # calculate the IQR\n    iqr = q75 - q25\n    \n    # calculate the outlier cutoff\n    cut_off = iqr * 1.5\n    \n    # calculate the lower and upper bound value of the range\n    lower, upper = q25 - cut_off, q75 + cut_off\n    print('The IQR for {} is {}'.format(column,iqr))\n    print('The lower bound value is', lower)\n    print('The upper bound value is', upper)\n    \n    \n    # Calculate the number of records below and above lower and above bound value respectively\n    df1 = df.index[(df[column] > upper) | (df[column] < lower)]\n    \n    print(\"The number of outliers for {} is {}\".format(column, len(df1)))\n    \n    # show the two data frames where the values are outside the range\n    return df.iloc[df1]","50052bdf":"outside_range(df,'math score')","638848ad":"outside_range(df,'reading score')","fea3345a":"outside_range(df, 'writing score')","e9dd1040":"df.iloc[842]","e8f25f2f":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()","9917ccb1":"df['parental level of education'] = label_encoder.fit_transform(df['parental level of education'])\ndf['lunch'] = label_encoder.fit_transform(df['lunch'])\ndf['test preparation course'] = label_encoder.fit_transform(df['test preparation course'])","0b4cda31":"df['race\/ethnicity'] = df['race\/ethnicity'].replace('group A', 1)\ndf['race\/ethnicity'] = df['race\/ethnicity'].replace('group B', 2)\ndf['race\/ethnicity'] = df['race\/ethnicity'].replace('group C', 3)\ndf['race\/ethnicity'] = df['race\/ethnicity'].replace('group D', 4)\ndf['race\/ethnicity'] = df['race\/ethnicity'].replace('group E', 5)","7c9f3539":"gender = pd.get_dummies(df['gender'],drop_first=True)\ndf = pd.concat([df,gender],axis=1)\ndf.head()","38303548":"df = df.drop('gender',axis=1)","b35a0fad":"df.head()","ec95d82e":"df['average score'] = (df['math score'] + df['reading score'] + df['writing score']) \/ 3","7efce969":"df.head()","faf7d820":"X = df[['race\/ethnicity','parental level of education','lunch','test preparation course','male']]\ny = df['average score']","b9f12118":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test_ave = train_test_split(X, y, test_size=0.3, random_state=101)","05f17cbe":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","c2c680c0":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train,y_train)\nlr_ave_pred = lr.predict(X_test)","3d17d664":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam","60797e49":"ave_deep_model = Sequential()\n\n# Input Layer\nave_deep_model.add(Dense(5,activation='relu'))\nave_deep_model.add(Dropout(0.25))\n\n# Hidden Layer 1\nave_deep_model.add(Dense(10,activation='relu'))\nave_deep_model.add(Dropout(0.25))\n\n# Hidden Layer 2\nave_deep_model.add(Dense(20,activation='relu'))\nave_deep_model.add(Dropout(0.25))\n\n# Hidden Layer 3\nave_deep_model.add(Dense(10,activation='relu'))\nave_deep_model.add(Dropout(0.25))\n\n# Output Layer\nave_deep_model.add(Dense(1,activation='relu'))\n\n# Compile Model\nave_deep_model.compile(optimizer='adam',loss='mse')","17cb3637":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=250)","ba500552":"ave_deep_model.fit(x=X_train, \n          y=y_train.values, \n          epochs=1000,\n          validation_data=(X_test, y_test_ave), verbose=1,\n          batch_size=64,\n          callbacks=[early_stop]\n          )","c25a54ea":"ave_deep_model_pred = ave_deep_model.predict(X_test)","ec8eb743":"X = df[['race\/ethnicity','parental level of education','lunch','test preparation course','male']]\ny = df[['math score','reading score','writing score']]","7cd2d8b0":"X_train, X_test, y_train, y_test_indiv = train_test_split(X, y, test_size=0.3, random_state=101)","d444cdac":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","5f9cb6b6":"lr3 = LinearRegression()\nlr3.fit(X_train,y_train)\nlr3_pred = lr3.predict(X_test)","5a0f4b24":"multi_deep_model = Sequential()\n\n# Input Layer\nmulti_deep_model.add(Dense(5,activation='relu'))\nmulti_deep_model.add(Dropout(0.25))\n\n# Hidden Layer 1\nmulti_deep_model.add(Dense(10,activation='relu'))\nmulti_deep_model.add(Dropout(0.25))\n\n# Hidden Layer 2\nmulti_deep_model.add(Dense(20,activation='relu'))\nmulti_deep_model.add(Dropout(0.25))\n\n# Hidden Layer 3\nmulti_deep_model.add(Dense(10,activation='relu'))\nmulti_deep_model.add(Dropout(0.25))\n\n# Output Layer\nmulti_deep_model.add(Dense(3,activation='relu'))\n\n# Compile Model\nmulti_deep_model.compile(optimizer='adam',loss='mse')","f9053155":"multi_deep_model.fit(x=X_train, \n          y=y_train.values, \n          epochs=1000,\n          validation_data=(X_test, y_test_indiv), verbose=1,\n          batch_size=64,\n          callbacks=[early_stop]\n          )","9479e335":"multi_deep_preds = multi_deep_model.predict(X_test)","d9e3b64c":"from sklearn.metrics import mean_absolute_error, mean_squared_error","78e009d3":"print(\"Linear Regression MAE: {}\".format(mean_absolute_error(y_test_ave,lr_ave_pred)))\nprint(\"Deep Learning Model MAE: {}\".format(mean_absolute_error(y_test_ave,ave_deep_model_pred)))\nprint(\"-\" * 40)\nprint(\"Linear Regression MSE: {}\".format(mean_squared_error(y_test_ave,lr_ave_pred)))\nprint(\"Deep Learning Model MSE: {}\".format(mean_squared_error(y_test_ave,ave_deep_model_pred)))","eb4c75ce":"print(\"Multi Target Linear Regression MAE: {}\".format(mean_absolute_error(y_test_indiv,lr3_pred)))\nprint(\"Multi Target Deep Network MAE: {}\".format(mean_absolute_error(y_test_indiv,multi_deep_preds)))\nprint(\"-\" * 50)\nprint(\"Multi Target Linear Regression MSE: {}\".format(mean_squared_error(y_test_indiv,lr3_pred)))\nprint(\"Multi Target Deep Network MAE: {}\".format(mean_squared_error(y_test_indiv,multi_deep_preds)))","f1ec480d":"def average_to_grade(x):\n    \n    if 0 <= x <= 40:\n        return 0\n    elif 40 < x <= 50:\n        return 1\n    elif 50 < x <= 60:\n        return 2\n    elif 60 < x <= 70:\n        return 3\n    elif 70 < x <= 80:\n        return 4\n    elif 80 < x <= 90:\n        return 5\n    else:\n        return 6","e5284dca":"df['grade'] = df['average score'].apply(average_to_grade)","620ab8de":"df['grade'].value_counts()","0459ef6c":"df = df.drop(['math score','reading score','writing score','average score'],axis=1)","f6032ef2":"df.head()","e9dc8c00":"from imblearn.over_sampling import SMOTENC","a5bd96a2":"data = df.values\nX = data[:, :-1]\ny = data[:, -1]\nX_columns = df.columns[:-1]\ny_columns = df.columns[-1]\n\noversample = SMOTENC([0,1,2,3])\nX, y = oversample.fit_sample(X, y)\nX_sampled = pd.DataFrame(X, columns=X_columns)\ny_sampled = pd.DataFrame(y, columns=[y_columns])\n\ndf = pd.concat([X_sampled,y_sampled],axis=1)","9b30baa1":"df['grade'].value_counts()","2d45f9ad":"X = df.drop('grade',axis=1)\ny = df['grade']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=101)","47b7a150":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression(multi_class='multinomial',max_iter=2000)","4d50c2d3":"log_model.fit(X_train,y_train)","8e18aae1":"log_model_preds = log_model.predict(X_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,log_model_preds))\nprint(\"\\n\")\nprint(classification_report(y_test,log_model_preds))","74acacb1":"from sklearn.neighbors import KNeighborsClassifier\nerror_rate = []\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","1f02868e":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","34e09f4a":"knn = KNeighborsClassifier(n_neighbors=17)\nknn.fit(X_train,y_train)\nknn_preds = knn.predict(X_test)\nprint(confusion_matrix(y_test,knn_preds))\nprint(\"\\n\")\nprint(classification_report(y_test,knn_preds))","9227e850":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train,y_train)","868bb16c":"rfc_preds = rfc.predict(X_test)\nprint(confusion_matrix(y_test,rfc_preds))\nprint(\"\\n\")\nprint(classification_report(y_test,rfc_preds))","550732cf":"We can see that our dataset contains the results of 3 exams in maths, reading and writing taken by 1000 students. Our dataset also contains information on the gender, race, parental education, lunch type and whether or not a preparation course was taken for each student. We shall attempt to determine which of these factors is most influential in predicting the exam results for each student. Let us begin our analysis.\n\n## 1. Exploratory Data Analysis and Visualisation\n\nIn this section we shall analyse the dataset and attempt to determine which variables are the most important in predicting students' exam results, as well as the relationships between our independant variables. Let us begin by analysing the relationship between the students' results in each of the three exams.\n\n### 1.1 Math, Reading and Writing Analysis\n\nLet us begin by producing histograms for each of the tests to determine whether the scores are normally distributed.","b1903988":"We can see that there is a clear, obvious and strong positive linear relationship between each of the three test results. This is to be expected, since students who are academically intelligent are likely to perform well in a wide range of subjects, whilst students who struggle with focus or motivation are just as likely to perform poorly across all of their subjects. The strength of the relationship between the reading and writing tests is the most prominent, likely due to the fact that these subjects are extremely highly correlated. \n\nAs a result of the strong linear relationships between the three test results, we are able to produce an average result scored for each student across the three tests. This will enable us to create a simpler model in which we are only required to predict one value rather than 3.\n\n### 1.2 Independant v Dependant Variables\n\nIn this section, we shall analyse the effect our independant variables have on the test scores achieved by the students.\n\n#### 1.2.1 Gender\n\nWe shall now begin to analyse how the other data features within our dataset affect the marks achieved by the students, starting with gender.\n\nSince we are considering three different test scores, gender may have an effect. Typically, female students tend to enjoy reading and writing more than their male counterparts, while more males than females enjoy the subject of mathematics. Let us investigate this by producing box plots.","0c2dffb2":"Let us now import the dataset and check it's head, info and describe methods.","372fa865":"Once again, we notice that within the minority group a higher percentage of students recieve discounted lunches. This further highlights the potential lack of funds available for minority groups. \n\n### 1.3.3 Race\/Ethnicity v Test Preparation Course\n\nLet us investigate the relationship between a students' race and whether or not they completed the test preparation course.","fbbed9d4":"Our logistic regression model achieved approximately 31% accuracy. \n\n### 6.2 K-Nearest Neighbors\n\nLet us first try to find the optimal number of neighbors by training KNN models with a range of different neighbor values and recording the error in a list.","df93302b":"We can see that we now have 260 instances in each of the 7 different grade classes, for a total of 1820 data points. We shall now split the data into a training and testing set and then begin to implement and analyse different machine learning algorithms for the classification of students' grades.","01de25bb":"Let us now investigate these values for each of the two models.","e85e6b4d":"All of our variables are now in a numerical format and are ready for use in our machine learning models. \n\n### 3.3 Generation of new dependent variable\n\nIn our model creation section, we shall create models that predict the average score obtained by the student, as well as creating models that predict the scores obtained in each individual exam. As a result, we must create a new column which contains the average score obtained for use in the model fitting and testing processes.","4c11cf47":"We shall now use the label encoder to transform our data within the \"parental level of education\" and \"lunch\" columns.","aca9d811":"We can now create a linear regression model to predict the results of the three tests individually.","882b4f3b":"### 6.1 Logistic Regression\n\nIn this section, we shall implement and analyse a logistic regression model for the problem of predicting students grades.","e8532761":"From the above, we can see that there were 3 students who achieved full marks in all 3 tests. The two female students raise slight suspicion, however. This is due to the fact that neither of them completed the test preparation course. As a result, these two students possibly cheated or are both extremely intelligent. Since we are unable to determine which of these assumptions is true, we shall leave both data points in the dataset. \n\nLet us now see if there were any students who failed to score any marks across all three tests.","6194a9e9":"#### 3.2.2 Dummy Variables\n\nLet us now use the pandas \"get_dummies\" function to convert the gender column. We must set the \"drop_first\" option to be true in order to reduce multicolinearity.","cef9fa4c":"Let us now create the predictions using our deep learning model which we shall analyse in the \"Model Analysis\" section.","0283bdfa":"# Student Exam Results\n\nFor years, students have been using their predicted grades in an effort to monitor their progress throughout their chosen subjects. More recently, as a result of the Covid-19 pandemic, predicted grades have been used by exam boards, particularly within the UK, as a student's final grade. As a result, the accurate prediction of these grades has grown significantly more important. \n\nThe dataset used within this project was found on kaggle.com and a big thank you to Kaggle user Jakki for providing this dataset for public use. \n\nThroughout this kernel, we shall undertake the following tasks.\n\n0. Package and Data Imports. In this section we shall import the basic required packages as well as the dataset. Note: The machine learning algorithms will be imported as and when they are required.\n1. Exploratory Data Analysis and Visualisation. In this section we shall attempt to identify the important factors used in the predictions of student results.\n2. Feature Engineering. In this section we shall try to extract as much information as possible from the dataset via the creation of new columns.\n3. Data Preprocessing. In this section we will prepare the data for use within a wide range of machine learning algorithms, which shall include the identification of outliers and adjusting the format of certain data columns.\n4. Model Creation. In this section we will create a wide range of machine learning algorithms for use in predicting exam results.\n5. Model Analysis. In this section we shall analyse the models created in the previous section and attempt to determine which model predicted grades most accurately.\n\n## 0. Package and Data Imports\n\nLet us begin by importing the necessary Python packages for our exploratory data analysis and data visualisation. ","4e9cff41":"We can see that for all 3 tests, approximately 50% of students scored between 61 & 80 marks. The math test had the lowest proportion of students scoring above 81 marks, with roughly 17% of students managing this. Approximately 20% and 23% of students achieved this threshold in the writing and reading tests, repectively. Furthermore, the math test had the highest percentage of students scoring below 50 marks, with 15% of students failing to reach this threshold. 10% and 13%, approximately, of students failed to score more than 50 marks in the reading and writing tests respectively. \n\nLet us investigate the relationship between each of the test scores, by producing a pairplot and a heatmap of the correlation between the variables.","70e3283b":"We are now able to build our linear regression model and use it to create predictions for our training set.","6a8de2f3":"We can see that this KNN model achieves an accuracy of approximately 40%, which is a significant improvement on the Logistic Regression model trained above.\n\n### 6.3 Random Forest\n\nIn this section we shall implement and evaluate a random forest classifier.","c7cc1d31":"#### 4.2.2 Deep Learning Model\n\nIn order to compare the results directly, we shall use the same model construction as before when predicting average scores. We shall simply change the output layer to predict 3 values.","574c58ca":"The plots above show that the results for each of the exams are approximately normally distributed, with no clear and obvious signs of skewness.\n\nLet us now investigate in more detail the breakdown of student scores for each of the 3 exams by calculating the percentage of students that scored marks, x, within the intervals:\n\n- x < 50\n- 51 < x < 60\n- 61 < x < 70\n- 71 < x < 80\n- 81 < x < 90\n- 91 < x < 100","f8a659fa":"We can see that group C is the most common ethnic group within this dataset and we also notice that students within this group achieve scores relatively close to the average scores achieved across the entire dataset. Group E students achieve the highest marks on average. The minority group within this dataset, group A, achieve the lowest scores on average across all 3 tests, which may hint at possible discrimation or neglection of students within this ethnic group. \n\nSince ethnicity and race does seem to have an impact on test results, we shall consider using this variable with our models that we shall use to predict students' results. The current string format of this variable is unusable for machine learning algorithms. As a result, in the data preprocessing section, we shall alter the format of the variable so that we can use it within our models.\n\n#### 1.2.4 Parental Education\n\nLet us now begin to investigate the effects that a parents' level of education has on their children's test scores. As a prediction, we anticipate that the higher the level of parental education, the higher results achieved by the students, since it is common to believe that intelligence is inherited and passed on from generation to generation.\n\nWe shall first consider the differing levels of parental education we have within this dataset.","9309bee6":"We shall now use the MinMax scaler to scale our input values.","f382060a":"We shall analyse the predictions obtained by the linear regression model in the analysis section below.\n\n#### 4.1.2 Deep Learning Model using Keras\n\nWe shall now build and train a deep learning model using the Keras library. First import the necessary packages","d677883b":"No student scored zero marks in all 3 tests. Does the same thing hold true for scoring less than 20 marks?","4f6e2a8e":"The plot above shows that the error rate is lowest when the number of neighbors used is 17. We shall retrain a KNN model using this value and then analyse the performance on our training set.","aabdf4c8":"The empty dataframes above show that there were no students who did above average in one test whilst simulatenously doing below average in another, which confirms the strong linear relationship we have between the test scores. \n\n#### 3.1.2 Interquartile Range Method\n\nLet us use the interquartile range method to find scores for each test that fall outside of the region given by [LQ - 1.5 x IQR, UQ + 1.5 x IQR], where IQR is the interquartile range of test scores, LQ is the lower quartile and UQ is the upper quartile.\n\nWe can create a function to find the outliers using this method.","d83cb8a0":"Firstly, it is important to note that we contain two values that are extremely similar, namely \"high school\" and \"some high school\". Let us begin by merging these two groups into the same group.","95b967e0":"Our random forest classifier achieved an accuracy of approximately 40%, similar to that achieved by the KNN model.\n\n## 7 Conclusions\n\nIn this project we have undertaken exploratory data analysis and used our findings to create a range of models to predict student grades. Unfortunately, our models only managed to attain a 40% accuracy. This may have been as a result of attempting to predict grades in a too specific way. Rather than predict the students grades, we may have been more accurate in determining whether a student is expected to pass or fail their exams. Furthermore, there was little feature engineering in this project which may also be an underlying reason for the poor performance of our models. \n\nAs an introductory project, I hope that I have been able to demonstrate understanding of the underlying data science techniques and ideas.","18da1af4":"We can clearly see that the lunch package received by the student has a significant impact on the test results they were able to obtain, with those students who receive a free\/reduced lunch scoring approximately 9 marks lower on average than those who recieve standard lunches. Once again, this variable been shown to be an important factor on determining the results obtained by the students and, as a result, the variable will need formatting and adjusting for use within our machine learning algorithms. This will be done within the data preprocessing section. \n\nThroughout this section, we have seen that all independant variables have a significant effect on students' test scores. As a result, all columns and information within the dataset shall be used within our machine learning algorithms. \n\n## 1.3 How do our independant variables relate to each other?\n\nIn this section, we shall look at the relationship between our independant variables. Although not strictly necessary for completing our goal of predicting students exam results, this section will enable us to throughly understand our dataset and may provide useful information about the reasons behind why our variables impact the test scores in the way they do.\n\n### 1.3.1 Race\/Ethnicity vs Parental Level of Education\n\nIt is extremely commonplace that people belonging to minority ethnic groups are unable to access higher levels of education, either through lack of funding or lack of opportunity. As a result, it is interesting to see whether this is the case within our dataset.","1326d60f":"Let us investigate how the error rate changes with the numbers of neighbors used.","d9d37215":"We shall apply our own ranking system the \"race\/ethnicity\" column.","d8b4690a":"For all three tests we can clearly see that as the level of parental education increases the average score achieved by the students increases, with the increase more prominent in the writing test. This more significant increase may be due to the fact that master's degrees commonly contain lots of high level report writing and as a result a child may be exposed more naturally to advanced writing techniques and a more expansive vocabulary range. \n\nAs with the race\/ethnicity consideration above, since we have concluded that parental education has a notable impact on the test scores achieved, we will need to adjust the format of the variables so that they can be used within our machine learning algorithms. This will be done within the data preprocessing section. \n\n#### 1.2.5 Lunch\n\nFrom the data contained within our dataset, we can see that some students are provided with a free or reduced lunch. Let us investigate whether or not this has an influence on the test scores achieved. ","b21e0290":"We can create early stopping criteria in an attempt to prevent over-fitting.","cec071e0":"We can see clearly that, as anticipated, students that completed the test preparation course scored higher marks on average than those that did not. Average marks increased by 5, 7 and 10 for the maths, reading and writing tests, repsectively. We can also observe that the dispersion of marks scored was narrower in cases where the test preparation course was completed for all three tests. However, the interquartile range of the marks acheived does not change significantly as a result of the test preparation course. It appears that the preparation course solely increased the mean and median marks for each test, rather than the width of the range of marks that were scored. Despite this, it is clear that the completion of the course has a significant impact on the test results achieved by the students.\n\n#### 1.2.3 Ethnicity\n\nLet us now investigate the effect that a student's ethnicity has on the results they achieved in the three tests. We predict that race should not be an influencing factor, since all students should be treated and taught equally regardless of the ethnic backgrounds and origins.","c3939ce4":"We can now fit the model to our training data.","4223d664":"We can see that both models implemented to predict the average score obtained by the student have a mean absolute error of approximately 10 marks. In some exams, the boundaries between grades can be as little as 7 marks. As a result, our models may predict entirely incorrect grades for some student, which could cause them problems when applying for university. \n\n### 5.2 Individual Scores","3a2bf6f5":"Let us create predictions and use a confusion matrix and classification report to analyse the performance.","2bfa8dee":"Let us check to ensure that we have perfectly balanced \"grade\" classes.","ea1fb747":"This method has found what it believes to be a total of 19 outliers over the three tests. However, since we are considering test scores, low results may be as a result of poor preparation. This seems to be true since in 18 of the 19 cases we have found, the test preparation course was not completed. Let us look depper into the case found above in which the test preparation course was completed.","06c6773a":"We can see that we have classes which are extremely unbalanced. As a result, we will require the use of SMOTE to produce more samples of each of the under represented classes. Let us first drop the now unnecessary target columns from our dataset.","6af4e602":"We notice that this student's parents only achieved a high school level of education and that the student recieves free\/reduced lunch. From our analysis, this seems to be the cause of this students' poor results, since both of these factors in combination will outway the benefits that the completion of the preparation course will bring. We shall therefore leave this record within our dataset. \n\nIn conclusion, we have decided that there are no outlying cases within this dataset and will therefore use all entries in the production and evaluation of our models. \n\n### 3.2 Dummy Variables\n\nSince all of our independent variables are in 'object' format, they will not be processed by any machine learning algorithms in their current state. In order to make use of them within our models, we must convert these categorical features into numerical features through the use of dummy variables and label encoding. \n\n#### 3.2.1 Label Encoding\n\nLabel encoding is used when the categorical feature is ordinal. As a result of our analysis above, we have found that the four variables \"race\/ethnicity\", \"parental level of education\", \"lunch\" and \"test preparation course\" have a significant order and we shall therefore apply label encoding to each of them. We must first generate an instance of a label encoder.","aff7796e":"We can see that one student failed to score more than 20 marks across all three tests. However, based on the analysis of our independent variables above, this student seems to follow our findings. Since the student is female, on average she shall perform better in reading and writing than she does in maths, which is true in this case. Also, her parents only achieved high school level of education, which means that she should perform in a manner less than expected. The same point holds true due to the fact that she recieves free\/reduced lunch and did not complete the test preparation course. For these reasons, we shall keep this record in our dataset.\n\nLet us now attempt to find students who were in the top 25% of students in one test, whilst simultaneously being in the bottom 25% for another. In order to do this, we will use the values obtained from the \".describe()\" method applied to the dataframe and consider the exams in pairs.","187dab35":"The three plots shown above, one for each test result, prove our hypothesis as to how gender will affect the test scores. Males, on average, scored higher than females in the maths test with a slightly narrower standard deviation, 14.5 for males in comparison to 15.5 for females. Furthermore, all males scored at least 27 marks whereas there was at least 1 female who failed to score any points in this test. \n\nIn the reading and writing tests, as predicted, females scored higher marks than males on average, by 6 and 9 marks respectively. In all three tests, we notice considerable dispersion in the range of marks achieved. Also, the box plots produced seem to highlight points which are classed as outliers. We shall investigate potential outliers in the data preprocessing section. \n\n#### 1.2.2 Test Preparation\n\nDuring the build up to taking the exams, students were able to complete a test preparation course. We expect to see that students who completed the course scored higher marks on average than those students who chosen not to complete the course. Let us see whether the completion or non completion of this course had an effect on the results of the tests.","e1fb6ae4":"### 4.2 Predicting Individual Scores\n\nIn this section we shall create models to predict the scores that the students will obtain in each of the three test individually.\n\n#### 4.2.1 Linear Regression Model\n\nLet us recreate our X and y variables and rescale using the MinMax scaler as before.\n","c5824948":"Let us repeat the plot from above to check that our merge worked correctly.","7972be72":"We can see that in the minority group, group A, nearly 50% of students' parents' only went to high school and never went on to college or university. This percentage reduces drastically as we progress from group A to group E. Furthermore, the percentage of parents achieving a master's degree in group A is only approximately 3.5%, while this percentage increases to nearly 9% in group D. \n\nThis highlights the potential lack of opportunities for people within minority ethnic groups.\n\n### 1.3.2 Race\/Ethnicity v Lunch\n\nLet us determine whether a students' race has an effect on the likeliness that they receive free or reduced price lunch.","02908d65":"We are now ready to train our model using the training datasets.","7348ba1c":"## 4 Model Creation\n\nIn this section we will create models to predict the scores achieved by each student in all 3 exams, as well as creating models to predict the average score achieved by each student.\n\n### 4.1 Predicting Average Scores\n\nWe shall create two models, one linear regression model and one deep learning model to predict average scores.\n\n#### 4.1.1 Linear Regression Model\n\nIn order to use our data in the deep learning models we will create, it is recommended that we should scale our data. First, let us perform a train\/test split and then we shall scale our inputs.","fafdb946":"Surprisingly, it appears that the race\/ethnicity of a student affects the results they obtain. For the maths test in particular, we can see a steady increase in the average score as we work our way through the groups. This is also the case in both the reading and writing tests, however the increase is not as significant. Let us investigate how many students belong to each ethnic group.","1a1182a8":"We can clearly see that our merge has worked sucessfully as we have only one \"high school\" bar showing. Now our dataset contains 5 different levels of education, ranging from college through to a master's degree. Let us see how this range of parental edcucation affects the scores achieved by the students within the three tests.","a553153d":"Let us now investigate how many students have achieved each different grade.","a6ba0c5a":"Once again, it appears we have a similar degree of accuracy when it comes to predicting the individual test scores rather than the average score. \n\nThis now leads us to whether the original problem was the correct one to investigate. Are we able to predict grades more accurately if we consider the problem as a classification problem rather than a regression problem?\n\n## 6 Changing the Type of Problem\n\nLet us make the assumption that grades are awarded based on the average score, x, for the three tests according to the following scale:\n\n- 0 < x <= 40: FAIL,\n- 40 < x <= 50: F,\n- 50 < x <= 60: E,\n- 60 < x <= 70: D,\n- 70 < x <= 80: C,\n- 80 < x <= 90: B,\n- 90 < x <= 100: A.\n\nIf we assign categories accoring the grades in the following way,\n\n- FAIL = 0,\n- F = 1,\n- E = 2,\n- D = 3,\n- C = 4,\n- B = 5,\n- A = 6,\n\nwe can convert our regression problem into a classification problem. We shall create a new column called 'grade' to store these new values.\n","3f9e8db9":"Our models have now all been trained and used to predict either average scores or each test score individually. \n\n## 5 Model Analysis\n\nIn this section, we shall analyse the two models created for each type of regression problem. \n\n### 5.1 Average Scores.\n\nLet us import the mean squared and mean absolute error functions from Scikit-learn.","2ce7a823":"Create predictions and produce a confusion matrix and classification report.","542ab71a":"Let us now begin using the SMOTE algorithm to create balanced classes for use in our machine learning algorithms.","3c1c3347":"We can immediately notice that the race\/ethnicity of a student seems to have little effect on whether they completed the test preparation course. However, those students in ethnic group E were slightly more likely than students in the remaining groups to complete the course. \n\n### 1.3.4 Parental Level of Education vs Test Preparation Course\n\nLet us investigate whether the education level achieved by a student's parent affected the likelihood that they completed the test preparation course. ","496fd39a":"We can see that we have created a new column called \"male\" which contains a value of 0 if the student is female and 1 if the student is male. As a result, we can now drop the \"gender\" column, since all of its information is contained within the new \"male\" column.","31ab3f7c":"The percentages shown above demonstrate no clear relationship between these two variables.\n\n## 2. Feature Engineering\n\nIn the analysis section above, we found that all independant variables in the dataset already had significant impact on the test scores achieved by students. As a result, feature engineering will not be necessary as it will prove difficult to extract extra useful information from the data.\n\n## 3. Data Preprocessing\n\nIn this section, we shall process our data so that it is ready for use within our machine learning algorithms. We shall identify any potential outlying data points and determine whether they should be removed from or kept in the dataset. We shall also create dummy variables for our categorical variables. \n\n### 3.1 Outlier Detection\n\nDuring this section, we shall attempt to determine if any of the entries within our dataset seem to be outliers. \n\n#### 3.1.1 Visual Estimations\n\nIn this section, we shall attempt to find outliers by thinking of scenarios that seem unlikely. We shall first begin by finding all students who scored 100 in each of the three tests.","23f46f82":"We will create a multi layered deep network which includes the use of dropout layers. Throughout the network we shall use the rectified linear unit activation function and set a dropout probability of 0.2. When compliing the model, we shall use the 'ADAM' optimiser and the loss function we will use will be the mean squared error."}}