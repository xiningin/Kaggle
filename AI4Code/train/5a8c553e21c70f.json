{"cell_type":{"71911118":"code","b810669c":"code","9d40fec8":"code","a3bda0b1":"code","6a73b902":"code","6e3d3ec1":"code","f6fa837b":"code","f1589a2a":"code","bc249ac7":"code","47c44864":"code","4d1d1163":"code","268b955f":"code","68f4ad4f":"code","beda6032":"code","10e8769d":"code","1e905d90":"code","28ad84b4":"code","8c7df30b":"code","a1ce5996":"code","51e0ca74":"code","355ea7aa":"code","f053ded5":"code","ada07431":"code","4f29f108":"code","9356b08c":"code","8a3cafdd":"code","2ddf0c25":"code","83ddf367":"markdown","06986f18":"markdown","0c1b0675":"markdown","c3d8191d":"markdown","8d302237":"markdown","8dec99f4":"markdown","7b402f17":"markdown","10529eb2":"markdown","b721e2b0":"markdown","c2b4d3d6":"markdown","45d21f7f":"markdown","5f24aa32":"markdown","a80f5082":"markdown","9bfe3e0b":"markdown","1bb4d815":"markdown","58004896":"markdown","e20da903":"markdown","e3e5415a":"markdown","5949f5b9":"markdown","ae3eaa0b":"markdown","ab685da4":"markdown","898b5346":"markdown","9f590102":"markdown","37433030":"markdown","5ebc4aee":"markdown","88aa0df7":"markdown","98fe0f29":"markdown","28c25e4c":"markdown"},"source":{"71911118":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sea\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.layers import Activation, Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import precision_recall_curve\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom random import randrange\n\nimport matplotlib.pyplot as plt","b810669c":"sea.set_style(\"darkgrid\")","9d40fec8":"data = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\n\ndata.head(10).style.set_precision(4). \\\n                    set_properties(**{\"min-width\": \"70px\"}). \\\n                    set_properties(**{\"color\": \"#111111\"}). \\\n                    set_properties(**{\"text-align\": \"center\"}). \\\n                    set_table_styles([\n                          {\"selector\": \"th\",\n                           \"props\": [(\"font-weight\", \"bold\"),\n                                     (\"font-size\", \"12px\"),\n                                     (\"text-align\", \"center\")]},\n                          {\"selector\": \"tr:nth-child(even)\",\n                           \"props\": [(\"background-color\", \"#f2f2f2\")]},\n                          {\"selector\": \"tr:nth-child(odd)\",\n                           \"props\": [(\"background-color\", \"#fdfdfd\")]},\n                          {\"selector\": \"tr:hover\",\n                           \"props\": [(\"background-color\", \"#bcbcbc\")]}])","a3bda0b1":"data.drop(\"Time\", axis=1, inplace=True)","6a73b902":"# disable SettingWithCopyWarning\npd.options.mode.chained_assignment = None\n\ndata_X = data.loc[:, data.columns != \"Class\"]\ndata_Y = data[[\"Class\"]]\n\nprint(\"\\ndata_X info:\\n\")\ndata_X.info()\nprint(\"\\ndata_Y info:\\n\")\ndata_Y.info()","6e3d3ec1":"weights = compute_class_weight(\"balanced\",\n                    classes = data_Y[\"Class\"].unique().ravel(),\n                    y = data_Y[\"Class\"]);\n\nprint(\"Class weights \", weights)","f6fa837b":"for s in range(10):\n    data_X, data_Y = shuffle(data_X, data_Y)\n\ntrain_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y,\n                                                    test_size=0.2,\n                                                    shuffle = True,\n                                                    stratify=data_Y,\n                                                    random_state=0)\n\ntrain_X.reset_index(drop=True, inplace=True);\ntest_X.reset_index(drop=True, inplace=True);\ntrain_Y.reset_index(drop=True, inplace=True);\ntest_Y.reset_index(drop=True, inplace=True);\n\nfeature_names = train_X.columns","f1589a2a":"fig, axes = plt.subplots(len(train_X.columns), 2, figsize=(10,100))\n\nfor i, f in enumerate(train_X.columns):\n    sea.distplot(train_X[f], kde = False, color = \"#53ab52\",\n                 hist_kws = dict(alpha=1), ax=axes[i][0]);\n    sea.violinplot(x=train_Y[\"Class\"], y=train_X[f],\n                 palette = [\"#5294e3\", \"#a94157\"], ax=axes[i][1]);","bc249ac7":"# divide train_X based on Class\ntrain_X_0 = train_X[train_Y[\"Class\"] == 0]\ntrain_X_1 = train_X[train_Y[\"Class\"] == 1]\ntrain_Y_0 = train_Y[train_Y[\"Class\"] == 0]\ntrain_Y_1 = train_Y[train_Y[\"Class\"] == 1]\n\ntrain_X_0.reset_index(drop=True, inplace=True);\ntrain_X_1.reset_index(drop=True, inplace=True);\ntrain_Y_0.reset_index(drop=True, inplace=True);\ntrain_Y_1.reset_index(drop=True, inplace=True);","47c44864":"# fit isolation forest to Class 0 samples\nisolate = IsolationForest(n_estimators = 100,\n                          max_samples = 2048,\n                          max_features = 0.9,\n                          random_state = 0)\n\n# predict outliers on Class 0\nisolate.fit(train_X_0)\nout_0 = isolate.predict(train_X_0)","4d1d1163":"# drop outliers from Class 0\ntrain_X_0_ = train_X_0[out_0 == 1]\ntrain_Y_0_ = train_Y_0[out_0 == 1]\n\ntrain_X_0_.reset_index(drop=True, inplace=True);\ntrain_Y_0_.reset_index(drop=True, inplace=True);\n\n# number of dropped samples from \nprint(\"Number of outliers dropped from Class 0: {}\".\n         format(len(train_X_0)-len(train_X_0_)))","268b955f":"train_X = pd.concat([train_X_0_, train_X_1])\ntrain_Y = pd.concat([train_Y_0_, train_Y_1])\n\nfor s in range(100):\n    train_X, train_Y = shuffle(train_X, train_Y,\n                               random_state = 0)\n    \ntrain_X.reset_index(drop=True, inplace=True);\ntrain_Y.reset_index(drop=True, inplace=True);","68f4ad4f":"upsampler = SMOTE(sampling_strategy = 0.0025, random_state = 0)\n\ntrain_X, train_Y = upsampler.fit_resample(train_X, train_Y)\n\ntrain_X.reset_index(drop=True, inplace=True);\ntrain_Y.reset_index(drop=True, inplace=True);\n\nweights = compute_class_weight(\"balanced\",\n                    classes = train_Y[\"Class\"].unique().ravel(),\n                    y = train_Y[\"Class\"]);\n\nprint(\"Class weights \", weights)","beda6032":"downsampler = RandomUnderSampler(sampling_strategy = 0.005,\n                                 random_state = 2)\n\ntrain_X, train_Y = downsampler.fit_resample(train_X, train_Y)\n\ntrain_X.reset_index(drop=True, inplace=True);\ntrain_Y.reset_index(drop=True, inplace=True);\n\nweights = compute_class_weight(\"balanced\",\n                    classes = train_Y[\"Class\"].unique().ravel(),\n                    y = train_Y[\"Class\"]);\n\nprint(\"Class weights \", weights)","10e8769d":"scaler = StandardScaler()\n\n# fit to train_X\nscaler.fit(train_X)\n\n# transform train_X\ntrain_X = scaler.transform(train_X)\ntrain_X = pd.DataFrame(train_X, columns = feature_names)\n\n# transform test_X\ntest_X = scaler.transform(test_X)\ntest_X = pd.DataFrame(test_X, columns = feature_names)","1e905d90":"corr_matrix = pd.concat([train_X, train_Y], axis=1).corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=np.bool))\n\nplt.figure(figsize=(10,8))\nsea.heatmap(corr_matrix,annot=False, fmt=\".1f\", vmin=-1,\n            vmax=1, linewidth = 1,\n            center=0, mask=mask, cmap=\"RdBu_r\");","28ad84b4":"lr = LogisticRegression(max_iter = 1000, class_weight=\"balanced\")\nlr.fit(train_X, train_Y.values.ravel())\n\ns = permutation_importance(lr, train_X, train_Y, n_repeats = 20,\n                           scoring = \"f1\", random_state = 0)\n\nfor i in s.importances_mean.argsort()[::-1]:\n    print(\"{:10}\\t{: .4f}\\t{: .4f}\".format(feature_names[i],\n                                           s.importances_mean[i],\n                                           s.importances_std[i]))","8c7df30b":"for i in s.importances_mean.argsort()[:2]:\n    train_X.drop(feature_names[i], axis=1, inplace=True)\n    test_X.drop(feature_names[i], axis=1, inplace=True)\n\nfeature_num = len(train_X.columns)","a1ce5996":"np_train_X = np.array(train_X)\nnp_train_Y = np.array(train_Y)\nnp_test_X = np.array(test_X)\nnp_test_Y = np.array(test_Y)","51e0ca74":"model_count = 6\n\ndef bootstrap(X, Y):\n    \n    n_samples = len(Y)\n    \n    bag_X = []\n    bag_Y = []\n    \n    # mask stores the indices used in resampling\n    mask = np.zeros(n_samples, dtype=np.int16)\n    \n    for s in range(n_samples):\n        p = randrange(n_samples)\n        mask[p] = 1\n        \n        bag_X.append(X[p])\n        bag_Y.append(Y[p])\n    \n    # oob samples were not used in resampling\n    oobag_X = X[mask==0]\n    oobag_Y = Y[mask==0]\n    \n    return np.array(bag_X), np.array(bag_Y), \\\n           np.array(oobag_X), np.array(oobag_Y)\n    \nbs_X = []\nbs_Y = []    \noob_X = []\noob_Y = []\n\nfor b in range(model_count):\n    bag_X, bag_Y, oobag_X, oobag_Y = bootstrap(np_train_X, np_train_Y)\n    bs_X.append(bag_X)\n    bs_Y.append(bag_Y)\n    oob_X.append(oobag_X)\n    oob_Y.append(oobag_Y)","355ea7aa":"epoch = 200\n\n# Learning rate scheduler\ndef decay(inp):   \n    lr_init = 0.00001\n    lr_max = 0.001\n    lin_lr = 5\n    if inp <= lin_lr:\n        lr = inp*(lr_max - lr_init) \/ lin_lr + lr_init\n    else:\n        lr = lr_max * np.exp(-0.02*(inp - lin_lr))\n        \n    return lr\n\nlrs = LearningRateScheduler(decay)\n\nx = np.linspace(0,epoch,200)\ny = [decay(i) for i in x]\n\nplt.xticks(range(0,epoch+1,20))\nplt.plot(x,y);\nplt.ylabel(\"Learning Rate\");\nplt.xlabel(\"epoch\");","f053ded5":"def create_model():\n    \n    inp_tensor = Input(shape=(feature_num), name=\"input\")\n \n    x = Dense(2048, activation=\"relu\")(inp_tensor)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(0.3)(x)\n\n    output = Dense(1, activation=\"sigmoid\")(x)\n    \n    model = Model(inputs=inp_tensor, outputs=output,\n                  name=\"Fraud_Detector\")\n    \n    #model.summary()\n    \n    opt = tf.keras.optimizers.Adam()\n    \n    model.compile(loss=\"binary_crossentropy\",\n                optimizer=opt,\n                metrics=[tf.keras.metrics.Precision(name=\"precision\"),\n                         tf.keras.metrics.Recall(name=\"recall\")])\n    \n    return model\n\nmodels = []\n\nfor m in range(model_count):\n    models.append(create_model())","ada07431":"batch_num = 2048\n\nfor i, m in enumerate(models):\n    \n    print(\"\\nModel \" + str(i) + \"\\n\")\n    \n    # In each OOB, weights change slightly\n    weights = compute_class_weight(\"balanced\",\n                             classes = np.array(np.unique(bs_Y[i])),\n                             y = np.array(bs_Y[i].ravel()));\n    \n    # convert weights array to dict for keras\n    class_weights = {i: weights[i] for i in range(weights.shape[0])}\n\n    m.fit(bs_X[i], bs_Y[i],\n          validation_data=(oob_X[i], oob_Y[i]),\n          epochs=epoch, batch_size=batch_num,\n          class_weight=class_weights, verbose=1,\n          callbacks=[lrs])","4f29f108":"opt_thr = []\n\nfor i, m in enumerate(models):\n    # compute predictions on oob sample\n    oob_preds = m.predict(oob_X[i])\n    \n    # compute precision-recall curve\n    prec, rec, thr = precision_recall_curve(oob_Y[i], oob_preds)\n    \n    # compute optimal operating point (look for max F1-score)\n    ind = np.argmax(list(map(lambda x, y: (2*x*y)\/(x+y), prec, rec)))\n    \n    # store optimal threshold\n    opt_thr.append(thr[ind])","9356b08c":"y_prob = []\n\nfor m in models:\n    y_prob.append(m.predict(test_X))\n\nprint(\"                   Precision     Recall     F1 score\")\nprint(\"                  -----------   --------   ----------\")\nfor i, prob in enumerate(y_prob):\n    y_pred = [1 if x >= opt_thr[i] else 0 for x in prob]\n    p, r, f, _ = precision_recall_fscore_support(test_Y,\n                                    y_pred, average=\"binary\")\n    print(\"Neural Network {}:    {:.4f}      {:.4f}      {:.4f}\".\n                                          format(i, p, r, f))","8a3cafdd":"e_thr = np.median(opt_thr)\n\ny_ensemble = np.mean(y_prob, axis=0)\ny_pred = [1 if x >= e_thr else 0 for x in y_ensemble]\np, r, f, _ = precision_recall_fscore_support(test_Y, y_pred,\n                                             average=\"binary\")\nprint(\"                   Precision     Recall     F1 score\")\nprint(\"                  -----------   --------   ----------\")\nprint(\"Bagging Ensemble:    {:.4f}      {:.4f}      {:.4f}\".\n                                          format( p, r, f))","2ddf0c25":"prec, rec, thr = precision_recall_curve(test_Y, y_ensemble)\nplt.figure(figsize=(6,6))\nsea.lineplot(x=rec, y=prec);\nplt.ylabel(\"Precision\");\nplt.xlabel(\"Recall\");","83ddf367":"## Load Data\n\nWe use **Credit Card Fraud Detection** dataset. The dataset contains transactions made by credit cards in September 2013. There are 492 frauds out of 284807 transactions.\n\nFeatures from V1 to V28 are the principal components obtained with PCA. Feature **Time** contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature **Amount** is the transaction amount. **Class** is the target variable.\n\nWe load dataset from input csv file using **Pandas**. **Pandas** extracts the data and stores it in a dataframe.","06986f18":"## Bootstrap Samples\n\n**Scikit-learn** **resample** function can be used to get **bootstrap** samples but it does not give the **out of bag (OOB)** samples. **OOB** samples are the ones that are not chosen during resampling. **OOB** samples are used for validation. 6 bootstrap samples for 6 ensemble members are created with **custom bootstrapping function** below. Each sample has the same size with training set. Two empty lists, one for features and one for labels, are created to store **bootstrap** samples and two empty lists are created to store **OOB** samples.","0c1b0675":"## Outlier Check with Isolation Forest\n\nIsolation Forest is used to detect outliers. The idea is to isolate each sample with random trees. Each tree is trained on a subset of training data. Since outliers are different from the general trend (in terms of feature values), it is easy to isolate them. The path from root node to a sample in a random tree is expected to be shorter for an outlier. The path length is averaged over a forest of random trees.\n\nSince the number of Class 1 samples is very low, outlier analysis is done only on Class 0 samples. Divide train_X based on Class values.","c3d8191d":"Dataset is severely imbalanced. Class weights computed above will be fed to **Keras** fit function during training. Class weights are used in loss computation, loss related to samples coming from Class 1 will be multiplied with a higher coefficient to compensate the sparseness of Class 1 samples.","8d302237":"As can be understood from color saturation in correlation matrix cells, there aren't any serious correlation issues, the correlations are at low and mid-level.","8dec99f4":"Fit IsolationForest to only Class 0 samples.","7b402f17":"## Correlation Analysis","10529eb2":"Drop Class 0 outliers.","b721e2b0":"Dataset has 284807 rows (training samples). All features are numerical (continuous) and there aren't any null values. Let's look at the target Class column.","c2b4d3d6":"Class 1 is upsampled such that the ratio of samples of Class 1 to Class 0 becomes 0.0025","45d21f7f":"Combine Class 0 and 1 samples again and shuffle.","5f24aa32":"## Minority Class Upsample with SMOTE","a80f5082":"Averaging predicted probabilities decreases the variance.\n\nPrecision-recall curve of the ensemble is depicted below","9bfe3e0b":"## Standardization\n\nStandardScaler is only fit to training data to prevent data leakage.","1bb4d815":"## Permutation Feature Importance\n\nPermutation feature importance method is used to evaluate the effect of each feature on the classification task. It is defined as the decrease in model performance when a specific feature is shuffled. Process is repeated a number of times for each feature and mean decrease is taken into account.","58004896":"Features and corresponding labels are assigned to **data_X** and **data_Y**, respectively. Using **Pandas** **info** function, we inspect column data types and number of non-null values in **data_X** and **data_Y**.","e20da903":"## Majority Class Downsample with RandomUnderSampler\n\nClass 0 is downsampled such that the ratio of samples of Class 1 to Class 0 becomes 0.005","e3e5415a":"The least important 2 features are dropped. Note that these features have very low correlation with Class (refer to correlation matrix).","5949f5b9":"Time feature is dropped.","ae3eaa0b":"Upsampling and downsampling should be mild, because too much upsampling or downsampling have side effects like increasing the correlation between features.","ab685da4":"## Testing\n\nEach neural network makes makes slightly different predictions for each test sample due to random initialization and high variance of neural networks.","898b5346":"Each neural network model is trained on a seperate **bootstrap sample**. Validation is done using **OOB** samples.","9f590102":"## Visualization\n\nFor each feature, a histogram and a violin plot (feature vs target) are drawn.","37433030":"Next, we evaluate **bagging ensemble** which is the average of all models. **y_ensemble** holds the average decisions for all samples in **test_X**. As classification threshold, we use the average of thresholds computed above for each method.","5ebc4aee":"## Split Data\n\nDataset is split as training and test sets. We use stratify parameter of train_test_split function to get the same class distribution across train and test sets.","88aa0df7":"In this notebook, we experiment with **neural networks** and observe how the predictions change as we apply modifications to training set. Why are **neural networks** defined as **high variance** machine learning methods? We will answer this question.\n\nLater, we look for a remedy to high variance problem and use an ensemble of neural networks. Each member of the ensemble is trained on a slightly different subset of training data. Each subset is obtained by resampling the original training set with replacement, a method known as **bootstrapping**. For inference, the decisions coming from each ensemble member are aggregated. Whole method is called **bootstrap aggregating** or **bagging**.\n\n* Load Data\n* Split Data\n* Visualization\n* Outlier Check with Isolation Forest\n* Outlier Check with IQR\n* Minority Class Upsample with SMOTE\n* Majority Class Downsample with RandomUnderSampler\n* Standardization\n* Correlation Analysis\n* Permutation Feature Importance\n* Bootstrap Samples\n* Training and Validation\n* Testing\n","98fe0f29":"## Training and Validation\n\nA neural network model is created using functional API of Keras with TensorFlow backend. Since we are dealing with a severely imbalanced data set, the performance metrics are precision and recall.\n\nA learning rate schedule is defined. First, it starts very low, then ramps up linearly upto max learning rate. Neural network is initialized with random numbers. At first epochs, it is better to have small learning rates for the adaptation of weights. After reaching max, learning rate decays exponentially.","28c25e4c":"Neural networks output probabilities. In binary case, we need a threshold to convert a probability to one of two classes. OOB samples can be used to determine the thresholds for each classifier. Precision-recall curves are drawn to find optimal operating point and threshold. We find the precision-recall pair which is giving the highest F1-score."}}