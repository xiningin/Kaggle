{"cell_type":{"4a7c1aa0":"code","8eefd34b":"code","a3b314ab":"code","7cef4be3":"code","1990bd8d":"code","855a5ac7":"code","a4f04c38":"code","56102c61":"code","99dc7f28":"code","75c06368":"code","f5df51ae":"code","b5d171f7":"code","9f4063aa":"code","1c5bbb86":"code","29aad750":"code","516c8fbc":"code","febe3719":"code","cbeb64c9":"markdown","c4447bb1":"markdown","42cf497b":"markdown","b8eb4f0a":"markdown","f15276f9":"markdown","b2b0db79":"markdown","15bbf64c":"markdown","80275157":"markdown","75b36472":"markdown","0a5cddc5":"markdown","8b00468f":"markdown","3c3af525":"markdown","2161ffd2":"markdown","98a6e2af":"markdown","47816c89":"markdown","fb0e8bc5":"markdown"},"source":{"4a7c1aa0":"import numpy as np\nimport pandas as pd\nimport io\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer \nimport os, re, csv, math, codecs\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport torch\nimport torch.nn as nn\nimport tensorflow as tf  # we use both tensorflow and pytorch (pytorch for main part) , tensorflow for tokenizer\n\ntorch.manual_seed(42);","8eefd34b":"# Read data\ndf = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf.head(3)","a3b314ab":"# Convert sentiment columns to numerical values\ndf.sentiment = df.sentiment.apply(lambda x: 1 if x=='positive' else 0)\n## Cross validation \n# create new column \"kfold\" and assign a random value\ndf['kfold'] = -1\n# Random the rows of data\ndf = df.sample(frac=1).reset_index(drop=True)\n# get label\ny = df.sentiment.values\n# initialize kfold\nkf = model_selection.StratifiedKFold(n_splits=5)\n# fill the new values to kfold column\nfor fold, (train_, valid_) in enumerate(kf.split(X=df, y=y)):\n    df.loc[valid_, 'kfold'] = fold\ndf.head(3)","7cef4be3":"#load fasttext embeddings\nprint('loading word embeddings...')\nfasttext_embedding = {}\nf = codecs.open('..\/input\/fasttext\/wiki.simple.vec', encoding='utf-8')\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    fasttext_embedding[word] = coefs\nf.close()","1990bd8d":"# Different embedding version will have different dimension. \n# we have to check the dimension of this fasttext embedding version\n# Because this step is important,coz it relate to the later step when we define the dimension for embedding matrix\nfasttext_embedding['hello'].shape","855a5ac7":"# Load Standford Glove embedding.\nglove = pd.read_csv('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)\nglove_embedding = {key: val.values for key, val in glove.T.items()}\n\n# Check check the dimension of this fasttext embedding version\nglove_embedding['hello'].shape","a4f04c38":"\nclass IMDBDataset:\n    def __init__(self, reviews, targets):\n        \"\"\"\n        Argument:\n        reviews: a numpy array\n        targets: a vector array\n        \n        Return xtrain and ylabel in torch tensor datatype, stored in dictionary format\n        \"\"\"\n        self.reviews = reviews\n        self.target = targets\n    \n    def __len__(self):\n        # return length of dataset\n        return len(self.reviews)\n    \n    def __getitem__(self, index):\n        # given an idex (item), return review and target of that index in torch tensor\n        review = torch.tensor(self.reviews[index,:], dtype = torch.long)\n        target = torch.tensor(self.target[index], dtype = torch.float)\n        \n        return {'review': review,\n                'target': target}","56102c61":"class LSTM(nn.Module):\n    def __init__(self, embedding_matrix):\n        \"\"\"\n        Given embedding_matrix: numpy array with vector for all words\n        return prediction ( in torch tensor format)\n        \"\"\"\n        super(LSTM, self).__init__()\n        # Number of words = number of rows in embedding matrix\n        num_words = embedding_matrix.shape[0]\n        # Dimension of embedding is num of columns in the matrix\n        embedding_dim = embedding_matrix.shape[1]\n        # Define an input embedding layer\n        self.embedding = nn.Embedding(\n                                      num_embeddings=num_words,\n                                      embedding_dim=embedding_dim)\n        # Embedding matrix actually is collection of parameter\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype = torch.float32))\n        # Because we use pretrained embedding (GLove, Fastext,etc) so we turn off requires_grad-meaning we do not train gradient on embedding weight\n        self.embedding.weight.requires_grad = False\n        # LSTM with hidden_size = 128\n        self.lstm = nn.LSTM(\n                            embedding_dim, \n                            128,\n                            bidirectional=True,\n                            batch_first=True,\n                             )\n        # Input(512) because we use bi-directional LSTM ==> hidden_size*2 + maxpooling **2  = 128*4 = 512, will be explained more on forward method\n        self.out = nn.Linear(512, 1)\n    def forward(self, x):\n        # pass input (tokens) through embedding layer\n        x = self.embedding(x)\n        # fit embedding to LSTM\n        hidden, _ = self.lstm(x)\n        # apply mean and max pooling on lstm output\n        avg_pool= torch.mean(hidden, 1)\n        max_pool, index_max_pool = torch.max(hidden, 1)\n        # concat avg_pool and max_pool ( so we have 256 size, also because this is bidirectional ==> 256*2 = 512)\n        out = torch.cat((avg_pool, max_pool), 1)\n        # fit out to self.out to conduct dimensionality reduction from 512 to 1\n        out = self.out(out)\n        # return output\n        return out","99dc7f28":"def train(data_loader, model, optimizer, device):\n    \"\"\"\n    this is model training for one epoch\n    data_loader:  this is torch dataloader, just like dataset but in torch and devide into batches\n    model : lstm\n    optimizer : torch optimizer : adam\n    device:  cuda or cpu\n    \"\"\"\n    # set model to training mode\n    model.train()\n    # go through batches of data in data loader\n    for data in data_loader:\n        reviews = data['review']\n        targets = data['target']\n        # move the data to device that we want to use\n        reviews = reviews.to(device, dtype = torch.long)\n        targets = targets.to(device, dtype = torch.float)\n        # clear the gradient\n        optimizer.zero_grad()\n        # make prediction from model\n        predictions = model(reviews)\n        # caculate the losses\n        loss = nn.BCEWithLogitsLoss()(predictions, targets.view(-1,1))\n        # backprob\n        loss.backward()\n        #single optimization step\n        optimizer.step()","75c06368":"def evaluate(data_loader, model, device):\n    final_predictions = []\n    final_targets = []\n    model.eval()\n    # turn off gradient calculation\n    with torch.no_grad():\n        for data in data_loader:\n            reviews = data['review']\n            targets = data['target']\n            reviews = reviews.to(device, dtype = torch.long)\n            targets = targets.to(device, dtype=torch.float)\n            # make prediction\n            predictions = model(reviews)\n            # move prediction and target to cpu\n            predictions = predictions.cpu().numpy().tolist()\n            targets = data['target'].cpu().numpy().tolist()\n            # add predictions to final_prediction\n            final_predictions.extend(predictions)\n            final_targets.extend(targets)\n    return final_predictions, final_targets","f5df51ae":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 8\nEPOCHS = 5","b5d171f7":"def create_embedding_matrix(word_index, embedding_dict=None, d_model=100):\n    \"\"\"\n     this function create the embedding matrix save in numpy array\n    :param word_index: a dictionary with word: index_value\n    :param embedding_dict: a dict with word embedding\n    :d_model: the dimension of word pretrained embedding, here I just set to 100, we will define again\n    :return a numpy array with embedding vectors for all known words\n    \"\"\"\n    embedding_matrix = np.zeros((len(word_index) + 1, d_model))\n    ## loop over all the words\n    for word, index in word_index.items():\n        if word in embedding_dict:\n            embedding_matrix[index] = embedding_dict[word]\n    return embedding_matrix","9f4063aa":"# STEP 1: Tokenization\n# use tf.keras for tokenization,  \ntokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(df.review.values.tolist())","1c5bbb86":"print('Load fasttext embedding')\nembedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=fasttext_embedding, d_model=300)\n\n# I just run 1 fold to reduce the time. You can try more fold to get better generalization\nfor fold in range(1):\n    # STEP 2: cross validation\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n    \n    # STEP 3: pad sequence\n    xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n    xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n    \n    # zero padding\n    xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n    xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n    \n    # STEP 4: initialize dataset class for training\n    train_dataset = IMDBDataset(reviews=xtrain, targets=train_df.sentiment.values)\n    \n    # STEP 5: Load dataset to Pytorch DataLoader\n    # after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n    # initialize dataset class for validation\n    valid_dataset = IMDBDataset(reviews=xtest, targets=valid_df.sentiment.values)\n    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n    \n    # STEP 6: Running \n    device = torch.device('cuda')\n    # feed embedding matrix to lstm\n    model_fasttext = LSTM(embedding_matrix)\n    # set model to cuda device\n    model_fasttext.to(device)\n    # initialize Adam optimizer\n    optimizer = torch.optim.Adam(model_fasttext.parameters(), lr=1e-3)\n    \n    print('training model')\n   \n    for epoch in range(EPOCHS):\n        #train one epoch\n        train(train_data_loader, model_fasttext, optimizer, device)\n        #validate\n        outputs, targets = evaluate(valid_data_loader, model_fasttext, device)\n        # threshold\n        outputs = np.array(outputs) >= 0.5\n        # calculate accuracy\n        accuracy = metrics.accuracy_score(targets, outputs)\n        print(f'FOLD:{fold}, epoch: {epoch}, accuracy_score: {accuracy}')","29aad750":"print('Load Glove embedding')\nembedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=glove_embedding, d_model=100)\n\nfor fold in range(1):\n    # STEP 2: cross validation\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n    \n    # STEP 3: pad sequence\n    xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n    xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n    \n    # zero padding\n    xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n    xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n    \n    # STEP 4: initialize dataset class for training\n    train_dataset = IMDBDataset(reviews=xtrain, targets=train_df.sentiment.values)\n    \n    # STEP 5: Load dataset to Pytorch DataLoader\n    # after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n    # initialize dataset class for validation\n    valid_dataset = IMDBDataset(reviews=xtest, targets=valid_df.sentiment.values)\n    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n    \n    # STEP 6: Running \n    device = torch.device('cuda')\n    # feed embedding matrix to lstm\n    model_glove = LSTM(embedding_matrix)\n    # set model to cuda device\n    model_glove.to(device)\n    # initialize Adam optimizer\n    optimizer = torch.optim.Adam(model_glove.parameters(), lr=1e-3)\n    \n    print('training model')\n   \n    for epoch in range(EPOCHS):\n        #train one epoch\n        train(train_data_loader, model_glove, optimizer, device)\n        #validate\n        outputs, targets = evaluate(valid_data_loader, model_glove, device)\n        # threshold\n        outputs = np.array(outputs) >= 0.5\n        # calculate accuracy\n        accuracy = metrics.accuracy_score(targets, outputs)\n        print(f'FOLD:{fold}, epoch: {epoch}, accuracy_score: {accuracy}')","516c8fbc":"def Interact_user_input(model):\n    '''\n    model: trained model : fasttext model or glove model\n    '''\n    model.eval()\n    \n    sentence = ''\n    while True:\n        try:\n            sentence = input('Review: ')\n            if sentence in ['q','quit']: \n                break\n            sentence = np.array([sentence])\n            sentence_token = tokenizer.texts_to_sequences(sentence)\n            sentence_token = tf.keras.preprocessing.sequence.pad_sequences(sentence_token, maxlen = MAX_LEN)\n            sentence_train = torch.tensor(sentence_token, dtype = torch.long).to(device, dtype = torch.long)\n            predict = model(sentence_train)\n            if predict.item() > 0.5:\n                print('------> Positive')\n            else:\n                print('------> Negative')\n        except KeyError:\n            print('please enter again')\n    ","febe3719":"Interact_user_input(model_fasttext)","cbeb64c9":"![fasttext.webp](attachment:fasttext.webp)\n\nFasttext is a word embedding development by Facebook released in 2016.\nFastText improves on Word2Vec by taking word parts into account, enables training of embeddings on smaller datasets and generalization to unknown words.\n\nThe full version of fasttext can be found here : https:\/\/fasttext.cc\/docs\/en\/english-vectors.html \n\nDue to the size of memory, ( the full version will be around 13GB RAM after loading), so I use the mini version of fasttext.  If you use the full version just replace the path and everything will be running normally on local machine (RAM atleast 15GB to run smoothly)","c4447bb1":"# **1. Processing dataset** <a class=\"anchor\" id=\"1\"><\/a>\n\n[Go back to table of contents](#0.1)","42cf497b":"## Enter reviews and test the result","b8eb4f0a":"After we building some helper function, now is time to running the model. \nThe entire workflow will be as following steps:\n\n==> **Step1**: Creating a tokenizer function to convert sentences of dataset to token index\nAfter converting we'll have a dictionary contain word and its index. We feed it to creating an embedding matrix\n\n==> **Step2**: Cross validation of dataset to devide into train_df and valid_df\n\n==> **Step3**: Applying tokenizer pad_sequence to token index to ensure all sentence has the same vector dimension ( example: the sentence with 10 words will have longer vector dimension then the sentence with 2 words, using pad_sequence to ensure the same length, The length is set to a fixed number)\n\nI use tokenizer from tensorflow 2 because of its convenience in pad_sequence. you can use tokenizer and pad_sequence from pytorch too but I felt it's quite longer implementation. If you would like to use pytorch pad_sequence, You can visit my other work that I have used pytorch\nhttps:\/\/www.kaggle.com\/tientd95\/understanding-attention-in-neural-network\n\n**Remember after zero_padding, the result would return in numpy array datatype**\n\n==> **Step4**: Initialize Dataset class, actually this step is just convet the numpy array (in Step3) to torch tensor \n\n==> **Step5**: We load the Dataset which created in step4 to Pytorch DataLoader in order to devide the dataset to batches\n\n==> **Step6**: Till now, we have almost necessary components to start training. Calling model, optimizer, send model to device and start running","f15276f9":"## Config","b2b0db79":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n# **Table of Contents**\n\n\n1.\t[Processing dataset](#1)\n2.  [Pretrained word embedding](#2)\n3.  [Building model pipeline](#3)\n4.  [Training model with Fasttext embedding](#4)\n5.  [Training model with Glove embedding](#5)\n6.  [Interact with User's input review](#6)\n","15bbf64c":"# **5. Training model with Glove embedding** <a class=\"anchor\" id=\"5\"><\/a>\n\n[Go back to table of contents](#0.1)\n\n\nRemember Glove embedding version in this kernel is 100,  So we set d_model =100","80275157":"# **6. Interact with User's input review** <a class=\"anchor\" id=\"6\"><\/a>\n\n[Go back to table of contents](#0.1)\n\nNow it's time to test model by entering any review we can think and see the model reaction","75b36472":"### Dataset class\n\nFirst we need to create a Dataset class, take input in numpy array(embedding matrix) and return torch tensor output datatype ","0a5cddc5":"# **2. Pretrained word embedding** <a class=\"anchor\" id=\"2\"><\/a>\n\n[Go back to table of contents](#0.1)","8b00468f":"# **4. Training model with Fasttext embedding** <a class=\"anchor\" id=\"4\"><\/a>\n\n[Go back to table of contents](#0.1)\n\n\nRemember fasttext embedding version in this kernel is 300,  So we set d_model =300","3c3af525":"After buidling the model class, we move to create train and evaluate function","2161ffd2":"This kernel is a complete guide on training neural net for sentiment analysis . From loading pretrained embedding to test the model performance on User's input. Different pretrained embeddings (Fasttext, Glove,..) will be used in order to compare their performance.    \n\nAfter running this kernel, we can play with model and see the result like this\n\n![Test_user_input.JPG](attachment:Test_user_input.JPG)\n\nWith my best effort, I hope you will find this guide is comfortable and useful.\n\nThank you","98a6e2af":"![Capture.JPG](attachment:Capture.JPG)\n\nGlove is a word embedding development by Standford released in 2014.\nGloVe use the frequency of co-occurrences as vital information and should not be \u201cwasted \u201das additional training examples. GloVe builds word embeddings in a way that a combination of word vectors relates directly to the probability of these words\u2019 co-occurrence in the corpus.\n","47816c89":"# **3. Building model pipeline** <a class=\"anchor\" id=\"3\"><\/a>\n\n[Go back to table of contents](#0.1)\n","fb0e8bc5":"**Now we move on to build a model class. Before that, there's something to remembe**r:\n\n* The input feed to model is served as embedding matrix (each row corresponding to an embedding vector of a word)\n* Number of words (for entire dataset) = number of row in embeddng matrix \n* Dimension of embedding is the num of columns in matrix, = dimention of pretrained embedding (fasttext, glove,..in case we use pretrained embedding). \n* Pretrained embeddings have several versions with different dimension so we should check the dimension before set dimension to model.\n* In case we use pretrainde embedding (this kernel), we will not do gadient calculation on these embedding (required grads = False)\n* In case we train embedding from scratch, we will treat embedding matrix as weight parameter and training on them (required grads = True)\n"}}