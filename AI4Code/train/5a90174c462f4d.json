{"cell_type":{"493c6595":"code","100706b1":"code","0e8314f0":"code","832932ba":"code","3a875aa8":"code","0fc95448":"code","a27d823e":"code","c47d9884":"code","f560182b":"code","6edaffad":"code","14f3285a":"code","28ad0438":"code","a0cc5be0":"code","fdb9e6bf":"code","02e706a8":"code","9229c0a2":"code","5ec60045":"code","9273a5c1":"code","dbedd695":"code","5a2c38a1":"code","a00caa1c":"code","e224673a":"code","e5364f52":"code","44190e47":"code","d244f410":"code","e0755214":"code","388e2586":"code","2ffb4cc5":"code","803dc37c":"code","35d81634":"code","2af8a207":"code","7a112b5e":"code","8cfc169d":"code","26e60f65":"code","c387ff72":"code","26909a0e":"code","fc46a616":"code","60ed9d24":"code","e66ab538":"code","b1fceb9e":"code","98870f60":"code","ea8f7b43":"code","6ba00fce":"code","c7a9d921":"code","bcad768b":"code","e0525140":"code","3cfa97d9":"code","20c25e2e":"code","85ebab2a":"code","8de1039d":"code","35fc5391":"code","93b53251":"code","e79137b7":"code","b2bba51c":"code","78ceca79":"code","24a01a10":"code","11d903f2":"code","78cb62ce":"code","6c12cd66":"code","9a3e4e14":"code","b281bb1e":"code","01d87a5f":"code","25613391":"code","5be36008":"code","cd644fa0":"code","ce7bbcec":"code","74430fe3":"code","b39e9640":"code","68478ea5":"code","4fbddd3c":"code","9cbae929":"code","b5dbdd64":"code","84d07b4d":"code","e114c270":"code","1f5f7e47":"code","caad069f":"code","ba171be1":"code","d0dffdd5":"code","cea7569b":"code","760583d0":"code","50b8461a":"markdown","5583dd9a":"markdown","e1020e83":"markdown","6bd9fcac":"markdown","215e4159":"markdown","93603917":"markdown","be1055eb":"markdown","b186fbdd":"markdown","4547a07c":"markdown","6e7dcaf7":"markdown","5fb4e43d":"markdown","88aaf8b4":"markdown","6257342d":"markdown","83498919":"markdown","fe621765":"markdown","6bc97c5a":"markdown","a1d14cee":"markdown","56004482":"markdown","f8346e10":"markdown","d337228f":"markdown","836fecb2":"markdown","bbc4f5a3":"markdown","0941f2e6":"markdown","5d517387":"markdown","69d64d42":"markdown","7edb7033":"markdown","7a117390":"markdown","864ac5e9":"markdown","05958112":"markdown","106d8dd7":"markdown","6175507b":"markdown","111bd086":"markdown","a3d10b97":"markdown","991c51c5":"markdown","6e49beb6":"markdown","f81365d8":"markdown","fd6c2351":"markdown","7e30eafc":"markdown","db829a1c":"markdown","1810051a":"markdown","df9310b4":"markdown","b53f07a5":"markdown","8e558935":"markdown","d6546787":"markdown","7464badf":"markdown","0aa37e77":"markdown"},"source":{"493c6595":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.gridspec as gridspec\nimport warnings\nfrom sklearn.model_selection import train_test_split\n\nwarnings.filterwarnings('ignore')","100706b1":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","0e8314f0":"data.head()","832932ba":"data = data.rename(columns={\"Class\": \"Fraud\"})","3a875aa8":"data.head()","0fc95448":"print(f'Total of data rows are:',data.shape[0],',and the total columns are:',data.shape[1])","a27d823e":"print(f'Total missing value is',data.isnull().sum()) ","c47d9884":"data['Fraud'].value_counts()","f560182b":"(data.Time[data['Fraud'] == 1].count() \/ data.Time[data['Fraud'] == 0].count())*100","6edaffad":"print(\"Fraud\")\nprint(data[data['Fraud'] ==1].describe())","14f3285a":"print(\"Not Fraud\")\nprint(data[data['Fraud'] ==0].describe())","28ad0438":"print('Fraud')\nprint (data.Time[data.Fraud == 1].describe())\nprint()\nprint('Normal')\nprint(data.Time[data.Fraud == 0].describe())","a0cc5be0":"from sklearn.preprocessing import StandardScaler, RobustScaler\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndata['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))","fdb9e6bf":"data.head()","02e706a8":"corr = data.iloc[:,:].corr().sort_values(by='Fraud',ascending=False).round(2)","9229c0a2":"corr","5ec60045":"abscorr = corr[abs(corr['Fraud']) > 0.15].index","9273a5c1":"cm = np.corrcoef(data[abscorr].values.T)","dbedd695":"sns.set(font_scale=1)\nhm = sns.heatmap(cm,annot=True,yticklabels=abscorr.values, xticklabels=abscorr.values)\nplt.show()","5a2c38a1":"df = data.loc[:,['Time','V17','V14','V12','V10','V16','V3','V7','V18','V4','Amount','Fraud']]","a00caa1c":"df.head()","e224673a":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(16,8))\n\nax1.hist(df.Time[df.Fraud == 1], bins = 80)\nax1.set_title('Fraud')\n\nax2.hist(df.Time[df.Fraud == 0], bins = 80)\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Number of Transactions')\nplt.show()","e5364f52":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nbins = 25\n\nax1.hist(df.Amount[df.Fraud == 1], bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(df.Amount[df.Fraud == 0], bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.yscale('log')\nplt.show()","44190e47":"df[df['Fraud'] == 1]","d244f410":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,6))\n\nax1.scatter(df.Time[df.Fraud == 1], df.Amount[df.Fraud == 1])\nax1.set_title('Fraud')\n\nax2.scatter(df.Time[df.Fraud == 0], df.Amount[df.Fraud == 0])\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","e0755214":"temp = df[df['Amount']<20000]","388e2586":"temp","2ffb4cc5":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,6))\n\nax1.scatter(df.Time[df.Fraud == 1], df.Amount[df.Fraud == 1])\nax1.set_title('Fraud')\n\nax2.scatter(df.Time[df.Fraud == 0], df.Amount[df.Fraud == 0])\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","803dc37c":"df.head()","35d81634":"v_features = df.iloc[:,1:9].columns\nv_features","2af8a207":"plt.figure(figsize=(20,100))\ngs = gridspec.GridSpec(9, 1)\nfor i, cn in enumerate(df[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[cn][df['Fraud'] == 1], bins=50)\n    sns.distplot(df[cn][df['Fraud'] == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(cn))\nplt.show()","7a112b5e":"v_features","8cfc169d":"df['V3_FCheck'] = df.V3.map(lambda x: 1 if x < -4 else 0)\ndf['V7_FCheck'] = df.V7.map(lambda x: 1 if x < -3 else 0)\ndf['V10_FCheck'] = df.V10.map(lambda x: 1 if x < -2.5 else 0)\ndf['V12_FCheck'] = df.V12.map(lambda x: 1 if x < -2 else 0)\ndf['V14_FCheck'] = df.V14.map(lambda x: 1 if x < -2.5 else 0)\ndf['V16_FCheck'] = df.V16.map(lambda x: 1 if x < -2 else 0)\ndf['V17_FCheck'] = df.V17.map(lambda x: 1 if x < -2 else 0)\ndf['V18_FCheck'] = df.V18.map(lambda x: 1 if x < -2 else 0)","26e60f65":"df.head()","c387ff72":"df = df.drop(['Time','V17','V14','V12','V10','V16','V3','V7','V18','V4','Amount'],axis=1)\ndf.head()","26909a0e":"y_imba = df.iloc[:,0]\ny_imba","fc46a616":"X_imba = df.iloc[:,1:]\nX_imba","60ed9d24":"X_imbatrain, X_imbatest, y_imbatrain,y_imbatest = train_test_split(X_imba,y_imba, test_size=0.3,random_state=123)","e66ab538":"from sklearn.naive_bayes import GaussianNB \nmodel = GaussianNB()                       \nmodel.fit(X_imbatrain, y_imbatrain)                 \ny_model = model.predict(X_imbatest)  ","b1fceb9e":"from sklearn.metrics import accuracy_score\n","98870f60":"print(f'the accuracy of this model:',100*accuracy_score(y_imbatest, y_model))","ea8f7b43":"from sklearn.linear_model import LogisticRegression\nLogRmodel = LogisticRegression()\nLogRmodel.fit(X_imbatrain,y_imbatrain)\nLogRpredict = LogRmodel.predict(X_imbatest)","6ba00fce":"print(f'the accuracy of this model:',100*accuracy_score(y_imbatest,LogRpredict))","c7a9d921":"from sklearn import svm\nSVMmodel = svm.SVC()\nSVMmodel.fit(X_imbatrain, y_imbatrain)\nSVMpredict = SVMmodel.predict(X_imbatest)","bcad768b":"print(f'the accuracy of this model:',100*accuracy_score(y_imbatest,SVMpredict))","e0525140":"from sklearn import tree\nDTmodel = tree.DecisionTreeClassifier()\nDTmodel.fit(X_imbatrain,y_imbatrain)\nDTpredict = DTmodel.predict(X_imbatest) ","3cfa97d9":"DTpredict","20c25e2e":"print(f'the accuracy of this model:',100*accuracy_score(y_imbatest,DTpredict))","85ebab2a":"\ntree.plot_tree(DTmodel,);","8de1039d":"# Shuffle the data before creating subsamples\ndata = data.sample(frac=1)","35fc5391":"fraud_df = data.loc[data['Fraud'] == 1]\nnon_fraud_df = data.loc[df['Fraud'] == 0][:492]","93b53251":"normal_df = pd.concat([fraud_df,non_fraud_df])\n\n#Shuffle data frame rows\nnew_df = normal_df.sample(frac = 1 , random_state=42)\n\nnew_df.head()","e79137b7":"new_df['Fraud'].value_counts()","b2bba51c":"new_df.head()","78ceca79":"new_df = new_df.drop(['Amount','Time'],axis=1)","24a01a10":"new_df","11d903f2":"corr = new_df.iloc[:,:].corr().sort_values(by='Fraud',ascending=False).round(2)\ncorr","78cb62ce":"hicorr = abs(corr['Fraud']).sort_values(ascending=False).head(15)","6c12cd66":"hicorrindex = abs(corr['Fraud']).sort_values(ascending=False).head(15).index","9a3e4e14":"hicorr","b281bb1e":"corrcoef = np.corrcoef(new_df[hicorrindex].values.T)","01d87a5f":"plt.subplots(figsize=(12,9))\nsns.set(font_scale=1)\nhm = sns.heatmap(corrcoef,annot=True,yticklabels=abscorr.values, xticklabels=abscorr.values)\nplt.show()","25613391":"new_df.head()","5be36008":"v_analyze = new_df[hicorrindex].iloc[:,0:28].columns\nv_analyze","cd644fa0":"plt.figure(figsize=(20,90))\ngs = gridspec.GridSpec(16, 1)\nfor i, cn in enumerate(new_df[v_analyze]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(new_df[cn][new_df['Fraud'] == 1], bins=50)\n    sns.distplot(new_df[cn][new_df['Fraud'] == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(cn))\nplt.show()","ce7bbcec":"hicorr_df = new_df[hicorrindex]\nhicorr_df","74430fe3":"hicorr_df['V14_'] = hicorr_df.V14.map(lambda x: 1 if x < -2.5 else 0)\nhicorr_df['V4_'] =  hicorr_df.V4.map(lambda x: 1 if x > 2.5 else 0)\nhicorr_df['V11_'] = hicorr_df.V11.map(lambda x: 1 if x > 2 else 0)\nhicorr_df['V12_'] = hicorr_df.V12.map(lambda x: 1 if x < -2 else 0)\nhicorr_df['V10_'] = hicorr_df.V10.map(lambda x: 1 if x < -2.5 else 0)\nhicorr_df['V16_'] = hicorr_df.V16.map(lambda x: 1 if x < -2 else 0)\nhicorr_df['V3_'] =  hicorr_df.V3.map(lambda x: 1 if x < -4 else 0)\nhicorr_df['V17_'] = hicorr_df.V17.map(lambda x: 1 if x < -2 else 0)\nhicorr_df['V9_'] =  hicorr_df.V9.map(lambda x: 1 if x < -2 else 0)\nhicorr_df['V2_'] =  hicorr_df.V2.map(lambda x: 1 if x > 2.5 else 0)\nhicorr_df['V7_'] =  hicorr_df.V7.map(lambda x: 1 if x < -3 else 0)\nhicorr_df['V18_'] = hicorr_df.V18.map(lambda x: 1 if x < -2 else 0)\nhicorr_df['V1_'] =  hicorr_df.V1.map(lambda x: 1 if x < -3 else 0)\nhicorr_df['V6_'] =  hicorr_df.V6.map(lambda x: 1 if x < -2.5 else 0)\n","b39e9640":"hicorr_df.head()","68478ea5":"fin_df = hicorr_df.drop(hicorr_df.iloc[:,1:15],axis=1)","4fbddd3c":"fin_df","9cbae929":"X = fin_df.drop('Fraud', axis = 1)\ny = fin_df['Fraud']","b5dbdd64":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)","84d07b4d":"LogRmodel = LogisticRegression()\nLogRmodel.fit(X_train,y_train)\nLogRpredict = LogRmodel.predict(X_test)","e114c270":"print(f'the accuracy of this model:',100*accuracy_score(y_test,LogRpredict))","1f5f7e47":"SVMmodel = svm.SVC()\nSVMmodel.fit(X_train, y_train)\nSVMpredict = SVMmodel.predict(X_test)","caad069f":"print(f'the accuracy of this model:',100*accuracy_score(y_test,SVMpredict))","ba171be1":"DTmodel = tree.DecisionTreeClassifier()\nDTmodel.fit(X_train,y_train)\nDTpredict = DTmodel.predict(X_test) ","d0dffdd5":"print(f'the accuracy of this model:',100*accuracy_score(y_test,DTpredict))","cea7569b":"model = GaussianNB()                       \nmodel.fit(X_train, y_train)                 \ny_model = model.predict(X_test)  ","760583d0":"accuracy_score(y_test, y_model)\nprint(f'the accuracy of this model:',100*accuracy_score(y_test, y_model))","50b8461a":"Pick only variables that have correlation to fraud more than 10%.","5583dd9a":"# Dealing with imbalanced Datasetes","e1020e83":"Create the X and y variable for spliting before build a model.","6bd9fcac":"### Logistic Regression","215e4159":"### Quick Summary","93603917":"Now we have a data frame that Fraud and Normal Transactions are 50% and 50%","be1055eb":"Not getting much information from compare amount to time","b186fbdd":"I want to know the difference between data of Fraud and Not Fraud by basic inspection.","4547a07c":"# Summary","6e7dcaf7":"## Analyzing New Data Frame","5fb4e43d":"This one is fraud analyst, virtualization, creating model and dealing with Imbalanced Datasets\n\nThanks to:\nhttps:\/\/www.kaggle.com\/currie32\/predicting-fraud-with-tensorflow and\n\nhttps:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets\n\nto give me idea to analyze further and dealing with datasets and if you like it please upvote this notebook for me too.\n\nMy github for my previous works: https:\/\/github.com\/northpr\n\nIf you have any questions or any mistakes, I'm always open to talk to.","88aaf8b4":"No missing values so we don't have to deal with it.","6257342d":"Since the start, this datasets have a big number of normal transaction for almost 99.99% so wen need to rechange to a better dataset","83498919":"From the heatmap above we could know that V17,V14,V12 are the top 3 highest correlation to fraud.","fe621765":"### Decision Tree","6bc97c5a":"### Gaussian","a1d14cee":"This dataset are not allowed to provide the original features and more background information about the data. Features V1,V2...,V28 are obtained with PCA and the variables which have not been transformed with PCA are 'Time' and 'Amout'.","56004482":"It might not be a good set of data for analyze fraud because it have fraud only 0.17% which is very little so I will create 2 model that is the former dataset and the dataset that I deal with the imbalanced between 'Fraud' and 'Normal Transaction'","f8346e10":"Even the dataset is not imbalanced we could still learn the difference of normal transaction and fraud","d337228f":"I want to check the correlation between fraud and not by using corrrelation heatmap","836fecb2":"I could argue that fraudulent transactions are more uniformly distributted, while normal transactions have a cyclical distribution. This could make it easier to deteect a fradulent transaction during at an 'off-peak' time.","bbc4f5a3":"Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.","0941f2e6":"## Data preparation for creating the model","5d517387":"Even the accuracy rate from the imbalanced datasets is lower but somehow it's unbiased as the first dataset that I've used. To detect fraud or others we should the model that are awareness of uncommon transactions and alert to the workers in the fraud department.","69d64d42":"## Exploratory Data Analysis","7edb7033":"## Predict the mode before dealing with imbalanced datasets","7a117390":"# Introduction","864ac5e9":"## Import important libraries and basic manipulation","05958112":"### SVM","106d8dd7":"Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. So I change from 'Class' to 'Fraud' for better understanding.","6175507b":"## Random under sampling","111bd086":"# Model building : Data Preparation","a3d10b97":"**Do not use any of this model because because it's not the dataset is imbalanced. It's consist of 'Normal Transaction' for 99.99% but 'Fraud Transaction' only 0.01%**","991c51c5":"### SVM Model","6e49beb6":"<b>Sure, all the model above will have more than 95% accuracy even the worst one because the dataset is imbalanced<\/b>","f81365d8":"Now we have the virtualization data that we could seperate fraud and normal transaction easier based on the plot above","fd6c2351":"## Data Analyzation and Cleaning","7e30eafc":"Time is one of the most thing we can inspect","db829a1c":"Analyze only the anonymized features","1810051a":"### Decision Tree","df9310b4":"### Logistic Regression","b53f07a5":"### Naive Bayes","8e558935":"The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.","d6546787":"We need to remove some of the variables because it's high correlated to each other.","7464badf":"Split the data into train and test","0aa37e77":"### Splitting the balance dataset"}}