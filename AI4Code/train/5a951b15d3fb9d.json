{"cell_type":{"c0148fce":"code","5240fa1e":"code","2c51ee3e":"code","5e1c925f":"code","20ab9e40":"code","e061c7b9":"code","28523995":"code","00746bbf":"code","d06904e7":"code","e88142de":"markdown","0a2a9dab":"markdown","11c7001e":"markdown","8c01da4b":"markdown","3c8a7f4e":"markdown","7367b4e2":"markdown"},"source":{"c0148fce":"import numpy as np \nimport pandas as pd \nimport os\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport pickle","5240fa1e":"# specify number of folds (suppose 5 folds)\nN_FOLDS = 5 \n# set seed for reproducibility\nSEED = 42 # favourite number lol","2c51ee3e":"sub = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsub.head()","5e1c925f":"original_test = pd.read_csv('..\/input\/titanic\/test.csv')\noriginal_test.head()","20ab9e40":"df_test = pd.read_csv('..\/input\/part-2-preprocessing-feature-engineering\/test_preprocessed.csv')\ndf_test.head()","e061c7b9":"k_fold_models = []\nstratified_k_fold_models = []\nfor i in range(N_FOLDS):\n    # load k-fold models\n    with open(f'..\/input\/part-3-model-training-validation-strategies\/logistic_regression_kfold_fold_{i}.pkl', 'rb') as f:\n        k_fold_models.append(pickle.load(f))\n    # load stratified k-fold models\n    with open(f'..\/input\/part-3-model-training-validation-strategies\/logistic_regression_stratifiedkfold_fold_{i}.pkl', 'rb') as f:\n        stratified_k_fold_models.append(pickle.load(f))","28523995":"preds_kfold = []\npreds_stratified_kfold = []\nfor model in k_fold_models:\n    preds_kfold.append(model.predict_proba(df_test.values)[:, 1]) # positive class predictions\nfor model in stratified_k_fold_models:\n    preds_stratified_kfold.append(model.predict_proba(df_test.values)[:, 1]) # positive class predictions\n\n# mean of K-fold predictions\npreds_kfold = np.mean(preds_kfold, axis=0)\npreds_stratified_kfold = np.mean(preds_stratified_kfold, axis=0)\n\n# round predictions to 0 or 1 \npreds_kfold = preds_kfold > 0.5 # threshold 0.5\npreds_stratified_kfold = preds_stratified_kfold > 0.5 # threshold 0.5","00746bbf":"sub['Survived'] = preds_kfold.astype(int)\nsub.to_csv('submission_kfold.csv', index=False)\nsub.head()","d06904e7":"sub['Survived'] = preds_stratified_kfold.astype(int)\nsub.to_csv('submission_stratified_kfold.csv', index=False)\nsub.head()","e88142de":"# Imports","0a2a9dab":"# Introduction\n\nIn this notebook series, I'll be sharing my typical approach to machine learning (ML) problems (in this case competitions), in an end-to-end ML pipeline starting with the data analyses, to feature engineering, validation strategies, model training and finally inference (with post-processing techniques).\n\nMy notebooks in this series can be found in the links below:\n- [Exploratory Data Analysis (EDA)](https:\/\/www.kaggle.com\/khoongweihao\/part-1-exploratory-data-analysis-eda)\n- [Preprocessing & Feature Engineering](https:\/\/www.kaggle.com\/khoongweihao\/part-2-preprocessing-feature-engineering)\n- [Model Training & Validation Strategies](https:\/\/www.kaggle.com\/khoongweihao\/part-3-model-training-validation-strategies)\n- [Inference & Post-processing Techniques](https:\/\/www.kaggle.com\/khoongweihao\/part-4-inference-and-post-processing-techniques)\n\nBonus notebooks include adoption of recent research in terms of models, hyperparameter search, etc. They can be found in the links below:\n- Hyperparameter optimization with Optuna\n- TabNet ","11c7001e":"# Load Saved Models","8c01da4b":"# Make Submission","3c8a7f4e":"# Inference (K-Fold)","7367b4e2":"# Finishing Remarks\n\nThanks for reading and I welcome your feedback and suggestions for improvement. The notebook will be updated periodically as well.\n\nHappy Kaggling!\n\n---------------------------------------------------------------------\nMy notebooks in this series can be found in the links below:\n- [Exploratory Data Analysis (EDA)](https:\/\/www.kaggle.com\/khoongweihao\/part-1-exploratory-data-analysis-eda)\n- [Preprocessing & Feature Engineering](https:\/\/www.kaggle.com\/khoongweihao\/part-2-preprocessing-feature-engineering)\n- [Model Training & Validation Strategies](https:\/\/www.kaggle.com\/khoongweihao\/part-3-model-training-validation-strategies)\n- [Inference & Post-processing Techniques](https:\/\/www.kaggle.com\/khoongweihao\/part-4-inference-and-post-processing-techniques)\n\nBonus notebooks include adoption of recent research in terms of models, hyperparameter search, etc. They can be found in the links below:\n- Hyperparameter optimization with Optuna\n- TabNet "}}