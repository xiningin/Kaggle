{"cell_type":{"8774158e":"code","ec8e4051":"code","40529df0":"code","fe7fd749":"code","76c744b0":"code","8f10848d":"code","9c07a49f":"code","6a30b946":"code","c2b1d49c":"code","48ccf8cf":"code","b24cea8e":"code","b983d542":"code","5f0a1c27":"code","68b4eb5c":"code","490ed958":"code","a8b9335a":"code","b236d960":"code","c6b2c19a":"code","77c5665b":"code","c6d0e7ff":"code","7eaed1bc":"code","d1bec49b":"code","15d76c7c":"code","c124fe52":"code","5b17f203":"code","cb19a897":"code","e54d956f":"code","8d79ea9d":"code","a60d956f":"code","885cbcdb":"code","0a6b13fb":"code","15c7820d":"code","b3091d96":"code","3367a885":"code","4f2a4b44":"code","562a5507":"code","7f010bb3":"code","86945415":"code","52036a13":"code","808e8e10":"code","74b762aa":"code","91205e4d":"code","216f560a":"code","e098f779":"code","a0e96f6f":"code","eaace0d3":"code","3f38d8a7":"code","11152def":"code","c26c260a":"code","7a764092":"markdown","7e6463b2":"markdown","4c17a562":"markdown","f057d765":"markdown","f4f65e0e":"markdown","8fad4b7e":"markdown","c2f21ac6":"markdown","0e850f19":"markdown","a2959ee2":"markdown","6540fdeb":"markdown","3d936ccc":"markdown","c4e66073":"markdown"},"source":{"8774158e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt \nimport folium\nfrom folium import Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\nimport tqdm\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec8e4051":"data = pd.read_csv('\/kaggle\/input\/capstone-car-accident-serveity\/Data_Collisions.csv')\ndata.head()","40529df0":"data.columns\nprint(data.columns)\nlen(data.columns)","fe7fd749":"data.describe()","76c744b0":"data.info()","8f10848d":"data.drop(['SEVERITYCODE.1'], axis = 1, inplace = True)","9c07a49f":"for col in data.columns:\n    print(col,\" \",len(data[col].unique()))","6a30b946":"boolean = not data[\"INCKEY\"].is_unique      # True (credit to @Carsten)\nboolean = data['INCKEY'].duplicated().any() # True\nboolean","c2b1d49c":"day_of_year = []\nday = []\nmonth = []\nyear = []\ndate = pd.to_datetime(data['INCDATE'])\nfor i in range(len(date)):\n    actual_date = date.iloc[i]\n    new_date = pd.Timestamp(year=actual_date.year, month=1, day=1, tz ='UTC')\n    day_of_year.append((actual_date - new_date).days + 1)\n    day_1 = actual_date.strftime('%a')\n    day.append(day_1)\n    month_1 = actual_date.strftime('%b')\n    month.append(month_1)\n\ndata['INCDTTM'] = pd.to_datetime(data['INCDTTM'])\n   \ndf = data[['OBJECTID','X','Y','SEVERITYCODE','PERSONCOUNT','PEDCOUNT','VEHCOUNT','PEDCYLCOUNT']]\ndf['DATE'] = data['INCDTTM'].dt.strftime('%d\/%m\/%Y')\ndf['year'] = df['DATE'].str[-4:].astype(int)\ndf['dayofyear'] = day_of_year\ndf['month'] = month\ndf['day'] = day\ndf['TIME'] = data['INCDTTM'].dt.strftime('%H:%M:%S')\ndf['TIME'] = df.TIME.replace('00:00:00',np.nan)\ndf = df.dropna(subset=['TIME']) \ndf['TIME'] = df['TIME'].astype(str)\ndf['HOUR'] = df['TIME'].str[:2].astype(int)\ndf.head()\n    ","48ccf8cf":"data['ROADCOND'].unique()","b24cea8e":"data['WEATHER'].unique()","b983d542":"data['LIGHTCOND'].unique()","5f0a1c27":"df_feature = data[['OBJECTID','WEATHER','LIGHTCOND','ROADCOND','SEVERITYCODE','X','Y','PERSONCOUNT','PEDCOUNT','VEHCOUNT','PEDCYLCOUNT','UNDERINFL','SPEEDING', 'HITPARKEDCAR']]\ndf_feature = pd.merge(df_feature,df[['year', 'day', 'month','OBJECTID','HOUR','dayofyear']], on='OBJECTID')\n\ndf_feature = df_feature.dropna(subset=['WEATHER','LIGHTCOND','ROADCOND','X','Y'])\ndf_feature['SPEEDING'].fillna('N', inplace=True)\n\nfor n in ['WEATHER','LIGHTCOND','ROADCOND']:\n    a = np.where(df_feature[n] == 'Unknown')[0]\n    df_feature = df_feature.drop(df_feature.index[a])\n    b = np.where(df_feature[n] == 'Other')[0]\n    df_feature = df_feature.drop(df_feature.index[b])\n    \n\ndf_feature = pd.concat([df_feature,pd.get_dummies(df_feature['LIGHTCOND'])], axis=1)\ndf_feature['Dark'] = df_feature['Dark - No Street Lights'] + df_feature['Dark - Street Lights Off'] + df_feature['Dark - Unknown Lighting'] + df_feature['Dark - Street Lights On'] #dark\ndf_feature['Dawn'] = df_feature['Dawn'] + df_feature['Dusk'] \ndf_feature.drop(['Dusk','Dark - No Street Lights','Dark - Unknown Lighting','Dark - Street Lights On','Dark - Street Lights Off'], axis = 1, inplace=True)\n\ndf_feature = pd.concat([df_feature,pd.get_dummies(df_feature['WEATHER'])],axis=1)\ndf_feature['Cloudy'] = df_feature['Fog\/Smog\/Smoke'] + df_feature['Partly Cloudy'] + df_feature['Blowing Sand\/Dirt'] + df_feature['Overcast'] #vision obstructors\ndf_feature['Rain\/Snow'] = df_feature['Raining'] + df_feature['Sleet\/Hail\/Freezing Rain'] + df_feature['Snowing'] + df_feature['Severe Crosswind'] #surface\/steering obstructors\ndf_feature.drop(['Fog\/Smog\/Smoke','Partly Cloudy','Blowing Sand\/Dirt','Overcast','Raining','Sleet\/Hail\/Freezing Rain','Snowing','Severe Crosswind'], axis=1,inplace=True)\n\ndf_feature = pd.concat([df_feature,pd.get_dummies(df_feature['ROADCOND'])], axis=1)\ndf_feature['Wet'] = df_feature['Standing Water'] + df_feature['Wet'] + df_feature['Snow\/Slush'] + df_feature['Oil'] + df_feature['Sand\/Mud\/Dirt'] +df_feature['Ice'] #Non-dry surface\ndf_feature.drop(['Standing Water','Snow\/Slush','Oil','Sand\/Mud\/Dirt','Ice'],axis=1,inplace=True)\n\n\n\na = len(df_feature['OBJECTID'])\nDAY = np.zeros(a)\nMONTH = np.zeros(a)\ndays = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']\nmonths = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nfor n in range(a):\n    DAY[n] = ((days.index(df_feature['day'].iloc[n])) + 1)\n    MONTH[n] = (months.index((df_feature['month'].iloc[n])) + 1)   \n            \n    \ndf_feature['DAY'] = DAY\ndf_feature['MONTH'] = MONTH\n\ndf_feature = df_feature.drop(columns=['WEATHER','ROADCOND','LIGHTCOND','day', 'month'])\ndf_feature","68b4eb5c":"one = pd.get_dummies(df_feature['UNDERINFL'])\ndf_feature['UNDERINFL'] = one['1'] + one['Y']\none = pd.get_dummies(df_feature['SPEEDING'])\ndf_feature['SPEEDING'] = one['Y']\none = pd.get_dummies(df_feature['HITPARKEDCAR'])\ndf_feature['HITPARKEDCAR'] = one['Y']\ndf_feature","490ed958":"\nmat =  df_feature.corr()\nf, ax = plt.subplots(figsize =(12, 10))\n  \nsns.heatmap(mat, ax = ax, cmap =\"YlGnBu\", \n            linewidths = 0.1) ","a8b9335a":"severity_map = folium.Map(location=[47.619543,-122.330518],\n                        zoom_start=13,\n                        tiles=\"CartoDB dark_matter\")\ndf_value = df_feature['PERSONCOUNT'] + df_feature['PEDCOUNT'] + df_feature['PEDCYLCOUNT'] + df_feature['VEHCOUNT']\nfor i in range(7800,8800):\n    lat = df_feature['Y'].iloc[i]\n    long = df_feature['X'].iloc[i]\n    radius = df_value.iloc[i] \/ 0.8\n    \n    if radius > 10:\n        color = \"#FF4500\"\n     \n    else:\n        color = \"#008080\"\n    \n    popup_text = \"\"\"Latitude : {}<br>\n                Longitude : {}<br>\n                LOCATION : {}<br>\n                PEOPLE INVOLVEMENT: {}<br>\n                SEVERITY CODE:{}<br>\"\"\"\n    popup_text = popup_text.format(lat,\n                               long,\n                               data['LOCATION'].iloc[i],\n                               df_value.iloc[i],\n                               data['SEVERITYCODE'].iloc[i]\n                               )\n    folium.CircleMarker(location = [lat, long], popup= popup_text,radius = radius, color = color, fill = True).add_to(severity_map)\nseverity_map","b236d960":"m_2 = folium.Map(location=[47.619543,-122.330518], tiles='cartodbdark_matter', zoom_start=12)\nHeatMap(data=df_feature[['Y', 'X']], radius=10).add_to(m_2)\n\nm_2","c6b2c19a":"year = np.zeros((16,2))\n\nb = 0\nfor a in range(2004,2020):\n\n    for m in [1,2]:        \n        year[b,m-1] = (np.sum((df_feature['year']== a) & (df_feature['SEVERITYCODE'] == m)))\n    b = b + 1\nplt.figure(figsize=(20,10))\n\n\nx=np.arange(2004,2020)\n\nplt.plot(x,year)\nplt.xlabel('Year',fontsize=20)\nplt.ylabel('Total number of incidents',fontsize=20)\nplt.legend(['1','2'],loc=1)\nplt.show()","77c5665b":"len(df_feature['PERSONCOUNT'].unique())","c6d0e7ff":"year = np.zeros((16,2))\n\nb = 0\nfor a in range(2004,2020):\n\n    for m in [1,2]:        \n        year[b,m-1] = (np.sum((df_feature['year']== a) & (df_feature['PERSONCOUNT'] == m)))\n    b = b + 1\nplt.figure(figsize=(20,10))\n\n\nx=np.arange(2004,2020)\n\nplt.plot(x,year)\nplt.xlabel('Year',fontsize=20)\nplt.ylabel('Total number of incidents',fontsize=20)\nplt.legend(['1','2'],loc=1)\nplt.show()","7eaed1bc":"year = np.zeros((16,2))\n\nb = 0\nfor a in range(2004,2020):\n\n    for m in [1,2]:        \n        year[b,m-1] = (np.sum((df_feature['year']== a) & (df_feature['PEDCOUNT'] == m)))\n    b = b + 1\nplt.figure(figsize=(20,10))\n\n\nx=np.arange(2004,2020)\n\nplt.plot(x,year)\nplt.xlabel('Year',fontsize=20)\nplt.ylabel('Total number of incidents',fontsize=20)\nplt.legend(['1','2'],loc=1)\nplt.show()","d1bec49b":"year = np.zeros((16,2))\n\nb = 0\nfor a in range(2004,2020):\n\n    for m in [1,2]:        \n        year[b,m-1] = (np.sum((df_feature['year']== a) & (df_feature['PEDCYLCOUNT'] == m)))\n    b = b + 1\nplt.figure(figsize=(20,10))\n\n\nx=np.arange(2004,2020)\n\nplt.plot(x,year)\nplt.xlabel('Year',fontsize=20)\nplt.ylabel('Total number of incidents',fontsize=20)\nplt.legend(['1','2'],loc=1)\nplt.show()","15d76c7c":"year = np.zeros((16,2))\n\nb = 0\nfor a in range(2004,2020):\n\n    for m in [1,2]:        \n        year[b,m-1] = (np.sum((df_feature['year']== a) & (df_feature['VEHCOUNT'] == m)))\n    b = b + 1\nplt.figure(figsize=(20,10))\n\n\nx=np.arange(2004,2020)\n\nplt.plot(x,year)\nplt.xlabel('Year',fontsize=20)\nplt.ylabel('Total number of incidents',fontsize=20)\nplt.legend(['1','2'],loc=1)\nplt.show()","c124fe52":"year_array = np.zeros((16,2))\n\nb = 0\nfor a in range(2004,2020):            \n        year_array[b,0] = (np.sum((df_feature['year']== a) & (df_feature['Dry'] == 1)))\n        year_array[b,1] = (np.sum((df_feature['year']== a) & (df_feature['Wet'] == 1)))       \n        b = b + 1\nplt.figure(figsize=(20,10))\n\n\nx=np.arange(2004,2020)\n\nplt.plot(x,year_array)\nplt.xlabel('Year')\nplt.ylabel('Total number of incidents')\nplt.legend(['Dry','Wet'],loc=1)\nplt.show()","5b17f203":"year_array = np.zeros((16,3))\n\nb = 0\nfor a in range(2004,2020):            \n        year_array[b,0] = (np.sum((df_feature['year']== a) & (df_feature['Clear'] == 1)))\n        year_array[b,1] = (np.sum((df_feature['year']== a) & (df_feature['Cloudy'] == 1)))\n        year_array[b,2] = (np.sum((df_feature['year']== a) & (df_feature['Rain\/Snow'] == 1)))\n        b = b + 1\nplt.figure(figsize=(20,10))\n\nx=np.arange(2004,2020)\n\nplt.plot(x,year_array)\nplt.xlabel('Year',fontsize=20)\nplt.ylabel('Total number of incidents',fontsize=20)\nplt.legend(['Clear','Cloudy','Rain\/Snow'],loc=1)\nplt.show()\n\nyear_array = np.zeros((16,3))\n\nb = 0\nfor a in range(2004,2020):            \n        year_array[b,0] = (np.sum((df_feature['year']== a) & (df_feature['SPEEDING'] == 1)))\n        year_array[b,1] = (np.sum((df_feature['year']== a) & (df_feature['UNDERINFL'] == 1)))\n        year_array[b,2] = (np.sum((df_feature['year']== a) & (df_feature['HITPARKEDCAR'] == 1)))\n        b = b + 1\nplt.figure(figsize=(20,10))\n\nx=np.arange(2004,2020)\n\nplt.plot(x,year_array)\nplt.xlabel('Year',fontsize=20)\nplt.ylabel('Total number of incidents',fontsize=20)\nplt.legend(['SPEEDING', 'DRINK AND DRIVE', 'HIT PARKED CAR'],loc=1)\nplt.show()","cb19a897":"months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nmonth_array = np.zeros((12,2))\n\nb = 0\nfor a in range(1,13):\n\n    for m in [1,2]:        \n        month_array[b,m-1] = (np.sum((df_feature['MONTH']== a) & (df_feature['SEVERITYCODE'] == m)))\n    b = b + 1    \n    x = [i for i, _ in enumerate(months)]\nplt.figure(figsize=(20,10))\n\nplt.xticks(x, months)\n\nplt.plot(x,month_array)\nplt.xlabel('Month',fontsize=20)\nplt.ylabel('Total number of incidents',fontsize=20)\nplt.legend(['1','2'],loc=5)\nplt.show()","e54d956f":"months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nmonth_array = np.zeros((12,2))\n\nb = 0\nfor a in range(1,13):\n\n    for m in [1,2]:        \n        month_array[b,m-1] = (np.sum((df_feature['MONTH']== a) & (df_feature['PERSONCOUNT'] == m)))\n    b = b + 1    \n    x = [i for i, _ in enumerate(months)]\nplt.figure(figsize=(20,10))\n\nplt.xticks(x, months)\n\nplt.plot(x,month_array)\nplt.xlabel('Month',fontsize=10)\nplt.ylabel('Total number of incidents',fontsize=20)\nplt.legend(['1','2'],loc=5)\nplt.show()","8d79ea9d":"months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nmonth_array = np.zeros((12,2))\n\nb = 0\nfor a in range(1,13):\n\n    for m in [1,2]:        \n        month_array[b,m-1] = (np.sum((df_feature['MONTH']== a) & (df_feature['PEDCOUNT'] == m)))\n    b = b + 1    \n    x = [i for i, _ in enumerate(months)]\nplt.figure(figsize=(20,10))\n\nplt.xticks(x, months)\n\nplt.plot(x,month_array)\nplt.xlabel('Month',fontsize=20)\nplt.ylabel('Total number of incidents',fontsize=20)\nplt.legend(['1','2'],loc=5)\nplt.show()","a60d956f":"months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nmonth_array = np.zeros((12,2))\n\nb = 0\nfor a in range(1,13):\n\n    for m in [1,2]:        \n        month_array[b,m-1] = (np.sum((df_feature['MONTH']== a) & (df_feature['VEHCOUNT'] == m)))\n    b = b + 1    \n    x = [i for i, _ in enumerate(months)]\nplt.figure(figsize=(20,10))\n\nplt.xticks(x, months)\n\nplt.plot(x,month_array)\nplt.xlabel('Month',fontsize=20)\nplt.ylabel('Total number of incidents',fontsize=20)\nplt.legend(['1','2'],loc=5)\nplt.show()\nmonths = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nmonth_array = np.zeros((12,2))\n\nb = 0\nfor a in range(1,13):\n\n    for m in [1,2]:        \n        month_array[b,m-1] = (np.sum((df_feature['MONTH']== a) & (df_feature['PEDCYLCOUNT'] == m)))\n    b = b + 1    \n    x = [i for i, _ in enumerate(months)]\nplt.figure(figsize=(20,10))\n\nplt.xticks(x, months,size=10)\n\nplt.plot(x,month_array)\nplt.xlabel('Month',fontsize=20)\nplt.ylabel('Total number of incidents', fontsize=20)\nplt.legend(['1','2'],loc=5)\nplt.show()","885cbcdb":"months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nmonth_array = np.zeros(12)\n\nfor b in range(2004,2020,1):\n    df_date = df_feature[df_feature['year'] == b] \n    n = 0\n    for j in range(1,13):\n        month_array[n] = np.sum(df_date['PERSONCOUNT'][df_date['MONTH'] == j])\n        n = n + 1\n        x = [i for i, _ in enumerate(months)]\n    \n    plt.figure()\n    plt.ylim([0,3000])\n    plt.text(10,2800,str(b))    \n    plt.xticks(x, months)\n    plt.xlabel('Month',fontsize=20)\n    plt.ylabel('Total number of people involved in incidents',fontsize=10)\n    plt.bar(x,month_array)\nplt.show()","0a6b13fb":"severity_array = np.zeros((2,365))\n    \nfor n in range(365):\n    a = 0\n    for m in [1,2]:        \n        severity_array[a,n] = (np.sum((df_feature['dayofyear']== n+1) & (df_feature['SEVERITYCODE'] == m)))\n        a = a + 1\nplt.figure(figsize=(20,10))\n# plt.ylim([0,400])\n# plt.text(175,65,str(b))\nfor x in range(2):        \n    plt.plot(range(365),severity_array[x])\n    plt.xlabel('Day of year')\n    plt.ylabel('Total number of incidents')\n    plt.legend(['1','2'],loc=3)\nplt.show()","15c7820d":"for b in range(2004,2020,1):\n    df_date = df_feature[df_feature['year'] == b]\n    severity_array = np.zeros((2,365))\n    \n    for n in range(365):\n        a = 0\n        for m in [1,2]:        \n            severity_array[a,n] = (np.sum((df_date['dayofyear']==n) & (df_date['SEVERITYCODE'] == m)))\n            a = a + 1\n    plt.figure(figsize=(20,5))\n    plt.ylim([0,70])\n    plt.text(175,65,str(b))\n    for x in range(2):        \n        plt.plot(range(365),severity_array[x])\n        plt.xlabel('Day of year')\n        plt.ylabel('Total number of incidents')\n    plt.legend(['1','2'],loc=1)\nplt.show()","b3091d96":"days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']\nday_array = np.zeros(7)\n\nfor b in range(2004,2020,1):\n    df_date = df_feature[df_feature['year'] == b] \n    n = 0\n    for a in range(1,8):\n        day_array[n] = np.sum(df_date['PERSONCOUNT'][df_date['DAY'] == a])\n        n = n + 1\n        x = [i for i, _ in enumerate(days)]\n    plt.figure()\n    plt.ylim([0,5000])\n    plt.text(-0.5,4500,str(b))  \n    plt.xticks(x, days)\n    plt.bar(x,day_array)\n    plt.xlabel('Day')\n    plt.ylabel('Total number of people involved')\nplt.show()","3367a885":"days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']\nday_array = np.zeros(7)\n\nfor b in range(2004,2020,1):\n    df_date = df_feature[df_feature['year'] == b]\n    n = 0\n    for a in range(1,8):\n        day_array[n] = np.sum(df_date['VEHCOUNT'][df_date['DAY'] == a])\n        n = n + 1\n        x = [i for i, _ in enumerate(days)]\n    plt.figure()\n    plt.ylim([0,4000])\n    plt.text(-0.5,3400,str(b))\n    plt.xticks(x, days)\n    plt.bar(x,day_array)\n    plt.xlabel('Day')\n    plt.ylabel('Total number of vehicles involved')\nplt.show()","4f2a4b44":"hour_array = np.zeros((24,2))\ntimes = np.arange(0,24,1)\nb = 0\nfor a in range(0,24):\n\n    for m in [1,2]:        \n        hour_array[b,m-1] = (np.sum((df_feature['HOUR']== a) & (df_feature['SEVERITYCODE'] == m)))\n    b = b + 1\n    x = [i for i, _ in enumerate(times)]\nplt.figure()\n\n\nplt.plot(x,hour_array)\nplt.xticks(x, times)\nplt.xlabel('Hour')\nplt.ylabel('Total number of incidents')\nplt.legend(['1','2'],loc=1)\nplt.show()","562a5507":"hour_array = np.zeros((24,2))\ntimes = np.arange(0,24,1)\nb = 0\nfor a in range(0,24):\n\n    for m in [1,2]:        \n        hour_array[b,m-1] = (np.sum((df_feature['HOUR']== a) & (df_feature['PERSONCOUNT'] == m)))\n    b = b + 1\n    x = [i for i, _ in enumerate(times)]\nplt.figure()\n\n\n\nplt.plot(x,hour_array)\nplt.xticks(x, times)\nplt.xlabel('Hour')\nplt.ylabel('Total number of people')\nplt.legend(['1','2'],loc=1)\nplt.show()","7f010bb3":"hour_array = np.zeros((24,2))\ntimes = np.arange(0,24,1)\nb = 0\nfor a in range(0,24):\n\n    for m in [1,2]:        \n        hour_array[b,m-1] = (np.sum((df_feature['HOUR']== a) & (df_feature['VEHCOUNT'] == m)))\n    b = b + 1\n    x = [i for i, _ in enumerate(times)]\nplt.figure()\n\n\n\nplt.plot(x,hour_array)\nplt.xticks(x, times)\nplt.xlabel('Hour')\nplt.ylabel('Total number of vehicles')\nplt.legend(['1','2'],loc=1)\nplt.show()","86945415":"for b in range(2004,2020,1):\n    df_time= df_feature[df_feature['year'] == b] #can change year\n    severity_array = np.zeros((2,24))\n    \n    for n in range(24):\n        a = 0\n        for m in [1,2]:        \n            severity_array[a,n] = (np.sum((df_time['HOUR']==n) & (df_time['SEVERITYCODE'] == m)))\n            a = a + 1\n    plt.figure()\n    plt.ylim([0,700])\n    plt.text(22,650,str(b))\n    for x in range(2):        \n        plt.bar(range(24),severity_array[x])\n        plt.xlabel('Hour (in 24 hour time)')\n        plt.ylabel('Total number of incidents')\n    plt.legend(['1','2'],loc=2)\nplt.show()","52036a13":"times = np.arange(0,24,1)\ntime_array = np.zeros((2,24))\n\nfor b in range(2004,2020,1):\n    df_time = df_feature[df_feature['year'] == b] \n    n = 0\n    for a in times:\n        for m in [1,2]:\n            time_array[m-1,n] = np.sum(df_time['PERSONCOUNT'][df_time['HOUR'] == a][df_time['SEVERITYCODE'] == m])\n        n = n + 1\n        x = [i for i, _ in enumerate(times)]\n    plt.figure()\n    plt.ylim([0,1500])\n    plt.text(20,1200,str(b))\n    plt.xticks(x, times)\n    for i in range(2):\n        plt.bar(x,time_array[i])\n    plt.legend(['1','2'],loc=2)\n    plt.xlabel('Hour (in 24 hour time)')\n    plt.ylabel('Total number of people involved')\nplt.show()","808e8e10":"times = np.arange(0,24,1)\ntime_array = np.zeros((2,24))\n\nfor b in range(2004,2020,1):\n    df_time = df_feature[df_feature['year'] == b] \n    n = 0\n    for a in times:\n        for m in [1,2]:\n            time_array[m-1,n] = np.sum(df_time['VEHCOUNT'][df_time['HOUR'] == a][df_time['SEVERITYCODE'] == m])\n        n = n + 1\n        x = [i for i, _ in enumerate(times)]\n    plt.figure()\n    plt.ylim([0,1500])\n    plt.text(20,1200,str(b))\n    plt.xticks(x, times)\n    for i in range(2):\n        plt.bar(x,time_array[i])\n    plt.legend(['1','2'],loc=2)\n    plt.xlabel('Hour (in 24 hour time)')\n    plt.ylabel('Total number of vechiles involved')\nplt.show()","74b762aa":"from sklearn.preprocessing import RobustScaler\n\nscale = RobustScaler()\ndf_feature[['PERSONCOUNT','VEHCOUNT','PEDCYLCOUNT','PEDCOUNT']] = scale.fit_transform(df_feature[['PERSONCOUNT','VEHCOUNT','PEDCYLCOUNT','PEDCOUNT']])\ndf_feature.head()","91205e4d":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\ny = df_feature['SEVERITYCODE'].values\nx = df_feature.loc[:,df_feature.columns!= 'SEVERITYCODE']\nx = preprocessing.StandardScaler().fit(x).transform(x)\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)","216f560a":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nsgd = SGDClassifier(loss='modified_huber',shuffle=True,random_state=101)\nsgd.fit(x_train,y_train)\ny_pred = sgd.predict(x_test)\nscore1 = accuracy_score(y_test, y_pred)\nf1_score1 = f1_score(y_test, y_pred, average='weighted')\nscore1","e098f779":"from sklearn.linear_model import LogisticRegression\nlgr = LogisticRegression()\nlgr.fit(x_train,y_train)\ny_pred = lgr.predict(x_test)\nscore2 = accuracy_score(y_test, y_pred)\nf1_score2 = f1_score(y_test, y_pred, average='weighted')\nscore2","a0e96f6f":"from sklearn.neighbors import KNeighborsClassifier\n\nneigh = KNeighborsClassifier(n_neighbors=14).fit(x_train, y_train)\ny_pred = neigh.predict(x_test)\nscore3 = accuracy_score(y_test, y_pred)\nf1_score3 = f1_score(y_test, y_pred, average='weighted')\nscore3","eaace0d3":"# from sklearn.tree import DecisionTreeClassifier\n# import matplotlib.image as mpimg\n# from sklearn import tree\n\n# modeltree = DecisionTreeClassifier(criterion=\"entropy\")\n# modeltree.fit(x_train, y_train)\n\n\n# predTree = modeltree.predict(x_test)\n# print(predTree[0:5])\n# print(y_test[0:5])\n# accuracy_score(y_test,predTree)","3f38d8a7":"from sklearn.ensemble import  RandomForestClassifier\nrfm = RandomForestClassifier(n_estimators = 160, random_state = 101, min_samples_leaf=30)\nrfm.fit(x_train,y_train)\ny_pred = rfm.predict(x_test)\nscore4 = accuracy_score(y_test, y_pred)\nf1_score4 = f1_score(y_test, y_pred, average='weighted')\nscore4","11152def":"scores = [score1, score2, score3, score4]\nf1_scores = [f1_score1,f1_score2,f1_score3,f1_score4]\nalgorithms = ['SGDClassifier', 'Logistic Regression', 'KNN', 'Random Forest']\ndf = pd.DataFrame(scores, index=['SGDClassifier', 'Logistic Regression', 'KNN', 'Random Forest'])\ndf.columns = ['Accuracy']\ndf['f1_scores'] = f1_scores\ndf.columns.name = 'Algorithm'\ndf","c26c260a":"plt.figure(figsize=(20,5))\nplt.plot(algorithms, scores)\nplt.plot(algorithms,f1_scores)\nplt.legend(['Accuracy','f1_score'],loc=2,prop={\"size\":15})\nplt.show()","7a764092":"'<h1 font-color='violet' align='center' style=\"font-family:'Courier New'\">Business Problem<\/h1>\n<div align='center' style=\"font-family:'Lucida Console'\">\n<p align='center'>The Seattle government is going to prevent avoidable car accidents by employing methods that alert drivers, health system, and police to remind them to be more careful in critical situations.<br>\nIn most cases, not paying enough attention during driving, abusing drugs and alcohol or driving at very high speed are the main causes of occurring accidents that can be prevented by enacting harsher regulations. Besides the aforementioned reasons, weather, visibility, or road conditions are the major uncontrollable factors that can be prevented by revealing hidden patterns in the data and announcing warning to the local government, police and drivers on the targeted roads.<br>\nThe target audience of the project is local Seattle government, police, rescue groups, and last but not least, car insurance institutes. The model and its results are going to provide some advice for the target audience to make insightful decisions for reducing the number of accidents and injuries for the city.\n<\/p>\n<\/div>","7e6463b2":"'<h1 font-color='violet' align='center' style=\"font-family:'Courier New'\">An Investigation On Road Accident<br> And<br>Severity in Seattle<\/h1>","4c17a562":"'<h1 font-color='violet' align='center' style=\"font-family:'Courier New'\">Cleaning for Modeling\n<\/h1>\n<div align='center' style=\"font-family:'Lucida Console'\">\n<p>Before using classification methods to predict the outcome of severity, the attributes\nwere organised into numerical form. The attributes being:\n    <ol align='left'>\n<li>Weather<\/li><li>Light condition<\/li><li>Road condition<\/li><li>Day<\/li>\n        <\/ol>\nFirstly, the a new dataframe is created merging day number and month number\nfrom the previous date dataframe. All rows with \u2019Unknown\u2019 or \u2019Other\u2019 values were\nremoved to keep all data entries consistent. This resulted in 7% drop in data. Again,\n\u2019OBJECTID\u2019 was kept as a consistent attribute.\nMany of the attributes have multiple unique values, some of which are repeated.\nIn order to employ the best possible outcome of severity but to also keep runtime\nfairly low, these values have been grouped as follows 2.1. One hot encoding will take\nthese categorical variables and transform them into numerical ones under binary\nvariables.\n\n\n<\/p>\n<\/div>","f057d765":"'<h1 font-color='violet' align='center' style=\"font-family:'Courier New'\">Total Incident Per Day\n<\/h1>\n<div align='center' style=\"font-family:'Lucida Console'\">\n<p>\nThe property damage incidents occur twice as often as injuries.\nWhat is noticeable is that Christmas day has the least amount of incidents, which is\nexpected as far fewer people travel on that day. It is noticeable also that days with a\ndrop in incidents occur around public holidays [11].<br><br>\nWhat is noticeable across is that although the number of incidents involving\nproperty damage reduces across the year, the amount of injuries stays fairly consistent. This could be due to people being more careful on the road to avoid cars\nbeing damaged. 2008 had the highest number of incidents per day.\nWhat was seen in the data however that after this period, the incidents decreased\ndramatically. Total incidents per day for year 2019 which had the lowest\nincidents has dropped to have more consistent amounts of incidents.\n\n<\/p>\n<\/div>","f4f65e0e":"'<h1 font-color='violet' align='center' style=\"font-family:'Courier New'\">Data<\/h1>\n<div align='center' style=\"font-family:'Lucida Console'\">\n<p align='center'>The data was collected by the Seattle Police Department and Accident Traffic Records Department from 2004 to present.\nThe data consists of 37 independent variables and 194,673 rows. The dependent variable, \u201cSEVERITYCODE\u201d, contains numbers that correspond to different levels of severity caused by an accident from 0 to 4.<br>\nSeverity codes are as follows:<br>\n0: Little to no Probability (Clear Conditions)<br>\n1: Very Low Probability \u2014 Chance or Property Damage<br>\n2: Low Probability \u2014 Chance of Injury<br>\n3: Mild Probability \u2014 Chance of Serious Injury<br>\n4: High Probability \u2014 Chance of Fatality<br>\n    <br>\n \nFurthermore, because of the existence of null values in some records, the data needs to be preprocessed before any further processing.\n\n\n<\/p>\n<\/div>","8fad4b7e":"<div align=\"\">\n  <img  width=\"1000\" height=\"1000\" src=\"https:\/\/static01.nyt.com\/images\/2015\/09\/25\/multimedia\/seattle-survivor\/seattle-survivor-superJumbo.jpg\">\n<\/div>","c2f21ac6":"<h1 font-color='violet' align='center' style=\"font-family:'Courier New'\">Predictive Modelling<\/h1>\n<div align='center' style=\"font-family:'Lucida Console'\">\n<p>\nAs data contain outliers so by using Robust Scaler data can remove outliers and process and scale the data according to outliers. <br><br>\nThe machine learning models used are Logistic Regression, K-Nearest Neighbor (KNN), SGD classifier and Random Forest. The logistic regression model is the basic one to predict the binary outcome using a logistic function. KNN uses a non-parametric method to classify new items based on similarity measures (distance).<br><br>\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. However, data characteristics can affect their performance.<br><br>\n    Stochastic Gradient Descent (SGD) is a simple yet efficient optimization algorithm used to find the values of parameters\/coefficients of functions that minimize a cost function. In other words, it is used for discriminative learning of linear classifiers under convex loss functions such as SVM and Logistic regression. It has been successfully applied to large-scale datasets because the update to the coefficients is performed for each training instance, rather than at the end of instances.\n\n<\/p>\n<\/div>","0e850f19":"<h1 font-color='violet' align='center' style=\"font-family:'Courier New'\">Result of Predictive Modelling<\/h1>","a2959ee2":"![](https:\/\/miro.medium.com\/max\/700\/0*XjpMvUSYJ91EpFVz)\n<h1 font-color='violet' align='center' style=\"font-family:'Courier New'\">Total Incident Per Hour\n<\/h1>\n<div align='center' style=\"font-family:'Lucida Console'\">\n<p>\nFinally, let\u2019s look at the total number of incidents across time.the amount of incidents drops between 2am and 6am. This is expected as\nmost people will be sleeping during these hours. The incidents peak at 5pm, which\nwould be rush hour. This suggests that travelling later in the day could reduce\nthe likelihood of an incident occurring.property damage has no\ncorrelation to time. Otherwise, these graphs suggest that there is a correlation in\nnumber of incidents and number of people involved at peak times.\n\n<\/p>\n<\/div>","6540fdeb":"'<h1 font-color='violet' align='center' style=\"font-family:'Courier New'\">Data Analysis\n<\/h1>\n<div align='center' style=\"font-family:'Lucida Console'\">\n<p>\nThe first comparison made is the severity against the new date attributes. The\nseverity code obeys the following,<br>\n\u2022 3 - Fatalilty<br>\n\u2022 2b - Serious injury<br>\n\u2022 2 - Injury<br>\n\u2022 1 - Prop damage<br>\n\u2022 0 - Unknown<br>\n    <br>\nThe column was analysed to see the count of each severity. The only severity\ncodes within the dateset are 1 and 2. This allowed for optimising the function of the\ncode and improve the runtime.<br><br>\nFirst, let\u2019s look at the total number of incidents across the years. This can be seen in\n3.1a. This shows a peak at 2006 but then continues to decrease beyond this point.\nThe number of incidents have effectively halved across the 16 years (neglecting the\npeak at 2015). This figure also shows that incidents including injury are a lot less\nlikely than those of property damage. Suggesting that injuries are very minimal in\nSeattle, and decreasing.<br><br>\nEqually, the total amount of vehicles involved peaked at the same time (as\nexpected), and is also decreasing. This will be important to note, as the weather\nconditions will affect the amount of vehicles damaged. An interesting observation is\nthe amount of vehicles involved in incidents is more than that of people involved,\nsuggesting there are a noticeable amount of stationary, empty vehicles involved in\ncollisions. An example being a car driving at night in January on black ice, could\nswerve and hit multiple parked cars.\n<br>\n\n\n<\/p>\n<\/div>","3d936ccc":"'<h1 font-color='violet' align='center' style=\"font-family:'Courier New'\">Total Incident Per Month\n<\/h1>\n<div align='center' style=\"font-family:'Lucida Console'\">\n<p>\nHaving analysed the total amounts across the year, it\u2019s important to also analyse\nincidents across the months. From analysis we can see that the amount of incidents\nreported are fairly consistent across the months. The drop in incidents occurs in\nFebruary, potentially due to fewer drivers on the road. A spike in property damage\n8 incidents occurs in October, which could be due to weather, similar to January.\nOtherwise, in the Spring\/Summer\/Autumn months, the number of incidents are\nconsistent.\n\n<\/p>\n<\/div>","c4e66073":"'<h1 font-color='violet' align='center' style=\"font-family:'Courier New'\">Cleaning for Analysis\n<\/h1>\n<div align='center' style=\"font-family:'Lucida Console'\">\n<p align='center'>The first quality to check is to make sure there is at least one attribute to use as a\nuniversal identifier. This attribute needs to comprise of unique values of length\ncomparable to the amount of rows in the dataframe. a Boolean check was performed\non OBJECTID to verify this. The other unique attributes (INCKEY, COLDETKEY,\nREPORTNO, INTKEY) can be redacted from the dataframe as some have missing\nvalues or have different types of formats within the column.<br>\n    Since this report will focus predominantly on the date and time of the collision,\nthe date and time format will need to be altered. The dataframe itself has fairly\ninconsistent values for these columns.<br>\nThe datetime (INCDTTM) attribute was split into two separate columns, and\nthen put into d\/m\/Y and H\/M\/S string format. The year could then be easily\nextracted by taking the last four letters. The date (INCDATE) attribute was cleaned\nby using the pandas function strftime to split the date into day, month, year and\nday of year. A 00:00 time appears for cells with date and no time after applying\nstring format to INCDTTM. This was created by first checking how many cells had\nincidents at midnight (this turned out to be zero), then 00:00 was replaced with an\nundefined value (NaN). For the purpose of consistent dataframes, the rows with\nNaN values were removed. This was around 2% of the dataset. The hour was\nextracted from the time the same way the year was extracted from date.\n\n\n\n<\/p>\n<\/div>"}}