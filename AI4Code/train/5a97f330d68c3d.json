{"cell_type":{"b54fa1cf":"code","199bd133":"code","1c56bc6d":"code","9c5b4837":"code","cc1968fd":"code","d696da06":"code","07c1f726":"code","d10032b6":"code","e5ceece0":"code","a6769979":"code","60a456fc":"code","f1bba253":"code","0a786572":"code","089b68d7":"code","8957b08c":"code","2b15f622":"code","1930b78e":"code","952d8e51":"code","08836fbf":"code","026223ab":"code","29684d21":"code","ff4267bb":"code","a3bf189f":"code","b3875fac":"code","d21e87a3":"code","518245d1":"code","ed9e3c56":"code","cd835539":"code","7266ed05":"code","0347f022":"code","2bd3093d":"code","d365a392":"code","65bbb0e4":"code","47e03afc":"code","ec20a8e8":"code","b7ea0bf5":"code","76e15f72":"code","5df281c9":"code","4cdf1472":"code","6b146e2e":"code","5b424dae":"code","eaaed707":"code","5a508938":"code","fed35df7":"markdown","ef43a61a":"markdown","28f6237a":"markdown","3ae16882":"markdown","4c5b5230":"markdown","2985e0b5":"markdown","e1c9cc1e":"markdown","4d063fa2":"markdown","c9c31b16":"markdown","fab3460e":"markdown","7fe5ae30":"markdown","d9853e04":"markdown"},"source":{"b54fa1cf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf","199bd133":"from sklearn import datasets\n\nX, y = datasets.make_blobs(n_samples = [150, 120, 90, 100], \n                                         n_features = 2, \n                                         centers = [(6, 40), (4, 50), (6, 55), (4, 65)], \n                                         cluster_std = 3.3, random_state = 1123)\nX","1c56bc6d":"cdict = {0:'red', 1:'blue', 2:'yellow', 3:'green'}\nplt.figure(figsize = (6, 6))\nfor i in np.unique(y):\n    indices = np.where(y == i)\n    plt.scatter(x = X[indices, 0], y = X[indices, 1], c = cdict[i], label = i, marker = '*', alpha = 0.5)\n    \nplt.legend()","9c5b4837":"y[np.where(y == 3)] = 0\ny[np.where(y == 2)] = 1","cc1968fd":"cdict = {0:'red', 1:'blue'}\nplt.figure(figsize = (6, 6))\nfor i in np.unique(y):\n    indices = np.where(y == i)\n    plt.scatter(x = X[indices, 0], y = X[indices, 1], c = cdict[i], label = i, marker = '*', alpha = 0.5)\n    \nplt.legend()","d696da06":"n_features = X.shape[1]\nn_outcomes = len(np.unique(y))\nprint(n_features)\nprint(n_outcomes)","07c1f726":"# Befor start the training loop, let normalize our data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(X)","d10032b6":"from tensorflow import keras\nfrom keras.layers import Dense\nfrom keras import Sequential","e5ceece0":"def shallow_net(feature, target, opt, loss, activation_function = 'sigmoid', n_epochs = 150, batch_size = 258, mean_weight_init = 0, sd_weight_init = 0.01):\n    \n    weight_initializer = tf.keras.initializers.RandomNormal(mean = mean_weight_init, stddev = sd_weight_init)\n    # Define The Model Architecture\n    model = Sequential()\n    model.add(Dense(4, activation = activation_function, input_dim = n_features, kernel_initializer = weight_initializer))\n    model.add(Dense(1, activation = 'sigmoid', kernel_initializer = weight_initializer))\n    \n    model.compile(optimizer = opt, loss = loss, metrics = ['accuracy'])\n    history = model.fit(feature, target, epochs = n_epochs, batch_size = batch_size, validation_split = 0.2, verbose = 1)\n    return(history)","a6769979":"# Model 1: shallow Network\/ Sigmoid Activation Function\/Learning Rate: 0.1\/ Scaled Features\/ Epochs: 250\/ Batch Size = 32\nopt = keras.optimizers.SGD(learning_rate = 0.1)\nmodel1 = shallow_net(features_scaled, y, opt, activation_function = 'sigmoid', n_epochs = 250, batch_size = 32, loss = 'binary_crossentropy')","60a456fc":"plt.figure(figsize = (8, 6))\nplt.plot(model1.history['loss'], label = 'train')\nplt.plot(model1.history['val_loss'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs for model1')\nplt.legend()","f1bba253":"plt.figure(figsize = (8, 6))\nplt.plot(model1.history['accuracy'], label = 'train')\nplt.plot(model1.history['val_accuracy'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs for model1')\nplt.legend()","0a786572":"# Model 2: shallow Network\/ Sigmoid Activation Function\/ Learning Rate: 0.1\/ Scaled Features\/ Epochs: 500\/ Batch Size = 32\nopt = keras.optimizers.SGD(learning_rate = 0.1)\nmodel2 = shallow_net(features_scaled, y, opt, activation_function = 'sigmoid', n_epochs = 500, batch_size = 32, loss = 'binary_crossentropy')","089b68d7":"plt.figure(figsize = (8, 6))\nplt.plot(model2.history['loss'], label = 'train')\nplt.plot(model2.history['val_loss'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs for model2')\nplt.legend()","8957b08c":"plt.figure(figsize = (8, 6))\nplt.plot(model2.history['accuracy'], label = 'train')\nplt.plot(model2.history['val_accuracy'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs for model2')\nplt.legend()","2b15f622":"# It seems that our model does not have enough time to train let him more time again\n# Model 3: shallow Network\/ Sigmoid Activation Function\/ Learning Rate: 0.1\/ Scaled Features\/ Epochs: 1000\/ Batch Size = 32\nopt = keras.optimizers.SGD(learning_rate = 0.1)\nmodel3 = shallow_net(features_scaled, y, opt, activation_function = 'sigmoid', n_epochs = 1000, batch_size = 32, loss = 'binary_crossentropy')","1930b78e":"plt.figure(figsize = (8, 6))\nplt.plot(model3.history['loss'], label = 'train')\nplt.plot(model3.history['val_loss'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs for model3')\nplt.legend()","952d8e51":"plt.figure(figsize = (8, 6))\nplt.plot(model3.history['accuracy'], label = 'train')\nplt.plot(model3.history['val_accuracy'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs for model3')\nplt.legend()","08836fbf":"# Ok, with 500 epochs, our model fit good\n# But let me change the learning Rate to 0.3 and n_epochs to 250\n\n# Model 4: shallow Network\/ Sigmoid Activation Function\/ Learning Rate: 0.3\/ Scaled Features\/ Epochs: 250\/ Batch Size = 32\nopt = keras.optimizers.SGD(learning_rate = 0.3)\nmodel4 = shallow_net(features_scaled, y, opt, activation_function = 'sigmoid', n_epochs = 250, batch_size = 32, loss = 'binary_crossentropy')","026223ab":"plt.figure(figsize = (8, 6))\nplt.plot(model4.history['loss'], label = 'train')\nplt.plot(model4.history['val_loss'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs for model4')\nplt.legend()","29684d21":"plt.figure(figsize = (8, 6))\nplt.plot(model4.history['accuracy'], label = 'train')\nplt.plot(model4.history['val_accuracy'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs for model4')\nplt.legend()","ff4267bb":"# what's happend? \n# suddenly loss decrese \n# It seems that model need to forced bigger learning rate rather than a small learning rate","a3bf189f":"# Model 5: shallow Network\/ tanh Activation Function\/ Learning Rate: 0.1\/ Scaled Features\/ Epochs: 250\/ Batch Size = 32\nopt = keras.optimizers.SGD(learning_rate = 0.1)\nmodel5 = shallow_net(features_scaled, y, opt, activation_function = 'tanh', n_epochs = 250, batch_size = 32, loss = 'binary_crossentropy')","b3875fac":"plt.figure(figsize = (8, 6))\nplt.plot(model5.history['loss'], label = 'train')\nplt.plot(model5.history['val_loss'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs for model5')\nplt.legend()","d21e87a3":"plt.figure(figsize = (8, 6))\nplt.plot(model5.history['accuracy'], label = 'train')\nplt.plot(model5.history['val_accuracy'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs for model5')\nplt.legend()","518245d1":"# With about 100 epochs we get acceptable result.","ed9e3c56":"# Model 6: shallow Network\/ Relu Activation Function\/ Learning Rate: 0.1\/ Scaled Features\/ Epochs: 250\/ Batch Size = 32\nopt = keras.optimizers.SGD(learning_rate = 0.1)\nmodel6 = shallow_net(features_scaled, y, opt, activation_function = 'relu', n_epochs = 250, batch_size = 32, loss = 'binary_crossentropy')","cd835539":"plt.figure(figsize = (8, 6))\nplt.plot(model6.history['loss'], label = 'train')\nplt.plot(model6.history['val_loss'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs for model6')\nplt.legend()","7266ed05":"plt.figure(figsize = (8, 6))\nplt.plot(model6.history['accuracy'], label = 'train')\nplt.plot(model6.history['val_accuracy'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs for model6')\nplt.legend()","0347f022":"# when we use relu activation function, our model converge verey fast","2bd3093d":"# Model 7: shallow Network\/ softsign Activation Function\/ Learning Rate: 0.1\/ Scaled Features\/ Epochs: 250\/ Batch Size = 32\nopt = keras.optimizers.SGD(learning_rate = 0.1)\nmodel7 = shallow_net(features_scaled, y, opt, activation_function = 'softsign', n_epochs = 250, batch_size = 32, loss = 'binary_crossentropy')","d365a392":"plt.figure(figsize = (8, 6))\nplt.plot(model7.history['loss'], label = 'train')\nplt.plot(model7.history['val_loss'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs for model7')\nplt.legend()","65bbb0e4":"plt.figure(figsize = (8, 6))\nplt.plot(model7.history['accuracy'], label = 'train')\nplt.plot(model7.history['val_accuracy'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs for model7')\nplt.legend()","47e03afc":"# Model 8: shallow Network\/ selu Activation Function\/ Learning Rate: 0.1\/ Scaled Features\/ Epochs: 250\/ Batch Size = 32\nopt = keras.optimizers.SGD(learning_rate = 0.1)\nmodel8 = shallow_net(features_scaled, y, opt, activation_function = 'selu', n_epochs = 250, batch_size = 32, loss = 'binary_crossentropy')","ec20a8e8":"plt.figure(figsize = (8, 6))\nplt.plot(model8.history['loss'], label = 'train')\nplt.plot(model8.history['val_loss'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs for model8')\nplt.legend()","b7ea0bf5":"plt.figure(figsize = (8, 6))\nplt.plot(model8.history['accuracy'], label = 'train')\nplt.plot(model8.history['val_accuracy'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs for model8')\nplt.legend()","76e15f72":"# Model 9: shallow Network\/ elu Activation Function\/ Learning Rate: 0.1\/ Scaled Features\/ Epochs: 250\/ Batch Size = 32\nopt = keras.optimizers.SGD(learning_rate = 0.1)\nmodel9 = shallow_net(features_scaled, y, opt, activation_function = 'elu', n_epochs = 250, batch_size = 32, loss = 'binary_crossentropy')","5df281c9":"plt.figure(figsize = (8, 6))\nplt.plot(model9.history['loss'], label = 'train')\nplt.plot(model9.history['val_loss'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs for model9')\nplt.legend()","4cdf1472":"plt.figure(figsize = (8, 6))\nplt.plot(model9.history['accuracy'], label = 'train')\nplt.plot(model9.history['val_accuracy'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs for model9')\nplt.legend()","6b146e2e":"# Model 10: shallow Network\/ exponential Activation Function\/ Learning Rate: 0.1\/ Scaled Features\/ Epochs: 250\/ Batch Size = 32\nopt = keras.optimizers.SGD(learning_rate = 0.1)\nmodel10 = shallow_net(features_scaled, y, opt, activation_function = 'exponential', n_epochs = 250, batch_size = 32, loss = 'binary_crossentropy')","5b424dae":"plt.figure(figsize = (8, 6))\nplt.plot(model10.history['loss'], label = 'train')\nplt.plot(model10.history['val_loss'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs for model10')\nplt.legend()","eaaed707":"plt.figure(figsize = (8, 6))\nplt.plot(model10.history['accuracy'], label = 'train')\nplt.plot(model10.history['val_accuracy'], label = 'test')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs for model10')\nplt.legend()","5a508938":"# conclusion: \n# In this notebook, we change three important hyperparameters in the Neural Net. \n# It seems that the Relu activation function is the best in this dataset.","fed35df7":"## Define Sallow Network","ef43a61a":"## selu activation function","28f6237a":"# Change the activation function, I want to use the hyperbolic tangant","3ae16882":"### Softsign acitvation function","4c5b5230":"## elu activation function","2985e0b5":"#### Why model1 can't converge? (think about it)\n#### Let him more time(more epoch) mabay it hasn't enogh time to train on data","e1c9cc1e":"### Rlue activation function","4d063fa2":"### Let's create a synthetic dataset","c9c31b16":"#### the shallow Network is something like this\n\n![shllow.PNG](attachment:bf205dbd-3fd0-4925-be4f-418e13edc5b1.PNG)","fab3460e":"####  In This notebook,  I want to show you how hyperparameters can affect the output of our deep learning models.","7fe5ae30":"##### first of all, I want to show you how sigmoid activation function(for hidden layers, not output layers) can affect your result","d9853e04":"## exponential activation function"}}