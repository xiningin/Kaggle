{"cell_type":{"54f67642":"code","0ef03b32":"code","64515d63":"code","0552ba81":"code","d1c4504d":"code","bc8b99af":"code","79d15a7b":"code","5ece9d02":"code","2f18bba9":"code","6571370f":"code","1fb91f57":"code","db5e9843":"code","15dee8d8":"code","a98711a3":"code","d7ea2756":"code","d153337c":"markdown","612e02c1":"markdown","4385c425":"markdown","8fdd2d21":"markdown","ffcdd6dc":"markdown","57688a73":"markdown","20ac4203":"markdown","d6ab1e87":"markdown","4c722079":"markdown"},"source":{"54f67642":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport pickle\nimport multiprocessing\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport dask.dataframe as dd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tnrange, tqdm_notebook\nfrom collections import OrderedDict\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom datetime import datetime, timedelta\nfrom matplotlib import gridspec\n\nprint(os.listdir(\"..\/input\/\"))","0ef03b32":"import warnings\nwarnings.filterwarnings('ignore')","64515d63":"df = pd.read_csv('..\/input\/PLAsTiCC-2018\/training_set.csv')\ndf_meta = pd.read_csv('..\/input\/PLAsTiCC-2018\/training_set_metadata.csv')","0552ba81":"df['unix_time'] = (df['mjd'] - 40587)*86400\ndf['datetime'] = pd.to_datetime(df['unix_time'], unit='s')","d1c4504d":"objects = df_meta['object_id'].values","bc8b99af":"def load_dmdt_images(objects, base_dir='train'):\n    dmdt_img_dict = OrderedDict()\n    for obj in objects:\n        key = '{}\/{}_dmdt.pkl'.format(base_dir, obj)\n        if os.path.isfile(key):\n            with(open(key, 'rb')) as f:\n                dmdt_img_dict[obj] = pickle.load(f)\n    return dmdt_img_dict","79d15a7b":"dmdt_img_dict = load_dmdt_images(objects, '..\/input\/plasticc_dmdt_images\/train\/train')","5ece9d02":"classes = np.sort(df_meta['target'].drop_duplicates().values)","2f18bba9":"fig = plt.figure(figsize=(15,6))\nax = sns.countplot(df_meta['target'])","6571370f":"samples = df_meta.groupby('target')['object_id', 'target'].head(1).values","1fb91f57":"def gen_plots(df, samples):\n    for sample in samples:\n        fig = plt.figure(figsize=(21,9))\n        cbar_ax = fig.add_axes([.91, .3, .03, .4])\n        outer_grid = gridspec.GridSpec(1, 2)\n        object_id = sample[0]\n        label = sample[1]\n        df_obj = df.loc[df.object_id == object_id]\n        gen_flux_plots(df_obj, object_id, label, outer_grid[0], fig)\n        viz_dmdt(object_id, label, outer_grid[1], fig, cbar_ax)\n        fig.suptitle(\"Time-series Light Curve and DMDT Images for all 6 passband for object ID - {} of class {}\".format(object_id, label), fontsize=16)\n        #rect=[0, 0, 0.91, 0.95]\n        fig.tight_layout(rect=[0, 0, 0.91, 0.95])","db5e9843":"def gen_flux_plot(df, ax, labels):\n    passband = df['passband'].drop_duplicates().values[0]\n    label = labels[passband]\n    sns.scatterplot(ax=ax, x=df['datetime'], y=df['flux'], label=label)\n    ax.set_xlim(df.iloc[0]['datetime'] - timedelta(days=20), df.iloc[-1]['datetime'] + timedelta(days=20))","15dee8d8":"def gen_flux_plots(df, object_id, label, outer_grid, fig):\n    ax = fig.add_subplot(outer_grid)\n    labels = ['u', 'g', 'r', 'i', 'z', 'Y']\n    sps = df.groupby('passband').apply(lambda x : gen_flux_plot(x, ax, labels))\n    ax.legend()\n    fig.add_subplot(ax)\n    #fig.suptitle('Time-series Light Curve for all 6 passbands for object - {} of class {}'.format(object_id, label), fontsize=16)","a98711a3":"def viz_dmdt(object_id, label, outer_grid, fig, cbar_ax):\n    dmdt_img = dmdt_img_dict[object_id]\n    inner_grid = gridspec.GridSpecFromSubplotSpec(2, 3, subplot_spec=outer_grid)\n    shared_ax = None\n    for i in range(6): #num passband\n        i_idx = 0 if i < 3 else 1\n        j_idx = i%3\n        gs = inner_grid[i_idx, j_idx]\n        ax = fig.add_subplot(gs) if shared_ax is None else fig.add_subplot(gs, sharex=shared_ax, sharey=shared_ax)\n        sns.heatmap(ax=ax, data=dmdt_img[:,:,i], cmap=\"hot\", cbar=(i==0), cbar_ax=None if i else cbar_ax)\n    #fig.suptitle(\"DMDT Images for all 6 passband for object ID - {} of class {}\".format(object_id, label), fontsize=16)\n    ","d7ea2756":"gen_plots(df, samples)","d153337c":"The kernel contains some basic visualizations for the DMDT Images that are described in the kernel - [CNN based Classification of Light Curves](https:\/\/www.kaggle.com\/pankajb64\/cnn-based-classification-of-light-curves\/)\n\nThe DMDT Images are included here as an additional dataset.\n","612e02c1":"The code to load the dmdt images, same as in the parent kernel.","4385c425":"The code below generates the plots for each object in the sample. The two plots are shown side by side to make it easy to associate characteristics.\n\nNote - You'll see 7 legend entries in the time-series plot, this is because I'm doing a group by passband and generating a plot for each group, and pandas calls apply twice on the first group (they do it for code optimization, and presently there is no way around it), so 7 different scatter plots are generted. I couldn't find a way to remove the duplicate label from the legend, but if you know how to, let me know!","8fdd2d21":"- Load the model, with the custom loss function.\n- Get individual losses for each of the sample in the training set (since we don't really know which was train and which was val originally) - maybe do this in the original kernel post training ?\n- Sort by loss values, descending.\n- For the top k losses, generate flux plots and dmdt images.\n    - Also do this for the lowest k, to know what was it that made them easy to qualify ?\n- Maybe visualize the inner layers of the CNN ? See https:\/\/github.com\/raghakot\/keras-vis and https:\/\/www.codeastar.com\/visualize-convolutional-neural-network\/","ffcdd6dc":"We'll set warning to ignore, since matplotlib generates a few warnings about incompatible axes in our case when displaying plots.","57688a73":"Lets convert the Modified Julian Date to a Pandas datetime object so it's easier to read on the plots. To do that, we first convert the MJD to a unix timestamp and then parse it as a datetime.","20ac4203":"Its uneven, so there is a class imbalance. Lets take a sample object per class and look at its Time-series light curve and its DMDT Image.","d6ab1e87":"Read it the data frames for the meta-data and the time series data for objects in the training set.","4c722079":"Lets look at the distribution of objects across classes."}}