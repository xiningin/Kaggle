{"cell_type":{"19bc68a7":"code","90ff14ab":"code","7cbee9e6":"code","4d8445f2":"markdown"},"source":{"19bc68a7":"import pickle\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nscan_types    = ['FLAIR','T1w','T1wCE','T2w']","90ff14ab":"# Define, train, and evaluate model\n# source: https:\/\/keras.io\/examples\/vision\/3D_image_classification\/\ndef get_model(width=128, height=128, depth=64, name='3dcnn'):\n    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n\n    inputs = tf.keras.Input((width, height, depth, 1))\n\n    x = tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.GlobalAveragePooling3D()(x)\n    x = tf.keras.layers.Dense(units=512, activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n\n    outputs = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")(x)\n\n    # Define the model.\n    model = tf.keras.Model(inputs, outputs, name=name)\n    \n    # Compile model.\n    initial_learning_rate = 0.0001\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n    )\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n        metrics=[\"acc\"],\n    )\n    \n    return model","7cbee9e6":"for scan_type in scan_types:\n    # load train_dataset dataset\n    tf_data_path = f'..\/input\/nifti-to-split-dataset-with-nibabel\/datasets\/{scan_type}_train_dataset'\n    with open(tf_data_path + '\/element_spec', 'rb') as in_:\n        es = pickle.load(in_)\n    train_dataset = tf.data.experimental.load(tf_data_path, es, compression='GZIP')\n    \n    # load validation_dataset\n    tf_data_path = f'..\/input\/nifti-to-split-dataset-with-nibabel\/datasets\/{scan_type}_validation_dataset'\n    with open(tf_data_path + '\/element_spec', 'rb') as in_:\n        es = pickle.load(in_)\n    validation_dataset = tf.data.experimental.load(tf_data_path, es, compression='GZIP')\n\n    # Get Model\n    model = get_model(width=128, height=128, depth=64,name=scan_type)\n    \n    # Define callbacks.\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n        f'{scan_type}_3d_image_classification.h5', save_best_only=True\n    )\n    early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n\n    epochs = 100\n    model.fit(\n        train_dataset,\n        validation_data=validation_dataset,\n        epochs=epochs,\n        shuffle=True,\n        verbose=2,\n        callbacks=[checkpoint_cb, early_stopping_cb],\n    )\n    \n    #save model\n    model.save(f'.\/models\/{scan_type}')\n    \n    # show metrics\n    fig, ax = plt.subplots(1, 2, figsize=(20, 3))\n    ax = ax.ravel()\n\n    for i, metric in enumerate([\"acc\", \"loss\"]):\n        ax[i].plot(model.history.history[metric])\n        ax[i].plot(model.history.history[\"val_\" + metric])\n        ax[i].set_title(\"{} Model {}\".format(scan_type, metric))\n        ax[i].set_xlabel(\"epochs\")\n        ax[i].set_ylabel(metric)\n        ax[i].legend([\"train\", \"val\"])","4d8445f2":"This kernel uses Tensorflow to define, train, and evaluate a model.\n\nThis is part of a larger solution found at: https:\/\/www.kaggle.com\/ohbewise\/a-rsna-mri-solution-from-dicom-to-submission"}}