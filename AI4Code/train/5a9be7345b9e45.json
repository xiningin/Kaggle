{"cell_type":{"dadcb870":"code","cd6a834b":"code","d8589442":"code","b7eb3a27":"code","d0e24b0f":"code","2fe492c5":"code","310e44c9":"code","d4dee647":"code","a2536519":"code","27277757":"code","4fd963e2":"code","0f71168d":"code","c90996f3":"code","917e88b8":"code","f8dd32a3":"code","6c78e3a3":"code","5f8f9b72":"code","c03baae5":"code","1829374b":"code","271089fd":"code","2adc28ff":"code","0258b995":"code","f925ae97":"code","de05db5c":"code","d50edd22":"code","d6c5dd52":"code","62d159f1":"code","d9ba5229":"code","1a0c04ba":"code","09cc6f66":"code","090a2a6c":"code","e1e85806":"code","547b9996":"code","dfb9add5":"code","e603ef67":"code","5edb3fae":"code","9f975230":"code","4cd15c6e":"code","488616d7":"code","79105f55":"code","f11b33c3":"code","0ce5e6e2":"code","96a1de5b":"code","fbc10174":"code","a5afd5d0":"code","92a532bc":"code","aa393938":"markdown","12cf2e76":"markdown","88a8a0c2":"markdown","e71a3382":"markdown","5bb98ff5":"markdown","15a65639":"markdown","4a421d0c":"markdown","9a21e0e7":"markdown","eb1c56bb":"markdown","06332ce0":"markdown","d358a615":"markdown","3b0eefad":"markdown","17bb4bc9":"markdown","25d4a690":"markdown","8dec966b":"markdown","1467e297":"markdown","3240895f":"markdown","a609a546":"markdown","877c6c1f":"markdown","3077c383":"markdown","17805f31":"markdown","6d9edcf6":"markdown","1ce180fb":"markdown","1e2b08e1":"markdown","a5dd98be":"markdown"},"source":{"dadcb870":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cd6a834b":"df_train = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv')\ndf_submit = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/submission.csv')","d8589442":"df_train.head()","b7eb3a27":"df_train.info()","d0e24b0f":"print(\"Number of Country_Region: \", df_train['Country_Region'].nunique())\nprint(\"Dates are ranging from day\", min(df_train['Date']), \"to day\", max(df_train['Date']), \", a total of\", df_train['Date'].nunique(), \"days\")\nprint(\"The countries that have Province\/Region given are : \", df_train[df_train['Province_State'].isna()==False]['Country_Region'].unique())","2fe492c5":"df_train.columns","310e44c9":" df_train['Province_State'].unique()","d4dee647":"plt.figure(figsize=(40,40))\ntemp_df= df_train[df_train['ConfirmedCases']>5000]\nsns.barplot(y = temp_df['Country_Region'] , x = temp_df['ConfirmedCases']>10000)\nsns.set_context('paper')\nplt.ylabel(\"Country_Region\",fontsize=30)\nplt.xlabel(\"Counts\",fontsize=30)\nplt.title(\"Counts of Countries affected by the pandemic that have confirmed cases > 5000\",fontsize=30)\nplt.xticks(rotation = 90)","a2536519":"confirmed_total_dates = df_train.groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_dates = df_train.groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_dates = confirmed_total_dates.join(fatalities_total_dates)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))\ntotal_dates.plot(ax=ax1)\nax1.set_title(\"Global confirmed cases\", size=13)\nax1.set_ylabel(\"Total Number of cases\", size=13)\nax1.set_xlabel(\"Date\", size=13)\nfatalities_total_dates.plot(ax=ax2, color='orange')\nax2.set_title(\"Global deceased cases\", size=13)\nax2.set_ylabel(\"Total Number of cases\", size=13)\nax2.set_xlabel(\"Date\", size=13)","27277757":"italy = df_train[df_train['Country_Region'] == 'Italy']\nplt.figure(figsize=(20,10))\nsns.lineplot(x = 'Date' , y = 'ConfirmedCases' , data = italy)\nplt.xticks(rotation = 90,size=12)\nplt.xlabel('Date',size=15)\nplt.ylabel('Confirmed Cases',size=15)\nplt.title('Confirmed Cases per date in Italy',size=20)\nplt.show()","4fd963e2":"italy = df_train[df_train['Country_Region'] == 'Italy']\nplt.figure(figsize=(20,10))\nsns.lineplot(x = 'Date' , y = 'Fatalities' , data = italy,color='orange')\nplt.xticks(rotation = 90,size=12)\nplt.xlabel('Date',size=15)\nplt.ylabel('Fatalities',size=15)\nplt.title('Fatalities in Italy per Date',size=20)\nplt.show()","0f71168d":"usa = df_train[df_train['Country_Region'] == 'US']\nplt.figure(figsize=(20,10))\nsns.lineplot(x = 'Date' , y = 'ConfirmedCases' , data = usa,color='g')\nplt.xticks(rotation = 90,size=13)\nplt.xlabel('Date',size=15)\nplt.ylabel('Confirmed Cases',size=15)\nplt.title('Confirmed Cases in US per Date',size=20)\nplt.show()","c90996f3":"plt.figure(figsize=(20,10))\nsns.lineplot(x = 'Date' , y = 'Fatalities' , data = usa,color='purple')\nplt.title('Fatalities in US per Date',size=20)\nplt.xticks(rotation = 90,size=13)\nplt.xlabel('Date',size=15)\nplt.ylabel('Fatalities',size=15)\nplt.show()","917e88b8":"plt.figure(figsize=(20,10))\nsns.barplot(x='Province_State',y='ConfirmedCases',data=usa,ci=None)\nplt.xticks(rotation = 90,size=13)\nplt.xlabel('Province_State',size=15)\nplt.ylabel('Confirmed Cases',size=15)\nplt.title('Confirmed Cases in US Province_State ',size=20)\nplt.show()","f8dd32a3":"#we now do the analysis of NYC as per week.\nimport warnings\nwarnings.filterwarnings('ignore')\ntemp_df = usa[usa['Province_State'] == 'New York']\ntemp_df['Date'] = pd.to_datetime(temp_df['Date'])\ntemp_df.insert(6,'Week',temp_df['Date'].dt.week)\nf,axes = plt.subplots(1,2,figsize=(12,5))\nsns.lineplot(x = 'Week',y = 'ConfirmedCases',color='r',data=temp_df,ax = axes[0])\nsns.lineplot(x = 'Week',y = 'Fatalities',color='b',data=temp_df,ax = axes[1])\n\naxes[0].title.set_text('Confirmed Cases in NYC per week')\naxes[1].title.set_text('Fatalities in NYC per week')","6c78e3a3":"china  = df_train[df_train['Country_Region'] == 'China']\n\nplt.figure(figsize=(20,10))\nsns.lineplot(x = 'Date' , y = 'ConfirmedCases' , data = china,color='aqua')\nplt.xticks(rotation = 90,size=12)\nplt.xlabel('Date',size=15)\nplt.ylabel('Confirmed Cases',size=15)\nsns.set_context('paper')\nplt.title('Confirmed Cases in China per Date',size=20)\nplt.show()","5f8f9b72":"china  = df_train[df_train['Country_Region'] == 'China']\n\nplt.figure(figsize=(20,10))\nsns.lineplot(x = 'Date' , y = 'Fatalities' , data = china,color='grey')\nplt.xticks(rotation = 90,size=12)\nplt.xlabel('Date',size=15)\nplt.ylabel('Fatalities',size=15)\nsns.set_context('paper')\nplt.title('Fatalities in China per Date',size=20)\nplt.show()","c03baae5":"plt.figure(figsize=(20,10))\nsns.barplot(x='Province_State',y='ConfirmedCases',data=china)\nplt.xticks(rotation = 90,size=13)\nplt.title('Confirmed Cases in China Province_State',size=20)\nplt.ylabel('Confirmed Cases',size=15)\nplt.xlabel('Province_State',size=15)\nplt.show()","1829374b":"#we now do the analysis of Hubei as per week.\nimport warnings\nwarnings.filterwarnings('ignore')\nchina_t = china[china['Province_State'] == 'Hubei']\nchina_t['Date'] = pd.to_datetime(china_t['Date'])\nchina_t.insert(6,'Week',china_t['Date'].dt.week)\nf,axes = plt.subplots(1,2,figsize=(12,5))\nsns.lineplot(x = 'Week',y = 'ConfirmedCases',color='r',data=china_t,ax = axes[0])\nsns.lineplot(x = 'Week',y = 'Fatalities',color='b',data=china_t,ax = axes[1])\n\naxes[0].title.set_text('Confirmed Cases in Hubei per week')\n\naxes[1].title.set_text('Fatalities in Hubei per week')","271089fd":"df_train = df_train[['Date','Province_State','Country_Region','ConfirmedCases','Fatalities']]\ndf_train.head()","2adc28ff":"#Using pd.to_datetime for adding new features\ndf_train['Date'] = pd.to_datetime(df_train['Date'])\ndf_train.insert(1,'Week',df_train['Date'].dt.week)\ndf_train.insert(2,'Day',df_train['Date'].dt.day)\ndf_train.insert(3,'DayofWeek',df_train['Date'].dt.dayofweek)\ndf_train.insert(4,'DayofYear',df_train['Date'].dt.dayofyear)\n\ndf_test['Date'] = pd.to_datetime(df_test['Date'])\ndf_test.insert(1,'Week',df_test['Date'].dt.week)\ndf_test.insert(2,'Day',df_test['Date'].dt.day)\ndf_test.insert(3,'DayofWeek',df_test['Date'].dt.dayofweek)\ndf_test.insert(4,'DayofYear',df_test['Date'].dt.dayofyear)","0258b995":"df_train.head()","f925ae97":"# Replacing all the Province_State that are null by the Country_Region values\ndf_train.Province_State.fillna(df_train.Country_Region, inplace=True)\ndf_test.Province_State.fillna(df_test.Country_Region, inplace=True)","de05db5c":"df_train.head()","d50edd22":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndf_train.Country_Region = le.fit_transform(df_train.Country_Region)\ndf_train['Province_State'] = le.fit_transform(df_train['Province_State'])\n\ndf_test.Country_Region = le.fit_transform(df_test.Country_Region)\ndf_test['Province_State'] = le.fit_transform(df_test['Province_State'])\n","d6c5dd52":"#One Hot Encoding columns\ndef one_hot(df, cols):\n    \"\"\"\n    @param df pandas DataFrame\n    @param cols a list of columns to encode \n    @return a DataFrame with one-hot encoding\n    \"\"\"\n    i = 0\n    for each in cols:\n        #print (each)\n        dummies = pd.get_dummies(df[each], prefix=each, drop_first= True)\n        if i == 0: \n            print (dummies)\n            i = i + 1\n        df = pd.concat([df, dummies], axis=1)\n    return df","62d159f1":"#Handling categorical data\n\nobjList = df_train.select_dtypes(include = \"object\").columns\ndf_train = one_hot(df_train, objList) \ndf_test = one_hot(df_test, objList) \n\nprint (df_train.shape)","d9ba5229":"#Avoiding duplicated data.\ndf_train = df_train.loc[:,~df_train.columns.duplicated()]\ndf_test = df_test.loc[:,~df_test.columns.duplicated()]\nprint (df_test.shape)","1a0c04ba":"# Dropping the object type columns\ndf_train.drop(objList, axis=1, inplace=True)\ndf_test.drop(objList, axis=1, inplace=True)\nprint (df_train.shape)","09cc6f66":"df_train.head()","090a2a6c":"#Selecting only the type Object Columns\ndf_train.select_dtypes(include = \"object\").columns","e1e85806":"df_train","547b9996":"X = df_train.drop(['Date', 'ConfirmedCases', 'Fatalities'], axis=1)\ny = df_train[['ConfirmedCases', 'Fatalities']]","dfb9add5":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import BayesianRidge \nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import make_scorer, r2_score, mean_squared_log_error\nfrom sklearn.ensemble import BaggingRegressor","e603ef67":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","5edb3fae":"y_train.head()","9f975230":"n_folds = 5\ncv = KFold(n_splits = 5, shuffle=True, random_state=42).get_n_splits(X_train.values)","4cd15c6e":"def predict_scores(reg_alg):\n    r2 = make_scorer(r2_score)\n    m = reg_alg()\n    m.fit(X_train, y_train['ConfirmedCases'])\n    y_pred = m.predict(X_test)\n    m_r = cross_val_score(m, X_train, y_train['ConfirmedCases'], cv=cv, scoring = r2)\n    sc_Cases.append(m_r)\n    \n    m.fit(X_train, y_train['Fatalities'])\n    y_pred = m.predict(X_test)\n    m_r2 = cross_val_score(m, X_train, y_train['Fatalities'], cv=cv, scoring = r2)\n    sc_Fatalities.append(m_r2)\n\n\n    \nreg_models = [KNeighborsRegressor, LinearRegression, RandomForestRegressor, GradientBoostingRegressor, DecisionTreeRegressor,BayesianRidge,\n              BaggingRegressor]\n\nsc_Cases = []\nsc_Fatalities = []\n\nfor x in reg_models:\n    predict_scores(x)","488616d7":"sc_Cases","79105f55":"sc_Fatalities","f11b33c3":"from sklearn.ensemble import BaggingRegressor","0ce5e6e2":"\n#Hyperparameter tuning\n\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n\nparam_grid = {\n              'n_estimators':[10, 30, 50, 100,250,500,750,1000,1250,1500,1750],\n              'max_samples':[2,4,6,8,10,20,40,60,100],\n              \"max_features\": [0.5, 1.0],\n              'n_jobs':[-2, -1, 1, 2, 3, 4, 5],\n              \"bootstrap_features\": [True, False]\n             }\n'''param_grid = {\"criterion\": [\"mae\"],\n              \"min_samples_split\": [10, 20, 40],\n              \"max_depth\": [2, 6, 8],\n              \"min_samples_leaf\": [20, 40, 100],\n              \"max_leaf_nodes\": [5, 20, 100],\n              }'''\n\nasdf = BaggingRegressor()\n\n\nclf_CC = RandomizedSearchCV(asdf, param_grid )\nclf_Fat = RandomizedSearchCV(asdf, param_grid )\n\nclf_CC.fit(X_train, y_train['ConfirmedCases'])\nclf_Fat.fit(X_train, y_train['Fatalities'])\n","96a1de5b":"model1 = clf_CC\nmodel1.fit(X_train, y_train['ConfirmedCases'])\n\nmodel2 = clf_Fat\nmodel2.fit(X_train, y_train['Fatalities'])","fbc10174":"df_test['ConfirmedCases'] = model1.predict(df_test.drop(['Date', 'ForecastId'], axis=1))\ndf_test['Fatalities'] = model2.predict(df_test.drop(['Date', 'ForecastId', 'ConfirmedCases'], axis=1))","a5afd5d0":"import warnings\nwarnings.filterwarnings('ignore')\ndf_results = df_test[['ForecastId', 'ConfirmedCases', 'Fatalities']] \ndf_results['ConfirmedCases'] = df_results['ConfirmedCases'].astype(int)\ndf_results['Fatalities'] = df_results['Fatalities'].astype(int)\n\ndf_results.head()","92a532bc":"df_results.to_csv('submission.csv', index=False)","aa393938":"* Now we check the Data for the country **ITALY** as we are aware that the cases per day are increasing.","12cf2e76":"We see that the **New York city** has the highest number of confirmed cases. \nWe can now find out how many cases did NYC have per date.\nAlso we can do analysis ","88a8a0c2":"We observe that the highest r2 score is for the DecisionTreeRegressor","e71a3382":"We observe that the cases per date rise tremendously. This even indicates the rate of fatalities may similarly follow the exact pattern.","5bb98ff5":"Following Italy , we now try to study the observation for the **United States** which records the **highest** number of cases and thereby multiplying to a large extent per day.","15a65639":"Now we will move on to our feature engineering and models.","4a421d0c":"We observe that China has a huge amount of cases per day.We will further do the analysis for a particular week.","9a21e0e7":"So, now we see what are the unique Province_Region. Out of curiousity , I personally wanted to have a look at it which may\/may not help as a feature for selecting during model training part.","eb1c56bb":"We will now make our Test Set Predictions.","06332ce0":"Performed Hyperparameter Tuning on DecisionTreeRegressor.","d358a615":"Filling Null or NaN values.","3b0eefad":"Using LabelEncoder for Encoding the Country_Region and Province_State.\nWe will also use OneHotEncoder further.","17bb4bc9":"**Hubei** province records the maximum cases of the pandemic. We will now do the analysis as per week.","25d4a690":"Since the US records the highest number of cases , let's check which province is affected the most.","8dec966b":"Fatalities vary per date as we observe there are some peaks which indicates that it recorded highest death rate on that day.","1467e297":"We can see the rising cases tremendously.Also the shaded region shows the 95 % confidence interval for mean.","3240895f":"Feature Engineering","a609a546":"The pattern  of confirmed cases here follows a unique shape rising from week 4 and then the curve starts to flatten","877c6c1f":" We have data of 163 countries with a tenure of 2 months which will help us to gain insights from the dataset.","3077c383":"We see that the cases started taking an elevation since the week 11 followed by fatalities.","17805f31":"The lineplot gives an idea that the cases of fatalities are increasing widely and the confidence interval of the mean can be observed as well.","6d9edcf6":"As of now I have performed the above visualisations , I will continue to make more valuable insights by performing more of them.","1ce180fb":"As we see there are null values as well it wouldn't be nice to choose it as a feature because we do not know what province the cases were from. We will try to fill these nan values by going through the countrywise dataset on other covid-19 detectors.","1e2b08e1":"The global curve shows a rich fine structure, but these numbers are strongly affected by the vector zero country, China. Given that COVID-19 started there, during the initial expansion of the virus there was no reliable information about the real infected cases. In fact, the criteria to consider infection cases was modified around 2020-02-11, which strongly perturbed the curve as you can see from the figure.","a5dd98be":"We observe that the countries majorly that involve a higher count of the pandemic are Italy followed by Iran , Spain , US , Australia ,Germany, The UK."}}