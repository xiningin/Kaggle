{"cell_type":{"f9a90b8d":"code","9d7922e7":"code","07e16bbc":"code","b60e38d1":"code","fabc9b7c":"code","5804a38f":"code","7b434e3d":"code","7ce78342":"code","d4558fef":"code","50620c7b":"code","7675577b":"code","adab3654":"code","ca379da9":"code","fb0dec2c":"code","9a614921":"code","a8d360ed":"code","711c2fd2":"code","b2476a6b":"code","0f277f95":"markdown","20875f13":"markdown","3e3258af":"markdown","b9e1fbc1":"markdown","39c72eae":"markdown","3cd7be38":"markdown","326bacdf":"markdown","3604a35f":"markdown","94e6193d":"markdown","7f1872b0":"markdown","9c9a73a6":"markdown","90758cec":"markdown","cc33ce49":"markdown","a218c5a9":"markdown","d5cc758f":"markdown","76bb73eb":"markdown","f0724b8f":"markdown","38b3aa1b":"markdown","e248a978":"markdown","471142ff":"markdown","26c0c33d":"markdown","f5ee4874":"markdown","be02c9c9":"markdown","2866cb20":"markdown","e46357c6":"markdown"},"source":{"f9a90b8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","9d7922e7":"ecoli_df =  pd.read_csv(\"..\/input\/ecoli-data-set\/ecoli.csv\",header=None,sep=\"\\s+\")\ncol_names = [\"squence_name\",\"mcg\",\"gvh\",\"lip\",\"chg\",\"aac\",\"alm1\",\"alm2\",\"site\"]\necoli_df.columns = col_names\necoli_df.head()","07e16bbc":"ecoli_df.isnull().sum()","b60e38d1":"ecoli_df.loc[:,ecoli_df.dtypes == \"object\"].columns.tolist()","fabc9b7c":"print(len(ecoli_df[\"squence_name\"].unique()) == ecoli_df.shape[0])\nprint(ecoli_df['lip'].value_counts())\nprint(ecoli_df['chg'].value_counts())","5804a38f":"def cleaning_object(ecoli_df,cols_to_drop,class_col):\n    #ob1 suppose to be squence_name\n    ecoli_df = ecoli_df.drop(cols_to_drop,axis=1)\n        \n    #drop classes with less than 10 instances\n    uni_class = ecoli_df[class_col].unique().tolist()\n    for class_label in uni_class:\n        num_rows = sum(ecoli_df[class_col] == class_label)\n        if num_rows < 10:\n            class_todrop = ecoli_df[ecoli_df[class_col] == class_label].index\n            ecoli_df.drop(class_todrop,inplace = True)\n    return ecoli_df","7b434e3d":"cleaned_ecoli_df = cleaning_object(ecoli_df,[\"squence_name\",'lip','chg'],\"site\")\ncleaned_ecoli_df.head()","7ce78342":"cleaned_ecoli_df[\"site\"].value_counts()","d4558fef":"sns.pairplot(cleaned_ecoli_df[['mcg','gvh','aac','alm1','alm2','site']],hue='site')","50620c7b":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score, make_scorer","7675577b":"param_grid = {'max_depth': np.arange(4, 7),\"criterion\":['gini','entropy'],\n              \"max_features\":np.arange(2,5),\"min_samples_split\": np.arange(3,6)}\n#f1 = make_scorer(f1_score , average='samples')\nclf = GridSearchCV(DecisionTreeClassifier(random_state=22), param_grid, cv=3,scoring =\"f1_weighted\",\n                  return_train_score=True)\nclf_scaled = GridSearchCV(DecisionTreeClassifier(random_state=22), param_grid, cv=3,scoring =\"f1_weighted\",\n                  return_train_score=True)","adab3654":"scaler = StandardScaler()\n#Scale the cleaned data\nscaled_ecoli_df = scaler.fit_transform(cleaned_ecoli_df[['mcg','gvh','aac','alm1','alm2']])\n#put the scaled data back into data frame with column names\nscaled_ecoli_df = pd.DataFrame(scaled_ecoli_df,columns = ['mcg','gvh','aac','alm1','alm2'])\n#Merge that with the class column.\nscaled_ecoli_df = scaled_ecoli_df.merge(cleaned_ecoli_df[\"site\"].to_frame(),left_index=True, right_index=True)","ca379da9":"clf.fit(cleaned_ecoli_df[['mcg','gvh','aac','alm1','alm2']],cleaned_ecoli_df[\"site\"])\nclf_scaled.fit(scaled_ecoli_df[['mcg','gvh','aac','alm1','alm2']],scaled_ecoli_df[\"site\"])","fb0dec2c":"print('The best score for raw data decision tree: ', clf.best_score_)\nprint('The best score for scaled data decision tree: ',clf_scaled.best_score_)","9a614921":"print('The best combination of raw data decision tree: ',clf.best_params_)\nprint('The best combination of scaled data decision tree: ',clf_scaled.best_params_)","a8d360ed":"from sklearn.metrics import plot_confusion_matrix","711c2fd2":"#Confusion matrix for the raw features model\nplot_confusion_matrix(clf,cleaned_ecoli_df[['mcg','gvh','aac','alm1','alm2']],\n                      cleaned_ecoli_df[\"site\"])","b2476a6b":"#Confusion matrix for the scaled features model\nplot_confusion_matrix(clf_scaled,scaled_ecoli_df[['mcg','gvh','aac','alm1','alm2']],\n                      scaled_ecoli_df[\"site\"])","0f277f95":"Number of unique value in squence name is equal to the number of rows of the data frame. \n\nEach of the squence_name appear only once, it is similar to a unique identifier of each row. \n\nTherefore, we *can drop* this column without affecting our further analysis and model building.\n\n","20875f13":"\n*Assigning column names*","3e3258af":"# Clean the data","b9e1fbc1":"# Model building","39c72eae":"A linear trend between alm1 and alm2. As the graph shown, for alm1 and alm2 with values lowers than 0.5, the class is mainly cp, for the values higher than 0.5, the class is mainly imU (green). And there are some class pp below the straight line. And the other classes are scatter around. \n\nFor mch with others variabels, we can see there are some clusters. ","3cd7be38":"Above confusion matrix shows that most of the predictions are correct. However, the model does not perform well when predicting imU. There are 13 instances of imU were predicted as im in the model without scale the data. And 2 instances and 1 instance are predicted as om and cp respectively while they are imU in fact. ","326bacdf":"Below will display the confustion matrices","3604a35f":"lip and chg are the binary columns, we will drop them from the visualization. Meanwhile, lip has only either 0.48 or 1.","94e6193d":"# Decision Tree example","7f1872b0":"Inspect the data.\nCheck whether there is any obvious missing values.","9c9a73a6":"Although model without scaling perform better in term of average cross-valdiation f1 scores, it is not necessary always the case. The training results depend on the way the train data is splitted in cross validation. Each time the code run, the data is splitted randomly. \n\nIn fact, decision tree do not require feature scaling to be performed as they are not sensitive to the the variance in the data.\n\nTo sum up, the difference between the raw model and scaled model is due to the split of the folds in the cross validartion.","90758cec":"Scale the data by standarize them. (PS Scaling doesn't affect the performace of decision tree)\nTo show you scaling doesn't affect the structure of decision tree, I built two decision trees, one with scaled data, another withou.","cc33ce49":"Show the two non-numerical columns.","a218c5a9":"We can see the squence_name column is dropped.\nAnd below table shows that there is no more classes having less than 10 instances.","d5cc758f":"The results above show that almost all values in chg are 335.\n\nAnd there are only 10 rows with 1. \n\nDropping classes having less than 10 instances, there is only 3 instances in lip is 1, so I drop lip and chg.","76bb73eb":"# Data Preprocessing","f0724b8f":"# Train-test split affects how the decision tree constructed\n\nBoth models did not perform well when predicting imU. In fact, their predictions or model itself are quite similar, since scaling would not affect the performance of decision tree. The split of the tree are based on gini index or information gain(entropy), scaling does not affect the those values. As we perfore cross-validation, the data used to build the models are different, therefore, the information gain or gini index from the data are different each time. \n\nThe 'random' split of the data is affecting how the tree split, since a subset of the data might have differnet distribution, information gain or gini index then other subset. \n\nIf the sclaed data and the raw data use the same data to build the model, they should obtain the same split.","38b3aa1b":"# Models comparison","e248a978":"Build the model with raw features:","471142ff":"A function to clean the data, the first thing is to drop the squence_name column. The second is to drop the classes that have less than 10 instances.","26c0c33d":"The above plots show the actual values of compressive Strength against the predicted values. The closer the data points to the straight line, the more accuracy the prediction is. \n\nSupport vector regressor performs better than the other estimators. \n\nThe predictions of decision tree regression are given by the average of the value of the dependent variables(features) in that leaf node. So we can see the predictions of the regression tree are lying horizontally, which means they made the same predicted values. The instances in the same leaf node will make the same prediction. In other words, the data points with the same predicted value are in the same left node.\n\nSVM Regression tries to fit as many instances as possible on the *street* while limiting margin violations. It can be fitted to non-linear data. As a result, the data points of SVR are much more closer to the actual values. The hyperplane is more likely to wiggle aroudn the data points.","f5ee4874":"Grid search for the best combination of parameters.","be02c9c9":"Above confusion matrix shows that most of the predictions are correct. However, the model does not perform well when predicting imU. There are 13 instances of imU were predicted as im in the model without scale the data.","2866cb20":"# Data Visualization","e46357c6":"The best parameters are differnt as shown below.Max features is the only parameter that has the same value across this tow model."}}