{"cell_type":{"2e64ba18":"code","09d238e4":"code","e2c7e57a":"code","279e5800":"code","1ee89f89":"code","2c2b04d8":"code","5a626951":"code","2f8e5aa3":"code","9ab60794":"code","d4d906be":"code","d3b58eb0":"code","f10442b3":"code","270179a4":"code","8808bcfd":"code","19906d0e":"code","4ed94838":"code","9c91f75b":"markdown","9c90855e":"markdown","3c77431d":"markdown","517e62a6":"markdown","37a4d281":"markdown","fe17ddcb":"markdown","0ca1fe41":"markdown","404f6fce":"markdown","a1edcc23":"markdown","6df7533f":"markdown","4cb7f21c":"markdown"},"source":{"2e64ba18":"import numpy as np\nimport tensorflow as tf\nimport keras\nimport cv2\nfrom keras.layers import MaxPool2D,Conv2D,UpSampling2D,Input,Dropout\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import img_to_array\nimport os\nfrom tqdm import tqdm\nimport re\nimport matplotlib.pyplot as plt\nimport random\nfrom torch.utils.data import DataLoader","09d238e4":"import warnings\nwarnings.filterwarnings('ignore')","e2c7e57a":"!pip install torchsummary\nfrom torchsummary import summary","279e5800":"home = '\/kaggle\/input\/landscape-image-colorization\/landscape Images\/'\ntotal_images = len(os.listdir(home+'color'))","1ee89f89":"random_indices = random.sample(list(range(total_images)),total_images)\ntrain_nums = round(total_images*0.8)\ntrain_indices = random_indices[:train_nums]\ntest_indices = random_indices[train_nums:]\nlen(train_indices), len(test_indices)","2c2b04d8":"import os\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torch.utils.data import Dataset\nimport torch.nn.functional as F\nimport torch\nfrom skimage.color import rgb2lab, lab2rgb\n\nclass EncoderDataset(Dataset):\n    def __init__(self, indices, img_dir, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        self.img_indices = indices\n        self.gray_path = img_dir+'gray\/'\n        self.color_path = img_dir+'color\/'\n    \n    def __len__(self):\n        return len(self.img_indices)\n        \n    def __getitem__(self, idx):\n        img_name = str(idx)+'.jpg'\n        image = read_image(self.gray_path+img_name)\n        image = image.unsqueeze(0)\n        image = F.interpolate(image,(160,160))\n        image = image.squeeze(0)\n        image = image.permute(1,2,0)\n        image = image.repeat(1,1,3)\n        image = image.permute(2,0,1)\n        label = read_image(self.color_path+img_name)\n        label = label.unsqueeze(0)\n        label = F.interpolate(label,(160,160))\n        label = label.squeeze(0)\n        label = label.permute(1,2,0)\n        label = label.permute(2,0,1)\n        image = torch.tensor(rgb2lab(image.permute(1,2,0)\/255))\n        label = torch.tensor(rgb2lab(label.permute(1,2,0)\/255))\n        \n        image = (image + torch.tensor([0, 128, 128])) \/ torch.tensor([100, 255, 255])\n        label = (label + torch.tensor([0, 128, 128])) \/ torch.tensor([100, 255, 255])\n        \n        image = image.permute(2,0,1)\n        label = label.permute(2,0,1)\n        #Use L channel from image to predict a,b channels of label\n        image = image[:1,:,:]\n        label = label[1:,:,:] \n        return image, label","5a626951":"## Not Required. Placeholder kept for future improvement\nimport torch\nfrom torchvision.transforms import transforms\n\ntrain_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n#     transforms.RandomHorizontalFlip(),\n    transforms.ToTensor()]\n)\n\ntest_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor()]\n)","2f8e5aa3":"train_dataset = EncoderDataset(indices = train_indices,img_dir = home)\ntest_dataset = EncoderDataset(indices = test_indices,img_dir = home)\ntrain_dataloader = DataLoader(train_dataset,batch_size=16,shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)","9ab60794":"img,label = next(iter(train_dataloader))\nsample_image,sample_label = img[0], label[0]\nprint(sample_image.shape, sample_label.shape)","d4d906be":"fig = plt.figure(figsize=(16,16))\nplt.subplot(441)\nplt.imshow(sample_image.permute(1,2,0),cmap='gray')\nplt.title('Image - Gray Scale \"L\" Channel')\nplt.subplot(442)\nplt.imshow(sample_label.permute(1,2,0)[:,:,0],cmap='Greens')\nplt.title('Lab Image - \"a\" Channel')\n\nplt.subplot(443)\nplt.imshow(sample_label.permute(1,2,0)[:,:,1],cmap='Blues')\nplt.title('Lab Image - \"b\" Channel')\n\nplt.subplot(444)\ncolor_image = torch.cat((sample_image,sample_label),dim=0).permute(1,2,0)\ncolor_image = color_image * torch.tensor([100,255,255]) -torch.tensor([0,128,128])\ncolor_image = lab2rgb(color_image)\nplt.imshow(color_image)\nplt.title('RGB Image')\nplt.show()","d3b58eb0":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvAutoencoder(nn.Module):\n    def __init__(self):\n        super(ConvAutoencoder, self).__init__()\n        ## encoder layers ##\n        self.conv1 = nn.Conv2d(1, 64, 3,stride=1,padding=1)\n        self.conv2 = nn.Conv2d(64, 64, 3, stride=2,padding=1)\n        self.conv3 = nn.Conv2d(64, 128, 3,stride=2,padding=1)\n        self.conv4 = nn.Conv2d(128, 256, 3,stride=2,padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        ## decoder layers ##\n        self.t_conv1 = nn.ConvTranspose2d(256, 128, 3, stride=2,padding=1,output_padding=1)\n        self.t_conv2 = nn.ConvTranspose2d(256, 64, 3, stride=2,padding=1,output_padding=1)\n        self.t_conv3 = nn.ConvTranspose2d(128, 128, 3, stride=2,padding=1,output_padding=1)\n        self.t_conv4 = nn.ConvTranspose2d(192, 15, 3, stride=1,padding=1)\n        self.dropout = nn.Dropout(0.2)\n        self.converge = nn.Conv2d(16,2,3,stride=1,padding=1)\n\n    def forward(self, x):\n        x1 = F.relu(self.conv1(x))\n        x2 = F.relu(self.conv2(x1))\n        x3 = F.relu(self.conv3(x2))\n        x4 = F.relu(self.conv4(x3))\n        xd = F.relu(self.t_conv1(x4))\n        xd = torch.cat((xd, x3), dim=1)\n        xd = self.dropout(xd)\n        xd = F.relu(self.t_conv2(xd))\n        xd = torch.cat((xd, x2), dim=1)\n        xd = self.dropout(xd)\n        xd = F.relu(self.t_conv3(xd))\n        xd = torch.cat((xd, x1), dim=1)\n        xd = self.dropout(xd)\n        xd = F.relu(self.t_conv4(xd))\n        xd = torch.cat((xd, x), dim=1)\n        x_out = F.relu(self.converge(xd))\n        return x_out\n\nmodel = ConvAutoencoder()\nprint(model)\nmodel = model.to('cuda')","f10442b3":"summary(model,input_size=(1,160,160))","270179a4":"# specify loss function\ncriterion = nn.MSELoss()\n\n# specify optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","8808bcfd":"# number of epochs to train the model\nn_epochs = 30\ntrain_losses = []\ntest_losses = []\nfor epoch in range(1, n_epochs+1):\n    train_loss = 0.0\n    for data in tqdm(train_dataloader):\n        images, labels = data\n        images = images.float().to('cuda')\n        labels = labels.float().to('cuda')\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()*images.size(0)\n            \n    train_loss = train_loss\/len(train_dataloader)\n    train_losses.append(train_loss)\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n        epoch, \n        train_loss\n        ))\n    \n    test_loss = 0\n    # Turn off gradients for validation, saves memory and computations\n    with torch.no_grad():\n        model.eval()\n        for images, labels in test_dataloader:\n            images, labels = images.to('cuda'), labels.to('cuda')\n            output = model(images)\n            loss = criterion(output, labels)\n            test_loss += loss.item()*images.size(0)                \n    model.train()\n    test_loss = test_loss\/len(test_dataloader)\n    print(\"Test Loss: {:.3f}.. \".format(test_loss))\n    test_losses.append(test_loss)","19906d0e":"##Train Data\ni=0\nwhile i<20:\n    test_img,test_label = next(iter(train_dataloader))\n    pred = model.forward(test_img[0].float().cuda().view(1,1,160,160))\n    lab_pred = torch.cat((test_img[0].view(1,160,160),pred[0].cpu()),dim=0)\n    lab_pred_inv_scaled = lab_pred.permute(1,2,0) * torch.tensor([100,255,255]) - torch.tensor([0,128,128])\n    rgb_pred = lab2rgb(lab_pred_inv_scaled.detach().numpy())\n    fig = plt.figure(figsize=(10,10))\n    plt.subplot(221)\n    plt.imshow(test_img[0].permute(1,2,0),cmap='gray')\n    plt.title('GrayScale Image')\n    plt.subplot(222)\n    plt.imshow(rgb_pred)\n    plt.title('Predicted Color Image')\n    plt.show()\n    i+=1","4ed94838":"##Test Data\ni=0\nwhile i<20:\n    test_img,test_label = next(iter(test_dataloader))\n    pred = model.forward(test_img[0].float().cuda().view(1,1,160,160))\n    lab_pred = torch.cat((test_img[0].view(1,160,160),pred[0].cpu()),dim=0)\n    lab_pred_inv_scaled = lab_pred.permute(1,2,0) * torch.tensor([100,255,255]) - torch.tensor([0,128,128])\n    rgb_pred = lab2rgb(lab_pred_inv_scaled.detach().numpy())\n    fig = plt.figure(figsize=(10,10))\n    plt.subplot(221)\n    plt.imshow(test_img[0].permute(1,2,0),cmap='gray')\n    plt.title('GrayScale Image')\n    plt.subplot(222)\n    plt.imshow(rgb_pred)\n    plt.title('Predicted Color Image')\n    plt.show()\n    i+=1","9c91f75b":"## <font color = green> Create Datasets <\/font>","9c90855e":"## <font color = green> Convolution Autoencoder <\/font>\n    - Network takes 1 channel input\n    - No maxpool layer is added at the encoder step\n    - Transpose Convolution takes place in decoder step\n    - Decoder outputs are concatenated with encoder output of the same layer\n    - Dropout layer added in decoder layer only\n    - Final layer CNN is the converging layer which outputs 2 channels","3c77431d":"## <font color = green>Visualize Prediction <\/font>\n - While the model is trained as per expectation and producing colors which are not way-off, this model can be set for a base line model. ","517e62a6":"## <font color = green> Sample Image <\/font>\n    - Remember we have 1 color channel input for image and 2 color channel output of label","37a4d281":"## <font color = green> Encoder Dataset <\/font>\n> Class takes input of training and test indices and creates datasets accordingly\n> Transformation of images are done inside dataset class only\n\n### <font color = orange> Steps <\/font>\n - read image\n - repeat grayscale image channel 3 times to create a prototype RGB image. Later the two other dimensions will be thrown away. This step is just to cconvert RGB image to LAB color space\n - divide RGB image by 255 to make values between (0,1)\n - Output of rgb2lab() provides L in range of (0,100), a in range of (-128(Green), 127(Red)) and b in range of (-128(Blue), 127(Yellow)). Hence tensor([0,128,128]) is added to each of the dimension followed by normalizing by tensor([100,255,255]).\n - Take L channel from GrayScale image to predict a,b Channel from Color Image\n\n### Note\n - Due to functional constraints permute() have been called multiple times. In Future improvement they will be reduced.","fe17ddcb":"## <font color = green> Set up Training and Test Indices <\/font>\n> 80% images are used for training","0ca1fe41":"<th><Name><\/th>","404f6fce":"<th><Name><\/th>","a1edcc23":"## <font color = green> Visualize Image <\/font>\n    Grayscale and Color images produce same L (intensity) channel pixel values. Here first training image is plotted which is grayscale image. Followed by, from Label(Color image) 'a' and 'b' channels are plotted. Finally Grayscale image L channel and Color image 'a', 'b' channels are concatenated followed by lab2rgb() providing RGB channel output (plotted at the last)","6df7533f":"## <font color = green> Set Paths<\/font>","4cb7f21c":"## <font color = green> Import Libraries<\/font>"}}