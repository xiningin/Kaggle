{"cell_type":{"aa947b0f":"code","bf161fdd":"code","b68957db":"code","bf725997":"code","555ebfcb":"code","6b7e66d9":"code","3f3b45a9":"code","6cfd5ad2":"code","63fc25e5":"code","c833c721":"code","8ba46c21":"code","72bbc41f":"code","bb37af34":"code","aff11157":"code","ee6a30e3":"code","f34487a5":"code","54f098dc":"code","f6d869a1":"code","b909060c":"code","363144cd":"code","d8d376f4":"code","380408c8":"code","0a5e1599":"code","2d688d22":"code","ad02621f":"code","c08ba9da":"code","71da9594":"code","de8f649f":"code","30f742af":"code","b0ad6087":"code","6df01f95":"markdown","aa5ea217":"markdown","e98f3b1e":"markdown","4e2de300":"markdown","67908b02":"markdown","a9ce1513":"markdown","55b5ce28":"markdown","7b64fe79":"markdown","33ebbba6":"markdown","e73478ce":"markdown","189b5db9":"markdown","f81fc2f8":"markdown","ad8714ee":"markdown","81c64696":"markdown","dcc78efd":"markdown","f606153b":"markdown","cdcacd7f":"markdown","93881d9a":"markdown","79ec3a62":"markdown"},"source":{"aa947b0f":"!pip install '\/kaggle\/input\/torch-15\/yacs-0.1.7-py3-none-any.whl'\n!pip install '\/kaggle\/input\/torch-15\/fvcore-0.1.1.post200513-py3-none-any.whl'\n!pip install '\/kaggle\/input\/pycocotools\/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl'\n!pip install '\/kaggle\/input\/detectron2\/detectron2-0.1.3cu101-cp37-cp37m-linux_x86_64.whl'","bf161fdd":"import numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\nimport seaborn as sns\n%matplotlib inline\n\nimport cv2\nimport itertools\nimport copy\n\n\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor, DefaultTrainer\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.structures import BoxMode\nimport detectron2.data.transforms as T\nfrom detectron2.data import detection_utils as utils","b68957db":"DATA_DIR  = '\/kaggle\/input\/global-wheat-detection\/train\/'\nTEST_DIR  = '\/kaggle\/input\/global-wheat-detection\/test\/'\nList_Data_dir = os.listdir(DATA_DIR)","bf725997":"# for example model\n\nMODEL_USE = 3\n\nif MODEL_USE == 1:\n    MODEL = 'COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml'\n    WEIGHT_PATH = '..\/input\/global-wheat-detection-model\/Detectron_2\/faster_rcnn_R_50_FPN_3x.pth'\nelif MODEL_USE == 2:\n    MODEL = 'COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml'\n    WEIGHT_PATH = '..\/input\/global-wheat-detection-model\/Detectron_2_v2\/R-50_5k_augmen.pth'\nelif MODEL_USE == 3:\n    MODEL = 'COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml'\n    WEIGHT_PATH = '..\/input\/global-wheat-detection-model\/Detectron_2_v2\/R-50_10k_augmen.pth'\n    \nMAX_ITER = 5000 #10000","555ebfcb":"raw = pd.read_csv('\/kaggle\/input\/global-wheat-detection\/train.csv')\nraw","6b7e66d9":"print(f'Total number of train images: {raw.image_id.nunique()}')\nprint(f'Total number of test images: {len(os.listdir(TEST_DIR))}')","3f3b45a9":"plt.figure(figsize=(15,8))\nplt.title('Wheat Distribution', fontsize= 20)\nsns.countplot(x=\"source\", data=raw)\n\n# based on the chart, there are seven types of wheat from images data, with the most types 'ethz_1' and the least is 'inrae_1'","6cfd5ad2":"# Extract bbox column to xmin, ymin, width, height, then create xmax, ymax, and area columns\n\nraw[['xmin','ymin','w','h']] = pd.DataFrame(raw.bbox.str.strip('[]').str.split(',').tolist()).astype(float)\nraw['xmax'], raw['ymax'], raw['area'] = raw['xmin'] + raw['w'], raw['ymin'] + raw['h'], raw['w'] * raw['h']\nraw","63fc25e5":"def show_image(image_id):\n    \n    fig, ax = plt.subplots(1, 2, figsize = (24, 24))\n    ax = ax.flatten()\n    \n    bbox = raw[raw['image_id'] == image_id ]\n    img_path = os.path.join(DATA_DIR, image_id + '.jpg')\n    \n    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n    image \/= 255.0\n    image2 = image\n    \n    ax[0].set_title('Original Image')\n    ax[0].imshow(image)\n    \n    for idx, row in bbox.iterrows():\n        x1 = row['xmin']\n        y1 = row['ymin']\n        x2 = row['xmax']\n        y2 = row['ymax']\n        label = row['source']\n        \n        cv2.rectangle(image2, (int(x1),int(y1)), (int(x2),int(y2)), (255,255,255), 2)\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        cv2.putText(image2, label, (int(x1),int(y1-10)), font, 1, (255,255,255), 2)\n    \n    ax[1].set_title('Image with Bondary Box')\n    ax[1].imshow(image2)\n\n    plt.show()","c833c721":"show_image(raw.image_id.unique()[1111])","8ba46c21":"# split train, val\nunique_files = raw.image_id.unique()\n\ntrain_files = set(np.random.choice(unique_files, int(len(unique_files) * 0.90), replace = False))\ntrain_df = raw[raw.image_id.isin(train_files)]\ntest_df = raw[~raw.image_id.isin(train_files)]","72bbc41f":"print(len(train_df.image_id.unique()), len(test_df.image_id.unique()))","bb37af34":"def custom_dataset(df, dir_image):\n    \n    dataset_dicts = []\n    \n    for img_id, img_name in enumerate(df.image_id.unique()):\n        \n        record = {}\n        image_df = df[df['image_id'] == img_name]\n        img_path = dir_image + img_name + '.jpg'\n        \n        record['file_name'] = img_path\n        record['image_id'] = img_id\n        record['height'] = int(image_df['height'].values[0])\n        record['width'] = int(image_df['width'].values[0])\n                \n        objs = []\n        for _, row in image_df.iterrows():\n            \n            x_min = int(row.xmin)\n            y_min = int(row.ymin)\n            x_max = int(row.xmax)\n            y_max = int(row.ymax)\n            \n            poly = [(x_min, y_min), (x_max, y_min),\n                    (x_max, y_max), (x_min, y_max) ]\n            \n            poly = list(itertools.chain.from_iterable(poly))\n            \n            obj = {\n               \"bbox\": [x_min, y_min, x_max, y_max],\n               \"bbox_mode\": BoxMode.XYXY_ABS,\n               \"segmentation\": [poly],\n               \"category_id\": 0,\n               \"iscrowd\" : 0\n                \n                  }\n            \n            objs.append(obj)\n            \n        record['annotations'] = objs\n        dataset_dicts.append(record)\n        \n    return dataset_dicts","aff11157":"def custom_mapper(dataset_dict):\n    # Implement a mapper, similar to the default DatasetMapper, but with your own customizations\n    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n    transform_list = [T.Resize((800,800)),\n                      T.RandomFlip(prob=0.5, horizontal=False, vertical=True),\n                      T.RandomFlip(prob=0.5, horizontal=True, vertical=False), \n                      ]\n    image, transforms = T.apply_transform_gens(transform_list, image)\n    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n\n    annos = [\n        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n        for obj in dataset_dict.pop(\"annotations\")\n        if obj.get(\"iscrowd\", 0) == 0\n    ]\n    instances = utils.annotations_to_instances(annos, image.shape[:2])\n    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n    return dataset_dict\n\n\nclass WheatTrainer(DefaultTrainer):\n    \n    @classmethod\n    def build_train_loader(cls, cfg):\n        return build_detection_train_loader(cfg, mapper=custom_mapper)\n","ee6a30e3":"def register_dataset(df, dataset_label='wheat_train', image_dir = DATA_DIR):\n    \n    # Register dataset - if dataset is already registered, give it a new name    \n    try:\n        DatasetCatalog.register(dataset_label, lambda d=df: custom_dataset(df, image_dir))\n        MetadataCatalog.get(dataset_label).set(thing_classes = ['wheat'])\n    except:\n        # Add random int to dataset name to not run into 'Already registered' error\n        n = random.randint(1, 1000)\n        dataset_label = dataset_label + str(n)\n        DatasetCatalog.register(dataset_label, lambda d=df: custom_dataset(df, image_dir))\n        MetadataCatalog.get(dataset_label).set(thing_classes = ['wheat'])\n\n    return MetadataCatalog.get(dataset_label), dataset_label","f34487a5":"metadata, train_dataset = register_dataset(train_df)\nmetadata, val_dataset = register_dataset(test_df, dataset_label='wheat_test')","54f098dc":"MODEL","f6d869a1":"cfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(MODEL))\ncfg.DATASETS.TRAIN = (train_dataset,)\ncfg.DATASETS.TEST = ()\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(MODEL)  \ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR =  0.00025 \ncfg.SOLVER.MAX_ITER = MAX_ITER\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128     \ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = WheatTrainer(cfg)","b909060c":"# create data loader\n# code from https:\/\/www.kaggle.com\/julienbeaulieu\/detectron2-wheat-detection-eda-training-eval\n\ntrain_data_loader = trainer.build_train_loader(cfg)\ndata_iter = iter(train_data_loader)\nbatch = next(data_iter)","363144cd":"# visualization\n\nrows, cols = 1, 2\nplt.figure(figsize=(20,20))\n\nfor i, per_image in enumerate(batch[:4]):\n    \n    plt.subplot(rows, cols, i+1)\n    \n    # Pytorch tensor is in (C, H, W) format\n    img = per_image[\"image\"].permute(1, 2, 0).cpu().detach().numpy()\n    img = utils.convert_image_to_rgb(img, cfg.INPUT.FORMAT)\n\n    visualizer = Visualizer(img, metadata=metadata, scale=0.5)\n\n    target_fields = per_image[\"instances\"].get_fields()\n    labels = None\n    vis = visualizer.overlay_instances(\n        labels=labels,\n        boxes=target_fields.get(\"gt_boxes\", None),\n        masks=target_fields.get(\"gt_masks\", None),\n        keypoints=target_fields.get(\"gt_keypoints\", None),\n    )\n    plt.imshow(vis.get_image()[:, :, ::-1])","d8d376f4":"trainer.resume_or_load(resume=False)\ntrainer.train()","380408c8":"# model + path\n\nprint(MODEL)\nprint(WEIGHT_PATH)","0a5e1599":"cfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(MODEL))\ncfg.MODEL.WEIGHTS = WEIGHT_PATH\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\ncfg.DATASETS.TEST = ('wheat_test', )\npredictor = DefaultPredictor(cfg)","2d688d22":"evaluator = COCOEvaluator(val_dataset, cfg, False, output_dir=\".\/output\/\")\nval_loader = build_detection_test_loader(cfg, val_dataset)\ninference_on_dataset(trainer.model, val_loader, evaluator)","ad02621f":"df_sub = pd.read_csv('..\/input\/global-wheat-detection\/sample_submission.csv')\ndf_sub","c08ba9da":"# CONFIG\n\nfont = cv2.FONT_HERSHEY_SIMPLEX     \nfontScale = 1 \ncolor = (255, 255, 0)\nthickness = 2\nresults = []\n\ndef format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)\n\n\ndef result_show(df, color):\n    \n    for image_id in df_sub['image_id']:\n        im = cv2.imread('{}\/{}.jpg'.format(TEST_DIR, image_id))\n        boxes = []\n        scores = []\n        labels = []\n        outputs = predictor(im)\n        out = outputs[\"instances\"].to(\"cpu\")\n        scores = out.get_fields()['scores'].numpy()\n        boxes = out.get_fields()['pred_boxes'].tensor.numpy().astype(int)\n        labels= out.get_fields()['scores'].numpy()\n        boxes = boxes.astype(int)\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        result = {'image_id': image_id,'PredictionString': format_prediction_string(boxes, scores)}\n        results.append(result)\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB).astype(np.float32)\n        im \/= 255.0\n        \n        for b,s in zip(boxes,scores):\n            cv2.rectangle(im, (b[0],b[1]), (b[0]+b[2],b[1]+b[3]), color, thickness)\n            cv2.putText(im, '{:.2}'.format(s), (b[0],b[1]), font, 1, color, thickness)\n                \n        plt.figure(figsize=(12,12))\n        plt.imshow(im)","71da9594":"result_show(df_sub['image_id'], color = (255, 255, 255))","de8f649f":"result_show(df_sub['image_id'], color = (255, 255, 0))","30f742af":"result_show(df_sub['image_id'], color = (255, 0, 0))","b0ad6087":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.to_csv('submission.csv', index=False)\ntest_df","6df01f95":"<h1>Introduction<\/h1>\n\n<p style=\"text-align:justify;\">This competition wants us to predict bounding boxes around whaet heads from images of wheat plants, to solve this problom we have a sample of image, and csv file containing the image_id (the unique image ID), the width and height of the images, and bounding box, formatted as a Python-style list of [xmin, ymin, width, height]<\/p>\n\n***\n\nHello, this is my second notebook in this competition, if you want to see my other notebook, [Global Wheat Detection FASTER R-CNN [EDA - AUGMENTATION - COMPARE MODELS]](https:\/\/www.kaggle.com\/dhiiyaur\/fasterrcnn-eda-augmentation-compare-models). The main idea of this notebook its to compare models in detectron 2 with this dataset ( test with Nvidia T4).\n","aa5ea217":"## Evaluation and Inference","e98f3b1e":"<h3><center>Thank you for reading my notebook, upvote if you like this notebook    :)<h3><center>\n    \n****","4e2de300":"![11Untitled.png](attachment:11Untitled.png)","67908b02":"## Data Preprocessing","a9ce1513":"## Load Data and Simple EDA","55b5ce28":"# Result","7b64fe79":"## Create a model and training","33ebbba6":"## With Augmentation\n\n> Augmentation\n> * RandomFlip Vertical (0.5)\n> * RandomFlip Horizontal (0.5)\n> * iter 5k & 10k\n\n>![Untitled.png](attachment:Untitled.png)","e73478ce":"## Model 3\n\n> - faster_rcnn_R_50_FPN_3x\n> - Iter 10000\n> - RandomFlip Vertical (0.5)\n> - RandomFlip Horizontal (0.5)","189b5db9":"## Model 2\n\n> - faster_rcnn_R_50_FPN_3x\n> - Iter 5000\n> - RandomFlip Vertical (0.5)\n> - RandomFlip Horizontal (0.5)","f81fc2f8":"<h1><center>DETECTRON 2 COMPARE MODELS<\/h1>\n    \n***","ad8714ee":"## Import necessary libraries","81c64696":"## Without Augmentation\n\n> ![Untitled.png](attachment:Untitled.png)","dcc78efd":"## Model 1\n\n> - faster_rcnn_R_50_FPN_3x\n> - Iter 5000","f606153b":"## Changelog\n\n### V1\n\n> - add new model\n> - add augmentation","cdcacd7f":"### Config","93881d9a":"## References\n\n\n* https:\/\/github.com\/facebookresearch\/detectron2\n\n* https:\/\/www.kaggle.com\/dhiiyaur\/pytorch-fasterrcnn-eda-augmentation-training\n    \n* https:\/\/www.kaggle.com\/orkatz2\/inference-detectron2-resnest","79ec3a62":"## Augmentation Visualization"}}