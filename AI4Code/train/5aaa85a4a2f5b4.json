{"cell_type":{"edd526d5":"code","3cdd485b":"code","1441b3bc":"code","affa59b4":"code","c93a0a8c":"code","f408ba7e":"code","0c2bad30":"code","b00b0333":"code","f7ff8fa9":"code","dc3df33a":"code","611d7efc":"code","5238a34c":"code","8d8d5417":"code","9a79a192":"code","a481a0fd":"code","3e93b4c3":"code","23488967":"code","5264a508":"code","03552958":"code","033634be":"code","03f45f15":"code","9aa31cdd":"code","9f9034a1":"code","16ea7eb9":"code","fc53281f":"code","12c624e4":"code","750d39b1":"code","0e5b3a46":"code","d7ae31e9":"code","7f84a3be":"code","47929d40":"code","79382596":"code","6af6c91e":"code","47075dc2":"code","6f8b716d":"code","ec577a38":"code","a19cd61f":"code","67806472":"code","d734fb2f":"code","d7777a71":"code","d9ca4eb3":"code","1838c97a":"code","5b32b48e":"code","47a5667f":"code","32c1df58":"code","c1a15f36":"code","10be7706":"code","1070a332":"code","53552b46":"code","a1bd43ee":"code","7d91cf91":"code","a66c392c":"code","7b7e8f1c":"code","fa4102ec":"code","9d3aa81b":"code","9c8ab448":"code","308f68a4":"code","75a58b0c":"code","4829a79e":"code","28808b66":"code","02ddb6a9":"code","47453829":"code","c800c2e9":"code","87e67a4d":"code","e4d97b90":"code","92c86a55":"code","bb9fd9f2":"code","e71fe26d":"code","be6ef5f5":"code","b31dfbfb":"code","f5b7eed0":"code","a2edb218":"code","9e16ce06":"code","e22b1efb":"code","a46595a1":"code","460e2c2f":"code","8f2c0989":"code","7a35fa0d":"code","7f8ffb63":"code","2b7d66db":"code","01abb8bc":"code","791bef2c":"code","a53ca646":"code","ff41dbb7":"code","2b26e926":"code","ada23965":"code","f824a24e":"code","7514fef6":"code","dc5cf1bb":"code","0c8f7fe1":"code","797ac087":"code","e9d6905c":"code","79b2821a":"code","0d8d8414":"code","96949005":"code","1f317bfc":"code","fd71ed38":"markdown","94228db5":"markdown","a3123775":"markdown","c8dd1965":"markdown","7bd28d48":"markdown","3faddd01":"markdown","4428d859":"markdown","8546bca3":"markdown","e6c01785":"markdown","c24b0e0f":"markdown","a3ac7719":"markdown","04f3b4a7":"markdown","d20b4b00":"markdown","1c9d1f5b":"markdown"},"source":{"edd526d5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import uniform\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3cdd485b":"from xgboost import XGBClassifier","1441b3bc":"%matplotlib inline","affa59b4":"nira = pd.read_csv('customers (1).csv')","c93a0a8c":"nira.shape","f408ba7e":"nira.head()","0c2bad30":"#Drop Uniqiue id and test columns\n# nira.drop(['RowNumber',\"CustomerId\"],axis=1, inplace=True)","b00b0333":"# Check columns type\nnira.info()","f7ff8fa9":"nira.columns.values","dc3df33a":"# Check unique column values\n\n# for i in nira.columns.values:\n#     print(i,'\\t',len(nira[i].unique()))\nnira.nunique()","611d7efc":"#Check for missing\/null values\nprint(nira.isnull().values.any())\nprint(nira.isna().sum())","5238a34c":"nira.describe()","8d8d5417":"print('tenure\\t\\t', nira['Tenure'].unique())\nprint('Geography\\t', nira['Geography'].unique())\nprint('NumOfProducts\\t', nira['NumOfProducts'].unique())","9a79a192":"nira['HasCrCard'].value_counts()","a481a0fd":"nira['Exited'].value_counts()","3e93b4c3":"nira[nira['HasCrCard']==1]['Exited'].value_counts()","23488967":"nira[nira['HasCrCard']==0]['Exited'].value_counts()","5264a508":"nira['Reason for exiting company'].value_counts()","03552958":"# Behavior of Exited customers for various categorical features\nfig, axarr = plt.subplots(2,2, figsize=(16,10))\n# plt.figure(figsize=(15,5))\nsns.countplot(x='Geography', hue = 'Exited',data = nira, ax=axarr[0][0])\nsns.countplot(x='Gender', hue = 'Exited',data = nira, ax=axarr[0][1])\nsns.countplot(x='HasCrCard', hue = 'Exited',data = nira, ax=axarr[1][0])\nsns.countplot(x='IsActiveMember', hue = 'Exited',data = nira, ax=axarr[1][1])\n","033634be":"# Relations based on the continuous data attributes\nfig, axarr = plt.subplots(3, 2, figsize=(20, 12))\nsns.boxplot(y='CreditScore',x = 'Exited', hue = 'Exited',data = nira, ax=axarr[0][0])\nsns.boxplot(y='Age',x = 'Exited', hue = 'Exited',data = nira , ax=axarr[0][1])\nsns.boxplot(y='Tenure',x = 'Exited', hue = 'Exited',data = nira, ax=axarr[1][0])\nsns.boxplot(y='Balance',x = 'Exited', hue = 'Exited',data = nira, ax=axarr[1][1])\nsns.boxplot(y='NumOfProducts',x = 'Exited', hue = 'Exited',data = nira, ax=axarr[2][0])\nsns.boxplot(y='EstimatedSalary',x = 'Exited', hue = 'Exited',data = nira, ax=axarr[2][1])","03f45f15":"sns.distplot(nira.EstimatedSalary,kde=False)","9aa31cdd":"sns.distplot(nira.Age,kde=False)","9f9034a1":"sns.distplot(nira.Balance,kde=False)","16ea7eb9":"sns.distplot(nira.Tenure,kde=False)","fc53281f":"len(nira[nira['Exited']==1]['Surname'].unique())","12c624e4":"# lb = LabelEncoder()\n# nira['Geography'] = lb.fit_transform(nira['Geography'])\n# Need not to encode reasons as its available only for exited customers and does not contribute towards churn.\n# nira['Reason for exiting company'] = lb.fit_transform(nira['Reason for exiting company'])\n# nira['Gender'] = lb.fit_transform(nira['Gender'])\n# nira['Surname'] = lb.fit_transform(nira['Surname'])","750d39b1":"# nira.head()","0e5b3a46":"# One hot encode the categorical variables\n# lst = ['Geography', 'Gender']\n# remove = list()\n\n# for i in lst:\n\n#     if (nira[i].dtype == np.str or nira[i].dtype == np.object):\n#         for j in nira[i].unique():\n#             nira[i+'_'+j] = np.where(nira[i] == j,1,-1)\n#         remove.append(i)\n# nira = nira.drop(remove, axis=1)\n# nira.head()","d7ae31e9":"# nira.columns","7f84a3be":"def prepare_data(df):\n    \n    #One hot encoding\n    lst = ['Geography', 'Gender']\n    remove = list()\n\n    for i in lst:\n\n        if (df[i].dtype == np.str or df[i].dtype == np.object):\n            for j in df[i].unique():\n                df[i+'_'+j] = np.where(df[i] == j,1,-1)\n            remove.append(i)\n    df = df.drop(remove, axis=1)\n    \n    # Create Features\n    df['BalToSalRatio'] = df['Balance']\/df['EstimatedSalary']\n    df['TenureByAge'] = df.Tenure\/(df.Age - 18)\n    df['CreditScoreGivenAge'] = df.CreditScore\/(df.Age - 18)\n    df.loc[df.HasCrCard == 0, 'HasCrCard'] = -1\n    df.loc[df.IsActiveMember == 0, 'IsActiveMember'] = -1\n    \n    # Arrange columns by data type for easier manipulation\n    continuous_vars = ['CreditScore',  'Age', 'Tenure', 'Balance','NumOfProducts', 'EstimatedSalary', 'BalToSalRatio',\n                       'TenureByAge','CreditScoreGivenAge']\n    cat_vars = ['HasCrCard', 'IsActiveMember', 'Geography_France', 'Geography_Spain',\n           'Geography_Germany', 'Gender_Female', 'Gender_Male']\n    df = df[['RowNumber', 'CustomerId','Exited'] + continuous_vars + cat_vars]\n    # minMax scaling the continuous variables\n    continousv = ['CreditScore',  'Age', 'Tenure', 'Balance','NumOfProducts', 'EstimatedSalary']\n    minVec = nira[continousv].min().copy()\n    maxVec = nira[continousv].max().copy()\n    df[continousv] = (df[continousv]-minVec)\/(maxVec-minVec)\n    df[df==np.inf]=np.nan\n    df.fillna(nira.mean(), inplace=True)\n    return df","47929d40":"#Create a new feature Balance to salary ratio.\n# nira['BalToSalRatio'] = nira['Balance']\/nira['EstimatedSalary']\n# nira['TenureByAge'] = nira.Tenure\/(nira.Age - 18)\n# nira['CreditScoreGivenAge'] = nira.CreditScore\/(nira.Age - 18)\n# nira.loc[nira.HasCrCard == 0, 'HasCrCard'] = -1\n# nira.loc[nira.IsActiveMember == 0, 'IsActiveMember'] = -1","79382596":"nira.columns","6af6c91e":"# Arrange columns by data type for easier manipulation\n# continuous_vars = ['CreditScore',  'Age', 'Tenure', 'Balance','NumOfProducts', 'EstimatedSalary', 'BalToSalRatio',\n#                    'TenureByAge','CreditScoreGivenAge']\n# cat_vars = ['HasCrCard', 'IsActiveMember', 'Geography_France', 'Geography_Spain',\n#        'Geography_Germany', 'Gender_Female', 'Gender_Male']\n# nira = nira[['RowNumber', 'CustomerId','Exited'] + continuous_vars + cat_vars]\n# nira.head()","47075dc2":"# minMax scaling the continuous variables\n# continousv = ['CreditScore',  'Age', 'Tenure', 'Balance','NumOfProducts', 'EstimatedSalary']\n# minVec = nira[continousv].min().copy()\n# maxVec = nira[continousv].max().copy()\n# nira[continousv] = (nira[continousv]-minVec)\/(maxVec-minVec)\n# nira.head()","6f8b716d":"# nira[nira==np.inf]=np.nan\n# nira.fillna(nira.mean(), inplace=True)","ec577a38":"nira_data = prepare_data(nira)","a19cd61f":"nira_data.columns","67806472":"features= [i for i in nira_data.columns if i not in ['RowNumber', 'CustomerId','Surname','Reason for exiting company','Exited']]\ntarget = 'Exited'","d734fb2f":"nira_data.head()","d7777a71":"#split test and train set\n# nira.fillna(0)\n# nira.round(2)\nX_train, X_test, Y_train, Y_test = train_test_split(nira_data[features],nira_data[target], test_size = 0.2, random_state = 0)","d9ca4eb3":"print(X_train.shape,Y_train.shape, X_test.shape, Y_test.shape)","1838c97a":"# col_mask=nira.isnull().any(axis=0)\n# col_mask\n# row_mask=nira.isnull().any(axis=1)\n# row_mask\n# nira.loc[row_mask,col_mask]","5b32b48e":"print(nira.isnull().values.any())\nprint(nira.isna().sum())\n# (np.where(np.isnan(nira)))","47a5667f":"# Function to give best model score and parameters\ndef best_model(model):\n    print(model.best_score_)    \n    print(model.best_params_)\n    print(model.best_estimator_)\ndef get_auc_scores(y_actual, method,method2):\n    auc_score = roc_auc_score(y_actual, method); \n    fpr_df, tpr_df, _ = roc_curve(y_actual, method2); \n    return (auc_score, fpr_df, tpr_df)","32c1df58":"# Fit primal logistic regression\nparam_grid = {'C': [0.1,0.5,1,10,50,100], 'max_iter': [250], 'fit_intercept':[True],'intercept_scaling':[1],\n              'penalty':['l2'], 'tol':[0.00001,0.0001,0.000001]}\nlog_primal_Grid = GridSearchCV(LogisticRegression(),param_grid, cv=10, refit=True, verbose=0)\nlog_primal_Grid.fit(X_train,Y_train)\nbest_model(log_primal_Grid)\n# lr = LogisticRegression()\n# lr.fit(X_train,Y_train)\n","c1a15f36":"# Fit logistic regression with degree 2 polynomial kernel\nparam_grid = {'C': [0.1,10,50], 'max_iter': [300,500], 'fit_intercept':[True],'intercept_scaling':[1],'penalty':['l2'],\n              'tol':[0.0001,0.000001]}\npoly2 = PolynomialFeatures(degree=2)\ndf_train_pol2 = poly2.fit_transform(X_train)\nlog_pol2_Grid = GridSearchCV(LogisticRegression(solver = 'liblinear'),param_grid, cv=5, refit=True, verbose=0)\nlog_pol2_Grid.fit(X_train,Y_train)\nbest_model(log_pol2_Grid)","10be7706":"# Fit SVM with RBF Kernel\nparam_grid = {'C': [0.5,100,150], 'gamma': [0.1,0.01,0.001],'probability':[True],'kernel': ['rbf']}\nSVM_grid = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=0)\nSVM_grid.fit(X_train,Y_train)\nbest_model(SVM_grid)","1070a332":"# # Fit SVM with pol kernel\n# param_grid = {'C': [1,10], 'gamma': [0.1,0.01],'probability':[True],'kernel': ['poly'],'degree':[2] }\n# SVM_grid = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=0)\n# SVM_grid.fit(X_train,Y_train)\n# best_model(SVM_grid)","53552b46":"# Fit random forest classifier\nparam_grid = {'max_depth': [3, 5, 6], 'max_features': [2,4,7,9],'n_estimators':[50,100],'min_samples_split': [3, 5, 6, 7]}\nRanFor_grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, refit=True, verbose=0)\nRanFor_grid.fit(X_train,Y_train)\nbest_model(RanFor_grid)","a1bd43ee":"# Fit Extreme Gradient boosting classifier\nparam_grid = {'max_depth': [5,6,7,8], 'gamma': [0.01,0.001,0.001],'min_child_weight':[1,5,10], 'learning_rate': [0.05,0.1, 0.2, 0.3], 'n_estimators':[5,10,20,100]}\nxgb_grid = GridSearchCV(XGBClassifier(), param_grid, cv=5, refit=True, verbose=0)\nxgb_grid.fit(X_train,Y_train)\nbest_model(xgb_grid)","7d91cf91":"xgb_grid.feature_importances_","a66c392c":"# Fit primal logistic regression\nlog_primal = LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=250, multi_class='warn',n_jobs=None, \n                                penalty='l2', random_state=None, solver='lbfgs',tol=1e-05, verbose=0, warm_start=False)\nlog_primal.fit(X_train,Y_train)\n","7b7e8f1c":"print(classification_report(Y_train, log_primal.predict(X_train)))","fa4102ec":"# Fit logistic regression with pol 2 kernel\npoly2 = PolynomialFeatures(degree=2)\ndf_train_pol2 = poly2.fit_transform(X_train)\nlog_pol2 = LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=300, multi_class='warn', n_jobs=None, \n                              penalty='l2', random_state=None, solver='liblinear',tol=0.0001, verbose=0, warm_start=False)\nlog_pol2.fit(df_train_pol2,Y_train)","9d3aa81b":"print(classification_report(Y_train,  log_pol2.predict(df_train_pol2)))","9c8ab448":"# Fit SVM with RBF Kernel\nSVM_RBF = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf', max_iter=-1, probability=True, \n              random_state=None, shrinking=True,tol=0.001, verbose=False)\nSVM_RBF.fit(X_train,Y_train)","308f68a4":"print(classification_report(Y_train,  SVM_RBF.predict(X_train)))","75a58b0c":"# # Fit SVM with Pol Kernel\n# SVM_POL = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,  decision_function_shape='ovr', degree=2, gamma=0.1, kernel='poly',  max_iter=-1,\n#               probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False)\n# SVM_POL.fit(X_train,Y_train)","4829a79e":"# print(classification_report(Y_train,  SVM_POL.predict(X_train)))","28808b66":"# Fit Random Forest classifier\nRF = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',max_depth=8, max_features=6, max_leaf_nodes=None,min_impurity_decrease=0.0,\n                            min_impurity_split=None,min_samples_leaf=1, min_samples_split=3,min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n                            oob_score=False, random_state=None, verbose=0,warm_start=False)\nRF.fit(X_train,Y_train)\n","02ddb6a9":"RF.classes_","47453829":"print(classification_report(Y_train,  RF.predict(X_train)))","c800c2e9":"predictions = pd.DataFrame(RF.predict_proba(nira[features]),columns = ['Prob_0', 'Prob_1'])","87e67a4d":"Resultant_nira = pd.concat([nira,predictions],axis=1)","e4d97b90":"Resultant_nira[['RowNumber','CustomerId','Exited','Prob_0','Prob_1']]","92c86a55":"top_churners =Resultant_nira.sort_values(['Prob_1'],ascending=0)","bb9fd9f2":"#top churners\ntop_churners[['CustomerId','Prob_1','Exited', 'Balance', 'Age', 'CreditScoreGivenAge', 'BalToSalRatio', 'NumOfProducts', 'TenureByAge','Prob_0']].head(4)\n# top_churners.columns","e71fe26d":"# Top retained customers\ntop_churners[['CustomerId','Prob_1','Exited','Balance', 'Age', 'CreditScoreGivenAge', 'BalToSalRatio', 'NumOfProducts', 'TenureByAge','Prob_0']].tail(4)","be6ef5f5":"From above top churners and top retained list. we can conclude that:\n    1. Senior citizens are more likely to churn.\n    2. Exited customers have higher BalToSalRatio,CreditScoreGivenAge,TenureByAge ratio.\n    3. NumOfProducts value is significantly higher for churners.","b31dfbfb":"top_churners.head(4)['CustomerId'].values","f5b7eed0":"\nResultant_nira[Resultant_nira['CustomerId'].isin([75572918,23109012])]\n# Resultant_nira[Resultant_nira['CustomerId']==23109012]","a2edb218":"print('min id',Resultant_nira['CustomerId'].min())\nprint('max id',Resultant_nira['CustomerId'].max())","9e16ce06":"Resultant_nira.columns","e22b1efb":"# Fit Extreme Gradient Boost Classifier\nXGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0.01, learning_rate=0.1, max_delta_step=0,max_depth=7,\n                    min_child_weight=5, missing=None, n_estimators=20,n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,reg_alpha=0, \n                    reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1)\nXGB.fit(X_train,Y_train)","a46595a1":"print(classification_report(Y_train,  XGB.predict(X_train)))","460e2c2f":"XGB.feature_importances_","8f2c0989":"from xgboost import plot_importance\nplt.bar(range(len(XGB.feature_importances_)), XGB.feature_importances_)\nplt.show()","7a35fa0d":"plot_importance(XGB)\nplt.show()","7f8ffb63":"y = Y_train\nX = X_train\nX_pol2 = df_train_pol2\nauc_log_primal, fpr_log_primal, tpr_log_primal = get_auc_scores(y, log_primal.predict(X),log_primal.predict_proba(X)[:,1])\nauc_log_pol2, fpr_log_pol2, tpr_log_pol2 = get_auc_scores(y, log_pol2.predict(X_pol2),log_pol2.predict_proba(X_pol2)[:,1])\nauc_SVM_RBF, fpr_SVM_RBF, tpr_SVM_RBF = get_auc_scores(y, SVM_RBF.predict(X),SVM_RBF.predict_proba(X)[:,1])\n# auc_SVM_POL, fpr_SVM_POL, tpr_SVM_POL = get_auc_scores(y, SVM_POL.predict(X),SVM_POL.predict_proba(X)[:,1])\nauc_RF, fpr_RF, tpr_RF = get_auc_scores(y, RF.predict(X),RF.predict_proba(X)[:,1])\nauc_XGB, fpr_XGB, tpr_XGB = get_auc_scores(y, XGB.predict(X),XGB.predict_proba(X)[:,1])","2b7d66db":"plt.figure(figsize = (12,6), linewidth= 1)\nplt.plot(fpr_log_primal, tpr_log_primal, label = 'log primal Score: ' + str(round(auc_log_primal, 5)))\nplt.plot(fpr_log_pol2, tpr_log_pol2, label = 'log pol2 score: ' + str(round(auc_log_pol2, 5)))\nplt.plot(fpr_SVM_RBF, tpr_SVM_RBF, label = 'SVM RBF Score: ' + str(round(auc_SVM_RBF, 5)))\n# plt.plot(fpr_SVM_POL, tpr_SVM_POL, label = 'SVM POL Score: ' + str(round(auc_SVM_POL, 5)))\nplt.plot(fpr_RF, tpr_RF, label = 'RF score: ' + str(round(auc_RF, 5)))\nplt.plot(fpr_XGB, tpr_XGB, label = 'XGB score: ' + str(round(auc_XGB, 5)))\nplt.plot([0,1], [0,1], 'k--', label = 'Random: 0.5')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC Curve')\nplt.legend(loc='best')\n#plt.savefig('roc_results_ratios.png')\nplt.show()","01abb8bc":"test_nira= pd.read_csv('test_nira.csv')","791bef2c":"test_nira.shape","a53ca646":"# test_nira.drop(['Exited','Reason for exiting company'],axis=1,inplace=True)","ff41dbb7":"test_data = prepare_data(test_nira)","2b26e926":"test_nira.drop(['Exited','Reason for exiting company'],axis=1,inplace=True)","ada23965":"test_data","f824a24e":"predict = pd.DataFrame(RF.predict(test_data[features]),columns = ['Exit_Prediction'])","7514fef6":"predict","dc5cf1bb":"preds = pd.DataFrame(RF.predict_proba(test_data[features]),columns = ['Prob_0', 'Prob_1'])","0c8f7fe1":"test_results = pd.concat([test_data,preds,predict],axis=1)","797ac087":"test_data_original = pd.concat([test_nira,preds,predict],axis=1)","e9d6905c":"# Sort the test data with probability to churn \ntest_results_sorted = test_results.sort_values(['Prob_1'],ascending=0)\ntest_results_sorted.to_csv('test_normalised_results_sorted.csv')\ntest_data_original= test_data_original.sort_values(['Prob_1'],ascending=0)\ntest_data_original.to_csv('test_data_results.csv')","79b2821a":"test_results_sorted","0d8d8414":"test_results_sorted.columns","96949005":"test_results_sorted[['CustomerId','Prob_1','Prob_0','Exited', 'Balance', 'Age', 'CreditScoreGivenAge', 'BalToSalRatio', 'NumOfProducts', 'TenureByAge']]","1f317bfc":"75572918,23109012","fd71ed38":"# Conclusions","94228db5":"## Top 4 customers about to churn are:\n15700801, 15647725, 15641175, 15672056","a3123775":"Unique columns values","c8dd1965":"# Build model to predict customer attrition","7bd28d48":"From above plots we can infer that:\n    1. Germany has higher churn ratio than Spain and France.\n    2. Females churners are more as compared to male.\n    3. Proportion of customers having credit cards is higher than non-credit card holders.\n    4. Inactive member have higher risk to churn.","3faddd01":"# Feature Engineering","4428d859":"1. Using Bagging Random forest classifier gives best results with best score of 88%.\n2. Balance, Age, CreditScoreGivenAge, BalToSalRatio, NumberOfProducts, TenureByAge are the top contributers to predict customer attrition.","8546bca3":"# Data Preprocessing","e6c01785":"### Customer Id 75572918,23109012 are not present in dataset","c24b0e0f":"# Top 4 Employee Who are Likely to Leave and their probability.","a3ac7719":"# Feature Importances","04f3b4a7":"Since we don't have prior history or bank statements. so we should create another feature with balance given.","d20b4b00":"For the model fitting, I will try out the following\n\n    Logistic regression in the primal space and with different kernels\n    SVM in the primal and with different Kernels\n    Ensemble models","1c9d1f5b":"Inferences:\n    1. There is no significant contribution of CreditScore,NumOfProducts and EstimatedSalary in retained and churned customers.\n    2. Aged customers show more tendency to churn.\n    3. Average tenure customers as safe players. low and hight tenure products are more likely to churn.\n    4. The bank is losing customers with significant bank balances which is likely to hit their available capital for lending."}}