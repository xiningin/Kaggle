{"cell_type":{"30fe86bc":"code","d742ba16":"code","36cb226f":"code","56676986":"code","fac4b1b9":"code","845628fb":"code","e640375b":"code","8a703693":"code","64f12799":"code","b25c7422":"code","a7c3fe4e":"code","da828126":"code","3f70e138":"code","ecea0d69":"code","7f638f99":"code","6dfd37b1":"code","5bca8900":"code","6ff1ed7f":"code","dc5cf3dd":"code","f89de0b9":"code","f79c766a":"code","8ab8f355":"code","370937af":"code","3cd6cc4d":"code","d646b46a":"code","316065c4":"code","1c1ca48e":"code","49ff62ab":"code","b80c62d0":"code","8c078617":"code","45ca6502":"code","1986c8fe":"code","f5de3105":"code","8196d156":"code","ec095fbb":"code","abf3024a":"code","7855cef5":"code","eeeccc91":"code","0da3d189":"code","9e543b22":"code","7dee8859":"code","b2067d4d":"code","d6adc3a7":"code","81829177":"code","cb9553d9":"code","533b371a":"code","f1a24907":"code","7fe72a4d":"code","20874579":"code","cd101c5e":"code","5eaa5283":"code","2892138e":"code","9826f916":"code","40c11ed1":"markdown","6f20581e":"markdown","855a431b":"markdown","20793fc0":"markdown","234e7563":"markdown","91b9b8bd":"markdown","6e2d21a3":"markdown","80bb274e":"markdown","84c01194":"markdown","b36e15fd":"markdown","138affab":"markdown","7b6b6edb":"markdown","0e2ff249":"markdown","99c999ad":"markdown","1ccb62cf":"markdown","5628d9a2":"markdown","46f4e246":"markdown","e0d6e7fa":"markdown","02a1763f":"markdown","5d889487":"markdown","47d7a9df":"markdown","45f7882c":"markdown","33364357":"markdown","f6687e8b":"markdown","d11c1be8":"markdown","f0c38234":"markdown"},"source":{"30fe86bc":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport gc\nimport scipy\nfrom wordcloud import WordCloud\n\n## Text Cleaning\nimport string\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\n\nfrom collections import Counter, OrderedDict\n## Text Embedding\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\n\n\n# dimensionality reduction\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\n## Modelling\nfrom sklearn.linear_model import Ridge\n\nplt.style.use('ggplot')\n%matplotlib inline","d742ba16":"PATH = \"..\/input\/\"","36cb226f":"df_train = pd.read_csv(f'{PATH}train.tsv', sep='\\t')\ndf_test = pd.read_csv(f'{PATH}test_stg2.tsv', sep='\\t')","56676986":"print(f\"Train consists of {df_train.shape[0]:,} observations\\nTest consists of {df_test.shape[0]:,} observations\")","fac4b1b9":"df_train['price'].describe()","845628fb":"df_train['price'].isnull().sum()","e640375b":"print(\"There Exist {} items sold for Free!\".format(df_train[df_train['price'] == 0].shape[0]))","8a703693":"fig, ax = plt.subplots(2,figsize=(11,7))\n\nsns.distplot( df_train[df_train['price'] <  df_train['price'].quantile(0.99)]['price'], ax=ax[0], \n             label='Price', color=\"green\")\n\nsns.distplot( np.log1p(df_train[df_train['price'] <  df_train['price'].quantile(0.99)]['price']), ax=ax[1],\n             label=\"log Price\", color=\"blue\")\n\nax[0].legend(loc=0)\nax[1].legend(loc=0)\nax[0].set_title(\"\")\nax[1].set_title(\"\")\nax[0].axis(\"off\")\nax[1].axis(\"off\")\nplt.show()","64f12799":"nrow_train = df_train.shape[0]\ny_train = np.log1p(df_train['price'])","b25c7422":"null_df = pd.DataFrame(df_train.dtypes).T.rename(index={0: 'dtype'})\nnull_df = null_df.append(pd.DataFrame(df_train.isnull().sum()).T.rename(index={0: 'count'}))\nnull_df = null_df.append(pd.DataFrame(df_train.isnull().sum() \/df_train.shape[0] * 100 ).T.rename(index={0: '%'}))\nnull_df","a7c3fe4e":"df_train['brand_name'] = df_train['brand_name'].fillna(\"Other\")\ndf_train['item_description'] = df_train['item_description'].fillna(\"No description yet\")\ndf_train['item_description'] = df_train['item_description'].astype(str)\ndf_train['shipping'] = df_train['shipping'].astype('category')\ndf_train['item_condition_id'] = df_train['item_condition_id'].astype('category')\n\ndf_test['brand_name'] = df_test['brand_name'].fillna(\"Other\")\ndf_test['item_description'] = df_test['item_description'].fillna(\"No description yet\")\ndf_test['item_description'] = df_test['item_description'].astype(str)\ndf_test['shipping'] = df_test['shipping'].astype('category')\ndf_test['item_condition_id'] = df_test['item_condition_id'].astype('category')\n\ndf_train['logprice'] = y_train\n\n","da828126":"df_train.duplicated().sum()","3f70e138":"plt.figure(figsize=(11,7))\n\nsns.kdeplot(df_train[df_train['shipping'] == 1].loc[:, 'logprice'], shade=True, color=\"g\", bw=.09, label=\"Seller\")\n\nsns.kdeplot(df_train[df_train['shipping'] == 0].loc[:, 'logprice'], shade=True, color=\"r\", bw=.09, label=\"Buyer\")\n\nplt.show()","ecea0d69":"plt.figure(figsize=(15,7))\nax = sns.boxplot(x='shipping', y='logprice', data=df_train)\nax.set_xticklabels([\"buyer\", \"seller\"])\nplt.ylabel(\"Log (price)\")\nplt.xlabel(\"Item Condition\")\n\nplt.title(\"Shipping and price interaction\")\nplt.show()","7f638f99":"brand_cat = df_train.groupby(by='brand_name').agg({'price':np.median})\nbrand_cat = pd.DataFrame(brand_cat).rename(columns={1:'price'}).reset_index()\nbrand_cat = brand_cat.sort_values('price', ascending=False)","6dfd37b1":"plt.figure(figsize=(15,7))\nax = sns.scatterplot(x='price', y='brand_name', data=brand_cat[:25], color='r')\nax.invert_yaxis()\nplt.title(\"Most 25 Expensive Brands\")\nplt.show()","5bca8900":"len(brand_cat)","6ff1ed7f":"d = dict()\nfor brand, pr in zip(brand_cat.brand_name[:200], brand_cat.price[:200]):\n    d[brand] = pr","dc5cf3dd":"cloud = WordCloud(width=1440, height=1080, background_color='lightgrey', colormap='cividis', random_state=42).generate_from_frequencies(frequencies=d)\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","f89de0b9":"conditions = df_train['item_condition_id'].value_counts()","f79c766a":"plt.figure(figsize=(15,7))\nax = sns.countplot(y = df_train['item_condition_id'])\nax.set_xticklabels([int(x\/1000) for x in ax.get_xticks()])\nplt.ylabel(\"Item Condition\")\nplt.xlabel(\"Freq (000s)\")\nplt.title(\"Frequency of Each item Condition\")\nplt.show()","8ab8f355":"plt.figure(figsize=(15,7))\nsns.boxplot(x='item_condition_id', y='logprice', data=df_train)\nplt.ylabel(\"log(price)\")\nplt.xlabel(\"Item Condition\")\nplt.title(\"Item Condition Iteraction with price\")\nplt.show()","370937af":"df_train.category_name.head()","3cd6cc4d":"def split_categ(c):\n        try:\n            c1, c2, c3 = c.split(\"\/\")\n            return c1, c2, c3\n        except:\n            return (\"No label\",\"No label\",\"No label\")","d646b46a":"df_train['cat1'], df_train['cat2'], df_train['cat3']= zip(*df_train.category_name.apply(split_categ))\ndf_test['cat1'], df_test['cat2'], df_test['cat3']= zip(*df_test.category_name.apply(split_categ))","316065c4":"plt.figure(figsize=(15,7))\nax = sns.countplot(df_train.cat1)\nax.set_yticklabels([int(y\/1000) for y in ax.get_yticks()])\nplt.ylabel(\"Freq (000s)\")\nplt.xlabel(\"Main Category classes\")\nplt.title(\"Most Category popular\")\nplt.show()","1c1ca48e":"plt.figure(figsize=(15,7))\nsns.boxplot(x='cat1', y='logprice', data=df_train)\nplt.ylabel(\"log(price)\")\nplt.xlabel(\"Item Category\")\nplt.title(\"Item Category Iteraction with price\")\nplt.show()","49ff62ab":"cat2S = df_train.groupby('cat2')['cat2'].count()\ncat2S = pd.DataFrame(cat2S).rename(columns={'cat2': 'freq'}).reset_index()\ncat2S = cat2S.sort_values(by='freq', ascending=False)","b80c62d0":"len(cat2S)","8c078617":"plt.figure(figsize=(15,7))\nax = sns.scatterplot(y='cat2', x='freq', data=cat2S.head(25))\nax.invert_yaxis()\nax.set_xticklabels([int(x\/1000) for x in ax.get_xticks()])\nplt.xlabel(\"Freq (000s)\")\nplt.ylabel(\"Sub Category classes\")\nplt.title(\"Most 25 Sub Category popular\")\nplt.show()","45ca6502":"cat3S = df_train.groupby('cat3')['cat3'].count()\ncat3S = pd.DataFrame(cat3S).rename(columns={'cat3': 'freq'}).reset_index()\ncat3S = cat3S.sort_values(by='freq', ascending=False)","1986c8fe":"plt.figure(figsize=(15,7))\nax = sns.scatterplot(y='cat3', x='freq', data=cat3S.head(25))\nax.invert_yaxis()\nax.set_xticklabels([int(x\/1000) for x in ax.get_xticks()])\nplt.xlabel(\"Freq (000s)\")\nplt.ylabel(\"Sub Category classes\")\nplt.title(\"Most 25 Sub-Sub Category popular\")\nplt.show()","f5de3105":"cloud = WordCloud(width=1440, height=1080, background_color='lightgrey', colormap='cividis', random_state=4).generate(\" \".join(df_train['cat3'].astype(str)[:200]))\n\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","8196d156":"vectorizer = TfidfVectorizer(max_df=0.9, max_features=1_000, stop_words = \"english\")","ec095fbb":"X_desc_train = vectorizer.fit_transform(df_train['item_description'])\nX_desc_test  = vectorizer.transform(df_test['item_description'])","abf3024a":"print(\"We have {} keywords from our Description Corpus\".format(len(vectorizer.get_feature_names())))","7855cef5":"print(\"Tf-IDF Matrix shape {}, the Original matrix shape {:,}\".format(X_desc_train.get_shape(), df_train.shape[0]))","eeeccc91":"NUM_TOP_BRANDS = 2500\ntop_brands = df_train['brand_name'].value_counts().index[:NUM_TOP_BRANDS]\ndf_train.loc[~df_train['brand_name'].isin(top_brands), 'brand_name'] = \"Other\"\n\nbrand_lv = LabelBinarizer(sparse_output=True)\nX_brand_train  = brand_lv.fit_transform(df_train['brand_name'])\nX_brand_test   = brand_lv.transform(df_test['brand_name'])","0da3d189":"X_brand_train.get_shape() , X_brand_test.get_shape() ","9e543b22":"MIN_NAME_DF = 10\nname_cv = CountVectorizer(min_df=MIN_NAME_DF)\nX_name_train  = name_cv.fit_transform(df_train['name']) \nX_name_test   = name_cv.transform(df_test['name']) ","7dee8859":"X_name_train.shape, X_name_test.shape","b2067d4d":"cat1_cv = CountVectorizer()\nX_cat1_train  = cat1_cv.fit_transform(df_train['cat1'])\nX_cat1_test   = cat1_cv.transform(df_test['cat1'])","d6adc3a7":"X_cat1_train.shape, X_cat1_test.shape","81829177":"cat2_cv = CountVectorizer()\nX_cat2_train  = cat2_cv.fit_transform(df_train['cat2'])\nX_cat2_test   = cat2_cv.transform(df_test['cat2'])","cb9553d9":"X_cat2_train.shape, X_cat2_test.shape","533b371a":"X_dummies_train = scipy.sparse.csr_matrix( pd.get_dummies(df_train[['shipping', 'item_condition_id']], sparse=True).values)\nX_dummies_test  = scipy.sparse.csr_matrix( pd.get_dummies(df_test[['shipping', 'item_condition_id']], sparse=True).values)","f1a24907":"X_dummies_train.shape, X_dummies_test.shape","7fe72a4d":"X_train = scipy.sparse.hstack((\n                        X_dummies_train,\n                        X_desc_train,\n                        X_name_train,\n                        X_brand_train,\n                        X_cat1_train,\n                        X_cat2_train\n                        )).tocsr()\n\nX_test = scipy.sparse.hstack((\n                        X_dummies_test,\n                        X_desc_test,\n                        X_name_test,\n                        X_brand_test,\n                        X_cat1_test,\n                        X_cat2_test\n                        )).tocsr()","20874579":"X_train.shape, X_test.shape","cd101c5e":"model = Ridge(solver='lsqr', fit_intercept=False)\nmodel.fit(X_train, y_train)","5eaa5283":"preds = model.predict(X_test)","2892138e":"df_test['price'] = np.expm1(preds)","9826f916":"df_test[['test_id', 'price']].to_csv('ridge_submission_v2.csv', index=False)","40c11ed1":"### Tf-IDF","6f20581e":"Determine whether seller payed the shipping fees, or the buyer who payed","855a431b":"Well, Items are more likely to be in **first (best) condition**, also **3rd Conditioned** are **more popular** than **2nd ones**","20793fc0":"We got no duplicates in our dataset","234e7563":"Apparently, Category is splitted into 3 sub-categories, so let's split them","91b9b8bd":"Well, Price is **Right skewed** as been expected, which induce  **Log transformation**<br>\n(we add 1 as some `price` values equal to `zero`)","6e2d21a3":"So for each docuemnt of the **2M** we got a vector of around **1000** dimension describing how important its keywords, and how these keywords are indicative to such document","80bb274e":"That's a little bit wierd... Worst Condtioned items got the highest value ....<br> After all, Price didn't vary much across Item Condition.","84c01194":"Well,\n`brand name` got about **43%** missing values, but it is not a big deal,<br> as it's not as important as the `item description` which fortunately got about **0%** null values <br><br>\nOther null values is from the **concatenation** process, which is done to maintain the **data consistency** ","b36e15fd":"And now men products are the most expensive ... maybe that's a one reason where men not interested in such place<br>\nAnyway, Such **Feature** still **doesn't** give us the value we need for building the model, <br>**still no correlation with price**","138affab":"### Item Category","7b6b6edb":"**Women** are the most ones making shopping after all,","0e2ff249":"**Data Description**\n****\n- **price** : (**Target Variable**) : float variable, should be predicted.\n- **brand** :  name : String Value, brand of each product.\n- **category** :  name: String Value, Consists of **3 parts** Main Category and 2 other sub-c**ategories**(will be examined further).\n- **item** :  condition id : int value, where id = 1 means product is in best Condition, greater value low condition.\n- **item** :  description : String value, the description of product (the bulk in our problem).\n- **name** :  : String value, the Name of the product.\n- **shipping** :  : boolean value(dummy variable), `shipping = 1` means seller pay the shipping fees `shipping = 0` buyer who pay such fees.","99c999ad":"Now let's see how **`price` interaction** goes with **item condition**","1ccb62cf":"###  Modeling","5628d9a2":"### Brand","46f4e246":"Also Women products are on the top of The second Sub category products","e0d6e7fa":"#### Main Category and price interaction","02a1763f":"So apparently items which `buyer` pay their **shipping fees**, are **more Expensive**","5d889487":"### Shipping","47d7a9df":"# Target Variable\n\n### Price","45f7882c":"### item condition","33364357":"So **Demdaco** is the most Expensive Brand with over **400 \u00a3** which could be treated as **Outlier** according to other brands. <br> May **`brand`** considered as an important feature to our model.","f6687e8b":"items are ordered Feature, ranges from [1,5], where <br>**1 value** means item is in its **Best condition** and **5 value** is **Worst**","d11c1be8":"**Notes**\n- some prices include zero values about (0.05%)\n- mean price is around **26 \u00a3** while max  price is around **2009 \u00a3** ( which means a **skewness** in our variable )\n- No null vaues (good)","f0c38234":"### Data Cleaning"}}