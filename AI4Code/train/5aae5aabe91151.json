{"cell_type":{"35615150":"code","441e8fc0":"code","ab1154bf":"code","246e33bc":"code","34fe05b1":"code","6ae10911":"code","3e932518":"code","dc0f0376":"code","6caa0819":"code","dda3c632":"code","113c1a1a":"code","92c5b87b":"code","9004c7a3":"code","6c2fedfe":"code","f8a1615e":"code","c389d624":"code","e374e877":"code","a2b74552":"code","217f102a":"code","859a6c4c":"code","c84c3231":"code","6b45d157":"markdown","f1b06503":"markdown","4680a574":"markdown","6b2ae4a1":"markdown","a1c47bc7":"markdown","434e7559":"markdown"},"source":{"35615150":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#Credit: https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705\/notebook\n\n#Goal of this notebook: How to tokenize the data, create question answer targets, and how to build a custom question answer head for RoBERTa\n# in TensorFlow. Note that HuggingFace transformers don't have a TFRobertaForQuestionAnswering so we must make our own from TFRobertaModel.\n\n# Here's a pro tip for people using TPU. Start each fold loop with-\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n# This will prevent the TPU from running out of memory during 5 Fold.\n\n#v5: got .706 score with max_len 192","441e8fc0":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","ab1154bf":"# quick check of the datasets\ntrain_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntrain_df.head()","246e33bc":"train_df.info()","34fe05b1":"# target values\ntrain_df['sentiment'].unique()","6ae10911":"train_df.sentiment.value_counts()","3e932518":"# load datasets\ndef read_train():\n    train=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\n    train['text'] = train['text'].astype(str) #ensuring data type is string to avoid any error\n    train['selected_text'] = train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\n    test['text'] = test['text'].astype(str)\n    return test\n\ndef read_submission():\n    sub = pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n    return sub","dc0f0376":"# load datasets\ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","6caa0819":"MAX_LEN = 96 #try max_len=192 for longer training otherwise use 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\n# tokenizer.encode('positive').ids\n# tokenizer.encode('negative').ids\n# tokenizer.encode('neutral').ids\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974} #encoded values of  a particular sentiment","dda3c632":"# required step to transform data into RoBERTa format\nct = train_df.shape[0]\n# 1 for tokens and 0 for padding \ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')","113c1a1a":"import time","92c5b87b":"%%time\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","9004c7a3":"# tokenize the test data also as we did above for train data\nct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","6c2fedfe":"def scheduler(epoch):\n    return 3e-5 * 0.2**epoch","f8a1615e":"# build a RoBERTa model\ndef build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model","c389d624":"# define the metric\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","e374e877":"# %%time\n# n_splits = 5\n# jac = []; VER='v5'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n# oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n# oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n\n# skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n# for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n\n#     print('#'*25)\n#     print('### FOLD %i'%(fold+1))\n#     print('#'*25)\n    \n#     K.clear_session()\n#     model = build_model()\n        \n#     reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\n#     sv = tf.keras.callbacks.ModelCheckpoint(\n#         '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n#         save_weights_only=True, mode='auto', save_freq='epoch')\n        \n#     hist = model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n#         epochs=5, batch_size=8, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n#         validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n#         [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n#     print('Loading model...')\n#     model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    \n#     print('Predicting OOF...')\n#     oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n#     # DISPLAY FOLD JACCARD\n#     all = []\n#     for k in idxV:\n#         a = np.argmax(oof_start[k,])\n#         b = np.argmax(oof_end[k,])\n#         if a>b: \n#             st = train_df.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n#         else:\n#             text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n#             enc = tokenizer.encode(text1)\n#             st = tokenizer.decode(enc.ids[a-1:b])\n#         all.append(jaccard(st,train_df.loc[k,'selected_text']))\n#     jac.append(np.mean(all))\n#     print('#### FOLD %i Jaccard score='%(fold+1),np.mean(all))\n#     print()","a2b74552":"# print('#### OVERALL 5Fold CV Jaccard score=',np.mean(jac))","217f102a":"%%time\nn_splits = 5\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(5):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    model.load_weights('\/kaggle\/input\/model4\/v4-roberta-%i.h5'%i)\n#     model.load_weights('\/kaggle\/input\/roberta-trained-model-by-prateekg\/v5-roberta-%i.h5'%i)\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/n_splits\n    preds_end += preds[1]\/n_splits","859a6c4c":"# make submission file\nall = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_df.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","c84c3231":"test_df['selected_text'] = all\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)","6b45d157":"## What words in tweets support a positive, negative, or neutral sentiment? \n## How can we help make that determination using machine learning tools?\n## Objective in this competition is to construct a model that can do the same!\n## We're attempting to predict the word or phrase from the tweet that exemplifies the provided sentiment.\n## The word or phrase should include all characters within that span (i.e. including commas, spaces, etc.) and submission file headers are: textID,selected_text\n## For example: 2,\"very good\"\n## The metric in this competition is the word-level Jaccard score","f1b06503":"Use inference in below cell if yo don't want to run above cell.\nOtherwise comment\/neglect below cell.\nHere I am using my trained models with max len 192 got from above cell.","4680a574":"Uncomment below cell if you want to train the model.\nHere we train with 5 Stratified KFolds (based on sentiment stratification). \nEach fold, the best model weights are saved and then reloaded before oof prediction and test prediction. \nTherefore you can run this code offline and upload your 5 fold models to a private Kaggle dataset. \nThen run this notebook and comment out the line model.fit(). \nInstead your notebook will load your model weights from offline training in the line model.load_weights(). \nUpdate this to have the correct path. Also make sure you change the KFold seed below to match your offline training. \nThen this notebook will proceed to use your offline models to predict oof and predict test.","6b2ae4a1":"Above code we combine those two column values (text & sentiment) together using [2,2], instead [2, 0] because-\nin HuggingFace tokenizer, RoBERTa tokenization accepts the output like: [0] + ? + [2,2] + ? + [2]","a1c47bc7":"Learn more about HuggingFace tokenizer-\nhttps:\/\/github.com\/huggingface\/tokenizers\n\nExample-\n\nfrom transformers import *\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\ntokenizer.encode_plus('sentence one','sentence two')['token_type_ids']\n\ntokenizer.save_vocabulary('.')","434e7559":"We use a pretrained roBERTa base model and add a custom question answer head. First tokens are input into bert_model and we use BERT's first output, i.e. x[0] below. These are embeddings of all input tokens and have shape (batch_size, MAX_LEN, 768). Next we apply tf.keras.layers.Conv1D(filters=1, kernel_size=1) and transform the embeddings into shape (batch_size, MAX_LEN, 1). We then flatten this and apply softmax, so our final output from x1 has shape (batch_size, MAX_LEN). These are one hot encodings of the start tokens indicies (for selected_text). And x2 are the end tokens indicies."}}