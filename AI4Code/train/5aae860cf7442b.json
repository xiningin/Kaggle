{"cell_type":{"7efa7679":"code","35322ea0":"code","d7ff39a2":"code","39f2d4d8":"code","a453e351":"code","fe62305b":"code","d9c66bc8":"code","1b3813da":"code","a7779c13":"code","f791f081":"code","17e491e1":"code","6a1ba07e":"code","17b68a33":"code","546b5062":"code","0f08a213":"code","25b68a73":"code","69727d69":"code","546b90ff":"code","bc593fb2":"code","2eb89458":"code","f5b9245a":"code","9a89fdcf":"code","d05494e8":"code","e9ea97cf":"code","c2ef0751":"code","8bbeff05":"code","96dc0c18":"code","4e695916":"code","ee47f521":"code","f94ea06b":"code","d0551446":"code","77c5ef45":"code","f84805e4":"code","f21c0525":"code","f3391fb4":"code","bc9af348":"markdown","6c7bb1fc":"markdown","33f44ed6":"markdown","5f4d611e":"markdown","b15c6af5":"markdown","683461e3":"markdown","c3ecdc4f":"markdown","41fcd3f7":"markdown","f297bc54":"markdown","206cba52":"markdown","fb2ad748":"markdown","aa67c7e5":"markdown","127a3fbd":"markdown","10dfb66b":"markdown","b19695c2":"markdown","b71dcc6a":"markdown","c4b24e3e":"markdown","43d8a10d":"markdown","5a44188b":"markdown","f83a643e":"markdown","141866e6":"markdown","085c1b2f":"markdown","ee00611c":"markdown","74073389":"markdown","d1ce3a8d":"markdown","7448c244":"markdown","3b0de1c6":"markdown","5774e211":"markdown","04e47345":"markdown","a1fc9c82":"markdown","54f7fded":"markdown","ce4ec8a8":"markdown","614b016b":"markdown","43d7dfbe":"markdown","2f8c056c":"markdown"},"source":{"7efa7679":"!pip install pytrends\n!pip install Cryptory","35322ea0":"\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nimport warnings\nfrom scipy import stats\n\n\nfrom cryptory import Cryptory\n\nimport os\nfrom itertools import product\nfrom datetime import datetime\n\nfrom plotly import tools\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', 7)\nall_coins_names = ['btc', 'eth', 'xrp', 'bch', 'ltc', 'dash', 'xmr', 'etc', 'zec', \n                        'doge', 'rdd', 'vtc', 'ftc', 'nmc', 'blk','nvc']","d7ff39a2":"help(Cryptory)","39f2d4d8":"\n# initialise object \n# pull data from start of 2011 to present day\nmy_cryptory = Cryptory(from_date = \"2011-02-01\", to_date = \"2019-05-11\" )\n\n# get historical bitcoin prices from bitinfocharts\nbtc_price_data=my_cryptory.extract_bitinfocharts(\"btc\")                                                 \nbtc_price_data = btc_price_data.set_index('date')\nbtc_price_data\n","a453e351":"# get historical bitcoin data from coinmarketcap\nbtc_marketcap = my_cryptory.extract_coinmarketcap(\"bitcoin\")\nbtc_marketcap","fe62305b":"plt.figure(figsize=(20,7))\nax = plt.subplot(111)    \nplt.yticks(fontsize=14)    \nplt.xticks(fontsize=14)\nax.spines[\"top\"].set_visible(False)        \nax.spines[\"right\"].set_visible(False)    \nplt.grid(linewidth=0.2)\nplt.title('Bitcoin Market Capitalization (2013-2018)', fontsize=18)\nplt.plot(btc_marketcap.date,btc_marketcap.marketcap)\nplt.show()","d9c66bc8":"plt.figure(figsize=(20,7))\nax = plt.subplot(111)    \nplt.yticks(fontsize=14)    \nplt.xticks(fontsize=14)\nax.spines[\"top\"].set_visible(False)        \nax.spines[\"right\"].set_visible(False)\nbtc_price_data.btc_price.plot()\nplt.grid(linewidth=0.2)\nplt.title('Historical Bitcoin Prices (2013-2018)', fontsize=18)\nplt.show()\n\n","1b3813da":"plt.figure(figsize=(20,7))\nax = plt.subplot(111)    \nplt.yticks(fontsize=14)    \nplt.xticks(fontsize=14)\nax.spines[\"top\"].set_visible(False)        \nax.spines[\"right\"].set_visible(False)    \nplt.grid(linewidth=0.2)\nplt.title('Bitcoin Market Volume (2013-2018)', fontsize=18)\nplt.plot(btc_marketcap.date,btc_marketcap.volume)\nplt.show()\n","a7779c13":"# Resampling to monthly frequency\ndf_month = btc_price_data.resample('M').mean()\n\n# Resampling to annual frequency\ndf_year = btc_price_data.resample('A-DEC').mean()\n\n# Resampling to quarterly frequency\ndf_Q = btc_price_data.resample('Q-DEC').mean()\n","f791f081":"fig = plt.figure(figsize=[20, 8])\nplt.suptitle('Bitcoin exchanges, mean USD', fontsize=22)\n\nplt.subplot(221)\nplt.plot(btc_price_data.btc_price, '-', label='By Days')\nplt.legend()\n\nplt.subplot(222)\nplt.plot(df_month.btc_price, '-', label='By Months')\nplt.legend()\n\nplt.subplot(223)\nplt.plot(df_Q.btc_price, '-', label='By Quarters')\nplt.legend()\n\nplt.subplot(224)\nplt.plot(df_year.btc_price, '-', label='By Years')\nplt.legend()\n\n# plt.tight_layout()\nplt.show()","17e491e1":"plt.figure(figsize=[20,8])\nsm.tsa.seasonal_decompose(df_month.btc_price).plot()\nplt.show()","6a1ba07e":"def ADF(df):\n    result = adfuller(df)\n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))","17b68a33":"ADF(df_month.btc_price)","546b5062":"df_month['Price_Box'], lmbda = stats.boxcox(df_month.btc_price)\nADF(df_month.Price_Box)","0f08a213":"df_month['Price_Box_Diff_12'] = df_month.Price_Box - df_month.Price_Box.shift(12)\nADF(df_month.Price_Box_Diff_12[12:])","25b68a73":"df_month['Price_Box_Diff_13'] = df_month.Price_Box_Diff_12 - df_month.Price_Box_Diff_12.shift(1)\nADF(df_month.Price_Box_Diff_13[13:])","69727d69":"plt.figure(figsize=[20,8])\nsm.tsa.seasonal_decompose(df_month.Price_Box_Diff_13[13:]).plot()\nplt.show()","546b90ff":"# Initial approximation of parameters using Autocorrelation and Partial Autocorrelation Plots\nplt.figure(figsize=(15,7))\nax = plt.subplot(211)\nsm.graphics.tsa.plot_acf(df_month.Price_Box_Diff_13[13:].values.squeeze(), lags=40, ax=ax)\nax = plt.subplot(212)\nsm.graphics.tsa.plot_pacf(df_month.Price_Box_Diff_13[13:].values.squeeze(), lags=40, ax=ax)\nplt.tight_layout()\nplt.show()","bc593fb2":"# Initial approximation of parameters\nQs = range(0, 2)\nqs = range(0, 3)\nPs = range(0, 3)\nps = range(0, 3)\nD=1\nd=1\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)\n\n# Model Selection\nresults = []\nbest_aic = float(\"inf\")\nwarnings.filterwarnings('ignore')\nfor p in parameters_list:\n    try:\n        model=sm.tsa.statespace.SARIMAX(df_month.Price_Box, order=(p[0], d, p[1]), \n                                        seasonal_order=(p[2], D, p[3], 12)).fit(disp=-1)\n    except ValueError:\n        print('wrong parameters:', p)\n        continue\n    aic = model.aic\n    if aic < best_aic:\n        best_model = model\n        best_aic = aic\n        best_param = p\n    results.append([p, model.aic])","2eb89458":"# Best Models\nresult_table = pd.DataFrame(results)\nresult_table.columns = ['parameters', 'aic']\nprint(result_table.sort_values(by = 'aic', ascending=True).head())\nprint(best_model.summary())","f5b9245a":"# STL-decomposition\nplt.figure(figsize=(15,7))\nplt.subplot(211)\nbest_model.resid[13:].plot()\nplt.ylabel(u'Residuals')\nax = plt.subplot(212)\nsm.graphics.tsa.plot_acf(best_model.resid[13:].values.squeeze(), lags=48, ax=ax)\n\nADF(best_model.resid[13:])\n\nplt.tight_layout()\nplt.show()","9a89fdcf":"# Inverse Box-Cox Transformation\ndef invboxcox(y,lmbda):\n   if lmbda == 0:\n      return(np.exp(y))\n   else:\n      return(np.exp(np.log(lmbda*y+1)\/lmbda))","d05494e8":"# Prediction\ndf_month2 = df_month[['btc_price']]\ndate_list = [datetime(2019, 6, 30), datetime(2019, 7, 31), datetime(2019, 8, 31), datetime(2019, 9, 30), \n             datetime(2019, 10, 31), datetime(2019, 11, 30), datetime(2019, 12, 31), datetime(2020, 1, 31),\n             datetime(2020, 1, 28)]\nfuture = pd.DataFrame(index=date_list, columns= df_month.columns)\ndf_month2 = pd.concat([df_month2, future])\ndf_month2['forecast'] = invboxcox(best_model.predict(start=0, end=160), lmbda)\nplt.figure(figsize=(15,7))\ndf_month2.btc_price.plot()\ndf_month2.forecast.plot(color='r', ls='--', label='Predicted Weighted_Price')\nplt.legend()\nplt.title('Bitcoin exchanges, by months')\nplt.ylabel('mean USD')\nplt.show()","e9ea97cf":"def get_coins_price(coins,start_date, end_date):\n    cryptory = Cryptory(from_date = start_date, to_date = end_date)\n    all_coins_df = cryptory.extract_bitinfocharts(coins[0])\n    for coin in coins[1:]:\n        all_coins_df = all_coins_df.merge(cryptory.extract_bitinfocharts(coin), on=\"date\", how=\"left\")\n    return all_coins_df\n\n\ndef draw_pearson_correlation(start_date, end_date):\n    all_coins_df = get_coins_price(all_coins_names,start_date, end_date)\n    corr = all_coins_df.iloc[:,1:].pct_change().corr(method='pearson')\n    corr = corr.dropna(axis=0, how='all').dropna(axis=1, how='all').round(2)\n    sns.set(font_scale=1)\n    sns.heatmap(corr, \n                xticklabels=[col.replace(\"_price\", \"\") for col in corr.columns.values],\n                yticklabels=[col.replace(\"_price\", \"\") for col in corr.columns.values],\n                annot=True, linewidths=.5,\n                vmin=0, vmax=1)","c2ef0751":"fig = plt.figure(figsize=[22, 20])\nplt.tight_layout()\ni = 221\nfor year in range(2016,2019):\n    sub=plt.subplot(i)\n    sub.set_title(\"Correlation of crypto in \"+str(year), fontsize=22)\n    draw_pearson_correlation(datetime(year,1,1),datetime(year,11,3))\n    i+=1\nplt.show()","8bbeff05":"btc_google = my_cryptory.get_google_trends(kw_list=['bitcoin'])\n","96dc0c18":"btc_google.index = btc_google.date\nbtc_google","4e695916":"\nk = btc_google.resample('M').mean()\n\nplt.figure(figsize=(20,7))\nax = plt.subplot(111)    \nplt.yticks(fontsize=14)    \nplt.xticks(fontsize=14)\nax.spines[\"top\"].set_visible(False)        \nax.spines[\"right\"].set_visible(False)    \nplt.grid(linewidth=0.2)\nplt.title('BTC x USD (Bitfinex)')\nplt.plot(df_month.btc_price.apply(np.log))\nplt.plot(k.bitcoin.apply(np.log))\n\nplt.show()","ee47f521":"selected = ['btc','eth','ltc','dash']\ndata = get_coins_price(selected,datetime(2016,1,1), datetime(2016,12,31))\ndata","f94ea06b":"clean = data.set_index('date')\ntable = clean.sort_values(by = 'date',ascending = True).dropna()\n\n# calculate daily and annual returns of crypto\nreturns_daily = table.pct_change()\nreturns_annual = returns_daily.mean() * 365\n\n# get daily and covariance of returns of crypto\ncov_daily = returns_daily.cov()\ncov_annual = cov_daily * 365\n\n# returns, volatility and weights of portfolios\nport_returns = []\nport_volatility = []\nsharpe_ratio = []\nstock_weights = []\n\nnum_assets = len(selected)\nnum_portfolios = 50000\n\nnp.random.seed(101)\n\nfor single_portfolio in range(num_portfolios):\n    weights = np.random.random(num_assets)\n    weights \/= np.sum(weights)\n    returns = np.dot(weights, returns_annual)\n    volatility = np.sqrt(np.dot(weights.T, np.dot(cov_annual, weights)))\n    sharpe = returns \/ volatility\n    sharpe_ratio.append(sharpe)\n    port_returns.append(returns)\n    port_volatility.append(volatility)\n    stock_weights.append(weights)\n\n# a dict with Returns and Risk values of portfolio\nportfolio = {'Returns': port_returns,\n             'Volatility': port_volatility,\n             'Sharpe Ratio': sharpe_ratio}\n\nfor counter,symbol in enumerate(selected):\n    portfolio[symbol+' Weight'] = [Weight[counter] for Weight in stock_weights]\n\n# create dataframe\ndf = pd.DataFrame(portfolio)\n\n# change labels\ncolumn_order = ['Returns', 'Volatility', 'Sharpe Ratio'] + [stock+' Weight' for stock in selected]\n\n# reorder columns\ndf = df[column_order]","d0551446":"plt.style.use('seaborn-dark')\ndf.plot.scatter(x='Volatility', y='Returns', c='Sharpe Ratio',\n                cmap='magma', edgecolors='black', figsize=(10, 10), grid=True)\nplt.xlabel('Volatility (Std. Deviation)')\nplt.ylabel('Expected Returns')\nplt.title('Efficient Frontier')\nplt.show()","77c5ef45":"# find min Volatility & max sharpe values\nmin_volatility = df['Volatility'].min()\nmax_sharpe = df['Sharpe Ratio'].max()\n\nsharpe_portfolio = df.loc[df['Sharpe Ratio'] == max_sharpe]\nmin_variance_port = df.loc[df['Volatility'] == min_volatility]\n","f84805e4":"plt.style.use('seaborn-dark')\ndf.plot.scatter(x='Volatility', y='Returns', c='Sharpe Ratio',\n                cmap='magma', edgecolors='black', figsize=(10, 8), grid=True)\nplt.scatter(x=sharpe_portfolio['Volatility'], y=sharpe_portfolio['Returns'], c='red', marker='D', s=100)\nplt.scatter(x=min_variance_port['Volatility'], y=min_variance_port['Returns'], c='blue', marker='D', s=100 )\nplt.xlabel('Volatility (Std. Deviation)')\nplt.ylabel('Expected Returns')\nplt.title('Efficient Frontier')\nplt.show()","f21c0525":"\nmin_variance_port.T","f3391fb4":"sharpe_portfolio.T","bc9af348":"The most risk-averse investor would construct the minimum variance portfolio which has an expected return of 100.7% with an accompanying expected volatility of 31.2%","6c7bb1fc":"**Box-Cox transformation**\n\n","33f44ed6":"**Trends**\n\nAs we can see there was strong upward trend in 2017 - 2018.\n\n**Seasonality**\n\nThe prices of most financial assets (particularly stocks) typically tend to be under pressure in the months of September and October. When examining Bitcoin, we can identify the same pattern.\n\nThe most interesting time period for investors starts after this seasonal decline end and the market turns back up. Beginning in mid October, Bitcoin traditionally experiences a rally that continues until December.","5f4d611e":"### **Portfolio optimization**\n<div id=\"intLink3\">\n<\/div>\n\nNow lets try to apply Portfolio Theory on Cryptocurrensies market.\nAs we saw in previous part, correlation of different crypto is very hight now, so i will take data from 2016 to construct over portfolio.\nI want to believe, that in future crypto market will stabilize and we can apply this theory on relevant data.\n\nFor my portfolio I take currencies with lowest correlaton, that existed in 2016. \n\n","b15c6af5":"Each bar in graph above represents the correlation between the original series and its kth lag. The first bar is always equal to one as this is simply measuring the variable correlated with itself. The blue line apparent in any plot represents statistically significant values other than 0, meaning any bars underneath this threshold are not statistically significant.","683461e3":"\nThe fundamental tenet of this theory is the possibility for investors to construct an \u201cefficient set of portfolios\u200a\u2014\u200aEfficient Frontier\u201d that offers the maximum expected returns for a given level of risk. An investor\u2019s tolerance for risk determines the type of \u201cefficient portfolio\u201d he opts for. An investor with the lowest tolerance opts for a portfolio that offers him the maximum expected return given the lowest possible risk and vice versa.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*4Ke6NhIUYPR12Sa2nrIpxw.jpeg\"\/>","c3ecdc4f":"**Fitting model**\n\nHere we will fit Seasonal ARIMA (SARIMA) model. It is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component.\n\nIt adds three new hyperparameters to specify the autoregression (AR), differencing (I) and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality.\n\nThere are four seasonal elements that are not part of ARIMA that must be configured; they are:\n\n* P: Seasonal autoregressive order.\n* D: Seasonal difference order.\n* Q: Seasonal moving average order.\n* m: The number of time steps for a single seasonal period.\n\nFrom previous part we already know that:\n* The degree of differencing is 1 (D = d = 1)\n* Seasaonal time step is 12 (m = 12)","41fcd3f7":"Adopting an ARIMA model for a time series assumes that the underlying process that generated the observations is an ARIMA process.\n\nFirst of all we need to make sure that the time-series is stationary, thats where differencing comes into place (degree corrects the level of non-stationarity if possible).\n\nLet's resample our data with different frequency and visualize it","f297bc54":"Let's visualize our data using a method called time-series decomposition that allows us to decompose our time series into three distinct components: trend, seasonality, and noise.","206cba52":"**Seasonal Differencing**","fb2ad748":"**Augmented Dickey-Fuller**\n\nTo determine if a time series is stationary or not, we will use the ADF test which is a type of unit root test. Unit roots are a cause for non-stationarity, the ADF test will test if unit root is present.\n\nThe Null and Alternate hypothesis of the Augmented Dickey-Fuller test are defined as follows:\n\n* **Null Hypothesis(H0)**: states there is presence of a unit root, meaning it is non-stationary\n* **Alternate Hypothesis(H1)**: states there is no unit root. In other words, Stationarity exists, meaning it is stationary. It does not have time-dependent structure.","aa67c7e5":"I will make log() transformation on our data .","127a3fbd":"Now, it's easier to see upward and downward trends","10dfb66b":"**Detrend by Differencing**","b19695c2":"* This first heat map is looking at the year 2016. What we can take from this chart here is the correlation between BTC\/LTC. We also see a noticeable positive correlation between DOGE\/VTC. This is because Bitcoin and Litecoin are part of the same family with similar functions and prospects.\n\n* The visualisation of the correlation in 2017 shows something interesting.  The majority of pairs started to show dependency and increasing correlation.\n\n* In 2018 the chart looks even more curious. Like a fire spreading through the woods, Bitcoin\u2019s magnetism seems to affect every coin if you give it time. We noticed an increase in the Pearson correlation coefficient over 0.5 for nearly all pairs studied in this report. If in the beginning we could notice correlations based on the sector of activity, in 2018 it isn\u2019t just pairs of cryptocurrencies from the same family but all cryptos across the market tend to synchronize their movements.  We noticed how correlated a great variety of cryptocurrencies are and that this trend is more strong as the days go by. The reasons for why this happens is the subject of much speculation. We try to present some of the things we think might affect this without being biased to any of the options.\n\n**Main reasons**:\n* Lack of important data.\n\n  This is useful for differentiating between capabilities and future risks of various projects. Investors might consider cryptocurrencies     as a group rather than individual projects with unique trajectories.\n\n* Unregulated and still young market dominated by retail investors.\n\n  Only a few investors take investment decisions based on intensive research. In general retail investors are more exposed to behavioral     biases. This is often due to a lack of technical economic education. Frequent trading, buying when prices go up and selling when they go   down are good examples of this behaviour.\n\n* Limited or no options for trading or investing other currencies than BTC\/fiat, BTC\/ETH.\n\n  Investors must first buy BTC or ETH before they can exchange one of these for the desired cryptocurrency. This pushes the dominance of     BTC over the market, resulting in a \u201cfollow the leader\u201d behaviour.    ","b71dcc6a":"Here is our best models.\n\nWe will measure them with AIC.\n\nThe Akaike Information Critera (AIC) is a widely used measure of a statistical model. It basically quantifies the goodness of fit, and  the simplicity\/parsimony, of the model into a single statistic. When comparing two models, the one with the lower AIC is generally \u201cbetter\u201d. ","c4b24e3e":"It is clear that our data is non-stationary, but we will examine one more strong tool - ADF test.","43d8a10d":"There we have it. An Efficient Frontier with a highlight of:\n* The optimal (as per the risk-adjusted return metric) portfolio (Red)\n* Portfolio with the minimum volatility (Blue).\n\nHere is their details:","5a44188b":"Now, data is stationary.","f83a643e":"Lets visualize Historical Bitcoin Market Volume (2013-2018)\n","141866e6":"Here is our prediction of Bitcoin (BTC) for 2019.","085c1b2f":"### **Correlation**\n<div id=\"intLink2\">\n<\/div>\nIn this section, I want to check different cryptocurrencies correlation.\n\nWe will visualise them with  heatmaps of Pearson correlation.","ee00611c":"### **Data**\n\nIn this section we just explore the Data i.e the Historic Bitcoin Prices and try to find some insights.\nFor research I will use Cryptory library for Python. It can give us us historical data from coinmarketcap.com and bitinfocharts.com\nHere is the functional library provide:","74073389":"Here is the brief Bitcoin timeline:\n\n<img src=\"https:\/\/en.bitcoinwiki.org\/upload\/en\/images\/c\/cf\/Bitcoin_history_price.jpg\"\/>","d1ce3a8d":"Next up, let\u2019s try to locate the optimal portfolio and another portfolio with the minimum volatility for the most risk-averse investor out there:","7448c244":"Let's try to visualise behavioral biases, we were talking about.\n\nTo do this we use google trends, that gives us frequency of google searches.","3b0de1c6":"This is the endpoint of my research,\n\nThank you for your attention.","5774e211":"To get the efficient frontier, we need to simulate imaginary combinations of portfolios (50,000 portfolios of different combinations in this case).We use  Monte Carlo Simulation (sort of manual way) to do this.\n\nTo find best of them we will use Sharpe ratio.\n\nSharpe ratio is simply as a measure of the performance of an investment\u2019s returns given its risk. This ratio adjusts the returns of an investment which makes it possible to compare different investments on a scale that incorporates risk. Without this scale of comparison, it would be virtually impossible to compare different investments with different combinations and their accompanying risks and returns.","04e47345":"## Cryptocurrencies analysis and forecasting\n*Research for Econometrics and Finance courses*\n\n### **Introduction**\n\nCryptocurrencies are digital money based on a distributed, safe and decentralized payment system that is created by means of cryptographic methods and transfered on a distributed ledger known as a \u201eblockchain\u201c. The blockchain is used to record information, primarily about the balance of every adress. The approach can be extended to lots of applications.\n\nThe first cryptocurrency, Bitcoin (BTC), originated with a whitepaper that was published in 2008 by a pseudonym called \u201eSatoshi Nakamoto\u201c. The main goal was to create a decentralized payment system that allows electronic transactions with the absence of intermediary institutions like banks, etc. Blockchain technology allows us to store and transfer money without the need of a central authority.\n\nCryptocurrencies such as bitcoin, ethereum, litecoin and monero have recently both gained and fallen rapidly in value. Such price volatility is presumably driven by changes in the policy and practices of private firms and individuals as well governments that affect their supply; as well as sentiment and linkage factors not directly related to their fundamental value. Cryptocurrencies have understandably attracted a plethora of attention from investors, regulators and the media.\n\nIn this research I want to focus on Time-Series Forecasting of Historical Bicoin Prices and analysis of all cryptocurrencies market.\n\nMain parts of my research:\n* [ARIMA for Bitcoin forecasting](#intLink1)\n* [Cryptocurrencies correlations](#intLink1)\n* [Optimization of cryptocurrencies portflio](#intLink3)\n\n\n\n","a1fc9c82":"Investors seeking the maximum risk-adjusted return would opt for portfolio that with the maximum Sharpe Ratio which has an expected return of 135.2% with expected volatility pegged at 34.9%","54f7fded":"### **ARIMA**\n<div id=\"intLink1\">\n<\/div>","ce4ec8a8":"As you can see Bitcoin price and Google trends are much correlated.","614b016b":"This is daily data of Google search - 'bitcoin'.","43d7dfbe":"For Bitcoin Forecasting I will use ARIMA model.\n\nARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. It is a generalization of the simpler AutoRegressive Moving Average and adds the notion of integration.\n\nThis acronym is descriptive, capturing the key aspects of the model itself. Briefly, they are:\n\n* AR: Autoregression. Past time points of time series data can impact current and future time points. ARIMA models take this concept into account when forecasting current and future values. Model uses a number of lagged observations of time series to forecast observations. A weight is applied to each of the past term and the weights can vary based on how recent they are.\n<img src=\"https:\/\/www.machinelearningplus.com\/wp-content\/uploads\/2019\/02\/Equation-1-min.png\"\/>\n* I: Integrated. If a trend exists then time series is considered non stationary and shows seasonality. Integrated is a property that reduces seasonality from a time series. .\n* MA: Moving Average. Error terms of previous time points are used to predict current and future point\u2019s observation. Moving average (MA) removes non-determinism or random movements from a time series.\n<img src=\"https:\/\/www.machinelearningplus.com\/wp-content\/uploads\/2019\/02\/Equation-2-min.png\"\/>\n\nAn ARIMA model is one where the time series was differenced at least once to make it stationary and you combine the AR and the MA terms. So the equation becomes:\n<img src=\"https:\/\/www.machinelearningplus.com\/wp-content\/uploads\/2019\/02\/Equation-4-min-1024x91.png\"\/>\n\nEach of these components are explicitly specified in the model as a parameter. A standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used.\n\nThe parameters of the ARIMA model are defined as follows:\n\n* p: The number of lag observations included in the model, also called the lag order.\n* d: The number of times that the raw observations are differenced, also called the degree of differencing.\n* q: The size of the moving average window, also called the order of moving average.","2f8c056c":"P-value is high, it shows that our data is non-stationary.\n\nTo detrend data we will use differencing and transformations.\n"}}