{"cell_type":{"d9d4ad28":"code","73114d3a":"code","e4effe54":"code","74800c6c":"code","152a2ad3":"code","f1a07d0f":"code","e5f7907c":"code","8a72ffd8":"code","f0c42367":"code","e6e431e1":"code","b837a730":"markdown","a4b841fd":"markdown","565b2dba":"markdown","c32b4df3":"markdown","c8932b49":"markdown","ab4c5e4e":"markdown"},"source":{"d9d4ad28":"import numpy as np\n#NumPy is a python library used for working with arrays.\n#It also has functions for working in domain of linear algebra, fourier transform, \n#and matrices.\nimport pandas as pd \n#it offers data structures and operations for manipulating numerical tables and time series. \nimport os\nimport matplotlib.pyplot as plt\n#Matplotlib is a comprehensive library for creating static, animated, and \n#interactive visualizations in Python.\nimport seaborn as sns\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n#UTF-8 is a variable-width character encoding standard \n#that uses between one and four eight-bit bytes to represent all valid Unicode code points.\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.\n\ndata=pd.read_csv(\"\/kaggle\/input\/student-grade-prediction\/student-mat.csv\")\ndata.head()","73114d3a":"data.info()","e4effe54":"data = data.drop(labels= ['guardian', 'nursery','address','failures'], axis=1)","74800c6c":"# Dropping the duplicate rows\nduplicate_rows_data = data[data.duplicated()]\nprint('number of duplicate rows: ', duplicate_rows_data.shape)","152a2ad3":"data = data.drop_duplicates()\ndata.count()","f1a07d0f":"#Dropping the missing or null values.\n# Dropping the missing values.\ndata = data.dropna() \ndata.count()","e5f7907c":"data.info()","8a72ffd8":"data.head(5)","f0c42367":"x = np.array(data.loc[:,'studytime']).reshape(-1,1)\ny = np.array(data.loc[:,'G1']).reshape(-1,1)\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(x,y)\nplt.xlabel('studytime')\nplt.ylabel('G1')\nplt.show()","e6e431e1":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n# Predict space\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n# Fit\nreg.fit(x,y)\n# Predict\npredicted = reg.predict(predict_space)\n# R^2 \nprint('R^2 score: ',reg.score(x, y))\n# Plot regression line and scatter\nplt.plot(predict_space, predicted, color='black', linewidth=3)\nplt.scatter(x,y)\nplt.xlabel('studytime')\nplt.ylabel('G1')\nplt.show()","b837a730":"![image.png](attachment:image.png)\nStandard Deviation Reduction \t\t\nThe standard deviation reduction is based on the decrease in standard deviation after a dataset is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest standard deviation reduction (i.e., the most homogeneous branches).","a4b841fd":"### Random Forest Regression\n\nRandom forest is an ensemble approach where we take into account the predictions of several decision regression trees.\n\n    1. Select K random points\n    2. Identify n where n is the number of decision tree regressors to be created. Repeat steps 1 and 2 to create several regression trees.\n    3. The average of each branch is assigned to the leaf node in each decision tree.\n    4. To predict output for a variable, the average of all the predictions of all decision trees are taken into consideration.\n Random Forest prevents overfitting (which is common in decision trees) by creating random subsets of the features and building smaller trees using these subsets.    ","565b2dba":"### Decision Tree Regression  \nDecision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision on the numerical target. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data. \n\n![image.png](attachment:image.png)\n\nThe core algorithm for building decision trees called ID3 by J. R. Quinlan which employs a top-down, greedy search through the space of possible branches with no backtracking. The ID3 algorithm can be used to construct a decision tree for regression by replacing Information Gain with Standard Deviation Reduction.\nStandard Deviation : A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous). We use standard deviation to calculate the homogeneity of a numerical sample. If the numerical sample is completely homogeneous its standard deviation is zero.","c32b4df3":"![image.png](attachment:image.png)","c8932b49":"# REGRESSION\nRegression models are used to predict a continuous value. Predicting prices of a house given the features of house like size, price etc is one of the common examples of Regression. It is a supervised technique.\n\n* Types of Regression\n\n   1. Simple Linear Regression\n   2. Polynomial Regression\n   3. Support Vector Regression\n   4. Decision Tree Regression\n   5. Random Forest Regression\n   \n### Simple Linear Regression\n\nThis is one of the most common and interesting type of Regression technique. Here we predict a target variable Y based on the input variable X. A linear relationship should exist between target variable and predictor and so comes the name Linear Regression.Consider predicting the salary of an employee based on his\/her age. We can easily identify that there seems to be a correlation between employee\u2019s age and salary (more the age more is the salary). The hypothesis of linear regression is\n\n* Y=a+bX\n\nY represents salary, X is employee\u2019s age and a and b are the coefficients of the equation. So in order to predict Y (salary) given X (age), we need to know the values of a and b (the model\u2019s coefficients).While training and building a regression model, it is these coefficients which are learned and fitted to training data. The aim of the training is to find the best fit line such that cost function is minimized. The cost function helps in measuring the error. During the training process, we try to minimize the error between actual and predicted values and thus minimizing the cost function.\n\n### Polynomial Regression\n\nIn polynomial regression, we transform the original features into polynomial features of a given degree and then apply Linear Regression on it. Consider the above linear model Y = a+bX is transformed into something like\n\n* Y=a+bX+cX^2\n\nIt is still a linear model but the curve is now quadratic rather than a line. Scikit-Learn provides PolynomialFeatures class to transform the features.\nIf we increase the degree to a very high value, the curve becomes overfitted as it learns the noise in the data as well.\n\n### Support Vector Regression\n\nIn SVR, we identify a hyperplane with maximum margin such that the maximum number of data points are within that margin. SVRs are almost similar to the SVM classification algorithm. Instead of minimizing the error rate as in simple linear regression, we try to fit the error within a certain threshold. Our objective in SVR is to basically consider the points that are within the margin. Our best fit line is the hyperplane that has the maximum number of points.\n![image.png](attachment:image.png)\nIn simple regression we try to minimise the error rate. While in SVR we try to fit the error within a certain threshold. This might be a bit confusing but let me explain.\n\n    Kernel: The function used to map a lower dimensional data into a higher dimensional data.\n    \n    Hyper Plane: In SVM this is basically the separation line between the data classes. Although in SVR we are going to define it as the line that will will help us predict the continuous value or target value.\n    \n    Boundary line: In SVM there are two lines other than Hyper Plane which creates a margin . The support vectors can be on the Boundary lines or outside it. This boundary line separates the two classes. In SVR the concept is same.\n    \n    Support vectors: This are the data points which are closest to the boundary. The distance of the points is minimum or least.\n   ","ab4c5e4e":"# Information about Dataset\n\nThis data approach student achievement in secondary education of two Portuguese schools. The data attributes include student grades, demographic, social and school-related features) and it was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). In [Cortez and Silva, 2008], the two datasets were modeled under binary\/five-level classification and regression tasks. \n\nImportant note: the target attribute G3 has a strong correlation with attributes G2 and G1. This occurs because G3 is the final year grade (issued at the 3rd period), while G1 and G2 correspond to the 1st and 2nd period grades. It is more difficult to predict G3 without G2 and G1, but such prediction is much more useful (see paper source for more details).\n\nthese grades are related with the course subject, Math or Portuguese:\n\n      * G1 - first period grade (numeric: from 0 to 20)\n      * G2 - second period grade (numeric: from 0 to 20)\n      * G3 - final grade (numeric: from 0 to 20, output target)\n    \n## Attribute Information:\n\n    school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\n    sex - student's sex (binary: 'F' - female or 'M' - male)\n    age - student's age (numeric: from 15 to 22)\n    address - student's home address type (binary: 'U' - urban or 'R' - rural)\n    famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n    Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n    Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 \u00e2\u20ac\u201c 5th to 9th grade, 3 \u00e2\u20ac\u201c secondary education or 4 \u00e2\u20ac\u201c higher education)\n    Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 \u00e2\u20ac\u201c 5th to 9th grade, 3 \u00e2\u20ac\u201c secondary education or 4 \u00e2\u20ac\u201c higher education)\n    Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n    Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n    reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n    guardian - student's guardian (nominal: 'mother', 'father' or 'other')\n    traveltime - home to school travel time (numeric: 1 - 1 hour)\n    studytime - weekly study time (numeric: 1 - 10 hours)\n    failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n    schoolsup - extra educational support (binary: yes or no)\n    famsup - family educational support (binary: yes or no)\n    paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n    activities - extra-curricular activities (binary: yes or no)\n    nursery - attended nursery school (binary: yes or no)\n    higher - wants to take higher education (binary: yes or no)\n    internet - Internet access at home (binary: yes or no)\n    romantic - with a romantic relationship (binary: yes or no)\n    famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n    freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n    goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n    Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n    Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n    health - current health status (numeric: from 1 - very bad to 5 - very good)\n    absences - number of school absences (numeric: from 0 to 93)\n    \n\n"}}