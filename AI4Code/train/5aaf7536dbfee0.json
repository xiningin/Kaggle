{"cell_type":{"a4bc8a35":"code","a09c7a39":"code","f9a35e99":"code","8e5da4ab":"code","e5d98cde":"code","633f7a64":"code","f3b78483":"code","442e574c":"code","fc07e10a":"code","11af3782":"code","363f1f17":"code","6cbb5f0c":"markdown","e3a995c2":"markdown","073b45bc":"markdown","6df710e1":"markdown","bf23e0ad":"markdown","b4906bc5":"markdown","cfa20ac9":"markdown","eff29092":"markdown"},"source":{"a4bc8a35":"import numpy as np \nimport pandas as pd\nimport tensorflow\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\nfrom sklearn.metrics import log_loss\nimport tensorflow as tf\n","a09c7a39":"data_train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ndata_test = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ndata_train_target_ns = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ndata_train_target_s = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\nsub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')","f9a35e99":"display(data_train.head())\nprint(\"SHAPE of training data--\",data_train.shape)","8e5da4ab":"display(data_test.head())\nprint(\"SHAPE of test data--\",data_test.shape)","e5d98cde":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72:2})\n    del df['sig_id']\n    return df\n\ntrain = preprocess(data_train)\ntest = preprocess(data_test)\n\ndel data_train_target_s['sig_id']","633f7a64":"def create_model(num_columns):\n    model = Sequential()\n    model.add(Input(num_columns))\n    model.add(BatchNormalization())\n    model.add(Dense(2048, activation='relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1024, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(206, activation='sigmoid'))\n    \n    optimizer = tfa.optimizers.Lookahead('adam',sync_period=10)\n    \n    model.compile(optimizer=optimizer,\n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n    \n    model.summary()\n    return model","f3b78483":"def metric(y_true, y_pred):\n    metrics = []\n    for _target in data_train_target_s.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)","442e574c":"N_STARTS = 4\ntf.random.set_seed(42)\n\nres = data_train_target_s.copy()\nsub.loc[:, data_train_target_s.columns] = 0\nsub.loc[:, data_train_target_s.columns] = 0\n\nfor seed in range(N_STARTS):\n    for n, (train_idx, test_idx) in enumerate(KFold(n_splits=5, random_state=seed, shuffle=True).split(data_train_target_s, data_train_target_s)):\n        print(f'Fold {n}')\n    \n        model = create_model(875)\n        checkpoint_path = f'repeat:{seed}_Fold:{n}.h5'\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n        model.fit(train.values[train_idx],\n                  data_train_target_s.values[train_idx],\n                  validation_data=(train.values[test_idx], data_train_target_s.values[test_idx]),\n                  epochs=25, batch_size=128,\n                  callbacks=[reduce_lr_loss, cb_checkpt], verbose=1\n                 )\n        \n        model.load_weights(checkpoint_path)\n        test_predict = model.predict(test.values)\n        val_predict = model.predict(train.values[test_idx])\n        \n        sub.loc[:, data_train_target_s.columns] += test_predict\n        res.loc[test_idx, data_train_target_s.columns] += val_predict\n        print('')\n    \nsub.loc[:, data_train_target_s.columns] \/= ((n+1) * N_STARTS)\nres.loc[:, data_train_target_s.columns] \/= N_STARTS","fc07e10a":"print(f'OOF Metric: {metric(data_train_target_s, res)}')","11af3782":"sub.loc[test['cp_type']==1, data_train_target_s.columns] = 0","363f1f17":"sub.to_csv('submission.csv', index=False)","6cbb5f0c":"## Training Model \nWe will create 4 models and each model will have 5 kfold split. In last we average out the predictions and save it to csv","e3a995c2":"# Mechanisms of Action (MoA) Prediction","073b45bc":"### Encode cp_type, cp_dose, cp_time and remove sig_id","6df710e1":"**train_features.csv** - Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).<br>\n**train_targets_scored.csv** - The binary MoA targets that are scored.<br>\n**train_targets_nonscored.csv** - Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.<br>\n**test_features.csv** - Features for the test data. You must predict the probability of each scored MoA for each row in the test data.<br>\n**sample_submission.csv** - A submission file in the correct format.","bf23e0ad":"## Step 1: Understanding the Data","b4906bc5":"## Creating Model ","cfa20ac9":"## Loading Data","eff29092":"## Do upvote, help me reach expert in notebooks :)"}}