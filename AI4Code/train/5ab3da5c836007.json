{"cell_type":{"f6f83035":"code","b64383eb":"code","707d706e":"code","f3a5f1af":"code","42df08c3":"code","259fe243":"code","2d01a9c0":"code","a22c4a77":"code","0425b71d":"code","ff809bff":"code","2f24c64a":"code","9a3581dd":"code","b85e8714":"code","064f0b82":"code","7b1d436a":"code","e7f40b11":"code","dde1df23":"code","4c1e2ec5":"code","1c054c9b":"code","f3a51f59":"code","96984c3b":"code","4586c55f":"code","ba62dd13":"code","a828c59d":"code","dd8d4a9d":"code","e77ea78f":"code","fdaad28d":"code","b156a355":"code","ce4fa2a5":"code","3781aaac":"code","5f55574f":"code","754fe5b5":"code","cdc1c326":"code","a338eabf":"code","d8365f35":"code","ac3ffa61":"code","4ba49f9c":"code","0a1bf75b":"code","ab9aebe3":"code","648f2b76":"code","aeaa073f":"code","7aabd0a4":"code","618b6080":"code","09ac3b9d":"code","c5309c35":"code","971f1686":"code","82b598f1":"code","f44b5254":"code","5234a56e":"code","f8dafcf7":"code","86704ba1":"code","835bde30":"code","e7ef3eb6":"code","b758bb3c":"code","c7eea131":"code","fec34670":"code","70eaed79":"code","5aac0d0d":"code","9544884f":"code","8bf3123f":"code","042d7d70":"code","8be2f5cf":"code","2aed24ec":"code","a1d773a4":"code","7f52e3b5":"code","a91010ed":"code","c5298476":"code","81c2931e":"code","2a00fcbb":"code","47c2ab9a":"code","e9284ff6":"code","c3490d5c":"code","7d551acc":"code","96c89a8c":"code","c4428d90":"code","00ad73ba":"code","aa063358":"markdown","b73f12cc":"markdown","670b6275":"markdown","53b61afb":"markdown","eb800f8d":"markdown","e7ca1de4":"markdown","9a043929":"markdown","e0d64451":"markdown","54fbf477":"markdown","4822e707":"markdown","4395a18b":"markdown","b8e5638c":"markdown","c590237a":"markdown","dcc18793":"markdown","1b03a67c":"markdown","1ba6657b":"markdown","fa380708":"markdown","cad0a081":"markdown","86e90998":"markdown","a8610626":"markdown","44879b32":"markdown","eaf5ae51":"markdown","733ab9be":"markdown","bdbb4157":"markdown","1e420ddd":"markdown","676aaef0":"markdown","033f3db5":"markdown","b0d1b808":"markdown","b64d1344":"markdown","7b49ebe3":"markdown","268f3cd5":"markdown","5ed0cfda":"markdown","203ed70c":"markdown","82ac1b49":"markdown","9d663310":"markdown","fcd2e464":"markdown","91628b78":"markdown","ecc7893d":"markdown"},"source":{"f6f83035":"import pandas as pd\nimport numpy as np\nimport string, os,re,pickle\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense,Dropout\nfrom keras.preprocessing.text import Tokenizer\nimport keras.utils as ku \nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport pickle\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport io\nimport cufflinks as cf\ncf.go_offline()\n\n%matplotlib inline","b64383eb":"df1 = pd.read_csv('..\/input\/ted-talks\/ted_main.csv')\ntranscripts = pd.read_csv('..\/input\/ted-talks\/transcripts.csv')","707d706e":"df1.head()","f3a5f1af":"transcripts.head()","42df08c3":"df1.info()","259fe243":"df1.isnull().sum()","2d01a9c0":"df1['speaker_occupation'] = df1.speaker_occupation.fillna(df1.speaker_occupation.mode()[0])","a22c4a77":"#converting to datetime object\ndf1['film_date'] = pd.to_datetime(df1['film_date'], unit='s')\ndf1['published_date'] = pd.to_datetime(df1['published_date'], unit='s')\n\n#creating a year column\ndf1['year'] = df1['film_date'].apply(lambda x: x.year)\n\n#converting duration unit to minute\ndf1['duration'] = df1['duration'].apply(lambda x: round(x\/60, 2))","0425b71d":"fig, ax = plt.subplots(figsize=(7, 5))    \nsns.heatmap(df1.corr(), annot = True, linewidths =.5, ax = ax, cmap = 'mako')","ff809bff":"df1.hist(figsize=(20, 20), bins=50, xlabelsize=8, ylabelsize=8); ","2f24c64a":"views = df1.sort_values(by = 'views', ascending = False).head(10).reset_index()\n\n\nfig = px.bar_polar(views, \n                   r=\"duration\", \n                   theta=\"title\", \n                   color=\"views\",\n                   color_discrete_sequence= px.colors.sequential.Plasma_r,\n                   title=\"top 10 talks, number of views and duration\"\n                  )\nfig.show()","9a3581dd":"events = pd.DataFrame(df1['event'].value_counts().head(20)).reset_index()\n\n\nfig = px.bar(events, \n             x ='index', \n             y='event',\n             color = 'index',\n             color_continuous_scale = px.colors.sequential.Oryel, \n             title=' Number of TED talks throughout the years',\n             labels=dict(x= \"year\", y=\"number of talks\", color = 'year'))\nfig.show()","b85e8714":"fig = px.scatter(df1, \n                 x = \"comments\", \n                 y = \"views\", \n                 size = \"comments\", \n                 color = \"views\",\n                 hover_name=\"title\", \n                 log_x=True, \n                 size_max=60)\nfig.show()","064f0b82":"speakers = pd.DataFrame(df1['speaker_occupation'].value_counts().head(10)).reset_index()\n\nfig = px.pie(speakers, \n             values='speaker_occupation', \n             names='index', \n             title='the most repeated occupations of speakers')\nfig.show()\nprint(\"we can see that writers represent the greatest portion of speaker's occupations\")","7b1d436a":"duration = df1.sort_values(by = 'duration', ascending = False).head(10)\n\nfig = px.bar(duration, \n             x ='title', \n             y='duration',\n             color = 'views',\n             color_continuous_scale = px.colors.sequential.Oryel, \n             title='longest 10 talks')\nfig.show()","e7f40b11":"years = pd.DataFrame(df1['year'].value_counts()).reset_index()\n\nfig = px.bar(years, \n             x ='index', \n             y='year',\n             color = 'year',\n             color_continuous_scale = px.colors.sequential.Oryel, \n             title=' Number of TED talks throughout the years',\n             labels=dict(x= \"year\", y=\"number of talks\", color = 'number'))\nfig.show()","dde1df23":"transcripts['number_of_words']= transcripts['transcript'].str.split().str.len()\n\nfig = px.histogram(transcripts, x=\"number_of_words\", hover_data=transcripts.columns)\nfig.show()","4c1e2ec5":"transcripts.head()","1c054c9b":"transcripts.shape","f3a51f59":"c=0\nfor i in transcripts[\"transcript\"]:\n    c=c+len(i)\nprint (\"Total Words in Data:\",c)","96984c3b":"transcripts[\"transcript\"][0]","4586c55f":"text=\"\"\nfor i in transcripts[\"transcript\"]:\n    text+=i","ba62dd13":"text[-100:]","a828c59d":"len(text)","dd8d4a9d":"train_text=text[:500000]","e77ea78f":"len(train_text)","fdaad28d":"cleaned_text=[]\ntxt=re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", train_text)\ntxt=txt.strip().replace(\"\\'\",\"'\")\nfor i in txt:\n    if i not in string.punctuation.replace(\"'\",\"\"):\n        cleaned_text.append(i)\n    else:\n        cleaned_text.append(\" \")\ncleaned_text=\"\".join(cleaned_text).lower()","b156a355":"cleaned_text[:100]","ce4fa2a5":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts([cleaned_text])\n\n# saving the tokenizer for predict function\n#pickle.dump(tokenizer, open('\/content\/drive\/MyDrive\/ted talks\/token.pkl', 'wb'))\n\nsequence_data = tokenizer.texts_to_sequences([cleaned_text])[0]\nsequence_data[:15]","3781aaac":"tokenizer.index_word","5f55574f":"len(sequence_data)","754fe5b5":"vocab_size = len(tokenizer.word_index) + 1\nprint(vocab_size)","cdc1c326":"sequences = []\n\nfor i in range(3, len(sequence_data)):\n    words = sequence_data[i-3:i+1]\n    sequences.append(words)\n    \nprint(\"The Length of sequences are: \", len(sequences))\nsequences = np.array(sequences)\nsequences[:10]","a338eabf":"X = []\ny = []\n\nfor i in sequences:\n    X.append(i[0:3])\n    y.append(i[3])\n    \nX = np.array(X)\ny = np.array(y)","d8365f35":"print(\"Data: \", X[:10])\nprint(\"Response: \", y[:10])","ac3ffa61":"y = to_categorical(y, num_classes=vocab_size)\ny[:5]","4ba49f9c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","0a1bf75b":"model1 = Sequential()\nmodel1.add(Embedding(vocab_size, 10, input_length=3))\nmodel1.add(LSTM(1000, return_sequences=True))\nmodel1.add(LSTM(1000))\nmodel1.add(Dense(1000, activation=\"relu\"))\nmodel1.add(Dense(vocab_size, activation=\"softmax\"))","ab9aebe3":"model1.summary()","648f2b76":"#checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\nmodel1.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001),metrics=[\"accuracy\"])\nmodel1.fit(X_train, y_train, epochs=70, batch_size=32)","aeaa073f":"losses1=pd.DataFrame(model1.history.history)\nlosses1.plot()","7aabd0a4":"# Load the model and tokenizer\n#model = load_model('next_words.h5')\n#tokenizer = pickle.load(open('token.pkl', 'rb'))","618b6080":"def Predict_Words(model, tokenizer, text,n_word):\n    result=text\n    for _ in range(n_word):\n        sequence = tokenizer.texts_to_sequences([text])\n        sequence = np.array(sequence)\n        preds = np.argmax(model.predict(sequence))\n        predicted_word = \"\"\n\n        for key, value in tokenizer.word_index.items():\n            if value == preds:\n                predicted_word = key\n                break\n        result+=\" \"+predicted_word\n        text=str(text)\n        text = text.split(\" \")\n        text = text[-3:]\n        text+=\" \"+predicted_word\n\n    return result","09ac3b9d":"#good\nprint(Predict_Words(model1, tokenizer, \"What surprised you\",1))\nprint(Predict_Words(model1, tokenizer, \"have been three\",1))\nprint(Predict_Words(model1, tokenizer, \"I get a chance\",2))\nprint(Predict_Words(model1, tokenizer, \"last case of\",2))\nprint(Predict_Words(model1, tokenizer, \"I'm really grateful\",4))\nprint(Predict_Words(model1, tokenizer, \"human creativity in\",2))","c5309c35":"#not so good\nprint(Predict_Words(model1, tokenizer, \"good morning my\",2))\nprint( Predict_Words(model1, tokenizer, \"I would have never\",3))\nprint(Predict_Words(model1, tokenizer, \"I have an interest\",2))\nprint(Predict_Words(model1, tokenizer, \"If you work\",4))","971f1686":"print( Predict_Words(model1, tokenizer, \"I would have never\",22))","82b598f1":"model2 = Sequential()\nmodel2.add(Embedding(vocab_size, 10, input_length=3))\nmodel2.add(LSTM(1000, return_sequences=True))\nmodel2.add(Dropout(0.2))\nmodel2.add(LSTM(1000))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(1000, activation=\"relu\"))\nmodel2.add(Dense(vocab_size, activation=\"softmax\"))","f44b5254":"model2.summary()","5234a56e":"#checkpoint = ModelCheckpoint(\"next_words_3.h5\", monitor='loss', verbose=1, save_best_only=True)\nmodel2.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001),metrics=[\"accuracy\"])\nmodel2.fit(X_train, y_train, epochs=70, batch_size=64,validation_data=[X_test,y_test])","f8dafcf7":"losses2=pd.DataFrame(model2.history.history)\nlosses2.head()","86704ba1":"losses2[[\"loss\",\"accuracy\"]].plot()","835bde30":"losses2.plot()","e7ef3eb6":"# Load the model and tokenizer\n#loaded_model = load_model('next_words_3.h5')\n#tokenizer = pickle.load(open('token.pkl', 'rb'))","b758bb3c":"#good\nprint(Predict_Words(model2, tokenizer, \"What surprised you\",2))\nprint(Predict_Words(model2, tokenizer, \"have been three\",1))\nprint(Predict_Words(model2, tokenizer, \"last case of\",3))\nprint(Predict_Words(model2, tokenizer, \"I'm really grateful\",4))\nprint(Predict_Words(model2, tokenizer, \"human creativity in\",2))\nprint(Predict_Words(model2, tokenizer, \"good morning my\",3))\nprint( Predict_Words(model2, tokenizer, \"I would have never\",2))","c7eea131":"#not so good\nprint(Predict_Words(model2, tokenizer, \"I get a chance\",5))\nprint(Predict_Words(model2, tokenizer, \"I have an interest\",2))\nprint(Predict_Words(model2, tokenizer, \"If you work\",5))","fec34670":"print( Predict_Words(model2, tokenizer, \"I would have never\",22))","70eaed79":"sequences = []\n\nfor i in range(2, len(sequence_data)):\n    words = sequence_data[i-2:i+1]\n    sequences.append(words)\n    \nprint(\"The Length of sequences are: \", len(sequences))\nsequences = np.array(sequences)\nsequences[:10]","5aac0d0d":"X = []\ny = []\n\nfor i in sequences:\n    X.append(i[0:2])\n    y.append(i[2])\n    \nX = np.array(X)\ny = np.array(y)","9544884f":"print(\"Data: \", X[:10])\nprint(\"Response: \", y[:10])\n","8bf3123f":"y = to_categorical(y, num_classes=vocab_size)\ny[:5]","042d7d70":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","8be2f5cf":"len(X_train)","2aed24ec":"model3 = Sequential()\nmodel3.add(Embedding(vocab_size, 10, input_length=2))\nmodel3.add(LSTM(1000, return_sequences=True))\nmodel3.add(Dropout(0.2))\nmodel3.add(LSTM(1000, return_sequences=True))\nmodel3.add(Dropout(0.2))\nmodel3.add(LSTM(800))\nmodel3.add(Dropout(0.2))\nmodel3.add(Dense(1000, activation=\"relu\"))\nmodel3.add(Dense(vocab_size, activation=\"softmax\"))\n\nmodel3.summary()","a1d773a4":"#checkpoint = ModelCheckpoint(\"next_words_4.h5\", monitor='loss', verbose=1, save_best_only=True)\nmodel3.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001),metrics=['accuracy'])\nmodel3.fit(X_train, y_train, epochs=70, batch_size=64,validation_data=(X_test,y_test))\n","7f52e3b5":"losses3=pd.DataFrame(model3.history.history)\nlosses3.plot()","a91010ed":"losses3[[\"loss\",\"accuracy\"]].plot()","c5298476":"# Load the model and tokenizer\n#loaded_model = load_model('next_words_4.h5')\n#tokenizer = pickle.load(open('token.pkl', 'rb'))","81c2931e":"#good\nprint(Predict_Words(model3, tokenizer, \"What surprised\",2))\nprint(Predict_Words(model3, tokenizer, \"have been three\",1))\nprint(Predict_Words(model3, tokenizer, \"last case of\",2))\nprint(Predict_Words(model3, tokenizer, \"I'm really grateful\",4))\nprint(Predict_Words(model3, tokenizer, \"human creativity in\",2))\nprint(Predict_Words(model3, tokenizer, \"good morning my\",3))\nprint( Predict_Words(model3, tokenizer, \"I would have never\",2))","2a00fcbb":"print( Predict_Words(model3, tokenizer, \"I would have never\",22))","47c2ab9a":"model4 = Sequential()\nmodel4.add(Embedding(vocab_size, 10, input_length=2))\nmodel4.add(LSTM(1000, return_sequences=True))\nmodel4.add(Dropout(0.1))\nmodel4.add(LSTM(1000, return_sequences=True))\nmodel4.add(Dropout(0.1))\nmodel4.add(LSTM(800))\nmodel4.add(Dropout(0.1))\nmodel4.add(Dense(800, activation=\"relu\"))\nmodel4.add(Dense(600, activation=\"relu\"))\nmodel4.add(Dense(vocab_size, activation=\"softmax\"))\nmodel4.summary()","e9284ff6":"#checkpoint = ModelCheckpoint(\"next_words_5.h5\", monitor='loss', verbose=1, save_best_only=True)\nmodel4.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001),metrics=['accuracy'])\nmodel4.fit(X_train, y_train, epochs=70, batch_size=64,validation_data=(X_test,y_test))","c3490d5c":"losses4=pd.DataFrame(model4.history.history)\nlosses4.plot()","7d551acc":"losses4[[\"loss\",\"accuracy\"]].plot()","96c89a8c":"# Load the model and tokenizer\n#loaded_model = load_model('next_words_5.h5')\n#tokenizer = pickle.load(open('token.pkl', 'rb'))","c4428d90":"print(Predict_Words(model4, tokenizer, \"What surprised\",5))\nprint(Predict_Words(model4, tokenizer, \"have been three\",1))\nprint(Predict_Words(model4, tokenizer, \"last case of\",2))\nprint(Predict_Words(model4, tokenizer, \"I'm really grateful\",4))\nprint(Predict_Words(model4, tokenizer, \"human creativity in\",2))\nprint(Predict_Words(model4, tokenizer, \"good morning my\",3))\nprint( Predict_Words(model4, tokenizer, \"I would have never\",2))","00ad73ba":"print( Predict_Words(model4, tokenizer, \"I would have never\",22))","aa063358":"## preprocessing for Model","b73f12cc":"## prediction","670b6275":"## Model Predict","53b61afb":"## load model","eb800f8d":"### \u25b6 what are the most repeated occupations of speakers","e7ca1de4":"### \u25b6 Number of words in each talk","9a043929":"## load Model","e0d64451":"### \u25b6 Number of TED talks throughout the years","54fbf477":"## tokenizer","4822e707":"# Model 1","4395a18b":"another model take 3 words as input","b8e5638c":"## load model","c590237a":"# Train Test Split","dcc18793":"# split sequences ","1b03a67c":"# Model 2","1ba6657b":"## Load Model 1","fa380708":"## Text Generation Model","cad0a081":"# **EXPLORATORY DATA ANALYSIS**","86e90998":"# Model 3","a8610626":"# Model 4","44879b32":"## Predict Words","eaf5ae51":"### missing null values\nas there are only 6 null values in the occupation column we can fill it with the mode value of the column to keep the 2550 numbers even","733ab9be":"take 2 words as input","bdbb4157":"## Model","1e420ddd":"## convert data to text","676aaef0":"### \u25b6 Longest 10 talks","033f3db5":"another model take 2 words as input","b0d1b808":"# Make sequences","b64d1344":"### \u25b6 Checking the distribution of view, comments, and duration","7b49ebe3":"### \u25b6 top 10 TED events held","268f3cd5":"### \u25b6 relation between views and comments of talks","5ed0cfda":"## Model ","203ed70c":"this model takes 3 words as inputs","82ac1b49":"## text Cleaning","9d663310":"# Imoprt important libraries","fcd2e464":"As the data is huge..\nWe selected slice of text to train the model","91628b78":"### \u25b6 Number of TED talks throughout the years","ecc7893d":"## prediction"}}