{"cell_type":{"86a8cd7e":"code","934906fb":"code","d2e69e36":"code","931447d8":"code","767db04f":"code","47a0edfd":"code","3d6135e2":"markdown"},"source":{"86a8cd7e":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom transformers import (AutoModel,AutoModelForMaskedLM, \n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)","934906fb":"fold = 0","d2e69e36":"data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\n#data = train_data[train_data.kfold !=fold].reset_index(drop=True)\n#data = pd.concat([train_data,test_data])\ndata['excerpt'] = data['excerpt'].apply(lambda x: x.replace('\\n',''))\n\ntext  = '\\n'.join(data.excerpt.tolist())\n\nwith open('text.txt','w') as f:\n    f.write(text)","931447d8":"model_name = 'roberta-base'\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained('.\/clrp_roberta_base');","767db04f":"train_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\", #mention train text file here\n    block_size=256)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\", #mention valid text file here\n    block_size=256)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\ntraining_args = TrainingArguments(\n    output_dir=\".\/clrp_roberta_base_chk\", #select model path for checkpoint\n    overwrite_output_dir=True,\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy= 'steps',\n    save_total_limit=2,\n    eval_steps=200,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end =True,\n    prediction_loss_only=True,\n    report_to = \"none\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)","47a0edfd":"trainer.train()\ntrainer.save_model(f'.\/clrp_roberta_base')","3d6135e2":"This notebooks shows how to pretrain any language model easily\n\n\n1. Pretrain Roberta Model: this notebook\n2. Finetune Roberta Model: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune<br\/>\n   Finetune Roberta Model [TPU]: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune-tpu\n3. Inference Notebook: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-inference\n4. Roberta + SVM: https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm\n"}}