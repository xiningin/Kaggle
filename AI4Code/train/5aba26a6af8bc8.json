{"cell_type":{"a87d55f1":"code","363c4397":"code","e1ebb58f":"code","4d05232e":"code","377e2963":"code","11b4407a":"code","3c8ae275":"code","81f641ca":"code","2fc5fc04":"code","9f155d99":"code","0d9c90e0":"code","611f9291":"code","8cede30d":"code","1a90a342":"code","e60356f0":"code","c03615de":"code","a2622194":"code","3c42023a":"code","86cc0568":"code","909a4aaf":"code","fc6fd875":"code","5e723972":"code","4a5a1564":"code","7e7e3f2b":"code","5c550e5a":"code","3bfc086e":"code","7dc2ad93":"code","c668f4a8":"code","df3bbc24":"code","4f7796e2":"code","5367f568":"code","3e2c6e4b":"code","2b4984aa":"code","8c2c1141":"code","cf4d723b":"code","b724f9f1":"code","4bd5c034":"code","d9e8118a":"code","7538caf3":"code","b03060bd":"code","8a3f724f":"code","deee78c9":"code","c6b90f59":"code","ba3ee888":"code","0e4e5703":"code","4b5dacc5":"code","5a7eacab":"code","cb801d41":"code","c3811fee":"code","31be0084":"code","f01bdde5":"code","a8ab6edc":"code","b04ba8c0":"code","83a6b3b9":"code","c7b182d1":"code","c52aa432":"code","eaecbd05":"code","9e43dc55":"code","575d32bc":"code","f45f37ee":"code","e4c20fcb":"code","f80c618f":"code","e59163ef":"code","55b21d2b":"code","b21b2348":"code","c0d57940":"code","15c19770":"code","6bb9a94d":"code","e532fddf":"code","086d5e3e":"code","bdc16dd7":"code","7a56b0e8":"code","9b97afcf":"code","937359d5":"code","e355a0cf":"code","a5d99abb":"code","285d56d1":"code","5d56b856":"code","050bc979":"code","d17303fc":"code","74af8c0a":"code","cde7c13b":"code","2ed7d44a":"code","f0af9082":"code","609f108a":"code","c2c73d48":"code","4fb3600f":"code","706cb135":"code","8c4c8d3c":"code","4d98fcec":"code","2db9662c":"code","2e440cb5":"code","edb631d1":"code","dc901c04":"code","69a1d70c":"code","fab829fa":"code","abdc0639":"markdown","035b6c5a":"markdown","4a019cd7":"markdown","13c41fbf":"markdown","a3320f48":"markdown","90cd5fd1":"markdown","c5cf10e1":"markdown","de46bb47":"markdown","7f8367ba":"markdown","cb9bca75":"markdown","903bf5df":"markdown","11fc882a":"markdown","6e0aa682":"markdown","b69e670c":"markdown","7d598244":"markdown","f40681b1":"markdown","aa9b0f03":"markdown","36c355bb":"markdown","a6ffcf2e":"markdown","b485aadd":"markdown","b481b5d3":"markdown","411439c6":"markdown","253c1c11":"markdown","263e0fdd":"markdown","efb28eb2":"markdown","1f09ec3c":"markdown","e0b42ad9":"markdown","72215210":"markdown","9eed63bb":"markdown","b5cf58c2":"markdown","0fa9c5c2":"markdown","b9ea3823":"markdown","1f306f54":"markdown","a9e24838":"markdown","80929249":"markdown","027373d8":"markdown","bc4ccaf7":"markdown","05ff01c4":"markdown","d5afa5cb":"markdown","3e014dc1":"markdown","e49461a9":"markdown","670db6f6":"markdown","61a5ba32":"markdown","f112bbfd":"markdown","cd60c7e8":"markdown"},"source":{"a87d55f1":"import pandas as pd\nimport numpy as np\nimport pandas_profiling\nimport seaborn as sns\n\n\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","363c4397":"train.head(5)","e1ebb58f":"train.shape","4d05232e":"test.head()","377e2963":"train.shape","11b4407a":"test.shape","3c8ae275":"train.info()","81f641ca":"test.info()","2fc5fc04":"sns.heatmap(train.isnull())","9f155d99":"train.isnull().sum()","0d9c90e0":"sns.heatmap(test.isnull())","611f9291":"test.isnull().sum()","8cede30d":"pandas_profiling.ProfileReport(train)","1a90a342":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set() # setting seaborn default for plots","e60356f0":"def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))","c03615de":"bar_chart('Sex')","a2622194":"bar_chart('Pclass')","3c42023a":"bar_chart('SibSp')","86cc0568":"bar_chart('Parch')","909a4aaf":"bar_chart('Embarked')","fc6fd875":"train.head()","5e723972":"train_test_data = [train, test] # combining train and test dataset\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","4a5a1564":"train['Title'].value_counts()","7e7e3f2b":"test['Title'].value_counts()","5c550e5a":"title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n                 \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)","3bfc086e":"train.head()","7dc2ad93":"test.head()","c668f4a8":"bar_chart('Title')","df3bbc24":"# delete unnecessary feature from dataset\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","4f7796e2":"train.head()","5367f568":"test.head()","3e2c6e4b":"sex_mapping = {\"male\": 0, \"female\": 1}\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)","2b4984aa":"bar_chart('Sex')","8c2c1141":"for dataset in train_test_data:\n    dataset.Embarked[ dataset.Embarked.isnull() ] = dataset.Embarked.dropna().mode().values","cf4d723b":"# fill missing Fare with median fare for each Pclass\ntrain[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntest[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntrain.head()","b724f9f1":"for dataset in train_test_data:\n    print(dataset.Cabin.value_counts())","4bd5c034":"for dataset in train_test_data:\n    dataset['Cabin'] = dataset['Cabin'].str[:1]","d9e8118a":"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\nPclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\nPclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))","7538caf3":"cabin_mapping = {\"A\": 0, \"B\": 0.4, \"C\": 0.8, \"D\": 1.2, \"E\": 1.6, \"F\": 2, \"G\": 2.4, \"T\": 2.8}\nfor dataset in train_test_data:\n    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)","b03060bd":"# fill missing Fare with median fare for each Pclass\ntrain[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\ntest[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)","8a3f724f":"train.head()","deee78c9":"for dataset in train_test_data:\n    dataset.drop(['Ticket'], axis = 1, inplace = True)","c6b90f59":"train.head()","ba3ee888":"test.head()","0e4e5703":"from sklearn.ensemble import RandomForestRegressor\n\n### Populate missing ages using RandomForestClassifier\ndef setMissingAges(df):\n    # Grab all the features that can be included in a Random Forest Regressor\n    age_df = df[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Title']]\n    # Split into sets with known and unknown Age values\n    knownAge = age_df.loc[ (df.Age.notnull()) ]\n    unknownAge = age_df.loc[ (df.Age.isnull()) ]\n\n    # All age values are stored in a target array\n    y = knownAge.values[:, 0]\n\n    # All the other values are stored in the feature array\n    X = knownAge.values[:, 1::]\n\n    # Create and fit a model\n    rtr = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\n    rtr.fit(X, y)\n\n    # Use the fitted model to predict the missing values\n    predictedAges = rtr.predict(unknownAge.values[:, 1::])\n\n    # Assign those predictions to the full data set\n    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges\n\n    return df","4b5dacc5":"for dataset in train_test_data:\n    setMissingAges(dataset)","5a7eacab":"train.isnull().sum()","cb801d41":"test.isnull().sum()","c3811fee":"train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1","31be0084":"family_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\nfor dataset in train_test_data:\n    dataset['FamilySize'] = dataset['FamilySize'].map(family_mapping)","f01bdde5":"from category_encoders import TargetEncoder\nfrom sklearn.preprocessing import LabelEncoder\n","a8ab6edc":"train","b04ba8c0":"train['Fare'] = pd.qcut(train['Fare'], 6)\ntest['Fare'] = pd.qcut(test['Fare'], 6)\n    \ntrain['Fare'] = LabelEncoder().fit_transform(train['Fare'])\ntest['Fare'] = LabelEncoder().fit_transform(test['Fare'])\n","83a6b3b9":"train['Age'] = pd.cut(train.Age, bins=[0,16,26,62, np.inf])\ntest['Age'] = pd.cut(test.Age, bins=[0,17, 30,100, np.inf])\n    \ntrain['Age'] = LabelEncoder().fit_transform(train['Age'])\ntest['Age'] = LabelEncoder().fit_transform(test['Age'])","c7b182d1":"test.head()","c52aa432":"dummy_df = pd.get_dummies(train.Pclass, drop_first = True)\ndummy_df = dummy_df.rename(columns=lambda x: 'Pclass_' + str(x))\ntrain = pd.concat([train, dummy_df], axis=1)","eaecbd05":"dummy_df = pd.get_dummies(test.Pclass, drop_first = True)\ndummy_df = dummy_df.rename(columns=lambda x: 'Pclass_' + str(x))\ntest = pd.concat([test, dummy_df], axis=1)","9e43dc55":"dummy_df = pd.get_dummies(train.Embarked, drop_first = True)\ndummy_df = dummy_df.rename(columns=lambda x: 'Embarked_' + str(x))\ntrain = pd.concat([train, dummy_df], axis=1)","575d32bc":"dummy_df = pd.get_dummies(test.Embarked, drop_first = True)\ndummy_df = dummy_df.rename(columns=lambda x: 'Embarked_' + str(x))\ntest = pd.concat([test, dummy_df], axis=1)","f45f37ee":"dummy_df = pd.get_dummies(train.Title, drop_first = True)\ndummy_df = dummy_df.rename(columns=lambda x: 'Title_' + str(x))\ntrain = pd.concat([train, dummy_df], axis=1)","e4c20fcb":"dummy_df = pd.get_dummies(test.Title, drop_first = True)\ndummy_df = dummy_df.rename(columns=lambda x: 'Title_' + str(x))\ntest = pd.concat([test, dummy_df], axis=1)","f80c618f":"train.head()","e59163ef":"test.head()","55b21d2b":"features_drop = ['SibSp', 'Parch', 'Pclass', 'Embarked', 'Title']\ntrain = train.drop(features_drop, axis=1)\ntest = test.drop(features_drop, axis=1)\ntrain = train.drop(['PassengerId'], axis=1)","b21b2348":"train_data = train.drop('Survived', axis=1)\ntarget = train['Survived']\n\ntrain_data.shape, target.shape","c0d57940":"test","15c19770":"train_data.head(10)","6bb9a94d":"test_data = test.drop('PassengerId', axis=1)\npassenger = test['PassengerId']","e532fddf":"test_data\n","086d5e3e":"passenger","bdc16dd7":"# Importing Classifier Modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nimport numpy as np","7a56b0e8":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","9b97afcf":"clf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","937359d5":"# kNN Score\nround(np.mean(score)*100, 2)","e355a0cf":"clf.fit(train_data, target)","a5d99abb":"y_preds = clf.predict(test_data)\nsubmission = pd.DataFrame({'PassengerId':passenger, \n              'Survived':y_preds})\nsubmission.to_csv('submission.csv', index=False)\npd.read_csv('submission.csv')","285d56d1":"clf1 = DecisionTreeClassifier()\nscoring = 'accuracy'\nscore = cross_val_score(clf1, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","5d56b856":"# decision tree Score\nround(np.mean(score)*100, 2)","050bc979":"clf1.fit(train_data, target)\ny_preds1 = clf1.predict(test_data)\nsubmission1 = pd.DataFrame({'PassengerId':passenger, \n              'Survived':y_preds1})\nsubmission1.to_csv('submission1.csv', index=False)\npd.read_csv('submission1.csv')","d17303fc":"clf2 = RandomForestClassifier(n_estimators=12)\nscoring = 'accuracy'\nscore = cross_val_score(clf2, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\n","74af8c0a":"# Random Forest Score\nround(np.mean(score)*100, 2)","cde7c13b":"clf2.fit(train_data, target)\ny_preds2 = clf2.predict(test_data)\nsubmission2 = pd.DataFrame({'PassengerId':passenger, \n              'Survived':y_preds2})\nsubmission2.to_csv('submission2.csv', index=False)\npd.read_csv('submission2.csv')","2ed7d44a":"clf3 = GaussianNB()\nscoring = 'accuracy'\nscore = cross_val_score(clf3, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","f0af9082":"# Naive Bayes Score\nround(np.mean(score)*100, 2)","609f108a":"clf3.fit(train_data, target)\ny_preds3 = clf3.predict(test_data)\nsubmission3 = pd.DataFrame({'PassengerId':passenger, \n              'Survived':y_preds3})\nsubmission3.to_csv('submission3.csv', index=False)\npd.read_csv('submission3.csv')","c2c73d48":"clf4 = SVC()\nscoring = 'accuracy'\nscore = cross_val_score(clf4, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","4fb3600f":"round(np.mean(score)*100,2)","706cb135":"clf4.fit(train_data, target)\ny_preds4 = clf4.predict(test_data)\nsubmission4 = pd.DataFrame({'PassengerId':passenger, \n              'Survived':y_preds4})\nsubmission4.to_csv('submission4.csv', index=False)\npd.read_csv('submission4.csv')","8c4c8d3c":"from sklearn.linear_model import LogisticRegression\nclf5 = LogisticRegression(solver = 'newton-cg', random_state = 0)\nscoring = 'accuracy'\nscore = cross_val_score(clf5, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","4d98fcec":"round(np.mean(score)*100,2)","2db9662c":"clf5.fit(train_data,target)\ny_preds5 = clf5.predict(test_data)\nsubmission5 = pd.DataFrame({'PassengerId':passenger, \n              'Survived':y_preds5})\nsubmission5.to_csv('submission5.csv', index=False)\npd.read_csv('submission5.csv')","2e440cb5":"from keras.models import Sequential\nfrom keras.layers import Dense\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=12, activation='relu'))\nmodel.add(Dense(12, input_dim=12, activation='relu'))\nmodel.add(Dense(3, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","edb631d1":"#compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'mean_absolute_error'])","dc901c04":"\nmodel.summary()","69a1d70c":"\n#fit the model\nmodel.fit(train_data, target, epochs=500, batch_size=50)","fab829fa":"scores = model.evaluate(train_data,target)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","abdc0639":"# Model Analysis\nI just created a deep learning CNN model just for knowledge that how DL model performs on this small dataset\n\nSVM is giving us the max Score.\n\nKNN and Linear Regression score is above 81 in train data\n\nDecison Tree And Random forest also worked good\n\nNB doesn't perform well\n","035b6c5a":"## 4. Feature Engineering\n\nThe features you use influence more than everything else the result. No algorithm alone, to my knowledge, can supplement the information gain given by correct feature engineering.\n\n\u2014 Luca Massaron\n\n### 4.1 Name","4a019cd7":"# Titanic EDA --Let's compare diffrerent Models\nThis kernel serves as a beginner tutorial and basic guideline for approaching the Exploratory data analysis. I decided to write this kernel because Titanic: Machine Learning from Disaster is one of my favorite competitions on Kaggle. This is a beginner level kernel which focuses on Exploratory Data Analysis,Feature Engineering and Modelling. A lot of people start Kaggle with this competition and they get lost in extremely long tutorial kernels. This is a short kernel compared to the other ones. I hope this will be a good guide for starters and inspire them with new feature engineering ideas.\n\nExploratory Data Analysis(EDA): 1)Analysis of the features. 2)Finding any relations or trends considering multiple features.\n\nFeature Engineering-How to make new features\n\nModelling-I have used Logistic Regression,Support Vector Machine,K-Nearest Neighbour,Naive Bayes,Decison Tree,Random Forest\n\nInthis kernel let's see how all the model behave on same dataset.\n\nThanks a lot for having a look at this notebook. If you found this notebook useful, Do Upvote.","13c41fbf":"# OBSERVATIONS\nAge,Cabin have null values that must be treated.\n\nOut of 891 passengers in training set, only around 350 survived i.e Only 38.4% of the total training set survived the crash.\n\nWomen more likely survivied than Men\n\nThe analysis for outliers show that Fare, Embarked and Parch column have some outliers. Fare and Survive has best correlation.Variable are not very much correlated so we can use them all.\n\nThe chances for survival for Port C is highest around 0.55 while it is lowest for S.\n\nIt is an important feature as it reveals that passengers with family size 2 - 4 had a better survival rate than passengers travelling alone or who had larger families.\n\nPerson aboarded from C slightly more likely survived,Q more likely dead,S more likely dead.\n\n1st class more likely survivied than other classes 3rd class more likely dead than other classes","a3320f48":"# Thanks a lot for having a look at this notebook. If you found this notebook useful, Do Upvote.\nIf you have forked the kernel and not upvoted yet, then show the support by upvoting :)\n\nPlease leave you constructive criticism and suggestion in comments below!!","90cd5fd1":"### Removing Extra features","c5cf10e1":"We can see that *Age* value is missing for many rows. \n\nOut of 891 rows, the *Age* value is present only in 714 rows.\n\nSimilarly, *Cabin* values are also missing in many rows. Only 204 out of 891 rows have *Cabin* values.","de46bb47":"The Chart confirms **a person aboarded with more than 2 parents or children** more likely survived  \nThe Chart confirms ** a person aboarded alone** more likely dead","7f8367ba":"There are 86 rows with missing *Age*, 327 rows with missing *Cabin* and 2 rows with missing *Embarked* information.","cb9bca75":"### 7.3 Title","903bf5df":"### 4.3.2 Fare","11fc882a":"### Naive Bayes","6e0aa682":"#### feature vector map:  \nchild: 0  \nyoung: 1  \nadult: 2  \nmid-age: 3  \nsenior: 4","b69e670c":"## 2. Exploratory data analysis\nPrinting first 5 rows of the train dataset.","7d598244":"The Chart confirms **Women** more likely survivied than **Men**","f40681b1":"The outputs of prediction and feature engineering are a set of label times, historical examples of what we want to predict, and features, predictor variables used to train a model to predict the label. The process of modeling means training a machine learning algorithm to predict the labels from the features, tuning it for the challenge need, and validating it on holdout data.\n","aa9b0f03":"### import python lib for visualization","36c355bb":"## 3. Feature engineering\n\nFeature engineering is the process of using domain knowledge of the data  \nto create features (**feature vectors**) that make machine learning algorithms work.  \n\nfeature vector is an n-dimensional vector of numerical features that represent some object.  \nMany algorithms in machine learning require a numerical representation of objects,  \nsince such representations facilitate processing and statistical analysis.","a6ffcf2e":"# Logistic Regression","b485aadd":"# Modelling","b481b5d3":"## Drop Ticket Column","411439c6":"## 6.Scaling Age and Fare","253c1c11":"Roughly 20 percent of the Age data is missing. The proportion of Age missing is likely small enough for reasonable replacement with some form of imputation. Looking at the Cabin column, it looks like we are just missing too much of that data to do something useful with at a basic level. We'll probably drop this later, or change it to another feature like \"Cabin Known: 1 or 0\"","263e0fdd":"## 7. Category Embarked and Pclass Title Dummy Variables\n### 7.1 Pclass","efb28eb2":"### Bar Chart for Categorical Features\n- Pclass\n- Sex\n- SibSp ( # of siblings and spouse)\n- Parch ( # of parents and children)\n- Embarked\n- Cabin","1f09ec3c":"### 4.2 Sex\n\nmale: 0\nfemale: 1","e0b42ad9":"### Ramdom Forest","72215210":"### SVM","9eed63bb":"### KNN","b5cf58c2":"The Chart confirms **1st class** more likely survivied than **other classes**  \nThe Chart confirms **3rd class** more likely dead than **other classes**","0fa9c5c2":"### 4.3.3 Cabin","b9ea3823":"#### Title map\nMr : 0  \nMiss : 1  \nMrs: 2  \nOthers: 3\n","1f306f54":"**Total rows and columns**\n\nWe can see that there are 891 rows and 12 columns in our training dataset.","a9e24838":"# Understanding data using single line","80929249":"## 4.3 Filling Missing Values\n### 4.3.1 Embarked","027373d8":"The Chart confirms **a person aboarded from C** slightly more likely survived  \nThe Chart confirms **a person aboarded from Q** more likely dead  \nThe Chart confirms **a person aboarded from S** more likely dead","bc4ccaf7":"### Decision Tree","05ff01c4":"### 4.3.4 Age","d5afa5cb":"### Data Dictionary\n\nVariable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","3e014dc1":"# NN with 3 Layers","e49461a9":"Whenever you want to get started on a problem like regression and classification using machine learning just use ProfileReport it will give us initial insights that will be very useful to understand the data provided.Let's see what we have got..","670db6f6":"The Chart confirms **a person aboarded with more than 2 siblings or spouse** more likely survived  \nThe Chart confirms ** a person aboarded without siblings or spouse** more likely dead","61a5ba32":"## 5. Derived Variable\n### 5.1 FamilySize","f112bbfd":"### Cross Validation (K-fold)","cd60c7e8":"### 7.2 Embarked"}}