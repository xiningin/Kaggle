{"cell_type":{"df60f8c5":"code","5589fa88":"code","f4fedfd6":"code","91e53c74":"code","654d0fa8":"code","aba23bfe":"code","ee8da066":"code","c3437e29":"code","c1d041ac":"code","9db2ecba":"markdown","37ace0ff":"markdown","a507dfcd":"markdown","d0f272c5":"markdown","076473b3":"markdown","38ed2da2":"markdown","588b38fe":"markdown","c59be1d5":"markdown"},"source":{"df60f8c5":"#importing important libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\n\n# Importing Deep Learning Libraries\n\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\nfrom keras.models import Model,Sequential\nfrom tensorflow.keras.optimizers import Adam,SGD,RMSprop","5589fa88":"#taking inputs images for training\npicture_size = 48\nfolder_path = \"..\/input\/face-expression-recognition-dataset\/images\/\"","f4fedfd6":"#checking disgust image\nexpression = 'disgust'\n\nplt.figure(figsize= (12,12))\nfor i in range(1, 10, 1):\n    plt.subplot(3,3,i)\n    img = load_img(folder_path+\"train\/\"+expression+\"\/\"+\n                  os.listdir(folder_path + \"train\/\" + expression)[i], target_size=(picture_size, picture_size))\n    plt.imshow(img)   \nplt.show()","91e53c74":"#splitting data into train, test and validation set\nbatch_size  = 128\n\ndatagen_train  = ImageDataGenerator()\ndatagen_val = ImageDataGenerator()\n\ntrain_set = datagen_train.flow_from_directory(folder_path+\"train\",\n                                              target_size = (picture_size,picture_size),\n                                              color_mode = \"grayscale\",\n                                              batch_size=batch_size,\n                                              class_mode='categorical',\n                                              shuffle=True)\n\n\ntest_set = datagen_val.flow_from_directory(folder_path+\"validation\",\n                                              target_size = (picture_size,picture_size),\n                                              color_mode = \"grayscale\",\n                                              batch_size=batch_size,\n                                              class_mode='categorical',\n                                              shuffle=False)","654d0fa8":"#building model with 7 classes\nfrom tensorflow.keras.optimizers import Adam,SGD,RMSprop\n\n\nno_of_classes = 7\n\nmodel = Sequential()\n\n#1st CNN layer\nmodel.add(Conv2D(64,(3,3),padding = 'same',input_shape = (48,48,1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout(0.25))\n\n#2nd CNN layer\nmodel.add(Conv2D(128,(5,5),padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout (0.25))\n\n#3rd CNN layer\nmodel.add(Conv2D(512,(3,3),padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout (0.25))\n\n#4th CNN layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n\n#Fully connected 1st layer\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(no_of_classes, activation='softmax'))\n\n\n\nopt = Adam(lr = 0.0001)\nmodel.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","aba23bfe":"#visualizing the model\nimport tensorflow as tf\ntf.keras.utils.plot_model(\n    model,\n    to_file=\"model.png\",\n    show_shapes=False,\n    show_dtype=False,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n    layer_range=None,\n)","ee8da066":"#importing tensorflow library and package\nfrom tensorflow.keras.optimizers import RMSprop,SGD,Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\ncheckpoint = ModelCheckpoint(\".\/model.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n#Stopping training when a monitored metric has stopped improving.\nearly_stopping = EarlyStopping(monitor='val_loss',\n                          min_delta=0,\n                          patience=3,\n                          verbose=1,\n                          restore_best_weights=True\n                          )\n\nreduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.2,\n                              patience=3,\n                              verbose=1,\n                              min_delta=0.0001)\n\ncallbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n\nepochs = 50\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer = Adam(lr=0.001),\n              metrics=['accuracy'])","c3437e29":"#fitting model with 48 epoch\nhistory = model.fit_generator(generator=train_set,\n                                steps_per_epoch=train_set.n\/\/train_set.batch_size,\n                                epochs=epochs,\n                                validation_data = test_set,\n                                validation_steps = test_set.n\/\/test_set.batch_size,\n                                callbacks=callbacks_list\n                                )","c1d041ac":"#plotting graph to check accuracy and loss\nplt.style.use('dark_background')\n\nplt.figure(figsize=(20,10))\nplt.subplot(1, 2, 1)\nplt.suptitle('Optimizer : Adam', fontsize=10)\nplt.ylabel('Loss', fontsize=16)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend(loc='upper right')\n\nplt.subplot(1, 2, 2)\nplt.ylabel('Accuracy', fontsize=16)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.show()","9db2ecba":"# Visualize model\n\nThe plot_model() function in Keras will create a plot of your network. This function takes a few useful arguments:\n\n* model: (required) The model that you wish to plot.\n* to_file: (required) The name of the file to which to save the plot.\n* show_shapes: (optional, defaults to False) Whether or not to show the output shapes of each layer.\n* show_layer_names: (optional, defaults to True) Whether or not to show the name for each lay","37ace0ff":"# Model Building\n","a507dfcd":"# Displaying Images\n","d0f272c5":"# Fitting the Model with Training and Validation Data\n","076473b3":"* Model = sequential : A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n\n* Activation = relu :The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.\n\n* padding = The padding parameter of the Keras Conv2D class can take one of two values: 'valid' or 'same'.\nSetting the value to \u201cvalid\u201d parameter means that the input volume is not zero-padded and the spatial dimensions are allowed to reduce via the natural application of convolution.\n\n* Maxpooling = Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map. The results are down sampled or pooled feature maps that highlight the most present feature in the patch, not the average presence of the feature in the case of average pooling.\n\n* Batch normalization = Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n\n* Dropout = Dropout is a technique used to prevent a model from overfitting. Dropout works by randomly setting the outgoing edges of hidden units (neurons that make up hidden layers) to 0 at each update of the training phase.\n\n* Adam = Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum.\n\n* SGD = Stochastic Gradient Descent (SGD) addresses both of these issues by following the negative gradient of the objective after seeing only a single or a few training examples. The use of SGD In the neural network setting is motivated by the high cost of running back propagation over the full training set\n\n* RMSprop = RMSprop is a gradient based optimization technique used in training neural networks. ... This normalization balances the step size (momentum), decreasing the step for large gradients to avoid exploding, and increasing the step for small gradients to avoid vanishing.","38ed2da2":"# Making Training and Validation Data\n","588b38fe":"# Problem Statement\nProject Introduction : \nThe Indian education landscape has been undergoing rapid changes for the past 10 years owing to\nthe advancement of web-based learning services, specifically, eLearning platforms.\nGlobal E-learning is estimated to witness an 8X over the next 5 years to reach USD 2B in 2021. India\nis expected to grow with a CAGR of 44% crossing the 10M users mark in 2021. Although the market\nis growing on a rapid scale, there are major challenges associated with digital learning when\ncompared with brick and mortar classrooms. One of many challenges is how to ensure quality\nlearning for students. Digital platforms might overpower physical classrooms in terms of content\nquality but when it comes to understanding whether students are able to grasp the content in a live\nclass scenario is yet an open-end challenge.\nIn a physical classroom during a lecturing teacher can see the faces and assess the emotion of the\nclass and tune their lecture accordingly, whether he is going fast or slow. He can identify students who\nneed special attention. Digital classrooms are conducted via video telephony software program (exZoom) where it\u2019s not possible for medium scale class (25-50) to see all students and access the\nmood. Because of this drawback, students are not focusing on content due to lack of surveillance.\nWhile digital platforms have limitations in terms of physical surveillance but it comes with the power of\ndata and machines which can work for you. It provides data in the form of video, audio, and texts\nwhich can be analysed using deep learning algorithms. Deep learning backed system not only solves\nthe surveillance issue, but it also removes the human bias from the system, and all information is no\nlonger in the teacher\u2019s brain rather translated in numbers that can be analysed and tracked.\n\n\nWe will solve the above-mentioned challenge by applying deep learning algorithms to live video data.\nThe solution to this problem is by recognizing facial emotions.\n## Face Emotion Recognition\nThis is a few shot learning live face emotion detection system. The model should be able to real-time\nidentify the emotions of students in a live class.","c59be1d5":"# Plotting Accuracy & Loss\n"}}