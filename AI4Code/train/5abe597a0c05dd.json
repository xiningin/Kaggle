{"cell_type":{"13e55e21":"code","de17270b":"code","5cdb34a0":"code","28807edf":"code","073c9152":"code","2e2f4ae7":"code","19b9b379":"code","562f80b1":"code","fc4e0bea":"code","5e290ba4":"code","c21cb576":"code","137bfd8a":"code","f23e9d25":"code","00bf38de":"code","63ff73d7":"code","a9d7539e":"code","f591b210":"code","2da5bbad":"code","f30b24cd":"code","fb948085":"code","e290a128":"code","3b6fc5e7":"code","66762500":"code","04cd6984":"code","3280971f":"code","ad9593b2":"code","30cdbcf1":"code","49c6cf48":"code","67cb94a9":"code","cf220331":"code","3750e382":"code","d9c45f2a":"code","bce3bd24":"code","80ea8a54":"code","92c265d8":"code","4ef4829c":"code","7836b58f":"code","837fae1a":"code","bf7cdc2d":"code","76b30c03":"code","40d29a7b":"code","a8c6aa88":"markdown","de122f0f":"markdown","d76de649":"markdown","27c31120":"markdown","f39954c4":"markdown","fbeb19fe":"markdown","e03b98d8":"markdown","acaf8b69":"markdown","7e8f4f1d":"markdown","e00d1eb8":"markdown","1b10341f":"markdown","5ab42fe7":"markdown","38b89c5d":"markdown","08efe9e3":"markdown","af4b8eba":"markdown","4155417b":"markdown","fa1aea17":"markdown","76520137":"markdown","270d56ba":"markdown"},"source":{"13e55e21":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom operator import itemgetter\nfrom collections import OrderedDict\nimport os\nimport torch \nimport torch.nn.functional as F\nfrom PIL import Image\nfrom torch import optim,nn\nfrom torchvision import transforms as T,models\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision.utils import make_grid\nfrom tqdm import tqdm \n\n\npd.options.plotting.backend = \"plotly\"\n\n\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.figure_factory as ff\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\nimport plotly.graph_objs as go\n#import plotly.plotly as py\nimport plotly\nfrom plotly import tools\n#plotly.tools.set_credentials_file(username='XXX', api_key='XXX')\n#init_notebook_mode(connected=True)\n#pd.set_option('display.max_columns', 100)","de17270b":"data = pd.read_csv('..\/input\/sample\/sample_labels.csv')\ndata.head()","5cdb34a0":"data['Patient Gender'].value_counts().iplot(kind='bar', title='Bar plot of Gender Patient')","28807edf":"data['Patient Age'].apply(lambda x : int(x[1:3])).iplot(\n    kind='hist',\n    bins=100,\n    xTitle='value',\n    linecolor='black',\n    yTitle='count',\n    title='Histogram of Patient Age')","073c9152":"data['Patient Age'].apply(lambda x : int(x[1:3])).iplot(kind='box', title='Box plot of Patient Age')","2e2f4ae7":"pathology_list = ['Cardiomegaly','Emphysema','Effusion','Hernia','Nodule','Pneumothorax','Atelectasis','Pleural_Thickening','Mass','Edema','Consolidation','Infiltration','Fibrosis','Pneumonia']\n\nfor pathology in pathology_list :\n    data[pathology] = data['Finding Labels'].apply(lambda x: 1 if pathology in x else 0)\n    \ndata['No Findings'] = data['Finding Labels'].apply(lambda x: 1 if 'No Finding' in x else 0)","19b9b379":"data = data.drop(list(data.iloc[:,1:11].columns.values),axis = 1)","562f80b1":"data","fc4e0bea":"data.iloc[:,1:].sum().iplot(kind='bar', orientation='h')","5e290ba4":"data = data.drop(['No Findings'],axis = 1)","c21cb576":"data.iloc[:,1:].sum().iplot(kind='bar', orientation='h')","137bfd8a":"data.iloc[:,1:].mean().iplot(kind='bar', orientation='h')","f23e9d25":"df = pd.DataFrame({\"Class\": pathology_list, \"Label\": \"Positive\", \"Value\": freq_pos})\ndf = df.append([{\"Class\": pathology_list[l], \"Label\": \"Negative\", \"Value\": v} for l,v in enumerate(freq_neg)], ignore_index=True)\n\nplt.xticks(rotation=90)\nf = sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=df, palette=\"pastel\", alpha=.6)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","00bf38de":"def compute_class_freqs(labels):\n    \n    labels = np.array(labels)\n    \n    N = labels.shape[0]\n    \n    positive_frequencies = np.sum(labels,axis = 0) \/ N\n    negative_frequencies = 1 - positive_frequencies\n    \n    return positive_frequencies, negative_frequencies","63ff73d7":"freq_pos, freq_neg = compute_class_freqs(data.iloc[:,1:])","a9d7539e":"df = pd.DataFrame({\"Class\": pathology_list, \"Label\": \"Positive\", \"Value\": freq_pos})\ndf = df.append([{\"Class\": pathology_list[l], \"Label\": \"Negative\", \"Value\": v} for l,v in enumerate(freq_neg)], ignore_index=True)\n\nplt.xticks(rotation=90)\nf = sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=df)","f591b210":"pos_weights = freq_neg\nneg_weights = freq_pos\npos_contribution = freq_pos * pos_weights \nneg_contribution = freq_neg * neg_weights","2da5bbad":"df = pd.DataFrame({\"Class\": pathology_list, \"Label\": \"Positive\", \"Value\": pos_contribution})\ndf = df.append([{\"Class\": pathology_list[l], \"Label\": \"Negative\", \"Value\": v} for l,v in enumerate(neg_contribution)], ignore_index=True)\n\nplt.xticks(rotation=90)\nf = sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=df)","f30b24cd":"def weighted_loss(pos_weights,neg_weights,y_pred,y_true,epsilon = 1e-7):\n    \n    loss = 0.0\n    for i in range(len(pos_weights)):\n        loss_pos = -1 * torch.mean(pos_weights[i] * y_true[:,i] * torch.log(y_pred[:,i] + epsilon))\n        loss_neg = -1 * torch.mean(neg_weights[i] * (1-y_true[:,i]) * torch.log((1-y_pred[:,i]) + epsilon))\n        loss += loss_pos + loss_neg\n        \n    return loss","fb948085":"class NIH_Dataset(Dataset):\n    \n    def __init__(self,data,img_dir,transform = None):\n        self.data = data\n        self.img_dir = img_dir \n        self.transform = transform \n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self,idx):\n        \n        img_file = self.img_dir + self.data.iloc[:,0][idx]\n        img = Image.open(img_file).convert('RGB')\n        label = np.array(self.data.iloc[:,1:].iloc[idx])\n        \n        if self.transform:\n            img = self.transform(img)\n            \n        return img,label","e290a128":"data_transform = T.Compose([\n    T.RandomRotation((-20,+20)),\n    T.Resize((224,224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.480, 0.450, 0.400],std=[0.220, 0.220, 0.220])\n])","3b6fc5e7":"trainds = NIH_Dataset(data,'..\/input\/sample\/sample\/sample\/images\/',transform = data_transform)","66762500":"def deprocess(img):\n    img = img.permute(1,2,0)\n    img = img * torch.Tensor([0.229, 0.224, 0.225]) + torch.Tensor([0.485, 0.456, 0.406])\n    return img","04cd6984":"image, label = trainds[0]\nclass_labels = list(np.where(label==1)[0])\nplt.imshow(deprocess(image))\nplt.title(itemgetter(*class_labels)(pathology_list));","3280971f":"print(len(data))","ad9593b2":"trainset, validset, testset = random_split(trainds, [5000,303,303])\n\nprint(\"Length of trainset : {}\".format(len(trainset)))\nprint(\"Length of testset : {}\".format(len(testset)))\nprint(\"Length of validset : {}\".format(len(validset)))","30cdbcf1":"def get_dataloader(dataset):\n    return DataLoader(dataset,batch_size = 32,shuffle = True)","49c6cf48":"trainloader, testloader, validloader = get_dataloader(trainset),get_dataloader(testset),get_dataloader(validset)","67cb94a9":"model = models.vgg19_bn()\nmodel.load_state_dict(torch.load(\"..\/input\/pretrained-model-weights-pytorch\/vgg19_bn-c79401a0.pth\"))","cf220331":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","3750e382":"for param in model.parameters():\n    param.requires_grad = False\n\n\n\n\nclassifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(25088, 4096)),\n                                         ('relu', nn.ReLU()),\n                                         ('dropout',nn.Dropout(0.3)),\n                                         ('fc2', nn.Linear(4096, 4096)),\n                                         ('relu', nn.ReLU()),\n                                         ('drop', nn.Dropout(0.3)),\n                                         ('fc3', nn.Linear(4096, 14)), \n                                         ('output', nn.Sigmoid())]))\n\nmodel.classifier = classifier\nmodel.to(device)","d9c45f2a":"optimizer = optim.Adam(model.parameters(),lr = 0.001)\nschedular = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.1,patience = 5)\nepochs = 15\nvalid_loss_min = np.Inf","bce3bd24":"for i in range(epochs):\n    \n    train_loss = 0.0\n    valid_loss = 0.0\n    train_acc = 0.0\n    valid_acc = 0.0 \n    \n    \n    model.train()\n    \n    for images,labels in tqdm(trainloader):\n        \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        ps = model(images)\n        loss = weighted_loss(pos_weights,neg_weights,ps,labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        \n    avg_train_loss = train_loss \/ len(trainloader)\n        \n    model.eval()\n    with torch.no_grad():\n        \n        for images,labels in tqdm(validloader):\n            \n            images = images.to(device)\n            labels = labels.to(device)\n            \n            ps = model(images)\n            loss = weighted_loss(pos_weights,neg_weights,ps,labels)\n            \n            valid_loss += loss.item()\n            \n            \n        avg_valid_loss = valid_loss \/ len(validloader)\n        \n        schedular.step(avg_valid_loss)\n        \n        if avg_valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).   Saving model ...'.format(valid_loss_min,avg_valid_loss))\n            torch.save({\n                'epoch' : i,\n                'model_state_dict' : model.state_dict(),\n                'optimizer_state_dict' : optimizer.state_dict(),\n                'valid_loss_min' : avg_valid_loss\n            },'Pneumonia_model.pt')\n            \n            valid_loss_min = avg_valid_loss\n            \n            \n    print(\"Epoch : {} Train Loss : {:.6f} \".format(i+1,avg_train_loss))\n    print(\"Epoch : {} Valid Loss : {:.6f} \".format(i+1,avg_valid_loss))","80ea8a54":"def class_accuracy(dataloader, model):\n    \n    per_class_accuracy = [0 for i in range(len(pathology_list))]\n    total = 0.0\n    \n    with torch.no_grad():\n        \n        for images,labels in dataloader:\n            \n            ps = model(images.to(device))\n            labels = labels.to(device)\n            ps = (ps >= 0.5).float()\n        \n            for i in range(ps.shape[1]):\n                \n                x1 = ps[:,i:i+1]\n                x2 = labels[:,i:i+1]\n                per_class_accuracy[i] += int((x1 == x2).sum())\n                \n        per_class_accuracy = [(i\/len(dataloader.dataset))*100.0 for i in per_class_accuracy]\n        \n    return per_class_accuracy     \n\n\ndef get_acc_data(class_names,acc_list):\n    df = pd.DataFrame(list(zip(class_names, acc_list)), columns =['Labels', 'Acc']) \n    return df \n","92c265d8":"print(\"Train Dataset Accuracy Report\")\nacc_list = class_accuracy(trainloader,model)\nget_acc_data(pathology_list,acc_list)\n","4ef4829c":"print(\"Test Dataset Accuracy Report\")\nacc_list = class_accuracy(testloader,model)\nget_acc_data(pathology_list,acc_list)","7836b58f":"print(\"Valid Dataset Accuracy Report\")\nacc_list = class_accuracy(validloader,model)\nget_acc_data(pathology_list,acc_list)","837fae1a":"def view_classify(img,ps,label):\n    \n    class_name = pathology_list\n    classes = np.array(class_name)\n\n    ps = ps.cpu().data.numpy().squeeze()\n    img = deprocess(img)\n    class_labels = list(np.where(label==1)[0])\n\n    if not class_labels :\n        title = 'No Findings'\n    else : \n        title = itemgetter(*class_labels)(class_name)\n    \n    \n\n    fig, (ax1, ax2) = plt.subplots(figsize=(8,12), ncols=2)\n    ax1.imshow(img)\n    ax1.set_title('Ground Truth : {}'.format(title))\n    ax1.axis('off')\n    ax2.barh(classes, ps)\n    ax2.set_aspect(0.1)\n    ax2.set_yticks(classes)\n    ax2.set_yticklabels(classes)\n    ax2.set_title('Predicted Class')\n    ax2.set_xlim(0, 1.1)\n\n    plt.tight_layout()\n\n    return None","bf7cdc2d":"image,label = testset[123]\n\nps = model(image.unsqueeze(0).to(device))\n\nview_classify(image,ps,label)","76b30c03":"image,label = trainset[998]\n\nps = model(image.unsqueeze(0).to(device))\n\nview_classify(image,ps,label)","40d29a7b":"image,label = validset[234]\n\nps = model(image.unsqueeze(0).to(device))\n\nview_classify(image,ps,label)","a8c6aa88":"# $\\texttt{Avoid Data Imbalance using Weighted Loss}$","de122f0f":"# $\\texttt{Define Pre-trained Model}$","d76de649":"### $\\texttt{Check the lenth of the data}$","27c31120":"### $\\texttt{Plot the freqencies to each class, in order to check the data balance-imbalance}$","f39954c4":"# $\\texttt{HEY EVERYONE!}$\n\n### $\\texttt{Dataset}$ : $\\texttt{Random Sample of NIH Chest X-ray Dataset} $\n\n$\\texttt{Outline}$ \n\n- $\\texttt{EDA}$\n- $\\texttt{Avoid Data Imbalance using Weighted Loss}$\n- $\\texttt{Loading Dataset and Applying Transforms}$\n- $\\texttt{Define Pre-trained Model}$ \n- $\\texttt{Train Model}$\n- $\\texttt{Each label Accuracy}$\n- $\\texttt{Plot results}$","fbeb19fe":"### $\\texttt{Check out the CSV Data:}$","e03b98d8":"$\\texttt{As we see in the above plot, the contributions of positive cases is significantly lower than that of the}$\n$\\texttt{negative ones. However, we want the contributions to be equal. }$\n$\\texttt{One way of doing this is by multiplying each example from each class by a  $w_{pos}$ and class-specific}$ \n$\\texttt{weight factor, $w_{neg}$, so that the overall contribution of each class is the same.}$ \n\n$\\texttt{To have this, we want}$ \n\n$$w_{pos} \\times freq_{p} = w_{neg} \\times freq_{n},$$\n\n$\\texttt{which we can do simply by taking }$\n\n$$w_{pos} = freq_{neg}$$\n$$w_{neg} = freq_{pos}$$\n\n$\\texttt{This way, we will be balancing the contribution of positive and negative labels.}$","acaf8b69":"# $\\texttt{EDA}$","7e8f4f1d":"### $\\texttt{Visualize our data, by each specific coulmn:}$","e00d1eb8":"# $\\texttt{Each Class Accuracy}$","1b10341f":"### $\\texttt{First of all, let's import the necessary packages.}$","5ab42fe7":"## $\\texttt{Impact of class imbalance on loss function:}$\n\n$\\texttt{Let's take a closer look at this. Assume we would have used a normal cross-entropy loss for each}$ $\\texttt{pathology. We recall that the cross-entropy loss contribution from the $i^{th}$ training data case is:}$\n\n$$\\mathcal{L}_{cross-entropy}(x_i) = -(y_i \\log(f(x_i)) + (1-y_i) \\log(1-f(x_i))),$$\n\n$\\texttt{where $x_i$ and $y_i$ are the input features and the label, and $f(x_i)$ is the output of the model, }$ $\\texttt{the probability that it is positive.}$ \n\n$\\texttt{Note that for any training case, either $y_i=0$ or else $(1-y_i)=0$, so only one of these terms }$\n$\\texttt{contributes to the loss (the other term is multiplied by zero, and becomes zero).}$\n\n$\\texttt{We can rewrite the overall average cross-entropy loss over the entire training set $\\mathcal{D}$ of size}$ \n$\\texttt{$N$ as follows:}$\n\n$$\\mathcal{L}_{cross-entropy}(\\mathcal{D}) = - \\frac{1}{N}\\big( \\sum_{\\text{positive examples}} \\log (f(x_i)) + \\sum_{\\text{negative examples}} \\log(1-f(x_i)) \\big).$$\n\n","38b89c5d":"$\\texttt{Using this formulation, we can see that if there is a large imbalance with very few positive training}$\n\n$\\texttt{cases, for example, then the loss will be dominated by the negative class. Summing the contribution }$\n\n$\\texttt{over all the training cases for each class (i.e. pathological condition), we see that the contribution }$\n\n$\\texttt{of  each class (i.e. positive or negative) is:}$\n\n$$freq_{p} = \\frac{\\text{number of positive examples}}{N} $$\n\n$$\\text{and}$$\n\n$$freq_{n} = \\frac{\\text{number of negative examples}}{N}.$$","08efe9e3":"# $\\texttt{Plot Results}$","af4b8eba":"# $\\texttt{Loading Dataset and Applying Transforms}$","4155417b":"# $\\texttt{Split Dataset and create dataloaders}$","fa1aea17":"# $\\texttt{Train Model}$ ","76520137":"## $\\texttt{Manupilate the DataFrame}$","270d56ba":"$\\texttt{As the above figure shows, by applying these weightings the positive and negative labels within each }$\n\n$\\texttt{class would have the same aggregate contribution to the loss function. Now let's implement such a los-}$\n$\\texttt{s function.}$\n\n$\\texttt{After computing the weights, our final weighted loss for each training case will be }$\n\n$$\\mathcal{L}_{cross-entropy}^{w}(x) = - (w_{p} y \\log(f(x)) + w_{n}(1-y) \\log( 1 - f(x) ) ).$$"}}