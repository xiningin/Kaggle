{"cell_type":{"a1731c73":"code","f22ddaaf":"code","5e243c73":"code","218a3c9f":"code","253583f2":"code","dfe93597":"code","d0de296f":"code","dfd48165":"code","800984ef":"code","55c90fe8":"code","ac1ef1e2":"code","7108cf1b":"code","2cab277c":"code","e37bfd41":"code","ac521962":"code","418b50af":"code","1c8cc139":"code","5cc91e8c":"code","2436cb78":"code","da26eeff":"code","32f5252e":"code","56cd7875":"code","297bce05":"code","5e4e153d":"code","cbcf77c0":"code","efb0d998":"code","65445181":"code","1f06b702":"code","ad8d6cae":"code","492f194f":"code","9bcd34cb":"code","71be2094":"code","5656b409":"code","d38286df":"code","9c82d50f":"code","93191228":"code","202082c0":"code","d4b3263a":"markdown","dd702d5b":"markdown","ebf80104":"markdown","d83399b8":"markdown","a45e8ceb":"markdown","a8c30bae":"markdown","a33599e1":"markdown","e6606fcf":"markdown","923c9fef":"markdown"},"source":{"a1731c73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f22ddaaf":"import seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\n#from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport math\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier","5e243c73":"filepath= '\/kaggle\/input\/titanic\/train.csv'\ntraindata= pd.read_csv(filepath)\ntraindata.head()","218a3c9f":"traindata = traindata[['PassengerId','Age','Pclass','Name','Sex','SibSp','Parch','Ticket','Cabin','Embarked','Survived']]\ntraindata.head()\n","253583f2":"#Here we see that there are a large number of null values and it only makes sense to remove them\nsns.heatmap(traindata.isnull(),yticklabels=False,cbar=False,cmap='viridis')","dfe93597":"#We need to tend to the missing values in the age column. For this, instead of dropping the entire column, we fill the null values by the mean of all the ages\ntraindata['Age'].fillna((traindata['Age'].mean()), inplace=True)  ","d0de296f":"#Thus we can see that there are no more null values in the age column\nsns.heatmap(traindata.isnull(),yticklabels=False,cbar=False,cmap='viridis')","dfd48165":"traindata.head()","800984ef":"#Now we plot the number of survivors \nsns.set_style('whitegrid')\nsns.countplot(x='Survived',data=traindata,palette='RdBu_r')","55c90fe8":"#We see how the Sex of the passenger determines the survival of a passenger\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=traindata,palette='RdBu_r')","ac1ef1e2":"#We see how the class the passenger travels in determines the survival of a passenger\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=traindata,palette='rainbow')","7108cf1b":"#We convert the names column to just the honorofic to check if it influences the survival rate\ncol_one_list = traindata['Name'].tolist()\np=[]\nfor a in col_one_list:\n    b=a.split(' ')\n    #print(b)\n    #p.append(b[1])\n    if b[1]=='Mr.' or b[1]=='Mrs.' or b[1]=='Miss.' or b[1]=='Master.':\n          p.append(b[1])\n    else:\n          p.append('rare')\n            \ntraindata['honorifics'] = p\ntraindata.tail()","2cab277c":"# We perform one hot encoding on the honorifics column\none_hot = pd.get_dummies(traindata['honorifics'])\ntraindata = traindata.drop('honorifics',axis = 1)\ntraindata = traindata.join(one_hot)\ntraindata.head()","e37bfd41":"#We remove the Cabin column because it has too many null values.\ntraindata=traindata.drop('Cabin',axis=1)\n#We also remove the Ticket number, passenger ID and the Name columns because they are irrelavant to our model\n\ntraindata=traindata.drop('PassengerId',axis=1)\ntraindata=traindata.drop('Name',axis=1)\ntraindata=traindata.drop('Ticket',axis=1)","ac521962":"#Perform one hot encoding on the Pclass column\none_hot = pd.get_dummies(traindata['Pclass'])\n# Drop column Product as it is now encoded\ntraindata = traindata.drop('Pclass',axis = 1)\n# Join the encoded df\ntraindata = traindata.join(one_hot)\ntraindata.head()","418b50af":"# Similarly, perform one hot encoding on the Embarked column\none_hot = pd.get_dummies(traindata['Embarked'])\ntraindata = traindata.drop('Embarked',axis = 1)\ntraindata = traindata.join(one_hot)\ntraindata.head()","1c8cc139":"#Change the categorical variables in the Sex column to numbers\ntraindata['Sex'] = traindata['Sex'].replace('male', 0)\ntraindata['Sex'] = traindata['Sex'].replace('female', 1)\ntraindata.head()","5cc91e8c":"\n\ny=traindata['Survived']\nx=traindata.drop('Survived',axis=1)\n#Splitting training and testing data\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.70,test_size=0.30, random_state=0)","2436cb78":"\n\n\n#Logistic Regression\nLogisticRegressor = LogisticRegression(max_iter=10000)\nLogisticRegressor.fit(x_train, y_train)\ny_predicted = LogisticRegressor.predict(x_test)\nmse = mean_squared_error(y_test, y_predicted)\nr = r2_score(y_test, y_predicted)\nmae = mean_absolute_error(y_test,y_predicted)\nprint(\"Mean Squared Error:\",mse)\nprint(\"R score:\",r)\nprint(\"Mean Absolute Error:\",mae)\nprint('accuracy score:')\nprint(accuracy_score(y_test,y_predicted))\nprint('f1 score:')\nprint(f1_score(y_test,y_predicted))","da26eeff":"# Random Forest\nrf = RandomForestClassifier()\nrf.fit(x_train,y_train);\ny_predicted_r = rf.predict(x_test)\nmse = mean_squared_error(y_test, y_predicted_r)\nr = r2_score(y_test, y_predicted_r)\nmae = mean_absolute_error(y_test,y_predicted_r)\nprint(\"Mean Squared Error:\",mse)\nprint(\"R score:\",r)\nprint(\"Mean Absolute Error:\",mae)\nprint('accuracy score:')\nprint(accuracy_score(y_test,y_predicted_r))\nprint('f1 score:')\nprint(f1_score(y_test,y_predicted_r))","32f5252e":"math.sqrt(len(y_test)) #Therefore we use 15 as the number of neighbors","56cd7875":"#KNN\nmath.sqrt(len(y_test))\n\nclassify= KNeighborsClassifier (n_neighbors=15, p =2, metric= 'euclidean')\nclassify.fit(x_train,y_train)\nypred1=classify.predict(x_test)\n\nmsee = mean_squared_error(y_test, ypred1)\nr = r2_score(y_test, ypred1)\nmaee = mean_absolute_error(y_test,ypred1)\nprint(\"Mean Squared Error:\",msee)\nprint(\"R score:\",r)\nprint(\"Mean Absolute Error:\",maee)\n\nprint('f1 score:')\nprint(f1_score(y_test,ypred1))\nprint('accuracy score:')\nprint(accuracy_score(y_test,ypred1))","297bce05":"#SVM\n\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(x_train, y_train)\ny_pred2 = svclassifier.predict(x_test)\n\nmseew = mean_squared_error(y_test, y_pred2)\nra = r2_score(y_test, y_pred2)\nmaeew = mean_absolute_error(y_test,y_pred2)\nprint(\"Mean Squared Error:\",mseew)\nprint(\"R score:\",ra)\nprint(\"Mean Absolute Error:\",maeew)\n\nprint('f1 score:')\nprint(f1_score(y_test,y_pred2))\nprint('accuracy score:')\n\nprint(accuracy_score(y_test,y_pred2))","5e4e153d":"#DecisionTreeClassifier\ndt_model=DecisionTreeClassifier()\ndt_model.fit(x_train,y_train)\ndt_pred = dt_model.predict(x_test)\n\nmsee1 = mean_squared_error(y_test, y_pred2)\nra1 = r2_score(y_test, y_pred2)\nmaee1 = mean_absolute_error(y_test,y_pred2)\nprint(\"Mean Squared Error:\",msee1)\nprint(\"R score:\",ra1)\nprint(\"Mean Absolute Error:\",maee1)\nprint('f1 score:')\nprint(f1_score(y_test,dt_pred))\nprint('accuracy score:')\nprint(accuracy_score(y_test,dt_pred))","cbcf77c0":"#XGBClassifier\nxgboost = XGBClassifier(n_estimators=1000)\nxgboost.fit(x_train,y_train)\nxg_pred = xgboost.predict(x_test)\nmsee21 = mean_squared_error(y_test, y_pred2)\nra21 = r2_score(y_test, y_pred2)\nmaee21 = mean_absolute_error(y_test,y_pred2)\nprint(\"Mean Squared Error:\",msee21)\nprint(\"R score:\",ra21)\nprint(\"Mean Absolute Error:\",maee21)\nprint('f1 score:')\nprint(f1_score(y_test,xg_pred))\nprint('accuracy score:')\nprint(accuracy_score(y_test,xg_pred))","efb0d998":"filepath= '\/kaggle\/input\/titanic\/test.csv'\ntestdata= pd.read_csv(filepath)\nsubmissiondata=testdata\ntestdata.head()","65445181":"testdata = testdata[['PassengerId','Age','Pclass','Name','Sex','SibSp','Parch','Ticket','Cabin','Embarked']]\ntestdata.head()\n","1f06b702":"#We convert the names column to just the honorofic to check if it influences the survival rate\ncol_one_list = testdata['Name'].tolist() \np=[]\nfor a in col_one_list:\n    b=a.split(' ')\n    #print(b)\n    #p.append(b[1])\n    if b[1]=='Mr.' or b[1]=='Mrs.' or b[1]=='Miss.' or b[1]=='Master.':\n          p.append(b[1])\n    else:\n          p.append('rare')\n            \ntestdata['honorifics'] = p\ntestdata.tail()\nsns.heatmap(testdata.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n","ad8d6cae":"testdata=testdata.drop('Cabin',axis=1)\ntestdata=testdata.drop('PassengerId',axis=1)\ntestdata=testdata.drop('Name',axis=1)\ntestdata=testdata.drop('Ticket',axis=1)\nsns.heatmap(testdata.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n","492f194f":"testdata['Age'].fillna((testdata['Age'].mean()), inplace=True)","9bcd34cb":"# Perform one hot encoding on the honorifics column\none_hot = pd.get_dummies(testdata['honorifics'])\ntestdata = testdata.drop('honorifics',axis = 1)\ntestdata = testdata.join(one_hot)\ntestdata.head()","71be2094":"#Perform one hot encoding on the Pclass column\none_hot = pd.get_dummies(testdata['Pclass'])\n# Drop column Product as it is now encoded\ntestdata = testdata.drop('Pclass',axis = 1)\n# Join the encoded df\ntestdata = testdata.join(one_hot)\ntestdata.head()","5656b409":"# Similarly, perform one hot encoding on the Embarked column\none_hot = pd.get_dummies(testdata['Embarked'])\ntestdata = testdata.drop('Embarked',axis = 1)\ntestdata = testdata.join(one_hot)\ntestdata.head()","d38286df":"#Change the categorical variables in the Sex column to numbers\ntestdata['Sex'] = testdata['Sex'].replace('male', 0)\ntestdata['Sex'] = testdata['Sex'].replace('female', 1)\ntestdata.head()","9c82d50f":"sns.heatmap(testdata.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n\n\n","93191228":"#Logistic Regression\nLogisticRegressor = LogisticRegression(max_iter=10000)\nLogisticRegressor.fit(x_train, y_train)\ny_predicted = LogisticRegressor.predict(testdata)","202082c0":"predictionlist=y_predicted.tolist()\nPassengerid=submissiondata['PassengerId'].tolist() \noutput=pd.DataFrame(list(zip(Passengerid, predictionlist)),\n              columns=['PassengerId','Survived'])\noutput.head()\noutput.to_csv('my_submission.csv', index=False)","d4b3263a":"**Finally, we convert the predictions to a csv file for submission.**","dd702d5b":"**Now we will run a few machine learning techiniques to see which one is the most applicable**","ebf80104":"**We use logistic regression because it is effecient when compared with the other models.**","d83399b8":"**Define the dependant and independant variables and divide them into training and testing data**","a45e8ceb":"**Thus all null values are taken care of.**","a8c30bae":"**Now we process the categorical variables to convert them to numerical form.**","a33599e1":"**Exploratory data analysis--**","e6606fcf":"**Now we preprocess the test data exactly like we did the training data.**","923c9fef":"**Data Preprocessing**--"}}