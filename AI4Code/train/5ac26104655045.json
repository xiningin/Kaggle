{"cell_type":{"3acf9f9e":"code","8439ed57":"code","eaac2233":"code","cf46049d":"code","90c176fd":"code","779377bd":"code","ed5b349f":"code","30416420":"code","e383adf0":"code","a8b2b723":"code","3d1a7fa4":"code","1b69c198":"code","338f26c4":"code","43eaf50d":"code","71203ab7":"code","01cd4824":"code","a8304610":"code","62374bf9":"code","dbb474aa":"code","895079b7":"code","1c418ed7":"code","2d8381c0":"code","bcfe0ecf":"code","e5c9ba0e":"code","1bb361bf":"code","19c1ff4f":"code","650f92b9":"code","2cf516cc":"code","ecea4ad6":"code","eac20268":"code","e7e846fb":"markdown","f6bcc142":"markdown","119b1201":"markdown","440d41d7":"markdown","ced303a0":"markdown","1651ba94":"markdown"},"source":{"3acf9f9e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n## Most Important\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom pathlib import Path\nfrom PIL import Image\nimport scipy\n\n## less Important\nfrom functools import partial\nimport os\nfrom scipy import stats\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,AveragePooling2D\nfrom keras.callbacks import LearningRateScheduler,ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\n\n## Sklearn\nfrom sklearn import datasets\n## Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n## Metrics\nfrom sklearn.metrics import accuracy_score\n\n## tensorflow & Keras\nimport tensorflow as tf    ## i will use tf for every thing and for keras using tf.keras\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8439ed57":"train_labels = pd.read_csv('..\/input\/arabic-hwr-ai-pro-intake1\/train.csv')\ntrain_images = Path(r'..\/input\/arabic-hwr-ai-pro-intake1\/train')\n\n## read these all training images paths as Series\ntrain_images_paths = pd.Series(sorted(list(train_images.glob(r'*.png'))), name='Filepath').astype(str)\n\ntrain_images_paths.head()","eaac2233":"img_key_value = {}\nfor value in train_labels['label'].unique():\n    img_key_value[value] = train_labels[train_labels['label']==value].index[0]\n    \nimg_index = list(img_key_value.values())\nimg_label = list(img_key_value.keys())\n\nfig, ax = plt.subplots(4, 7, figsize=(12, 8))\n\ni = 0\nfor row in range(4):\n    for col in range(7):\n        plt.sca(ax[row, col])\n        plt.title(f'label = {img_label[i]}')\n        img = plt.imread(train_images_paths.iloc[img_index[i]])\n        plt.imshow(img)\n        plt.axis('off')\n        i+=1","cf46049d":"# know th shape \nprint('Number of Instances in train_set =>', len(train_images_paths))\nprint('Number of Instances in train_labels =>', len(train_labels))\n\nprint()\n\nimg = plt.imread(train_images_paths.iloc[img_index[0]])\nprint('shape of each Image is =>', img.shape)","90c176fd":"train_full_labels = train_labels['label'].values\ntrain_full_set = np.empty((13440, 32, 32, 3), dtype=np.float32)  #take only the first 2 channels\n\nfor idx, path in enumerate(train_images_paths):\n    img = plt.imread(path)\n    img = img[:,:,:3]\n    train_full_set[idx] = img\n    \nprint('train_full_set.shape =>', train_full_set.shape)\nprint('train_full_labels.shape =>', train_full_labels.shape)","779377bd":"X_train, X_valid, y_train, y_valid = train_test_split(train_full_set, train_full_labels, \n                                                      test_size=0.2, shuffle=True, random_state=42)\n\nprint('X_train.shape =>', X_train.shape)\nprint('X_valid.shape =>', X_valid.shape)\nprint('y_train.shape =>', y_train.shape)\nprint('y_valid.shape =>', y_valid.shape)","ed5b349f":"#Onehot Encoding the labels.\nimport keras\nimport keras.utils\nfrom keras import utils as np_utils\nfrom sklearn.utils.multiclass import unique_labels\nfrom tensorflow.keras.utils import to_categorical\n\n\n\n#Since we have 10 classes we should expect the shape[1] of y_train,y_val and y_test to change from 1 to 10\ny_train=to_categorical(y_train)\ny_valid=to_categorical(y_valid)\n\n#Verifying the dimension after one hot encoding\nprint((X_train.shape,y_train.shape))\nprint((X_valid.shape,y_valid.shape))","30416420":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,AveragePooling2D\n","e383adf0":"# model = tf.keras.models.Sequential([\n#     tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu',input_shape=(32, 32, 3)),\n#     tf.keras.layers.MaxPooling2D(pool_size=2),\n    \n#     tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'),\n#     tf.keras.layers.MaxPooling2D(pool_size=2),\n    \n#     tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', ),\n#     tf.keras.layers.MaxPooling2D(pool_size=2),\n    \n#     tf.keras.layers.GlobalAveragePooling2D(),\n#     tf.keras.layers.Dense(29, activation='softmax')\n \n# ])","a8b2b723":"allow_soft_placement=True\n","3d1a7fa4":"nets = 5\nmodel = [0] *nets\nfor j in range(nets):\n    model[j] = Sequential()\n    model[j].add(Conv2D(32, kernel_size = 3, activation='relu', input_shape=(32, 32, 3)))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(32, kernel_size = 3, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Dropout(0.4))\n\n    #Second Layer of CNN\n    model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Dropout(0.4))\n\n    #Third layer of CNN\n    model[j].add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Flatten())\n    model[j].add(Dropout(0.4))\n\n    #Output layer\n    model[j].add(Dense(29, activation='softmax'))\n\n    # Compile each model\n    model[j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    early_stopp = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)","1b69c198":"model","338f26c4":"len(model)","43eaf50d":"# Generate batches of tensor image data with real-time data augmentation more detail: https:\/\/keras.io\/preprocessing\/image\/\ndatagen = ImageDataGenerator(rotation_range=2, zoom_range = 0.1, width_shift_range=0.1, height_shift_range=0.1)\ndatagen.fit(X_train)","71203ab7":"batch_size = 32 # Handle 32 pictures at each round\nepochs = 240 ","01cd4824":"for j in range(5):\n    print(f'Individual Net : {j+1}')   \n    model[j].fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n                                        epochs = epochs, steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                                        validation_data = (X_valid,y_valid), \n                                        callbacks=[ReduceLROnPlateau(monitor='loss', patience=3, factor=0.1)], \n                                        verbose=2)","a8304610":"# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# early_stopp = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)","62374bf9":"# history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), \n#                     epochs=10, batch_size=32, callbacks=[early_stopp])","dbb474aa":"model","895079b7":"test_labels = pd.read_csv('..\/input\/arabic-hwr-ai-pro-intake1\/test.csv')\ntest_images = Path(r'..\/input\/arabic-hwr-ai-pro-intake1\/test')\n\n## read these all training images paths as Series\ntest_images_paths = pd.Series(sorted(list(test_images.glob(r'*.png'))), name='Filepath').astype(str)\n\ntest_images_paths.head()","1c418ed7":"print('Number of Instances in test_set is', len(test_images_paths))\n","2d8381c0":"test_full_set = np.empty((3360, 32, 32, 3), dtype=np.float32)  #take only the first 3 channels\n\nfor idx, path in enumerate(test_images_paths):\n    img = plt.imread(path)\n    img = img[:,:,:3]\n    test_full_set[idx] = img\n    \nprint('test_full_set.shape =>', test_full_set.shape)","bcfe0ecf":"# y_preds_classes = np.argmax(model.predict(test_full_set), axis=-1)\n# Predict labels with models\nlabels = []\nfor m in model:\n    predicts = np.argmax(m.predict(test_full_set), axis=1)\n    labels.append(predicts)\n    \n# Ensemble with voting\nlabels = np.array(labels)\nlabels = np.transpose(labels, (1, 0))\nlabels = scipy.stats.mode(labels, axis=1)[0]\nlabels = np.squeeze(labels)\n","e5c9ba0e":"labels.shape","1bb361bf":"test_labels['label'] = labels","19c1ff4f":"test_labels","650f92b9":"# Dump predictions into submission file\npd.DataFrame({'ImageId' : np.arange(1, predicts.shape[0] + 1), 'Label' : labels }).to_csv('\/kaggle\/working\/submission.csv', index=False)","2cf516cc":"# test_labels['label'] = y_preds_classes\n","ecea4ad6":"# test_labels\n","eac20268":"test_labels[['id', 'label']].to_csv('\/kaggle\/working\/submission.csv', index=False)\n","e7e846fb":"# Evaluate on test set","f6bcc142":"# Model training","119b1201":"# Data Preprocessing","440d41d7":"# Split the data","ced303a0":"# Reading Data","1651ba94":"# Explore the data"}}