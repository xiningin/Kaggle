{"cell_type":{"dcae4e40":"code","f4dfd5df":"code","7e520acf":"code","1d8fbef9":"code","02df14f3":"code","19b2cec5":"code","b43b4078":"code","5f8235e8":"code","d7e8287a":"code","f69231e5":"code","ef7fa2e1":"code","27c59b85":"code","f439deb0":"code","814492b2":"code","b121d1fa":"code","41827ca1":"code","4b5f3d29":"code","dbdfb282":"code","1cbd4de1":"code","e1e2851e":"code","3930df28":"code","96716ac2":"code","fcea3d42":"code","2f918a25":"code","2011cb29":"markdown"},"source":{"dcae4e40":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\npd.set_option('display.max_columns', None)","f4dfd5df":"data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndataTest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\npassengerID = dataTest.PassengerId\nclean_data = [data,dataTest]","7e520acf":"#perProcess data\n#ID Ticket Name \u5bf9\u7ed3\u679c\u6ca1\u4ec0\u4e48\u5f71\u54cd\uff0c\u53bb\u6389,complete miss value.\nfor dataset in clean_data:\n    dataset.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True)\n    dataset['Age'].fillna(dataset['Age'].median(), axis=0,inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0],axis = 0,inplace=True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), axis=0,inplace = True)\n    \n#feature create\nfor dataset in clean_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 1\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n    ","1d8fbef9":"#create faeture \nFareBins = [0,50,100,150,200,350,400,500,550]\nfor dataset in clean_data:\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)","02df14f3":"AgeBins = [0,10,20,30,40,50,60,70,80]\nfor dataset in clean_data:\n    dataset['AgeBin'] = pd.cut(dataset['Age'],AgeBins)","19b2cec5":"plt.figure(figsize=(15, 6))\nsns.countplot(x='AgeBin',data = data)\nplt.legend()\nplt.figure(figsize=(15, 6))\nsns.countplot(x='FareBin',data = data)\nplt.legend()","b43b4078":"plt.figure(figsize=[30,12])\nplt.subplot(234)\nplt.hist(x = [data[data['Survived']==1]['Fare'], data[data['Survived']==0]['Fare']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Fare Histogram by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(235)\nplt.hist(x = [data[data['Survived'] == 1]['Age'] , data[data['Survived'] == 0]['Age']], \n        stacked=True,color=['g','r'],label = ['Survived', 'Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (year)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(236)\nplt.hist([data[data['Survived'] == 1]['IsAlone'], data[data['Survived']==0]['IsAlone']],stacked=True,color=['g','r'],\n        label=['Survived','Dead'])\nplt.title('IsAlone Histogram by Survival')\nplt.xlabel('IsAlone (year)')\nplt.ylabel('# of Passengers')\nplt.legend()","5f8235e8":"#convert categorical data to dummy variables for mathematical analysis\nlabel = LabelEncoder()\nfor dataset in clean_data:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n    \n#define y variable aka target\/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\ndata1_x = ['Sex','Pclass', 'Embarked','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name\/values for charts\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code','SibSp', 'Parch', 'Age', 'Fare','FamilySize','IsAlone'] #coded for algorithm calculation\ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n\n#define x variables for original w\/bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\ndata1_x_total = ['Sex_Code','Pclass', 'Embarked_Code','SibSp', 'Parch', 'Age', 'Fare','FamilySize','IsAlone','AgeBin_Code', 'FareBin_Code']\ndata1_xy_total = Target + data1_x_total\nprint(\"Total X Y: \",data1_xy_total,'\\n')","d7e8287a":"print(data[data1_x_total].describe())","f69231e5":"#\u6b63\u5219\u5316\u6570\u636e\uff0c\u89c4\u8303\u6570\u636e\u8303\u56f4\u51cf\u5c11\u5947\u5f02\u503c\u5f71\u54cd\u7b97\u6cd5\u7cbe\u5ea6\nfrom sklearn.preprocessing import Normalizer\nscaler = Normalizer(norm='l1')\nscale_data = scaler.fit_transform(data[data1_x_bin])\nscale_data = pd.DataFrame(scale_data,columns=data1_x_bin)\nprint(scale_data.describe())","ef7fa2e1":"#\u6027\u522b\u5bb6\u5ead\u4eba\u6570\u5bf9\u4e8e\u5b58\u6d3b\u7387\u5206\u6790\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis2)","27c59b85":"#\u5206\u6790\u6e2f\u53e3\u767b\u5f55\u7684\u7968\u4ef7\u548c\u5b58\u6d3b\u7387\uff0c\u4e00\u822c\u7684\u5973\u7684\u6bd4\u8f83\u5bb9\u6613\u6d3b\uff0c\u6709\u94b1\u7684\u7537\u4eba\u6d3b\uff0c\nplt.figure(figsize=[30,12])\nplt.subplot(345)\nsns.stripplot(x='Embarked',y='Fare',data=data)\ne = sns.FacetGrid(data, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()","f439deb0":"#Age\u7684\u5b58\u6d3b\u7387\u6b7b\u4ea1\u7387\u5206\u5e03\na = sns.FacetGrid( data, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.add_legend()","814492b2":"h = sns.FacetGrid(data, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()","b121d1fa":"#\u67e5\u770b\u603b\u4f53\u4e0a\u6bcf\u4e00\u5217\u7279\u5f81\u5bf9\u4e8e\u5b58\u6d3b\u7387\u7684\u76f8\u5173\u7cfb\u6570\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data)","41827ca1":"plt.figure(figsize=[30,12])\nsns.pairplot(data, hue=\"Survived\")","4b5f3d29":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\nMLA_predict = data[Target]\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, data[data1_x_total], data[Target],return_train_score=True, cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(data[data1_x_total], data[Target])\n    MLA_predict[MLA_name] = alg.predict(data[data1_x_total])\n    \n    row_index+=1\n#print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict","dbdfb282":"#barplot using https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nplt.figure(figsize=[15,10])\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","1cbd4de1":"#\u6311\u9009RandomForestClassifier\u8fdb\u884c\u5b66\u4e60\nclf = ensemble.RandomForestClassifier()\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [2,4,6,8,10,None],\n}\ntune_model = model_selection.GridSearchCV(clf, param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)","e1e2851e":"tune_model.fit(data[data1_x_total],data[Target])","3930df28":"result = pd.DataFrame(tune_model.cv_results_)\nprint(result)","96716ac2":"bestCLF = tune_model.best_estimator_\nprint(bestCLF)","fcea3d42":"Y_Pred = bestCLF.predict(dataTest[data1_x_total])","2f918a25":"print(dataTest.columns)\nsubmission = pd.DataFrame({\n        \"PassengerId\": passengerID,\n        \"Survived\": Y_Pred})\nsubmission.to_csv('.\/submission.csv', index=False)","2011cb29":"Hello everyone, for this kernel I porpose a simple DecisionTreeClassifier method to improve the prediction of who survive or die in Titanic incident\n\nThese are the steps for this work\n\n1. **Cleaning and Standalize data**\n2. **Data Exploration\/Visualization**\n3. **Use Machine Learning methods to predict the target value in data**\n4. **Put all methods together to get a better prediction**\n\nSo let's get start!"}}