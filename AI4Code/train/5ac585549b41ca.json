{"cell_type":{"d72497e3":"code","b382d6c8":"code","c490bf1b":"code","c07381b4":"code","d343024b":"code","06af7b4c":"code","d256af8a":"code","a74ee523":"code","ac5a8aab":"code","a553180a":"code","46264e05":"code","396c05d3":"code","8ad74a60":"code","9e03c65d":"code","b0613d89":"code","47b05028":"code","76943db6":"code","8da1ac0a":"code","d5fc2a3e":"code","acc8bb98":"code","01cb6faa":"code","96bc466d":"code","2f6ae521":"code","c919f305":"code","6d4fcb23":"code","de7007ee":"code","0cbe1d3b":"code","7d0dd532":"code","72e9dabe":"code","3870c12d":"code","08d2afbc":"code","423697b7":"code","8896fbe1":"code","2b73d6ac":"code","84a21275":"code","4678bab9":"code","01d3bc2d":"code","ebb3bba8":"code","b07aff93":"code","81d071e4":"code","c79ba27e":"code","87ec5262":"code","080bd5e5":"code","7e09f069":"code","f24a29fe":"code","cc309bd0":"code","d045a8da":"code","1f04aad3":"code","0d0f942e":"code","056fc1c5":"code","cbfda274":"code","4ab72c89":"code","ec11154c":"code","67edcfea":"code","6717f4c0":"code","f903af6c":"code","7d841b54":"code","b1904cb8":"code","fcb6b7a3":"code","fff9ebfa":"code","99d3b376":"code","11b73d5e":"code","27698c6b":"markdown","f61fd4fe":"markdown","cf11520f":"markdown","92482e08":"markdown","66050a3c":"markdown","9e7b2a61":"markdown","97dea06f":"markdown","b9050863":"markdown","fbf4d69c":"markdown","121419f5":"markdown"},"source":{"d72497e3":"# Loading Libraires\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom torch.autograd import Variable\nfrom torchvision import models\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Dataset,DataLoader","b382d6c8":"is_cuda = False\nif torch.cuda.is_available():\n    is_cuda = True","c490bf1b":"simple_transform = transforms.Compose([transforms.Resize((120,120))\n                                       ,transforms.ToTensor()\n                                      ])\ndataset = ImageFolder('..\/input\/flowers-recognition\/flowers\/flowers',simple_transform)\nprint(len(dataset), len(dataset.classes))","c07381b4":"#Loader\ndata_loader = DataLoader(dataset,batch_size=16,num_workers=3,shuffle=True)","d343024b":"#Function to get Mean and std of dataset\ndef get_mean_std(loader):\n    #VAR[X] = E[X**2] - E[X] **2\n    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n    \n    for data, _ in loader:\n        channels_sum +=torch.mean(data, dim=[0,2,3]) #BxCxHxW \n        channels_squared_sum+=torch.mean(data**2, dim=[0,2,3])\n        num_batches +=1\n        \n    mean = channels_sum\/num_batches\n    std = (channels_squared_sum\/num_batches -mean**2)**0.5\n    \n    return mean, std","06af7b4c":"mean, std = get_mean_std(data_loader)\nprint(mean)\nprint(std)","d256af8a":"simple_transform = transforms.Compose([transforms.Resize((120,120))\n                                       ,transforms.ToTensor()\n                                       ,transforms.Normalize([0.4591, 0.4201, 0.3004], [0.2890, 0.2584, 0.2817])\n                                      ])\ndataset = ImageFolder('..\/input\/flowers-recognition\/flowers\/flowers',simple_transform)\nprint(len(dataset), len(dataset.classes))","a74ee523":"#Spliting the dataset into training and testing\ntrain, val = torch.utils.data.random_split(dataset, [3000,1323])","ac5a8aab":"#DataLoader\ntrain_data_loader = DataLoader(train,batch_size=16,num_workers=3,shuffle=True)\ntest_data_loader = DataLoader(val,batch_size=16,num_workers=3,shuffle=True)","a553180a":"#Function to Display Image\ndef imshow(inp,cmap=None):\n    inp = inp.numpy().transpose((1, 2, 0)) # Changing into HxWxC in numpy\n    print(inp.shape)\n    mean = np.array([0.4591, 0.4201, 0.3004])\n    std = np.array([0.2890, 0.2584, 0.2817])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp,cmap)\n","46264e05":"imshow(train[75][0])","396c05d3":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        \n        self.layer3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        \n        self.layer4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n    \n            \n        self.fc1 = nn.Linear(7*7*256, 128)\n        self.fc2 = nn.Linear(128, 5)   \n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc1(out)\n        out = F.dropout(out, training=self.training)\n        out = self.fc2(out)\n        return F.log_softmax(out,dim=1)\n","8ad74a60":"model = Net()\nif is_cuda:\n    model.cuda()","9e03c65d":"# optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","b0613d89":"def fit(epoch,model,data_loader,phase='training',volatile=False):\n    if phase == 'training':\n        model.train()\n    if phase == 'validation':\n        model.eval()\n        volatile=True\n    running_loss = 0.0\n    running_correct = 0\n    for batch_idx , (data,target) in enumerate(data_loader):\n        if is_cuda:\n            data,target = data.cuda(),target.cuda()\n        data , target = Variable(data,volatile),Variable(target)\n        if phase == 'training':\n            optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output,target)\n        \n        running_loss += F.nll_loss(output,target,size_average=False).item()\n        preds = output.data.max(dim=1,keepdim=True)[1]\n        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n        if phase == 'training':\n            loss.backward()\n            optimizer.step()\n    \n    loss = running_loss\/len(data_loader.dataset)\n    accuracy = 100. * running_correct\/len(data_loader.dataset)\n    \n    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}\/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n    return loss,accuracy","47b05028":"train_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,20):\n    epoch_loss, epoch_accuracy = fit(epoch,model,train_data_loader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,test_data_loader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","76943db6":"#Loss \nplt.plot(range(1,len(train_losses)+1),train_losses,'b',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()","8da1ac0a":"#Accuracy\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'b',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()","d5fc2a3e":"model = Net()\nif is_cuda:\n    model.cuda()","acc8bb98":"# optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)","01cb6faa":"train_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,20):\n    epoch_loss, epoch_accuracy = fit(epoch,model,train_data_loader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,test_data_loader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","96bc466d":"#Loss \nplt.plot(range(1,len(train_losses)+1),train_losses,'b',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()","2f6ae521":"#Accuracy\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'b',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()","c919f305":"simple_transform = transforms.Compose([transforms.Resize((224,224)),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.RandomRotation(0.2),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.4591, 0.4201, 0.3004], [0.2890, 0.2584, 0.2817])\n                                     ])\ndataset = ImageFolder('..\/input\/flowers-recognition\/flowers\/flowers',simple_transform)\nprint(len(dataset), len(dataset.classes))","6d4fcb23":"#Spliting the dataset into training and testing\ntrain, val = torch.utils.data.random_split(dataset, [3000,1323])","de7007ee":"#DataLoader\ntrain_data_loader = DataLoader(train,batch_size=16,num_workers=3,shuffle=True)\ntest_data_loader = DataLoader(val,batch_size=16,num_workers=3,shuffle=True)","0cbe1d3b":"vgg = models.vgg16(pretrained=True)\nvgg =vgg.cuda()","7d0dd532":"vgg","72e9dabe":"vgg.classifier[6].out_features = 5\nfor param in vgg.features.parameters(): param.requires_grad = False","3870c12d":"optimizer = torch.optim.Adam(vgg.classifier.parameters(),lr=0.001)","08d2afbc":"def fit(epoch,model,data_loader,phase='training',volatile=False):\n    if phase == 'training':\n        model.train()\n    if phase == 'validation':\n        model.eval()\n        volatile=True\n    running_loss = 0.0\n    running_correct = 0\n    for batch_idx , (data,target) in enumerate(data_loader):\n        if is_cuda:\n            data,target = data.cuda(),target.cuda()\n        data , target = Variable(data,volatile),Variable(target)\n        if phase == 'training':\n            optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output,target)\n        \n        running_loss += F.cross_entropy(output,target,size_average=False).item()\n        preds = output.data.max(dim=1,keepdim=True)[1]\n        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n        if phase == 'training':\n            loss.backward()\n            optimizer.step()\n    \n    loss = running_loss\/len(data_loader.dataset)\n    accuracy = 100. * running_correct\/len(data_loader.dataset)\n    \n    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}\/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n    return loss,accuracy","423697b7":"train_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,10):\n    epoch_loss, epoch_accuracy = fit(epoch,vgg,train_data_loader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,vgg,test_data_loader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","8896fbe1":"#Loss \nplt.plot(range(1,len(train_losses)+1),train_losses,'b',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()","2b73d6ac":"#Accuracy\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'b',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()","84a21275":"vgg = models.vgg16(pretrained=True)\nvgg =vgg.cuda()\n\nvgg.classifier[6].out_features = 5\nfor param in vgg.features.parameters(): param.requires_grad = False","4678bab9":"#Changing the Optimizer\noptimizer = torch.optim.SGD(vgg.classifier.parameters(),lr=0.01)","01d3bc2d":"train_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,10):\n    epoch_loss, epoch_accuracy = fit(epoch,vgg,train_data_loader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,vgg,test_data_loader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","ebb3bba8":"#Loss \nplt.plot(range(1,len(train_losses)+1),train_losses,'b',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()","b07aff93":"#Accuracy\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'b',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()","81d071e4":"resnet = models.resnet18(pretrained=True)","c79ba27e":"resnet","87ec5262":"resnet.fc.out_features = 5\nresnet = resnet.cuda()","080bd5e5":"optimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)","7e09f069":"train_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,10):\n    epoch_loss, epoch_accuracy = fit(epoch,resnet,train_data_loader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,resnet,test_data_loader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","f24a29fe":"#Loss \nplt.plot(range(1,len(train_losses)+1),train_losses,'b',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()","cc309bd0":"#Accuracy\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'b',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()","d045a8da":"#Changing Optimizer (SGD with momentum)\nresnet = models.resnet18(pretrained=True)\nresnet = resnet.cuda()\n\noptimizer = torch.optim.SGD(resnet.parameters(), lr=0.01, momentum=0.5)","1f04aad3":"train_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,10):\n    epoch_loss, epoch_accuracy = fit(epoch,resnet,train_data_loader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,resnet,test_data_loader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","0d0f942e":"#Loss \nplt.plot(range(1,len(train_losses)+1),train_losses,'b',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()","056fc1c5":"#Accuracy\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'b',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()","cbfda274":"googlenet = models.googlenet(pretrained=True)","4ab72c89":"googlenet","ec11154c":"googlenet.fc.out_features = 5\ngooglenet = googlenet.cuda()","67edcfea":"optimizer = torch.optim.Adam(googlenet.parameters(), lr=0.001)","6717f4c0":"train_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,10):\n    epoch_loss, epoch_accuracy = fit(epoch,googlenet,train_data_loader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,googlenet,test_data_loader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","f903af6c":"#Loss \nplt.plot(range(1,len(train_losses)+1),train_losses,'b',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()","7d841b54":"#Accuracy\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'b',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()","b1904cb8":"googlenet = models.googlenet(pretrained=True)\ngooglenet.fc.out_features = 5\ngooglenet = googlenet.cuda()","fcb6b7a3":"optimizer = torch.optim.SGD(googlenet.parameters(), lr=0.01)","fff9ebfa":"train_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\nfor epoch in range(1,10):\n    epoch_loss, epoch_accuracy = fit(epoch,googlenet,train_data_loader,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,googlenet,test_data_loader,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)","99d3b376":"#Loss \nplt.plot(range(1,len(train_losses)+1),train_losses,'b',label = 'training loss')\nplt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\nplt.legend()","11b73d5e":"#Accuracy\nplt.plot(range(1,len(train_accuracy)+1),train_accuracy,'b',label = 'train accuracy')\nplt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\nplt.legend()","27698c6b":"# Custom CNN","f61fd4fe":"**VGG16 with SGD Optimizer**","cf11520f":"**VGG16 with Adam Optimizer**","92482e08":"**Changing the Learning Rate**","66050a3c":"**ResNet18 with SGD Optimizer with momentum**","9e7b2a61":"# Flower Classification\nComparison between different CNN model with different optimizers.\n> Model Used:- \n> * Custom CNN with Adam (different Learning Rates)\n* Pretrained VGG16 with Adam and SGD optimizer\n* Pretrained ResNet18 with Adam and SGD optimizer with momentum\n* Pretrained GoogLeNet with Adam and SGD optimizer\n","97dea06f":"**GoogLeNet with SGD Optimizer**","b9050863":"# Transfer Learning","fbf4d69c":"**ResNet18 with Adam Optimizer**","121419f5":"**GoogLeNet with Adam Optimizer**"}}