{"cell_type":{"e7f2d50d":"code","19e932a4":"code","50644772":"code","648e9e66":"code","6bbbce08":"code","793440b2":"code","c3c12229":"code","a5efd573":"code","0cea674a":"code","bc730ff3":"code","378e4b88":"code","209cc170":"code","16c16cf3":"code","ca60bda8":"code","4e77c49b":"code","a34426bb":"code","7c2a2341":"code","2ab85089":"code","4da5d856":"code","6d89083b":"code","bf737e61":"code","204b937c":"code","3d171833":"markdown","8cc597b0":"markdown"},"source":{"e7f2d50d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","19e932a4":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nwine = pd.read_csv(\"..\/input\/winequality-red.csv\")\nwine.shape","50644772":"wine['quality'] = np.where(wine['quality']<6.5, 'Bad', 'Good')\nwine[:10]","648e9e66":"#One hot encoding\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\n#Since many ML alogrithms operate with numerical data well than categorical data,\n#encoding the target variable to have 0 & 1 in place of Good and Bad\nEncoder = LabelEncoder()\nwine['quality'] = Encoder.fit_transform(wine['quality'])\nwine[:10]","6bbbce08":"import matplotlib.pyplot as plt","793440b2":"X = wine.drop('quality',axis = 1)\n\ny  = wine.quality\nwine[:10]","c3c12229":"import seaborn as sns\n\nwine.quality.value_counts()\nsns.countplot(wine.quality)\nplt.show()\n#The distribution of the target population classes is skewed","a5efd573":"#Train test split\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\nx_train.shape\n","0cea674a":"#Exploring Classification algorithms\n#Random forest\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix\nrf = RandomForestClassifier(random_state = 10)\nrf.fit(x_train,y_train)\npred_rfc = rf.predict(x_test)\n\n","bc730ff3":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test, pred_rfc))\n#Random forest gives 93% accuracy\nprint(confusion_matrix(y_test, pred_rfc))\nplt.figure(figsize=(5,5))\n\ncm_rf = confusion_matrix(y_test,pred_rfc)\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=20)\n\n#plt.subplot(2,3,1)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rf,cbar=False,annot=True,cmap=\"Greens\",fmt=\"d\")\n#Read more about precision and recall in this article\n#https:\/\/towardsdatascience.com\/model-evaluation-techniques-for-classification-models-eac30092c38b#:~:text=In%20machine%20learning%2C%20we%20often,predicted%20result%20of%20population%20data.&text=The%20training%20dataset%20trains%20the,Decision%20tree%2C%20Naive%20Bayes%20etc.","378e4b88":"predctn_RF =pd.DataFrame({'Actual':y_test, 'Predicted':pred_rfc})\npredctn_RF","209cc170":"print( np.unique( pred_rfc) )\n#Model is predicting for both 0 & 1\n","16c16cf3":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state = 10)\ndt.fit(x_train,y_train)\npred_dt = dt.predict(x_test)\nprint(classification_report(y_test,pred_dt))\nprint(confusion_matrix(y_test, pred_dt))\n","ca60bda8":"plt.figure(figsize=(5,5))\ncm_dt = confusion_matrix(y_test,pred_dt)\nplt.suptitle(\"Confusion Matrixes\",fontsize=20)\n\n#plt.subplot(2,3,1)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_dt,cbar=False,annot=True,cmap=\"Blues\",fmt=\"d\")\n","4e77c49b":"predctn_DT =pd.DataFrame({'Actual':y_test, 'Predicted':pred_dt})\npredctn_DT","a34426bb":"print( np.unique( pred_dt) )\n#Model is predicting for both 0 & 1","7c2a2341":"from sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\nlogit.fit(x_train,y_train)","2ab85089":"logit_pred = logit.predict(x_test)\nprint(classification_report(y_test,logit_pred))\nprint(confusion_matrix(y_test, logit_pred))\n","4da5d856":"print( np.unique( logit_pred) )\n#Model is predicting for both 0 & 1","6d89083b":"plt.figure(figsize=(5,5))\n\ncm_lg = confusion_matrix(y_test,logit_pred)\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=20)\n\n#plt.subplot(2,3,1)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_lg,cbar=False,annot=True,cmap=\"Oranges\",fmt=\"d\")","bf737e61":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nsvc1= SVC(random_state = 42, C = 10, gamma = 1, kernel = 'rbf')\nsvc1.fit(x_train, y_train)\n\nac = accuracy_score(y_test,svc1.predict(x_test))\n#accuracies['SVM'] = ac\n\n\nprint('Accuracy is: ',ac, '\\n')\ncm = confusion_matrix(y_test,svc1.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")\n\nprint('SVM report\\n',classification_report(y_test, svc1.predict(x_test)))","204b937c":"plt.scatter(x_train.iloc[:, 0], x_train.iloc[:, 1], c=y_train, s=100, cmap='autumn')\n\nplt.scatter(model.support_vectors_[:,0],model.support_vectors_[:,1])","3d171833":"Please upvote and fork if you find this kernel helpful.Thanks!","8cc597b0":"Here we should not talk about the accuracy but the precision and recall rates, more soon...."}}