{"cell_type":{"e81cb467":"code","ac451da6":"code","2871cd7f":"code","5fe6b1cb":"code","b6c4afaa":"code","d6b4c7da":"code","002491ef":"code","890bd61c":"code","536da4f3":"code","1d474a90":"code","dcb42c66":"code","1b89e49a":"code","9e366826":"code","0c60225a":"code","e8275ad4":"code","fd7df2b9":"code","d8e43b74":"code","a19fd67a":"code","027457d7":"code","6b65bc48":"code","4abd09e8":"code","06ae8a7f":"code","a7cdeba9":"code","1f962744":"code","91527071":"code","7daa9044":"code","45015994":"code","f2d853f0":"code","42d2e86f":"code","cee33c62":"code","a1da2783":"code","063d7b49":"code","cb47344f":"code","d1a3f804":"code","f5d7cb7e":"code","fbb1d30e":"code","4a16ea14":"code","9abd4988":"code","f472f245":"code","7bd94a19":"code","4f6a5158":"code","20d27a46":"code","a0f6861d":"code","8a1852c9":"code","4b207c92":"code","0eff454b":"code","a02a27aa":"code","0c70ab86":"code","c0086127":"code","3430dc0e":"code","b216e1e8":"code","22c4cc02":"code","8dbd9ec9":"code","2711915f":"code","57aaf012":"code","66ddf703":"code","52117e26":"code","813ac164":"code","36a4b0cd":"code","c92a11aa":"markdown","367bcff2":"markdown","21507544":"markdown","6be8068a":"markdown","51035637":"markdown","63416ef4":"markdown","c3359e17":"markdown","28f6bc6f":"markdown","933db735":"markdown","61cc132c":"markdown","19b22c3a":"markdown","40c9c870":"markdown","9a66529e":"markdown","5c247a5d":"markdown","3a8daab3":"markdown","de8e0c01":"markdown","4722363a":"markdown","bf42eb0a":"markdown","74bfe35a":"markdown","67dafcf9":"markdown","a5e0d0f1":"markdown","bbc7d015":"markdown"},"source":{"e81cb467":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ac451da6":"import sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os","2871cd7f":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5fe6b1cb":"df =  pd.read_csv(\"..\/input\/amazon-fine-food-reviews\/Reviews.csv\")\ndf.dataframeName = 'Reviews.csv'\nprint(\"Columns : \", df.columns.values)\nprint(\"Shape of data : \", df.shape)","b6c4afaa":"df.describe()","d6b4c7da":"print(\"Shape of before : \", df.shape)\ndf.drop_duplicates(subset={\"UserId\",\"Time\",'HelpfulnessNumerator',\n                           'HelpfulnessDenominator',\"Summary\",\"Text\"},\n                       keep='first', inplace=True)\nprint(\"Shape of after: \", df.shape)","002491ef":"df[\"Score\"].hist()","890bd61c":"df[\"Score\"].value_counts().values \/ df.shape[0]","536da4f3":"def re_score(x):\n    if x <= 3:\n        return 0\n    else:\n        return 1\ndf[\"Score\"] = df[\"Score\"].apply(re_score)","1d474a90":"user_review_counts =  df.groupby('UserId').count()[\"Id\"].value_counts()","dcb42c66":"x = user_review_counts.index\ny = user_review_counts.values\nplt.plot(x, y)\n#plt.xlim(0,160)","1b89e49a":"x = user_review_counts.index\ny = user_review_counts.values\nplt.plot(x, y)\nplt.xlim(0,100) #To make a better picture in our mind about the distribution, we limit our x axis till 100","9e366826":"x = user_review_counts.index\ny = user_review_counts.values\nplt.plot(x, y)\nplt.xlim(0,15)","0c60225a":"df[\"Text\"] = df[\"Text\"].apply(lambda x: x.lower())\n\n# While replacing Summary data it was seen that some values had float points instead\n# of text, so we had to modified\ndf[\"Summary\"] = df[\"Summary\"].apply(lambda x: x.lower() if x.__class__ == \"\".__class__ else \"\")","e8275ad4":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\n\nfrom nltk.corpus import stopwords\n\nhttp_stopwords = [\"br\", \"http\", \"https\", \"span\"]\n\nSTOPWORDS = http_stopwords + list(stopwords.words(\"english\")) + ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]\n\nSTOPWORDS = set(STOPWORDS)","fd7df2b9":"# https:\/\/stackoverflow.com\/a\/47091490\/\nimport re\n\ndef decontract_and_clean(phrase):\n    # specific\n    phrase = re.sub(\"[^a-zA-Z]\",  \" \", str(phrase))\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = phrase.replace('\\\\r', ' ')\n    phrase = phrase.replace('\\\\\"', ' ')\n    phrase = phrase.replace('\\\\n', ' ')\n    phrase = re.sub('[^A-Za-z0-9]+', ' ', phrase)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    phrase = ' '.join(e for e in phrase.split() if e.lower() not in STOPWORDS)\n    return phrase","d8e43b74":"df[\"Text\"] = df[\"Text\"].apply(decontract_and_clean)\ndf[\"Summary\"] = df[\"Summary\"].apply(decontract_and_clean)","a19fd67a":"from nltk.stem import WordNetLemmatizer\n  \nlemmatizer = WordNetLemmatizer()\n\ndf[\"Text\"] = df[\"Text\"].apply(lemmatizer.lemmatize)\ndf[\"Summary\"] = df[\"Summary\"].apply(lemmatizer.lemmatize)","027457d7":"parts_of_speech_counts = {}\n\nfor sentence_pos_tagged in tqdm(text_pos_tagged):\n    \n    for word_pos_tagged in sentence_pos_tagged:\n        #word = word_pos_tagged[0]\n        pos = word_pos_tagged[1]\n        if pos in parts_of_speech_counts:\n            parts_of_speech_counts[pos] += 1\n        else:\n            parts_of_speech_counts[pos] = 1\n            \nprint(parts_of_speech_counts)\n\nplt.xticks(rotation=90)\nplt.bar([key for key, value in parts_of_speech_counts.items()],\n        [value for key, value in parts_of_speech_counts.items()])\nplt.show()","6b65bc48":"word_pos_dictionary = {}\n\n\nfor sentence_pos_tagged in tqdm(text_pos_tagged):\n    \n    \n    for word_pos_tagged in sentence_pos_tagged:\n        #word = word_pos_tagged[0]\n        word = word_pos_tagged[0]\n        pos = word_pos_tagged[1]\n        if word not in word_pos_dictionary:\n            word_pos_dictionary[word] = pos \n            \nprint(len(word_pos_dictionary))\nprint(word_pos_dictionary[\"large\"])","4abd09e8":"def remove_non_relevant_pos(sentense):\n    \n    new_sentense = []\n    allowed_pos = [\"JJ\", \"DT\", \"JJR\", \"JJS\", \"IN\", \n                    \"VB\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\" ,\"VBG\" ,\n                    \"RB\", \"RBR\", \"RBS\"]\n    for word in sentense.split():\n        try:\n            if word_pos_dictionary[word] in allowed_pos:\n                new_sentense.append(word)\n        except:\n            new_sentense.append(word)\n            \n    return \" \".join(new_sentense)","06ae8a7f":"for sentense in df[\"Text\"].iloc[:10]:\n    \n    print(sentense)\n    print(  remove_non_relevant_pos( sentense )  )\n    print(\"-\"*30)","a7cdeba9":"import gensim\n\nour_dictionary = gensim.corpora.Dictionary(df[\"Text\"].apply( lambda x: x.split() ) )\nour_bow_corpus = [our_dictionary.doc2bow(text) for text in df[\"Text\"].apply(lambda x: x.split())]\n\nour_dictionary[our_bow_corpus[1][1][1]]","1f962744":"lda_model = gensim.models.LdaMulticore(our_bow_corpus, num_topics=10, id2word=our_dictionary, passes=2, workers=2)","91527071":"lda_model.print_topics(-1)","7daa9044":"def topic_vector(sentense):\n    bow = our_dictionary.doc2bow( sentense.split() )\n\n    topic_vector = lda_model.get_document_topics(bow)\n\n    return topic_vector\n\nprint(topic_vector(df[\"Text\"].iloc[9]))","45015994":"X = df[[\"Text\", \"Summary\"]]\ny = df[\"Score\"]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3, random_state=43)\n#X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=.3)","f2d853f0":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer_1 = TfidfVectorizer(ngram_range=(1,2),min_df=10,max_features=5000)\ntfidf_vectorizer_1.fit(X_train[\"Text\"])\ntfidf_vectorizer_1_features = tfidf_vectorizer_1.get_feature_names()\n\n# Transforming Train\nX_train_text_tfidf_vectorizer_1_transformed = tfidf_vectorizer_1.transform(X_train[\"Text\"])\nX_train_summary_tfidf_vectorizer_1_transformed = tfidf_vectorizer_1.transform(X_train[\"Summary\"])\n\n# Transforming Test\nX_test_text_tfidf_vectorizer_1_transformed = tfidf_vectorizer_1.transform(X_test[\"Text\"])\nX_test_summary_tfidf_vectorizer_1_transformed = tfidf_vectorizer_1.transform(X_test[\"Summary\"])\n\nX_test_text_tfidf_vectorizer_1_transformed = tfidf_vectorizer_1.transform(X_test[\"Text\"])\nX_test_summary_tfidf_vectorizer_1_transformed = tfidf_vectorizer_1.transform(X_test[\"Summary\"])\n\nprint(X_train_text_tfidf_vectorizer_1_transformed.shape)\nprint(X_train_summary_tfidf_vectorizer_1_transformed.shape)\nprint(X_test_text_tfidf_vectorizer_1_transformed.shape)\nprint(X_test_summary_tfidf_vectorizer_1_transformed.shape)","42d2e86f":"from scipy.sparse import hstack\nX_train_tfidf = hstack((X_train_text_tfidf_vectorizer_1_transformed,\n                      X_train_summary_tfidf_vectorizer_1_transformed))\n\nX_test_tfidf = hstack((X_test_text_tfidf_vectorizer_1_transformed,\n                      X_test_summary_tfidf_vectorizer_1_transformed))","cee33c62":"try:\n    del X_train_text_tfidf_vectorizer_1_transformed\n    del X_train_summary_tfidf_vectorizer_1_transformed\n    del X_test_text_tfidf_vectorizer_1_transformed\n    del X_test_summary_tfidf_vectorizer_1_transformed\nexcept:\n    pass","a1da2783":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\ntrain_GridSearchCV = False\nif train_GridSearchCV:\n    lr_model = LogisticRegression()\n\n    params = { 'C': [0.001, 0.01, 0.1, 1, 10] }\n\n    grid_search = GridSearchCV(estimator = LogisticRegression(),\n                               param_grid = params,\n                               scoring = 'f1',\n                               return_train_score=True,\n                               cv=3)\n\n    grid_search.fit(X_train_tfidf, y_train)","063d7b49":"if train_GridSearchCV:\n    lr_model_tfidf = grid_search.best_estimator_\nelse:\n    lr_model_tfidf = LogisticRegression(C=1, max_iter=1000)","cb47344f":"lr_model_tfidf.fit(X_train_tfidf, y_train)","d1a3f804":"y_train_predicted = lr_model_tfidf.predict(X_train_tfidf)\ny_test_predicted = lr_model_tfidf.predict(X_test_tfidf)\n\nfrom sklearn import metrics\n\nlr_model_tfidf_confusion_matrix_train = metrics.confusion_matrix(y_train, y_train_predicted)\nlr_model_tfidf_classification_report_train = metrics.classification_report(y_train,\n                                                                     y_train_predicted)\n\nlr_model_tfidf_confusion_matrix_test = metrics.confusion_matrix(y_test, y_test_predicted)\nlr_model_tfidf_classification_report_test = metrics.classification_report(y_test,\n                                                                     y_test_predicted)","f5d7cb7e":"import pickle\nwith open(\"lr_model_tfidf_classification_report_test.pkl\", \"wb\") as f:\n    pickle.dump(lr_model_tfidf_classification_report_test, f)","fbb1d30e":"print(\"Confusion Matrix : \")\nprint(\"=\"*30 + \" Train \" + \"=\"*30)\nprint(lr_model_tfidf_confusion_matrix_train)\nprint(\"=\"*30 + \" Test \" + \"=\"*30)\nprint(lr_model_tfidf_confusion_matrix_test)","4a16ea14":"print(\"Classification Report\")\nprint(\"=\"*30 + \" Train \" + \"=\"*30)\nprint(lr_model_tfidf_classification_report_train)\nprint(\"=\"*30 + \" Test \" + \"=\"*30)\nprint(lr_model_tfidf_classification_report_test)","9abd4988":"from gensim.models import Word2Vec","f472f245":"split_words_text_train = X_train['Text'].apply(lambda x: x.split())\nsplit_words_summary_train = X_train['Summary'].apply(lambda x: x.split())\n\nsplit_words_text_test = X_test['Text'].apply(lambda x: x.split())\nsplit_words_summary_test = X_test['Summary'].apply(lambda x: x.split())","7bd94a19":"print(split_words_text_train.iloc[0], \"\\n\",  split_words_summary_train.iloc[0])\ncorpus_for_word2vec_training = split_words_text_train + split_words_summary_train\nprint(corpus_for_word2vec_training.iloc[0])","4f6a5158":"model_self_trained = Word2Vec(corpus_for_word2vec_training,\n                              vector_size=100,\n                              window=10,\n                              min_count=5,\n                             workers=4)\ntry:\n    del corpus_for_word2vec_training\n    del split_words_text_train\n    del split_words_summary_train\n    del split_words_text_test\n    del split_words_summary_train\nexcept:\n    pass","20d27a46":"print(\"No of vectors in our Model : \", model_self_trained.wv['words'].shape[0])\nprint(\"No of words in vocab of Model : \", len(model_self_trained.wv.key_to_index))","a0f6861d":"def word2vec_transformer(data, model):\n    \n    word2vec_avg = []\n    model_wv = model.wv\n    for sentence_words in tqdm(data):\n        vector = np.zeros(100)\n        counter = 0\n        for word in sentence_words:\n            if model_wv.has_index_for(word):\n                vector = vector + model_wv.get_vector(word)\n                counter = counter + 1\n        if counter != 0:\n            vector = vector \/ counter\n        word2vec_avg.append(vector)\n        \n    return word2vec_avg","8a1852c9":"X_train_text_word2vec_self_avg = word2vec_transformer(X_train[\"Text\"], model_self_trained)\nX_train_summary_word2vec_self_avg = word2vec_transformer(X_train[\"Summary\"], model_self_trained)\nX_test_text_word2vec_self_avg = word2vec_transformer(X_test[\"Text\"], model_self_trained)\nX_test_summary_word2vec_self_avg = word2vec_transformer(X_test[\"Summary\"], model_self_trained)","4b207c92":"X_train_word2vec_self = np.hstack((X_train_text_word2vec_self_avg, X_train_summary_word2vec_self_avg))\nX_test_word2vec_self = np.hstack((X_test_text_word2vec_self_avg, X_test_summary_word2vec_self_avg))","0eff454b":"try:\n    del X_train_text_word2vec_self_avg\n    del X_train_summary_word2vec_self_avg\n    del X_test_text_word2vec_self_avg\n    del X_test_summary_word2vec_self_avg\nexcept:\n    pass","a02a27aa":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\ndo_GridSearchCV_training = False\n\nif do_GridSearchCV_training:\n    lr_model = LogisticRegression()\n\n    params = { 'C': [0.001, 0.01, 0.1, 1, 10],\n               'max_iter' : [500]}\n\n    grid_search = GridSearchCV(estimator = LogisticRegression(),\n                               param_grid = params,\n                               scoring = 'f1',\n                               return_train_score=True,\n                               cv=3)\n\n    grid_search.fit(X_train_word2vec, y_train)\nelse:\n    print(\"Best Parameter from GridSearchCv has been found to be C : 0.01\")","0c70ab86":"#lr_model_word2vec = grid_search.best_estimator_\nlr_model_word2vec_self = LogisticRegression(C=1, max_iter=1000, class_weight= {0:1, 1:5})\nlr_model_word2vec_self.fit(X_train_word2vec_self, y_train)","c0086127":"y_train_predicted = lr_model_word2vec_self.predict(X_train_word2vec_self)\ny_test_predicted = lr_model_word2vec_self.predict(X_test_word2vec_self)\n\nfrom sklearn import metrics\n\nlr_model_word2vec_self_confusion_matrix_train = metrics.confusion_matrix(y_train, y_train_predicted)\nlr_model_word2vec_self_classification_report_train = metrics.classification_report(y_train,\n                                                                     y_train_predicted)\n\nlr_model_word2vec_self_confusion_matrix_test = metrics.confusion_matrix(y_test, y_test_predicted)\nlr_model_word2vec_self_classification_report_test = metrics.classification_report(y_test,\n                                                                     y_test_predicted)","3430dc0e":"import pickle\nwith open(\"lr_model_word2vec_self_classification_report_test.pkl\", \"wb\") as f:\n    pickle.dump(lr_model_word2vec_self_classification_report_test,f)","b216e1e8":"print(\"Confusion Matrix : \")\nprint(\"=\"*30 + \" Train \" + \"=\"*30)\nprint(lr_model_word2vec_self_confusion_matrix_train)\nprint(\"=\"*30 + \" Test \" + \"=\"*30)\nprint(lr_model_word2vec_self_confusion_matrix_test)","22c4cc02":"print(\"Classification Report\")\nprint(\"=\"*30 + \" Train \" + \"=\"*30)\nprint(lr_model_word2vec_self_classification_report_train)\nprint(\"=\"*30 + \" Test \" + \"=\"*30)\nprint(lr_model_word2vec_self_classification_report_test)","8dbd9ec9":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\n\nclf = MultinomialNB()\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\nparameters ={'alpha':[0.0001, 0.001, 0.01, 0.1, 1, 10]}\ngs = GridSearchCV(clf , parameters, cv=3, scoring='accuracy',return_train_score=True)\ngs.fit(X_train_tfidf, y_train);","2711915f":"gs.best_estimator_","57aaf012":"nbc_model = MultinomialNB(alpha=0.1)\nnbc_model.fit(X_train_tfidf, y_train)","66ddf703":"y_train_pred = nbc_model.predict(X_train_tfidf)\ny_test_pred = nbc_model.predict(X_test_tfidf)\n\nfrom sklearn import metrics\n\nnbc_model_tfidf_confusion_matrix_train = metrics.confusion_matrix(y_train, y_train_predicted)\nnbc_model_tfidf_classification_report_train = metrics.classification_report(y_train,\n                                                                     y_train_predicted)\n\nnbc_model_tfidf_confusion_matrix_test = metrics.confusion_matrix(y_test, y_test_predicted)\nnbc_model_tfidf_classification_report_test = metrics.classification_report(y_test,\n                                                                     y_test_predicted)","52117e26":"import pickle\nwith open(\"nbc_model_tfidf_classification_report_test.pkl\", \"wb\") as f:\n    pickle.dump(nbc_model_tfidf_classification_report_test, f)","813ac164":"print(\"Confusion Matrix : \")\nprint(\"=\"*30 + \" Train \" + \"=\"*30)\nprint(nbc_model_tfidf_confusion_matrix_train)\nprint(\"=\"*30 + \" Test \" + \"=\"*30)\nprint(nbc_model_tfidf_confusion_matrix_test)","36a4b0cd":"print(\"Classification Report\")\nprint(\"=\"*30 + \" Train \" + \"=\"*30)\nprint(nbc_model_tfidf_classification_report_train)\nprint(\"=\"*30 + \" Test \" + \"=\"*30)\nprint(nbc_model_tfidf_classification_report_test)","c92a11aa":"We see that we are having more nouns followed by Adjectives and then Verb of past tense\nWe have now decided that we would only include those parts of speach that are more relavant to determine sentimnetys, which are :\n\n**Adjectives\nadverb\nDeterminers\nprepositions**\n\nSo just as we romeved stopwords we would now remove all the words which are not having nouns as above\n\nFirst creating list of all words in our corpus with there pos tags","367bcff2":"classification Setup 2 :\n---- word2vec self trained with 150 vectors\n---- Features will be averaged word2vec\n---- Logistic Regression with GridCVSearch\n","21507544":"**STOPWORDS removal**\n\nI have decided to remove following group of stopwords from our text data\n\nstopwords from nltk\nstopwords got from this github repo : # https:\/\/gist.github.com\/sebleier\/554280\nhttp stopwords","6be8068a":"**Lemmetization**","51035637":"Training\nThe model we got from GridCVSearch is a model with value of C as 1.","63416ef4":"**LDA**\n\nNow let's try to do LDA on our Text and see what kind of topics we see","c3359e17":"We see that our data is having too much difference in the frequncy of our data","28f6bc6f":"**Confusion Matrix and Classification report**","933db735":"Our data size has drastically reduced to (396254, 10)","61cc132c":"***Parts of Speech Tagging***\n*\n\nWe will also remove non relavant pos rom our document","19b22c3a":"#### Below GridSearchCv will take too much time to run, to do not run, parameters which were to be found has been found. Only train if necessary","40c9c870":"Logistic Regression with GridCVSearch\nWe would use Logistic Regression with l2 regularizationo with varyinig values of C\nAs our data was highly imbalanced we would be using f1 as our scoring method","9a66529e":"## Logistic Regression with GridCVSearch\n\n* We would use Logistic Regression with l2 regularizationo with varyinig values of C(as we did earlier)\n* As our data was highly imbalanced we would be using f1 as our scoring method","5c247a5d":"Data Pre processing","3a8daab3":"Opinion on LDA for this dataset\nWe see above after printing topics that,\n\nby LDA we are able to tell about the product, like coffee, or something to eat, or somethingt to eat\nNearly all topics are having words like good, better, best etc. Hence not much distintion on sentiments of the review(however note that LDA is mainly used for topic modeling)\nLDA will not be a better fir for Sentimnet Analysis with any of the algorithms experimented below\n","de8e0c01":"Data Spliting to train, test,\nValidation set is extracted from train later in the notebook, it maybe done automatically by GrdiSearchCV or manually by us","4722363a":"Classification setup 1 :\n---- Tf-Idf with upto 2-grams\n---- Logistic Regression","bf42eb0a":"# Classification Setup 4\n## Naieve Bais Classifier with GridSearch\n* GridCVSearch will find out best value of alpha","74bfe35a":"It was decided that i would rate our scores in 2 levbel instead of 5 levels. i.e instead of scores being 1,2,3,4,5 i will do 1,0. If score > 3, then score = 1 if score =< 3, then score = 0\n\nThis was done to give us so that my model would get more amount of data on each target value, as i saw previously that my data was having very low no of rows with score = 2, 3, 1. So i clubed togther 3, 2, 1 and made them 0, and clubed together 4, 5 to make them 1","67dafcf9":"Setup 1 result:\nBest value of C of l2 regularized LR model: 1\nTrain Accuracy : 93%\nTest Accuracy : 92%\n","a5e0d0f1":"We see above that we are getting a distribution closely matching of that of poison distribution. This also suggests us the universal pareto law being in action on the no of reviews people leave at the site, and that is that 20% of people are responsible for 80% of the reviews\nbelow we again limit our x axis to 15, to have a closer look at top 15 frequencies","bbc7d015":"Removinig data Duplication****\n\nAfter reviewing the data file in excel sheet, it was found that same review was seen for products with different product id, and it was understood by our personal experience that that it might be the case that different variants of same product had different productId and hence review was duplicated among those vraiants, so we have top remove those first. We can remove by dropping duplicats on the basis of text and summary, however, we thought to take all the attributes as subset to drop the duplicates"}}