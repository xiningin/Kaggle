{"cell_type":{"878bca9d":"code","ad24ed58":"code","2f07df94":"code","02e6936e":"code","710c2bbb":"code","0b90f0a8":"code","146ef4ef":"code","c617e7de":"code","33146d8f":"code","c784d0b7":"code","32d94304":"code","976818ff":"code","a8eca8a7":"code","db013f3c":"code","5045f468":"markdown"},"source":{"878bca9d":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split \nimport lightgbm as lgb","ad24ed58":"t_res = pd.read_csv('..\/input\/wdatafiles\/WNCAATourneyCompactResults.csv')\nt_ds = pd.read_csv('..\/input\/wdatafiles\/WNCAATourneySeeds.csv')\nsub = pd.read_csv('..\/input\/WSampleSubmissionStage1.csv')","2f07df94":"t_ds['seed_int'] = t_ds.Seed.apply(lambda a : int(a[1:3]))","02e6936e":"drop_lbls = ['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT']\nt_ds.drop(labels=['Seed'], inplace=True, axis=1)\nt_res.drop(labels=drop_lbls, inplace=True, axis=1)","710c2bbb":"ren1 = {'TeamID':'WTeamID', 'seed_int':'WS'}\nren2 = {'TeamID':'LTeamID', 'seed_int':'LS'}","0b90f0a8":"df1 = pd.merge(left=t_res, right=t_ds.rename(columns=ren1), how='left', on=['Season', 'WTeamID'])\ndf2 = pd.merge(left=df1, right=t_ds.rename(columns=ren2), on=['Season', 'LTeamID'])\n\ndf_w = pd.DataFrame()\ndf_w['dff'] = df2.WS - df2.LS\ndf_w['rsl'] = 1\n\ndf_l = pd.DataFrame()\ndf_l['dff'] = -df_w['dff']\ndf_l['rsl'] = 0\n\ndf_prd = pd.concat((df_w, df_l))","146ef4ef":"X = df_prd.dff.values.reshape(-1,1)\ny = df_prd.rsl.values","c617e7de":"X_test = np.zeros(shape=(len(sub), 1))","33146d8f":"for ind, row in sub.iterrows():\n    yr, o, t = [int(x) for x in row.ID.split('_')]  \n    X_test[ind, 0] = t_ds[(t_ds.TeamID == o) & (t_ds.Season == yr)].seed_int.values[0] - t_ds[(t_ds.TeamID == t) & (t_ds.Season == yr)].seed_int.values[0]","c784d0b7":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1,shuffle = True, random_state=2019)","32d94304":"from sklearn.metrics import f1_score\nparams = {'num_leaves': 70,\n          \"boosting\": \"gbdt\",\n          'min_data_in_leaf': 120,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          }\n\nf1s_valid = []\nf1s_train = []\ndtrain = lgb.Dataset(X_train, y_train)\ndvalid = lgb.Dataset(X_val, y_val, reference=dtrain)\nbst = lgb.train(params, dtrain, 1000, valid_sets=dvalid, verbose_eval=200,\n   early_stopping_rounds=200)\n\n#clf = lgb.LGBMClassifier(n_estimators=50, silent=True).fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=200, verbose=100)\n#rg = lgb.LGBMRegressor(params,n_estimators=50, silent=True).fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=200, verbose=100)\ny_pred_valid = np.where(bst.predict(X_val, num_iteration= bst.best_iteration) > 0.5, 1, 0)\ny_pred_train = np.where(bst.predict(X_train, num_iteration= bst.best_iteration) > 0.5, 1, 0)\n\nf1s_valid.append(f1_score(y_val, y_pred_valid))\nf1s_train.append(f1_score(y_train, y_pred_train))\n#model = AdaBoostClassifier(n_estimators=200, learning_rate=1.4)\n#model = sklearn.ensemble.GradientBoostingClassifier(learning_rate=0.01,max_leaf_nodes = 120)\n#model.fit(X_train, y_train)","976818ff":"print('CV mean train score: {0:.4f}, std: {1:.4f}.'.format(np.mean(f1s_train), np.std(f1s_train)))","a8eca8a7":"test_pred = bst.predict(\n    X_test)\nprint('Log Loss:', test_pred)","db013f3c":"sub.Pred = test_pred   \nsub.to_csv('submission.csv', index=False)","5045f468":"## This kernel base on [Alexander Teplyuk](https:\/\/www.kaggle.com\/ateplyuk\/lgbm-str-w) here I tuned min_data_in_leaf to 120, add random state and shuffle = True"}}