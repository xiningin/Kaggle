{"cell_type":{"5acda4a3":"code","09ce2865":"code","5c9bb510":"code","bb271ab4":"code","812d4a9b":"code","3df45a51":"code","97498c03":"code","9c264ef0":"code","56f0f960":"code","76e216ca":"code","eef561d3":"code","88cd5e1f":"code","b6faa8d6":"code","f90085d2":"code","e98be80e":"code","2151869c":"code","2405b725":"code","133b44e6":"code","dd711967":"code","c64749e5":"code","57570138":"code","66a56d35":"code","b6397a53":"code","dff51b0f":"code","f4018556":"code","dea4f80d":"code","a363c825":"code","df5d792d":"markdown","9d54f4e7":"markdown","53f71287":"markdown","b28a981f":"markdown","d2ad1cd4":"markdown","76ac3875":"markdown","a1278ec3":"markdown","b7572e7d":"markdown","0ab2cc8c":"markdown","e7c43336":"markdown","564e88e3":"markdown","e40038dd":"markdown","7399f575":"markdown","485941e7":"markdown","77245ebe":"markdown","01f48b4b":"markdown","c2ebfeee":"markdown","94e4b819":"markdown","c111c752":"markdown","31f2c507":"markdown","6fa7581a":"markdown","b1a177bd":"markdown","029bea58":"markdown","7ec09118":"markdown","20d7b880":"markdown","35da677b":"markdown","b57497a6":"markdown","651d58b9":"markdown"},"source":{"5acda4a3":"# Load all of the libraries used for this notebook \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load in the data\ndf = pd.read_csv(r'..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\n\n# inspect the first few rows of the data\ndf.head()","09ce2865":"# Convert quality to 1 if >= 7, else 0.\ndf['quality'] =(df['quality']>= 7).astype(int)  \n    ","5c9bb510":"df.info()","bb271ab4":"df['quality'].value_counts()","812d4a9b":"sns.pairplot(df, hue='quality')","3df45a51":"df['quality'].replace({1:'Good', 0:'Bad'}, inplace= True)","97498c03":"sns.boxplot(data=df, y='fixed acidity', x='quality')","9c264ef0":"sns.boxplot(data=df, y= 'volatile acidity', x='quality')","56f0f960":"sns.boxplot(data=df, y= 'citric acid', x='quality')","76e216ca":"sns.boxplot(data=df, y= 'residual sugar', x='quality')","eef561d3":"sns.boxplot(data=df, y='chlorides', x='quality')","88cd5e1f":"sns.boxplot(data=df, y= 'free sulfur dioxide', x='quality')","b6faa8d6":"sns.boxplot(data=df, y= 'density', x='quality')","f90085d2":"sns.boxplot(data=df, y= 'pH', x='quality')","e98be80e":"sns.boxplot(data=df, y= 'sulphates', x='quality')","2151869c":"sns.boxplot(data=df, y= 'alcohol', x='quality')","2405b725":"# Change quality back to 1 for good and 0 for bad\ndf['quality'].replace({'Good': 1,'Bad': 0}, inplace= True)\n\n# Set dimentions for the visual\nfig, ax = plt.subplots(figsize=(25,5))\n\n# Create correlation plot\nmask = np.triu(np.ones_like(df.corr()))\nsns.heatmap(df.corr(), annot=True, mask=mask, cmap='Reds')\nplt.suptitle('Wine Feature Correlation', fontsize=20)","133b44e6":"# Import libraries and tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom imblearn.over_sampling import SMOTE","dd711967":"# Separate target from features\nX = df.drop(['quality'], axis=1)\ny = df['quality']\n\n# Create an Instance of Standard Scale\nsc = StandardScaler()\n\n# Scale The data\nX = sc.fit_transform(X)\n\n# Break data up into testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","c64749e5":"# Create an instance of SMOTE\nsm = SMOTE(random_state=101)\n\n# Balance the Classes\nX_res, y_res = sm.fit_resample(X_train, y_train)","57570138":"# Create Instance of Random Forest and fit the data\nrfc = RandomForestClassifier(n_estimators=100, random_state = 101).fit(X_res, y_res)\n\n#  Create Predictions\nrfc_predictions = rfc.predict(X_test)\n\n# Print out Confusion Matrix and Classification Report\nprint(confusion_matrix(y_test,rfc_predictions))\nprint('\\n')\nprint(classification_report(y_test,rfc_predictions))","66a56d35":"# Create Instance of Support Vector Classification and fit the data\nsvc = SVC(probability=True).fit(X_res, y_res)\n\n#  Create Predictions\nsvc_predictions = svc.predict(X_test)\n\n# Print out Confusion Matrix and Classification Report\nprint(confusion_matrix(y_test,svc_predictions))\nprint('\\n')\nprint(classification_report(y_test,svc_predictions))","b6397a53":"# Create param_grid, a dictionary of things to test\nparam_grid =  {\n    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],\n    'kernel':['linear', 'rbf'],\n    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n}\n\n# Create an instance of Grid Search\ngrid = GridSearchCV(SVC(probability=True),param_grid,refit=True,verbose=3)\n\n# Test out the parameters\ngrid.fit(X_res, y_res)","dff51b0f":"# Print out the best parameters for reference\ngrid.best_params_","f4018556":"# Create Predictions using the best parameters\ngrid_predictions = grid.predict(X_test)\n\n\n# Print out Confusion Matrix and Classification Report\nprint(confusion_matrix(y_test,grid_predictions))\nprint('\\n')\nprint(classification_report(y_test,grid_predictions))","dea4f80d":"# Create random guessing probability and probability for the models\nrandom_probs = [0 for _ in range(len(y_test))]\nrfc_probs = rfc.predict_proba(X_test)\nsvc_probs = svc.predict_proba(X_test)\ngrid_probs = grid.predict_proba(X_test)\n\n# Keep only what I need\nrfc_probs = rfc_probs[:, 1]\nsvc_probs = svc_probs[:, 1]\ngrid_probs = grid_probs[:, 1]\n\n# Calculate AUC\nrandom_auc = roc_auc_score(y_test, random_probs)\nrfc_auc = roc_auc_score(y_test, rfc_probs)\nsvc_auc = roc_auc_score(y_test, svc_probs)\ngrid_auc = roc_auc_score(y_test, grid_probs)\n\n# False Positive and True Positive Rates for ROC Curve\nrandom_fpr, random_tpr, _ = roc_curve(y_test, random_probs)\nrfc_fpr, rfc_tpr, _ = roc_curve(y_test, rfc_probs)\nsvc_fpr, svc_tpr, _ = roc_curve(y_test, svc_probs)\ngrid_fpr, grid_tpr, _ = roc_curve(y_test, grid_probs)","a363c825":"# Set size and style of plot\nplt.style.use('fivethirtyeight')\nfig, ax = plt.subplots(figsize=(10,10))\n\n# Create the plot\nplt.plot(random_fpr, random_tpr, linestyle='--', label = 'Random Guessing (AUROC = %0.2f)' % random_auc)\nplt.plot(rfc_fpr, rfc_tpr, marker='.', label = 'Random Forest (AUROC = %0.2f)' % rfc_auc)\nplt.plot(svc_fpr, svc_tpr, marker='.', label = 'SVC (AUROC = %0.2f)' % svc_auc)\nplt.plot(grid_fpr, grid_tpr, marker='.', label = 'Grid (AUROC = %0.2f)' % grid_auc)\n\n# Set title, axis names and display legend\nplt.title('ROC AUC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()","df5d792d":"### 4.5 Boxplot: Citric Acid\n>Good wine tends to have a higher citric acid value.  Additionally, the box itself is much smaller, but the whiskers are similar to those on the plot for the bad wine.   The box represents 50% of the values, so that means that half of all the good wines have a citric acid value in a small range.","9d54f4e7":"### 4.12 Boxplot: Alcohol\n>There is a much higher alcohol value in the good wine than in the bad wine.  There are a lot of jokes that could be made here, but I\u2019m sure there is some reason behind this.  As I mentioned previously, I don\u2019t know that much about wine, but I do know that the fermentation process converts sugar to alcohol, and that I previously showed that the residual sugar does not seem to be a good indicator as to whether a wine is good or not.  My guess is that there is some relationship with how well the grapes ferment in the wine making process.","53f71287":"### 4.13 Correlation Heat Map\n>Before I create the correlation plot, I am going to change the quality indicator back to 1 and 0. <br><br>\n>Correlation plots compare each feature in the dataframe against each other to indicate any sort of correlation that may exist.  In this chart, the most important part is the bottom row which represents quality.  A darker box indicates a strong correlation between two variables, while a lighter box indicates a weaker correlation.  I also included annotations to remove the guess work.  This chart suggests that the two features with the greatest correlation in good wine is citric acid and alcohol content.  This is very much in line with what I demonstrated with the boxplots. ","b28a981f":"### 4.4 Boxplot: Volatile Acidity\n>Good wine tends to have a slightly lower volatile acidity than bad wines.","d2ad1cd4":"### 5.3 Balance Classes Using SMOTE","76ac3875":"# SECTION 10: Conclusion\nNow that I've examined all of the data and I've created 3 different models, it's time offer some conclusions. \n\nOverall, I think the Random Forest Model is the best, because it has the best AUC score.  All of the models are better than random guessing and no model is perfect, but they also all have room for improvement.  If I were to make red wine suggestions based on what I found here, I would suggest buying a wine that has a lot of alcohol and high citric acid value.\n\nThank you for reading through this project. I hope that this was helpful!","a1278ec3":"### 4.9 Boxplot: Density\n>There is a noticeable difference between the density of a good wine and a bad wine.  The good wines median just below .996, while the bad wines find their median just above it.  The difference here is slight, but the pattern is noticeable.  Additionally, the 50% of the wines surrounding the median value have a larger range than those of the bad wine.","b7572e7d":"### 5.1 Import Machine Learning Libraries and Tools","0ab2cc8c":"# SECTION 8: Use Grid Search to See if Support Vector Classification Can Be Improved","e7c43336":"# SECTION 9: ROC and AUC\nThis is another way to compare the models","564e88e3":"### 4.1 Breakdown of Good Wine vs Bad Wine\n> When I split good wine and bad wine at a rating of 7, we can see that there are slightly more than 6 times as many bad wines as there are good.  This is going to be important later when I create some models.  It is important that when you train a model, the classes are balanced.  In our case, this should prevent the models from simply guessing that all the wines are bad.","e40038dd":"### 3.2 Verify that there are no missing values\nThis will be extremely important for modeling.","7399f575":"### 4.10 Boxplot: pH\n>The pH values in good and bad wines are similar, with the good wines having slightly lower values, and a smaller range of values. ","485941e7":"### 4.6 Boxplot: Residual Sugar\n>This plot is interesting because the boxes are nearly identical, except that the bad wine has far more outliers that span a greater range than the good wine.  This might indicate that residual sugar is not a good metric to gauge the quality of wine.","77245ebe":"### 5.2 Break Up the Data","01f48b4b":"### 4.11 Boxplot: Sulphates\n>The median sulphate value is noticeably higher in the good wines.  Even the minimum is significantly higher than minimum of the bad wines.  The good wines do not have nearly as high of a maximum value as the bad wines though, indicating that too much of a good thing becomes a bad thing.","c2ebfeee":"# SECTION 1: Introduction\n\nHello, and welcome to my quality and classification analysis of red wine.  First and foremost, I want to thank UCI Machine Learning for providing this data.  If you would like to look at the data for yourself, you can find it [here](https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009). <br><br>\n\n\nI'm going to do my best to break this down and make it as easy to understand as possible, while keeping it informative.  I have decided to use Python for this analysis but you should be able to get the same or similar results by using R or any other machine learning technology. <br><br>\n\nA few notes about the data before diving in - \n* I am **not** a data scientist, and I'm sure that there are better ways to approach this problem.  I am doing this for fun so I'm not going to stress about creating the best possible model.  I am just going to focus on creating the best model that I can.\n* This dataset is old.  It looks like the data was posted to [kaggle](kaggle.com) 4 years ago, but the data itself is from 2009.\n* I'm not much of a drinker, so I'm basing all my findings off the data.  I don\u2019t know how wine is graded, so I cannot tell you if a wine should be considered good or not. Additionally, the wine has not been labeled, so it would be tough to say which specific wines are good or bad.\n* Taste is subjective.  If you like wine that is classified as bad, then good for you.  Enjoy it!\n* In the description of the data, the author suggests an arbitrary cut point 7 to separate good and bad wine.  I am going to use that suggestion, but this could be open to interpretation.\n\n### Key Terms:\nIf you are not familiar with machine learning, I want to give you a few terms to help you understand what you will see in this notebook.  My intention is not to provide an in-depth guide to machine learning, but I want to make this as accessible as I can for everyone. \n\n> Accuracy<br>\n>> Accuracy = Total Correct Wine Classification \/ Total Number of Guesses <br>\n>> Example:  If the model predicts that 9 out of 10 wines correctly then the equation is 9\/10 and my accuracy is 90%. <br>\n\n> Precision \n>> Precision = Total Correct Wine Classifications \/ Total That The Model Classifies That Way<br>\n>> Example: If the Model predicts that 5 wines are good, and 3 of them are actually good, then the equation is 3\/5 and my precision is 60% <br>\n\n> Recall<br>\n>> Recall = Total Correct Wine Classifications \/ Total Actual <br>\n>> Example: If the model predicts 3 wines were good and there are actually 4 good wines (regardless of how the model classifies them) then the equation is 3\/5 and my recall is 75%\n\n### Notebook Layout\n1. Introduction (That's this part!)\n2. Findings\n3. Load and Prepare Data\n4. Observations\n5. Model Preparation\n6. Create Random Forest Model\n7. Create Support Vector Classification Model\n8. Use Grid Search to See if Support Vector Classification Can Be Improved\n9. ROC and AUC\n10. Conclusion","94e4b819":"### 4.3 Boxplot: Fixed Acidity\n>Good wine tends to have a slightly higher fixed acidity than bad wines. To make the visualisations more readable, I am temporarliy going to change 1 to 'Good' and 0 to 'Bad'","c111c752":"### 4.2 Pairplot of all features\n>A pairplot will allow us to get a quick look at how the features all relate to each other.  In the visual below, blue represents bad wine and orange represents good wine.","31f2c507":"# SECTION 2: Findings\nIn other notebooks I've written, this section was included so that readers would not have to go through all the visuals and code if they did not want to.  In this notebook I have taken a slightly different approach.  I've summarized some of my thoughts here, but I've included more detailed explanations with the code and visuals below.  I've done this because I think in this case, it is important to look at the explanation with the visual at the same time.\n\n#### Section 4:\n> It seems like there are some features that would be better indicators of the quality of a wine.  If I had more time, I would consider running each of the models again without `Residual Sugar` and `Chlorides` because they break down almost identically in the good and bad wines.  I am going to use all of the provided features in my models.  I\u2019ve decided to use all the features because I know very little about wine and I am making the assumption that if a feature was included in this data set, it means that it is used in the real world to grade the quality of a red wine. \n \n#### Sections 6, 7, and 8.\n> I am not going to go into detail about each one of these models.  If you are interested in how each of the models works, there are a lot of great resources online that can do a much better job of explaining this stuff than I can.  Just know that I have used 3 different models to which, if any of them, is the best for this data.\n#### Section 9: ROC and AUC\n> Again, I am not going to go into all the detail here, but the big thing to take away from this is that the lines I generated each represent a model.  The blue dotted line can be thought of as a coin toss, which guesses whether a wine is good 50% of the time.  What we are interested in here us the **Area Under Curve** or *AUC*.   The chart goes from 0-1 on both the y and x-axes.  The AUC represents the percent of the area underneath the line that represents the mode.  The larger the number, the better the model.","6fa7581a":"### 3.1 Convert `quality` to 1\/0 to indicate good\/bad wine.","b1a177bd":"### 4.8 Boxplot: Free Sulfur Dioxide\n>There is a slightly difference between the good and the bad wine in this visual.  The good wines tend to have a lower value, but not by much.","029bea58":"# Section 3: Load and Prep Data\nThis data is already in a very usable format for my purposes.  To get started I am only going to make one change to the data.  I am going to convert the quality column to 1\/0 to indicate if it is a good or bad wine. ","7ec09118":"# SECTION 4: Observations\nI'm going to do some basic visualizations here to demonstrate some differecnces between good wine and bad wine.","20d7b880":"### 4.7 Boxplot: Chlorides\n>Like the residual sugar, the chloride box plots are very similar to each other.  The higher quality wine does appear to have a slightly lower value, but not by much.  There are also fewer outliers.  This feature might also not be the best to gauge wine quality with.","35da677b":"# SECTION 5: Model Preparation\nHere I am going to prepare the data for modeling.  This should be pretty straight forward, as the data is in pretty good shape. \n\n#### 5.1 Import Machine Learning Libraries and Tool\n> There are some additional libraries that need to be imported for prepping the data as well as for creating the modes.  I'm going to import those here\n#### 5.2 Break Up the Data\n> To create models, it is important to separate out data into training and testing data.  \n#### 5.3 Balance Classes Using SMOTE\n> This is very important. For my models to make good predictions, the data it trains on needs to represent balanced classes.  I don't want to get into the weeds on all the math on this, but basically, I am going to create synthetic examples of good wine so that the models I create does not just assume that everything is a bad one.  The method I am going to use for this is called *Synthetic Minority Oversampling Technique*, or **SMOTE**.\n","b57497a6":"# SECTION 6: Create Random Forest Model","651d58b9":"# SECTION 7: Create Support Vector Classification Model"}}