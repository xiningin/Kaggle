{"cell_type":{"d22b4f1f":"code","ed1068de":"code","90adffbc":"code","b777f5e2":"code","b85786bc":"code","f01f1b1b":"code","654edd50":"code","a0620558":"code","22af990c":"code","52750b56":"code","688c4640":"code","444827d2":"code","f71e194b":"code","844e59d0":"code","975461c9":"code","319350c2":"code","44c9306d":"code","9d1a3363":"code","cdf83a2a":"code","c9e6186f":"code","4fc52611":"code","939701ff":"code","707a262d":"code","abc20c05":"code","96e0df4d":"code","98179bca":"code","467cb13d":"code","f2386984":"code","283c7db1":"code","659661b8":"code","4885279e":"code","54327dc1":"code","0f0d3475":"code","7a38b8f0":"code","7d6d20dc":"code","ff2906bc":"code","ffd608df":"code","d3a495cf":"code","47a858cc":"code","ae3acc86":"code","8ec4fc44":"code","65f2807e":"code","bd3c2135":"code","c35b6446":"code","5041c2f9":"code","1422079c":"code","9cf3e1a5":"code","e1118eb2":"markdown","7b313444":"markdown","8858b3b3":"markdown","b2b4f303":"markdown","060d33d3":"markdown","c903c72b":"markdown","dbea5b72":"markdown","f52cd19d":"markdown","4fd041eb":"markdown","2738d75a":"markdown","03380757":"markdown","8823f532":"markdown","96a51f34":"markdown","8299d68b":"markdown","a447a301":"markdown","4b6a179f":"markdown","422d04ef":"markdown","a1637212":"markdown","e94ab0c4":"markdown","5e5112e7":"markdown","4d4b2a2d":"markdown"},"source":{"d22b4f1f":"import torch\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nfrom tqdm.notebook import tqdm","ed1068de":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))# df.set_index('id', inplace=True)\n# \ndf = pd.read_csv('\/kaggle\/input\/emotion-detection-from-text\/tweet_emotions.csv')\n\n# Let's have a look at it\ndf.head()","90adffbc":"df.content.iloc[-10:]","b777f5e2":"df.sentiment.value_counts()\n#nocode is simply no clear emotions in this tweet","b85786bc":"#we want to remove small datasets \n# df = df[~df.sentiment.str.contains('\\|')]   \ndf = df[df.sentiment != 'anger'] #& 'boredom' & 'enthusiasm' & 'empty'\ndf = df[df.sentiment != 'boredom']\ndf = df[df.sentiment != 'enthusiasm']\ndf = df[df.sentiment != 'empty']\ndf = df[df.sentiment != 'sentiment'] #there is sentiment in sentiments!","f01f1b1b":"df.sentiment.value_counts()\n#class imbalance","654edd50":"#build dictionary, key: emotion, value: \npossible_labels = df.sentiment.unique()","a0620558":"label_dict = {}\n#loop over index\nfor index, possible_label in enumerate(possible_labels):\n    label_dict[possible_label] = index\nprint(label_dict)","22af990c":"#build new column for these values\ndf['label'] = df.sentiment.replace(label_dict)\ndf.head()","52750b56":"from sklearn.model_selection import train_test_split","688c4640":"#stratified split\nX_train, X_val, y_train, y_val = train_test_split(df.index.values,\n                                                 df.label.values,\n                                                 test_size = 0.15,\n#                                                  random_state=17,\n                                                 stratify = df.label.values\n                                                 )","444827d2":"df['data_type'] = ['not_set']*df.shape[0]","f71e194b":"df.head()","844e59d0":"df.loc[X_train, 'data_type'] = 'train'\ndf.loc[X_val, 'data_type'] = 'val'","975461c9":"df.groupby(['sentiment', 'label', 'data_type']).count()\n#group by using count","319350c2":"# Install spaCy (run in terminal\/prompt)\n# import sys\n# !{sys.executable} -m pip install spacy\n\n# Download spaCy's  'en' Model\n# !{sys.executable} -m spacy download en\n\n# !pip install -U symspellpy\n\n#for spell and slang correction\n# !pip install gingerit\n# from gingerit.gingerit import GingerIt\n\n#for emoticons\n!pip install emot --upgrade\nimport emot \nemot_obj = emot.core.emot() \n\n# from symspellpy.symspellpy import SymSpell, Verbosity\n# import pkg_resources\nimport re, string, json\n# import spacy","44c9306d":"\ncontraction_list = json.loads(open('\/kaggle\/input\/english-contractions\/english_contractions.json.txt', 'r').read())\ncharacter_entity= {'&lt;3':'heart', '&amp;':'and','&quot;':' quote '}\ncontraction_list = {**contraction_list, **character_entity}\n\n\ndef normalization_pipeline(sentences):\n    print(\"##############################\")\n    print(\"Starting Normalization Process\")\n    sentences = _simplify_punctuation_and_whitespace(sentences) # !!!!! \"      \"\n    sentences = _normalize_contractions(sentences) #also corrects spelling now\n    print(\"Normalization Process Finished\")\n    print(\"##############################\")\n    return sentences\n\n    \ndef _simplify_punctuation_and_whitespace(sentence_list):\n    \"\"\"\n    words with more than 4 all-capital words will get <-EMPW \n    \"\"\"\n    norm_sents = []\n    print(\"Replacing -URL- , Replacing @MENTION and #HASHTAG, Reducing character repetitions, \")\n    print(\"Simplifying punctuation, Removing whitespaces\")\n\n    for sentence in tqdm(sentence_list):\n        sent = _replace_urls(sentence)\n        sent = _mention_hash(sent)\n        sent = _simplify_punctuation(sent)\n        sent = _reduce_repetitions(sent)\n        sent = _normalize_whitespace(sent)\n        norm_sents.append(sent)\n    return norm_sents\n\n\ndef _replace_urls(text):\n    url_regex = r'(https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n    text = re.sub(url_regex, \"-URL-\", text)\n    return text\n\n\ndef _mention_hash(in_str):\n    \"\"\"\n     @MENTIONs and #HASHTAGs will take forms of @men and #has \n    note: BEWARE OF USES OF # AND @ AND SPACES BETWEEN THEM\n    \"\"\"\n    in_str = str(in_str)\n    in_str = re.sub('@\\w+', '@MEN', in_str,flags=re.IGNORECASE) # use @\\w+ for word replacement or @ with space after @MEN for keeping mention\n    in_str = re.sub('#', '#HAS ', in_str,flags=re.IGNORECASE)\n#     in_str = re.sub(r'([\\w])\\1+', r'\\1\\1', in_str) #reduce repeated characters to 2\n    return in_str.strip()\n\n\ndef _simplify_punctuation(text):\n    \"\"\"\n    puntuations like '!!!!!' will be transformed into '!! <-EMPP'\n    This function simplifies doubled or more complex punctuation. The exception is '...'. #?! ??? !!!\n    \"\"\"\n    corrected = str(text)\n    corrected = re.sub(r'([!?,;])\\1+', r'\\1\\1 <-EMPP', corrected) #\\1\\1 makes it to 2 consecutive punctuation\n    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n    return corrected\n\ndef _reduce_repetitions(text):\n    \"\"\"\n    Auxiliary function to help with exxagerated (repeated characters in) words.\n    Examples:\n        woooooords -> woords <-EMPW\n        dooorwaaay -> doorwaay <-EMPW\n        SICK -> sick <-EMPU\n    \"\"\"\n    correction = str(text)\n    for index, words in enumerate(str(text).split()):\n        if _is_EMP_word(words)==True :\n            #insert EMPW after word\n            correction = correction.replace(words, words + ' <-EMPW')\n        if (len(words) > 4) & (words.isupper()==True) & (words[0] not in string.punctuation):\n            correction = correction.replace(words, words + ' <-EMPU')\n    #TODO work on complexity reduction.\n    return re.sub(r'([\\w])\\1+', r'\\1\\1', correction) #\\1\\1 will only keep 2 consecutive characters\n\n\ndef _is_EMP_word(word):\n    \"\"\"\n    True\/ False: checks if the word has 3 consecutive characters\"\"\"\n    count=1\n    if len(word)>1:\n        for i in range(1,len(word)):\n            if word[i] in string.punctuation: #this function is only for words!\n                return False\n            if word[i-1]==word[i]:\n                count+=1\n                if(count>=3):\n                     return True\n            else :\n                if(count>=3):\n                    return True\n                count=1\n    else :\n        return False\n    return False\n\n\ndef _normalize_whitespace(text):\n    \"\"\"\n    normalizes whitespaces, removing duplicates.\n    \"\"\"\n    corrected = str(text)\n    corrected = re.sub(r\"\/\/t\",r\"\\t\", corrected)\n    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n    return corrected.strip(\" \")\n    \n    \n#Substitution of contractions:  -----------------------------------------------------------------------------------------------      \ndef _normalize_contractions(sentence_list):\n    \"\"\"\n    it will correct each word in a sentence for slangs(ginger), emojis -> meaning, entity references and abbreviations(json file) : file can be manually modified above\n    also makes everything lowercase (including EMPW,EMPU, EMPP, URL, etc)\n    \"\"\"\n    #uses contraction_list (a json file) BE SURE TO IMPORT IT ALREADY\n    norm_sents = []\n    print(\"Normalizing contractions, abbreviations, slangs, emojis, character entities\")\n    for sentence in tqdm(sentence_list):\n        norm_sents.append(_normalize_contractions_slang_emoji_entity(sentence))\n    return norm_sents\n\ndef _normalize_contractions_slang_emoji_entity(text):\n    \"\"\"\n    part1:normalizes english contractions.\n    \"\"\"\n    contractions = contraction_list\n    for word in text.split():\n         if word.lower() in contractions:\n            text = text.replace(word, contractions[word.lower()])\n#             print('replacing contraction: '+ word + ' to '+contractions[word.lower()])\n    \"\"\"\n    part 2: using gingerit SMS slang correction:\n    this is too slow and can take many hours for the whole dataset to run\n    \"\"\"\n#     parser = GingerIt()\n#     result=parser.parse(text)\n#     # corrections = result['corrections']\n#     sentence = result['result']\n    sentence = text\n    \"\"\"\n    part3: emoji and character entity reference conversion to meaning\n    \"\"\"\n#     if emot_obj.emoji(sentence)['value'] !=[] : #we do not have emojis in this database text\n#         print(\"found emoji: \"+str(emot_obj.emoji(sentence)['value'])+ sentence)\n    emoticons = emot_obj.emoticons(sentence)\n#     if((emoticons['value']!=[]) ): #for printing\n#         print(\"found: \"+str(emoticons['value']) +'  emoticons in:   '+ sentence) \n    for i in range(0,len(emoticons['value'])):\n#         print('replacing  ' + emoticons['value'][i] + '  with ' +  emoticons['mean'][i])\n        sentence = sentence.replace(emoticons['value'][i], emoticons['mean'][i])\n    \"\"\"\n    part4: make everything lowercase\n    \"\"\"\n    sentence = sentence.lower()\n    return sentence\n\n","9d1a3363":"#assessment and examples:\n# # original_examples = ['hi @someone WATCH me #proud :) ;) ...... i h8 it bt w8 !!!!!  <3  wanna go &amp; &lt;3 tHeRe  &quot; bcs my finls clooooose &quot;bananas&quot; &amp; ']\n# original_examples=df.content[0:10]\n# preprocessed_examples = normalization_pipeline(original_examples)\n# for example_index,example in enumerate(preprocessed_examples):\n# #     print(original_examples[example_index])\n#     print(original_examples.values[example_index])\n#     print(example)\n\n\n    \n#run preprocessing\n# df_original=df\n# df.content=normalization_pipeline(df.content.values ) #about 10 minutes to run\n\n#save\n# df.to_csv('df_processed.csv',index=False)\n\n#load\ndf = pd.read_csv('\/kaggle\/input\/english-contractions\/df_processed.csv')\n\n","cdf83a2a":"import matplotlib.pyplot as plt\nfrom collections import Counter\n\ntokenizee=[]\nfor words in tqdm(range(1,len(df.content)-1)):\n    tokenizee.append(spacy_process(df.content[words]))\n    \n#overview of preprocessed data\nwords = Counter()\nfor s in tokenizee:\n  for w in s:\n    words[w] += 1\n\nsorted_words = list(words.keys())\nsorted_words.sort(key=lambda w: words[w], reverse=True)\nprint(f\"Number of different Tokens in our Dataset: {len(sorted_words)}\")\nprint(sorted_words[:100])\n\n\ncount_occurences = sum(words.values())\naccumulated = 0\ncounter = 0\nwhile accumulated < count_occurences * 0.8:\n  accumulated += words[sorted_words[counter]]\n  counter += 1\n\nprint(f\"The {counter * 100 \/ len(words)}% most common words \"\n      f\"account for the {accumulated * 100 \/ count_occurences}% of the occurrences\")\n\nplt.bar(range(100), [words[w] for w in sorted_words[:100]])\nplt.show()","c9e6186f":"from transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n                                         #all lower case\n                                         do_lower_case = True,\n                                         )","4fc52611":"#batch using multiple strings and convert them into tokens\nencoded_data_train = tokenizer.batch_encode_plus(\n        df[df.data_type == 'train'].content.values,\n        add_special_tokens = True,\n        #to know when sentence begins and ends\n        return_attention_mask = True,\n        #set max length to large values for big sentences\n        padding = True,\n        truncation=True, ###\n        max_length = 40,\n        return_tensors = 'pt'\n        #pt: pytorch\n        )\n\nencoded_data_val = tokenizer.batch_encode_plus(\n        df[df.data_type == 'val'].content.values,\n        add_special_tokens = True,\n        #to know when sentence begins and ends\n        return_attention_mask = True,\n        #set max length to large values for big sentences\n        padding = True,\n        truncation=True, ###\n        max_length = 40,\n        return_tensors = 'pt'\n        #pt: pytorch\n        )\n\ninput_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(df[df.data_type == 'val'].label.values)","939701ff":"for eachS in range(0,5):\n    print(tokenizer.decode(input_ids_train[eachS]))\n","707a262d":"dataset_train = TensorDataset(input_ids_train,\n                              attention_masks_train,\n                              labels_train)\n\ndataset_val = TensorDataset(input_ids_val,\n                              attention_masks_val,\n                              labels_val)","abc20c05":"len(dataset_train)","96e0df4d":"len(dataset_val)","98179bca":"from transformers import BertForSequenceClassification","467cb13d":"#each sequence will be dealt separate classification\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    #the other cased one is larger and takes more computation power\n    #we want to fine tune the parts we need\n    num_labels = len(label_dict),\n    output_attentions = False,\n    output_hidden_states = False\n                                     )\n#450 MB needs to be fetched and loaded into memory\n#bert takes into text and encodes into meaningful way according to the huge corpus it was intitially exposed to\n#we are just lying on top of it to get our 6 classes classifier","f2386984":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler     #will use for training\n \n                    #will use for our validation dataset, gradients are fixed","283c7db1":"batch_size = 8   #very small due to machine low specs but can increase to 32\n\ndataloader_train = DataLoader(\n            dataset_train,\n            sampler = RandomSampler(dataset_train),\n            #to avoid it learning from any sequences\n            batch_size = batch_size\n            )\n\ndataloader_val = DataLoader(\n            dataset_val,\n            sampler = RandomSampler(dataset_val),\n            #to avoid it learning from any sequences\n            batch_size = 32    #here no many computation, no backpropagation\n            )","659661b8":"#Optimizer defines our learning rate and how it changed throught each epoch\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n#Adam with weight decay, stochastic optimizer","4885279e":"optimizer = AdamW(\n                model.parameters(),\n                lr = 1e-5,         #recommended: 2e-5 > 5e-5\n                eps = 1e-8,\n                )","54327dc1":"epochs = 2\n\nschedular = get_linear_schedule_with_warmup(\n        optimizer,     #Adam\n        num_warmup_steps = 0,\n        num_training_steps = len(dataloader_train)*epochs\n        )","0f0d3475":"import numpy as np","7a38b8f0":"from sklearn.metrics import f1_score","7d6d20dc":"#f1-score is good bec. of class imbalance\n#accuracy alone will give me skewed results,\n    #based on f1-score not actually representing what we want\n\ndef f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis = 1).flatten()\n    #flatten to get single list and not array\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average = 'weighted')\n#can changed weighted to macro","ff2906bc":"def accuracy_per_class(preds, labels):\n    label_dict_inverse = {v: k for k, v in label_dict.items()}\n    \n    preds_flat = np.argmax(preds, axis = 1).flatten()\n    labels_flat = labels.flatten()\n    \n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat == label]\n#here we are using numpy indexing to index 2 array of the same shape by each other\n        y_true = labels_flat[labels_flat == label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy:  {len(y_preds[y_preds == label])}\/{len(y_true)}\\n')","ffd608df":"import random\n\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","d3a495cf":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#send model to device we are using\nmodel.to(device)\nprint(device)","47a858cc":"torch.cuda.is_available()","ae3acc86":"def evaluate(dataloader_val):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in tqdm(dataloader_val):\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total\/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals\n","8ec4fc44":"print(f'total epochs:{epochs}')\nfor epoch in tqdm(range(1, epochs+1)):\n    print(f'epoch # {epoch}')\n    model.train()\n    \n    loss_train_total = 0\n    #we set it initially as 0\n    \n    progress_bar = tqdm(dataloader_train,\n                        desc = f'Epoch {epoch}',\n                        leave = False,   #overwrite after each epoch\n                        disable = False                        \n                       )\n    #to see where are we, has it crashed\n    \n    for batch in progress_bar:\n#         print(f\"{}\")\n        model.zero_grad()\n        #gradient set to zero\n        \n        batch = tuple(b.to(device) for b in batch)\n        #this is imp for cuda gpu use\n        \n        inputs = {\n            'input_ids':         batch[0],\n            'attention_mask':    batch[1],\n            'labels' :           batch[2]\n        }\n        \n        outputs = model(**inputs)\n        #outputs dictionary directly into inputs\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward() #?\n        \n        \n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        #clip our gradient\n        #take gradient and give it normal value that we provide as 1\n        #stop gradients from slipping into becoming exceptionally small or too big\n        #promote generalization\n        \n        optimizer.step()\n        schedular.step()\n#         stroftrainloss = loss.item()\/len(batch)\n#         progress_bar.set_postfix(f'training_loss: {stroftrainloss}')\n        #append small dictionary\n        \n#     torch.save(model.state_dict(), f'\/kaggle\/working\/Bert_ft_epoch{epoch}.model')\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n        \n    loss_train_avg = loss_train_total\/len(dataloader_train)\n    tqdm.write(f'Training loss: {loss_train_avg}')\n        \n    val_loss, predictions, true_vals = evaluate(dataloader_val)\n        #this is imp if over training\n        #model will have no generalization abilities\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (weighted): {val_f1}')\n        \n#Cpu takes 40 minutes\n#gpu takes 30 seconds","65f2807e":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                      num_labels=len(label_dict),\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)","bd3c2135":"model.to(device)\npass   #to not get alot of text output","c35b6446":"model.load_state_dict(\n    torch.load('Models\/finetuned_bert_epoch_1_gpu_trained.model',\n              map_location = torch.device('cpu')))","5041c2f9":"_, prediction, true_vals = evaluate(dataloader_val)\n#7 batches\n#will take almost 2 minutes","1422079c":"accuracy_per_class(prediction, true_vals)","9cf3e1a5":"#To summarize:\n# model was trained on Google colab --GPU Instance(k80)\n# batch size  = 32\n# epoch = 10","e1118eb2":"preprocess the data lemmatization","7b313444":"TRY DIFFERENT MAX_LENGTHS\nhere for tweets I used 50","8858b3b3":"# BEHOLD THE TRAINING!!!","b2b4f303":"preds = [0.9 0.05 0.05 0 0 0]\n\nwe want to convert it to [1 0 0 0 0 0]","060d33d3":"NUMBER OF EPOCHS","c903c72b":"Approach adapted from an older version of HuggingFace's `run_glue.py` script. Accessible [here](https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128).","dbea5b72":"the rest can give error","f52cd19d":"BATCH SIZE CAN BE INCREASED:","4fd041eb":"## Task 9: Creating our Training Loop","2738d75a":"# **PREPROCESSING**","03380757":"Accuracy metric approach originally used in accuracy function in [this tutorial](https:\/\/mccormickml.com\/2019\/07\/22\/BERT-fine-tuning\/#41-bertforsequenceclassification).","8823f532":"## Task 7: Setting Up Optimizer and Scheduler","96a51f34":"LEARNING RATE:","8299d68b":"# Sentiment Analysis with Deep Learning using BERT","a447a301":"#uncomment for data normalization (else just load from the data)","4b6a179f":"## Task 8: Defining our Performance Metrics","422d04ef":"Import the libraries","a1637212":"## Task 10: Loading and Evaluating our Model","e94ab0c4":"## Task 3: Training\/Validation Split","5e5112e7":"## Task 6: Creating Data Loaders","4d4b2a2d":"## Task 5: Setting up BERT Pretrained Model"}}