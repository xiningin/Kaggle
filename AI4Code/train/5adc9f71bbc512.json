{"cell_type":{"6dbe6287":"code","ff3dc117":"code","96b504bf":"code","f51131e5":"code","003d47a8":"code","32f75831":"code","8c8bd3a5":"code","6d5c6437":"code","46bbe58b":"code","266d1d2f":"code","2e21e4e7":"code","7c7408de":"code","6fc71a38":"code","eca12b8a":"code","7f6650ad":"code","ee0f392a":"code","1e764eea":"code","31290e85":"markdown","8b92c2ed":"markdown","fc466582":"markdown","9d102bf2":"markdown","683954fa":"markdown","40593caa":"markdown","8d238aa9":"markdown","e83d2232":"markdown","2a4ea364":"markdown","a15055fc":"markdown","a0ba7357":"markdown","1629a3fa":"markdown","4cc65ba2":"markdown","cad99b35":"markdown"},"source":{"6dbe6287":"# %load_ext tensorboard.notebook\n# %tensorboard --logdir logs","ff3dc117":"from tensorflow.keras.preprocessing.text import Tokenizer\n\nclass Natural_Language_Processing():\n    def __init__(self):\n        pass\n\n    '''\n    Very simple tokenization based on Keras API.\n    Note: Tokenizer API by default removes punctuations as well.\n    '''\n    def corpus_tokenizer(self, sentence_list):\n        #setting hyper parameter of the Tokenizer API to 100, will pick top 100 words only.\n        tokenizer = Tokenizer(num_words=100)\n        tokenizer.fit_on_texts(sentence_list)\n        tokenized_index = tokenizer.word_index\n        return tokenized_index\n\n    '''\n    Convert text to sequences\n    '''\n    def texts_to_sequences(self, text):\n        tokenizer = Tokenizer(num_words=100)\n        tokenizer.fit_on_texts(text)\n        text_to_sequence = tokenizer.texts_to_sequences(text)\n        return text_to_sequence\n    ","96b504bf":"object = Natural_Language_Processing()\n\nsentence = [\n    'i love my dog', 'i love my cat', 'i love my country',\n    'india is my country and it is full of nature'\n]\n\n# Tokenization\ntokenized_word_index = object.corpus_tokenizer(sentence)\nprint(tokenized_word_index)\n\n#Text to sequence\ntexts_to_sequences = object.texts_to_sequences(sentence)\nprint(\"\\n\\n\", texts_to_sequences)","f51131e5":"!pip install tensorflow-datasets","003d47a8":"import numpy as np\nimport tensorflow_datasets as tfds\n\n#To tokenize the corpus\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n#To maintain the uniform structure of the sequences we need to pad them\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nclass ReviewClassifier():\n    def __init__(self, vocab_size, embedding_dim, sentence_max_length,\n                 truncation_type):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.max_length = sentence_max_length\n        self.trunc_type = truncation_type\n        self.oov_tok = \"<OOV>\"\n\n    '''\n    Load the IMDB dataset using tfds API from Tendorflow.\n    '''\n    def dataLoader(self):\n        imdb, info = tfds.load('imdb_reviews',\n                               with_info=True,\n                               as_supervised=True)\n        return imdb, info\n\n    '''\n    Definition to split the data into train and test\n    '''\n    def dataSplitter(self, imdb):\n        # 25k samples for each sample set\n        train_data, test_data = imdb['train'], imdb['test']\n        return train_data, test_data\n\n    '''\n    We need to convert the training and testing dataset to an array of sentences\n    '''\n    def dataConverter(self, imdb):\n\n        train_data, test_data = self.dataSplitter(imdb)\n\n        training_sentences = []\n        training_labels = []\n\n        testing_sentences = []\n        testing_labels = []\n\n        for sentence, label in train_data:\n            training_sentences.append(str(sentence.numpy()))\n            training_labels.append(str(label.numpy()))\n\n        for sentence, label in test_data:\n            testing_sentences.append(str(sentence.numpy()))\n            testing_labels.append(str(label.numpy()))\n\n        training_labels_final = np.array(training_labels)\n        training_labels_final = training_labels_final.astype(np.float)\n        testing_labels_final = np.array(testing_labels)\n        testing_labels_final = testing_labels_final.astype(np.float)\n\n        tokenizer = Tokenizer(num_words=self.vocab_size,\n                              oov_token=self.oov_tok)\n        tokenizer.fit_on_texts(training_sentences)\n        word_index = tokenizer.word_index\n\n        training_sequences = tokenizer.texts_to_sequences(training_sentences)\n        training_padded = pad_sequences(training_sequences,\n                                        maxlen=self.max_length,\n                                        truncating=self.trunc_type)\n\n        testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n        testing_padded = pad_sequences(testing_sequences,\n                                       maxlen=self.max_length)\n\n        return training_sentences, training_padded,training_labels_final, testing_sentences, testing_padded,testing_labels_final\n\n    def decode_review(self, text):\n        tokenizer = Tokenizer(num_words=self.vocab_size,\n                              oov_token=self.oov_tok)\n        tokenizer.fit_on_texts(training_sentences)\n        word_index = tokenizer.word_index\n        reverse_word_index = dict([(value, key)\n                                   for (key, value) in word_index.items()])\n        return ' '.join([reverse_word_index.get(i, '?') for i in text]) , reverse_word_index","32f75831":"'''\nvocab_size,\nembedding_dim,\nsentence_max_length,\ntruncation_type\n'''\n\nobj = ReviewClassifier(1000, 16, 120, 'post')\nimdb, info = obj.dataLoader()\ntrain, test = obj.dataSplitter(imdb)\ntraining_sentences, training_padded, training_labels_final, testing_sentences, testing_padded, testing_labels_final = obj.dataConverter(\n    imdb)\nreverse_word_index = obj.decode_review(training_padded[1])","8c8bd3a5":"print(train)\nprint(test)","6d5c6437":"import datetime\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Flatten\n\nclass MyCustomCallback(tf.keras.callbacks.Callback):\n    def on_train_batch_begin(self, batch, logs=None):\n        print('Training: batch {} begins at {}'.format(\n            batch,\n            datetime.datetime.now().time()))\n\n    def on_train_batch_end(self, batch, logs=None):\n        print('Training: batch {} ends at {}'.format(\n            batch,\n            datetime.datetime.now().time()))\n\n    def on_test_batch_begin(self, batch, logs=None):\n        print('Evaluating: batch {} begins at {}'.format(\n            batch,\n            datetime.datetime.now().time()))\n\n    def on_test_batch_end(self, batch, logs=None):\n        print('Evaluating: batch {} ends at {}'.format(\n            batch,\n            datetime.datetime.now().time()))\n\n    def on_proper_accuracy(self, logs={}):\n        accuracy = logs.get('accuracy')\n        if type(logs.get('accuracy')) != None and (accuracy > 0.95):\n            print(\n                \"\\n Chances are model learning headaing towards Overfitting\\n\\n\"\n            )\n            self.model.stop_training = True\n\nclass Model_Builder:\n    def __init__(self):\n        pass\n\n    def model(self, vocab_size, embedding_dim, max_length):\n        model = Sequential()\n        model.add(Embedding(vocab_size, embedding_dim,\n                            input_length=max_length))\n        model.add(Flatten())\n        model.add(Dense(6, activation='relu'))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(loss='binary_crossentropy',\n                      optimizer='adam',\n                      metrics=['accuracy'])\n\n        model.summary()\n        return model\n\nobj = Model_Builder()\ncallback = MyCustomCallback()\nmodel = obj.model(1000, 16, 120)\n\nhistory = model.fit(training_padded,\n                    training_labels_final,\n                    epochs=10,\n                    validation_data=(testing_padded, testing_labels_final),\n                    verbose=1)","46bbe58b":"import matplotlib.pyplot as plt\naccuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\n\nepochs = range(len(accuracy))\n\nplt.figure(figsize=(17,12))\nplt.plot(epochs, accuracy, 'r', label='Training Accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc=0)\nplt.grid()\nplt.show()","266d1d2f":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(loss))\nplt.figure(figsize=(17,12))\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc=0)\nplt.grid()\nplt.show()","2e21e4e7":"## Model Overfitted\n#Model.layers[0] means layer zero and it contains the embedding information\n#Let's fetch it and have a look\ne = model.layers[0]\nweights = e.get_weights()[0]\n\nprint(weights.shape)\n\n#Let's save the embedding metadatas so that we can later visualize the embeddings \nimport io\nreverse_word_index = reverse_word_index[1]\nout_v = io.open('vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('meta.tsv', 'w', encoding='utf-8')\nfor word_num in range(1, 1000):\n    word = reverse_word_index[word_num]\n    embeddings = weights[word_num]\n    out_m.write(word + \"\\n\")\n    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\nout_v.close()\nout_m.close()","7c7408de":"!wget -x --load-cookies cookies.txt -nH \"https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/sarcasm.json\"","6fc71a38":"import json\nimport tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nclass Sarcasm_Classifier:\n    def __init__(self):\n        self.vocab_size = 100000\n        self.embedding_dim = 16\n        self.max_length = 32\n        self.trunc_type = 'post'\n        self.padding_type = 'post'\n        self.oov_tok = \"<OOV>\"\n        self.training_size = 13354\n        self.testing_size = 13354\n        self.word_index = 0\n\n    def data_colector(self):\n        sentences = []\n        lablels = []\n        tuplet = []\n        with open(\"\/kaggle\/working\/laurencemoroney-blog.appspot.com\/sarcasm.json\", 'r') as f:\n            datastore = json.load(f)\n\n            for item in datastore:\n                sentences.append(item['headline'])\n                lablels.append(item['is_sarcastic'])\n                tuplet.append((item['headline'], item['is_sarcastic']))\n        return sentences, lablels, tuplet\n\n    def data_renderer(self, tuplet):\n        count = 1\n        for i in tuplet:\n            print(\"Article-\" + str(count) + \":\" + i[0] + \"----->>>>>  \" +\n                  str(i[1]))\n            count += 1\n\n    def data_splitor(self, sentences, lables):\n        tokenizer = Tokenizer()\n        training_sentences = sentences[:self.training_size]\n        tokenizer.fit_on_texts(training_sentences)\n        self.word_index = tokenizer.word_index\n        training_sequences = tokenizer.texts_to_sequences(training_sentences)\n        training_padded = pad_sequences(training_sequences,\n                                        maxlen=self.max_length,\n                                        truncating=self.trunc_type)\n        training_labels = lables[:self.training_size]\n        training_labels_final = np.array(training_labels)\n        training_labels_final = training_labels_final.astype(np.float)\n\n        \n        \n        testing_sentences = sentences[self.testing_size:]\n        tokenizer.fit_on_texts(testing_sentences)\n        testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n        testing_padded = pad_sequences(testing_sequences,\n                                       maxlen=self.max_length,\n                                       truncating=self.trunc_type)\n        testing_labels = lables[self.testing_size:]\n        testing_labels_final = np.array(testing_labels)\n        testing_labels_final = testing_labels_final.astype(np.float)\n        \n        return training_padded, training_labels_final, testing_padded, testing_labels_final\n\n    def modeler(self):\n        model = Sequential()\n        model.add(\n            Embedding(self.vocab_size,\n                      self.embedding_dim,\n                      input_length=self.max_length))\n        model.add(Flatten())\n        model.add(Dense(6, activation='relu'))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(loss='binary_crossentropy',\n                      optimizer='adam',\n                      metrics=['accuracy'])\n        model.summary()\n        return model","eca12b8a":"obj = Sarcasm_Classifier()\nsentences,lables, tuplet = obj.data_colector()\ntraining_padded,training_labels,testing_padded,testing_labels = obj.data_splitor(sentences,lables)\nmodel = obj.modeler()\nhistory = model.fit(training_padded,\n                    training_labels,\n                    epochs=5,\n                    batch_size = 100,\n                    validation_data=(testing_padded, testing_labels),\n                    verbose=1)","7f6650ad":"for i,j,k in os.walk(\"\/kaggle\/working\"):\n    print(i)","ee0f392a":"import matplotlib.pyplot as plt\naccuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\n\nepochs = range(len(accuracy))\nplt.figure(figsize=(17,12))\nplt.plot(epochs, accuracy, 'r', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend(loc=0)\nplt.grid()\nplt.show()","1e764eea":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(loss))\nplt.figure(figsize=(17,12))\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc=0)\nplt.grid()\nplt.show()","31290e85":"Let's start learning by doing.<br> **TensorFlow Dataset** aka **tfds** is great resource of data for exploring multiple Machine learning Ideas.<br>\nFor better understanding of this module please click on to [Tensorflow Dataset API](https:\/\/www.tensorflow.org\/datasets\/api_docs\/python\/tfds)","8b92c2ed":"# PART - 3","fc466582":"**What is word embedding?**","9d102bf2":"As it is visible, after training to 10 epochs the validation loss seems to be on sky high flight, There must be some tweaks to control this behavior during learning process as it is clear sign of over fitting. Both Training and validation loss must be moving towards minimal values and we can tolerate at some trade off between them.<br>\nLet's change some hyper parameters to tackle this.","683954fa":"**Let's Work on Sarcasm dataset**","40593caa":"Sarcasm data download\nIt would download the data to \/kaggle\/working\/","8d238aa9":"Make sure it is usign Python 3.X <br>\nIf using tensorflow version 1.X we need to run the below command<br>\n**tf.enable_eager_execution()**\n<br>\nReason behind running this command is that **Tensorflow 2.X** is by default have the eager execution on.\n<br><br>\nLet's use IMDB movie review dataset for our learning purpose.\nI would love to say that before you start the experiments and getting into hot water, just have a look about the information regarding this dataset.\n[IMDB Reviews](https:\/\/www.tensorflow.org\/datasets\/catalog\/imdb_reviews)","e83d2232":"For the purpose of our learning we would target dataset which is provided by Tensorflow dataset, Tensorflow Dataset is great source of open source of data and it you can add your own dataset to that.\nFor greater confidence in that please have a look at [](https:\/\/www.tensorflow.org\/datasets\/overview)","2a4ea364":"# PART - 1","a15055fc":"### **Word Embeddings**","a0ba7357":"# PART - 2","1629a3fa":"**Sentiment in Text**","4cc65ba2":"**Word embeddings** are basically a form of word representation that bridges the human understanding of language to that of a machine.<br>Word embeddings are distributed representations of text in an n-dimensional space.<br>These are essential for solving most NLP problems.<br><br> For better understanding please have a look at [Word embedding in NLP](https:\/\/hackernoon.com\/word-embeddings-in-nlp-and-its-applications-fab15eaf7430)","cad99b35":"In this notebook i will try to show the use case of tensorflow and Keras for Natural Language Processing.\nWe will look to have the flow of the work in three Parts.\n\n1: Sentiment in Text<br>\n2: Word Embeddings<br>\n3: Sarcasm Data Prediction Modeling<br>"}}