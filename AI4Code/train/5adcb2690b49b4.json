{"cell_type":{"261003ce":"code","a55aaf76":"code","fa2abd97":"code","96cfa642":"code","bc63a612":"code","fc7d9948":"code","c7eab09a":"code","5439436b":"code","d6a79fbc":"code","5691a3cd":"code","6bb69fe0":"code","ee6cb3db":"code","2c59951f":"code","dbfd73a3":"code","e007b28a":"code","4dbd0800":"code","0da593fb":"code","cd53c486":"code","ac1f8019":"code","490eac17":"code","b4d4aef2":"code","6720ff87":"code","fecc2be1":"code","f55ceb46":"code","1b319e58":"code","b72e9bf7":"code","a4902601":"code","bd79fbf2":"code","5613fd67":"code","6788cd4e":"markdown","968d8792":"markdown","d4fba4fc":"markdown","3c666721":"markdown","26fc82b8":"markdown","eb7c1b56":"markdown","2bb72cac":"markdown","07a2ade4":"markdown","d4ba76e7":"markdown","42edab7d":"markdown","ffbe3fd4":"markdown","764783d1":"markdown","d3cbaf57":"markdown","57ba9ef8":"markdown","53f802d8":"markdown","272980b1":"markdown","62dfea6e":"markdown"},"source":{"261003ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n### Import Libraries ###","a55aaf76":"import matplotlib.pyplot as plt\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.porter import PorterStemmer\nimport re\nfrom gensim.parsing.preprocessing import STOPWORDS","fa2abd97":"data_df = pd.read_csv('\/kaggle\/input\/yelp-csv\/yelp_academic_dataset_review.csv')","96cfa642":"data_df.head()","bc63a612":"data_df.shape","fc7d9948":"top_data_df = data_df[['text','stars']]\ntop_data_df","c7eab09a":"print(\"Number of rows per star rating:\\n\", top_data_df['stars'].value_counts())","5439436b":"# Function to map stars to sentiment\ndef map_sentiment(stars_received):\n    if stars_received <= 2:\n        return -1\n    elif stars_received == 3:\n        return 0\n    else:\n        return 1\n# Mapping stars to sentiment into three categories\ntop_data_df['sentiment'] = [ map_sentiment(x) for x in top_data_df['stars']]\n# Plotting the sentiment distribution\nplt.figure()\npd.value_counts(top_data_df['sentiment']).plot.bar(title=\"Sentiment distribution in df\")\nplt.xlabel(\"Sentiment\")\nplt.ylabel(\"No. of rows in df\")\nplt.show()","d6a79fbc":"top_data_df","5691a3cd":"# Function to retrieve top few number of each category\ndef get_top_data(top_n = 5000):\n    top_data_df_positive = top_data_df[top_data_df['sentiment'] == 1].head(top_n)\n    top_data_df_negative = top_data_df[top_data_df['sentiment'] == -1].head(top_n)\n    top_data_df_neutral = top_data_df[top_data_df['sentiment'] == 0].head(top_n)\n    top_data_df_small = pd.concat([top_data_df_positive, top_data_df_negative, top_data_df_neutral])\n    return top_data_df_small\n\n# Function call to get the top 10000 from each sentiment\ntop_data_df_small = get_top_data(top_n=10000)\n\n# After selecting top few samples of each sentiment\nprint(\"After segregating and taking equal number of rows for each sentiment:\")\nprint(top_data_df_small['sentiment'].value_counts())\ntop_data_df_small = top_data_df_small.reset_index()\nprint(top_data_df_small.head())\ntop_data_df_small.to_csv('selecteddata.csv')\n#print(top_data_df_small.loc[15220])","6bb69fe0":"for row in range(len(top_data_df_small)):\n    top_data_df_small.text.loc[row] = re.sub('[^a-zA-Z]', ' ',top_data_df_small['text'][row])","ee6cb3db":"print(top_data_df_small['sentiment'].value_counts())","2c59951f":"print(top_data_df_small['text'][10])\nprint(remove_stopwords(top_data_df_small['text'][10]))","dbfd73a3":"from gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.parsing.preprocessing import STOPWORDS\nall_stopwords_gensim = STOPWORDS\nsw_list = {\"not\"}\nall_stopwords_gensim = STOPWORDS.difference(sw_list)","e007b28a":"top_data_df_small['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in top_data_df_small['text']]\nprint(top_data_df_small['tokenized_text'].head(10))\nfor row in range (len(top_data_df_small)):\n    list_of_words = []\n    mod_list = []\n    list_of_words = top_data_df_small['tokenized_text'][row]\n    for i in range(len(list_of_words)):\n        if not list_of_words[i] in all_stopwords_gensim:\n            mod_list.append(list_of_words[i])\n    top_data_df_small['tokenized_text'][row] = mod_list\nprint(top_data_df_small['tokenized_text'].head(10))","4dbd0800":"print(top_data_df_small['sentiment'].value_counts())","0da593fb":"from gensim.parsing.porter import PorterStemmer\nporter_stemmer = PorterStemmer()\n# Get the stemmed_tokens\ntop_data_df_small['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in top_data_df_small['tokenized_text'] ]\ntop_data_df_small['stemmed_tokens'].head(10)","cd53c486":"print(top_data_df_small['sentiment'].value_counts())","ac1f8019":"top_data_df_small=top_data_df_small.drop(\"index\",axis=1)\ntop_data_df_small","490eac17":"\nfrom sklearn.model_selection import train_test_split\n# Train Test Split Function\ndef split_train_test(top_data_df_small, test_size=0.3, shuffle_state=True):\n    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[['stars', 'text', 'tokenized_text', 'stemmed_tokens']], \n                                                        top_data_df_small['sentiment'], \n                                                        shuffle=shuffle_state,\n                                                        test_size=test_size, \n                                                        random_state=15)\n    print(\"Value counts for Train sentiments\")\n    print(Y_train.value_counts())\n    print(\"Value counts for Test sentiments\")\n    print(Y_test.value_counts())\n    print(type(X_train))\n    print(type(Y_train))\n    X_train = X_train.reset_index()\n    X_test = X_test.reset_index()\n    Y_train = Y_train.to_frame()\n    Y_train = Y_train.reset_index()\n    Y_test = Y_test.to_frame()\n    Y_test = Y_test.reset_index()\n    print(X_train.head())\n    print(Y_train.head())\n    return X_train, X_test, Y_train, Y_test\n\n# Call the train_test_split\nX_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small)","b4d4aef2":"from gensim.models import Word2Vec\nimport gensim.downloader as api\nfrom gensim import models\nimport time\n# Skip-gram model (sg = 1)\nsize = 1000\nwindow = 3\nmin_count = 1\nworkers = 3\nsg = 1\n\nword2vec_model_file = 'word2vec_' + str(size) + '.model'\nstart_time = time.time()\nstemmed_tokens = pd.Series(top_data_df_small['stemmed_tokens']).values\n# Train the Word2Vec Model\n#w2v_model = api.load('word2vec-google-news-300')\nw2v_model = Word2Vec(stemmed_tokens, min_count = min_count, vector_size = size, workers = workers, window = window,sg = sg)\n#w2v_model.train(stemmed_tokens,total_examples=len(stemmed_tokens),epochs=10)\nprint(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\nw2v_model.save(word2vec_model_file)","6720ff87":"import numpy as np\n# Load the model from the model file\nsg_w2v_model = Word2Vec.load(word2vec_model_file)\n#sg_w2v_model=w2v_model\n# Unique ID of the word\nprint(\"Index of the word 'action':\")\nprint(sg_w2v_model.wv.key_to_index[\"action\"])\nprint(\"The most similar to the word 'goldberg':\")\nprint(sg_w2v_model.wv.most_similar(positive=[\"goldberg\"]))\n# Total number of the words \nprint(len(sg_w2v_model.wv.key_to_index))\n# Print the size of the word2vec vector for one word\nprint(\"Length of the vector generated for a word\")\nprint(len(sg_w2v_model.wv['action']))\n#print(sg_w2v_model.wv['action'])\n# Get the mean for the vectors for an example review\nprint(\"Print the length after taking average of all word vectors in a sentence:\")\n#testlist = [['action','eat','sleep','feel'],['drink','roam','walk','run']]\nprint(np.mean([sg_w2v_model.wv[token] for token in top_data_df_small['stemmed_tokens'][0]], axis=0))\n#print(np.mean([sg_w2v_model[token] for token in testlist[0]], axis=0))\nprint(sg_w2v_model.wv)","fecc2be1":"sg_w2v_model.wv.index_to_key[3898]","f55ceb46":"# Store the vectors for train data in following file\nword2vec_filename = 'train_review_word2vec.csv'\nwith open(word2vec_filename, 'w+') as word2vec_file:\n    for index, row in X_train.iterrows():\n        model_vector = (np.mean([sg_w2v_model.wv[token] for token in row['stemmed_tokens']], axis=0)).tolist()\n        if index == 0:\n            header = \",\".join(str(ele) for ele in range(size))\n            word2vec_file.write(header)\n            word2vec_file.write(\"\\n\")\n        # Check if the line exists else it is vector of zeros\n        if type(model_vector) is list:  \n            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n        else:\n            line1 = \",\".join([str(0) for i in range(size)])\n        word2vec_file.write(line1)\n        word2vec_file.write('\\n')","1b319e58":"# Store the vectors for test data in following file\nword2vec_tst_filename = 'test_review_word2vec.csv'\nwith open(word2vec_tst_filename, 'w+') as word2vec_tst_file:\n    for index, row in X_test.iterrows():\n        model_vector = (np.mean([sg_w2v_model.wv[token] for token in row['stemmed_tokens']], axis=0)).tolist()\n        if index == 0:\n            header = \",\".join(str(ele) for ele in range(size))\n            word2vec_tst_file.write(header)\n            word2vec_tst_file.write(\"\\n\")\n        # Check if the line exists else it is vector of zeros\n        if type(model_vector) is list:  \n            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n        else:\n            line1 = \",\".join([str(0) for i in range(size)])\n        word2vec_tst_file.write(line1)\n        word2vec_tst_file.write('\\n')","b72e9bf7":"import time\n#Import the DecisionTreeeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n# Load from the filename\nword2vec_df = pd.read_csv(word2vec_filename)\n#Initialize the model\nclf_decision_word2vec = RandomForestClassifier(max_depth=11,n_estimators=25)\n\nstart_time = time.time()\n# Fit the model\nclf_decision_word2vec.fit(word2vec_df, Y_train['sentiment'])\nprint(\"Time taken to fit the model with word2vec vectors: \" + str(time.time() - start_time))","a4902601":"from sklearn.metrics import classification_report\nword2vec_tst_df = pd.read_csv(word2vec_tst_filename)\ntest_predictions_word2vec = clf_decision_word2vec.predict(word2vec_tst_df)\nprint(classification_report(Y_test['sentiment'],test_predictions_word2vec))","bd79fbf2":"from sklearn.metrics import accuracy_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nmodel = BernoulliNB()\nmodel.fit(word2vec_df, Y_train['sentiment'])\ntest_predictions_word2vec = model.predict(word2vec_tst_df)\nprint(classification_report(Y_test['sentiment'],test_predictions_word2vec))","5613fd67":"from sklearn.model_selection import GridSearchCV\nRF=RandomForestClassifier()\ncomponents = np.arange(1,100,5)\nestimators=np.arange(5,30,5)\ndegree = np.arange(10,30,10)\ndepth = np.arange(1,30,5)\nalpha = np.arange(0,1,0.1)\ngamma = np.arange(0,1,0.1)\ngrid = GridSearchCV(cv=10,estimator=RF, scoring='accuracy',param_grid=dict(#pca__n_components=components,\n                                                                       max_depth=depth,\n                                                                       #poly__degree=degree,\n                                                                       #C = degree\n                                                                       #SVC__gamma = gamma\n                                                                       #Ridge__alpha=alpha\n                                                                       #Lasso__alpha=alpha\n                                                                       n_estimators=estimators\n                                                                       ))\ngrid.fit(word2vec_df, Y_train['sentiment'])\nprint(grid.best_score_)\nprint(grid.best_estimator_)\nprint(grid.best_params_)\nprint(grid.score(word2vec_tst_df, Y_test['sentiment']))\nprint(grid.score(word2vec_df, Y_train['sentiment']))","6788cd4e":"### Count Number of star ratings ###","968d8792":"#### Training Sentiment Classification Models Using Word2Vec ####","d4fba4fc":"#### Access Word2Vec Dictionary ####","3c666721":"## Filter the number to lesser number of records ##","26fc82b8":"#### Removing Stopwords ####","eb7c1b56":"#### Train Test Splitting ####","2bb72cac":"### Read the file ###","07a2ade4":"We have 1 million + rows. To reduce processing time and create a balance dataset picking up 10000 rows from each of the sentiment for Text processing","d4ba76e7":"#### Generating Word2Vec ####","42edab7d":"### Display the top 5 rows, shape of the data and filter only the useful columns for Text Processing  ###","ffbe3fd4":"## Text Preprocessing ##","764783d1":"#### Special Character Removal ####","d3cbaf57":"#### Tokenization ####","57ba9ef8":"### Convert the ratings into a smaller set and plot them e.g 4 and 5 are positive (1), 3 is neutral (0) and 1 and 2 are poor (-1) ###","53f802d8":"### Print Data with sentiment ###","272980b1":"#### Stemming ####","62dfea6e":"#### Run Word2Vec ####"}}