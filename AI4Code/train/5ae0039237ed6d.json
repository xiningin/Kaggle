{"cell_type":{"36885c6f":"code","8ca11a8e":"code","3a3eee4c":"code","24d1f58e":"code","bb3e0e54":"code","032c7692":"code","df1e1f68":"code","e91f1189":"code","421b8f82":"code","79f63b74":"code","a66abb62":"code","e753f910":"code","98aac83f":"code","e0d1a662":"code","9f42c368":"code","e0a2cfac":"code","eeeb5451":"code","d022a111":"code","fe06a850":"code","ecf8e943":"code","fa644dc0":"code","03f07399":"code","ead1bade":"code","2540637d":"code","c84e07b1":"code","49723f0f":"code","78426582":"code","27a4b828":"code","fddd3462":"code","1fc89184":"code","119679d1":"code","3bb9b1d2":"code","d35cf39d":"code","8e614f52":"code","d1fdd1b8":"code","c2a6faef":"code","101add13":"code","9ff45907":"code","9505cc06":"code","07aa5e74":"code","3c6389e8":"code","94631a16":"code","4a21d426":"code","aa1ee18b":"code","d4c455b5":"code","67638148":"code","3ef3cd5a":"code","41c94b3b":"code","7d460be9":"code","d20c84ac":"markdown","60e983db":"markdown","7e0f67f1":"markdown","5c8479e4":"markdown","116e899e":"markdown","3d73f6db":"markdown","1d21fc50":"markdown","31bb92f3":"markdown","51e1475c":"markdown","b130af24":"markdown","f1e4b92c":"markdown","68ed8467":"markdown","dfbd318f":"markdown","978ebe70":"markdown","66c1e9ab":"markdown","bca71948":"markdown","88297493":"markdown","b384a83b":"markdown","a073f464":"markdown","7e4d0987":"markdown","b20a390a":"markdown","1d745595":"markdown","7017f7a9":"markdown","84baa7f3":"markdown","196206aa":"markdown","d33b31f4":"markdown","91036c5c":"markdown","46dd2e0a":"markdown","bd84f878":"markdown","d6dc4724":"markdown","4e396042":"markdown","722c6d61":"markdown","d68effa2":"markdown","bbdf7e76":"markdown","8f87ec8f":"markdown","a640cb34":"markdown","09f3c545":"markdown","4fc21d6a":"markdown","79d35cd7":"markdown","6f6f2845":"markdown","3b0ae4ce":"markdown","5708c6c7":"markdown","504d06fd":"markdown","38810f65":"markdown","483dbb94":"markdown","1120a71c":"markdown","5b88b1e0":"markdown","c4dbf46a":"markdown","854768e3":"markdown","2b6a5efc":"markdown","f87cab29":"markdown","88fbd580":"markdown","ca665edb":"markdown","25171d4e":"markdown","3e4db3f8":"markdown","3f6812d0":"markdown","eee4074e":"markdown","33c14532":"markdown","1dcbebc8":"markdown","9db1834c":"markdown","2a933867":"markdown","16093e94":"markdown"},"source":{"36885c6f":"# Import Basic Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8ca11a8e":"## Loading the data\ndf = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf.head()","3a3eee4c":"print('Amount of patients without a heart disease:', (df['HeartDisease']==0).sum())\nprint('Amount of patients with a heart disease:', (df['HeartDisease']==1).sum())\n\nsns.countplot(data=df, x='HeartDisease');","24d1f58e":"cat_columns = df.select_dtypes(include='object').columns.tolist()\nnum_columns = df.select_dtypes(exclude='object').columns.tolist()\n\nprint('The categorical columns are', cat_columns)\nprint('The numerical columns are', num_columns)","bb3e0e54":"cat_columns.append('FastingBS')\nnum_columns.remove('FastingBS')","032c7692":"indexes = [ (i,j) for i in range(3) for j in range(2)]\nbunch_of_colors = ['red', 'lightgreen', 'purple', 'pink', 'darkgreen', 'darkblue']\n\nfig, axes = plt.subplots(3, 2, figsize=(12, 16))\n\nfig.suptitle('Distribution of the numerical features')\n\nfor index, column, color in zip(indexes, df[num_columns].columns, bunch_of_colors):\n    sns.set_theme()\n    sns.histplot(ax=axes[index], data=df[num_columns], x=df[num_columns][column], color=color)\n    \nfig.subplots_adjust(hspace=.3)","df1e1f68":"g = sns.pairplot(df, hue=\"HeartDisease\", corner=True, palette = 'Set2')\n\ng._legend.set_bbox_to_anchor((.7, .7))\ng._legend.get_title().set_fontsize(20);","e91f1189":"fig, axes = plt.subplots(1, 2, figsize=(16, 10))\n\nsns.histplot(ax=axes[0], data=df, x='Age', hue='HeartDisease', multiple='stack')\naxes[0].set_xlabel(xlabel='Age', fontsize=18)\n\nsns.histplot(ax=axes[1], data=df, x='Oldpeak', hue='HeartDisease', multiple='stack')\naxes[1].set_xlabel(xlabel='Oldpeak', fontsize=18);","421b8f82":"plt.figure(figsize=(12,10))\nsns.heatmap(df.corr(), annot=True, cmap='viridis')\nplt.yticks(rotation=0)\nplt.title('Correlation of the features with the Heart disease');","79f63b74":"indexes = [ (i,j) for i in range(3) for j in range(2)]\n\nfig, axes = plt.subplots(3, 2, figsize=(12, 16))\n\nfig.suptitle('Countplots from the categorical features by heart disease')\n\nfor index, column in zip(indexes, df[cat_columns].columns):\n    sns.countplot(ax=axes[index], data=df, x=df[cat_columns][column], hue='HeartDisease')\n    \nfig.subplots_adjust(hspace=.3)","a66abb62":"# Separating features from the target we want to predict\nX = df.drop('HeartDisease', axis=1)\ny = df['HeartDisease']\n\n# Performing one-hot encoding for the categorical features\nX = pd.get_dummies(X, drop_first=True)","e753f910":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# Splitting our data into train\/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Standard scaling\nscaler = StandardScaler()\nst_scaled_X_train = scaler.fit_transform(X_train)\nst_scaled_X_test = scaler.transform(X_test)\n\n# Normal scaling\nscaler = MinMaxScaler()\nnormal_scaled_X_train = scaler.fit_transform(X_train)\nnormal_scaled_X_test = scaler.transform(X_test)\n\n# Storing out three types of data\nX_train_datasets = [X_train, st_scaled_X_train, normal_scaled_X_train]\nX_test_datasets = [X_test, st_scaled_X_test, normal_scaled_X_test]","98aac83f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\n\n# Setting up our three logistic regression models.\n\nlr = LogisticRegression(solver='liblinear')\nlr_l1 = LogisticRegressionCV(Cs=10, cv=5, penalty='l1', solver='liblinear')\nlr_l2 = LogisticRegressionCV(Cs=10, cv=5, penalty='l2', solver='liblinear')","e0d1a662":"from sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import accuracy_score\n\nmodels = [lr, lr_l1, lr_l2]\nprecision = []\nrecall = []\nf1_score = []\naccuracy = []\n\nfor X_train_data, X_test_data in zip(X_train_datasets, X_test_datasets):\n    for model in models:\n        \n        model.fit(X_train_data, y_train)\n    \n        predictions = model.predict(X_test_data)\n    \n        precision.append( score(y_test, predictions, average='weighted')[0] )\n        recall.append( score(y_test, predictions, average='weighted')[1] )\n        f1_score.append( score(y_test, predictions, average='weighted')[2] )\n        accuracy.append( accuracy_score(y_test, predictions) )","9f42c368":"scores = [precision, recall, f1_score, accuracy]\n\ndf_lr = round(pd.DataFrame(scores, \n            index=['Precision', 'Recall', 'f1_score', 'Accuracy'],\n            columns=['lr', 'lr_l1', 'lr_l2', \n                     'lr_st', 'lr_l1_st', 'lr_l2_st',\n                     'lr_normal', 'lr_l1_normal', 'lr_l2_normal']), 3)\n\ndf_lr","e0a2cfac":"# Our chosen logistic regression model\n# And trained on data without scaling\n\nlr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\npredictions_lr = lr.predict(X_test)","eeeb5451":"# Storing the scores\n\ndf_lr = df_lr['lr'].to_frame()\ndf_lr","d022a111":"from sklearn.neighbors import KNeighborsClassifier\n\nk = 100\naccuracy_k = []\n\nfor X_train_data, X_test_data in zip(X_train_datasets, X_test_datasets):\n    for k in range(1, k+1):\n\n        knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n        knn = knn.fit(X_train_data, y_train)\n\n        predictions = knn.predict(X_test_data)\n        accuracy_pred = accuracy_score(y_test, predictions)\n\n        accuracy_k.append(accuracy_pred)","fe06a850":"# Trained on data without scaling\nplt.figure(figsize=(7,6))\nplt.title('KNN trained on data without scaling')\nplt.ylabel('Accuracy Score')\nplt.xlabel('K value')\nsns.lineplot(x=range(1, k+1), y=accuracy_k[:100]);","ecf8e943":"print('max accuracy obtained:', round(max(accuracy_k[:100]), 3))","fa644dc0":"# Trained on data with standard scaling\nplt.figure(figsize=(7,6))\nplt.title('KNN trained on data with standard scaling')\nplt.ylabel('Accuracy Score')\nplt.xlabel('K value')\nsns.lineplot(x=range(1, k+1), y=accuracy_k[100:200]);","03f07399":"sns.lineplot(x=range(1, k+1), y=accuracy_k[100:200], marker='o')\nplt.xlim(22,26)\nplt.ylim(.89,.91);","ead1bade":"# Trained on data with normal scaling\nplt.figure(figsize=(7,6))\nplt.title('KNN trained on data with normal scaling')\nplt.ylabel('Accuracy Score')\nplt.xlabel('K value')\nsns.lineplot(x=range(1, k+1), y=accuracy_k[200:]);","2540637d":"knn = KNeighborsClassifier(n_neighbors=24, weights='distance')\nknn = knn.fit(st_scaled_X_train, y_train)\n    \npredictions_knn = knn.predict(st_scaled_X_test)\n\nknn_scores = []\n\nknn_scores.append( score(y_test, predictions_knn, average='weighted')[0] )\nknn_scores.append( score(y_test, predictions_knn, average='weighted')[1] )\nknn_scores.append( score(y_test, predictions_knn, average='weighted')[2] )\nknn_scores.append( accuracy_score(y_test, predictions_knn) )\n\ndf_knn = pd.DataFrame(knn_scores, columns=['knn'],\n            index=['Precision', 'Recall', 'f1_score', 'Accuracy'])\n\ndf_knn['knn'] = np.round(df_knn['knn'], 3)\n\ndf_knn","c84e07b1":"from sklearn.svm import SVC\n\naccuracy_SVC = []\n\nfor X_train_data, X_test_data in zip(X_train_datasets, X_test_datasets):\n    \n    svc = SVC()\n    svc = svc.fit(X_train_data, y_train)\n    \n    predictions = svc.predict(X_test_data)\n    accuracy_pred = accuracy_score(y_test, predictions)\n\n    accuracy_SVC.append(round(accuracy_pred, 3))\n    \naccuracy_SVC","49723f0f":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C':[0.001, 0.01, 0.1, 1.25, 1.5, 2],\n             'kernel': ['linear', 'poly', 'rbf'],\n             'degree': [1,2,3,4],\n             'gamma': ['scale', 'auto']}\n\nsvc = SVC()\ngrid_model = GridSearchCV(svc, param_grid, scoring='accuracy', n_jobs=-1)\n\ngrid_model.fit(st_scaled_X_train, y_train)\n\ngrid_model.best_params_","78426582":"print('Accuracy with grid model:', round(accuracy_score(y_test, grid_model.predict(st_scaled_X_test)), 3))","27a4b828":"svc = SVC()\nsvc = svc.fit(st_scaled_X_train, y_train)\n    \npredictions_svc = svc.predict(st_scaled_X_test)\n\nsvc_scores = []\n\nsvc_scores.append( score(y_test, predictions_svc, average='weighted')[0] )\nsvc_scores.append( score(y_test, predictions_svc, average='weighted')[1] )\nsvc_scores.append( score(y_test, predictions_svc, average='weighted')[2] )\nsvc_scores.append( accuracy_score(y_test, predictions_svc) )\n\ndf_svc = pd.DataFrame(svc_scores, columns=['svc'],\n            index=['Precision', 'Recall', 'f1_score', 'Accuracy'])\n\ndf_svc['svc'] = np.round(df_svc['svc'], 3)\n\ndf_svc","fddd3462":"from sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier(oob_score=True, \n                            random_state=42, \n                            warm_start=True,\n                            n_jobs=-1)\n\noob_list = list()\naccuracy_scores = []\nlist_of_trees = [15, 20, 30, 40, 50, 100, 150, 200, 300, 400, 500]\n\nfor n_trees in list_of_trees:\n    \n    RF.set_params(n_estimators=n_trees)\n    RF.fit(X_train, y_train)\n\n    oob_error = 1 - RF.oob_score_\n    \n    predictions = RF.predict(X_test)\n    accuracy_pred = accuracy_score(y_test, predictions)\n    accuracy_scores.append(accuracy_pred)\n    \n    oob_list.append(pd.Series({'n_trees': n_trees, 'oob_error': oob_error}))\n\nrf_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')\n\nrf_oob_df['accuracy'] = accuracy_scores\nrf_oob_df","1fc89184":"# oob error\nplt.figure(figsize=(9,6))\nplt.title('Out-of-Bag error plot')\nplt.xlabel('n_trees')\nsns.lineplot(data=rf_oob_df, x=list_of_trees, y='oob_error', marker='o');","119679d1":"# accuracy scores\nplt.figure(figsize=(9,6))\nplt.title('Accuracy of Random Forests')\nplt.xlabel('n_trees')\nsns.lineplot(data=rf_oob_df, x=list_of_trees, y='accuracy', marker='o');","3bb9b1d2":"rf = RF.set_params(n_estimators=100, warm_start=False)\nrf = rf.fit(X_train, y_train)\n    \npredictions_rf = rf.predict(X_test)\n\nrf_scores = []\n\nrf_scores.append( score(y_test, predictions_rf, average='weighted')[0] )\nrf_scores.append( score(y_test, predictions_rf, average='weighted')[1] )\nrf_scores.append( score(y_test, predictions_rf, average='weighted')[2] )\nrf_scores.append( accuracy_score(y_test, predictions_rf) )\n\ndf_rf = pd.DataFrame(rf_scores, columns=['rf'],\n            index=['Precision', 'Recall', 'f1_score', 'Accuracy'])\n\ndf_rf['rf'] = np.round(df_rf['rf'], 3)\n\ndf_rf","d35cf39d":"from sklearn.ensemble import GradientBoostingClassifier\n\nerror_list = list()\ntree_list = [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]\n\nfor n_trees in tree_list:\n\n    GBC = GradientBoostingClassifier(n_estimators=n_trees, random_state=42)\n    GBC.fit(X_train, y_train)\n    \n    y_pred_gb = GBC.predict(X_test)\n\n    error = 1.0 - accuracy_score(y_test, y_pred_gb)\n    error_list.append(pd.Series({'n_trees': n_trees, 'error': error}))\n\nerror_df_gb = pd.concat(error_list, axis=1).T.set_index('n_trees')\n\nerror_df_gb","8e614f52":"# Plotting the error\nplt.figure(figsize=(10,8))\nplt.title('Gradient Boosting Error')\nsns.lineplot(data=error_df_gb, marker='o');","d1fdd1b8":"gb= GradientBoostingClassifier(n_estimators=30, random_state=42)\ngb = gb.fit(X_train, y_train)\n    \npredictions_gb = gb.predict(X_test)\n\ngb_scores = []\n\ngb_scores.append( score(y_test, predictions_gb, average='weighted')[0] )\ngb_scores.append( score(y_test, predictions_gb, average='weighted')[1] )\ngb_scores.append( score(y_test, predictions_gb, average='weighted')[2] )\ngb_scores.append( accuracy_score(y_test, predictions_gb) )\n\ndf_gb = pd.DataFrame(gb_scores, columns=['gb'],\n            index=['Precision', 'Recall', 'f1_score', 'Accuracy'])\n\ndf_gb['gb'] = np.round(df_gb['gb'], 3)\n\ndf_gb","c2a6faef":"from sklearn.metrics import plot_confusion_matrix, classification_report\nsns.set_theme(style=\"white\")","101add13":"# Logistic Regression classification report\nprint(classification_report(y_test, predictions_lr))","9ff45907":"# Logistic Regression confusion matrix\nplot_confusion_matrix(lr, X_test, y_test);","9505cc06":"# KNN classification report\nprint(classification_report(y_test, predictions_knn))","07aa5e74":"# KNN confusion matrix\nplot_confusion_matrix(knn, st_scaled_X_test, y_test);","3c6389e8":"# SVC classification report\nprint(classification_report(y_test, predictions_svc))","94631a16":"# SVC confusion matrix\nplot_confusion_matrix(svc, st_scaled_X_test, y_test);","4a21d426":"# Random Forest\nprint(classification_report(y_test, predictions_rf))","aa1ee18b":"# Random Forest confusion matrix\nplot_confusion_matrix(rf, X_test, y_test);","d4c455b5":"# Gradient Boosting report\nprint(classification_report(y_test, predictions_gb))","67638148":"# Gradient Boosting confusion matrix\nplot_confusion_matrix(gb, X_test, y_test);","3ef3cd5a":"pd.concat((df_lr, df_knn, df_svc, df_rf, df_gb), axis=1)","41c94b3b":"rf_feature = pd.DataFrame(index=X.columns, data=rf.feature_importances_).sort_values(0)\n\nplt.figure(figsize=(10,5))\nsns.barplot(data=rf_feature, x=rf_feature.index.values, y=rf_feature[0].values)\nplt.title('Feature Importance Random Forest')\nplt.xticks(rotation=90);","7d460be9":"gb_feature = pd.DataFrame(index=X.columns, data=gb.feature_importances_).sort_values(0)\n\nplt.figure(figsize=(10,5))\nsns.barplot(data=gb_feature, x=gb_feature.index.values, y=gb_feature[0].values)\nplt.title('Feature Importance Gradient Boosting')\nplt.xticks(rotation=90);","d20c84ac":"Clearly these KNN models have performed way better on scaled data. Let's take a closer look at the peak score.","60e983db":"Next is the K-Nearest Neighbors model. In order to find the best K (the optimal number of neighbors), as well as what type of data (scaled or not scaled) suits our training best, we can run the following loop, ","7e0f67f1":"## KNN","5c8479e4":"The oob_error consistently goes down as the number of trees increases. Let's see how the accuracy behaves.","116e899e":"- We obtained 12 false positives out of 77 true negatives.\n- We obtained 11 false negatives out of 107 true positives.","3d73f6db":"Let's explore a pairplot across variables based on HeartDisease.","1d21fc50":"Clearly, the obtained models are pretty bad. Take a look at the best score per k:","31bb92f3":"We can see that the orange ratio (when HeartDisease is equal to 1) tends to get bigger compared to the blue part the more Age and Oldpeak increase.","51e1475c":"We choose the Random Forest model with n_trees=100.","b130af24":"We can see that the CV models didn't do better than the regular one, and that scaling the data didn't help either. Therefore, we simply choose and store the regular model trained on the regular data without scaling.","f1e4b92c":"## SVC report","68ed8467":"Both the models have considered the ST Slope (with Up:Upsloping) as their most important feature. Several other features have obtained high importance, such as Oldpeak, Sex_M (Masculine) and Cholesterol.","dfbd318f":"There doesn't seem to be high skewness (aside from the target, clearly), so we leave these features as they are.","978ebe70":"- We obtained 11 false positives out of 77 true negatives.\n- We obtained 12 false negatives out of 107 true positives.","66c1e9ab":"The KNN models also benefit a lot from normal scaled data, but not as well as with standard scaling. Thus, we choose our K value as 24, and we train on data with standard scaling as our best KNN model.","bca71948":"Let's first prepare our data in order to train our models. In the first three models (Logistic Regression, K-Nearest Neighbors and Support Vector Machine), we will use regular train\/test data as well as scaled data, such as standardized and normalized data. This will help us choose the correct kind of data for our models.","88297493":"The difference is not significant. Hence, we can assume that the target is balanced.","b384a83b":"The scatterplots don't actually seem to reveal interesting patterns. Let's take a look at correlation numbers with a heatmap.","a073f464":"# Introduction","7e4d0987":"The categorical features do seem to reveal having more relation to the target:\n\n1. Men are more than twice as likely to get a heart disease than women.\n2. Concerning chest pain type, it tunrs out that ASY (Asymptomatic) is the most common type among people with a heart disease.\n3. Excercise induced angina with Y (Yes) is more likely to have a heart disease than N (No).\n4. The slope of the peak exersice ST as flat is the most related one to the target.\n5. Fasting Blood Sugar as 1 (FastingBS > 120 mg\/dl) is a clear sign of potentially having a heart disease.","b20a390a":"The first thing we need to do, is to check if our target label is balance or unbalanced. A countplot should suffice to find out.","1d745595":"## Random Forest report","7017f7a9":"## SVC","84baa7f3":"# Exploratory Data Analysis","196206aa":"Pretty interesting info we find here:\n\n1. As suspected from the pairplot, the numerical features that are more correlated to a heart disease are Age and Oldpeak (remember that this feature is related to depression, so it makes sense). Fasting Blood Sugar is also significant, but we will see about this detail clearer as categorical feature below.\n\n\n\n2. Other numerical features don't have a significant correlation with HeartDisease. And actually, Max Heart Rate and Cholesterol have negative correlation with the target. But this is not what we're usually told by medics. A reasonable explanation is that these two features need some sort of interaction between other features in order to trigger a heart disease. ","d33b31f4":"As it is well known, we don't need to scale the data to train random forests models. So we will go with regular data without scaling.\n\nWe will try with several number of trees to find the best Random Forest model. We will keep track on the out-of-bag error as well as the accuracy on the test data.","91036c5c":"## Logistic Regression report","46dd2e0a":"## KNN report","bd84f878":"Let's compare the results obtained based on the model and type of train\/test data.","d6dc4724":"## Random Forest","4e396042":"The SVC has a similar behaviour compared to the KNN model: The model performs pretty bad when trained on data without scaling, but does well on standard scaled data.\n\n\nLet's now try to tune the model with Grid Search Cross Validation.","722c6d61":"It seems like a good choice to set n_trees=30. So let's store these results.","d68effa2":"- We obtained 10 false positives out of 77 true negatives.\n- We obtained 17 false negatives out of 107 true positives.","bbdf7e76":"- We obtained 7 false positives out of 77 true negatives.\n- We obtained 11 false negatives out of 107 true positives.","8f87ec8f":"## Gradient Boosting report","a640cb34":"In this notebook, we will explore the [Heart Failure Prediction Dataset](https:\/\/www.kaggle.com\/fedesoriano\/heart-failure-prediction). This dataset consists of 11 clinical features (such as age, sex, cholesterol, etc.) and 1 target label, namely HeartDisease, from 918 observations (patients). The main objective is to create a model that will succesfully predict whether a person has a cardiovascular disease or not, based on these 11 clinical features. Thus, this is a binary classification problem. A machine learning model would be of great help on early detections. \n\nSource info:\n\nThis dataset was created by combining different datasets already available independently but not combined before. In this dataset, 5 heart datasets are combined over 11 common features which makes it the largest heart disease dataset available so far for research purposes. The five datasets used for its curation are:\n\nCleveland: 303 observations\n\nHungarian: 294 observations\n\nSwitzerland: 123 observations\n\nLong Beach VA: 200 observations\n\nStalog (Heart) Data Set: 270 observations\n\nTotal: 1190 observations\n\nDuplicated: 272 observations\n\nFinal dataset: 918 observations\n\nEvery dataset used can be found under the Index of heart disease datasets from UCI Machine Learning Repository on the following link: https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/heart-disease\/","09f3c545":"- We obtained 10 false positives out of 77 true negatives.\n- We obtained 13 false negatives out of 107 true positives.","4fc21d6a":"# Data and Libraries","79d35cd7":"Let's now identify which columns are categorical, and which are numerical.","6f6f2845":"Thus, we have found our optimal K=24. \n\n\nLet's also check the case with normal scaled data.","3b0ae4ce":"Unfortunately, tuning the model didn't have a great impact on performance. Thus, we choose the base SVC model for our predictions.","5708c6c7":"It's time to see the results from our models. For each one of them, we'll take a look at their classification reports, confusion matrix and the number of false positives and false negatives.","504d06fd":"Finally, our last model is the Gradient Boosting, which is based on Random Forests. Thus, again, we don't need to worry about scaling the data.\n\nAs we did with RF, we try several number of trees to find the best one.","38810f65":"Our next model will be the Support Vector Machine Classifier. We will give the base model a try (on data with and without scaling), and then will try to tune the hyperparameters in order to find a better model.","483dbb94":"## Gradient Boosting","1120a71c":"## Logistic Regression","5b88b1e0":"There are 12 features in total (including the target). Here's the detailed explanation of each feature provided with the dataset:\n\n\n1. Age: Age of the patient in years\n2. Sex: Gender of the patient. [M: Male, F: Female]\n3. ChestPain Type: Type of chest pain[ TA: Typical Angina, ATA: Atypical Angina, NAP: Non Anginal Pain, ASY: Asymptomatic]\n4. Resting BP: Resting blood pressure in units mm Hg\n5. Cholesterol: Serum Cholesterol in units mm\/dl\n6. FastingBS: Fasting Blood Sugar 1 if FastingBS > 120 mg\/dl, 0 Otherwise\n7. RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n8. MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n9. ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n10. Oldpeak: oldpeak = ST [Numeric value measured in depression]\n11. ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n\nOur goal is to predict:\n\n12. HeartDisease: output class [1: heart disease, 0: Normal]","c4dbf46a":"Let's plot the results.","854768e3":"# Model Evaluation","2b6a5efc":"We separate our data into train\/test, by randomly leaving 20% of the data as a validation set. Next, we will use the standard scaler and the normal (MinMax) scaler from scikit learn to scale the train data and store these three obtained sets (with and without scaling).","f87cab29":"Let's take a look at both the oob_error, as well as the accuracy scores obtained by the number of trees.","88fbd580":"As we can see, all of our models obtained decent results. The Logistic Regression model performed slightly worse than the others, and the Support Vector Classifier, Random Forest and Gradient Boosting got exactly the same scores, but with slight differences with False Positives\/Negatives. Finally, the KNN model performed best, reaching 90% accuracy which is impressive.","ca665edb":"# Model Building","25171d4e":"We'll take a look at what the distribution of the numerical columns looks like.","3e4db3f8":"To finish this notebook, we can also take a look at the feature importance of both Random Forest and Gradient Boosting.","3f6812d0":"Our first model to try will be Logistic Regression. We will also test how well the Cross Validation version of the LR model performs, considering the two types of penalty L1 and L2.\n\nRemember also that our dataset is not really unbalanced, so it might be better to just look at how good accuracy is. However, we will take into account the other common scores for classification problems, namely precision, recall and f1_score.","eee4074e":"# Feature Engineering","33c14532":"It seems that older people tend to be more vulnerable to heart diseases, and also people with high oldpeak (measured in depression). We check this with a pair of histplots.","1dcbebc8":"We will now fit our three models on our three separate kinds of data (without scaling, with standard scaling and normal scaling).","9db1834c":"We will now explore how the categorical features relate to the likelihood of having a heart disease.","2a933867":"# Final Comments","16093e94":"It should be noted that although FastingBS is stored as integer, since it only has two values (1 if FastingBS > 120 mg\/dl, 0 otherwise), it's better if we treat it as categorical."}}