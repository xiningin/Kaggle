{"cell_type":{"6b220b3f":"code","2a299698":"code","16d60ea6":"code","c8d7cfbd":"code","09f48032":"code","55611006":"code","98c7d844":"code","ace7d936":"code","8a887838":"code","da999bc8":"code","e4d793ba":"code","328bdc9b":"code","21f940c7":"code","a63e48ab":"code","87b8f268":"code","a175c9de":"code","d63f3228":"code","1065ad4b":"code","3a2752d8":"code","049ea718":"code","d2ffb67a":"code","4a83c6dc":"code","1d84c1ab":"code","7ef40374":"code","d14e8579":"code","81c3a485":"code","ba44db89":"code","006001d4":"code","5ee3650b":"markdown","bc4bccca":"markdown","f35fc8f5":"markdown","5647cb54":"markdown","6b22e437":"markdown","7274ae6f":"markdown","068b4ec9":"markdown","1dcc24b5":"markdown","6826b0f4":"markdown","f5698269":"markdown","b0d534a3":"markdown","49a56602":"markdown","9e5dd90f":"markdown","dd34e82b":"markdown","88c52429":"markdown","a7c1ede0":"markdown"},"source":{"6b220b3f":"import pandas as pd\nimport numpy as np\nimport re  # \u6b63\u5219\u5316\u8868\u8fbe\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\n%matplotlib inline\n\nimport missingno as mg  # \u7528\u4e8e\u67e5\u770b\u7f3a\u5931\u503c\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# \u75285\u4e2a\u6a21\u578b\u505a\u7b2c\u4e00\u5c42\u878d\u5408\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold","2a299698":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ny = train['Survived'].ravel()  # \u4fdd\u5b58\u6807\u7b7e\ny = pd.DataFrame({'Survived': y})\n\nPassengerId = test['PassengerId']  # \u628a\u6d4b\u8bd5\u96c6index\u53d6\u51fa\u6765\n\nprint(f'\u8bad\u7ec3\u96c6\u6709{train.shape[0]}\u884c{train.shape[1]}\u5217')\nprint(f'\u6d4b\u8bd5\u96c6\u6709{test.shape[0]}\u884c{test.shape[1]}\u5217')\n# \u6d4b\u8bd5\u96c6\u6ca1\u6709\u7ed3\u679c(\u662f\u5426\u5b58\u6d3b)\nntrain = train.shape[0]\nntest = test.shape[0]\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\n\nall_data.drop(['Survived', 'PassengerId'], axis=1, inplace=True)\n","16d60ea6":"all_data.describe().T  # \u6570\u636e\u6027\u5206\u6790(\u4e0d\u5305\u62ec\u6587\u5b57\u7279\u5f81)","c8d7cfbd":"# \u7edf\u8ba1\u8bad\u7ec3\u96c6\u7279\u5f81\u53d8\u91cf\u7684\u5c5e\u6027\u548c\u5185\u5b58\nall_data.info()","09f48032":"# \u4f7f\u7528missingno\u8fdb\u884c\u7f3a\u5931\u503c\u53ef\u89c6\u5316\nmg.matrix(all_data)","55611006":"# \u7edf\u8ba1\u6bcf\u4e2a\u53d8\u91cf\u7f3a\u5931\u7684\u6bd4\u4f8b\ndef missing_percentage(df):  \n    # \u603b\u7f3a\u5931\u503c\u7531\u9ad8\u5230\u4f4e\u6392\u5e8f\n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().\n                                                             sum().sort_values(ascending = False) != 0]\n    # \u7f3a\u5931\u767e\u5206\u6bd4\n    percentage = round(df.isnull().sum().sort_values(ascending = False) \/ len(df) * 100, 2)[df.isnull().\n                                                            sum().sort_values(ascending = False) != 0]\n    # axis=1: \u6a2a\u7740\u5e76\u62e2\n    return pd.concat([total, percentage], axis=1, keys=['Total', 'Percentage'])\nmissing_percentage(all_data)","98c7d844":"# Cabin\u7f3a\u592a\u591a,\u76f4\u63a5\u7528\u65b0\u7279\u5f81\u6709\u65e0Cabin\u4ee3\u66ff,\u6240\u4ee5\u53ea\u7528\u586b\u4e09\u4e2a: Fare, Embarked, Age\n# 1.Fare\u7f3a\u4e00\u4e2a(\u6d4b\u8bd5\u96c6\u91cc),\u7528test\u7684\u4e2d\u4f4d\u6570\u586b\u5145\nall_data['Fare'] = all_data['Fare'].fillna(test['Fare'].median())\n# 2.Embarked\u7f3a\u4e00\u4e2a,\u76f4\u63a5\u7528S\u586b\u5145\nall_data['Embarked'] = all_data['Embarked'].fillna('S')\n# 3.Age,\u7528\u968f\u673a\u6570\u586b\u8865\nage_avg = all_data['Age'].mean()\nage_std = all_data['Age'].std()\nage_null_count = all_data['Age'].isnull().sum()\nage_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\nage_nan_index = np.isnan(all_data['Age'])\nall_data['Age'][age_nan_index] = age_null_random_list\nall_data['Age'] = all_data['Age'].astype(int)","ace7d936":"# \u68c0\u67e5\u586b\u8865\u60c5\u51b5, Cabin\u4e0d\u7ba1\nmissing_percentage(all_data)","8a887838":"# \u65b0\u589e\u4e00\u4e9b\u7279\u5f81\n\n# \u6539\u753b\u98ce\u4ee5\u53ca\u8bbe\u7f6e\u56fe\u7247\u5927\u5c0f\u4ee3\u7801!!!\nstyle.use('fivethirtyeight')\nfig, axes = plt.subplots(3, 1, constrained_layout = True, figsize=(18,36))  # \u5b9a\u4e49\u6846\u67b6\n\n# 1.\u540d\u5b57\u957f\u5ea6\nall_data['Name_length'] = all_data['Name'].apply(len)\n# 2.\u662f\u5426\u6709\u8231, null\u5373\u4ee3\u8868\u6ca1\u6709\u8231\nall_data['Has_Cabin'] = 1 - all_data['Cabin'].isnull()\n# \u9ad8\u7ea7\u5199\u6cd5, null\u662ffloat\u6570\u636e\u7c7b\u578b\n# all_data['Has_Cabin'] = all_data[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n# 3.\u5bb6\u65cf\u89c4\u6a21, \u5144\u5f1f\u59d0\u59b9,\u4f34\u4fa3,\u7238\u5988\u5c0f\u5b69\u52a0\u81ea\u5df1\nall_data['FamilySize'] = all_data['SibSp'] + all_data['Parch'] + 1\n# 4.\u5b64\u8eab\u4e00\u4eba\nall_data['IsAlone'] = 0\nall_data.loc[all_data['FamilySize'] == 1, 'IsAlone'] = 1\n# 5.\u7968\u4ef7\u6863\u6b21(\u5747\u52064\u6863)\nall_data['CategoricalFare'] = pd.qcut(all_data['Fare'], 4)  \n\nsns.countplot(all_data['CategoricalFare'], ax=axes[0])\n# pd.qcut: \u6570\u636e\u5206\u7bb1,\u628a\u8fde\u7eed\u578b\u6570\u5b57\u5747\u5206\u62104\u4e2a\u79bb\u6563\u6863\u6b21(0%~25%, 25%~50%, 50%~75%, 75%~100%),\u9002\u7528\u4e8e\u4e0a\u4e0b\u9650\u5206\u5e03\u4e0d\u5747\u7684\u6570\u636e\n# pd.cut: \u7b49\u8ddd\u5206\u7bb1,\u628a\u8fde\u7eed\u578b\u6570\u5b57\u62104\u4e2a\u7b49\u8ddd\u79bb\u79bb\u6563\u6863\u6b21(0~25\u5c81, 25~50, 50~75, 75~100),\u9002\u7528\u4e8e\u5747\u5300\u5206\u5e03\u7684\u6570\u636e\u96c6\n# 6.\u5e74\u9f84(\u7b49\u8ddd\u5206\u7bb1)\nall_data['CategoricalAge'] = pd.cut(all_data['Age'], 5)\nsns.countplot(all_data['CategoricalAge'], ax=axes[1])\n# 7.\u79f0\u53f7(Ex:\u8239\u957f, \u670d\u52a1\u5458, \u535a\u58eb...)\n# \u53d6\u51fa.\u524d\u9762(\u5305\u62ec.)\u7684\u5b57\u7b26\u4e32\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # re.search:\u627e\u51fa\u7b2c\u4e00\u4e2a\u7b26\u5408\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u5b57\u7b26\u4e32,\u5982\u679c\u6ca1\u627e\u5230\u8fd4\u56deNone\n    if title_search:\n        return title_search.group(1)\n    return \"\"\nall_data['Title'] = all_data['Name'].apply(get_title)\n# \u753b\u76f4\u65b9\u56fe\u770b\u4e0b\u54ea\u4e9b\u79f0\u53f7\u662f\u51b7\u95e8\u79f0\u53f7\n\nsns.countplot(all_data['Title'], ax=axes[2])  # \u5728\u6846\u67b6\u91cc\u753b\u753b\n\naxes[0].set_title('Fare distribution')  # \u8bbe\u7f6e\u56fe\u7247\u6807\u9898\naxes[1].set_title('Age distribution')\naxes[2].set_title('Title of Name')","da999bc8":"# \u628a\u51b7\u95e8\u79f0\u53f7\u7edf\u4e00\u4e3aRare, (Countess:\u4f2f\u7235\u592b\u4eba)\nall_data['Title'] = all_data['Title'].replace(['Lady', 'Countess','Capt', 'Col', \n                                'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n# \u5f52\u5e76\u5230\u540c\u4e49\u8bcd\nall_data['Title'] = all_data['Title'].replace('Mlle', 'Miss')\nall_data['Title'] = all_data['Title'].replace('Ms', 'Miss')\nall_data['Title'] = all_data['Title'].replace('Mme', 'Mrs')\n# \u518d\u770b\u4e00\u4e0b\u5206\u5e03\u60c5\u51b5\nfig, ax1 = plt.subplots(constrained_layout = True, figsize=(18,12))  \nsns.countplot(all_data['Title'], ax=ax1)  ","e4d793ba":"# 1.\u6027\u522b\nall_data['Sex'] = all_data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n# 2.\u79f0\u53f7\ntitle_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \"Master\": 3, \"Rare\": 4}\nall_data['Title'] = all_data['Title'].map(title_mapping)\n# 3.\u767b\u8239\u70b9\nall_data['Embarked'] = all_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\n# \u4ee5\u4e0b\u76844&5\u7684\u5206\u6cd5\u7531\u4e0a\u9762\u7684\u56fe\u5206\u6790\u5f97\u6765\u7684\n# 4.\u8239\u7968\u94b1(\u52064\u6863)\n# .loc[\u884c, \u5217] = \u5b9a\u4f4d\u8d4b\u503c\nall_data.loc[all_data['Fare'] <= 7.91, 'Fare'] = 0\nall_data.loc[(all_data['Fare'] > 7.91) & (all_data['Fare'] <= 14.454), 'Fare'] = 1\nall_data.loc[(all_data['Fare'] > 14.454) & (all_data['Fare'] <= 31), 'Fare'] = 2\nall_data.loc[all_data['Fare'] > 31, 'Fare'] = 3\n# \u5b57\u7b26\u4e32 -> \u6574\u578b \nall_data['Fare'] = all_data['Fare'].astype(int)\n# 5.\u5e74\u9f84(\u52065\u6863)\nall_data.loc[all_data['Age'] <= 16, 'Age'] = 0\nall_data.loc[(all_data['Age'] > 16) & (all_data['Age'] <= 32), 'Age'] = 1\nall_data.loc[(all_data['Age'] > 32) & (all_data['Age'] <= 48), 'Age'] = 2\nall_data.loc[(all_data['Age'] > 48) & (all_data['Age'] <= 64), 'Age'] = 3\nall_data.loc[all_data['Age'] > 64, 'Age'] = 4 ;\nall_data.head()","328bdc9b":"drop_features = ['Name', 'Ticket', 'Cabin', 'SibSp', 'CategoricalAge', 'CategoricalFare']\nall_data = all_data.drop(drop_features, axis=1)\nall_data.head()","21f940c7":"train = all_data[:ntrain]\ntest = all_data[ntrain:]\n\ntrain_with_survived = pd.concat((train, y), axis=1)\nprint(train_with_survived.head())\n# \u753b\u4e00\u4e2a\u591a\u91cd\u5171\u7ebf\u6027(train.corr())\u7684\u5173\u7cfb\u7684\u70ed\u529b\u56fe\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize=(18, 12))\n# \u641e\u4e00\u4e2a\u63a9\u7801mask\u76d6\u4f4f\u4e0a\u4e09\u89d2\u90e8\u5206\nmask = np.zeros_like(train_with_survived.corr(), dtype=np.bool)\n# np.triu_indices_from(mask)  # \u8fd4\u56de\u4e0a\u4e09\u89d2\u77e9\u9635\u7684idx\nmask[np.triu_indices_from(mask)] = True  # \u4e0a\u4e09\u89d2\u5168\u906e\u6389\nsns.heatmap(train_with_survived.corr(), cmap=sns.diverging_palette(20, 220, n=200),  # \u8c03\u8272\n           mask=mask, annot=True, # \u5728\u683c\u5b50\u91cc\u5199\u5165\u6570\u5b57\n           center=0,  # \u70ed\u529b\u56fe\u4e2d\u95f4\u7684\u989c\u8272\n           )\nplt.title('heatmap of features', fontsize=30)","a63e48ab":"# train_with_survived.to_csv('processed_train.csv', index=False)\n# test.to_csv('processed_test.csv', index=False)","87b8f268":"SEED = 0  # \u4fdd\u8bc1\u7ed3\u679c\u4e00\u6837\nNFOLDS = 5  # \u6bcf\u6b21\u5207\u5206\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u6bd4\u4f8b4\u6bd41\nkf = KFold(NFOLDS, shuffle=False)\n\n# sklearn\u878d\u5408\u6a21\u578b\u901a\u7528\u8bad\u7ec3\u6a21\u5757\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):  # train\u4e0d\u8fd4\u56de\u6a21\u578b\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):                  # fit\u8fd4\u56de\u6a21\u578b\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):  # \u770b\u54ea\u4e9b\u7279\u5f81\u6bd4\u8f83\u5173\u952e\n        print(self.clf.fit(x,y).feature_importances_)\n        \n# \u5207\u5206\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):  # \u5faa\u73af\u6d17\u724c\u4ea7\u751f\u65b0\u7684\u8bad\u7ec3\u96c6&\u6d4b\u8bd5\u96c6\u5bf9\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)  # \u7528\u968f\u51fa\u6765\u7684\u8fd9\u4e00\u7ec4\u8bad\u7ec3\u96c6\u8bad\u7ec3\n\n        oof_train[test_index] = clf.predict(x_te)  # \u586b\u7a7a: 5\u6b21\u8bad\u7ec3\u7684\u5408\u96c6\u5728\u8bad\u7ec3\u96c6\u4e0a\u7684\u8868\u73b0\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)  # 5\u4e2a\u6a21\u578b\u7684test\u8f93\u51fa\u53d6\u5e73\u5747\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","a175c9de":"# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',  # \u7ebf\u6027\u5206\u5272,\u5373SVM\n    'C' : 0.025\n    }\n\n\n# Create 5 objects that represent our 4 models\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\n\n# \u5b58\u4e00\u4efdnumpy\ny_train = y['Survived'].ravel()\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n# \u8bad\u7ec35\u79cd\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u4f5c\u4e3a\u65b0\u7279\u5f81\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test)    # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test)     # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test)     # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test)  # Support Vector Classifier","d63f3228":"# \u67e5\u770b\u7279\u5f81\u91cd\u8981\u6027,\u8fd9\u4e2a\u6ca1\u6709\u8fd4\u56de\u503c,svc\u6ca1\u6709\u67e5\u770b\u7279\u5f81\u91cd\u8981\u503c\u7684\u529f\u80fd\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n# \u518d\u5b58\u4e00\u4e0b,\u7b49\u4e0b\u753b\u56fe\nrf_features = [0.12293045, 0.19933909, 0.03011221, 0.02078426, 0.07190575, 0.02402937,\n 0.11061079, 0.06533695, 0.06884935, 0.01334137, 0.2727604 ]\net_features = [0.12063052, 0.37921456, 0.03001601, 0.01608535, 0.05543606, 0.02858977,\n 0.04730554, 0.08336605, 0.04363067, 0.02182896, 0.17389651]\nada_features = [0.032, 0.014, 0.016, 0.06,  0.04,  0.01,  0.688, 0.014, 0.052, 0.004, 0.07 ]\ngb_features = [0.08762653, 0.01263095, 0.04843801, 0.01295041, 0.05270699, 0.02623192,\n 0.17676883, 0.03698747, 0.11064902, 0.00595497, 0.42905489]","1065ad4b":"cols = train.columns.values\n\nprint(cols)","3a2752d8":"feature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\nprint(feature_dataframe.shape)\nfeature_dataframe['features'].values","049ea718":"# \u968f\u673a\u68ee\u6797\ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='greens',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2,\n        mirror=True,\n        ticks='outside',\n        showline=True\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","d2ffb67a":"# \u989d\u5916\u6811\ntrace = go.Scatter(\n    y = feature_dataframe['Extra Trees  feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Extra Trees  feature importances'].values,\n        colorscale='greens',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title= 'Extra Trees Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2,\n        mirror=True,\n        ticks='outside',\n        showline=True\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","4a83c6dc":"# AdaBoost\ntrace = go.Scatter(\n    y = feature_dataframe['AdaBoost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['AdaBoost feature importances'].values,\n        colorscale='greens',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title= 'AdaBoost Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2,\n        mirror=True,\n        ticks='outside',\n        showline=True\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","1d84c1ab":"# Gradient Boost\ntrace = go.Scatter(\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale='greens',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title= 'Gradient Boosting Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2,\n        mirror=True,\n        ticks='outside',\n        showline=True\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","7ef40374":"# \u5e73\u5747\u4e00\u4e0b\u6a21\u578b\u7684\u7279\u5f81\u91cd\u8981\u5ea6\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1: \u6a2a\u7740\u53d6\u5e73\u5747\nfeature_dataframe.head(3)","d14e8579":"y = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='greens',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n    xaxis= dict(\n        title= 'Pop',\n        ticklen= 5,\n        zeroline= False,\n        gridwidth= 2,\n        mirror=True,\n        ticks='outside',\n        showline=True\n    ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2,\n        mirror=True,\n        ticks='outside',\n        showline=True\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='bar-direct-labels')","81c3a485":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n     'GradientBoost': gb_oof_train.ravel(),\n     'Support Vector Classifier': svc_oof_train.ravel(),\n     'Real y': y_train,\n    })\nbase_predictions_test = pd.DataFrame( {'RandomForest': rf_oof_test.ravel(),\n     'ExtraTrees': et_oof_test.ravel(),\n     'AdaBoost': ada_oof_test.ravel(),\n     'GradientBoost': gb_oof_test.ravel(),\n     'Support Vector Classifier': svc_oof_test.ravel(),\n    })\nprint(base_predictions_train.head())\nprint(base_predictions_train.shape)\n# print(et_oof_test.shape)\nbase_predictions_train.to_csv('5train.csv', index=False)\nbase_predictions_test.to_csv('5test.csv', index=False)","ba44db89":"# \u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6,\u6216\u8005\u8bf4\u5173\u8054\u6027\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='greens',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","006001d4":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n eval_metric = 'logloss',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"ensemble_model.csv\", index=False)","5ee3650b":"### \u7b80\u5355\u5206\u6790\u4e00\u4e0b, \u5b58\u6d3b\u7387: \u4e00\u7b49\u5ea7>\u4e09\u7b49\u5ea7, \u5973>\u7537, \u8239\u7968\u8d35>\u4fbf\u5b9c, \u540d\u5b57\u957f>\u77ed, \u6709Cabin>\u6ca1\u6709, \u5b64\u5bb6\u5be1\u4eba\u5bb9\u6613\u6b7b, \u9ad8\u7ea7\u804c\u79f0\u5bb9\u6613\u5b58\u6d3b","bc4bccca":"# \u7279\u5f81\u5de5\u7a0b","f35fc8f5":"# \u624b\u52a8\u5c06n\u79cd\u7c7b\u578b\u7279\u5f81\u8f6c\u5316\u6210\u6570\u5b57(0~n-1)","5647cb54":"# \u641e\u8bad\u7ec3","6b22e437":"# \u52a0\u8f7d\u6570\u636e","7274ae6f":"### \u770b\u7740\u8212\u670d\u591a\u4e86","068b4ec9":"# \u586b\u8865\u7f3a\u5931\u503c","1dcc24b5":"# \u628a5\u4e2a\u4e00\u7ea7\u6a21\u578b\u7684\u8f93\u5165\u5582\u5165XGB,\u8f93\u51fa\u6700\u540e\u7684\u9884\u6d4b\u7ed3\u679c","6826b0f4":"## \u5220\u9664\u4e00\u4e9b\u6ca1\u7528\u7684\u7279\u5f81:\n### Ex: \u540d\u5b57, \u8239\u7968\u5e8f\u53f7, \u8239\u8231, \u5144\u5f1f\u59d0\u59b9\u4f34\u4fa3, \u8fd8\u6709\u7528\u4e8e\u89c2\u5bdf\u7684\u5df2\u7ecf\u6570\u5b57\u5316\u7684\u5e74\u9f84\u3001\u8239\u8d39\u79cd\u7c7b","f5698269":"# \u53ef\u4ee5\u770b\u5230\u79f0\u53f7\u548c\u6027\u522b\u6bd4\u8f83\u91cd\u8981","b0d534a3":"# \u5bfc\u5165\u5305","49a56602":"# \u8f93\u51fa\u4e00\u4e0b\u5207\u597d\u7684\u6570\u636e","9e5dd90f":"# \u753b\u4e00\u4e0b\u4e0d\u540c\u6a21\u578b\u4e2d\u7279\u5f81\u7684\u91cd\u8981\u6027","dd34e82b":"# \u7f3a\u5931\u503c\u7edf\u8ba1","88c52429":"## \u7528\u5b57\u5178\u5f62\u5f0f\u89c4\u5b9a\u4e0d\u540c\u6a21\u578b\u7684\u53c2\u6570","a7c1ede0":"### \u7528\u51e0\u4e2a\u6a21\u578b\u878d\u5408,\u5c06\u4e0d\u540c\u6a21\u578b\u7684\u9884\u6d4b\u503c\u9001\u5165\u53e6\u4e00\u4e2a\u6a21\u578b,\u518d\u5c06\u878d\u5408\u540e\u7684\u6a21\u578b\u4e0e\u5176\u4ed6\u6a21\u578b\u505a\u52a0\u6743\u5e73\u5747"}}