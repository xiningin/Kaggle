{"cell_type":{"a5b72cde":"code","be2ea193":"code","42f1be29":"code","32081e1d":"code","c571ed16":"code","404ef1a8":"code","085d66fd":"code","be14c918":"code","2ae03f82":"code","1b35cb76":"code","ea1f5c78":"code","50d5dd10":"code","0c4ad9ce":"code","1a1a011e":"code","8fb8e866":"code","cfe27a16":"code","caead13e":"code","b5470ca9":"code","5c217364":"code","7b5db6df":"code","68515f57":"code","63588853":"code","4117f4e4":"code","c81d66f6":"code","fd856aad":"code","be145643":"code","c1293a22":"code","c3f4b2c0":"code","01d919be":"code","d53bc0bb":"code","131569eb":"code","6c125457":"code","b00a3554":"code","a01ee8ad":"code","f0ee5279":"code","90bba872":"code","cf3c1979":"code","718f1f08":"code","fe28eb2b":"markdown","0751f0d9":"markdown","ea31f544":"markdown","598c6ea2":"markdown","83458015":"markdown","1bbf0ff4":"markdown","e6b77835":"markdown","45a638de":"markdown","fc35725b":"markdown","2f25fa4c":"markdown","2398da7c":"markdown","6bc24f75":"markdown","7508ec9c":"markdown","962edfa5":"markdown","5905ce5d":"markdown","cbd6bd45":"markdown","4288d9e4":"markdown","e46716b8":"markdown","60ce697e":"markdown","602dad37":"markdown","69a8a6b5":"markdown","0d3a25b7":"markdown","85f3c92c":"markdown","f8de5fa6":"markdown","65f5dcc8":"markdown","892548cd":"markdown","55c99a24":"markdown","f7d57396":"markdown","65578f27":"markdown","38b56395":"markdown","14587d56":"markdown","b63ff8ea":"markdown","6ab31211":"markdown","083c082a":"markdown","741b9c1d":"markdown","c3a0f474":"markdown","ed3e3211":"markdown","93cd2445":"markdown","2de5fc38":"markdown"},"source":{"a5b72cde":"from keras.datasets import fashion_mnist, mnist\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout\nfrom keras.models import Model\n\nimport os,cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20, 10\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd# Any results you write to the current directory are saved as output.\nfrom IPython.display import display, Image\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n\n# Any results you write to the current directory are saved as output.\nfrom IPython.display import display, Image","be2ea193":"# get the data\nfilname = '..\/input\/facial-expression\/fer2013\/fer2013.csv'\n\n#different labels of images(not useful known about for current problem)\nlabel_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n\n#different features names\nnames=['emotion','pixels','usage']\n\n#Reading data in dataframe\ndf=pd.read_csv('..\/input\/facial-expression\/fer2013\/fer2013.csv',names=names, na_filter=False)\nim=df['pixels']\ndf.head(10)","42f1be29":"#reading data and labels from dataset and appending in list\n\ndef getData(filname):\n    # images are 48x48\n    # N = 35887\n    Y = []\n    X = []\n    first = True\n    for line in open(filname):\n        if first:\n            first = False\n        else:\n            row = line.split(',')\n            Y.append(int(row[0]))\n            X.append([int(p) for p in row[1].split()])\n\n    X, Y = np.array(X), np.array(Y)\n    return X, Y","32081e1d":"#extracting data from dataset\nX, Y = getData(filname)\nnum_class = len(set(Y))\n#print(num_class)","c571ed16":"# keras with tensorflow backend\nN, D = X.shape\n\n#reshaping the dataset\nX = X.reshape(N, 48, 48, 1)","404ef1a8":"#splitting data in train, test\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)","085d66fd":"#Taking 5000 images \n\nx_train = x_train[:5000]\nx_test = x_test[:5000]","be14c918":"x_train.shape","2ae03f82":"#NOrmalizing the images\nx_train = x_train.astype('float32') \/ 255.\nx_test = x_test.astype('float32') \/ 255.\n\n#reshaping the images\nx_train = np.reshape(x_train, (len(x_train), 48, 48, 1))  # adapt this if using `channels_first` image data format\nx_test = np.reshape(x_test, (len(x_test), 48, 48, 1))  # adapt this if using `channels_first` image data format\n\n\n#adding noise in data\nnoise_factor = 0.1\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n\n#clipping put data near to 0--->0 aand data near to 1-->1(eg=0.3-->0 or 0.7-->1)\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)","1b35cb76":"n = 10","ea1f5c78":"plt.figure(figsize=(48, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_train_noisy[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","50d5dd10":"display(Image(filename=\"\/kaggle\/input\/images-architecture\/images_architecture\/autoencoder.png\"))","0c4ad9ce":"display(Image(filename=\"\/kaggle\/input\/autoencoder-unet\/autoencoder_unet\/autoencoder.png\"))","1a1a011e":"input_img = Input(shape=(48, 48, 1))  # adapt this if using `channels_first` image data format\n\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Dropout(0.2)(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\n\n\n# at this point the representation is (7, 7, 32)\n\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Dropout(0.2)(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\n\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = Model(input_img, decoded)\nautoencoder.compile(optimizer='adam', loss='MSE')","8fb8e866":"autoencoder.summary()","cfe27a16":"autoencoder.fit(x_train_noisy, x_train,\n                epochs=35,\n                batch_size=64,\n                shuffle=True,\n                validation_data=(x_test_noisy, x_test))","caead13e":"epochs = range(len(autoencoder.history.history['loss']))\n\nplt.plot(epochs,autoencoder.history.history['loss'],'r', label='train_loss')\nplt.plot(epochs,autoencoder.history.history['val_loss'],'b', label='val_loss')\nplt.title('train_loss vs val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.figure()","b5470ca9":"predict = autoencoder.predict(x_test_noisy)","5c217364":"n=10","7b5db6df":"plt.figure(figsize=(40, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_test[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","68515f57":"plt.figure(figsize=(40, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_test_noisy[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","63588853":"plt.figure(figsize=(40, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(predict[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","4117f4e4":"from skimage.measure import compare_ssim\nfrom skimage import data, img_as_float","c81d66f6":"compare_ssim(x_test, predict, multichannel=True)","fd856aad":"display(Image(filename=\"\/kaggle\/input\/images-architecture\/images_architecture\/unet.png\"))","be145643":"display(Image(filename=\"\/kaggle\/input\/autoencoder-unet\/autoencoder_unet\/autoencoder_unet.png\"))","c1293a22":"import tensorflow as tf","c3f4b2c0":"input_img = tf.keras.layers.Input(shape=(48,48,1))\n\nx1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\nx1 = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x1)\nx_drop = tf.keras.layers.Dropout(0.2)(x1)\nx = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x_drop)\nencoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n\n\n# at this point the representation is (7, 7, 32)\n\nx = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\nx = tf.keras.layers.concatenate([x,x1])\nx = tf.keras.layers.Dropout(0.2)(x)\n\n\nx = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\ndecoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder_unet = tf.keras.Model(input_img, decoded)\nautoencoder_unet.compile(optimizer='adam', loss='MSE')","01d919be":"autoencoder_unet.summary()","d53bc0bb":"autoencoder_unet.fit(x_train_noisy, x_train,\n                epochs=35,\n                batch_size=64,\n                shuffle=True,\n                validation_data=(x_test_noisy, x_test))","131569eb":"predict = autoencoder_unet.predict(x_test_noisy)","6c125457":"n=10","b00a3554":"plt.figure(figsize=(40, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_test[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","a01ee8ad":"plt.figure(figsize=(48, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_test_noisy[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","f0ee5279":"plt.figure(figsize=(40, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(predict[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","90bba872":"epochs = range(len(autoencoder.history.history['loss']))\n\nplt.plot(epochs,autoencoder_unet.history.history['loss'],'r', label='train_loss')\nplt.plot(epochs,autoencoder_unet.history.history['val_loss'],'b', label='val_loss')\nplt.title('train_loss vs val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.figure()","cf3c1979":"compare_ssim(x_test, predict, multichannel=True)","718f1f08":"import pickle\n\n# save the model to disk\nprint(\"[INFO] Saving model...\")\npickle.dump(autoencoder_unet,open('unet_model.pkl', 'wb'))","fe28eb2b":"## Making Prediction","0751f0d9":"**Anwser taken from quora: https:\/\/www.quora.com\/Why-is-U-Net-considered-as-an-autoencoder**\n\nThe classical auto-encoder architecture has the following property:\n- First, it takes an input and reduces the receptive field of the input as it goes through the layers of its encoder units. Finally at the end of the encoder part of the architecture, the input is reduced to a linear feature representation.\n- Next, the linear feature representation is upsampled (or its receptive field increased) by the decoder portion of the auto-encoder. So that at the other end of the autoencoder the result is of the same dimension as the input it received.\nSuch an architecture is ideal for preserving the dimensionality of input->output. But, the linear compression of the input leads to a bottleneck that does not transmit all features.\n\nThe U-Net has both the properties listed above, but it uses deconv units and overcomes the bottleneck limitation by adding skip connections that allow feature representations to pass through the bottleneck.","ea31f544":"## AutoEncoder Architecture","598c6ea2":"# Visualizing the prediction","83458015":"# Data Preprocessing","1bbf0ff4":"# Construction of Model","e6b77835":"# Generated Test images","45a638de":"## Reshaping images","fc35725b":"# Structural Similarity Index\n\n* When comparing images, the mean squared error (MSE)--while simple to implement--is not highly indicative of perceived similarity. Structural similarity aims to address this shortcoming by taking texture into account","2f25fa4c":"# Construction of Model","2398da7c":"## Adding labels and images(pixel values) in respective array","6bc24f75":"# Not quite good. Lets try using the concept of unet in AutoEncoder","7508ec9c":"# Training Model","962edfa5":"# Model Architecture we are Constructing","5905ce5d":"# Saving Model","cbd6bd45":"# AutoEncoder Summary","4288d9e4":"# UNET: Train Loss VS validation loss","e46716b8":"# Visualization of 10 Data","60ce697e":"# AutoEncoder Summary","602dad37":"# Training Model","69a8a6b5":"**Refer to Keras Blog for better idea : https:\/\/blog.keras.io\/building-autoencoders-in-keras.html**","0d3a25b7":"**I am trying to use the concept:**\n* adding skip connections that allow feature representations to pass through the bottleneck in autoencoder\n* If you find this useful, do upvote","85f3c92c":"* Refer to original paper for better idea: https:\/\/arxiv.org\/abs\/1505.04597\n* Implementation : https:\/\/towardsdatascience.com\/u-net-b229b32b4a71","f8de5fa6":"## Generated Test images","65f5dcc8":"# Structural Similarity Index\n\n* When comparing images, the mean squared error (MSE)--while simple to implement--is not highly indicative of perceived similarity. Structural similarity aims to address this shortcoming by taking texture into account","892548cd":"# Noised Test images","55c99a24":"# Summary\n\n* Whole point of doing this is making UNET concept more clear, so that it is easy to follow for beginners. As, the original paper is bit complex.\n* Loss has reduce incase of UNET more\n* More clear image is generated in case of original UNET described in paper\n* Refer to: https:\/\/www.kaggle.com\/milan400\/fer2013-denoising-using-autoencoder-and-unet","f7d57396":"# Visualizing the prediction","65578f27":"## Extract data from CSV","38b56395":"## Noised Test images","14587d56":"# One of the way we can achieve our goal of removing noise is AutoEncoder\n\n**Copied from Keras Blog(https:\/\/blog.keras.io\/building-autoencoders-in-keras.html):\n**\n* What are autoencoders good for?\n* Today two interesting practical applications of autoencoders are data denoising (which we feature later in this post), and dimensionality reduction for data visualization. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.","b63ff8ea":"# Making Prediction","6ab31211":"## Original Test images","083c082a":"# Importing libraries","741b9c1d":"# Model Architecture we are Constructing","c3a0f474":"# UNET Structure","ed3e3211":"* Image denoising is to remove noise from a noisy image, so as to restore the true image\n* In this notebook FER2013 dataset is used which contains approx 35 thousand images of 7 different emotions\n* Image is grayscale of size 48*48","93cd2445":"# Extracting Data and splitting train and test ","2de5fc38":"# AutoEncoder: Train Loss VS validation loss"}}