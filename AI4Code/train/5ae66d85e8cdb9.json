{"cell_type":{"f1ec0578":"code","52fe8a20":"code","366d97bb":"code","d08af6a8":"code","498e1bed":"code","ec9f4adf":"code","8cec6a25":"code","a713f1ac":"code","70c6f9bc":"code","86786769":"code","5b1a0cbc":"code","c8903ab0":"code","f89756e0":"code","8a6e4ebc":"code","9a3c861a":"code","6be5be79":"code","f8bcc239":"code","2a82a16f":"code","a59ec595":"code","428254fd":"code","956d4a8f":"code","a794f6cd":"code","bca14539":"code","bc1ffad9":"code","cf10241c":"code","0875e142":"code","4ea51725":"code","1cf6ac57":"code","1afa15a0":"code","7af4b2d8":"code","5839fe69":"code","f8e6580b":"markdown","29a40048":"markdown","fc43a598":"markdown","4c8f6dcf":"markdown","10c9dc75":"markdown","748dddc6":"markdown","80c50339":"markdown","1bd06c19":"markdown","e8a6334b":"markdown","ba473bf5":"markdown","d6a277b6":"markdown","298f8865":"markdown","439eb2f3":"markdown","d9b14451":"markdown","aab333ac":"markdown","ffcf5a2b":"markdown"},"source":{"f1ec0578":"#For working with dataframes and numbers\n\nimport pandas as pd\nimport numpy as np\n\n\n#For Exploratory data analysis\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\n%matplotlib inline\n\n#Modelling and eveluation\nimport sklearn.model_selection as ms\n#from xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport sklearn.metrics as sklm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\n\n","52fe8a20":"#import datasets\ntrain = pd.read_csv(\"..\/input\/train_technidus_clf.csv\")\ntest = pd.read_csv(\"..\/input\/test_technidus_clf.csv\")","366d97bb":"#Conduct mini data check\/analysis","d08af6a8":"\ntest.shape","498e1bed":"train.head()","ec9f4adf":"test.shape","8cec6a25":"#Check data for missing values\n#train.isnull().sum()\ntrain.isnull().sum()","a713f1ac":"#Check category split\n#train.BikeBuyer.value_counts()\ntrain.BikeBuyer.value_counts()","70c6f9bc":"#Univariate continuous- Histogram\ntrain[\"AveMonthSpend\"].plot.hist(color='blue',bins=50)\nplt.show()","86786769":"#Univariate categorical- Countplot(Barplot)\nf, ax = plt.subplots(figsize=(8, 4))\nsns.countplot(\"Occupation\", data=train)","5b1a0cbc":"#Bivariate analysis \n#Continuous and categorical (Box plot)\nsns.boxplot('BikeBuyer','AveMonthSpend', data=train)","c8903ab0":"#Bivariate analysis \n#Continuous and continuous\nplt.scatter('AveMonthSpend',\"YearlyIncome\", data = train)","f89756e0":"#Example is generating Age column from Birth date variable\ntrain.BirthDate.head(2)","8a6e4ebc":"train['Birthdate_int'] = train.BirthDate.str[-4:]\ntrain  = train.dropna(subset=['Birthdate_int'])\ntrain['Birthdate_int'] = train['Birthdate_int'].astype(int)\ntrain['Birthdate_int'].head()","9a3c861a":"train['today_date'] = 1998\ntrain['Age'] = train['today_date']-train['Birthdate_int']\ntrain['Age'].head()","6be5be79":"#Deletion\n#For example delete age greater than 90 years\ntrain['Age'] = train['Age']<90","f8bcc239":"#Example of Binning\nbins = [0,25,45,55,100]\nlabels = [\"0-24\",\"25-44\", \"45-55\",\"56-100\"]\ntrain[\"Agebin\"] = pd.cut(train.Age,bins = bins,labels=labels)\n#train.Agebin.value_counts()","2a82a16f":"train[['Age','Agebin']].head()","a59ec595":"#Deletion\n#Pairwise\n#train = train.dropna(axis=1,how='any')\n#listwise\n#train = train.dropna(axis=1,how='all')\n\n#Mean\/mode imputation\ntrain['Age'].fillna(train['Age'].mean(), inplace=True) \ntrain['Age'].fillna(train['Age'].mode()[0], inplace=True) ","428254fd":"corr= train.corr()\n#corr\nf, ax = plt.subplots(figsize=(10, 5))\nsns.heatmap(corr,cmap='coolwarm',linewidths=2.0, annot=True)","956d4a8f":"#Convert selected variables to array which scikit learn recognises\nX = train[['NumberCarsOwned',\n          'NumberChildrenAtHome',\n          'YearlyIncome','CountryRegionName']]\ny = train['BikeBuyer']\nXb=test[['NumberCarsOwned',\n          'NumberChildrenAtHome',\n          'YearlyIncome','CountryRegionName']]","a794f6cd":"#Convert categorical variables to numerical through one-hotencoding\nX=pd.get_dummies(X)\nXb=pd.get_dummies(Xb)","bca14539":"#Holdout 30%\nfrom sklearn.model_selection import train_test_split\nx_train, x_cv, y_train, y_cv = train_test_split(X,y, test_size =0.3)","bc1ffad9":"#Build various models","cf10241c":"#Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\n#model = LogisticRegression()\nmodel.fit(x_train, y_train)\n","0875e142":"#Evaluate logistic regression\npred_cv = model.predict(x_cv)\nscore = accuracy_score(y_cv,pred_cv)\nprint('accuracy_score',score)","4ea51725":"#Build Random forest\nfrom sklearn.ensemble import RandomForestClassifier\nmodel1 = RandomForestClassifier(random_state=1, max_depth=110,n_estimators= 1400,class_weight=\"balanced\")\nmodel1.fit(X,y)","1cf6ac57":"#Evaluate RF\npred_cv = model.predict(x_cv)\nscore = accuracy_score(y_cv,pred_cv)\nprint('accuracy_score',score)","1afa15a0":"#Save for submission\ntest['BikeBuyer']=model1.predict(Xb)\ntest['CustomerID']= test['CustomerID']","7af4b2d8":"test['BikeBuyer'] = test['BikeBuyer'].astype(int)\ntest[['CustomerID','BikeBuyer']].head(10)","5839fe69":"test[['CustomerID','BikeBuyer']].to_csv('perfrect_score2.csv',index= False)","f8e6580b":"#Feature Engineering\n\nFeature engineering is the science (and art) of extracting more information from existing data. \nYou are not adding any new data here, but you are actually making the data you already have more useful.\n\nFeature engineering itself can be divided in 2 steps:\n* Variable transformation.\n* Variable \/ Feature creation.\n","29a40048":"# Import all critical libraries","fc43a598":"# Missing value treatment","4c8f6dcf":"# Feature Engineering","10c9dc75":"It is an approach of analysing data to summarise main characteristics often with visual methods.","748dddc6":"# Validation","80c50339":"#Feature Selection\n\nFilter methods are generally used as a preprocessing step. \nThe selection of features is independent of any machine learning algorithms.\nInstead, features are selected on the basis of their scores in various statistical tests \nfor their correlation with the outcome variable. The correlation is a subjective term here\n\nForward Selection: Forward selection is an iterative method in which we start with having no feature in the model. \nIn each iteration, we keep adding the feature which best improves our model till \nan addition of a new variable does not improve\u00a0the performance of the model.\n\n","1bd06c19":"# Feature Selection","e8a6334b":"#Missing value treatment\n\nDeletion:\u00a0Deletion methods are used when the nature of missing data is \u201cMissing completely at random\u201d else \nnon random missing values can bias the model output. \n\nMean\/ Mode\/ Median Imputation:\u00a0Imputation is a method\u00a0to fill in the missing values with estimated ones. \nSimilar case Imputation\n\nPrediction Model: \u00a0Prediction model is one of the sophisticated method\u00a0for handling missing data. \n\nKNN Imputation: In this method of imputation, the missing values of an\u00a0attribute\u00a0are imputed using the given number of attributes that are most similar to the attribute whose values are missing. ","ba473bf5":"# Exploratory data analysis","d6a277b6":"#Holdout method\n\nTo avoid the resubstitution error, the data is split into two different datasets labeled as a training and a testing dataset. \nThis can be a 60\/40 or 70\/30 or 80\/20 split. This technique is called the hold-out validation technique. \nIn this case, there is a likelihood that uneven distribution of different classes of data is found in training and test dataset. To fix this, the training and test dataset is created with equal distribution of different classes of data. This process is called stratification.\n","298f8865":"#Outlier treatment\n\nOutlier is a commonly used\u00a0terminology by analysts and data scientists as\u00a0it needs\u00a0close attention else \nit\u00a0can result in wildly wrong\u00a0estimations. Simply speaking,\u00a0Outlier is an observation that appears far away and \ndiverges from an overall pattern in a sample.\n\n\nMost commonly used method to detect outliers is visualization. We use various visualization methods, \nlike Box-plot, Histogram, Scatter Plot \n\nDeleting observations:\u00a0We delete outlier values if it is due to data entry error, data processing error or \noutlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.\n\nTransforming and binning values:\u00a0Transforming variables can also eliminate outliers.\nNatural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. \n\nImputing:\u00a0Like\u00a0imputation\u00a0of missing values, we can also impute\u00a0outliers. \nWe can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier \nor artificial. \n\nTreat separately:\u00a0If there are significant number of outliers, we should treat them separately\u00a0in the statistical model. ","439eb2f3":"# Outlier treatment","d9b14451":"Bivariate analysis\n\nBi-variate Analysis finds out the relationship between two variables. Here, we look for\u00a0association and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process.\n\nContinuous & Continuous: While doing bi-variate analysis between two continuous variables, we should look at scatter plot. It is a nifty way to\u00a0find out the relationship between two variables. The pattern of scatter plot indicates the relationship between variables. The relationship can be linear or non-linear.\n\nCategorical & Categorical:\u00a0To find the relationship between two categorical variables, we can use following methods:\n\nTwo-way table: We can start\u00a0analyzing the relationship by creating a two-way table of count and count%. The rows represents the category of one variable and the columns represent\u00a0the categories of the other variable. We show count or count% of observations available in each combination of row and column categories.","aab333ac":"# Modelling","ffcf5a2b":"#Conduct full EDA\n\nUnivariate analysis\n\nAt this stage, we explore variables one by one. Method to perform uni-variate analysis \nwill depend on whether the variable type\u00a0is categorical or\u00a0continuous\n\nContinuous Variables:-\u00a0In case of continuous variables, we need to understand the central tendency \nand spread of the variable. These are measured using various statistical metrics visualization methods as shown below\n\nCategorical Variables:- For categorical variables, we\u2019ll use frequency table to understand distribution \nof\u00a0each category. We can also read as percentage of values\u00a0under each category. \nIt can be be measured using two metrics, Count and Count% against each category. \nBar chart can be used as visualization."}}