{"cell_type":{"1a4eb45c":"code","424ab6ee":"code","592e96e0":"code","7041ff4b":"code","aeadcf50":"code","f585e6b1":"code","357068d1":"code","2ea1ee50":"code","53d3cc9d":"code","90b33a8d":"markdown","99814113":"markdown","171af7f5":"markdown","c430678f":"markdown","608e831e":"markdown","ce77c4ba":"markdown","eb2f6ecf":"markdown","e0e4ea76":"markdown","59241999":"markdown"},"source":{"1a4eb45c":"import os\nimport math\nimport logging\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer,\\\n                         AdamW, DataCollatorForLanguageModeling,\\\n                         get_scheduler","424ab6ee":"class Config:\n    model_name = 'roberta-large'\n    max_length = 512\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    validation_size = 0.05\n    mlm_probability = 0.15\n    \n    train_batch_size = 4\n    eval_batch_size = 4\n    \n    learning_rate = 2.5e-5\n    \n    num_train_epochs = 3\n        \n    lr_scheduler_type = 'constant_with_warmup'\n    num_warmup_steps = 0\n\nargs = Config()","592e96e0":"def create_mlm_csv():\n    \"\"\" Read all training texts to a csv file with one column 'text' \"\"\"\n    texts = []\n    \n    for f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/train'))):\n        with open('..\/input\/feedback-prize-2021\/train\/' + f, 'r') as fp:\n            texts.append(fp.read())\n    \n    df = pd.DataFrame({'text': texts})\n    \n    display(df.head())\n    df.to_csv(\"mlm_train.csv\", index=False)\n    return df\n\ndf = create_mlm_csv()","7041ff4b":"model = AutoModelForMaskedLM.from_pretrained(args.model_name)\nmodel.to(args.device)\n\ntokenizer = AutoTokenizer.from_pretrained(args.model_name)","aeadcf50":"def tokenize_function(examples):\n    return tokenizer(examples['text'], return_special_tokens_mask=True)\n\ndef group_texts(examples):\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    total_length = (total_length \/\/ args.max_length) * args.max_length\n    result = {\n        k: [t[i : i + args.max_length] for i in range(0, total_length, args.max_length)]\n        for k, t in concatenated_examples.items()\n    }\n    return result","f585e6b1":"raw_datasets = load_dataset(\"csv\", data_files={'train': 'mlm_train.csv'})\n\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=['text'])\\\n                                 .map(group_texts, batched=True)\n\ntokenized_datasets = tokenized_datasets[\"train\"].train_test_split(test_size=args.validation_size)\ntokenized_datasets['validation'] = tokenized_datasets.pop(\"test\")\n\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=args.mlm_probability)\n\n\ndl_train = DataLoader(tokenized_datasets[\"train\"], \n                      shuffle=True, \n                      collate_fn=data_collator, \n                      batch_size=args.train_batch_size)\n\ndl_val = DataLoader(tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=args.eval_batch_size)","357068d1":"optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n\nnum_training_steps = args.num_train_epochs * len(dl_train)\nlr_scheduler = get_scheduler(\n    name=args.lr_scheduler_type,\n    optimizer=optimizer,\n    num_warmup_steps=args.num_warmup_steps,\n    num_training_steps=num_training_steps,\n)","2ea1ee50":"print(\"***** Running training *****\")\nprint(f\"  Num examples = {len(tokenized_datasets['train'])}\")\nprint(f\"  Num Epochs = {args.num_train_epochs}\")\nprint(f\"  Total training steps = {num_training_steps}\")","53d3cc9d":"progress_bar = tqdm(range(num_training_steps))\ncompleted_steps = 0\n\nfor epoch in range(args.num_train_epochs):\n    model.train()\n    cum_loss = 0\n    for batch_idx, batch in enumerate(dl_train, 1):\n        \n        outputs = model(**{k: v.to(args.device) for k, v in batch.items()})\n        loss = outputs.loss\n        cum_loss += loss.item()\n        \n        \n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n        progress_bar.set_postfix({'loss': cum_loss \/ batch_idx})\n        #if batch_idx > 100:\n        #    break\n\n    model.eval()\n    losses = []\n    for batch_idx, batch in enumerate(dl_val, 1):\n        with torch.no_grad():\n            outputs = model(**{k: v.to(args.device) for k, v in batch.items()})\n\n        loss = outputs.loss\n        losses.append(loss)\n        #if batch_idx > 100:\n        #    break\n\n    losses = torch.tensor(losses)\n    losses = losses[: len(tokenized_datasets['validation'])]\n    perplexity = math.exp(torch.mean(losses))\n\n    print(f\"Epoch {epoch}: perplexity: {perplexity}\")\n    model.save_pretrained(f'roberta_large-itpt-e{epoch}')","90b33a8d":"# Training\/validation loop","99814113":"# \ud83d\udcd6 Torch Roberta - ITPT - Intra-task pre-training\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/31779\/logos\/header.png)\n\n## Intra-task pre-training of a `roberta-large` (but trivially adaptable to any MLM model) over the [Feedback Prize - Evaluating Student Writing](https:\/\/www.kaggle.com\/c\/feedback-prize-2021)\n\n\nBased on this notebook by [torch](): [CommonLit Readability Prize - RoBERTa Torch|ITPT](https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-itpt), which in turn is based on this script by huggingface: https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/pytorch\/language-modeling\/run_mlm_no_trainer.py\n\nA good learning reference for this is also the Chapter 7 of the HuggingFace course, there is a detailed step-by-step explanation of the code atoms of this notebook: [Chapter 7 - Section 3 - Fine-tuning a masked language model](https:\/\/huggingface.co\/course\/chapter7\/3?fw=pt).\n\n\n# \ud83e\udd17 Please _DO_ upvote if you find this helpful or interesting! \ud83e\udd17\n","171af7f5":"# Configuration","c430678f":"# Model and Tokenizer","608e831e":"# Imports","ce77c4ba":"# Dataset and DataLoader","eb2f6ecf":"# Create one CSV with all the texts","e0e4ea76":"# Optimizer and Scheduler","59241999":"# \ud83e\udd17 Please _DO_ upvote if you find this helpful or interesting! \ud83e\udd17"}}