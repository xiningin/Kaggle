{"cell_type":{"5f075b86":"code","17865e9d":"code","a47a98df":"code","390fccf2":"code","fc2a42fd":"code","566165a3":"code","07fe72ed":"code","44df1635":"code","88fe5784":"code","639aafbb":"code","8524ab98":"code","8f7853d8":"code","81307198":"code","38c43a28":"code","f065a6a3":"code","de287325":"code","edeb2948":"code","74ec4ef2":"code","5deaffa1":"code","e3ff5fd3":"code","14cf6683":"code","17ec91b1":"code","ad3bdeb0":"code","9072176a":"code","694586a2":"code","6bc196ec":"code","8527835e":"code","6ad2ccbe":"code","cd66f492":"code","560cf5ac":"code","00191b93":"code","b8417a66":"code","dfd4734a":"markdown","df2b5972":"markdown","06eebfc6":"markdown","9128a65b":"markdown","92643051":"markdown","07bd288d":"markdown","7fe7eeb0":"markdown","2e129d06":"markdown","2e203b47":"markdown","600dd7e2":"markdown","343e075a":"markdown","d1082126":"markdown","809c5089":"markdown","601d8d15":"markdown","32abc2a9":"markdown","3b76594f":"markdown","806a660a":"markdown","fe2c7b1f":"markdown","ce83179d":"markdown","a508b94c":"markdown","6b48fe74":"markdown","3aae19dc":"markdown","3343aeee":"markdown","19b4c0ce":"markdown","2b054da3":"markdown","067928e9":"markdown","e4be3bb7":"markdown","e21b72e3":"markdown","b4896d95":"markdown","04f04e5d":"markdown","f9205f0c":"markdown","7557ab51":"markdown","c28fd38d":"markdown","783eaa05":"markdown","9c41ab8b":"markdown","fd8c811e":"markdown"},"source":{"5f075b86":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset('titanic')\ndf.head(10)","17865e9d":"df.describe()","a47a98df":"df.describe(include='all')","390fccf2":"df.info()","fc2a42fd":"#Indexing titanic data with row(1,7,21,10) and column(sex,age,fare,who,class)\nsmallData = df.loc[[1,7,21,10], ['sex','age','fare','who','class']]\nsmallData","566165a3":"#Creating a DataFrame with exact matching columns of smallData\nnewData = pd.DataFrame({'sex':['female', 'male','male'], \n                        'age':[25,49,35], \n                        'fare':[89.22,70.653,30.666], \n                        'who':['child', 'women', 'man'], \n                        'class':['First','First','First']})\nnewData","07fe72ed":"pd.concat([smallData, newData])","44df1635":"pd.concat([ newData,smallData], ignore_index=True)","88fe5784":"pd.concat([smallData, newData], axis=1)","639aafbb":"print('-----newData------\\n',newData)\nprint('-----smallData----\\n',smallData)","8524ab98":"newData = pd.DataFrame({'fare':[89.22,70.653,30.666,100],\n                        'who':['child', 'women', 'man', 'women'], \n                        'class':['First','First','First','Second'],\n                       'adult_male': [True, False, True, False]})\nnewData","8f7853d8":"pd.concat([smallData, newData])","81307198":"pd.concat([smallData, newData], join='inner')","38c43a28":"df1 = pd.DataFrame({'employee_name':['Tasha','Linda', 'Olliver','Jack'],\n                    'department':['Engineering', 'Accounting', 'HR', 'HR']})\ndf2 = pd.DataFrame({'employee_name':['Linda', 'Tasha', 'Jack', 'Olliver'],\n                    'salary':[35000, 20500, 90000, 68000]})\nprint('----df1----\\n',df1)\nprint('----df2----\\n',df2)","f065a6a3":"df3 = pd.merge(df1,df2)\ndf3","de287325":"df4 = pd.DataFrame({'department':['Engineering', 'Accounting', 'HR'],\n                  'supervisor': ['Jonas', 'Martha', 'Martin']})\nprint('----df3----\\n',df3)\nprint('----df4----\\n',df4)\nprint('----merged----\\n',pd.merge(df3, df4))","edeb2948":"df5 = pd.DataFrame({'department':['Engineering', 'Engineering','Accounting',\n                                  'Accounting', 'HR', 'HR'],\n                  'skills': ['Coding', 'Soft skills', 'Math', 'Excel', \n                             'Organizing', 'Decision making']})\nprint('----df3----\\n',df3)\nprint('----df5----\\n',df5)\nprint('----merged----\\n',pd.merge(df3, df5))","74ec4ef2":"df2 = pd.DataFrame({'name':['Linda', 'Tasha', 'Jack', 'Olliver'],\n                    'salary':[35000, 20500, 90000, 68000]})\nprint('--------df1---------\\n',df1)\nprint('--------df2---------\\n',df2)\nprint('-------merged--------\\n',pd.merge(df1, df2, left_on='employee_name', right_on='name'))","5deaffa1":"df1=pd.DataFrame({'employee_name':['Tasha','Linda','Olliver','Jack'],  'department':['Engineering', 'Accounting', 'HR', 'HR']})\ndf2 = pd.DataFrame({'employee_name':['Linda', 'Mary'],'salary':[35000, 20500]})\nprint('--------df1---------\\n',df1)\nprint('--------df2---------\\n',df2)\nprint('\\n-------merged--------\\n',pd.merge(df1, df2))\n","e3ff5fd3":"print('-------left join--------\\n',pd.merge(df1, df2, how='left'))\nprint('\\n-------right join--------\\n',pd.merge(df1,df2,how='right'))","14cf6683":"print(df.groupby('sex'))\ndf.groupby('sex').sum()","17ec91b1":"data = df.groupby('sex')['survived'].sum()\nprint('% of male survivers',(data['male']\/(data['male']+data['female']))*100)\nprint('% of male female',(data['female']\/(data['male']+data['female']))*100)","ad3bdeb0":"df.groupby('sex')['survived'].aggregate(['sum', np.mean,'median'])","9072176a":"df.groupby('survived').filter(lambda x: x['fare'].std() > 50)","694586a2":"df.groupby('survived').transform(lambda x: x - x.mean())","6bc196ec":"def func(x):\n    x['fare'] = x['fare'] \/ x['fare'].sum()\n    return x\ndf.groupby('survived').apply(func)","8527835e":"df.groupby(['sex', 'pclass'])['survived'].aggregate('mean').unstack()","6ad2ccbe":"df.pivot_table('survived', index='sex', columns='pclass')","cd66f492":"age = pd.cut(df['age'], [0, 18, 40, 80])\npivotTable = df.pivot_table('survived', ['sex', age], 'class')\npivotTable","560cf5ac":"pivotTable = pivotTable.unstack()\npivotTable","00191b93":"pivotTable.unstack(level=0)","b8417a66":"pivotTable.stack()","dfd4734a":"**This is more insightful, we can easily make out passengers in the third class section of the Titanic are less likely to be survived.**\n\n**This type of operation is very common in the analysis. Hence, Pandas provides the function .pivot_table() which performs the same with more flexibility and less complexity.**","df2b5972":"# **Merge on uncommon column names and values**\n\n# *1. Uncommon column names*\n\n**Many times merging is not that simple since the data we receive will not be so clean. We saw how the merge does all the work provided we have one common column. What if we have no common columns at all? or there is more than one common column. Pandas provide us the flexibility to explicitly specify the columns to act as the key in both DataFrames.**\n\n**Suppose we change our \u2018employee_name\u2019 column to \u2018name\u2019 in \u2018df2\u2019. Let\u2019s see how datasets look and how to tell merge explicitly the key columns.**","06eebfc6":"# *4. Apply*\n\n**Apply is very flexible unlike filter and transform, the only criteria are it takes a DataFrame and returns Pandas object or scalar. We have the flexibility to do anything we wish in the function.**","9128a65b":"# Concatenation\n\n**Concatenation of two DataFrames is very straightforward, thanks to the Pandas method concat(). Let us take a small section of our Titanic data with the help of vector indexing. Vector indexing is a way to specify the row and column name\/integer we would like to index in any order as a list. Also, I have created a dataset with matching columns to explain concatenation.**","92643051":"**Another method is .info(). It gives metadata of a dataset. We can see the size of the dataset, dtype, and count of null values in each column.**","07bd288d":"**If you are familiar with SQL join operation we can notice that .concat() performs outer join by default. Missing values for unmatched columns are filled with NaN.**","7fe7eeb0":"**Notice, printing only the groupby without performing any operation gives GroupBy object. Since there are only two unique values in the column \u2018sex\u2019 we can see a summation of every other column grouped by male and female. More insightful would be to get the percentage. We will capture only the \u2018survived\u2019 column of groupby result above upon summation and calculate percentages.**","2e129d06":"**We can also convert the outermost row index(sex) into the innermost column index by using parameter \u2018level\u2019.**","2e203b47":"# *2. Many-to-one*\n\n**Many-to-one is a type of join in which one of the two key columns have duplicate values. Suppose we have supervisors for each department and there are many employees in each department hence, Many employees to one supervisor.**","600dd7e2":"**Parameter \u2018left_on\u2019 to specify the key of the first column and \u2018right_on\u2019 for the key of the second. Remember, the value of \u2018left_on\u2019 should match with the columns of the first DataFrame you passed and \u2018right_on\u2019 with second. Notice, we get redundant column \u2018name\u2019, we can drop it if not needed.**","343e075a":"**By default, the concatenation happens row-wise. Let\u2019s see how the new dataset looks when we concat the two DataFrames.**","d1082126":"# GroupBy\n\n**GroupBy is a very flexible abstraction, we can think of it as a collection of DataFrame. It allows us to do many different powerful operations. In simple words, it groups the entire data set by the values of the column we specify and allows us to perform operations to extract insights.**\n\n***Let\u2019s come back to our Titanic dataset***\n\n**Suppose we would like to see how many male and female passengers survived.**","809c5089":"**Interestingly female children and teenagers in the second class have a 100% survival rate. This is the kind of power the pivot table of Pandas has.**","601d8d15":"# Summarizing data\n\n**The very first thing any data scientist would like to know is the statistics of the entire data. With the help of the Pandas .describe() method, we can see the summary stats of each feature. Notice, the stats are given only for numerical columns which is an obvious behavior. We can also ask describe function to include categorical columns with the parameter \u2018include\u2019 and value equal to \u2018all\u2019 ( include=\u2018all\u2019).**","32abc2a9":"**The result of the pivot table function is a DataFrame, unlike groupby which returned a groupby object. We can perform all the DataFrame operations normally on it.**\n\n**We can also add a third dimension to our result. Suppose we want to see how \u2018age\u2019 has also affected the survival rate along with \u2018sex\u2019 and \u2018pclass\u2019. Let\u2019s divide our \u2018age\u2019 into groups within it: 0\u201318 child\/teenager, 18\u201340 adult, and 41\u201380 old.**","3b76594f":"**We can also perform concatenation in SQL join fashion. Let\u2019s create a new DataFrame \u2018newData\u2019 having a few columns the same as smallData but not all.**","806a660a":"# **Reshaping Multi-index DataFrame**\n\n**To see a multi-index DataFrame from a different view we reshape it. Stack and Unstack are the two methods to accomplish this.**\n\n# *1. unstack( )*\n\n**It is the process of converting the row index to the column index. The pivot table we created previously is multi-indexed row-wise. We can get the innermost row index(age groups) into the innermost column index.**","fe2c7b1f":"**Since the standard deviation of \u2018fare\u2019 is greater than 50 only for values of \u2018survived\u2019 equal to 1, we can see data only where \u2018survived\u2019 is 1.**","ce83179d":"# **6 Pandas Operations You Must Know for EDA**\n\n**Pandas is used mainly for reading, cleaning, and extracting insights from data. We will see an advanced use of Pandas which are very important to a Data Scientist. These operations are used to analyze data and manipulate it if required. These are used in the steps performed before building any machine learning model.**\n\n**1. Summarising Data**\n\n**2. Concatenation**\n\n**3. Merge and Join**\n\n**4. Grouping**\n\n**5. Pivot Table**\n\n**6. Reshaping multi-index DataFrame**\n\n**We will be using the very famous Titanic dataset to explore the functionalities of Pandas. Let\u2019s just quickly import NumPy, Pandas, and load Titanic Dataset from Seaborn.**","a508b94c":"# Thank you for reading.\n**Upvotes are appreciated**","6b48fe74":"# *2. Uncommon values*\n\n**Previously we saw that all the employee names present in one dataset were also present in other. What if the names are missing.**","3aae19dc":"# *2. stack( )*\n\n**Stacking is exactly inverse of unstacking. We can convert the column index of multi-index DataFrame into a row index. The innermost column index \u2018sex\u2019 is converted to the innermost row index. The result is slightly different from the original DataFrame because we unstacked with level 0 previously.**","3343aeee":"**We can control the type of join operation with \u2018join\u2019 parameter. Let\u2019s perform an inner join that takes only common columns from two.**","19b4c0ce":"**These functions and methods are very helpful to understand data, further used to manipulation, or to build a predictive model. We can also plot graphs to get visual insights.**","2b054da3":"# *2. Filter*\n\n**The filter function allows us to drop data based on group property. Suppose we want to see data where the standard deviation of \u2018fare\u2019 is greater than the threshold value say 50 when grouped by \u2018survived\u2019.**","067928e9":"# 1. One-to-one\n\n**One-to-one merge is very similar to column-wise concatenation. To combine \u2018df1\u2019 and \u2018df2\u2019 we use .merge() method. Merge is capable of recognizing common columns in the datasets and uses it as a key, in our case column \u2018employee_name\u2019. Also, the names are not in order. Let\u2019s see how the merge does the work for us by ignoring the indices.**","e4be3bb7":"**By default merge applies inner join, meaning join in performed only on common values which is always not preferred way since there will be data loss. The method of joining can be controlled by using the parameter \u2018how\u2019. We can perform left join or right join to overcome data loss. The missing values will be represented as NaN by Pandas.**","e21b72e3":"**What if we want to concatenate ignoring the index? just set the ingore_index parameter to True.**","b4896d95":"**There are different types of join operations:**\n\n**1. One-to-one**\n\n**2. Many-to-one**\n\n**3. Many-to-many**\n\n**The classic data used to explain joins in SQL in the employee dataset. Lets create DataFrames.**","04f04e5d":"**Under the hood, the GroupBy function performs three operations: split-apply-combine.**\n\n**1. Split - breaking the DataFrame in order to group it into the specified key.**\n\n**2. Apply - it involves computing the function we wish like aggregation or transformation or filter.**\n\n**3. Combine - merging the output into a single DataFrame.**\n\n![](https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/figures\/03.08-split-apply-combine.png)\n\n**Perhaps, more powerful operations that can be performed on groupby are:**\n\n**1. Aggregate**\n\n**2. Filter**\n\n**3. Transform**\n\n**4. Apply**\n\n**Let\u2019s see each one with an example.**\n\n# *1. Aggregate*\n\n**The aggregate function allows us to perform more than one aggregation at a time. We need to pass the list of required aggregates as a parameter to .aggregate() function.**","f9205f0c":"# Merge and Join\n\n**Pandas provide us an exclusive and more efficient method .merge() to perform in-memory join operations. Merge method is a subset of relational algebra that comes under SQL.**\n\n> I will be moving away from our Titanic dataset only for this section to ease the understanding of join operation with less complex data.","7557ab51":"# *3. Transform*\n\n**Transform returns the transformed version of the entire data. The best example to explain is to center the dataset. Centering the data is nothing but subtracting each value of the column with the mean value of its respective column.**","c28fd38d":"# Pivot tables\n\n**Previously in GroupBy, we saw how \u2018sex\u2019 affected survival, the survival rate of females is much larger than males. Suppose we would also like to see how \u2018pclass\u2019 affected the survival but both \u2018sex\u2019 and \u2018pclass\u2019 side by side. Using GroupBy we would do something like this.**","783eaa05":"**If we wish to concatenate along with the columns we just have to change the axis parameter to 1.**","9c41ab8b":"**Notice the changes? As soon we concatenated column-wise Pandas arranged the data in an order of row indices. In smallData, row 0 and 2 are missing but present in newData hence insert them in sequential order. But we have row 1 in both the data and Pandas retained the data of the 1st dataset because that was the 1st dataset we passed as a parameter to concat. Also, the missing data is represented as NaN.**","fd8c811e":"# *3. Many-to-many*\n\n**This is the case where the key column in both the dataset has duplicate values. Suppose many skills are mapped to each department then the resulting DataFrame will have duplicate entries.**"}}