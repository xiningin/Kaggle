{"cell_type":{"b403ad77":"code","ca85860a":"code","b83f80d1":"code","5dada9b0":"code","59048b32":"code","08e32e51":"code","b520dfd7":"code","f4c7521d":"code","f1e5d8fe":"code","20f4de26":"code","89f6ffca":"code","bd6ec2eb":"code","01979f5b":"code","ea9e3700":"code","7a5dc6b2":"code","1c1d54ad":"code","19be5844":"code","beedcba2":"code","0abe6cfc":"code","2f8889ee":"code","0567ef87":"code","d98558d6":"markdown","75e7d2c2":"markdown","5676d620":"markdown","f4375f42":"markdown","12c4852a":"markdown","172056a4":"markdown","98ab785a":"markdown","bcb566ae":"markdown","5c9d0afd":"markdown"},"source":{"b403ad77":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import tree\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer, accuracy_score\n# Figures inline and set visualization style\n%matplotlib inline\nsns.set()\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ca85860a":"data_test = pd.read_csv(\"..\/input\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/train.csv\")","b83f80d1":"sns.countplot(x='Survived', data=train);\n","5dada9b0":"sns.countplot(x='Sex', data=train);\n","59048b32":"sns.catplot(x='Survived', col='Sex', kind='count', data=train);\n","08e32e51":"#Drop coloums that are not needed\ntrain.drop('Name',axis = 1, inplace =True)\ntrain.drop('Ticket', axis = 1, inplace = True)\ndata_test.drop('Name',axis = 1, inplace =True)\ndata_test.drop('Ticket', axis = 1, inplace = True)","b520dfd7":"# feature scale sex column \nm = {'m' : 1, 'f' : 0}\ntrain['Sex'] = train['Sex'].str[0].str.lower().map(m)\ndata_test['Sex'] = data_test['Sex'].str[0].str.lower().map(m)","f4c7521d":"# feature scale Embarked\nem = {'S':0, 'C': 1, 'Q' :2}\ntrain['Embarked'] = train['Embarked'].str[0].str.upper().map(em)\ntrain['Embarked'] = train['Embarked'].fillna(1)\ndata_test['Embarked'] = data_test['Embarked'].str[0].str.upper().map(em)\ndata_test['Embarked'] = data_test['Embarked'].fillna(1)","f1e5d8fe":"# filled missing ages with the average age \ntrain['Age'] = train['Age'].fillna(train['Age'].median())\ndata_test['Age'] = train['Age'].fillna(train['Age'].median())","20f4de26":"# Replace missing values of fare with the average \ntrain['Fare'] = train['Fare'].fillna(train['Fare'].median())\ndata_test['Fare'] = train['Fare'].fillna(train['Fare'].median())","89f6ffca":"# change age and fare types to int\ntrain['Age'] = train['Age'].astype('int64')\ndata_test['Age'] = data_test['Age'].astype('int64')\ntrain['Fare'] = train['Fare'].astype('int64')\ndata_test['Fare'] = data_test['Fare'].astype('int64')","bd6ec2eb":"# drop cabin\ntrain.drop('Cabin', axis = 1, inplace = True)\ndata_test.drop('Cabin', axis = 1, inplace = True)","01979f5b":"# corrilation heat map of data after feature scaleing \ncorr = train.corr()\nsns.heatmap(corr, \n        xticklabels=train.columns,\n        yticklabels=train.columns)","ea9e3700":"'''\n# normlising coloumns \ntrain=(train-train.min())\/(train.max()-train.min())\ntemp = data_test['PassengerId']\ndata_test = (data_test-data_test.min())\/(data_test.max()-data_test.min())\ndata_test['PassengerId'] = temp\n'''","7a5dc6b2":"# seperating the data for training \nX_all = train.drop(['Survived', 'PassengerId'], axis=1)\ny_all = train['Survived']\n\nnum_test = 0.20\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=num_test, random_state=23, shuffle = True)","1c1d54ad":"'''\n# testing models \nmodel  = [\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    gaussian_process.GaussianProcessClassifier(),\n    linear_model.LogisticRegressionCV(),\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    tree.DecisionTreeClassifier(),\n    XGBClassifier(),\n    SGDClassifier()\n    ]\nfor i in model:\n    print(str(i) + str(cross_val_score(i,X_test, y_test, scoring = \"accuracy\", cv = 10)))\n'''\n","19be5844":"'''\n# gridsearch for paramaters \n\nrfc = XGBClassifier() \n\nparam_grid = {\"learning_rate\"    : [0.01, 0.05] ,\n              \"max_depth\"        : [ 3],\n              \"gamma\"            : [ 0.1 ],\n              \"colsample_bytree\" : [ 0.5, 0.7 ],\n              'n_estimators': [1000],\n              'subsample': [0.4]\n              \n}\n\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5, scoring=\"accuracy\")\nCV_rfc.fit(X_train, y_train)\nprint(CV_rfc.best_params_)\n\n'''","beedcba2":"'''\n# fine tune paramaters \ncross_val = cross_val_score(ensemble.RandomForestClassifier(random_state = 90, warm_start = True, \n                                  min_samples_leaf = 1,\n                                  min_samples_split = 2,\n                                  n_estimators = 20,\n                                  max_depth = 5, \n                                  max_features = 'sqrt'), X_test, y_test, cv=100)\nprint(np.mean(cross_val))\nprint(np.std(cross_val))\n'''","0abe6cfc":"\nmodel = (ensemble.RandomForestClassifier(warm_start = True, \n                                  min_samples_leaf = 1,\n                                  min_samples_split = 2,\n                                  n_estimators = 26,\n                                  max_depth = 6, \n                                  max_features = 'sqrt'))\nmodel.fit(X_all,y_all)\nmodel.score(X_test,y_test)\n","2f8889ee":"'''\nmodel1 =  ensemble.RandomForestClassifier(random_state = 10, warm_start = True, \n                                  n_estimators = 26,\n                                  max_depth = 6, \n                                  max_features = 'sqrt')\nmodel2 =  ensemble.GradientBoostingClassifier(criterion = 'friedman_mse',\n                                              learning_rate = 0.075,\n                                              loss = 'deviance',\n                                              max_depth = 5,\n                                              max_features = 'sqrt',\n                                              n_estimators = 10,\n                                              subsample = 0.9)\nmodel3 = XGBClassifier(colsample_bytree = 0.7,\n                       gamma = 0.1, \n                       learning_rate = 0.05, \n                       max_depth = 3,\n                       n_estimators = 1000,\n                       subsample = 0.4)\n\nmodel = VotingClassifier(estimators=[('rf', model1), ('Gb', model2), ('XGB', model3)], voting='soft')\nmodel.fit(X_all,y_all)\nmodel.score(X_test,y_test)\n'''","0567ef87":"# useing model to make predictions and making the submission file \ntest_pre = model.predict(data_test.drop(['PassengerId'], axis=1))\ndata_test[\"Survived\"] = test_pre.astype(int)\ndata_test[['PassengerId', 'Survived']].to_csv('submission.csv', index = False)","d98558d6":"I tried ensembling a few models but got bad results I think I needed to spend more time on parameters or stacking the models ","75e7d2c2":"## Introduction <a name=\"intro\">\nThis Kernal is from the Titanic dataset the objective is to predict if somone has survived based on some features. There is a training set with labeled data and a test set that you have to make predictions of. This Dataset is used by allot of people as an Introduction to Kaggle and I'm useing it for the same purpose\n\n","5676d620":"## Modeling  <a name=model>","f4375f42":"## Imports <a name=\"Imports\">","12c4852a":"## Conclusion <a name=con> \nI tried RandomForestClassifier as it had the best results out of the models I tested I got reasonable results with it. I tried ensamble modeling and after optimising got similar results I would like to try a stacked the model to see if I can get better results. I had allot of fun with this dataset and learnt allot  ","172056a4":"# Table of contents\n1. [Introduction](#intro)\n2. [Imports](#Imports)\n3. [Acquire data](#Acquire)\n4. [Visualisations](#vis)\n5. [Feature Scaleing and Droping unnecessary columns](#feature)\n6. [Modeling](#model)\n7. [Conclusion](#con)","98ab785a":"## Acquire data <a name=Acquire>","bcb566ae":"## Feature Scaleing and Droping unnecessary columns <a name=feature>","5c9d0afd":"## Visualisations <a name=vis> "}}