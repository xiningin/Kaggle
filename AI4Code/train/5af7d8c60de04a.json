{"cell_type":{"a281ed79":"code","c30f350c":"code","73e8eb51":"code","945dc9b0":"code","14d80599":"code","c0d5b65b":"code","2d70edee":"code","1ed38a07":"code","1e1a6260":"code","71e5f89d":"code","4c5a4177":"code","2290f55b":"code","8c479b55":"code","711d76ea":"code","f9ee43ad":"code","e36969c1":"code","36e3df91":"code","e3ab6b1c":"code","c531515a":"code","cc36afdb":"code","e6b85b52":"code","3ce887f3":"code","07c36f84":"code","5504d611":"code","37353db9":"code","1fdb512a":"code","39e37798":"code","09d89ed1":"code","20b18bea":"code","147d5f7a":"code","e9b5a65d":"markdown","d71eb509":"markdown","dbe9e0d8":"markdown","b9e9d73c":"markdown","5f4f4a2b":"markdown","1a66dd43":"markdown","8c195754":"markdown"},"source":{"a281ed79":"from IPython.display import display\n\nimport pandas as pd\nimport numpy as np\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nfrom tqdm import tqdm\n\nimport pickle\n\nimport re","c30f350c":"from sklearn.linear_model import Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","73e8eb51":"\n!apt-get install -y mecab libmecab-dev mecab-ipadic-utf8\n!pip install mecab-python3\nimport os\nos.environ['MECABRC']= \"\/etc\/mecabrc\"","945dc9b0":"# \u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af\u306b\u3088\u308b\u30a8\u30e9\u30fc\u56de\u907f\n!ln -s \/etc\/mecabrc \/usr\/local\/etc\/mecabrc","14d80599":"!echo `mecab-config --dicdir`\"\/mecab-ipadic-neologd\"","c0d5b65b":"# \u5168\u89d2\u534a\u89d2\u5909\u63db\u30e9\u30a4\u30d6\u30e9\u30ea\n!pip install mojimoji > \/dev\/null","2d70edee":"from typing import List, Optional\n\nimport MeCab\nimport mojimoji\n\n\ndicdir = !echo `mecab-config --dicdir`\"\/mecab-ipadic-neologd\"\ntagger = MeCab.Tagger(dicdir[0])","1ed38a07":"def tokenize(sentence: str) -> List[str]:\n    splitted = tagger.parse(sentence).split(\"\\n\")\n    filtered = list(\n        filter(\n            lambda x: x != \"\" and x != \"EOS\",\n            splitted\n        )\n    )\n    return filtered\n\n\ndef split_sentence(\n    sentence: str, \n    normalize=False, \n    pos_allowed: Optional[List[str]] = None, \n    disallow: Optional[List[str]] = None\n) -> List[str]:\n    if normalize:\n        sentence = mojimoji.zen_to_han(sentence, kana=False).lower()\n        sentence = mojimoji.han_to_zen(sentence, digit=False, ascii=False)\n    splitted = [result.split(\"\\t\") for result in tokenize(sentence)]\n    if disallow is None:\n        disallow = []\n    \n    if pos_allowed is None:\n        words = [\n            result[0] for result in splitted\n            if result[1].split(\",\")[1] not in disallow\n        ]\n    else:\n        words = [\n            result[0] for result in splitted\n            if result[1].split(\",\")[1] not in disallow\n            and result[1].split(\",\")[0] in pos_allowed\n        ]\n    return words","1e1a6260":"text = \"\u667a\u306b\u50cd\u3051\u3070\u89d2\u304c\u7acb\u3064\u3002\u60c5\u306b\u68f9\u3055\u305b\u3070\u6d41\u3055\u308c\u308b\u3002\u610f\u5730\u3092\u901a\u305b\u3070\u7aae\u5c48\u3060\u3002\u514e\u89d2\u306b\u4eba\u306e\u4e16\u306f\u4f4f\u307f\u306b\u304f\u3044\u3002\"\ntokenize(text)","71e5f89d":"!git clone https:\/\/github.com\/geolonia\/japanese-addresses.git","4c5a4177":"addresses_df = pd.read_csv('.\/japanese-addresses\/data\/latest.csv')\naddresses_df","2290f55b":"addresses_df['merge_text'] = addresses_df[\"\u90fd\u9053\u5e9c\u770c\u540d\"].str.cat([addresses_df[\"\u5e02\u533a\u753a\u6751\u540d\"],addresses_df[\"\u5927\u5b57\u753a\u4e01\u76ee\u540d\"]])\naddresses_df['merge_text']","8c479b55":"# \u5c0f\u5b57\u30fb\u901a\u79f0\u540d\u304c\u5165\u3063\u3066\u3044\u308b\u305f\u3081\u91cd\u8907 87232\u884c\naddresses_df[addresses_df['merge_text'].duplicated()]","711d76ea":"# # \u5c0f\u5b57\u30fb\u901a\u79f0\u540d\u304cNaN\u3067\u306f\u306a\u3044\u3082\u306e\u306f\u8ffd\u52a0\n# alias_index = addresses_df[addresses_df[\"\u5c0f\u5b57\u30fb\u901a\u79f0\u540d\"].notna()].index\n# addresses_df.loc[alias_index, 'merge_text'] = addresses_df.loc[alias_index, \"merge_text\"].str.cat([addresses_df.loc[alias_index, \"\u5c0f\u5b57\u30fb\u901a\u79f0\u540d\"]])\n# addresses_df.loc[alias_index, 'merge_text']\n\n\n# \u91cd\u8907\u884c\u3092\u524a\u9664\naddresses_df = addresses_df[~addresses_df['merge_text'].duplicated()]\nlen(addresses_df)","f9ee43ad":"addresses_df['until_city_name'] = addresses_df['merge_text'].copy()\naddresses_df['until_city_name']","e36969c1":"def cleanText(in_text):\n    text = in_text\n    text = text.translate(str.maketrans({chr(0xFF01 + i): chr(0x21 + i) for i in range(94)}))\n    text = re.sub('[-\uff0d\ufe63\u2212\u2010\u2043\u2011\u2012\u2013\u2014\ufe58\u2015\u23af\u23e4\u30fc\uff70\u2500\u2501\u301c]', '-', text)\n    text = text.translate(str.maketrans({'1':'\u4e00','2':'\u4e8c','3':'\u4e09','4':'\u56db','5':'\u4e94','6':'\u516d','7':'\u4e03','8':'\u516b','9':'\u4e5d', '0':'\u3007'}))\n    return text","36e3df91":"for index, row in addresses_df.sample(10).iterrows():\n    display(tokenize(row['merge_text']))","e3ab6b1c":"split_sentence(addresses_df['merge_text'][1])","c531515a":"%%time\n\nvectorizer = TfidfVectorizer(analyzer=split_sentence)\n# vectorizer = CountVectorizer(analyzer=split_sentence)\nv_addresses = vectorizer.fit_transform(addresses_df['merge_text'])\n","cc36afdb":"v_addresses","e6b85b52":"tfidf_array = v_addresses[:10,].toarray()\ncs = cosine_similarity(tfidf_array,tfidf_array)  # cos\u985e\u4f3c\u5ea6\u8a08\u7b97\nprint(cs)","3ce887f3":"def get_most_similar(in_text):\n    text = cleanText(in_text)\n    incorrect_text = vectorizer.transform([text])\n    \n    similar_list = cosine_similarity(incorrect_text,v_addresses)\n\n    # Output result\n    return {\n        'addresses': addresses_df.iloc[similar_list[0].argsort()[:-11:-1]]['merge_text'].values,\n        'scores': np.sort(similar_list[0])[:-11:-1],\n    }","07c36f84":"# \u5e02\u3068\u533a\u304c\u629c\u3051\u3066\u3044\u308b\n%time result = get_most_similar('\u5317\u6d77\u9053\u672d\u5e4c\u4e2d\u592e\u65ed\u30b1\u4e18\u56db\u4e01\u76ee')\nfor i in range(len(result['scores'])):\n    print(i, np.round(result['scores'][i], 5), result['addresses'][i])\nprint()","5504d611":"# \u770c\u3084\u5e02\u539f\u5e02\u304c\u629c\u3051\u3066\u3044\u308b\n%time result = get_most_similar('\u5343\u8449\u82e5\u5bae\u4e8c\u4e01\u76ee')\nfor i in range(len(result['scores'])):\n    print(i, np.round(result['scores'][i], 5), result['addresses'][i])\nprint()","37353db9":"# \u9806\u756a\u306f\u30d0\u30e9\u30d0\u30e9\u3067\u3001\u5343\u8449\u3067\u3082\u306a\u3044\n%time result = get_most_similar('\u4e09\u4e01\u76ee\u7c73\u6ca2\u5343\u8449\u6625\u65e5')\nfor i in range(len(result['scores'])):\n    print(i, np.round(result['scores'][i], 5), result['addresses'][i])\nprint()","1fdb512a":"# # \u5165\u529b\u3057\u305f\u30a2\u30c9\u30ec\u30b9\u306b\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\u304c\u8fd1\u3044\u9806\u306b\u30a2\u30c9\u30ec\u30b9\u5019\u88dc\u3092\u8fd4\u3059\u3002\n\n# input_text = ''\n# while input_text != 'end':\n#     input_text = input('Enter address: ')\n#     %time result = get_most_similar(input_text)\n    \n#     #output\n#     for i in range(len(result['scores'])):\n#         print(i, np.round(result['scores'][i], 5), result['addresses'][i])\n#     print()","39e37798":"def getAddr(in_text):\n    text = cleanText(in_text)\n    text_list = re.split('(?<=[\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\u5341\u3007])', text)\n\n    results = {'addresses':np.array([]), 'addr':np.array([]), 'scores':np.array([])}\n    \n    for i in range(len(text_list)):\n        stackText = ''.join(text_list[:i+1])\n        temp_addr = ''.join(text_list[i+1:])\n        temp_result = get_most_similar(stackText)\n\n        results['addresses'] = np.hstack([results['addresses'], temp_result['addresses']])\n        results['addr'] = np.hstack([results['addr'], [temp_addr] * len(temp_result['scores'])])\n        results['scores'] = np.hstack([results['scores'], temp_result['scores']])\n    return results","09d89ed1":"%time result = getAddr('\u7c73\u6ca2\u5e02\u6625\u65e5\u4e00\u306e3\u306e\u4e8c')\nindex = result['scores'].argsort()[:-11:-1]\nfor i, index in enumerate(index):\n    print(i, np.round(result['scores'][index], 5), result['addresses'][index],  result['addr'][index])","20b18bea":"# \u9069\u5f53\u306a\u4f4f\u6240\n%time result = getAddr('\u6771\u4eac\u90fd1\u5bae3\u75302\u306e3\u306e-7')\nindex = result['scores'].argsort()[:-11:-1]\nfor i, index in enumerate(index):\n    print(i, np.round(result['scores'][index], 5), result['addresses'][index],  result['addr'][index])\n","147d5f7a":"# input_text = ''\n# while input_text != 'end':\n#     input_text = input('Enter address: ')\n#     %time result = getAddr(input_text)\n#     index = result['scores'].argsort()[:-11:-1]\n#     for i, index in enumerate(index):\n#         print(i, np.round(result['scores'][index], 5), result['addresses'][index], result['addr'][index])\n#     print()","e9b5a65d":"# \u5165\u529b\u30d5\u30a9\u30fc\u30e0(\u753a\u4e01\u76ee\u4ee5\u964d\u307e\u3067)","d71eb509":"### Reference [How to use Mecab and BERT_JP](https:\/\/www.kaggle.com\/kaerunantoka\/how-to-use-mecab-and-bert-jp)","dbe9e0d8":"\u6b63\u898f\u8868\u73fe\u306a\u3069\u306e\u30d1\u30bf\u30fc\u30f3\u306f geolonia\/normalize-japanese-addresses \u3092\u53c2\u8003\n\n\u6570\u5b57\u3067\u533a\u5207\u3063\u3066\u3001\u7dcf\u5f53\u305f\u308a\u3067\u4e00\u756a\u30b9\u30b3\u30a2\u304c\u9ad8\u3044\u306e\u3092\u9078\u3076","b9e9d73c":"\u4f4f\u6240\u306e\u5165\u529b\u3067\u3001\u5e02\u3084\u753a\u304c\u629c\u3051\u3066\u3066\u3082\u691c\u7d22\u3067\u304d\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u66f8\u304d\u307e\u3057\u305f\u3002\n\n\n## \u4f7f\u7528\u3057\u305f\u30c7\u30fc\u30bf\n### [geolonia](https:\/\/github.com\/geolonia)\/[japanese-addresses](https:\/\/github.com\/geolonia\/japanese-addresses)\n**Geolonia \u4f4f\u6240\u30c7\u30fc\u30bf**\n\n\u5168\u56fd\u306e\u753a\u4e01\u76ee\u3001\u5927\u5b57\u3001\u5c0f\u5b57\u30ec\u30d9\u30eb\u306e\u4f4f\u6240\u30c7\u30fc\u30bf\uff08277,191\u4ef6\uff09\u3092\u30aa\u30fc\u30d7\u30f3\u30c7\u30fc\u30bf\u3068\u3057\u3066\u516c\u958b\u3044\u305f\u3057\u307e\u3059\u3002\n\n\u672c\u30c7\u30fc\u30bf\u306f\u3001\u56fd\u571f\u4ea4\u901a\u7701\u4f4d\u7f6e\u53c2\u7167\u60c5\u5831\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u30b5\u30fc\u30d3\u30b9\u3067\u914d\u5e03\u3055\u308c\u3066\u3044\u308b\u300c\u5927\u5b57\u30fb\u753a\u4e01\u76ee\u30ec\u30d9\u30eb\u4f4d\u7f6e\u53c2\u7167\u60c5\u5831\u300d\u3092\u30d9\u30fc\u30b9\u3068\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u300c\u5927\u5b57\u30fb\u753a\u4e01\u76ee\u30ec\u30d9\u30eb\u4f4d\u7f6e\u53c2\u7167\u60c5\u5831\u300d\u30c7\u30fc\u30bf\u306f\u5e74\u306b\u4e00\u56de\u66f4\u65b0\u3067\u3042\u308b\u306e\u306b\u5bfe\u3057\u3066\u3001\u672c\u30ea\u30dd\u30b8\u30c8\u30ea\u3067\u914d\u5e03\u3059\u308b\u30c7\u30fc\u30bf\u306f\u6bce\u6708\u66f4\u65b0\u3057\u3066\u3044\u307e\u3059\u3002\n\n(\u5f15\u7528 https:\/\/github.com\/geolonia\/japanese-addresses)\n\n\n## \u6d41\u308c\n\u4e0a\u8a18\u306e\u4f4f\u6240\u30c7\u30fc\u30bf\u306e\u4f4f\u6240\u6587\u5b57\u5217\u3092Mecab\u3067\u5358\u8a9e\u306b\u5206\u5272\n\nTF-IDF\u3067\u3001\u4f4f\u6240\u306b\u4f7f\u308f\u308c\u308b\u5358\u8a9e\u3092\u51fa\u73fe\u983b\u5ea6\u3092\u8a55\u4fa1\u3057\u3001\u30d9\u30af\u30c8\u30eb\u5316\n\n\u5358\u7d14\u306b\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\u304c\u9ad8\u3044\u9806\u306b\u53d6\u5f97\n\n\n## \u7d50\u679c\n\u4e0b\u8a18\u306e\u3088\u3046\u306b\u30b7\u30f3\u30d7\u30eb\u306a\u5272\u306b\u306f\u3044\u3044\u7cbe\u5ea6\u3067\u4f4f\u6240\u5019\u88dc\u3092\u53d6\u5f97\u3067\u304d\u3066\u3044\u308b\u3068\u601d\u3046\u3002\n\n```\nTfidfVectorizer\n\n# \u770c\u3084\u5e02\u539f\u5e02\u304c\u629c\u3051\u3066\u3044\u308b\nresult = get_most_similar('\u5343\u8449\u82e5\u5bae\u4e8c\u4e01\u76ee')\nfor i in range(len(result['scores'])):\n    print(i, np.round(result['scores'][i], 5), result['addresses'][i])\nprint()\n0 0.81341 \u5343\u8449\u770c\u5e02\u539f\u5e02\u82e5\u5bae\u4e8c\u4e01\u76ee\n1 0.80249 \u5343\u8449\u770c\u5e02\u5ddd\u5e02\u82e5\u5bae\u4e8c\u4e01\u76ee\n2 0.72346 \u611b\u77e5\u770c\u8c4a\u7530\u5e02\u82e5\u5bae\u753a\u4e8c\u4e01\u76ee\n3 0.72327 \u5343\u8449\u770c\u5e02\u539f\u5e02\u82e5\u5bae\u4e00\u4e01\u76ee\n4 0.71916 \u5343\u8449\u770c\u5e02\u539f\u5e02\u82e5\u5bae\u4e09\u4e01\u76ee\n5 0.71357 \u5343\u8449\u770c\u5e02\u5ddd\u5e02\u82e5\u5bae\u4e00\u4e01\u76ee\n6 0.71269 \u5343\u8449\u770c\u5e02\u539f\u5e02\u82e5\u5bae\u56db\u4e01\u76ee\n7 0.70962 \u5343\u8449\u770c\u5e02\u5ddd\u5e02\u82e5\u5bae\u4e09\u4e01\u76ee\n8 0.70554 \u5343\u8449\u770c\u5e02\u539f\u5e02\u82e5\u5bae\u4e94\u4e01\u76ee\n9 0.69794 \u5343\u8449\u770c\u5e02\u539f\u5e02\u82e5\u5bae\u516d\u4e01\u76ee\n\n\n# \u9806\u756a\u306f\u30d0\u30e9\u30d0\u30e9\u3067\u3001\u5343\u8449\u3067\u3082\u306a\u3044\nresult = get_most_similar('\u4e09\u4e01\u76ee\u7c73\u6ca2\u5343\u8449\u6625\u65e5')\nfor i in range(len(result['scores'])):\n    print(i, np.round(result['scores'][i], 5), result['addresses'][i])\nprint()\n0 0.83248 \u5c71\u5f62\u770c\u7c73\u6ca2\u5e02\u6625\u65e5\u4e09\u4e01\u76ee\n1 0.74894 \u5c71\u5f62\u770c\u7c73\u6ca2\u5e02\u6625\u65e5\u4e8c\u4e01\u76ee\n2 0.7485 \u5c71\u5f62\u770c\u7c73\u6ca2\u5e02\u6625\u65e5\u4e00\u4e01\u76ee\n3 0.73729 \u5c71\u5f62\u770c\u7c73\u6ca2\u5e02\u6625\u65e5\u56db\u4e01\u76ee\n4 0.72972 \u5c71\u5f62\u770c\u7c73\u6ca2\u5e02\u6625\u65e5\u4e94\u4e01\u76ee\n5 0.60596 \u798f\u5ca1\u770c\u6625\u65e5\u5e02\u6625\u65e5\u4e09\u4e01\u76ee\n6 0.58711 \u5343\u8449\u770c\u5e02\u539f\u5e02\u7c73\u6ca2\n7 0.57447 \u5343\u8449\u770c\u5343\u8449\u5e02\u4e2d\u592e\u533a\u6625\u65e5\u4e8c\u4e01\u76ee\n8 0.5742 \u5343\u8449\u770c\u5343\u8449\u5e02\u4e2d\u592e\u533a\u6625\u65e5\u4e00\u4e01\u76ee\n9 0.56082 \u5c71\u5f62\u770c\u7c73\u6ca2\u5e02\u4e2d\u592e\u4e09\u4e01\u76ee\n\n```\n\nCountVectorizer\u3067\u3082\u5b9f\u88c5\u3057\u3066\u307f\u305f\n```\n# \u770c\u3084\u5e02\u539f\u5e02\u304c\u629c\u3051\u3066\u3044\u308b\nresult = get_most_similar('\u5343\u8449\u82e5\u5bae\u4e8c\u4e01\u76ee')\nfor i in range(len(result['scores'])):\n    print(i, np.round(result['scores'][i], 5), result['addresses'][i])\nprint()\n0 0.75593 \u5343\u8449\u770c\u5e02\u539f\u5e02\u82e5\u5bae\u4e8c\u4e01\u76ee\n1 0.75593 \u5343\u8449\u770c\u5e02\u5ddd\u5e02\u82e5\u5bae\u4e8c\u4e01\u76ee\n2 0.61237 \u5343\u8449\u770c\u5370\u897f\u5e02\u539f\u4e8c\u4e01\u76ee\n3 0.60302 \u5343\u8449\u770c\u5343\u8449\u5e02\u82e5\u8449\u533a\u5fa1\u6210\u53f0\u4e8c\u4e01\u76ee\n4 0.60302 \u5343\u8449\u770c\u5343\u8449\u5e02\u7f8e\u6d5c\u533a\u5e55\u5f35\u897f\u4e8c\u4e01\u76ee\n5 0.60302 \u5343\u8449\u770c\u5343\u8449\u5e02\u4e2d\u592e\u533a\u5357\u753a\u4e8c\u4e01\u76ee\n6 0.60302 \u5343\u8449\u770c\u5343\u8449\u5e02\u4e2d\u592e\u533a\u677e\u6ce2\u4e8c\u4e01\u76ee\n7 0.60302 \u5343\u8449\u770c\u5343\u8449\u5e02\u4e2d\u592e\u533a\u672c\u753a\u4e8c\u4e01\u76ee\n8 0.60302 \u5343\u8449\u770c\u5343\u8449\u5e02\u4e2d\u592e\u533a\u5f01\u5929\u4e8c\u4e01\u76ee\n9 0.60302 \u5343\u8449\u770c\u5343\u8449\u5e02\u4e2d\u592e\u533a\u5bcc\u58eb\u898b\u4e8c\u4e01\u76ee\n\n\n# \u9806\u756a\u306f\u30d0\u30e9\u30d0\u30e9\u3067\u3001\u5343\u8449\u3067\u3082\u306a\u3044\nresult = get_most_similar('\u4e09\u4e01\u76ee\u7c73\u6ca2\u5343\u8449\u6625\u65e5')\nfor i in range(len(result['scores'])):\n    print(i, np.round(result['scores'][i], 5), result['addresses'][i])\nprint()\n0 0.67612 \u5c71\u5f62\u770c\u7c73\u6ca2\u5e02\u6625\u65e5\u4e09\u4e01\u76ee\n1 0.59628 \u798f\u5ca1\u770c\u6625\u65e5\u5e02\u6625\u65e5\u4e09\u4e01\u76ee\n2 0.56569 \u5343\u8449\u770c\u8239\u6a4b\u5e02\u4e09\u5c71\u4e09\u4e01\u76ee\n3 0.54772 \u5343\u8449\u770c\u5370\u897f\u5e02\u539f\u4e09\u4e01\u76ee\n4 0.53936 \u5343\u8449\u770c\u5343\u8449\u5e02\u4e2d\u592e\u533a\u5357\u753a\u4e09\u4e01\u76ee\n5 0.53936 \u5343\u8449\u770c\u5343\u8449\u5e02\u82e5\u8449\u533a\u82e5\u677e\u53f0\u4e09\u4e01\u76ee\n6 0.53936 \u5343\u8449\u770c\u5343\u8449\u5e02\u7f8e\u6d5c\u533a\u7a32\u6bdb\u6d77\u5cb8\u4e09\u4e01\u76ee\n7 0.53936 \u5343\u8449\u770c\u5343\u8449\u5e02\u7f8e\u6d5c\u533a\u78ef\u8fba\u4e09\u4e01\u76ee\n8 0.53936 \u5343\u8449\u770c\u5343\u8449\u5e02\u7dd1\u533a\u3042\u3059\u307f\u304c\u4e18\u4e09\u4e01\u76ee\n9 0.53936 \u5343\u8449\u770c\u5343\u8449\u5e02\u82e5\u8449\u533a\u897f\u90fd\u8cc0\u4e09\u4e01\u76ee\n```\n\n\nTfidfVectorizer\u306e\u65b9\u304c\u7cbe\u5ea6\u304c\u9ad8\u305d\u3046\u3002\n\n\u300c\u4e09\u4e01\u76ee\u7c73\u6ca2\u5343\u8449\u6625\u65e5\u300d\u3068\u5165\u529b\u3057\u305f\u6642\u306a\u3069\u3001\u9593\u9055\u3063\u305f\u5165\u529b\u3067\u3042\u308b\u300c\u5343\u8449\u300d\u3088\u308a\u3001\u3082\u3063\u3068\u983b\u51fa\u983b\u5ea6\u304c\u5c11\u306a\u3044\u5358\u8a9e\u3067\u3042\u308b\u300c\u7c73\u6ca2\u300d\u3084\u300c\u6625\u65e5\u300d\u306e\u65b9\u304c\u985e\u4f3c\u5ea6\u306e\u8a08\u7b97\u3067\u91cd\u8981\u306b\u306a\u308b\u304b\u3089\u304b\u3082\u3002","5f4f4a2b":"### Reference [How to use Mecab in Kaggle Notebook](https:\/\/www.kaggle.com\/general\/170117)","1a66dd43":"# \u4f4f\u6240\u5165\u529b\u30d5\u30a9\u30fc\u30e0","8c195754":"# Using [geolonia](https:\/\/github.com\/geolonia)\/[japanese-addresses](https:\/\/github.com\/geolonia\/japanese-addresses)\n"}}