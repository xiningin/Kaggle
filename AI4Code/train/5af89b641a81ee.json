{"cell_type":{"dc4632a7":"code","ab59b3c7":"code","0cdb0dec":"code","6f5330d4":"code","19ff0684":"code","c8e34d1a":"code","5277f581":"code","380f671a":"code","3fee2805":"code","9c30e2fb":"code","f5658ccb":"code","eff3a82f":"code","381fb290":"code","1c642828":"code","93cfbf3f":"code","83a1cbdc":"code","a5d2f3e2":"code","a4295492":"code","796238e2":"code","3c499541":"code","615dc499":"code","4ffdc184":"code","fc93814b":"code","ef873d60":"code","b70683f7":"code","b5c3bf91":"code","669236d9":"code","6616fb7f":"code","1e9c8217":"code","eaef528b":"code","ae058cd7":"code","393b8edc":"code","5760c871":"code","bf86eb95":"code","fc9f8b84":"code","b0a16fb5":"code","31f85a29":"code","bf7cf2ed":"code","92f224ed":"code","b2d945a5":"code","626f66f6":"code","4667a59a":"code","8324f692":"code","bf1ec646":"code","1e55a434":"code","c4ce9dbd":"code","1880bde1":"code","8024365f":"code","5e80967e":"code","3b333ad1":"code","5686788e":"code","04d2e789":"code","4bbaf81b":"code","aaec2a46":"code","d313dc85":"code","0e593537":"code","09ba1790":"code","0a8ae436":"code","4fbfc407":"code","f3445139":"markdown","c262fee7":"markdown","918d1b71":"markdown","0b5787b2":"markdown","7f04dd52":"markdown","601d1e71":"markdown","fdc93018":"markdown","344abc7c":"markdown","d6c9c36f":"markdown","ee3be489":"markdown","1b9b538a":"markdown","1a4d5c39":"markdown","aa34acb8":"markdown","ea34b5b1":"markdown","cf975b00":"markdown","c2725008":"markdown","420e89c1":"markdown","b988f080":"markdown","af904e6b":"markdown","f377f63d":"markdown","30495f99":"markdown","65df9d12":"markdown","085f27dd":"markdown","88f9d872":"markdown","9b666693":"markdown","dcb02b6c":"markdown","b86eb2a9":"markdown","e5684d19":"markdown","a04680e6":"markdown","a6e593e8":"markdown","3c93e158":"markdown","0f7206db":"markdown","06321fe8":"markdown","12cf6065":"markdown","bc875eec":"markdown","c8ba974a":"markdown","9dcbc6fe":"markdown","197b8109":"markdown","846f3ff6":"markdown","2d4856c0":"markdown","610bca41":"markdown","60b9b9f0":"markdown","a1d8f985":"markdown","0cda4ac5":"markdown","7c0518c4":"markdown","998d5666":"markdown","9e3322f8":"markdown","c6bdbd18":"markdown","1e025922":"markdown","b55090f1":"markdown"},"source":{"dc4632a7":"import warnings\nwarnings.filterwarnings('ignore')","ab59b3c7":"# import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport joblib\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom imblearn.over_sampling import SMOTE, BorderlineSMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import manifold\nfrom sklearn.decomposition import PCA\nfrom mlxtend.plotting import plot_confusion_matrix\n\n%matplotlib inline\npd.pandas.set_option('display.max_columns', None)","0cdb0dec":"# Read data from csv file\ndf_cc = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\n\n# Print first few rows\ndf_cc.head()","6f5330d4":"# Shape of the dataset\ndf_cc.shape","19ff0684":"# The data is stardarized, I will explore them later For now I will look the \"normal\" columns\ndf_cc[[\"Time\",\"Amount\",\"Class\"]].describe()","c8e34d1a":"# Explore the Features available in DataFrame\nprint(df_cc.info())\nprint()\nprint(f\"There are {df_cc.isnull().sum().max()} NULL values in Dataset\")","5277f581":"# The general statistics of frauds and no frauds data\ndf_fraud = df_cc[df_cc['Class'] == 1]\ndf_normal = df_cc[df_cc['Class'] == 0]\n\nprint(\"Fraud Transaction Statistics\")\nprint(df_fraud[\"Amount\"].describe())\nprint(\"\\nNormal Transaction Statistics\")\nprint(df_normal[\"Amount\"].describe())","380f671a":"df_cc['Time'].max()","3fee2805":"plt.figure(figsize=(7,5))\nsns.distplot(df_cc[\"Time\"])\nplt.xlabel('Time (in seconds)')\nplt.title('Distribution of Time');","9c30e2fb":"fraud_time = df_cc[df_cc['Class'] == 1]['Time']\nno_fraud_time = df_cc[df_cc['Class'] == 0]['Time']\n\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,5))\nbins=50\n\nax1.hist(fraud_time, bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(no_fraud_time, bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nfig.text(0.04,0.5, 'Number of Transactions', va='center', rotation='vertical')\n\nplt.show()","f5658ccb":"fraud_amt = df_cc[df_cc['Class'] == 1]['Amount']\nno_fraud_amt = df_cc[df_cc['Class'] == 0]['Amount']\n\nplt.subplots(1, 2, figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.distplot(no_fraud_amt)\nplt.xlabel('Amount ($)')\nplt.title('Distribution of Non-Fraudulent Data Amount')\n\nplt.subplot(1, 2, 2)\nsns.distplot(fraud_amt)\nplt.xlabel('Amount ($)')\nplt.title('Distribution of Fraudulent Data Amount');","eff3a82f":"plt.subplots(1, 2, figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nsns.boxplot(x='Class', y='Amount', hue='Class', data=df_cc, showfliers=True)\n\nplt.subplot(1, 2, 2)\nsns.boxplot(x='Class', y='Amount', hue='Class', data=df_cc, showfliers=False);","381fb290":"print(f\"Fraud Amount Info: \\n {fraud_amt.describe()}\")\nprint()\nprint(f\"Non-Fraud Amount Info: \\n {no_fraud_amt.describe()}\")","1c642828":"# Count the occurences of Fraud and Non-Fraud Cases\nocc = df_cc['Class'].value_counts()\nprint(f\"Total NON-FRAUD CASES: {occ[0]}, {(occ[0]\/len(df_cc.index)*100):0.3f}%\")\nprint(f\"Total FRAUD CASES: {occ[1]}, {(occ[1]\/len(df_cc.index)*100):0.3f}%\")\n\nplt.bar(x=occ.index, height=occ.values, data=occ, color=['#5976A2', '#CB8866'])\nplt.title('Class Distribution');\nplt.yscale('log')\nplt.xticks([0, 1], ['Non-Fraud', 'Fraud'], rotation=0)\nplt.yticks([500,3000,10000,30000,100000,300000], ['500','3K','10K','30K', '100K', '300K'])\nplt.show();","93cfbf3f":"plt.figure(figsize = (14,14))\nplt.title('Credit Card Transactions Features Correlation Plot (Pearson)')\ncorr = df_cc.corr()\nsns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Blues\")\nplt.show()","83a1cbdc":"def prep_undersampled_data(df):\n    fraud_df = df.loc[df['Class'] == 1]\n    non_fraud_df = df.loc[df['Class']==0][:fraud_df.shape[0]]\n    undersampled_df = pd.concat([fraud_df, non_fraud_df])\n    df_col = [column for column in undersampled_df.columns if column not in ['Time','Class']]\n    X = undersampled_df.loc[:, df_col]\n    X = np.array(X).astype(np.float)\n    y = undersampled_df.loc[:, undersampled_df.columns == 'Class']\n    y = np.array(y).astype(np.float).reshape(-1,)\n    return X, y","a5d2f3e2":"X_under, y_under = prep_undersampled_data(df_cc)","a4295492":"# TSNE\ntsne = manifold.TSNE(n_components=2, random_state=2020)\ntransformed_data = tsne.fit_transform(X_under)\ntsne_df = pd.DataFrame(np.column_stack((transformed_data, y_under)), columns=[\"X\",\"Y\",\"Targets\"])\ntsne_df.loc[:,\"Targets\"] = tsne_df.Targets.astype(int)","796238e2":"# PCA\npca = PCA(n_components=2, random_state=2020)\ntransformed_data = pca.fit_transform(X_under)\npca_df = pd.DataFrame(np.column_stack((transformed_data, y_under)), columns=[\"X\",\"Y\",\"Targets\"])\npca_df.loc[:,\"Targets\"] = pca_df.Targets.astype(int)","3c499541":"ax, f = plt.subplots(1, 2, figsize=(24,10))\n\nplt.subplot(121)\nsns.scatterplot(\"X\",\"Y\", hue='Targets', data=tsne_df)\nplt.title('TSNE', fontsize=14)\nplt.grid(True)\n\nplt.subplot(122)\nsns.scatterplot(\"X\",\"Y\", hue='Targets', data=pca_df)\nplt.title('PCA', fontsize=14)\nplt.grid(True)\n\nplt.show()","615dc499":"df_cc['normAmount'] = StandardScaler().fit_transform(df_cc['Amount'].values.reshape(-1, 1))\ndf_cc = df_cc.drop(['Amount'],axis=1)\ndf_cc.head()","4ffdc184":"def prep_data(df):\n    df_col = [column for column in df.columns if column not in ['Time','Class']]\n    X = df.loc[:, df_col]\n    X = np.array(X).astype(np.float)\n    y = df.loc[:, df.columns == 'Class']\n    y = np.array(y).astype(np.float).reshape(-1,)\n    return X, y","fc93814b":"# Define a function to create a scatter plot of our data and labels\ndef plot_data(X, y):\n    plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class: Non-Fraud\", alpha=0.5, linewidth=0.15)\n    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class: Fraud\", alpha=0.5, linewidth=0.15, c='r')\n    plt.legend()\n    return plt.show()","ef873d60":"# Create X and y from our above defined function\nX, y = prep_data(df_cc)","b70683f7":"# Plot our data by running plot_data function on X and y\nplot_data(X, y)","b5c3bf91":"# Define the resampling method\nsmote_method = SMOTE()\n\n# Create the resampled feature set\nX_resampled, y_resampled = smote_method.fit_sample(X, y)\n\n# Plot the resampled data\nplot_data(X_resampled, y_resampled)","669236d9":"X_resampled.shape, y_resampled.shape","6616fb7f":"def compare_plot(X,y,X_resampled,y_resampled, method):\n    # Start a plot figure\n    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n    \n    # sub-plot number 1, this is our normal data\n    c0 = ax1.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\",alpha=0.5)\n    c1 = ax1.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\",alpha=0.5, c='r')\n    ax1.set_title('Original set')\n    \n    # sub-plot number 2, this is our oversampled data\n    ax2.scatter(X_resampled[y_resampled == 0, 0], X_resampled[y_resampled == 0, 1], label=\"Class #0\", alpha=.5)\n    ax2.scatter(X_resampled[y_resampled == 1, 0], X_resampled[y_resampled == 1, 1], label=\"Class #1\", alpha=.5,c='r')\n    ax2.set_title(method)\n    \n    plt.figlegend((c0, c1), ('Class: Non-Fraud', 'Class: Fraud'), loc='lower center',\n                  ncol=2, labelspacing=0.)\n    return plt.show()","1e9c8217":"print(f\"Total NON-FRAUD cases in Original Dataset: {pd.Series(y).value_counts()[0]}\")\nprint(f\"Total FRAUD cases in Original Dataset: {pd.Series(y).value_counts()[1]}\")\nprint()\nprint(f\"Total NON-FRAUD cases in SMOTE Resampled Dataset: {pd.Series(y_resampled).value_counts()[0]}\")\nprint(f\"Total FRAUD cases in SMOTE Resampled Dataset: {pd.Series(y_resampled).value_counts()[1]}\")\n\ncompare_plot(X, y, X_resampled, y_resampled, method=\"SMOTE\")","eaef528b":"def plot_roc_curve(true_y, pred_y):\n    \"\"\"\n    Plot the ROC curve along with the curves AUC for a given model. Note make sure true_y and pred_y are from the same model as model_name\n    :param model_name: Name of model used for saving plot\n    :param true_y: true labels for dataset\n    :param pred_y: predicted labels for dataset\n    \"\"\"\n    fig, ax = plt.subplots(1,1, figsize=(7,7))\n    fpr, tpr, thresholds = roc_curve(true_y, pred_y)\n    ax.plot(fpr, tpr, label=f'AUC: {auc(fpr, tpr):.2f}')\n    ax.legend()\n    ax.set_xlabel(\"False Positive Rate\")\n    ax.set_ylabel(\"True Positive Rate\")\n    \n    return","ae058cd7":"# Let's Split Resampled Dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2020)","393b8edc":"# Logistic Regression Combined with SMOTE\nresampling = SMOTE()\nmodel_lr = LogisticRegression()\n\npipeline = Pipeline([('SMOTE', resampling), ('Logistic Regression', model_lr)])\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)","5760c871":"# Print the Classification report and confusion matrix\nprint('Classification report:\\n', classification_report(y_test, predictions))\n\nprint(f'ROC-AUC Score: {roc_auc_score(y_test, predictions):.2f}\\n')\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=predictions)\n\nprint('Confusion matrix:\\n', conf_mat)\n\ntn, fp, fn, tp = conf_mat.ravel()\n\nprint()\nprint(f\"TN : {tn}\")\nprint(f\"FP : {fp}\")\nprint(f\"FN : {fn}\")\nprint(f\"TP : {tp}\")","bf86eb95":"plot_roc_curve(y_test, predictions)","fc9f8b84":"# Split data into train and test set (70-30)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2020)","b0a16fb5":"# Logistic Regression\nmodel_lr = LogisticRegression()\nmodel_lr.fit(X_train, y_train)\n\n# Obtain Model Prediction\npredictions_lr = model_lr.predict(X_test)","31f85a29":"# Print the Classification report and confusion matrix\nprint('Classification report:\\n', classification_report(y_test, predictions_lr))\n\nprint(f'ROC-AUC Score: {roc_auc_score(y_test, predictions_lr):.2f}\\n')\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=predictions_lr)\n\nprint('Confusion matrix:\\n', conf_mat)\n\ntn,fp,fn,tp = conf_mat.ravel()\n\nprint()\nprint(f\"TN : {tn}\")\nprint(f\"FP : {fp}\")\nprint(f\"FN : {fn}\")\nprint(f\"TP : {tp}\")","bf7cf2ed":"plot_roc_curve(y_test, predictions_lr)","92f224ed":"# Logistic Regression Combined with SMOTE\nresampling = BorderlineSMOTE()\n\npipeline = Pipeline([('SMOTE', resampling), ('Logistic Regression', model_lr)])\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)","b2d945a5":"# Print the Classification report and confusion matrix\nprint('Classification report:\\n', classification_report(y_test, predictions))\n\nprint(f'ROC-AUC Score: {roc_auc_score(y_test, predictions):.2f}\\n')\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=predictions)\n\nprint('Confusion matrix:\\n', conf_mat)\n\ntn,fp,fn,tp = conf_mat.ravel()\nprint()\nprint(f\"TN : {tn}\")\nprint(f\"FP : {fp}\")\nprint(f\"FN : {fn}\")\nprint(f\"TP : {tp}\")","626f66f6":"plot_roc_curve(y_test, predictions)","4667a59a":"# Count the Total Number of Observations from the length of y\ntotal_obs = len(y)\n\n# Count the total number of non-fraudulent observations\nnon_fraud = [i for i in y if i==0]\ncount_non_fraud = non_fraud.count(0)\n\n# Percentage of Non-Fraud Observations\npercentage = count_non_fraud \/ total_obs * 100\nprint(f\"Percentage of NON-FRAUD observations: {percentage:0.2f}%\")","8324f692":"# Random Forest Model\nmodel_rf = RandomForestClassifier(random_state=2020, n_estimators=20)\n\n# Fit the model to our training set\nmodel_rf.fit(X_train, y_train)","bf1ec646":"\n# Obtain predictions from the test data\npredictions_rf = model_rf.predict(X_test)\n\n# Predict Probabilities\nprobs = model_rf.predict_proba(X_test)\n\n# Print Accuracy Score\nprint(f\"Accuracy Score : {accuracy_score(y_test, predictions_rf):0.4f}%\")\nprint()\n# Print ROC Score\nprint(f\"ROC Score : {roc_auc_score(y_test, probs[:,1])}\")\nprint()\n# Print the Classification report and confusion matrix\nprint('Classification report:\\n', classification_report(y_test, predictions_rf))\n\nprint(f'ROC-AUC Score: {roc_auc_score(y_test, predictions_rf):.2f}\\n')\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=predictions_rf)\n\nprint()\n\nprint('Confusion matrix:\\n', conf_mat)\ntn,fp,fn,tp = conf_mat.ravel()\nprint()\nprint(f\"TN : {tn}\")\nprint(f\"FP : {fp}\")\nprint(f\"FN : {fn}\")\nprint(f\"TP : {tp}\")","1e55a434":"plot_roc_curve(y_test, predictions_rf)","c4ce9dbd":"# Calculate average precision and the PR curve\naverage_precision = average_precision_score(y_test, predictions_rf)\nprint(f'Average Precision: {average_precision:.3f}%')","1880bde1":"# Obtain Precision and Recall \nprecision, recall, _ = precision_recall_curve(y_test, predictions_rf)\nprint(f'Precision: {precision}\\nRecall: {recall}')","8024365f":"def plot_pr_curve(recall, precision, average_precision):\n    from inspect import signature\n    plt.figure()\n    step_kwargs = ({'step': 'post'}\n                   if 'step' in signature(plt.fill_between).parameters\n                   else {})\n\n    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.0])\n    plt.xlim([0.0, 1.0])\n    plt.title(f'2-Class Precision-Recall curve: AP = {average_precision:0.2f}')\n    return plt.show()","5e80967e":"# Plot the recall precision tradeoff\nplot_pr_curve(recall, precision, average_precision)","3b333ad1":"# Define the model with balanced subsample\nmodel = RandomForestClassifier(class_weight='balanced_subsample', random_state=2020, n_estimators=100)\n\n# Fit your training model to your training set\nmodel.fit(X_train, y_train);","5686788e":"# Obtain the predicted values and probabilities from the model \npredicted = model.predict(X_test)\nprobs = model.predict_proba(X_test)\n\nprint('\\nClassification Report:')\n\nprint(classification_report(y_test, predicted))\n\nprint(f'ROC-AUC Score: {roc_auc_score(y_test, predicted):.2f}\\n')\n\nprint('\\nConfusion Matrix:')\nprint(confusion_matrix(y_test, predicted))\n\ntn,fp,fn,tp = confusion_matrix(y_test, predicted).ravel()\nprint()\nprint(f\"TN : {tn}\")\nprint(f\"FP : {fp}\")\nprint(f\"FN : {fn}\")\nprint(f\"TP : {tp}\")","04d2e789":"plot_roc_curve(y_test, predicted)","4bbaf81b":"def get_model_results(X_train: np.ndarray, y_train: np.ndarray,\n                      X_test: np.ndarray, y_test: np.ndarray, model):\n    \"\"\"\n    model: sklearn model (e.g. RandomForestClassifier)\n    \"\"\"\n    # Fit your training model to your training set\n    model.fit(X_train, y_train)\n\n    # Obtain the predicted values and probabilities from the model \n    predicted = model.predict(X_test)\n\n    print('\\nClassification Report:')\n    \n    print(classification_report(y_test, predicted))\n\n    print(f'ROC-AUC Score: {roc_auc_score(y_test, predicted):.2f}\\n')\n\n    print('\\nConfusion Matrix:')\n    plt.figure()\n    cm = confusion_matrix(y_test, predicted)\n    \n    plot_confusion_matrix(cm, figsize=(4,4), hide_ticks=True, cmap=plt.cm.Blues)\n    plt.xticks(range(2), ['Normal', 'Fraud'], fontsize=16)\n    plt.yticks(range(2), ['Normal', 'Fraud'], fontsize=16)\n    plt.show()\n    plot_roc_curve(y_test, predictions);","aaec2a46":"def compute_class_freqs(labels):\n    \"\"\"\n    Compute positive and negative frequences for each class.\n\n    Args:\n        labels (np.array): matrix of labels, size (num_examples, num_classes)\n    Returns:\n        positive_frequencies (np.array): array of positive frequences for each\n                                         class, size (num_classes)\n        negative_frequencies (np.array): array of negative frequences for each\n                                         class, size (num_classes)\n    \"\"\"\n    # Total number of patients (rows)\n    N = labels.shape[0]\n    \n    positive_frequencies = np.sum(labels, axis=0) \/ N\n    negative_frequencies = 1 - positive_frequencies\n    \n    return positive_frequencies, negative_frequencies","d313dc85":"# Computing class frequencies for our training set\nfreq_pos, freq_neg = compute_class_freqs(y_train)\npos_weights = freq_neg\nneg_weights = freq_pos\nclass_weights = {0: neg_weights, 1:pos_weights}\nclass_weights","0e593537":"# Change the model options\nmodel = RandomForestClassifier(bootstrap=True,\n                               class_weight = class_weights,\n                               criterion='entropy',\n                               # Change depth of model\n                               max_depth=10,\n                               # Change the number of samples in leaf nodes\n                               min_samples_leaf=10, \n                               # Change the number of trees to use\n                               n_estimators=20,\n                               n_jobs=-1,\n                               random_state=2020)\n\n# Run the function get_model_results\nget_model_results(X_train, y_train, X_test, y_test, model)","09ba1790":"# Define the paramter sets to test\nparam_grid = {'n_estimators': [1,30],\n              'max_features': ['auto', 'log2'],\n              'max_depth': [4, 8, 10, 12],\n              'criterion': ['gini', 'entropy']}\n\n# Define the mode to use\nmodel = RandomForestClassifier(random_state=2020)\n\n# Combine the parameter sets with the defined model\nCV_model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1)\n\n# Fit the model to our training data and obtain best parameters\nCV_model.fit(X_train, y_train)\nCV_model.best_params_","0a8ae436":"# Input the optimal parameters in the model\nmodel = RandomForestClassifier(class_weight=class_weights,\n                               criterion='gini',\n                               max_depth=12,\n                               max_features='auto', \n                               min_samples_leaf=10,\n                               n_estimators=30,\n                               n_jobs=-1,\n                               random_state=2020)\nmodel.fit(X_train, y_train)","4fbfc407":"# Get results from your model\nget_model_results(X_train, y_train, X_test, y_test, model);","f3445139":"**NOTE:** Not in all cases resampling necessarily lead to better results. When the fraud cases are very spread and scattered over the data, using SMOTE can introduce a bit of bias. Nearest neighbors aren't necessarily also fraud cases, so the synthetic samples might 'confuse' the model slightly. ","c262fee7":"<a id='section4_5'><\/a>\n### 4.5 Correlation between features\n\nIt would be interesting to know if there is any significant correlation between our predictors. Let's do this using heatmap.","918d1b71":"<a id='section6_1_2'><\/a>\n### 6.1.2 Fit the ressampled data to our LR Model","0b5787b2":"| Algorithm | Accuracy (%) | Recall (%) | Precision (%) | F1 Score (%) | ROC-AUC Score (%) |\n| --- | --- | --- | --- | --- | --- |\n| Logistic Regression | 100 | 63 | 84 | 72 | 81 |  \n| Logistic Regression with SMOTE| 97 | 91 | 5 | 10 | 94 |\n| Logistic Regression with Borderline SMOTE| 99 | 84 | 16 | 27 | 92 |\n| RandomForest Classifier | 100 | 78 | 93 | 85 | 89 |\n| RandomForest with balanced_subsample | 100 | 78 | 95 | 86 | 89 |\n| RandomForest with class_weights | 100 | 84 | 82 | 83 | 92 |\n| RandomForest with GridSearchCV | 100 | 84 | 82 | 83 | 92 |","7f04dd52":"**NOTE:** We have now obtained more meaningful performance metrics that tell us how well the model performs, given the highly imbalanced data that we're working with. \n\n\nThis model predict 104 cases of fraud out of 134 fraud cases.\n\nWe have only 8 False Positives, as a result we have achieved high precision score. \n\nBut unfortunately we got higher number of False Negative i.e. 30 which will ultimately gives a low recall which is not good for this problem.","601d1e71":"<a id='section7'><\/a>\n### 7. Conclusion","fdc93018":"**NOTE:** The graph indicates that Fraud transaction time are evenly distributed all over the place while non-fraud transaction time is bimodal in nature.","344abc7c":"**NOTE:** As we can see, some of the predictors do seem to have correlation between them. But majority of the predictors are not correlated. This could be due to some factors:\n\n* The dimensionality of data is already reduced using PCA(Principle Componenet Analysis), therefore our predictors are principal components. Principal Components are orthogonal to each other.\n\n* The huge class imbalance might distort the importance of certain correlations with regards to our class variable.","d6c9c36f":"**NOTE:** Since there is a class imbalance problem in our dataset as we can see from above graph, we are going to increase class 1(i.e. Fraud Transaction) data points.\n\nWe are going to use Synthetic Minority Over-sampling Technique (SMOTE), because it creates new, synthetic, samples that are quite similar to the existing observations in the minority class. SMOTE is therefore slightly more sophisticated than just copying observations, so let's apply SMOTE to our credit card data.","ee3be489":"<a id='section1'><\/a>\n### 1. Introduction","1b9b538a":"<a id='section4_2'><\/a>\n\n<a id='section4_2_1'><\/a>\n### 4.2.1 Overall Time Distribution","1a4d5c39":"**NOTE:** From above stats, we have noticed that the maximum amount of transaction in fraudulent data is 2125 and amount of transaction in non-fraudulent data is quite high i.e. 25691.\n\nIn both cases(fraud and no-fraud) mean is greater than median, so we can say that the distribution of both of them are right skewed.","aa34acb8":"**NOTE:** This tells us that by doing nothing, we would be correct in 99.83% of the cases. So, if we get an accuracy of less than this number, our model does not actually add any value in predicting how many cases are correct. Let's see how a random forest does in predicting fraud in our data.","ea34b5b1":"<a id='section4_4'><\/a>\n### 4.4 Class Distribution","cf975b00":"<a id='section6_2'><\/a>\n### 6.2 Fit the data(without resample) to LR Model","c2725008":"<a id='section4_6'><\/a>\n### 4.6 Dimensionality Reduction and Clustering to visualize data in two dimension","420e89c1":"**NOTE:** Since our dataset is already processed as PCA is done on 29 features and we have also standardized other remaining features, we are good to go ahead.\n\nIf any changes are required in the dataset during the modeling process, we will do it on the go.","b988f080":"<a id='section6_6'><\/a>\n### 6.6 Using Class Weights ","af904e6b":"<a id='section6_1'><\/a>\n\n<a id='section6_1_1'><\/a>\n### 6.1.1 Using SMOTE to address Class Imbalance Problem","f377f63d":"<a id='section4_1'><\/a>\n### 4.1 Descriptive Analysis","30495f99":"The Feature `Amount` ranges from 0 to 25691.16. To reduce its wide range, I used Standardization to remove the mean and scale to unit variance, so that 68% of the values lie in betweeen (-1,1).","65df9d12":"<a id='section6_5'><\/a>\n### 6.5 Random Forest Classifier with 'balanced_subsample'\n\n`balanced` mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples \/ (n_classes * np.bincount(y)).\n\nThe `balanced_subsample` mode is the same as `balanced` except that weights are computed based on the bootstrap sample for every tree grown.","085f27dd":"**NOTE:** As we can clearly see that model without resampled data gives better results as compare to the model with resampled data. This might be due to the remsampling method SMOTE which probably has introduced some bias in our dataset.\n\nLet's try another type of SMOTE i.e. Borderline SMOTE","88f9d872":"<a id='section5'><\/a>\n### 5. Data Preprocessing","9b666693":"##  \ud83d\udcb2Credit Card Fraud Detection\ud83d\udcb3","dcb02b6c":"**NOTE:** From the above graph, we have noticed that the distribution of time is bimodal in nature which inturns also indicates that there is a sudden fall in the volume of transactions after 28 hours of the first transaction been made.\n\nAs the timing of the transactions are not provided, we can assume that the drop in volume occured during night.","b86eb2a9":"##### Compare SMOTE to original data","e5684d19":"**NOTE:** This is the best model we have got till now, it predicts fraud and non-fraud transactions pretty well.\n\nThere is a perfect balance between Recall and Precision which is a good thing here.","a04680e6":"**NOTE:** The dataset contains 284,807 transactions, the mean transaction amount for non-fraudulent data is `$88.29`  and median is `$9` while the maximum transaction amount turns out to be `$25691`.\nWe can say that the distribution of non-fraudulent transactions is heavily right skewed i.e. mostly all transaction are under amount `$180` rest are outliers.\n\nIn case of fraudulent data, the mean transaction amount is `$121`  and median is `$22` while the maximum transaction amount is `$2125`. This means that the distribution of fraud transaction amount is also right skewed i.e. mostly all transaction are under amount `$250` rest are outliers.","a6e593e8":"<a id='section6'><\/a>\n### 6. Model Training, Tunning and Performance Evaluation","3c93e158":"SMOTE has balanced our data completely, and that the minority class is now equal in size to the majority class. Visualizing the data shows the effect on data very clearly.","0f7206db":"<a id='section4'><\/a>\n### 4. Exploratory Data Analysis(EDA)","06321fe8":"**NOTE:** By defining more options in the model, we obtained better predictions. \n\nWe have effectively reduced the number of false negatives, i.e. we are catching more cases of fraud, while keeping the number of false positives low.\n\nLet's use GridSearchCV to find out the best parameters to fit our model.","12cf6065":"A balance between precision and recall needs to be achieved in our model, otherwise we might end up with many false positives, or not enough actual fraud cases caught. To achieve this and to compare performance, the precision-recall curves come in handy.","bc875eec":"<a id='section6_4'><\/a>\n### 6.4 Fit the data using Random Forest Classifier ","c8ba974a":"### Index\n* <a href='#section1'>1. Introduction<\/a><br>\n    * <a href='#section1_1'>Types of Credit Card Frauds<\/a><br>\n* <a href='#section2'>2. Problem Statement<\/a>\n* <a href='#section3'>3. Data Description<\/a>\n* <a href='#section4'>4. Exploratory Data Analysis(EDA)<\/a>\n    * <a href='#section4_1'>4.1 Descriptive Analysis<\/a><br>\n    * <a href='#section4_2'>4.2 Time Distribution<\/a><br>\n        * <a href='#section4_2_1'>4.2.1 Overall Time Distribution<\/a><br>\n        * <a href='#section4_2_2'>4.2.2 Fraud VS Non-Fraud Time Distribution<\/a><br>\n    * <a href='#section4_3'>4.3 Amount Distribution<\/a><br>\n    * <a href='#section4_4'>4.4 Class Distribution<\/a><br>\n    * <a href='#section4_5'>4.5 Correlation between features<\/a><br>\n    * <a href='#section4_6'>4.6 Dimensionality Reduction and Clustering to visualize data in two dimension<\/a><br>\n* <a href='#section5'>5. Preprocessing<\/a><br>\n    * <a href='#section5_1'>5.1 Standardization<\/a><br>\n* <a href='#section6'>6. Model Training, Tunning and Performance Evaluation<\/a><br>\n    * <a href='#section6_1'>6.1 Class Imbalance Problem<\/a><br>     \n        * <a href='#section6_1_1'>6.1.1 Using SMOTE to address Class Imbalance Problem<\/a><br>\n        * <a href='#section6_1_2'>6.1.2 Fit the ressampled data to our LR Model<\/a><br>\n    * <a href='#section6_2'>6.2 Fit the data(without resample) to LR Model<\/a><br>\n    * <a href='#section6_3'>6.3 Borderline SMOTE<\/a><br>\n    * <a href='#section6_4'>6.4 Random Forest Classifier<\/a><br>\n    * <a href='#section6_5'>6.5 Random Forest Classifier with 'balanced_subsample'<\/a><br>\n    * <a href='#section6_6'>6.6 Using Class Weights<\/a><br>\n* <a href='#section7'>7. Conclusion<\/a><br>","9dcbc6fe":"As world is getting more towards digitalization, the risk of online fraud is also increasing. Most of the e-commerce and online websites are moving towards online payment mode which ultimately gives rise to online frauds. Also, due to this pandemic situation(COVID-19), everyone prefers to do cashless transaction which increases the chances of people getting trapped into such frauds. \n\nAmong all of the online frauds, one such fraud is credit card fraud which is an ever growing menace in the financial industry. Detecting fraudulent transaction is of great importance for any credit card company.\n\nIn this project, we are going to approach this real life problem with Machine Learning.\n\n![CC.png](attachment:CC.png)\n\n<a id='section1_1'><\/a>\n**Types of Credit Card Fraud**\n* `Card-Not-Present(CNP) Fraud`: Customers who are away from their physical card simply enter details and make a purchase. Customers should be aware when entering details, for instance they should not click on the link provided somewhere and don't get tricked into divulging confidential data such as password.\n\n\n* `Lost and Stolen Card Frauds`: If Card has been stolen or lost, then pickpockter is free to use that card until it's blocked, cancelled or has hit the credit limit.\n\n\n* `Card Never Arrived Fraud`: Card ordered by a customer but they never recieve it. Card is either intercepted by the wrong person before it reached to the customer or if your card is simply pinched from your letterbox.\n\n\n* `False Application Fraud`: It occurs where the account was established using someone's else identity.\n\n\n* `Counterfeit or Skimming Fraud`: Counterfeit occurs when details are illegaly taken to create a counterfeit credit card. \nSkimming is when a device steals the details of your credit card from its magnetic strip, generally happens in ATM.\n\n![card-fraud-2018.png](attachment:card-fraud-2018.png)\n","197b8109":"<a id='section3'><\/a>\n### 3. Data Description\n\nThe dataset is obtained from <a href=\"https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\">Kaggle<\/a>. \n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, they did not provided the original features and more background information about the data. \n\nFeatures V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. \n\nThere are **284807** number of transactions(rows) and **31** features in this dataset.\n\n`Time`: It contains the seconds elapsed between each transaction and the first transaction in the dataset. \n\n`Amount`: It is the transaction Amount.\n\n`Class`: It is the response variable and it takes value 1 in case of fraud and 0 otherwise.","846f3ff6":"We will firstly focus on the features such as Time, Amount and Class, since rest of them are anonymized(unnamed).","2d4856c0":"**NOTE:** No big changes as compared to the previous model.\n\nLet's explore more methods to get a good recall as well as precision.","610bca41":"Time is the number of seconds elapsed between each transaction and the first transaction in the dataset.\n\nThe maximum time elapsed between last transaction and first transaction is 172792 seconds i.e. 48 hours. So, we can see that this dataset consist of transaction record of two days.\n\nLet's explore more about it by visualizing the data.","60b9b9f0":"We have investigated the data to get some insights out of it, checked for data imbalance, visualized all features and their relationship with each other. \n\nIn order to detect Credit Card Fraud with overly imbalanced data is such a difficult and time-consuming task. \n\nI have tried many things to keep up the balance between recall and precision. \n\nWe started with `SMOTE` to balance the dataset, but after fitting data I find out that it doesn't work well may be because new examples are synthesized from the existing examples. Precision and F1-Score are very low.\n\nNext we have implemented other type of SMOTE i.e. `Borderline  SMOTE` to overcome class imbalance problem. Our model definitely improved as compared to normal SMOTE but still the results are not satisfactory.Precision and F1-score are low.\n\nThen, We have used `Random Forest` Ensemble Algorithm, which has definitely decreased False Positives but False Negatives are higher in number. No balance between Recall and Precision. \n\nWe then followed with `Random Forest with Class Weights`, which did not improved our model.\n\nWe then found the positive and negative `class weights` and presented to the `Random Forest` Algorithm, False Negatives has decresed from 29 to 22, but precision again increased from 6 to 24, which is not a great thing. Balance between Recall and Precision, satisfactory F1-score.\n\nLastly, we get into `GridSearchCV` with `Random Forest` and find out our best estimators which has given a good recall and precision.","a1d8f985":"**NOTE:** As we can see the number of fraudulent transaction are very low as compared to Non-Fraudulent transaction. \n\nBuilding a model with such huge class difference would not be a great idea because then the model will not able to recognise fraudulent transaction, we might get a lot of errors and our algorithm will probably overfit since it will assume all transactions as `fraud`. We need to keep that in mind while building our model.\n\n**Things to Remember:** Class Imbalance problem in the dataset.","0cda4ac5":"**NOTE:** Not in all cases resampling necessarily lead to better results. When the fraud cases are very spread and scattered over the data, using SMOTE can introduce a bit of bias. Nearest neighbors aren't necessarily also fraud cases, so the synthetic samples might 'confuse' the model slightly. ","7c0518c4":"<a id='section5_1'><\/a>\n### 5.1 Standardization","998d5666":"<a id='section2'><\/a>\n### 2. Problem Statement\n\nThe development of a model that provide best results in identifying credit card fraudulent transactions.\n\nThis helps both, the credit card company and the customers from getting charged unecessarily. ","9e3322f8":"<a id='section4_2_2'><\/a>\n### 4.2.2 Fraud VS Non-Fraud Time Distribution","c6bdbd18":"<a id='section6_3'><\/a>\n### 6.3 Borderline SMOTE\n\nThis algorithm is a variant of the original SMOTE algorithm. Borderline samples will be detected and used to generate new synthetic samples.","1e025922":"Our minority class is now much more prominently visible in our data.","b55090f1":"<a id='section4_3'><\/a>\n### 4.3 Amount Distribution"}}