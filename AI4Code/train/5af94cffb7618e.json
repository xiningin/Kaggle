{"cell_type":{"607c283d":"code","dc64d0c9":"code","0ebb48a8":"code","d8ba4565":"code","da2e9494":"code","aa09bf2a":"code","aa385122":"code","1237b361":"code","c2ba9029":"code","22d0781f":"code","5f132ba1":"code","c40ab548":"code","cdc919cc":"code","a330bef0":"code","4fbc8f38":"code","701d23e6":"code","4cfd3db1":"code","1b0d472d":"code","e3213a6b":"code","59406861":"code","f83e4ae0":"code","f42faac8":"code","ae3804a7":"code","473de744":"code","201f0d04":"code","9e54508e":"code","36ff7e58":"code","dc6337ec":"markdown","c6883ee1":"markdown","8654a19e":"markdown","2a58b2a0":"markdown","764b34d4":"markdown","579d9acd":"markdown","5309d3a2":"markdown","877760aa":"markdown","1647875e":"markdown","7591c254":"markdown","539fc74a":"markdown","4ede4ccf":"markdown","49b987ba":"markdown","b14d9a0c":"markdown","7a70199f":"markdown","3d73daff":"markdown","a516fcd2":"markdown","6b246aa7":"markdown","7b7af139":"markdown","d9c26332":"markdown","5d0fac1d":"markdown","ada82991":"markdown","dce4d20f":"markdown","23a8dfea":"markdown","df61b270":"markdown","02018d98":"markdown","8414eb3b":"markdown","966be65a":"markdown","0747f000":"markdown","b2de0e8f":"markdown","478daddb":"markdown","3df7d7eb":"markdown","54e31297":"markdown","d8af0b41":"markdown","45d9ab97":"markdown","1426c1fa":"markdown","ddf3f892":"markdown","e7558c27":"markdown"},"source":{"607c283d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Scikit learn librairies\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import accuracy_score\n\nimport pandas as pd\nimport numpy as np\n\n%matplotlib inline","dc64d0c9":"df_test= pd.read_csv(\"..\/input\/give-me-some-credit-dataset\/cs-test.csv\")\ndf= pd.read_csv(\"..\/input\/give-me-some-credit-dataset\/cs-training.csv\")\ndftmp=pd.read_csv(\"..\/input\/give-me-some-credit-dataset\/cs-training.csv\")\nsample_entry=pd.read_csv(\"..\/input\/give-me-some-credit-dataset\/sampleEntry.csv\")","0ebb48a8":"df.shape","d8ba4565":"df.head(10)","da2e9494":"df.describe()","aa09bf2a":"sample_entry.head()","aa385122":"df.columns","1237b361":"df.dtypes.value_counts()","c2ba9029":"df.isnull().sum()","22d0781f":"#A function to print every graph with the ID as \ndef print_all_values():\n    df1=df.drop('Unnamed: 0',axis=1)\n    cols=df1.columns\n    for col in cols:\n        if (df[col].dtypes !='object'):\n\n            fig1=plt.figure()\n            ax1=plt.axes()\n            plt.scatter(df[[col]],df['Unnamed: 0'],alpha=1,s=0.5)\n            plt.title(col)\n            ax1 = ax1.set(xlabel=col, ylabel='ID')\n            plt.show()\n            \n            \nprint_all_values()","5f132ba1":"print(df.shape)\ndef delete_absurd_values(df_transformed,cols,max_value,percentage):\n        \n        \n        for col in cols:\n            if (df_transformed[col].dtypes !='object'):\n                       \n                q99=df_transformed[col].quantile(q=percentage)\n                q01=df_transformed[col].quantile(q=(1-percentage))\n                for i in df_transformed.index:\n                    \n                    if (df_transformed.loc[i,col]> max_value*q99 or df_transformed.loc[i,col]< q01\/max_value):\n                        df_transformed=df_transformed.drop(index=i)\n        \n        return df_transformed\n\ncols=['DebtRatio', 'MonthlyIncome',\n       'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',\n       'NumberRealEstateLoansOrLines',\n       'NumberOfDependents']\ndf=delete_absurd_values(df,cols,4,0.999)\nprint(df.shape)","c40ab548":"df=df[df.RevolvingUtilizationOfUnsecuredLines <30000]\ndf=df[df.DebtRatio <100000]\ndf=df[df.MonthlyIncome <15000000]\ndf=df[df.NumberRealEstateLoansOrLines <40]","cdc919cc":"df.fillna(df.median(), inplace=True)","a330bef0":"df.isnull().sum()","4fbc8f38":"fig11=plt.figure()\nax11=plt.axes()\nthe_target = dftmp['SeriousDlqin2yrs']\nthe_target.replace(to_replace=[1,0], value= ['YES','NO'], inplace = True)\nplt.title('Target repartition')\nax11 = ax11.set(xlabel='Default proportion', ylabel='Number of people')\nthe_target.value_counts().plot.pie(startangle=90, autopct='%1.1f%%')\nplt.show()","701d23e6":"sns.set(style = 'whitegrid', context = 'notebook', rc={'figure.figsize':(20,15)})\n\n\ncols = ['SeriousDlqin2yrs',\n       'RevolvingUtilizationOfUnsecuredLines', 'age',\n       'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome',\n       'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',\n       'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse',\n       'NumberOfDependents']\n\n#sns.pairplot(df[cols])\nplt.show()","4cfd3db1":"#Correlation Matrix calcul\ncorr_mat = df.corr()\n\nfig2=plt.figure()\nsns.set(rc={'figure.figsize':(25,15)})\nk = 20\ncols = corr_mat.nlargest(k, 'SeriousDlqin2yrs')['SeriousDlqin2yrs'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.title('Correlation Matrix')\nplt.show()","1b0d472d":"X = df.drop('SeriousDlqin2yrs',axis=1)\ny = df['SeriousDlqin2yrs']  \n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)","e3213a6b":"from sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100,random_state=0)\nlogisticRegr.fit(X_train, y_train)","59406861":"#ERROR\nerror = (1 - logisticRegr.score(X_test, y_test))*100\nprint('Score  = ',logisticRegr.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","f83e4ae0":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda=LinearDiscriminantAnalysis(solver='svd',shrinkage=None,store_covariance=True)\nlda.fit(X_train, y_train)","f42faac8":"#ERROR\nerror = (1 - lda.score(X_test, y_test))*100\nprint('Score  = ',lda.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","ae3804a7":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=300, oob_score=True, random_state=0)\nrf.fit(X_train,y_train)","473de744":"error = (1 - rf.score(X_test, y_test))*100\nprint('Score  = ',rf.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","201f0d04":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train,y_train)","9e54508e":"error = (1 - clf.score(X_test, y_test))*100\nprint('Score  = ',clf.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","36ff7e58":"print('Taux de r\u00e9ussite par mod\u00e8le:\\n\\nR\u00e9gression Logistique:',logisticRegr.score(X_test, y_test)*100,'%','\\n\\nLDA:',lda.score(X_test, y_test)*100,'%','\\n\\nRandom Forest Classifier:',rf.score(X_test, y_test)*100,'%','\\n\\nDecision Tree Classifier:',clf.score(X_test, y_test)*100,'%')","dc6337ec":"Commencons par afficher les dimensions de la base de donn\u00e9e.","c6883ee1":"#### Score et erreur","8654a19e":"## S\u00e9parer les donn\u00e9es entre les donn\u00e9e d'entrainement et de test","2a58b2a0":"## Regression Logistique","764b34d4":"Nous chargons les bases de donn\u00e9es dont nous allons avoir besoin.","579d9acd":"On fini par supprimer manuellement les valeurs qui parraissent toujours aberrantes.","5309d3a2":"# Give me some credit dataset","877760aa":"Apr\u00e8s entrainement de nos algorithmes, nous avons obtenu les r\u00e9sultats suivants sur le jeu de test:","1647875e":"#### Matrice de Corr\u00e9lation","7591c254":"Afin d'avoir un vue claire de la r\u00e9partition des donn\u00e9es, nous avons \u00e9crit une fonction afin d'afficher la valeur de chaque cellule pour chaque cat\u00e9gorie.","539fc74a":"## LDA","4ede4ccf":"En ce qui concerne les valeurs manquantes, comme nous l'avons dit pr\u00e9c\u00e9dement, nous avons d\u00e9cid\u00e9 de les remplacer par la valeur m\u00e9diane de chaque classe. Avant cela, nous avons essay\u00e9 plusieurs techniques, parmis elles : de les supprimer, supprimer la colonne, remplacer par la moyenne, remplacer par un constante.","49b987ba":"Gr\u00e2ce \u00e0 cette matrice de corr\u00e9lation on remarque que les features qui ont la plus grande influence sur la target sont l'age et d'avoir fait d\u00e9faut pas le passer. Etonnement le salaire ne semble pas \u00e0 premi\u00e8re vue avoir un grand impact. Cependant ces coefficients restent relativement faibles.\n\nOn notera \u00e9galement qu'il y a un tr\u00e8s grande corr\u00e9lation entre les differents temps de d\u00e9faut de cr\u00e9dit.","b14d9a0c":"## Chargement des librairies","7a70199f":"Le nombre important de features rend difficile une lecture intelligible de ce graphique. Nous pr\u00e9f\u00e9rerons utiliser une matrice de corr\u00e9lation pour comprendre le lien entre les features.","3d73daff":"#### Score et erreur","a516fcd2":"Il n'y a que des valeurs num\u00e9riques, cela facilitera grandement le traitement de la base.","6b246aa7":"Nous nous int\u00e9ressons au type de chaque variable.","7b7af139":"On utilise 80% du dataset pour entrainer notre algorithme et 20% pour effectuer les tests. On donne un random_state pour qu'\u00e0 chaque execution de la fonction, celle-ci s\u00e9pare les donn\u00e9es de la meme facon et ainsi on introduit un biais constant \u00e0 chaque it\u00e9ration.","d9c26332":"# R\u00e9sultats","5d0fac1d":"---------------------------------------------------------------------------------------------\n#                  Algorithmes de Machine Learning\n----------------------------------------------------------------------------------------------","ada82991":"#### Graphiques compar\u00e9s","dce4d20f":"Nous remarquons quelque chose de probl\u00e9matique. Il manque pr\u00e8s de 30 000 valeurs pour le salaire, soit environ 20% des valeurs. Il est pourtant \u00e9vident que cette donn\u00e9e est cruciale pour r\u00e9ussir notre classification. Apr\u00e8s plusieurs essais, nous en avons conclu que la m\u00e9thode de la m\u00e9diane \u00e9tait la plus efficace pour g\u00e9rer ces valeurs manquantes. Nous effectuerons cette op\u00e9ration par la suite.","23a8dfea":"Enfin une description math\u00e9matique des donn\u00e9es.","df61b270":"## Random Forest Classifier","02018d98":"Il s'agit de la probabilit\u00e9 de d\u00e9faut calcul\u00e9 par la banque pour chaque individu. Nous n'utiliserons \u00e9videment pas cette donn\u00e9e puisqu'elle a d\u00e9j\u00e0 \u00e9t\u00e9 calcul\u00e9e par la banque elle m\u00eame !","8414eb3b":"Puis les 10 premiers \u00e9l\u00e9ments du dataset.","966be65a":"#### Score et erreur","0747f000":"Regardons maintenant le nombre de valeurs nulles par colonne","b2de0e8f":"Regardons \u00e0 pr\u00e9sent les 5 premi\u00e8res valeurs du dataframe sample_entry.csv","478daddb":"On v\u00e9rifie qu'il ne reste plus de valeur nulles","3df7d7eb":"-------------------------------------------------------------------------\n       Matthieu\n--------------------------------------------------------------------------","54e31297":"V\u00e9rifions que la r\u00e9partition de la target est inchang\u00e9e","d8af0b41":"Nous pouvons supprimer les valeurs \"aberrantes\" de la base de donn\u00e9e. Cela nous aidera \u00e0 am\u00e9liorer les pr\u00e9dictions de nos mod\u00e8les. Pour nous aider dans cette t\u00e2che nous avons cr\u00e9\u00e9 une fonction qui supprime les valeurs qui paraissent trop extr\u00eames. Elle prend en param\u00e8tre la limite maximum qu'une valeur peut prendre en fonction des autres valeurs du m\u00eame feature.","45d9ab97":"## Tree Decision Classifier","1426c1fa":"#### Score et erreur","ddf3f892":"Pour cette analyse nous allons avoir besoin des librairies de calculs (panda et numpy), de graphiques (matplotlib et seaborn) et de scikit learn pour le machine learning.","e7558c27":"Affichons les colonnes de notre base."}}