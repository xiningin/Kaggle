{"cell_type":{"bf837916":"code","aedb31bd":"code","e1246478":"code","4b9fc2bc":"code","16a97f5d":"code","b698b66f":"code","e08c7f3f":"code","6c43a2ac":"code","97e1893d":"code","66feec9e":"code","df92cadb":"code","95c01a18":"code","d5600272":"code","ad766fad":"code","35ddcbbb":"code","2b73a51e":"code","de1c9d52":"code","bdc69a0d":"code","c3e0b021":"code","4d4bd09f":"code","32e801b1":"code","0b4e38dd":"code","9a458b8a":"markdown","3f2fc45e":"markdown","b3fa5b98":"markdown","1f74607f":"markdown","a02e7c0a":"markdown","13c8a603":"markdown"},"source":{"bf837916":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import accuracy_score, mean_squared_error, log_loss\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom matplotlib.colors import LinearSegmentedColormap","aedb31bd":"data = pd.read_csv('..\/input\/heart.csv')","e1246478":"data.head()","4b9fc2bc":"X = data.drop(columns='target')\nY = data['target']","16a97f5d":"X.head()","b698b66f":"Y.head()","e08c7f3f":"print(X.shape, Y.shape)","6c43a2ac":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0, stratify=Y)","97e1893d":"print(X_train.shape, Y_train.shape)\nprint(X_test.shape, Y_test.shape)","66feec9e":"X_train = X_train.values\nX_test = X_test.values\nY_train = Y_train.values\nY_test = Y_test.values","df92cadb":"std_scaler = StandardScaler()","95c01a18":"X_scaled_train = std_scaler.fit_transform(X_train)","d5600272":"X_scaled_test = std_scaler.transform(X_test)","ad766fad":"minmax_scaler = MinMaxScaler()","35ddcbbb":"Y_scaled_train = minmax_scaler.fit_transform(Y_train.reshape(-1, 1))","2b73a51e":"Y_scaled_test = minmax_scaler.transform(Y_test.reshape(-1, 1))","de1c9d52":"print(Y_scaled_train.min(), Y_scaled_train.max())\nprint(Y_scaled_test.min(), Y_scaled_test.max())","bdc69a0d":"class SigmoidNeuron:\n    def __init__(self):\n        self.w = None\n        self.b = None\n    \n    def perceptron(self, x):\n        return np.dot(x, self.w.T) + self.b\n    \n    def sigmoid(self, x):\n        return 1.0\/(1.0 + np.exp(-x))\n    \n    def grad_w_mse(self, x, y):\n        y_pred = self.sigmoid(self.perceptron(x))\n        return (y_pred - y) * y_pred * (1 - y_pred) * x\n    \n    def grad_b_mse(self, x, y):\n        y_pred = self.sigmoid(self.perceptron(x))\n        return (y_pred - y) * y_pred * (1 - y_pred)\n    \n    def grad_w_ce(self, x, y):\n        y_pred = self.sigmoid(self.perceptron(x))\n        if y == 0:\n            return y_pred * x\n        elif y == 1:\n            return -1 * (1 - y_pred) * x\n        else:\n            raise ValueError(\"y should be 0 or 1\")\n        \n    def grad_b_ce(self, x, y):\n        y_pred = self.sigmoid(self.perceptron(x))\n        return (-1 * (1 - y_pred)) if y ==1 else y_pred\n            \n    def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, loss_fn=\"mse\", display_loss=False):\n    \n        # initialise w, b\n        if initialise:\n            self.w = np.random.randn(1, X.shape[1])\n            self.b = 0\n            \n        if display_loss:\n            loss = {}\n        \n        for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n            dw = 0\n            db = 0\n            for x, y in zip(X, Y):\n                if loss_fn == \"mse\":\n                    dw += self.grad_w_mse(x, y)\n                    db += self.grad_b_mse(x, y) \n                elif loss_fn == \"ce\":\n                    dw += self.grad_w_ce(x, y)\n                    db += self.grad_b_ce(x, y)\n\n                self.w -= learning_rate * dw\n                self.b -= learning_rate * db\n                \n            if display_loss:\n                Y_pred = self.sigmoid(self.perceptron(X))\n                if loss_fn == \"mse\":\n                    loss[i] = mean_squared_error(Y, Y_pred)\n                elif loss_fn == \"ce\":\n                    loss[i] = log_loss(Y, Y_pred)\n        \n        if display_loss:\n            plt.plot(loss.values())\n            plt.xlabel('Epochs')\n            if loss_fn == \"mse\":\n                plt.ylabel('Mean Squared Error')\n            elif loss_fn == \"ce\":\n                plt.ylabel('Log Loss')\n            plt.show()\n            \n    def predict(self, X):\n        Y_pred = [self.sigmoid(self.perceptron(x)) for x in X]\n        return np.array(Y_pred)\n    \n    def predict_accuracy(self, X, Y, threshold=0.5):\n        Y_pred = [self.sigmoid(self.perceptron(x)) for x in X]\n        Y_pred = np.array(Y_pred)\n        Y_pred = (Y_pred >= threshold).astype(int)\n        \n        Y = (Y >= threshold).astype(int)\n        \n        return accuracy_score(Y_pred, Y)\n        ","c3e0b021":"# Training the model using Mean Squared Error loss function\nsn_mse = SigmoidNeuron()\nsn_mse.fit(X_scaled_train, Y_scaled_train, epochs=1000, learning_rate=0.015, loss_fn=\"mse\", display_loss=True)","4d4bd09f":"# Training the model usign Cross Entropy loss function\nsn_ce = SigmoidNeuron()\nsn_ce.fit(X_scaled_train, Y_scaled_train, epochs=40, learning_rate=0.0006, loss_fn=\"ce\", display_loss=True)","32e801b1":"print('MSE Training Accuracy-->', sn_mse.predict_accuracy(X_scaled_train, Y_scaled_train))\nprint('CE Training Accuracy-->', sn_ce.predict_accuracy(X_scaled_train, Y_scaled_train))","0b4e38dd":"print('MSE Training Accuracy-->', sn_mse.predict_accuracy(X_scaled_train, Y_scaled_train))\nprint('MSE Test Accuracy-->', sn_mse.predict_accuracy(X_scaled_test, Y_scaled_test))\nprint('--'*40)\nprint('CE Training Accuracy-->', sn_ce.predict_accuracy(X_scaled_train, Y_scaled_train))\nprint('CE Test Accuracy-->', sn_ce.predict_accuracy(X_scaled_test, Y_scaled_test))","9a458b8a":"# Sigmoid Neuron Class","3f2fc45e":"# Standardisation of Data","b3fa5b98":"# Training the Model","1f74607f":"# Model Testing","a02e7c0a":"# Train-Test Split","13c8a603":"# Importing Data"}}