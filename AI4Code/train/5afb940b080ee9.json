{"cell_type":{"5f290050":"code","9a45dfa7":"code","90f48307":"code","ea228528":"code","eb1e34e6":"code","192d3a99":"code","734e662d":"code","d746aaa8":"code","df918062":"code","ec9eceff":"code","af30a096":"code","ab69687b":"code","2f954445":"code","d4883d09":"code","5bb8df45":"code","1e785a85":"code","d9b034d4":"code","cd0db072":"code","3b797fd8":"code","29d5df91":"code","677f725e":"code","921cb2dd":"code","a2871e41":"code","827107d5":"code","06c543d6":"code","3bdbc4f0":"code","b282b7cc":"code","ff4572bc":"markdown","bba03305":"markdown","d24e2733":"markdown","10efa806":"markdown"},"source":{"5f290050":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9a45dfa7":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport seaborn as sns\nimport random\nfrom datetime import datetime\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori, association_rules","90f48307":"# Import data\ndf = pd.read_csv(\"..\/input\/BreadBasket_DMS.csv\")\nprint(df.head())\nprint(df.info())\n# Check unique item categories\nprint(\"No of unique item: {}\".format(df['Item'].nunique()))\nprint(\"\\n\")\nprint(\"Top items and counts:\",\"\\n\", df['Item'].value_counts().head(5))","ea228528":"# Drop row of NONE items\ndf.drop(df[df['Item']=='NONE'].index,inplace=True)\n\n# Create month and hour columns\ndf['Month'] = df['Date'].apply(lambda x:x.split(\"-\")[1])\ndf['Hour'] = df['Time'].apply(lambda x:int(str(pd.to_datetime(x).round('H')).split(\" \")[1].split(\":\")[0]))\n\n#Busiest hours\ndf.groupby('Hour')['Transaction'].nunique().plot(figsize=(8,5))\nplt.title(\"Hourly number of transactions\")","eb1e34e6":"#Busiest day of week\n#Get total transaction for each date\nbyday=df.groupby('Date')['Transaction'].nunique().reset_index()\n#Create dayofweek column\nbyday['DayofWeek'] = byday['Date'].apply(lambda x:pd.to_datetime(x).weekday())\n\n#Plot average transactions per day\nbyday.groupby('DayofWeek')['Transaction'].mean().plot(figsize=(8,5))\nplt.title(\"Average number of transactions per weekday\")\nplt.ylabel(\"# of Transactions\")\nfig=plt.xticks(np.arange(7),['Mon','Tue','Wed','Thur','Fri','Sat','Sun'])","192d3a99":"# Busiest months\nbymonth=df.groupby(\"Month\")['Transaction'].nunique().reset_index()\nbymonth['Month'] = ['Jan','Feb','Mar','Apr','Oct','Nov','Dec']\nbymonth['Order'] = [4,5,6,7,1,2,3]\nbymonth.sort_values(by='Order',inplace=True)\nplt.plot(bymonth['Month'],bymonth['Transaction'])\nplt.title(\"Monthly number of transactions\")","734e662d":"# Daily transaction\ndf.groupby(\"Date\")['Transaction'].nunique().plot(figsize=(12,6),label=\"Daily number of transaction\")\ndf.groupby(\"Date\")['Transaction'].size().plot(figsize=(12,6),label=\"Daily number of items sold\")\nplt.legend(loc='best')","d746aaa8":"# Get counts of each item per hour for each months\nbyitem=df.groupby([\"Month\",\"Hour\",'Item']).size().reset_index().sort_values(by=\"Hour\")\nbyitem.rename(columns={0:'Total'},inplace=True)\nbyitem.head()","df918062":"item_toplot = ['Coffee','Bread','Tea','Pastry','Cake']\nfor item in item_toplot:\n    byitem[byitem['Item']==item].groupby(\"Hour\")['Total'].mean().plot(label=item,figsize=(8,5))\n\nplt.legend(loc='best')\nplt.title(\"Average items sold per hour\")","ec9eceff":"df_agg = byitem.groupby(['Month','Hour','Item']).agg({'Total':sum})\ng = df_agg['Total'].groupby(level=[0,1], group_keys=False)\ntopitem_hour=g.nlargest(5).reset_index()\ntopitem_hour.head(10)","af30a096":"# Get the item basket lists for all transactions\nitems = []\nfor i in df['Transaction'].unique():\n    itemlist = list(set(df[df[\"Transaction\"]==i]['Item']))\n    if len(itemlist) > 0:\n        items.append(itemlist)\n    \nprint(items[0:5])\nprint(len(items))","ab69687b":"tran_encoder = TransactionEncoder()\noht_ary = tran_encoder.fit(items).transform(items)\ndataframe = pd.DataFrame(oht_ary, columns=tran_encoder.columns_)\nprint (dataframe.head())           \n \nfrequent_itemsets = apriori(dataframe, use_colnames=True, min_support=0.02)\nprint (frequent_itemsets.head())\n \n#association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.05)\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=0.5)\nprint (rules.head())","2f954445":"# Scatter plot of support vs. confidence\nplt.figure(figsize=(8,5))\nplt.scatter(rules['support'],rules['confidence'],marker='*',edgecolors='grey',s=100,c=rules['lift'],cmap='rainbow')\nplt.colorbar(label='Lift')\nplt.xlabel('support')\nplt.ylabel('confidence') ","d4883d09":"import networkx as nx  \nrules_to_show = len(rules)\nG1 = nx.DiGraph()\n    \ncolor_map=[]\nN = 400\ncolors = np.random.rand(N)    \nstrs=[]\nfor i in range(rules_to_show):\n    strs.append('R'+str(i))\n    \nfor i in range (rules_to_show):      \n    G1.add_nodes_from([\"R\"+str(i)])         \n        \n    for a in rules.iloc[i]['antecedents']:                \n        G1.add_nodes_from([a])        \n        G1.add_edge(a, \"R\"+str(i), color=colors[i] , weight = 1.5)\n        \n    for c in rules.iloc[i]['consequents']:         \n        G1.add_nodes_from([c])\n        G1.add_edge(\"R\"+str(i), c, color=colors[i],  weight=1.5)\n    \nfor node in G1:\n    if node in strs:\n        color_map.append('yellow')\n    else:\n        color_map.append('pink')\n            \nedges = G1.edges()\ncolors = [G1[u][v]['color'] for u,v in edges]\nweights = [G1[u][v]['weight'] for u,v in edges]","5bb8df45":"# modified positions of nodes\n# Get coordinates in circle\ndef PointsInCircum(r,n=20):\n    import math\n    return [math.cos(2*math.pi\/n*x)*r for x in np.arange(0,n+1)], [math.sin(2*math.pi\/n*x)*r for x in np.arange(0,n+1)]\ncir_x,cir_y = PointsInCircum(40,rules_to_show*2)[0],PointsInCircum(40,rules_to_show*2)[1]\n\npos = {}\nindex1 = int(rules_to_show\/4)+1\nindex2 = -(int(rules_to_show\/4)+1)\nfor node in G1:\n    if node not in strs:\n        pos[node]= [cir_x[index1],cir_y[index1]]\n        index1 += 1 \n    else:\n        pos[node]= [cir_x[index2],cir_y[index2]]\n        index2 -=1\nplt.figure(figsize=(12,10))\nnx.draw(G1, pos, edges=edges, node_color = color_map, edge_color=colors, width=weights, font_size=20, with_labels=False)            \n\nfor p in pos:  # raise text positions\n    pos[p][1] += 3\n\nfig=nx.draw_networkx_labels(G1, pos)","1e785a85":"import statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA\n\ntransac = df.groupby(\"Date\")['Transaction'].nunique().reset_index()\ntransac.set_index(\"Date\",inplace=True)\ntransac.index = pd.to_datetime(transac.index)\ntransac['Transaction']=transac['Transaction'].astype(float)\n\ntransac['7-day-SMA'] = transac['Transaction'].rolling(window=7).mean()\ntransac.plot(figsize=(12,6))","d9b034d4":"decomposition = seasonal_decompose(transac['Transaction'], freq=7)  \nfig = plt.figure()  \nfig = decomposition.plot()  \nfig.set_size_inches(15, 8)","cd0db072":"# Store in a function for later use!\ndef adf_check(time_series):\n    \"\"\"\n    Pass in a time series, returns ADF report\n    \"\"\"\n    result = adfuller(time_series)\n    print('Augmented Dickey-Fuller Test:')\n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n\n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n    \n    if result[1] <= 0.05:\n        print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\n    else:\n        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")","3b797fd8":"adf_check(transac['Transaction'])","29d5df91":"transac['First Difference'] = transac['Transaction'] - transac['Transaction'].shift(1)\ntransac['First Difference'].plot()","677f725e":"transac['Seasonal Difference'] = transac['First Difference'] - transac['First Difference'].shift(7)\ntransac['Seasonal Difference'].plot()","921cb2dd":"adf_check(transac['Seasonal Difference'].dropna())","a2871e41":"from pandas.plotting import autocorrelation_plot\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(transac['Seasonal Difference'].iloc[13:], lags=20, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(transac['Seasonal Difference'].iloc[13:], lags=20, ax=ax2)","827107d5":"model = sm.tsa.statespace.SARIMAX(transac['Transaction'],order=(1,0,0), seasonal_order=(1,1,1,7))\nresults = model.fit()\nprint(results.summary())","06c543d6":"transac['forecast'] = results.predict(start = 120, end= 158, dynamic= True)  \ntransac[['Transaction','forecast']].plot(figsize=(12,8))","3bdbc4f0":"from pandas.tseries.offsets import DateOffset\nfuture_dates = [transac.index[-1] + DateOffset(n=x) for x in range(0,100) ]\nfuture_dates_df = pd.DataFrame(index=future_dates[1:],columns=transac.columns)\nfuture_df = pd.concat([transac,future_dates_df])","b282b7cc":"future_df['forecast'] = list(future_df['forecast'][0:159])+list(results.predict(start=158,end=256))\nfuture_df[['Transaction', 'forecast']].plot(figsize=(12, 8)) ","ff4572bc":"## III-Time Series prediction of daily number of transactions <a class=\"anchor\" id=\"III\"><\/a>","bba03305":"### Transactions from a bakery - Market Basket Analysis - Kaggle\n**Data set** <br>\n- Date: 2016-10-30  to 2017-04-09 <br>\n- Time<br>\n- Transaction<br>\n- Item <br>\n\n**Questions to explore** <br>\nCheck out my app visualizing this data set at https:\/\/utn100-marketbasket.herokuapp.com\/ \n<br>\n[I.Data exploratory](#I)\n    - Busiest times of the day are from 9am-3pm, peaks around 10 am\n    - Amount of transactions increases toward the end of week, peaks on Saturday\n    - Among months that have full data (Nov-Mar), November has the highest number of transaction, but in general, there is not significant variation through months.\n    - Examples of most popular items sold are coffee, bread, pastry, and cake\n    \n[II. Market Basket Analysis](#II) <br>\nAnalyzing association of items: market basket analysis. Codes were modified from http:\/\/intelligentonlinetools.com\/blog\/2018\/02\/10\/how-to-create-data-visualization-for-association-rules-in-data-mining\/\n\n[III.Time series prediction of daily number of  transactions](#III)","d24e2733":"## I-Exploratory Data Analysis <a class=\"anchor\" id=\"I\"><\/a>","10efa806":"## II-Market Basket Analysis <a class=\"anchor\" id=\"II\"><\/a>"}}