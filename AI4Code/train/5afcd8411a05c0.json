{"cell_type":{"b0ba0ff9":"code","4271e632":"code","eca0b554":"code","f7635aac":"code","5b5b1204":"code","2bcfa30a":"code","f7654a37":"code","88510831":"code","1de40b7d":"code","2e9d8861":"code","7ff66935":"code","cda2e296":"code","33a376aa":"code","b4722c90":"code","a486fa8d":"code","6a514cc6":"code","a4b43226":"code","b6b0d677":"code","6f324dbc":"code","41f335f5":"code","287b22f1":"code","c59a0edf":"code","f7ec9143":"code","0757a7af":"code","513cef53":"code","1680a4a0":"code","e8c627b8":"code","9f9b5627":"code","df45d56c":"code","3d1809d3":"code","9050a609":"code","d443fb7a":"code","4dcbdf96":"code","9387b36a":"code","0fe70cbf":"code","61646021":"code","fba22490":"code","866e4e47":"code","d043b961":"code","10f855c2":"code","0487144a":"code","9f814184":"code","13ed2fab":"code","a873f569":"code","6f39e285":"code","54f600e5":"code","1c952309":"code","60134cd3":"code","dee5417a":"code","8243f056":"code","efc13332":"code","59266207":"code","68a23eb2":"code","d7127a35":"code","9f785384":"code","4f1801ca":"code","41b5fee3":"code","665f323c":"code","11a4df4f":"code","ed0f96e6":"code","0ef5bdc7":"code","5282c3bb":"code","9aaad65b":"code","003db68c":"markdown","e09d304f":"markdown","e536647c":"markdown","31edca2b":"markdown","352a0bde":"markdown","3cdb4603":"markdown","41cc3749":"markdown","89d368a0":"markdown","27b86f02":"markdown","8aeefff3":"markdown","35a0b9ec":"markdown","ae3ec5df":"markdown","2a555660":"markdown","63317656":"markdown","33c4578d":"markdown","cb8c7478":"markdown","1bd45270":"markdown","54505272":"markdown","8fb74f16":"markdown","dbc7dbf8":"markdown","b7a6d38a":"markdown","d120e768":"markdown","0d80afda":"markdown","5f74a900":"markdown","f47525de":"markdown","a589d02b":"markdown","9ad0ddac":"markdown","bf9d0ca3":"markdown","34d770ef":"markdown","be972f38":"markdown","3f0220f6":"markdown","4c7d5c0a":"markdown","29fabf85":"markdown","d739cd02":"markdown","c00cc4db":"markdown","2c67c804":"markdown","a373975e":"markdown","0248a4a5":"markdown","e92699a5":"markdown","9608c4c1":"markdown","fe19253f":"markdown","de7d22f0":"markdown","8f55aa16":"markdown","f5999376":"markdown","a5bcdc96":"markdown","825bedae":"markdown","33105e2a":"markdown","f135d1c2":"markdown","bdd09d83":"markdown","c564ea4a":"markdown","24e0ada0":"markdown","b84ee2c7":"markdown","fb85b0c0":"markdown","7b7727b9":"markdown","6b5a3939":"markdown","57848a7b":"markdown","91070c6e":"markdown","637dfca8":"markdown","c3037004":"markdown","93896897":"markdown","95c9a4ca":"markdown","1614c528":"markdown","e0eca26f":"markdown","d3a51fdf":"markdown","46a7adba":"markdown","8416a4d9":"markdown","14e111cd":"markdown","aabf131f":"markdown","7655088f":"markdown","e58f1090":"markdown","660f7414":"markdown","00f43e76":"markdown","413994ed":"markdown","8647acec":"markdown","87a6c350":"markdown","b1d6f929":"markdown","3455d25a":"markdown","98ffb1e6":"markdown"},"source":{"b0ba0ff9":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline\n\n# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\n# Seaborn : librairie de visualisation et graphiques tr\u00e8s personalis\u00e9s\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n","4271e632":"from sklearn import model_selection\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import datasets","eca0b554":"import tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential, load_model\n\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\n\nfrom tensorflow.keras.utils import to_categorical","f7635aac":"np.random.seed(1)","5b5b1204":"import cv2\nimport os\nimport glob\nimport gc\n\ndef lire_images(img_dir, xdim, ydim, nmax=5000) :\n    \"\"\" \n    Lit les images dans les sous r\u00e9pertoires de img_dir\n    nmax images lues dans chaque r\u00e9pertoire au maximum\n    Renvoie :\n    X : liste des images lues, matrices xdim*ydim\n    y : liste des labels num\u00e9riques\n    label : nombre de labels\n    label_names : liste des noms des r\u00e9pertoires lus\n    \"\"\"\n    label = 0\n    label_names = []\n    X = []\n    y=[]\n    for dirname in os.listdir(img_dir):\n        print(dirname)\n        label_names.append(dirname)\n        data_path = os.path.join(img_dir + \"\/\" + dirname,'*g')\n        files = glob.glob(data_path)\n        n=0\n        for f1 in files:\n            if n>nmax : break\n            img = cv2.imread(f1) # Lecture de l'image dans le repertoire\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Conversion couleur RGB\n            img = cv2.resize(img, (xdim,ydim)) # Redimensionnement de l'image\n            X.append(np.array(img)) # Conversion en tableau et ajout a la liste des images\n            y.append(label) # Ajout de l'etiquette de l'image a la liste des etiquettes\n            n=n+1\n        print(n,' images lues')\n        label = label+1\n    X = np.array(X)\n    y = np.array(y)\n    gc.collect() # R\u00e9cup\u00e9ration de m\u00e9moire\n    return X,y, label, label_names","2bcfa30a":"def plot_scores(train) :\n    accuracy = train.history['accuracy']\n    val_accuracy = train.history['val_accuracy']\n    epochs = range(len(accuracy))\n    plt.plot(epochs, accuracy, 'b', label='Score apprentissage')\n    plt.plot(epochs, val_accuracy, 'r', label='Score validation')\n    plt.title('Scores')\n    plt.legend()\n    plt.show()","f7654a37":"X,y,Nombre_classes,Classes = lire_images(\"..\/input\/skin-cancer-malignant-vs-benign\/train\", 224, 244, 1200)","88510831":"Nombre_classes","1de40b7d":"Classes","2e9d8861":"X_test,y_test,_,__ = lire_images(\"..\/input\/skin-cancer-malignant-vs-benign\/test\", 224, 244,400) #Nousmettons 400 afin de lire \n#toutes les donn\u00e9es contenues dans le dossier du test","7ff66935":"import random\nplt.figure(figsize=(20,5))\nfor i in range(0,20) :\n    plt.subplot(2,10,i+1)\n    j = random.randint(0,len(X))\n    plt.axis('off')\n    plt.imshow(X[j])\n    plt.title(Classes[y[j]])","cda2e296":"# Normalisation entre 0 et 1\nX = X \/ 255\nX_test = X_test \/ 255","33a376aa":"print(X.shape)\nprint(X_test.shape)","b4722c90":"# ratio du jeu de validation\nratio_val = 0.20\n\n# Subdivision en training et validation set\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=ratio_val,random_state=1,shuffle=True)","a486fa8d":"# Mod\u00e8le CNN plus profond\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3),padding='same', input_shape=(224, 244, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(20, (3, 3),padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(10, (3, 3),padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(5, (3, 3),padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(Nombre_classes, activation='softmax'))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-3), metrics=['accuracy'])","6a514cc6":"model.summary()","a4b43226":"tf.keras.utils.plot_model(\n    model, to_file='model.png', show_shapes=True,\n    show_layer_names=True,\n)","b6b0d677":"# Apprentissage\ntrain = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=35, verbose=1)\n\n# Test\nscores = model.evaluate(X_val, y_val, verbose=0)\nprint(\"Score : %.2f%%\" % (scores[1]*100))","6f324dbc":"plot_scores(train)","41f335f5":"# Prediction\ny_cnn = np.argmax(model.predict(X_test),axis=1)","287b22f1":"plt.figure(figsize=(15,5))\nplt.subplot(1, 2, 1)\nplt.plot(train.history['accuracy'], label='accuracy')\nplt.plot(train.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(train.history['loss'], label='loss')\nplt.plot(train.history['val_loss'], label = 'val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')","c59a0edf":"cm = confusion_matrix(y_test, y_cnn)\nsns.heatmap(cm,annot = True, cmap=\"YlGnBu\")","f7ec9143":"print(classification_report(y_test, y_cnn, target_names = ['Benign (Class 0)','Malignant(Class 1)']))","0757a7af":"plt.figure(figsize=(20,25))\nn_test = X_test.shape[0]\ni= 1 \nfor j in range(0,len(X_test)):\n    if (y_cnn[j] != y_test[j]) & (i<150) :\n        plt.subplot(15,10,i)\n        plt.axis('off')\n        plt.imshow(X_test[j])\n        plt.title('%s \/ %s' % (Classes[y_cnn[j]], Classes[y_test[j]]))\n        i+=1\nprint(f\"nombre d'images mal class\u00e9es:--> {i-1}\\n\")","513cef53":"from tensorflow.keras.applications import VGG16","1680a4a0":"vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(244,224,3))\nvgg16.trainable = False","e8c627b8":"vgg16.summary()","9f9b5627":"model = Sequential()\nmodel.add(vgg16)\nmodel.add(Flatten())\nmodel.add(Dense(Nombre_classes, activation='softmax'))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","df45d56c":"tf.keras.utils.plot_model(\n    model, to_file='model.png', show_shapes=True,\n    show_layer_names=True,\n)","3d1809d3":"model.summary()","9050a609":"train = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=35, verbose=1)\n# Test\nscores = model.evaluate(X_val, y_val, verbose=0)\nprint(\"Score : %.2f%%\" % (scores[1]*100))","d443fb7a":"plot_scores(train)","4dcbdf96":"plt.figure(figsize=(15,5))\nplt.subplot(1, 2, 1)\nplt.plot(train.history['accuracy'], label='accuracy')\nplt.plot(train.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(train.history['loss'], label='loss')\nplt.plot(train.history['val_loss'], label = 'val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')","9387b36a":"# Prediction\ny_cnn = np.argmax(model.predict(X_test),axis=1)","0fe70cbf":"cm = confusion_matrix(y_test, y_cnn)\nsns.heatmap(cm,annot = True, cmap=\"YlGnBu\")","61646021":"print(classification_report(y_test, y_cnn, target_names = ['Benign (Class 0)','Malignant(Class 1)']))","fba22490":"plt.figure(figsize=(20,25))\nn_test = X_test.shape[0]\ni = 1\n\nfor j in range(0,len(X_test)):\n    if (y_cnn[j] != y_test[j]) & (i<150) :\n        plt.subplot(15,10,i)\n        plt.axis('off')\n        plt.imshow(X_test[j])\n        plt.title('%s \/ %s' % (Classes[y_cnn[j]], Classes[y_test[j]]))\n        i+=1\ny_cnn_misclassified = i - 1\nprint(f\"nombre d'images mal class\u00e9es:--> {y_cnn_misclassified}\\n\")","866e4e47":"for i in range (len(vgg16.layers)):\n    print (i,vgg16.layers[i])","d043b961":"for layer in vgg16.layers[0:5]:\n    layer.trainable=False\nfor layer in vgg16.layers[5:10]:\n    layer.trainable=True\nfor layer in vgg16.layers[10:15]:\n    layer.trainable=False\nfor layer in vgg16.layers[15:]:\n    layer.trainable=True","10f855c2":"model = Sequential()\nmodel.add(vgg16)\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\nmodel.add(Dense(Nombre_classes, activation='softmax'))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])","0487144a":"tf.keras.utils.plot_model(\n    model, to_file='model.png', show_shapes=True,\n    show_layer_names=True,\n)","9f814184":"train = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, verbose=1)\n# Test\nscores = model.evaluate(X_val, y_val, verbose=0)\nprint(\"Score : %.2f%%\" % (scores[1]*100))","13ed2fab":"plot_scores(train)","a873f569":"plt.figure(figsize=(15,5))\nplt.subplot(1, 2, 1)\nplt.plot(train.history['accuracy'], label='accuracy')\nplt.plot(train.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(train.history['loss'], label='loss')\nplt.plot(train.history['val_loss'], label = 'val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\n","6f39e285":"# Prediction\ny_cnn = np.argmax(model.predict(X_test),axis=1)","54f600e5":"cm = confusion_matrix(y_test, y_cnn)\nsns.heatmap(cm,annot = True, cmap=\"YlGnBu\")","1c952309":"print(classification_report(y_test, y_cnn, target_names = ['Benign (Class 0)','Malignant(Class 1)']))","60134cd3":"plt.figure(figsize=(30,25))\nn_test = X_test.shape[0]\ni = 1\n\nfor j in range(0,len(X_test)):\n    if (y_cnn[j] != y_test[j]) & (i<100) :\n        plt.subplot(10,10,i)\n        plt.axis('off')\n        plt.imshow(X_test[j])\n        plt.title('%s \/ %s' % (Classes[y_cnn[j]], Classes[y_test[j]]))\n        i+=1\ny_cnn_misclassified = i - 1\nprint(f\"nombre d'images mal class\u00e9es:--> {y_cnn_misclassified}\\n\")","dee5417a":"from tensorflow.keras.applications import ResNet50V2","8243f056":"ResNet50V2 = ResNet50V2(weights='imagenet', include_top=False, input_shape=(244,224,3))\nResNet50V2.trainable = False","efc13332":"ResNet50V2.summary()","59266207":"for i in range (len(ResNet50V2.layers)):\n    print (i,ResNet50V2.layers[i])","68a23eb2":"for layer in ResNet50V2.layers[:180]:\n    layer.trainable=False\nfor layer in ResNet50V2.layers[180:]:\n    layer.trainable=True","d7127a35":"model = Sequential()\nmodel.add(ResNet50V2)\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\nmodel.add(Dense(Nombre_classes, activation='softmax'))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])","9f785384":"model.summary()","4f1801ca":"tf.keras.utils.plot_model(\n    model, to_file='model.png', show_shapes=True,\n    show_layer_names=True,\n)","41b5fee3":"train = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, verbose=1)\n# Test\nscores = model.evaluate(X_val, y_val, verbose=0)\nprint(\"Score : %.2f%%\" % (scores[1]*100))","665f323c":"plot_scores(train)","11a4df4f":"plt.figure(figsize=(15,5))\nplt.subplot(1, 2, 1)\nplt.plot(train.history['accuracy'], label='accuracy')\nplt.plot(train.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(train.history['loss'], label='loss')\nplt.plot(train.history['val_loss'], label = 'val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\n","ed0f96e6":"# Prediction\ny_cnn = np.argmax(model.predict(X_test),axis=1)","0ef5bdc7":"cm = confusion_matrix(y_test, y_cnn)\nsns.heatmap(cm, annot = True, cmap=\"YlGnBu\")","5282c3bb":"print(classification_report(y_test, y_cnn, target_names = ['Benign (Class 0)','Malignant(Class 1)']))","9aaad65b":"plt.figure(figsize=(20,25))\nn_test = X_test.shape[0]\ni = 1\n\nfor j in range(0,len(X_test)):\n    if (y_cnn[j] != y_test[j]) & (i<150) :\n        plt.subplot(15,10,i)\n        plt.axis('off')\n        plt.imshow(X_test[j])\n        plt.title('%s \/ %s' % (Classes[y_cnn[j]], Classes[y_test[j]]))\n        i+=1\ny_cnn_misclassified = i - 1\nprint(f\"nombre d'images mal class\u00e9es:--> {y_cnn_misclassified}\\n\")","003db68c":"## Phase d'apprentissage et de validation sur les training et le validation set","e09d304f":"#### R\u00e9sum\u00e9 de notre mod\u00e8le:","e536647c":"**Conclusion 4:** On a un score de 85% sur le jeu de validation, 99% sur le jeu d'entrainement et un score 85% sur le test set. On fait par contre un sur-apprentissage et on a 104 images qui sont mal class\u00e9es sur 660. Nous gardons toujours l'id\u00e9e que l'erreur de bayes(human error) n'est pas non plus faible.","31edca2b":"## 3. **Personalistion du mod\u00e8le VGG16**","352a0bde":"> **Remarque:** Notre fonction de co\u00fbt ou perte semble diminuer rapidement apr\u00e8s deux it\u00e9rations et cela se traduit \u00e9galement sur la courbe de la performance.\n    Par contre il diminue lentement apr\u00e8s. Ceci est une cons\u00e9quence de la diminution du taux d'apprentissage.","3cdb4603":"#### Importation de la librairie contenant le mod\u00e8le ResNet50V2 et t\u00e9l\u00e9chargement des poids utilis\u00e9 sur imagenet","41cc3749":"   #### **Conclusion 1**:\n       Nous avons un accuracy de 89% sur le training set, 87,7% sur la validation un f1_score de 81%. Cel\u00e0 semble pas mal malgr\u00e9s qu'on fasse un petit surajustement sur le jeu de validation. On a un bon mod\u00e8l qui pourrait n\u00e9amoins continuer \u00e0 \u00e9voluer. On pourra \u00e9ventuellement augmenter le dataset pour rem\u00e9dier au surajustement du validation set.\n       Parmi les images sur lesquuels il fait des erreurs on remarque qu'il y a assez d'ambiguit\u00e9 entre une image repr\u00e9sentant un cancer b\u00e9nin et un cancer malin. Ce qui nous permet de conclure que l'erreur de bayes (human error) n'est pas non plus faible et donc le mod\u00e8le fait un travail raisonnable. Un humain aura besoin de plus de temps, d'exp\u00e9rience et d'examen compl\u00e9mentaire pour faire une distinction raisonable et justifi\u00e9e.","89d368a0":"## Phase d'apprentissage et de validation sur les training et le validation set","27b86f02":"### Framework Tensorflow et Keras inclu\nLe framework keras de tensorflow nous permettra de construire nos mod\u00e8le sans avoir \u00e0 r\u00e9ecrire de multiples fonctions d\u00e9j\u00e0 impl\u00e9ment\u00e9.","8aeefff3":"> **Remarque:** On r\u00e9alise effectivement une parfomance plus raisonnable","35a0b9ec":"## Phase d'apprentissage et de validation sur les training et le validation set","ae3ec5df":"### D\u00e9composition en trois jeu de donn\u00e9es: Training, Validation et Test set\n\n>  **Remarque**: Notre test set \u00e0 bien d\u00e9j\u00e0 \u00e9t\u00e9 d\u00e9finie au dessus.\n> En mettant le param\u00e8tre shuffle \u00e0 **True** nous arivons \u00e0 m\u00e9langer notre jeu de donn\u00e9es avant la subdivision afin d'\u00eare sur d'avoir une bonne r\u00e9partition de nos donn\u00e9es ","2a555660":"## Les images sur lesquels on se trompe:","63317656":"## **Commen\u00e7ons le Skin Cancer Classification**\n\nNous faisons une classification \u00e0 partir de 3600 images de cancer de dimensions 224 X 244","33c4578d":"## Courbe de performance et de la fonction de co\u00fbt ou d'erreur","cb8c7478":"On a un score de 85,62%. On fait un surajustement sur le jeu d'entrainement avec score atteignant les 99% et qui ne cesse de cro\u00eetre tout en s'\u00e9cartant du score de la vaidation. ","1bd45270":"   #### **Conclusion 2**:\n       Nous avons un accuracy de 99% sur le training set, 85,62% sur la validation un f1_score de 85%. Cel\u00e0 semble pas mal malgr\u00e9s qu'on fasse un surajustement sur le jeu d'entrainement. On a un bon mod\u00e8l qui pourrait n\u00e9amoins continuer \u00e0 \u00e9voluer. On pourra \u00e9ventuellement augmenter diminuer le taux d'apprentissage entrainer certains couches pour qu'ils apprennent les caract\u00e9ristiques contenus dans nos images.\n       \n       On remarque \u00e9galement que parmis les images sur lesquels il fait des erreurs il y a assez d'ambiguit\u00e9 entre une image repr\u00e9sentant un cancer b\u00e9nin et un cancer malin, l'erreur de bayes (human error) n'est pas non plus faible et donc le mod\u00e8le fait un travail raisonnable. Un humain aura besoin de plus de temps, d'exp\u00e9rience et d'examen compl\u00e9mentaire pour faire une distinction raisonable et justifi\u00e9e. Il se trompe sur 102 images sur 660.\n       \n       Nous affinons dans la suite ce mod\u00e8le.","54505272":"## Conclusion g\u00e9n\u00e9rale:\n\n>        Des 4 conclusion pr\u00e9cedentes, on conclut que le model VGG16 personalis\u00e9 nous donne une meilleure performance en concid\u00e9rant toutes les m\u00e9triques d'\u00e9valuation.\n>         On a une performance de 83,33% sur le jeu de validation et 88,95% sur le jeu d'entrainement. Sur le test on a une pr\u00e9cision moyenne de 85%, un recall moyen de 85% et un f1_score de 85%. N'oublions pas qu'on a 60 images de plus pour les cancers b\u00e9nins et que l'erreur de bayes n'est pas faible.\n>         Le probl\u00e8me de classification du cancer de la peau est un probl\u00e8me relativement complexe. Nos mod\u00e8le pourraient continuer par apprendre. Voici quelques perspectives d'id\u00e9es:\n> >  * Augmenter le dataset en la synth\u00e9tisant\n> >  * Faire un parameter tuning pour affinier de plus en plus le mod\u00e8le\n> >  * Annoter notre dataset sur makesense.ai et utiliser l'algorithme Yolo afin de d\u00e9tecter directement les zones dans lesquelles la tumeur est d\u00e9tect\u00e9e dans nos images.","8fb74f16":"### V\u00e9rifions les dimensions de nos images et le nombres de training examples et de test exemples dont nous disposons","dbc7dbf8":"# **Mise en place de nos mod\u00e8les et entrainements**","b7a6d38a":"## Phase de Test du mod\u00e8le sur le Test set","d120e768":"## Phase de Test du mod\u00e8le sur le Test set","0d80afda":"Nous avons en tout **2** classes qui seront nos cibles en termes de classification","5f74a900":"#### R\u00e9sum\u00e9 du model","f47525de":"## Matrice de confusion affich\u00e9 avec le heatmap de seaborn","a589d02b":"## Matrice de confusion affich\u00e9 avec le heatmap de seaborn","9ad0ddac":"> **Remarque:** On remarque bien \u00e9videmment que ce sur-apprentissage se traduit sur la courbe de la performance en fonction des \u00e9poques et sur la courbe de perte en fonction des \u00e9poques. la performance ne semble plus constante sur le jeu de validation et croissante sur le jeu d'entrainent. La courbe de perte traduit \u00e9galement ce \u00e9cart.\n\nEn r\u00e9alit\u00e9 le mod\u00e8le n'arrive pas \u00e0 bien g\u00e9n\u00e9raliser.","bf9d0ca3":"## Courbe d'apprentissage","34d770ef":"## Fonction permettant d'afficher les courbes d'apprentissage","be972f38":"## 1. **Convolutional Neural Network CNN model**","3f0220f6":"> **Remarque:** On fait moins d'erreur et on g\u00e9n\u00e9ralise assez bien. Confirmons cette conclusion avec le rapport de classification","4c7d5c0a":"## Phase de Test du mod\u00e8le sur le Test set","29fabf85":"Pour le moment nous n'entrainons pas le model c'est-\u00e0-dire que nous utilisons les poids pr\u00e9-entrain\u00e9s de **imagenet**. Notre dataset \u00e9tant n\u00e9amoins assez grand nous ferons plus tard l'entrainement de certaines couches afin que VGG16 apprenne de nos images et mets \u00e0 jour les poids correspondants.","d739cd02":"## Courbe de performance et de la fonction de co\u00fbt ou d'erreur","c00cc4db":"## T\u00e9l\u00e9chargement du mod\u00e8le avec les poind pr\u00e9-entrain\u00e9s","2c67c804":"> **Petite conclusion:** On a une performance de 83,33% sur le jeu de validation et 88,95% sur le jeu d'entrainement. Sur le test on a une pr\u00e9cision moyenne de 85%, un recall moyen de 85% et un f1_score de 85%. N'oublions pas qu'on a 60 images de plus pour les cancers b\u00e9nins. Ce r\u00e9sultat est un piste d'am\u00e9lioration et pourra faire objet de solution finale.","a373975e":"## Lectures des images qui serviront d'entrainement et de validation pour nos mod\u00e8les","0248a4a5":"Sch\u00e9ma du mod\u00e8le","e92699a5":"## **Apprentissage par transfert**","9608c4c1":"Notre fonction nous permet bien de lire les classes dont feront objets, notre classification:\n1. malignant\n2. benign","fe19253f":"> **Reparque:** Le nombre de cas sur lesquels on se trompe est n\u00e9amoins assez faible","de7d22f0":"### Nous lisons ensuite les images qui servirons pour le test de nos mod\u00e8les\n\n> ***Remarque :*** *Nous subdivisons notre data set en trois parties*\n > > * Training set : Jeu de donn\u00e9es qui servira \u00e0 entrainer le mod\u00e8le.\n > > * Validation set : Jeu de donn\u00e9es qui servira \u00e0 valider le mod\u00e8le afin d'\u00e9viter le sur-apprentissage ou variance \u00e9lev\u00e9e et le sous-apprentissage ou biais \u00e9lev\u00e9 sur le training set afin que le mod\u00e8le puisse g\u00e9n\u00e9raliser.\n > > * Test set : Jeu de donn\u00e9es qui servira \u00e0 evaluer le mod\u00e8le et \u00e0 savoir son taux d'exactitude.\n> *Ce proc\u00e9d\u00e9 de subdivision en trois partie permet d'avoir un mod\u00e8le vraiment fiable et qui nous permet de faire fine tuner le mod\u00e8le*","8f55aa16":"Voici les modifications que nous apportons \u00e0 notre mod\u00e8le:\n>  * Nous n'entrainons pas les 4 premi\u00e8res couches et les couches de 10 \u00e0 14. Aucune modification n'est donc apport\u00e9e \u00e0 leurs poids.\n>  * Nous entrainons les couches de 5 \u00e0 9 et les couches de 10 \u00e0 18\n>  * Notre taux d'apprentissage est aussi diminu\u00e9 \u00e0 1e-4 afin d'\u00e9viter le surapprentissage. Nous entrainons le mod\u00e8le cette fois ci sur moins d'\u00e9poque(ou it\u00e9rations) \u00e9galement; 20 \u00e9poques.","f5999376":"# **Skin Cancer Classification Using Tensorflow**\n**Authors:** *GAGLOZOU Komlan Petro Mighty & DJOGDOM TCHAPPI Ince Loic*","a5bcdc96":"### Afin de permettre la reproductibilit\u00e9","825bedae":"## Phase d'apprentissage et de validation sur les training et le validation set","33105e2a":"### Biblioth\u00e8ques divers de python pour la visualisation et l'analyse des donn\u00e9es","f135d1c2":"### Biblioth\u00e8ques python de machine learning\n\nNous utiliserons **scikit-learn** nous permettant dans notre cas de subdiviser notre jeu de donn\u00e9e et d'\u00e9valuer nos mod\u00e8les","bdd09d83":"## Matrice de confusion affich\u00e9 avec le heatmap de seaborn","c564ea4a":"## Courbe d'apprentissage\n\n        On a une performance de 83,33% sur le jeu de validation et 88,95% sur le jeu d'entrainement.Cet \u00e9cart s'est visiblement r\u00e9duit \u00e0 cause des modifications apport\u00e9 (\u00e9num\u00e9r\u00e9e un peu plus au dessus). La performance semble croitre au fil des it\u00e9rations. On constate n\u00e9amoins du bruit. L'hypoth\u00e8se serait qu'il soit possible qu'il rencontre des images difficile \u00e0 apprendre et en m\u00eame temps \u00e0 pr\u00e9dire","24e0ada0":"#### Sch\u00e9ma de notre mod\u00e8le:","b84ee2c7":"### Normalisation entre 0 et 1 afin de permettre une rapidit\u00e9 de calcul. Nos images \u00e9tant sur une \u00e9chelle 0 \u00e0 255","fb85b0c0":"## Fonction permettant la lecture de nos images","7b7727b9":"> ***Remarque: Dans ce mod\u00e8le nous utilisons un taux d'apprentissage de 1e-4, nous rajoutons un dropout 30% pour \u00e9viter le surajustement car ce mod\u00e8le est consid\u00e9rablement profond. Nous activons les couches de 180 \u00e0 189 afin qu'elles puissent apprendre des caract\u00e9ristiques de nos images de cancer. elles seront entrain\u00e9 sur 10 it\u00e9rations.***","6b5a3939":"## 4. **Personalistion et tranfert learning avec le mod\u00e8le ResNet50V2** \n\nLe mod\u00e8le ResNet50v2 est bas\u00e9 sur les r\u00e9seaux r\u00e9siduels([*Deep Residual Learning for Image Recognition He et al, 2015*](https:\/\/arxiv.org\/abs\/1512.03385)).\n        \n       Ce mod\u00e8le est 8 \u00e0 9 fois plus profond que le mod\u00e8le VGG. Il comporte 189 couches et a montr\u00e9 de meilleurs r\u00e9sultat que VGG16 sur le dataset Imagenet.\n       \n![Resnet Architecture](http:\/\/i.stack.imgur.com\/gI4zT.png)\n\n\nCommen\u00e7ons...!","57848a7b":"Nous adaptons le mod\u00e8le sans toucher aux poids de d\u00e9part. Le mod\u00e8le \u00e0 \u00e9t\u00e9 entrain\u00e9 pour la reconnaissance de 1000 classes et contient 1,281,167 training images, 50,000 validation images et 100,000 test images","91070c6e":"## **2. mod\u00e8le VGG16 sans personalisation**","637dfca8":"## **Les images sur lesquels on se trompe:**","c3037004":"## **Importations des diff\u00e9rentes biblioth\u00e8ques**","93896897":"## Courbe de performance et de la fonction de co\u00fbt ou d'erreur","95c9a4ca":"> **Remarque:** Nous ajoutons une couche Dropout toujours afin d'\u00e9viter le surapprentissage","1614c528":"### Affichons 20 images prises al\u00e9atoirement dans notre training set","e0eca26f":"## **Les images sur lesquels on se trompe:**","d3a51fdf":"Les neurones convolutifs sont bas\u00e9es sur des convolutions matriciels. \n* Nous avons opt\u00e9s pour des convolutions ayant des filters 3X3. Notre input sera 224X244X3 car nos images sont RGB.\n* Nous utilisons des dropouts(0.20) afin que les couches sur lesquels on les appliques aient 20% de leurs neurones \u00e9teints ou qu'ils oublis 20% des donn\u00e9es apprissent \n* On utilisera l'optimiseur Adam qui fait un tr\u00e8s bon travail, avec un taux d'apprentissage de 0.001(ce taux d'apprentissage a \u00e9t\u00e9 bien pens\u00e9 et vari\u00e9. Par contre un taux de 0,001 fait un tr\u00e8s bon travail tout en \u00e9vitant le surapprentissage\n* l'utilisation d'un padding same Nous permettra de ne pas pas perdre des informations sur les bords ","46a7adba":"## Courbe d'apprentissage\n\n      On a un score de 85% sur le jeu de validation et 99% sur le jeu d'entrainement. On fait bien \u00e9videmment un sur-apprentissage. Cet \u00e9cart est traduit par la courbe ci-apr\u00e8s. Cel\u00e0 \u00e9tait tout \u00e0 fait pr\u00e9visible vu la profondeur de la couche.","8416a4d9":"### Affichage des couches","14e111cd":"### Quelques diff\u00e9rentes fonctions dont nous aurons besoin\n\n   * Fonction de lecture de nos images \u00e0 partir de nos sous r\u00e9pertoires contenants nos donn\u00e9es  images\n   * Fonction d'affichage de la courbe d'apprentissage sur nos diff\u00e9rents set","aabf131f":"## Courbe de performance et de la fonction de co\u00fbt ou d'erreur","7655088f":"Nous lisons pr\u00e8s de 1200 images pour chaque classes","e58f1090":"> ***Remarque: Bien \u00e9videmment le r\u00e9sultat du sur-apprentissage se traduit sur les courbes ci-dessus***","660f7414":"#### Affichage des couches","00f43e76":"## Phase de Test du mod\u00e8le sur le Test set","413994ed":"## Courbe d'apprentissage","8647acec":"## **Qu'est ce que le cancer de la peau**\n![Exemple de Cancer de la peau](https:\/\/img.passeportsante.net\/1000x526\/2021-05-03\/i104763-cancer-peau-pm.webp \"Cancer de la peau\")\n\n\nOn peut diviser les cancers de la peau en 2 cat\u00e9gories principales : les non-m\u00e9lanomes et les m\u00e9lanomes.\n* **Les non-m\u00e9lanomes ou carcinomes**\nLe terme \u00ab **carcinome** \u00bb d\u00e9signe **les tumeurs malignes** d\u2019origine \u00e9pith\u00e9liale (l'\u00e9pith\u00e9lium est la structure histologique constitutive de la peau et de certaines muqueuses).\n\n* Les m\u00e9lanomes :\nOn donne le nom de **m\u00e9lanomes** aux **tumeurs malignes** qui se forment dans les m\u00e9lanocytes, les cellules qui produisent la m\u00e9lanine (un pigment) et qui se trouvent notamment dans la peau et les yeux. Ils se manifestent habituellement par une tache noir\u00e2tre.\n\n## **Ce qui peut causer le cancer de la peau ?**\n\nL'exposition aux rayons ultraviolets du soleil est la principale cause de cancer de la peau.\n\n## **Comment le diagnostiquer ?**\n* **Dermoscopie**: \n    Il s'agit d'un examen avec une sorte de loupe appel\u00e9e dermoscope, qui permet de voir la structure des l\u00e9sions cutan\u00e9es et d'affiner leur diagnostic.\n* **Biopsie:**\n    Si le m\u00e9decin soup\u00e7onne un cancer, il pr\u00e9l\u00e8ve un \u00e9chantillon de peau \u00e0 l\u2019endroit de la manifestation suspecte, dans le but de la soumettre \u00e0 une analyse de laboratoire. Cela lui permettra de savoir si les tissus sont bel et bien canc\u00e9reux et cela lui donnera une id\u00e9e de l\u2019\u00e9tat de progression de la maladie.\n* **Autres tests:**\n    Si la biopsie r\u00e9v\u00e8le que le sujet est atteint de cancer, le m\u00e9decin demandera d\u2019autres tests pour \u00e9valuer de fa\u00e7on plus approfondie le stade de progression de la maladie. Les tests permettent de savoir si le cancer est encore circonscrit localement ou s\u2019il a commenc\u00e9 \u00e0 se r\u00e9pandre \u00e0 l\u2019ext\u00e9rieur des tissus cutan\u00e9s.\n    \n* **Limites de ces diagnostiques:**\n    Les risques associ\u00e9s \u00e0 la biopsie varient en fonction de la technique utilis\u00e9e. Chez certains patients atteints d'un cancer, la tumeur peut augmenter et les cellules canc\u00e9reuses peuvent envahir les structures voisines. Il existe \u00e9galement un risque de saignement, d'infection et d'atteinte nerveuse.\n    Co\u00fbt en temps d'analyse et de r\u00e9sultat.\n    \n***Ce type de cancer \u00e9tant un cancer qui \u00e9volue assez rapidement, il faudrait une m\u00e9thode de dianostic sans risque \u00e9norme et dont le r\u00e9sultat qui ne prenne pas du temps.***\n\n**Id\u00e9e de solution: Diagnostiquer le cancer de la peau en utilisant des r\u00e9seaux de neurones**\n**Pour ce faire, nous construirons un mod\u00e8le bas\u00e9 sur les R\u00e9seaux de Neurones Convolutionnels et nous personnaliserons deux mod\u00e8les pr\u00e9-entrain\u00e9s afin de faire une apprentissage par transfert:**\n#### 1. *Convolutional Neural Network CNN model*\n#### 2. *Mod\u00e8le VGG16 sans aucune personalisation*\n#### 3. *Mod\u00e8le VGG16 personalis\u00e9*\n#### 4. *Mod\u00e8le ResNet50v2 personalis\u00e9*","87a6c350":"## **Les images sur lesquels on se trompe:**","b1d6f929":"On a un score qui atteint parfois 87,71%. En consid\u00e9rant la courbe ci-apr\u00e8s on aurait tendence \u00e0 \u00eatre de plus en plus performant au fil des it\u00e9ration(\u00e9poques) mais nous avons r\u00e9alis\u00e9s des exp\u00e9riences et nous constatons le contraire. Il s'av\u00e8re que 35 \u00e9poques est un point assez culminant. Il traine vraiment \u00e0 faire mieux et le cas \u00e9ch\u00e9ant sa performance diminue. Ce qu'on constate de bien c'est qu'on arrive assez bien \u00e0 g\u00e9n\u00e9raliser","3455d25a":"**Conclusion 3:**\n\n        On a une performance de 83,33% sur le jeu de validation et 88,95% sur le jeu d'entrainement. Sur le test on a une pr\u00e9cision moyenne de 85%, un recall moyen de 85% et un f1_score de 85%. N'oublions pas qu'on a 60 images de plus pour les cancers b\u00e9nins. Ce r\u00e9sultat est un piste d'am\u00e9lioration et pourra faire objet de solutin finale. Nous \u00e9vitons de peu le surapprentissage et nous avons un score qui est bon.\n        \n        On remarque toujours une ambiguit\u00e9 entre une image repr\u00e9sentant un cancer b\u00e9nin et un cancer malin, l'erreur de bayes (human error) n'est pas non plus faible et donc le mod\u00e8le fait un travail raisonnable. Un humain aura besoin de plus de temps, d'exp\u00e9rience et d'examen compl\u00e9mentaire pour faire une distinction raisonable et justifi\u00e9e. Il ne se trompe que sur 98 images sur 660.","98ffb1e6":"## Matrice de confusion affich\u00e9 avec le heatmap de seaborn"}}