{"cell_type":{"ac949bf9":"code","da038230":"code","6122e4d4":"code","a910e15e":"code","c9074307":"code","ff349a71":"code","7dcaf36e":"code","cb2b4434":"code","aec0f830":"code","740d9997":"code","a21436ef":"code","281f0362":"code","3c1729d2":"code","ef8101dc":"code","1ba332f2":"code","70723e67":"code","cc71a7eb":"code","f42fbedd":"code","def22de1":"markdown","0aa3191b":"markdown","b70a5196":"markdown","10a71434":"markdown","e71b02db":"markdown","ccc4c5b9":"markdown","94b27ce8":"markdown","b42e2caa":"markdown","7bb8589d":"markdown","e1afcacd":"markdown","28dd8f7d":"markdown","ddd78d08":"markdown","0b264df6":"markdown","e739b4c0":"markdown","fbb9f0b1":"markdown","efea5127":"markdown","d969cf81":"markdown","0d54ac12":"markdown","5afbaedd":"markdown","2c44f33d":"markdown","36da628b":"markdown","6b8208a7":"markdown","3edcc02f":"markdown","18a42bd4":"markdown","510758ec":"markdown"},"source":{"ac949bf9":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nsns.set_style('white')\nsns.set(font_scale=2)","da038230":"df_train = pd.read_excel(\"\/kaggle\/input\/Concrete_Data.xls\")\nprint(df_train.shape)\ndisplay(df_train.head())","6122e4d4":"print(df_train.info())","a910e15e":"display(df_train.describe())","c9074307":"cols = df_train.columns\ncolor = ['dimgray', 'khaki', 'mediumorchid','cornflowerblue', 'crimson','orangered', 'navy', 'salmon']\nsns.set(font_scale=1)\n\nsns.jointplot(data=df_train, x=cols[0], y=cols[-1]\n                  ,kind='reg',color=color[0])\nplt.show()","ff349a71":"sns.jointplot(data=df_train, x=cols[1], y=cols[-1]\n                  ,kind='kde',color=color[1])\nplt.show()","7dcaf36e":"sns.jointplot(data=df_train, x=cols[2], y=cols[-1]\n                  ,kind='kde',color=color[2])\nplt.show()","cb2b4434":"sns.jointplot(data=df_train, x=cols[3], y=cols[-1]\n                  ,kind='reg',color=color[3])\nplt.show()","aec0f830":"sns.jointplot(data=df_train, x=cols[4], y=cols[-1]\n                  ,kind='reg',color=color[4])\nplt.show()","740d9997":"sns.jointplot(data=df_train, x=cols[5], y=cols[-1]\n                  ,kind='kde',color=color[5])\nplt.show()","a21436ef":"sns.jointplot(data=df_train, x=cols[6], y=cols[-1]\n                  ,kind='kde',color=color[6])\nplt.show()","281f0362":"sns.jointplot(data=df_train, x=cols[7], y=cols[-1]\n                  ,kind='kde',color=color[7])\nplt.show()","3c1729d2":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX = pd.DataFrame(columns=cols[:-1], data=sc.fit_transform(df_train.drop(cols[-1],axis=1)))\ndisplay(X.head(3))\ny = df_train[cols[-1]]\ndisplay(y.head(3))","ef8101dc":"sns.set(font_scale=3)\ncols = X.columns\nn_row = len(cols)\nn_col = 2\nn_sub = 1\nfig = plt.figure(figsize=(20,40))\nfor i in range(len(cols)):\n    plt.subplots_adjust(left=-0.3, right=1.3, bottom=-0.3, top=1.3)\n    plt.subplot(n_row, n_col, n_sub)\n    sns.distplot(X[cols[i]],norm_hist=False,kde=False, color=color[i],\n                 label=['mean '+str('{:.2f}'.format(X.iloc[:,i].mean()))\n                        +'\\n''std '+str('{:.2f}'.format(X.iloc[:,i].std()))\n                        +'\\n''min '+str('{:.2f}'.format(X.iloc[:,i].min()))\n                        +'\\n''max '+str('{:.2f}'.format(X.iloc[:,i].max()))])                                                        \n    n_sub+=1\n    plt.legend()\nplt.show()","1ba332f2":"from sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom time import time\n\ndef test_models(mlds):\n    for i in range(len(mlds)):\n        r2 = []\n        mae = []\n        model = mlds[i]\n        n= 0 \n        for tr, te in KFold(n_splits=5,random_state=42, shuffle=True).split(X, y):\n            st_time = time()\n            X_tr = X.iloc[tr, :]\n            y_tr = y.iloc[tr]\n            X_val = X.iloc[te, :]\n            y_val = y.iloc[te]\n            model.fit(X_tr, y_tr)\n            y_preds = model.predict(X_val)\n            r2.append(r2_score(y_val, y_preds))\n            mae.append(mean_absolute_error(y_val, y_preds))\n            en_time = time()\n            print('Time:',str(en_time-st_time),'Fold:',str(n),'r2:',str(r2[n]),'mae:',str(mae[n]))\n            n+=1\n        print('mean_r2', np.mean(r2))\n        print('-----------------------------')\n\n    \nseed = 42\nmodels = [LinearRegression(),RandomForestRegressor(random_state=seed, n_jobs=-1),\n          XGBRegressor(random_state=seed, n_jobs=-1),LGBMRegressor(random_state=seed,n_jobs=-1)]\ntest_models(models)","70723e67":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef create_model(hid_layers,num_cols, drop_rate):\n    inp = layers.Input(shape=(num_cols,))\n    x = layers.BatchNormalization()(inp)\n    for i, units in enumerate(hid_layers):\n        x= layers.Dense(units, 'relu')(x)\n        x = layers.Dropout(drop_rate)(x)\n        x = layers.BatchNormalization()(x)\n\n    output = layers.Dense(1, 'linear')(x)\n    \n    model = tf.keras.models.Model(inputs=inp,outputs=output)\n    model.compile(optimizer='adam', loss='mae')\n    return model\n\nhid_layers = [300,200,100]\nmodel = create_model(hid_layers, X.shape[1], 0.2)\n","cc71a7eb":"\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\ndef callbacks():\n    rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 0, \n                                min_delta = 1e-4, min_lr = 1e-6, mode = 'min')\n        \n    ckp = ModelCheckpoint(f'bests_weights.hdf5', monitor = 'val_loss', verbose = 0, \n                              save_best_only = True, save_weights_only = True, mode = 'min')\n        \n    es = EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 15, mode = 'min', \n                           baseline = None, restore_best_weights = True, verbose = 0)\n    return [rlr, ckp, es]","f42fbedd":"import tensorflow.keras.backend as K\nloss_mae = []\nr2_scores = []\n\nfold = 0\nfor tr, te in KFold(n_splits=5, random_state=42, shuffle=True).split(X,y):\n    X_tr = X.iloc[tr, :]\n    X_val = X.iloc[te, :]\n    y_tr = y.iloc[tr]\n    y_val = y.iloc[te]\n    history = model.fit(X_tr, y_tr, validation_data=(X_val, y_val), callbacks=callbacks(),\n                        epochs=300, verbose=0)\n    model.load_weights('bests_weights.hdf5')\n    y_preds = model.predict(X_val)\n    loss_mae.append(mean_absolute_error(y_val, y_preds))\n    r2_scores.append(r2_score(y_val, y_preds))\n    print(f'fold',str(fold)+':','mae:',loss_mae[fold],'r2_score:',r2_scores[fold])\n    K.clear_session()\n    fold+=1\nprint(\"mae: %0.2f (+\/- %0.2f)\" % (np.mean(loss_mae), np.std(loss_mae) * 2),'mean_r2:', np.mean(r2_scores))\n\n","def22de1":"<a id=\"32\"><\/a>\n## Neural network\nAnd here, we train a neural network.","0aa3191b":"<a id=\"22\"><\/a>\n\n## Data types","b70a5196":"Here we visualize the features distribution after normalization.","10a71434":"<a id=\"248\"><\/a>\n\n### Age ","e71b02db":"<a id=\"4\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Evaluation and Summary<\/center><\/h2>","ccc4c5b9":"<a id=\"247\"><\/a>\n\n### Fine Aggregate ","94b27ce8":"<a id=\"24\"><\/a>\n\n## Features\n\nNext, we will plot the features distribution and it's relationship with the target variable.","b42e2caa":"<a id=\"1\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Problem Description<\/center><\/h2>\n    \nConcrete is the most used material for construction in the world! There are some components that should be combined to make the concrete. These components can affect the compressive strength of the concrete. To obtain the real compressive strength of concrete (target labels in the dataset), an engineer needs to break the cylinder samples under the compression-testing machine. The failure load is divided by the cylinder's cross-section to obtain the compressive strength. Engineers use different kinds of concretes for different building purposes. For example, the strength of concrete used for residential buildings should not be lower than 2500 psi (17.2 MPa). Concrete is a material with high strength in compression, but low strength in tension. That is why engineers use reinforced concrete (usually with steel rebars) to build structures. Let's see if we can find a linear or non-linear relationship between concrete's components and its compressive strength. The building components in this dataset are Cement, Blast Furnace Slag, Fly Ash, Water, Superplasticizer,Coarse Aggregate, and Fine Aggregate. We have also the information of the age, which is an important factor.\n","7bb8589d":"<a id=\"31\"><\/a>\n\n## Scikit-learn models\nWe will train four different models of linear regression, random forest, Xgboost, and lightgbm. We use KFold with 5 splits to perform cross-validation. Mean absolute error and r2_score are used to evaluate the models.","e1afcacd":"<a id=\"3\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Training<\/center><\/h2> ","28dd8f7d":"<h1><center>Concrete Compressive Strength Prediction: Data analysis, visualization, and modeling<\/center><\/h1>\n<center><img src=\"https:\/\/images.unsplash.com\/photo-1519301697757-523109cfc623?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=792&q=80\" width=50%><\/center>\n","ddd78d08":"<a id=\"21\"><\/a>\n\n## Data examples","0b264df6":"<a id=\"242\"><\/a>\n\n### Blast Furnace Slag","e739b4c0":"<a id=\"5\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>References<\/center><\/h2>","fbb9f0b1":"<a id=\"244\"><\/a>\n\n### Water","efea5127":"We can see all columns have non-null values which means there are no missing values. All the columns are float data types except the age column which is integer.","d969cf81":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h2>\n\n    \n    \n* [Problem Description](#1)\n* [Explanatory Data Analysis (EDA)](#2)\n    - [Data examples](#21)\n    - [Data types](#22)\n    - [Statistics](#23)\n    - [Features](#24)\n        - [Cement](#241)\n        - [Blast Furnace Slag](#242)\n        - [Fly ash](#243)\n        - [Water](#244)\n        - [Superplasticizer](#245)\n        - [Coarse aggregate](#246)\n        - [Fine aggregate](#247)\n        - [Age](#248)\n    - [Preprocessing](#25)\n* [Training](#3)\n    - [Scikit-learn models](#31)\n    - [Neural network](#32)\n* [Evaluation and Summary](#4)\n* [References](#5)\n","0d54ac12":"<a id=\"246\"><\/a>\n\n### Coarse Aggregate ","5afbaedd":"<a id=\"243\"><\/a>\n\n### Fly ash","2c44f33d":"<a id=\"241\"><\/a>\n\n### Cement","36da628b":"<a id=\"245\"><\/a>\n\n### Superplasticizer","6b8208a7":"# To be continued...","3edcc02f":"<a id=\"23\"><\/a>\n\n## Statistics","18a42bd4":"<a id=\"25\"><\/a>\n\n## Preprocessing\n\nWe will normalize our features using standardscaler which sets the mean to zero and standard deviation to 1.","510758ec":"<a id=\"2\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Explanatory Data Analysis (EDA)<\/center><\/h2>"}}