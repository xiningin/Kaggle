{"cell_type":{"6826b82a":"code","331e6e6b":"code","3be137c1":"code","fc5a484d":"code","68280c98":"code","a70c65e2":"code","0ea0ec32":"code","d4c72c7e":"code","f60f198a":"code","7363c5d8":"code","ea25503e":"code","55593b9b":"code","37549e3d":"code","d74bc538":"code","06aa254d":"code","abd14731":"code","6d9c32de":"code","06013997":"code","7700bcba":"code","49b8d0ac":"code","4eaaf24a":"code","2626e676":"code","17173322":"code","112eeb50":"code","c6965840":"code","83cefb2d":"code","01887d33":"code","db4d8ba6":"code","234299f8":"code","a541f9fc":"code","beb0bce3":"code","c98df0cd":"code","977662fe":"code","89ecd0f7":"code","fe23d443":"code","a9c2821c":"code","f69dea36":"code","f3e5e4d9":"code","b0191d4e":"code","97e1752f":"code","13573f00":"code","9e560b3a":"code","419cfd5b":"code","6f4521ef":"code","ea0e6b68":"code","46720af5":"code","19359a1c":"code","0abb8d06":"code","e60a6423":"code","aa06d11c":"code","3c4a5480":"code","3e70f4a5":"code","db2bad8a":"code","f2d6dc10":"code","063eab51":"code","2a25bec2":"code","2e4d4f99":"code","6f8fb063":"code","091f3329":"code","9a281d66":"code","ec4ef3c6":"code","4cc26938":"code","09c0caf8":"code","e32a66f0":"code","7109ef0f":"code","9b0ae4d9":"code","9454e7f5":"code","a4c2315b":"code","ec307b91":"code","01d32348":"code","71014564":"code","d1044543":"code","261b9470":"code","9d5ca14a":"code","7b160561":"code","2111cd76":"markdown","52de8071":"markdown","b0bbb792":"markdown","e5401bc4":"markdown","9a2c34cf":"markdown","9e8125ad":"markdown","a09b242d":"markdown","d856b245":"markdown","b9bd309a":"markdown","c1f82ae7":"markdown","4726d6b2":"markdown","80ae0a0e":"markdown","81088caa":"markdown","fe771ab4":"markdown","c27648af":"markdown","8d2d17f3":"markdown","4b21f03f":"markdown","57621495":"markdown","bb814b91":"markdown","d410b412":"markdown","0a891467":"markdown","3a46cd91":"markdown","b96a1f21":"markdown","0dcd78af":"markdown","bf955945":"markdown","1465ac88":"markdown","99a75b7d":"markdown","d276243b":"markdown","5f65418b":"markdown","97310eb1":"markdown","a0543410":"markdown","eebba934":"markdown","905b4a61":"markdown","b1567add":"markdown","6fe800b9":"markdown","6b4890ef":"markdown","9b4e0708":"markdown","93c09a5b":"markdown","3fd2f214":"markdown","f882d5ad":"markdown","b740a036":"markdown","6fc31ce4":"markdown","cc5a6880":"markdown","26d02e6e":"markdown","88b70556":"markdown","849ee59f":"markdown","07a1f106":"markdown","d6c3f8d7":"markdown","27898c84":"markdown","5ba19e94":"markdown","ddeb26a1":"markdown","cfc06680":"markdown","f55dc8a1":"markdown","d2da1844":"markdown","53b0caf6":"markdown","e1bbe0a9":"markdown","77465a62":"markdown","b3cff8d1":"markdown","d7a37679":"markdown","f63a4cab":"markdown","e2a2d590":"markdown","523f7894":"markdown","f21d681a":"markdown","fea7c737":"markdown","9e146a97":"markdown","efe95be7":"markdown","b82d7a83":"markdown","d0752ef3":"markdown","a8d9d57c":"markdown","ff0fff39":"markdown","22078dd0":"markdown","db60b125":"markdown","9f767d26":"markdown","9e35780c":"markdown","506936cf":"markdown","5885e50a":"markdown","96475a68":"markdown","7e373157":"markdown","aa791ee3":"markdown","4d373cdd":"markdown","4e7d9c36":"markdown","438fde66":"markdown","2c30280b":"markdown","97071f72":"markdown","6b9eca3e":"markdown","761fcb6c":"markdown","1da5a797":"markdown"},"source":{"6826b82a":"import pandas as pd # for dataframes\nimport numpy as np # for arrays & math functions\n\n%matplotlib inline\nimport matplotlib.pyplot as plt # for plotting\n\nimport warnings\nwarnings.filterwarnings('ignore') # ignoring any warnings","331e6e6b":"PATH = '..\/input\/tmdb-movie-metadata\/tmdb_5000_movies.csv'\n\nmovies_df = pd.read_csv(PATH) # load data into a pandas dataframe","3be137c1":"SEED = 2020 # for reproducability\n\nmovies_df.sample(3,random_state=SEED)","fc5a484d":"movies_df.info()","68280c98":"movies_df['popularity'].describe()","a70c65e2":"movies_df.sort_values(by='popularity',ascending=False)[:5]","0ea0ec32":"movies_df.sort_values(by='popularity',ascending=True)[:5]","d4c72c7e":"movies_df.sort_values(by='vote_average', ascending = False)[:5]","f60f198a":"movies_df['original_language'].value_counts()","7363c5d8":"### Pie Chart\n\nlabels = np.array(['English','Other'])\nsizes = np.array([4505, sum(movies_df['original_language'].value_counts()) - 4505])\n\nplt.figure(figsize=(8,9))\n\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', explode=[0,0.08], startangle=90)\nplt.title('Original Languages of Movies', fontdict={'fontsize': 14})\nplt.axis('equal')","ea25503e":"features_df = movies_df[['budget','genres','original_language','runtime','vote_average','vote_count']]\nlabels_df = movies_df['popularity']","55593b9b":"features_df = features_df.dropna()","37549e3d":"features_df = features_df[features_df['vote_count'] >= 10]","d74bc538":"features_df = features_df[features_df['runtime'] != 0.0]","06aa254d":"labels_df = labels_df[features_df.index]","abd14731":"features_df.describe()","6d9c32de":"from sklearn.model_selection import train_test_split\n\n# split data into 80% training and 20% testing\nx_train, x_test, y_train, y_test = train_test_split(features_df, labels_df, test_size=0.2, random_state = SEED)","06013997":"# Plot histogram\nplt.figure(figsize=(10,8))\nplt.hist(y_train.values,bins=50)\nplt.title('Movies popularity histogram')\nplt.xlabel('Popularity')\nplt.ylabel('# of movies')\nplt.show()","7700bcba":"y_train.skew()","49b8d0ac":"y_train = np.log(y_train)\ny_test = np.log(y_test)","4eaaf24a":"# Plot histogram\nplt.figure(figsize=(10,8))\nplt.hist(y_train.values,bins=50)\nplt.title('Movies popularity histogram')\nplt.xlabel('Popularity')\nplt.ylabel('# of movies')\nplt.show()","2626e676":"print('Popularity skew:', y_train.skew())","17173322":"### Plot histograms\nplt.rcParams['figure.figsize'] = 12, 12\nfig, axs = plt.subplots(2,2)\nfig.suptitle('Numerical Feature Histograms',y=0.95,fontsize=16)\n\naxs[0,1].hist(x_train['budget'].values,bins=30,color='salmon')\naxs[0,1].set_title('Budget')\naxs[0,1].set(xlabel='US dollars')\naxs[0,0].hist(x_train['runtime'].values,bins=30,color='salmon')\naxs[0,0].set_title('Runtime (min)')\naxs[0,0].set(xlabel='Minutes')\naxs[1,0].hist(x_train['vote_average'].values,bins=30,color='salmon')\naxs[1,0].set_title('Vote Average')\naxs[1,1].hist(x_train['vote_count'].values,bins=30,color='salmon')\naxs[1,1].set_title('Vote Count')\nplt.show()","112eeb50":"print('Vote count skew:', x_train['vote_count'].skew())\nprint('Vote average skew:', x_train['vote_average'].skew())\nprint('Runtime skew:', x_train['runtime'].skew())\nprint('Budget skew:', x_train['budget'].skew())","c6965840":"x_train['vote_count'] = np.log(x_train['vote_count'].values)\nx_train['budget'] = np.sqrt(x_train['budget'].values)\n\nx_test['vote_count'] = np.log(x_test['vote_count'].values)\nx_test['budget'] = np.sqrt(x_test['budget'].values)","83cefb2d":"### Histograms\n\nplt.rcParams['figure.figsize'] = 12, 12\nfig, axs = plt.subplots(2,2)\nfig.suptitle('Numerical Feature Histograms',y=0.95,fontsize=16)\n\naxs[0,1].hist(x_train['budget'].values,bins=30,color='salmon')\naxs[0,1].set_title('Budget')\naxs[0,1].set(xlabel='US dollars')\naxs[0,0].hist(x_train['runtime'].values,bins=30,color='salmon')\naxs[0,0].set_title('Runtime (min)')\naxs[0,0].set(xlabel='Minutes')\naxs[1,0].hist(x_train['vote_average'].values,bins=30,color='salmon')\naxs[1,0].set_title('Vote Average')\naxs[1,1].hist(x_train['vote_count'].values,bins=30,color='salmon')\naxs[1,1].set_title('Vote Count')\nplt.show()","01887d33":"print(x_train['vote_count'].skew())\nprint(x_train['vote_average'].skew())\nprint(x_train['runtime'].skew())\nprint(x_train['budget'].skew())","db4d8ba6":"# Scatterplots\n\nplt.figure(figsize=(12,12))\n\nfig, axs = plt.subplots(2,2)\nfig.suptitle('Correlation to target',y=0.95,fontsize=16)\naxs[0,1].scatter(x_train['vote_average'].values, y_train.values,color='green')\naxs[0,1].set(xlabel='Vote Average',ylabel='Populariy')\naxs[0,0].scatter(x_train['budget'].values, y_train.values,color='green')\naxs[0,0].set(xlabel='Budget (US dollars)',ylabel='Populariy')\naxs[1,0].scatter(x_train['vote_count'].values, y_train.values,color='green')\naxs[1,0].set(xlabel='Vote Count',ylabel='Populariy')\naxs[1,1].scatter(x_train['runtime'].values, y_train.values,color='green')\naxs[1,1].set(xlabel='Runtime (min)',ylabel='Populariy')\nplt.show()","234299f8":"corr_matrix = pd.concat([x_train,y_train],axis=1).corr()\n\ncorr_matrix['popularity'].sort_values(ascending=False)","a541f9fc":"from pandas.plotting import scatter_matrix\n\nscatter_matrix(x_train[['budget','runtime','vote_count','vote_average']], figsize=(12,12))","beb0bce3":"x_train.corr()","c98df0cd":"### Encode language \n\nx_train['Language'] = x_train['original_language'].apply(lambda x: 1 if 'en' == x else 0)\nx_test['Language'] = x_test['original_language'].apply(lambda x: 1 if 'en' == x else 0)","977662fe":"genre_list = ['Action', 'Adventure', 'Fantasy', 'Science Fiction', 'Crime', 'Drama', 'Thriller', 'Animation',\n 'Family', 'Western', 'Comedy', 'Romance', 'Horror', 'Mystery', 'History', 'War', 'Music']","89ecd0f7":"# make column for each genre and encode it\n\nfor genre in genre_list:\n  x_train[genre] = x_train['genres'].apply(lambda x: 1 if genre in x else 0)\n  x_test[genre] = x_test['genres'].apply(lambda x: 1 if genre in x else 0)","fe23d443":"x_train.describe()","a9c2821c":"corr_matrix = pd.concat([x_train,y_train],axis=1).corr()\n\ncorr_matrix['popularity'].sort_values(ascending=False)","f69dea36":"x_train = x_train.drop(columns=['original_language','genres']) # drop encoded columns\nx_test = x_test.drop(columns=['original_language','genres']) # drop encoded columns","f3e5e4d9":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nscaler = MinMaxScaler()\n\nx_train_scaled = pd.DataFrame(scaler.fit_transform(x_train),index=x_train.index, columns=x_train.columns)\nx_test_scaled = pd.DataFrame(scaler.transform(x_test),index=x_test.index, columns=x_test.columns)","b0191d4e":"# Verify\n\nx_train_scaled.head()","97e1752f":"from sklearn.model_selection import cross_val_score\n\nrmse_list = []\nstd_list = []\n\ndef get_score(model):\n  cv_score = cross_val_score(model, x_train_scaled, y_train, scoring = \"neg_mean_squared_error\", cv = 8)\n  rmse = np.sqrt(-cv_score)\n  print('Cross-Validation Root Mean Squared Error:', rmse)\n  print('Average Root Mean Squared Error:', round(np.mean(rmse), 5))\n  rmse_list.append(round(np.mean(rmse), 5))\n  print('Standard deviation:', round(rmse.std(), 5))\n  std_list.append(round(rmse.std(), 5))","13573f00":"from sklearn.linear_model import LinearRegression, Ridge, Lasso","9e560b3a":"### Linear Regression\n\nmodel_1 = LinearRegression()\n\nmodel_1.fit(x_train_scaled,y_train)\n\nget_score(model_1)","419cfd5b":"### Ridge Regression\n\nmodel_2 = Ridge(random_state=SEED)\n\nmodel_2.fit(x_train_scaled,y_train)\n\nget_score(model_2)","6f4521ef":"### Lasso Regression\n\nmodel_3 = Lasso(random_state=SEED)\n\nmodel_3.fit(x_train_scaled,y_train)\n\nget_score(model_3)","ea0e6b68":"from sklearn.ensemble import RandomForestRegressor","46720af5":"model_4 = RandomForestRegressor(random_state=SEED)\n\nmodel_4.fit(x_train_scaled,y_train)\n\nget_score(model_4)","19359a1c":"from sklearn.svm import SVR","0abb8d06":"model_5 = SVR()\n\nmodel_5.fit(x_train_scaled,y_train)\n\nget_score(model_5)","e60a6423":"from xgboost import XGBRegressor","aa06d11c":"model_6 = XGBRegressor(random_state=SEED,verbose=0,objective='reg:squarederror')\n\nmodel_6.fit(x_train_scaled,y_train)\n\nget_score(model_6)","3c4a5480":"import tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom keras.wrappers.scikit_learn import KerasRegressor","3e70f4a5":"def get_tf_model():\n    model = tf.keras.Sequential([\n        L.Input(shape=(x_train_scaled.shape[1])),\n        L.Dense(250, activation='relu'),\n        L.BatchNormalization(),\n        L.Dense(200, activation='relu'),\n        L.BatchNormalization(),\n        L.Dense(200, activation='relu'),\n        L.BatchNormalization(),\n        L.Dense(1)\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss = 'mse',\n        metrics=['accuracy','mse']\n    )\n    \n    return model","db2bad8a":"get_tf_model().summary()","f2d6dc10":"model_7 = KerasRegressor(build_fn = get_tf_model, epochs = 10, verbose = 0, batch_size = 100)\nmodel_7.fit(x_train_scaled,y_train.values)","063eab51":"get_score(model_7)","2a25bec2":"# For creating tables that render in Github\n!pip install --upgrade plotly\n!pip install -U kaleido","2e4d4f99":"import plotly.graph_objects as go\n\n# Create table\n\nmodels_list = ['Linear Regression','Ridge Regression','Lasso Regression','Random Forest', \n               'Support Vector Regressor','XGBoost', 'Neural Network']\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Model', 'RMSE', 'Standard Deviation']),\n                 cells=dict(values=[models_list, rmse_list, std_list]))\n                     ])\n\nfig.update_layout(\n    title={\n        'text': \"Starting Model Cross Validation Scores\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig.show(\"png\")","6f8fb063":"from sklearn.model_selection import GridSearchCV\n\n# Enter model and parameter options and returns best model\ndef grid_search(model,params):\n  search = GridSearchCV(model, params, cv=5, scoring='neg_mean_squared_error')\n  search.fit(x_train_scaled,y_train)\n  return search.best_estimator_","091f3329":"model_3.get_params()","9a281d66":"param_grid = [\n              {'alpha': [0.1,0.05,0.01,0.005] , \n               \"fit_intercept\": [True, False], \n               'normalize': [True, False],\n               \"tol\": [0.0005,.0001,0.00005]}\n]\n\nmodel_3_grid = grid_search(model_3,param_grid)\n\nmodel_3_grid.get_params() # these will be our new parameters","ec4ef3c6":"get_score(model_3_grid)","4cc26938":"# Support Vector Regression\n\nmodel_5.get_params()","09c0caf8":"param_grid = [\n              {'kernel': ['linear', 'rbf'],\n               'tol': [0.015, 0.01],\n               'epsilon': [0.2, 0.15] }\n]\n\nmodel_5_grid = grid_search(model_5,param_grid)\n\nmodel_5_grid.get_params()","e32a66f0":"get_score(model_5_grid)","7109ef0f":"model_6.get_params()","9b0ae4d9":"param_grid = [\n              {'gamma': [10,5],\n               'max_depth': [7,5],\n               'min_child_weight': [30,20],\n               'learning_rate': [0.05,0.01]}\n]\n\nmodel_6_grid = grid_search(model_6,param_grid)\n\nmodel_6_grid.get_params()","9454e7f5":"get_score(model_6_grid)","a4c2315b":"# Create table\n\nmodels_list = ['Lasso Regression','Support Vector Regressor','XGBoost']\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Model', 'Original RMSE', 'Grid Search RMSE', \n                                                   'Original Standard Deviation', 'Grid Search Standard Deviation']),\n                 cells=dict(values=[models_list, [rmse_list[2], rmse_list[4], rmse_list[5]], rmse_list[-3:],\n                                    [std_list[2], std_list[4], std_list[5]], std_list[-3:]]))\n                     ])\n\nfig.update_layout(\n    title={\n        'text': \"Grid Searched Model Comparisons\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n\nfig.show(\"png\")","ec307b91":"from sklearn.metrics import mean_squared_error\n\npredictions = []\nfinal_scores = []\n\ndef get_results(preds):\n  score = np.sqrt(mean_squared_error(preds,y_test.values))\n  final_scores.append(round(score,5))","01d32348":"### Regression\n\npreds = np.array(model_1.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Ridge\n\npreds = np.array(model_2.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Lasso\n\npreds = np.array(model_3.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Forest\n\npreds = np.array(model_4.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### SVR\n\npreds = np.array(model_5.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### XGBoost\n\npreds = np.array(model_6.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Neural Network\n\npreds = model_7.predict(x_test_scaled).reshape(len(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Grid searched Lasso\n\npreds = np.array(model_3_grid.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Grid searched SVR\n\npreds = np.array(model_5_grid.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Grid searched XGBoost\n\npreds = np.array(model_6_grid.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)","71014564":"# average all model predictions\nensemble_1 = np.mean(predictions,axis=0) \nget_results(ensemble_1)\n\n\n# average last three model predictions\nensemble_2 = np.mean(predictions[-3:],axis=0) \nget_results(ensemble_2)\n\n\n# average top three model predictions\nensemble_3 = np.mean([predictions[0], predictions[1],predictions[8]],axis=0) \nget_results(ensemble_3)","d1044543":"# Create table\n\nmodels_list = ['Linear Regression','Ridge Regression','Lasso Regression','Random Forest','Support Vector Regressor',\n               'XGBoost','Neural Network','Grid Search Lasso','Grid Search Support Vector Regressor',\n               'Grid Search XGBoost','All Model Ensemble','Grid Search Model Ensemble', 'Top 3 Ensemble']\n\nmodels_ranked_df = pd.DataFrame(data={'model': models_list, 'score': final_scores}).sort_values(by='score')\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Model', 'Final RMSE']),\n                 cells=dict(values=[models_ranked_df.model, models_ranked_df.score ]))\n                     ])\n\nfig.update_layout(\n    title={\n        'text': \"All Models Ranked\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig.show(\"png\")","261b9470":"from yellowbrick.regressor import ResidualsPlot\n\nvisualizer = ResidualsPlot(model_6, is_fitted=True, train_color='b', test_color='g', size=(1080,720))\nvisualizer.fit(x_train_scaled, y_train)\nvisualizer.score(x_test_scaled,y_test)\nvisualizer.poof() ","9d5ca14a":"# Create table\n\nfeatures_ranked_df = pd.DataFrame(data={'feature': x_test_scaled.columns, \n                                        'importance': model_6.feature_importances_}\n                                  ).sort_values(by='importance', ascending = False)\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Feature', 'Importance']),\n                 cells=dict(values=[features_ranked_df.feature, [round(x,5) for x in features_ranked_df.importance]]))\n                     ])\n\n\nfig.update_layout(\n    title={\n        'text': \"Features Ranked\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig.show(\"png\")","7b160561":"def score(model):\n  return round(model.score(x_test_scaled, y_test),5)\n\n# Create table\n\nmodel_r2_list = [score(model_1), score(model_2), score(model_3), score(model_4), score(model_5),\n                 score(model_6), score(model_7), score(model_3_grid), score(model_5_grid), score(model_6_grid)]\n\nr2_ranked_df = pd.DataFrame(data={'model': models_list[:10], 'r2':model_r2_list}\n                                  ).sort_values(by='r2', ascending = False)\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Model', 'R^2']),\n                 cells=dict(values=[r2_ranked_df.model, r2_ranked_df.r2 ]))\n                     ])\n\nfig.update_layout(\n    title={\n        'text': \"R^2 Values\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig.show(\"png\")","2111cd76":"# Cleaning","52de8071":"## Random Forest","b0bbb792":"# Results","e5401bc4":"### Comparison","9a2c34cf":"That looks much better, but let's verify by checking the skew","9e8125ad":"## Regression","a09b242d":"Now let's get some insight into our cleaned features","d856b245":"These results are not too surprising - they end up very similar to our RMSE table. However, these scores show how much room we still have for improvement. Although 0.84393 is a good score, it still has a bit to go before being a great model. We can also see that our original Lasso Regression model, and our Neural Network fit the data pretty terribly. We were able to improve the Lasso model through grid search, but we will need to experiment quite a bit with Neural Networks to make it more accurate. ","b9bd309a":"### XGBoost","c1f82ae7":"Let's begin by importing some necessary libraries so that we can explore our data!","4726d6b2":"I played with the architecture a bit (number of layers, dropout, neurons per layer, etc.) and this seemed to yield the best results.","80ae0a0e":"Sure enough, the movies with the best vote averages were only reviewed one or two times! We will have to cut off a certain threshold of vote counts to fix this.","81088caa":"## Cross-Validation Scores","fe771ab4":"Let's see how the final RMSEs compare between all these models","c27648af":"We could work to normalize these more, but for the sake of this project, we will keep it how it is. Now that we have made the distributions more normal, let's look at the correlations between our variables.","8d2d17f3":"We can already see that the average vote column has no zeros and no 10s, which is good. Furthermore, the shortest runtime is 25 minutes and the longest is 338 minutes, which seem accurate.\n\nThere seem to be some films that have a budget of 0. We will keep these as valid because there are plenty of low-budget films (think Blair Witch Project)","4b21f03f":"Let's take a look at a couple instances to see what information we have","57621495":"Vote count definitely looks the most correlated to popularity, but let's check their actual correlation coefficients.","bb814b91":"Lasso regression also adds a regularization term to the cost function, but this time it is the sum of the magnitudes of the coefficients.","d410b412":"Wow! I rememeber the Minions movie being popular but not *that* popular! The other top movies seem valid as well - I was working at a movie theater when Deadpool came out, and it was massively popular!\n\nAlthough these are outliers, they are still valid instances, and thus we will not modify or drop any of them.\n\nNow let's look at some of the lowest popularities.","0a891467":"We could do this all day to zero in on the absolute best hyperparemters, but for the sake of this project, we will stop here. Let's look at the comparisons","3a46cd91":"Random Forest is a simple yet powerful machine learning model. It uses many decision trees to make it's final decision. ","b96a1f21":"And just for fun, let's try to ensemble some of these predictions and see how they do. The first ensemble will be an average of every model we have made up to this point. The second ensemble will be an average of just the three grid searched models. The third ensemble will be an average of our top three models up to this point (Linear Regression, Ridge Regression, Grid searched Support Vector Regressor). \n","0dcd78af":"Neural Networks have been around for a while, but have recently taken over all areas of machine learning with architecture advancements and better processing units.  ","bf955945":"Any regression model can also provide an $R^2$ score. This is the coefficient of determination and shows how well our model fit the data between -1 and 1. The closer the score is to 1 the better. Let's take a look at how these compare.","1465ac88":"Now let's try it for Support Vector Regressor","99a75b7d":"Both runtime and vote average seem fairly normal, but vote count and budget are definitely skewed.","d276243b":"Now let's look at the distributions and skew values","5f65418b":"For genres, we will choose to one-hot encode them. Normally, we would use a OneHotEncoder class here, however it will not work with the genre objects we have. This is because every genre entry is a dict with all of the genres they contain, so a OneHotEncoder would make a unique column for each combination. We will have to do it manually.","97310eb1":"We'll pick some options, and apply gird search","a0543410":"Perfect! Now we are finally ready to train some models!","eebba934":"## Support Vector Regressor","905b4a61":"### Lasso","b1567add":"The data is heavily skewed left. The skew method will return how asymmetric the data is (with zero being completely symmetric)","6fe800b9":"Lasso performed the worst, so let's try to improve it by first looking at what parameters there are.","6b4890ef":"Support Vector Machines are typically used for classification tasks, but they can be used for regression as well.","9b4e0708":"Great! Now let's look at the numerical features","93c09a5b":"Up to this point, we have only looked at our training data. Now we can try out our models on the test data to see how they perform on data they have never seen. We will first declare some arrays to store our predictions and scores, and make a function to calculate the Root Mean Squared Error. \n\nIt is also very important to note that the popularity predictions we get will be the logarithm of the actual popularity predictions. Hence, to get the real popularity prediction, we must take the exponential. This is because we need to reverse the log function we applied earlier when we normalized our target variable. ","3fd2f214":"This plot gives a lot of useful information. First, our residuals distribution on the right is approximately normal. This is good - if it was skewed, we would not be able to accurately use this information for statistical decisions. Looking at the main section now, we can see that the training and testing residuals are very similar. Our training data seems to be a bit more spread out overall, which we can verify by checking the $R^2$ scores at the top. These measure how well our model fit the data, with 1 being perfect. Since the training score is a bit better than our testing score, there is a bit of overfitting occuring. Ideally, these would about be the same. Our residuals also center around 0 which means that they predict more than the actual value just about as often as they predict less.\n\n","f882d5ad":"93.8% of the movies are in english. There are a few different ways we could encode this column. One method would be to one-hot encode by creating a binary column for each different language. This would add a ton of new features to the data, with some only having 1 or 2 positive instances. Another way we could encode this is by converting it to just english or not english. This would only add one column and likely contribute to popularity still.","b740a036":"Now it is time to split our data into training and testing data. This is crucial, as we do not want to notice any overall patterns before our models run. We will only use the training data from now on and use our testing data to test our final models.","6fc31ce4":"Grid search improved Lasso by a lot, improved Support Vector by a bit, and actually didn't improve XGBoost. This is why it is important to keep trying different combinations of hyperparameter values to see what works. It also improved standard deviation in Lasso and Support Vector, but made XGBoost worse.","cc5a6880":"Ridge regression adds a regularization term to the cost function which helps prevent overfitting. The term added is the sum of the square of the coefficients.","26d02e6e":"# Hyperparameter Tuning","88b70556":"Now let's drop any values with a runtime of zero","849ee59f":"We will begin with the classic linear regression models. Once again, the results and validity of these depend on the four assumptions that we already discussed. \n\n","07a1f106":"**Hunter Mitchell**\n\n**Movie Popularity Prediction Project**\n\n**September 22nd, 2020**","d6c3f8d7":"# Exploratory Data Analysis","27898c84":"While there is a bit more correlation between some independent variables than we would like, we will choose to ignore it for the scope of this project. Therefore, we now have considered all four assumptions and we can move on to getting the data ready for our models!","5ba19e94":"First let's drop any rows with null values","ddeb26a1":"Linear regression should have all variables be approximately normal, so let's fix this. There are a couple ways we could deal with skewed distributions. These include applying logarithms, square roots, or using the box cox method. I am going to use the log method here. Note that we have to do this to our testing data too. ","cfc06680":"Regular Linear Regression and Ridge Regression perform the best. Lasso Regression definitely performs worse - probably because the hyperparameters aren't tuned. XGBoost, Random Forest, and Support Vector Regressor also perform well. Neural Networks kind of let us down, but if we played with the structure more, I'm sure we could make it better.","f55dc8a1":"We can see that there may be a bit of correlation between our independent variables. Let's look at the correlation coefficients.","d2da1844":"# Model Selection","53b0caf6":"XGBoost uses gradient boosted trees to make it's decision. They often outperform most models when the hyperparameters are tuned correctly.","e1bbe0a9":"## Neural Network","77465a62":"***What makes a movie popular?***\n\nThis project attempts to answer that question by examining various factors of films including budget, run time, genre, language and rating!\n\nWe will utilize Machine Learning and Data Science fundamentals to create a model that will predict a movie's popularity. This could be useful for film companies to determine what factors into a movie's popularity, or for other content creators to better understand their audience.\n\nThe data I am using is obtained from a Kaggle dataset found [here](https:\/\/www.kaggle.com\/tmdb\/tmdb-movie-metadata). I encourage anyone to explore the data for themselves and predict other potentially useful imformation (revenue, rating, etc.) \n\nWe must also verify four main assumptions to get valid results from Linear Regression. These are:\n\n1.   Linearity between features and target\n2.   Multivariate normality\n3.   Little multicollinearity\n4.   Homoscedasticity \n\nWe will examine each of these before implementing a Linear Regression model. Now let's get started!","b3cff8d1":"# Conclusion","d7a37679":"And lastly we will use grid search on our XGBoost model","f63a4cab":"We can see already that there is some missing data that we will need to take care of. There are also entries of zero that could be false.\n\nAccording to the dataset, popularity is measured as the cumulative number of star ratings. It is also unknown whether the budget and revenue are in USD or some other currency. \n\nLet's look at all of our different columns.\n","e2a2d590":"Wow! Just like that our RMSE improved a lot!","523f7894":"With the data all ready to go, it is finally time to train some models! Let's define a few lists to score our metrics, and a function to print them. The cross_val_score will use cross validation to score the model predictions on all parts of our training data. The main metric we will be looking at is Root Mean Squared Error. This is a common metrics for regression tasks. ","f21d681a":"Now we can make test predictions for each model","fea7c737":"We still have categorical features that we need to encode. \n\nFor language, we will make a binary variable with 1 for english and 0 for non-english. ","9e146a97":"Let's see how the models compare","efe95be7":"Let's look at our target variable distribution by plotting a histogram.","b82d7a83":"It looks like we may have a few outliers present. To see if those are valid instances or not, let's look at the corresponding rows.","d0752ef3":"### Support Vector Regressor","a8d9d57c":"Now let's scale our features. Machine Learning models tend to perform better when scaled.","ff0fff39":"And lastly, we have to set the labels dataframe to only include the rows of the new features dataframe","22078dd0":"These results are interesting in themselves! It looks like the adventure and action genres correlate most positively to popularity, and comedy, romance, and drama correlate most negatively. There are a few genres that do not seem to correlate at all, but we will choose to keep them as features.\n\nWe also have to remember to drop our original categorical columns! ","db60b125":"# Feature Engineering","9f767d26":"Let's normalize the budget and vote count variables. We will do another log transformation to vote count and do a square root transformation to budget. This is because budget contains zero values and taking the log of these would give us undefined values. ","9e35780c":"From the models that we tried, XGBoost worked the best. I am confident that with more hyperparameter tuning and ensembling, we could achieve a much better RMSE and $R^2$ score. There are also many other models and methods we could try that are outside the scope of this project. I would like to work with on this further in the future with more data. I would also like to predict other things such as revenue or ratings, and try different feature combinations. Overall, this project gave a lot of insight into what increases the popularity of movies. I think it could be useful for production companies or independent filmmakers. I also personally learned a lot doing this and look forward to working on more complex projects in the future! ","506936cf":"This provides insight into what type of instances we may need to drop - two of these have a runtime of 0! We can also see some missing or suspicious values that we will need to look at. Furthermore, I'm a bit worried about instances when the vote count is very low, as this will make vote average not representative. Let's look at some of the vote average extremes.","5885e50a":"XGBoost also let's us look at how important the features were in it's predictions","96475a68":"## XGBoost","7e373157":"Now let's drop instances with very few vote counts. I decided to have 10 as the cutoff - if 10 or more people voted, it will likely have a fairly accurate vote average. ","aa791ee3":"Let's create our features dataframe and clean it up a bit. I am choosing all the features that I believe impact a movies popularity. I am also leaving out revenue as a feature, as this is something revealed a while after a movie comes out.","4d373cdd":"Surprisingly, our regular XGBoost model performs the best out of all models! We could definitely make it better by finding the right hyperparameters through a more refined grid search. Our grid searched models also perform well, with two of our ensembles coming in 3rd and 4th place. Most of these results are similar to how the models performed on our training data. This is good because it shows there is little to no overfitting and underfitting happening. We could also try some other ensembles to find the best combination. Normally ensembles work the best because they average out all of the residuals between the models. \n\nSpeaking of residuals, let's look at a residuals plot from our best model (XGBoost)","4e7d9c36":"It looks like vote count was the most important feature. This is expected as it was the most correlated to popularity. We have a somewhat unexpected next most important feature: the War genre. This was one of the least correlated to popularity, so it is definitely surprising to see it so high up on this list. I would have expected our other numerical features to be higher up on this list as well. There are some features not shown as they are approximately of zero importance. This is not too surprising though - there were very few instances with these features to begin with and they showed very little correlation. ","438fde66":"As you can see, we have 19 columns to work with. Many of these will not correlate to popularity, though.\n\nOverall, we don't have many null values. There are a lot in the homepage column, but that shouldn't be correlated to popularity, so we can safely ignore it as a feature. Our dataset contains 4,803 instances: not a ton, but it should be plenty for a regression model.\n\nNow let's check out our target variable 'popularity'.","2c30280b":"# Getting to know our data","97071f72":"# Intro","6b9eca3e":"Now let's take a look at the different languages we have.","761fcb6c":"Vote count is indeed the most correlated. The others are still correlated and we can now check off the linearity assumption of Linear Regression. We can also check off our homoscedasticity assumption as the residuals are approximately equal throughout each scatterplot. If they were not equal, we would see a strong cone shape. Now let's look at how our features correlate to each other.","1da5a797":"Let's try to improve some of these scores. Kaggle competitors may spend days or even weeks figuring out the best hyperparameters for their models as well as which models to ensemble. \n\n\nWe will just pick a few to improve by using grid search. Grid search reveals which hyperparameter combinations provide the best results by trying many different combinations from what you give it."}}