{"cell_type":{"b1887b1f":"code","65049624":"code","0c222bd5":"code","e3a0c27c":"code","59e44432":"code","5d1ac507":"code","ca117c90":"code","9f361200":"code","2036c3b7":"code","2bbf523a":"code","b69b903a":"code","7e010206":"code","ed388fcb":"code","2187493c":"code","9e8737ae":"code","dc9f9284":"code","99e1605d":"code","76d91730":"code","1aee98e3":"code","35ab33c6":"code","e4467de1":"code","19fa5cce":"code","ec0e3759":"code","472b0ee8":"code","e8b9d86a":"code","46d63818":"code","17670d1a":"code","f07baf1a":"code","94e6b7f3":"code","967821df":"code","20e7ffad":"code","b5f23c55":"code","53a1caa8":"code","18e1594d":"code","7aed82fe":"code","ee0850b2":"code","ac367dc2":"code","da825646":"code","8fa6709f":"code","2f8a7f16":"code","44caf3be":"code","4d9042d2":"code","86ec07df":"code","9e14a17c":"code","1451273f":"code","5ba2dc8e":"code","f619c54a":"code","7482ab20":"code","6880d28a":"code","1d2a7752":"code","bf7c1774":"code","63cc1d04":"code","5048862f":"code","5a7e9d48":"code","9c916cca":"code","ffef82f4":"code","8439f194":"code","c8ce341c":"code","e6607b03":"code","02332c57":"code","46b436a4":"code","0c86d0ab":"code","d4c63711":"code","aec11268":"code","194934ab":"code","e84961e5":"code","811469b3":"code","ea10042d":"code","f90fb561":"code","ef07d4e3":"code","0d5f4cef":"code","9769e6b4":"code","3e49b9e4":"code","3192a614":"markdown"},"source":{"b1887b1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","65049624":"import pandas as pd\nimport numpy as np\n\n# Text preprocessing\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n#XGboost\nimport xgboost as xbg\nfrom xgboost import XGBClassifier\n\n# Sklearn\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing ,decomposition,model_selection,metrics,pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\n# Matplotlib and Seaborn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize']=10,6\nplt.rcParams['axes.grid']=True\nplt.gray()\n\nimport seaborn as sns\n\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')","0c222bd5":"#Training data\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\nprint('Training data shape: ', train.shape)\ntrain.head()","e3a0c27c":"# Testing data\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nprint('Testing data shape: ',test.shape)\ntest.head()","59e44432":"def missing_value_of_data(data):\n    total = data.isnull().sum().sort_values(ascending=False)\n    percentage= round(total\/data.shape[0]*100,2)\n    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])","5d1ac507":"# Train missing Percentage\nmissing_value_of_data(train)","ca117c90":"# Test Missing Percentage\nmissing_value_of_data(test)","9f361200":"def count_values_in_column(data,feature):\n    total=data.loc[:,feature].value_counts(dropna=False)\n    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])","2036c3b7":"count_values_in_column(train,'target')","2bbf523a":"g =sns.barplot(train['target'].value_counts().index,train['target'].value_counts(),palette='winter')\n# g.set_xticks(range(len(train))) # <--- set the ticks first\ng.set_xticklabels(['Good','Bad'])\nplt.show()","b69b903a":"# A disaster tweet\ndisaster_tweets = train[train['target']==1]['text']\ndisaster_tweets.values[1]","7e010206":"#not a disaster tweet\nnon_disaster_tweets = train[train['target']==0]['text']\nnon_disaster_tweets.values[1]","ed388fcb":"sns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20],\n            orient='h')","2187493c":"train.loc[train['text'].str.contains('disaster', na=False, case=False)].target.value_counts()","9e8737ae":"# Replacing the ambigious locations name with Standard names\ntrain['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\n\nsns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5],\n            orient='h')","dc9f9284":"def unique_values_in_column(data,feature):\n    unique_val=pd.Series(data.loc[:,feature].unique())\n    return pd.concat([unique_val],axis=1,keys=['Unique Values'])","99e1605d":"unique_values_in_column(train,'keyword')","76d91730":"def duplicated_values_data(data):\n    dup=[]\n    columns=data.columns\n    for i in data.columns:\n        dup.append(sum(data[i].duplicated()))\n    return pd.concat([pd.Series(columns),pd.Series(dup)],axis=1,keys=['Columns','Duplicate count'])","1aee98e3":"duplicated_values_data(train)","35ab33c6":"train.describe()","e4467de1":"def find_url(string): \n    text = re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',string)\n    return \"\".join(text) # converting return value from list to string","19fa5cce":"train['url']=train['text'].apply(lambda x: find_url(x))","ec0e3759":"train.head()","472b0ee8":"def find_emoji(text):\n    emo_text=emoji.demojize(text)\n    line=re.findall(r'\\:(.*?)\\:',emo_text)\n    return line\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","e8b9d86a":"def find_email(text):\n    line = re.findall(r'[\\w\\.-]+@[\\w\\.-]+',str(text))\n    return \",\".join(line)","46d63818":"train['email']=train['text'].apply(lambda x: find_email(x))","17670d1a":"def find_hash(text):\n    line=re.findall(r'(?<=#)\\w+',text)\n    return \" \".join(line)","f07baf1a":"train['hash']=train['text'].apply(lambda x: find_hash(x))","94e6b7f3":"# @ David that is mention\ndef find_at(text):\n    line=re.findall(r'(?<=@)\\w+',text)\n    return \" \".join(line)","967821df":"train['at_mention']=train['text'].apply(lambda x: find_at(x))","20e7ffad":"# Pick only number from the snetence\ndef find_number(text):\n    line=re.findall(r'[0-9]+',text)\n    return \" \".join(line)","b5f23c55":"train['number']=train['text'].apply(lambda x: find_number(x))","53a1caa8":"def find_phone_number(text):\n    line=re.findall(r\"\\b\\d{10}\\b\",text)\n    return \"\".join(line)","18e1594d":"train['phonenumber']=train['text'].apply(lambda x: find_phone_number(x))","7aed82fe":"def find_year(text):\n    line=re.findall(r\"\\b(19[40][0-9]|20[0-1][0-9]|2020)\\b\",text)\n    return line","ee0850b2":"train['year']=train['text'].apply(lambda x: find_year(x))","ac367dc2":"def find_nonalp(text):\n    line = re.findall(\"[^A-Za-z0-9 ]\",text)\n    return line","da825646":"train['non_alp']=train['text'].apply(lambda x: find_nonalp(x))","8fa6709f":"#Retrieve punctuations from sentence.\ndef find_punct(text):\n    line = re.findall(r'[!\"\\$%&\\'()*+,\\-.\\\/:;=#@?\\[\\\\\\]^_`{|}~]*', text)\n    string=\"\".join(line)\n    return list(string)","2f8a7f16":"train['punctuation']=train['text'].apply(lambda x : find_punct(x))","44caf3be":"\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text\n#","4d9042d2":"# Applying the cleaning function to both test and training datasets\ntrain['text_clean'] = train['text'].apply(str).apply(lambda x: text_preprocessing(x))","86ec07df":"# Applying the cleaning function to both test and training datasets\ntest['text_clean'] = test['text'].apply(str).apply(lambda x: text_preprocessing(x))","9e14a17c":"# Analyzing Text statistics\n\ntrain['text_len'] = train['text_clean'].astype(str).apply(len)\ntrain['text_word_count'] = train['text_clean'].apply(lambda x: len(str(x).split()))","1451273f":"non_disaster_tweets.head()","5ba2dc8e":"disaster_tweets.head()","f619c54a":"# A disaster tweet\ndisaster_tweets = train[train['target']==1]\ndisaster_tweets.values[1]\n'Forest fire near La Ronge Sask. Canada'\n#not a disaster tweet\nnon_disaster_tweets = train[train['target']==0]\nnon_disaster_tweets.values[1]","7482ab20":"disaster_tweets.head()","6880d28a":"# Sentence length analysis\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.hist(disaster_tweets['text_len'],bins=50,color='r',alpha=0.5)\nplt.title('Disaster Text Length Distribution')\nplt.xlabel('text_len')\nplt.ylabel('count')\n\n\nplt.subplot(1, 2, 2)\nplt.hist(non_disaster_tweets['text_len'],bins=50,color='g',alpha=0.5)\nplt.title('Non Disaster Text Length Distribution')\nplt.xlabel('text_len')\nplt.ylabel('count')\n","1d2a7752":"#source of code : https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","bf7c1774":"#Distribution of top unigrams\ndisaster_tweets_unigrams = get_top_n_words(disaster_tweets['text_clean'],20)\nnon_disaster_tweets_unigrams = get_top_n_words(non_disaster_tweets['text_clean'],20)\n\n\ndf1 = pd.DataFrame(disaster_tweets_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='r')\nplt.ylabel('Count')\nplt.title('Top 20 unigrams in Disaster text')\nplt.show()\n\ndf2 = pd.DataFrame(non_disaster_tweets_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.title('Top 20 unigram in Non Disaster text')\nplt.show()\n","63cc1d04":"def get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","5048862f":"#Distribution of top Bigrams\ndisaster_tweets_bigrams = get_top_n_gram(disaster_tweets['text_clean'],(2,2),20)\nnon_disaster_tweets_bigrams = get_top_n_gram(non_disaster_tweets['text_clean'],(2,2),20)\n\ndf1 = pd.DataFrame(disaster_tweets_bigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='r')\nplt.ylabel('Count')\nplt.title('Top 20 Bigrams in Disaster text')\nplt.show()\n\ndf2 = pd.DataFrame(non_disaster_tweets_bigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.title('Top 20 Bigram in Non Disaster text')\nplt.show()","5a7e9d48":"# Finding top trigram\ndisaster_tweets_trigrams = get_top_n_gram(disaster_tweets['text_clean'],(3,3),20)\nnon_disaster_tweets_trigrams = get_top_n_gram(non_disaster_tweets['text_clean'],(3,3),20)\n\ndf1 = pd.DataFrame(disaster_tweets_trigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 trigrams in Disaster text')\nplt.show()\n\ndf2 = pd.DataFrame(non_disaster_tweets_trigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 trigram in Non Disaster text')\nplt.show()","9c916cca":"#Wordclouds\n# Wordclouds to see which words contribute to which type of polarity.\n\nfrom wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster text',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disater text',fontsize=40);\n\n","ffef82f4":"text = \"Are you coming , aren't you\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \",text)\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","8439f194":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()","c8ce341c":"def remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\ntrain['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntrain.head()","e6607b03":"# Stemming and Lemmatization examples\ntext = \"feet cats wolves talked\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer\nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer=nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","02332c57":"# After preprocessing, the text format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain['text']\ntrain.head()","46b436a4":"# text preprocessing function\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","0c86d0ab":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors[0].todense())","d4c63711":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","aec11268":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores\n\nclf.fit(train_vectors, train[\"target\"])","194934ab":"# Fitting a simple Logistic Regression on TFIDF\nclf_tfidf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf_tfidf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","e84961e5":"# Naives Bayes Classifier\n# Well, this is a decent score. Let's try with another model that is said to work well with text data : Naive Bayes.\n\n# Fitting a simple Naive Bayes on Counts\nclf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","811469b3":"clf_NB.fit(train_vectors, train[\"target\"])","ea10042d":"# Fitting a simple Naive Bayes on TFIDF\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","f90fb561":"clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])","ef07d4e3":"# XGBOOST\nimport xgboost as xgb\nclf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","0d5f4cef":"import xgboost as xgb\nclf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","9769e6b4":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","3e49b9e4":"submission_file_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\ntest_vectors=test_tfidf\nsubmission(submission_file_path,clf_NB_TFIDF,test_vectors)","3192a614":"### TFIDF Features\n> A problem with the Bag of Words approach is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much \u201cinformational content\u201d. Also, it will give more weight to longer documents than shorter documents.\n\n> One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like \u201cthe\u201d that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency-Inverse Document Frequency, or TF-IDF for short, where:\n\nTerm Frequency: is a scoring of the frequency of the word in the current document.\n\n> TF = (Number of times term t appears in a document)\/(Number of terms in the document)\nInverse Document Frequency: is a scoring of how rare the word is across documents.\n\n> IDF = 1+log(N\/n), where, N is the number of documents and n is the number of documents a term t has appeared in."}}