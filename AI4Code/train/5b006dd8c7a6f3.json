{"cell_type":{"266d2a12":"code","34d41f25":"code","739605bb":"code","06dd7d92":"code","4a5ea7a2":"code","3dc46a47":"code","6c16a043":"code","183c31fa":"code","8d40827b":"code","93fd1ca4":"code","b1379ace":"code","69910580":"code","86b664fb":"code","558e8434":"code","7bf310fc":"code","b2c5b1d1":"markdown","b985a400":"markdown","8c82fd3a":"markdown"},"source":{"266d2a12":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold,train_test_split \n\nimport tensorflow as tf\nprint(f'Tensorflow Version',tf.__version__)\nimport tensorflow_hub as hub\n\nfrom tqdm.notebook import tqdm\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","34d41f25":"train_full_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')","739605bb":"train_full_df.shape","06dd7d92":"train_df, valid_df = train_test_split(train_full_df, random_state=42, train_size=0.8)\ntrain_df.shape, valid_df.shape","4a5ea7a2":"train_df.head()","3dc46a47":"train_sent_len = [len(i.split()) for i in train_df['excerpt']]\ntest_sent_len = [len(i.split()) for i in test_df['excerpt']]\n\nplt.hist(train_sent_len, bins=range(min(train_sent_len), max(train_sent_len) + 1, 1), \n              alpha=0.4, color=\"red\")\n\nplt.hist(test_sent_len, bins=range(min(test_sent_len), max(test_sent_len) + 1, 1), \n              alpha=0.4, color=\"blue\")\n\n\nlabels = ['Train','Test']\nplt.legend(labels)\nplt.xlabel(\"length of sentence\")\nplt.ylabel(\"proportion\")\nplt.title(\"comparing number of words per sentence distribution in Train and Test\")\nplt.show()","6c16a043":"train_df['target'].plot(kind='hist', title='Target distribution');","183c31fa":"train_full_df['target'].max(),train_full_df['target'].min()","8d40827b":"def train_and_evaluate_model(module_url, embed_size, name, trainable=False):\n  hub_layer = hub.KerasLayer(module_url, input_shape=[], output_shape=[embed_size,], dtype=tf.string, trainable=trainable)\n  \n  def mapping_to_target_range(x, target_min=-3, target_max=2) :\n    x02 = tf.keras.backend.tanh(x) + 1 # x in range(0,2)\n    scale = ( target_max-target_min )\/2.\n    return  x02 * scale + target_min\n    \n  model = tf.keras.models.Sequential([\n                                      hub_layer,\n                                      tf.keras.layers.Dense(256, activation='relu'),\n                                      tf.keras.layers.Dropout(0.2),\n                                      tf.keras.layers.Dense(128, activation='relu'),\n                                      tf.keras.layers.Dropout(0.3),\n                                      tf.keras.layers.Dense(64, activation=mapping_to_target_range),\n                                      tf.keras.layers.Dropout(0.4),\n                                      tf.keras.layers.Dense(32, activation=mapping_to_target_range),\n                                      tf.keras.layers.Dropout(0.2),\n                                      tf.keras.layers.Dense(1, activation=mapping_to_target_range)\n                                      ])\n  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n              loss='mean_squared_error',\n              metrics=[tf.keras.metrics.RootMeanSquaredError()]) # mean_absolute_error\n  model.summary()\n  history = model.fit(train_df['excerpt'], train_df['target'],\n                    epochs=20,\n                    batch_size=64,\n                    validation_data=(valid_df['excerpt'], valid_df['target']),\n                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min')],\n                    verbose=1)\n  return model, history","93fd1ca4":"module_url = '..\/input\/universalsentenceencoderlarge5\/'\nmodel, history = train_and_evaluate_model(module_url, embed_size=512, name='universal-sentence-encoder-large')","b1379ace":"fig, axes = plt.subplots(1, 2, figsize=(15,5))\naxes[0].plot(history.history['root_mean_squared_error'])\naxes[0].plot(history.history['val_root_mean_squared_error'])\naxes[0].set_xlabel('Epochs')\naxes[0].set_ylabel('Root Mean Square Error')\naxes[0].legend(['Training','Testing'])\naxes[0].grid(True)\n\naxes[1].plot(history.history['loss'])\naxes[1].plot(history.history['val_loss'])\naxes[1].set_xlabel('Epochs')\naxes[1].set_ylabel('Loss')\naxes[1].legend(['Training','Testing'])\naxes[1].grid(True)","69910580":"preds = model.predict(test_df['excerpt'])","86b664fb":"# preds","558e8434":"preds = preds[:,-1]\n# preds","7bf310fc":"pd.DataFrame({\n    'id':test_df.id,\n    'target':preds\n}).to_csv('submission.csv',index=False)","b2c5b1d1":"* Version 1: \nloss: 0.1164 - root_mean_squared_error: 0.3409 - val_loss: 0.3881 - val_root_mean_squared_error: 0.6230\n* Version 2: loss: 0.2204 - root_mean_squared_error: 0.4692 - val_loss: 0.3971 - val_root_mean_squared_error: 0.6302\n\n","b985a400":"Submission","8c82fd3a":"Prediction"}}