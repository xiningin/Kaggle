{"cell_type":{"ff661f9b":"code","1a8386ce":"code","f3ef6801":"code","ac8f8d8e":"code","5e87c78e":"code","4533e9d9":"code","2ce976c7":"code","1acaa29c":"code","cb26f01b":"code","7c85108e":"code","75c09c1b":"code","ebb02ad5":"markdown","1c08cac5":"markdown","0655c015":"markdown","8beb9ebd":"markdown","dcbf616e":"markdown","05d3e740":"markdown","f0382c79":"markdown"},"source":{"ff661f9b":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport time\nimport pickle\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation,Dropout, BatchNormalization\nfrom sklearn.model_selection import KFold\nimport xgboost\nfrom xgboost import plot_importance\nimport lightgbm as lgb\nfrom sklearn.model_selection import  train_test_split\nfrom sklearn.model_selection import GridSearchCV   #Performing grid search\nfrom sklearn.model_selection import validation_curve\nimport gc\nimport os\nprint(os.listdir(\"..\/input\"))","1a8386ce":"#Loading Data \nfile_train = \"..\/input\/train_v2.csv\"\nfile_test = \"..\/input\/test_v2.csv\"\nchunk_size = 10000\n\ndef load_data(file,chunk_size,nrows_load=None,test_data=False):\n    df_res = pd.DataFrame()\n    df_reader = pd.read_csv(file,\n                            dtype={ 'date': str, 'fullVisitorId': str},\n                            chunksize=10000)\n    \n    for cidx, df in enumerate(df_reader):\n        df.reset_index(drop=True, inplace=True)   \n        process_df(df,test_data)\n        df_res = pd.concat([df_res,df ], axis=0).reset_index(drop=True)\n        del df #free memory\n        gc.collect()\n        #print every 20 iterations\n        if cidx % 20 == 0:\n            print('{}: rows loaded: {}'.format(cidx, df_res.shape[0]))\n        if nrows_load:\n            if res.shape[0] >= nrows_load:\n                break\n    return df_res","f3ef6801":"#every column as key and the important features to extract from each column\n\ndef parse_json(x,s):\n    res = json.loads(x)\n    try:\n        return res[s]\n    except:\n        return float('NaN') \n\ndef process_df(df,test_data):\n    #process date \n    df['days'] = df['date'].str[-2:]\n    df['days'] = df['days'].astype(int)\n    df['month'] = df['date'].str[-4:-2]\n    df['month'] = df['month'].astype(int)\n    df['year'] = df['date'].str[:4]\n    df['year'] = df['year'].astype(int)\n\n    #process json fields\n    process_dict = {\n        'totals':['transactionRevenue','newVisits','pageviews','hits'] ,\n        'trafficSource':['campaign','source','medium'] ,\n        'device':['browser'],\n        'geoNetwork': ['country','city','continent','region','subContinent']\n    }\n \n    #add new columns from json in df\n    for c,l in process_dict.items():\n        for it in l:\n            df[it] = df[c].apply(lambda x : parse_json(x,it))\n    \n    #process custom dimensions\n    #df['customDimensions_index'] = df['customDimensions'].apply(lambda x : parse_json(x,'index'))\n    #df['customDimensions_val'] = df['customDimensions'].apply(lambda x : parse_json(x,'value'))\n    \n    \n    #labelencoding for continuous data\n    cols = ['country','campaign','source','medium','continent','city','region','socialEngagementType','browser'\n             ,'channelGrouping','subContinent','date']\n    labelencoder_X=LabelEncoder()\n    for c in cols:\n        df.loc[:,c] = labelencoder_X.fit_transform(df.loc[:,c])\n        \n    \n    #Dealing with missing values\n    #transactionsRevenue and NewVisits:  nans ->  0\n    df['transactionRevenue'].fillna(0,inplace=True)\n    df['newVisits'].fillna(0,inplace=True)\n    df['pageviews'].fillna(0,inplace=True)\n    \n    \n    #Casting Str columns to int\n    df['transactionRevenue'] = df['transactionRevenue'].astype('float32')\n    df['newVisits']= df['newVisits'].astype('uint16')\n    df['pageviews'] = df['pageviews'].astype('uint16')\n    df['hits'] = df['hits'].astype('uint32')\n    #df['index'] = df['index'].astype('uint32')\n    \n    #remove json field columns and some unwanted columns\n    #(some removed for saving memory)\n    \n    rm_col = ['subContinent',\n             'channelGrouping','date','continent','customDimensions','fullVisitorId']\n    if test_data:\n        rm_col = rm_col[:-1]\n    df.drop(list(process_dict.keys()) + rm_col, axis=1,inplace=True)\n    \n#load and process\ndf = load_data(file_train,chunk_size)\ndf_test =load_data(file_test,chunk_size,test_data=True)","ac8f8d8e":"#percent not zero \nper = sum(df['transactionRevenue'] > 0) \/ len(df['transactionRevenue'])\nprint('Percentage of transactions greater than 0 :   {} %'.format(per*100))","5e87c78e":"#Setting up Data\nX = df[df.columns[df.columns != 'transactionRevenue']]\nY = np.log1p(df['transactionRevenue'])\nX_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.4, random_state=42)\n\nX_test = df_test[df_test.columns[df_test.columns != 'transactionRevenue']]\nY_test = np.log1p(df_test['transactionRevenue'])\n\n#Handling the imbalanced dataset and looking at how it effects training\n\n#Sampling \n#Using all the 1% non zero transactions revenue with an equal num of rows from the zero results\nindices_nonzero = np.where (y_train > 0)\nindices_zero = np.where (y_train == 0)\nnum_nonzero = len(indices_nonzero[0])\nnum_zero = len(indices_zero[0])\nprint('Number of non-zero transactions revenues = ', num_nonzero)\nprint('Number of zero transactions revenues = ', num_zero)\n\n#Creating a sample dataset containing 50% of rows with non-zero transactions \nall_indx = list(indices_zero[0][0:num_nonzero]) + list(indices_nonzero[0])\nX_sample = X_train.iloc[all_indx,:]\ny_sample = y_train.iloc[all_indx]\n#split train validation \nX_train_sample, X_val_sample, y_train_sample, y_val_sample = train_test_split(X_sample, y_sample, test_size=0.45, random_state=42)\nprint('Sample X_Train shape ', X_train_sample.shape)","4533e9d9":"def create_model_xgboost(X_train,y_train,X_val=None,y_val=None):\n    params = {'objective': 'reg:linear',\n              'eval_metric': 'rmse',\n              'min_child_weight':1.5,    \n              'eta': 0.01,\n              'max_depth': 4,\n              'subsample': 0.7,\n              'colsample_bytree': 0.6,\n              'reg_alpha':1,\n              'reg_lambda':0.45,\n              'random_state': 42,\n              'silent': True}\n    \n    xgb_train_data = xgboost.DMatrix(X_train, y_train)\n    if not X_val is None:\n        xgb_val_data = xgboost.DMatrix(X_val, y_val)\n        evals=[(xgb_train_data, 'train'), (xgb_val_data, 'valid')]\n    else:\n        evals=[(xgb_train_data, 'train')]\n    model = xgboost.train(params, xgb_train_data, \n                      num_boost_round=1200, \n                      evals= evals,\n                      early_stopping_rounds=50, \n                      verbose_eval=300) \n    return model\n\n#Model  definition\ndef create_model_nn(in_dim,layer_size=250):\n    model = Sequential()\n    model.add(Dense(layer_size,activation='relu',input_dim=in_dim))\n    model.add(BatchNormalization())\n    model.add(Dense(layer_size,activation='relu'))\n    model.add(Dense(layer_size,activation='relu'))\n    model.add(Dense(1, activation='linear'))\n    adam = optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n    model.compile(optimizer=adam,loss='mse')    \n    return model\n\n\ndef create_model_lgbm(X_train,y_train,X_val=None,y_val=None):\n    dtrain = lgb.Dataset(X_train,label=y_train)\n    dval = lgb.Dataset(X_val,label=y_val)\n\n    param_up = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 2, \"min_child_samples\": 20, \n               \"reg_alpha\": 1.5, \"reg_lambda\": 1.5,\n               \"num_leaves\" : 15, \"learning_rate\" : 0.1, \n               \"subsample\" : 1, \"colsample_bytree\" : 1, \n               \"verbosity\": -1,'data_random_seed':4}\n    if not X_val is None:\n        valid_sets = (dtrain,dval)\n        valid_names = ['train','valid']\n    else:\n        valid_sets = (dtrain)\n        valid_names = ['train']\n    model = lgb.train(param_up,dtrain,num_boost_round=5000,valid_sets=valid_sets,valid_names=['train','valid'],verbose_eval=300,\n                     early_stopping_rounds=50)\n    return model","2ce976c7":"\n#Fitting and Training\ndef fit_train(x,y,X_val,y_val,layer_size=64,mod = 'nn'):    \n    if mod =='nn':\n        model = create_model_nn(x.shape[1],layer_size)\n        history = model.fit(x, y, epochs=3, batch_size=128,validation_data=(X_val,y_val),verbose=1)\n        print('Mean RMSE : ',np.sqrt(history.history['val_loss']).mean())\n    elif mod =='xgboost':\n         model = create_model_xgboost(x,y,X_val,y_val)\n    elif mod =='lgbm':\n        model = create_model_lgbm(x,y,X_val,y_val)\n    else:\n        raise Exception ('invalid model')\n    return model\n\n\ndef calc_rmse(pred,y):\n    diff =  pred - y\n    RMSE = ((diff ** 2).mean()) ** .5\n    print('RMSE : ',RMSE)\n\n#Evaluation\ndef eval_set(model,x,y,mod='nn'):\n        if mod == 'nn':\n            p = model.predict(x,batch_size=64,verbose=1)\n            pred = p[:,0]\n        elif mod == 'xgboost':\n            dx = xgboost.DMatrix(x)\n            pred = model.predict(dx,ntree_limit=model.best_ntree_limit)\n        elif mod == 'lgbm':\n            pred = model.predict(x, num_iteration=model.best_iteration)\n        else:\n            raise Exception ('invalid model')\n        calc_rmse(pred,y)","1acaa29c":"from keras import backend as K\nimport tensorflow as tf\nfrom keras import optimizers\nimport keras as k\n\n\n#Training\nprint('Training on training\/val Dataset')\nmodel_training = fit_train(X_train,y_train,X_val,y_val)\nprint('-'*40)\nprint('Training on Sample Dataset')\nmodel_sample = fit_train(X_train_sample,y_train_sample,X_val_sample,y_val_sample,layer_size=500)\nprint('-'*40)\nprint('Training on Full training Dataset')\nmodel_full = create_model_nn(X.shape[1])\nmodel_full.fit(X, Y, epochs=3, batch_size=128,verbose=1)\n\nprint('-'*40)","cb26f01b":"#Training\nprint('Training on training\/val Dataset')\nmodel_training = fit_train(X_train,y_train,X_val,y_val,mod='xgboost')\nprint('-'*40)\nprint('Training on Sample Dataset')\nmodel_sample = fit_train(X_train_sample,y_train_sample,X_val_sample,y_val_sample,layer_size=500,mod='xgboost')\nprint('-'*40)\nprint('Training on Full training Dataset')\nmodel_full = create_model_xgboost(X, Y)\n\nprint('Full Training set -- Using Sample training model')\neval_set(model_sample,X_train,y_train,mod='xgboost')","7c85108e":"#Training\nprint('Training on training\/val Dataset')\nmodel_training = fit_train(X_train,y_train,X_val,y_val,mod='lgbm')\nprint('-'*40)\nprint('Training on Sample Dataset')\nmodel_sample = fit_train(X_train_sample,y_train_sample,X_val_sample,y_val_sample,layer_size=500,mod='lgbm')\nprint('-'*40)\nprint('Training on Full training Dataset')\nmodel_full = create_model_lgbm(X, Y)\n\nprint('Full Training set -- Using Sample training model')\neval_set(model_sample,X_train,y_train,mod='lgbm')\n\nprint('-'*40)","75c09c1b":"#Baseline Predictions\ndf_test['predictions'] = model_full.predict(df_test.loc[:,df_test.columns[1:]],num_iteration=model_full.best_iteration)\ndf_test.loc[df_test['predictions']< 0 ,'predictions']= 0\n#set up dataframe for submission\nsub_df = pd.DataFrame({'fullVisitorId':df_test['fullVisitorId'] , 'PredictedLogRevenue': np.expm1(df_test['predictions'])})\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"baseline_lgb_submission.csv\", index=False)","ebb02ad5":"# Testing BaseLine Models\n\nThe notebook includes testing different base models on the dataset \n\n1. [Defining Models](#def_models)\n2. [Defining Training and Evaluation Methods](#t_evals)\n3. Models: <br>\n    3.1.   [Keras](#kr)<br>\n    3.2.   [ XGBoost ](#another_cell)<br>\n    3.3.  [LightGBM](#lgbm)<br>\n4. [Submission](#sub)<br>\n","1c08cac5":"# Submission\n<a id='sub'><\/a>","0655c015":"# LightGBM\n<a id='lgbm'><\/a>","8beb9ebd":"# Defining Training and Evaluation Methods\n<a id='t_evals'><\/a>","dcbf616e":"# Defining Models\n<a id='def_models'><\/a>","05d3e740":"1. # Keras NN Sequential model\n<a id='kr'><\/a>","f0382c79":" # XGBOOST Model \n<a id='another_cell'><\/a>"}}