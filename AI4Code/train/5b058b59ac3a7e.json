{"cell_type":{"e14239f1":"code","97c665f0":"code","cf327d88":"code","8a189dd3":"code","d9d76b11":"code","69263418":"code","46c9954e":"code","9f0080e9":"code","4b138383":"code","3a65ab84":"code","1747da7d":"code","0f30fba5":"code","542ae06f":"code","fa12ac87":"code","bbb32d56":"code","819af871":"code","a14add54":"code","fb3a1bd4":"code","d4ede63b":"code","cb0a8ebf":"code","7fd13a53":"code","af485807":"code","676d6279":"code","837b3819":"code","b3df0e8d":"code","bc4c72bb":"code","ce7b6e95":"code","a0cce98e":"code","b5076565":"code","f800528c":"code","b5d52b2b":"code","25ccb595":"code","7992444b":"code","3300f1cc":"code","227b9195":"markdown","dbd84ef1":"markdown","c164cb7e":"markdown","84221c3b":"markdown","f01a22c2":"markdown","8fd35e1c":"markdown","2d47c7ce":"markdown","bdfe264d":"markdown","0f8746c1":"markdown","ebe8666a":"markdown","1c52aee0":"markdown","64d698d3":"markdown","ad9101cc":"markdown","2281de19":"markdown","eccb8da0":"markdown","0859043a":"markdown","3e994d52":"markdown","52465f2c":"markdown","ec9c3bb3":"markdown","01353830":"markdown","51aa3b6d":"markdown","66693ae9":"markdown","711c644e":"markdown","1be660f9":"markdown","9ba20c78":"markdown","3ab2785d":"markdown","e778feb2":"markdown","a15fe6d7":"markdown","e0130ec8":"markdown","082519d0":"markdown","2711def2":"markdown","63c94374":"markdown","79605043":"markdown","586335b9":"markdown","bfb3b195":"markdown","bbd9a7b7":"markdown","33deddcf":"markdown","f74048f4":"markdown","c1ea8bb5":"markdown","f6792768":"markdown"},"source":{"e14239f1":"import json\nimport glob\nimport random\nimport collections\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\n\nimport numpy as np\nimport pandas as pd\n\nimport os\nimport plotly.express as px\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport cv2\n\n#Text Color\nfrom termcolor import colored\n\npackage_path = \"..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master\/\"\nimport sys \nsys.path.append(package_path)\n\nimport time\n\nimport torch\nfrom torch import nn\nfrom torch.utils import data as torch_data\nfrom sklearn import model_selection as sk_model_selection\nfrom torch.nn import functional as torch_functional\nimport efficientnet_pytorch\n\nfrom sklearn.model_selection import StratifiedKFold\n\n# W&B for experiment tracking\nimport wandb\nwandb.login()","97c665f0":"class config:\n    DIRECTORY_PATH = \"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\"\n    TRAIN_LABELS_PATH = DIRECTORY_PATH + \"\/train_labels.csv\"\n    \n# wandb config\nWANDB_CONFIG = {\n    'competition': 'rsna-miccai-brain', \n          '_wandb_kernel': 'neuracort'\n}","cf327d88":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\n\nset_seed(42)","8a189dd3":"train_df = pd.read_csv(config.TRAIN_LABELS_PATH)\ntrain_df.head()","d9d76b11":"# Function to print stylized text\ndef style_text(text, text_color = 'yellow', attributes = ['bold'], data = False):\n    \"\"\"\n    Function to stylize print Text using Colored by Termcolor\n    \n    parameters: text(str) - Input Text to be Stylized\n                text_color(str) - Color of text\n                attributes(list of strings) - Attributes to be applied on text\n                data - To be printed with text \n    \"\"\"\n    if data:\n        print(colored(text, text_color, attrs = attributes), data)\n        \n    else:\n        print(colored(text, text_color, attrs = attributes))","69263418":"# Data shape\nstyle_text(\"No. of Rows in train_df: \", data = train_df.shape[0])\nstyle_text(\"No. of Columns in train_df: \", data = train_df.shape[1])","46c9954e":"# Missing Values\nstyle_text(\"Missing Values in train_df:\")\nprint(train_df.isnull().sum())","9f0080e9":"#Dataset Info\nstyle_text(\"Info about train_df:\")\ntrain_df.info()","4b138383":"plt.figure(figsize=(5, 5))\nsns.countplot(data=train_df, x=\"MGMT_value\");","3a65ab84":"wandb.init(project='brain-tumor-viz', config=WANDB_CONFIG)","1747da7d":"def load_dicom(path):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data","0f30fba5":"df = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv\")\ndf_train, df_valid = sk_model_selection.train_test_split(\n    df, \n    test_size=0.2, \n    random_state=42, \n    stratify=train_df[\"MGMT_value\"],\n)","542ae06f":"class DataRetriever(torch_data.Dataset):\n    def __init__(self, paths, targets):\n        self.paths = paths\n        self.targets = targets\n          \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, index):\n        _id = self.paths[index]\n        patient_path = f\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{str(_id).zfill(5)}\/\"\n        channels = []\n        for t in (\"FLAIR\", \"T1w\", \"T1wCE\"): # \"T2w\"\n            t_paths = sorted(\n                glob.glob(os.path.join(patient_path, t, \"*\")), \n                key=lambda x: int(x[:-4].split(\"-\")[-1]),\n            )\n            # start, end = int(len(t_paths) * 0.475), int(len(t_paths) * 0.525)\n            x = len(t_paths)\n            if x < 10:\n                r = range(x)\n            else:\n                d = x \/\/ 10\n                r = range(d, x - d, d)\n                \n            channel = []\n            # for i in range(start, end + 1):\n            for i in r:\n                channel.append(cv2.resize(load_dicom(t_paths[i]), (256, 256)) \/ 255)\n            channel = np.mean(channel, axis=0)\n            channels.append(channel)\n            \n        y = torch.tensor(self.targets[index], dtype=torch.float)\n        \n        return {\"X\": torch.tensor(channels).float(), \"y\": y}","fa12ac87":"train_data_retriever = DataRetriever(\n    df_train[\"BraTS21ID\"].values, \n    df_train[\"MGMT_value\"].values, \n)\n\nvalid_data_retriever = DataRetriever(\n    df_valid[\"BraTS21ID\"].values, \n    df_valid[\"MGMT_value\"].values,\n)","bbb32d56":"plt.figure(figsize=(16, 6))\nfor i in range(3):\n    plt.subplot(1, 3, i + 1)\n    plt.imshow(train_data_retriever[100][\"X\"].numpy()[i], cmap=\"gray\")","819af871":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = efficientnet_pytorch.EfficientNet.from_name(\"efficientnet-b0\")\n        checkpoint = torch.load(\"..\/input\/efficientnet-pytorch\/efficientnet-b0-08094119.pth\")\n        self.net.load_state_dict(checkpoint)\n        n_features = self.net._fc.in_features\n        self.net._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n    \n    def forward(self, x):\n        out = self.net(x)\n        return out","a14add54":"class LossMeter:\n    def __init__(self):\n        self.avg = 0\n        self.n = 0\n\n    def update(self, val):\n        self.n += 1\n        # incremental update\n        self.avg = val \/ self.n + (self.n - 1) \/ self.n * self.avg\n\n        \nclass AccMeter:\n    def __init__(self):\n        self.avg = 0\n        self.n = 0\n        \n    def update(self, y_true, y_pred):\n        y_true = y_true.cpu().numpy().astype(int)\n        y_pred = y_pred.cpu().numpy() >= 0\n        last_n = self.n\n        self.n += len(y_true)\n        true_count = np.sum(y_true == y_pred)\n        # incremental update\n        self.avg = true_count \/ self.n + last_n \/ self.n * self.avg","fb3a1bd4":"class Trainer:\n    def __init__(\n        self, \n        model, \n        device, \n        optimizer, \n        criterion, \n        loss_meter, \n        score_meter\n    ):\n        self.model = model\n        self.device = device\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.loss_meter = loss_meter\n        self.score_meter = score_meter\n        \n        self.best_valid_score = -np.inf\n        self.n_patience = 0\n        \n        self.messages = {\n            \"epoch\": \"[Epoch {}: {}] loss: {:.5f}, score: {:.5f}, time: {} s\",\n            \"checkpoint\": \"The score improved from {:.5f} to {:.5f}. Save model to '{}'\",\n            \"patience\": \"\\nValid score didn't improve last {} epochs.\"\n        }\n    \n    def fit(self, epochs, train_loader, valid_loader, save_path, patience):        \n        for n_epoch in range(1, epochs + 1):\n            self.info_message(\"EPOCH: {}\", n_epoch)\n            \n            train_loss, train_score, train_time = self.train_epoch(train_loader)\n            valid_loss, valid_score, valid_time = self.valid_epoch(valid_loader)\n            \n            self.info_message(\n                self.messages[\"epoch\"], \"Train\", n_epoch, train_loss, train_score, train_time\n            )\n            \n            self.info_message(\n                self.messages[\"epoch\"], \"Valid\", n_epoch, valid_loss, valid_score, valid_time\n            )\n\n            if True:\n#             if self.best_valid_score < valid_score:\n                self.info_message(\n                    self.messages[\"checkpoint\"], self.best_valid_score, valid_score, save_path\n                )\n                self.best_valid_score = valid_score\n                self.save_model(n_epoch, save_path)\n                self.n_patience = 0\n            else:\n                self.n_patience += 1\n            \n            if self.n_patience >= patience:\n                self.info_message(self.messages[\"patience\"], patience)\n                break\n            \n    def train_epoch(self, train_loader):\n        \n        wandb.watch(model)    # Use wandb.watch() to provide the model to be logged upon\n\n        self.model.train()\n        t = time.time()\n        train_loss = self.loss_meter()\n        train_score = self.score_meter()\n                \n        for step, batch in enumerate(train_loader, 1):\n            X = batch[\"X\"].to(self.device)\n            targets = batch[\"y\"].to(self.device)\n            self.optimizer.zero_grad()\n            outputs = self.model(X).squeeze(1)\n            \n            loss = self.criterion(outputs, targets)\n            wandb.log({\"train_loss\": loss})    # Use wandb.log() to log desired metrics \n            \n            loss.backward()\n\n            train_loss.update(loss.detach().item())\n            train_score.update(targets, outputs.detach())\n\n            self.optimizer.step()\n            \n            _loss, _score = train_loss.avg, train_score.avg\n            message = 'Train Step {}\/{}, train_loss: {:.5f}, train_score: {:.5f}'\n            self.info_message(message, step, len(train_loader), _loss, _score, end=\"\\r\")\n        \n        return train_loss.avg, train_score.avg, int(time.time() - t)\n    \n    def valid_epoch(self, valid_loader):\n        self.model.eval()\n        t = time.time()\n        valid_loss = self.loss_meter()\n        valid_score = self.score_meter()\n\n        for step, batch in enumerate(valid_loader, 1):\n            with torch.no_grad():\n                X = batch[\"X\"].to(self.device)\n                targets = batch[\"y\"].to(self.device)\n\n                outputs = self.model(X).squeeze(1)\n                loss = self.criterion(outputs, targets)\n                wandb.log({\"valid_loss\": loss})    # Use wandb.log() to log desired metrics \n\n                valid_loss.update(loss.detach().item())\n                valid_score.update(targets, outputs)\n                \n            _loss, _score = valid_loss.avg, valid_score.avg\n            message = 'Valid Step {}\/{}, valid_loss: {:.5f}, valid_score: {:.5f}'\n            self.info_message(message, step, len(valid_loader), _loss, _score, end=\"\\r\")\n        \n        return valid_loss.avg, valid_score.avg, int(time.time() - t)\n    \n    def save_model(self, n_epoch, save_path):\n        torch.save(\n            {\n                \"model_state_dict\": self.model.state_dict(),\n                \"optimizer_state_dict\": self.optimizer.state_dict(),\n                \"best_valid_score\": self.best_valid_score,\n                \"n_epoch\": n_epoch,\n            },\n            save_path,\n        )\n    \n    @staticmethod\n    def info_message(message, *args, end=\"\\n\"):\n        print(message.format(*args), end=end)","d4ede63b":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_data_retriever = DataRetriever(\n    df_train[\"BraTS21ID\"].values, \n    df_train[\"MGMT_value\"].values, \n)\n\nvalid_data_retriever = DataRetriever(\n    df_valid[\"BraTS21ID\"].values, \n    df_valid[\"MGMT_value\"].values,\n)\n\ntrain_loader = torch_data.DataLoader(\n    train_data_retriever,\n    batch_size=8,\n    shuffle=True,\n    num_workers=8,\n)\n\nvalid_loader = torch_data.DataLoader(\n    valid_data_retriever, \n    batch_size=8,\n    shuffle=False,\n    num_workers=8,\n)\n\nmodel = Model()\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch_functional.binary_cross_entropy_with_logits\n\ntrainer = Trainer(\n    model, \n    device, \n    optimizer, \n    criterion, \n    LossMeter, \n    AccMeter\n)\n\nhistory = trainer.fit(\n    1, \n    train_loader, \n    valid_loader, \n    f\"best-model-0.pth\", \n    100,\n)","cb0a8ebf":"models = []\nfor i in range(1):\n    model = Model()\n    model.to(device)\n    \n    checkpoint = torch.load(f\"best-model-{i}.pth\")\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.eval()\n    \n    models.append(model)","7fd13a53":"class DataRetriever(torch_data.Dataset):\n    def __init__(self, paths):\n        self.paths = paths\n          \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, index):\n        _id = self.paths[index]\n        patient_path = f\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/test\/{str(_id).zfill(5)}\/\"\n        channels = []\n        for t in (\"FLAIR\", \"T1w\", \"T1wCE\"): # \"T2w\"\n            t_paths = sorted(\n                glob.glob(os.path.join(patient_path, t, \"*\")), \n                key=lambda x: int(x[:-4].split(\"-\")[-1]),\n            )\n            # start, end = int(len(t_paths) * 0.475), int(len(t_paths) * 0.525)\n            x = len(t_paths)\n            if x < 10:\n                r = range(x)\n            else:\n                d = x \/\/ 10\n                r = range(d, x - d, d)\n                \n            channel = []\n            # for i in range(start, end + 1):\n            for i in r:\n                channel.append(cv2.resize(load_dicom(t_paths[i]), (256, 256)) \/ 255)\n            channel = np.mean(channel, axis=0)\n            channels.append(channel)\n        \n        return {\"X\": torch.tensor(channels).float(), \"id\": _id}","af485807":"submission = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv\")\n\ntest_data_retriever = DataRetriever(\n    submission[\"BraTS21ID\"].values, \n)\n\ntest_loader = torch_data.DataLoader(\n    test_data_retriever,\n    batch_size=4,\n    shuffle=False,\n    num_workers=8,\n)","676d6279":"y_pred = []\nids = []\n\nfor e, batch in enumerate(test_loader):\n    print(f\"{e}\/{len(test_loader)}\", end=\"\\r\")\n    with torch.no_grad():\n        tmp_pred = np.zeros((batch[\"X\"].shape[0], ))\n        for model in models:\n            tmp_res = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n            tmp_pred += tmp_res\n        y_pred.extend(tmp_pred)\n        ids.extend(batch[\"id\"].numpy().tolist())","837b3819":"submission = pd.DataFrame({\"BraTS21ID\": ids, \"MGMT_value\": y_pred})\nsubmission.to_csv(\"submission.csv\", index=False)","b3df0e8d":"submission","bc4c72bb":"# Store all wandb image paths in a list\n\nwandb_img_paths = []\nfolder_path = \"..\/input\/wandb-rsna\/wandb_1.png\"\n\nfor i in range(1, 8):\n    path = \"..\/input\/wandb-rsna\/wandb_\" + str(i) + \".png\"\n    wandb_img_paths.append(path)","ce7b6e95":"def display_img(img_path):\n    \"\"\"\n    Function which takes an image path and displays it.\n    \n    params: img_path(str): Path of Image to be displayed\n    \"\"\"\n\n    fig = matplotlib.pyplot.gcf()\n    fig.set_size_inches(25.5, 17.5)\n\n    img = cv2.imread(img_path)\n\n    plt.axis('off')\n    plt.imshow(img)","a0cce98e":"display_img(wandb_img_paths[0])","b5076565":"display_img(wandb_img_paths[1])","f800528c":"display_img(wandb_img_paths[2])","b5d52b2b":"display_img(wandb_img_paths[3])","25ccb595":"display_img(wandb_img_paths[4])","7992444b":"display_img(wandb_img_paths[5])","3300f1cc":"display_img(wandb_img_paths[6])","227b9195":"## Comparison of T1 vs T2 vs Flair (Brain)\n<center><img src = \"https:\/\/case.edu\/med\/neurology\/NR\/t1t2flairbrain.jpg\"\/><\/center> ","dbd84ef1":"**Weights & Biases** is the machine learning platform for developers to build better models faster. \n\nYou can use W&B's lightweight, interoperable tools to \n- quickly track experiments, \n- version and iterate on datasets, \n- evaluate model performance, \n- reproduce models, \n- visualize results and spot regressions, \n- and share findings with colleagues. \n\nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.\n\nIn this notebook I will use Weights and Biases's amazing features to perform wonderful visualizations seamlessly. ","c164cb7e":"<a id=\"load-datasets\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load Datasets<\/center><\/h2>","84221c3b":"> **[01]** [Magnetic Resonance Imaging (MRI) of the Brain and Spine: Basics](https:\/\/case.edu\/med\/neurology\/NR\/MRI%20Basics.htm)  \n> **[02]** [\n> Brain Tumor EDA and Interactive Viz with W&B](https:\/\/www.kaggle.com\/ayuraj\/brain-tumor-eda-and-interactive-viz-with-w-b)  \n> **[03]** [\ud83e\udde0Brain Tumor\ud83e\udde0 - EDA with Animations and Modeling](https:\/\/www.kaggle.com\/ihelon\/brain-tumor-eda-with-animations-and-modeling\/data)","f01a22c2":"<h1><center> <\/center><\/h1>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:1; color:white' role=\"tab\" aria-controls=\"home\"><center>RSNA | EDA + Visual + DeepUnderstanding + W&B<\/center><\/h1>\n\n\n<center><img src = \"https:\/\/www.radiologybusiness.com\/sites\/default\/files\/2019-12\/rsna_copy.jpg\" width = \"550\" height = \"300\"\/><\/center>                                                                                               ","8fd35e1c":"Credits to the model goes to [\nYaroslav Isaienkov](https:\/\/www.kaggle.com\/ihelon)\n\nI will take his model a step ahead and incorporate Weights and Biases in it with the explanation on how to utilise it.","2d47c7ce":"## Comparison of T1 vs T2 - Spine\n<center><img src = \"https:\/\/case.edu\/med\/neurology\/NR\/t1t2spine.jpg\"\/><\/center> ","bdfe264d":"**Magnetic resonance imaging (MRI)** is one of the most commonly used tests in neurology and neurosurgery. MRI provides exquisite detail of brain, spinal cord and vascular anatomy, and has the advantage of being able to visualize anatomy in all three planes: axial, sagittal and coronal (see the example image below).\n\n","0f8746c1":"## CountPlot for MGMT Value","ebe8666a":"<a id=\"tabular-exploration\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Tabular Exploration<\/center><\/h2>","1c52aee0":"<a id=\"understanding-mri\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Understanding MRI<\/center><\/h2>","64d698d3":"<a id=\"libraries\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries<\/center><\/h2>","ad9101cc":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:purple; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> This notebook will be updated frequently so keep checking for furthur developments.<\/center><\/h3>","2281de19":"<a id=\"wandb-system-metrics\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>wandb System Metrics<\/center><\/h2>","eccb8da0":"And that's it, with just 2 Steps you have successfully integrated wandb to your project. Now you can go to your dashboard and check the logged metrics. You can even follow the same method to log the hyperparameters.","0859043a":"<a id=\"references\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>References<\/center><\/h2>","3e994d52":"Submissions are evaluated on the [area under the ROC curve](http:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic) between the predicted probability and the observed target.","52465f2c":"<a id=\"model\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Model<\/center><\/h2>","ec9c3bb3":"<a id=\"global-config\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Global Config<\/center><\/h2>","01353830":"## Comparison of T1 vs T1 with Gadolinium\n<center><img src = \"https:\/\/case.edu\/med\/neurology\/NR\/T1%20T1%20gad.jpg\"\/><\/center> ","51aa3b6d":"## Evaluation Criteria","66693ae9":"Thus, there are no missing values in the `train_df` dataset.","711c644e":"### Connect with me on [LinkedIn](https:\/\/www.linkedin.com\/in\/ishandutta0098) :-)","1be660f9":"**Wandb Step 1:** In the first step we need to initialize wandb with the name of a `project` where we want to save our runs.","9ba20c78":"## Basic Tabular Details","3ab2785d":"<center><img src = \"https:\/\/case.edu\/med\/neurology\/NR\/mri%20slices%20new.jpg\"\/><\/center> ","e778feb2":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents<\/center><\/h2>","a15fe6d7":"<a id=\"competition-overview\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Competition Overview<\/center><\/h2>","e0130ec8":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:purple; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.<\/center><\/h3>","082519d0":"<center><img src = \"https:\/\/i.imgur.com\/1sm6x8P.png\" width = \"750\" height = \"500\"\/><\/center>  ","2711def2":"**Wandb Step 2:** In this example we are going to log the Training and Validation losses. To do this we need to instruct wandb to `watch` the `model`","63c94374":"MRI has an advantage over CT in being able to detect flowing blood and cryptic vascular malformations. It can also detect demyelinating disease, and has no beam-hardening artifacts such as can be seen with CT. \n\nThus, the posterior fossa is more easily visualized on MRI than CT. Imaging is also performed without any ionizing radiation.","79605043":"## Comparison of Flair vs Diffusion Weighted\n<center><img src = \"https:\/\/case.edu\/med\/neurology\/NR\/flairdwicom.jpg\"\/><\/center> ","586335b9":"The Radiological Society of North America (RSNA) has teamed up with the Medical Image Computing and Computer Assisted Intervention Society (the MICCAI Society) to improve diagnosis and treatment planning for patients with glioblastoma. \n\nIn this competition you will predict the genetic subtype of glioblastoma using MRI (magnetic resonance imaging) scans to train and test your model to detect for the presence of MGMT promoter methylation.\n\nIf successful, you'll help brain cancer patients receive less invasive diagnoses and treatments. The introduction of new and customized treatment strategies before surgery has the potential to improve the management, survival, and prospects of patients with brain cancer.","bfb3b195":"<a id=\"weights-and-biases\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Weights and Biases<\/center><\/h2>","bbd9a7b7":"Yet another interesting fact is that you can view your hardware utilization too in the wandb dashboard. I am putting up some examples here for reference. These can be viewed in my [project page](https:\/\/wandb.ai\/ishandutta\/brain-tumor-viz\/runs\/1ipijldy\/overview?workspace=user-ishandutta) as well.","33deddcf":"## Data Files\n\n- **train\/** - folder containing the training files, with each top-level folder representing a subject\n\n- **train_labels.csv** - file containing the target MGMT_value for each subject in the training data (e.g. the presence of MGMT promoter methylation)\n\n- **test\/** - the test files, which use the same structure as train\/; your task is to predict the MGMT_value for each subject in the test data. NOTE: the total size of the rerun test set (Public and Private) is ~5x the size of the Public test set\n\n- **sample_submission.csv** - a sample submission file in the correct format","f74048f4":"## Description","c1ea8bb5":"1. [Competition Overview](#competition-overview)  \n2. [Understanding MRI](#understanding-mri)\n3. [Libraries](#libraries)  \n4. [Weights and Biases](#weights-and-biases)\n5. [Global Config](#global-config)\n6. [Load Datasets](#load-datasets)  \n7. [Tabular Exploration](#tabular-exploration)  \n8. [Model](#model)\n9. [wandb System Metrics](#wandb-system-metrics)\n9. [References](#references)  ","f6792768":"## MRI Imaging Sequences\n\nThe most common MRI sequences are **T1-weighted** and **T2-weighted** scans. \n\n- **T1-weighted** images are produced by using short TE and TR times. The contrast and brightness of the image are predominately determined by T1 properties of tissue. \n\n- **T2-weighted** images are produced by using longer TE and TR times. In these images, the contrast and brightness are predominately determined by the T2 properties of tissue.\n\nIn general, T1- and T2-weighted images can be easily differentiated by looking the CSF. **CSF** is dark on T1-weighted imaging and bright on T2-weighted imaging.\n\nA third commonly used sequence is the **Fluid Attenuated Inversion Recovery (Flair)**. The Flair sequence is similar to a T2-weighted image except that the TE and TR times are very long. By doing so, abnormalities remain bright but normal CSF fluid is attenuated and made dark. This sequence is very sensitive to pathology and makes the differentiation between CSF and an abnormality much easier."}}