{"cell_type":{"2e91f528":"code","b5690e1d":"code","13a031c2":"code","5ccb92c1":"code","9758a3ae":"code","347e413a":"code","13123f37":"code","766acea3":"code","228d38b8":"code","e7553780":"code","f9e4c063":"code","c809a2b2":"code","ff1fb251":"code","1e34530e":"code","d0135499":"code","515ec821":"code","8112a251":"code","3103c371":"markdown"},"source":{"2e91f528":"!pip install cython\n# Install pycocotools, the version by default in Colab\n# has a bug fixed in https:\/\/github.com\/cocodataset\/cocoapi\/pull\/354\n!pip install -U 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'","b5690e1d":"from PIL import Image\nImage.open('..\/input\/pennfudan\/PennFudanPed\/PNGImages\/FudanPed00003.png')","13a031c2":"mask = Image.open('..\/input\/pennfudan\/PennFudanPed\/PedMasks\/FudanPed00003_mask.png')\n# each mask instance has a different color, from zero to N, where\n# N is the number of instances. In order to make visualization easier,\n# let's adda color palette to the mask.\nmask.putpalette([\n    0, 0, 0, # black background\n    255, 0, 0, # index 1 is red\n    255, 255, 0, # index 2 is yellow\n    255, 153, 0, # index 3 is orange\n])\nmask","5ccb92c1":"import os\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images ad masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        # because each color corresponds to a different instance\n        # with 0 being background\n        mask = Image.open(mask_path)\n\n        mask = np.array(mask)\n        # instances are encoded as different colors\n        obj_ids = np.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)","9758a3ae":"dataset = PennFudanDataset('..\/input\/pennfudan\/PennFudanPed')\ndataset[0]","347e413a":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n      \ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n\n    return model","13123f37":"!git init","766acea3":"!git clone https:\/\/github.com\/pytorch\/vision.git\n!cd vision\n\n\n","228d38b8":"!cp .\/vision\/references\/detection\/utils.py .\/\n!cp .\/vision\/references\/detection\/transforms.py .\/\n!cp .\/vision\/references\/detection\/coco_eval.py .\/\n!cp .\/vision\/references\/detection\/engine.py .\/\n!cp .\/vision\/references\/detection\/coco_utils.py .\/","e7553780":"from engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)","f9e4c063":"# use our dataset and defined transformations\ndataset = PennFudanDataset('..\/input\/pennfudan\/PennFudanPed', get_transform(train=True))\ndataset_test = PennFudanDataset('..\/input\/pennfudan\/PennFudanPed', get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)","c809a2b2":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 5 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=5,\n                                               gamma=0.1)","ff1fb251":"# let's train it for 20 epochs\nnum_epochs = 20\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=5)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)","1e34530e":"# pick one image from the test set\nimport random\nnum = random.randrange(0, 40)\nimg, _ = dataset_test[num]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])","d0135499":"prediction","515ec821":"Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())","8112a251":"Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())","3103c371":"That's it, this will make model be ready to be trained and evaluated on our custom dataset.\n\n## Training and evaluation functions\n\nIn `references\/detection\/,` we have a number of helper functions to simplify training and evaluating detection models.\nHere, we will use `references\/detection\/engine.py`, `references\/detection\/utils.py` and `references\/detection\/transforms.py`.\n\nLet's copy those files (and their dependencies) in here so that they are available in the notebook"}}