{"cell_type":{"f53604fd":"code","3fd416f1":"code","5eb3791e":"code","b9b63ffe":"code","4e9df1da":"code","0dffc24f":"code","6a36f46f":"code","520f44d1":"code","efee3076":"code","abd7e505":"code","c8554521":"code","624e9be9":"code","95d1f3a3":"code","bf86480f":"code","7196d4ac":"code","3a9f9e75":"code","12ec7031":"code","7887db78":"code","3fa2528a":"code","0e0392cb":"code","15658b3a":"code","d2912f33":"code","3ebdc4ce":"code","da12600d":"markdown","981f5fea":"markdown","699641c8":"markdown","a9cdd6c2":"markdown","1ff87b8d":"markdown","fc68b1fa":"markdown","9065b8bf":"markdown","e179e0c4":"markdown","225f6eb4":"markdown","5b857758":"markdown","e2918948":"markdown","0c5b1317":"markdown","5e230f30":"markdown","b50be0bc":"markdown","230f8eb0":"markdown","dae79287":"markdown","9505d564":"markdown","ea71a876":"markdown","4162f218":"markdown","26253dc0":"markdown"},"source":{"f53604fd":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport string, os \nimport tensorflow as tf\n\n# keras module for building LSTM \nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, Dropout, LSTM, Dense, Bidirectional \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","3fd416f1":"# csv file\ndf = pd.read_csv('..\/input\/scrapped-lyrics-from-6-genres\/lyrics-data.csv')","5eb3791e":"# first few rows\ndf.head()","b9b63ffe":"# drop\ndf.drop(['ALink','SName','SLink'],axis=1,inplace=True)","4e9df1da":"# shape\ndf.shape","0dffc24f":"# value count\ndf['Idiom'].value_counts()","6a36f46f":"# take only english songs\ndf = df[df['Idiom']=='ENGLISH']","520f44d1":"# take first 700 rows\ndf = df[:700]","efee3076":"# shape\ndf.shape","abd7e505":"df['Number_of_words'] = df['Lyric'].apply(lambda x:len(str(x).split()))\ndf.head()","c8554521":"# statistical info\ndf['Number_of_words'].describe()","624e9be9":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.figure(figsize=(12,6))\nsns.distplot(df['Number_of_words'],kde = False,color=\"red\",bins=200)\nplt.title(\"Frequency distribution of number of words for each text extracted\", size=20)","95d1f3a3":"# Tokenization\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['Lyric'].astype(str).str.lower())\n\ntotal_words = len(tokenizer.word_index)+1\ntokenized_sentences = tokenizer.texts_to_sequences(df['Lyric'].astype(str))\ntokenized_sentences[0]","bf86480f":"# Slash sequences into n gram sequence\ninput_sequences = list()\nfor i in tokenized_sentences:\n    for t in range(1, len(i)):\n        n_gram_sequence = i[:t+1]\n        input_sequences.append(n_gram_sequence)\n        \n# Pre padding\nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))","7196d4ac":"input_sequences[:10]","3a9f9e75":"# create predictors and label\nX, labels = input_sequences[:,:-1],input_sequences[:,-1]\ny = tf.keras.utils.to_categorical(labels, num_classes=total_words)","12ec7031":"# create model\nmodel = Sequential()\nmodel.add(Embedding(total_words, 40, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(250)))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nearlystop = EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\nhistory = model.fit(X, y, epochs=10, verbose=1, callbacks=[earlystop])","7887db78":"# plot the accuracy\nplt.plot(history.history['accuracy'], label='train acc')\nplt.legend()\nplt.show()\nplt.savefig('AccVal_acc')","3fa2528a":"def complete_this_song(seed_text, next_words):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        output_word = \"\"\n        for word, index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \" + output_word\n    return seed_text","0e0392cb":"complete_this_song(\"the sky is blue\", 40)","15658b3a":"complete_this_song(\"This is the beginning\", 140)","d2912f33":"complete_this_song(\"I am missing you\", 80)","3ebdc4ce":"# save model\nfrom tensorflow.keras.models import load_model\nmodel.save('song_lyrics_generator.h5')","da12600d":"Hey,what's going on.Well I am fine and here I am going to do some fun by generating my own songs lyrics\nI will going to use text generation technique.Text Generation is a type of Language Modelling problem. Language Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization.\nText Generation is one such task which can be be architectured using deep learning models, particularly Recurrent Neural Networks.","981f5fea":"# Save the model","699641c8":"Increase the epochs to increase the accuracy.I haven't done that because it is taking a very long time","a9cdd6c2":"Our function is ready so are you ready to generate your own lyrics","1ff87b8d":"Now our model is ready.So lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence.","fc68b1fa":"For the model, we need to create predictors and label. We are going to create N-grams sequence as predictors and the next word of the N-gram as label. ","9065b8bf":"Now first of all we are going to import libraries","e179e0c4":"# Data Preprocessing","225f6eb4":"I don't have much memory in this kaggle kernel.So we can't train the model on whole dataset.So I am going to take first 700 rows.If you have more memory in your local then go ahead and train the model on whole dataset","5b857758":"# Model","e2918948":"Now I want to take only english songs,so I need to subset the dataset accordingly","0c5b1317":"# Generate new song lyrics","5e230f30":"# Import Libraries","b50be0bc":"So from above we can see the max length is 656 and average song lyrics words length is 251","230f8eb0":"# Data Visualization","dae79287":"So we are going to do text generation,we don't need require others column so we will drop it.But I am not going to drop Idiom column as I have to take english songs,so this column is require for subsetting the dataset ","9505d564":"**Well that's done, in future I am going to update this notebook after learning more about text generation and don't forget to upvote it if you like it**\n","ea71a876":"### Frequency distribution of number of words","4162f218":"We will not going to create RNN model due to its vanishing gradient problem instead of that we will going to create LSTM model.LSTMs have an additional state called \u2018cell state\u2019 through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively.","26253dc0":"First of all we are going to do tokenization then we will generate sequence of n-grams.After that we will going to do padding.Padding is required because all the sentences are of different length so we need to make them of same length.We will going to do this by adding 0 in the beginning of the text with the help of pad_sequences function of keras"}}