{"cell_type":{"d3acec0a":"code","74cd7047":"code","8e2e61b5":"code","c1ae9cca":"code","0ebca2bc":"code","fae6cd16":"code","d04ea794":"code","cc11facf":"code","d435a0d4":"code","2cd46e21":"code","52b0b592":"code","be067a75":"code","a2c241a7":"code","07404caa":"code","f7c080a1":"code","911284c8":"code","ecca5f22":"code","9f3c9a88":"code","659e2f81":"code","57818bb0":"code","bb622385":"code","c8b2eda4":"code","7bae6b48":"code","70c9a19f":"code","b094a038":"code","e57ec678":"code","e0f913ea":"code","5d81dc5e":"code","6aec467d":"code","032046ac":"code","e93caee6":"code","3552f98a":"code","6c4d16b6":"code","5f3b1e90":"code","94697b0d":"code","92acb6c3":"code","33cf4fcd":"code","34f8eff6":"code","4721864e":"code","a6155f0f":"code","6ce18d3e":"code","7b099701":"code","6e1ae4d1":"code","3e027d3f":"code","4b79c6c1":"code","c152b857":"code","d9628dd2":"code","477a52d1":"code","b351f927":"code","2bb28bfe":"code","3a9ca54f":"code","433185db":"markdown","876582db":"markdown","f3d3dc01":"markdown","1565068c":"markdown","07cde170":"markdown","e2f3d57f":"markdown","c43384b8":"markdown","e84238c2":"markdown","7f3cd183":"markdown","9886b59a":"markdown","527058fe":"markdown","4bc12634":"markdown","03da65c4":"markdown","6dabdff5":"markdown","523fe8c7":"markdown","365a5241":"markdown","6559cc8c":"markdown","cc83c3f8":"markdown","b2f5aa17":"markdown","b5de389e":"markdown","96535464":"markdown","08af7500":"markdown","8de9da2b":"markdown","e9ee4e58":"markdown"},"source":{"d3acec0a":"#Let us begin with importing with all the necessary library that we'll be using\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #used for data visulization\nimport seaborn as sns  #used for data visulization\n\nimport warnings  #This will ignore all the unnecessary warnings                \nwarnings.simplefilter(\"ignore\")\n\nimport os  #Display the list of files present in the directory.         \nprint(os.listdir(\"..\/input\"))\n\n#It is used display the visulization into this notebook\n%matplotlib inline ","74cd7047":"#Loading Data\ntrain=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')\ntrain.head()","8e2e61b5":"#We can see that our target column (SalePrice) has a mean value of 180921.\n#We can also note the minimum and maximum value of that column\ntrain.describe()","c1ae9cca":"#Let us now create a histogram of our Target column.\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nsns.distplot(train['SalePrice'],bins=30,kde=True)","0ebca2bc":"#Let us have a look at all the other columns in our dataset \ntrain.info()","fae6cd16":"#Lets check whether we have any duplicate data\nsum(train['Id'].duplicated()),sum(test['Id'].duplicated())","d04ea794":"# Drop Id column from train dataset\ntrain.drop(columns=['Id'],inplace=True)","cc11facf":"#categorize the data:\n\nnum_cols=[var for var in train.columns if train[var].dtypes != 'O']\ncat_cols=[var for var in train.columns if train[var].dtypes != 'int64' and train[var].dtypes != 'float64']\n\nprint('No of Numerical Columns: ',len(num_cols))\nprint('No of Categorical Columns: ',len(cat_cols))\nprint('Total No of Cols: ',len(num_cols+cat_cols))","d435a0d4":"#Lets create a heatmap to see which all columns has null values\nplt.figure(figsize=(30,12))\nsns.heatmap(train.isnull(), yticklabels=False, cmap='viridis',cbar='cyan')","2cd46e21":"#Columns with null values in the Train dataFrame\nvar_with_na=[var for var in train.columns if train[var].isnull().sum()>=1 ]\n\nfor var in var_with_na:\n    print(var, np.round(train[var].isnull().mean(),3), '% missing values')","52b0b592":"#Columns with null values in the Test dataFrame\nvar_with_na2=[var for var in test.columns if test[var].isnull().sum()>=1 ]\n\nfor var in var_with_na2:\n    print(var, np.round(test[var].isnull().mean(),3), '% missing values')","be067a75":"#I have categorized the missing columns based on the datatypes and no of null values.\n\nmissing_cols=['MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n           'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond']\nnum_list=['LotFrontage','MasVnrArea','GarageArea','GarageCars','BsmtFinSF1','BsmtFinSF2',\n           'BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']\ndrop_cols=['PoolQC','Fence','MiscFeature','GarageYrBlt','Alley']","a2c241a7":"combine=[train,test]\n\nfor df in combine:\n    # Fill missing values in cetgorical variables with None\n    for col in missing_cols:\n        df[col]=df[col].fillna('None')\n    # Fill missing values in numerical variables with 0\n    for col in num_list:\n        df[col]=df[col].fillna(0)\n    # Drop columns with large number of missing values\n    df.drop(columns=drop_cols,inplace=True) ","07404caa":"plt.figure(figsize=(30,12))\nsns.heatmap(train.isnull(), yticklabels=False, cmap='viridis',cbar='cyan')  ","f7c080a1":"#Columns with null values in the Train dataFrame\nvar_with_na=[var for var in train.columns if train[var].isnull().sum()>=1 ]\n\nfor var in var_with_na:\n    print(var, np.round(train[var].isnull().mean(),3), '% missing values')","911284c8":"#Columns with null values in the Test dataFrame\nvar_with_na2=[var for var in test.columns if test[var].isnull().sum()>=1 ]\n\nfor var in var_with_na2:\n    print(var, np.round(test[var].isnull().mean(),3), '% missing values')","ecca5f22":"train['Electrical'].fillna('None', inplace=True)","9f3c9a88":"# Fill missing values in categorical variables in test dataset\nmode_list=['Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional','SaleType','MSZoning']\nfor col in mode_list:\n    mode=test[col].mode()\n    test[col]=test[col].fillna(mode[0])","659e2f81":"train.isnull().sum().max(),test.isnull().sum().max()","57818bb0":"plt.figure(figsize=(30,12))\nsns.heatmap(train.isnull(), yticklabels=False, cmap='viridis',cbar='cyan') ","bb622385":"plt.figure(figsize=(20,10))\nsns.heatmap(train.corr(), vmax=.8, square=True,cbar=True,cmap='RdGy');","c8b2eda4":"# Plot the correlation matrix\nimp_list=['SalePrice','OverallQual', 'YearBuilt', 'TotalBsmtSF', 'GrLivArea' ,'GarageArea',\n         'GarageCars','LotArea','PoolArea','Fireplaces','1stFlrSF','FullBath']\ncorrmat = train[imp_list].corr()\nplt.subplots(figsize=(10, 9))\nsns.heatmap(corrmat, vmax=.8, square=True,cbar=True, annot=True, fmt='.2f', annot_kws={'size': 10});","7bae6b48":"#Lets plot a boxplot on Overall Quality Vs The salesprice\nplt.figure(figsize=(10,8))\nsns.boxplot(x='OverallQual',y='SalePrice',data=train)","70c9a19f":"#Lets plot a barplot on Built Year Vs The salesprice\nplt.figure(figsize=(30,8))\nsns.barplot(x=train['YearBuilt'],y=train['SalePrice'],palette='viridis_r')","b094a038":"#Lets plot a scatterplot on Total Basement Area Vs salesprice\nplt.figure(figsize=(10,8))\nsns.scatterplot(x=train['TotalBsmtSF'],y=train['SalePrice'], alpha=0.8)","e57ec678":"index=train[train['TotalBsmtSF']>4000].index\ntrain.drop(index,inplace=True)","e0f913ea":"#Lets plot a scatterplot on Total Basement Area Vs salesprice\nplt.figure(figsize=(10,8))\nsns.scatterplot(x=train['TotalBsmtSF'],y=train['SalePrice'], alpha=0.8)","5d81dc5e":"#Lets plot a Jointplot on Carpet Area Vs The salesprice\nplt.figure(figsize=(10,8))\nsns.scatterplot(x=train['GrLivArea'],y=train['SalePrice'])","6aec467d":"# Remove outliers from Above Ground Area\nindex=train[train['GrLivArea']>4000].index\ntrain.drop(index,inplace=True)","032046ac":"#Lets plot a Jointplot on Carpet Area Vs The salesprice\nplt.figure(figsize=(10,8))\nsns.scatterplot(x=train['GrLivArea'],y=train['SalePrice'])","e93caee6":"#Lets plot a boxplot on Garage Capacity Vs The salesprice\nplt.figure(figsize=(10,8))\nsns.boxplot(x=train['GarageCars'],y=train['SalePrice'])","3552f98a":"# Removing outliers manually (More than 4-cars, less than $300k)\ntrain = train.drop(train[(train['GarageCars']>3) & (train['SalePrice']<300000)].index).reset_index(drop=True)","6c4d16b6":"plt.figure(figsize=(10,8))\nsns.boxplot(x=train['GarageCars'],y=train['SalePrice'])","5f3b1e90":"#Lets plot a line plot on Lot Area vs Salesprice\nplt.figure(figsize=(10,6))\nsns.lineplot(x=train['LotArea'],y=train['SalePrice'],palette='viridis_r')","94697b0d":"# Bivariate plot of Quality vs. Area\nplt.figure(figsize=(10,10))\nplt.subplot(3,1,1)\nsns.boxplot(data=train,y='TotalBsmtSF',x='OverallQual');\nplt.subplot(3,1,2)\nsns.boxplot(data=train,y='GarageArea',x='OverallQual');\nplt.subplot(3,1,3)\nsns.boxplot(data=train,y='GrLivArea',x='OverallQual');","92acb6c3":"#Lets plot a scatterplot on Firstfloor sqFt  Vs The salesprice\nplt.figure(figsize=(10,8))\nsns.jointplot(x=train['1stFlrSF'],y=train['SalePrice'],kind='hex')","33cf4fcd":"# Drop object features\nfor df in combine:\n    df.drop(columns=['MSZoning','SaleCondition','SaleType','PavedDrive','GarageCond','GarageQual','GarageFinish',\n                    'GarageType','FireplaceQu','Functional','KitchenQual','Heating','HeatingQC','CentralAir',\n                     'Electrical','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure',\n                     'BsmtFinType1','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Street',\n                     'LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1',\n                     'Condition2','BldgType','HouseStyle','BsmtFinType2' ],inplace=True)","34f8eff6":"# Merge the two datasets\nntrain = train.shape[0]\nntest = test.shape[0]\nall_data = pd.concat((train, test))","4721864e":"# Get dummy variables\nall_data=pd.get_dummies(all_data)","a6155f0f":"# Seperate the combined dataset into test and train data\ntest=all_data[all_data['SalePrice'].isnull()]\ntrain=all_data[all_data['Id'].isnull()]","6ce18d3e":"# Check if the new and old sizes are equal\nassert train.shape[0]==ntrain\nassert test.shape[0]==ntest","7b099701":"# Drop extra columns\ntest.drop(columns='SalePrice',inplace=True)\ntrain.drop(columns='Id',inplace=True)\ntest['Id']=test['Id'].astype(int)","6e1ae4d1":"X_train=train.drop(columns='SalePrice')\nY_train=train['SalePrice']\nX_test=test.drop(columns='Id')","3e027d3f":"Y_train.head()","4b79c6c1":"#from sklearn.ensemble import RandomForestClassifier","c152b857":"'''\n# Apply Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=1000)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest\n'''","d9628dd2":"from sklearn.linear_model import RidgeCV\n\nridge_cv = RidgeCV(alphas=(0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10))\nridge_cv.fit(X_train, Y_train)\nridge_cv_preds=ridge_cv.predict(X_test)","477a52d1":"# Apply XGBRegressor\nimport xgboost as xgb\n\nxgb = xgb.XGBRegressor(n_estimators=340, learning_rate=0.08, max_depth=2)\nxgb.fit(X_train,Y_train)\nY_pred = xgb.predict(X_test)","b351f927":"predictions = ( ridge_cv_preds + Y_pred )\/2","2bb28bfe":"final_df = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": predictions\n    })\n\nsolution = pd.DataFrame(final_df)\nsolution.head()","3a9ca54f":"# Save the dataframe to a csv file\nfinal_df.to_csv('submission.csv',index=False)","433185db":"## EDA: Exploratory Data Analysis\n\nThis step is all about understanding the data. Let us now analyse our data and look for areas where we need to work on. Upon investigating I have found couple of interesting things about our data.","876582db":"Observation: Correlation wrt Saleprice","f3d3dc01":"**Observation:** There is nothing new here, the price increases with increase in quality","1565068c":"**Observation:** We can see some outliner present. lets remove them","07cde170":"**Observations:** It seems to be there is still some null values left. Lets check again what all columns are left out...","e2f3d57f":"Observation: We can see that all the 3 factors have a positive correlation wrt to saleprice","c43384b8":"Now that we have replaced all the null values and dropped columns with major null values. Lets us see our heatmap again!","e84238c2":"Now we can say there are no null values in both of our dataFrame.","7f3cd183":"**Observation:** We can see that there is a positive correlation between the 1st Floor carpet area and the saleprice. Seems to be most people prefer to buy houses with carpet area between 800-1200 sq feet.","9886b59a":"From this whole correlation map, I have handpicked some columns which I felt is important when it comes to influencing SalePrice.","527058fe":"**Observation:** We can see there are few columns will null values\n\nLet us now get details of these columns.","4bc12634":"## **Missing Data** \n\nLet us now check for columns in our train dataframe which has missing data.\n","03da65c4":"**Observation:** Most of the houses have a basement area of 500-1500","6dabdff5":"## **House Prices: Advanced Regression Techniques**\n \n **Objective:** The purpose of this project is to predict the SalesPrice of houses based on their engineering factors.For Example- No of Garages,Carpet Area,etc. We need to find out what all factors actually influence the price of the houses.","523fe8c7":"**Observation:** There is a gradual increase in saleprice with time, apart from some few spikes inbetween due to inflation or something.","365a5241":"## Model And Prediction","6559cc8c":"This looks much better...","cc83c3f8":"Let us now Categorize out data based on the datatypes","b2f5aa17":"Lets fill up these missing values one by one.","b5de389e":"**Observation:** We can see the GrLivArea and SalePrice have a positive corelation and a large chunk of data is between 800 - 1500 sq feet.","96535464":"**Observation:** There are many columns with null values in both the dataframes. We'll try to fill these null values with suitables values. Columns with huge null values can be dropped. ","08af7500":">** Observation:**\nWe can see that with the increase in Garage capacity saleprice also increases. But there is something wrong with the Garagecars 4. So lets remove it for simplicity and correct prediction.","8de9da2b":"**Observation:** We can see that the SalePrice reaches a max peak of somewhere around $730000","e9ee4e58":"Let us try the find the correlation between each columns from the Train dataframe."}}