{"cell_type":{"83b4f63b":"code","d790b19b":"code","c0e2c6e1":"code","250e53be":"code","8c4a523d":"code","97ac5a1a":"code","2d8009ad":"code","f9d54bb9":"code","2ee5ee82":"code","b1d7607f":"markdown","0386128c":"markdown","dd6118a8":"markdown","0b8a1834":"markdown","bd8eeb08":"markdown","15e6fab4":"markdown","f634f29c":"markdown","2253ba56":"markdown","207120e2":"markdown"},"source":{"83b4f63b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d790b19b":"#importing other necessary libraries\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (20.0, 10.0) #for changing the figure size parameter of the plot\n\n#reading the data\ndata = pd.read_csv('..\/input\/headbrain\/headbrain.csv')\nprint(data.shape) #print the dimensions of the data matrix (height, width)\ndata.head() #show the first 5 values of the data","c0e2c6e1":"#collecting X(independent var) and Y(dependent variable)\nX = data['Head Size(cm^3)'].values\nY = data['Brain Weight(grams)'].values","250e53be":"#finding mean \u0233 and mean x\u0305\nmean_x = np.mean(X)\nmean_y = np.mean(Y)\n\nn = len(X) #total number of values in x; used in the for loop\n\n#using the formular, let's find m\nnumer = 0\ndenum = 0\n\nfor i in range(n):\n    numer = (X[i] - mean_x) * (Y[i] - mean_y) + numer\n    denum = ((X[i] - mean_x) ** 2) + denum\nm = numer \/ denum\nprint (m)","8c4a523d":"#finding c\nc = mean_y - (m * mean_x)\nprint(\"c is equal to\", c, \"and m is equal to\", m)","97ac5a1a":"#plotting values and regression line\n#setting the limits of the x and y axis of the plot, the 100 is added to make all the points visible\nmax_x = np.max(X) + 100 \nmin_x = np.min(X) - 100\n\n#make a linear space of 1000 evenly spaced numbers from min_x to max_x\nx = np.linspace(min_x, max_x, 1000) #this is used to plot the x axis of the regression line\ny = c + (m * x) #this is used to plot the y axis of the regression line","2d8009ad":"#plotting Scatter points\nplt.scatter(X, Y, color='#ef4423', label='Scatter points')\n#plotting line\nplt.plot(x, y, color='#52b920', label='Regression line')\nplt.xlabel('Head size in cm3')\nplt.ylabel('Brain Weight in grams')\nplt.legend()\nplt.show()","f9d54bb9":"ss_denum = 0\nss_numer = 0\nfor i in range(n): #n is the len of X from above\n    y_pred = (m * X[i]) + c\n    ss_numer += (Y[i] - y_pred) ** 2\n    ss_denum += (Y[i] - mean_y) ** 2\nr_squared = 1 - (ss_numer \/ ss_denum)\nprint(r_squared * 100, \"percent\")","2ee5ee82":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nX = X.reshape(n, 1) #we cannot use rank 1 matrices in sklearn so we reshape\n\n#model definition\nbrain_model = LinearRegression()\n\n#model fitting\nbrain_model.fit(X, Y)\n\n#model prediction\npred = brain_model.predict(X)\n\n#model evaluation\nr_squared_score = r2_score(Y, pred)\n\nprint((r_squared_score * 100), \"percent\")","b1d7607f":"# LEAST SQAURE METHOD - *finding the best fit line*","0386128c":"As you can see, it is a perfect replication of what was built from scratch.\nThere you have it, **linear regression from scratch** ","dd6118a8":"# SCIKIT LEARN IMPLEMENTATION","0b8a1834":"This project is heavily but not entirely based on the Eduregard tutorial with the 'notebook title' name.","bd8eeb08":"Now it's time to visualize the algorithm on a plot graph","15e6fab4":"Remember we are trying to see if brain weight **DEPENDS** on head size. So brain weight is **dependent** and head size is **independent**.","f634f29c":"# R-SQUARED METHOD - *goodness of fit*\n\nBy definition, this is the measure of how close the data are fitted to the regression line.\nBy formular; \n    R\u00b2 = 1 - (\u03a3(y-y\u209a)\u00b2) \/ (\u03a3(y-\u0233)\u00b2)","2253ba56":"Using the just calculated m, we can now find c by changing the subject of the linear formular\n\nc = \u0233 - mx\u0305","207120e2":"Linear Regression Model Formular:\n\n\u0233 = mx\u0305 + c\n\nwhere; *m* = \u03a3((x-x\u0305)(y-\u0233)) \/ \u03a3(x-x\u0305)\u00b2\n\nUsing *m* for in the least square method, we can find the best fitting line, the regression line."}}