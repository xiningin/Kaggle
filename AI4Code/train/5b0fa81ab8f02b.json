{"cell_type":{"01240554":"code","7c0a6648":"code","28459d27":"code","370fdd7a":"code","c594550b":"code","cc08f683":"code","290c59f3":"code","982d0b48":"code","48635078":"code","32a82b25":"code","d18c7cbc":"code","c64ee69e":"code","77b7c926":"code","870bbaa7":"code","a5ff972c":"code","ea7ecb59":"code","be1083b3":"code","bda5b07b":"code","600462a3":"code","d059eced":"code","a7ff331d":"code","b7dfe82f":"code","29d64089":"code","7c900744":"code","5bde35bc":"code","172cc26c":"code","01a51b3a":"code","e5ac5465":"code","98450962":"code","24c18b3c":"code","6bf8c430":"code","3cd355e7":"code","642cd9ee":"code","cadcbd32":"code","11f580d2":"code","bf82dad1":"code","91b6fa6a":"code","e822e314":"code","93666bac":"code","ab82d6e4":"code","5f8191d1":"code","1e99e3bb":"code","e72e6fe9":"code","957468bc":"code","2c4da64b":"code","ed2afa13":"code","f3894be3":"code","6615a396":"code","28043d0a":"code","603aa71b":"code","ef953f58":"code","afd10c3b":"code","f5d20c69":"code","91878353":"code","03d43d9c":"code","76cafba7":"code","6f0252c5":"code","3ac64316":"code","b649d2c7":"code","adabf381":"code","482ddf2e":"code","9f677744":"code","151cf539":"code","ca260af8":"code","ae0c4eb1":"code","cc01c5b6":"code","7e0a658e":"code","4e282901":"code","67fb6603":"code","bc55e46d":"code","fd5a0bcb":"code","6b9f49eb":"code","31801b6a":"code","6c317f52":"code","7ddc1e9d":"code","04359d4a":"code","0de0c68b":"code","946bf91b":"code","364a4927":"code","cd2a32d4":"code","78156425":"code","73e1d222":"code","42729042":"code","349efc4e":"code","f0cf3192":"code","b301fb8d":"code","64fca417":"code","ee2a3954":"code","c004143c":"markdown","ca7d04be":"markdown","9c6ad34e":"markdown","1fb494a4":"markdown","b20f6497":"markdown","9467199b":"markdown","373bd77e":"markdown","5e6e0975":"markdown","a2ecfd81":"markdown","75bb0d08":"markdown","62ddf094":"markdown","3af5fba9":"markdown","d0a57e1c":"markdown","c7573684":"markdown","f15a915b":"markdown","9fd30233":"markdown","f25d2520":"markdown","e97685c1":"markdown","12f3a811":"markdown","c72beb8d":"markdown","6cec983a":"markdown","256de48f":"markdown","70479813":"markdown","ada8c4cc":"markdown","a276660b":"markdown","ca3b68a6":"markdown","32dc3f33":"markdown","91d51b43":"markdown","5169218f":"markdown","a8787eb8":"markdown","97147022":"markdown","fade5fbf":"markdown","cc5309c8":"markdown","a7f81bbc":"markdown","ff6f041c":"markdown","66b761dc":"markdown","865cc7bb":"markdown","540dc951":"markdown","9ecbfc57":"markdown","8c288a36":"markdown","3178d432":"markdown","c607d9e4":"markdown","f3c400d5":"markdown","215c1d25":"markdown"},"source":{"01240554":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","7c0a6648":"# Importing relevant libararies\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas_profiling\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')","28459d27":"df = pd.read_csv(\"..\/input\/credit-card-customers\/BankChurners.csv\")","370fdd7a":"# Check the head of the dataset\ndf.head()","c594550b":"df.info()","cc08f683":"df.describe()","290c59f3":"df.shape","982d0b48":"# Checking the number of variables with object type\ndf_categorical = df.select_dtypes(exclude=['float64','int64'])\nlen(df_categorical.columns)\ndf_categorical.columns","48635078":"# Checking the number of variables with float & int type\ndf_numerical = df.select_dtypes(exclude=['object'])\nlen(df_numerical.columns)\ndf_numerical.columns","32a82b25":"# Dropping following columns as they don't look useful\n\ncol = ['CLIENTNUM','Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2']\n\ndf.drop(col,axis=1,inplace=True)\ndf.columns","d18c7cbc":"df.Attrition_Flag.value_counts()","c64ee69e":"# Checking the data imbalance\ntemp = df['Attrition_Flag'].value_counts()\ndf_1 = pd.DataFrame({'labels': temp.index,'values': temp.values})\ndf_1.iplot(kind='pie',labels='labels',values='values', title=\"% Data Imbalance\") ","77b7c926":"# Converting the target variable to binary\nbinary_map = {'Attrited Customer': 1, \"Existing Customer\": 0}\n\ndf['Attrition_Flag'] = df['Attrition_Flag'].map(binary_map)","870bbaa7":"df.Attrition_Flag.value_counts()","a5ff972c":"# Creating two different set of dataframes - for churners & non churners\n\ndf_churn = df[df.Attrition_Flag==1]\ndf_nochurn = df[df.Attrition_Flag==0]","ea7ecb59":"def pltfunction(var):\n    plt.figure(figsize=(15,5))\n    plt.subplot(1, 2, 1)\n    temp = df_churn[var].value_counts(normalize = True).mul(100)\n    df_1 = pd.DataFrame({'labels': temp.index,'values': temp.values})\n    #print(\"df_1 :{}\".format(df_1))\n    sns.barplot(x=\"labels\",y=\"values\",data=df_1)\n    plt.title('Percent of '+ '%s' %var +' for Churners', fontsize=14)\n    plt.xlabel(var)\n    plt.xticks(rotation=90)\n    plt.ylabel('% Churners')\n    plt.subplot(1, 2, 2)\n    temp_1 = df_nochurn[var].value_counts(normalize = True).mul(100)\n    df_2 = pd.DataFrame({'labels': temp_1.index,'values': temp_1.values})\n    #print(\"df_2 :{}\".format(df_2))\n    sns.barplot(x=\"labels\",y=\"values\",data=df_2)\n    plt.title('Percent of '+ '%s' %var +' for Non-Churners', fontsize=14)\n    plt.xlabel(var)\n    plt.xticks(rotation=90)\n    plt.ylabel('% Non-Churners')\n    plt.show()","be1083b3":"# Refreshing the categorical variables\ndf_categorical = df.select_dtypes(exclude=['float64','int64'])\ndf_categorical.columns","bda5b07b":"#Plotting the Gender\npltfunction('Gender')","600462a3":"#Plotting the Education level\npltfunction('Education_Level')","d059eced":"#Plotting the Education level\npltfunction('Marital_Status')","a7ff331d":"#Plotting the Income level\npltfunction('Income_Category')","b7dfe82f":"#Plotting the Income level\npltfunction('Card_Category')","29d64089":"# Refreshing the numerical variables post removing irrelavant columns\ndf_numerical = df.select_dtypes(exclude=['object'])\nlen(df_numerical.columns)","7c900744":"# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(16, 10))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)\n\nplt.show()","5bde35bc":"# Removing the columns having high correlation\ncol = ['Months_on_book','Avg_Open_To_Buy','Total_Trans_Ct']\ndf.drop(col,axis=1,inplace=True)\ndf.columns","172cc26c":"# Checking for outliers\ndf_numerical.describe(percentiles=[.25, .5, .75, .90, .95, .99])","01a51b3a":"df_numerical.columns","e5ac5465":"# Plotting boxplot\nplt.figure(figsize=(12, 12))\nplt.subplot(2,2,1)\nsns.boxplot(y = 'Credit_Limit', data = df)\nplt.subplot(2,2,2)\nsns.boxplot(y = 'Total_Amt_Chng_Q4_Q1', data = df)\nplt.subplot(2,2,3)\nsns.boxplot(y = 'Total_Ct_Chng_Q4_Q1', data = df)\nplt.show()","98450962":"df.columns","24c18b3c":"df_categorical.columns","6bf8c430":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(df[['Gender', 'Education_Level', 'Marital_Status', 'Income_Category','Card_Category']], drop_first=True)\n\n# Adding the results to the master dataframe\ndf_refined = pd.concat([df, dummy1], axis=1)\n\ndf_refined.head()","3cd355e7":"## Dropping the columns for whom dummy variables have been created\n\ncol = ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category','Card_Category']\ndf_refined.drop(col,axis=1,inplace=True)\ndf_refined.columns\n","642cd9ee":"from sklearn.model_selection import train_test_split","cadcbd32":"# Putting feature variable to X\nX = df_refined.drop(['Attrition_Flag'], axis=1)\n\nX.head()","11f580d2":"# Putting response variable to y\ny = df_refined['Attrition_Flag']\n\ny.head()","bf82dad1":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","91b6fa6a":"from sklearn.preprocessing import MinMaxScaler","e822e314":"scaler = MinMaxScaler()\n\nX_train[['Customer_Age', 'Dependent_count','Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']] = scaler.fit_transform(X_train[['Customer_Age', 'Dependent_count','Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']])\n\nX_train.head()","93666bac":"from imblearn.over_sampling import SMOTE\nsmt = SMOTE(random_state=0)\nX_train_SMOTE, y_train_SMOTE = smt.fit_sample(X_train, y_train)","ab82d6e4":"# checking the churn rate before and after applying SMOTE\nchurn_1 = (sum(y_train)\/len(y_train))*100\nchurn_2 = (sum(y_train_SMOTE)\/len(y_train_SMOTE))*100\nprint(\"Before smote the number of records {} and the churn rate is {}\".format(len(y_train),round(churn_1,2)))\nprint(\"After smote the number of records {} and the churn rate is {}\".format(len(y_train_SMOTE),round(churn_2,2)))\nprint('Number of records in X_train before SMOTE {}'.format(len(X_train)))\nprint('Number of records in X_train after SMOTE {}'.format(len(X_train_SMOTE)))","5f8191d1":"import statsmodels.api as sm","1e99e3bb":"# Logistic regression model\nlogm1 = sm.GLM(y_train_SMOTE,(sm.add_constant(X_train_SMOTE)), family = sm.families.Binomial())\nlogm1.fit().summary()","e72e6fe9":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","957468bc":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 12)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train_SMOTE, y_train_SMOTE)","2c4da64b":"rfe.support_","ed2afa13":"list(zip(X_train_SMOTE.columns, rfe.support_, rfe.ranking_))","f3894be3":"# Columns to be retained\ncol = X_train_SMOTE.columns[rfe.support_]\ncol","6615a396":"# Columns to not be considered\nX_train_SMOTE.columns[~rfe.support_]","28043d0a":"X_train_sm = sm.add_constant(X_train_SMOTE[col])\nlogm2 = sm.GLM(y_train_SMOTE,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","603aa71b":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","ef953f58":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_SMOTE[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_SMOTE[col].values, i) for i in range(X_train_SMOTE[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","afd10c3b":"# Removing Total_Amt_Chng_Q4_Q1 and updating the new defination of col\ncol = ['Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Total_Revolving_Bal',\n       'Total_Trans_Amt', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio',\n       'Gender_M', 'Income_Category_$40K - $60K',\n       'Income_Category_Less than $40K', 'Income_Category_Unknown']","f5d20c69":"# Removing Total_Amt_Chng_Q4_Q1 and running the model again\nX_train_sm = sm.add_constant(X_train_sm[col])\nlogm3 = sm.GLM(y_train_SMOTE,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","91878353":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_SMOTE[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_SMOTE[col].values, i) for i in range(X_train_SMOTE[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","03d43d9c":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred.head()","76cafba7":"y_train_pred_final = pd.DataFrame({'Churn':y_train_SMOTE.values, 'Churn_Prob':y_train_pred})\ny_train_pred_final['CustID'] = y_train_SMOTE.index\ny_train_pred_final.head()","6f0252c5":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","3ac64316":"from sklearn import metrics","b649d2c7":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","adabf381":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","482ddf2e":"y_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.5 else 0)\n\ny_train_pred_final.head()","9f677744":"# Plotting the confusion matrix\ncm2 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.final_predicted)\n\nplt.subplots(figsize=(10,10))\nax = sns.heatmap(cm2,annot=True,cmap='coolwarm',fmt='d')\nax.set_title('Prediction on training data using logistic regression',fontsize=18)\nax.set_xticklabels(['Predicted Not Churn','Predicted Churn'],fontsize=15)\nax.set_yticklabels(['Actual Not Churn','Actual Churn'],fontsize=15)\n\nplt.show() ","151cf539":"# Let's check the overall accuracy.\na = round(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.final_predicted),2)*100\nprint(\"Accuracy is {}%\".format(a))","ca260af8":"from sklearn.metrics import precision_score, recall_score","ae0c4eb1":"p = round(precision_score(y_train_pred_final.Churn, y_train_pred_final.final_predicted),2)*100 \nprint(\"Precision is {}%\".format(p))","cc01c5b6":"r = round(recall_score(y_train_pred_final.Churn, y_train_pred_final.final_predicted),2)*100\nprint(\"Recall is {}%\".format(r))","7e0a658e":"X_test[['Customer_Age', 'Dependent_count','Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']] = scaler.transform(X_test[['Customer_Age', 'Dependent_count','Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']])\n\nX_test.head()","4e282901":"# Removing the col which were identified during RFE\n\nX_test = X_test[col]\nprint(len(X_test.columns))","67fb6603":"# Adding constant\nX_test_sm = sm.add_constant(X_test)","bc55e46d":"y_test_pred = res.predict(X_test_sm)","fd5a0bcb":"y_test_pred.head()","6b9f49eb":"# Converting y_pred to a dataframe which was Pandas series\ny_pred_1 = pd.DataFrame(y_test_pred)","31801b6a":"# Let's see the head\ny_pred_1.head()","6c317f52":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","7ddc1e9d":"# Putting CustID to index\ny_test_df['CustID'] = y_test_df.index","04359d4a":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","0de0c68b":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","946bf91b":"y_pred_final.head()","364a4927":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Churn_Prob'})","cd2a32d4":"# Rearranging the columns\ny_pred_final = y_pred_final[['CustID','Attrition_Flag','Churn_Prob']]","78156425":"# Let's see the head of y_pred_final\ny_pred_final.head()","73e1d222":"# Applying the optimum threshold\ny_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)","42729042":"y_pred_final.head()","349efc4e":"# Plotting the confusion matrix\n\ncm3 = metrics.confusion_matrix(y_pred_final.Attrition_Flag, y_pred_final.final_predicted)\n\nplt.subplots(figsize=(10,10))\nax = sns.heatmap(cm3,annot=True,cmap='coolwarm',fmt='d')\nax.set_title('Prediction on testing data using logistic regression',fontsize=18)\nax.set_xticklabels(['Predicted Not Churn','Predicted Churn'],fontsize=15)\nax.set_yticklabels(['Actual Not Churn','Actual Churn'],fontsize=15)\n\nplt.show() ","f0cf3192":"# Let's check the overall accuracy.\na1 = round(metrics.accuracy_score(y_pred_final.Attrition_Flag, y_pred_final.final_predicted),2)*100\nprint(\"Accuracy is {}%\".format(a1))","b301fb8d":"p1 = round(precision_score(y_pred_final.Attrition_Flag, y_pred_final.final_predicted),2)*100 \nprint(\"Precision is {}%\".format(p1))","64fca417":"r1 = round(recall_score(y_pred_final.Attrition_Flag, y_pred_final.final_predicted),2)*100\nprint(\"Recall is {}%\".format(r1))","ee2a3954":"#Lets check the churn rate in testing data\n\nchurn_test = round((sum(y_test)\/len(y_test)),2)*100\nprint(\"Churn rate in test data {}%\".format(churn_test))","c004143c":"**Insights and Observations**\n\nAll variables have a good value of VIF. So we need not drop any variables and we can proceed with making predictions using this model only","ca7d04be":"### One hot encoding the categorical variables","9c6ad34e":"**Insights and Observation**\n\nThere is a hige drop in the precision and recall score when the model is applied on test data. The scores drop nearly by half. The plausible reason could be the data imbalance in test data","1fb494a4":"# Predicting Bank Churners using Logistic Regression","b20f6497":"**Insights\/Obervations**\n\nTotal_Amt_Chng_Q4_Q1 - There seems to be some outliers but don't think it would impact the analysis","9467199b":"### Dropping columns which do not have business relevance","373bd77e":"**Insights and Observations**\n\nThe up sampling has been done such that the number of churn cases increases from 15% to 50% and now the model can be build using the new training sets","5e6e0975":"**Insights and Observations**\n\nGraduates have a higher chance to churn compared to other education level. There is a category of unknown which needs to be classified","a2ecfd81":"## Step 6 Model Evaluation using testing data","75bb0d08":"### Reassesing the model using the above selected features","62ddf094":"## Step 4: Finding Optimal Cutoff point","3af5fba9":"### Checking the precision and recall score","d0a57e1c":"**Insights and Observations**\n\nThe precision and recall score of the model is 79% & 78% which is pretty decent result using logistic regression","c7573684":"**Insights and Observations**\n\nTotal_Amt_Chng_Q4_Q1 has high VIF as well as p value - thus removing it","f15a915b":"## Step 3 Model Building and Feature Selections","9fd30233":"### Making predictions on the test set","f25d2520":"**Insights\/Observation**\n\n1. There doesn't seem to be any null values in any of the columns except for last two which can be dropped during data preparation\n2. Clientnumber can be dropped as it doesn't have any business significance\n3. We can check for outliers in next step\n","e97685c1":"### Feature Scaling in Training set","12f3a811":"### Scaling the features of test data","c72beb8d":"**Insights\/Obervations**\n\nThere seems to be significant difference between mean and max values for the following variables:\nCredit_Limit,Total_Amt_Chng_Q4_Q1,Total_Ct_Chng_Q4_Q1\nWe inspect it further using boxplot","6cec983a":"## Step 1: Reading and Understanding the Data\n","256de48f":"### Inspecting categorical variables","70479813":"### Inspecting numerical variables","ada8c4cc":"**Insights and Observations**\n\nBlue card holders seems to dominate in the dataset so nothing very conclusive can be concluded\n","a276660b":"The model has been built using Logistic regression and using SMOTE(Synthetic Minority Over-Sampling technique) to address the problem of data imbalance.The up sampling has been done in the training dataset to increase the churn rate from **16%** to **50%**. However, these synthetic samples have not been generated for training in order to determine the true accuracy.\n\nThe precision and recall obtained on training set is **79%** & **78%** respectively. However, when the model is ran on test data the precision falls to **44%** while recall is **75%**\nThe reason for the drop in precision is due to **17%** churn rate in testing data. The cut off determined in training is low thus mis classifying the non churn customers as churn. This could be fixed by finding optimum cut off (higher than 0.5) using precision\/recall curve.","ca3b68a6":"### Calculating the churn probability","32dc3f33":"**Insights and Observations**\n\nThe p values are all 0 except for Total_Amt_Chng_Q4_Q1 - lets check the VIF value","91d51b43":"## Step 2: Data Preparation","5169218f":"**Observation**\n\nThere is disbalance in data with only 16% Attrition customer against 84%. We will be using SMOTE technique to oversample and get a better model","a8787eb8":"**Insights and Observations**\n\nSensitivity and Specificity balance each other at 0.5 probability which could be used as cut off to determine the churn & non churn cases","97147022":"**Insights and Observations**\n\nFollowing columns have high correlation and one of them can be removed as they may skew the analysis\n1. Customer Age and Months on Book\n2. Credit Limit and Average Open to Buy\n3. Total Trans amt and Total Trans Ct","fade5fbf":"### Data Balance","cc5309c8":"**Insights and Observation**\n\nThere is only 17% churned customers in test data. This could be the plausible reason","a7f81bbc":"### Feature selection using RFE","ff6f041c":"### Checking th VIFs value in order to remove multicolinearity","66b761dc":"**Insights and Observations**\n\nFew of the variables have higher p value (>0.05), thus to select the relevant predictors - using the RFE technique\n","865cc7bb":"### Checking Accuracy, Precision and Recall on the testing data","540dc951":"**Insights and Observations**\n\nMarried people tend to churn more compared to other groups","9ecbfc57":"## Step 5: Model evaluation using the training dataset","8c288a36":"**Insights and Observations**\n\nFemales churn more compared to males","3178d432":"### Test-train split and Feature scalling","c607d9e4":"**Insights and Observations**\n\nMost of the churners have less than 40K dollars","f3c400d5":"### SMOTE technique to increase samples of churned customers compared to not churned","215c1d25":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Predicting-Bank-Churners-using-Logistic-Regression\" data-toc-modified-id=\"Predicting-Bank-Churners-using-Logistic-Regression-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Predicting Bank Churners using Logistic Regression<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Reading-and-Understanding-the-Data\" data-toc-modified-id=\"Step-1:-Reading-and-Understanding-the-Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Step 1: Reading and Understanding the Data<\/a><\/span><\/li><li><span><a href=\"#Step-2:-Data-Preparation\" data-toc-modified-id=\"Step-2:-Data-Preparation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Step 2: Data Preparation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Dropping-columns-which-do-not-have-business-relevance\" data-toc-modified-id=\"Dropping-columns-which-do-not-have-business-relevance-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;<\/span>Dropping columns which do not have business relevance<\/a><\/span><\/li><li><span><a href=\"#Data-Balance\" data-toc-modified-id=\"Data-Balance-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;<\/span>Data Balance<\/a><\/span><\/li><li><span><a href=\"#Inspecting-categorical-variables\" data-toc-modified-id=\"Inspecting-categorical-variables-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;<\/span>Inspecting categorical variables<\/a><\/span><\/li><li><span><a href=\"#Inspecting-numerical-variables\" data-toc-modified-id=\"Inspecting-numerical-variables-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;<\/span>Inspecting numerical variables<\/a><\/span><\/li><li><span><a href=\"#One-hot-encoding-the-categorical-variables\" data-toc-modified-id=\"One-hot-encoding-the-categorical-variables-1.2.5\"><span class=\"toc-item-num\">1.2.5&nbsp;&nbsp;<\/span>One hot encoding the categorical variables<\/a><\/span><\/li><li><span><a href=\"#Test-train-split-and-Feature-scalling\" data-toc-modified-id=\"Test-train-split-and-Feature-scalling-1.2.6\"><span class=\"toc-item-num\">1.2.6&nbsp;&nbsp;<\/span>Test-train split and Feature scalling<\/a><\/span><\/li><li><span><a href=\"#Feature-Scaling-in-Training-set\" data-toc-modified-id=\"Feature-Scaling-in-Training-set-1.2.7\"><span class=\"toc-item-num\">1.2.7&nbsp;&nbsp;<\/span>Feature Scaling in Training set<\/a><\/span><\/li><li><span><a href=\"#SMOTE-technique-to-increase-samples-of-churned-customers-compared-to-not-churned\" data-toc-modified-id=\"SMOTE-technique-to-increase-samples-of-churned-customers-compared-to-not-churned-1.2.8\"><span class=\"toc-item-num\">1.2.8&nbsp;&nbsp;<\/span>SMOTE technique to increase samples of churned customers compared to not churned<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Step-3-Model-Building-and-Feature-Selections\" data-toc-modified-id=\"Step-3-Model-Building-and-Feature-Selections-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;<\/span>Step 3 Model Building and Feature Selections<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Feature-selection-using-RFE\" data-toc-modified-id=\"Feature-selection-using-RFE-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;<\/span>Feature selection using RFE<\/a><\/span><\/li><li><span><a href=\"#Reassesing-the-model-using-the-above-selected-features\" data-toc-modified-id=\"Reassesing-the-model-using-the-above-selected-features-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;<\/span>Reassesing the model using the above selected features<\/a><\/span><\/li><li><span><a href=\"#Checking-th-VIFs-value-in-order-to-remove-multicolinearity\" data-toc-modified-id=\"Checking-th-VIFs-value-in-order-to-remove-multicolinearity-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;<\/span>Checking th VIFs value in order to remove multicolinearity<\/a><\/span><\/li><li><span><a href=\"#Calculating-the-churn-probability\" data-toc-modified-id=\"Calculating-the-churn-probability-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;<\/span>Calculating the churn probability<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Step-4:-Finding-Optimal-Cutoff-point\" data-toc-modified-id=\"Step-4:-Finding-Optimal-Cutoff-point-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;<\/span>Step 4: Finding Optimal Cutoff point<\/a><\/span><\/li><li><span><a href=\"#Step-5:-Model-evaluation-using-the-training-dataset\" data-toc-modified-id=\"Step-5:-Model-evaluation-using-the-training-dataset-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;<\/span>Step 5: Model evaluation using the training dataset<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Checking-the-precision-and-recall-score\" data-toc-modified-id=\"Checking-the-precision-and-recall-score-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;<\/span>Checking the precision and recall score<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Step-6-Model-Evaluation-using-testing-data\" data-toc-modified-id=\"Step-6-Model-Evaluation-using-testing-data-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;<\/span>Step 6 Model Evaluation using testing data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Scaling-the-features-of-test-data\" data-toc-modified-id=\"Scaling-the-features-of-test-data-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;<\/span>Scaling the features of test data<\/a><\/span><\/li><li><span><a href=\"#Making-predictions-on-the-test-set\" data-toc-modified-id=\"Making-predictions-on-the-test-set-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;<\/span>Making predictions on the test set<\/a><\/span><\/li><li><span><a href=\"#Checking-Accuracy,-Precision-and-Recall-on-the-testing-data\" data-toc-modified-id=\"Checking-Accuracy,-Precision-and-Recall-on-the-testing-data-1.6.3\"><span class=\"toc-item-num\">1.6.3&nbsp;&nbsp;<\/span>Checking Accuracy, Precision and Recall on the testing data<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/div>"}}