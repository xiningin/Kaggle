{"cell_type":{"07b1088a":"code","7b1b13d5":"code","695ed223":"code","f79d5a54":"code","cde18ba9":"code","0d995a66":"code","c578a5d4":"code","0537fdf1":"code","85270024":"code","e6b76748":"code","e9a4de88":"code","5a57b92c":"code","e65fa2ab":"code","5ba9e109":"code","daef45bc":"code","0582fccc":"code","81ab72c4":"code","075482bc":"code","a201a245":"code","4bb92460":"code","61f3b557":"code","fdb63aa4":"code","51877a1e":"code","4f069717":"code","db6d63a7":"code","b27586bb":"code","db5dca27":"code","a4ae3679":"code","4b40df6d":"code","008e5b02":"code","78ee5358":"code","0289711f":"code","2eedd5fb":"code","80f775b3":"code","e46fb854":"code","e012d083":"code","9687b537":"code","3750ba50":"markdown","1ccf6541":"markdown","5c282a15":"markdown","61c3bcbd":"markdown"},"source":{"07b1088a":"#importing the packages\nimport numpy as np \nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, explained_variance_score, confusion_matrix, accuracy_score, classification_report, log_loss\nfrom math import sqrt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom termcolor import colored as cl","7b1b13d5":"#loading the dataset \napple = pd.read_csv(\"..\/input\/applem1-data-finalcsv\/applem1_data_final.csv\")","695ed223":"#displaying the df named apple\napple","f79d5a54":"#displaying the columns\napple.columns","cde18ba9":"#viewing the values for the status column\napple['status'].unique()","0d995a66":"#viewing the values for the domain column \napple['domain'].unique()","c578a5d4":"#viewing the values for age (in this dataset the age is grouped and encoded)\napple['age_group'].unique()","0537fdf1":"#viewing the values for income (in this dataset the age is grouped and encoded)\napple['income_group'].unique()","85270024":"#cleaning and prepping the dataset\n\n#defining df\ndf = apple\n\n#renameing columns so they are easier to work with\ndf.rename(columns={\"appleproducts_count\": \"apple_count\", \"f_batterylife\": \"f_battery\", \"f_multitasking\": \"f_multi\", \"f_performanceloss\": \"f_perloss\", \"age_group\": \"age\", \"income_group\": \"income\"}, inplace=True)\n\n#replacing values\/encoding\n#pd.get_dummies(df['column']) could be another way to do this\ndf = df.replace(['Yes'],'1')\ndf = df.replace(['No'],'0')\ndf = df.replace(['PC'],'2')\ndf = df.replace(['Apple'],'1')\ndf = df.replace(['Male'],'2')\ndf = df.replace(['Female'],'1')\ndf = df.replace(['Hp'],'2')\ndf = df.replace(['Other'],'2')\n\n#weird value 'Student ant employed' put in student group\ndf['status'] = df['status'].replace(['Student','Employed','Retired','Student ant employed','Unemployed','Self-Employed'],['0','1','2','1','3','4'])\n\n#retired is the last value (21)\ndf['domain'] = df['domain'].replace(['Science','Finance','IT & Technology','Arts & Culture','Hospitality','Politics','Social Sciences','Administration & Public Services','Education','Engineering','Marketing','Healthcare','Business','Retired','Economics','Law','Agriculture','Communication ','Realestate','Logistics','Consulting ','Retail'],['0','1','2','3','4','5','6','7','8','9','10','11','12','21','13','14','15','16','17','18','29','20'])\n\n#average value of each group instead of groups for income \ndf['income'] = df['income'].replace({1:0, 2:7500, 3:22500, 4:37500, 5:52500, 6:67500, 7:75000})\n\n#average value of each group instead of groups for age\ndf['age'] = df['age'].replace({1:18, 2:23, 3:28, 4:33, 5:38, 6:43, 7:48, 8:53, 9:58, 10:60})\n\n#reordering columns to put our (y) 'm1_purchase' at the end (to be able to easily index with -1 afterwards)\ncolumn_names = [\"trust_apple\", \"interest_computers\", \"age_computer\",\t\"user_pcmac\",\t\"apple_count\",\t\"familiarity_m1\",\t\"f_battery\",\t\"f_price\",\t\"f_size\",\t\"f_multi\",\t\"f_noise\",\t\"f_performance\",\t\"f_neural\",\t\"f_synergy\",\t\"f_perloss\",\t\"m1_consideration\",\t\"gender\",\t\"age\",\t\"income\",\t\"status\",\t\"domain\", \"m1_purchase\"]\ndf = df.reindex(columns=column_names)\n\n#displaying the new df\ndf","e6b76748":"#changing all datatypes to int & checking if columns have nulls\n#df.nunique().value_counts()        #to check the counts of each column\n#df.isnull().sum()                  #to get the null count\ndf[['trust_apple', 'user_pcmac']] = df[['trust_apple', 'user_pcmac']].astype(int)\ndf[['familiarity_m1', 'gender']] = df[['familiarity_m1', 'gender']].astype(int)\ndf[['income', 'domain', 'm1_purchase', 'status']] = df[['income', 'domain', 'm1_purchase', 'status']].astype(int)\ndf.info()","e9a4de88":"#exploring the data\n#investigating the distribution of 'm1_purchase', our (y) variable \nsns.countplot(x='m1_purchase', data = df, palette='Set3')","5a57b92c":"#exploring all the features in relation to 'm1_purchase'\n#from this visualisation we can see which columns might be too similar to be useful for our model\nfeatures = [\"trust_apple\", \"interest_computers\", \"age_computer\",\t\n            \"user_pcmac\",\t\"apple_count\",\t\"familiarity_m1\",\t\"f_battery\",\t\n            \"f_price\",\t\"f_size\",\t\"f_multi\",\t\"f_noise\",\t\"f_performance\",\t\n            \"f_neural\",\t\"f_synergy\",\t\"f_perloss\",\t\"m1_consideration\",\t\n            \"gender\",\t\"age\",\t\"income\",\t\"status\",\t\"domain\"]\n\nfor f in features:\n    sns.countplot(x = f, data = df, palette = 'Set3', hue = 'm1_purchase')\n    plt.show()","e65fa2ab":"#using seaborn to display a complete correlation materix of the features\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(21,21))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"coolwarm\")\n#RdYlGn to change theme of correlation materix ","5ba9e109":"\n#split X (dependant variable) and Y (independant variables)\nX = df.drop('m1_purchase', axis = 1).values\nY = df['m1_purchase']\nprint(X.shape)\nprint(Y.shape)","daef45bc":"#running feature importance with DecisionTreeClassifier\n#entropy is used to measure disorder in the columns\n#max_depth defines the maximum depth of the tree\/max features\ndt = DecisionTreeClassifier(random_state = 15, criterion = 'entropy', max_depth = 10)\ndt.fit(X,Y)","0582fccc":"#creating empty lists to append the values \ncol = []\nfi = []\n\n#the parameter axis=1 refer to columns, while 0 refers to rows\nfor i, column in enumerate(df.drop('m1_purchase', axis = 1)):\n  col.append(column)\n  fi.append(dt.feature_importances_[i])\n\n#displaying fi_df as a new dataframe displaying the feature importances \nfi_df = zip(col, fi)\nfi_df = pd.DataFrame(fi_df, columns = ['feature', 'feature_importance'])\n\n#sorting the feature importance in ascending order\nfi_df = fi_df.sort_values('feature_importance', ascending = False).reset_index()\n\n#the last 5 columns have no relation to y at all\nfi_df","81ab72c4":"#keep our 8 most important features (x)\ncolumns_to_keep = fi_df['feature'][0:8]\n\n#original dataframe \nprint(df.shape)\n\n#new dataframe with only the features we want (x)\nprint(df[columns_to_keep].shape)","075482bc":"#displaying 8 'most important' features that we decided to use\ncolumns_to_keep","a201a245":"#displaying the 8 most important features with data\ndf[columns_to_keep]","4bb92460":"#just to show another method of feature selection based on chi square\n#this might be a better selection, but we chose to go with the other one\n\n#feature selection based on chi square \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nX = df.iloc[:,0:21]  #independent columns\ny = df.iloc[:,-1]    #target column i.e price range\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Features','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","61f3b557":"#split the data into X & y\n#y = np.array(df[\"Height\"]).reshape(-1,1)\n\nX = df[columns_to_keep].values\nX.astype(int)\n#X = X.reshape(-1,1)\n\ny = np.array(df['m1_purchase']).reshape(-1,1)\ny = y.astype(int)\n\nprint(X.shape)\nprint(y.shape)","fdb63aa4":"#creating a training and test set \n#the train and test size is set to the same size as it gave the best accuracy score\nx_train, x_test, y_train, y_test = train_test_split(X, y, train_size = 0.20, test_size = 0.20,random_state=25)\n\n#scaling and normalizing the data (this also imporved the score)\nsc = StandardScaler() \nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","51877a1e":"#checking the shape of the train and test set\nprint(y_train.shape)\nprint(x_train.shape)\nprint(y_test.shape)\nprint(x_test.shape)","4f069717":"#training my model \nlog_reg = LogisticRegression(random_state=20, multi_class='multinomial')\nlog_reg.fit(x_train, np.ravel(y_train,order='C'))","db6d63a7":"#checking the accuray of our model\n#'score' returns the mean accuracy on the given test data and label\n\n#the training accuracy\nscore_train = log_reg.score(x_train, y_train)\n\n#the testing accuracy\nscore_test = log_reg.score(x_test, y_test)\n\n#after tuning the random state of log_reg and traintestsplit this is the best accuracy that we got\nprint(f'The Training Accuracy is: {round(score_train*100, 2)} %')\nprint(f'The Testing Accuracy is: {round(score_test*100, 2)} %')","b27586bb":"#with k - fold validation we receive a testing accuracy of 87% which is higher than the one achived on the normal test set\nkfold = model_selection.KFold(n_splits=10)\nmodel_kfold = LogisticRegression()\nresults_kfold = model_selection.cross_val_score(model_kfold, x_test, np.ravel(y_test,order='C'), cv=kfold)\nprint(f'The K-fold Testing Accuracy Is: {round(results_kfold.mean()*100.0, 2)} %')","db5dca27":"#print the tunable parameters\nparams = log_reg.get_params()\nprint(params)","a4ae3679":"#making predictions\n#specific predictions can also be made by indexing\ny_pred = log_reg.predict(x_test)","4b40df6d":"#predicted values based on the training set \nprint(f'\\nPredicted m1 purchases: {y_pred}')","008e5b02":"#actual values of y \nprint(f'Real m1 purchases: {y_test.ravel()}')","78ee5358":"#printing the model parameters \nprint('Intercept: \\n', log_reg.intercept_)\nprint('Coefficients: \\n', log_reg.coef_)","0289711f":"#shape of coefficient and intercept \nprint(log_reg.coef_.shape) \nprint(log_reg.intercept_.shape) ","2eedd5fb":"#the predicted probabilities of data instances being classified as 1 (customers will purchase m1) when the model is applied on the test set\nlog = log_reg.predict_proba(x_test)[:,1].reshape(-1,1)\ndf_prob = pd.DataFrame(log, columns = ['probability_of_purchase'])\ndf_prob.probability_of_purchase = (round(df_prob.probability_of_purchase * 100, 2)).astype(str) + '%'\ndf_prob.sort_values('probability_of_purchase', ascending = False).reset_index()","80f775b3":"#displaying the 5 most predicted most probable puchasers (data instances) in a dataframe with their features \n#the classification for purchaser3 and purchaser4 is likely inaccurate\nuser1 = df.iloc[26,:]\nuser2 = df.iloc[12,:] \nuser3 = df.iloc[7,:] \nuser4 = df.iloc[17,:] \nuser5 = df.iloc[18,:]\n\nusers = zip(user1, user2, user3, user4, user5)\nusers = pd.DataFrame(users, columns = ['purchaser1', 'purchaser2', 'purchaser3', 'purchaser4', 'purchaser5'])#.reset_index()\nusers = users.rename(index={0: 'trust_apple', 1: 'interest_computers', 2: 'age_computer', 3: 'user_pcmac', 4: 'apple_count', 5: 'familiarity_m1', 6: 'f_battery', 7: 'f_price', 8: 'f_size', 9: 'f_multi', 10: 'f_noise', 11: 'f_performance', 12: 'f_neural', 13: 'f_synergy', 14: 'f_perloss', 15: 'm1_consideration', 16: 'gender', 17: 'age', 18: 'income', 19: 'status', 20: 'domain', 21: 'm1_purchase'})\nusers","e46fb854":"#confusion materix \nconfmtrx = np.array(confusion_matrix(y_test, y_pred))\n\n\ncm = pd.DataFrame(confmtrx, index=['Actual Not Buyers', 'Actual Buyers'],\ncolumns=['Predicted Not Buyers', 'Predicted Buyers'])\ncm","e012d083":"#calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\n\nFP = confmtrx.sum(axis=0) - np.diag(confmtrx)\nFN = confmtrx.sum(axis=1) - np.diag(confmtrx)\nTP = np.diag(confmtrx)\nTN = confmtrx.sum() - (FP + FN + TP)\n\n#true positive rate (sensitivity)\nTPR = TP \/ (TP + FN)\n\n#true negative (precision)\nTNR = TP \/ (TP + FP)\n\n#false positive rate or False alarm rate\nFPR = FP \/ (FP + TN)\n\n#false negative rate or Miss Rate\nFNR = FN \/ (FN + TP)\n\n#the averages\nprint(f'The average True Positive Rate is: {round((TPR.sum()\/2)*100, 2)} %')\nprint(f'The average True Negative Rate is: {round((TNR.sum()\/2)*100, 2)} %')\nprint(f'The average False Positive Rate is: {round((FPR.sum()\/2)*100, 2)} %')\nprint(f'The average False Negative Rate is: {round((FNR.sum()\/2)*100, 2)} %')\n","9687b537":"#calculating the logarithmic loss of our model\n#as log loss measurer uncertainty -> a low value equals a model of low uncertainty which is good\nprint(cl('The Log Loss of the Model is {}'.format(log_loss(y_test, y_pred).round(2))))","3750ba50":"**Applying Feature Selection (dropping columns)**\n\nIn the following section **feature selection** methodology is applied to be able to find the columns (X) that have the strongest relation to (y). The goal of this is to figure out which columns to drop in the dataset before fitting the data to the logsitic regression algorithm. In this example, an algorithm from 'DecisionTreeClassifier' is used to give each feature a score in relation to (y).\nThe score is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature. Below are some general benefits of applying feature selection. \n\nFeature Selection: \n*   Reduces Overfitting: less redundant data means less opportunity to make decisions based on noise.\n*   Improves Accuracy: less misleading data means modeling accuracy improves.\n*   Reduces Training Time: fewer data points reduce algorithm complexity and algorithms train faster.","1ccf6541":"# Predictive Marketing Classification using Logistic Regression\nThe goal of this project is to predict new buyers of one of the new m1 macs that apple first released last november. To collect the data, a survey has been conducted to gather relevant customer data. The dataset consists of a number of features relevant to a m1 purchase, such as current apple product and interest in computers, in addition to personal information like age and occupation. The aim of the analysis is to understand which features have the stongest relation to the (y), and to to use them (x) to fit a model that can predict if new customers will buy one of the new m1 macbooks or not. The algorithm used will be Logistic Regression from sklearn. ","5c282a15":"In the small training and testing sets used, the algorithm classified the data instance **wrong 5\/27** and **correct 22\/27** times as shown in the confusion matrix. To further increase the accuracy of this model, hyper parameter tuning would need to be applied. It is also worth noting that even with optimal parameters the model might not perform better due to the data being insufficient. ","61c3bcbd":"**Algorithm: Logistic Regression (Classification)**\n\nAs our input is one y ('m1_purchase'), and multiple x, the logistic regression algorithm can help us predict which customers (which features) are most likely to buy the new m1 mac. The output will be a probability for each feature to be 0 or 1, where 0 = False and 1 = True.\n\n\nLogistic regression is a commonly used algorithm for classification problems that works by fitting an 'S' shaped function on the data.  Specifically, through a Sigmoid function, the data is normalized and given a percentage (%) between 0 and 1 of the feature being classified in one class. \n\n\n\n\n"}}