{"cell_type":{"e9139c39":"code","5f89b736":"code","16088f40":"code","2edc4cad":"code","3e83ccfe":"code","19aebd2a":"code","9ace150f":"code","da4c5661":"code","d067fbf2":"code","d470f979":"markdown"},"source":{"e9139c39":"#1. import all packages\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport math\nfrom collections import Counter \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import multilabel_confusion_matrix\nfrom statistics import mean\n","5f89b736":"#2. load testing and training dataset\n\ndf_train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\",header=0)\ndf_test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\",header=0)\ndf_sample_submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\",header=0)\n\ndf_train.head(10)","16088f40":"all_label = set(df_train['label'].tolist())\nprint(\"All labels: {0}\".format(all_label))\n\nall_label_sub = set(df_sample_submission['Label'].tolist())\nprint(\" 'sample_submission.csv' pnly has one class: {0}\".format(all_label_sub))\n\ndst_train = Counter(df_train['label'])\nprint(dst_train)\n\nsns.countplot(df_train['label'])","2edc4cad":"#3. Share to test and train data \nx = df_train.iloc[:, 1:]\ny = df_train['label'].tolist()\n\nx_test = x.iloc[0:10000, :].values.astype('float32')\ny_test = y[0:10000]\nx_train = x.iloc[10000:, :].values.astype('float32')\ny_train = y[10000:]\n\n#df_train.iloc[0:1, 1:].values","3e83ccfe":"#4. Previes 10 image data\n\n%matplotlib inline\nplt.figure(figsize=(12,10))\nx_cor, y_cor = 4, 4\nfor i in range(10):\n    arr_img = np.asarray(x_train[i].reshape((28,28)));\n    plt.subplot(y_cor, x_cor, i+1)\n    plt.imshow(arr_img, cmap='gray')\n    \n    plt.show()","19aebd2a":"#5. Feature normalization for both (test & train)\n\nprint((min(x_train[2]), max(x_train[2])))\n\nx_train = x_train\/255.0\nx_test = x_test\/255.0\n\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')","9ace150f":"#6. Train random forest algorithm\n#model_clf = RandomForestClassifier(n_estimators=500, max_depth=2, random_state=0)\n#model_clf.fit(x_train, y_train)\n#y_pred = model_clf.predict(x_test)\n\nmodel_clf = Perceptron(max_iter=6000, eta0=100, random_state=0)\nmodel_clf.fit(x_train, y_train)\ny_pred = model_clf.predict(x_test)","da4c5661":"#7. Multiclass confusion matrix for each class\n\nlst_actual_class = y_test\nlst_predicted_class = y_pred\n\nlst_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\narr_out_matrix = multilabel_confusion_matrix(lst_actual_class, lst_predicted_class, labels=lst_classes)\n\nstore_sens = [];\nstore_spec = [];\nstore_acc = [];\nstore_bal_acc = [];\nstore_prec = [];\nstore_fscore = [];\nstore_mcc = [];\n\nfor no_class in range(len(lst_classes)):\n    arr_data = arr_out_matrix[no_class];\n    print(\"Predicted performance of digit label\/class: {0}\".format(no_class));\n    \n    tp = arr_data[1][1]\n    fp = arr_data[0][1]\n    tn = arr_data[0][0]\n    fn = arr_data[1][0]\n    \n    sensitivity = round(tp\/(tp+fn), 3);\n    specificity = round(tn\/(tn+fp), 3);\n    accuracy = round((tp+tn)\/(tp+fp+tn+fn), 3);\n    balanced_accuracy = round((sensitivity+specificity)\/2, 3);\n    precision = round(tp\/(tp+fp), 3)\n    fscore = round((2 * ((precision * sensitivity) \/ (precision + sensitivity))), 3)\n    mcc = round((((tp * tn)-(fp * fn))\/ math.sqrt((tp + fp)*(tp + fn)*(tn + fp)*(tn + fn))),3)\n\n    store_sens.append(sensitivity);\n    store_spec.append(specificity);\n    store_acc.append(accuracy);\n    store_bal_acc.append(balanced_accuracy);\n    store_prec.append(precision);\n    store_fscore.append(fscore);\n    store_mcc.append(mcc);\n    \n    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n    print(\"Sensitivity: {0}\".format(sensitivity));\n    print(\"Specificity: {0}\".format(specificity));\n    print(\"Accuracy: {0}\".format(accuracy));\n    print(\"Balanced Accuracy: {0}\".format(balanced_accuracy));\n    print(\"Precision: {0}\".format(precision));\n    print(\"F1-Score: {0}\".format(fscore))\n    print(\"MCC: {0}\\n\".format(mcc))","d067fbf2":"#8. Overall final prediction performance\n\nprint(\"Overall Performance Prediction:\");\nprint(\"Sensitivity: {0}%\".format(round(mean(store_sens)*100, 4)));\nprint(\"Specificity: {0}%\".format(round(mean(store_spec)*100, 4)));\nprint(\"Accuracy: {0}%\".format(round(mean(store_acc)*100, 4)));\nprint(\"Balanced Accuracy: {0}%\".format(round(mean(store_bal_acc)*100, 4)));\nprint(\"Precision: {0}%\".format(round(mean(store_prec)*100, 4)));\nprint(\"F1-Score: {0}%\".format(round(mean(store_fscore)*100, 4)))\nprint(\"MCC: {0}\\n\".format(round(mean(store_mcc), 4)))","d470f979":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session"}}