{"cell_type":{"77dbe27f":"code","297c1396":"code","2c5bb0fe":"code","f55b216f":"code","fe0bd3d8":"code","45c99a7d":"code","ea7c56ef":"code","38a79f25":"code","cb86df7e":"code","1b29f71f":"code","026ffa1e":"code","bab265d5":"code","9f87ad5b":"code","990457fa":"code","b1bdf565":"code","21a8dfb0":"code","aec4d4db":"code","303e5615":"code","63af906c":"code","a683bc92":"code","75788e37":"code","c5f9406a":"code","7e232856":"code","5d27d608":"code","b0341d95":"code","8bf9f8bb":"code","fa7396df":"code","8a8581eb":"code","3a04d64e":"code","3ad36b3a":"code","a07461b3":"code","013ba885":"code","343cfc5e":"code","e80b6bd3":"code","5b52e4d0":"code","9580f7a6":"code","e3ba4a52":"code","84568139":"code","843d5496":"code","d70cb03d":"code","c030f57a":"code","7def6df7":"code","803c545a":"code","e019c87c":"code","fdb13d76":"code","ddb642f5":"code","97619c75":"code","0b22fc87":"code","a69910a0":"markdown","b9e8074c":"markdown","d5378c8a":"markdown","d1ff4f8b":"markdown","6fe31760":"markdown","4cbac384":"markdown","f22fea1a":"markdown","abfc3a31":"markdown","e182f14f":"markdown","34fca99a":"markdown","960baa00":"markdown","3492be57":"markdown","68d6235a":"markdown","5fc5e2d0":"markdown","71c9957e":"markdown","c5d55bd7":"markdown","5398dcac":"markdown","8c52e1f0":"markdown","72a432a3":"markdown","bab5d6bc":"markdown","2548b7bb":"markdown","f1f69291":"markdown","a79ddb56":"markdown","c3322162":"markdown","bb182c3e":"markdown"},"source":{"77dbe27f":"import os\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom gensim.models import Word2Vec\n\nimport plotly.express as px\n\nfrom tqdm import tqdm","297c1396":"train = pd.read_csv('..\/input\/data-science-winter-osaka2\/train.csv')\ntest = pd.read_csv('..\/input\/data-science-winter-osaka2\/test.csv')","2c5bb0fe":"def eval_data(row):\n    if row == row:\n        return eval(row)\n    else:\n        return np.nan\n\nfor column in ['popular_tags', 'categories', 'minimum_requirements', 'recommended_requirements']:\n    train[column] = train[column].apply(eval_data)\n    test[column] = test[column].apply(eval_data)","f55b216f":"developer_train = train['developer'].value_counts().reset_index()","fe0bd3d8":"df = pd.concat([train,test], axis=0)\ndf = df.groupby('developer')['name'].count().reset_index().rename(columns={'name':'count_developer'})","45c99a7d":"train = pd.merge(train, df, on='developer', how='left')\ntest = pd.merge(test, df, on='developer', how='left')","ea7c56ef":"train.groupby('user_reviews')['count_developer'].agg(['mean', 'std', 'median'])","38a79f25":"fig = px.histogram(train[(train['count_developer'] > 20)&(train['user_reviews'] != 'c2')], x='count_developer', \n             color='user_reviews',\n             barmode='overlay')\n\nfig.show()\n\nfig = px.histogram(train[(train['count_developer'] > 20)&(train['user_reviews'] == 'c2')], x='count_developer', \n             color='user_reviews',\n             barmode='overlay')\n\nfig.show()","cb86df7e":"df = pd.concat([train,test], axis=0)\ndf = df.groupby('publisher')['name'].count().reset_index().rename(columns={'name':'count_publisher'})\n\ntrain = pd.merge(train, df, on='publisher', how='left')\ntest = pd.merge(test, df, on='publisher', how='left')","1b29f71f":"train['developer'].value_counts()\ndeveloper_train = developer_train[developer_train['developer'] > 20]\nlen(developer_train)\ndeveloper_name = developer_train['index'].tolist()","026ffa1e":"pivot_train = pd.pivot_table(train[train['developer'].isin(developer_name)], \n                             columns='user_reviews', index='developer', \n                             values='name', aggfunc='count')\npivot_train","bab265d5":"pivot_train['count'] = pivot_train.sum(axis=1)\nfor name in ['c0', 'c1', 'c2']:\n    pivot_train[name] = pivot_train[name] \/ pivot_train['count']\npivot_train = pivot_train.drop(columns='count')\npivot_train = pivot_train.fillna(0)\npivot_train","9f87ad5b":"fig = px.imshow(pivot_train, aspect=\"auto\")\nfig.show()","990457fa":"train['price'] = train['price'].replace({'free':0})\ntrain['price'] = train['price'].fillna(0)\ntrain['price'] = train['price'].astype(float)","b1bdf565":"px.violin(train, y='price', color='user_reviews', box=True) ","21a8dfb0":"df = pd.concat([train, test], axis=0).reset_index(drop=True)\ndf['price'] = df['price'].replace({'free':0})\ndf['price'] = df['price'].fillna(0)\ndf['price'] = df['price'].astype(float)\ndf = df.groupby('developer')['price'].agg(['mean', 'std', 'median']).fillna(0)\ndf = df.add_prefix('price_developer_').reset_index()","aec4d4db":"train = pd.merge(train, df, on='developer', how='left')\ntest = pd.merge(test, df, on='developer', how='left')","303e5615":"# \u5e73\u5747\u5024\ntrain.groupby('user_reviews')['price_developer_mean'].agg(['mean', 'std', 'median', 'count'])","63af906c":"# \u4e2d\u592e\u5024\ntrain.groupby('user_reviews')['price_developer_median'].agg(['mean', 'std', 'median', 'count'])","a683bc92":"px.violin(train, y='price_developer_mean', color='user_reviews', box=True) ","75788e37":"df = pd.concat([train, test], axis=0).reset_index(drop=True)\ndf['price'] = df['price'].replace({'free':0})\ndf['price'] = df['price'].fillna(0)\ndf['price'] = df['price'].astype(float)\ndf = df.groupby('publisher')['price'].agg(['mean', 'std', 'median']).fillna(0)\ndf = df.add_prefix('price_publisher_').reset_index()","c5f9406a":"train = pd.merge(train, df, on='publisher', how='left')\ntest = pd.merge(test, df, on='publisher', how='left')","7e232856":"px.violin(train, y='price_publisher_mean', color='user_reviews', box=True) ","5d27d608":"pd.pivot_table(train, \n                             columns='user_reviews', index='publisher', \n                             values='name', aggfunc='count')\n","b0341d95":"train['is_train'] = True\ntest['user_reviews'] = np.nan\ntest['is_train'] = False\n\ndf = pd.concat([train, test], axis=0)\ndf = df.reset_index(drop=True)\n","8bf9f8bb":"def category_numeric_preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    # \u30bf\u30b0\u306e\u6570\u306b\u3088\u3063\u3066\u8a55\u4fa1\u304c\u5909\u308f\u308b\uff1f\n    df['popular_tags_len'] = df['popular_tags'].fillna(\"\").apply(len)\n    df['categories_len'] = df['categories'].fillna(\"\").apply(len)\n    \n    # \u5024\u6bb5\u306b\u3088\u3063\u3066\u8a55\u4fa1\u304c\u5909\u308f\u308b\uff1f\n    df = price_preprocess(df)\n    \n    # developer\u3068publisher\u3092label encode\u3059\u308b\u3002\n    for column in ['developer', 'publisher']:\n        df = apply_label_encode(df, column)\n    \n    # \u5e74\u306b\u3088\u3063\u3066\u8a55\u4fa1\u304c\u5909\u308f\u308b\uff1f\n    # \u6b63\u898f\u8868\u73fe\u3067\u5e74\u3092\u62bd\u51fa\u3059\u308b\u3002\n    df['year'] = df['release_date'].str.extract(r'(\\d{4})')\n    df['year'] = df['year'].astype(float)\n    return df\n\ndef price_preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    df['price'] = df['price'].replace({'free':0})\n    price_null_index = df[df['price']==''].index\n    df.loc[price_null_index, 'price'] = -1\n    df['price'] = df['price'].astype(float)\n    return df\n\ndef apply_label_encode(df:pd.DataFrame, column:str)-> pd.DataFrame:\n    label_df = df[~df[column].isnull()]\n    le = LabelEncoder()\n    le_data = le.fit_transform(label_df[column])\n    label_df_index = label_df.index\n    df.loc[label_df_index, f'{column}_label_encoding'] = le_data\n    return df","fa7396df":"df = category_numeric_preprocess(df)","8a8581eb":"def tfidf_preprocess(df: pd.DataFrame, column: str, embedding_dim: int=20)-> pd.DataFrame:\n    # tfidf\u3067\u5909\u63db\u3057\u305f\u5f8c\u306b\u3001SVD\u3067\u6642\u9650\u524a\u6e1b\u3059\u308b\u3002\n    tfidf_vec = TfidfVectorizer(ngram_range=(1,2))\n    text_tfidf = tfidf_vec.fit_transform(df[column].fillna('').values.tolist())\n    svd = TruncatedSVD(n_components=embedding_dim, algorithm='arpack',random_state=9999)\n    return svd.fit_transform(text_tfidf)\n\nclass W2VSWEM(object):\n    def __init__(self, word_list: List[str], embedding_dim:int=20):\n        # word2vec\u3067\u7279\u5fb4\u91cf\u3092\u4f5c\u3063\u3066\u3001wrod2vec\u306e\u5e73\u5747\u5024\u3092\u8a08\u7b97\u3059\u308b\u3002\n        self.word_list = word_list\n        self.embedding_dim = embedding_dim\n        self.__validation_type()\n        self.__train_w2v()\n        \n    def __validation_type(self):\n        if isinstance(self.word_list, pd.Series):\n            self.word_list = self.word_list.tolist()\n        if not isinstance(self.word_list, list):\n            raise TypeError(f'you should use list object, however you are using {type(word_list)}.')\n        \n    def __train_w2v(self):\n        self.w2v_model = Word2Vec(self.word_list, vector_size=self.embedding_dim, \n                                  workers=1, seed=1234)\n        self.vocab = self.w2v_model.wv.key_to_index\n        \n    def _get_single_column_vec(self, words) -> np.array:\n        result = []\n        \n        for word in words:\n            if word in self.vocab:\n                \n                vector = self.w2v_model.wv[word]\n                result.append(vector)\n        result = np.array(result)\n        return result\n    \n    def get_result(self):\n        self.swem_result = np.zeros([len(self.word_list), self.embedding_dim])\n        for num, i in enumerate(self.word_list):\n            swem = self._get_single_column_vec(i)\n            if len(swem):\n                swem = np.mean(swem, axis=0)\n            else:\n                swem = np.zeros(self.embedding_dim)\n            self.swem_result[num, :] = swem    ","3a04d64e":"# word2vec\u3067tags\u3092encode\u3059\u308b\ntags_df = []\nfor column in ['categories', 'popular_tags']:\n    df[column] = df[column].fillna('')\n    w2v_swem = W2VSWEM(df[column].tolist())\n    w2v_swem.get_result()\n    result = w2v_swem.swem_result\n    _df = pd.DataFrame(result, columns=[f'w2v_{column}_{x}' for x in range(result.shape[1])])\n    tags_df.append(_df)\ntags_df = pd.concat(tags_df, axis=1)","3ad36b3a":"# \u30b2\u30fc\u30e0\u306e\u8aac\u660e\u3092tfidf\u3067\u7279\u5fb4\u91cf\u306b\u3059\u308b\ntext_svd = tfidf_preprocess(df, 'description')\ntext_svd_df = pd.DataFrame(text_svd, columns=[f'text_svd_{x}' for x in range(text_svd.shape[1])])","a07461b3":"nlp_df = pd.concat([tags_df, text_svd_df], axis=1)","013ba885":"df.columns","343cfc5e":"use_df = df.copy()\nuse_columns = ['name', 'price', 'popular_tags_len', 'categories_len', 'year', 'user_reviews', 'is_train', \n               'count_developer', 'price_developer_mean', 'price_developer_std',  'count_publisher', 'price_publisher_mean','price_publisher_std']\nuse_df = use_df[use_columns]\n# 'publisher_label_encoding', ","e80b6bd3":"use_df = pd.concat([use_df, nlp_df], axis=1)","5b52e4d0":"train = use_df[use_df['is_train']==True].reset_index(drop=True)\ntest =  use_df[use_df['is_train']!=True].reset_index(drop=True)","9580f7a6":"train = train.drop(columns=['is_train'])\ntest = test.drop(columns=['is_train', 'user_reviews'])","e3ba4a52":"train.shape, test.shape","84568139":"X = train.copy().drop(columns='user_reviews')\ny = train['user_reviews']","843d5496":"y = y.map({'c0':0, 'c1':1, 'c2':2})","d70cb03d":"from typing import Optional, List\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\n\nclass TreeModel:\n    \"\"\"LGB\/XGB\u306eAPI\u3092\u7d71\u4e00\u3057\u305fwrapper\"\"\"\n\n    def __init__(self, model_type: str):\n        self.model_type = model_type\n        self.trn_data = None\n        self.val_data = None\n        self.model = None\n\n    def train(self,\n              params: dict,\n              X_train: pd.DataFrame,\n              y_train: np.ndarray,\n              X_val: pd.DataFrame,\n              y_val: np.ndarray,\n              train_weight: Optional[np.ndarray] = None,\n              val_weight: Optional[np.ndarray] = None,\n              train_params: dict = {}):\n        if self.model_type == \"lgb\":\n            self.trn_data = lgb.Dataset(X_train, label=y_train, weight=train_weight)\n            self.val_data = lgb.Dataset(X_val, label=y_val, weight=val_weight)\n            self.model = lgb.train(params=params,\n                                   train_set=self.trn_data,\n                                   valid_sets=[self.trn_data, self.val_data],\n                                   **train_params)\n        elif self.model_type == \"xgb\":\n            self.trn_data = xgb.DMatrix(X_train, y_train, weight=train_weight, enable_categorical=True)\n            self.val_data = xgb.DMatrix(X_val, y_val, weight=val_weight, enable_categorical=True)\n            self.model = xgb.train(params=params,\n                                   dtrain=self.trn_data,\n                                   evals=[(self.trn_data, \"train\"), (self.val_data, \"val\")],\n                                   **train_params)\n        else:\n            raise NotImplementedError\n        return self.model\n\n    def predict(self, X: pd.DataFrame):\n        if self.model_type == \"lgb\":\n            return self.model.predict(X, num_iteration=self.model.best_iteration)  # type: ignore\n        elif self.model_type == \"xgb\":\n            X_DM = xgb.DMatrix(X)\n            return self.model.predict(X_DM)  # type: ignore\n        else:\n            raise NotImplementedError\n\n    @property\n    def feature_names_(self):\n        if self.model_type == \"lgb\":\n            return self.model.feature_name()\n        elif self.model_type == \"xgb\":\n            return list(self.model.get_score(importance_type=\"gain\").keys())\n        else:\n            raise NotImplementedError\n\n    @property\n    def feature_importances_(self):\n        if self.model_type == \"lgb\":\n            return self.model.feature_importance(importance_type=\"gain\")\n        elif self.model_type == \"xgb\":\n            return list(self.model.get_score(importance_type=\"gain\").values())\n        else:\n            raise NotImplementedError","c030f57a":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\n\n\nfeature_importances = pd.DataFrame()\nscores=0.0\nnum_fold=5\noof = pd.DataFrame()\n    \nskf = StratifiedKFold(n_splits=num_fold, random_state=1213, shuffle=True)\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(\"*\" * 100)\n    print(f\"Fold: {fold}\")\n\n    X_trn = X.iloc[trn_idx].drop(columns='name')\n    X_val = X.iloc[val_idx].drop(columns='name')\n    y_trn = y.iloc[trn_idx].reset_index(drop=True)\n    y_val = y.iloc[val_idx].reset_index(drop=True)\n        \n\n    model = TreeModel(model_type='lgb')\n    params = {\n                \"objective\": \"softmax\",\n                \"boosting\": \"gbdt\",\n                \"num_class\":3,\n                \"learning_rate\": 0.1,\n                \"seed\": 1213,\n                'metric': 'multi_logloss',\n                'num_leaves': 5,\n                'verbose': 1,\n                'lambda_l2': 3,\n            }\n    model.train(params=params,\n                X_train=X_trn,\n                y_train=y_trn.values.astype('float32'),\n                X_val=X_val,\n                y_val=y_val.values.astype('float32'),\n                train_params={\n                                \"num_boost_round\": 20000,\n                                \"early_stopping_rounds\": 100,\n                                \"verbose_eval\": 1000,\n                })\n    fi_tmp = pd.DataFrame()\n    fi_tmp[\"feature\"] = model.feature_names_\n    fi_tmp[\"importance\"] = model.feature_importances_\n    fi_tmp[\"fold\"] = fold\n    feature_importances = feature_importances.append(fi_tmp)\n\n    val_pred = model.predict(X_val)\n    score = log_loss(y_val, val_pred)\n    scores += score \/ num_fold\n\n    pred = model.predict(test.drop(columns='name'))\n    if fold == 0:\n        prediction = np.copy(pred) \/ num_fold\n    else:\n        prediction += pred \/ num_fold\n    print(f\"score: {score:.5f}\")\n    oof = oof.append(pd.DataFrame({\"name\": X.loc[val_idx, \"name\"], \"preds_c0\": val_pred[:, 0], \n                                   \"preds_c1\": val_pred[:, 1],\"preds_c2\": val_pred[:, 2]}))\nprint(f'average_score: {scores:.5f}')","7def6df7":"oof['prediction'] = np.argmax(np.array(oof.iloc[:, 1:]), axis=1)","803c545a":"from sklearn.metrics import accuracy_score\naccuracy_score(y, oof.sort_index()['prediction'])","e019c87c":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\norder = list(feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 30))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=order)\nplt.title(f\"LGBM importance, average auc score {score}\")\nplt.tight_layout()","fdb13d76":"test_prediction = np.argmax(prediction, axis=1)\npred_df = pd.DataFrame(test_prediction)\npred_df = pred_df[0].map({0:'c0', 1:'c1', 2:'c2'})","ddb642f5":"pred_df","97619c75":"sub = pd.read_csv('..\/input\/data-science-winter-osaka2\/sample_submission.csv')\nsub['user_reviews'] = pred_df.tolist()\nsub","0b22fc87":"sub.to_csv('submission.csv', index=False)","a69910a0":"## \u30a2\u30a4\u30c7\u30a21\u6d3e\u751f : \u8a55\u4fa1\u304c\u9ad8\u3044developer\u304c\u3042\u308b\u306e\u3067\u306f\u306a\u3044\u304b\uff1f\n- developer\u304c\u305f\u304f\u3055\u3093\u3042\u308b\u3082\u306e\u3092\u62bd\u51fa\u3059\u308b\u3002\n- pivot table\u3092\u4f7f\u3063\u3066\u8a55\u4fa1\u306e\u9ad8\u3044\u3082\u306e\u3060\u3051\u3092\u96c6\u3081\u3066\u307f\u308b","b9e8074c":"\u307b\u307c\u3001c2\u3001\u307e\u305f\u306fc1\u306b\u96c6\u4e2d\u3057\u3066\u3044\u308b\u4f1a\u793e\u304c\u3042\u308b\u3002<br>\n\n\u4eee\u8aac\u3068\u3057\u3066\u3001c2\u306b\u96c6\u4e2d\u3057\u3066\u3044\u308b\u4f1a\u793e\u306f\u5225\u306e\u30b2\u30fc\u30e0\u3067\u3082c2\u3067\u3042\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u3068\u8003\u3048\u308b\u3002<br>\n\u3053\u308c\u3092\u4e0a\u624b\u304f\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f7f\u3048\u306a\u3044\u304b\uff1f\uff08\u3053\u3053\u3067\u306f\u4f7f\u3046\u30b3\u30fc\u30c9\u306f\u5171\u6709\u3057\u307e\u305b\u3093\uff09","d5378c8a":"publisher\u3082\u540c\u69d8\u306b\u51e6\u7406\u3059\u308b\u3002","d1ff4f8b":"\u5024\u6bb5\u3068publisher,\u5e74\u304c\u5927\u304d\u306a\u5f71\u97ff\u3092\u53ca\u307c\u3057\u3066\u3044\u307e\u3059\u3002","6fe31760":"### \u30a2\u30a4\u30c7\u30a24:\u6642\u7cfb\u5217\u6027\u3092\u4f7f\u3046\n\u4eca\u56de\u30c7\u30fc\u30bf\u304c\u6c5a\u3044\u306e\u3067\u96e3\u3057\u3044\u3067\u3059\u304c\u3001\u6642\u7cfb\u5217\u306e\u5dee\u5206\u3092\u3068\u3063\u3066\u3001\u30b2\u30fc\u30e0\u3092\u767a\u8868\u3059\u308b\u983b\u5ea6\u3092\u4f7f\u3046\u3068\u3044\u3046\u306e\u3082\u624b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002","4cbac384":"positive\u8a55\u4fa1\uff08c0)\u306e\u4e00\u90e8\u30d4\u30fc\u30af\u304c\u3042\u308b\u306e\u306f\u3001\uff11\uff16\uff10\u2212\uff11\uff17\uff19\u3067\u3001\u3053\u306e\u304f\u3089\u3044\u306e\u30b2\u30fc\u30e0\u6570\u3092\u51fa\u3059\u898f\u6a21\u306edeveloper\u304c\u9ad8\u8a55\u4fa1\u3092\u51fa\u3057\u3084\u3059\u3044\u306e\u3067\u306f\u306a\u3044\u304b\uff1f\n\u4e00\u65b9\u3067\u3001\u5927\u91cf\u306e\u30b2\u30fc\u30e0\u3092\u3068\u308a\u3042\u3048\u305a\u51fa\u3057\u3066\u3044\u308b\u306e\u306f\u3001\u3042\u307e\u308a\u826f\u304f\u306a\u3044\uff08\u30af\u30bd\u30b2\u30fc\uff09\u3092\u983b\u767a\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002\n\n\u3053\u306e\u7279\u5fb4\u91cf\u3092\u6728\u30e2\u30c7\u30eb\u306b\u5165\u308c\u308b\u3053\u3068\u3067\u3001\u30d4\u30fc\u30af\u306e\u60c5\u5831\u304c\u62bd\u51fa\u3067\u304d\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u305f\u3081\u3001\u3053\u306e\u307e\u307e\u5b66\u7fd2\u306b\u4f7f\u3046\u3002","f22fea1a":"### \u30a2\u30a4\u30c7\u30a23\uff1a\u30c6\u30ad\u30b9\u30c8\u7279\u5fb4\u91cf\u306e\u53d6\u308a\u6271\u3044\n- \u30ab\u30c6\u30b4\u30ea\u30c7\u30fc\u30bf\u3092w2v\u3067embedding\u3057\u305f\u304c\u3001w2v\u4ee5\u5916\uff08fasttext\u306a\u3069\u3067\uff09\u3067embedding\u3057\u3066\u3082\u306e\u3092\u52a0\u3048\u3066\u307f\u308b\n- embedding\u3057\u305f\u3082\u306e\u3092\u5225\u306e\u5b66\u7fd2\u6a5f\u306b\u5165\u308c\u3066\u5b66\u7fd2\u3055\u305b\u3066\u7d50\u679c\u3092\u5229\u7528\u3059\u308b\u3002\n- bert\u3067\u5b66\u7fd2\u3055\u305b\u305f\u7d50\u679c\u3092\u5229\u7528\u3057\u3066\u307f\u308b\u3002\n\u3053\u3053\u3067\u306f\u5b9f\u88c5\u3057\u307e\u305b\u3093\u304c\u3001\u3053\u3046\u3044\u3063\u305f\u7279\u5fb4\u91cf\u3082\u4f7f\u3048\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002","abfc3a31":"## \u30a2\u30a4\u30c7\u30a22\uff1a\u9ad8\u3044\u30bd\u30d5\u30c8\u3092\u51fa\u3057\u3066\u3044\u308bdeveloper\u306e\u8a55\u4fa1\u304c\u9ad8\u3044\u306e\u3067\u306f\u306a\u3044\u304b\uff1f\n- \u9ad8\u3044\u30bd\u30d5\u30c8\u3068\u3044\u3046\u3044\u3046\u3060\u3051\u3067\u5f37\u6c17\u3067\u3082\u58f2\u308c\u308b\u3001\u3068\u3044\u3046\u3053\u3068\u306a\u306e\u3067\u9ad8\u8a55\u4fa1\u304c\u96c6\u307e\u3089\u306a\u3044\u304b\uff1f","e182f14f":"\u5e73\u5747\u5024\u3067\u5dee\u304c\u51fa\u3066\u304d\u3066\u3044\u308b\u69d8\u5b50\u304c\u308f\u304b\u308b\u3002<br>\ndeveloper\u3054\u3068\u306e\u5dee\u3092\u307f\u308b\u3002","34fca99a":"## \u30a2\u30a4\u30c7\u30a21\uff1a\u305f\u304f\u3055\u3093\u30b2\u30fc\u30e0\u3092\u51fa\u3059\u4f1a\u793e\u306f\u8a55\u4fa1\u304c\u9ad8\u3044\nhttps:\/\/www.kaggle.com\/c\/data-science-winter-osaka2\/discussion\/295000#1619642\n\n\u30b2\u30fc\u30e0\u3092\u305f\u304f\u3055\u3093\u51fa\u3057\u3066\u3044\u308bdeveloper\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304b\u3089\u6c42\u3081\u3089\u308c\u308b\u3082\u306e\u3092\u305f\u304f\u3055\u3093\u51fa\u305b\u308b\u305f\u3081\u3067\u306f\u306a\u3044\u304b\uff1f","960baa00":"## \u30e2\u30c7\u30eb\u4f5c\u6210\naccuracy\u304cmetric\u3067\u3059\u304c\u3001\u4e88\u6e2c\u78ba\u7387\u3092\u51fa\u3057\u30665 fold\u306e\u5e73\u5747\u3092\u53d6\u308a\u305f\u3044\u306e\u3067\u3001log_loss\u3067\u30e2\u30c7\u30eb\u3092\u4e00\u5ea6\u4f5c\u6210\u3057\u307e\u3059\u3002","3492be57":"\u5dee\u304c\u3042\u308b\u306e\u304b\u7d71\u8a08\u91cf\u3092\u898b\u3066\u307f\u308b","68d6235a":"## CV\u306eaccuracy\u3092\u51fa\u3059","5fc5e2d0":"### pivot table\u3092\u4f5c\u308b","71c9957e":"\u53ef\u8996\u5316\u3057\u3066\u307f\u308b","c5d55bd7":"# Submisson file\u3092\u4f5c\u308b","5398dcac":"## \u4e0a\u3067\u5165\u308c\u305f\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u5b66\u7fd2\u3092\u884c\u306a\u3063\u3066\u307f\u308b","8c52e1f0":"\u5dee\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u306a\u3044\u3002plot\u3057\u3066\u307f\u308b","72a432a3":"\u305d\u3082\u305d\u3082price\u306e\u5206\u5e03\u304c\u3069\u3046\u306a\u3063\u3066\u308b\u306e\u304b\u898b\u3066\u307f\u308b\u3002","bab5d6bc":"## \u8a00\u8a9e\u7cfb\u306e\u7279\u5fb4\u91cf\u3092\u4f5c\u308b","2548b7bb":"\u3053\u306e\u307e\u307e\u3060\u3068\u6bd4\u8f03\u30fb\u53ef\u8996\u5316\u3057\u306b\u304f\u3044\u306e\u3067\u3001\u30b2\u30fc\u30e0\u6570\u3067\u305d\u308c\u305e\u308c\u306e\u8a55\u4fa1\u3092\u5272\u3063\u3066\u3001\u8a55\u4fa1\u304c\u9ad8\u3044developer\u304c\u306a\u3044\u304b\u898b\u3066\u307f\u308b","f1f69291":"# \u5b66\u7fd2\u3055\u305b\u308b","a79ddb56":"c2\u306b\u5b89\u3044\u30b2\u30fc\u30e0\u304c\u591a\u3044\u3053\u3068\u304c\u308f\u304b\u308b\u3002\u3053\u308c\u3082\u305d\u306e\u307e\u307e\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f7f\u3063\u3066\u307f\u308b\u3002<br>\n\npublisher\u3082\u540c\u69d8\u306b\u884c\u3046\u3002","c3322162":"\u305d\u3082\u305d\u3082publisher\u306bc2\u304c\u306a\u3044\u3053\u3068\u304c\u308f\u304b\u308b\u3002\n\n# \u307e\u3068\u3081\n\n\u4ed6\u306b\u3082year\u3067groupby\u3092\u3057\u3066\u307f\u308bnotebook\u304c\u3042\u308b\u306e\u3067\u3001\u4e0a\u4f4d\u306e\u4ed6\u306e\u8981\u7d20\u540c\u58eb\u306egroupby\u3092\u884c\u306a\u3063\u3066\u95a2\u4fc2\u3042\u308a\u305d\u3046\u306a\u90e8\u5206\u3092\u8003\u3048\u3066\u307f\u308b\u3068\u3001score\u304c\u4e0a\u304c\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002","bb182c3e":"\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u5dee\u304c\u3067\u304d\u308b\u306e\u304b\u6bd4\u8f03\u3092\u884c\u3046"}}