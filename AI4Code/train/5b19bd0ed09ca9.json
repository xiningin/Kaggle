{"cell_type":{"bcc4287e":"code","47ac7f01":"code","612fa322":"code","a73ddfa2":"code","b347ec27":"code","7a659f6b":"code","e321b4ae":"code","e451a3e0":"code","3bc27782":"code","84c0a3c9":"code","ecea2052":"code","684c5ded":"code","b86cf8b0":"code","38964a4b":"markdown","bb3ccff3":"markdown","36195731":"markdown","3aa223c3":"markdown","b737dcca":"markdown","90bdcf89":"markdown","f14f7c45":"markdown","66262192":"markdown"},"source":{"bcc4287e":"import numpy as np\nimport pandas as pd\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D,AveragePooling1D, Flatten,AtrousConvolution1D,SpatialDropout1D, Dropout, GlobalAveragePooling1D,GlobalMaxPooling1D\nfrom keras.layers.normalization import BatchNormalization\n\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\n\n# https:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/\nfrom keras.callbacks import EarlyStopping\n\nfrom keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split","47ac7f01":"\n\n\n# Merge the two Data set together\n# Drop duplicates (unlike the original script)\ndf = pd.read_csv('..\/input\/pdb_data_no_dups.csv').merge(pd.read_csv('..\/input\/pdb_data_seq.csv'), how='inner', on='structureId').drop_duplicates([\"sequence\"]) # ,\"classification\"\n# Drop rows with missing labels\ndf = df[[type(c) == type('') for c in df.classification.values]]\ndf = df[[type(c) == type('') for c in df.sequence.values]]\n# select proteins\ndf = df[df.macromoleculeType_x == 'Protein']\ndf.reset_index()\nprint(df.shape)\ndf.head()","612fa322":"print(df.residueCount_x.quantile(0.9))\ndf.residueCount_x.describe()","a73ddfa2":"df = df.loc[df.residueCount_x<1200]\nprint(df.shape[0])\ndf.residueCount_x.describe()","b347ec27":"import matplotlib.pyplot as plt\nfrom collections import Counter\n\n# count numbers of instances per class\ncnt = Counter(df.classification)\n# select only K most common classes! - was 10 by default\ntop_classes = 10\n# sort classes\nsorted_classes = cnt.most_common()[:top_classes]\nclasses = [c[0] for c in sorted_classes]\ncounts = [c[1] for c in sorted_classes]\nprint(\"at least \" + str(counts[-1]) + \" instances per class\")\n\n# apply to dataframe\nprint(str(df.shape[0]) + \" instances before\")\ndf = df[[c in classes for c in df.classification]]\nprint(str(df.shape[0]) + \" instances after\")\n\nseqs = df.sequence.values\nlengths = [len(s) for s in seqs]\n\n# visualize\nfig, axarr = plt.subplots(1,2, figsize=(20,5))\naxarr[0].bar(range(len(classes)), counts)\nplt.sca(axarr[0])\nplt.xticks(range(len(classes)), classes, rotation='vertical')\naxarr[0].set_ylabel('frequency')\n\naxarr[1].hist(lengths, bins=50, normed=False)\naxarr[1].set_xlabel('sequence length')\naxarr[1].set_ylabel('# sequences')\nplt.show()","7a659f6b":"from sklearn.preprocessing import LabelBinarizer\n\n# Transform labels to one-hot\nlb = LabelBinarizer()\nY = lb.fit_transform(df.classification)","e321b4ae":"\n# maximum length of sequence, everything afterwards is discarded! Default 256\n# max_length = 256\nmax_length = 350\n\n#create and fit tokenizer\ntokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts(seqs)\n#represent input data as word rank number sequences\nX = tokenizer.texts_to_sequences(seqs)\nX = sequence.pad_sequences(X, maxlen=max_length)","e451a3e0":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.2)","3bc27782":"embedding_dim = 25 # orig 8\n\n# create the model\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length))\nmodel.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu',dilation_rate=1))\n# model.add(Conv1D(filters=64, kernel_size=6, padding='same', activation='relu')) #orig\nmodel.add(Conv1D(filters=128, kernel_size=5, padding='valid', activation='relu',dilation_rate=1))\n# model.add(BatchNormalization())\n# model.add(MaxPooling1D(pool_size=2))\nmodel.add(AveragePooling1D(pool_size=2))\n# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')) # orig\nmodel.add(Conv1D(filters=128, kernel_size=7, padding='valid', activation='relu',dilation_rate=2)) \nmodel.add(BatchNormalization())\n# model.add(MaxPooling1D(pool_size=2))\nmodel.add(AveragePooling1D(pool_size=2))\n\n# model.add(Flatten()) ## Could do pooling instead \n# GlobalAveragePooling1D,GlobalMaxPooling1D\nmodel.add(GlobalAveragePooling1D())\n\nmodel.add(Dense(256, activation='relu')) # 128\nmodel.add(BatchNormalization())\nmodel.add(Dense(128, activation='relu')) # 128\nmodel.add(BatchNormalization())\nmodel.add(Dense(top_classes, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","84c0a3c9":"es = EarlyStopping(monitor='val_acc', verbose=1, patience=3)\n\nmodel.fit(X_train, y_train,  batch_size=128, verbose=1, validation_split=0.15,callbacks=[es],epochs=25) # epochs=15, # batch_size=128","ecea2052":"from keras.layers import Conv1D, MaxPooling1D, Concatenate, Input\nfrom keras.models import Sequential,Model\n\nunits = 256\nnum_filters = 32\nfilter_sizes=(3,5, 9,15,21)\nconv_blocks = []\n\nembedding_layer = Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length)\nes2 = EarlyStopping(monitor='val_acc', verbose=1, patience=4)\n\nsequence_input = Input(shape=(max_length,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\n\nz = Dropout(0.1)(embedded_sequences)\n\nfor sz in filter_sizes:\n    conv = Conv1D(\n        filters=num_filters,\n        kernel_size=sz,\n        padding=\"valid\",\n        activation=\"relu\",\n        strides=1)(z)\n    conv = MaxPooling1D(pool_size=2)(conv)\n    conv = Flatten()(conv)\n    conv_blocks.append(conv)\nz = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\nz = Dropout(0.25)(z)\nz = BatchNormalization()(z)\nz = Dense(units, activation=\"relu\")(z)\nz = BatchNormalization()(z)\npredictions = Dense(top_classes, activation=\"softmax\")(z)\nmodel2 = Model(sequence_input, predictions)\nmodel2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model2.summary())\n\nmodel2.fit(X_train, y_train,  batch_size=64, verbose=1, validation_split=0.15,callbacks=[es],epochs=30)","684c5ded":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nimport itertools\n\ntrain_pred = model.predict(X_train)\ntest_pred = model.predict(X_test)\nprint(\"train-acc = \" + str(accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_pred, axis=1))))\nprint(\"test-acc = \" + str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))))\n\n# Compute confusion matrix\ncm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))\n\n# Plot normalized confusion matrix\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nnp.set_printoptions(precision=2)\nplt.figure(figsize=(10,10))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(lb.classes_))\nplt.xticks(tick_marks, lb.classes_, rotation=90)\nplt.yticks(tick_marks, lb.classes_)\n#for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n#    plt.text(j, i, format(cm[i, j], '.2f'), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > cm.max() \/ 2. else \"black\")\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(classification_report(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1), target_names=lb.classes_))","b86cf8b0":"train_pred = model2.predict(X_train)\ntest_pred = model2.predict(X_test)\nprint(\"train-acc = \" + str(accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_pred, axis=1))))\nprint(\"test-acc = \" + str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))))\n\n# Compute confusion matrix\ncm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))\n\n# Plot normalized confusion matrix\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nnp.set_printoptions(precision=2)\nplt.figure(figsize=(10,10))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(lb.classes_))\nplt.xticks(tick_marks, lb.classes_, rotation=90)\nplt.yticks(tick_marks, lb.classes_)\n#for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n#    plt.text(j, i, format(cm[i, j], '.2f'), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > cm.max() \/ 2. else \"black\")\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(classification_report(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1), target_names=lb.classes_))","38964a4b":"# Transform labels\nFirst, the dataset is reduced to those samples wich are one of the ten most common classes. The length of sequences ranges from very few amino acids to several thousand amino acids (see plot below).\n\nSecond, using **LabelBinarizer** from **sklearn.preprocessing** transforms the labels in string to one hot representation.","bb3ccff3":"* Other Models\n* http:\/\/nadbordrozd.github.io\/blog\/2017\/08\/12\/looking-for-the-text-top-model\/","36195731":"# Preprocessing and visualization of dataset\n**Ideally: **For comparison I also decided to focus only on those classes where the number of instances is greater than 1000 (as in [this kernel of Akil](https:\/\/www.kaggle.com\/abharg16\/predicting-protein-classification\/code)) which corresponds to the 43 most common classes. \n\n**But:** one hour on 4 CPU is not sufficient for such big datasets, instead only 10 most common classes are considered.","3aa223c3":"Further improvements are possible (deeper model, smaller sequences, using n-grams as words for word embedding to accelerate learning) and should be validated via cross-validation. Could also be improved by Dropout, batchnorm, changing the architecture or regularizing by adding penalties for large weights (kernel_regularizer) or large activations (activation_regularizer). ","b737dcca":"# Import Dataset, drop NaN's, select Proteins\nThis notebooks shows how to classify protein families soley based on their sequence of aminoacids. This work is based on the current success of deep learning models in natural language processing (NLP) and assumes the proteins sequences can be viewed as as a language. Please note, that there are notable search engines such as BLAST for this task.\n\nBefore we dive into, first some preprocessing of the data:\n1. merge on *structureId*\n2. drop rows without labels\n3. drop rows without sequence\n4. select proteins","90bdcf89":"### remove some longer\/shorter sequences\n* the length is very skewed. We'll exclude some of the longer ones (many papers do <300-500 length sequences).\n* For now we'l lleave the peptides (or potential fragments) in, i.e the very short (<30 AA) sequences","f14f7c45":"# Build keras model and fit\n**Previous approaches:**\n\n [Kernel of hnike](https:\/\/www.kaggle.com\/hnike25\/best-machine-learning-model-on-the-dataset\/code)  relied on numerical features of the data (with molecule weight, residue count etc.) 90% considering only the three most common classes. Without any claim of generality, some quick tests with this scenario showed superior results. \n \n[Kernel of abharg](https:\/\/www.kaggle.com\/abharg16\/predicting-protein-classification\/code) used **CountVectorizer** from sklearn with 4-grams followed by a simple classification model and yielded accuaracy of about 76% considering the first 43 classes (classes where there are more than one thousand samples). Note that this approach is also based soley on the sequence and will be compared with my results.\n\n## Further improvements\nNow the most crucial part of this kernel:\n1. recent success in NLP suggest to use word embeddings which is already implemented as a keras layer ([**Embedding**](https:\/\/keras.io\/layers\/embeddings\/)). Note that in our case, there are only 20 different words (for each amino acid). Instead of word embedding, one could try a 2D convolution on the one hot representation of the sequence, but this approach focuses on applying NLP-theory to protein sequences. \n2. instead of using every n-gram, consider using [1D-convolution]() on the embedded sequences. The size of the convolutional kernel can be seen as size of n-grams and the number of filters as number of words. \n3. to improve performance also a deep architecture (subsequent layers of convolution and pooling) can be used, here two layers, where the first layer has 64 filters with convolutional size of 6 and the second layer has 32 filters of size 3.\n4. Flatten and pass activations into fully connceted layers where the last layer is a softmax activation and size corresponding to the number of classes.\n\n* Likely improvement: filter bank (instead of a linear chain of convolutions)","66262192":"# Further preprocessing of sequences with keras\nUsing the ** keras** library for text processing, \n1. ** Tokenizer**: translates every character of the sequence into a number\n2. **pad_sequences:** ensures that every sequence has the same length (max_length). I decided to use a maximum length of 512, which should be sufficient for most sequences. **Note:** reduced to 256 for computational time reasons.\n3. **train_test_split:** from sklearn splits the data into training and testing samples."}}