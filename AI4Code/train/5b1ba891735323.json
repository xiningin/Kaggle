{"cell_type":{"a7a59c4b":"code","8e62f7b6":"code","21128b69":"code","a319e516":"code","87b63133":"code","492bd695":"code","fd439b2a":"code","0171c416":"code","cf9a160f":"markdown","ff9e1892":"markdown","f3ef9969":"markdown"},"source":{"a7a59c4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e62f7b6":"from sklearn import preprocessing\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","21128b69":"data_train = pd.read_csv('\/kaggle\/input\/main-tenis\/play_tennis_train.csv')\ndata_test = pd.read_csv('\/kaggle\/input\/main-tenis\/play_tennis_test.csv')","a319e516":"le = preprocessing.LabelEncoder()\ndata_train_df = pd.DataFrame(data_train)\ndata_train_df_encoded = data_train_df.apply(le.fit_transform)\ndata_test_df = pd.DataFrame(data_test)\ndata_test_df_encoded = data_test_df.apply(le.fit_transform)","87b63133":"#Train\nx_train = data_train_df_encoded.drop(['play'],axis=1)\ny_train = data_train_df_encoded['play']\n\n#Test\nx_test = data_test_df_encoded.drop(['play'],axis=1)\ny_test = data_test_df_encoded['play']","492bd695":"scal = StandardScaler()\nscal.fit(x_train)\nx_train_scaled = scal.transform(x_train)\nx_test_scaled = scal.transform(x_test)\nsvm = SVC()\nsvm.fit(x_train_scaled, y_train)\n\nprint(\"Skor Data Train: {:.2f}\".format(svm.score(x_train_scaled, y_train)))\nprint(\"Skor Data Test: {:.2f}\".format(svm.score(x_test_scaled, y_test)))","fd439b2a":"y_pred = svm.predict(x_test_scaled)\nprint(confusion_matrix(y_test, y_pred))","0171c416":"print(classification_report(y_test, y_pred, target_names=[\"yes\", \"no\"]))","cf9a160f":"*selanjutnya kita masuk ke proses SVM...*","ff9e1892":"*ya, seperti biasa, pertama tama kita masukkan library yang akan kita gunakan...*","f3ef9969":"*lalu kita panggil, dataset yang akan kita gunakan...*"}}