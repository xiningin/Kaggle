{"cell_type":{"6c06d942":"code","75fcefdf":"code","e47662f3":"code","e7911a90":"code","9cb48552":"code","81d50b2b":"code","ba70186e":"code","f5cd549f":"code","1832532d":"code","3e949d4f":"code","39eaa62f":"code","9ea271f2":"code","2678d55c":"code","4373b592":"code","a07f5ccf":"code","10f27800":"code","0006c715":"code","6d7f1924":"code","381b8c9e":"code","5925e06f":"code","fa908f7a":"code","f92aec91":"code","71b49527":"code","4e5f9a04":"code","889d9664":"code","07f74f0c":"code","7b9e7fcb":"code","1737c795":"code","811c50d9":"code","5e94f4ee":"code","fe225ebe":"code","bc234be1":"code","adbd1fcd":"code","96f8c3a7":"code","69af5c21":"code","b07f4f2c":"code","c5a2c4b1":"code","e26d35dd":"code","a5356ce3":"code","fa3d36eb":"code","e7e4cdbe":"code","484525f4":"code","93189a44":"code","4c8cb36b":"code","ddb8e22a":"code","4d50006f":"code","41c2364a":"code","f59df8f9":"code","1f5a61d8":"code","9f065e7e":"code","d9c02c33":"code","81bbee06":"code","bc49dc3a":"code","43e507e1":"code","454a5f68":"code","6ae098a0":"code","9944e64c":"code","fe11c0d9":"code","aca74af7":"code","7ed17b9c":"code","d65172cc":"code","b7167bc9":"code","3179616f":"code","7094494a":"code","806feb88":"code","38a8a18a":"code","d07ad80b":"code","86df531b":"code","e689869f":"code","6f26ada2":"code","1f099262":"code","dc5cf68a":"code","bc88bc66":"code","ad18a304":"code","eebbc0c1":"code","da66029e":"code","78fb5e3a":"code","3cbd7d20":"code","82652f30":"code","8fe2063c":"code","6a5544ff":"code","f7a8e63d":"code","66b9fe88":"code","39f0aece":"code","ea38d438":"code","a610c945":"code","e8180426":"code","b696e8e4":"code","f396e5fe":"code","67d3332b":"code","3d9a7837":"code","4be2def4":"code","7f4bdc77":"code","b86b2c00":"code","75458a98":"code","baf9ca27":"code","40a7dbc1":"code","ad7a723b":"code","7f35c052":"code","51f52d34":"code","2bbd6853":"code","92deafd5":"code","a16f976d":"code","1525b97e":"code","dc97589f":"code","ac873e6c":"code","f226a4b0":"code","3da582cd":"code","3a2123a9":"markdown"},"source":{"6c06d942":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport os","75fcefdf":"os.listdir('..\/input\/hotel-id-2021-fgvc8')","e47662f3":"df = pd.read_csv('..\/input\/hotel-id-2021-fgvc8\/train.csv')\ndf.head()","e7911a90":"train_images ='..\/input\/hotel-id-2021-fgvc8\/train_images\/'","9cb48552":"df['train_images']= train_images + df.chain.astype(str) +'\/'+ df.image\ndf.head()","81d50b2b":"df_dash = df[:100]\ndf_dash.head()","ba70186e":"df_dash.shape","f5cd549f":"# import required libraries\nimport random\nfrom IPython.core.display import Image\nfrom IPython.display import display","1832532d":"for n in range(3):\n    tr_image = random.choice(list(df['train_images']))\n    print(tr_image)\n    display(Image(tr_image,width=400,height=400))\n    print('\\n')","3e949d4f":"def train_images_reading(path,labels):\n    \n    images = tf.io.read_file(path)\n    \n    images = tf.image.decode_jpeg(images, channels=3)\n    \n    images = tf.image.resize(images, [64,64])\n    \n    images = tf.cast(images, tf.float32)\n    \n    images \/= 255.0\n    \n    return(images,labels)","39eaa62f":"# data augmentation techniques on image used from this colab file \n#https:\/\/colab.research.google.com\/github\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/images\/data_augmentation.ipynb\ndef train_images_preprocessing(images, labels):\n    \n    images = tf.image.stateless_random_flip_left_right(images,seed=(2,0))\n    \n    images = tf.image.stateless_random_flip_up_down(images,seed=(2,0))\n    \n    images = tf.image.rot90(images)\n    \n    images = tf.image.stateless_random_brightness(images, max_delta=32.0 \/ 255.0,seed=(2,0))\n    \n    images = tf.image.stateless_random_saturation(images, lower=0.5, upper=1.5,seed=(2,0))\n    \n    images = tf.image.stateless_random_hue(images, 0.2,seed=(2,0))\n\n    #Make sure the image is still in [0, 1]\n    images = tf.clip_by_value(images, 0.0, 1.0)\n\n    return (images, labels) , labels","9ea271f2":"# import library for performing Label Encoding on hotel ids\nfrom sklearn.preprocessing import LabelEncoder","2678d55c":"labelencoder = LabelEncoder()\ndf['hotel_id'] = labelencoder.fit_transform(df['hotel_id'])","4373b592":"labelencoder.classes_","a07f5ccf":"import pickle","10f27800":"# Save label encoder to use in prediction\nle_path = 'labelencoder.pkl'\nwith open(le_path, 'wb') as fw:\n    pickle.dump(labelencoder, fw)","0006c715":"# save the train images and hotel ids as a tensor\nlabel_ids = tf.convert_to_tensor(np.array(df['hotel_id']), dtype=tf.int32)\nimage_files  = tf.convert_to_tensor(df['train_images'].tolist(), dtype=tf.string)","6d7f1924":"len(label_ids)","381b8c9e":"image_files[:2]","5925e06f":"label_ids[0:5]","fa908f7a":"# Create a train dataset\n#https:\/\/udibhaskar.github.io\/practical-ml\/debugging%20nn\/neural%20network\/overfit\/underfit\/2020\/02\/03\/Effective_Training_and_Debugging_of_a_Neural_Networks.html\n#read all the train file names along with labels to create a Dataset type object\ntrain_dataset = tf.data.Dataset.from_tensor_slices((image_files, label_ids))\n\n# shuffle the dataset\ntrain_dataset = train_dataset.shuffle(len(image_files))\n\n#this map fucntion is also taken from \n#https:\/\/colab.research.google.com\/github\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/images\/data_augmentation.ipynb\n# Map the read_train_image function on the dataset in order to read dataset\ntrain_dataset = train_dataset.map(train_images_reading,num_parallel_calls=tf.data.AUTOTUNE)\n\n# Map the read_train_image function on the dataset in order to get preprocessed dataset\ntrain_dataset = train_dataset.map(train_images_preprocessing,num_parallel_calls=tf.data.AUTOTUNE)\n\n#bacth and prefetch the dataset\ntrain_dataset = train_dataset.batch(64)\ntrain_dataset = train_dataset.prefetch(2)","f92aec91":"train_dataset","71b49527":"for input_text, output_label in train_dataset:\n    print(input_text[0:3], output_label[0:3])\n    break","4e5f9a04":"len(train_dataset)","889d9664":"# import required model libraries\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense,Dropout,Flatten,Input\nfrom tensorflow.keras.applications import ResNet50","07f74f0c":"#https:\/\/www.kaggle.com\/chankhavu\/keras-layers-arcface-cosface-adacos\n#Image representations were trained by Arcface as it is very good for face recognition and also used by 1'st place winner and bucnch of people in this competiton \nclass ArcMarginLoss(tf.keras.layers.Layer):\n    \n    def __init__(self, num_classes, margin=0.5, logist_scale=64, **kwargs):\n        super(ArcMarginLoss, self).__init__(**kwargs)\n        self.num_classes = num_classes\n        self.margin = margin\n        self.logist_scale = logist_scale\n\n    def build(self, input_shape):\n        self.w = self.add_variable(\n            \"weights\", shape=[int(input_shape[-1]), self.num_classes])\n        self.cos_m = tf.identity(math.cos(self.margin), name='cos_m')\n        self.sin_m = tf.identity(math.sin(self.margin), name='sin_m')\n        self.th = tf.identity(math.cos(math.pi - self.margin), name='th')\n        self.mm = tf.multiply(self.sin_m, self.margin, name='mm')\n\n    def call(self, embds, labels):\n        normed_embds = tf.nn.l2_normalize(embds, axis=1, name='normed_embd')\n        normed_w = tf.nn.l2_normalize(self.w, axis=0, name='normed_weights')\n\n        cos_t = tf.matmul(normed_embds, normed_w, name='cos_t')\n        sin_t = tf.sqrt(1. - cos_t ** 2, name='sin_t')\n\n        cos_mt = tf.subtract(\n            cos_t * self.cos_m, sin_t * self.sin_m, name='cos_mt')\n\n        cos_mt = tf.where(cos_t > self.th, cos_mt, cos_t - self.mm)\n\n        mask = tf.one_hot(tf.cast(labels, tf.int32), depth=self.num_classes,\n                          name='one_hot_mask')\n\n        logists = tf.where(mask == 1., cos_mt, cos_t)\n        logists = tf.multiply(logists, self.logist_scale, 'arcface_logist')\n\n        return logists","7b9e7fcb":"num_classes=len(set(df['hotel_id'].values))","1737c795":"#import keras\nfrom tensorflow.keras.layers import SpatialDropout1D, LSTM, BatchNormalization,concatenate,Flatten,Embedding,Dense,Dropout,MaxPooling2D,Reshape","811c50d9":"#def ArcFaceModel():\n    \n# The input layer\nx = inputs = Input(shape=(64,64,3,), name='input_image')\n\n# The ResNet50 model using imagenet weights\nx = ResNet50(include_top=False,weights='imagenet')(x)\n\nx = tf.keras.layers.BatchNormalization()(x)\n\n#Dropout layer\nx = Dropout(rate=0.5)(x)\n\n#Flatten layer\nx = Flatten()(x)\n\n#A dense layer of size 64 with l2 regularization\nx = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(5e-4))(x)\n\noutput = Dense(num_classes, activation=\"softmax\")(x)\n\nmodel = Model(inputs=inputs, outputs=output)\n\n    #Creates embeddings of size (batchsize,64)\n    #embeds =tf.keras.layers.BatchNormalization()(x)\n    \n    #Input layer for labels\n    #labels = Input([], name='label')\n    \n            \n    #compue logits using the ArcMarginPenaltyLogists call giving embeddings and labels as inputs\n    #logist = ArcMarginLoss(num_classes=len(set(df['hotel_id'].values)), margin=0.5,\n                             #logist_scale=64)(embeds, labels)\n    \n    #return the model which takes (input,label) as input and gives logits as output \n    #return Model((inputs, labels), output)\n    \n","5e94f4ee":"model.summary()","fe225ebe":"# clear session\ntf.keras.backend.clear_session()","bc234be1":"import math","adbd1fcd":"# create the model\nmodel = ArcFaceModel()","96f8c3a7":"# check the architecture of the model\nmodel.summary()","69af5c21":"\ndef SoftmaxLoss():\n    \"\"\"Function calls softmax loss \n    to output the cross entropy loss between true labels and the predicted logits\"\"\"\n    def softmax_loss(y_true, y_pred):\n        # y_true: sparse target\n        # y_pred: logist\n        \n        y_true = tf.cast(tf.reshape(y_true, [-1]), tf.int32)\n        ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n                                                            logits=y_pred)\n        \n        return tf.reduce_mean(ce)\n    return softmax_loss","b07f4f2c":"# the hyper parameters - learning rate, optimizer SGD and custom loss function SoftmaxLoss\nlearning_rate = tf.constant(0.01)\noptimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True, clipvalue=0.5)\n#loss_fn = SoftmaxLoss()","c5a2c4b1":"# compile the model with above hyperparameters\nmodel.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy())","e26d35dd":"import datetime","a5356ce3":"\nfrom time import time\nfrom tensorflow.python.keras.callbacks import TensorBoard\n#create a directory to save logs during model training\nlog_dir=\"model_logs_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n\n#create a tensorboard callback with the log directory path\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,\n                                                      write_graph=True,write_grads=True)\nfilepath=\".\/weights_model_1.best.h5\" \ncheck = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, save_best_only = True, verbose = True,monitor='val_loss', mode=\"min\")\n\ncallback_list = [tensorboard_callback, check]","fa3d36eb":"# train the model for 15 epochs\nmodel.fit(train_dataset,epochs=1,verbose=1,callbacks=[callback_list])","e7e4cdbe":"model.save_weights(\".\/weights_1_model.h5\")","484525f4":"#load the saved weights\nmodel.load_weights(\"..\/input\/weights\/weights_1_model.h5\")","93189a44":"# train the model for 15 epochs\nmodel.fit(train_dataset,epochs=15,verbose=1,callbacks=[callback_list])","4c8cb36b":"model.save_weights(\".\/weights_2_model.h5\")","ddb8e22a":"#load the saved weights\nmodel.load_weights(\"..\/input\/weights2\/weights_2_model.h5\")","4d50006f":"# train the model for 15 epochs\nmodel.fit(train_dataset,epochs=15,verbose=1,callbacks=[callback_list])","41c2364a":"model.save_weights(\".\/weights_3_model.h5\")","f59df8f9":"#load the saved weights\nmodel.load_weights(\"..\/input\/weight3\/weights_3_model.h5\")","1f5a61d8":"# train the model for 15 epochs\nmodel.fit(train_dataset,epochs=20,verbose=1,callbacks=[callback_list])","9f065e7e":"model.save_weights(\".\/weights_4_model.h5\")","d9c02c33":"#load the saved weights\nmodel.load_weights(\"..\/input\/weight4\/weights_4_model.h5\")","81bbee06":"# train the model for 15 epochs\nmodel.fit(train_dataset,epochs=10,verbose=1,callbacks=[callback_list])","bc49dc3a":"model.save_weights(\".\/weights_5_model.h5\")","43e507e1":"#load the saved weights\nmodel.load_weights(\"..\/input\/weight5\/weights_5_model.h5\")","454a5f68":"model.save('.\/model_saved')","6ae098a0":"# import required libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import Model","9944e64c":"model_1 = tf.keras.models.load_model('.\/model_saved', compile=False)","fe11c0d9":"# check its architecture\nmodel_1.summary()","aca74af7":"# Create a model which takes a 64x64x3 image as input and returns the 64 dim embeddings as output\ntest_model = Model(model_1.input[0],model_1.layers[6].output)","7ed17b9c":"test_model","d65172cc":"# save the model for testing purpose\ntest_model.save('.\/test_model')","b7167bc9":"# load the final saved test model\ntest_model = tf.keras.models.load_model('test_model', compile=False)","3179616f":"def read_image(path):\n    '''\n    The function reads an image path\n    Read the image from image path, decode the channels in the image\n    Resize it to 64x64\n    Cast it to a float32 array and normalize each pixel\n    Returns the preprocessed image\n    '''\n    \n    #read image file at the path\n    image = tf.io.read_file(path)\n    \n    image = tf.image.decode_jpeg(image, channels=3)\n    \n    #resize image\n    image = tf.image.resize(image, [64,64])\n    \n    #convert to float32\n    image = tf.cast(image, tf.float32)\n    \n    # normalize image to [0,1] range\n    image \/= 255.0\n    \n    \n    return image","7094494a":"# read a test image\ntest_image = np.expand_dims(read_image('..\/input\/hotel-id-2021-fgvc8\/test_images\/99e91ad5f2870678.jpg'),0)","806feb88":"test_image","38a8a18a":"# Convert the model to tflite\nconverter = tf.lite.TFLiteConverter.from_saved_model('test_model') # path to the SavedModel directory\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.float16]\ntflite_model = converter.convert()\n\n# Save the converted model\nwith open('.\/test_model_tflite', 'wb') as f:\n    f.write(tflite_model)","d07ad80b":"#read the optimized tflite test model\ntflite_test_model = tf.lite.Interpreter('.\/test_model_tflite')","86df531b":"#allocate the tensors\ntflite_test_model.allocate_tensors()","e689869f":"# Get input and output tensors.\ninput_details = tflite_test_model.get_input_details()\noutput_details = tflite_test_model.get_output_details()","6f26ada2":"#set the input details tensor in the test model\ntflite_test_model.set_tensor(input_details[0]['index'], test_image)","1f099262":"#invoke the test model\ntflite_test_model.invoke()","dc5cf68a":"# The function `get_tensor()` returns a copy of the tensor data\ntest_embeds = tflite_test_model.get_tensor(output_details[0]['index'])\nprint(test_embeds)","bc88bc66":"# normalize the embeddings\nnorm = np.linalg.norm(test_embeds, axis=1, keepdims=True)\ntest_embeds = test_embeds \/ norm\ntest_embeds","ad18a304":"# Read train data\ntrain_df = pd.read_csv('..\/input\/hotel-id-2021-fgvc8\/train.csv')","eebbc0c1":"# store the image path as the absolute path because each image is contained in a folder of some hotel chain\ntrain_df['image_path'] = \"..\/input\/hotel-id-2021-fgvc8\/train_images\/\" + train_df['chain'].astype(str) +'\/'+ train_df['image']","da66029e":"train_df.head()","78fb5e3a":"# save the image paths as a tensor\ntrain_filenames  = tf.convert_to_tensor(train_df['image_path'].tolist(), dtype=tf.string)\n\n# Create a train dataset\n\n# read all the train file names to create a Dataset type object\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_filenames)\n\n# shuffle the dataset\ntrain_dataset = train_dataset.shuffle(len(train_filenames))\n\n# Map the read_train_image function on the dataset in order to read dataset\ntrain_dataset = train_dataset.map(read_image,num_parallel_calls=4)\n\n#batch and prefetch the dataset\ntrain_dataset = train_dataset.batch(64)\ntrain_dataset = train_dataset.prefetch(1)\n\n# check shape of train dataset\ntrain_dataset","3cbd7d20":"#get embeddings for the train images using the saved test model\ntrain_embeds = test_model.predict(train_dataset)","82652f30":"# check the shape of the embeddings\ntrain_embeds.shape","8fe2063c":"# normalize the embeddings\nnorm = np.linalg.norm(train_embeds, axis=1, keepdims=True)\ntrain_embeds = train_embeds \/ norm\ntrain_embeds","6a5544ff":"# save the train embeddings\nnp.save('.\/train_embeddings.npy',train_embeds)","f7a8e63d":"pip install faiss-cpu","66b9fe88":"#import the faiss library\nimport faiss","39f0aece":"# load the train embeddings\ntrain_embeds = np.load('.\/train_embeddings.npy')","ea38d438":"# define the hyperparameters - no. of dimensions, no. of clusters(centroids), no. of iterations\n#https:\/\/github.com\/facebookresearch\/faiss\/wiki\/Faiss-building-blocks:-clustering,-PCA,-quantization\n#https:\/\/towardsdatascience.com\/understanding-faiss-619bb6db2d1a\ndim = train_embeds.shape[1]\nn_centroids = 7770\nn_iter = 20\nverbose = True","a610c945":"# define a kmeans model with the hyper parameters and train the model with train embeddings\nkmeans = faiss.Kmeans(dim, n_centroids, niter=n_iter, verbose=verbose)\nkmeans.train(train_embeds)","e8180426":"# the error after training for n iterations\nkmeans.obj[-1]","b696e8e4":"import pickle","f396e5fe":"# save the error for future reference\nwith open(\".\/kmeans_error.pkl\", \"wb\") as f:\n    pickle.dump(kmeans.obj[-1], f)","67d3332b":"# return the nearest centroid and its distance for each line vector in train_embeds\ntrain_to_centroid_dist, train_to_centroid_ind = kmeans.index.search(train_embeds, 1)","3d9a7837":"# check the shape of results\ntrain_to_centroid_dist.shape","4be2def4":"# check the number and dimensions of centroids found\nkmeans.centroids.shape","7f4bdc77":"# train an index with the centroids found from trained kmeans model\nkmeans_index = faiss.IndexFlatL2(dim)\nkmeans_index.add(kmeans.centroids)","b86b2c00":"# save the trained index\nfaiss.write_index(kmeans_index, \".\/kmeans_trained.index\")","75458a98":"# load the saved index\nkmeans_index = faiss.read_index(\".\/kmeans_trained.index\")","baf9ca27":"#find the 5 nearest distance in test_embeds to the computed centroids\ntest_to_centroid_dist, test_to_centroid_ind = kmeans_index.search(test_embeds, 5)","40a7dbc1":"# check the number of distances returned\ntest_to_centroid_dist.shape","ad7a723b":"# check distances\ntest_to_centroid_dist","7f35c052":"# check the nearest centroid indices\ntest_to_centroid_ind","51f52d34":"# load the saved label encoder\nwith open('.\/labelencoder.pkl', \"rb\") as input_file:\n    le = pickle.load(input_file)","2bbd6853":"# get the actual labels for these predicted labels (which were transformed earlier using label encoder)\nactual_output_labels = le.inverse_transform(test_to_centroid_ind[0])\nactual_output_labels","92deafd5":"# import required libraries for visualization of clusters and nearest centroids to test data\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nsns.set()","a16f976d":"# load the train embeddings\ntrain_embeds = np.load('.\/train_embeddings.npy')","1525b97e":"# reduce the high dimensional data to 2D for visualization\npca1 = PCA(2)\npca2 = PCA(2)\n \n#Transform the data\ncentroids = pca1.fit_transform(kmeans.centroids)\ntrain_data = pca2.fit_transform(train_embeds)\n","dc97589f":"#save the dimensionality reduced centroids\npickle.dump(centroids, open('.\/kmeans_centroids_pca.pkl', 'wb'))","ac873e6c":"#read the train data csv\ntrain_df = pd.read_csv('..\/input\/hotel-id-2021-fgvc8\/train.csv')","f226a4b0":"''' Saving the plot which has the train embeddings along with the cluster centers (classes)'''\n#create the figure size\nfig,ax = plt.subplots(1,1)\nfig.set_size_inches(20, 15)\n\n#get the unique original hotel id's from the train csv\nu_labels = np.unique(train_df['hotel_id'])\n \n\n#plotting the results:\n\n#iterate over the unique hotel id's\nfor i in u_labels:\n    \n    #plot the images of each unique hotel id in a separate colour\n    ax.scatter(train_data[train_df['hotel_id']==i,0] , train_data[train_df['hotel_id']==i,1], cmap='viridis', s=50)\n    \n#plot the centroids of each of the above cluster\nax.scatter(centroids[:,0] , centroids[:,1], s = 5, color = 'black',alpha=0.8,label='Hotel ID centroid')\n\n#for faster access, saving this master plot which serves as a common plot for each inference\npickle.dump((fig,ax), open('.\/train_fig.pkl', 'wb'))","3da582cd":"''' Loading the saved master plot which has the train embeddings along with the cluster centers (classes)\n    and plotting the inference predictions on top of it\n'''\n\n#load the saved plot and unpack to its figure and axes\nfig2,ax2 = pickle.load(open('.\/train_fig.pkl', 'rb'))\nfig2.set_size_inches(20, 15)\n\n##load the dimensionality reduced centroids\ncentroids = pickle.load(open('.\/kmeans_centroids_pca.pkl', 'rb'))\n\n# define colours for marking the 5 nearest neighbours\npredicted_colours = ['tomato','gold','springgreen','cornflowerblue','hotpink']\ncount=0\n\n#get the max dimensions of the x and y axis\nleft_x, right_x = plt.xlim()\nmin_y, max_y = plt.ylim()\n\n#iterate over the 5 nearest predicted centroid indices for the test embeddings\nfor j in list(test_to_centroid_ind[0]):\n    \n    #plot the nearest centroid i.e. predicted hotel ID for the test input image\n    ax2.scatter(centroids[j,0] , centroids[j,1], s=1500, c=predicted_colours[count], alpha=0.9,label='Predicted Hotel ID '+str(count+1),marker='*',edgecolors='white',linewidth=2)\n    \n    # also show the predicted hotel ID on the marked point\n    ax2.text(0.05+left_x+(0.05*count), min_y+(0.01),str(actual_output_labels[count]),fontsize=15,bbox=dict(edgecolor='black',facecolor=predicted_colours[count], alpha=0.9))\n    \n    count+=1\n    \n#add legend to mark labels\nplt.legend(fontsize='large')\n","3a2123a9":"print random images to see that my train images is showing or not"}}