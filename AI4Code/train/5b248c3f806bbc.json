{"cell_type":{"f78daf0b":"code","269a7c2d":"code","3eb2e92b":"code","275e0dbe":"code","3ac3a692":"code","2831befb":"code","e60ac77b":"code","dd2853a6":"code","fbf82e03":"code","6410f031":"code","469ed637":"code","365e75dd":"code","7be6cbb2":"code","c21c88f8":"code","2be3109e":"code","f31e6b3f":"code","0c5b29a5":"code","62446976":"code","d803aad7":"code","f8cdbc33":"code","a4d767b0":"code","26aebedf":"code","e8c1da34":"code","5e6f8ae0":"code","a3ed7fd1":"code","9745ccf0":"code","ebd52ebd":"code","17a7b6f3":"code","eb3b8af4":"code","537d849a":"code","68900386":"code","98d7e135":"code","2d0a34d7":"code","3c260d02":"code","8497eda6":"code","b6b9212c":"code","d741bdd8":"code","65a22adb":"code","1e26b3cc":"code","da824516":"code","7a447e33":"code","ce0e8fd6":"code","4053147c":"code","fca55227":"code","73826c58":"code","32f5e350":"markdown"},"source":{"f78daf0b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","269a7c2d":"import sentencepiece as sp\nimport time\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom numpy.random import shuffle\n\ninit_notebook_mode(connected=True)","3eb2e92b":"def read_text(filename): \n        # open the file \n        file = open(filename, mode='rt', encoding='utf-8') \n        \n        # read all text \n        text = file.read() \n        file.close() \n        return text","275e0dbe":"file_path = \"\/kaggle\/input\/en-dutch-pairs\/nld.txt\"","3ac3a692":"start_time = time.time()\ntrain_df = pd.read_csv(file_path, sep='\\t', lineterminator='\\n', names=[\"EN\",\"NL\"])\nprint(f\"File loading time: {time.time()-start_time}\")\nprint(train_df.shape)","2831befb":"train_df.head()","e60ac77b":"def write_trainer_file(col, filename):\n    texts = list(col.values)\n    with open(filename, 'w',encoding='utf-8') as f:\n        for text in texts:\n            f.write(text + \"\\n\")","dd2853a6":"en_sp_trainer = \"en_spm.txt\"\nwrite_trainer_file(train_df[\"EN\"], en_sp_trainer)","fbf82e03":"nl_sp_trainer = \"nl_spm.txt\"\nwrite_trainer_file(train_df[\"NL\"], nl_sp_trainer)","6410f031":"def createSPModel(trainer_file, model_prefix, vocab_size, sp):\n    spm_train_param = f\"--input={trainer_file} --model_prefix={model_prefix} --vocab_size={vocab_size}\"\n    sp.SentencePieceTrainer.Train(spm_train_param)\n    lang_sp = sp.SentencePieceProcessor()\n    lang_sp.Load(f\"{model_prefix}.model\")\n    return lang_sp\n    ","469ed637":"en_sp = createSPModel(en_sp_trainer, \"en_sp\", 7150, sp)","365e75dd":"print(en_sp.EncodeAsPieces(\"This is a test.\"))\nprint(en_sp.EncodeAsIds(\"This is a test.\"))\nprint(en_sp.DecodeIds(en_sp.EncodeAsIds(\"This is a test.\")))","7be6cbb2":"nl_sp = createSPModel(nl_sp_trainer, \"nl_sp\", 9600, sp)","c21c88f8":"nl_sp.EncodeAsPieces(\"Ik wil graag een fles water.\")","2be3109e":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, RepeatVector\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\nfrom keras import optimizers\nfrom sklearn.model_selection import train_test_split","f31e6b3f":"def encode_sentence(df, lang, spm):\n    lang_pieces = []\n    lang_lens = []\n    for index, row in df.iterrows():\n        lang_piece = spm.EncodeAsIds(row[lang])\n        lang_pieces.append(lang_piece)\n        lang_lens.append(len(lang_piece)) \n    df[f\"{lang}_pieces\"] = lang_pieces\n    df[f\"{lang}_len\"] = lang_lens","0c5b29a5":"start_time = time.time()\nencode_sentence(train_df, \"EN\", en_sp)\nencode_sentence(train_df, \"NL\", nl_sp)\nprint(f\"Encoding time: {time.time()-start_time} sec\")\n","62446976":"train_df.tail()","d803aad7":"def plotLangLen(lang1, lang2):\n    trace1 = go.Histogram(\n        x=train_df[f\"{lang1}_len\"].values,\n        opacity=0.75,\n        name = f\"Length of {lang1} sentences\",\n        marker=dict(color='rgba(171, 50, 96, 0.6)'))\n    trace2 = go.Histogram(\n        x=train_df[f\"{lang2}_len\"].values,\n        opacity=0.75,\n        name = f\"Length of {lang2} sentences\",\n        marker=dict(color='rgba(12, 50, 196, 0.6)'))\n\n    data = [trace1, trace2]\n    layout = go.Layout(barmode='overlay',\n                       title=f\"Lengths of {lang1} and {lang2} sentences\",\n                       xaxis=dict(title='Length'),\n                       yaxis=dict( title='Count'),\n    )\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig, config={'showLink': True})","f8cdbc33":"plotLangLen(\"EN\", \"NL\")","a4d767b0":"en_vocab_size = en_sp.get_piece_size()\nnl_vocab_size = nl_sp.get_piece_size()\nprint(f\"EN vocab size: {en_vocab_size}\")\nprint(f\"NL vocab size: {nl_vocab_size}\")","26aebedf":"print(en_sp.piece_to_id('__MUST_BE_UNKNOWN__'))\nprint(en_sp.id_to_piece(0))","e8c1da34":"en_max_length = train_df[\"EN_len\"].max()\nnl_max_length = train_df[\"NL_len\"].max()\n#30 for faster processing time\nen_max_length=30\nnl_max_length=en_max_length\nprint(en_max_length)\nprint(nl_max_length)","5e6f8ae0":"#use post padding to fill up short sentence with 0\nen_padded_seq = pad_sequences(train_df[\"EN_pieces\"].tolist(), maxlen=en_max_length, padding='post')\nnl_padded_seq = pad_sequences(train_df[\"NL_pieces\"].tolist(), maxlen=nl_max_length, padding='post')","a3ed7fd1":"#pick a sample\nen_padded_seq[2]","9745ccf0":"train_seq_df = pd.DataFrame( {'en_seq':en_padded_seq.tolist(), 'nl_seq':nl_padded_seq.tolist()})","ebd52ebd":"train_seq_df.tail()","17a7b6f3":"def define_model(input_vocab,output_vocab, input_length,output_length,output_dim):\n      model = Sequential()\n      #mark_zero , set 0 as special character reserved for unknown words  \n      model.add(Embedding(input_vocab, output_dim, input_length=input_length, mask_zero=True))\n      model.add(LSTM(output_dim))\n      #repeat the input (n) times\n#return_sequences=True returns all the outputs the encoder observed in the past, while RepeatVector repeats the very last output of the encoder.    \n      model.add(RepeatVector(output_length))\n    #return the full sequences\n      model.add(LSTM(output_dim, return_sequences=True))\n      #model.add(TimeDistributed(Dense(output_vocab, activation='softmax')))\n      \n      model.add(Dense(output_vocab, activation='softmax'))\n      return model","eb3b8af4":"train_seq_df.shape","537d849a":"train, test = train_test_split(train_seq_df, test_size=0.1, random_state = 3)","68900386":"'''\n#K-fold didn't help much in this case\n\nkfold = KFold(n_splits=3)\nk_preds = []\nfor i, (train_set, val_set ) in enumerate(kfold.split(train)):\n  print(f\"Running fold:{i}\") \n  train_set_X = np.asarray(train.iloc[ train_set, 1].tolist()) \n  train_set_Y = np.asarray(train.iloc[ train_set, 0].tolist())\n\n  val_set_X = np.asarray(train.iloc[ val_set, 1].tolist()) \n  val_set_Y = np.asarray(train.iloc[ val_set, 0].tolist()) \n\n  k_model = None \n  k_model = define_model(ja_vocab_size, en_vocab_size, ja_max_length, en_max_length, 1024) \n  k_model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy')\n  k_filename = f\"nmt_model_{i}\"\n  k_checkpoint = ModelCheckpoint(k_filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n  \n  # train model\n#  k_history = k_model.fit(train_set_X, train_set_Y.reshape(train_set_Y.shape[0], train_set_Y.shape[1], 1),\n#                    epochs=10, batch_size=64, validationdata = (val_set_X, val_set_Y.reshape(val_set_Y.shape[0], val_set_Y.shape[1], 1)), callbacks=[checkpoint], \n#                    verbose=1)\n  k_history = k_model.fit(train_set_X, train_set_Y.reshape(train_set_Y.shape[0], train_set_Y.shape[1], 1),\n                    epochs=10, batch_size=64, callbacks=[k_checkpoint],validation_split = 0.2,\n                    verbose=1)\n  k_model = load_model(f\"nmt_model_{i}\")\n  k_preds.append(model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1]))))\n\n#  model = loadmodel(filepath) \n#  pred += model.predict(Xtest, batchsize = batchsizes, verbose = 1)\n#  preds = pred\/n_folds\n#for train_set, val_set in kfold.split(train):    \n#   trainX = np.asarray(train_seq_df.loc[ train, \"ja_seq\" ].tolist()) \n#   trainY = np.asarray(train_seq_df.loc[ train, \"en_seq\" ].tolist())\n    \n#   testX = np.asarray(train_seq_df.loc[ test, \"ja_seq\" ].tolist()) \n#   testY = np.asarray(train_seq_df.loc[ test, \"en_seq\" ].tolist()) \n'''","98d7e135":"trainX = np.asarray(train[\"nl_seq\"].tolist())\ntrainY = np.asarray(train[\"en_seq\"].tolist())\n\ntestX = np.asarray(test[\"nl_seq\"].tolist())\ntestY = np.asarray(test[\"en_seq\"].tolist())","2d0a34d7":"model = define_model(nl_vocab_size, en_vocab_size, nl_max_length, en_max_length, 1024)","3c260d02":"#RMSprop is recommended for RNN, sparse_categorical_crossentropy for densed target output as integers\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy')","8497eda6":"def encode_output(sequences, vocab_size):\n   ylist = list()\n   for sequence in sequences:\n    encoded = to_categorical(sequence, num_classes=vocab_size)\n    ylist.append(encoded)\n    y = array(ylist)\n    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n    return y","b6b9212c":"filename = 'nmt_model'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\n# train model\nhistory = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n                    epochs=15, batch_size=64, validation_split = 0.1,callbacks=[checkpoint], \n                    verbose=1)\n","d741bdd8":"trace1 = go.Scatter(\n    y=history.history['loss'],\n    name = \"Training Loss\",\n    marker=dict(color='rgba(171, 50, 96, 0.6)'))\ntrace2 = go.Scatter(\n    y=history.history['val_loss'],\n    name = \"Validation Loss\",\n    marker=dict(color='rgba(12, 50, 196, 0.6)'))\n\ndata = [trace1, trace2]\nlayout = go.Layout(title='Loss and Val_Loss in 15 Epochs',\n                   xaxis=dict(title='Epoch'),\n                   yaxis=dict( title='Loss'),\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig, config={'showLink': True})","65a22adb":"model = load_model('nmt_model')","1e26b3cc":"preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))","da824516":"def get_word(ids, tokenizer):\n    return tokenizer.DecodeIds(list(filter(lambda a: a != 0, ids.tolist())))\n","7a447e33":"import random","ce0e8fd6":"y_indexs = random.sample(range(len(testY)),  30)\n\nfor y_index in y_indexs: \n  print(f\"--- Index: {y_index}\")\n  print(f\"NL: {get_word(testX[y_index], nl_sp)}\") \n  print(f\"EN: {get_word(testY[y_index], en_sp)}\") \n  print(f\"MT: {get_word(preds[y_index], en_sp)}\")\n # print(get_word(k_preds[0][y_index], en_sp))\n # print(get_word(k_preds[1][y_index], en_sp))\n # print(get_word(k_preds[2][y_index], en_sp))\n","4053147c":"test_ids = []\ntest_nls = []\ntest_ens = []\ntest_mts = []\nfor y_index in range(len(testY)): \n  test_ids.append(y_index)\n  test_nls.append(get_word(testX[y_index], nl_sp))\n  test_ens.append(get_word(testY[y_index], en_sp))\n  test_mts.append(get_word(preds[y_index], en_sp))\n\npredict_df = pd.DataFrame( {'id':test_ids, 'NL':test_nls, 'EN':test_ens, 'MT':test_mts})","fca55227":"pd.set_option('display.max_colwidth', 80)\n","73826c58":"predict_df.sample(10)","32f5e350":"[![Make an Easy NMT](https:\/\/i1.wp.com\/www.codeastar.com\/wp-content\/uploads\/2019\/11\/nmt_2.png)](https:\/\/www.codeastar.com\/nmt-make-an-easy-neural-machine-translator\/)\n\nThis is a demonstration on making an easy NMT with Keras and SentencePiece. For more details, please visit https:\/\/www.codeastar.com\/nmt-make-an-easy-neural-machine-translator\/ ."}}