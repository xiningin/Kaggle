{"cell_type":{"eaa956b9":"code","e78024aa":"code","c8acf577":"code","6c482f6e":"code","a7f68dcc":"code","7164bd62":"code","72b6e599":"code","a4dc97d4":"code","fba8c1ae":"code","08af5fbb":"code","66ca7856":"code","350f2270":"code","7d4eecac":"markdown"},"source":{"eaa956b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm #progress bars\nimport datetime as dt\nimport tensorflow as tf\n\nimport collections\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, Model\n# Input data files are available in the \"..\/input\/\" directory.\n\n","e78024aa":"train = pd.read_feather(\"..\/input\/preprocessor-for-data-bowl-2019\/train_processed.fth\")\n#test = pd.read_feather(\"..\/input\/preprocessor-for-data-bowl-2019\/test_processed.fth\")\ntrain_labels = pd.read_feather(\"..\/input\/preprocessor-for-data-bowl-2019\/train_labels_processed.fth\").set_index('installation_id')\n#test_sessions_map = pd.read_feather(\"..\/input\/preprocessor-for-data-bowl-2019\/test_labels_processed.fth\").to_dict()","c8acf577":"# The longest sequence has length 58988\nSEQ_LENGTH = 2000 #13000\n# see preprocessor -- this is 99 or 95 percentile of length","6c482f6e":"#### Parameters\nparams = {'dim': (SEQ_LENGTH,4),\n          'batch_size': 16,  \n          'shuffle': True\n}\n\nmodel_params = {\n          'LEARNING_RATE': 0.0001, #default is 0.001\n          'LOSS_FN': tf.keras.losses.CategoricalCrossentropy(),\n          'METRICS': ['categorical_accuracy'],\n                     #{'output0': ['accuracy', qwk],\n                     # 'output1': ['accuracy', qwk],\n                     # 'output2': ['accuracy', qwk],\n                     # 'output3': ['accuracy', qwk],\n                     # 'output4': ['accuracy', qwk],}\n         'CLIP_NORM': 1,\n         'LSTM_L2': 0.000001,\n    'DENSE_DROPOUT': 0.5,\n    'LSTM_DROPOUT': 0.4\n}\n\nBATCH_SIZE = params['batch_size']","a7f68dcc":"from tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.regularizers import L1L2\n\nnp.random.seed(0) # set random seed for reproducibility\n# input which receives correct_assessment, session_number, game_time\n# these are not embedded!\nnon_embedded_input = tf.keras.Input(dtype='float32',name='non_embedded_input', shape=(SEQ_LENGTH, 3))# batch_shape=(BATCH_SIZE, SEQ_LENGTH, 3))\n\nevent_id_input = tf.keras.Input(dtype='float32',name='event_id_input',shape=(SEQ_LENGTH))# batch_shape=(BATCH_SIZE, SEQ_LENGTH))\ntype_input = tf.keras.Input(dtype='float32',name='type_input',  shape=(SEQ_LENGTH))\nworld_input = tf.keras.Input(dtype='float32',name='world_input',shape=(SEQ_LENGTH))\n\nevent_id_embedding_layer = (tf.keras.layers.Embedding(input_dim=390, # +1 bc of masking\n                                                     output_dim=10,  # TODO: is this too high?\n                                                     input_length=SEQ_LENGTH, # should I set this?\n                                                     embeddings_initializer='uniform', \n                                                     #embeddings_regularizer=l2(.000001), \n                                                     #activity_regularizer=l2(.000001), \n                                                     embeddings_constraint=None, \n                                                     mask_zero=True \n                                                    )(event_id_input))\ntype_embedding_layer =      (tf.keras.layers.Embedding(input_dim=5, # +1 bc of masking\n                                                     output_dim=3, \n                                                     input_length=SEQ_LENGTH, # should I set this?\n                                                     embeddings_initializer='uniform', \n                                                     #embeddings_regularizer=l2(.000001), \n                                                     #activity_regularizer=l2(.000001), \n                                                     embeddings_constraint=None, \n                                                     mask_zero=True \n                                                    )(type_input))\nworld_embedding_layer =    (tf.keras.layers.Embedding(input_dim=5, # +1 bc of masking\n                                                     output_dim=3, \n                                                     input_length=SEQ_LENGTH, # should I set this?\n                                                     embeddings_initializer='uniform', \n                                                     #embeddings_regularizer=l2(.000001), \n                                                     #activity_regularizer=l2(.000001), \n                                                     embeddings_constraint=None, \n                                                     mask_zero=True \n                                                    )(world_input))\n\n","7164bd62":"# should consider regularizing here or with an auxiliary output?\nfull_input = tf.keras.layers.concatenate([non_embedded_input,\n                                         event_id_embedding_layer,\n                                         type_embedding_layer,\n                                         world_embedding_layer], axis=-1)\n","72b6e599":"#MASK_VALUE = 0\n#masked_input = tf.keras.layers.Masking(mask_value=MASK_VALUE)(full_input)\nLSTM_LAYERS = 32 #\nlstm_layer = tf.keras.layers.LSTM(units=LSTM_LAYERS,\n                                 kernel_regularizer=l2(model_params['LSTM_L2']),\n                                 #activity_regularizer=l2(model_params['LSTM_L2']),# there's a bengio article saying that this is bad, could be added to the dense layer?\n                               #  dropout=model_params['LSTM_DROPOUT'],\n                                 #kernel_regularizer = L1L2(l1=0.01, l2=0.01),\n                                  return_sequences=True)(full_input) \nlstm_layer2 = tf.keras.layers.LSTM(units=LSTM_LAYERS,\n                                 kernel_regularizer=l2(model_params['LSTM_L2']),\n                                 #activity_regularizer=l2(model_params['LSTM_L2']),# there's a bengio article saying that this is bad, could be added to the dense layer?\n                                 dropout=model_params['LSTM_DROPOUT'],\n                                 #kernel_regularizer = L1L2(l1=0.01, l2=0.01\n                                                           )(lstm_layer) \n\n\n# should consider:\n#  stateful = True\n## DO NOT:  this holds the state *between aligned samples*.  E.g. you slice your long sequence into several, then feed those sequences in one at a time.","a4dc97d4":"normalized_layer = tf.keras.layers.BatchNormalization()(lstm_layer2)\nx = tf.keras.layers.Dropout(model_params['DENSE_DROPOUT'])(tf.keras.layers.Dense(64, activation='relu')(normalized_layer))\n#x = tf.keras.layers.Dropout(model_params['DENSE_DROPOUT'])(tf.keras.layers.Dense(64, activation='relu')(x))\n#x = tf.keras.layers.Dropout(model_params['DENSE_DROPOUT'])(tf.keras.layers.Dense(64, activation='relu')(x))\n#x = tf.keras.layers.Dropout(model_params['DENSE_DROPOUT'])(tf.keras.layers.Dense(64, activation='relu')(x))\n\noutput0 = tf.keras.layers.Dropout(0)(tf.keras.layers.Dense(4, activation='softmax', name='output0')(lstm_layer2))\n#output1 = tf.keras.layers.Dense(4, activation='softmax', name='output1')(x)\n#output2 = tf.keras.layers.Dense(4, activation='softmax', name='output2')(x)\n#output3 = tf.keras.layers.Dense(4, activation='softmax', name='output3')(x)\n#output4 = tf.keras.layers.Dense(4, activation='softmax', name='output4')(x)","fba8c1ae":"train_labels = train_labels.groupby('installation_id')['accuracy_group'].apply(lambda x : x.to_numpy()).to_frame()\ntrain_labels = train_labels.to_dict(orient='dict')['accuracy_group']","08af5fbb":"model_inputs = [non_embedded_input, event_id_input, type_input, world_input]\nmodel = tf.keras.Model(inputs=model_inputs, outputs=output0)\n\nmy_optimizer = tf.keras.optimizers.Adam(learning_rate=model_params['LEARNING_RATE'],  \n                                        beta_1=0.9, \n                                        beta_2=0.999, \n                                        amsgrad=True,)\n#                                        clipnorm = model_params['CLIP_NORM'])\n\n#my_optimizer_2 = tf.keras.optimizers.Adamax(learning_rate=model_params['LEARNING_RATE'])\n\nimport pickle\nwith open('..\/input\/preprocessor-for-data-bowl-2019\/event_ids_map.pkl', 'rb') as file:\n    event_ids_map = pickle.load(file)\nwith open('..\/input\/preprocessor-for-data-bowl-2019\/took_assessments_map.pkl','rb') as file:\n    took_assessments_map = pickle.load(file)\n#assessment_codes = [k for k in reverse_activities_map if 'Assessment' in reverse_activities_map[k]]\nthe_models = {i : tf.keras.models.clone_model(model) for i in took_assessments_map}\n","66ca7856":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef save_name(activity):\n    return ModelCheckpoint(filepath=str(activity), save_best_only = True)\n\ndef useful_callbacks(activity):\n    return [#save_name(activity), \n            EarlyStopping(patience=50, restore_best_weights=True)]\n\nhistory = {}\n\nfor assessment in the_models:  \n    print(\"Starting model for \" + assessment + \".\")\n    the_models[assessment].compile(optimizer = my_optimizer,\n                        loss = model_params['LOSS_FN'],\n                        metrics= model_params['METRICS']\n                            )\n    print(\"Compiled model for \" + assessment + \".\")\n    print(\"Time to fit.\")\n    with np.load(\"..\/input\/preprocessor-for-data-bowl-2019\/data\/X_\" + assessment + \".npz\", allow_pickle=True) as data_X: # TODO: could consider mmap_mode?  dunno\n        X0 = data_X['x0']#[:,-SEQ_LENGTH:]\n        X1 = data_X['x1']#[:,-SEQ_LENGTH:]\n        X2 = data_X['x2']#[:,-SEQ_LENGTH:]\n        X3 = data_X['x3']#[:,-SEQ_LENGTH:]\n    Y = np.load(\"..\/input\/preprocessor-for-data-bowl-2019\/data\/Y_\" + assessment + \".npy\", allow_pickle=True)\n    history[assessment] = the_models[assessment].fit([X0, X1, X2, X3], Y,\n                         validation_split=.1,\n                         epochs = 200,\n                         callbacks = useful_callbacks(assessment),\n                         verbose=1)\n\n","350f2270":"for assessment in the_models:\n    tf.keras.models.save_model(the_models[assessment], assessment + '.h5', save_format='h5')\n    with open('history_' + assessment + '.pkl', 'wb') as open_file:\n        pickle.dump(history[assessment].history,open_file)","7d4eecac":"for assessment in history:\n    # Plot training & validation accuracy values\n    plt.plot(history[assessment].history['accuracy'])\n    plt.plot(history[assessment].history['validation_accuracy'])\n    plt.title('Model accuracy: ' + assessment)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n# Plot training & validation loss values\n    plt.plot(history[assessment].history['loss'])\n    plt.plot(history[assessment].history['val_loss'])\n    plt.title('Model loss: ' + assessment)\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()"}}