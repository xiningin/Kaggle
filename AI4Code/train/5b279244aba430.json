{"cell_type":{"8d33e0c7":"code","3cbd173d":"code","f700f02a":"code","5d40e80b":"code","5a8d75ab":"code","5628ed8c":"code","0ed1a98d":"code","2d0ca4c0":"code","1f8620ad":"code","0c55c282":"code","bf534b66":"code","befcba69":"code","b1ca049f":"code","88dc2e87":"code","ee02630f":"code","af9f702b":"code","e96b16d9":"code","4214c6f0":"code","cb770979":"code","2b12f9c8":"code","a48ed123":"code","131dd599":"code","9709a0b3":"code","944288bc":"code","859cc542":"code","28886dfc":"code","25dccb55":"code","3a42afe3":"markdown","a5b6d80a":"markdown","64245f48":"markdown","150e7a1f":"markdown","e5396947":"markdown","29bf2739":"markdown","057ffa94":"markdown","f3d7e480":"markdown","520be17b":"markdown","e5dec06f":"markdown","59a47ac7":"markdown","9c98a0f9":"markdown","e3878854":"markdown","d6ccc454":"markdown","910720dd":"markdown","b2dd0890":"markdown","f8dcfd05":"markdown","cee9cebc":"markdown","343cd1d4":"markdown","832914bd":"markdown","cf9b6d43":"markdown","425b7f23":"markdown","3ecbe18e":"markdown","867744ad":"markdown","b76900ac":"markdown","1bb20a43":"markdown","18c4f80c":"markdown","74ea5caa":"markdown","3105e5b5":"markdown","eb192181":"markdown","91dafac8":"markdown","9e11ba97":"markdown","3f849ff9":"markdown","44797fd4":"markdown","ad4d91b6":"markdown","e3c73090":"markdown","6c12da7f":"markdown"},"source":{"8d33e0c7":"# Data Management\nfrom dateutil import relativedelta as rd\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n\npyo.init_notebook_mode()\nimport seaborn as sns\n\n# Regression\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport statsmodels.graphics.api as smg","3cbd173d":"# Read and rename column country\n# cty_info = pd.read_csv('..\/input\/countryinfo\/covid19countryinfo.csv').rename(columns={'country':'Country'})\n# cty_info = pd.read_csv(local_path + 'covid19countryinfo.csv').rename(columns={'country':'Country'})\ncty_info = pd.read_csv('..\/input\/countryinfo\/covid19countryinfo.csv').rename(columns={'country':'Country'})\n\n# Filter observations with aggregate country-level information\n# The column region for region-level observations is populated\ncty_info = cty_info[cty_info.region.isnull()]\n\n# Convert string data type to floating data type\n# Remove comma from the fields\ncty_info['healthexp'] = cty_info[~cty_info['healthexp'].isnull()]['healthexp'].str.replace(',','').astype('float')\ncty_info['gdp2019'] = cty_info[~cty_info['gdp2019'].isnull()]['gdp2019'].str.replace(',','').astype('float')\n\n# Convert to date objects with to_datetime method\ngov_actions = ['quarantine', 'schools', 'gathering', 'nonessential', 'publicplace']\n\nfor gov_action in gov_actions:\n    cty_info[gov_action] = pd.to_datetime(cty_info[gov_action], format = '%m\/%d\/%Y')\n    \n# Filter columns of interest\n# Note: feel free to explore other variables or datasets\ncty_info = cty_info[['Country', 'avghumidity', 'avgtemp', 'fertility', 'medianage', 'urbanpop', 'quarantine', 'schools', \\\n                    'publicplace', 'gatheringlimit', 'gathering', 'nonessential', 'hospibed', 'smokers', \\\n                    'sex0', 'sex14', 'sex25', 'sex54', 'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', \\\n                    'malelung', 'gdp2019', 'healthexp', 'healthperpop']]\n\n# cty_info.describe()\ncty_info.info()\n#cty_info.head(20)\n","f700f02a":"# Worldometer data\n# ================\n\n# worldometer_data = pd.read_csv('..\/input\/corona-virus-report\/worldometer_data.csv')\n# worldometer_data = pd.read_csv(local_path + 'worldometer_data.csv')\nworldometer_data = pd.read_csv(\"..\/input\/corona-virus-report\/worldometer_data.csv\")\n\n# Replace missing values '' with NAN and then 0\nworldometer_data = worldometer_data.replace(\"\", np.nan).fillna(0)\n\n# Transform variables and round them up to the two decimal points\n# Note that there are instances of division by zero issue when there are either zero total tests or total cases\nworldometer_data[\"Case Positivity\"] = round(\n    worldometer_data[\"TotalCases\"] \/ worldometer_data[\"TotalTests\"], 2\n)\nworldometer_data[\"Case Fatality\"] = round(\n    worldometer_data[\"TotalDeaths\"] \/ worldometer_data[\"TotalCases\"], 2\n)\n\n# Resolve the division by zero issue by replacing infinity value with zero\nworldometer_data[worldometer_data[\"Case Positivity\"] == np.inf] = 0\nworldometer_data[worldometer_data[\"Case Fatality\"] == np.inf] = 0\n\n# Place case positivity into three bins\nworldometer_data[\"Case Positivity Bin\"] = pd.qcut(\n    worldometer_data[\"Case Positivity\"], q=3, labels=[\"low\", \"medium\", \"high\"]\n)\n\n# Population Structure\n# worldometer_pop_struc = pd.read_csv('..\/input\/covid19-worldometer-snapshots-since-april-18\/population_structure_by_age_per_contry.csv')\n# worldometer_pop_struc = pd.read_csv(local_path + 'population_structure_by_age_per_contry.csv')\nworldometer_pop_struc = pd.read_csv(\"..\/input\/covid19-worldometer-snapshots-since-april-18\/population_structure_by_age_per_contry.csv\")\n\n# Replace missing values with zeros\nworldometer_pop_struc = worldometer_pop_struc.fillna(0)\n\n# Merge datasets by common key country\nworldometer_data = worldometer_data.merge(\n    worldometer_pop_struc, how=\"inner\", left_on=\"Country\/Region\", right_on=\"Country\"\n)\nworldometer_data = worldometer_data[worldometer_data[\"Country\/Region\"] != 0]\n\n# Country information\nworldometer_data = worldometer_data.merge(cty_info, how=\"left\", on=\"Country\")\n\n# Replace space in variable names with '_'\nworldometer_data.columns = worldometer_data.columns.str.replace(\" \", \"_\")\n\nworldometer_data.describe()\nworldometer_data.info()","5d40e80b":"# Full data\n# =========\n\n# full_table = pd.read_csv('..\/input\/corona-virus-report\/covid_19_clean_complete.csv')\n# full_table = pd.read_csv(local_path + 'covid_19_clean_complete.csv')\nfull_table = pd.read_csv(\"..\/input\/corona-virus-report\/covid_19_clean_complete.csv\")\nfull_table[\"Date\"] = pd.to_datetime(full_table[\"Date\"])\n\n# Examine DataFrame (object type, shape, columns, dtypes)\nfull_table.info()\n\n# type(full_table)\n# full_table.shape\n# full_table.columns\n# full_table.dtypes\n\n# Deep dive into the DataFrame\nfull_table.head()","5a8d75ab":"# Grouped by day, country\n# =======================\n\n# full_grouped = pd.read_csv('..\/input\/corona-virus-report\/full_grouped.csv')\n# full_grouped = pd.read_csv(local_path + 'full_grouped.csv')\nfull_grouped = pd.read_csv(\"..\/input\/corona-virus-report\/full_grouped.csv\")\nfull_grouped[\"Date\"] = pd.to_datetime(full_grouped[\"Date\"])\n# full_grouped.loc[full_grouped['Country\/Region'] == 'US', 'Country\/Region'] = 'USA'\nfull_grouped.head()\n\n# Correct country names in worldometer to make them consistent with dataframe full_grouped column Country\/Region before merging\nworldometer_data[\"Country\/Region\"].replace(\n    {\n        \"USA\": \"US\",\n        \"UAE\": \"United Arab Emirates\",\n        \"S. Korea\": \"South Korea\",\n        \"UK\": \"United Kingdom\",\n    },\n    inplace=True,\n)\n\n# Draw population and country-level data\nfull_grouped = full_grouped.merge(\n    worldometer_data[[\"Country\/Region\", \"Population\"]], how=\"left\", on=\"Country\/Region\"\n)\nfull_grouped = full_grouped.merge(\n    cty_info, how=\"left\", left_on=\"Country\/Region\", right_on=\"Country\"\n)\nfull_grouped[\"Confirmed per 1000\"] = (\n    full_grouped[\"Confirmed\"] \/ full_grouped[\"Population\"] * 1000\n)\n\n# Backfill data\nfull_grouped = full_grouped.fillna(method=\"ffill\")\n\n# Create post-invention indicators\ngov_actions = [\"quarantine\", \"schools\", \"gathering\", \"nonessential\", \"publicplace\"]\n\nfor gov_action in gov_actions:\n    full_grouped[\"post_\" + gov_action] = (\n        full_grouped[\"Date\"] >= full_grouped[gov_action]\n    )\n    full_grouped[\"day_rel_to_\" + gov_action] = (\n        full_grouped[\"Date\"] - full_grouped[gov_action]\n    ).dt.days\n\n# Create percent changes in covid19 outcomes\ncovid_outcomes = [\"Confirmed\", \"Deaths\", \"Recovered\", \"Active\", \"Confirmed per 1000\"]\n\nfor covid_outcome in covid_outcomes:\n    full_grouped[\"pct_change_\" + covid_outcome] = full_grouped.groupby(\n        [\"Country\/Region\"]\n    )[covid_outcome].pct_change()\n    full_grouped[full_grouped[\"pct_change_\" + covid_outcome] == np.inf] = 0\n\n# Replace space in variable names with '_'\nfull_grouped.columns = full_grouped.columns.str.replace(\" \", \"_\")\n\nfull_grouped.info()\n# full_grouped.tail(20)\n# print(full_grouped.iloc[0,0])\n# full_grouped[full_grouped['quarantine'] != None]['Country\/Region'].unique()\n# full_grouped[full_grouped['Country\/Region'] == 'Germany'][['quarantine','day_rel_to_quarantine']]\n# list(full_grouped.columns.values)\n# full_grouped.describe()","5628ed8c":"# Visualize the missingness isue in the dataset\nsns.heatmap(cty_info.isnull(), cbar=False)","0ed1a98d":"# Create a function to plot (reusing from the previous BootCamp)\n\n\ndef gt_n(n):\n    countries = full_grouped[full_grouped[\"Confirmed\"] > n][\"Country\/Region\"].unique()\n    temp = full_table[full_table[\"Country\/Region\"].isin(countries)]\n    temp = temp.groupby([\"Country\/Region\", \"Date\"])[\"Confirmed\"].sum().reset_index()\n    temp = temp[temp[\"Confirmed\"] > n]\n    temp[\"Log Confirmed\"] = np.log(1 + temp[\"Confirmed\"])\n    # print(temp.head())\n\n    min_date = temp.groupby(\"Country\/Region\")[\"Date\"].min().reset_index()\n    min_date.columns = [\"Country\/Region\", \"Min Date\"]\n    # print(min_date.head())\n\n    from_nth_case = pd.merge(temp, min_date, on=\"Country\/Region\")\n    from_nth_case[\"Date\"] = pd.to_datetime(from_nth_case[\"Date\"])\n    from_nth_case[\"Min Date\"] = pd.to_datetime(from_nth_case[\"Min Date\"])\n    from_nth_case[\"N days\"] = (\n        from_nth_case[\"Date\"] - from_nth_case[\"Min Date\"]\n    ).dt.days\n    # print(from_nth_case.head())\n\n    fig = px.line(\n        from_nth_case,\n        x=\"N days\",\n        y=\"Confirmed\",\n        color=\"Country\/Region\",\n        title=\"N days from \" + str(n) + \" case\",\n        height=600,\n    )\n    fig.show()\n\n    fig = px.line(\n        from_nth_case,\n        x=\"N days\",\n        y=\"Log Confirmed\",\n        color=\"Country\/Region\",\n        title=\"N days from \" + str(n) + \" case\",\n        height=600,\n    )\n    fig.show()","2d0ca4c0":"def graph_cty_exceeding_cases(n, full_grouped_df, full_table_df):\n    \"\"\"\n    Function graphs the countries with more than n confirmed cases \n    \n    Parameters:\n        n (int): The threshold minimum number of cases \n        fully_grouped_df (pandas.DataFrame): the data source used. From Kaggle's full_grouped data set \n        full_data_df (pandas.DataFrame): the data source used. From Kaggle's covid_19_clean_complete data set \n    Returns:\n        null\n    \"\"\"\n    # Data Preprocessing\n    countries = full_grouped_df[full_grouped_df[\"Confirmed\"] > n][\n        \"Country\/Region\"\n    ].unique()\n    temp = full_table_df[full_table_df[\"Country\/Region\"].isin(countries)]\n    temp = temp.groupby([\"Country\/Region\", \"Date\"])[\"Confirmed\"].sum().reset_index()\n    temp = temp[temp[\"Confirmed\"] > n]\n    temp[\"Log Confirmed\"] = np.log(1 + temp[\"Confirmed\"])\n    # print(temp.head())\n\n    min_date = temp.groupby(\"Country\/Region\")[\"Date\"].min().reset_index()\n    min_date.columns = [\"Country\/Region\", \"Min Date\"]\n    # print(min_date.head())\n\n    from_nth_case = pd.merge(temp, min_date, on=\"Country\/Region\")\n    from_nth_case[\"Date\"] = pd.to_datetime(from_nth_case[\"Date\"])\n    from_nth_case[\"Min Date\"] = pd.to_datetime(from_nth_case[\"Min Date\"])\n    from_nth_case[\"N days\"] = (\n        from_nth_case[\"Date\"] - from_nth_case[\"Min Date\"]\n    ).dt.days\n    # print(from_nth_case.head())\n\n    fig = px.line(\n        from_nth_case,\n        x=\"N days\",\n        y=\"Confirmed\",\n        color=\"Country\/Region\",\n        title=\"N days from \" + str(n) + \" case\",\n        height=600,\n    )\n    fig.show()\n\n    fig = px.line(\n        from_nth_case,\n        x=\"N days\",\n        y=\"Log Confirmed\",\n        color=\"Country\/Region\",\n        title=\"N days from \" + str(n) + \" case\",\n        height=600,\n    )\n    fig.show()","1f8620ad":"def plot_gov_action(covid_outcome, gov_action, full_grouped_df):\n    \"\"\"\n    Function plots the government action and outcome \n    \n    Parameters:\n        covid_outcome (str): The outcome from covid\n        gov_action (str): The government action to be analysed \n        full_grouped_df (pandas.DataFrame): the data source used. From Kaggle's full_grouped data set \n    Returns:\n        null\n    \"\"\"\n    fig = px.scatter(\n        full_grouped_df[full_grouped_df[gov_action] != None],\n        x=\"day_rel_to_\" + gov_action,\n        y=covid_outcome,\n        color=\"Country\/Region\",\n        title=\"N days from \" + gov_action,\n        height=600,\n    )\n    fig.update_layout(yaxis=dict(range=[0, 10]))\n    fig.show()","0c55c282":"# gov_actions = ['quarantine', 'schools', 'gathering', 'nonessential', 'publicplace']\n\nplot_gov_action('pct_change_Confirmed_per_1000', 'quarantine', full_grouped)\nplot_gov_action('pct_change_Confirmed_per_1000', 'schools', full_grouped)\n","bf534b66":"full_grouped['Confirmed_per_1000'].describe()","befcba69":"full_grouped['log_Confirmed_per_1000'] = np.log(full_grouped['Confirmed_per_1000']+1)\nfull_grouped['log_Confirmed_per_1000'].describe()","b1ca049f":"#Plot pairplot with countries organized by WHO regions\ng = sns.pairplot(full_grouped[['log_Confirmed_per_1000', 'avghumidity', 'avgtemp', 'urbanpop', 'WHO_Region']], hue='WHO_Region')","88dc2e87":"#plt.matshow(full_grouped.corr())\n#plt.show()\n\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = full_grouped.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Plot heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, square=True, ax=ax)","ee02630f":"full_grouped.columns","af9f702b":"# 'post_schools', 'post_gathering', 'post_nonessential', 'post_publicplace',\n\n# Create interaction term\nfull_grouped['quarXurbanpop'] = full_grouped['post_quarantine'] * full_grouped['urbanpop']\n\n# OLS regression\ny = full_grouped['log_Confirmed_per_1000']\nX = full_grouped[['post_quarantine', 'avghumidity', 'avgtemp', 'urbanpop', 'quarXurbanpop']]\nX = sm.add_constant(X)\n\nols_model=sm.OLS(y,X.astype(float), missing='drop')\nresult=ols_model.fit()\nprint(result.summary2())","e96b16d9":"from statsmodels.graphics.gofplots import ProbPlot\n\nmodel_norm_residuals = result.get_influence().resid_studentized_internal\n\nQQ = ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n\n# annotations\nabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i,\n                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));","4214c6f0":"from statsmodels.graphics.regressionplots import plot_leverage_resid2\n\nfig, ax = plt.subplots(figsize=(8,6))\nfig = plot_leverage_resid2(result, ax = ax)","cb770979":"import statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\n\nname = ['Breusch-Pagan Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(result.resid, result.model.exog)\nlzip(name, test)","2b12f9c8":"## Function to get the pandemic countries\ndef get_explosive_countries(df, confirmed_cases, n_explosive_week):\n    \"\"\"\n    Function filters the dataframe based on a threshold number of confirmed_cases, \n    and minimum number of weeks where there is over 100% growth in cases \n    \n    Parameters:\n        df (pandas.DataFrame): The dataframe used in the filtering \n        confirmed_cases (int): The threshold minimum number of cases to be considered for filtering \n        n_explosive_week (int): The threshold minimum number of weeks in which there is explosive growth in cases\n    Returns:\n        tuple: (list - countries with explosive rates, numpy.float - median rate of growth)\n    \"\"\"\n    explosive_country = []\n    confirmed_cases_per_1000_at_explosion = []\n    \n    # identify unique set of countries\n    for cty in df['Country\/Region'].unique():\n        \n        # filter observations one country at a time\n        country = df[df['Country\/Region']==cty]\n        #print(cty)\n        \n        #By plotting the confirmed cases over time,\n        #the confirmed cases takes exponential shape after critical mass of n confirmed cases\n        \n        country = country[country['Confirmed'] > confirmed_cases]\n\n        if len(country): \n        \n            # print('... confirmed cases more than ' + str(confirmed_cases))  \n            # print('... first day is ' + str(country.iloc[0,0]))  \n            \n            country.reset_index(drop=True, inplace=True)\n            spread_rate=country['Confirmed'].pct_change(7).values\n            explosive_spread_counter=0\n            tmp_list=[]\n\n            #Check if there is an explosive growth over one-week period\n            for i in range(7,len(spread_rate),7):\n                if spread_rate[i] > 1.0: #100% growth over one-week\n                    explosive_spread_counter += 1\n                    tmp_list.append(country.iloc[i,38]) #confirmed cases per 1000 population\n                    # print(tmp_list)\n                    \n            #Term a country pandemic if doubling effect continued for more than a week        \n            if explosive_spread_counter > n_explosive_week: #100% growth over one-week for at least one week\n                explosive_country.append(cty)\n                confirmed_cases_per_1000_at_explosion.extend(tmp_list)\n                \n        else: \n            \n            pass\n            # print('... confirmed cases less than' + str(confirmed_cases))    \n    \n    \n    # print(confirmed_cases_per_1000_at_explosion)\n                     \n    median_rate = np.quantile(confirmed_cases_per_1000_at_explosion,0.5)\n    \n    return explosive_country, median_rate","a48ed123":"# identify current epicenters: at least 1000 confirmed cases and at least two weekly 100% surge in confirmed cases\nexplosive_country, median_rate = get_explosive_countries(full_grouped[full_grouped['Confirmed_per_1000'].notnull()],1000,2)\n\n# display the list of explosive countries\nprint('List of Explosive Countries: ', explosive_country)\n\n# display the median confirmed cases per 1000 population at explosion\nprint('Median Confirmed Cases per 1000 Population at Explosion: ', str(median_rate))","131dd599":"# create a variable 'explosive_country' indicating whether a country is in the list of explosive country\n# turn datatype boolean to integer\nworldometer_data['explosive_country'] = worldometer_data['Country\/Region'].isin(explosive_country).astype('int')\n\n# filter observations that are not in the list of explosive countries\nworldometer_data[worldometer_data['explosive_country'] == 0]","9709a0b3":"# review the variables available\nworldometer_data.columns","944288bc":"# dependent\/target\/outcome variable\ny = worldometer_data['explosive_country']\n\n# independent\/predictor\/explanatory variable\nX = worldometer_data[['avghumidity', 'avgtemp', 'urbanpop', 'gdp2019', 'healthperpop', 'TotalTests']]\n\n# logit regression\n# turn independent variables into floating type (best practice)\n# 'missing='drop'' drops rows with missing values from the regression\nlogit_model=sm.Logit(y,X.astype(float), missing='drop' )\n\n# fit logit model into the data\nresult=logit_model.fit()\n\n# summarize the logit model\nprint(result.summary2())","859cc542":"# calculate the predicted probability with the parameter estimates and values of Xs (i.e., the independent variables)\ny_hats2 = result.predict(X)\n\n# assign the values of predicted probabilities to worldometer_data\nworldometer_data['explosive_country_probability'] = y_hats2\n\n# filter countries that are not in the earlier list of countries with explosive number of confirmed cases\nnext_explosive_countries = worldometer_data[~worldometer_data['Country\/Region'].isin(explosive_country)]\n\n# sort dataframe next_explosive_countries by the predicted probabilities in a descending order\n# and then display the 20 countries with the highest probabilities of experiencing explosive number of confirmed cases \n# note our covid19 dataset (i.e., confirmed cases) was last updated on 27 July 2020\nnext_explosive_countries.sort_values(by='explosive_country_probability', ascending=False)[['Country\/Region', 'explosive_country_probability']].head(20)","28886dfc":"gt_n(100000)","25dccb55":"graph_cty_exceeding_cases(100000, full_grouped, full_table)","3a42afe3":"Canada ![image.png](attachment:image.png)","a5b6d80a":"<a id='section-one'><\/a>\n# 1. INTRODUCTION\n\nIn this notebook, we examine whether government interventions (e.g., quarantine, school closure, etc.) matter and how much do they matter if they do using ordinary least square (OLS) regressions. This is an example of inferential statistics (i.e., we are trying to infer the effect of the various government interventions from a sample of countries that have imposed some forms of restrictions relative to those that have not). Then, we use logistic regression to predict the next epicenter that is likely to experience a surge in confirmed cases of Covid19.","64245f48":" F-statistic: The null hypothesis states that the model with no independent variables (intercept-only) fits the data as well as your model.   \n Omnibus and Jarque-Bera: The null hypothesis states that the residuals are normally distributed.   \n Condition No: A test of potential multicollinearity issue. The informal rule of thumb: >=15 multicollinearity is an issue, > 30 a serious concern.   \n Durbin-Watson: The null hypothesis states that the residuals are not serially correlated.   \n   \n ![image.png](attachment:image.png)\n Source: Analystforum.com","150e7a1f":"<a id='section-three'><\/a>\n# 3. STORY","e5396947":"The World ![image.png](attachment:image.png)","29bf2739":"### A. Normality of residuals","057ffa94":"It looks like our model has given us a little foresight into countries that may experience strong resurgence in Covid19 confirmed cases. The timing of the resurgence is unknown, but all five countries experience a resurgence in cases within 7 weeks after.","f3d7e480":"Please add these datasets:  \n1. countryinfo: https:\/\/www.kaggle.com\/koryto\/countryinfo   \n2. Covid19 datasets: https:\/\/www.kaggle.com\/imdevskp\/corona-virus-report   \n3. Covid19 worldometer: https:\/\/www.kaggle.com\/selfishgene\/covid19-worldometer-snapshots-since-april-18   ","520be17b":"India ![image.png](attachment:image.png)","e5dec06f":"Q-Q Plot","59a47ac7":"# TABLE OF CONTENTS\n\n* [1. INTRODUCTION](#section-one)\n* [2. SETUP](#section-two)\n    - [2.1 Draw Packages](#subsection-two-one)\n    - [2.2 Import and Wrangle Data](#subsection-two-two)\n    - [2.3 Define Function](#subsection-two-three)\n* [3. STORY](#section-three)\n    - [3.1 Question 1: Do government actions matter?](#subsection-three-one)\n    - [3.2 Question 2: How much do government interventions matter?](#subsection-three-two)\n    - [3.3 Question 3: Which countries are epicenters now?](#subsection-three-three)\n    - [3.4 Question 4: Which countries may be the epicenters next?](#subsection-three-four)\n* [4. CONCLUSION](#section-four)\n* [5. REFERENCES](#section-five)","9c98a0f9":"<a id='subsection-two-two'><\/a>\n## 2.2 Import and Wrangle Data","e3878854":"Only the dependent\/response variable is log-transformed. Exponentiate the coefficient, subtract one from this number, and multiply by 100. This gives the percent increase (or decrease) in the response for every one-unit increase in the independent variable. The coefficient is -0.1455. (e^(-0.1455)-1) * 100 = -13.54%. So, the imposition of quarantine lowers confirmed cases per 1000 by 13.54%.","d6ccc454":"Kazakhstan ![image.png](attachment:image.png)","910720dd":"In summary, we need to be careful with any inferences we make with this model specification because of the non-normal residual (Breusch Pagan test), heteroskedastic residual (Q-Q plot, Jaeque-Bera test, Omnibus test), multicollinearity concern (Condition number), and the presence of strong outliers (Influence test), even though there is no evidence of serial correlation in the residuals (Durbin-Watson test) and the model specification is better than an intercept-only model (F-test).","b2dd0890":"Iran ![image.png](attachment:image.png)","f8dcfd05":"<a id='subsection-three-three'><\/a>\n## 3.3 Question 3: Which countries are the epicenters now?","cee9cebc":"Evidence of a violation of normality assumption of the residuals is present (visually). This is consistent with the inferential tests with Omnibus and Jarque-Bera tests.","343cd1d4":"<a id='section-four'><\/a>\n# 4. CONCLUSION\n\nIn this notebook, we first explore whether government actions matter and how much do they matter? We regress the log (number of Covid19 confirmed cases + 1) on a list of independent variables, including the indicators of various government actions. We model the regression with an OLS and then diagnose the model with various tests. The model specification is less than ideal. So, we need to be cautious in making any inferences from the results. Then, we turn our attention to the pandemic epicenters. We define an epicenter along two dimensions: the level of cumulative Covid19 confirmed cases and the number of outbreaks (which is a function of the number of weekly 100% or more surges in confirmed cases). Finally, we predict with Logit regression whether a country will become an epicenter (same definition) using a cross-sectional data (from Worldometer). Our top 5 predictions experience a surge in confirmed cases within 7 weeks.","832914bd":"<a id='subsection-three-four'><\/a>\n## 3.4 Question 4: Which countries may be the epicenters next?","cf9b6d43":"The number of global confirmed Covid19 cases have somewhat plateau out.","425b7f23":"# 2.3a Edited","3ecbe18e":"# Edited","867744ad":"### B. Influence test","b76900ac":"Japan ![image.png](attachment:image.png)","1bb20a43":"<a id='subsection-two-one'><\/a>\n## 2.1 Draw Packages","18c4f80c":"The null hypothesis of homoskedascity is rejected in favour of heteroskedasticity assumption.","74ea5caa":"The top 5 countries that are likely to experience explosive growth in confirmed cases are India, Japan, Iran, Canada, and Kazakhstan. Let's how our model predictions go. We will begin by revisiting at the plot of new Covid19 cases around the world and ask whether these 5 countries are expected to experience explosive growth in new cases from past trends. Recall that the list of explosive countries includes  ['Brazil', 'Ecuador', 'France', 'Germany', 'Italy', 'Russia', 'Spain', 'Turkey', 'USA', 'United Kingdom']","3105e5b5":"<a id='section-two'><\/a>\n# 2. SETUP","eb192181":"It looks like only India is among the top five countries not identified as countries with explosive growth in confirmed cases. Next we explore the plots of new Covid19 cases for the top 5 countries we predict to be the next epicenters. The screenshots are captured from Google as of 23 September 2020. We begin with the global number of confirmed cases to see whether there is a global resurgence. If that is the case, seeing a surge in confirmed cases in the top 5 countries is not surprising.","91dafac8":"<a id='subsection-two-three'><\/a>\n## 2.3 Define Function","9e11ba97":"We see the presence of some strong residuals. ","3f849ff9":"<a id='section-five'><\/a>\n# 5. REFERENCES\n\nThis notebook is partially adapted from code written by [Devakumar KP](https:\/\/www.kaggle.com\/imdevskp) and [ideas2it](https:\/\/www.kaggle.com\/ideas2it).\n\nDatasets are from [Worldometers](https:\/\/www.worldometers.info\/) and [COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University](https:\/\/github.com\/CSSEGISandData\/COVID-19). [Devakumar KP](https:\/\/www.kaggle.com\/imdevskp) has kindly extracted, cleaned, and shared these datasets on Kaggle.","44797fd4":"Note: sex ratio (i.e., amount of males per female) in each country and sex ratio by age groups. Lung disease data (i.e., death rate per 100k people) in each country and lung disease by sex.","ad4d91b6":"### C. Heteroskedasticity Test","e3c73090":"<a id='subsection-three-two'><\/a>\n## 3.2 Question 2: How much do government interventions matter? ","6c12da7f":"<a id='subsection-three-one'><\/a>\n## 3.1 Question 1: Do government actions matter? [Edited] "}}