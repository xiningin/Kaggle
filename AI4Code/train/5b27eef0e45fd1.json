{"cell_type":{"1792da96":"code","a43c9198":"code","3b7f80dc":"code","1e33a601":"code","44fcf445":"code","50a65617":"code","b2ae2c3d":"code","2629d1f6":"code","061d6f2d":"code","24b3a41c":"code","cefcd511":"code","5018282e":"code","17296a1a":"code","3f22466a":"code","63dbeee3":"code","ddfde898":"code","fd97d467":"code","52c81fa9":"code","fce78045":"code","c8948ba4":"code","bd7b1a72":"code","064d1605":"code","8a8454c9":"code","a2da55ed":"code","52be1e53":"code","5bb2807e":"code","4d72840f":"code","01f9d156":"code","9af48b72":"code","7d8c74c5":"code","6b5ad8ae":"code","030ce5e3":"markdown","eaab50f1":"markdown","fd8f04f4":"markdown","a90db986":"markdown","ed1009e0":"markdown","ef9d6eb7":"markdown","378ab69b":"markdown","b25126e5":"markdown","77254ee8":"markdown","f541b232":"markdown"},"source":{"1792da96":"# Import needed packages\nimport gensim\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nimport string\nimport re\nimport emoji\nimport operator \nimport tqdm\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nimport timeit\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nfrom IPython.display import Image\nimport nltk\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\ngc.enable()","a43c9198":"# Dictionary for lean import\ndtypesDict_tr = {\n'id'                            :         'int32',\n'target'                        :         'float16',\n'severe_toxicity'               :         'float16',\n'obscene'                       :         'float16',\n'identity_attack'               :         'float16',\n'insult'                        :         'float16',\n'threat'                        :         'float16',\n'asian'                         :         'float16',\n'atheist'                       :         'float16',\n'bisexual'                      :         'float16',\n'black'                         :         'float16',\n'buddhist'                      :         'float16',\n'christian'                     :         'float16',\n'female'                        :         'float16',\n'heterosexual'                  :         'float16',\n'hindu'                         :         'float16',\n'homosexual_gay_or_lesbian'     :         'float16',\n'intellectual_or_learning_disability':    'float16',\n'jewish'                        :         'float16',\n'latino'                        :         'float16',\n'male'                          :         'float16',\n'muslim'                        :         'float16',\n'other_disability'              :         'float16',\n'other_gender'                  :         'float16',\n'other_race_or_ethnicity'       :         'float16',\n'other_religion'                :         'float16',\n'other_sexual_orientation'      :         'float16',\n'physical_disability'           :         'float16',\n'psychiatric_or_mental_illness' :         'float16',\n'transgender'                   :         'float16',\n'white'                         :         'float16',\n'publication_id'                :         'int8',\n'parent_id'                     :         'float32',\n'article_id'                    :         'int32',\n'funny'                         :         'int8',\n'wow'                           :         'int8',\n'sad'                           :         'int8',\n'likes'                         :         'int16',\n'disagree'                      :         'int16',\n'sexual_explicit'               :         'float16',\n'identity_annotator_count'      :         'int16',\n'toxicity_annotator_count'      :         'int16'\n}","3b7f80dc":"# Load dataset\ntrain_data = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\n# kill all other columns except comment text and target\ncols_to_keep = ['comment_text','target']\ntrain_data = train_data.drop(train_data.columns.difference(cols_to_keep), axis=1)\ngc.collect()","1e33a601":"\"\"\"preprocessing.py\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https:\/\/colab.research.google.com\/drive\/1OGfaNb3J6wHJbSjAh2qlwbgBe_e4M2z1\n\n# Preprocessing.py\n\"\"\"\n\nimport pandas as pd\nimport matplotlib as plt\nimport re\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import Word\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nimport nltk\nimport gensim\nfrom nltk.corpus import stopwords\nimport emoji\nfrom nltk.corpus import wordnet\nimport datetime\nimport time\nimport operator\nfrom textblob import TextBlob\nfrom tqdm import tqdm, trange\nfrom nltk.tokenize import TweetTokenizer\n\ndef replace_contractions(text):\n  \n  \"\"\"\n  This functions check's whether a text contains contractions or not. \n  In case a contraction is found, the corrected value from the dictionary is \n  returned.\n  Example: \"I've\" towards \"I have\"\n  \"\"\"\n  \n  #replace words with \"'ve\" to \"have\"\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]ve\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'[\\'`\u00b4]ve\\b', \" have\", text)\n  \n  #replace words with \"'re\" to \"are\"\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]re\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'[\\'`\u00b4]re\\b', \" are\", text)\n  \n  #replace words with \"'ll\" to \"will\"\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]ll\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'[\\'`\u00b4]ll\\b', \" will\", text)\n  \n  #replace words with \"'m\" to \"am\"\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]m\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'[\\'`\u00b4]m\\b', \" am\", text)\n  \n  #replace words with \"'d\" to \"would\"\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]d\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'[\\'`\u00b4]d\\b', \" would\", text)\n  \n  #replace words with contraction according to the contraction_dict\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]\\w+\\b', text)\n  for x in matches:\n    if x in contraction_dict.keys():\n      text = text.replace(x, contraction_dict.get(x))\n  \n  # replace all \"'s\" by space\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]s\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'[\\'`\u00b4]s\\b', \" \", text)\n  return text\n\n# Dictionary of contractions coming out of the pre-investigation in the other kernel\ncontraction_dict = {\"Can't\":\"can not\", \"Didn't\":\"did not\", \"Doesn't\":\"does not\", \n                    \"Isn't\":\"is not\", \"Don't\":\"do not\", \"Aren't\":\"are not\", \"#\":\"\",\n                    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\",\n                    \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n                    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\",\n                    \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n                    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n                    \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\",\n                    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n                    \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\",\n                    \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n                    \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n                    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n                    \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n                    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n                    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n                    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n                    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n                    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                    \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\n                    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n                    \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n                    \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n                    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n                    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n                    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n                    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n                    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n                    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n                    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n                    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n                    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n                    \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n                    \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n                    \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\n                    \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n                    \"you're\": \"you are\", \"you've\": \"you have\", \"c'mon\":\"come on\",\n                    \"Don''t\":\"do not\", \"Haden't\":\"had not\", \"Grab'em\":\"grab them\", \"USA''s\":\"USA\",\n                    \"Pick'em\":\"pick them\", \"I'lll\":\"I will\", \"Tell'em\":\"tell them\", \"Y'all\":\"you all\",\n                    \"Wouldn't\":\"would not\", \"Shouldn't\":\"should not\", \"I'DVE\":\"I would have\",\n                    \"SHOOT'UM\":\"shoot them\", \"CANN'T\":\"can not\", \"COUD'VE\":\"could have\", \"Yo'ure\":\"you are\",\n                    \"LOCK'EM\":\"lock them\", \"G'night\":\"goodnight\", \"W'ell\":\"we will\", \"IT'D\":\"it would\",\n                    \"Couldn't\":\"could not\", \"LOCK'UM\":\"lock them\", \"WOULD'NT\":\"would not\", \"Cant't\":\"can not\",\n                    \"HADN'T\":\"had not\", \"It''s\":\"it is\", \"Don'ts\":\"do not\", \"Arn't\":\"are not\",\n                    \"We'll\":\"we will\", \"G'Night\":\"goodnight\", \"THAT'LL\":\"that will\", \"Dpn't\":\"do not\",\n                    \"Idon'tgetitatall\":\"I do not get it at all\", \"THEY'VE\":\"they have\", \"Le'ts\":\"let us\",\n                    \"SEND'EM\":\"send them\", \"AIN'T\":\"is not\", \"WE'D\":\"we would\", \"I'vemade\":\"I have made\",\n                    \"SHE'LL\":\"she will\", \"I'llbe\":\"I will be\", \"I'mma\":\"I am a\", \"Could'nt\":\"could not\",\n                    \"You'very\":\"you are very\", \"Light'em\":\"light them\", \"Con't\":\"can not\", \"I'\u039c\":\"I am\",\n                    \"Kick'em\":\"kick them\", \"Shoudn't\":\"should not\", \"That''s\":\"that is\",\n                    \"Didn't_work\":\"did not work\", \"You'rethinking\":\"you are thinking\", \"Dn't\":\"do not\",\n                    \"CON'T\":\"can not\", \"DON'T\":\"do not\", \"C'Mon\":\"come on\", \"You'res\":\"you are\",\n                    \"Amn't\":\"is not\", \"WE'RE\":\"we are\", \"Can't\":\"can not\", \"Kouldn't\":\"could not\",\n                    \"SHouldn't\":\"should not\", \"Does't\":\"does not\", \"COULD'VE\":\"could have\",\n                    \"TrumpIDin'tCare\":\"Trump did not care\", \"Iv'e\":\"I have\", \"Dose't\":\"does not\",\n                    \"DOESEN'T\":\"does not\", \"Give'em\":\"give them\", \"Won'tdo\":\"will not do\",\n                    \"They'l\":\"they will\", \"He''s\":\"he is\", \"I'veve\":\"I have\", \"Wern't\":\"were not\",\n                    \"Pay'um\":\"pay them\", \"She''l\":\"she will\", \"Y'know\":\"you know\", \"DIdn't\":\"did not\",\n                    \"O'bamacare\":\"Obamacare\", \"I'ma\":\"I am a\", \"Ma'am\":\"madam\", \"WASN'T\":\"was not\",\n                    \"Dont't\":\"do not\", \"Is't\":\"is not\", \"OU'RE\":\"you are\", \"YOU'RE\":\"you are\",\n                    \"Ther'es\":\"there is\", \"C'mooooooon\":\"come on\", \"They_didn't\":\"they did not\",\n                    \"Som'thin\":\"something\", \"Love'em\":\"love them\", \"You''re\":\"you are\", \"I'D\":\"I would\",\n                    \"HASN'T\":\"has not\", \"WOULD'VE\":\"would have\", \"WAsn't\":\"was not\", \"ARE'NT\":\"are not\",\n                    \"Dowsn't\":\"does not\", \"It'also\":\"it is also\", \"Geev'um\":\"give them\", \"Theyv'e\":\"they have\",\n                    \"Theyr'e\":\"they are\", \"Take'em\":\"take them\", \"Book'em\":\"book them\", \"Havn't\":\"have not\",\n                    \"DOES'NT\":\"does not\", \"Who''s\":\"who is\", \"WON't\":\"will not\", \"I'Il\":\"I will\",\n                    \"I'don\":\"I do not\", \"AREN'T\":\"are not\", \"Ev'rybody\":\"everybody\", \"Hold'um\":\"hold them\",\n                    \"WE'LL\":\"we will\", \"Cab't\":\"can not\", \"IJustDon'tThink\":\"I just do not think\",\n                    \"Wouldn'T\":\"would not\", \"U'r\":\"you are\", \"I''ve\":\"I have\", \"DONT'T\":\"do not\",\n                    \"G'morning\":\"good morning\", \"You'ld\":\"you would\", \"We''ll\":\"we will\", \"YOUR'E\":\"you are\",\n                    \"TrumpDoesn'tCare\":\"Trump does not care\", \"Wasn't\":\"was not\", \"You'all\":\"you all\",\n                    \"Y'ALL\":\"you all\", \"G'bye\":\"goodbye\", \"YOU'VE\":\"you have\", \"Does'nt\":\"does not\",\n                    \"Don'TCare\":\"do not care\",  \"Weren't\":\"were not\", \"Y'All\":\"you all\", \"They'lll\":\"they will\",\n                    \"You'reOnYourOwnCare\":\"you are on your own care\", \"I'veposted\":\"I have posted\",\n                    \"Run'em\":\"run them\", \"Vote'em\":\"vote them\", \"Would't\":\"would not\", \"I'l\":\"I will\",\n                    \"Ddn't\":\"did not\", \"I'mm\":\"I am\", \"Sshouldn't\":\"should not\", \"Your'e\":\"you are\",\n                    \"I'v\":\"I have\", \"We'really\":\"we are really\", \"DOESN'T\":\"does not\", \"DiDn't\":\"did not\",\n                    \"Needn't\":\"need not\", \"They'er\":\"they are\", \"Look'em\":\"look them\", \"I'v\u00c8\":\"I have\",\n                    \"Didn`t\":\"did not\", \"I'lll\":\"I will\", \"Wouldn't\":\"would not\", \"It`s\":\"it is\", \"What's\":\"what is\",\n                    \"ISN`T\":\"is not\", \"WE'RE\":\"we are\", \"Are'nt\":\"are not\", \"DOesn't\":\"does not\", \"I'M\":\"I am\",\n                    \"WON'T\":\"will not\", \"WEREN'T\":\"were not\", \"TrumpDon'tCareAct\":\"Trump do not care act\",\n                    \"HAVEN'T\":\"have not\", \"That''s\":\"that is\", \"Do'nt\":\"do not\"}\n\ndef replace_symbol_special(text,check_vocab=False, vocab=None): \n\n    ''' \n    This method can be used to replace dashes ('-') around and within the words using regex.\n    It only removes dashes for words which are not known to the vocabluary.\n    Next to that, common word separators like underscores ('_') and slashes ('\/') are replaced by spaces. \n    '''\n\n        \n    # replace all dashes and abostropes at the beginning of a word with a space\n    matches = re.findall(r\"\\s+(?:-|')\\w*\", text)\n    # if there is a match is in text\n    if len(matches) != 0:\n      # remove the dash from the match or better text\n      for match in matches:\n        text = re.sub(match, re.sub(r\"(?:-|')\", ' ', match), text)\n    \n    # replace all dashes and abostrophes at the end of a word with a space\n    # function works as above\n    matches = re.findall(r\"\\w*(?:-|')\\s+\", text)\n    if len(matches) != 0:\n      for match in matches:\n        text = re.sub(match, re.sub(r\"(?:-|')\", ' ', match), text)\n    \n    if check_vocab == True:\n      # replace dashes and abostrophes in the middle of the word only in case it is not known to a dictionary\n      # function works as above\n      matches = re.findall(r\"\\w*(?:-|')\\w*\", text)\n      if len(matches) != 0:\n        for match in matches:\n          #check if the word with dash in the middle in in the vocabluary\n          if match not in vocab.keys():\n            text = re.sub(match, re.sub(r\"(?:-|')\", ' ', match), text)\n    \n    #\n    text = re.sub(r'(?:_|\\\/)', ' ', text)\n    \n    text = re.sub(r' +', ' ', text)#-\n    return text\n  \n# Initially we consideredto remove the dash for words with this beginning. \n# However we found that it had almost no impact. Applying it to the total text, would kill correct spellings.\n# pre_suffix_dict = {'bi-':'bi', \t'co-':'co','re-':'re',\t'de-':'de','pre-':'pre',\t'sub-':'sub', 'un-':'un'}\n\ndef find_smilies(text):\n  \n  '''\n  For investigation only: Find most common keyboard typed smilies in the text.\n  '''\n  \n  #define a pattern to find typical keyboard smilies\n  pattern = r\"((?:3|<)?(?::|;|=|B)(?:-|'|'-)?(?:\\)|D|P|\\*|\\(|o|O|\\]|\\[|\\||\\\\|\\\/)\\s)\"\n  # Find the matches n the text\n  matches = re.findall(pattern, text)\n  # If the text contain matches print the text and the smilies found\n  if len(matches) != 0:\n    print(text, matches)\n    \n    \n\n    \ndef replace_smilies(text):\n  \n  '''\n  Simplyfied method to replace keyboard smilies with its very simple translation.\n  '''\n  \n  #Find and replace all happy smilies\n  matches = re.findall(r\"((?:<|O|o|@)?(?::|;|=|B)(?:-|'|'-)?(?:\\)|\\]))\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"((?:<|O|o|@)?(?::|;|=|B)(?:-|'|'-)?(?:\\)|\\]))\", \" smile \", text)\n  \n  #Find and replace all laughing smilies\n  matches = re.findall(r\"((?:<)?(?::|;|=)(?:-|'|'-)?(?:d|D|P|p)\\b)\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"((?:<)?(?::|;|=)(?:-|'|'-)?(?:d|D|P|p)\\b)\", \" smile \", text)\n  \n  #Find and replace all unhappy smilies\n  matches = re.findall(r\"((?:3|<)?(?::|;|=|8)(?:-|'|'-)?(?:\\(|\\[|\\||\\\\|\\\/))\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"((?:3|<)?(?::|;|=|8)(?:-|'|'-)?(?:\\(|\\[|\\||\\\\|\\\/))\", \" unhappy \", text)\n  \n  #Find and replace all kissing smilies\n  matches = re.findall(r\"((?:<)?(?::|;|=)(?:-|'|'-)?(?:\\*))\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"((?:<)?(?::|;|=)(?:-|'|'-)?(?:\\*))\", \" kiss \", text)\n  \n  #Find and replace all surprised smilies\n  matches = re.findall(r\"((?::|;|=)(?:-|'|'-)?(?:o|O)\\b)\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"((?::|;|=)(?:-|'|'-)?(?:o|O)\\b)\", \" surprised \", text)\n    \n  #Find and replace all screaming smilies\n  matches = re.findall(r\"((?::|;|=)(?:-|'|'-)?(?:@)\\b)\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"((?::|;|=)(?:-|'|'-)?(?:@)\\b)\", \" screaming \", text)\n    \n  #Find and replace all hearts\n  matches = re.findall(r\"\u2665|\u2764|<3|\u2765|\u2661\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"(?:\u2665|\u2764|<3|\u2765|\u2661)\", \" love \", text)\n  \n  text = re.sub(' +', ' ',text)\n  return text\n\ndef remove_stopwords(text, stop_words):\n  \n  ''' \n  Remove stopwords and multiple whitespaces around words\n  '''\n  \n  #Compile stopwords separated by | and stopped by word boundary \n  stopword_re = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b')\n  # Replace the stopwords by space\n  text = stopword_re.sub(' ', text)\n  #Replace double spaces by a single space\n  text = re.sub(' +', ' ',text)\n  return text\n\ndef clean_text(text, scope='general'):\n  \n  '''\n  This function handles text cleaning from various symbols.\n  - it translates special font types into the standard text type of python.\n  - it removes all symbols except for dashes and abostrophes being handled by \n    \"replace_symbol_special\".\n  - it handles multi letter appearances like \"comiiii\" > \"comi\"\n  - typical unknown words like \"Trump\"\n  '''\n  \n  \n  \n  #compile all special symbols from the dictionary to one regex function\n  translate_regex = re.compile(r'(' + r'|'.join(translate_dictionary.keys()) + r')')\n  \n  # find all matches of special symbols in the text\n  matches = re.findall(translate_regex, text)\n  # if there is one or more matches\n  if len(matches) != 0:\n    for x in matches:\n      if x in translate_dictionary.keys():\n        #replace the symbol by its replacement item\n        text = re.sub(x, translate_dictionary.get(x), text)\n  \n  # find and remove all \"http\" links\n  matches = re.findall(r'http\\S+', text)\n  if len(matches) != 0:\n    text = re.sub(r'http\\S+', '', text)\n  \n  #remove all backslashes\n  matches = re.findall(r'\\\\', text)\n  if len(matches) != 0:\n    text = re.sub(r'\\\\', ' ', text)\n  \n  # compile all remaining special characters into one translate line and replace them by space\n  # the translate function is really fast thus here our preferred choice\n  text = text.translate(str.maketrans(''.join(puncts), len(''.join(puncts))*' '))  \n  \n  #find words where 4 repetitions of a letter goes in a row and reduce them to only one\n  #we are not correcting words with 2 or three identical letters in a row as this could destroy correct words\n  #first find repeating characters\n  matches = re.findall(r'(.)\\1{3,}', text)\n  # is some are found\n  if len(matches) != 0:\n    #for each match replace it with its first letter (x[0])\n    for x in matches:\n      character_re = re.compile(x + '{3,}')\n      matchesInside = re.findall(character_re, text)\n      if len(matchesInside) != 0:\n        for x in matchesInside:\n          text = re.sub(x, x[0], text)\n          \n  # hahaha s by one haha \n  matches = re.findall(r'\\b[h,a]{4,}\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'\\b[h,a]{4,}\\b', 'haha', text)\n  \n  # as we found many unknown word variations including 'Trump' we reduce thse  words just to Trump\n  # being represented in most word vectors\n  matches = re.findall(r'\\w*[Tt][Rr][uU][mM][pP]\\w*', text)\n  if len(matches) != 0:\n    for x in matches:\n      text = re.sub(x, 'Trump', text)\n      \n  #remove potential double spaces generated during processing        \n  text = re.sub(' +', ' ',text) \n  \n  # those symbols are not touched by this function ->see replace_contraction or replace_special_symbols\n  #keep = [\"'\", '-', '\u00b4']\n  \n  \n  return text\n\n\n\n\n\n# The dictionary was generated in the compare and investigation phase in the other notebook\ntranslate_dictionary = {'\\t': 't', '0': '0', '1': '1', '2': '2', '3': '3', '5': '5', '6': '6',\n                         '8': '8', '9': '9', 'd': 'd', 'e': 'e', 'h': 'h', 'm': 'm', 't': 't',\n                         '\u00b2': '2', '\u00b9': '1', '\u011d': 'g', '\u0153': 'ae', '\u015d': 's', '\u01e7': 'g', '\u0251': '\u0251',\n                         '\u0252': 'a', '\u0254': 'c', '\u0259': 'e', '\u025b': 'e', '\u0261': 'g', '\u0262': 'g', '\u026a': 'i',\n                         '\u0274': 'n', '\u0280': 'r', '\u028f': 'y', '\u0299': 'b', '\u029c': 'h', '\u029f': 'l', '\u02b0': 'h',\n                         '\u02b3': 'r', '\u02b7': 'w', '\u02b8': 'y', '\u02e2': '5', '\u035e': '-', '\u035f': '_', '\u0366': 'o',\n                         '\u0391': 'a', '\u0392': 'b', '\u0395': 'e', '\u039c': 'm', '\u039d': 'n', '\u039f': 'o', '\u03a4': 't',\n                         '\u03ad': 'e', '\u03af': 'i', '\u03b1': 'a', '\u03ba': 'k', '\u03c7': 'x', '\u0406': 'i', '\u0410': 'a',\n                         '\u0411': 'e', '\u0415': 'e', '\u0417': '#', '\u0418': 'n', '\u041a': 'k', '\u041c': 'm', '\u041d': 'h',\n                         '\u041e': 'o', '\u0420': 'p', '\u0421': 'c', '\u0423': 'y', '\u0425': 'x',  '\u0432': 'b',\n                         '\u043a': 'k', '\u043c': 'm', '\u043d': 'h', '\u044b': 'bi', '\u044c': 'b', '\u0451': 'e', '\u0459': 'jb',\n                         '\u0493': 'f', '\u04af': 'y', '\u051c': 'w', '\u0570': 'h', '\u05d0': 'n', '\u0be6': '0', '\u0c66': 'o',\n                         '\u0d66': 'o', '\u0ed0': 'o', '\u13a5': 'i', '\u13ab': 'j', '\u13e7': 'd', '\u1428': '-', '\u1438': '<',\n                         '\u1472': 'b', '\u1473': 'b', '\u15de': 'd', '\u1d00': 'a', '\u1d04': 'c', '\u1d05': 'n', '\u1d07': 'e',\n                         '\u1d0a': 'j', '\u1d0b': 'k', '\u1d0d': 'm', '\u1d0f': 'o', '\u1d11': 'o', '\u1d18': 'p', '\u1d1b': 't',\n                         '\u1d1c': 'u', '\u1d20': 'v', '\u1d21': 'w', '\u1d35': 'i', '\u1d37': 'k', '\u1d3a': 'n', '\u1d3c': 'o',\n                         '\u1d49': 'e', '\u1d52': 'o', '\u1d57': 't', '\u1d58': 'u', '\u1e83': 'w', '\u1f00': 'a', '\u1f08': 'a',\n                         '\u1f0c': 'a', '\u1f76': 'l', '\u1f7a': 'u', '\u2012': '-', '\u2081': '1', '\u2083': '3', '\u2084': '4',\n                         '\u210b': 'h', '\u2120': 'sm', '\u212f': 'e', '\u2134': 'c', '\u254c': '--', '\u2c8f': 'h', '\u2ca3': 'p',\n                         '\u4e0b': 'under', '\u4e0d': 'Do not', '\u4eba': 'people', '\u4f0e': 'trick', '\u4f1a': 'meeting',\n                         '\u4f5c': 'Make', '\u4f60': 'you', '\u514b': 'Gram', '\u5173': 'turn off', '\u522b': 'do not',\n                         '\u52a0': 'plus', '\u534e': 'China', '\u5356': 'Sell', '\u53bb': 'go with', '\u54e5': 'brother',\n                         '\u56ed': 'garden', '\u56fd': 'country', '\u5706': 'circle', '\u571f': 'soil', '\u5730': 'Ground',\n                         '\u574f': 'Bad', '\u5916': 'outer', '\u5927': 'Big', '\u5931': 'Lost', '\u5b50': 'child', '\u5c0f': 'small',\n                         '\u6210': 'to make', '\u6226': 'War', '\u6240': 'Place', '\u62ff': 'take', '\u6545': 'Therefore',\n                         '\u6587': 'Text', '\u660e': 'Bright', '\u662f': 'Yes', '\u6709': 'Have', '\u6b4c': 'song', \n                         '\u6b8a': 'special', '\u6cb9': 'oil', '\u6e29': 'temperature', '\u7279': 'special', \n                         '\u7344': 'prison', '\u7684': 'of', '\u7a0e': 'tax', '\u7cfb': 'system', '\u7fa4': 'group',\n                         '\u821e': 'dance', '\u82f1': 'English', '\u8521': 'Cai', '\u8bae': 'Discussion', '\u8c37': 'Valley',\n                         '\u8c46': 'beans', '\u90fd': 'All', '\u94b1': 'money', '\u964d': 'drop', '\u969c': 'barrier',\n                         '\u9a97': 'cheat', '\uc138': 'three', '\uc548': 'within', '\uc601': 'spirit', '\uc694': 'Yo',\n                          '\u037a': '', '\u039b': 'L', '\u039e': 'X', '\u03ac': 'a', '\u03ae': 'or', '\u03b9': 'j',\n                         '\u03be': 'X', '\u03c2': 's', '\u03c8': 't', '\u03cc': 'The', '\u03cd': 'gt;', '\u03ce': 'o',\n                         '\u03d6': 'e.g.', '\u0413': 'R', '\u0414': 'D', '\u0416': 'F', '\u041b': 'L', '\u041f': 'P', \n                         '\u0424': 'F', '\u0428': 'Sh', '\u0431': 'b', '\u043f': 'P', '\u0444': 'f', '\u0446': 'c', \n                         '\u0447': 'no', '\u0448': 'sh', '\u0449': 'u', '\u044d': 'uh', '\u044e': 'Yu', '\u0457': 'her',\n                         '\u045b': 'ht', '\u0541': 'Winter', '\u0561': 'a', '\u0564': 'd', '\u0565': 'e', '\u056b': 's',\n                         '\u0571': 'h', '\u0574': 'm', '\u0575': 'y', '\u0576': 'h', '\u057c': 'r', '\u057d': 'c', \n                         '\u0580': 'p', '\u0582': '\u00b3', '\u05d1': 'B', '\u05d3': 'D', '\u05d4': 'God', '\u05d5': 'and',\n                         '\u05d8': 'ninth', '\u05d9': 'J', '\u05da': 'D', '\u05db': 'about', '\u05dc': 'To', '\u05dd': 'From', \n                         '\u05de': 'M', '\u05df': 'Estate', '\u05e0': 'N', '\u05e1': 'S.', '\u05e2': 'P', '\u05e3': 'Jeff',\n                         '\u05e4': 'F', '\u05e6': 'C', '\u05e7': 'K.', '\u05e8': 'R.', '\u05e9': 'That', '\u05ea': 'A',\n                         '\u0621': 'Was', '\u0622': 'Ah', '\u0623': 'a', '\u0625': 'a', '\u0627': 'a', '\u0629': 'e', \n                         '\u062a': 'T', '\u062c': 'C', '\u062d': 'H', '\u062e': 'Huh', '\u062f': 'of the', '\u0631': 'T',\n                         '\u0632': 'Z', '\u0633': 'Q', '\u0634': 'Sh', '\u0635': 's', '\u0637': 'I', '\u0639': 'AS', '\u063a': 'G',\n                         '\u0641': 'F', '\u0642': 'S', '\u0643': 'K', '\u0644': 'to', '\u0645': 'M', '\u0646': 'N', '\u0647': 'e', \n                         '\u0648': 'And', '\u0649': 'I', '\u064a': 'Y', '\u0686': 'What', '\u06a9': 'K', '\u06cc': 'Y', \n                         '\u0915': 'A', '\u092e': 'M', '\u0930': 'And', '\u0a97': 'C', '\u0a9c': 'The same', \n                         '\u0aa4': 'I', '\u0ab0': 'I', '\u0b9c': 'SAD', '\u10da': 'L', '\u1e51': 'o', '\u1f10': 'e',\n                         '\u1f14': '\u00cb', '\u1f21': 'or', '\u1f31': '\u0131', '\u1f34': 'i', '\u1f40': 'The', '\u1f41': 'The',\n                         '\u1f50': '\u00ff', '\u1f70': 'a', '\u1f72': '.', '\u1f78': 'The', '\u1f7b': 'gt;', '\u1fb6': 'a', \n                         '\u1fc6': 'or', '\u1fd6': '\u0e01', '\u1fe6': 'I', '\u3046': 'U', '\u3055': 'The', '\u3063': 'What',\n                         '\u3064': 'One', '\u306a': 'The', '\u3088': 'The', '\u3089': 'Et al', '\u30a8': 'The', \n                         '\u30af': 'The', '\u30b5': 'The', '\u30b7': 'The', '\u30b8': 'The', '\u30b9': 'The',\n                         '\u30c1': 'The', '\u30c4': 'The', '\u30cb': 'D', '\u30cf': 'Ha', '\u30de': 'Ma', \n                         '\u30ea': 'The', '\u30eb': 'Le', '\u30ec': 'Les', '\u30ed': 'The', '\u30f3': 'The',\n                         '\u4e00': 'One', '\u4e0e': 'versus', '\u4e14': 'And', '\u4e3a': 'for', '\u4e70': 'buy',\n                         '\u4e86': 'Up', '\u4e9b': 'some', '\u4ed6': 'he', '\u4ee5': 'Take', '\u4eec': 'They',\n                         '\u4ef6': 'Items', '\u4f20': 'pass', '\u4f26': 'Lun', '\u4f46': 'but', '\u4fe1': 'letter',\n                         '\u5019': 'Waiting', '\u507d': 'Pseudo', '\u5168': 'all', '\u516c': 'public', '\u5176': 'its',\n                         '\u517b': 'support', '\u51ac': 'winter', '\u51f8': 'Convex', '\u51fb': 'hit', '\u5224': 'Judge',\n                         '\u5230': 'To', '\u53cb': 'Friend', '\u53ef': 'can', '\u5417': 'What?', '\u548c': 'with',\n                         '\u552f': 'only', '\u56e0': 'because', '\u5723': 'Holy', '\u5728': 'in', '\u57fa': 'base',\n                         '\u5802': 'Hall', '\u58eb': 'Shishi', '\u590d': 'complex', '\u591a': 'many', '\u5929': 'day',\n                         '\u597d': 'it is good', '\u5982': 'Such as', '\u5a5a': 'marriage', '\u5b69': 'child', \n                         '\u5ba0': 'Pet', '\u5bd3': 'Apartment', '\u5bf9': 'Correct', '\u5c41': 'fart', \n                         '\u5c48': 'Qu', '\u5de8': 'huge', '\u5df1': 'already', '\u5f0f': 'formula', '\u5f53': 'when',\n                         '\u5f7c': 'he', '\u5f92': 'only', '\u5f97': 'Got', '\u6012': 'angry', '\u602a': 'strange',\n                         '\u6050': 'fear', '\u60e7': 'fear', '\u60f3': 'miss you', '\u6124': 'anger', '\u6211': 'I',\n                         '\u6218': 'war', '\u6279': 'Batch', '\u628a': 'Put', '\u62c9': 'Pull', '\u62f7': 'Copy', \n                         '\u63a5': 'Connect', '\u64cd': 'Fuck', '\u6536': 'Receive', '\u653f': 'Politics', \n                         '\u6559': 'teach', '\u65a4': 'jin', '\u65af': 'S', '\u65b0': 'new', '\u65f6': 'Time', \n                         '\u666e': 'general', '\u66fe': 'Once', '\u672c': 'this', '\u6740': 'kill', '\u6781': 'pole',\n                         '\u67e5': 'check', '\u6817': 'chestnut', '\u682a': 'stock', '\u6837': 'kind', '\u68c0': 'Check',\n                         '\u6b22': 'Happy', '\u6b7b': 'dead', '\u6c49': 'Chinese', '\u6ca1': 'No', '\u6cbb': 'rule', \n                         '\u6cd5': 'law', '\u6d3b': 'live', '\u70b9': 'point', '\u71fb': 'Moth', '\u7269': 'object',\n                         '\u731c': 'guess', '\u7334': 'monkey', '\u7406': 'Rational', '\u751f': 'Health', '\u7528': 'use',\n                         '\u767d': 'White', '\u767e': 'hundred', '\u76f4': 'straight', '\u76f8': 'phase', '\u770b': 'Look',\n                         '\u7763': 'Supervisor', '\u77e5': 'know', '\u793e': 'Society', '\u795d': 'wish', '\u79ef': 'product',\n                         '\u7a23': 'Jesus', '\u7ecf': 'through', '\u7ed3': 'Knot', '\u7ed9': 'give', '\u7f8e': 'nice', \n                         '\u8036': 'Yay', '\u804a': 'chat', '\u80dc': 'Win', '\u81f3': 'to', '\u865a': 'Virtual', '\u88fd': 'Made', \n                         '\u8981': 'Want', '\u8ba4': 'recognize', '\u8ba8': 'discuss', '\u8ba9': 'Let', '\u8bc6': 'knowledge',\n                         '\u8bdd': 'words', '\u8bed': 'language', '\u8bf4': 'Say', '\u8c0a': 'friendship', \n                         '\u8c13': 'Predicate', '\u8c61': 'Elephant', '\u8d3a': 'He', '\u8d62': 'win', '\u8fce': 'welcome',\n                         '\u8fd8': 'also', '\u8fd9': 'This', '\u901a': 'through', '\u9244': 'iron', '\u95ee': 'ask', \n                         '\u963f': 'A', '\u9898': 'question', '\u989d': 'amount', '\u9b3c': 'ghost', '\u9e21': 'Chicken',\n                         '\uac00': 'end', '\uac08': 'Go', '\uac8c': 'to', '\uaca9': 'case', '\uacbd': 'circa', '\uad00': 'tube',\n                         '\uad6d': 'soup', '\uae08': 'gold', '\ub098': 'I', '\ub294': 'The', '\ub2c8': 'Nee', '\ub2e4': 'All',\n                         '\ub300': 'versus', '\ub3c4': 'Degree', '\ub41c': 'The', '\ub4dc': 'De', '\ub4e4': 'field', \n                         '\ub54c': 'time', '\ub7f0': 'Run', '\ub835': 'Hi', '\ub85d': 'rock', '\ub93c': 'Crown', \n                         '\ub9ac': 'Lee', '\ub9c8': 'hemp', '\ub9cc': 'just', '\ubc18': 'half', '\ubd84': 'minute', \n                         '\uc0ac': 'four', '\uc0c1': 'Prize', '\uc11c': 'book', '\uc11d': 'three', '\uc131': 'castle',\n                         '\uc2a4': 'The', '\uc2dc': 'city', '\uc54a': 'Not', '\uc57c': 'Hey', '\uc57d': 'about', \n                         '\uc5b4': 'uh', '\uc640': 'Wow', '\uc6a9': 'for', '\uc720': 'U', '\uc744': 'of', '\uc774': 'this',\n                         '\uc778': 'sign', '\uc798': 'well', '\uc81c': 'My', '\uc950': 'rat', '\uc9c0': 'G', '\ucd08': 'second',\n                         '\uce90': 'Can', '\ud0f1': 'Tang', '\ud2b8': 'The', '\ud2f0': 'tea', '\ud328': 'tile', '\ud488': 'Width', \n                         '\ud55c': 'One', '\ud569': 'synthesis', '\ud574': 'year', '\ud5c8': 'Huh', '\ud654': 'anger', '\ud669': 'sulfur',\n                         '\ud558': 'Ha', '\ufb01': 'be', '\uff10': '#', '\uff12': '#', '\uff18': '#', '\uff25': 'e', '\uff27': 'g',\n                         '\uff28': 'h', '\uff2d': 'm', '\uff2e': 'n', '\uff2f': 'O', '\uff33': 's', '\uff35': 'U', '\uff37': 'w',\n                         '\uff41': 'a', '\uff42': 'b', '\uff43': 'c', '\uff44': 'd', '\uff45': 'e', '\uff46': 'f', '\uff47': 'g',\n                         '\uff48': 'h', '\uff49': 'i', '\uff4b': 'k', '\uff4c': 'l', '\uff4d': 'm', '\uff4e': 'n', '\uff4f': 'o',\n                         '\uff52': 'r', '\uff53': 's', '\uff54': 't', '\uff55': 'u', '\uff56': 'v', '\uff57': 'w', '\uff59': 'y',\n                         '\ud835\udc00': 'a', '\ud835\udc02': 'c', '\ud835\udc03': 'd', '\ud835\udc05': 'f', '\ud835\udc07': 'h', '\ud835\udc0a': 'k', '\ud835\udc0d': 'n', \n                         '\ud835\udc0e': 'o', '\ud835\udc11': 'r', '\ud835\udc13': 't', '\ud835\udc14': 'u', '\ud835\udc18': 'y', '\ud835\udc19': 'z', '\ud835\udc1a': 'a',\n                         '\ud835\udc1b': 'b', '\ud835\udc1c': 'c', '\ud835\udc1d': 'd', '\ud835\udc1e': 'e', '\ud835\udc1f': 'f', '\ud835\udc20': 'g', '\ud835\udc21': 'h', \n                         '\ud835\udc22': 'i', '\ud835\udc23': 'j', '\ud835\udc25': 'i', '\ud835\udc26': 'm', '\ud835\udc27': 'n', '\ud835\udc28': 'o', '\ud835\udc29': 'p',\n                         '\ud835\udc2a': 'q', '\ud835\udc2b': 'r', '\ud835\udc2c': 's', '\ud835\udc2d': 't', '\ud835\udc2e': 'u', '\ud835\udc2f': 'v', '\ud835\udc30': 'w',\n                         '\ud835\udc31': 'x', '\ud835\udc32': 'y', '\ud835\udc33': 'z', '\ud835\udc65': 'x', '\ud835\udc66': 'y', '\ud835\udc67': 'z', '\ud835\udc69': 'b',\n                         '\ud835\udc6a': 'c', '\ud835\udc6b': 'd', '\ud835\udc6c': 'e', '\ud835\udc6d': 'f', '\ud835\udc6e': 'g', '\ud835\udc6f': 'h', '\ud835\udc70': 'i',\n                         '\ud835\udc71': 'j', '\ud835\udc72': 'k', '\ud835\udc73': 'l', '\ud835\udc74': 'm', '\ud835\udc75': 'n', '\ud835\udc76': '0', '\ud835\udc77': 'p',\n                         '\ud835\udc79': 'r', '\ud835\udc7a': 's', '\ud835\udc7b': 't', '\ud835\udc7e': 'w', '\ud835\udc80': 'y', '\ud835\udc81': 'z', '\ud835\udc82': 'a',\n                         '\ud835\udc83': 'b', '\ud835\udc84': 'c', '\ud835\udc85': 'd', '\ud835\udc86': 'e', '\ud835\udc87': 'f', '\ud835\udc88': 'g', '\ud835\udc89': 'h',\n                         '\ud835\udc8a': 'i', '\ud835\udc8b': 'j', '\ud835\udc8c': 'k', '\ud835\udc8d': 'l', '\ud835\udc8e': 'm', '\ud835\udc8f': 'n', '\ud835\udc90': 'o', \n                         '\ud835\udc91': 'p', '\ud835\udc92': 'q', '\ud835\udc93': 'r', '\ud835\udc94': 's', '\ud835\udc95': 't', '\ud835\udc96': 'u', '\ud835\udc97': 'v', \n                         '\ud835\udc98': 'w', '\ud835\udc99': 'x', '\ud835\udc9a': 'y', '\ud835\udc9b': 'z', '\ud835\udca9': 'n', '\ud835\udcb6': 'a', '\ud835\udcb8': 'c',\n                         '\ud835\udcbd': 'h', '\ud835\udcbe': 'i', '\ud835\udcc0': 'k', '\ud835\udcc1': 'l', '\ud835\udcc3': 'n', '\ud835\udcc5': 'p', '\ud835\udcc7': 'r',\n                         '\ud835\udcc8': 's', '\ud835\udcc9': 't', '\ud835\udcca': 'u', '\ud835\udccc': 'w', '\ud835\udcce': 'y', '\ud835\udcd2': 'c', '\ud835\udcec': 'c',\n                         '\ud835\udcee': 'e', '\ud835\udcf2': 'i', '\ud835\udcf4': 'k', '\ud835\udcf5': 'l', '\ud835\udcfb': 'r', '\ud835\udcfc': 's', '\ud835\udcfd': 't',\n                         '\ud835\udcff': 'v', '\ud835\udd74': 'j', '\ud835\udd78': 'm', '\ud835\udd7f': 'i', '\ud835\udd82': 'm', '\ud835\udd86': 'a', '\ud835\udd87': 'b',\n                         '\ud835\udd88': 'c', '\ud835\udd89': 'd', '\ud835\udd8a': 'e', '\ud835\udd8b': 'f', '\ud835\udd8c': 'g', '\ud835\udd8d': 'h', '\ud835\udd8e': 'i', \n                         '\ud835\udd92': 'm', '\ud835\udd93': 'n', '\ud835\udd95': 'p', '\ud835\udd97': 'r', '\ud835\udd98': 's', '\ud835\udd99': 't', '\ud835\udd9a': 'u',\n                         '\ud835\udd9b': 'v', '\ud835\udd9c': 'w', '\ud835\udd9e': 'n', '\ud835\udd9f': 'z', '\ud835\uddd5': 'b', '\ud835\uddd8': 'e', '\ud835\uddd9': 'f',\n                         '\ud835\uddde': 'k', '\ud835\udddf': 'l', '\ud835\udde0': 'm', '\ud835\udde2': 'o', '\ud835\udde4': 'q', '\ud835\udde6': 's', '\ud835\udde7': 't',\n                         '\ud835\uddea': 'w', '\ud835\udded': 'z', '\ud835\uddee': 'a', '\ud835\uddef': 'b', '\ud835\uddf0': 'c', '\ud835\uddf1': 'd', '\ud835\uddf2': 'e',\n                         '\ud835\uddf3': 'f', '\ud835\uddf4': 'g', '\ud835\uddf5': 'h', '\ud835\uddf6': 'i', '\ud835\uddf7': 'j', '\ud835\uddf8': 'k', '\ud835\uddf9': 'i',\n                         '\ud835\uddfa': 'm', '\ud835\uddfb': 'n', '\ud835\uddfc': 'o', '\ud835\uddfd': 'p', '\ud835\uddff': 'r', '\ud835\ude00': 's', '\ud835\ude01': 't',\n                         '\ud835\ude02': 'u', '\ud835\ude03': 'v', '\ud835\ude04': 'w', '\ud835\ude05': 'x', '\ud835\ude06': 'y', '\ud835\ude07': 'z', '\ud835\ude10': 'l',\n                         '\ud835\ude13': 'l', '\ud835\ude16': 'o', '\ud835\ude22': 'a', '\ud835\ude23': 'b', '\ud835\ude24': 'c', '\ud835\ude25': 'd', '\ud835\ude26': 'e',\n                         '\ud835\ude27': 'f', '\ud835\ude28': 'g', '\ud835\ude29': 'h', '\ud835\ude2a': 'i', '\ud835\ude2b': 'j', '\ud835\ude2c': 'k', '\ud835\ude2e': 'm',\n                         '\ud835\ude2f': 'n', '\ud835\ude30': 'o', '\ud835\ude31': 'p', '\ud835\ude32': 'q', '\ud835\ude33': 'r', '\ud835\ude34': 's', '\ud835\ude35': 't',\n                         '\ud835\ude36': 'u', '\ud835\ude37': 'v', '\ud835\ude38': 'w', '\ud835\ude39': 'x', '\ud835\ude3a': 'y', '\ud835\ude3c': 'a', '\ud835\ude3d': 'b',\n                         '\ud835\ude3e': 'c', '\ud835\ude3f': 'd', '\ud835\ude40': 'e', '\ud835\ude43': 'h', '\ud835\ude45': 'j', '\ud835\ude46': 'k', '\ud835\ude47': 'l', \n                         '\ud835\ude48': 'm', '\ud835\ude4a': 'o', '\ud835\ude4b': 'p', '\ud835\ude4d': 'r', '\ud835\ude4f': 't', '\ud835\ude52': 'w', '\ud835\ude54': 'y',\n                         '\ud835\ude56': 'a', '\ud835\ude57': 'b', '\ud835\ude58': 'c', '\ud835\ude59': 'd', '\ud835\ude5a': 'e', '\ud835\ude5b': 'f', '\ud835\ude5c': 'g',\n                         '\ud835\ude5d': 'h', '\ud835\ude5e': 'i', '\ud835\ude5f': 'j', '\ud835\ude60': 'k', '\ud835\ude62': 'm', '\ud835\ude63': 'n', '\ud835\ude64': 'o',\n                         '\ud835\ude65': 'p', '\ud835\ude67': 'r', '\ud835\ude68': 's', '\ud835\ude69': 't', '\ud835\ude6a': 'u', '\ud835\ude6b': 'v', '\ud835\ude6c': 'w',\n                         '\ud835\ude6d': 'x', '\ud835\ude6e': 'y', '\ud835\udfce': '0', '\ud835\udfcf': '1', '\ud835\udfd0': '2', '\ud835\udfd3': '5', '\ud835\udfd4': '6',\n                         '\ud835\udfd6': '8', '\ud835\udfec': '0', '\ud835\udfed': '1', '\ud835\udfee': '2', '\ud835\udfef': '3', '\ud835\udff0': '4', '\ud835\udff1': '5',\n                         '\ud835\udff2': '6', '\ud835\udff3': '7', '\ud835\udfd1':'3', '\ud835\udfd2':'4', '\ud835\udfd5':'7', '\ud835\udfd7':'9',\n                         '\ud83c\udde6': 'a', '\ud83c\udde9': 'd', '\ud83c\uddea': 'e', '\ud83c\uddec': 'g', '\ud83c\uddee': 'i', \n                         '\ud83c\uddf3': 'n', '\ud83c\uddf4': 'o', '\ud83c\uddf7': 'r', '\ud83c\uddf9': 't', '\ud83c\uddfc': 'w', '\ud83d\udd92': 'thumps up',\n                         '\u210f':'h', '\u02b2':'j', '\uff23':'c', '\u013a':'i', '\uff2a':'j', '\u0138':'k', '\uff30':'p'}\n\n\n\n\n\n\n# List was cerated in separate notebook investigating on word embedding. \n# These dictionary is used to remove unwanted characters from the text\npuncts =                 ['_','!', '?','\\x08', '\\n', '\\x0b', '\\r', '\\x10', '\\x13', '\\x1f', ' ', ' # ', '\"', '#', \n                         '# ', '$', '%', '&',  '(', ')', '*', '+', ',',  '\/', '.', ':', ';', '<',\n                         '=', '>', '@', '[', '\\\\', ']', '^', '`', '{', '|', '}', '~', '\\x7f', '\\x80',\n                         '\\x81', '\\x85', '\\x91', '\\x92', '\\x95', '\\x96', '\\x9c', '\\x9d', '\\x9f', '\\xa0', \n                         '\u00a1', '\u00a2\u0f3c', '\u00a3', '\u00a4', '\u00a5', '\u00a7', '\u00a8', '\u00a9', '\u00ab', '\u00ac', '\\xad', '\u00af', '\u00b0', '\u00b1', '\u00b3',\n                         '\u00b6', '\u00b7', '\u00b8', '\u00ba', '\u00bb', '\u00bc', '\u00bd', '\u00be', '\u00bf', '\u00d7', '\u00d8', '\u00f7', '\u00f8', '\u0184', '\u01bd',\n                         '\u01d4', '\u023b', '\u025c', '\u0269', '\u0283', '\u028c', '\u02bb', '\u02bc', '\u02c8', '\u02cc', '\u02d0', '\u02d9', '\u02da', '\u0301', '\u0304', '\u0305', \n                         '\u0307', '\u0308', '\u0323', '\u0328', '\u032f', '\u0331', '\u0332', '\u0336', '\u035c', '\u035d', '\u035e', '\u035f', '\u0361', '\u0366', '\u061f', '\u064e', '\u0650', '\u06a1', \n                         '\u06de', '\u06e9', '\u0701', '\u093e', '\u094d', '\u0abe', '\u0ac0', '\u0ac1', '\u0e4f', '\u0e4f\u032f\u0361', '\u0f3c', '\u0f3d', '\u1403', '\u1423', '\u1426', '\u1427',\n                         '\u144e', '\u146d', '\u146f', '\u14a7', '\u14c0', '\u14c2', '\u14c3', '\u14c7', '\u152d', '\u1d26', '\u1d28', '\u1d7b', '\u1f38', '\u1f39', '\u1f7c', \n                         '\u1fbd', '\u1fc3', '\\u2000', '\\u2001', '\\u2002', '\\u2003', '\\u2004', '\\u2005', '\\u2006', \n                         '\\u2007', '\\u2008', '\\u2009', '\\u200a', '\\u200b', '\\u200c', '\\u200d', '\\u200e',\n                         '\\u200f', '\u2010', '\u2011', '\u2012', '\u2013', '\u2014', '\u2015', '\u2016', '\u2018', '\u2019', '\u201a', '\u201b', '\u201c', '\u201d', '\u201e',\n                         '\u2020', '\u2021', '\u2022', '\u2023', '\u2026', '\\u2028', '\\u202a', '\\u202c', '\\u202d', '\\u202f', '\u2030',\n                         '\u2032', '\u2033', '\u2039', '\u203a', '\u203f', '\u2044', '\u204d\u0334\u031b\\u3000', '\u204e', '\u2074', '\u2082', '\u20ac', '\u20b5', '\u20bd', '\u2103', '\u2105',\n                         '\u2110', '\u2122', '\u212e', '\u2153', '\u2190', '\u2191', '\u2192', '\u2193', '\u21b3', '\u21b4', '\u21ba', '\u21cc', '\u21d2', '\u21e4', '\u2206', '\u220e',\n                         '\u220f', '\u2212', '\u2215', '\u2219', '\u221a', '\u221e', '\u2229', '\u2234', '\u2235', '\u223c', '\u2248', '\u2260', '\u2264', '\u2265', '\u2282', '\u2295',\n                         '\u2298', '\u22c5', '\u22c6', '\u2320', '\u238c', '\u23d6', '\u2500', '\u2501', '\u2503', '\u2508', '\u250a', '\u2517', '\u2523', '\u252b', '\u2533', '\u254c', '\u2550',\n                         '\u2551', '\u2554', '\u2557', '\u255a', '\u2563', '\u2566', '\u2569', '\u256a', '\u256d', '\u256d\u256e', '\u256e', '\u256f', '\u2570', '\u2571', '\u2572', '\u2580',\n                         '\u2582', '\u2583', '\u2584', '\u2585', '\u2586', '\u2587', '\u2588', '\u258a', '\u258b', '\u258f', '\u2591', '\u2592', '\u2593', '\u2594', '\u2595', \n                         '\u2599', '\u25a0', '\u25aa', '\u25ac', '\u25b0', '\u25b1', '\u25b2', '\u25b7', '\u25b8', '\u25ba', '\u25bc', '\u25be', '\u25c4', '\u25c7', '\u25cb',\n                         '\u25cf', '\u25d0', '\u25d4', '\u25d5', '\u25dd', '\u25de', '\u25e1', '\u25e6', '\u2605', '\u2606', '\u260f', '\u2610', '\u2612', '\u2619', '\u261b',\n                         '\u261c', '\u261e', '\u262d', '\u263b', '\u263c', '\u2666', '\u2669', '\u266a', '\u266b', '\u266c', '\u266d', '\u2672', '\u2686', '\u26ad', '\u26b2', '\u2700',\n                         '\u2713', '\u2718', '\u271e', '\u2727', '\u272c', '\u272d', '\u2730', '\u273e', '\u2746', '\u2767', '\u27a4', '\u27a5', '\u2800', '\u290f', '\u2981',\n                         '\u2a5b', '\u2b2d', '\u2b2f', '\\u3000', '\u3001', '\u3002', '\u300a', '\u300b', '\u300c', '\u300d', '\u3014', '\u30fb', '\u3138', '\u3153',\n                         '\u951f', '\ua725', '\\ue014', '\\ue600', '\\ue602', '\\ue607', '\\ue608', '\\ue613', '\\ue807',\n                         '\\uf005', '\\uf020', '\\uf04a', '\\uf04c', '\\uf070',  '\\uf202\\uf099', '\\uf203',\n                         '\\uf071\\uf03d\\uf031\\uf02f\\uf032\\uf028\\uf070\\uf02f\\uf032\\uf02d\\uf061\\uf029',\n                         '\\uf099', '\\uf09a', '\\uf0a7', '\\uf0b7', '\\uf0e0', '\\uf10a', '\\uf202', \n                         '\\uf203\\uf09a', '\\uf222', '\\uf222\\ue608', '\\uf410', '\\uf410\\ue600', '\\uf469', \n                         '\\uf469\\ue607', '\\uf818', '\ufd3e', '\ufd3e\u0361', '\ufd3f', '\ufdfb', '\\ufeff', '\uff01', '\uff05', '\uff07',\n                         '\uff08', '\uff09', '\uff0c', '\uff0d', '\uff0e', '\uff0f', '\uff1a', '\uff1e', '\uff1f', '\uff3c', '\uff5c', '\uffe6', '\ufffc', '\ufffd',\n                         '\ud835\udcbb', '\ud835\udd7e', '\ud835\udd84', '\ud835\udd90', '\ud835\udd91', '\ud835\udd94', '\ud835\udddc', '\ud835\ude0a', '\ud835\ude2d', '\ud835\ude44', '\ud835\ude61', '\ud835\udf48', '\ud83d\udd91', '\ud83d\udd92']\n\ndef clean_numbers(x):\n  \n  \"\"\"\n  The following function is used to format the numbers.\n  In the beginning \"th, st, nd, rd\" are removed\n  \"\"\"\n  \n  #remove \"th\" after a number\n  matches = re.findall(r'\\b\\d+\\s*th\\b', x)\n  if len(matches) != 0:\n    x = re.sub(r'\\s*th\\b', \" \", x)\n    \n  #remove \"rd\" after a number \n  matches = re.findall(r'\\b\\d+\\s*rd\\b', x)\n  if len(matches) != 0:\n    x = re.sub(r'\\s*rd\\b', \" \", x)\n  \n  #remove \"st\" after a number\n  matches = re.findall(r'\\b\\d+\\s*st\\b', x)\n  if len(matches) != 0:\n    x = re.sub(r'\\s*st\\b', \" \", x)\n    \n  #remove \"nd\" after a number\n  matches = re.findall(r'\\b\\d+\\s*nd\\b', x)\n  if len(matches) != 0:\n    x = re.sub(r'\\s*nd\\b', \" \", x)\n  \n  # replace standalone numbers higher than 10 by #\n  # this function does not touch numbers linked to words like \"G-20\"\n  if bool(re.search(r'\\d+', x)):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    #we do include the range from 1 to 10 as all word-vectors include them\n    #x = re.sub('[0-9]{1}', '#', x)\n    \n  return x\n\ndef year_and_hour(text):\n  \"\"\"\n  This function is used to replace \"yr,yrs\" by year and \"hr,hrs\" by hour.\n  \"\"\"\n  \n  # Find matches for \"yr\", \"yrs\", \"hr\", \"hrs\"\n  matches_year = re.findall(r'\\b\\d+\\s*yr\\b', text)\n  matches_years = re.findall(r'\\b\\d+\\s*yrs\\b', text)\n  matches_hour = re.findall(r'\\b\\d+\\s*hr\\b', text)\n  matches_hours = re.findall(r'\\b\\d+\\s*hrs\\b', text)\n  \n  # replace all matches accordingly\n  if len(matches_year) != 0:\n    text = re.sub(r'\\b\\d+\\s*yr\\b', \"year\", text)\n  if len(matches_years) != 0:\n    text = re.sub(r'\\b\\d+\\s*yrs\\b', \"year\", text)\n  if len(matches_hour) != 0:\n    text = re.sub(r'\\b\\d+\\s*hr\\b', \"hour\", text)\n  if len(matches_hours) != 0:\n    text = re.sub(r'\\b\\d+\\s*hrs\\b', \"hour\", text)\n  return text\n\ndef textBlobLemmatize(sentence):\n  \"\"\"\n  This function uses the Word lemmatizer function of the textBlob package.\n  \"\"\"  \n  #for each word in the text, replace the word by its lemmatized version\n  for x in sentence.split():\n    sentence = sentence.replace(x, Word(x).lemmatize())\n  return sentence\n\ndef build_vocab(df):\n  \n  '''Build a dictionary of words and its number of occurences from the data frame'''\n  \n  #initialize the tokenizer\n  tokenizer = TweetTokenizer()\n  \n  vocab = {}\n  for i, row in enumerate(df):\n      #tokenize the sentence \n      words = tokenizer.tokenize(row)\n      #for each word, check if it is in the dict otherwise add a new entry\n      for w in words:\n       \n        try:\n            vocab[w] += 1\n        except KeyError:\n            vocab[w] = 1\n  \n  return vocab\n\n#https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-for-glove-part1-eda\ndef check_coverage(vocab,embeddings_index, print_oov_num=100):\n  '''\n  This function checks what part of the vocabluary and the text is covered by the embedding index.\n  It returns a list of tuples of unknown words and its occuring frequency.\n  '''\n  \n  a = {}\n  oov = {}\n  k = 0\n  i = 0\n\n  # for every word in vocab\n  for word in vocab:\n      # check if it can be found in the embedding\n      try:\n          # store the embedding index to a\n          a[word] = embeddings_index[word]\n          # count up by #of occurences in df\n          k += vocab[word]\n      except:\n          # if no embedding for word, add to oov\n          oov[word] = vocab[word]\n          # # count up by #of occurences in df\n          i += vocab[word]\n          pass\n  # calc percentage of #of found words by length of vocab\n  print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n  # devide number of found words by number of all words from df\n  print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n\n  # return unknown words sorted by number of occurences\n  sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n  print('Top unknown words are:', sorted_x[:print_oov_num])\n\n  #return dict of unknown words + occurences\n  return oov\n\ndef  load_embedding_vocab(path):\n  '''\n  Load the embeddings in the right format and return the vocab dictionary. \n  '''  \n  # Print starting info about the pre-processing\n  starttime = datetime.datetime.now().replace(microsecond=0)\n  print(\"Starttime: \", starttime)\n\n  def timediff(time):\n    return time - starttime\n  \n  EMBEDDING_FILE = path\n  def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n  embeddings_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(EMBEDDING_FILE)) \n    \n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Embedding model loaded and vocab returned. Time since start: \", timediff(time))\n  \n  #return the vocab\n  return embeddings_index\n\ndef preprocessing_NN(df, model_vocab, calc_coverage=True, print_oov_num=100):\n  \"\"\"\n  This function is only correcting words which are not out of the box known towards the embedding dictionary.\n  It is optimized using the nltk TweetTokenizer.\n\n  Function that combines the whole pre-processing process specifically for neural networks where less pre-processing is required compared to conventional methods.\n  This means we will not remove stopwords, lemmatize or remove typical punctuation.\n  \"\"\"\n  \n  # Set parameters\n  tokenizer = TweetTokenizer()\n  \n  # Print starting info about the pre-processing\n  starttime = datetime.datetime.now().replace(microsecond=0)\n  print('Dataset Length: ', len(df), \"Starttime: \", starttime)\n\n  def timediff(time):\n    return time - starttime\n  \n  # build a vocabulary from the text \n  vocab = build_vocab(df.comment_text)\n  print('Embedding vectors are loaded. \\n')\n  # check the coverage and receive a dictionary of unknown words\n  unknown = check_coverage(vocab,model_vocab, print_oov_num=print_oov_num)\n  # extract the list of unknown words\n  unknown = unknown.keys()\n  \n  ## Process the unknown words\n  # The replace_contractions function is applied on the data frame\n  corrected = [replace_contractions(x) for x in unknown]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Contractions have been replaced. Time since start: \", timediff(time))\n\n  # Replace emojis with text\n  corrected = [emoji.demojize(x) for x in corrected]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Emojis have been converted to text. Time since start: \", timediff(time))\n\n  # Replace keyboard smilies with text\n  corrected = [replace_smilies(x) for x in corrected]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Smilies have been converted to text. Time since start: \", timediff(time))\n\n  # The clean_text function is applied on the data frame\n  corrected = [clean_text(x) for x in corrected]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"All signs have been removed. Time since start: \", timediff(time))\n  \n  # The clean_numbers function is applied\n  corrected = [clean_numbers(x) for x in corrected]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"All numbers have been replaced with ###. Time since start: \", timediff(time))\n  \n    # Replace or remove special characters like - \/ _ according to rules\n  corrected = [replace_symbol_special(x, check_vocab=True, vocab=model_vocab) for x in corrected]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Special symbols have been processed. Time since start: \", timediff(time))\n\n  # Abbreviations are replaced by year and hour\n  corrected = [year_and_hour(x) for x in corrected]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Yr and hr have been replaced by year and hour. Time since start: \", timediff(time))\n  \n  # *Takes too long\n  #Correct spelling mistakes\n  #corrected = [TextBlob(x).correct() for x in corrected]\n  #time = datetime.datetime.now().replace(microsecond=0)\n  #print(\"Yr and hr have been replaced by year and hour. Time since start: \", timediff(time))\n  \n  #create a dictionary from word and correction\n  dictionary = dict(zip(unknown, corrected))\n  keys = dictionary.keys()\n  \n  #remove all keys where unknown equals correction after processing\n  #create a new dict\n  dict_mispell = dict()\n  for key in dictionary.keys():\n    # if the correction differs from the unknown word add it to the new dict\n    if key != dictionary.get(key):\n      dict_mispell[key] = dictionary.get(key)\n  \n  time = datetime.datetime.now().replace(microsecond=0)\n  print('Correction dictionary of unknown words prepared. Time since start: ', timediff(time))\n  #print(dict_mispell, '\\n')\n  \n  def clean_mispell(text, dict_mispell):\n    '''Replaces the unknown words in the text by its corrections.'''\n    #tokenize the text with TweetTokenizer\n    words = tokenizer.tokenize(text)\n    for i, word in enumerate(words):\n      # if the word is among the misspellings\n      if word in dict_mispell.keys():\n        #replace it by the corrected word\n        words[i] = dict_mispell.get(word)\n    #merge text by space\n    text = ' '.join(words)\n    # remove all double spaces potentially appearing after pre-processing.\n    text  = re.sub(r' +', ' ', text)\n    return text\n      \n  \n  #tqdm.pandas()\n  df.comment_text = df.comment_text.apply(lambda x: clean_mispell(x, dict_mispell))\n  time = datetime.datetime.now().replace(microsecond=0)\n  print('Unknown words replaced excluding coverage check. Time since start: ', timediff(time))\n  \n  # print the final result\n  if calc_coverage == True: \n    vocab = build_vocab(df.comment_text)\n    unknown = check_coverage(vocab,model_vocab, print_oov_num=print_oov_num)\n    time = datetime.datetime.now().replace(microsecond=0)\n    print('Pre-processing done including coverage check. Time since start: ', timediff(time))\n  \n  return df","44fcf445":"# extract all characters from text which arenot in ascii from a list\ndef get_characters(_list):\n  character = set()\n  for element in _list:\n    for letter in element:\n      character.add(letter)\n  character = [c for c in character if c not in list(string.ascii_letters)]    \n  return character","50a65617":"# extract all characters from text which arenot in ascii from a df\ndef extract_used_characters(df):\n    \n    used_characters = set()\n    for i, row in enumerate(df):\n        characters = list(row)\n        for x in characters:\n            used_characters.add(x)\n    used_characters = [c for c in used_characters if c not in list(string.ascii_letters)]\n    return used_characters","b2ae2c3d":"# Check the coverage of the signs\ndef check_coverage_characters(df_charac, model_charac):\n  \n    i = 0\n    for c in df_charac:\n      if c in model_charac:\n        i += 1\n    \n    print('{:.2%} of the signs in the data frame are covered by the model'.format((i \/ len(df_charac))))","2629d1f6":"# Create general vocabulary for the train data\ngeneral_vocab = build_vocab(train_data.comment_text)","061d6f2d":"# Remove emojies from train_data\ntrain_data.comment_text = train_data.comment_text.apply(lambda x: emoji.demojize(x))","24b3a41c":"# get the characters from the df\ncharac_df = extract_used_characters(train_data.comment_text)","cefcd511":"# Load dataset\ntrain_data = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\n# kill all other columns except comment text and target\ncols_to_keep = ['comment_text','target']\ntrain_data = train_data.drop(train_data.columns.difference(cols_to_keep), axis=1)\ngc.collect()","5018282e":"# Load model\nmodel_google = KeyedVectors.load_word2vec_format(\"..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin\", binary=True)\nmodel_google_vocab = model_google.vocab","17296a1a":"# Check the out of the box coverage\ncoverage_google = check_coverage(general_vocab,model_google_vocab)","3f22466a":"# Extract all non ascii characters from the model\ncharac_google = get_characters(model_google_vocab.keys())","63dbeee3":"# Check character coverage\ncheck_coverage_characters(charac_df, charac_google)","ddfde898":"# Preprocess with google embedding\nstopWords = set(stopwords.words('english'))\ntrain_data.comment_text = train_data.comment_text.apply(lambda x: remove_stopwords(x, stopWords))\ntrain_preprocessed_google = preprocessing_NN(train_data, model_google.vocab)","fd97d467":"# Load dataset\ntrain_data = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\n# kill all other columns except comment text and target\ncols_to_keep = ['comment_text','target']\ntrain_data = train_data.drop(train_data.columns.difference(cols_to_keep), axis=1)\ngc.collect()","52c81fa9":"del train_preprocessed_google, model_google_vocab, charac_google, model_google\ngc.collect()","fce78045":"# Load model\nmodel_fasttext_vocab = load_embedding_vocab('..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec')","c8948ba4":"# Check the out of the box coverage\ncoverage_fasttext = check_coverage(general_vocab,model_fasttext_vocab)","bd7b1a72":"# Extract all non ascii characters from the model\ncharac_fasttext = get_characters(model_fasttext_vocab.keys())","064d1605":"# Check character coverage\ncheck_coverage_characters(charac_df, charac_fasttext)","8a8454c9":"# Preprocess with fasttext embedding\ntrain_preprocessed_fasttext = preprocessing_NN(train_data, model_fasttext_vocab)","a2da55ed":"# Load dataset\ntrain_data = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\n# kill all other columns except comment text and target\ncols_to_keep = ['comment_text','target']\ntrain_data = train_data.drop(train_data.columns.difference(cols_to_keep), axis=1)\ngc.collect()","52be1e53":"del train_preprocessed_fasttext, model_fasttext_vocab, charac_fasttext\ngc.collect()","5bb2807e":"# Load model vocab\nmodel_glove_vocab = load_embedding_vocab('..\/input\/glove840b300dtxt\/glove.840B.300d.txt')","4d72840f":"# Check the out of the box coverage\ncoverage_glove = check_coverage(general_vocab,model_glove_vocab)","01f9d156":"# Extract all non ascii characters from the model\ncharac_glove = get_characters(model_glove_vocab.keys())","9af48b72":"# Check character coverage\ncheck_coverage_characters(charac_df, charac_glove)","7d8c74c5":"# Preprocess with glove embedding\ntrain_preprocessed_glove = preprocessing_NN(train_data, model_glove_vocab)","6b5ad8ae":"del train_preprocessed_glove, model_glove_vocab, charac_glove\ngc.collect()","030ce5e3":"**2. Used Functions**\n\nThe check_coverage function, load_embedding_vocab and the build_vocab function were already covered in the preprocessing notebook and will not be covered in this notebook. The get_characters function returns all non ascii characters that are in a given list. The extract_used_characters gets all used characters from a given dataframe and the check_coverage_characters function return the percentage of signs covered by an entered character list of a word vector.","eaab50f1":"**3. Word Vectors**\n\nAs mentioned above we will look at three different word vectors. First of all we will check their out of the box performance without having applied any preprocessing.\n\nPrevious we will build a general vocabulary for all the comments of the train data. This vocabulary will be used for all three word vectors. Afterwards we convert the emojis of the train data in text to afterwards extract all non ascii signs from the dataframe.","fd8f04f4":"**4. Comparison & Conclusion**\n\nIn general all three word vectors require similar preprocessing. Lowercase should not be applied in any of them. Punctuations should not be removed as all include some punctuation and substantial information might get lost. Contractions should be cleaned as many are unknown and with the cleaning the text gets harmonized. Also translation and spellchecking should be executed for all of them to get any possible information and reduce the noise. Lemmatization should not be performed as all vectors can handle the different word versions. Unknown words get ignored by all models and therefore need to be handled separately. Regarding the replacement of emojis FastText differs from the other two as it includes a huge amount of emojis and therefore the replacement is not necessarily needed. For the other two the replacement should be performed. Google News differs from the other two vectors in the case of the removal of stopwords. It does not include any and therefore they need to be removed. Same behaviour regarding numbers for Google News.\n\nLooking at our obtained results from this notebook GloVe and FastText seem to be the best out-of-the-box choices as they already cover around 98% of the text and around 50% of the vocabulary. Google News performs rather poorly obtaining only 77% of text coverage and 35% on the vocabulary. This performance can be explained by the above mentioned aspect that it does not cover stopwords and so a huge amount of words are unknown.\n\nMeanwhile FastText and GloVe only get slight improvements after having preprocessed data, the effect on Google News is a lot bigger but it again this can be explained with the stopwords.\n\nFastText offers an impressing amount of non-ascii characters. It covers 65% of all non-ascii characters that appear in the dataframe of the competition. Meanwhile Google News and GloVe only cover between 32 - 35%.\n\nIn the end all three vectors get to a result around 99% of text coverage. Therefore all three are good candidates for this competition. Nevertheless probably FastText and GloVe are the better choices as Google requires some additional preprocessing which might result in information getting lost.","a90db986":"**FastText**","ed1009e0":"**Insights about different embedding vectors - GloVe, FastText & Google News**\n\nAfter our EDA: https:\/\/www.kaggle.com\/s7anmerk\/lean-import-to-save-ram-and-eda\n\nAnd our Super Fast Preprocessing: https:\/\/www.kaggle.com\/annekel\/fastpreprocess-14min-99-7-coverage-gl-ov\n\nIn this kernel we want to provide you some insights gained in our project regarding the different embedding vectors GloVe, FastText and Google News. As it's difficult to find information about the content of the different vectors our insights were generated during the application of those vectors improving our knowledge step by step.\n\nThis kernel includes the import of necessary files as well as the loading of needed packages. Some functions used in the process. Comparison of the three word vectors regarding their out-of-the-box performance, their non-ascii characters coverage  and then their perfomance having applied our preprocessing function (Click [here](https:\/\/www.kaggle.com\/annekel\/fastpreprocess-14min-99-7-coverage-gl-ov) for more information regarding our preprocessing).\n\n1. Load files and packages\n2. Used Functions\n3. Word Vectors\n   * Google News\n   * FastText\n   * GloVe\n4. Comparison & Conclusion\n\n","ef9d6eb7":"As we noted in the process of comparing the three word vectors,  the google word vector does not recognize numbers and stopwords. Therefore in the next step we will not only execute our preprocessing function but also remove the stopwords.","378ab69b":"**Google News**","b25126e5":"**GloVe**","77254ee8":"With the extracted signs we will check the percentage of signs covered by each model. Although this notebook will not include any details about the preprocessing, it will apply our preprocessing function for each word vector. Once preprocessed we will use the check_coverage function again to see the effect on the result.","f541b232":"**1. Load files and packages**"}}