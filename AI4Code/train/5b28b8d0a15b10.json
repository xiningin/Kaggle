{"cell_type":{"0f73d63d":"code","c292568c":"code","93269dcb":"code","528f80c5":"code","98914b4c":"code","6341973f":"code","cd0fe170":"code","6593d237":"code","f67659b4":"code","57048b4a":"code","a6b73c95":"code","b9bdeba2":"code","c4bdc5e3":"code","82dcb3ab":"code","f8638ff3":"code","8f7a857e":"code","faab52c7":"code","b5d27918":"code","65913dd8":"code","62e876c2":"code","2d7ceb87":"code","4da2e27a":"code","d8b4206c":"markdown","81f3bcf4":"markdown","5517c83f":"markdown","8effbc12":"markdown","9e7f23c5":"markdown","9372fb84":"markdown","c3d55539":"markdown","18291532":"markdown","7bd873e6":"markdown","9e5b896e":"markdown","76ddf156":"markdown","8cb82981":"markdown","4a3a2e23":"markdown","87c4b673":"markdown","f5ec86ad":"markdown","7174f7c0":"markdown","87c8d5fe":"markdown","a13110d5":"markdown","f27da267":"markdown","d8b99262":"markdown","9318b546":"markdown","bf58b9e5":"markdown","f6421264":"markdown"},"source":{"0f73d63d":"!pip install alibi","c292568c":"import tensorflow as tf\ntf.get_logger().setLevel(40) # suppress deprecation messages\ntf.compat.v1.disable_v2_behavior() # disable TF2 behaviour as alibi code still relies on TF1 constructs\nfrom tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, Input, UpSampling2D\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.utils import to_categorical\n\nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom time import time\nfrom alibi.explainers import CounterFactualProto\n\nprint('TF version: ', tf.__version__)\nprint('Eager execution enabled: ', tf.executing_eagerly()) # False","93269dcb":"(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nprint('x_train shape:', x_train.shape, 'y_train shape:', y_train.shape)","528f80c5":"x_train = x_train.astype('float32') \/ 255\nx_test = x_test.astype('float32') \/ 255\nx_train = np.reshape(x_train, x_train.shape + (1,))\nx_test = np.reshape(x_test, x_test.shape + (1,))\nprint('x_train shape:', x_train.shape, 'x_test shape:', x_test.shape)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\nprint('y_train shape:', y_train.shape, 'y_test shape:', y_test.shape)","98914b4c":"xmin, xmax = -.5, .5\nx_train = ((x_train - x_train.min()) \/ (x_train.max() - x_train.min())) * (xmax - xmin) + xmin\nx_test = ((x_test - x_test.min()) \/ (x_test.max() - x_test.min())) * (xmax - xmin) + xmin","6341973f":"def cnn_model():\n    x_in = Input(shape=(28, 28, 1))\n    x = Conv2D(filters=32, kernel_size=2, padding='same', activation='relu')(x_in)\n    x = MaxPooling2D(pool_size=2)(x)\n    x = Dropout(0.3)(x)\n\n    x = Conv2D(filters=64, kernel_size=2, padding='same', activation='relu')(x)\n    x = MaxPooling2D(pool_size=2)(x)\n    x = Dropout(0.3)(x)\n\n    x = Flatten()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x_out = Dense(10, activation='softmax')(x)\n\n    cnn = Model(inputs=x_in, outputs=x_out)\n    cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return cnn","cd0fe170":"cnn = cnn_model()\ncnn.fit(x_train, y_train, batch_size=32, epochs=3, verbose=0)\ncnn.save('fashion_mnist_cnn.h5', save_format='h5')","6593d237":"cnn = load_model('fashion_mnist_cnn.h5')\nscore = cnn.evaluate(x_test, y_test, verbose=0)\nprint('Test accuracy: ', score[1])","f67659b4":"def ae_model():\n    # encoder\n    x_in = Input(shape=(28, 28, 1))\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x_in)\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    encoded = Conv2D(1, (3, 3), activation=None, padding='same')(x)\n    encoder = Model(x_in, encoded)\n\n    # decoder\n    dec_in = Input(shape=(14, 14, 1))\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(dec_in)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n    decoded = Conv2D(1, (3, 3), activation=None, padding='same')(x)\n    decoder = Model(dec_in, decoded)\n\n    # autoencoder = encoder + decoder\n    x_out = decoder(encoder(x_in))\n    autoencoder = Model(x_in, x_out)\n    autoencoder.compile(optimizer='adam', loss='mse')\n\n    return autoencoder, encoder, decoder","57048b4a":"ae, enc, dec = ae_model()\nae.fit(x_train, x_train, batch_size=128, epochs=4, validation_data=(x_test, x_test), verbose=0)\nae.save('fashion_mnist_ae.h5', save_format='h5')\nenc.save('fashion_mnist_enc.h5', save_format='h5')","a6b73c95":"X = x_test[10].reshape((1,) + x_test[10].shape)\nplt.imshow(X.reshape(28, 28));\nprint('Prediction on instance to be explained: {}'.format([np.argmax(cnn.predict(X))]))\nprint('Prediction probabilities for each class on the instance: {}'.format(cnn.predict(X)))","b9bdeba2":"shape = (1,) + x_train.shape[1:]\ngamma = 100.\ntheta = 100.\nc_init = 1.\nc_steps = 2\nmax_iterations = 1000\nfeature_range = (x_train.min(),x_train.max())","c4bdc5e3":"# initialize explainer, fit and generate counterfactual\ncf = CounterFactualProto(cnn, shape, gamma=gamma, theta=theta,\n                         ae_model=ae, enc_model=enc, max_iterations=max_iterations,\n                         feature_range=feature_range, c_init=c_init, c_steps=c_steps)\nstart_time = time()\ncf.fit(x_train)  # find class prototypes\nprint('Time to find prototypes each class: {:.3f} sec'.format(time() - start_time))\nstart_time = time()\nexplanation = cf.explain(X)\nprint('Explanation took {:.3f} sec'.format(time() - start_time))","82dcb3ab":"print('Counterfactual prediction: {}'.format(explanation.cf['class']))\nprint('Closest prototype class: {}'.format(explanation.id_proto))\nplt.imshow(explanation.cf['X'].reshape(28, 28));","f8638ff3":"X = x_test[12].reshape((1,) + x_test[12].shape)\nplt.imshow(X.reshape(28, 28));\nprint('Prediction on instance to be explained: {}'.format([np.argmax(cnn.predict(X))]))","8f7a857e":"# initialize explainer, fit and generate counterfactuals\ncf = CounterFactualProto(cnn, shape, gamma=gamma, theta=theta,\n                         ae_model=ae, enc_model=enc, max_iterations=max_iterations,\n                         feature_range=feature_range, c_init=c_init, c_steps=c_steps)\ncf.fit(x_train)\nexplanation_1 = cf.explain(X, k=5, k_type='mean')\nproto_1 = explanation_1.id_proto\nexplanation_2 = cf.explain(X, k=5, k_type='mean', target_class=[8])\nproto_2 = explanation_2.id_proto","faab52c7":"print('Counterfactual prediction: {}'.format(explanation_1.cf['class']))\nprint('Closest prototype class: {}'.format(proto_1))\nplt.imshow(explanation_1.cf['X'].reshape(28, 28));","b5d27918":"print('Counterfactual prediction: {}'.format(explanation_2.cf['class']))\nprint('Closest prototype class: {}'.format(proto_2))\nplt.imshow(explanation_2.cf['X'].reshape(28, 28));","65913dd8":"X = x_test[20].reshape(1, 28, 28, 1)\nplt.imshow(X.reshape(28, 28));\nprint('Prediction on instance to be explained: {}'.format([np.argmax(cnn.predict(X))]))","62e876c2":"c_init = 0. \nc_steps = 1  ","2d7ceb87":"predict_fn = lambda x: cnn.predict(x)\n\n# initialize explainer, fit and generate counterfactuals\ncf = CounterFactualProto(predict_fn, shape, gamma=gamma, theta=theta,\n                         ae_model=ae, enc_model=enc, max_iterations=max_iterations,\n                         feature_range=feature_range, c_init=c_init, c_steps=c_steps)\ncf.fit(x_train)\nstart_time = time()\nexplanation = cf.explain(X, k=1)\nprint('Explanation took {:.3f} sec'.format(time() - start_time))","4da2e27a":"print('Counterfactual prediction: {}'.format(explanation.cf['class']))\nprint('Closest prototype class: {}'.format(explanation.id_proto))\nplt.imshow(explanation.cf['X'].reshape(28, 28))","d8b4206c":"The nearest protype class is found by considering the mean of the data points and the nearest prototype is identified to be 7 that is image of sneaker. The above change takes place naturally when no target class is specified by us.","81f3bcf4":"The shape of the sandal is made to look similar to a bag, which is the specified target class -8.","5517c83f":"Defining and training the auto-encoder :","8effbc12":"The counterfactuals guided by prototypes method works on black-box models. In the context of predictive models, a counterfactual instance describes the necessary change in input features of a test instance that alter the prediction to a predefined output (e.g. a prediction class). \n\nIt can use both an encoder or k-d trees to define the prototypes.This method can speed up the search, especially for black box models, and create interpretable counterfactuals. This is because only the prototypes (which are less in number) are used for finding out the counterfactuals.\n","9e7f23c5":"Generating counterfactual guided by the nearest class prototype ","9372fb84":"# COUNTERFACTUALS GUIDED BY PROTOTYPES ON FASHION MNIST\n\n\n\n","c3d55539":"The original instance has the target class 4 which represents coat.","18291532":"Now, instead of the nearest prototype class, we can also explicitly specify the target class in order to move the counterfactual to a specific prediction class.","7bd873e6":"The nearest prorotype class is searched and found to be 2 i.e.  pullover, which can be expected as a coat is similar to pullover. The counterfactual preduction is same as the nearest prototype which implies that the counterfactual makes the features change to approach to the nearest prototype class successfully.","9e5b896e":"We take an instance that is an image of a sandal.","76ddf156":"Now, here the nearest prorotype class is found to be 6 i.e.  shirt, which is very similar to the original prediction pullover. The changes to the pixels are made to the shirt to make it look like shirt and thus change the prediction to 6.","8cb82981":"Original instance :","4a3a2e23":"As shown, the time taken is quite large if autoencoder is used.","87c4b673":"The original prediction is class 2 which is an image of pullover.","f5ec86ad":"Specify prototype classes","7174f7c0":"Defining and training the CNN model :","87c8d5fe":"Since the accuracy is good, the model is good.","a13110d5":"To speed up the process(which is one of the advantages of prototype counterfactual over counterfactual), the prediction loss term can be removed  as shown below: ","f27da267":"Results :","d8b99262":"The weight of prediction loss term is set to 0.","9318b546":"Loading the data :","bf58b9e5":"Counterfactual parameters :","f6421264":"Now much less time is taken for the explanation."}}