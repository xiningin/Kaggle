{"cell_type":{"2d554cd9":"code","d880ef74":"code","2118e2f9":"code","f1acafd4":"code","56afd445":"code","7a7d56c4":"code","a170c4cd":"code","b7102d7c":"code","febc9762":"code","70be3a3d":"code","179265e0":"code","1777b6a7":"code","0354d98a":"code","a9bea88b":"code","883564e7":"code","8eb9126b":"code","aad0f361":"code","0c559264":"code","d300b7f2":"code","d235cbb0":"code","7a873acb":"code","41c38786":"code","1a9cf147":"code","86437a9c":"code","0ef48777":"code","b173fa59":"code","f564256a":"code","a4174a9d":"code","87944f74":"code","154a59c7":"code","0c0d1a61":"code","d7166892":"code","2341669a":"code","2be528ff":"code","ec6481eb":"markdown","36285269":"markdown","5bfad577":"markdown","1d3e7a1c":"markdown","3bca6491":"markdown","f4ca57f9":"markdown","67a49129":"markdown","9ab4122f":"markdown","ff41ac8e":"markdown","210cbf9a":"markdown","4a377f07":"markdown","a075793d":"markdown","c23341c5":"markdown","b0e8c980":"markdown","adb49e5a":"markdown","0bb6df69":"markdown","438cd3c7":"markdown","c7875934":"markdown","de724255":"markdown","6f485c9c":"markdown"},"source":{"2d554cd9":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\n\nimport json\nimport ast\nimport eli5\nimport shap\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split\nfrom sklearn.linear_model import Ridge, RidgeCV\nimport gc\nfrom catboost import CatBoostClassifier\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport altair as alt\nfrom  altair.vega import v3\nfrom IPython.display import HTML\nfrom sklearn.linear_model import LinearRegression\n\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nfrom scipy import stats\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom bayes_opt import BayesianOptimization","d880ef74":"# Preparing altair. I use code from this great kernel: https:\/\/www.kaggle.com\/notslush\/altair-visualization-2018-stackoverflow-survey\n\nvega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + v3.SCHEMA_VERSION\nvega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\nvega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\nvega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\nnoext = \"?noext\"\n\npaths = {\n    'vega': vega_url + noext,\n    'vega-lib': vega_lib_url + noext,\n    'vega-lite': vega_lite_url + noext,\n    'vega-embed': vega_embed_url + noext\n}\n\nworkaround = \"\"\"\nrequirejs.config({{\n    baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n    paths: {}\n}});\n\"\"\"\n\n#------------------------------------------------ Defs for future rendering\ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n            \n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\nHTML(\"\".join((\n    \"<script>\",\n    workaround.format(json.dumps(paths)),\n    \"<\/script>\",\n)))\n\n","2118e2f9":"train = pd.read_csv('..\/input\/X_train.csv')\ny = pd.read_csv('..\/input\/y_train.csv')\ntest = pd.read_csv('..\/input\/X_test.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')","f1acafd4":"train.shape, test.shape","56afd445":"train.head()","7a7d56c4":"test.head()","a170c4cd":"train['series_id'].nunique(), test['series_id'].nunique()","b7102d7c":"y['surface'].unique()","febc9762":"y['group_id'].nunique()","70be3a3d":"y['surface'].value_counts().reset_index().rename(columns={'index': 'target'})","179265e0":"target_count = y['surface'].value_counts().reset_index().rename(columns={'index': 'target'})\nrender(alt.Chart(target_count).mark_bar().encode(\n    y=alt.Y(\"target:N\", axis=alt.Axis(title='Surface'), sort=list(target_count['target'])),\n    x=alt.X('surface:Q', axis=alt.Axis(title='Count')),\n    tooltip=['target', 'surface']\n).properties(title=\"Counts of target classes\", width=400).interactive())","1777b6a7":"train.head()","0354d98a":"plt.figure(figsize=(26, 16))\nfor i, col in enumerate(train.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.hist(train[col], color='blue', bins=100)\n    plt.hist(test[col], color='green', bins=100)\n    plt.title(col)","a9bea88b":"plt.figure(figsize=(26, 16))\nfor i, col in enumerate(train.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.plot(train.loc[train['series_id'] == 1, col])\n    plt.title(col)","883564e7":"train_df = train[['series_id']].drop_duplicates().reset_index(drop=True)","8eb9126b":"for col in train.columns:\n    if 'orient' in col:\n        scaler = StandardScaler()\n        train[col] = scaler.fit_transform(train[col].values.reshape(-1, 1))\n        test[col] = scaler.transform(test[col].values.reshape(-1, 1))","aad0f361":"def calc_change_rate(x):\n    change = (np.diff(x) \/ x[:-1]).values\n    change = change[np.nonzero(change)[0]]\n    change = change[~np.isnan(change)]\n    change = change[change != -np.inf]\n    change = change[change != np.inf]\n    return np.mean(change)\n\ndef add_trend_feature(arr, abs_values=False):\n    idx = np.array(range(len(arr)))\n    if abs_values:\n        arr = np.abs(arr)\n    lr = LinearRegression()\n    lr.fit(idx.reshape(-1, 1), arr)\n    return lr.coef_[0]\n\ndef classic_sta_lta(x, length_sta, length_lta):\n    \n    sta = np.cumsum(x ** 2)\n\n    # Convert to float\n    sta = np.require(sta, dtype=np.float)\n\n    # Copy for LTA\n    lta = sta.copy()\n\n    # Compute the STA and the LTA\n    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n    sta \/= length_sta\n    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n    lta \/= length_lta\n\n    # Pad zeros\n    sta[:length_lta - 1] = 0\n\n    # Avoid division by zero by setting zero values to tiny float\n    dtiny = np.finfo(0.0).tiny\n    idx = lta < dtiny\n    lta[idx] = dtiny\n\n    return sta \/ lta\nfor col in tqdm_notebook(train.columns[3:]):\n    train_df[col + '_mean'] = train.groupby(['series_id'])[col].mean()\n    train_df[col + '_std'] = train.groupby(['series_id'])[col].std()\n    train_df[col + '_max'] = train.groupby(['series_id'])[col].max()\n    train_df[col + '_min'] = train.groupby(['series_id'])[col].min()\n    train_df[col + '_max_to_min'] = train_df[col + '_max'] \/ train_df[col + '_min']\n\n    for i in train_df['series_id']:\n        train_df.loc[i, col + '_mean_change_abs'] = np.mean(np.diff(train.loc[train['series_id'] == i, col]))\n        train_df.loc[i, col + '_mean_change_rate'] = calc_change_rate(train.loc[train['series_id'] == i, col])\n        \n        train_df.loc[i, col + '_q95'] = np.quantile(train.loc[train['series_id'] == i, col], 0.95)\n        train_df.loc[i, col + '_q99'] = np.quantile(train.loc[train['series_id'] == i, col], 0.99)\n        train_df.loc[i, col + '_q05'] = np.quantile(train.loc[train['series_id'] == i, col], 0.05)\n        \n        train_df.loc[i, col + '_abs_min'] = np.abs(train.loc[train['series_id'] == i, col]).min()\n        train_df.loc[i, col + '_abs_max'] = np.abs(train.loc[train['series_id'] == i, col]).max()\n        \n        train_df.loc[i, col + '_trend'] = add_trend_feature(train.loc[train['series_id'] == i, col].values)\n        train_df.loc[i, col + '_abs_trend'] = add_trend_feature(train.loc[train['series_id'] == i, col].values, abs_values=True)\n        train_df.loc[i, col + '_abs_mean'] = np.abs(train.loc[train['series_id'] == i, col]).mean()\n        train_df.loc[i, col + '_abs_std'] = np.abs(train.loc[train['series_id'] == i, col]).std()\n        \n        train_df.loc[i, col + '_mad'] = train.loc[train['series_id'] == i, col].mad()\n        train_df.loc[i, col + '_kurt'] = train.loc[train['series_id'] == i, col].kurtosis()\n        train_df.loc[i, col + '_skew'] = train.loc[train['series_id'] == i, col].skew()\n        train_df.loc[i, col + '_med'] = train.loc[train['series_id'] == i, col].median()\n        \n        train_df.loc[i, col + '_Hilbert_mean'] = np.abs(hilbert(train.loc[train['series_id'] == i, col])).mean()\n        \n        train_df.loc[i, col + '_Hann_window_mean'] = (convolve(train.loc[train['series_id'] == i, col], hann(15), mode='same') \/ sum(hann(15))).mean()\n        train_df.loc[i, col + '_classic_sta_lta1_mean'] = classic_sta_lta(train.loc[train['series_id'] == i, col], 10, 50).mean()\n\n        train_df.loc[i, col + '_Moving_average_10_mean'] = train.loc[train['series_id'] == i, col].rolling(window=10).mean().mean(skipna=True)\n        train_df.loc[i, col + '_Moving_average_16_mean'] = train.loc[train['series_id'] == i, col].rolling(window=16).mean().mean(skipna=True)\n        train_df.loc[i, col + '_Moving_average_10_std'] = train.loc[train['series_id'] == i, col].rolling(window=10).std().mean(skipna=True)\n        train_df.loc[i, col + '_Moving_average_16_std'] = train.loc[train['series_id'] == i, col].rolling(window=16).std().mean(skipna=True)\n        \n        train_df.loc[i, col + 'iqr'] = np.subtract(*np.percentile(train.loc[train['series_id'] == i, col], [75, 25]))\n        train_df.loc[i, col + 'ave10'] = stats.trim_mean(train.loc[train['series_id'] == i, col], 0.1)\n","0c559264":"test_df = sub[['series_id']]","d300b7f2":"for col in tqdm_notebook(test.columns[3:]):\n    test_df[col + '_mean'] = test.groupby(['series_id'])[col].mean()\n    test_df[col + '_std'] = test.groupby(['series_id'])[col].std()\n    test_df[col + '_max'] = test.groupby(['series_id'])[col].max()\n    test_df[col + '_min'] = test.groupby(['series_id'])[col].min()\n    test_df[col + '_max_to_min'] = test_df[col + '_max'] \/ test_df[col + '_min']\n\n    for i in test_df['series_id']:\n        test_df.loc[i, col + '_mean_change_abs'] = np.mean(np.diff(test.loc[test['series_id'] == i, col]))\n        test_df.loc[i, col + '_mean_change_rate'] = calc_change_rate(test.loc[test['series_id'] == i, col])\n        \n        test_df.loc[i, col + '_q95'] = np.quantile(test.loc[test['series_id'] == i, col], 0.95)\n        test_df.loc[i, col + '_q99'] = np.quantile(test.loc[test['series_id'] == i, col], 0.99)\n        test_df.loc[i, col + '_q05'] = np.quantile(test.loc[test['series_id'] == i, col], 0.05)\n        \n        test_df.loc[i, col + '_abs_min'] = np.abs(test.loc[test['series_id'] == i, col]).min()\n        test_df.loc[i, col + '_abs_max'] = np.abs(test.loc[test['series_id'] == i, col]).max()\n        \n        test_df.loc[i, col + '_trend'] = add_trend_feature(test.loc[test['series_id'] == i, col].values)\n        test_df.loc[i, col + '_abs_trend'] = add_trend_feature(test.loc[test['series_id'] == i, col].values, abs_values=True)\n        test_df.loc[i, col + '_abs_mean'] = np.abs(test.loc[test['series_id'] == i, col]).mean()\n        test_df.loc[i, col + '_abs_std'] = np.abs(test.loc[test['series_id'] == i, col]).std()\n        \n        test_df.loc[i, col + '_mad'] = test.loc[test['series_id'] == i, col].mad()\n        test_df.loc[i, col + '_kurt'] = test.loc[test['series_id'] == i, col].kurtosis()\n        test_df.loc[i, col + '_skew'] = test.loc[test['series_id'] == i, col].skew()\n        test_df.loc[i, col + '_med'] = test.loc[test['series_id'] == i, col].median()\n        \n        test_df.loc[i, col + '_Hilbert_mean'] = np.abs(hilbert(test.loc[test['series_id'] == i, col])).mean()\n        \n        test_df.loc[i, col + '_Hann_window_mean'] = (convolve(test.loc[test['series_id'] == i, col], hann(15), mode='same') \/ sum(hann(15))).mean()\n        test_df.loc[i, col + '_classic_sta_lta1_mean'] = classic_sta_lta(test.loc[test['series_id'] == i, col], 10, 50).mean()\n\n        test_df.loc[i, col + '_Moving_average_10_mean'] = test.loc[test['series_id'] == i, col].rolling(window=10).mean().mean(skipna=True)\n        test_df.loc[i, col + '_Moving_average_16_mean'] = test.loc[test['series_id'] == i, col].rolling(window=16).mean().mean(skipna=True)\n        test_df.loc[i, col + '_Moving_average_10_std'] = test.loc[test['series_id'] == i, col].rolling(window=10).std().mean(skipna=True)\n        test_df.loc[i, col + '_Moving_average_16_std'] = test.loc[test['series_id'] == i, col].rolling(window=16).std().mean(skipna=True)\n        \n        test_df.loc[i, col + 'iqr'] = np.subtract(*np.percentile(test.loc[test['series_id'] == i, col], [75, 25]))\n        test_df.loc[i, col + 'ave10'] = stats.trim_mean(test.loc[test['series_id'] == i, col], 0.1)","d235cbb0":"train_df.head()","7a873acb":"n_fold = 3\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)","41c38786":"le = LabelEncoder()\nle.fit(y['surface'])\ny['surface'] = le.transform(y['surface'])\n\ntrain_df = train_df.drop(['series_id'], axis=1)\ntest_df = test_df.drop(['series_id'], axis=1)","1a9cf147":"def lgbm_evaluate(**params):\n    warnings.simplefilter('ignore')\n    \n    params['num_leaves'] = int(params['num_leaves'])\n    params['max_depth'] = int(params['max_depth'])\n        \n    clf = lgb.LGBMClassifier(**params, n_estimators=20000, nthread=-1)\n\n    test_pred_proba = np.zeros((train_df.shape[0], 9))\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y['surface'])):\n        X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n        y_train, y_valid = y['surface'].iloc[train_idx], y['surface'].iloc[valid_idx]\n        \n        model = lgb.LGBMClassifier(**params, n_estimators = 10000, n_jobs = -1)\n        model.fit(X_train, y_train, \n                eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='multi_logloss',\n                verbose=False, early_stopping_rounds=200)\n\n        y_pred_valid = model.predict_proba(X_valid)\n\n        test_pred_proba[valid_idx] = y_pred_valid\n\n    return accuracy_score(y_valid, y_pred_valid.argmax(1))","86437a9c":"params = {'colsample_bytree': (0.6, 0.8),\n      'learning_rate': (.0001, .5), \n      'num_leaves': (2, 124), \n      'subsample': (0.6, 1), \n      'max_depth': (3, 120), \n      'reg_alpha': (.001, 15.0), \n      'reg_lambda': (.001, 15.0), \n      'min_split_gain': (.001, .03),\n      'min_child_weight': (2, 80)}\n\nbo = BayesianOptimization(lgbm_evaluate, params)\nbo.maximize(init_points=5, n_iter=20)","0ef48777":"bo.max['params']","b173fa59":"def eval_acc(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'acc', accuracy_score(labels, preds.argmax(1)), True\n\ndef train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros((len(X), 9))\n    prediction = np.zeros((len(X_test), 9))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators = 10000, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='multi_logloss',\n                    verbose=5000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            score = accuracy_score(y_valid, y_pred_valid.argmax(1))\n            print(f'Fold {fold_n}. Accuracy: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000,  eval_metric='MAE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid\n        scores.append(accuracy_score(y_valid, y_pred_valid.argmax(1)))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","f564256a":"params = {'num_leaves': int(bo.max['params']['num_leaves']),\n          'min_data_in_leaf': int(bo.max['params']['min_child_weight']),\n          'min_split_gain': bo.max['params']['min_split_gain'],\n          'objective': 'multiclass',\n          'max_depth': int(bo.max['params']['max_depth']),\n          'learning_rate': bo.max['params']['learning_rate'],\n          \"boosting\": \"gbdt\",\n          \"bagging_freq\": 5,\n          \"bagging_fraction\": bo.max['params']['subsample'],\n          \"bagging_seed\": 11,\n          \"verbosity\": -1,\n          'reg_alpha': bo.max['params']['reg_alpha'],\n          'reg_lambda': bo.max['params']['reg_lambda'],\n          \"num_class\": 9,\n          'nthread': -1\n         }\noof_lgb, prediction_lgb, feature_importance = train_model(X=train_df, X_test=test_df, y=y['surface'], params=params, model_type='lgb', plot_feature_importance=True)","a4174a9d":"# I use code from this kernel: https:\/\/www.kaggle.com\/theoviel\/deep-learning-starter\nimport itertools\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","87944f74":"plot_confusion_matrix(y['surface'], oof_lgb.argmax(1), le.classes_)","154a59c7":"model = lgb.LGBMClassifier(**params, n_estimators = 20000, n_jobs = -1)\nX_train, X_valid, y_train, y_valid = train_test_split(train_df, y['surface'], test_size=0.2, stratify=y['surface'])\nmodel.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=5000, early_stopping_rounds=200)","0c0d1a61":"eli5.show_weights(model, targets=[0, 1], feature_names=list(X_train.columns), top=40, feature_filter=lambda x: x != '<BIAS>')","d7166892":"scaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(train_df))\nX_test_scaled = pd.DataFrame(scaler.transform(test_df))","2341669a":"model = SVC(probability=True)\noof_svc, prediction_svc = train_model(X=X_train_scaled, X_test=X_test_scaled, y=y['surface'], params=None, model_type='sklearn', model=model)","2be528ff":"sub['surface'] = le.inverse_transform(prediction_lgb.argmax(1))\nsub.to_csv('lgb_sub.csv', index=False)\nsub['surface'] = le.inverse_transform(prediction_svc.argmax(1))\nsub.to_csv('scv_sub.csv', index=False)\nsub['surface'] = le.inverse_transform((prediction_lgb + prediction_svc).argmax(1))\nsub.to_csv('blend.csv', index=False)","ec6481eb":"Hm. Don't see any patterns.","36285269":"### target","5bfad577":"## General information\n\nIn this competition we have data about small mobile robot driving over different floor surfaces. We need to predict the floor type based on robot's sensor data.\n\nThis kernel uses content of my previous kernel: https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive\n\nAlso I add bayesian optimization from this library: https:\/\/github.com\/fmfn\/BayesianOptimization\n\n```\nBayesian optimization works by constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not, as seen in the picture below.\n```\n![](https:\/\/raw.githubusercontent.com\/fmfn\/BayesianOptimization\/master\/examples\/bo_example.png)\n\n*work in progress*","1d3e7a1c":"### Feature generation\n\nI'll generate a lot of aggregate features. Descriptions will be done later. I use ideas from my kernel for another competition: https:\/\/www.kaggle.com\/artgor\/earthquakes-fe-more-features-and-samples","3bca6491":"We have a serious disbalance, some classes exist only in several series.","f4ca57f9":"Function to train models:","67a49129":"### Feature distribution","9ab4122f":"### Orientation - quaternion coordinates\n\nYou could notice that there are 4 coordinates: X, Y, Z, W.\n\nUsually we have X, Y, Z - Euler Angles. But Euler Angles are limited by a phenomenon called \"gimbal lock,\" which prevents them from measuring orientation when the pitch angle approaches +\/- 90 degrees. Quaternions provide an alternative measurement technique that does not suffer from gimbal lock. Quaternions are less intuitive than Euler Angles and the math can be a little more complicated.\n\nHere are some articles about it:\n\nhttp:\/\/www.chrobotics.com\/library\/understanding-quaternions\n\nhttp:\/\/www.tobynorris.com\/work\/prog\/csharp\/quatview\/help\/orientations_and_quaternions.htm\n\nBasically 3D coordinates are converted to 4D vectors.","ff41ac8e":"Let's have a look at the values of features in a single time-series","210cbf9a":"### train and test data\n\nFor train and test we have the following data:\n- ~3800 separate time-series\n- 128 measurements in each time-series with data on robot orientation, angular velocity and linear acceleration\n\nTarget have 1 class per series, so we can aggregate train and test data on series. We have 9 unique classes as a target.\n\nOne more important point: measurements are taken in groups (73 groups in total), so this data could be used in validation. We'll try it later.","4a377f07":"## Blending","a075793d":"## Data exploration","c23341c5":"### ELI5","b0e8c980":"## Loading and basic exploring of data","adb49e5a":"Feature generation for test data is the same.","0bb6df69":"Velocity and acceleration have normal distribution, orientation features seem to have normalized values (using tanh function).\n\nFeature distributions in train and test are quite similar.","438cd3c7":"Blue values show histograms for train data, green - test data.","c7875934":"### Bayesian optimization\n\nThere are several steps in using this library for bayesian optimization:\n* write a function to evaluate the hyperparameters. It should accept parameters as an input and return the value of the metric to maximize. If the metric should be minimized, then simply add `-` before the returned value;\n* initialize `BayesianOptimization` and run `maximize` method;\n* get optimized parameters.","de724255":"## Model interpretation","6f485c9c":"## Building model"}}