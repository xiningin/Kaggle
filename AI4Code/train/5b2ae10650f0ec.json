{"cell_type":{"7725c524":"code","63a439ec":"code","9b3a88f9":"code","6e10683a":"code","fb5b293e":"code","3f838960":"code","9d655dbc":"code","107d19bb":"code","2d26a5c4":"code","17cf9aef":"code","cc6882db":"code","bef7a7af":"code","1fb13479":"code","64ca2c8b":"code","7d58775b":"code","ef861c9a":"code","ba689502":"code","b83380da":"code","040f8881":"code","fe3c4704":"code","affbb492":"code","25296e64":"code","8f06c747":"code","1183fefa":"code","cb1527d3":"code","e7bda4a2":"code","039b8a7c":"code","477af356":"code","f08e2a55":"code","09b25140":"code","a7a7dcb9":"code","7ac28429":"code","1bf5177a":"code","cdec0d21":"code","e17824a4":"code","403d3d05":"code","f568bbe9":"code","5bf234f3":"code","62c66dc2":"code","030ccee0":"code","bb12166f":"code","c16cae2c":"code","6e8e3bcd":"code","8c3cf3f1":"code","430bfa0b":"code","6b1e9c95":"markdown","aa5836b8":"markdown","1f4f640f":"markdown","46cab331":"markdown","6dfa1ee2":"markdown","f3259bea":"markdown","9e31901d":"markdown","d33f3c33":"markdown"},"source":{"7725c524":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler #to normalise the data preprocessing\n\nimport warnings\nimport matplotlib.pyplot as plt\nimport cv2\nwarnings.filterwarnings('ignore') #to suppress the warnings\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","63a439ec":"mit_test = pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_test.csv',header=None)\nmit_train = pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_train.csv', header=None)\nptb_abnormal = pd.read_csv('\/kaggle\/input\/heartbeat\/ptbdb_abnormal.csv', header=None)\nptb_normal = pd.read_csv('\/kaggle\/input\/heartbeat\/ptbdb_normal.csv', header=None)","9b3a88f9":"mit_test.head()","6e10683a":"mit_train.head()","fb5b293e":"ptb_abnormal.head()","3f838960":"mit_test.rename(columns={187:\"Class\"}, inplace=True)\nmit_train.rename(columns={187:\"Class\"}, inplace=True)\nptb_abnormal.rename(columns={187:\"Class\"}, inplace=True)\nptb_normal.rename(columns={187:\"Class\"}, inplace=True)","9d655dbc":"print (\"MIT Train classes: \\n\", mit_train[\"Class\"].value_counts())\nprint (\"\\nMIT Test classes: \\n\", mit_test[\"Class\"].value_counts())\nprint (\"\\nPTB Abnormal classes: \\n\", ptb_abnormal[\"Class\"].value_counts())\nprint (\"\\nPTB Normal classes: \\n\", ptb_normal[\"Class\"].value_counts())","107d19bb":"# Setting Dictionary to define the type of Heartbeat for both datasets\nMIT_Outcome = {0. : 'Normal Beat',\n               1. : 'Supraventricular premature beat',\n               2. : 'Premature ventricular contraction',\n               3. : 'Fusion of ventricular and normal beat',\n               4. : 'Unclassifiable beat'}\nPTB_Outcome = {0. : 'Normal',\n               1. : 'Abnormal'}","2d26a5c4":"#Plotting 10 random samples from the MIT training dataset with their classification\nplt.figure(figsize=(25,10))\nnp_count = np.linspace(0,186,187)\nnp_time = np.tile(np_count,(10,1))\nrnd = np.random.randint(0,mit_train.shape[0],size=(10,))\n\n\nfor i in range(np_time.shape[0]):\n    ax = plt.subplot(2,5,i+1)\n    ax.plot(mit_train.iloc[rnd[i],np_time[i,:]])\n    ax.set_title(MIT_Outcome[mit_train.loc[rnd[i],'Class']])\n\nplt.show()","17cf9aef":"#Plotting 10 random samples from the PTB training dataset with their classification\nplt.figure(figsize=(25,10))\nrnd = np.random.randint(0,ptb_normal.shape[0],size=(5,))\nrnd1 = np.random.randint(0,ptb_abnormal.shape[0], size=(5,))\n\nfor i in range(np_time.shape[0]):\n    ax = plt.subplot(2,5,i+1)\n    if (i < 5):\n        ax.plot(ptb_normal.iloc[rnd[i],np_time[i,:]])\n        ax.set_title(PTB_Outcome[ptb_normal.loc[rnd[i],'Class']])\n    else:\n        ax.plot(ptb_abnormal.iloc[rnd1[i-5],np_time[i,:]])\n        ax.set_title(PTB_Outcome[ptb_abnormal.loc[rnd1[i-5],'Class']])\n\nplt.show()","cc6882db":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import classification_report, plot_confusion_matrix, confusion_matrix\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout, InputLayer, LSTM, GRU, BatchNormalization, Bidirectional, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.optimizers import SGD, RMSprop\nfrom tensorflow.keras.utils import to_categorical","bef7a7af":"#Preparing the training, validation and test sets for the PTB Data set\nptb_full = pd.concat([ptb_normal, ptb_abnormal], axis=0).reset_index()\nptb_full.drop(columns='index', inplace=True)\nptb_full = ptb_full.sample(ptb_full.shape[0], random_state=42)\ntrain_ptb, test_ptb, out_train_ptb, out_test_ptb = train_test_split(ptb_full.iloc[:,:187], ptb_full.iloc[:,-1], test_size=0.15, random_state=42)\ntrain_ptb, valid_ptb, out_train_ptb, out_valid_ptb = train_test_split(train_ptb, out_train_ptb, test_size=0.2, random_state=42 )","1fb13479":"normal, abnormal = np.bincount(ptb_full.loc[:,'Class'])\nnorm_weight = (1\/normal) * ((normal+abnormal)\/2)\nabnorm_weight = (1\/abnormal) * ((normal+abnormal)\/2)\nclass_weight = {0: norm_weight, 1: abnorm_weight}","64ca2c8b":"print(\"Traing dataset size: \", train_ptb.shape)\nprint(\"Validation dataset size: \", valid_ptb.shape)\nprint(\"Test dataset size: \", test_ptb.shape)","7d58775b":"#Normalizing the training, validation & test data \ntrain_ptb = normalize(train_ptb, axis=0, norm='max')\nvalid_ptb = normalize(valid_ptb, axis=0, norm='max')\ntest_ptb = normalize(test_ptb, axis=0, norm='max')","ef861c9a":"# Reshaping the dataframe into a 3-D Numpy array (batch, Time Period, Value)\nx_train_ptb = train_ptb.reshape(len(train_ptb),train_ptb.shape[1],1)\nx_valid_ptb = valid_ptb.reshape(len(valid_ptb),valid_ptb.shape[1],1)\nx_test_ptb = test_ptb.reshape(len(test_ptb),test_ptb.shape[1],1)\n\n# Converting the output into a categorical array\ny_train_ptb = to_categorical(out_train_ptb)\ny_valid_ptb = to_categorical(out_valid_ptb)\ny_test_ptb = to_categorical(out_test_ptb)","ba689502":"print(\"Traing dataset size: \", x_train_ptb.shape , \" -- Y size: \", y_train_ptb.shape)\nprint(\"Validation dataset size: \", x_valid_ptb.shape , \" -- Y size: \", y_valid_ptb.shape)\nprint(\"Test dataset size: \", x_test_ptb.shape , \" -- Y size: \", y_test_ptb.shape)","b83380da":"tf.keras.backend.clear_session()\n\n#Function to build Convolutional 1D Networks\ndef build_conv1d_model (input_shape=(x_train_ptb.shape[1],1)):\n    model = keras.models.Sequential()\n    \n    model.add(Conv1D(32,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(64,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(128,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n    model.add(Conv1D(256,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n    model.add(Conv1D(512,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(2, activation=\"softmax\"))\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(2,\"micro\")])\n    return model","040f8881":"checkpoint_cb = ModelCheckpoint(\"conv1d_ptb.h5\", save_best_only=True)\n\nearlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n\nmodel_conv1d_ptb= build_conv1d_model(input_shape=(x_train_ptb.shape[1], x_train_ptb.shape[2]))\nmodel_conv1d_ptb.summary()","fe3c4704":"history_conv1d_ptb = model_conv1d_ptb.fit(x_train_ptb, y_train_ptb, epochs=40, batch_size=32, \n                                          class_weight=class_weight, validation_data=(x_valid_ptb, y_valid_ptb),  \n                                          callbacks=[checkpoint_cb, earlystop_cb])","affbb492":"model_conv1d_ptb.load_weights(\"conv1d_ptb.h5\")\nmodel_conv1d_ptb.evaluate(x_test_ptb,y_test_ptb)","25296e64":"# Calculating the predictions based on the highest probability class\nconv1d_pred_proba_ptb = model_conv1d_ptb.predict (x_test_ptb)\nconv1d_pred_ptb = np.argmax(conv1d_pred_proba_ptb, axis=1)","8f06c747":"print(classification_report(out_test_ptb, conv1d_pred_ptb > 0.5, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","1183fefa":"cm = confusion_matrix(y_true= out_test_ptb, y_pred=conv1d_pred_ptb, labels=list(PTB_Outcome.keys()))\nplt.figure(figsize=(8,6))\nplt.imshow(cm, cmap=\"Oranges\")\nplt.colorbar()\nplt.xticks(list(PTB_Outcome.keys()),[PTB_Outcome[i] for i in PTB_Outcome], rotation=90)\nplt.yticks(list(PTB_Outcome.keys()),[PTB_Outcome[i] for i in PTB_Outcome])\nplt.show()","cb1527d3":"# Plotting the training and validatoin results\nplt.figure(figsize=(25,12))\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['loss'],\n           color='r', label='Train loss')\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['val_loss'],\n           color='b', label='Val loss' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['f1_score'],\n           color='g', label='Train F1')\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['val_f1_score'],\n           color='c', label='Val F1' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","e7bda4a2":"def build_conv1d_res_model (input_shape=(x_train_ptb.shape[1],1)):\n    model = keras.models.Sequential()\n    \n    input_ = tf.keras.layers.Input (shape=(input_shape))\n    \n    conv1_1 = Conv1D(64,7, padding='same', input_shape=input_shape) (input_)\n    conv1_1 = BatchNormalization() (conv1_1)\n    conv1_1 = tf.keras.layers.ReLU() (conv1_1)\n\n    conv1_2 = Conv1D(64,7, padding='same') (conv1_1)\n    conv1_2 = BatchNormalization() (conv1_2)\n    conv1_2 = tf.keras.layers.ReLU() (conv1_2)\n   \n    conv1_3 = Conv1D(64,7, padding='same') (conv1_2)\n    conv1_3 = BatchNormalization() (conv1_3)\n    conv1_3 = tf.keras.layers.ReLU() (conv1_3)\n\n    concat_1 = Concatenate()([conv1_1 , conv1_3 ])\n    max_1 = MaxPool1D(5, padding=\"same\") (concat_1)\n    \n    conv1_4 = Conv1D(128,7, padding='same') (max_1)\n    conv1_4 = BatchNormalization() (conv1_4)\n    conv1_4 = tf.keras.layers.ReLU() (conv1_4)\n\n    conv1_5 = Conv1D(128,7, padding='same', input_shape=input_shape) (conv1_4)\n    conv1_5 = BatchNormalization() (conv1_5)\n    conv1_5 = tf.keras.layers.ReLU() (conv1_5)\n    \n    conv1_6 = Conv1D(128,7, padding='same', input_shape=input_shape) (conv1_5)\n    conv1_6 = BatchNormalization() (conv1_6)\n    conv1_6 = tf.keras.layers.ReLU() (conv1_6)\n\n    concat_2 = Concatenate()([conv1_4, conv1_6])\n    max_2 = MaxPool1D(5, padding=\"same\") (concat_2)\n\n    flat = Flatten() (max_2)\n    dense_1 = Dense(512, activation='relu') (flat)\n    drop_1 = Dropout(0.5) (dense_1)\n    dense_2 = Dense(256, activation='relu') (drop_1)\n    drop_2 = Dropout(0.5) (dense_2)\n    dense_3 = Dense(128, activation='relu') (drop_2)\n    dense_4 = Dense(64, activation='relu') (dense_3)\n    dense_5 = Dense(32, activation='relu') (dense_4)\n    dense_6 = Dense(2, activation=\"softmax\") (dense_5)\n    \n    model = Model (inputs=input_ , outputs=dense_6)\n    \n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(2,\"micro\")])\n    return model","039b8a7c":"checkpoint_cb = ModelCheckpoint(\"conv1d_res_ptb.h5\", save_best_only=True)\n\nearlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n\ninp_shape = (x_train_ptb.shape[1], x_train_ptb.shape[2])\nmodel_conv1d_res_ptb= build_conv1d_res_model(input_shape=(x_train_ptb.shape[1], x_train_ptb.shape[2]))\n#model_conv1d_res_ptb.build(inp_shape)","477af356":"history_conv1d_res_ptb = model_conv1d_res_ptb.fit(x_train_ptb, y_train_ptb, epochs=40, batch_size=32, \n                                          class_weight=class_weight, validation_data=(x_valid_ptb, y_valid_ptb),  \n                                          callbacks=[checkpoint_cb, earlystop_cb])","f08e2a55":"model_conv1d_res_ptb.load_weights(\"conv1d_res_ptb.h5\")\nmodel_conv1d_res_ptb.evaluate(x_test_ptb,y_test_ptb)","09b25140":"# Calculating the predictions based on the highest probability class\nconv1d_res_pred_proba_ptb = model_conv1d_res_ptb.predict (x_test_ptb)\nconv1d_res_pred_ptb = np.argmax(conv1d_res_pred_proba_ptb, axis=1)","a7a7dcb9":"print(classification_report(out_test_ptb, conv1d_res_pred_ptb > 0.5, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","7ac28429":"cm = confusion_matrix(y_true= out_test_ptb, y_pred=conv1d_res_pred_ptb, labels=list(PTB_Outcome.keys()))\nplt.figure(figsize=(8,6))\nplt.imshow(cm, cmap=\"Oranges\")\nplt.colorbar()\nplt.xticks(list(PTB_Outcome.keys()),[PTB_Outcome[i] for i in PTB_Outcome], rotation=90)\nplt.yticks(list(PTB_Outcome.keys()),[PTB_Outcome[i] for i in PTB_Outcome])\nplt.show()","1bf5177a":"# Plotting the training and validatoin results\nplt.figure(figsize=(25,12))\nplt.plot(history_conv1d_res_ptb.epoch, history_conv1d_res_ptb.history['loss'],\n           color='r', label='Train loss')\nplt.plot(history_conv1d_res_ptb.epoch, history_conv1d_res_ptb.history['val_loss'],\n           color='b', label='Val loss' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(history_conv1d_res_ptb.epoch, history_conv1d_res_ptb.history['f1_score'],\n           color='g', label='Train F1')\nplt.plot(history_conv1d_res_ptb.epoch, history_conv1d_res_ptb.history['val_f1_score'],\n           color='c', label='Val F1' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","cdec0d21":"# Since the MIT dataset already comes as a train set and test set, we just split 20% of the training set for validation\ntrain_mit, valid_mit, out_train_mit, out_valid_mit = train_test_split(mit_train.iloc[:,:187], mit_train.iloc[:,-1], test_size=0.20, random_state=42)\n\n#we remove the targets from the test set\ntest_mit, out_test_mit = mit_test.iloc[:,:187], mit_test.iloc[:,-1]\n\n#Normalizing the training & test data \ntrain_mit = normalize(train_mit, axis=0, norm='max')\nvalid_mit = normalize(valid_mit, axis=0, norm='max')\ntest_mit = normalize(test_mit, axis=0, norm='max')","e17824a4":"# Reshaping the dataframe into a 3-D Numpy array (batch, Time Period, Value)\nx_train_mit = train_mit.reshape(len(train_mit),train_mit.shape[1],1)\nx_valid_mit = valid_mit.reshape(len(valid_mit),valid_mit.shape[1],1)\nx_test_mit = test_mit.reshape(len(test_mit),test_mit.shape[1],1)\n\n# Converting the output into a categorical array for each class\ny_train_mit = to_categorical(out_train_mit)\ny_valid_mit = to_categorical(out_valid_mit)\ny_test_mit = to_categorical(out_test_mit)","403d3d05":"print(\"Traing dataset size: \", x_train_mit.shape , \" -- Y size: \", y_train_mit.shape)\nprint(\"Validation dataset size: \", x_valid_mit.shape , \" -- Y size: \", y_valid_mit.shape)\nprint(\"Test dataset size: \", x_test_mit.shape , \" -- Y size: \", y_test_mit.shape)","f568bbe9":"# Calculating the class weights dictionary to feed into the model\nnormalmit, superpre, prevent, fusion, unclass = np.bincount(mit_train.loc[:,'Class'])\ntotal_rec = len(mit_train)\nnormmit_weight = (1\/normalmit) * (total_rec\/5)\nsuperpre_weight = (1\/superpre) * (total_rec\/5)\nprevent_weight = (1\/prevent) * (total_rec\/5)\nfusion_weight =  (1\/fusion) * (total_rec\/5)\nunclass_weight = (1\/unclass) * (total_rec\/5)\nclass_weight_mit = {0: normmit_weight, 1: superpre_weight, 2: prevent_weight,\n               3: fusion_weight, 4: unclass_weight}","5bf234f3":"tf.keras.backend.clear_session()\n\n#Function to build Convolutional 1D Networks\ndef build_conv1d_model_mit (input_shape=(x_train_mit.shape[1],1)):\n    model = keras.models.Sequential()\n    \n    model.add(Conv1D(32,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(64,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(128,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(256,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n    \n    model.add(Conv1D(512,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(5, activation=\"softmax\"))\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(5,\"micro\")])\n    return model","62c66dc2":"checkpoint_cb = ModelCheckpoint(\"conv1d_mit.h5\", save_best_only=True)\n\nearlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n\nmodel_conv1d_mit= build_conv1d_model_mit(input_shape=(x_train_mit.shape[1], x_train_mit.shape[2]))\nmodel_conv1d_mit.summary()","030ccee0":"history_conv1d_mit = model_conv1d_mit.fit(x_train_mit, y_train_mit, epochs=40, batch_size=32, \n                             class_weight=class_weight_mit, validation_data=(x_valid_mit, y_valid_mit),  \n                             callbacks=[checkpoint_cb, earlystop_cb])","bb12166f":"model_conv1d_mit.load_weights(\"conv1d_mit.h5\")\nmodel_conv1d_mit.evaluate(x_test_mit, y_test_mit)\nconv1d_pred_mit_proba = model_conv1d_mit.predict (x_test_mit)\nconv1d_pred_mit = np.argmax(conv1d_pred_mit_proba,axis=1)","c16cae2c":"print(classification_report(out_test_mit, conv1d_pred_mit, target_names=[MIT_Outcome[i] for i in MIT_Outcome]))","6e8e3bcd":"cm = confusion_matrix(y_true= out_test_mit, y_pred=conv1d_pred_mit, labels=list(MIT_Outcome.keys()))\nplt.figure(figsize=(8,6))\nplt.imshow(cm, cmap=\"Oranges\")\nplt.colorbar()\nplt.xticks(list(MIT_Outcome.keys()),[MIT_Outcome[i] for i in MIT_Outcome], rotation=90)\nplt.yticks(list(MIT_Outcome.keys()),[MIT_Outcome[i] for i in MIT_Outcome])\nplt.show()","8c3cf3f1":"# Plotting the training and validatoin results\nplt.figure(figsize=(25,12))\nplt.plot(history_conv1d_mit.epoch, history_conv1d_mit.history['loss'],\n           color='r', label='Train loss')\nplt.plot(history_conv1d_mit.epoch, history_conv1d_mit.history['val_loss'],\n           color='b', label='Val loss' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(history_conv1d_mit.epoch, history_conv1d_mit.history['f1_score'],\n           color='g', label='Train F1')\nplt.plot(history_conv1d_mit.epoch, history_conv1d_mit.history['val_f1_score'],\n           color='c', label='Val F1' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","430bfa0b":"def build_conv1d_res_model_mit (input_shape=(x_train_mit.shape[1],1)):\n    model = keras.models.Sequential()\n    \n    input_ = tf.keras.layers.Input (shape=(input_shape))\n        \n    conv1_1 = Conv1D(64,7, padding='same', input_shape=input_shape) (input_)\n    conv1_1 = BatchNormalization() (conv1_1)\n    conv1_1 = tf.keras.layers.ReLU() (conv1_1)\n\n    conv1_2 = Conv1D(64,7, padding='same') (conv1_1)\n    conv1_2 = BatchNormalization() (conv1_2)\n    conv1_2 = tf.keras.layers.ReLU() (conv1_2)\n    \n    conv1_3 = Conv1D(64,7, padding='same') (conv1_2)\n    conv1_3 = BatchNormalization() (conv1_3)\n    conv1_3 = tf.keras.layers.ReLU() (conv1_3)\n\n    concat_1 = Concatenate()([conv1_1 , conv1_3 ])\n    max_1 = MaxPool1D(5, padding=\"same\") (concat_1)\n    \n    conv1_4 = Conv1D(128,7, padding='same') (max_1)\n    conv1_4 = BatchNormalization() (conv1_4)\n    conv1_4 = tf.keras.layers.ReLU() (conv1_4)\n\n    conv1_5 = Conv1D(128,7, padding='same', input_shape=input_shape) (conv1_4)\n    conv1_5 = BatchNormalization() (conv1_5)\n    conv1_5 = tf.keras.layers.ReLU() (conv1_5)\n    \n    conv1_6 = Conv1D(128,7, padding='same', input_shape=input_shape) (conv1_5)\n    conv1_6 = BatchNormalization() (conv1_6)\n    conv1_6 = tf.keras.layers.ReLU() (conv1_6)\n\n    concat_2 = Concatenate()([conv1_4, conv1_6])\n    max_2 = MaxPool1D(5, padding=\"same\") (concat_2)\n    \n    conv1_7 = Conv1D(256,7, padding='same') (max_2)\n    conv1_7 = BatchNormalization() (conv1_7)\n    conv1_7 = tf.keras.layers.ReLU() (conv1_7)\n\n    conv1_8 = Conv1D(256,7, padding='same') (conv1_7)\n    conv1_8 = BatchNormalization() (conv1_8)\n    conv1_8 = tf.keras.layers.ReLU() (conv1_8)\n\n    conv1_9 = Conv1D(256,7, padding='same') (conv1_8)\n    conv1_9 = BatchNormalization() (conv1_9)\n    conv1_9 = tf.keras.layers.ReLU() (conv1_9)\n\n    concat_3 = Concatenate()([conv1_7, conv1_9])\n    max_3 = MaxPool1D(5, padding=\"same\") (concat_3)\n\n    flat = Flatten() (max_3)\n    dense_1 = Dense(512, activation='relu') (flat)\n    drop_1 = Dropout(0.5) (dense_1)\n    dense_2 = Dense(256, activation='relu') (drop_1)\n    drop_2 = Dropout(0.5) (dense_2)\n    dense_3 = Dense(128, activation='relu') (drop_2)\n    dense_4 = Dense(64, activation='relu') (dense_3)\n    dense_5 = Dense(32, activation='relu') (dense_4)\n    dense_6 = Dense(5, activation=\"softmax\") (dense_5)\n        \n    model = Model (inputs=input_ , outputs=dense_6)\n    \n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(2,\"micro\")])\n    return model","6b1e9c95":"Defining Conv1D model for MIT\u00b6\nCreating a model based on a series of Conv1D layers that are connected to another series of full connected dense layers","aa5836b8":"Looking at how many classes are there in each dataset The MIT dataset has 5 clases:\n\n* 0 = N (Normal Beat)\n* 1 = S (Supraventricular premature beat)\n* 2 = V (Premature ventricular contraction)\n* 3 = F (Fusion of ventricular and normal beat)\n* 4 = Q (Unclassifiable beat)\nCompared to the PTB dataset which is 1 for abnormal and 0 for normal","1f4f640f":"# Deep Learning Analysis","46cab331":"# Defining Conv1D Residual model for PTB\nCreating a model based on a series of Conv1D layers with 2 residual blocks that are connected to another series of full connected dense layers","6dfa1ee2":"# Generating Plots of some of the samples in the dataset","f3259bea":"Defining Conv1D Residual model for MIT\nCreating a model based on a series of Conv1D layers with 3 residual blocks that are connected to another series of full connected dense layers","9e31901d":"# MIT Dataset Analysis","d33f3c33":"# Calculating the class weight\nCreate a dictionary of each class weight to feed into the model since the data is imbalanced"}}