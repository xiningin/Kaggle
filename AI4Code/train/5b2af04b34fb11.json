{"cell_type":{"2a7491ca":"code","2ce49436":"code","6302d5d3":"code","34d68fa5":"code","4e3dfc5d":"code","7e0d4669":"code","cc2571b6":"code","b429aadb":"code","b6710a51":"code","8cd2da47":"code","1ba33424":"code","2afb0e68":"code","aad916a4":"code","dc2e3275":"code","87097509":"code","c1fb2f5a":"code","37610103":"markdown","c0c36e02":"markdown","7e5ec6b7":"markdown","cbbf7062":"markdown","3eda269b":"markdown","4147581e":"markdown","55955009":"markdown","0a7f4567":"markdown","dbf63d98":"markdown","d2de61e6":"markdown","021360ad":"markdown","e75cba1a":"markdown","e4ab1a37":"markdown"},"source":{"2a7491ca":"import numpy as np \nimport pandas as pd \nimport random as rn\nimport re\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report","2ce49436":"rn.seed(a=42)\np = 0.004  # to randomly select n% of the rows\n\ndf_reviews = pd.read_csv('\/kaggle\/input\/steam-reviews\/dataset.csv', \n                         skiprows=lambda i: i>0 and rn.random() > p)\n\n# size of dataframe\ndf_reviews.shape\n# display the head of data\ndisplay(df_reviews.head())","6302d5d3":"print(df_reviews.review_score.unique())\nprint(df_reviews.review_votes.unique())","34d68fa5":"# convert review text to string\ndf_reviews[\"review_text\"] = df_reviews[\"review_text\"].astype(str)\ndf_reviews[\"review_votes\"] = df_reviews[\"review_votes\"].astype(str)\ndf_reviews.review_text = df_reviews.review_text.apply(lambda s: s.strip())\n\n# add review_votes as a test to the beginning of reviews\n# I found that if we do this we will get slightly better result on \n# the test and holdout overall (on aerage). No worries if you don't do it ...\ndf_reviews[\"review_text\"] = df_reviews.review_votes + ' . ' + df_reviews.review_text\n\ndisplay(df_reviews.head())","4e3dfc5d":"# drop the reviews with null score\ndf_reviews_2 = df_reviews[df_reviews[\"review_score\"].notnull()]\n\n# change the scores from 1, -1 to 1 and 0\ndf_reviews_2[\"review_score\"] = \\\nnp.where(df_reviews_2[\"review_score\"]==-1, 0, df_reviews_2[\"review_score\"])\n\n# distribution of negative and positive reviews\ndf_reviews_2[\"review_score\"].value_counts()","7e0d4669":"df_reviews_2['word_count'] = df_reviews_2.review_text.apply(lambda s: len(s.split()))\n\nprint(df_reviews_2.word_count.min())\nprint(df_reviews_2.word_count.quantile(0.05))\nprint(df_reviews_2.word_count.quantile(0.25))\nprint(df_reviews_2.word_count.median())\nprint(df_reviews_2.word_count.quantile(0.95))\nprint(df_reviews_2.word_count.quantile(0.99))\nprint(df_reviews_2.word_count.max())","cc2571b6":"a = sns.histplot(data=df_reviews_2, x=\"word_count\", binwidth=20)","b429aadb":"## Let's remove the \"Early Access Review\" comments. \n# These are the reviews with no comments writen by a human\/reviewer. \ndf_reviews_2 = df_reviews_2[df_reviews_2.review_text != \"Early Access Review\"]\ndf_reviews_2 = df_reviews_2[~df_reviews_2.review_text.isin(['nan'])]\nprint(df_reviews_2.shape)\n\n# Drop duplicates if there is any\ndf_reviews_2.drop_duplicates(['review_text', 'review_score'], inplace = True)\nprint(df_reviews_2.shape)","b6710a51":"## Text Cleaning\ndef replace_hearts_with_PAD(text):\n    return re.sub(r\"[\u2665]+\", ' **** ' ,text)\n\ndf_reviews_2['review_text_clean'] = df_reviews_2.review_text.apply(replace_hearts_with_PAD)","8cd2da47":"df_reviews_3 = df_reviews_2[['review_text_clean', 'review_score']]\ndf_reviews_3 = df_reviews_3.rename({\"review_text_clean\": \"text\", \"review_score\": \"labels\"});\ndf_reviews_3.head()","1ba33424":"# split the dataset into train, test and holdout sets (60-20-20)\n\ntrain_df, eval_df = train_test_split(df_reviews_3, test_size = 0.4, random_state = 42)\ntest_df , holdout_df = train_test_split(eval_df, test_size = 0.5, random_state = 42)\n\nprint(train_df.shape)\nprint(test_df.shape)\nprint(holdout_df.shape)","2afb0e68":"# # downsample the positive comments\n# coeff = 2\n# train_df_pos = train_df[train_df.review_score == 1]\n# train_df_neg = train_df[train_df.review_score == 0]\n# print(train_df_pos.shape[0])\n# print(train_df_neg.shape[0])\n# train_df_pos_sample = train_df_pos.sample(n = int(coeff*train_df_neg.shape[0]), random_state = 142)\n# train_df_balanced = pd.concat([train_df_pos_sample, train_df_neg])\n# for i in range(100): # for extra shuffeling!! I know ... \n#     train_df_balanced = train_df_balanced.sample(n = train_df_balanced.shape[0], random_state = 142)\n# print(train_df_balanced.shape)\n# print(train_df_balanced.groupby('review_score')['review_text'].count())\n# train_df = train_df_balanced\n# train_df_balanced.head()","aad916a4":"!pip install -q simpletransformers","dc2e3275":"from simpletransformers.classification import ClassificationModel\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n\n# Create a ClassificationModel\nmodel_type = \"roberta\"\nmodel_name = \"roberta-base\"\n\nroberta_model = ClassificationModel(\n                          model_type,\n                          model_name,\n                          args={'num_train_epochs' : 1,\n                                 \"train_batch_size\": 16,\n                                 \"eval_batch_size\": 16,\n                                 \"fp16\": False,\n                                 \"optimizer\": \"AdamW\",\n                                 \"adam_epsilon\": 1e-8,\n                                 \"learning_rate\": 1e-5,\n                                 \"weight_decay\": 0.7,\n                                 'overwrite_output_dir': True,\n                                 \"save_eval_checkpoints\": False,\n                                 \"save_model_every_epoch\": False,\n                                 \"no_cache\": True,\n                                 \"manual_seed\": 12345})\n\n# The reason that we set num_train_epochs to 1 and repeat the training in the following loop is that\n# in the transformer models the learning rate is impacted by the epoch\/(total number of epochs).\n# That means, for example, if you run the model with num_train_epochs=5 and you find that the best\n# model was the one at the third epoch, if you rerun the model with num_train_epochs =3 you will\n# not get the same result you saw on the third epoch of num_train_epochs=5. But with making the\n# following loop we will not see that issue.\n\nfor i in range(2):\n\n    # Train the model\n    roberta_model.train_model(train_df)\n    # Evaluate the model\n    result, model_outputs, wrong_predictions = roberta_model.eval_model(test_df)\n    print(\"Accuracy= \" ,(result['tp'] + result['tn']) \/ (result['tp'] + result['tn'] + \\\n                                                         result['fp'] + result['fn']))\n    print(\"Recall = \",(result['tn']) \/ (result['tn'] + result['fn'])) # simpletransformers mistakenly reports fn and fp. You need top flip them\n    print(result)\n    print(classification_report(np.argmax(model_outputs, axis = 1), test_df.review_score.values))","87097509":"result, model_outputs, wrong_predictions = roberta_model.eval_model(holdout_df)\nprint(\"Accuracy= \" ,(result['tp'] + result['tn']) \/ (result['tp'] + result['tn'] + \\\n                                                         result['fp'] + result['fn']))\nprint(\"Recall = \",(result['tn']) \/ (result['tn'] + result['fn'])) # simpletransformers mistakenly reports fn and fp. You need top flip them\nprint(result)","c1fb2f5a":"print(classification_report(np.argmax(model_outputs, axis = 1), holdout_df.review_score.values))","37610103":"<a id=\"section-2\"><\/a>\n## Read and Prep Data","c0c36e02":"Let's check the performance metrics on the holdout set","7e5ec6b7":"<a id=\"section-3\"><\/a>\n## Sentiment Classification with RoBERTa - SimpleTransformers","cbbf7062":"Let's check out the unique value of review_score and review_votes columns:","3eda269b":"<a id=\"section-1\"><\/a>\n## Import Libraries","4147581e":"### Intro\n\nI am inspired by [this notebook](https:\/\/www.kaggle.com\/pegahpooya\/steam-reviews-sentiment-classification) (Thank you for sharing your great work on this dataset). That notebook intrigued me to play with this dataset and use simpletransformers package that truely simplifies the training task and do a bit more data cleaning and apply roberta-base model for the transfer learning. RoBERTa and XLNet models are the state of the art models for classification. See the leaderboards [here](https:\/\/paperswithcode.com\/task\/text-classification). For this task, I got better result from RoBERTa.\n\nFor more modelings with simpletransformers [see my other notebook](https:\/\/www.kaggle.com\/dardodel\/microsoft-layoutlm-with-simpletransformers). ","55955009":"#### The original dataset is quite large. I only read a subset of rows for faster run.","0a7f4567":"99% of the reviews have less than 531 words which is good. It is in the range of max_length = 512 in roberta. Plot the histogram of word counts:","dbf63d98":"First of all the classificaiton report on both test and holdout sets are great! \n92-93% overal accuracy and 92% weighted averaged f1-score. \nThe model does a great job on the positive comments (positive recall = 94% vs negative recall = 85% on the holdout set). That is one of the reasons I went back and added more negative comments to the train set but I did not get a remarkably better overla performance. Yes, your model learns better from negative comments but do not perform well on the whole set because you basically change the data distribution. However, this needs more investigations and experiments. ","d2de61e6":"## Sentiment Classification - Transfer Learning - RoBERTa","021360ad":"I had some idea to imporve the overall accuracy or the recall score by balancing the negative and positive reviews or at least by reducing the unbalancedness. I tested with different coefficients but I could not get a significantly better result than using the original dataset (original ratio of pos and neg comments), so, I leave the next cell commented. ","e75cba1a":"Since the transformer models (here we use roberta) have maximum length of 512 tokens, it is a good idea to take a look at the distribution of reviews (number of words)","e4ab1a37":"I found that all of the F words and similar words (such as shit, etc.) are replaced by multiple \"\u2665\"s in the original text. I found that this is a source of confusion to the model. Heart is a symbol of love while they apear in the negative comments, therefore, it will impact our classifier. I replace them with '****' which seems a token close to the F words. The model performance improved after this cleaning step."}}