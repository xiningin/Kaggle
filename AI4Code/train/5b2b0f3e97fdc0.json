{"cell_type":{"ec64e085":"code","7d6aaaa8":"code","eeb39390":"code","c024dab3":"code","4bb4a49e":"code","62360c8b":"code","5c8863b4":"code","15375cc2":"code","889ece71":"code","662aa0f6":"code","b5e97622":"code","bb7a45fa":"code","271f81c8":"code","2cc9032d":"code","7956caa6":"code","424072dc":"code","b40eedcf":"code","e393a7ba":"markdown","e8ffd793":"markdown","73ca1f96":"markdown","7a213f10":"markdown","fecc310b":"markdown","abe81c3e":"markdown","d7b4060c":"markdown"},"source":{"ec64e085":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7d6aaaa8":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\n\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.display import Image\n\nimport matplotlib.pyplot as plt","eeb39390":"Train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nTest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\n\nprint(\"OK!\")","c024dab3":"print(\"Train shape: {}\".format(Train.shape))\nprint(\"Test shape: {}\".format(Test.shape))\nprint(\"Submission shape: {}\".format(submission.shape))","4bb4a49e":"# Drop labels from Training Set\nX = Train.drop(['label'], 1).values\n# Create labels\ny = Train['label'].values\n\nX_Test = Test.values\n\n# Scale Pixels\nX = X \/ 255.0\nX_Test = X_Test \/ 255.0\n\nprint(\"X shape: {}\".format(X.shape))\nprint(\"X_Test shape: {}\".format(X_Test.shape))","62360c8b":"# Reshape image (height = 28, width = 28 , canal = 1 [GRAY])\nX = X.reshape(-1,28,28,1)\nX_Test = X_Test.reshape(-1,28,28,1)\n\n# One-Hot encoding for labels\ny = to_categorical(y, num_classes=10)","5c8863b4":"# Split data\nrandom_state = 18\n\ntraining_images, validation_images, training_labels, validation_labels = train_test_split(X, y, test_size=0.1, shuffle=True, random_state=random_state)","15375cc2":"print(training_images.shape)\nprint(validation_images.shape)\nprint(training_labels.shape)\nprint(validation_labels.shape)","889ece71":"# Network architecture\nmodel = tf.keras.models.Sequential([\n    \n    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.BatchNormalization(),\n\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.BatchNormalization(),\n    \n    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(10, activation='softmax')\n])","662aa0f6":"# Show Architecture\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\nImage(\"model.png\")","b5e97622":"# Parameters\nepochs = 50\nbatch_size = 64\nnum_train_sumples = training_images.shape[0] \nnum_val_sumples = validation_images.shape[0]\noptimizer = 'adam' #RMSprop with appropriate lr,beta can increase Acc => Try Grid Search with TPU\nloss = \"categorical_crossentropy\"\n\n# Set optimizer and loss function\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\n# Data Augmantation\ntrain_datagen = ImageDataGenerator(\n       rotation_range=10,\n       width_shift_range=0.1,\n       height_shift_range=0.1,\n       zoom_range=0.1,\n       horizontal_flip=False,\n       vertical_flip=False)\n\n\ntrain_generator = train_datagen.flow(training_images, training_labels, batch_size=batch_size)\nvalidation_generator = train_datagen.flow(validation_images, validation_labels, batch_size=batch_size)","bb7a45fa":"# Train\nhistory = model.fit(train_generator, \n                    steps_per_epoch = num_train_sumples \/\/ batch_size, \n                    validation_data = validation_generator, \n                    validation_steps = num_val_sumples \/\/ batch_size, \n                    epochs=epochs)","271f81c8":"# Plot the loss and accuracy curves\nfig, ax = plt.subplots(2,1, figsize=(18, 10))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","2cc9032d":"# Confussion Matrix\ny_predict = model.predict(validation_images)\n\ny_predicted = np.argmax(y_predict, axis = 1) + 1\nY_test = np.argmax(validation_labels, axis = 1) + 1\nconfusion_matrix = pd.crosstab(y_predicted, Y_test, rownames = ['y_predicted'], colnames = ['y_test'])\nprint(confusion_matrix)","7956caa6":"# Classification for Test\nfrom sklearn.metrics import classification_report\ntarget_names = [\"Class {}\".format(i) for i in range(10)]\nprint(classification_report(Y_test, y_predicted, target_names=target_names))","424072dc":"# predict results\nresults = y_predict\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","b40eedcf":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"submission_output.csv\",index=False)","e393a7ba":"## MNIST: CNN with Keras (Data Augmentation, Dropout, BatchNorm,etc.) [T_Acc >%99.5, Val_Acc >99.5]\n### Training time: 50(epochs) x ~285ms\/step => CPU: 2GHz \u01305(quad-core)\n* 1. Import Library\n* 2. Import Data\n* 3. Data Modification \n* 4. Training\n* 5. Training vs. Validation Curves\n* 6. Test Results","e8ffd793":"# 5. Train vs Validation Curves","73ca1f96":"## 3. Data Modifications","7a213f10":"## 2. Import Data","fecc310b":"## 1. Import Library","abe81c3e":"# 6. Test","d7b4060c":"## 4. Training"}}