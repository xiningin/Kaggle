{"cell_type":{"1021905d":"code","29a90176":"code","7291061c":"code","9a3fa099":"code","fdd17dba":"code","74e21f4b":"code","aa918018":"code","f8f7b6e8":"code","1a94e743":"code","c5380d25":"code","043f019f":"markdown","e44072bb":"markdown","e46f618f":"markdown","cd5e25ba":"markdown","97bdf537":"markdown","66433c10":"markdown","fca41576":"markdown","86ea9f95":"markdown","88028ef4":"markdown","bd90f9f0":"markdown","fc34e572":"markdown","0c8a8ee8":"markdown","0fcb0a49":"markdown","3e5f33b1":"markdown","23cd7394":"markdown","83cb3c85":"markdown","a0e05ef3":"markdown","9a511a14":"markdown"},"source":{"1021905d":"### Importing Relevant Library and reading dataset\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nUSvideos = pd.read_csv(\"..\/input\/youtube-new\/USvideos.csv\")\n\n### Preliminary Data Cleaning to remove data which are not relevant to us\nUSvideos = USvideos[USvideos['video_error_or_removed'] == False] ###Remove video with error\nUSvideos = USvideos[USvideos['comments_disabled'] == False] ###Remove video that does not allow comment\/ratings(likes\/dislikes)\nUSvideos = USvideos[USvideos['ratings_disabled'] == False]\n\nUSvideos_id_date = USvideos.loc[:,['video_id','tags']].drop_duplicates()\n### Creating a new column which tells us the number of tags that the video has. \nUSvideos_id_date['tag_counts'] = USvideos_id_date['tags'].str.split(\"|\").apply(len)\nUSvideos_id_date['tag_counts'].plot.box()\nplt.show()\nUSvideos_id_date['tag_counts'].hist(bins=80)\nplt.xlabel('tag count')\nplt.ylabel('number of videos')\nplt.show()  \n","29a90176":"###Min-Max normalization\nUSvideos['views_mmnorm'] = (USvideos['views'] - USvideos['views'].min()) \/ (USvideos['views'].max() - USvideos['views'].min())\nUSvideos['likes_mmnorm'] = (USvideos['likes'] - USvideos['likes'].min()) \/ (USvideos['likes'].max() - USvideos['likes'].min())\nUSvideos['dislikes_mmnorm'] = (USvideos['dislikes'] - USvideos['dislikes'].min()) \/ (USvideos['dislikes'].max() - USvideos['dislikes'].min())\nUSvideos['comment_count_mmnorm'] = (USvideos['comment_count'] - USvideos['comment_count'].min()) \/ (USvideos['comment_count'].max() - USvideos['comment_count'].min())\nmmnorm_col = ['video_id', 'views_mmnorm','likes_mmnorm','dislikes_mmnorm','comment_count_mmnorm','tag_counts']\n\n###Mean normalization\nUSvideos['views_mnorm'] = (USvideos['views'] - USvideos['views'].min()) \/ (USvideos['views'].max() - USvideos['views'].min())\nUSvideos['likes_mnorm'] = (USvideos['likes'] - USvideos['likes'].min()) \/ (USvideos['likes'].max() - USvideos['likes'].min())\nUSvideos['dislikes_mnorm'] = (USvideos['dislikes'] - USvideos['dislikes'].min()) \/ (USvideos['dislikes'].max() - USvideos['dislikes'].min())\nUSvideos['comment_count_mnorm'] = (USvideos['comment_count'] - USvideos['comment_count'].min()) \/ (USvideos['comment_count'].max() - USvideos['comment_count'].min())\nmnorm_col = ['video_id', 'views_mnorm','likes_mnorm','dislikes_mnorm','comment_count_mnorm','tag_counts']\n\n###Standardizing data\nUSvideos['views_std'] = (USvideos['views'] - USvideos['views'].mean()) \/ (USvideos['views'].std())\nUSvideos['likes_std'] = (USvideos['likes'] - USvideos['likes'].mean()) \/ (USvideos['likes'].std())\nUSvideos['dislikes_std'] = (USvideos['dislikes'] - USvideos['dislikes'].mean()) \/ (USvideos['dislikes'].std())\nUSvideos['comment_count_std'] = (USvideos['comment_count'] - USvideos['comment_count'].mean()) \/ (USvideos['comment_count'].std())\nstd_col = ['video_id', 'views_std','likes_std','dislikes_std','comment_count_std','tag_counts']\n\nUSvideos['tag_counts'] = USvideos['tags'].str.split(\"|\").apply(len)","7291061c":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndf_mmnorm = USvideos[mmnorm_col].groupby(['video_id'])\ndf_mnorm = USvideos[mnorm_col].groupby(['video_id'])\ndf_std = USvideos[std_col].groupby(['video_id'])\n","9a3fa099":"###Testing with Linear Regression\ndef lin_reg_helper(df,norm_name):\n    data = []\n    for vid_id,rows in df:\n    #     print(\"vid_id: \" + vid_id)\n        rel_rows = rows.drop(['video_id'],axis=1).drop(['tag_counts'],axis=1)\n        days_trended = len(rel_rows)\n\n        if days_trended >= 5:\n            Xs = rel_rows\n            y = rows['tag_counts'].values.reshape(-1,1)\n            lin_reg = LinearRegression()\n            lin_reg.fit(Xs, y)\n    #         print('Intercept: \\n', lin_reg.intercept_)\n    #         print('Coefficients: \\n', lin_reg.coef_)\n            view_coef = lin_reg.coef_[0][0]\n            likes_coef = lin_reg.coef_[0][1]\n            dislikes_coef = lin_reg.coef_[0][2]\n            comment_count_coef = lin_reg.coef_[0][3]\n            data.append([vid_id, days_trended, view_coef,likes_coef,dislikes_coef,comment_count_coef])\n    data_df = pd.DataFrame(data, columns = ['Video ID', 'Trending Duration','view_coef','likes_coef','dislikes_coef','comment_count_coef']) \n\n    fig = plt.figure()\n    plt.title(norm_name + ' Linear')\n    # Divide the figure into a 2x1 grid, and give me the first section\n    ax1 = fig.add_subplot(221)\n    ax2 = fig.add_subplot(222)\n    ax3 = fig.add_subplot(223)\n    ax4 = fig.add_subplot(224)\n    data_df.plot.scatter(x ='Trending Duration', y= 'view_coef', c= 'blue', ax=ax1)\n    data_df.plot.scatter(x ='Trending Duration', y= 'likes_coef', c= 'orange', ax=ax2)\n    data_df.plot.scatter(x ='Trending Duration', y= 'dislikes_coef', c= 'orange', ax=ax3)\n    data_df.plot.scatter(x ='Trending Duration', y= 'comment_count_coef', c= 'orange', ax=ax4)\n    fig.savefig(norm_name + '_lin_reg')\n    \n# df_lin_mmnorm = lin_reg_helper(df_mmnorm,'lin_mmnorm')\n# df_lin_mnorm = lin_reg_helper(df_mnorm,'lin_mnorm')\n# df_lin_std = lin_reg_helper(df_std,'lin_std')","fdd17dba":"from sklearn.preprocessing import PolynomialFeatures\n\n###Testing with Polynomial Regression\ndef poly_reg_helper(df,norm_name,deg):\n    data = []\n    for vid_id,rows in df:\n    #     print(\"vid_id: \" + vid_id)\n        rel_rows = rows.drop(['video_id'],axis=1).drop(['tag_counts'],axis=1)\n        days_trended = len(rel_rows)\n\n        if days_trended >= 5:\n            Xs = rel_rows\n            y = rows['tag_counts'].values.reshape(-1,1)\n            \n            polynomial_features= PolynomialFeatures(degree=deg)\n            x_poly = polynomial_features.fit_transform(Xs)\n\n            lin_reg = LinearRegression()\n            lin_reg.fit(x_poly, y)\n            view_coef = lin_reg.coef_[0][0]\n            likes_coef = lin_reg.coef_[0][1]\n            dislikes_coef = lin_reg.coef_[0][2]\n            comment_count_coef = lin_reg.coef_[0][3]\n            data.append([vid_id, days_trended, view_coef,likes_coef,dislikes_coef,comment_count_coef])\n    data_df = pd.DataFrame(data, columns = ['Video ID', 'Trending Duration','view_coef','likes_coef','dislikes_coef','comment_count_coef']) \n\n    fig = plt.figure()\n    plt.title(norm_name + ' Polynomial - ' + str(deg))\n    # Divide the figure into a 2x1 grid, and give me the first section\n    ax1 = fig.add_subplot(221)\n    ax2 = fig.add_subplot(222)\n    ax3 = fig.add_subplot(223)\n    ax4 = fig.add_subplot(224)\n    data_df.plot.scatter(x ='Trending Duration', y= 'view_coef', c= 'blue', ax=ax1)\n    data_df.plot.scatter(x ='Trending Duration', y= 'likes_coef', c= 'orange', ax=ax2)\n    data_df.plot.scatter(x ='Trending Duration', y= 'dislikes_coef', c= 'orange', ax=ax3)\n    data_df.plot.scatter(x ='Trending Duration', y= 'comment_count_coef', c= 'orange', ax=ax4)\n    fig.savefig(norm_name + '_poly_reg' + str(deg))\n    return fig\n    \n# df_lin_mmnorm = poly_reg_helper(df_mmnorm,'mmnorm',2)\n# df_lin_mnorm = poly_reg_helper(df_mnorm,'mnorm',2)\n# df_lin_std = poly_reg_helper(df_std,'std',2)\n# df_lin_mmnorm = poly_reg_helper(df_mmnorm,'mmnorm',3)\n# df_lin_mnorm = poly_reg_helper(df_mnorm,'mnorm',3)\n# df_lin_std = poly_reg_helper(df_std,'std',3)\n# df_lin_mmnorm = poly_reg_helper(df_mmnorm,'mmnorm',4)\n# df_lin_mnorm = poly_reg_helper(df_mnorm,'mnorm',4)\n# df_lin_std = poly_reg_helper(df_std,'std',4)","74e21f4b":"import warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings(action='ignore',category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore',category=FutureWarning)\n\n###Testing with Ridge Regression\ndef ridge_helper(df,norm_name):\n    data = []\n    for vid_id,rows in df:\n        rel_rows = rows.drop(['video_id'],axis=1).drop(['tag_counts'],axis=1)\n        days_trended = len(rows)\n        if days_trended >= 5:\n            Xs = rel_rows\n            y = rows['tag_counts'].values.reshape(-1,1)\n            alpha = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n            ridge = Ridge()\n            parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3]}\n            ridge_regressor = GridSearchCV(ridge, parameters,scoring='neg_mean_squared_error', cv=2)\n            ridge_regressor.fit(Xs, y)\n            print(ridge_regressor.coef_)\n            data.append([vid_id, days_trended, ridge_regressor.best_params_['alpha'], ridge_regressor.best_score_])\n    data_df = pd.DataFrame(data, columns = ['Video ID', 'Trending Duration','intercept','coef']) \n\n    fig = plt.figure()\n    plt.title(norm_name + ' Ridge Regression')\n    # Divide the figure into a 2x1 grid, and give me the first section\n    ax1 = fig.add_subplot(221)\n    ax2 = fig.add_subplot(222)\n    data_df.plot.scatter(x = 'Trending Duration', y = 'intercept', c= 'blue', ax = ax1)\n    data_df.plot.scatter(x = 'Trending Duration', y = 'coef', c= 'orange',ax = ax2)\n    fig.savefig(norm_name + '_ridge_reg')\n\n# df_rid_mmnorm = ridge_helper(df_mmnorm,'lin_mmnorm')\n# df_rid_mnorm = ridge_helper(df_mnorm,'lin_mnorm')\n# df_rid_std = ridge_helper(df_std,'lin_std')","aa918018":"#In the code below, we create a regression model based on Tags & Trending Duration\n\nUSvideos['tag_counts'] = USvideos['tags'].str.split(\"|\").apply(len)\nrel_col = ['video_id','views','tag_counts','tags','title']\ndf = USvideos[rel_col].groupby(['video_id'])\ndata = [] \n\n# Editing the data such that we are able to loop through each tag in each column\nfor vid_id,rows in df:\n    days_trended = len(rows)\n    data.append([vid_id,rows['title'].values[0], days_trended,rows['tags'].values[0].split(\"|\"),rows['tag_counts'].values[0]])\n\n# Creating new dataframe with the following columns\ndf = pd.DataFrame(data, columns = ['Video ID', 'Title','Trending Duration','Tags','Tag Counts'])\n\ndic_freq = {}\ndic_trendsum = {}\ndic_trendsum_weighted = {}\n# Looping through the rows\nfor index, row in df.iterrows():\n    # Looping through each tag\n    for item in row['Tags']:\n        if item in dic_freq:\n            dic_freq[item] = dic_freq[item] + 1\n        else:\n            dic_freq[item] = 1\n\n        if item in dic_trendsum:\n            dic_trendsum[item] = dic_trendsum[item] + (row['Trending Duration'] \/ row['Tag Counts'])\n#             dic_trendsum[item] = dic_trendsum[item] + (row['Trending Duration'])\n        else:\n#             dic_trendsum[item] = (row['Trending Duration'] \/ row['Tag Counts'])\n            dic_trendsum[item] = (row['Trending Duration'])\n    \n        if item in dic_trendsum_weighted:\n            dic_trendsum_weighted[item] = dic_trendsum[item] + (row['Trending Duration'])\n        else:\n            dic_trendsum_weighted[item] = (row['Trending Duration'] \/ row['Tag Counts'])\n            \ntag_df = pd.DataFrame([dic_freq]).T\ntag_df.columns = ['word_freq']\n\ntag_trendsum = pd.DataFrame([dic_trendsum]).T\ntag_trendsum.columns = ['trendsum']\ntag_df = tag_df.join(tag_trendsum)\ntag_df['trendmean'] = tag_df['trendsum'] \/ tag_df['word_freq']\n\ntag_trendsum_weighted = pd.DataFrame([dic_trendsum_weighted]).T\ntag_trendsum_weighted.columns = ['trendsum_weight']\ntag_df = tag_df.join(tag_trendsum_weighted)\ntag_df['trendmean_weight'] = tag_df['trendsum_weight'] \/ tag_df['word_freq']\n","f8f7b6e8":"tag_df = tag_df[tag_df['word_freq'] > 5]\n# # tag_df = tag_df.sort_values(by=['trendsum'],ascending=False)\ntag_df.plot.scatter(x ='word_freq', y= 'trendmean', c= 'blue')\n# ax.set_xlim(0,50)\nplt.show()","1a94e743":"def cal_tag_val(tag_lst,metric):\n    dicc = tag_df[metric].T.to_dict()\n    return sum(list(filter(None,map(dicc.get,tag_lst))))\n    #     return sum(list(map(dicc,tag_lst)))\n\ndf['tag_trend_val1'] = df['Tags'].apply(cal_tag_val, metric = 'trendsum')\ndf['tag_trend_val2'] = df['Tags'].apply(cal_tag_val, metric = 'trendmean')\ndf['tag_trend_val3'] = df['Tags'].apply(cal_tag_val, metric = 'trendsum_weight')\ndf['tag_trend_val4'] = df['Tags'].apply(cal_tag_val, metric = 'trendmean_weight')\nprint(df.sort_values(by=['tag_trend_val1'],ascending=False).head(10))\nprint(df.sort_values(by=['tag_trend_val2'],ascending=False).head(10))\nprint(df.sort_values(by=['tag_trend_val3'],ascending=False).head(10))\nprint(df.sort_values(by=['tag_trend_val4'],ascending=False).head(10))","c5380d25":"from sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\ndf = pd.DataFrame(df)\ndef metric_testing(lst,dura):\n    score_lst = []\n    for metric in lst:\n        X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(df[metric]), pd.DataFrame(df['Trending Duration'] >= dura), test_size = 0.2)\n        knn = KNeighborsClassifier(n_neighbors=5)\n        knn.fit(X_train,y_train)\n        # Print the accuracy\n        score_lst.append(knn.score(X_test, y_test))\n    return score_lst\n\nprint(metric_testing(['tag_trend_val1','tag_trend_val2','tag_trend_val3','tag_trend_val4'],2))\nprint(metric_testing(['tag_trend_val1','tag_trend_val2','tag_trend_val3','tag_trend_val4'],3))\nprint(metric_testing(['tag_trend_val1','tag_trend_val2','tag_trend_val3','tag_trend_val4'],4))\nprint(metric_testing(['tag_trend_val1','tag_trend_val2','tag_trend_val3','tag_trend_val4'],5))\nprint(metric_testing(['tag_trend_val1','tag_trend_val2','tag_trend_val3','tag_trend_val4'],6))\nprint(metric_testing(['tag_trend_val1','tag_trend_val2','tag_trend_val3','tag_trend_val4'],7))\nprint(metric_testing(['tag_trend_val1','tag_trend_val2','tag_trend_val3','tag_trend_val4'],8))","043f019f":"<a id=\"methodology_3_5\"> <\/a>\n\n### 3.3.5 Ridge Regression\n\nSince our analysis contains multiple relevant columns, a straightforward linear regression might not be sufficient to detect or show a relationship between the variables. With this, Ridge Regression might possibly serves as better algorithm for linear relationships between multiple variable.Ridge Regression is a technique for analyzing multiple regression data and understanding the multicollinearity between various variables. Multicollinearity  is the existence of near-linear relationships among the independent variables. ","e44072bb":"Title of project: Tags Team (ITE351 Artificial Intelligence project in Hanyang during Fall 2019)\nMembers:  \nDerek Lim Wei Liang,9336820199 ,School of Business, SMU, derek.lim.2017@business.smu.edu.sg  \nChang Bo Lam,9281620197 ,School of Business, FHNW, changbo.lam@students.fhnw.ch  \nTee Zhi Yao,9308120192 ,Department of Computer Science, SUTD, teezhiyao@gmail.com  \n\n[Video](https:\/\/smu-my.sharepoint.com\/personal\/derek_lim_2017_business_smu_edu_sg\/_layouts\/15\/onedrive.aspx?id=%2Fpersonal%2Fderek%5Flim%5F2017%5Fbusiness%5Fsmu%5Fedu%5Fsg%2FDocuments%2FSMU%2FSMU%20Y3S1%5FHanyang%20University%2F1%5FCore%20Syllabus%2FITE3051%20Artificial%20Intelligence%20and%20Application%2FAssignment%2FAssignment%204%2FITE%203051%20Artificial%20Intelligence%20Project%20%2D%20Tags%20Tube%2Emp4&parent=%2Fpersonal%2Fderek%5Flim%5F2017%5Fbusiness%5Fsmu%5Fedu%5Fsg%2FDocuments%2FSMU%2FSMU%20Y3S1%5FHanyang%20University%2F1%5FCore%20Syllabus%2FITE3051%20Artificial%20Intelligence%20and%20Application%2FAssignment%2FAssignment%204&originalPath=aHR0cHM6Ly9zbXUtbXkuc2hhcmVwb2ludC5jb20vOnY6L2cvcGVyc29uYWwvZGVyZWtfbGltXzIwMTdfYnVzaW5lc3Nfc211X2VkdV9zZy9FWVhVMDFPeVRDZEl2ekkyWHdwZlE5SUJyM1VnY0FOR0pPV3VQMmhZSzY4TUN3P3J0aW1lPU9YMDAwNDFfMTBn)  \n\n-  [1. Introduction](#introduction)  \n   - [Motivation](#motivation) \n-  [2. Datasets](#datasets) \n   - [2.1 About the dataset](#datasets_1)\n-  [3. Methodology](#methodology) \n   - [3.1 \"Video Tags\" as an Input Variable](#methodology_1)\n   - [3.2 Understanding 'Video Performance'](#methodology_2)\n   - [3.3 Quantitative Analysis - Tag Count Analysis](#methodology_3)  \n       - [3.3.1 Data cleaning](#methodology_3_1)  \n       - [3.3.2 Data normalization\/processing](#methodology_3_2)  \n       - [3.3.3 Linear Regression](#methodology_3_3)  \n       - [3.3.4 Polynomial Regression](#methodology_3_4)  \n       - [3.3.5 Ridge Regression](#methodology_3_5)  \n   - [3.4 Quantitative evaluation](#quantitative_eval)  \n   - [3.5 Qualitative Analysis - Tag Content Analysis](#methodology_4)  \n   - [3.6 Qualitative Evaluation](#qualitative_eval)\n-  [4. Related Work](#related_work)\n    - [4.1 Libraries](#libraries)\n    - [4.2 Tools](#tools)\n    - [4.3 Documentation](#documentation)\n-  [5. Conclusion: Discussion](#conclusion)","e46f618f":"<a id=\"methodology\"> <\/a>\n\n## 3. Methodology  \n<a id=\"methodology_1\"> <\/a>\n### 3.1 \"Video Tags\" as an Input Variable\n\nWe will be analysing both the qualitative and quantitative aspects of \"Video Tags\" for each video.\n\nFor Quantitative:\nWe will be looking at the gross total number of video tags attached to each video.\nIn the dataset we used, the tag is given as a long list of string. To calculate the number of tags, we processed the string by splitting the string into a list using '|'. We then calculate the length of the list using the \"len\" function. \n\nFor Qualitative: \nWe will be looking at the contents of video tags attached to each video.\nIn the dataset used, we compiled a set of all video tags.\nThe number of occurences of each video tag across different trending videos is calculated, as an indicator of tag popularity.\nA metric is subsequently created to calculate the significance of individual tags. \nThe value is then used with the original dataset to give each video a credit value  \n\n<a id=\"methodology_2\"> <\/a>\n### 3.2 Understanding \"Video Performance\"\n\nThere are many different ways to measure a video's performance.  \nIn our study, we will look into video performance based on 2 key aspects.\nAspect 1: Popularity\nAspect 2: Engagement\n\nWe have identified the following columns (Without modification) in our dataset that we will use as inputs for our analysis. \n- views\n- likes\n- dislikes\n- comments\n- trending_date  \n\nThe data above will be further processed via our algorithm into these statistics.\n\nDirect indicator of Aspect 1: Popularity\n- (Number of) likes\n- (Number of) dislikes \n- comment(_count)\n\nDirect indicator of Aspect 2: Engagement\n- (Number of) views \n- trending duration = (current date - trending_date)\n\n<a id=\"methodology_3\"> <\/a>\n### 3.3 Quantitative Analysis - Tag Count Analysis\n\n\nBelow are the steps we will take to process our input data for the analysis with tag counts\n\n- Step 1 - Approriate data cleaning & common manipulation\n- Step 2 - Data normalization\/processing\n- Step 3 - Create a regression for each video which trended for at least 3 days.  \n    - Step 3.1 - We attempted multiple form of regressions such as Linear, Polynomial & Ridge regression  \n- Step 4 - The parameters for the regressions is then extracted and further analysis is done.  \n\n","cd5e25ba":"<a id=\"methodology_4\"> <\/a>\n\n### 3.4 Method\/Algorithm 2\nIn the follow segments below, we proceed to analyse the tag contents. The following are the steps taken during the analysis\n\n- Step 1 - Calculating Word Frequency & Metric for tags\n- Step 2 - Calculating tag significance \n- Step 3 - Classifier Testing \/ Modelling\n\n### 3.4.1 Calculating Word Frequency & Metric for tags\nBy looking at the tags from each individual video, a table containing the occurence of each tag is created.\n\n| Columns Header         | Description |\n| --------------         | -------------- |\n| Tag Name               | Name of the tag |\n| Word_freq              | How many times a tag appears |\n| trendsum               | The sum of the trending days of each video which contains the tags |\n| trendmean              | Average trending days of each video, obtained by trendsum\/word_freq |\n| trendsum_weight        | Value of tag in video is weighted based on tag counts, a video tag will be more significant when total tag belonging to that video is lesser  |\n| trendmean_weight        | trendsum_weight\/word_freq  |\n","97bdf537":"###### <a id=\"related_work\"> <\/a>\n## 4. Related Work\nHeckner's paper discussed how tags are used across different social media platforms. They stipulated that tags on Youtube are primarily used to reach a wider audience and not to sort and categorize a collection of personal videos. Some user seem to copy the video title into the tag section. Youtube seperates tages based on spaces between words. This results in a disproportionate amount of tags that consist of single words with seemingly no connection to the video itself. The way in which this relates to our own project is that we sought to discern whether tags had an influence on a video's performance.\n \nHeckner, M., Neubauer, T., & Wolff, C. (2008). Tree, funny, to_read, google: What are Tags Supposed to Achieve?. University of Regensburg. Retrieved from https:\/\/epub.uni-regensburg.de\/6760\/1\/ssm-heckner.pdf","66433c10":"### 3.4.2 Calculating tag significance \n","fca41576":"<a id=\"qualitative_eval\"> <\/a>\n### 4.2 Qualitative evaluation \n\nUsing a KNN classifier alongside with the 4 different metric which we have come up with. We created and tested a model by varying the number of trending day in the duration. The first row of result shows the prediction result for a video trending for at least 2 days. The subsequent rows shows the prediction for different trending duration.  \n\n#### Observations\nDue to the nature of using a classifier, random guessing would give an accuracy of 50% (0.5). We see that the accuracy drops to a minimum  of around 50% when using prediction for at least 6 trending day. This tells us that trying to classifier if a video will trend at least 5-7 trending days is the hardest as the accuracy becomes close to random guessing.  \n\nAnother observation is that as the trending duration increases, the prediction accuracy tend to 100%. This could be the case of predicting all videos to be less than the trending duration and having a high accuracy due to the fact that majority of the videos not having a trending duration which can be that high.\n[](http:\/\/)\nBetween the classifier,we were unable to observe significant difference between the different metrics which we created. Because of the consistency between the different metric, it could mean that tag content does not hold much relationship with video performance\/trending days. However, this could also be due to underlying issues with all the metrics (Which we were unable to identify) or issues in terms of the classifier or how we processed the data.\n\n|Trend Duration|trendsum|trendmean|trendsum_weight|trendmean_weight |\n|---------|--------|--------|--------|--------|\n|>= 2 Trending Day|0.87740 | 0.87259 | 0.86618 | 0.88782|\n|>= 3 Trending Day|0.78445 | 0.76201 | 0.77964 | 0.80528|\n|>= 4 Trending Day|0.64983 | 0.67868 | 0.69871 | 0.63381|\n|>= 5 Trending Day|0.58653 | 0.58814 | 0.56089 | 0.61618|\n|>= 6 Trending Day|0.51842 | 0.51762 | 0.51762 | 0.51602|\n|>= 7 Trending Day|0.52403 | 0.55208 | 0.55208 | 0.56971|\n|>= 8 Trending Day|0.63621 | 0.64182 | 0.68509 | 0.64903|\n\n","86ea9f95":"<a id=\"methodology_3_1\"> <\/a>\n### 3.3.1 Data cleaning  \nBefore the actual algorithm, we need to sanitize our input data.\n\nFirst off, videos that are not useful for our case are removed from the sample. This includes videos with errors, where comments and\/or ratings are disabled and duplicate videos.\n\nSubsequently, a new column is created. The summation function is used to calculate the number of tags for the remaining videos. (The original dataset only consists of the tags and does not provide the tag count)","88028ef4":"<a id=\"datasets\"> <\/a>  \n## 2. Datasets \n<a id=\"datasets_1\"> <\/a>  \n### 2.1 About the Data  \nDataset used: [Trending YouTube Video Statistics](https:\/\/www.kaggle.com\/datasnaek\/youtube-new#USvideos.csv)      \n\nThe dataset consists of daily records of the top 200 trending YouTube videos, ordered by country. \nFor the purpose of this analysis, we will be focusing on videos originating from the US,\nfor the time period between 14 November 2017 and 14 June 2018, a total of 205 days.\n    \nDescription of relevant columns from dataset are shown in the table below. \n\n| Columns Header         | Quantity | Type   | Description |\n| --------------         | -------- | ------ | ----------- |\n| video_id               | 40949    | object | Unique Value for each video |\n| trending_date          | 40949    | object | Date in which video is trending. Video can trend for over a day. | \n| title                  | 40949    | object | Video's title |\n| channel_title          | 40949    | object | Name of uploading channel |\n| category_id            | 40949    | int64  | Video category |\n| publish_time           | 40949    | object | Time of upload |\n| tags                   | 40949    | object | Tags associated with the video |\n| views                  | 40949    | int64  | Number of views |\n| likes                  | 40949    | int64  | Number of likes |\n| dislikes               | 40949    | int64  | Number of dislikes |\n| comment_count          | 40949    | int64  | Number of comments |\n| comments_disabled      | 40949    | bool   | Comment section enabled or not |\n| ratings_disabled       | 40949    | bool   | Like\/Dislike ratio shown |\n| video_error_or_removed | 40949    | bool   | Was the video deleted or not |\n| description            | 40379    | object | Video description |\n  \n<a id=\"datasets_2\"> <\/a>  ","bd90f9f0":"<a id=\"conclusion\"> <\/a>\n## 5. Conclusion  \nContrary to our initial assumptions, there was little to no correlation between the number of tags a video had and how the video performed in terms of views, likes, dislikes and number of comments. Despite using three different normalisation methods (Min-max normalization, Mean normalization, Standardization), neither linear, polynomial nor ridge regression showed any signs of correlation.\nThe qualitative research had seemingly better results, with an accuracy of approximately 50%. But this holds little meaning due to the binary nature of the problem's solution.\n\nFuture research could look for patterns in the way tags are combined, using neural networks. Another possibility would be to further describe tags and try to find similarities between tags of well-performing videos.","fc34e572":" <a id=\"quantitative_eval\"> <\/a>\n### 3.4 Quantitative evaluation\n\nAs attached below is the graphs which we obtain from using different regressions to analyse if the coefficient from the various regression shows an indication between tag count and the various relevant columns. We varied the both the Regression & Normalization method to give us the 15 figures below (5 regression, 3 normalization method). In each individual figure, the 4 plots shows the relationship between the coefficient obtained from a regression and the trending duration. This coefficient is extracted from a graph which plots tag counts and the relevant columns (Likes,dislikes,views, comment_counts).  \n\nThe main observation that we can see from the graphs below is that majority of the coefficients are zero regardless of the regression or normalization method. We can conclude that using merely the tag counts is insufficient for us to really predict nor see any sort of relationship with video popularity. \n\nAnother observation when comparing between the different normalization methods, we can see that there is no significant difference between the min-max normalization and mean normalization. Another interesting observation is that using standard deviation as a form of normalization seems to identify more outliers as compared to the other forms of normalization. \n\nLinear Regression  \n\n![lin_std_lin_reg.png](attachment:lin_std_lin_reg.png)![lin_mnorm_lin_reg.png](attachment:lin_mnorm_lin_reg.png)![lin_mmnorm_lin_reg.png](attachment:lin_mmnorm_lin_reg.png)\n\nRidge Regression\n![lin_mmnorm_ridge_reg.png](attachment:lin_mmnorm_ridge_reg.png)![lin_mnorm_ridge_reg.png](attachment:lin_mnorm_ridge_reg.png)![lin_std_ridge_reg.png](attachment:lin_std_ridge_reg.png)\n\nPolynomial - Quadratic\n![mmnorm_poly_reg2.png](attachment:mmnorm_poly_reg2.png)\n![mnorm_poly_reg2.png](attachment:mnorm_poly_reg2.png)\n![std_poly_reg2.png](attachment:std_poly_reg2.png)\n\nPolynomial - 3 degree\n![mmnorm_poly_reg3.png](attachment:mmnorm_poly_reg3.png)![mnorm_poly_reg3.png](attachment:mnorm_poly_reg3.png)![std_poly_reg3.png](attachment:std_poly_reg3.png)\n\nPolynomial - 4 degree \n![mmnorm_poly_reg4.png](attachment:mmnorm_poly_reg4.png)![mnorm_poly_reg4.png](attachment:mnorm_poly_reg4.png)![std_poly_reg4.png](attachment:std_poly_reg4.png)\n","0c8a8ee8":"<a id=\"libraries\"> <\/a>\n## 4.1 Libraries\n- **numpy**: Scientific calculations\n- **sklearn**: Regressions\n- **pandas**: Working with the dataset\n- **matplotlib**: Plotting, graphs\n\n\n","0fcb0a49":"<a id=\"methodology_3_3\"> <\/a>\n\n### 3.3.3 Linear Regression\n\nThe algorithm continues to perform linear regression.\n\nIn this step, relationships between input and output data are established\nBy plotting our dataset on a linear regression line, we are able to explore how the change of input X affects output Y, scaled to a coefficient value.\n\nHere, trending duration is first calculated, by doing a subtraction of the current date and time, to the first trending date. Trending Duration is subsequently used as an input variable X, equated to 4 different Y variables (view_counts, likes_counts, dislikes_counts, comment_counts). Through the line of best fit via Least Square Regression, we are able to obtain a coefficient for each variable.\n\nEach variable's coefficent will give us an indication of the direction and significance of the relationship.\nR-Square value gives us the indication of the strength of the model in explaining the variation in the data.\n\n","3e5f33b1":"<a id=\"introduction\"> <\/a>\n    \n## 1. Introduction  \n<a id=\"motivation\"> <\/a>\n### Motivation  \nYoutube is the world\u2019s largest video platform with millions of concurrent users everyday and vast influence on customer behaviour, beliefs and opinions. Owing to that, maximizing video performance has gained tangible economic value and many startups use it to gain traction and garner interest in their products and services. \n\nIn this study, we aim to create a model to analyse the effects of video tags in a video's performance.\nWe will explore both the quantitative and qualitative features of a video tags, comparing them to the video's overall performance through a series of metrics such as views counts, like\/dislike ratio and number of days trending.","23cd7394":"<a id=\"tools\"> <\/a>\n## 4.2 Tools\n- Jupyter Notebook\n- PyCharm\n- Kaggle\n- Visual Studio Code\n\nAll the coding was done using PyCharm, due to its ease of installing new Python modules and due to its availability and popularity.\nWorking on a single Jupyter Notebook via Kaggle was not a good experience in our eyes, conflicts with multiple concurrent contributers were frequent. We solved this problem by working offline and by not opening and committing multiple kernels at the same time. The offline changes were made using Visual Studio Code and the Jupyter Notebook application. In hindsight we should have used Github for our project.","83cb3c85":"<a id=\"methodology_3_2\"> <\/a>\n### 3.3.2 Data normalization\/processing\n\nThe algorithm continues to perform data normalisation.\n\nData normalisation is important to allow us to compare across different metrics, especially when metrics are each measured on a different scale or units.\n\nHere, we explored 3 different techniques of normalisation. Each technique tapps on a unique way of data scaling and differe\n\n- Min-max normalization\n\nAllows us to explore the data spread, between minimum and maximum datapoint\n- Mean normalization\n\nAllows us to explore the averages of the data, scaled down to the same scale.\n\n- Standardization \n\nBasic scaling of absolute data to the same scale.\n","a0e05ef3":"<a id=\"documentation\"> <\/a>\n## 4.3 Documentation\nThe Python Tutorial \u2014 Python 3.8.0 documentation. (2019). Retrieved 6 December 2019, from https:\/\/docs.python.org\/3\/tutorial\/\n\nOverview \u2014 Matplotlib 3.1.1 documentation. (2019). Retrieved 1 December 2019, from https:\/\/matplotlib.org\/contents.html\n\npandas: powerful Python data analysis toolkit \u2014 pandas 0.25.3 documentation. (2019). Retrieved 2 December 2019, from https:\/\/pandas.pydata.org\/pandas-docs\/stable\/\n\nQuickstart tutorial \u2014 NumPy v1.19.dev0 Manual. (2019). Retrieved 3 December 2019, from https:\/\/numpy.org\/devdocs\/user\/quickstart.html\n\nscikit-learn Tutorials \u2014 scikit-learn 0.22 documentation. (2019). Retrieved 6 December 2019, from https:\/\/scikit-learn.org\/stable\/tutorial\/index.html\n\nNCSS Statistical Software - Ridge Regression. (2019). Retrieved 2 December 2019, from https:\/\/ncss-wpengine.netdna-ssl.com\/wp-content\/themes\/ncss\/pdf\/Procedures\/NCSS\/Ridge_Regression.pdf\n","9a511a14":"<a id=\"methodology_3_4\"> <\/a>\n### 3.3.4 Polynomial Regression"}}