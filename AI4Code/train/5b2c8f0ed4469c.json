{"cell_type":{"280b7b42":"code","6f86acb9":"code","cbc294c8":"code","b5a3c410":"code","9d3ded4f":"code","4c7faae9":"code","3bcdfd29":"code","3674a2d0":"markdown","5df40803":"markdown","42cf0c2e":"markdown","6942260a":"markdown","423e860d":"markdown"},"source":{"280b7b42":"import pandas as pd #pandas - for data manipulation\nnew_data = pd.read_csv('\/kaggle\/input\/wildfire-satellite-data\/fire_nrt_M6_156000.csv') #load new data (June 2020->present)\nold_data = pd.read_csv('\/kaggle\/input\/wildfire-satellite-data\/fire_archive_M6_156000.csv') #load old data (Sep 2010->June 2020)\ndata = pd.concat([old_data.drop('type',axis=1), new_data]) #concatenate old and new data\ndata = data.reset_index().drop('index',axis=1)\ndata['satellite'] = data['satellite'].map({'Terra':0,'Aqua':1})\ndata['daynight'] = data['daynight'].map({'D':0,'N':1})\ndata.drop('instrument', axis=1, inplace=True)\ndata['month'] = data['acq_date'].apply(lambda x:int(x.split('-')[1]))\ndata = data.sample(frac=0.2)\ndata = data.reset_index().drop(\"index\", axis=1)\ndata.head()","6f86acb9":"X = data[['latitude','longitude','month','brightness','scan','track','acq_time','bright_t31','daynight']]\ny = data['frp']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","cbc294c8":"# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.metrics import mean_absolute_error as mae\n# model = RandomForestRegressor(n_estimators = 150, max_depth = 15)\n# model.fit(X_train, y_train)\n# mae(model.predict(X_train), y_train)\n# print(f\"Train MAE: {mae(model.predict(X_train), y_train)}\")\n# print(f\"Test MAE: {mae(model.predict(X_test), y_test)}\")","b5a3c410":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error as mae\nmodel1 = GradientBoostingRegressor(n_estimators = 100, learning_rate=0.1,\n                                  max_depth = 10, random_state = 0, loss = 'ls')\nmodel1.fit(X_train, y_train)\nprint(f\"Train MAE: {mae(model1.predict(X_train), y_train)}\")\nprint(f\"Test MAE: {mae(model1.predict(X_test), y_test)}\")","9d3ded4f":"import shap\nexplainer = shap.TreeExplainer(model1)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test)#, plot_type=\"bar\")","4c7faae9":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(model1, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","3bcdfd29":"from pdpbox import pdp, info_plots\nimport matplotlib.pyplot as plt\nbase_features = X.columns.values.tolist()\nfor column in X.columns:\n    feat_name = column\n    pdp_dist = pdp.pdp_isolate(model=model1, dataset=X_test, model_features=base_features, feature=feat_name)\n    pdp.pdp_plot(pdp_dist, feat_name)\n    plt.show()","3674a2d0":"# Random Forest Approach","5df40803":"### ShAP Values\nHow important each feature is.","42cf0c2e":"## Data Loading and Preparation","6942260a":"# Gradient Boosting Approach","423e860d":"## Explainability"}}