{"cell_type":{"58256c3f":"code","3795d781":"code","551a1ab9":"code","f5397880":"code","096bcdc6":"code","85a484e6":"code","d2d5d9b5":"code","12006c01":"code","5fa089e6":"code","93d78e86":"code","125a8424":"code","f315ad16":"code","de61cbf6":"code","4eb26b73":"code","e4535532":"code","00f99a68":"code","7bc01d11":"code","36abd955":"code","01e2b20f":"markdown"},"source":{"58256c3f":"import tensorflow as tf\nimport numpy as np\nimport unicodedata\nimport re\nimport os\nimport time\n\nMODE = 'train'\nNUM_EPOCHS = 100\nNUMBER_OF_DATASET = 1000\nBATCH_SIZE = 64","3795d781":"def read_dataset(number):\n\n    english_data = []\n    with open('..\/input\/jw300entw\/jw300.en-tw.en') as file:\n\n        line = file.readline()\n        cnt = 1\n        while line:\n            english_data.append(line.strip())\n            line = file.readline()\n            cnt += 1\n\n\n    twi_data = []\n    with open('..\/input\/jw300entw\/jw300.en-tw.tw') as file:\n\n        # twi=file.read()\n        line = file.readline()\n        cnt = 1\n        while line:\n            twi_data.append(line.strip())\n            line = file.readline()\n            cnt += 1\n\n    return english_data[:number],twi_data[:number]\n    # return english_data,twi_data\u00f7","551a1ab9":"def unicode_to_ascii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn')\n\n\ndef normalize_eng(s):\n    s = unicode_to_ascii(s)\n    s = re.sub(r'([!.?])', r' \\1', s)\n    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n    s = re.sub(r'\\s+', r' ', s)\n    return s\n\ndef normalize_twi(s):\n    s = unicode_to_ascii(s)\n    s = re.sub(r'([!.?])', r' \\1', s)\n    s = re.sub(r'[^a-zA-Z.\u0186\u0254\u025b\u0190!?\u2019]+', r' ', s)\n    s = re.sub(r'\\s+', r' ', s)\n    return s\n","f5397880":"raw_data_en,raw_data_twi = read_dataset(NUMBER_OF_DATASET)\n# raw_data_en, raw_data_twi = list(zip(*raw_data))\nraw_data_en = [normalize_eng(data) for data in raw_data_en]\nraw_data_twi_in = ['<start> ' + normalize_twi(data) for data in raw_data_twi]\nraw_data_twi_out = [normalize_twi(data) + ' <end>' for data in raw_data_twi]\n\nprint(len(raw_data_twi_in))","096bcdc6":"en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\nen_tokenizer.fit_on_texts(raw_data_en)\ndata_en = en_tokenizer.texts_to_sequences(raw_data_en)\ndata_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,\n                                                        padding='post')\n\ntwi_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\ntwi_tokenizer.fit_on_texts(raw_data_twi_in)\ntwi_tokenizer.fit_on_texts(raw_data_twi_out)\ndata_twi_in = twi_tokenizer.texts_to_sequences(raw_data_twi_in)\ndata_twi_in = tf.keras.preprocessing.sequence.pad_sequences(data_twi_in,\n                                                           padding='post')\n\ndata_twi_out = twi_tokenizer.texts_to_sequences(raw_data_twi_out)\ndata_twi_out = tf.keras.preprocessing.sequence.pad_sequences(data_twi_out,\n                                                            padding='post')","85a484e6":"BATCH_SIZE = 64\ndataset = tf.data.Dataset.from_tensor_slices(\n    (data_en, data_twi_in, data_twi_out))\ndataset = dataset.shuffle(len(data_en)).batch(BATCH_SIZE)\nprint(dataset)","d2d5d9b5":"\"\"\"## Create the Positional Embedding\"\"\"\n\n\ndef positional_encoding(pos, model_size):\n    \"\"\" Compute positional encoding for a particular position\n\n    Args:\n        pos: position of a token in the sequence\n        model_size: depth size of the model\n    \n    Returns:\n        The positional encoding for the given token\n    \"\"\"\n    PE = np.zeros((1, model_size))\n    for i in range(model_size):\n        if i % 2 == 0:\n            PE[:, i] = np.sin(pos \/ 10000 ** (i \/ model_size))\n        else:\n            PE[:, i] = np.cos(pos \/ 10000 ** ((i - 1) \/ model_size))\n    return PE\n","12006c01":"max_length = max(len(data_en[0]), len(data_twi_in[0]))\nMODEL_SIZE = 128\n\npes = []\nfor i in range(max_length):\n    pes.append(positional_encoding(i, MODEL_SIZE))\n\npes = np.concatenate(pes, axis=0)\npes = tf.constant(pes, dtype=tf.float32)\n\n\nprint(pes.shape)\nprint(data_en.shape)\nprint(data_twi_in.shape)","5fa089e6":"\"\"\"## Create the Multihead Attention layer\"\"\"\n\n\nclass MultiHeadAttention(tf.keras.Model):\n    \"\"\" Class for Multi-Head Attention layer\n\n    Attributes:\n        key_size: d_key in the paper\n        h: number of attention heads\n        wq: the Linear layer for Q\n        wk: the Linear layer for K\n        wv: the Linear layer for V\n        wo: the Linear layer for the output\n    \"\"\"\n    def __init__(self, model_size, h):\n        super(MultiHeadAttention, self).__init__()\n        self.key_size = model_size \/\/ h\n        self.h = h\n        self.wq = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n        self.wk = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n        self.wv = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(value_size) for _ in range(h)]\n        self.wo = tf.keras.layers.Dense(model_size)\n\n    def call(self, query, value, mask=None):\n        \"\"\" The forward pass for Multi-Head Attention layer\n\n        Args:\n            query: the Q matrix\n            value: the V matrix, acts as V and K\n            mask: mask to filter out unwanted tokens\n                  - zero mask: mask for padded tokens\n                  - right-side mask: mask to prevent attention towards tokens on the right-hand side\n        \n        Returns:\n            The concatenated context vector\n            The alignment (attention) vectors of all heads\n        \"\"\"\n        # query has shape (batch, query_len, model_size)\n        # value has shape (batch, value_len, model_size)\n        query = self.wq(query)\n        key = self.wk(value)\n        value = self.wv(value)\n        \n        # Split matrices for multi-heads attention\n        batch_size = query.shape[0]\n        \n        # Originally, query has shape (batch, query_len, model_size)\n        # We need to reshape to (batch, query_len, h, key_size)\n        query = tf.reshape(query, [batch_size, -1, self.h, self.key_size])\n        # In order to compute matmul, the dimensions must be transposed to (batch, h, query_len, key_size)\n        query = tf.transpose(query, [0, 2, 1, 3])\n        \n        # Do the same for key and value\n        key = tf.reshape(key, [batch_size, -1, self.h, self.key_size])\n        key = tf.transpose(key, [0, 2, 1, 3])\n        value = tf.reshape(value, [batch_size, -1, self.h, self.key_size])\n        value = tf.transpose(value, [0, 2, 1, 3])\n        \n        # Compute the dot score\n        # and divide the score by square root of key_size (as stated in paper)\n        # (must convert key_size to float32 otherwise an error would occur)\n        score = tf.matmul(query, key, transpose_b=True) \/ tf.math.sqrt(tf.dtypes.cast(self.key_size, dtype=tf.float32))\n        # score will have shape of (batch, h, query_len, value_len)\n        \n        # Mask out the score if a mask is provided\n        # There are two types of mask:\n        # - Padding mask (batch, 1, 1, value_len): to prevent attention being drawn to padded token (i.e. 0)\n        # - Look-left mask (batch, 1, query_len, value_len): to prevent decoder to draw attention to tokens to the right\n        if mask is not None:\n            score *= mask\n\n            # We want the masked out values to be zeros when applying softmax\n            # One way to accomplish that is assign them to a very large negative value\n            score = tf.where(tf.equal(score, 0), tf.ones_like(score) * -1e9, score)\n        \n        # Alignment vector: (batch, h, query_len, value_len)\n        alignment = tf.nn.softmax(score, axis=-1)\n        \n        # Context vector: (batch, h, query_len, key_size)\n        context = tf.matmul(alignment, value)\n        \n        # Finally, do the opposite to have a tensor of shape (batch, query_len, model_size)\n        context = tf.transpose(context, [0, 2, 1, 3])\n        context = tf.reshape(context, [batch_size, -1, self.key_size * self.h])\n        \n        # Apply one last full connected layer (WO)\n        heads = self.wo(context)\n        \n        return heads, alignment","93d78e86":"\"\"\"## Create the Encoder\"\"\"\n\n\nclass Encoder(tf.keras.Model):\n    \"\"\" Class for the Encoder\n\n    Args:\n        model_size: d_model in the paper (depth size of the model)\n        num_layers: number of layers (Multi-Head Attention + FNN)\n        h: number of attention heads\n        embedding: Embedding layer\n        embedding_dropout: Dropout layer for Embedding\n        attention: array of Multi-Head Attention layers\n        attention_dropout: array of Dropout layers for Multi-Head Attention\n        attention_norm: array of LayerNorm layers for Multi-Head Attention\n        dense_1: array of first Dense layers for FFN\n        dense_2: array of second Dense layers for FFN\n        ffn_dropout: array of Dropout layers for FFN\n        ffn_norm: array of LayerNorm layers for FFN\n    \"\"\"\n    def __init__(self, vocab_size, model_size, num_layers, h):\n        super(Encoder, self).__init__()\n        self.model_size = model_size\n        self.num_layers = num_layers\n        self.h = h\n        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n        self.attention = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n        self.attention_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n\n        self.attention_norm = [tf.keras.layers.LayerNormalization(\n            epsilon=1e-6) for _ in range(num_layers)]\n\n        self.dense_1 = [tf.keras.layers.Dense(\n            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n        self.dense_2 = [tf.keras.layers.Dense(\n            model_size) for _ in range(num_layers)]\n        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n            epsilon=1e-6) for _ in range(num_layers)]\n\n    def call(self, sequence, training=True, encoder_mask=None):\n        \"\"\" Forward pass for the Encoder\n\n        Args:\n            sequence: source input sequences\n            training: whether training or not (for Dropout)\n            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n        \n        Returns:\n            The output of the Encoder (batch_size, length, model_size)\n            The alignment (attention) vectors for all layers\n        \"\"\"\n        embed_out = self.embedding(sequence)\n\n        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n        embed_out += pes[:sequence.shape[1], :]\n        embed_out = self.embedding_dropout(embed_out)\n\n        sub_in = embed_out\n        alignments = []\n\n        for i in range(self.num_layers):\n            sub_out, alignment = self.attention[i](sub_in, sub_in, encoder_mask)\n            sub_out = self.attention_dropout[i](sub_out, training=training)\n            sub_out = sub_in + sub_out\n            sub_out = self.attention_norm[i](sub_out)\n            \n            alignments.append(alignment)\n            ffn_in = sub_out\n\n            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n            ffn_out = ffn_in + ffn_out\n            ffn_out = self.ffn_norm[i](ffn_out)\n\n            sub_in = ffn_out\n\n        return ffn_out, alignments\n\n\nH = 8\nNUM_LAYERS = 4\nvocab_size = len(en_tokenizer.word_index) + 1\nencoder = Encoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\nprint(vocab_size)\nsequence_in = tf.constant([[1, 2, 3, 0, 0]])\nencoder_output, _ = encoder(sequence_in)\nencoder_output.shape","125a8424":"class Decoder(tf.keras.Model):\n    \"\"\" Class for the Decoder\n\n    Args:\n        model_size: d_model in the paper (depth size of the model)\n        num_layers: number of layers (Multi-Head Attention + FNN)\n        h: number of attention heads\n        embedding: Embedding layer\n        embedding_dropout: Dropout layer for Embedding\n        attention_bot: array of bottom Multi-Head Attention layers (self attention)\n        attention_bot_dropout: array of Dropout layers for bottom Multi-Head Attention\n        attention_bot_norm: array of LayerNorm layers for bottom Multi-Head Attention\n        attention_mid: array of middle Multi-Head Attention layers\n        attention_mid_dropout: array of Dropout layers for middle Multi-Head Attention\n        attention_mid_norm: array of LayerNorm layers for middle Multi-Head Attention\n        dense_1: array of first Dense layers for FFN\n        dense_2: array of second Dense layers for FFN\n        ffn_dropout: array of Dropout layers for FFN\n        ffn_norm: array of LayerNorm layers for FFN\n\n        dense: Dense layer to compute final output\n    \"\"\"\n    def __init__(self, vocab_size, model_size, num_layers, h):\n        super(Decoder, self).__init__()\n        self.model_size = model_size\n        self.num_layers = num_layers\n        self.h = h\n        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n        self.attention_bot = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n        self.attention_bot_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n        self.attention_bot_norm = [tf.keras.layers.LayerNormalization(\n            epsilon=1e-6) for _ in range(num_layers)]\n        self.attention_mid = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n        self.attention_mid_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n        self.attention_mid_norm = [tf.keras.layers.LayerNormalization(\n            epsilon=1e-6) for _ in range(num_layers)]\n\n        self.dense_1 = [tf.keras.layers.Dense(\n            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n        self.dense_2 = [tf.keras.layers.Dense(\n            model_size) for _ in range(num_layers)]\n        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n            epsilon=1e-6) for _ in range(num_layers)]\n\n        self.dense = tf.keras.layers.Dense(vocab_size)\n\n    def call(self, sequence, encoder_output, training=True, encoder_mask=None):\n        \"\"\" Forward pass for the Decoder\n\n        Args:\n            sequence: source input sequences\n            encoder_output: output of the Encoder (for computing middle attention)\n            training: whether training or not (for Dropout)\n            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n        \n        Returns:\n            The output of the Encoder (batch_size, length, model_size)\n            The bottom alignment (attention) vectors for all layers\n            The middle alignment (attention) vectors for all layers\n        \"\"\"\n        # EMBEDDING AND POSITIONAL EMBEDDING\n        embed_out = self.embedding(sequence)\n\n        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n        embed_out += pes[:sequence.shape[1], :]\n        embed_out = self.embedding_dropout(embed_out)\n\n        bot_sub_in = embed_out\n        bot_alignments = []\n        mid_alignments = []\n\n        for i in range(self.num_layers):\n            # BOTTOM MULTIHEAD SUB LAYER\n            seq_len = bot_sub_in.shape[1]\n\n            if training:\n                mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n            else:\n                mask = None\n            bot_sub_out, bot_alignment = self.attention_bot[i](bot_sub_in, bot_sub_in, mask)\n            bot_sub_out = self.attention_bot_dropout[i](bot_sub_out, training=training)\n            bot_sub_out = bot_sub_in + bot_sub_out\n            bot_sub_out = self.attention_bot_norm[i](bot_sub_out)\n            \n            bot_alignments.append(bot_alignment)\n\n            # MIDDLE MULTIHEAD SUB LAYER\n            mid_sub_in = bot_sub_out\n\n            mid_sub_out, mid_alignment = self.attention_mid[i](\n                mid_sub_in, encoder_output, encoder_mask)\n            mid_sub_out = self.attention_mid_dropout[i](mid_sub_out, training=training)\n            mid_sub_out = mid_sub_out + mid_sub_in\n            mid_sub_out = self.attention_mid_norm[i](mid_sub_out)\n            \n            mid_alignments.append(mid_alignment)\n\n            # FFN\n            ffn_in = mid_sub_out\n\n            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n            ffn_out = ffn_out + ffn_in\n            ffn_out = self.ffn_norm[i](ffn_out)\n\n            bot_sub_in = ffn_out\n\n        logits = self.dense(ffn_out)\n\n        return logits, bot_alignments, mid_alignments\n\n\nvocab_size = len(twi_tokenizer.word_index) + 1\ndecoder = Decoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n\nsequence_in = tf.constant([[14, 24, 36, 0, 0]])\ndecoder_output, _, _ = decoder(sequence_in, encoder_output)\ndecoder_output.shape\n\n\ncrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True)","f315ad16":"def loss_func(targets, logits):\n    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n    mask = tf.cast(mask, dtype=tf.int64)\n    loss = crossentropy(targets, logits, sample_weight=mask)\n\n    return loss\n\n\nclass WarmupThenDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    \"\"\" Learning schedule for training the Transformer\n\n    Attributes:\n        model_size: d_model in the paper (depth size of the model)\n        warmup_steps: number of warmup steps at the beginning\n    \"\"\"\n    def __init__(self, model_size, warmup_steps=4000):\n        super(WarmupThenDecaySchedule, self).__init__()\n\n        self.model_size = model_size\n        self.model_size = tf.cast(self.model_size, tf.float32)\n\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        step_term = tf.math.rsqrt(step)\n        warmup_term = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.model_size) * tf.math.minimum(step_term, warmup_term)","de61cbf6":"lr = WarmupThenDecaySchedule(MODEL_SIZE)\noptimizer = tf.keras.optimizers.Adam(lr,\n                                     beta_1=0.9,\n                                     beta_2=0.98,\n                                     epsilon=1e-9)","4eb26b73":"def predict(test_source_text=None):\n    \"\"\" Predict the output sentence for a given input sentence\n\n    Args:\n        test_source_text: input sentence (raw string)\n    \n    Returns:\n        The encoder's attention vectors\n        The decoder's bottom attention vectors\n        The decoder's middle attention vectors\n        The input string array (input sentence split by ' ')\n        The output string array\n    \"\"\"\n    if test_source_text is None:\n        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n    print(test_source_text)\n    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n    print(test_source_seq)\n\n    en_output, en_alignments = encoder(tf.constant(test_source_seq), training=False)\n\n    de_input = tf.constant(\n        [[twi_tokenizer.word_index['<start>']]], dtype=tf.int64)\n\n    out_words = []\n\n    while True:\n        de_output, de_bot_alignments, de_mid_alignments = decoder(de_input, en_output, training=False)\n        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n        out_words.append(twi_tokenizer.index_word[new_word.numpy()[0][0]])\n\n        # Transformer doesn't have sequential mechanism (i.e. states)\n        # so we have to add the last predicted word to create a new input sequence\n        de_input = tf.concat((de_input, new_word), axis=-1)\n\n        # TODO: get a nicer constraint for the sequence length!\n        if out_words[-1] == '<end>':\n            break\n\n    print(' '.join(out_words))\n    return en_alignments, de_bot_alignments, de_mid_alignments, test_source_text.split(' '), out_words","e4535532":"@tf.function\ndef train_step(source_seq, target_seq_in, target_seq_out):\n    \"\"\" Execute one training step (forward pass + backward pass)\n\n    Args:\n        source_seq: source sequences\n        target_seq_in: input target sequences (<start> + ...)\n        target_seq_out: output target sequences (... + <end>)\n    \n    Returns:\n        The loss value of the current pass\n    \"\"\"\n    with tf.GradientTape() as tape:\n        encoder_mask = 1 - tf.cast(tf.equal(source_seq, 0), dtype=tf.float32)\n        # encoder_mask has shape (batch_size, source_len)\n        # we need to add two more dimensions in between\n        # to make it broadcastable when computing attention heads\n        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n        encoder_output, _ = encoder(source_seq, encoder_mask=encoder_mask)\n\n        decoder_output, _, _ = decoder(\n            target_seq_in, encoder_output, encoder_mask=encoder_mask)\n\n        loss = loss_func(target_seq_out, decoder_output)\n\n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n\n    return loss","00f99a68":"NUM_EPOCHS = 100\n\nstarttime = time.time()\nfor e in range(NUM_EPOCHS):\n    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n        loss = train_step(source_seq, target_seq_in,\n                          target_seq_out)\n        if batch % 100 == 0:\n            print('Epoch {} Batch {} Loss {:.4f} Elapsed time {:.2f}s'.format(\n                e + 1, batch, loss.numpy(), time.time() - starttime))\n            starttime = time.time()\n\n    try:\n        predict()\n        \n    except Exception as e:\n        print(e)\n        continue","7bc01d11":"test_sents = (\n\n   'What kind of wrong activities are common where you live, and how are you and your family affected?',\n    'How are you benefiting from an evening set aside for family worship or personal study?',\n    \"A man's worth lies in what he is.\",\n    'He asked the graduating class: \u201cHow are you going to view yourselves as you go to your missionary assignment?',\n    \"How Are You Benefiting From the New Meeting Format and the Workbook?: (5 min.) Discussion.\",\n    \"Just how are you recommending yourself to others in this regard?\",\n    \"\u201cSerpents, offspring of vipers,\u201d he says, \u201chow are you to flee from the judgment of Gehenna?\u201d\",\n    \"Ask, \u2018How are you?\u2019\",\n    \"How Are You Affected by God\u2019s Dignity and Splendor?\",\n    \"By the way, how are you doing on time?\",\n    \"How Are You Affected?\",\n    \"How are you treating the gift that God has given you?\",\n    \"How are you involved in the issue that Satan raised regarding Job?\",\n    \"How are you personally affected?\",\n    \"All the information needed to repeat a phrase like \u201cHow are you doing?\u201d\",\n    \"Ask, \u2018How are you?\u2019\",\n    'What a ridiculous concept!',\n    'Your idea is not entirely crazy.',\n    \"A man's worth lies in what he is.\",\n    'What he did is very wrong.',\n    \"All three of you need to do that.\",\n    \"Are you giving me another chance?\",\n    \"Both Tom and Mary work as models.\",\n    \"Can I have a few minutes, please?\",\n    \"Could you close the door, please?\",\n    \"Did you plant pumpkins this year?\",\n    \"Do you ever study in the library?\",\n    \"Don't be deceived by appearances.\",\n    \"Excuse me. Can you speak English?\",\n    \"Few people know the true meaning.\",\n    \"Germany produced many scientists.\",\n    \"Guess whose birthday it is today.\",\n    \"He acted like he owned the place.\",\n    \"Honesty will pay in the long run.\",\n    \"How do we know this isn't a trap?\",\n    \"I can't believe you're giving up.\",\n)","36abd955":"for i, test_sent in enumerate(raw_data_en[:50]):\n    test_sequence = normalize_eng(test_sent)\n    predict(test_sequence)\n    # print(\u00f7)\n    print()","01e2b20f":"# Preliminaries"}}