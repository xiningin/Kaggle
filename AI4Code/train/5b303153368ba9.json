{"cell_type":{"661d3eb3":"code","4967dd44":"code","a9b10c99":"code","e5613cb5":"code","1c7a6135":"code","fe4b7d5d":"code","c645e60a":"code","31a49121":"code","31816394":"code","6a65f445":"code","81ce7bef":"code","27098608":"code","96da8818":"code","223971c0":"code","aa2007e3":"code","1bab5358":"code","1b3e57b4":"code","d25e0886":"code","c3078a38":"code","9ffef046":"code","fa835504":"code","5bab02c1":"code","726a3b50":"code","e9d91375":"code","49ebf40c":"code","f61803ff":"code","0df854a6":"code","225161b0":"code","717e33b0":"code","1a498b5b":"code","802378bb":"code","82f3bcbe":"code","26ae9859":"code","306a0ef7":"code","d95f3e25":"code","c0179fd2":"code","f05fee06":"code","7c77f872":"code","5ed2e282":"code","4de1dee5":"code","4fd0ab51":"code","b30af772":"code","86c1a1c5":"code","c48143a8":"code","c934bdf1":"code","23dead31":"code","a0df701b":"code","0548df7b":"code","20e479c7":"code","ff2f3f52":"code","8ad970a1":"code","6a6e041f":"code","2a0feb0d":"code","cc21414b":"code","b7c5c2ba":"code","03ab767c":"code","ef78b554":"code","0b127011":"code","893eff2e":"code","d9112307":"code","5697e766":"code","01e5aecc":"code","5624b6c0":"code","fb03ed4e":"code","fd83263d":"code","af17613c":"code","d9b174d1":"code","e37ba7d4":"code","c60799f8":"code","6f0d0c2b":"code","cb74c34d":"code","b8d06c81":"code","e3e86523":"code","adb99b86":"code","41bd152f":"code","8143f13d":"code","9b089f35":"code","ccea5de1":"code","e6207fa6":"code","2d7ca1b1":"code","72d3c502":"code","84e7c31a":"code","8e21f877":"code","9e899abe":"code","00fc3b29":"markdown","60eefd45":"markdown","e125d689":"markdown","71d44391":"markdown","21d108f2":"markdown","5fae44af":"markdown","6ce31386":"markdown","ec8253f6":"markdown","2e801bc6":"markdown","cf3e90aa":"markdown","743eecad":"markdown","80444270":"markdown","4a5f4bf7":"markdown"},"source":{"661d3eb3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4967dd44":"train_df = pd.read_csv('\/kaggle\/input\/1056lab-credit-card-customer-churn-prediction\/train.csv', index_col=0)\ntrain_df.head()","a9b10c99":"test_df = pd.read_csv('\/kaggle\/input\/1056lab-credit-card-customer-churn-prediction\/test.csv', index_col=0)\ntest_df.head()","e5613cb5":"df = pd.concat([train_df.drop(['Attrition'],axis=1), test_df])\ndf.head()","1c7a6135":"df[df.isnull().any(1)]","fe4b7d5d":"df.isnull().sum()","c645e60a":"median = df['Total_Amt_Chng_Q4_Q1'].median()\ndf['Total_Amt_Chng_Q4_Q1'] = df['Total_Amt_Chng_Q4_Q1'].fillna(median)","31a49121":"df.isnull().sum()","31816394":"train_df.columns","6a65f445":"import plotly.express as px\nyprop = 'Customer_Age'\nxprop = 'Months_Inactive_12_mon'\nh= 'Attrition'\npx.scatter(train_df, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","81ce7bef":"yprop = 'Months_on_book'\nxprop = 'Dependent_count'\nh= 'Attrition'\npx.scatter(train_df, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","27098608":"fig = px.box(train_df, x='Gender',y='Avg_Utilization_Ratio', color='Attrition', notched=True)\nfig.update_layout(legend=dict(orientation=\"h\",yanchor=\"bottom\",y=1.02,xanchor=\"right\",x=1))\nfig.show()","96da8818":"yprop = 'Total_Amt_Chng_Q4_Q1'\nxprop = 'Total_Trans_Amt'\nh= 'Attrition'\npx.scatter(train_df, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","223971c0":"df.info()","aa2007e3":"data = pd.DataFrame(df, columns = ['Customer_Age', 'Gender', 'Dependent_count','Education_Level', 'Marital_Status', \n                                     'Income_Category', 'Card_Category',\n                                     'Months_on_book',\n                                     'Total_Relationship_Count', \n                                     'Months_Inactive_12_mon','Contacts_Count_12_mon',\n                                     'Credit_Limit',\n                                     'Total_Revolving_Bal',\n                                     'Avg_Open_To_Buy',\n                                     'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n                                     'Total_Trans_Ct',\n                                     'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio'\n                                     ])","1bab5358":"import copy\ndf_train=copy.deepcopy(data)\ncols=np.array(data.columns[data.dtypes != object])\nfor i in df_train.columns:\n    if i not in cols:\n        df_train[i]=df_train[i].map(str)\ndf_train.drop(columns=cols,inplace=True)","1b3e57b4":"from sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\n\n# build dictionary function\ncols=np.array(data.columns[data.dtypes != object])\nd = defaultdict(LabelEncoder)\n\n# only for categorical columns apply dictionary by calling fit_transform \ndf_train= df_train.apply(lambda x: d[x.name].fit_transform(x))\ndf_train[cols] = data[cols]","d25e0886":"df_train.head(2)","c3078a38":"df_train.columns","9ffef046":"numeric_columns = ['Customer_Age', 'Dependent_count', \n                   'Months_on_book',\n                   'Total_Relationship_Count', \n                   'Months_Inactive_12_mon','Contacts_Count_12_mon',\n                   'Credit_Limit', 'Total_Revolving_Bal',\n                   'Avg_Open_To_Buy',\n                   'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n                   'Total_Trans_Ct', \n                   'Total_Ct_Chng_Q4_Q1',\n                   'Avg_Utilization_Ratio']\n\nfor col in numeric_columns:\n    df[col] = (df[col] - df[col].mean()) \/ df[col].std()\ndf.head()","fa835504":"df_train.count()","5bab02c1":"df_train.columns","726a3b50":"#categorical_columns = ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']\n#df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n#df.head()","e9d91375":"nrow, ncol = train_df.shape\ntrain_df = pd.concat([df_train[:nrow],train_df['Attrition']], axis=1)\ntrain_df.head()","49ebf40c":"train_df.count()","f61803ff":"test_df = df_train[nrow:]\ntest_df.head()","0df854a6":"test_df.count()","225161b0":"from mlxtend.preprocessing import standardize\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nplt.style.use('ggplot')\nfig = plt.figure(figsize=(10, 10))\nsns.boxplot(data=train_df[numeric_columns], orient='h')\nplt.show()","717e33b0":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\ncorr = train_df.corr()\n\nplt.style.use('ggplot')\nfig = plt.figure(figsize=(10, 10))\ng = sns.heatmap(corr, cmap='coolwarm', square=True, linecolor='w', linewidth=.5)\nplt.show()","1a498b5b":"from collections import Counter\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.over_sampling import SMOTE\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import LGBMRegressor\nfrom numpy import where\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n\nfrom sklearn.ensemble.forest import ExtraTreesClassifier\nfrom sklearn.ensemble.forest import ExtraTreesRegressor\n\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRegressor","802378bb":"from sklearn.linear_model import LogisticRegression\n\nX = train_df.drop(['Attrition'], axis=1).to_numpy()\ny = train_df['Attrition'].to_numpy()\n\nmodel = ExtraTreesRegressor(n_estimators=305)\nmodel.fit(X,y)","82f3bcbe":"model.score(X,y)","26ae9859":"test_X = test_df.to_numpy()\np = model.predict(test_X)\nsubmit_df = pd.read_csv('\/kaggle\/input\/1056lab-credit-card-customer-churn-prediction\/sampleSubmission.csv', index_col=0)\nsubmit_df['Attrition'] = p\nsubmit_df.head()","306a0ef7":"submit_df.to_csv('submission_extra.csv')","d95f3e25":"# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\nfrom optuna import create_study","c0179fd2":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\ndef objective(trial):\n    n_estimators = int(trial.suggest_loguniform('n_estimators', 100, 800))\n    max_depth = int(trial.suggest_loguniform('max_depth',1, 300))\n    min_samples_split =int(trial.suggest_loguniform('min_samples_split', 1, 64))\n    #min_samples_leaf=int(trial.suggest_loguniform('min_samples_leaf', 1, 64))\n    model = ExtraTreesRegressor(n_estimators=n_estimators, \n                          max_depth=max_depth,\n                          #min_samples_leaf= min_samples_leaf,\n                          min_samples_split=min_samples_split,\n                          )\n    X_train_, _, y_train_, _ = train_test_split(X, y, train_size=0.8, random_state=42)  # fast but inaccurate tuning\n    scores = cross_validate(model, X_train_, y_train_, scoring='roc_auc', cv=kf)\n    return scores['test_score'].mean()\n\nstudy = create_study(direction='maximize')\nstudy.optimize(objective, n_trials=5)\nbest_trial =  study.best_trial\nprint('AUC: {}'.format(best_trial.value))\nprint('Best params: {}'.format(best_trial.params))","f05fee06":"\nmodel1 =LGBMRegressor()\nmodel1.fit(X,y)","7c77f872":"model1.score(X,y)","5ed2e282":"test_X = test_df.to_numpy()\np = model1.predict(test_X)\nsubmit_df1 = pd.read_csv('\/kaggle\/input\/1056lab-credit-card-customer-churn-prediction\/sampleSubmission.csv', index_col=0)\nsubmit_df1['Attrition'] =  p\nsubmit_df1.head()","4de1dee5":"submit_df1.to_csv('submission_XGB.csv')","4fd0ab51":"model2 =GradientBoostingRegressor()\nmodel2.fit(X,y)","b30af772":"model2.score(X,y)","86c1a1c5":"test_X = test_df.to_numpy()\np = model2.predict(test_X)\nsubmit_df2 = pd.read_csv('\/kaggle\/input\/1056lab-credit-card-customer-churn-prediction\/sampleSubmission.csv', index_col=0)\nsubmit_df2['Attrition'] = p\nsubmit_df2.head()","c48143a8":"submit_df2.to_csv('submission_GradientBoosting.csv')","c934bdf1":"model3 =XGBRegressor()\nmodel3.fit(X,y)","23dead31":"model3.score(X,y)","a0df701b":"test_X = test_df.to_numpy()\np = model3.predict(test_X)\nsubmit_df3 = pd.read_csv('\/kaggle\/input\/1056lab-credit-card-customer-churn-prediction\/sampleSubmission.csv', index_col=0)\nsubmit_df3['Attrition'] =  p\nsubmit_df3.head()","0548df7b":"submit_df3.to_csv('submission_XBG.csv')","20e479c7":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\ndef objective(trial):\n    n_estimators = int(trial.suggest_loguniform('n_estimators', 100, 5000))\n    max_depth = int(trial.suggest_loguniform('max_depth', 300, 1000))\n    min_child_weight = int(trial.suggest_loguniform('min_child_weight', 1, 128))\n    \n    model = XGBRegressor( learning_rate =0.01, \n                         n_estimators=n_estimators,\n                          max_depth=max_depth,\n                          min_child_weight=min_child_weight,\n                          nthread=-1,\n                          seed=0,\n                          objective= 'binary:logistic',\n                          subsample=0.9,\n                          random_state=42,\n                          colsample_bytree=0.8)\n    X_train_, _, y_train_, _ = train_test_split(X, y, train_size=0.95, random_state=42)  # fast but inaccurate tuning\n    scores = cross_validate(model, X_train_, y_train_, scoring='roc_auc', cv=kf)\n    return scores['test_score'].mean()\n\nstudy = create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\nbest_trial =  study.best_trial\nprint('AUC: {}'.format(best_trial.value))\nprint('Best params: {}'.format(best_trial.params))","ff2f3f52":"n_estimators = int(best_trial.params['n_estimators'])\nmax_depth = int(best_trial.params['max_depth'])\nmin_child_weight = int(best_trial.params['min_child_weight'])","8ad970a1":"model4=XGBRegressor( learning_rate =0.01, \n                         n_estimators=n_estimators,\n                          max_depth=max_depth,\n                          min_child_weight=min_child_weight,\n                          nthread=-1,\n                           seed=0,\n                          objective= 'binary:logistic',\n                          subsample=0.9,\n                          random_state=42,\n                          colsample_bytree=0.8)\nmodel4.fit(X,y)","6a6e041f":"test_X = test_df.to_numpy()\np = model4.predict(test_X)\nsubmit_df4 = pd.read_csv('\/kaggle\/input\/1056lab-credit-card-customer-churn-prediction\/sampleSubmission.csv', index_col=0)\nsubmit_df4['Attrition'] =  p\nsubmit_df4.head()","2a0feb0d":"submit_df4.to_csv('submission_XBG_tuning.csv')","cc21414b":"df.shape","b7c5c2ba":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain_df['class'] = le.fit_transform(train_df['Attrition'])\ntrain_df = train_df.drop('Attrition', axis=1)","03ab767c":"categorical_columns = train_df.select_dtypes(exclude=['int64','float64']).columns\nnumerical_columns = train_df.drop('class', axis=1).select_dtypes(include=['int64','float64']).columns\ncategorical_columns","ef78b554":"train_df.shape","0b127011":"train_df","893eff2e":"# Generate x and y sets\nx = train_df.drop('class', axis=1).values\ny = train_df['class']","d9112307":"# Importing packages for SMOTE\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom imblearn.pipeline import Pipeline\n\nfrom collections import Counter","5697e766":"sm = SMOTE(sampling_strategy='auto', random_state=1234)\nx_sm, y_sm = sm.fit_resample(X, y)","01e5aecc":"print(Counter(y))\nprint(Counter(y_sm))","5624b6c0":"over = BorderlineSMOTE(sampling_strategy=0.3)\nunder = RandomUnderSampler(sampling_strategy=0.6)\n\nsteps = [('o', over), ('u', under)]","fb03ed4e":"pipeline = Pipeline(steps=steps)\n\n# transform the dataset\nx_sm_us, y_sm_us = pipeline.fit_resample(X, y)\n\nprint(Counter(y))\nprint(Counter(y_sm_us))","fd83263d":"list(Counter(y).keys())","af17613c":"x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=1234)","d9b174d1":"feature_names = list(train_df.drop('class', axis=1).columns)","e37ba7d4":"sm_us_x = np.concatenate((x_sm_us, x_test))\nsm_us_y = np.concatenate((y_sm_us, y_test))","c60799f8":"sm_us_x.shape","6f0d0c2b":"sm_us_df = pd.DataFrame(np.column_stack([sm_us_y, sm_us_x]), columns=['class'] + feature_names)\nsm_us_df.head()","cb74c34d":"sm_us_df.shape","b8d06c81":"y_sm_us.head()","e3e86523":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\ndef objective(trial):\n    n_estimators = int(trial.suggest_loguniform('n_estimators', 100, 5000))\n    max_depth = int(trial.suggest_loguniform('max_depth', 300, 1000))\n    min_child_weight = int(trial.suggest_loguniform('min_child_weight', 1, 128))\n    \n    model = XGBRegressor( learning_rate =0.01, \n                         n_estimators=n_estimators,\n                          max_depth=max_depth,\n                          min_child_weight=min_child_weight,\n                          nthread=-1,\n                           seed=0,\n                          objective= 'binary:logistic',\n                          subsample=0.9,\n                          random_state=1234,\n                          colsample_bytree=0.8)\n    X_train_, _, y_train_, _ = train_test_split(x_sm_us, y_sm_us, train_size=0.9, random_state=42)  # fast but inaccurate tuning\n    scores = cross_validate(model, X_train_, y_train_, scoring='roc_auc', cv=kf)\n    return scores['test_score'].mean()\n\nstudy = create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\nbest_trial =  study.best_trial\nprint('AUC: {}'.format(best_trial.value))\nprint('Best params: {}'.format(best_trial.params))","adb99b86":"n_estimators = int(best_trial.params['n_estimators'])\nmax_depth = int(best_trial.params['max_depth'])\nmin_child_weight = int(best_trial.params['min_child_weight'])","41bd152f":"model4=XGBRegressor( learning_rate =0.01, \n                         n_estimators=n_estimators,\n                          max_depth=max_depth,\n                          min_child_weight=min_child_weight,\n                          nthread=-1,\n                           seed=0,\n                          objective= 'binary:logistic',\n                          subsample=0.9,\n                          random_state=1234,\n                          colsample_bytree=0.8)\nmodel4.fit(x_sm_us, y_sm_us)","8143f13d":"test_X = test_df.to_numpy()\np2 = model4.predict(test_X)\nsubmit_df4 = pd.read_csv('\/kaggle\/input\/1056lab-credit-card-customer-churn-prediction\/sampleSubmission.csv', index_col=0)\nsubmit_df4['Attrition'] =  p2\nsubmit_df4.head()","9b089f35":"submit_df4.to_csv('submission_XBG_tuning_SMOTE.csv')","ccea5de1":"model_SMOTE1=XGBRegressor( learning_rate =0.01, \n                         n_estimators=n_estimators,\n                          max_depth=max_depth,\n                          min_child_weight=min_child_weight,\n                          nthread=-1,\n                           seed=0,\n                          objective= 'binary:logistic',\n                          subsample=0.9,\n                          random_state=42,\n                          colsample_bytree=0.8)\nmodel_SMOTE1.fit(X,y)","e6207fa6":"test_X = test_df.to_numpy()\np2_model_SMOTE1 = model_SMOTE1.predict(test_X)\nsubmit_df5 = pd.read_csv('\/kaggle\/input\/1056lab-credit-card-customer-churn-prediction\/sampleSubmission.csv', index_col=0)\nsubmit_df5['Attrition'] =  p2_model_SMOTE1\nsubmit_df5.head()","2d7ca1b1":"submit_df5.to_csv('submission_XBG_tuning_SMOTE2.csv')","72d3c502":"from catboost import CatBoostRegressor\nmodel_cat = CatBoostRegressor(iterations=10000)\nmodel_cat.fit(X,y)","84e7c31a":"model.score(X,y)","8e21f877":"test_X = test_df.to_numpy()\np_cat = model_cat.predict(test_X)\n\nsubmit_df6 = pd.read_csv('\/kaggle\/input\/1056lab-credit-card-customer-churn-prediction\/sampleSubmission.csv', index_col=0)\nsubmit_df6['Attrition'] =  p_cat\nsubmit_df6.head()","9e899abe":"submit_df5.to_csv('submission_cat_tuning_SMOTE2.csv')","00fc3b29":"# GradientBoostingRegressor","60eefd45":"## Rebuilding train data and test data","e125d689":"# XBG","71d44391":"# Generate new DataFrame with SMOTE and UnderSampling data","21d108f2":"## Missing values imputing","5fae44af":"# LGM","6ce31386":"# Over-Sampling | SMOTE","ec8253f6":"## Dummy variables","2e801bc6":"# EXtra","cf3e90aa":"# Preprocessing","743eecad":"# Logistic Regression","80444270":"# Visualizing","4a5f4bf7":"## Standardizing"}}