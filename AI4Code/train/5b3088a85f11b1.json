{"cell_type":{"57cf9f17":"code","26aace56":"code","22f473dd":"code","10b301c3":"code","18c87d2e":"code","4a6e48bd":"code","aa126fcb":"code","d57baf91":"code","d5652e1a":"code","774b378f":"code","ee4fc83d":"code","1369c523":"code","aeea8ffc":"code","311ddb7a":"code","8aaf1e48":"code","939226af":"code","b8869592":"code","2375fcc8":"code","32c75186":"code","6ba1049b":"code","9e9ad4fc":"code","4c7b0578":"code","8f1eee9e":"code","f1522262":"code","fb14f7e4":"code","0669e9e2":"code","a830e672":"code","605ffb44":"code","7d058539":"code","a9ca11e7":"code","89fb2551":"code","95594b0a":"code","0507b691":"code","452e9cd3":"code","bf79d49e":"code","b797f2a6":"code","13f4c134":"code","d506f2f1":"code","950110d6":"code","698d3904":"code","3c202fe8":"code","e4dc658f":"code","9e3b923d":"code","114503aa":"code","94561c84":"code","fa7b1103":"code","5e3ad2cb":"code","82b49fc7":"code","719ec08f":"code","2b8c19e7":"code","74312625":"code","8e54282f":"code","69dbc681":"code","faef70b7":"code","b7605216":"code","db3ae002":"code","c8781760":"code","c5f95978":"code","e48c94fc":"code","831754ed":"code","1f3ce81f":"code","e32f7e52":"code","54f8b44c":"code","a682bc0b":"code","d5c537fb":"code","4ecd4cf1":"code","c47f423b":"code","5f01319a":"code","a3e69417":"code","673ce1de":"code","006480fb":"code","ff06dcfb":"code","254987f4":"code","5db830b7":"code","1fd08261":"code","93811c40":"code","ad831bb2":"code","dcedb187":"code","92963de9":"code","345b530b":"code","844c284e":"code","1e341e6e":"code","7c072f79":"code","1df59f20":"code","dddc0099":"code","200a9f9f":"code","ae2c26a6":"code","45865048":"code","b84ea908":"code","d5ba5134":"code","7b153a48":"code","416e79db":"code","50b69d2f":"code","9db78eee":"code","b20418d1":"code","6db5f4bd":"code","61a2fc33":"code","7503499e":"code","1a8a2efe":"code","73271b35":"code","04347bcf":"code","d7bad55c":"code","a5c132de":"code","cd32a6b7":"code","6e2b4c67":"code","a6ef1740":"code","90c17dd7":"code","150b0622":"code","fcc8cb21":"code","d7661125":"code","22332a4f":"code","ef4818e2":"code","8a42c3a3":"code","6f4ffa5f":"code","2acbf677":"code","caecbf32":"code","a4bd9464":"code","416d16fa":"code","c5bfd100":"code","2035dfb6":"code","e4d750fc":"code","26d97bdb":"code","87def4f6":"code","8f330ff1":"code","10ee0aa2":"code","e91e2d1d":"code","a41f4b8c":"code","e206524a":"code","ef13e18e":"code","9d9bd157":"code","fe33b8df":"code","4ccbd7ee":"code","42b94b70":"code","44ae8639":"code","e6368db4":"code","f7966211":"code","ad124ec9":"code","eccfffac":"code","efa4ecb4":"code","1d7441ed":"code","b91c90c1":"code","9a1a4357":"code","0489ee68":"code","2400f71f":"code","29cce220":"code","eb3de6d2":"code","c745e533":"code","f07edf48":"code","6795f3e8":"code","dbf202bd":"code","40bd5c02":"code","42e65c33":"code","9b2bffa2":"code","713bb58f":"code","cd769272":"code","7b4f0fb3":"code","c13d2461":"code","af187c1f":"code","f6f73570":"code","359715d6":"code","ac1c6047":"code","42d401d0":"code","f89edbbf":"code","0bc06e85":"code","29fbcba4":"code","1faca116":"code","ffd38d2a":"code","d89e9691":"code","646f9f34":"code","d71fbeeb":"code","5c4890e8":"code","dacf5801":"code","2c795a00":"code","6484d2b3":"code","77b00665":"code","1da84e3d":"code","8f93a39c":"code","f5cb0f25":"code","c6658953":"code","57f7f7c8":"code","6277b3c2":"code","3c707dc2":"code","0986fa87":"code","c7055d37":"code","0e634aa4":"code","d2127cad":"code","d245dd8b":"code","91810174":"code","f4e28dde":"code","43a3fba3":"code","77344fff":"code","1125e562":"code","4c0b9aa5":"code","202a0e36":"code","7baae66b":"code","bf6b0426":"code","59d1455d":"code","02cac5c5":"code","6dee5c01":"code","b74c3ddc":"code","993123cf":"code","e4852762":"code","8e0ac8a7":"code","83d7b94c":"code","7c0086c9":"code","761d5804":"code","61bdd536":"code","f1675c37":"code","82f4d353":"code","015b1791":"code","c8a61d43":"code","d5bded23":"code","7116fff2":"code","cd89de0a":"code","eb5b93ad":"code","ee366b92":"code","d46609a4":"code","08992305":"code","ff518b49":"code","8d293f97":"code","1540fa90":"code","8bf9c8a1":"code","343ec7be":"code","7f58a1eb":"code","426fd1ed":"code","1fa9faac":"code","933f2bf7":"code","77f1fe56":"code","a49fb739":"code","9dc28b87":"code","97de41e3":"code","08fe0068":"code","828e9f97":"code","9d60da4b":"code","815435aa":"code","baf79eaf":"code","a4aa78bb":"code","d8599825":"code","d7deb9be":"code","32b45268":"code","ee440ba1":"code","20e4d336":"code","90f8300e":"code","d22e3e21":"code","29db3bec":"markdown","25a5e605":"markdown","84102921":"markdown","63af2db8":"markdown","6d6d9b4f":"markdown","4ac58dcb":"markdown","a0b874dc":"markdown","45d22bbf":"markdown","329a0de5":"markdown","e3e45e75":"markdown","7dabe9b7":"markdown","858c9a54":"markdown","971098e3":"markdown","e675cf52":"markdown","5484f869":"markdown","4c90db9c":"markdown","89412732":"markdown","71faf4aa":"markdown","b0e2e3a3":"markdown","de52ce55":"markdown","b279567c":"markdown","c41a0f14":"markdown","b356a90a":"markdown","c3c64659":"markdown","5a97a9da":"markdown","93318d34":"markdown","9e32e46a":"markdown","5c7b504f":"markdown","10084c69":"markdown","5de2fae5":"markdown","a3b619dc":"markdown","fec684bd":"markdown","b4b55b97":"markdown","830c0793":"markdown","e1a42b64":"markdown","3332a818":"markdown","f66c9a2c":"markdown","842bee63":"markdown","ca89aea0":"markdown","f03949f6":"markdown","cd43f6af":"markdown","a0135ab1":"markdown","6b8d4908":"markdown","95600f12":"markdown","1c2471c5":"markdown","8c138074":"markdown","0a77774a":"markdown","375d2fc5":"markdown","269299c3":"markdown","2d85d22c":"markdown","1bc82787":"markdown","b7bd1fb8":"markdown","9458c3df":"markdown","3b7b02de":"markdown","95be27ed":"markdown","5bc782d3":"markdown","8525e61f":"markdown","fb7973d8":"markdown","8961f04d":"markdown","83baf33d":"markdown","c8a55936":"markdown","fee3003c":"markdown","d0e4d55f":"markdown","59f091c9":"markdown","34da71e7":"markdown","84fa4c3a":"markdown","221e784d":"markdown","359604c0":"markdown","75135de2":"markdown","199dfeb2":"markdown","19b4e482":"markdown","e591d1fe":"markdown","4e2030bd":"markdown","f9c2c03b":"markdown","37f88246":"markdown","af926fba":"markdown","a88613d9":"markdown","744be31e":"markdown","ee201438":"markdown","2250d62f":"markdown","c6a8c214":"markdown","a4634209":"markdown","53f55a26":"markdown","86c1882e":"markdown","ff971598":"markdown","c6e4fa54":"markdown","8a30a241":"markdown","1a730db3":"markdown","5b50555f":"markdown","23746dc1":"markdown","65e3e8a5":"markdown","a36f2a41":"markdown","cf555641":"markdown","cbed0ed6":"markdown","b4b63c2d":"markdown","46fe7eed":"markdown","4680d0a3":"markdown"},"source":{"57cf9f17":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom scipy.special import boxcox1p\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n%matplotlib inline","26aace56":"train_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsales_price_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntrain_data.shape, test_data.shape, sales_price_data.shape","22f473dd":"sales_price_data = sales_price_data.drop('Id', axis=1)","10b301c3":"test_data['SalePrice'] = sales_price_data","18c87d2e":"test_data.head()","4a6e48bd":"train_data.shape, test_data.shape","aa126fcb":"all_data = pd.concat((train_data, test_data)).reset_index(drop=True)","d57baf91":"all_data.head()","d5652e1a":"all_data.info()","774b378f":"all_data.describe()","ee4fc83d":"corr_matrix = all_data.corr()\nf, ax = plt.subplots(figsize=(20,10))\nsns.heatmap(corr_matrix, vmax=.5, cmap=\"Blues\", square=True)\nplt.show()","1369c523":"High_corr = corr_matrix.nlargest(10, 'SalePrice')['SalePrice'].index\nHigh_corr","aeea8ffc":"temp_data = pd.concat([all_data['SalePrice'], all_data['GrLivArea']], axis=1)\ntemp_data.plot.scatter(x='GrLivArea', y='SalePrice', alpha=0.3, ylim=(0,800000))\nplt.show()","311ddb7a":"temp_data = pd.concat([all_data['SalePrice'], all_data['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=all_data['OverallQual'], y=\"SalePrice\", data=temp_data)\nfig.axis(ymin=0, ymax=800000);","8aaf1e48":"temp_data = pd.concat([all_data['SalePrice'], all_data['TotRmsAbvGrd']], axis=1)\ntemp_data.plot.scatter(x='TotRmsAbvGrd', y='SalePrice', alpha=0.3, ylim=(0,800000))\nplt.show()","939226af":"temp_data = pd.concat([all_data['SalePrice'], all_data['GarageCars']], axis=1)\ntemp_data.plot.scatter(x='GarageCars', y='SalePrice', alpha=0.3, ylim=(0,800000))\nplt.show()","b8869592":"temp_data = pd.concat([all_data['SalePrice'], all_data['TotalBsmtSF']], axis=1)\ntemp_data.plot.scatter(x='TotalBsmtSF', y='SalePrice', alpha=0.3, ylim=(0,800000))\nplt.show()","2375fcc8":"temp_data = pd.concat([all_data['SalePrice'], all_data['YearBuilt']], axis=1)\ntemp_data.plot.scatter(x='YearBuilt', y='SalePrice', alpha=0.3, ylim=(0,800000))\nplt.show()","32c75186":"# find the total missing data from each columns \ntotal_missing = all_data.isnull().sum().sort_values(ascending=False)\n# find the precentage \nprecentage = (all_data.isnull().sum()\/all_data.isnull().count()).sort_values(ascending=False)\n# putting the result in one table \nmissing_data = pd.concat([total_missing, precentage], axis=1, keys=['Total', 'precentage'])\nmissing_data.head(40)","6ba1049b":"sns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(15, 12))\nsns.set_color_codes(palette='deep')\nmissing = round(all_data.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(color=\"b\")\nax.set(ylabel=\"Percent of missing values\")\nax.set(xlabel=\"Features\")\nax.set(title=\"Percent missing data by feature\")\nsns.despine(trim=True, left=True)","9e9ad4fc":"missing_more_than_4 = missing_data[missing_data['Total'] > 4].index ","4c7b0578":"for i in missing_more_than_4:\n    all_data = all_data.drop([i], axis=1)","8f1eee9e":"all_data['MSZoning'] = all_data['MSZoning'].fillna(\"RL\")\nall_data['Utilities'] = all_data['Utilities'].fillna(\"AllPub\")\nall_data['Functional'] = all_data['Functional'].fillna(\"Typ\")\nall_data['BsmtFullBath'] = all_data['BsmtFullBath'].fillna(1)\nall_data['BsmtHalfBath'] = all_data['BsmtHalfBath'].fillna(0)\nall_data['GarageArea'] = all_data['GarageArea'].fillna(int(all_data['GarageArea'].mean()))\nall_data['BsmtFinSF2'] = all_data['BsmtFinSF2'].fillna(int(all_data['BsmtFinSF2'].mean()))\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(\"VinylSd\")\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna(int(all_data['TotalBsmtSF'].mean()))\nall_data['GarageCars'] = all_data['GarageCars'].fillna(1)\nall_data['BsmtUnfSF'] = all_data['BsmtUnfSF'].fillna(int(all_data['BsmtUnfSF'].mean()))\nall_data['Electrical'] = all_data['Electrical'].fillna(\"SBrkr\")\nall_data['BsmtFinSF1'] = all_data['BsmtFinSF1'].fillna(int(all_data['BsmtFinSF1'].mean()))\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(\"TA\")\nall_data['SaleType'] = all_data['SaleType'].fillna(\"WD\")\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(\"VinylSd\")","f1522262":"all_data.info()","fb14f7e4":"all_data.isnull().sum().max()","0669e9e2":"new_df = all_data.copy()","a830e672":"new_df.shape, all_data.shape","605ffb44":"# This function will calculate the IQR for us and save the values that is higher or lower as follow\ndef IQR(column_name):\n    Q1 = new_df[column_name].quantile(0.05) # you can change this to 25%\n    Q3 = new_df[column_name].quantile(0.95) # you can change this to 75%\n    IQR = Q3 - Q1\n    upper_limit = Q3 + 1.5 * IQR\n    lower_limit = Q1 - 1.5 * IQR\n    values_upper = new_df[new_df[column_name] > upper_limit]\n    values_lower = new_df[new_df[column_name] < lower_limit]\n    \n    return values_upper, values_lower, upper_limit, lower_limit","7d058539":"# this Function will check if the returned shape from IQR is higher than zero \n# why zero! cos the output will be for example like this (2,63) that means there are 2 rows contains outliers \n# and if it more than zero it will show us this rows\ndef upper(column_name):\n    if values_upper.shape[0] > 0:\n        print(\"Outliers upper than the higher limit: \")\n        return new_df[new_df[column_name] > upper_limit]\n    else:\n        print(\"There are no values higher than the upper limit!\")","a9ca11e7":"# same as above but for lower values\ndef lower(column_name):\n    if values_lower.shape[0] > 0:\n        print(\"Outliers lower than the higher limit: \")\n        return new_df[new_df[column_name] < lower_limit]\n    else:\n        print(\"There are no values lower than the lower limit!\")","89fb2551":"# this function will delete any outliers upper or lower the limit\ndef Outliers_del(column_name):\n    # we will make new_df global to consider the global variable not the local\n    global new_df\n    new_df = new_df[new_df[column_name] < upper_limit]\n    new_df = new_df[new_df[column_name] > lower_limit]\n    print(\"the old data shape is :\", all_data.shape)\n    print(\"the new data shape is :\", new_df.shape)","95594b0a":"def Outliers_del_row(row_num):\n    global new_df\n    new_df = new_df.drop(new_df.index[row_num])\n    print(\"the old data shape is :\", all_data.shape)\n    print(\"the new data shape is :\", new_df.shape)","0507b691":"# this function is for ploting the data \ndef plot(column_name):\n    plt.figure(figsize=(16,5))\n    plt.subplot(1,2,1)\n    # we will use fit norm to draw the normal distibutions that the data sould be it will be in black \n    sns.distplot(all_data[column_name], fit=norm)\n    plt.subplot(1,2,2)\n    sns.boxplot(all_data[column_name])\n    plt.show()","452e9cd3":"# checking and make sure that this column doesnot contain zeros or negative values\ndef zeros(column_name):\n    if ((new_df[column_name] == 0).any() or (new_df[column_name] < 0).any()) == False:\n        print(\"Your column is clean!\")\n    else:\n        print(\"Watch out you have zeros or negative values here!\")","bf79d49e":"# this one to Compare the old data before doing any edit on it like outlier detecting or correcting the skeness and after \ndef Compare(column_name):\n    plt.figure(figsize=(16,8))\n    plt.subplot(2,2,1)\n    sns.distplot(all_data[column_name], fit=norm)\n    plt.subplot(2,2,2)\n    sns.boxplot(all_data[column_name])\n    plt.subplot(2,2,3)\n    sns.distplot(new_df[column_name], fit=norm)\n    plt.subplot(2,2,4)\n    sns.boxplot(new_df[column_name])\n    plt.show()","b797f2a6":"# transform the data with log \ndef log(column_name):\n    new_df[column_name] = np.log(new_df[column_name])","13f4c134":"def log_1(column_name):\n    new_df[column_name] = np.log(new_df[column_name]+1)","d506f2f1":"# transform the data with square root \ndef sqrt(column_name):\n    new_df[column_name] = np.sqrt(new_df[column_name])","950110d6":"# calculate the skewness\ndef skew(column_name):\n    print(\"Skewness: %f\" % new_df[column_name].skew())","698d3904":"Upper_Outliers_columns = []\nLower_Outliers_columns = []\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nfor column in new_df:\n    if new_df[column].dtype in numeric_dtypes:\n        values_upper, values_lower, upper_limit, lower_limit = IQR(column)\n        if values_upper.shape[0] > 0:\n            Upper_Outliers_columns.append(column)\n        if values_lower.shape[0] > 0:\n            Lower_Outliers_columns.append(column)","3c202fe8":"print('Columns upper the limit is: ', Upper_Outliers_columns)\nprint('Columns upper the limit is: ', Lower_Outliers_columns)","e4dc658f":"matches = set(Upper_Outliers_columns) & set(High_corr)\nprint(\"The columns that have a high correlation and also have Outliers are: \", matches)","9e3b923d":"set(Lower_Outliers_columns) & set(High_corr)","114503aa":"un_matches = set(Upper_Outliers_columns) - set(matches)\nprint(\"The columns that have Outliers and doesnot have high correlation are: \", un_matches)","94561c84":"plot('SalePrice')","fa7b1103":"values_upper, values_lower, upper_limit, lower_limit = IQR('SalePrice')","5e3ad2cb":"upper('SalePrice')","82b49fc7":"lower('SalePrice')","719ec08f":"Outliers_del('SalePrice')","2b8c19e7":"skew('SalePrice')","74312625":"zeros('SalePrice')","8e54282f":"log('SalePrice')","69dbc681":"Compare('SalePrice')","faef70b7":"plot('GrLivArea')","b7605216":"values_upper, values_lower, upper_limit, lower_limit = IQR('GrLivArea')","db3ae002":"upper('GrLivArea')","c8781760":"lower('GrLivArea')","c5f95978":"Outliers_del_row([1298])","e48c94fc":"skew('GrLivArea')","831754ed":"zeros('GrLivArea')","1f3ce81f":"log('GrLivArea')","e32f7e52":"Compare('GrLivArea')","54f8b44c":"plot('1stFlrSF')","a682bc0b":"values_upper, values_lower, upper_limit, lower_limit = IQR('1stFlrSF')","d5c537fb":"upper('1stFlrSF')","4ecd4cf1":"lower('1stFlrSF')","c47f423b":"Outliers_del_row([1298])","5f01319a":"skew('1stFlrSF')","a3e69417":"zeros('1stFlrSF')","673ce1de":"log('1stFlrSF')","006480fb":"Compare('1stFlrSF')","ff06dcfb":"plot('FullBath')","254987f4":"values_upper, values_lower, upper_limit, lower_limit = IQR('1stFlrSF')","5db830b7":"upper('FullBath')","1fd08261":"lower('FullBath')","93811c40":"plot('TotalBsmtSF')","ad831bb2":"values_upper, values_lower, upper_limit, lower_limit = IQR('TotalBsmtSF')","dcedb187":"upper('TotalBsmtSF')","92963de9":"lower('TotalBsmtSF')","345b530b":"Outliers_del_row([1298])","844c284e":"skew('TotalBsmtSF')","1e341e6e":"zeros('TotalBsmtSF')","7c072f79":"sqrt('TotalBsmtSF')","1df59f20":"Compare('TotalBsmtSF')","dddc0099":"plot('BsmtFinSF2')","200a9f9f":"values_upper, values_lower, upper_limit, lower_limit = IQR('BsmtFinSF2')","ae2c26a6":"upper('BsmtFinSF2')","45865048":"lower('BsmtFinSF2')","b84ea908":"Outliers_del_row([322, 470, 542, 854])","d5ba5134":"skew('BsmtFinSF2')","7b153a48":"zeros('BsmtFinSF2')","416e79db":"sqrt('BsmtFinSF2')","50b69d2f":"Compare('BsmtFinSF2')","9db78eee":"plot('BsmtFullBath')","b20418d1":"values_upper, values_lower, upper_limit, lower_limit = IQR('BsmtFullBath')","6db5f4bd":"upper('BsmtFullBath')","61a2fc33":"lower('BsmtFullBath')","7503499e":"Outliers_del_row([738])","1a8a2efe":"skew('BsmtFullBath')","73271b35":"zeros('BsmtFullBath')","04347bcf":"sqrt('BsmtFullBath')","d7bad55c":"Compare('BsmtFullBath')","a5c132de":"plot('BsmtFinSF1')","cd32a6b7":"values_upper, values_lower, upper_limit, lower_limit = IQR('BsmtFinSF1')","6e2b4c67":"upper('BsmtFinSF1')","a6ef1740":"lower('BsmtFinSF1')","90c17dd7":"Outliers_del_row([1298])","150b0622":"skew('BsmtFinSF1')","fcc8cb21":"zeros('BsmtFinSF1')","d7661125":"sqrt('BsmtFinSF1')","22332a4f":"Compare('BsmtFinSF1')","ef4818e2":"plot('PoolArea')","8a42c3a3":"(new_df['PoolArea'] > 0).sum()","6f4ffa5f":"for i in new_df['PoolArea']:\n    if i > 0:\n        new_df['PoolArea'] = new_df['PoolArea'].replace(to_replace =i, value =1)","2acbf677":"Compare('PoolArea')","caecbf32":"skew('PoolArea')","a4bd9464":"zeros('PoolArea')","416d16fa":"sqrt('PoolArea')","c5bfd100":"Compare('PoolArea')","2035dfb6":"plot('EnclosedPorch')","e4d750fc":"(new_df['EnclosedPorch'] == 0).sum()","26d97bdb":"values_upper, values_lower, upper_limit, lower_limit = IQR('EnclosedPorch')","87def4f6":"upper('EnclosedPorch')","8f330ff1":"lower('EnclosedPorch')","10ee0aa2":"Outliers_del_row([197])","e91e2d1d":"zeros('EnclosedPorch')","a41f4b8c":"sqrt('EnclosedPorch')","e206524a":"Compare('EnclosedPorch')","ef13e18e":"plot('ScreenPorch')","9d9bd157":"values_upper, values_lower, upper_limit, lower_limit = IQR('ScreenPorch')","fe33b8df":"upper('ScreenPorch')","4ccbd7ee":"lower('ScreenPorch')","42b94b70":"Outliers_del_row([185, 1328, 1386])","44ae8639":"skew('ScreenPorch')","e6368db4":"zeros('ScreenPorch')","f7966211":"sqrt('ScreenPorch')","ad124ec9":"Compare('ScreenPorch')","eccfffac":"plot('LotArea')","efa4ecb4":"values_upper, values_lower, upper_limit, lower_limit = IQR('LotArea')","1d7441ed":"upper('LotArea')","b91c90c1":"lower('LotArea')","9a1a4357":"Outliers_del_row([53, 249, 271, 313, 335, 384, 451, 457, 523, 661, 706, 769, 848, 1298, 1396])","0489ee68":"skew('LotArea')","2400f71f":"zeros('LotArea')","29cce220":"log('LotArea')","eb3de6d2":"Compare('LotArea')","c745e533":"plot('MiscVal')","f07edf48":"values_upper, values_lower, upper_limit, lower_limit = IQR('MiscVal')","6795f3e8":"upper('MiscVal')","dbf202bd":"plot('KitchenAbvGr')","40bd5c02":"values_upper, values_lower, upper_limit, lower_limit = IQR('KitchenAbvGr')","42e65c33":"upper('KitchenAbvGr')","9b2bffa2":"lower('KitchenAbvGr')","713bb58f":"plot('LowQualFinSF')","cd769272":"values_upper, values_lower, upper_limit, lower_limit = IQR('LowQualFinSF')","7b4f0fb3":"upper('LowQualFinSF')","c13d2461":"plot('3SsnPorch')","af187c1f":"values_upper, values_lower, upper_limit, lower_limit = IQR('3SsnPorch')","f6f73570":"upper('3SsnPorch')","359715d6":"plot('BedroomAbvGr')","ac1c6047":"values_upper, values_lower, upper_limit, lower_limit = IQR('BedroomAbvGr')","42d401d0":"upper('BedroomAbvGr')","f89edbbf":"lower('BedroomAbvGr')","0bc06e85":"Outliers_del_row([635])","29fbcba4":"zeros('BedroomAbvGr')","1faca116":"skew('BedroomAbvGr')","ffd38d2a":"Compare('BedroomAbvGr')","d89e9691":"plot('OpenPorchSF')","646f9f34":"values_upper, values_lower, upper_limit, lower_limit = IQR('OpenPorchSF')","d71fbeeb":"upper('OpenPorchSF')","5c4890e8":"lower('OpenPorchSF')","dacf5801":"Outliers_del_row([495, 583, 1328])","2c795a00":"skew('OpenPorchSF')","6484d2b3":"zeros('OpenPorchSF')","77b00665":"sqrt('OpenPorchSF')","1da84e3d":"Compare('OpenPorchSF')","8f93a39c":"plot('WoodDeckSF')","f5cb0f25":"values_upper, values_lower, upper_limit, lower_limit = IQR('WoodDeckSF')","c6658953":"upper('WoodDeckSF')","57f7f7c8":"lower('WoodDeckSF')","6277b3c2":"skew('WoodDeckSF')","3c707dc2":"zeros('WoodDeckSF')","0986fa87":"sqrt('WoodDeckSF')","c7055d37":"Compare('WoodDeckSF')","0e634aa4":"new_df = new_df.drop(['MiscVal', 'KitchenAbvGr', 'LowQualFinSF', '3SsnPorch'], axis=1)","d2127cad":"new_df.shape, all_data.shape","d245dd8b":"from scipy.stats import skew\n\nskewness_list = {}\nfor i in new_df:\n    if new_df[i].dtype != \"object\":\n        skewness_list[i] = skew(new_df[i])\n\nskewness = pd.DataFrame({'Skew' :skewness_list})\n\n\nplt.figure(figsize=(15,9))\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Skewness', fontsize=15)\nplt.xticks(rotation='90')\nplt.bar(range(len(skewness_list)), list(skewness_list.values()), align='center')\nplt.xticks(range(len(skewness_list)), list(skewness_list.keys()))\n\nplt.show()","91810174":"skewness_list","f4e28dde":"for i in skewness_list:\n    if skewness_list[(i)] > 0.5:\n        new_df[i] = boxcox1p(new_df[i], 0.15)\n        ","43a3fba3":"from scipy.stats import skew\n\nskewness_list = {}\nfor i in new_df:\n    if new_df[i].dtype != \"object\":\n        skewness_list[i] = skew(new_df[i])\n\nskewness = pd.DataFrame({'Skew' :skewness_list})\nplt.figure(figsize=(15,9))\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Skewness', fontsize=15)\nplt.xticks(rotation='90')\nplt.bar(range(len(skewness_list)), list(skewness_list.values()), align='center')\nplt.xticks(range(len(skewness_list)), list(skewness_list.keys()))\n\nplt.show()","77344fff":"skewness_list","1125e562":"new_df = new_df.drop('PoolArea', axis=1)","4c0b9aa5":"unique_value = {}\nfor i in new_df:\n    if new_df[i].dtypes == 'object':\n        unique_value[i] = new_df[i].unique()","202a0e36":"unique_value","7baae66b":"def encoding(data):\n    replace_dic = {}\n    for i in unique_value:\n        labels = data[i].astype('category').cat.categories.tolist()\n        replace_dic[i] = {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}\n    final_replace_dic = {}\n    final_replace_dic = final_replace_dic.fromkeys(replace_dic.keys(),[])\n    for i in replace_dic:\n        final_replace_dic[i] = list(replace_dic[i].values()) \n    for i in unique_value:\n        data[i] = data[i].replace(to_replace =unique_value[i], value =final_replace_dic[i])","bf6b0426":"encoding(new_df)","59d1455d":"new_df.info()","02cac5c5":"train = new_df[:1421]","6dee5c01":"test = new_df[1421:]","b74c3ddc":"train.shape, test.shape","993123cf":"y_train = train['SalePrice']","e4852762":"X_train = train.drop(['SalePrice','Id'], axis=1)","8e0ac8a7":"y_test = test['SalePrice']","83d7b94c":"X_test = test.drop(['SalePrice','Id'], axis=1)","7c0086c9":"X_train.shape, X_test.shape","761d5804":"y_train.shape, y_test.shape","61bdd536":"from sklearn.preprocessing import MinMaxScaler","f1675c37":"scaler = MinMaxScaler()","82f4d353":"X_train = scaler.fit_transform(X_train)","015b1791":"X_test = scaler.transform(X_test)","c8a61d43":"X_train","d5bded23":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error","7116fff2":"cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)","cd89de0a":"RF = RandomForestRegressor()\nRF_scores = cross_val_score(RF, X_train, y_train, cv=cv)\nRF_scores","eb5b93ad":"RF_scores.mean()","ee366b92":"RF_cv_pred = cross_val_predict(RF, X_train, y_train, cv=10)\nmean_squared_error(y_train, RF_cv_pred)","d46609a4":"RF.fit(X_train, y_train)\nRF.score(X_train, y_train)","08992305":"KNN = KNeighborsRegressor()\nKNN_scores = cross_val_score(KNN, X_train, y_train, cv=cv)\nKNN_scores","ff518b49":"KNN_scores.mean()","8d293f97":"KNN_cv_pred = cross_val_predict(KNN, X_train, y_train, cv=10)\nmean_squared_error(y_train, KNN_cv_pred)","1540fa90":"KNN.fit(X_train, y_train)\nKNN.score(X_train, y_train)","8bf9c8a1":"from sklearn.model_selection import GridSearchCV","343ec7be":"param_grid = [\n        {'n_estimators': [10, 25, 50, 100,150, 200, 250], 'criterion': ['gini','entropy']},\n        {'max_depth': [1, 2, 3, 4, 5,6,7,8,9,10], 'max_features': ['auto', 'sqrt', 'log2']},\n]\n\nRF = RandomForestRegressor()\ngrid_search = GridSearchCV(RF, param_grid, cv=cv,scoring='neg_mean_squared_error',return_train_score=True)\ngrid_search.fit(X_train, y_train)","7f58a1eb":"grid_search.best_params_","426fd1ed":"grid_search.best_estimator_","1fa9faac":"param_grid = [\n        {'n_neighbors': [1,2,3,4,5,10,15,20], 'p': [1,2]}\n]\n\nKNN = KNeighborsRegressor()\ngrid_search = GridSearchCV(KNN, param_grid, cv=cv,scoring='neg_mean_squared_error',return_train_score=True)\ngrid_search.fit(X_train, y_train)","933f2bf7":"grid_search.best_estimator_","77f1fe56":"RF = RandomForestRegressor(max_depth=10, max_features='sqrt')\nRF_scores = cross_val_score(RF, X_train, y_train, cv=cv)\nRF_scores","a49fb739":"RF_scores.mean()","9dc28b87":"RF_cv_pred = cross_val_predict(RF, X_train, y_train, cv=10)\nmean_squared_error(y_train, RF_cv_pred)","97de41e3":"RF.fit(X_train, y_train)\nRF.score(X_train, y_train)","08fe0068":"KNN = KNeighborsRegressor(n_neighbors=10, p=1)\nKNN_scores = cross_val_score(KNN, X_train, y_train, cv=cv)\nKNN_scores","828e9f97":"KNN_scores.mean()","9d60da4b":"KNN_cv_pred = cross_val_predict(KNN, X_train, y_train, cv=10)\nmean_squared_error(y_train, KNN_cv_pred)","815435aa":"KNN.fit(X_train, y_train)\nKNN.score(X_train, y_train)","baf79eaf":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","a4aa78bb":"RF_pred = RF.predict(X_test)","d8599825":"mean_squared_error(y_test, RF_pred)","d7deb9be":"mean_squared_error(y_test, RF_pred, squared=False)","32b45268":"mean_absolute_error(y_test, RF_pred)","ee440ba1":"KNN_pred = KNN.predict(X_test)","20e4d336":"mean_squared_error(y_test, KNN_pred)","90f8300e":"mean_squared_error(y_test, KNN_pred, squared=False)","d22e3e21":"mean_absolute_error(y_test, KNN_pred)","29db3bec":"## Fine-Tune Our Model","25a5e605":"now we will delete all columns that have mising data more than 4 values ","84102921":"## Performance Measures","63af2db8":"well now we have a column that have only zero or one \"have a pool or not\"","6d6d9b4f":"#### RandomForestRegressor","4ac58dcb":"#### RandomForestRegressor","a0b874dc":"We need to talk here\n\nafter we prepare our data all we have done was to the numerical columns but also we have categorical columns so we use some methods to transform these columns to numerical there are a lot of methods to do this like\n    \n    Encoding \n    Replacing \n    Get dummies \n    and more\n    \nthe most popular one is \"get_dummies\" but a big note here if you going to use get dummies you have to use it before splitting the data and if you try to apply \"get_dummies\" to the traing set and test set you will get two different results and it not going to work! ","45d22bbf":"i will drop this one too","329a0de5":"ok lets see how the new hyperparameters will performe","e3e45e75":"#### LowQualFinSF","7dabe9b7":"### Low priority\n\n{'LowQualFinSF', 'MiscVal', 'KitchenAbvGr', 'BsmtFinSF1', '3SsnPorch', 'ScreenPorch', 'BsmtFullBath', 'BsmtFinSF2', 'OpenPorchSF', 'PoolArea', 'LotArea', 'WoodDeckSF', 'EnclosedPorch', 'BedroomAbvGr'}","858c9a54":"In this method we will find out all unique data for each object column ","971098e3":"i will drop this one too","e675cf52":"### Data spliting ","5484f869":"#### OpenPorchSF","4c90db9c":"#### BsmtFinSF1","89412732":"so we need to work on '1stFlrSF', 'GrLivArea', 'SalePrice' and 'TotalBsmtSF' first as they are the ones who have high correlations and then we will work on the other ones \"un_matches\" lets start with SalePrice","71faf4aa":"now lets Comparee the old data before removing outliers and also before correcting the skeness and after","b0e2e3a3":"we will use Outliers_del cos we dont have any rows that belongs to test set, the train set is 1460 row any number above taht we will delete it!","de52ce55":"RMSE","b279567c":"so we have a skewness and also the data is not normal and we have here on the left \"boxplot\" we have some outliers \n\nso lets start applying the IQR","c41a0f14":"as we can see there is a positive skewness \n\nnote \n\n    if the output for the skew() was more than 0 and especially more than positive one then we have \n    positive skewness\n    \n    and if the output was less than 0 or -1 then we have negative skewness and we will take a look after a bit \n    of how we will correct that skewness\n    ","b356a90a":"remmeber we concatiate all the data to all_data dataframe so we need to split it again like it was i know we delete some outliers so we have to watch out, and remember we didnt remove any rows from test set so we will keep 1459 rows!\n\nif we can delete rows that contain outliers in test set we will get better results, in a real life project we will delete test set outliers.","c3c64659":"#### GrLivArea","5a97a9da":"#### BsmtFullBath","93318d34":"### High priority\n{'1stFlrSF', 'TotalBsmtSF', 'SalePrice', 'FullBath', 'GrLivArea'}\n\nso here is how we will do it\n\n    plot           >> take a look\n    IQR            >> compute the IQR\n    upper          >> fetch values upper IQR\n    lower          >> fetch values lower IQR\n    Outliers_del   >> delete outliers\n    skew           >> look at the skew\n    zeros          >> is there any zeros or negative!\n    log or sqrt    >> correct the skewness\n    compare        >> compare the result with the old data \n ","9e32e46a":"#### RandomForestRegressor","5c7b504f":"### Understanding our data","10084c69":"Why i will drop those columns?\n\nwell those columns have alot of outliers and aslo those columns does not have a high correlation with our target data \"SalePrice\" so instead of losing alot of rows i drop those columns to keep as much data as i could ","5de2fae5":"Well I concatenate all the data in one data frame so we can do all data preparation processes on it and we going to split it again ","a3b619dc":"#### KNeighborsRegressor","fec684bd":"here we will use boxcox1p cos we have zeros in our columns so to avoid any errors and to save some time you will use boxcox1p, we will apply this method for any column that has 0.5 or more","b4b55b97":"we have here alot of ouliers i will drop this column id want to lose 102 rows!","830c0793":"Ok good after we delete the outliers now time to correct the skewness \n\nwe have two situations positive skewness or negative skewness and if you saw any positive skewness its alert to use Log function it is the best way to correct that skewness but take care of this only for positive if you have a negative skewness you should use Exponential transformation or Power transformation those works fine for a negative skewness\n\nand another note before use Log make sure your columns doesn't have any zeros or negative values ","e1a42b64":"So, those are the features with the highest correlation with our target \"Sale Price\" we will give them a high priority!","3332a818":"also, there are outliers that we need to delete take a look at the difference between the \"75%\" of the data and its max value!","f66c9a2c":"#### TotalBsmtSF","842bee63":"#### FullBath","ca89aea0":"#### RandomForestRegressor","f03949f6":"#### KitchenAbvGr","cd43f6af":"Well not bad i also do feature selection but i did not get a massive diffrance in the result. ","a0135ab1":"#### ScreenPorch","6b8d4908":"## Data visualization ","95600f12":"#### But before getting our hands dirty lets define some functions that we will use a lot like \n    \"IQR\" to calculate the IQR for us \n    \"Upper and Lower\" to fetch upper values and lower values that contain outliers \n    \"Outliers_del\" to delete them \n    \"Plot\" function to plot the curves \n    \"Zeros\" to find zeros and negative values in the columns \"For data transformation\"\n    \"Compare\" to compare the data before deleting outliers and correct the skewness and after\n    \"log\" to transform the data with log \n    \"sqrt\" to transform the data with log \n    \"Skew\" to calculate the skewness\n\nI will write a comment for each function when creating it","1c2471c5":"### Encoding ","8c138074":"### Feature scaling or Data scaling","0a77774a":"#### EnclosedPorch","375d2fc5":"MSE","269299c3":"ok now lets start detect those outliers","2d85d22c":"MAE","1bc82787":"#### BedroomAbvGr","b7bd1fb8":"Ok as we know detecting outliers or correcting the skewness is only for numerical columns but here we have numerical and categorical so we will iterate throw the data and check if the column is numerical we will check if it has any outliers! this will avoid us to check every column manually and at the end, we will have two lists one for columns that have values higher than the IQR limit and another one for columns that has lower values lower than the IQR","9458c3df":"## Data preperation ","3b7b02de":"RMSE","95be27ed":"#### 1stFlrSF","5bc782d3":"Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled.\n\nwe have three methods in sklearn \n\nMinMaxScaler(feature_range = (0, 1)) will transform each value in the column proportionally within the range [0,1]. Use this as the first scaler choice to transform a feature, as it will preserve the shape of the dataset (no distortion).\n\nStandardScaler() will transform each value in the column to range about the mean 0 and standard deviation 1, ie, each value will be normalized by subtracting the mean and dividing by standard deviation. Use StandardScaler if you know the data distribution is normal.\n\nIf there are outliers, use RobustScaler(). Alternatively, you could remove the outliers and use either of the above 2 scalers (choice depends on whether data is normally distributed)\n\nWe delete most outliers earlier so we can use MinMaxScaler or StandardScaler\n","8525e61f":"great now we dont have any missing data in our data frame lets move on!","fb7973d8":"#### SalePrice","8961f04d":"### Importing needed libraries","83baf33d":"## Modeing and Evaluation Using Cross-Validation","c8a55936":"i will drop this one too","fee3003c":"Well, here we have a lot of zeros, and zeros here means that this house does not have a pool and the other values I guess it's the pool size maybe! let's suppose that, ok I am going to convert all other numbers from its vales to the value of 1 so the final column will have zeros and ones representing this house has a pool or not","d0e4d55f":"To evaluate our model we will use some metrices like MSE, MAE and RMSE and note here do not use score to evaluate regression problem!\n\nMSE is calculated by taking the average of the square of the difference between the original and predicted values of the data\n\nMAE the absolute difference between the actual or true values and the values that are predicted. Absolute difference means that if the result has a negative sign, it is ignored.\n\nRMSE is the standard deviation of the errors which occur when a prediction is made on a dataset. This is the same as MSE (Mean Squared Error) but the root of the value is considered while determining the accuracy of the model.\n\n\n<img src=\"https:\/\/econbrowser.com\/wp-content\/uploads\/2019\/07\/msemae.png\">\n\nHere N is the total number of observations\/rows in the dataset. ","59f091c9":"ok usually i delete any rows that has missing value but we cant here cos we cant delete row from test set we need all rows for submission so we will replace the NaN value","34da71e7":"### Loading the data","84fa4c3a":"so let's try another way it is more complicated but we will walk throw it step by step I know here are some modules that do the same following steps so go and check it out but for now, let's do it","221e784d":"## Conclusion","359604c0":"well lets take a fianl look at skweness a correct the rest of our data ","75135de2":"Well, that is the end of our journey, and by the way that was my first code for machine learning ever so, I tried my best here, if you have any good ideas that will improve my code feel free to comment and let me know!","199dfeb2":"#### WoodDeckSF","19b4e482":"lets take some highly correlated feature and see how it related to our target \"SalePrice\"","e591d1fe":"#### KNeighborsRegressor","4e2030bd":"Ooook we have some columns that need to be clean but we need to give priority and some sort of ranking which columns are more important depending on their correlation with SalePrice so we will Compare the two lists first one \"Upper_Outliers_columns\" which contain the columns that have outliers and second one \"High_corr\" which contains the columns that have high correlation with SalePrice as follow","f9c2c03b":"# Our methodology\n\nWell before we start it is important to determine what we will do, so here is our methodology if you got lost just come back here and find out what is going on and what is the next step!\n\n## Data visualization \n    Loading the data\n    Take a quick look at our data\n    Finding the correlations \n    Understanding our data\n    \n## Data preperation \n    Dealing with the missing data \n    Outliers detection\n    Skweness correction\n    Encoding \n    Data spliting\n    Feature scaling\n    \n## Modeling\n    Building the model\n    Evaluation with cross-validation\n    \n## Fine-tuning \n    Finding the best hyperparameters\n    \n## Performance evaluation\n    Evaluate our model with the new hyperparameters\n    \n## Testing our model\n    Evaluate the model with the test set\n    ","37f88246":"now after finding out the unique values lets define a function to replace each value with a number ","af926fba":"### Find the correlations","a88613d9":"#### KNeighborsRegressor","744be31e":"#### MiscVal","ee201438":"### Skewness","2250d62f":"MAE","c6a8c214":"### Dealing with the missing data","a4634209":"also check lower outliers values with high-corr","53f55a26":"### Take a quick look at our data","86c1882e":"Well, we have a lot of missing data here!, we will fix this later.","ff971598":"Good after checking time to correct the skewness using Log as mentioned before ","c6e4fa54":"here we will delete by row number to avoid delete any row from test set!","8a30a241":"#### PoolArea","1a730db3":"#### LotArea","5b50555f":"MSE","23746dc1":"After we trained our model and take an idea about how it performed no time to find the optimal hyperparameters of the model\nOne way to do that would be to fiddle with the hyperparameters manually until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.\nInstead, you should get Scikit-Learn\u2019s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation","65e3e8a5":"v.good now all our data is int or float you can export that data to a CSV file \"all_data.to_csv(\"all_data.csv\")\" to take a look at what exactly happened","a36f2a41":"we have various methods to detect the outliers I am going to use IQR here this method works fine for me but \n\nyou can try other methods like \n\n            1- Z-score method\n            2. Robust Z-score\n            3. Winterization method(Percentile Capping)\n            4. DBSCAN Clustering\n            5. Isolation Forest\n            6. Visualizing the data\n            \nIQR stands for \"Inter Quartiles Range\"\n\nthis method depends on two values \n    \n    Q1 >> which represents a quarter of the way through the list of all data usually this value is 0.25 but I will use .05 trying not to delete a lot of data \n    \n    Q3 >> which represents three-quarters of the way through the list of all data usually this value is 0.75 but I will use .95 for the same reason\n    \nhow IQR works :\n    well first it sorts the data and finds its median \n    then separate the numbers before the median and finds its own median \"Q1\"  and also separates the numbers \n    after the total median and finds its own median \"Q3\"\n    \n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/1a\/Boxplot_vs_PDF.svg\/1200px-Boxplot_vs_PDF.svg.png\">\n\nthen we will take the difference between Q3 and Q1\nthis method works well when the data is skewed","cf555641":"#### 3SsnPorch","cbed0ed6":"### Outliers detection and Skweness correction","b4b63c2d":"#### BsmtFinSF2","46fe7eed":"## Evaluate Our System on the Test Set ","4680d0a3":"#### KNeighborsRegressor"}}