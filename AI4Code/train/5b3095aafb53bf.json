{"cell_type":{"e5eb30c7":"code","c966a295":"code","cd9af422":"code","d4bc89af":"code","b3d3dd47":"code","11ff5f7e":"code","5bcba77e":"code","c5cc8c54":"code","a17108ec":"code","1a1aeb01":"code","d8cc7c64":"code","6f8a90d0":"code","9623fa59":"code","a9c62169":"code","ed42bf8f":"code","d235f6ec":"code","9909c9e3":"code","333c1d0a":"code","a99d5954":"code","eb6f6f1c":"code","d26465ca":"code","751126cc":"code","0edd5b94":"code","3524de26":"code","bc6dd5bb":"code","cee6cb09":"code","23ab858b":"code","30e5a4b8":"code","91191425":"code","0373ddff":"code","6108f246":"code","6d12aa3f":"code","445544f7":"code","f27e4f15":"code","251b861f":"code","bc622de4":"code","b6cc3fb3":"code","913d5296":"code","39a504c3":"code","e232a6ab":"code","8e258b46":"code","7f31505d":"code","4b7df9fc":"code","70f39a75":"code","881a1496":"markdown","0bb46324":"markdown","39d65a2a":"markdown","bcb12b4f":"markdown","7ddade2d":"markdown","4a9768a6":"markdown","ec1c6551":"markdown","479c6ad7":"markdown","672f6c7b":"markdown","3675174d":"markdown","95454ef9":"markdown"},"source":{"e5eb30c7":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np","c966a295":"import torchvision\nimport torchvision.transforms as transforms\ntrainset = torchvision.datasets.CIFAR10(root='.\/data', train=True,\n                                       download=True,\n                                       transform=transforms.ToTensor())","cd9af422":"classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')","d4bc89af":"trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)","b3d3dd47":"dataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\n\nprint(images[1].shape)\nprint(labels[1].item())","11ff5f7e":"img = images[1]\nprint(type(img))","5bcba77e":"npimg = img.numpy()\nprint(npimg.shape)","c5cc8c54":"npimg = np.transpose(npimg, (1, 2, 0))\nprint(npimg.shape)","a17108ec":"plt.figure(figsize = (1, 1))\nplt.imshow(npimg)\nplt.show()","1a1aeb01":"def imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()","d8cc7c64":"imshow(torchvision.utils.make_grid(images))\nprint(' '.join(classes[labels[j]] for j in range(4)))","6f8a90d0":"import torch.nn as nn\n\nclass FirstCNN(nn.Module):\n    def __init__(self):\n        super(FirstCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, padding=(1, 1), stride=(2, 2))# padding =(1,1), stride=(2, 2)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        return x","9623fa59":"net = FirstCNN()","a9c62169":"out = net(images)\nout.shape","ed42bf8f":"for param in net.parameters():\n    print(param.shape)","d235f6ec":"out1 = out[0, 0, :, :].detach().numpy()\nprint(out1.shape)","9909c9e3":"plt.imshow(out[0,0, :, :].detach().numpy())\nplt.show()","333c1d0a":"class FirstCNN_v2(nn.Module):\n    def __init__(self):\n        super(FirstCNN_v2, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 8, 3), # (N, 3, 32, 32) -> (N, 8, 30, 30)\n            nn.Conv2d(8, 16, 3)) # (N, 9, 30, 30) -> (N, 16, 28, 28)\n    \n    def forward(self, x):\n        x = self.model(x)\n        return x","a99d5954":"net = FirstCNN_v2()\nout = net(images)\nout.shape","eb6f6f1c":"plt.imshow(out[0, 0, :, :].detach().numpy())","d26465ca":"class FirstCNN_v3(nn.Module):\n    def __init__(self):\n        super(FirstCNN_v3, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 6, 5),           # (N, 3, 32, 32) -> (N, 6, 28 ,28)\n            nn.AvgPool2d(2, stride=2),    # (N, 6, 28, 28) -> (N, 6, 14, 14)\n            nn.Conv2d(6, 16, 5),          # (N, 6, 14, 14) -> (N, 16, 10, 10)\n            nn.AvgPool2d(2, stride=2))    # (N, 16, 10, 10) -> (N, 16, 5, 5)\n        \n     \n    def forward(self, x):\n        x = self.model(x)\n        return x","751126cc":"net =  FirstCNN_v3()\nout = net(images)\nout.shape","0edd5b94":"plt.imshow(out[0, 0, :, :].detach().numpy())","3524de26":"class LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.cnn_model = nn.Sequential(\n            nn.Conv2d(3, 6, 5),          # (N, 3, 32, 32) -> (N, 6, 28, 28)\n            nn.LeakyReLU(),\n            nn.AvgPool2d(2, stride=2),   # (N, 6, 28, 28) -> (N, 6, 14, 14)\n            nn.Conv2d(6, 16, 5),         # (N, 6, 14, 14) -> (N, 16, 10, 10)\n            nn.LeakyReLU(),\n            nn.AvgPool2d(2, stride=2)    # (N, 16, 10, 10) -> (N, 16, 5, 5)\n        )\n        self.fc_model = nn.Sequential(\n            nn.Linear(400, 120),         # (N, 400) -> (N, 120)\n            nn.LeakyReLU(),\n            nn.Linear(120, 84),          # (N, 120) -> (N, 84)\n            nn.LeakyReLU(),\n            nn.Linear(84, 10))           # (N, 84) -> (N, 10)\n        \n    \n    def forward(self, x):\n        print(x.shape)\n        x = self.cnn_model(x)\n        print(x.shape)\n        x = x.view(x.size(0), -1)\n        print(x.shape)\n        x = self.fc_model(x)\n        print(x.shape)\n        return x","bc6dd5bb":"net = LeNet()\nout = net(images)","cee6cb09":"print(out)","23ab858b":"max_values, pred_class = torch.max(out.data, 1)\nprint(pred_class)","30e5a4b8":"class LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.cnn_model = nn.Sequential(\n            nn.Conv2d(3, 6, 5),          # (N, 3, 32, 32) -> (N, 6, 28, 28)\n            nn.LeakyReLU(),\n            nn.AvgPool2d(2, stride=2),   # (N, 6, 28, 28) -> (N, 6, 14, 14)\n            nn.Conv2d(6, 16, 5),         # (N, 6, 14, 14) -> (N, 16, 10, 10)\n            nn.LeakyReLU(),\n            nn.AvgPool2d(2, stride=2)    # (N, 16, 10, 10) -> (N, 16, 5, 5)\n        )\n        self.fc_model = nn.Sequential(\n            nn.Linear(400, 120),         # (N, 400) -> (N, 120)\n            nn.LeakyReLU(),\n            nn.Linear(120, 84),          # (N, 120) -> (N, 84)\n            nn.LeakyReLU(),\n            nn.Linear(84, 10))           # (N, 84) -> (N, 10)\n        \n    \n    def forward(self, x):\n        x = self.cnn_model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_model(x)\n        return x","91191425":"batch_size = 128\ntrainset = torchvision.datasets.CIFAR10(root='.\/data', train=True, download=True, transform=transforms.ToTensor())\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n\ntestset = torchvision.datasets.CIFAR10(root='.\/data',train=False, download=True, transform=transforms.ToTensor())\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)","0373ddff":"def evaluation(dataloader):\n    total, correct = 0, 0\n    for data in dataloader:\n        inputs, labels = data\n        outputs = net(inputs)\n        _, pred = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (pred == labels).sum().item()\n    return 100 * correct \/ total","6108f246":"net = LeNet()","6d12aa3f":"import torch.optim as optim \n \nloss_fn = nn.CrossEntropyLoss()\nopt = optim.Adam(net.parameters())","445544f7":"%%time\nloss_arr = []\nloss_epoch_arr = []\nmax_epochs = 16\n\nfor epoch in range(max_epochs):\n    \n    for i, data in enumerate(trainloader, 0):\n        \n        inputs, labels = data\n        \n        opt.zero_grad()\n        \n        outputs = net(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        opt.step()\n        \n        loss_arr.append(loss.item())\n        \n    loss_epoch_arr.append(loss.item())\n\n    print('Epoch: %d\/%d, Test acc: %0.2f, Train acc: %0.2f' % (epoch, max_epochs, evaluation(testloader), evaluation(trainloader)))\n    \nplt.plot(loss_epoch_arr)\nplt.show()","f27e4f15":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","251b861f":"def evaluation(dataloader):\n    total, correct = 0, 0\n    for data in dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = net(inputs)\n        _, pred = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (pred == labels).sum().item()\n    return 100 * correct \/ total","bc622de4":"net = LeNet().to(device)\nloss_fn = nn.CrossEntropyLoss()\nopt = optim.Adam(net.parameters()) ","b6cc3fb3":"%%time\nmax_epochs = 16\n\nfor epoch in range(max_epochs):\n    \n    for i, data in enumerate(trainloader, 0):\n        \n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        opt.zero_grad()\n        \n        outputs = net(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        opt.step()\n        \n    print('Epoch: %d\/%d' % (epoch, max_epochs))","913d5296":"print('Test acc: %0.2f, Train acc: %0.2f' % (evaluation(testloader), evaluation(trainloader)))","39a504c3":"imshow(torchvision.utils.make_grid(images))","e232a6ab":"net = net.to('cpu')","8e258b46":"out = net(images)\nprint(out.shape)","7f31505d":"out = net.cnn_model[0](images)\nout.shape","4b7df9fc":"image_id = 3\nplt.figure(figsize = (2, 2))\nimshow(images[image_id,])","70f39a75":"plt.figure(figsize = (6,6))\nplt.subplot(321)\nfor i in range(6):\n    ax1 = plt.subplot(3, 2, i+1)\n    plt.imshow(out[image_id, i ,:, :].detach().numpy(), cmap=\"binary\")\nplt.show()","881a1496":"### LeNet","0bb46324":"### Visualise Data","39d65a2a":"* Datasets Detail site [link](https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html)","bcb12b4f":"### Data Loading\n","7ddade2d":"### Single Convolutional Layer","4a9768a6":"### Outline \n* Downloading datasets (CIFAR10)\n* Dataloader in torch.utils\n* Visualising images\n* Single and multiple convolutional layers\n* LeNet\n* Training LeNet\n* Simple Visualisation\n","ec1c6551":"### Move to GPU","479c6ad7":"### Deep Convolution Network","672f6c7b":"### Training LeNet","3675174d":"### Basic Visualisation","95454ef9":"### Convolutional Neural Networks\n   \n   Convolutional Neural Networks is the standard form of neural network architecture for solving tasks associated with images. Solutions for tasks such as object detection, face detection, pose estimation and more all have CNN architecture variants.\n\n   A few characteristics of the CNN architecture makes them more favourable in several computer vision tasks. I have written previous articles that dive into each characteristic.\n\n* Local Receptive Fields\n* Sub-Sampling\n* Weight Sharing\n\n### LeNet-5\n\n   LeNet-5 CNN architecture is made up of 7 layers. The layer composition consists of 3 convolutional layers, 2 subsampling layers and 2 fully connected layers\n\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/1_ueA-rooOaiIo3s2rVVz3Ww.png?raw=true'>\n\n   The diagram above shows a depiction of the LeNet-5 architecture, as illustrated in the [original paper](http:\/\/vision.stanford.edu\/cs598_spring07\/papers\/Lecun98.pdf).\n    \n   The first layer is the input layer \u2014 this is generally not considered a layer of the network as nothing is learnt in this layer. The input layer is built to take in 32x32, and these are the dimensions of images that are passed into the next layer. Those who are familiar with the MNIST dataset will be aware that the MNIST dataset images have the dimensions 28x28. To get the MNIST images dimension to the meet the requirements of the input layer, the 28x28 images are padded.\n\n   The grayscale images used in the research paper had their pixel values normalized from 0 to 255, to values between -0.1 and 1.175. The reason for normalization is to ensure that the batch of images have a mean of 0 and a standard deviation of 1, the benefits of this is seen in the reduction in the amount of training time. In the image classification with LeNet-5 example below, we\u2019ll be normalizing the pixel values of the images to take on values between 0 to 1.\n**The LeNet-5 architecture utilizes two significant types of layer construct: convolutional layers and subsampling layers.**\n\n* Convolutional layers\n* Sub-sampling layers\n   \n Within the research paper and the image below, convolutional layers are identified with the \u2018Cx\u2019, and subsampling layers are identified with \u2018Sx\u2019, where \u2018x\u2019 is the sequential position of the layer within the architecture. \u2018Fx\u2019 is used to identify fully connected layers. This method of layer identification can be seen in the image above.\n\n   The official first layer convolutional layer C1 produces as output 6 feature maps, and has a kernel size of 5x5. The kernel\/filter is the name given to the window that contains the weight values that are utilized during the convolution of the weight values with the input values. 5x5 is also indicative of the local receptive field size each unit or neuron within a convolutional layer. The dimensions of the six feature maps the first convolution layer produces are 28x28.\n   A subsampling layer \u2018S2\u2019 follows the \u2018C1\u2019 layer\u2019. The \u2018S2\u2019 layer halves the dimension of the feature maps it receives from the previous layer; this is known commonly as downsampling.\n   The \u2018S2\u2019 layer also produces 6 feature maps, each one corresponding to the feature maps passed as input from the previous layer. This link contains more information on subsampling layers."}}