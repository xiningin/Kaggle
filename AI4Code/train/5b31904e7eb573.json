{"cell_type":{"bce0cf0f":"code","5e9bf055":"code","05ad6db4":"code","6ae3d123":"code","4782f4bb":"code","ab23ad16":"code","65fcc0c5":"code","ca720024":"code","9527e50e":"code","1dd2cdaf":"code","e3e93162":"code","8f294dd8":"code","a76246c5":"code","3e65f0e1":"code","5ab4102b":"code","7a65a287":"code","1f274446":"code","4145cef3":"code","d70e6386":"code","6ae24929":"code","9f082895":"code","dfa7cbcd":"code","4efb9726":"code","56088ff0":"code","2dcc35e9":"code","22782b9e":"markdown","5c38d8a0":"markdown","b7da789b":"markdown","9a30ec39":"markdown","876a9902":"markdown","515d2202":"markdown","18b7b87f":"markdown","fe3496fd":"markdown","8ab66930":"markdown","90a57aec":"markdown","79b29479":"markdown","40bbd087":"markdown","0aeff72e":"markdown","f5e38d34":"markdown","59c7fccd":"markdown","d8fdd5b6":"markdown","41a60fb2":"markdown","9b54e8c2":"markdown"},"source":{"bce0cf0f":"\n#Importing the required Libraries\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport numpy as np \nimport tensorflow as tf \nimport re \nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nimport seaborn as sns \nplt.style.use('ggplot')\nprint(\"Tensorflow version \" + tf.__version__)","5e9bf055":"try: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","05ad6db4":"fake_df = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\nreal_df = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')","6ae3d123":"fake_df.isnull().sum()\nreal_df.isnull().sum()","4782f4bb":"fake_df.subject.unique()\nreal_df.subject.unique()","ab23ad16":"fake_df.drop(['date', 'subject'], axis=1, inplace=True)\nreal_df.drop(['date', 'subject'], axis=1, inplace=True)","65fcc0c5":"fake_df['class'] = 0 \nreal_df['class'] = 1","ca720024":"plt.figure(figsize=(10, 5))\nplt.bar('Fake News', len(fake_df), color='orange')\nplt.bar('Real News', len(real_df), color='green')\nplt.title('Distribution of Fake News and Real News', size=15)\nplt.xlabel('News Type', size=15)\nplt.ylabel('# of News Articles', size=15)","9527e50e":"print('Difference in news articles:',len(fake_df)-len(real_df))","1dd2cdaf":"news_df = pd.concat([fake_df, real_df], ignore_index=True, sort=False)\nnews_df","e3e93162":"news_df['text'] = news_df['title'] + news_df['text']\nnews_df.drop('title', axis=1, inplace=True)\n","8f294dd8":"features = news_df['text']\ntargets = news_df['class']\n\nX_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.20, random_state=18)","a76246c5":"def normalize(data):\n    normalized = []\n    for i in data:\n        i = i.lower()\n        # get rid of urls\n        i = re.sub('https?:\/\/\\S+|www\\.\\S+', '', i)\n        # get rid of non words and extra spaces\n        i = re.sub('\\\\W', ' ', i)\n        i = re.sub('\\n', '', i)\n        i = re.sub(' +', ' ', i)\n        i = re.sub('^ ', '', i)\n        i = re.sub(' $', '', i)\n        normalized.append(i)\n    return normalized\n\nX_train = normalize(X_train)\nX_test = normalize(X_test)","3e65f0e1":"max_vocab = 10000\ntokenizer = Tokenizer(num_words=max_vocab)\ntokenizer.fit_on_texts(X_train)","5ab4102b":"# tokenize the text into vectors \nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","7a65a287":"X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=256)\nX_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=256)","1f274446":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_vocab, 32),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.summary()","4145cef3":"# We are going to use early stop, which stops when the validation loss no longer improve.\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=10,validation_split=0.1, batch_size=30, shuffle=True, callbacks=[early_stop])","d70e6386":"history_dict = history.history\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\nepochs = history.epoch\n\nplt.figure(figsize=(12,9))\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss', size=20)\nplt.xlabel('Epochs', size=20)\nplt.ylabel('Loss', size=20)\nplt.legend(prop={'size': 20})\nplt.show()\n\nplt.figure(figsize=(12,9))\nplt.plot(epochs, acc, 'g', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy', size=20)\nplt.xlabel('Epochs', size=20)\nplt.ylabel('Accuracy', size=20)\nplt.legend(prop={'size': 20})\nplt.ylim((0.5,1))\nplt.show()\n\n","6ae24929":"model.evaluate(X_test, y_test)","9f082895":"pred = model.predict(X_test)\n\nbinary_predictions = []\n\nfor i in pred:\n    if i >= 0.5:\n        binary_predictions.append(1)\n    else:\n        binary_predictions.append(0)","dfa7cbcd":"print('Accuracy on testing set:', accuracy_score(binary_predictions, y_test))\nprint('Precision on testing set:', precision_score(binary_predictions, y_test))\nprint('Recall on testing set:', recall_score(binary_predictions, y_test))\n","4efb9726":"\nmatrix = confusion_matrix(binary_predictions, y_test, normalize='all')\nplt.figure(figsize=(16, 9))\nax= plt.subplot()\nsns.heatmap(matrix, annot=True, ax = ax)\n\n# labels, title and ticks\nax.set_xlabel('Predicted Labels', size=20)\nax.set_ylabel('True Labels', size=20)\nax.set_title('Confusion Matrix', size=20) \nax.xaxis.set_ticklabels([0,1], size=15)\nax.yaxis.set_ticklabels([0,1], size=15)","56088ff0":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape) # shape: (vocab_size, embedding_dim)","2dcc35e9":"word_index = list(tokenizer.word_index.keys())\nword_index = word_index[:max_vocab-1]","22782b9e":"# Check out the distribution of fake news compare to real news","5c38d8a0":"# Evaluate the testing set","b7da789b":"# Training and Test Split","9a30ec39":"### Saves the weights for visualiation \/ If any (for forks)","876a9902":"\n# Visualize our training over time\n\n","515d2202":"# Combining the title with the text,","18b7b87f":"Fake news 0 , Real news 1","fe3496fd":"Drop the date from the dataset.\nI don't think there is a strong correlation between date and validity of the news. As we see above, subjects are not distributed evenly. We do not want that to influence the accuracy of our classifier. Therefore, we need to drop that as well.","8ab66930":"# Building the RNN.","90a57aec":"# Confusion matrix","79b29479":"# Normalizing Data\nlower case, get rid of extra spaces, and url links.","40bbd087":"<div style=\"text-align:center\"><span style=\"color:darkyellow; font-size:3em;\"> Fake \u274c News Detection using Tenosrflow<\/span><\/div>","0aeff72e":"# Read Data","f5e38d34":"### <span style=\"color:darkorange\"> This notebook has very few topics to expalain as it's basic python coding using tensorflow, i would request you to follow other notebooks if you are looking for tutorials<\/u><\/span> \ud83d\ude04\u270c\ufe0f","59c7fccd":"![unnamed.jpg](attachment:unnamed.jpg)","d8fdd5b6":"# Checking for unique values for subject.\nWe want both data frames to have a similar distribution.","41a60fb2":"# Checking for null values","9b54e8c2":"# Convert text to vectors (classifier takes only numerical data)"}}