{"cell_type":{"c4f9d58d":"code","0bd11087":"code","5ba76b88":"code","739b9d9d":"code","cb9be385":"code","b25055d4":"code","5065e3e0":"code","e05467a1":"code","3f51c183":"code","cf5e140e":"code","459414dc":"code","fb4518ed":"code","5fef8375":"code","9764f337":"code","8a3e3629":"code","7090c10a":"code","1738da05":"code","2fcde7d7":"code","1a939f67":"code","7ed0b542":"code","7c0b38c2":"code","2b6d3295":"code","542af62b":"code","fc98a221":"code","5463ff1b":"code","cadd8411":"code","98450a9e":"code","ac0079ed":"code","fbe01c4a":"code","3cf46dc3":"code","975020c3":"code","b773101b":"code","217c94b0":"code","9c5d156c":"code","edb1c219":"code","8e239fe8":"code","cbaf027e":"code","1a74313b":"code","7de464c3":"code","e43d5d21":"code","c9dcbd36":"code","fc68bb41":"code","1aa75a81":"code","11c493fa":"code","edc68eff":"code","d3b05270":"code","59863682":"code","557c96cb":"code","4283c4b7":"code","9c70f330":"code","09f8bb10":"code","281c1d72":"code","f8bdcb71":"code","bd4bb6cf":"code","f9dec0b4":"code","64bc7e74":"markdown"},"source":{"c4f9d58d":"# Importing the libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualisation (2-D)\n%matplotlib inline\nimport seaborn as sns # data visualisation (3-D)\nplt.style.use('seaborn') # set style for graph\nimport warnings \nwarnings.filterwarnings('ignore') # ignore all types of warnings\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0bd11087":"# Loading the red wine dataset\nwine = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","5ba76b88":"display(wine.head(), wine.shape)","739b9d9d":"wine.info()","cb9be385":"# data doesn't contain any null value ","b25055d4":"wine.describe()","5065e3e0":"# its look like our data contains some outliers in most of the features, let's take a look at them","e05467a1":"# lets check the correlation of the features\nwine.corr()","3f51c183":"plt.figure(figsize = (10, 8))\nsns.heatmap(wine.corr(), square = True, cmap = 'Blues')","cf5e140e":"# its looks like features are not strongly correlated to each other\n# lets do some visualisation to detect the outliers in the data","459414dc":"plt.figure(figsize = (10, 6))\nsns.set_context('talk')\nsns.boxplot(wine['quality'], wine['free sulfur dioxide'], data = wine)","fb4518ed":"# free sulphur dioxide contains some extreme values in the dataset","5fef8375":"plt.figure(figsize = (10, 8))\nsns.boxplot(wine['quality'], wine['citric acid'], data = wine)","9764f337":"# citric acid only contains 2 extremes in it which is avoidable for now","8a3e3629":"plt.figure(figsize = (10, 8))\nsns.boxplot(wine['quality'], wine['total sulfur dioxide'], data = wine)","7090c10a":"# total sulphur dioxide containes some outliers and we have to remove them","1738da05":"plt.figure(figsize = (10, 8))\nsns.boxplot(wine['quality'], wine['sulphates'], data = wine)","2fcde7d7":"# sulphates also contains so many outliers","1a939f67":"plt.figure(figsize = (10, 8))\nsns.boxplot(wine['quality'], wine['residual sugar'], data = wine)","7ed0b542":"# residual sugar contains outliers too","7c0b38c2":"outliers = []  # list to store outliers value\n\n# method for detecting the outliers using interquantilerange technique \ndef detect_outliers(data): \n    quantile1, quantile3 = np.percentile(data, [25, 75])  # create two quantiles for 25% and 75%\n    iqr_val = quantile3 - quantile1                       # interquantilerange value\n    lower_bound_value = quantile1 - (1.5 * iqr_val)       # lower limit of the data, anything greater are not outliers\n    upper_bound_value = quantile3 + (1.5 * iqr_val)       # upper limit of the data, anything less are not outliers\n    \n    for i in data:\n        if lower_bound_value < i < upper_bound_value:     # if data[value] is greater than lbv and less than ubv than it is not considered as an outlier\n            pass\n        else:\n            outliers.append(i)\n            \n    return lower_bound_value, upper_bound_value           # return lower bound and upper bound value for the data","2b6d3295":"# fwith the help of boxplot visualisation we can notice that the outliers are only present through the upper bound value so we only check ubv","542af62b":"# detection of outliers from residual sugar\ndetect_outliers(wine['residual sugar'])","fc98a221":"wine = wine.drop(wine[wine['residual sugar'] > 3.65].index) # drop values which contains outliers","5463ff1b":"# detection of outliers from sulphates\ndetect_outliers(wine['sulphates'])","cadd8411":"wine = wine.drop(wine[wine['sulphates'] > 0.99].index)","98450a9e":"# detection of outliers from free sulphur dioxide\ndetect_outliers(wine['free sulfur dioxide'])","ac0079ed":"wine = wine.drop(wine[wine['free sulfur dioxide'] > 40.5].index)","fbe01c4a":"# detect outliers from total sulphur dioxide\ndetect_outliers(wine['total sulfur dioxide'])","3cf46dc3":"wine = wine.drop(wine[wine['total sulfur dioxide'] > 112].index)","975020c3":"# outliers are removed from the dataset. now do some preprocessing on data","b773101b":"# lets change the wine quality in only two categories 'bad' and 'good' it makes it easier for classification","217c94b0":"wine['quality'] = pd.cut(wine['quality'], bins = [0, 6, 8], labels = ['bad', 'good'])","9c5d156c":"wine['quality']","edb1c219":"# lets do labelencoding on wine quality to make it '0' and '1'\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nwine['quality'] = le.fit_transform(wine['quality'])","8e239fe8":"# split the dataset into featurees and label\nX = wine.drop(['quality'], axis = 1)\ny = wine['quality']","cbaf027e":"# split the dataset into training and testing set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","1a74313b":"# lets take a look at the labels\nsns.countplot(y)","7de464c3":"#  it is clearly seen here that their is an imbalance in the dataset, 1's are very few as compared to 0's","e43d5d21":"# lets make the dataset balance by using over sampling \nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 42, ratio = 0.9)\nX_train, y_train = sm.fit_sample(X_train, y_train)","c9dcbd36":"# lets normalize data \nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","fc68bb41":"# Building classification model using randomForestClassiifier\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)","1aa75a81":"predictions = clf.predict(X_test)  # predictions generated by our model","11c493fa":"from sklearn.metrics import confusion_matrix, classification_report\nprint(classification_report(y_test, predictions))","edc68eff":"# we have get an accuracy of 92 % \n# lets check cross validation score for our predictions\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf, X_train, y_train, cv = 10)\nscores.mean()","d3b05270":"# cross validation mean is 91","59863682":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators = 90, criterion = 'gini', max_features = 'auto')\nclf.fit(X_train, y_train)# lets use gridsearchcv to parameter tuning \nfrom sklearn.model_selection import GridSearchCV\nparam = {\n    'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200],\n    'criterion' : ['gini', 'entropy'],\n    'max_features' : ['auto', 'sqrt', 'log2']\n} \n\ngrid_search = GridSearchCV(estimator = clf,\n                          param_grid = param,\n                          scoring = 'f1',\n                          cv = 'warn',\n                          n_jobs = -1)\n\ngrid_search = grid_search.fit(X_train, y_train)\n\nprint(f'best score : {grid_search.best_score_}')\nprint(f'best parameters : {grid_search.best_params_}')","557c96cb":"# lets make changes in parameters in RandomForestClassifier","4283c4b7":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators = 60, criterion = 'gini', max_features = 'log2')\nclf.fit(X_train, y_train)","9c70f330":"predictions_update = clf.predict(X_test)","09f8bb10":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions_update))","281c1d72":"# Great ! we got an accuracy of 94","f8bdcb71":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf, X_train, y_train, cv = 10)\nscores.mean()","bd4bb6cf":"# Great! we got a score of 93","f9dec0b4":"# hope you like ths notebook\n# thank you","64bc7e74":"Detection and Removal of Outliers in dataset"}}