{"cell_type":{"73e42582":"code","e29aa846":"code","0f7efc10":"code","6f93ca7e":"code","d339a60d":"code","1be7847a":"code","2a568dba":"code","d5fa1b0a":"code","8ef0dc8f":"code","d6e83488":"code","36f85d26":"code","26c6756e":"code","6153ff8b":"code","87714b0c":"code","0586e93e":"code","97d128f9":"code","87909fe7":"code","69abe04b":"code","0922d9de":"code","e16acac9":"code","6a492e14":"code","acfb3e40":"code","520585c2":"code","2a703e9b":"code","ce85dc2a":"code","c31e69ab":"code","cc7f59fa":"code","60229412":"code","05004b0b":"code","7bd35775":"code","1dd08e3e":"code","33ae0c07":"code","d30c0f2a":"code","3a86e4b7":"code","19b71649":"code","2c429028":"code","07d2daf0":"code","90553b4c":"code","f3a69826":"code","383b9167":"code","d1772475":"markdown","5db715cb":"markdown","d79e0d99":"markdown","1ae92905":"markdown","90c948b0":"markdown","b85c9747":"markdown"},"source":{"73e42582":"import os\nprint(os.listdir(\"..\/input\"))\n# Ignore  the warnings\nimport warnings \nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore') \n# data visualisation and manipulation \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns \n# sets matplotlib to inline and displays graphs belo w the corressponding cell. \n% matplotlib inline   \nstyle.use('fivethirtyeight') \nsns.set(style='whitegrid',color_codes=True) \n#nltk \nimport nltk \n#preprocessing \nfrom nltk.corpus import stopwords  #stopwords \nfrom nltk import word_tokenize,sent_tokenize # tokenizing \nfrom nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others from nltk.stem.snowball import SnowballStemmer from nltk.stem import WordNetLemmatizer  # lammatiz er from WordNet \n# for part-of-speech tagging\nfrom nltk import pos_tag \n# for named entity recognition (NER) \nfrom nltk import ne_chunk \n# vectorizers for creating the document-term-matrix (DTM) \nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer \n# BeautifulSoup libraray \nfrom bs4 import BeautifulSoup  \nimport re # regex \n#model_selection \nfrom sklearn.model_selection import train_test_split,cross_validate \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import GridSearchCV \n#evaluation \nfrom sklearn.metrics import accuracy_score,roc_auc_score \nfrom sklearn.metrics import classification_report \nfrom mlxtend.plotting import plot_confusion_matrix \n#preprocessing scikit\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder \n#classifiaction. \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.svm import LinearSVC,SVC \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB \n#stop-words \nstop_words=set(nltk.corpus.stopwords.words('english')) \n#keras\nimport keras \nfrom keras.preprocessing.text import one_hot,Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Flatten,Embedding,Input,CuDNNLSTM,LSTM \nfrom keras.models import Model\nfrom keras.preprocessing.text import text_to_word_sequence\n#gensim word2vec \nfrom gensim.models import Word2Vec","e29aa846":"#LOADING THE DATASET\nreviews_fram=pd.read_csv(\"..\/input\/Reviews.csv\") ","0f7efc10":"df=reviews_fram.copy() \ndf.head() ","6f93ca7e":"df=df[['Text','Score']] \ndf['review']=df['Text'] \ndf['rating']=df['Score']\ndf.drop(['Text','Score'],axis=1,inplace=True)\nprint(df.shape)\ndf.head() ","d339a60d":"# check for null values\nprint(df['rating'].isnull().sum())\ndf['review'].isnull().sum() ","1be7847a":"# remove duplicates for every duplicate we will keep only one row of that type. \ndf.drop_duplicates(subset=['rating','review'],keep='first',inplace=True)  ","2a568dba":"# now check the shape.  \nprint(df.shape)\ndf.head() ","d5fa1b0a":"# printing some reviews to see insights.\nfor review in df['review'][:5]: \n    print(review+'\\n'+'\\n')","8ef0dc8f":"#First break text into sentences and then clean those sentences.\n#since we are doing sentiment analysis, convert the values in score column to sentiment. Sentiment is 0 for ratings or scores less than 3 and 1 or more elsewhere.\ndef mark_sentiment(rating):  \n    if(rating<=3):    \n        return 0  \n    else:   \n        return 1 ","d6e83488":"df['sentiment']=df['rating'].apply(mark_sentiment) \n","36f85d26":"df.drop(['rating'],axis=1,inplace=True) \ndf.head() ","26c6756e":"df['sentiment'].value_counts() ","6153ff8b":"# function to clean and pre-process the text. \ndef clean_reviews(review):          \n    # 1. Removing html tags    \n    review_text = BeautifulSoup(review,\"lxml\").get_text()        \n    # 2. Retaining only alphabets.  \n    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n    # 3. Converting to lower case and splitting    \n    word_tokens= review_text.lower().split() \n    # 4. Remove stopwords    \n    le=WordNetLemmatizer()    \n    stop_words= set(stopwords.words(\"english\"))    \n    word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]  \n    \n    cleaned_review=\" \".join(word_tokens)   \n    return cleaned_review ","87714b0c":"# To balance the class, taken equal instances of each sentiment.\npos_df=df.loc[df.sentiment==1,:][:50000] \nneg_df=df.loc[df.sentiment==0,:][:50000]","0586e93e":"pos_df.head()","97d128f9":"neg_df.head() ","87909fe7":"#combining \ndf=pd.concat([pos_df,neg_df],ignore_index=True) \nprint(df.shape)\ndf.head() ","69abe04b":"# shuffling rows \ndf = df.sample(frac=1).reset_index(drop=True) \nprint(df.shape) \ndf.head()","0922d9de":"#from gensim.models import KeyedVectors \n#w2v_model_google = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary= True) ","e16acac9":"from nltk.stem import WordNetLemmatizer \ntokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\nsentences=[]\nsum=0\nfor review in df['review']:\n    sents=tokenizer.tokenize(review.strip())\n    sum+=len(sents)\n    for sent in sents:\n        cleaned_sent=clean_reviews(sent)\n        sentences.append(cleaned_sent.split()) # can use word_tokenize also.\nprint(sum)\nprint(len(sentences))  # total no of sentences","6a492e14":"# trying to print few sentences\nfor te in sentences[:5]:\n    print(te,\"\\n\")","acfb3e40":"# create word 2 vec embeddings\nimport gensim\nw2v_model=gensim.models.Word2Vec(sentences=sentences,size=300,window=10,min_count=1)","520585c2":"w2v_model.train(sentences,epochs=10,total_examples=len(sentences))","2a703e9b":"# embedding of a particular word.\nw2v_model.wv.get_vector('fair')","ce85dc2a":"# total numberof extracted words.\nvocab=w2v_model.wv.vocab\nprint(\"The total number of words are : \",len(vocab))","c31e69ab":"# words most similar to a given word.\nw2v_model.wv.most_similar('fair')","cc7f59fa":"# similaraity b\/w two words\nw2v_model.wv.similarity('fair','reasonable')","60229412":"print(\"The no of words :\",len(vocab))","05004b0b":"# print(vocab)\nvocab=list(vocab.keys())","7bd35775":"word_vec_dict={}\nfor word in vocab:\n    word_vec_dict[word]=w2v_model.wv.get_vector(word)\nprint(\"The no of key-value pairs : \",len(word_vec_dict))","1dd08e3e":"# cleaning reviews.\ndf['clean_review']=df['review'].apply(clean_reviews)","33ae0c07":"# number of unique words = 56379.\n\n# now since we will have to pad we need to find the maximum lenght of any document.\n\nmaxi=-1\nfor i,rev in enumerate(df['clean_review']):\n    tokens=rev.split()\n    if(len(tokens)>maxi):\n        maxi=len(tokens)\nprint(maxi)","d30c0f2a":"tok = Tokenizer()\ntok.fit_on_texts(df['clean_review'])\nvocab_size = len(tok.word_index) + 1\nencd_rev = tok.texts_to_sequences(df['clean_review'])\nmax_rev_len=1565  # max lenght of a review\nvocab_size = len(tok.word_index) + 1  # total no of words\nembed_dim=300 # embedding dimension as choosen in word2vec constructor\n# now padding to have a amximum length of 1565\npad_rev= pad_sequences(encd_rev, maxlen=max_rev_len, padding='post')\npad_rev.shape   # note that we had 100K reviews and we have padded each review to have  a lenght of 1565 words.","3a86e4b7":"# now creating the embedding matrix\nembed_matrix=np.zeros(shape=(vocab_size,embed_dim))\nfor word,i in tok.word_index.items():\n    embed_vector=word_vec_dict.get(word)\n    if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n    \n     embed_matrix[i]=embed_vector\n  # if word is not found then embed_vector corressponding to that vector will stay zero.","19b71649":"print(embed_matrix[14])","2c429028":"# prepare train and val sets first\nY=keras.utils.to_categorical(df['sentiment'])  # one hot target as required by NN.\nx_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.20,random_state=42)","07d2daf0":"from keras.initializers import Constant\nfrom keras.layers import ReLU\nfrom keras.layers import Dropout\nmodel=Sequential()\nmodel.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=max_rev_len,embeddings_initializer=Constant(embed_matrix)))\n# model.add(CuDNNLSTM(64,return_sequences=False)) # loss stucks at about \nmodel.add(Flatten())\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dropout(0.50))\n# model.add(Dense(16,activation='relu'))\n# model.add(Dropout(0.20))\nmodel.add(Dense(2,activation='sigmoid'))","90553b4c":"model.summary()","f3a69826":"# compile the model\nmodel.compile(optimizer=keras.optimizers.RMSprop(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy'])","383b9167":"epochs=10\nbatch_size=64\n# fitting the model.\nmodel.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_test,y_test))","d1772475":"> **Creating Google word2vec Word Embedding in Gensim** \n> \n> Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing.\n","5db715cb":">** DATA CLEANING AND PRE-PROCESSING**\n","d79e0d99":"**TEXT CLASSIFICATION**","1ae92905":"> **Natural Language Processing on Amazon Fine Food Dataset with Word2Vec Word Embeddings in Gensim and training using LSTM In Keras.**","90c948b0":"**Keras embedding**","b85c9747":"> **Pre-processing** \n> *  Removing punctuation and html tags if any.\n> *  Remove the stop words and shorter words as they cause noise.\n> *  Stem or Lemmatize the words depending on what does better.\n"}}