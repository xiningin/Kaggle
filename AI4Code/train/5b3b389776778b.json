{"cell_type":{"394fc42c":"code","baf0b1cf":"code","d88634a1":"code","1aebc226":"code","6e398bfd":"code","d9f8496d":"code","d77af6bb":"code","83cbd3c1":"code","304eaf5e":"code","15f8ddd5":"code","8d69605c":"code","aa27e737":"code","129ba6a4":"code","479640b5":"code","9bbf1671":"code","a9bd0f61":"code","d87ef7fb":"code","7c8a7f36":"code","3cb24864":"code","637ece3f":"code","e2fd0881":"code","08826a71":"code","45c1f6dc":"code","fb08bab0":"code","0e3c7df7":"markdown","d7d0017d":"markdown","039ab742":"markdown","f8572075":"markdown","a212ec64":"markdown","888293ff":"markdown","cff99581":"markdown","ac3adbaa":"markdown","eb3416e5":"markdown","a75a6f31":"markdown","15df8fb8":"markdown","df7b2ba1":"markdown","9a77368b":"markdown","fad37821":"markdown","3c0b182e":"markdown","d352877e":"markdown","1030dca8":"markdown","4a728bcd":"markdown","f389c97e":"markdown","2f5ce58b":"markdown","076ed30c":"markdown","f50aaeb2":"markdown"},"source":{"394fc42c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport random\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator # Data Agumentation \nfrom keras.callbacks import ReduceLROnPlateau # Learning rate \n\n\nsns.set(style='white', context='notebook', palette='deep')","baf0b1cf":"df_train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ndf_test  = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nY_train = df_train[\"label\"]\nY_train\n# Drop 'label' column output column \ndf_X_train = df_train.drop(labels = [\"label\"],axis = 1) \n# free some space by delete df_train dataset \ndel df_train \n\n\n# Show data as plot  \nY_train.value_counts().plot.bar()","d88634a1":"df_X_train.isnull().any().describe()","1aebc226":"df_test.isnull().any().describe()","6e398bfd":"df_X_train=df_X_train\/255.0\ndf_test=df_test\/255.0","d9f8496d":"#Reshape images in 3 dimensions (h=28px , w=28px ,chanal=1)\ndf_X_train=df_X_train.values.reshape(-1,28,28,1)\ndf_test=df_test.values.reshape(-1,28,28,1)\n","d77af6bb":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_train = to_categorical(Y_train, num_classes = 10)","83cbd3c1":"Y_train","304eaf5e":"# Set the random seed\nrandom_seed = 2\n\n# Split the train and the validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(df_X_train, Y_train, test_size = 0.1, random_state=random_seed)","15f8ddd5":"# show X_train shape and X_val shape \nprint(\"Shape of X_train :\"+str(X_train.shape))\nprint(\"Shape of X_valdation :\"+str(X_val.shape))\nprint(\"Shape of Y_train :\"+str(Y_train.shape))\nprint(\"Shape of Y_validation :\"+str(Y_val.shape))\n\n# as we show we have 37800 row in train and 4200 row in validation data \n#I choosed to split the train set in two parts : a small fraction (10%)\n#became the validation set which the model is evaluated and the rest (90%) is used to train the model.\n\n# Good Let's Continue ^_^","8d69605c":"# Some examples of our data \nimgg = plt.imshow(X_train[1110])","aa27e737":"# Set the CNN model \n# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\n\nmodel.summary()\n","129ba6a4":"# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","479640b5":"# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","9bbf1671":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","a9bd0f61":"epochs = 20 \nbatch_size = 86","d87ef7fb":"# Without data augmentation i obtained an accuracy of 0.98114\n#history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n#          validation_data = (X_val, Y_val), verbose = 2)","7c8a7f36":"# With data augmentation to prevent overfitting (accuracy 0.99286)\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train)","3cb24864":"# Fit the model ^_^ Let's Gooooo\nhistory = model.fit(datagen.flow(X_train,Y_train, batch_size=batch_size),epochs=20 ,\n                    callbacks=[learning_rate_reduction],\n                    validation_data=(X_val, Y_val)) \n                   \n                   ","637ece3f":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","e2fd0881":"# Look at confusion matrix \n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","08826a71":"# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","45c1f6dc":"# predict results\nresults = model.predict(df_test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","fb08bab0":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"submissionn.csv\",index=False)","0e3c7df7":"## 2.6 Split training and valdiation set ","d7d0017d":"# Display some error results ","039ab742":"Train and test images (28px x 28px) has been stock into pandas.Dataframe as 1D vectors of 784 values. We reshape all data to 28x28x1 3D matrices. ","f8572075":"## 2.3 Normalization","a212ec64":"Moreover the CNN converg faster on [0..1] data than on [0..255].","888293ff":"## 2.3 Reshape","cff99581":"The validation accuracy is greater than the training accuracy almost evry time during the training. \nThat means that our model dosen't not overfit the training set.\n\nOur model is very well trained  !!! ","ac3adbaa":"# 4. Evaluate the model\n## 4.1 Training and validation curves","eb3416e5":"## 3.2 Set the optimizer and annealer\n\nOnce our layers are added to the model, we need to set up a score function, a loss function and an optimisation algorithm.","a75a6f31":"In order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\nThe improvement is important : \n   - Without data augmentation i obtained an accuracy of 98.114%\n   - With data augmentation i achieved 99.67% of accuracy","15df8fb8":"## 2.5 Label encoding","df7b2ba1":"## 4.2 Confusion matrix\nConfusion matrix can be very helpfull to see your model drawbacks.\n\nI plot the confusion matrix of the validation results.","9a77368b":"# Setps of CNN project\n\n* **1. Introduction**\n* **2. Data preparation**\n    * 2.1 Load data\n    * 2.2 Check for null and missing values\n    * 2.3 Normalization\n    * 2.4 Reshape\n    * 2.5 Label encoding\n    * 2.6 Split training and valdiation set\n* **3. CNN**\n    * 3.1 Define the model\n    * 3.2 Set the optimizer and annealer\n    * 3.3 Data augmentation\n* **4. Evaluate the model**\n    * 4.1 Training and validation curves\n    * 4.2 Confusion matrix\n* **5. Prediction and submition**\n    * 5.1 Predict and Submit results","fad37821":"## 2.2 Check for null and missing values","3c0b182e":"I used the Keras Sequential API, where you have just to add one layer at a time, starting from the input.\n","d352877e":"For the data augmentation, i choosed to :\n   - Randomly rotate some training images by 10 degrees\n   - Randomly  Zoom by 10% some training images\n   - Randomly shift images horizontally by 10% of the width\n   - Randomly shift images vertically by 10% of the height\n   \nI did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9.\n\nOnce our model is ready, we fit the training dataset .","1030dca8":"# 1. Introduction\n\nThis is a 5 layers Sequential Convolutional Neural Network for digits recognition trained on MNIST dataset. I choosed to build it with keras API (Tensorflow backend) which is very intuitive.\nI achieved 99.671% of accuracy with this CNN trained in 3h\nThis Notebook follows three main parts:\n* The data preparation\n* The CNN modeling and evaluation\n* The results prediction and submission","4a728bcd":"## 3.3 Data augmentation ","f389c97e":"# 2. Data preparation\n## 2.1 Load data","2f5ce58b":"# 3. CNN\n## 3.1 Define the model","076ed30c":"Labels are 10 digits numbers from 0 to 9. We need to encode these lables to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0]).","f50aaeb2":"I choosed RMSprop (with default values), it is a very effective optimizer. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate.\nWe could also have used Stochastic Gradient Descent ('sgd') optimizer, but it is slower than RMSprop."}}