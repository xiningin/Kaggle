{"cell_type":{"445a8d9c":"code","c4ba88c8":"code","68ec5670":"code","1092c39f":"code","aa4114ea":"code","b127b203":"code","0391477b":"code","0cffa409":"code","0b024165":"code","dac0db45":"code","39b5bcb3":"code","62f86fd9":"code","e047cad5":"code","8c24c9b8":"code","8247101a":"code","911972ee":"code","72ef8c3a":"code","631b135f":"code","1d63a44d":"code","8f1c2229":"code","6eda740f":"code","0b7fba83":"code","d3deb93d":"code","e45cec2f":"code","e0b11817":"code","0ab60eba":"code","0f952d03":"code","840784fc":"code","838a236f":"code","e185dd0d":"code","3b6a8bad":"code","e6b4761e":"code","b638ad94":"code","d20f311e":"code","6bb0ad96":"code","1625e326":"code","b13c5d8e":"code","05d532cf":"code","56c3ad5d":"code","bb6f9037":"code","3a1d80e4":"code","52b08345":"code","0d6c7e4d":"code","45894ee0":"code","daa30f58":"code","3b0001eb":"code","e900065a":"code","acadf35b":"code","95ec0ef0":"code","2367db79":"code","404ee2e7":"code","37e43619":"code","0a60427e":"code","70e4aa55":"code","02313d1b":"code","26287d9e":"code","9bcbb843":"code","aad04022":"code","8999a179":"code","0f953bf7":"code","790d93e8":"code","bf6e949a":"code","24ab1ed4":"code","343d129c":"code","c96bf831":"code","adb118ea":"code","7bbd95e2":"code","206f4665":"code","9a437bc2":"code","5e5b91d5":"code","b9a97531":"code","45f067ca":"code","e903a3a9":"code","9c20db88":"code","b03c43aa":"code","c67ca726":"code","6740070d":"code","f90e39e9":"code","beefd9f2":"code","e21b4390":"code","6be15475":"code","29ed0768":"code","353ed071":"code","d7980df6":"code","584d99ce":"code","28114a38":"code","243bb18a":"code","1ce49c7d":"code","d957d1c7":"code","3e4e9f52":"code","95de4ce2":"code","0376b89f":"code","95f3104a":"code","a5ab5aaf":"markdown","13a6099e":"markdown","336ceb41":"markdown","8cddbb32":"markdown","3c687304":"markdown","88e506bd":"markdown","588cc571":"markdown","95a1a003":"markdown","2ba2703e":"markdown","02f63d3f":"markdown","af2905d2":"markdown","673ebb48":"markdown","b34b3617":"markdown"},"source":{"445a8d9c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score,cross_val_predict\nfrom sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,precision_recall_curve,roc_auc_score,roc_curve\nimport gensim\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf\nfrom keras.preprocessing import text, sequence\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c4ba88c8":"path = '\/kaggle\/input\/fake-and-real-news-dataset\/'\nfake = pd.read_csv(path+'Fake.csv')\ntrue = pd.read_csv(path+'True.csv')","68ec5670":"fake.head(2)","1092c39f":"print('Shape ->',fake.shape)\nprint('Description ->',fake.describe())\nprint('Checking null values .. . ',fake.isnull().sum())","aa4114ea":"plt.figure(figsize=(12,6))\nsns.countplot(x='subject',data=fake)\nplt.show()","b127b203":"true.head(2)","0391477b":"true.title.value_counts()","0cffa409":"print('Shape ->',true.shape)\nprint('Description ->',true.describe())\nprint('Checking null values .. . ',true.isnull().sum())","0b024165":"plt.figure(figsize=(12,6))\nsns.countplot(x='subject',data=true)\nplt.show()","dac0db45":"true['category'] = 1\nfake['category'] = 0\ndf = pd.concat([true,fake])\nprint('Shape ->',df.shape)","39b5bcb3":"sns.countplot(df.category)\nplt.show()","62f86fd9":"plt.figure(figsize=(12,6))\nsns.countplot(df.subject)\nplt.show()","e047cad5":"df['combined_text'] = df['text'] + ' ' + df['title'] + ' ' +df['subject']\ndel df['title']\ndel df['text']\ndel df['subject']\ndel df['date']","8c24c9b8":"df.head(2)","8247101a":"import re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nnltk.download('stopwords')\nnltk.download('wordnet')","911972ee":"STOPWORDS = set(stopwords.words('english'))\npunctuations = string.punctuation\nSTOPWORDS.update(punctuations)","72ef8c3a":"def show_word_cloud(data,title=None):\n    word_cloud = WordCloud(\n        background_color = 'white',\n        max_words =1000,\n        width=1600,\n        height=800,\n        stopwords=STOPWORDS,\n        max_font_size = 40, \n        scale = 3,\n        random_state = 42 ).generate(data)\n    fig = plt.figure(1, figsize = (20, 20))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.3)\n\n    plt.imshow(word_cloud)\n    plt.show()","631b135f":"print(STOPWORDS)","1d63a44d":"def clean_text(text):\n    text = text.lower()\n    text = \" \".join([word for word in text.split() if word not in STOPWORDS])\n    text = re.sub(r'https?:\/\/\\S+|www\\.\\S+',r'',text)\n    text = re.sub('[\\d]',r'',text)\n    text = re.sub('[()]',r'',text)\n    text = re.sub(r'(<.*?>)',r'',text)\n    text = re.sub(r'[^(A-Za-z)]',r' ',text)\n    text = re.sub(r'\\s+',r' ',text)\n  \n    return text  ","8f1c2229":"df['text'] = df['combined_text'].apply(lambda x : clean_text(x))","6eda740f":"df[:30]","0b7fba83":"show_word_cloud(\" \".join(df[df.category == 1].text),'True Words')","d3deb93d":"show_word_cloud(\" \".join(df[df.category == 0].text),'Fake Words')","e45cec2f":"fake_len = df[df.category == 0].text.str.len()\ntrue_len = df[df.category == 1].text.str.len()\n\nplt.hist(fake_len, bins=20, label=\"fake_length\")\nplt.hist(true_len, bins=20, label=\"true_length\")\nplt.legend()\nplt.show()","e0b11817":"fake_word_token = word_tokenize(\" \".join(df[df.category == 0].text))\ntrue_word_token = word_tokenize(\" \".join(df[df.category == 1].text))","0ab60eba":"freq_20_fake = Counter(fake_word_token).most_common(20)\nfreq_20_true = Counter(true_word_token).most_common(20)","0f952d03":"def plot_most_comman_words(data,label):\n    most_comman_dict = {}\n    palette=''\n    for x in data:\n        tup = x\n        most_comman_dict[tup[0]] = tup[1]\n    d = pd.DataFrame({label: list(most_comman_dict.keys()),\n                  'Count': list(most_comman_dict.values())})\n    if label=='fake words':\n        palette='plasma_r'\n    else:\n        palette='rocket'\n    plt.figure(figsize=(12, 8))\n    ax = sns.barplot(data=d, x= \"Count\", y = label,palette=palette)\n    ax.set(ylabel = label)\n    plt.show()","840784fc":"plot_most_comman_words(freq_20_fake,'fake words')\nplot_most_comman_words(freq_20_true,'true words')","838a236f":"X = df.text\ny = df.category","e185dd0d":"x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","3b6a8bad":"vect = CountVectorizer()\nX_train = vect.fit_transform(x_train)","e6b4761e":"bow_clf = MultinomialNB()\nhistory = bow_clf.fit(X_train,y_train)","b638ad94":"history.score(X_train,y_train)","d20f311e":"val_score = cross_val_score(bow_clf,X_train,y_train,cv=10,scoring='accuracy')\nprint(val_score.mean())","6bb0ad96":"y_train_pred = cross_val_predict(bow_clf,X_train, y_train, cv=10)\ny_train_pred","1625e326":"bow_train_cm = confusion_matrix(y_train,y_train_pred)","b13c5d8e":"def cm_scores(cm):\n    TN = cm[0][0]\n    FP = cm[0][1]\n    FN = cm[1][0]\n    TP = cm[1][1]\n    precision = TP\/(TP+FP)\n    recall = TP\/(TP+FN)\n    f1_scre = 2*((precision*recall)\/(precision+recall))\n    return TN,FP,FN,TP,precision,recall,f1_scre","05d532cf":"TN,FP,FN,TP,precision,recall,f1_score = cm_scores(bow_train_cm)\nprint('Precision : ',precision)\nprint('Recall : ',recall)\nprint('F1 Score:',f1_score)","56c3ad5d":"precisions,recalls,thresholds = precision_recall_curve(y_train,y_train_pred)","bb6f9037":"def plot_precision_vs_recall(precisions,recalls):\n    plt.plot(recalls,precisions,'g-',linewidth=2)\n    plt.grid(True)\n    plt.ylabel('Precision')\n    plt.xlabel('Recall')\n\nplt.figure(figsize=(12, 10))","3a1d80e4":"def plot_precision_recall_vs_threshold(precisions,recalls,thresholds):\n    plt.figure(figsize=(8, 4))\n    plt.plot(thresholds,precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n    plt.plot(thresholds,recalls[:-1], \"g--\", label=\"Recall\", linewidth=2)\n    plt.legend(loc=\"center right\", fontsize=16) \n    plt.xlabel(\"Threshold\", fontsize=16)        \n    plt.grid(True)                              ","52b08345":"def plot_roc_curve(FPR,TPR,label):\n    plt.plot(FPR,TPR,'b--',linewidth=2,label=label)\n    plt.grid(True)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate (FPR)')\n    plt.ylabel('True Positive Rate (TPR)')\n    plt.legend(loc='best')\n    plt.show()","0d6c7e4d":"recalls[np.argmax(precisions >= 0.90)]","45894ee0":"plot_precision_vs_recall(precisions,recalls)\nplt.plot([0.9636, 0.9636], [0., 0.9], \"r:\")\nplt.plot([0.0, 0.9636], [0.9, 0.9], \"r:\")\nplt.plot([0.9636], [0.9], \"ro\")\nplt.show()","daa30f58":"plot_precision_recall_vs_threshold(precisions,recalls,thresholds)","3b0001eb":"FPR,TPR,thresholds = roc_curve(y_train,y_train_pred)","e900065a":"plot_roc_curve(FPR,TPR,'MultinomialNB')","acadf35b":"roc_auc_score = roc_auc_score(y_train,y_train_pred)\nprint('roc_auc_score -- >',roc_auc_score)","95ec0ef0":"X_test = vect.transform(x_test)","2367db79":"predictions = history.predict(X_test)\nprint(predictions[:10])","404ee2e7":"bow_cm_test = confusion_matrix(y_test,predictions)\nTN,FP,FN,TP,precision,recall,f1_score = cm_scores(bow_cm_test)\nprint('Precision : ',precision)\nprint('Recall : ',recall)\nprint('F1 Score:',f1_score)","37e43619":"tfidf = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\nX_train = tfidf.fit_transform(x_train.values)","0a60427e":"tfidf_clf = MultinomialNB()\nhistory = tfidf_clf.fit(X_train,y_train)\nprint('Model Score: ',history.score(X_train,y_train))","70e4aa55":"rf_clf = RandomForestClassifier(n_estimators=500, random_state=42).fit(X_train, y_train)\nprint('Model Score: ',rf_clf.score(X_train,y_train))","02313d1b":"val_score = cross_val_score(tfidf_clf,X_train,y_train,cv=5,scoring='accuracy')\nprint(val_score.mean())","26287d9e":"%time\nrf_val_score = cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='accuracy')\nprint(rf_val_score.mean())","9bcbb843":"y_train_pred = cross_val_predict(tfidf_clf,X_train, y_train, cv=5)\ny_train_pred","aad04022":"rf_y_train_pred = cross_val_predict(rf_clf,X_train, y_train, cv=5)\nrf_y_train_pred","8999a179":"tfidf_train_cm = confusion_matrix(y_train,y_train_pred)\nrf_tfidf_train_cm = confusion_matrix(y_train,rf_y_train_pred)","0f953bf7":"TN,FP,FN,TP,precision,recall,f1_score = cm_scores(tfidf_train_cm)\nprint('Precision : ',precision)\nprint('Recall : ',recall)\nprint('F1 Score:',f1_score)","790d93e8":"rf_TN,rf_FP,rf_FN,rf_TP,rf_precision,rf_recall,rf_f1_score = cm_scores(rf_tfidf_train_cm)\nprint('Precision : ',rf_precision)\nprint('Recall : ',rf_recall)\nprint('F1 Score:',rf_f1_score)","bf6e949a":"precisions,recalls,thresholds = precision_recall_curve(y_train,y_train_pred)\nrf_precisions,rf_recalls,rf_thresholds = precision_recall_curve(y_train,rf_y_train_pred)","24ab1ed4":"recalls[np.argmax(precisions >= 0.90)]","343d129c":"plot_precision_vs_recall(precisions,recalls)\nplt.plot([0.9336, 0.9336], [0., 0.9], \"r:\")\nplt.plot([0.0, 0.9336], [0.9, 0.9], \"r:\")\nplt.plot([0.9336], [0.9], \"ro\")\nplt.show()","c96bf831":"plot_precision_recall_vs_threshold(precisions,recalls,thresholds)","adb118ea":"FPR,TPR,thresholds = roc_curve(y_train,y_train_pred)\nrf_FPR,rf_TPR,rf_thresholds = roc_curve(y_train,rf_y_train_pred)","7bbd95e2":"plt.figure(figsize=(8, 6))\nplt.plot(rf_FPR,rf_TPR,\"g:\", linewidth=2,label='Random Forest')\nplt.grid(True)\nplt.legend(loc='best')\nplot_roc_curve(FPR,TPR,'MultinomialNB')","206f4665":"y_train_pred[:4]","9a437bc2":"X_test = tfidf.transform(x_test)","5e5b91d5":"predictions = history.predict(X_test)\nrf_predictions =rf_clf.predict(X_test)\nprint(rf_predictions[:10])","b9a97531":"tfidf_cm_test = confusion_matrix(y_test,predictions)\nTN,FP,FN,TP,precision,recall,f1_score = cm_scores(tfidf_cm_test)\nprint('Precision : ',precision)\nprint('Recall : ',recall)\nprint('F1 Score:',f1_score)","45f067ca":"rf_tfidf_cm_test = confusion_matrix(y_test,rf_predictions)\nrf_TN,rf_FP,rf_FN,rf_TP,rf_precision,rf_recall,rf_f1_score = cm_scores(rf_tfidf_cm_test)\nprint('Precision : ',rf_precision)\nprint('Recall : ',rf_recall)\nprint('F1 Score:',rf_f1_score)","e903a3a9":"max_features=1000\nmaxlen = 300","9c20db88":"GLOVE_MODEL = '..\/input\/glove-twitter\/glove.twitter.27B.100d.txt'","b03c43aa":"def get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(GLOVE_MODEL))","c67ca726":"print(embeddings_index.get(\"leaders\"))","6740070d":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(x_train)\ntokenized_train = tokenizer.texts_to_sequences(x_train)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","f90e39e9":"tokenized_test = tokenizer.texts_to_sequences(x_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","beefd9f2":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","e21b4390":"type(embedding_matrix)","6be15475":"batch_size = 256\nepochs = 5\nembed_size = 100","29ed0768":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)","353ed071":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n#LSTM \nmodel.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\nmodel.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\nmodel.add(Dense(units = 32 , activation = 'relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","d7980df6":"history = model.fit(X_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs , callbacks = [learning_rate_reduction])","584d99ce":"y_pred = model.predict_classes(X_test)\ny_pred","28114a38":"epochs = [i for i in range(5)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","243bb18a":"cm= confusion_matrix(y_test,y_pred)\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Reds\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Fake','Not Fake'] , yticklabels = ['Fake','Not Fake'])\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")","1ce49c7d":"TN,FP,FN,TP,precision,recall,f1_score = cm_scores(cm)\nprint('Precision : ',precision)\nprint('Recall : ',recall)\nprint('F1 Score:',f1_score)","d957d1c7":"precisions,recalls,thresholds = precision_recall_curve(y_test,y_pred)","3e4e9f52":"thrs = recalls[np.argmax(precisions >= 0.90)]","95de4ce2":"plot_precision_vs_recall(precisions,recalls)\nplt.plot([thrs, thrs], [0., 1.0], \"r:\")\nplt.plot([0.0, thrs], [1.0, 1.0], \"r:\")\nplt.plot([thrs], [1.0], \"ro\")\nplt.show()","0376b89f":"FPR,TPR,thresholds = roc_curve(y_test,y_pred)","95f3104a":"plot_roc_curve(FPR,TPR,'LSTM with Glove Emb')","a5ab5aaf":"Prediction","13a6099e":"* Word Distribution between True and Fake Words","336ceb41":"![fake_news](https:\/\/media-assets-02.thedrum.com\/cache\/images\/thedrum-prod\/s3-news-tmp-140656-fake_news--default--1280.jpg)","8cddbb32":"* Ploting Most comman True and Fake words in dataset","3c687304":"> Please Upvote if you find this kernel useful.Thankyou :)","88e506bd":"## ReduceLROnPlateau -> *Reduces learning rate when a metric has stopped improving.*","588cc571":"*Glove","95a1a003":"> Splitting into Train-Test","2ba2703e":"Word Embeddings","02f63d3f":"Predictions","af2905d2":"Combining Features ","673ebb48":"TF-IDF","b34b3617":"BOW"}}