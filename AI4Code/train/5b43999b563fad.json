{"cell_type":{"acb20a18":"code","ebe1eee9":"code","4f81a62a":"code","6fc8fa60":"code","b59aa5ad":"code","30bc5a24":"code","30a36baf":"code","70e63f34":"code","1a716d58":"markdown","08171197":"markdown","8a64b258":"markdown","39573687":"markdown","e7d0fcab":"markdown","42a50871":"markdown","9694d90d":"markdown","f33ef010":"markdown"},"source":{"acb20a18":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport glob\nimport itertools\nimport numpy\nimport os\nimport tensorflow\n\nfrom keras.callbacks import Callback\nfrom keras.callbacks import CSVLogger\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Concatenate\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers import Input\nfrom keras.layers import Reshape\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import UpSampling2D\nfrom keras.models import Model\nfrom matplotlib import pyplot\nfrom PIL import Image","ebe1eee9":"TIMESTAMP        = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n\nRANDOM_SEED      = 99999999\n\nARCHITECTURE     = 'fannet'\n\nSOURCE_CHARS     = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nTARGET_CHARS     = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n\nTRAIN_IMAGES_DIR = '..\/input\/google-fonts-for-stefann\/fannet\/fannet\/train\/'\nVALID_IMAGES_DIR = '..\/input\/google-fonts-for-stefann\/fannet\/fannet\/valid\/'\nPAIRS_IMAGES_DIR = '..\/input\/google-fonts-for-stefann\/fannet\/fannet\/pairs\/'\nOUTPUT_DIR       = 'output\/{}\/{}\/'.format(ARCHITECTURE, TIMESTAMP)\n\nIMAGE_FILE_EXT   = '.jpg'\nIMAGE_READ_MODE  = 'L'\n\nFUNCTION_OPTIM   = 'adam'\nFUNCTION_LOSS    = 'mae'\n\nINPUT_SHAPE_IMG  = (64, 64, 1)\nINPUT_SHAPE_HOT  = (len(SOURCE_CHARS), 1)\n\nSCALE_COEFF_IMG  = 1.\nBATCH_SIZE       = 64\nNUM_EPOCHS       = 10\n\nVERBOSE_LEVEL    = 2\n\nSAVE_IMAGES      = False\nSHOW_IMAGES      = True\nMAX_IMAGES       = 20","4f81a62a":"class DataGenerator(object):\n    def __init__(self, source_chars, target_chars, image_dir, image_ext='.jpg',\n                 mode='RGB', target_shape=(64, 64), rescale=1.0, batch_size=1,\n                 seed=None):\n        self._chars = source_chars\n        self._perms = list(itertools.product(list(source_chars),\n                                             list(target_chars),\n                                             os.listdir(image_dir)))\n        self._imdir = image_dir\n        self._imext = image_ext\n        self._imtyp = mode\n        self._shape = target_shape\n        self._scale = rescale\n        self._batch = batch_size\n        self._steps = int(len(self._perms) \/ self._batch + 0.5)\n        self._index = 0\n        numpy.random.seed(seed)\n        numpy.random.shuffle(self._perms)\n    \n    def flow(self):\n        while True:\n            x = []\n            y = []\n            onehot = []\n            endidx = self._index + self._batch\n            subset = self._perms[self._index:endidx]\n            numpy.random.shuffle(subset)\n            self._index = endidx if endidx < len(self._perms) else 0\n            for perm in subset:\n                ch_src = str(ord(perm[0]))\n                ch_dst = str(ord(perm[1]))\n                ch_fnt = perm[2]\n                im_src = os.path.join(self._imdir, ch_fnt, ch_src + self._imext)\n                im_dst = os.path.join(self._imdir, ch_fnt, ch_dst + self._imext)\n                try:\n                    img_x0 = Image.open(im_src).convert(self._imtyp).resize(self._shape)\n                    img_x0 = numpy.asarray(img_x0, dtype=numpy.uint8)\n                    img_x0 = numpy.atleast_3d(img_x0)\n                    img_y0 = Image.open(im_dst).convert(self._imtyp).resize(self._shape)\n                    img_y0 = numpy.asarray(img_y0, dtype=numpy.uint8)\n                    img_y0 = numpy.atleast_3d(img_y0)\n                except:\n                    continue\n                x.append(img_x0)\n                y.append(img_y0)\n                idx = self._chars.find(perm[1])\n                hot = [0] * len(self._chars)\n                hot[idx] = 1\n                onehot.append(numpy.asarray(hot, numpy.uint8).reshape(-1, 1))\n            x = numpy.asarray(x, numpy.float32) * self._scale\n            y = numpy.asarray(y, numpy.float32) * self._scale\n            onehot = numpy.asarray(onehot, numpy.float32)\n            yield [[x, onehot], y]","6fc8fa60":"class FANnet(object):\n    def __new__(self, input_shapes, optimizer, loss, weights=None):\n        # build network\n        x1 = Input(input_shapes[0])\n        x2 = Input(input_shapes[1])\n        \n        y1 = Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu')(x1)\n        y1 = Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu')(y1)\n        y1 = Conv2D(filters=1, kernel_size=(3, 3), padding='same', activation='relu')(y1)\n        y1 = Flatten()(y1)\n        y1 = Dense(units=512, activation='relu')(y1)\n        \n        y2 = Flatten()(x2)\n        y2 = Dense(units=512, activation='relu')(y2)\n        \n        y = Concatenate()([y1, y2])\n        y = Dense(units=1024, activation='relu')(y)\n        y = Dropout(0.5)(y)\n        y = Dense(units=1024, activation='relu')(y)\n        y = Reshape(target_shape=(8, 8, 16))(y)\n        y = UpSampling2D(size=(2, 2))(y)\n        y = Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu')(y)\n        y = UpSampling2D(size=(2, 2))(y)\n        y = Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu')(y)\n        y = UpSampling2D(size=(2, 2))(y)\n        y = Conv2D(filters=1, kernel_size=(3, 3), padding='same', activation='relu')(y)\n        \n        # compile network\n        model = Model(inputs=[x1, x2], outputs=y)\n        model.compile(optimizer=optimizer, loss=loss)\n        \n        # optionally load existing weights into network\n        try:\n            if not weights is None:\n                model.load_weights(weights)\n        except:\n            pass\n        \n        return model","b59aa5ad":"class ProgressMonitor(Callback):\n    def __init__(self, out_dir, charset, img_dir, img_ext='.jpg', mode='RGB',\n                 rescale=1.0, thumbnail_size=(64, 64), save=True, show=False, max_img=10):\n        self._out_dir = out_dir\n        self._charset = charset\n        self._img_dir = img_dir\n        self._img_ext = img_ext\n        self._img_typ = mode\n        self._rescale = rescale\n        self._tn_size = thumbnail_size\n        self._im_save = save\n        self._im_show = show\n        self._max_img = max_img\n    \n    def on_epoch_end(self, epoch, logs):\n        result = []\n        images = glob.glob(self._img_dir + '\/**\/*' + self._img_ext, recursive=True)[:self._max_img]\n        for image in images:\n            try:\n                im_org = Image.open(image).convert(self._img_typ)\n                im_src = im_org.crop((0, 0, im_org.width \/\/ 2, im_org.height))\n                im_dst = im_org.crop((im_org.width \/\/ 2, 0, im_org.width, im_org.height))\n                img_x0 = im_src.resize(self.model.input_shape[0][1:3])\n                img_x0 = numpy.asarray(img_x0, numpy.float32) * self._rescale\n                img_x0 = numpy.atleast_3d(img_x0)\n                img_x0 = numpy.expand_dims(img_x0, 0)\n                dst_ch = os.path.splitext(os.path.basename(image))[0].split('_')[-1]\n                idx_ch = self._charset.find(chr(int(dst_ch)))\n                onehot = [0] * len(self._charset)\n                onehot[idx_ch] = 1\n                onehot = numpy.asarray(onehot, numpy.uint8).reshape(1, -1, 1)\n                img_y0 = self.model.predict([img_x0, onehot])\n                img_y0 = numpy.squeeze(img_y0)\n                img_y0 = numpy.asarray(img_y0 \/ self._rescale, numpy.uint8)\n                im_prd = Image.fromarray(img_y0)\n                result.append([im_src, im_dst, im_prd])\n            except:\n                continue\n        result = self._combine_images(result, self._tn_size, border_width=4, padding=20)\n        if self._im_save:\n            impath = self._out_dir + '\/epoch_{}.jpg'.format(epoch + 1)\n            self._save_image(impath, result)\n        if self._im_show:\n            imdesc = 'Epoch {} - Top to Bottom: Source | Target | Generated'.format(epoch + 1)\n            self._show_image(result, imdesc)\n    \n    def _combine_images(self, images=[], size=(64, 64), bg_color=(0, 0, 0),\n                        border_color=(255, 255, 255), border_width=0, padding=0):\n        for i, result in enumerate(images):\n            w1 = size[0] + 2 * border_width\n            h1 = size[1] * len(result) + 2 * border_width\n            bg = Image.new('RGB', (w1, h1), border_color)\n            for j, image in enumerate(result):\n                x1 = border_width\n                y1 = border_width + j * size[1]\n                fg = image.convert('RGB').resize(size, resample=Image.BILINEAR)\n                bg.paste(fg, (x1, y1))\n            images[i] = bg\n        w2 = len(images) * (w1 + padding) + padding\n        h2 = h1 + 2 * padding\n        bg = Image.new('RGB', (w2, h2), bg_color)\n        for k, image in enumerate(images):\n            x2 = k * (w1 + padding) + padding\n            y2 = padding\n            bg.paste(image, (x2, y2))\n        return bg\n    \n    def _save_image(self, filepath, image):\n        directory = os.path.dirname(filepath)\n        if not os.path.isdir(directory) and directory != '':\n            os.makedirs(directory)\n        image.save(filepath)\n    \n    def _show_image(self, image, title=None):\n        pyplot.figure(figsize=(image.width\/100, image.height\/100), dpi=100)\n        pyplot.axis('off')\n        if title:\n            pyplot.title(title)\n        pyplot.imshow(numpy.uint8(image))\n        pyplot.show()","30bc5a24":"def tensorflow_version():\n    return int(tensorflow.__version__.split('.')[0])","30a36baf":"def train():\n    # setup seed for random number generators for reproducibility\n    numpy.random.seed(RANDOM_SEED)\n    \n    if tensorflow_version() == 2:\n        tensorflow.random.set_seed(RANDOM_SEED)\n    else:\n        tensorflow.set_random_seed(RANDOM_SEED)\n    \n    # setup paths\n    mdl_dir = os.path.join(OUTPUT_DIR, 'models')\n    log_dir = os.path.join(OUTPUT_DIR, 'logs')\n    cpt_dir = os.path.join(OUTPUT_DIR, 'checkpoints')\n    pro_dir = os.path.join(OUTPUT_DIR, 'progress')\n    \n    setup_flag = True\n    for directory in [TRAIN_IMAGES_DIR, VALID_IMAGES_DIR]:\n        if not os.path.isdir(directory):\n            print('[INFO] Data directory not found at {}'.format(directory))\n            setup_flag = False\n    if not os.path.isdir(PAIRS_IMAGES_DIR):\n        print('[INFO] Data directory not found at {}'.format(directory))\n    for directory in [OUTPUT_DIR, mdl_dir, log_dir, cpt_dir, pro_dir]:\n        if not os.path.isdir(directory):\n            os.makedirs(directory)\n        elif len(glob.glob(os.path.join(directory, '*.*'))) > 0:\n            print('[INFO] Output directory {} must be empty'.format(directory))\n            setup_flag = False\n    if not setup_flag:\n        return\n    \n    mdl_file = os.path.join(mdl_dir, '{}.json'.format(ARCHITECTURE))\n    log_file = os.path.join(log_dir, '{}_training.csv'.format(ARCHITECTURE))\n    cpt_file_best = os.path.join(cpt_dir, '{}_weights_best.h5'.format(ARCHITECTURE))\n    cpt_file_last = os.path.join(cpt_dir, '{}_weights_last.h5'.format(ARCHITECTURE))\n    \n    # initialize train data generator\n    train_datagen = DataGenerator(source_chars=SOURCE_CHARS,\n                                  target_chars=TARGET_CHARS,\n                                  image_dir=TRAIN_IMAGES_DIR,\n                                  image_ext=IMAGE_FILE_EXT,\n                                  mode=IMAGE_READ_MODE,\n                                  target_shape=INPUT_SHAPE_IMG[:2],\n                                  rescale=SCALE_COEFF_IMG,\n                                  batch_size=BATCH_SIZE,\n                                  seed=RANDOM_SEED)\n    \n    # initialize valid data generator\n    valid_datagen = DataGenerator(source_chars=SOURCE_CHARS,\n                                  target_chars=TARGET_CHARS,\n                                  image_dir=VALID_IMAGES_DIR,\n                                  image_ext=IMAGE_FILE_EXT,\n                                  mode=IMAGE_READ_MODE,\n                                  target_shape=INPUT_SHAPE_IMG[:2],\n                                  rescale=SCALE_COEFF_IMG,\n                                  batch_size=BATCH_SIZE,\n                                  seed=RANDOM_SEED)\n    \n    # build and serialize network\n    print('[INFO] Building network... ', end='')\n    fannet = FANnet(input_shapes=[INPUT_SHAPE_IMG, INPUT_SHAPE_HOT],\n                    optimizer=FUNCTION_OPTIM, loss=FUNCTION_LOSS, weights=None)\n    print('done')\n    fannet.summary()\n    \n    with open(mdl_file, 'w') as file:\n        file.write(fannet.to_json())\n    \n    # create callbacks\n    csv_logs = CSVLogger(filename=log_file, append=True)\n    cpt_best = ModelCheckpoint(filepath=cpt_file_best,\n                               monitor='val_loss',\n                               verbose=1,\n                               save_best_only=True,\n                               save_weights_only=True)\n    cpt_last = ModelCheckpoint(filepath=cpt_file_last,\n                               monitor='val_loss',\n                               verbose=0,\n                               save_best_only=False,\n                               save_weights_only=True)\n    progress = ProgressMonitor(out_dir=pro_dir,\n                               charset=SOURCE_CHARS,\n                               img_dir=PAIRS_IMAGES_DIR,\n                               img_ext=IMAGE_FILE_EXT,\n                               mode=IMAGE_READ_MODE,\n                               rescale=SCALE_COEFF_IMG,\n                               thumbnail_size=(64, 64),\n                               save=SAVE_IMAGES,\n                               show=SHOW_IMAGES,\n                               max_img=MAX_IMAGES)\n    \n    # train network\n    fannet.fit_generator(generator=train_datagen.flow(),\n                         steps_per_epoch=train_datagen._steps,\n                         epochs=NUM_EPOCHS,\n                         callbacks=[csv_logs, cpt_best, cpt_last, progress],\n                         validation_data=valid_datagen.flow(),\n                         validation_steps=valid_datagen._steps,\n                         verbose=VERBOSE_LEVEL)","70e63f34":"train()","1a716d58":"# STEFANN: Scene Text Editor using Font Adaptive Neural Network (CVPR 2020)\n\n>In this paper, we propose a generalized method for realistic modification of textual content present in a scene image at chracter-level. We approach the problem in two stages. At first, the unobserved character (target) is generated from an observed character (source) being modified. Next, we replace the source character with the generated target character maintaining both geometric and visual consistency with neighboring characters.\n\n[Project](https:\/\/prasunroy.github.io\/stefann) \u2022 [Paper](https:\/\/prasunroy.github.io\/static\/docs\/publications\/CVPR2020-8915.pdf) \u2022 [Video](https:\/\/www.youtube.com\/watch?v=HTVQXHPIKKo&list=PLfztDj7uiveWDLqGf41bheERg__t8JWWl) \u2022 [Code](https:\/\/github.com\/prasunroy\/stefann) \u2022 [CVF Open Access](http:\/\/openaccess.thecvf.com\/content_CVPR_2020\/html\/Roy_STEFANN_Scene_Text_Editor_Using_Font_Adaptive_Neural_Network_CVPR_2020_paper.html)\n\n![STEFANN](https:\/\/prasunroy.github.io\/static\/imgs\/publications\/CVPR2020-8915.png)\n\n## Starter \\#1: Font Generation with FANnet\n\nOur generation pipeline involves two separate neural networks:\n- **FANnet** to achieve structural consistency with source font\n- **Colornet** to preserve color of source font\n\n### This kernel demonstrates the font adaptive generation by FANnet. The code is adopted from the [original implementation](https:\/\/github.com\/prasunroy\/stefann).\n\nColornet is demonstrated in a separate kernel [Starter #2](https:\/\/www.kaggle.com\/prasunroy\/starter-2-color-transfer-stefann-cvpr-2020).\n\n## Citation\n\n```\n@InProceedings{Roy_2020_CVPR,\n  title     = {STEFANN: Scene Text Editor using Font Adaptive Neural Network},\n  author    = {Roy, Prasun and Bhattacharya, Saumik and Ghosh, Subhankar and Pal, Umapada},\n  booktitle = {The IEEE\/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month     = {June},\n  year      = {2020}\n}\n```","08171197":"## Custom Callback - Progress Monitor","8a64b258":"## Data Generator","39573687":"## Get TensorFlow Version Number","e7d0fcab":"## Configurations","42a50871":"## FANnet - Font Adaptive Neural Network\n\n![FANnet](attachment:fannet.svg)","9694d90d":"## Imports","f33ef010":"## Train FANnet"}}