{"cell_type":{"a7002e55":"code","252d0439":"code","a54b012a":"code","753a4fac":"code","a809d9e6":"code","e58d5544":"code","6c30cebd":"code","2cbfb2b1":"code","a5691d3b":"code","58ef6375":"code","29ff19cd":"code","e9f6ca77":"markdown","9e541e89":"markdown","f0154d39":"markdown","14d21e51":"markdown","17cb1e73":"markdown","8a546bb9":"markdown","01e1d32d":"markdown"},"source":{"a7002e55":"import tensorflow as tf\nimport tensorflow.keras as keras\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython import display\nfrom tqdm.auto import trange","252d0439":"def plot_results(images, n_cols=None, title=None):\n    \n    n_cols = n_cols or len(images)\n    n_rows = (len(images) - 1) \/\/ n_cols + 1\n\n    if images.shape[-1] == 1:\n        images = np.squeeze(images, axis=-1)\n    \n    fig = plt.figure(figsize=(n_cols, n_rows))\n    \n    for index, image in enumerate(images):\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(image, cmap=\"binary\")\n        plt.axis(\"off\")\n        \n    plt.suptitle(title)","a54b012a":"BATCH_SIZE = 128\nCODINGS_SIZE = 32\nN_EPOCHS = 150\nD_STEPS = 5\nGP_WEIGHT = 10.0\n\nN_CHANNELS = 1\nN_CLASSES = 10\nIM_SIZE = 28\n\nG_INP_CHANNELS = CODINGS_SIZE + N_CLASSES\nD_INP_CHANNELS = N_CHANNELS + N_CLASSES","753a4fac":"def prepare_data(batch_size):\n    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n    X_all = np.concatenate([X_train, X_test])\n    y_all = np.concatenate([y_train, y_test])\n    \n    X_all = X_all.astype(np.float32) \/ 255\n    X_all = X_all.reshape(-1, 28, 28, 1) * 2. - 1.\n    y_all = keras.utils.to_categorical(y_all, 10)\n\n    dataset = tf.data.Dataset.from_tensor_slices((X_all, y_all))\n    dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n    \n    return dataset\n\ndef prepare_images(label):\n    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n    X_all = np.concatenate([X_train, X_test])\n    y_all = np.concatenate([y_train, y_test])\n    \n    X_all = X_all.astype(np.float32) \/ 255\n    X_all = X_all.reshape(-1, 28, 28, 1) * 2. - 1.\n    X_train = X_all[np.where(y_all == label)]\n    \n    return X_train","a809d9e6":"def build_generator():\n    inputs = keras.Input(shape=[G_INP_CHANNELS])\n    x = keras.layers.Dense(7 * 7 * 128)(inputs)\n    x = keras.layers.Reshape([7, 7, 128])(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"SAME\", activation=\"selu\")(x)\n    x = keras.layers.BatchNormalization()(x)\n    skip = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"SAME\", activation=\"selu\")(x)\n    skip = keras.layers.BatchNormalization()(skip)\n    skip = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"SAME\", activation=\"selu\")(skip)\n    skip = keras.layers.BatchNormalization()(skip)\n    x = keras.layers.add([x, skip])\n    x = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding=\"SAME\",activation=\"selu\")(x)\n    skip = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"SAME\", activation=\"selu\")(x)\n    skip = keras.layers.BatchNormalization()(skip)\n    skip = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"SAME\", activation=\"selu\")(skip)\n    skip = keras.layers.BatchNormalization()(skip)\n    x = keras.layers.add([x, skip])\n    outputs = keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"SAME\",activation=\"tanh\")(x)\n    return keras.models.Model(inputs, outputs, name='generator')","e58d5544":"def build_discriminator():\n    return keras.models.Sequential([\n    keras.layers.Conv2D(64, kernel_size=3, strides=1, padding=\"SAME\", activation=keras.layers.LeakyReLU(0.2), input_shape=[28, 28, D_INP_CHANNELS]),\n    keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"SAME\", activation=keras.layers.LeakyReLU(0.2)),\n    keras.layers.Conv2D(128, kernel_size=3, strides=2, padding=\"SAME\", activation=keras.layers.LeakyReLU(0.2)),\n    keras.layers.Dropout(0.4),\n    keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"SAME\", activation=keras.layers.LeakyReLU(0.2)),\n    keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"SAME\", activation=keras.layers.LeakyReLU(0.2)),\n    keras.layers.Conv2D(128, kernel_size=3, strides=2, padding=\"SAME\", activation=keras.layers.LeakyReLU(0.2)),\n    keras.layers.Dropout(0.4),\n    keras.layers.Flatten(),\n    keras.layers.Dense(1)\n], name='discriminator')","6c30cebd":"class ConditionalGAN(keras.Model):\n    def __init__(\n        self,\n        discriminator,\n        generator,\n        latent_dim,\n    ):\n        super().__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n\n    def compile(self, d_optimizer, g_optimizer, loss_fn):\n        super().compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.loss_fn = loss_fn\n\n    def train_step(self, data):\n        # Unpack the data.\n        real_images, one_hot_labels = data\n        image_size = real_images.shape[1]\n        num_classes = one_hot_labels.shape[-1]\n        \n        # Add dummy dimensions to the labels so that they can be concatenated with\n        # the images. This is for the discriminator.\n        image_one_hot_labels = one_hot_labels[:, :, None, None]\n        image_one_hot_labels = tf.repeat(\n            image_one_hot_labels, repeats=[image_size * image_size]\n        )\n        image_one_hot_labels = tf.reshape(\n            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n        )\n\n        # Sample random points in the latent space and concatenate the labels.\n        # This is for the generator.\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n        random_vector_labels = tf.concat(\n            [random_latent_vectors, one_hot_labels], axis=1\n        )\n\n        # Decode the noise (guided by labels) to fake images.\n        generated_images = self.generator(random_vector_labels)\n\n        # Combine them with real images. Note that we are concatenating the labels\n        # with these images here.\n        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n        combined_images = tf.concat(\n            [fake_image_and_labels, real_image_and_labels], axis=0\n        )\n\n        # Assemble labels discriminating real from fake images.\n        labels = tf.concat(\n            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n        )\n\n        # Train the discriminator.\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(combined_images)\n            d_loss = self.loss_fn(labels, predictions)\n        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n        self.d_optimizer.apply_gradients(\n            zip(grads, self.discriminator.trainable_weights)\n        )\n\n        # Sample random points in the latent space.\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n        random_vector_labels = tf.concat(\n            [random_latent_vectors, one_hot_labels], axis=1\n        )\n\n        # Assemble labels that say \"all real images\".\n        misleading_labels = tf.zeros((batch_size, 1))\n\n        # Train the generator (note that we should *not* update the weights\n        # of the discriminator)!\n        with tf.GradientTape() as tape:\n            fake_images = self.generator(random_vector_labels)\n            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n            predictions = self.discriminator(fake_image_and_labels)\n            g_loss = self.loss_fn(misleading_labels, predictions)\n        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n\n        return {\"d_loss\": d_loss, \"g_loss\": g_loss} ","2cbfb2b1":"generator = build_generator()\ndiscriminator = build_discriminator()\nprint('Generator Summary\\n\\n')\ngenerator.summary()\nprint('\\n\\nDiscriminator Summary\\n\\n')\ndiscriminator.summary()\nkeras.utils.plot_model(generator, show_shapes=True, expand_nested=True, to_file='generator.png')\nkeras.utils.plot_model(discriminator, show_shapes=True, expand_nested=True, to_file='discriminator.png')\nfig, ax = plt.subplots(1, 2, figsize=(20, 12))\nax[0].imshow(plt.imread('generator.png'))\nax[0].set_title('Generator', fontsize=18)\nax[1].imshow(plt.imread('discriminator.png'))\nax[1].set_title('Discriminator', fontsize=18)\nax[0].axis(\"off\")\nax[1].axis(\"off\")\nplt.show()","a5691d3b":"dataset = prepare_data(BATCH_SIZE)\n\ngan = ConditionalGAN(\n    discriminator=discriminator, generator=generator, \n    latent_dim=CODINGS_SIZE\n)\ngan.compile(\n    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True)\n)\n\nhistory = gan.fit(dataset, epochs=N_EPOCHS, verbose=1)\n\nfig, ax = plt.subplots(figsize=(20, 6))\nax.set_title(f'Learning Curve', fontsize=18)\npd.DataFrame(history.history).plot(ax=ax)\nax.grid()\n\ngenerator.save(f'MNIST-AUG-ConditionalGAN.h5')","58ef6375":"from scipy.linalg import sqrtm\n\ndef frechet_distance(act1, act2):\n    mu1, sigma1 = np.mean(act1, axis=0), np.cov(act1, rowvar=False)\n    mu2, sigma2 = np.mean(act2, axis=0), np.cov(act2, rowvar=False)\n    ssdiff = np.sum((mu1 - mu2)**2.0)\n    covmean = sqrtm(sigma1.dot(sigma2))\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n    return fid\n\nevaluator = keras.models.Sequential(keras.models.load_model('..\/input\/mnist-net\/mnist_net.h5').layers[:-1])\nscores = []\ngenerator = keras.models.load_model(f'MNIST-AUG-ConditionalGAN.h5')\n\nfor i in range(10):\n    inp_vect = tf.concat(\n        (tf.random.normal([128, CODINGS_SIZE]), keras.utils.to_categorical(np.ones((128, 1))*i, 10)), \n        axis=1\n    )\n    fake_images = generator(inp_vect)\n    embeddings_real = evaluator(prepare_images(i))\n    embeddings_fake = evaluator(fake_images)\n    scores.append(frechet_distance(embeddings_real, embeddings_fake))\n    plot_results(fake_images, 16, f'Images Generated for class {i}')                     \n    plt.show()  ","29ff19cd":"pd.Series(scores, name=\"Frechet Distance\")","e9f6ca77":"# Generated Images","9e541e89":"# GAN in Action","f0154d39":"# Prepare the Dataset","14d21e51":"# Evaluation","17cb1e73":"# Build the Model\n\nGenerative Adversarial Networks (GANs) let us generate novel image data, video data, or audio data from a random input. Typically, the random input is sampled from a normal distribution, before going through a series of transformations that turn it into something plausible (image, video, audio, etc.).\n\nHowever, a simple DCGAN doesn't let us control the appearance (e.g. class) of the samples we're generating. For instance, with a GAN that generates MNIST handwritten digits, a simple DCGAN wouldn't let us choose the class of digits we're generating. To be able to control what we generate, we need to condition the GAN output on a semantic input, such as the class of an image.\n\n### Generator\n\nFor the generator, we take in random noise and eventually transform it to the shape of the MNIST images. The general steps are:\n\n* Feed the input noise to a dense layer.\n* Reshape the output to have three dimensions. This stands for the (length, width, number of filters).\n* Perform a deconvolution (with Conv2DTranspose), reducing the number of filters by half and using a stride of `2`.\n* The final layer upsamples the features to the size of the training images. In this case 28 x 28 x 1.\n\nNotice that batch normalization is performed except for the final deconvolution layer. As best practice, `selu` is the activation used for the intermediate deconvolution while `tanh` is for the output.\n\n### Discriminator\n\nThe discriminator will use strided convolutions to reduce the dimensionality of the input images. As best practice, these are activated by LeakyRELU. The output features will be flattened and fed to a 1-unit dense layer without any activation.","8a546bb9":"# Conditional GAN -MNIST Augmentation","01e1d32d":"## Utilities"}}