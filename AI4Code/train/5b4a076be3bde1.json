{"cell_type":{"a8e3da7f":"code","833506f8":"code","10322a87":"code","ad04dda4":"code","1038d145":"code","f249ef3d":"code","5874fc21":"code","05084941":"code","fedc3d20":"code","fabfb3af":"code","ba4f63ed":"code","044c62c1":"code","4db89921":"code","5a89d548":"code","4b0f1ffc":"code","8ce7cc5a":"code","2f445227":"code","8808ba7b":"code","c111c0fb":"code","10b6b6dc":"code","8a40c052":"code","360befd7":"code","3afd0e3f":"code","5d4543f1":"code","f1fb73a3":"code","605464dc":"code","deff60f1":"code","f5fd540e":"code","2a299ced":"code","c2bfb39e":"code","bee2b719":"code","6d8b196f":"code","6f00b1f9":"code","647d65fb":"code","d341f33b":"markdown","9038388b":"markdown","252de162":"markdown","4b0d1ed4":"markdown","4ad68897":"markdown","940ba163":"markdown","01498a69":"markdown","476e6338":"markdown","44fa064b":"markdown","a2b8495b":"markdown","88106958":"markdown","098d7260":"markdown","dd2c43bc":"markdown"},"source":{"a8e3da7f":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom matplotlib import pyplot as plt \nimport seaborn as sns\nsns.set(color_codes = True)","833506f8":"# initialise some random values for multi class classfification problem. \n\nX  = np.random.rand(15,5)\nY = np.random.randint(3, size=(15,1))\n\n# We dont need to standarise these values as they np.random.rand intitialises uniform distribution between 0 & 1. ","10322a87":"np.divide(X[1,:],X[2,:])","ad04dda4":"# architecture\n\nX_eg  = np.concatenate((np.ones((15,1)),X), axis=1)    # put a constant feature which will capture constant coef. \n\nprint(X_eg.shape)\n# we want one hidden layer of  size 7. \n\nWeights_1_eg = np.random.rand(6,7)\n\nprint(Weights_1_eg.shape)\n\nhidden_layer_1_eg = np.dot(X_eg,Weights_1_eg)\n\nhidden_layer_1_eg  = np.concatenate((np.ones((15,1)),hidden_layer_1_eg), axis=1)    # put a constant feature which will capture constant coef.\n\n# There will be an activation here:\n\nprint(hidden_layer_1_eg.shape)\n\nWeights_2_eg = np.random.rand(8,3)\n\nprint(Weights_2_eg.shape)\n\nOutput_layer_eg = np.dot(hidden_layer_1_eg,Weights_2_eg)\n\n# There will be an activation here:\n\nprint(Output_layer_eg.shape)","1038d145":"def sigmoid(z):\n    a = 1\/(1+ np.exp(-z))\n    return a","f249ef3d":"def sigmoidGradient(z):\n    a = sigmoid(z) * (1-sigmoid(z))\n    return a","5874fc21":"# Note: All the hastags are just running examlpes for better understanding. They are arbitaray to final output. \n\n\ndef One_hidden_layer_ANN(X,y,len_of_hidden_layer,alpha,epoch):\n    m = len(X)   # 3,5   ---> 3\n    X = np.concatenate((np.ones((m,1)),X), axis=1) #3,6 \n    y_mx = np.array(pd.get_dummies(y.ravel())) #3,4\n    len_output_layer = y_mx.shape[1] #4\n    \n    Weights_1 = np.random.randn(X.shape[1],len_of_hidden_layer) * np.sqrt(2\/X.shape[1]) #6,2  # he initialisation\n    Weights_2 = np.random.randn(len_of_hidden_layer+1,len_output_layer) * np.sqrt(2\/len_of_hidden_layer+1) #3,4  \n    \n    J = []   \n    \n    Gradient_Weight_1 = np.zeros((Weights_1.shape)) #6,2\n    Gradient_Weight_2 = np.zeros((Weights_2.shape)) #3,4\n    \n    for i in range(0,epoch):\n        Z2 = np.dot(X,Weights_1) # 3,6 * 6,2 = 3,2\n        A2 = sigmoid(Z2) #3,2\n        A2_with_constant = np.concatenate((np.ones((A2.shape[0],1)),A2), axis=1) #3,3\n        Z3 = np.dot(A2_with_constant,Weights_2) #3,3 * 3,4 --> 3,4\n        A3 = sigmoid(Z3) #3,4\n        J.append((-1 \/ m) * sum(sum(y_mx*np.log(A3) + (1 - y_mx) * np.log(1 - A3))))\n        if i == 0:\n            for t in range(0,m):\n                x1 = np.reshape(X[t,:],(1,X.shape[1]))# 1,6\n                z2 = np.reshape(Z2[t,:],(1,Z2.shape[1]))# 1,2\n                a2 = np.reshape(A2[t,:],(1,A2.shape[1])) #1,2\n                a2_with_constant = np.reshape(A2_with_constant[t,:],(1,A2_with_constant.shape[1]))# 1,3\n                #z3 = np.reshape(Z3[t,:],(1,Z3.shape[1]))# 1,4\n                a3 = np.reshape(A3[t,:],(1,A3.shape[1])) #1,4\n                y_i = np.reshape(y_mx[t,:],(1,y_mx.shape[1]))# 1,4\n                d3 = a3 - y_i #1,4\n                #d3 = - (np.divide(y_i, a3) - np.divide(1 - y_i, 1 - a3))\n                Gradient_Weight_2 = Gradient_Weight_2 +  np.dot(a2_with_constant.T,d3) # 3,1 * 1,4 --> 3,4\n\n                d2 = np.dot(d3,Weights_2.T) * np.concatenate((np.ones((1,1)),sigmoidGradient(z2)), axis=1)    #  1,4 4,3 ---> 1,3 * 1,3 = 1,3\n                Gradient_Weight_1 = Gradient_Weight_1 + np.dot(x1.T,d2[:,1:]) #  6,1 1,2  6,2\n\n            Weights_1 = Weights_1 - (alpha\/m) * Gradient_Weight_1 \n            Weights_2 = Weights_2 - (alpha\/m) * Gradient_Weight_2\n        if i != 0:\n            if J[i] > J[i-1]:\n                break\n            else:\n                for t in range(0,m):\n                    x1 = np.reshape(X[t,:],(1,X.shape[1]))# 1,6\n                    z2 = np.reshape(Z2[t,:],(1,Z2.shape[1]))# 1,2\n                    a2 = np.reshape(A2[t,:],(1,A2.shape[1])) #1,2\n                    a2_with_constant = np.reshape(A2_with_constant[t,:],(1,A2_with_constant.shape[1]))# 1,3\n                    #z3 = np.reshape(Z3[t,:],(1,Z3.shape[1]))# 1,4\n                    a3 = np.reshape(A3[t,:],(1,A3.shape[1])) #1,4\n                    y_i = np.reshape(y_mx[t,:],(1,y_mx.shape[1]))# 1,4\n                    #d3 = - (np.divide(y_i, a3) - np.divide(1 - y_i, 1 - a3))\n                \n                    d3 = a3 - y_i #1,4\n                    Gradient_Weight_2 = Gradient_Weight_2 +  np.dot(a2_with_constant.T,d3) # 3,1 * 1,4 --> 3,4\n\n                    d2 = np.dot(d3,Weights_2.T) * np.concatenate((np.ones((1,1)),sigmoidGradient(z2)), axis=1)    #  1,4 4,3 ---> 1,3 * 1,3 = 1,3\n                    Gradient_Weight_1 = Gradient_Weight_1 + np.dot(x1.T,d2[:,1:]) #  6,1 1,2  6,2\n                Weights_1 = Weights_1 - (alpha\/m) * Gradient_Weight_1 \n                Weights_2 = Weights_2 - (alpha\/m) * Gradient_Weight_2\n                \n                \n            \n            \n    return J\n    \n    \n            \n            \n              \n        ","05084941":"J = One_hidden_layer_ANN(X,Y,6,0.01,20)\n\nJ = pd.DataFrame(J,columns=['J'])\nJ.reset_index().plot(x='index', y='J')","fedc3d20":"J = One_hidden_layer_ANN(X,Y,6,0.01,20)\n\nJ = pd.DataFrame(J,columns=['J'])\nJ.reset_index().plot(x='index', y='J')","fabfb3af":"J[:-1]","ba4f63ed":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = Sequential()\n","044c62c1":"model = Sequential()\nmodel.add(Dense(units=6, activation='sigmoid',name=\"hidden_layer\"))\nmodel.add(Dense(units=3, activation='sigmoid',name=\"output_layer\"))\nmodel(X)\nmodel.summary()\n","4db89921":"Y = np.array(pd.get_dummies(Y.ravel()))","5a89d548":"model.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n# This builds the model for the first time:\nhistory = model.fit(X,Y,epochs=len(J))\n","4b0f1ffc":"f = pd.DataFrame(history.history[\"loss\"],columns=['f'])\nf.reset_index().plot(x='index', y='f')","8ce7cc5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" dir\"ectory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f445227":"%cd \/kaggle\/input\/mnist-data-for-digit-recognation\/","8808ba7b":"from mlxtend.data import loadlocal_mnist\nimport platform","c111c0fb":"X, y = loadlocal_mnist(\n        images_path='train-images.idx3-ubyte', \n        labels_path='train-labels.idx1-ubyte')","10b6b6dc":"X.shape","8a40c052":"y.shape","360befd7":"print('Digits:  0 1 2 3 4 5 6 7 8 9')\nprint('labels: %s' % np.unique(y))\nprint('Class distribution: %s' % np.bincount(y))","3afd0e3f":"rows = 3\ncols = 4\nfig, ax = plt.subplots(rows, cols, figsize=(10,10))\nfig.suptitle(\"Showing one random image from dataset\", y=1.05, fontsize=24) # Adding  y=1.05, fontsize=24 helped me fix the suptitle overlapping with axes issue\nfor i in range(rows):\n  for j in range(cols):\n    img = X[np.random.choice(X.shape[0])].reshape(28,28,1)\n    ax[i][j].imshow(img)\n    \nplt.setp(ax, xticks=[],yticks=[])\nplt.tight_layout()\n","5d4543f1":" import numpy as np\n from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n  X, y, test_size=0.25, random_state=42\n)","f1fb73a3":"model = Sequential()\nmodel.add(Dense(units=20, activation='sigmoid',name=\"hidden_layer\"))\nmodel.add(Dense(units=10, activation='sigmoid',name=\"output_layer\"))\nmodel(X_train)\nmodel.summary()\n","605464dc":"from keras import optimizers\n\noptimizer = optimizers.Adam(learning_rate=0.0001)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\n# This builds the model for the first time:\nhistory = model.fit(X_train,pd.get_dummies(y_train).values,epochs=15)\n","deff60f1":"y_pred  = model.predict(X_test)","f5fd540e":"y_pred = np.argmax(y_pred, axis=1)\n\n#Returns the indices of the maximum values along an axis.","2a299ced":"from sklearn.metrics import accuracy_score","c2bfb39e":"accuracy_score(y_pred,y_test)","bee2b719":"\n\nfrom keras import layers, models, optimizers\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.models import Sequential\n\n","6d8b196f":"model = Sequential()\nmodel.add(Conv2D(filters = 64, kernel_size = 3, strides=(1,1), padding='Same', activation='relu', input_shape = [28,28,1]))\nmodel.add(Conv2D(filters = 64, kernel_size = 3, strides=(2,2), padding='Same', activation='relu'))\nmodel.add(MaxPool2D(pool_size = (2,2)))\n\nmodel.add(Conv2D(filters = 128, kernel_size = 3, strides=(1,1), padding='Same', activation='relu'))\nmodel.add(Conv2D(filters = 128, kernel_size = 3, strides=(2,2), padding='Same', activation='relu'))\nmodel.add(MaxPool2D(pool_size = (2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dropout(0.10))\nmodel.add(Dense(10, activation=\"softmax\"))","6f00b1f9":"X_train.shape","647d65fb":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n# This builds the model for the first time:\nhistory = model.fit(np.reshape(X_train,(45000,28,28,1)),pd.get_dummies(y_train).values,epochs=10)","d341f33b":"# Lets implement our own ANN and check how it performs against TF or keras!!","9038388b":"# Lets apply a ANN on our MNIST dataset using Keras API. ","252de162":"### Upvote if you like !!! Cheers.!!!","4b0d1ed4":"1. Shapes of matrices are fundamental for any vectorization. Its a agood idea to be clear about it. ","4ad68897":"\n\n<p><img src=\"https:\/\/miro.medium.com\/max\/1100\/1*YgJ6SYO7byjfCmt5uV0PmA.png\" alt=\"Alt Text\" title=\"Optional Title\"><\/p>\n","940ba163":"# This implementaion uses vectorization method of matrix multiplication. Therefore we are not using any for loops instead passing the whole dataset once for one epoch. ","01498a69":"## Not Bad for a simple ANN with one hidden layer. ","476e6338":"## + 99% accuracy. But This notebook was not about CNN. Therefore it was rough implementaion. ","44fa064b":"## Conclusion\n\n1. Just three lines of code vs. 50 lines. \n2. Robust and stable. \n3. Better performance. ","a2b8495b":"## Can we do better?","88106958":"## Now lets use Tensorflow library for an easy implementaion and see how it performs. !!!","098d7260":"# Deciding the architecture of the ANN. \n\n1. The input layer and the  output layer is dependent on the dataset.  \n2. In this case. input layer is of dimensions: $ X^{i} \u2208 \\mathbb{R}^{1,n} $  Where n means no.of features. \n3. The output layer is of dimensions: $ X^{-i} --> y \u2208 {unique(y)} $\n4. We need to decide the number of hidden layers and the hidden inputs in each hidden layer. \n5. We need to initialise the set of Weights. In a one hidden layer ANN example we will have two sets of weights and each set\n   will have the lenghth of  $X^{i}$ * the length $X^{i+1}$ no. of weights. \n6. Its Crucial there should be a randm initialisation and not by any scaler value. ","dd2c43bc":"## The answer is CNN which perfomrs amazing on images. "}}